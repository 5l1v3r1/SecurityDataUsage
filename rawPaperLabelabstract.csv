Data,|abstract
the semantics of online authentication in the web are rather
straightforward:
if alice has a certiﬁcate binding bob’s
name to a public key, and if a remote entity can prove knowl-
edge of bob’s private key, then (barring key compromise)
that remote entity must be bob. however, in reality, many
websites—and the majority of the most popular ones—are
hosted at least in part by third parties such as content deliv-
ery networks (cdns) or web hosting providers. put simply:
administrators of websites who deal with (extremely) sensi-
tive user data are giving their private keys to third parties.
importantly, this sharing of keys is undetectable by most
users, and widely unknown even among researchers.

in this paper, we perform a large-scale measurement study
of key sharing in today’s web. we analyze the prevalence
with which websites trust third-party hosting providers with
their secret keys, as well as the impact that this trust has on
responsible key management practices, such as revocation.
our results reveal that key sharing is extremely common,
with a small handful of hosting providers having keys from
the majority of the most popular websites. we also ﬁnd that
hosting providers often manage their customers’ keys, and
that they tend to react more slowly yet more thoroughly to
compromised or potentially compromised keys.

1.

|
Data,|abstract 
advanced persistent threats (apts) are a new breed of internet 
based  smart  threats,  which  can  go  undetected  with  the  existing 
state of-the-art internet traffic monitoring and protection systems.  
with  the  evolution  of  internet  and  cloud  computing,  a  new 
generation  of  smart  apt  attacks  has  also evolved  and  signature 
based  threat  detection  systems  are  proving  to  be  futile  and 
insufficient. one of the essential strategies in detecting apts is to 
continuously  monitor  and  analyze  various  features  of  a  tcp/ip 
connection,  such  as  the  number  of  transferred  packets,  the  total 
count  of  the  bytes  exchanged,  the  duration  of  the  tcp/ip 
connections,  and  details  of  the  number  of  packet  flows.  the 
current  threat  detection  approaches  make  extensive  use  of 
machine learning algorithms that utilize statistical and behavioral 
knowledge  of  the  traffic.  however,  the  performance  of  these 
algorithms  is  far  from  satisfactory  in  terms  of  reducing  false 
negatives  and  false  positives  simultaneously.  mostly,  current 
algorithms  focus  on  reducing  false  positives,  only.  this  paper 
presents  a  fractal  based  anomaly  classification  mechanism,  with 
the  goal  of  reducing  both  false  positives  and  false  negatives, 
simultaneously.  a  comparison  of  the  proposed  fractal  based 
method  with  a  traditional  euclidean  based  machine  learning 
algorithm  (k-nn)  shows  that  the  proposed  method  significantly 
outperforms  the  traditional  approach  by  reducing  false  positive 
and  false  negative  rates,  simultaneously,  while  improving  the 
overall classification rates.  
general terms 
cyber security 
keywords 
advanced persistent threats (apt); remote trojans; machine 
learning; classification; cyber threats; complexity; multifractal 
 

 
 
 
permission to make digital or hard copies of all or part of this work for personal or 
classroom use is granted without fee provided that copies are not made or distributed 
for  profit  or  commercial  advantage  and  that  copies  bear  this  notice  and  the  full 
citation on the first page. copyrights for components of this work owned by others 
than acm must be honored. abstracting with credit is permitted. to copy otherwise, 
or  republish,  to  post  on  servers  or  to  redistribute  to  lists,  requires  prior  specific 
permission and/or a fee. request permissions from permissions@acm.org. 
iwspa'16, march 11 2016, new orleans, la, usa  
© 2016 acm. isbn 978-1-4503-4077-9/16/03…$15.00  
doi: http://dx.doi.org/10.1145/2875475.2875484  

 
 

 
1.  |
Data,|abstract

people travel in the real world and leave their location history in a form of trajectories. these trajec-
tories do not only connect locations in the physical world but also bridge the gap between people and
locations. this paper introduces a social networking service, called geolife, which aims to understand
trajectories, locations and users, and mine the correlation between users and locations in terms of user-
generated gps trajectories. geolife offers three key applications scenarios: 1) sharing life experiences
based on gps trajectories; 2) generic travel recommendations, e.g., the top interesting locations, travel
sequences among locations and travel experts in a given region; and 3) personalized friend and location
recommendation.

1

|
Non-data,|abstract
secure multilinear maps (mmaps) have been shown to have
remarkable applications in cryptography, such as multi-input
functional encryption (mife) and program obfuscation. to
date, there has been little evaluation of the performance of
these applications.
in this paper we initiate a systematic
study of mmap-based constructions. we build a general
framework, called 5gen, to experiment with these applica-
tions. at the top layer we develop a compiler that takes
in a high-level program and produces an optimized matrix
branching program needed for the applications we consider.
next, we optimize and experiment with several mife and
obfuscation constructions and evaluate their performance.
the 5gen framework is modular and can easily accommo-
date new mmap constructions as well as new mife and
obfuscation constructions, as well as being an open-source
tool that can be used by other research groups to experiment
with a variety of mmap-based constructions.

1.

|
Non-data,|abstract
cryptocurrencies, such as bitcoin and 250 similar alt-coins, em-
body at their core a blockchain protocol — a mechanism for a dis-
tributed network of computational nodes to periodically agree on
a set of new transactions. designing a secure blockchain protocol
relies on an open challenge in security, that of designing a highly-
scalable agreement protocol open to manipulation by byzantine or
arbitrarily malicious nodes. bitcoin’s blockchain agreement proto-
col exhibits security, but does not scale: it processes 3–7 transac-
tions per second at present, irrespective of the available computa-
tion capacity at hand.

in this paper, we propose a new distributed agreement proto-
col for permission-less blockchains called elastico. elastico
scales transaction rates almost linearly with available computation
for mining: the more the computation power in the network, the
higher the number of transaction blocks selected per unit time.
elastico is efﬁcient in its network messages and tolerates byzan-
tine adversaries of up to one-fourth of the total computational power.
technically, elastico uniformly partitions or parallelizes the min-
ing network (securely) into smaller committees, each of which pro-
cesses a disjoint set of transactions (or “shards”). while sharding
is common in non-byzantine settings, elastico is the ﬁrst candi-
date for a secure sharding protocol with presence of byzantine ad-
versaries. our scalability experiments on amazon ec2 with up to
1, 600 nodes conﬁrm elastico’s theoretical scaling properties.

1.

|
Non-data,|abstract
we present a software approach to mitigate access-driven
side-channel attacks that leverage last-level caches (llcs)
shared across cores to leak information between security do-
mains (e.g., tenants in a cloud). our approach dynami-
cally manages physical memory pages shared between secu-
rity domains to disable sharing of llc lines, thus prevent-
ing “flush-reload” side channels via llcs. it also man-
ages cacheability of memory pages to thwart cross-tenant
“prime-probe” attacks in llcs. we have implemented
our approach as a memory management subsystem called
cachebar within the linux kernel to intervene on such
side channels across container boundaries, as containers are
a common method for enforcing tenant isolation in platform-
as-a-service (paas) clouds. through formal veriﬁcation,
principled analysis, and empirical evaluation, we show that
cachebar achieves strong security with small performance
overheads for paas workloads.

keywords
cache-based side channel; prime-probe; ﬂush-reload

1.

|
Data,|abstract
in december 2015, juniper networks announced multiple security
vulnerabilities stemming from unauthorized code in screenos, the
operating system for their netscreen vpn routers. the more so-
phisticated of these vulnerabilities was a passive vpn decryption
capability, enabled by a change to one of the elliptic curve points
used by the dual ec pseudorandom number generator.

in this paper, we describe the results of a full independent analysis
of the screenos randomness and vpn key establishment proto-
col subsystems, which we carried out in response to this incident.
while dual ec is known to be insecure against an attacker who
can choose the elliptic curve parameters, juniper had claimed in
2013 that screenos included countermeasures against this type of
attack. we ﬁnd that, contrary to juniper’s public statements, the
screenos vpn implementation has been vulnerable since 2008 to
passive exploitation by an attacker who selects the dual ec curve
point. this vulnerability arises due to apparent ﬂaws in juniper’s
countermeasures as well as a cluster of changes that were all in-
troduced concurrently with the inclusion of dual ec in a single
2008 release. we demonstrate the vulnerability on a real netscreen
device by modifying the ﬁrmware to install our own parameters,
and we show that it is possible to passively decrypt an individual
vpn session in isolation without observing any other network trafﬁc.
we investigate the possibility of passively ﬁngerprinting screenos
implementations in the wild. this incident is an important example
of how guidelines for random number generation, engineering, and
validation can fail in practice.

1.

|
Data,|abstract
to adapt to the rapidly evolving landscape of cyber threats, secu-
rity professionals are actively exchanging indicators of compro-
mise (ioc) (e.g., malware signatures, botnet ips) through public
sources (e.g. blogs, forums, tweets, etc.). such information, of-
ten presented in articles, posts, white papers etc., can be converted
into a machine-readable openioc format for automatic analysis
and quick deployment to various security mechanisms like an in-
trusion detection system. with hundreds of thousands of sources
in the wild, the ioc data are produced at a high volume and veloc-
ity today, which becomes increasingly hard to manage by humans.
efforts to automatically gather such information from unstructured
text, however, is impeded by the limitations of today’s natural lan-
guage processing (nlp) techniques, which cannot meet the high
standard (in terms of accuracy and coverage) expected from the
iocs that could serve as direct input to a defense system.

in this paper, we present iace, an innovation solution for fully
automated ioc extraction. our approach is based on the obser-
vation that the iocs in technical articles are often described in a
predictable way: being connected to a set of context terms (e.g.,
“download”) through stable grammatical relations. leveraging this
observation, iace is designed to automatically locate a putative
ioc token (e.g., a zip ﬁle) and its context (e.g., “malware”, “down-
load”) within the sentences in a technical article, and further an-
alyze their relations through a novel application of graph mining
techniques. once the grammatical connection between the tokens
is found to be in line with the way that the ioc is commonly pre-
sented, these tokens are extracted to generate an openioc item
that describes not only the indicator (e.g., a malicious zip ﬁle) but
also its context (e.g., download from an external source). running
on 71,000 articles collected from 45 leading technical blogs, this
new approach demonstrates a remarkable performance: it gener-
ated 900k openioc items with a precision of 95% and a coverage
over 90%, which is way beyond what the state-of-the-art nlp tech-
nique and industry ioc tool can achieve, at a speed of thousands of
articles per hour. further, by correlating the iocs mined from the
articles published over a 13-year span, our study sheds new light on

1the two lead authors are ordered alphabetically.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:13) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978315

figure 1: example of openioc schema.

the links across hundreds of seemingly unrelated attack instances,
particularly their shared infrastructure resources, as well as the im-
pacts of such open-source threat intelligence on security protection
and evolution of attack strategies.
1.
|
Non-data,|abstract
differential privacy is a promising formal approach to data privacy,
which provides a quantitative bound on the privacy cost of an al-
gorithm that operates on sensitive information. several tools have
been developed for the formal veriﬁcation of differentially private
algorithms, including program logics and type systems. however,
these tools do not capture fundamental techniques that have emerged
in recent years, and cannot be used for reasoning about cutting-edge
differentially private algorithms. existing techniques fail to handle
three broad classes of algorithms: 1) algorithms where privacy de-
pends on accuracy guarantees, 2) algorithms that are analyzed with
the advanced composition theorem, which shows slower growth in
the privacy cost, 3) algorithms that interactively accept adaptive
inputs.

we address these limitations with a new formalism extending
aprhl [6], a relational program logic that has been used for proving
differential privacy of non-interactive algorithms, and incorporating
ahl [11], a (non-relational) program logic for accuracy properties.
we illustrate our approach through a single running example, which
exempliﬁes the three classes of algorithms and explores new variants
of the sparse vector technique, a well-studied algorithm from the
privacy literature. we implement our logic in easycrypt, and for-
mally verify privacy. we also introduce a novel coupling technique
called optimal subset coupling that may be of independent interest.

1.

|
Non-data,|abstract
this paper extends the choice available for secure real num-
ber implementations with two new contributions. we will
consider the numbers represented in form a − ϕb where ϕ
is the golden ratio, and in form (−1)s · 2e where e is a
ﬁxed-point number. we develop basic arithmetic operations
together with some frequently used elementary functions.
all the operations are implemented and benchmarked on
sharemind secure multi-party computation framework. it
turns out that the new proposals provide viable alternatives
to standard ﬂoating- and ﬁxed-point implementations from
the performance/error viewpoint in various settings. how-
ever, the optimal choice still depends on the exact require-
ments of the numerical algorithm to be implemented.

keywords
secure ﬁxed- and ﬂoating-point arithmetic, privacy-preserving
data analysis, secure computations

1.

|
Non-data,|abstract
motivated by the impossibility of achieving fairness in secure com-
putation [cleve, stoc 1986], recent works study a model of fair-
ness in which an adversarial party that aborts on receiving output is
forced to pay a mutually predeﬁned monetary penalty to every other
party that did not receive the output. these works show how to de-
sign protocols for secure computation with penalties that guaran-
tees that either fairness is guaranteed or that each honest party ob-
tains a monetary penalty from the adversary. protocols for this task
are typically designed in an hybrid model where parties have access
to a “claim-or-refund” transaction functionality denote f∗

in this work, we obtain improvements on the efﬁciency of these
constructions by amortizing the cost over multiple executions of
secure computation with penalties. more precisely, for computa-
tional security parameter λ, we design a protocol that implements
(cid:96) = poly(λ) instances of secure computation with penalties where
the total number of calls to f∗
keywords: secure computation, fairness, bitcoin, amortization.

cr is independent of (cid:96).

cr.

1.

|
Data,|abstract
when created, the java platform was among the ﬁrst run-
times designed with security in mind. yet, numerous java
versions were shown to contain far-reaching vulnerabilities,
permitting denial-of-service attacks or even worse allowing
intruders to bypass the runtime’s sandbox mechanisms, open-
ing the host system up to many kinds of further attacks.

this paper presents a systematic in-depth study of 87 pub-
licly available java exploits found in the wild. by collecting,
minimizing and categorizing those exploits, we identify their
commonalities and root causes, with the goal of determining
the weak spots in the java security architecture and possible
countermeasures.

our ﬁndings reveal that the exploits heavily rely on a
set of nine weaknesses, including unauthorized use of re-
stricted classes and confused deputies in combination with
caller-sensitive methods. we further show that all attack
vectors implemented by the exploits belong to one of three
categories: single-step attacks, restricted-class attacks, and
information hiding attacks.

the analysis allows us to propose ideas for improving the

security architecture to spawn further research in this area.

1.

|
Data,|abstract
user-driven access control improves the coarse-grained ac-
cess control of current operating systems (particularly in
the mobile space) that provide only all-or-nothing access to
a resource such as the camera or the current location. by
granting appropriate permissions only in response to explicit
user actions (for example, pressing a camera button), user-
driven access control better aligns application actions with
user expectations. prior work on user-driven access con-
trol has relied in essential ways on operating system (os)
modiﬁcations to provide applications with uncompromisable
access control gadgets, distinguished user interface (ui) ele-
ments that can grant access permissions.

this work presents a design, implementation, and evalu-
ation of user-driven access control that works with no os
modiﬁcations, thus making deployability and incremental
adoption of the model more feasible. we develop (1) a user-
level trusted library for access control gadgets, (2) static
analyses to prevent malicious creation of ui events,
ille-
gal ﬂows of sensitive information, and circumvention of our
library, and (3) dynamic analyses to ensure users are not
tricked into granting permissions. in addition to providing
the original user-driven access control guarantees, we use
static information ﬂow to limit where results derived from
sensitive sources may ﬂow in an application.

our implementation targets android applications. we
port open-source applications that need interesting resource
permissions to use our system. we determine in what ways
user-driven access control in general and our implementa-
tion in particular are good matches for real applications.
we demonstrate that our system is secure against a variety
of attacks that malware on android could otherwise mount.

1.

|
Non-data,|abstract
authenticated encryption (ae) schemes are symmetric-key
encryption schemes ensuring strong notions of conﬁden-
tiality and integrity. although various ae schemes are
known, there remains signiﬁcant interest in developing
schemes that are more eﬃcient, meet even stronger secu-
rity notions (e.g., misuse-resistance), or satisfy certain non-
cryptographic properties (e.g., being patent-free).

we present an automated approach for analyzing and syn-
thesizing blockcipher-based ae schemes, signiﬁcantly ex-
tending prior work by malozemoﬀ et al. (csf 2014) who syn-
thesize encryption schemes satisfying conﬁdentiality only.
our main insight is to restrict attention to a certain class of
schemes that is expressive enough to capture several known
constructions yet also admits automated reasoning about
security. we use our approach to generate thousands of
ae schemes with provable security guarantees, both known
(e.g., variants of ocb and ccm) and new. implementing
two of these new schemes, we ﬁnd their performance com-
petitive with state-of-the-art ae schemes.

1.

|
Data,|abstract
there
along with the increasing popularity of mobile devices,
exist severe security and privacy concerns for mobile apps. on
google play, user reviews provide a unique understanding of
security/privacy issues of mobile apps from users’ perspective,
and in fact they are valuable feedbacks from users by considering
users’ expectations. to best assist the end users, in this paper, we
automatically learn the security/privacy related behaviors inferred
from analysis on user reviews, which we call review-to-behavior
ﬁdelity. we design the system autoreb that automatically
assesses the review-to-behavior ﬁdelity of mobile apps. autoreb
employs the state-of-the-art machine learning techniques to
infer the relations between users’ reviews and four categories of
security-related behaviors. moreover, it uses a crowdsourcing
approach to automatically aggregate the security issues from
review-level to app-level. to our knowledge, autoreb is the ﬁrst
work that explores the user review information and utilizes the
review semantics to predict the risky behaviors at both review-level
and app-level.

we crawled a real-world dataset of 2, 614, 186 users, 12, 783
apps and 13, 129, 783 reviews from google play, and use it to
comprehensively evaluate autoreb. the experiment result shows
that our method can predict the mobile app behaviors at user-review
level with accuracy as high as 94.05%, and also it can predict
the security issues at app-level by aggregating the predictions at
review-level. our research offers an insight into understanding the
mobile app security concerns from users’ perspective, and helps
bridge the gap between the security issues and users’ perception.

categories and subject descriptors
d.4.6 [operating system]:
[artiﬁcial intelligence]: learning

security and protection;

i.2.6

general terms
security, privacy, algorithm

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.

c(cid:2) 2015 acm. isbn 978-1-4503-3832-5/15/10 ...$15.00.

doi: http://dx.doi.org/10.1145/2810103.2813689.

user reviews

auto
reb

app 

behaviors

spamming, ads
financial issue

over-claimed permission
data leakage  

….
…...

autoreb

reviewͲlevelsecurity
behaviorinference

appͲlevelsecurity
behaviorinference

figure 1:
infer the security-related behaviors from users’ reviews.
overview of the framework of autoreb : (a) engine 1: review-level
security behavior inference engine;
(b) engine 2: app-level security
behavior inference engine.

keywords
android; app; risk; information retrieval; perception; learning

1.

|
Non-data,|abstract
kernel hardening has been an important topic since many applica-
tions and security mechanisms often consider the kernel as part of
their trusted computing base (tcb). among various hardening
techniques, kernel address space layout randomization (kaslr)
is the most effective and widely adopted defense mechanism that
can practically mitigate various memory corruption vulnerabilities,
such as buffer overflow and use-after-free. in principle, kaslr
is secure as long as no memory leak vulnerability exists and high
entropy is ensured.

in this paper, we introduce a highly stable timing attack against
kaslr, called drk, that can precisely de-randomize the mem-
ory layout of the kernel without violating any such assumptions.
drk exploits a hardware feature called intel transactional synchro-
nization extension (tsx) that is readily available in most modern
commodity cpus. one surprising behavior of tsx, which is es-
sentially the root cause of this security loophole, is that it aborts a
transaction without notifying the underlying kernel even when the
transaction fails due to a critical error, such as a page fault or an
access violation, which traditionally requires kernel intervention.
drk turned this property into a precise timing channel that can
determine the mapping status (i.e., mapped versus unmapped) and
execution status (i.e., executable versus non-executable) of the priv-
ileged kernel address space. in addition to its surprising accuracy
and precision, drk is universally applicable to all oses, even in
virtualized environments, and generates no visible footprint, making
it difficult to detect in practice. we demonstrated that drk can break
the kaslr of all major oses (i.e., windows, linux, and os x)
with near-perfect accuracy in under a second. finally, we propose
potential countermeasures that can effectively prevent or mitigate
the drk attack.

we urge our community to be aware of the potential threat of
having intel tsx, which is present in most recent intel cpus—100%
in workstation and 60% in high-end intel cpus since skylake—and
is even available on amazon ec2 (x1).

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24 - 28, 2016, vienna, austria
c⃝ 2016 copyright held by the owner/author(s). publication rights licensed to acm.
isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978321

figure 1: the adoption status of both user-space and kernel-space aslr in
popular operating systems, ordered by year [62].

1.

|
Data,|abstract
typical security contests focus on breaking or mitigating the
impact of buggy systems. we present the build-it, break-it,
fix-it (bibifi) contest, which aims to assess the ability to
securely build software, not just break it. in bibifi, teams
build speciﬁed software with the goal of maximizing correct-
ness, performance, and security. the latter is tested when
teams attempt to break other teams’ submissions. win-
ners are chosen from among the best builders and the best
breakers. bibifi was designed to be open-ended—teams
can use any language, tool, process, etc. that they like. as
such, contest outcomes shed light on factors that correlate
with successfully building secure software and breaking inse-
cure software. during 2015, we ran three contests involving
a total of 116 teams and two diﬀerent programming prob-
lems. quantitative analysis from these contests found that
the most eﬃcient build-it submissions used c/c++, but
submissions coded in other statically-typed languages were
less likely to have a security ﬂaw; build-it teams with di-
verse programming-language knowledge also produced more
secure code. shorter programs correlated with better scores.
break-it teams that were also successful build-it teams were
signiﬁcantly better at ﬁnding security bugs.

1.

|
Non-data,|abstract
remote attestation is a crucial security service particularly relevant
to increasingly popular iot (and other embedded) devices. it al-
lows a trusted party (veriﬁer) to learn the state of a remote, and
potentially malware-infected, device (prover). most existing ap-
proaches are static in nature and only check whether benign soft-
ware is initially loaded on the prover. however, they are vulnerable
to runtime attacks that hijack the application’s control or data ﬂow,
e.g., via return-oriented programming or data-oriented exploits.

as a concrete step towards more comprehensive runtime remote
attestation, we present the design and implementation of control-
flow attestation (c-flat) that enables remote attestation of an
application’s control-ﬂow path, without requiring the source code.
we describe a full prototype implementation of c-flat on rasp-
berry pi using its arm trustzone hardware security extensions.
we evaluate c-flat’s performance using a real-world embedded
(cyber-physical) application, and demonstrate its efﬁcacy against
control-ﬂow hijacking attacks.

keywords
remote attestation; control-ﬂow attacks; embedded system security

|
Data,|abstract
the cached internet content served by content delivery net-
works (cdn) comprises a large fraction of today’s inter-
net traﬃc, yet, there is little study on how real-world cen-
sors deal with blocking forbidden cdn-hosted internet con-
tent. we investigate the techniques used by the great fire-
wall of china to block cdn-hosted content, and demon-
strate that blocking cdn content poses unique technical
and non-technical challenges to the censors. we therefore
design a client-side circumvention system, cachebrowser,
that leverages the censors’ diﬃculties in blocking cdn con-
tent. we implement cachebrowser and use it to unblock
cdn-hosted content in china with a download latency sig-
niﬁcantly smaller than traditional proxy-based circumven-
tion systems like tor. cachebrowser’s superior quality-of-
service is thanks to its publisher-centric approach, which
retrieves blocked content directly from content publishers
with no use of third-party proxies.

categories and subject descriptors
c.2.0 [computer-communication networks]: general—
security and protection; e.3 [data]: data encryption

general terms
algorithms, design, security

keywords
censorship resistance; unobservability; cdn; content cache

1.

|
Data,|abstract
android is the most commonly used mobile device opera-
tion system. the core of android, the system server (ss),
is a multi-threaded process that provides most of the system
services. based on a new understanding of the security risks
introduced by the callback mechanism in system services,
we have discovered a general type of design ﬂaw. a vulner-
ability detection tool has been designed and implemented
based on static taint analysis.we applied the tool on all the
80 system services in the ss of android 5.1.0. with its help,
we have discovered six previously unknown vulnerabilities,
which are further conﬁrmed on android 2.3.7-6.0.1. accord-
ing to our analysis, about 97.3% of the entire 1.4 billion real-
world android devices are vulnerable. our proof-of-concept
attack proves that the vulnerabilities can enable a malicious
app to freeze critical system functionalities or soft-reboot the
system immediately. it is a neat type of denial-of-service at-
tack. we also proved that the attacks can be conducted at
mission critical moments to achieve meaningful goals, such
as anti anti-virus, anti process-killer, hindering app updates
or system patching. after being informed, google conﬁrmed
our ﬁndings promptly. several suggestions on how to use
callbacks safely are also proposed to google.

keywords
mobile security; denial of service; vulnerability detection;
synchronous callback; taint analysis

1.

|
Data,|abstract
android is the most commonly used mobile device opera-
tion system. the core of android, the system server (ss),
is a multi-threaded process that provides most of the system
services. based on a new understanding of the security risks
introduced by the callback mechanism in system services,
we have discovered a general type of design ﬂaw. a vulner-
ability detection tool has been designed and implemented
based on static taint analysis.we applied the tool on all the
80 system services in the ss of android 5.1.0. with its help,
we have discovered six previously unknown vulnerabilities,
which are further conﬁrmed on android 2.3.7-6.0.1. accord-
ing to our analysis, about 97.3% of the entire 1.4 billion real-
world android devices are vulnerable. our proof-of-concept
attack proves that the vulnerabilities can enable a malicious
app to freeze critical system functionalities or soft-reboot the
system immediately. it is a neat type of denial-of-service at-
tack. we also proved that the attacks can be conducted at
mission critical moments to achieve meaningful goals, such
as anti anti-virus, anti process-killer, hindering app updates
or system patching. after being informed, google conﬁrmed
our ﬁndings promptly. several suggestions on how to use
callbacks safely are also proposed to google.

keywords
mobile security; denial of service; vulnerability detection;
synchronous callback; taint analysis

1.

|
Non-data,|abstract
automatically analyzing information ﬂow within android
applications that rely on cryptographic operations with their
computational security guarantees imposes formidable chal-
lenges that existing approaches for understanding an app’s
behavior struggle to meet. these approaches do not distin-
guish cryptographic and non-cryptographic operations, and
hence do not account for cryptographic protections: f (m)
is considered sensitive for a sensitive message m irrespective
of potential secrecy properties oﬀered by a cryptographic
operation f . these approaches consequently provide a safe
approximation of the app’s behavior, but they mistakenly
classify a large fraction of apps as potentially insecure and
consequently yield overly pessimistic results.

in this paper, we show how cryptographic operations can
be faithfully included into existing approaches for automated
app analysis. to this end, we ﬁrst show how cryptographic
operations can be expressed as symbolic abstractions within
the comprehensive dalvik bytecode language. these ab-
stractions are accessible to automated analysis and can be
conveniently added to existing app analysis tools using mi-
nor changes in their semantics. second, we show that our
abstractions are faithful by providing the ﬁrst computational
soundness result for dalvik bytecode, i.e., the absence of at-
tacks against our symbolically abstracted program entails
the absence of any attacks against a suitable cryptographic
program realization. we cast our computational soundness
result in the cosp framework, which makes the result mod-
ular and composable.

keywords
android, computational soundness, secure information
flow

1.

|
Non-data,|abstract
coverage-based greybox fuzzing (cgf) is a random testing
approach that requires no program analysis. a new test
is generated by slightly mutating a seed input. if the test
exercises a new and interesting path, it is added to the set of
seeds; otherwise, it is discarded. we observe that most tests
exercise the same few “high-frequency” paths and develop
strategies to explore signiﬁcantly more paths with the same
number of tests by gravitating towards low-frequency paths.
we explain the challenges and opportunities of cgf using
a markov chain model which speciﬁes the probability that
fuzzing the seed that exercises path i generates an input
that exercises path j. each state (i.e., seed) has an energy
that speciﬁes the number of inputs to be generated from that
seed. we show that cgf is considerably more e cient if en-
ergy is inversely proportional to the density of the stationary
distribution and increases monotonically every time that
seed is chosen. energy is controlled with a power schedule.
we implemented the exponential schedule by extending
afl. in 24 hours, aflfast exposes 3 previously unreported
cves that are not exposed by afl and exposes 6 previously
unreported cves 7x faster than afl. aflfast produces at
least an order of magnitude more unique crashes than afl.
ccs concepts:
•security and privacy!vulnerability scanners; •software and
its engineering!software testing and debugging;
1.

|
Non-data,|abstract
covert channels present serious security threat because they
allow secret communication between two malicious pro-
cesses even if the system inhibits direct communication.
we describe, implement and quantify a new covert channel
through shared hardware random number generation (rng)
module that is available on modern processors. we demon-
strate that a reliable, high-capacity and low-error covert
channel can be created through the rng module that works
across cpu cores and across virtual machines. we quan-
tify the capacity of the rng channel under diﬀerent set-
tings and show that transmission rates in the range of 7-200
kbit/s can be achieved depending on a particular system
used for transmission, assumptions, and the load level. fi-
nally, we describe challenges in mitigating the rng channel,
and propose several mitigation approaches both in software
and hardware.

ccs concepts
•security and privacy → side-channel analysis and
countermeasures; security in hardware;

keywords
covert channels; random number generator

1.

|
Data,|abstract
on modern operating systems, applications under the same user
are separated from each other, for the purpose of protecting them
against malware and compromised programs. given the complex-
ity of today’s oses, less clear is whether such isolation is effective
against different kind of cross-app resource access attacks (called
xara in our research). to better understand the problem, on the
less-studied apple platforms, we conducted a systematic security
analysis on mac os x and ios. our research leads to the discov-
ery of a series of high-impact security weaknesses, which enable
a sandboxed malicious app, approved by the apple stores, to gain
unauthorized access to other apps’ sensitive data. more speciﬁ-
cally, we found that the inter-app interaction services, including
the keychain, websocket and nsconnection on os x and url
scheme on os x and ios, can all be exploited by the malware
to steal such conﬁdential information as the passwords for icloud,
email and bank, and the secret token of evernote. further, the de-
sign of the app sandbox on os x was found to be vulnerable, ex-
posing an app’s private directory to the sandboxed malware that hi-
jacks its apple bundle id. as a result, sensitive user data, like the
notes and user contacts under evernote and photos under wechat,
have all been disclosed. fundamentally, these problems are caused
by the lack of app-to-app and app-to-os authentications. to bet-
ter understand their impacts, we developed a scanner that automat-
ically analyzes the binaries of os x and ios apps to determine
whether proper protection is missing in their code. running it on
hundreds of binaries, we conﬁrmed the pervasiveness of the weak-
nesses among high-impact apple apps. since the issues may not be
easily ﬁxed, we built a simple program that detects exploit attempts
on os x, helping protect vulnerable apps before the problems can
be fully addressed. we further discuss the insights from this study
and the lessons learnt for building a securer system.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.
c(cid:13) 2015 acm. isbn 978-1-4503-3832-5/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2810103.2813609.

1.

|
Data,|abstract
after a program has crashed and terminated abnormally, it typically
leaves behind a snapshot of its crashing state in the form of a core
dump. while a core dump carries a large amount of information,
which has long been used for software debugging, it barely serves
as informative debugging aids in locating software faults, particu-
larly memory corruption vulnerabilities. a memory corruption is a
special type of software fault that may lead to manipulation of the
content at a certain memory. as such, a core dump may contain a
certain amount of corrupted data, which increases the difﬁculty in
identifying useful debugging information (e.g., a crash point and
stack traces). without a proper mechanism to deal with this problem,
a core dump can be practically useless for software failure diagnosis.
in this work, we develop credal, an automatic debugging tool
that employs the source code of a crashing program to enhance
core dump analysis and turns a core dump to an informative aid
in tracking down memory corruption vulnerabilities. speciﬁcally,
credal systematically analyzes a potentially corrupted core dump
and identiﬁes the crash point and stack frames. for a core dump
carrying corrupted data, it goes beyond the crash point and stack
trace. in particular, credal further pinpoints the variables hold-
ing corrupted data using the source code of the crashing program
along with the stack frames. to assist software developers (or secu-
rity analysts) in tracking down a memory corruption vulnerability,
credal also performs analysis and highlights the code fragments
corresponding to data corruption.

to demonstrate the utility of credal, we use it to analyze 80
crashes corresponding to 73 memory corruption vulnerabilities
archived in offensive security exploit database. we show that,
credal can accurately pinpoint the crash point and (fully or par-
tially) restore a stack trace even though a crashing program stack
carries corrupted data. in addition, we demonstrate credal can
potentially reduce the manual effort of ﬁnding the code fragment
that is likely to contain memory corruption vulnerabilities.

keywords
core dump; memory corruption; vulnerability analysis

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than acm
must be honored. abstracting with credit is permitted. to copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior speciﬁc permission and/or a
fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:13) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978340

1.

|
Data,|abstract
cross-site search (xs-search) attacks circumvent the same-
origin policy and extract sensitive information, by using the
time it takes for the browser to receive responses to search
queries. this side-channel is usually considered impractical,
due to the limited attack duration and high variability of
delays. this may be true for naive xs-search attacks; how-
ever, we show that the use of better tools facilitates eﬀective
xs-search attacks, exposing information eﬃciently and pre-
cisely.

we present and evaluate three types of tools:

(1) ap-
propriate statistical tests, (2) ampliﬁcation of the timing
side-channel, by ‘inﬂating’ communication or computation,
and (3) optimized, tailored divide-and-conquer algorithms,
to identify terms from large ‘dictionaries’. these techniques
may be applicable in other scenarios.

we implemented and evaluated the attacks against the
popular gmail and bing services, in several environments
and ethical experiments, taking careful, irb-approved mea-
sures to avoid exposure of personal information.

categories and subject descriptors
j.0 [computer applications]: general

keywords
side channel attacks; web; privacy; security

1.

|
Data,|abstract
content security policy (csp)—which has been standardized by
w3c and adopted by all major commercial browsers—is one of the
most promising approaches for defending against cross-site script-
ing (xss) attacks. although client-side adoption of csp is suc-
cessful, server-side adoption is far behind the client side: according
to a large-scale survey, less than 0.002% of alexa top 1m websites
enabled csp.

to facilitate the adoption of csp, we propose cspautogen to
enable csp in real-time, without server modiﬁcations, and being
compatible with real-world websites. speciﬁcally, cspautogen
trains so-called templates for each domain, generates csps based
on the templates, rewrites incoming webpages on the ﬂy to apply
those generated csps, and then serves those rewritten webpages
to client browsers. cspautogen is designed to automatically en-
force the most secure and strict version of csp without enabling
“unsafe-inline” and “unsafe-eval”, i.e., cspautogen can handle all
the inline and dynamic scripts.

we have implemented a prototype of cspautogen, and our eval-
uation shows that cspautogen can correctly render all the alexa
top 50 websites. moreover, we conduct extensive case studies on
ﬁve popular websites, indicating that cspautogen can preserve the
behind-the-login functionalities, such as sending emails and post-
ing comments. our security analysis shows that cspautogen is
able to defend against all the tested real-world xss attacks.

1.

|
Data,|abstract
machine learning techniques based on neural networks are
achieving remarkable results in a wide variety of domains.
often, the training of models requires large, representative
datasets, which may be crowdsourced and contain sensitive
information. the models should not expose private informa-
tion in these datasets. addressing this goal, we develop new
algorithmic techniques for learning and a reﬁned analysis of
privacy costs within the framework of diﬀerential privacy.
our implementation and experiments demonstrate that we
can train deep neural networks with non-convex objectives,
under a modest privacy budget, and at a manageable cost in
software complexity, training eﬃciency, and model quality.

1.

|
Non-data,|abstract
diﬀerential privacy is a precise mathematical constraint meant
to ensure privacy of individual pieces of information in a
database even while queries are being answered about the
aggregate. intuitively, one must come to terms with what
diﬀerential privacy does and does not guarantee. for exam-
ple, the deﬁnition prevents a strong adversary who knows all
but one entry in the database from further inferring about
the last one. this strong adversary assumption can be over-
looked, resulting in misinterpretation of the privacy guaran-
tee of diﬀerential privacy.

herein we give an equivalent deﬁnition of privacy using
mutual information that makes plain some of the subtleties
of diﬀerential privacy. the mutual-information diﬀerential
privacy is in fact sandwiched between -diﬀerential privacy
and (, δ)-diﬀerential privacy in terms of its strength. in con-
trast to previous works using unconditional mutual informa-
tion, diﬀerential privacy is fundamentally related to condi-
tional mutual information, accompanied by a maximization
over the database distribution. the conceptual advantage of
using mutual information, aside from yielding a simpler and
more intuitive deﬁnition of diﬀerential privacy, is that its
properties are well understood. several properties of diﬀer-
ential privacy are easily veriﬁed for the mutual information
alternative, such as composition theorems.

keywords
diﬀerential privacy, information theory.

1.

|
Non-data,|abstract
we present privinfer, an expressive framework for writing
and verifying diﬀerentially private bayesian machine learning
algorithms. programs in privinfer are written in a rich func-
tional probabilistic programming language with constructs
for performing bayesian inference. then, diﬀerential pri-
vacy of programs is established using a relational reﬁnement
type system, in which reﬁnements on probability types are
indexed by a metric on distributions. our framework lever-
ages recent developments in bayesian inference, probabilistic
programming languages, and in relational reﬁnement types.
we demonstrate the expressiveness of privinfer by verifying
privacy for several examples of private bayesian inference.

1.

|
Data,|abstract
dynamic spectrum access (dsa) has great potential to address
worldwide spectrum shortage by enhancing spectrum efﬁciency.
it allows unlicensed secondary users to access the underutilized
licensed spectrum when the licensed primary users are not trans-
mitting. as a key enabler for dsa systems, crowdsourced spec-
trum sensing (css) allows a spectrum sensing provider (ssp) to
outsource the sensing of spectrum occupancy to distributed mobile
users. in this paper, we propose dpsense, a novel framework that
allows the ssp to select mobile users for executing spatiotempo-
ral spectrum-sensing tasks without violating the location privacy of
mobile users. detailed evaluations on real location traces conﬁr-
m that dpsense can provide differential location privacy to mobile
users while ensuring that the ssp can accomplish spectrum-sensing
tasks with overwhelming probability and also the minimal cost.

ccs concepts
•security and privacy → privacy-preserving protocols; mobile
and wireless security;

keywords
dynamic spectrum access; differential privacy; crowdsourced spec-
trum sensing; location privacy

1.

|
Non-data,|abstract
in-app embedded browsers are commonly used by app developers
to display web content without having to redirect the user to heavy-
weight web browsers. just like the conventional web browsers, em-
bedded browsers can allow the execution of web code. in addition,
they provide mechanisms (viz., javascript bridges) to give web code
access to internal app code that might implement critical function-
alities and expose device resources. this is intrinsically dangerous
since there is currently no means for app developers to perform
origin-based access control on the javascript bridges, and any web
code running in an embedded browser is free to use all the exposed
app and device resources. previous work that addresses this prob-
lem provided access control solutions that work only for apps that
are built using hybrid frameworks. additionally, these solutions fo-
cused on protecting only the parts of javascript bridges that expose
permissions-protected resources. in this work, our goal is to provide
a generic solution that works for all apps that utilize embedded web
browsers and protects all channels that give access to internal app
and device resources. towards realizing this goal, we built draco,
a uniform and ﬁne-grained access control framework for web code
running on android embedded browsers (viz., webview). draco
provides a declarative policy language that allows developers to
deﬁne policies to specify the desired access characteristics of web
origins in a ﬁne-grained fashion, and a runtime system that dynami-
cally enforces the policies. in contrast with previous work, we do
not assume any modiﬁcations to the android operating system, and
implement draco in the chromium android system webview app
to enable seamless deployment. our evaluation of the the draco
runtime system shows that draco incurs negligible overhead, which
is in the order of microseconds.

keywords
android, webview, access control, origin, javascript bridges, ex-
ploitation, javascript, html5

1.

|
Data,|abstract
searchable symmetric encryption aims at making possible
searching over an encrypted database stored on an untrusted
server while keeping privacy of both the queries and the
data, by allowing some small controlled leakage to the server.
recent work shows that dynamic schemes – in which the
data is eﬃciently updatable – leaking some information on
updated keywords are subject to devastating adaptative at-
tacks breaking the privacy of the queries. the only way
to thwart this attack is to design forward private schemes
whose update procedure does not leak if a newly inserted
element matches previous search queries.

this work proposes Σoφoς as a forward private sse scheme
with performance similar to existing less secure schemes, and
that is conceptually simpler (and also more eﬃcient) than
previous forward private constructions. in particular, it only
relies on trapdoor permutations and does not use an oram-
like construction. we also explain why Σoφoς is an optimal
point of the security/performance tradeoﬀ for sse.

finally, an implementation and evaluation results demon-

strate its practical eﬃciency.

ccs concepts
•security and privacy → privacy-preserving proto-
cols; security protocols; management and querying
of encrypted data;

keywords
searchable symmetric encryption; forward privacy; prov-
able security; implementation

1.

|
Non-data,|abstract
we describe a lightweight protocol for oblivious evaluation of
a pseudorandom function (oprf) in the presence of semi-
honest adversaries.
in an oprf protocol a receiver has
an input r; the sender gets output s and the receiver gets
output f (s, r), where f is a pseudorandom function and s
is a random seed. our protocol uses a novel adaptation of 1-
out-of-2 ot-extension protocols, and is particularly eﬃcient
when used to generate a large batch of oprf instances.
the cost to realize m oprf instances is roughly the cost
to realize 3.5m instances of standard 1-out-of-2 ots (using
state-of-the-art ot extension).

we explore in detail our protocol’s application to semi-
honest secure private set intersection (psi). the fastest state-
of-the-art psi protocol (pinkas et al., usenix 2015) is based
on eﬃcient ot extension. we observe that our oprf can
be used to remove their psi protocol’s dependence on the
bit-length of the parties’ items. we implemented both psi
protocol variants and found ours to be 3.1–3.6× faster than
pinkas et al. for psi of 128-bit strings and suﬃciently large
sets. concretely, ours requires only 3.8 seconds to securely
compute the intersection of 220-size sets, regardless of the
bit length of the items. for very large sets, our protocol is
only 4.3× slower than the insecure na¨ıve hashing approach
for psi.

1.

|
Non-data,|abstract
failing to properly isolate components in the same address
space has resulted in a substantial amount of vulnerabilities.
enforcing the least privilege principle for memory accesses
can selectively isolate software components to restrict at-
tack surface and prevent unintended cross-component mem-
ory corruption. however, the boundaries and interactions
between software components are hard to reason about and
existing approaches have failed to stop attackers from ex-
ploiting vulnerabilities caused by poor isolation.

we present the secure memory views (smv) model: a
practical and eﬃcient model for secure and selective mem-
ory isolation in monolithic multithreaded applications. smv
is a third generation privilege separation technique that of-
fers explicit access control of memory and allows concurrent
threads within the same process to partially share or fully
isolate their memory space in a controlled and parallel man-
ner following application requirements. an evaluation of our
prototype in the linux kernel (tcb < 1,800 loc) shows
negligible runtime performance overhead in real-world ap-
plications including cherokee web server (< 0.69%), apache
httpd web server (< 0.93%), and mozilla firefox web browser
(< 1.89%) with at most 12 loc changes.

1.

|
Data,|abstract
recommender systems typically require users’ history data
to provide a list of recommendations and such recommen-
dations usually reside on the cloud/server. however, the
release of such private data to the cloud has been shown to
put users at risk. it is highly desirable to provide users high-
quality personalized services while respecting their privacy.
in this paper, we develop the ﬁrst enhanced privacy-built-
in client for personalized recommendation (epicrec) sys-
tem that performs the data perturbation on the client side
to protect users’ privacy. our system needs no assumption
of trusted server and no change on the recommendation al-
gorithms on the server side; and needs minimum user inter-
action in their preferred manner, which makes our solution
ﬁt very well into real world practical use.

the design of epicrec system incorporates three main
modules: (1) usable privacy control interface that enables
two user preferred privacy controls, overall and category-
based controls, in the way they understand; (2) user privacy
level quantiﬁcation that automatically quantiﬁes user pri-
vacy concern level from these user understandable inputs;
(3) lightweight data perturbation algorithm that perturbs
user private data with provable guarantees on both diﬀeren-
tial privacy and data utility.

using large-scale real world datasets, we show that, for
both overall and category-based privacy controls, epicrec
performs best with respect to both perturbation quality and
personalized recommendation, with negligible computational
overhead. therefore, epicrec enables two contradictory
goals, privacy preservation and recommendation accuracy.
we also implement a proof-of-concept epicrec system to
demonstrate a privacy-preserving personal computer for movie
recommendation with web-based privacy controls. we be-
lieve epicrec is an important step towards designing a prac-
tical system that enables companies to monetize on user data
using high quality personalized services with strong provable
privacy protection to gain user acceptance and adoption of
their services.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:13) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978316

keywords
privacy-preserving recommendation; diﬀerential privacy;
privacy paradox

1.

|
Non-data,|abstract
contemporary vehicles are getting equipped with an increasing
number of electronic control units (ecus) and wireless connec-
tivities. although these have enhanced vehicle safety and efﬁ-
ciency, they are accompanied with new vulnerabilities. in this pa-
per, we unveil a new important vulnerability applicable to several
in-vehicle networks including control area network (can), the
de facto standard in-vehicle network protocol. speciﬁcally, we
propose a new type of denial-of-service (dos), called the bus-
off attack, which exploits the error-handling scheme of in-vehicle
networks to disconnect or shut down good/uncompromised ecus.
this is an important attack that must be thwarted, since the attack,
once an ecu is compromised, is easy to be mounted on safety-
critical ecus while its prevention is very difﬁcult. in addition to
the discovery of this new vulnerability, we analyze its feasibility
using actual in-vehicle network trafﬁc, and demonstrate the attack
on a can bus prototype as well as on two real vehicles. based on
our analysis and experimental results, we also propose and evaluate
a mechanism to detect and prevent the bus-off attack.

1.

|
Data,|abstract
sybil attacks present a signiﬁcant threat to many internet
systems and applications, in which a single adversary in-
serts multiple colluding identities in the system to compro-
mise its security and privacy. recent work has advocated
the use of social-network-based trust relationships to defend
against sybil attacks. however, most of the prior security
analyses of such systems examine only the case of social net-
works at a single instant in time. in practice, social network
connections change over time, and attackers can also cause
limited changes to the networks. in this work, we focus on
the temporal dynamics of a variety of social-network-based
sybil defenses. we describe and examine the eﬀect of novel
attacks based on: (a) the attacker’s ability to modify sybil-
controlled parts of the social-network graph, (b) his ability
to change the connections that his sybil identities main-
tain to honest users, and (c) taking advantage of the regular
dynamics of connections forming and breaking in the hon-
est part of the social network. we ﬁnd that against some
defenses meant to be fully distributed, such as sybillimit
and persea, the attacker can make dramatic gains over time
and greatly undermine the security guarantees of the sys-
tem. even against centrally controlled sybil defenses, the
attacker can eventually evade detection (e.g. against sybil-
infer and sybilrank) or create denial-of-service conditions
(e.g. against ostra and sumup). after analysis and simula-
tion of these attacks using both synthetic and real-world so-
cial network topologies, we describe possible defense strate-
gies and the trade-oﬀs that should be explored. it is clear
from our ﬁndings that temporal dynamics need to be ac-
counted for in sybil defense or else the attacker will be able
to undermine the system in unexpected and possibly dan-
gerous ways.

categories and subject descriptors
c.2.0 [computer-communication networks]: general
– security and protection; k.4.1 [computers and soci-

permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for third-party components of this work must be honored.
for all other uses, contact the owner/author(s).
copyright is held by the owner/author(s).
ccs’15, october 12–16, 2015, denver, colorado, usa.
acm 978-1-4503-3832-5/15/10.
http://dx.doi.org/10.1145/2810103.2813693.

ety]: public policy issues – abuse and crime involving
computers; k.6.5 [management of computing and in-
formation systems]: security and protection – authenti-
cation

keywords
sybil attacks; temporal dynamics

1.

|
Data,|abstract
malware detection increasingly relies on machine learning
techniques, which utilize multiple features to separate the
malware from the benign apps. the eﬀectiveness of these
techniques primarily depends on the manual feature engi-
neering process, based on human knowledge and intuition.
however, given the adversaries’ eﬀorts to evade detection
and the growing volume of publications on malware behav-
iors, the feature engineering process likely draws from a frac-
tion of the relevant knowledge.

we propose an end-to-end approach for automatic feature
engineering. we describe techniques for mining documents
written in natural language (e.g. scientiﬁc papers) and for
representing and querying the knowledge about malware in
a way that mirrors the human feature engineering process.
speciﬁcally, we ﬁrst identify abstract behaviors that are as-
sociated with malware, and then we map these behaviors to
concrete features that can be tested experimentally. we im-
plement these ideas in a system called featuresmith, which
generates a feature set for detecting android malware. we
train a classiﬁer using these features on a large data set of
benign and malicious apps. this classiﬁer achieves a 92.5%
true positive rate with only 1% false positives, which is com-
parable to the performance of a state-of-the-art android
malware detector that relies on manually engineered fea-
tures. in addition, featuresmith is able to suggest informa-
tive features that are absent from the manually engineered
set and to link the features generated to abstract concepts
that describe malware behaviors.

1.

|
Non-data,|abstract
lattice-based cryptography oﬀers some of the most attrac-
tive primitives believed to be resistant to quantum com-
puters. following increasing interest from both companies
and government agencies in building quantum computers, a
number of works have proposed instantiations of practical
post-quantum key exchange protocols based on hard prob-
lems in ideal lattices, mainly based on the ring learning
with errors (r-lwe) problem. while ideal lattices facil-
itate major eﬃciency and storage beneﬁts over their non-
ideal counterparts, the additional ring structure that en-
ables these advantages also raises concerns about the as-
sumed diﬃculty of the underlying problems. thus, a ques-
tion of signiﬁcant interest to cryptographers, and especially
to those currently placing bets on primitives that will with-
stand quantum adversaries, is how much of an advantage
the additional ring structure actually gives in practice.

despite conventional wisdom that generic lattices might
be too slow and unwieldy, we demonstrate that lwe-based
key exchange is quite practical: our constant time imple-
mentation requires around 1.3ms computation time for each
party; compared to the recent newhope r-lwe scheme,
communication sizes increase by a factor of 4.7×, but remain
under 12 kib in each direction. our protocol is competitive
when used for serving web pages over tls; when partnered
with ecdsa signatures, latencies increase by less than a fac-
tor of 1.6×, and (even under heavy load) server throughput
only decreases by factors of 1.5× and 1.2× when serving typ-
ical 1 kib and 100 kib pages, respectively. to achieve these
practical results, our protocol takes advantage of several in-
∗
large parts of this work were done when valeria nikolaenko was
an intern at google.

permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for third-party components of this work must be honored.
request permissions from permissions@acm.org.
ccs’16 october 24–28, 2016, vienna, austria
copyright is held by the owner/author(s). publication rights licensed to acm.
acm isbn 978-1-4503-4139-4/16/10...15.00
doi: http://dx.doi.org/10.1145/2976749.2978425

novations. these include techniques to optimize communi-
cation bandwidth, dynamic generation of public parameters
(which also oﬀers additional security against backdoors),
carefully chosen error distributions, and tight security pa-
rameters.

keywords
post-quantum cryptography; learning with errors; key ex-
change; openssl; tls

1.

|
Non-data,|abstract
this work describes the design, implementation, and eval-
uation of Λ◦ λ, a general-purpose software framework for
lattice-based cryptography. the Λ◦λ framework has several
novel properties that distinguish it from prior implementa-
tions of lattice cryptosystems, including the following.
generality, modularity, concision: Λ◦λ deﬁnes a collection
of general, highly composable interfaces for mathematical
operations used across lattice cryptography, allowing for a
wide variety of schemes to be expressed very naturally and
at a high level of abstraction. for example, we implement
an advanced fully homomorphic encryption (fhe) scheme
in as few as 2–5 lines of code per feature, via code that very
closely matches the scheme’s mathematical deﬁnition.
theory aﬃnity: Λ◦ λ is designed from the ground-up
around the specialized ring representations, fast algorithms,
and worst-case hardness proofs that have been developed for
the ring-lwe problem and its cryptographic applications.
in particular, it implements fast algorithms for sampling
from theory-recommended error distributions over arbitrary
cyclotomic rings, and provides tools for maintaining tight
control of error growth in cryptographic schemes.
safety: Λ◦λ has several facilities for reducing code com-
plexity and programming errors, thereby aiding the correct
implementation of lattice cryptosystems. in particular, it
uses strong typing to statically enforce—i.e., at compile time—
a wide variety of constraints among the various parameters.
advanced features: Λ◦ λ exposes the rich hierarchy of
cyclotomic rings to cryptographic applications. we use this
to give the ﬁrst-ever implementation of a collection of fhe
operations known as “ring switching,” and also deﬁne and
analyze a more eﬃcient variant that we call “ring tunneling.”
lastly, this work deﬁnes and analyzes a variety of mathe-
matical objects and algorithms for the recommended usage
of ring-lwe in cyclotomic rings, which we believe will serve
as a useful knowledge base for future implementations.

∗supported by nsf career award ccf-1054495, darpa

fa8750-11-c-0096, the sloan foundation, and google.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24 - 28, 2016, vienna, austria
c(cid:13) 2016 copyright held by the owner/author(s). publication rights licensed to acm.
isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978402

1.

|
Non-data,|abstract
we present simple, practical, and powerful new techniques
for garbled circuits. these techniques result in signiﬁcant
concrete and asymptotic improvements over the state of the
art, for several natural kinds of computations.

for arithmetic circuits over the integers, our construction
results in garbled circuits with free addition, weighted
threshold gates with cost independent of fan-in, and ex-
ponentiation by a ﬁxed exponent with cost independent of
the exponent. for boolean circuits, our construction gives
an exponential improvement over the state of the art for
threshold gates (including and/or gates) of high fan-in.

our construction can be eﬃciently instantiated with prac-
tical symmetric-key primitives (e.g., aes), and is proven
secure under similar assumptions to that of the free-xor
garbling scheme (kolesnikov & schneider, icalp 2008). we
give an extensive comparison between our scheme and state-
of-the-art garbling schemes applied to boolean circuits.

1.

|
Non-data,|abstract
authenticated encryption schemes guarantee both privacy
and integrity, and have become the default level of encryp-
tion in modern protocols. one of the most popular au-
thenticated encryption schemes today is aes-gcm due to
its impressive speed. the current caesar competition is
considering new modes for authenticated encryption that
will improve on existing methods. one property of impor-
tance that is being considered more today – due to multiple
real-life cases of faulty sources of randomness – is that re-
peating nonces and ivs can have disastrous eﬀects on secu-
rity. a (full) nonce misuse-resistant authenticated encryp-
tion scheme has the property that if the same nonce is used
to encrypt the same message twice, then the same cipher-
text is obtained and so the fact that the same message was
encrypted is detected. otherwise, full security is obtained –
even if the same nonce is used for diﬀerent messages.

in this paper, we present a new fully nonce misuse-resistant
authenticated encryption scheme that is based on carefully
combining the gcm building blocks into the siv paradigm
of rogaway and shrimpton. we provide a full proof of secu-
rity of our scheme, and an optimized implementation using
the aes-ni and pclmulqdq instruction sets. we com-
pare our performance to the highly optimized openssl 1.0.2
implementation of gcm and show that our nonce misuse-
resistant scheme is only 14% slower on haswell architecture
and 19% slower on broadwell architecture. on broadwell,
gcm-siv encryption takes only 0.92 cycles per byte, and
gcm-siv decryption is exactly the same as gcm decryp-
tion taking only 0.77 cycles per byte. in addition, we com-
pare to other optimized authenticated-encryption implemen-
tations carried out by bogdanov et al., and conclude that our
mode is very competitive. beyond being very fast, our new
∗
supported by the pqcrypto project, which was partially funded
by the european commission horizon 2020 research programme,
grant #645622.
†
supported by the biu center for research in applied cryptography
and cyber security in conjunction with the israel national cyber
bureau in the prime minster’s oﬃce.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.
c(cid:13) 2015 acm. isbn 978-1-4503-3832-5/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2810103.2813613.

mode of operation uses the same building blocks as gcm
and so existing hardware and software can be utilized to eas-
ily deploy gcm-siv. we conclude that gcm-siv is a viable
alternative to gcm, providing full nonce misuse-resistance
at little cost.

1.

|
Non-data,|abstract
an android app’s graphical user interface (gui) displays
rich semantic and contextual information about the smart-
phone’s owner and app’s execution. such information pro-
vides vital clues to the investigation of crimes in both cyber
and physical spaces. in real-world digital forensics however,
once an electronic device becomes evidence most manual in-
teractions with it are prohibited by criminal investigation
protocols. hence investigators must resort to “image-and-
analyze” memory forensics (instead of browsing through the
subject phone) to recover the apps’ guis. unfortunately,
gui reconstruction is still largely impossible with state-
of-the-art memory forensics techniques, which tend to fo-
cus only on individual in-memory data structures. an an-
droid gui, however, displays diverse visual elements each
built from numerous data structure instances. furthermore,
whenever an app is sent to the background, its gui struc-
ture will be explicitly deallocated and disintegrated by the
android framework. in this paper, we present guitar, an
app-independent technique which automatically reassembles
and redraws all apps’ guis from the multitude of gui data
elements found in a smartphone’s memory image. to do so,
guitar involves the reconstruction of (1) gui tree topol-
ogy, (2) drawing operation mapping, and (3) runtime envi-
ronment for redrawing. our evaluation shows that guitar
is highly accurate (80-95% similar to original screenshots) at
reconstructing guis from memory images taken from a vari-
ety of android apps on popular phones. moreover, guitar
is robust in reconstructing meaningful guis even when fac-
ing gui data loss.
categories and subject descriptors
d.4.6 [operating systems]: security and protection
general terms
security
keywords
memory forensics; android; digital forensics

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.
copyright is held by the owner/author(s). publication rights licensed to acm.
c(cid:13) 2015 acm. isbn 978-1-4503-3832-5/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2810103.2813650.

1.

|
Data,|abstract
in local diﬀerential privacy (ldp), each user perturbs her
data locally before sending the noisy data to a data collector.
the latter then analyzes the data to obtain useful statistics.
unlike the setting of centralized diﬀerential privacy, in ldp
the data collector never gains access to the exact values of
sensitive data, which protects not only the privacy of data
contributors but also the collector itself against the risk of
potential data leakage. existing ldp solutions in the liter-
ature are mostly limited to the case that each user possesses
a tuple of numeric or categorical values, and the data collec-
tor computes basic statistics such as counts or mean values.
to the best of our knowledge, no existing work tackles more
complex data mining tasks such as heavy hitter discovery
over set-valued data.

in this paper, we present a systematic study of heavy hit-
ter mining under ldp. we ﬁrst review existing solutions,
extend them to the heavy hitter estimation, and explain why
their eﬀectiveness is limited. we then propose ldpminer,
a two-phase mechanism for obtaining accurate heavy hitters
with ldp. the main idea is to ﬁrst gather a candidate set
of heavy hitters using a portion of the privacy budget, and
focus the remaining budget on reﬁning the candidate set in a
second phase, which is much more eﬃcient budget-wise than
obtaining the heavy hitters directly from the whole dataset.
we provide both in-depth theoretical analysis and extensive
experiments to compare ldpminer against adaptations of
previous solutions. the results show that ldpminer signif-
icantly improves over existing methods. more importantly,

∗

this work was conducted while the ﬁrst author was doing

internship at qatar computing research institute.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:2) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978409

ldpminer successfully identiﬁes the majority true heavy
hitters in practical settings.

keywords
local diﬀerential privacy; heavy hitter

1.

|
Data,|abstract
intrusive multi-step attacks, such as advanced persistent
threat (apt) attacks, have plagued enterprises with signif-
icant ﬁnancial losses and are the top reason for enterprises
to increase their security budgets. since these attacks are
sophisticated and stealthy, they can remain undetected for
years if individual steps are buried in background “noise.”
thus, enterprises are seeking solutions to “connect the sus-
picious dots” across multiple activities. this requires ubiq-
uitous system auditing for long periods of time, which in
turn causes overwhelmingly large amount of system audit
events. given a limited system budget, how to eﬃciently
handle ever-increasing system audit logs is a great challenge.
this paper proposes a new approach that exploits the de-
pendency among system events to reduce the number of log
entries while still supporting high-quality forensic analysis.
in particular, we ﬁrst propose an aggregation algorithm that
preserves the dependency of events during data reduction to
ensure the high quality of forensic analysis. then we pro-
pose an aggressive reduction algorithm and exploit domain
knowledge for further data reduction. to validate the eﬃ-
cacy of our proposed approach, we conduct a comprehensive
evaluation on real-world auditing systems using log traces of
more than one month. our evaluation results demonstrate
that our approach can signiﬁcantly reduce the size of system
logs and improve the eﬃciency of forensic analysis without
losing accuracy.

1work done during an internship in nec labs america, inc.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:13) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978378

1.

|
Non-data,|abstract
in this paper, we describe a new information-theoretic proto-
col (and a computationally-secure variant) for secure three-
party computation with an honest majority. the proto-
col has very minimal computation and communication; for
boolean circuits, each party sends only a single bit for every
and gate (and nothing is sent for xor gates). our protocol
is (simulation-based) secure in the presence of semi-honest
adversaries, and achieves privacy in the client/server model
in the presence of malicious adversaries.

on a cluster of three 20-core servers with a 10gbps con-
nection, the implementation of our protocol carries out over
1.3 million aes computations per second, which involves
processing over 7 billion gates per second. in addition, we
developed a kerberos extension that replaces the ticket-
granting-ticket encryption on the key distribution center
(kdc) in mit-kerberos with our protocol, using keys/ pass-
words that are shared between the servers. this enables the
use of kerberos while protecting passwords. our implemen-
tation is able to support a login storm of over 35,000 logins
per second, which suﬃces even for very large organizations.
our work demonstrates that high-throughput secure com-
putation is possible on standard hardware.

|
Data,|abstract
few users have a single, authoritative, source from whom
they can request digital-security advice. rather, digital-
security skills are often learned haphazardly, as users ﬁl-
ter through an overwhelming quantity of security advice.
by understanding the factors that contribute to users’ ad-
vice sources, beliefs, and security behaviors, we can help
to pare down the quantity and improve the quality of ad-
vice provided to users, streamlining the process of learn-
ing key behaviors. this paper rigorously investigates how
users’ security beliefs, knowledge, and demographics corre-
late with their sources of security advice, and how all these
factors inﬂuence security behaviors. using a carefully pre-
tested, u.s.-census-representative survey of 526 users, we
present an overview of the prevalence of respondents’ ad-
vice sources, reasons for accepting and rejecting advice from
those sources, and the impact of these sources and demo-
graphic factors on security behavior. we ﬁnd evidence of a
“digital divide” in security: the advice sources of users with
higher skill levels and socioeconomic status diﬀer from those
with fewer resources. this digital security divide may add to
the vulnerability of already disadvantaged users. addition-
ally, we conﬁrm and extend results from prior small-sample
studies about why users accept certain digital-security ad-
vice (e.g., because they trust the source rather than the con-
tent) and reject other advice (e.g., because it is inconvenient
and because it contains too much marketing material). we
conclude with recommendations for combating the digital
divide and improving the eﬃcacy of digital-security advice.

1.

|
Non-data,|abstract
back and bentov (arxiv 2014) and andrychowicz et al. (security
and privacy 2014) introduced techniques to perform secure multi-
party computations on bitcoin. among other things, these works
constructed lottery protocols that ensure that any party that aborts
after learning the outcome pays a monetary penalty to all other par-
ties. following this, andrychowicz et al. (bitcoin workshop 2014)
and concurrently bentov and kumaresan (crypto 2014) extended
the solution to arbitrary secure function evaluation while guaran-
teeing fairness in the following sense: any party that aborts after
learning the output pays a monetary penalty to all parties that did
not learn the output. andrychowicz et al. (bitcoin workshop 2014)
also suggested extending to scenarios where parties receive a pay-
off according to the output of a secure function evaluation, and out-
lined a 2-party protocol for the same that in addition satisﬁes the
notion of fairness described above.

in this work, we formalize, generalize, and construct multiparty
protocols for the primitive suggested by andrychowicz et al. we
call this primitive secure cash distribution with penalties. our for-
mulation of secure cash distribution with penalties poses it as a
multistage reactive functionality (i.e., more general than secure func-
tion evaluation) that provides a way to securely implement smart
contracts in a decentralized setting, and consequently sufﬁces to
capture a wide variety of stateful computations involving data and/or
money, such as decentralized auctions, markets, and games such
as poker, etc. our protocol realizing secure cash distribution with
penalties works in a hybrid model where parties have access to a
claim-or-refund transaction functionality f (cid:63)
cr which can be efﬁ-
ciently realized in (a variant of) bitcoin, and is otherwise indepen-
dent of the bitcoin ecosystem. we emphasize that our protocol is
dropout-tolerant in the sense that any party that drops out during
the protocol is forced to pay a monetary penalty to all other parties.
our formalization and construction generalize both secure compu-
tation with penalties of bentov and kumaresan (crypto 2014), and
secure lottery with penalties of andrychowicz et al. (security and
privacy 2014).

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12-16, 2015, denver, co, usa.
copyright 2015 acm 978-1-4503-3832-5/15/10 ...$15.00.
http://dx.doi.org/10.1145/2810103.2813712.

categories and subject descriptors
c.2.0 [computer-communication networks]: general—secu-
rity and protection

keywords
secure computation; bitcoin; smart contracts; markets; poker.

1.

|
Non-data,|abstract
attackers can get physical control of a computer in sleep
(s3/suspend-to-ram), if it is lost, stolen, or the owner is
being coerced. high-value memory-resident secrets, includ-
ing disk encryption keys, and private signature/encryption
keys for pgp, may be extracted (e.g., via cold-boot or dma
attacks), by physically accessing such a computer. our goal
is to alleviate threats of extracting secrets from a computer
in sleep, without relying on an internet-facing service.

we propose hypnoguard to protect all memory-resident
os/user data across s3 suspensions, by ﬁrst performing an
in-place full memory encryption before entering sleep, and
then restoring the plaintext content at wakeup-time through
an environment-bound, password-based authentication pro-
cess. the memory encryption key is eﬀectively “sealed” in a
trusted platform module (tpm) chip with the measure-
ment of the execution environment supported by cpu’s
trusted execution mode (e.g., intel txt, amd-v/svm).
password guessing within hypnoguard may cause the mem-
ory content to be permanently inaccessible, while guessing
without hypnoguard is equivalent to brute-forcing a high-
entropy key (due to tpm protection). we achieved full
memory encryption/decryption in less than a second on a
mainstream computer (intel i7-4771 cpu with 8gb ram,
taking advantage of multi-core processing and aes-ni), an
apparently acceptable delay for sleep-wake transitions. to
the best of our knowledge, hypnoguard provides the ﬁrst
wakeup-time secure environment for authentication and key
unlocking, without requiring per-application changes.
1.

|
Non-data,|abstract
mobile device losses and thefts are skyrocketing. the sen-
sitive data hosted on a lost/stolen device are fully exposed
to the adversary. although password-based authentication
mechanisms are available on mobile devices, many users re-
portedly do not use them, and a device may be lost/stolen
while in the unlocked mode. this paper presents the de-
sign and evaluation of ilock, a secure and usable defense
against data theft on a lost/stolen mobile device.
ilock
automatically, quickly, and accurately recognizes the user’s
physical separation from his/her device by detecting and
analyzing the changes in wireless signals. once signiﬁcant
physical separation is detected, the device is immediately
locked to prevent data theft.
ilock relies on acoustic sig-
nals and requires at least one speaker and one microphone
that are available on most cots (commodity-oﬀ-the-shelf)
mobile devices. extensive experiments on samsung galaxy
s5 show that ilock can lock the device with negligible false
positives and negatives.

ccs concepts
•human-centered computing → mobile devices; •security
and privacy → mobile and wireless security;

keywords
device locking, fmcw, audio ranging, smartphone security

1.

|
Non-data,|abstract
motivated by the impossibility of achieving fairness in secure com-
putation [cleve, stoc 1986], recent works study a model of fair-
ness in which an adversarial party that aborts on receiving output
is forced to pay a mutually predeﬁned monetary penalty to every
other party that did not receive the output. these works show how
to design protocols for secure computation with penalties that tol-
erate an arbitrary number of corruptions.

in this work, we improve the efﬁciency of protocols for secure
computation with penalties in a hybrid model where parties have
access to the “claim-or-refund” transaction functionality. our ﬁrst
improvement is for the ladder protocol of bentov and kumaresan
(crypto 2014) where we improve the dependence of the script com-
plexity of the protocol (which corresponds to miner veriﬁcation
load and also space on the blockchain) on the number of parties
from quadratic to linear (and in particular, is completely indepen-
dent of the underlying function). our second improvement is for
the see-saw protocol of kumaresan et al. (ccs 2015) where we re-
duce the total number of claim-or-refund transactions and also the
script complexity from quadratic to linear in the number of parties.
we also present a ‘dual-mode’ protocol that offers different guar-
antees depending on the number of corrupt parties: (1) when s <
n/2 parties are corrupt, this protocol guarantees fairness (i.e., ei-
ther all parties get the output or none do), and (2) when t > n/2
parties are corrupt, this protocol guarantees fairness with penalties
(i.e., if the adversary gets the output, then either the honest parties
get output as well or they get compensation via penalizing the ad-
versary). the above protocol works as long as t + s < n, matching
the bound obtained for secure computation protocols in the stan-
dard model (i.e., replacing “fairness with penalties” with “security-
with-abort” (full security except fairness)) by ishai et al. (sicomp
2011).
keywords: bitcoin, secure computation, fairness.

1.

|
Non-data,|abstract
device-to-device communication is important to emerging
mobile applications such as internet of things and mobile
social networks. authentication and key agreement among
multiple legitimate devices is the important ﬁrst step to
build a secure communication channel. existing solutions
put the devices into physical proximity and use the common
radio environment as a proof of identities and the common
secret to agree on a same key. however they experience very
slow secret bit generation rate and high errors, requiring sev-
eral minutes to build a 256-bit key. in this work, we design
and implement an authentication and key agreement proto-
col for mobile devices, called the dancing signals (tds),
being extremely fast and error-free. tds uses channel state
information (csi) as the common secret among legitimate
devices. it guarantees that only devices in a close physical
proximity can agree on a key and any device outside a cer-
tain distance gets nothing about the key. compared with
existing solutions, tds is very fast and robust, support-
s group key agreement, and can eﬀectively defend against
predictable channel attacks. we implement tds using com-
modity oﬀ-the-shelf 802.11n devices and evaluate its perfor-
mance via extensive experiments. results show that tds
only takes a couple of seconds to make devices agree on a
256-bit secret key with high entropy.

keywords
group authentication; key agreement; wifi; csi

1.

|
Data,|abstract
with the booming sale of ios devices, the number of ios
applications has increased signiﬁcantly in recent years. to
protect the security of ios users, apple requires every ios
application to go through a vetting process called app re-
view to detect uses of private apis that provide access to
sensitive user information. however, recent attacks have
shown the feasibility of using private apis without being
detected during app review.

to counter such attacks, we propose a new ios applica-
tion vetting system, called iris, in this paper.
iris ﬁrst
applies fast static analysis to resolve api calls. for those
that cannot be statically resolved, iris uses a novel iterative
dynamic analysis approach, which is slower but more power-
ful compared to static analysis. we have ported valgrind to
ios and implemented a prototype of iris on top of it. we
evaluated iris with 2019 applications from the oﬃcial app
store. from these, iris identiﬁed 146 (7%) applications
that use a total number of 150 diﬀerent private apis, in-
cluding 25 security-critical apis that access sensitive user
information, such as device serial number. by analyzing
ios applications using iris, we also identiﬁed a suspicious
advertisement service provider which collects user privacy
information in its advertisement serving library. our results
show that, contrary to popular belief, a nontrivial number of
ios applications that violate apple’s terms of service exist
in the app store. iris is eﬀective in detecting private api
abuse missed by app review.

categories and subject descriptors
d.2.4 [software engineering]: software/program veriﬁ-
cation; d.4.6 [operating systems]: security and protec-
tion

general terms
security

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.
copyright is held by the owner/author(s). publication rights licensed to acm.
acm 978-1-4503-3832-5/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2810103.2813675 .

keywords
ios; application vetting; private api; forced execution;
binary instrumentation; static analysis; dynamic analysis

1.

|
Data,|abstract
from pencils to commercial aircraft, every man-made object must
be designed and manufactured. when it is cheaper or easier to steal a
design or a manufacturing process speciﬁcation than to invent one’s
own, the incentive for theft is present. as more and more manufac-
turing data comes online, incidents of such theft are increasing.

in this paper, we present a side-channel attack on manufacturing
equipment that reveals both the form of a product and its manufac-
turing process, i.e., exactly how it is made. in the attack, a human
deliberately or accidentally places an attack-enabled phone close
to the equipment or makes or receives a phone call on any phone
nearby. the phone executing the attack records audio and, optional-
ly, magnetometer data. we present a method of reconstructing the
product’s form and manufacturing process from the captured data,
based on machine learning, signal processing, and human assistance.
we demonstrate the attack on a 3d printer and a cnc mill, each
with its own acoustic signature, and discuss the commonalities in the
sensor data captured for these two different machines. we compare
the quality of the data captured with a variety of smartphone models.
capturing data from the 3d printer, we reproduce the form and
process information of objects previously unknown to the recon-
structors. on average, our accuracy is within 1 mm in reconstructing
the length of a line segment in a fabricated object’s shape and within
1 degree in determining an angle in a fabricated object’s shape.

we conclude with recommendations for defending against these

attacks.

1.

|
Non-data,|abstract
we show that equivocation, i.e., making conﬂicting state-
ments to others in a distributed protocol, can be monetar-
ily disincentivized by the use of crypto-currencies such as
bitcoin. to this end, we design completely decentralized
non-equivocation contracts, which make it possible to pe-
nalize an equivocating party by the loss of its money. at
the core of these contracts, there is a novel cryptographic
primitive called accountable assertions, which reveals the
party’s bitcoin credentials if it equivocates.
non-equivocation contracts are particularly useful for dis-
tributed systems that employ public append-only logs to
protect data integrity, e.g., in cloud storage and social net-
works. moreover, as double-spending in bitcoin is a special
case of equivocation, the contracts enable us to design a
payment protocol that allows a payee to receive funds at
several unsynchronized points of sale, while being able to
penalize a double-spending payer after the fact.

categories and subject descriptors
c2.4 [computer-communication networks]: distributed
systems; k4.4 [computers and society]: electronic com-
merce—cybercash, digital cash, payment schemes, security

keywords
crypto-currencies; bitcoin; equivocation; append-only logs;
accountability; double-spending; payment channels

1.

|
Data,|abstract
while attacks on information systems have for most prac-
tical purposes binary outcomes (information was manipu-
lated/eavesdropped, or not), attacks manipulating the sen-
sor or control signals of industrial control systems (ics) can
be tuned by the attacker to cause a continuous spectrum in
damages. attackers that want to remain undetected can at-
tempt to hide their manipulation of the system by following
closely the expected behavior of the system, while injecting
just enough false information at each time step to achieve
their goals.

in this work, we study if physics-based attack detection
can limit the impact of such stealthy attacks. we start with
a comprehensive review of related work on attack detection
schemes in the security and control systems community. we
then show that many of these works use detection schemes
that are not limiting the impact of stealthy attacks. we pro-
pose a new metric to measure the impact of stealthy attacks
and how they relate to our selection on an upper bound on
false alarms. we ﬁnally show that the impact of such attacks
can be mitigated in several cases by the proper combination
and conﬁguration of detection schemes. we demonstrate
the e↵ectiveness of our algorithms through simulations and
experiments using real ics testbeds and real ics systems.

keywords
industrial control systems; intrusion detection; security
metrics; stealthy attacks; physics-based detection; cyber-
physical systems

1.

|
Data,|abstract
cryptocurrencies record transactions in a decentralized data
structure called a blockchain. two of the most popular
cryptocurrencies, bitcoin and ethereum, support the fea-
ture to encode rules or scripts for processing transactions.
this feature has evolved to give practical shape to the ideas
of smart contracts, or full-ﬂedged programs that are run on
blockchains. recently, ethereum’s smart contract system
has seen steady adoption, supporting tens of thousands of
contracts, holding millions dollars worth of virtual coins.

in this paper, we investigate the security of running smart
contracts based on ethereum in an open distributed network
like those of cryptocurrencies. we introduce several new se-
curity problems in which an adversary can manipulate smart
contract execution to gain proﬁt. these bugs suggest subtle
gaps in the understanding of the distributed semantics of the
underlying platform. as a reﬁnement, we propose ways to
enhance the operational semantics of ethereum to make con-
tracts less vulnerable. for developers writing contracts for
the existing ethereum system, we build a symbolic execution
tool called oyente to ﬁnd potential security bugs. among
19, 366 existing ethereum contracts, oyente ﬂags 8, 833 of
them as vulnerable, including the thedao bug which led
to a 60 million us dollar loss in june 2016. we also discuss
the severity of other attacks for several case studies which
have source code available and conﬁrm the attacks (which
target only our accounts) in the main ethereum network.

1.

|
Data,|abstract
the increase of distributed denial-of-service (ddos) at-
tacks in volume, frequency, and complexity, combined with
the constant required alertness for mitigating web applica-
tion threats, has caused many website owners to turn to
cloud-based security providers (cbsps) to protect their in-
frastructure. these solutions typically involve the rerouting
of tra c from the original website through the cbsp’s net-
work, where malicious tra c can be detected and absorbed
before it ever reaches the servers of the protected website.
the most popular cloud-based security providers do not re-
quire the purchase of dedicated tra c-rerouting hardware,
but rely solely on changing the dns settings of a domain
name to reroute a website’s tra c through their security in-
frastructure. consequently, this rerouting mechanism can
be completely circumvented by directly attacking the web-
site’s hosting ip address. therefore, it is crucial for the
security and availability of these websites that their real ip
address remains hidden from potential attackers.

in this paper, we discuss existing, as well as novel “origin-
exposing” attack vectors which attackers can leverage to dis-
cover the ip address of the server where a website protected
by a cbsp is hosted. to assess the impact of the discussed
origin-exposing vectors on the security of cbsp-protected
websites, we consolidate all vectors into cloudpiercer, an
automated origin-exposing tool, which we then use to con-
duct the ﬁrst large-scale analysis of the e↵ectiveness of the
origin-exposing vectors. our results show that the problem
is severe: 71.5% of the 17,877 cbsp-protected websites that
we tested, expose their real ip address through at least one
of the evaluated vectors. the results of our study categori-
cally demonstrate that a comprehensive adoption of cbsps
is harder than just changing dns records. our ﬁndings
can steer cbsps and site administrators towards e↵ective
countermeasures, such as proactively scanning for origin ex-
posure and using appropriate network conﬁgurations that
can greatly reduce the threat.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.
c  2015 acm. isbn 978-1-4503-3832-5/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2810103.2813633.

categories and subject descriptors
c.2.0 [computer-communication networks]: [security
and protection]; k.6.5 [security and protection]: [unau-
thorized access]

keywords
cloud-based security; ddos attacks; web attacks

1.

|
Non-data,|abstract
we consider the task of secure multi-party computation of
arithmetic circuits over a ﬁnite ﬁeld. unlike boolean cir-
cuits, arithmetic circuits allow natural computations on in-
tegers to be expressed easily and eﬃciently. in the strongest
setting of malicious security with a dishonest majority —
where any number of parties may deviate arbitrarily from
the protocol — most existing protocols require expensive
public-key cryptography for each multiplication in the pre-
processing stage of the protocol, which leads to a high total
cost.

we present a new protocol that overcomes this limita-
tion by using oblivious transfer to perform secure multipli-
cations in general ﬁnite ﬁelds with reduced communication
and computation. our protocol is based on an arithmetic
view of oblivious transfer, with careful consistency checks
and other techniques to obtain malicious security at a cost of
less than 6 times that of semi-honest security. we describe a
highly optimized implementation together with experimen-
tal results for up to ﬁve parties. by making extensive use of
parallelism and sse instructions, we improve upon previous
runtimes for mpc over arithmetic circuits by more than 200
times.

keywords
multi-party computation; oblivious transfer

1.

|
Data,|abstract
the continuous decrease in cost of molecular proﬁling tests
is revolutionizing medical research and practice, but it also
raises new privacy concerns. one of the ﬁrst attacks against
privacy of biological data, proposed by homer et al. in 2008,
showed that, by knowing parts of the genome of a given in-
dividual and summary statistics of a genome-based study,
it is possible to detect if this individual participated in the
study. since then, a lot of work has been carried out to fur-
ther study the theoretical limits and to counter the genome-
based membership inference attack. however, genomic data
are by no means the only or the most inﬂuential biological
data threatening personal privacy. for instance, whereas
the genome informs us about the risk of developing some
diseases in the future, epigenetic biomarkers, such as mi-
crornas, are directly and deterministically aﬀected by our
health condition including most common severe diseases.

in this paper, we show that the membership inference at-
tack also threatens the privacy of individuals contributing
their microrna expressions to scientiﬁc studies. our results
on real and public microrna expression data demonstrate
that disease-speciﬁc datasets are especially prone to mem-
bership detection, oﬀering a true-positive rate of up to 77%
at a false-negative rate of less than 1%. we present two at-
tacks: one relying on the l1 distance and the other based on
the likelihood-ratio test. we show that the likelihood-ratio
test provides the highest adversarial success and we derive
a theoretical limit on this success. in order to mitigate the
membership inference, we propose and evaluate both a dif-
ferentially private mechanism and a hiding mechanism. we
also consider two types of adversarial prior knowledge for
the diﬀerentially private mechanism and show that, for rel-
atively large datasets, this mechanism can protect the pri-
vacy of participants in mirna-based studies against strong
adversaries without degrading the data utility too much.
based on our ﬁndings and given the current number of mir-
nas, we recommend to only release summary statistics of
datasets containing at least a couple of hundred individuals.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24 - 28, 2016, vienna, austria
c(cid:13) 2016 copyright held by the owner/author(s). publication rights licensed to acm.
isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978355

keywords
health privacy; membership privacy; diﬀerential privacy

1.

|
Non-data,|abstract
a key requirement for most security solutions is to provide
secure cryptographic key storage in a way that will easily
scale in the age of the internet of things. in this paper, we
focus on providing such a solution based on physical unclon-
able functions (pufs). to this end, we focus on microelec-
tromechanical systems (mems)-based gyroscopes and show
via wafer-level measurements and simulations, that it is fea-
sible to use the physical and electrical properties of these
sensors for cryptographic key generation. after identifying
the most promising features, we propose a novel quantiza-
tion scheme to extract bit strings from the mems analog
measurements. we provide upper and lower bounds for the
minimum entropy of the derived bit strings and fully analyze
the intra- and inter-class distributions across the operation
range of the mems device. we complement these mea-
surements via monte-carlo simulations based on the distri-
butions of the parameters measured on actual devices. we
also propose and evaluate a complete cryptographic key gen-
eration chain based on fuzzy extractors. we derive a full en-
tropy 128-bit key using the obtained min-entropy estimates,
requiring 1219 bits of helper data with an (authentication)
failure probability of 4 · 10−7.
in addition, we propose a
dedicated mems-puf design, which is superior to our mea-
sured sensor, in terms of chip area, quality and quantity of
key seed features.

keywords
hardware security; iot security; mobile security and privacy

1.

|
Non-data,|abstract
hardware trojan detection has emerged as a critical chal-
lenge to ensure security and trustworthiness of integrated
circuits. a vast majority of research eﬀorts in this area has
utilized side-channel analysis for trojan detection. func-
tional test generation for logic testing is a promising al-
ternative but it may not be helpful if a trojan cannot be
fully activated or the trojan eﬀect cannot be propagated
to the observable outputs. side-channel analysis, on the
other hand, can achieve signiﬁcantly higher detection cover-
age for trojans of all types/sizes, since it does not require
activation/propagation of an unknown trojan. however,
they have often limited eﬀectiveness due to poor detection
sensitivity under large process variations and small trojan
footprint in side-channel signature. in this paper, we address
this critical problem through a novel side-channel-aware test
generation approach, based on a concept of multiple exci-
tation of rare switching (mers), that can signiﬁcantly in-
crease trojan detection sensitivity. the paper makes several
important contributions: i) it presents in detail the statisti-
cal test generation method, which can generate high-quality
testset for creating high relative activity in arbitrary tro-
jan instances; ii) it analyzes the eﬀectiveness of generated
testset in terms of trojan coverage; and iii) it describes two
judicious reordering methods can further tune the testset
and greatly improve the side channel sensitivity. simulation
results demonstrate that the tests generated by mers can
signiﬁcantly increase the trojans sensitivity, thereby making
trojan detection eﬀective using side-channel analysis.

ccs concepts
•hardware → hardware test; very large scale integra-
tion design; hardware validation;

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:13) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978396

keywords
hardware security; hardware trojan detection; side-channel
analysis; statistical test generation.

1.

|
Non-data,|abstract

we give attacks on feistel-based format-preserving encryp-
tion (fpe) schemes that succeed in message recovery (not
merely distinguishing scheme outputs from random) when
the message space is small. for 4-bit messages, the at-
tacks fully recover the target message using 221 examples
for the ff3 nist standard and 225 examples for the ff1
nist standard. the examples include only three messages
per tweak, which is what makes the attacks non-trivial even
though the total number of examples exceeds the size of the
domain. the attacks are rigorously analyzed in a new deﬁni-
tional framework of message-recovery security. the attacks
are easily put out of reach by increasing the number of feistel
rounds in the standards.

|
Data,|abstract
electronic ﬁnancial transactions in the us, even those en-
abled by bitcoin, have relatively high transaction costs. as
a result, it becomes infeasible to make micropayments, i.e.
payments that are pennies or fractions of a penny.

to circumvent the cost of recording all transactions, wheeler

(1996) and rivest (1997) suggested the notion of a prob-
abilistic payment, that is, one implements payments that
have expected value on the order of micro pennies by run-
ning an appropriately biased lottery for a larger payment.
while there have been quite a few proposed solutions to such
lottery-based micropayment schemes, all these solutions rely
on a trusted third party to coordinate the transactions; fur-
thermore, to implement these systems in today’s economy
would require a a global change to how either banks or elec-
tronic payment companies (e.g., visa and mastercard) han-
dle transactions.

we put forth a new lottery-based micropayment scheme
for any ledger-based transaction system, that can be used
today without any change to the current infrastructure. we
implement our scheme in a sample web application and show
how a single server can handle thousands of micropayment
requests per second. we analyze how the scheme can work
at internet scale.

1.

|
Non-data,|abstract
we present an approach to enforce access revocation on re-
sources stored at external cloud providers. the approach
relies on a resource transformation that provides strong mu-
tual inter-dependency in its encrypted representation. to
revoke access on a resource, it is then suﬃcient to update a
small portion of it, with the guarantee that the resource as
a whole (and any portion of it) will become unintelligible to
those from whom access is revoked. the extensive experi-
mental evaluation on a variety of conﬁgurations conﬁrmed
the eﬀectiveness and eﬃciency of our solution, which showed
excellent performance and compatibility with several imple-
mentation strategies.

keywords
access control; policy revocation; resource encryption;
mix&slice

1.

|
Data,|abstract
we discuss the design of symmetric primitives, in partic-
ular pseudo-random functions (prfs) which are suitable
for use in a secret-sharing based mpc system. we consider
three diﬀerent prfs: the naor-reingold prf, a prf based
on the legendre symbol, and a specialized block cipher de-
sign called mimc. we present protocols for implementing
these prfs within a secret-sharing based mpc system, and
discuss possible applications. we then compare the per-
formance of our protocols. depending on the application,
diﬀerent prfs may oﬀer diﬀerent optimizations and advan-
tages over the classic aes benchmark. thus, we cannot
conclude that there is one optimal prf to be used in all
situations.

1.

|
Data,|abstract
additive manufacturing, also known as 3d printing, has
been increasingly applied to fabricate highly intellectual prop-
erty (ip) sensitive products. however, the related ip protec-
tion issues in 3d printers are still largely underexplored. on
the other hand, smartphones are equipped with rich onboard
sensors and have been applied to pervasive mobile surveil-
lance in many applications. these facts raise one critical
question:
is it possible that smartphones access the side-
channel signals of 3d printer and then hack the ip infor-
mation? to answer this, we perform an end-to-end study
on exploring smartphone-based side-channel attacks against
3d printers. speciﬁcally, we formulate the problem of the
ip side-channel attack in 3d printing. then, we investigate
the possible acoustic and magnetic side-channel attacks us-
ing the smartphone built-in sensors. moreover, we explore a
magnetic-enhanced side-channel attack model to accurately
deduce the vital directional operations of 3d printer. ex-
perimental results show that by exploiting the side-channel
signals collected by smartphones, we can successfully re-
construct the physical prints and their g-code with mean
tendency error of 5.87% on regular designs and 9.67% on
complex designs, respectively. our study demonstrates this
new and practical smartphone-based side channel attack on
compromising ip information during 3d printing.

1.

|
Data,|abstract
sms (short messaging service) is a text messaging service for mo-
bile users to exchange short text messages. it is also widely used to
provide sms-powered services (e.g., mobile banking). with the
rapid deployment of all-ip 4g mobile networks, the underlying
technology of sms evolves from the legacy circuit-switched net-
work to the ims (ip multimedia subsystem) system over packet-
switched network.
in this work, we study the insecurity of the
ims-based sms. we uncover its security vulnerabilities and ex-
ploit them to devise four sms attacks: silent sms abuse, sms
spooﬁng, sms client dos, and sms spamming. we further dis-
cover that those sms threats can propagate towards sms-powered
services, thereby leading to three malicious attacks: social network
account hijacking, unauthorized donation, and unauthorized sub-
scription. our analysis reveals that the problems stem from the
loose security regulations among mobile phones, carrier networks,
and sms-powered services. we ﬁnally propose remedies to the
identiﬁed security issues.

keywords
mobile networks; lte; ims; sms; attack; defense

1.

|
Non-data,|abstract
with the proliferation of internet of things, there is a grow-
ing interest in embedded system attacks, e.g., key extrac-
tion attacks and ﬁrmware modiﬁcation attacks. code execu-
tion tracking, as the ﬁrst step to locate vulnerable instruction
pieces for key extraction attacks and to conduct control-ﬂow
integrity checking against ﬁrmware modiﬁcation attacks, is
therefore of great value. because embedded systems, espe-
cially legacy embedded systems, have limited resources and
may not support software or hardware update, it is impor-
tant to design low-cost code execution tracking methods that
require as little system modiﬁcation as possible. in this work,
we propose a non-intrusive code execution tracking solution
via power-side channel, wherein we represent the code ex-
ecution and its power consumption with a revised hidden
markov model and recover the most likely executed instruc-
tion sequence with a revised viterbi algorithm. by observing
the power consumption of the microcontroller unit during ex-
ecution, we are able to recover the program execution ﬂow
with a high accuracy and detect abnormal code execution be-
havior even when only a single instruction is modiﬁed.

1.

|
Non-data,|abstract
bitcoin provides two incentives for miners: block rewards
and transaction fees. the former accounts for the vast ma-
jority of miner revenues at the beginning of the system, but
it is expected to transition to the latter as the block rewards
dwindle. there has been an implicit belief that whether
miners are paid by block rewards or transaction fees does
not aﬀect the security of the block chain.

we show that this is not the case. our key insight is that
with only transaction fees, the variance of the block reward is
very high due to the exponentially distributed block arrival
time, and it becomes attractive to fork a “wealthy” block
to “steal” the rewards therein. we show that this results
in an equilibrium with undesirable properties for bitcoin’s
security and performance, and even non-equilibria in some
circumstances. we also revisit selﬁsh mining and show that
it can be made proﬁtable for a miner with an arbitrarily low
hash power share, and who is arbitrarily poorly connected
within the network. our results are derived from theoretical
analysis and conﬁrmed by a new bitcoin mining simulator
that may be of independent interest.

we discuss the troubling implications of our results for
bitcoin’s future security and draw lessons for the design of
new cryptocurrencies.

1.

|
Data,|abstract
while modern block ciphers, such as aes, have a block size of
at least 128 bits, there are many 64-bit block ciphers, such as
3des and blowﬁsh, that are still widely supported in internet
security protocols such as tls, ssh, and ipsec. when used
in cbc mode, these ciphers are known to be susceptible
to collision attacks when they are used to encrypt around
232 blocks of data (the so-called birthday bound). this
threat has traditionally been dismissed as impractical since it
requires some prior knowledge of the plaintext and even then,
it only leaks a few secret bits per gigabyte. indeed, practical
collision attacks have never been demonstrated against any
mainstream security protocol, leading to the continued use
of 64-bit ciphers on the internet.

in this work, we demonstrate two concrete attacks that
exploit collisions on short block ciphers. first, we present
an attack on the use of 3des in https that can be used
to recover a secret session cookie. second, we show how a
similar attack on blowﬁsh can be used to recover http
basicauth credentials sent over openvpn connections. in
our proof-of-concept demos, the attacker needs to capture
about 785gb of data, which takes between 19-38 hours in
our setting. this complexity is comparable to the recent rc4
attacks on tls: the only fully implemented attack takes 75
hours. we evaluate the impact of our attacks by measuring
the use of 64-bit block ciphers in real-world protocols. we
discuss mitigations, such as disabling all 64-bit block ciphers,
and report on the response of various software vendors to
our responsible disclosure of these attacks.

1.

|
Non-data,|abstract
proof of work (pow) powered blockchains currently account for
more than 90% of the total market capitalization of existing digi-
tal cryptocurrencies. although the security provisions of bitcoin
have been thoroughly analysed, the security guarantees of variant
(forked) pow blockchains (which were instantiated with different
parameters) have not received much attention in the literature.

in this paper, we introduce a novel quantitative framework to
analyse the security and performance implications of various con-
sensus and network parameters of pow blockchains. based on
our framework, we devise optimal adversarial strategies for double-
spending and selﬁsh mining while taking into account real world
constraints such as network propagation, different block sizes, block
generation intervals, information propagation mechanism, and the
impact of eclipse attacks. our framework therefore allows us to
capture existing pow-based deployments as well as pow blockchain
variants that are instantiated with different parameters, and to objec-
tively compare the tradeoffs between their performance and security
provisions.

1.

|
Non-data,|abstract
visual cryptography has been applied to design human com-
putable authentication protocols.
in such a protocol, the
user and the server share a secret key in the form of an im-
age printed on a transparent medium, which the user super-
imposes on server-generated image challenges, and visually
decodes a response code from the image. an example of
such protocols is passwindow, an award-winning commercial
product. we study the security and usability of segment-
based visual cryptographic authentication protocols (svaps),
which include passwindow as a particular case. in an svap,
the images consist of segments and are thus structured. our
overall ﬁndings are negative. we introduce two attacks that
together can break all svaps we considered in the paper.
moreover, our attacks exploit fundamental weaknesses of
svaps that appear diﬃcult to ﬁx. we have also evaluated
the usability of diﬀerent svaps and found that the protocol
that oﬀers the best security has the poorest usability.

keywords
visual cryptography; user authentication; attack

1.

|
Non-data,|abstract
we present a highly eﬃcient cryptographic protocol to pro-
tect user passwords against server compromise by distribut-
ing the capability to verify passwords over multiple servers.
password veriﬁcation is a single-round protocol and requires
from each server only one exponentiation in a prime-order
group. in spite of its simplicity, our scheme boasts security
against dynamic and transient corruptions, meaning that
servers can be corrupted at any time and can recover from
corruption by going through a non-interactive key refresh
procedure. the users’ passwords remain secure against of-
ﬂine dictionary attacks as long as not all servers are cor-
rupted within the same time period between refreshes. the
only currently known scheme to achieve such strong security
guarantees incurs the considerable cost of several hundred
exponentiations per server. we prove our scheme secure
in the universal composability model, which is well-known
to oﬀer important beneﬁts for password-based primitives,
under the gap one-more diﬃe-hellman assumption in the
random-oracle model. server initialization and refresh must
take place in a trusted execution environment.
initializa-
tion additionally requires a secure message to each server,
but the refresh procedure is non-interactive. we show that
these requirements are easily met in practice by providing
an example deployment architecture.

categories and subject descriptors
d.4.6 [security and protection]: cryptographic control;
d.4.6 [security and protection]: access controls; d.4.6
[security and protection]: authentication

keywords
password veriﬁcation, proactive security, uc security.

1.

|
Non-data,|abstract
in the setting of secure multiparty computation, a set of
parties with private inputs wish to compute some function
of their inputs without revealing anything but their output.
over the last decade, the eﬃciency of secure two-party com-
putation has advanced in leaps and bounds, with speedups of
some orders of magnitude, making it fast enough to be of use
in practice. in contrast, progress on the case of multiparty
computation (with more than two parties) has been much
slower, with very little work being done. currently, the only
implemented eﬃcient multiparty protocol has many rounds
of communication (linear in the depth of the circuit being
computed) and thus is not suited for internet-like settings
where latency is not very low.

in this paper, we construct highly eﬃcient constant-round
protocols for the setting of multiparty computation for semi-
honest adversaries. our protocols work by constructing a
multiparty garbled circuit, as proposed in bmr (beaver et
al., stoc 1990). our ﬁrst protocol uses oblivious transfer
and constitutes the ﬁrst concretely-eﬃcient constant-round
multiparty protocol for the case of no honest majority. our
second protocol uses bgw, and is signiﬁcantly more eﬃcient
than the fairplaymp protocol (ben-david et al., ccs 2008)
that also uses bgw.

we ran extensive experimentation comparing our diﬀer-
ent protocols with each other and with a highly-optimized
implementation of semi-honest gmw. due to our protocol
being constant round, it signiﬁcantly outperforms gmw in
internet-like settings. for example, with 13 parties situated
in the virginia and ireland amazon regions and the sha256
∗
supported by israel science foundation grant 544/13, the cyber se-
curity research center at ben gurion university, and by the frankel
center for computer science.
†
supported by the european research council under the erc con-
solidators grant agreement n. 615172 (hips) and by the biu center
for research in applied cryptography and cyber security in conjunc-
tion with the israel national cyber bureau in the prime minister’s
oﬃce.
‡
istry and by israel science foundation grant 544/13.

supported by a grant from the israeli science and technology min-

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:13) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978347

circuit with 90,000 gates and of depth 4000, the overall run-
ning time of our protocol is 25 seconds compared to 335
seconds for gmw. furthermore, our online time is under
half a second compared to 330 seconds for gmw.

1.

|
Non-data,|abstract
in the last few years, there has been signiﬁcant interest in de-
veloping methods to search over encrypted data. in the case
of range queries, a simple solution is to encrypt the contents
of the database using an order-preserving encryption (ope)
scheme (i.e., an encryption scheme that supports compar-
isons over encrypted values). however, naveed et al. (ccs
2015) recently showed that ope-encrypted databases are
extremely vulnerable to “inference attacks.”

in this work, we consider a related primitive called order-
revealing encryption (ore), which is a generalization of
ope that allows for stronger security. we begin by con-
structing a new ore scheme for small message spaces which
achieves the “best-possible” notion of security for ore. next,
we introduce a “domain-extension” technique and apply it to
our small-message-space ore. while our domain-extension
technique does incur a loss in security, the resulting ore
scheme we obtain is more secure than all existing (state-
less and non-interactive) ope and ore schemes which are
practical. all of our constructions rely only on symmetric
primitives. as part of our analysis, we also give a tight lower
bound for ope and show that no eﬃcient ope scheme can
satisfy best-possible security if the message space contains
just three messages. thus, achieving strong notions of secu-
rity for even small message spaces requires moving beyond
ope.

finally, we examine the properties of our new ore scheme
and show how to use it to construct an eﬃcient range query
protocol that is robust against the inference attacks of naveed
et al. we also give a full implementation of our new ore
scheme, and show that not only is our scheme more secure
than existing ope schemes, it is also faster: encrypting a
32-bit integer requires just 55 microseconds, which is more
than 65 times faster than existing ope schemes.

∗the full version of this paper [43] with complete proofs is

available at http://eprint.iacr.org/2016/612.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:13) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978376

1.

|
Data,|abstract
in this paper, we study the over-the-top (ott) bypass
fraud, a recent form of interconnect telecom fraud. in ott
bypass, a normal phone call is diverted over ip to a voice
chat application on a smartphone, instead of being termi-
nated over the normal telecom infrastructure. this rerout-
ing (or hijack) is performed by an international transit op-
erator in coordination with the ott service provider, but
without explicit authorization from the caller, callee and
their operators. by doing so, they collect a large share of
the call charge and induce a signiﬁcant loss of revenue to
the bypassed operators. moreover, this practice degrades
the quality of service without providing any beneﬁts for the
users.

in this paper, we study the possible techniques to detect
and measure this fraud and evaluate the real impact of ott
bypass on a small european country. for this, we performed
more than 15,000 test calls during 8 months and conducted
a user study with more than 8,000 users.

in our measurements, we observed up to 83% of calls being
subject to ott bypass. additionally, we show that ott
bypass degrades the quality of service, and sometimes collide
with other fraud schemes, exacerbating the quality issues.
our user study shows that ott bypass and its eﬀects are
poorly understood by users.

1.

|
Data,|abstract
recently there has been much interest in performing search queries
over encrypted data to enable functionality while protecting sensi-
tive data. one particularly efﬁcient mechanism for executing such
queries is order-preserving encryption/encoding (ope) which re-
sults in ciphertexts that preserve the relative order of the underlying
plaintexts thus allowing range and comparison queries to be per-
formed directly on ciphertexts. recently, popa et al. (s&p 2013)
gave the ﬁrst construction of an ideally-secure ope scheme and
kerschbaum (ccs 2015) showed how to achieve the even stronger
notion of frequency-hiding ope. however, as naveed et al. (ccs
2015) have recently demonstrated, these constructions remain vul-
nerable to several attacks. additionally, all previous ideal ope
schemes (with or without frequency-hiding) either require a large
round complexity of o(log n) rounds for each insertion, or a large
persistent client storage of size o(n), where n is the number of
items in the database. it is thus desirable to achieve a range query
scheme addressing both issues gracefully.

in this paper, we propose an alternative approach to range queries
over encrypted data that is optimized to support insert-heavy work-
loads as are common in “big data” applications while still maintain-
ing search functionality and achieving stronger security. speciﬁ-
cally, we propose a new primitive called partial order preserving
encoding (pope) that achieves ideal ope security with frequency
hiding and also leaves a sizable fraction of the data pairwise in-
comparable. using only o(1) persistent and o(n) non-persistent
client storage for 0 <  < 1, our pope scheme provides extremely
fast batch insertion consisting of a single round, and efﬁcient search
with o(1) amortized cost for up to o(n1−) search queries. this
improved security and performance makes our scheme better suited
for today’s insert-heavy databases.

|
Non-data,|abstract
pseudo-random number generators (prngs) are a critical
infrastructure for cryptography and security of many com-
puter applications. at the same time, prngs are surpris-
ingly diﬃcult to design, implement, and debug. this paper
presents the ﬁrst static analysis technique speciﬁcally for
quality assurance of cryptographic prng implementations.
the analysis targets a particular kind of implementation
defect, the entropy loss. entropy loss occurs when the en-
tropy contained in the prng seed is not utilized to the
full extent for generating the pseudo-random output stream.
the debian openssl disaster, probably the most prominent
prng-related security incident, was one but not the only
manifestation of such a defect.

together with the static analysis technique, we present its
implementation, a tool named entroposcope. the tool of-
fers a high degree of automation and practicality. we have
applied the tool to ﬁve real-world prngs of diﬀerent de-
signs and show that it eﬀectively detects both known and
previously unknown instances of entropy loss.

keywords
pseudo-random number generator; prng; entropy loss;
information ﬂow; openssl; static analysis; bounded model
checking

1.

|
Non-data,|abstract
modern operating systems use hardware support to protect
against control-ﬂow hijacking attacks such as code-injection
attacks. typically, write access to executable pages is pre-
vented and kernel mode execution is restricted to kernel code
pages only. however, current cpus provide no protection
against code-reuse attacks like rop. aslr is used to pre-
vent these attacks by making all addresses unpredictable for
an attacker. hence, the kernel security relies fundamentally
on preventing access to address information.

we introduce prefetch side-channel attacks, a new class
of generic attacks exploiting major weaknesses in prefetch
instructions. this allows unprivileged attackers to obtain
address information and thus compromise the entire system
by defeating smap, smep, and kernel aslr. prefetch can
fetch inaccessible privileged memory into various caches on
intel x86. it also leaks the translation-level for virtual ad-
dresses on both intel x86 and armv8-a. we build three at-
tacks exploiting these properties. our ﬁrst attack retrieves
an exact image of the full paging hierarchy of a process,
defeating both user space and kernel space aslr. our sec-
ond attack resolves virtual to physical addresses to bypass
smap on 64-bit linux systems, enabling ret2dir attacks.
we demonstrate this from unprivileged user programs on
linux and inside amazon ec2 virtual machines. finally,
we demonstrate how to defeat kernel aslr on windows 10,
enabling rop attacks on kernel and driver binary code. we
propose a new form of strong kernel isolation to protect com-
modity systems incuring an overhead of only 0.06–5.09%.

ccs concepts
•security and privacy → side-channel analysis and
countermeasures; systems security; operating sys-
tems security;

keywords
aslr; kernel vulnerabilities; timing attacks

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24 - 28, 2016, vienna, austria
c(cid:13) 2016 copyright held by the owner/author(s). publication rights licensed to acm.
isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978356

1.

|
Non-data,|abstract
security against hardware trojans is currently becoming an
essential ingredient to ensure trust in information systems.
a variety of solutions have been introduced to reach this
goal, ranging from reactive (i.e., detection-based) to pre-
ventive (i.e., trying to make the insertion of a trojan more
diﬃcult for the adversary). in this paper, we show how test-
ing (which is a typical detection tool) can be used to state
concrete security guarantees for preventive approaches to
trojan-resilience. for this purpose, we build on and formal-
ize two important previous works which introduced “input
scrambling” and “split manufacturing” as countermeasures
to hardware trojans. using these ingredients, we present
a generic compiler that can transform any circuit into a
trojan-resilient one, for which we can state quantitative se-
curity guarantees on the number of correct executions of the
circuit thanks to a new tool denoted as “testing ampliﬁca-
tion”. compared to previous works, our threat model covers
an extended range of hardware trojans while we stick with
the goal of minimizing the number of honest elements in
our transformed circuits. since transformed circuits essen-
tially correspond to redundant multiparty computations of
the target functionality, they also allow reasonably eﬃcient
implementations, which can be further optimized if special-
ized to certain cryptographic primitives and security goals.

1.

|
Non-data,|abstract
defenders of enterprise networks have a critical need to
quickly identify the root causes of malware and data leak-
age.
increasingly, usb storage devices are the media of
choice for data exﬁltration, malware propagation, and even
cyber-warfare. we observe that a critical aspect of ex-
plaining and preventing such attacks is understanding the
provenance of data (i.e., the lineage of data from its creation
to current state) on usb devices as a means of ensuring
their safe usage. unfortunately, provenance tracking is
not oﬀered by even sophisticated modern devices. this
work presents provusb, an architecture for ﬁne-grained
provenance collection and tracking on smart usb devices.
provusb maintains data provenance by recording reads
and writes at the block layer and reliably identifying hosts
editing those blocks through attestation over the usb chan-
nel. our evaluation ﬁnds that provusb imposes a one-time
850 ms overhead during usb enumeration, but approaches
nearly-bare-metal runtime performance (90% of through-
put) on larger ﬁles during normal execution, and less than
0.1% storage overhead for provenance in real-world work-
loads. provusb thus provides essential new techniques in
the defense of computer systems and usb storage devices.

1.

|
Data,|abstract
third-party libraries on android have been shown to be se-
curity and privacy hazards by adding security vulnerabilities
to their host apps or by misusing inherited access rights.
correctly attributing improper app behavior either to app
or library developer code or isolating library code from their
host apps would be highly desirable to mitigate these prob-
lems, but is impeded by the absence of a third-party library
detection that is eﬀective and reliable in spite of obfuscated
code. this paper proposes a library detection technique that
is resilient against common code obfuscations and that is
capable of pinpointing the exact library version used in apps.
libraries are detected with proﬁles from a comprehensive
library database that we generated from the original library
sdks. we apply our technique to the top apps on google
play and their complete histories to conduct a longitudinal
study of library usage and evolution in apps. our results
particularly show that app developers only slowly adapt new
library versions, exposing their end-users to large windows
of vulnerability. for instance, we discovered that two long-
known security vulnerabilities in popular libs are still present
in the current top apps. moreover, we ﬁnd that misuse of
cryptographic apis in advertising libs, which increases the
host apps’ attack surface, aﬀects 296 top apps with a cu-
mulative install base of 3.7bn devices according to play. to
the best of our knowledge, our work is ﬁrst to quantify the
security impact of third-party libs on the android ecosystem.

1.

|
Non-data,|abstract
cache side-channel attacks have been extensively studied
on x86 architectures, but much less so on arm processors.
the technical challenges to conduct side-channel attacks on
arm, presumably, stem from the poorly documented arm
cache implementations, such as cache coherence protocols
and cache ﬂush operations, and also the lack of understand-
ing of how diﬀerent cache implementations will aﬀect side-
channel attacks. this paper presents a systematic explo-
ration of vectors for flush-reload attacks on arm pro-
cessors. flush-reload attacks are among the most well-
known cache side-channel attacks on x86. it has been shown
in previous work that they are capable of exﬁltrating sensi-
tive information with high ﬁdelity. we demonstrate in this
work a novel construction of ﬂush-reload side channels on
last-level caches of arm processors, which, particularly, ex-
ploits return-oriented programming techniques to reload in-
structions. we also demonstrate several attacks on android
os (e.g., detecting hardware events and tracing software ex-
ecution paths) to highlight the implications of such attacks
for android devices.

keywords
cache side channels; ﬂush-reload

1.

|
Non-data,|abstract
modern applications often operate on data in multiple administra-
tive domains. in this federated setting, participants may not fully
trust each other. these distributed applications use transactions as
a core mechanism for ensuring reliability and consistency with per-
sistent data. however, the coordination mechanisms needed for
transactions can both leak conﬁdential information and allow unau-
thorized inﬂuence.

by implementing a simple attack, we show these side channels
can be exploited. however, our focus is on preventing such attacks.
we explore secure scheduling of atomic, serializable transactions
in a federated setting. while we prove that no protocol can guaran-
tee security and liveness in all settings, we establish conditions for
sets of transactions that can safely complete under secure schedul-
ing. based on these conditions, we introduce staged commit, a
secure scheduling protocol for federated transactions. this proto-
col avoids insecure information channels by dividing transactions
into distinct stages. we implement a compiler that statically checks
code to ensure it meets our conditions, and a system that schedules
these transactions using the staged commit protocol. experiments
on this implementation demonstrate that realistic federated transac-
tions can be scheduled securely, atomically, and efﬁciently.

1.

|
Non-data,|abstract
large numbers of smart connected devices, also named as the in-
ternet of things (iot), are permeating our environments (homes,
factories, cars, and also our body—with wearable devices) to collect
data and act on the insight derived. ensuring software integrity (in-
cluding os, apps, and conﬁgurations) on such smart devices is then
essential to guarantee both privacy and safety. a key mechanism to
protect the software integrity of these devices is remote attestation:
a process that allows a remote veriﬁer to validate the integrity of
the software of a device. this process usually makes use of a signed
hash value of the actual device’s software, generated by dedicated
hardware. while individual device attestation is a well-established
technique, to date integrity veriﬁcation of a very large number of
devices remains an open problem, due to scalability issues.

in this paper, we present sana, the ﬁrst secure and scalable pro-
tocol for efﬁcient attestation of large sets of devices that works under
realistic assumptions. sana relies on a novel signature scheme to
allow anyone to publicly verify a collective attestation in constant
time and space, for virtually an unlimited number of devices. we
substantially improve existing swarm attestation schemes [5] by sup-
porting a realistic trust model where: (1) only the targeted devices
are required to implement attestation; (2) compromising any device
does not harm others; and (3) all aggregators can be untrusted. we
implemented sana and demonstrated its efﬁciency on tiny sensor
devices. furthermore, we simulated sana at large scale, to assess
its scalability. our results show that sana can provide efﬁcient
attestation of networks of 1, 000, 000 devices, in only 2.5 seconds.

1.

|
Non-data,|abstract
recent literature on ios security has focused on the ma-
licious potential of third-party applications, demonstrating
how developers can bypass application vetting and code-
level protections. in addition to these protections, ios uses
a generic sandbox proﬁle called “container” to conﬁne ma-
licious or exploited third-party applications. in this paper,
we present the ﬁrst systematic analysis of the ios container
sandbox proﬁle. we propose the sandscout framework to
extract, decompile, formally model, and analyze ios sand-
box proﬁles as logic-based programs. we use our prolog-
based queries to evaluate ﬁle-based security properties of the
container sandbox proﬁle for ios 9.0.2 and discover seven
classes of exploitable vulnerabilities. these attacks aﬀect
non-jailbroken devices running later versions of ios. we are
working with apple to resolve these attacks, and we expect
that sandscout will play a signiﬁcant role in the develop-
ment of sandbox proﬁles for future versions of ios.

1.

|
Data,|abstract
because of rampant security breaches in iot devices, searching
vulnerabilities in massive iot ecosystems is more crucial than ever.
recent studies have demonstrated that control-ﬂow graph (cfg)
based bug search techniques can be effective and accurate in iot
devices across different architectures. however, these cfg-based
bug search approaches are far from being scalable to handle an
enormous amount of iot devices in the wild, due to their expen-
sive graph matching overhead. inspired by rich experience in im-
age and video search, we propose a new bug search scheme which
addresses the scalability challenge in existing cross-platform bug
search techniques and further improves search accuracy. unlike
existing techniques that directly conduct searches based upon raw
features (cfgs) from the binary code, we convert the cfgs into
high-level numeric feature vectors. compared with the cfg fea-
ture, high-level numeric feature vectors are more robust to code
variation across different architectures, and can easily achieve real-
time search by using state-of-the-art hashing techniques.

we have implemented a bug search engine, genius, and com-
pared it with state-of-art bug search approaches. experimental re-
sults show that genius outperforms baseline approaches for vari-
ous query loads in terms of speed and accuracy. we also evaluated
genius on a real-world dataset of 33,045 devices which was col-
lected from public sources and our system. the experiment showed
that genius can ﬁnish a search within 1 second on average when
performed over 8,126 ﬁrmware images of 420,558,702 functions.
by only looking at the top 50 candidates in the search result, we
found 38 potentially vulnerable ﬁrmware images across 5 vendors,
and conﬁrmed 23 of them by our manual analysis. we also found
that it took only 0.1 seconds on average to ﬁnish searching for all
154 vulnerabilities in two latest commercial ﬁrmware images from
d-link. 103 of them are potentially vulnerable in these images,
and 16 of them were conﬁrmed.

keywords
firmware security; machine learning; graph encoding

acm acknowledges that this contribution was authored or co-authored by an em-
ployee, or contractor of the national government. as such, the government retains
a nonexclusive, royalty-free right to publish or reproduce this article, or to allow oth-
ers to do so, for government purposes only. permission to make digital or hard copies
for personal or classroom use is granted. copies must bear this notice and the full ci-
tation on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. to copy otherwise, distribute, republish, or post, requires prior
speciﬁc permission and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:13) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978370

1.

|
Data,|abstract
email as we use it today makes no guarantees about message in-
tegrity, authenticity, or conﬁdentiality. users must explicitly en-
crypt and sign message contents using tools like pgp if they wish
to protect themselves against message tampering, forgery, or eaves-
dropping. however, few do, leaving the vast majority of users open
to such attacks. fortunately, transport-layer security mechanisms
(available as extensions to smtp, imap, pop3) provide some de-
gree of protection against network-based eavesdropping attacks. at
the same time, dkim and spf protect against network-based mes-
sage forgery and tampering.

in this work we evaluate the security provided by these proto-
cols, both in theory and in practice. using a combination of mea-
surement techniques, we determine whether major providers sup-
ports tls at each point in their email message path, and whether
they support spf and dkim on incoming and outgoing mail. we
found that while more than half of the top 20,000 receiving mtas
supported tls, and support for tls is increasing, servers do not
check certiﬁcates, opening the internet email system up to man-
in-the-middle eavesdropping attacks. at the same time, while use
of spf is common, enforcement is limited. moreover, few of the
senders we examined used dkim, and fewer still rejected invalid
dkim signatures. our ﬁndings show that the global email system
provides some protection against passive eavesdropping, limited
protection against unprivileged peer message forgery, and no pro-
tection against active network-based attacks. we observe that pro-
tection even against the latter is possible using existing protocols
with proper enforcement.

1.

|
Data,|abstract
censorship-circumvention systems are designed to help users by-
pass internet censorship. as more sophisticated deep-packet-
inspection (dpi) mechanisms have been deployed by censors to de-
tect circumvention tools, activists and researchers have responded
by developing network protocol obfuscation tools. these have
proved to be effective in practice against existing dpi and are now
distributed with systems such as tor.

in this work, we provide the ﬁrst in-depth investigation of the
detectability of in-use protocol obfuscators by dpi. we build a
framework for evaluation that uses real network trafﬁc captures to
evaluate detectability, based on metrics such as the false-positive
rate against background (i.e., non obfuscated) trafﬁc. we ﬁrst
exercise our framework to show that some previously proposed
attacks from the literature are not as effective as a censor might
like. we go on to develop new attacks against ﬁve obfuscation
tools as they are conﬁgured in tor, including:
two variants of
obfsproxy, fte, and two variants of meek. we conclude by using
our framework to show that all of these obfuscation mechanisms
could be reliably detected by a determined censor with sufﬁciently
low false-positive rates for use in many censorship settings.

categories and subject descriptors
c.2.0 [computer-communication networks]: general—secu-
rity and protection

keywords
censorship-resistance; network obfuscation; tor

1.

|
Data,|abstract
random walks form a critical foundation in many social net-
work based security systems and applications. currently,
the design of such social security mechanisms is limited to
the classical paradigm of using ﬁxed-length random walks
for all nodes on a social graph. however, the ﬁxed-length
walk paradigm induces a poor trade-oﬀ between security and
other desirable properties.

in this paper, we propose smartwalk, a security enhanc-
ing system which incorporates adaptive random walks in so-
cial network security applications. we utilize a set of su-
pervised machine learning techniques to predict the neces-
sary random walk length based on the structural charac-
teristics of a social graph. using experiments on multiple
real world topologies, we show that the desired walk length
starting from a speciﬁc node can be well predicted given the
local features of the node, and limited knowledge for a small
set of training nodes. we describe node-adaptive and path-
adaptive random walk usage models, where the walk length
adaptively changes based on the starting node and the inter-
mediate nodes on the path, respectively. we experimentally
demonstrate the applicability of adaptive random walks on
a number of social network based security and privacy sys-
tems, including sybil defenses, anonymous communication
and link privacy preserving systems, and show up to two
orders of magnitude improvement in performance.

1.

|
Data,|abstract
this work presents a new approach for deobfuscating an-
droid apks based on probabilistic learning of large code
bases (termed “big code”). the key idea is to learn a prob-
abilistic model over thousands of non-obfuscated android
applications and to use this probabilistic model to deob-
fuscate new, unseen android apks. the concrete focus
of the paper is on reversing layout obfuscation, a popular
transformation which renames key program elements such
as classes, packages and methods, thus making it diﬃcult to
understand what the program does.

concretely, the paper: (i) phrases the layout deobfusca-
tion problem of android apks as structured prediction in
a probabilistic graphical model, (ii) instantiates this model
with a rich set of features and constraints that capture the
android setting, ensuring both semantic equivalence and
high prediction accuracy, and (iii) shows how to leverage
powerful inference and learning algorithms to achieve over-
all precision and scalability of the probabilistic predictions.
we implemented our approach in a tool called deguard
and used it to: (i) reverse the layout obfuscation performed
by the popular proguard system on benign, open-source ap-
plications, (ii) predict third-party libraries imported by be-
nign apks (also obfuscated by proguard), and (iii) rename
obfuscated program elements of android malware. the ex-
perimental results indicate that deguard is practically ef-
fective:
it recovers 79.1% of the program element names
obfuscated with proguard, it predicts third-party libraries
with accuracy of 91.3%, and it reveals string decoders and
classes that handle sensitive data in android malware.

1.

|
Non-data,|abstract
differential power analysis (dpa) is a side-channel attack in which
an adversary retrieves cryptographic material by measuring and
analyzing the power consumption of the device on which the crypto-
graphic algorithm under attack executes. an effective countermea-
sure against dpa is to mask secrets by probabilistically encoding
them over a set of shares, and to run masked algorithms that com-
pute on these encodings. masked algorithms are often expected to
provide, at least, a certain level of probing security.

leveraging the deep connections between probabilistic infor-
mation ﬂow and probing security, we develop a precise, scalable,
and fully automated methodology to verify the probing security of
masked algorithms, and generate them from unprotected descrip-
tions of the algorithm. our methodology relies on several contribu-
tions of independent interest, including a stronger notion of probing
security that supports compositional reasoning, and a type system
for enforcing an expressive class of probing policies. finally, we
validate our methodology on examples that go signiﬁcantly beyond
the state-of-the-art.

1.

|
Data,|abstract
mobile operating systems like android failed to provide suf-
ﬁcient protection on personal data, and privacy leakage be-
comes a major concern. to understand the security risks
and privacy leakage, analysts have to carry out data-ﬂow
analysis. in 2014, android upgraded with a fundamentally
new design known as android runtime (art) environ-
ment in android 5.0. art adopts ahead-of-time compi-
lation strategy and replaces previous virtual-machine-based
dalvik. unfortunately, many data-ﬂow analysis systems like
taintdroid [19] were designed for the legacy dalvik environ-
ment. this makes data-ﬂow analysis of new apps and mal-
ware infeasible. we design a multi-level information-ﬂow
tracking system for the new android system called taint-
art. taintart employs a multi-level taint analysis tech-
nique to minimize the taint tag storage. therefore, taint
tags can be stored in processor registers to provide eﬃcient
taint propagation operations. we also customize the art
compiler to maximize performance gains of the ahead-of-
time compilation optimizations. based on the general de-
sign of taintart, we also implement a multi-level privacy
enforcement to prevent sensitive data leakage. we demon-
strate that taintart only incurs less than 15 % overheads
on a cpu-bound microbenchmark and negligible overhead
on built-in or third-party applications. compared to legacy
dalvik environment in android 4.4, taintart achieves
about 99.7 % faster performance for java runtime bench-
mark.

1.

|
Non-data,|abstract
process-based isolation, suggested by several research prototypes,
is a cornerstone of modern browser security architectures. google
chrome is the ﬁrst commercial browser that adopts this architec-
ture. unlike several research prototypes, chrome’s process-based
design does not isolate different web origins, but primarily promises
to protect “the local system” from “the web”. however, as bil-
lions of users now use web-based cloud services (e.g., dropbox
and google drive), which are integrated into the local system, the
premise that browsers can effectively isolate the web from the local
system has become questionable. in this paper, we argue that, if the
process-based isolation disregards the same-origin policy as one of
its goals, then its promise of maintaining the “web/local system
(local)” separation is doubtful. speciﬁcally, we show that exist-
ing memory vulnerabilities in chrome’s renderer can be used as a
stepping-stone to drop executables/scripts in the local ﬁle system,
install unwanted applications and misuse system sensors. these at-
tacks are purely data-oriented and do not alter any control ﬂow or
import foreign code. thus, such attacks bypass binary-level pro-
tection mechanisms, including aslr and in-memory partitioning.
finally, we discuss various full defenses and present a possible way
to mitigate the attacks presented.

1.

|
Non-data,|abstract
the surprising success of cryptocurrencies has led to a surge of inter-
est in deploying large scale, highly robust, byzantine fault tolerant
(bft) protocols for mission-critical applications, such as ﬁnancial
transactions. although the conventional wisdom is to build atop a
(weakly) synchronous protocol such as pbft (or a variation thereof),
such protocols rely critically on network timing assumptions, and
only guarantee liveness when the network behaves as expected. we
argue these protocols are ill-suited for this deployment scenario.

we present an alternative, honeybadgerbft, the ﬁrst practical
asynchronous bft protocol, which guarantees liveness without mak-
ing any timing assumptions. we base our solution on a novel atomic
broadcast protocol that achieves optimal asymptotic efﬁciency. we
present an implementation and experimental results to show our
system can achieve throughput of tens of thousands of transactions
per second, and scales to over a hundred nodes on a wide area net-
work. we even conduct bft experiments over tor, without needing
to tune any parameters. unlike the alternatives, honeybadgerbft
simply does not care about the underlying network.

1.

|
Data,|abstract
in this work, we conduct the ﬁrst systematic study in un-
derstanding the security properties of the usage of unix do-
main sockets by both android apps and system daemons
as an ipc (inter-process communication) mechanism, espe-
cially for cross-layer communications between the java and
native layers. we propose a tool called sinspector to ex-
pose potential security vulnerabilities in using unix domain
sockets through the process of identifying socket addresses,
detecting authentication checks, and performing data ﬂow
analysis. our in-depth analysis revealed some serious vul-
nerabilities in popular apps and system daemons, such as
root privilege escalation and arbitrary ﬁle access. based
on our ﬁndings, we propose countermeasures and improved
practices for utilizing unix domain sockets on android.

1.

|
Non-data,|abstract
thanks to their anonymity (pseudonymity) and elimina-
tion of trusted intermediaries, cryptocurrencies such as bit-
coin have created or stimulated growth in many businesses
and communities. unfortunately, some of these are crim-
inal, e.g., money laundering, illicit marketplaces, and ran-
somware.

next-generation cryptocurrencies such as ethereum will
include rich scripting languages in support of smart con-
tracts, programs that autonomously intermediate transac-
tions. in this paper, we explore the risk of smart contracts
fueling new criminal ecosystems. speciﬁcally, we show how
what we call criminal smart contracts (cscs) can facilitate
leakage of conﬁdential information, theft of cryptographic
keys, and various real-world crimes (murder, arson, terror-
ism).

we show that cscs for leakage of secrets (`a la wikileaks)
are eﬃciently realizable in existing scripting languages such
as that in ethereum. we show that cscs for theft of crypto-
graphic keys can be achieved using primitives, such as suc-
cinct non-interactive arguments of knowledge (snarks),
that are already expressible in these languages and for which
eﬃcient supporting language extensions are anticipated. we
show similarly that authenticated data feeds, an emerging
feature of smart contract systems, can facilitate cscs for
real-world crimes (e.g., property crimes).

our results highlight the urgency of creating policy and
technical safeguards against cscs in order to realize the
promise of smart contracts for beneﬁcial goals.

keywords
criminal smart contracts; ethereum

the ring of gyges is a mythical magical artifact men-
tioned by the philosopher plato in book 2 of his republic.
it granted its owner the power to become invisible at will.
—wikipedia, “ring of gyges”

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
© 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978362

“[on wearing the ring,] no man would keep his hands oﬀ
what was not his own when he could safely take what he liked
out of the market, or go into houses and lie with anyone at
his pleasure, or kill or release from prison whom he would...
” —plato, the republic, book 2 (2.360b) (trans. benjamin
jowett)

1.

|
Data,|abstract
reducing user burden underlying traditional two-factor authentica-
tion constitutes an important research effort. an interesting repre-
sentative approach, sound-proof, leverages ambient sounds to de-
tect the proximity between the second factor device (phone) and
the login terminal (browser). sound-proof was shown to be secure
against remote attackers and highly usable, and is now under early
deployment phases.

in this paper, we identify a weakness of the sound-proof sys-
tem, namely, the remote attacker does not have to predict the am-
bient sounds near the phone as assumed in the sound-proof paper,
but rather can deliberately make—or wait for—the phone to pro-
duce predictable or previously known sounds (e.g., ringer, notiﬁca-
tion or alarm sounds). exploiting this weakness, we build sound-
danger, a full attack system that can successfully compromise the
security of sound-proof. the attack involves buzzing the victim
user’s phone, or waiting for the phone to buzz, and feeding the cor-
responding sounds at the browser to login on behalf of the user.
the attack works precisely under sound-proof’s threat model.

our contributions are three-fold. first, we design and develop
the sound-danger attack system that exploits a wide range of a
smartphone’s functionality to break sound-proof, such as by ac-
tively making a phone or voip call, sending an sms and creating
an app-based notiﬁcation, or by passively waiting for the phone to
trigger an alarm. second, we re-implement sound-proof’s audio
correlation algorithm and evaluate it against sound-danger under
a large variety of attack settings. our results show that many of our
attacks succeed with a 100% chance such that the sound-proof cor-
relation algorithm will accept the attacked audio samples as valid.
third, we collect general population statistics via an online sur-
vey to determine the phone usage habits relevant to our attacks.
we then use these statistics to show how our different correlation-
based attacks can be carefully executed to, for instance, compro-
mise about 57% user accounts in just the ﬁrst attempt and about
83% user accounts in less than a day. finally, we provide some
mitigation strategies and future directions that may help overcome
some of our attacks and strengthen sound-proof.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:13) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978328

maliheh shirvanian

university of alabama at birmingham

maliheh@uab.edu

nitesh saxena

university of alabama at birmingham

saxena@cis.uab.edu

1.

|
Non-data,|abstract
smart contracts are programs that execute autonomously
on blockchains. their key envisioned uses (e.g. ﬁnancial
instruments) require them to consume data from outside the
blockchain (e.g. stock quotes). trustworthy data feeds that
support a broad range of data requests will thus be critical
to smart contract ecosystems.

we present an authenticated data feed system called town
crier (tc). tc acts as a bridge between smart contracts and
existing web sites, which are already commonly trusted for
non-blockchain applications. it combines a blockchain front
end with a trusted hardware back end to scrape https-
enabled websites and serve source-authenticated data to re-
lying smart contracts.

tc also supports conﬁdentiality. it enables private data
requests with encrypted parameters. additionally, in a gen-
eralization that executes smart-contract logic within tc,
the system permits secure use of user credentials to scrape
access-controlled online data sources.

we describe tc’s design principles and architecture and
report on an implementation that uses intel’s recently in-
troduced software guard extensions (sgx) to furnish data
to the ethereum smart contract system. we formally model
tc and deﬁne and prove its basic security properties in the
universal composibility (uc) framework. our results in-
clude deﬁnitions and techniques of general interest relating
to resource consumption (ethereum’s “gas” fee system) and
tcb minimization. we also report on experiments with
three example applications.

we plan to launch tc soon as an online public service.

keywords: authenticated data feeds; smart contracts;
trusted hardware; intel sgx; ethereum; bitcoin

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
© 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978326

1.

|
Non-data,|abstract
we put forth a new cryptographic primitive called a traitor
deterring scheme (tds). a tds is a multi-recipient public-
key encryption scheme where an authority issues decryption
keys to a set of users. the distinguishing feature of a tds is
that secret-keys are issued only after the users provide some
private information as a form of collateral. the traitor de-
terring property ensures that if a malicious coalition of users
(aka “traitors”) produces an unauthorized (aka “pirate”) de-
cryption device, any recipient of the device will be able to re-
cover at least one of the traitors’ collaterals with only black-
box access to the device. on the other hand, honest users’
collaterals are guaranteed to remain hidden. in this fashion
a tds deincentivizes malicious behavior among users.

we model, construct and analyze tds’s based on various
cryptographic assumptions and we show how bitcoin can be
used as collateral for real world deployment of tds’s for the
distribution of digital content. along the way, we present
cryptographic building blocks that may be of independent
interest, namely fuzzy lockers, and comparison predicate en-
cryption schemes for exponentially large domains. we also
compare tds with previous primitives speciﬁcally traitor
tracing schemes (tts) introduced by chor et al. [9] and
digital signets for self enforcement introduced by dwork et
al. [12]. a tds constitutes a strict strengthening of a tts
and, when modeled in what we call the “known ciphertext
model”, it is a reformulation of digital signets in the public-
key, black-box secure setting. in digital signets the adver-
sary attempts to transmit a pirate copy at a favorable “space
rate”, i.e., without having to send the whole plaintext (and
without revealing the traitor collaterals). it is an open ques-
tion from [12] to construct o(1) space rate schemes under a
falsiﬁable assumption. with our tds constructions we re-
solve this open question showing feasibility for space rates
o(log λ/λ) and infeasibility for space rates Ω(log2 λ/λ).

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.
c(cid:13) 2015 acm. isbn 978-1-4503-3832-5/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2810103.2813698.

categories and subject descriptors
k.6 [management of computing and information sys-
tems]: security and protection; e.3 [data encryption]:
public key cryptosystems

keywords
digital rights management; public-key cryptography; self-
enforcement; key management; bitcoin

1.

|
Non-data,|abstract
in this paper, we initiate a formal study of transparency,
which in recent years has become an increasingly critical
requirement for the systems in which people place trust.
we present the abstract concept of a transparency overlay,
which can be used in conjunction with any system to give it
provable transparency guarantees, and then apply the over-
lay to two settings: certiﬁcate transparency and bitcoin.
in the latter setting, we show that the usage of our trans-
parency overlay eliminates the need to engage in mining and
allows users to store a single small value rather than the
entire blockchain. our transparency overlay is generically
constructed from a signature scheme and a new primitive
we call a dynamic list commitment, which in practice can
be instantiated using a collision-resistant hash function.

1.

|
Data,|abstract
subtle ﬂaws in integer computations are a prime source for
exploitable vulnerabilities in system code. unfortunately,
even code shown to be secure on one platform can be vul-
nerable on another, making the migration of code a notable
security challenge. in this paper, we provide the ﬁrst study
on how code that works as expected on 32-bit platforms can
become vulnerable on 64-bit platforms. to this end, we sys-
tematically review the eﬀects of data model changes between
platforms. we ﬁnd that the larger width of integer types
and the increased amount of addressable memory introduce
previously non-existent vulnerabilities that often lie dormant
in program code. we empirically evaluate the prevalence
of these ﬂaws on the source code of debian stable (“jessie”)
and 200 popular open-source projects hosted on github.
moreover, we discuss 64-bit migration vulnerabilities that
have been discovered as part of our study, including vulnera-
bilities in chromium, the boost c++ libraries, libarchive,
the linux kernel, and zlib.

keywords
software security; data models; integer-based vulnerabilities

1.

|
Non-data,|abstract
the low-level c++ programming language is ubiquitously
used for its modularity and performance. typecasting is
a fundamental concept in c++ (and object-oriented pro-
gramming in general) to convert a pointer from one object
type into another. however, downcasting (converting a base
class pointer to a derived class pointer) has critical security
implications due to potentially diﬀerent object memory lay-
outs. due to missing type safety in c++, a downcasted
pointer can violate a programmer’s intended pointer seman-
tics, allowing an attacker to corrupt the underlying memory
in a type-unsafe fashion. this vulnerability class is receiving
increasing attention and is known as type confusion (or bad-
casting). several existing approaches detect diﬀerent forms
of type confusion, but these solutions are severely limited
due to both high run-time performance overhead and low
detection coverage.

this paper presents typesan, a practical type-confusion
detector which provides both low run-time overhead and
high detection coverage. despite improving the coverage
of state-of-the-art techniques, typesan signiﬁcantly reduces
the type-confusion detection overhead compared to other
solutions. typesan relies on an eﬃcient per-object meta-
data storage service based on a compact memory shadowing
scheme. our scheme treats all the memory objects (i.e.,
globals, stack, heap) uniformly to eliminate extra checks on
the fast path and relies on a variable compression ratio to
minimize run-time performance and memory overhead. our
experimental results conﬁrm that typesan is practical, even
when explicitly checking almost all the relevant typecasts in
a given c++ program. compared to the state of the art,
typesan yields orders of magnitude higher coverage at 4–
10 times lower performance overhead on spec and 2 times
on firefox. as a result, our solution oﬀers superior protec-

∗
†
‡

vrije universiteit amsterdam
amsterdam department of informatics
purdue university

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24 - 28, 2016, vienna, austria
© 2016 copyright held by the owner/author(s). publication rights licensed to acm.
isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978405

tion and is suitable for deployment in production software.
moreover, our highly eﬃcient metadata storage back-end is
potentially useful for other defenses that require memory
object tracking.

ccs concepts
•security and privacy → systems security; software
and application security;

keywords
type safety; typecasting; type confusion; downcasting

1.

|
Non-data,|abstract 
with  the  progress  in  mobile  computing,  web  services  are 
increasingly delivered to their users through mobile apps, instead 
of  web  browsers.  however,  unlike  the  browser,  which  enforces 
origin-based security policies to mediate the interactions between 
the web content from different sources,  today’s mobile oses do 
not have a comparable security mechanism to control the cross-
origin communications between apps, as well as those between an 
app  and  the  web.  as  a  result,  a  mobile  user’s  sensitive  web 
resources could be exposed to the harms from a malicious origin.  
in this paper, we report the first systematic study on this mobile 
cross-origin  risk.  our  study  inspects  the  main  cross-origin 
channels on android and ios, including intent, scheme and web-
accessing  utility  classes,  and  further  analyzes  the  ways  popular 
web services (e.g., facebook, dropbox, etc.) and their apps utilize 
those channels to serve other apps. the research shows that lack 
of origin-based protection opens the door to a wide spectrum of 
cross-origin attacks. these attacks are unique to mobile platforms, 
and their consequences are serious: for example, using carefully 
designed  techniques  for  mobile  cross-site  scripting  and  request 
forgery,  an  unauthorized  party  can  obtain  a  mobile  user’s 
facebook/dropbox authentication credentials and record her text 
input. we report our findings to related software vendors, who all 
acknowledged  their  importance.  to  address  this  threat,  we 
designed an origin-based protection mechanism, called morbs, for 
mobile  oses.  morbs  labels  every  message  with  its  origin 
information,  lets  developers  easily  specify  security  policies, and 
enforce the policies on the mobile channels based on origins. our 
evaluation demonstrates the effectiveness of our new technique in 
defeating  unauthorized  origin  crossing,  its  efficiency  and  the 
convenience for the developers to use such protection. 

categories and subject descriptors 
d.4.6 [operating systems]: security and protection – access 
controls, invasive software 
keywords: android, ios, same-origin policy, mobile 
platform. 

1.  |
Data,|abstract
the operating system kernel is the de facto trusted computing base
for most computer systems. to secure the os kernel, many security
mechanisms, e.g., kaslr and stackguard, have been increasingly
deployed to defend against attacks (e.g., code reuse attack). how-
ever, the effectiveness of these protections has been proven to be
inadequate—there are many information leak vulnerabilities in the
kernel to leak the randomized pointer or canary, thus bypassing
kaslr and stackguard. other sensitive data in the kernel, such as
cryptographic keys and file caches, can also be leaked. according to
our study, most kernel information leaks are caused by uninitialized
data reads. unfortunately, existing techniques like memory safety
enforcements and dynamic access tracking tools are not adequate or
efficient enough to mitigate this threat.

in this paper, we propose unisan, a novel, compiler-based ap-
proach to eliminate all information leaks caused by uninitialized
read in the os kernel. unisan achieves this goal using byte-level,
flow-sensitive, context-sensitive, and field-sensitive initialization
analysis and reachability analysis to check whether an allocation has
been fully initialized when it leaves kernel space; if not, it automati-
cally instruments the kernel to initialize this allocation. unisan’s
analyses are conservative to avoid false negatives and are robust by
preserving the semantics of the os kernel. we have implemented
unisan as passes in llvm and applied it to the latest linux kernel
(x86_64) and android kernel (aarch64). our evaluation showed
that unisan can successfully prevent 43 known and many new unini-
tialized data leak vulnerabilities. further, 19 new vulnerabilities in
the latest kernels have been confirmed by linux and google. our
extensive performance evaluation with lmbench, apachebench,
android benchmarks, and the spec benchmarks also showed that
unisan imposes a negligible performance overhead.

keywords
kernel information leak; uninitialized read; reachability analysis;
initialization analysis; memory initialization

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. copyrights for components of this work owned by others than acm
must be honored. abstracting with credit is permitted. to copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c⃝ 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978366

1.

|
Data,|abstract
eye tracking devices have recently become increasingly pop-
ular as an interface between people and consumer-grade elec-
tronic devices. due to the fact that human eyes are fast, re-
sponsive, and carry information unique to an individual, an-
alyzing person’s gaze is particularly attractive for eﬀortless
biometric authentication. unfortunately, previous propos-
als for gaze-based authentication systems either suﬀer from
high error rates, or require long authentication times.

we build upon the fact that some eye movements can be
reﬂexively and predictably triggered, and develop an interac-
tive visual stimulus for elicitation of reﬂexive eye movements
that supports the extraction of reliable biometric features in
a matter of seconds, without requiring any memorization or
cognitive eﬀort on the part of the user. as an important ben-
eﬁt, our stimulus can be made unique for every authentica-
tion attempt and thus incorporated in a challenge-response
biometric authentication system. this allows us to prevent
replay attacks, which are possibly the most applicable attack
vectors against biometric authentication.

using a gaze tracking device, we build a prototype of our
system and perform a series of systematic user experiments
with 30 participants from the general public. we investigate
the performance and security guarantees under several dif-
ferent attack scenarios and show that our system surpasses
existing gaze-based authentication methods both in achieved
equal error rates (6.3%) and signiﬁcantly lower authentica-
tion times (5 seconds).

1.

|
Data,|abstract
despite the security community’s best eﬀort, the number
of serious vulnerabilities discovered in software is increasing
rapidly. in theory, security audits should ﬁnd and remove
the vulnerabilities before the code ever gets deployed. how-
ever, due to the enormous amount of code being produced,
as well as a the lack of manpower and expertise, not all code
is suﬃciently audited. thus, many vulnerabilities slip into
production systems. a best-practice approach is to use a
code metric analysis tool, such as flawﬁnder, to ﬂag poten-
tially dangerous code so that it can receive special attention.
however, because these tools have a very high false-positive
rate, the manual eﬀort needed to ﬁnd vulnerabilities remains
overwhelming.

in this paper, we present a new method of ﬁnding poten-
tially dangerous code in code repositories with a signiﬁcantly
lower false-positive rate than comparable systems. we com-
bine code-metric analysis with metadata gathered from code
repositories to help code review teams prioritize their work.
the paper makes three contributions. first, we conducted
the ﬁrst large-scale mapping of cves to github commits
in order to create a vulnerable commit database. second,
based on this database, we trained a svm classiﬁer to ﬂag
suspicious commits. compared to flawﬁnder, our approach
reduces the amount of false alarms by over 99 % at the same
level of recall. finally, we present a thorough quantitative
and qualitative analysis of our approach and discuss lessons
learned from the results. we will share the database as
a benchmark for future research and will also provide our
analysis tool as a web service.

categories and subject descriptors
d.2.4 [software engineering]: software/program veriﬁ-
cation; k.6.5 [management of computing and infor-
mation systems]: security and protection

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.
c(cid:13) 2015 acm. isbn 978-1-4503-3832-5/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2810103.2813604.

keywords
vulnerabilities; static analysis; machine learning

1.

|
Non-data,|abstract
the ubiquity of modern smartphones means that nearly ev-
eryone has easy access to a camera at all times.
in the
event of a crime, the photographic evidence that these cam-
eras leave in a smartphone’s memory becomes vital pieces of
digital evidence, and forensic investigators are tasked with
recovering and analyzing this evidence. unfortunately, few
existing forensics tools are capable of systematically recov-
ering and inspecting such in-memory photographic evidence
produced by smartphone cameras. in this paper, we present
vcr, a memory forensics technique which aims to ﬁll this
void by enabling the recovery of all photographic evidence
produced by an android device’s cameras. by leveraging
key aspects of the android framework, vcr extends existing
memory forensics techniques to improve vendor-customized
android memory image analysis. based on this, vcr tar-
gets application-generic artifacts in an input memory image
which allow photographic evidence to be collected no matter
which application produced it. further, vcr builds upon the
android framework’s existing image decoding logic to both
automatically recover and render any located evidence. our
evaluation with commercially available smartphones shows
that vcr is highly eﬀective at recovering all forms of photo-
graphic evidence produced by a variety of applications across
several diﬀerent android platforms.

categories and subject descriptors
d.4.6 [operating systems]: security and protection

general terms
security

keywords
memory forensics; android; digital forensics

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.
copyright is held by the owner/author(s). publication rights licensed to acm.
c(cid:13) 2015 acm. isbn 978-1-4503-3832-5/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2810103.2813720.

1.

|
Data,|abstract

voice authentication is drawing increasing attention and
becomes an attractive alternative to passwords for mobile
authentication. recent advances in mobile technology fur-
ther accelerate the adoption of voice biometrics in an array
of diverse mobile applications. however, recent studies show
that voice authentication is vulnerable to replay attacks,
where an adversary can spoof a voice authentication system
using a pre-recorded voice sample collected from the vic-
tim. in this paper, we propose voicelive, a practical liveness
detection system for voice authentication on smartphones.
voicelive detects a live user by leveraging the user’s unique
vocal system and the stereo recording of smartphones. in
particular, with the phone closely placed to a user’s mouth,
it captures time-diﬀerence-of-arrival (tdoa) changes in a
sequence of phoneme sounds to the two microphones of the
phone, and uses such unique tdoa dynamic which doesn’t
exist under replay attacks for liveness detection. voicelive
is practical as it doesn’t require additional hardware but
two-channel stereo recording that is supported by virtually
all smartphones. our experimental evaluation with 12 par-
ticipants and diﬀerent types of phones shows that voicelive
achieves over 99% detection accuracy at around 1% equal
error rate (eer). results also show that voicelive is ro-
bust to diﬀerent phone placements and is compatible to dif-
ferent sampling rates and phone models.

keywords
voice recognition; liveness detection; phoneme localization

1.

|
Data,|abstract
performing detailed forensic analysis of real-world web security
incidents targeting users, such as social engineering and phishing
attacks, is a notoriously challenging and time-consuming task. to
reconstruct web-based attacks, forensic analysts typically rely on
browser cache ﬁles and system logs. however, cache ﬁles and logs
provide only sparse information often lacking adequate detail to
reconstruct a precise view of the incident.

to address this problem, we need an always-on and lightweight
(i.e., low overhead) forensic data collection system that can be eas-
ily integrated with a variety of popular browsers, and that allows
for recording enough detailed information to enable a full recon-
struction of web security incidents, including phishing attacks.

to this end, we propose webcapsule, a novel record and replay
forensic engine for web browsers. webcapsule functions as an
always-on system that aims to record all non-deterministic inputs
to the core web rendering engine embedded in popular browsers,
including all user interactions with the rendered web content, web
trafﬁc, and non-deterministic signals and events received from the
runtime environment. at the same time, webcapsule aims to be
lightweight and introduce low overhead. in addition, given a previ-
ously recorded trace, webcapsule allows a forensic analyst to fully
replay and analyze past web browsing sessions in a controlled iso-
lated environment.

we design webcapsule to also be portable, so that it can be
integrated with minimal or no changes into a variety of popular
web-rendering applications and platforms. to achieve this goal,
we build webcapsule as a self-contained instrumented version of
google’s blink rendering engine and its tightly coupled v8 java-
script engine.

we evaluate webcapsule on numerous real-world phishing at-
tack instances, and demonstrate that such attacks can be recorded
and fully replayed.
in addition, we show that webcapsule can
record complex browsing sessions on popular websites and differ-
ent platforms (e.g., linux and android) while imposing reasonable
overhead, thus making always-on recording practical.

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’15, october 12–16, 2015, denver, colorado, usa.
c(cid:13) 2015 acm. isbn 978-1-4503-3832-5/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2810103.2813656.

categories and subject descriptors
c.2.0 [computer-communication networks]: general—secu-
rity and protection

general terms
security; forensics

keywords
forensic engine; web security; browsing replay

1.

|
Data,|abstract
the security of order-revealing encryption (ore) has been
unclear since its invention. dataset characteristics for which
ore is especially insecure have been identiﬁed, such as small
message spaces and low-entropy distributions. on the other
hand, properties like one-wayness on uniformly-distributed
datasets have been proved for ore constructions.

this work shows that more plaintext information can be
extracted from ore ciphertexts than was previously thought.
we identify two issues: first, we show that when multi-
ple columns of correlated data are encrypted with ore,
attacks can use the encrypted columns together to reveal
more information than prior attacks could extract from the
columns individually. second, we apply known attacks, and
develop new attacks, to show that the leakage of concrete
ore schemes on non-uniform data leads to more accurate
plaintext recovery than is suggested by the security theorems
which only dealt with uniform inputs.

keywords
database encryption; order-revealing encryption; inference
attacks

1.

|
Data,|abstract
in this study, we present windtalker, a novel and practi-
cal keystroke inference framework that allows an attacker
to infer the sensitive keystrokes on a mobile device through
wifi-based side-channel information. windtalker is moti-
vated from the observation that keystrokes on mobile devices
will lead to diﬀerent hand coverage and the ﬁnger motions,
which will introduce a unique interference to the multi-path
signals and can be reﬂected by the channel state informa-
tion (csi). the adversary can exploit the strong correlation
between the csi ﬂuctuation and the keystrokes to infer the
user’s number input. windtalker presents a novel approach
to collect the target’s csi data by deploying a public wifi
hotspot. compared with the previous keystroke inference
approach, windtalker neither deploys external devices close
to the target device nor compromises the target device. in-
stead, it utilizes the public wifi to collect user’s csi data,
which is easy-to-deploy and diﬃcult-to-detect. in addition,
it jointly analyzes the traﬃc and the csi to launch the
keystroke inference only for the sensitive period where pass-
word entering occurs. windtalker can be launched without
the requirement of visually seeing the smart phone user’s in-
put process, backside motion, or installing any malware on
the tablet. we implemented windtalker on several mobile
phones and performed a detailed case study to evaluate the
practicality of the password inference towards alipay, the
largest mobile payment platform in the world. the evalua-
tion results show that the attacker can recover the key with
a high successful rate.

keywords
password inference; channel state information; online pay-
ment; wireless security; traﬃc analysis

∗

corresponding author, email: zhu-hj@cs.sjtu.edu.cn

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
ccs’16, october 24-28, 2016, vienna, austria
c(cid:2) 2016 acm. isbn 978-1-4503-4139-4/16/10. . . $15.00
doi: http://dx.doi.org/10.1145/2976749.2978397

1.

|
Non-data,|abstract
if someone has the ability to take control of a botnet, can
they just clean up all the infected hosts? can we deceive
users, if our goal is to better understand how they are de-
ceived by attackers? can we demonstrate the need for better
methods, by breaking something that people rely on today?
to be eﬀective, we must ﬁnd ways to balance societal needs
and ethical issues surrounding our research, lest we drift to
the extremes—becoming the very thing we deplore, or ced-
ing the internet to the miscreants because we fear to act.
in this paper, we advocate for a community dialogue on the
ethical issues in computer security and the ethical standards
that we intend to enforce as a community.

1.

|
Non-data,|abstract

threats propagate randomly,

the internet today is beset with constant attacks tar-
geting users and infrastructure. one popular method of
detecting these attacks and the infected hosts behind them
is to monitor unused network addresses. because many
internet
infection attempts
can be captured by monitoring the unused spaces be-
tween live addresses. sensors that monitor these unused
address spaces are called darknets, network telescopes, or
blackholes. they capture important information about a
diverse range of threats such as internet worms, denial of
services attacks, and botnets. in this paper, we describe
and analyze the important measurement issues associated
with deploying darknets, evaluating the placement and
service conﬁguration of darknets, and analyzing the data
collected by darknets. to support the discussion, we lever-
age 4 years of experience operating the internet motion
sensor (ims), a network of distributed darknet sensors
monitoring 60 distinct address blocks in 19 organizations
over 3 continents.

i. |
Data,|abstract

persistently saturated links are abnormal conditions that indicate bottlenecks in internet traﬃc. network operators are interested
in detecting such links for troubleshooting, to improve capacity planning and traﬃc estimation, and to detect denial-of-service
attacks. currently bottleneck links can be detected either locally, through snmp information, or remotely, through active probing
or passive ﬂow-based analysis. however, local snmp information may not be available due to administrative restrictions, and
existing remote approaches are not used systematically because of their network or computation overhead. this paper proposes a
new approach to remotely detect the presence of bottleneck links using spectral and statistical analysis of traﬃc. our approach is
passive, operates on aggregate traﬃc without ﬂow separation, and supports remote detection of bottlenecks, addressing some of the
major limitations of existing approaches. our technique assumes that traﬃc through the bottleneck is dominated by packets with
a common size (typically the maximum transfer unit, for reasons discussed in section 5.1). with this assumption, we observe that
bottlenecks imprint periodicities on packet transmissions based on the packet size and link bandwidth. such periodicities manifest
themselves as strong frequencies in the spectral representation of the aggregate traﬃc observed at a downstream monitoring point.
we propose a detection algorithm based on rigorous statistical methods to detect the presence of bottleneck links by examining
strong frequencies in aggregate traﬃc. we use data from live internet traces to evaluate the performance of our algorithm under
various network conditions. results show that with proper parameters our algorithm can provide excellent accuracy (up to 95%)
even if the traﬃc through the bottleneck link accounts for less than 10% of the aggregate traﬃc.

key words: spectral analysis, bottleneck detection, traﬃc analysis

1. |
Non-data,|abstract. a key way in which banks mitigate the eﬀects of phishing is to re-
move fraudulent websites or suspend abusive domain names. this ‘take-down’ is
often subcontracted to specialist ﬁrms. prior work has shown that these take-down
companies refuse to share ‘feeds’ of phishing website urls with each other, and
consequently, many phishing websites are not removed because the ﬁrm with the
take-down contract remains unaware of their existence. the take-down compa-
nies are reticent to exchange feeds, fearing that competitors with less compre-
hensive lists might ‘free-ride’ oﬀ their eﬀorts by not investing resources to ﬁnd
new websites, as well as use the feeds to poach clients. in this paper, we propose
the phish market protocol, which enables companies with less comprehensive
feeds to learn about websites impersonating their own clients that are held by
other ﬁrms. the protocol is designed so that the contributing ﬁrm is compensated
only for those websites aﬀecting its competitor’s clients and only those previ-
ously unknown to the receiving ﬁrm. crucially, the protocol does not reveal to
the contributing ﬁrm which urls are needed by the receiver, as this is viewed as
sensitive information by take-down ﬁrms. using complete lists of phishing urls
obtained from two large take-down companies, our elliptic-curve-based imple-
mentation added a negligible average 5 second delay to securely share urls.

1

|
Data,|abstract—this paper develops two parametric methods
to detect low-rate denial-of-service attacks and other simi-
lar near-periodic trafﬁc, without the need for ﬂow separa-
tion. the ﬁrst method, the periodic attack detector, is based
on a previous approach that exploits the near-periodic
nature of attack trafﬁc in aggregate trafﬁc by modeling the
peak frequency in the trafﬁc spectrum. the new method
adopts simple statistical models for attack and background
trafﬁc in the time-domain. both approaches use sequential
probability ratio tests (sprts), allowing control over false
alarm rate while examining the trade-off between detection
time and attack strength. we evaluate these methods with
real and synthetic traces, observing that the new poisson-
based scheme uniformly detects attacks more rapidly, often
in less than 200ms, and with lower complexity than the
periodic attack detector. current entropy-based detection
methods provide an equivalent
time to detection but
require ﬂow-separation since they utilize source/destination
ip addresses. we evaluate sensitivity to attack strength
(compared to the rate of background trafﬁc) with synthetic
traces, ﬁnding that the new approach can detect attacks
that represent only 10% of the total trafﬁc bitrate in
fractions of a second.

i. |
Data,|abstract-this 
extraction 
dimension 
series. 
is considered 
attacks. 
transforming 
variance dimension 
stationarity 
are extracted 
extracted 
set of features 
trajectory 
attack when the denoised 
this technique 
variance 
in 
finding  changing  patterns 
of a data series due to the 
presence 
is not dependent 
measurement 
technique 
windows in a highly non stationary 

to remove high variability 
noise. the 
of an 
the presence 

dimensions 
in data series. 

of noise and denial of service 

attack because it 
and mono-scale 

and locally stationary 

fractal dimension. 

shows increasing 

of the trajectory 

of variations 

then features 

data series. 

dimension 

estimates 

there are various 

where dimensions 

the complexity 
of an 
to 

multifractal 
object in a non-integer 
monoscale  analysis 
integers 
only. the concept of non-integer 
arises due to the power law relationship 
samples. 
from a practical 
method [3], the katz-servcik 
fluctuation 
analysis 
method that 
fractal 
measures 
scale for a 
given data series [7]. in this work, we utilized 
fractal 
to characterize 
data series for the identification 
attack. 

analysis 
is another 
at multiple 

(vfd) estimate 
the change in variance 

analysis [5], power spectral 

of the presence 
of an 

and critical 

dimension 

estimates 

dimension 

estimation 
aspect, 

keeping statistical 

of stationarity 

of a data series 

is superior 

of fractal 

moreover, 
this 

indicates 

exponent 

on integer 

in the data 

adaptive 

provides 

a dns count 

is another 
of fractal features 

density 
[6]. variance 

that the rate of change of variance 

method [4], detrended 

dimension 

dimension 

variance 

time series such as the box-counting 

keywords-cognitive machine learning, chaos, fractal, 
dns, ddos amplification, 
internet, anomaly detection, 
cyber threats, data traffic, multifractal, 
dimension, wavelet, haar function, 
sationary, 
series analysis. 

change detection, 
non 
window, time 

trend analysis, adaptive  sliding 

variance fractal 

i. |
Data,|abstract
bitcoin is a purely online virtual currency, unbacked by either phys-
ical commodities or sovereign obligation; instead, it relies on a
combination of cryptographic protection and a peer-to-peer proto-
col for witnessing settlements. consequently, bitcoin has the un-
intuitive property that while the ownership of money is implicitly
anonymous, its ﬂow is globally visible. in this paper we explore
this unique characteristic further, using heuristic clustering to group
bitcoin wallets based on evidence of shared authority, and then us-
ing re-identiﬁcation attacks (i.e., empirical purchasing of goods and
services) to classify the operators of those clusters. from this anal-
ysis, we characterize longitudinal changes in the bitcoin market,
the stresses these changes are placing on the system, and the chal-
lenges for those seeking to use bitcoin for criminal or fraudulent
purposes at scale.
categories and subject descriptors
k.4.4 [electronic commerce]: payment schemes
keywords
bitcoin; measurement; anonymity
1.

|
Data,|abstract
the large-scale collection and exploitation of personal infor-
mation to drive targeted online advertisements has raised
privacy concerns. as a step towards understanding these
concerns, we study the relationship between how much in-
formation is collected and how valuable it is for advertising.
we use http traces consisting of millions of users to aid our
study and also present the ﬁrst comparative study between
aggregators. we develop a simple model that captures the
various parameters of today’s advertising revenues, whose
values are estimated via the traces. our results show that
per aggregator revenue is skewed (5% accounting for 90%
of revenues), while the contribution of users to advertising
revenue is much less skewed (20% accounting for 80% of
revenue). google is dominant in terms of revenue and reach
(presence on 80% of publishers). we also show that if all
5% of the top users in terms of revenue were to install pri-
vacy protection, with no corresponding reaction from the
publishers, then the revenue can drop by 30%.

categories and subject descriptors
h.1.0 [models and principles]: general

general terms
economics, measurement

keywords
advertising, privacy, cpm, do-not-track, publishers, ag-
gregators

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
imc’13, october 23–25, 2013, barcelona, spain.
copyright 2013 acm 978-1-4503-1953-9/13/10 ...$15.00.
http://dx.doi.org/10.1145/2504730.2504768.

dina papagiannaki,

pablo rodriguez
telefonica research

dina@tid.es,
pablorr@tid.es

1.

|
Data,|abstract

middleboxes such as ﬁrewalls, nat, proxies, or deep pack-
et inspection play an increasingly important role in various
types of ip networks, including enterprise and cellular net-
works. recent studies have shed the light on their impact on
real traﬃc and the complexity of managing them. network
operators and researchers have few tools to understand the
impact of those boxes on any path. in this paper, we pro-
pose tracebox, an extension to the widely used traceroute
tool, that is capable of detecting various types of middle-
box interference over almost any path. tracebox sends ip
packets containing tcp segments with diﬀerent ttl values
and analyses the packet encapsulated in the returned icmp
messages. further, as recent routers quote, in the icmp
message, the entire ip packet that they received, tracebox
is able to detect any modiﬁcation performed by upstream
middleboxes. in addition, tracebox can often pinpoint the
network hop where the middlebox interference occurs. we
evaluate tracebox with measurements performed on plan-
etlab nodes. our analysis reveals various types of mid-
dleboxes that were not expected on such an experimental
testbed supposed to be connected to the internet without
any restriction.

categories and subject descriptors

c.2.3 [network operations]: network monitoring

keywords

network discovery, middleboxes, tracebox

1.

|
Data,|abstract
impediments to resolving ipv6 router aliases have precluded
understanding the emerging router-level ipv6 internet topol-
ogy. in this work, we design, implement, and validate the
ﬁrst internet-scale alias resolution technique for ipv6. our
technique, speedtrap,
leverages the ability to induce frag-
mented ipv6 responses from router interfaces in a particu-
lar temporal pattern that produces distinguishing per-router
ﬁngerprints. our algorithm surmounts three fundamental
challenges to internet-scale ipv6 alias resolution using frag-
ment identiﬁer values: (1) unlike for ipv4, the identiﬁer
counters on ipv6 routers have no natural velocity, (2) the
values of these counters are similar across routers, and (3)
the packet size required to collect inferences is 46 times
larger than required in ipv4. we demonstrate the eﬃcacy
of the technique by producing router-level internet ipv6
topologies using measurements from caida’s distributed
infrastructure. our preliminary work represents a step to-
ward understanding the internet’s ipv6 router-level topol-
ogy, an important objective with respect to ipv6 network
resilience, security, policy, and longitudinal evolution.

categories and subject descriptors
c.2.5 [local and wide-area networks]: internet; c.2.1
[network architecture and design]: network topology;
c.2.3 [computer communication networks]: network
operations—network monitoring

keywords
internet topology; alias resolution; ipv6

1.

|
Data,|abstract
we report the results of a study to collect and analyze ipv6 internet
background radiation. this study, the largest of its kind, collects
unclaimed trafﬁc on the ipv6 internet by announcing ﬁve large /12
covering preﬁxes; these cover the majority of allocated ipv6 space
on today’s internet. our analysis characterizes the nature of this
trafﬁc across regions, over time, and by the allocation and routing
status of the intended destinations, which we show help to identify
the causes of this trafﬁc. we compare results to unclaimed trafﬁc in
ipv4, and highlight case studies that explain a large fraction of the
data or highlight notable properties. we describe how announced
covering preﬁxes differ from traditional network telescopes, and
show how this technique can help both network operators and the
research community identify additional potential issues and mis-
conﬁgurations in this critical internet transition period.

categories and subject descriptors
c.2.3 [computer-communication networks]: network opera-
tions—network management, network monitoring

keywords
ipv6; darknet; routing; network pollution; internet background
radiation; measurement

1.

|
Data,|abstract
spammers register a tremendous number of domains to evade
blacklisting and takedown efforts. current techniques to detect
such domains rely on crawling spam urls or monitoring lookup
trafﬁc. such detection techniques are only effective after the spam-
mers have already launched their campaigns, and thus these coun-
termeasures may only come into play after the spammer has already
reaped signiﬁcant beneﬁts from the dissemination of large volumes
of spam. in this paper we examine the registration process of such
domains, with a particular eye towards features that might indicate
that a given domain likely has a malicious purpose at registration
time, before it is ever used for an attack. our assessment includes
exploring the characteristics of registrars, domain life cycles, regis-
tration bursts, and naming patterns. by investigating zone changes
from the .com tld over a 5-month period, we discover that spam-
mers employ bulk registration, that they often re-use domains pre-
viously registered by others, and that they tend to register and host
their domains over a small set of registrars. our ﬁndings suggest
steps that registries or registrars could use to frustrate the efforts
of miscreants to acquire domains in bulk, ultimately reducing their
agility for mounting large-scale attacks.

categories and subject descriptors
c.2.3 [computer-communication networks]: network oper-
ations—network monitoring; k.6.5 [security and protection];
k.4.1 [computers and society]: public policy issues—abuse and
crime involving computers

general terms
measurement, security

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. copyrights for components of this work owned by others than
acm must be honored. abstracting with credit is permitted. to copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
imc’13, october 23–25, 2013, barcelona, spain.
copyright 2013 acm 978-1-4503-1953-9/13/10 ...$15.00.
http://dx.doi.org/10.1145/2504730.2504753.

keywords
dns; domain registration; spam; blacklist

1.

|
Data,|abstract
black hat search engine optimization (seo), the practice of abu-
sively manipulating search results, is an enticing method to acquire
targeted user trafﬁc. in turn, a range of interventions—from mod-
ifying search results to seizing domains—are used to combat this
activity. in this paper, we examine the effectiveness of these inter-
ventions in the context of an understudied market niche, counterfeit
luxury goods. using eight months of empirical crawled data, we
identify 52 distinct seo campaigns, document how well they are
able to place search results for sixteen luxury brands, how this ca-
pability impacts the dynamics of their order volumes and how well
existing interventions undermine this business when employed.

1.

|
Data,|abstract

threats to the privacy of users and to the availability of
internet infrastructure are evolving at a tremendous rate.
to characterize these emerging threats, researchers must
effectively balance monitoring the large number of hosts
needed to quickly build conﬁdence in new attacks, while
still preserving the detail required to differentiate these at-
tacks. one class of techniques that attempts to achieve
this balance involves hybrid systems that combine the scal-
able monitoring of unused address blocks (or darknets)
with forensic honeypots (or honeyfarms). in this paper we
examine the properties of individual and distributed dark-
nets to determine the effectiveness of building scalable hy-
brid systems. we show that individual darknets are dom-
inated by a small number of sources repeating the same
actions. this enables source-based techniques to be effec-
tive at reducing the number of connections to be evaluated
by over 90%. we demonstrate that the dominance of lo-
cally targeted attack behavior and the limited life of ran-
dom scanning hosts result in few of these sources being
repeated across darknets. to achieve reductions beyond
source-based approaches, we look to source-distribution
based methods and expand them to include notions of lo-
cal and global behavior. we show that this approach is
effective at reducing the number of events by deploying
it in 30 production networks during early 2005. each of
the identiﬁed events during this period represented a ma-
jor globally-scoped attack including the wins vulnerabil-
ity scanning, veritas backup agent vulnerability scanning,
and the mysql worm.

1

|
Data,|abstract
network-wide activity is when one computer (the origina-
tor ) touches many others (the targets). motives for activity
may be benign (mailing lists, cdns, and research scanning),
malicious (spammers and scanners for security vulnerabili-
ties), or perhaps indeterminate (ad trackers). knowledge
of malicious activity may help anticipate attacks, and un-
derstanding benign activity may set a baseline or charac-
terize growth. this paper identiﬁes dns backscatter as
a new source of information about network-wide activity.
backscatter is the reverse dns queries caused when tar-
gets or middleboxes automatically look up the domain name
of the originator. queries are visible to the authoritative
dns servers that handle reverse dns. while the fraction of
backscatter they see depends on the server’s location in the
dns hierarchy, we show that activity that touches many tar-
gets appear even in sampled observations. we use informa-
tion about the queriers to classify originator activity using
machine-learning. our algorithm has reasonable precision
(70–80%) as shown by data from three diﬀerent organiza-
tions operating dns servers at the root or country-level.
using this technique we examine nine months of activity
from one authority to identify trends in scanning, identify-
ing bursts corresponding to heartbleed and broad and con-
tinuous scanning of ssh.

categories and subject descriptors
c.2.3 [computer-communication networks]: network
operations—network monitoring

general terms
measurement

keywords
internet; domain name system; dns; network activity;
scanning

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
imc’15, october 28–30, 2015, tokyo, japan.
copyright is held by the owner/author(s). publication rights licensed to acm.
acm 978-1-4503-3848-6/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2815675.2815706.

1.

|
Data,|abstract
active probing techniques, such as ping, have been used to
detect outages. when a previously responsive end host fails
to respond to a probe, studies sometimes attempt to conﬁrm
the outage by retrying the ping or attempt to identify the
location of the outage by using other tools such as tracer-
oute. the latent problem, however, is, how long should one
wait for a response to the ping? too short a timeout risks
confusing congestion or other delay with an outage. too
long a timeout may slow the process and prevent observ-
ing and diagnosing short-duration events, depending on the
experiment’s design.

we believe that conventional timeouts for active probes
are underestimates, and analyze data collected by heide-
mann et al.
in 2006–2015. we ﬁnd that 5% of pings from
5% of addresses take more than 5 seconds. put another way,
for 5% of the responsive ip addresses probed by heidemann,
a false 5% loss rate would be inferred if using a timeout of 5
seconds. to arrive at this observation, we ﬁltered artifacts
of the data that could occur with too-long a timeout, includ-
ing responses to probes sent to broadcast addresses. we also
analyze icmp data collected by zmap in 2015 to ﬁnd that
around 5% of all responsive addresses observe a greater than
one second round-trip time consistently. further, the preva-
lence of high round trip time has been increasing and it is
often associated with the ﬁrst ping, perhaps due to negotiat-
ing a wireless connection. in addition, we ﬁnd that the au-
tonomous systems with the most high-latency addresses are
typically cellular. this paper describes our analysis process
and results that should encourage researchers to set longer
timeouts when needed and report on timeout settings in the
description of future measurements.

categories and subject descriptors
c.2.5 [computer-communication networks]: local and
widearea networks—internet

permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. copyrights for components of this work owned by others than the
author(s) must be honored. abstracting with credit is permitted. to copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. request permissions from permissions@acm.org.
imc’15, october 28–30, 2015, tokyo, japan.
copyright is held by the owner/author(s). publication rights licensed to acm.
acm 978-1-4503-3848-6/15/10 ...$15.00.
doi: http://dx.doi.org/10.1145/2815675.2815704.

keywords
timeouts; icmp echo; ping; outages; reachability; out-
age detection; maximum segment lifetime

1.

|
Data,|abstract drive-by downloads are the preferred distrib-
ution vector for many malware families. in the drive-by
ecosystem, many exploit servers run the same exploit kit and
it is a challenge understanding whether the exploit server
is part of a larger operation. in this paper, we propose a
technique to identify exploit servers managed by the same
organization. we collect over time how exploit servers are
conﬁgured, which exploits they use, and what malware they
distribute, grouping servers with similar conﬁgurations into
operations. our operational analysis reveals that although
individual exploit servers have a median lifetime of 16 h,
long-lived operations exist that operate for several months.
to sustain long-lived operations, miscreants are turning to
the cloud, with 60 % of the exploit servers hosted by special-
ized cloud hosting services. we also observe operations that
distribute multiple malware families and that pay-per-install
afﬁliate programs are managing exploit servers for their afﬁl-
iates to convert trafﬁc into installations. furthermore, we
analyze the exploit polymorphism problem, measuring the
repacking rate for different exploit types. to understand how
difﬁcult is to takedown exploit servers, we analyze the abuse
reporting process and issue abuse reports for 19 long-lived
servers. we describe the interaction with isps and hosting
providers and monitor the result of the report. we ﬁnd that
a. nappa (b) · j. caballero

imdea software institute, madrid, spain
e-mail: antonio.nappa@imdea.org

j. caballero
e-mail: juan.caballero@imdea.org

a. nappa
universidad politécnica de madrid, madrid, spain

m. z. raﬁque
iminds-distrinet, ku leuven, leuven, belgium
e-mail: zubair.raﬁque@cs.kuleuven.be

61 % of the reports are not even acknowledged. on aver-
age, an exploit server still lives for 4.3 days after a report.
finally, we detail the malicia dataset we have collected and
are making available to other researchers.
keywords drive-by download operations · malicia
dataset · malware distribution · cybercrime

1 |
Data,|abstract
even though human movement and mobility patterns have a high
degree of freedom and variation, they also exhibit structural pat-
terns due to geographic and social constraints. using cell phone
location data, as well as data from two online location-based social
networks, we aim to understand what basic laws govern human mo-
tion and dynamics. we ﬁnd that humans experience a combination
of periodic movement that is geographically limited and seemingly
random jumps correlated with their social networks. short-ranged
travel is periodic both spatially and temporally and not effected by
the social network structure, while long-distance travel is more in-
ﬂuenced by social network ties. we show that social relationships
can explain about 10% to 30% of all human movement, while pe-
riodic behavior explains 50% to 70%. based on our ﬁndings, we
develop a model of human mobility that combines periodic short
range movements with travel due to the social network structure.
we show that our model reliably predicts the locations and dynam-
ics of future human movement and gives an order of magnitude
better performance than present models of human mobility.
categories and subject descriptors: h.2.8 [database manage-
ment]: database applications – data mining
general terms: algorithms, theory, experimentation.
keywords: human mobility, communication networks, social net-
works.
1.

|
Data,|abstract

malicious code, or malware, executed on compro-
mised hosts provides a platform for a wide variety of
attacks against the availability of the network and the
privacy and conﬁdentiality of its users. unfortunately,
the most popular techniques for detecting and pre-
venting malware have been shown to be signiﬁcantly
ﬂawed [11], and it is widely believed that a signiﬁcant
fraction of the internet consists of malware infected
machines [17]. in response, defenders have turned to
coarse-grained, reputation-based techniques, such as
real time blackhole lists, for blocking large numbers
of potentially malicious hosts and network blocks. in
this paper, we perform a preliminary study of a type of
reputation-based blacklist, namely those used to block
unsolicited email, or spam. we show that, for the net-
work studied, these blacklists exhibit non-trivial false
positives and false negatives. we investigate a number
of possible causes for this low accuracy and discuss
the implications for other types of reputation-based
blacklists.

1 |
Data,|abstract—location check-ins contain both geographical and
semantic information about the visited venues, in the form of
tags (e.g., “restaurant”). such data might reveal some personal
information about users beyond what they actually want to
disclose, hence their privacy is threatened. in this paper, we study
users’ motivations behind location check-ins, and we quantify the
effect of a privacy-preserving technique (i.e., generalization) on
the perceived utility of check-ins. by means of a targeted user-
study on foursquare (n = 77), we show that the motivation behind
foursquare check-ins is a mediator of the loss of utility caused
by generalization. using these ﬁndings, we propose a machine-
learning method for determining the motivation behind each
check-in, and we design a motivation-based predictive model for
utility. our results show that the model accurately predicts the
loss of utility caused by semantic and geographical generalization;
this model enables the design of utility-aware, privacy-enhancing
mechanisms in location-based social networks.

i.

|
Data,|abstract

as national

infrastructure becomes intertwined with
emerging global data networks, the stability and integrity of
the two have become synonymous. this connection, while
necessary, leaves network assets vulnerable to the rapidly
moving threats of today’s internet, including fast moving
worms, distributed denial of service attacks, and routing
exploits. this paper introduces the internet motion sen-
sor (ims), a globally scoped internet monitoring system
whose goal is to measure, characterize, and track threats.
the ims architecture is based on three novel components.
first, a distributed monitoring infrastructure increases vis-
ibility into global threats. second, a lightweight active
responder provides enough interactivity that trafﬁc on the
same service can be differentiated independent of applica-
tion semantics. third, a payload signatures and caching
mechanism avoids recording duplicated payloads, reducing
overhead and assisting in identifying new and unique pay-
loads. we explore the architectural tradeoffs of this system
in the context of a 3 year deployment across multiple dark
address blocks ranging in size from /24s to a /8. these sen-
sors represent a range of organizations and a diverse sam-
ple of the routable ipv4 space including nine of all routable
/8 address ranges. data gathered from these deployments
is used to demonstrate the ability of the ims to capture and
characterize several important internet threats: the blaster
worm (august 2003), the bagle backdoor scanning efforts
(march 2004), and the sco denial of service attacks (de-
cember 2003).

1

|
Data,|abstract
we report the results of a large-scale measurement study of the
https certiﬁcate ecosystem—the public-key infrastructure that un-
derlies nearly all secure web communications. using data collected
by performing 110 internet-wide scans over 14 months, we gain
detailed and temporally ﬁne-grained visibility into this otherwise
opaque area of security-critical infrastructure. we investigate the
trust relationships among root authorities, intermediate authorities,
and the leaf certiﬁcates used by web servers, ultimately identify-
ing and classifying more than 1,800 entities that are able to issue
certiﬁcates vouching for the identity of any website. we uncover
practices that may put the security of the ecosystem at risk, and we
identify frequent conﬁguration problems that lead to user-facing
errors and potential vulnerabilities. we conclude with lessons and
recommendations to ensure the long-term health and security of the
certiﬁcate ecosystem.

categories and subject descriptors
c.2.2 [computer-communication networks]: [network proto-
cols]; c.2.3 [computer-communication networks]: [network
operations]; e.3 [data encryption]: [public key cryptosystems,
standards]

keywords
tls; ssl; https; public-key infrastructure; x.509; certiﬁcates;
security; measurement; internet-wide scanning

1.

|
Data,|abstract
after several ipv4 address exhaustion milestones in the last three
years, it is becoming apparent that the world is running out of ipv4
addresses, and the adoption of the next generation internet proto-
col, ipv6, though nascent, is accelerating. in order to better un-
derstand this unique and disruptive transition, we explore twelve
metrics using ten global-scale datasets to create the longest and
broadest measurement of ipv6 adoption to date. using this per-
spective, we ﬁnd that adoption, relative to ipv4, varies by two or-
ders of magnitude depending on the measure examined and that
care must be taken when evaluating adoption metrics in isolation.
further, we ﬁnd that regional adoption is not uniform. finally, and
perhaps most surprisingly, we ﬁnd that over the last three years, the
nature of ipv6 utilization—in terms of trafﬁc, content, reliance on
transition technology, and performance—has shifted dramatically
from prior ﬁndings, indicating a maturing of the protocol into pro-
duction mode. we believe ipv6’s recent growth and this changing
utilization signal a true quantum leap.

categories and subject descriptors
c.2.5 [local and wide-area networks]: internet

keywords
internet; ip; ipv4; ipv6; dns; measurement

1.

|
Data,|abstract—the ssl man-in-the-middle attack uses forged ssl
certiﬁcates to intercept encrypted connections between clients
and servers. however, due to a lack of reliable indicators, it is
still unclear how commonplace these attacks occur in the wild. in
this work, we have designed and implemented a method to detect
the occurrence of ssl man-in-the-middle attack on a top global
website, facebook. over 3 million real-world ssl connections
to this website were analyzed. our results indicate that 0.2%
of the ssl connections analyzed were tampered with forged
ssl certiﬁcates, most of them related to antivirus software and
corporate-scale content ﬁlters. we have also identiﬁed some ssl
connections intercepted by malware. limitations of the method
and possible defenses to such attacks are also discussed.
keywords-ssl; certiﬁcates; man-in-the-middle attack;

i. |
Non-data,|abstract

defenders of today’s critical cyber-infrastructure (e.g.,
the internet) are equipped with a wide array of security
techniques including network-based intrusion detection sys-
tems (ids), host-based anti-virus systems (av), and decoy
or reconnaissance systems such as host-based honeypots or
network-based telescopes. while effective at detecting and
mitigating some of the threats posed to critical infrastruc-
ture, the ubiquitous nature of malicious activity (e.g., phish-
ing, spam, ddos) on the internet indicates that the cur-
rent deployments of these tools do not fully live up to their
promise. over the past 10 years our research group has
investigated ways of detecting and stopping cyber-attacks
by using the context available in the network, host, and
the environment. in this paper, we explain what exactly we
mean by context, why it is difﬁcult to measure, and what
one can do with context when it is available. we illustrate
these points by examining several studies in which context
was used to enable or enhance new security techniques. we
conclude with some ideas about the future of context-aware
security.

1 |
Data,|abstract

although the internet is widely used today, there are few
sound estimates of network demographics. decentralized
network management means questions about internet use
cannot be answered by a central authority, and ﬁrewalls and
sensitivity to probing means that active measurements must
be done carefully and validated against known data. build-
ing on frequent icmp probing of 1% of the internet address
space, we develop a clustering algorithm to estimate how in-
ternet addresses are used. we show that adjacent addresses
often have similar characteristics and are used for similar
purposes (61% of addresses we probe are consistent blocks
of 64 neighbors or more). we then apply this block-level
clustering to provide data to explore several open questions
in how networks are managed. first, the nearing full allo-
cation of ipv4 addresses makes it increasingly important to
estimate the costs of better management of the ipv4 space
as a component of an ipv6 transition. we provide about
how effectively network addresses blocks appear to be used,
ﬁnding that a signiﬁcant number of blocks are only lightly
used (about one-ﬁfth of /24 blocks have most addresses in
use less than 10% of the time). second, we provide new
measurements about dynamically managed address space,
showing nearly 40% of /24 blocks appear to be dynamically
allocated, and dynamic addressing is most widely used in
countries more recently to the internet (more than 80% in
china, while less then 30% in the u.s.).

categories and subject descriptors

c.2.1 [computer-communication networks]: net-
work architecture and design—network topology; c.2.3
[computer-communication networks]: network
operations—network management

general terms: measurement

keywords: internet address allocation, survey, pat-
tern analysis, clustering, classiﬁcation, availability, volatil-
ity

1.

|
Data,|abstract

we present new biases in rc4, break the wi-fi protected
access temporal key integrity protocol (wpa-tkip),
and design a practical plaintext recovery attack against
the transport layer security (tls) protocol. to empir-
ically ﬁnd new biases in the rc4 keystream we use sta-
tistical hypothesis tests. this reveals many new biases in
the initial keystream bytes, as well as several new long-
term biases. our ﬁxed-plaintext recovery algorithms are
capable of using multiple types of biases, and return a
list of plaintext candidates in decreasing likelihood.

to break wpa-tkip we introduce a method to gen-
erate a large number of identical packets. this packet is
decrypted by generating its plaintext candidate list, and
using redundant packet structure to prune bad candidates.
from the decrypted packet we derive the tkip mic key,
which can be used to inject and decrypt packets. in prac-
tice the attack can be executed within an hour. we also
attack tls as used by https, where we show how to
decrypt a secure cookie with a success rate of 94% using
9· 227 ciphertexts. this is done by injecting known data
around the cookie, abusing this using mantin’s absab
bias, and brute-forcing the cookie by traversing the plain-
text candidates. using our trafﬁc generation technique,
we are able to execute the attack in merely 75 hours.

1

|
Data,|abstract

as defense solutions against control-ﬂow hijacking at-
tacks gain wide deployment, control-oriented exploits
from memory errors become difﬁcult. as an alterna-
tive, attacks targeting non-control data do not require
diverting the application’s control ﬂow during an at-
tack. although it is known that such data-oriented at-
tacks can mount signiﬁcant damage, no systematic meth-
ods to automatically construct them from memory er-
rors have been developed.
in this work, we develop a
new technique called data-ﬂow stitching, which system-
atically ﬁnds ways to join data ﬂows in the program to
generate data-oriented exploits. we build a prototype
embodying our technique in a tool called flowstitch
that works directly on windows and linux binaries. in
our experiments, we ﬁnd that flowstitch automati-
cally constructs 16 previously unknown and three known
data-oriented attacks from eight real-world vulnerable
programs. all the automatically-crafted exploits respect
ﬁne-grained cfi and dep constraints, and 10 out of the
19 exploits work with standard aslr defenses enabled.
the constructed exploits can cause signiﬁcant damage,
such as disclosure of sensitive information (e.g., pass-
words and encryption keys) and escalation of privilege.

1

|
Data,|abstract
source code authorship attribution is a signiﬁcant pri-
vacy threat to anonymous code contributors. however,
it may also enable attribution of successful attacks from
code left behind on an infected system, or aid in resolv-
ing copyright, copyleft, and plagiarism issues in the pro-
gramming ﬁelds. in this work, we investigate machine
learning methods to de-anonymize source code authors
of c/c++ using coding style. our code stylometry fea-
ture set is a novel representation of coding style found
in source code that reﬂects coding style from properties
derived from abstract syntax trees.

our random forest and abstract syntax tree-based ap-
proach attributes more authors (1,600 and 250) with sig-
niﬁcantly higher accuracy (94% and 98%) on a larger
data set (google code jam) than has been previously
achieved. furthermore, these novel features are robust,
difﬁcult to obfuscate, and can be used in other program-
ming languages, such as python. we also ﬁnd that (i) the
code resulting from difﬁcult programming tasks is easier
to attribute than easier tasks and (ii) skilled programmers
(who can complete the more difﬁcult tasks) are easier to
attribute than less skilled programmers.

1

|
Data,|abstract

though journalists are often cited as potential users of
computer security technologies, their practices and men-
tal models have not been deeply studied by the academic
computer security community. such an understanding,
however, is critical to developing technical solutions that
can address the real needs of journalists and integrate
into their existing practices. we seek to provide that in-
sight in this paper, by investigating the general and com-
puter security practices of 15 journalists in the u.s. and
france via in-depth, semi-structured interviews. among
our ﬁndings is evidence that existing security tools fail
not only due to usability issues but when they actively in-
terfere with other aspects of the journalistic process; that
communication methods are typically driven by sources
rather than journalists; and that journalists’ organizations
play an important role in inﬂuencing journalists’ behav-
iors. based on these and other ﬁndings, we make recom-
mendations to the computer security community for im-
provements to existing tools and future lines of research.
1
in recent decades, improved digital communication tech-
nologies have reduced barriers to journalism worldwide.
security weaknesses in these same technologies, how-
ever, have put journalists and their sources increasingly
at risk of identiﬁcation, prosecution, and persecution by
powerful entities, threatening efforts in investigative re-
porting, transparency, and whistleblowing.

|
Data,|abstract

cybercriminals misuse accounts on online services (e.g.,
webmails and online social networks) to perform ma-
licious activity, such as spreading malicious content or
stealing sensitive information.
in this paper, we show
that accounts that are accessed by botnets are a popular
choice by cybercriminals. since botnets are composed
of a ﬁnite number of infected computers, we observe that
cybercriminals tend to have their bots connect to multiple
online accounts to perform malicious activity.

we present evilcohort, a system that detects on-
line accounts that are accessed by a common set of in-
fected machines. evilcohort only needs the mapping
between an online account and an ip address to operate,
and can therefore detect malicious accounts on any on-
line service (webmail services, online social networks,
storage services) regardless of the type of malicious ac-
tivity that these accounts perform. unlike previous work,
our system can identify malicious accounts that are con-
trolled by botnets but do not post any malicious content
(e.g., spam) on the service. we evaluated evilcohort
on multiple online services of different types (a webmail
service and four online social networks), and show that
it accurately identiﬁes malicious accounts.

1

|
Data,|abstract
mandatory protection systems such as selinux and se-
android harden operating system integrity. unfortu-
nately, policy development is error prone and requires
lengthy reﬁnement using audit logs from deployed sys-
tems. while prior work has studied selinux policy in
detail, seandroid is relatively new and has received lit-
tle attention. seandroid policy engineering differs sig-
niﬁcantly from selinux: android fundamentally differs
from traditional linux; the same policy is used on mil-
lions of devices for which new audit logs are continu-
ally available; and audit logs contain a mix of benign
and malicious accesses. in this paper, we propose ease-
android, the ﬁrst seandroid analytic platform for auto-
matic policy analysis and reﬁnement. our key insight is
that the policy reﬁnement process can be modeled and
automated using semi-supervised learning. given an ex-
isting policy and a small set of known access patterns,
easeandroid continually expands the knowledge base
as new audit logs become available, producing sugges-
tions for policy reﬁnement. we evaluate easeandroid
on 1.3 million audit logs from real-world devices. ease-
android successfully learns 2,518 new access patterns
and generates 331 new policy rules. during this process,
easeandroid discovers eight categories of attack access
patterns in real devices, two of which are new attacks di-
rectly against the seandroid mac mechanism.

1

|
Data,|abstract
guessability—how many
parameterized
guesses a particular cracking algorithm with particular
training data would take to guess a password—has
become a common metric of password security. unlike
statistical metrics, it aims to model real-world attackers
and to provide per-password strength estimates. we
investigate how cracking approaches often used by
researchers compare to real-world cracking by profes-
sionals, as well as how the choice of approach biases
research conclusions.

we ﬁnd that semi-automated cracking by profession-
als outperforms popular fully automated approaches, but
can be approximated by combining multiple such ap-
proaches. these approaches are only effective, however,
with careful conﬁguration and tuning; in commonly used
default conﬁgurations, they underestimate the real-world
guessability of passwords. we ﬁnd that analyses of large
password sets are often robust to the algorithm used for
guessing as long as it is conﬁgured effectively. however,
cracking algorithms differ systematically in their effec-
tiveness guessing passwords with certain common fea-
tures (e.g., character substitutions). this has important
implications for analyzing the security of speciﬁc pass-
word characteristics or of individual passwords (e.g., in a
password meter or security audit). our results highlight
the danger of relying only on a single cracking algorithm
as a measure of password strength and constitute the ﬁrst
scientiﬁc evidence that automated guessing can often ap-
proximate guessing by professionals.

1

|
Non-data,|abstract

i/o virtualization (sriov)

single root
is a hard-
ware/software interface that allows devices to “self virtu-
alize” and thereby remove the host from the critical i/o
path. sriov thus brings near bare-metal performance to
untrusted guest virtual machines (vms) in public clouds,
enterprise data centers, and high-performance comput-
ing setups. we identify a design ﬂaw in current ethernet
sriov nic deployments that enables untrusted vms to
completely control the throughput and latency of other,
unrelated vms. the attack exploits ethernet ”pause”
frames, which enable network ﬂow control functional-
ity. we experimentally launch the attack across sev-
eral nic models and ﬁnd that it is effective and highly
accurate, with substantial consequences if left unmiti-
gated:
(1) to be safe, nic vendors will have to mod-
ify their nics so as to ﬁlter pause frames originating
from sriov instances; (2) in the meantime, administra-
tors will have to either trust their vms, or conﬁgure their
switches to ignore pause frames, thus relinquishing ﬂow
control, which might severely degrade networking per-
formance. we present the virtualization-aware network
flow controller (vanfc), a software-based sriov nic
prototype that overcomes the attack. vanfc ﬁlters pause
frames from malicious virtual machines without any loss
of performance, while keeping sriov and ethernet ﬂow
control hardware/software interfaces intact.

1

|
Non-data,|abstract

we describe a largely automated and systematic analysis
of tls implementations by what we call ‘protocol state
fuzzing’: we use state machine learning to infer state ma-
chines from protocol implementations, using only black-
box testing, and then inspect the inferred state machines
to look for spurious behaviour which might be an indica-
tion of ﬂaws in the program logic. for detecting the pres-
ence of spurious behaviour the approach is almost fully
automatic: we automatically obtain state machines and
any spurious behaviour is then trivial to see. detecting
whether the spurious behaviour introduces exploitable
security weaknesses does require manual investigation.
still, we take the point of view that any spurious func-
tionality in a security protocol implementation is danger-
ous and should be removed.

we analysed both server- and client-side implemen-
tations with a test harness that supports several key ex-
change algorithms and the option of client certiﬁcate au-
thentication. we show that this approach can catch an
interesting class of implementation ﬂaws that is appar-
ently common in security protocol implementations: in
three of the tls implementations analysed new security
ﬂaws were found (in gnutls, the java secure socket
extension, and openssl). this shows that protocol state
fuzzing is a useful technique to systematically analyse
security protocol implementations. as our analysis of
different tls implementations resulted in different and
unique state machines for each one, the technique can
also be used for ﬁngerprinting tls implementations.

1 |
Data,|abstract
for increased security during tls certiﬁcate valida-
tion, a common recommendation is to use a vari-
ation of pinning. especially non-browser software
developers are encouraged to limit the number of
trusted certiﬁcates to a minimum, since the default
ca-based approach is known to be vulnerable to se-
rious security threats.
the decision for or against pinning is always a trade-
oﬀ between increasing security and keeping mainte-
nance eﬀorts at an acceptable level. in this paper,
we present an extensive study on the applicability
of pinning for non-browser software by analyzing
639,283 android apps. conservatively, we propose
pinning as an appropriate strategy for 11,547 (1.8%)
apps or for 45,247 tls connections (4.25%) in our
sample set. with a more optimistic classiﬁcation of
borderline cases, we propose pinning for considera-
tion for 58,817 (9.1%) apps or for 140,020 (3.8%1)
tls connections. this weakens the assumption that
pinning is a widely usable strategy for tls security
in non-browser software. however, in a nominal-
actual comparison, we ﬁnd that only 45 apps ac-
tually implement pinning. we collected developer
feedback from 45 respondents and learned that only
a quarter of them grasp the concept of pinning, but
still ﬁnd pinning too complex to use. based on their
feedback, we built an easy-to-use web-application
that supports developers in the decision process and
guides them through the correct deployment of a
pinning-protected tls implementation.

1 |
Non-data,|abstract

in a provenance-aware system, mechanisms gather
and report metadata that describes the history of each ob-
ject being processed on the system, allowing users to un-
derstand how data objects came to exist in their present
state. however, while past work has demonstrated the
usefulness of provenance, less attention has been given
to securing provenance-aware systems. provenance it-
self is a ripe attack vector, and its authenticity and in-
tegrity must be guaranteed before it can be put to use.

we present linux provenance modules

(lpm),
the ﬁrst general framework for the development of
provenance-aware systems. we demonstrate that lpm
creates a trusted provenance-aware execution environ-
ment, collecting complete whole-system provenance
while imposing as little as 2.7% performance overhead
on normal system operation. lpm introduces new mech-
anisms for secure provenance layering and authenticated
communication between provenance-aware hosts, and
also interoperates with existing mechanisms to provide
strong security assurances. to demonstrate the poten-
tial uses of lpm, we design a provenance-based data
loss prevention (pb-dlp) system. we implement pb-
dlp as a ﬁle transfer application that blocks the trans-
mission of ﬁles derived from sensitive ancestors while
imposing just tens of milliseconds overhead. lpm is the
ﬁrst step towards widespread deployment of trustworthy
provenance-aware applications.

1

|
Data,|abstract
the monitoring of unused internet address space has been shown to
be an effective method for characterizing internet threats including
internet worms and ddos attacks. because there are no legitimate
hosts in an unused address block, trafﬁc must be the result of mis-
conﬁguration, backscatter from spoofed source addresses, or scan-
ning from worms and other probing. this paper extends previous
work characterizing trafﬁc seen at speciﬁc unused address blocks
by examining differences observed between these blocks. while
past research has attempted to extrapolate the results from a small
number of blocks to represent global internet trafﬁc, we present ev-
idence that distributed address blocks observe dramatically differ-
ent trafﬁc patterns. this work uses a network of blackhole sensors
which are part of the internet motion sensor (ims) collection in-
frastructure. these sensors are deployed in networks belonging to
service providers, large enterprises, and academic institutions rep-
resenting a diverse sample of the ipv4 address space. we demon-
strate differences in trafﬁc observed along three dimensions: over
all protocols and services, over a speciﬁc protocol and service, and
over a particular worm signature. this evidence is then combined
with additional experimentation to build a list of sensor properties
providing plausible explanations for these differences. using these
properties, we conclude with recommendations for better under-
standing the implications of sensor placement.

categories and subject descriptors
d.4.6 [operating systems]: security and protection—invasive
software

general terms
security measurement

keywords
network security, computer worms, globally scoped threats, black-
hole monitoring, blackhole placement, internet motion sensor

permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. to copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
worm’04, october 29, 2004, washington, dc, usa.
copyright 2004 acm 1-58113-970-5/04/0010 ...$5.00.

1.

|
Data,|abstract—in the last decade many works has been done on the 
internet topology  at router  or  autonomous system (as) level. as 
routers is the essential composition of ases while ases dominate 
the  behavior  of  their  routers.  it  is  no  doubt  that  identifying  the 
affiliation  between  routers  and  ases  can  let  us  gain  a  deeper 
understanding  on  the  topology.  however,  the  existing  methods 
that assign a router to an as just based on the origin ases of its ip 
addresses, which does not make full use of information in our hand. 
in this paper, we propose a methodology to assign routers to their 
owner ases based on community discovery tech. first, we use the 
origin  ases  information  along  with  router-pairs  similarities  to 
construct a weighted router level topology; secondly, for enormous 
topology data (more than 2m nodes and 19m edges) from caida 
itdk project, we propose a fast hierarchy clustering which time 
and  space  complex  are  both  linear  to  do  ases  community 
discovery, last we do router-to-as  mapping based on these ases 
communities.  experiments  show  that  combining  with  ases 
communities our methodology discovers, the best accuracy rate of 
router-to-as  mapping  can  reach  to  82.62%,  which  is  drastically 
high comparing to prior works that stagnate on 65.44%. 

keywords—router-to-as  mapping;  community  discovery; 

global router topology; fast hierarchy clustering 

i.   |
Data,|abstract
the results of botnet detection methods are usually presented without any comparison. although
it is generally accepted that more comparisons with third-party methods may help to improve the
area, few papers could do it. among the factors that prevent a comparison are the diﬃculties to
share a dataset, the lack of a good dataset, the absence of a proper description of the methods and
the lack of a comparison methodology. this paper compares the output of three diﬀerent botnet
detection methods by executing them over a new, real, labeled and large botnet dataset. this
dataset includes botnet, normal and background traﬃc. the results of our two methods (bclus
and camnep) and bothunter were compared using a methodology and a novel error metric
designed for botnet detections methods. we conclude that comparing methods indeed helps to
better estimate how good the methods are, to improve the algorithms, to build better datasets and
to build a comparison methodology.

keywords: botnet detection, malware detection, methods comparison, botnet dataset, anomaly

detection, network traﬃc

1. |
Data,|abstract

much interest has been taken in understanding the global routing structure of the internet, both

to model and protect the current structures and to modify the structure to improve resilience.

these studies rely on trace-routes and algorithmic inference to resolve individual ip addresses

into connected routers, yielding a network of routers. using whois registries, parsing of dns

registries, as well as simple latency-based triangulation, these routers can often be geolocated to at

least their country of origin, if not speciﬁc regions. in this work, we use node subgraph summary

statistics to present evidence that the router-level (ipv4) network is spatially embedded, with the

similarity (or dissimilarity) of a node from it’s neighbor strongly correlating with the attributes of

other routers residing in the same country or region. we discuss these results in context of the

recently proposed gravity models of the internet, as well as the potential application to geolocation

inferrence.

4
1
0
2

 
t
c
o
9

 

 
 
]
i

n
.
s
c
[
 
 

1
v
0
4
3
3

.

0
1
4
1
:
v
i
x
r
a

1

i.

|
Data,|abstract—traceroute is largely considered as the number-one
tool when troubleshooting the network, with innumerable appli-
cations, such as pinpointing the routing deﬁciencies or detecting
and locating network outages. previous works have extensively in-
vestigated pitfalls and ﬂaws causing the measurements performed
with this tool to be inaccurate or incomplete. in this paper, we
show how, even in the absence of all these well-investigated pitfalls
and ﬂaws, our ability to properly troubleshoot the network
with traceroute is strongly limited. indeed, by using state-
of-the-art alias resolution techniques, we investigate how and
how much the ip-level description provided by traceroute can
distort our understanding of the characteristics of internet paths.
we experimentally evaluate the impact on path properties like
equal-cost multipaths, loops, routing cycles, load balancing, route
prevalence and persistence. our results conﬁrm that researchers
and network operators relying on traceroute may poorly estimate
(i) the number of multiple equal-cost routes to the destination;
(ii) the presence of suboptimal routing in the network; (iii) the
routing stability.

i. |
Data,|abstract
one challenge in understanding the evolution of internet in-
frastructure is the lack of systematic mechanisms for moni-
toring the extent to which allocated ip addresses are actually
used. in this paper we try to advance the science of infer-
ring ipv4 address space utilization by analyzing and corre-
lating results obtained through different types of measure-
ments. we have previously studied an approach based on
passive measurements that can reveal used portions of the
address space unseen by active approaches. in this paper,
we study such passive approaches in detail, extending our
methodology to four different types of vantage points, iden-
tifying trafﬁc components that most signiﬁcantly contribute
to discovering used ipv4 network blocks. we then combine
the results we obtained through passive measurements to-
gether with data from active measurement studies, as well as
measurements from bgp and additional datasets available to
researchers. through the analysis of this large collection of
heterogeneous datasets, we substantially improve the state
of the art in terms of: (i) understanding the challenges and
opportunities in using passive and active techniques to study
address utilization; and (ii) knowledge of the utilization of
the ipv4 space.

1.

|
Data,|abstract. based on solid theoretical foundations, we present strong evidences that a number of real-
life networks, taken from diﬀerent domains like internet measurements, biological data, web graphs,
social and collaboration networks, exhibit tree-like structures from a metric point of view. we investigate
few graph parameters, namely, the tree-distortion and the tree-stretch, the tree-length and the tree-
breadth, the gromov’s hyperbolicity, the cluster-diameter and the cluster-radius in a layering partition
of a graph, which capture and quantify this phenomenon of being metrically close to a tree. by bringing
all those parameters together, we not only provide eﬃcient means for detecting such metric tree-like
structures in large-scale networks but also show how such structures can be used, for example, to
eﬃciently and compactly encode approximate distance and almost shortest path information and to
fast and accurately estimate diameters and radii of those networks. estimating the diameter and the
radius of a graph or distances between its arbitrary vertices are fundamental primitives in many data
and graph mining algorithms.

1

|
Data,|abstract—full knowledge of the routing topology of the internet
is useful for a multitude of network management tasks. however,
the full topology is often not known and is instead estimated
using topology inference algorithms. many of these algorithms use
traceroute to probe paths and then use the collected information
to infer the topology. we perform real experiments and show
that, in practice, routers may severely disrupt the operation of
traceroute and cause it to only provide partial information. we
propose itop, an algorithm for inferring the network topology
when only partial information is available.
itop constructs a
virtual topology, which overestimates the number of network
components, and then repeatedly merges links in this topology to
resolve it toward the structure of the true network. we perform
extensive simulations to compare itop to state-of-the-art infer-
ence algorithms. results show that itop signiﬁcantly outperforms
previous approaches and its inferred topologies are within 5% of
the original networks for all considered metrics. additionally, we
show that the topologies inferred by itop signiﬁcantly improve the
performance of fault localization algorithms when compared with
other approaches.

index terms—topology inference, partial information, fault

localization.

i. |
Data,|abstract

route diversity in networks is elemental for establishing reliable, high-capacity connections with appropriate security between
endpoints. as for the internet, route diversity has already been studied at both autonomous system- and router-level topologies by
means of graph theoretical disjoint paths. in this paper we complement these approaches by proposing a method for measuring the
diversity of internet paths in a geographical sense. by leveraging the recent developments in ip geolocation we show how to map
the paths discovered by traceroute into geographically equivalent classes. this allows us to identify the geographical footprints
of the major transmission paths between end-hosts, and building on our observations, we propose a quantitative measure for
geographical diversity of internet routes between any two hosts.

geodiversity; traceroute; geolocation; disjoint routes

index terms

i. |
Data,|abstract
the internet is composed of routing devices connected between them and
organized into independent administrative entities: the autonomous systems. the
existence of diﬀerent types of autonomous systems (like large connectivity providers,
internet service providers or universities) together with geographical and economical
constraints, turns the internet into a complex modular and hierarchical network. this
organization is reﬂected in many properties of the internet topology, like its high
degree of clustering and its robustness.

in this work we study the modular structure of the internet router-level graph in
order to assess to what extent the autonomous systems satisfy some of the known
notions of community structure. we observe that most of the classical community
detection methods fail to detect the autonomous systems as communities, mainly
because the modular structure of the internet (as that of many complex networks) is
much richer than what can be captured by optimizing a global functional:
autonomous systems have largely variable sizes, structures and functions. classical
methods are severely aﬀected by resolution limits and by the heterogeneity of the
communities; even when using multiresolution methods, there is no single resolution
at which most of the communities can be captured.

however, we show that multiresolution methods do ﬁnd the community structure

of the autonomous systems, but each of them has to be observed at the correct
resolution level. then we develop a low-complexity multiresolution modularity
optimization algorithm that ﬁnds communities at diﬀerent resolution levels in a
continuous scale, in one single run. using this method, we show that with a scarce
knowledge of the node aﬃliations, multiresolution methods can be adjusted to
retrieve the autonomous systems, signiﬁcantly improving the results of classical
single-resolution methods. finally, in the light of our results, we discuss recent work
concerning the use of a priori information to ﬁnd community structure in complex
networks.
keywords: internet topology; community structure; autonomous systems; complex
networks

1 |
Data,|abstract
there are currently no requirements (technical or otherwise)
that bgp paths must be contained within national bound-
aries. indeed, some paths experience international detours,
i.e., originate in one country, cross international boundaries
and return to the same country. in most cases these are sensi-
ble trafﬁc engineering or peering decisions at isps that serve
multiple countries. in some cases such detours may be sus-
picious. characterizing international detours is useful to a
number of players: (a) network engineers trying to diagnose
persistent problems, (b) policy makers aiming at adhering to
certain national communication policies, (c) entrepreneurs
looking for opportunities to deploy new networks, or (d)
privacy-conscious states trying to minimize the amount of
internal communication traversing different jurisdictions.

in this paper we characterize international detours in the
internet during the month of january 2016. to detect detours
we sample bgp ribs every 8 hours from 461 routeviews
and ripe ris peers spanning 30 countries. then geolocate
visible ases by geolocating each bgp preﬁx announced by
each as, mapping its presence at ixps and geolocation in-
frastructure ips. finally, analyze each global bgp rib en-
try looking for detours. our analysis shows more than 5k
unique bgp preﬁxes experienced a detour. a few ases
cause most detours and a small fraction of preﬁxes were af-
fected the most. we observe about 544k detours. detours
either last for a few days or persist the entire month. out of
all the detours, more than 90% were transient detours that
lasted for 72 hours or less. we also show different countries
experience different characteristics of detours.

keywords
as geolocation, routing detours, mitm
1.

|
Data,|abstract
network tarpits, whereby a single host or appliance can mas-
querade as many fake hosts on a network and slow network
scanners, are a form of defensive cyber-deception.
in this
work, we develop degreaser , an eﬃcient ﬁngerprinting tool
to remotely detect tarpits. in addition to validating our tool
in a controlled environment, we use degreaser to perform an
internet-wide scan. we discover tarpits of non-trivial size in
the wild (preﬁxes as large as /16), and characterize their dis-
tribution and behavior. we then show how tarpits pollute
existing network measurement surveys that are tarpit-na¨ıve,
e.g. internet census data, and how degreaser can improve the
accuracy of such surveys. lastly, our ﬁndings suggest sev-
eral ways in which to advance the realism of current network
tarpits, thereby raising the bar on tarpits as an operational
security mechanism.

categories and subject descriptors
c.2.0 [computer-communication networks]: general—
security and protection; c.4 [performance of systems]:
measurement techniques

keywords
tarpits; internet census; sticky honeypot; deception

1.

|
Data,|abstract—features of an internet traffic time series can 
be  estimated  using  dynamical  systems.  dynamical 
systems may exhibit chaos and strange attractors [1] [2]. 
since  internet  traffic  shows  non  stationarity  and  long 
term  dependence  among  data  samples,  a  cognitive 
polyscale approach should be taken to analyze the hidden 
features in a nonlinear data time series. it is necessary to 
estimate a reasonable window of time series so that the 
polyscale analysis can be performed without violating the 
statistical bounds of the analysis. in this work, a feature 
extraction algorithm is developed using variance fractal 
dimension trajectory and the statistical parameters of the 
calculation  are  validated  using  an  autonomous  varying 
window  of  data  samples.  our  analysis  shows  promising 
results since the algorithm is able to capture the presence 
of  dns  denial  of  service  attack  and  has  extracted  the 
bursts of data sample accurately.   

keywords—  cognitive  machine  learning,  fractal,  polyscale, 
dns  ddos  amplification  attacks,  anomaly  detection,  cyber 
threats,  variance  fractal  dimension,  non  stationary  trend 
analysis. 

i. 

|
Data,|abstract: network security requires real-time monitoring of network trafﬁc in order
to detect new and unexpected attacks. attack detection methods based on deep packet
inspection are time consuming and costly, due to their high computational demands. this
paper proposes a fast, lightweight method to distinguish different attack types observed
in an ip darkspace monitor. the method is based on entropy measures of trafﬁc-ﬂow
features and machine learning techniques. the explored data belongs to a portion of the
internet background radiation from a large ip darkspace, i.e., real trafﬁc captures that
exclusively contain unsolicited trafﬁc, ongoing attacks, attack preparation activities and
attack aftermaths. results from an in-depth trafﬁc analysis based on packet headers and
content are used as a reference to label data and to evaluate the quality of the entropy-based
classiﬁcation. full ip darkspace trafﬁc captures from a three-week observation period in
april, 2012, are used to compare the entropy-based classiﬁcation with the in-depth trafﬁc
analysis. results show that several trafﬁc types present a high correlation to the respective
trafﬁc-ﬂow entropy signals and can even ﬁt polynomial regression models. therefore,
sudden changes in trafﬁc types caused by new attacks or attack preparation activities can
be identiﬁed based on entropy variations.

keywords: network security;
classiﬁcation; signal modeling

information entropy;

time series analysis; supervised

entropy 2015, 17

1. |
Data,|abstract—internet traffic exhibits long range dependence 
(persistence), scale invariance and self-similarity or self-
affinity  which  are  the  known  characteristics  of  fractals. 
moreover, 
fractals  can  be 
extracted and quantified from an internet data time series 
using  non-integer  dimensions  (fractal  dimensions).  the 
notion of cognitive complexity is also very well represented 
by  the  fractal  dimensions,  e.g.,  high  value  of  fractal 
dimension of an object implies that the complexity of this 
object is higher than the one with lower fractal dimension. 
in addition, a multifractal object is more complex than a 
monofractal object and this can also be characterized to 
identify  the  degree  of  complexity.  in  this  work,  we  have 
shown that the complexity introduced by distributed denial 
of service (ddos) attack packets in dns (domain name 
system) traffic is higher than the complexity of dns traffic 
with no ddos attack packets. a power spectrum density of 
the data series was used to calculate the spectral fractal 
dimension, and the performance of the proposed algorithm 
is validated using mathematical fractal brownian motion 
process  (fbm)  and  the  real  data  sets.  a  sequence  of 
spectral fractal dimension measurements of the time series 
(also known as a trajectory of spectral fractal dimension 
measurements  or  spectral  fractal  dimension  trajectory 
(sfdt)) was generated to show the changing complexity 
of the series in time domain.  

keywords—denial  of  service,  domain  name  system  (dns), 
cyber threats, complexity, multifractal, power spectrum density, 
time  series,  spectral  fractal  dimension  trajectory  (sfdt), 
variance fractal dimension trajectory, malicious traffic. 

i. 

|
