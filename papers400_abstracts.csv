p628-cangialosi.pdf,|
The semantics of online authentication in the web are rather
straightforward:
if Alice has a certificate binding Bobs
name to a public key, and if a remote entity can prove knowl-
edge of Bobs private key, then (barring key compromise)
that remote entity must be Bob. However, in reality, many
websitesand the majority of the most popular onesare
hosted at least in part by third parties such as Content Deliv-
ery Networks (CDNs) or web hosting providers. Put simply:
administrators of websites who deal with (extremely) sensi-
tive user data are giving their private keys to third parties.
Importantly, this sharing of keys is undetectable by most
users, and widely unknown even among researchers.

In this paper, we perform a large-scale measurement study
of key sharing in todays web. We analyze the prevalence
with which websites trust third-party hosting providers with
their secret keys, as well as the impact that this trust has on
responsible key management practices, such as revocation.
Our results reveal that key sharing is extremely common,
with a small handful of hosting providers having keys from
the majority of the most popular websites. We also find that
hosting providers often manage their customers keys, and
that they tend to react more slowly yet more thoroughly to
compromised or potentially compromised keys.

1.

|,Data
p1639-pereida-garcia.pdf,|
TLS and SSH are two of the most commonly used proto-
cols for securing Internet traffic. Many of the implemen-
tations of these protocols rely on the cryptographic primi-
tives provided in the OpenSSL library. In this work we dis-
close a vulnerability in OpenSSL, affecting all versions and
forks (e.g. LibreSSL and BoringSSL) since roughly October
2005, which renders the implementation of the DSA signa-
ture scheme vulnerable to cache-based side-channel attacks.
Exploiting the software defect, we demonstrate the first pub-
lished cache-based key-recovery attack on these protocols:
260 SSH-2 handshakes to extract a 1024/160-bit DSA host
key from an OpenSSH server, and 580 TLS 1.2 handshakes
to extract a 2048/256-bit DSA key from an stunnel server.

Keywords
applied cryptography; digital signatures; side-channel anal-
ysis; timing attacks; cache-timing attacks; DSA; OpenSSL;
CVE-2016-2178

1.

|,Data
p981-lewi.pdf,|
Secure multilinear maps (mmaps) have been shown to have
remarkable applications in cryptography, such as multi-input
functional encryption (MIFE) and program obfuscation. To
date, there has been little evaluation of the performance of
these applications.
In this paper we initiate a systematic
study of mmap-based constructions. We build a general
framework, called 5Gen, to experiment with these applica-
tions. At the top layer we develop a compiler that takes
in a high-level program and produces an optimized matrix
branching program needed for the applications we consider.
Next, we optimize and experiment with several MIFE and
obfuscation constructions and evaluate their performance.
The 5Gen framework is modular and can easily accommo-
date new mmap constructions as well as new MIFE and
obfuscation constructions, as well as being an open-source
tool that can be used by other research groups to experiment
with a variety of mmap-based constructions.

1.

|,Non-data
sec14-paper-tripp.pdf,|

Mobile apps often require access to private data, such
as the device ID or location. At the same time, popular
platforms like Android and iOS have limited support for
user privacy. This frequently leads to unauthorized dis-
closure of private information by mobile apps, e.g. for
advertising and analytics purposes. This paper addresses
the problem of privacy enforcement in mobile systems,
which we formulate as a classification problem: When
arriving at a privacy sink (e.g., database update or outgo-
ing web message), the runtime system must classify the
sinks behavior as either legitimate or illegitimate. The
traditional approach of information-flow (or taint) track-
ing applies binary classification, whereby information
release is legitimate iff there is no data flow from a pri-
vacy source to sink arguments. While this is a useful
heuristic, it also leads to false alarms.

We propose to address privacy enforcement as a learn-
ing problem, relaxing binary judgments into a quanti-
tative/probabilistic mode of reasoning. Specifically, we
propose a Bayesian notion of statistical classification,
which conditions the judgment whether a release point is
legitimate on the evidence arising at that point. In our
concrete approach, implemented as the BAYESDROID
system that is soon to be featured in a commercial prod-
uct, the evidence refers to the similarity between the data
values about to be released and the private data stored on
the device. Compared to TaintDroid, a state-of-the-art
taint-based tool for privacy enforcement, BAYESDROID
is substantially more accurate. Applied to 54 top-popular
Google Play apps, BAYESDROID is able to detect 27 pri-
vacy violations with only 1 false alarm.

1

|,Data
07259368.pdf,|-This 
extraction 
dimension 
series. 
is considered 
attacks. 
transforming 
variance dimension 
stationarity 
are extracted 
extracted 
set of features 
trajectory 
attack when the denoised 
This technique 
variance 
in 
finding  changing  patterns 
of a data series due to the 
presence 
is not dependent 
measurement 
technique 
windows in a highly non stationary 

to remove high variability 
noise. The 
of an 
the presence 

dimensions 
in data series. 

of noise and denial of service 

attack because it 
and mono-scale 

and locally stationary 

fractal dimension. 

shows increasing 

of the trajectory 

of variations 

Then features 

data series. 

dimension 

estimates 

There are various 

where dimensions 

the complexity 
of an 
to 

Multifractal 
object in a non-integer 
monoscale  analysis 
integers 
only. The concept of non-integer 
arises due to the power law relationship 
samples. 
from a practical 
Method [3], the Katz-Servcik 
Fluctuation 
Analysis 
method that 
fractal 
measures 
scale for a 
given data series [7]. In this work, we utilized 
fractal 
to characterize 
data series for the identification 
attack. 

Analysis 
is another 
at multiple 

(VFD) estimate 
the change in variance 

Analysis [5], Power Spectral 

of the presence 
of an 

and Critical 

dimension 

estimates 

dimension 

Estimation 
aspect, 

keeping statistical 

of stationarity 

of a data series 

is superior 

of fractal 

Moreover, 
this 

indicates 

Exponent 

on integer 

in the data 

adaptive 

provides 

a DNS count 

is another 
of fractal features 

Density 
[6]. Variance 

that the rate of change of variance 

method [4], Detrended 

dimension 

dimension 

variance 

time series such as the Box-Counting 

Keywords-Cognitive Machine Learning, Chaos, Fractal, 
DNS, DDoS Amplification, 
Internet, Anomaly Detection, 
Cyber threats, Data traffic, Multifractal, 
Dimension, Wavelet, Haar function, 
Sationary, 
Series Analysis. 

Change Detection, 
Non 
Window, Time 

Trend Analysis, Adaptive  Sliding 

Variance Fractal 

I. |,Data
p1204-fett.pdf,|
The OAuth 2.0 protocol is one of the most widely deployed au-
thorization/single sign-on (SSO) protocols and also serves as the
foundation for the new SSO standard OpenID Connect. Despite the
popularity of OAuth, so far analysis efforts were mostly targeted at
finding bugs in specific implementations and were based on formal
models which abstract from many web features or did not provide
a formal treatment at all.

In this paper, we carry out the first extensive formal analysis of
the OAuth 2.0 standard in an expressive web model. Our analy-
sis aims at establishing strong authorization, authentication, and
session integrity guarantees, for which we provide formal defini-
tions. In our formal analysis, all four OAuth grant types (autho-
rization code grant, implicit grant, resource owner password cre-
dentials grant, and the client credentials grant) are covered. They
may even run simultaneously in the same and different relying par-
ties and identity providers, where malicious relying parties, identity
providers, and browsers are considered as well. Our modeling and
analysis of the OAuth 2.0 standard assumes that security recommen-
dations and best practices are followed in order to avoid obvious
and known attacks.

When proving the security of OAuth in our model, we discovered
four attacks which break the security of OAuth. The vulnerabilities
can be exploited in practice and are present also in OpenID Connect.
We propose fixes for the identified vulnerabilities, and then, for
the first time, actually prove the security of OAuth in an expressive
web model. In particular, we show that the fixed version of OAuth
(with security recommendations and best practices in place) pro-
vides the authorization, authentication, and session integrity proper-
ties we specify.

1.

|,Non-data
imc182-meiklejohnA.pdf,|
Bitcoin is a purely online virtual currency, unbacked by either phys-
ical commodities or sovereign obligation; instead, it relies on a
combination of cryptographic protection and a peer-to-peer proto-
col for witnessing settlements. Consequently, Bitcoin has the un-
intuitive property that while the ownership of money is implicitly
anonymous, its flow is globally visible. In this paper we explore
this unique characteristic further, using heuristic clustering to group
Bitcoin wallets based on evidence of shared authority, and then us-
ing re-identification attacks (i.e., empirical purchasing of goods and
services) to classify the operators of those clusters. From this anal-
ysis, we characterize longitudinal changes in the Bitcoin market,
the stresses these changes are placing on the system, and the chal-
lenges for those seeking to use Bitcoin for criminal or fraudulent
purposes at scale.
Categories and Subject Descriptors
K.4.4 [Electronic Commerce]: Payment schemes
Keywords
Bitcoin; Measurement; Anonymity
1.

|,Data
06547112.pdf,|We consider interactive, proof-based verifiable computa-
tion: how can a client machine specify a computation to a server,
receive an answer, and then engage the server in an interactive
protocol that convinces the client that the answer is correct, with
less work for the client than executing the computation in the first
place? Complexity theory and cryptography offer solutions in prin-
ciple, but if implemented naively, they are ludicrously expensive.
Recently, however, several strands of work have refined this the-
ory and implemented the resulting protocols in actual systems. This
work is promising but suffers from one of two problems: either it
relies on expensive cryptography, or else it applies to a restricted
class of computations. Worse, it is not always clear which protocol
will perform better for a given problem.

We describe a system that (a) extends optimized refinements of
the non-cryptographic protocols to a much broader class of compu-
tations, (b) uses static analysis to fail over to the cryptographic ones
when the non-cryptographic ones would be more expensive, and (c)
incorporates this core into a built system that includes a compiler
for a high-level language, a distributed server, and GPU accelera-
tion. Experimental results indicate that our system performs better
and applies more widely than the best in the literature.
1 |,Non-data
sec14-paper-costin.pdf,|

1 |,Data
sec14-paper-li-zhigong.pdf,|
Users speaking different languages may prefer different
patterns in creating their passwords, and thus knowledge
on English passwords cannot help to guess passwords
from other languages well. Research has already shown
Chinese passwords are one of the most difficult ones to
guess. We believe that the conclusion is biased because,
to the best of our knowledge, little empirical study has
examined regional differences of passwords on a large
scale, especially on Chinese passwords. In this paper, we
study the differences between passwords from Chinese
and English speaking users, leveraging over 100 million
leaked and publicly available passwords from Chinese
and international websites in recent years. We found that
Chinese prefer digits when composing their passwords
while English users prefer letters, especially lowercase
letters. However, their strength against password guess-
ing is similar. Second, we observe that both users pre-
fer to use the patterns that they are familiar with, e.g.,
Chinese Pinyins for Chinese and English words for En-
glish users. Third, we observe that both Chinese and En-
glish users prefer their conventional format when they
use dates to construct passwords. Based on these obser-
vations, we improve a PCFG (Probabilistic Context-Free
Grammar) based password guessing method by inserting
Pinyins (about 2.3% more entries) into the attack dictio-
nary and insert our observed composition rules into the
guessing rule set. As a result, our experiments show that
the efficiency of password guessing increases by 34%.

1

|,Data
sec14-paper-blond.pdf,|

We present an empirical analysis of targeted attacks
against a human-rights Non-Governmental Organization
(NGO) representing a minority living in China. In par-
ticular, we analyze the social engineering techniques, at-
tack vectors, and malware employed in malicious emails
received by two members of the NGO over a four-year
period. We find that both the language and topic of
the emails were highly tailored to the victims, and that
sender impersonation was commonly used to lure them
into opening malicious attachments. We also show that
the majority of attacks employed malicious documents
with recent but disclosed vulnerabilities that tend to
evade common defenses. Finally, we find that the NGO
received malware from different families and that over a
quarter of the malware can be linked to entities that have
been reported to engage in targeted attacks against polit-
ical and industrial organizations, and Tibetan NGOs.
1 |,Data
1512.03916.pdf,|In the last decade many works has been done on the 
Internet topology  at router  or  autonomous system (AS) level. As 
routers is the essential composition of ASes while ASes dominate 
the  behavior  of  their  routers.  It  is  no  doubt  that  identifying  the 
affiliation  between  routers  and  ASes  can  let  us  gain  a  deeper 
understanding  on  the  topology.  However,  the  existing  methods 
that assign a router to an AS just based on the origin ASes of its IP 
addresses, which does not make full use of information in our hand. 
In this paper, we propose a methodology to assign routers to their 
owner ASes based on community discovery tech. First, we use the 
origin  ASes  information  along  with  router-pairs  similarities  to 
construct a weighted router level topology; secondly, for enormous 
topology data (more than 2M nodes and 19M edges) from CAIDA 
ITDK project, we propose a fast hierarchy clustering which time 
and  space  complex  are  both  linear  to  do  ASes  community 
discovery, last we do router-to-AS  mapping based on these ASes 
communities.  Experiments  show  that  combining  with  ASes 
communities our methodology discovers, the best accuracy rate of 
router-to-AS  mapping  can  reach  to  82.62%,  which  is  drastically 
high comparing to prior works that stagnate on 65.44%. 

Keywordsrouter-to-AS  mapping;  community  discovery; 

global router topology; fast hierarchy clustering 

I.   |,Data
p1591-green.pdf,|
We present a protocol to enable privacy preserving adver-
tising reporting at scale. Unlike previous systems, our work
scales to millions of users and tens of thousands of distinct
ads. Our approach builds on the homomorphic encryption
approach proposed by Adnostic [42], but uses new crypto-
graphic proof techniques to efficiently report billions of ad
impressions a day using an additively homomorphic voting
schemes. Most importantly, our protocol scales without im-
posing high loads on trusted third parties. Finally, we inves-
tigate a cost effective method to privately deliver ads with
computational private information retrieval.

1.

|,Non-data
06547120.pdf,|Perceptual, context-aware applications that ob-
serve their environment and interact with users via cameras
and other sensors are becoming ubiquitous on personal com-
puters, mobile phones, gaming platforms, household robots,
and augmented-reality devices. This raises new privacy risks.
We describe the design and implementation of DARKLY, a
practical privacy protection system for the increasingly com-
mon scenario where an untrusted, third-party perceptual ap-
plication is running on a trusted device. DARKLY is integrated
with OpenCV, a popular computer vision library used by such
applications to access visual inputs. It deploys multiple privacy
protection mechanisms, including access control, algorithmic
privacy transforms, and user audit.

We evaluate DARKLY on 20 perceptual applications that per-
form diverse tasks such as image recognition, object tracking,
security surveillance, and face detection. These applications
run on DARKLY unmodified or with very few modifications
and minimal performance overheads vs. native OpenCV. In
most cases, privacy enforcement does not reduce the appli-
cations functionality or accuracy. For the rest, we quantify
the tradeoff between privacy and utility and demonstrate that
utility remains acceptable even with strong privacy protection.

I. |,Data
p17-luu.pdf,|
Cryptocurrencies, such as Bitcoin and 250 similar alt-coins, em-
body at their core a blockchain protocol  a mechanism for a dis-
tributed network of computational nodes to periodically agree on
a set of new transactions. Designing a secure blockchain protocol
relies on an open challenge in security, that of designing a highly-
scalable agreement protocol open to manipulation by byzantine or
arbitrarily malicious nodes. Bitcoins blockchain agreement proto-
col exhibits security, but does not scale: it processes 37 transac-
tions per second at present, irrespective of the available computa-
tion capacity at hand.

In this paper, we propose a new distributed agreement proto-
col for permission-less blockchains called ELASTICO. ELASTICO
scales transaction rates almost linearly with available computation
for mining: the more the computation power in the network, the
higher the number of transaction blocks selected per unit time.
ELASTICO is efficient in its network messages and tolerates byzan-
tine adversaries of up to one-fourth of the total computational power.
Technically, ELASTICO uniformly partitions or parallelizes the min-
ing network (securely) into smaller committees, each of which pro-
cesses a disjoint set of transactions (or shards). While sharding
is common in non-byzantine settings, ELASTICO is the first candi-
date for a secure sharding protocol with presence of byzantine ad-
versaries. Our scalability experiments on Amazon EC2 with up to
1, 600 nodes confirm ELASTICOs theoretical scaling properties.

1.

|,Non-data
p871-zhou.pdf,|
We present a software approach to mitigate access-driven
side-channel attacks that leverage last-level caches (LLCs)
shared across cores to leak information between security do-
mains (e.g., tenants in a cloud). Our approach dynami-
cally manages physical memory pages shared between secu-
rity domains to disable sharing of LLC lines, thus prevent-
ing Flush-Reload side channels via LLCs. It also man-
ages cacheability of memory pages to thwart cross-tenant
Prime-Probe attacks in LLCs. We have implemented
our approach as a memory management subsystem called
CacheBar within the Linux kernel to intervene on such
side channels across container boundaries, as containers are
a common method for enforcing tenant isolation in Platform-
as-a-Service (PaaS) clouds. Through formal verification,
principled analysis, and empirical evaluation, we show that
CacheBar achieves strong security with small performance
overheads for PaaS workloads.

Keywords
Cache-based side channel; prime-probe; flush-reload

1.

|,Non-data
p1480-albrecht.pdf,|
This work presents a systematic analysis of symmetric encryp-
tion modes for SSH that are in use on the Internet, providing
deployment statistics, new attacks, and security proofs for
widely used modes. We report deployment statistics based on
two Internet-wide scans of SSH servers conducted in late 2015
and early 2016. Dropbear and OpenSSH implementations
dominate in our scans. From our first scan, we found 130,980
OpenSSH servers that are still vulnerable to the CBC-mode-
specific attack of Albrecht et al. (IEEE S&P 2009), while we
found a further 20,000 OpenSSH servers that are vulnerable
to a new attack on CBC-mode that bypasses the counter-
measures introduced in OpenSSH 5.2 to defeat the attack of
Albrecht et al. At the same time, 886,449 Dropbear servers in
our first scan are vulnerable to a variant of the original CBC-
mode attack. On the positive side, we provide formal security
analyses for other popular SSH encryption modes, namely
ChaCha20-Poly1305, generic Encrypt-then-MAC, and AES-
GCM. Our proofs hold for detailed pseudo-code descriptions
of these algorithms as implemented in OpenSSH. Our proofs
use a corrected and extended version of the fragmented de-
cryption security model that was specifically developed for
the SSH setting by Boldyreva et al. (Eurocrypt 2012). These
proofs provide strong confidentiality and integrity guaran-
tees for these alternatives to CBC-mode encryption in SSH.
However, we also show that these alternatives do not meet ad-
ditional, desirable notions of security (boundary-hiding under
passive and active attacks, and denial-of-service resistance)
that were formalised by Boldyreva et al.

1.

|,Data
p468-checkoway.pdf,|
In December 2015, Juniper Networks announced multiple security
vulnerabilities stemming from unauthorized code in ScreenOS, the
operating system for their NetScreen VPN routers. The more so-
phisticated of these vulnerabilities was a passive VPN decryption
capability, enabled by a change to one of the elliptic curve points
used by the Dual EC pseudorandom number generator.

In this paper, we describe the results of a full independent analysis
of the ScreenOS randomness and VPN key establishment proto-
col subsystems, which we carried out in response to this incident.
While Dual EC is known to be insecure against an attacker who
can choose the elliptic curve parameters, Juniper had claimed in
2013 that ScreenOS included countermeasures against this type of
attack. We find that, contrary to Junipers public statements, the
ScreenOS VPN implementation has been vulnerable since 2008 to
passive exploitation by an attacker who selects the Dual EC curve
point. This vulnerability arises due to apparent flaws in Junipers
countermeasures as well as a cluster of changes that were all in-
troduced concurrently with the inclusion of Dual EC in a single
2008 release. We demonstrate the vulnerability on a real NetScreen
device by modifying the firmware to install our own parameters,
and we show that it is possible to passively decrypt an individual
VPN session in isolation without observing any other network traffic.
We investigate the possibility of passively fingerprinting ScreenOS
implementations in the wild. This incident is an important example
of how guidelines for random number generation, engineering, and
validation can fail in practice.

1.

|,Data
p1438-krawczyk.pdf,|
We study the question of how to build compilers that
transform a unilaterally authenticated (UA) key-exchange
protocol into a mutually-authenticated (MA) one. We present
a simple and efficient compiler and characterize the UA pro-
tocols that the compiler upgrades to the MA model, showing
this to include a large and important class of UA protocols.
The question, while natural, has not been studied widely.
Our work is motivated in part by the ongoing work on the
design of TLS 1.3, specifically the design of the client au-
thentication mechanisms including the challenging case of
post-handshake authentication. Our approach supports the
analysis of these mechanisms in a general and modular way,
in particular aided by the notion of functional security that
we introduce as a generalization of key exchange models and
which may be of independent interest.

1.

|,Non-data
p1528-sharif.pdf,|
Machine learning is enabling a myriad innovations, including
new algorithms for cancer diagnosis and self-driving cars.
The broad use of machine learning makes it important to
understand the extent to which machine-learning algorithms
are subject to attack, particularly when used in applications
where physical security or safety is at risk.

In this paper, we focus on facial biometric systems, which
are widely used in surveillance and access control. We de-
fine and investigate a novel class of attacks: attacks that
are physically realizable and inconspicuous, and allow an at-
tacker to evade recognition or impersonate another individ-
ual. We develop a systematic method to automatically gen-
erate such attacks, which are realized through printing a pair
of eyeglass frames. When worn by the attacker whose image
is supplied to a state-of-the-art face-recognition algorithm,
the eyeglasses allow her to evade being recognized or to im-
personate another individual. Our investigation focuses on
white-box face-recognition systems, but we also demonstrate
how similar techniques can be used in black-box scenarios,
as well as to avoid face detection.

1.

|,Data
p755-liao.pdf,|
To adapt to the rapidly evolving landscape of cyber threats, secu-
rity professionals are actively exchanging Indicators of Compro-
mise (IOC) (e.g., malware signatures, botnet IPs) through public
sources (e.g. blogs, forums, tweets, etc.). Such information, of-
ten presented in articles, posts, white papers etc., can be converted
into a machine-readable OpenIOC format for automatic analysis
and quick deployment to various security mechanisms like an in-
trusion detection system. With hundreds of thousands of sources
in the wild, the IOC data are produced at a high volume and veloc-
ity today, which becomes increasingly hard to manage by humans.
Efforts to automatically gather such information from unstructured
text, however, is impeded by the limitations of todays Natural Lan-
guage Processing (NLP) techniques, which cannot meet the high
standard (in terms of accuracy and coverage) expected from the
IOCs that could serve as direct input to a defense system.

In this paper, we present iACE, an innovation solution for fully
automated IOC extraction. Our approach is based on the obser-
vation that the IOCs in technical articles are often described in a
predictable way: being connected to a set of context terms (e.g.,
download) through stable grammatical relations. Leveraging this
observation, iACE is designed to automatically locate a putative
IOC token (e.g., a zip file) and its context (e.g., malware, down-
load) within the sentences in a technical article, and further an-
alyze their relations through a novel application of graph mining
techniques. Once the grammatical connection between the tokens
is found to be in line with the way that the IOC is commonly pre-
sented, these tokens are extracted to generate an OpenIOC item
that describes not only the indicator (e.g., a malicious zip file) but
also its context (e.g., download from an external source). Running
on 71,000 articles collected from 45 leading technical blogs, this
new approach demonstrates a remarkable performance: it gener-
ated 900K OpenIOC items with a precision of 95% and a coverage
over 90%, which is way beyond what the state-of-the-art NLP tech-
nique and industry IOC tool can achieve, at a speed of thousands of
articles per hour. Further, by correlating the IOCs mined from the
articles published over a 13-year span, our study sheds new light on

1The two lead authors are ordered alphabetically.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978315

Figure 1: Example of OpenIOC schema.

the links across hundreds of seemingly unrelated attack instances,
particularly their shared infrastructure resources, as well as the im-
pacts of such open-source threat intelligence on security protection
and evolution of attack strategies.
1.
|,Data
sec14-paper-demmler.pdf,|

Secure two-party computation allows two mutually dis-
trusting parties to jointly compute an arbitrary function
on their private inputs without revealing anything but the
result. An interesting target for deploying secure compu-
tation protocols are mobile devices as they contain a lot
of sensitive user data. However, their resource restriction
makes the deployment of secure computation protocols a
challenging task.

In this work, we optimize and implement

the
secure computation protocol by Goldreich-Micali-
Wigderson (GMW) on mobile phones. To increase per-
formance, we extend the protocol by a trusted hardware
token (i.e., a smartcard). The trusted hardware token al-
lows to pre-compute most of the workload in an initial-
ization phase, which is executed locally on one device
and can be pre-computed independently of the later com-
munication partner. We develop and analyze a proof-
of-concept implementation of generic secure two-party
computation on Android smart phones making use of
a microSD smartcard. Our use cases include private
set intersection for finding shared contacts and private
scheduling of a meeting with location preferences. For
private set intersection, our token-aided implementation
on mobile phones is up to two orders of magnitude faster
than previous generic secure two-party computation pro-
tocols on mobile phones and even as fast as previous
work on desktop computers.

1

|,Non-data
p55-barthe.pdf,|
Differential privacy is a promising formal approach to data privacy,
which provides a quantitative bound on the privacy cost of an al-
gorithm that operates on sensitive information. Several tools have
been developed for the formal verification of differentially private
algorithms, including program logics and type systems. However,
these tools do not capture fundamental techniques that have emerged
in recent years, and cannot be used for reasoning about cutting-edge
differentially private algorithms. Existing techniques fail to handle
three broad classes of algorithms: 1) algorithms where privacy de-
pends on accuracy guarantees, 2) algorithms that are analyzed with
the advanced composition theorem, which shows slower growth in
the privacy cost, 3) algorithms that interactively accept adaptive
inputs.

We address these limitations with a new formalism extending
apRHL [6], a relational program logic that has been used for proving
differential privacy of non-interactive algorithms, and incorporating
aHL [11], a (non-relational) program logic for accuracy properties.
We illustrate our approach through a single running example, which
exemplifies the three classes of algorithms and explores new variants
of the Sparse Vector technique, a well-studied algorithm from the
privacy literature. We implement our logic in EasyCrypt, and for-
mally verify privacy. We also introduce a novel coupling technique
called optimal subset coupling that may be of independent interest.

1.

|,Non-data
sec15-paper-vanhoef.pdf,|

We present new biases in RC4, break the Wi-Fi Protected
Access Temporal Key Integrity Protocol (WPA-TKIP),
and design a practical plaintext recovery attack against
the Transport Layer Security (TLS) protocol. To empir-
ically find new biases in the RC4 keystream we use sta-
tistical hypothesis tests. This reveals many new biases in
the initial keystream bytes, as well as several new long-
term biases. Our fixed-plaintext recovery algorithms are
capable of using multiple types of biases, and return a
list of plaintext candidates in decreasing likelihood.

To break WPA-TKIP we introduce a method to gen-
erate a large number of identical packets. This packet is
decrypted by generating its plaintext candidate list, and
using redundant packet structure to prune bad candidates.
From the decrypted packet we derive the TKIP MIC key,
which can be used to inject and decrypt packets. In prac-
tice the attack can be executed within an hour. We also
attack TLS as used by HTTPS, where we show how to
decrypt a secure cookie with a success rate of 94% using
9 227 ciphertexts. This is done by injecting known data
around the cookie, abusing this using Mantins ABSAB
bias, and brute-forcing the cookie by traversing the plain-
text candidates. Using our traffic generation technique,
we are able to execute the attack in merely 75 hours.

1

|,Non-data
p1414-liu.pdf,|
In a dangling DNS record (Dare), the resources pointed to by the
DNS record are invalid, but the record itself has not yet been purged
from DNS. In this paper, we shed light on a largely overlooked
threat in DNS posed by dangling DNS records. Our work reveals
that Dare can be easily manipulated by adversaries for domain hi-
jacking. In particular, we identify three attack vectors that an ad-
versary can harness to exploit Dares. In a large-scale measurement
study, we uncover 467 exploitable Dares in 277 Alexa top 10,000
domains and 52 edu zones, showing that Dare is a real, preva-
lent threat. By exploiting these Dares, an adversary can take full
control of the (sub)domains and can even have them signed with a
Certificate Authority (CA). It is evident that the underlying cause
of exploitable Dares is the lack of authenticity checking for the
resources to which that DNS record points. We then propose three
defense mechanisms to effectively mitigate Dares with little human
effort.
Keywords
DNS; Dangling records; Domain hijacking
1.

|,Data
06547098.pdf,|Existing

fine-grained,

dynamic
information-flow control assume that
is acceptable to
terminate the entire system when an incorrect flow is
detectedi.e,
they give up availability for the sake of
confidentiality and integrity. This is an unrealistic limitation
for systems such as long-running servers.
We identify public labels and delayed exceptions as crucial
ingredients for making information-flow errors recoverable in
a sound and usable language, and we propose two new error-
handling mechanisms that make all errors recoverable. The
first mechanism builds directly on these basic ingredients,
using not-a-values (NaVs) and data flow to propagate errors.
The second mechanism adapts the standard exception model
to satisfy the extra constraints arising from information flow
control, converting thrown exceptions to delayed ones at certain
points. We prove that both mechanisms enjoy the fundamental
soundness property of non-interference. Finally, we describe
a prototype implementation of a full-scale language with
NaVs and report on our experience building robust software
components in this setting.

Keywords-dynamic information flow control, fine-grained
labeling, availability, reliability, error recovery, exception han-
dling, programming-language design, public labels, delayed
exceptions, not-a-values, NaVs

|,Non-data
06547109.pdf,|This paper presents a novel mechanism, called
Ally Friendly Jamming, which aims at providing an intelligent
jamming capability that can disable unauthorized (enemy)
wireless communication but at
the same time still allow
authorized wireless devices to communicate, even if all these
devices operate at the same frequency. The basic idea is to
jam the wireless channel continuously but properly control the
jamming signals with secret keys, so that the jamming signals
are unpredictable interference to unauthorized devices, but are
recoverable by authorized ones equipped with the secret keys.
To achieve the ally friendly jamming capability, we develop
new techniques to generate ally jamming signals, to identify
and synchronize with multiple ally jammers. This paper
also reports the analysis, implementation, and experimental
evaluation of ally friendly jamming on a software defined
radio platform. Both the analytical and experimental results
indicate that the proposed techniques can effectively disable
enemy wireless communication and at the same time maintain
wireless communication between authorized devices.

Keywords-Wireless; friendly jamming; interference cancella-

tion

I. |,Non-data
p553-dimitrov.pdf,|
This paper extends the choice available for secure real num-
ber implementations with two new contributions. We will
consider the numbers represented in form a  b where 
is the golden ratio, and in form (1)s  2e where e is a
fixed-point number. We develop basic arithmetic operations
together with some frequently used elementary functions.
All the operations are implemented and benchmarked on
Sharemind secure multi-party computation framework. It
turns out that the new proposals provide viable alternatives
to standard floating- and fixed-point implementations from
the performance/error viewpoint in various settings. How-
ever, the optimal choice still depends on the exact require-
ments of the numerical algorithm to be implemented.

Keywords
Secure fixed- and floating-point arithmetic, privacy-preserving
data analysis, secure computations

1.

|,Non-data
p418-kumaresan.pdf,|
Motivated by the impossibility of achieving fairness in secure com-
putation [Cleve, STOC 1986], recent works study a model of fair-
ness in which an adversarial party that aborts on receiving output is
forced to pay a mutually predefined monetary penalty to every other
party that did not receive the output. These works show how to de-
sign protocols for secure computation with penalties that guaran-
tees that either fairness is guaranteed or that each honest party ob-
tains a monetary penalty from the adversary. Protocols for this task
are typically designed in an hybrid model where parties have access
to a claim-or-refund transaction functionality denote F

In this work, we obtain improvements on the efficiency of these
constructions by amortizing the cost over multiple executions of
secure computation with penalties. More precisely, for computa-
tional security parameter , we design a protocol that implements
(cid:96) = poly() instances of secure computation with penalties where
the total number of calls to F
Keywords: Secure computation, fairness, Bitcoin, amortization.

CR is independent of (cid:96).

CR.

1.

|,Non-data
a0b46f4325d28d6e02f4ebbf70f40a1263a5.pdf,|
The results of botnet detection methods are usually presented without any comparison. Although
it is generally accepted that more comparisons with third-party methods may help to improve the
area, few papers could do it. Among the factors that prevent a comparison are the difficulties to
share a dataset, the lack of a good dataset, the absence of a proper description of the methods and
the lack of a comparison methodology. This paper compares the output of three different botnet
detection methods by executing them over a new, real, labeled and large botnet dataset. This
dataset includes botnet, normal and background traffic. The results of our two methods (BClus
and CAMNEP) and BotHunter were compared using a methodology and a novel error metric
designed for botnet detections methods. We conclude that comparing methods indeed helps to
better estimate how good the methods are, to improve the algorithms, to build better datasets and
to build a comparison methodology.

Keywords: Botnet detection, Malware detection, Methods comparison, Botnet dataset, Anomaly

detection, Network traffic

1. |,Data
p1216-yang.pdf,|
Mnemonic strategy has been recommended to help users generate
secure and memorable passwords. We evaluated the security of 6
mnemonic strategy variants in a series of online studies involving
5, 484 participants. In addition to applying the standard method
of using guess numbers or similar metrics to compare the gen-
erated passwords, we also measured the frequencies of the most
commonly chosen sentences as well as the resulting passwords.
While metrics similar to guess numbers suggested that all variants
provided highly secure passwords, statistical metrics told a differ-
ent story.
In particular, differences in the exact instructions had
a tremendous impact on the security level of the resulting pass-
words. We examined the mental workload and memorability of 2
mnemonic strategy variants in another online study with 752 par-
ticipants. Although perceived workloads for the mnemonic strategy
variants were higher than that for the control group where no strat-
egy is required, no significant reduction in password recall after 1
week was obtained.

1.

|,Data
06547127.pdf,|Order-preserving

encryption
scheme where the sort order of ciphertexts matches the sort
order of the corresponding plaintextsallows databases and
other applications to process queries involving order over
encrypted data efficiently. The ideal security guarantee for
order-preserving encryption put forth in the literature is for
the ciphertexts to reveal no information about the plaintexts
besides order. Even though more than a dozen schemes were
proposed, all these schemes leak more information than order.
This paper presents the first order-preserving scheme that
achieves ideal security. Our main technique is mutable cipher-
texts, meaning that over time, the ciphertexts for a small
number of plaintext values change, and we prove that mutable
ciphertexts are needed for ideal security. Our resulting protocol
is interactive, with a small number of interactions.

We implemented our scheme and evaluated it on mi-
crobenchmarks and in the context of an encrypted MySQL
database application. We show that in addition to providing
ideal security, our scheme achieves 12 orders of magnitude
higher performance than the state-of-the-art order-preserving
encryption scheme, which is less secure than our scheme.

Keywords-order-preserving encryption, encoding

I. |,Non-data
p779-holzinger.pdf,|
When created, the Java platform was among the first run-
times designed with security in mind. Yet, numerous Java
versions were shown to contain far-reaching vulnerabilities,
permitting denial-of-service attacks or even worse allowing
intruders to bypass the runtimes sandbox mechanisms, open-
ing the host system up to many kinds of further attacks.

This paper presents a systematic in-depth study of 87 pub-
licly available Java exploits found in the wild. By collecting,
minimizing and categorizing those exploits, we identify their
commonalities and root causes, with the goal of determining
the weak spots in the Java security architecture and possible
countermeasures.

Our findings reveal that the exploits heavily rely on a
set of nine weaknesses, including unauthorized use of re-
stricted classes and confused deputies in combination with
caller-sensitive methods. We further show that all attack
vectors implemented by the exploits belong to one of three
categories: single-step attacks, restricted-class attacks, and
information hiding attacks.

The analysis allows us to propose ideas for improving the

security architecture to spawn further research in this area.

1.

|,Data
sec14-paper-durumeric.pdf,|

While it is widely known that port scanning is widespread,
neither the scanning landscape nor the defensive reactions
of network operators have been measured at Internet scale.
In this work, we analyze data from a large network tele-
scope to study scanning activity from the past year, un-
covering large horizontal scan operations and identifying
broad patterns in scanning behavior. We present an analy-
sis of who is scanning, what services are being targeted,
and the impact of new scanners on the overall landscape.
We also analyze the scanning behavior triggered by recent
vulnerabilities in Linksys routers, OpenSSL, and NTP.
We empirically analyze the defensive behaviors that orga-
nizations employ against scanning, shedding light on who
detects scanning behavior, which networks blacklist scan-
ning, and how scan recipients respond to scans conducted
by researchers. We conclude with recommendations for
institutions performing scans and with implications of
recent changes in scanning behavior for researchers and
network operators.

1

|,Data
p1663-zhang.pdf,|
ION is a unified memory management interface for Android
that is widely used on virtually all ARM based Android de-
vices. ION attempts to achieve several ambitious goals that
have not been simultaneously achieved before (not even on
Linux). Different from managing regular memory in the
system, ION is designed to share and manage memory with
special constraints, e.g., physically contiguous memory. De-
spite the great flexibility and performance benefits offered,
such a critical subsystem, as we discover, unfortunately has
flawed security assumptions and designs.

In this paper, we systematically analyze ION related vul-
nerabilities from conceptual root causes to detailed imple-
mentation decisions. Since ION is often customized heavily
for different Android devices, the specific vulnerabilities of-
ten manifest themselves differently. By conducting a range
of runtime testing as well as static analysis, we are able to
uncover a large number of serious vulnerabilities on the lat-
est Android devices (e.g., Nexus 6P running Android 6.0 and
7.0 preview) such as denial-of-service and dumping memo-
ry from the system and arbitrary applications (e.g., email
content, passwords). Finally, we offer suggestions on how to
redesign the ION subsystem to eliminate these flaws. We
believe that the lessons learned can help guide the future
design of similar memory management subsystems.

1.

|,Data
06547118.pdf,|

We present the design, security proof, and implementation
of an anonymous subscription service. Users register for the
service by providing some form of identity, which might or
might not be linked to a real-world identity such as a credit
card, a web login, or a public key. A user logs on to the system
by presenting a credential derived from information received
at registration. Each credential allows only a single login in
any authentication window, or epoch. Logins are anonymous
in the sense that the service cannot distinguish which user
is logging in any better than random guessing. This implies
unlinkability of a user across different logins.

We find that a central tension in an anonymous subscription
service is the service providers desire for a long epoch (to
reduce server-side computation) versus users desire for a short
epoch (so they can repeatedly re-anonymize their sessions).
We balance this tension by having short epochs, but adding an
efficient operation for clients who do not need unlinkability to
cheaply re-authenticate themselves for the next time period.
We measure performance of a research prototype of our pro-
tocol that allows an independent service to offer anonymous
access to existing services. We implement a music service, an
Android-based subway-pass application, and a web proxy, and
show that adding anonymity adds minimal client latency and
only requires 33 KB of server memory per active user.

I. |,Non-data
sec14-paper-heuser.pdf,|

Android, iOS, and Windows 8 are changing the applica-
tion architecture of consumer operating systems. These
new architectures required OS designers to rethink secu-
rity and access control. While the new security archi-
tectures improve on traditional desktop and server OS
designs, they lack sufficient protection semantics for dif-
ferent classes of OS customers (e.g., consumer, enter-
prise, and government). The Android OS in particular
has seen over a dozen research proposals for security
enhancements. This paper seeks to promote OS secu-
rity extensibility in the Android OS. We propose the An-
droid Security Modules (ASM) framework, which pro-
vides a programmable interface for defining new refer-
ence monitors for Android. We drive the ASM design by
studying the authorization hook requirements of recent
security enhancement proposals and identify that new
OSes such as Android require new types of authorization
hooks (e.g., replacing data). We describe the design and
implementation of ASM and demonstrate its utility by
developing reference monitors called ASM apps. Finally,
ASM is not only beneficial for security researchers. If
adopted by Google, we envision ASM enabling in-the-
field security enhancement of Android devices without
requiring root access, a significant limitation of existing
bring-your-own-device solutions.

1 |,Non-data
p1505-fan.pdf,|
In this work, we give a lattice attack on the ECDSA imple-
mentation in the latest version of OpenSSL, which imple-
ment the scalar multiplication by windowed Non-Adjacent
Form method. We propose a totally di(cid:11)erent but more ef-
(cid:12)cient method of extracting and utilizing information from
the side-channel results, remarkably improving the previous
attacks. First, we develop a new e(cid:14)cient method, which can
extract almost all information from the side-channel results,
obtaining 105.8 bits of information per signature on average
for 256-bit ECDSA. Then in order to make the utmost of our
extracted information, we translate the problem of recover-
ing secret key to the Extended Hidden Number Problem,
which can be solved by lattice reduction algorithms. Final-
ly, we introduce the methods of elimination, merging, most
signi(cid:12)cant digit recovering and enumeration to improve the
attack. Our attack is mounted to the secp256k1 curve, and
the result shows that only 4 signatures would be enough to
recover the secret key if the Flush+Reload attack is im-
plemented perfectly without any error,which is much better
than the best known result needing at least 13 signatures.

CCS Concepts
Security and privacy  Digital signatures; Crypt-
analysis and other attacks; Side-channel analysis and
countermeasures;

Keywords
ECDSA; OpenSSL; lattice attack; windowed Non-Adjacent
Form; Extended Hidden Number Problem; Flush+Reload
attack

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978400

1.

|,Non-data
p204-ringer.pdf,|
User-driven access control improves the coarse-grained ac-
cess control of current operating systems (particularly in
the mobile space) that provide only all-or-nothing access to
a resource such as the camera or the current location. By
granting appropriate permissions only in response to explicit
user actions (for example, pressing a camera button), user-
driven access control better aligns application actions with
user expectations. Prior work on user-driven access con-
trol has relied in essential ways on operating system (OS)
modifications to provide applications with uncompromisable
access control gadgets, distinguished user interface (UI) ele-
ments that can grant access permissions.

This work presents a design, implementation, and evalu-
ation of user-driven access control that works with no OS
modifications, thus making deployability and incremental
adoption of the model more feasible. We develop (1) a user-
level trusted library for access control gadgets, (2) static
analyses to prevent malicious creation of UI events,
ille-
gal flows of sensitive information, and circumvention of our
library, and (3) dynamic analyses to ensure users are not
tricked into granting permissions. In addition to providing
the original user-driven access control guarantees, we use
static information flow to limit where results derived from
sensitive sources may flow in an application.

Our implementation targets Android applications. We
port open-source applications that need interesting resource
permissions to use our system. We determine in what ways
user-driven access control in general and our implementa-
tion in particular are good matches for real applications.
We demonstrate that our system is secure against a variety
of attacks that malware on Android could otherwise mount.

1.

|,Data
sec15-paper-hu.pdf,|

As defense solutions against control-flow hijacking at-
tacks gain wide deployment, control-oriented exploits
from memory errors become difficult. As an alterna-
tive, attacks targeting non-control data do not require
diverting the applications control flow during an at-
tack. Although it is known that such data-oriented at-
tacks can mount significant damage, no systematic meth-
ods to automatically construct them from memory er-
rors have been developed.
In this work, we develop a
new technique called data-flow stitching, which system-
atically finds ways to join data flows in the program to
generate data-oriented exploits. We build a prototype
embodying our technique in a tool called FLOWSTITCH
that works directly on Windows and Linux binaries. In
our experiments, we find that FLOWSTITCH automati-
cally constructs 16 previously unknown and three known
data-oriented attacks from eight real-world vulnerable
programs. All the automatically-crafted exploits respect
fine-grained CFI and DEP constraints, and 10 out of the
19 exploits work with standard ASLR defenses enabled.
The constructed exploits can cause significant damage,
such as disclosure of sensitive information (e.g., pass-
words and encryption keys) and escalation of privilege.

1

|,Data
sec14-paper-soska.pdf,|

Significant recent research advances have made it possi-
ble to design systems that can automatically determine
with high accuracy the maliciousness of a target website.
While highly useful, such systems are reactive by nature.
In this paper, we take a complementary approach, and at-
tempt to design, implement, and evaluate a novel classi-
fication system which predicts, whether a given, not yet
compromised website will become malicious in the fu-
ture. We adapt several techniques from data mining and
machine learning which are particularly well-suited for
this problem. A key aspect of our system is that the set
of features it relies on is automatically extracted from the
data it acquires; this allows us to be able to detect new
attack trends relatively quickly. We evaluate our imple-
mentation on a corpus of 444,519 websites, containing
a total of 4,916,203 webpages, and show that we man-
age to achieve good detection accuracy over a one-year
horizon; that is, we generally manage to correctly predict
that currently benign websites will become compromised
within a year.
1
Online criminal activities take many different forms,
ranging from advertising counterfeit goods through spam
email [21],
to hosting drive-by-downloads services
[29] that surreptitiously install malicious software (mal-
ware) on the victim machine, to distributed denial-of-
service attacks [27], to only name a few. Among those,
research on analysis and classification of end-host mal-
ware  which allows an attacker to take over the vic-
tims computer for a variety of purposes  has been a
particularly active field for years (see, e.g., [6, 7, 16]
among many others). More recently, a number of stud-
ies [8,15,20,22,36] have started looking into webserver
malware, where, instead of targeting arbitrary hosts for
compromise, the attacker attempts to inject code on ma-
chines running web servers. Webserver malware dif-
fers from end-host malware in its design and objectives.

|,Data
p530-kong.pdf,|
there
Along with the increasing popularity of mobile devices,
exist severe security and privacy concerns for mobile apps. On
Google Play, user reviews provide a unique understanding of
security/privacy issues of mobile apps from users perspective,
and in fact they are valuable feedbacks from users by considering
users expectations. To best assist the end users, in this paper, we
automatically learn the security/privacy related behaviors inferred
from analysis on user reviews, which we call review-to-behavior
fidelity. We design the system AUTOREB that automatically
assesses the review-to-behavior fidelity of mobile apps. AUTOREB
employs the state-of-the-art machine learning techniques to
infer the relations between users reviews and four categories of
security-related behaviors. Moreover, it uses a crowdsourcing
approach to automatically aggregate the security issues from
review-level to app-level. To our knowledge, AUTOREB is the first
work that explores the user review information and utilizes the
review semantics to predict the risky behaviors at both review-level
and app-level.

We crawled a real-world dataset of 2, 614, 186 users, 12, 783
apps and 13, 129, 783 reviews from Google play, and use it to
comprehensively evaluate AUTOREB. The experiment result shows
that our method can predict the mobile app behaviors at user-review
level with accuracy as high as 94.05%, and also it can predict
the security issues at app-level by aggregating the predictions at
review-level. Our research offers an insight into understanding the
mobile app security concerns from users perspective, and helps
bridge the gap between the security issues and users perception.

Categories and Subject Descriptors
D.4.6 [Operating System]:
[Artificial Intelligence]: Learning

Security and Protection;

I.2.6

General Terms
Security, Privacy, Algorithm

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS15, October 1216, 2015, Denver, Colorado, USA.

c(cid:2) 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.

DOI: http://dx.doi.org/10.1145/2810103.2813689.

User reviews

AUTO
REB

App 

Behaviors

Spamming, Ads
Financial issue

over-claimed permission
Data leakage  

....
......

AUTOREB

ReviewlevelSecurity
BehaviorInference

ApplevelSecurity
BehaviorInference

Figure 1:
Infer the security-related behaviors from users reviews.
Overview of the framework of AUTOREB : (a) Engine 1: Review-level
security behavior inference engine;
(b) Engine 2: App-level security
behavior inference engine.

Keywords
Android; App; Risk; Information Retrieval; Perception; Learning

1.

|,Data
sec14-paper-kirat.pdf,|

1

|,Data
p1614-chaidos.pdf,|
We propose a new voting scheme, BeleniosRF, that offers both
receipt-freeness and end-to-end verifiability. It is receipt-free
in a strong sense, meaning that even dishonest voters cannot
prove how they voted. We provide a game-based definition
of receipt-freeness for voting protocols with non-interactive
ballot casting, which we name strong receipt-freeness (sRF).
To our knowledge, sRF is the first game-based definition
of receipt-freeness in the literature, and it has the merit
of being particularly concise and simple. Built upon the
Helios protocol, BeleniosRF inherits its simplicity and does
not require any anti-coercion strategy from the voters. We
implement BeleniosRF and show its feasibility on a number
of platforms, including desktop computers and smartphones.

1.

|,Non-data
sec14-paper-egele.pdf,|
Matching function binariesthe process of identifying
similar functions among binary executablesis a chal-
lenge that underlies many security applications such as
malware analysis and patch-based exploit generation. Re-
cent work tries to establish semantic similarity based on
static analysis methods. Unfortunately, these methods do
not perform well if the compared binaries are produced
by different compiler toolchains or optimization levels. In
this work, we propose blanket execution, a novel dynamic
equivalence testing primitive that achieves complete cov-
erage by overriding the intended program logic. Blanket
execution collects the side effects of functions during
execution under a controlled randomized environment.
Two functions are deemed similar, if their corresponding
side effects, as observed under the same environment, are
similar too.

We implement our blanket execution technique in a sys-
tem called BLEX. We evaluate BLEX rigorously against
the state of the art binary comparison tool BinDiff. When
comparing optimized and un-optimized executables from
the popular GNU coreutils package, BLEX outperforms
BinDiff by up to 3.5 times in correctly identifying similar
functions. BLEX also outperforms BinDiff if the binaries
have been compiled by different compilers. Using the
functionality in BLEX, we have also built a binary search
engine that identifies similar functions across optimiza-
tion boundaries. Averaged over all indexed functions, our
search engine ranks the correct matches among the top
ten results 77% of the time.
1 |,Data
sec14-paper-bhoraskar.pdf,|
We present an app automation tool called Brahmastra for
helping app stores and security researchers to test third-
party components in mobile apps at runtime. The main
challenge is that call sites that invoke third-party code
may be deeply embedded in the app, beyond the reach
of traditional GUI testing tools. Our approach uses static
analysis to construct a page transition graph and discover
execution paths to invoke third-party code. We then per-
form binary rewriting to jump start the third-party code
by following the execution path, efficiently pruning out
undesired executions. Compared with the state-of-the-
art GUI testing tools, Brahmastra is able to successfully
analyse third-party code in 2.7 more apps and decrease
test duration by a factor of 7. We use Brahmastra to un-
cover interesting results for two use cases: 175 out of
220 childrens apps we tested display ads that point to
web pages that attempt to collect personal information,
which is a potential violation of the Childrens Online
Privacy Protection Act (COPPA); and 13 of the 200 apps
with the Facebook SDK that we tested are vulnerable to
a known access token attack.
1
Third-party libraries provide a convenient way for mo-
bile application developers to integrate external services
in the application code base. Advertising that is widely
featured in free applications is one example: 95% of
114,000 popular Android applications contain at least
one known advertisement library according to a recent
study [22]. Social media add-ons that streamline or en-
rich the user experience are another popular family of
third-party components. For example, Facebook Login
lets applications authenticate users with their existing
Facebook credentials, and post content to their feed.

|,Data
p380-jang.pdf,|
Kernel hardening has been an important topic since many applica-
tions and security mechanisms often consider the kernel as part of
their Trusted Computing Base (TCB). Among various hardening
techniques, Kernel Address Space Layout Randomization (KASLR)
is the most effective and widely adopted defense mechanism that
can practically mitigate various memory corruption vulnerabilities,
such as buffer overflow and use-after-free. In principle, KASLR
is secure as long as no memory leak vulnerability exists and high
entropy is ensured.

In this paper, we introduce a highly stable timing attack against
KASLR, called DrK, that can precisely de-randomize the mem-
ory layout of the kernel without violating any such assumptions.
DrK exploits a hardware feature called Intel Transactional Synchro-
nization Extension (TSX) that is readily available in most modern
commodity CPUs. One surprising behavior of TSX, which is es-
sentially the root cause of this security loophole, is that it aborts a
transaction without notifying the underlying kernel even when the
transaction fails due to a critical error, such as a page fault or an
access violation, which traditionally requires kernel intervention.
DrK turned this property into a precise timing channel that can
determine the mapping status (i.e., mapped versus unmapped) and
execution status (i.e., executable versus non-executable) of the priv-
ileged kernel address space. In addition to its surprising accuracy
and precision, DrK is universally applicable to all OSes, even in
virtualized environments, and generates no visible footprint, making
it difficult to detect in practice. We demonstrated that DrK can break
the KASLR of all major OSes (i.e., Windows, Linux, and OS X)
with near-perfect accuracy in under a second. Finally, we propose
potential countermeasures that can effectively prevent or mitigate
the DrK attack.

We urge our community to be aware of the potential threat of
having Intel TSX, which is present in most recent Intel CPUs100%
in workstation and 60% in high-end Intel CPUs since Skylakeand
is even available on Amazon EC2 (X1).

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24 - 28, 2016, Vienna, Austria
c 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978321

Figure 1: The adoption status of both user-space and kernel-space ASLR in
popular operating systems, ordered by year [62].

1.

|,Non-data
p1353-grubbs.pdf,|
We develop a systematic approach for analyzing client-server
applications that aim to hide sensitive user data from un-
trusted servers. We then apply it to Mylar, a framework
that uses multi-key searchable encryption (MKSE) to build
Web applications on top of encrypted data.

We demonstrate that (1) the Popa-Zeldovich model for
MKSE does not imply security against either passive or ac-
tive attacks; (2) Mylar-based Web applications reveal users
data and queries to passive and active adversarial servers;
and (3) Mylar is generically insecure against active attacks
due to system design flaws. Our results show that the prob-
lem of securing client-server applications against actively
malicious servers is challenging and still unsolved.

We conclude with general lessons for the designers of sys-
tems that rely on property-preserving or searchable encryp-
tion to protect data from untrusted servers.

1.

|,Data
p690-ruef.pdf,|
Typical security contests focus on breaking or mitigating the
impact of buggy systems. We present the Build-it, Break-it,
Fix-it (BIBIFI) contest, which aims to assess the ability to
securely build software, not just break it. In BIBIFI, teams
build specified software with the goal of maximizing correct-
ness, performance, and security. The latter is tested when
teams attempt to break other teams submissions. Win-
ners are chosen from among the best builders and the best
breakers. BIBIFI was designed to be open-endedteams
can use any language, tool, process, etc. that they like. As
such, contest outcomes shed light on factors that correlate
with successfully building secure software and breaking inse-
cure software. During 2015, we ran three contests involving
a total of 116 teams and two different programming prob-
lems. Quantitative analysis from these contests found that
the most efficient build-it submissions used C/C++, but
submissions coded in other statically-typed languages were
less likely to have a security flaw; build-it teams with di-
verse programming-language knowledge also produced more
secure code. Shorter programs correlated with better scores.
Break-it teams that were also successful build-it teams were
significantly better at finding security bugs.

1.

|,Data
sec14-paper-dautrich.pdf,|
We present Burst ORAM, the first oblivious cloud stor-
age system to achieve both practical response times
and low total bandwidth consumption for bursty work-
loads. For real-world workloads, Burst ORAM can at-
tain response times that are nearly optimal and orders
of magnitude lower than the best existing ORAM sys-
tems by reducing online bandwidth costs and aggres-
sively rescheduling shuffling work to delay the bulk of
the IO until idle periods.

We evaluate our design on an enterprise file system
trace with about 7,500 clients over a 15 day period,
comparing to an insecure baseline encrypted block store
without ORAM. We show that when baseline response
times are low, Burst ORAM response times are compa-
rably low. In a 32TB ORAM with 50ms network latency
and sufficient bandwidth capacity to ensure 90% of re-
quests have baseline response times under 53ms, 90% of
Burst ORAM requests have response times under 63ms,
while requiring only 30 times the total bandwidth con-
sumption of the insecure baseline. Similarly, with suffi-
cient bandwidth to ensure 99.9% of requests have base-
line responses under 70ms, 99.9% of Burst ORAM re-
quests have response times under 76ms.
1
Cloud computing allows customers to outsource the bur-
den of data management and benefit from economy of
scale, but privacy concerns hinder its growth [3]. En-
cryption alone is insufficient to ensure privacy in storage
outsourcing applications, as information about the con-
tents of encrypted records may still leak via data access
patterns. Existing work has shown that access patterns
on an encrypted email repository may leak sensitive key-
word search queries [12], and that accesses to encrypted
database tuples may reveal ordering information [5].

|,Data
sec14-paper-bao.pdf,|
Function identification is a fundamental challenge in re-
verse engineering and binary program analysis. For in-
stance, binary rewriting and control flow integrity rely on
accurate function detection and identification in binaries.
Although many binary program analyses assume func-
tions can be identified a priori, identifying functions in
stripped binaries remains a challenge.

In this paper, we propose BYTEWEIGHT, a new au-
tomatic function identification algorithm. Our approach
automatically learns key features for recognizing func-
tions and can therefore easily be adapted to different
platforms, new compilers, and new optimizations. We
evaluated our tool against three well-known tools that
feature function identification: IDA, BAP, and Dyninst.
Our data set consists of 2,200 binaries created with three
different compilers, with four different optimization lev-
els, and across two different operating systems. In our
experiments with 2,200 binaries, we found that BYTE-
WEIGHT missed 44,621 functions in comparison with the
266,672 functions missed by the industry-leading tool
IDA. Furthermore, while IDA misidentified 459,247 func-
tions, BYTEWEIGHT misidentified only 43,992 functions.
1
Binary analysis is an essential security capability with
extensive applications, including protecting binaries with
control flow integrity (CFI) [1], extracting binary code
sequences from malware [9], and hot patching vulnerabil-
ities [25]. Research interest in binary analysis shows no
sign of waning. In 2013 alone, several papers such as CFI
for COTS [34] (referred to as COTS-CFI in this paper),
the Rendezvous search engine for binaries [21], and the
Phoenix decompiler [28] focus on developing new binary
analysis techniques.

|,Data
p743-abera.pdf,|
Remote attestation is a crucial security service particularly relevant
to increasingly popular IoT (and other embedded) devices. It al-
lows a trusted party (verifier) to learn the state of a remote, and
potentially malware-infected, device (prover). Most existing ap-
proaches are static in nature and only check whether benign soft-
ware is initially loaded on the prover. However, they are vulnerable
to runtime attacks that hijack the applications control or data flow,
e.g., via return-oriented programming or data-oriented exploits.

As a concrete step towards more comprehensive runtime remote
attestation, we present the design and implementation of Control-
FLow ATtestation (C-FLAT) that enables remote attestation of an
applications control-flow path, without requiring the source code.
We describe a full prototype implementation of C-FLAT on Rasp-
berry Pi using its ARM TrustZone hardware security extensions.
We evaluate C-FLATs performance using a real-world embedded
(cyber-physical) application, and demonstrate its efficacy against
control-flow hijacking attacks.

Keywords
remote attestation; control-flow attacks; embedded system security

|,Non-data
p92-wang.pdf,|
Android is the most commonly used mobile device opera-
tion system. The core of Android, the System Server (SS),
is a multi-threaded process that provides most of the system
services. Based on a new understanding of the security risks
introduced by the callback mechanism in system services,
we have discovered a general type of design flaw. A vulner-
ability detection tool has been designed and implemented
based on static taint analysis.We applied the tool on all the
80 system services in the SS of Android 5.1.0. With its help,
we have discovered six previously unknown vulnerabilities,
which are further confirmed on Android 2.3.7-6.0.1. Accord-
ing to our analysis, about 97.3% of the entire 1.4 billion real-
world Android devices are vulnerable. Our proof-of-concept
attack proves that the vulnerabilities can enable a malicious
app to freeze critical system functionalities or soft-reboot the
system immediately. It is a neat type of denial-of-service at-
tack. We also proved that the attacks can be conducted at
mission critical moments to achieve meaningful goals, such
as anti anti-virus, anti process-killer, hindering app updates
or system patching. After being informed, Google confirmed
our findings promptly. Several suggestions on how to use
callbacks safely are also proposed to Google.

Keywords
Mobile Security; Denial of Service; Vulnerability Detection;
Synchronous Callback; Taint Analysis

1.

|,Data
sec14-paper-shi.pdf,|
Malware analysis relies heavily on the use of virtual
machines for functionality and safety. There are subtle
differences in operation between virtual machines and
physical machines. Contemporary malware checks for
these differences to detect that it is being run in a vir-
tual machine, and modifies its behavior to thwart being
analyzed by the defenders. Existing approaches to un-
cover these differences use randomized testing, or mal-
ware analysis, and cannot guarantee completeness.

In this paper we propose Cardinal Pill Testinga
modification of Red Pill Testing [21] that aims to enu-
merate the differences between a given VM and a phys-
ical machine, through carefully designed tests. Cardinal
Pill Testing finds five times more pills by running fif-
teen times fewer tests than Red Pill Testing. We further
examine the causes of pills and find that, while the ma-
jority of them stem from the failure of virtual machines
to follow CPU design specifications, a significant num-
ber stem from under-specification of the effects of certain
instructions by the Intel manual. This leads to divergent
implementations in different CPU and virtual machine
architectures. Cardinal Pill Testing successfully enumer-
ates differences that stem from the first cause, but only
exhaustive testing or an understanding of implementa-
tion semantics can enumerate those that stem from the
second cause. Finally, we sketch a method to hide pills
from malware by systematically correcting their outputs
in the virtual machine.

1 |,Non-data
06547121.pdf,|The balance between coercion-resistance, election
verifiability and usability remains unresolved in remote elec-
tronic voting despite significant research over the last few years.
We propose a change of perspective, replacing the requirement
of coercion-resistance with a new requirement of coercion-
evidence: there should be public evidence of the amount of
coercion that has taken place during a particular execution of
the voting system.

We provide a formal definition of coercion-evidence that
has two parts. Firstly, there should be a coercion-evidence test
that can be performed against the bulletin board to accurately
determine the degree of coercion that has taken place in any
given run. Secondly, we require coercer independence, that is
the ability of the voter to follow the protocol without being
detected by the coercer.

To show how coercion-evidence can be achieved, we propose
a new remote voting scheme, Caveat Coercitor, and we prove
that it satisfies coercion-evidence. Moreover, Caveat Coercitor
makes weaker trust assumptions than other remote voting
systems, such as JCJ/Civitas and Helios, and has better
usability properties.

Keywords-Coercion resistance; coercion evidence; electronic
voting; verifiable elections; security protocols; security models;
usability

I. |,Non-data
p641-alhuzali.pdf,|
We tackle the problem of automated exploit generation for
web applications. In this regard, we present an approach
that significantly improves the state-of-art in web injection
vulnerability identification and exploit generation. Our ap-
proach for exploit generation tackles various challenges as-
sociated with typical web application characteristics: their
multi-module nature, interposed user input, and multi-tier
architectures using a database backend. Our approach de-
velops precise models of application workflows, database
schemas, and native functions to achieve high quality exploit
generation. We implemented our approach in a tool called
Chainsaw. Chainsaw was used to analyze 9 open source
applications and generated over 199 first- and second-order
injection exploits combined, significantly outperforming sev-
eral related approaches.

Keywords
Exploit generation; Web security; Injection vulnerabilities

1.

|,Data
06547129.pdf,|Several techniques in computer security, including
generic protocols for secure computation and symbolic exe-
cution, depend on implementing algorithms in static circuits.
Despite substantial improvements in recent years, tools built
using these techniques remain too slow for most practical
uses. They require transforming arbitrary programs into either
Boolean logic circuits, constraint sets on Boolean variables, or
other equivalent representations, and the costs of using these
tools scale directly with the size of the input circuit. Hence,
techniques for more efficient circuit constructions have benefits
across these tools. We show efficient circuit constructions for
various simple but commonly used data structures including
stacks, queues, and associative maps. While current practice
requires effectively copying the entire structure for each oper-
ation, our techniques take advantage of locality and batching
to provide amortized costs that scale polylogarithmically in
the size of the structure. We demonstrate how many common
array usage patterns can be significantly improved with the
help of these circuit structures. We report on experiments
using our circuit structures for both generic secure computation
using garbled circuits and automated test input generation
using symbolic execution, and demonstrate order of magnitude
improvements for both applications.

I. |,Non-data
p717-backes.pdf,|
Automatically analyzing information flow within Android
applications that rely on cryptographic operations with their
computational security guarantees imposes formidable chal-
lenges that existing approaches for understanding an apps
behavior struggle to meet. These approaches do not distin-
guish cryptographic and non-cryptographic operations, and
hence do not account for cryptographic protections: f (m)
is considered sensitive for a sensitive message m irrespective
of potential secrecy properties offered by a cryptographic
operation f . These approaches consequently provide a safe
approximation of the apps behavior, but they mistakenly
classify a large fraction of apps as potentially insecure and
consequently yield overly pessimistic results.

In this paper, we show how cryptographic operations can
be faithfully included into existing approaches for automated
app analysis. To this end, we first show how cryptographic
operations can be expressed as symbolic abstractions within
the comprehensive Dalvik bytecode language. These ab-
stractions are accessible to automated analysis and can be
conveniently added to existing app analysis tools using mi-
nor changes in their semantics. Second, we show that our
abstractions are faithful by providing the first computational
soundness result for Dalvik bytecode, i.e., the absence of at-
tacks against our symbolically abstracted program entails
the absence of any attacks against a suitable cryptographic
program realization. We cast our computational soundness
result in the CoSP framework, which makes the result mod-
ular and composable.

Keywords
Android, Computational Soundness, Secure Information
Flow

1.

|,Non-data
p1365-calzavara.pdf,|
Content Security Policy (CSP) is an emerging W3C stan-
dard introduced to mitigate the impact of content injection
vulnerabilities on websites. We perform a systematic, large-
scale analysis of four key aspects that impact on the ef-
fectiveness of CSP: browser support, website adoption, cor-
rect configuration and constant maintenance. While browser
support is largely satisfactory, with the exception of few no-
table issues, our analysis unveils several shortcomings rela-
tive to the other three aspects. CSP appears to have a rather
limited deployment as yet and, more crucially, existing poli-
cies exhibit a number of weaknesses and misconfiguration
errors. Moreover, content security policies are not regularly
updated to ban insecure practices and remove unintended
security violations. We argue that many of these problems
can be fixed by better exploiting the monitoring facilities of
CSP, while other issues deserve additional research, being
more rooted into the CSP design.

1.

|,Data
06547132.pdf,|The web has become an essential part of our
society and is currently the main medium of information
delivery. Billions of users browse the web on a daily basis, and
there are single websites that have reached over one billion
user accounts. In this environment, the ability to track users
and their online habits can be very lucrative for advertising
companies, yet very intrusive for the privacy of users.

In this paper, we examine how web-based device fingerprint-
ing currently works on the Internet. By analyzing the code
of three popular browser-fingerprinting code providers, we
reveal the techniques that allow websites to track users without
the need of client-side identifiers. Among these techniques, we
show how current commercial fingerprinting approaches use
questionable practices, such as the circumvention of HTTP
proxies to discover a users real IP address and the installation
of intrusive browser plugins.

At the same time, we show how fragile the browser ecosystem
is against fingerprinting through the use of novel browser-
identifying techniques. With so many different vendors involved
in browser development, we demonstrate how one can use
diversions in the browsers
implementation to distinguish
successfully not only the browser-family, but also specific major
and minor versions. Browser extensions that help users spoof
the user-agent of their browsers are also evaluated. We show
that current commercial approaches can bypass the extensions,
and, in addition, take advantage of their shortcomings by using
them as additional fingerprinting features.

I. |,Data
p1032-bohme.pdf,|
Coverage-based Greybox Fuzzing (CGF) is a random testing
approach that requires no program analysis. A new test
is generated by slightly mutating a seed input. If the test
exercises a new and interesting path, it is added to the set of
seeds; otherwise, it is discarded. We observe that most tests
exercise the same few high-frequency paths and develop
strategies to explore significantly more paths with the same
number of tests by gravitating towards low-frequency paths.
We explain the challenges and opportunities of CGF using
a Markov chain model which specifies the probability that
fuzzing the seed that exercises path i generates an input
that exercises path j. Each state (i.e., seed) has an energy
that specifies the number of inputs to be generated from that
seed. We show that CGF is considerably more e cient if en-
ergy is inversely proportional to the density of the stationary
distribution and increases monotonically every time that
seed is chosen. Energy is controlled with a power schedule.
We implemented the exponential schedule by extending
AFL. In 24 hours, AFLFast exposes 3 previously unreported
CVEs that are not exposed by AFL and exposes 6 previously
unreported CVEs 7x faster than AFL. AFLFast produces at
least an order of magnitude more unique crashes than AFL.
CCS Concepts:
Security and privacy!Vulnerability scanners; Software and
its engineering!Software testing and debugging;
1.

|,Non-data
p843-evtyushkin.pdf,|
Covert channels present serious security threat because they
allow secret communication between two malicious pro-
cesses even if the system inhibits direct communication.
We describe, implement and quantify a new covert channel
through shared hardware random number generation (RNG)
module that is available on modern processors. We demon-
strate that a reliable, high-capacity and low-error covert
channel can be created through the RNG module that works
across CPU cores and across virtual machines. We quan-
tify the capacity of the RNG channel under different set-
tings and show that transmission rates in the range of 7-200
kbit/s can be achieved depending on a particular system
used for transmission, assumptions, and the load level. Fi-
nally, we describe challenges in mitigating the RNG channel,
and propose several mitigation approaches both in software
and hardware.

CCS Concepts
Security and privacy  Side-channel analysis and
countermeasures; Security in hardware;

Keywords
Covert channels; Random number generator

1.

|,Non-data
p529-xu.pdf,|
After a program has crashed and terminated abnormally, it typically
leaves behind a snapshot of its crashing state in the form of a core
dump. While a core dump carries a large amount of information,
which has long been used for software debugging, it barely serves
as informative debugging aids in locating software faults, particu-
larly memory corruption vulnerabilities. A memory corruption is a
special type of software fault that may lead to manipulation of the
content at a certain memory. As such, a core dump may contain a
certain amount of corrupted data, which increases the difficulty in
identifying useful debugging information (e.g., a crash point and
stack traces). Without a proper mechanism to deal with this problem,
a core dump can be practically useless for software failure diagnosis.
In this work, we develop CREDAL, an automatic debugging tool
that employs the source code of a crashing program to enhance
core dump analysis and turns a core dump to an informative aid
in tracking down memory corruption vulnerabilities. Specifically,
CREDAL systematically analyzes a potentially corrupted core dump
and identifies the crash point and stack frames. For a core dump
carrying corrupted data, it goes beyond the crash point and stack
trace. In particular, CREDAL further pinpoints the variables hold-
ing corrupted data using the source code of the crashing program
along with the stack frames. To assist software developers (or secu-
rity analysts) in tracking down a memory corruption vulnerability,
CREDAL also performs analysis and highlights the code fragments
corresponding to data corruption.

To demonstrate the utility of CREDAL, we use it to analyze 80
crashes corresponding to 73 memory corruption vulnerabilities
archived in Offensive Security Exploit Database. We show that,
CREDAL can accurately pinpoint the crash point and (fully or par-
tially) restore a stack trace even though a crashing program stack
carries corrupted data. In addition, we demonstrate CREDAL can
potentially reduce the manual effort of finding the code fragment
that is likely to contain memory corruption vulnerabilities.

Keywords
Core Dump; Memory Corruption; Vulnerability Analysis

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978340

1.

|,Data
p1394-gelernter.pdf,|
Cross-site search (XS-search) attacks circumvent the same-
origin policy and extract sensitive information, by using the
time it takes for the browser to receive responses to search
queries. This side-channel is usually considered impractical,
due to the limited attack duration and high variability of
delays. This may be true for naive XS-search attacks; how-
ever, we show that the use of better tools facilitates effective
XS-search attacks, exposing information efficiently and pre-
cisely.

We present and evaluate three types of tools:

(1) ap-
propriate statistical tests, (2) amplification of the timing
side-channel, by inflating communication or computation,
and (3) optimized, tailored divide-and-conquer algorithms,
to identify terms from large dictionaries. These techniques
may be applicable in other scenarios.

We implemented and evaluated the attacks against the
popular Gmail and Bing services, in several environments
and ethical experiments, taking careful, IRB-approved mea-
sures to avoid exposure of personal information.

Categories and Subject Descriptors
J.0 [Computer Applications]: General

Keywords
Side channel attacks; Web; Privacy; Security

1.

|,Data
p1376-weichselbaum.pdf,|
Content Security Policy is a web platform mechanism de-
signed to mitigate cross-site scripting (XSS), the top security
vulnerability in modern web applications [24]. In this paper,
we take a closer look at the practical benefits of adopting
CSP and identify significant flaws in real-world deployments
that result in bypasses in 94.72% of all distinct policies.

We base our Internet-wide analysis on a search engine cor-
pus of approximately 100 billion pages from over 1 billion
hostnames; the result covers CSP deployments on 1,680,867
hosts with 26,011 unique CSP policies  the most compre-
hensive study to date. We introduce the security-relevant
aspects of the CSP specification and provide an in-depth
analysis of its threat model, focusing on XSS protections.
We identify three common classes of CSP bypasses and ex-
plain how they subvert the security of a policy.

We then turn to a quantitative analysis of policies de-
ployed on the Internet in order to understand their secu-
rity benefits. We observe that 14 out of the 15 domains
most commonly whitelisted for loading scripts contain un-
safe endpoints; as a consequence, 75.81% of distinct policies
use script whitelists that allow attackers to bypass CSP. In
total, we find that 94.68% of policies that attempt to limit
script execution are ineffective, and that 99.34% of hosts
with CSP use policies that offer no benefit against XSS.

Finally, we propose the strict-dynamic keyword, an
addition to the specification that facilitates the creation of
policies based on cryptographic nonces, without relying on
domain whitelists. We discuss our experience deploying such
a nonce-based policy in a complex application and provide
guidance to web authors for improving their policies.

Keywords
Content Security Policy; Cross-Site Scripting; Web Security

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
CCS16 October 24-28, 2016, Vienna, Austria
 2016 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-4139-4/16/10.
DOI: http://dx.doi.org/10.1145/2976749.2978363

1.

|,Data
p653-pan.pdf,|
Content security policy (CSP)which has been standardized by
W3C and adopted by all major commercial browsersis one of the
most promising approaches for defending against cross-site script-
ing (XSS) attacks. Although client-side adoption of CSP is suc-
cessful, server-side adoption is far behind the client side: according
to a large-scale survey, less than 0.002% of Alexa Top 1M websites
enabled CSP.

To facilitate the adoption of CSP, we propose CSPAutoGen to
enable CSP in real-time, without server modifications, and being
compatible with real-world websites. Specifically, CSPAutoGen
trains so-called templates for each domain, generates CSPs based
on the templates, rewrites incoming webpages on the fly to apply
those generated CSPs, and then serves those rewritten webpages
to client browsers. CSPAutoGen is designed to automatically en-
force the most secure and strict version of CSP without enabling
unsafe-inline and unsafe-eval, i.e., CSPAutoGen can handle all
the inline and dynamic scripts.

We have implemented a prototype of CSPAutoGen, and our eval-
uation shows that CSPAutoGen can correctly render all the Alexa
Top 50 websites. Moreover, we conduct extensive case studies on
five popular websites, indicating that CSPAutoGen can preserve the
behind-the-login functionalities, such as sending emails and post-
ing comments. Our security analysis shows that CSPAutoGen is
able to defend against all the tested real-world XSS attacks.

1.

|,Data
sec15-paper-caliskan-islam.pdf,|
Source code authorship attribution is a significant pri-
vacy threat to anonymous code contributors. However,
it may also enable attribution of successful attacks from
code left behind on an infected system, or aid in resolv-
ing copyright, copyleft, and plagiarism issues in the pro-
gramming fields. In this work, we investigate machine
learning methods to de-anonymize source code authors
of C/C++ using coding style. Our Code Stylometry Fea-
ture Set is a novel representation of coding style found
in source code that reflects coding style from properties
derived from abstract syntax trees.

Our random forest and abstract syntax tree-based ap-
proach attributes more authors (1,600 and 250) with sig-
nificantly higher accuracy (94% and 98%) on a larger
data set (Google Code Jam) than has been previously
achieved. Furthermore, these novel features are robust,
difficult to obfuscate, and can be used in other program-
ming languages, such as Python. We also find that (i) the
code resulting from difficult programming tasks is easier
to attribute than easier tasks and (ii) skilled programmers
(who can complete the more difficult tasks) are easier to
attribute than less skilled programmers.

1

|,Data
06547099.pdf,|New operating systems, such as the Capsicum
capability system, allow a programmer to write an application
that satisfies strong security properties by invoking security-
specific system calls at a few key points in the program.
However, rewriting an application to invoke such system
calls correctly is an error-prone process: even the Capsicum
developers have reported difficulties in rewriting programs to
correctly invoke system calls.

This paper describes capweave, a tool that takes as input
(i) an LLVM program, and (ii) a declarative policy of the
possibly-changing capabilities that a program must hold during
its execution, and rewrites the program to use Capsicum
system calls to enforce the policy. Our experiments demonstrate
that capweave can be applied to rewrite security-critical
UNIX utilities to satisfy practical security policies. capweave
itself works quickly, and the runtime overhead incurred in
the programs that capweave produces is generally low for
practical workloads.

I. |,Non-data
p308-abadi.pdf,|
Machine learning techniques based on neural networks are
achieving remarkable results in a wide variety of domains.
Often, the training of models requires large, representative
datasets, which may be crowdsourced and contain sensitive
information. The models should not expose private informa-
tion in these datasets. Addressing this goal, we develop new
algorithmic techniques for learning and a refined analysis of
privacy costs within the framework of differential privacy.
Our implementation and experiments demonstrate that we
can train deep neural networks with non-convex objectives,
under a modest privacy budget, and at a manageable cost in
software complexity, training efficiency, and model quality.

1.

|,Data
06547125.pdf,| We present

implementation, and
verification of XMHF an eXtensible and Modular Hypervisor
Framework. XMHF is designed to achieve three goals  modu-
lar extensibility, automated verification, and high performance.
XMHF includes a core that provides functionality common
to many hypervisor-based security architectures and supports
extensions that augment the core with additional security or
functional properties while preserving the fundamental hyper-
visor security property of memory integrity (i.e., ensuring that
the hypervisors memory is not modified by software running
at a lower privilege level). We verify the memory integrity of
the XMHF core  6018 lines of code  using a combination of
automated and manual techniques. The model checker CBMC
automatically verifies 5208 lines of C code in about 80 seconds
using less than 2GB of RAM. We manually audit the remaining
422 lines of C code and 388 lines of assembly language code that
are stable and unlikely to change as development proceeds. Our
experiments indicate that XMHFs performance is comparable
to popular high-performance general-purpose hypervisors for
the single guest that it supports.

Keywords-Hypervisor Framework, Memory Integrity, Verifi-

cation, Hypervisor Applications (Hypapps)

I. |,Non-data
p197.pdf,|
Network-wide activity is when one computer (the origina-
tor ) touches many others (the targets). Motives for activity
may be benign (mailing lists, CDNs, and research scanning),
malicious (spammers and scanners for security vulnerabili-
ties), or perhaps indeterminate (ad trackers). Knowledge
of malicious activity may help anticipate attacks, and un-
derstanding benign activity may set a baseline or charac-
terize growth. This paper identifies DNS backscatter as
a new source of information about network-wide activity.
Backscatter is the reverse DNS queries caused when tar-
gets or middleboxes automatically look up the domain name
of the originator. Queries are visible to the authoritative
DNS servers that handle reverse DNS. While the fraction of
backscatter they see depends on the servers location in the
DNS hierarchy, we show that activity that touches many tar-
gets appear even in sampled observations. We use informa-
tion about the queriers to classify originator activity using
machine-learning. Our algorithm has reasonable precision
(7080%) as shown by data from three different organiza-
tions operating DNS servers at the root or country-level.
Using this technique we examine nine months of activity
from one authority to identify trends in scanning, identify-
ing bursts corresponding to Heartbleed and broad and con-
tinuous scanning of ssh.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
OperationsNetwork monitoring

General Terms
Measurement

Keywords
Internet; Domain Name System; DNS; network activity;
scanning

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
IMC15, October 2830, 2015, Tokyo, Japan.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3848-6/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2815675.2815706.

1.

|,Data
Thatte08a.pdf,|This paper develops two parametric methods
to detect low-rate denial-of-service attacks and other simi-
lar near-periodic traffic, without the need for flow separa-
tion. The first method, the periodic attack detector, is based
on a previous approach that exploits the near-periodic
nature of attack traffic in aggregate traffic by modeling the
peak frequency in the traffic spectrum. The new method
adopts simple statistical models for attack and background
traffic in the time-domain. Both approaches use sequential
probability ratio tests (SPRTs), allowing control over false
alarm rate while examining the trade-off between detection
time and attack strength. We evaluate these methods with
real and synthetic traces, observing that the new Poisson-
based scheme uniformly detects attacks more rapidly, often
in less than 200ms, and with lower complexity than the
periodic attack detector. Current entropy-based detection
methods provide an equivalent
time to detection but
require flow-separation since they utilize source/destination
IP addresses. We evaluate sensitivity to attack strength
(compared to the rate of background traffic) with synthetic
traces, finding that the new approach can detect attacks
that represent only 10% of the total traffic bitrate in
fractions of a second.

I. |,Data
p43-cuff.pdf,|
Differential privacy is a precise mathematical constraint meant
to ensure privacy of individual pieces of information in a
database even while queries are being answered about the
aggregate. Intuitively, one must come to terms with what
differential privacy does and does not guarantee. For exam-
ple, the definition prevents a strong adversary who knows all
but one entry in the database from further inferring about
the last one. This strong adversary assumption can be over-
looked, resulting in misinterpretation of the privacy guaran-
tee of differential privacy.

Herein we give an equivalent definition of privacy using
mutual information that makes plain some of the subtleties
of differential privacy. The mutual-information differential
privacy is in fact sandwiched between -differential privacy
and (, )-differential privacy in terms of its strength. In con-
trast to previous works using unconditional mutual informa-
tion, differential privacy is fundamentally related to condi-
tional mutual information, accompanied by a maximization
over the database distribution. The conceptual advantage of
using mutual information, aside from yielding a simpler and
more intuitive definition of differential privacy, is that its
properties are well understood. Several properties of differ-
ential privacy are easily verified for the mutual information
alternative, such as composition theorems.

Keywords
Differential privacy, information theory.

1.

|,Non-data
p68-barthe.pdf,|
We present PrivInfer, an expressive framework for writing
and verifying differentially private Bayesian machine learning
algorithms. Programs in PrivInfer are written in a rich func-
tional probabilistic programming language with constructs
for performing Bayesian inference. Then, differential pri-
vacy of programs is established using a relational refinement
type system, in which refinements on probability types are
indexed by a metric on distributions. Our framework lever-
ages recent developments in Bayesian inference, probabilistic
programming languages, and in relational refinement types.
We demonstrate the expressiveness of PrivInfer by verifying
privacy for several examples of private Bayesian inference.

1.

|,Non-data
p296-jin.pdf,|
Dynamic spectrum access (DSA) has great potential to address
worldwide spectrum shortage by enhancing spectrum efficiency.
It allows unlicensed secondary users to access the underutilized
licensed spectrum when the licensed primary users are not trans-
mitting. As a key enabler for DSA systems, crowdsourced spec-
trum sensing (CSS) allows a spectrum sensing provider (SSP) to
outsource the sensing of spectrum occupancy to distributed mobile
users. In this paper, we propose DPSense, a novel framework that
allows the SSP to select mobile users for executing spatiotempo-
ral spectrum-sensing tasks without violating the location privacy of
mobile users. Detailed evaluations on real location traces confir-
m that DPSense can provide differential location privacy to mobile
users while ensuring that the SSP can accomplish spectrum-sensing
tasks with overwhelming probability and also the minimal cost.

CCS Concepts
Security and privacy  Privacy-preserving protocols; Mobile
and wireless security;

Keywords
Dynamic spectrum access; differential privacy; crowdsourced spec-
trum sensing; location privacy

1.

|,Data
p104-tuncay.pdf,|
In-app embedded browsers are commonly used by app developers
to display web content without having to redirect the user to heavy-
weight web browsers. Just like the conventional web browsers, em-
bedded browsers can allow the execution of web code. In addition,
they provide mechanisms (viz., JavaScript bridges) to give web code
access to internal app code that might implement critical function-
alities and expose device resources. This is intrinsically dangerous
since there is currently no means for app developers to perform
origin-based access control on the JavaScript bridges, and any web
code running in an embedded browser is free to use all the exposed
app and device resources. Previous work that addresses this prob-
lem provided access control solutions that work only for apps that
are built using hybrid frameworks. Additionally, these solutions fo-
cused on protecting only the parts of JavaScript bridges that expose
permissions-protected resources. In this work, our goal is to provide
a generic solution that works for all apps that utilize embedded web
browsers and protects all channels that give access to internal app
and device resources. Towards realizing this goal, we built Draco,
a uniform and fine-grained access control framework for web code
running on Android embedded browsers (viz., WebView). Draco
provides a declarative policy language that allows developers to
define policies to specify the desired access characteristics of web
origins in a fine-grained fashion, and a runtime system that dynami-
cally enforces the policies. In contrast with previous work, we do
not assume any modifications to the Android operating system, and
implement Draco in the Chromium Android System WebView app
to enable seamless deployment. Our evaluation of the the Draco
runtime system shows that Draco incurs negligible overhead, which
is in the order of microseconds.

Keywords
Android, WebView, access control, origin, JavaScript bridges, ex-
ploitation, JavaScript, HTML5

1.

|,Non-data
p1675-van-der-veen.pdf,|
Recent work shows that the Rowhammer hardware bug can
be used to craft powerful attacks and completely subvert a
system. However, existing efforts either describe probabilis-
tic (and thus unreliable) attacks or rely on special (and often
unavailable) memory management features to place victim
objects in vulnerable physical memory locations. Moreover,
prior work only targets x86 and researchers have openly won-
dered whether Rowhammer attacks on other architectures,
such as ARM, are even possible.

We show that deterministic Rowhammer attacks are feasi-
ble on commodity mobile platforms and that they cannot be
mitigated by current defenses. Rather than assuming special
memory management features, our attack, Drammer, solely
relies on the predictable memory reuse patterns of standard
physical memory allocators. We implement Drammer on
Android/ARM, demonstrating the practicability of our at-
tack, but also discuss a generalization of our approach to
other Linux-based platforms. Furthermore, we show that
traditional x86-based Rowhammer exploitation techniques
no longer work on mobile platforms and address the resulting
challenges towards practical mobile Rowhammer attacks.

To support our claims, we present the first Rowhammer-
based Android root exploit relying on no software vulner-
ability, and requiring no user permissions. In addition, we
present an analysis of several popular smartphones and find
that many of them are susceptible to our Drammer attack.
We conclude by discussing potential mitigation strategies
and urging our community to address the concrete threat of
faulty DRAM chips in widespread commodity platforms.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 2428, 2016, Vienna, Austria.
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978406

1.

|,Non-data
sec14-paper-saltaformaggio.pdf,|

State-of-the-art memory forensics involves signature-
based scanning of memory images to uncover data struc-
ture instances of interest to investigators. A largely unad-
dressed challenge is that investigators may not be able to
interpret the content of data structure fields, even with a
deep understanding of the data structures syntax and se-
mantics. This is very common for data structures with
application-specific encoding, such as those represent-
ing images, figures, passwords, and formatted file con-
tents. For example, an investigator may know that a
buffer field is holding a photo image, but still can-
not display (and hence understand) the image. We call
this the data structure content reverse engineering chal-
lenge.
In this paper, we present DSCRETE, a system
that enables automatic interpretation and rendering of in-
memory data structure contents. DSCRETE is based on
the observation that the application in which a data struc-
ture is defined usually contains interpretation and render-
ing logic to generate human-understandable output for
that data structure. Hence DSCRETE aims to identify
and reuse such logic in the programs binary and create
a scanner+renderer tool for scanning and rendering in-
stances of the data structure in a memory image. Differ-
ent from signature-based approaches, DSCRETE avoids
reverse engineering data structure signatures. Our evalu-
ation with a wide range of real-world application binaries
shows that DSCRETE is able to recover a variety of ap-
plication data  e.g., images, figures, screenshots, user
accounts, and formatted files and messages  with high
accuracy. The raw contents of such data would otherwise
be unfathomable to human investigators.

1

|,Non-data
sec14-paper-vogl.pdf,|

1

|,Non-data
sec15-paper-wang-ruowen.pdf,|
Mandatory protection systems such as SELinux and SE-
Android harden operating system integrity. Unfortu-
nately, policy development is error prone and requires
lengthy refinement using audit logs from deployed sys-
tems. While prior work has studied SELinux policy in
detail, SEAndroid is relatively new and has received lit-
tle attention. SEAndroid policy engineering differs sig-
nificantly from SELinux: Android fundamentally differs
from traditional Linux; the same policy is used on mil-
lions of devices for which new audit logs are continu-
ally available; and audit logs contain a mix of benign
and malicious accesses. In this paper, we propose EASE-
Android, the first SEAndroid analytic platform for auto-
matic policy analysis and refinement. Our key insight is
that the policy refinement process can be modeled and
automated using semi-supervised learning. Given an ex-
isting policy and a small set of known access patterns,
EASEAndroid continually expands the knowledge base
as new audit logs become available, producing sugges-
tions for policy refinement. We evaluate EASEAndroid
on 1.3 million audit logs from real-world devices. EASE-
Android successfully learns 2,518 new access patterns
and generates 331 new policy rules. During this process,
EASEAndroid discovers eight categories of attack access
patterns in real devices, two of which are new attacks di-
rectly against the SEAndroid MAC mechanism.

1

|,Data
p1626-genkin.pdf,|
We show that elliptic-curve cryptography implementations
on mobile devices are vulnerable to electromagnetic and
power side-channel attacks. We demonstrate full extraction
of ECDSA secret signing keys from OpenSSL and CoreBit-
coin running on iOS devices, and partial key leakage from
OpenSSL running on Android and from iOSs Common-
Crypto. These non-intrusive attacks use a simple magnetic
probe placed in proximity to the device, or a power probe
on the phones USB cable. They use a bandwidth of merely
a few hundred kHz, and can be performed cheaply using an
audio card and an improvised magnetic probe.

1
|,Data
sec14-paper-wang-tao.pdf,|

Website fingerprinting attacks allow a local, passive
eavesdropper to identify a users web activity by lever-
aging packet sequence information. These attacks break
the privacy expected by users of privacy technologies,
including low-latency anonymity networks such as Tor.
In this paper, we show a new attack that achieves sig-
nificantly higher accuracy than previous attacks in the
same field, further highlighting website fingerprinting as
a genuine threat to web privacy. We test our attack under
a large open-world experimental setting, where the client
can visit pages that the attacker is not aware of. We found
that our new attack is much more accurate than previous
attempts, especially for an attacker monitoring a set of
sites with low base incidence rate. We can correctly de-
termine which of 100 monitored web pages a client is
visiting (out of a significantly larger universe) at an 85%
true positive rate with a false positive rate of 0.6%, com-
pared to the best of 83% true positive rate with a false
positive rate of 6% in previous work.

To defend against such attacks, we need provably ef-
fective defenses. We show how simulatable, determinis-
tic defenses can be provably private, and we show that
bandwidth overhead optimality can be achieved for these
defenses by using a supersequence over anonymity sets
of packet sequences. We design a new defense by ap-
proximating this optimal strategy and demonstrate that
this new defense is able to defeat any attack at a lower
cost on bandwidth than the previous best.

When a client browses the web, she reveals her desti-
nation and packet content to intermediate routers, which
are controlled by ISPs who may be susceptible to ma-
licious attackers, eavesdroppers, and legal pressure. To
protect her web-browsing privacy, the client would need
to encrypt her communication traffic and obscure her
destinations with a proxy such as Tor. Website finger-
printing refers to the set of techniques that seek to re-
identify these clients destination web pages by passively
observing their communication traffic. The traffic will
contain packet lengths, order, and timing information
that could uniquely identify the page, and website fin-
gerprinting attacks use machine classification to extract
and use this information (see Section 2).

A number of attacks have been proposed that would
compromise a clients expected privacy, and defenses
have been proposed to counter these attacks (see Sec-
tion 3). Most previous defenses have been shown to fail
against more advanced attacks [4, 6, 15]; this is because
they were evaluated only against specific attacks, with no
notion of provable effectiveness (against all possible at-
tacks). In this paper, we will show an attack that further
highlights the fact that clients need a provably effective
defense, for which an upper bound on the accuracy of
any possible attack can be given. We will then show how
such a defense can be constructed. Only with a prov-
ably effective defense can we be certain that clients are
protected against website fingerprinting.

The contributions of our paper are as follows:

1

|,Data
p818-kolesnikov.pdf,|
We describe a lightweight protocol for oblivious evaluation of
a pseudorandom function (OPRF) in the presence of semi-
honest adversaries.
In an OPRF protocol a receiver has
an input r; the sender gets output s and the receiver gets
output F (s, r), where F is a pseudorandom function and s
is a random seed. Our protocol uses a novel adaptation of 1-
out-of-2 OT-extension protocols, and is particularly efficient
when used to generate a large batch of OPRF instances.
The cost to realize m OPRF instances is roughly the cost
to realize 3.5m instances of standard 1-out-of-2 OTs (using
state-of-the-art OT extension).

We explore in detail our protocols application to semi-
honest secure private set intersection (PSI). The fastest state-
of-the-art PSI protocol (Pinkas et al., Usenix 2015) is based
on efficient OT extension. We observe that our OPRF can
be used to remove their PSI protocols dependence on the
bit-length of the parties items. We implemented both PSI
protocol variants and found ours to be 3.13.6 faster than
Pinkas et al. for PSI of 128-bit strings and sufficiently large
sets. Concretely, ours requires only 3.8 seconds to securely
compute the intersection of 220-size sets, regardless of the
bit length of the items. For very large sets, our protocol is
only 4.3 slower than the insecure na ve hashing approach
for PSI.

1.

|,Non-data
p1192-schneider.pdf,|
Password authentication still constitutes the most widespread
authentication concept on the Internet today, but the human
incapability to memorize safe passwords has left this concept
vulnerable to various attacks ever since. Affected enterprises
such as Facebook now strive to mitigate such attacks by
involving external cryptographic services that harden pass-
words. Everspaugh et al. provided the first comprehensive
formal treatment of such a service, and proposed the Pythia
PRF-Service as a cryptographically secure solution (Usenix
Security15). Pythia relies on a novel cryptographic primi-
tive called partially oblivious pseudorandom functions and its
security is proven under a strong new interactive assumption
in the random oracle model.

In this work, we prove that this strong assumption is inher-
ently necessary for the Pythia construction, i.e., it cannot be
weakened without invalidating the security of Pythia. More
generally, it is impossible to reduce the security of Pythia
to any non-interactive assumptions. Hence any efficient,
scalable password hardening service that is secure under
weaker assumptions necessarily requires a conceptually differ-
ent construction. To this end, we propose a construction for
password hardening services based on a novel cryptographic
primitive called partially oblivious commitments, along with
an efficient secure instantiation based on simple assumptions.
The performance and storage evaluation of our prototype
implementation shows that our protocol runs almost twice
as fast as Pythia, while achieving a slightly relaxed security
notion but relying on weaker assumptions.

1.

|,Non-data
06547128.pdf,|We advocate schemes based on fixed-key AES as
the best route to highly efficient circuit-garbling. We provide
such schemes making only one AES call per garbled-gate
evaluation. On the theoretical side, we justify the security of
these methods in the random-permutation model, where parties
have access to a public random permutation. On the practical
side, we provide the JustGarble system, which implements our
schemes. JustGarble evaluates moderate-sized garbled-circuits
at an amortized cost of 23.2 cycles per gate (7.25 nsec), far
faster than any prior reported results.

Keywords-Garbled circuits; garbling schemes; multiparty
computation; random-permutation model; timing study; Yaos
protocol.

I. |,Non-data
sec14-paper-tice.pdf,|
Constraining dynamic control transfers is a common tech-
nique for mitigating software vulnerabilities. This de-
fense has been widely and successfully used to protect
return addresses and stack data; hence, current attacks
instead typically corrupt vtable and function pointers to
subvert a forward edge (an indirect jump or call) in the
control-flow graph. Forward edges can be protected us-
ing Control-Flow Integrity (CFI) but, to date, CFI im-
plementations have been research prototypes, based on
impractical assumptions or ad hoc, heuristic techniques.
To be widely adoptable, CFI mechanisms must be inte-
grated into production compilers and be compatible with
software-engineering aspects such as incremental compi-
lation and dynamic libraries.

This paper presents implementations of fine-grained,
forward-edge CFI enforcement and analysis for GCC and
LLVM that meet the above requirements. An analysis
and evaluation of the security, performance, and resource
consumption of these mechanisms applied to the SPEC
CPU2006 benchmarks and common benchmarks for the
Chromium web browser show the practicality of our ap-
proach: these fine-grained CFI mechanisms have signif-
icantly lower overhead than recent academic CFI proto-
types. Implementing CFI in industrial compiler frame-
works has also led to insights into design tradeoffs and
practical challenges, such as dynamic loading.
1
The computer security research community has developed
several widely-adopted techniques that successfully pro-
tect return addresses and other critical stack data [13, 20].
So, in recent years, attackers have changed their focus
to non-stack-based exploits. Taking advantage of heap-
based memory corruption bugs can allow an attacker to
overwrite a function-pointer value, so that arbitrary ma-
chine code gets executed when that value is used in an
indirect function call [6]. Such exploits are referred to as
forward-edge attacks, as they change forward edges in
the programs control-flow graph (CFG).

|,Non-data
p393-hsu.pdf,|
Failing to properly isolate components in the same address
space has resulted in a substantial amount of vulnerabilities.
Enforcing the least privilege principle for memory accesses
can selectively isolate software components to restrict at-
tack surface and prevent unintended cross-component mem-
ory corruption. However, the boundaries and interactions
between software components are hard to reason about and
existing approaches have failed to stop attackers from ex-
ploiting vulnerabilities caused by poor isolation.

We present the secure memory views (SMV) model: a
practical and efficient model for secure and selective mem-
ory isolation in monolithic multithreaded applications. SMV
is a third generation privilege separation technique that of-
fers explicit access control of memory and allows concurrent
threads within the same process to partially share or fully
isolate their memory space in a controlled and parallel man-
ner following application requirements. An evaluation of our
prototype in the Linux kernel (TCB < 1,800 LOC) shows
negligible runtime performance overhead in real-world ap-
plications including Cherokee web server (< 0.69%), Apache
httpd web server (< 0.93%), and Mozilla Firefox web browser
(< 1.89%) with at most 12 LOC changes.

1.

|,Non-data
p180-shen.pdf,|
Recommender systems typically require users history data
to provide a list of recommendations and such recommen-
dations usually reside on the cloud/server. However, the
release of such private data to the cloud has been shown to
put users at risk. It is highly desirable to provide users high-
quality personalized services while respecting their privacy.
In this paper, we develop the first Enhanced Privacy-built-
In Client for Personalized Recommendation (EpicRec) sys-
tem that performs the data perturbation on the client side
to protect users privacy. Our system needs no assumption
of trusted server and no change on the recommendation al-
gorithms on the server side; and needs minimum user inter-
action in their preferred manner, which makes our solution
fit very well into real world practical use.

The design of EpicRec system incorporates three main
modules: (1) usable privacy control interface that enables
two user preferred privacy controls, overall and category-
based controls, in the way they understand; (2) user privacy
level quantification that automatically quantifies user pri-
vacy concern level from these user understandable inputs;
(3) lightweight data perturbation algorithm that perturbs
user private data with provable guarantees on both differen-
tial privacy and data utility.

Using large-scale real world datasets, we show that, for
both overall and category-based privacy controls, EpicRec
performs best with respect to both perturbation quality and
personalized recommendation, with negligible computational
overhead. Therefore, EpicRec enables two contradictory
goals, privacy preservation and recommendation accuracy.
We also implement a proof-of-concept EpicRec system to
demonstrate a privacy-preserving personal computer for movie
recommendation with web-based privacy controls. We be-
lieve EpicRec is an important step towards designing a prac-
tical system that enables companies to monetize on user data
using high quality personalized services with strong provable
privacy protection to gain user acceptance and adoption of
their services.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978316

Keywords
Privacy-Preserving Recommendation; Differential Privacy;
Privacy Paradox

1.

|,Data
p1044-cho.pdf,|
Contemporary vehicles are getting equipped with an increasing
number of Electronic Control Units (ECUs) and wireless connec-
tivities. Although these have enhanced vehicle safety and effi-
ciency, they are accompanied with new vulnerabilities. In this pa-
per, we unveil a new important vulnerability applicable to several
in-vehicle networks including Control Area Network (CAN), the
de facto standard in-vehicle network protocol. Specifically, we
propose a new type of Denial-of-Service (DoS), called the bus-
off attack, which exploits the error-handling scheme of in-vehicle
networks to disconnect or shut down good/uncompromised ECUs.
This is an important attack that must be thwarted, since the attack,
once an ECU is compromised, is easy to be mounted on safety-
critical ECUs while its prevention is very difficult. In addition to
the discovery of this new vulnerability, we analyze its feasibility
using actual in-vehicle network traffic, and demonstrate the attack
on a CAN bus prototype as well as on two real vehicles. Based on
our analysis and experimental results, we also propose and evaluate
a mechanism to detect and prevent the bus-off attack.

1.

|,Non-data
1410.3340v1.pdf,|

Much interest has been taken in understanding the global routing structure of the Internet, both

to model and protect the current structures and to modify the structure to improve resilience.

These studies rely on trace-routes and algorithmic inference to resolve individual IP addresses

into connected routers, yielding a network of routers. Using WHOIS registries, parsing of DNS

registries, as well as simple latency-based triangulation, these routers can often be geolocated to at

least their country of origin, if not specific regions. In this work, we use node subgraph summary

statistics to present evidence that the router-level (IPv4) network is spatially embedded, with the

similarity (or dissimilarity) of a node from its neighbor strongly correlating with the attributes of

other routers residing in the same country or region. We discuss these results in context of the

recently proposed gravity models of the Internet, as well as the potential application to geolocation

inferrence.

4
1
0
2

 
t
c
O
9

 

 
 
]
I

N
.
s
c
[
 
 

1
v
0
4
3
3

.

0
1
4
1
:
v
i
X
r
a

1

I.

|,Data
sec15-paper-stringhini.pdf,|

Cybercriminals misuse accounts on online services (e.g.,
webmails and online social networks) to perform ma-
licious activity, such as spreading malicious content or
stealing sensitive information.
In this paper, we show
that accounts that are accessed by botnets are a popular
choice by cybercriminals. Since botnets are composed
of a finite number of infected computers, we observe that
cybercriminals tend to have their bots connect to multiple
online accounts to perform malicious activity.

We present EVILCOHORT, a system that detects on-
line accounts that are accessed by a common set of in-
fected machines. EVILCOHORT only needs the mapping
between an online account and an IP address to operate,
and can therefore detect malicious accounts on any on-
line service (webmail services, online social networks,
storage services) regardless of the type of malicious ac-
tivity that these accounts perform. Unlike previous work,
our system can identify malicious accounts that are con-
trolled by botnets but do not post any malicious content
(e.g., spam) on the service. We evaluated EVILCOHORT
on multiple online services of different types (a webmail
service and four online social networks), and show that
it accurately identifies malicious accounts.

1

|,Data
sec14-paper-kuhrer.pdf,|

Amplification vulnerabilities in many UDP-based net-
work protocols have been abused by miscreants to launch
Distributed Denial-of-Service (DDoS) attacks that ex-
ceed hundreds of Gbps in traffic volume. However, up
to now little is known about the nature of the amplifica-
tion sources and about countermeasures one can take to
remediate these vulnerable systems. Is there any hope in
mitigating the amplification problem?

In this paper, we aim to answer this question and tackle
the problem from four different angles. In a first step, we
monitored and classified amplification sources, showing
that amplifiers have a high diversity in terms of operat-
ing systems and architectures. Based on these results,
we then collaborated with the security community in a
large-scale campaign to reduce the number of vulnera-
ble NTP servers by more than 92%. To assess possible
next steps of attackers, we evaluate amplification vulner-
abilities in the TCP handshake and show that attackers
can abuse millions of hosts to achieve 20x amplifica-
tion. Lastly, we analyze the root cause for amplification
attacks: networks that allow IP address spoofing. We
deploy a method to identify spoofing-enabled networks
from remote and reveal up to 2,692 Autonomous Systems
that lack egress filtering.

1

|,Data
p805-liu.pdf,|
Sybil attacks present a significant threat to many Internet
systems and applications, in which a single adversary in-
serts multiple colluding identities in the system to compro-
mise its security and privacy. Recent work has advocated
the use of social-network-based trust relationships to defend
against Sybil attacks. However, most of the prior security
analyses of such systems examine only the case of social net-
works at a single instant in time. In practice, social network
connections change over time, and attackers can also cause
limited changes to the networks. In this work, we focus on
the temporal dynamics of a variety of social-network-based
Sybil defenses. We describe and examine the effect of novel
attacks based on: (a) the attackers ability to modify Sybil-
controlled parts of the social-network graph, (b) his ability
to change the connections that his Sybil identities main-
tain to honest users, and (c) taking advantage of the regular
dynamics of connections forming and breaking in the hon-
est part of the social network. We find that against some
defenses meant to be fully distributed, such as SybilLimit
and Persea, the attacker can make dramatic gains over time
and greatly undermine the security guarantees of the sys-
tem. Even against centrally controlled Sybil defenses, the
attacker can eventually evade detection (e.g. against Sybil-
Infer and SybilRank) or create denial-of-service conditions
(e.g. against Ostra and SumUp). After analysis and simula-
tion of these attacks using both synthetic and real-world so-
cial network topologies, we describe possible defense strate-
gies and the trade-offs that should be explored. It is clear
from our findings that temporal dynamics need to be ac-
counted for in Sybil defense or else the attacker will be able
to undermine the system in unexpected and possibly dan-
gerous ways.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General
 Security and Protection; K.4.1 [Computers and Soci-

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the Owner/Author(s).
Copyright is held by the Owner/Author(s).
CCS15, October 1216, 2015, Denver, Colorado, USA.
ACM 978-1-4503-3832-5/15/10.
http://dx.doi.org/10.1145/2810103.2813693.

ety]: Public Policy Issues  Abuse and Crime Involving
Computers; K.6.5 [Management of Computing and In-
formation Systems]: Security and Protection  Authenti-
cation

Keywords
Sybil attacks; temporal dynamics

1.

|,Data
sec14-paper-pinkas.pdf,|

Private set intersection (PSI) allows two parties to com-
pute the intersection of their sets without revealing any
information about items that are not in the intersection.
It is one of the best studied applications of secure com-
putation and many PSI protocols have been proposed.
However, the variety of existing PSI protocols makes it
difficult to identify the solution that performs best in a re-
spective scenario, especially since they were not all im-
plemented and compared in the same setting.

In this work, we give an overview on existing PSI pro-
tocols that are secure against semi-honest adversaries.
We take advantage of the most recent efficiency improve-
ments in OT extension to propose significant optimiza-
tions to previous PSI protocols and to suggest a new PSI
protocol whose runtime is superior to that of existing pro-
tocols. We compare the performance of the protocols
both theoretically and experimentally, by implementing
all protocols on the same platform, and give recommen-
dations on which protocol to use in a particular setting.

1

|,Non-data
p767-zhu.pdf,|
Malware detection increasingly relies on machine learning
techniques, which utilize multiple features to separate the
malware from the benign apps. The effectiveness of these
techniques primarily depends on the manual feature engi-
neering process, based on human knowledge and intuition.
However, given the adversaries efforts to evade detection
and the growing volume of publications on malware behav-
iors, the feature engineering process likely draws from a frac-
tion of the relevant knowledge.

We propose an end-to-end approach for automatic feature
engineering. We describe techniques for mining documents
written in natural language (e.g. scientific papers) and for
representing and querying the knowledge about malware in
a way that mirrors the human feature engineering process.
Specifically, we first identify abstract behaviors that are as-
sociated with malware, and then we map these behaviors to
concrete features that can be tested experimentally. We im-
plement these ideas in a system called FeatureSmith, which
generates a feature set for detecting Android malware. We
train a classifier using these features on a large data set of
benign and malicious apps. This classifier achieves a 92.5%
true positive rate with only 1% false positives, which is com-
parable to the performance of a state-of-the-art Android
malware detector that relies on manually engineered fea-
tures. In addition, FeatureSmith is able to suggest informa-
tive features that are absent from the manually engineered
set and to link the features generated to abstract concepts
that describe malware behaviors.

1.

|,Data
06547105.pdf,|Malicious Web activities continue to be a major
threat to the safety of online Web users. Despite the plethora
forms of attacks and the diversity of their delivery channels,
in the back end, they are all orchestrated through malicious
Web infrastructures, which enable miscreants to do business
with each other and utilize others resources. Identifying the
linchpins of the dark infrastructures and distinguishing those
valuable to the adversaries from those disposable are critical
for gaining an upper hand in the battle against them.

In this paper, using nearly 4 million malicious URL paths
crawled from different attack channels, we perform a large-
scale study on the topological relations among hosts in the
malicious Web infrastructure. Our study reveals the existence
of a set of topologically dedicated malicious hosts that play
orchestrating roles in malicious activities. They are well con-
nected to other malicious hosts and do not receive traffic
from legitimate sites. Motivated by their distinctive features
in topology, we develop a graph-based approach that relies
on a small set of known malicious hosts as seeds to detect
dedicate malicious hosts in a large scale. Our method is
general across the use of different types of seed data, and
results in an expansion rate of over 12 times in detection
with a low false detection rate of 2%. Many of the detected
hosts operate as redirectors, in particular Traffic Distribution
Systems (TDSes) that are long-lived and receive traffic from
new attack campaigns over time. These TDSes play critical
roles in managing malicious traffic flows. Detecting and taking
down these dedicated malicious hosts can therefore have more
impact on the malicious Web infrastructures than aiming at
short-lived doorways or exploit sites.

I. |,Data
sec14-paper-yarom.pdf,|
Sharing memory pages between non-trusting processes
is a common method of reducing the memory footprint
of multi-tenanted systems.
In this paper we demon-
strate that, due to a weakness in the Intel X86 processors,
page sharing exposes processes to information leaks. We
present FLUSH+RELOAD, a cache side-channel attack
technique that exploits this weakness to monitor access
to memory lines in shared pages. Unlike previous cache
side-channel attacks, FLUSH+RELOAD targets the Last-
Level Cache (i.e. L3 on processors with three cache lev-
els). Consequently, the attack program and the victim do
not need to share the execution core.

We demonstrate the efficacy of the FLUSH+RELOAD
attack by using it to extract the private encryption keys
from a victim program running GnuPG 1.4.13. We tested
the attack both between two unrelated processes in a sin-
gle operating system and between processes running in
separate virtual machines. On average, the attack is able
to recover 96.7% of the bits of the secret key by observ-
ing a single signature or decryption round.

1

|,Non-data
imc184s-gillAemb.pdf,|
The large-scale collection and exploitation of personal infor-
mation to drive targeted online advertisements has raised
privacy concerns. As a step towards understanding these
concerns, we study the relationship between how much in-
formation is collected and how valuable it is for advertising.
We use HTTP traces consisting of millions of users to aid our
study and also present the first comparative study between
aggregators. We develop a simple model that captures the
various parameters of todays advertising revenues, whose
values are estimated via the traces. Our results show that
per aggregator revenue is skewed (5% accounting for 90%
of revenues), while the contribution of users to advertising
revenue is much less skewed (20% accounting for 80% of
revenue). Google is dominant in terms of revenue and reach
(presence on 80% of publishers). We also show that if all
5% of the top users in terms of revenue were to install pri-
vacy protection, with no corresponding reaction from the
publishers, then the revenue can drop by 30%.

Categories and Subject Descriptors
H.1.0 [Models and Principles]: General

General Terms
Economics, Measurement

Keywords
Advertising, Privacy, CPM, Do-not-track, Publishers, Ag-
gregators

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
IMC13, October 2325, 2013, Barcelona, Spain.
Copyright 2013 ACM 978-1-4503-1953-9/13/10 ...$15.00.
http://dx.doi.org/10.1145/2504730.2504768.

Dina Papagiannaki,

Pablo Rodriguez
Telefonica Research

dina@tid.es,
pablorr@tid.es

1.

|,Data
mobile-kdd11.pdf,|
Even though human movement and mobility patterns have a high
degree of freedom and variation, they also exhibit structural pat-
terns due to geographic and social constraints. Using cell phone
location data, as well as data from two online location-based social
networks, we aim to understand what basic laws govern human mo-
tion and dynamics. We find that humans experience a combination
of periodic movement that is geographically limited and seemingly
random jumps correlated with their social networks. Short-ranged
travel is periodic both spatially and temporally and not effected by
the social network structure, while long-distance travel is more in-
fluenced by social network ties. We show that social relationships
can explain about 10% to 30% of all human movement, while pe-
riodic behavior explains 50% to 70%. Based on our findings, we
develop a model of human mobility that combines periodic short
range movements with travel due to the social network structure.
We show that our model reliably predicts the locations and dynam-
ics of future human movement and gives an order of magnitude
better performance than present models of human mobility.
Categories and Subject Descriptors: H.2.8 [Database Manage-
ment]: Database Applications  Data mining
General Terms: Algorithms, theory, experimentation.
Keywords: Human mobility, Communication networks, Social net-
works.
1.

|,Data
p1006-bos-1.pdf,|
Lattice-based cryptography offers some of the most attrac-
tive primitives believed to be resistant to quantum com-
puters. Following increasing interest from both companies
and government agencies in building quantum computers, a
number of works have proposed instantiations of practical
post-quantum key exchange protocols based on hard prob-
lems in ideal lattices, mainly based on the Ring Learning
With Errors (R-LWE) problem. While ideal lattices facil-
itate major efficiency and storage benefits over their non-
ideal counterparts, the additional ring structure that en-
ables these advantages also raises concerns about the as-
sumed difficulty of the underlying problems. Thus, a ques-
tion of significant interest to cryptographers, and especially
to those currently placing bets on primitives that will with-
stand quantum adversaries, is how much of an advantage
the additional ring structure actually gives in practice.

Despite conventional wisdom that generic lattices might
be too slow and unwieldy, we demonstrate that LWE-based
key exchange is quite practical: our constant time imple-
mentation requires around 1.3ms computation time for each
party; compared to the recent NewHope R-LWE scheme,
communication sizes increase by a factor of 4.7, but remain
under 12 KiB in each direction. Our protocol is competitive
when used for serving web pages over TLS; when partnered
with ECDSA signatures, latencies increase by less than a fac-
tor of 1.6, and (even under heavy load) server throughput
only decreases by factors of 1.5 and 1.2 when serving typ-
ical 1 KiB and 100 KiB pages, respectively. To achieve these
practical results, our protocol takes advantage of several in-

Large parts of this work were done when Valeria Nikolaenko was
an intern at Google.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
Request permissions from Permissions@acm.org.
CCS16 October 2428, 2016, Vienna, Austria
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-4139-4/16/10...15.00
DOI: http://dx.doi.org/10.1145/2976749.2978425

novations. These include techniques to optimize communi-
cation bandwidth, dynamic generation of public parameters
(which also offers additional security against backdoors),
carefully chosen error distributions, and tight security pa-
rameters.

Keywords
Post-quantum cryptography; learning with errors; key ex-
change; OpenSSL; TLS

1.

|,Non-data
sec14-paper-oren.pdf,|

In the attempt to bring modern broadband Internet fea-
tures to traditional broadcast television, the Digital Video
Broadcasting (DVB) consortium introduced a specifi-
cation called Hybrid Broadcast-Broadband Television
(HbbTV), which allows broadcast streams to include em-
bedded HTML content which is rendered by the televi-
sion. This system is already in very wide deployment
in Europe, and has recently been adopted as part of the
American digital television standard.

Our analyses of the specifications, and of real systems
implementing them, show that the broadband and broad-
cast systems are combined insecurely. This enables a
large-scale exploitation technique with a localized geo-
graphical footprint based on radio frequency (RF) injec-
tion, which requires a minimal budget and infrastructure
and is remarkably difficult to detect. Despite our respon-
sible disclosure to the standards body, our attack was
viewed as too expensive and with limited pay-off to the
attackers.

In this paper, we present the attack methodology and
a number of follow-on exploitation techniques that pro-
vide significant flexibility to attackers. Furthermore, we
demonstrate that the technical complexity and required
budget are low, making this attack practical and realis-
tic, especially in areas with high population density  in a
dense urban area, an attacker with a budget of about $450
can target more than 20,000 devices in a single attack. A
unique aspect of this attack is that, in contrast to most In-
ternet of Things/Cyber-Physical System threat scenarios
where the attack comes from the data network side and

affects the physical world, our attack uses the physical
broadcast network to attack the data network.

|,Data
p1292-boyle.pdf,|
Function Secret Sharing (FSS), introduced by Boyle et al.
(Eurocrypt 2015), provides a way for additively secret-sharing
a function from a given function family F. More concretely,
an m-party FSS scheme splits a function f : {0, 1}n  G, for
some abelian group G, into functions f1, . . . , fm, described
by keys k1, . . . , km, such that f = f1 + . . . + fm and every
strict subset of the keys hides f . A Distributed Point Func-
tion (DPF) is a special case where F is the family of point
functions, namely functions f, that evaluate to  on the
input  and to 0 on all other inputs.

FSS schemes are useful for applications that involve pri-
vately reading from or writing to distributed databases while
minimizing the amount of communication. These include
different flavors of private information retrieval (PIR), as
well as a recent application of DPF for large-scale anony-
mous messaging.

We improve and extend previous results in several ways:
 Simplified FSS constructions. We introduce a ten-
soring operation for FSS which is used to obtain a con-
ceptually simpler derivation of previous constructions
and present our new constructions.

 Improved 2-party DPF. We reduce the key size of
the PRG-based DPF scheme of Boyle et al. roughly
by a factor of 4 and optimize its computational cost.
The optimized DPF significantly improves the concrete
costs of 2-server PIR and related primitives.

 FSS for new function families. We present an ef-
ficient PRG-based 2-party FSS scheme for the family
of decision trees, leaking only the topology of the tree
and the internal node labels. We apply this towards
FSS for multi-dimensional intervals. We also present
a general technique for extending FSS schemes by in-
creasing the number of parties.

1 , . . . , k

 Verifiable FSS. We present efficient protocols for ver-
ifying that keys (k
m), obtained from a poten-
tially malicious user, are consistent with some f  F.
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978429

Such a verification may be critical for applications that
involve private writing or voting by many users.

Keywords: Function secret sharing, private information
retrieval, secure multiparty computation, homomorphic en-
cryption

1.

|,Non-data
p993-crockett.pdf,|
This work describes the design, implementation, and eval-
uation of  , a general-purpose software framework for
lattice-based cryptography. The  framework has several
novel properties that distinguish it from prior implementa-
tions of lattice cryptosystems, including the following.
Generality, modularity, concision:  defines a collection
of general, highly composable interfaces for mathematical
operations used across lattice cryptography, allowing for a
wide variety of schemes to be expressed very naturally and
at a high level of abstraction. For example, we implement
an advanced fully homomorphic encryption (FHE) scheme
in as few as 25 lines of code per feature, via code that very
closely matches the schemes mathematical definition.
Theory affinity:   is designed from the ground-up
around the specialized ring representations, fast algorithms,
and worst-case hardness proofs that have been developed for
the Ring-LWE problem and its cryptographic applications.
In particular, it implements fast algorithms for sampling
from theory-recommended error distributions over arbitrary
cyclotomic rings, and provides tools for maintaining tight
control of error growth in cryptographic schemes.
Safety:  has several facilities for reducing code com-
plexity and programming errors, thereby aiding the correct
implementation of lattice cryptosystems. In particular, it
uses strong typing to statically enforcei.e., at compile time
a wide variety of constraints among the various parameters.
Advanced features:   exposes the rich hierarchy of
cyclotomic rings to cryptographic applications. We use this
to give the first-ever implementation of a collection of FHE
operations known as ring switching, and also define and
analyze a more efficient variant that we call ring tunneling.
Lastly, this work defines and analyzes a variety of mathe-
matical objects and algorithms for the recommended usage
of Ring-LWE in cyclotomic rings, which we believe will serve
as a useful knowledge base for future implementations.

Supported by NSF CAREER Award CCF-1054495, DARPA

FA8750-11-C-0096, the Sloan Foundation, and Google.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978402

1.

|,Non-data
p1727-nasr.pdf,|
Decoy routing is a promising new approach for censorship
circumvention that relies on traffic re-direction by volun-
teer autonomous systems. Decoy routing is subject to a
fundamental censorship attack, called routing around decoy
(RAD), in which the censors re-route their clients Internet
traffic in order to evade decoy routing autonomous systems.
Recently, there has been a heated debate in the community
on the real-world feasibility of decoy routing in the pres-
ence of the RAD attack. Unfortunately, previous studies
rely their analysis on heuristic-based mechanisms for decoy
placement strategies as well as ad hoc strategies for the im-
plementation of the RAD attack by the censors.

In this paper, we perform the first systematic analysis of
decoy routing in the presence of the RAD attack. We use
game theory to model the interactions between decoy router
deployers and the censors in various settings. Our game-
theoretic analysis finds the optimal decoy placement strate-
giesas opposed to heuristic-based placementsin the pres-
ence of RAD censors who take their optimal censorship ac-
tionsas opposed to some ad hoc implementation of RAD.
That is, we investigate the best decoy placement given the
best RAD censorship.

We consider two business models for the real-world de-
ployment of decoy routers: a central deployment that re-
sembles that of Tor and a distributed deployment where au-
tonomous systems individually decide on decoy deployment
based on their economic interests. Through extensive sim-
ulation of Internet routes, we derive the optimal strategies
in the two models for various censoring countries and under
different assumptions about the budget and preferences of
the censors and decoy deployers. We believe that our study
is a significant step forward in understanding the practicality
of the decoy routing circumvention approach.

1.

|,Data
p565-ball.pdf,|
We present simple, practical, and powerful new techniques
for garbled circuits. These techniques result in significant
concrete and asymptotic improvements over the state of the
art, for several natural kinds of computations.

For arithmetic circuits over the integers, our construction
results in garbled circuits with free addition, weighted
threshold gates with cost independent of fan-in, and ex-
ponentiation by a fixed exponent with cost independent of
the exponent. For boolean circuits, our construction gives
an exponential improvement over the state of the art for
threshold gates (including AND/OR gates) of high fan-in.

Our construction can be efficiently instantiated with prac-
tical symmetric-key primitives (e.g., AES), and is proven
secure under similar assumptions to that of the Free-XOR
garbling scheme (Kolesnikov & Schneider, ICALP 2008). We
give an extensive comparison between our scheme and state-
of-the-art garbling schemes applied to boolean circuits.

1.

|,Non-data
p1329-kellaris.pdf,|
Recently, various protocols have been proposed for securely
outsourcing database storage to a third party server, rang-
ing from systems with full-fledged security based on strong
cryptographic primitives such as fully homomorphic encryp-
tion or oblivious RAM, to more practical implementations
based on searchable symmetric encryption or even on deter-
ministic and order-preserving encryption. On the flip side,
various attacks have emerged that show that for some of
these protocols confidentiality of the data can be compro-
mised, usually given certain auxiliary information.

We take a step back and identify a need for a formal un-
derstanding of the inherent efficiency/privacy trade-off in
outsourced database systems, independent of the details of
the system. We propose abstract models that capture se-
cure outsourced storage systems in sufficient generality, and
identify two basic sources of leakage, namely access pattern
and communication volume. We use our models to distin-
guish certain classes of outsourced database systems that
have been proposed, and deduce that all of them exhibit at
least one of these leakage sources.

We then develop generic reconstruction attacks on any
system supporting range queries where either access pattern
or communication volume is leaked. These attacks are in a
rather weak passive adversarial model, where the untrusted
server knows only the underlying query distribution. In par-
ticular, to perform our attack the server need not have any
prior knowledge about the data, and need not know any of
the issued queries nor their results. Yet, the server can re-
construct the secret attribute of every record in the database
after about N 4 queries, where N is the domain size. We pro-
vide a matching lower bound showing that our attacks are
Work supported by NSF Grants no. CNS-1414119 and
Work partially supported by the NSF CNS-1414119 grant.
Work supported by NSF Grant no. NSF CNS-1565387 and

CNS-1565387.

grants from the Sloan Foundation.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978386

essentially optimal. Our reconstruction attacks using com-
munication volume apply even to systems based on homo-
morphic encryption or oblivious RAM in the natural way.

Finally, we provide experimental results demonstrating
the efficacy of our attacks on real datasets with a variety of
different features. On all these datasets, after the required
number of queries our attacks successfully recovered the se-
cret attributes of every record in at most a few seconds.
Keywords: Secure outsourced databases, generic attacks

1.

|,Data
geolife.pdf,|

People travel in the real world and leave their location history in a form of trajectories. These trajec-
tories do not only connect locations in the physical world but also bridge the gap between people and
locations. This paper introduces a social networking service, called GeoLife, which aims to understand
trajectories, locations and users, and mine the correlation between users and locations in terms of user-
generated GPS trajectories. GeoLife offers three key applications scenarios: 1) sharing life experiences
based on GPS trajectories; 2) generic travel recommendations, e.g., the top interesting locations, travel
sequences among locations and travel experts in a given region; and 3) personalized friend and location
recommendation.

1

|,Non-data
06547107.pdf,|Electromagnetic interference (EMI) affects cir-
cuits by inducing voltages on conductors. Analog sensing of
signals on the order of a few millivolts is particularly sensitive
to interference. This work (1) measures the susceptibility of
analog sensor systems to signal injection attacks by intentional,
low-power emission of chosen electromagnetic waveforms, and
(2) proposes defense mechanisms to reduce the risks.

Our experiments use specially crafted EMI at varying power
and distance to measure susceptibility of sensors in implantable
medical devices and consumer electronics. Results show that
at distances of 12 m, consumer electronic devices containing
microphones are vulnerable to the injection of bogus audio
signals. Our measurements show that in free air, intentional
EMI under 10 W can inhibit pacing and induce defibrillation
shocks at distances up to 12 m on implantable cardiac elec-
tronic devices. However, with the sensing leads and medical
devices immersed in a saline bath to better approximate the
human body, the same experiment decreased to under 5 cm.

Our defenses range from prevention with simple analog
shielding to detection with a signal contamination metric based
on the root mean square of waveform amplitudes. Our con-
tribution to securing cardiac devices includes a novel defense
mechanism that probes for forged pacing pulses inconsistent
with the refractory period of cardiac tissue.

Keywords-Attacks and defenses; embedded systems security;

hardware security; analog sensors.

I. |,Non-data
sec14-paper-michalevsky.pdf,|

We show that the MEMS gyroscopes found on mod-
ern smart phones are sufficiently sensitive to measure
acoustic signals in the vicinity of the phone. The re-
sulting signals contain only very low-frequency infor-
mation (<200Hz). Nevertheless we show, using signal
processing and machine learning, that this information is
sufficient to identify speaker information and even parse
speech. Since iOS and Android require no special per-
missions to access the gyro, our results show that apps
and active web content that cannot access the micro-
phone can nevertheless eavesdrop on speech in the vicin-
ity of the phone.

1

|,Non-data
p1304-fiore.pdf,|
Proof systems for verifiable computation (VC) have the po-
tential to make cloud outsourcing more trustworthy. Recent
schemes enable a verifier with limited resources to delegate
large computations and verify their outcome based on suc-
cinct arguments: verification complexity is linear in the size
of the inputs and outputs (not the size of the computation).
However, cloud computing also often involves large amounts
of data, which may exceed the local storage and I/O capa-
bilities of the verifier, and thus limit the use of VC.

In this paper, we investigate multi-relation hash & prove
schemes for verifiable computations that operate on succinct
data hashes. Hence, the verifier delegates both storage and
computation to an untrusted worker. She uploads data and
keeps hashes; exchanges hashes with other parties; verifies
arguments that consume and produce hashes; and selectively
downloads the actual data she needs to access.

Existing instantiations that fit our definition either target
restricted classes of computations or employ relatively ineffi-
cient techniques. Instead, we propose efficient constructions
that lift classes of existing arguments schemes for fixed rela-
tions to multi-relation hash & prove schemes. Our schemes
(1) rely on hash algorithms that run linearly in the size of
the input; (2) enable constant-time verification of arguments
on hashed inputs; (3) incur minimal overhead for the prover.
Their main benefit is to amortize the linear cost for the ver-
ifier across all relations with shared I/O. Concretely, com-
pared to solutions that can be obtained from prior work,
our new hash & prove constructions yield a 1,400x speed-
up for provers. We also explain how to further reduce the
linear verification costs by partially outsourcing the hash
computation itself, obtaining a 480x speed-up when applied
to existing VC schemes, even on single-relation executions.

Work done at Microsoft Research.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978368

1.

|,Non-data
abstract_22.pdf,|
If someone has the ability to take control of a botnet, can
they just clean up all the infected hosts? Can we deceive
users, if our goal is to better understand how they are de-
ceived by attackers? Can we demonstrate the need for better
methods, by breaking something that people rely on today?
To be effective, we must find ways to balance societal needs
and ethical issues surrounding our research, lest we drift to
the extremesbecoming the very thing we deplore, or ced-
ing the Internet to the miscreants because we fear to act.
In this paper, we advocate for a community dialogue on the
ethical issues in computer security and the ethical standards
that we intend to enforce as a community.

1.

|,Non-data
p192-qin.pdf,|
In local differential privacy (LDP), each user perturbs her
data locally before sending the noisy data to a data collector.
The latter then analyzes the data to obtain useful statistics.
Unlike the setting of centralized differential privacy, in LDP
the data collector never gains access to the exact values of
sensitive data, which protects not only the privacy of data
contributors but also the collector itself against the risk of
potential data leakage. Existing LDP solutions in the liter-
ature are mostly limited to the case that each user possesses
a tuple of numeric or categorical values, and the data collec-
tor computes basic statistics such as counts or mean values.
To the best of our knowledge, no existing work tackles more
complex data mining tasks such as heavy hitter discovery
over set-valued data.

In this paper, we present a systematic study of heavy hit-
ter mining under LDP. We first review existing solutions,
extend them to the heavy hitter estimation, and explain why
their effectiveness is limited. We then propose LDPMiner,
a two-phase mechanism for obtaining accurate heavy hitters
with LDP. The main idea is to first gather a candidate set
of heavy hitters using a portion of the privacy budget, and
focus the remaining budget on refining the candidate set in a
second phase, which is much more efficient budget-wise than
obtaining the heavy hitters directly from the whole dataset.
We provide both in-depth theoretical analysis and extensive
experiments to compare LDPMiner against adaptations of
previous solutions. The results show that LDPMiner signif-
icantly improves over existing methods. More importantly,



This work was conducted while the first author was doing

internship at Qatar Computing Research Institute.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:2) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978409

LDPMiner successfully identifies the majority true heavy
hitters in practical settings.

Keywords
Local Differential Privacy; Heavy Hitter

1.

|,Data
06547115.pdf,|This paper introduces a novel information hiding
technique for Flash memory. The method hides data within an
analog characteristic of Flash, the program time of individual
bits. Because the technique uses analog behaviors, normal Flash
memory operations are not affected and hidden information is
invisible in the data stored in the memory. Even if an attacker
checks a Flash chips analog characteristics, experimental
results indicate that the hidden information is difficult to
distinguish from inherent manufacturing variation or normal
wear on the device. Moreover, the hidden data can survive
erasure of the Flash memory data, and the technique can be
used on current Flash chips without hardware changes.

Keywords-flash memory; security; steganography;

I. |,Non-data
p504-xu.pdf,|
Intrusive multi-step attacks, such as Advanced Persistent
Threat (APT) attacks, have plagued enterprises with signif-
icant financial losses and are the top reason for enterprises
to increase their security budgets. Since these attacks are
sophisticated and stealthy, they can remain undetected for
years if individual steps are buried in background noise.
Thus, enterprises are seeking solutions to connect the sus-
picious dots across multiple activities. This requires ubiq-
uitous system auditing for long periods of time, which in
turn causes overwhelmingly large amount of system audit
events. Given a limited system budget, how to efficiently
handle ever-increasing system audit logs is a great challenge.
This paper proposes a new approach that exploits the de-
pendency among system events to reduce the number of log
entries while still supporting high-quality forensic analysis.
In particular, we first propose an aggregation algorithm that
preserves the dependency of events during data reduction to
ensure the high quality of forensic analysis. Then we pro-
pose an aggressive reduction algorithm and exploit domain
knowledge for further data reduction. To validate the effi-
cacy of our proposed approach, we conduct a comprehensive
evaluation on real-world auditing systems using log traces of
more than one month. Our evaluation results demonstrate
that our approach can significantly reduce the size of system
logs and improve the efficiency of forensic analysis without
losing accuracy.

1Work done during an internship in NEC Labs America, Inc.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978378

1.

|,Data
p805-araki.pdf,|
In this paper, we describe a new information-theoretic proto-
col (and a computationally-secure variant) for secure three-
party computation with an honest majority. The proto-
col has very minimal computation and communication; for
Boolean circuits, each party sends only a single bit for every
AND gate (and nothing is sent for XOR gates). Our protocol
is (simulation-based) secure in the presence of semi-honest
adversaries, and achieves privacy in the client/server model
in the presence of malicious adversaries.

On a cluster of three 20-core servers with a 10Gbps con-
nection, the implementation of our protocol carries out over
1.3 million AES computations per second, which involves
processing over 7 billion gates per second. In addition, we
developed a Kerberos extension that replaces the ticket-
granting-ticket encryption on the Key Distribution Center
(KDC) in MIT-Kerberos with our protocol, using keys/ pass-
words that are shared between the servers. This enables the
use of Kerberos while protecting passwords. Our implemen-
tation is able to support a login storm of over 35,000 logins
per second, which suffices even for very large organizations.
Our work demonstrates that high-throughput secure com-
putation is possible on standard hardware.

|,Non-data
p1516-chen.pdf,|
The Host header is a security-critical component in an HTTP
request, as it is used as the basis for enforcing security and
caching policies. While the current specification is generally
clear on how host-related protocol fields should be parsed
and interpreted, we find that the implementations are prob-
lematic. We tested a variety of widely deployed HTTP im-
plementations and discover a wide range of non-compliant
and inconsistent host processing behaviours. The particu-
lar problem is that when facing a carefully crafted HTTP
request with ambiguous host fields (e.g., with multiple Host
headers), two different HTTP implementations often accept
and understand it differently when operating on the same
request in sequence. We show a number of techniques to
induce inconsistent interpretations of host between HTTP
implementations and how the inconsistency leads to severe
attacks such as HTTP cache poisoning and security policy
bypass. The prevalence of the problem highlights the poten-
tial negative impact of gaps between the specifications and
implementations of Internet protocols.

1.

|,Data
routerpaths_lanman2016.pdf,|Traceroute is largely considered as the number-one
tool when troubleshooting the network, with innumerable appli-
cations, such as pinpointing the routing deficiencies or detecting
and locating network outages. Previous works have extensively in-
vestigated pitfalls and flaws causing the measurements performed
with this tool to be inaccurate or incomplete. In this paper, we
show how, even in the absence of all these well-investigated pitfalls
and flaws, our ability to properly troubleshoot the network
with Traceroute is strongly limited. Indeed, by using state-
of-the-art alias resolution techniques, we investigate how and
how much the IP-level description provided by Traceroute can
distort our understanding of the characteristics of Internet paths.
We experimentally evaluate the impact on path properties like
equal-cost multipaths, loops, routing cycles, load balancing, route
prevalence and persistence. Our results confirm that researchers
and network operators relying on Traceroute may poorly estimate
(i) the number of multiple equal-cost routes to the destination;
(ii) the presence of suboptimal routing in the network; (iii) the
routing stability.

I. |,Data
p666-redmiles.pdf,|
Few users have a single, authoritative, source from whom
they can request digital-security advice. Rather, digital-
security skills are often learned haphazardly, as users fil-
ter through an overwhelming quantity of security advice.
By understanding the factors that contribute to users ad-
vice sources, beliefs, and security behaviors, we can help
to pare down the quantity and improve the quality of ad-
vice provided to users, streamlining the process of learn-
ing key behaviors. This paper rigorously investigates how
users security beliefs, knowledge, and demographics corre-
late with their sources of security advice, and how all these
factors influence security behaviors. Using a carefully pre-
tested, U.S.-census-representative survey of 526 users, we
present an overview of the prevalence of respondents ad-
vice sources, reasons for accepting and rejecting advice from
those sources, and the impact of these sources and demo-
graphic factors on security behavior. We find evidence of a
digital divide in security: the advice sources of users with
higher skill levels and socioeconomic status differ from those
with fewer resources. This digital security divide may add to
the vulnerability of already disadvantaged users. Addition-
ally, we confirm and extend results from prior small-sample
studies about why users accept certain digital-security ad-
vice (e.g., because they trust the source rather than the con-
tent) and reject other advice (e.g., because it is inconvenient
and because it contains too much marketing material). We
conclude with recommendations for combating the digital
divide and improving the efficacy of digital-security advice.

1.

|,Data
sec14-paper-kapravelos.pdf,|
We present Hulk, a dynamic analysis system that de-
tects malicious behavior in browser extensions by mon-
itoring their execution and corresponding network activ-
ity. Hulk elicits malicious behavior in extensions in two
ways. First, Hulk leverages HoneyPages, which are dy-
namic pages that adapt to an extensions expectations in
web page structure and content. Second, Hulk employs
a fuzzer to drive the numerous event handlers that mod-
ern extensions heavily rely upon. We analyzed 48K ex-
tensions from the Chrome Web store, driving each with
over 1M URLs. We identify a number of malicious ex-
tensions, including one with 5.5 million affected users,
stressing the risks that extensions pose for todays web
security ecosystem, and the need to further strengthen
browser security to protect user data and privacy.

1

|,Data
p945-zhao.pdf,|
Attackers can get physical control of a computer in sleep
(S3/suspend-to-RAM), if it is lost, stolen, or the owner is
being coerced. High-value memory-resident secrets, includ-
ing disk encryption keys, and private signature/encryption
keys for PGP, may be extracted (e.g., via cold-boot or DMA
attacks), by physically accessing such a computer. Our goal
is to alleviate threats of extracting secrets from a computer
in sleep, without relying on an Internet-facing service.

We propose Hypnoguard to protect all memory-resident
OS/user data across S3 suspensions, by first performing an
in-place full memory encryption before entering sleep, and
then restoring the plaintext content at wakeup-time through
an environment-bound, password-based authentication pro-
cess. The memory encryption key is effectively sealed in a
Trusted Platform Module (TPM) chip with the measure-
ment of the execution environment supported by CPUs
trusted execution mode (e.g., Intel TXT, AMD-V/SVM).
Password guessing within Hypnoguard may cause the mem-
ory content to be permanently inaccessible, while guessing
without Hypnoguard is equivalent to brute-forcing a high-
entropy key (due to TPM protection). We achieved full
memory encryption/decryption in less than a second on a
mainstream computer (Intel i7-4771 CPU with 8GB RAM,
taking advantage of multi-core processing and AES-NI), an
apparently acceptable delay for sleep-wake transitions. To
the best of our knowledge, Hypnoguard provides the first
wakeup-time secure environment for authentication and key
unlocking, without requiring per-application changes.
1.

|,Non-data
p1143-bost.pdf,|
Searchable Symmetric Encryption aims at making possible
searching over an encrypted database stored on an untrusted
server while keeping privacy of both the queries and the
data, by allowing some small controlled leakage to the server.
Recent work shows that dynamic schemes  in which the
data is efficiently updatable  leaking some information on
updated keywords are subject to devastating adaptative at-
tacks breaking the privacy of the queries. The only way
to thwart this attack is to design forward private schemes
whose update procedure does not leak if a newly inserted
element matches previous search queries.

This work proposes oo as a forward private SSE scheme
with performance similar to existing less secure schemes, and
that is conceptually simpler (and also more efficient) than
previous forward private constructions. In particular, it only
relies on trapdoor permutations and does not use an ORAM-
like construction. We also explain why oo is an optimal
point of the security/performance tradeoff for SSE.

Finally, an implementation and evaluation results demon-

strate its practical efficiency.

CCS Concepts
Security and privacy  Privacy-preserving proto-
cols; Security protocols; Management and querying
of encrypted data;

Keywords
Searchable Symmetric Encryption; Forward Privacy; Prov-
able Security; Implementation

1.

|,Data
p1426-krupp.pdf,|
Amplification DDoS attacks have gained popularity and be-
come a serious threat to Internet participants. However,
little is known about where these attacks originate, and re-
vealing the attack sources is a non-trivial problem due to the
spoofed nature of the traffic. In this paper, we present novel
techniques to uncover the infrastructures behind amplifica-
tion DDoS attacks. We follow a two-step approach to tackle
this challenge: First, we develop a methodology to impose a
fingerprint on scanners that perform the reconnaissance for
amplification attacks that allows us to link subsequent at-
tacks back to the scanner. Our methodology attributes over
58% of attacks to a scanner with a confidence of over 99.9%.
Second, we use Time-to-Live-based trilateration techniques
to map scanners to the actual infrastructures launching the
attacks. Using this technique, we identify 34 networks as be-
ing the source for amplification attacks at 98% certainty.

1.

|,Data
p1464-zhao.pdf,|
Identity concealment and zero-round trip time (0-RTT) con-
nection are two of current research focuses in the design
and analysis of secure transport protocols, like TLS1.3 and
Googles QUIC, in the client-server setting.
In this work,
we introduce a new primitive for identity-concealed authen-
ticated encryption in the public-key setting, referred to as
higncryption, which can be viewed as a novel monolithic inte-
gration of public-key encryption, digital signature, and iden-
tity concealment. We then present the security definitional
framework for higncryption, and a conceptually simple (yet
carefully designed) protocol construction.

As a new primitive, higncryption can have many applica-
tions. In this work, we focus on its applications to 0-RTT
authentication, showing higncryption is well suitable to and
compatible with QUIC and OPTLS, and on its applications
to identity-concealed authenticated key exchange (CAKE)
and unilateral CAKE (UCAKE). Of independent interest
is a new concise security definitional framework for CAKE
and UCAKE proposed in this work, which unifies the tradi-
tional BR and (post-ID) frameworks, enjoys composability,
and ensures very strong security guarantee. Along the way,
we make a systematically comparative study with related
protocols and mechanisms including Zhengs signcryption,
one-pass HMQV, QUIC, TLS1.3 and OPTLS, most of which
are widely standardized or in use.

1.

|,Non-data
p933-li.pdf,|
Mobile device losses and thefts are skyrocketing. The sen-
sitive data hosted on a lost/stolen device are fully exposed
to the adversary. Although password-based authentication
mechanisms are available on mobile devices, many users re-
portedly do not use them, and a device may be lost/stolen
while in the unlocked mode. This paper presents the de-
sign and evaluation of iLock, a secure and usable defense
against data theft on a lost/stolen mobile device.
iLock
automatically, quickly, and accurately recognizes the users
physical separation from his/her device by detecting and
analyzing the changes in wireless signals. Once significant
physical separation is detected, the device is immediately
locked to prevent data theft.
iLock relies on acoustic sig-
nals and requires at least one speaker and one microphone
that are available on most COTS (commodity-off-the-shelf)
mobile devices. Extensive experiments on Samsung Galaxy
S5 show that iLock can lock the device with negligible false
positives and negatives.

CCS Concepts
Human-centered computing  Mobile devices; Security
and privacy  Mobile and wireless security;

Keywords
Device locking, FMCW, audio ranging, smartphone security

1.

|,Non-data
06547126.pdf,|TLS is possibly the most used protocol for secure
communications, with a 18-year history of flaws and fixes,
ranging from its protocol logic to its cryptographic design, and
from the Internet standard to its diverse implementations.

We develop a verified reference implementation of TLS 1.2.
Our code fully supports its wire formats, ciphersuites, sessions
and connections, re-handshakes and resumptions, alerts and
errors, and data fragmentation, as prescribed in the RFCs; it
interoperates with mainstream web browsers and servers. At the
same time, our code is carefully structured to enable its modular,
automated verification, from its main API down to computational
assumptions on its cryptographic algorithms.

Our implementation is written in F# and specified in F7. We
present security specifications for its main components, such as
authenticated stream encryption for the record layer and key
establishment for the handshake. We describe their verification
using the F7 typechecker. To this end, we equip each crypto-
graphic primitive and construction of TLS with a new typed
interface that captures its security properties, and we gradually
replace concrete implementations with ideal functionalities. We
finally typecheck the protocol state machine, and obtain precise
security theorems for TLS, as it is implemented and deployed.
We also revisit classic attacks and report a few new ones.

I. |,Non-data
p406-kumaresan.pdf,|
Motivated by the impossibility of achieving fairness in secure com-
putation [Cleve, STOC 1986], recent works study a model of fair-
ness in which an adversarial party that aborts on receiving output
is forced to pay a mutually predefined monetary penalty to every
other party that did not receive the output. These works show how
to design protocols for secure computation with penalties that tol-
erate an arbitrary number of corruptions.

In this work, we improve the efficiency of protocols for secure
computation with penalties in a hybrid model where parties have
access to the claim-or-refund transaction functionality. Our first
improvement is for the ladder protocol of Bentov and Kumaresan
(Crypto 2014) where we improve the dependence of the script com-
plexity of the protocol (which corresponds to miner verification
load and also space on the blockchain) on the number of parties
from quadratic to linear (and in particular, is completely indepen-
dent of the underlying function). Our second improvement is for
the see-saw protocol of Kumaresan et al. (CCS 2015) where we re-
duce the total number of claim-or-refund transactions and also the
script complexity from quadratic to linear in the number of parties.
We also present a dual-mode protocol that offers different guar-
antees depending on the number of corrupt parties: (1) when s <
n/2 parties are corrupt, this protocol guarantees fairness (i.e., ei-
ther all parties get the output or none do), and (2) when t > n/2
parties are corrupt, this protocol guarantees fairness with penalties
(i.e., if the adversary gets the output, then either the honest parties
get output as well or they get compensation via penalizing the ad-
versary). The above protocol works as long as t + s < n, matching
the bound obtained for secure computation protocols in the stan-
dard model (i.e., replacing fairness with penalties with security-
with-abort (full security except fairness)) by Ishai et al. (SICOMP
2011).
Keywords: Bitcoin, secure computation, fairness.

1.

|,Non-data
p616-xi.pdf,|
Device-to-device communication is important to emerging
mobile applications such as Internet of Things and mobile
social networks. Authentication and key agreement among
multiple legitimate devices is the important first step to
build a secure communication channel. Existing solutions
put the devices into physical proximity and use the common
radio environment as a proof of identities and the common
secret to agree on a same key. However they experience very
slow secret bit generation rate and high errors, requiring sev-
eral minutes to build a 256-bit key. In this work, we design
and implement an authentication and key agreement proto-
col for mobile devices, called The Dancing Signals (TDS),
being extremely fast and error-free. TDS uses channel state
information (CSI) as the common secret among legitimate
devices. It guarantees that only devices in a close physical
proximity can agree on a key and any device outside a cer-
tain distance gets nothing about the key. Compared with
existing solutions, TDS is very fast and robust, support-
s group key agreement, and can effectively defend against
predictable channel attacks. We implement TDS using com-
modity off-the-shelf 802.11n devices and evaluate its perfor-
mance via extensive experiments. Results show that TDS
only takes a couple of seconds to make devices agree on a
256-bit secret key with high entropy.

Keywords
Group authentication; Key agreement; WiFi; CSI

1.

|,Non-data
sec15-paper-mcgregor.pdf,|

Though journalists are often cited as potential users of
computer security technologies, their practices and men-
tal models have not been deeply studied by the academic
computer security community. Such an understanding,
however, is critical to developing technical solutions that
can address the real needs of journalists and integrate
into their existing practices. We seek to provide that in-
sight in this paper, by investigating the general and com-
puter security practices of 15 journalists in the U.S. and
France via in-depth, semi-structured interviews. Among
our findings is evidence that existing security tools fail
not only due to usability issues but when they actively in-
terfere with other aspects of the journalistic process; that
communication methods are typically driven by sources
rather than journalists; and that journalists organizations
play an important role in influencing journalists behav-
iors. Based on these and other findings, we make recom-
mendations to the computer security community for im-
provements to existing tools and future lines of research.
1
In recent decades, improved digital communication tech-
nologies have reduced barriers to journalism worldwide.
Security weaknesses in these same technologies, how-
ever, have put journalists and their sources increasingly
at risk of identification, prosecution, and persecution by
powerful entities, threatening efforts in investigative re-
porting, transparency, and whistleblowing.

|,Data
sec14-paper-brocker.pdf,|TheubiquitouswebcamindicatorLEDisanimportantprivacyfeaturewhichprovidesavisualcuethatthecam-eraisturnedon.WedescribehowtodisabletheLEDonaclassofAppleinternaliSightwebcamsusedinsomeversionsofMacBooklaptopsandiMacdesktops.Thisenablesvideotobecapturedwithoutanyvisualindicationtotheuserandcanbeaccomplishedentirelyinuserspacebyanunprivileged(non-root)application.ThesametechniquethatallowsustodisabletheLED,namelyreprogrammingthefirmwarethatrunsontheiSight,enablesavirtualmachineescapewherebymalwarerunninginsideavirtualmachinereprogramsthecameratoactasaUSBHumanInterfaceDevice(HID)keyboardwhichexecutescodeinthehostoperatingsystem.Webuildtwoproofs-of-concept:(1)anOSXapplica-tion,iSeeYou,whichdemonstratescapturingvideowiththeLEDdisabled;and(2)avirtualmachineescapethatlaunchesTerminal.appandrunsshellcommands.Tode-fendagainsttheseandrelatedthreats,webuildanOSXkernelextension,iSightDefender,whichprohibitsthemodificationoftheiSightsfirmwarefromuserspace.1|,Non-data
sec14-paper-vijayakumar.pdf,|

Processes retrieve a variety of resources, such as files,
from the operating system to function. However, se-
curely accessing resources has proven to be a challenging
task, accounting for 10-15% of vulnerabilities reported
each year. Current defenses address only a subset of
these vulnerabilities in ad-hoc and incomplete ways. In
this paper, we provide a comprehensive defense against
vulnerabilities during resource access. First, we iden-
tify a fundamental reason that resource access vulnera-
bilities exist  a mismatch between programmer expec-
tations and the actual environment the program runs in.
To address such mismatches, we propose JIGSAW, a sys-
tem that can automatically derive programmer expecta-
tions and enforce it on the deployment. JIGSAW con-
structs programmer expectations as a name flow graph,
which represents the data flows from the inputs used to
construct file pathnames to the retrieval of system re-
sources using those pathnames. We find that whether
a program makes any attempt to filter such flows im-
plies expectations about the threats the programmer ex-
pects during resource retrieval, the enabling JIGSAW to
enforce those expectations. We evaluated JIGSAW on
widely-used programs and found that programmers have
many implicit expectations. These mismatches led us to
discover two previously-unknown vulnerabilities and a
default misconfiguration in the Apache webserver. JIG-
SAW enforces program expectations for approximately
5% overhead for Apache webservers, thus eliminating
vulnerabilities due to resource access efficiently and in
a principled manner.

|,Non-data
06547134.pdf,|Fine-grained address space layout randomization
(ASLR) has recently been proposed as a method of efficiently
mitigating runtime attacks. In this paper, we introduce the
design and implementation of a framework based on a novel
attack strategy, dubbed just-in-time code reuse, that undermines
the benefits of fine-grained ASLR. Specifically, we derail the
assumptions embodied in fine-grained ASLR by exploiting the
ability to repeatedly abuse a memory disclosure to map an
applications memory layout on-the-fly, dynamically discover
API functions and gadgets, and JIT-compile a target program
using those gadgetsall within a script environment at the
time an exploit is launched. We demonstrate the power of
our framework by using it in conjunction with a real-world
exploit against Internet Explorer, and also provide extensive
evaluations that demonstrate the practicality of just-in-time
code reuse attacks. Our findings suggest that fine-grained
ASLR may not be as promising as first thought.

I. |,Non-data
p883-hojjati.pdf,|
From pencils to commercial aircraft, every man-made object must
be designed and manufactured. When it is cheaper or easier to steal a
design or a manufacturing process specification than to invent ones
own, the incentive for theft is present. As more and more manufac-
turing data comes online, incidents of such theft are increasing.

In this paper, we present a side-channel attack on manufacturing
equipment that reveals both the form of a product and its manufac-
turing process, i.e., exactly how it is made. In the attack, a human
deliberately or accidentally places an attack-enabled phone close
to the equipment or makes or receives a phone call on any phone
nearby. The phone executing the attack records audio and, optional-
ly, magnetometer data. We present a method of reconstructing the
products form and manufacturing process from the captured data,
based on machine learning, signal processing, and human assistance.
We demonstrate the attack on a 3D printer and a CNC mill, each
with its own acoustic signature, and discuss the commonalities in the
sensor data captured for these two different machines. We compare
the quality of the data captured with a variety of smartphone models.
Capturing data from the 3D printer, we reproduce the form and
process information of objects previously unknown to the recon-
structors. On average, our accuracy is within 1 mm in reconstructing
the length of a line segment in a fabricated objects shape and within
1 degree in determining an angle in a fabricated objects shape.

We conclude with recommendations for defending against these

attacks.

1.

|,Data
sec14-paper-luchaup.pdf,|

Encryption schemes where the ciphertext must abide by a
specified format have diverse applications, ranging from
in-place encryption in databases to per-message encryp-
tion of network traffic for censorship circumvention. De-
spite this, a unifying framework for deploying such en-
cryption schemes has not been developed. One conse-
quence of this is that current schemes are ad-hoc; another
is a requirement for expert knowledge that can disuade
one from using encryption at all.

We present a general-purpose library (called libfte)
that aids engineers in the development and deploy-
ment of format-preserving encryption (FPE) and format-
transforming encryption (FTE) schemes. It incorporates
a new algorithmic approach for performing FPE/FTE
using the nondeterministic finite-state automata (NFA)
representation of a regular expression when specifying
formats. This approach was previously considered un-
workable, and our approach closes this open problem.
We evaluate libfte and show that, compared to other en-
cryption solutions, it introduces negligible latency over-
head, and can decrease diskspace usage by as much
as 62.5% when used for simultaneous encryption and
compression in a PostgreSQL database (both relative to
conventional encryption mechanisms).
In the censor-
ship circumvention setting we show that, using regular-
expression formats lifted from the Snort IDS, libfte can
reduce client/server memory requirements by as much as
30%.

1

|,Non-data
p1092-urbina.pdf,|
While attacks on information systems have for most prac-
tical purposes binary outcomes (information was manipu-
lated/eavesdropped, or not), attacks manipulating the sen-
sor or control signals of Industrial Control Systems (ICS) can
be tuned by the attacker to cause a continuous spectrum in
damages. Attackers that want to remain undetected can at-
tempt to hide their manipulation of the system by following
closely the expected behavior of the system, while injecting
just enough false information at each time step to achieve
their goals.

In this work, we study if physics-based attack detection
can limit the impact of such stealthy attacks. We start with
a comprehensive review of related work on attack detection
schemes in the security and control systems community. We
then show that many of these works use detection schemes
that are not limiting the impact of stealthy attacks. We pro-
pose a new metric to measure the impact of stealthy attacks
and how they relate to our selection on an upper bound on
false alarms. We finally show that the impact of such attacks
can be mitigated in several cases by the proper combination
and configuration of detection schemes. We demonstrate
the eectiveness of our algorithms through simulations and
experiments using real ICS testbeds and real ICS systems.

Keywords
Industrial Control Systems; Intrusion Detection; Security
Metrics; Stealthy Attacks; Physics-Based Detection; Cyber-
Physical Systems

1.

|,Data
lost_in_space.pdf,|
One challenge in understanding the evolution of Internet in-
frastructure is the lack of systematic mechanisms for moni-
toring the extent to which allocated IP addresses are actually
used. In this paper we try to advance the science of infer-
ring IPv4 address space utilization by analyzing and corre-
lating results obtained through different types of measure-
ments. We have previously studied an approach based on
passive measurements that can reveal used portions of the
address space unseen by active approaches. In this paper,
we study such passive approaches in detail, extending our
methodology to four different types of vantage points, iden-
tifying traffic components that most significantly contribute
to discovering used IPv4 network blocks. We then combine
the results we obtained through passive measurements to-
gether with data from active measurement studies, as well as
measurements from BGP and additional datasets available to
researchers. Through the analysis of this large collection of
heterogeneous datasets, we substantially improve the state
of the art in terms of: (i) understanding the challenges and
opportunities in using passive and active techniques to study
address utilization; and (ii) knowledge of the utilization of
the IPv4 space.

1.

|,Data
06547131.pdf,|The Transport Layer Security (TLS) protocol
aims to provide confidentiality and integrity of data in transit
across untrusted networks. TLS has become the de facto secure
protocol of choice for Internet and mobile applications. DTLS
is a variant of TLS that is growing in importance. In this
paper, we present distinguishing and plaintext recovery attacks
against TLS and DTLS. The attacks are based on a delicate
timing analysis of decryption processing in the two protocols.
We include experimental results demonstrating the feasibility of
the attacks in realistic network environments for several differ-
ent implementations of TLS and DTLS, including the leading
OpenSSL implementations. We provide countermeasures for
the attacks. Finally, we discuss the wider implications of our
attacks for the cryptographic design used by TLS and DTLS.
Keywords-TLS, DTLS, CBC-mode encryption, timing attack,

plaintext recovery

I. |,Non-data
p1541-liao.pdf,|
The popularity of cloud hosting services also brings in new security
challenges:
it has been reported that these services are increas-
ingly utilized by miscreants for their malicious online activities.
Mitigating this emerging threat, posed by such bad repositories
(simply Bar), is challenging due to the different hosting strategy
to traditional hosting service, the lack of direct observations of the
repositories by those outside the cloud, the reluctance of the cloud
provider to scan its customers repositories without their consent,
and the unique evasion strategies employed by the adversary. In this
paper, we took the first step toward understanding and detecting this
emerging threat. Using a small set of seeds (i.e., confirmed Bars),
we identified a set of collective features from the websites they
serve (e.g., attempts to hide Bars), which uniquely characterize the
Bars. These features were utilized to build a scanner that detected
over 600 Bars on leading cloud platforms like Amazon, Google,
and 150K sites, including popular ones like groupon.com, using
them. Highlights of our study include the pivotal roles played by
these repositories on malicious infrastructures and other important
discoveries include how the adversary exploited legitimate cloud
repositories and why the adversary uses Bars in the first place that
has never been reported. These findings bring such malicious ser-
vices to the spotlight and contribute to a better understanding and
ultimately eliminating this new threat.

1.

|,Data
p254-luu.pdf,|
Cryptocurrencies record transactions in a decentralized data
structure called a blockchain. Two of the most popular
cryptocurrencies, Bitcoin and Ethereum, support the fea-
ture to encode rules or scripts for processing transactions.
This feature has evolved to give practical shape to the ideas
of smart contracts, or full-fledged programs that are run on
blockchains. Recently, Ethereums smart contract system
has seen steady adoption, supporting tens of thousands of
contracts, holding millions dollars worth of virtual coins.

In this paper, we investigate the security of running smart
contracts based on Ethereum in an open distributed network
like those of cryptocurrencies. We introduce several new se-
curity problems in which an adversary can manipulate smart
contract execution to gain profit. These bugs suggest subtle
gaps in the understanding of the distributed semantics of the
underlying platform. As a refinement, we propose ways to
enhance the operational semantics of Ethereum to make con-
tracts less vulnerable. For developers writing contracts for
the existing Ethereum system, we build a symbolic execution
tool called Oyente to find potential security bugs. Among
19, 366 existing Ethereum contracts, Oyente flags 8, 833 of
them as vulnerable, including the TheDAO bug which led
to a 60 million US dollar loss in June 2016. We also discuss
the severity of other attacks for several case studies which
have source code available and confirm the attacks (which
target only our accounts) in the main Ethereum network.

1.

|,Data
sec14-paper-wang-gang.pdf,|

Recent work in security and systems has embraced the
use of machine learning (ML) techniques for identify-
ing misbehavior, e.g. email spam and fake (Sybil) users
in social networks. However, ML models are typically
derived from fixed datasets, and must be periodically
retrained.
In adversarial environments, attackers can
adapt by modifying their behavior or even sabotaging
ML models by polluting training data.
In this paper1, we perform an empirical study of ad-
versarial attacks against machine learning models in the
context of detecting malicious crowdsourcing systems,
where sites connect paying users with workers willing to
carry out malicious campaigns. By using human work-
ers, these systems can easily circumvent deployed se-
curity mechanisms, e.g. CAPTCHAs. We collect a
dataset of malicious workers actively performing tasks
on Weibo, Chinas Twitter, and use it to develop ML-
based detectors. We show that traditional ML techniques
are accurate (95%99%) in detection but can be highly
vulnerable to adversarial attacks, including simple eva-
sion attacks (workers modify their behavior) and power-
ful poisoning attacks (where administrators tamper with
the training set). We quantify the robustness of ML clas-
sifiers by evaluating them in a range of practical adver-
sarial models using ground truth data. Our analysis pro-
vides a detailed look at practical adversarial attacks on
ML models, and helps defenders make informed deci-
sions in the design and configuration of ML detectors.

1 |,Data
p1530-vissers.pdf,|
The increase of Distributed Denial-of-Service (DDoS) at-
tacks in volume, frequency, and complexity, combined with
the constant required alertness for mitigating web applica-
tion threats, has caused many website owners to turn to
Cloud-based Security Providers (CBSPs) to protect their in-
frastructure. These solutions typically involve the rerouting
of tra c from the original website through the CBSPs net-
work, where malicious tra c can be detected and absorbed
before it ever reaches the servers of the protected website.
The most popular Cloud-based Security Providers do not re-
quire the purchase of dedicated tra c-rerouting hardware,
but rely solely on changing the DNS settings of a domain
name to reroute a websites tra c through their security in-
frastructure. Consequently, this rerouting mechanism can
be completely circumvented by directly attacking the web-
sites hosting IP address. Therefore, it is crucial for the
security and availability of these websites that their real IP
address remains hidden from potential attackers.

In this paper, we discuss existing, as well as novel origin-
exposing attack vectors which attackers can leverage to dis-
cover the IP address of the server where a website protected
by a CBSP is hosted. To assess the impact of the discussed
origin-exposing vectors on the security of CBSP-protected
websites, we consolidate all vectors into Cloudpiercer, an
automated origin-exposing tool, which we then use to con-
duct the first large-scale analysis of the eectiveness of the
origin-exposing vectors. Our results show that the problem
is severe: 71.5% of the 17,877 CBSP-protected websites that
we tested, expose their real IP address through at least one
of the evaluated vectors. The results of our study categori-
cally demonstrate that a comprehensive adoption of CBSPs
is harder than just changing DNS records. Our findings
can steer CBSPs and site administrators towards eective
countermeasures, such as proactively scanning for origin ex-
posure and using appropriate network configurations that
can greatly reduce the threat.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS15, October 1216, 2015, Denver, Colorado, USA.
c  2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813633.

Categories and Subject Descriptors
C.2.0 [Computer-communication Networks]: [Security
and protection]; K.6.5 [Security and Protection]: [Unau-
thorized access]

Keywords
Cloud-based security; DDoS attacks; Web attacks

1.

|,Data
p830-keller.pdf,|
We consider the task of secure multi-party computation of
arithmetic circuits over a finite field. Unlike Boolean cir-
cuits, arithmetic circuits allow natural computations on in-
tegers to be expressed easily and efficiently. In the strongest
setting of malicious security with a dishonest majority 
where any number of parties may deviate arbitrarily from
the protocol  most existing protocols require expensive
public-key cryptography for each multiplication in the pre-
processing stage of the protocol, which leads to a high total
cost.

We present a new protocol that overcomes this limita-
tion by using oblivious transfer to perform secure multipli-
cations in general finite fields with reduced communication
and computation. Our protocol is based on an arithmetic
view of oblivious transfer, with careful consistency checks
and other techniques to obtain malicious security at a cost of
less than 6 times that of semi-honest security. We describe a
highly optimized implementation together with experimen-
tal results for up to five parties. By making extensive use of
parallelism and SSE instructions, we improve upon previous
runtimes for MPC over arithmetic circuits by more than 200
times.

Keywords
Multi-party computation; oblivious transfer

1.

|,Non-data
sigcomm14_ipv6.pdf,|
After several IPv4 address exhaustion milestones in the last three
years, it is becoming apparent that the world is running out of IPv4
addresses, and the adoption of the next generation Internet proto-
col, IPv6, though nascent, is accelerating. In order to better un-
derstand this unique and disruptive transition, we explore twelve
metrics using ten global-scale datasets to create the longest and
broadest measurement of IPv6 adoption to date. Using this per-
spective, we find that adoption, relative to IPv4, varies by two or-
ders of magnitude depending on the measure examined and that
care must be taken when evaluating adoption metrics in isolation.
Further, we find that regional adoption is not uniform. Finally, and
perhaps most surprisingly, we find that over the last three years, the
nature of IPv6 utilizationin terms of traffic, content, reliance on
transition technology, and performancehas shifted dramatically
from prior findings, indicating a maturing of the protocol into pro-
duction mode. We believe IPv6s recent growth and this changing
utilization signal a true quantum leap.

Categories and Subject Descriptors
C.2.5 [Local and Wide-Area Networks]: Internet

Keywords
Internet; IP; IPv4; IPv6; DNS; Measurement

1.

|,Data
sec15-paper-ur.pdf,|
guessabilityhow many
Parameterized
guesses a particular cracking algorithm with particular
training data would take to guess a passwordhas
become a common metric of password security. Unlike
statistical metrics, it aims to model real-world attackers
and to provide per-password strength estimates. We
investigate how cracking approaches often used by
researchers compare to real-world cracking by profes-
sionals, as well as how the choice of approach biases
research conclusions.

We find that semi-automated cracking by profession-
als outperforms popular fully automated approaches, but
can be approximated by combining multiple such ap-
proaches. These approaches are only effective, however,
with careful configuration and tuning; in commonly used
default configurations, they underestimate the real-world
guessability of passwords. We find that analyses of large
password sets are often robust to the algorithm used for
guessing as long as it is configured effectively. However,
cracking algorithms differ systematically in their effec-
tiveness guessing passwords with certain common fea-
tures (e.g., character substitutions). This has important
implications for analyzing the security of specific pass-
word characteristics or of individual passwords (e.g., in a
password meter or security audit). Our results highlight
the danger of relying only on a single cracking algorithm
as a measure of password strength and constitute the first
scientific evidence that automated guessing can often ap-
proximate guessing by professionals.

1

|,Data
p319-backes.pdf,|
The continuous decrease in cost of molecular profiling tests
is revolutionizing medical research and practice, but it also
raises new privacy concerns. One of the first attacks against
privacy of biological data, proposed by Homer et al. in 2008,
showed that, by knowing parts of the genome of a given in-
dividual and summary statistics of a genome-based study,
it is possible to detect if this individual participated in the
study. Since then, a lot of work has been carried out to fur-
ther study the theoretical limits and to counter the genome-
based membership inference attack. However, genomic data
are by no means the only or the most influential biological
data threatening personal privacy. For instance, whereas
the genome informs us about the risk of developing some
diseases in the future, epigenetic biomarkers, such as mi-
croRNAs, are directly and deterministically affected by our
health condition including most common severe diseases.

In this paper, we show that the membership inference at-
tack also threatens the privacy of individuals contributing
their microRNA expressions to scientific studies. Our results
on real and public microRNA expression data demonstrate
that disease-specific datasets are especially prone to mem-
bership detection, offering a true-positive rate of up to 77%
at a false-negative rate of less than 1%. We present two at-
tacks: one relying on the L1 distance and the other based on
the likelihood-ratio test. We show that the likelihood-ratio
test provides the highest adversarial success and we derive
a theoretical limit on this success. In order to mitigate the
membership inference, we propose and evaluate both a dif-
ferentially private mechanism and a hiding mechanism. We
also consider two types of adversarial prior knowledge for
the differentially private mechanism and show that, for rel-
atively large datasets, this mechanism can protect the pri-
vacy of participants in miRNA-based studies against strong
adversaries without degrading the data utility too much.
Based on our findings and given the current number of miR-
NAs, we recommend to only release summary statistics of
datasets containing at least a couple of hundred individuals.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978355

Keywords
Health privacy; Membership privacy; Differential privacy

1.

|,Data
p591-willers.pdf,|
A key requirement for most security solutions is to provide
secure cryptographic key storage in a way that will easily
scale in the age of the Internet of Things. In this paper, we
focus on providing such a solution based on Physical Unclon-
able Functions (PUFs). To this end, we focus on microelec-
tromechanical systems (MEMS)-based gyroscopes and show
via wafer-level measurements and simulations, that it is fea-
sible to use the physical and electrical properties of these
sensors for cryptographic key generation. After identifying
the most promising features, we propose a novel quantiza-
tion scheme to extract bit strings from the MEMS analog
measurements. We provide upper and lower bounds for the
minimum entropy of the derived bit strings and fully analyze
the intra- and inter-class distributions across the operation
range of the MEMS device. We complement these mea-
surements via Monte-Carlo simulations based on the distri-
butions of the parameters measured on actual devices. We
also propose and evaluate a complete cryptographic key gen-
eration chain based on fuzzy extractors. We derive a full en-
tropy 128-bit key using the obtained min-entropy estimates,
requiring 1219 bits of helper data with an (authentication)
failure probability of 4  107.
In addition, we propose a
dedicated MEMS-PUF design, which is superior to our mea-
sured sensor, in terms of chip area, quality and quantity of
key seed features.

Keywords
Hardware security; IoT security; Mobile security and privacy

1.

|,Non-data
p130-huang.pdf,|
Hardware Trojan detection has emerged as a critical chal-
lenge to ensure security and trustworthiness of integrated
circuits. A vast majority of research efforts in this area has
utilized side-channel analysis for Trojan detection. Func-
tional test generation for logic testing is a promising al-
ternative but it may not be helpful if a Trojan cannot be
fully activated or the Trojan effect cannot be propagated
to the observable outputs. Side-channel analysis, on the
other hand, can achieve significantly higher detection cover-
age for Trojans of all types/sizes, since it does not require
activation/propagation of an unknown Trojan. However,
they have often limited effectiveness due to poor detection
sensitivity under large process variations and small Trojan
footprint in side-channel signature. In this paper, we address
this critical problem through a novel side-channel-aware test
generation approach, based on a concept of Multiple Exci-
tation of Rare Switching (MERS), that can significantly in-
crease Trojan detection sensitivity. The paper makes several
important contributions: i) it presents in detail the statisti-
cal test generation method, which can generate high-quality
testset for creating high relative activity in arbitrary Tro-
jan instances; ii) it analyzes the effectiveness of generated
testset in terms of Trojan coverage; and iii) it describes two
judicious reordering methods can further tune the testset
and greatly improve the side channel sensitivity. Simulation
results demonstrate that the tests generated by MERS can
significantly increase the Trojans sensitivity, thereby making
Trojan detection effective using side-channel analysis.

CCS Concepts
Hardware  Hardware test; Very large scale integra-
tion design; Hardware validation;

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978396

Keywords
Hardware Security; Hardware Trojan Detection; Side-Channel
Analysis; Statistical Test Generation.

1.

|,Non-data
p444-bellare.pdf,|

We give attacks on Feistel-based format-preserving encryp-
tion (FPE) schemes that succeed in message recovery (not
merely distinguishing scheme outputs from random) when
the message space is small. For 4-bit messages, the at-
tacks fully recover the target message using 221 examples
for the FF3 NIST standard and 225 examples for the FF1
NIST standard. The examples include only three messages
per tweak, which is what makes the attacks non-trivial even
though the total number of examples exceeds the size of the
domain. The attacks are rigorously analyzed in a new defini-
tional framework of message-recovery security. The attacks
are easily put out of reach by increasing the number of Feistel
rounds in the standards.

|,Non-data
1402.3364.pdf,|. Based on solid theoretical foundations, we present strong evidences that a number of real-
life networks, taken from different domains like Internet measurements, biological data, web graphs,
social and collaboration networks, exhibit tree-like structures from a metric point of view. We investigate
few graph parameters, namely, the tree-distortion and the tree-stretch, the tree-length and the tree-
breadth, the Gromovs hyperbolicity, the cluster-diameter and the cluster-radius in a layering partition
of a graph, which capture and quantify this phenomenon of being metrically close to a tree. By bringing
all those parameters together, we not only provide efficient means for detecting such metric tree-like
structures in large-scale networks but also show how such structures can be used, for example, to
efficiently and compactly encode approximate distance and almost shortest path information and to
fast and accurately estimate diameters and radii of those networks. Estimating the diameter and the
radius of a graph or distances between its arbitrary vertices are fundamental primitives in many data
and graph mining algorithms.

1

|,Data
p1268-liu.pdf,|
Volumetric attacks, which overwhelm the bandwidth of a
destination, are amongst the most common DDoS attacks
today. One practical approach to addressing these attacks is
to redirect all destination traffic (e.g., via DNS or BGP) to
a third-party, DDoS-protection-as-a-service provider (e.g.,
CloudFlare) that is well provisioned and equipped with fil-
tering mechanisms to remove attack traffic before passing
the remaining benign traffic to the destination. An alterna-
tive approach is based on the concept of network capabili-
ties, whereby source sending rates are determined by receiver
consent, in the form of capabilities enforced by the network.
While both third-party scrubbing services and network ca-
pabilities can be effective at reducing unwanted traffic at
an overwhelmed destination, DDoS-protection-as-a-service
solutions outsource all of the scheduling decisions (e.g., fair-
ness, priority and attack identification) to the provider, while
capability-based solutions require extensive modifications to
existing infrastructure to operate. In this paper we intro-
duce MiddlePolice, which seeks to marry the deployability of
DDoS-protection-as-a-service solutions with the destination-
based control of network capability systems. We show that
by allowing feedback from the destination to the provider,
MiddlePolice can effectively enforce destination-chosen poli-
cies, while requiring no deployment from unrelated parties.

1.

|,Data
sec14-paper-lau.pdf,|

Users are increasingly storing, accessing, and ex-
changing data through public cloud services such as
those provided by Google, Facebook, Apple, and Mi-
crosoft. Although users may want to have faith in cloud
providers to provide good security protection, the confi-
dentiality of any data in public clouds can be violated,
and consequently, while providers may not be doing
evil, we can not and should not trust them with data con-
fidentiality.

To better protect the privacy of user data stored in the
cloud, in this paper we propose a privacy-preserving sys-
tem called Mimesis Aegis (M-Aegis) that is suitable for
mobile platforms. M-Aegis is a new approach to user
data privacy that not only provides isolation but also pre-
serves the user experience through the creation of a con-
ceptual layer called Layer 7.5 (L-7.5), which is inter-
posed between the application (OSI Layer 7) and the user
(Layer 8). This approach allows M-Aegis to implement
true end-to-end encryption of user data with three goals
in mind: 1) complete data and logic isolation from un-
trusted entities; 2) the preservation of original user ex-
perience with target apps; and 3) applicable to a large
number of apps and resilient to app updates.

In order to preserve the exact application workflow
and look-and-feel, M-Aegis uses L-7.5 to put a transpar-
ent window on top of existing application GUIs to both
intercept plaintext user input before transforming the in-
put and feeding it to the underlying app, and to reverse-
transform the output data from the app before displaying
the plaintext data to the user. This technique allows M-
Aegis to transparently integrate with most cloud services
without hindering usability and without the need for re-
verse engineering. We implemented a prototype of M-
Aegis on Android and show that it can support a number
of popular cloud services, e.g. Gmail, Facebook Messen-
ger, WhatsApp, etc.

Our performance evaluation and user study show that
users incur minimal overhead when adopting M-Aegis

on Android:
imperceptible encryption/decryption la-
tency and a low and adjustable false positive rate when
searching over encrypted data.

1

|,Non-data
p217-bacis.pdf,|
We present an approach to enforce access revocation on re-
sources stored at external cloud providers. The approach
relies on a resource transformation that provides strong mu-
tual inter-dependency in its encrypted representation. To
revoke access on a resource, it is then sufficient to update a
small portion of it, with the guarantee that the resource as
a whole (and any portion of it) will become unintelligible to
those from whom access is revoked. The extensive experi-
mental evaluation on a variety of configurations confirmed
the effectiveness and efficiency of our solution, which showed
excellent performance and compatibility with several imple-
mentation strategies.

Keywords
Access control; Policy revocation; Resource encryption;
Mix&Slice

1.

|,Non-data
p430-grassi.pdf,|
We discuss the design of symmetric primitives, in partic-
ular Pseudo-Random Functions (PRFs) which are suitable
for use in a secret-sharing based MPC system. We consider
three different PRFs: the Naor-Reingold PRF, a PRF based
on the Legendre symbol, and a specialized block cipher de-
sign called MiMC. We present protocols for implementing
these PRFs within a secret-sharing based MPC system, and
discuss possible applications. We then compare the per-
formance of our protocols. Depending on the application,
different PRFs may offer different optimizations and advan-
tages over the classic AES benchmark. Thus, we cannot
conclude that there is one optimal PRF to be used in all
situations.

1.

|,Non-data
p895-song.pdf,|
Additive manufacturing, also known as 3D printing, has
been increasingly applied to fabricate highly intellectual prop-
erty (IP) sensitive products. However, the related IP protec-
tion issues in 3D printers are still largely underexplored. On
the other hand, smartphones are equipped with rich onboard
sensors and have been applied to pervasive mobile surveil-
lance in many applications. These facts raise one critical
question:
is it possible that smartphones access the side-
channel signals of 3D printer and then hack the IP infor-
mation? To answer this, we perform an end-to-end study
on exploring smartphone-based side-channel attacks against
3D printers. Specifically, we formulate the problem of the
IP side-channel attack in 3D printing. Then, we investigate
the possible acoustic and magnetic side-channel attacks us-
ing the smartphone built-in sensors. Moreover, we explore a
magnetic-enhanced side-channel attack model to accurately
deduce the vital directional operations of 3D printer. Ex-
perimental results show that by exploiting the side-channel
signals collected by smartphones, we can successfully re-
construct the physical prints and their G-code with Mean
Tendency Error of 5.87% on regular designs and 9.67% on
complex designs, respectively. Our study demonstrates this
new and practical smartphone-based side channel attack on
compromising IP information during 3D printing.

1.

|,Data
sec14-paper-jansen.pdf,|

Tors growing popularity and user diversity has re-
sulted in network performance problems that are not
well understood. A large body of work has attempted
to solve these problems without a complete understand-
ing of where congestion occurs in Tor.
In this paper,
we first study congestion in Tor at individual relays as
well as along the entire end-to-end Tor path and find
that congestion occurs almost exclusively in egress ker-
nel socket buffers. We then analyze Tors socket interac-
tions and discover two major issues affecting congestion:
Tor writes sockets sequentially, and Tor writes as much
as possible to each socket. We thus design, implement,
and test KIST: a new socket management algorithm that
uses real-time kernel information to dynamically com-
pute the amount to write to each socket while consider-
ing all writable circuits when scheduling new cells. We
find that, in the medians, KIST reduces circuit conges-
tion by over 30 percent, reduces network latency by 18
percent, and increases network throughput by nearly 10
percent. We analyze the security of KIST and find an ac-
ceptable performance and security trade-off, as it does
not significantly affect the outcome of well-known la-
tency and throughput attacks. While our focus is Tor,
our techniques and observations should help analyze and
improve overlay and application performance, both for
security applications and in general.

1

|,Data
p1118-tu.pdf,|
SMS (Short Messaging Service) is a text messaging service for mo-
bile users to exchange short text messages. It is also widely used to
provide SMS-powered services (e.g., mobile banking). With the
rapid deployment of all-IP 4G mobile networks, the underlying
technology of SMS evolves from the legacy circuit-switched net-
work to the IMS (IP Multimedia Subsystem) system over packet-
switched network.
In this work, we study the insecurity of the
IMS-based SMS. We uncover its security vulnerabilities and ex-
ploit them to devise four SMS attacks: silent SMS abuse, SMS
spoofing, SMS client DoS, and SMS spamming. We further dis-
cover that those SMS threats can propagate towards SMS-powered
services, thereby leading to three malicious attacks: social network
account hijacking, unauthorized donation, and unauthorized sub-
scription. Our analysis reveals that the problems stem from the
loose security regulations among mobile phones, carrier networks,
and SMS-powered services. We finally propose remedies to the
identified security issues.

Keywords
Mobile networks; LTE; IMS; SMS; attack; defense

1.

|,Non-data
06547114.pdf,|. We design and build ObliviStore, a high performance,
distributed ORAM-based cloud data store secure in the malicious
model. To the best of our knowledge, ObliviStore is the fastest
ORAM implementation known to date, and is faster by 10X or
more in comparison with the best known ORAM implementation.
ObliviStore achieves high throughput by making I/O operations
asynchronous. Asynchrony introduces security challenges, i.e., we
must prevent information leakage not only through access patterns,
but also through timing of I/O events. We propose various practical
optimizations which are key to achieving high performance, as well
as techniques for a data center to dynamically scale up a distributed
ORAM. We show that with 11 trusted machines (each with a
modern CPU), and 20 Solid State Drives, ObliviStore achieves a
throughput of 31.5MB/s with a block size of 4KB.

I. |,Non-data
p1019-liu.pdf,|
With the proliferation of Internet of Things, there is a grow-
ing interest in embedded system attacks, e.g., key extrac-
tion attacks and firmware modification attacks. Code execu-
tion tracking, as the first step to locate vulnerable instruction
pieces for key extraction attacks and to conduct control-flow
integrity checking against firmware modification attacks, is
therefore of great value. Because embedded systems, espe-
cially legacy embedded systems, have limited resources and
may not support software or hardware update, it is impor-
tant to design low-cost code execution tracking methods that
require as little system modification as possible. In this work,
we propose a non-intrusive code execution tracking solution
via power-side channel, wherein we represent the code ex-
ecution and its power consumption with a revised hidden
Markov model and recover the most likely executed instruc-
tion sequence with a revised Viterbi algorithm. By observing
the power consumption of the microcontroller unit during ex-
ecution, we are able to recover the program execution flow
with a high accuracy and detect abnormal code execution be-
havior even when only a single instruction is modified.

1.

|,Non-data
06547108.pdf,|Wireless communication provides unique security
challenges, but also enables novel ways to defend against
attacks. In the past few years, a number of works discussed the
use of friendly jamming to protect the confidentiality of the com-
municated data as well as to enable message authentication and
access control. In this work, we analytically and experimentally
evaluate the confidentiality that can be achieved by the use of
friendly jamming, given an attacker with multiple receiving
antennas. We construct a MIMO-based attack that allows the
attacker to recover data protected by friendly jamming and
refine the conditions for which this attack is most effective.
Our attack shows that friendly jamming cannot provide strong
confidentiality guarantees in all settings. We further test our
attack in a setting where friendly jamming is used to protect
the communication to medical implants.

I. |,Non-data
1601.01116v1.pdf,|

Route diversity in networks is elemental for establishing reliable, high-capacity connections with appropriate security between
endpoints. As for the Internet, route diversity has already been studied at both Autonomous System- and router-level topologies by
means of graph theoretical disjoint paths. In this paper we complement these approaches by proposing a method for measuring the
diversity of Internet paths in a geographical sense. By leveraging the recent developments in IP geolocation we show how to map
the paths discovered by traceroute into geographically equivalent classes. This allows us to identify the geographical footprints
of the major transmission paths between end-hosts, and building on our observations, we propose a quantitative measure for
geographical diversity of Internet routes between any two hosts.

geodiversity; traceroute; geolocation; disjoint routes

Index Terms

I. |,Data
sec14-paper-karapanos.pdf,|

In this paper we consider TLS Man-In-The-Middle
(MITM) attacks in the context of web applications,
where the attacker is able to successfully impersonate
the legitimate server to the user, with the goal of imper-
sonating the user to the server and thus compromising
the users online account and data. We describe in detail
why the recently proposed client authentication protocols
based on TLS Channel IDs, as well as client web authen-
tication in general, cannot fully prevent such attacks.

Nevertheless, we show that strong client authentica-
tion, such as Channel ID-based authentication, can be
combined with the concept of server invariance, a weaker
and easier to achieve property than server authentica-
tion, in order to protect against the considered attacks.
We specifically leverage Channel ID-based authentica-
tion in combination with server invariance to create a
novel mechanism that we call SISCA: Server Invariance
with Strong Client Authentication. SISCA resists user
impersonation via TLS MITM attacks, regardless of how
the attacker is able to successfully achieve server imper-
sonation. We analyze our proposal and show how it can
be integrated in todays web infrastructure.
1

|,Non-data
sec14-paper-wang-tielei.pdf,|

While Apple iOS has gained increasing attention from
attackers due to its rising popularity, very few large scale
infections of iOS devices have been discovered because
of iOS advanced security architecture.
In this paper,
we show that infecting a large number of iOS devices
through botnets is feasible. By exploiting design flaws
and weaknesses in the iTunes syncing process, the de-
vice provisioning process, and in file storage, we demon-
strate that a compromised computer can be instructed to
install Apple-signed malicious apps on a connected iOS
device, replace existing apps with attacker-signed ma-
licious apps, and steal private data (e.g., Facebook and
Gmail app cookies) from an iOS device. By analyzing
DNS queries generated from more than half a million
anonymized IP addresses in known botnets, we measure
that on average, 23% of bot IP addresses demonstrate
iOS device existence and Windows iTunes purchases,
implying that 23% of bots will eventually have connec-
tions with iOS devices, thus making a large scale infec-
tion feasible.

1

|,Data
p154-carlsten.pdf,|
Bitcoin provides two incentives for miners: block rewards
and transaction fees. The former accounts for the vast ma-
jority of miner revenues at the beginning of the system, but
it is expected to transition to the latter as the block rewards
dwindle. There has been an implicit belief that whether
miners are paid by block rewards or transaction fees does
not affect the security of the block chain.

We show that this is not the case. Our key insight is that
with only transaction fees, the variance of the block reward is
very high due to the exponentially distributed block arrival
time, and it becomes attractive to fork a wealthy block
to steal the rewards therein. We show that this results
in an equilibrium with undesirable properties for Bitcoins
security and performance, and even non-equilibria in some
circumstances. We also revisit selfish mining and show that
it can be made profitable for a miner with an arbitrarily low
hash power share, and who is arbitrarily poorly connected
within the network. Our results are derived from theoretical
analysis and confirmed by a new Bitcoin mining simulator
that may be of independent interest.

We discuss the troubling implications of our results for
Bitcoins future security and draw lessons for the design of
new cryptocurrencies.

1.

|,Non-data
p456-bhargavan.pdf,|
While modern block ciphers, such as AES, have a block size of
at least 128 bits, there are many 64-bit block ciphers, such as
3DES and Blowfish, that are still widely supported in Internet
security protocols such as TLS, SSH, and IPsec. When used
in CBC mode, these ciphers are known to be susceptible
to collision attacks when they are used to encrypt around
232 blocks of data (the so-called birthday bound). This
threat has traditionally been dismissed as impractical since it
requires some prior knowledge of the plaintext and even then,
it only leaks a few secret bits per gigabyte. Indeed, practical
collision attacks have never been demonstrated against any
mainstream security protocol, leading to the continued use
of 64-bit ciphers on the Internet.

In this work, we demonstrate two concrete attacks that
exploit collisions on short block ciphers. First, we present
an attack on the use of 3DES in HTTPS that can be used
to recover a secret session cookie. Second, we show how a
similar attack on Blowfish can be used to recover HTTP
BasicAuth credentials sent over OpenVPN connections. In
our proof-of-concept demos, the attacker needs to capture
about 785GB of data, which takes between 19-38 hours in
our setting. This complexity is comparable to the recent RC4
attacks on TLS: the only fully implemented attack takes 75
hours. We evaluate the impact of our attacks by measuring
the use of 64-bit block ciphers in real-world protocols. We
discuss mitigations, such as disabling all 64-bit block ciphers,
and report on the response of various software vendors to
our responsible disclosure of these attacks.

1.

|,Data
sec14-paper-checkoway.pdf,|
This paper analyzes the actual cost of attacking TLS im-
plementations that use NISTs Dual EC pseudorandom
number generator, assuming that the attacker generated
the constants used in Dual EC. It has been known for
several years that an attacker generating these constants
and seeing a long enough stretch of Dual EC output bits
can predict all future outputs; but TLS does not natu-
rally provide a long enough stretch of output bits, and the
cost of an attack turns out to depend heavily on choices
made in implementing the RNG and on choices made in
implementing other parts of TLS.

Specifically, this paper investigates OpenSSL-FIPS,
Windows SChannel, and the C/C++ and Java versions of
the RSA BSAFE library. This paper shows that Dual EC
exploitability is fragile, and in particular is stopped by an
outright bug in the certified Dual EC implementation in
OpenSSL. On the other hand, this paper also shows that
Dual EC exploitability benefits from a modification made
to the Dual EC standard in 2007; from several attack op-
timizations introduced here; and from various proposed
TLS extensions, one of which is implemented in BSAFE,
though disabled in the version we obtained and stud-
ied. The papers attacks are implemented; benchmarked;
tested against libraries modified to use new Dual EC con-
stants; and verified to successfully recover TLS plaintext.

|,Data
p1651-fersch.pdf,|
Among the signature schemes most widely deployed in prac-
tice are the DSA (Digital Signature Algorithm) and its ellip-
tic curves variant ECDSA. They are represented in many in-
ternational standards, including IEEE P1363, ANSI X9.62,
and FIPS 186-4. Their popularity stands in stark contrast
to the absence of rigorous security analyses: Previous works
either study modified versions of (EC)DSA or provide a se-
curity analysis of unmodified ECDSA in the generic group
model. Unfortunately, works following the latter approach
assume abstractions of non-algebraic functions over generic
groups for which it remains unclear how they translate to
the security of ECDSA in practice. For instance, it has been
pointed out that prior results in the generic group model ac-
tually establish strong unforgeability of ECDSA, a property
that the scheme de facto does not possess. As, further, no
formal results are known for DSA, understanding the secu-
rity of both schemes remains an open problem.

In this work we propose GenDSA, a signature framework
that subsumes both DSA and ECDSA in unmodified form.
It carefully models the modulo q conversion function of
(EC)DSA as a composition of three independent functions.
The two outer functions mimic algebraic properties in the
functions domain and range, the inner one is modeled as a
bijective random oracle. We rigorously prove results on the
security of GenDSA that indicate that forging signatures in
(EC)DSA is as hard as solving discrete logarithms. Impor-
tantly, our proofs do not assume generic group behavior.

Keywords
Provable security; DSA; ECDSA; GOST; SM2

1.

|,Non-data
p3-gervais.pdf,|
Proof of Work (PoW) powered blockchains currently account for
more than 90% of the total market capitalization of existing digi-
tal cryptocurrencies. Although the security provisions of Bitcoin
have been thoroughly analysed, the security guarantees of variant
(forked) PoW blockchains (which were instantiated with different
parameters) have not received much attention in the literature.

In this paper, we introduce a novel quantitative framework to
analyse the security and performance implications of various con-
sensus and network parameters of PoW blockchains. Based on
our framework, we devise optimal adversarial strategies for double-
spending and selfish mining while taking into account real world
constraints such as network propagation, different block sizes, block
generation intervals, information propagation mechanism, and the
impact of eclipse attacks. Our framework therefore allows us to
capture existing PoW-based deployments as well as PoW blockchain
variants that are instantiated with different parameters, and to objec-
tively compare the tradeoffs between their performance and security
provisions.

1.

|,Non-data
p603-wang.pdf,|
Visual cryptography has been applied to design human com-
putable authentication protocols.
In such a protocol, the
user and the server share a secret key in the form of an im-
age printed on a transparent medium, which the user super-
imposes on server-generated image challenges, and visually
decodes a response code from the image. An example of
such protocols is PassWindow, an award-winning commercial
product. We study the security and usability of segment-
based visual cryptographic authentication protocols (SVAPs),
which include PassWindow as a particular case. In an SVAP,
the images consist of segments and are thus structured. Our
overall findings are negative. We introduce two attacks that
together can break all SVAPs we considered in the paper.
Moreover, our attacks exploit fundamental weaknesses of
SVAPs that appear difficult to fix. We have also evaluated
the usability of different SVAPs and found that the protocol
that offers the best security has the poorest usability.

Keywords
Visual Cryptography; User Authentication; Attack

1.

|,Non-data
p1230-golla.pdf,|
Password vaults are used to store login credentials, usually
encrypted by a master password, relieving the user from
memorizing a large number of complex passwords. To man-
age accounts on multiple devices, vaults are often stored at
an online service, which substantially increases the risk of
leaking the (encrypted) vault. To protect the master pass-
word against guessing attacks, previous work has introduced
cracking-resistant password vaults based on Honey Encryp-
tion. If decryption is attempted with a wrong master pass-
word, they output plausible-looking decoy vaults, thus seem-
ingly disabling offline guessing attacks.

In this work, we propose attacks against cracking-resistant
password vaults that are able to distinguish between real and
decoy vaults with high accuracy and thus circumvent the
offered protection. These attacks are based on differences
in the generated distribution of passwords, which are mea-
sured using KullbackLeibler divergence. Our attack is able
to rank the correct vault into the 1.3 % most likely vaults
(on median), compared to 37.8 % of the best-reported at-
tack in previous work. (Note that smaller ranks are better,
and 50 % is achievable by random guessing.) We demon-
strate that this attack is, to a certain extent, a fundamental
problem with all static Natural Language Encoders (NLE),
where the distribution of decoy vaults is fixed. We propose
the notion of adaptive NLEs and demonstrate that they sub-
stantially limit the effectiveness of such attacks. We give one
example of an adaptive NLE based on Markov models and
show that the attack is only able to rank the decoy vaults
with a median rank of 35.1 %.

Keywords
Password Managers; Natural Language Encoders; Honey
Encryption; Cracking-Resistance

1.

|,Data
p1388-englehardt.pdf,|
We present the largest and most detailed measurement of
online tracking conducted to date, based on a crawl of the
top 1 million websites. We make 15 types of measurements
on each site, including stateful (cookie-based) and stateless
(fingerprinting-based) tracking, the effect of browser privacy
tools, and the exchange of tracking data between different
sites (cookie syncing). Our findings include multiple so-
phisticated fingerprinting techniques never before measured
in the wild.

This measurement is made possible by our open-source
web privacy measurement tool, OpenWPM1, which uses an
automated version of a full-fledged consumer browser.
It
supports parallelism for speed and scale, automatic recovery
from failures of the underlying browser, and comprehensive
browser instrumentation. We demonstrate our platforms
strength in enabling researchers to rapidly detect, quantify,
and characterize emerging online tracking behaviors.

1.

|,Data
sec14-paper-rebert.pdf,|
Randomly mutating well-formed program inputs or sim-
ply fuzzing, is a highly effective and widely used strategy
to find bugs in software. Other than showing fuzzers find
bugs, there has been little systematic effort in understand-
ing the science of how to fuzz properly. In this paper,
we focus on how to mathematically formulate and reason
about one critical aspect in fuzzing: how best to pick seed
files to maximize the total number of bugs found during
a fuzz campaign. We design and evaluate six different
algorithms using over 650 CPU days on Amazon Elas-
tic Compute Cloud (EC2) to provide ground truth data.
Overall, we find 240 bugs in 8 applications and show that
the choice of algorithm can greatly increase the number
of bugs found. We also show that current seed selection
strategies as found in Peach may fare no better than pick-
ing seeds at random. We make our data set and code
publicly available.

1

|,Data
p578-ben-efraim.pdf,|
In the setting of secure multiparty computation, a set of
parties with private inputs wish to compute some function
of their inputs without revealing anything but their output.
Over the last decade, the efficiency of secure two-party com-
putation has advanced in leaps and bounds, with speedups of
some orders of magnitude, making it fast enough to be of use
in practice. In contrast, progress on the case of multiparty
computation (with more than two parties) has been much
slower, with very little work being done. Currently, the only
implemented efficient multiparty protocol has many rounds
of communication (linear in the depth of the circuit being
computed) and thus is not suited for Internet-like settings
where latency is not very low.

In this paper, we construct highly efficient constant-round
protocols for the setting of multiparty computation for semi-
honest adversaries. Our protocols work by constructing a
multiparty garbled circuit, as proposed in BMR (Beaver et
al., STOC 1990). Our first protocol uses oblivious transfer
and constitutes the first concretely-efficient constant-round
multiparty protocol for the case of no honest majority. Our
second protocol uses BGW, and is significantly more efficient
than the FairplayMP protocol (Ben-David et al., CCS 2008)
that also uses BGW.

We ran extensive experimentation comparing our differ-
ent protocols with each other and with a highly-optimized
implementation of semi-honest GMW. Due to our protocol
being constant round, it significantly outperforms GMW in
Internet-like settings. For example, with 13 parties situated
in the Virginia and Ireland Amazon regions and the SHA256

Supported by Israel Science Foundation grant 544/13, the cyber se-
curity research center at Ben Gurion University, and by the Frankel
center for computer science.

Supported by the European Research Council under the ERC con-
solidators grant agreement n. 615172 (HIPS) and by the BIU Center
for Research in Applied Cryptography and Cyber Security in conjunc-
tion with the Israel National Cyber Bureau in the Prime Ministers
Office.

istry and by Israel Science Foundation grant 544/13.

Supported by a grant from the Israeli Science and Technology min-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978347

circuit with 90,000 gates and of depth 4000, the overall run-
ning time of our protocol is 25 seconds compared to 335
seconds for GMW. Furthermore, our online time is under
half a second compared to 330 seconds for GMW.

1.

|,Non-data
p1167-lewi.pdf,|
In the last few years, there has been significant interest in de-
veloping methods to search over encrypted data. In the case
of range queries, a simple solution is to encrypt the contents
of the database using an order-preserving encryption (OPE)
scheme (i.e., an encryption scheme that supports compar-
isons over encrypted values). However, Naveed et al. (CCS
2015) recently showed that OPE-encrypted databases are
extremely vulnerable to inference attacks.

In this work, we consider a related primitive called order-
revealing encryption (ORE), which is a generalization of
OPE that allows for stronger security. We begin by con-
structing a new ORE scheme for small message spaces which
achieves the best-possible notion of security for ORE. Next,
we introduce a domain-extension technique and apply it to
our small-message-space ORE. While our domain-extension
technique does incur a loss in security, the resulting ORE
scheme we obtain is more secure than all existing (state-
less and non-interactive) OPE and ORE schemes which are
practical. All of our constructions rely only on symmetric
primitives. As part of our analysis, we also give a tight lower
bound for OPE and show that no efficient OPE scheme can
satisfy best-possible security if the message space contains
just three messages. Thus, achieving strong notions of secu-
rity for even small message spaces requires moving beyond
OPE.

Finally, we examine the properties of our new ORE scheme
and show how to use it to construct an efficient range query
protocol that is robust against the inference attacks of Naveed
et al. We also give a full implementation of our new ORE
scheme, and show that not only is our scheme more secure
than existing OPE schemes, it is also faster: encrypting a
32-bit integer requires just 55 microseconds, which is more
than 65 times faster than existing OPE schemes.

The full version of this paper [43] with complete proofs is

available at http://eprint.iacr.org/2016/612.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978376

1.

|,Non-data
p1106-sahin.pdf,|
In this paper, we study the Over-The-Top (OTT) bypass
fraud, a recent form of interconnect telecom fraud. In OTT
bypass, a normal phone call is diverted over IP to a voice
chat application on a smartphone, instead of being termi-
nated over the normal telecom infrastructure. This rerout-
ing (or hijack) is performed by an international transit op-
erator in coordination with the OTT service provider, but
without explicit authorization from the caller, callee and
their operators. By doing so, they collect a large share of
the call charge and induce a significant loss of revenue to
the bypassed operators. Moreover, this practice degrades
the quality of service without providing any benefits for the
users.

In this paper, we study the possible techniques to detect
and measure this fraud and evaluate the real impact of OTT
bypass on a small European country. For this, we performed
more than 15,000 test calls during 8 months and conducted
a user study with more than 8,000 users.

In our measurements, we observed up to 83% of calls being
subject to OTT bypass. Additionally, we show that OTT
bypass degrades the quality of service, and sometimes collide
with other fraud schemes, exacerbating the quality issues.
Our user study shows that OTT bypass and its effects are
poorly understood by users.

1.

|,Data
sec14-paper-backes.pdf,|

The latest effective defense against code reuse attacks is
fine-grained, per-process memory randomization. How-
ever, such process randomization prevents code shar-
ing since there is no longer any identical code to share
between processes. Without shared libraries, however,
tremendous memory savings are forfeit. This drawback
may hinder the adoption of fine-grained memory ran-
domization.

We present Oxymoron, a secure fine-grained memory
randomization technique on a per-process level that does
not interfere with code sharing. Executables and libraries
built with Oxymoron feature memory-layout-agnostic
code, which runs on a commodity Linux. Our theoreti-
cal and practical evaluations show that Oxymoron is the
first solution to be secure against just-in-time code reuse
attacks and demonstrate that fine-grained memory ran-
domization is feasible without forfeiting the enormous
memory savings of shared libraries.

1

|,Non-data
sec14-paper-silver.pdf,|

We study the security of popular password managers and
their policies on automatically filling in Web passwords.
We examine browser built-in password managers, mo-
bile password managers, and 3rd party managers. We
observe significant differences in autofill policies among
password managers. Several autofill policies can lead
to disastrous consequences where a remote network at-
tacker can extract multiple passwords from the users
password manager without any interaction with the user.
We experiment with these attacks and with techniques to
enhance the security of password managers. We show
that our enhancements can be adopted by existing man-
agers.

1

|,Data
sec14-paper-florencio.pdf,|. We explore how to manage a portfolio of pass-
words. We review why mandating exclusively strong
passwords with no re-use gives users an impossible task
as portfolio size grows. We find that approaches justified
by loss-minimization alone, and those that ignore impor-
tant attack vectors (e.g., vectors exploiting re-use), are
amenable to analysis but unrealistic. In contrast, we pro-
pose, model and analyze portfolio management under a
realistic attack suite, with an objective function costing
both loss and user effort. Our findings directly challenge
accepted wisdom and conventional advice. We find, for
example, that a portfolio strategy ruling out weak pass-
words or password re-use is sub-optimal. We give an op-
timal solution for how to group accounts for re-use, and
model-based principles for portfolio management.

1

|,Non-data
sec14-paper-chen.pdf,|

The security of smartphone GUI frameworks remains
an important yet under-scrutinized topic.
In this pa-
per, we report that on the Android system (and likely
other OSes), a weaker form of GUI confidentiality can
be breached in the form of UI state (not the pixels) by a
background app without requiring any permissions. Our
finding leads to a class of attacks which we name UI state
inference attack. The underlying problem is that popular
GUI frameworks by design can potentially reveal every
UI state change through a newly-discovered public side
channel  shared memory. In our evaluation, we show
that for 6 out of 7 popular Android apps, the UI state in-
ference accuracies are 8090% for the first candidate UI
states, and over 93% for the top 3 candidates.

Even though the UI state does not reveal the exact pix-
els, we show that it can serve as a powerful building
block to enable more serious attacks. To demonstrate
this, we design and fully implement several new attacks
based on the UI state inference attack, including hijack-
ing the UI state to steal sensitive user input (e.g., login
credentials) and obtain sensitive camera images shot by
the user (e.g., personal check photos for banking apps).
We also discuss non-trivial challenges in eliminating the
identified side channel, and suggest more secure alterna-
tive system designs.

1 |,Non-data
p1402-han.pdf,|
Phishing is a form of online identity theft that deceives un-
aware users into disclosing their confidential information.
While significant effort has been devoted to the mitigation
of phishing attacks, much less is known about the entire
life-cycle of these attacks in the wild, which constitutes,
however, a main step toward devising comprehensive anti-
phishing techniques. In this paper, we present a novel ap-
proach to sandbox live phishing kits that completely protects
the privacy of victims. By using this technique, we perform
a comprehensive real-world assessment of phishing attacks,
their mechanisms, and the behavior of the criminals, their
victims, and the security community involved in the process
 based on data collected over a period of five months.

Our infrastructure allowed us to draw the first compre-
hensive picture of a phishing attack, from the time in which
the attacker installs and tests the phishing pages on a com-
promised host, until the last interaction with real victims
and with security researchers. Our study presents accurate
measurements of the duration and effectiveness of this pop-
ular threat, and discusses many new and interesting aspects
we observed by monitoring hundreds of phishing campaigns.

1.

|,Data
06547113.pdf,|
To instill greater confidence in computations outsourced to
the cloud, clients should be able to verify the correctness
of the results returned. To this end, we introduce Pinoc-
chio, a built system for efficiently verifying general computa-
tions while relying only on cryptographic assumptions. With
Pinocchio, the client creates a public evaluation key to de-
scribe her computation; this setup is proportional to evalu-
ating the computation once. The worker then evaluates the
computation on a particular input and uses the evaluation key
to produce a proof of correctness. The proof is only 288
bytes, regardless of the computation performed or the size of
the inputs and outputs. Anyone can use a public verification
key to check the proof.

|,Non-data
p1255-park.pdf,|
Accelerated Processing Unit (APU) is a heterogeneous mul-
ticore processor that contains general-purpose CPU cores
and a GPU in a single chip.
It also supports Heteroge-
neous System Architecture (HSA) that provides coherent
physically-shared memory between the CPU and the GPU.
In this paper, we present the design and implementation
of a high-performance IPsec gateway using a low-cost com-
modity embedded APU. The HSA supported by the APUs
eliminates the data copy overhead between the CPU and the
GPU, which is unavoidable in the previous discrete GPU ap-
proaches. The gateway is implemented in OpenCL to exploit
the GPU and uses zero-copy packet I/O APIs in DPDK. The
IPsec gateway handles the real-world network traffic where
each packet has a different workload. The proposed packet
scheduling algorithm significantly improves GPU utilization
for such traffic.
It works not only for APUs but also for
discrete GPUs. With three CPU cores and one GPU in
the APU, the IPsec gateway achieves a throughput of 10.36
Gbps with an average latency of 2.79 ms to perform AES-
CBC+HMAC-SHA1 for incoming packets of 1024 bytes.

CCS Concepts
Security and privacy  Network security;

Keywords
IPsec; APU; GPU; Cryptography; Heterogeneous comput-
ing; OpenCL

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:2) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978329

|,Non-data
p1131-roche.pdf,|
Recently there has been much interest in performing search queries
over encrypted data to enable functionality while protecting sensi-
tive data. One particularly efficient mechanism for executing such
queries is order-preserving encryption/encoding (OPE) which re-
sults in ciphertexts that preserve the relative order of the underlying
plaintexts thus allowing range and comparison queries to be per-
formed directly on ciphertexts. Recently, Popa et al. (S&P 2013)
gave the first construction of an ideally-secure OPE scheme and
Kerschbaum (CCS 2015) showed how to achieve the even stronger
notion of frequency-hiding OPE. However, as Naveed et al. (CCS
2015) have recently demonstrated, these constructions remain vul-
nerable to several attacks. Additionally, all previous ideal OPE
schemes (with or without frequency-hiding) either require a large
round complexity of O(log n) rounds for each insertion, or a large
persistent client storage of size O(n), where n is the number of
items in the database. It is thus desirable to achieve a range query
scheme addressing both issues gracefully.

In this paper, we propose an alternative approach to range queries
over encrypted data that is optimized to support insert-heavy work-
loads as are common in big data applications while still maintain-
ing search functionality and achieving stronger security. Specifi-
cally, we propose a new primitive called partial order preserving
encoding (POPE) that achieves ideal OPE security with frequency
hiding and also leaves a sizable fraction of the data pairwise in-
comparable. Using only O(1) persistent and O(n) non-persistent
client storage for 0 <  < 1, our POPE scheme provides extremely
fast batch insertion consisting of a single round, and efficient search
with O(1) amortized cost for up to O(n1) search queries. This
improved security and performance makes our scheme better suited
for todays insert-heavy databases.

|,Data
p1179-zhang.pdf,|
Anonymous authentication allows one to authenticate her-
self without revealing her identity, and becomes an impor-
tant technique for constructing privacy-preserving Internet
connections. Anonymous password authentication is high-
ly desirable as it enables a client to authenticate herself
by a human-memorable password while preserving her pri-
vacy.
In this paper, we introduce a novel approach for
designing anonymous password-authenticated key exchange
(APAKE) protocols using algebraic message authentication
codes (MACs), where an algebraic MAC wrapped by a pass-
word is used by a client for anonymous authentication, and
a server issues algebraic MACs to clients and acts as the
verifier of login protocols. Our APAKE construction is se-
cure provided that the algebraic MAC is strongly existential-
ly unforgeable under random message and chosen verifica-
tion queries attack (suf-rmva), weak pseudorandom and tag-
randomization simulatable, and has simulation-sound ex-
tractable non-interactive zero-knowledge proofs (SE-NIZKs).
To design practical APAKE protocols, we instantiate an
algebraic MAC based on the q-SDH assumption which sat-
isfies all the required properties, and construct credential
presentation algorithms for the MAC which have optimal ef-
ficiency for a randomize-then-prove paradigm. Based on the
algebraic MAC, we instantiate a highly practical APAKE
protocol and denote it by APAKE, which is much more efficien-

The work is supported by National Basic Research Program

of China (No.2013CB338003) and National Natural Science
Foundation of China (No.U1536205, 61572485, 61502527).

Corresponding author

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978354

t than the mechanisms specified by ISO/IEC 20009-4. An
efficient revocation mechanism for APAKE is also proposed.

We integrate APAKE into TLS to present an anonymous
client authentication mode where clients holding passwords
can authenticate themselves to a server anonymously. Our
implementation with 128-bit security shows that the aver-
age connection time of APAKE-based ciphersuite is 2.8 ms.
With APAKE integrated into the OpenSSL library and us-
ing an Apache web server on a 2-core desktop computer,
we could serve 953 ECDHE-ECDSA-AES128-GCM-SHA256 HTTP-
S connections per second for a 10 KB payload. Compared
to ECDSA-signed elliptic curve Diffie-Hellman ciphersuite
with mutual authentication, this means a 0.27 KB increased
handshake size and a 13% reduction in throughput.
1.

|,Non-data
p1715-zolfaghari.pdf,|
CDNBrowsing is a promising approach recently proposed for
censorship circumvention. CDNBrowsing relies on the fact
that blocking content hosted on public CDNs can poten-
tially cause the censors collateral damage due to disrupting
benign content publishers.
In this work, we identify vari-
ous low-cost attacks against CDNBrowsing, demonstrating
that the design of practically unobservable CDNBrowsing
systems is significantly more challenging than what thought
previously. We particularly devise unique website finger-
printing attacks against CDNBrowsing traffic, and discover
various forms of information leakage in HTTPS that can be
used to block the previously proposed CDNBrowsing sys-
tem. Motivated by the attacks, we design and implement
a new CDNBrowsing system called CDNReaper, which de-
feats the discovered attacks.

By design, a CDNBrowsing system can browse only par-
ticular types of webpages due to its proxy-less design. We
perform a comprehensive measurement to classify popular
Internet websites based on their browsability by CDNBrowsing
systems. To further increase the reach of CDNBrowsing, we
devise several mechanisms that enable CDNBrowsing sys-
tems to browse a larger extent of Internet webpages, partic-
ularly partial-CDN webpages.

1.

|,Data
06547133.pdf,|Control Flow Integrity (CFI) provides a strong
protection against modern control-flow hijacking attacks. How-
ever, performance and compatibility issues limit its adoption.
We propose a new practical and realistic protection method
called CCFIR (Compact Control Flow Integrity and Random-
ization), which addresses the main barriers to CFI adoption.
CCFIR collects all legal targets of indirect control-transfer in-
structions, puts them into a dedicated Springboard section in
a random order, and then limits indirect transfers to flow only
to them. Using the Springboard section for targets, CCFIR can
validate a target more simply and faster than traditional CFI,
and provide support for on-site target-randomization as well
as better compatibility. Based on these approaches, CCFIR can
stop control-flow hijacking attacks including ROP and return-
into-libc. Results show that ROP gadgets are all eliminated. We
observe that with the wide deployment of ASLR, Windows/x86
PE executables contain enough information in relocation tables
which CCFIR can use to find all legal instructions and jump
targets reliably, without source code or symbol information.

We evaluate our prototype implementation on common web
browsers and the SPEC CPU2000 suite: CCFIR protects large
applications such as GCC and Firefox completely automati-
cally, and has low performance overhead of about 3.6%/8.6%
(average/max) using SPECint2000. Experiments on real-world
exploits also show that CCFIR-hardened versions of IE6,
Firefox 3.6 and other applications are protected effectively.

I. |,Non-data
ciss06_final.pdf,|

threats propagate randomly,

The Internet today is beset with constant attacks tar-
geting users and infrastructure. One popular method of
detecting these attacks and the infected hosts behind them
is to monitor unused network addresses. Because many
Internet
infection attempts
can be captured by monitoring the unused spaces be-
tween live addresses. Sensors that monitor these unused
address spaces are called darknets, network telescopes, or
blackholes. They capture important information about a
diverse range of threats such as Internet worms, denial of
services attacks, and botnets. In this paper, we describe
and analyze the important measurement issues associated
with deploying darknets, evaluating the placement and
service configuration of darknets, and analyzing the data
collected by darknets. To support the discussion, we lever-
age 4 years of experience operating the Internet Motion
Sensor (IMS), a network of distributed darknet sensors
monitoring 60 distinct address blocks in 19 organizations
over 3 continents.

I. |,Non-data
p678-dorre.pdf,|
Pseudo-random number generators (PRNGs) are a critical
infrastructure for cryptography and security of many com-
puter applications. At the same time, PRNGs are surpris-
ingly difficult to design, implement, and debug. This paper
presents the first static analysis technique specifically for
quality assurance of cryptographic PRNG implementations.
The analysis targets a particular kind of implementation
defect, the entropy loss. Entropy loss occurs when the en-
tropy contained in the PRNG seed is not utilized to the
full extent for generating the pseudo-random output stream.
The Debian OpenSSL disaster, probably the most prominent
PRNG-related security incident, was one but not the only
manifestation of such a defect.

Together with the static analysis technique, we present its
implementation, a tool named Entroposcope. The tool of-
fers a high degree of automation and practicality. We have
applied the tool to five real-world PRNGs of different de-
signs and show that it effectively detects both known and
previously unknown instances of entropy loss.

Keywords
Pseudo-Random Number Generator; PRNG; entropy loss;
information flow; OpenSSL; static analysis; bounded model
checking

1.

|,Non-data
p1317-kiayias.pdf,|
In this work, we significantly improve the efficiency of non-
malleable codes in the split state model, by constructing a
code with codeword length (roughly)  s +9k, where  s  is the
length of the message, and k is the security parameter. This
is a substantial improvement over previous constructions,
both asymptotically and concretely.

Our construction relies on a new primitive which we define
and study, called (cid:96)-more extractable hash functions. This
notion, which may be of independent interest, is strictly
stronger than the previous notion of extractable hash by
Goldwasser et al. (Eprint 11) and Bitansky et al. (ITCS
12, Eprint 14), yet we can instantiate it under the same
assumption used for the previous extractable hash function
(a variant of the Knowledge of Exponent Assumption).

Keywords
Non-malleable codes, hash functions, split-state model

1.

|,Non-data
06547110.pdf,|Due to the prevalence of control-flow hijacking at-
tacks, a wide variety of defense methods to protect both user
space and kernel space code have been developed in the past years.
A few examples that have received widespread adoption include
stack canaries, non-executable memory, and Address Space Layout
Randomization (ASLR). When implemented correctly (i.e., a given
system fully supports these protection methods and no information
leak exists), the attack surface is significantly reduced and typical
exploitation strategies are severely thwarted. All modern desktop
and server operating systems support these techniques and ASLR
has also been added to different mobile operating systems recently.
In this paper, we study the limitations of kernel space ASLR
against a local attacker with restricted privileges. We show that
an adversary can implement a generic side channel attack against
the memory management system to deduce information about the
privileged address space layout. Our approach is based on the
intrinsic property that the different caches are shared resources
on computer systems. We introduce three implementations of our
methodology and show that our attacks are feasible on four
different x86-based CPUs (both 32- and 64-bit architectures)
and also applicable to virtual machines. As a result, we can
successfully circumvent kernel space ASLR on current operating
systems. Furthermore, we also discuss mitigation strategies against
our attacks, and propose and implement a defense solution with
negligible performance overhead.

Keywords-Address Space Layout Randomization; Timing At-

tacks; Kernel Vulnerabilities; Exploit Mitigation

I. |,Non-data
sec14-paper-stock.pdf,|

The current generation of client-side Cross-Site Scripting
filters rely on string comparison to detect request values
that are reflected in the corresponding responses HTML.
This coarse approximation of occurring data flows is in-
capable of reliably stopping attacks which leverage non-
trivial injection contexts. To demonstrate this, we con-
duct a thorough analysis of the current state-of-the-art in
browser-based XSS filtering and uncover a set of concep-
tual shortcomings, that allow efficient creation of filter
evasions, especially in the case of DOM-based XSS. To
validate our findings, we report on practical experiments
using a set of 1,602 real-world vulnerabilities, achiev-
ing a rate of 73% successful filter bypasses. Motivated
by our findings, we propose an alternative filter design
for DOM-based XSS, that utilizes runtime taint tracking
and taint-aware parsers to stop the parsing of attacker-
controlled syntactic content. To examine the efficiency
and feasibility of our approach, we present a practi-
cal implementation based on the open source browser
Chromium. Our proposed approach has a low false pos-
itive rate and robustly protects against DOM-based XSS
exploits.

|,Data
p1568-hao.pdf,|
Miscreants register thousands of new domains every day to launch
Internet-scale attacks, such as spam, phishing, and drive-by down-
loads. Quickly and accurately determining a domains reputation
(association with malicious activity) provides a powerful tool for mit-
igating threats and protecting users. Yet, existing domain reputation
systems work by observing domain use (e.g., lookup patterns, content
hosted)often too late to prevent miscreants from reaping benefits of
the attacks that they launch.

As a complement to these systems, we explore the extent to which
features evident at domain registration indicate a domains subsequent
use for malicious activity. We develop PREDATOR, an approach that
uses only time-of-registration features to establish domain reputation.
We base its design on the intuition that miscreants need to obtain
many domains to ensure profitability and attack agility, leading to
abnormal registration behaviors (e.g., burst registrations, textually
similar names). We evaluate PREDATOR using registration logs of
second-level .com and .net domains over five months. PREDATOR
achieves a 70% detection rate with a false positive rate of 0.35%, thus
making it an effectiveand earlyfirst line of defense against the
misuse of DNS domains. It predicts malicious domains when they
are registered, which is typically days or weeks earlier than existing
DNS blacklists.

CCS Concepts
Security and privacy  Intrusion/anomaly detection and mal-
ware mitigation; Networks  Network domains;

Keywords
Domain Registration; Reputation System; Early Detection.

1.

|,Data
Shokri-NDSS2015.pdf,|Location check-ins contain both geographical and
semantic information about the visited venues, in the form of
tags (e.g., restaurant). Such data might reveal some personal
information about users beyond what they actually want to
disclose, hence their privacy is threatened. In this paper, we study
users motivations behind location check-ins, and we quantify the
effect of a privacy-preserving technique (i.e., generalization) on
the perceived utility of check-ins. By means of a targeted user-
study on Foursquare (N = 77), we show that the motivation behind
Foursquare check-ins is a mediator of the loss of utility caused
by generalization. Using these findings, we propose a machine-
learning method for determining the motivation behind each
check-in, and we design a motivation-based predictive model for
utility. Our results show that the model accurately predicts the
loss of utility caused by semantic and geographical generalization;
this model enables the design of utility-aware, privacy-enhancing
mechanisms in location-based social networks.

I.

|,Data
p368-gruss.pdf,|
Modern operating systems use hardware support to protect
against control-flow hijacking attacks such as code-injection
attacks. Typically, write access to executable pages is pre-
vented and kernel mode execution is restricted to kernel code
pages only. However, current CPUs provide no protection
against code-reuse attacks like ROP. ASLR is used to pre-
vent these attacks by making all addresses unpredictable for
an attacker. Hence, the kernel security relies fundamentally
on preventing access to address information.

We introduce Prefetch Side-Channel Attacks, a new class
of generic attacks exploiting major weaknesses in prefetch
instructions. This allows unprivileged attackers to obtain
address information and thus compromise the entire system
by defeating SMAP, SMEP, and kernel ASLR. Prefetch can
fetch inaccessible privileged memory into various caches on
Intel x86. It also leaks the translation-level for virtual ad-
dresses on both Intel x86 and ARMv8-A. We build three at-
tacks exploiting these properties. Our first attack retrieves
an exact image of the full paging hierarchy of a process,
defeating both user space and kernel space ASLR. Our sec-
ond attack resolves virtual to physical addresses to bypass
SMAP on 64-bit Linux systems, enabling ret2dir attacks.
We demonstrate this from unprivileged user programs on
Linux and inside Amazon EC2 virtual machines. Finally,
we demonstrate how to defeat kernel ASLR on Windows 10,
enabling ROP attacks on kernel and driver binary code. We
propose a new form of strong kernel isolation to protect com-
modity systems incuring an overhead of only 0.065.09%.

CCS Concepts
Security and privacy  Side-channel analysis and
countermeasures; Systems security; Operating sys-
tems security;

Keywords
ASLR; Kernel Vulnerabilities; Timing Attacks

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978356

1.

|,Non-data
sec14-paper-pattuck.pdf,|

In a typical infrastructure-as-a-service cloud setting,
different clients harness the cloud providers services by
executing virtual machines (VM). However, recent studies
have shown that the cryptographic keys, the most crucial
component in many of our daily used cryptographic pro-
tocols (e.g., SSL/TLS), can be extracted using cross-VM
side-channel attacks. To defeat such a threat, this paper
introduces HERMES, a new system that aims to protect
the cryptographic keys in the cloud against any kind of
cross-VM side-channel attacks by simply partitioning the
cryptographic keys into random shares, and storing each
share in a different VM. Moreover, it also periodically
re-shares the cryptographic keys, thereby invalidating the
potentially extracted partial ones. We have implemented
HERMES as a library extension that is transparent to the
application software, and performed deep case studies
with a web and a mail server on Amazon EC2 cloud. Our
experimental results show that the runtime overhead of
the proposed system can be as low as 1%.

1

|,Non-data
sec14-paper-fredrikson-privacy.pdf,|

We initiate the study of privacy in pharmacogenetics,
wherein machine learning models are used to guide med-
ical treatments based on a patients genotype and back-
ground. Performing an in-depth case study on privacy
in personalized warfarin dosing, we show that suggested
models carry privacy risks, in particular because attack-
ers can perform what we call model inversion: an at-
tacker, given the model and some demographic infor-
mation about a patient, can predict the patients genetic
markers.

As differential privacy (DP) is an oft-proposed solu-
tion for medical settings such as this, we evaluate its ef-
fectiveness for building private versions of pharmacoge-
netic models. We show that DP mechanisms prevent our
model inversion attacks when the privacy budget is care-
fully selected. We go on to analyze the impact on utility
by performing simulated clinical trials with DP dosing
models. We find that for privacy budgets effective at pre-
venting attacks, patients would be exposed to increased
risk of stroke, bleeding events, and mortality. We con-
clude that current DP mechanisms do not simultaneously
improve genomic privacy while retaining desirable clin-
ical efficacy, highlighting the need for new mechanisms
that should be evaluated in situ using the general method-
ology introduced by our work.

1

|,Non-data
06547119.pdf,|Ridge regression is an algorithm that takes as
input a large number of data points and finds the best-fit
linear curve through these points. The algorithm is a building
block for many machine-learning operations. We present a
system for privacy-preserving ridge regression. The system
outputs the best-fit curve in the clear, but exposes no other
information about the input data. Our approach combines
both homomorphic encryption and Yao garbled circuits, where
each is used in a different part of the algorithm to obtain
the best performance. We implement the complete system
and experiment with it on real data-sets, and show that it
significantly outperforms pure implementations based only on
homomorphic encryption or Yao circuits.

I. |,Data
p142-dziembowski.pdf,|
Security against hardware trojans is currently becoming an
essential ingredient to ensure trust in information systems.
A variety of solutions have been introduced to reach this
goal, ranging from reactive (i.e., detection-based) to pre-
ventive (i.e., trying to make the insertion of a trojan more
difficult for the adversary). In this paper, we show how test-
ing (which is a typical detection tool) can be used to state
concrete security guarantees for preventive approaches to
trojan-resilience. For this purpose, we build on and formal-
ize two important previous works which introduced input
scrambling and split manufacturing as countermeasures
to hardware trojans. Using these ingredients, we present
a generic compiler that can transform any circuit into a
trojan-resilient one, for which we can state quantitative se-
curity guarantees on the number of correct executions of the
circuit thanks to a new tool denoted as testing amplifica-
tion. Compared to previous works, our threat model covers
an extended range of hardware trojans while we stick with
the goal of minimizing the number of honest elements in
our transformed circuits. Since transformed circuits essen-
tially correspond to redundant multiparty computations of
the target functionality, they also allow reasonably efficient
implementations, which can be further optimized if special-
ized to certain cryptographic primitives and security goals.

1.

|,Non-data
sec14-paper-zimmeck.pdf,|
Privacy policies on websites are based on the notice-
and-choice principle. They notify Web users of their
privacy choices. However, many users do not read pri-
vacy policies or have difficulties understanding them.
In order to increase privacy transparency we propose
Priveea software architecture for analyzing essential
policy terms based on crowdsourcing and automatic clas-
sification techniques. We implement Privee in a proof of
concept browser extension that retrieves policy analysis
results from an online privacy policy repository or, if
no such results are available, performs automatic clas-
sifications. While our classifiers achieve an overall F-1
score of 90%, our experimental results suggest that clas-
sifier performance is inherently limited as it correlates
to the same variable to which human interpretations
correlatethe ambiguity of natural language. This find-
ing might be interpreted to call the notice-and-choice
principle into question altogether. However, as our re-
sults further suggest that policy ambiguity decreases over
time, we believe that the principle is workable. Conse-
quently, we see Privee as a promising avenue for facilitat-
ing the notice-and-choice principle by accurately notify-
ing Web users of privacy practices and increasing privacy
transparency on the Web.

1 |,Data
06547111.pdf,|Privacy has become an issue of paramount im-
portance for many users. As a result, encryption tools such as
TrueCrypt, OS-based full-disk encryption such as FileVault,
and privacy modes in all modern browsers have become
popular. However, although such tools are useful, they are not
perfect. For example, prior work has shown that browsers
still leave many traces of user information on disk even if
they are started in private browsing mode. In addition, disk
encryption alone is not sufficient, as key disclosure through
coercion remains possible. Clearly,
it would be useful and
highly desirable to have OS-level support that provides strong
privacy guarantees for any application  not only browsers.

In this paper, we present the design and implementation
of PRIVEXEC, the first operating system service for private
execution. PRIVEXEC provides strong, general guarantees of
private execution, allowing any application to execute in a mode
where storage writes, either to the filesystem or to swap, will not
be recoverable by others during or after execution. PRIVEXEC
does not require explicit application support, recompilation, or
any other preconditions. We have implemented a prototype of
PRIVEXEC by extending the Linux kernel that is performant,
practical, and that secures sensitive data against disclosure.

Keywords-privacy; operating systems;

I. |,Data
p1280-kontaxis.pdf,|
Unencrypted and unauthenticated protocols present secu-
rity and privacy risks to end-to-end communications. At the
same time we observe that only 30% of popular web servers
offer HTTPS. Even when services support it, implementa-
tion vulnerabilities threaten their security.
In this paper
we propose an architecture called Topology-aware Network
Tunnels (TNT) which minimizes insecure network paths to
Internet services without their participation. TNT is not
a substitute for TLS. We determine that popular web des-
tinations are collocated in a small set of networks with 10
autonomous systems hosting 66% of traffic. At the same
time cloud providers own these networks or are very close
to them. Therefore clients can strategically establish secure
tunnels to these providers and route their traffic through
them. As a result adversaries not able to compromise the
web service or its hosting provider are presented with en-
crypted and authenticated traffic instead of todays plain
text. The strategic placement of network tunnels, gathering
of network intelligence and routing decisions of the TNT ar-
chitecture are not found in VPN services, network proxies
or Tor. Existing overlay routing systems such as RON and
one-hop source routing cannot substitute TNT. We imple-
ment our proposal as a routing software suite and evaluate
it extensively using diverse cloud and ISP networks. We
eliminate plain-text traffic to the Internet for 20% of web
servers, reduce it to 1 network hop for an additional 20%
and minimize it for the rest. We preserve the original net-
work latency and page load time. TNT is practical and can
be deployed by clients today.

1.

|,Data
sec15-paper-de-ruiter.pdf,|

We describe a largely automated and systematic analysis
of TLS implementations by what we call protocol state
fuzzing: we use state machine learning to infer state ma-
chines from protocol implementations, using only black-
box testing, and then inspect the inferred state machines
to look for spurious behaviour which might be an indica-
tion of flaws in the program logic. For detecting the pres-
ence of spurious behaviour the approach is almost fully
automatic: we automatically obtain state machines and
any spurious behaviour is then trivial to see. Detecting
whether the spurious behaviour introduces exploitable
security weaknesses does require manual investigation.
Still, we take the point of view that any spurious func-
tionality in a security protocol implementation is danger-
ous and should be removed.

We analysed both server- and client-side implemen-
tations with a test harness that supports several key ex-
change algorithms and the option of client certificate au-
thentication. We show that this approach can catch an
interesting class of implementation flaws that is appar-
ently common in security protocol implementations: in
three of the TLS implementations analysed new security
flaws were found (in GnuTLS, the Java Secure Socket
Extension, and OpenSSL). This shows that protocol state
fuzzing is a useful technique to systematically analyse
security protocol implementations. As our analysis of
different TLS implementations resulted in different and
unique state machines for each one, the technique can
also be used for fingerprinting TLS implementations.

1 |,Non-data
p242-tian.pdf,|
Defenders of enterprise networks have a critical need to
quickly identify the root causes of malware and data leak-
age.
Increasingly, USB storage devices are the media of
choice for data exfiltration, malware propagation, and even
cyber-warfare. We observe that a critical aspect of ex-
plaining and preventing such attacks is understanding the
provenance of data (i.e., the lineage of data from its creation
to current state) on USB devices as a means of ensuring
their safe usage. Unfortunately, provenance tracking is
not offered by even sophisticated modern devices. This
work presents ProvUSB, an architecture for fine-grained
provenance collection and tracking on smart USB devices.
ProvUSB maintains data provenance by recording reads
and writes at the block layer and reliably identifying hosts
editing those blocks through attestation over the USB chan-
nel. Our evaluation finds that ProvUSB imposes a one-time
850 ms overhead during USB enumeration, but approaches
nearly-bare-metal runtime performance (90% of through-
put) on larger files during normal execution, and less than
0.1% storage overhead for provenance in real-world work-
loads. ProvUSB thus provides essential new techniques in
the defense of computer systems and USB storage devices.

1.

|,Non-data
06547116.pdf,|In recent years, PUF-based schemes have not
only been suggested for the basic security tasks of tamper
sensitive key storage or system identification, but also for
more complex cryptographic protocols like oblivious transfer
(OT), bit commitment (BC), or key exchange (KE). In these
works, so-called Strong PUFs are regarded as a new, fun-
damental cryptographic primitive of their own, comparable to
the bounded storage model, quantum cryptography, or noise-
based cryptography. This paper continues this line of research,
investigating the correct adversarial attack model and the
actual security of such protocols.

In its first part, we define and compare different attack
models. They reach from a clean, first setting termed the
stand-alone, good PUF model to stronger scenarios like the
bad PUF model and the PUF re-use model. We argue why
these attack models are realistic, and that existing protocols
would be faced with them if used in practice. In the second part,
we execute exemplary security analyses of existing schemes in
the new attack models. The evaluated protocols include recent
schemes from Brzuska et al. published at Crypto 2011 [1]
and from Ostrovsky et al. [18]. While a number of protocols
are certainly secure in their own, original attack models, the
security of none of the considered protocols for OT, BC, or KE
is maintained in all of the new, realistic scenarios.

One consequence of our work is that the design of advanced
cryptographic PUF protocols needs to be strongly reconsidered.
Furthermore, it suggests that Strong PUFs require additional
hardware properties in order to be broadly usable in such
protocols: Firstly, they should ideally be erasable, meaning
that single PUF-responses can be erased without affecting other
responses. If the area efficient implementation of this feature
turns out to be difficult, new forms of Controlled PUFs [8] (such
as Logically Erasable and Logically Reconfigurable PUFs [13])
may suffice in certain applications. Secondly, PUFs should be
certifiable, meaning that one can verify that the PUF has been
produced faithfully and has not been manipulated in any way
afterwards. The combined implementation of these features
represents a pressing and challenging problem, which we pose
to the PUF hardware community in this work.

Keywords-(Strong) Physical Unclonable Functions; (Strong)
PUFs; Attack Models; Oblivious Transfer; Bit Commitment;
Key Exchange; Erasable PUFs; Certifiable PUFs

I. |,Non-data
p356-backes.pdf,|
Third-party libraries on Android have been shown to be se-
curity and privacy hazards by adding security vulnerabilities
to their host apps or by misusing inherited access rights.
Correctly attributing improper app behavior either to app
or library developer code or isolating library code from their
host apps would be highly desirable to mitigate these prob-
lems, but is impeded by the absence of a third-party library
detection that is effective and reliable in spite of obfuscated
code. This paper proposes a library detection technique that
is resilient against common code obfuscations and that is
capable of pinpointing the exact library version used in apps.
Libraries are detected with profiles from a comprehensive
library database that we generated from the original library
SDKs. We apply our technique to the top apps on Google
Play and their complete histories to conduct a longitudinal
study of library usage and evolution in apps. Our results
particularly show that app developers only slowly adapt new
library versions, exposing their end-users to large windows
of vulnerability. For instance, we discovered that two long-
known security vulnerabilities in popular libs are still present
in the current top apps. Moreover, we find that misuse of
cryptographic APIs in advertising libs, which increases the
host apps attack surface, affects 296 top apps with a cu-
mulative install base of 3.7bn devices according to Play. To
the best of our knowledge, our work is first to quantify the
security impact of third-party libs on the Android ecosystem.

1.

|,Data
He09a.pdf,|

Persistently saturated links are abnormal conditions that indicate bottlenecks in Internet traffic. Network operators are interested
in detecting such links for troubleshooting, to improve capacity planning and traffic estimation, and to detect denial-of-service
attacks. Currently bottleneck links can be detected either locally, through SNMP information, or remotely, through active probing
or passive flow-based analysis. However, local SNMP information may not be available due to administrative restrictions, and
existing remote approaches are not used systematically because of their network or computation overhead. This paper proposes a
new approach to remotely detect the presence of bottleneck links using spectral and statistical analysis of traffic. Our approach is
passive, operates on aggregate traffic without flow separation, and supports remote detection of bottlenecks, addressing some of the
major limitations of existing approaches. Our technique assumes that traffic through the bottleneck is dominated by packets with
a common size (typically the maximum transfer unit, for reasons discussed in Section 5.1). With this assumption, we observe that
bottlenecks imprint periodicities on packet transmissions based on the packet size and link bandwidth. Such periodicities manifest
themselves as strong frequencies in the spectral representation of the aggregate traffic observed at a downstream monitoring point.
We propose a detection algorithm based on rigorous statistical methods to detect the presence of bottleneck links by examining
strong frequencies in aggregate traffic. We use data from live Internet traces to evaluate the performance of our algorithm under
various network conditions. Results show that with proper parameters our algorithm can provide excellent accuracy (up to 95%)
even if the traffic through the bottleneck link accounts for less than 10% of the aggregate traffic.

Key words: Spectral Analysis, Bottleneck Detection, Traffic Analysis

1. |,Non-data
sec14-paper-kemerlis.pdf,|

Return-to-user (ret2usr) attacks redirect corrupted kernel
pointers to data residing in user space. In response, sev-
eral kernel-hardening approaches have been proposed to
enforce a more strict address space separation, by pre-
venting arbitrary control flow transfers and dereferences
from kernel to user space. Intel and ARM also recently
introduced hardware support for this purpose in the form
of the SMEP, SMAP, and PXN processor features. Un-
fortunately, although mechanisms like the above prevent
the explicit sharing of the virtual address space among
user processes and the kernel, conditions of implicit shar-
ing still exist due to fundamental design choices that
trade stronger isolation for performance.

In this work, we demonstrate how implicit page frame
sharing can be leveraged for the complete circumven-
tion of software and hardware kernel isolation protec-
tions. We introduce a new kernel exploitation technique,
called return-to-direct-mapped memory (ret2dir), which
bypasses all existing ret2usr defenses, namely SMEP,
SMAP, PXN, KERNEXEC, UDEREF, and kGuard. We
also discuss techniques for constructing reliable ret2dir
exploits against x86, x86-64, AArch32, and AArch64
Linux targets. Finally, to defend against ret2dir attacks,
we present the design and implementation of an exclu-
sive page frame ownership scheme for the Linux ker-
nel that prevents the implicit sharing of physical memory
pages with minimal runtime overhead.

1 |,Non-data
p858-zhang.pdf,|
Cache side-channel attacks have been extensively studied
on x86 architectures, but much less so on ARM processors.
The technical challenges to conduct side-channel attacks on
ARM, presumably, stem from the poorly documented ARM
cache implementations, such as cache coherence protocols
and cache flush operations, and also the lack of understand-
ing of how different cache implementations will affect side-
channel attacks. This paper presents a systematic explo-
ration of vectors for Flush-Reload attacks on ARM pro-
cessors. Flush-Reload attacks are among the most well-
known cache side-channel attacks on x86. It has been shown
in previous work that they are capable of exfiltrating sensi-
tive information with high fidelity. We demonstrate in this
work a novel construction of flush-reload side channels on
last-level caches of ARM processors, which, particularly, ex-
ploits return-oriented programming techniques to reload in-
structions. We also demonstrate several attacks on Android
OS (e.g., detecting hardware events and tracing software ex-
ecution paths) to highlight the implications of such attacks
for Android devices.

Keywords
Cache side channels; flush-reload

1.

|,Non-data
imc032s-detalA.pdf,|

Middleboxes such as firewalls, NAT, proxies, or Deep Pack-
et Inspection play an increasingly important role in various
types of IP networks, including enterprise and cellular net-
works. Recent studies have shed the light on their impact on
real traffic and the complexity of managing them. Network
operators and researchers have few tools to understand the
impact of those boxes on any path. In this paper, we pro-
pose tracebox, an extension to the widely used traceroute
tool, that is capable of detecting various types of middle-
box interference over almost any path. tracebox sends IP
packets containing TCP segments with different TTL values
and analyses the packet encapsulated in the returned ICMP
messages. Further, as recent routers quote, in the ICMP
message, the entire IP packet that they received, tracebox
is able to detect any modification performed by upstream
middleboxes. In addition, tracebox can often pinpoint the
network hop where the middlebox interference occurs. We
evaluate tracebox with measurements performed on Plan-
etLab nodes. Our analysis reveals various types of mid-
dleboxes that were not expected on such an experimental
testbed supposed to be connected to the Internet without
any restriction.

Categories and Subject Descriptors

C.2.3 [Network Operations]: Network Monitoring

Keywords

Network Discovery, Middleboxes, tracebox

1.

|,Data
sec14-paper-meyer.pdf,|
As a countermeasure against the famous Bleichenbacher
attack on RSA based ciphersuites, all TLS RFCs starting
from RFC 2246 (TLS 1.0) propose to treat incorrectly
formatted messages in a manner indistinguishable from
correctly formatted RSA blocks.

In this paper we show that this objective has not been
achieved yet (cf. Table 1): We present four new Blei-
chenbacher side channels, and three successful Bleichen-
bacher attacks against the Java Secure Socket Extension
(JSSE) SSL/TLS implementation and against hardware
security appliances using the Cavium NITROX SSL ac-
celerator chip. Three of these side channels are timing-
based, and two of them provide the first timing-based
Bleichenbacher attacks on SSL/TLS described in the lit-
erature. Our measurements confirmed that all these side
channels are observable over a switched network, with
timing differences between 1 and 23 microseconds. We
were able to successfully recover the PreMasterSecret
using three of the four side channels in a realistic mea-
surement setup.

1

|,Non-data
sec14-paper-carlini.pdf,|

Return Oriented Programming (ROP) has become the ex-
ploitation technique of choice for modern memory-safety
vulnerability attacks. Recently, there have been multi-
ple attempts at defenses to prevent ROP attacks. In this
paper, we introduce three new attack methods that break
many existing ROP defenses. Then we show how to break
kBouncer and ROPecker, two recent low-overhead de-
fenses that can be applied to legacy software on existing
hardware. We examine several recent ROP attacks seen in
the wild and demonstrate that our techniques successfully
cloak them so they are not detected by these defenses. Our
attacks apply to many CFI-based defenses which we argue
are weaker than previously thought. Future defenses will
need to take our attacks into account.

1

|,Non-data
p229-sheff.pdf,|
Modern applications often operate on data in multiple administra-
tive domains. In this federated setting, participants may not fully
trust each other. These distributed applications use transactions as
a core mechanism for ensuring reliability and consistency with per-
sistent data. However, the coordination mechanisms needed for
transactions can both leak confidential information and allow unau-
thorized influence.

By implementing a simple attack, we show these side channels
can be exploited. However, our focus is on preventing such attacks.
We explore secure scheduling of atomic, serializable transactions
in a federated setting. While we prove that no protocol can guaran-
tee security and liveness in all settings, we establish conditions for
sets of transactions that can safely complete under secure schedul-
ing. Based on these conditions, we introduce staged commit, a
secure scheduling protocol for federated transactions. This proto-
col avoids insecure information channels by dividing transactions
into distinct stages. We implement a compiler that statically checks
code to ensure it meets our conditions, and a system that schedules
these transactions using the staged commit protocol. Experiments
on this implementation demonstrate that realistic federated transac-
tions can be scheduled securely, atomically, and efficiently.

1.

|,Non-data
p1553-jansen.pdf,|
Tor is a popular network for anonymous communication.
The usage and operation of Tor is not well-understood, how-
ever, because its privacy goals make common measurement
approaches ineffective or risky. We present PrivCount, a sys-
tem for measuring the Tor network designed with user pri-
vacy as a primary goal. PrivCount securely aggregates mea-
surements across Tor relays and over time to produce differ-
entially private outputs. PrivCount improves on prior ap-
proaches by enabling flexible exploration of many diverse
kinds of Tor measurements while maintaining accuracy and
privacy for each. We use PrivCount to perform a measure-
ment study of Tor of sufficient breadth and depth to inform
accurate models of Tor users and traffic. Our results indi-
cate that Tor has 710,000 users connected but only 550,000
active at a given time, that Web traffic now constitutes 91%
of data bytes on Tor, and that the strictness of relays connec-
tion policies significantly affects the type of application data
they forward.

1.

|,Data
p731-ambrosin.pdf,|
Large numbers of smart connected devices, also named as the In-
ternet of Things (IoT), are permeating our environments (homes,
factories, cars, and also our bodywith wearable devices) to collect
data and act on the insight derived. Ensuring software integrity (in-
cluding OS, apps, and configurations) on such smart devices is then
essential to guarantee both privacy and safety. A key mechanism to
protect the software integrity of these devices is remote attestation:
A process that allows a remote verifier to validate the integrity of
the software of a device. This process usually makes use of a signed
hash value of the actual devices software, generated by dedicated
hardware. While individual device attestation is a well-established
technique, to date integrity verification of a very large number of
devices remains an open problem, due to scalability issues.

In this paper, we present SANA, the first secure and scalable pro-
tocol for efficient attestation of large sets of devices that works under
realistic assumptions. SANA relies on a novel signature scheme to
allow anyone to publicly verify a collective attestation in constant
time and space, for virtually an unlimited number of devices. We
substantially improve existing swarm attestation schemes [5] by sup-
porting a realistic trust model where: (1) only the targeted devices
are required to implement attestation; (2) compromising any device
does not harm others; and (3) all aggregators can be untrusted. We
implemented SANA and demonstrated its efficiency on tiny sensor
devices. Furthermore, we simulated SANA at large scale, to assess
its scalability. Our results show that SANA can provide efficient
attestation of networks of 1, 000, 000 devices, in only 2.5 seconds.

1.

|,Non-data
p704-deshotels.pdf,|
Recent literature on iOS security has focused on the ma-
licious potential of third-party applications, demonstrating
how developers can bypass application vetting and code-
level protections. In addition to these protections, iOS uses
a generic sandbox profile called container to confine ma-
licious or exploited third-party applications. In this paper,
we present the first systematic analysis of the iOS container
sandbox profile. We propose the SandScout framework to
extract, decompile, formally model, and analyze iOS sand-
box profiles as logic-based programs. We use our Prolog-
based queries to evaluate file-based security properties of the
container sandbox profile for iOS 9.0.2 and discover seven
classes of exploitable vulnerabilities. These attacks affect
non-jailbroken devices running later versions of iOS. We are
working with Apple to resolve these attacks, and we expect
that SandScout will play a significant role in the develop-
ment of sandbox profiles for future versions of iOS.

1.

|,Non-data
p480-feng.pdf,|
Because of rampant security breaches in IoT devices, searching
vulnerabilities in massive IoT ecosystems is more crucial than ever.
Recent studies have demonstrated that control-flow graph (CFG)
based bug search techniques can be effective and accurate in IoT
devices across different architectures. However, these CFG-based
bug search approaches are far from being scalable to handle an
enormous amount of IoT devices in the wild, due to their expen-
sive graph matching overhead. Inspired by rich experience in im-
age and video search, we propose a new bug search scheme which
addresses the scalability challenge in existing cross-platform bug
search techniques and further improves search accuracy. Unlike
existing techniques that directly conduct searches based upon raw
features (CFGs) from the binary code, we convert the CFGs into
high-level numeric feature vectors. Compared with the CFG fea-
ture, high-level numeric feature vectors are more robust to code
variation across different architectures, and can easily achieve real-
time search by using state-of-the-art hashing techniques.

We have implemented a bug search engine, Genius, and com-
pared it with state-of-art bug search approaches. Experimental re-
sults show that Genius outperforms baseline approaches for vari-
ous query loads in terms of speed and accuracy. We also evaluated
Genius on a real-world dataset of 33,045 devices which was col-
lected from public sources and our system. The experiment showed
that Genius can finish a search within 1 second on average when
performed over 8,126 firmware images of 420,558,702 functions.
By only looking at the top 50 candidates in the search result, we
found 38 potentially vulnerable firmware images across 5 vendors,
and confirmed 23 of them by our manual analysis. We also found
that it took only 0.1 seconds on average to finish searching for all
154 vulnerabilities in two latest commercial firmware images from
D-LINK. 103 of them are potentially vulnerable in these images,
and 16 of them were confirmed.

Keywords
Firmware Security; Machine Learning; Graph Encoding

ACM acknowledges that this contribution was authored or co-authored by an em-
ployee, or contractor of the national government. As such, the Government retains
a nonexclusive, royalty-free right to publish or reproduce this article, or to allow oth-
ers to do so, for Government purposes only. Permission to make digital or hard copies
for personal or classroom use is granted. Copies must bear this notice and the full ci-
tation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. To copy otherwise, distribute, republish, or post, requires prior
specific permission and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978370

1.

|,Data
sec14-paper-varadarajan.pdf,|

Public infrastructure-as-a-service clouds, such as Ama-
zon EC2 and Microsoft Azure allow arbitrary clients
to run virtual machines (VMs) on shared physical in-
frastructure.
This practice of multi-tenancy brings
economies of scale, but also introduces the threat of
malicious VMs abusing the scheduling of shared re-
sources. Recent works have shown how to mount cross-
VM side-channel attacks to steal cryptographic secrets.
The straightforward solution is hard isolation that dedi-
cates hardware to each VM. However, this comes at the
cost of reduced efficiency.

We investigate the principle of soft isolation: reduce
the risk of sharing through better scheduling. With ex-
perimental measurements, we show that a minimum run
time (MRT) guarantee for VM virtual CPUs that lim-
its the frequency of preemptions can effectively prevent
existing Prime+Probe cache-based side-channel attacks.
Through experimental measurements, we find that the
performance impact of MRT guarantees can be very low,
particularly in multi-core settings. Finally, we integrate a
simple per-core CPU state cleansing mechanism, a form
of hard isolation, into Xen. It provides further protection
against side-channel attacks at little cost when used in
conjunction with an MRT guarantee.

1

|,Non-data
sec14-paper-lentz-update.pdf,|

Emerging mobile social apps use short-range radios to dis-
cover nearby devices and users. The device discovery proto-
col used by these apps must be highly energy-efficient since it
runs frequently in the background. Also, a good protocol must
enable secure communication (both during and after a period
of device co-location), preserve user privacy (users must not
be tracked by unauthorized third parties), while providing se-
lective linkability (users can recognize friends when strangers
cannot) and efficient silent revocation (users can permanently
or temporarily cloak themselves from certain friends, unilater-
ally and without re-keying their entire friend set).

We introduce SDDR (Secure Device Discovery and Recog-
nition), a protocol that provides secure encounters and satisfies
all of the privacy requirements while remaining highly energy-
efficient. We formally prove the correctness of SDDR, present
a prototype implementation over Bluetooth, and show how ex-
isting frameworks, such as Haggle, can directly use SDDR. Our
results show that the SDDR implementation, run continuously
over a day, uses only 10% of the battery capacity of a typical
smartphone. This level of energy consumption is four orders
of magnitude more efficient than prior cryptographic protocols
with proven security, and one order of magnitude more effi-
cient than prior (unproven) protocols designed specifically for
energy-constrained devices.

1 |,Non-data
p359.pdf,|
Black hat search engine optimization (SEO), the practice of abu-
sively manipulating search results, is an enticing method to acquire
targeted user traffic. In turn, a range of interventionsfrom mod-
ifying search results to seizing domainsare used to combat this
activity. In this paper, we examine the effectiveness of these inter-
ventions in the context of an understudied market niche, counterfeit
luxury goods. Using eight months of empirical crawled data, we
identify 52 distinct SEO campaigns, document how well they are
able to place search results for sixteen luxury brands, how this ca-
pability impacts the dynamics of their order volumes and how well
existing interventions undermine this business when employed.

1.

|,Data
p1602-doerner.pdf,|
When a group of individuals and organizations wish to compute a
stable matchingfor example, when medical students are matched
to medical residency programsthey often outsource the compu-
tation to a trusted arbiter in order to preserve the privacy of par-
ticipants preferences. Secure multi-party computation offers the
possibility of private matching processes that do not rely on any
common trusted third party. However, stable matching algorithms
have previously been considered infeasible for execution in a se-
cure multi-party context on non-trivial inputs because they are com-
putationally intensive and involve complex data-dependent mem-
ory access patterns.

We adapt the classic Gale-Shapley algorithm for use in such a
context, and show experimentally that our modifications yield a
lower asymptotic complexity and more than an order of magni-
tude in practical cost improvement over previous techniques. Our
main improvements stem from designing new oblivious data struc-
tures that exploit the properties of the matching algorithms. We ap-
ply a similar strategy to scale the Roth-Peranson instability chain-
ing algorithm, currently in use by the National Resident Matching
Program. The resulting protocol is efficient enough to be useful
at the scale required for matching medical residents nationwide,
taking just over 18 hours to complete an execution simulating the
2016 national resident match with more than 35,000 participants
and 30,000 residency slots.

1.

|,Data
sec15-paper-smolyar.pdf,|

I/O virtualization (SRIOV)

Single root
is a hard-
ware/software interface that allows devices to self virtu-
alize and thereby remove the host from the critical I/O
path. SRIOV thus brings near bare-metal performance to
untrusted guest virtual machines (VMs) in public clouds,
enterprise data centers, and high-performance comput-
ing setups. We identify a design flaw in current Ethernet
SRIOV NIC deployments that enables untrusted VMs to
completely control the throughput and latency of other,
unrelated VMs. The attack exploits Ethernet pause
frames, which enable network flow control functional-
ity. We experimentally launch the attack across sev-
eral NIC models and find that it is effective and highly
accurate, with substantial consequences if left unmiti-
gated:
(1) to be safe, NIC vendors will have to mod-
ify their NICs so as to filter pause frames originating
from SRIOV instances; (2) in the meantime, administra-
tors will have to either trust their VMs, or configure their
switches to ignore pause frames, thus relinquishing flow
control, which might severely degrade networking per-
formance. We present the Virtualization-Aware Network
Flow Controller (VANFC), a software-based SRIOV NIC
prototype that overcomes the attack. VANFC filters pause
frames from malicious virtual machines without any loss
of performance, while keeping SRIOV and Ethernet flow
control hardware/software interfaces intact.

1

|,Non-data
sec14-paper-mowery.pdf,|
Advanced imaging technologies are a new class of peo-
ple screening systems used at airports and other sensitive
environments to detect metallic as well as nonmetallic
contraband. We present the first independent security
evaluation of such a system, the Rapiscan Secure 1000
full-body scanner, which was widely deployed at airport
checkpoints in the U.S. from 2009 until 2013. We find
that the system provides weak protection against adaptive
adversaries: It is possible to conceal knives, guns, and
explosives from detection by exploiting properties of the
devices backscatter X-ray technology. We also investi-
gate cyberphysical threats and propose novel attacks that
use malicious software and hardware to compromise the
the effectiveness, safety, and privacy of the device. Over-
all, our findings paint a mixed picture of the Secure 1000
that carries lessons for the design, evaluation, and opera-
tion of advanced imaging technologies, for the ongoing
public debate concerning their use, and for cyberphysical
security more broadly.
1
|,Non-data
p450-foster.pdf,|
Email as we use it today makes no guarantees about message in-
tegrity, authenticity, or confidentiality. Users must explicitly en-
crypt and sign message contents using tools like PGP if they wish
to protect themselves against message tampering, forgery, or eaves-
dropping. However, few do, leaving the vast majority of users open
to such attacks. Fortunately, transport-layer security mechanisms
(available as extensions to SMTP, IMAP, POP3) provide some de-
gree of protection against network-based eavesdropping attacks. At
the same time, DKIM and SPF protect against network-based mes-
sage forgery and tampering.

In this work we evaluate the security provided by these proto-
cols, both in theory and in practice. Using a combination of mea-
surement techniques, we determine whether major providers sup-
ports TLS at each point in their email message path, and whether
they support SPF and DKIM on incoming and outgoing mail. We
found that while more than half of the top 20,000 receiving MTAs
supported TLS, and support for TLS is increasing, servers do not
check certificates, opening the Internet email system up to man-
in-the-middle eavesdropping attacks. At the same time, while use
of SPF is common, enforcement is limited. Moreover, few of the
senders we examined used DKIM, and fewer still rejected invalid
DKIM signatures. Our findings show that the global email system
provides some protection against passive eavesdropping, limited
protection against unprivileged peer message forgery, and no pro-
tection against active network-based attacks. We observe that pro-
tection even against the latter is possible using existing protocols
with proper enforcement.

1.

|,Data
p57-wang.pdf,|
Censorship-circumvention systems are designed to help users by-
pass Internet censorship. As more sophisticated deep-packet-
inspection (DPI) mechanisms have been deployed by censors to de-
tect circumvention tools, activists and researchers have responded
by developing network protocol obfuscation tools. These have
proved to be effective in practice against existing DPI and are now
distributed with systems such as Tor.

In this work, we provide the first in-depth investigation of the
detectability of in-use protocol obfuscators by DPI. We build a
framework for evaluation that uses real network traffic captures to
evaluate detectability, based on metrics such as the false-positive
rate against background (i.e., non obfuscated) traffic. We first
exercise our framework to show that some previously proposed
attacks from the literature are not as effective as a censor might
like. We go on to develop new attacks against five obfuscation
tools as they are configured in Tor, including:
two variants of
obfsproxy, FTE, and two variants of meek. We conclude by using
our framework to show that all of these obfuscation mechanisms
could be reliably detected by a determined censor with sufficiently
low false-positive rates for use in many censorship settings.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: GeneralSecu-
rity and protection

Keywords
Censorship-resistance; network obfuscation; Tor

1.

|,Data
06547124.pdf,|In contrast to testing, mathematical reasoning and
formal verification can show the absence of whole classes of
security vulnerabilities. We present the, to our knowledge, first
complete, formal, machine-checked verification of information
flow security for the implementation of a general-purpose mi-
crokernel; namely seL4. Unlike previous proofs of information
flow security for operating system kernels, ours applies to the
actual 8,830 lines of C code that implement seL4, and so rules
out the possibility of invalidation by implementation errors in
this code. We assume correctness of compiler, assembly code,
hardware, and boot code. We prove everything else. This proof
is strong evidence of seL4s utility as a separation kernel, and
describes precisely how the general purpose kernel should be
configured to enforce isolation and mandatory information flow
control. We describe the information flow security statement
we proved (a variant of intransitive noninterference), including
the assumptions on which it rests, as well as the modifications
that had to be made to seL4 to ensure it was enforced. We
discuss the practical limitations and implications of this result,
including covert channels not covered by the formal proof.

I. |,Non-data
p1690-argyros.pdf,|
Finding differences between programs with similar function-
ality is an important security problem as such differences can
be used for fingerprinting or creating evasion attacks against
security software like Web Application Firewalls (WAFs)
which are designed to detect malicious inputs to web ap-
plications. In this paper, we present SFADiff, a black-box
differential testing framework based on Symbolic Finite Au-
tomata (SFA) learning. SFADiff can automatically find
differences between a set of programs with comparable func-
tionality. Unlike existing differential testing techniques, in-
stead of searching for each difference individually, SFADiff
infers SFA models of the target programs using black-box
queries and systematically enumerates the differences be-
tween the inferred SFA models. All differences between the
inferred models are checked against the corresponding pro-
grams. Any difference between the models, that does not
result in a difference between the corresponding programs,
is used as a counterexample for further refinement of the in-
ferred models. SFADiffs model-based approach, unlike ex-
isting differential testing tools, also support fully automated
root cause analysis in a domain-independent manner.

We evaluate SFADiff in three different settings for find-
ing discrepancies between: (i) three TCP implementations,
(ii) four WAFs, and (iii) HTML/JavaScript parsing imple-
mentations in WAFs and web browsers. Our results demon-
strate that SFADiff is able to identify and enumerate the
differences systematically and efficiently in all these settings.
We show that SFADiff is able to find differences not only
between different WAFs but also between different versions
of the same WAF. SFADiff is also able to discover three
previously-unknown differences between the HTML/Java-
Script parsers of two popular WAFs (PHPIDS 0.7 and Ex-
pose 2.4.0) and the corresponding parsers of Google Chrome,
Firefox, Safari, and Internet Explorer. We confirm that all
these differences can be used to evade the WAFs and launch
successful cross-site scripting attacks.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978383

1.

|,Non-data
sec14-paper-goktas.pdf,|
Code-reuse attacks based on return oriented program-
ming are among the most popular exploitation tech-
niques used by attackers today. Few practical defenses
are able to stop such attacks on arbitrary binaries with-
out access to source code. A notable exception are the
techniques that employ new hardware, such as Intels
Last Branch Record (LBR) registers, to track all indirect
branches and raise an alert when a sensitive system call is
reached by means of too many indirect branches to short
gadgetsunder the assumption that such gadget chains
would be indicative of a ROP attack. In this paper, we
evaluate the implications. What is too many and how
short is short? Getting the thresholds wrong has seri-
ous consequences. In this paper, we show by means of
an attack on Internet Explorer that while current defenses
based on these techniques raise the bar for exploitation,
they can be bypassed. Conversely, tuning the thresholds
to make the defenses more aggressive, may flag legit-
imate program behavior as an attack. We analyze the
problem in detail and show that determining the right val-
ues is difficult.

1

|,Non-data
p1702-bocovich.pdf,|
As the capabilities of censors increase and their ability to perform
more powerful deep-packet inspection techniques grows, more pow-
erful systems are needed in turn to disguise user traffic and allow
users under a censors influence to access blocked content on the In-
ternet. Decoy routing is a censorship resistance technique that hides
traffic under the guise of a HTTPS connection to a benign, uncen-
sored overt site. However, existing techniques far from perfectly
mimic a typical access of content on the overt server. Artificial
latency introduced by the system, as well as differences in packet
sizes and timings betray their use to a censor capable of performing
basic packet and latency analysis. While many of the more recent
decoy routing systems focus on deployability concerns, they do so
at the cost of security, adding vulnerabilities to both passive and
active attacks. We propose Slitheen, a decoy routing system capa-
ble of perfectly mimicking the traffic patterns of overt sites. Our
system is secure against previously undefended passive attacks, as
well as known active attacks. Further, we show how recent in-
novations in traffic-shaping technology for ISPs mitigate previous
deployability challenges.

CCS Concepts
Security and privacy  Privacy protections; Network security;
Networks  Network protocols;

Keywords
censorship resistance, decoy routing, network latency, TLS, HTTP
state

1.

|,Data
p492-liu.pdf,|
Random walks form a critical foundation in many social net-
work based security systems and applications. Currently,
the design of such social security mechanisms is limited to
the classical paradigm of using fixed-length random walks
for all nodes on a social graph. However, the fixed-length
walk paradigm induces a poor trade-off between security and
other desirable properties.

In this paper, we propose SmartWalk, a security enhanc-
ing system which incorporates adaptive random walks in so-
cial network security applications. We utilize a set of su-
pervised machine learning techniques to predict the neces-
sary random walk length based on the structural charac-
teristics of a social graph. Using experiments on multiple
real world topologies, we show that the desired walk length
starting from a specific node can be well predicted given the
local features of the node, and limited knowledge for a small
set of training nodes. We describe node-adaptive and path-
adaptive random walk usage models, where the walk length
adaptively changes based on the starting node and the inter-
mediate nodes on the path, respectively. We experimentally
demonstrate the applicability of adaptive random walks on
a number of social network based security and privacy sys-
tems, including Sybil defenses, anonymous communication
and link privacy preserving systems, and show up to two
orders of magnitude improvement in performance.

1.

|,Data
06547101.pdf,|Memory corruption bugs in software written in
low-level languages like C or C++ are one of the oldest problems
in computer security. The lack of safety in these languages
allows attackers to alter the programs behavior or take full
control over it by hijacking its control flow. This problem has
existed for more than 30 years and a vast number of potential
solutions have been proposed, yet memory corruption attacks
continue to pose a serious threat. Real world exploits show that
all currently deployed protections can be defeated.

This paper sheds light on the primary reasons for this
by describing attacks that succeed on todays systems. We
systematize the current knowledge about various protection
techniques by setting up a general model for memory corrup-
tion attacks. Using this model we show what policies can stop
which attacks. The model identifies weaknesses of currently
deployed techniques, as well as other proposed protections
enforcing stricter policies.

We analyze the reasons why protection mechanisms imple-
menting stricter polices are not deployed. To achieve wide
adoption, protection mechanisms must support a multitude of
features and must satisfy a host of requirements. Especially
important
is performance, as experience shows that only
solutions whose overhead is in reasonable bounds get deployed.
A comparison of different enforceable policies helps de-
signers of new protection mechanisms in finding the balance
between effectiveness (security) and efficiency. We identify some
open research problems, and provide suggestions on improving
the adoption of newer techniques.

I. |,Non-data
06547104.pdf,|Centralized botnets are easy targets for takedown
efforts by computer security researchers and law enforcement.
Thus, botnet controllers have sought new ways to harden the
infrastructures of their botnets. In order to meet this objective,
some botnet operators have (re)designed their botnets to use
Peer-to-Peer (P2P) infrastructures. Many P2P botnets are far
more resilient to takedown attempts than centralized botnets,
because they have no single points of failure. However, P2P
botnets are subject to unique classes of attacks, such as node
enumeration and poisoning. In this paper, we introduce a
formal graph model to capture the intrinsic properties and
fundamental vulnerabilities of P2P botnets. We apply our
model to current P2P botnets to assess their resilience against
attacks. We provide assessments on the sizes of all eleven active
P2P botnets, showing that some P2P botnet families contain
over a million bots. In addition, we have prototyped several
mitigation strategies to measure the resilience of existing P2P
botnets. We believe that the results from our analysis can
be used to assist security researchers in evaluating mitigation
strategies against current and future P2P botnets.

I. |,Non-data
06547117.pdf,|Secure data deletion is the task of deleting data ir-
recoverably from a physical medium. In the digital world, data
is not securely deleted by default; instead, many approaches
add secure deletion to existing physical medium interfaces.
Interfaces to the physical medium exist at different layers, such
as user-level applications, the file system, the device driver, etc.
Depending on which interface is used, the properties of an
approach can differ significantly.

In this paper, we survey the related work in detail and
organize existing approaches in terms of their interfaces to
physical media. We further present a taxonomy of adversaries
differing in their capabilities as well as a systematization for the
characteristics of secure deletion approaches. Characteristics
include environmental assumptions, such as how the interfaces
use affects the physical medium, as well as behavioural prop-
erties of the approach such as the deletion latency and physical
wear. We perform experiments to test a selection of approaches
on a variety of file systems and analyze the assumptions made
in practice.

Keywords-Secure deletion, Flash memory, Magnetic memory,

File systems

I. |,Non-data
06547130.pdf,|Internet users today depend daily on HTTPS for
secure communication with sites they intend to visit. Over
the years, many attacks on HTTPS and the certificate trust
model it uses have been hypothesized, executed, and/or evolved.
Meanwhile the number of browser-trusted (and thus, de facto,
user-trusted) certificate authorities has proliferated, while the
due diligence in baseline certificate issuance has declined. We
survey and categorize prominent security issues with HTTPS
and provide a systematic treatment of the history and on-going
challenges, intending to provide context for future directions.
We also provide a comparative evaluation of current proposals
for enhancing the certificate infrastructure used in practice.
Keywords-SSL; certificates; browser trust model; usability.

I. INTRODUCTORY REMARKS

Enabling end users to easily communicate sensitive data
online was a significant milestone in the development of
todays web, and, arguably, a necessary condition for its
explosive growth. Little-changed since its early days (1994
2000), the core SSL/TLS technology persists as the basis
for securing many aspects of todays Internet
including
software download, data transfer, user passwords, and for
site authentication. While centred on the HTTPS protocol
(HTTP over SSL/TLS), its security servicesconfidentiality,
message integrity, and site authenticationfundamentally
rely on the correct interplay of out-of-band infrastructures,
procedures, and trust decisions.

While the web has moved from serving static information
pages to one which is relied on for billions of dollars of
commerce and for supporting critical infrastructures, there
has been an erosion of confidence in the HTTPS certificate
infrastructure for multiple reasons, e.g., increasing issuance
of server certificates through fully-automated (domain val-
idated) procedures, a proliferation of certificate authorities
(CAs) which may either directly issue site certificates or
certificates for other CAs, and the compromise of real-world
CAs leading to increased concern amongst security experts
of real-world man-in-the-middle (MITM) attacks on HTTPS.
SSL/TLS has evolved in response to the discovery of
cryptographic weaknesses and protocol design flaws. Prob-
lems with the certificate model appear to be more chal-
lenging, including among others: design and implementation

Extended version available [32].

issues in the CA/Browser (CA/B) trust model leading to
fragility (compromise of a single CA can, at least tem-
porarily, undermine system-wide security) and lack of trust
agility, poor support for certificate revocation, a reduction
in CA diligence in certificate issuance, and user interface
challenges related to reliably signalling to end-users,
in
ways not ignored or spoofed, security indicators and site
authentication information.

In this paper, we provide a broad perspective of the
SSL/TLS (henceforth TLS) mechanism, as employed with
web browsers for securing HTTP traffic. We consider
HTTPS,
the underlying CA infrastructure, CA/B trust
model, and proposed enhancements. Among many important
HTTPS-related topics beyond our main focus are: phishing,
performance enhancements, use of certificates for client-
authentication, and the use of TLS beyond securing HTTP.
Our main contributions are the following: (1) We classify
and put into a broader context disparate contributions on
HTTPS security, spanning elements of cryptographic de-
sign and implementation, systems software, operations, and
human factors. (2) We provide a comparative evaluation
of existing proposals to enhance security aspects of the
CA/B model, deconstructing and evaluating their core ideas.
(3) Building on this contextual review, classification, and
analysis, we summarize open problems and future research
directions. In addition, by systematic discussion of security
issues in a single place, we hope to provide perspective based
on the hindsight of a multitude of historical problems. Our
work highlights the overall complexity, including algorithms,
protocols, infrastructure, configuration, and interfaces, and
contributes an overall understanding of which issues are
addressed by which enhancements and protocol revisions.

II. BACKGROUND

Historical Objectives: SSL was developed to address
Netscapes needs for securing web traffic, and specifically
designed to work well with HTTP [84]. Network protection
of data like credit card details sent from client to server
motivated two major design goals: confidentiality, and server
authenticationsensitive data should be released only to
a party one would intend to do business with, i.e., the

 2012, Jeremy Clark. Under license to IEEE.
DOI 10.1109/SP.2013.41

511

correct web server.1 Client authentication was an optional
third goal, however the credit card number largely replaced
user identity. Even today, while TLS supports client-side au-
thentication, this feature is little-used on the public internet;
we do not consider it at length herein.

As Netscape intended SSL to be a core technology beyond
use with HTTP alone, and since most high-runner internet
protocols ran over TCP, SSL was designed to provide a gen-
eral channel that can be adopted with minimal modification
by almost any TCP-based protocol seeking some security.
An important property was termed transparency: the data
that one end writes is exactly what the other end reads [84].
Protocol Specification: HTTPS combines the network
protocol HTTP with the cryptographic protocol TLS. The
TLS protocol (v1.0, 1.1, 1.2) updates the older public
SSL protocol (v3.0). TLS provides a secure tunnel to a
server, which is most commonly authenticated by an X.509
certificate. Specification of the cryptographic primitives used
by X.509 is largely delegated to PKCS standards. We do
not focus on protocols (e.g., IMAP or SMTP) other than
HTTP run over TLS, nor the use of TLS with transport
layer protocols other than TCP (e.g., DTLS).

III. CRYPTO PROTOCOL ISSUES IN HTTPS

In this section, we consider attacks on the TLS protocol
which relate to HTTPS security. Section IV expands focus
to the broader CA/B infrastructure and human decisions
involved. As TLS is well-documented, we assume familiarity
with the basic protocol. Many attacks refine known tech-
niques; examining both historical and recent attacks provides
a fuller perspective.

A. Weaknesses in Cryptographic Primitives

1) Weak Encryption & Signature Key Lengths: Several
encryption functions offered in the ciphersuites of early
versions of TLS are no longer considered secure. Any
symmetric key encryption scheme with 40, 56, or 64 bit
keys is subject to a brute-force attack. TLS supported DES,
RC2, and RC4 with some of these key lengths. Asymmetric
encryption schemes like RSA are subject to factoring attacks
when used with a 512 bit modulus. A 2007 analysis of TLS
servers found that while only 4% of sites still offered RSA-
512, 93% supported (single) DES [68]. Note that support-
ing an insecure primitive does not imply it is ever used,
as security parameters are negotiated (but see Downgrade
Attacks below). NIST strongly recommends that primitives
hold the equivalent of 112 bits (symmetric) security strength
and will require this by 2014 [22] (e.g., phasing out 1024-bit
RSA/DSA and 193-bit ECDSA).

Key length is also an issue for certificates. Sufficient key
lengths should be used by the certificate authority to sign a

1The meaning of correct remains challenging today (see Section VI).

certificate, and CAs should only sign certificates containing
public keys that are of sufficient length.2

2) Weak Hash Functions: To issue a site certificate, CAs
sign its hash. Collision-resistance of the hash is paramount:
an adversary that could construct two meaningful certificates
with the same digest could transfer a CA signature from
a benign site certificate to a malicious CA certificate. The
MD5 hash function, published in 1992, has been eligible
for providing certificate digests. However the collision re-
sistance of MD5 has deteriorated over time, from generic
attacks [102] to the first published collision [37] to the gen-
eration of meaningful collisions [98], and finally finding
collisions that are structured enough to be both an acceptable
benign site certificate and a malicious root certificate [99].
Use of MD5 is discouraged (RFC 3279) and certificates
digested with MD5 are in decline [54]. MD5 remains
recommended in other places in the TLS protocol where
collision-resistance of the hash function is not critical, i.e.,
HMAC and key derivation [65], [25], [66].

B. Implementation Flaws and Related Attacks

1) PRNG Seeding: Many values in the TLS protocol
are generated randomly, including secret keys. This requires
a strong pseudorandom number generator (PRNG), seeded
with a high entropy seed. The Netscape browser (prior
to 1.22) relied on a PRNG implementation with weak
keys [51] allowing the TLS session key (master secret) to be
predictable. A 2008 change to the Debian operating system
reduced the randomness served to OpenSSL, which was
used to generate TLS certificates with predictable private
keys [24], [15], [110]. Recently, 0.5% of TLS certificates
were found to have recoverable RSA private keys due to
shared prime factors [53], [70]; most originated from poor
PRG seeding in embedded devices.

2) Remote Timing Attacks: Remote timing attacks have
been used against TLS servers that use an optimized variant
of RSA decryption, the default in OpenSSL versions prior to
0.9.7b [30], [13]. The decryption algorithm makes branching
decisions that are functionally dependent on the long-term
certified secret key. This results in measurable differences
in execution time, leaking information about the key during
TLS handshakes. Previous OpenSSL implementations of
ECDSA enabled similar remote timing attacks [29].

C. Oracle Attacks

The following attacks interactively and adaptively query
the victims protocol implementation, treating it as an oracle.
1) RSA Encoding: SSL 3.0 with the RSA ciphersuite uses
textbook RSA (which enables ciphertext malleability) for
transporting a PKCS#1 v1.5 encoded premaster secret to
the server during the handshake. If upon decryption and
decoding, the plaintext is not properly encoded, an error

2An intermediate CA in Nov 2011 was revoked for issuing certificates

for 512-bit RSA keys. http://www.entrust.net/advisories/malaysia.htm.

512

is returned to the client. An adversary could capture an
encrypted premaster secret, and,
in separate handshakes
with the same server, submit adaptively modified versions
of it, learning if they are conformant [27]. With just this
information, the adversary can eventually (1M queries)
recover the premaster secret. TLS 1.0 consequently recom-
mends that encoding errors are handled indistinguishably
from successful decryptions.3

2) CBC Initialization: In TLS 1.0 and earlier, all block
ciphers are used in cipher block chaining (CBC) mode.
Records are encrypted individually, however the initializa-
tion vector for each (except the first) is set equal to the last
block of ciphertext sent (i.e., in a predictable way). CBC
with predictable IVs is not secure against chosen plaintext
attacks [23], and thus an adversary capable of injecting
partial plaintext into a TLS connection and of observing the
transmitted ciphertext can determine semantic information
about the rest of the plaintext [20], [21]. In one instantiation
of this attack, BEAST, an adversary submits adaptively
chosen cross-site requests for a domain with a secure cookie
to learn the value of the cookie (and by adjusting the amount
of the value included in a single block, due to partitioning,
the value can be guessed byte-by-byte) [38]. This issue is
resolved in TLS 1.1, not applicable to any stream cipher
(e.g., RC4), and is purportedly mitigated by first sending
the first byte as a separate record (1/n-1 record splitting).4
3) Compression: The use of data compression is a ne-
gotiable option in TLS, although one never broadly sup-
ported by browsers. TLS does not obfuscate the length of a
compressed TLS record, thus again an adversary capable
of injecting partial plaintext into a TLS connection and
observing the post-compression record length can determine
semantic information about the rest of the plaintext [64].
An instantiation of this attack, CRIME, used a similar
setup to BEAST for recovering secret values from secure
cookies [89]. As a result, all major browsers have disabled
TLS compression.

4) CBC Padding: An extended version of this paper [32]
discusses oracle attacks on CBC padding [103], [31], which
until very recently [16] applied only to non-HTTPS proto-
cols run over TLS.

D. Protocol-level Attacks

1) Ciphersuite Downgrade Attack: The ciphersuite used
by the client and server is negotiated during the TLS hand-
shake. In SSL 2.0, a man-in-the-middle could influence the
negotiation and downgrade the strength of the ciphersuite to
the weakest acceptable by both parties. This is fixed in SSL
3.0 and all versions of TLS by having the client send, once
the MAC keys have been established, an authenticated digest

3Also note that while RSA key transport support is ubiquitous, 6070%
of servers also support Diffe-Hellmen key exchange [86], [104] which has
the added benefit of perfect forward secrecy.

4A. Langley, BEAST Followup, ImperialViolet (blog), 15 Jan 2012.

of the previous handshake messages and waiting for an
authenticated confirmation from the server. Thus, downgrade
prevention is contingent on the unavailability of weak MAC
functions for negotiation.

2) Version Downgrade Attack: The TLS version is also
negotiated and while version downgrade attacks are not
possible against a strict implementation of the TLS spec-
ification, many client implementations respond to certain
server errors by reconnecting with an older TLS version.
These server errors can be spoofed by an attacker. To prevent
an adversary from first downgrading to SSL 2.0 and then
downgrading the ciphersuite, TLS prohibits downgrading
to SSL 2.0. TLS implementations may still be vulnerable
to downgrades from later version to earlier versions (e.g.,
from TLS 1.1+ to TLS 1.0 to exploit CBC initialization
vulnerabilities). One mitigation is to include the highest
supported version number in the list of ciphersuites during
negotiation, extending ciphersuite-downgrade protection to
versions [5].

3) Renegotiation Attack: Once a TLS connection has
been established, either party can at any point request a
new handshake, within the existing tunnel, to renegotiate
the cipher suite, session key, or other relevant connection
parameters. The renegotiation protocol was discovered to be
flawed in 20095 and was subsequently updated [1]. The erro-
neous version allowed an adversary to establish a connection
to a server, send data, renegotiate, and pass the renegotiated
connection onto a client
is forming an
initial connection. This effectively allowed the adversary
to prepend chosen records to new HTTPS connections. An
extension [50] to the standardized countermeasures [1] can
provide a strong notion of renegotiation security.

that believes it

4) Cross-Protocol Attacks: An extended version of this
paper [32] addresses cross-protocol attacks [105], [74] where
parameters intended to be used in one setting (e.g., Diffie-
Hellmen) are replayed in a different setting (e.g., RSA).

IV. TRUST MODEL ISSUES IN HTTPS

Section III narrowly considered attacks on the TLS proto-
col and the cryptographic algorithms it involves. This section
assumes a perfectly functioning TLS protocol and considers
attacks on the broader CA/B infrastructure. Our analysis
covers the certification process itself, who is allowed to be
a certificate authority (anchoring trust), how this authority
can be delegated (transitivity of trust), how certificates
are revoked (maintenance of trust), and how users interact
with certificate information (indication and interpretation of
trust). In what follows, we specifically note which issues
remain unresolved.

5M. Ray, Authentication Gap in TLS Renegotiation, Extended Subset

(blog), 4 Nov 2009.

513

A. Certification

A web certificate binds a public signing key to an iden-
tity. The correctness of the binding is asserted through a
digital signature, by a CA implicitly expected to maintain
the accuracy of the binding over time. TLS enables client
software to establish a confidential channel terminated by the
entity holding the private key associated with the certificate.
The essential attribute that all HTTPS server certificates
have is a domain name which the certificate holder controls.
This is placed in the commonName (CN) attribute under
Subject, unless one or more domains are indicated in the
subject alternative name field in an X.509 extension. If an
entity requests a certificate for a domain name, the CA will
typically challenge the requester to demonstrate control over
the domain. Note that this implicitly assumes that domain
names are mapped to the correct webserver (IP address), a
mapping accomplished through DNS. Such certificates are
called domain validated (DV) certificates.

Issued certificates may include additional CA-verified
information, such as organization name and postal address.
Validation procedures have degraded over time, exemplified
by more CAs using a completely automated process (e.g.,
automated DV certificates). In response, the CA/Browser
Forum established extended validation (EV) certificates and
guidelines for their issuance,6 including diligent human val-
idation of a sites identity and business registration details.

Security Issues (Certification)

Hostname Validation (CAs): Automated domain val-
idation services provided by a CA will typically send a
validation email to a fixed email address associated with
the CNs top-level domain (e.g., admin@domain) or one
taken from CNs WhoIS record. Both mechanisms rely on
accurate domain information; thus any disruption to the CAs
ability to receive accurate DNS records (e.g., DNS cache
poisoning [61], [94]) could result in an improperly issued
certificate. For email validation, CAs should also ensure the
email address is only accessible to the site administrator. For
example, a certificate for the login page of Microsofts public
webmail service, login.live.com (formally Hotmail),
was wrongfully issued by a CA that offered to validate
through sslcertificates@live.com, an email ad-
dress that was open to public registration [111].

Even with non-automated validation, an adversary may
employ social engineering. For example, in 2001, a CA
wrongfully issued two certificates to someone posing as a
current Microsoft employee.7

Hostname Validation (Clients): Although current
browser platforms validate that a received site certificate

6CA/Browser Forum: Guidelines For The Issuance And Management Of

Extended Validation Certificates (v1.4), 2012.

7Microsoft MS01-017: Erroneous VeriSign-Issued Digital Certificates

Pose Spoofing Hazard, 22 Mar 2001.

matches the hostname, some non-browser software had inad-
equate validation. Many mobile applications display HTTPS
content and one study found that 1074 of 13500 Android
apps did not validate the hostname (aside from other TLS
implementation flaws) [42]. A concurrent study identified
the lack of hostname validation in cloud clients (Ama-
zons EC2 libraries), e-commerce backend systems (Paypals
SDK), online shopping carts, ad networks (AdMob), and
other non-browser software employing HTTPS [49].

Parsing attacks: Flaws relating to parsing enable im-
proper issuance (incorrect CA parsing) and validation (in-
correct browser parsing) of certificates. Certificate requests
containing a null character () in the CN can be misinter-
preted. For example, a CN of bank.comevil.com was
validated by some CAs automated domain validation as
evil.com while browsers have been known to accept it
as a valid CN for bank.com [62], [71]. A dangerous vari-
ation is *evil.com, which grants a universal wildcard
certificate acceptable to older NSS-based browsers [71].

Some CAs and browsers have also inconsistently in-
terpreted the object IDs specifying which string is the
commonName:
for example, CN is identified by OID
2.5.4.3 but some browser parsers accepted 2.5.4.003 or
2.5.4.18446744073709551619 (64-bit uint overflow) as the
CN, while some CAs ignore them [62].

EV downgrading: Many of the problems associated
with automated domain validation are claimed to be thwarted
by EV certificates. However a site that holds an EV certifi-
cate can be downgraded to normal HTTPS by a man-in-
the-middle (MITM) attack with a fraudulent DV certificate.
Furthermore, such an adversary can arrange for the EV cer-
tificate to be displayed through a rebinding attack [112],
[96] that is consistent with the browsers origin policy [55].

B. Anchoring Trust

Validating that a certificate request comes from the entity
specified in the SubjectName is an important CA func-
tion. As no one entity has universal control of all names-
paces, it is not clear who is best suited for such validation.
As a result, there exists a spectrum of CAs, with the majority
of site certificates being issued by commercial CAs with ties
to the security or domain registration industries.

Software vendors (e.g., Microsoft, Apple, Mozilla, Opera)
configure a default
list of self-signed CA certificates in
operating systems and/or browser as trust anchors. Each
HTTPS site whose site certificate the browser accepts is thus
de facto trusted by users because its certificate has been
vouched for (directly or indirectly) by at least one of the
trust anchors. Mozillas Firefox 15, for example, includes
150 trust anchors from 50 organizations. However since
CAs with trust anchors can issue certificates empowering
other organizations to act as a CA (see below), the number
of automatically trusted CAs is much larger. The SSL Ob-
servatory reports that between Microsofts Internet Explorer

514

and Mozillas Firefox, 1500 CA certificates from 650
organizations8 in 50 countries are browser-accepted [39].
On private networks, particularly in corporate environ-
ments, a root certificate for the organization may be con-
figured as a trust anchor on employees machines. The
organization can then proxy (i.e., MITM) HTTPS connec-
tions with middleware boxes specifically designed for this
task to perform content inspection. The corporation may
even be able to obtain a browser-accepted CA certificate
for doing this, although issuing such a certificate is against
CA policies. For example, Trustwave admitted to issuing
certificates for this purpose but later revoked them,9 while,
purportedly by accident, TURKTRUST issued certificates
that were discovered being used this way.10 The mobile
browser OperaMini openly proxies HTTPS connections in
this way to allow compression between the client and
proxy [83]. Proxies assume all responsibility for certificate
validation, with a recent study finding that many implemen-
tations had validation flaws [59].

Security Issues (Anchoring Trust)

CA Compromise: Without further enhancement (i.e.,
without the primitives evaluated in Section V), any trusted
CA can issue a browser-acceptable certificate for any site.
Thus an adversary can target the weakest CA to obtain a
fraudulent certificate and, assuming clients would not notice
a different CA,
this certificate enables the adversary to
evade detection in a MITM attack. In 2011, two CAs were
compromised: Comodo11 and DigiNotar.12 In both cases,
certificates for high profile sites were illegitimately obtained
and, in the second case, reportedly used in a MITM attack.13
Compelled Certificates: Concerns have also been raised
about the abilities of nation-states to compel certificates
from a browser-accepted CA [93]. Governmental entities
are often well-positioned to proxy (i.e., MITM) HTTPS
connections by controlling network infrastructure and/or
compelling ISPs. For example, reportedly, HTTPS connec-
tions to Facebook over multiple ISPs within Syria were
MITMed with a Facebook certificate issued by the Syrian
Telecom Ministry.14 In this case, the Ministry was not a
browser-acceptable CA, and so it is not an example of a

8The reported number of organizations may be inflated due to variation
in organization name (or division) across certificates. Also in some cases,
the issuing CA retains actual possession of the intermediate certificate.

9Bug 724929, Bugzilla@Mozilla, reported: 7 Feb 2012.
10A. Langley, Enhancing digital certificate security, Google Online

Security Blog, 3 Jan 2013.

11J. Appelbaum, Detecting Certificate Authority compromises and web

browser collusion, Tor Blog, 22 Mar 2011.

12Black Tulip Report of the investigation into the DigiNotar Certificate

Authority breach, Fox-IT (Tech. Report), 13 Aug 2012.

state compelled certificate, but one that demonstrates the
danger posed.

C. Transitivity of Trust

Given that trust anchors can issue intermediate CA certifi-
cates (and intermediates can be enabled to do the same), a
site certificate is browser-acceptable if the browser can build
a chain of certificates that lead to a trust anchor. One study
found 20% of valid certificates required no intermediate and
38% used one [18].

The path validation algorithm is specified in RFC 5280 to
begin with the server certificate and build the path forward
to the trust anchor, although there are efficiencies to building
in reverse [40]. Certificate chains are subject to constraints.
Intermediate CA certificates must be authorized to be a CA
(CA:TRUE under basicConstraints). CA certificates
may also restrict the number of CAs that can precede it
(i.e., toward the leaf) in the chain (e.g., pathlen:0 un-
der basicConstraints means the CA cannot delegate
further CAs and can only sign leaf certificates).

Servers are mandated to present an entire chain, but in
practice, browsers may use a chain discovery mechanism
(e.g., AIA: Authority Information Access). Intermediate CAs
are invisible to client software until
their certificate is
encountered, and yet they are essentially as trusted as an
anchor. This makes it difficult for users or OSs/browsers to
preemptively know about and remove unacceptable interme-
diate CA certificates. As above, while only 50 organiza-
tions have visible trust anchors, many more organizations
have acceptable intermediate CA certificates. Technically,
Ford, Marks and Spencer, and the US Dept. of Homeland
Security are authorized for issuing acceptable certificates for
any website [39].

Security Issues (Transitivity of Trust)

Basic Constraints: Certificate path validation must also
check the constraints during validation, in particular that
each intermediate CA certificate has CA:TRUE set under
basicConstraints. If this is not checked, a certificate
obtained for a webserver could issue browser-acceptable
certificates for any other website. Initially not checked by
Microsofts CryptoAPI,15 this has now been patched.16 A
decade later, the issue resurfaced in Apples iOS.17

D. Maintenance of Trust

Another important function of a CA is to terminate the
validity of a certificate prior to its preconfigured expira-
tion date upon becoming aware of certain circumstances,
e.g., mistaken issuance, site or CA compromise, affiliation

13C. Arthur, Rogue web certificate could have been used to attack Iran

(online), 5 Apr 2002.

dissidents, The Guardian, 30 Aug 2011.

14P. Eckersley, A Syrian Man-In-The-Middle Attack against Facebook,

16MS02-050: Microsoft certificate validation flaw, 2002.
17CVE-2011-0228: iOS certificate chain validation issue in handling of

15M. Marlinspike, Internet Explorer SSL Vulnerability, thoughtcrime

EFF Deeplinks (blog), 5 May 2011.

X.509 certificates, 2011.

515

change, a superseding certificate, or cessation of the holders
operations [45]. Revocation status must also be available
through the issuing CA, i.e., by certificate revocation lists
(CRLs) or online certificate status checking protocol (OCSP)
responders. CRLs are signed by the CAs key while OCSP
responses are produced by servers designated by the CA.
CAs often prefer OCSP responders as they can be updated
on-demand without use of the (generally offline) CA signing
key and due to response size.

In practice, some CA certificates do not include any re-
vocation information, and when OCSP responders are spec-
ified, they are often unresponsive. Thus, current browsers
fail open, accepting certificates for which revocation in-
formation cannot be located (browsers should downgrade
all EV certificates to a regular certificate, or warn, as
responsive revocation is an EV requirement).18 In response
to the failings of revocation, some browsers (e.g., Chrome)
maintain an updatable certificate blacklist (see Section V-C).
While the mandatory expiration date field provides an
eventual default form of revocation, many certificates are
valid for multiple years (the median lifetime varies across
measurements: 12 [86], 1215 [54], or 24 [18] months).

Security Issues (Maintenance of Trust)

Blocking Revocation: If an adversary is able to obtain
a fraudulent certificate for a site which is subsequently
revoked, it may take several days for this information to be
available to clients, even with OCSP, due to caching [101].
Even then, the clients may not be able to reach an OCSP
responder or CRL distribution point. Further, a MITM
adversary could respond to a clients request with an HTTP
error (e.g., error 500: internal server error) or OCSP error
(response status 3: try again later [71]); in this case, the
revoked certificate typically continues to be accepted.

Similarly, a MITM adversary could prevent a browser
blacklist from updating but this would require persistent
blockingthe OCSP check would only occur when the
client encounters the fraudulent certificate and the MITM
adversary is already in position. The CAs CRL could have
been obtained by checking an unrelated certificate.

Ownership Transfer: Since TLS site certificates are
bound to a domain name, certificates should be revoked
when domain ownership expires or is transferred. This is
however not typically enforced. For example, Facebook (the
target of the Syrian MITM attack mentioned above) acquired
fb.com for $8.5M in 2010 but can have no assurance that the
previous owner does not have a valid unexpired certificate
for the site that could enable a MITM attack.19

E. Indication and Interpretation of Trust

Some HTTPS security protections rely on user due dili-
gence. Perhaps naively, users are expected to verify the
outcome of each connection attempt, typically indicated by
a visual cue in the browser window. More diligent users
may verify certificate details; e.g., that the subject name
organization, address, countrymatches their expectation.
Finally, the browser may require users to respond to warning
dialogues in some cases.

Browser Security Cues: Desktop browsers typically use
two primary cues to indicate a website is being accessed
over HTTPS: (1) the URL in the address bar begins with
https:// and (2) a lock icon is displayed somewhere
in the browsers chrome (i.e., the boundary region of the
window populated by the browser itself). Typically, clicking
on the lock icon will display information about the certifi-
cate. One impedance to better user understanding of browser
security indicators is the inconsistency of how cues are
implemented across browsers [19]. Guidelines for browser
cues have been published.20

One study used eyetracking to find that of 16 primed
participants interacting with an HTTPS site, 11 viewed the
lock, 7 the https:// indicator, and only 2 interacted with
the lock to display certificate information [108]. Another
study found that 63 of 67 participants logged into a hypo-
thetical banking website with all HTTPS indicators removed,
suggesting that many did not notice the difference [90]
(although one researcher argues this is enlarged by flaws in
the study21). The |,Non-data
06547122.pdf,|Sybil attacks in which an adversary forges a
potentially unbounded number of identities are a danger to
distributed systems and online social networks. The goal of
sybil defense is to accurately identify sybil identities.

This paper surveys the evolution of sybil defense protocols
that leverage the structural properties of the social graph
underlying a distributed system to identify sybil identities. We
make two main contributions. First, we clarify the deep con-
nection between sybil defense and the theory of random walks.
This leads us to identify a community detection algorithm that,
for the first time, offers provable guarantees in the context of
sybil defense. Second, we advocate a new goal for sybil defense
that addresses the more limited, but practically useful, goal of
securely white-listing a local region of the graph.

I. |,Non-data
sec14-paper-cox.pdf,|

This paper presents SpanDex, a set of extensions to An-
droids Dalvik virtual machine that ensures apps do not
leak users passwords. The primary technical challenge
addressed by SpanDex is precise, sound, and efficient
handling of implicit information flows (e.g., information
transferred by a programs control flow). SpanDex han-
dles implicit flows by borrowing techniques from sym-
bolic execution to precisely quantify the amount of infor-
mation a process control flow reveals about a secret. To
apply these techniques at runtime without sacrificing per-
formance, SpanDex runs untrusted code in a data-flow
sensitive sandbox, which limits the mix of operations that
an app can perform on sensitive data. Experiments with
a SpanDex prototype using 50 popular Android apps and
an analysis of a large list of leaked passwords predicts
that for 90% of users, an attacker would need over 80
login attempts to guess their password. Today the same
attacker would need only one attempt for all users.

1

|,Data
SAM3798.pdf,|Internet traffic exhibits long range dependence 
(persistence), scale invariance and self-similarity or self-
affinity  which  are  the  known  characteristics  of  fractals. 
Moreover, 
fractals  can  be 
extracted and quantified from an internet data time series 
using  non-integer  dimensions  (fractal  dimensions).  The 
notion of cognitive complexity is also very well represented 
by  the  fractal  dimensions,  e.g.,  high  value  of  fractal 
dimension of an object implies that the complexity of this 
object is higher than the one with lower fractal dimension. 
In addition, a multifractal object is more complex than a 
monofractal object and this can also be characterized to 
identify  the  degree  of  complexity.  In  this  work,  we  have 
shown that the complexity introduced by distributed denial 
of service (DDoS) attack packets in DNS (Domain Name 
System) traffic is higher than the complexity of DNS traffic 
with no DDoS attack packets. A power spectrum density of 
the data series was used to calculate the spectral fractal 
dimension, and the performance of the proposed algorithm 
is validated using mathematical fractal Brownian motion 
process  (fBm)  and  the  real  data  sets.  A  sequence  of 
spectral fractal dimension measurements of the time series 
(also known as a trajectory of spectral fractal dimension 
measurements  or  spectral  fractal  dimension  trajectory 
(SFDT)) was generated to show the changing complexity 
of the series in time domain.  

KeywordsDenial  of  service,  Domain  Name  System  (DNS), 
cyber threats, complexity, multifractal, power spectrum density, 
time  series,  spectral  fractal  dimension  trajectory  (SFDT), 
variance fractal dimension trajectory, malicious traffic. 

I. 

|,Data
imc40s-luckieAemb.pdf,|
Impediments to resolving IPv6 router aliases have precluded
understanding the emerging router-level IPv6 Internet topol-
ogy. In this work, we design, implement, and validate the
first Internet-scale alias resolution technique for IPv6. Our
technique, speedtrap,
leverages the ability to induce frag-
mented IPv6 responses from router interfaces in a particu-
lar temporal pattern that produces distinguishing per-router
fingerprints. Our algorithm surmounts three fundamental
challenges to Internet-scale IPv6 alias resolution using frag-
ment identifier values: (1) unlike for IPv4, the identifier
counters on IPv6 routers have no natural velocity, (2) the
values of these counters are similar across routers, and (3)
the packet size required to collect inferences is 46 times
larger than required in IPv4. We demonstrate the efficacy
of the technique by producing router-level Internet IPv6
topologies using measurements from CAIDAs distributed
infrastructure. Our preliminary work represents a step to-
ward understanding the Internets IPv6 router-level topol-
ogy, an important objective with respect to IPv6 network
resilience, security, policy, and longitudinal evolution.

Categories and Subject Descriptors
C.2.5 [Local and Wide-Area Networks]: Internet; C.2.1
[Network Architecture and Design]: Network topology;
C.2.3 [Computer Communication Networks]: Network
Operationsnetwork monitoring

Keywords
Internet topology; alias resolution; IPv6

1.

|,Data
sec14-paper-zhou.pdf,|

Correctly integrating third-party services into web ap-
plications is challenging, and mistakes can have grave
consequences when third-party services are used for
security-critical tasks such as authentication and autho-
rization. Developers often misunderstand integration re-
quirements and make critical mistakes when integrating
services such as single sign-on APIs. Since traditional
programming techniques are hard to apply to programs
running inside black-box web servers, we propose to de-
tect vulnerabilities by probing behaviors of the system.
This paper describes the design and implementation of
SSOScan, an automatic vulnerability checker for appli-
cations using Facebook Single Sign-On (SSO) APIs. We
used SSOScan to study the twenty thousand top-ranked
websites for five SSO vulnerabilities. Of the 1660 sites
in our study that employ Facebook SSO, over 20% were
found to suffer from at least one serious vulnerability.

1

|,Data
sec14-paper-dahse.pdf,|
Web applications evolved in the last decades from sim-
ple scripts to multi-functional applications. Such com-
plex web applications are prone to different types of se-
curity vulnerabilities that lead to data leakage or a com-
promise of the underlying web server. So called second-
order vulnerabilities occur when an attack payload is first
stored by the application on the web server and then later
on used in a security-critical operation.

In this paper, we introduce the first automated static
code analysis approach to detect second-order vulnera-
bilities and related multi-step exploits in web applica-
tions. By analyzing reads and writes to memory loca-
tions of the web server, we are able to identify unsani-
tized data flows by connecting input and output points of
data in persistent data stores such as databases or ses-
sion data. As a result, we identified 159 second-order
vulnerabilities in six popular web applications such as
the conference management systems HotCRP and Open-
Conf. Moreover, the analysis of web applications eval-
uated in related work revealed that we are able to detect
several critical vulnerabilities previously missed.

1

|,Data
p343-bichsel.pdf,|
This work presents a new approach for deobfuscating An-
droid APKs based on probabilistic learning of large code
bases (termed Big Code). The key idea is to learn a prob-
abilistic model over thousands of non-obfuscated Android
applications and to use this probabilistic model to deob-
fuscate new, unseen Android APKs. The concrete focus
of the paper is on reversing layout obfuscation, a popular
transformation which renames key program elements such
as classes, packages and methods, thus making it difficult to
understand what the program does.

Concretely, the paper: (i) phrases the layout deobfusca-
tion problem of Android APKs as structured prediction in
a probabilistic graphical model, (ii) instantiates this model
with a rich set of features and constraints that capture the
Android setting, ensuring both semantic equivalence and
high prediction accuracy, and (iii) shows how to leverage
powerful inference and learning algorithms to achieve over-
all precision and scalability of the probabilistic predictions.
We implemented our approach in a tool called DeGuard
and used it to: (i) reverse the layout obfuscation performed
by the popular ProGuard system on benign, open-source ap-
plications, (ii) predict third-party libraries imported by be-
nign APKs (also obfuscated by ProGuard), and (iii) rename
obfuscated program elements of Android malware. The ex-
perimental results indicate that DeGuard is practically ef-
fective:
it recovers 79.1% of the program element names
obfuscated with ProGuard, it predicts third-party libraries
with accuracy of 91.3%, and it reveals string decoders and
classes that handle sensitive data in Android malware.

1.

|,Data
p1580-mao.pdf,|
Multi-User MIMO has attracted much attention due to its
significant advantage of increasing the utilization ratio of
wireless channels. Recently a serious eavesdropping attack,
which exploits the CSI feedback of the FDD system, is dis-
covered in MU-MIMO networks.
In this paper, we firstly
show a similar eavesdropping attack for the TDD system is
also possible by proposing a novel, feasible attack approach.
Following it, a malicious user can eavesdrop on other users
downloads by transforming training sequences. To prevent
this attack, we propose a secure CSI estimation scheme for
instantaneous CSI. Furthermore, we extend this scheme to
achieve adaptive security when CSI is relatively statistical.
We have implemented our scheme for both uplink and down-
link of MU-MIMO and performed a series of experiments.
Results show that our secure CSI estimation scheme is highly
effective in preventing downlink leakage against malicious
users.

CCS Concepts
Security and privacy  Security protocols; Mobile
and wireless security; Networks  Mobile networks;

Keywords
Multi-User MIMO, Channel State Information, Eavesdrop-
ping, Physical Security

This work was supported in part by the Jiangsu Province

Double Innovation Talent Program and in part by the Na-
tional Natural Science Foundation of China under Grant
NSFC-61300235, Grant NSFC-61321491, Grant NSFC-
61402223, and Grant NSFC-61425024. The corresponding
author is Sheng Zhong. All authors are with the State Key
Laboratory for Novel Software Technology, Nanjing Univer-
sity, Nanjing 210023, P.R.China, and with the Computer
Science and Technology Department, Nanjing University,
Nanjing 210023, P.R. China.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS 16, October 2428, 2016, Vienna, Austria.
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978412

1.

|,Non-data
sec14-paper-davi.pdf,|
Return-oriented programming (ROP) offers a robust at-
tack technique that has, not surprisingly, been exten-
sively used to exploit bugs in modern software programs
(e.g., web browsers and PDF readers). ROP attacks re-
quire no code injection, and have already been shown
to be powerful enough to bypass fine-grained memory
randomization (ASLR) defenses. To counter this in-
genious attack strategy, several proposals for enforce-
ment of (coarse-grained) control-flow integrity (CFI)
have emerged. The key argument put forth by these
works is that coarse-grained CFI policies are sufficient to
prevent ROP attacks. As this reasoning has gained trac-
tion, ideas put forth in these proposals have even been
incorporated into coarse-grained CFI defenses in widely
adopted tools (e.g., Microsofts EMET framework).

In this paper, we provide the first comprehensive
security analysis of various CFI solutions (covering
kBouncer, ROPecker, CFI for COTS binaries, ROP-
Guard, and Microsoft EMET 4.1). A key contribution
is in demonstrating that these techniques can be effec-
tively undermined, even under weak adversarial assump-
tions. More specifically, we show that with bare mini-
mum assumptions, turing-complete and real-world ROP
attacks can still be launched even when the strictest of
enforcement policies is in use. To do so, we intro-
duce several new ROP attack primitives, and demonstrate
the practicality of our approach by transforming existing
real-world exploits into more stealthy attacks that bypass
coarse-grained CFI defenses.
1
Today, runtime attacks remain one of the most prevalent
attack vectors against software programs. The continued
success of these attacks can be attributed to the fact that
large portions of software programs are implemented in
type-unsafe languages (C, C++, or Objective-C) that do
not enforce bounds checking on data inputs. Moreover,
even type-safe languages (e.g., Java) rely on interpreters

|,Non-data
p116-barthe.pdf,|
Differential power analysis (DPA) is a side-channel attack in which
an adversary retrieves cryptographic material by measuring and
analyzing the power consumption of the device on which the crypto-
graphic algorithm under attack executes. An effective countermea-
sure against DPA is to mask secrets by probabilistically encoding
them over a set of shares, and to run masked algorithms that com-
pute on these encodings. Masked algorithms are often expected to
provide, at least, a certain level of probing security.

Leveraging the deep connections between probabilistic infor-
mation flow and probing security, we develop a precise, scalable,
and fully automated methodology to verify the probing security of
masked algorithms, and generate them from unprotected descrip-
tions of the algorithm. Our methodology relies on several contribu-
tions of independent interest, including a stronger notion of probing
security that supports compositional reasoning, and a type system
for enforcing an expressive class of probing policies. Finally, we
validate our methodology on examples that go significantly beyond
the state-of-the-art.

1.

|,Non-data
sec14-paper-ben-sasson.pdf,|

Cryptography offers a powerful tool to address these se-
curity concerns: zero-knowledge proofs [43]. The server,
acting as the prover, attempts to convince the client, act-
ing as the verifier, that the following NP statement is true:
there exists w such that z = F(x,w). Indeed:
 The soundness property of the proof system guarantees
that, if the NP statement is false, the prover cannot
convince the verifier (with high probability). Thus,
soundness addresses the clients integrity concern.

 The zero-knowledge property of the proof system guar-
antees that, if the NP statement is true, the prover can
convince the verifier without leaking any information
about w (beyond was is leaked by the output z). Thus,
zero knowledge addresses the servers confidentiality.
Moreover, the client sometimes not only seeks soundness
but also proof of knowledge [43, 11], which guarantees
that, whenever he is convinced, not only can he deduce
that a witness w exists, but also that the server knows one
such witness. This stronger property is often necessary to
security if F encodes cryptographic computations, and is
satisfied by most zero-knowledge proof systems.

Efficiency. Besides the aforementioned security desider-
ata, many settings also call for efficiency desiderata. The
client may be either unable or unwilling to engage in
lengthy interactions with the server, or to perform large
computations beyond the bare minimum of sending the
input x and receiving the output z. For instance, the client
may be a computationally-weak device with intermittent
connectivity (e.g., a smartphone).

Thus, it is desirable for the proof to be non-interactive
[25, 55, 23]: the server just send the claimed output  z,
along with a non-interactive proof string  that attests
that  z is the correct output. Moreover, it is also desirable
for the proof to be succinct:  has size O (1) and can be
verified in time O ( F  + x  + z ), where O () is some
polynomial in a security parameter  ; in other words,  is
very short and easy to verify (i.e., verification time does
not depend on  w , nor Fs running time).

We build a system that provides succinct non-interactive
zero-knowledge proofs (zk-SNARKs) for program execu-
tions on a von Neumann RISC architecture. The system
has two components: a cryptographic proof system for
verifying satisfiability of arithmetic circuits, and a circuit
generator to translate program executions to such circuits.
Our design of both components improves in functionality
and efficiency over prior work, as follows.

Our circuit generator is the first to be universal:

it
does not need to know the program, but only a bound
on its running time. Moreover, the size of the output
circuit depends additively (rather than multiplicatively)
on program size, allowing verification of larger programs.
The cryptographic proof system improves proving and
verification times, by leveraging new algorithms and a
pairing library tailored to the protocol.

|,Non-data
p1492-somorovsky.pdf,|
We present TLS-Attacker, an open source framework for
evaluating the security of TLS libraries. TLS-Attacker al-
lows security engineers to create custom TLS message flows
and arbitrarily modify message contents using a simple in-
terface in order to test the behavior of their libraries.

Based on TLS-Attacker, we present a two-stage fuzzing
approach to evaluate TLS server behavior. Our approach
automatically searches for cryptographic failures and bound-
ary violation vulnerabilities. It allowed us to find unusual
padding oracle vulnerabilities and overflows/overreads in
widely used TLS libraries, including OpenSSL, Botan, and
MatrixSSL.

Our findings motivate developers to create comprehensive
test suites, including positive as well as negative tests, for the
evaluation of TLS libraries. We use TLS-Attacker to create
such a test suite framework which finds further problems in
Botan.

1.

|,Non-data
p331-sun.pdf,|
Mobile operating systems like Android failed to provide suf-
ficient protection on personal data, and privacy leakage be-
comes a major concern. To understand the security risks
and privacy leakage, analysts have to carry out data-flow
analysis. In 2014, Android upgraded with a fundamentally
new design known as Android RunTime (ART) environ-
ment in Android 5.0. ART adopts ahead-of-time compi-
lation strategy and replaces previous virtual-machine-based
Dalvik. Unfortunately, many data-flow analysis systems like
TaintDroid [19] were designed for the legacy Dalvik environ-
ment. This makes data-flow analysis of new apps and mal-
ware infeasible. We design a multi-level information-flow
tracking system for the new Android system called Taint-
ART. TaintART employs a multi-level taint analysis tech-
nique to minimize the taint tag storage. Therefore, taint
tags can be stored in processor registers to provide efficient
taint propagation operations. We also customize the ART
compiler to maximize performance gains of the ahead-of-
time compilation optimizations. Based on the general de-
sign of TaintART, we also implement a multi-level privacy
enforcement to prevent sensitive data leakage. We demon-
strate that TaintART only incurs less than 15 % overheads
on a CPU-bound microbenchmark and negligible overhead
on built-in or third-party applications. Compared to legacy
Dalvik environment in Android 4.4, TaintART achieves
about 99.7 % faster performance for Java runtime bench-
mark.

1.

|,Data
sec14-paper-wustrow.pdf,|

In response to increasingly sophisticated state-sponsored
Internet censorship, recent work has proposed a new ap-
proach to censorship resistance: end-to-middle proxying.
This concept, developed in systems such as Telex, Decoy
Routing, and Cirripede, moves anticensorship technology
into the core of the network, at large ISPs outside the
censoring country. In this paper, we focus on two techni-
cal obstacles to the deployment of certain end-to-middle
schemes: the need to selectively block flows and the need
to observe both directions of a connection. We propose a
new construction, TapDance, that removes these require-
ments. TapDance employs a novel TCP-level technique
that allows the anticensorship station at an ISP to function
as a passive network tap, without an inline blocking com-
ponent. We also apply a novel steganographic encoding
to embed control messages in TLS ciphertext, allowing us
to operate on HTTPS connections even under asymmetric
routing. We implement and evaluate a TapDance proto-
type that demonstrates how the system could function
with minimal impact on an ISPs network operations.

|,Non-data
p1242-wang.pdf,|
While trawling online/offline password guessing has been inten-
sively studied, only a few studies have examined targeted online
guessing, where an attacker guesses a specific victims password
for a service, by exploiting the victims personal information such
as one sister password leaked from her another account and some
personally identifiable information (PII). A key challenge for tar-
geted online guessing is to choose the most effective password can-
didates, while the number of guess attempts allowed by a servers
lockout or throttling mechanisms is typically very small.

We propose TarGuess, a framework that systematically charac-
terizes typical targeted guessing scenarios with seven sound math-
ematical models, each of which is based on varied kinds of data
available to an attacker. These models allow us to design novel and
efficient guessing algorithms. Extensive experiments on 10 large
real-world password datasets show the effectiveness of TarGuess.
Particularly, TarGuess I(cid:24)IV capture the four most representative
scenarios and within 100 guesses: (1) TarGuess-I outperforms its
foremost counterpart by 142% against security-savvy users and by
46% against normal users; (2) TarGuess-II outperforms its fore-
most counterpart by 169% on security-savvy users and by 72%
against normal users; and (3) Both TarGuess-III and IV gain suc-
cess rates over 73% against normal users and over 32% against
security-savvy users. TarGuess-III and IV, for the first time, address
the issue of cross-site online guessing when given the victims one
sister password and some PII.
Keywords
Password authentication; Targeted online guessing; Personal infor-
mation; Password reuse; Probabilistic model.
1.

|,Data
sec14-paper-hardy.pdf,|
Targeted attacks on civil society and non-governmental
organizations have gone underreported despite the fact
that these organizations have been shown to be frequent
targets of these attacks. In this paper, we shed light on
targeted malware attacks faced by these organizations by
studying malicious e-mails received by 10 civil society
organizations (the majority of which are from groups re-
lated to China and Tibet issues) over a period of 4 years.
Our study highlights important properties of malware
threats faced by these organizations with implications on
how these organizations defend themselves and how we
quantify these threats. We find that the technical sophis-
tication of malware we observe is fairly low, with more
effort placed on socially engineering the e-mail con-
tent. Based on this observation, we develop the Targeted
Threat Index (TTI), a metric which incorporates both so-
cial engineering and technical sophistication when as-
sessing the risk of malware threats. We demonstrate that
this metric is more effective than simple technical sophis-
tication for identifying malware threats with the high-
est potential to successfully compromise victims. We
also discuss how education efforts focused on changing
user behaviour can help prevent compromise. For two
of the three Tibetan groups in our study simple steps
such as avoiding the use of email attachments could
cut document-based malware threats delivered through
e-mail that we observed by up to 95%.

1

|,Data
sec14-paper-komanduri.pdf,|
To discourage the creation of predictable passwords, vul-
nerable to guessing attacks, we present Telepathwords.
As a user creates a password, Telepathwords makes real-
time predictions for the next character that user will type.
While the concept is simple, making accurate predictions
requires efficient algorithms to model users behavior
and to employ already-typed characters to predict subse-
quent ones. We first made the Telepathwords technology
available to the public in late 2013 and have since served
hundreds of thousands of user sessions.

We ran a human-subjects experiment to compare pass-
word policies that use Telepathwords to those that rely
on composition rules, comparing participants passwords
using two different password-evaluation algorithms. We
found that participants create far fewer weak passwords
using the Telepathwords-based policies than policies
based only on character composition. Participants using
Telepathwords were also more likely to report that the
password feedback was helpful.

|,Data
p791-jia.pdf,|
Process-based isolation, suggested by several research prototypes,
is a cornerstone of modern browser security architectures. Google
Chrome is the first commercial browser that adopts this architec-
ture. Unlike several research prototypes, Chromes process-based
design does not isolate different web origins, but primarily promises
to protect the local system from the web. However, as bil-
lions of users now use web-based cloud services (e.g., Dropbox
and Google Drive), which are integrated into the local system, the
premise that browsers can effectively isolate the web from the local
system has become questionable. In this paper, we argue that, if the
process-based isolation disregards the same-origin policy as one of
its goals, then its promise of maintaining the web/local system
(local) separation is doubtful. Specifically, we show that exist-
ing memory vulnerabilities in Chromes renderer can be used as a
stepping-stone to drop executables/scripts in the local file system,
install unwanted applications and misuse system sensors. These at-
tacks are purely data-oriented and do not alter any control flow or
import foreign code. Thus, such attacks bypass binary-level pro-
tection mechanisms, including ASLR and in-memory partitioning.
Finally, we discuss various full defenses and present a possible way
to mitigate the attacks presented.

1.

|,Data
06547106.pdf,|We present the Crossfire attack  a powerful
attack that degrades and often cuts off network connections to a
variety of selected server targets (e.g., servers of an enterprise,
a city, a state, or a small country) by flooding only a few
network links. In Crossfire, a small set of bots directs low-
intensity flows to a large number of publicly accessible servers.
The concentration of these flows on the small set of carefully
chosen links floods these links and effectively disconnects
selected target servers from the Internet. The sources of the
Crossfire attack are undetectable by any targeted servers, since
they no longer receive any messages, and by network routers,
since they receive only low-intensity, individual flows that are
indistinguishable from legitimate flows. The attack persistence
can be extended virtually indefinitely by changing the set of
bots, publicly accessible servers, and target links while main-
taining the same disconnection targets. We demonstrate the
attack feasibility using Internet experiments, show its effects
on a variety of chosen targets (e.g., servers of universities, US
states, East and West Coasts of the US), and explore several
countermeasures.

I. |,Non-data
sec14-paper-li-zhiwei.pdf,|

We conduct a security analysis of five popular web-based
password managers. Unlike local password managers,
web-based password managers run in the browser. We
identify four key security concerns for web-based pass-
word managers and, for each, identify representative vul-
nerabilities through our case studies. Our attacks are se-
vere: in four out of the five password managers we stud-
ied, an attacker can learn a users credentials for arbi-
trary websites. We find vulnerabilities in diverse features
like one-time passwords, bookmarklets, and shared pass-
words. The root-causes of the vulnerabilities are also di-
verse: ranging from logic and authorization mistakes to
misunderstandings about the web security model, in ad-
dition to the typical vulnerabilities like CSRF and XSS.
Our study suggests that it remains to be a challenge for
the password managers to be secure. To guide future de-
velopment of password managers, we provide guidance
for password managers. Given the diversity of vulner-
abilities we identified, we advocate a defense-in-depth
approach to ensure security of password managers.
1 |,Non-data
p31-miller.pdf,|
The surprising success of cryptocurrencies has led to a surge of inter-
est in deploying large scale, highly robust, Byzantine fault tolerant
(BFT) protocols for mission-critical applications, such as financial
transactions. Although the conventional wisdom is to build atop a
(weakly) synchronous protocol such as PBFT (or a variation thereof),
such protocols rely critically on network timing assumptions, and
only guarantee liveness when the network behaves as expected. We
argue these protocols are ill-suited for this deployment scenario.

We present an alternative, HoneyBadgerBFT, the first practical
asynchronous BFT protocol, which guarantees liveness without mak-
ing any timing assumptions. We base our solution on a novel atomic
broadcast protocol that achieves optimal asymptotic efficiency. We
present an implementation and experimental results to show our
system can achieve throughput of tens of thousands of transactions
per second, and scales to over a hundred nodes on a wide area net-
work. We even conduct BFT experiments over Tor, without needing
to tune any parameters. Unlike the alternatives, HoneyBadgerBFT
simply does not care about the underlying network.

1.

|,Non-data
2005-bc-ndss.pdf,|

As national

infrastructure becomes intertwined with
emerging global data networks, the stability and integrity of
the two have become synonymous. This connection, while
necessary, leaves network assets vulnerable to the rapidly
moving threats of todays Internet, including fast moving
worms, distributed denial of service attacks, and routing
exploits. This paper introduces the Internet Motion Sen-
sor (IMS), a globally scoped Internet monitoring system
whose goal is to measure, characterize, and track threats.
The IMS architecture is based on three novel components.
First, a Distributed Monitoring Infrastructure increases vis-
ibility into global threats. Second, a Lightweight Active
Responder provides enough interactivity that traffic on the
same service can be differentiated independent of applica-
tion semantics. Third, a Payload Signatures and Caching
mechanism avoids recording duplicated payloads, reducing
overhead and assisting in identifying new and unique pay-
loads. We explore the architectural tradeoffs of this system
in the context of a 3 year deployment across multiple dark
address blocks ranging in size from /24s to a /8. These sen-
sors represent a range of organizations and a diverse sam-
ple of the routable IPv4 space including nine of all routable
/8 address ranges. Data gathered from these deployments
is used to demonstrate the ability of the IMS to capture and
characterize several important Internet threats: the Blaster
worm (August 2003), the Bagle backdoor scanning efforts
(March 2004), and the SCO Denial of Service attacks (De-
cember 2003).

1

|,Data
sec14-paper-szurdi.pdf,|
Typosquatting is a speculative behavior that leverages
Internet naming and governance practices to extract profit
from users misspellings and typing errors. Simple and
inexpensive domain registration motivates speculators
to register domain names in bulk to profit from display
advertisements, to redirect traffic to third party pages,
to deploy phishing sites, or to serve malware. While
previous research has focused on typosquatting domains
which target popular websites, speculators also appear
to be typosquatting on the long tail of the popularity
distribution: millions of registered domain names appear
to be potential typos of other site names, and only 6.8%
target the 10,000 most popular .com domains.

Investigating the entire distribution can give a more
complete understanding of the typosquatting phenomenon.
In this paper, we perform a comprehensive study of ty-
posquatting domain registrations within the .com TLD.
Our methodology helps us to significantly improve upon
existing solutions in identifying typosquatting domains
and their monetization strategies, especially for less pop-
ular targets. We find that about half of the possible typo
domains identified by lexical analysis are truly typo do-
mains. From our zone file analysis, we estimate that 20%
of the total number of .com domain registrations are true
typo domains and their number is increasing with the ex-
pansion of the .com domain space. This large number of
typo registrations motivates us to review intervention at-
tempts and implement efficient user-side mitigation tools
to diminish the financial benefit of typosquatting to mis-
creants.

1

|,Data
malicia_dataset.pdf,| Drive-by downloads are the preferred distrib-
ution vector for many malware families. In the drive-by
ecosystem, many exploit servers run the same exploit kit and
it is a challenge understanding whether the exploit server
is part of a larger operation. In this paper, we propose a
technique to identify exploit servers managed by the same
organization. We collect over time how exploit servers are
configured, which exploits they use, and what malware they
distribute, grouping servers with similar configurations into
operations. Our operational analysis reveals that although
individual exploit servers have a median lifetime of 16 h,
long-lived operations exist that operate for several months.
To sustain long-lived operations, miscreants are turning to
the cloud, with 60 % of the exploit servers hosted by special-
ized cloud hosting services. We also observe operations that
distribute multiple malware families and that pay-per-install
affiliate programs are managing exploit servers for their affil-
iates to convert traffic into installations. Furthermore, we
analyze the exploit polymorphism problem, measuring the
repacking rate for different exploit types. To understand how
difficult is to takedown exploit servers, we analyze the abuse
reporting process and issue abuse reports for 19 long-lived
servers. We describe the interaction with ISPs and hosting
providers and monitor the result of the report. We find that
A. Nappa (B)  J. Caballero

IMDEA Software Institute, Madrid, Spain
e-mail: antonio.nappa@imdea.org

J. Caballero
e-mail: juan.caballero@imdea.org

A. Nappa
Universidad Politecnica de Madrid, Madrid, Spain

M. Z. Rafique
iMinds-DistriNet, KU Leuven, Leuven, Belgium
e-mail: zubair.rafique@cs.kuleuven.be

61 % of the reports are not even acknowledged. On aver-
age, an exploit server still lives for 4.3 days after a report.
Finally, we detail the Malicia dataset we have collected and
are making available to other researchers.
Keywords Drive-by download operations  Malicia
dataset  Malware distribution  Cybercrime

1 |,Data
p80-shao.pdf,|
In this work, we conduct the first systematic study in un-
derstanding the security properties of the usage of Unix do-
main sockets by both Android apps and system daemons
as an IPC (Inter-process Communication) mechanism, espe-
cially for cross-layer communications between the Java and
native layers. We propose a tool called SInspector to ex-
pose potential security vulnerabilities in using Unix domain
sockets through the process of identifying socket addresses,
detecting authentication checks, and performing data flow
analysis. Our in-depth analysis revealed some serious vul-
nerabilities in popular apps and system daemons, such as
root privilege escalation and arbitrary file access. Based
on our findings, we propose countermeasures and improved
practices for utilizing Unix domain sockets on Android.

1.

|,Data
06547102.pdf,|In response to the growing popularity of Tor
and other censorship circumvention systems, censors in non-
democratic countries have increased their technical capabilities
and can now recognize and block network traffic generated by
these systems on a nationwide scale. New censorship-resistant
communication systems such as SkypeMorph, StegoTorus, and
CensorSpoofer aim to evade censors observations by imitating
common protocols like Skype and HTTP.

We demonstrate that these systems completely fail to achieve
unobservability. Even a very weak,
local censor can easily
distinguish their traffic from the imitated protocols. We show
dozens of passive and active methods that recognize even a
single imitated session, without any need to correlate multiple
network flows or perform sophisticated traffic analysis.

We enumerate the requirements that a censorship-resistant
system must satisfy to successfully mimic another protocol and
conclude that unobservability by imitation is a fundamentally
flawed approach. We then present our recommendations for the
design of unobservable communication systems.

Keywords-Censorship circumvention; unobservable commu-

nications; Tor pluggable transports

I. |,Non-data
p283-juels.pdf,|
Thanks to their anonymity (pseudonymity) and elimina-
tion of trusted intermediaries, cryptocurrencies such as Bit-
coin have created or stimulated growth in many businesses
and communities. Unfortunately, some of these are crim-
inal, e.g., money laundering, illicit marketplaces, and ran-
somware.

Next-generation cryptocurrencies such as Ethereum will
include rich scripting languages in support of smart con-
tracts, programs that autonomously intermediate transac-
tions. In this paper, we explore the risk of smart contracts
fueling new criminal ecosystems. Specifically, we show how
what we call criminal smart contracts (CSCs) can facilitate
leakage of confidential information, theft of cryptographic
keys, and various real-world crimes (murder, arson, terror-
ism).

We show that CSCs for leakage of secrets (`a la Wikileaks)
are efficiently realizable in existing scripting languages such
as that in Ethereum. We show that CSCs for theft of crypto-
graphic keys can be achieved using primitives, such as Suc-
cinct Non-interactive ARguments of Knowledge (SNARKs),
that are already expressible in these languages and for which
efficient supporting language extensions are anticipated. We
show similarly that authenticated data feeds, an emerging
feature of smart contract systems, can facilitate CSCs for
real-world crimes (e.g., property crimes).

Our results highlight the urgency of creating policy and
technical safeguards against CSCs in order to realize the
promise of smart contracts for beneficial goals.

Keywords
Criminal Smart Contracts; Ethereum

The Ring of Gyges is a mythical magical artifact men-
tioned by the philosopher Plato in Book 2 of his Republic.
It granted its owner the power to become invisible at will.
Wikipedia, Ring of Gyges

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978362

[On wearing the ring,] no man would keep his hands off
what was not his own when he could safely take what he liked
out of the market, or go into houses and lie with anyone at
his pleasure, or kill or release from prison whom he would...
 Plato, The Republic, Book 2 (2.360b) (trans. Benjamin
Jowett)

1.

|,Non-data
p1341-pouliot.pdf,|
Encrypting Internet communications has been the subject
of renewed focus in recent years. In order to add end-to-end
encryption to legacy applications without losing the conve-
nience of full-text search, ShadowCrypt and Mimesis Aegis
use a new cryptographic technique called efficiently deploy-
able efficiently searchable encryption (EDESE) that allows
a standard full-text search system to perform searches on
encrypted data. Compared to other recent techniques for
searching on encrypted data, EDESE schemes leak a great
deal of statistical information about the encrypted messages
and the keywords they contain. Until now, the practical
impact of this leakage has been difficult to quantify.

In this paper, we show that the adversarys task of match-
ing plaintext keywords to the opaque cryptographic identi-
fiers used in EDESE can be reduced to the well-known com-
binatorial optimization problem of weighted graph match-
ing (WGM). Using real email and chat data, we show how
off-the-shelf WGM solvers can be used to accurately and
efficiently recover hundreds of the most common plaintext
keywords from a set of EDESE-encrypted messages. We
show how to recover the tags from Bloom filters so that the
WGM solver can be used with the set of encrypted mes-
sages that utilizes a Bloom filter to encode its search tags.
We also show that the attack can be mitigated by carefully
configuring Bloom filter parameters.

Keywords
Security; Efficiently Deployable Efficiently Searchable En-
cryption; Encrypted Email

1.

|,Data
p908-shrestha.pdf,|
Reducing user burden underlying traditional two-factor authentica-
tion constitutes an important research effort. An interesting repre-
sentative approach, Sound-Proof, leverages ambient sounds to de-
tect the proximity between the second factor device (phone) and
the login terminal (browser). Sound-Proof was shown to be secure
against remote attackers and highly usable, and is now under early
deployment phases.

In this paper, we identify a weakness of the Sound-Proof sys-
tem, namely, the remote attacker does not have to predict the am-
bient sounds near the phone as assumed in the Sound-Proof paper,
but rather can deliberately makeor wait forthe phone to pro-
duce predictable or previously known sounds (e.g., ringer, notifica-
tion or alarm sounds). Exploiting this weakness, we build Sound-
Danger, a full attack system that can successfully compromise the
security of Sound-Proof. The attack involves buzzing the victim
users phone, or waiting for the phone to buzz, and feeding the cor-
responding sounds at the browser to login on behalf of the user.
The attack works precisely under Sound-Proofs threat model.

Our contributions are three-fold. First, we design and develop
the Sound-Danger attack system that exploits a wide range of a
smartphones functionality to break Sound-Proof, such as by ac-
tively making a phone or VoIP call, sending an SMS and creating
an app-based notification, or by passively waiting for the phone to
trigger an alarm. Second, we re-implement Sound-Proofs audio
correlation algorithm and evaluate it against Sound-Danger under
a large variety of attack settings. Our results show that many of our
attacks succeed with a 100% chance such that the Sound-Proof cor-
relation algorithm will accept the attacked audio samples as valid.
Third, we collect general population statistics via an online sur-
vey to determine the phone usage habits relevant to our attacks.
We then use these statistics to show how our different correlation-
based attacks can be carefully executed to, for instance, compro-
mise about 57% user accounts in just the first attempt and about
83% user accounts in less than a day. Finally, we provide some
mitigation strategies and future directions that may help overcome
some of our attacks and strengthen Sound-Proof.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978328

Maliheh Shirvanian

University of Alabama at Birmingham

maliheh@uab.edu

Nitesh Saxena

University of Alabama at Birmingham

saxena@cis.uab.edu

1.

|,Data
sec15-paper-oltrogge.pdf,|
For increased security during TLS certificate valida-
tion, a common recommendation is to use a vari-
ation of pinning. Especially non-browser software
developers are encouraged to limit the number of
trusted certificates to a minimum, since the default
CA-based approach is known to be vulnerable to se-
rious security threats.
The decision for or against pinning is always a trade-
off between increasing security and keeping mainte-
nance efforts at an acceptable level. In this paper,
we present an extensive study on the applicability
of pinning for non-browser software by analyzing
639,283 Android apps. Conservatively, we propose
pinning as an appropriate strategy for 11,547 (1.8%)
apps or for 45,247 TLS connections (4.25%) in our
sample set. With a more optimistic classification of
borderline cases, we propose pinning for considera-
tion for 58,817 (9.1%) apps or for 140,020 (3.8%1)
TLS connections. This weakens the assumption that
pinning is a widely usable strategy for TLS security
in non-browser software. However, in a nominal-
actual comparison, we find that only 45 apps ac-
tually implement pinning. We collected developer
feedback from 45 respondents and learned that only
a quarter of them grasp the concept of pinning, but
still find pinning too complex to use. Based on their
feedback, we built an easy-to-use web-application
that supports developers in the decision process and
guides them through the correct deployment of a
pinning-protected TLS implementation.

1 |,Data
1606.05047.pdf,|
There are currently no requirements (technical or otherwise)
that BGP paths must be contained within national bound-
aries. Indeed, some paths experience international detours,
i.e., originate in one country, cross international boundaries
and return to the same country. In most cases these are sensi-
ble traffic engineering or peering decisions at ISPs that serve
multiple countries. In some cases such detours may be sus-
picious. Characterizing international detours is useful to a
number of players: (a) network engineers trying to diagnose
persistent problems, (b) policy makers aiming at adhering to
certain national communication policies, (c) entrepreneurs
looking for opportunities to deploy new networks, or (d)
privacy-conscious states trying to minimize the amount of
internal communication traversing different jurisdictions.

In this paper we characterize international detours in the
Internet during the month of January 2016. To detect detours
we sample BGP RIBs every 8 hours from 461 RouteViews
and RIPE RIS peers spanning 30 countries. Then geolocate
visible ASes by geolocating each BGP prefix announced by
each AS, mapping its presence at IXPs and geolocation in-
frastructure IPs. Finally, analyze each global BGP RIB en-
try looking for detours. Our analysis shows more than 5K
unique BGP prefixes experienced a detour. A few ASes
cause most detours and a small fraction of prefixes were af-
fected the most. We observe about 544K detours. Detours
either last for a few days or persist the entire month. Out of
all the detours, more than 90% were transient detours that
lasted for 72 hours or less. We also show different countries
experience different characteristics of detours.

Keywords
AS Geolocation, Routing Detours, MITM
1.

|,Data
sec14-paper-viswanath.pdf,|

Users increasingly rely on crowdsourced information,
such as reviews on Yelp and Amazon, and liked posts
and ads on Facebook. This has led to a market for black-
hat promotion techniques via fake (e.g., Sybil) and com-
promised accounts, and collusion networks. Existing ap-
proaches to detect such behavior relies mostly on super-
vised (or semi-supervised) learning over known (or hy-
pothesized) attacks. They are unable to detect attacks
missed by the operator while labeling, or when the at-
tacker changes strategy.

We propose using unsupervised anomaly detection
techniques over user behavior to distinguish potentially
bad behavior from normal behavior. We present a tech-
nique based on Principal Component Analysis (PCA)
that models the behavior of normal users accurately and
identifies significant deviations from it as anomalous. We
experimentally validate that normal user behavior (e.g.,
categories of Facebook pages liked by a user, rate of like
activity, etc.) is contained within a low-dimensional sub-
space amenable to the PCA technique. We demonstrate
the practicality and effectiveness of our approach using
extensive ground-truth data from Facebook: we success-
fully detect diverse attacker strategiesfake, compro-
mised, and colluding Facebook identitieswith no a pri-
ori labeling while maintaining low false-positive rates.
Finally, we apply our approach to detect click-spam in
Facebook ads and find that a surprisingly large fraction
of clicks are from anomalous users.

1 |,Data
06547100.pdf,|This paper describes our experience of performing
reactive security audit of known security vulnerabilities in core op-
erating system and browser COM components, using an extended
static checker HAVOC-LITE. We describe the extensions made
to the tool to be applicable on such large C++ components, along
with our experience of using an extended static checker in the
large. We argue that the use of such checkers as a configurable
static analysis in the hands of security auditors can be an effective
tool for finding variations of known vulnerabilities. The effort has
led to finding and fixing around 70 previously unknown security
vulnerabilities in over 10 millions lines operating system and
browser code.

Keywords-security audit; program verification; static analysis;

extended static checkers

I. |,Data
sec14-paper-bonneau.pdf,|
Challenging the conventional wisdom that users cannot
remember cryptographically-strong secrets, we test the
hypothesis that users can learn randomly-assigned 56-
bit codes (encoded as either 6 words or 12 characters)
through spaced repetition. We asked remote research
participants to perform a distractor task that required log-
ging into a website 90 times, over up to two weeks, with
a password of their choosing. After they entered their
chosen password correctly we displayed a short code (4
letters or 2 words, 18.8 bits) that we required them to
type. For subsequent logins we added an increasing de-
lay prior to displaying the code, which participants could
avoid by typing the code from memory. As participants
learned, we added two more codes to comprise a 56.4-
bit secret. Overall, 94% of participants eventually typed
their entire secret from memory, learning it after a me-
dian of 36 logins. The learning component of our system
added a median delay of just 6.9 s per login and a to-
tal of less than 12 minutes over an average of ten days.
88% were able to recall their codes exactly when asked
at least three days later, with only 21% reporting having
written their secret down. As one participant wrote with
surprise, the words are branded into my brain.

1

|,Non-data
p270-zhang.pdf,|
Smart contracts are programs that execute autonomously
on blockchains. Their key envisioned uses (e.g. financial
instruments) require them to consume data from outside the
blockchain (e.g. stock quotes). Trustworthy data feeds that
support a broad range of data requests will thus be critical
to smart contract ecosystems.

We present an authenticated data feed system called Town
Crier (TC). TC acts as a bridge between smart contracts and
existing web sites, which are already commonly trusted for
non-blockchain applications. It combines a blockchain front
end with a trusted hardware back end to scrape HTTPS-
enabled websites and serve source-authenticated data to re-
lying smart contracts.

TC also supports confidentiality. It enables private data
requests with encrypted parameters. Additionally, in a gen-
eralization that executes smart-contract logic within TC,
the system permits secure use of user credentials to scrape
access-controlled online data sources.

We describe TCs design principles and architecture and
report on an implementation that uses Intels recently in-
troduced Software Guard Extensions (SGX) to furnish data
to the Ethereum smart contract system. We formally model
TC and define and prove its basic security properties in the
Universal Composibility (UC) framework. Our results in-
clude definitions and techniques of general interest relating
to resource consumption (Ethereums gas fee system) and
TCB minimization. We also report on experiments with
three example applications.

We plan to launch TC soon as an online public service.

Keywords: Authenticated Data Feeds; Smart Contracts;
Trusted Hardware; Intel SGX; Ethereum; Bitcoin

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978326

1.

|,Non-data
p168-chase.pdf,|
In this paper, we initiate a formal study of transparency,
which in recent years has become an increasingly critical
requirement for the systems in which people place trust.
We present the abstract concept of a transparency overlay,
which can be used in conjunction with any system to give it
provable transparency guarantees, and then apply the over-
lay to two settings: Certificate Transparency and Bitcoin.
In the latter setting, we show that the usage of our trans-
parency overlay eliminates the need to engage in mining and
allows users to store a single small value rather than the
entire blockchain. Our transparency overlay is generically
constructed from a signature scheme and a new primitive
we call a dynamic list commitment, which in practice can
be instantiated using a collision-resistant hash function.

1.

|,Non-data
06547103.pdf,|Tor

is

popular

the most

volunteer-based
anonymity network consisting of over 3000 volunteer-operated
relays. Apart from making connections to servers hard to
trace to their origin it can also provide receiver privacy for
Internet services through a feature called hidden services.

In this paper we expose flaws both in the design and
implementation of Tors hidden services that allow an attacker
to measure the popularity of arbitrary hidden services, take
down hidden services and deanonymize hidden services. We
give a practical evaluation of our techniques by studying: (1) a
recent case of a botnet using Tor hidden services for command
and control channels; (2) Silk Road, a hidden service used to
sell drugs and other contraband; (3) the hidden service of the
DuckDuckGo search engine.

Keywords-Tor; anonymity network; privacy; hidden services

I. |,Data
sec14-paper-kosba.pdf,|

1

|,Non-data
sec15-paper-bates.pdf,|

In a provenance-aware system, mechanisms gather
and report metadata that describes the history of each ob-
ject being processed on the system, allowing users to un-
derstand how data objects came to exist in their present
state. However, while past work has demonstrated the
usefulness of provenance, less attention has been given
to securing provenance-aware systems. Provenance it-
self is a ripe attack vector, and its authenticity and in-
tegrity must be guaranteed before it can be put to use.

We present Linux Provenance Modules

(LPM),
the first general framework for the development of
provenance-aware systems. We demonstrate that LPM
creates a trusted provenance-aware execution environ-
ment, collecting complete whole-system provenance
while imposing as little as 2.7% performance overhead
on normal system operation. LPM introduces new mech-
anisms for secure provenance layering and authenticated
communication between provenance-aware hosts, and
also interoperates with existing mechanisms to provide
strong security assurances. To demonstrate the poten-
tial uses of LPM, we design a Provenance-Based Data
Loss Prevention (PB-DLP) system. We implement PB-
DLP as a file transfer application that blocks the trans-
mission of files derived from sensitive ancestors while
imposing just tens of milliseconds overhead. LPM is the
first step towards widespread deployment of trustworthy
provenance-aware applications.

1

|,Non-data
p541-wressnegger.pdf,|
Subtle flaws in integer computations are a prime source for
exploitable vulnerabilities in system code. Unfortunately,
even code shown to be secure on one platform can be vul-
nerable on another, making the migration of code a notable
security challenge. In this paper, we provide the first study
on how code that works as expected on 32-bit platforms can
become vulnerable on 64-bit platforms. To this end, we sys-
tematically review the effects of data model changes between
platforms. We find that the larger width of integer types
and the increased amount of addressable memory introduce
previously non-existent vulnerabilities that often lie dormant
in program code. We empirically evaluate the prevalence
of these flaws on the source code of Debian stable (Jessie)
and 200 popular open-source projects hosted on GitHub.
Moreover, we discuss 64-bit migration vulnerabilities that
have been discovered as part of our study, including vulnera-
bilities in Chromium, the Boost C++ Libraries, libarchive,
the Linux Kernel, and zlib.

Keywords
Software security; Data models; Integer-based vulnerabilities

1.

|,Data
p517-haller.pdf,|
The low-level C++ programming language is ubiquitously
used for its modularity and performance. Typecasting is
a fundamental concept in C++ (and object-oriented pro-
gramming in general) to convert a pointer from one object
type into another. However, downcasting (converting a base
class pointer to a derived class pointer) has critical security
implications due to potentially different object memory lay-
outs. Due to missing type safety in C++, a downcasted
pointer can violate a programmers intended pointer seman-
tics, allowing an attacker to corrupt the underlying memory
in a type-unsafe fashion. This vulnerability class is receiving
increasing attention and is known as type confusion (or bad-
casting). Several existing approaches detect different forms
of type confusion, but these solutions are severely limited
due to both high run-time performance overhead and low
detection coverage.

This paper presents TypeSan, a practical type-confusion
detector which provides both low run-time overhead and
high detection coverage. Despite improving the coverage
of state-of-the-art techniques, TypeSan significantly reduces
the type-confusion detection overhead compared to other
solutions. TypeSan relies on an efficient per-object meta-
data storage service based on a compact memory shadowing
scheme. Our scheme treats all the memory objects (i.e.,
globals, stack, heap) uniformly to eliminate extra checks on
the fast path and relies on a variable compression ratio to
minimize run-time performance and memory overhead. Our
experimental results confirm that TypeSan is practical, even
when explicitly checking almost all the relevant typecasts in
a given C++ program. Compared to the state of the art,
TypeSan yields orders of magnitude higher coverage at 4
10 times lower performance overhead on SPEC and 2 times
on Firefox. As a result, our solution offers superior protec-





Vrije Universiteit Amsterdam
Amsterdam Department of Informatics
Purdue University

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24 - 28, 2016, Vienna, Austria
 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978405

tion and is suitable for deployment in production software.
Moreover, our highly efficient metadata storage back-end is
potentially useful for other defenses that require memory
object tracking.

CCS Concepts
Security and privacy  Systems security; Software
and application security;

Keywords
Type safety; Typecasting; Type confusion; Downcasting

1.

|,Non-data
p635-wang.pdf,| 
With  the  progress  in  mobile  computing,  web  services  are 
increasingly delivered to their users through mobile apps, instead 
of  web  browsers.  However,  unlike  the  browser,  which  enforces 
origin-based security policies to mediate the interactions between 
the web content from different sources,  todays mobile OSes do 
not have a comparable security mechanism to control the cross-
origin communications between apps, as well as those between an 
app  and  the  web.  As  a  result,  a  mobile  users  sensitive  web 
resources could be exposed to the harms from a malicious origin.  
In this paper, we report the first systematic study on this mobile 
cross-origin  risk.  Our  study  inspects  the  main  cross-origin 
channels on Android and iOS, including intent, scheme and web-
accessing  utility  classes,  and  further  analyzes  the  ways  popular 
web services (e.g., Facebook, Dropbox, etc.) and their apps utilize 
those channels to serve other apps. The research shows that lack 
of origin-based protection opens the door to a wide spectrum of 
cross-origin attacks. These attacks are unique to mobile platforms, 
and their consequences are serious: for example, using carefully 
designed  techniques  for  mobile  cross-site  scripting  and  request 
forgery,  an  unauthorized  party  can  obtain  a  mobile  users 
Facebook/Dropbox authentication credentials and record her text 
input. We report our findings to related software vendors, who all 
acknowledged  their  importance.  To  address  this  threat,  we 
designed an origin-based protection mechanism, called Morbs, for 
mobile  OSes.  Morbs  labels  every  message  with  its  origin 
information,  lets  developers  easily  specify  security  policies, and 
enforce the policies on the mobile channels based on origins. Our 
evaluation demonstrates the effectiveness of our new technique in 
defeating  unauthorized  origin  crossing,  its  efficiency  and  the 
convenience for the developers to use such protection. 

Categories and Subject Descriptors 
D.4.6 [Operating Systems]: Security and Protection  access 
controls, invasive software 
Keywords: Android, iOS, same-origin policy, mobile 
platform. 

1.  |,Non-data
Cai09b.pdf,|

Although the Internet is widely used today, there are few
sound estimates of network demographics. Decentralized
network management means questions about Internet use
cannot be answered by a central authority, and firewalls and
sensitivity to probing means that active measurements must
be done carefully and validated against known data. Build-
ing on frequent ICMP probing of 1% of the Internet address
space, we develop a clustering algorithm to estimate how In-
ternet addresses are used. We show that adjacent addresses
often have similar characteristics and are used for similar
purposes (61% of addresses we probe are consistent blocks
of 64 neighbors or more). We then apply this block-level
clustering to provide data to explore several open questions
in how networks are managed. First, the nearing full allo-
cation of IPv4 addresses makes it increasingly important to
estimate the costs of better management of the IPv4 space
as a component of an IPv6 transition. We provide about
how effectively network addresses blocks appear to be used,
finding that a significant number of blocks are only lightly
used (about one-fifth of /24 blocks have most addresses in
use less than 10% of the time). Second, we provide new
measurements about dynamically managed address space,
showing nearly 40% of /24 blocks appear to be dynamically
allocated, and dynamic addressing is most widely used in
countries more recently to the Internet (more than 80% in
China, while less then 30% in the U.S.).

Categories and Subject Descriptors

C.2.1 [Computer-Communication Networks]: Net-
work Architecture and DesignNetwork topology; C.2.3
[Computer-Communication Networks]: Network
OperationsNetwork management

General Terms: Measurement

Keywords: Internet address allocation, survey, pat-
tern analysis, clustering, classification, availability, volatil-
ity

1.

|,Data
imc027-czyzA.pdf,|
We report the results of a study to collect and analyze IPv6 Internet
background radiation. This study, the largest of its kind, collects
unclaimed traffic on the IPv6 Internet by announcing five large /12
covering prefixes; these cover the majority of allocated IPv6 space
on todays Internet. Our analysis characterizes the nature of this
traffic across regions, over time, and by the allocation and routing
status of the intended destinations, which we show help to identify
the causes of this traffic. We compare results to unclaimed traffic in
IPv4, and highlight case studies that explain a large fraction of the
data or highlight notable properties. We describe how announced
covering prefixes differ from traditional network telescopes, and
show how this technique can help both network operators and the
research community identify additional potential issues and mis-
configurations in this critical Internet transition period.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Opera-
tionsNetwork management, Network monitoring

Keywords
IPv6; Darknet; Routing; Network Pollution; Internet Background
Radiation; Measurement

1.

|,Data
sec14-paper-alrwais.pdf,|
Domain parking is a booming business with millions of
dollars in revenues. However, it is also among the least
regulated: parked domains have been routinely found to
connect to illicit online activities even though the roles
they play there have never been clarified. In this paper,
we report the first systematic study on this dark side of
domain parking based upon a novel infiltration analysis
on domains hosted by major parking services. The idea
here is to control the traffic sources (crawlers) of the do-
main parking ecosystem, some of its start nodes (parked
domains) and its end nodes (advertisers and traffic buy-
ers) and then connect the dots, delivering our own traf-
fic to our end nodes across our own start nodes with other
monetization entities (parking services, ad networks, etc)
in-between. This provided us a unique observation of the
whole monetization process and over one thousand seed
redirection chains where some ends were under our con-
trol. From those chains, we were able to confirm the
presence of click fraud, traffic spam and traffic stealing.
To further understand the scope and magnitude of this
threat, we extracted a set of salient features from those
seed chains and utilized them to detect illicit activities on
24 million monetization chains we collected from lead-
ing parking services over 5.5 months. This study reveals
the pervasiveness of those illicit monetization activities,
parties responsible for them and the revenues they gener-
ate which approaches 40% of the total revenue for some
parking services. Our findings point to an urgent need
for a better regulation of domain parking.

|,Data
imc247-haoA.pdf,|
Spammers register a tremendous number of domains to evade
blacklisting and takedown efforts. Current techniques to detect
such domains rely on crawling spam URLs or monitoring lookup
traffic. Such detection techniques are only effective after the spam-
mers have already launched their campaigns, and thus these coun-
termeasures may only come into play after the spammer has already
reaped significant benefits from the dissemination of large volumes
of spam. In this paper we examine the registration process of such
domains, with a particular eye towards features that might indicate
that a given domain likely has a malicious purpose at registration
time, before it is ever used for an attack. Our assessment includes
exploring the characteristics of registrars, domain life cycles, regis-
tration bursts, and naming patterns. By investigating zone changes
from the .com TLD over a 5-month period, we discover that spam-
mers employ bulk registration, that they often re-use domains pre-
viously registered by others, and that they tend to register and host
their domains over a small set of registrars. Our findings suggest
steps that registries or registrars could use to frustrate the efforts
of miscreants to acquire domains in bulk, ultimately reducing their
agility for mounting large-scale attacks.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Oper-
ationsNetwork monitoring; K.6.5 [Security and Protection];
K.4.1 [Computers and Society]: Public Policy IssuesAbuse and
crime involving computers

General Terms
Measurement, Security

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
IMC13, October 2325, 2013, Barcelona, Spain.
Copyright 2013 ACM 978-1-4503-1953-9/13/10 ...$15.00.
http://dx.doi.org/10.1145/2504730.2504753.

Keywords
DNS; Domain Registration; Spam; Blacklist

1.

|,Data
p920-lu.pdf,|
The operating system kernel is the de facto trusted computing base
for most computer systems. To secure the OS kernel, many security
mechanisms, e.g., kASLR and StackGuard, have been increasingly
deployed to defend against attacks (e.g., code reuse attack). How-
ever, the effectiveness of these protections has been proven to be
inadequatethere are many information leak vulnerabilities in the
kernel to leak the randomized pointer or canary, thus bypassing
kASLR and StackGuard. Other sensitive data in the kernel, such as
cryptographic keys and file caches, can also be leaked. According to
our study, most kernel information leaks are caused by uninitialized
data reads. Unfortunately, existing techniques like memory safety
enforcements and dynamic access tracking tools are not adequate or
efficient enough to mitigate this threat.

In this paper, we propose UniSan, a novel, compiler-based ap-
proach to eliminate all information leaks caused by uninitialized
read in the OS kernel. UniSan achieves this goal using byte-level,
flow-sensitive, context-sensitive, and field-sensitive initialization
analysis and reachability analysis to check whether an allocation has
been fully initialized when it leaves kernel space; if not, it automati-
cally instruments the kernel to initialize this allocation. UniSans
analyses are conservative to avoid false negatives and are robust by
preserving the semantics of the OS kernel. We have implemented
UniSan as passes in LLVM and applied it to the latest Linux kernel
(x86_64) and Android kernel (AArch64). Our evaluation showed
that UniSan can successfully prevent 43 known and many new unini-
tialized data leak vulnerabilities. Further, 19 new vulnerabilities in
the latest kernels have been confirmed by Linux and Google. Our
extensive performance evaluation with LMBench, ApacheBench,
Android benchmarks, and the SPEC benchmarks also showed that
UniSan imposes a negligible performance overhead.

Keywords
kernel information leak; uninitialized read; reachability analysis;
initialization analysis; memory initialization

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978366

1.

|,Data
p1056-sluganovic.pdf,|
Eye tracking devices have recently become increasingly pop-
ular as an interface between people and consumer-grade elec-
tronic devices. Due to the fact that human eyes are fast, re-
sponsive, and carry information unique to an individual, an-
alyzing persons gaze is particularly attractive for effortless
biometric authentication. Unfortunately, previous propos-
als for gaze-based authentication systems either suffer from
high error rates, or require long authentication times.

We build upon the fact that some eye movements can be
reflexively and predictably triggered, and develop an interac-
tive visual stimulus for elicitation of reflexive eye movements
that supports the extraction of reliable biometric features in
a matter of seconds, without requiring any memorization or
cognitive effort on the part of the user. As an important ben-
efit, our stimulus can be made unique for every authentica-
tion attempt and thus incorporated in a challenge-response
biometric authentication system. This allows us to prevent
replay attacks, which are possibly the most applicable attack
vectors against biometric authentication.

Using a gaze tracking device, we build a prototype of our
system and perform a series of systematic user experiments
with 30 participants from the general public. We investigate
the performance and security guarantees under several dif-
ferent attack scenarios and show that our system surpasses
existing gaze-based authentication methods both in achieved
equal error rates (6.3%) and significantly lower authentica-
tion times (5 seconds).

1.

|,Data
p426-perl.pdf,|
Despite the security communitys best effort, the number
of serious vulnerabilities discovered in software is increasing
rapidly. In theory, security audits should find and remove
the vulnerabilities before the code ever gets deployed. How-
ever, due to the enormous amount of code being produced,
as well as a the lack of manpower and expertise, not all code
is sufficiently audited. Thus, many vulnerabilities slip into
production systems. A best-practice approach is to use a
code metric analysis tool, such as Flawfinder, to flag poten-
tially dangerous code so that it can receive special attention.
However, because these tools have a very high false-positive
rate, the manual effort needed to find vulnerabilities remains
overwhelming.

In this paper, we present a new method of finding poten-
tially dangerous code in code repositories with a significantly
lower false-positive rate than comparable systems. We com-
bine code-metric analysis with metadata gathered from code
repositories to help code review teams prioritize their work.
The paper makes three contributions. First, we conducted
the first large-scale mapping of CVEs to GitHub commits
in order to create a vulnerable commit database. Second,
based on this database, we trained a SVM classifier to flag
suspicious commits. Compared to Flawfinder, our approach
reduces the amount of false alarms by over 99 % at the same
level of recall. Finally, we present a thorough quantitative
and qualitative analysis of our approach and discuss lessons
learned from the results. We will share the database as
a benchmark for future research and will also provide our
analysis tool as a web service.

Categories and Subject Descriptors
D.2.4 [Software Engineering]: Software/Program Verifi-
cation; K.6.5 [Management of Computing and Infor-
mation Systems]: Security and Protection

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS15, October 1216, 2015, Denver, Colorado, USA.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813604.

Keywords
Vulnerabilities; Static Analysis; Machine Learning

1.

|,Data
p1080-zhang.pdf,|

Voice authentication is drawing increasing attention and
becomes an attractive alternative to passwords for mobile
authentication. Recent advances in mobile technology fur-
ther accelerate the adoption of voice biometrics in an array
of diverse mobile applications. However, recent studies show
that voice authentication is vulnerable to replay attacks,
where an adversary can spoof a voice authentication system
using a pre-recorded voice sample collected from the vic-
tim. In this paper, we propose VoiceLive, a practical liveness
detection system for voice authentication on smartphones.
VoiceLive detects a live user by leveraging the users unique
vocal system and the stereo recording of smartphones. In
particular, with the phone closely placed to a users mouth,
it captures time-difference-of-arrival (TDoA) changes in a
sequence of phoneme sounds to the two microphones of the
phone, and uses such unique TDoA dynamic which doesnt
exist under replay attacks for liveness detection. VoiceLive
is practical as it doesnt require additional hardware but
two-channel stereo recording that is supported by virtually
all smartphones. Our experimental evaluation with 12 par-
ticipants and different types of phones shows that VoiceLive
achieves over 99% detection accuracy at around 1% Equal
Error Rate (EER). Results also show that VoiceLive is ro-
bust to different phone placements and is compatible to dif-
ferent sampling rates and phone models.

Keywords
Voice recognition; Liveness detection; Phoneme localization

1.

|,Data
06547135.pdf,|We present three techniques for extracting en-

tropy during boot on embedded devices.

Our first technique times the execution of code blocks early
in the Linux kernel boot process. It is simple to implement and
has a negligible runtime overhead, but, on many of the devices
we test, gathers hundreds of bits of entropy.

Our second and third techniques, which run in the boot-
loader, use hardware features  DRAM decay behavior and
PLL locking latency, respectively  and are therefore less
portable and less generally applicable, but their behavior is
easier to explain based on physically unpredictable processes.
We implement and measure the effectiveness of our tech-
niques on ARM-, MIPS-, and AVR32-based systems-on-a-chip
from a variety of vendors.

I. |,Non-data
p1155-durak.pdf,|
The security of order-revealing encryption (ORE) has been
unclear since its invention. Dataset characteristics for which
ORE is especially insecure have been identified, such as small
message spaces and low-entropy distributions. On the other
hand, properties like one-wayness on uniformly-distributed
datasets have been proved for ORE constructions.

This work shows that more plaintext information can be
extracted from ORE ciphertexts than was previously thought.
We identify two issues: First, we show that when multi-
ple columns of correlated data are encrypted with ORE,
attacks can use the encrypted columns together to reveal
more information than prior attacks could extract from the
columns individually. Second, we apply known attacks, and
develop new attacks, to show that the leakage of concrete
ORE schemes on non-uniform data leads to more accurate
plaintext recovery than is suggested by the security theorems
which only dealt with uniform inputs.

Keywords
database encryption; order-revealing encryption; inference
attacks

1.

|,Data
p1068-li.pdf,|
In this study, we present WindTalker, a novel and practi-
cal keystroke inference framework that allows an attacker
to infer the sensitive keystrokes on a mobile device through
WiFi-based side-channel information. WindTalker is moti-
vated from the observation that keystrokes on mobile devices
will lead to different hand coverage and the finger motions,
which will introduce a unique interference to the multi-path
signals and can be reflected by the channel state informa-
tion (CSI). The adversary can exploit the strong correlation
between the CSI fluctuation and the keystrokes to infer the
users number input. WindTalker presents a novel approach
to collect the targets CSI data by deploying a public WiFi
hotspot. Compared with the previous keystroke inference
approach, WindTalker neither deploys external devices close
to the target device nor compromises the target device. In-
stead, it utilizes the public WiFi to collect users CSI data,
which is easy-to-deploy and difficult-to-detect. In addition,
it jointly analyzes the traffic and the CSI to launch the
keystroke inference only for the sensitive period where pass-
word entering occurs. WindTalker can be launched without
the requirement of visually seeing the smart phone users in-
put process, backside motion, or installing any malware on
the tablet. We implemented Windtalker on several mobile
phones and performed a detailed case study to evaluate the
practicality of the password inference towards Alipay, the
largest mobile payment platform in the world. The evalua-
tion results show that the attacker can recover the key with
a high successful rate.

Keywords
Password Inference; Channel State Information; Online Pay-
ment; Wireless Security; Traffic Analysis



Corresponding author, Email: zhu-hj@cs.sjtu.edu.cn

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full cita-
tion on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS16, October 24-28, 2016, Vienna, Austria
c(cid:2) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978397

1.

|,Data
sec14-paper-marczak.pdf,|
Repressive nation-states have long monitored telecommunica-
tions to keep tabs on political dissent. The Internet and online
social networks, however, pose novel technical challenges to
this practice, even as they open up new domains for surveil-
lance. We analyze an extensive collection of suspicious files
and links targeting activists, opposition members, and non-
governmental organizations in the Middle East over the past
several years. We find that these artifacts reflect efforts to at-
tack targets devices for the purposes of eavesdropping, stealing
information, and/or unmasking anonymous users. We describe
attack campaigns we have observed in Bahrain, Syria, and the
United Arab Emirates, investigating attackers, tools, and tech-
niques. In addition to off-the-shelf remote access trojans and
the use of third-party IP-tracking services, we identify commer-
cial spyware marketed exclusively to governments, including
Gammas FinSpy and Hacking Teams Remote Control Sys-
tem (RCS). We describe their use in Bahrain and the UAE, and
map out the potential broader scope of this activity by conduct-
ing global scans of the corresponding command-and-control
(C&C) servers. Finally, we frame the real-world consequences
of these campaigns via strong circumstantial evidence linking
hacking to arrests, interrogations, and imprisonment.

1

|,Data
sec14-paper-peng.pdf,|
This paper introduces X-Force, a novel binary analysis
engine. Given a potentially malicious binary executable,
X-Force can force the binary to execute requiring no in-
puts or proper environment. It also explores different ex-
ecution paths inside the binary by systematically forc-
ing the branch outcomes of a very small set of condi-
tional control transfer instructions. X-Force features a
crash-free execution model that can detect and recover
from exceptions. In particular, it can fix invalid mem-
ory accesses by allocating memory on-demand and set-
ting the offending pointers to the allocated memory. We
have applied X-Force to three security applications. The
first is to construct control flow graphs and call graphs
for stripped binaries. The second is to expose hidden
behaviors of malware, including packed and obfuscated
APT malware. X-Force is able to reveal hidden mali-
cious behaviors that had been missed by manual inspec-
tion. In the third application, X-Force substantially im-
proves analysis coverage in dynamic type reconstruction
for stripped binaries.

1

|,Non-data
sec14-paper-lecuyer.pdf,|

Todays Web services  such as Google, Amazon, and
Facebook  leverage user data for varied purposes,
including personalizing recommendations,
targeting
advertisements, and adjusting prices. At present, users
have little insight into how their data is being used.
Hence, they cannot make informed choices about the
services they choose.

To increase transparency, we developed XRay,

the
first fine-grained, robust, and scalable personal data
tracking system for the Web. XRay predicts which
data in an arbitrary Web account (such as emails,
searches, or viewed products) is being used to target
which outputs (such as ads, recommended products, or
prices). XRays core functions are service agnostic and
easy to instantiate for new services, and they can track
data within and across services. To make predictions
independent of the audited service, XRay relies on the
following insight: by comparing outputs from different
accounts with similar, but not identical, subsets of data,
one can pinpoint targeting through correlation. We
show both theoretically, and through experiments on
Gmail, Amazon, and YouTube,
that XRay achieves
high precision and recall by correlating data from a
surprisingly small number of extra accounts.

|,Non-data
sec14-paper-fredrikson-z0.pdf,|
Traditionally, confidentiality and integrity have been two
desirable design goals that are have been difficult to com-
bine. Zero-Knowledge Proofs of Knowledge (ZKPK) of-
fer a rigorous set of cryptographic mechanisms to bal-
ance these concerns. However, published uses of ZKPK
have been difficult for regular developers to integrate into
their code and, on top of that, have not been demon-
strated to scale as required by most realistic applications.
This paper presents Z (pronounced zee-not), a
compiler that consumes applications written in C#
into code that automatically produces scalable zero-
knowledge proofs of knowledge, while automatically
splitting applications into distributed multi-tier code. Z
builds detailed cost models and uses two existing zero-
knowledge back-ends with varying performance charac-
teristics to select the most efficient translation. Our case
studies have been directly inspired by existing sophisti-
cated widely-deployed commercial products that require
both privacy and integrity. The performance delivered
by Z is as much as 40 faster across six complex ap-
plications. We find that when applications are scaled to
real-world settings, existing zero-knowledge compilers
often produce code that fails to run or even compile in
a reasonable amount of time. In these cases, Z is the
only solution we know about that is able to provide an
application that works at scale.
1
As popular applications rely on personal, privacy-
sensitive information about users, factors such as legal
regulations, industry self-regulation, and a growing body
of privacy-conscious users all pressure developers to re-
spond to demands for privacy. Storing users data in
the cloud creates downsides for the application provider,
both immediately and down the road. While policy mea-
sures such as DoNotTrack and anonymous advertising
identifiers become increasingly popular, a recent trend
explored in several research projects has been to move
functionality to the client [13, 17, 37, 40]. Because ex-
ecution happens on the client, such as a mobile device
or even in the browser, this alone provides a degree of
privacy in the computation: only relevant data, if any, is
disclosed (to a server). However, in many cases, moving

|,Non-data
06547123.pdf,|Bitcoin is the first e-cash system to see widespread
adoption. While Bitcoin offers the potential for new types of
financial interaction, it has significant limitations regarding
privacy. Specifically, because the Bitcoin transaction log is
completely public, users privacy is protected only through the
use of pseudonyms. In this paper we propose Zerocoin, a crypto-
graphic extension to Bitcoin that augments the protocol to allow
for fully anonymous currency transactions. Our system uses
standard cryptographic assumptions and does not introduce
new trusted parties or otherwise change the security model of
Bitcoin. We detail Zerocoins cryptographic construction, its
integration into Bitcoin, and examine its performance both in
terms of computation and impact on the Bitcoin protocol.

I. |,Non-data
27_Chen.pdf,|. Set-valued data brings enormous opportunities to data min-
ing tasks for various purposes. Many anonymous methods for set-valued
data have been proposed to effectively protect individuals privacy a-
gainst identify linkable attacks and item linkage attacks. In these meth-
ods, sensitive items are protected by a privacy threshold to limit the
re-identified probability of sensitive items. However, lots of set-valued
data have diverse sensitivity on data items. Then it leads to the over
protection problem that these existing privacy-preserving methods are
applied to process the data items with diverse sensitivity, and it reduces
the utility of data. In this paper, we propose a sensitivity-adaptive -
uncertainty model to prevent over-generalization and over-suppression
by using adaptive privacy thresholds. Thresholds, which accurately cap-
ture the hidden privacy features of the set-valued dataset, are defined
by uneven distribution of different sensitive items. Under the model, we
develop a fine-grained privacy preserving technique through Local Gen-
eralization and Partial Suppression, which optimizes a balance between
privacy protection and data utility. Experiments show that our method
effectively improves the utility of anonymous data.
Keywords: Set-valued Data; Anonymization; Privacy Preserving; Gen-
eralization and Suppression

1

|,Data
31_Hanzlik.pdf,|. This paper concerns blind signature schemes. We focus on
two moves constructions, which imply concurrent security. There are
known efficient blind signature schemes based on the random oracle
model and on the common reference string model. However, construct-
ing two move blind signatures in the standard model is a challenging
task, as shown by the impossibility results of Fischlin et al. The recent
construction by Garg et al. (Eurocrypt14) bypasses this result by us-
ing complexity leveraging, but it is impractical due to the signature size
( 100 kB). Fuchsbauer et al. (Crypto15) presented a more practical
construction, but with a security argument based on interactive assump-
tions. We present a blind signature scheme that is two-move, setup-free
and comparable in terms of efficiency with the results of Fuchsbauer et
al. Its security is based on a knowledge assumption.

Keywords: blind signature, Okamoto-Uchiyama cryptosystem, knowl-
edge assumption

1

|,Non-data
21_Anand.pdf,|. Keyboard acoustic side channel attacks have been shown to utilize the
audio leakage from typing on the keyboard to infer the typed words up to a certain
degree of accuracy. Researchers have continued to improve upon the accuracy of
such attacks by employing different techniques and attack vectors such as feature
extraction and classification, keyboard geometry and triangulation.
While research is still ongoing towards further improving acoustic side chan-
nel attacks, much work has been lacking in building a working defense mecha-
nism against such class of attacks. In this paper, we set out to propose a practical
defense mechanism against keyboard acoustic attacks specifically on password
typing and test its performance against several attack vectors. Our defense in-
volves the use of various background sounds to mask the audio leakage from
the keyboard thereby preventing the side channel attacks from gaining usable in-
formation about the typed password. The background sounds are generated by
the device that is used to input the passwords. We also evaluate the usability of
our approach and show that the addition of background sounds does not hamper
users capability to input passwords.

1

|,Data
05_Garman.pdf,|. Decentralized ledger-based currencies such as Bitcoin provide
a means to construct payment systems without requiring a trusted bank.
Removing this trust assumption comes at the significant cost of transac-
tion privacy. A number of academic works have sought to improve the
privacy offered by ledger-based currencies using anonymous electronic
cash (e-cash) techniques. Unfortunately, this strong degree of privacy
creates new regulatory concerns, since the new private transactions can-
not be subject to the same controls used to prevent individuals from
conducting illegal transactions such as money laundering. We propose
an initial approach to addressing this issue by adding privacy preserving
policy-enforcement mechanisms that guarantee regulatory compliance,
allow selective user tracing, and admit tracing of tainted coins (e.g.,
ransom payments). To accomplish this new functionality we also provide
improved definitions for Zerocash and, of independent interest, an efficient
construction for simulation sound zk-SNARKs.

1

|,Non-data
p1-dainotti.pdf,|
Botnets are the most common vehicle of cyber-criminal activity.
They are used for spamming, phishing, denial of service attacks,
brute-force cracking, stealing private information, and cyber war-
fare. Botnets carry out network scans for several reasons, includ-
ing searching for vulnerable machines to infect and recruit into
the botnet, probing networks for enumeration or penetration, etc.
We present the measurement and analysis of a horizontal scan of
the entire IPv4 address space conducted by the Sality botnet in
February of last year. This 12-day scan originated from approxi-
mately 3 million distinct IP addresses, and used a heavily coordi-
nated and unusually covert scanning strategy to try to discover and
compromise VoIP-related (SIP server) infrastructure. We observed
this event through the UCSD Network Telescope, a /8 darknet con-
tinuously receiving large amounts of unsolicited traffic, and we cor-
relate this traffic data with other public sources of data to validate
our inferences. Sality is one of the largest botnets ever identified by
researchers, its behavior represents ominous advances in the evo-
lution of modern malware: the use of more sophisticated stealth
scanning strategies by millions of coordinated bots, targeting crit-
ical voice communications infrastructure. This work offers a de-
tailed dissection of the botnets scanning behavior, including gen-
eral methods to correlate, visualize, and extrapolate botnet behavior
across the global Internet.

Categories and Subject Descriptors
C.2.3 [Network Operations]: Network Monitoring;
C.2.5 [Local and Wide-Area Networks]: Internet;

General Terms
Measurement, Security

Keywords
Darknet, Network Telescope, Internet Background Radiation, Bot-
net, SIP, Scan, Probing, Stealth, Covert, Coordination, Sality, Bot,
VoIP

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
03_Fernandes.pdf,|. App-based deception attacks are increasingly a problem on
mobile devices and they are used to steal passwords, credit card num-
bers, text messages, etc. Current versions of Android are susceptible to
these attacks. Recently, Bianchi et al. proposed a novel solution What
the App is That that included a host-based system to identify apps to
users via a security indicator and help assure them that their input goes
to the identified apps [7]. Unfortunately, we found that the solution has
a significant side channel vulnerability as well as susceptibility to click-
jacking that allow non-privileged malware to completely compromise the
defenses, and successfully steal passwords or other keyboard input. We
discuss the vulnerabilities found, propose possible defenses, and then
evaluate the defenses against different types of UI deception attacks.

1

|,Non-data
35_Murdoch.pdf,|. Fraud victims are often refused a refund by their bank on the
grounds that they failed to comply with their banks terms and conditions
about PIN safety. We, therefore, conducted a survey of how many PINs
people have, and how they manage them. We found that while only a
third of PINs are ever changed, almost half of bank customers write at
least one PIN down. We also found bank conditions that are too vague
to test, or even contradictory on whether PINs could be shared across
cards. Yet, some hazardous practices are not forbidden by many banks: of
the 22.9% who re-use PINs across devices, half also use their bank PINs
on their mobile phones. We conclude that many bank contracts fail a
simple test of reasonableness, and strong authentication, as required by
the Payment Services Directive II, should include usability testing.

1

|,Data
p467-nazir.pdf,|
We employ user activity data from three highly popular gift-
ing applications on Facebook to study the evolution of user
activity on applications through the most commonly-used
growth mechanism, namely Application Requests. We find
user activity graphs differ from friendship graphs in large
part due to the inherent directionality of user activity, and
node transience. Our results show that, unlike degree distri-
butions in friendship graphs, activity graphs exhibit strong
asymmetry in in- and out-degree distributions, and that out-
degrees are not accurately described by currently known
parametric distributions. As such, user activity graphs can-
not be simulated through existing intent- and feature-driven
algorithms that can model friendship graphs.

We present a novel probabilistic growth model for user ac-
tivity on the gifting genre of social applications. Our model
decouples in- and out-degrees based on their distinct na-
ture exhibited by our empirical data. We use the insight
that regardless of increasing, declining or stable user activ-
ity, gifting application user activity exhibits the same graph
structure. Our model produces synthetic graphs that con-
sist of disconnected components with low clustering of nodes,
and exhibit degree structures very similar to our real activ-
ity data. We discuss the benefits and shortfalls of our model
and its applicability to other types of OSN-based applica-
tions, such as social games. To the best of our knowledge
this study is the first to explore and model user activity
growth processes on OSN-based applications.

Categories and Subject Descriptors: C.2.0 [Computer
- Communication Networks]: General; H.4.3 [Information
Systems Applications]: Communications Applications

General Terms: Measurement, Algorithms

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

Author Keywords: Online Social Networks, Social Games,
Social Gifting, Facebook, Applications, Algorithms

1.

|,Data
p343-vallinarodriguez.pdf,|

1.

|,Data
p115-halepovic.pdf,|
Cellular network operators have a compelling interest to
monitor HTTP transaction latency because it is an impor-
tant component of the user experience. Existing techniques
to monitor latency require active probing or use passive anal-
ysis to estimate round-trip time (RTT). Unfortunately, it is
impractical to use active probing to monitor entire cellular
networks, and RTT is only one component of HTTP latency
in cellular networks. This paper presents a new passive tech-
nique to estimate HTTP transaction latency that overcomes
the scaling and completeness limitations of prior approaches.
We validate our technique in an operational cellular network
and present results for traffic in the wild.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations

Keywords
Time To First Byte, Round-Trip Time, Network measure-
ment, Cellular, Wireless, Mobile

1.

|,Data
p301-sommers.pdf,|

Cellular and 802.11 WiFi are compelling options for mobile Inter-
net connectivity. The goal of our work is to understand the per-
formance afforded by each of these technologies in diverse en-
vironments and use conditions.
In this paper, we compare and
contrast cellular and WiFi performance using crowd-sourced data
from Speedtest.net. Our study considers spatio-temporal per-
formance (upload/download throughput and latency) using over 3
million user-initiated tests from iOS and Android apps in 15 dif-
ferent metro areas collected over a 15 week period. Our basic per-
formance comparisons show that (i) WiFi provides better absolute
download/upload throughput, and a higher degree of consistency in
performance; (ii) WiFi networks generally deliver lower absolute
latency, but the consistency in latency is often better with cellular
access; (iii) throughput and latency vary widely depending on the
particular access type (e.g., HSPA, EVDO, LTE, WiFi, etc.) and
service provider. More broadly, our results show that performance
consistency for cellular and WiFi is much lower than has been re-
ported for wired broadband. Temporal analysis shows that average
performance for cell and WiFi varies with time of day, with the best
performance for large metro areas coming at non-peak hours. Spa-
tial analysis shows that performance is highly variable across metro
areas, but that there are subregions that offer consistently better per-
formance for cell or WiFi. Comparisons between metro areas show
that larger areas provide higher throughput and lower latency than
smaller metro areas, suggesting where ISPs have focused their de-
ployment efforts. Finally, our analysis reveals diverse performance
characteristics resulting from the rollout of new cell access tech-
nologies and service differences among local providers.

Categories and Subject Descriptors

C.2.1 [Network Architecture and Design]: Wireless communica-
tion; C.4 [Performance of Systems]: Performance attributes; C.4
[Performance of Systems]: Measurement Techniques

General Terms

Design, Experimentation, Measurement, Performance

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

Keywords

Cellular, WiFi

1.

|,Data
p37-glatz.pdf,|
Internet background radiation (IBR) is a very interesting
piece of Internet traffic as it is the result of attacks and
misconfigurations. Previous work primarily analyzed IBR
traffic to large unused IP address blocks called network tele-
scopes. In this work, we build new techniques for monitor-
ing one-way traffic in live networks with the main goals of
1) expanding our understanding of this interesting type of
traffic towards live networks as well as of 2) making it useful
for detecting and analyzing the impact of outages. Our first
contribution is a classification scheme for dissecting one-way
traffic into useful classes, including one-way traffic due to un-
reachable services, scanning, peer-to-peer applications, and
backscatter. Our classification scheme is helpful for moni-
toring IBR traffic in live networks solely based on flow-level
data. After thoroughly validating our classifier, we use it
to analyze a massive data-set that covers 7.41 petabytes
of traffic from a large backbone network to shed light into
the composition of one-way traffic. We find that the main
sources of one-way traffic are malicious scanning, peer-to-
peer applications, and outages.
In addition, we report a
number of interesting observations including that one-way
traffic makes a very large fraction, i.e., between 34% and
67%, of the total number of flows to the monitored network,
although it only accounts for 3.4% of the number of pack-
ets on average, which suggests a new conceptual model for
Internet traffic in which IBR traffic is dominant in terms of
flows. Finally, we demonstrate the utility of one-way traffic
of the particularly interesting class of unreachable services
for monitoring network and service outages by analyzing the
impact of interesting events we detected in the network of
our university.

Categories and Subject Descriptors
C.2.3 [COMPUTER-COMMUNICATION NETWORKS]:
Network OperationsNetwork monitoring

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

General Terms
Measurement, Security

Keywords
Measurement methods, Traffic Analysis (anomaly detection,
classification)

1.

|,Data
p29-khare.pdf,|
A concurrent prefix hijack happens when an unauthorized
network originates IP prefixes of multiple other networks.
Its extreme case is leaking the entire routing table, i.e., hi-
jacking all the prefixes in the table. This is a well-known
problem and there exists a preventive measure in practice
to safeguard against it. However, we investigated and un-
covered many concurrent prefix hijacks that didnt involve
a full-table leak. We report these events and their impact
on Internet routing. By correlating suspicious routing an-
nouncements and comparing it with a networks past routing
announcements, we develop a method to detect a networks
abnormal behavior of offending multiple other networks si-
multaneously. Applying the detection algorithm to BGP
routing updates from 2003 through 2010, we identify five to
twenty concurrent prefix hijacks every year, most of which
are previously unknown to the research and operation com-
munities at large. They typically hijack prefixes owned by a
few tens of networks, last from a few minutes to a few hours,
and pollute routes at most vantage points.

Categories and Subject Descriptors
Computer Systems Organization [Computer-Communication
Networks]: Network OperationsNetwork Monitoring

General Terms
Algorithms, Measurement, Verification

Keywords
BGP Security, Prefix Hijacking
The material in this article is based upon the work par-
tially supported by DHS grant N66001-08-C-2028 and by
Open Project of Shenzhen Key Lab of Cloud Computing
Technology & Applications (SPCCTA). Any opinions, find-
ings, and conclusions or recommendations expressed in this
material are those of the author(s) and do not necessarily
reflect the views of the sponsors.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
10_Damgaard.pdf,|. We report on the design and implementation of a system that uses mul-
tiparty computation to enable banks to benchmark their customers confidential
performance data against a large representative set of confidential performance
data from a consultancy house. The system ensures that both the banks and the
consultancy houses data stays confidential, the banks as clients learn nothing but
the computed benchmarking score. In the concrete business application, the de-
veloped prototype helps Danish banks to find the most efficient customers among
a large and challenging group of agricultural customers with too much debt. We
propose a model based on linear programming for doing the benchmarking and
implement it using the SPDZ protocol by Damg ard et al., which we modify using
a new idea that allows clients to supply data and get output without having to
participate in the preprocessing phase and without keeping state during the com-
putation. We ran the system with two servers doing the secure computation using
a database with information on about 2500 users. Answers arrived in about 25
seconds.

1

|,Data
p365-papapanagiotou.pdf,|
The Dynamic Host Configuration Protocol (DHCP) was in-
troduced nearly 20 years ago as a mechanism for hosts to
automatically acquire IP addresses. While the protocol re-
mains the same, its usage has evolved, especially in the last
decade with the |,Data
p225-huang.pdf,|
Todays commercial video streaming services use dynamic
rate selection to provide a high-quality user experience. Most
services host content on standard HTTP servers in CDNs,
so rate selection must occur at the client. We measure three
popular video streaming services  Hulu, Netflix, and Vudu
 and find that accurate client-side bandwidth estimation
above the HTTP layer is hard. As a result, rate selection
based on inaccurate estimates can trigger a feedback loop,
leading to undesirably variable and low-quality video. We
call this phenomenon the downward spiral effect, and we
measure it on all three services, present insights into its root
causes, and validate initial solutions to prevent it.

Categories and Subject Descriptors
C.2.0 [Computer Systems Organization]: Computer-
Communication NetworksGeneral ; C.4 [Performance of
Systems]: [Measurement techniques]

General Terms
Measurement

Keywords
HTTP-based Video Streaming, Video Rate Adaptation

1.

|,Non-data
p523-otto.pdf,|
Content Delivery Networks (CDNs) rely on the Domain
Name System (DNS) for replica server selection. DNS-
based server selection builds on the assumption that,
in
the absence of information about the clients actual network
location, the location of a clients DNS resolver provides
a good approximation. The recent growth of remote DNS
services breaks this assumption and can negatively impact
clients web performance.

In this paper, we assess the end-to-end impact of using
remote DNS services on CDN performance and present
the first evaluation of an industry-proposed solution to the
problem. We find that remote DNS usage can indeed
significantly impact clients web performance and that the
proposed solution, if available, can effectively address the
problem for most clients. Considering the performance cost
of remote DNS usage and the limited adoption base of
the industry-proposed solution, we present and evaluate an
alternative approach, Direct Resolution, to readily obtain
comparable performance improvements without requiring
CDN or DNS participation.

Categories and Subject Descriptors
C.2.4 [Communication Networks]: Distributed Systems
Distributed applications; C.2.5 [Communication Net-
works]: Local and Wide-Area NetworksInternet; C.4
[Performance of Systems]: Measurement techniques

General Terms
Experimentation, Measurement, Performance

Keywords
CDN, Content distribution, DNS, DNS extension, Internet,
measurement

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
p441-murynets.pdf,|
The Short Messaging Service (SMS), one of the most suc-
cessful cellular services, generates millions of dollars in rev-
enue for mobile operators. Estimates indicate that billions
of text messages are traveling the airwaves daily. Never-
theless, text messaging is becoming a source of customer
dissatisfaction due to the rapid surge of messaging abuse
activities. Although spam is a well tackled problem in the
email world, SMS spam experiences a yearly growth larger
than 500%.
In this paper we present, to the best of our
knowledge, the first analysis of SMS spam traffic from a
tier-1 cellular operator. Communication patterns of spam-
mers are compared to those of legitimate cell-phone users
and Machine to Machine (M2M) connected appliances. The
results indicate that M2M systems exhibit communication
profiles similar to spammers, which could mislead spam fil-
ters. Beyond the expected results, such as a large load of
text messages sent out to a wide target list, other interest-
ing findings are made. For example, the results indicate that
the great majority of the spammers connect to the network
with just a handful of different hardware models. We find
the main geographical sources of messaging abuse in the US.
We also find evidence of spammer mobility, voice and data
traffic resembling the behavior of legitimate customers.

Categories and Subject Descriptors
K.6.5 [Security and Protection]; K.4.1 [Computers and
Society]: Public Policy IssuesAbuse and crime involv-
ing computers; C.2.3 [Computer Communications Net-
works]: Network OperationsNetwork Monitoring

General Terms
Experimentation, Measurement

Keywords
SMS, abuse, spam, traffic analysis, cellular networks

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
13_Carter.pdf,|. Mobile computing has experienced enormous growth in mar-
ket share and computational power in recent years. As a result, mobile
malware is becoming more sophisticated and more prevalent, leading to
research into dynamic sandboxes as a widespread approach for detect-
ing malicious applications. However, the event-driven nature of Android
applications renders critical the capability to automatically generate de-
terministic and intelligent user interactions to drive analysis subjects
and improve code coverage. In this paper, we present CuriousDroid, an
automated system for exercising Android application user interfaces in
an intelligent, user-like manner. CuriousDroid operates by decomposing
application user interfaces on-the-fly and creating a context-based model
for interactions that is tailored to the current user layout. We integrated
CuriousDroid with Andrubis, a well-known Android sandbox, and con-
ducted a large-scale evaluation of 38,872 applications taken from different
data sets. Our evaluation demonstrates significant improvements in both
end-to-end sample classification as well as increases in the raw number
of elicited behaviors at runtime.

Keywords: User Interface Analysis, Android, Dynamic Analysis

1 |,Data
p15-shi.pdf,|
Border Gateway Protocol (BGP) plays a critical role in the
Internet inter-domain routing reliability. Invalid routes gen-
erated by mis-configurations or forged by malicious attacks
may hijack the traffic and devastate the Internet routing sys-
tem, but it is unlikely that a secure BGP can be deployed in
the near future to completely prevent them. Although many
hijacking detection systems have been developed, they more
or less have weaknesses such as long detection delay, high
false alarm rate and deployment difficulty, and no systemat-
ic detection results have been studied. This paper proposes
Argus, an agile system that can accurately detect prefix hi-
jackings and deduce the underlying cause of route anomalies
in a very fast way. Argus is based on correlating the con-
trol and data plane information closely and pervasively, and
has been continuously monitoring the Internet for more than
one year. During this period, around 40K routing anoma-
lies were detected, from which 220 stable prefix hijackings
were identified. Our analysis on these events shows that, hi-
jackings that have only been theoretically studied before do
exist in the Internet. Although the frequency of new hijack-
ings is nearly stable, more specific prefixes are hijacked more
frequently. Around 20% of the hijackings last less than ten
minutes, and some can pollute 90% of the Internet in less
than two minutes. These characteristics make Argus espe-
cially useful in practice. We further analyze some represen-
tative cases in detail to help increase the understanding of
prefix hijackings in the Internet.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]:
[Gener-
al Security and Protection]; C.2.3 [Network Operations]:
[Network Monitoring]

Keywords
BGP, Security, Prefix Hijacking, Hijacking Detection

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
16_Caulfield.pdf,| We introduce a model for examining the factors that lead
to the adoption of new encryption technologies. Building on the work
of Brock and Durlauf, the model describes how agents make choices,
in the presence of social interaction, between competing technologies
given their relative cost, functionality, and usability. We apply the model
to examples about the adoption of encryption in communication (email
and messaging) and storage technologies (self-encrypting drives) and also
consider our models predictions for the evolution of technology adoption
over time.

1

|,Non-data
p413-bermudez.pdf,|
A careful perusal of the Internet evolution reveals two major
trends - explosion of cloud-based services and video stream-
ing applications. In both of the above cases, the owner (e.g.,
CNN, YouTube, or Zynga) of the content and the organiza-
tion serving it (e.g., Akamai, Limelight, or Amazon EC2) are
decoupled, thus making it harder to understand the associ-
ation between the content, owner, and the host where the
content resides. This has created a tangled world wide web
that is very hard to unwind, impairing ISPs and network
administrators capabilities to control the traffic flowing in
their networks.

In this paper, we present DN-Hunter, a system that lever-
ages the information provided by DNS traffic to discern the
tangle. Parsing through DNS queries, DN-Hunter tags traf-
fic flows with the associated domain name. This association
has several applications and reveals a large amount of use-
ful information: (i) Provides a fine-grained traffic visibility
even when the traffic is encrypted (i.e., TLS/SSL flows), thus
enabling more effective policy controls, (ii) Identifies flows
even before the flows begin, thus providing superior net-
work management capabilities to administrators, (iii) Un-
derstand and track (over time) different CDNs and cloud
providers that host content for a particular resource, (iv)
Discern all the services/content hosted by a given CDN or
cloud provider in a particular geography and time interval,
and (v) Provides insights into all applications/services run-
ning on any given layer-4 port number.

We conduct extensive experimental analysis and show re-
sults from real traffic traces (including FTTH and 4G ISPs)
that support our hypothesis. Simply put, the information
provided by DNS traffic is one of the key components re-
quired for understanding the tangled web, and bringing the
ability to effectively manage network traffic back to the op-
erators.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Miscella-
neous; C.4 [Performance of Systems]: Measurement Tech-
niques

General Terms
Measurement, Performance

Keywords
DNS, Service Identification.

1.

|,Data
15_Heuser.pdf,|. Smart mobile devices process and store a vast amount of
security- and privacy sensitive data. To protect this data from mali-
cious applications mobile operating systems, such as Android, adopt fine-
grained access control architectures. However, related work has shown
that these access control architectures are susceptible to application-
layer privilege escalation attacks. Both automated static and dynamic
program analysis promise to proactively detect such attacks. Though
while state-of-the-art static analysis frameworks cannot adequately ad-
dress native and highly obfuscated code, dynamic analysis is vulnerable
to malicious applications using logic bombs to avoid early detection.
In contrast, the long-term observation of application behavior could help
users and security analysts better understand malicious apps. In this pa-
per we present the design and implementation of DroidAuditor, which
observes application behavior on real Android devices and generates a
graph-based representation. It visualizes this behavior graph, which en-
ables users to develop an intuitive understanding of application inter-
nals. Our solution further allows security analysts to query the behavior
graph for malicious patterns. We present the design of the DroidAudi-
tor framework and instantiate it using the Android Security Modules
(ASM) access control architecture. We evaluate its capability to detect
application-layer privilege escalation attacks, such as confused deputy
and collusion attacks. In addition, we demonstrate how our architecture
can be used to analyze malicious spyware applications.

1

|,Non-data
14_Coletta.pdf,| After analyzing several Android mobile banking trojans, we
observed the presence of repetitive artifacts that describe valuable infor-
mation about the distribution of this class of malicious apps. Motivated
by the high threat level posed by mobile banking trojans and by the
lack of publicly available analysis and intelligence tools, we automated
the extraction of such artifacts and created a malware tracker named
DroydSeuss. DroydSeuss first processes applications both statically and
dynamically, extracting relevant strings that contain traces of commu-
nication endpoints. Second, it prioritizes the extracted strings based on
the APIs that manipulate them. Finally, DroydSeuss correlates the end-
points with descriptive metadata from the samples, providing aggregated
statistics, raw data, and cross-sample information that allow researchers
to pinpoint relevant groups of applications.
We connected DroydSeuss to the VirusTotal daily feed, consuming An-
droid samples that perform banking-trojan activity. We manually ana-
lyzed its output and found supporting evidence to confirm its correctness.
Remarkably, the most frequent itemset unveiled a campaign currently
spreading against Chinese and Korean bank customers.
Although motivated by mobile banking trojans, DroydSeuss can be used
to analyze the communication behavior of any suspicious application.

|,Data
p145-allamanis.pdf,|
Connections established by users of online social networks
are influenced by mechanisms such as preferential attach-
ment and triadic closure. Yet, recent research has found
that geographic factors also constrain users: spatial prox-
imity fosters the creation of online social ties. While the
effect of space might need to be incorporated to these social
mechanisms, it is not clear to which extent this is true and
in which way this is best achieved.

To address these questions, we present a measurement
study of the temporal evolution of an online location-based
social network. We have collected longitudinal traces over 4
months, including information about when social links are
created and which places are visited by users, as revealed
by their mobile check-ins. Thanks to this fine-grained tem-
poral information, we test and compare whether different
probabilistic models can explain the observed data adopting
an approach based on likelihood estimation, quantitatively
comparing their statistical power to reproduce real events.
We demonstrate that geographic distance plays an impor-
tant role in the creation of new social connections: node de-
gree and spatial distance can be combined in a gravitational
attachment process that reproduces real traces. Instead, we
find that links arising because of triadic closure, where users
form new ties with friends of existing friends, and because of
common focus, where connections arise among users visiting
the same place, appear to be mainly driven by social factors.
We exploit our findings to describe a new model of net-
work growth that combines spatial and social factors. We
extensively evaluate our model and its variations, demon-
strating that it is able to reproduce the social and spatial
properties observed in our traces. Our results offer useful
insights for systems that take advantage of the spatial prop-
erties of online social services.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12,
Copyright 2012 ACM 978-1-4503-1685-9/12/10 ...$15.00.

October 2930, 2012, Austin, Texas, USA.

Categories and Subject Descriptors
H.4 [Information Systems Applications]: Miscellaneous;
H.2.8 [Database Management]: Database applications
data mining

Keywords
social network, graph evolution, gravity models

1.

|,Data
p131-gong.pdf,|
Understanding social network structure and evolution has impor-
tant implications for many aspects of network and system design
including provisioning, bootstrapping trust and reputation systems
via social networks, and defenses against Sybil attacks. Several re-
cent results suggest that augmenting the social network structure
with user attributes (e.g., location, employer, communities of inter-
est) can provide a more fine-grained understanding of social net-
works. However, there have been few studies to provide a system-
atic understanding of these effects at scale.

We bridge this gap using a unique dataset collected as the Google+
social network grew over time since its release in late June 2011.
We observe novel phenomena with respect to both standard social
network metrics and new attribute-related metrics (that we define).
We also observe interesting evolutionary patterns as Google+ went
from a bootstrap phase to a steady invitation-only stage before a
public release.

Based on our empirical observations, we develop a new gener-
ative model to jointly reproduce the social structure and the node
attributes. Using theoretical analysis and empirical evaluations, we
show that our model can accurately reproduce the social and at-
tribute structure of real social networks. We also demonstrate that
our model provides more accurate predictions for practical appli-
cation contexts.

Categories and Subject Descriptors
J.4 [Computer Applications]: Social and behavioral sciences

Keywords
Social network measurement, Node attributes, Social network evo-
lution, Heterogeneous network measurement and modeling, Google+

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
18_Schoenmakers.pdf,|. We present explicit optimal binary pebbling algorithms for reversing
one-way hash chains. For a hash chain of length 2k, the number of hashes performed
in each output round does not exceed (cid:100)k/2(cid:101), whereas the number of hash values
stored throughout is at most k. This is optimal for binary pebbling algorithms
characterized by the property that the midpoint of the hash chain is computed just
once and stored until it is output, and that this property applies recursively to both
halves of the hash chain.
We introduce a framework for rigorous comparison of explicit binary pebbling algo-
rithms, including simple speed-1 binary pebbles, Jakobssons binary speed-2 peb-
bles, and our optimal binary pebbles. Explicit schedules describe for each pebble
exactly how many hashes need to be performed in each round. The optimal sched-
ule turns out to be essentially unique and exhibits a nice recursive structure, which
allows for fully optimized implementations that can readily be deployed. In partic-
ular, we develop the first in-place implementations with minimal storage overhead
(essentially, storing only hash values), and fast implementations with minimal com-
putational overhead. Moreover, we show that our approach is not limited to hash
chains of length n = 2k, but accommodates hash chains of arbitrary length n  1,
without incurring any overhead. Finally, we show how to run a cascade of pebbling
algorithms along with a bootstrapping technique, facilitating sequential reversal of
an unlimited number of hash chains growing in length up to a given bound.

1

|,Non-data
19_Valenta.pdf,| The difficulty of integer factorization is fundamental to modern
cryptographic security using RSA encryption and signatures. Although a
512-bit RSA modulus was first factored in 1999, 512-bit RSA remains
surprisingly common in practice across many cryptographic protocols.
Popular understanding of the difficulty of 512-bit factorization does not
seem to have kept pace with developments in computing power. In this
paper, we optimize the CADO-NFS and Msieve implementations of the
number field sieve for use on the Amazon Elastic Compute Cloud platform,
allowing a non-expert to factor 512-bit RSA public keys in under four
hours for $75. We go on to survey the RSA key sizes used in popular
protocols, finding hundreds or thousands of deployed 512-bit RSA keys
in DNSSEC, HTTPS, IMAP, POP3, SMTP, DKIM, SSH, and PGP.

1

|,Data
17_Algwil.pdf,|. We report novel API attacks on a Captcha web service, and
discuss lessons that we have learned. In so doing, we expand the horizon
of security APIs research by extending it to a new setting. We also show
that system architecture analysis is useful both for identifying vulnera-
bilities in security APIs and for fixing them.

Keywords: API attacks, architecture analysis for security, Captcha,
web security

1

|,Data
12_Kupcu.pdf,|. Secure two party computation (2PC) is a well-studied prob-
lem with many real world applications. Due to Cleves result on general
impossibility of fairness, however, the state-of-the-art solutions only pro-
vide security with abort. We investigate fairness for 2PC in presence of
a trusted Arbiter, in an optimistic setting where the Arbiter is not in-
volved if the parties act fairly. Existing fair solutions in this setting are
by far less efficient than the fastest unfair 2PC.
We close this efficiency gap by designing protocols for fair 2PC with
covert and malicious security that have competitive performance with
the state-of-the-art unfair constructions. In particular, our protocols only
requires the exchange of a few extra messages with sizes that only depend
on the output length; the Arbiters load is independent of the compu-
tation size; and a malicious Arbiter can only break fairness, but not
covert/malicious security even if he colludes with a party. Finally, our
solutions are designed to work with the state-of-the-art optimizations ap-
plicable to garbled circuits and cut-and-choose 2PC such as free-XOR,
half-gates, and the cheating-recovery paradigm.

Keywords: secure two-party computation, covert adversaries, cut-and-choose,
garbled circuits, fair secure computation, optimistic fair exchange.

|,Non-data
p73-dhawan.pdf,|
For analyzing network performance issues, there can be great utility
in having the capability to measure directly from the perspective of
end systems. Because end systems do not provide any external pro-
gramming interface to measurement functionality, obtaining this
capability today generally requires installing a custom executable
on the system, which can prove prohibitively expensive.
In this
work we leverage the ubiquity of web browsers to demonstrate the
possibilities of browsers themselves offering such a programmable
environment. We present Fathom, a Firefox extension that imple-
ments a number of measurement primitives that enable websites or
other parties to program network measurements using JavaScript.
Fathom is lightweight, imposing < 3.2% overhead in page load
times for popular web pages, and often provides 1 ms timestamp
accuracy. We demonstrate Fathoms utility with three case studies:
providing a JavaScript version of the Netalyzr network characteri-
zation tool, debugging web access failures, and enabling web sites
to diagnose performance problems of their clients.
Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General; C.4
[Performance of Systems]: Measurement
techniques; D.2.8
[Software Engineering]: Metricsperformance measures
General Terms
Design, Measurement, Performance, Security
Keywords
Network troubleshooting, network performance, end-host network
measurement, web browser, browser extension
This work was done while the author was visiting ICSI.
:This work was done while the author was visiting UC Berkeley
& ICSI.

 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not  made  or  distributed  for  profit  or  commercial  advantage  and  that 
copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy 
otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, 
requires prior specific permission and/or a fee. 
IMC12, November 1416, 2012, Boston, Massachusetts, USA. 
Copyright 2012 ACM  978-1-4503-1705-4/12/11...$15.00. 
 

1 |,Non-data
02_Molloy.pdf,|. We present a new approach to cross channel fraud detection:
build graphs representing transactions from all channels and use analyt-
ics on features extracted from these graphs. Our underlying hypothesis
is community based fraud detection: an account (holder) performs nor-
mal or trusted transactions within a community that is local to the
account. We explore several notions of community based on graph prop-
erties. Our results show that properties such as shortest distance between
transaction endpoints, whether they are in the same strongly connected
component, whether the destination has high page rank, etc., provide
excellent discriminators of fraudulent and normal transactions whereas
traditional social network analysis yields poor results. Evaluation on a
large dataset from a European bank shows that such methods can sub-
stantially reduce false positives in traditional fraud scoring. We show that
classifiers built purely out of graph properties are very promising, with
high AUC, and can complement existing fraud detection approaches.

1

|,Non-data
24_Konoth.pdf,|. Exponential growth in smartphone usage combined with re-
cent advances in mobile technology is causing a shift in (mobile) app
behavior: application vendors no longer restrict their apps to a single
platform, but rather add synchronization options that allow users to
conveniently switch from mobile to PC or vice versa in order to access
their services. This process of integrating apps among multiple platforms
essentially removes the gap between them. Current, state of the art, mo-
bile phone-based two-factor authentication (2FA) mechanisms, however,
heavily rely on the existence of such separation. They are used in a vari-
ety of segments (such as consumer online banking services or enterprise
secure remote access) to protect against malware. For example, with
2FA in place, attackers should no longer be able to use their PC-based
malware to instantiate fraudulent banking transactions.
In this paper, we analyze the security implications of diminishing gaps
between platforms and show that the ongoing integration and desire
for increased usability results in violation of key principles for mobile
phone 2FA. As a result, we identify a new class of vulnerabilities dubbed
2FA synchronization vulnerabilities. To support our findings, we present
practical attacks against Android and iOS that illustrate how a Man-
in-the-Browser attack can be elevated to intercept One-Time Passwords
sent to the mobile phone and thus bypass the chain of 2FA mechanisms
as used by many financial services.

Keywords: Two-Factor Authentication, Smartphone Security, Finan-
cial Trojans, Synchronization, Anywhere Computing

1

|,Non-data
08_Guasch.pdf,|. An electronic voting protocol provides cast-as-intended ver-
ifiability if the voter can verify that her encrypted vote contains the
voting options that she selected. There are some proposals of protocols
with cast-as-intended verifiability in the literature, but all of them have
drawbacks either in terms of usability or in terms of security. In this
paper, we propose a new voting scheme with cast-as-intended verifiabil-
ity which allows to audit the vote to be cast, while providing measures
for avoiding coercion by allowing the voter to create fake proofs of the
content of her vote. We provide an efficient implementation and formally
analize its security properties.

1

|,Non-data
28_Schrijvers.pdf,|. In this paper we introduce a game-theoretic model for re-
ward functions in Bitcoin mining pools. Our model consists only of an
unordered history of reported shares and gives participating miners the
strategy choices of either reporting or delaying when they discover a share
or full solution. We defined a precise condition for incentive compatibil-
ity to ensure miners strategy choices optimize the welfare of the pool as
a whole. With this definition we show that proportional mining rewards
are not incentive compatible in this model. We introduce and analyze a
novel reward function which is incentive compatible in this model. Fi-
nally we show that the popular reward function pay-per-last-N-shares is
also incentive compatible in a more general model.

1

|,Non-data
26_Arshad.pdf,|. Modern websites include various types of third-party con-
tent such as JavaScript, images, stylesheets, and Flash objects in order
to create interactive user interfaces. In addition to explicit inclusion of
third-party content by website publishers, ISPs and browser extensions
are hijacking web browsing sessions with increasing frequency to inject
third-party content (e.g., ads). However, third-party content can also in-
troduce security risks to users of these websites, unbeknownst to both
website operators and users. Because of the often highly dynamic nature
of these inclusions as well as the use of advanced cloaking techniques in
contemporary malware, it is exceedingly difficult to preemptively recog-
nize and block inclusions of malicious third-party content before it has
the chance to attack the users system.
In this paper, we propose a novel approach to achieving the goal of
preemptive blocking of malicious third-party content inclusion through
an analysis of inclusion sequences on the Web. We implemented our
approach, called Excision, as a set of modi(cid:12)cations to the Chromium
browser that protects users from malicious inclusions while web pages
load. Our analysis suggests that by adopting our in-browser approach,
users can avoid a signi(cid:12)cant portion of malicious third-party content on
the Web. Our evaluation shows that Excision effectively identi(cid:12)es mali-
cious content while introducing a low false positive rate. Our experiments
also demonstrate that our approach does not negatively impact a users
browsing experience when browsing popular websites drawn from the
Alexa Top 500.

Keywords: Web security, Malvertising, Machine learning

1

|,Data
p481-drago.pdf,|
Personal cloud storage services are gaining popularity. With
a rush of providers to enter the market and an increasing of-
fer of cheap storage space, it is to be expected that cloud
storage will soon generate a high amount of Internet traffic.
Very little is known about the architecture and the perfor-
mance of such systems, and the workload they have to face.
This understanding is essential for designing efficient cloud
storage systems and predicting their impact on the network.
This paper presents a characterization of Dropbox, the
leading solution in personal cloud storage in our datasets.
By means of passive measurements, we analyze data from
four vantage points in Europe, collected during 42 consecu-
tive days. Our contributions are threefold: Firstly, we are
the first to study Dropbox, which we show to be the most
widely-used cloud storage system, already accounting for a
volume equivalent to around one third of the YouTube traffic
at campus networks on some days. Secondly, we character-
ize the workload users in different environments generate to
the system, highlighting how this reflects on network traf-
fic. Lastly, our results show possible performance bottle-
necks caused by both the current system architecture and
the storage protocol. This is exacerbated for users connected
far from storage data-centers.

All measurements used in our analyses are publicly avail-
able in anonymized form at the SimpleWeb trace repository:
http://traces.simpleweb.org/dropbox/

Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Miscella-
neous; C.4 [Performance of Systems]: Measurement
Techniques

General Terms
Measurement, Performance

 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not  made  or  distributed  for  profit  or  commercial  advantage  and  that 
copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy 
otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, 
requires prior specific permission and/or a fee. 
IMC12, November 1416, 2012, Boston, Massachusetts, USA. 
Copyright 2012 ACM  978-1-4503-1705-4/12/11...$15.00. 
 

Keywords
Dropbox, Cloud Storage, Internet Measurement.

1.

|,Data
04_Aljuraidan.pdf,|. Outsourcing computation to remote parties (workers) is
an increasingly common practice, owing in part to the growth of cloud
computing. However, outsourcing raises concerns that outsourced tasks
may be completed incorrectly, whether by accident or because workers
cheat to minimize their cost and optimize their gain. The goal of this
paper is to explore, using game theory, the conditions under which the
incentives for all parties can be configured to efficiently disincentivize
worker misbehavior, either inadvertent or deliberate. By formalizing mul-
tiple scenarios with game theory, we establish conditions to discourage
worker cheating that take into account the dynamics of multiple workers,
workers with limited capacity, and changing levels of trust. A key novelty
of our work is modeling the use of a reputation system to decide how
computation tasks are allocated to workers based on their reliability, and
we provide insights on strategies for using a reputation system to increase
the expected quality of results. Overall, our results contribute to make
outsourcing computation more reliable, consistent, and predictable.

1

|,Non-data
32_Carrigan.pdf,| The most common method for a user to gain access to a system,
service, or resource is to provide a secret, often a password, that verifies
her identity and thus authenticates her. Password-based authentication
is considered strong only when the password meets certain length and
complexity requirements, or when it is combined with other methods in
multi-factor authentication. Unfortunately, many authentication systems
do not enforce strong passwords due to a number of limitations; for
example, the time taken to enter complex passwords. We present an
authentication system that addresses these limitations by prompting a
user for credentials once and then storing an authentication ticket in a
wearable device that we call Kerberos Bracelet Identification (KBID).

Keywords: authentication, kerberos, wearables, passwords

(cid:1)

|,Non-data
p453-hohlfeld.pdf,|
This paper investigates the origins of the spamming process,
specifically concerning address harvesting on the web, by re-
lying on an extensive measurement data set spanning over
three years. Concretely, we embedded more than 23 mil-
lion unique spamtrap addresses in web pages. 0.5% of the
embedded trap addresses received a total of 620,000 spam
messages. Besides the scale of the experiment, the critical
aspect of our methodology is the uniqueness of the issued
spamtrap addresses, which enables the mapping of crawling
activities to the actual spamming process.

Our observations suggest that simple obfuscation meth-
ods are still efficient for protecting addresses from being
harvested. A key finding is that search engines are used
as proxies, either to hide the identity of the harvester or to
optimize the harvesting process.

Categories and Subject Descriptors
C.2.3 [Computer-communication networks]: Network
operationsNetwork monitoring; H.4.3 [Information Sys-
tems Applications]: Communications ApplicationsElec-
tronic mail

General Terms
Measurement, Security

Keywords
Spam, E-Mail, Address Harvesting

1.

|,Data
p101-lee.pdf,|
Latency has become an important metric for network moni-
toring since the emergence of new latency-sensitive applica-
tions (e.g., algorithmic trading and high-performance com-
puting). To satisfy the need, researchers have proposed
new architectures such as LDA and RLI that can provide
fine-grained latency measurements. However, these archi-
tectures are fundamentally ossified in their design as they
are designed to provide only a specific pre-configured aggre-
gate measurementeither average latency across all packets
(LDA) or per-flow latency measurements (RLI). Network op-
erators, however, need latency measurements at both finer
(e.g., packet) as well as flexible (e.g., flow subsets) levels of
granularity. To bridge this gap, we propose an architecture
called MAPLE that essentially stores packet-level latencies
in routers and allows network operators to query the latency
of arbitrary traffic sub-populations. MAPLE is built using
scalable data structures with small storage needs (uses only
12.8 bits/packet), and uses a novel mechanism to reduce the
query bandwidth significantly (by a factor of 17 compared to
the naive method of sending packet queries individually).

Categories and Subject Descriptors
C.2.3 [Computer Communication Networks]: Network
management

General Terms
Measurement, algorithms

Keywords
Latency, bloom filter, approximation

1.

|,Non-data
p537-dhamdhere.pdf,|
We use historical BGP data and recent active measurements
to analyze trends in the growth, structure, dynamics and
performance of the evolving IPv6 Internet, and compare
them to the evolution of IPv4. We find that the IPv6 net-
work is maturing, albeit slowly. While most core Internet
transit providers have deployed IPv6, edge networks are lag-
ging. Early IPv6 network deployment was stronger in Eu-
rope and the Asia-Pacific region, than in North America.
Current IPv6 network deployment still shows the same pat-
tern. The IPv6 topology is characterized by a single domi-
nant player  Hurricane Electric  which appears in a large
fraction of IPv6 AS paths, and is more dominant in IPv6
than the most dominant player in IPv4. Routing dynamics
in the IPv6 topology are largely similar to those in IPv4,
and churn in both networks grows at the same rate as the
underlying topologies. Our measurements suggest that per-
formance over IPv6 paths is comparable to that over IPv4
paths if the AS-level paths are the same, but can be much
worse than IPv4 if the AS-level paths differ.

Categories and Subject Descriptors
C.2.2 [COMPUTER-COMMUNICATION
NETWORKS]: Network ProtocolsRouting Protocols

Keywords
IPv6, BGP, Internet topology, routing, performance

General Terms
Experimentation, Measurement

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
p551-alcock.pdf,|
The Copyright (Infringing File Sharing) Amendment Act
2011 (CAA) is a New Zealand law that aims to provide
copyright holders with legal recourse when content is ille-
gally shared over the Internet. This paper presents a study
of residential DSL user behaviour using packet traces cap-
tured at a New Zealand ISP before, shortly after and several
months after the CAA coming into effect. We use libpro-
toident to classify the observed traffic based on the applica-
tion protocol being used to identify and examine any changes
in traffic patterns that may be a result of the new law. We
find that the use of peer-to-peer applications declined sig-
nificantly once the CAA was in effect, suggesting a strong
correlation. We also found that there were increases in tun-
neling, secure file transfer and remote access traffic amongst
a small segment of the user population, which may indicate
an increased uptake in the use of foreign seedboxes to bypass
the jurisdiction of the CAA.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations

Keywords
P2P, seedbox, traffic classification, residential DSL, Internet
law

1.

|,Data
p87-zander.pdf,|
Despite the predicted exhaustion of unallocated IPv4 addresses be-
tween 2012 and 2014, it remains unclear how many current clients
can use its successor, IPv6, to access the Internet. We propose a
refinement of previous measurement studies that mitigates intrin-
sic measurement biases, and demonstrate a novel web-based tech-
nique using Google ads to perform IPv6 capability testing on a
wider range of clients. After applying our sampling error reduction,
we find that 6% of world-wide connections are from IPv6-capable
clients, but only 12% of connections preferred IPv6 in dual-stack
(dual-stack failure rates less than 1%). Except for an uptick around
IPv6-day 2011 these proportions were relatively constant, while the
percentage of connections with IPv6-capable DNS resolvers has in-
creased to nearly 60%. The percentage of connections from clients
with native IPv6 using happy eyeballs has risen to over 20%.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Oper-
ationsNetwork Monitoring; C.4 [Performance of Systems]:
Measurement Techniques

Keywords
IPv6 deployment, banner-ad-based measurement

1.

|,Data
p171-zhao.pdf,|
social network
Data confidentiality policies at major
providers have severely limited researchers access to large-
scale datasets. The biggest impact has been on the study of
network dynamics, where researchers have studied citation
graphs and content-sharing networks, but few have analyzed
detailed dynamics in the massive social networks that dom-
inate the web today.
In this paper, we present results of
analyzing detailed dynamics in a large Chinese social net-
work, covering a period of 2 years when the network grew
from its first user to 19 million users and 199 million edges.
Rather than validate a single model of network dynamics,
we analyze dynamics at different granularities (per-user, per-
community, and network-wide) to determine how much, if
any, users are influenced by dynamics processes at differ-
ent scales. We observe independent predictable processes
at each level, and find that the growth of communities has
moderate and sustained impact on users.
In contrast, we
find that significant events such as network merge events
have a strong but short-lived impact on users, and they are
quickly eclipsed by the continuous arrival of new users.

Categories and Subject Descriptors
J.4 [Computer Applications]: Social and Behavioral Sci-
ences; H.3.5 [Information Storage and Retrieval]: On-
line Information Services

General Terms
Algorithms, Measurement

Keywords
Dynamic Graphs, Online Social Networks

1.

|,Data
p315-chen.pdf,|
Smart mobile handheld devices (MHDs) such as smartphones
have been used for a wide range of applications. Despite the
recent flurry of research on various aspects of smart MHDs,
little is known about their network performance in WiFi net-
works. In this paper, we measure the network performance
of smart MHDs inside a university campus WiFi network,
and identify the dominant factors that affect the network
performance. Specifically, we analyze 2.9TB of data col-
lected over three days by a monitor that is located at a gate-
way router of the network, and make the following findings:
(1) Compared to non-handheld devices (NHDs), MHDs use
well provisioned Akamai and Google servers more heavily,
which boosts the overall network performance of MHDs.
Furthermore, MHD flows, particularly short flows, bene-
fit from the large initial congestion window that has been
adopted by Akamai and Google servers.
(2) MHDs tend
to have larger local delays inside the WiFi network and are
more adversely affected by the number of concurrent flows.
(3) Earlier versions of Android OS (before 4.X) cannot take
advantage of the large initial congestion window adopted by
many servers. On the other hand, the large receive window
adopted by iOS is not fully utilized by most flows, poten-
tially leading to waste of resources. (4) Some application-
level protocols cause inefficient use of network and operat-
ing system resources of MHDs in WiFi networks. Our ob-
servations provide valuable insights on content distribution,
server provisioning, MHD system design, and application-
level protocol design.

Categories and Subject Descriptors
D.4.8 [Performance]: Measurements

General Terms
Performance, Measurement

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

Keywords
Smart Mobile Handhelds, WiFi Networks, Performance Mea-
surement

1.

|,Data
p159-magno.pdf,|
This paper presents a detailed analysis of the Google+ social net-
work. We identify the key differences and similarities with other
popular networks like Facebook and Twitter, in order to determine
whether Google+ is a new paradigm or yet another social network.
This work is based on large-scale crawls of over 27 million user
profiles that represented nearly 50% of the entire network in 2011.
We observe that the average path length between users is slightly
higher than other networks, possibly because Google+ is a new sys-
tem where relationships are still rapidly growing. Google+ shows
a higher level of reciprocity than Twitter, which also has directed
social links. The newly available places lived field could be used
to study how users are distributed around the world and how ag-
gressively the service has been adopted in different countries. We
find that Google+ is popular in countries with relatively low Inter-
net penetration rate. Based on the amount and types of information
publicly shared in user profiles, we also find that the notion of pri-
vacy varies significantly across different cultures.

Categories and Subject Descriptors
J.4 [Computer Applications]: Social and behavioral sciencesMis-
cellaneous; H.3.5 [Online Information Services]: Web-based ser-
vices

General Terms
Human Factors, Measurement

Keywords
Google+, Online Social Network, Geo-location

|,Data
p461-lumezanu.pdf,|
Spam is pervasive across many types of electronic commu-
nication, including email, instant messaging, and social net-
works. To reach more users and increase financial gain,
many spammers now use multiple content-sharing platforms
including online social networksto disseminate spam. In
this paper, we perform a joint analysis of spam in email and
social networks. We use spam data from Yahoos web-based
email service and from Twitter to characterize the pub-
lishing behavior and effectiveness of spam advertised across
both platforms. We show that email spammers that also ad-
vertise on Twitter tend to send more email spam than those
advertising exclusively through email. Further, we use DNS
lookup information to show that sending spam on both email
and Twitter correlates with a significant increase in cover-
age: spam domains appearing on both platforms are looked
up by an order of magnitude more networks than domains
using just one of the two platforms.

Categories and Subject Descriptors
C.2.3 [Computer-communication networks]: Network
Operations; K.4.2 [Computers and society]: Social is-
sues; H.0 [Information systems]: General

General Terms
Measurement, Security

Keywords
Twitter, email, spam, DNS, multiple platform spam

1.

|,Data
p287-gember.pdf,|
Network service providers, and other parties, require an ac-
curate understanding of the performance cellular networks
deliver to users.
In particular, they often seek a measure
of the network performance users experience solely when
they are interacting with their devicea measure we call
in-context. Acquiring such measures is challenging due to
the many factors, including time and physical context, that
influence cellular network performance. This paper makes
two contributions. First, we conduct a large scale measure-
ment study, based on data collected from a large cellular
provider and from hundreds of controlled experiments, to
shed light on the issues underlying in-context measurements.
Our novel observations show that measurements must be
conducted on devices which (i) recently used the network as
a result of user interaction with the device, (ii) remain in
the same macro-environment (e.g., indoors and stationary),
and in some cases the same micro-environment (e.g., in the
users hand), during the period between normal usage and
a subsequent measurement, and (iii) are currently sending/
receiving little or no user-generated traffic. Second, we de-
sign and deploy a prototype active measurement service for
Android phones based on these key insights. Our analysis
of 1650 measurements gathered from 12 volunteer devices
shows that the system is able to obtain average throughput
measurements that accurately quantify the performance ex-
perienced during times of active device and network usage.

Categories and Subject Descriptors
C.4 [Performance of Systems]: Measurement techniques;
C.2.3 [Computer-Communication Networks]: Network
OperationsNetwork monitoring

Keywords
Active measurement, Cellular network performance, Device
context

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
p253-gregori.pdf,|
In the last decade many studies have used the Internet AS-
level topology to perform several analyses, from discovering
its graph properties to assessing its impact on the effective-
ness of worm-containment strategies. Yet, the BGP data
typically used to reveal the topologies are far from being
complete. Our contribution is three-fold. Firstly, we anal-
yse BGP data currently gathered by RouteViews, RIS and
PCH route collectors, and investigate the reasons for its in-
completeness. We found that large areas of the Internet are
not properly captured due to the geographic placement of
the current route collector feeders and due to BGP filters,
such as BGP export policies and BGP decision processes.
Secondly, we propose a methodology to select the optimal
number of ASes that should join a route collector project
to obtain a view of the Internet AS level topology closer to
reality. We applied this methodology to the global AS-level
topology and to five regional AS-level topologies, highlight-
ing that the particular characteristics of the Internet at a re-
gional level cannot be ignored during this process. Thirdly,
we provide a characterization of the ASes that we found to
be part of at least one optimal solution set. By analysing
these ASes we found that the current route collector infras-
tructure is rarely connected to them, highlighting that much
more effort should be made in devising a route collector in-
frastructure that ideally would be able to capture a complete
view of the Internet.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network
Architecture and DesignNetwork topology; C.2.2 [Com-
puter-Communication Networks]: Network Protocols
Routing protocols; C.2.3 [Computer-Communication
Networks]: Network OperationsNetwork monitoring

Keywords
Autonomous Systems, BGP, incompleteness, Internet, mea-
surement, topology

1.

|,Data
p399-gursun.pdf,|
The ability of an ISP to infer traffic volumes that are not di-
rectly measurable can be useful for research, engineering, and
business intelligence. Previous work has shown that traffic ma-
trix completion is possible, but there is as yet no clear under-
standing of which ASes are likely to be able to perform TM
completion, and which traffic flows can be inferred.
In this paper we investigate the relationship between the AS-
level topology of the Internet and the ability of an individual
AS to perform traffic matrix completion. We take a three-stage
approach, starting from abstract analysis on idealized topolo-
gies, and then adding realistic routing and topologies, and fi-
nally incorporating realistic traffic on which we perform actual
TM completion.
Our first set of results identifies which ASes are best-
positioned to perform TM completion. We show, surprisingly,
that for TM completion it does not help for an AS to have
many peering links. Rather, the most important factor enabling
an AS to perform TM completion is the number of direct cus-
tomers it has. Our second set of results focuses on which flows
can be inferred. We show that topologically close flows are
easier to infer, and that flows passing through customers are
particularly well suited for inference.
Categories and Subject Descriptors
C.2.3 [Network Operations]: Network monitoring; C.2.5 [Local
and Wide-Area Networks]: Internet  BGP
Keywords
Interdomain Routing, Matrix Completion
1.
Interdomain traffic  the traffic flowing between autonomous
systems  is the fundamental workload of the Internet. It re-
flects global economic activity and information flow. Knowl-

|,Data
30_Sapirshtein.pdf,|. The Bitcoin protocol requires nodes to quickly distribute
newly created blocks. Strong nodes can, however, gain higher payoffs
by withholding blocks they create and selectively postponing their pub-
lication. The existence of such selfish mining attacks was first reported
by Eyal and Sirer, who have demonstrated a specific deviation from the
standard protocol (a strategy that we name SM1).
In this paper we investigate the profit threshold  the minimal fraction of
resources required for a profitable attack. Our analysis provides a bound
under which the system can be considered secure against such attacks.
Our techniques can be adapted to protocol modifications to assess their
susceptibility to selfish mining, by computing the optimal attack under
different variants. We find that the profit threshold is strictly lower than
the one induced by the SM1 scheme. The policies given by our algorithm
dominate SM1 by better regulating attack-withdrawals. We further eval-
uate the impact of some previously suggested countermeasures, and show
that they are less effective than previously conjectured.
We then gain insight into selfish mining in the presence of communication
delays, and show that, under a model that accounts for delays, the profit
threshold vanishes, and even small attackers have incentive to occasion-
ally deviate from the protocol. We conclude with observations regarding
the combined power of selfish mining and double spending attacks.

1

|,Non-data
07_Aly.pdf,|. We study the problem of securely building single-commodity
multi-markets auction mechanisms. We introduce a novel greedy algo-
rithm and its corresponding privacy preserving implementation using se-
cure multiparty computation. More specifically, we determine the quan-
tity of supply and demand bids maximizing welfare. Each bid is attached
to a specific market, but exchanges between different markets are allowed
up to some upper limit. The general goal is for the players to bid their
intended valuations without concerns about what the other players can
learn. This problem is inspired by day-ahead electricity markets where
there are substantial transmission capacity between the different mar-
kets, but applies to other commodity markets like gas. Furthermore, we
provide computational results with a specific C++ implementation of our
algorithm and the necessary MPC primitives. We can solve problems of
1945 bids and 4 markets in 1280 seconds when online/offline phases are
considered. Finally, we report on possible set-ups, workload distributions
and possible trade-offs for real-life applications of our results based on
this experimentation and prototyping.

1

|,Non-data
06_Barki.pdf,|. Most electronic payment systems for applications, such as
eTicketing and eToll, involve a single entity acting as both merchant
and bank. In this paper, we propose an efficient privacy-preserving post-
payment eCash system suitable for this particular use case that we refer
to, afterwards, as private eCash. To this end, we introduce a new partially
blind signature scheme based on a recent Algebraic MAC scheme due to
Chase et al. Unlike previous constructions, it allows multiple presentations
of the same signature in an unlinkable way. Using it, our system is the first
versatile private eCash system where users must only hold a sole reusable
token (i.e. a reusable coin spendable to a unique merchant). It also enables
identity and token revocations as well as flexible payments. Indeed, our
payment tokens are updated in a partially blinded way to collect refunds
without invading users privacy. By implementing it on a Global Platform
compliant SIM card, we show its efficiency and suitability for real-world
use cases, even for delay-sensitive applications and on constrained devices
as a transaction can be performed in only 205 ms.

Keywords: eCash, post-payment, refunds, partially blind signature,
anonymity, eToll, eTicketing, EVC.

1

|,Non-data
p199-abrahamsson.pdf,|
Today increasingly large volumes of TV and video are dis-
tributed over IP-networks and over the Internet. It is there-
fore essential for traffic and cache management to under-
stand TV program popularity and access patterns in real
networks.

In this paper we study access patterns in a large TV-on-
Demand system over four months. We study user behaviour
and program popularity and its impact on caching.

The demand varies a lot in daily and weekly cycles. There
are large peaks in demand, especially on Friday and Satur-
day evenings, that need to be handled.

We see that the cacheability, the share of requests that
are not first-time requests, is very high. Furthermore, there
is a small set of programs that account for a large fraction of
the requests. We also find that the share of requests for the
top most popular programs grows during prime time, and
the change rate among them decreases. This is important
for caching. The cache hit ratio increases during prime time
when the demand is the highest, and caching makes the
biggest difference when it matters most.

We also study the popularity (in terms of number of re-

quests and rank) of individual programs and how that changes
over time. Also, we see that the type of programs offered
determines what the access pattern will look like.

Categories and Subject Descriptors
C.4 [Computer Systems Organization]: Performance
of Systems; C.2.3 [Computer Systems Organization]:
Computer-Communication NetworksNetwork Operations

General Terms
Measurement, Performance

Keywords
IPTV, TV-on-Demand, Program Popularity

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Non-data
p265-flach.pdf,|
Initially, packet forwarding in the Internet was destination-based 
that is, a router would forward all packets with the same destination
address to the same next hop. In this paper, we use active probing
methods to quantify and characterize deviations from destination-
based forwarding in todays Internet. From over a quarter million
probes, we analyze the forwarding behavior of almost 40,000 in-
termediate routers. We find that, for 29% of the targeted routers,
the router forwards traffic going to a single destination via different
next hops, and 1.3% of the routers even select next hops in differ-
ent ASes. Load balancers are unlikely to explain these AS-level
variations, and in fact we uncover causes including routers inside
MPLS tunnels that otherwise employ default routes. We also find
that these violations can significantly affect the results of measure-
ment tools that rely on destination-based forwarding, and we dis-
cuss some ideas for making these tools more robust against these
violations.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Opera-
tionsNetwork management; C.2.6 [Computer-Communication
Networks]: InternetworkingRouters

Keywords
Routing, Measurements, Forwarding, Ping, Traceroute

1.

|,Data
p385-zarifzadeh.pdf,|
The objective of early network tomography approaches was
to produce a point estimate for the performance of each
network link (Analog tomography). When it became clear
that the previous approach is error-prone in practice, re-
search shifted to Boolean tomography where each link is
estimated as either good or bad. The Boolean approach
is more practical but its resolution is too coarse. We pro-
pose a new tomography framework that combines the best
of both worlds: we still distinguish between good and bad
links (for practicality reasons) but we also infer a range es-
timate for the performance of each bad link. We apply the
Range tomography framework in two path performance met-
ric functions (Min and Sum) and propose an efficient algo-
rithm for each problem. Together with simulations, we have
also applied Range tomography in three operational net-
works allowing us to identify the location of bad links and
to estimate their performance during congestion episodes.
We also compare the proposed method with existing Analog
and Boolean tomography algorithms.

Categories and Subject Descriptors
C.2.3 [Computer-communication Networks]: Network
OperationsNetwork management, Network monitoring; C.4
[Performance of Systems]: Measurement techniques

Keywords
Network tomography, localization, performance metric

1.

|,Non-data
p51-fusco.pdf,|
Network traffic archival solutions are fundamental for a num-
ber of emerging applications that require: a) efficient stor-
age of high-speed streams of traffic records and b) support
for interactive exploration of massive datasets. Compres-
sion is a fundamental building block for any traffic archival
solution. However, present solutions are tied to general-
purpose compressors, which do not exploit patterns of net-
work traffic data and require to decompress a lot of redun-
dant data for high selectivity queries. In this work we in-
troduce RasterZIP, a novel domain-specific compressor de-
signed for network traffic monitoring data. RasterZIP uses
an optimized lossless encoding that exploits patterns of traf-
fic data, like the fact that IP addresses tend to share a com-
mon prefix. RasterZIP also introduces a novel decompres-
sion scheme that accelerates highly selective queries target-
ing a small portion of the dataset. With our solution we can
achieve high-speed on-the-fly compression of more than half
a million traffic records per second. We compare RasterZIP
with the fastest Lempel-Ziv-based compressor and show that
our solution improves the state-of-the-art both in terms of
compression ratios and query response times without intro-
ducing penalty in any other performance metric.

Categories and Subject Descriptors
D.4.1 [Data]: Coding and Information Theory Data com-
paction and compression

General Terms
Algorithms

Keywords
Network monitoring, network traffic archives, data compres-
sion, NetFlow

 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not  made  or  distributed  for  profit  or  commercial  advantage  and  that 
copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy 
otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, 
requires prior specific permission and/or a fee. 
IMC12, November 1416, 2012, Boston, Massachusetts, USA. 
Copyright 2012 ACM  978-1-4503-1705-4/12/11...$15.00. 
 

1.

|,Non-data
p495-mirkovic.pdf,|
Network testbeds have become widely used in computer science,
both for evaluation of research technologies and for hands-on teach-
ing. This can naturally lead to oversubscription and resource alloca-
tion failures, as limited testbed resources cannot meet the increasing
demand.

This paper examines the causes of resource allocation failures
on DeterLab testbed and finds three main culprits that create per-
ceived resource oversubscription, even when available nodes exist:
(1) overuse of mapping constraints by users, (2) testbed software
errors and (3) suboptimal resource allocation. We propose solu-
tions that could resolve these issues and reduce allocation failures
to 57.3% of the baseline.
In the remaining cases, real resource
oversubscription occurs. We examine testbed usage patterns and
show that a small fraction of unfair projects starve others for re-
sources under the current first-come-first-served allocation policy.
Due to interactive use of testbeds traditional fair-sharing techniques
are not suitable solutions. We then propose two novel approaches
 Take-a-Break and Borrow-and-Return  that temporarily pause
long-running experiments. These approaches can reduce resource
allocation failures to 25% of the baseline case by gently prolonging
12.5% of instances. While our investigation is done on DeterLab
testbeds data, it should apply to all testbeds that run Emulab soft-
ware.

Categories and Subject Descriptors
C.2.1 [Computer Communication Networks]: Network Archi-
tecture and Design; C.2.3 [Computer Communication Networks]:
Network Operations

Keywords
network testbeds, Emulab, resource allocation

1.

|,Data
p273-canadi.pdf,|

Understanding the empirical characteristics of broadband perfor-
mance is of intrinsic importance to users and providers, and has
been a significant focus of recent efforts by the Federal Communi-
cations Commission (FCC) [9]. A series of recent studies have re-
ported results of empirical studies of broadband performance (e.g., [11,
15, 22]). In this paper, we reappraise previous empirical findings
on broadband performance. Our study is based on a unique cor-
pus of crowd-sourced data consisting of over 54 million individual
tests collected from 59 metropolitan markets over a 6 month pe-
riod by Speedtest.net. Following analytic approaches from prior
studies, our results confirm many of the raw performance results
(upload/download/latency) for ISPs in specific US markets. How-
ever, the size and scope of our data enable us to examine the details
of characteristics that were not identified in prior studies, thereby
providing a more comprehensive view of broadband performance.
Furthermore, we also report results of broadband performance char-
acteristics in 35 metropolitan markets outside of the US. This not
only provides an important baseline for future study in those mar-
kets, but also enables relative comparison of broadband perfor-
mance between markets world wide.

Categories and Subject Descriptors

C.2.3 [Network Operations]: Network management; C.4 [Performance
of Systems]: Performance attributes; C.4 [Performance of Sys-
tems]: Measurement techniques

General Terms

Experimentation, Measurement, Performance

Keywords

Access networks, Broadband access

1.

|,Data
p239-gursun.pdf,|

Characterizing the set of routes used between domains is
an important and difficult problem. The size and complex-
ity of the millions of BGP paths in use at any time can
hide important phenomena and hinder attempts to under-
stand the path selection behavior of ASes.
In this paper
we introduce a new approach to analysis of the interdomain
routing system designed to shed light on collective routing
policies. Our approach starts by defining a new metric for
distance between prefixes, which we call routing state dis-
tance (RSD). We show that RSD has a number of properties
that make it attractive for use in visualizing and analyzing
the state of the BGP system. Further, since RSD is a met-
ric, it lends itself naturally to use in clustering prefixes or
ASes. In fact, the properties of RSD allow us to define a
natural clustering criterion, and we show that this criterion
admits to a simple clustering algorithm with provable ap-
proximation guarantees. We then show that by clustering
ASes using RSD, one can uncover macroscopic behavior in
BGP that was previously hidden. For example, we show
how to identify groups of ASes having similar routing poli-
cies with respect to certain destinations, which apparently
reflects shared sensitivity to economic or performance con-
siderations. These routing patterns represent a considerable
generalization and extension of the notion of BGP atoms to
the case where routing policies are only locally and approx-
imately similar across a set of prefixes.

Categories and Subject Descriptors

C.2.3 [Network Operations]: Network monitoring; C.2.5
[Local and Wide-Area Networks]: Internet  BGP

Keywords

BGP, Interdomain Routing

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
p357-huang.pdf,|
Todays cellular systems operate under diverse resource constraints:
limited frequency spectrum, network processing capability, and hand-
set battery life. We consider a novel and important factor, hand-
set screen status, i.e., whether the screen is on or off, which was
ignored by previous approaches for optimizing cellular resource
utilization. Based on analyzing real smartphone traffic collected
from 20 users over five months, we find that off-screen traffic ac-
counts for 58.5% of the total radio energy consumption although
their traffic volume contribution is much smaller. Such unexpected
results are attributed to the unique cellular resource management
policy that is not well understood by developers, leading to cellular-
unfriendly mobile apps. We then make a further step by propos-
ing screen-aware optimization, by leveraging the key observation
that screen-off traffic is much more delay-tolerant than its screen-
on counterpart due to a lack of user interaction. Our proposal can
better balance the key tradeoffs in cellular networks. It saves up
to 60.92% of the network energy and reduces signaling and delay
overhead by 25.33% and 30.59%, respectively.

Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: wireless communica-
tion; C.4 [Performance of Systems]: design studies, performance
attributes

Keywords
Screen-off traffic, cellular network, traffic optimization, fast dor-
mancy, batching, LTE, radio resource optimization

1.

|,Data
25_Lang.pdf,|. Security Keys are second-factor devices that protect users
against phishing and man-in-the-middle attacks. Users carry a single de-
vice and can self-register it with any online service that supports the
protocol. The devices are simple to implement and deploy, simple to
use, privacy preserving, and secure against strong attackers. We have
shipped support for Security Keys in the Chrome web browser and
in Googles online services. We show that Security Keys lead to both
an increased level of security and user satisfaction by analyzing a two
year deployment which began within Google and has extended to our
consumer-facing web applications. The Security Key design has been
standardized by the FIDO Alliance, an organization with more than
250 member companies spanning the industry. Currently, Security Keys
have been deployed by Google, Dropbox, and GitHub. An updated and
extended tech report is available at https://github.com/google/u2f-
ref-code/docs/SecurityKeys_TechReport.pdf.

1

|,Non-data
p509-gyarmati.pdf,|

We study the problem of how to share the cost of a backbone
network among its customers. A variety of empirical cost-
sharing policies are used in practice by backbone network
operators but very little ever reaches the research literature
about their properties. Motivated by this, we present a sys-
tematic study of such policies focusing on the discrepancies
between their cost allocations. We aim at quantifying how
the selection of a particular policy biases an operators un-
derstanding of cost generation.

We identify F-discrepancies due to the specific function
used to map traffic into cost (e.g., volume vs. peak rate vs.
95-percentile) and M-discrepancies, which have to do with
where traffic ismetered (per device vs.
ingress metering).
We also identify L-discrepancies relating to the liability of
individual customers for triggered upgrades and consequent
costs (full vs. proportional), and finally, TCO-discrepancies
emanating from the fact that the cost of carrying a bit is not
uniform across the network (old vs. new equipment, high vs.
low energy or real estate costs, etc.).

Using extensive traffic, routing, and cost data from a tier-1
network we show that F-discrepancies are large when looking
at individual links but cancel out when considering network-
wide cost-sharing. Metering at ingress points is convenient
but leads to large M-discrepancies, while TCO-discrepancies
are huge. Finally, L-discrepancies are intriguing and esoteric
but understanding them is central to determining the cost a
customer inflicts on the network.

Categories and Subject Descriptors

C.2.3 [Computer-Communication Networks]: Network
Operations; J.4 [Social and Behavioral Sciences]: Eco-
nomics

Keywords

cost sharing, backbone network, network economics, fairness

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

|,Data
p329-jiang.pdf,|
The problem of overbuffering in the current Internet (termed
as bufferbloat) has drawn the attention of the research com-
munity in recent years. Cellular networks keep large buffers
at base stations to smooth out the bursty data traffic over
the time-varying channels and are hence apt to bufferbloat.
However, despite their growing importance due to the boom
of smart phones, we still lack a comprehensive study of
bufferbloat in cellular networks and its impact on TCP per-
formance. In this paper, we conducted extensive measure-
ment of the 3G/4G networks of the four major U.S. carriers
and the largest carrier in Korea. We revealed the severity
of bufferbloat in current cellular networks and discovered
some ad-hoc tricks adopted by smart phone vendors to mit-
igate its impact. Our experiments show that, due to their
static nature, these ad-hoc solutions may result in perfor-
mance degradation under various scenarios. Hence, a dy-
namic scheme which requires only receiver-side modification
and can be easily deployed via over-the-air (OTA) updates
is proposed. According to our extensive real-world tests, our
proposal may reduce the latency experienced by TCP flows
by 25%  49% and increase TCP throughput by up to 51%
in certain scenarios.

Categories and Subject Descriptors
C.2.2 [Computer-Communication Networks]: Network
Protocols

General Terms
Design, Measurement, Performance

Keywords
Bufferbloat, Cellular Networks, TCP, Receive Window

1.

|,Data
p427-pitsillidis.pdf,|
E-mail spam has been the focus of a wide variety of measure-
ment studies, at least in part due to the plethora of spam
data sources available to the research community. However,
there has been little attention paid to the suitability of such
data sources for the kinds of analyses they are used for. In
spite of the broad range of data available, most studies use
a single spam feed and there has been little examination
of how such feeds may differ in content. In this paper we
provide this characterization by comparing the contents of
ten distinct contemporaneous feeds of spam-advertised do-
main names. We document significant variations based on
how such feeds are collected and show how these variations
can produce differences in findings as a result.

Categories and Subject Descriptors
E.m [Data]: Miscellaneous; H.3.5 [Information Storage
and Retrieval]: On-line Information Services

General Terms
Measurement, Security

Keywords
Spam e-mail, Measurement, Domain blacklists

1.

|,Data
36_Vasek.pdf,|. In the cryptocurrency Bitcoin, users can deterministically
derive the private keys used for transmitting money from a password.
Such brain wallets are appealing because they free users from storing
their private keys on untrusted computers. Unfortunately, they also en-
able attackers to conduct unlimited offline password guessing. In this pa-
per, we report on the first large-scale measurement of the use of brain wal-
lets in Bitcoin. Using a wide range of word lists, we evaluated around 300
billion passwords. Surprisingly, after excluding activities by researchers,
we identified just 884 brain wallets worth around $100K in use from
September 2011 to August 2015. We find that all but 21 wallets were
drained, usually within 24 hours but often within minutes. We find that
around a dozen drainers are competing to liquidate brain wallets as
soon as they are funded. We find no evidence that users of brain wallets
loaded with more bitcoin select stronger passwords, but we do find that
brain wallets with weaker passwords are cracked more quickly.

Keywords: Bitcoin, brain wallets, passwords, cybercrime measurement

1

|,Data
33_Krombholz.pdf,|. We present the first large-scale survey to investigate how users ex-
perience the Bitcoin ecosystem in terms of security, privacy and anonymity. We
surveyed 990 Bitcoin users to determine Bitcoin management strategies and iden-
tified how users deploy security measures to protect their keys and bitcoins. We
found that about 46% of our participants use web-hosted solutions to manage at
least some of their bitcoins, and about half of them use exclusively such solu-
tions. We also found that many users do not use all security capabilities of their
selected Bitcoin management tool and have significant misconceptions on how to
remain anonymous and protect their privacy in the Bitcoin network. Also, 22% of
our participants have already lost money due to security breaches or self-induced
errors. To get a deeper understanding, we conducted qualitative interviews to ex-
plain some of the observed phenomena.

|,Data
p123-hu.pdf,|
Previous measurement-based IP geolocation algorithms have
focused on accuracy, studying a few targets with increasingly
sophisticated algorithms taking measurements from tens of
vantage points (VPs). In this paper, we study how to scale
up existing measurement-based geolocation algorithms like
Shortest Ping and CBG to cover the whole Internet. We
show that with many vantage points, VP proximity to the
target is the most important factor affecting accuracy. This
observation suggests our new algorithm that selects the best
few VPs for each target from many candidates. This ap-
proach addresses the main bottleneck to geolocation scala-
bility: minimizing traffic into each target (and also out of
each VP) while maintaining accuracy. Using this approach
we have currently geolocated about 35% of the allocated,
unicast, IPv4 address-space (about 85% of the addresses
in the Internet that can be directly geolocated). We visu-
alize our geolocation results on a web-based address-space
browser.

Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network
Architecture and DesignNetwork topology; C.2.5 [Computer-
Communication Networks]: Local and Wide-Area Net-
worksInternet; C.2.6 [Computer-Communication Net-
works]: Internetworking
General Terms: Experimentation, Measurement
Keywords: IP geolocation, IPv4

1.

|,Data
01_Park.pdf,|. Fraudulently posted online rental listings, rental scams, have
been frequently reported by users. However, our understanding of the
structure of rental scams is limited. In this paper, we conduct the first
systematic empirical study of online rental scams on Craigslist. This
study is enabled by a suite of techniques that allowed us to identify scam
campaigns and our automated system that is able to collect additional
information by conversing with scammers. Our measurement study sheds
new light on the broad range of strategies different scam campaigns em-
ploy and the infrastructure they depend on to profit. We find that many
of these strategies, such as credit report scams, are structurally differ-
ent from the traditional advanced fee fraud found in previous studies. In
addition, we find that Craigslist remove less than half of the suspicious
listings we detected. Finally, we find that many of the larger-scale cam-
paigns we detected depend on credit card payments, suggesting that a
payment level intervention might effectively demonetize them.

1

|,Data
p211-krishnan.pdf,|
The distribution of videos over the Internet is drastically
transforming how media is consumed and monetized. Con-
tent providers, such as media outlets and video subscrip-
tion services, would like to ensure that their videos do not
fail, startup quickly, and play without interruptions. In re-
turn for their investment in video stream quality, content
providers expect less viewer abandonment, more viewer en-
gagement, and a greater fraction of repeat viewers, resulting
in greater revenues. The key question for a content provider
or a CDN is whether and to what extent changes in video
quality can cause changes in viewer behavior. Our work
is the first to establish a causal relationship between video
quality and viewer behavior, taking a step beyond purely
correlational studies. To establish causality, we use Quasi-
Experimental Designs, a novel technique adapted from the
medical and social sciences.

We study the impact of video stream quality on viewer
behavior in a scientific data-driven manner by using exten-
sive traces from Akamais streaming network that include
23 million views from 6.7 million unique viewers. We show
that viewers start to abandon a video if it takes more than 2
seconds to start up, with each incremental delay of 1 second
resulting in a 5.8% increase in the abandonment rate. Fur-
ther, we show that a moderate amount of interruptions can
decrease the average play time of a viewer by a significant
amount. A viewer who experiences a rebuffer delay equal to
1% of the video duration plays 5% less of the video in com-
parison to a similar viewer who experienced no rebuffering.
Finally, we show that a viewer who experienced failure is
2.32% less likely to revisit the same site within a week than
a similar viewer who did not experience a failure.

Categories and Subject Descriptors
C.4 [Performance of Systems]: Measurement techniques,
Performance attributes; C.2.4 [Computer-Communication
Networks]: Distributed SystemsClient/server

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

Keywords
Video quality, Internet Content Delivery, User Behavior,
Causal Inference, Quasi-Experimental Design, Streaming Video,
Multimedia

1.

|,Data
p371-xu.pdf,|
Video telephony requires high-bandwidth and low-delay voice
and video transmissions between geographically distributed
users.
It is challenging to deliver high-quality video tele-
phony to end-consumers through the best-effort Internet.
In this paper, we present our measurement study on three
popular video telephony systems on the Internet: Google+,
iChat, and Skype. Through a series of carefully designed ac-
tive and passive measurements, we are able to unveil impor-
tant information about their key design choices and perfor-
mance, including application architecture, video generation
and adaptation schemes, loss recovery strategies, end-to-end
voice and video delays, resilience against random and bursty
losses, etc. Obtained insights can be used to guide the design
of applications that call for high-bandwidth and low-delay
data transmissions under a wide range of best-effort net-
work conditions.

Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Network
Protocols

Keywords
Measurement, Video conferencing

1.

|,Data
p185-li.pdf,|
In this paper, we examine mobile users behavior and their
corresponding video viewing patterns from logs extracted
from the servers of a large scale VoD system. We focus
on the analysis of the main discrepancies that might exist
when users access the VoD system catalog from WiFi or
3G connections. We also study factors that might impact
mobile users interests and video popularity. The users be-
havior exhibits strong daily and weekly patterns, with mo-
bile users interests being surprisingly spread across almost
all categories and video lengths, independently of the con-
nection type. However, by examining the activity of users
individually, we observed a concentration of interests and
peculiar access patterns, which allows to classify the user-
s and thus better predict their behavior. We also find a
skewed video popularity distribution and then demonstrate
that the popularity of a video can be predicted using its very
early popularity level. We further analyzed the sources of
video viewing and found that even if search engines are the
dominant sources for a majority of videos, they represen-
t less than 10% (resp. 20%) of the sources for the highly
popular videos in 3G (resp. WiFi) network. We report that
both the type of connections and mobile devices in use have
an impact on the viewing time and the source of viewing.
Based on our findings, we provide insights and recommen-
dations that can be used to design intelligent mobile VoD
systems and help improving personalized services on these
platforms.

Categories and Subject Descriptors
C.2.4 [Computer Applications]: Distributed application-
s; C.4 [Performance of Systems]: Measurement tech-
niques

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
IMC12, November 1416, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

General Terms
Measurement, Performance, Human Factors

Keywords
Mobile VoD, user behavior, video popularity, view source

1.

|,Data
29_Teutsch.pdf,|. Bitcoin and hundreds of other cryptocurrencies employ a
consensus protocol called Nakamoto consensus which reward miners for
maintaining a public blockchain. In this paper, we study the security
of this protocol with respect to rational miners and show how a minor-
ity of the computation power can incentivize the rest of the network
to accept a blockchain of the minoritys choice. By deviating from the
mining protocol, a mining pool which controls at least 38.2% of the net-
works total computational power can, with modest financial capacity,
gain mining advantage over honest mining. Such an attack creates a
longer valid blockchain by forking the honest blockchain, and the at-
tackers blockchain need not disrupt any legitimate non-mining trans-
actions present on the honest blockchain. By subverting the consensus
protocol, the attacking pool can double-spend money or simply create
a blockchain that pays mining rewards to the attackers pool. We show
that our attacks are easy to encode in any Nakamoto-consensus-based
cryptocurrency which supports a scripting language that is sufficiently
expressive to encode its own mining puzzles.

1

|,Non-data
09_Abadi.pdf,|. Private set intersection (PSI) protocols have many real world applica-
tions. With the emergence of cloud computing the need arises to carry out PSI on
outsourced datasets where the computation is delegated to the cloud. However,
due to the possibility of cloud misbehaviors, it is essential to verify the integrity
of any outsourced datasets, and result of delegated computation. Verifiable Com-
putation on private datasets that does not leak any information about the data is
very challenging, especially when the datasets are outsourced independently by
different clients. In this paper we present VD-PSI, a protocol that allows mul-
tiple clients to outsource their private datasets and delegate computation of set
intersection to the cloud, while being able to verify the correctness of the re-
sult. Clients can independently prepare and upload their datasets, and with their
agreement can verifiably delegate the computation of set intersection an unlim-
ited number of times, without the need to download or maintain a local copy of
their data. The protocol ensures that the cloud learns nothing about the datasets
and the intersection. VD-PSI is efficient as its verification cost is linear to the in-
tersection cardinality, and its computation and communication costs are linear to
the dataset cardinality. Also, we provide a formal security analysis in the standard
model.

1

|,Non-data
22_Vanrykel.pdf,|. Over the last decade, mobile devices and mobile applica-
tions have become pervasive in their usage. Although many privacy risks
associated with mobile applications have been investigated, prior work
mainly focuses on the collection of user information by application de-
velopers and advertisers. Inspired by the Snowden revelations, we study
the ways mobile applications enable mass surveillance by sending unique
identifiers over unencrypted connections. Applying passive network fin-
gerprinting, we show how a passive network adversary can improve his
ability to target mobile users traffic.
Our results are based on a large-scale automated study of mobile applica-
tion network traffic. The framework we developed for this study down-
loads and runs mobile applications, captures their network traffic and
automatically detects identifiers that are sent in the clear. Our findings
show that a global adversary can link 57% of a users unencrypted mobile
traffic. Evaluating two countermeasures available to privacy aware mo-
bile users, we find their effectiveness to be very limited against identifier
leakage.

1

|,Data
23_Krasnova.pdf,|. In many communication scenarios it is not sufficient to pro-
tect only the content of the communication, it is necessary to also protect
the identity of communicating parties. Various protocols and technolo-
gies have been proposed to offer such protection, for example, anonymous
proxies, mix-networks, or onion routing. The protocol that offers the
strongest anonymity guarantees, namely unconditional sender and recip-
ient untraceability, is the Dining Cryptographer (DC) protocol proposed
by Chaum in 1988. Unfortunately the strong anonymity guarantees come
at the price of limited performance and scalability and multiple issues
that make deployment complicated in practice.
In this paper we address one of those issues, namely slot reservation.
We propose footprint scheduling as a new technique for participants to
negotiate communication slots without losing anonymity and at the same
time hiding the number of actively sending users. Footprint scheduling is
at the same time simple, efficient and yields excellent results, in particular
in very dynamic networks with a frequently changing set of participants
and frequently changing activity rate.

Keywords: DC-net, scheduling, anonymity, slot-reservation

1

|,Non-data
20_Verheul.pdf,|. Recently an unlinkable version of the U-Prove attribute-
based credential scheme was proposed at Financial Crypto 14 [9]. Un-
fortunately, the new scheme is forgeable: if sufficiently many users work
together then they can construct new credentials, containing any set of
attributes of their choice, without any involvement of the issuer. In this
note we show how they can achieve this and we point out the error in
the unforgeability proof.

1

|,Non-data
34_McCorry.pdf,|. BIP70 is a community-accepted Payment Protocol standard
that governs how merchants and customers perform payments in Bitcoin.
This standard is supported by most major wallets and the two dominant
Payment Processors: Coinbase and BitPay, who collectively provide the
infrastructure for accepting Bitcoin as a form of payment to more than
100,000 merchants. In this paper, we present new attacks on the Payment
Protocol, which affect all BIP70 merchants. The Silkroad Trader attack
highlights an authentication vulnerability in the Payment Protocol while
the Marketplace Trader attack exploits the refund policies of existing
Payment Processors. Both attacks have been experimentally verified on
real-life merchants using a modified Bitcoin wallet. The attacks have been
acknowledged by both Coinbase and Bitpay with temporary mitigation
measures put in place. However, to fully address the identified issues will
require revising the BIP70 standard. We present a concrete proposal to
revise BIP70 by providing the merchant with publicly verifiable evidence
to prevent both attacks.

1

|,Non-data
p65-santiago.pdf,|

In this paper we present a software-based traffic classifica-
tion engine running on commodity multi-core hardware, able
to process in real-time aggregates of up to 14.2 Mpps over a
single 10 Gbps interface  i.e., the maximum possible packet
rate over a 10 Gbps Ethernet links given the minimum frame
size of 64 Bytes.

This significant advance with respect to the current state
of the art in terms of achieved classification rates are made
possible by:
(i) the use of an improved network driver,
PacketShader, to efficiently move batches of packets from
the NIC to the main CPU; (ii) the use of lightweight statis-
tical classification techniques exploiting the size of the first
few packets of every observed flow; (iii) a careful tuning
of critical parameters of the hardware environment and the
software application itself.

Categories and Subject Descriptors

C.2.3 [Network Operations]: Network Monitoring

Keywords

Statistical Identification, Commodity Hardware, Traffic Mon-
itoring

1.

|,Data
11_Kilinc.pdf,|. Secure two-party computation cannot be fair against mali-
cious adversaries, unless a trusted third party (TTP) or a gradual-release
type super-constant round protocol is employed. Existing optimistic fair
two-party computation protocols with constant rounds are either too
costly to arbitrate (e.g., the TTP may need to re-do almost the whole
computation), or require the use of electronic payments. Furthermore,
most of the existing solutions were proven secure and fair via a partial
simulation, which, we show, may lead to insecurity overall. We propose
a new framework for fair and secure two-party computation that can
be applied on top of any secure two party computation protocol based
on Yaos garbled circuits and zero-knowledge proofs. We show that our
fairness overhead is minimal, compared to all known existing work. Fur-
thermore, our protocol is fair even in terms of the work performed by
Alice and Bob. We also prove our protocol is fair and secure simulta-
neously, through one simulator, which guarantees that our fairness ex-
tensions do not leak any private information. Lastly, we ensure that the
TTP never learns the inputs or outputs of the computation. Therefore,
even if the TTP becomes malicious and causes unfairness by colluding
with one party, the security of the underlying protocol is still preserved.
|,Non-data
WEIS_2016_paper_14-1.pdf,| 

This paper examines how instances of identity theft that are sufficiently severe to induce 
consumers to place an extended fraud alert in their credit reports affect their risk scores, 
delinquencies, and other credit bureau variables on impact and thereafter. We show that for many 
consumers these effects are relatively small and transitory. However, for a significant number of 
consumers, especially those with lower risk scores prior to the event, there are more persistent 
and generally positive effects on credit bureau variables, including risk scores. We argue that 
these positive changes for subprime consumers are consistent with the effect of increased 
salience of credit file information to the consumer at the time of the identity theft. 

Keywords:  Inattention, identity theft, fraud alert, consumer protection, credit report, Fair and 

Accurate Credit Transactions Act (FACTA), propensity score matching 

JEL Codes:  D14, D18, G02 

 

* Contact: Julia Cheney, Payment Cards Center, Federal Reserve Bank of Philadelphia, Ten Independence Mall, 
Philadelphia, PA 19106; e-mail: julia.cheney@phil.frb.org. We wish to thank Dennis Carlson, Amy Crews Cutts, 
Bradley Dear, April Ferguson, and Henry Korytkowski of Equifax for assistance with the data. We thank Susan 
Herbst-Murphy, Blake Prichard, Peter Schnall, Chet Wiermanski, and Stephanie Wilshusen for helpful suggestions. 
We especially thank Loretta Mester for making this research possible. The views expressed here are those of the 
authors and do not necessarily reflect the views of the Federal Reserve Bank of Philadelphia or the Federal Reserve 
System. No statements here should be treated as legal advice. This paper is available free of charge at 
http://www.philadelphiafed.org/consumer-credit-and-payments/payment-cards-center/publications/. 

I. 

|,Data
WEIS_2016_paper_15-2.pdf,|. Many malicious and fraudulent endeavors on the web exhibit
characteristics of asymmetric conflict and guerrilla warfare. Defenders
work continuously to detect and take down malicious websites, while
attackers respond by resisting takedowns, evading detection, or creating
large numbers of new sites. This is reminiscent of the arcade game of
whack-a-mole  the faster the moles pop in and out of the holes, the
harder it becomes for the player to hit every one of them. In this work,
we present the Colonel Blotto Web Security (CBWS) framework to model
the asymmetric conflict and guerrilla warfare in web security. We find
that even with a resource asymmetry disadvantage, an attacker can still
realize significant utilities, provided that it can exploit an information
asymmetry in its favor. In some cases, an attacker can realize a high
utility with just a minimal number of websites that go undetected. In
other cases, an attacker may realize little if any utility even after creating
a large number of websites. The CBWS framework also allows us to
model the eects of competition among multiple attackers. We find that
competition weakens the eects of information asymmetry, and leads to a
degradation of attacker utilities, even as more malicious sites are created.

Keywords: Web Security, Colonel Blotto, Attacker Competition

1

|,Non-data
WEIS_2016_paper_19-1.pdf,|

We report on a set of 40 semi-structured interviews with information security executives and
managers at a variety of firms and government agencies. The purpose of the interviews was to
learn more about how organizations make cybersecurity investment decisions: how much support
they receive to execute their mission, how they prioritize which threats to defend against, and
how they choose between competing security controls. We find that most private sector execu-
tives believe that their firms adequately fund cybersecurity, but that finding qualified personnel
inhibits the pace of adoption of new controls. Most firms do not calculate return on investment
(ROI) or other outcome-based quantitative investment metrics; instead, they opt for process-
based frameworks such as NIST and COBIT to guide strategic investment decisions. Finally,
we note that CISOs in government face considerable challenges compared to their private-sector
counterparts.

1

|,Data
WEIS_2016_paper_20-1.pdf,|

I examine how firms strategically bundle news reports to offset the negative effects
of a privacy breach disclosure. Using a complete dataset of privacy breaches from 2005
to 2014, I find that firms experience a small and significant 0.27% decrease in their
stock price on average following the breaking news disclosure of the privacy breach.
But controlling for media coverage, this small decline is offset by an increase in the
effect of a larger than usual number of positive news reports released by the firm on
that day, which could increase the returns by 0.47% for every additional positive news
report compared to their usual media coverage. I further find that disclosure laws have
a significant and negative effect on the returns, even when news releases are used to
alleviate the decrease. Moreover, a portfolio constructed with breached firms controlling
for state disclosure laws outperforms the market over the 2007-2014 period, especially
in the case of breached firms in mandatory disclosure states.

Keywords: News Events, Media, Information, Market Efficiency, Security breaches,

Event Study, Risk analysis, Information Breach, Privacy, Market Valuation.

JEL Classification: D18, G10, G12, G14, G17, G30, K13, K20, L51, L86, M31, M37.

Department of Economics, University of Chicago, 1126 E. 59th Street, Chicago IL, 60637 USA. All

remaining mistakes are my own. E-mail address: sgay@uchicago.edu.
I am very grateful to Louis Serranito for excellent research assistance. I appreciate the helpful comments
of Alessandro Acquisti, Stephane Bonhomme, Pierre-Andre Chiappori, Victor Lima, John List, Paul Rubin,
one anonymous referee, and participants at the APEE conference, CIGO conference, and the George Mason
Law School Privacy and Data Security Conference.

1

1

|,Data
WEIS_2016_paper_21-2.pdf,|While the majority of security practice  and
spending  is focused on post-development products and
enterprise approaches, some have sought to change the focus of
security from the networks we manage to the systems we build.
The burgeoning Secure Software Engineering (SSE) commu-
nity has sought to identify and espouse activities, built upon
traditional software engineering, that address the |,Non-data
WEIS_2016_paper_22-5.pdf,|

Risk management lies at the core of information security. Professionals
need to assess risk and make decisions on how to treat risk. Risk per-
ception and judgement of individuals are inherently involved in this
process. This paper examines information security professionals at-
titude to risk. We conduct an online experiment and survey which
solicits preferences using risky lotteries. We also test whether framing
of decisions as gains, losses, or individually separated losses has an ef-
fect on their risk attitude. Framing is found to diversify professionals
risk behaviour significantly. Our findings suggest that professionals
reveal a preference for paying to reduce risk instead of paying to elimi-
nate it. They also prefer to reduce the expected loss of threat scenarios
rather than reducing the vulnerability associated with this loss. Over-
all, professionals are risk averse when they face lotteries with small
probabilities of loss and risk seeking for lotteries with large probabili-
ties.

1

|,Non-data
WEIS_2016_paper_33-2.pdf,|

The patching approach to security in the software industry has been less effective than desired.
One critical issue with the status quo is that the endowment of patching rights (the ability for a
consumer to choose whether security updates are applied) lacks the incentive structure to induce
better security-related decisions. In this paper, we establish how producers can differentiate their
products based on the provision of patching rights and how the optimal pricing of these rights can
segment the market in a manner that leads to both greater security and greater profitability. We
characterize the price for these rights, the discount provided to those who relinquish rights and
have their systems automatically updated, and the consumption and protection strategies taken
by users in equilibrium as they strategically interact due to the security externality associated
with product vulnerabilities. We quantify the effectiveness of priced patching rights, its impact
on welfare, and the ability of taxes to achieve an analogous effect in the open-source domain.
In this domain, we demonstrate why large populations of unpatched users remain even when
automatic updating is available, and then characterize how taxes on patching rights should
optimally be structured.

(cid:3)

Rady School of Management, University of California, San Diego, La Jolla, CA 92093-0553. Visiting IlJin

y

z

dddao@ucsd.edu@ucsd.edu

Professor, Korea University Business School, Seoul, Korea, 136-701. e-mail: taugust@ucsd.edu

Rady School of Management, University of California, San Diego, La Jolla, CA 92093-0553.

e-mail:

Korea University Business School, Seoul, Korea, 136-701. e-mail: kihoon@korea.ac.kr

1 |,Non-data
WEIS_2016_paper_38-2-1.pdf,|. The security of computers is a function of both their inher-
ent vulnerability and the environment in which they operate. Much as
with the public health of human populations, the public health of com-
puter populations can be studied in terms of what factors influence their
security. Using data collected from Microsoft Windows Malicious Soft-
ware Removal Tool (MSRT) running on more than one billion machines,
we conduct a multi-country analysis of malware infections and measures
of economic development, educational achievement, Internet infrastruc-
ture, and cybersecurity preparedness. We find that while increases in
these factors is often correlated with reduced infection rates, their signif-
icance and magnitude vary considerably. In contrast to past work, these
variations suggest that policy interventions, such as efforts to increase
the quality of home Internet connections, are likely to decrease infection
rates in only some circumstances.

Keywords: risk factor, malware, ecological study, public policy, cyber-
security, population health

1

|,Data
WEIS_2016_paper_4-1.pdf,| 

 
 

This paper presents a game theoretic analysis of the relationship between an information 
technology platforms market share, its level of security, and the extent to which malware 
creators (hackers) target a platform in order to proliferate via the platforms network 
externalities. In equilibrium, a platforms market share is shown to be the square root of the ratio 
of its competitors vulnerability to its own vulnerability. This implies that in order to maintain 
market share, platform leaders must make increasing investments in cybersecurity, thereby 
decreasing the platforms vulnerability.

|,Non-data
WEIS_2016_paper_48.pdf,|Free content and services on the Web are often
supported by ads. However, with the proliferation of intrusive
and privacy-invasive ads, a significant proportion of users have
started to use ad blockers. As existing ad blockers are radical
(they block all ads) and are not designed taking into account
their economic impact, ad-based economic model of the Web is
in danger today.

In this paper, we target privacy-sensitive users and provide
them with fine-grained control over tracking. Our working
assumption is that some categories of web pages (for example,
related to health, religion, etc.) are more privacy-sensitive to users
than others (education, science, etc.). Therefore, our proposed
approach consists in providing users with an option to specify
the categories of web pages that are privacy-sensitive to them
and block trackers present on such web pages only. As tracking
is prevented by blocking network connections of third-party
domains, we avoid not only tracking but also third-party ads.
Since users will continue receiving ads on web pages belonging
to non-sensitive categories, our approach essentially provides a
trade-off between privacy and economy. To test the viability of
our solution, we implemented it as a Google Chrome extension,
named MyTrackingChoices (available on Chrome Web Store).
Our real-world experiments with MyTrackingChoices show that
the economic impact of ad blocking exerted by privacy-sensitive
users can be significantly reduced.

Index TermsAd blocking; privacy; economy

I. |,Data
WEIS_2016_paper_54-2.pdf,|. While cybercrime has existed for many years and is still re-
ported to be a growing problem, reliable estimates of the economic impacts
are rare. We develop a survey instrument tailored to measure the costs of
consumer-facing cybercrime systematically, by aggregating dierent cost fac-
tors into direct losses and expenses for protection measures. We use our
instrument to collect representative primary data on the prevalence of seven
dierent types of consumer-facing cybercrime in six European countries. Our
results show that cybercrime rather causes losses of time than money and
that the losses of victims are dwarfed by the expenses for preventive pro-
tection. We identify scams to be the worst type of cybercrime in terms of
losses. While identity thefts associated with financial accounts cause high
initial losses for the victims, most of them receive substantial compensation.
We find that loss distributions are skewed to the left, bearing the risk of
overestimating costs when looking at figures summarized by the arithmetic
mean.

Keywords. Costs of cybercrime, Consumer research, Empirical measure-
ment

Corresponding author.

1

1 |,Data
WEIS_2016_paper_58.pdf,|. We present the first measurement study of JoinMarket, a
growing marketplace for more anonymous transfers in the Bitcoin ecosys-
tem. Our study reveals that this market is funded with multiple thousand
bitcoins and generated a turnover of almost 8 million USD over the course
of eight months. Assessing the resilience of the market against a well-
funded attacker, we discover that in a typical scenario, a selective attack
with 90 % success rate requires an investment of 32,000 USD (which is
recoverable after the attack). We formulate stylized economic models
of supply and demand to explain the existence of this novel market for
anonymity and underpin some theoretical arguments with empirical data.

1

|,Data
WEIS_2016_paper_68-2.pdf,|

Online and offline storage of digital currency present conflicting risks for a Bitcoin exchange. While

bitcoins stored on online devices are continually vulnerable to malware and other network-based attacks,

offline reserves are endangered on access, as transferring bitcoins requires the exposure of otherwise

encrypted and secured private keys. In particular, fluctuations in customer demand for deposited bitcoin

require exchanges to periodically refill online storage systems with bitcoins held offline. This raises the

natural question of what upper limit on online reserves minimizes losses due to theft over time.

In this paper, we investigate this optimization problem, developing a model that predicts the optimal

ceiling on online reserves, given average rates of deposits, withdrawals, and theft. We evaluate our theory

with an event driven simulation of the setup, and find that our equation yields a numerical value for the

threshold that differs by less than 2% from empirical results. We conclude by considering open questions

regarding more complex storage architectures.

1 |,Non-data
WEIS_2016_paper_7.pdf,|

We investigate how distributed denial-of-service (DDoS) attacks and other disrup-
tions aect the Bitcoin ecosystem. In particular, we investigate the impact of shocks on
trading activity at the leading Mt. Gox exchange between April 2011 and November
2013. We find that following DDoS attacks on Mt. Gox, the number of large trades on
the exchange fell sharply. In particular, the distribution of the daily trading volume be-
comes less skewed (fewer big trades) and had smaller kurtosis on days following DDoS
attacks. The results are robust to alternative specifications, as well as to restricting the
data to activity prior to March 2013, i.e., the period before the first large appreciation
in the price of and attention paid to Bitcoin.

1

|,Data
WEIS_2016_paper_71-1.pdf,|

In 2008 security forces in Colombia found that one of the companies hired to repair electric transmission
towers from guerrilla attacks had hired guerrilla members to demolish towers in their contracted area of service.
As a result, their business boomed as they were called often to repair electric towers. We model this problem as
a game between contractors and the power transmission company, we show how misaligned incentives enabled
contractors to profit by hiring guerrilla groups, and then model the changes to contracts that the transmission
company implemented in order to minimize the incentives for future contractors to collude with guerrilla members
in destroying electric towers.

I. |,Non-data
WEIS_2016_paper_76.pdf,|. Bug bounty programs offer a modern platform for organizations to
crowdsource their software security and for security researchers to be fairly re-
warded for the vulnerabilities they find. Little is known however on the incentives
set by bug bounty programs: How they drive new bug discoveries, and how they
supposedly improve security through the progressive exhaustion of discoverable
vulnerabilities. Here, we recognize that bug bounty programs create tensions, for
organizations running them on the one hand, and for security researchers on the
other hand. At the level of one bug bounty program, security researchers face a
sort of St-Petersburg paradox: The probability of finding additional bugs decays
fast, and thus can hardly be matched with a sufficient increase of monetary re-
wards. Furthermore, bug bounty program managers have an incentive to gather
the largest possible crowd to ensure a larger pool of expertise, which in turn
increases competition among security researchers. As a result, we find that re-
searchers have high incentives to switch to newly launched programs, for which
a reserve of low-hanging fruit vulnerabilities is still available. Our results in-
form on the technical and economic mechanisms underlying the dynamics of bug
bounty program contributions, and may in turn help improve the mechanism de-
sign of bug bounty programs that get increasingly adopted by cybersecurity savvy
organizations.

1

|,Data
WEIS_2016_paper_78-1.pdf,|

We set out to investigate how customers comprehend bank terms and condi-
tions (T&Cs).
If T&Cs are incomprehensible, then it is unreasonable to expect
customers to comply with them. An expert analysis of 30 bank contracts across
25 countries found that in most cases the contract terms were too vague to be
understood; in some cases they differ by product type, and advice can even be
contradictory. While many banks allow customers to write PINs down as long as
they are disguised and not kept with the card, 20% of banks do not allow PINs
to be written down at all, and a handful do not allow PINs to be shared between
accounts. We test our findings on 151 participants in Germany, the US and UK.
They mostly agree: only 35% fully understand the T&Cs, and 28% find that sec-
tions are unclear. There are strong regional variations: Germans find their T&Cs
particularly hard to understand, but Americans assume harsher T&Cs than they
actually are, and tend to be reassured when they actually read them.

1

|,Data
WEIS_2016_paper_34-4.pdf,|

This paper develops a model of data security investment in which concerns
about reputation - consumers belief about a firms security level - provide the
underlying incentive for the firm to invest. I consider two set-ups. In baseline
model, a website makes an unobserved one-time security investment that con-
sumers learn about over time. Underinvestment in security arises as the website
does not internalise consumers losses from data breaches; imperfect breach de-
tection further limits the consumers ability to punish the firm. In the extended
model, I introduce the consumers bank. Under this set-up, the overall level of
security is jointly determined by the websites and the banks investment levels.
I examine the implications of regulatory policies - specifically, mandatory breach
notification and a minimum security standard - targeted at raising the level of
security. I show that these well-intentioned policies may not always result in a
higher overall level of security, demonstrating the importance of accounting for
the agents strategic behaviours in regulatory interventions.

1

|,Non-data
WEIS_2016_paper_43_1.pdf,|The payment industry has been characterized
by a small number of players that operate the schemes for
the facilitation of credit and debit card payments. Over
the years, various initiatives have been taken in order to
increase competition and hence cost efficiency within the
industry. One of the latest efforts is the |,Data
