Acing the IOC Game: Toward Automatic Discovery and

Analysis of Open-Source Cyber Threat Intelligence

Xiaojing Liao1∗, Kan Yuan2∗, XiaoFeng Wang2, Zhou Li3, Luyi Xing2, Raheem Beyah1

1Georgia Institute of Technology, 2Indiana University Bloomington, 3ACM member

{xliao, rbeyah}@gatech.edu, {kanyuan, xw7, luyixing}@indiana.edu, lzcarl@gmail.com

ABSTRACT
To adapt to the rapidly evolving landscape of cyber threats, secu-
rity professionals are actively exchanging Indicators of Compro-
mise (IOC) (e.g., malware signatures, botnet IPs) through public
sources (e.g. blogs, forums, tweets, etc.). Such information, of-
ten presented in articles, posts, white papers etc., can be converted
into a machine-readable OpenIOC format for automatic analysis
and quick deployment to various security mechanisms like an in-
trusion detection system. With hundreds of thousands of sources
in the wild, the IOC data are produced at a high volume and veloc-
ity today, which becomes increasingly hard to manage by humans.
Efforts to automatically gather such information from unstructured
text, however, is impeded by the limitations of today’s Natural Lan-
guage Processing (NLP) techniques, which cannot meet the high
standard (in terms of accuracy and coverage) expected from the
IOCs that could serve as direct input to a defense system.

In this paper, we present iACE, an innovation solution for fully
automated IOC extraction. Our approach is based on the obser-
vation that the IOCs in technical articles are often described in a
predictable way: being connected to a set of context terms (e.g.,
“download”) through stable grammatical relations. Leveraging this
observation, iACE is designed to automatically locate a putative
IOC token (e.g., a zip ﬁle) and its context (e.g., “malware”, “down-
load”) within the sentences in a technical article, and further an-
alyze their relations through a novel application of graph mining
techniques. Once the grammatical connection between the tokens
is found to be in line with the way that the IOC is commonly pre-
sented, these tokens are extracted to generate an OpenIOC item
that describes not only the indicator (e.g., a malicious zip ﬁle) but
also its context (e.g., download from an external source). Running
on 71,000 articles collected from 45 leading technical blogs, this
new approach demonstrates a remarkable performance: it gener-
ated 900K OpenIOC items with a precision of 95% and a coverage
over 90%, which is way beyond what the state-of-the-art NLP tech-
nique and industry IOC tool can achieve, at a speed of thousands of
articles per hour. Further, by correlating the IOCs mined from the
articles published over a 13-year span, our study sheds new light on

1The two lead authors are ordered alphabetically.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978315

Figure 1: Example of OpenIOC schema.

the links across hundreds of seemingly unrelated attack instances,
particularly their shared infrastructure resources, as well as the im-
pacts of such open-source threat intelligence on security protection
and evolution of attack strategies.
1.
INTRODUCTION

According to Gartner, Cyber Threat Intelligence (CTI) is deﬁned
as “evidence-based knowledge, including context, mechanisms, in-
dicators, implications and actionable advice, about an existing or
emerging menace or hazard to assets that can be used to inform
decisions regarding the subject’s response to that menace or haz-
ard” [42]. Such knowledge is essential for an organization to gain
visibility into the fast-evolving threat landscape, timely identify
early signs of an attack and the adversary’s strategies, tactics and
techniques, and effectively contain the attack with proper means.
Given its importance, CTI has been aggressively collected and in-
creasingly exchanged across organizations, often in the form of In-
dicators of Compromise (IOC) [35], which are forensic artifacts
of an intrusion such as virus signatures, IPs/domains of botnets,
MD5 hashes of attack ﬁles, etc. Once collected, these IOCs can
be automatically transformed and fed into various defense mecha-
nisms (e.g., intrusion detection systems) when they are formatted
in accordance with a threat information sharing standard, such as
OpenIOC [9], that enables characterization of sophisticated attacks
like drive-by downloads. The challenge, however, comes from the
effective gathering of such information, which entails signiﬁcant
burdens for timely analyzing a large amount of data.
Finding IOCs online: challenges. While IOCs can be extracted
from traditional blacklists, like CleanMX [22] and PhishTank [37],
the information delivered by such IOCs is rather thin: only a small
number of IOC classes are covered (URL, domain, IP and MD5),
the relation between IOCs is not revealed and no context informa-
tion is provided (e.g., the criminal group behind malfeasance). An-
alyzing the cyber-attack campaigns and triaging incident responses
become quite difﬁcult when relying on such information. Instead,
IOCs from articles in technical blogs and posts in forums are more
favorable to security practitioners and extensively harvested, since
comprehensive descriptions of the attack are often found there. Such

<?xml version='1.0' encoding='UTF-8'?><OpenIOCxmlns:xsi= ……><description>This IOC detects ...</description><authored_by>@iocbucket</authored_by><authored_date>2014-02-07T14:35:14</authored_date><definition><Indicator id="1a0ee12e-dc16-4ae9-b508-d7728df81a5e" operator="OR"><IndicatorItemid="ac152ab7-370a-48cb-ad7a-042e9bdf4bc2" condition="contains" preserve-case="false" negate="false"><Context document="RegistryItem" search="RegistryItem/KeyPath"type="mir"/><Content type="string">Windows\CurrentVersion\Run</Content></IndicatorItem>……</Indicator>……</definition></OpenIOC>HeaderDefinition“Alina makes use of the HKCU\Software\Microsoft\Windows\CurrentVersion\Run registry key”755descriptions are typically informal, in natural languages, and need
to be analyzed semantically to recover related attack indicators, be-
fore they can be converted into the standard IOC format such as
OpenIOC, as illustrated in Figure 1. For years, this has been done
manually by security analysts. Increasingly, however, the volume
and velocity of the information generated from these sources be-
come hard to manage by humans in a cost-effective way. As an
example, in our research, we studied over 71,000 articles from
45 technical blogs extensively used by security professionals and
found that the number of articles posted here has grown from merely
a handful back 10 years ago to over 1,000 every month since last
year (see Section 4.1). Note that these blogs are just a drop in the
bucket: for example, Recorded Future is reported to utilize over
650,000 open web sources in 7 languages to harvest IOCs [41].
With the huge amount of information produced by those sources,
new technologies are in dire need to automate the identiﬁcation and
extraction of valuable CTI involved.

Automatic collection of IOCs from natural-language texts is chal-
lenging. Simple approaches like ﬁnding IP, MD5 and other IOC-
like strings in an article, as today’s IOC providers (AlienVault,
Recorded Future) do, does not work well in practice, which eas-
ily brings in false positives, mistaking non-IOCs for IOCs: e.g., as
illustrated in Figure 2, although three zip ﬁles show up in attack-
related articles, MSMSv25-Patch1.zip is clearly not an IOC
while the other two are. Further, even for a conﬁrmed IOC, we need
to know its context, e.g., whether it is linked to drive-by download
(ok.zip in the ﬁgure) or Phishing (clickme.zip), in order to
convert it to the IOC format and help an organization determine its
response. This can only be done by establishing a relation between
the IOC token and other content in the article, such as the terms
“downloads”, “attachment” in the example.

Identifying semantic elements (called Named Entity Recognition
or NER [33]) and extracting relations between them (called Rela-
tion Extraction, or RE [18]) have been extensively studied in the
Natural Language Processing (NLP) community. However, ex-
isting NLP techniques cannot be directly applied for IOC and its
context discovery. NER systems are known to be brittle, highly
domain-speciﬁc — those designed for one domain hardly work
well on the other domain [45]. So far, we are not aware of any
NER techniques developed speciﬁcally for recognizing IOCs and a
direct use of the state-of-the-art tools like Stanford NER [26] leads
to low precision (around 70%) and recall (less than 50%) (see Sec-
tion 4). Further, the current study on RE focuses on the relation
between two known entities, which are typically nominals [24],
whereas the relations between an IOC and the terms describing
its context (which we call context terms) are way more compli-
cated: e.g., “downloads” and ok.zip has a verb-noun relation.
Also important, the accuracy and coverage offered by the existing
RE tools are typically low. For example, the classic tree kernel ap-
proach [24] reports a precision between 50 and around 90% and a
recall between 10 and 50%. As another example, a recent proposal
for extracting events among genes and proteins from biomedical
literature [45] has a precision and recall around 50%. This level of
performance cannot meet the demand for high-quality IOCs, which
could go straight to a security system to offer immediate protection
for an organization.
iACE. A key observation from the open intelligence sources pro-
ducing high-quality IOCs is that technical articles and posts tend to
describe IOCs in a simple and straightforward manner, using a ﬁxed
set of context terms (e.g., “download”, “attachment”, “PE”, “Reg-
istry”, etc.), which are related to the iocterms used in OpenIOC to
label the types of IOCs [9]. Further, the grammatical connections
between such terms and their corresponding IOCs are also quite

Figure 2: Examples of sentences with/without IOC.

stable: e.g., the verb “downloads” followed by the nouns “ﬁle” and
ok.zip (the IOC) with a compound relation; “attachments” and
clickme.zip also with the compound relation. This allows us
to leverage such relations to identify an IOC and its context tokens,
combining the NER and RE steps together. To this end, we devel-
oped a new approach, called iACE (IOC Automatic Extractor), in
our research, which tailors NLP techniques to the unique features
of IOC discovery. More speciﬁcally, after preprocessing articles
(using topic-term classiﬁcation to ﬁnd those likely involving IOCs
and converting ﬁgures to text), iACE utilizes a set of regular expres-
sions (regex) and common context terms extracted from iocterms to
locate the sentences within the articles that contain putative IOC to-
kens, such as IP, MD5-like string. Within each of such sentences,
our approach attempts to establish a relation between the IOC token
and the context term: it converts the sentence into a Dependency
Graph (DG) to describe its grammatical structure and extracts the
shortest paths linking each pair of a context token and the putative
IOC token; over each path, a graph mining technique is applied to
analyze the relation between the tokens, which cannot be handled
by existing RE techniques, for the purpose of determining whether
they indeed include an IOC and its context.

This simple approach leverages the known “anchors” (context
and regex terms) to locate potential IOC-carrying sentences and
the predictable relations among them to capture IOCs, avoiding the
complexity of direct application of existing NLP techniques, which
needs to solve the NER ﬁrst to identify an IOC token before ad-
dressing the RE problem to ﬁnd its context. Our evaluations on a
prototype we built show that this new technique is very effective in
practice: it achieved a precision of 95% at a coverage over 90%. In
the meantime, our system is also highly efﬁcient, capable of ana-
lyzing four articles per second even with a single process.
Our discoveries. Running on all 71,000 articles collected from the
45 blogs (including AlienVault, Malwarebytes, Webroot, etc.) in
the past 13 years (from 2003/01 to 2016/04), iACE automatically
extracted 900K IOC tokens together with their context. By inspect-
ing these items and correlating them across different blogs over
the long time frame, we were able to gain an unprecedented un-
derstanding of the relations between different attacks reported, the
impact of open-source threat intelligence on attack evolutions, the
defense responses it triggered, as well as the qualities of these blogs
and effectiveness of the IOCs they document. More speciﬁcally, we
found that some apparently unrelated attack instances were actually
connected, sharing the same attack assets and infrastructures such
as command and control (C&C) hosts. Particularly, by linking to-
gether 396 articles and more than 7,000 IOCs, our study reveals that
a C&C campaign continued to evolve over a four-year span, chang-
ing the targets of exploits from one vulnerability (CVE-2010-1885)
to another (CVE-2013-0422). Also interestingly, we observed that
the attackers might adjust their strategies in response to the release
of IOCs: e.g., we found that the IOCs receiving intensive reports
tend to be short-lived, typically disappearing after a month.

On the other hand, organizations do not seem to react quickly to
the release of IOCs: it could take around 2 days for AV scanners to

IOC tokenContext term✓The Trojan downloads a file ok.zip from the server.detnsubjdetdobjcompoundcasedetnmod:from✓All e-mails collected have had attachments clickme.zip.✓It contains a shellcode at offset 3344 that downloads and execute a PE32 file from the server.•It’s available as a Free 30 day trial download.•Microsoft has already released an out-of-band patch MSMSv25-Patch1.zip•The malware does not modify AndroidManifest.xml in such a way.756include the hash of new malware and over 12 days for web scanners
to update their domain and IP blacklists, after related information
was made public by the blogs. Also, we found that a buffer over-
ﬂow vulnerability CVE-2012-0158 was ﬁrst reported by AlienVault
in April, 2012 for APT attacks on the military and aerospace indus-
try and then showed up again in an article on TrendMicro, Septem-
ber 2012, for an attack on political organizations; later the same
vulnerability was found in malware distribution (2013) and Spear
Phishing campaigns on the ﬁlm industry (2014) and banks (2015),
indicating that such a long-standing IOC was not adopted timely.

In terms of the qualities of open-source intelligence, we found
that Hexacorn and Naked Security often provide timely and com-
prehensive information about new attacks. For example, articles
from Naked Security ﬁrst reported three malicious name servers
m.sea.sy, mod.sea.sy and sea.sy under the control of the Syrian
Electronic Army for DNS hijacking. Also, some IOCs apparently
are more predictive than others. An example is the name server
“132.248.49.112”, which was reported by 19 blogs for the multiple-
theme Spear Phishing attack and remained unchanged for 140 days.
Contributions. The contributions of the paper are as follows:
• Novel IOC discovery technique. We present iACE, the ﬁrst fully-
automated technique for generating OpenIOC compatible, semantic-
rich intelligence, which addresses the emerging challenge in the
effective analysis of massive open-source data for timely CTI gath-
ering. Our approach leverages the unique features of IOCs and the
way they are described in mainstream technical articles to come up
with a specialized, scalable information extraction technique that
achieves high accuracy and coverage. Our evaluation shows that
iACE can effectively recover valuable attack indicators from popu-
lar technical blogs and convert it into industry-standard, machine-
readable threat intelligence, which cannot be done by any existing
techniques, to the best of our knowledge.
• New ﬁndings. Running iACE on over 71,000 articles from 45
most popular technical blogs across 13 years, our study sheds new
light on the effectiveness of such open-source intelligence exchange,
and the impact that it may have on the security industry and the
adversary’s strategies. The new understandings are invaluable for
improving the IOC release, discovery and utilization process, con-
tributing to the better protection of organizations’ information as-
sets.
2. BACKGROUND
2.1 Cyber Threat Intelligence
CTI gathering. As mentioned earlier, CTI is a collection of infor-
mation that details the current and emerging security threats, which
enables an organization to determine its responses at the strategic,
operational and tactical levels [42]. At the center of CTI are IOCs,
which elaborate the forensic artifacts of an attack and can therefore
be used to analyze the attack once it happens or counter it during
its execution. An IOC includes not only individual data ﬁngerprints
involved in a speciﬁc attack, such as an attack domain, the MD5 of
the malware delivered, but also the context of the attack and an
analysis of the adversary’s behavior, like the type of the attack or
the speciﬁc technique deployed (e.g., change to the Windows Reg-
istry). To ﬁnd out such information, CTI gathering includes identi-
ﬁcation of the adversary’s tools, techniques and attack procedures,
which, together with the ﬁngerprints, helps an organization’s secu-
rity team to understand their security posture, detect early signs of
threats and continuously improve their security controls.

The sources of CTI can be closed, e.g., a corporation’s internal
network traces, or public, such as technical blogs or online forums.
Examples of public sources include the blogs of AlienVault, Fire-

Table 1: Examples of iocterms and their corresponding general
type and context terms.

iocterms

context terms

General

type

IP

Hash

Email/ReceivedFromIP, PortItem/remoteIP

ServiceItem/serviceDLLsha1sum,

FileItem/Md5sum

Datetime

Email/Date, EventLogItem/genTime

email, IP, received,

remote, port

service, dll, sha1,

md5

email, date, event,

log, generation

Eye, Malwarebytes and Webroot. With the surge of cyber attacks in
recent years, a large number of attack artifacts have emerged, which
has been extensively reported by the public online sources and ag-
gressively collected by different organizations. To bootstrap this
research, we reached out to a security company and obtained a list
of 45 blogs which were operated by renowned organizations and
practitioners. These blogs altogether covered major security inci-
dents in the world and were consistently publishing veriﬁed IOCs.
We monitored these 45 blogs and were able to download as many
as 71,000 articles. Also noteworthy is that the number of IOC-
related articles there continued to grow in the past 13 years (2003/01
to 2016/04), from 20 to 1,000 per month, with an increased rate of
500% (see Figure 5a). While the volume of articles we have col-
lected is substantial, it only constitutes a small piece of the IOC
landscape. Rapidly collecting and sharing such information, and
deploying it to various security systems is the key to quickly detect-
ing, responding and containing different kinds of security threats
organizations face, which requires the descriptions of IOCs to be
standardized and machine digestible. To this end, multiple IOC
frameworks have been proposed, including OpenIOC [9], STIX [8]
and yara [12], with each format easily convertible to the others.
In our research, we utilized OpenIOC for automatic extraction of
CTIs from technical articles.
OpenIOC framework. OpenIOC is an extensible XML schema
created by Mandiant to record technical characteristics that iden-
tify a known threat, an attacker’s methodology, or other evidence
of a compromise. As an example, Figure 1 shows how to use the
OpenIOC format to describe a family of POS malware, Alina [30].
Such a description includes two components, a header and a def-
inition. The header part has a summary of the attack (under the
description tag) and the source of the information (under au
thored_by and authored_date). The deﬁnition contains a
set of indicator items (under Indicator Item), each represent-
ing an IOC (Content) and its context (Context). Two such
items are illustrated in Figure 1. In each of them, the document
attribute under Context gives the main category of an IOC (e.g.,
process, event, ﬁle, registry, etc.) and search elaborates on its
subcategory using an iocterm, essentially a concatenation of com-
mon terminologies (e.g., process, name, etc.) security profession-
als use to describe the IOC. The OpenIOC standard provides 600
such iocterms, covering various types of artifacts left by differ-
ent attacks, such as cookie history, DNS entry, Email informa-
tion, Hook items and others. The examples of iocterms related
to IP, hash and datetime are shown in Table 1. For example, In
the case of IP, its iocterms describe 15 different contexts, such
as spammer’s IP (Email/ReceivedFromIP), IP in malicious infras-
tructure (RouteEntryItem/Destination), or C&C attack’s remote IP
(ProcessItem/PortList/PortItem/remoteIP).

In Figure 1, the two indicator items record the registry change
made by the Alina malware:
the ﬁrst shows the location where
the change happens, i.e., the path Windows\CurrentVersion\Run
with the context RegistryItem/KeyPath, and the second identiﬁes
the name of the code dropped there, i.e., ALINA.

7572.2 Information Extraction

To collect IOCs automatically, we leveraged a set of NLP tools.
Also, due to the unique characteristics of this open problem, the
generic information extraction techniques cannot achieve good per-
formance for our tasks, and therefore we also applied graph mining
techniques to analyze relations between context terms and IOC an-
chors. These techniques are brieﬂy introduced here.
Dependency parsing. Dependency parsing is an NLP technique
for describing grammatical relations between words in a sentence.
Such relations include direct object, determinant, noun compound
modiﬁer and others. Also, the content of a relative clause is fur-
ther analyzed to identify the dependencies between the words it in-
cludes. For example, the sentence in Figure 2 is parsed into depen-
dency relations, such as the determinant relation between “trojan”
and “the”, det(Trojan-2, the-1) (where the number shows
the position of the word in the sentence), and the nominal-subject
relation between “trojan” and “download”, nsubj(downloads-3,
Trojan-2). Each of the formulae represents a binary relation be-
tween a governor (the ﬁrst term) and a dependent (the second one).
Such dependency relations within a sentence form a directed and
weighted graph (V, E, W ), where each token is a vertex in V , and
the relation between them is represented by a set of edges in E.
Each arc connecting two tokens can also be assigned a weight W
to differentiate the relation between them from others. Figure 2
further illustrates the dependency graph for the example sentence.
The state-of-the-art dependency parser (e.g., Stanford’s typed de-
pendencies system [20]) can achieve a 92.2% unlabeled attachment
score (UAS) in discovering the grammatical relations in a sentence.
In our research, we utilized the relations discovered by the parser
to capture the semantic links between context terms and an IOC to-
ken. For example, the dependency of clickme.zip on “attach-
ments” in the sentence “all e-mails collected have had attachments
clickme.zip” reveals a compound relation between the terms (the
content of the “attachment” is clickme.zip), which falls in line
with the descriptions typically explaining the security issues related
to email attachments.
Content term extraction. Another set of techniques extensively
utilized in information extraction is content term extraction, which
automatically determines important terms within a given piece of
text. It includes parts-of-speech (POS) tagging that labels a word
corresponding to a particular part of speech (such as nouns and
verbs), and phrase parsing that divides sentences into phrases logi-
cally belonging together. Speciﬁcally, after parsing phrases from
the given content, a POS tagger labels its terminological candi-
dates, such as syntactically plausible terminological noun phrases.
Then, these candidates are analyzed using statistical approaches
(e.g., point-wise mutual information) to ﬁnd out important terms.
Graph mining. Our approach tailors graph mining techniques to
analyze the dependency graphs constructed from sentences of in-
terest. Graph mining is a structural data mining problem with the
purpose of identifying distinguishing characteristics (such as com-
mon subgraph) of graphs. The problem can be stated as follows.
Given a dataset of graphs Gi ∈ D, with i = 1...N, each graph
Gi = (Vi, Ei) is a collection of vertices Vi = {vi1,··· , vin} and
edges Ei = {(va, vb)|va, vb ∈ Vi}. Gi may also have labels on
its nodes and edges, which are drawn from a common label set of
the whole dataset D. Also, each graph Gi is in a class with a label
ci ∈ C. The goal of the graph classiﬁcation problem is to learn a
model f : D → C that predicts the class label for any graph, that
is, classifying the graph to a class based on its similarity to other
graphs as measured by various graph kernel methods, such as di-

Figure 3: The architecture of iACE.

rect product kernel [27] and subtree kernel [39]. In our research,
we utilize graph mining to determine whether a sentence involving
context terms and an IOC token indeed contains IOC.

3.

IACE: DESIGN AND IMPLEMENTATION
Although the generic problem of information extraction from
natural language text is hard, particularly when high accuracy and
coverage are required (see Section 7), articles documenting IOCs
tend to utilize common technical terminologies (e.g., IP, process,
etc.) and present the artifacts and their context in a predictable
way (e.g., utilizing compound dependency, see the example in Fig-
ure 2), thereby making identiﬁcation of IOCs more manageable.
These unique features were fully leveraged in the design of iACE,
which utilizes high-conﬁdence context terms from iocterms and a
set of regexes to locate the sentences likely containing IOCs. Then
iACE analyzed the relations just between those anchors (the con-
text terms and the putative IOC token) against a model learned off-
line to accurately and efﬁciently capture IOCs. Below we present
the high-level design of this technique and explicate how it works
through an example.
Architecture. Figure 3 illustrates the architecture of iACE, in-
cluding a blog scraper (BS), a blog preprocessor (BP), a relevant-
content picker (RCP), a relation checker (RC) and an IOC generator
(IG). The BS ﬁrst automatically collects (“scrapes”) technical arti-
cles from different technical blogs, removing irrelevant information
(e.g., advertisements) from individual blog pages. These articles
are then inspected by the BP, using NLP techniques to ﬁlter out
those unlikely to contain IOCs. For each article considered to be
IOC-relevant, the RCP converts all its content, including pictures,
to text when possible, breaks the content into sentences and other
special content elements (tables and lists) and then searches among
the sentences for those likely involving IOCs with a set of context
terms and regexes, which describe the formats of IOCs such as IP
addresses (Section 3.1). The context terms here are automatically
extracted from iocterms and can also come from other manually
labeled sources, and the regex is built speciﬁcally for each IOC
type and its consistency with the content terms is ﬁrst checked be-
fore the sentence is selected. For each sentence, the RC analyzes
its grammatical structure connecting the context anchors (context
terms) and the IOC anchor (the string matched by the regex), using
a learned model to determine whether the latter is indeed an IOC,
and if so, mark the IOC and the context terms (Section 3.2). Using
such labeled content, the IG automatically creates the header and
the deﬁnition components, including the indicator items for all the
IOCs identiﬁed, according to the OpenIOC standard.
An example. Here, we use the excerpts of an article posted on
Trustwave (Figure 1) to go through the whole IOC extraction proce-
dure. From the title and other keywords identiﬁed from the article,

(cid:18)(cid:23)(cid:29)(cid:32)(cid:28)(cid:20)(cid:35)(cid:23)(cid:1)(cid:16)(cid:23)(cid:29)(cid:31)(cid:36)(cid:27)(cid:30)(cid:25)(cid:12)(cid:19)(cid:25)(cid:15)(cid:32)(cid:23)(cid:28)(cid:27)(cid:1)(cid:7)(cid:22)(cid:19)(cid:17)(cid:24)(cid:19)(cid:30)(cid:3)(cid:12)(cid:7)(cid:4)(cid:6)(cid:25)(cid:28)(cid:21)(cid:1)(cid:11)(cid:30)(cid:19)(cid:29)(cid:30)(cid:28)(cid:17)(cid:19)(cid:31)(cid:31)(cid:28)(cid:30)(cid:1)(cid:3)(cid:6)(cid:11)(cid:4)(cid:12)(cid:19)(cid:25)(cid:19)(cid:33)(cid:15)(cid:27)(cid:32)(cid:1)(cid:7)(cid:28)(cid:27)(cid:32)(cid:19)(cid:27)(cid:32)(cid:1)(cid:11)(cid:23)(cid:17)(cid:24)(cid:19)(cid:30)(cid:3)(cid:12)(cid:7)(cid:11)(cid:4)(cid:9)(cid:10)(cid:7)(cid:1)(cid:8)(cid:19)(cid:27)(cid:19)(cid:30)(cid:15)(cid:32)(cid:28)(cid:30)(cid:3)(cid:9)(cid:8)(cid:4)(cid:9)(cid:10)(cid:7)(cid:5)(cid:30)(cid:19)(cid:25)(cid:19)(cid:33)(cid:15)(cid:27)(cid:32)(cid:15)(cid:30)(cid:32)(cid:23)(cid:17)(cid:25)(cid:19)(cid:31)(cid:17)(cid:32)(cid:23)(cid:21)(cid:27)(cid:20)(cid:28)(cid:1)(cid:5)(cid:31)(cid:30)(cid:35)(cid:23)(cid:30)(cid:35)(cid:1)(cid:7)(cid:38)(cid:35)(cid:33)(cid:20)(cid:21)(cid:35)(cid:31)(cid:33)(cid:2)(cid:14)(cid:5)(cid:16)(cid:17)(cid:23)(cid:30)(cid:35)(cid:23)(cid:30)(cid:21)(cid:23)(cid:1)(cid:17)(cid:32)(cid:28)(cid:27)(cid:35)(cid:35)(cid:23)(cid:33)(cid:10)(cid:14)(cid:5)(cid:2)(cid:5)(cid:31)(cid:30)(cid:35)(cid:23)(cid:38)(cid:35)(cid:18)(cid:23)(cid:33)(cid:29)(cid:34)(cid:12)(cid:31)(cid:21)(cid:20)(cid:35)(cid:31)(cid:33)(cid:17)(cid:15)(cid:27)(cid:18)(cid:23)(cid:18)(cid:15)(cid:32)(cid:19)(cid:1)(cid:31)(cid:19)(cid:27)(cid:32)(cid:19)(cid:27)(cid:17)(cid:19)(cid:31)(cid:6)(cid:23)(cid:32)(cid:23)(cid:30)(cid:22)(cid:23)(cid:30)(cid:21)(cid:39)(cid:1)(cid:15)(cid:20)(cid:33)(cid:34)(cid:23)(cid:33)(cid:8)(cid:33)(cid:20)(cid:32)(cid:26)(cid:1)(cid:11)(cid:23)(cid:33)(cid:30)(cid:23)(cid:28)(cid:30)(cid:15)(cid:34)(cid:1)(cid:9)(cid:10)(cid:7)(cid:31)(cid:1)(cid:2)(cid:1)(cid:17)(cid:28)(cid:27)(cid:32)(cid:19)(cid:35)(cid:32)(cid:1)(cid:32)(cid:19)(cid:30)(cid:26)(cid:31)(cid:20)(cid:28)(cid:30)(cid:26)(cid:15)(cid:32)(cid:32)(cid:19)(cid:18)(cid:1)(cid:9)(cid:10)(cid:7)(cid:20)(cid:23)(cid:25)(cid:19)(cid:31)(cid:9)(cid:23)(cid:20)(cid:22)(cid:23)(cid:33)(cid:1)(cid:8)(cid:23)(cid:30)(cid:23)(cid:33)(cid:20)(cid:35)(cid:31)(cid:33)(cid:6)(cid:23)(cid:24)(cid:27)(cid:30)(cid:27)(cid:35)(cid:27)(cid:31)(cid:30)(cid:1)(cid:8)(cid:23)(cid:30)(cid:23)(cid:33)(cid:20)(cid:35)(cid:31)(cid:33)(cid:6)(cid:25)(cid:28)(cid:21)(cid:1)(cid:13)(cid:17)(cid:30)(cid:15)(cid:29)(cid:19)(cid:30)(cid:1)(cid:3)(cid:6)(cid:13)(cid:4)(cid:19)(cid:32)(cid:22)(cid:20)(cid:35)(cid:23)(cid:1)(cid:13)(cid:31)(cid:30)(cid:27)(cid:35)(cid:31)(cid:33)(cid:18)(cid:31)(cid:32)(cid:27)(cid:21)(cid:1)(cid:5)(cid:28)(cid:20)(cid:34)(cid:34)(cid:27)(cid:24)(cid:27)(cid:23)(cid:33)(cid:5)(cid:31)(cid:30)(cid:35)(cid:23)(cid:30)(cid:35)(cid:1)(cid:17)(cid:20)(cid:30)(cid:27)(cid:35)(cid:27)(cid:40)(cid:27)(cid:30)(cid:25)(cid:4)(cid:28)(cid:31)(cid:25)(cid:1)(cid:3)(cid:33)(cid:21)(cid:26)(cid:27)(cid:36)(cid:23)(cid:1)(cid:5)(cid:33)(cid:20)(cid:37)(cid:28)(cid:23)(cid:33)(cid:14)(cid:19)(cid:17)(cid:22)(cid:27)(cid:23)(cid:17)(cid:15)(cid:25)(cid:1)(cid:16)(cid:25)(cid:28)(cid:21)(cid:15)(cid:30)(cid:32)(cid:23)(cid:17)(cid:25)(cid:19)(cid:31)(cid:11)(cid:12)(cid:16)(cid:1)(cid:5)(cid:28)(cid:20)(cid:34)(cid:34)(cid:27)(cid:24)(cid:27)(cid:23)(cid:33)758using content term extraction (an NLP technique), iACE detects
that the article is likely to contain IOCs for a malware infection,
and then marks the link content and summarizes the article using
TextRank [32] to construct the header component. After that, it
converts the ﬁgures in the article into text using Tesseract [6], and
searches for the context terms and IOC token on each sentence.

Among all the sentences captured, the following one is found
to include a string ﬁtting the description of a key path of registry,
along with the context terms “registry” and “path”: “It includes the
addition of the registry value on the path HKCU\Software\Microsoft\
Windows\CurrentVersion\Run that virus use to maintain persistence.”
This sentence is then analyzed against the IOC recognition model
built through relation analysis, which conﬁrms that the connec-
tion across the terms and the IOC anchor here (“HKCU\Software\
Microsoft\Windows\CurrentVersion\Run” ) is indeed commonly used
to describe the key path of an injected registry item. As a result,
these terms and the IOC are labeled. From such content, the tech-
nical details of the attack reported by the article are automatically
extracted and presented in the OpenIOC format (Figure 1).
Assumptions. The current design of our system is for analyzing
technical blogs, under the assumptions that the blog writers present
their ﬁndings in a professional fashion. Such assumption is rea-
sonable intuitively, as the blogs are written by security profession-
als and their purpose is to quickly share technical details to their
peers1. It was also conﬁrmed by our analysis of the labeled dataset
DS-Labeled (see Section 4.1 for description). All their descrip-
tions of IOCs were found to be in line with our assumptions (Sec-
tion 3.1), involving professional terms and predictable grammatical
structures. On the other hand, the effectiveness of the technique on
other types of IOC open sources like online forums needs to be
further studied (Section 6).
3.1 Relevant Content Identiﬁcation

As mentioned earlier, to automatically extract IOCs from tech-
nical blogs, we ﬁrst need to scrape related web pages from blog
sites, pre-process their content and remove noise, before ﬁltering
out those irrelevant and identifying from the remaining articles the
sentences that may carry IOCs for the follow-up relationship check
and IOC extraction. Here we elaborate on these individual steps.
Blog scraping and pre-processing. Our blog scraper is essentially
a crawler designed to continuously monitor a list of technical blogs
to collect their articles. For each blog site, the BS ﬁrst scraps all its
existing articles before it is set to monitoring mode to look for new
ones. Speciﬁcally, the scraper performs breadth-ﬁrst crawling on
the blog, starting from its homepage to explore each link it discov-
ers, until no new link can be found. A problem here is that the web
pages gathered in this way may not all be articles, and may also
contain login pages, contact pages, and others. To automatically
remove these unrelated pages, we leverage two unique observa-
tions: the articles posted on the blog are all framed within the same
HTML template, which is very different from those used by other
pages such as login, contact, and others; also, on any blog site, more
article pages are hosted than other types of pages. Based on the ob-
servations, the BS compares each page’s DOM tree with those of
others to ﬁnd out a small set of pages framed over the templates
unlike the ones used by the majority of the pages. These pages
are considered to be outliers and dropped from the dataset scraped

1Note that asking those professionals to directly post formalized IOCs may
not be realistic in the near future, due to the complexity of manually creating
the content and their intent to get humans involved in the discussion. As a
supporting evidence, we examined all 45 blog sites and found only one of
them export IOCs in some articles (AlienVault).

from the blog site. In our implementation, the BS uses the python
library beautifulsoup [2] to extract from each page its HTML
template (with tags and attributes but no content) and groups the
pages using their templates’ hash values to identify those unrelated
to articles (the ones outside the group).

Even on those indeed relevant web pages, there still is content
unrelated to the technical articles, such as blog contributors’ pic-
tures, advertisements, featured content, etc. Such content needs
to be removed before the pages can be used for IOC identiﬁca-
tion. This purpose is served by another pre-processing step the
BS takes to get rid of such non-UGC (user-generated-content) data
from each page. Speciﬁcally, our approach compares all pages’
DOM trees to ﬁnd out the nodes with the non-UGC, character-
ized by their largely unchanged content across these pages (e.g.,
blog contributors’ photos will be the same across different arti-
cles). This is different from the article content generated by the
user, which varies signiﬁcantly between pages. Such a difference
is captured by an information-theoretic metric called composite im-
portance (CI) [47] that is used by the BS to measure the uniqueness
of each node on each page’s DOM tree with regards to other pages:
the nodes found to be less unique (rather common across different
pages) are discovered and dropped from the DOM tree, using an
algorithm proposed in the prior research [47].

Under such a “sanitized” DOM tree, still some content cannot
be directly analyzed by our text-based approach, including images
and embedded PDF ﬁles. So, the last pre-processing step is to con-
vert such content into text, if it indeed involves text information.
To this end, we incorporated into our implementation of an optical
character recognition engine Tesseract. Tesseract [6] is capable of
discovering texts within an image with an accuracy of 99% in gen-
eral. However, for the images collected from blogs, we found that
the accuracy went down to merely 70%, due to the low quality of
the images and the non-dictionary words they often have. To ad-
dress this issue, we used Gimp [4] to resize the blog image a for
better image quality, and add to Tesseract the new words (e.g., IP,
MD5, HTTP) discovered from the text of an article. We tested this
approach on 100 randomly sampled images from 10 blogs, which
was found to push the accuracy of Tesseract to above 95%.
Topic ﬁltering. Once all the article pages have been selected and
pre-processed, we start looking into their content, ﬁrst removing
those not including any IOCs. Examples of such articles are those
for product promotion, news or software update, which we call non-
IOC articles. To separate the non-IOC articles from those with
IOCs (called IOC articles), iACE runs the TC, a classiﬁer using a
set of features as described below:
• Topic words: Intuitively, an IOC article focuses on security risks
and vulnerabilities, whose theme is reﬂected by its topic terms (e.g.,
malware, exploit). These terms are less likely to appear in a non-
IOC article. Topic term extraction is an extensively studied NLP
technique. In our implementation, we utilized an open-source tool
topia.termextract [10] to collect the terms within each ar-
ticle. The top 20 terms discovered, together with their frequencies,
are part of the features for the classiﬁcation.
• Article length: Since the blog sites are meant to be the channels
for IOC exchanges, the IOC article it contains tends to be longer,
including detailed descriptions of IOCs and their context, while
non-IOC articles in technical blogs are often news and digests, and
hence tend to be shorter.
• Dictionary-word density: Compared with non-IOC articles, IOC
articles tend to have a lower dictionary-word density, because most
IOCs are non-dictionary words (e.g., IP, hash values, ﬁle path). Our
implementation employs the enchant library [15] to ﬁnd dictio-

759General type

IPv4
hash

int number
ﬂoat number

Table 2: Examples of regexes.

Regex

(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.)\{3\}(?:25[0-5]
|2[0-4][0-9]|[01]?[0-9][0-9]?)(/([0-2][0-9]|3[0-2]|[0-9]))?
\b[a-fA-F\d]{32}\b|\b[a-fA-F\d]{40}\b|\b[a-fA-F\d]{64}\b
((?<=[\s([\s\(\[\{:\+\-=\\])|^)(\+|-)?\d+(?!([a-zA-Z0-9]|\.\S))

\+\-=\\])|^)(\+|-)?(\d+)?\.(\d+)(?!([a-zA-Z0-9]|\.\S))

((?<=[\s\(\[\{:

nary words in an article. Then, its density within the article is cal-
culated as the ratio of the dictionary words with regards to all the
words the article contains.

Using these features, we ran a support vector machine (SVM)
to train a classiﬁcation model over DS-Labeled, including 150
IOC articles and 300 non-IOC articles. The model was evaluated
through a 10-fold cross validation, and found to achieve a precision
of 98% and a recall of approximate 100%. We further used this TC
to analyze all 71,000 articles from the unknown set DS-Unknown
(described in Section 4.1) and manually validated 500 instances
randomly selected from the classiﬁed results (Section 4.2). The
validation shows that the classiﬁer had a high accuracy (96%) and
coverage (99%).
IOC sentence identiﬁcation. From each IOC article, the relevant
content picker identiﬁes the sentences, tables, and lists likely to in-
clude IOCs before they are further evaluated by the relation checker
(Section 3.2). Such content is selected from the article based on
the anchors they contain, i.e., context terms and putative IOC to-
kens. Speciﬁcally, the RCP parses the HTML content of each arti-
cle, breaking the text into sentences and detecting tables and lists.
From each sentence, we look for the presence of both the string
matched by the regex and its compatible context terms: e.g., “hash”
goes with an MD5 checksum. From tables and lists, we increase the
search scope for context terms also to table headers, captions and
the nearest sentences (see supplementary material). Figure 2 shows
an example.

The OpenIOC standard speciﬁes 600 categories of IOCs using a
list of iocterms [9]. We further summarized these IOCs into 19 dif-
ferent data types, including IP, hash, int, float, and others.
The regex for each of these categories was manually constructed
and carefully examined. As we can see here, such expressions
could introduce false positives (e.g., the string type matching many
IOC strings, even though oftentimes, we only seek non-dictionary
words), if we do not also look at the context terms and the rela-
tions between identiﬁed tokens. These terms are automatically col-
lected from the 600 iocterms through tokenizing dictionary-word
elements within each iocterm and by removing common terms like
“item” . We further used Semanticlink [14] to recover other seman-
tic related terms, e.g., “portable executable” (related to PE).

Altogether, we gathered 5,283 context terms in our research,
which were found to indeed provide good coverage of the com-
mon terminologies used by technical blogs. In our research, we
gathered 80 public IOC ﬁles and their corresponding blog articles
(according to their description tags) from labeled dataset. By man-
ually inspecting the sentences carrying the IOCs, we found that all
such sentences also contain at least one context term and all such
terms are on the list we created.
3.2 Relation Checking and IOC Creation

Although context terms and regexes can ﬁnd us the sentences
likely involving IOCs, they are insufﬁcient for detecting true IOCs
with high accuracy. For example, Figure 2 shows four sentence
pairs collected from two articles posted on AlienVault. As we can
see, each pair contains both context terms, like “download”, and the
strings ﬁtting the descriptions of their corresponding IOCs, such as
“3344”; however, the ﬁfth one does not include any real IOC while

the third one does. Fundamentally, the coincidence of relevant to-
kens (context term and IOC token) does not necessarily indicate
the presence of an IOC-related description. Most important here
is the consistency between the relation of these tokens and what is
expected from the narrative of IOCs: in the above example, 3344
is supposed to be the offset, not the PE ﬁle. Actually, in the case
that such a relation is incorrect, even when the IOC extracted is in-
deed an attack indicator, its context can be wrong and as a result,
the OpenIOC record created can be erroneous. The third sentence
in the ﬁgure is such an example.

As mentioned earlier, identifying IOCs is essentially an NER
problem and connecting them to their context is an RE issue. In the
NLP community, solutions to the problems are pipelined: individ-
ual name entities within a sentence are ﬁrst recognized (i.e., NER)
and then their relation is established (i.e., RE). For our problem,
however, this pipeline becomes unnecessary, since the putative to-
kens for an IOC and its context are already located in a sentence,
and all we need to do is check the consistency of their relation with
what is expected to happen between them. In other words, we are in
the position to address both the NER and RE together. On the other
hand, existing RE techniques are inapplicable here, because they
are designed to work on the nominal relation between two nouns,
whereas the links between an IOC and its context terms are more
diverse, including nominal, verb and adjective (e.g., “attachment”,
“download” and “injected”). To handle such diverse relations, we
came up with an idea that models the analysis on the grammatical
connection between the IOC candidate and its context as a graph
mining problem [23]. This allows us to apply graph similarity
comparisons to detect the presence of the desired relation, which
achieves both a high accuracy (95%) and a high coverage (above
90%) that the more generic RE techniques cannot attain [24]. Be-
low we present how the approach works.
Relation representation. To analyze the relation between an IOC
candidate and a context term, our approach ﬁrst uses a dependency
parser to transform a sentence into a dependency graph (DG). A
DG describes the grammatical connections between different lin-
guistic tokens, with verbs being the structural center of a clause and
other tokens directly or indirectly depending on the center. Such a
dependency describes the grammatical connection among tokens
such as direct object, determinant, noun compound modiﬁer, etc.
Formally, a DG is a directed and weighted graph g = (V, E, W ),
where words in the sentence are nodes V , two related nodes are
connected by an edge E and the speciﬁc grammatical relation link-
ing them together is modeled as an edge weight W . In our research,
the DG was constructed using the Stanford dependency parser [20],
the state-of-the-art open-source tool that converts text to simpliﬁed
grammatical patterns suitable for relations extraction.

Unlike the more generic RE techniques, which work on the DG
of the whole sentence, our approach takes advantage of the known
anchors to focus on the smallest dependency graph gni,nc connect-
ing an IOC candidate ni and a context term nc together. This is
because the information carried by this subgraph (called core) is
most relevant to the understanding of the relations between the an-
chors, which is all we care about (see an example in Figure 4).
Note that we also add negation dependencies as child nodes or sib-
ling nodes of the nodes on the core to capture IOC-related negative
descriptions. As an example, in the fourth sentence in Figure 2,
the relation between the context term “modify” and the IOC token
“AndroidManifest.xml” is affected by the word “not”, which
adds a negation dependency on the verb “modify”.
Similarity comparison. Over a dependency subgraph gni,nc mod-
eling the relation between a context term and an IOC candidate, we

760Figure 4: Workﬂow of the relation checker (RC).

E = {(u1, u2), (v1, v2) ∈ V × V : (u1, v1) ∈ E1, (u2, v2) ∈ E2}

W =(cid:26) 1,

if W1(u1, v1) = W2(u2, v2)

0, otherwise

want to ﬁnd out whether another subgraph is similar, which indi-
cates that the same relation also exists between its anchors. This
similarity comparison is important, since it is the foundation for
classifying a sentence, determining whether IOC relations are in-
deed there to bind anchors or they are not. Given the fact that
now we only need to work on simple subgraphs with a few labeled
nodes (see Figure 4), the focus is to compare the paths linking cor-
responding nodes (with identical labels) across the subgraphs. For
this purpose, we customize a well-known graph mining technique,
called direct product kernel, a function that measures the similar-
ity of two graphs by counting the number of all possible pairs of
arbitrarily long random walks with identical label sequences [27].
Speciﬁcally, let g1 = (V1, E1, W1) and g2 = (V2, E2, W2) be
directed weighted graphs. The direct product between g1 and g2 is
a directed weighted graph G = g1 × g2 = (V, E, W ), where

V = {(v1, v2) ∈ V1 × V2}

In other words, for each pair of nodes (u1, u2) and (v1, v2) in
the new graph G, they are adjacent (connected by a directed edge)
if and only if an edge with the same direction and weight exists
both between u1 and v1 in g1 and between u2 and v2 in g2.
It
is important to note that we bring the weights into the product to
compare the type of the grammatical relation between two nodes
(words): only when the adjacent word pairs in two sentences have
the same grammatical relation, will that relation be preserved in
the new graph. Usually, it takes O(|V |4) to compute this direct
product. However, all the subgraphs we consider are just “paths”
(after removing the directions) between a context term and an IOC
candidate, whose direct product can be computed in O(|V |2).
tween the two subgraph g1 and g2 as follows:

Over the direct-product graph G, we calculate the similarity be-

k(g1, g2) =(cid:88)i,j

[

∞(cid:88)l=0

λlAl]i,j =(cid:88)i,j

[(I − λA)−1]i,j

1

1

where each entry of Al is the number of the walks of length l from
vi to vj in the direct product graph G = g1 × g2, hence Al can
be calculated as the l-th power of the adjacency matrix of G; λ
min(di,do) with di, do being the
is the decay constant and λ ≤
maximum in-degree and out-degree of G, representatively. In our
min(di,do) .
research, we set λ =
Classiﬁcation. Based on our customized direct-product kernel, we
run a classiﬁer (the relation checker) to determine the presence of
IOC relations between a context term and an IOC candidate within
a sentence. The classiﬁer is trained over DS-Labeled with 1500
positive instances and 3000 negative instances. From the dataset, a
model is learned to work on a kernel vector generated from the fea-
tures of a subgraph gi: (m1,··· , mj,··· ), where mj = k(gi, tj)

and tj is the subgraph for the jth instance in the training set. In
other words, each new instance gi is classiﬁed into the positive set
(with the IOC relation) or the negative set (without the relation)
based on its similarity with every instance in the labeled set. This
classiﬁcation is executed efﬁciently with multi-threading in our im-
plementation.

The classiﬁer can be trained with different kinds of machine
learning algorithms. Our implementation utilizes logistic regres-
sion, since the algorithm works well on a small labeled dataset.
More speciﬁcally, it optimizes the log likelihood function to deter-
mine a probability that is a logistic function of a linear combination
of the training dataset, and every training point has a certain inﬂu-
ence on the estimated logistic regression function. In our research,
we compared the recall and precision of ﬁve classiﬁcation models
on the labeled dataset through a 5-fold cross-validation. With the
regularization parameter set to 3.0, our logistic regression classiﬁer
yielded the best results. On unknown set DS-Unknown , this clas-
siﬁcation model achieved a high accuracy (with a precision of 95%
and recall of 90%).
IOC creation. After identifying the IOC and its corresponding
context terms, our IOC generator can automatically convert the
CTI content of a technical blog to the OpenIOC record. Speciﬁ-
cally, each indicator item in the record is created by ﬁlling in the
search attribute in the Context tag with a context term and the
Content tag with its corresponding IOC. The content of other
ﬁelds on the item can be derived from these two ﬁelds. For exam-
ple, the type attribute of the Content tag and the document
attribute of the Context tag are actually the iocterms of the IOC
discovered.

For the header of the record, the IG generates the content for the
Description tag using the open-source automatic summariza-
tion tool TextRank to create a summary for the blog article. Also,
the original blog link address is used to ﬁll the link tag, and the
content of authored_by and authored_date tag are set to
our prototype name and the ﬁle generation time.

4. EVALUATION
4.1 Settings

In our study, we ran our implementation of iACE to automati-
cally analyze 71,000 real-world technical articles, on an R730xd
server with 40 of Intel Xeon E5-2650 v3 2.3GHz, 25M Cache
CPUs and 16 of 16GB memories. Here we explain the datasets
used in the study and the parameter settings of the system.
Datasets. We utilized two datasets in our study: a labeled set for
training our topic classiﬁer (Section 3.1) and relation checker (Sec-
tion 3.2), and an unknown set for evaluating our technique.
• Labeled dataset (DS-Labeled). The dataset contains 80 IOC
ﬁles and their corresponding blog articles. These IOC ﬁles were
collected in our research from two public feeds, iocbucket [29] and
openiocdb [1], both providing threat intelligence through OpenIOC

!",$%,!",$%,…,!(",$%)The	Trojan	downloadsa	file	ok.zipfrom	the	server.2.	Extract	Shortest	Path1.	Generate	Dependency	Graph!($%,$%)!($%,$))…!($),$%)!($),$))…⋮⋮⋱!($%,$,)!($),$,)⋮!($,,$%)!($,,$%)…!($,,$,)3.	Generate	Kernel	Matrices-./0123345467v•IOC:	ok.zip•Context	term:	downloadsPositiveNegativeTrainingModel4.	PredictingDirect	Product	KernelPredictive	Matrixk(g1,g2)=Xi,j[(I  A) 1]i,j761items. Under the description tags of these items, we found
the links pointing to these items’ sources and recovered 150 arti-
cles from 22 blogs, including their HTML pages and image ﬁles.
Also, we manually gathered, from the same blogs, 300 other arti-
cles. Each of them was manually checked to ensure that they do
not have any IOCs but contain some IOC-like strings (e.g., IP ad-
dresses, MD5, etc.). Most of them are technical news or articles for
product promotion.

1

1

min(di,do) otherwise.

From these articles, we further extracted two kinds of sentences,
those with IOCs (true IOC sentences) and those without but involv-
ing IOC-like strings (false IOC sentences). More speciﬁcally, the
1,500 true IOC sentences were identiﬁed using the context terms
and IOC tokens speciﬁed in the OpenIOC items we collected, and
further manually inspected to ensure their correctness. The 3,000
false IOC sentences were those matched by the regular expressions
and context terms used by iACE (Section 3.1) but did not include
any IOCs, as conﬁrmed by a manual check.
• Unknown set (DS-Unknown). As mentioned earlier, the dataset
used for evaluating our system was gathered from 45 security-related
technical blogs. These blogs are well recognized to be the leading
sources for CTI collection, which includes AlienVault, Malware-
bytes, and others (see supplementary material for the full list). On
these blogs, we ran a crawler that scraped 71K articles posted there
between 2003/04 and 2016/05. Figure 5a shows the increase of the
number of the articles per month on these blogs over 13 years.
Parameter settings. In the experiments, the parameters of our pro-
totype system were set as follow:
• Kernel decay constant (λ). The decay constant is a parameter
for calculating a direct product when the impact of a long random
walk needs to be discounted (Section 3.2).
It was set according
to the convention of evaluating the kernel function: λ = 0.9 if
min(di,do) = 1 and λ =
• Inverse of regularization strength (C). Regularization is a param-
eter for reducing the over-ﬁtting to the labeled data when we built
the relation models using logistical regression. In our implemen-
tation, we utilized a C = 3.0, which gave the best performance
among other C values from 0.3 to 15.
• Threshold of sentence length (l). The current dependency parser
is limited by its capability to process long sentences. When a sen-
tence grows longer, the accuracy of parsing goes down and its over-
head goes up. In our implementation, we set the maximum length
of the sentence to 200 words. Given IOCs may exist in sentence
length larger than 200 words, we directly extract IOCs from sen-
tences longer than 200 words without building dependency graphs.
In our implementation, we directly extract the IOCs from sentences
longer than 200 words if the sentence has two continuous IOC to-
kens or has more than ﬁve IOC tokens which were split by short
terms (less than 5 characters) (see supplementary material).
4.2 Results
Accuracy and coverage. In our study, we ﬁrst evaluated the topic
classiﬁer (i.e., TC) over the 450 IOC and non-IOC articles, and
the relevant content picker and the relation checker (i.e., RCP/RC)
over 1,500 true IOC sentences and 3,000 false IOC sentences in
DS-Labeled, both using a ﬁve-fold cross-validation. Our proto-
type achieved a precision of 98% and a recall of 100% in ﬁnding
IOC articles, and a precision of 98% and a recall of 92% in identi-
fying true IOCs and its context. Figure 5b illustrates the ROC curve
of the RCP and RC.

Further, we ran our system over DS-Unknown across all 71K
articles. Altogether, iACE automatically extracted IOC tokens and
their context terms, and further converted them into OpenIOC items.
To understand the accuracy and coverage of the information ex-

(a) The number of the articles per
month.

(b) ROC curve of iACE.

Figure 5: The increase in the number of articles over 13 years
and the effectiveness of iACE.
tracted, we sampled the unknown set using two different methods:
we ﬁrst grouped the articles in the unknown set according to their
publication time (4 consecutive months per group), and then their
publishers (45 blogs), and in each case, we randomly picked up a
few articles from each group. Altogether, in this validation step,
we manually inspected 820 articles and in total, 25K reported sen-
tences, and concluded that iACE achieved a precision of 95% (23K
out of 25K reported IOCs were correct), and a recall of 90% (across
the articles, 90% of IOCs were reported).

Table 3: The number of samples and average accuracy and re-
call in each method.

# of Groups

Samples per Group

Precision

Method
Time span

Blog

39
45

10
10

93%
96%

Recall
92%
91%

To compare our approach with state-of-the-art alternatives, we
ran iACE against the top-of-the-line NER tool Stanford NER [26]
and the commercial IOC identiﬁer integrated within AlienVault
OTX [17]. Note that none of them (actually none of the existing
systems we are aware of) can also identify the context for an IOC
and therefore generate machine-readable OpenIOC items. In our
experiment, Stanford NER was trained on our labeled true/false
IOC sentences as describe in Section 4.1 (the same set for training
iACE). For AlienVault OTX, we utilized its API to submit articles
to their web service and retrieve the IOC tokens identiﬁed. This
study shows that our relation-based approach is indeed much more
effective. Speciﬁcally, we ran all three systems on 500 randomly
selected articles from 25 blogs in DS-Unknown and compared
their ﬁndings: iACE extracted the IOC items across 427 OpenIOC
categories, such as FileDownload HistoryItem/FileName,
Email/ReceivedFromIP, with a precision of 98% and a re-
call of 93%, while OTX could only ﬁnd the IOCs in 8 categories
(IP, hash value, domain, etc.) with a precision of 72% and a recall
of 56%, a performance mirrored by Stanford NER. Both OTX and
Stanford NER tend to introduce a lot of false positives: for exam-
ple, OTX treated the reference links of articles as malicious URLs.
Table 4 summarizes the result of this study.

Table 4: Accuracy and coverage comparison of iACE, Alien-
Vault OTX and self-trained Stanford NER.

Tool
iACE

AlienVault OTX
Stanford NER

Precision

98%
72%
71%

Recall
93%
56%
47%

Performance. To understand the performance of iACE, we mea-
sured the time it spent on each real-world article in the unknown
set and the breakdowns of the overhead in each analysis stage, BP,
RCP, RC, and IG. In the experiment, our prototype was running on
our R730xd server, using 40 threads. On average, it took around
0.25 second to inspect one article, as illustrated in Table 5. This

762result provides strong evidence that iACE can easily scale to the
level expected for processing a massive amount of CTI generated
every day.

average time (ms/article)

Table 5: Running time at different stages.
Stage
Std Deviation (ms)
BP
RCP
RC
IG
total

252.5
2.5
278.6

-
2.5
37.5

5
20

-
-

In the meantime, considering the DG parser is largely affected by
sentence length, we measure the performance of the RC model in
different sentence lengths. Figure 6 illustrates the average running
time on sentences of different lengths. We found that the running
time of iACE gradually increases when sentence length increases.
This is because iACE only extracts the shortest paths linking each
pair of a context token and the IOC token, which mitigates the im-
pact of sentence length .

Figure 6: Average running time on the sentences in different
lengths.
5. MEASUREMENT AND ANALYSIS

The IOCs automatically extracted from the 71,000 articles on
45 blogs present to us a comprehensive view of the cyber threats
that the world has been facing in the past 13 years. By analyzing
these IOCs, looking at their relations and evolution over a large
time frame, across thousands of articles, our study brings to light
new ﬁndings of attacks’ strategies and evolution, as well as new
insights into the impact of open-source intelligence. Particularly,
we found that hundreds of apparently independent attacks were ac-
tually related, sharing unique IOCs such as IP address, register’s
email and domain, etc. Among them, a set of command and control
(C&C) servers were reported by 396 articles (with little reference
among them) and linked to over 7,000 unique IOCs over a four-
year span, indicating that these separate attacks could all be part of
a massive, previously unknown campaign. Further, through corre-
lating different articles, we observe that the same vulnerability has
been continuously utilized for a long period of time. Also, the IOCs
intensively reported tend to be short-lived, demonstrating the possi-
ble impacts of the CTI on the adversaries. On the defender side, the
response to the IOCs seems less timely than one hopes, taking days
to get IOCs into malware scanners, blacklists etc. Also, our study
reveals the quality of the IOCs reported by different blogs and the
ways these blogs react to emerging threats, which helps better un-
derstand the effectiveness of such intelligence sources. Below we
elaborate on the details of this study.
5.1 Landscape

Our study shows that these technical blogs are indeed a gold
mine for threat intelligence gathering: altogether, 900K IOCs and
their context were recovered from 71,000 articles (20K IOC arti-
cles), including 20K exploit hash values, 55K registry key, 58K
malicious IPs, 180K FQDNs etc. Table 6 presents the 10 IOC

(a) Cumulative distributions for
the numbers of IOCs and iocterms
per article.

(b) Cumulative distribution of the
numbers of articles and reference
per cluster.

Figure 7: Distribution of IOCs across different articles and
clusters across different blogs.
types (described by their corresponding iocterms) with the most
instances found by iACE. We observe the largest amount of IOCs
with the type PortItem/remoteIP, which was related to the popu-
larity of drive-by-download, Phishing and other web attacks in the
wild.

Table 6: Top-10 iocterms with the largest number of IOCs.

iocterm

PortItem/remoteIP

RegistryItem/ValueName
UrlHistoryItem/HostName

RegistryItem/Path

FileDownloadHistoryItem/SourceURL

ServiceItem/serviceDLLmd5sum

PortItem/remotePort

FileDownloadHistoryItem/FileName

Email/ReceivedFromIP
FileItem/FileExtension

# of
IOCs
18,243
12,572
12,020
11, 777
10,324
9,908
9,831
8,720
8,225
8,100

general

type
IP

string
URL
string
URL
hash
int

string

IP

string

We looked into the distribution of IOCs across different arti-
cles and blogs, as illustrated by the cumulative distributions for the
numbers of IOCs and iocterms displayed in Figure 7a. On average,
each article contains 52 IOCs and 70% of the articles have more
than 10 IOCs. Particularly, the blog hpHost has 350 IOCs per arti-
cle, the largest one among the blogs we inspected. Also, an article
on CyberCrime talking about a Google redirection Spam reported
3,417 IOCs. When it comes to the diversity of IOC context, we
found that on average, each article includes 6 different iocterms
and 30% of articles have more than 10 different iocterms. Also
interestingly, the blog with more IOCs does not necessarily come
with more diverse IOC types: for example, even though hpHost
has the largest number of IOC per article, each article only has 8
iocterms.
5.2 Understanding Threats

Through mining the IOCs across articles and blogs, we gained
new insights into reported attacks and discovered connections never
known before. Such connections shed new light on the way the
adversaries organize their campaigns and adapt their techniques.
Further linking the reported IOCs to auxiliary data sources reveals
the impact of such CTI on the responses to emerging threats.
Correlation analysis. To understand the relations across differ-
ent attack campaigns reported by articles, we studied the sharing
of critical attack resources in these campaigns, through measuring
their common infrastructure-related IOCs, including IP, register’s
email and domain. Speciﬁcally, using these IOCs, we were able
to group all the articles into 527 clusters (each group with more
than three articles): two articles were put in the same cluster if they
share at least one IOC IP, email or domain. Note that we removed
IP addresses in the private address ranges (10.*.*.*, 172.16.*.*,
192.168.*.* ). To ﬁnd out whether those in the same cluster ac-

763tually refer to a common source, so they essentially talk about the
same attack instance, we also looked at the URLs in each of the
articles to identify those pointing to the 45 blogs and other well-
known intelligent sources. Figure 7b illustrates the distribution of
the clusters with various percentages of the articles involving such
references. It turns out that many clusters (including thousands of
articles) have low reference percentages: in other words, the au-
thors of these articles apparently did not realize that the attacks
they were documenting were related to other instances.

We further picked out 5 clusters with at least 25 articles but
at most 5% of them include references to others. None of such
reference-carrying articles were found to mention the relations among
the attack instances reported by other articles in the same clusters.
Also, a sampling of these articles did not show any references to
other blogs not on our list. This indicates that the infrastructure re-
lations linking all these attacks together have never been reported
before, which was conﬁrmed by our additional search for related
literature across the Internet. Table 7 provides the information
about those clusters. Most interestingly, we found that for the IOC
“132.248.49.112”, a shared IP reported by 19 blogs, turned out to
point to a C&C campaign’s name server. We believe that all the
independently documented attacks actually belonged to a massive
campaign whose scale was not known before. Note that this corre-
lation effort is highly important because it informs us of the critical
resources attackers share, which are likely to be their weakest link.

Table 7: The 5 Clusters.

# of IOCs of
mal. infra.

Cluster

# of articles

1
2
3
4
5

396
178
30
28
25

7,363
4,271
215
897
897

# of IOCs of total

10,533
8,110
960
1,302
1,222

# of

references

21
3
0
0
0

Evolution. Looking into the C&C campaign reported by 396 arti-
cles and related to 7,000+ unique IOCs, we were surprised to ﬁnd
that it lasted for a long time (2009-2013), and continued to adapt
its attack strategies and techniques. Speciﬁcally, it distributed mal-
ware by sending Spam emails to users, compromising legitimate
websites, and others, and the vulnerabilities it exploited evolved
from CVE-2010-1885 to CVE-2013-0422. In the meantime, the
campaign utilized a small set of IPs for its C&C servers, and some
of them share the same register email “gkook@checkjemail.nl”.
Apparently, taking down these servers could signiﬁcantly affect the
effectiveness of this long lasting, large-scale attack.

Another step we took to correlate the attacks reported by differ-
ent articles was to cluster them according to the indicators of their
attack vectors, including malware hash, vulnerability CVE and the
content of the registries. The purpose is to understand the evolu-
tion of the vectors with regard to the releases of related IOCs. To
this end, we looked at the shortest period of time, in terms of the
number of consecutive months, during which a speciﬁc IOC (e.g.,
132.248.49.112) was continuously covered by new articles. This
period, which we call “decay time”, demonstrates how long the at-
tack instances related to the IOC continue to pop up before they can
be stopped (at least temporarily). As illustrated in Table 8, in gen-
eral, we found that the IOCs reported by a large number of articles
tend to disappear quickly, indicating that either the related prob-
lems were quickly ﬁxed or the adversaries reacted to the reports
to change their strategies. However, there are long-lasting IOCs
even though they are well known: as a prominent example, a buffer
overﬂow vulnerability CVE-2012-0158 kept showing up in blog ar-
ticles for four months, and after a few recesses, continues to appear
in other attacks over a four year period! Speciﬁcally, it was used
in APT attacks on the military and aerospace industry (2012/04),

(a) Distributions of the duration
after an IOC was released and be-
fore it was uploaded for a scan.

(b) Cumulative distributions for
the numbers of ﬁrst-reported blog
per cluster.

Figure 8: IOC impacts and timeliness of blog.

and then on political organizations (2012/09), generic malware dis-
tribution (2013), and more recently Spear Phishing attacks on the
ﬁlm industry (2014) and banks (2015). This clearly shows that or-
ganizations have not done their due diligence to adequately respond
to the problem.

Table 8: Decay time of IOCs.

Avg. # of articles

per month

Decay time (month)

0-1
1-2
2-3
3-4
>4

68
23
8
6
3

% of total IOCs.

92%
5%
1%
1%
1%

IOC impacts. Further, we studied the impact of such open-source
intelligence on security protection: whether the security industry
(e.g., anti-virus service providers) quickly responded to the reports
of IOCs. To this end, we estimated the possible time intervals be-
tween the ﬁrst release of the IOCs and their adoption by anti-virus
(AV) tools and web scanners. In our research, we submitted a set
of IOCs, including IPs and domains of malicious sources, and the
hashes of malware to VirusTotal [11], a platform that hosts 56 main-
stream AV scanners and CleanMX [22], an IP/URL scanner. From
VirusTotal, we collected the time stamps for the ﬁrst time when the
submitted IOCs were seen by at least one of the AV systems. For
CleanMX, we fetched its IP/URL blacklist database archived from
2009/01 to 2015/04. We found that 47% of the IOCs were updated
to these systems before they were reported by the blogs. For the
rest of them, Figure 8a shows the distributions of the duration after
such an IOC was released and before it was ﬁrst used for a scan.
We observed days of delay before the IOCs were put in place for
protection, if indeed the uploading time (or the time when the IOCs
were ﬁrst used for a scan) was close to the moment when the IOCs
(IP, domains, hashes) were added to the systems. Particularly, for
IPs and domains, the whole process often took more than 12 days.
On the other hand, the malware hashes were often quickly added,
in most cases within 2 days.
5.3 Understanding Intelligence Sources

The availability of the longitudinal data (the IOCs collected over
a span of 13 years) also enables us to investigate the qualities of
the indicators produced by different sources and their timeliness
against new threats, as reported below.
Timeliness. Using the aforementioned attack clusters (see Table 7),
we analyzed the distribution of the articles ﬁrst reporting the at-
tacks over different blogs, as shown in Figure 8b. We found that 10
blogs were responsible for the ﬁrst report of 60% the clusters (each
cluster likely to be a campaign). For example, the blog Dancho
Danchev ﬁrst report 12 clusters, each time involving 45 IOCs on
average, which later also showed up on other blogs.

764Table 9: Quality of selected intelligence sources (10 out of 45)

% of
covered
iocterms

62%
55%
38%
79%
37%
61%
35%
44%
48%
57%

% of
timely
IOCs
14%
54%
41%
13%
52%
31%
43%
15%
26%
59%

Blog

Dancho Danchev
Naked Security

THN

Webroot
ThreatPost
TaoSecurity

Sucuri
PaloAlto

Malwarebytes

Hexacorn

% of
covered
IOCs
42%
43%
38%
54%
26%
57%
34%
39%
32%
49%

% of
robust
IOCs
84%
45%
51%
84%
29%
68%
52%
87%
72%
76%

Table 9 shows the average percentage of IOCs ﬁrst reported among
all the IOCs ﬁnally discovered from a cluster (i.e., the number of
ﬁrst-reported IOCs and those only reported once vs. the total num-
ber of IOCs in a cluster). We found that most IOCs reported ﬁrst by
Hexacorn and Naked Security were also mentioned by other blogs
later. Also, they provide a large amount of IOCs not documented
by other blogs. We observed that even though Webroot only has
an average of 13% of the earliest-reported IOCs for the clusters we
monitored, 84% of its IOCs were not reported by other sources.
Completeness. On the other hand, the early reports often only
contain a small portion of IOCs. In our study, we measured the
percentages of the IOC tokens and their iocterms for different at-
tack clusters that were included in the ﬁrst report: 6 blogs reported
more than 40% of the IOC tokens and 9 blogs covered more than
50% of iocterms (related to attack behavior) per cluster. We further
checked the blogs whose articles give the most complete descrip-
tions of attack clusters. Altogether, TaoSecurity were found to have
the largest number of such articles.
Robustness. In our research, we compared the robustness of differ-
ent IOC tokens, in terms of their stability across the whole period
of an attack cluster (the clusters in Table 7). From the data men-
tioned above, we found that the name server, C&C server, registry
email are the most robust indicators, which remained unchanged in
10 to 30 percent of the clusters we analyzed (see Table 9). Using
such information, we further measured the blogs likely to report
these tokens: the top blogs providing most of such tokens (i.e., the
number of robust IOCs vs. the number of total IOCs in a cluster)
are in Table 9. Interestingly, looking into the IPs of these servers,
we found that many of them actually shared the same IP preﬁxes,
which makes us believe that they might all come from a small set
of malicious Autonomous Systems.
6. DISCUSSION

Our study shows that iACE makes an important step toward fully
automated cyber threat intelligence gathering. This is signiﬁcant
not only for the convenient collection of information, but also for
effective analysis of such information, as demonstrated by our mea-
surement study. With a large amount of IOCs automatically recov-
ered from the wild and converted into a machine-readable form,
their intrinsic relations can be quickly discovered and effectively
utilized to counter emerging threats. For example, knowing the
sharing of C&C servers across multiple attack instances could en-
able the defender to disable or block the servers to stop the attacks.
On the other hand, our current design is still preliminary. Here,
we discuss the limitations of our systems and potential follow-up
research.
Error/missing analysis. Our study shows that iACE has a high ac-
curacy and coverage, well beyond what standard NLP techniques
can achieve. However, still our technique introduces some false

discoveries and misses some IOCs. These problems mostly come
from the limitations of underlying tools we use and abnormal ways
of presentation. Speciﬁcally, Tesseract, the optical character recog-
nizer, is less than perfect, and its accuracy affects the outcome of
our analysis. Also, the state-of-the-art dependency parser still can-
not maintain its accuracy when sentences become too long. Even
though iACE only works on the shortest path between an IOC and
its context token, which mitigates the problem, still there are sen-
tences too long for the parser to understand the dependencies be-
tween words correctly. Also, adding to the complication are typos:
as an example, in the case that one forgets to put a space after the
period, a sentence becomes stuck with the follow-up one, which
could cause an error in IOC sentence identiﬁcation. Another in-
teresting observation is that in some articles, authors deliberately
misspell URLs to prevent the readers from inadvertently clicking
on them: e.g., changing “http” to “hxxp” or add “[]” among dot
in a URL. iACE includes a list of typical obfuscation tricks to rec-
ognize such transfers. However, there are always approaches we do
not recognize, making IOC tokens fall through the cracks. Further-
more, a large amount of polluted original contents of articles might
also lead to false discoveries. For example, an active attacker can
compromise the blog websites and inject fake IOCs into the arti-
cles, so as to trigger iACE to report false IOCs. Further effort is
needed to better address these issues.
Other intelligence sources and standards. The current design
of iACE is for gathering threat intelligence from technical blogs,
based on the unique ways that IOCs are described. We believe that
it will also work well on other equally or more formal sources,
such as white papers and other technical articles (e.g., research pa-
pers), though further study is certainly needed here. What is less
clear is the technique’s effectiveness on less formal sources, like
technical forums (e.g., Google groups [5], SecurityFocus [7]). The
writing styles there are bit different, particularly, the use of more
diverse context terms and the sentences with irregular grammat-
ical structures. Extending iACE to this setting needs further ef-
fort. Also, the intelligence sources we use to feed iACE are all
English articles. Considering the intelligence sources of other lan-
guages, iACE should import new modules for language translation
of context terms and re-trained dependency parser of different lan-
guages. Further, as mentioned earlier, iACE is meant to support
the OpenIOC CTI model. Although there are other models such
as STIX [8] and yara [12], tools exist to convert the information
across these standards.
7. RELATED WORK
Threat intelligence exchange. To help the organizations and se-
curity community defend against the fast-evolving cyber attacks,
there have been great efforts on threat intelligence sharing. Face-
book ThreatExchange [25] and Defense Industrial Base voluntary
information sharing program (dibnet) [3] are platforms developed
for exchanging IOCs between certiﬁed participants. Meanwhile,
AlienVault OTX [17], OpenIOC DB [1] and IOC Bucket [29] are
established to share public (unclassiﬁed) IOCs. Regardless of the
type of platform, public sources like blogs still contribute a big por-
tion of IOCs. Our approach, iACE, will contribute to the establish-
ment of a fast tunnel between these public sources and exchange
platforms and help the participated organizations receive IOC up-
dates timely. As far as we know, AlienVault [17] and Recorded
Future [13] are the only two IOC providers that support automatic
IOC extraction. Even though Recorded Future (which does not
provide public services) utilized NER techniques [40], both tools
are simply looking for IOC entities without linking them to at-
tack context, and therefore cannot generate machine-readable Ope-

765nIOC items. Instead, iACE customized graph mining techniques to
capture the relation between entities, which produces high-quality
IOCs and their attack context with high accuracy. IOC extraction
from other sources were also studied recently. Catakoglu et al. [19]
demonstrated a framework to extract external components (web
IOCs) from web pages served in honeypots, which compared with
our approach, is more in line with the work on automatic signature
generation. Sabottke et al. [43] developed a warning system to alert
the user of the ongoing attacks reported by tweets (i.e., looking for
the tweets with “CVE”). This is different from our work, which
aims at generating machine-readable IOCs from attack reports.
NER/RE. NER today mainly relies on a sequence of words to iden-
tify the presence of pre-deﬁned entities (such as PERSON, LOCA-
TION). For example, Stanford NER [26] utilize a Hidden Markov
model to ﬁnd the most likely sequence of entities from unstructured
text. Other examples include Illinois NER [21] (based on super-
vised learning) and LIPI [16] (based on n-gram character language
models, etc.). When it comes to RE, today’s approaches use the
tree kernel with SVM [24], heuristic matches with self-supervised
learning [46], open pattern templates [44] and other techniques to
detect speciﬁc relations between two known entities. By compar-
ison, iACE leverages the unique features of IOC-related articles,
using the relation detection to help identify true IOCs and their
context. This combines both NER and RE steps together, which
has never been done before. Our customized graph mining algo-
rithm also enriches the RE techniques.
NLP for security and privacy. Compared with its application in
other areas (e.g., bioinformatics), NLP has only been recently used
for security and privacy research. Prior work utilized NLP for ana-
lyzing web privacy policies (by extracting its key terms) [49], gen-
erating privacy policies for Android apps [48], analyzing app de-
scriptions to infer required permissions by Android apps [38, 36],
detecting compromised websites [31] and identifying sensitive user
input from apps [28, 34]. Our work showcases a new application
of NLP, demonstrating that innovative NLP techniques need to be
developed to address real-world security challenges.
8. CONCLUSION

In this paper, we present iACE, a novel technique for automatic
extraction of IOCs from unstructured text. iACE is designed to spe-
cialize NLP techniques to threat intelligence gathering, combining
the NER and RE steps together based on the unique features of
IOCs and the technical articles describing them. By anchoring a
sentence with putative IOC tokens and context terms, our approach
can efﬁciently validate the correctness of these elements using their
relations, through a novel application of graph similarity compari-
son. This simple technique is found to be highly effective, vastly
outperforming the top-of-the-line industry IOC analyzer and NER
tool in terms of precision and coverage. Our evaluation of over
71,000 articles released in the past 13 years further reveals intrin-
sic connections across hundreds of seemingly unrelated attack in-
stances and the impacts of open-source IOCs on the defense against
emerging threats, which highlights the signiﬁcance of this ﬁrst step
toward fully automated cyber threat intelligence gathering.
9. ACKNOWLEDGMENT

This work was supported in part by the National Science Founda-
tion (grants CNS-1223477, 1223495, 1527141 and 1618493). We
thank our anonymous reviewers for their useful comments.

10. REFERENCES
[1] A community OpenIOC resource. https://openiocdb.com/.
[2] Beautiful Soup. https://www.crummy.com/software/BeautifulSoup/.

[3] Defense Industrial Base Cybersecurity Information Sharing Program.

http://dibnet.dod.mil/.

[4] GIMP. https://www.gimp.org/downloads/.
[5] Google groups. https://groups.google.com/.
[6] Open Source OCR Engine. https://github.com/tesseract-ocr/tesseract.
[7] SecurityFocus. www.securityfocus.com/.
[8] Structured Threat Information eXpression. https://stixproject.github.io.
[9] The OpenIOC Framework. http://www.openioc.org.
[10] topia.termextract 1.1.0. https://pypi.python.org/pypi/topia.termextract.
[11] Virustotal. https://www.virustotal.com/.
[12] YARA. http://plusvic.github.io/yara/.
[13] Real-Time Threat Intelligence. https://www.recordedfuture.com/, 2016.
[14] Semantic Link: ﬁnd related words. http://semantic-link.com/, 2016.
[15] Abiword. Enchant. http://www.abisource.com/projects/enchant/, 2010.
[16] Alias-i. Lingpipe 4.1.0. http://alias-i.com/lingpipe, 2008.
[17] AlienVault. Open Threat Intelligence. https://otx.alienvault.com/, 2016.
[18] N. Bach and S. Badaskar. A review of relation extraction. Literature review for

Language and Statistics II, 2007.

[19] O. Catakoglu, M. Balduzzi, and D. Balzarotti. Automatic extraction of
indicators of compromise for web applications. In WWW 2016, 2016.

[20] D. Chen and C. D. Manning. A fast and accurate dependency parser using

neural networks. In EMNLP, pages 740–750, 2014.

[21] J. Clarke, V. Srikumar, M. Sammons, and D. Roth. An nlp curator (or: How i

learned to stop worrying and love nlp pipelines). In LREC, 5 2012.

[22] CleanMX. http://lists.clean-mx.com/cgi-bin/mailman/listinfo/viruswatch/.
[23] D. J. Cook and L. B. Holder. Mining graph data. John Wiley & Sons, 2006.
[24] A. Culotta and J. Sorensen. Dependency tree kernels for relation extraction. In

Proceedings of ACL’04.

[25] Facebook. https://developers.facebook.com/products/threat-exchange.
[26] J. R. Finkel, T. Grenager, et al. Incorporating non-local information into

information extraction systems by gibbs sampling. In Proceedings of ACL’05.

[27] T. Gärtner, P. Flach, and S. Wrobel. On graph kernels: Hardness results and

efﬁcient alternatives. In Learning Theory and Kernel Machines. 2003.

[28] J. Huang, Z. Li, and X. Xiao. Supor: Precise and scalable sensitive user input

detection for android apps. In USENIX Security’15.

[29] IOCbucket. IOCbucket. https://www.iocbucket.com/, 2016.
[30] Josh Grunzweig. Alina: Casting a Shadow on POS. https://www.trustwave.com/

Resources/SpiderLabs-Blog/Alina--Casting-a-Shadow-on-POS/, 2013.
[31] X. Liao, K. Yuan, X. Wang, et al. Seeking nonsense, looking for trouble:
Efﬁcient promotional-infection detection through semantic inconsistency
search. In Proceedings of S&P’16.

[32] R. Mihalcea and P. Tarau. Textrank: Bringing order into texts. Association for

Computational Linguistics, 2004.

[33] D. Nadeau and S. Sekine. A survey of named entity recognition and

classiﬁcation. Lingvisticae Investigationes, 30(1):3–26, 2007.

[34] Y. Nan, M. Yang, Z. Yang, et al. Uipicker: User-input privacy identiﬁcation in

mobile applications. In USENIX Security’15.

[35] L. Obrst, P. Chase, and R. Markeloff. Developing an ontology of the cyber

security domain. In STIDS, pages 49–56, 2012.

[36] R. Pandita, X. Xiao, et al. Whyper: Towards automating risk assessment of

mobile applications. In USENIX Security’13.

[37] PhishTank. https://www.phishtank.com/.
[38] Z. Qu, V. Rastogi, and X. Zhang. Autocog: Measuring the

description-to-permission ﬁdelity in android applications. CCS ’14.

[39] J. Ramon and T. Gärtner. Expressivity versus efﬁciency of graph kernels. In
First international workshop on mining graphs, trees and sequences, pages
65–74. Citeseer, 2003.

[40] Recorded Future. http://info.recordedfuture.com/Portals/252628/resources/

cyber-anatomy-white-paper.pdf.

[41] Recorded Future. Recorded Future at SITA. https://go.recordedfuture.com/

hs-fs/hub/252628/ﬁle-2607572540-pdf/case-studies/sita.pdf, 2015.

[42] Rob McMillan. Open Threat Intelligence.

https://www.gartner.com/doc/2487216/deﬁnition-threat-intelligence, 2013.

[43] C. Sabottke, O. Suciu, et al. Vulnerability disclosure in the age of social media:

Exploiting twitter for predicting real-world exploits. In USENIX Security’15.

[44] M. Schmitz, R. Bart, S. Soderland, et al. Open language learning for

information extraction. In Proceedings of the JCEMNLP’12.

[45] B. Settles. Abner: an open source tool for automatically tagging genes, proteins

and other entity names in text. Bioinformatics, 21(14):3191–3192, 2005.
[46] F. Wu and D. S. Weld. Open information extraction using wikipedia. In

Proceedings of ACL’10.

[47] L. Yi, B. Liu, and X. Li. Eliminating noisy information in web pages for data

mining. In Proceedings of ACM SIGKDD’03.

[48] L. Yu, T. Zhang, X. Luo, and L. Xue. Autoppg: Towards automatic generation

of privacy policy for android applications. SPSM ’15, 2015.

[49] S. Zimmeck and S. M. Bellovin. Privee: An architecture for automatically

analyzing web privacy policies. In USENIX Security 14, 2014.

766Coverage-based Greybox Fuzzing as Markov Chain

Marcel Böhme

Van-Thuan Pham

Abhik Roychoudhury

School of Computing, National University of Singapore, Singapore

{marcel,thuanpv,abhik}@comp.nus.edu.sg

ABSTRACT
Coverage-based Greybox Fuzzing (CGF) is a random testing
approach that requires no program analysis. A new test
is generated by slightly mutating a seed input. If the test
exercises a new and interesting path, it is added to the set of
seeds; otherwise, it is discarded. We observe that most tests
exercise the same few “high-frequency” paths and develop
strategies to explore signiﬁcantly more paths with the same
number of tests by gravitating towards low-frequency paths.
We explain the challenges and opportunities of CGF using
a Markov chain model which speciﬁes the probability that
fuzzing the seed that exercises path i generates an input
that exercises path j. Each state (i.e., seed) has an energy
that speciﬁes the number of inputs to be generated from that
seed. We show that CGF is considerably more e cient if en-
ergy is inversely proportional to the density of the stationary
distribution and increases monotonically every time that
seed is chosen. Energy is controlled with a power schedule.
We implemented the exponential schedule by extending
AFL. In 24 hours, AFLFast exposes 3 previously unreported
CVEs that are not exposed by AFL and exposes 6 previously
unreported CVEs 7x faster than AFL. AFLFast produces at
least an order of magnitude more unique crashes than AFL.
CCS Concepts:
•Security and privacy!Vulnerability scanners; •Software and
its engineering!Software testing and debugging;
1.

INTRODUCTION

“Ultimately, the key to winning the hearts and minds
of practitioners is very simple: you need to show them
how the proposed approach ﬁnds new, interesting bugs
in the software they care about.” – Michal Zalewski [27]

Recently, there has been much debate about the e ciency
of symbolic execution-based fuzzers versus more lightweight
fuzzers. Symbolic execution is a systematic e↵ort to stress
di↵erent behaviors and thus considerably more e↵ective. Yet,
today most vulnerabilities are exposed by particularly light-
weight fuzzers that do not leverage any program analysis [27].

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
c  2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978428

It turns out that even the most e↵ective technique is less
e cient than blackbox fuzzing if the time spent generating
a test case takes relatively too long [3]. Symbolic execution
is very e↵ective because each new test exercises a di↵erent
path in the program. However, this e↵ectiveness comes at
the cost of spending signiﬁcant time doing program analysis
and constraint solving. Blackbox fuzzing, on the other hand,
does not require any program analysis and generates several
orders of magnitude more tests in the same time.

Coverage-based Greybox Fuzzing (CGF) is an attempt
to make fuzzing more e↵ective at path exploration without
sacriﬁcing time for program analysis. CGF uses lightweight
(binary) instrumentation to determine a unique identiﬁer for
the path that is exercised by an input. New tests are gener-
ated by slightly mutating the provided seed inputs (we also
call the new tests as fuzz ). If some fuzz exercises a new and
interesting path, the fuzzer retains that input; otherwise, it
discards that input. The provided and retained seeds are
fuzzed in a continuous loop, contributing even more seeds.
Compared to symbolic execution, CGF does not require
program analysis which brings several beneﬁts. There is no
imprecision, for instance, in the lifting of the control-ﬂow
graph from the program binary or the encoding of the path
condition as SMT formula. CGF is more scalable because
the time to generate a test does not increase with the pro-
gram size. CGF is highly parallelizable because the retained
seeds represent the only internal state. AFL implements the
state-of-the-art of CGF, is behind hundreds of high-impact
vulnerability discoveries [21], has been shown to generate
valid image ﬁles (JPEGs) from an initial seed that is vir-
tually empty [24], and has also been integrated with sym-
bolic execution (which helps where AFL “gets stuck”) [19].
Clearly, increasing the e ciency of fuzzers, like AFL, has a
real and practical impact on vulnerability detection.

We discuss challenges of existing CGFs and opportunities
to boost their e ciency by an order of magnitude. Our key
observation is that most fuzz exercises the same few paths:
Existing CGFs generate too many inputs which stress the
same behavior. More e cient CGFs exercise more paths
with the same amount of fuzz. For instance, suppose we
want to expose vulnerabilities in libpng. Fuzzing a valid
image ﬁle, there may be a 90% chance that a mutated variant
exercises a path ⇡ which rejects invalid image ﬁles. Fuzzing
an invalid image ﬁle, there may be a 99.999% chance that a
mutated variant exercises the same path ⇡. So, independent
of the initial seed image ﬁles, an above-average amount of
fuzz is bound to exercise that path ⇡ which rejects invalid
inputs. Informally, we call ⇡ a high-frequency path.

1032In this paper, we propose several strategies to focus most
of the fuzzing e↵ort on low-frequency paths so as to explore
more paths with the same amount of fuzz. The results are
very encouraging. Our AFL extension AFLFast discovered
9 vulnerabilities in GNU binutils which are now listed as
CVEs in the US National Vulnerability Database. AFLFast
exposes 6 CVEs up to 14 times faster than AFL and exposes
3 CVEs that are not exposed by AFL in eight runs of 24
hours. AFLFast reports an order of magnitude more unique
crashes than AFL.1 We will argue that our strategies have
no detrimental impact on the e↵ectiveness of AFL. So, given
more than 24 hours, AFL is expected to report the same
unique crashes and expose the three remaining CVEs.

To explain the remarkable performance gains of AFLFast,
we model Coverage-based Greybox Fuzzing (CGF) as Markov
chain. The chain speciﬁes the probability pij that fuzzing
the seed exercising path i generates an input exercising path j.
We let each state (i.e., seed) have an energy that speciﬁes
the amount of fuzz that is generated by fuzzing that seed
when it is chosen next. For instance, the minimum energy
required to discover a new and interesting path j by fuzzing
the seed which exercises path i is expected to be p 1
ij . How-
ever, in practice pij is clearly unknown. The energy of a
state is controlled by a pre-deﬁned power schedule.

AFL implements a power schedule that assigns an energy
that is constant in the number of times s(i) the seed has been
chosen for fuzzing. Almost every time the seed is chosen, the
same number of inputs are generated. In some cases, AFL
might assign signiﬁcantly more than the minimum energy re-
quired to discover a new and interesting path; in other cases,
AFL might assign not enough energy. In contrast, AFLFast
implements a power schedule that assigns an energy that is
exponential in s(i). When the seed is fuzzed for the ﬁrst
time, very low energy is assigned. Every time the seed is
chosen thereafter, exponentially more inputs are generated
up to a certain bound. This allows to rapidly approach the
minimum energy required to discover a new path.

In fact, AFL implements a power schedule that assigns
constantly high energy. Often, 80k inputs are generated for
each seed which takes about one minute. This addresses
the problem of rapid mixing: Independent of the initial seed
inputs, after a (burn-in) time some paths will always be ex-
ercised by signiﬁcantly more fuzz than others. Assigning a
lot of energy to the inital seeds allows to discover many more
“neighbors” that exercise low -frequency paths. For instance,
it makes sense to fuzz a valid image ﬁle for the longest time
with the objective to generate many more valid image ﬁles.
It is also a good idea to assign a lot of energy to these neigh-
bors and their neighbors. However, after some time, as more
seeds are discovered, many seeds will start to exercise high-
frequency paths and AFL ends up assigning way too much
energy. The chance to generate a valid image ﬁle is signiﬁ-
cantly lower if the latest seed is an invalid image ﬁle.

In contrast, AFLFast implements a power schedule that
assigns energy that is inversely proportional to the density
of the stationary distribution. In other words, it assigns low
energy to seeds exercising high-frequency paths and high
energy to seeds exercising low-frequency paths. We approx-
imate the density of the stationary distribution by counting
the number of fuzz f (i) that exercises path i.

1AFL reports an input that exercises a new and interesting
path and crashes the program (i.e., which would otherwise
be retained as new seed) as a unique crash.

AFL chooses seeds in the order they are added. Once all
seeds have been fuzzed, AFL resumes with the ﬁrst. A new
cycle begins. AFLFast e↵ectuates a di↵erent search strategy.
It chooses seeds in the order of their likely progressiveness
(while choosing a seed only once per cycle).
In the same
cycle, AFLFast chooses seeds earlier i) that exercise lower-
frequency paths and ii) that have been chosen less often.
The search strategy allows to fuzz the best seeds early on.
However, independent of the search strategy and given the
same power schedule, when a cycle is completed the same
seeds will have been fuzzed.

We note that power schedules and search strategies merely
impact AFL’s e ciency (i.e., #paths explored per unit time),
not its e↵ectiveness (i.e., #paths explored in expectation).
Since we do not modify the mutation operators2 that are
being used for fuzzing, the probability pij to discover path j
by fuzzing the input exercising path i does not change from
AFL to AFLFast. In other words, AFLFast exposes exactly
the same vulnerabilities as AFL – only signiﬁcantly faster.
In summary, we argue that the e↵ectivness of symbolic ex-
ecution stems from the systematic enumeration of paths in
the program. This allows to expose vulnerabilities that hide
deep in the program. Unfortunately, most fuzzers trade this
systematic path coverage for scalability. However, coverage-
based greybox fuzzers maintain some of this e↵ectiveness by
retaining fuzz that exercises paths that have previously not
been exercised. Each new seed results in progress towards
generating even more seeds that exercise even“deeper”paths.
Still, even coverage-based fuzzers tend to visit certain paths
with high frequency, generating too much fuzz that exer-
cises the same few paths. Our main conceptual contribution
is to smartly control the amount of fuzz generated from a
seed, thereby veering the search towards paths that are ex-
ercised with low frequency, towards paths where vulnerabil-
ities may lurk. Technically, we achieve this enhanced path
coverage using power schedules and search strategies that do
not require program analysis. Since CGF is highly paralleliz-
able, an e ciency improvement of one order of magnitude
for one AFL instance should result in an improvement of
about 1 + log10(N ) orders of magnitude for N instances.
Speciﬁcally, our paper makes the following contributions:

• Markov Chain Model. We model coverage-based
greybox fuzzing as a systematic exploration of the state
space of a Markov chain. We provide insight about the
machinery that drives AFL, which is arguably the most
successful vulnerability detection tool to date. We uti-
lize the model to explain the challenges of AFL and the
remarkable performance gains of our tool AFLFast.

• Power Schedules. We introduce and evaluate several
strategies to control the number of inputs generated
from a seed, with the objective to exercise a larger
number of low-frequency paths in the same time.

• Search Strategies. We devise and evaluate several
strategies to control the order in which seeds are chosen
for fuzzing, with the same objective.

• Tool. We publish AFLFast as a fork of AFL. AFLFast
was used by Team Codejitsu who came in 2nd in terms
of number of bugs found 3 at the DARPA Cyber Grand
Challenge: https://github.com/mboehme/aﬂfast
2AFL’s mutation operators include bit ﬂips, boundary value
substitution, simple arithmetics & block deletion/insertion.
3See red result bar for Galactica at http://bit.do/cgcresults.

1033⌥
⌃

2. BACKGROUND
2.1 Coverage-based Greybox Fuzzing

Fuzz – an automated random testing tool was ﬁrst devel-
oped by Miller et al. [13] in early 1990s to understand the re-
liability of UNIX tools. Since then, fuzzing has evolved sub-
stantially, become widely adopted into practice, and exposed
serious vulnerabilities in many important software programs
[23, 25, 26, 22]. There are three major categories depend-
ing on the degree of leverage of internal program structure:
black-box fuzzing only requires the program to execute [23,
25, 28] while white-box fuzzing [5, 11, 8, 9] requires binary
lifting and program analysis, for instance, to construct the
control-ﬂow graph. Greybox fuzzing is situated inbetween
and uses only lightweight binary instrumentation to glean
some program structure. Without program analysis, grey-
box fuzzing may be more e cient than whitebox fuzzing.
With more information about internal structure, it may be
more e↵ective than blackbox fuzzing.

Coverage-based greybox fuzzers (CGF) [22] use lightweight
instrumentation to gain coverage information. For instance,
AFL’s instrumentation captures basic block transitions, along
with coarse branch-taken hit counts. A sketch of the code
that is injected at each branch point in the program is shown
in Listing 1:

1 cur_location = < COMPILE_TIME_RANDOM >;
2 shared_mem [ cur_location ^ prev_location ]++;
3 prev_location = cur_location >> 1;

Listing 1: AFL’s instrumentation.

The variable cur_location identiﬁes the current basic block.
Its random identiﬁer is generated at compile time. Variable
shared_mem[] is a 64 kB shared memory region. Every byte
that is set in the array marks a hit for a particular tuple
(A, B) in the instrumented code where basic block B is ex-
ecuted after basic block A. The shift operation in Line 3
preserves the directionality [(A, B) versus (B, A)]. A hash
computed over the elements in shared_mem[] is used as the
path identiﬁer.

A CGF uses the coverage information to decide which gen-
erated inputs to retain for fuzzing, which input to fuzz next
and for how long. Algorithm 1 provides a general overview
of the process and is illustrated in the following by means of
AFL’s implementation. If the CGF is provided with seeds
S, they are added to the queue T ; otherwise an empty ﬁle
is generated as a starting point (lines 1–5). The seeds are
choosen in a continuous loop until a timeout is reached or
the fuzzing is aborted (line 7). AFL classiﬁes a seed as a
favorite if it is the fastest and smallest input for any of
the control-ﬂow edges it exercises. AFL’s implementation of
chooseNext mostly ignores non-favorite seeds.

For each seed input t, the CGF determines the number
of inputs that are generated by fuzzing t (i.e., #fuzz for t;
line 8). AFL’s implementation of assignEnergy uses the
execution time, block transition coverage, and creation time
of t. Then, the fuzzer generates p new inputs by mutating t
according to deﬁned mutation operators (line 10). AFL’s im-
plementation of mutate input uses bit ﬂips, simple arith-
metics, boundary values, and block deletion and insertion
strategies to generate new inputs.4
4https://lcamtuf.blogspot.sg/2014/08/
binary-fuzzing-strategies-what-works.html

⌅
⇧

add empty ﬁle to T

Algorithm 1 Coverage-based Greybox Fuzzing
Input: Seed Inputs S
1: T7 = ;
2: T = S
3: if T = ; then
4:
5: end if
6: repeat
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: until timeout reached or abort-signal
Output: Crashing Inputs T7

t = chooseNext(T )
p = assignEnergy(t)
for i from 1 to p do

t0 = mutate input(t)
if t0 crashes then

else if isInteresting(t0) then

add t0 to T7

add t0 to T

end if

end for

If the generated input t0 is considered to be “interesting”,
it is added to the circular queue (line 14). AFL’s implementa-
tion of isInteresting returns true depending on the num-
ber of times the basic block transitions, that are executed
by t0, have been executed by other seeds in the queue. More
speciﬁcally, t0 is interesting if t0 executes a path where tran-
sition b is exercised n times and for all other seeds t00 2 T
that exercise b for m times, we have that blog2 nc 6= blog2 mc
where b·c is the ﬂoor function. AFL uses this “bucketing” to
address path explosion [19]. Intuitively, AFL retains inputs
as new seeds that execute a new block transition or a path
where a block transition is exercised twice when it is nor-
mally exercised only once. At the same time, AFL discards
inputs that execute a path where some transition is exer-
cised 102 times when it has previously been exercised 101
times. If the generated input t0 crashes the program, it is
added to the set T7 of crashing inputs (line 12). A crashing
input that is also interesting is marked as unique crash.

Binary instrumentation. AFL supports both, source code
instrumentation and binary instrumentation via QEMU [1].
While QEMU does the instrumentation during interpreta-
tion at runtime, AFLDynInst [20] injects the instrumenta-
tion shown in Listing 1 directly into the binary.

Modiﬁcations. Our changes of AFL concern only the func-
tions chooseNext which implements the search strategy
and assignEnergy which implements the power schedules.
2.2 Markov Chain

A Markov chain is a stochastic process that transitions
from one state to another [14]. At any time, the chain can
be in only one state. The set of all states is called the chain’s
state space. The process transitions from one state to an-
other with a certain probability that is called the transition
probability. This probability depends only upon the current
state rather than upon the path to the present state.

More formally, a Markov chain is a sequence of random
variables {X0, X1, . . . , Xn} where Xi describes the state of
the process at time i. Given a set of states S = {1, 2, . . . , N}
for some N 2 N, the value of the random variables Xi are
taken from S. The probability that the Markov chain starts
out in state i is given by the initial distribution P(X0 = i).

1034The probability matrix PPP = (pij) speciﬁes the transition
rules. If |S| = N , then PPP is a N ⇥N stochastic matrix where
each entry is non-negative and the sum of each row is 1. The
conditional probability pij deﬁnes the probability that the
chain transitions to state j at time t + 1, given that it is in
state i at time t,

pij = P(Xt+1 = j | Xt = i)

A Markov chain is called time-homogeneous if the proba-
bility matrix (pij) does not depend on the time n. In other
words, every time the chain is in state i, the probability of
jumping to state j is the same.

If a Markov chain is time homogeneous, then the vector ⇡⇡⇡
is called a stationary distribution of the Markov chain if for
all j 2 S it satisﬁes

0  ⇡j  1
⇡i

1 =Xi2S
⇡j =Xi2S

⇡ipij

Informally, a Markov chain {X0, X1, . . . , Xn} is called rapidly
mixing if Xn is “close” to the stationary distribution for a
su ciently low number of steps n. In other words, rapidly
mixing Markov chains approach the stationary distribution
within a reasonable time – independent of the initial state.
Random walkers sample the distribution that is described
by a Markov chain. A walker starts at a state according to
the initial distribution and transitions from one state to the
next according to the transition probabilities. The state at
which the walker arrives after n steps is considered a sample
of the distribution. There may be an ensemble of walkers
that move around randomly.

For instance, the crawling of web pages can be modelled
as Markov chain. Pages are the states while the links are
the transitions. Given page i with qi links where one link
goes to page j, the probability pij that a random surfer
reaches j from i in one click is pij = 1/qi. A crawler, like
Google, seeks to index the important pages of the internet.
Brin and Page [4] developed an algorithm, called PageRank
that assigns an importance score to each page. Intuitively,
the PageRank value of a page measures the chance that
a random surfer will land on that page after a sequence
of clicks. More formally, the PageRank approximates the
density of the stationary distribution of the Markov chain
where important pages are located in high-density regions.

3. MARKOV CHAIN MODEL

In this paper, we model the probability that fuzzing a seed
which exercises program path i generates a seed which exer-
cises path j as transition probability pij in a Markov chain.
This allows us to discuss the objective of greybox fuzzing
as the e cient exploration of the chain’s state space and
to explain the challenges and opportunities of CGF and of
AFL speciﬁcally. We argue that a coverage-based greybox
fuzzer exercises more distinct paths per unit time if it does
focus on inputs in low-density regions of the Markov chain.
Hence, we devise several strategies to bias the traversal to-
wards visiting more states in low-density regions and less
states in high-density regions of the stationary distribution.
Before discussing these strategies, we introduce the Markov
chain model.

3.1 Coverage-based Fuzzing as Markov Chain
Time-inhomogeneous model. Suppose, after providing the
fuzzer with an initial seed input t0 that exercises path 0, the
fuzzer immediately explores path i + 1 by randomly mu-
tating the previous input ti which exercises path i. Every
input that is generated is directly chosen as next seed. The
sequence of paths that the fuzzer exercises is described by
a Markov chain. The transition probability pij is deﬁned as
the probability to generate an input that exercises path j
by randomly mutating the previous input ti that exercises
path i. Clearly, this Markov chain is not time-homogeneous.
The transition probability pij depends on the path in the
Markov chain by which the state i was reached. Say, a
di↵erent input t0i is fuzzed that also exercises path i, the
probability pij to generate an input that exercises path j
might be very di↵erent. While this is still a Markov chain,
it is not time-homogeneous. The analysis is di cult and the
existence of a stationary distribution is not guaranteed.

Time-homogeneous model. A stationary distribution does
exist for the following model of coverage-based fuzzing. The
state space of the Markov chain is deﬁned by the discovered
paths and their immediate neighbors. Given seeds T , let S+
be the set of (discovered) paths that are exercised by T and
S  be the set of (undiscovered) paths that are exercised by
inputs generated by randomly mutating any t 2 T .5 Then
the set of states S of the Markov chain is given as

S = S+ [ S 

The probability matrix P = (pij) of the Markov chain is de-
ﬁned as follows. If path i is a discovered path exercised by
ti 2 T (i.e., i 2 S+), then pij is the probability that ran-
domly mutating seed ti generates an input that exercises the
path j. Else if path i is an undiscovered path that is not ex-
ercised by some t 2 T (i.e., i 2 S ), then pii = 1 Ptj2T pji

and pij = pji for all tj 2 T . In other words, without loss
of generality we make the following two assumptions. We
assume that generating an input that exercises path j from
(undiscovered) seed ti is as likely as generating from seed
tj an input that exercises (undiscovered) path i. We also
assume that i 2 S  has no other undiscovered neighbors.
The stationary distribution ⇡⇡⇡ of the Markov chain gives
the probability that a random walker that takes N steps
spends roughly N⇡ i time periods in state i. In other words,
the proportion of time spent in state i converges to ⇡i as
N goes to inﬁnity. We call a high-density region of ⇡⇡⇡ a
neighborhood of paths I where µi2I (⇡i) > µtj2T (⇡j) and µ is
the arithmetic mean. Similarly, we call a low-density region
of ⇡⇡⇡ a neighborhood of paths I where µi2I (⇡i) < µtj2T (⇡j).
It is not di cult to see that a greybox fuzzer is more likely
to exercise paths in a high-density region of ⇡⇡⇡ than in a low-
density region. Note that we get a new Markov chain once
an undiscovered path i 2 S  is discovered.
Energy & Power Schedules. We let each state s 2 S+ have
an energy. The energy of state i determines the number of
inputs that should be generated by fuzzing the seed ti when
ti is next chosen from the queue T . The energy of a state is
controlled by a pre-deﬁned power schedule. Note that energy
is a local property speciﬁc to a state (unlike temperature in
simmulated annealing). In Algorithm 1, the power schedule
is implemented by the function assignEnergy.

5An input ti is randomly mutated using mutate_input on
ti in Algorithm 1.

1035s
e
s
a
C

 
t
s
e
T

 
f

o
 
r
e
b
m
u
N

105
104
103
102
101
100

●●
●
●

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

●●●●●●●●●●

●●●●●●●●●●

mean =  1288

0

20 40 60 80 100 120 140 160 180 200 220 240 260 280

Path Index

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

Figure 1: #Fuzz exercising a path (on a log-scale)
after running AFL for 10 minutes on the nm-tool.

Long tails. In our experiments, we observe several notable
properties of the Markov chain model of coverage-based grey-
box fuzzing. For one, the stationary distribution has a large
number of very-low-density regions and a small number of
very-high-density regions. As shown in Figure 1, 30% of
the paths are exercised by just a single generated test input
while 10% of the paths are exercised by 1k to 100k generated
test inputs. In other words, most inputs exercise a few high-
frequency paths. Often, these inputs are invalid while the
few inputs exercising the low-frequency paths are valid and
interesting. Basically, almost each valid input would exercise
di↵erent behavior. Hence, in this paper we devise strategies
to explore such low-density regions more e ciently.

Rapid mixing. Moreover, such Markov chains are mostly
rapidly mixing. Given our exploration objective, this is most
unfortunate. It takes only a few transitions to “forget” the
initial state and arrive in a high-density region that is visited
by most walkers. After a few transitions, the probability
that the current state corresponds to a high-frequency path
is high, no matter whether the walker started with an initial
seed that exercises a low-frequency path or not, or whether
the walker started with a valid or an invalid input.

Beneﬁts. The Markov chain model of coverage-based grey-
box fuzzing has several beneﬁts. For example,
it opens
fuzzing for the e cient approximation of numerical program
properties, such as the worst-case or average execution time
or energy consumption. There exist several Markov Chain
Monte Carlo (MCMC) methods, like Simulated Annealing
[12] that o↵er guarantees on the convergence to the actual
value. In the context of vulnerabilty research, the Markov
chain model allows to explain the challenges and opportuni-
ties of existing coverage-based fuzzers, such as AFL.
3.2 Running Example

On a high level, we model the probability that fuzzing a
test input t 2 T which exercises some path i generates an
input which exercises path j as transition probabilities pij
in a Markov chain. We illustrate this model using the simple
program in Listing 2 which takes as input a 4-character word
and crashes for the input “bad!”.

⌥

1 void crashme ( char * s ) {
2

if ( s [0] == ’b ’)

3

4

5

6

7 }⌃

if ( s [1] == ’a ’)

if ( s [2] == ’d ’)

if ( s [3] == ’! ’)

abort () ;

Listing 2: Motivating example.

The program has ﬁve execution paths. Path 0 (****) is
executed by all strings that do not start with the letter ’b’.
Path 1 (b***) is executed by all strings starting with “b” that
do not continue with the letter ’a’. Path 2 (ba**) is executed
by all strings starting with“ba” that do not continue with the
letter ’d’. Path 3 (bad*) is executed by all strings starting
with “bad” that do not continue with the letter ’ !’. Finally,
Path 4 is executed only by the input “bad!”.

Now, let us specify the implementation of mutate input
(MI ) in Algorithm 1 to modify a seed input s = hc0, c1, c2, c3i
to generate new inputs. MI chooses with equal probability
a character c from s and substitutes it by a character that is
randomly chosen from the set of 28 ASCII characters. For
example, the word “bill” exercises Path 1. With probability
1/4, MI chooses the second character c1 and with probabil-
ity 1/28 it chooses the letter ’a’ for the substitution. With a
total probability of 210, MI generates the word “ball” from
“bill” as the next test input which exercises Path 2.

****

1   2 10
2 10

b***

3
4
2 10

ba**

1

4   2 10

1

2 + 2 10
2 10

bad*

1

4 + 2 9
2 10

bad!

2 8

Figure 2: Markov chain for motivating example

Figure 2 represents the simpliﬁed transition matrix pij as
a state diagram.6 For example, if the current input is the
word “bill”, the Markov Chain is in the state b***. The like-
lihood to transition to the state ba** is 2 10 as explained
earlier. In other words, on average it takes 210 = 1024 exe-
cutions of MI on the word “bill” to exercise Path 3 and reach
state ba**. Given the word “bill”, the likelihood to transi-
tion to the same state b*** is 0.75 because MI may choose
the ﬁrst letter and ’b’ as substitute or the second letter and
any letter except ’a’ as substitute with a total probability
of 0.25 and it may choose the third or fourth letter with
a total probability of 0.5. The probability to transition to

state **** is 1/4   2 10  because MI may choose the ﬁrst

of four letters and substitute it with any letter except ’b’.

Notice that there is a very high probability density in state
****. Most 4-character words do not start with ’b’ such
that the initial distribution is heavily biased towards that
state. The random walker can transition to the next state
only with probability 2 10, stays in b*** with probability
3/4 and comes back to the state **** with the approximate
probability 1/4. Many inputs will be generated until the
walker reaches the state bad!.
6For simplicity, we ignore some low probability transitions,
e.g., from state **** to state bad!.

⌅

⇧

10363.3 Challenges of Coverage-based Fuzzers

A coverage-based greybox fuzzer is an ensemble of random
walkers in the Markov chain. There is one walker for each
seed t 2 T . The objective is to discover an interesting path
s 2 S  that is not exercised by any t 2 T while generating
a minimal number of inputs. Conceptually, all walkers can
move simultaneously. Technically, resources are limited and
we need to choose which walker can move and how often. In
a sequential setting, the fuzzer chooses the next input to fuzz
t 2 T according to chooseNext and generates as many in-
puts as determined by p = assignEnergy(t) in Algorithm 1.
Usually, p < M where M 2 N gives an upper bound on the
number of generated inputs. In AFL, M ⇡ 160k.
More Energy Than Needed. AFL implements a schedule
that assigns energy that is constant in the number of times
the corresponding seed has been chosen from the queue. Let
Xij be the random variable that describes the minimum
energy that should be assigned to state i 2 S+ so that the
fuzzer discovers the new state j 2 S  where pij > 0. Then,

E[Xij] =

1
pij

Now, AFL’s constant schedule might assign signiﬁcantly more
or signiﬁcantly less energy than is actually required.

Example. Let AFL’s power schedule assign an energy of
p(i) = 216 = 64k to a state i every time ti is chosen. Since
most 4-character words do not start with ’b’, the ﬁrst input
t0 likely exercises Path 0. After 216 inputs have been gener-
ated by fuzzing t0, several inputs are expected to begin with
the letter ’b’. One input that exercises Path 1 is retained
as seed t1. After another 216 inputs have been generated by
fuzzing t1, at least one input is expected to exercise Path 2
and is retained as t2. Figure 3 shows how the procedure con-
tinues. After a total of 256k inputs were generated from the
four seeds that were retained for each path, the crashing in-
put is found. A more e cient fuzzer would need to generate
no more than E[X01]+E[X12]+E[X23]+E[X34] = 4·210 = 4k
inputs to expose the same vulnerability.

#Total Tests State Explored States

1
216 + 1
2 · 216 + 1
3 · 216 + 1
4 · 216 + 1

****
b***
ba**
bad*
bad!

****
****, b***
****, b***, ba**
****, b***, ba**, bad*
****, b***, ba**, bad*, bad!

Figure 3: The crash is found after 218 = 256k inputs
were generated by fuzzing when p = 216 is constant.

Excessive Energy for High-Density Regions. AFL’s power
schedule also assigns constantly high energy: Fuzzing a seed
input often takes about a minute on our machine. This
addresses the problem of rapid mixing. Initial seeds are of-
ten provided such that they exercise interesting paths in
a low-density region in the stationary distribution of the
Markov chain. Assigning high energy to the inital seeds and
the seeds in the immediate neighborhood allows to discover
many more neighbors in the same low-density region. How-
ever, as the retained inputs exercise paths in high-density
regions – and there is a natural tendency – too much energy
is assigned to these states. By deﬁnition, the higher the den-
sity of the stationary distribution of the Markov chain for
the given state i, the higher the proportion of inputs gener-
ated by fuzzing ti that will exercise high-frequency paths.

State
ba**
****
b***
ba**
****
b***
bad*
ba**
****
b***
bad*

1
2
3
4
5
6
7
8
9
10
11

****
1 · 27
5 · 27
6 · 27
7 · 27
11 · 27
12 · 27
13 · 27
14 · 27
18 · 27
19 · 27
20 · 27

b***
1 · 27
1 · 27
4 · 27
5 · 27
5 · 27
8 · 27
9 · 27
10 · 27
10 · 27
13 · 27
14 · 27

ba**
2 · 27
2 · 27
2 · 27
4 · 27
4 · 27
4 · 27
5 · 27
7 · 27
7 · 27
7 · 27
8 · 27

bad*

0
0
0
1
1
1
1 · 27
1 · 27
1 · 27
1 · 27
2 · 27

bad!

0
0
0
0
0
0
0
0
0
0
1

Figure 4: Total #fuzz exercising the corresponding
path when fuzzing the given state. Too much energy
assigned to state **** and not enough to state bad*
once it is discovered. Lines indicate new cycles.

Example. Let the initial seed input be the word ball
and let AFL’s power schedule assign an energy of p(i) =
29 = 512 to a state i every time ti is chosen. This allows
us to discuss the case where the next state is not found
in a single fuzzing iteration and several cycles through the
circular queue might be required. Recall that AFL chooses
the seeds in the order they are added. Figure 4 elaborates
the example. After fuzzing the initial seed input for 29 times,
two new seeds are discovered. About one quarter of the fuzz
(i.e., 27 inputs) exercises paths **** and b***, respectively
(see Fig. 2 and Fig. 4, Row 1). Fuzzing the ﬁrst discovered
seed (Row 2), all fuzz exercises the same path. Fuzzing
the second discovered seed (Row 3), a quarter of the fuzz
exercises path **** and three quarters exercises path b***.
Since no new seeds are discovered, a new cycle begins with
the initial seed (Row 4). This procedure continues until the
vulnerability is exposed (Row 11). In each row we see that
most fuzz exercises path ****. Evidently, the fuzzer spends
way too much time exercising this high-frequency path. The
same time would be better spent fuzzing the seed exercising
the low-frequency path bad*.

In summary, two challenges of existing coverage-based

greybox fuzzers are: Their power schedules

1. may assign more energy than is required in expectation

to discover a new and interesting path and

2. may assign too much energy to states in high-density
regions of the chain’s stationary distribution and not
enough energy to states in low-density regions.

4. BOOSTING GREYBOX FUZZING

A more e cient coverage-based greybox fuzzer discovers
an undiscovered state in a low-density region while assigning
the least amount of total energy. More speciﬁcally,

1. Search Strategy. The fuzzer chooses i 2 S+ such
that 9j 2 S  where ⇡j is low and E[Xij] is minimal.
2. Power Schedule. The fuzzer assigns the energy p(i) =
E[Xij] to the chosen state i in order to limit the fuzzing
time to the minimum that is required to be expected
to discover a path in a low-density region.

In this paper, we propose monotonous power schedules
that ﬁrst assign low energy which monotonously increases
every time the corresponding seed is chosen from the queue.
This allows to rapidly approach E[Xij]. Moreover, our power
schedules assign energy that is inversely proportional to the
density of the stationary distribution of the Markov chain.

1037Intuitively, as soon as a new path is discovered, we want to
swiftly explore its general neighborhood expending only low
energy. This allows us to get a ﬁrst estimate of whether i
lives in a high-density region. Every time i is chosen there-
after, it is assigned more energy. Intuitively, after the neigh-
borhood is explored and it is established that i lives in a low-
density region, the fuzzer can invest signiﬁcantly more en-
ergy trying to ﬁnd paths in the low-density neighborhood of i.

We also propose and evaluate search strategies that are
aimed at the fuzzer expending most energy for paths in low-
density regions. For instance, to establish whether a state
is in a low-density region, we prioritize such t 2 T that have
been chosen from the circular queue least often and such t
that exercise paths that have least often been exercised by
other generated test inputs.
4.1 Power Schedules

A power schedule regulates the energy p(i) of a state.
More speciﬁcally, a power schedule decides how many in-
puts are generated by fuzzing the seed ti 2 T which exer-
cises path i when ti is selected next. In general, p(i) is a
function of a) the number of times s(i) that ti has previ-
ously been choosen from the queue T and b) the number of
generated inputs f (i) that exercise i. In fact, f (i) serves as
approximation of the distribution’s density. We discuss and
evaluate several power schedules.

The exploitation-based constant schedule (EXPLOIT)
is implemented by most greybox fuzzers. After some burn-
in, the assigned energy is fairly constant every time s(i) that
ti is being chosen from the circular queue. The energy p(i)
for state i is computed as

p(i) = ↵(i)

e.g., for AFL

(1)

where ↵(i) is the CGF’s present implem. of assignEnergy in
Algorithm 1 and remains constant as s(i) orf (i) varies. For
instance, AFL computes ↵(i) depending on the execution
time, block transition coverage, and creation time of ti. The
example in Figure 3 is derived using a constant schedule.

The exploration-based constant schedule (EXPLORE)
is a schedule that assigns constant but also fairly low energy.
The energy p(i) for state i is computed as

p(i) =

↵(i)

 

(2)

where ↵(i)/  maintaints the fuzzer’s original judgement ↵(i)
of the quality of ti and where  > 1 is a constant.

Cut-O↵ Exponential (COE) is an exponential schedule
that prevents high-frequency paths to be fuzzed until they
become low-frequency paths. The COE increases the fuzzing
time of ti exponentially each time s(i) that ti is chosen from
the circular queue. The energy p(i) is computed as

p(i) =(0

min⇣ ↵(i)

  · 2s(i), M⌘

if f (i) > µ
otherwise.

(3)

where ↵(i) maintaints the fuzzer’s original judgement and
 > 1 is a constant that puts the fuzzer in exploration mode
for ti that have only recently been discovered (i.e., s(i) is
low), and where µ is the mean number of fuzz exercising a
discovered path

µ = Pi2S+ f (i)

|S+|

where S+ is the set of discovered paths. Intuitively, high-
frequency paths where f (i) > µ that receive a lot of fuzz
even from fuzzing other seeds are considered low-priority
and not fuzzed at all until they are below the mean again.
The constant M provides an upper bound on the number of
inputs that are generated per fuzzing iteration.

#Tests State Explored States

1
210
2 · 210
3 · 210
4 · 210

****
b***
ba**
bad*
bad!

****
****, b***
****, b***, ba**
****, b***, ba**, bad*
****, b***, ba**, bad*, bad!

Figure 5: The crash is found after 212 = 4k inputs
were generated by fuzzing with a power schedule.

Example. Figure 5 depicts the states that a greybox fuzzer
explores with the COE power schedule with ↵(i)/  = 1. The
ﬁrst test input is chosen at random from the program’s input
space. Since most 4-character words do not start with ’b’,
the ﬁrst input t0 likely exercises path 0 which corresponds
to state ****. The ﬁrst time that t0 is fuzzed, s(0) = 0
and f (0) = µ = 1 so that ↵(0) = 20. Next time, s(0) = 1
and f (0) = µ = 2 so that ↵(0) = 21. When s(0) = 9
and ↵(0) = 29, 210 test inputs will be generated so that
one generated test input t1 is expected to start with the
letter ’b’ and the state b*** is discovered (see Fig. 2). Now,
the newly discovered state is assigned low energy ↵(1) =
20. However, f (0) > µ so that soley t1 will be fuzzed in
a similar fashion as t0 until s(1) = 9, ↵(1) = 29 and 210
test inputs have been generated by fuzzing t1. Again, one
test input is expected to start with “ba” and the state ba**
is discovered. Table 5 shows how the procedure continues.
After 4k test inputs were generated from the four inputs
that were retained for each path, the crashing input is found.
The random generation of the same string would require ﬁve
orders of magnitude more inputs on average (4·106k random
inputs) while the constant schedule in Figure 3 would require
one order of magnitude more test inputs on average (256k).

The exponential schedule (FAST) is an extension of COE.
Instead of not fuzzing ti at all if f (i) > µ, the power sched-
ule induces to fuzz ti inversely proportional to the amount
of fuzz f (i) that exercises path i. The energy p(i) that this
schedule assigns to state i is computed as

p(i) = min✓ ↵(i)

  ·

2s(i)
f (i)

, M◆

(4)

Intuitively, f (i) in the denominator allows to exploit ti that
have not received a high number of fuzz in the past and is
thus more likely to be in a low-density region. The expo-
nential increase with s(i) allows more and more energy for
paths were we are more and more conﬁdent that they live
in a low-density region.

The linear schedule (LINEAR) increases the energy of a
state i in a linear manner w.r.t. the number of times s(i) that
ti has been chosen from T , yet is also inversely proportional
to the amount of fuzz f (i) that exercises path i.

p(i) = min✓ ↵(i)

  ·

s(i)
f (i)

, M◆

(5)

1038The quadratic schedule (QUAD) increases the energy of
a statei
in a quadratic manner w.r.t. the number of times
s(i) that ti has been chosen from T , yet is also proportional
to the amount of fuzz f (i) that exercises path i. The energy
p(i) for state i is computed as

p(i) = min✓ ↵(i)

  ·

s(i)2
f (i)

, M◆

(6)

4.2 Search Strategies

While a power schedule regulates the time spent fuzzing a
seed, a search strategy decide which seed is chosen next. The
decision is purely based on the number the number of times
a seed has been fuzzed before and the amount of fuzz exer-
cising the same path as the seed. An e cient coverage-based
greybox fuzzer prioritizes inputs that have not been fuzzed
very often and inputs that exercise low-frequency paths.

Prioritize small s(i)s(i)s(i). This strategy chooses ti 2 T such
that the number of times s(i) that ti has been fuzzed is mini-
mal. However, the fuzzer may still decide to skip the choosen
test input, for instance if it is not a designated favourite. In
that case, the search strategy is applied again until the fuzzer
does not skip the input. E↵ectively, the queue is reordered
using the search strategy. Intuitively, the fuzzer can estab-
lish early whether or not path i is a low-frequency path and
whether it should invest more energy into fuzzing ti.

Prioritize small f (i)f (i)f (i). This strategy chooses ti 2 T such
that the number f (i) of generated inputs that exercise path
i is minimal. The fuzzer may skip the chosen test input, for
instance if it is not a designated favourite, until ﬁnally an
input is chosen according to the search strategy and accepted
for fuzzing.
Intuitively, fuzzing an input that exercises a
low-frequency path might generate more inputs exercising
low-frequency paths.
4.3

Implementation of AFLFast

AFL is a coverage-based greybox fuzzer that collects infor-
mation on the basic block transitions that are exercised by
an input. AFL’s binary instrumentation is discussed in Sec-
tion. 2.1. In our experiments, we extended version 1.94b.
AFL implements certain strategies to select “interesting” in-
puts from the fuzz to add to the queue. We did not change
this functionality. AFL addresses path explosion by “buck-
eting” – the grouping of paths according to the number of
times all executed basic block transitions are exercised. We
did not change this functionality either. All changes were
made to assignEnergy and chooseNext in Algorithm 1.
Changes for Power Schedule. We changed the computation
of the amount of fuzz p(i) that is generated for an input ti.
Firstly, AFL computes p(i) depending on execution time,
transition coverage, and creation time of ti. Essentially, if it
executes more quickly, covers more, and is generated later,
then the number of fuzz is greater. We maintain this eval-
uation in the various power schedules discussed above. Sec-
ondly, AFL executes the deterministic stage the ﬁrst time ti
is fuzzed. Since our power schedules assign signiﬁcantly less
energy for the ﬁrst stage, our extension executes the deter-
ministic stage later when the assigned energy is equal to the
energy spent by deterministic fuzzing. Lastly, AFL might
initially compute a low value for p(i) and then dynamically
increase p(i) in the same run if “interesting” inputs are gen-
erated. Since our implementation controls p(i) via a power
schedule, we disabled this dynamic increase for AFLFast.

Changes for Search Strategy. We changed the order in
which AFL chooses the inputs from the queue and how AFL
designates “favourite” inputs that are e↵ectively exclusively
chosen from the queue. Firstly, for all executed basic block
transitions b, AFL chooses as favourite the fastest and small-
est inputs executing b. AFLFast ﬁrst chooses the input ex-
ercising b with the smallest number of time s(i) that it has
been chosen from the queue, and if there are several, then
the input that exercises a path exercised by the least amount
of fuzz f (i), and if there are still several, then the fastest and
smallest input. Secondly, AFL chooses the next favourite in-
put which follows the current input in the queue. AFLFast
chooses the next favourite input with the smallest number of
time s(i) that it has been chosen from the queue and if there
are several, it chooses that which exercises a path exercised
by the least amount of fuzz f (i).
5. EVALUATION
5.1 Vulnerabilities

We chose GNU binutils as subject because it is non-trivial
and widely used for the analysis of program binaries. It con-
sists of several tools including nm, objdump, strings, size,
and c++filt. We zoom into some results by discussing the
results for nm in more detail.7 Binutils is a di cult subject
because the fuzzer needs to generate some approximation
of a program binary in order to exercise interesting behav-
iors of the programs. We found a large number of serious
vulnerabilities and several bugs (listed in Table 1).

Type
Exploitable Bu↵er Overﬂow
Invalid Write due to a Use-After-Free
Invalid Write due to a Use-After-Free
Invalid Write due to Integer Overﬂow

Table 1: CVE-IDs and Exploitation Type
Vulnerability
CVE-2016-2226
CVE-2016-4487
CVE-2016-4488
CVE-2016-4489
CVE-2016-4490 Write Access Violation
CVE-2016-4491 Various Stack Corruptions
CVE-2016-4492 Write Access Violation
CVE-2016-4493 Write Access Violation
CVE-2016-6131
Bug 1
Bug 2
Bug 3

Stack Corruption
Bu↵er Overﬂow (Invalid Read)
Bu↵er Overﬂow (Invalid Read)
Bu↵er Overﬂow (Invalid Read)

All vulnerabilities were previously unreported and rated
as medium security risk. We informed the maintainers, sub-
mitted patches, and informed the security community via
the ossecurity mailing list.8 Mitre assigned nine (9) CVEs.
At the time of writing, all but one patches have been ac-
cepted while one is still under review. These vulnerabilities
a↵ect most available binary analysis tools including valgrind,
gdb, binutils, gcov and other libbfd-based tools. An at-
tacker might modify a program binary such that it executes
malicious code upon analysis, e.g., an analysis to identify
whether the binary is malicious in the ﬁrst place or during
the attempt of reverse-engineering the binary.

Measure of #paths. AFL maintains a unique path inden-
tiﬁer cksum for each input in the queue that is computed as
a hash over the shared memory region that has a bit set for
each basic block transition that is exercised by t. We imple-
mented a map {(cksum(i), f (i)) | ti 2 T} that keeps track of
the number of generated (and potentially discarded) inputs
for each exercised path.
7Manual analysis and patching of 1.2k plus unique crashes
took much time and hence was done for one program.
8http://www.openwall.com/lists/oss-security/2016/05/05/3

1039Measure of #crashes. AFL deﬁnes unique crash as follows.
If two crashing inputs exercise a path in the same “bucket”,
then both inputs e↵ectively expose the same unique crash.
Experimental Infrastructure. We ran our experiments on
a 64-bit machine with 40 cores (2.6 GHz Intel R  Xeon R  E5-
2600), 64GB of main memory, and Ubuntu 14.04 as host
OS. We ran each experiment at least eight times for six
or 24 hours. We ran 40 experiments simultaneously, that
is, one experiment was run on one core. For each exper-
iment, only one seed input is provided — the empty ﬁle.
Time is measured using unix time stamps. We tested nm -C,
objdump -d, readelf -a, and the others without options.
5.2 General Results

s
e
h
s
a
r
C
e
u
q
n
U

 

i

 
f

o

 
r
e
b
m
u
N

c++filt

readelf

1000

10

1000

10

0

2

4

6 0

nm

size

objdump

strings

2

4

Time (in hours)

6 0

2

4

6

Figure 6: #Crashes over time (on a log-scale) for
AFLFast (solid line) vs. AFL (dashed line)

Crashes over time. After 6h, AFLFast found one and
two orders of magnitude more unique crashes than AFL in
c++filt and nm, respectively.9 AFLFast found 30 unique
crashes in objdump where AFL found no crash at all. None
of the fuzzers found a crashing input for the remaining three
studied tools in any of eight runs of six hours. For each tool,
the number of crashes found over time is shown in Figure 6.
In what follows, we investigate the unique crashes generated
for nm with a 24 hour budget in more details.

Vulnerabilities in nm. On average, AFLFast exposes the
CVEs seven (7) times faster than AFL and exposes three (3)
CVEs that are not exposed by AFL in any of eight runs in 24
hours. AFLFast exposes all vulnerabilities in 2h17m, on av-
erage while AFL would require more than 12h30m. The ﬁrst
three rows of Figure 7 show the results for the vulnerabilities
in the nm tool in more details. Each facet compares AFLFast
on the left hand-side and AFL on the right hand side using a
box plot with a jitter overlay. In all of eight runs, AFLFast
consistently and signiﬁcantly outperforms classic AFL. The
average time to ﬁrst exposure is shown in Figure 8. All
vulnerabilities are exposed within the ﬁrst six hours. The
exponential power schedule and improved search strategies
clearly boost the e ciency of the state-of-the-art coverage-
based greybox fuzzer.

Bugs in nm. AFLFast ﬁnds two bu↵er overﬂows seven (7)
times faster than AFL. AFLFast also exposes a third bug
which is not exposed by AFL at all. The three overﬂows
are invalid reads and unlikely to be exploitable. The last
row of Figure 1 shows more details. Again, our extension
consistently outperforms the classic version of AFL.

9Notice the logarithmic scale in Figure 6.

CVE−2016−2226

CVE−2016−4487

CVE−2016−4488

●

●

●
●●

●

●

●

●

●●

●
●
●

●
●
●

●●
●

●
●
●●
● ●
●

●

●

●

●

●
●
●
● ●

●

●

●

CVE−2016−4489

CVE−2016−4490

CVE−2016−4491

●

●
●

●

●
●
●

●

●

●

●

●

●
●
● ●

●

●

●
●

●

●

●●
●

●

●
●

●

●● ●●

● ●●●

CVE−2016−4492

CVE−2016−4493

CVE−2016−6131

)
s
r
u
o
h

 

n
i
(
 

e
m
T

i

24
20
16
12
8
4
0

24
20
16
12
8
4
0

24
20
16
12
8
4
0

24
20
16
12
8
4
0

●

●

●
●

●

●

●

●

●

●

●

●

●

●●
●
●

Bug 1

●

●

●

●
●

●

●

●●
●
●

●

AFL−FAST

AFL

●

●
●

●

●●
●

●

●

●
●

●●
●

●
●

●

Bug 2

●

Bug 3

●

●

●

●

●
●

AFL−FAST
 

AFL

AFL−FAST

AFL

Figure 7: Time to expose the vulnerability.

Vulnerability
CVE-2016-2226 > 24.00 h
2.63 h
CVE-2016-4487
6.92 h
CVE-2016-4488
10.68 h
CVE-2016-4489
CVE-2016-4490
3.68 h
CVE-2016-4491 > 24.00 h
12.18 h
CVE-2016-4492
CVE-2016-4493
4.48 h
CVE-2016-6131 > 24.00 h
20.43 h
Bug 1
20.91 h
Bug 2
Bug 3
> 24.00 h

AFL AFL-Fast Factor
N/A
5.8
7.0
3.8
9.1
N/A
14.1
4.5
N/A
6.0
7.2
N/A

3.85 h
0.46 h
0.98 h
2.78 h
0.41 h
4.74 h
0.87 h
1.00 h
5.48 h
3.38 h
2.89 h
5.07 h

Figure 8: Time to expose the vulnerability.

Independent Evaluation. We note that our collaborators,
Team Codejitsu at DARPA Cyber Grand Challenge (CGC),
evaluated both AFL and AFLFast on all 150 benchmark
programs that are provided as part of the CGC. On these
binaries, AFLFast exposes errors 19x faster than AFL, on
average. In one run, AFL exposed four errors that are not
exposed by our extension. However, AFLFast exposed seven
errors that are not exposed by AFL. Team Codejitsu inte-
grated AFLFast in their bot Galatica to prove vulnerabilities
in the other teams’ binaries. Galactica went on to take 2nd
place in the CGC ﬁnals in terms of number of bugs found.
A thorough discussion and reﬂection of the CGC experience
will not be covered in this article. However, we think that
Codejitsu’s success demonstrates the potential of AFLFast.

1040s
e
s
a
C

 
t
s
e
T

 
f

o
 
r
e
b
m
u
N

105
104
103
102
101
100

s
e
s
a
C

 
t
s
e
T

 
f

o
 
r
e
b
m
u
N

105
104
103
102
101
100

AFL−FAST

●●
●
●
●●●
●
●●●●
●●●●

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

●●●●●●●●●●●●●●●

●●●●●●●●●●●●●

mean =  382

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

350

400

450

500

250

200
300
Path Index

AFL

mean =  1288

0

50

100

150

●●
●
●

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

●●●●●●●●●●

●●●●●●●●●●

0

50

100

150

●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●

250

200
300
Path Index

350

400

450

500

Figure 9: #Fuzz exercising a path (on a log-scale)
after running AFL for 10 minutes on the nm-tool.

Low-frequency Paths.

In this paper, we argue that the
fuzzing time is better spent exploring low-frequency paths.
Firstly, we believe that low-frequency paths are more likely
to be exercised by valid inputs that stress di↵erent behav-
iors of the program. Secondly, less time is wasted fuzzing
high-frequency paths that are exercised by most fuzz any-
ways. Finally, it allows the coverage-based greybox fuzzer
to e ciently discover more paths per generated input. As
we can see in Figure 9, indeed our heuristics generate more
fuzz for low-frequency paths and less fuzz for high-frequency
paths.
In 10 minutes, AFLFast discovered twice as many
paths as AFL. For AFLFast only 10% of the discovered
(low-frequency) paths are exercised by just one input while
for AFL, 30% are exercised by just one input. The mean
amount of generated test inputs per path is about three
times higher for AFLFast. This clearly demonstrates the ef-
fectiveness of our heuristics in exploring a maximal number
of (low-frequency) paths while expending minimum energy.
5.3 Comparison of Power Schedules

Earlier, we introduced two constant and four monotonous
power schedules. AFL adopts a constant power schedule and
assigns a fairly high amount of energy. Basically, the same
input will get the same performance score the next time it
is fuzzed. This is the exploitation-based constant schedule
(exploit). To understand the impact of our choice to start
with a reduced fuzzing time per input, we also investigate an
exploration-based constant schedule (explore) that assigns a
fairly low and constant amount of energy. The monotonous
schedules increase the fuzzing time in a linear, quadratic, or
exponential manner. Speciﬁcally, AFLFast implements an
exponential schedule.

1250

1000

750

500

250

0

s
e
h
s
a
r
C
 
e
u
q
n
U

i

 
f
o
 
r
e
b
m
u
N

Schedule
afl−fast
coe
exploit (afl)
explore
linear
quad

0

5

10

15
Time (in hours)

20

25

Results. The exponential schedule that is implemented
in AFLFast outperforms all other schedules. The cut-o↵ ex-
ponential schedule (coe) performs only slightly worse than
AFLFast. After 24 hours, both schedules (fast and coe)
exposed 50% more unique crashes than the other three (lin-
ear, quad, and explore). Interestingly, the exploration-based
constant schedule (explore) starts o↵ by discovering a larger
number of crashes than any of the other schedules; it fuzzes
each input quickly and swiftly moves on to the next. How-
ever, this strategy does not pay o↵ in the longer run. After
24 hours, it performs worse than any of the other schedules
(except AFL’s exploitation-based constant schedule). The
quadratic schedule (quad) starts o↵ revealing a similar num-
ber of unique crashes as AFLFast but at the end of the 24
hour budget it performs comparably to the other two (linear
and explore).
5.4 Comparison of Search Strategies

Our search strategies prioritize inputs that have not been
fuzzed very often (small s(i)) and inputs that exercise low-
frequency paths (small f (i)). In the following, we investigate
two strategies targeting the implementation of perf score
and chooseNext in Algorithm 1. Strategy 1 designates
as favourites ti 2 T where s(i) and f (i) are small, and then
where execution time, transition coverage, and creation time
are minimal.10 Without Strategy 1, AFLFast (like AFL)
designates as favorites ti 2 T where execution time, tran-
sition coverage, and creation time are minimal. Strategy 2
chooses the next input ti from the queue where s(i) and
f (i) are minimal and ti is a favourite. Without Strategy 2
AFLFast (like AFL) chooses the next input from the queue
that is marked as favourite. All strategies are run with the
exponential power schedule.

1250

1000

750

500

250

0

Schedule

Both Strategies
No Strategy
Strategy 1
Strategy 2

s
e
h
s
a
r
C
e
u
q
n
U

 

i

 
f

o

 
r
e
b
m
u
N

0

5

10

15
Time (in hours)

20

25

Figure 11: #Crashes over Time (Search Strategies).

Results. The combination of both strategies is signiﬁ-
cantly more e↵ective than any of the strategies individually.
Until about 12 hours the other strategies perform very simi-
larly. After 24 hours as individual strategy, strategy 1 which
changes how AFL designates the favourite is more e↵ective
than strategy 2 and no strategy in the long run. As indi-
vidual strategy, the strategy 2 which changes the order in
which test inputs are chosen from the queue seems to be not
e↵ective at all. It performs similarly compared to running
AFLFast without any strategies (comparable to AFL but
with exponential power schedule). However, after 24 hours,
AFLFast with both strategies exposes almost twice as many
unique crashes as AFLFast with no strategy or with only
strategy 1.

Figure 10: #Crashes over Time (Schedules).

10For more details see Section 4.3.

10415.5 Result Summary

We evaluated AFLFast and several schedules plus search
strategies on the GNU binutils. The exponential schedule
outperforms all other schedules while our search strategies
turn out to be e↵ective. In eight runs of six hours, AFLFast
with an exponential schedule found an average of more than
one order of magnitude more unique crashes than AFL for
the tools nm and c++filt; it found crashing inputs for obj-
dump where AFL did not expose any crashes at all. In eight
runs of 24 hours, AFLFast found 6 vulnerabilities in nm 7x
faster than AFL and exposed 3 vulnerabilities that were not
exposed by AFL. AFLFast also exposes two bugs in nm (that
are unlikely exploitable) about seven times faster than AFL
and exposed one bug that is not exposed by AFL. An in-
dependent evaluation of Team Codejitsu on all 150 binaries
that are provided in the benchmark for the Cyber Grand
Challenge establishes similar results. On average, AFLFast
exposes an error 19 times faster than AFL and also exposes
7 errors that are not found by AFL, at all.
6. RELATED WORK

Several techniques [30, 17, 6, 22] have been proposed to in-
crease the e ciency of automated fuzzing. An important op-
timization pertains to selecting the seed inputs wisely from a
wealth of inputs [17]. Our work makes no assumptions about
the existance seed inputs; we seeded our experiments with
the empty ﬁle. However, Coverage-based Greybox Fuzzing
(CGF) would clearly beneﬁt from a smart seed selection if
many seed ﬁles are available. Others suggest to use program
analysis to detect dependencies among the bit positions of
an input [6]. For instance, the image width occupies four
bytes in the PNG image ﬁle format which are best modiﬁed
together. The dependency analysis allows to fuzz such de-
pendent bytes as a group. In our work, we do not change the
mutation operators or ratio. Woo et al.
[30] recognize the
exploration-exploitation trade-o↵ between fuzzing an input
for a shorter versus a longer amount of time. They proceed
to model blackbox fuzzing as a multi-armed bandit problem
where the seed’s “energy” is computed based on whether or
not it has exposed a (unique) crash in any previous fuzzing
iteration. So, the fuzzer is e↵ectively biased towards gener-
ating more crashing inputs for already known errors. In our
work, there is no such bias. Instead, we direct the search
towards low-frequency paths in order to stress more of the
program’s behavior in the same time.

Symbolic execution-based whitebox fuzzers can generate
ﬁles that stress low-frequency paths. Probabilistic symbolic
execution [10] uses model counting to compute the proba-
bility that a random input exercises a given path. Symbolic
execution is very e↵ective because it enumerates paths es-
sentially independent of their “frequency” and because it can
be directed towards “dangerous” program locations [5, 8, 11,
2].
It can generate the speciﬁc values that are needed in
order to negate an if-condition and exercise the alternative
branch. Taint-based fuzzing [9, 29] is a directed whitebox
fuzzing technique. It exploits classical taint analysis to lo-
calize parts of the input which should be marked symbolic.
For instance, it marks portions of the input ﬁle as sym-
bolic that control arguments of executed and critical system
calls. Model-based Whitebox Fuzzing [16] leverages an input
model to synthesize and “transplant” complete data chunks
to exercise so called critical branches that are only exercised
if a certain data chunk is present in the input ﬁle. However,

symbolic execution-based techniques rely on program anal-
ysis and constraint solving which hampers their scalability.
Imprecisions during lifting of the program binary and during
the encoding of the path constraints hamper their applica-
bility.
In contrast, CGF completely relinquishes program
analysis for the sake of scalability with tremendous success
in the vulnerability detection practice [27].

Colleagues have combined lightweight blackbox/greybox
fuzzers and symbolic execution-based whitebox fuzzers to
get the best of both worlds [19, 15]. For instance, Hybrid-
Fuzz ﬁrst runs symbolic execution to generate inputs leading
to “frontier nodes” and then passes these inputs to a black-
box fuzzer. In contrast, Driller [19] begins with AFL and
seeks help from symbolic execution when it “gets stuck”, for
instance, to generate a magic number. Our monotonous
power schedules allow to employ expensive symbolic execu-
tion for seeds/states with a su ciently high energy.

Markov chains can model a variety of random processes in
fuzz testing. Markov Chain Monte Carlo Random Testing
(MCMC-RT) uses a Markov Chain Monte Carlo (MCMC)
method to leverage knowledge about an input’s probability
to reveal an error. However, MCMC-RT is not entirely scal-
able because it maintains this probability for every input in
the program’s input space. While CGF can be well explained
as Markov chain, it does not actually maintain the chain or
any probabilities in-memory. While MCMC-RT is biased to-
wards revealing suspected or known errors, CGF can expose
unknown errors that hide deep in the program. The bias of
boosted CGF is towards low-frequency paths. Chen et al.
[7] utilize MCMC to leverage knowledge about a mutation
operator’s e↵ectiveness. Operators that have been shown to
be more e↵ective in previous fuzzing iterations are chosen
with greater probability during fuzzing. Sparks et al.
[18]
model program control-ﬂow as Markov chain to prioritize
seeds that exercise less explored paths. In contrast, we use
Markov chains to explain why it is more e cient to smartly
control the time spent fuzzing a seed and which seed to fuzz
next without program analysis.

7. CONCLUSION

While symbolic execution-based techniques have gained
prominence, their scalability has not approached those of
blackbox or greybox fuzzers. While blackbox and greybox
techniques have shown e↵ectiveness, the limited semantic
oversight of these techniques do not allow us to explain the
working of these techniques even when they are e↵ective.

In this work, we take a state-of-the-art greybox fuzzer
AFL which keeps track of path identiﬁers. We enhance the
e↵ectiveness and e ciency of AFL in producing crashes, as
evidenced by our experiments and those of our collabora-
tors. AFLFast, our extension of AFL exposes an order of
magnitude more unique crashes than AFL in the same time
budget. Moreover, AFLFast can expose several bugs and
vulnerabilities that AFL cannot ﬁnd. Other vulnerabilities
AFLFast exposes substantially earlier than AFL.

More importantly, we provide an explanation of the en-
hanced e↵ectiveness by visualizing CGF as the exploration
of the state space of a Markov chain. We observe that ex-
isting CGF tools much too often visit states in high-density
regions. We have devised and investigated several strategies
to force the CGF tool to visit more states that are otherwise
hidden in a low-density region and to generate less inputs
for states in a high-density region.

10428. ACKNOWLEDGMENTS

This research was partially supported by a grant from
the National Research Foundation, Prime Minister’s O ce,
Singapore under its National Cybersecurity R&D Program
(TSUNAMi project, No. NRF2014NCR-NCR001-21) and
administered by the National Cybersecurity R&D Directorate.

9. REFERENCES
[1] F. Bellard. Qemu, a fast and portable dynamic

translator. In Proceedings of the Annual Conference
on USENIX Annual Technical Conference, ATEC ’05,
pages 41–41, 2005.

[15] B. S. Pak. Hybrid fuzz testing: Discovering software
bugs via fuzzing and symbolic execution. In Master’s
thesis, School of Computer Science, Carnegie Mellon
University, 2012.

[16] V.-T. Pham, M. B¨ohme, and A. Roychoudhury.

Model-based whitebox fuzzing for program binaries. In
Proceedings of the 31st IEEE/ACM International
Conference on Automated Software Engineering, ASE,
pages 552–562, 2016.

[17] A. Rebert, S. K. Cha, T. Avgerinos, J. Foote,

D. Warren, G. Grieco, and D. Brumley. Optimizing
seed selection for fuzzing. In Proceedings of the 23rd
USENIX Conference on Security Symposium, SEC’14,
pages 861–875, 2014.

[2] M. B¨ohme, B. C. d. S. Oliveira, and A. Roychoudhury.

[18] S. Sparks, S. Embleton, R. Cunningham, and C. Zou.

Regression tests to expose change interaction errors.
In Proceedings of the 2013 9th Joint Meeting on
Foundations of Software Engineering, ESEC/FSE
2013, pages 334–344, 2013.

Automated Vulnerability Analysis: Leveraging
Control Flow for Evolutionary Input Crafting. In 23d
Annual Computer Security Applications Conference
(ACSAC), pages 477–486, 2007.

[3] M. B¨ohme and S. Paul. A probabilistic analysis of the

[19] N. Stephens, J. Grosen, C. Salls, A. Dutcher,

e ciency of automated software testing. IEEE
Transactions on Software Engineering, 42(4):345–360,
April 2016.

[4] S. Brin and L. Page. The anatomy of a large-scale

hypertextual web search engine. In Proceedings of the
Seventh International Conference on World Wide Web
7, WWW7, pages 107–117, 1998.

R. Wang, J. Corbetta, Y. Shoshitaishvili, C. Kruegel,
and G. Vigna. Driller: Augmenting fuzzing through
selective symbolic execution. In NDSS ’16, pages 1–16,
2016.

[20] Tool. Aﬂ binary instrumentation. https://github.com/

vrtadmin/moﬂow/tree/master/aﬂ-dyninst. Accessed:
2016-05-13.

[5] C. Cadar, D. Dunbar, and D. Engler. Klee: Unassisted

[21] Tool. Aﬂ vulnerability trophy case.

and automatic generation of high-coverage tests for
complex systems programs. In Proceedings of the 8th
USENIX Conference on Operating Systems Design
and Implementation, OSDI’08, pages 209–224, 2008.

[6] S. K. Cha, M. Woo, and D. Brumley.

Program-adaptive mutational fuzzing. In Proceedings
of the 2015 IEEE Symposium on Security and
Privacy, SP ’15, pages 725–741, 2015.

[7] Y. Chen, T. Su, C. Sun, Z. Su, and J. Zhao.
Coverage-directed di↵erential testing of jvm
implementations. In PLDI’ 16, pages 85–99, 2016.

[8] V. Chipounov, V. Kuznetsov, and G. Candea. S2e: A

platform for in-vivo multi-path analysis of software
systems. In ASPLOS XVI, pages 265–278, 2011.
[9] V. Ganesh, T. Leek, and M. Rinard. Taint-based

directed whitebox fuzzing. In Proceedings of the 31st
International Conference on Software Engineering,
ICSE ’09, pages 474–484, 2009.

[10] J. Geldenhuys, M. B. Dwyer, and W. Visser.

Probabilistic symbolic execution. In Proceedings of the
2012 International Symposium on Software Testing
and Analysis, ISSTA 2012, pages 166–176, 2012.
[11] P. Godefroid, M. Y. Levin, and D. Molnar. Sage:

Whitebox fuzzing for security testing. Queue,
10(1):20:20–20:27, Jan. 2012.

[12] S. Kirkpatrick, C. Jr. Gelatt, and M. Vecchi.

Optimization by simulated annealing. Science,
220(4598):671–680, 1983.

http://lcamtuf.coredump.cx/aﬂ/#bugs. Accessed:
2016-05-13.

[22] Tool. American fuzzy lop (aﬂ) fuzzer.

http://lcamtuf.coredump.cx/aﬂ/technical details.txt.
Accessed: 2016-05-13.

[23] Tool. Peach Fuzzer Platform. http:

//www.peachfuzzer.com/products/peach-platform/.
Accessed: 2016-05-13.

[24] Tool. Pulling jpegs out of thin air.

https://lcamtuf.blogspot.com/2014/11/
pulling-jpegs-out-of-thin-air.html. Accessed:
2016-05-13.

[25] Tool. SPIKE Fuzzer Platform.

http://www.immunitysec.com. Accessed: 2016-05-13.

[26] Tool. Suley Fuzzer.

https://github.com/OpenRCE/sulley. Accessed:
2016-05-13.

[27] Tool. Symbolic execution in vulnerability research.

https://lcamtuf.blogspot.sg/2015/02/
symbolic-execution-in-vuln-research.html. Accessed:
2016-05-13.

[28] Tool. Zzuf: multi-purpose fuzzer.

http://caca.zoy.org/wiki/zzuf. Accessed: 2016-05-13.
[29] T. Wang, T. Wei, G. Gu, and W. Zou. Taintscope: A

checksum-aware directed fuzzing tool for automatic
software vulnerability detection. In Proceedings of the
2010 IEEE Symposium on Security and Privacy, SP
’10, pages 497–512, 2010.

[13] B. P. Miller, L. Fredriksen, and B. So. An empirical

[30] M. Woo, S. K. Cha, S. Gottlieb, and D. Brumley.

study of the reliability of unix utilities. Commun.
ACM, 33(12):32–44, Dec. 1990.

[14] J. R. Norris. Markov Chains (Cambridge Series in

Statistical and Probabilistic Mathematics). Cambridge
University Press, July 1998.

Scheduling black-box mutational fuzzing. In
Proceedings of the 2013 ACM SIGSAC Conference on
Computer & Communications Security, CCS ’13,
pages 511–522, 2013.

1043Limiting the Impact of Stealthy Attacks on Industrial

Control Systems

David I. Urbina1, Jairo Giraldo1, Alvaro A. Cardenas1, Nils Ole Tippenhauer2,

Junia Valente1, Mustafa Faisal1, Justin Ruths1, Richard Candell3, and Henrik Sandberg4

1University of Texas at Dallas, 2Singapore University of Technology and Design,

3National Institute of Standards and Technology, and 4KTH Royal Institute of Technology

{david.urbina, jairo.giraldo, alvaro.cardenas, juniavalente, mustafa.faisal, jruths}@utdallas.edu,

nils_tippenhauer@sutd.edu.sg, richard.candell@nist.gov, and hsan@kth.se

ABSTRACT
While attacks on information systems have for most prac-
tical purposes binary outcomes (information was manipu-
lated/eavesdropped, or not), attacks manipulating the sen-
sor or control signals of Industrial Control Systems (ICS) can
be tuned by the attacker to cause a continuous spectrum in
damages. Attackers that want to remain undetected can at-
tempt to hide their manipulation of the system by following
closely the expected behavior of the system, while injecting
just enough false information at each time step to achieve
their goals.

In this work, we study if physics-based attack detection
can limit the impact of such stealthy attacks. We start with
a comprehensive review of related work on attack detection
schemes in the security and control systems community. We
then show that many of these works use detection schemes
that are not limiting the impact of stealthy attacks. We pro-
pose a new metric to measure the impact of stealthy attacks
and how they relate to our selection on an upper bound on
false alarms. We ﬁnally show that the impact of such attacks
can be mitigated in several cases by the proper combination
and conﬁguration of detection schemes. We demonstrate
the e↵ectiveness of our algorithms through simulations and
experiments using real ICS testbeds and real ICS systems.

Keywords
Industrial Control Systems; Intrusion Detection; Security
Metrics; Stealthy Attacks; Physics-Based Detection; Cyber-
Physical Systems

1.

INTRODUCTION

One of the fundamentally unique and intrinsic proper-
ties of Industrial Control Systems (ICS)—when compared
to general Information Technology (IT) systems— is that
changes in the system’s state must follow immutable laws of
physics. For example, the physical properties of water sys-

Publication rights licensed to ACM. ACM acknowledges that this contribution was
authored or co-authored by an employee, contractor or afﬁliate of the United States
government. As such, the Government retains a nonexclusive, royalty-free right to
publish or reproduce this article, or to allow others to do so, for Government purposes
only.
CCS’16, October 24 - 28, 2016, Vienna, Austria
© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978388

tems (ﬂuid dynamics) or the power grid (electromagnetics)
can be used to create prediction models that we can then
use to conﬁrm that the control commands sent to the ﬁeld
were executed correctly and that the information coming
from sensors is consistent with the expected behavior of the
system: if we opened an intake valve, we would expect the
water tank level to rise, otherwise we may have a problem
with the control, actuator, or the sensor.

The idea of using physics-based models of the normal op-
eration of control systems to detect attacks has been used in
an increasing number of publications in security conferences
in the last couple of years. Applications include water con-
trol systems [21], state estimation in the power grid [35, 36],
boilers in power plants [67], chemical process control [10],
electricity consumption data from smart meters [40], and a
variety of industrial control systems [42].

The growing number of publications shows the importance
of leveraging the physical properties of control systems for
security; however, a missing element in this growing body
of work is a uniﬁed adversary model and security metric to
help us compare the e↵ectiveness of previous proposals. In
particular, the problem we consider is one where the attacker
knows the attack-detection system is in place and bypasses it
by launching attacks imitating our expected behavior of the
system, but di↵erent enough that over long periods of time
it can drive the system to an unsafe operating state. This
attacker is quite powerful and can provide an upper bound
on the worst performance of our attack-detection tools.
Contributions. (i) We propose a strong adversary model
that will always be able to bypass attack-detection mech-
anisms and propose a new evaluation metric for attack-
detection algorithms that quantiﬁes the negative impact of
these stealthy attacks and the inherent trade-o↵ with false
alarms. Our new metric helps us compare in a fair way
previously proposed attack-detection mechanisms.

(ii) We compare previous attack-detection proposals across
three di↵erent experimental settings: a) a testbed operating
real-world systems, b) network data we collected from an
operational large-scale Supervisory Control and Data Acqui-
sition (SCADA) system that manages more than 100 Pro-
grammable Logic Controllers (PLCs), and c) simulations.

(iii) Using these three scenarios we ﬁnd the following re-
sults: (a) while the vast majority of previous work uses state-
less tests on residuals, stateful tests are better in limiting
the impact of stealthy attackers (for the same levels of false
alarms), (b) limiting the impact of a stealthy attacker can
also depend on the speciﬁc control algorithm used and not
only on the attack-detection algorithm, (c) linear state-space

1092models outperform output-only autoregressive models, (d)
time and space correlated models outperform models that
do not exploit these correlations, and (e) from the point of
view of an attacker, launching undetected actuator attacks is
more di cult than launching undetected false-data injection
for sensor values.

The remainder of this paper is organized as follows: In
§ 2, we provide the scope of the paper, and provide the
background to analyze previous proposals. We introduce
our attacker model and the need for new metrics in § 3. We
introduce a way to evaluate the impact of undetected attacks
and attack-detection systems in § 4, and then we use this
adversary model and metric to evaluate the performance of
these systems in physical testbeds, real-world systems, and
simulations in § 5.
2. BACKGROUND AND TAXONOMY
Scope of Our Study. We focus on using real-time mea-
surements of the physical world to build indicators of at-
tacks. In particular, we look at the physics of the process un-
der control but our approach can be extended to the physics
of devices as well [18]. Our work is motivated by false sensor
measurements [35, 58] or false control signals like manipu-
lating vehicle platoons [19], manipulating demand-response
systems [58], and the sabotage Stuxnet created by manip-
ulating the rotation frequency of centrifuges [17, 32]. The
question we are trying to address is how to detect these
false sensor or false control attacks in real-time.
2.1 Background

A general feedback control system has ﬁve components:
(1) the physical phenomena of interest (sometimes called
the process or plant), (2) sensors that send a time series yk
denoting the value of the physical measurement zk at time
k (e.g., the voltage at 3am is 120kV) to a controller, (3)
based on the sensor measurements received yk, the controller

K(yk) sends control commands uk (e.g., open a valve by 10

%) to actuators, and (4) actuators that produce a physical
change vk in response to the control command (the actuator
is the device that opens the valve).

A general security monitoring architecture for control sys-
tems that looks into the “physics” of the system needs an
anomaly detection system that receives as inputs the sensor
measurements yk from the physical system and the control
commands uk sent to the physical system, and then uses
them to identify any suspicious sensor or control commands
is shown in Fig. 1.
2.2 Taxonomy

Anomaly detection is usually performed in two steps. First
we need a model of the physical system that predicts the
output of the system ˆyk. The second step compares that
prediction ˆyk to the observations yk and then performs a
statistical test on the di↵erence. The di↵erence between
prediction and observation is usually called the residual rk.
We now present our new taxonomy for related work, based
on four aspects: (1) physical model, (2) detection statistic,
(3) metrics, and (4) validation.
Physical Model. The model of how a physical system be-
haves can be developed from physical equations (Newton’s
laws, ﬂuid dynamics, or electromagnetic laws) or it can be
learned from observations through a technique called system
identiﬁcation [4, 38]. In system identiﬁcation one often has
to use either Auto-Regressive Moving Average with eXoge-
nous inputs (ARMAX) or linear state-space models. Two

Figure 1: Di↵erent attack points in a control sys-
tem: (1) Attack on the actuators (blue): vk j uk, (2)
Attack on the sensors (purple): yk j zk, (3) Attack

on the controller (red): uk j K(yk)

popular models used by the papers we survey are Auto-
Regressive (AR) models and Linear Dynamical State-
space (LDS) models.

An AR model for a time series yk is given by

ˆyk+1 =

k=

i=k N

↵iyi + ↵0

(1)

where ↵i are obtained through system identiﬁcation and yi
the last N sensor measurements. The coe cients ↵i can be
obtained by solving an optimization problem that minimizes
the residual error (e.g., least squares) [37].

If we have inputs (control commands uk) and outputs
(sensor measurements yk) available, we can use subspace
model identiﬁcation methods, producing LDS models:

xk+1 = Axk + Buk + ✏k
yk = Cxk + Duk + ek

(2)

where A, B, C, and D are matrices modeling the dynamics
of the physical system. Most physical systems are strictly
causal and therefore D = 0 in general. The control com-
mands uk " Rp a↵ect the next time step of the state of the
system xk " Rn and sensor measurements yk " Rq are mod-
eled as a linear combination of these hidden states. ek and
✏k are sensor and perturbation noise, and are assumed to be
a random process with zero mean. To make a prediction,
we i) ﬁrst need yk and uk to obtain a state estimate ˆxk+1
and ii) use the estimate to predict ˆyk+1 = C ˆxk+1. A large
body of work on power systems employs the second equation
from Eq. (2) without the dynamic state equation. We refer
to this special case of LDS used in power systems as Static
Linear State-space (SLS) models.
Detection Statistic. If the observations we get from sen-
sors yk are signiﬁcantly di↵erent from the ones we expect
(i.e., if the residual is large) we generate an alert. A State-
less test, raises an alarm for every deviation at time k: i.e.,

if∂yk   ˆyk∂ = rk ' ⌧ , where ⌧ is a threshold.

In a Stateful test we compute an additional statistic Sk
that keeps track of the historical changes of rk (no mat-
ter how small) and generate an alert if Sk ' ⌧ ,
if
there is a persistent deviation across multiple time-steps.
There are many tests that can keep track of the histori-
cal behavior of the residual rk such as taking an average
over a time-window, an exponential weighted moving aver-
age (EWMA), or using change detection statistics such as
the non-parametric CUmulative SUM (CUSUM) statistic.

i.e.,

The nonparametric CUSUM statistic is deﬁned recursively

as S0 = 0 and Sk+1 =(Sk +∂rk∂    )+, where(x)+ represents
max(0, x) and   is selected so that the expected value of
∂rk∂     < 0 under hypothesis H0 (i.e.,   prevents Sk from

1093increasing consistently under normal operation). An alert
is generated whenever the statistic is greater than a previ-
ously deﬁned threshold Sk > ⌧ and the test is restarted with
Sk+1 = 0. The summary of our taxonomy for modeling the
system and to detect an anomaly in the residuals is given in
Fig. 2

Detection
Residual Generation

yk
yk 1
uk

ˆyk

rk = yk   ˆyk

Physical 
Model
LDS or AR

rk

Anomaly 
Detection:
Sateless or 
Stateful

alert

Figure 2: The detection block from Fig. 1 focusing
on our taxonomy.

Metrics. An evaluation metric is used to determine the ef-
fectiveness of the physics-based attack detection algorithm.
Popular evaluation metrics are the True Positive Rate (TPR)
and the False Positive Rate (FPR)—the trade-o↵ between
these two numbers is called the Receiver Operating Char-
acteristic (ROC) curve. Some papers just plot the residuals
(without quantifying the TPR or FPR values), and other
papers just measure the impact of attacks.
Validation. The experimental setting to validate proposals
can use simulations, data from real-world operating systems,
and testbeds. Testbeds can be classiﬁed as testbeds control-
ling a real-system or a testbed with Hardware-in-the-Loop
(HIL) where part of the physical system is simulated in a
computer. For our purposes a HIL testbed is similar to
having pure simulations, because the model of the physical
system is given by the algorithm running on a computer.
2.3 Limitations of Previous Work

There is a large variety of previous work but because of the
diversity of domains (e.g., power systems, industrial control,
and theoretical studies) and academic venues (e.g., security,
control theory, and power systems conferences), the ﬁeld
has not been presented in a uniﬁed way with a common
language that can be used to identify trends, alternatives,
and limitations. Using our previously deﬁned taxonomy, in
this section we discuss previous work and summarize our
results in Table 1.

The columns in Table 1 are arranged by conference venue
(we assigned workshops to the venue that the main confer-
ence is associated with), we also assigned conferences asso-
ciated with CPSWeek to control conferences because of the
overlap of attendees to both venues. We make the follow-
ing observations: (1) the vast majority of prior work use
stateless tests; (2) most control and power grid venues use
LDS (or their static counterpart SLS) to model the physical
system, while computer security venues tend to use a vari-
ety of models, several of them are non-standard and di cult
to replicate by other researchers; (3) there is no consistent
metric or adversary model used to evaluate proposed attack-
detection algorithms; and (4) no previous work has validated
their work with all three options: simulations, testbeds and
real-world data.

The ﬁrst three observations (1-3) are related: while previ-
ous work has used di↵erent statistical tests (stateless vs. state-
ful) and models of the physical system to predict its expected
behavior, so far they have not been compared against each

other, and this makes it di cult to build upon previous work
(it is impossible to identify best practices without a way
to compare di↵erent proposals). To address this problem
we propose a general-purpose evaluation metric in § 4 that
leverages our stealthy adversary model, and then compare
previously proposed methods. Our results show that while
stateless tests are more popular in the literature, stateful
tests are better to limit the impact of stealthy attackers.
In addition, we show that LDS models are better than AR
models, that AR models proposed in previous work can be
improved by leveraging correlation among di↵erent signals,
and that having an integral controller can limit the impact
of stealthy actuation attacks.

To address point (4) we conduct experiments using all
three options: a testbed with a real physical process under
control § 5.1, real-world data § 5.2, and simulations § 5.3. We
show the advantages and disadvantages of each experimental
setup, and the insights each of these experiments provide.

3. MOTIVATING EXAMPLE

The testbed we use for our experiments is a room-size,
water treatment plant consisting of 6 stages to purify raw
water. The testbed has a total of 12 PLCs (6 main PLCs and
6 in backup conﬁguration to take over if the main PLC fails).
The general description of each stage is as follows: Raw wa-
ter storage is the part of the process where raw water is
stored and it acts as the main water bu↵er supplying water
to the water treatment system. It consists of one tank, an
on/o↵ valve that controls the inlet water, and a pump that
transfers the water to the ultra ﬁltration (UF) tank. In Pre-
treatment the Conductivity, pH, and Oxidation-Reduction
Potential (ORP) are measured to determine the activation of
chemical dosing to maintain the quality of the water within
some desirable limits. This stage is illustrated in Fig. 3 and
will be used in our motivating example. Ultra Filtration is
used to remove the bulk of the feed water solids and col-
loidal material by using ﬁne ﬁltration membranes that only
allow the ﬂow of small molecules. After the small residu-
als are removed by the UF system, the remaining chlorines
are destroyed in the Dechlorinization stage, using ultraviolet
chlorine destruction unit and by dosing a solution of sodium
bisulphite. Reverse Osmosis (RO) system is designed to
reduce inorganic impurities by pumping the ﬁltrated and
dechlorinated water with a high pressure. Finally, in RO
ﬁnal product stage stores the RO product (clean water).

Figure 3: Stage controlling the pH level.

Attacking the pH level. In this process, the water’s pH
level is controlled by dosing the water with Hydrochloric
Acid (HCl). Fig. 4 illustrates the normal operation of the
plant: if the pH sensor reports a level above 7.05, the PLC
sends a signal to turn On the HCl pump, and if the sensor
reports a level below 6.95, it sends a signal to turn it O↵.

1094Table 1: Taxonomy of related work. Columns are organized by publication venue.

.
l
a

t
e

g
r
e
b
d
n
a
S

.
l
a

t
e

a
r
i
e
x
i
e
T

]
4
5
[

]
9
5
[

.
l
a

t
e

a
b
b
o
B

]
8
[

i
l
o
p
o
n
S

i

,
o
M

.
l
a

t
e

o
M

]
6
4
[

]
4
4
[

a
t
p
u
G

,
i
a
B

]
6
[

.
l
a

t
e

i
a
B

]
7
[

.
l
a

t
e

i
t
t
e
l
a
u
q
s
a
P

.
l
a

t
e

a
r
i
e
x
i
e
T

.
l
a

t
e

n
o
w
K

.
l
a

t
e

a
r
i
e
x
i
e
T

.
l
a

t
e

o
D

.
l
a

t
e

o
a
i
M

.
l
a

t
e

u
o
H

.
l
a

t
e

i
s
i
y
E

.
l
a

t
e

o
M

]
3
4
[

]
3
2
[

]
6
1
[

]
5
4
[

]
0
5
[

]
1
6
[

]
1
3
[

]
0
6
[

]
5
1
[

.
l
a

t
e

i

n
m
A

]
2
,
1
[

.
l
a

t
e

s
n
r
e
K

.
l
a

.
l
a

t
e

t
e

g
n
a
i
L

i

n
a
i
G

h
t
i

m
S

g
r
e
b
d
n
a
S

,

n
a
D

.
l
a

t
e

t
u
s
o
K

.
l
a

t
e

s
i
v
a
D

r
o
o
P

,

m
K

i

u
s
a
r
a
d
n
i
v
o
G

,
r
a
h
d

i
r
S

.
l
a

t
e

a
i
r
d
n
a
s
t
u
o
K

.
l
a

t
e

i

a
m
h
s
a
M

]
6
5
[

]
5
2
[

]
3
3
[

]
0
2
[

]
3
1
[

]
8
2
[

]
6
2
[

]
4
1
[

]
7
5
[

]
9
2
[

]
0
4
[

.
l
a

t
e

c
i
v
o
n
a
m
s
o
i
z
d
a
H

.
l
a

t
e

s
a
n
e
d
r
a
C

s
a
n
e
d
r
a
C

,
e
t
n
e
l
a
V

.
l
a

t
e

g
n
a
W

n

i
l

h
g
u
a
L
c
M

.
l
a

t
e

d
a
j
j
a
S

.
l
a

t
e

a
i

n
a
v
r
a
P

.
l
a

t
e

y
r
k
u
o
h
S

.
l
a

t
e

i

n
L

.
l
a

t
e

l

ﬁ
o
t
o
r
K

n
a
D

,
c
i
v
o
k
u
V

.
l
a

t
e
w
o
r
r
o
M

.
l
a

t
e

i

u
C

]
9
4
[

]
4
3
[

]
5
5
[

]
1
2
[

]
0
1
[

]
7
6
[

]
2
4
[

]
3
5
[

]
5
6
[

]
0
3
[

]
6
6
[

]
7
4
[

]
2
1
[

.
l
a

t
e

i

u
L

]
6
3
,
5
3
[

.
l
a

t
e

o
n
a
c
r
a
C

]
9
[

.
l
a

t
e

i
e
H

.
l
a

t
e

s
s
i
K

]
2
2
[

]
7
2
[

Venue

Control

Smart/Power Grid

Security

Misc.

stateless c c c -
stateful

-

-

- c c c G# c c c c c - c G# c - c c c c c - c - c G# c - c - G# G# -

- l - c -

- l - c -

- c -

- l -

- c c c c c - c
- c - l -

- c c c -

-

-

-

-

-

-

-

-

-

-

-

-

- G# l l -

-

-
-

Detection Statistic

Physical Model

LDS
other
Metricsò
impact
statistic

AR -

-

-

SLS c c G# -

-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-

-

- c c c c c c c c c c c c c c c c -
-
-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

-

- c c c c c G# -

-

-

-
-

-

-
-

-

-
-

-

-

- c -
-
-

-
- c -
-
-
-
- c c -

-

- c -
-
-

-
-
-
-
- c - c -

-

-
-

-
-
-
-
- c -

-
- c c - c c -

-

-

-

-

-

-

- G# G# c -

-

-

-

-
-

-

-
-

-
-
-

-
-
-

- G# G# -

- c G# c

- c - c -
-
TPR -
FPR -

- c - c -
-
-

- c c -
- c -
-

-
-

- c - c c - c c c c -

-

- c c -

- c c c c - c -
-
-

- c -
- c -

- c -
- c -

-
-

-
-

-
-

-
-

- c c - c - c -
-
- c -
-
- c - c -

- c c c - c -
- c - c -
-
-
-

- c -
- c - c -
- c -
-

- c -
-
-

- c -
-
-
- c c - c -
- c -
- c -

- c - c - c -
- c
-
-

- c c c - c -
-
-

- c -
- c -

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-

-

Validation
simulation
real data
testbed

- c c c c c c c c c c c c c - c - c c c - c c c c c G# - c - c -

- c -

-

-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-
-

-

-
- c -

- G# c - c -

-

-

-

-

-

-

-
- c -

-
-

-
-

-
-

-
-

-
-

-
-

-

- c c - c - c c c c -

-

-
- c -

- c -
-

-

-
-

-
-

- c
- c -
-

-
- c -

- c -

- G# - c c -

Legend: c: feature considered by authors, G#: feature assumed implicitly but exhibits ambiguity, l: a windowed stateful
detection method is used, òEvaluation options have been abbreviated in the table: Attack Impact, Statistic Visualization,
True Positive Rate, False Positive Rate.

The wide oscillations of the pH levels occur because there is
a delay between the control actions of the HCl pump, and
the water pH responding to it.

by injecting a malicious device in the EtherNet/IP ring of
the testbed, given that the implementation of this protocol
is unauthenticated. A detailed implementation of our attack
is given in our previous work [64]. In particular, our MitM
intercepts sensor values coming from the HCL pump and
the pH sensor, and intercept actuator commands going to
the HCl pump, to inject false sensor readings and commands
sent to the PLC and HCl pump.

Figure 4: During normal operation, the water pH is
kept in safe levels.

To detect attacks on the PLC, the pump or the sensor,
we need to create a model of the physical system. While
the system is nonlinear, let us ﬁrst attempt it to model it
as time-delayed LDS of order 2. The model is described by
pHk+1 = pHk + uk Tdelay , where we estimate (by observing
the process behavior) uk Tdelay =  0.1 after a delay of 35
time steps after the pump is turned On, and 0.1 after a delay
of 20 time steps after it is turned O↵. We then compare the
predicted and observed behavior, compute the residual, and
apply a stateless, and a stateful test to the residual. If either
of these statistics goes above a deﬁned threshold, we raise
an alarm.

We note that high or low pH levels can be dangerous.
In particular, if the attacker can drive the pH below 5, the
acidity of the water will damage the membranes of the Ultra
Filtration and Reverse Osmosis stages, the pipes, and even
sensor probes.

We launch a wired Man-In-The-Middle (MitM) attack be-
tween the ﬁeld devices (sensors and actuators) and the PLC

14
12
10
8
6
4

H
p
 
r
e
t
a
W

 

c
i
r
t
e
M
n
o
i
t
c
e
t
e
D

5
4
3
2
1
0

Real Water pH
Compromised pH

Attack

1

2

3

4

6

5
7
Time(min)

Stateful
Stateless

8

9

10 11

Alarm

1

2

3

4

6

5
7
Time(min)

8

9

10 11

Figure 5: Attack to the pH sensor.

Our attack sends false sensor data to the PLC, faking a
high pH level so the pump keeps running, and thus driving
the acidity of the water to unsafe levels, as illustrated in
Fig. 5. Notice that both, stateless and stateful tests detect
this attack (each test has a di↵erent threshold set to main-

OffOnPump State12345678910Time(min)66.577.588.5Water pHWater pH measureHCl Pump109510

8

6

4

1.5

1

0.5

H
p
 
r
e
t
a
W

c

 

i
r
t
e
M
n
o
i
t
c
e
t
e
D

1

2

3

4

5

6

Time(min)

Stateful
Stateless

Alarm

0

1

2

3

4

5

6

Time(min)

Attack

On
Off

p
m
u
P

 
l

Real Water pH
Compromised HCl Pump

C
H

7

8

9

10

7

8

9

10

Figure 6: Attack to the pump actuator.

tain a probability of false alarm of 0.01). We also launched
an attack on the pump (actuator). Here the pump ignores
O↵ control commands from the PLC, and sends back mes-
sages stating that it is indeed O↵, while in reality it is On.
As illustrated in Fig. 6, only the stateful test detects this
attack. We also launched several random attacks that were
easily detected by the stateful statistic, and if we were to
plot the ROC curve of these attacks, we would get 100%
detection rate.
Observations. As we can see, it is very easy to create
attacks that can be detected. Under these simulations we
could initially conclude that our LDS model combined with
the stateful anomaly detection are good enough; after all,
they detected all attacks we launched. However, are these
attacks enough to conclude that our LDS model is good
enough? And if these attacks are not enough, then which
types of attacks should we launch?

Notice that for any physical system, a sophisticated at-
tacker can spoof deviations that follow relatively close the
“physics” of the system while still driving the system to a
di↵erent state. How can we measure the performance of our
anomaly detection algorithm against these attacks? How
can we measure the e↵ectiveness of our anomaly detection
tool if we assume that the attacker will always adapt to our
algorithms and launch an undetected attack? And if our
algorithms are not good enough, how can we design better
algorithms? If by deﬁnition the attack is undetected, then
we will always have a 0% true positive rate, therefore we
need to devise new metrics to evaluate our systems.

Figure 7: Our attacker adapts to di↵erent detection
thresholds: If we select ⌧2 the adversary launches
an attack such that the detection statistic (dotted
blue) remains below ⌧2. If we lower our threshold to
⌧1, the adversary selects a new attack such that the
detection statistic (solid red) remains below ⌧1.

4. A STRONGER ADVERSARY MODEL

We assume an attacker that has compromised a sensor
(e.g. pH level in our motivating example) or an actuator
(e.g. pump in our motivating example) in our system. We
also assume that the adversary has complete knowledge of
our system, i.e. she knows the physical model we use, the
statistical test we use, and the thresholds we select to raise
alerts. Given this knowledge, she generates a stealthy at-
tack, where the detection statistic will always remain below
the selected threshold.

While similar stealthy attacks have been previously pro-
posed [13, 35, 36], in this paper we extend them for generic
control systems including process perturbations and mea-
surement noise, we force the attacks to remain stealthy against
stateful tests, and also force the adversary to optimize the
negative impact of the attack. In addition, we assume our
adversary is adaptive, so if we lower the threshold to ﬁre
an alert, the attacker will also change the attack so that
the anomaly detection statistic remains below the thresh-
old. This last property is illustrated in Fig. 7.

Notice that this type of adaptive behavior is di↵erent from
how traditional metrics such as ROC curves work, because
they use the same attacks for di↵erent thresholds of the
anomaly detector. On the other hand, our adversary model
requires a new and unique (undetected) attack speciﬁcally
tailored for every anomaly detection threshold.
If we try
to compute an ROC curve under our adversary model we
would get a 0% detection rate because the attacker would
generate a new undetected attack for every anomaly detec-
tion threshold.

This problem is not unique to ROC curves: most popular
metrics for evaluating the classiﬁcation accuracy of intrusion
detection systems (like the intrusion detection capability, the
Bayesian detection rate, accuracy, expected cost, etc.) are
known to be a multi-criteria optimization problem between
two fundamental trade-o↵ properties: the false alarm rate,
and the true positive rate [11], and as we have argued, using
any metric that requires a true positive rate will be inef-
fective against our adversary model launching undetected
attacks.

Observation. Most intrusion detection metrics are varia-
tions of the fundamental trade-o↵ between false alarms and
true positive rates [11], however, our adversary by deﬁnition
will never be detected so we cannot use true positive rates
(or variations thereof). Notice however that by forcing our
adversary to remain undetected, we are e↵ectively forcing
her to launch attacks that follow closely the physical behav-
ior of the system (more precisely, we are forcing our attacker
to follow more closely our Physical Model), and by following
closer the behavior of the system, then the attack impact is
reduced: the attack needs to appear to be a plausible phys-
ical system behavior. So the trade-o↵ we are looking for
with this new adversary model is not one of false positives
vs.
true positives, but one between false positives and the
impact of undetected attacks.

New Metric. To deﬁne precisely what we mean by impact
of undetected attack we select one (or more) variables of
interest (usually a variable whose compromise can a↵ect the
safety of the system) in the process we want to control–
e.g., the pH level in our motivating example. The impact
of the undetected attack will then be, how much can the
attacker drive that value towards its intended goal (e.g., how
much can the attacker lower the pH level while remaining
undetected) per unit of time.

Therefore we propose a new metric consisting of the trade-

AttacksDT1096attack ya
measurements are compromised.

k is a new vector where some (or all) of the sensor

k+1

k+1 = arg maxya

to quantify the attack impact) subject to not raising an alert

 and f are the initial and ﬁnal attack times, respectively),
satisﬁes the equation: yaò

An optimal greedy-attack (yaò) at timek "[, f] (where
f(ya
k+1) (where
f(ya
k+1) is deﬁned by the designer of the detection method
(instead of max it can be min). For instance, if f(ya
k+1) =
k+1Ω, the greedy attack for a stateless test is: yaò
Ωyk+1 ya
k+1 = max{ya
Sk+1 & ⌧}. Because Sk+1 =(Sk +rk   ) the optimal attack is
k+1 = ˆyk+1±(⌧ +  Sk).

k+1 =
ˆyk+1 ± ⌧. The greedy optimization problem for an attacker
facing a stateful CUSUM test becomes yaò
k+1 ⇥
given when Sk = ⌧ , which results in yaò
For all attack times k greater than the initial time of attack
, Sk = ⌧ and yaò

k+1 = ˆyk+1 ±  .

Generating undetectable actuator attacks is more di -
cult than sensor attacks because in several practical cases
it is impossible to predict the outcome yk+1 with 100% accu-
racy, given the actuation attack signal vk in Fig. 1. For our
experiments when the control signal is compromised in § 5.3,
we use the linear state space model from Eq. (2) to do a re-
verse prediction from the intended yaò
k+1 to obtain the control
signal vk that will generate that next sensor observation.

Computing the X-axis of our Metric. Most of the lit-
erature that reports false alarms uses the false alarm rate
metric. This value obscures the practical interpretation of
false alarms: for example a 0.1% false alarm rate depends
on the number of times an anomaly decision was made, and
the time-duration of the experiment: and these are vari-
ables that can be selected: for example a stateful anomaly
detection algorithm that monitors the di↵erence between ex-
pected ˆyk and observed yk behavior has three options with
every new observation k: (1) it can declare the behavior as
normal, (2) it can generate an alert, (3) it can decide that
the current evidence is inconclusive, and it can decide to
take one more measurement yk+1.

Because the amount of time T that we have to observe the
process and then make a decision is not ﬁxed, but rather is
a variable that can be selected, using the false alarm rate is
misleading and therefore we have to use ideas from sequential
detection theory [24]. In particular, we use the average time
between false alarms TF A, or more precisely, the expected

time between false alarms E[TF A]. We argue that telling

security analysts that e.g., they should expect a false alarm
every hour is a more direct and intuitive metric rather than
giving them a probability of false alarm number over a deci-
sion period that will be variable if we use stateful anomaly
detection tests. This way of measuring alarms also deals
with the base rate fallacy, which is the problem where low
false alarm rates such as 0.1% do not have any meaning un-
less we understand the likelihood of attacks in the dataset
(the base rate of attacks). If the likelihood of attack is low,
then low false alarm rates can be deceptive [5].

In all the experiments, the usability metric for each evalu-
ated detection mechanism is obtained by counting the num-
ber of false alarms nF A for an experiment with a duration
TE under normal operation (without attack), so for each
threshold ⌧ we calculate the estimated time for a false alarm

by E[Tf a] ⌅ TE/nF A. Computing the average time be-

tween false alarms in the CUSUM test is more complicated
than with the stateless test. In the CUSUM case, we need to
compute the evolution of the statistic Sk for every threshold
we test, because once Sk hits the threshold we have to reset
it to zero.

Notice that while we have deﬁned a speciﬁc impact for

Figure 8: Illustration of our proposed tradeo↵ met-
ric. The y-axis is a measure of the maximum devi-
ation imposed by undetected attacks per time unit

 X/T U , and the x-axis represents the expected time
between false alarms E[Tf a]. Anomaly detection al-

gorithms are then evaluated for di↵erent points in
this space.

o↵ between the maximum deviation per time unit imposed
by undetected attacks (y-axis) and the expected time be-
tween false alarms (x-axis). Our proposed trade-o↵ metric
is illustrated in Fig. 8, and its comparison to the perfor-
mance of Receiver Operating Characteristic (ROC) curves
against our proposed adversary model is illustrated in Fig. 9.

Figure 9: Comparison of ROC curves with our pro-
posed metric: ROC curves are not a useful metric
against a stealthy and adaptive adversary.

Notice that while the y-axis of our proposed metric is com-
pletely di↵erent to ROC curves, the x-axis is similar, but
with a key di↵erence:
instead of using the probability of
false alarms, we use instead the expected time between false

alarms E[Tf a]. This quantity has a couple of advantages

over the false alarm rate: (1) it addresses the deceptive na-
ture of low false alarm rates due to the base-rate fallacy [5],
and (2) it addresses the problem that several anomaly de-
tection statistics make a decision (“alarm” or “normal be-
havior”) at non-constant time-intervals.

We now describe how to compute the y-axis and the x-axis

of our proposed metric.
4.1 Computing the X and Y axis of Fig. 8
Computing Attacks Designed for the Y-axis of our
Metric. The adversary wants to maximize the deviation
of a variable of interest yk (per time unit) without being
detected. The true value of this variable is yk, yk+1, . . . , yN ,
and the attack starts at time k, resulting in a new observed
time series ya
N . The goal of the attacker is to

k+1, . . . , ya

k, ya

maximize the distance maxi∂∂yi ya

i∂∂. Recall that in general

yk can be a vector of n sensor measurements, and that the

Longer time between false alarms = More UsableDetector 2 is better than Detector 1:For the same level of false alarms,undetected attackers can causeless damage to the systemLess deviation = More SecureUsability Metric: Expected time between false alarmsSecurity Metric:Maximum deviation imposed by undetected attacks per time unitTradeoﬀ Curve of Detector 1Tradeoﬀ Curve of Detector 2ROC forStronger Adversary ModelNew Metrics forStronger Adversary Model1097Algorithm 1: Computing Y axis

1: Deﬁne f(ya
k+1)
2: Select ⌧set ={⌧1,⌧ 2, . . .}, , f , and
Kset ={, . . . , kf   1}
3: æ(⌧, k) " ⌧set ✓ Kset, ﬁnd
f(ya
k+1(⌧) = arg max
k+1)

yaò

ya
k+1

4:

s.t.
Detection Statistic & ⌧

5: æ⌧ " ⌧set, calculate

y   axis = max
k"Kset

f(yaò
k+1(⌧))

Algorithm 2: Computing X axis

1: Observations Y na with no attacks of time-duration TE
2: æ⌧ " ⌧set, compute

Detection Statistic: DS(Y na)
Number of false alarms: nF A(DS,⌧)
x   axis = E[Tf a(⌧)] = TE/nF A

undetected attacks in our y-axis for clarity, we believe that
designers who want to evaluate their system using our met-
ric should deﬁne an appropriate worst case undetected attack
optimization problem speciﬁcally for their system. In par-
ticular, the y-axis can be a representation of a cost function
f of interest to the designer. There are a variety of metrics
(optimization objectives) that can be measured such as the
product degradation from undetected attacks, or the histor-

ical deviation of the system under attack <i∂yi   ˆya
deviation at the end of the attack∂yN   ˆya

mary of how to compute the y-axis and the x-axis of our
metric is given in Algorithms 1 and 2.

i∂ or the
N∂, etc. A sum-

5. EXPERIMENTAL RESULTS

Table 2: Advantages and disadvantages of di↵erent
evaluation setups.

Reliability of:

X-Axis

Y-Axis

Real Data
Testbed
Simulation

 
G#
#

#
G#
 

  = well suited, G# = partially suitable, # = least suitable

We evaluate anomaly detection systems under the light of
our Stronger Adversary Model (see section § 4), using our
new metrics in a range of test environments, with individ-
ual strengths and weaknesses (see Table 2). As shown in
the table, real-world data allows us to analyze operational
large-scale scenarios, and therefore it is the best way to test

the x-axis metric E[Tf a]. Unfortunately, real-world data

does not give researchers the ﬂexibility to launch attacks
and measure the impact on all parts of the system. Such
interactive testing requires the use of a dedicated physical
testbed.

A physical testbed has typically a smaller scale than a
real-world operational system, so the ﬁdelity in false alarms
might not be as good as with real data, but on the other
hand, we can launch attacks. The attacks we can launch are,
however, constrained because physical components and de-
vices may su↵er damage by attacks that violate the safety re-
quirements and conditions for which they were designed for.
Moreover, attacks could also drive the testbed to states that
endanger the operator’s and environment’s safety. There-
fore, while a testbed provides more experimental interaction
than real data, it introduces safety constraints for launching
attacks.

Simulations on the other hand, do not have these con-
straints and a wide variety of attacks can be launched. So
our simulations will focus on attacks to actuators and demon-
strate settings that cannot be achieved while operating a
real-world system because of safety constraints. Simulations
also allow us to easily change the control algorithms and to
our surprise, we found that control algorithms have a big
impact on the ability of our attacker to achieve good results
in the y-axis of our metric. However, while simulations allow
us to test a wide variety of attacks, the problem is that the
false alarms measured with a simulation are not going to be
as representative as those obtained from real data or from a
testbed.
5.1 Physical Testbed (EtherNet/IP packets)

In this section, we focus on testbeds that control a real
physical process, as opposed to testbeds that use a Hardware-
In-the-Loop (HIL) simulation of the physical process. A HIL
testbed is similar to the experiments we describe in § 5.3.
We developed an attacker who has complete knowledge
of the physical behavior of the system and can manipulate
EtherNet/IP packets and inject attacks. We now apply our
metric to the experiments we started in section § 3.
Attacking pH Level. Because this system is highly non-
linear, apart from the simple physical model (LDS) of or-
der 2 we presented in section § 3, we also applied a system
identiﬁcation to calculate higher order system models: an
LDS model of order 20 and two nonlinear models (order
50 and 100) based on wavelet networks [52]. Fig. 10 shows
the minimum pH achieved by the attacker after 4-minutes
and against three di↵erent models. Notice that the nonlin-
ear models limited the impact of the stealthy attack by not
allowing deviations below a pH of 5, while our linear model
(which was successful in detecting attacks in our motivating
example) was not able to prevent the attacker from taking
the pH below 5.

H
p

8

7

6

5

5

Attack

pH with Nonlinear order-100
pH with Nonlinear order-50
pH with LDS order-20
pH without Attack
6

7

89
Time (min)

10

11 12

Figure 10: pH deviation imposed by greedy attacks
while using stateful detection (⌧ = 0.05) with both,
LDS and nonlinear models.

1098water level attacks with di↵erent increment rates, starting
from the Low level setting and stopping at the High level
setting, and their induced maximum   over the real level.
Only attacks a1 and a2 achieve a successful overﬂow (only
a2 achieves a water spill), while a3 deviates the water level
without overﬂow. In our experiment, High corresponds to a
water level of 0.8 m and Low to 0.5 m. Overﬂow occurs at
1.1 m. The testbed has a drainage system to allow attacks
that overﬂow the tank.

Figure 12: Impact of di↵erent increment rates on
overﬂow attack. The attacker has to select the rate
of increase with the lowest slope while remaining
undetected.

c
e
s
 
/
 

m

0.3

0.25

0.2

0.15

0.1

0.05

0

0

=0.3

0.3

0.2

0.1

0

0

=0.0042

=1

=2

=2.5

=0.0014

=0.0011

0.1
50

0.2

0.3

100
E[Tfa] (min)

Stateless
Stateful

150

Figure 13: Comparison of stateful and stateless de-
tection. At 0.3m the tank overﬂows, so stateless
tests are not good for this use case. ⌧b,⌧ c correspond

to the threshold associated to some E[Tf a].

Because it was derived from “ﬁrst principles”, our LDS
model is a highly accurate physical model of the system, so
there is no need to test alternative physical models. How-
ever, we can combine our LDS model with a stateless test,
and with a stateful test and see which of these detection
tests can limit the impact of stealthy attacks.

In particular, to compute our metric we need to test state-
less and stateful mechanisms and obtain the security metric
that quantiﬁes the impact   of undetected attacks for sev-
eral thresholds ⌧ . We selected the parameter   = 0.002 for
the stateful (CUSUM) algorithm, such that the detection
metric Sk remains close to zero when there is no attack.
The usability metric is calculated for TE = 8 h, which is the
time of the experiment without attacks.

Fig. 13 illustrates the maximum impact caused by 20 dif-
ferent undetected attacks, each of them averaging 40 min-
utes. Even though the attacks remained undetected, the
impact using stateless detection is such that a large amount
of water can be spilled. Only for very small thresholds is it
possible to avoid overﬂow, but it causes a large number of
false alarms. On the other hand, stateful detection limits

Figure 11: Comparison of LDS and nonlinear mod-
els to limit attack impact using our metric. Higher
order nonlinear models perform better.

Fig. 11 illustrates the application of our proposed metric
over 10 di↵erent undetected greedy attacks, each averaging
4 minutes, to evaluate the three system models used for
detection. Given enough time, it is not possible to restrict a

the nonlinear model of order 100 performs better than the
nonlinear model of order 50 and the LDS of order 20, limiting

deviation of pH below 5. Nevertheless, for all E[Tf a](min),
the impact of the attack per minute  pH/min. It would take

over 5 minutes for the attacker to deviate the pH below 5
without being detected using a nonlinear model of order 100,
whereas it would take less than 3 minutes with the nonlinear
of order 50 and the LDS of order 20.

5.1.1 Attacking the Water Level
Now we turn to another stage in our testbed. The goal of
the attacker this time is to deviate the water level in a tank
as much as possible until the tank overﬂows.

While in the pH example we had to use system identiﬁ-
cation to learn LDS and nonlinear models, the evolution of
the water level in a tank is a well-known LDS system that
can be derived from ﬁrst principles. In particular, we use a
mass balance equation that relates the change in the water
level h with respect to the inlet Qin and outlet Qout volume
dt = Qin   Qout, where Area is the
of water, given by Area dh
cross-sectional area of the base of the tank. Note that in
this process the control actions for the valve and pump are
On/O↵. Hence, Qin or Qout remain constant if they are
open, and zero otherwise. Using a time-discretization of 1 s,
we obtain an LDS model of the form

hk+1 = hk + Qin

k   Qout
Area

k

.

1

Note that while this equation might look like an AR model,
it is in fact an LDS model because the input Qin
k   Qout
changes over time, depending on the control actions of the
In particular
PLC (open/close inlet or start/stop pump).

k

k , Qout

k ]T , B =

Area], A = 1, and C = 1.

it is an LDS model with xk = hk, uk =[Qin
[

Area ,   1
Recall that the goal of the attacker is to deviate the water
level in a tank as much as possible until the tank overﬂows.
In particular, the attacker increases the water level sensor
signal at a lower rate than the real level of water (Fig. 12)
with the goal of overﬂowing the tank. A successful attack
occurs if the PLC receives from the sensor a High water-level
message (the point when the PLC sends a command to close
the inlet), and at that point, the deviation ( ) between the
real level of water and the“fake”level (which just reached the
High warning) is   ' Overﬂow   High. Fig. 12 shows three

E[Tfa] (min)0.511.522.533.5pH / minNonlinear Model order-100Nonlinear Model order-50LDS order-200.511.521099the impact of the adversary. Note that to start spilling wa-
ter (i.e.,   > 0.3 m) a large threshold is required. Clearly,

selecting a threshold such that E[Tf a] = 170 min can avoid

the spilling of water with a considerable tolerable number of
false alarms.

In addition to attacking sensor values, we would like to
analyze undetected actuation attacks. To launch attacks on
the actuators (pumps) of this testbed, we would need to turn
them On and O↵ in rapid succession in order try to main-
tain the residuals of the system low enough to avoid being
detected. We cannot do this on real equipment because the
pumps would get damaged. Therefore, we will analyze unde-
tected actuator attacks with simulations (where equipment
cannot be damaged) in § 5.3.
5.2 Large-Scale Operational Systems (Modbus

packets)

We were allowed to place a network sni↵er on a real-world
operational large-scale water facility in the U.S. We collected
more than 200GB of network packet captures of a system
using the Modbus/TCP [63] industrial protocol. Our goal
is to extract the sensor and control commands from this
trace and evaluate and compare alternatives presented in
the survey.

The network has more than 100 controllers, some of them
with more than a thousand registers. In particular, 1) 95%
of transmissions are Modbus packets and the rest 5% corre-
sponds to general Internet protocols; 2) the trace captured
108 Modbus devices, of which one acts as central master,
one as external network gateway, and 106 are slave PLCs;
3) of the commands sent from the master to the PLCs, 74%
are Read/Write Multiple Registers (0x17) commands, 20%
are Read Coils (0x01) commands, and 6% are Read Discrete
Inputs (0x02) commands; and 4) 78% of PLCs count with
200 to 600 registers, 15% between 600 to 1000, and 7% with
more than 1000.

We replay the tra c traces in packet capture (pcap) for-
mat and use Bro [51] to track the memory map of holding
(read/write) registers from PLCs. We then use Pandas [68],
a Python Data Analysis Library, to parse the log generated
by Bro and to extract per PLC the time series correspond-
ing to each of the registers. Each time series corresponds to
a signal (yk) in our experiments. We classify the signals as
91.5% constant, 5.3% discrete and 3.2% continuous based
on the data characterization approach proposed to analyze
Modbus traces [21] and uses AR models (as in Eq. (1)). We
follow that approach by modeling the continuous time-series
in our dataset with AR models. The order of the AR model
is selected using the Best Fit criteria from the Matlab sys-
tem identiﬁcation toolbox [39], which uses unexplained out-
put variance, i.e., the portion of the output not explained
by the AR model for various orders [41].

Using the AR model, our ﬁrst experiment centers on de-
ciding which statistical detection test is better, a stateless
test or the stateful CUSUM change detection test. Fig. 14
shows the comparison of stateless vs. stateful tests with our
proposed metrics (where the duration of an undetected at-
tack is 10 minutes). As expected, once the CUSUM statis-
tic reaches the threshold Sk = ⌧ , the attack no longer has
enough room to continue deviating the signal without be-
ing detected, and larger thresholds ⌧ do not make a di↵er-
ence once the attacker reaches the threshold, whereas for
the stateless test, the attacker has the ability to change the
measurement by ⌧ units at every time step.

Having shown that a CUSUM (stateful) test reduces the

impact of a stealthy attack when compared to the stateless
test we now show how to improve the AR physical model
previously used by Hadziosmanovic et al. [21]. In particular,
we notice that Hadziosmanovic et al. use an AR model per
signal; this misses the opportunity of creating models of how
multiple signals are correlated, creating correlated physical
models will limit the impact of undetected attacks.

c
e
s
 
/

600

500

400

300

200

100

0

200

Stateless
Stateful

800

1000

400

600

E[T fa] (sec)

Figure 14: Stateful performs better than stateless
detection: The attacker can send larger undetected
false measurements for the same expected time to
false alarms.

Correlated Signals

0

1

2

3

4

Time (sec)

5

s8
s16
s19

9
×104
s16
s19

6

7

8

140

120

100

s
t

80

60

40

112

n
e
m
e
r
u
s
a
e
106
M
104

108

110

102

100

5.2

5.4

5.6

6

5.8

Time (sec)

6.2

6.4

6.6

6.8
×104

Figure 15: Three example signals with signiﬁcant
correlations. Signal S16 is more correlated with S19
than it is with S8.

Spatial and Temporal Correlation.
In an ideal situ-
ation the water utility operators could help us identify all
control loops and spatial correlations of all variables (the
water pump that controls the level of water in a tank etc.);
however, this process becomes di cult to perform in a large-
scale system with thousands of control and sensor signals
exchanged every second; therefore we now attempt to ﬁnd
correlations empirically from our data. We correlate sig-
nals by computing the correlation coe cients of di↵erent
signals s1, s2, ⇧, sN . The correlation coe cient is a nor-
malized variant of the mathematical covariance function:

the covariance between si and sj and correlation ranges

corr(si, sj) =
’cov(si,si)cov(sj ,sj) where cov(si, sj) denotes
cov(si,sj)
between  1 & corr(si, sj) & 1. We then calculate the p-

value of the test to measure the signiﬁcance of the corre-
lation between signals. The p-value is the probability of
having a correlation as large (or as negative) as the ob-
served value when the true correlation is zero (i.e., testing
the null hypothesis of no correlation, so lower values of p
indicate higher evidence of correlation). We were able to
ﬁnd 8,620 correlations to be highly signiﬁcant with p = 0.

1100niﬁcant correlated pairs. We narrow down our attention to

signals we found. Signals s16 and s19 are highly correlated

Because corr(si, sj) = corr(sj, si) there are 4,310 unique sig-
corr(si, sj) > .96. Fig. 15 illustrates three of the correlated
with corr(s16, s19) = .9924 while s8 and s19 are correlated
but with a lower correlation coe cient of corr(s8, s19) =
correlated signal pairs we found with corr(S8, S17) = .9996.

.9657. For our study we selected to use signal s8 and its
most correlated signal s17 which are among the top most

output signal is not under complete control of the attacker:
consumers can also a↵ect the frequency of the system (by
increasing or decreasing electricity consumption), and there-
fore they can cause an alarm to be generated if the attacker
is not conservative. We assume the worst possible case of
an omniscient adversary that knows how much consumption
will happen at the next time-step (this is a conservative ap-
proach to evaluate the security of our system, in practice
we expect the anomaly detection system to perform better
because no attacker can predict the future).

Figure 16: Using the deﬁned metrics, we show how
our new correlated AR models perform better (with
stateless or stateful tests) than the AR models of
independent signals.

Our experiments show that an AR model trained with cor-
related signals (see Fig. 16) is more e↵ective in limiting the
maximum deviation the attacker can achieve (assuming the
attacker only compromises one of the signals). For that rea-
son, we encourage future work to use correlated AR models
rather than AR models of single signals.
5.3 Simulations of the Physical World

With simulations we can launch actuator attacks without
the safety risk of damaging physical equipment. In partic-
ular, in this section we launch actuation attacks and show
how the control algorithm used can signiﬁcantly limit the
impact of stealthy attackers. In particular we show that the
Integrative part of a Proportional Integral Derivative (PID)
control algorithm (or a PI or I control algorithm) can correct
the deviation injected by the malicious actuator, and force
the system to return to the correct operating state.

We use simulations of primary frequency control in the
power grid as this is the scenario used by the Aurora at-
tack [69]. Our goal is to maintain the frequency of the power
grid as close as possible to 60Hz, subject to perturbations—
i.e., changes in the Mega Watt (MW) demand by consumers—
and attacks.

We assume that the attacker takes control of the actua-
tors. When we consider attacks on a control signal, we need
to be careful to specify whether or not the anomaly detection
system can observe the false control signal. In this section,
we assume the worst case: our anomaly detection algorithm
cannot see the manipulated signal and indirectly observes
the attack e↵ects from sensors (e.g., vk is controlled by the
attacker, while the detection algorithm observes the valid uk
control signal, see Fig. 1).

Attacking a sensor is easier for our stealthy adversary be-
cause she knows the exact false sensor value ˆy that will al-
low her to remain undetected while causing maximum dam-
age. On the other hand, by attacking the actuator the at-
tacker needs to ﬁnd the input uk that deviates the frequency
enough, but still remains undetected. This is harder be-
cause even if the attacker has a model of the system, the

Figure 17: These ﬁgures show two things: (1) the
stateful (CUSUM) test performs better than state-
less tests when using AR (left) or LDS (right) mod-
els, and (2) LDS models perform an order of mag-
nitude better than AR models (right vs left). Only
for really small values of ⌧ <   (0.04 minutes on av-
erage between false alarms), will the stateless test
performs better than the stateful test.

We now evaluate all possible combinations of the pop-
ular physical models and detection statistics illustrated in
Table 1. In particular we want to test AR models vs. LDS
models estimated via system identiﬁcation (SLS models do
not make sense here because our system is dynamic) and
stateless detection tests vs. stateful detection tests.

We launch an undetected actuator attack after 50 seconds
using stateless and stateful detection tests for both: AR and
LDS physical models. Our experiments show that LDS mod-
els outperform AR models, and that stateful models (again)
outperform stateless models, as illustrated in Fig 17. These
wide variations in frequency would not be tolerated in a real
system, but we let the simulations continue for large fre-
quency deviations to illustrate the order of magnitude ability
from LDS models to limit the impact of stealthy attackers
when compared to AR models.

Having settled for LDS physical models with CUSUM as
the optimal combination of physical models with detection
tests, we now evaluate the performance of di↵erent control
algorithms, a property that has rarely been explored in our
survey of related work. In particular, we show how Integra-
tive control is able to correct undetected actuation attacks.
In particular we compare one of the most popular control
algorithms: P control, and then we compare it to PI control.
If the system operator has a P control of the form uk = Kyk,
the attacker can a↵ect the system signiﬁcantly, as illustrated
in Fig. 18. However, if the system operator uses a PI control,
the e↵ects of the attacker are limited: The actuator attack
will tend to deviate the frequency signal, but this deviation
will cause the controller to generate a cumulative compensa-
tion (due to the integral term) and because the LDS model
knows the e↵ect of this cumulative compensation, it is going
to expect the corresponding change in the sensor measure-

02004006008001000E[Tfa] (sec)0200400600800/ secStateless02004006008001000E[Tfa] (sec)90100110120130140150160/ secStateful02460.10.20.30.40.50.60.7LDS modelE[Tfa] (min)Maximum ∆f (Hz)00246101214161820AR modelE[Tfa] (min)Maximum ∆f (Hz)800.0500.20.4Stateless testStateful testStateless testStateful test110166

64.8

63.6

62.4

61.2

)
z
H

(
 
y
c
n
e
u
q
e
r
F

Actuator attack I/O with P control

0.15

)

W
M

(
 

a
U

 
,
 

U

0.1

0.05

0

Real freq.
Estimated freq.

Real control
Compromised control

60

30

40

50
60
Time (sec)

70

80

-0.05

30

40

50
60
Time (sec)

70

80

Figure 18: Left: The real (and trusted) frequency
signal is increased to a level higher than the one ex-
pected (red) by our model of physical system given
the control commands. Right: If the defender uses a
P control algorithm, the attacker is able to maintain
a large deviation of the frequency from its desired
60Hz set point.

Actuator attack with I/O estimation and PI control

64
63
62
61
60
-59
58

)
z
H

(
 
y
c
n
e
u
q
e
r
F

Real freq.
Estimated freq.

)

W
M

(
 
a
U
U

,

0.1
0.05
0
-0.05
-0.1
-0.15
-0.2

30

Real control
Compromised control

40

50
60
Time (sec)

70

80

30

40

50
60
Time (sec)

70

80

Figure 19: Same setup as in Fig. 18, but this time
the defender uses a PI control algorithm: this results
in the controller being able to drive the system back
to the desired 60Hz operation point.

ment. As a consequence, to maintain the distance between
the estimated and the real frequency below the threshold,
the attack would have to decrease its action. At the end,
the only way to maintain the undetected attack is when the
attack is non-existent ua

k = 0, as shown in Fig. 19.

c
e
s
/
F
"

 

4.5
4
3.5

3
2.5
2

1.5
1
0.5

0 
0

 Attack with P control

Attack with PI control

4.5

4

Max. deviation with sensor attack
Final deviation with sensor attack
Max. deviation with actuator attack
Final deviation with actuator attack

3.5

3
2.5

0.04

0.02
0

5

2

4

E[Tfa] (min)

6

2
1.5
1
0.5

0 
0

0.4
0.2
0

5

2

4

E[Tfa] (min)

6

Figure 20: Di↵erences between attacking sensors
and actuators, and e↵ects when the controller runs
a P control algorithm vs. a PI control algorithm.

In all our previous examples with attacked sensors (except
for the pH case), the worst possible deviation was achieved
at the end of the attack, but for actuation attacks (and PI
control), we can see that the controller is compensating the
attack in order to correct the observed frequency deviation,
and thus the ﬁnal deviation will be zero: that is, the asymp-
totic deviation is zero, while the transient impact of the
attacker can be high. Fig. 20 illustrates the di↵erence be-
tween measuring the maximum ﬁnal deviation of the state

of the system achieved by the attacker, and the maximum
temporary deviation of the state of the system achieved by
the attacker.

As we can see, the control algorithm plays a fundamental
role in how e↵ective an actuation attack can be. An at-
tacker that can manipulate the actuators at will can cause
a larger frequency error but for a short time when we use
PI control; however, if we use P control, the attacker can
launch more powerful attacks causing long-term e↵ects. On
the other hand, attacks on sensors have the same long-term
negative e↵ects independent of the type of control we use
(P or PI). Depending on the type of system, short-term ef-
fects may be more harmful than long-term errors.
In our
power plant example, a sudden frequency deviation larger
than 0.5 Hz can cause irreparable damage on the generators
and equipment in transmission lines (and will trigger pro-
tection mechanisms disconnecting parts of the grid). Small
long-term deviations may cause cascading e↵ects that can
propagate and damage the whole grid.

While it seems that the best option to protect against
actuator attacks is to deploy PI controls in all generators,
several PI controllers operating in parallel in the grid can
lead to other stability problems. Therefore often only the
central Automatic Generation Control (AGC) implements a
PI controller although distributed PI control schemes have
been proposed recently [3].

Recall that we assumed the actuation attack was launched
by an omniscient attacker that knows even the speciﬁc load
the system is going to be subjected (i.e., it knows exactly
how much will consumers demand electricity at every time-
step, something not even the controller knows). For many
practical applications, it will be impossible for the attacker
to predict exactly the consequence of its actuation attack
due to model uncertainties (consumer behavior) and random
perturbations. As such, the attacker has a non-negligible
risk of being detected when launching actuation attacks when
compared to the 100% certainty the attacker has of not be-
ing detected when launching sensor attacks.
In practice,
we expect that an attacker that would like to remain unde-
tected using actuation attacks will behave conservatively to
accommodate for the uncertainties of the model, and thus
we expect that the maximum transient deviation from actu-
ation attacks will be lower.

6. CONCLUSIONS
6.1 Findings

We introduced theoretical and practical contributions to
the growing literature of physics-based attack detection in
control systems. Our literature review from di↵erent do-
mains of expertise uniﬁes disparate terminology, and nota-
tion. We hope our e↵orts can help other researchers reﬁne
and improve a common language to talk about physics-based
attack detection across computer security, control theory,
and power system venues.

In particular, in our survey we identiﬁed a lack of uniﬁed
metrics and adversary models. We explained in this paper
the limitations of previous metrics and adversary models,
and proposed a novel stealthy and adaptive adversary model,
together with its derived intrusion detection metric, that can
be used to study the e↵ectiveness of physics-based attack-
detection algorithms in a systematic way.

We validated our approaches in multiple setups, includ-
ing: a room-size water treatment testbed, a real large-scale
operational system managing more than 100 PLCs, and sim-

1102ulations of primary frequency control in the power grid. We
showed in Table 2 how each of these validation setups has
advantages and disadvantages when evaluating the x-axis
and y-axis of our proposed metric.

One result we obtained across our testbed, real opera-
tional systems, and simulations,
is the fact that stateful
tests perform better than stateless tests. This is in stark
contrast to the popularity of stateless detection statistics
as summarized in Table 1. We hope our paper motivates
more implementations of stateful instead of stateless tests
in future work.

We also show that for a stealthy actuator attack, PI con-
trols play an important role in limiting the impact of this
attack. In particular we show that the Integrative part of
the controller corrects the system deviation and forces the
attacker to have an e↵ective negligible impact asymptoti-
cally.

Finally, we also provided the following novel observations:
(1) ﬁnding spatio-temporal correlations of Modbus signals
has not been proposed before, and we showed that these
models are better than models of single signals, (2) while
input/output models like LDS are popular in control the-
ory, they are not frequently used in papers published in se-
curity conferences, and we should start using them because
they perform better than the alternatives, unless we deal
with a highly-nonlinear model, in which case the only way
to limit the impact of stealthy attacks is to estimate non-
linear physical models of the system, and (3) we show why
launching undetected attacks in actuators is more di cult
than in sensors.
6.2 Discussion and Future Work

While physics-based attack detection can improve the se-
curity of control systems, there are some limitations. For ex-
ample, in all our experiments the attacks a↵ected the resid-
uals and anomaly detection statistics while keeping them
below the thresholds; however, there are special cases where
depending on the power of the attacker or the characteris-
tics of the plant, the residuals can remain zero (ignoring the
noise) while the attacker can drive the system to an arbi-
trary state. For example, if the attacker has control of all
sensors and actuators, then it can falsify the sensor readings
so that our detector believes the sensors are reporting the
expected state given the control signal, while in the mean-
time, the actuators can control the system to an arbitrary
unsafe condition.

Similarly, some properties of the physical systems can
also limit us from detecting attacks. For example, systems
vulnerable to zero-dynamics attacks [61], unbounded sys-
tems [62], and highly non-linear or chaotic systems [48].

Finally, one of the biggest challenges for future work is
the problem of how to respond to alerts. While in some
control systems simply reporting the alert to operators can
be considered enough, we need to consider automated re-
sponse mechanisms in order to guarantee the safety of the
system. Similar ideas in our metric can be extended to
this case, where instead of measuring the false alarms, we
measure the impact of a false response. For example, our
previous work [10] considered switching a control system to
open-loop control whenever an attack in the sensors was de-
tected (meaning that the control algorithm will ignore sensor
measurements and will attempt to estimate the state of the
system based only on the expected consequences of its con-
trol commands). As a result, instead of measuring the false
alarm rate, we focused on making sure that a reconﬁguration
triggered by a false alarm would never drive the system to

an unsafe state. Therefore maintaining safety under both,
attacks and false alarms, will need to take priority in the
study of any automatic response to alerts.

Acknowledgments
The work at UT Dallas was supported by NIST under award
70NANB14H236 from the U.S. Department of Commerce
and by NSF CNS-1553683. The work of Justin Ruths at
SUTD was supported by grant NRF2014NCR-NCR001-40
from NRF Singapore. H. Sandberg was supported in part
by the Swedish Research Council (grant 2013-5523) and the
Swedish Civil Contingencies Agency through the CERCES
project. We thank the iTrust center at SUTD for enabling
the experiments on the SWaT testbed.

Disclaimer
Certain commercial equipment, instruments, or materials
are identiﬁed in this paper in order to specify the experimen-
tal procedure adequately. Such identiﬁcation is not intended
to imply recommendation or endorsement by the National
Institute of Standards and Technology, nor is it intended to
imply that the materials or equipment identiﬁed are neces-
sarily the best available for the purpose.

7. REFERENCES
[1] S. Amin, X. Litrico, S. Sastry, and A. Bayen. Cyber
security of water SCADA systems; Part I: Analysis
and experimentation of stealthy deception attacks.
IEEE Transactions on Control Systems Technology,
21(5):1963–1970, 2013.

[2] S. Amin, X. Litrico, S. Sastry, and A. Bayen. Cyber

security of water SCADA systems; Part II: Attack
detection using enhanced hydrodynamic models. IEEE
Transactions on Control Systems Technology,
21(5):1679–1693, 2013.

[3] M. Andreasson, D. V. Dimarogonas, H. Sandberg, and

K. H. Johansson. Distributed pi-control with
applications to power systems frequency control. In
Proceedings of American Control Conference (ACC),
pages 3183–3188. IEEE, 2014.

[4] K. J. ˚Astr¨om and P. Eykho↵. System identiﬁcation—a

survey. Automatica, 7(2):123–162, 1971.

[5] S. Axelsson. The base-rate fallacy and the di culty of
intrusion detection. ACM Transactions on Information
and System Security (TISSEC), 3(3):186–205, 2000.
[6] C.-z. Bai and V. Gupta. On Kalman ﬁltering in the

presence of a compromised sensor : Fundamental
performance bounds. In Proceedings of American
Control Conference, pages 3029–3034, 2014.

[7] C.-z. Bai, F. Pasqualetti, and V. Gupta. Security in
stochastic control systems : Fundamental limitations
and performance bounds. In Proceedings of American
Control Conference, 2015.

[8] R. B. Bobba, K. M. Rogers, Q. Wang, H. Khurana,

K. Nahrstedt, and T. J. Overbye. Detecting false data
injection attacks on DC state estimation. In
Proceedings of Workshop on Secure Control Systems,
volume 2010, 2010.

[9] A. Carcano, A. Coletta, M. Guglielmi, M. Masera,

I. N. Fovino, and A. Trombetta. A multidimensional
critical state analysis for detecting intrusions in
SCADA systems. IEEE Transactions on Industrial
Informatics, 7(2):179–186, 2011.

1103[10] A. A. Cardenas, S. Amin, Z.-S. Lin, Y.-L. Huang,

C.-Y. Huang, and S. Sastry. Attacks against process
control systems: risk assessment, detection, and
response. In Proceedings of the ACM symposium on
information, computer and communications security,
pages 355–366, 2011.

[11] A. A. C´ardenas, J. S. Baras, and K. Seamon. A

framework for the evaluation of intrusion detection
systems. In Proceedings of Symposium on Security and
Privacy, pages 77–91. IEEE, 2006.

[12] S. Cui, Z. Han, S. Kar, T. T. Kim, H. V. Poor, and

A. Tajer. Coordinated data-injection attack and
detection in the smart grid: A detailed look at
enriching detection solutions. Signal Processing
Magazine, IEEE, 29(5):106–115, 2012.

[13] G. D´an and H. Sandberg. Stealth attacks and

protection schemes for state estimators in power
systems. In Proceedings of Smart Grid
Commnunications Conference (SmartGridComm),
October 2010.

[14] K. R. Davis, K. L. Morrow, R. Bobba, and E. Heine.

Power ﬂow cyber attacks and perturbation-based
defense. In Proceedings of Conference on Smart Grid
Communications (SmartGridComm), pages 342–347.
IEEE, 2012.

[15] V. L. Do, L. Fillatre, and I. Nikiforov. A statistical

method for detecting cyber/physical attacks on
SCADA systems. In Proceedings of Control
Applications (CCA), pages 364–369. IEEE, 2014.
[16] E. Eyisi and X. Koutsoukos. Energy-based attack

detection in networked control systems. In Proceedings
of the Conference on High Conﬁdence Networked
Systems (HiCoNs), pages 115–124, New York, NY,
USA, 2014. ACM.

[17] N. Falliere, L. O. Murchu, and E. Chien. W32. stuxnet

dossier. White paper, Symantec Corp., Security
Response, 2011.

[18] D. Formby, P. Srinivasan, A. Leonard, J. Rogers, and

R. Beyah. Who’s in control of your control system?
Device ﬁngerprinting for cyber-physical systems. In
Network and Distributed System Security Symposium
(NDSS), Feb, 2016.

[19] R. M. Gerdes, C. Winstead, and K. Heaslip. CPS: an

e ciency-motivated attack against autonomous
vehicular transportation. In Proceedings of the Annual
Computer Security Applications Conference (ACSAC),
pages 99–108. ACM, 2013.

[20] A. Giani, E. Bitar, M. Garcia, M. McQueen,

P. Khargonekar, and K. Poolla. Smart grid data
integrity attacks: characterizations and
countermeasures ⇡. InProceedings of Smart Grid
Communications Conference (SmartGridComm),
pages 232–237. IEEE, 2011.

[21] D. Hadˇziosmanovi´c, R. Sommer, E. Zambon, and P. H.
Hartel. Through the eye of the PLC: semantic security
monitoring for industrial processes. In Proceedings of
the Annual Computer Security Applications
Conference (ACSAC), pages 126–135. ACM, 2014.
[22] X. Hei, X. Du, S. Lin, and I. Lee. PIPAC: patient

infusion pattern based access control scheme for
wireless insulin pump system. In Proceedings of
INFOCOM, pages 3030–3038. IEEE, 2013.

[23] F. Hou, Z. Pang, Y. Zhou, and D. Sun. False data

injection attacks for a class of output tracking control

systems. In Proceedings of Chinese Control and
Decision Conference, pages 3319–3323, 2015.

[24] T. Kailath and H. V. Poor. Detection of stochastic

processes. IEEE Transactions on Information Theory,
44(6):2230–2231, 1998.

[25] A. J. Kerns, D. P. Shepard, J. A. Bhatti, and T. E.
Humphreys. Unmanned aircraft capture and control
via gps spooﬁng. Journal of Field Robotics,
31(4):617–636, 2014.

[26] T. T. Kim and H. V. Poor. Strategic protection

against data injection attacks on power grids. IEEE
Transactions on Smart Grid, 2(2):326–333, 2011.

[27] I. Kiss, B. Genge, and P. Haller. A clustering-based
approach to detect cyber attacks in process control
systems. In Proceedings of Conference on Industrial
Informatics (INDIN), pages 142–148. IEEE, 2015.

[28] O. Kosut, L. Jia, R. Thomas, and L. Tong. Malicious

data attacks on smart grid state estimation: Attack
strategies and countermeasures. In Proceedings of
Smart Grid Commnunications Conference
(SmartGridComm), October 2010.

[29] G. Koutsandria, V. Muthukumar, M. Parvania,

S. Peisert, C. McParland, and A. Scaglione. A hybrid
network IDS for protective digital relays in the power
transmission grid. In Proceedings of Smart Grid
Communications (SmartGridComm), 2014.

[30] M. Krotoﬁl, J. Larsen, and D. Gollmann. The process

matters: Ensuring data veracity in cyber-physical
systems. In Proceedings of Symposium on Information,
Computer and Communications Security (ASIACCS),
pages 133–144. ACM, 2015.

[31] C. Kwon, W. Liu, and I. Hwang. Security analysis for

cyber-physical systems against stealthy deception
attacks. In Proceedings of American Control
Conference, pages 3344–3349, 2013.

[32] R. Langner. Stuxnet: Dissecting a cyberwarfare

weapon. Security & Privacy, IEEE, 9(3):49–51, 2011.
[33] J. Liang, O. Kosut, and L. Sankar. Cyber attacks on

ac state estimation: Unobservability and physical
consequences. In Proceedings of PES General Meeting,
pages 1–5, July 2014.

[34] H. Lin, A. Slagell, Z. Kalbarczyk, P. W. Sauer, and

R. K. Iyer. Semantic security analysis of SCADA
networks to detect malicious control commands in
power grids. In Proceedings of the workshop on Smart
energy grid security, pages 29–34. ACM, 2013.

[35] Y. Liu, P. Ning, and M. K. Reiter. False data injection

attacks against state estimation in electric power
grids. In Proceedings of ACM conference on Computer
and communications security (CCS), pages 21–32.
ACM, 2009.

[36] Y. Liu, P. Ning, and M. K. Reiter. False data injection

attacks against state estimation in electric power
grids. ACM Transactions on Information and System
Security (TISSEC), 14(1):13, 2011.

[37] L. Ljung. The Control Handbook, chapter System
Identiﬁcation, pages 1033–1054. CRC Press, 1996.

[38] L. Ljung. System Identiﬁcation: Theory for the User.
Prentice Hall PTR, Upper Saddle River, NJ, USA, 2
edition, 1999.

[39] L. Ljung. System Identiﬁcation Toolbox for Use with

MATLAB. The MathWorks, Inc., 2007.

[40] D. Mashima and A. A. C´ardenas. Evaluating

electricity theft detectors in smart grid networks. In

1104Research in Attacks, Intrusions, and Defenses, pages
210–229. Springer, 2012.

[41] I. MathWorks. Identifying input-output polynomial

models. www.mathworks.com/help/ident/ug/
identifying-input-output-polynomial-models.html,
October 2014.

[42] S. McLaughlin. CPS: Stateful policy enforcement for

control system device usage. In Proceedings of the
Annual Computer Security Applications Conference
(ACSAC), pages 109–118, New York, NY, USA, 2013.
ACM.

[43] F. Miao, Q. Zhu, M. Pajic, and G. J. Pappas. Coding

sensor outputs for injection attacks detection. In
Proceedings of Conference on Decision and Control,
pages 5776–5781, 2014.

[44] Y. Mo and B. Sinopoli. Secure control against replay

attacks. In Proceedings of Allerton Conference on
Communication, Control, and Computing (Allerton),
pages 911–918. IEEE, 2009.

[45] Y. Mo, S. Weerakkody, and B. Sinopoli. Physical

authentication of control systems: designing
watermarked control inputs to detect counterfeit
sensor outputs. IEEE Control Systems, 35(1):93–109,
2015.

[46] Y. L. Mo, R. Chabukswar, and B. Sinopoli. Detecting

integrity attacks on SCADA systems. IEEE
Transactions on Control Systems Technology,
22(4):1396–1407, 2014.

[47] K. L. Morrow, E. Heine, K. M. Rogers, R. B. Bobba,

and T. J. Overbye. Topology perturbation for
detecting malicious data injection. In Proceedings of
Hawaii International Conference on System Science
(HICSS), pages 2104–2113. IEEE, 2012.

[48] E. Ott, C. Grebogi, and J. A. Yorke. Controlling
chaos. Physical review letters, 64(11):1196, 1990.
[49] M. Parvania, G. Koutsandria, V. Muthukumary,

S. Peisert, C. McParland, and A. Scaglione. Hybrid
control network intrusion detection systems for
automated power distribution systems. In Proceedings
of Conference on Dependable Systems and Networks
(DSN), pages 774–779, June 2014.

[50] F. Pasqualetti, F. Dorﬂer, and F. Bullo. Attack

detection and identiﬁcation in cyber-physical systems.
Automatic Control, IEEE Transactions on,
58(11):2715–2729, Nov 2013.

[51] V. Paxson. Bro: a system for detecting network

intruders in real-time. Computer networks,
31(23):2435–2463, 1999.

[52] S. Postalcioglu and Y. Becerikli. Wavelet networks for

nonlinear system modeling. Neural Computing and
Applications, 16(4-5):433–441, 2007.

[53] I. Sajjad, D. D. Dunn, R. Sharma, and R. Gerdes.
Attack mitigation in adversarial platooning using
detection-based sliding mode control. In Proceedings of
the ACM Workshop on Cyber-Physical
Systems-Security and/or PrivaCy (CPS-SPC), pages
43–53, New York, NY, USA, 2015. ACM.
http://doi.acm.org/10.1145/2808705.2808713.

[54] H. Sandberg, A. Teixeira, and K. H. Johansson. On

security indices for state estimators in power
networks. In Proceedings of Workshop on Secure
Control Systems, 2010.

[55] Y. Shoukry, P. Martin, Y. Yona, S. Diggavi, and

M. Srivastava. PyCRA: Physical challenge-response

authentication for active sensors under spooﬁng
attacks. In Proceedings of the ACM SIGSAC
Conference on Computer and Communications
Security (CCS), pages 1004–1015, New York, NY,
USA, 2015. ACM.

[56] R. Smith. A decoupled feedback structure for covertly

appropriating networked control systems. In
Proceedings of IFAC World Congress, volume 18,
pages 90–95, 2011.

[57] S. Sridhar and M. Govindarasu. Model-based attack

detection and mitigation for automatic generation
control. Smart Grid, IEEE Transactions on,
5(2):580–591, 2014.

[58] R. Tan, V. Badrinath Krishna, D. K. Yau, and

Z. Kalbarczyk. Impact of integrity attacks on
real-time pricing in smart grids. In Proceedings of the
SIGSAC conference on Computer & communications
security (CCS), pages 439–450. ACM, 2013.

[59] A. Teixeira, S. Amin, H. Sandberg, K. H. Johansson,

and S. S. Sastry. Cyber security analysis of state
estimators in electric power systems. In Proceedings of
Conference on Decision and Control (CDC), pages
5991–5998. IEEE, 2010.

[60] A. Teixeira, D. P´erez, H. Sandberg, and K. H.

Johansson. Attack models and scenarios for networked
control systems. In Proceedings of the conference on
High Conﬁdence Networked Systems (HiCoNs), pages
55–64. ACM, 2012.

[61] A. Teixeira, I. Shames, H. Sandberg, and K. H.
Johansson. Revealing stealthy attacks in control
systems. In Proceedings of Allerton Conference on
Communication, Control, and Computing (Allerton),
pages 1806–1813. IEEE, 2012.

[62] A. Teixeira, I. Shames, H. Sandberg, and K. H.

Johansson. A secure control framework for
resource-limited adversaries. Automatica, 51:135–148,
2015.

[63] The Modbus Organization. Modbus application

protocol speciﬁcation, 2012. Version 1.1v3.

[64] D. Urbina, J. Giraldo, N. Tippenhauer, and

A. C´ardenas. Attacking ﬁeldbus communications in
ics: Applications to the swat testbed. In Proceedings
of the Singapore Cyber-Security Conference
(SG-CRC), Singapore, volume 14, pages 75–89, 2016.

[65] J. Valente and A. A. Cardenas. Using visual

challenges to verify the integrity of security cameras.
In Proceedings of Annual Computer Security
Applications Conference (ACSAC). ACM, 2015.

[66] O. Vukovi´c and G. D´an. On the security of distributed
power system state estimation under targeted attacks.
In Proceedings of the Symposium on Applied
Computing, pages 666–672. ACM, 2013.

[67] Y. Wang, Z. Xu, J. Zhang, L. Xu, H. Wang, and

G. Gu. SRID: State relation based intrusion detection
for false data injection attacks in SCADA. In
Proceedings of European Symposium on Research in
Computer Security (ESORICS), pages 401–418.
Springer, 2014.

[68] Pandas: Python data analysis library.

http://pandas.pydata.org, November 2015.
[69] M. Zeller. Myth or reality—does the aurora

vulnerability pose a risk to my generator? In
Proceedings of Conference for Protective Relay
Engineers, pages 130–136. IEEE, 2011.

1105Maneuvering Around Clouds:

Bypassing Cloud-based Security Providers

Thomas Vissers‡, Tom Van Goethem‡, Wouter Joosen‡, Nick Nikiforakis†

‡iMinds-Distrinet, KU Leuven, 3001 Leuven, Belgium

ﬁrstname.lastname@cs.kuleuven.be

†Department of Computer Science, Stony Brook University

nick@cs.stonybrook.edu

ABSTRACT
The increase of Distributed Denial-of-Service (DDoS) at-
tacks in volume, frequency, and complexity, combined with
the constant required alertness for mitigating web applica-
tion threats, has caused many website owners to turn to
Cloud-based Security Providers (CBSPs) to protect their in-
frastructure. These solutions typically involve the rerouting
of tra c from the original website through the CBSP’s net-
work, where malicious tra c can be detected and absorbed
before it ever reaches the servers of the protected website.
The most popular Cloud-based Security Providers do not re-
quire the purchase of dedicated tra c-rerouting hardware,
but rely solely on changing the DNS settings of a domain
name to reroute a website’s tra c through their security in-
frastructure. Consequently, this rerouting mechanism can
be completely circumvented by directly attacking the web-
site’s hosting IP address. Therefore, it is crucial for the
security and availability of these websites that their real IP
address remains hidden from potential attackers.

In this paper, we discuss existing, as well as novel “origin-
exposing” attack vectors which attackers can leverage to dis-
cover the IP address of the server where a website protected
by a CBSP is hosted. To assess the impact of the discussed
origin-exposing vectors on the security of CBSP-protected
websites, we consolidate all vectors into Cloudpiercer, an
automated origin-exposing tool, which we then use to con-
duct the ﬁrst large-scale analysis of the e↵ectiveness of the
origin-exposing vectors. Our results show that the problem
is severe: 71.5% of the 17,877 CBSP-protected websites that
we tested, expose their real IP address through at least one
of the evaluated vectors. The results of our study categori-
cally demonstrate that a comprehensive adoption of CBSPs
is harder than just changing DNS records. Our ﬁndings
can steer CBSPs and site administrators towards e↵ective
countermeasures, such as proactively scanning for origin ex-
posure and using appropriate network conﬁgurations that
can greatly reduce the threat.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
c  2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813633.

Categories and Subject Descriptors
C.2.0 [Computer-communication Networks]: [Security
and protection]; K.6.5 [Security and Protection]: [Unau-
thorized access]

Keywords
Cloud-based security; DDoS attacks; Web attacks

1.

INTRODUCTION

Although Distributed Denial-of-Service (DDoS) attacks
have threatened the availability of online services for years,
attacks are rapidly increasing in volume, complexity and
frequency. Early 2014, the Network Time Protocol (NTP)
was exploited in order to conduct ampliﬁcation attacks [45]
of previously unseen magnitudes, leading to multiple record-
breaking volumetric attacks that reached up to 500 Gbps [35,
52]. Unfortunately, these powerful attacks are no longer ex-
ceptional cases. For instance, in 2014, there were four times
as many attacks that crossed the 100 Gbps barrier as com-
pared to 2013 [4]. Consequently, these massive attacks are
now regarded as “the new normal” [23], an observation fur-
ther conﬁrmed by the frequent news reports of high-proﬁle
websites and web applications that become victims of such
attacks [15].

Aside from advancing in strength and complexity, DDoS
attacks are becoming increasingly accessible to the general
public. The main cause is the rising popularity of web-
sites o↵ering DDoS attacks as a service, which enable non-
technical users to launch DDoS attacks with the click of a
button. These services, often called booters or stressers, al-
low their customers to orchestrate powerful DDoS attacks
for just a few dollars through convenient, and user-friendly
web interfaces [20].

To cope with the elevated risk and increased di culty
in fending o↵ large DDoS attacks, several companies engi-
neered highly capable, globally distributed networks that
are able to deal with with DDoS tra c and malicious web
requests. The resulting cloud-based defense infrastructure
is then shared among the companies’ customers. It is safe
to assume that not all customers will be su↵ering from a
large DDoS attack simultaneously, and thus companies can
dedicate enough bandwidth and processing power to clients
that are, at any given point, under attack.

Since several of these Cloud-based Security Providers (CB-
SPs) solely rely on changing the DNS settings of a domain
name to reroute a website’s tra c through their security in-

1530frastructure, the rerouting mechanism can be, in principle,
completely circumvented by directly attacking the website’s
hosting IP address. Therefore, it is crucial for the security
and availability of these websites that their real IP address,
referred to as the origin, remains hidden from potential at-
tackers. Past reports have claimed that the origin of CBSP
customers can potentially be acquired through various meth-
ods, such as querying historical DNS data for a domain, and
searching for subdomains that directly resolve to a server’s
real IP address [34]. Although these origin-exposing attack
vectors have been known since 2013, the global extent of this
issue has not yet been evaluated.

In this paper, we assess the magnitude of this problem on
a large scale, i.e., we evaluate the number of protected do-
mains whose CBSP-based protection can be bypassed. First,
we discuss eight existing as well as novel vectors that have
the potential to expose the underlying IP address of a CBSP-
protected web server. Next, we consolidate these vectors
into Cloudpiercer, an automated origin-exposing tool. We
deploy Cloudpiercer and conduct the ﬁrst large-scale ex-
periment where we evaluate 17,877 domains that are pro-
tected by ﬁve di↵erent CBSPs. Cloudpiercer uses a novel
veriﬁcation method to ensure that an IP address retrieved by
the vectors is indeed the real origin of a website. After this
veriﬁcation step, we ﬁnd that over 70% of protected domains
expose their real IP address and, as a consequence, can be
attacked directly, rendering the cloud-based protection ser-
vice useless. Furthermore, we elaborate on the impact and
prevalence of each exposing vector and discuss the feasibility
of remediating the problem.

The main contributions of this paper are the following:
• We provide a comprehensive overview of novel and pre-
viously known origin-exposing vectors that allow at-
tackers to bypass CBSPs.

• We report on the ﬁrst large-scale measurement of this
crucial security issue and conclude that the majority
of CBSP clients are at risk, while providing insights
into which vectors are most widespread.

• We discuss the di culties of mitigating origin expo-
sure, while suggesting several e↵ective countermeasures
that can vastly remediate the problem.

2. BACKGROUND

As Distributed Denial-of-Service (DDoS) attacks are be-
coming increasingly powerful, it becomes infeasible for web-
sites to protect their own infrastructure. Even advanced, on-
site, defense systems are rendered useless when the amount
of tra c exceeds the processing capabilities of upstream de-
vices or simply saturates the entire network connection. Fur-
thermore, with the constant evolution of web application
threats, there is also a need for increasing resources to fend
o↵ breaches. As a result, website owners turn to Cloud-
based Security Providers (CBSPs) to protect their infras-
tructure. These companies reroute tra c from the original
website through their network where malicious tra c is ﬁl-
tered before it ever reaches the network of their customer.
2.1 Modus Operandi of CBSPs

CBSPs act as reverse proxies for the web servers they are
protecting. They inspect incoming tra c for various clients
simultaneously, by routing it through their own distributed

LEGITIMATE TRAFFIC

MALICIOUS TRAFFIC

LEGITIMATE TRAFFIC

MALICIOUS TRAFFIC

WWW

Unprotected Server

CBSP

WWW

Protected Server

Figure 1: An unprotected server receives malicious tra c,
potentially breaching the web server or denying service to
the legitimate tra c (upper). Malicious tra c heading to-
wards the protected server is absorbed by the CBSP, only
allowing legitimate tra c to pass through (lower).

infrastructure. These cloud-based infrastructures, often re-
ferred to as scrubbing centers, act as highly-available tra c
ﬁlters that are capable of absorbing extremely large volu-
metric DDoS attacks. Furthermore, they often integrate
Web Application Firewalls (WAFs) to ﬁlter out malicious
web application tra c, such as application-layer DDoS at-
tempts, SQL injections and XSS attacks.

As depicted in Figure 1, all tra c towards a CBSP-protected

web server, often referred to as the origin,
is redirected
through cloud-based scrubbing centers. After inspection of
the incoming requests, only “clean tra c” is forwarded to
the web server, e↵ectively stopping attacks before they even
reach the customer’s premises.

Rerouting mechanisms
Several di↵erent strategies exist to route a web server’s traf-
ﬁc through the cloud-based infrastructure. For instance, a
website administrator can either opt for an always-on or
for an on-demand strategy. The former redirects all tra c
through the scrubbing centers on a permanent basis. The
latter only starts redirecting tra c when necessary. Usually,
this requires customer-premises equipment (CPE), that lo-
cally monitors incoming tra c. In case an attack is detected,
this device initiates the redirection mechanism.

When tra c-redirection is active, there are two mecha-
nisms to reroute tra c through the scrubbing centers. The
ﬁrst option is DNS rerouting, where an administrator changes
the DNS settings of his website’s domain name so that it re-
solves to an IP address that belongs to the CBSP. Normally,
when a visitor requests a webpage, e.g., from example.com,
his computer will ﬁrst make a request to a DNS server to
discover the corresponding IP address. Next, the visitor’s
browser can request the page from example.com’s web server
using the discovered IP address.
In the case of CBSPs,
the visitors of the protected domain will receive an IP ad-
dress of the CBSP’s scrubbing center from the DNS server.
Hence, the visitor will direct his requests to the scrubbing
center, which in turn will transparently forward the legit-
imate requests to the origin, i.e., the actual web server of
example.com.

Alternatively, a technique called BGP rerouting can be
adopted. When the entity managing the website controls
an entire /24 IP block, it can withdraw the BGP announce-

1531ments for that block from its own routers. At this point, the
CBSP can initiate BGP announcements for that IP range
from their own network. Consequently, all tra c destined
for the web server’s IP address will start ﬂowing through
the CBSP’s scrubbing centers. Since BGP rerouting is only
available to entities that manage entire IP blocks and are
able to install dedicated hardware, DNS rerouting has be-
come the cloud-based security alternative for the masses [7].
2.2 CDNs as CBSPs

At their core, Content Distribution Networks (CDNs) are
globally deployed services that increase the performance of
websites by bringing static web content closer to users. The
network usually consists out of a large set of geographically-
distributed cache servers. This allows a CDN to quickly
serve cached content from a server that is near a particular
user. This setup reduces response times, load, and band-
width of a website’s main web server.

Similar to CBSPs, a CDN intercepts requests to a web
server, which enables it to inspect incoming requests and se-
lectively decide whether to serve cached content or forward
the request to the web server for a dynamically generated
response. Therefore, tra c towards the web server has to
be constantly redirected through the CDN. To achieve this,
CDNs either opt for URL rewriting or DNS rerouting [28].
Considering that a CDN’s infrastructure is inherently ca-
pable of inspecting requests to leverage intelligent caching
techniques, they are ideally placed to provide cloud-based
security as well. Since tra c is already being redirected
through their CDN, scrubbing centers and WAFs can be
conveniently chained in the infrastructure. Moreover,
in
terms of volumetric DDoS attacks, a CDN is an ideal ﬁt for
mitigation strategies due to their geographically distributed
and highly-available network. By using Anycast [1], servers
spread across the globe can each process a small portion of
the distributed attack, e↵ectively making it feasible to ab-
sorb large amounts of malicious tra c.

As a result from this overlapping feature set, a signiﬁ-
cant share of CBSPs has emerged from CDN providers that
started o↵ering security services on top of their existing
platform. Similarly, several security-focused companies that
provided cloud-based services, have also started incorporat-
ing caching features to their infrastructure. Consequently,
the line between CDNs and CBSPs is blurred. As such,
the origin-exposing vectors that we discuss in Section 3 are
applicable to CBSPs as well as to CDNs with security ex-
tensions.

3. POTENTIAL ORIGIN EXPOSURE

While CBSPs have become really popular because of their
ability to stop real, large DDoS [38] and web application at-
tacks, there are concerns about their DNS rerouting mecha-
nisms. The concept of cloud-based security relies on keeping
the underlying web server, the so-called origin, secret and
inaccessible by direct tra c. However, in the case of DNS
rerouting, this is achieved by hiding the origin’s IP address
and relying on redirection through the use of the website’s
domain name. Consequently, as illustrated in Figure 2, the
website is only protected against tra c that uses the domain
name to initiate the connection. So, in principle, if attackers
are able to discover the real IP address of the origin, they can
target tra c to the web server directly, thereby circumvent-
ing all security mechanisms present in the CBSP’s network.

MALICIOUS TRAFFIC
 TO DOMAIN

MALICIOUS TRAFFIC
TO IP ADDRESS

CBSP

WWW

Protected Server

(origin)

Figure 2: In the case of DNS rerouting, only tra c that uses
the domain name is diverted through the CBSP’s network.
Tra c that uses the IP address of the protected server can
reach the web server directly.

We refer to this security concern as the risk of origin ex-
posure. This issue, which is speciﬁc to DNS rerouting, has
been raised several times before [31, 34], and has, in the
past, received some attention by the press, followed by sev-
eral reactions from the security companies [27,48,53]. Many
di↵erent potential vulnerabilities exist that might expose a
CBSP-protected website’s origin. We refer to these potential
vulnerabilities as origin-exposing vectors. In the remainder
of this section, we discuss eight origin-exposing vectors, of
wich four have been reported previously, as well as four novel
vectors, namely Temporary DNS exposure, SSL Certiﬁcates
and speciﬁc instances of Sensitive Files and Outbound Con-
nection Triggering. All vectors combined later form the basis
of our automated scanning tool, Cloudpiercer.
3.1

IP History

When setting up cloud-based security, website adminis-
trators are required to change the DNS settings for their
domain. From that point on, the origin’s IP address is no
longer listed in the DNS records of the domain name. As
already mentioned in earlier sections, this secrecy is crucial
for preventing origin exposure. However, if the origin is still
assigned the same IP address as before the adoption of a
CBSP, the server can be exposed through historical knowl-
edge of the domain and its corresponding IP address.

Several companies specialize in harvesting data about do-
main names by continually tracking their DNS conﬁgura-
tion. This allows them to build a vast database of historical
DNS records, mainly used for domain marketing research,
which can also be leveraged to track down an origin’s IP
address.

Accessing these databases is almost e↵ortless and some of
these services even o↵er a small number of free queries. How-
ever, these databases do not cover all existing domains as
some TLDs do not share their zone ﬁles, making it harder to
discover and monitor some domains. At the same time, do-
mains that are not indexed in these databases are certainly
not guaranteed to be safe from IP history vulnerabilities.
For instance, if an attacker has been targeting a particular
victim for a prolonged period, he could have manually gath-
ered information about the domain and its origin before it
was protected by the CBSP.

Because of the multitude of parties that could be collecting
historical information about websites and their IP addresses,
several CBSPs recommend administrators to assign a new
IP address to their web server after migrating their DNS
records to the CBSP [48].

15323.2 Subdomains

Since the CBSP acts as a reverse proxy for multiple clients
simultaneously, it relies on information available in HTTP
requests to distinguish between requests intended for di↵er-
ent clients. More speciﬁcally, by looking at the domain listed
in the HTTP Host header, the CBSP can correctly forward
incoming tra c to the intended origin. An unfortunate side-
e↵ects is that protocols that do not contain host information,
such as FTP and SSH, cannot be properly handled by the
CBSPs’ proxies and are thus, by default, broken.

There are two ways around this problem: ﬁrst, instead
of using the domain name, an administrator can directly
specify the origin’s IP address when working with non-web
protocols. This, however, lacks the ﬂexibility of a domain-
name-based solution since the IP address must be either
hardcoded in scripts and program proﬁles, or remembered
by a website’s administrator.

Alternatively, administrators can create a speciﬁc subdo-
main, such as origin.example.com, that directly resolves to
the origin’s IP address. This provides a convenient tool for
non-web protocols to bypass the CBSP and establish a direct
connection with the origin. However, since this workaround
e↵ectively creates a direct path to a website’s origin, it is
a potential backdoor that, if discovered, can be abused by
attackers. In the absence of misconﬁgured DNS servers al-
lowing unauthenticated Zone Transfers, subdomains are not
directly visible when querying the DNS records of the main
domain name. An attacker can, however, perform a dic-
tionary attack by trying to guess valid subdomains, using
dictionaries of words popularly used in subdomains.

3.3 DNS records

Once a website is protected, the DNS A record of its do-
main name points to an IP address of the CBSP instead of
directly to the origin. However, it is possible that traces of
the origin are still present in other DNS records.

For instance, MX records reference the mail servers that are
responsible for accepting email messages that are destined
for mailboxes on a given domain. When only HTTP tra c is
forwarded by the CBSP, SMTP needs to be able to establish
a direct connection with the mail server. Therefore, the MX
records should directly resolve to the mail server’s IP address
in order to keep email services operational. This can lead to
origin exposure, especially when the mail server is listening
on the same network interface as the origin’s web server.

Another potentially problematic case are TXT records, of-
ten used for mechanisms such as the Sender Policy Frame-
work (SPF) [21]. This framework aims to counter email
address spooﬁng by validating the IP address of the sender
against a list of approved IP addresses. The list of addresses
from which emails may be sent, has to be placed in an TXT
record of the domain [30]. Thus, if one wants the origin
server to be able to send out emails using the SPF mecha-
nism, they are forced to expose its IP address in the appro-
priate TXT record. Note that the solution to this problem is
not obvious; an administrator has to choose to either aban-
don the Sender Policy Framework (thereby opening himself
to email abuse), or accept that the protected web server
cannot send veriﬁed emails.

The origin exposure, unfortunately, is not limited to TXT
and MX records. Especially when a CBSP does not manage
the DNS records of its customers’ domains, exposure from

other records may be overlooked by the customer. For in-
stance, if the origin is accessible through IPv6, AAAA records
are present.
If the CBSP’s setup instructions only cover
the change of the A record of the domain, the AAAA record
might be left unchanged, e↵ectively keeping the origin ex-
posed through its IPv6 address.

3.4 Temporary exposure

Administrators might temporarily pause the cloud-based
security service, e.g., for maintenance or server migrations.
During this time, the domain might temporarily resolve to
its origin, e↵ectively leading to a brief origin exposure. Tem-
porary leaks can occur in many DNS record types, including
MX, CNAMEs, and TXT. Attackers who are closely monitoring
their victim might be able to witness a temporary exposure.
Once the origin is known, the web server remains vulnera-
ble even after the leak has disappeared, since the attacker
can keep reusing the leaked IP address. The leak will only
be remediated when the administrator decides to, yet again,
change the IP address associated with the victim website.

3.5 SSL certiﬁcates

If administrators want to enable HTTPS for their web-
site while under the protection of a CBSP, they can let the
CBSP set up a certiﬁcate for their domain. This enables
the CBSP to take care of securing the front-end connection
between their own cloud infrastructure and a visitor. Alter-
natively, the administrator can hand over the private key of
their origin’s certiﬁcate to the CBSP. In this case, the CBSP
can set up the front-end SSL connection with the website’s
own certiﬁcate. In order to secure the back-end connection
between the CBSP and the origin, the origin must present a
certiﬁcate. However, this certiﬁcate lists the domain name
as the subject, and therefore identiﬁes itself as the origin.
In other words, if an attacker is able to scan all IP addresses
and retrieve all SSL certiﬁcates, he can ﬁnd the IP addresses
of hosts with certiﬁcates that are associated with the domain
he is trying to expose. Because of recent advancements in
network scanners, performing such a massive scan has be-
come quite feasible. For example, using ZMAP [14] and an
appropriately fast network connection, allows an attacker to
conduct a scan of the entire IPv4 address space on a single
port in 45 minutes.
3.6 Sensitive ﬁles

Sensitive ﬁles located on the server form another vector
through which a server’s IP address can be exposed. For
instance, ﬁles that were created during the development or
conﬁguration phase, in order to aid the administrator, can
be used to expose a server’s origin, especially when they
show detailed information regarding the server. Further-
more, as already explained by Akamai [27], verbose error
pages and log ﬁles can also disclose the origin that is meant
to be kept secret. Usually, these types of ﬁles are meant to
be removed or given proper access restrictions once a web-
site goes into production, but presumably this is not always
done correctly.

15333.7 Origin in content

Instead of using a domain name to link to content, a web-
page is free to use the IP address of the server directly. For
example, a developer might use the IP address directly in
the HTML of a page during an early development phase of
the website. Although this is probably rather uncommon,
it does form a potential origin-exposing vector. Further-
more, the IP address might be listed in the HTML as part
of server-side software calculating web server statistics.

3.8 Outbound connections

Although a web server’s incoming connections are rerouted
through the CBSP’s infrastructure, this is not the case for
outbound connections. When a web server initiates an out-
going connection on its own accord, the CBSP is not used
as a proxy. Consequently, the origin establishes a direct
connection with an external host, e↵ectively exposing its IP
address to that particular host.

In order to exploit this phenomenon, an attacker can at-
tempt to deliberately trigger the origin to initiate outgo-
ing connections. Many possibilities exist in this regard and
these are usually very speciﬁc to the applications running
on the web server. Some examples include the possibility
to upload a ﬁle via a URL, or link back mechanisms such
as PingBack [25], which retrieve external webpages to verify
whether a claimed link to their website is real or not.

4. LARGE-SCALE ANALYSIS

To assess the magnitude of the origin-exposure problem,
we conduct a large-scale analysis in which we attempt to
uncover the origin of CBSP-protected domains. First, we
consolidate the eight origin-exposing vectors into one auto-
mated origin-exposing system called Cloudpiercer. Then,
we assemble a list of clients from ﬁve CBSP companies by
studying their DNS conﬁgurations and obtaining their adop-
tion rate across the Alexa top 1 million websites. Starting
from these client lists, we use Cloudpiercer to evaluate
17,877 long-term, CBSP-protected domains against origin
exposure. In the ﬁnal step of Cloudpiercer, all collected
candidate IP addresses are validated with a novel veriﬁcation
method to assess whether each discovered IP address is in-
deed the one of a protected website. Using Cloudpiercer,
we are not only able to measure the amount of bypassable
domains but also to gauge which origin-exposing vectors are
the most prevalent.

4.1 CBSP Providers

For our purposes, we are interested in analyzing various
always-on, DNS rerouting CBSPs. As mentioned in Sec-
tion 2.2, several CBSPs are CDNs that o↵er additional se-
curity services, and vice versa. Since it is not straightfor-
ward to externally distinguish between clients that only use
the CDN capabilities from those who are speciﬁcally paying
for a plan that includes security, we selected ﬁve well-known
providers that have a speciﬁc focus on security, i.e., at least
some form of cloud-based security is present by default in all
of these provider’s pricing plans. The selected providers are
CloudFlare, Incapsula, DOSarrest, Prolexic (PLXedge) and
Sucuri (Cloud Proxy). We gather a list of clients from each
provider, enabling us to study their necessary conﬁgurations
and their adoption by popular websites.

Security Provider

DNS Conﬁguration Domains

CloudFlare
Incapsula
DOSarrest
Prolexic
Sucuri Cloud Proxy

NS
A, CNAME
A
A
A

35,552
1,841
1,295
829
281

Table 1: Cloud-based security providers, along with the DNS
records that are adjusted by their clients, and the number
of protected domains that were found in the Alexa top 1
million.

Clients
In order to identify protected clients, we need to be aware of
the di↵erent DNS conﬁgurations that are required by each of
the CBSPs. To retrieve this information, we ﬁrst attempted
to subscribe to each company, and took note of the set up
process. If we were not able to register, e.g., due to the ab-
sence of trial or free service tiers, we searched for publicly
available instructions or retrieved representative conﬁgura-
tion settings by manually ﬁnding other existing clients.

Generally, we found two di↵erent types of DNS conﬁgu-
rations that are used to reroute a website’s tra c: chang-
ing the NS records and changing the A records. Incapsula,
DOSarrest, Sucuri and Prolexic instruct their clients to change
their domain’s A record to a speciﬁc IP address, that is under
the CBSP’s control. In some cases, the CNAME or A record of
the www subdomain is conﬁgured as well.

When the NS records of the domain have to be changed, as
it is the case with CloudFlare, the entire DNS records of the
domain name become actively managed by the CBSP’s name
servers. Consequently, all DNS records of the domain and its
subdomains fall under their direct authority. This enables
the CBSP to provide all the necessary DNS records in order
for rerouting to take place. However, the conﬁguration of
additional custom records, such as the MX records to identify
the domain’s mail server, has to be managed through the
CBSP’s own custom interface where these additional records
need to be added by the client.
Adoption
To assess the adoption of CBSPs, we analyze the top 1 mil-
lion most popular websites, according to Alexa [3]. By re-
trieving each domain’s DNS records and comparing them
with the collected CBSP conﬁgurations, we can straightfor-
wardly compile a list of the most popular CBSP-protected
domains. Table 1 lists the number of clients found for each
company, along with the type of DNS conﬁguration that was
used for identiﬁcation.

When we evaluate the adoption of cloud-based security,
we ﬁnd that 4% of the web’s most popular 1 million web-
sites are protected by one of the ﬁve companies under anal-
ysis. Moreover, cloud-based security services appear to be
a more prominent solution amongst the more popular web-
sites, since, if we restrict our search to the top 10K websites,
the CBSP adoption increases to 9%. To further quantify
the relationship between CBSP adoption and website pop-
ularity, Figure 3 shows the distribution of each company’s
client list across rankings. Four out of ﬁve companies have a
signiﬁcantly higher portion of domains in the top 100K seg-
ment, further strengthening the correlation between CBSP
adoption and website popularity. More speciﬁcally, Cloud-

1534CloudFlare

Incapsula

DOSarrest

Prolexic

Sucuri

i

s
n
a
m
o
D

 
f

o

 

n
o

i
t
c
a
r
F

20%

15%

10%

5%

0%

20%

15%

10%

5%

0%

20%

15%

10%

5%

0%

20%

15%

10%

5%

0%

20%

15%

10%

5%

0%

0

200,000

400,000

600,000

1,000,000
800,000

0

200,000

400,000

600,000

1,000,000
800,000

0

600,000
400,000
200,000
Alexa rank

1,000,000
800,000

0

200,000

400,000

600,000

1,000,000
800,000

0

200,000

400,000

600,000

1,000,000
800,000

Figure 3: The portion of domains protected by each company, across segments of the Alexa top 1 million. For example, about
15% of the domains protected by Incapsula are situated between rank 100,000 and 200,000.

Flare, Incapsula and Sucuri have visibly less clients com-
ing from the lower parts of the Alexa ranking. DOSarrest
and Prolexic do not show this kind of correlation. However,
we found that both companies have large domain parking
services as one of their customers. These parking services
are responsible for a large number of relatively unpopular
undeveloped domains that are placed under protection by
the CBSP thereby a↵ecting the ranking distribution of the
clients of these CBSPs.
Evaluated domains
For our large-scale analysis, we subjected the entire list of
clients in the Alexa top 1 million of Incapsula, DOSarrest,
Prolexic and Sucuri, as input to Cloudpiercer. Because
of the disproportional popularity of CloudFlare, we decided
to test a random sample of only half of their clients. This
sample is small enough to allow us to conduct our experi-
ments in a reasonable amount of time and large enough so
that any conclusions can be safely generalized to the pop-
ulation of CloudFlare’s clients. In addition, we limited the
experiment to domains that remained customers of a CBSP
during, at least, our 6-month monitoring period. Through
this ﬁltering process, we aim to remove negative bias, by ex-
cluding customers that were simply trying-out a CBSP and
were, perhaps, not interested enough to take all necessary
precautionary steps and eliminate origin-exposure vectors.
Overall, this process resulted in a ﬁnal list of 17,877 do-

mains, which we refer to as the evaluation set.
4.2 Origin Veriﬁcation

To determine whether a discovered IP address is the actual
origin of a CBSP-protected website, we evaluate whether we
can retrieve the website’s landing page using that IP ad-
dress. First, we ensure that the IP address is a valid candi-
date by verifying that it does not belong to an address block
owned by a CBSP. Then, our veriﬁcation starts by visiting
the website through its CBSP-protected domain name to
retrieve the URL of the landing page. For example, when
issuing a regular HTTP request to http://example.com, the
browser might be redirected to a landing page with a dif-
ferent scheme, host and path, such as https://blog.example.
com/about me.html. Next, we use PhantomJS [18], an in-
strumented browser, to initiate an HTTP request to the can-
didate IP address, incorporating the previously extracted
scheme, host and path of the landing page. If the candidate
IP address is the actual origin of the website, this HTTP re-
quest should return the same webpage as the request using

the domain name, as both requests are identical from the
web server’s perspective.

Determining, however, whether two webpages are iden-
tical is not as straightforward as executing a simple string
comparison. For instance, when loaded twice, the same page
can result in di↵erent HTML as dynamically generated con-
tent may be included in the website’s response, such as, ro-
tating articles and advertisements. In addition, timestamps
and random values embedded in a webpage can also alter
the resulting HTML. Moreover, several CBSPs inject con-
tent into the displayed page, such as, analytics scripts, which
will not be present in a direct response from the origin.

To account for this natural variability, we designed a more
intelligent and robust HTML comparison technique. Instead
of comparing strings, we examine the structure of the DOM
(Document Object Model). We parse both HTML strings
with LXML and BeautifulSoup [43] into a tree represen-
tation of the nodes in the DOM. Next, we determine the
di↵erence between the two trees by calculating the Zhang-
Shasha’s tree edit distance [55], which counts the number
of edit operations (insertions, deletions and substitutions of
nodes) to get from one tree to the other. Furthermore, we
extended the implementation [17] by incorporating normal-
ization which is necessary to meaningfully compare the mea-
sured distances between tree-pairs of di↵erent sizes. Normal-
ization is achieved by dividing the calculated edit distance
by the sum of the tree sizes. We refer to this metric as the
Normalized DOM-edit Distance (NDD).

Prior to our large-scale analysis, we measured the inter-
page and intra-page NDD distributions from a random set
of domains from the Alexa top 1 million, enabling us to cal-
culate an optimal threshold. Additionally, we evaluated the
e↵ect of a more coarse-grained tree comparison by pruning
the DOM trees to a certain maximum nesting depth. We
measured the NDD between 3,500 pairs of di↵erent web-
site’s landing pages, as well as between 3,500 pairs of the
same landing page loaded twice. Furthermore, we conducted
this test for di↵erent tree pruning levels. Afterwards, we
used this data to choose an optimal threshold that is used
to decide whether two di↵erent HTML documents are, in
fact, the same webpage. When evaluating thresholds, we
focussed primarily on limiting false positives (two di↵erent
webpages that are falsely marked as identical). At the end
of this process, we found that a threshold of 0.18, at a maxi-
mum nesting depth of 5 levels, results in zero false positives
and only 0.36% false negatives.

1535l

e
b
a
s
s
a
p
y
b
 
s
i
 

 

P
S
B
C
e
r
e
h
w
 
s
n
a
m
o
d

i

 
f

 

o
%

100%

80%

60%

40%

●

●

●

●

●

●

●

0

200,000

400,000

600,000

Alexa Rank

)

%

i

(
 
s
n
a
m
o
D

 
f
o
 
e
g
a
t
n
e
c
r
e
P

70
60
50
40
30
20
10
0

CBSP
● Incapsula
CloudFlare
Prolexic
Sucuri
DOSarrest

●

●

●

800,000

1,000,000

1
6
Minimum number of origin−exposing vulnerabilities

2

3

4

5

Figure 4: For each 100K-rank segment in the Alexa top
1 million, the percentage of domains where the CBSP is
bypassable.

Figure 5: The percentage of domains that is susceptible to
one or more origin-exposing vectors.

4.3 Ethical Considerations

To realistically assess the magnitude of the origin-exposing
problem in the wild, one cannot avoid scanning real on-
line websites and web applications. During our analysis and
the development of Cloudpiercer, we took all appropriate
steps to ensure that neither the origins, nor the CBSPs, were
negatively impacted by our measurements. In addition, we
only used publicly available webpages and data from pub-
licly available sources.

Since the evaluated domain names are a subset of the
most visited websites in the world, their infrastructure is
capable of processing an abundant amount of requests on a
daily basis. Nevertheless, we took several steps to minimize
the impact of our analysis. For instance, the number of con-
tacted PingBack endpoints was limited to three per domain,
although, often, many more were present. Furthermore, web
requests and DNS queries were adequately spaced in time in
order to minimize impact on servers. Overall, we believe
that the e↵ects of our experiment on each individual site
were minimal and we are conﬁdent that for the majority of
websites, the extra tra c generated by our requests was just
part of the expected tra c variability.
4.4 Results

All 17,877 domains in the evaluation set were subjected,
by Cloudpiercer, to each of the eight origin-exposing vec-
tors. Afterwards, Cloudpiercer used the origin veriﬁca-
tion algorithm to determine which IP addresses were the ac-
tual websites’ hosting IP addresses. These results allow us
to measure, both the origin-exposing power of each attack
vector, as well as the overall risk of the origin being exposed.
We manually inspected a sample of 250 exposed origins and
saw that there were no false-positive veriﬁcations.

Overall, we found that 71.5% of protected domains is by-
passable by combining the e↵ect of all origin-exposing vec-
tors. Table 2 lists the success-rate of each individual vec-
tor for the client domains of the di↵erent CBSPs. Subdo-
mains and IP history are clearly the major vulnerabilities,
both compromising the origin of more than 40% of domains.
Figure 4 sheds light on the di↵erences in the bypass-ratio
between highly-ranked and less popular domains. We ob-
serve that for four out of ﬁve companies, domains in the
top 100K are less susceptible to being exposed. A possible
explanation is that higher ranked websites are more secu-

rity conscious and more concerned with preventing origin
exposure. A similar phenomenon was also observed in [51],
where the top 100K-ranked websites were found to adopt
signiﬁcantly more web security mechanisms. Conversely, the
risk of being exposed through SSL certiﬁcates is up to 3.6
times higher in that same top segment, presumably due to a
higher SSL adoption-rate amongst these security conscious
websites. Except for that ﬁrst segment, there are no clear
global trends across the remaining lower ranks.

As shown in Figure 5, 42% of domains are susceptible to
exposure by more than one origin-exposing vector. More
speciﬁcally, 19% of domains need to patch at least three
origin-exposing vectors before they are safe. These num-
bers indicate that the problem is prevalent as well as multi-
faceted.

In the following paragraphs we discuss the results in more
detail by examining the speciﬁcs of each origin-exposing vec-
tor.

Subdomain exposure
Overall, the most prevalent attack vector is the existence of
origin-exposing subdomains. The feasibility of an attacker
ﬁnding origin-exposing subdomains was tested by trying a
list 5000 possible subdomains, provided by DNS Recon [36]
on each domain in the evaluation set. If an entry existed for
one or more of the tested subdomains, we veriﬁed whether
the IP address to which it resolved was the origin. Our re-
sults indicate that CloudFlare’s and Sucuri’s customers are
particularly vulnerable, with respectively 48.9% and 51.5%
of domains disclosing their real IP address through subdo-
mains.

When we take a closer look at which speciﬁc subdomains
are responsible for the exposure, we ﬁnd that the ftp sub-
domain is the most dominant problem, with 3,952 out of
17,877 domains having this “backdoor.” This result implies
a strong desire by website administrators to be able to use
an FTP client that is able to connect to the server through
a subdomain. Other subdomains that frequently reveal the
origin are often related to email services, such as mail (3,203
domains), webmail (1,662) and smtp (258). Furthermore, a
large number of exposing subdomains is related to cPanel, a
hosting control panel that provides a web interface to help
administrators conﬁgure their websites [10]. The discovered,
origin-exposing subdomains are: cpanel (1,456 domains),
webdisk (1,645) and whm (1,359). These subdomains are

1536Provider

CloudFlare

Incapsula DOSarrest Prolexic

Sucuri All Providers

IP History
Subdomains
DNS records
Temporary DNS
SSL Certiﬁcates
Sensitive ﬁles
Origin in content
PingBack (OC)
RefBack (OC)

All Combined

37.0%
48.9%
32.6%
4.1%
9.4%
6.4%
1.2%
8.2%
0.5%

72.5%

36.4%
31.7%
11.2%
0.8%
10.7%
1.5%
0.4%
2.2%
0.1%

53.8%

88.8%
3.3%
0.9%
0.6%
2.5%
0.4%
-
0.3%
-

92.0%

40.4% 66.7%
7.3% 51.5%
1.2% 29.0%
2.0%
0.9%
6.7% 17.3%
8.2%
0.2%
2.2%
0.9%
3.9%
-
0.3%
-

52.0% 77.9%

40.5%
43.4%
27.9%
3.6%
9.1%
5.4%
1.0%
6.9%
0.5%

71.5%

Table 2: The percentage of domains that can be bypassed using each origin-exposing vector, for each cloud-based security
provider’s customers.

tied to particular services and interfaces incorporated into
cPanel. Although these are HTTP services, they have to be
accessed through non-standard ports which are often inac-
cessible when standard ﬁrewall policies are used. Therefore,
cPanel creates these “proxy subdomains” that are directly
linked to a speciﬁc port on the origin [9]. Despite the e↵ort
of some CBSPs to support some typical ports used by these
control panels [37], these origin-exposing proxy subdomains
are still frequently used.

The second-most dominating origin-exposing subdomain,
namely direct (3,583 domains), is rather speciﬁc to Cloud-
Flare customers. This subdomain was, in the past, given
as an example when a user ﬁrst conﬁgures his domain on
CloudFlare’s web interface [8]. Apparently, a large number
of these clients used the company’s instructions to the let-
ter and thus kept this example subdomain bypass to link
directly to their origin.

Interestingly, DOSarrest and Prolexic customers are less
prone to subdomain exposure, with only 3.3% and 7.3% of
exposed domains respectively. This is most likely due to
the fact that each of their customers receives a dedicated IP
address. This one-to-one mapping between an IP address
of the CBSP and an IP address of the origin enables the
CBSP to simply forward certain ports to the correct origin,
without requiring any additional information to identify the
intended host.
IP history exposure
To assess the number of domains that are still accessible
through a previously documented IP address, we used two
domain tracking services, DomainTools [12] and MyIP.ms [33].
We queried these databases for every domain in our eval-
uation set and all historical IP addresses were marked as
candidates.

After the veriﬁcation of the collected, “historical” IP ad-
dresses, it is evident that exposure though historical data
is severe. Across all providers, we ﬁnd that 40.5% of the
domains are vulnerable to being exposed by consulting IP
History databases. Furthermore, the issue is prevalent with
all ﬁve provider’s customers. This implies that, in general,
CBSP’s customers often fail to conﬁgure a new IP address
after setting up their cloud-based security service. This, in
turn, indicates that customers are either unaware of this at-
tack vector, or are neglecting the CBSP’s recommendation
to change their IP address, possibly because of operational
or infrastructural barriers. Regarding DOSarrest and Pro-

lexic, it should be noted that the misconﬁguration of a single
client is greatly inﬂuencing the global number of IP history
bypasses. Namely, 92% of DOSarrest’s domains that were
vulnerable to IP history exposure, were caused by domains
that belonged to a single domain parking service. For Pro-
lexic, a similar parking service is responsible for 86% of their
historically exposed subdomains.
DNS record exposure
DNS records are arguably the most trivial and practical at-
tack vector that we studied. To assess whether they reveal
a domain’s origin, we simply retrieved all records for each
domain at a single point in time. From each record we ex-
tracted all IP addresses and marked them as candidates for
the origin. Additionally, if a domain was present in the DNS
record, we resolved the domain and marked the resulting IP
address as a candidate.

Despite its simplicity, we ﬁnd that a signiﬁcant number
of domains is exposed by this vector. More speciﬁcally, the
origin of CloudFlare-protected domains is exposed by DNS
records in 32.6% of the cases. For Sucuri and Incapsula, this
is 29.0% and 11.2% respectively. Most of these domains are
leaking their IP address through their MX record (4,390 do-
mains), followed by TXT records (1,134), where SPF is often
the reason, as described in Section 3.3. The frequent ex-
posure through these two records suggests that web servers
that send and receive email are responsible for a substantial
fraction of the discovered origin exposure. Interestingly, we
also found 16 domains that were exposed through their A
records. In these cases, both the origin and the CBSP’s IP
address were present in the domain’s A records. We spec-
ulate that in this situation, the client has created an addi-
tional A record for the CBSP’s IP address, while forgetting
to remove the existing record that pointed to the origin. For
the domains under the protection of CloudFlare, the DNS
records are managed by the CBSP. Therefore, we excluded
CloudFlare customers that were exposed through their A
record, as this indicates that the administrator has deliber-
ately paused the CBSP rerouting through CloudFlare’s web
interface.
Temporary exposure
To determine whether origins were temporarily exposed due
to an interruption of the cloud-based security service or due
to a transient leak in another DNS record, we repeatedly
retrieved, on a daily basis, all DNS records of protected do-

1537mains for a period of 10 weeks (Sucuri and Prolexic) or 16
weeks (CloudFlare, Incapsula and DOSarrest). We excluded
the domains that were already exposed by the one-time DNS
records retrieval, described in the previous paragraph. This
allows us to isolate the domains that only temporarily ex-
posed their origin. The number of temporal exposures is
considerable. We discover that more than 3% of domains
transiently revealed their origin through their DNS records
during a 10 or 16-week period. The vast majority of them
were exposed through their A record, indicating a brief dis-
abling of the protection system.

SSL certiﬁcate exposure
In order to ﬁnd IP addresses hosting SSL certiﬁcates associ-
ated with the domains in the evaluation set, we made use of
the publicly available data of Rapid7’s Project Sonar [42].
This project uses ZMAP [14] to periodically conduct scans
of the entire IPv4 address range in search for, among other
things, SSL certiﬁcates. We used their certiﬁcate data [41]
and extracted all IP addresses that presented certiﬁcates re-
lated to the domains in the evaluation set. According to Du-
rumeric et al. [13], 129,695 of the domains in the Alexa top 1
million (13%) possess browser-trusted certiﬁcates. This ap-
pears to be in line with the 9% of origins that we discovered
by looking for IP addresses that presented a certiﬁcate for
those domains. If the origin desires to secure the back-end
connection (the one between the CBSP and the origin) with
HTTPS, a certiﬁcate for its domain has to be presented by
the origin. Paradoxically, this, in turn, makes the entire set
up less secure by introducing the risk of origin exposure.

Sensitive ﬁles
We limit our search of sensitive ﬁles to the so-called phpinfo
ﬁles. These ﬁles execute the PHP function phpinfo() [49],
which outputs a large amount of data regarding the server,
the execution environment, PHP compilation options, etc.
This function is particularly interesting because it dynam-
ically retrieves all this data each time it is called. Fur-
thermore, it usually displays the server’s IP address in the
SERVER_ADDR ﬁeld.

We attempted to ﬁnd ﬁles that execute this function
by trying several obvious ﬁle names, namely phpinfo.php,
info.php, test.php and phpMyAdmin/phpinfo.php. Over-
all, we found that 5.4% of domains had at least one of these
ﬁles accessible and exposed their origin in this manner. Pre-
sumably, the ﬁles are a remainder of the development setup,
which the developers forgot to remove.

Origin in content
For the vectors involving analyzing the HTML content of
pages, we crawled each domain in the evaluation set. First,
we queried Bing for each domain using site:example.com
to retrieve an initial seed of 50 webpages. Starting from this
seed, we crawled additional pages by visiting internal links,
up to a maximum of 500 pages per domain. On average we
retrieved 328 pages per domain in the evaluation set.

To detect whether the origin was present on the website’s
pages, we searched the HTML source code of every crawled
page for the presence of IP addresses. We found only a small
number of domains (1%) that included the real IP address
of their web server in one of their pages, making it one of
the least e↵ective origin-exposing vectors.

Outbound connections
Since triggering outbound connections is closely tied to the
applications that run on any given web server, it is near
impossible to get a comprehensive measurement of the as-
sociated risk.
In order to get an impression of what is
possible, we chose to conduct two experiments on poten-
tially widespread mechanisms. The ﬁrst one revolved around
the Pingback mechanism, which is mostly found on Word-
Press [54], the most wide-spread blogging software [40]. The
second experiment focussed on the veriﬁcation of the HTTP
referrer header, which is being used, e.g., by RefBack [47],
to discover incoming links.

Pingback exposure. Pingback is a protocol that allows
website owners to get notiﬁed when one of their pages or
articles is mentioned on another website. When a server re-
ceives a notiﬁcation, Pingback should automatically visit the
other website to verify whether it actually contains a valid
hyperlink. This veriﬁcation procedure can be leveraged to
trigger an outbound connection from the origin. For the Ori-
gin in content vector, every domain in the evaluation set was
crawled. During this process, we simultaneously searched for
Pingback enabled webpages. Next, we made an XML-RPC
request to the Pingback endpoints, in which we included a
URL of a page on our server that contained a unique token
for each domain. As a result, we could extract candidate
origin IP addresses by monitoring the incoming requests on
our server and recording which IP addresses accessed which
domain-speciﬁc, tokenized URLs.

Essentially, Pingbacks allow a third party to force a web
server to issue a request to an arbitrary host. In the past,
this had lead to the creation of entire WordPress botnets,
which were abused to conduct DDoS attacks on websites [5].
As a consequence, awareness about Pingback abuse was in-
creased, encouraging many security companies and admin-
istrators to take steps towards preventing it from happen-
ing again [32, 46]. During our analysis, we often found that
websites and CBSPs were actively blocking our Pingback re-
quests, or refrained from initiating any outbound connection
to our server. However, we were still able to conﬁrm that
6.9% of protected domains expose their origin’s IP address
through the Pingback mechanism.

Referrer veriﬁcation exposure. In order to test expo-
sure through referrer veriﬁcation, we set the HTTP Refer-
rer header to a tokenized URL during the entire domain
crawling process. Similar to our Pingback approach, we
monitored whether there were any connections made to our
unique URLs, potentially by a web application of the origin
that wanted to inspect the referrer page that had lead a user
to the origin’s website. Our results indicate that this vector
poses only a minor risk for origin exposure. Only 0.5% of
domains were exposed by making an outbound connection
from their origin to the referrer of a visitor on their website.
Our server was, however, contacted by a plethora of other
IP addresses which mostly belonged to web spiders, such as,
Googlebot [16] and Proximic [39].

5. DISCUSSION & COUNTERMEASURES
Our ﬁndings categorically demonstrate that a comprehen-
sive adoption of CBSPs is harder than just changing DNS
records. Multiple origin-exposing vectors are highly preva-
lent and they generally involve di↵erent underlying causes,

1538making the problem complex and multifaceted. Addition-
ally, the results of our large-scale analysis are lower bounds.
In the wild, an attacker can go to a greater extent to discover
the origin of a particular targeted victim. For instance, if an
attacker has found an IP address associated with a website
through one of the origin-exposing vectors, he could scan the
entire IP address block to which it belongs in further search
for the origin. This can be e↵ective when a victim has re-
quested a new, “clean” IP address, but that address is pos-
sibly close to the previous one, since it is distributed by the
same ISP. Similarly, when associated servers, such as mail
servers, are discovered through subdomains or DNS records,
it is a reasonable assumption that the origin is located at a
nearby address. Furthermore, attackers can manually ana-
lyze the website to trigger outbound connections, search for
speciﬁc conﬁguration ﬁles, test for more subdomains, and
perform much more intrusive tests than those included in
Cloudpiercer.

Ultimately, unlike us, an attacker is not necessarily bound
to origin veriﬁcation. As noted in [34], an attacker can de-
duce the location of the origin by starting a DDoS attack
on one or more plausible IP addresses and observing the ef-
fect it has on the CBSP-protected website. If the origin is
taken down by this attack, the CBSP will display either a
static cached copy of the o✏ine website, or a 404-like error
message.

Countermeasures
Complete mitigation of origin exposure is hard, as adminis-
trators are required to fully understand the potential risks
and comprehensively address all vulnerabilities in order to
fully prevent an attacker from circumventing the CBSP.
However, a tool similar to Cloudpiercer could be deployed
by CBSPs to proactively scan their client’s domains for ex-
posed origins, creating awareness and helping administrators
ﬁx speciﬁc vulnerabilities.

Apart from countering each origin-exposing vector, the
logical ﬁrst line of defense is a proper ﬁrewall conﬁguration
that blocks all connections except those originating from
the CBSP. This will signiﬁcantly complicate the life of an
attacker who will not be able to tell whether an IP address
is unreachable, or whether it, in fact, is the origin of a tar-
get website. Together with requesting a new IP address,
this ﬁrewall conﬁguration should be standard practice when
cloud-based security is utilized. We can safely assume that
the vast majority of customers are currently not adopting
such a strategy, since, if they did, our origin veriﬁcation
method would have failed. It appears that administrators
are either uninformed about the risks, or are deterred by
the complications of properly whitelisting all IP addresses
necessary to keep the website operational. We conducted
a small-scale survey asking vulnerable websites about the
missing ﬁrewall conﬁgurations and their CBSP-related se-
curity expectations but we, unfortunately, received no re-
sponses.

CBSPs could actively monitor whether their client’s do-
main was assigned a fresh IP address, and whether the client’s
web server is blocking requests coming from outside of the
CBSP’s network. This information could then further be
used to explicitly warn and motivate administrators to take
the necessary measures to prevent exposure.

Another beneﬁcial strategy for CBSPs is to assign a unique
IP address to each customer, which is already the case with

Prolexic and DOSarrest. As our results showed, this has
a signiﬁcant e↵ect on the number of subdomains and DNS
records exposures. If the necessary ports can be forwarded
to the origin, there is no need to set up subdomains or MX
records that connect directly to the origin’s IP address. We
expect that as the adoption of IPv6 expands, this defense
mechanism will become increasingly more practical, even for
very large CBSPs, such as CloudFlare.

Possibly, some larger websites that possess entire /24 IP
blocks might be able to initiate BGP rerouting once the
origin has been attacked directly. However, relying on this
fallback scenario defeats the beneﬁts of the always-on strat-
egy and eliminates the protection against web application
attacks.

6. RELATED WORK

To the best of our knowledge, our research is the ﬁrst to
review existing origin-exposing attack vectors for the bypass-
ing of CBSPs, propose new ones, and systematically assess
the magnitude of the exposure problem in the wild.

Over the years, a plethora of DDoS defense systems have
been proposed. However, destination-based systems are usu-
ally rendered ine↵ective against large volumetric attacks that
are able to saturate a site’s uplink. Additionally, according
to Huang et al. [19], systems that seek cooperation of many
di↵erent parties usually face deployment issues. The authors
argue that a lack of incentive prevents these cooperative sys-
tems from being widely deployed across the Internet’s infras-
tructure. For instance, the proﬁt of transit providers greatly
depends on the amount of tra c they forward. Hence, these
providers are cautious with implementing ﬁltering systems
that might negatively impact their business. In constrast,
recent publications [11, 24] have documented the decline of
the NTP DDoS attacks, impacted by a large-scale collabo-
rative e↵ort amongst ISPs, CERTs and academia.

A feasible non-collaborative solution for a victimized au-
tonomous system (AS) was introduced in 2003 by Argawal
et al. [2]. The concept is to reroute DDoS tra c through
o↵-site cleaning centers that are dedicated to ﬁltering and
absorbing malicious attack tra c. The authors studied var-
ious network-layer techniques for diverting DDoS tra c to
cleaning centers and, afterwards, redirecting the clean traf-
ﬁc to the protected web server. This work later became the
inspiration for the patents of several popular DDoS miti-
gation services, such as Prolexic [29]. The use of rerout-
ing techniques, such as BGP diversion and GRE tunnelling,
resurfaced in Shield by Kline et al. [22]. In that paper, the
authors focus on leveraging the o↵-site DDoS mitigation as
an insurance model to solve the incentive problem. The au-
thors also note that CDNs can be leveraged as DDoS defense
systems in a similar fashion. In 2007, Lee et al. [26] already
studied the inherent DDoS resilience of CDNs and proposed
a novel scheme to further improve their robustness against
attacks.

As CDNs further incorporated security features into their
products, their business extended increasingly into cloud-
based security providers. Thereupon, various studies evalu-
ated these security components and several problems were
uncovered. For instance, Liang et al. [28] analyzed how
HTTPS was implemented within the context of CDNs. In-
herently, a CDN is a man-in-the-middle (MITM) between
the website and its visitors. This allows them to inspect
incoming requests for the purpose of serving cached content

1539and ﬁltering out malicious requests. However, as HTTPS is
intended for end-to-end encryption, this introduces various
complications.
In their study, the authors report on sev-
eral implementation issues, including private key sharing,
insecure back-end communication and numerous issues with
invalid, stale and revoked certiﬁcates.

Another issue, discovered by Triukose et al. [50], allows
CDNs to be abused to conduct a bandwidth ampliﬁcation
DDoS attack against their own customers. The vulnerabil-
ity leveraged the fact that requests to CDN-enabled websites
typically involve two decoupled TCP connections, with the
CDN as a MITM. However, once the CDN forwards an at-
tacker’s request to the origin, the attacker can cleverly break
o↵ his own TCP connection with the CDN. Thereupon, the
origin will waste bandwidth by sending a response to the
CDN that will no longer be forwarded to the attacker.

Finally, in 2013, Nixon et al. [34] and McDonald [31] raised
awareness of origin-exposing vectors that could enable at-
tackers to bypass CBSPs and CDNs. We extend their work
by proposing novel origin-exposing vectors and combining
them into one automated origin-exposing tool with origin-
veriﬁcation capabilities, which we then deployed to conduct
the ﬁrst large-scale assessment of the issue. DOM-based
similarities, which we leveraged for origin-veriﬁcation, were
previously used by [44] to detect phising attempts.

7. CONCLUSION

Cloud-based security is a popular solution to counter the
increasing threat of DDoS and web application attacks. CB-
SPs that use proxying via DNS are adopted by at least 9%
of the 10K most popular websites. Presumably, the trivial
setup without infrastructural investments, combined with
the beneﬁt of an always-on protection service, attracts a
large user base. The mechanism itself, however, su↵ers from
a critical weakness. The entire mitigation service is com-
pletely dependent on the secrecy of the website’s hosting
IP address, the so-called origin. Moreover, several vulnera-
bilities are reported that have the potential to expose this
origin.

In this paper, we discussed eight origin-exposing vectors,
including various novel vulnerabilities. We consolidated all
vectors into Cloudpiercer, an automated origin-exposing
tool, which we then used to conduct the ﬁrst large-scale
analysis to measure the global risk of origin exposure. Our
results demonstrate that the problem is severe: 71.5% of
the 17,877 CBSP-protected websites that we tested, exposed
their real IP address through at least one of the evaluated
vectors.

Taking into account the severe consequences of an exposed
origin and its prevalence amongst CBSP-protected websites,
we opine that the problem is currently inadequately ad-
dressed. However, the ﬁndings of our research can be used
both by CBSPs to encourage better practices regarding the
adoption of their security infrastructure, as well as by ad-
ministrators of CBSP-protected websites who can verify and
remediate their own origin-exposing vulnerabilities. All ﬁve
CBSPs have been notiﬁed of our ﬁndings prior to publica-
tion.

Finally, a silver lining of our ﬁndings is that a tool like
Cloudpiercer can, in principle, be used by law enforce-
ment. It is well known that miscreants use CBSPs to hide
their real hosting location [6], making it harder to track and
shut them down. Consequently, the discussed vectors and

their reported e↵ectiveness can be leveraged by the appro-
priate institutions to react quicker against malicious online
activities.

8. AVAILABILITY

Cloudpiercer will be made available as a web service
on https://distrinet.cs.kuleuven.be/software/cloudpiercer/,
where users of CBSPs, after proving ownership of their web-
sites, will be able to submit their URLs for scanning and
get a detailed report on all the origin-exposing vectors that
Cloudpiercer was able to ﬁnd. We hope that the commu-
nity will beneﬁt from this service by allowing administrators
to discover and eliminate vulnerabilities on their websites,
before they are discovered and abused by attackers.

Acknowledgments
We thank the anonymous reviewers for their valuable com-
ments, and Linode for providing us with virtual machines
that made our large-scale experiments possible. For KU
Leuven, this research is partially funded by the Research
Fund KU Leuven, and by the EU FP7 project NESSoS.
With the ﬁnancial support from the Prevention of and
Fight against Crime Programme of the European Union
(B-CCENTRE). For Stony Brook University, this work was
supported by the National Science Foundation (NSF) under
grant CNS-1527086.

9. REFERENCES
[1] J. Abley and K. E. Lindqvist. Operation of anycast

services. 2006.

[2] S. Agarwal, T. Dawson, and C. Tryfonas. Ddos

mitigation via regional cleaning centers. Technical
report.

[3] Alexa. Alexa - Actionable Analytics for the Web.

http://www.alexa.com/, 2014.

[4] Arbor Networks. Worldwide Infrastructure Security
Report. http://pages.arbornetworks.com/rs/arbor/
images/WISR2014 EN2014.pdf, 2015.

[5] D. Cid. More Than 162,000 WordPress Sites Used for

Distributed Denial of Service Attack.
http://blog.sucuri.net/2014/03/more-than-162000-
wordpress-sites-used-for-distributed-denial-of-service-
attack.html, 2014.

[6] CloudFlare Watch. http://www.crimeﬂare.com/.
[7] CloudFlare. Cloudﬂare sees explosive growth in 2013.

http://www.marketwired.com/press-
release/cloudﬂare-sees-explosive-growth-2013-passes-
15-million-customers-revenue-up-450-network-
1862981.htm, 2013.

[8] CloudFlare, Inc. Sign up | CloudFlare | The web

performance and security company.
https://www.cloudﬂare.com/sign-up, 2015.

[9] cPanel. Tweak Settings - Domains.

https://documentation.cpanel.net/display/ALD/
Tweak+Settings+-+Domains#TweakSettings-
Domains-Proxysubdomains, 2015.

[10] cPanel, Inc. cPanel and WHM. http://cpanel.net/,

2015.

[11] J. Czyz, M. Kallitsis, M. Gharaibeh, C. Papadopoulos,

M. Bailey, and M. Karir. Taming the 800 pound
gorilla: The rise and decline of ntp ddos attacks. In
Proceedings of the 2014 Conference on Internet
Measurement Conference, pages 435–448. ACM, 2014.

[12] DomainTools, LLC. Domain Whois Lookup, Whois

API and DNS Data Research - DomainTools.
http://www.domaintools.com/, 2015.

1540[13] Z. Durumeric, J. Kasten, M. Bailey, and J. A.

Halderman. Analysis of the https certiﬁcate ecosystem.
In Proceedings of the 2013 conference on Internet
measurement conference, pages 291–304. ACM, 2013.

[14] Z. Durumeric, E. Wustrow, and J. A. Halderman.

Zmap: Fast internet-wide scanning and its security
applications. In USENIX Security, pages 605–620.
Citeseer, 2013.

[15] K. Fiveash. PlayStation clambers back online days

after DDoS attack PARALYSED network.
http://www.theregister.co.uk/2014/12/27/
playstation clambers back online/, 2014.

[16] Google. Googlebot. https://support.google.com/

webmasters/answer/182072?hl=en, 2015.

[17] T. Henderson and S. Johnson. Zhang-Shasha: Tree

edit distance in Python.
https://github.com/timtadh/zhang-shasha, 2014.

[18] A. Hidayat. PhantomJS - a headless WebKit scriptable

with a JavaScript API. http://phantomjs.org, 2015.
[19] Y. Huang, X. Geng, and A. B. Whinston. Defeating

ddos attacks by ﬁxing the incentive chain. ACM
Transactions on Internet Technology (TOIT), 7(1):5,
2007.

[20] M. Karami and D. McCoy. Understanding the

emerging threat of ddos-as-a-service. In LEET, 2013.

[21] Kitterman, Scott. Sender Policy Framework (SPF) for

Authorizing Use of Domains in Email, Version 1.
http://tools.ietf.org/html/rfc7208, 2014.

[22] E. Kline, A. Afanasyev, and P. Reiher. Shield: Dos

ﬁltering using tra c deﬂecting. In Network Protocols
(ICNP), 2011 19th IEEE International Conference on,
pages 37–42. IEEE, 2011.

[23] B. Krebs. The New Normal: 200-400 Gbps DDoS
Attacks. http://krebsonsecurity.com/2014/02/the-
new-normal-200-400-gbps-ddos-attacks/, 2014.

[24] M. K¨uhrer, T. Hupperich, C. Rossow, and T. Holz.

Exit from hell? reducing the impact of ampliﬁcation
ddos attacks. In USENIX Security Symposium, 2014.

[25] S. Langridge and I. Hickson. Pingback 1.0.

http://www.hixie.ch/specs/pingback/pingback, 2002.

[26] K.-W. Lee, S. Chari, A. Shaikh, S. Sahu, and P.-C.

Cheng. Improving the resilience of content distribution
networks to large scale distributed denial of service
attacks. Computer Networks, 51(10):2753–2770, 2007.

[27] D. Lewis. Bypassing Content Delivery Security.

https://blogs.akamai.com/2013/08/bypassing-content-
delivery-security.html, 2013.

[28] J. Liang, J. Jiang, H. Duan, K. Li, T. Wan, and J. Wu.

When https meets cdn: A case of authentication in
delegated service. In Security and Privacy (SP), 2014
IEEE Symposium on, pages 67–82. IEEE, 2014.

[29] B. Lyon. Network overload detection and mitigation

system and method, Jan. 13 2009. US Patent
7,478,429.

[30] K. Martens, J. Mehnle, and S. Kitterman. SPF

Record Syntax.
http://www.openspf.org/SPF Record Syntax, 2008.

[31] D. McDonald. The Pentesters Guide to Akamai.

https://www.nccgroup.com/media/230388/the
pentesters guide to akamai.pdf, 2013.

[32] Moore, Simon. WordPress Pingback Attacks and our

WAF. https://blog.cloudﬂare.com/wordpress-
pingback-attacks-and-our-waf/, 2014.

[33] MYIP.MS. My IP Address - Shows IPv4 and IPv6 |
Blacklist IP Check - Hosting Info. http://myip.ms/,
2015.

[34] A. Nixon and C. Camejo. Ddos protection bypass

techniques. Black Hat USA, 2013.

[35] P. Olson. The Largest Cyber Attack In History Has
Been Hitting Hong Kong Sites. http://www.forbes.

com/sites/parmyolson/2014/11/20/the-largest-cyber-
attack-in-history-has-been-hitting-hong-kong-sites/,
2014.

[36] C. Perez. DNS Recon.

https://github.com/darkoperator/dnsrecon, 2015.

[37] M. Prince. CloudFlare Now Supporting More Ports.

https://blog.cloudﬂare.com/cloudﬂare-now-
supporting-more-ports/, 2012.

[38] M. Prince. The DDoS That Almost Broke the

Internet. https://blog.cloudﬂare.com/the-ddos-that-
almost-broke-the-internet/, 2013.

[39] Proximic. Proximic Spider.

http://www.proximic.com/spider.html, 2015.

[40] Q-Success. Market share trends for content

management systems for websites.
http://w3techs.com/technologies/history overview/
content management, 2015.

[41] Rapid7 Labs. Internet-Wide Scan Data Repository.

Project Sonar: IPv4 SSL Certiﬁcates.
https://sonar.labs.rapid7.com/, 2015.

[42] Rapid7 Labs. Project Sonar.

https://sonar.labs.rapid7.com/, 2015.

[43] L. Richardson. Beautiful Soup.

http://www.crummy.com/software/BeautifulSoup/,
2013.

[44] A. P. Rosiello, E. Kirda, C. Kruegel, and F. Ferrandi.

A layout-similarity-based approach for detecting
phishing pages. In Security and Privacy in
Communications Networks and the Workshops, 2007.
SecureComm 2007. Third International Conference
on, pages 454–463. IEEE, 2007.

[45] C. Rossow. Ampliﬁcation Hell: Revisiting Network

Protocols for DDoS Abuse. In Proceedings of the 2014
Network and Distributed System Security (NDSS)
Symposium, February 2014.

[46] Stephenson, MaAnna. Disable XML-RPC in

WordPress to Prevent DDoS Attack.
http://www.blogaid.net/disable-xml-rpc-in-
wordpress-to-prevent-ddos-attack, 2014.

[47] H. Story and A. Sambra. Friending on the Social Web.

http://bblﬁsh.net/tmp/2011/05/09/, 2011.

[48] N. Sullivan. DDoS Prevention: Protecting The Origin.

https://blog.cloudﬂare.com/ddos-prevention-
protecting-the-origin/, 2013.

[49] The PHP Group. PHP: phpinfo - Manual.

http://php.net/manual/en/function.phpinfo.php,
2015.

[50] S. Triukose, Z. Al-Qudah, and M. Rabinovich.

Content delivery networks: protection or threat? In
Computer Security–ESORICS 2009, pages 371–389.
Springer, 2009.

[51] T. Van Goethem, P. Chen, N. Nikiforakis, L. Desmet,

and W. Joosen. Large-scale security analysis of the
web: Challenges and ﬁndings. In Trust and
Trustworthy Computing, volume 7, pages 110–125.
Springer, 2014.

[52] S. J. Vaughan-Nichols. Worst DDoS attack of all time
hits French site. http://www.zdnet.com/article/worst-
ddos-attack-of-all-time-hits-french-site/, 2014.

[53] R. Westervelt. Cloud-Based DDoS Protection Is Easily

Bypassed, Says Researcher.
http://www.crn.com/news/security/240159295/cloud-
based-ddos-protection-is-easily-bypassed-says-
researcher.htm, 2013.

[54] WordPress.org. WordPress: Blog Tool, Publishing

Platform, and CMS. http://wordpress.org, 2015.

[55] K. Zhang and D. Shasha. Simple fast algorithms for

the editing distance between trees and related
problems. SIAM journal on computing,
18(6):1245–1262, 1989.

1541