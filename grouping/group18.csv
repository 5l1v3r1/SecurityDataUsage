 6303. DATASETS Our Internet-wide study of key sharing in the HTTPS ecosystem is driven by four datasets: SSL certificates We use SSL certificates from full IPv4 scans as the basis of our measurements,Data,0
| Our SSL scans [30] also contain information on the IP address(es) that advertised each certificate. To obtain in- formation about the entity that controls this IP address, we use full IPv4 reverse DNS scans [29] that are also conducted by Rapid7|,Data,0
| Each AS is assigned an AS Number (ASN): for example, MIT is AS 3 and the Chicago Public Schools are AS 1416 [26]. CAIDA collects and publishes mappings between IP addresses and ASNs via their Route- Views datasets [7]|,Data,0
| For example, AT&T owns 160 unique ASNs. To aggregate these, we use CAIDA’s AS- to-Organization dataset [8] to group together ASes owned by the same organization|,Data,0
| For that, we rely on WHOIS [12], a protocol for querying domain registrars to obtain data on the domain owner. In practice, WHOIS data often contains fields such as the con- tact information for the owner of the domain, the contact for technical issues, where to send abuse complaints, and so on|,Data,0
| Here, we expand upon these prior findings by evaluating whether there is a correlation between centralized management and the quality of the keys chosen. Figure 13 compares several different features of self- managed and outsourced certificates across our entire cor- pus of leaf certificates (3,275,635 self-managed and 1,781,962 outsourced): (a) Key lengths in self-managed certificates are nearly identical to those managed by third-party hosting providers|,Data,0
|1 Combining Packet Capture (PCAP) Files The data set used in this study is a combination of the packet capture files obtained from two main sources. First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour|,Data,1
| First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour. The normal and non-malicious data is obtained from PREDICT internet data set repository [18] under the category of “DARPA Scalable Network Monitoring (SNM) Program Traffic”|,Data,1
| The data collection was performed during April 2016 using ZGrab, an application-layer scanner that operates with ZMap [15]. In the first phase, we performed an Internet-wide scan of all IPv4 addresses on port 500 to determine which hosts were configured 16This defect was corrected quite recently, years after the version of OpenSSL ScreenOS uses was written|,Data,6
|, download from an external source). Running on 71,000 articles collected from 45 leading technical blogs, this new approach demonstrates a remarkable performance: it gener- ated 900K OpenIOC items with a precision of 95% and a coverage over 90%, which is way beyond what the state-of-the-art NLP tech- nique and industry IOC tool can achieve, at a speed of thousands of articles per hour|,Data,7
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
 5. ANALYSIS AND FINDINGS In the following we use the extensive documentation of the 61 minimal exploits to provide insight into how attackers use specific vulnerabilities and features of the Java platform to implement their attacks,Data,11
| We run our event analysis on the top 100 free applications in the Android application store to determine how often this happens. In total, our analysis finds 1060 errors across 88 of the top 100 applications (10|,Data,12
| To our knowledge, AUTOREB is the first work that explores the user review information and utilizes the review semantics to predict the risky behaviors at both review-level and app-level. We crawled a real-world dataset of 2, 614, 186 users, 12, 783 apps and 13, 129, 783 reviews from Google play, and use it to comprehensively evaluate AUTOREB|,Data,14
| 4.1 Data collection For each team, we collected a variety of observed and self- reported data|,Data,16
| To demonstrate this, we scraped greatfire.org for websites in the top 1000 Alexa websites that are blocked by the GFW|,Data,18
| (cid:15) Identifying New Vulnerabilities. Our tool successfully an- alyzed 1,591 service interfaces of all the 80 system services in Android 5|,Data,19
| To understand the scope and magnitude of this new XARA threat, we developed an ana- lyzer for automatically inspecting Apple apps’ binaries to deter- mine their susceptibility to the XARA threat, that is, whether they perform security checks when using vulnerable resource-sharing mechanisms and IPC channels, a necessary step that has never been made clear by Apple. In our study, we ran the analyzer on 1,612 most popular MAC apps and 200 iOS apps, and found that more than 88|,Data,24
| To assist software developers (or secu- rity analysts) in tracking down a memory corruption vulnerability, CREDAL also performs analysis and highlights the code fragments corresponding to data corruption. To demonstrate the utility of CREDAL, we use it to analyze 80 crashes corresponding to 73 memory corruption vulnerabilities archived in Offensive Security Exploit Database|,Data,25
| These techniques may be applicable in other scenarios. We implemented and evaluated the attacks against the popular Gmail and Bing services, in several environments and ethical experiments, taking careful, IRB-approved mea- sures to avoid exposure of personal information|,Data,26
|, CSPAutoGen can handle all the inline and dynamic scripts. We have implemented a prototype of CSPAutoGen, and our eval- uation shows that CSPAutoGen can correctly render all the Alexa Top 50 websites|,Data,27
| 5. EXPERIMENTAL RESULTS This section reports on our evaluation of the moments ac- countant, and results on two popular image datasets: MNIST and CIFAR-10|,Data,28
| 6.1 Mobility Trace Dataset We use the CRAWDAD dataset roma/taxi [2, 3] for our simu- lations|,Data,31
 6.1 Evaluation We evaluated the performance of Σoφoς using 4 data sets of increasing size and also the English Wikipedia,Data,33
|1 Datasets, Metrics, Competitors & Settings Datasets. We test EpicRec on two real-world datasets: MovieLens1: a movie rating dataset collected by the Grou- pLens Research Project at the University of Minnesota through the website movielens|,Data,36
| 1 http://grouplens.org/datasets/movielens 188Yelp2: a business rating data provided by RecSys Chal- lenge 2013, in which Yelp reviews, businesses and users are collected at Phoenix, AZ metropolitan area|,Data,36
| The number of movie categories is 18. We use the MovieLens- 1M, with 1000,209 ratings from 6,040 users on 3,883 movies|,Data,36
| Our goal is to show that an ad- versary can insert an unbounded number of Sybil identities in the SybilLimit protocol, breaking its security guarantees. For our evaluation, we consider a real-world Facebook inter- action graph from the New Orleans regional network [28]|,Data,38
| We utilize these papers to extract Android malware behaviors and to construct the semantic network. From the electronic proceedings distributed to conference participants, we collect the papers from the IEEE Sympo- sium on Security and Privacy (S&P’08–S&P’15)4, the Com- puter Security Foundations Symposium (CSF’00–CSF’14), and USENIX Security (Sec’11)|,Data,39
 We conduct experiments on two publicly available set-valued datasets. • AOL search log dataset [1],Data,45
 90% of the users have fewer than 84 keywords in their logs. • Kosarak dataset [2],Data,45
 We select one month of data for our study. The data logs we used are col- lected from more than 30 machines with various server mod- els and operating systems,Data,46
| This paper rigorously investigates how users’ security beliefs, knowledge, and demographics corre- late with their sources of security advice, and how all these factors influence security behaviors. Using a carefully pre- tested, U|,Data,48
 We have ported Valgrind to iOS and implemented a prototype of iRiS on top of it. We evaluated iRiS with 2019 applications from the official App Store,Data,54
| from manufacturing equipment, as shown in Figure 1. We capture the relevant sensor data by deliberately or accidentally placing an attack-enabled phone close to, on top of, or inside a piece of manu- facturing equipment while the machinery is fabricating the target object|,Data,55
| Our new metric helps us compare in a fair way previously proposed attack-detection mechanisms. (ii) We compare previous attack-detection proposals across three di↵erent experimental settings: a) a testbed operating real-world systems, b) network data we collected from an operational large-scale Supervisory Control and Data Acqui- sition (SCADA) system that manages more than 100 Pro- grammable Logic Controllers (PLCs), and c) simulations|,Data,57
| Evaluation. We ran Oyente on 19, 366 smart contracts from the first 1, 460, 000 blocks in Ethereum network and found that 8, 833 contracts potentially have the documented bugs|,Data,58
| First, we consolidate the eight origin-exposing vectors into one auto- mated origin-exposing system called Cloudpiercer. Then, we assemble a list of clients from five CBSP companies by studying their DNS configurations and obtaining their adop- tion rate across the Alexa top 1 million websites|,Data,59
| The vast majority of them were exposed through their A record, indicating a brief dis- abling of the protection system. SSL certificate exposure In order to find IP addresses hosting SSL certificates associ- ated with the domains in the evaluation set, we made use of the publicly available data of Rapid7’s Project Sonar [42]|,Data,59
| 4. LARGE-SCALE ANALYSIS To assess the magnitude of the origin-exposure problem, we conduct a large-scale analysis in which we attempt to uncover the origin of CBSP-protected domains|,Data,59
|1 Dataset Description The dataset was first presented and used by Keller et al. in [23], and is publicly available in the gene expression om- nibus (GEO) database under reference GSE61741|,Data,61
| Although the cost of stor- age and processing have diminished, the cost of maintaining reliable infrastructure for transaction logs is still noticeable. Figure 1: A plot of transaction fee versus frequency for 1 million transactions in May 2015|,Data,65
| To estimate the cost of producing the preprocessing data (multiplication triples, random bits etc.), we used figures from the recent MASCOT protocol [31], which uses OT ex- tensions to obtain what are currently the best reported triple generation times with active security|,Data,67
| In this section, we validate whether the smartphone’s acoustic data can be utilized to deduce the movements. To conduct the validation, we implement an application on Nexus 5 (Android OS v6|,Data,68
| As seen in Table 4, we found that about half of the servers in Alexa’s top 10 support a large number of requests without rekeying. For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client|,Data,72
| For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client. We identified 11483 different HTTPS servers11, and found that 226 of them (1|,Data,72
| In this paper, we study the possible techniques to detect and measure this fraud and evaluate the real impact of OTT bypass on a small European country. For this, we performed more than 15,000 test calls during 8 months and conducted a user study with more than 8,000 users|,Data,78
|, the server cannot learn their relative order) after some number of queries are performed over real-world data. Specifically, we ran an experiment where we inserted over 2 million public employee salary figures from [1] and then performed 1000 random range queries|,Data,79
| In this study, we are interested in finding answers to security- and privacy-related questions about libraries, such as “How prevalent are third- party libraries in the top apps and how up-to-date are the library versions?”, “Do app developers update the libs included in their apps and how quickly do they update?”, or “How prevalent are vulnerabilities identified in prior research [28, 9] in libraries and how many apps are affected?” To answer these questions, we first built a comprehensive repository of third-party libraries and applications (see Section 5). Our library set contains 164 libraries of different categories (Ad- vertising, Cloud,|,Data,84
|) and a total of 2,065 versions. We then collected and tracked the version histories for the top 50 apps of each category on Play between Sep 2015 and July 2016, accumulating to 96,995 packages from 3,590 apps|,Data,84
|6.1, we found in our sample set 360 affected packages from 23 distinct apps, when only considering exact library matches|,Data,84
|15 for Android, which contained an account hijacking vulnerability, on 06/11/2014. In the histories of our sample set apps, we discovered, in total, 394 affected packages from 51 distinct apps, when only considering packages with exact matches of the vulner- able lib version|,Data,84
| We used LibScout to detect the affected application packages in our data set. In total 2,667 app versions of 296 distinct apps with a cumu- lative install-base of 3|,Data,84
| We observed that there is a significant variance in ACFG size. To reduce the sampling bias, we first collect a dataset which covers ACFGs of different functions from various architec- tures|,Data,89
| This dataset was used for base- line comparison, and all functions in this dataset has known ground truth for metric validation. We prepared this dataset using BusyBox (v1|,Data,89
| Dataset II – Public dataset. Recent work such as Pewny et al [45] and Eschweiler et al [23] used the same public dataset based upon two publicly-available firmware images for baseline comparison [7, 8]|,Data,89
| Dataset III – Firmware image dataset. This dataset of 33,045 firmware images was collected from the wild|,Data,89
| As a result, we created a freely available vulnerability database for this effort and for the broader research community. To build this database, we mined official software websites to collect lists of vulnerabilities with the corresponding CVE num- bers|,Data,89
| We selected OpenSSL for demonstration, since it is widely used in IoT devices. The resulting vulnerability database includes 154 vulnerable functions|,Data,89
| Roughly speaking, our measurement methods can be divided into two kinds: those that could be fully automated and scaled eas- ily, and those that required some manual interaction. For the latter, we used a set of 302938 major email providers and email genera- tors, while for the former, we used a much larger set of a million popular providers occurring in the Adobe leak and the Alexa top million Web sites (as potential email generators)|,Data,90
1.2 Provider List We created the set of popular email providers based on the top 1 million email address domains occurring in the leaked Adobe user data set of September 2013,Data,90
| Using a combination of mea- surement techniques, we determine whether major providers sup- ports TLS at each point in their email message path, and whether they support SPF and DKIM on incoming and outgoing mail. We found that while more than half of the top 20,000 receiving MTAs supported TLS, and support for TLS is increasing, servers do not check certificates, opening the Internet email system up to man- in-the-middle eavesdropping attacks|,Data,90
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
| 3.1 Datasets We use two major types of datasets: (1) packet-level traffic traces collected at various locations in a campus network, and (2) packet-level traces for Tor Pluggable Transport traffic collected in controlled environments|,Data,91
 Evaluation: local mixing time in social graphs. We use 10 various large-scale real-world social network topolo- gies that mainly come from the Stanford Large Network Dataset Collection [23] and other sources [45] to evaluate the local mixing time for nodes in social graphs,Data,92
| Feature Functions and Weights. To learn all feature functions and weights, we downloaded 1784 non-obfuscated Android applications from F-Droid [3], a popular repository for open-source Android applications|,Data,93
2.2 Experiments with Malware Samples We randomly selected one sample from each of the 49 mal- ware families reported in [40],Data,93
1_r1). Apps in our dataset used for the case study are downloaded from the Google official market (Google Play) in May 2016,Data,95
| • Using SInspector, we perform the first study of Unix domain sockets on Android, including the categoriza- tion of usage, existing security measures being en- forced, and common flaws and security implications. We analyze 14,644 apps and 60 system daemons, find- ing that 45 apps, as well as 9 system daemons, have vulnerabilities, some of which are very serious|,Data,98
| We presented SInspector, a tool for discovering potential security vulnerabilities through the process of identifying socket addresses, detecting authen- tication checks, and performing data flow analysis on na- 90tive code. We analyzed 14,644 Android apps and 60 system daemons, finding that some apps, as well as certain system daemons, suffer from serious vulnerabilities, including root privilege escalation, arbitrary file access, and factory reset- ting|,Data,98
| Our results show that many of our attacks succeed with a 100% chance such that the Sound-Proof cor- relation algorithm will accept the attacked audio samples as valid. Third, we collect general population statistics via an online sur- vey to determine the phone usage habits relevant to our attacks|,Data,100
 We find that the larger width of integer types and the increased amount of addressable memory introduce previously non-existent vulnerabilities that often lie dormant in program code. We empirically evaluate the prevalence of these flaws on the source code of Debian stable (“Jessie”) and 200 popular open-source projects hosted on GitHub,Data,104
| We have applied UniSan to the latest Linux kernel and Android kernel and found that UniSan can successfully prevent 43 known uninitialized data leaks, as well as many new ones. In particular, 19 of the new data leak vulnerabilities in the latest kernels have been confirmed by the Linux community and Google|,Data,107
| This allows us to prevent replay attacks, which are possibly the most applicable attack vectors against biometric authentication. Using a gaze tracking device, we build a prototype of our system and perform a series of systematic user experiments with 30 participants from the general public|,Data,108
| If two commits were blamed for the same amount of lines, blame both. Our heuristic maps the 718 CVEs of our dataset to 640 VCCs|,Data,109
| However, improving our blame heuristics further is an interesting avenue for future research. Apart from the 640 VCCs, we have a large set of 169,502 unclassified commits|,Data,109
|9 The SVM detected a high amount of excep- tions, a high number of changed code, inline ASM code, and variables containing user input such as __input and user. 6As previously mentioned we use the years 2011–2014 as the test dataset, since we have ground truth data on which to base the discussion|,Data,109
| When given a source file, Flawfinder returns lines with suspected vul- nerabilities. It offers a short explanation of the finding as well as a link to the Common Weakness Enumeration (CVE) database|,Data,109
| The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database|,Data,109
| Our results show that our approach significantly outperforms the vulner- ability finder Flawfinder. We created a large test database containing 66 C and C++ project with 170,860 commits on which to evaluate and compare our approach|,Data,109
 VoiceLive takes advantages of the user’s unique vocal system and high quality stereo recording of smartphones. • We conduct extensive experiments with 12 participants and three different types of phones under various ex- perimental settings,Data,111
| To test if WebCapsule can successfully record and subsequently replay real-world phishing attacks, we proceeded as follows, us- ing Chromium on our desktop machine. We selected a large and diverse set of recently reported phishing web pages from Phish- Tank8|,Data,112
| 2.4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime|,Data,113
|4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime. The dataset Cal represents the latitude and longitude of about 21,000 intersections in the California road network1 (also used by Mavroforakis et al|,Data,113
294258. The dataset SpitzLoc consists of latitude and longitude coordinates tracking the movement of German Green party politician Malte Spitz over six months,Data,113
| In this section, we aim to explore whether the differences of keystroke wave- forms are large enough to be used for recognizing different keys inputs in the real-world setting. We collected training and testing data from 10 volunteers|,Data,114
 B. Real Attacks MAD uniformly detects attacks more quickly than the PAD; we use the former method to detect the presence of an attack in real Internet traces3,Data,119
 III. DATA SET  changes  The data used was the PREDICT ID USC-Lander!  (- 60  The total  were DNS attack packets,Data,120
|395326000  files IPs. There are total 59,928,920 packet counts out of which there was a total of  DoS_DNS_amplification-20130617 (2013-06-17) (2013-06-17) with anonymized million) 358019 DNS packets|,Data,120
| The maximum number of unique hosts per day we measured was 106,000. To understand these differences, we compared the observations from our network monitor to data collected from DShield (www|,Data,121
| 3.1 From our own transactions We engaged in 344 transactions with a wide variety of services, listed in Table 1, including mining pools, wallet services, bank ex- changes, non-bank exchanges, vendors, gambling sites, and mis- cellaneous services|,Data,122
| Wallets. We kept money with most of the major wallet services (10 in total), and made multiple deposit and withdrawal transac- Bank exchanges|,Data,122
|, in which the exchange rate is not fixed) also function as banks. As such, we tagged these services just as we did the wallets: by depositing into and withdrawing from our accounts (but rarely par- ticipating in any actual currency exchange)|,Data,122
|info/tags, including both addresses provided in users’ signatures for Bitcoin forums, as well as self-submitted tags. We collected all of these tags — over 5,000 in total — keeping in mind that the ones that were not self-submitted (and even the ones that were) could be regarded as less reliable than the ones we collected ourselves|,Data,122
| 3.1 Data analysis overview We use three data sets, summarized in Table 1|,Data,123
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
| We also describe our application of the technique to the IPv6 interface-level graph captured by CAIDA’s Archipelago (Ark) infrastructure [14] for March 2013. The graph consists of all the 52,986 IPv6 interfaces numbered within the 2000::/3 unicast prefix captured from all 27 Ark vantage points (VPs) with IPv6 connectivity|,Data,125
| cause the counters of distinct routers to diverge, and (4) confirm aliases with pairwise probing. Given the absence of velocity in ID counters and the large probes required for the technique to work, we probe at a low rate of 20pps from a single VP, producing 26Kbps of traffic|,Data,125
| 3. METHODOLOGY In this section, we describe the design of our experiment and our data collection methodology, as well as the mitigating steps and proactive measurements we conducted to ensure a minimal im- pact of our covering routes|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| Our IPv6 network telescope results suggest sev- eral important differences (and some similarities) compared to that body of work. To produce a more recent and valid comparison, we analyzed a single week of IPv4 background radiation captured during the course of our ongoing IPv6 packet capture|,Data,126
| 4. DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1|,Data,127
| DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1. Our primary dataset consists of changes made to the |,Data,127
| domains, (2) the removal of existing domains, and (3) changes to existing domains in terms of revisions to their associated name- servers. Our data includes captures of the DNZA files as recorded every five minutes, time periods we refer to as epochs|,Data,127
| Since we lack comprehensive ground truth regarding the ultimate use of domains, to this end we use two proxies: subsequent appearance of a newly registered do- main in: (1) an email spam campaign, or (2) a domain blacklist. For the first of these, we operated a spam trap, i|,Data,127
|com), by restricting our focus to domains recently registered (March–July 2012) we can filter down the do- mains appearing in the spam trap to those very likely used for spam- ming. For the second, we subscribed to three major DNS blacklists, URIBL, SURBL, and Spamhaus DBL|,Data,127
| In this paper, we examine the effectiveness of these inter- ventions in the context of an understudied market niche, counterfeit luxury goods. Using eight months of empirical crawled data, we identify 52 distinct SEO campaigns, document how well they are able to place search results for sixteen luxury brands, how this ca- pability impacts the dynamics of their order volumes and how well existing interventions undermine this business when employed|,Data,128
| For a small number of stores, we were also able to collect user traffic data that directly measures an SEO campaign’s effectiveness in attracting customers to their stores. Specifically, we were able to periodically collect AWStats data for 647 storefronts in 12 cam- paigns|,Data,128
| One issue that undermines coverage is that Google only labels the root of a Web site as “hacked”, and does not label search results that link to sub-pages within the same root domain. In the PSR data set, we found 68,193 “hacked” search results|,Data,128
| We begin by exam- ining the properties of individual darknets and in particular the behavior of source IP addresses. We provide these char- acterizations by looking at data from 14 darknet monitors ranging in size from a /25 monitor to a /17 monitor over a period of 10 days between August 18, 2004 and August 28, 2004|,Data,129
| Figure 10: The number of darknets (of 31) reporting a port in the top 10 ports over a day, week, and month time frame. The analysis is performed for the top 10 destination ports over a day, top 10 destination ports over a week, and top 10 destination ports over a month|,Data,129
| 3.6 Datasets This paper uses DNS datasets from three authorities: one national-level top-level domain, operators of two root servers as shown in Table 1|,Data,130
 JP-DNS operates the .jp country code domain for Japan; we have data from all seven of their anycast sites,Data,130
|) part of the 2014 DITL collection [16] (for B-Root, shortly after 2014 DITL). We also use data for M-Root’s 2015 DITL collection (§ 4|,Data,130
 These root datasets are available to re- searchers through DNS-OARC. For longitudinal analysis we draw on 9 months of data taken at the M-Root server,Data,130
| However, we treat the union of these classes together. We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness|,Data,131
| We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness. The dataset consists of all echo requests that were sent as part of the surveys in this period, as well as all echo responses that were received|,Data,131
|, “host unreachable”); we ignore all probes as- sociated with such responses since the latency of ICMP error responses is not relevant. In later sections, we will complement this dataset with results from Zmap [5] and additional experiments includ- ing more frequent probing with Scamper [13] and Scrip- troute [22]|,Data,131
| 3.2 Milking 3 Methodology To collect the information needed to cluster servers into oper- ations, we have built an infrastructure to track individual exploit servers over time, periodically collecting and classi- fying the malware they distribute|,Data,132
 2. We receive feeds of drive-by download URLs (Sect,Data,132
 2. CHARACTERISTICS OF CHECK-INS We use three different datasets that capture human mobility,Data,133
 First we consider two online location-based social networks. We col- lected all the public check-in data between Feb,Data,133
| There are 196,591 nodes, 950,327 edges in Gowalla and 58,228 nodes, 214,078 edges in Brightkite. To ensure that our observations on human movement are not specific to data based on check-ins from location-based social net- works, we also include a dataset of cell phone location trace data|,Data,133
| Backscatter DDoS is a commonly seen behaviour in darknets where the attacker uses simultaneous bots to generate the actual attack packets to reach the targeted (original) victim. In our study, five publicly available network traffic datasets from CAIDA’s archives are employed|,Data,134
| Datasets Employed In this research, five publicly available real-life network traffic traces (datasets) from CAIDA’s archives are employed. Three of them, which were captured by a passive darknet in 2007, 2008 and 2012 [27][26][28], namely UCSD Network Telescope [21], include mostly one-way malicious traffic while the remaining ones collected in 2008 [29] and 2014 [30] via CAIDA’s Internet backbone links include only normal traffic|,Data,134
| 3 Approach This section presents our approach for the evalua- tion of reputation based blacklists. We evaluated the blacklists by deploying them in a large academic net- work of over 7,000 hosts|,Data,135
| This was a preliminary step to preventing inexperienced and non-serious workers from participating in our survey. Our survey is based on the participants’ actual check-ins on Foursquare posted over the last 24 months (that we collected through a specific application we developed), and it requires a significant amount of time to complete (30-45 minutes)|,Data,136
| The third phase of worm activ- ity is the persistence phase which for the Blaster worm has continued through 2004. In this one-week period of measurement, the IMS system observed over 286,000 unique IP addresses displaying the characteristics of Blaster activity|,Data,137
| published a study in 2011 that focused on the dynamics of leaf cer- tificates and the distribution of certificates among IP addresses, and attempted to roughly classify the overall quality of served certifi- cates. The study was based on regular scans of the Alexa Top 1 Mil- lion Domains [1] and through passive monitoring of TLS traffic on the Munich Scientific Research Network [17]|,Data,138
| Our study is founded on what is, to the best of our knowledge, the most comprehensive dataset of the HTTPS ecosystem to date. Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443|,Data,138
| Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443. Over the course of 14 months, we completed upwards of 400 billion SYN probes and 2|,Data,138
| Content Provider e Service Provider v i t c e p s r e P Content Consumer Addressing Prerequisite IP Functions Routing Naming A1: Address Allocation; A2: Address Advertisement N1: Nameservers; R1: Server Readiness N2: Resolvers N3: Queries A2: Address Advertisement; T1: Topology End-to-End Reachability R1: Server Readiness Operational Characteristics Usage Profile Performance U3: Transition Technologies U1: Traffic Volume; U3: Transition Technologies P1: Network RTT R2: Client Readiness U2: Application Mix; N3: Queries Table 2: Dataset summary showing the time period, scale, and public or new status of the datasets we analyzed. Dataset RIR Address Allocations Routing: Route Views Routing: RIPE Google IPv6 Client Adoption Verisign TLD Zone Files CAIDA Ark Performance Data Arbor Networks ISP Traffic Data Verisign TLD Packets: IPv4 Verisign TLD Packets: IPv6 Alexa Top Host Probing Time Period Metrics Jan 2004 – Jan 2014 A1 Jan 2004 – Jan 2014 A2, T1 Jan 2004 – Jan 2014 A2, T1 Sep 2008 – Dec 2013 R2, U3 Apr 2007 – Jan 2014 N1 P1 Dec 2008 – Dec 2013 U1, U2, U3 Mar 2010 – Dec 2013 Jun 2011 – Dec 2013 N2, N3 N2, N3 Jun 2011 – Dec 2013 Apr 2011 – Dec 2013 R1 Recent Scale ≈18K allocation snapshots (5 daily) 45,271 BGP table snapshots millions of daily global samples daily snapshots of ≈2|,Data,139
com & .net) ≈10 million IPs probed daily ≈33-50% of global Internet traffic; 2013 daily median: 50 terabits/sec (avg,Data,139
| To put the IPv6 allocation data in context, Figure 1 also shows IPv4 prefix allocations over the same period. The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled|,Data,139
| There were less than 30 IPv6 prefixes al- located per month prior to 2007, generally increasing thereafter. In the past several years, we typically find more than 300 prefixes allocated per month, with a high point of 470 prefix allocations in February 2011|,Data,139
| The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled. 1 Overall, we find nearly 69K IPv4 prefix allocations at the beginning of our dataset and just over 136K at the end|,Data,139
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| Table 1 shows the datasets we use in our paper. We use two ICMP surveys taken by USC [12]: IT17ws and IT16ws; IT17ws is the main dataset used in this paper, while we use IT16ws for validation in Section 6|,Data,142
2. We collected VUSC s at our enterprise in order to compare our inferences with network operators as discussed in Section 6,Data,142
| # of Data-Oriented Attacks gives the number of attacks generated by FLOWSTITCH, includ- ing privilege escalation attacks and information leakage attacks. FLOWSTITCH generates 19 data-oriented attacks from 8 vulnerable programs|,Data,144
| Third, this method is not specific to C or C++, and can be applied to any programming language. We collected C++ source of thousands of contestants from the annual international competition “Google Code Jam”|,Data,145
| Finally, we analyze various attributes of programmers, types of programming tasks, and types of features that appear to influence the success of attribution. We identified the most important 928 fea- tures out of 120,000; 44% of them are syntactic, 1% are layout-based and the rest of the features are lexical|,Data,145
|3.1ScalingWecollectedalargerdatasetof1,600programmersfromvariousyears|,Data,145
| ) s y a D n i (    e m T i  7  6  5  4  3  2  1  10  20  30  40  50  60  70  80  90 Time Before Accounts Suspension Number of IP Addresses 2 Motivation: Analysis of Malicious Activ- ity on a Webmail Service We want to understand the way in which cybercrimi- nals abuse accounts on online services, to identify weak points that we could leverage for detection. To this end, we observed the email-sending activity on a large web- mail service|,Data,147
| Following accepted frameworks for qualitative research [18, 30, 35], we focus closely on a small number of participants. We interviewed 15 journalists employed in a range of well-respected journalistic institutions in the United States and France, analyzing these interviews using a grounded theory approach [18, 30]|,Data,146
| 3.1 Datasets We examine 13,345 passwords from four sets created under composition policies ranging from the typical to the currently less common to understand the suc- cess of password-guessing approaches against passwords of different characteristics|,Data,149
| Had we used any major password leak, their analysts would have already been familiar with most or all of the passwords contained in the leak, biasing results. The passwords in these sets were collected using Ama- zon’s Mechanical Turk crowdsourcing service|,Data,149
| The decision for or against pinning is always a trade- off between increasing security and keeping mainte- nance efforts at an acceptable level. In this paper, we present an extensive study on the applicability of pinning for non-browser software by analyzing 639,283 Android apps|,Data,152
| Therefore, we instrument telemetry data from a popular anti-virus software provider. We evaluate the update behaviour of 871,911 unique users from January 2014 to December 2014 and find that only 50% of the users update to a new app version within the first week after release|,Data,152
| Developer View Although pinning is only ap- plicable in relatively few cases, the nominal-actual comparison leaves room for improvement. We there- fore collected feedback from 45 developers of apps for which we would recommend pinning|,Data,152
| Section 4). Altogether we found 20,020,535 calls to network related API calls (cf|,Data,152
| Instability of the routes to the sensor address space can also result in reachability problems, especially given that route flap damping can be triggered during convergence to suppress unstable routes [9]. Using the BGP updates data from RouteViews BGP monitor, we studied the availability of the routes to the sensor blocks in our de- ployment from a large set of ASes|,Data,154
| This section probes these differences using three successively more specific views of traffic to a network of distributed blackhole sensors. The data was recorded over a one month period with SYN responders on TCP port 135, 445, 4444, and 9996 across all sen- sors|,Data,154
|  V. EXPERIMENT RESULTS  In this section, we mainly focus on how our router-to-AS Mapping method and other baseline methods behave on global router-level topology, as discussed above, we use PeeringDB data as ground truth, and apply clustering method on global topology based on CAIDA ITDK project|,Data,155
| It describes the properties that a dataset should have in order to be used for comparison purposes. The dataset used in the paper includes an IRC-based Botnet attack1, but the bot used for the attack was developed by the authors and therefore it may not represent a real botnet behavior|,Data,156
| This dataset may be downloaded with authorization. The Protected Repository for the Defense of Infrastructure Against Cyber Threats (PRE- DICT) indexed three Botnet datasets2 until May 16th, 2013|,Data,156
 None of them are labeled. A custom botnet dataset was created to verify five P2P botnet detection algorithms in Saad et al,Data,156
| Unfortunately, there is only one infected machine for each type of botnet, therefore no synchronization analysis can be done. The Traffic Laboratory at Ericsson Research created a normal dataset that was used in Saad et al|,Data,156
 This is the only normal dataset that is labeled inside the pcap file. A considerable amount of malware traffic in pcap format was published in the Contagio blog9,Data,156
| But since each scenario includes only one infected computer, it should be possible to label them. Another dataset with malware logs and benign logs was collected in NexGinRC (2013)|,Data,156
 Access to this dataset may be granted upon request10. The last dataset analyzed is currently created by the MAWI project described in Cho et al,Data,156
| Methodology and datasets We deployed Paris Traceroute with its Multipath Detection Algorithm (MDA) [29] enabled in 90 PlanetLab nodes. We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]|,Data,158
| We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]. Our dataset contains more than 900 thousand IP-level (multi)routes and 324,313 IP addresses|,Data,158
1 3.1 Address Allocation and BGP Data We analyzed BGP announcements captured by all collectors (24 collectors peering with 184 peers) of the Routeviews [3] and RIPE RIS [52] projects,Data,159
| For each /24 block, we computed the maximum number of peers that saw it reachable at any time within the full observation period of 92 days. To determine which address blocks are available for assignment, we used a dataset compiled by Geoff Hus- ton [23], which merges the extended delegation files from the 5 RIRs [4, 6, 7, 41, 51] with IANA’s published registries [31–36]|,Data,159
| SWITCH. We collected unsampled NetFlow records from all the border routers of SWITCH, a national aca- demic backbone network serving 46 single-homed uni- versities and research institutes in Switzerland [55]|,Data,159
| R-ISP. We collected per-flow logs from a vantage point monitoring traffic of about 25,000 residential ADSL customers of a major European ISP [21]|,Data,159
 UCSD-NT. We collected full packet traces from the /8 network telescope operated at the University of Cal- ifornia San Diego [1],Data,159
| IXP. Our fourth VP is a large European IXP inter- connecting more than 490 networks, exchanging more than 400 PB monthly [5]|,Data,159
|3 Active Measurements ISI. We used the ISI Internet Census dataset it55w- 20130723 [37], obtained by probing the routed IPv4 address space with ICMP echo requests and retaining only those probes that received an ICMP echo reply from an address that matched the one probed (as rec- ommended [38])|,Data,159
| HTTP. We extracted IP addresses from logs of Project Sonar’s HTTP (TCP port 80) scan of the entire IPv4 address space on October 29, 2013 [24]|,Data,159
| Definitions of graph parameters measuring metric tree-likeness of a graph, as well as notions and notations local to a section, are given in appropriate sections. 3 Datasets Our datasets come from different domains like Internet measurements, biological datasets, web graphs, social and collaboration networks|,Data,160
| The experiments were executed as follows. Traces were col- lected by using ICMP, UDP, and TCP Traceroute to probe the paths to a set of 100 destination websites from a source located on the Pennsylvania State University, University Park campus|,Data,161
| For UDP and TCP Traceroute, traces were collected using the default destination port numbers. We also collected traces using other ports and observed similar results|,Data,161
| Realistic Networks Here we compare the merged topologies produced by iTop, MN, and Isomap for realistic topologies. We use the Au- tonomous System (AS) topologies from both the Rocketfuel [20] and the CAIDA [21] projects, which represent IP-level connections between backbone/gateway routers of several ASes from major Internet Service Providers (ISPs) around the globe|,Data,161
| Although the paris-traceroute output of ITDK is more reliable than that of IPlane’s traceroute, the random selection of endpoints implemented by CAIDA hinders the collection of routes between the same vantage- and endpoints. Therefore we used the data of IPlane’s traceroute measurements|,Data,162
| They can also be used for constructing maps of the Internet at the Autonomous Systems level [, ]. In this work we used the CAIDA router-level Internet map from October th,  []|,Data,163
| 3 Table 1: Dataset Description Name BGP Usage AS Geolocation; Detour Detection Date 2016-01 Sources Info RouteViews, RIPE 38,688 RIBS, 416 peers, RIS 30 countries, 55GB Infrastructure IP List AS Geolocation 2016-01 to 2016-03 CAIDA Ark, iPlane, OpenIPMap, RIPE Atlas Measurements 3M Router IPs Infrastructure IPs to AS Mapping Infrastructure IP geolocation 2015-08 CAIDA ITDK, iPlane 6.6M IP to AS mappings AS to IXP Mapping AS Relationship AS Geolocation 2016-01 to 2016-03 Filtering peered paths from detection 2016-01 Traceroute Detour Validation 2016-05-01 IXP websites, PeeringDB, PCH CAIDA AS Relationship RIPE Atlas MaxMind Prefix Geolocation; Detour Validation 2016-01, 2016-03 MaxMind GeoLite City (free and paid) 368 IXP websites crawled 482,657 distinct relationships Used by Netra, 163 traceroutes Paid version used only for geolocating infrastructure IPs and detour validation longest prefix match on the global routing table and map the IP to the AS announcing the longest matching prefix|,Data,164
| As shown in Figure 3, we install LaBrea on a /29 subnetwork and use PlanetLab [9] to probe from multiple vantage points the entire /24 aggre- gate to which the /29 belongs. We scan the /24 network by attempting to establish TCP connections to each IP address in the subnet and capture the packets for further analysis|,Data,165
| • Active IPs in a Subnet: Intuitively, we might ex- pect high-occupancy subnets to be good indicators of pos- sible tarpits. To this end, we initially investigated using a hitlist of probable tarpits as inferred from the /24 subnets with more than 240 responding web hosts in the scans|,Data,165
| To facilitate large-scale scanning and avoid triggering anomaly detectors, degreaser uses permu- tation scanning [7, 12] to pseudo-randomly iterate through the IP address space when probing. Our real-world Internet scan, which probes at least one address in each /24 network in the Internet, discovers 107 different tarpit subnetworks (cid:20)(cid:24)(cid:25) ranging in size from /16 (with up to 216 fake hosts) to /24 (with up to 28 fake hosts)|,Data,165
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
| • Discovering correlations between anomalous traffic types detected with deep inspection techniques and traffic feature entropy variations. • Providing a traffic-type dissection (in-depth and entropy based) of a representative portion of the IBR for three weeks of April, 2012, with a 10-minute time scope|,Data,167
 Following is the summary of information about these data sets:  1. Data set from PREDICT USA [24] which contains traces of a DNS distributed denial of service attack (DDOS),Data,168
  from optical  2. Data set from CAIDA USA [25] which contains internet internet connectivity from 2002 and 2003,Data,168
  3. Data set from our experiment in which a PCAP file is captured from a lab computer which is being used for browsing and software development for the cyber security project,Data,168
| Note that secure computation with penalties can indeed implement each stage of the computation At first glance chaining them together seems to solve the problem However, this is an incorrect approach since there is no way to force players’ to continue to the next stage (in particular to supply inputs to the next stage) Indeed, the only guar- antees that we get from such an approach is that malicious play- ers who learn the output of a stage of computation cannot prevent honest parties from learning the same (except by paying a penalty)|,Non-data,49
| This is not enough to satisfy the notion of dropout tolerance that we desire since a player may dropout in the middle of a hand without getting penalized 4 REALIZING SCD In this section we provide the blueprint of our protocol that realizes secure cash distribution with penalties As we will see soon, our general strategy is to use a protocol that securely realizes a standard reactive functionality (with no coins, and unfair abort), denoted FF , to set things up such that the see-saw transaction mechanism of Section 5 applies to ensure that either the protocol is completed until the very end or all honest parties get compensated|,Non-data,49
| Then, to make the final transfers between parties we will make use of a cash distribution mechanism that we describe later in this section To simplify the presentation of our protocol, we consider the case when there is only a single stage in the computation, ie, ρ = 1 and F = f|,Non-data,49
| Essentially we are dealing with secure function evaluation 199πf r,i (TT r,i ← nmf r,i(TT but with an important difference: namely, aborts anywhere during the computation (ie, not only at the output delivery step) will be penalized The extension to multiple stages is straightforward and we describe it later|,Non-data,49
| First let us set up some notation We say (r, i) > (r(cid:48), i(cid:48)) iff either (1) r > r(cid:48), or (2) r = r(cid:48) and i > i(cid:48) For (r, i), let pred(r, i) be (r(cid:48), i(cid:48)) such that for every (r(cid:48)(cid:48), i(cid:48)(cid:48)) it holds that (r(cid:48)(cid:48), i(cid:48)(cid:48)) < (r, i) iff (r(cid:48)(cid:48), i(cid:48)(cid:48)) ≤ (r(cid:48), i(cid:48)) In other words, pred(r, i) is the “predecessor” of (r, i)|,Non-data,49
| Let πf be a m-round protocol that realizes function f For each i ∈ [n], let xi denote party Pi’s input to f We assume that in each round of the protocol, parties take turns to broadcast their πf message, ie|,Non-data,49
|, the entire protocol transcript is public1 Let TT r,i denote the transcript of protocol πf up until party Pi’s message in the r-th round Let nmf r,i denote the next message function for party Pi in round r The function nmf r,i takes as input the actual input xi, the private randomness of party Pi, denoted ωi, and the πf public transcript seen so far, i|,Non-data,49
|e, TT pred(r,i) In other words, we πf have that TT pred(r,i); (xi, ωi)) (ie|,Non-data,49
|, nmf r,i outputs the entire transcript so far) Also, since all messages are public broadcasts, there exists a function tvπf r,i which checks if a given transcript TTr,i (that contains all messages until and including party Pi’s message in round r) is valid or not By definition, we have πf that tvπf r,i ) = 1 For simplicity and wlog, we assume that all messages (i|,Non-data,49
|e, transcripts) broadcasted are signed by the sending party This implies that the function tv that checks validity of the transcript also checks for the necessary signatures Our strategy is to force each party Pi to deliver its round r message during its turn|,Non-data,49
| That is, first, we want party P1 to ei- πf 1,1 to all parties, or pay a ther reveal its first round message TT πf penalty to all parties 1,1, then P2 can apply πf 1,2 Now we want P2 to ei- nmf 1,2(TT πf 1,2 to all parties or otherwise pay a penalty to all ther reveal TT parties This way, we want to force every party to either make its move or pay a penalty|,Non-data,49
| If we implement this strategy successfully, then we have ensured that each party either learned its output, or is compensated with a penalty (Note that cash distribution at the end still needs to be handled) Designing a transaction mechanism for implementing the above strategy is one of the main contributions in this paper We defer the presentation of the transaction mechanism to Section 5, and devote the rest of this section to handling other issues|,Non-data,49
| Handling multiple valid transcripts In an actual implementation of the above strategy in the F (cid:63) CR-hybrid model, we will have par- ties receive multiple F (cid:63) CR transactions from other parties that can It is possible that be claimed if they produce a valid transcript a malicious party may claim a subset of these F (cid:63) CR transactions using one valid transcript and a different subset using a different valid transcript Such an “attack” may indeed be possible by vary- ing the actual input and private randomness input to the next mes- sage function|,Non-data,49
| Indeed malicious coalitions of k consecutive parties can potentially change the last k messages in the transcript (since they possess the required signing keys to do this) In applications to poker, a player (admittedly a novice) may leak an “expression of surprise” upon seeing a (malicious) player’s “confirmed” move, only to see this move modified by the next (malicious) player In any case, we consider such attacks as violations, and must compen- sate the honest parties upon such violations Note that a “proof” of πf 1,1; (x2, ω2)) to obtain TT If P1 revealed TT 1That is, all messages exchanged in the protocol are simply broad- casts|,Non-data,49
| Protocols secure against dishonest majority typically fall un- der this category For an explicity example, see the main construc- tion in [21, 3] See also the discussion in [22] r,i (TTi) = tvπf any such violation is readily obtained from the inconsistent tran- scripts|,Non-data,49
| We ask each party Pi to make F (cid:63) CR transactions to ev- ery other party that can be claimed by revealing a proof of viola- i = (TTi, TT(cid:48) i) such that for some tion: ie, pair of transcripts T vio r ∈ [m], it holds that tvπf i) and yet TTi (cid:54)= TT(cid:48) r,i (TT(cid:48) i Since transcripts are signed, a proof of violation against an honest party can never be obtained (except with negligible probability)|,Non-data,49
| P2 to denote an Following the notation in [10], we use P1 F (cid:63) CR transaction for coins(q) made by P1 that can be claimed by P2 if P2 produces witness T within time τ Thus to safeguard against violations we ask each Pi to make the following set of transactions for each j ∈ [n] \ {i}: T−−→ q,τ Pi −−−−−−−−−−−−−−→ i T vio n·q,τ Pj (Txvio i,j) Here τ is such that the transaction can be claimed until the end of the protocol Note that the transaction if claimed will transfer coins(n· q) from the violating party Pi This is because, upon such a violation an honest Pj will be asked to abort the rest of the proto- col and directly claim Txvio i,j where Pi is the violating party|,Non-data,49
| Since Pj aborts the rest of the protocol, it may be forced to pay a to- tal compensation of coins((n− 1)q) to the remaining parties Thus upon any violation by malicious parties, we ensure that each honest Pj will still be coins(q) up at the end of the protocol execution Handling multiple stages of computation At an abstract level, adding stages to a reactive computation merely amounts to adding more “next messages” to the transcript|,Non-data,49
| Indeed an intermediate stage of computation simply begins by reconstructing the current state, and then performing the computation on this state and the current inputs Thus it is trivial to merge multiple stages of com- putation into a single stage—simply append the protocol messages of the multiple stages together Since our strategy works by keep- ing track of the protocol transcript, it ensures that an abort at any round/stage of a multi-stage computation will be penalized Handling the cash distribution|,Non-data,49
| To do this, we first need parties to make deposits at the beginning of the protocol that will allow them to claim their returns at the end of the protocol Note that parties might have to transfer an arbitrary amount of coins between themselves Adopting an idea from [9], we ask parties to commit to money transfers for all powers of 2 up to the maximum possible n), and for each i ∈ [n], let sum In more detail, let d(cid:63) = (d(cid:63) i )(cid:101)|,Non-data,49
| The high level idea is to have, for every ordered mi = (cid:100)log(d(cid:63) pair (i, j) with i, j ∈ [n] and i (cid:54)= j, and for each k ∈ [mi], party Pi make an F (cid:63) CR transaction as follows: 1,    , d(cid:63) Pi −−−−−−−−−−−−−−−−−−−→ i,j,k T fin 2k,τfin Pj (Txfin i,j,k) Given these transactions, it is easy to see that Pj can claim any arbitrary amount of coins from the rest of the parties|,Non-data,49
| Also, we need to ensure that Pj obtains exactly the correct amount of coins That is, suppose the output of the reactive computation is zρ = (z, z(cid:63)) n), then we want Pj to obtain coins(z(cid:63) with z(cid:63) = (z(cid:63) j ) at 1 ,   |,Non-data,49
| , z(cid:63) the end of the protocol In other words, we need to provide Pj with the right subset of {T fin i,j,k}i,k that will allow it to claim exactly j ) This subset will obviously need to be transferred in the coins(z(cid:63) last computation stage fρ To make sure the deposits are made at the very beginning, the parties need to know the corresponding ver- ification circuits φfin i,j,k at the beginning as well|,Non-data,49
| To design the ver- ification circuits, we employ honest binding commitments [22, 10] (See also Appendix A) Let (S, R) be a honest binding commitment scheme (Note that such commitment schemes can be realized by cryptographic hash functions in the programmable random oracle 200model) More precisely we require parties to execute a standard, secure-with-abort MPC protocol at the very beginning that for all i ∈ [n], j ∈ [n] \ {i}, k ∈ [mi]: • chooses T fin i,j,k ← {0, 1}λ and ωfin • computes comfin i,j,k ← S(1λ, T fin • n-out-of-n secret shares each (T fin • outputs comfin i,j,k ← {0, 1}λ at random; i,j,k, ωfin i,j,k and (cid:96)-th share of (T fin i,j,k); i,j,k, ωfin i,j,k) to P(cid:96)|,Non-data,49
| i,j,k, ωfin i,j,k); i,j,k to make the transaction Txfin The secret sharing is done so that parties can reconstruct the i,j,k values (saved as part of the state) at the beginning of the T fin last stage of the computation Note that now parties possess the verification circuits φfin i,j,k Next we describe the modification to the last stage Instead of realizing fρ in the last stage, parties realize f(cid:48) • computes z(cid:63) = (z(cid:63) • computes A = g(d(cid:63), z(cid:63)) (cf|,Non-data,49
| Observation 1), let ai,j denote the i,j,mi be the binary n) by invoking fρ; (i, j)-th entry of matrix A, and let b(cid:63) representation of ai,j; i,j,1, , b(cid:63) 1 , |,Non-data,49
|   , z(cid:63) ρ which: • for all i ∈ [n], j ∈ [n] \ {i}, k ∈ [mi]: i,j,k (from stateρ−1); reconstructs T fin outputs T fin i,j,k if b(cid:63) i,j,k = 1, else outputs 0 Given the above it is easy to see that the set of transactions i,j,k} transfer the right amounts of money according to the {Txfin output z(cid:63)|,Non-data,49
| Next we show how to design the see-saw transaction mechanism that implements our strategy of forcing parties to send the next message of the protocol realizing FF  5 SEE-SAW MECHANISM Recall that our goal is to force parties to reveal their next message of say a m-round protocol for computing function f, one-by-one in a round-robin fashion round after round That is, party P1 first com- πf 1,1, then party P2 computes putes and reveals “token” T1,1 = TT πf 1,2, and so on until party (using T1,1) and reveals token T1,2 = TT πf 1,n|,Non-data,49
| (Note that the order Pn computes and reveals token T1,n = TT of revelations is important) Following this, parties move on to the next round, and so on and so forth until at the end Pn reveals token πf m,n What we need is a transaction mechanism that Tm,n = TT incentivizes parties to follow the above sequence of reveals More precisely for every i ∈ [n], r ∈ [m], we force Pi to pay a penalty to all other parties if (a) all parties P1, |,Non-data,49
|   , Pn revealed their tokens until round r− 1; and (b) in round r parties P1,  |,Non-data,49
|  , Pi−1, revealed their tokens; and (c) in round r party Pi did not reveal Tr,i Towards solving this problem, we let parties participate in a ini- tial deposit phase where parties make some sequence of transac- tions We are lenient towards any aborts during this initial deposit phase, i|,Non-data,49
|e, we do not penalize any party for an abort during this de- posit phase However once this deposit phase ends, then we enter the reveal phase Any party that deviates during its turn in any of the m rounds in the reveal phase has to pay a penalty to all the re- maining parties|,Non-data,49
| Contrast this with the “ladder mechanism” of [10], where a party that aborts without learning the final output may not necessarily pay penalties to all parties Honest parties’ strategy As mentioned earlier, our protocol will be an ordered sequence of claim-or-refund transactions In an hon- est execution of our protocol, all deposits will be made first before any of them is claimed|,Non-data,49
| Also, the sequence deposits will be claimed in the reverse order in which they are made Note that a malicious party may abort the protocol either (1) by not making a deposit it was supposed to make, or (2) by not claiming a deposit it could have claimed The following two rules of thumb may be kept in mind to understand how honest parties behave in the event of such aborts 1|,Non-data,49
| When it’s an honest party’s turn to make a deposit, it makes the deposit if and only if all the deposits that were supposed to made before its deposit were made That is, if a malicious party does not make a deposit during its turn, then no honest party makes any subsequent deposit in the protocol 2 When it’s an honest party’s turn to make a claim, it makes the claim if it possesses all the witnesses necessary for making the claim|,Non-data,49
| That is, an honest party may go ahead and claim a deposit even if (1) some deposits were not made, and (2) some claims were not made Two simplifying assumptions The first is that our constructions will try to penalize deviations of party Pi in round r only when (r, i) (cid:54)= (1, 1) Later in this section, we show how to handle the “bootstrapping” step of forcing P1 to start the protocol|,Non-data,49
| The second is that we assume parties can use only unique witnesses to claim F (cid:63) CR transactions In our constructions, the witnesses correspond to protocol transcripts, and we already discussed in the previous sec- tion how to handle the case when parties broadcast multiple valid transcripts We construct our final protocol in a step-by-step manner We start with n = 2 and m = 1|,Non-data,49
| Single-round two-party case Since we are in the single-round case we use Ti to denote the token T1,i Consider the following sequence of deposit transactions where τ2 > τ1: P1 T1∧T2 q,τ2 −−−−−−−−−−−→ −−−−−−−−→ P2 T1 q,τ1 P2 P1 (Tx2) (Tx1) Note that the verification circuits for these transactions are sim- ply the corresponding transcript checking functions tvπf r,i , and are already known to the parties, and thus the deposits can be made Once all the deposits are made, the deposits are claimed in reverse|,Non-data,49
| That is, P1 first claims Tx1 Using T1 revealed by P1, party P2 is able to claim Tx2 We first consider aborts during the initial deposit phase If P1 aborts without making Tx2, then clearly no money changes hands and we are good|,Non-data,49
| Now if P2 aborts without making Tx1, then note that P1 does not enter the reveal phase, and so does not reveal T1 This in turn ensures that P2 will not be able to claim Tx2, and thus no money changes hands, and we are good These attacks imply that we do not even get past the initial deposit phase (meaning that we are not required to penalize any party) Next, we consider aborts during the reveal phase|,Non-data,49
| Recall that once we enter the reveal phase, then we must penalize P2 if P1 revealed T1 but P2 did not reveal T2 First suppose P1 aborts, ie, does not claim Tx1|,Non-data,49
| Then note that Tx1 gets refunded back to P2, and no party is penalized Note that if P1 does claim Tx1, then P2 is able to claim Tx2, and the parties even out as well as obtain both T1 and T2 Next, we consider the case when P2 aborts the protocol, ie|,Non-data,49
|, does not claim Tx2 In this case, Tx2 gets refunded back to P1 Also, P1 would have already gained coins(q) after claiming Tx1 and hence is compensated at the end of the protocol We use the following notation to simplify the presentation: for r ∈ [m], let TTr = ∧r s=1(Ts,1 ∧ ··· ∧ Ts,n), and for i ∈ [n] and r ∈ [m], let TTr,i = TTr−1 ∧ (∧i j=1Tr,j)|,Non-data,49
| (Here TT stands for “transcript”) Also, let “(r(cid:48), i(cid:48)) > (r, i)” if either (1) r(cid:48) > r, or (2) r(cid:48) = r and i(cid:48) > i 201ROOF DEPOSIT −−−−−−−−−−−→ TTm,2 P2 SEE-SAW DEPOSITS|,Non-data,49
| For r = m − 1 to 1: q,τm,2 P1 P2 TTr+1,1 −−−−−−−−−−−−→ −−−−−−−−−−→ 2q,τr+1,1 TTr,2 P1 P1 P2 2q,τr,2 (Txm,2) (Txr+1,1) (Txr,2) FLOOR DEPOSIT P2 −−−−−−−−−−→ TT1,1 q,τ1,1 P1 (Tx1,1) Figure 3: Multi-round two party see-saw mechanism CR transactions Multi-round two-party see-saw mechanism|,Non-data,49
| The sequence of transactions is shown in Figure 3 where τr(cid:48),i(cid:48) > τr,i iff (r(cid:48), i(cid:48)) > (r, i) As in the single-round case, the reveals are made in re- verse: namely, P1 first claims Tx1,1 Using TT1,1 = T1,1 re- vealed by P1, party P2 is now able to claim Tx1,2 by revealing TT1,2 = T1,1 ∧ T1,2 Likewise parties P1 and P2 take turns claim- ing each others’ F (cid:63) We first consider aborts during the initial deposit phase|,Non-data,49
| Sup- pose Pi aborts without making Txr,j for j (cid:54)= i and some r First, this ensures that (1) Pj does not make Txr(cid:48),i for (r(cid:48), i) < (r, j), and (2) Pj will never reveal Tr,j (since Tr,j needs to be revealed only to claim Txr(cid:48),i(cid:48) for (r(cid:48), i(cid:48)) ≥ (r, j)), and (3) no party can claim Txr(cid:48),i(cid:48) for (r(cid:48), i(cid:48)) ≥ (r, j) (since Tr,j is necessary to claim Txr(cid:48),i(cid:48)), and (4) all the deposits Txr(cid:48),i(cid:48) for (r(cid:48), i(cid:48)) > (r, i) (ie, those that were made so far) will get automatically refunded after τr(cid:48),i(cid:48) (since Tr,j is need to claim this, but is never revealed by Pj)|,Non-data,49
| Thus in such a situation neither party stands to gain or lose coins Next, we discuss aborts by parties in the reveal phase First suppose P1 aborts without claiming Tx1,1 In this case, dishonest P1 will never obtain T1,2|,Non-data,49
| This is because P2 would not have obtained T1,1 from P1, and hence cannot claim Tx1,2 Now note that all deposits Txr,i for (r, i) ≥ (1, 2) require T1,2, and hence none of these deposits can be claimed Thus we have that neither party stands to lose or gain coins Recall that this corre- sponds to the case where the reveal phase hasn’t started yet, and so parties don’t get penalized yet|,Non-data,49
| Recall that once the reveal phase starts, we must penalize every party that did not reveal its token during its turn Suppose P1 does claim Tx1,1 (ie, the reveal phase has started)|,Non-data,49
| Then in this case, P2 is down coins(q) while P1 is up coins(q) If P2 aborts at this stage, then essentially P2 has compensated P1 with coins(q) On the other hand if P2 claims Tx1,2, then note that it gets coins(2q) from that claim Thus, it is now coins(q) up while P1 is down coins(q)|,Non-data,49
| It is easy to see that as the remaining claims are made, parties take turns going up and down coins(q) (hence the name “see-saw”) Thus we have the property that whenever a party Pi claims Txr,i (except for (r, i) = (m, 2)), it gains coins(q) while the other party loses coins(q) This incentivizes the other party to go ahead and claim F (cid:63) CR transaction immediately above Txr,i, say Txr(cid:48),i(cid:48) Indeed if the other party does not make the claim, then we have that the honest party (i|,Non-data,49
|e, Pi) is compensated with coins(q) at the end of the protocol This is because if Txr(cid:48),i(cid:48) is not claimed, then either (1) (r(cid:48), i(cid:48)) = (m, 2), and this case Pi does not lose coins from this transaction, and simply ends the protocol with coins(q) as compensation, or (2) (r(cid:48), i(cid:48)) (cid:54)= (m, 2), in which case Pi will never reveal Tr+1,i thus making it impossible for any Txr(cid:48)(cid:48),i(cid:48)(cid:48) to be claimed for any (r(cid:48)(cid:48), i(cid:48)(cid:48)) ≥ (r + 1, i), essentially en- suring that no further money transfers happen, and that Pi can end the protocol with coins(q) as compensation Finally, in an honest execution, when P2 claims the last transaction Txm,2 it gets only coins(q) from that claim, and thus in this case both parties even out|,Non-data,49
 Multiparty locked ladder mechanism Generalizing the two party solution is nontrivial To better understand the complications we will first look a naïve 3-party protocol Naïve single-round 3-party case,Non-data,49
| The high level idea is to try and ensure that all parties are already compensated by Pi just before the step where party Pi is required to reveal Ti Then after Pi is supposed to reveal Ti, we get the compensation that was delivered to the parties back to Pi (Observe that we do not need to apply the above strategy for i = 1) Consider the following implementation of the above strategy: ROOF DEPOSITS|,Non-data,49
| For j ∈ {1, 2}: Pj −−−−−−−−−−−−−−→ TT3 q,τ3,j THIRD STAGE DEPOSITS P3 −−−−−−−−−−−−−−→ TT2 3q,τ2,3 SECOND STAGE DEPOSITS P2 −−−−−−−−−−−−−−→ TT1 q,τ3,2 FIRST STAGE DEPOSITS P2 −−−−−−−−−−−−−−→ TT1 q,τ1,2 P3 P2 P3 P1 To see why the above may be a faithful implementation of the strategy, note that the end of the first two deposit stages, P2 has already compensated both P1 and P3 with coins(q), i|,Non-data,49
|e, P2 has lost coins(2q) Then, in the third stage, it claims coins(3q) from P3 by revealing T2 This is effectively equivalent to P3 compensating P2 with coins(q), and learning T1 and T2|,Non-data,49
| That is, at the end of the third stage, it is P3’s turn to reveal T3, and both P1 and P2 have already been compensated with coins(q) by P3 Then, in the roof stage, P3 claims back coins(q) from both P1 and P2 by revealing T3 (along with T1, T2), and thus all parties even out The problem with the above scheme is that it is not resistant to a “coalition attack” Consider a malicious P2 that does not make the first and second stage deposits|,Non-data,49
| Recall that the roof deposits and the third stage deposits have already been made Now a malicious coalition of P1 and P2 possesses both T1 and T2, ie, TT2 and can claim the third stage deposit of coins(3q)|,Non-data,49
| While P3 can use TT2 to claim the roof deposits, and learn all the tokens, it does so at an expense of coins(q) (ie, it claims coins(2q) from the roof deposits but has lost coins(3q) in the third stage deposits) This is clearly an undesirable situation as the honest party has lost coins(q)|,Non-data,49
| To avoid the “coalition attack,” we now introduce two new ideas that will help us construct our multiparty protocol The first idea is a locking mechanism that prevents the collusion attack that we just described on our naïve 3-party protocol The second is an in- tegration of the first idea with the ladder mechanism of [10] which allows transitions between different stages of the protocol We ex- plain these two ideas below|,Non-data,49
| Locking mechanism Recall that the high level idea in our naïve 3-party protocol was to ensure that all parties are already compen- sated by Pi just before the step where party Pi is required to reveal 202Ti Then after Pi reveals Ti, we get the compensation that was delivered to the parties back to Pi That is, we have a set of trans- actions S+i where Pi claims coins(q) each from a set of parties, followed by a set of transactions S−i where the same set of parties each claim coins(q) from Pi|,Non-data,49
| (Recall that transactions in S−i are claimed first, which forces Pi to reveal Ti and claim transactions in S+i) The general form of the attack on the naïve protocol is that Pi aborts when it has to make transactions in S−i Then colluding with parties P1,  |,Non-data,49
|  , Pi−1, party Pi starts claiming transactions in S+i This allows Pi to unfairly obtain additional coins from parties Pi+1,  |,Non-data,49
|  , Pn while ensuring that they are unable to claim deposits in S−i The main idea that we use to prevent such attacks is to “lock” transactions in S+i such that they can be “unlocked” and claimed only if the transactions in S−i were already claimed To do this, we make use of “dummy tokens” Ui,j that will be used by Pj (and known only to Pj) to lock transactions in S+i|,Non-data,49
| (We will generate these dummy token via an initial MPC protocol A similar strategy is used to “bootstrap” the computation, and we defer details until then) More concretely, to claim the transaction from Pj in S+i, party Pi needs to produce Ui,j in addition to TTi Then to enable an honest Pi to claim transactions in S+i, we let party Pj to claim transactions in S−i only if it produces Ui,j in addition to TTi−1|,Non-data,49
| Ladder mechanism While the above locking mechanism deals with aborts in the deposit phase, we must obviously be wary of aborts in the reveal phase Indeed, it turns out that the locking mecha- nism alone does not suffice To see why, watch what happens when it is (honest) Pi’s turn to reveal the witness, and yet none of the parties claim transactions in S−i thus disabling Pi from revealing its token|,Non-data,49
| In effect, all parties other than Pi have aborted, and yet Pi does not receive any compensation, thus violating our require- ments For a more concrete example of what we refer to as the “locked-out attack,” consider the naïve 3-party protocol enhanced with the locking mechanism (ie, both second stage as well as the third stage deposits are locked)|,Non-data,49
| Now P1 claims the first stage de- posit, and after that P3 simply aborts without claiming the sec- ond stage locked transaction This will disallow P2 from claiming the third stage deposit as it remains locked Thus, essentially P3 aborted the protocol, and yet P2 does not gain coins(q) (in fact, it loses coins(q) here) The above attack naturally leads us to include a F (cid:63) CR transaction to Pi that can be claimed just by revealing TTi, i|,Non-data,49
|e, it is essen- tially an unlocked transaction What the above would ensure is that Pi will never be stranded in a situation where it wishes to reveal its token, and yet is unable to claim any transactions While the above is true, unfortunately if we include unlocked F (cid:63) CR transac- tions from each Pj to Pi (i|,Non-data,49
|e, those that can be claimed just using TTi), then we have negated the locking mechanism, and are back to square one Thus, what we want to do is to give a chance to Pi to avoid the “locked-out attack” while at the same time preventing the “coalition attack” To do this, we let only Pi+1 make an unlocked F (cid:63) CR transaction to Pi that can be claimed by revealing just TTi|,Non-data,49
| In some sense, this breaks the symmetry of the protocol, but it also gives us a chance to make use of the ladder mechanism of [10] That is, following [10], we let Pi+1 make an unlocked F (cid:63) CR trans- action to Pi for coins(i · q) that can be claimed by revealing TTi We present our protocol in Figure 4 At a high level, the protocol proceeds by getting a roof deposit from each of the parties to Pn that can be claimed if Pn produces TTn|,Non-data,49
| Next, we enter the ladder deposits for each i = n − 1 down to 2 (note the order is important), where party Pi receives a de- posit that is locked with token Ui,j from each party Pj for j > i ROOF DEPOSITS For each j ∈ [n − 1]: Pj −−−−−−−−−−−−−−−−−→ TTn q,τ2n−2 Pn LADDER DEPOSITS For i = n − 1 down to 2: • Rung unlock: For j = n down to i + 1: Pj −−−−−−−−−−−−−−−−−→ TTi∧Ui,j q,τ2i−1 Pi • Rung climb: Pi+1 −−−−−−−−−−−−−−−−−→ TTi i·q,τ2i−2 Pi • Rung lock: For each j = n down to i + 1: FOOT DEPOSIT Pi P2 −−−−−−−−−−−−−−−−−−−→ TTi−1∧Ui,j q,τ2i−2 Pj −−−−−−−−−−−−−−−−−→ TT1 q,τ1 P1 Figure 4: Locked ladder mechanism|,Non-data,49
| (these correspond to S+i), an unlocked deposit from Pi+1 that can be claimed if Pi reveals TTi, and makes deposits to Pj for j > i that are locked with Ui,j (these correspond to S−i) Note that de- posits in S−i can be claimed with TTi−1 (in addition to Ui,j), and that deposits in S+i can be claimed with TTi (in addition to Ui,j) Finally, we have the foot deposit (essentially foot of the ladder that involves P1) where P2 makes a deposit to P1 that can be claimed with T1 As usual these deposits will be claimed in reverse|,Non-data,49
| That is, P1 first claims the floor deposit by revealing T1 Then parties enter the ladder reveal phase As in [10], the parties metaphorically climb the ladder as they take turns claiming the ladder deposits The dif- ference from [10] is that before climbing a rung of the ladder, par- ties first do a “rung lock” step, and after they climb the rung, they perform a “rung unlock” step|,Non-data,49
| Hence, while the protocol is being executed, Pi first pays the parties above it (who haven’t “played” yet), but Pi will then immediately be able to “play” by extending TTi−1 and thereby reclaim these coins that it paid, thus avoiding the locked-out and coalition attacks As in the ladder mechanism of [10], once the i-th ladder deposit is claimed, parties P1 through Pi become “inactive” in the sense that they no longer claim any deposits and nor are any of their lad- der deposits remain unclaimed (In fact their only unclaimed de- posits are those that are part of the roof deposits) It is easy to see that the “inactive” parties are always coins(q) up after the i-th lad- der rung unlock deposits are claimed, and that they remain coins(q) up until the beginning of roof claims|,Non-data,49
| As it turns out, the lock and ladder mechanisms are sufficient to deal with aborts in the deposit and reveal phases, respectively Multiparty see-saw mechanism Our idea is to mimic the two- party see-saw mechanism That is, all we need to do is to ensure that the end of each of the m rounds, party P1 has already com- 203pensated coins(q) to every other party, and is thus incentivized to send the first token for the next round|,Non-data,49
| This is quite straightfor- ward to implement For every round we invoke an instance of the single-round locked ladder mechanism (with the transcript verifi- cation circuits corresponding to the round of the protocol) These instances are invoked sequentially, and thus the timelocks have to be set accordingly Recall that at the end of the reveal phase of every instance of the locked ladder mechanism, parties have either already been com- pensated, or they learn all the protocol messages for the round, and are all evened out w|,Non-data,49
|rt deposits Then to apply the see-saw idea, we need to introduce new “chain” deposits between successive in- stances of the locked ladder mechanism|,Non-data,49
| CHAIN DEPOSITS • For j = 2 to n: Pj −−−−−−−−−−−−−−−−−→ TTr+1,1 q,τr+1,1 P1 j,1 ) (Txchain • For j = 2 to n: P1 −−−−−−−−−−−−−−−→ TTr,n q,τr+1,1 Pj (Txchain 1,j ) Remark Note that solving the single-round multiparty case yields a solution to the multi-round multiparty case as well To see why, let us denote the problem for the m-round n-party case by LLm,n|,Non-data,49
| Then the problem LLm,n is obtained by simply “folding” LL1,nm That is, for i ∈ [nm], interpret Pi’s move in LL1,nm as Pi mod n’s move in the ((cid:98)i/n(cid:99) + 1)-th round of LLm,n The key observa- tion behind why this transformation is secure is that any protocol π solving LL1,nm is resistant to malicious coalitions of any subset of nm parties, and therefore, the “folded” m-round n-party protocol obtained from π is also resistant to any coalitions of subset of the n parties Since the protocol in Figure 4 solves the single-round multiparty case, we trivially obtain the multi-round multiparty so- lution|,Non-data,49
| However note that the efficiency of such a solution obtained for the m-round two-party case ie, for LLm,2 by using the 2m-party locked ladder mechanism, has worse efficiency than the two-party see-saw protocol from Figure 3 While the see-saw protocol re- quires parties to each deposit coins(2mq), the amount deposited in the ladder grows as O(m2q)|,Non-data,49
| Bootstrapping Finally we focus on how to incentivize P1 to start the protocol (ie, reveal T1,1) or otherwise pay penalty|,Non-data,49
| To do this, we make use of “dummy tokens” {U1,j}j∈{2,|,Non-data,49
|n} These dummy tokens are obtained by the parties via an initial secure computation step In more detail, for all j ∈ [n] \ {1}, the secure computation protocol: • chooses U1,j ← {0, 1}λ and ωboot • computes comboot • outputs comboot where S is the sender algorithm of a honest binding commitment scheme Note that comboot 1,j is computed in order to allow parties to 1,j and Txboot generate the verification circuit for transaction Txboot described below|,Non-data,49
| Also, we stress that for j (cid:54)= 1, the dummy token j,1 U1,j is unknown to P1; it only knows the corresponding commit- ment comboot 1,j  (We note that the above MPC step can be combined with the MPC step for handling the cash distribution step (cf Sec- tion 4) as well as for generating the dummy tokens {Ui,j} in the 1,j ← S(1λ, U1,j, ωboot 1,j ); 1,j to all parties and (U1,j, ωboot 1,j ← {0, 1}λ at random; 1,j ) to Pj, lock mechanism) Consider the following set of deposit transac- tions where τ1 > τ0: BOOTSTRAP DEPOSITS|,Non-data,49
| • For j = 2 to n: Pj −−−−−−−−−−−−−−−−−→ T1∧U1,j q,τ1 P1 (Txboot j,1 ) • For j = 2 to n: P1 −−−−−−−−−−−−−−−→ U1,j q,τ0 Pj (Txboot 1,j ) That is, first each Pj makes a deposit Txboot j,1 to P1, and then P1 to each Pj Then in the reveal phase, the makes deposits Txboot 1,j claims are made in reverse: each Pj first claims Txboot 1,j using the dummy token U1,j Now P1 learns U1,j, and since it already knows T1, it can go ahead and claim each Txboot j,1  More importantly, note that once the bootstrap deposits are made, an honest Pj will always claim Txboot 1,j , and thus will be coins(q) up|,Non-data,49
| Thus the onus is on P1 to deliver the first token (and to reclaim its coins(q)), failing which it effectively pays a penalty coins(q) to each honest party The bootstrap deposits will be the last deposits to be made in the initial deposit phase, and will be the first deposits to be claimed in the reveal phase We are now ready to state our main theorem Since the ideal oblivious transfer primitive FOT is suffficient to obtain a common random string, we can then apply e|,Non-data,49
|g, [21] to obtain: CR)-hybrid model F,d(cid:63) in the (FOT,F (cid:63) Theorem 2 Let (F, d(cid:63)) be a bounded zero-sum reactive distribu- tion as in Definition 2|,Non-data,49
| Then assuming the existence of enhanced trapdoor permutations, there exists a protocol that SCC-realizes (cf Definition 1) F (cid:63) Proof sketch The main idea behind the proof is that the witnesses used to claim F (cid:63) CR transactions are simply successive messages of a secure computation protocol πF that realizes that standard reac- tive functionality FF  Since πF is secure by definition, we have that the computation also proceeds securely|,Non-data,49
| To do the simula- tion, we make use of (1) the simulator for πF , (2) the simulator for initial MPC step (alternatively access to ideal unfair function- ality realized in the FOT-hybrid model) that generates the dummy tokens for the lock mechanism, the bootstrap deposits (ie, the val- ues {Ui,j}), and also for the cash deposits at the end (ie|,Non-data,49
|, the val- ues {T fin i,j,k}), and (3) the simulator algorithms for honest-binding commitments Simulating the coins part of the protocol is more in- volved but closely follows the simulation of the ladder mechanism in [10] We defer further details to the full version 6|,Non-data,49
| EFFICIENT POKER PROTOCOL In this section we describe an optimized protocol for Texas hold ‘em poker that avoids non-black-box use of a secure computation protocol Our key observation is that in each stage, only player Pi has an input in a stage of the computation that corresponds to player Pi’s r-th round move Let (S, R) be a non-interactive honest binding commitment (cf Definition 3, Appendix A)|,Non-data,49
| Parties run a secure computation protocol that does the following: • selects hands hi uniformly at random for each party Pi, as well as the five community cards y1,    , y5; • performs an n-out-of-n secret sharing of each hand hi to obtain {hi,j}j∈[n], and a n-out-of-n secret sharing of each of the five cards yk to obtain {yk,j}j∈[n]; 204• applies the sender algorithm of an honest-binding commitment i,j and set i,j to secret share hi,j to obtain comh i,j and Tokenh i,j = (hi,j, ωh i,j); • applies the sender algorithm of an honest-binding commitment i,j and set i,j to secret share yi,j to obtain comy using random ωh Tagh i,j = comh using random ωy Tagy i,j = comy i,j and Tokeny i,j, Tagy i,j = (yi,j, ωy i,j); i,j}i,j∈[n]; and • sets AllTags = {Tagh • delivers AllTags,{Tokenh i,j, Tokeny i,j}j∈[n] to each Pi|,Non-data,49
| Note that at the end of this step, none of these cards are deliv- ered to the parties Instead all of these cards (including each party’s hands) are simply secret shared among the parties In addition, par- ties also receive (honest-binding) commitments on all the shares, and the decommitments to the shares held by them These are given so that parties can later verify if each party indeed reveals the correct shares by sending the decommitments corresponding to the public commimtments|,Non-data,49
| Once this is done, parties make a series of deposits as in the see-saw (alternatively, locked ladder) mechanism We defer the description of the φi,r for these deposits, and first focus on the structure of the protocol Each party Pj is first required to reveal Hj = {hi,j}i∈[n]\{j}, ie|,Non-data,49
|, the secret shares of other party’s hands This is so that each party learns its private hands Here we will make use of the see-saw mechanism to ensure that each party Pj either reveals Hj or pays a penalty to all other parties The ver- ification circuits for the F (cid:63) CR transactions will depend on comh i,j generated in the initial secure computation step|,Non-data,49
| Next parties enter a round of (pre-flop) betting Here we assume a bound on maximum number of stages of betting (this is so that we can ensure that parties make all the necessary F (cid:63) CR deposits in the see-saw mechanism) To place a bet, party Pi simply sends the entire transcript of bets made so far in this hand along with its new bet Note that each party signs its bet when it makes one, and thus when parties send a transcript containing the bets, they must also contain the necessary signatures|,Non-data,49
| We assume that there is a well-defined function tvr,i (tv stands for “transcript validity”) that takes the transcript of the poker game so far (including bets made so far, and the new bet made by party Pi in round r), and verifies if it is a valid bet Note that a bet bi made by Pi simply specifies the additional amount of coins it is willing to bet during its turn in pre-flop betting round (Similarly to fold, Pi simply sends a signed “fold” message) We wish to stress that no actual coins related to the bet amounts are transferred in this phase|,Non-data,49
| (These will all be transferred at the very end of the protocol) Now note that once this round of betting ends, the flops needs to be revealed to all the parties We adopt the same strategy that we used to reveal each party’s hands That is, each party Pj is required j = {y1,j, y2,j, y3,j}, i|,Non-data,49
|e, the secret shares of the flop to reveal Y 1 Once again we will make use of the see-saw mechanism to ensure that each party Pj either reveals Y 1 j or pays a penalty to all other parties Two additional rounds of betting take place before revealing the turn and the river|,Non-data,49
| These are handled exactly like the pre-flop bet- ting Once all the community cards are revealed, parties that wish to claim the pot start revealing their cards That is, parties execute an additional stage where they take turns to reveal their cards, ie|,Non-data,49
|, reveal their share hi,i (which reveals their hand) Once all parties complete the showdown round, and the entire transcript TTm,n is available, then the pot winner can be determined Note that we run only one MPC at the very beginning, and AllTags generated in this step is sufficient to design the verification circuits for all F (cid:63) CR deposits in the see-saw mechanism Since the see-saw mechanism now applies, any party that aborts the protocol before the winner has been determined will pay a penalty to all other parties|,Non-data,49
| The above description turns out to be sufficient to realize “mental poker” [1], but is not sufficient to realize standard poker (ie, poker with money) This is because we still haven’t let the winner(s) take the pot|,Non-data,49
| Next we describe the cash distribution stage Let i )(cid:101) d(cid:63) = (d(cid:63) As in Section 4, for every ordered pair (i, j) with i, j ∈ [n] and i (cid:54)= j, and for each k ∈ [mi], we let Pi make an F (cid:63) CR transaction as follows (we slightly abuse the F (cid:63) CR notation and use the verification circuit instead of the verifying witness): n), and for each i ∈ [n], let mi = (cid:100)log(d(cid:63) 1,  |,Non-data,49
|  , d(cid:63) −−−−−−−−−−−−−−−−−−−→ i,j,k φfin Pj Pi (Txfin i,j,k) 2k,τ0 i,j,k takes TTm,n as input and: where verification circuit φfin • outputs 0 and terminates if tvm,n(TTm,n) = 0; • computes z(cid:63) = (z(cid:63) 1 ,   |,Non-data,49
| , z(cid:63) i,j,1, , b(cid:63) n) using TTm,n, where z(cid:63) i represents the amount which party Pi is supposed to get at the end of the protocol; • computes A = g(d(cid:63), z(cid:63)) (cf|,Non-data,49
| Observation 1), lets ai,j denote i,j,mi be the the (i, j)-th entry of matrix A, and lets b(cid:63) binary representation of ai,j; i,j,k = 1, else outputs 0 • outputs 1 if b(cid:63) Efficiency Note that each party Pi makes (n − 1) · mi calls to CR and deposits a total of (n − 1) · d(cid:63) F (cid:63) i coins For implementing the see-saw mechanism we require O(n2m) calls to F (cid:63) CR and each party to make a maximum deposit of O(nm) where m represents the bound on the maximum number of betting rounds in a hand|,Non-data,49
| Note that we can preprocess both the secure computation, as well as the initial deposit phase (thus managing the long waiting times for transaction confirmation offline) Other than this, note that the messages in our secure poker protocol are mostly signed messages indicating the player’s move, and thus not very different from the messages in an insecure poker protocol 7 CONCLUSIONS In this paper, we presented formal definitions for secure cash distri- bution with penalties (SCD), a primitive that allows stateful compu- tations involving data and/or money, and guarantees a strong notion of dropout tolerance|,Non-data,49
| We then constructed a protocol for SCD that only makes use of a claim-or-refund transaction functionality F (cid:63) CR (which can be implemented in a variant of Bitcoin) and is otherwise independent of the Bitcoin ecosystem Our SCD protocol may be improved in a number of ways, including improvements to round complexity, validation complexity of F (cid:63) CR transactions, as well as alternate constructions that make only black-box use of MPC Acknowledgments We would like to thank Ananth Raghunathan, Yuval Ishai, and Stefan Dziembowski for useful discussions The work of the first author was supported by funding from NSF grants CNS- 1350619, CNS-1414119, and CNS-1413920, and funding from Qatar Computing Research Institute, and funding from the Euro- pean Union Seventh Framework Programme (FP7/2007-2013) un- der grant agreement number 259426|,Non-data,49
 The work of the second au- thor was supported by funding from ISF grant no 1790/13 and by the European Union Seventh Framework Programme (FP7/2007- 2013) under grant agreement number 293843 The work of the third author was supported by funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement number 240258 2058,Non-data,49
|ABSTRACT Attackers can get physical control of a computer in sleep (S3/suspend-to-RAM), if it is lost, stolen, or the owner is being coerced High-value memory-resident secrets, includ- ing disk encryption keys, and private signature/encryption keys for PGP, may be extracted (eg, via cold-boot or DMA attacks), by physically accessing such a computer|,Non-data,50
| Our goal is to alleviate threats of extracting secrets from a computer in sleep, without relying on an Internet-facing service We propose Hypnoguard to protect all memory-resident OS/user data across S3 suspensions, by first performing an in-place full memory encryption before entering sleep, and then restoring the plaintext content at wakeup-time through an environment-bound, password-based authentication pro- cess The memory encryption key is effectively “sealed” in a Trusted Platform Module (TPM) chip with the measure- ment of the execution environment supported by CPU’s trusted execution mode (eg|,Non-data,50
|, Intel TXT, AMD-V/SVM) Password guessing within Hypnoguard may cause the mem- ory content to be permanently inaccessible, while guessing without Hypnoguard is equivalent to brute-forcing a high- entropy key (due to TPM protection) We achieved full memory encryption/decryption in less than a second on a mainstream computer (Intel i7-4771 CPU with 8GB RAM, taking advantage of multi-core processing and AES-NI), an apparently acceptable delay for sleep-wake transitions To the best of our knowledge, Hypnoguard provides the first wakeup-time secure environment for authentication and key unlocking, without requiring per-application changes|,Non-data,50
| 1 INTRODUCTION Most computers, especially laptops, remain in sleep (S3/suspend-to-RAM), when not in active use (eg, as in a lid-close event); see e|,Non-data,50
|g, [49] A major concern for unat- tended computers in sleep is the presence of user secrets in system memory An attacker with physical access to a com- puter in sleep (e|,Non-data,50
|g, when lost/stolen, or by coercion) can launch side-channel memory attacks, eg, DMA attacks [37, 53, 6, 57] by exploiting vulnerable device drivers; common Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page|,Non-data,50
| Copyrights for components of this work owned by others than ACM must be honored Abstracting with credit is permitted To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from permissions@acm|,Non-data,50
|org CCS’16, October 24-28, 2016, Vienna, Austria c(cid:13) 2016 ACM ISBN 978-1-4503-4139-4/16/10 |,Non-data,50
  $1500 DOI: http://dxdoi,Non-data,50
|org/101145/29767492978372 mitigations include: bug fixes, IOMMU (Intel VT-d/AMD Vi), and disabling (FireWire) DMA when screen is locked (eg|,Non-data,50
|, Mac OS X 1072 and later, Windows 81 [37])|,Non-data,50
| A so- phisticated attacker can also resort to cold-boot attacks by exploiting DRAM memory remanence effect [25, 22] Sim- pler techniques also exist for memory extraction (eg, [16]); some tools (e|,Non-data,50
|g, [14]) may bypass OS lock screen and extract in-memory full-disk encryption (FDE) keys Some proposals address memory-extraction attacks by making the attacks difficult to launch, or by reducing ap- plicability of known attacks (eg|,Non-data,50
|, [47, 45, 56, 23, 63, 24]; see Section 8) Limitations of these solutions include: be- ing too application-specific (eg, disk encryption), not being scalable (i|,Non-data,50
|e, can support only a few application-specific se- crets), and other identified flaws (cf [5]) Most solutions also do not consider re-authentication when the computer wakes up from sleep|,Non-data,50
| If a regular re-authentication is mandated (eg, OS unlock), a user-chosen password may not provide enough entropy against guessing attacks (offline/online) Protecting only cryptographic keys also appears to be fun- damentally inadequate, as there exists more privacy/secu- rity sensitive content in RAM than keys and passwords|,Non-data,50
| Full memory encryption can be used to keep all RAM content encrypted, as used in proposals for encrypted execution (see XOM [36], and a comprehensive survey [27]) However, most such proposals require hardware architectural changes Microsoft BitLocker can be configured to provide cold boot protection by relying on S4/suspend-to-disk instead of S3 This introduces noticeable delays in the sleep-wake pro- cess|,Non-data,50
| More importantly, BitLocker is not designed to with- stand coercion and can provide only limited defence against password guessing attacks (discussed more in Section 8) We propose Hypnoguard to protect all memory-resident OS/user data across S3 suspensions, against memory ex- traction attacks, and guessing/coercion of user passwords during wakeup-time re-authentication Memory extraction is mitigated by performing an in-place full memory encryp- tion before entering sleep, and then restoring the plaintext content/secrets after the wakeup process The memory en- cryption key is encrypted by a Hypnoguard public key, the private part of which is stored in a Trusted Platform Mod- ule (TPM v1|,Non-data,50
|2) chip, protected by both the user password and the measurement of the execution environment sup- ported by CPU’s trusted execution mode, eg, Intel Trusted Execution Technology (TXT [30]) and AMD Virtualization (AMD-V/SVM [2]) The memory encryption key is thus bound to the execution environment, and can be released only by a proper re-authentication process|,Non-data,50
| 945Guessing via Hypnoguard may cause the memory content to be permanently inaccessible due to the deletion of the TPM-stored Hypnoguard private key, while guessing with- out Hypnoguard, eg, an attacker-chosen custom wakeup procedure, is equivalent to brute-forcing a high-entropy key, due to TPM protection A user-defined policy, e|,Non-data,50
|g, three failed attempts, or a special deletion password, determines when the private key is deleted As a result, either the pri- vate key cannot be accessed due to an incorrect measurement of an altered program, or the adversary takes a high risk to guess within the unmodified environment By encrypting the entire memory space, except a few system-reserved regions, where no OS/user data resides, we avoid per-application changes|,Non-data,50
| We leverage modern CPU’s AES-NI extension and multi-core processing to quickly en- crypt/decrypt commonly available memory sizes (up to 8GB, under a second), for avoiding degraded user experi- ence during sleep-wake cycles For larger memory systems (eg, 32/64GB), we also provide two variants, for encrypt- ing memory pages of user selected applications, or specific Hypnoguard-managed pages requested by applications|,Non-data,50
| Due to the peculiarity of the wakeup-time environment, we face several challenges in implementing Hypnoguard Un- like boot-time (when peripherals are initialized by BIOS) or run-time (when device drivers in the OS are active), at wakeup-time, the system is left in an undetermined state, eg, empty PCI configuration space and uninitialized I/O controllers|,Non-data,50
| We implement custom drivers and reuse dor- mant (during S3) OS-saved device configurations to restore the keyboard and VGA display to facilitate easy user in- put/output (inadequately addressed in the past, cf [46]) Several boot-time solutions (eg|,Non-data,50
|, [31, 64, 70]) also per- form system integrity check, authenticate the user, and may release FDE keys; however, they do not consider memory at- tacks during sleep-wake cycles For lost/stolen computers, some remote tracking services may be used to trigger remote deletion, assuming the computer can be reached online (with doubtful effectiveness, cf [13, 62]) Contributions: 1|,Non-data,50
| We design and implement Hypnoguard, a new approach that protects confidentiality of all memory regions con- taining OS/user data across sleep-wake cycles We pro- vide defense against memory attacks when the computer is in the wrong hands, and severely restrict guessing of weak authentication secrets (cf [70]) Several propos- als and tools exist to safeguard data-at-rest (e|,Non-data,50
|g, disk storage), data-in-transit (eg, network traffic), and data- in-use (e|,Non-data,50
|g, live RAM content); with Hypnoguard, we fill the gap of securing data-in-sleep 2 Our primary prototype implementation in Linux uses full memory encryption to avoid per-application changes|,Non-data,50
| The core part of Hypnoguard is decoupled from the under- lying OS and system BIOS, for better portability and security Leveraging modern CPU’s AES-NI extension and multi-core processing, we achieve around 87GB/s encryption/decryption speed for AES in the CTR mode with an Intel i7-4771 processor, leading to under a second additional delay in the sleep-wake process for 8GB RAM 3|,Non-data,50
| For larger memory systems (eg, 32GB), where full mem- ory encryption may add noticeable delay, we provide pro- tection for application-selected memory pages via the POSIX- compliant system call mmap() (requiring minor changes in applications, but no kernel patches) Alternatively, Hypnoguard can also be customized to take a list of ap- plications and only encrypt memory pages pertaining to them (no application changes)|,Non-data,50
| 4 We enable wakeup-time secure processing, previously un- explored, which can be leveraged for other use-cases, eg, OS/kernel integrity check|,Non-data,50
| 2 TERMINOLOGIES, GOALS AND THREAT MODEL We explain the terminologies used for Hypnoguard, and our goals, threat model and operational assumptions We use CPU’s trusted execution mode (eg|,Non-data,50
|, Intel TXT, AMD- V/SVM), and the trusted platform module (TPM) chip We provide brief description of some features as used in our proposal and implementation; for details, see, eg, Parno et al|,Non-data,50
| [48], Intel [30], and AMD [2] 21 Terminologies Hypnoguard key pair (HGpub, HGpriv): A pair of public and private keys generated during deployment The pri- vate key, HGpriv, is stored in a TPM NVRAM index, pro- tected by both the measurement of the environment and the Hypnoguard user password|,Non-data,50
| HGpriv is retrieved through the password evaluated by TPM with the genuine Hypnoguard program running, and can be permanently deleted in ac- cordance with a user-set policy The public key, HGpub, is stored unprotected in TPM NVRAM (for OS/file system independence), and is loaded in RAM after each boot Memory encryption key (SK): A high entropy symmetric key (eg|,Non-data,50
|, 128-bit), randomly generated each time before en- tering sleep, and used for full memory encryption Before the system enters sleep, SK is encrypted using HGpub and the resulting ciphertext is stored in the small non-encrypted region of memory Hypnoguard user password: A user-chosen password to un- lock the protected key HGpriv at wakeup-time It needs to withstand only a few guesses, depending on the actual un- locking policy|,Non-data,50
| This password is unrelated to the OS unlock password, which can be optionally suppressed TPM “sealing”: For protecting HGpriv in TPM, we use the TPM_NV_DefineSpace command, which provides envi- ronment binding (similar to TPM_Seal, but stores HGpriv in an NVRAM index) and authdata (password) protec- tion We use the term “sealing” to refer to this mechanism for simplicity 2|,Non-data,50
|2 Goals We primarily consider attacks targeting extraction of se- crets through physical access to a computer in S3 sleep (unattended, stolen, or when the owner is under coercion) We want to protect memory-resident secrets against side- channel attacks (eg, DMA/cold-boot attacks), but we do not consider compromising a computer in S3 sleep for evil- maid type attacks (unbeknownst to the user)|,Non-data,50
| More specifically, our goals include: (G1) Any user or OS data (secrets or otherwise), SK, and HGpriv must not remain in plaintext anywhere in RAM before resuming the OS to make memory attacks inapplicable (G2) The protected con- tent (in our implementation, the whole RAM) must not be retrieved by brute-forcing SK or HGpriv, even if Hypnoguard is not active, eg, via offline attacks|,Non-data,50
| (G3) No guessing at- tacks should be possible against the Hypnoguard user pass- 946word, unless a genuine copy of Hypnoguard is loaded as the only program in execution (G4) The legitimate user should be able to authenticate with routine effort, eg, memoriza- tion of strong passwords is not required|,Non-data,50
| (G5) Guessing the user password when Hypnoguard is active should be severely restricted by the penalty of having the secrets deleted An additional goal for coercion attacks during wakeup (similar to the boot-time protection of [70]): (AG1) when deletion is successful, there should be a cryptographic ev- idence that convinces the adversary that the RAM secrets are permanently inaccessible 23 Threat model and assumptions 1|,Non-data,50
| The adversary may be either an ordinary person with skills to mount memory/guessing attacks, or an organi- zation (non-state) with coercive powers, and considerable but not unbounded computational resources For exam- ple, the adversary may successfully launch sophisticated cold-boot attacks (eg, [25, 22]), but cannot brute-force a random 128-bit AES key, or defeat the TPM chip and CPU’s trusted execution environment (for known imple- mentation bugs and attacks, see e|,Non-data,50
|g, [59, 68, 54]); see also Item (f) in Section 7 2 Before the adversary gains physical control, the computer system (hardware and OS) has not been compromised|,Non-data,50
| After the adversary releases physical control, or a lost computer is found, the system is assumed to be untrust- worthy, ie, no further use without complete reinitial- ization We thus only consider directly extracting se- crets from a computer in sleep, excluding any attacks that rely on compromising first and tricking the user to use it later, the so-called evil-maid attacks, which can be addressed by adapting existing defenses, e|,Non-data,50
|g, [20] for wakeup-time However, no known effective defense exists for more advanced evil-maid attacks, including hardware modifications as in NSA’s ANT catalog [21] Note that, our AES-GCM based implementation can restrict modi- fication attacks on encrypted RAM content|,Non-data,50
| 3 The host OS is assumed to be general-purpose, eg, Win- dows or Linux; a TXT/SVM-aware kernel is not needed|,Non-data,50
| Also, the Hypnoguard tool may reside in an untrusted file system and be bootstrapped from a regular OS 4 We assume all user data, the OS, and any swap space used by the OS are stored encrypted on disk, eg|,Non-data,50
|, us- ing a properly configured software/hardware FDE sys- tem (cf [44, 12]) A secure boot-time solution should be used to enforce strong authentication (cf [70])|,Non-data,50
| The FDE key may remain in RAM under Hypnoguard’s protection This assumption can be relaxed, only if the data on disk is assumed non-sensitive, or in the case of a diskless node 5 Any information placed in memory by the user/OS is treated as sensitive|,Non-data,50
| With full memory encryption, it is not necessary to distinguish user secrets from non- sensitive data (eg, system binaries) 6|,Non-data,50
| The adversary must not be able to capture the computer while it is operating, ie, in Advanced Configuration and Power Interface (ACPI [1]) S0 We assume the computer goes into sleep after a period of inactivity, or through user actions (e|,Non-data,50
|g, lid-close of a laptop) 7 The adversary may attempt to defeat Hypnoguard’s pol- icy enforcement mechanism (i|,Non-data,50
|e, when to delete or unlock HGpriv during authentication) With physical access, he may intervene in the wakeup process, eg|,Non-data,50
|, by tampering Figure 1: Memory layout and key usage of Hypnoguard Shaded areas represent encrypted/protected data; different patterns refer to using different schemes/key types with the UEFI boot script for S3 [67], and may attempt to observe the input and output of our tool and influence its logic In all cases, he will fail to access HGpriv, unless he can defeat TXT/SVM/TPM (via an implementation flaw, or advanced hardware attacks)|,Non-data,50
| 8 In the case of coercion, the user never types the correct password but provides only deletion or incorrect pass- words, to trigger the deletion of HGpriv Coercion has been considered recently during boot-time [70], requir- ing the computer to be in a powered-off state before the coercive situation We consider coercion during wakeup; ideally, both systems should be used together|,Non-data,50
| 9 We require a system with a TPM chip and a TXT/SVM- capable CPU with AES-NI (available in many consumer- grade Intel and AMD CPUs) Without AES-NI, full memory encryption will be slow, and users must resort to partial memory encryption 3|,Non-data,50
| DESIGN In this section, we detail the architecture of Hypnoguard, and demonstrate how it achieves the design goals stated in Section 22 Technical considerations not specific to our cur- rent implementation are also discussed Overview|,Non-data,50
| Figure 1 shows the memory layout and key usage of Hypnoguard across sleep-wake cycles; the transition and execution flows are described in Section 41 User secrets are made unavailable from RAM by encrypting the whole system memory, regardless of kernel or user spaces, with a one-time random symmetric key SK before entering sleep Then SK is encrypted using HGpub and stored in system memory|,Non-data,50
| At this point, only HGpriv can decrypt SK HGpriv is sealed in the TPM chip with the measurements of the genuine copy of Hypnoguard protected by a user password At wakeup-time, Hypnoguard takes control in a trusted execution session (TXT/SVM), and prompts the user for the Hypnoguard user password Only when the correct pass- 947word is provided in the genuine Hypnoguard environment, HGpriv is unlocked from TPM (still in TXT/SVM)|,Non-data,50
| Then, HGpriv is used to decrypt SK and erased from memory im- mediately The whole memory is then decrypted with SK and the system exits from TXT/SVM back to normal OS operations SK is not reused for any future session 3|,Non-data,50
|1 Design choices and elements Trusted execution mode We execute the unlocking pro- gram in the trusted mode of modern CPUs (TXT/SVM), where an unforgeable measurement of the execution envi- ronment is generated and stored in TPM (used to access HGpriv) The use of TXT/SVM and TPM ensures that the whole program being loaded and executed will be re- flected in the measurement; ie|,Non-data,50
|, neither the measurement can be forged at the load time nor can the measured pro- gram be altered after being loaded, eg, via DMA attacks The memory and I/O space of the measured environment is also protected, e|,Non-data,50
|g, via Intel VT-d/IOMMU, from any external access attempt We choose to keep Hypnoguard as a standalone module (a) Small trusted separate from the OS for two reasons computing base (TCB): If Hypnoguard’s unlocking program is integrated with the OS, then we must also include OS components (at least the kernel and core OS services) in the TPM measurement; this will increase the TCB size sig- nificantly|,Non-data,50
| Also, in a consumer OS, maintaining the cor- rect measurements of such a TCB across frequent updates and run-time changes, will be very challenging Unless mea- suring the entire OS is the purpose (cf Unicorn [38]), a TXT/SVM-protected application is usually a small piece of code, not integrated with the OS, to achieve a stable and manageable TCB (eg|,Non-data,50
|, Flicker [40]) In our case, only the core Hypnoguard unlock logic must be integrity-protected (ie, bound to TPM measurement)|,Non-data,50
| The small size may also aid manual/automatic verification of the source code of an implementation (b) Portability: We make Hypnoguard less coupled with the hosting OS except for just a kernel driver, as we may need to work with different distributions/versions of an OS, or completely different OSes TPM’s role TPM serves three purposes in Hypnoguard: 1|,Non-data,50
| By working with TXT/SVM, TPM’s platform configura- tion registers (PCRs) maintain the unforgeable measure- ment of the execution environment 2 We use TPM NVRAM to store HGpriv safely with two layers of protection First, HGpriv is bound to the Hypno- guard environment (e|,Non-data,50
|g, the Intel SINIT module and the Hypnoguard unlocking program) Any binary other than the genuine copy of Hypnoguard will fail to ac- cess HGpriv Second, an authdata secret, derived from the Hypnoguard user password, is also used to protect HGpriv|,Non-data,50
 Failure to meet either of the above two condi- tions will lead to denial of access 3 If HGpriv is deleted by Hypnoguard (eg,Non-data,50
|, triggered via multiple authentication failures, or the entry of a deletion password), we also use TPM to provide a quote, which is a digest of the platform measurement signed by the TPM’s attestation identity key (AIK) seeded with an arbitrary value (eg, time stamp, nonce) Anyone, including the adversary, can verify the quote using TPM’s public key at a later time, and confirm that deletion has happened|,Non-data,50
| 4 For generation of the long-term key pair HGpriv and HGpub, and the per-session symmetric key SK, we need a reliable source of randomness We use the TPM_GetRandom command to get the required number of bytes from the random number generator in TPM [60] (and optionally, mix them with the output from the RDRAND instruction in modern CPUs) Necessity of HGpriv and HGpub|,Non-data,50
| Although we use ran- dom per sleep-wake cycle symmetric key (SK) for full mem- ory encryption, we cannot directly seal SK in TPM (un- der the Hypnoguard password), ie, avoid using (HGpriv, HGpub) The reason is that we perform the platform-bound user re-authentication only once at the wakeup time, and without involving the user before entering sleep, we cannot password-seal SK in TPM|,Non-data,50
| If the user is required to enter the Hypnoguard password every time before entering sleep, the user experience will be severely affected We thus keep SK encrypted under HGpub in RAM, and involve the password only at wakeup-time to release HGpriv (ie, the password input is similar to a normal OS unlock process)|,Non-data,50
| 32 Unlock/deletion policy and deployment Unlocking policy A user-defined unlocking policy will determine how Hypnoguard reacts to a given password, ie|,Non-data,50
|, what happens when the correct password is entered vs when a deletion or invalid password is entered If the policy al- lows many/unlimited online (ie|,Non-data,50
|, via Hypnoguard) guessing attempts, a dictionary attack might be mounted, violating goal G5 ; the risk to the attacker in this case is that he might unknowingly enter the deletion password If the composition of the allowed password is not properly chosen (eg, differ- ent character sets for the correct password and the deletion password), an adversary may be able to recognize the types of passwords, and thus avoid triggering deletion|,Non-data,50
| Static policies can be configured with user-selected pass- words and/or rule-based schemes that support evaluating an entered password at run-time Security and usability trade- offs should be considered, eg, a quick deletion trigger vs|,Non-data,50
| tolerating user mistyping or misremembering (cf [11]) Dur- ing setup, both unlocking and deletion passwords are cho- sen by the user, and they are set as the access passwords for corresponding TPM NVRAM indices: the deletion password protects an index with a deletion indicator and some random data (as dummy key), and the unlocking password protects an index containing a null indicator and HGpriv (similar to [70]) Note that, both the content and deletion indicator of an NVRAM index are protected (i|,Non-data,50
|e, attackers cannot exploit the indicator values) Multiple deletion passwords can also be defined We also use a protected monotonic counter to serve as a fail counter, sealed under Hypnoguard, and initialized to 0|,Non-data,50
| We use a regular numeric value sealed in NVRAM (ie, inaccessible outside of Hypnoguard); the TPM monotonic counter facility can also be used The fail counter is used to allow only a limited number of incorrect attempts, after which, deletion is triggered; this is specifi- cally important to deal with lost/stolen cases|,Non-data,50
| At run-time, only when the genuine Hypnoguard program is active, the fail counter is incremented by one, and a typed password is used to attempt to unlock the defined indices, sequentially, until an index is successfully opened, or all the indices are tried In this way, the evaluation of a password is performed only within the TPM chip and no information about any defined plaintext passwords or HGpriv is leaked in RAM—leaving no chance to cold-boot attacks If a typed password successfully unlocks an index (ie|,Non-data,50
|, a valid pass- 948word), the fail counter is decremented by one; otherwise, the password entry is considered a failed attempt and the incremented counter is not decremented When the counter reaches a preset threshold, deletion is triggered The counter is reset to 0 only when the correct password is entered (ie|,Non-data,50
|, HGpriv is successfully unlocked) Thus, a small thresh- old (eg, 10) may provide a good balance between security (quick deletion trigger) and usability (the number of incor- rect entries that are tolerated)|,Non-data,50
| For high-value data, the threshold may be set to 1, which will trigger deletion imme- diately after a single incorrect entry Deployment/setup phase With a setup program in the OS, we generate a 2048-bit RSA key pair and save HGpub in TPM NVRAM (unprotected), and ask the user to create her passwords for both unlocking and deletion With the unlocking password (as authdata secret), HGpriv is stored in an NVRAM index, bound to the expected PCR values of the Hypnoguard environment at wakeup (computed ana- lytically); similarly, indices with deletion indicators are al- located and protected with the deletion password(s)|,Non-data,50
| There is also certain OS-end preparation, eg, loading and initial- izing the Hypnoguard device drivers; see Section 41|,Non-data,50
| 33 How goals are achieved Hypnoguard’s goals are defined in Section 22 G1 is ful- filled by Hypnoguard’s full memory encryption, i|,Non-data,50
|e, replace- ment of all plaintext memory content, with corresponding ciphertext generated by SK As the OS or applications are not involved, in-place memory encryption can be performed reliably SK resides in memory encrypted under HGpub (right after full memory encryption is performed under SK)|,Non-data,50
| HGpriv can only be unlocked with the correct environment and password at wakeup-time, and is erased from RAM right after its use in the trusted execution mode A random SK with adequate length generated each time before entering sleep, and a strong public key pair (HGpub, HGpriv) generated during setup guarantee G2 TPM sealing (even with a weak Hypnoguard user pass- word) helps achieve G3 Without loading the correct binary, the adversary cannot forge the TPM measurement and trick TPM to access the NVRAM index (cf|,Non-data,50
| [30, 60]); note that, learning the expected PCR values of Hypnoguard does not help the attacker in any way The adversary is also unable to brute-force the potentially weak user password, if he is willing to program the TPM chip without Hypnoguard, as TPM ensures the consistent failure message for both incor- rect passwords and incorrect measurements The user is required to memorize a regular password for authentication If the adversary keeps the genuine environ- ment but does not know the correct password, he may be only left with a high risk of deleting HGpriv|,Non-data,50
| The legitimate user, however, knows the password and can control the risk of accidental deletion, eg, via setting an appropriate dele- tion threshold Therefore G4 is satisfied|,Non-data,50
| When the adversary guesses within Hypnoguard, the pass- word scheme (unlocking policy) makes sure that no (or only a few, for better usability) guessing attempts are allowed before deletion is triggered This achieves G5 The additional goal for coercion attacks is achieved through the TPM Quote operation The quote value relies on mainly two factors: the signing key, and the measurement to be signed|,Non-data,50
| An RSA key pair in TPM called AIK (Attestation Identity Key) serves as the signing key Its public part is signed by TPM’s unique key (Endorsement Key, aka EK, generated by the manufacturer and never leaves the chip in any operations) and certified by a CA in a separate process (eg|,Non-data,50
|, during setup) This ensures the validity of the signa- ture The data to be signed is the requested PCR values In TXT, the initial PCR value is set to 0, and all subsequent extend operations will update the PCR values in an unforge- able manner (via SHA1)|,Non-data,50
| As a result, as long as the quote matches the expected one, the genuine copy of the program must have been executed, and thus AG1 is achieved 4 IMPLEMENTATION In this section, we discuss our implementation of Hypno- guard under Linux using Intel TXT as the trusted exe- cution provider Note that Hypnoguard’s design is OS- independent, but our current implementation is Linux spe- cific; the only component that must be developed for other OSes is HypnoOSService (see below)|,Non-data,50
| We also performed an experimental evaluation of Hypnoguard’s user experi- ence (for 8GB RAM); no noticeable latency was observed at wakeup-time (eg, when the user sees the lit-up screen) We assume that a delay under a second before entering sleep and during wakeup is acceptable|,Non-data,50
| For larger memory sizes (eg, 32GB), we implement two variants to quickly encrypt selected memory regions 4|,Non-data,50
|1 Overview and execution steps The Hypnoguard tool consists of three parts: Hypno- Core (the unlocking logic and cipher engine), HypnoDrivers (device drivers used at wakeup-time), and HypnoOSService (kernel service to prepare for S3 and HypnoCore) Hyp- noCore and HypnoDrivers operate outside of the OS, and HypnoOSService runs within the OS The approximate code size of our implementation is: HypnoCore, 7767 LOC (in C/C++/assembly, including reused code for TPM, AES, RSA, SHA1); HypnoDrivers, 3263 LOC (in C, including reused code for USB); HypnoOSService, 734 LOC in C; GCM, 2773 LOC (in assembly, including both the original and our adapted constructions); and a shared framework between the components, 639 LOC in assembly Execution steps|,Non-data,50
| Figure 2 shows the generalized exe- cution steps needed to achieve the designed functionalities on an x86 platform (a) The preparation is done by Hyp- noOSService at any time while the OS is running before S3 is triggered HypnoCore, HypnoDrivers, ACM module for TXT, and the TXT policy file are copied into fixed mem- ory locations known by Hypnoguard (see Section 43)|,Non-data,50
| Also, HypnoOSService registers itself to the OS kernel so that if the user or a system service initiates S3, it can be invoked (b) Upon entry, necessary parameters for S3/TXT are pre- pared and stored (those that must be passed from the active OS to Hypnoguard), and the kernel’s memory tables are re- placed with ours, mapped for HypnoCore and HypnoDrivers (c) Then, HypnoCore encrypts the whole memory in a very quick manner through multi-core processing with AES CTR mode using SK SK is then encrypted by HGpub (an RSA- 2048 key)|,Non-data,50
| Before triggering the actual S3 action by sending commands to ACPI, Hypnoguard must replace the original OS waking vector to obtain control back when the machine is waken up (d) At S3 wakeup, the 16-bit realmode entry, residing below 1MB, of Hypnoguard waking vector is trig- gered It calls HypnoDrivers to re-initialize the keyboard and display, and prepares TXT memory structures (TXT 949Figure 2: Simplified execution steps of Hypnoguard heap) and page tables (e) Then the user is prompted for a password, which is used to unlock TPM NVRAM indices one by one|,Non-data,50
| Based on the outcome and the actual unlocking policy, either deletion of HGpriv happens right away and a quote is generated for further verification (and the system is restarted), or if the password is correct, HGpriv is un- locked into memory After decrypting SK, HGpriv is erased promptly from memory HypnoCore then uses SK to de- crypt the whole memory (f) TXT is torn down, and the OS is resumed by calling the original waking vector|,Non-data,50
 Machine configuration We use an Intel platform running Ubuntu 1504 (kernel version: 319,Non-data,50
|0) The development machine’s configuration includes: an Intel Core i7-4771 pro- cessor (350 GHz, 4 physical cores), with Intel’s integrated HD Graphics 4600, Q87M-E chipset, 8GB RAM (Kingston DDR3 4GBx2, clock speed 1600 MHz), and 500GB Seagate self-encrypting drive In theory, our tool should work on most machines with TPM, AES-NI and Intel TXT (or AMD SVM) support, with minor changes, such as downloading the corresponding SINIT module|,Non-data,50
| 42 Instrumenting the S3 handler Hypnoguard needs to gain control at wakeup-time before the OS resume process begins For simplicity, we follow the method as used in a similar scenario in Intel tboot [31] An x86 system uses ACPI tables to communicate with the system software (usually the OS) about power management parameters|,Non-data,50
| The firmware waking vector, contained in the Firmware ACPI Control Structure (FACS), stores the ad- dress of the first instruction to be executed after wakeup; and to actually put the machine to sleep, certain platform- specific data, found in the Fixed ACPI Description Table (FADT), must be written to corresponding ACPI registers We must register Hypnoguard with an OS callback for re- placing the waking vector, so as not to interfere with normal acpi os prepare sleep() call- OS operations In Linux, the back can be used, which will be invoked in the kernel space before entering sleep However, we cannot just replace the waking vector in this callback and return to the OS, as Linux overwrites the waking vector with its own at the end of S3 preparation, apparently, to ensure a smooth resume|,Non-data,50
| Fortu- nately, the required data to be written to ACPI registers is already passed in as arguments by the kernel, and as the OS is ready to enter sleep, we put the machine to sleep without returning to the OS 43 Memory considerations To survive across various contexts (Linux, non-OS native, initial S3 wakeup and TXT), and not to be concerned with paging and virtual memory addressing, we reserve a region from the system memory by providing a custom version of the e820 map, so that Linux will not touch it afterwards This is done by appending a kernel command line param- eter memmap|,Non-data,50
| In Windows, this can be done by adding those pages to BadMemoryList 1 MB space at 0x900000 is allocated for HypnoCore, HypnoDrivers and miscellaneous parameters to be passed between different states, eg, the SINIT module, original waking vector of Linux, policy data, stack space for each processor core, and Intel AES-NI library (see Section 5)|,Non-data,50
| Full memory coverage in 64-bit mode To support more than 4GB memory sizes, we need to make Hypnoguard 64-bit addressable However, we cannot simply compile the Hypnoguard binary into 64-bit mode as most other mod- ules, especially those for TXT and TPM access, are only available in 32-bit mode, and adapting them to 64-bit will be non-trivial (if possible), because of the significantly dif- ferent nature of 64-bit mode (eg|,Non-data,50
|, mandatory paging) We keep HypnoCore and HypnoDrivers unchanged, and write a trampoline routine for the 64-bit AES-NI library, where we prepare paging and map the 8GB memory before switching to the long mode (64-bit) After the AES-NI li- brary call, we go back to 32-bit mode Also, the x86 calling conventions may be different than x86-64 (e|,Non-data,50
|g, use of stack space vs additional registers) A wrapper function, before the trampoline routine goes to actual functions, is used to extract those arguments from stack and save them to corre- sponding registers|,Non-data,50
| In this way, the 64-bit AES-NI library runs as if the entire HypnoCore and HypnoDrivers binary is 64-bit, and thus we can access memory regions beyond 4GB, while the rest of Hypnoguard still remains in 32-bit mode 44 User interaction In a regular password-based wakeup-time authentication, the user is shown the password prompt dialog to enter the password In addition to the password input, we also need to display information in several instances, e|,Non-data,50
|g, interacting with the user to set up various parameters during deploy- ment, indicating when deletion is triggered, and displaying the quote (ie, proof of deletion)|,Non-data,50
| Providing both standard input and output is easy at boot-time (with BIOS support), and within the OS However, resuming from S3 is a special situation: no BIOS POST is executed, and no OS is ac- tive At this time, peripherals (eg|,Non-data,50
|, PCI, USB) are left in an uninitialized state, and unless some custom drivers are implemented, display and keyboard remain nonfunctional 950For display, we follow a common practice as used in Linux for S3 resume (applicable for most VGA adapters) Hypn- oDrivers invoke the legacy BIOS video routine using “lcallw 0xc000,3” (0xc0000 is the start of the VGA RAM where the video BIOS is copied to; the first 3 bytes are the signature and size of the region, and 0xc0003 is the entry point) For keyboard support, the S3 wakeup environment is more challenging (PS/2 keyboards can be easily supported via a simple driver)|,Non-data,50
| Most desktop keyboards are currently con- nected via USB, and recent versions of BIOS usually have a feature called “legacy USB support” Like a mini-OS, as part of the power-on check, the BIOS (or the more recent UEFI services) would set up the PCI configuration space, perform USB enumeration, and initialize the class drivers (eg, HID and Mass Storage)|,Non-data,50
| But when we examined the USB EHCI controller that our USB keyboard was connected to, we found that its base address registers were all zeros at wakeup-time, implying that it was uninitialized (same for video adapters) As far as we are aware, no reliable mecha- nisms exist for user I/O after wakeup TreVisor [46] resorted to letting the user input in a blank screen (ie|,Non-data,50
|, keyboard was active, but VGA was uninitialized) Note that the actual situation is motherboard-specific, determined mostly by the BIOS We found that only one out of our five test machines has the keyboard initialized at wakeup-time Loading a lightweight Linux kernel might be an option, which would increase the TCB size and (potentially) intro- duce additional attack surface|,Non-data,50
| Also, we must execute the kernel in the limited Hypnoguard-reserved space Instead, we enable USB keyboard support as follows: 1 Following the Linux kernel functions pci save state() and pci restore config space(), we save the PCI configuration space before entering S3, and restore it at wakeup-time to enable USB controllers in Hypnoguard 2|,Non-data,50
| We borrow a minimal set of functions from the USB stack of the GRUB project, to build a tiny USB driver only for HID keyboards operating on the “boot protocol” [61] 3 There are a few unique steps performed at boot-time for USB initialization that cannot be repeated during S3 wakeup For instance, a suspended hub port (connect- ing the USB keyboard) is ready to be waken up by the host OS driver and does not accept a new round of enu- meration (e|,Non-data,50
|g, getting device descriptor, assigning a new address) We thus cannot reuse all boot-time USB ini- tialization code from GRUB At the end, we successfully reconfigure the USB hub by initiating a port reset first|,Non-data,50
| With the above approach, we can use both the USB key- board and VGA display at wakeup-time This is hardware- agnostic, as restoring PCI configuration simply copies exist- ing values, and the USB stack as reused from GRUB follows a standard USB implementation We also implement an i8042 driver (under 100 LOC) to support PS/2 keyboards Our approach may help other projects that cannot rely on the OS/BIOS for input/output support, e|,Non-data,50
|g, [46, 15] 45 Moving data around Hypnoguard operates at different stages, connected by jumping to an address without contextual semantics|,Non-data,50
| Con- ventional parameter passing in programming languages and shared memory access are unavailable between these stages Therefore, we must facilitate binary data transfer between the stages To seamlessly interface with the host OS, we apply a similar method as in Flicker [40] to create a sysfs object in a user-space file system It appears in the directory “/sys/kernel” as a few subdirectories and two files: data (for accepting raw data) and control (for accepting commands)|,Non-data,50
| In HypnoOSService, the sysfs handlers write the received data to the 1MB reserved memory region When S3 is trig- gered, HypnoDrivers will be responsible for copying the re- quired (portion of) binary to a proper location, for instance, the real-mode wakeup code to 0x8a000, SINIT to the BIOS- determined location SINITBASE and the LCP policy to the OsMleData table, which resides in the TXT heap prepared by HypnoDrivers before entering TXT 4|,Non-data,50
|6 Unencrypted memory regions In our full memory encryption, the actual encrypted ad- dresses are not contiguous We leave BIOS/hardware re- served regions unencrypted, which fall under two categories (a) MMIO space: platform-mapped memory and registers of I/O devices, eg|,Non-data,50
|, the TPM locality base starts at 0xfed40000 (b) Platform configuration data: memory ranges used by BIOS/UEFI/ACPI; the properties of such regions vary sig- nificantly, from read-only to non-volatile storage Initially, when we encrypted the whole RAM, including the reserved regions, we observed infrequent unexpected sys- tem behaviors (eg|,Non-data,50
|, system crash) As much as we are aware of, no user or OS data is stored in those regions (cf [33]), and thus there should be no loss of confidentiality due to keep- ing those regions unencrypted Hypnoguard parses the e820 (memory mapping) table to determine the memory regions accessible by the OS|,Non-data,50
| In our test system, there is approxi- mately 700MB reserved space, spread across different ranges below 4GB The amount of physical memory is compensated by shifting the addresses, eg, for our 8GB RAM, the actual addressable memory range goes up to 8|,Non-data,50
|7GB 5 HIGH-SPEED FULL MEMORY ENCRYP- TION AND DECRYPTION The adoptability of the primary Hypnoguard variant based on full memory encryption/decryption mandates a minimal impact on user experience Below, we discuss issues related to our implementation of quick memory encryption|,Non-data,50
| For all our modes of operation with AES-NI, the process- ing is 16-byte-oriented (ie, 128-bit AES blocks) and han- dled in XMM registers In-place memory encryption/de- cryption is intrinsically supported by taking an input block at a certain location, and overwriting it with the output of the corresponding operation|,Non-data,50
| Therefore, no extra memory needs to be reserved, and thus no performance overhead for data transfer is incurred 51 Enabling techniques Native execution We cannot perform in-place memory encryption when the OS is active, due to OS memory pro- tection and memory read/write operations by the OS|,Non-data,50
| Thus, the OS must be inactive when we start memory encryption Likewise, at wakeup-time in TXT, there is no OS run-time support for decryption We need to perform a single-block RSA decryption using HGpriv to decrypt the 128-bit AES memory encryption key SK On the other hand, we need fast AES implementation to encrypt the whole memory (e|,Non-data,50
