 6303. DATASETS Our Internet-wide study of key sharing in the HTTPS ecosystem is driven by four datasets: SSL certificates We use SSL certificates from full IPv4 scans as the basis of our measurements,Data,0
| Our SSL scans [30] also contain information on the IP address(es) that advertised each certificate. To obtain in- formation about the entity that controls this IP address, we use full IPv4 reverse DNS scans [29] that are also conducted by Rapid7|,Data,0
| Each AS is assigned an AS Number (ASN): for example, MIT is AS 3 and the Chicago Public Schools are AS 1416 [26]. CAIDA collects and publishes mappings between IP addresses and ASNs via their Route- Views datasets [7]|,Data,0
| For example, AT&T owns 160 unique ASNs. To aggregate these, we use CAIDA’s AS- to-Organization dataset [8] to group together ASes owned by the same organization|,Data,0
| For that, we rely on WHOIS [12], a protocol for querying domain registrars to obtain data on the domain owner. In practice, WHOIS data often contains fields such as the con- tact information for the owner of the domain, the contact for technical issues, where to send abuse complaints, and so on|,Data,0
| Here, we expand upon these prior findings by evaluating whether there is a correlation between centralized management and the quality of the keys chosen. Figure 13 compares several different features of self- managed and outsourced certificates across our entire cor- pus of leaf certificates (3,275,635 self-managed and 1,781,962 outsourced): (a) Key lengths in self-managed certificates are nearly identical to those managed by third-party hosting providers|,Data,0
|1 Combining Packet Capture (PCAP) Files The data set used in this study is a combination of the packet capture files obtained from two main sources. First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour|,Data,1
| First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour. The normal and non-malicious data is obtained from PREDICT internet data set repository [18] under the category of “DARPA Scalable Network Monitoring (SNM) Program Traffic”|,Data,1
| The data collection was performed during April 2016 using ZGrab, an application-layer scanner that operates with ZMap [15]. In the first phase, we performed an Internet-wide scan of all IPv4 addresses on port 500 to determine which hosts were configured 16This defect was corrected quite recently, years after the version of OpenSSL ScreenOS uses was written|,Data,6
|, download from an external source). Running on 71,000 articles collected from 45 leading technical blogs, this new approach demonstrates a remarkable performance: it gener- ated 900K OpenIOC items with a precision of 95% and a coverage over 90%, which is way beyond what the state-of-the-art NLP tech- nique and industry IOC tool can achieve, at a speed of thousands of articles per hour|,Data,7
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
 5. ANALYSIS AND FINDINGS In the following we use the extensive documentation of the 61 minimal exploits to provide insight into how attackers use specific vulnerabilities and features of the Java platform to implement their attacks,Data,11
| We run our event analysis on the top 100 free applications in the Android application store to determine how often this happens. In total, our analysis finds 1060 errors across 88 of the top 100 applications (10|,Data,12
| To our knowledge, AUTOREB is the first work that explores the user review information and utilizes the review semantics to predict the risky behaviors at both review-level and app-level. We crawled a real-world dataset of 2, 614, 186 users, 12, 783 apps and 13, 129, 783 reviews from Google play, and use it to comprehensively evaluate AUTOREB|,Data,14
| 4.1 Data collection For each team, we collected a variety of observed and self- reported data|,Data,16
| To demonstrate this, we scraped greatfire.org for websites in the top 1000 Alexa websites that are blocked by the GFW|,Data,18
| (cid:15) Identifying New Vulnerabilities. Our tool successfully an- alyzed 1,591 service interfaces of all the 80 system services in Android 5|,Data,19
| To understand the scope and magnitude of this new XARA threat, we developed an ana- lyzer for automatically inspecting Apple apps’ binaries to deter- mine their susceptibility to the XARA threat, that is, whether they perform security checks when using vulnerable resource-sharing mechanisms and IPC channels, a necessary step that has never been made clear by Apple. In our study, we ran the analyzer on 1,612 most popular MAC apps and 200 iOS apps, and found that more than 88|,Data,24
| To assist software developers (or secu- rity analysts) in tracking down a memory corruption vulnerability, CREDAL also performs analysis and highlights the code fragments corresponding to data corruption. To demonstrate the utility of CREDAL, we use it to analyze 80 crashes corresponding to 73 memory corruption vulnerabilities archived in Offensive Security Exploit Database|,Data,25
| These techniques may be applicable in other scenarios. We implemented and evaluated the attacks against the popular Gmail and Bing services, in several environments and ethical experiments, taking careful, IRB-approved mea- sures to avoid exposure of personal information|,Data,26
|, CSPAutoGen can handle all the inline and dynamic scripts. We have implemented a prototype of CSPAutoGen, and our eval- uation shows that CSPAutoGen can correctly render all the Alexa Top 50 websites|,Data,27
| 5. EXPERIMENTAL RESULTS This section reports on our evaluation of the moments ac- countant, and results on two popular image datasets: MNIST and CIFAR-10|,Data,28
| 6.1 Mobility Trace Dataset We use the CRAWDAD dataset roma/taxi [2, 3] for our simu- lations|,Data,31
 6.1 Evaluation We evaluated the performance of Σoφoς using 4 data sets of increasing size and also the English Wikipedia,Data,33
|1 Datasets, Metrics, Competitors & Settings Datasets. We test EpicRec on two real-world datasets: MovieLens1: a movie rating dataset collected by the Grou- pLens Research Project at the University of Minnesota through the website movielens|,Data,36
| 1 http://grouplens.org/datasets/movielens 188Yelp2: a business rating data provided by RecSys Chal- lenge 2013, in which Yelp reviews, businesses and users are collected at Phoenix, AZ metropolitan area|,Data,36
| The number of movie categories is 18. We use the MovieLens- 1M, with 1000,209 ratings from 6,040 users on 3,883 movies|,Data,36
| Our goal is to show that an ad- versary can insert an unbounded number of Sybil identities in the SybilLimit protocol, breaking its security guarantees. For our evaluation, we consider a real-world Facebook inter- action graph from the New Orleans regional network [28]|,Data,38
| We utilize these papers to extract Android malware behaviors and to construct the semantic network. From the electronic proceedings distributed to conference participants, we collect the papers from the IEEE Sympo- sium on Security and Privacy (S&P’08–S&P’15)4, the Com- puter Security Foundations Symposium (CSF’00–CSF’14), and USENIX Security (Sec’11)|,Data,39
 We conduct experiments on two publicly available set-valued datasets. • AOL search log dataset [1],Data,45
 90% of the users have fewer than 84 keywords in their logs. • Kosarak dataset [2],Data,45
 We select one month of data for our study. The data logs we used are col- lected from more than 30 machines with various server mod- els and operating systems,Data,46
| This paper rigorously investigates how users’ security beliefs, knowledge, and demographics corre- late with their sources of security advice, and how all these factors influence security behaviors. Using a carefully pre- tested, U|,Data,48
 We have ported Valgrind to iOS and implemented a prototype of iRiS on top of it. We evaluated iRiS with 2019 applications from the official App Store,Data,54
| from manufacturing equipment, as shown in Figure 1. We capture the relevant sensor data by deliberately or accidentally placing an attack-enabled phone close to, on top of, or inside a piece of manu- facturing equipment while the machinery is fabricating the target object|,Data,55
| Our new metric helps us compare in a fair way previously proposed attack-detection mechanisms. (ii) We compare previous attack-detection proposals across three di↵erent experimental settings: a) a testbed operating real-world systems, b) network data we collected from an operational large-scale Supervisory Control and Data Acqui- sition (SCADA) system that manages more than 100 Pro- grammable Logic Controllers (PLCs), and c) simulations|,Data,57
| Evaluation. We ran Oyente on 19, 366 smart contracts from the first 1, 460, 000 blocks in Ethereum network and found that 8, 833 contracts potentially have the documented bugs|,Data,58
| First, we consolidate the eight origin-exposing vectors into one auto- mated origin-exposing system called Cloudpiercer. Then, we assemble a list of clients from five CBSP companies by studying their DNS configurations and obtaining their adop- tion rate across the Alexa top 1 million websites|,Data,59
| The vast majority of them were exposed through their A record, indicating a brief dis- abling of the protection system. SSL certificate exposure In order to find IP addresses hosting SSL certificates associ- ated with the domains in the evaluation set, we made use of the publicly available data of Rapid7’s Project Sonar [42]|,Data,59
| 4. LARGE-SCALE ANALYSIS To assess the magnitude of the origin-exposure problem, we conduct a large-scale analysis in which we attempt to uncover the origin of CBSP-protected domains|,Data,59
|1 Dataset Description The dataset was first presented and used by Keller et al. in [23], and is publicly available in the gene expression om- nibus (GEO) database under reference GSE61741|,Data,61
| Although the cost of stor- age and processing have diminished, the cost of maintaining reliable infrastructure for transaction logs is still noticeable. Figure 1: A plot of transaction fee versus frequency for 1 million transactions in May 2015|,Data,65
| To estimate the cost of producing the preprocessing data (multiplication triples, random bits etc.), we used figures from the recent MASCOT protocol [31], which uses OT ex- tensions to obtain what are currently the best reported triple generation times with active security|,Data,67
| In this section, we validate whether the smartphone’s acoustic data can be utilized to deduce the movements. To conduct the validation, we implement an application on Nexus 5 (Android OS v6|,Data,68
| As seen in Table 4, we found that about half of the servers in Alexa’s top 10 support a large number of requests without rekeying. For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client|,Data,72
| For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client. We identified 11483 different HTTPS servers11, and found that 226 of them (1|,Data,72
| In this paper, we study the possible techniques to detect and measure this fraud and evaluate the real impact of OTT bypass on a small European country. For this, we performed more than 15,000 test calls during 8 months and conducted a user study with more than 8,000 users|,Data,78
|, the server cannot learn their relative order) after some number of queries are performed over real-world data. Specifically, we ran an experiment where we inserted over 2 million public employee salary figures from [1] and then performed 1000 random range queries|,Data,79
| In this study, we are interested in finding answers to security- and privacy-related questions about libraries, such as “How prevalent are third- party libraries in the top apps and how up-to-date are the library versions?”, “Do app developers update the libs included in their apps and how quickly do they update?”, or “How prevalent are vulnerabilities identified in prior research [28, 9] in libraries and how many apps are affected?” To answer these questions, we first built a comprehensive repository of third-party libraries and applications (see Section 5). Our library set contains 164 libraries of different categories (Ad- vertising, Cloud,|,Data,84
|) and a total of 2,065 versions. We then collected and tracked the version histories for the top 50 apps of each category on Play between Sep 2015 and July 2016, accumulating to 96,995 packages from 3,590 apps|,Data,84
|6.1, we found in our sample set 360 affected packages from 23 distinct apps, when only considering exact library matches|,Data,84
|15 for Android, which contained an account hijacking vulnerability, on 06/11/2014. In the histories of our sample set apps, we discovered, in total, 394 affected packages from 51 distinct apps, when only considering packages with exact matches of the vulner- able lib version|,Data,84
| We used LibScout to detect the affected application packages in our data set. In total 2,667 app versions of 296 distinct apps with a cumu- lative install-base of 3|,Data,84
| We observed that there is a significant variance in ACFG size. To reduce the sampling bias, we first collect a dataset which covers ACFGs of different functions from various architec- tures|,Data,89
| This dataset was used for base- line comparison, and all functions in this dataset has known ground truth for metric validation. We prepared this dataset using BusyBox (v1|,Data,89
| Dataset II – Public dataset. Recent work such as Pewny et al [45] and Eschweiler et al [23] used the same public dataset based upon two publicly-available firmware images for baseline comparison [7, 8]|,Data,89
| Dataset III – Firmware image dataset. This dataset of 33,045 firmware images was collected from the wild|,Data,89
| As a result, we created a freely available vulnerability database for this effort and for the broader research community. To build this database, we mined official software websites to collect lists of vulnerabilities with the corresponding CVE num- bers|,Data,89
| We selected OpenSSL for demonstration, since it is widely used in IoT devices. The resulting vulnerability database includes 154 vulnerable functions|,Data,89
| Roughly speaking, our measurement methods can be divided into two kinds: those that could be fully automated and scaled eas- ily, and those that required some manual interaction. For the latter, we used a set of 302938 major email providers and email genera- tors, while for the former, we used a much larger set of a million popular providers occurring in the Adobe leak and the Alexa top million Web sites (as potential email generators)|,Data,90
1.2 Provider List We created the set of popular email providers based on the top 1 million email address domains occurring in the leaked Adobe user data set of September 2013,Data,90
| Using a combination of mea- surement techniques, we determine whether major providers sup- ports TLS at each point in their email message path, and whether they support SPF and DKIM on incoming and outgoing mail. We found that while more than half of the top 20,000 receiving MTAs supported TLS, and support for TLS is increasing, servers do not check certificates, opening the Internet email system up to man- in-the-middle eavesdropping attacks|,Data,90
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
| 3.1 Datasets We use two major types of datasets: (1) packet-level traffic traces collected at various locations in a campus network, and (2) packet-level traces for Tor Pluggable Transport traffic collected in controlled environments|,Data,91
 Evaluation: local mixing time in social graphs. We use 10 various large-scale real-world social network topolo- gies that mainly come from the Stanford Large Network Dataset Collection [23] and other sources [45] to evaluate the local mixing time for nodes in social graphs,Data,92
| Feature Functions and Weights. To learn all feature functions and weights, we downloaded 1784 non-obfuscated Android applications from F-Droid [3], a popular repository for open-source Android applications|,Data,93
2.2 Experiments with Malware Samples We randomly selected one sample from each of the 49 mal- ware families reported in [40],Data,93
1_r1). Apps in our dataset used for the case study are downloaded from the Google official market (Google Play) in May 2016,Data,95
| • Using SInspector, we perform the first study of Unix domain sockets on Android, including the categoriza- tion of usage, existing security measures being en- forced, and common flaws and security implications. We analyze 14,644 apps and 60 system daemons, find- ing that 45 apps, as well as 9 system daemons, have vulnerabilities, some of which are very serious|,Data,98
| We presented SInspector, a tool for discovering potential security vulnerabilities through the process of identifying socket addresses, detecting authen- tication checks, and performing data flow analysis on na- 90tive code. We analyzed 14,644 Android apps and 60 system daemons, finding that some apps, as well as certain system daemons, suffer from serious vulnerabilities, including root privilege escalation, arbitrary file access, and factory reset- ting|,Data,98
| Our results show that many of our attacks succeed with a 100% chance such that the Sound-Proof cor- relation algorithm will accept the attacked audio samples as valid. Third, we collect general population statistics via an online sur- vey to determine the phone usage habits relevant to our attacks|,Data,100
 We find that the larger width of integer types and the increased amount of addressable memory introduce previously non-existent vulnerabilities that often lie dormant in program code. We empirically evaluate the prevalence of these flaws on the source code of Debian stable (“Jessie”) and 200 popular open-source projects hosted on GitHub,Data,104
| We have applied UniSan to the latest Linux kernel and Android kernel and found that UniSan can successfully prevent 43 known uninitialized data leaks, as well as many new ones. In particular, 19 of the new data leak vulnerabilities in the latest kernels have been confirmed by the Linux community and Google|,Data,107
| This allows us to prevent replay attacks, which are possibly the most applicable attack vectors against biometric authentication. Using a gaze tracking device, we build a prototype of our system and perform a series of systematic user experiments with 30 participants from the general public|,Data,108
| If two commits were blamed for the same amount of lines, blame both. Our heuristic maps the 718 CVEs of our dataset to 640 VCCs|,Data,109
| However, improving our blame heuristics further is an interesting avenue for future research. Apart from the 640 VCCs, we have a large set of 169,502 unclassified commits|,Data,109
|9 The SVM detected a high amount of excep- tions, a high number of changed code, inline ASM code, and variables containing user input such as __input and user. 6As previously mentioned we use the years 2011–2014 as the test dataset, since we have ground truth data on which to base the discussion|,Data,109
| When given a source file, Flawfinder returns lines with suspected vul- nerabilities. It offers a short explanation of the finding as well as a link to the Common Weakness Enumeration (CVE) database|,Data,109
| The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database|,Data,109
| Our results show that our approach significantly outperforms the vulner- ability finder Flawfinder. We created a large test database containing 66 C and C++ project with 170,860 commits on which to evaluate and compare our approach|,Data,109
 VoiceLive takes advantages of the user’s unique vocal system and high quality stereo recording of smartphones. • We conduct extensive experiments with 12 participants and three different types of phones under various ex- perimental settings,Data,111
| To test if WebCapsule can successfully record and subsequently replay real-world phishing attacks, we proceeded as follows, us- ing Chromium on our desktop machine. We selected a large and diverse set of recently reported phishing web pages from Phish- Tank8|,Data,112
| 2.4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime|,Data,113
|4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime. The dataset Cal represents the latitude and longitude of about 21,000 intersections in the California road network1 (also used by Mavroforakis et al|,Data,113
294258. The dataset SpitzLoc consists of latitude and longitude coordinates tracking the movement of German Green party politician Malte Spitz over six months,Data,113
| In this section, we aim to explore whether the differences of keystroke wave- forms are large enough to be used for recognizing different keys inputs in the real-world setting. We collected training and testing data from 10 volunteers|,Data,114
 B. Real Attacks MAD uniformly detects attacks more quickly than the PAD; we use the former method to detect the presence of an attack in real Internet traces3,Data,119
 III. DATA SET  changes  The data used was the PREDICT ID USC-Lander!  (- 60  The total  were DNS attack packets,Data,120
|395326000  files IPs. There are total 59,928,920 packet counts out of which there was a total of  DoS_DNS_amplification-20130617 (2013-06-17) (2013-06-17) with anonymized million) 358019 DNS packets|,Data,120
| The maximum number of unique hosts per day we measured was 106,000. To understand these differences, we compared the observations from our network monitor to data collected from DShield (www|,Data,121
| 3.1 From our own transactions We engaged in 344 transactions with a wide variety of services, listed in Table 1, including mining pools, wallet services, bank ex- changes, non-bank exchanges, vendors, gambling sites, and mis- cellaneous services|,Data,122
| Wallets. We kept money with most of the major wallet services (10 in total), and made multiple deposit and withdrawal transac- Bank exchanges|,Data,122
|, in which the exchange rate is not fixed) also function as banks. As such, we tagged these services just as we did the wallets: by depositing into and withdrawing from our accounts (but rarely par- ticipating in any actual currency exchange)|,Data,122
|info/tags, including both addresses provided in users’ signatures for Bitcoin forums, as well as self-submitted tags. We collected all of these tags — over 5,000 in total — keeping in mind that the ones that were not self-submitted (and even the ones that were) could be regarded as less reliable than the ones we collected ourselves|,Data,122
| 3.1 Data analysis overview We use three data sets, summarized in Table 1|,Data,123
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
| We also describe our application of the technique to the IPv6 interface-level graph captured by CAIDA’s Archipelago (Ark) infrastructure [14] for March 2013. The graph consists of all the 52,986 IPv6 interfaces numbered within the 2000::/3 unicast prefix captured from all 27 Ark vantage points (VPs) with IPv6 connectivity|,Data,125
| cause the counters of distinct routers to diverge, and (4) confirm aliases with pairwise probing. Given the absence of velocity in ID counters and the large probes required for the technique to work, we probe at a low rate of 20pps from a single VP, producing 26Kbps of traffic|,Data,125
| 3. METHODOLOGY In this section, we describe the design of our experiment and our data collection methodology, as well as the mitigating steps and proactive measurements we conducted to ensure a minimal im- pact of our covering routes|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| Our IPv6 network telescope results suggest sev- eral important differences (and some similarities) compared to that body of work. To produce a more recent and valid comparison, we analyzed a single week of IPv4 background radiation captured during the course of our ongoing IPv6 packet capture|,Data,126
| 4. DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1|,Data,127
| DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1. Our primary dataset consists of changes made to the |,Data,127
| domains, (2) the removal of existing domains, and (3) changes to existing domains in terms of revisions to their associated name- servers. Our data includes captures of the DNZA files as recorded every five minutes, time periods we refer to as epochs|,Data,127
| Since we lack comprehensive ground truth regarding the ultimate use of domains, to this end we use two proxies: subsequent appearance of a newly registered do- main in: (1) an email spam campaign, or (2) a domain blacklist. For the first of these, we operated a spam trap, i|,Data,127
|com), by restricting our focus to domains recently registered (March–July 2012) we can filter down the do- mains appearing in the spam trap to those very likely used for spam- ming. For the second, we subscribed to three major DNS blacklists, URIBL, SURBL, and Spamhaus DBL|,Data,127
| In this paper, we examine the effectiveness of these inter- ventions in the context of an understudied market niche, counterfeit luxury goods. Using eight months of empirical crawled data, we identify 52 distinct SEO campaigns, document how well they are able to place search results for sixteen luxury brands, how this ca- pability impacts the dynamics of their order volumes and how well existing interventions undermine this business when employed|,Data,128
| For a small number of stores, we were also able to collect user traffic data that directly measures an SEO campaign’s effectiveness in attracting customers to their stores. Specifically, we were able to periodically collect AWStats data for 647 storefronts in 12 cam- paigns|,Data,128
| One issue that undermines coverage is that Google only labels the root of a Web site as “hacked”, and does not label search results that link to sub-pages within the same root domain. In the PSR data set, we found 68,193 “hacked” search results|,Data,128
| We begin by exam- ining the properties of individual darknets and in particular the behavior of source IP addresses. We provide these char- acterizations by looking at data from 14 darknet monitors ranging in size from a /25 monitor to a /17 monitor over a period of 10 days between August 18, 2004 and August 28, 2004|,Data,129
| Figure 10: The number of darknets (of 31) reporting a port in the top 10 ports over a day, week, and month time frame. The analysis is performed for the top 10 destination ports over a day, top 10 destination ports over a week, and top 10 destination ports over a month|,Data,129
| 3.6 Datasets This paper uses DNS datasets from three authorities: one national-level top-level domain, operators of two root servers as shown in Table 1|,Data,130
 JP-DNS operates the .jp country code domain for Japan; we have data from all seven of their anycast sites,Data,130
|) part of the 2014 DITL collection [16] (for B-Root, shortly after 2014 DITL). We also use data for M-Root’s 2015 DITL collection (§ 4|,Data,130
 These root datasets are available to re- searchers through DNS-OARC. For longitudinal analysis we draw on 9 months of data taken at the M-Root server,Data,130
| However, we treat the union of these classes together. We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness|,Data,131
| We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness. The dataset consists of all echo requests that were sent as part of the surveys in this period, as well as all echo responses that were received|,Data,131
|, “host unreachable”); we ignore all probes as- sociated with such responses since the latency of ICMP error responses is not relevant. In later sections, we will complement this dataset with results from Zmap [5] and additional experiments includ- ing more frequent probing with Scamper [13] and Scrip- troute [22]|,Data,131
| 3.2 Milking 3 Methodology To collect the information needed to cluster servers into oper- ations, we have built an infrastructure to track individual exploit servers over time, periodically collecting and classi- fying the malware they distribute|,Data,132
 2. We receive feeds of drive-by download URLs (Sect,Data,132
 2. CHARACTERISTICS OF CHECK-INS We use three different datasets that capture human mobility,Data,133
 First we consider two online location-based social networks. We col- lected all the public check-in data between Feb,Data,133
| There are 196,591 nodes, 950,327 edges in Gowalla and 58,228 nodes, 214,078 edges in Brightkite. To ensure that our observations on human movement are not specific to data based on check-ins from location-based social net- works, we also include a dataset of cell phone location trace data|,Data,133
| Backscatter DDoS is a commonly seen behaviour in darknets where the attacker uses simultaneous bots to generate the actual attack packets to reach the targeted (original) victim. In our study, five publicly available network traffic datasets from CAIDA’s archives are employed|,Data,134
| Datasets Employed In this research, five publicly available real-life network traffic traces (datasets) from CAIDA’s archives are employed. Three of them, which were captured by a passive darknet in 2007, 2008 and 2012 [27][26][28], namely UCSD Network Telescope [21], include mostly one-way malicious traffic while the remaining ones collected in 2008 [29] and 2014 [30] via CAIDA’s Internet backbone links include only normal traffic|,Data,134
| 3 Approach This section presents our approach for the evalua- tion of reputation based blacklists. We evaluated the blacklists by deploying them in a large academic net- work of over 7,000 hosts|,Data,135
| This was a preliminary step to preventing inexperienced and non-serious workers from participating in our survey. Our survey is based on the participants’ actual check-ins on Foursquare posted over the last 24 months (that we collected through a specific application we developed), and it requires a significant amount of time to complete (30-45 minutes)|,Data,136
| The third phase of worm activ- ity is the persistence phase which for the Blaster worm has continued through 2004. In this one-week period of measurement, the IMS system observed over 286,000 unique IP addresses displaying the characteristics of Blaster activity|,Data,137
| published a study in 2011 that focused on the dynamics of leaf cer- tificates and the distribution of certificates among IP addresses, and attempted to roughly classify the overall quality of served certifi- cates. The study was based on regular scans of the Alexa Top 1 Mil- lion Domains [1] and through passive monitoring of TLS traffic on the Munich Scientific Research Network [17]|,Data,138
| Our study is founded on what is, to the best of our knowledge, the most comprehensive dataset of the HTTPS ecosystem to date. Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443|,Data,138
| Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443. Over the course of 14 months, we completed upwards of 400 billion SYN probes and 2|,Data,138
| Content Provider e Service Provider v i t c e p s r e P Content Consumer Addressing Prerequisite IP Functions Routing Naming A1: Address Allocation; A2: Address Advertisement N1: Nameservers; R1: Server Readiness N2: Resolvers N3: Queries A2: Address Advertisement; T1: Topology End-to-End Reachability R1: Server Readiness Operational Characteristics Usage Profile Performance U3: Transition Technologies U1: Traffic Volume; U3: Transition Technologies P1: Network RTT R2: Client Readiness U2: Application Mix; N3: Queries Table 2: Dataset summary showing the time period, scale, and public or new status of the datasets we analyzed. Dataset RIR Address Allocations Routing: Route Views Routing: RIPE Google IPv6 Client Adoption Verisign TLD Zone Files CAIDA Ark Performance Data Arbor Networks ISP Traffic Data Verisign TLD Packets: IPv4 Verisign TLD Packets: IPv6 Alexa Top Host Probing Time Period Metrics Jan 2004 – Jan 2014 A1 Jan 2004 – Jan 2014 A2, T1 Jan 2004 – Jan 2014 A2, T1 Sep 2008 – Dec 2013 R2, U3 Apr 2007 – Jan 2014 N1 P1 Dec 2008 – Dec 2013 U1, U2, U3 Mar 2010 – Dec 2013 Jun 2011 – Dec 2013 N2, N3 N2, N3 Jun 2011 – Dec 2013 Apr 2011 – Dec 2013 R1 Recent Scale ≈18K allocation snapshots (5 daily) 45,271 BGP table snapshots millions of daily global samples daily snapshots of ≈2|,Data,139
com & .net) ≈10 million IPs probed daily ≈33-50% of global Internet traffic; 2013 daily median: 50 terabits/sec (avg,Data,139
| To put the IPv6 allocation data in context, Figure 1 also shows IPv4 prefix allocations over the same period. The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled|,Data,139
| There were less than 30 IPv6 prefixes al- located per month prior to 2007, generally increasing thereafter. In the past several years, we typically find more than 300 prefixes allocated per month, with a high point of 470 prefix allocations in February 2011|,Data,139
| The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled. 1 Overall, we find nearly 69K IPv4 prefix allocations at the beginning of our dataset and just over 136K at the end|,Data,139
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| Table 1 shows the datasets we use in our paper. We use two ICMP surveys taken by USC [12]: IT17ws and IT16ws; IT17ws is the main dataset used in this paper, while we use IT16ws for validation in Section 6|,Data,142
2. We collected VUSC s at our enterprise in order to compare our inferences with network operators as discussed in Section 6,Data,142
| # of Data-Oriented Attacks gives the number of attacks generated by FLOWSTITCH, includ- ing privilege escalation attacks and information leakage attacks. FLOWSTITCH generates 19 data-oriented attacks from 8 vulnerable programs|,Data,144
| Third, this method is not specific to C or C++, and can be applied to any programming language. We collected C++ source of thousands of contestants from the annual international competition “Google Code Jam”|,Data,145
| Finally, we analyze various attributes of programmers, types of programming tasks, and types of features that appear to influence the success of attribution. We identified the most important 928 fea- tures out of 120,000; 44% of them are syntactic, 1% are layout-based and the rest of the features are lexical|,Data,145
|3.1ScalingWecollectedalargerdatasetof1,600programmersfromvariousyears|,Data,145
| ) s y a D n i (    e m T i  7  6  5  4  3  2  1  10  20  30  40  50  60  70  80  90 Time Before Accounts Suspension Number of IP Addresses 2 Motivation: Analysis of Malicious Activ- ity on a Webmail Service We want to understand the way in which cybercrimi- nals abuse accounts on online services, to identify weak points that we could leverage for detection. To this end, we observed the email-sending activity on a large web- mail service|,Data,147
| Following accepted frameworks for qualitative research [18, 30, 35], we focus closely on a small number of participants. We interviewed 15 journalists employed in a range of well-respected journalistic institutions in the United States and France, analyzing these interviews using a grounded theory approach [18, 30]|,Data,146
| 3.1 Datasets We examine 13,345 passwords from four sets created under composition policies ranging from the typical to the currently less common to understand the suc- cess of password-guessing approaches against passwords of different characteristics|,Data,149
| Had we used any major password leak, their analysts would have already been familiar with most or all of the passwords contained in the leak, biasing results. The passwords in these sets were collected using Ama- zon’s Mechanical Turk crowdsourcing service|,Data,149
| The decision for or against pinning is always a trade- off between increasing security and keeping mainte- nance efforts at an acceptable level. In this paper, we present an extensive study on the applicability of pinning for non-browser software by analyzing 639,283 Android apps|,Data,152
| Therefore, we instrument telemetry data from a popular anti-virus software provider. We evaluate the update behaviour of 871,911 unique users from January 2014 to December 2014 and find that only 50% of the users update to a new app version within the first week after release|,Data,152
| Developer View Although pinning is only ap- plicable in relatively few cases, the nominal-actual comparison leaves room for improvement. We there- fore collected feedback from 45 developers of apps for which we would recommend pinning|,Data,152
| Section 4). Altogether we found 20,020,535 calls to network related API calls (cf|,Data,152
| Instability of the routes to the sensor address space can also result in reachability problems, especially given that route flap damping can be triggered during convergence to suppress unstable routes [9]. Using the BGP updates data from RouteViews BGP monitor, we studied the availability of the routes to the sensor blocks in our de- ployment from a large set of ASes|,Data,154
| This section probes these differences using three successively more specific views of traffic to a network of distributed blackhole sensors. The data was recorded over a one month period with SYN responders on TCP port 135, 445, 4444, and 9996 across all sen- sors|,Data,154
|  V. EXPERIMENT RESULTS  In this section, we mainly focus on how our router-to-AS Mapping method and other baseline methods behave on global router-level topology, as discussed above, we use PeeringDB data as ground truth, and apply clustering method on global topology based on CAIDA ITDK project|,Data,155
| It describes the properties that a dataset should have in order to be used for comparison purposes. The dataset used in the paper includes an IRC-based Botnet attack1, but the bot used for the attack was developed by the authors and therefore it may not represent a real botnet behavior|,Data,156
| This dataset may be downloaded with authorization. The Protected Repository for the Defense of Infrastructure Against Cyber Threats (PRE- DICT) indexed three Botnet datasets2 until May 16th, 2013|,Data,156
 None of them are labeled. A custom botnet dataset was created to verify five P2P botnet detection algorithms in Saad et al,Data,156
| Unfortunately, there is only one infected machine for each type of botnet, therefore no synchronization analysis can be done. The Traffic Laboratory at Ericsson Research created a normal dataset that was used in Saad et al|,Data,156
 This is the only normal dataset that is labeled inside the pcap file. A considerable amount of malware traffic in pcap format was published in the Contagio blog9,Data,156
| But since each scenario includes only one infected computer, it should be possible to label them. Another dataset with malware logs and benign logs was collected in NexGinRC (2013)|,Data,156
 Access to this dataset may be granted upon request10. The last dataset analyzed is currently created by the MAWI project described in Cho et al,Data,156
| Methodology and datasets We deployed Paris Traceroute with its Multipath Detection Algorithm (MDA) [29] enabled in 90 PlanetLab nodes. We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]|,Data,158
| We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]. Our dataset contains more than 900 thousand IP-level (multi)routes and 324,313 IP addresses|,Data,158
1 3.1 Address Allocation and BGP Data We analyzed BGP announcements captured by all collectors (24 collectors peering with 184 peers) of the Routeviews [3] and RIPE RIS [52] projects,Data,159
| For each /24 block, we computed the maximum number of peers that saw it reachable at any time within the full observation period of 92 days. To determine which address blocks are available for assignment, we used a dataset compiled by Geoff Hus- ton [23], which merges the extended delegation files from the 5 RIRs [4, 6, 7, 41, 51] with IANA’s published registries [31–36]|,Data,159
| SWITCH. We collected unsampled NetFlow records from all the border routers of SWITCH, a national aca- demic backbone network serving 46 single-homed uni- versities and research institutes in Switzerland [55]|,Data,159
| R-ISP. We collected per-flow logs from a vantage point monitoring traffic of about 25,000 residential ADSL customers of a major European ISP [21]|,Data,159
 UCSD-NT. We collected full packet traces from the /8 network telescope operated at the University of Cal- ifornia San Diego [1],Data,159
| IXP. Our fourth VP is a large European IXP inter- connecting more than 490 networks, exchanging more than 400 PB monthly [5]|,Data,159
|3 Active Measurements ISI. We used the ISI Internet Census dataset it55w- 20130723 [37], obtained by probing the routed IPv4 address space with ICMP echo requests and retaining only those probes that received an ICMP echo reply from an address that matched the one probed (as rec- ommended [38])|,Data,159
| HTTP. We extracted IP addresses from logs of Project Sonar’s HTTP (TCP port 80) scan of the entire IPv4 address space on October 29, 2013 [24]|,Data,159
| Definitions of graph parameters measuring metric tree-likeness of a graph, as well as notions and notations local to a section, are given in appropriate sections. 3 Datasets Our datasets come from different domains like Internet measurements, biological datasets, web graphs, social and collaboration networks|,Data,160
| The experiments were executed as follows. Traces were col- lected by using ICMP, UDP, and TCP Traceroute to probe the paths to a set of 100 destination websites from a source located on the Pennsylvania State University, University Park campus|,Data,161
| For UDP and TCP Traceroute, traces were collected using the default destination port numbers. We also collected traces using other ports and observed similar results|,Data,161
| Realistic Networks Here we compare the merged topologies produced by iTop, MN, and Isomap for realistic topologies. We use the Au- tonomous System (AS) topologies from both the Rocketfuel [20] and the CAIDA [21] projects, which represent IP-level connections between backbone/gateway routers of several ASes from major Internet Service Providers (ISPs) around the globe|,Data,161
| Although the paris-traceroute output of ITDK is more reliable than that of IPlane’s traceroute, the random selection of endpoints implemented by CAIDA hinders the collection of routes between the same vantage- and endpoints. Therefore we used the data of IPlane’s traceroute measurements|,Data,162
| They can also be used for constructing maps of the Internet at the Autonomous Systems level [, ]. In this work we used the CAIDA router-level Internet map from October th,  []|,Data,163
| 3 Table 1: Dataset Description Name BGP Usage AS Geolocation; Detour Detection Date 2016-01 Sources Info RouteViews, RIPE 38,688 RIBS, 416 peers, RIS 30 countries, 55GB Infrastructure IP List AS Geolocation 2016-01 to 2016-03 CAIDA Ark, iPlane, OpenIPMap, RIPE Atlas Measurements 3M Router IPs Infrastructure IPs to AS Mapping Infrastructure IP geolocation 2015-08 CAIDA ITDK, iPlane 6.6M IP to AS mappings AS to IXP Mapping AS Relationship AS Geolocation 2016-01 to 2016-03 Filtering peered paths from detection 2016-01 Traceroute Detour Validation 2016-05-01 IXP websites, PeeringDB, PCH CAIDA AS Relationship RIPE Atlas MaxMind Prefix Geolocation; Detour Validation 2016-01, 2016-03 MaxMind GeoLite City (free and paid) 368 IXP websites crawled 482,657 distinct relationships Used by Netra, 163 traceroutes Paid version used only for geolocating infrastructure IPs and detour validation longest prefix match on the global routing table and map the IP to the AS announcing the longest matching prefix|,Data,164
| As shown in Figure 3, we install LaBrea on a /29 subnetwork and use PlanetLab [9] to probe from multiple vantage points the entire /24 aggre- gate to which the /29 belongs. We scan the /24 network by attempting to establish TCP connections to each IP address in the subnet and capture the packets for further analysis|,Data,165
| • Active IPs in a Subnet: Intuitively, we might ex- pect high-occupancy subnets to be good indicators of pos- sible tarpits. To this end, we initially investigated using a hitlist of probable tarpits as inferred from the /24 subnets with more than 240 responding web hosts in the scans|,Data,165
| To facilitate large-scale scanning and avoid triggering anomaly detectors, degreaser uses permu- tation scanning [7, 12] to pseudo-randomly iterate through the IP address space when probing. Our real-world Internet scan, which probes at least one address in each /24 network in the Internet, discovers 107 different tarpit subnetworks (cid:20)(cid:24)(cid:25) ranging in size from /16 (with up to 216 fake hosts) to /24 (with up to 28 fake hosts)|,Data,165
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
| • Discovering correlations between anomalous traffic types detected with deep inspection techniques and traffic feature entropy variations. • Providing a traffic-type dissection (in-depth and entropy based) of a representative portion of the IBR for three weeks of April, 2012, with a 10-minute time scope|,Data,167
 Following is the summary of information about these data sets:  1. Data set from PREDICT USA [24] which contains traces of a DNS distributed denial of service attack (DDOS),Data,168
  from optical  2. Data set from CAIDA USA [25] which contains internet internet connectivity from 2002 and 2003,Data,168
  3. Data set from our experiment in which a PCAP file is captured from a lab computer which is being used for browsing and software development for the cyber security project,Data,168
| The client has to be able to concurrently access all the fragments of the object to exhibit good performance when accessing large resources If there are many fragments, this requires to create and keep open a large number of connections with the server Use of DLOs The DLO service of Swift has been intro- duced to support the management of large objects, going beyond the size limits of storage devices and providing finer granularity in the access|,Non-data,66
| When using DLOs, an object is separated into a number of sub-objects that can be down- loaded with a single request The fragments of our approach can then be stored into separate DLO fragments The Swift server is responsible for the management of the mapping from an object to its fragments, splitting a request for down- loading an object into a number of independent requests to the server nodes that are responsible to store the data (the Swift architecture has a server node directly offering an interface to the clients and uses a number of independent storage nodes; this architecture provides redundancy and availability) In this way, the client only generates a single get request for the object, independently from the number of fragments|,Non-data,66
 The descriptor of the object can be extended with the representation of the version of each fragment A similar approach can be realized when the object service of- fers the flexibility to operate with get and put only on a portion of the object The major constraint of this approach is the need to wait for the download of all the fragments before the decryption of the first macro-block can start As anticipated in Sec- tion 5,Non-data,66
|1, this causes delays and requires the client to keep available in RAM the complete encrypted representation of the object before it can be processed To mitigate this prob- lem, fragments can also be split into sub-fragments In this way, the download will be organized with a serial down- load of all the sub-fragments representing the same set of macro-blocks This is consistent with approaches used in cloud storage, where there is a common guideline to split resources larger than a few GiB (Swift forces a split at 5 GiB in its standard configuration)|,Non-data,66
| Experiments confirm that beyond 1 GiB, the throughput remains stable even for configurations with a large number of fragments keep the analysis at the level of object to be consistent with the discussion in the paper 2We had to change a parameter in the server, to support a large number of fragments in the DLO mode 225) s (  e m i t  1000  100  10  1  0|,Non-data,66
1  001 number of fragments 1024 256 64 16 4 1 64KB 256KB 1MB 4MB 16MB 64MB 256MB 1GB number of fragments 1024 256 64 16 4 1  12  1  08  0,Non-data,66
6  04  02 ) s / B M (  t u p h g u o r h t  0 64KB 256KB 1MB 4MB 16MB 64MB 256MB 1GB object size object size Figure 10: Time for the execution of get requests on Swift Figure 11: Throughput for a workload combining get and put_fragment requests on Swift 52,Non-data,66
|1 Experiments on the Overlay solution We built a Swift client application in Python that imple- ments the get and put_fragment methods that character- ize our technique We followed two implementation strate- gies, one using fragments as atomic separate objects, and the other adopting the DLO support offered by Swift Figure 10 compares, for different numbers of fragments, the time required for the execution of get requests assum- ing to map each fragment to a separate object The lines correspond to distinct values for the number f of fragments (i|,Non-data,66
|e, 1, 4, 16, 64, 256, and 1024) The parameters that drive the performance are the network bandwidth and the over- head imposed by the management of each request For get requests, the overhead introduced by the management of one request for each fragment dominates when the resource is small, whereas the increase in object size makes the net- work bandwidth the bottleneck|,Non-data,66
| The profile of put requests uploading the complete resource proved to be identical to the profile of get requests using a single fragment The exe- cution of put_fragment requests grows linearly with the size of the fragment The identification of the best number of fragments re- quires to consider the profile of the scenario We evaluated the behavior of a system on a collection of 1000 objects where, after each put_fragment request, a sequence of 50 get requests were executed on objects in the collection, all of the same size|,Non-data,66
| Figure 11 reports the results of these ex- periments As objects become larger, the benefits of frag- mentation in the application of policy updates compensate for the overhead imposed on the retrieval of the objects It is to note that the performance of the solution that does not use our technique corresponds to the line with one fragment The throughput of the configurations using fragments is or- ders of magnitude higher already for medium-size objects|,Non-data,66
| The graph also shows that the best number of fragments depends on the resource size The identification of the value to use requires to consider the configuration of the system and the expected workload A second set of experiments followed the same approach, but considering the use of DLOs in Swift The number of fragments still has a significant impact on the performance of the get request, because the server has to generate in- ternally the mapping for the single request originating from the client and the multiple requests addressed to the stor- age nodes|,Non-data,66
| The application of the same workload considered for the experiments in Figure 10, which interleaves get and put_fragment requests, produces the results presented in Figure 12 Comparing the cost with and without DLO we notice a significant benefit deriving from the use of DLOs 53 Ad-hoc solution The use of an ad-hoc protocol is able to provide the full range of benefits of our approach|,Non-data,66
| The protocol will have to support the basic primitives to upload (put) and download (get) a resource The put primitive, when used to upload the initial state of the resource, will have to provide a re- source descriptor that defines: the identifier of the key k0 used by the owner to encrypt the resource; the size of mini- blocks and the number of fragments (which determine the size of the macro-block); an array with an element for every fragment describing its version In addition to the put prim- itive, the server will recognize the put_fragment primitive, which will allow the owner to update a fragment Parame- ters of this primitive, in addition to the resource identifier and fragment content, will be the identifier of the fragment and its version number|,Non-data,66
| The put_fragment primitive re- quires the authentication of the user issuing the request, in the same way as the put primitive The get primitive can return to the user the resource, one macro-block after the other The client will be able to immediately start the decryption of macro-blocks, after a preliminary decryption with key ki of the mini-blocks be- longing to the fragments at version i > 0 In this way, the client does not have to wait for the completion of the down- load of all the fragments|,Non-data,66
| The answer to the get request always provides first the resource descriptor, with the rep- resentation of the version of each of the fragments Among the parameters of the get primitive we have the option to retrieve only a specific portion of the resource For this solution, we have to dedicate attention to the mapping of the logical structure to the physical represen- tation of data At the logical level, the resource is divided into fragments, and the content is represented by a sequence of macro-blocks|,Non-data,66
| At the physical level, the resource can be stored as a collection of separate fragments or as a sequence of macro-blocks In addition to these two options, there is a range of intermediate alternatives, with the interleaved representation of multiple fragments 226 9  8  7  6  5  4  3  2  1 ) s / B M (  t u p h g u o r h t  0 64KB number of fragments 1024 256 64 16 4 1 cost get update ) s (  e m i t  1000  100  10  1  01 256KB 1MB 4MB 16MB 64MB 256MB 1GB 1 2 4 8 16 32 64 128 256 512 1024 object size number of fragment partitions Figure 12: Throughput for a workload combining get and put_fragment requests with Swift DLOs Figure 13: Configurations for physical blocks 5|,Non-data,66
|31 Experiments on the Ad-hoc solution The advantage of a dedicated server is the ability to use an efficient protocol The use of an ad-hoc server makes the management of fragments more flexible and avoids the overheads associated with the generation of a number of in- dependent get requests equal to the number of fragments that are produced by the Overlay solution Still, the use of a potentially large number of fragments can introduce non- negligible costs|,Non-data,66
| In the extreme case where a large resource is managed with a single macro-block (ie, the number of fragments corresponds to the number of mini-blocks of the whole resource) The client will have to wait the complete download to start decryption, and decryption will involve a high number of rounds|,Non-data,66
| Also, when only a portion of the resource is needed, our approach requires the client to down- load the macro-blocks that contain the portion of interest; if macro-blocks are large, this may lead to a significant over- head As already discussed, the identification of the optimal number of fragments has to consider several features of the application domain In the current technological scenario, we notice that the use of an ad-hoc server can support a number of fragments larger than what is adequate for the Overlay solution, but extreme values cause inefficiencies As mentioned above, an important aspect that the imple- mentation of the ad-hoc server has to consider is the map- ping from the logical structure to its physical representa- tion|,Non-data,66
| In this analysis, we will consider a traditional scenario where the server uses the functions of the operating system to access the storage ability of mass memory devices In the experiments we used the Amazon EC2 instance and its access to the Elastic Block Storage The operating system offers an interface that allows to read and write physical blocks, typically a few KiB in size The mapping of the bidimensional logical structure with macro-blocks and frag- ments to the concrete physical structure realized by a se- quence of physical blocks can follow several strategies|,Non-data,66
| To compare these alternatives, we assume a scenario where we have 1024 fragments and map the structure to 4KiB physi- cal blocks A first strategy consists in storing the resource one macro-block after the other The dual strategy consists in storing the resource one fragment after the other Be- tween these two extremes, we have strategies that split each macro-block into a number of parts and store contiguously into a physical disk block all the macro-block portions that correspond to the mini-blocks in the same position|,Non-data,66
| The ra- tionale is that the organization along macro-blocks will be the most efficient to support get requests, but it will require to access all the physical disk blocks when a put_fragment request is received The representation based on fragments will instead be the most efficient to support put_fragment requests, but it will introduce a significant overhead when managing get requests For small resources these aspects do not have a large impact, whereas for large resources the per- formance benefit can be significant Figure 13 illustrates the results obtained on a container with 1000 files, each of 1 GiB in size|,Non-data,66
| The horizontal axis denotes the number of shares of each macro-block (1 represents the strategy with the macro- blocks stored in sequence, and 1024 represents the strategy with fragments stored in sequence) For a workload that in- terleaves a get request for every put_fragment request, the total cost is minimized when we use a solution with 256 frag- ments Interestingly, the two extremes with this workload do not represent the best option In these experiments, we measured the time required to access the data from storage|,Non-data,66
| In most systems we expect the network to be the bottleneck that limits the performance and the choice of physical rep- resentation will rarely be observed by the clients, but the performance benefit that is shown by the experiment can lead to a more efficient implementation of the server 6 RELATED WORK The idea of making the extraction of the information content of an encrypted resource dependent on the avail- ability of the complete resource has been first explored by Rivest [16], who proposed the all-or-nothing transform (AONT) The AONT requires that the extraction of a re- source where n bits of its transformed form are missing should require to attempt all the possible 2n combinations|,Non-data,66
| The AONT can be followed by encryption to produce an all- or-nothing encryption schema In [16], the author proposes the package transform, which realizes an AONT by apply- ing a CTR mode using a random key k The ciphertext is then suffixed with the used key k xor-ed with a hash of all the previous encrypted message blocks In this way, a modification on the encrypted message limits the ability to derive the encryption key|,Non-data,66
| This technique works under the assumption that the user who wants to decrypt the resource has never accessed the key before, but fails in a scenario where the user had previously accessed the key and now the 227access must be prevented (ie, revocation of privileges on encrypted files) The user, in fact, could have stored key k and so she would be able (depending on the encryption mode used) to partially retrieve the plaintext|,Non-data,66
| Key k can be seen as a digest: it is compact and its storage allows a receiver to access the majority of the file, even if one of the blocks was destroyed Most approaches for efficient secure deletion [5, 8] rely on the fact that the key is a digest for a resource and its content can be securely deleted by deleting the specific disk location that stores a piece of information that permits to derive the key used to encrypt the resource Such approaches are already used by commercial storage devices [17] and recent proposals have considered the integration of such approaches with flexible policies [5] All these approaches are not appli- cable in our scenario, where the encrypted resource is stored on a server that does not have access to (and hence does not store) the key and it is the user who has to decrypt the resource|,Non-data,66
 Making the encryption key unavailable to the user does not limit her access Other approaches for enforcing access control in the cloud through encryption have been developed along two research lines: attribute-based encryption (ABE) and selective en- cryption approaches ABE approaches (eg,Non-data,66
|, [11, 13, 15, 18]) provide access control enforcement by ensuring that the key used to protect a resource can be derived only by the users that satisfy a given condition on their attributes (eg, age, role) The main shortcoming of these solutions is due to their evaluation costs (they rely on public key encryption), and to the hardness in the support of revocations [13, 18]|,Non-data,66
| Approaches based on selective encryption (eg, [6, 7, 12]) assume to encrypt each resource with a key that only au- thorized users know or can derive In this scenario, policy updates are then either managed by the data owner, with considerable overhead, or delegated to the server through over-encryption [6, 7]|,Non-data,66
| Although over-encryption guarantees a prompt enforcement of policy updates and demonstrates to offer good performance, it requires stronger trust assump- tions on the server, which must provide dedicated support On the contrary, our technique can be used also if the server is completely unaware of its adoption 7 CONCLUSIONS We presented an approach for efficiently enforcing ac- cess revocation on encrypted resources stored at external providers|,Non-data,66
| Our solution enables data owners to effectively revoke access by simply overwriting a small portion of the (potentially large) resource and is resilient against attacks by users locally maintaining copies of previously-used keys Our implementation and experimental evaluation confirm the efficiency and effectiveness of our proposal, which enjoys orders of magnitude of improvement in throughput with re- spect to resource re-writing, and confirms its compatibility with current cloud storage solutions, making it also imme- diately applicable to many application domains 9 |,Non-data,66
|ABSTRACT With the proliferation of Internet of Things, there is a grow- ing interest in embedded system attacks, eg, key extrac- tion attacks and firmware modification attacks Code execu- tion tracking, as the first step to locate vulnerable instruction pieces for key extraction attacks and to conduct control-flow integrity checking against firmware modification attacks, is therefore of great value|,Non-data,70
| Because embedded systems, espe- cially legacy embedded systems, have limited resources and may not support software or hardware update, it is impor- tant to design low-cost code execution tracking methods that require as little system modification as possible In this work, we propose a non-intrusive code execution tracking solution via power-side channel, wherein we represent the code ex- ecution and its power consumption with a revised hidden Markov model and recover the most likely executed instruc- tion sequence with a revised Viterbi algorithm By observing the power consumption of the microcontroller unit during ex- ecution, we are able to recover the program execution flow with a high accuracy and detect abnormal code execution be- havior even when only a single instruction is modified 1|,Non-data,70
| INTRODUCTION Embedded devices controlled by microcontroller units are deployed everywhere They are not only widely spread in our daily life with the proliferation of Internet of Things (IoT), but also extensively used in the global IT environ- ments and critical infrastructures Consequently, there is a growing interest in embedded system attacks and defense mechanisms What makes both, especially defense, difficult is the limited capability of code execution monitoring on embedded systems, mainly caused by limited I/O interfaces and constrained-resources|,Non-data,70
| This situation is unlikely to be alleviated any time soon by adding extract features, since updating embedded systems, especially legacy systems, is hindered due to safety or cost concerns Thus, in the paper, we design a method for code execution tracking of embed- ded systems without requiring software or hardware modifi- Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full cita- tion on the first page Copyrights for components of this work owned by others than ACM must be honored Abstracting with credit is permitted|,Non-data,70
| To copy otherwise, or re- publish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from permissions@acmorg CCS’16, October 24-28, 2016, Vienna, Austria c(cid:13) 2016 ACM|,Non-data,70
 ISBN 978-1-4503-4139-4/16/10   $15,Non-data,70
00 DOI: http://dxdoiorg/101145/2976749,Non-data,70
|2978299 cation Such a method enables us to answer two important security related questions: (1) At a given time, which in- struction in a code is being executed? (2) Given a source code, has it been modified and is the microcontroller unit (MCU) executing a malicious code? Here, we illustrate how to utilize code execution tracking with two examples, but its applications are not limited to these two (1) Locate the vulnerable code section for extract- ing private information of a system during execution For in- stance, key extraction attacks [1, 2] assume that adversaries are aware of the code of the cryptographic algorithms|,Non-data,70
| They analyze the source code to find vulnerable code sections, and locate these code sections during execution for private infor- mation extraction Typically, prior work assumes locating code sections during execution is achievable and focus on the code analysis part Our work fills in the blank (2) Detect attacks that intend to hijack MCU’s control-flow to execute malicious code [3–5]|,Non-data,70
| One effective countermeasure to these attacks is to enforce control-flow integrity (CFI) [6], which tracks code execution and prevents code execution deviating from the control-flow graph (CFG) of the program Over the last decade, a large number of CFI techniques [7–12] have been proposed These techniques, despite their effectiveness, are inapplicable for many embedded systems, because the imposed overhead will overwhelm the resource-constrained devices and they typically require software and/or hardware modification, which is impossible for most embedded de- vices, especially legacy devices Our work enables to apply CFI on embedded systems|,Non-data,70
| Code execution tracking via power side-channel is promis- ing yet challenging The advantage is that power side-channel leaks information about instructions being executed on MCU and obtaining such information needs no modification on the MCU itself However, power measurement traces are quite noisy and it is difficult to extract useful information out of them Prior work on recovering the type of executed instruc- tion in a MCU via power-side channel [13] showed a rather low accuracy (about 60% in the best case)|,Non-data,70
| We manage to track code execution at a much higher accuracy by lever- aging the control transfer information from CFG and using frequency analysis to reduce the noise in power side-channel To be specific, the main contributions of this work include the followings • We propose to recover the instruction sequence by hid- den Markov model (HMM) To increase the identifica- tion accuracy, we take advantage of the fact that for a given program, instructions should be executed in se- quences obeying CFG, and identifying these sequences 1019can increase the noise resilience than identifying every single instruction independently|,Non-data,70
| Thus, we represent the CFG as the state machine in HMM To efficiently utilize control transfer information from CFG, we use basic blocks in CFG as states in HMM Correspond- ingly, we revise the classic HMM and Viterbi algorithm to cope with the challenges that basic blocks contain different numbers of instructions and hence have dif- ferent lengths • We propose signal extraction techniques to design the observation symbols in HMM|,Non-data,70
| By extracting high qual- ity signals from power measurement traces, the impact of power measurement noises is dramatically reduced and we are able to further improve instruction recog- nition accuracy • We apply our proposed code execution tracking tech- niques for control-flow integrity checking, ie, we ob- tain the likelihood of the reported instruction sequence from power side channels and its value reflects whether there exists abnormal execution behavior|,Non-data,70
| We evaluate the proposed code execution tracking solution on a 8051 MCU, a popular choice for IoT, wearable devices, industrial sensors, etc, because of their ease of software de- velopment, royalty-free licensing and low cost, and small sil- icon footprint We select nine programs as our benchmark suite We demonstrate that our method can significantly im- prove the tracking accuracy For the benchmark programs, we are able to achieve 99|,Non-data,70
|94% accuracy in recovering the type of executed instruction, which is 4255% higher than that of the previous method Besides recovering instruction type, our method can identify which instruction in code is executed at a given moment, with the average accuracy of 9856%|,Non-data,70
| In addition, we demonstrate that our method is able to detect abnormal execution behavior effectively, even for the case when a single instruction in the original code has been changed The remainder of the paper is organized as follows In Section 2, we present background knowledge and our prob- lem formulation Next, we give an overview of our method in Section 3, and we discuss our revised HMM and Viterbi algorithm in Section 4|,Non-data,70
| Section 5 describes how to design observation symbol and emission distribution function In Section 6, we explore how our method can facilitate de- tecting abnormal execution We evaluate our method with STC89C52 MCU in Section 7 and discuss the limitations of our method in Section 8 At last, we introduce related works in Section 9 and conclude this work in Section 10|,Non-data,70
| 2 BACKGROUND AND PROBLEM FORMULATION In this section, we first discuss the importance of code execution tracking on key extraction attack and CFI Then, we briefly describe CFG and HMM Finally, we formulate the problem to be investigated in this work|,Non-data,70
| 21 Key Extraction Attack When cryptographic algorithms are implemented in soft- ware, for security reasons, designers usually choose imple- mentations from open source libraries (eg, OpenSSL [14])|,Non-data,70
| Hence, adversaries are knowledgeable about the code, and typically extract keys via side-channel attacks or fault in- jection attacks Both attacks require to precisely track code execution before launching the attacks Side-channel attacks analyze MCU’s behavior on physical side-channels, eg|,Non-data,70
|, acoustic emission [15] and power con- sumption [16], when vulnerable instruction pieces are exe- cuted during encryption or decryption Such vulnerable in- struction pieces are carefully chosen from the code, so that their operations are correlated to the key bits For example, differential power analysis (DPA) [1] on Data Encryption Standard requires to obtain the power traces for the 16th encryption round; correlation power analysis (CPA) [17] on Advanced Encryption Standard (AES) requires to obtain the power traces just after the first AddRoundKey operation Fault injection attacks attempt to disturb cipher’s opera- tions and extract keys by analyzing the cipher’s faulty out- put [18, 19]|,Non-data,70
| For each fault attack method, attackers must inject faults into MCU at precise timing when vulnerable in- struction pieces are executed Take extracting the round key of AES-128 encryption algorithm [20] as an example The vulnerable sections can be the code between the 6th and the 7th MixColumns operations [21], in the last encryption round but before its SubBytes operation [2], in the previous rounds of the targeted round [22,23], and in the penultimate round but before its MixColumns operation [24] Thus, all these key extraction attacks require to locate the vulnerable instruction pieces during execution|,Non-data,70
| Conse- quently, code execution tracking is an essential step in such attacks and it is unfortunate that previous works in this domain often assume such method is readily available 22 Control Flow Integrity Code execution tracking is also the foundation for CFI techniques, which is effective to cope with control flow hi- jacking attacks, eg|,Non-data,70
|, return oriented programming [25], jump oriented programming [26], buffer overflow [27] and firmware modification [3] CFI tracks code execution and prevents any attempt to deviate execution flow from CFG [28] Fine-grained CFI checking [7] requires adding a piece of CFI guard code before every control-flow instruction (eg|,Non-data,70
|, indirect jump) and allocating an additional shadow stack to track and validate every function call, return, and excep- tion during execution To improve the performance, many CFI techniques require hardware support For instance, ROPecker [8] relies on last Branch Recording (LBR), which is a hardware unit introduced in Intel’s Nehalem architec- ture, for CFI, and hardware modification is needed when ap- plying it to other processors Similarly, HAFIX [10] depends on special hardware designs to track and verify function re- turns during execution, and other studies [11, 12] introduce dedicated hardware modules to track instruction execution sequence, calculate a signature for this sequence, and com- pare it with golden values|,Non-data,70
| Thus, the aforementioned CFI techniques require software or MCU modification and incur non-trivial overhead to the system For embedded systems with limited resources, es- pecially legacy embedded systems, such intrusive solutions are not applicable 23 Basics for CFG and HMM Control Flow Graph [29] is a directed graph and repre- sents how a program can transit between basic blocks dur- ing execution|,Non-data,70
 A basic block is a sequence of instructions 1020Figure 1: (a) An example Control Flow Graph (b) Illustration of a classic HMM(c) Illustration of a revised HMM that has only one entry point at the beginning and one exit point at the end,Non-data,70
| That is, a basic block can be considered as an execution primitive, and its instruction combination always run in the same order Figure 1(a) shows an exam- ple of CFG: each node in CFG represents a basic block and each edge in CFG represents a valid control transfer between basic blocks A hidden Markov model [30] consists of three parts: state machine, emission distribution, and observation symbol The visible observation depends on hidden states and the hidden state transition is a Markov process|,Non-data,70
| Each state has a prob- ability distribution over the possible observation There- fore, the sequence of observation provides some informa- tion about the sequence of hidden states Viterbi algo- rithm [30] is often used to find the most probable state sequence for a given observation sequence in HMM Fig- ure 1(b) shows an HMM example, which consists of three states ({s1, s2, s3}) and four possible observation symbol val- ues ({v1, v2, v3, v4})|,Non-data,70
| At time t, people can make an obser- vation ot, where ot ∈ {v1, v2, v3, v4} By continuously ob- serving the HMM, an observation sequence O is obtained O is generated by the HMM going through a state sequence Q, where qt (qt ∈ {s1, s2, s3}) in Q is the state of HMM at time t If HMM is in state si at time t, it will jump to state sj at time t + 1 with probability ai,j|,Non-data,70
| The probability to observe vk is ek,i = P r[vk||si], which depends on the hidden state si 24 Problem Formulation Term definition For the sake of clarity, we define the fol- lowing terms used throughout the paper before formulating the problem|,Non-data,70
| Instruction Instance We use an instruction instance to indicate a specific instruction, including both its machine code and location in the code If two instructions in the code have the same machine code but with different PC values, we treat them as different instruction instances For the sake of simplicity, an instruction sequence in this paper refers to a sequence of instruction instances|,Non-data,70
 Instruction Type Instruction type of an instruction is only determined by its operation code We treat instruction instances with the same operation code belong to the same instruction type Formulation,Non-data,70
| Although both key extraction and CFI rely on code execution tracking, their requirements are differ- ent For key extraction, they only need to accurately track the normal execution of the given code In normal execu- tion, the actual execution flow always obeys the CFG On the contrary, control flow hijacking attacks usually introduce invalid control transfers [3, 25–27], and the actual execution flow deviates from the CFG, namely abnormal execution|,Non-data,70
| The objective of CFI is therefore to detect whether there is abnormal execution in the system Thus, we formulate two sub-problems in this work 1 Normal Execution Tracking: Given the source code and the power measurement traces during code execu- tion, we would like to recognize which instruction in- stance is executed at each moment within the power traces|,Non-data,70
| 2 Abnormal Execution Tracking: Given the source code and the power measurement traces during code execution, we would like to detect whether abnormal execution is performed 3 OVERVIEW In this section, we introduce the overall flow of our execu- tion tracking method and discuss the main challenges|,Non-data,70
| To simplify discussion, we consider every instruction costs one unit of time Practically, some instructions may cost mul- tiple units of time In that case, we treat them as multiple one-unit instructions 3|,Non-data,70
|1 Overall Flow While we formulated two problems to tackle in this work, both of them share the same code execution tracking frame- work We model code execution on MCU and its power side- channel behavior as an HMM To be specific, we consider a basic block as an individual state, control transfer between basic blocks as state transition, and the power consump- tion of MCU as observation Then, tracking code execution is equivalent to recognizing how underlying state transition happens for a given power trace|,Non-data,70
| Workflow of Code Execution Tracking The overall flow of our execution tracking framework is illustrated in Figure 2, which contains an HMM construction phase and an execution tracking phase The final output of our framework includes two sequences: an instruction sequence and the cor- responding likelihood of each instance in the sequence The HMM construction phase determines the parameter values of the HMM|,Non-data,70
| We obtain substate (ie, instruction instance), state and state transition information from the CFG of the given code, which can be derived by analyz- ing the disassembled binary [31] Based on a set of power traces when executing various instructions, the observation symbols are obtained by performing signal extraction and 1021Figure 2: Workflow of the proposed code execution tracking framework|,Non-data,70
| dimension reduction, and emission distribution is modeled with Gaussian distribution, as detailed in Section 5 In the execution tracking phase, we first obtain observa- tion sequence from power trace, and then identify the most probable instruction sequence In particular, we first divide power traces into chunks that map to individual instruc- tions This is a straightforward procedure because power trace exhibits periodical characteristics with periods map- ping to instructions [13,32]|,Non-data,70
| Then, to obtain the observation symbol value for each chunk, we conduct filtering and linear transformation on the raw power trace within the chunk, which correspond to signal extraction and dimension reduc- tion, respectively With our revised Viterbi algorithm, we can recover the most probable instruction sequence from the obtained observation sequence Then, based on recovered instruction sequence and observation sequence, we can cal- culate the likelihood sequence Normal and Abnormal Execution Tracking|,Non-data,70
| The re- ported instruction sequence directly addresses the normal execution tracking problem To approach the abnormal ex- ecution tracking problem, we can examine the likelihood se- quence, as detailed in Section 6 32 Challenges To track code execution with HMM, we could define indi- vidual instruction type or individual instruction instance as a state in HMM, but both have limitations|,Non-data,70
| Using instruc- tion type as state only recovers the instruction type sequence instead of instruction sequence, which cannot solve the nor- mal execution tracking problem Using instruction instance as individual state can solve this problem, but its computa- tional complexity is prohibitive because the given code usu- ally contains a large amount of instruction instances, which creates a large number of states For instance, the space complexity of Viterbi algorithm is proportional to the num- ber of states and hence becomes inefficient In addition, every state requires an emission distribution function, and building individual emission distribution function for every instruction instance is impractical for large programs|,Non-data,70
| To reduce the number of states without sacrificing recognition accuracy, we define a basic block in CFG as state in HMM Because the instruction instances in a basic block always run in the same order, if we know how basic block transi- tion occurs during execution, the instruction sequence can be determined as well The above state definition in HMM, however, incurs many challenges Classic HMM defines emission distribution func- tion on the entire state, and it needs to divide given power trace into chunks, where each chunk corresponds to one unknown state|,Non-data,70
| However, basic blocks may contain var- ious number of instruction instances and hence different states have unequal lengths in our case, which makes divid- ing power trace for unknown states non-trivial Moreover, classic Viterbi algorithm assumes the length of every state is 1, and it cannot work for states with unequal lengths To tackle these problems, we introduce substates, which repre- sent instruction instances (see Figure 1(c)) in basic blocks, and define emission distribution function on substate By doing so, we only need to divide power trace into chunks that correspond to instructions|,Non-data,70
| We also revise Viterbi al- gorithm to work with unequal length states and the sub- states By combining states and substates, we are able to simultaneously preserve the CFG information in HMM and dramatically reduce computational complexity To reduce the cost of building emission distribution func- tion for every instruction instance, we build individual emis- sion distribution function for each instruction type and in- struction instances of the same type use the same distribu- tion function Such emission distribution function design suffices to track code execution, because different parts of the code usually have different instruction type sequences and accurately recognizing instruction type enables us to recover the underlying state|,Non-data,70
| To reduce the noise in instruc- tion type recognition, we try to extract high-quality signals from power traces and use them for observation symbols To further reduce the computational overhead of distribution function construction, we also exploit dimension reduction when designing observation symbols 4 REVISED HMM AND VITERBI ALGORITHM In this section, we discuss how to revise HMM and Viterbi algorithm for the problems investigated in this work|,Non-data,70
| 41 HMM Parameters Figure 1(c) shows an example of our revised HMM, and formally, our HMM is characterized by the following param- eters: i , s2 i ,   |,Non-data,70
| , sli i i }, where sm States, state lengths and substates States are given by S = {si || 1 ≤ i ≤ N}, where N is the number of basic blocks in the CFG, and state si corresponds to the ith basic block in the CFG We use li to represent the number of substates, ie|,Non-data,70
|, instruction instances, in state si Then si can be further represented as a sequence of li substates, ie, si = {s1 State transition probability and initial substate dis- tribution|,Non-data,70
| In our model, state transition represents con- trol transfer between basic blocks However, such transition probability distribution can vary significantly with different inputs to program, and the exact input for targeted execu- tion is not available to us Hence, we use a(si, sj) to indicate whether there is a valid control transfer in CFG from si to sj, that is is the mth substate in si (cid:26) 1 if transition is valid a(si, sj) = 0 otherwise |,Non-data,70
| (1) 1022be estimated as, J(Q, O) =  0, (cid:89) 1≤t≤T R(Q) contains invalid transition  (2) e(ot, qt), otherwise Figure 3: Example for Q and R(Q) cause, for any i, (cid:80) Consequently, a(si, sj) is no longer a probability value, be- 1≤j≤N a(si, sj) could be larger than 1 The first chunk in examined power trace can correspond to any instruction instance in any basic block|,Non-data,70
| To obtain the prior probability of instruction sequence, we also need the probability of an instruction instance, ie, substate, being the initial one This probability distribution also varies with different input data, and we simply assume all substates have equal probability to be the initial one|,Non-data,70
| Observation symbols and emission distribution We use V to represent the set of all possible observation sym- bol values As mentioned above, our emission distribution is defined on substate Emission distribution for substate i ) || v ∈ V }|,Non-data,70
| We detail sm i how to design observation symbol and emission distribution function in Section 5 is given by {e(v, sm i ) = p(v||sm 42 Likelihood Estimation Then, finding the most probable instruction sequence is equivalent to finding the most probable substate sequence Next, we discuss how to estimate the probability for a sub- state sequence given the observation sequence|,Non-data,70
| 8, s2 8, s3 2, s1 When calculating the probability of a substate sequence Q, we also need its corresponding state sequence R(Q) R(Q) explicitly represents the state transitions in Q For example, in Figure 3, for substate sequence {s3 8}, its 5, s1 2, s2 corresponding state sequence is {s5, s2, s8} as Q = {q1, q2, |,Non-data,70
|   , qT}, where qt ∈ (cid:83) Formally, a substate sequence of length T can be written 1≤i≤N si R(Q) can be written as R(Q) = {r1, r2, |,Non-data,70
|, rK}, where rk ∈ S|,Non-data,70
| Note that, q1 ∈ r1 and qT ∈ rK  We name r1 as the initial state, and name rK as the final state Because q1 and qT can be any intermediate substate between r1 and rK , the corresponding substate sequence parts for r1 and rK in Q can be incomplete Nevertheless, substate sequence parts for states between the initial state and the final state must be complete in Q|,Non-data,70
| Then, given an observation sequence O = {o1, o2,    , oT} of length T , a candidate substate sequence Q and its corre- sponding state sequence R(Q), the probability of Q given O is p(Q||O) = p(Q, O)/p(O)|,Non-data,70
| Because p(O) is the same for all candidate Qs, to find the most probable substate sequence, we only need to compare the p(Q, O) part, given as, p(Q, O) = p(O||Q) · p(Q) (cid:89) e(ot, qt) · [b(q1) · (cid:89) = 1≤t≤T a(rk−1, rk)], 2≤k≤K As indicated by Equation 1,(cid:81) where b(q1) is the probability that q1 is the initial substate 2≤k≤K a(rk−1, rk) equals 0 if R(Q) contains any invalid state transition, otherwise it is b(q1), whose value is the same for different q1 Consequently, the likelihood value J for a substate sequence Q given O can Then, the most probable substate sequence is simply the one with the maximum J value 4|,Non-data,70
|3 Revised Viterbi Algorithm Next, let us discuss how to efficiently find the most prob- able substate sequence, given observation sequence O of length T  Our algorithm follows the idea in classic Viterbi algorithm We first calculate the J value of most probable substate sequence by recurrence, and then reconstruct the most probable substate sequence by backtracking For each state sj and each time t, our revised Viterbi algorithm calculates a quantity, denoted by δt(j)|,Non-data,70
| When 1 ≤ t ≤ T , δt(j) represents the maximal J for substate se- quence that starts at time 1 and terminates at time t with slj (ie, the last substate of sj) So at time T, the calcu- j lated δT (j) corresponds to the maximal J value of substate sequence that ends exactly at the last substate of sj|,Non-data,70
| Given the observation can also end at any substate inside sj (eg, the case shown in Figure 3), we also calculate sj’s δ at time T + 1 to T + lj − 1 When T + 1 ≤ t ≤ T + lj − 1, δt(j) represents the maximal J for substate sequence which starts at time 1 and terminates at time T with slj−(t−T ) as the last substate|,Non-data,70
| j 2, s2 8, s2 8, s3 5, s1 8} and {s3 Let us first give the basic idea about how to calculate δ by recurrence For a given substate sequence, we can divide it into two parts: one part corresponds to its final state and the other part is the substate sequence before its final state For instance, we can divide the Q shown in Figure 3 into {s1 2} Then, according to Equation 2, the J value of a valid substate sequence is the product of J value for its final state part and J value for the former part (i|,Non-data,70
|e, the substate sequence part before the final state) It means, if a substate sequence terminates with slj j at time t(t > lj) and its J value is δt(j), the J value of its former part must be one of the δ values at time t − lj Otherwise, there must be other substate sequence that also terminates at time t with slj j , has larger J than it|,Non-data,70
| Consequently, we can calculate δt(j) based on δ values at time t−lj, and all δ values can be obtained by recurrence When recurrently calculating δt(j), we also use a quantity φt(j) to record which previous state’s δ at time t − lj maximizes δt(j) Next, we give the formal recurrence relation and initial- ization step for state sj For the sake of simplicity, we use Ω(sj, m, n) to represent the partial substate sequence in state sj, starting with sm j , i|,Non-data,70
|e, j and terminating with sn Ω(sj, m, n) = {sm j , sm+1 j j }, ,   |,Non-data,70
| , sn where 1 ≤ m ≤ n ≤ lj Recurrence When t ≥ 1 + lj and t ≤ T , according to Equation 2, we have δt(j) = [max i δt−lj (i)] · J(sj,{ot+1−lj ,  |,Non-data,70
|  , ot}) φt(j) = argmax i δt−lj (i) st a(si, sj) = 1|,Non-data,70
| (3) 1023When t ≥ 1 + lj and T < t ≤ T + lj − 1, we tackle substate sequences terminating in the middle of state sj at time T  In this case, δ is calculated by, δt(j) = [max i δt−lj (i)] · J(s j,{ot+1−lj ,   |,Non-data,70
| , oT}) (cid:48) φt(j) = argmax i δt−lj (i) st a(si, sj) = 1, j = Ω(sj, 1, T − t + lj) where s(cid:48) Initialization|,Non-data,70
| δt(j) and φt(j) for 1 ≤ t ≤ lj are set by initialization Because lj may be greater than T , we have • when 1 ≤ t ≤ lj and 1 < t ≤ T where s(cid:48) δt(j) = J(s j = Ω(sj, lj − t + 1, lj) j,{o1,  |,Non-data,70
|  , ot}), φt(j) = 0, (cid:48) • when 1 ≤ t ≤ lj and t > T j,{o1,   |,Non-data,70
| , oT}), φt(j) = 0, (cid:48) δt(j) = J(s j = Ω(sj, lj − t + 1, lj − t + T ) where s(cid:48) φt(j) = 0 indicates sj, terminating at t, is the initial state With above, if a substate sequence is of length T and uses sj as final state, its maximal J value is given by max{δT (j),  |,Non-data,70
|  , δT +lj−1(j)} Hence, the J value of the most probable substate sequence is given by {max{δT (j),  |,Non-data,70
|  , δT +lj−1(j)}} max j Once the J value of the most probable substate sequence is located, we can reconstruct the most probable substate sequence by backtracking the φ value accordingly, similar to classic Viterbi algorithm 4|,Non-data,70
|4 Complexity Analysis In this subsection, we analyze our method’s complexity, by comparing it to the naive method that treats each in- struction instance as individual state and uses classic Viterbi algorithm to solve it Because the instructions inside a basic block always run in the same order, it means most instruc- tion instances in the code only have single possible previous instruction However, classic Viterbi algorithm updates the δ value for a state at time t by enumerating all states’ δ val- ues at time t − 1 and records the φ value for every state at every moment, which is unnecessary for most instructions Suppose a program has X instruction instances and Y ba- sic blocks|,Non-data,70
| Because we usually observe MCU execution for a long time, the observation sequence length, denoted by T , should be much larger than the length of the longest basic block, denoted by lmax Both classic Viterbi algorithm and our revised one can be implemented in a dynamic program- ming manner The space complexity is mainly determined by the size of the array used in dynamic programming, which records φ and δ for every state at every moment Then, the space complexity of the naive solution is O(T × X), and ours is O((T + lmax − 1) × Y ) = O(T × Y )|,Non-data,70
| Let us take aes case shown in Table 1 as an example lmax in aes is 82 If T is 7065, then the size of the array with the naive method is 7065 × 1472 ≈ 107 and that with our method is (7065 + 82 − 1) × 55 ≈ 39 × 105|,Non-data,70
| Hence, we can reduce the memory overhead by about 961% The time complexity is mainly determined by the cost of updating elements in the array In both methods, the cost of updating one element consists of two parts: one is evaluating the likelihood of the state given the observation, e|,Non-data,70
|g, J(sj,{ot+1−lj ,   |,Non-data,70
| , ot}) in Equation 3, and the other one is enumerating previous states, eg, maxi δt−lj (i) in Equa- tion 3 Assume the complexity of calculating the likelihood for one instruction instance is O(1)|,Non-data,70
| In our method, let us consider the worst case that every element is updated with Equation 3 Then at time t, Y states need to evaluate the likelihood for X instructions in total and each state needs to enumerate Y previous states Hence, the total time com- plexity is O(T×(X +Y 2)) With the naive method, there are X states|,Non-data,70
| At time t, it also needs to evaluate X instructions, but each state needs to enumerate X states Therefore, the total time complexity is O(T × (X + X 2)), which is much larger than ours 5 OBSERVATION SYMBOL AND EMIS- SION DISTRIBUTION FUNCTION A good observation symbol design should enable us to recover the instruction sequence accurately, and reduce the overhead of building emission distribution function at the same time|,Non-data,70
| In order to achieve the above objectives, we need to solve the following two problems First, because we build individual emission distribution function for each instruction type, we should design the ob- servation symbol in such a manner that it facilitates rec- ognizing instruction type Consequently, signal extraction techniques are employed to increase the correlation between observation symbol and instruction type Second, because a chunk of power trace that corresponds to one instruction instance could contain hundreds of sample points, there is significant overhead to model the distribution of such a high-dimensional variable|,Non-data,70
| Therefore, dimension reduction technique is used to reduce computational com- plexity In the following, we first discuss our signal extraction tech- nique, and then present the overall design flow of our obser- vation symbol 51 Signal Extraction From the viewpoint of frequency domain, power signal is synthesized from different frequency components|,Non-data,70
| The ob- jective of signal extraction in this work is to select those frequency components that are highly correlated to instruc- tion type and filter out other components Frequency Components Selection The raw power sig- nal represents the total power consumption of the MCU, and there are at least four factors that affect MCU power consumption when an instruction is executed First, instruc- tion type affects power consumption by designating the mi- cro operations of the processor|,Non-data,70
| Next, when executing an instruction, the instruction operands and the instruction ex- ecuted prior to it affect the low-level switching activities of the circuit Finally, environment noise would also have some impact on the obtained power trace The last three factors would interfere with instruction type recognition, and their impact can be mitigated by increasing the correlation be- tween observation symbol and instruction type 1024Usually, a frequency component can be represented by its amplitude value Acom|,Non-data,70
| As raw power signal is determined by four factors, we simply model Acom as linear combination of two parts, given by Acom = Atype + Aother, (4) where Atype is determined by instruction type, and Aother represents the part determined by instruction operand, pre- vious executed instruction and environmental noise together We assume Atype and Aother in Equation 4 are indepen- dent Ideally, different instruction types have different Atype values and the same type of instruction has the same Atype value In this case, to evaluate the correlation between a frequency component and instruction type, we can use the correlation between Atype and Acom instead|,Non-data,70
| We use Pearson’s correlation coefficient to evaluate the correlation Then the correlation between Atype and Acom is given by ρ(Acom, Atype) = cov(Acom, Atype) (cid:112)DcomDtype , where cov(Acom, Atype) is the covariance between Atype and Acom, Dcom is the variance of Acom, and Dtype is the vari- ance of Atype Because Atype and Aother are independent, we have cov(Atype, Atype) + cov(Aother, Atype) ρ(Acom, Atype) = = (cid:112)DcomDtype Dtype + 0 (cid:112)DcomDtype (cid:114) Dtype =  Dcom Therefore, we should select those frequency components with the following two characteristics, 1|,Non-data,70
| Dtype/Dcom should be as large as possible in order to obtain larger correlation ρ(Acom, Atype) 2 In addition, the magnitude of Dtype should be as large as possible Larger Dtype means the difference on Atype among different instruction types is more significant, which is easier to be captured|,Non-data,70
| Evaluating Dcom and Dtype First, evaluating Dcom is simple, because Acom value can be obtained by transforming the raw power signal from time domain to frequency domain Second, although we cannot measure Atype directly, Dtype can be evaluated as follows Because Aother and Atype are independent, if Aother keeps constant during sampling, the conditional variance of Acom in this case is equal to Dtype, according to Equation 4|,Non-data,70
| As a result, to evaluate Dtype, we can sample Acom by randomly changing instruction types while keeping instruction operand and previous executed in- struction fixed To make environmental noise constant, we can measure the power trace for every instruction instance multiple times and use the averaged power trace instead To further improve the accuracy, we can calculate Dtype multiple times with different configurations of instruction operands and previous executed instruction, and use the av- eraged value when comparing different components Filtering|,Non-data,70
| Once the appropriate frequency components are selected, it is straightforward to obtain the filtered power signal That is, we can obtain the frequency amplitude spec- trum of one power trace with Fast Fourier Transformation, Figure 4: Observation Symbol Design Flow zero out the amplitude values of those inappropriate fre- quency components, and generate the filtered power trace by Inverse Fast Fourier Transformation 5|,Non-data,70
|2 Overall Design Flow Figure 4 shows our observation symbol design flow The input is a set of power traces with various instruction in- stances Among these instruction instances, the instruction type, the instruction operand and instruction executed prior to the sampled instruction are all randomly changed First, we conduct signal extraction according to the given set of power traces and generate the filtered power traces|,Non-data,70
| Next, we conduct dimension reduction with principle com- ponent analysis (PCA) [33] PCA can generate a linear transformation function that maps high-dimensional power signal to a lower-dimensional signal, while the transforma- tion preserves useful information as much as possible When applying PCA, we need to decide the dimensionality of the obtained lower-dimensional signal To solve this problem, we evaluate how dimensionality affects instruction type recog- nition rate with statistical classifiers, e|,Non-data,70
|g, Naive Bayes clas- sifier, and use the smallest dimensionality contributing to the highest recognition rate Finally, we use the low-dimensional signal obtained after applying PCA as our observation symbol For each instruc- tion type, we fit its emission distribution with Multivari- ate Gaussian Distribution Model, based on the above power trace set|,Non-data,70
| 6 ABNORMAL EXECUTION TRACKING Till now, we have shown how to recover the instruction sequence, and solve the normal execution tracking problem In this section, we discuss how to detect abnormal execu- tion, based on the fact that in abnormal execution cases, the most probable sequences typically have a reduced like- lihood compared to the normal execution cases Then, we discuss the possibility that attackers can evade our tracking method|,Non-data,70
| 61 Detection via Likelihood Sequence When invalid control transfers are introduced by control flow hijacking attack, our revised Viterbi algorithm would recognize the actual instruction sequence, deviating from CFG, as another valid instruction sequence that obeys CFG and has the largest probability to generate the observation sequence Because the actual instruction sequence, contain- ing abnormal execution, intends to implement a malicious function that does not exist in the original CFG, the actual instruction sequence’s instruction type sequence is usually different from that of any valid instruction sequence defined by the CFG Therefore, the type sequence of the actual 1025instruction sequence is different from that of the reported instruction sequence|,Non-data,70
| With the above, when tracking ab- normal execution, some instruction instances in the actual sequence would be incorrectly recognized as the wrong type of instruction instances in the reported sequence This phe- nomenon can thus be used for abnormal execution tracking Ideal Case Ideally, when tracking normal execution, in- struction instances should be all correctly recognized|,Non-data,70
| Hence, if we could distinguish between correctly recognized instruc- tion instances and incorrect ones, we are able to detect ab- normal execution In order to achieve this objective, we examine the likelihood of the reported instruction instance m given the corresponding observation v, ie, e(v, m), which is also the conditional probability of v given m|,Non-data,70
| When m is reported with incorrect recognition, the corresponding ob- servation, denoted by vinc, is actually generated by another instruction of different type Because different instruction types usually generate different observations, m is not likely to generate vinc If m is reported with correct recognition, the corresponding observation, denoted by vc is generated by m itself Hence, we have e(vinc, m) < e(vc, m)|,Non-data,70
| Calibrated Likelihood Motivated by the above, for each instruction instance in the code, we record its average like- lihood value in normal execution When detecting abnor- mal execution, for each instruction instance in the reported instruction sequence, we subtract the recorded average like- lihood for this instruction instance from its current likeli- hood value, and we name the obtained difference as cal- ibrated likelihood Then, the calibrated likelihood se- quence is given by {e(o1, q1) − h(q1), e(o2, q2) − h(q2), |,Non-data,70
|   , e(oT , qT ) − h(qT )}, where h(qt) is the average likelihood value of the instruction instance qt in normal execution If the instruction instance is correctly recognized, its cal- ibrated likelihood should be around zero|,Non-data,70
| Otherwise, the calibrated likelihood for incorrectly recognized instruction instance should be biased to be negative Note that, although an additional average likelihood num- ber is recorded for each instruction instance, this overhead is much smaller than that of building and recording individual emission distribution function for each instruction instance 62 Security Analysis Given different instruction types may have the same power emission model, an attacker may try to launch a mimic at- tack, which evades our detection by constructing adversarial instruction sequence whose power consumption fingerprint happens to be valid|,Non-data,70
| In this section, we discuss the proba- bility of such attack Threat Model To construct a malicious sequence to ful- fill an adversary’s hidden agenda from scratch is challenging We imagine that an adversary will utilize the well-known at- tack (i|,Non-data,70
|e, Call-Preceded Return-Oriented- Programming, in short CPROP [25]) to reuse the existing code to accomplish this goal Thus, for illustration purpose, we analyze the like- lihood of mimic attackers that utilize CPROP CPROP mali- ciously redirects the target of the ret instruction to a wrong instruction whose preceding instruction is a call instruc- tion|,Non-data,70
| Without loss of generality, suppose all control trans- fers in a program are caused by function calls or returns Then CPROP constructs adversarial instruction sequence by introducing invalid transitions among basic blocks We assume attackers know the power fingerprint of every basic block in the code We assume that the target program has m basic blocks, each basic block has n instructions (excluding the final con- trol transfer instruction), and each basic block has v (v ≤ m) valid next basic blocks in CFG (i|,Non-data,70
|e, outdegree of any node in CFG is v) Given different instruction types may have the same power emission, we assume that the instruction set contains a instruction types in total and can be divided into b groups, where each group contains a/b instruction types on average and the instruction types in the same group have the same power emission We can only distinguish instruction types from different groups via power side-channel|,Non-data,70
| When CPROP wants to insert a malicious basic block af- ter a legitimate basic block, she creates an invalid transition To evade detection, CPROP should create the malicious ba- sic block so that its power fingerprint is the same as one of the original v valid ones In the worst case, CPROP can use one of the m−v basic blocks (ie|,Non-data,70
|, the ones that create invalid transition) as the malicious one and the resulting adversarial instruction sequence only deviates from the valid instruction sequence by a single basic block Let us denote the prob- ability that such adversarial sequence evades detection by Pevade If the instructions in basic blocks are randomly and independently distributed, Pevade = 1 − [1 − ( )n]v(m−v) 1 b (5) The second term in Equation 5 gives the probability that any of the m − v malicious candidates has a different power fingerprint from those of v valid ones, i|,Non-data,70
|e, the probability of detecting the adversarial sequence We can expand this term in Binomial series, then [(−1)t k(k − 1)  |,Non-data,70
|  (k − t + 1)xt ], (6) t! Pevade = kx − k(cid:88) where x = ( t=2 )nand k = v(m − v) 1 b When kx < 1, the absolute value of (−1)t k(k−1)|,Non-data,70
|(k−t+1)xt decreases as t increases Hence, the second term on the right hand side of Equation 6 is always positive Then we have t! Pevade < kx = v(m − v) bn |,Non-data,70
| For the code size m and the basic block size n that are typical for mid-size embedded devices, the magnitude of kx is small and Pevade is close to 0 with the following reasons First, bn is exponentially proportional to n Second, differ- ent instruction types’ power emissions usually provide suf- ficient diversity and b is large For example, b ≈ 102 for the MCU used in our experiments, because a = 152 and the accuracy of classifying instruction types can be estimated by b/a, which is 70% in our case as shown in Section 7|,Non-data,70
|23 Third, k ≤ 025m2, kx ≤ m2/4bn|,Non-data,70
| Hence, kx is small for a typical code size m and a basic block size n For instance, for a target code of 106 basic blocks, we only need n > 7 to guarantee Pdetect > 9975% with the MCU used in our experiments Thus, the probability of constructing an adversarial in- struction sequence with a valid power consumption finger- print is close to 0 in general|,Non-data,70
| 10267 EVALUATION In this section, we conduct various experiments to evalu- ate the proposed solution First, we describe the hardware and software platforms used for evaluation and introduce the performance metrics used in this work The evaluation results are divided into four parts: designing observation symbol, tracking normal execution, tracking abnormal exe- cution, and tracking execution on different chips|,Non-data,70
| 71 Experimental Setup MCU under test The method proposed in this paper actually can be applied to any MCU model, as long as the execution time of every instruction is a constant Many MCU architectures in current market satisfy this require- ment, such as PIC12 [13] , 8bit AVR [32] and Intel’s 8051 [34]|,Non-data,70
| STC89C52, an implementation of 8051 architecture, is used in this evaluation Since there is no external RAM on its evaluation board, instructions relevant to external RAM (eg, MOVX) are excluded from evaluation|,Non-data,70
| Most instructions in this MCU cost only one machine cycle For instructions costing 2 or 4 machine cycles, we treat them as 2 or 4 dif- ferent single-cycle instructions As a result, the effective instruction set contains 152 different single-cycle instruction types This MCU is clocked at 11|,Non-data,70
|0592M Hz using an ex- ternal oscillator Power measurement To measure the power consumption of the MCU under test, a resistor of 467Ω is placed between VCC pin of the MCU and its power supply, and the volt- age drop over it is measured using a Tektronix MDO3034 oscilloscope with sampling rate of 1|,Non-data,70
|25GS/s Benchmark programs Our benchmark suite consists of 9 programs, in which eight of them are from Dalton Project [35] that is used to evaluate the performance of 8051 MCUs The remaining one is an implementation of AES-128 encryption algorithm migrated to our MCU|,Non-data,70
| The details of these nine programs are shown in Table 1, including the number of instruction instances (# of Inst), the number of basic blocks (# of BB), and the length of instruction se- quence tracked during program execution (Measured Inst)|,Non-data,70
| For the programs matrix, aes, pid and dct, we only mea- sure 7065 executed instructions, because their power traces for a complete execution will go beyond the maximal length that can be measured with our experimental setup For all the other programs, power traces of a complete execution are recorded Evaluation Metrics We evaluate our method with two metrics|,Non-data,70
| The first one is Instruction Sequence Accuracy (ISA), which demonstrate the accuracy of the recognized in- struction instance in the reported instruction sequence The second metric is Type Sequence Accuracy (TSA), which only measures the accuracy of the recognized instruction types TSA is a more important metric, because the performance of some configurations (to be introduced below) cannot be measured by ISA and our method relies on the instruction type information to track program execution We compare our work with the method proposed in [13], which recovers the instruction type sequence by treating ev- ery instruction type as a state in classic HMM, and instruc- tion type transition probabilities are extracted from code under test|,Non-data,70
 Their observation symbol is obtained by con- ducting dimension reduction on raw power signal directly Name Description aes sqroot sort matrix Matrix AES-128 Square root Bubble sort pid dct gcd fib multiplication Simulate cruise control in car Discrete cosine transform Euclidean algorithm Fibonacci sequence csumex Cumulative sum chart # of # of Measured Inst 1427 1002 233 413 Inst 7065 3800 4430 7065 BB,Non-data,70
| 55 98 37 30 1572 199 7065 560 69 159 89 51 11 24 12 7065 135 782 665 Table 1: Benchmark suite Figure 5: (a) Normalized Dtype/Dcom and Dtype for differ- ent frequency components (b) Classifying instruction type after PCA, when signal extraction is used (E) and not used (NOE) We denote the HMM defined in [13] with prefix TYPE and our proposed HMM with prefix BB (means Basic Block)|,Non-data,70
| We use suffix E and NOE to indicate whether signal extrac- tion technique is used or not in designing observation sym- bol Therefore, there are four configurations to be evalu- ated: TYPE NOE, TYPE E, BB NOE and BB E, where TYPE NOE corresponds to the method in [13] All the four configurations run on the same server with Intel Xeon E5- 2609 CPU and 16GB RAM 7|,Non-data,70
|2 Observation Symbol Design In this section, we demonstrate how to design observa- tion symbol and its impact on instruction type recognition accuracy In our experiment, the set of power traces used for designing observation symbol consists of about 180,000 power traces from various instruction instances, measured on the same chip Signal Extraction 72|,Non-data,70
|1 Let us first estimate Dtype/Dcom and Dtype We obtain the frequency amplitude spectrum of the power traces with Fast Fourier Transofrmation Figure 5(a) shows Dtype/Dcom and Dtype within frequency range 0 ∼ 100M Hz, where the value of Dtype/Dcom and Dtype are normalized for presen- tation Based on our discussion in Section 5|,Non-data,70
|1, we only select frequency components within range (0M Hz, 1138M Hz), be- cause both Dtype/Dcom and Dtype within this range are larger than those outside of this range and they are used for instruction type recognition 1027Configuration TYPE NOE TYPE E BB NOE BB E BB NOE BB E aes 4809 57|,Non-data,70
72 9989 9988 9055 90,Non-data,70
49 csumex 9952 9961 10000 100,Non-data,70
00 dct 7326 7077 10000 100,Non-data,70
00 fib 5659 9348 10000 100,Non-data,70
00 10000 10000 10000 100,Non-data,70
00 10000 10000 TSA(%) gcd 8118 78,Non-data,70
53 9824 10000 ISA(%) 9824 100,Non-data,70
00 matrix 8669 9562 9997 99,Non-data,70
98 pid 3704 5657 9343 99,Non-data,70
80 sort 9220 9789 10000 100,Non-data,70
00 sqroot 5642 5762 9975 99,Non-data,70
78 9997 9998 9281 99,Non-data,70
48 10000 10000 9704 97,Non-data,70
09 Average 7011 7865 9903 99,Non-data,70
94 9762 9856 Table 2: TSA and ISA in normal execution tracking The last column shows the average value for each row,Non-data,70
| 722 Dimension Reduction with PCA To decide the dimensionality of the final low-dimensional signal, we examine the instruction type recognition rate with Naive Bayes classifier and Gaussian Bayes classifier Figure 5(b) shows the instruction type recognition rates for different classifiers after applying PCA, when signal ex- traction is used and not used, respectively|,Non-data,70
| For both clas- sifiers that we have tested, cases with signal extraction re- quire only about 10 dimensions to achieve the maximum recognition rate, meanwhile the non-filtered cases require much more dimensions (about 35 shown in the figure) to achieve the same value Given this, if signal extraction is used, we use signals consisting of the first 10 dimensions af- ter applying PCA as observation symbol, otherwise we use signal consisting of first 35 dimensions after applying PCA as observation symbol For both cases, emission distribution function of each instruction type is built with Multivariate Gaussian Model 7|,Non-data,70
|23 Effectiveness of Signal Extraction We have another two observations from Figure 5(b) First, as fewer dimensions are required when signal extraction is used, it means our signal extraction technique can reduce the complexity in building the emission distribution func- tion with Multivariate Gaussian Model Second, with Naive Bayes classifier, the maximal recognition rate for case with signal extraction is larger than that of case without signal extraction|,Non-data,70
| This means, by selecting frequency components of larger Dtype/D and Dtype, we can recognize instruction types more accurately But the improvement on the max- imum recognition rate almost disappears for the Gaussian Bayes classifier case, where the maximum recognition rate is about 70% for both cases One possible reason is that, Gaussian Bayes classifier considers the dependency among different dimensions that can help instruction type recogni- tion, and the improvement introduced by signal extraction is thus much smaller 7|,Non-data,70
|3 Normal Execution Tracking Table 2 lists the accuracy of normal execution tracking for different configurations with different programs For each configuration and each program, we track its execution for five times and the average accuracy value is reported in the table From table 2, we have the following observations First, using BB model can always achieve higher TSA than using TYPE model|,Non-data,70
| No matter whether signal extrac- tion technique is used or not, with BB model, the TSA for tracking all 9 programs is over 93% In particular, when BB E configuration is used, the TSA is always over 997% For configurations with TYPE model, TSA varies a lot and is quite low in some cases|,Non-data,70
| For example, TYPE NOE only achieves 3704% TSA for pid, while it achieves 9952% TSA for csumex On average, our most powerful method BB E can outperform TYPE NOE by 42|,Non-data,70
|55% This is consistent with our expectation, because BB model preserves more knowledge from CFG and hence it has a higher probabil- ity to track the execution correctly Second, on average, signal extraction technique can im- prove TSA by 1218% for TYPE model, and 0|,Non-data,70
|92% for BB model Earlier we observed that the maximum recognition rate with Gaussian classifier almost keeps unchanged, no matter whether signal extraction is used or not This is not contradictory to our observation here The difference lies in experimental setup, i|,Non-data,70
|e, in instruction type classification experiment, the instruction type is uniformly distributed, which is different from the distributions in actual programs Third, configurations with BB model can also achieve very high ISA, which is over 97% on average It demonstrates that, by precisely recovering the executed instruction’s type, it is sufficient for our method to track which instruction in- stance in the code is executed|,Non-data,70
| On some programs, ISA is lower than TSA, such as aes We manually check the results of aes case, and find there are two basic blocks in the pro- gram which only differ at one location in their instruction type sequences At this location, one block uses XRL A,R0 instruction and the other one uses XRL A,direct instruc- tion These two instructions both implement exclusive-or function, differ in addressing mode, and belong to differ- ent types|,Non-data,70
| When one of them is incorrectly recognized as the other one, TSA only treats this XRL instruction is incor- rectly recognized while ISA regards all the instructions in the basic block are incorrectly recognized Therefore, ISA is much smaller than TSA in this case To sum up, our BB model outperforms the original TYPE model significantly The signal extraction technique further improves execution tracking accuracy|,Non-data,70
| 74 Abnormal Execution Tracking Because designing a full-fledged CFI method is beyond the scope of this work, in this subsection, we mainly demon- strate that abnormal execution could decrease the reported calibrated likelihood values, compared to that of normal ex- ecution cases We use firmware modification attack as an example In- tuitively, less modification on the original code is more dif- ficult to be detected|,Non-data,70
| Given this, we first study single in- struction replacement, insertion and deletion cases on aes program, which do not change the control transfers after modification In these three cases, we respectively replace one NOP instruction with an ADD A,0x00 instruction, insert a new NOP instruction, and delete an existing NOP instruction All these modifications are conducted at the beginning of 1028Figure 6: Calibrated log likelihood of the first 4000 instruction instances in reported instruction sequence for (a) normal execution, (b) single instruction replacement, (c) single instruction insertion, and (d) multi-instruction modification SubByte function in aes program, which is called 16 times within one measurement|,Non-data,70
| Next, to study multi-instruction modification case, we simulate an attack that replaces aes with dct during execution Each power measurement covers 7065 instructions and BB E configuration is used as execu- tion tracking method Figure 6 shows the calibrated log likelihood sequences in the attack cases and the normal execution case Because the magnitude of the original likelihood value is sometimes quite small, we perform calibration on log likelihood instead|,Non-data,70
| From the results, we have the following observations For the normal execution case (Figure 6(a)), the cali- brated likelihood for most instruction instances are close to zero and the mean value is -00284 Although several calibrated likelihood values in the sequence deviate from zero a lot, indicated by green circle, all these values cor- respond to the same instruction instance whose type is MOVC A,@A+DPTR, and their mean value is -43|,Non-data,70
| Most registers in 8051 are 8-bit registers, but DPTR consists of 16 bits Hence, the power consumption of MOVC is more sensitive to operands than other instructions, and its calibrated likeli- hood varies more significantly For the replacement case (Figure 6(b)), the distribution of the calibrated likelihood is significantly biased with a large negative mean value Based on the reported instruction se- quence, we can group the multiple occurrences of the same instruction instance and observe its calibrated likelihood’s distribution|,Non-data,70
| The ADD A,0x00 instruction after replacement is incorrectly recognized as NOP All sixteen occurrences of the replaced NOP, indicated by blue cross in the figure, have negative calibrated likelihood value, and their mean is - 287 This replaced NOP can be easily distinguished from the above-mentioned MOVC instruction The calibrated like- lihood values of MOVC instruction instance can be both neg- ative and positive, indicated by green circle, and their mean value is -56|,Non-data,70
| For the insertion and deletion cases, the observations are similar and the result for the insertion case is given in Fig- ure 6(c) as an example We observe that the calibrated like- lihood for many instruction instances around the inserted (or deleted) NOP, indicated by blue dash line, is biased to a large negative value This is because, instruction inser- tion/deletion can cause multiple instruction instances around it to be incorrectly recognized For example, if we delete the first instruction from a 4-instruction basic block, the second instruction in this basic block can be incorrectly considered as the start of this state during tracking, and the remain- ing three instructions in this basic block together with one instruction from the next basic block in the actual execu- tion flow may be incorrectly recognized as one state, which affects the following basic block|,Non-data,70
| For the multi-instruction modification case (Figure 6(d)), the calibrated likelihood for most instruction instances is bi- ased to be negative, and the mean value is -877743, which is much smaller than that in the normal execution case The degradation of the calibrated likelihood here is more signifi- cant than the single instruction modification cases, which is consistent with the intuition that multi-instruction modifi- cation is easier to be detected 7|,Non-data,70
5 Execution Tracking on Different Chips Chip No Chip1 Chip2 Chip3 Chip4 Avg STD TSA(%) TYPE NOE TYPE E BB NOE BB E 9994 44,Non-data,70
84 9993 7528 9993 67,Non-data,70
94 9992 7326 6533 99,Non-data,70
93 0007 14007 9366 99,Non-data,70
62 9960 9962 9813 2,Non-data,70
976 7904 7957 7951 71,Non-data,70
50 7740 3943 Table 3: Average TSA for four configurations on different chips The last two rows show the average and standard deviation for each column,Non-data,70
| In this subsection, we demonstrate that emission distri- bution function built with power traces from one chip (eg, Chip0) can be used to track code executions on other chips (eg|,Non-data,70
|, Chips 1∼4) from the same architecture family Results for normal execution tracking on Chip0 are listed in Table 2, and the TSA results on Chip1∼4 using the same emission distribution model derived from Chip0 are shown in Table 3 These data can lead us to the following observations First, the emission distribution model derived from Chip0 work very well on Chips 1∼4|,Non-data,70
| When comparing the average TSA accuracy of all chips (eg, the Avg row in table 3 and the last column of table 2), we found that they are very close to each other, even though applying the model to different chips can still introduce small accuracy loss (the largest TSA degradation is 6|,Non-data,70
|82% with TYPE NOE, and the degradation for BB E is almost zero) There are two possible reasons for such similarity First, the power consumption of each instruction is largely determined by its instruction types, bacause instructions of the same type share many on-chip hardware modules that contribute most of the overall power consumption, as shown in Figure 5 Second, different chips used in this experiment have the same architecture and sim- ilar layouts, so power consumptions of each instruction type are very close among different chips|,Non-data,70
| Although they may still have some unique features, the variances introduced by such differences are small 1029Another observation is that the TSA values for configura- tions with signal extraction techniques are more stable This is shown by the standard deviation (ie|,Non-data,70
|, the STD row) in Table 3 where BB E and TYPE E have much smaller stan- dard deviation than BB NOE and TYPE NOE does This is because signal extraction facilitates to eliminate certain fre- quency components that are more sensitive to the difference between multiple chips (eg, the static power corresponding to frequency 0)|,Non-data,70
| 8 LIMITATIONS AND FUTURE WORK Though our method has significantly reduced the compu- tational complexity compared to the naive solution, it can still induce undesired overhead when the target program is large and contains a large number of instructions and basic blocks Such a situation can get exacerbated when inter- rupts are enabled during execution, because interrupts can be triggered at any time during code execution and cre- ate various valid control transfers from every instruction in- stance to the beginning of interrupt service routines Under such circumstances, every instruction instance in the code becomes one individual basic block and it results in a larger number of states|,Non-data,70
| To tackle this problem, a hierarchical code execution tracking method can be used For instance, when an interrupt is triggered, a processor needs to perform special operations, eg, context switching|,Non-data,70
| It is possible to first identify such operations from power traces, determine the power traces corresponding to the execution of interrupt service routines, and remove them Then, we can concate- nate the remaining power trace segments, and conduct code execution tracking on the newly-constructed power trace In our experiments, the execution of the benchmark pro- grams do not use peripheral devices, and hence the mea- sured power trace is mainly contributed by the MCU itself When peripheral devices are used, however, the correlation between instruction type and measured power trace would decrease and it may result in reduced accuracy in code ex- ecution tracking|,Non-data,70
| As a direction for our future work, one can increase the measuring points of power traces That is, instead of measuring the overall power consumption of the system, we would collect power traces from multiple power pins on the MCU and investigate their correlations with the executed instructions, thereby mitigating the im- pact of the peripheral devices on proposed code execution tracking method 9 RELATED WORKS In this section, we briefly discuss related works, includ- ing execution tracking via digital channels, normal execu- tion tracking with side-channels, abnormality detection, and code reverse engineering|,Non-data,70
| Execution Tracking via Digital Channel Some ARM- based MCUs (eg, Cortex-M3) contain a dedicated hard- ware unit for code execution tracking, namely embedded trace macrocell (ETM) [36]|,Non-data,70
| However, it is usually not prac- tical to cycle-accurately track code execution at normal CPU speed with ETM Moreover, many MCUs do not have such hardware support for execution tracking Normal Execution Tracking via Side-channel Eisen- barth et al|,Non-data,70
| [13] utilized HMM to recover the instruction type sequence during code execution It treats an instruction type as a state, and extracts transition probabilities between in- struction types However, solely recovering instruction type is not able to locate instruction instance Msgna et al|,Non-data,70
| [32] tried to track execution flow by modeling one basic block in CFG as a state with classic HMM However, their method cannot tackle the general case where basic blocks have un- equal length Compared to the above works, our code ex- ecution tracking method can locate the exact instruction instance during execution accurately Abnormality Detection via Side-Channel|,Non-data,70
| Some works detect abnormal execution by calculating the cross correla- tion between examined execution’s side-channel trace, eg, power trace [37, 38] and RF trace [39, 40], and the corre- sponding side-channel trace of golden execution In prac- tice, however, it is difficult to determine the exact golden execution flow because embedded system’s execution usu- ally interacts with changeable environment and varies a lot in different runs|,Non-data,70
| By contrast, the detection technique based on our tracking method has no such requirement WattsUp- Doc [41] uses statistical tools to classify every 5-second power trace chunk’s corresponding execution to be normal or ab- normal, where features, such as mean and variance, are used for classification We have shown calibrated likelihood is a good feature for abnormal execution detection, and it can be used to enhance WattsUpDoc Side-channel Based Code Reverse Engineering|,Non-data,70
| These methods focus on recovering the code in the system, instead of tracking the execution flow for a given code Vermon et al [42] recovered the bytecodes running on a Java smart card However, this method requires calculating the av- erage power trace of the targeted sequence of bytecodes, which is impractical for the general cases|,Non-data,70
| Novak [43] and Clavier [44] showed how to recover the substitution tables of secret A3/A8 algorithm, but their method is limited to recovering the look-up table part Goldack and Paar [45] proposed to recover the type of single instruction instance by building power consumption templates for every instruc- tion type However, their template models the distribution of raw power signal after simple dimension reduction We have demonstrated that dimension reduction itself does not lead to high recognition accuracy, but it can be improved by our signal extraction technique|,Non-data,70
 10 CONCLUSION This paper proposes a non-intrusive yet highly-accurate code execution tracking method for embedded systems uti- lizing power-side channel This is achieved with signal ex- traction scheme to improve instruction type recognition and a revised Viterbi algorithm for effective instruction sequence extraction Experimental results show that our method is able to track code execution accurately in normal execution tracking and effectively capture code modification in abnor- mal execution tracking,Non-data,70
| 11 ACKNOWLEDGMENTS This work was supported in part by the Chinese Univer- sity of Hong Kong internal grant No 4055049, Hong Kong SA|,Non-data,70
|R Research Grants Council (RGC) under Early Career Scheme No 24207815, in part by National Natural Science Foundation of China (NSFC) under Grant No 61432017, 61532017, 61572415, and 61472358, and in part by National Science Foundation (CNS-1513107)|,Non-data,70
|ABSTRACT Bitcoin provides two incentives for miners: block rewards and transaction fees The former accounts for the vast ma- jority of miner revenues at the beginning of the system, but it is expected to transition to the latter as the block rewards dwindle There has been an implicit belief that whether miners are paid by block rewards or transaction fees does not affect the security of the block chain We show that this is not the case|,Non-data,71
| Our key insight is that with only transaction fees, the variance of the block reward is very high due to the exponentially distributed block arrival time, and it becomes attractive to fork a “wealthy” block to “steal” the rewards therein We show that this results in an equilibrium with undesirable properties for Bitcoin’s security and performance, and even non-equilibria in some circumstances We also revisit selfish mining and show that it can be made profitable for a miner with an arbitrarily low hash power share, and who is arbitrarily poorly connected within the network Our results are derived from theoretical analysis and confirmed by a new Bitcoin mining simulator that may be of independent interest|,Non-data,71
 We discuss the troubling implications of our results for Bitcoin’s future security and draw lessons for the design of new cryptocurrencies 1 INTRODUCTION The security of Bitcoin’s consensus protocol relies on min- ers behaving correctly They are incentivized to do so via mining revenues under the assumption that they are ratio- nal entities,Non-data,71
| Any deviant miner behavior that outperforms the default is thus a serious threat to the security of Bitcoin Miners receive two types of revenue: block rewards and transaction fees The former account for the vast majority of miner revenues at the beginning of the system, but it is expected to transition to the latter as the block rewards dwindle (specifically, they halve every four years) There has been an unexamined belief that in terms of the security of the block chain (including incentives of the mining game), Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page|,Non-data,71
| Copyrights for components of this work owned by others than the author(s) must be honored Abstracting with credit is permitted To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from permissions@acm|,Non-data,71
|org CCS’16, October 24 - 28, 2016, Vienna, Austria c(cid:13) 2016 Copyright held by the owner/author(s) Publication rights licensed to ACM ISBN 978-1-4503-4139-4/16/10|,Non-data,71
   $1500 DOI: http://dx,Non-data,71
doiorg/101145/29767492978408 Figure 1: One possible state of the block chain and two possible actions a miner could take,Non-data,71
| it is immaterial whether miners receive (say) 25 bitcoins in each block as a block reward or 25 bitcoins in expectation as transaction fees Illustrative example (Figure 1) Imagine a popula- tion of rational, self-interested miners Consider a block chain with blocks of exponentially distributed rewards, as we expect when the fixed block reward runs out|,Non-data,71
| A miner has numerous options to consider when mining, but let’s fo- cus on just two possibilities She could extend the longest chain (Option One), obtaining a reward of 5 and leaving a reward of 0 for the next miner (at least until more transac- tions arrive) Alternatively, she could fork it (Option Two), obtaining reward of 55 while leaving a reward of 50 Bitcoin unclaimed The Bitcoin protocol dictates Option One, but a quick reasoning suggests that Option Two is better|,Non-data,71
| To reason about this correctly, we must consider which strategies the other miners are using For instance, if all other miners follow the heuristic of mining on the block they heard about first in the case of a 1-block fork (and if there is no latency in the network), then forking is ineffective, and Option One is clearly superior On the other hand, since other miners are rational, perhaps they will choose to build on the fork instead of the older block, in which case Option Two would yield more rewards Examples like these reveal novel incentive issues that sim- ply don’t arise when block rewards are fixed|,Non-data,71
| The goal of this paper is to understand the potential impact on Bitcoin’s sta- bility by investigating the mining game in the regime where the block reward has dwindled to a negligible amount, and transaction fees dominate mining rewards We find new and surprising incentive issues in a transaction-fee regime, even 154assuming that transactions (and associated fees) arrive at a steady rate To be clear: the incentive issues we uncover arise not because transaction fees may arrive erratically, but because the time-varying nature of transaction fees allows for a richer set of strategic deviations that don’t arise in the block-reward model At a high level, there is an analogy with pool hopping [21]|,Non-data,71
