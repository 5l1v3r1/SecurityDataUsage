 6303. DATASETS Our Internet-wide study of key sharing in the HTTPS ecosystem is driven by four datasets: SSL certificates We use SSL certificates from full IPv4 scans as the basis of our measurements,Data,0
| Our SSL scans [30] also contain information on the IP address(es) that advertised each certificate. To obtain in- formation about the entity that controls this IP address, we use full IPv4 reverse DNS scans [29] that are also conducted by Rapid7|,Data,0
| Each AS is assigned an AS Number (ASN): for example, MIT is AS 3 and the Chicago Public Schools are AS 1416 [26]. CAIDA collects and publishes mappings between IP addresses and ASNs via their Route- Views datasets [7]|,Data,0
| For example, AT&T owns 160 unique ASNs. To aggregate these, we use CAIDA’s AS- to-Organization dataset [8] to group together ASes owned by the same organization|,Data,0
| For that, we rely on WHOIS [12], a protocol for querying domain registrars to obtain data on the domain owner. In practice, WHOIS data often contains fields such as the con- tact information for the owner of the domain, the contact for technical issues, where to send abuse complaints, and so on|,Data,0
| Here, we expand upon these prior findings by evaluating whether there is a correlation between centralized management and the quality of the keys chosen. Figure 13 compares several different features of self- managed and outsourced certificates across our entire cor- pus of leaf certificates (3,275,635 self-managed and 1,781,962 outsourced): (a) Key lengths in self-managed certificates are nearly identical to those managed by third-party hosting providers|,Data,0
|1 Combining Packet Capture (PCAP) Files The data set used in this study is a combination of the packet capture files obtained from two main sources. First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour|,Data,1
| First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour. The normal and non-malicious data is obtained from PREDICT internet data set repository [18] under the category of “DARPA Scalable Network Monitoring (SNM) Program Traffic”|,Data,1
| The data collection was performed during April 2016 using ZGrab, an application-layer scanner that operates with ZMap [15]. In the first phase, we performed an Internet-wide scan of all IPv4 addresses on port 500 to determine which hosts were configured 16This defect was corrected quite recently, years after the version of OpenSSL ScreenOS uses was written|,Data,6
|, download from an external source). Running on 71,000 articles collected from 45 leading technical blogs, this new approach demonstrates a remarkable performance: it gener- ated 900K OpenIOC items with a precision of 95% and a coverage over 90%, which is way beyond what the state-of-the-art NLP tech- nique and industry IOC tool can achieve, at a speed of thousands of articles per hour|,Data,7
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
 5. ANALYSIS AND FINDINGS In the following we use the extensive documentation of the 61 minimal exploits to provide insight into how attackers use specific vulnerabilities and features of the Java platform to implement their attacks,Data,11
| We run our event analysis on the top 100 free applications in the Android application store to determine how often this happens. In total, our analysis finds 1060 errors across 88 of the top 100 applications (10|,Data,12
| To our knowledge, AUTOREB is the first work that explores the user review information and utilizes the review semantics to predict the risky behaviors at both review-level and app-level. We crawled a real-world dataset of 2, 614, 186 users, 12, 783 apps and 13, 129, 783 reviews from Google play, and use it to comprehensively evaluate AUTOREB|,Data,14
| 4.1 Data collection For each team, we collected a variety of observed and self- reported data|,Data,16
| To demonstrate this, we scraped greatfire.org for websites in the top 1000 Alexa websites that are blocked by the GFW|,Data,18
| (cid:15) Identifying New Vulnerabilities. Our tool successfully an- alyzed 1,591 service interfaces of all the 80 system services in Android 5|,Data,19
| To understand the scope and magnitude of this new XARA threat, we developed an ana- lyzer for automatically inspecting Apple apps’ binaries to deter- mine their susceptibility to the XARA threat, that is, whether they perform security checks when using vulnerable resource-sharing mechanisms and IPC channels, a necessary step that has never been made clear by Apple. In our study, we ran the analyzer on 1,612 most popular MAC apps and 200 iOS apps, and found that more than 88|,Data,24
| To assist software developers (or secu- rity analysts) in tracking down a memory corruption vulnerability, CREDAL also performs analysis and highlights the code fragments corresponding to data corruption. To demonstrate the utility of CREDAL, we use it to analyze 80 crashes corresponding to 73 memory corruption vulnerabilities archived in Offensive Security Exploit Database|,Data,25
| These techniques may be applicable in other scenarios. We implemented and evaluated the attacks against the popular Gmail and Bing services, in several environments and ethical experiments, taking careful, IRB-approved mea- sures to avoid exposure of personal information|,Data,26
|, CSPAutoGen can handle all the inline and dynamic scripts. We have implemented a prototype of CSPAutoGen, and our eval- uation shows that CSPAutoGen can correctly render all the Alexa Top 50 websites|,Data,27
| 5. EXPERIMENTAL RESULTS This section reports on our evaluation of the moments ac- countant, and results on two popular image datasets: MNIST and CIFAR-10|,Data,28
| 6.1 Mobility Trace Dataset We use the CRAWDAD dataset roma/taxi [2, 3] for our simu- lations|,Data,31
 6.1 Evaluation We evaluated the performance of Σoφoς using 4 data sets of increasing size and also the English Wikipedia,Data,33
|1 Datasets, Metrics, Competitors & Settings Datasets. We test EpicRec on two real-world datasets: MovieLens1: a movie rating dataset collected by the Grou- pLens Research Project at the University of Minnesota through the website movielens|,Data,36
| 1 http://grouplens.org/datasets/movielens 188Yelp2: a business rating data provided by RecSys Chal- lenge 2013, in which Yelp reviews, businesses and users are collected at Phoenix, AZ metropolitan area|,Data,36
| The number of movie categories is 18. We use the MovieLens- 1M, with 1000,209 ratings from 6,040 users on 3,883 movies|,Data,36
| Our goal is to show that an ad- versary can insert an unbounded number of Sybil identities in the SybilLimit protocol, breaking its security guarantees. For our evaluation, we consider a real-world Facebook inter- action graph from the New Orleans regional network [28]|,Data,38
| We utilize these papers to extract Android malware behaviors and to construct the semantic network. From the electronic proceedings distributed to conference participants, we collect the papers from the IEEE Sympo- sium on Security and Privacy (S&P’08–S&P’15)4, the Com- puter Security Foundations Symposium (CSF’00–CSF’14), and USENIX Security (Sec’11)|,Data,39
 We conduct experiments on two publicly available set-valued datasets. • AOL search log dataset [1],Data,45
 90% of the users have fewer than 84 keywords in their logs. • Kosarak dataset [2],Data,45
 We select one month of data for our study. The data logs we used are col- lected from more than 30 machines with various server mod- els and operating systems,Data,46
| This paper rigorously investigates how users’ security beliefs, knowledge, and demographics corre- late with their sources of security advice, and how all these factors influence security behaviors. Using a carefully pre- tested, U|,Data,48
 We have ported Valgrind to iOS and implemented a prototype of iRiS on top of it. We evaluated iRiS with 2019 applications from the official App Store,Data,54
| from manufacturing equipment, as shown in Figure 1. We capture the relevant sensor data by deliberately or accidentally placing an attack-enabled phone close to, on top of, or inside a piece of manu- facturing equipment while the machinery is fabricating the target object|,Data,55
| Our new metric helps us compare in a fair way previously proposed attack-detection mechanisms. (ii) We compare previous attack-detection proposals across three di↵erent experimental settings: a) a testbed operating real-world systems, b) network data we collected from an operational large-scale Supervisory Control and Data Acqui- sition (SCADA) system that manages more than 100 Pro- grammable Logic Controllers (PLCs), and c) simulations|,Data,57
| Evaluation. We ran Oyente on 19, 366 smart contracts from the first 1, 460, 000 blocks in Ethereum network and found that 8, 833 contracts potentially have the documented bugs|,Data,58
| First, we consolidate the eight origin-exposing vectors into one auto- mated origin-exposing system called Cloudpiercer. Then, we assemble a list of clients from five CBSP companies by studying their DNS configurations and obtaining their adop- tion rate across the Alexa top 1 million websites|,Data,59
| The vast majority of them were exposed through their A record, indicating a brief dis- abling of the protection system. SSL certificate exposure In order to find IP addresses hosting SSL certificates associ- ated with the domains in the evaluation set, we made use of the publicly available data of Rapid7’s Project Sonar [42]|,Data,59
| 4. LARGE-SCALE ANALYSIS To assess the magnitude of the origin-exposure problem, we conduct a large-scale analysis in which we attempt to uncover the origin of CBSP-protected domains|,Data,59
|1 Dataset Description The dataset was first presented and used by Keller et al. in [23], and is publicly available in the gene expression om- nibus (GEO) database under reference GSE61741|,Data,61
| Although the cost of stor- age and processing have diminished, the cost of maintaining reliable infrastructure for transaction logs is still noticeable. Figure 1: A plot of transaction fee versus frequency for 1 million transactions in May 2015|,Data,65
| To estimate the cost of producing the preprocessing data (multiplication triples, random bits etc.), we used figures from the recent MASCOT protocol [31], which uses OT ex- tensions to obtain what are currently the best reported triple generation times with active security|,Data,67
| In this section, we validate whether the smartphone’s acoustic data can be utilized to deduce the movements. To conduct the validation, we implement an application on Nexus 5 (Android OS v6|,Data,68
| As seen in Table 4, we found that about half of the servers in Alexa’s top 10 support a large number of requests without rekeying. For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client|,Data,72
| For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client. We identified 11483 different HTTPS servers11, and found that 226 of them (1|,Data,72
| In this paper, we study the possible techniques to detect and measure this fraud and evaluate the real impact of OTT bypass on a small European country. For this, we performed more than 15,000 test calls during 8 months and conducted a user study with more than 8,000 users|,Data,78
|, the server cannot learn their relative order) after some number of queries are performed over real-world data. Specifically, we ran an experiment where we inserted over 2 million public employee salary figures from [1] and then performed 1000 random range queries|,Data,79
| In this study, we are interested in finding answers to security- and privacy-related questions about libraries, such as “How prevalent are third- party libraries in the top apps and how up-to-date are the library versions?”, “Do app developers update the libs included in their apps and how quickly do they update?”, or “How prevalent are vulnerabilities identified in prior research [28, 9] in libraries and how many apps are affected?” To answer these questions, we first built a comprehensive repository of third-party libraries and applications (see Section 5). Our library set contains 164 libraries of different categories (Ad- vertising, Cloud,|,Data,84
|) and a total of 2,065 versions. We then collected and tracked the version histories for the top 50 apps of each category on Play between Sep 2015 and July 2016, accumulating to 96,995 packages from 3,590 apps|,Data,84
|6.1, we found in our sample set 360 affected packages from 23 distinct apps, when only considering exact library matches|,Data,84
|15 for Android, which contained an account hijacking vulnerability, on 06/11/2014. In the histories of our sample set apps, we discovered, in total, 394 affected packages from 51 distinct apps, when only considering packages with exact matches of the vulner- able lib version|,Data,84
| We used LibScout to detect the affected application packages in our data set. In total 2,667 app versions of 296 distinct apps with a cumu- lative install-base of 3|,Data,84
| We observed that there is a significant variance in ACFG size. To reduce the sampling bias, we first collect a dataset which covers ACFGs of different functions from various architec- tures|,Data,89
| This dataset was used for base- line comparison, and all functions in this dataset has known ground truth for metric validation. We prepared this dataset using BusyBox (v1|,Data,89
| Dataset II – Public dataset. Recent work such as Pewny et al [45] and Eschweiler et al [23] used the same public dataset based upon two publicly-available firmware images for baseline comparison [7, 8]|,Data,89
| Dataset III – Firmware image dataset. This dataset of 33,045 firmware images was collected from the wild|,Data,89
| As a result, we created a freely available vulnerability database for this effort and for the broader research community. To build this database, we mined official software websites to collect lists of vulnerabilities with the corresponding CVE num- bers|,Data,89
| We selected OpenSSL for demonstration, since it is widely used in IoT devices. The resulting vulnerability database includes 154 vulnerable functions|,Data,89
| Roughly speaking, our measurement methods can be divided into two kinds: those that could be fully automated and scaled eas- ily, and those that required some manual interaction. For the latter, we used a set of 302938 major email providers and email genera- tors, while for the former, we used a much larger set of a million popular providers occurring in the Adobe leak and the Alexa top million Web sites (as potential email generators)|,Data,90
1.2 Provider List We created the set of popular email providers based on the top 1 million email address domains occurring in the leaked Adobe user data set of September 2013,Data,90
| Using a combination of mea- surement techniques, we determine whether major providers sup- ports TLS at each point in their email message path, and whether they support SPF and DKIM on incoming and outgoing mail. We found that while more than half of the top 20,000 receiving MTAs supported TLS, and support for TLS is increasing, servers do not check certificates, opening the Internet email system up to man- in-the-middle eavesdropping attacks|,Data,90
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
| 3.1 Datasets We use two major types of datasets: (1) packet-level traffic traces collected at various locations in a campus network, and (2) packet-level traces for Tor Pluggable Transport traffic collected in controlled environments|,Data,91
 Evaluation: local mixing time in social graphs. We use 10 various large-scale real-world social network topolo- gies that mainly come from the Stanford Large Network Dataset Collection [23] and other sources [45] to evaluate the local mixing time for nodes in social graphs,Data,92
| Feature Functions and Weights. To learn all feature functions and weights, we downloaded 1784 non-obfuscated Android applications from F-Droid [3], a popular repository for open-source Android applications|,Data,93
2.2 Experiments with Malware Samples We randomly selected one sample from each of the 49 mal- ware families reported in [40],Data,93
1_r1). Apps in our dataset used for the case study are downloaded from the Google official market (Google Play) in May 2016,Data,95
| • Using SInspector, we perform the first study of Unix domain sockets on Android, including the categoriza- tion of usage, existing security measures being en- forced, and common flaws and security implications. We analyze 14,644 apps and 60 system daemons, find- ing that 45 apps, as well as 9 system daemons, have vulnerabilities, some of which are very serious|,Data,98
| We presented SInspector, a tool for discovering potential security vulnerabilities through the process of identifying socket addresses, detecting authen- tication checks, and performing data flow analysis on na- 90tive code. We analyzed 14,644 Android apps and 60 system daemons, finding that some apps, as well as certain system daemons, suffer from serious vulnerabilities, including root privilege escalation, arbitrary file access, and factory reset- ting|,Data,98
| Our results show that many of our attacks succeed with a 100% chance such that the Sound-Proof cor- relation algorithm will accept the attacked audio samples as valid. Third, we collect general population statistics via an online sur- vey to determine the phone usage habits relevant to our attacks|,Data,100
 We find that the larger width of integer types and the increased amount of addressable memory introduce previously non-existent vulnerabilities that often lie dormant in program code. We empirically evaluate the prevalence of these flaws on the source code of Debian stable (“Jessie”) and 200 popular open-source projects hosted on GitHub,Data,104
| We have applied UniSan to the latest Linux kernel and Android kernel and found that UniSan can successfully prevent 43 known uninitialized data leaks, as well as many new ones. In particular, 19 of the new data leak vulnerabilities in the latest kernels have been confirmed by the Linux community and Google|,Data,107
| This allows us to prevent replay attacks, which are possibly the most applicable attack vectors against biometric authentication. Using a gaze tracking device, we build a prototype of our system and perform a series of systematic user experiments with 30 participants from the general public|,Data,108
| If two commits were blamed for the same amount of lines, blame both. Our heuristic maps the 718 CVEs of our dataset to 640 VCCs|,Data,109
| However, improving our blame heuristics further is an interesting avenue for future research. Apart from the 640 VCCs, we have a large set of 169,502 unclassified commits|,Data,109
|9 The SVM detected a high amount of excep- tions, a high number of changed code, inline ASM code, and variables containing user input such as __input and user. 6As previously mentioned we use the years 2011–2014 as the test dataset, since we have ground truth data on which to base the discussion|,Data,109
| When given a source file, Flawfinder returns lines with suspected vul- nerabilities. It offers a short explanation of the finding as well as a link to the Common Weakness Enumeration (CVE) database|,Data,109
| The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database|,Data,109
| Our results show that our approach significantly outperforms the vulner- ability finder Flawfinder. We created a large test database containing 66 C and C++ project with 170,860 commits on which to evaluate and compare our approach|,Data,109
 VoiceLive takes advantages of the user’s unique vocal system and high quality stereo recording of smartphones. • We conduct extensive experiments with 12 participants and three different types of phones under various ex- perimental settings,Data,111
| To test if WebCapsule can successfully record and subsequently replay real-world phishing attacks, we proceeded as follows, us- ing Chromium on our desktop machine. We selected a large and diverse set of recently reported phishing web pages from Phish- Tank8|,Data,112
| 2.4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime|,Data,113
|4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime. The dataset Cal represents the latitude and longitude of about 21,000 intersections in the California road network1 (also used by Mavroforakis et al|,Data,113
294258. The dataset SpitzLoc consists of latitude and longitude coordinates tracking the movement of German Green party politician Malte Spitz over six months,Data,113
| In this section, we aim to explore whether the differences of keystroke wave- forms are large enough to be used for recognizing different keys inputs in the real-world setting. We collected training and testing data from 10 volunteers|,Data,114
 B. Real Attacks MAD uniformly detects attacks more quickly than the PAD; we use the former method to detect the presence of an attack in real Internet traces3,Data,119
 III. DATA SET  changes  The data used was the PREDICT ID USC-Lander!  (- 60  The total  were DNS attack packets,Data,120
|395326000  files IPs. There are total 59,928,920 packet counts out of which there was a total of  DoS_DNS_amplification-20130617 (2013-06-17) (2013-06-17) with anonymized million) 358019 DNS packets|,Data,120
| The maximum number of unique hosts per day we measured was 106,000. To understand these differences, we compared the observations from our network monitor to data collected from DShield (www|,Data,121
| 3.1 From our own transactions We engaged in 344 transactions with a wide variety of services, listed in Table 1, including mining pools, wallet services, bank ex- changes, non-bank exchanges, vendors, gambling sites, and mis- cellaneous services|,Data,122
| Wallets. We kept money with most of the major wallet services (10 in total), and made multiple deposit and withdrawal transac- Bank exchanges|,Data,122
|, in which the exchange rate is not fixed) also function as banks. As such, we tagged these services just as we did the wallets: by depositing into and withdrawing from our accounts (but rarely par- ticipating in any actual currency exchange)|,Data,122
|info/tags, including both addresses provided in users’ signatures for Bitcoin forums, as well as self-submitted tags. We collected all of these tags — over 5,000 in total — keeping in mind that the ones that were not self-submitted (and even the ones that were) could be regarded as less reliable than the ones we collected ourselves|,Data,122
| 3.1 Data analysis overview We use three data sets, summarized in Table 1|,Data,123
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
| We also describe our application of the technique to the IPv6 interface-level graph captured by CAIDA’s Archipelago (Ark) infrastructure [14] for March 2013. The graph consists of all the 52,986 IPv6 interfaces numbered within the 2000::/3 unicast prefix captured from all 27 Ark vantage points (VPs) with IPv6 connectivity|,Data,125
| cause the counters of distinct routers to diverge, and (4) confirm aliases with pairwise probing. Given the absence of velocity in ID counters and the large probes required for the technique to work, we probe at a low rate of 20pps from a single VP, producing 26Kbps of traffic|,Data,125
| 3. METHODOLOGY In this section, we describe the design of our experiment and our data collection methodology, as well as the mitigating steps and proactive measurements we conducted to ensure a minimal im- pact of our covering routes|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| Our IPv6 network telescope results suggest sev- eral important differences (and some similarities) compared to that body of work. To produce a more recent and valid comparison, we analyzed a single week of IPv4 background radiation captured during the course of our ongoing IPv6 packet capture|,Data,126
| 4. DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1|,Data,127
| DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1. Our primary dataset consists of changes made to the |,Data,127
| domains, (2) the removal of existing domains, and (3) changes to existing domains in terms of revisions to their associated name- servers. Our data includes captures of the DNZA files as recorded every five minutes, time periods we refer to as epochs|,Data,127
| Since we lack comprehensive ground truth regarding the ultimate use of domains, to this end we use two proxies: subsequent appearance of a newly registered do- main in: (1) an email spam campaign, or (2) a domain blacklist. For the first of these, we operated a spam trap, i|,Data,127
|com), by restricting our focus to domains recently registered (March–July 2012) we can filter down the do- mains appearing in the spam trap to those very likely used for spam- ming. For the second, we subscribed to three major DNS blacklists, URIBL, SURBL, and Spamhaus DBL|,Data,127
| In this paper, we examine the effectiveness of these inter- ventions in the context of an understudied market niche, counterfeit luxury goods. Using eight months of empirical crawled data, we identify 52 distinct SEO campaigns, document how well they are able to place search results for sixteen luxury brands, how this ca- pability impacts the dynamics of their order volumes and how well existing interventions undermine this business when employed|,Data,128
| For a small number of stores, we were also able to collect user traffic data that directly measures an SEO campaign’s effectiveness in attracting customers to their stores. Specifically, we were able to periodically collect AWStats data for 647 storefronts in 12 cam- paigns|,Data,128
| One issue that undermines coverage is that Google only labels the root of a Web site as “hacked”, and does not label search results that link to sub-pages within the same root domain. In the PSR data set, we found 68,193 “hacked” search results|,Data,128
| We begin by exam- ining the properties of individual darknets and in particular the behavior of source IP addresses. We provide these char- acterizations by looking at data from 14 darknet monitors ranging in size from a /25 monitor to a /17 monitor over a period of 10 days between August 18, 2004 and August 28, 2004|,Data,129
| Figure 10: The number of darknets (of 31) reporting a port in the top 10 ports over a day, week, and month time frame. The analysis is performed for the top 10 destination ports over a day, top 10 destination ports over a week, and top 10 destination ports over a month|,Data,129
| 3.6 Datasets This paper uses DNS datasets from three authorities: one national-level top-level domain, operators of two root servers as shown in Table 1|,Data,130
 JP-DNS operates the .jp country code domain for Japan; we have data from all seven of their anycast sites,Data,130
|) part of the 2014 DITL collection [16] (for B-Root, shortly after 2014 DITL). We also use data for M-Root’s 2015 DITL collection (§ 4|,Data,130
 These root datasets are available to re- searchers through DNS-OARC. For longitudinal analysis we draw on 9 months of data taken at the M-Root server,Data,130
| However, we treat the union of these classes together. We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness|,Data,131
| We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness. The dataset consists of all echo requests that were sent as part of the surveys in this period, as well as all echo responses that were received|,Data,131
|, “host unreachable”); we ignore all probes as- sociated with such responses since the latency of ICMP error responses is not relevant. In later sections, we will complement this dataset with results from Zmap [5] and additional experiments includ- ing more frequent probing with Scamper [13] and Scrip- troute [22]|,Data,131
| 3.2 Milking 3 Methodology To collect the information needed to cluster servers into oper- ations, we have built an infrastructure to track individual exploit servers over time, periodically collecting and classi- fying the malware they distribute|,Data,132
 2. We receive feeds of drive-by download URLs (Sect,Data,132
 2. CHARACTERISTICS OF CHECK-INS We use three different datasets that capture human mobility,Data,133
 First we consider two online location-based social networks. We col- lected all the public check-in data between Feb,Data,133
| There are 196,591 nodes, 950,327 edges in Gowalla and 58,228 nodes, 214,078 edges in Brightkite. To ensure that our observations on human movement are not specific to data based on check-ins from location-based social net- works, we also include a dataset of cell phone location trace data|,Data,133
| Backscatter DDoS is a commonly seen behaviour in darknets where the attacker uses simultaneous bots to generate the actual attack packets to reach the targeted (original) victim. In our study, five publicly available network traffic datasets from CAIDA’s archives are employed|,Data,134
| Datasets Employed In this research, five publicly available real-life network traffic traces (datasets) from CAIDA’s archives are employed. Three of them, which were captured by a passive darknet in 2007, 2008 and 2012 [27][26][28], namely UCSD Network Telescope [21], include mostly one-way malicious traffic while the remaining ones collected in 2008 [29] and 2014 [30] via CAIDA’s Internet backbone links include only normal traffic|,Data,134
| 3 Approach This section presents our approach for the evalua- tion of reputation based blacklists. We evaluated the blacklists by deploying them in a large academic net- work of over 7,000 hosts|,Data,135
| This was a preliminary step to preventing inexperienced and non-serious workers from participating in our survey. Our survey is based on the participants’ actual check-ins on Foursquare posted over the last 24 months (that we collected through a specific application we developed), and it requires a significant amount of time to complete (30-45 minutes)|,Data,136
| The third phase of worm activ- ity is the persistence phase which for the Blaster worm has continued through 2004. In this one-week period of measurement, the IMS system observed over 286,000 unique IP addresses displaying the characteristics of Blaster activity|,Data,137
| published a study in 2011 that focused on the dynamics of leaf cer- tificates and the distribution of certificates among IP addresses, and attempted to roughly classify the overall quality of served certifi- cates. The study was based on regular scans of the Alexa Top 1 Mil- lion Domains [1] and through passive monitoring of TLS traffic on the Munich Scientific Research Network [17]|,Data,138
| Our study is founded on what is, to the best of our knowledge, the most comprehensive dataset of the HTTPS ecosystem to date. Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443|,Data,138
| Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443. Over the course of 14 months, we completed upwards of 400 billion SYN probes and 2|,Data,138
| Content Provider e Service Provider v i t c e p s r e P Content Consumer Addressing Prerequisite IP Functions Routing Naming A1: Address Allocation; A2: Address Advertisement N1: Nameservers; R1: Server Readiness N2: Resolvers N3: Queries A2: Address Advertisement; T1: Topology End-to-End Reachability R1: Server Readiness Operational Characteristics Usage Profile Performance U3: Transition Technologies U1: Traffic Volume; U3: Transition Technologies P1: Network RTT R2: Client Readiness U2: Application Mix; N3: Queries Table 2: Dataset summary showing the time period, scale, and public or new status of the datasets we analyzed. Dataset RIR Address Allocations Routing: Route Views Routing: RIPE Google IPv6 Client Adoption Verisign TLD Zone Files CAIDA Ark Performance Data Arbor Networks ISP Traffic Data Verisign TLD Packets: IPv4 Verisign TLD Packets: IPv6 Alexa Top Host Probing Time Period Metrics Jan 2004 – Jan 2014 A1 Jan 2004 – Jan 2014 A2, T1 Jan 2004 – Jan 2014 A2, T1 Sep 2008 – Dec 2013 R2, U3 Apr 2007 – Jan 2014 N1 P1 Dec 2008 – Dec 2013 U1, U2, U3 Mar 2010 – Dec 2013 Jun 2011 – Dec 2013 N2, N3 N2, N3 Jun 2011 – Dec 2013 Apr 2011 – Dec 2013 R1 Recent Scale ≈18K allocation snapshots (5 daily) 45,271 BGP table snapshots millions of daily global samples daily snapshots of ≈2|,Data,139
com & .net) ≈10 million IPs probed daily ≈33-50% of global Internet traffic; 2013 daily median: 50 terabits/sec (avg,Data,139
| To put the IPv6 allocation data in context, Figure 1 also shows IPv4 prefix allocations over the same period. The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled|,Data,139
| There were less than 30 IPv6 prefixes al- located per month prior to 2007, generally increasing thereafter. In the past several years, we typically find more than 300 prefixes allocated per month, with a high point of 470 prefix allocations in February 2011|,Data,139
| The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled. 1 Overall, we find nearly 69K IPv4 prefix allocations at the beginning of our dataset and just over 136K at the end|,Data,139
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| Table 1 shows the datasets we use in our paper. We use two ICMP surveys taken by USC [12]: IT17ws and IT16ws; IT17ws is the main dataset used in this paper, while we use IT16ws for validation in Section 6|,Data,142
2. We collected VUSC s at our enterprise in order to compare our inferences with network operators as discussed in Section 6,Data,142
| # of Data-Oriented Attacks gives the number of attacks generated by FLOWSTITCH, includ- ing privilege escalation attacks and information leakage attacks. FLOWSTITCH generates 19 data-oriented attacks from 8 vulnerable programs|,Data,144
| Third, this method is not specific to C or C++, and can be applied to any programming language. We collected C++ source of thousands of contestants from the annual international competition “Google Code Jam”|,Data,145
| Finally, we analyze various attributes of programmers, types of programming tasks, and types of features that appear to influence the success of attribution. We identified the most important 928 fea- tures out of 120,000; 44% of them are syntactic, 1% are layout-based and the rest of the features are lexical|,Data,145
|3.1ScalingWecollectedalargerdatasetof1,600programmersfromvariousyears|,Data,145
| ) s y a D n i (    e m T i  7  6  5  4  3  2  1  10  20  30  40  50  60  70  80  90 Time Before Accounts Suspension Number of IP Addresses 2 Motivation: Analysis of Malicious Activ- ity on a Webmail Service We want to understand the way in which cybercrimi- nals abuse accounts on online services, to identify weak points that we could leverage for detection. To this end, we observed the email-sending activity on a large web- mail service|,Data,147
| Following accepted frameworks for qualitative research [18, 30, 35], we focus closely on a small number of participants. We interviewed 15 journalists employed in a range of well-respected journalistic institutions in the United States and France, analyzing these interviews using a grounded theory approach [18, 30]|,Data,146
| 3.1 Datasets We examine 13,345 passwords from four sets created under composition policies ranging from the typical to the currently less common to understand the suc- cess of password-guessing approaches against passwords of different characteristics|,Data,149
| Had we used any major password leak, their analysts would have already been familiar with most or all of the passwords contained in the leak, biasing results. The passwords in these sets were collected using Ama- zon’s Mechanical Turk crowdsourcing service|,Data,149
| The decision for or against pinning is always a trade- off between increasing security and keeping mainte- nance efforts at an acceptable level. In this paper, we present an extensive study on the applicability of pinning for non-browser software by analyzing 639,283 Android apps|,Data,152
| Therefore, we instrument telemetry data from a popular anti-virus software provider. We evaluate the update behaviour of 871,911 unique users from January 2014 to December 2014 and find that only 50% of the users update to a new app version within the first week after release|,Data,152
| Developer View Although pinning is only ap- plicable in relatively few cases, the nominal-actual comparison leaves room for improvement. We there- fore collected feedback from 45 developers of apps for which we would recommend pinning|,Data,152
| Section 4). Altogether we found 20,020,535 calls to network related API calls (cf|,Data,152
| Instability of the routes to the sensor address space can also result in reachability problems, especially given that route flap damping can be triggered during convergence to suppress unstable routes [9]. Using the BGP updates data from RouteViews BGP monitor, we studied the availability of the routes to the sensor blocks in our de- ployment from a large set of ASes|,Data,154
| This section probes these differences using three successively more specific views of traffic to a network of distributed blackhole sensors. The data was recorded over a one month period with SYN responders on TCP port 135, 445, 4444, and 9996 across all sen- sors|,Data,154
|  V. EXPERIMENT RESULTS  In this section, we mainly focus on how our router-to-AS Mapping method and other baseline methods behave on global router-level topology, as discussed above, we use PeeringDB data as ground truth, and apply clustering method on global topology based on CAIDA ITDK project|,Data,155
| It describes the properties that a dataset should have in order to be used for comparison purposes. The dataset used in the paper includes an IRC-based Botnet attack1, but the bot used for the attack was developed by the authors and therefore it may not represent a real botnet behavior|,Data,156
| This dataset may be downloaded with authorization. The Protected Repository for the Defense of Infrastructure Against Cyber Threats (PRE- DICT) indexed three Botnet datasets2 until May 16th, 2013|,Data,156
 None of them are labeled. A custom botnet dataset was created to verify five P2P botnet detection algorithms in Saad et al,Data,156
| Unfortunately, there is only one infected machine for each type of botnet, therefore no synchronization analysis can be done. The Traffic Laboratory at Ericsson Research created a normal dataset that was used in Saad et al|,Data,156
 This is the only normal dataset that is labeled inside the pcap file. A considerable amount of malware traffic in pcap format was published in the Contagio blog9,Data,156
| But since each scenario includes only one infected computer, it should be possible to label them. Another dataset with malware logs and benign logs was collected in NexGinRC (2013)|,Data,156
 Access to this dataset may be granted upon request10. The last dataset analyzed is currently created by the MAWI project described in Cho et al,Data,156
| Methodology and datasets We deployed Paris Traceroute with its Multipath Detection Algorithm (MDA) [29] enabled in 90 PlanetLab nodes. We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]|,Data,158
| We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]. Our dataset contains more than 900 thousand IP-level (multi)routes and 324,313 IP addresses|,Data,158
1 3.1 Address Allocation and BGP Data We analyzed BGP announcements captured by all collectors (24 collectors peering with 184 peers) of the Routeviews [3] and RIPE RIS [52] projects,Data,159
| For each /24 block, we computed the maximum number of peers that saw it reachable at any time within the full observation period of 92 days. To determine which address blocks are available for assignment, we used a dataset compiled by Geoff Hus- ton [23], which merges the extended delegation files from the 5 RIRs [4, 6, 7, 41, 51] with IANA’s published registries [31–36]|,Data,159
| SWITCH. We collected unsampled NetFlow records from all the border routers of SWITCH, a national aca- demic backbone network serving 46 single-homed uni- versities and research institutes in Switzerland [55]|,Data,159
| R-ISP. We collected per-flow logs from a vantage point monitoring traffic of about 25,000 residential ADSL customers of a major European ISP [21]|,Data,159
 UCSD-NT. We collected full packet traces from the /8 network telescope operated at the University of Cal- ifornia San Diego [1],Data,159
| IXP. Our fourth VP is a large European IXP inter- connecting more than 490 networks, exchanging more than 400 PB monthly [5]|,Data,159
|3 Active Measurements ISI. We used the ISI Internet Census dataset it55w- 20130723 [37], obtained by probing the routed IPv4 address space with ICMP echo requests and retaining only those probes that received an ICMP echo reply from an address that matched the one probed (as rec- ommended [38])|,Data,159
| HTTP. We extracted IP addresses from logs of Project Sonar’s HTTP (TCP port 80) scan of the entire IPv4 address space on October 29, 2013 [24]|,Data,159
| Definitions of graph parameters measuring metric tree-likeness of a graph, as well as notions and notations local to a section, are given in appropriate sections. 3 Datasets Our datasets come from different domains like Internet measurements, biological datasets, web graphs, social and collaboration networks|,Data,160
| The experiments were executed as follows. Traces were col- lected by using ICMP, UDP, and TCP Traceroute to probe the paths to a set of 100 destination websites from a source located on the Pennsylvania State University, University Park campus|,Data,161
| For UDP and TCP Traceroute, traces were collected using the default destination port numbers. We also collected traces using other ports and observed similar results|,Data,161
| Realistic Networks Here we compare the merged topologies produced by iTop, MN, and Isomap for realistic topologies. We use the Au- tonomous System (AS) topologies from both the Rocketfuel [20] and the CAIDA [21] projects, which represent IP-level connections between backbone/gateway routers of several ASes from major Internet Service Providers (ISPs) around the globe|,Data,161
| Although the paris-traceroute output of ITDK is more reliable than that of IPlane’s traceroute, the random selection of endpoints implemented by CAIDA hinders the collection of routes between the same vantage- and endpoints. Therefore we used the data of IPlane’s traceroute measurements|,Data,162
| They can also be used for constructing maps of the Internet at the Autonomous Systems level [, ]. In this work we used the CAIDA router-level Internet map from October th,  []|,Data,163
| 3 Table 1: Dataset Description Name BGP Usage AS Geolocation; Detour Detection Date 2016-01 Sources Info RouteViews, RIPE 38,688 RIBS, 416 peers, RIS 30 countries, 55GB Infrastructure IP List AS Geolocation 2016-01 to 2016-03 CAIDA Ark, iPlane, OpenIPMap, RIPE Atlas Measurements 3M Router IPs Infrastructure IPs to AS Mapping Infrastructure IP geolocation 2015-08 CAIDA ITDK, iPlane 6.6M IP to AS mappings AS to IXP Mapping AS Relationship AS Geolocation 2016-01 to 2016-03 Filtering peered paths from detection 2016-01 Traceroute Detour Validation 2016-05-01 IXP websites, PeeringDB, PCH CAIDA AS Relationship RIPE Atlas MaxMind Prefix Geolocation; Detour Validation 2016-01, 2016-03 MaxMind GeoLite City (free and paid) 368 IXP websites crawled 482,657 distinct relationships Used by Netra, 163 traceroutes Paid version used only for geolocating infrastructure IPs and detour validation longest prefix match on the global routing table and map the IP to the AS announcing the longest matching prefix|,Data,164
| As shown in Figure 3, we install LaBrea on a /29 subnetwork and use PlanetLab [9] to probe from multiple vantage points the entire /24 aggre- gate to which the /29 belongs. We scan the /24 network by attempting to establish TCP connections to each IP address in the subnet and capture the packets for further analysis|,Data,165
| • Active IPs in a Subnet: Intuitively, we might ex- pect high-occupancy subnets to be good indicators of pos- sible tarpits. To this end, we initially investigated using a hitlist of probable tarpits as inferred from the /24 subnets with more than 240 responding web hosts in the scans|,Data,165
| To facilitate large-scale scanning and avoid triggering anomaly detectors, degreaser uses permu- tation scanning [7, 12] to pseudo-randomly iterate through the IP address space when probing. Our real-world Internet scan, which probes at least one address in each /24 network in the Internet, discovers 107 different tarpit subnetworks (cid:20)(cid:24)(cid:25) ranging in size from /16 (with up to 216 fake hosts) to /24 (with up to 28 fake hosts)|,Data,165
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
| • Discovering correlations between anomalous traffic types detected with deep inspection techniques and traffic feature entropy variations. • Providing a traffic-type dissection (in-depth and entropy based) of a representative portion of the IBR for three weeks of April, 2012, with a 10-minute time scope|,Data,167
 Following is the summary of information about these data sets:  1. Data set from PREDICT USA [24] which contains traces of a DNS distributed denial of service attack (DDOS),Data,168
  from optical  2. Data set from CAIDA USA [25] which contains internet internet connectivity from 2002 and 2003,Data,168
  3. Data set from our experiment in which a PCAP file is captured from a lab computer which is being used for browsing and software development for the cyber security project,Data,168
|3 Differential Privacy The definition of (, δ)-DP in Definition 1 has been now made precise with Definition 3 (neighbor) and Definition 4 ((, δ)-closeness) We define -DP and (δ)-DP by setting either of the two parameters to zero Definition 5 (-Differential Privacy [12]) A ran- domized mechanism PY ||Xn satisfies -DP if it satisfies (, 0)- DP|,Non-data,29
| (10) Definition 6 ((δ)-Differential Privacy) A random- ized mechanism PY ||Xn satisfies (δ)-DP if it satisfies (0, δ)- DP Mutual-information differential privacy was defined in the introduction in Definition 2 Finally, let us define one additional privacy metric based on Kullback-Leibler divergence, which we will call KL-DP|,Non-data,29
| Definition 7 (KL Differential Privacy) A random- ized mechanism PY ||Xn satisfies -KL-DP if for all neighbor- ing database instances xn and ̃xn D(cid:0)PY ||Xn=xn (cid:13)(cid:13)PY ||Xn= ̃xn (cid:1) ≤  nats (14) Property 2 By Pinsker’s inequality, D(P(cid:107)Q) ≤  nats =⇒ P (cid:16) √ ≈ 0, /2 (cid:17) 2|,Non-data,29
|4 Ordering of Privacy Metrics Q (12) This work is about showing equivalence of privacy metrics In order to do so, we must define an ordering 45Definition 8 (Stronger Privacy Metric)|,Non-data,29
| As a place- holder, take α-DP and β-DP to represent two generic pri- vacy guarantees with positive parameters α and β We say that α-DP is stronger than β-DP, denoted as α-DP (cid:23) β-DP, (15) if for all β(cid:48) > 0 there exists an α(cid:48) > 0 such that α’-DP =⇒ β’-DP (16) If the parameters are vectors, then β(cid:48) > 0 and α(cid:48) > 0 should be interpreted as inequalities on each coordinate Example 1|,Non-data,29
| It is clear that -DP (cid:23) (, δ)-DP and (δ)- DP (cid:23) (, δ)-DP, since -DP implies (, δ)-DP for any non- negative δ, and likewise for (δ)-DP, by definition Also, (, δ)-DP = (δ)-DP by Property 3 Notice that even if we set (cid:48) = 0, the quantity δ(cid:48), as defined in the property, goes to zero as  and δ go to zero and a mutual information constraint|,Non-data,29
| As in this work, there is a maximization over distributions of inputs to the random- ized mechanism; however, conditional mutual information is not a part of that result, while it is a necessary ingredient here Also, the notion of (, δ)-closeness goes by the name of Eγ distance in some of the information theory literature, (,δ)≈ Q is equivalent to such as [22] and [19] Specifically, P the pair of statements Ee (P(cid:107)Q) ≤ δ and Ee (Q(cid:107)P ) ≤ δ 3|,Non-data,29
|3 Proof of Theorem 1 We prove (17) of Theorem 1 by proving a stronger chain of inequalities: -DP (A)(cid:23) KL-DP (B)(cid:23) MI-DP (C)(cid:23) (δ)-DP (D) = (, δ)-DP (20) It is worth noting both (A) and (B) are in fact strict or- derings ((cid:31))—the reverse implications do not hold, even if cardinality bounds are assumed 3 MAIN RESULT 3|,Non-data,29
|1 Equivalence The emphasis of this work is the equivalence of mutual- information differential privacy with classical differential pri- vacy Theorem 1 (Main Result) -DP (cid:23) MI-DP (cid:23) (, δ)-DP (17) Therefore, We now state the components of the proof in separate lemmas|,Non-data,29
| Orderings (A) and (B) are the subject of Lemma 1, and ordering (C) is handled by Lemma 2 Equality (D) comes from Property 3, as discussed in Example 1 Lemma 1 (Orderings (A) and (B)) -DP =⇒ min(cid:8), 2(cid:9)-KL-DP, -DP =⇒ min(cid:8), 2(cid:9)-MI-DP|,Non-data,29
| -KL-DP =⇒ -MI-DP (21) (22) (23) Proof The first statement, (21), is established by Prop- erty 1 Both -DP and KL-DP are defined the same way in terms of neighboring database instances|,Non-data,29
| I(Xi; Y ||X The second statement, (22), is best understood through the geometric interpretation of capacity as the radius of the information ball [8, Theorem 1311] The radius cannot be more than the maximum of pairwise distances|,Non-data,29
| However, we will not directly use that machinery here Instead, consider the following direct proof Start by assuming that the randomized mechanism PY ||Xn satisfies -KL-DP Let i ∈ {1, |,Non-data,29
|   , n} and PXn be arbitrary For notational clarity, let ̄X n ∼ PXn , and begin with a rep- resentation of conditional mutual information for a general distribution in terms of Kullback-Leibler divergence: (cid:13)(cid:13)PY ||X−i= ̄X−i (cid:13)(cid:13)PY ||X−i=x−i (cid:1) for each in- (cid:105) −i) = E(cid:2)D(cid:0)PY ||Xn= ̄Xn Now we bound D(cid:0)PY ||Xn=xn PY ||X−i=x−i = E(cid:104) (cid:13)(cid:13)PY ||X−i=x−i D(cid:0)PY ||Xn=xn (cid:13)(cid:13)(cid:13)E(cid:104) (cid:16) (cid:13)(cid:13)(cid:13)PY ||Xi= ̃X,X−i=x−i (cid:16) ≤ E(cid:104) stance xn|,Non-data,29
| Fix xn arbitrarily, and let ̃X ∼ PXi||X−i=x−i  PY ||Xi= ̃X,X−i=x−i Therefore, by Jensen’s inequality, and using the fact that D(·(cid:107)·) is convex in the second argument, we conclude, PY ||Xi= ̃X,X−i=x−i = D PY ||Xn=xn (cid:105)(cid:17) (cid:17)(cid:105)  (25) Consider, (cid:1) (cid:1)(cid:3) (24) PY ||Xn=xn D ≤  nats, (26) where the last inequality is due to the fact that any two databases that agree on X−i are neighbors Furthermore, if the cardinality of the database entries or the query response is bounded, then MI-DP = (, δ)-DP, (18) where the relationship (, δ)-DP (cid:23) MI-DP is dependent on the cardinality bound (cid:110)||Y||, max ||Xi||(cid:111) i min |,Non-data,29
 (19) Precise bounds for the privacy parameters are given in the three lemmas in Section 33 32 Related Work Using information theoretic measures to quantify the pri- vacy guarantee of differential privacy is not a new idea,Non-data,29
| An upper bound of mutual information is shown in [20] in a two- party differential privacy setting Later this upper bound is used in [9] to get I(X n; Y ) ≤ 3n In [3] and [4], min-entropy is considered rather than the usual Shannon entropy, and up- per bounds are proven In fact, [4, Corollary 1] implies an ordering relationship similar to the first inequality of (17) but for min-entropy based information leakage with only a single database entry|,Non-data,29
| In [25], a “mutual information pri- vacy” metric is defined and studied These works have in common that they all consider the use of unconditional mutual information This doesn’t cap- ture the structure in the definition of differential privacy and the bounds are limited to the mutual information between the whole database and the sanitized query output, with no focus on individual entries Needless to say, an equivalence is not established|,Non-data,29
| Some of the information theory literature bares resem- blance to this work In [5], similar proof steps to this work are used to show an equivalence between semantic security 46y ∈ A, y /∈ A 1, 0, (29) PY ||X1=x1 = (27) Lemma 2 (Ordering (C)) -MI-DP =⇒ (cid:16) -MI-DP =⇒ (cid:0)0, δ 0, (cid:17) (cid:48)(cid:1)-DP, √ 2 -DP|,Non-data,29
| In fact, the tightest possible statement of this form is (28) with δ(cid:48) = 1 − 2h−1(ln 2 − ), where h−1 is the inverse of the increasing part of the binary entropy function in units of nats This formula holds for  ∈ [0, ln 2] For  > ln 2, the implication becomes (1)-DP, which is vacuous The claim in (27) is looser than that in (28) but asymp- totically tight for small |,Non-data,29
| Proof The essence of this claim is found in the binary case, with a binary database and a binary query response We show this reduction first Start by assuming that the randomized mechanism PY ||Xn satisfies -MI-DP|,Non-data,29
| Consider an arbitrary pair of neighbor- ing database instances xn and ̃xn, and let i be the location where they differ Denote by ∆xn, ̃xn the subset of proba- bility distributions over the space of databases D that only put positive mass on xn and ̃xn Therefore, all distributions in ∆xn, ̃xn are binary, and X−i is deterministic with respect to any of them Also, let A be an arbitrary measurable subset of Y|,Non-data,29
| Con- sider the indicator function B(y) = (cid:40) The random variable B is the binary function B(Y ) max PXn∈∆xn, ̃xn I(Xi; B) (a)≤ max PXn∈∆xn , ̃xn I(Xi; Y ) I(Xi; Y ||X (b) = ≤ sup PXn∈∆xn, ̃xn max I(Xi; Y ||X −i) PXn (c)≤  nats, −i) (30) where (a) is due to the data processing inequality, (b) comes from the fact that X−i is deterministic for all distributions in ∆xn, ̃xn , and (c) is by assumption of -MI-DP To summarize, we have arrived at a binary input and binary output randomized mechanism PB||Xi , where Xi ∈ {xi, ̃xi}, defined by PB||Xi=xi ({1}) = PY ||Xn=xn (A), PB||Xi= ̃xi ({1}) = PY ||Xn= ̃xn (A) (31) (32) This mechanism is shown in (30) to satisfy -MI-DP|,Non-data,29
| Also, since A, xn, and ̃xn were chosen arbitrarily, any (δ)-DP claim that can be made about PB||Xi must also hold for PY ||Xn  In Appendix C, we complete the proof by showing that Lemma 2 holds for all randomized mechanisms with a binary input and binary output, and that the characterization is tight A more complete characterization is also possible, of the -MI-DP =⇒ (cid:0) (cid:48) , δ (cid:48)(cid:1)-DP, form for a particular set of values ((cid:48), δ(cid:48)) which, among other things, has the property that δ(cid:48) must be greater than some positive threshold which depends on , and as δ(cid:48) approaches this threshold, (cid:48) must go to infinity This characterization is also arrived at by first reducing to the binary case as we have done above|,Non-data,29
| However, a description of the trade-off is too unwieldy for this discussion We prove (18) of Theorem 1 with the following claim The proof is given in Appendix D Lemma 3 (Reverse direction)|,Non-data,29
| If ||Xi|| is finite for all i ∈ {X1,    , Xn}, or if ||Y|| is finite, then where, for any δ ∈ [0, 1], (δ)-DP =⇒  (cid:48) -MI-DP, (cid:16) (cid:110)||Y||, max i (cid:48)  = 2h(δ) + 2δ ln min ||Xi|| + 1 (cid:111)(cid:17) (34) |,Non-data,29
| (35) Slightly tighter bounds can be found in (105) and (126) of the proof Although these bounds may have some looseness, the following example shows that they get roughly within a factor of two of the correct scaling for large cardinalities Example 2 (Erasure channel) Consider a database with only one entry, X1|,Non-data,29
| Let X1 = [N ] and Y = [N ] ∪ {0} Define 1 − δ, δ, 0, y = 0, y = x1, otherwise (36) This randomized mechanism is usually referred to as an erasure channel, where the output Y = 0 is considered an erasure It is known that the capacity of this channel is C = δ log N, (37) where N = ||X1|| = ||Y|| − 1|,Non-data,29
| This implies that there exists a distribution of the database (in this case, the uniform distri- bution for X1 ∈ X1) such that I(X1; Y ||X −1) = I(X1; Y ) = δ log ||X1|| = δ log(||Y|| − 1) (38) 4 PROPERTIES OF DIFF PRIVACY Now that we have MI-DP as an equivalent metric of pri- vacy, we explore the insights that this brings and simple proofs of properties about privacy|,Non-data,29
| The following are three basic and well-known properties of mutual information: Property 4 If U is independent of W , then I(U ; V ||W ) ≥ I(U ; V ) (39) Property 5 If U , V , and W form a Markov chain U − V −W , meaning that U and W are conditionally independent given V , then I(U ; V ||W ) ≤ I(U ; V )|,Non-data,29
| (Chain rule) Property 6 I(U ; V, W ) = I(U ; V ) + I(U ; W||V ) (40) (41) We will use these three properties (sometimes conditioned (33) on other random variables) to make claims about MI-DP 474|,Non-data,29
|1 The Strong Adversary Assumption We refer to a strong adversary as one who knows the entire database except for any one entry Xi Differential privacy is implicitly designed as a protection against further infor- mation leakage to this adversary The definition of MI-DP, now shown to be equivalent, makes this attribute explicit by conditioning on the remainder of the database and bounding I(Xi; Y ||X−i) But how much information does the sanitized output Y leak to an adversary with no prior knowledge? In [18], this is referred to as evidence of participation|,Non-data,29
| In the mutual information context, this may be measured by the unconditional mutual information I(Xi; Y ) It is pointed out in [18] that if the entries of the database are independent, the evidence of participation can be protected properly by differential privacy This claim is straightforward using MI- DP in light of Property 4 Corollary 1 (Independent Data)|,Non-data,29
| If {Xi}n tually independent and PY ||Xn satisfies -MI-DP, then i=1 are mu- sup P i,PXi X−i I(Xi; Y ) ≤ sup X−i P i,PXi I(Xi; Y ||X −i) ≤  nats (42) On the other hand, it is often the case that entries of a database are correlated Differential privacy does not pro- vide a strong guarantee about the evidence of participation in general Consider the following familiar example: Example 3 (Correlated database)|,Non-data,29
 Consider a database with n binary entries A data curator decides to release the mean of all entries and chooses the Laplace mechanism Noise with distribution Lap( 1 n ) is added to the sample mean to ensure -DP (also -MI-DP by Lemma 1) Now suppose all database entries are in fact equal to each other (maximally correlated),Non-data,29
| Let X ∼ Bern(05) and Xi = X for all i ∈ {1,   |,Non-data,29
| , n} For large enough n, the noise added is negligible, and the binary value of the sample mean can be estimated with high accuracy, revealing each individual entry In terms of mutual information, I(Xi; Y ) ≈ 1 bit for each i even while I(Xi; Y ||X−i) = 0 because H(Xi||X−i) = 0 4|,Non-data,29
|2 Composition Among the most important properties of differential pri- vacy is composability This states that a collection of queries, each satisfying differential privacy, collectively satisfies dif- ferential privacy with a parameter scaled proportional to the number of queries A great deal of effort has been made in deriving tight composition theorems for differential privacy A straight- forward composition theorem can be found in [13]|,Non-data,29
| More intricate trade-offs can be found in [14] and [17], with the latter establishing a tight characterization The following claims for MI-DP mirror those found in [21] for (, 0)-DP and are in fact tight Corollary 2 (Conditionally independent queries) If several query responses {Y1, |,Non-data,29
|   , Yk} are produced condi- tionally independently given the database, and each mecha- nism PYj||Xn satisfies j-MI-DP individually, then as a col- lection PY k||Xn satisfies (cid:16)(cid:80) -MI-DP (cid:17) j j Proof|,Non-data,29
| For any i and PXn , the chain rule of mutual in- formation (Property 6) gives (a), and Property 5 gives (b): I(Xi; Y k||X −1) I(Xi; Yj||X −i, Y j−1) I(Xi; Yj||X −i) j=1 (a) = k(cid:88) (b)≤ k(cid:88) ≤ k(cid:88) j=1 j nats (43) j=1 Corollary 2 states that the effect of releasing multiple con- ditionally independent query responses has no more than an additive effect on the parameter of privacy It is worth noting two important points First, query responses that are not conditionally independent (i|,Non-data,29
|e the noise from one query response is somehow reused in the next) have no such guarantee, as the following example illustrates Example 4 (Correlated query responses) Consider a database where each entry has a finite alphabet ||Xi|| ≤ ∞|,Non-data,29
| Consider two outputs of a query mechanism, Y1 = X1 ⊕ U and Y2 = U , where U is a uniformly distributed random variable on the set {1,    ,||X1||}, independent of the database instance, and ⊕ is addition modulo ||X1|||,Non-data,29
| In other words, the first output Y1 is X1 encrypted by a one-time pad, and the second output Y2 is the key to the one-time pad Clearly, the combination of Y1 and Y2 reveals X1 and violates differential privacy On the other hand, Example 4 does not imply that corre- lated query responses should not be considered Quite to the contrary, query responses that are carefully constructed to be correlated with each other have the potential to achieve significantly better privacy after multiple queries, as demon- strated in [15] and [6]|,Non-data,29
| In general, the same composition claim of Corollary 2 holds even if the query responses are correlated as long as each response in sequence is specifically designed to sat- isfy differential privacy even with respect to the previous responses The following corollary states this claim, and the proof follows directly from the proof of Corollary 2 simply by skipping (43) Corollary 3 (Sequential queries) If several query responses {Y1, |,Non-data,29
|   , Yk} are produced in sequence, and each mechanism PYj||Xn,Y j−1 satisfies j-MI-DP individually, then as a collection PY k||Xn satisfies (cid:16)(cid:80) -MI-DP (cid:17) j j The next claim is about query responses that each depend on different subsets of the database|,Non-data,29
| Corollary 4 (Partial queries) If several query re- sponses {Y1,   |,Non-data,29
| , Yk} are produced conditionally independently of each other from disjoint subsets of the database entries, denoted as XI1 ,    , XIk , with each mechanism PYj||XIj sat- isfying -MI-DP individually, then as a collection PY k||Xn also satisfies -MI-DP|,Non-data,29
| Proof Let f (i) be the index j such that i ∈ Ij For any i and PXn , the chain rule of mutual information (Property 6) 48gives: I(Xi; Y k||X −1) = I(Xi; Yf (i)||X = I(Xi; Yf (i)||X ≤  nats −i) + I(Xi; Y −i) −f (i)||X −i, Yf (i)) (44) 5|,Non-data,29
| A DISCREPANCY While most properties of -DP or (, δ)-DP are also prop- erties of MI-DP, it turns out that one basic property does not carry over Differential privacy is defined with respect to neighboring database instances What privacy can be guaranteed if some bounded number of entries are changed in the database? Similar to the composition properties, the closeness of the output distribution scales proportionally with the number of database changes The following properties are obtained by repeated application (5) and (6) from Definition 4|,Non-data,29
| Property 7 (Epsilon) Suppose xn and ̃xn are instances of the database that differ in at most k entries, and that the randomized mechanism PY ||Xn is -DP Then PY ||Xn=xn (k,0)≈ PY ||Xn= ̃xn  (45) Property 8 (Delta)|,Non-data,29
| Suppose xn and ̃xn are instances of the database that differ in at most k entries, and that the randomized mechanism PY ||Xn is (δ)-DP Then PY ||Xn=xn (0,kδ)≈ PY ||Xn= ̃xn  (46) Property 9 (General) Suppose xn and ̃xn are in- stances of the database that differ in at most k entries, and that the randomized mechanism PY ||Xn is (, δ)-DP|,Non-data,29
| Then (cid:19) (cid:18) k, ek−1 e−1 δ ≈ PY ||Xn=xn PY ||Xn= ̃xn  (47) On the other hand, MI-DP does not have an analogous property Even if a mechanism satisfies -MI-DP, there may not be a bound on I(XI; Y ||XIc ), where I represents a sub- set of ||I|| = k indices Consider the following example|,Non-data,29
| Example 5 Consider a database with two entries, X1 and X2, which are real valued The randomized mechanism PY ||X1,X2 produces an output Y which can be a real number or one of two special values e1 or e2 The behavior of the mechanism is best described in two cases: (cid:40) (cid:40) If X1 = X2: If X1 (cid:54)= X2: Y = Y = X1, with probability , e1, with probability 1 − |,Non-data,29
| e2, with probability , e1, with probability 1 −  (48) (49) This mechanism satisfies ( ln 2)-MI-DP Notice that for any value of X2 = x2, we have a binary erasure channel from X1 to Y , with binary input determined by whether X1 = x2 or not The symbol e1 serves as the erasure|,Non-data,29
| The symbol e2 represents the unerased indicator that X1 (cid:54)= x2 This binary erasure channel with erasure probability 1 −  has mutual information bounded above by  ln 2 nats (the capacity of the erasure channel) On the other hand, the mutual information I(X1, X2; Y ) is unbounded if there are no constraints on X1 and X2 In- deed, if we let X1 be a continuous random variable, and we set X2 = X1, then I(X1, X2; Y ) = ∞|,Non-data,29
| (50) More generally, if the domains X1 and X2 are equal, then the capacity of the erasure channel gives the achievable mu- tual information (where e2 represents an additional input symbol selected by any choice of X1 (cid:54)= X2): max PX1,X2 I(X1, X2; Y ) =  log(||X1|| + 1) =  log(||Y|| − 1) (51) In fact, Example 5 might be best interpreted as a fortunate advantage of MI-DP With any query mechanism, there is a trade-off between privacy and the informational utility to be gained from the output If we apply Property 7 with k = n, the conclusion is that PY ||Xn=xn (n,0)≈ PY ||Xn= ̃xn (52) for any two databases xn and ̃xn|,Non-data,29
| By revisiting the proof of Lemma 1, we obtain I(X n; Y ) ≤ min(cid:8)n, (n)2(cid:9) nats (53) One way to view this is as a crude bound on the utility of the query output The bound is detrimental if n is not large On the other hand, MI-DP does not imply such a constraint|,Non-data,29
| If, however, we take into account a cardinality bound on the database entries or the query output, then there is indeed an upper bound on the information leaked from a group of database entries This is obtained by using Property 8 in combination with Lemma 2, followed by repeating the proof of Lemma 3 for a group rather than an individual entry Corollary 5 Suppose the randomized mechanism PY ||Xn satisfies -MI-DP|,Non-data,29
| Then for any subset of indices I, with ||I|| = k, and with I c = [n] \ I, I(XI; Y ||XIc ) ≤ 2h sup PXn √ (cid:16) (cid:17) (cid:110)||Y||, (maxi ||Xi||)k + 1 k 2 + 2k (cid:111)  where M = min √ 2 log M, (54) 6 VARIATIONS OF DIFF PRIVACY Many variations of differential privacy have been proposed in the literature to provide different assurances|,Non-data,29
 Here we demonstrate how mutual-information differential-privacy can be adapted to correspond to these various definitions 61 Personalized Differential Privacy Personalized differential privacy [16] addresses the situa- tion where participants of the database may have different concerns about the level of privacy This is handled by as- signing a different i for each database entry Xi,Non-data,29
| That is, for any database instances xn and ̃xn which differ in only the ith place, PY ||Xn=xn (i,0)≈ PY ||Xn= ̃xn  (55) 49The modification to MI-DP would be to require that for 64 Adversarial Privacy each i, I(Xi; Y ||X −i) ≤ i nats (56) sup PXn 6|,Non-data,29
|2 Free-Lunch Privacy Free-lunch privacy was both defined and refuted in [18] as a stronger privacy definition which puts no restriction on which database instances must be indistinguishable A mechanism PY ||Xn is -free-lunch private if every pair of database instances xn and ̃xn satisfies PY ||Xn=xn (,0)≈ PY ||Xn= ̃xn  The MI-DP equivalent of this would be I(X n; Y ) ≤  nats sup PXn (57) (58) We can easily see the strength of this definition by applying the chain rule of mutual information (Property 6) to (58)|,Non-data,29
| The result is that for any pair of disjoint index sets I and J , I(XI; Y ||XJ ) ≤  nats sup PXn (59) On the other hand, (58) and (59) illustrate the poor utility provided by the -free-lunch privacy mechanism, as the in- formation contained in the output is always upper bounded by  regardless of distribution and prior knowledge 63 Bayesian Differential Privacy Bayesian differential privacy [26] deals with the possible privacy degradation of differential privacy if the entries in the database are correlated|,Non-data,29
| As was discussed in Section 41, a weak adversary who has less background knowledge of the database may stand to gain much more information than the adversary who knows all but one entry Bayesian differential privacy is meant to protect simultaneously against all adver- saries, but in order to do so it assumes a prior distribution on the database Given a prior distribution PXn , a mechanism PY ||Xn is - Bayesian differentially private if, for any index i and subset of indices I, PY ||Xi=xi,XI =xI (,0)≈ PY ||Xi= ̃xi,XI =xI |,Non-data,29
| (60) Notice that the conditional distributions are not necessarily conditioned on the entire database indices I, The MI-DP equivalent is, for any index i and subset of I(Xi; Y ||XI) ≤  nats, (61) which is in fact implied by (60) Furthermore, this notion of privacy can be strengthened by maximizing over database distributions, making it a stronger notion of privacy than differential privacy In spite of the additional strength of this privacy metric (especially when removing the Bayesian prior assumption by maximizing over the database distribution), this is not nearly as pessimistic as free-lunch privacy|,Non-data,29
| As a comparison, the chain rule of mutual information (Property 6) in this case implies that for any two disjoint index sets I and J , (62) (63) I(XI; Y ||XJ ) ≤ ||I|| nats Consequently, I(X n; Y ) ≤ n nats Adversarial privacy [23] does three things differently from differential privacy First, it assumes a prior distribution PXn on the database (like Bayesian differential privacy)|,Non-data,29
| Second, it does not restrict attention to neighboring database instances (like free-lunch privacy) Third, it asymmetrically requires ln dPXn||Y =y dPXn ≤  ∀y ∈ Y (64) The idea is that the adversary can not increase certainty about a particular database value by much, even while other database values may be eliminated Since mutual information is the expected value of the quantity on the left of (64), it is clear that adversarial pri- vacy implies I(X n; Y ) ≤  nats|,Non-data,29
| (65) Thus, adversarial privacy has similarity to free-lunch pri- vacy, though in the Bayesian setting The subtleties of the asymmetric constraint are not captured in this MI-DP vari- ant 7 RÉNYI ENTROPY GENERALIZATION The notion of α-mutual-information is the generalization of mutual information using R ́enyi information measures|,Non-data,29
| There are many proposed ways to accomplish such a gener- alization Here we adopt Sibson’s proposal (see [24]): I s α(X; Y ) = min QY Dα(PY ||X(cid:107)QY ||PX ), (66) where Dα is the conditional R ́enyi divergence of order α and the minimization is over all distributions QY on Y Shan- non’s mutual information corresponds to α = 1 all α ≥ 0, given in the following lemma|,Non-data,29
| A simple upper bound holds for α-mutual information for Lemma 4 α(Xi; Y ||X I s (α-mutual-information upper bound) If a randomized mechanism PY ||Xn satisfies -DP, then for all α ≥ 0, i, and instances of the remainder of the database x−i, (67) Proof Let α ≥ 0, i, and PXn be arbitrary In order to abbreviate notation, denote the event {X−i = x−i} as U |,Non-data,29
| Pick an arbitrary xi ∈ Xi, α(Xi; Y ||U ) = min I s Dα(PY ||Xi,U(cid:107)QY ||PXi||U ) −i) ≤  nats −i = x sup PXi QY ≤ Dα(PY ||Xi,U(cid:107)PY ||Xi=xi,U||PXi||U ) = Dα(PY ||Xi,U PXi||U(cid:107)PY ||Xi=xi,U PXi||U ) ∗ (cid:21)α−1 (cid:20) dPY ||Xi,U PXi||U (cid:20) dPY ||Xi,U dPY ||Xi=xi,U PXi||U α − 1 log E , X (Y = 1 1 ∗ ∗ ∗ (Y , X ) = α − 1 log E dPY ||Xi=xi,U (cid:21)α−1 ) , (68) where (Y ∗, X∗) ∼ PY ||Xi,U PXi||U  For α (cid:54)= 1, the -DP constraint implies that e for all values of Xi, thus dPY ||Xi U dPY ||Xi=xi ,U ≤ α(Xi; Y ||U ) ≤ 1 I s log E[e]α−1 α − 1 =  nats (69) 50For the case α = 1, the α-mutual-information reduces to α(Xi; Y ||U ) = Shannon’s mutual information, and we have I s I(Xi; Y ||U ) ≤  from Lemma 1|,Non-data,29
| [9] A De Lower bounds in differential privacy In Theory of Cryptography, pages 321–338|,Non-data,29
| Springer, 2012 [10] J C Duchi, M|,Non-data,29
| I Jordan, and M J Wainwright|,Non-data,29
| Combining Property 7 with the proof of Lemma 4 gives the following corollary: Corollary 6 If the mechanism PY ||Xn satisfies -DP, then α(X n; Y ) ≤ n nats I s sup PXn (70) Furthermore, for α > 0, when maximizing over database distributions PXn , all three notions of α-mutual-information discussed in [24] are equivalent Thus, I a α(X n; Y ) = sup PXn I s α(X n; Y ) = sup PXn sup PXn α(X n; Y ) ≤ n nats|,Non-data,29
| I c (71) In [3] and [4], the information leakage is defined as I∞(X n; Y ) = H∞(X n) − H∞(X n||Y ) where H∞(X n||Y ) = − log E(cid:104) (cid:105) (72)  (73) PXn||Y (xn||Y ) max xn This definition matches Arimoto’s proposal I a∞(X n; Y ), so it is a special case of (71) 8 ACKNOWLEDGEMENTS This work was supported by the Air Force Office of Sci- entific Research (grant FA9550-15-1-0180) and the National Science Foundation (grant CCF-1350595)|,Non-data,29
|ABSTRACT We present PrivInfer, an expressive framework for writing and verifying differentially private Bayesian machine learning algorithms Programs in PrivInfer are written in a rich func- tional probabilistic programming language with constructs for performing Bayesian inference Then, differential pri- vacy of programs is established using a relational refinement type system, in which refinements on probability types are indexed by a metric on distributions Our framework lever- ages recent developments in Bayesian inference, probabilistic programming languages, and in relational refinement types|,Non-data,30
 We demonstrate the expressiveness of PrivInfer by verifying privacy for several examples of private Bayesian inference 1 INTRODUCTION Differential privacy [17] is emerging as a gold standard in data privacy Its statistical guarantee ensures that the prob- ability distribution on outputs of a data analysis is almost the same as the distribution on outputs obtained by per- forming the same data analysis on a (hypothetical) dataset differing in one individual,Non-data,30
| A standard way to ensure dif- ferential privacy is by perturbing the data analysis adding some statistical noise The magnitude and the shape of noise must provide a protection to the influence of an individual on the result of the analysis, while ensuring that the algo- rithm provides useful results Two properties of differential privacy are especially relevant for this work: (1) composabil- ity, (2) the fact that differential privacy works well on large datasets, where the presence or absence of an individual has limited impact These two properties have led to the design ̊ : CNS1565365 and by EPSRC grant EP/M022358/1|,Non-data,30
| Partially supported by NSF grants TC-065060 and TWC- 1513694, and a grant from the Simons Foundation (#360368 to Justin Hsu) supported by NSF grants CNS1237235, Partially Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than the author(s) must be honored Abstracting with credit is permitted|,Non-data,30
| To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from permissions@acmorg CCS’16, October 24 - 28, 2016, Vienna, Austria © 2016 Copyright held by the owner/author(s)|,Non-data,30
 Publication rights licensed to ACM ISBN 978-1-4503-4139-4/16/10  ,Non-data,30
 $1500 DOI: http://dxdoiorg/10,Non-data,30
|1145/29767492978371 of tools for differentially private data analysis Many of these tools use programming language techniques to ensure that the resulting programs are indeed differentially private [2– 4, 6, 19–21, 29, 33] Moreover, property (2) has encouraged the interaction of the differential privacy community with the machine learning community to design privacy-preserving machine learning techniques, e|,Non-data,30
|g [12, 18, 25, 39] At the same time, researchers in probabilistic programming are exploring programming languages as tools for machine learning For example, in Bayesian inference, probabilistic programming allows data analysts to represent the probabilistic model, its parameters, and the data observations as a specially crafted program|,Non-data,30
| Given this program as input, we can then use inference algorithms to produce a distribution over the pa- rameters of the model representing our updated beliefs on them Several works have explored the design of program- ming languages to compute efficiently the updated beliefs in order to produce efficient and usable tools for machine learning, eg [22, 24, 28, 31, 32, 36]|,Non-data,30
| Recently, research in Bayesian inference and machine learning has turned to privacy-preserving Bayesian infer- ence [15, 38, 40, 41], where the observed data is private Bayesian inference is a deterministic process, and directly releasing the posterior distribution would violate differential privacy Hence, researchers have developed techniques to make Bayesian inference differentially private Basic tech- niques add noise on the input data, or add noise on the result of the data analysis, while more advanced techniques can ensure differential privacy by releasing random samples from the posterior distribution instead of releasing the pos- terior distribution itself|,Non-data,30
| The diversity of approaches makes Bayesian inference an attractive target for verification tools for differential privacy In this work we present PrivInfer, a programming frame- work combining verification techniques for differential privacy with learning techniques for Bayesian inference PrivInfer consists of two main components: a probabilistic functional language extending PCF for Bayesian inference, and a rela- tional higher-order type system that can verify differential privacy for programs written in this language The core idea of Bayesian learning is to use conditional distributions to represent the beliefs updated after some observations|,Non-data,30
| PrivInfer, similarly to other programming lan- guages for inference models conditioning on data explicitly using an observe statement An interesting aspect of Bayesian inference is that although the inferred output is a probability 68distribution, the process to generate it is deterministic To guarantee differential privacy, we need to inject some ran- domness into the inference process To handle these two roles of distributions, PrivInfer distinguishes between symbolic dis- tributions and actual distributions|,Non-data,30
| The former represent the result of an inference, while the latter are used to represent random computations, eg differentially private computa- tions (mechanisms) Moreover, we parametrize our language with an algorithm to perform Bayesian inference returning symbolic distributions, expressed by the statement infer, and mechanisms to ensure differential privacy returning actual distributions|,Non-data,30
| Differential privacy is a probabilistic 2-property, ie a property expressed over pairs of execution traces of the pro- gram To address this challenge, we use an approach based on approximate relational higher-order refinement type system, like the ones used in the system HOARe2 [6]|,Non-data,30
 PrivInfer extends this approach to deal with the construc- tions that are needed for Bayesian inference like the observe and infer constructs and the distinction between symbolic and actual distributions Another important aspect of the verification of differential privacy is reasoning about the sen- sitivity of a data analysis This measures the influence that two databases differing in one individual can have on the output Calibrating noise to sensitivity ensures that the data analysis provides sufficient privacy,Non-data,30
| In Bayesian inference, the output of the computation is a distribution (often defined by a few numeric parameters) for which one can consider dif- ferent measures A simple approach is to considered standard metrics (Euclidean, Hamming, etc) to measure the distance between the parameters Another more interesting approach is to consider distances between distributions, rather than the parameters|,Non-data,30
| The type system of PrivInfer allows one to reason about the parameters of a distribution, using stan- dard metrics, but also about the distribution itself using f -divergences, a class of probability metrics including some well known examples like total variation distance, Hellinger distance, KL divergence, etc In summary, PrivInfer extends the relational type system approach of HOARe2 in three directions: ‚ providing relational typing rules for observe and infer, ‚ providing relational typing rules to reason about sym- ‚ generalizing the probability polymonad of HOARe2 to bolic and actual distributions, reason about general f -divergences The combination of these three contributions allows us to ad- dress Bayesian inference, which is not supported by HOARe2 To illustrate the different features of our approach we show how different basic Bayesian data analysis can be guaranteed differentially private in three different ways: by adding noise on the input, by adding noise on the parameters with sen- sitivity measured using the (cid:2)p-norms, and finally by adding noise on the distributions with sensitivity measured using f -divergences|,Non-data,30
| This shows that PrivInfer can be used for a diverse set of Bayesian data analyses Summing up, the contributions of our work are: ‚ A probabilistic extension PCFp of PCF for Bayesian inference that serves as the language underlying our framework PrivInfer (§ 4) This includes observe and infer statements as well as primitives for handling sym- bolic and actual distributions ‚ A higher-order approximate relational type system for reasoning about properties of two runs of programs from PrivInfer (§ 5)|,Non-data,30
| In particular, the type system permits to reason about f -divergences The f -divergences can be used to reason about differential privacy as well as about program sensitivity for Bayesian inference ‚ We show on several examples how PrivInfer can be used to reason about differential privacy (§ 6) We will explore three ways to guarantee differential privacy: by adding noise on the input, by adding noise on the output parameters based on (cid:2)p-norms, and by adding noise on the output parameters based on f -divergences|,Non-data,30
| Our work is motivated by Bayesian inference, a statisti- 2 BAYESIAN INFERENCE cal method which takes a prior distribution Prpξq over a posterior distribution Prpξ || xq, an updated version of the parameter ξ and some observed data x, and produces the prior distribution Bayesian inference is based on Bayes’ theorem, which gives a formula for the posterior distribution: Prpξ || xq “ Prpx || ξq ̈ Prpξq Prpxq The expression Prpx || ξq is the likelihood of ξ when x is observed This is a function Lxpξq of the parameter ξ for fixed data x, describing the probability of observing the data x given a specific value of the parameter ξ|,Non-data,30
| Since the data x is considered fixed, the expression Prpxq denotes a nor- malization constant ensuring that Prpξ || xq is a probability distribution The choice of the prior reflects the prior knowl- edge or belief on the parameter ξ before any observation has been performed In practice, the prior and the likelihood are typically chosen so that the posterior belongs to the same family of distributions as the prior In this case the prior is said to be conjugate prior for the likelihood|,Non-data,30
| Using conjugate priors, besides being mathematically convenient in the derivations, ensures that Bayesian inference can be performed by a recursive process over the data Our goal is to perform Bayesian inference under differential privacy We provide the formal definition of differential privacy in Definition 31, but for the purpose of this section it is enough to know that differential privacy is a statistical guarantee that requires the answers of a data analysis to be statistically close when run on two adjacent databases, i|,Non-data,30
|e databases that differ in one individual In the vanilla version of differential privacy, the notion of “statistically close” is measured by a parameter  A typical way to achieve differential privacy is to add random noise, and we present several primitives for doing this in § 3|,Non-data,30
| For one example, the exponential mechanism (denoted ExpMech) returns a possible output with probability proportional to a quality score function Q The function Q takes in input a database and a potential output for the statistic computed on the database, and gives each output a score representing how good that output is for that database The privacy and the utility of the mechanism depend on  and on the sensitivity of the quality score function, ie|,Non-data,30
|, how much the quality score can differ for two adjacent databases As a motivating example we will consider a simple Bayesian inference task: learning the bias of a coin from some obser- vations For example, we can think of the observations as medical records asserting whether patients from a sample 69population have a disease or not We can perform Bayesian inference to establish how likely it is to have the disease in the population|,Non-data,30
| We will show how to make this task differentially private, and verify privacy in PrivInfer First, the input of this example is a set of binary observa- tions describing whether any given patient has the disease— this is the private information that we want to protect We assume that the number of patients n is public and that the adjacency condition for differential privacy states that two databases differ in the data of one patient In our concrete case this means that two databases d, d are adjacent if all of their records are the same except for one record that is 0 in one database and 1 in the other|,Non-data,30
| 1 While in abstract our problem can be described as esti- mating the bias of a coin, we need to be more formal and provide the precise model and the parameters that we want to estimate We can incorporate our initial belief on the fairness of the coin using a prior distribution on the bias ξ given by a beta distribution This is a distribution over r0, 1s with probability density: betapξ || a, bq “ ξa ́1p1 ́ ξb ́1q Bpa, bq where a, b P R` are parameters and B denotes the beta function The likelihood is the probability that a series of i|,Non-data,30
|id samples from a Bernoulli distributed random variable with bias ξ matches the observations Using an informal notation,1 we can write the following program in PrivInfer:  ̆  ̄ betapa, bq (1) infer observe λrbernoulliprq “ obs  ́ ` The term infer represents an inference algorithm and the observe statement is used to describe the model|,Non-data,30
| Expression (1) denotes the posterior distribution that is computed using Bayes’ theorem with prior betapa, bq and with likelihood pλrbernoulliprq “ obsq Now, we want to ensure differential privacy We have several options|,Non-data,30
| A first natural idea is to perturbate the input data using the exponential mechanism, corresponding to the following program:  ̄ observepλrbernoulliprq “ ExpMech Q obsq betapa, bq  ́ infer The fact that differential privacy is closed under post- processing ensures that this guarantees differential privacy for the whole program In more detail, in the notation above we denoted by Q the scoring function Since obs is a boolean, we can use a quality score function that gives score 1 to b if b “ obs and 0 otherwise|,Non-data,30
| This function has sensitivity 1 and so one achieves p, 0q-differential privacy This is a very simple approach, but in some situations it can already be very useful [38] posterior which is a betapa A different way of guaranteeing differential privacy is by adding noise on the output In this case the output is the |,Non-data,30
| Us- ing again the exponential mechanism we can consider the following program: observepλrbernoulliprq “obsqbeta pa, bq ̆ ̄ 1q for some values a ExpMech Q infer  ́ ` , b , b 1 1 1 So, a natural question is which Q we can use as quality score function and what is its sensitivity in this case 1 , b There are two natural choices The first one is to consider distance in term of some metric on vectors, e|,Non-data,30
|g the one given the parameters pa 1q as a vector and measure the possible by (cid:2)1 norm dppa, bq,pa 1|| The second is 1 to consider betapa 1q as an actual distribution and then use ΔHpbetapa, bq, betapa 1 a notion of distance on distributions, eg|,Non-data,30
| Hellinger distance 1qq “ ||a ́ a 1||`||b ́ b 1qq , b , b , b 1 These two approaches both guarantee privacy, but they have different utility properties Our system PrivInfer can prove privacy for both approaches 3|,Non-data,30
| BACKGROUND 31 Probability and Distributions In our work we will consider discrete distributions Fol- lowing Dwork and Roth [16] we will use standard names for several continuous distributions but we will consider them to be the approximate discrete versions of these distributions up to arbitrary precision ř We define the set DpAq of distributions over a set A as the set of functions μ : A Ñ r0, 1s with discrete supportpμq “ tx || μ x ‰ 0u, such that xPA μ x “ 1|,Non-data,30
| In our language we will consider only distribution over basic types, this guarantees that all our distributions are discrete (see § 4) We will use several basic distributions like uniform, bernoulli, normal, beta, etc These are all standard distributions and we omit their definition here We will also use some notation to describe distributions|,Non-data,30
| For instance, given an element a P A, we will denote by 1a the probability distribution that assigns all mass to the value a We will also denote by bind μ M the composition of a distribution μ over the set A with a function M that takes a value in A and returns a distribution over the set B 32 Differential Privacy 1 1 Differential privacy is a strong, quantitative notion of statistical privacy proposed by Dwork et al|,Non-data,30
| [17] In the standard setting, we consider a program (sometimes called a mechanism) that takes aprivate database d as input, and produces a distribution over outputs Intuitively, d represents a collection of data from different individuals When two are identical except for a single individual’s databases d, d are adjacent 2, and we write record, we say that d and d |,Non-data,30
| Then, differential privacy states that the output d Φ d distributions obtained from running the program on two adjacent databases should be statistically similar More formally: numeric parameters, let D be the set of databases, and let Definition 31 (Dwork et al [17])|,Non-data,30
| Let , δ ą 0 be two R be the set of possible outputs A program M : D Ñ DpRq satisfies p, δq-differential privacy if 1 PrpMpdq P Sq ď e PrpMpd and for every subset of outputs S Ď R for all pairs of adjacent databases d, d 1q P Sq ` δ 1 P D such that d Φ d 1 , In this case, the exponential mechanism is not applied to booleans but instead to distributions of the shape betapa 1q 1We omit in particular the monadic probabilistic construc- tions|,Non-data,30
| A formal description of this example is in § 6 , b 1 As shown by Barthe et al [3], we can reformulate differen- tial privacy using a specific statistical -distance -D: 2In our concrete examples we will consider sometime as adja- cent also two databases that differ by at most one individual 70Lemma 3|,Non-data,30
|1 Let , δ P R`  Let D be the set of databases, and let R be the set of possible outputs A program M : D Ñ DpRq satisfies p, δq-differential privacy iff -DpMpdq, Mpd 1qq ď ` are adjacent databases and δ, whered, d 1 -Dpμ1, μ2q ”max EĎR rx P Es ́ e ̈ Pr xÐμ2 Pr xÐμ1 rx P Es ̆ for μ1, μ2 P DpRq|,Non-data,30
| Differential privacy is an unusually robust notion of privacy It degrades smoothly when private mechanisms are composed in sequence or in parallel, and it is preserved under any post- processing that does not depend on the private database The following lemmas capture these properties: Lemma 32 (Post-processing)|,Non-data,30
| Let M : D Ñ DpRq be an p, δq-differentially private program Let N : R Ñ DpR 1q be an arbitrary randomized program Then λdbindpM dq N : D Ñ DpR 1q is p, δq-differentially private|,Non-data,30
| Differential privacy enjoys different composition schemes, we report here one of the simpler and most used Lemma 33 (Composition) Let M1 : D Ñ DpR1q, and M2 : D Ñ DpR2q respectively p1, δ1q and p2, δ2q differentially private programs|,Non-data,30
| Let M : D Ñ DpR1 ˆ R2q the program defined as Mpxq ” pM1pxq, M2pxqq Then, M is p1` 2, δ1` δ2q differentially private Accordingly, complex differentially private programs can be easily assembled from simpler private components, and researchers have proposed a staggering variety of private algorithms which we cannot hope to summarize here (Inter- ested readers can consult Dwork and Roth [16] for a textbook treatment|,Non-data,30
|) While these algorithms serve many different purposes, the vast majority are constructed from just three private operations, which we call primitives These primitives offer different ways to create private mechanisms from non-private functions Crucially, the function must satisfy the following sensitivity property:  Suppose f : A Ñ B is a Definition 3|,Non-data,30
|2 Let k P R` function, where A and B are equipped with distancesd A and dB Then f is k-sensitive if dBpfpaq, fpa 1 P A 1qq ď k ̈ dApa, a 1q for every a, a Intuitively, k-sensitivity bounds the effect of a small change in the input, a property that is similar in spirit to the differ- ential privacy guarantee|,Non-data,30
| With this property in hand, we can describe the three basic primitive operations in differential privacy, named after their noise distributions The Laplace mechanism The first primitive is the stan- dard way to construct a private version of a function that maps databases to numbers Such functions are also called numeric queries, and are fundamental tools for statistical analysis|,Non-data,30
| For instance, the function that computes the aver- age age of all the individuals in a database is a numeric query When the numeric query has bounded sensitivity, we can use the Laplace mechanism to guarantee differential privacy Definition 33|,Non-data,30
| Let  P R` and let f : D Ñ R be a numeric query Then, the Laplace mechanism maps a database d P D to fpdq ` ν, where ν is drawn form the Laplace distribution with scale 1{ This distribution has the following probability density function: Lap1{ pxq “  2 expp ́||x||q If f is a k-sensitive function, then the Laplace mechanism is pk, 0q-differentially private|,Non-data,30
| The Gaussian mechanism The Gaussian mechanism is an alternative to the Laplace mechanism, adding Gaussian noise with an appropriate standard deviation to release a numeric query Unlike the Laplace mechanism, the Gaussian mechanism does not satisfy p, 0q-privacy for any  However, it satisfies p, δq-differential privacy for δ P R` Definition 3|,Non-data,30
|4 Let , δ P R and let f : D Ñ R be a numeric d P D to fpdq ` ν, where ν is a drawn from the Gaussian query Then, the Gaussian mechanism maps a database  distribution with standard deviation 2 lnp1|,Non-data,30
|25{δq{ If f is a k-sensitive function for k ă 1{, then the Gaussian mechanism is pk, δq-differentially private σp, δq “a The exponential mechanism The first two primitives can make numeric queries private, but in many situations we may want to privately release a non-numeric value|,Non-data,30
| To accomplish this goal, the typical tool is the exponential mechanism [30], our final primitive This mechanism is parameterized by a setR , representing the range of possible outputs, and a quality score function q : D ˆ R Ñ R, assigning a real-valued The exponential mechanism releases an output r P R score to each possible output given a database with approximately the largest quality score on the private database The level of privacy depends on the sensitivity of q in the database|,Non-data,30
| Formally: Definition 35 (McSherry and Talwar [30]) Let  P R` Let R be the set of outputs, and q : D ˆ R Ñ R be the d P D releases r P R with probability proportional to quality score Then, the exponential mechanism on database |,Non-data,30
| ˆ  ̇ Prprq „ exp qpd, rq 2 If f is a k-sensitive function in d for any fixed r P R, then the exponential mechanism is pk, 0q-differentially private 33 f-divergences As we have seen, differential privacy is closely related to function sensitivity To verify differential privacy for the result of probabilistic inferences, we will need to work with several notions of distance between distributions|,Non-data,30
| These distances can be neatly described as f -divergences [13], a rich class of metrics on probability distributions Inspired by the definition of relative entropy, f -divergences are defined by a convex function f  Formally: Definition 36 (Csisz ́ar and Shields [13])|,Non-data,30
| Let fpxq be a convex function defined for x ą 0, with fp1q “ 0 Let μ1, μ2 ÿ denoted Δfpμ1 || μ2q is defined as: distributions over A Then, the f -divergence of μ1 from μ2,  ̄  ́ Δfpμ1 || μ2q “ μ2paqf μ1paq μ2paq aPA 71Simplified form 1 2 1 2  ̄2  ́a μ1paq ́a ||μ1paq ́ μ2paq|| ́ ̄ μ2paq ́ μ1paq ln μ1paq ́ eμ2paq, 0 μ1paq μ2paq max  ̄ aPA aPA ÿ ÿ ÿ ÿ aPA aPA f -diverg SDpxq HDpxq KLpxq -Dpxq 1 2 fpxq ||x ́ 1|| p? x ́ 1q2 x lnpxq ́ x ` 1 maxpx ́ e, 0q 1 2 Table 1: f -divergences for statistical distance (SD), Hellinger distance (HD), KL divergence (KL), and -distance (-D)  ́  ̄ where we assume 0 ̈ fp 0 “ lim tÑ0 0 ̈ f 0 a 0  ́ ̄ q “ 0 and t ̈ f a t “ a lim uÑ8  ́ fpuq  ̄ |,Non-data,30
| u If Δfpμ1 || μ2q ď δ we say that μ1 and μ2 are pf, δq-close Examples of f -divergences include KL-divergence, Hellinger distance, and total variation distance Moreover, Barthe and Olmedo [2] showed how the -distance of Lemma 31 can be seen as an f -divergence for differential privacy|,Non-data,30
| These f -divergences are summarized in Table 1 Notice that some of the f -divergences in the table above are not symmetric In particular, this is the case for KL-divergence and -distance, which we use to describe p, δq-differential privacy We will denote by F the class of functions meeting the requirements of Definition 3|,Non-data,30
|6 Not only do f -divergences measure useful statistical quan- tities, they also enjoy several properties that are useful for formal verification (eg|,Non-data,30
 see [13]) A property that is worth mentioning and that will be used implicitly in our example is the following Theorem 31 (Data processing inequality),Non-data,30
| Let f P F, μ1, μ2 be two distributions over A, and M be a function (potentially randomized) mapping values in A to distributions over B Then, we have: Δfpbind μ1 M, bind μ2 Mq ď Δfpμ1, μ2q Another important property for our framework is compo- sition As shown by Barthe and Olmedo [2] we can compose f -divergences in an additive way More specifically, they give the following definition|,Non-data,30
| Definition 37 (Barthe and Olmedo [2]) Let f1, f2, f3 P F We say that pf1, f2q are f3 composable if and only if for every A, B, two distributions μ1, μ2 over A, and two functions M1, M2 mapping values in A to distributions over B we have Δf3pbind μ1 M1, bind μ2 M2q ď Δf1pμ1, μ2q ` sup Δf2pM1 v, M2 vq v In particular, we have the following|,Non-data,30
| Lemma 34 (Barthe and Olmedo [2]) ‚ p1-D, 2-Dq are p1 ` 2q-DP composable ‚ pSD, SDq are SD composable|,Non-data,30
| ‚ pHD, HDq are HD composable ‚ pKL, KLq are KL composable This form of composition will be internalized by the rela- tional refinement type system that we will present in § 5 4|,Non-data,30
| PrivInfer The main components of PrivInfer are a language that permits to express Bayesian inference models and a type system for reasoning in a relational way about programs from the language 41 The language e The language underlying PrivInfer is a probabilistic pro- gramming extension of PCF that we will call PCFp Ex- pressions of PCFp are defined by the following grammar ::“ x || c || e e || λx|,Non-data,30
| e || || || || || letrec f x “ e || case e with rdi xi ñ eisi return e || mlet x “ e in e observe x ñ e in e || inferpeq || ranpeq bernoullipeq || normalpe, eq || betape, eq || uniformpq lapMechpe, eq || gaussMechpe, eq || expMechpe, e, eq where c represents a constant from a set C and x a variable We will denote by PCFp (X ) the set of expression of PrivInfer where the variables are taken from the set X  simple types of the form We will consider only expressions that are well typed using τ, σ ::“ rτ || Mrrτs || MrDrrτss || Drrτs ||τ Ñ σ rτ ` || r0, 1s ||rτ list ::“ ‚ || B || N || R || R` || R where rτ are basic types|,Non-data,30
 As usual a typing judgment is a judgment of the shape Γ $ e : τ where an environment Γ ` is an assignment of types to variables The simply typed system of PrivInfer is an extension of the one in Barthe et al [6]; in Figure 1 we only present the rules specific to PrivInfer The syntax and types of PCFp extend the ones of PCF by means of several constructors,Non-data,30
| Basic types include the unit type ‚ and types for booleans B and natural numbers N We also have types for real numbers R, positive real numbers R` , positive real number plus infinity R and for real numbers in the unit interval r0, 1s Finally we have lists over basic types ability monad Mrrτs over the basic type rτ , and a type Drrτs representing symbolic distributions over the basic typerτ |,Non-data,30
| The probabilistic monad Mrrτs that can be manipulated by the let- binder mlet x “ e1 in e2 and by the unit return e Symbolic like bernoullipeq for Bernoulli distributions, normalpe1, e2q for probability monad can also be over symbolic distributions Probabilities (actual distributions) are encapsulated in the Simple types combines basic types using arrow types, a prob- distributions are built using basic probabilistic primitives normal distribution, etc These primitives are assigned types as described in Figure 2|,Non-data,30
| For symbolic distributions we also assume that we have an operation getParams to extract the parameters We also have primitives lapMechpe1, e2q, gaussMechpe1, e2q and expMechpe1, e2, e3q that provide im- plementations for the mechanism ensuring differential privacy as described in § 32 learning|,Non-data,30
| The primitive observe x ñ e1 in e2 can be used to Finally, we have three special constructs for representing describe conditional distributions This is a functional version of a similar primitive used in languages like Fun [23] This primitive takes two arguments, a prior e2 and a predicate e1 over x The intended semantics is the one provided by Bayes’ theorem: it filters the prior by means of the observation provided by e1 and renormalize the obtained distribution (see Section § 4|,Non-data,30
|2 for more details) The primitives inferpeq and ranpeq are used to transform symbolic distributions in 72Γ $ e1 : MrT1s Γ, x : T1 $ e2 : MrT2s BindM Γ $ mlet x “ e1 in e2 : MrT2s Γ $ e1 : Mrrτs Γ, x :rτ $ e2 : MrBs Γ $ observe x ñ e2 in e1 : Mrrτs Ran UnitM Γ $ e : Drrτs Γ $ ranpeq : Mrrτs Observe Γ $ e : T Γ $ return e : MrTs Γ $ e : Mrrτs Γ $ inferpeq : Drrτs Infer Figure 1: PCFp type system (selected rules) uniform : Drr0, 1ss bernoulli : r0, 1s Ñ DrBs beta : R` ˆ R` Ñ Drr0, 1ss normal : R ˆ R` Ñ DrRs lapMech : R` ˆ R Ñ MrRs gaussMech : R` ˆ R Ñ MrRs expMech : R ˆ ppD, Rq Ñ Rq ˆ D Ñ MrRs Figure 2: Primitive distributions types actual distributions and vice versa In particular, inferpeq is the main component performing probabilistic inference|,Non-data,30
| 42 Denotational Semantics The semantics of PCFp is largely standard We con- sider only terminating programs and hence we can interpret them in a set-theoretic way Basic types are interpreted in the corresponding sets, e|,Non-data,30
|g (cid:2)‚(cid:3) “ t‚u, (cid:2)B(cid:3) “ ttrue, falseu, (cid:2)N(cid:3) “ t0, 1, 2,   |,Non-data,30
|u, etc As usual, arrow types (cid:2)τ Ñ σ(cid:3) are Mrτs for τ P trτ , Drrτsu is interpreted as the set of discrete interpreted as set of functions (cid:2)τ (cid:3) Ñ (cid:2)σ(cid:3) A monadic type ( (cid:2)Mrτs(cid:3) “(cid:3) μ x “ 1 Types of the shape Drrτs are interpreted in set of symbolic an example, DrBs is interpreted as: μ : (cid:2)τ (cid:3) Ñ R` || supppμq discrete^ representations for distributions parametrized by values As probabilities over τ , i|,Non-data,30
|e: ÿ xP(cid:2)τ (cid:3) (cid:2)DrBs(cid:3) “(cid:3) bernoullipvq || v P r0, 1s( The interpretation of expressions is given as usual under a validation θ which is a finite map from variables to values in the interpretation of types We will say that θ validates an environment Γ if @x : τ P Γ we haveθ pxq P (cid:2)τ (cid:3) For most of the expressions the interpretation is standard, we detail the less standard interpretations in Figure 3|,Non-data,30
| Probabilistic expressions are interpreted into discrete probabilities In particular, (cid:2)return e(cid:3)θ is defined as the Dirac distribution returning (cid:2)e(cid:3)θ with probability one The binding construct mlet x “ e1 in e2 composes probabilities The expression observe x ñ t in u filters the distribution (cid:2)u(cid:3)θ using the pred- icate x ñ t and rescales it in order to obtain a distribution|,Non-data,30
| The observe is the key component to have conditional distri- butions and to update a prior using Bayes’ theorem The semantics of infer relies on a given algorithm3 AlgInf for in- ference We leave the algorithm unspecified because it is not 3In this work we consider only exact inference, and we leave for future works to consider approximate inference We also consider only terminating programs with a well defined se- mantics (e|,Non-data,30
|g no observations of events with zero probability) and where the inference algorithms never fail (this could be easily simulated by using the maybe monad) central to our verification task Symbolic distributions are syntactic constructions, this is reflected in their interpreta- tion|,Non-data,30
| For an example, we give in Figure 3 the interpretation (cid:2)bernoullipeq(cid:3)θ The operator ran turns a symbolic distribu- tion in an actual distribution Its semantics is defined by cases on the given symbolic distribution In Figure 3 we give its interpretation for the case when the given symbolic distribution is bernoullipeq|,Non-data,30
| The cases for the other symbolic distributions are similar The soundness of the semantics is given by the following: Lemma 41 If Γ $ e : τ and θ validates Γ, then (cid:2)e(cid:3)θ P (cid:2)τ (cid:3)|,Non-data,30
 5 RELATIONAL TYPE SYSTEM 51 Relational Typing To reason about differential privacy as well as about f - divergences we will consider a higher-order relational re- finement type system We follow the approach proposed by Barthe et al,Non-data,30
| [6] We will distinguish two sets of variables: XR (relational ) Ť and XP (plain) Associated with every relational variable x P XR, we have a left instance xŸ and a right instance xŹ xPXRtxŸ, xŹu and X ’ for XR’ Y XP |,Non-data,30
| We write XR’ for The set of PrivInfer expressions E is the set of expressions defined over plain variables, ie expressions in PCFppXPq The set of PrivInfer relational expressions E ’ is the set of expressions, defined over plain and relational variables (ex- pressions in PCFppX ’q) where only non-relational variables The sets of relational types T “ tT, U, |,Non-data,30
|  u and assertions A “ tφ, ψ,  |,Non-data,30
| u are defined by the following grammars: ::“ rτ || Mf,δrtx ::rτ || φus || Mf,δrtx :: Drrτs || φus || Drrτs || Πpx :: Tq T || tx :: T || φu T, U P T px P XPq φ, ψ P A ::“ Q px : τq φ px P XRq Q px :: Tq|,Non-data,30
| φ || ΔD || e’ “ e’ || e’ ď e’ || Cpφ1,    , φnq f pe’, e’q ď δ || f P F “ tJ{0, K{0, (cid:4){1, _{2, ^{2, ñ{2u, can be bound|,Non-data,30
| where f, δ, e’ P E ’, and Q P t@,Du refinements of the shape tx :: T || φu This is a refinement Relational types extend simple types by means of relational C type that uses a relational assertion φ stating some rela- tional property that the inhabitants of this type have to satisfy Relational assertions are first order formulas over some basic predicates: ΔD on a specific f -divergence, f P F asserting that f and e’ “ e’ and e’ ď e’ for the equality and inequal- is a convex function meeting the requirements of Definition 3|,Non-data,30
|6, f pe’, e’q ď δ asserting a bound ity of relational expressions, respectively Relational types 73(cid:2)Γ $ return e : Mrτs(cid:3)θ “ 1(cid:2)e(cid:3)θ (cid:2)Γ $ mlet x “ e1 in e2 : Mrσs(cid:3)θ “ d ÞÑ (cid:2)Γ $ observe x ñ t in u : Mrτs(cid:3)θ “ d ÞÑ (cid:2)u(cid:3)θpdq ̈ p(cid:2)t(cid:3) p(cid:2)u(cid:3)θpgq ̈ p(cid:2)t(cid:3) θd x ptrueqq ptrueqqq θg x ÿ gP(cid:2)τ (cid:3)θ (cid:2)Γ $ bernoullipeq : DrBs(cid:3)θ “ bernoullip(cid:2)e(cid:3)θq (cid:2)Γ $ ranpbernoullipeqq : MrBs(cid:3)θ “ d ÞÑ ÿ gP(cid:2)τ (cid:3)θ p(cid:2)e1(cid:3)θpgq ̈ (cid:2)e2(cid:3) θg x pdqq (cid:2)Γ $ infer e : Drτs(cid:3)θ “ AlgInf e # (cid:2)e(cid:3)θ 1 ́ (cid:2)e(cid:3)θ if d “ true otherwise Figure 3: Interpretation for some of PCFp expressions (selected rules) Mf,δrtx :: T || φus, for T P trτ , Drrτsu, and it corresponds to also refines the probability monad This has now the shape a polymonad [26] or parametric effect monads [27] where f and δ are parameters useful to reason about f -divergences|,Non-data,30
| Relational expressions can appear in relational refinements and in the parameters of the probabilistic monad, so the usual arrow type constructor is replaced by the dependent type constructor Πpx :: Tq S Before introducing the typing rule of PrivInfer we need to introduce some notation A relational environment G is a finite sequence of bindings px :: Tq such that each variable x is never bound twice and only relational variables from XR are bound|,Non-data,30
| We write xG for the type of x in G We will denote by || ̈|| the type erasure from relational types to simple the notation } ̈}, to describe the following map from relational environments to environments xs}G} “ x||G|| iff x P dompGq, where s P tŸ,Źu For example, given a relational binding px :: Tq, we have }px :: Tq} “ xŸ : ||T||, xŹ : ||T|| proves typing judgment of the form G $ e1 „ e2 :: T |,Non-data,30
 We will use Γ $ e :: T as a shorthand for Γ $ e „ e :: T  Several of types and its extension to environments We will use instead We can now present our relational type system PrivInfer the typing rules of PrivInfer come from the system proposed by Barthe et al,Non-data,30
 [6] We report some of them in Figure 5 We also extend the subtyping relation of Barthe et al [6] with the rule for monadic subtyping in Figure 4,Non-data,30
| We present the rules that are specific to PrivInfer in Fig- ure 5 The rules UnitM and BindM correspond to the unit and the composition of the probabilistic polymonad These are similar to the usual rule for the unit and composition of mon- ads but additionally they require the indices to well behave In particular pf1, f2q are required to be f3 composable to guarantee that the composition is respected|,Non-data,30
| The rules Infer and Ran are similar to their simply typed version but they also transfer the information on the f -divergence from the indices to the refinements and viceversa The rule Observe requires that the sub-expressions are well typed relationally and it further requires the validity of the assertion: }G} $ Δfpobserve xŸ ñ eŸ in e Ÿ, observe xŹ ñ eŹ in e 1 Źq ď δ 1 2 2 for the δ that can then be used as a bound for the f - divergence in the conclusion This assertion may be surpris- ing at first, since it doesn’t consider the bounds δ and δ for the sub-expressions but instead requires to provide directly a bound for the conclusion—it is not really compositional The reason for this presentation is that a generic bound in term of δ and δ would be often too large to say something useful 1 1 G $ T ĺ U }G} $ fi P F @θ|,Non-data,30
| θ ( G, x :: T ñ (cid:2)f1 ď f2 ^ δ1 ď δ2 ă 8(cid:3)θ }G} $ δi : R ` G $ Mf1,δ1rTs ĺ Mf2,δ2rUs S-M Figure 4: Relational Subtyping (rule for monadic subtyping) about the conclusion In fact, estimating bounds on the f -divergence of conditional distributions is a hard task and often it is only expressed in term of prior perturbations [14] So, instead we prefer to postpone the task to verify a bound to the concrete application where a tighter bound can be found with some calculation We will see some uses of this rule in the examples in § 6|,Non-data,30
| 52 Relational Interpretation We want now to give a relational interpretation of relational types so that we can prove the soundness of the relational type system of PrivInfer Before doing this we need to introduce an important component of our interpretation, the notion of pf, δq-lifting of a relation, inspired from the relational lifting of f -divergences by Barthe and Olmedo [2] T2, let f be a convex function providing an f -divergence and Definition 5|,Non-data,30
|1 (pf, δq-Lifting of a relation Ψ) Let Ψ Ď T1ˆ let δ P R`  Then, we have that μ1 P MrT1s and μ2 P MrT2s are in the pf, δq-lifting of Ψ, denoted Lpf,δqpΨq iff there exist two distributions μL, μR P MrT1 ˆ T2s such that 1 μi pa, bq ą0 implies pa, bq P Ψ, for i P tL, Ru, 2|,Non-data,30
| π1 μL “ μ1 ^ π2 μR “ μ2, and ř 3 ΔfpμL, μRq ď δ y μpx, yq and π2 μ “ λy where π1 μ “ λx|,Non-data,30
| We will call the distributions μL and μR the left and right ř x μpx, yq witnesses for the lifting, respectively This notion of lifting will be used to give a relational interpretation of monadic types We say that a valuation θ validates a relational environment G, denoted θ ( G, ifθ ( }G} and @x P dompGq, pxŸθ, xŹθq P (cid:4)xG(cid:5)θ|,Non-data,30
| The relational interpretation (cid:2)φ(cid:3)θ P tJ,Ku of assertions φ with respect to a valuationθ ( Γ is an extension of the the one provided in Barthe et al [6] In Figure 7 we provide the extensions specific to PrivInfer Notice that we interpret the assertion f pe’ ΔD 2 q ď δ with the corresponding f -divergence|,Non-data,30
| 1 , e’ 74UnitM BindM Observe }G} $ f P F }G} $ δ : R ` G $ return e :: Mf,δrTs G $ e :: T Infer pf1, f2q are f3-composable G, x :: T1 $ e2 :: Mf2,δ2rT2s G $ mlet x “ e1 in e2 :: Mf3,δ1`δ2rT2s G $ e1 :: Mf1,δ1rT1s G $ Mf2,δ2rT2s G, x :rτ $ e 1 :: Mf,δ1rty :: B || yŸ “ yŹus G $ e :: Mf,δrty ::rτ || yŸ “ yŹus :: Mf,δ2rty ::rτ || yŸ “ yŹus }G} $ Δfpobserve xŸ ñ eŸ in e G $ observe x ñ e in e 1 Figure 5: PrivInfer Relational Typing Rules G $ e : Mf,δrtx ::rτ || xŸ “ xŹus G $ inferpeq : tx :: Drrτs ||Δ D f pxŸ, xŹq ď δu G $ e : tx :: Drrτs || ΔD G $ ranpeq : Mf,δrtx ::rτ || xŸ “ xŹus f pxŸ, xŹq ď δu Źq ďδ 1 Ÿ, observe xŹ ñ eŹ in e 1 Ran 2 (cid:2)f P F(cid:3)θ “ (cid:2)f (cid:3)θ P F p(cid:2)e’ 2 q ď δ(cid:3)θ “ Δ(cid:2)f (cid:3)θ f pe’ (cid:2)ΔD 1 , e’ 1 (cid:3)θ, (cid:2)e’ 2 (cid:3)θq ď (cid:2)δ(cid:3)θ ) ! xŸ ÞÑ d1 xŹ ÞÑ d2 Figure 7: Relational interpretation of assertions (added rules) θ ) ! xŸ ÞÑ d1 xŹ ÞÑ d2 (cid:2)φ(cid:3) θ pd1, d2q P (cid:4)tx :: T || φu(cid:5)θ pd1, d2q P(cid:4)T (cid:5)θ f1, f2 P (cid:2)||T|| Ñ ||U||(cid:3) d1, d2 P (cid:2)rτ (cid:3) pd1, d2q P (cid:4)rτ (cid:5)θ @pd1, d2q P (cid:4)T (cid:5)θpf1pd1q, f2pd2qq P (cid:4)U (cid:5) pf1, f2q P(cid:4)Πpx :: Tq U (cid:5)θ d1, d2 P (cid:2)Drrτs(cid:3) pd1, d2q P (cid:4)Drrτs(cid:5)θ μ1, μ2 P (cid:2)Mr||T||s(cid:3) Lf,δp(cid:4)T (cid:5)θq μ1 μ2 pμ1, μ2q P(cid:4)M f,δrTs(cid:5)θ Figure 6: Relational interpretation of types We give in Figure 6 the relational interpretation (cid:4)T (cid:5)θ of a relational type T with respect to the valuation θ ( }G} This corresponds to pairs of values in the standard interpre- tation of PCFp expressions|,Non-data,30
| To define this interpretation we use both the interpretation of relational assertions given in Figure 7 and the definition of lifting given in Definition 51 The interpretation of relational assertions is used in the inter- pretation of relational refinement types, while the lifting is used to provide interpretation to the probabilistic polymonad Notice that the relational interpretation of a type Drrτs is of Drrτs|,Non-data,30
 This can then be restricted by using relational just the set of pairs of values in the standard interpretation refinement types We can then prove that the relational re- finement type system is sound with respect to the relational interpretation of types Theorem 51 (Soundness),Non-data,30
| If G $ e1 „ e2 :: T , then for every valuation θ ||ù G we have p(cid:2)e1(cid:3)θ, (cid:2)e2(cid:3)θq P (cid:4)T (cid:5)θ The soundness theorem above give us a concrete way to reason about f -divergences Corollary 51 (f -divergence)|,Non-data,30
| If $ e :: Mf,δrty :: τ || yŸ “ yŹus then for every pμ1, μ2q P (cid:2)e(cid:3) we have Δfpμ1, μ2q ď δ Moreover, thanks to the characterization of differential privacy in terms of f -divergence given by Barthe and Olmedo [2] we can refine the previous result to show that PrivInfer accurately models differential privacy Corollary 52 (Differential Privacy)|,Non-data,30
| If $ e :: tx :: σ || Φu Ñ M-D,δty :: τ || yŸ “ yŹu then (cid:2)e(cid:3) is p, δq-differentially private wrt adjacency relation (cid:2)Φ(cid:3)|,Non-data,30
| 6 EXAMPLES In this section we show how we can use PrivInfer to guaran- tee differential privacy for Bayesian learning by adding noise on the input, noise on the output using (cid:2)1 norm, and noise on the output using f -divergences We will show some of these approaches on three classical examples from Bayesian learning: learning the bias of a coin from some observations (as discussed in § 2), its generalized process, ie the Dirich- let/multinomial model and the learning of the mean of a Gaussian|,Non-data,30
| In all the example we will use pseudo code that can be easily desugared into the language presented in § 4 Indeed, the following examples have been type-checked with an actual tool implementing PrivInfer 61 Input perturbation Input perturbation: Beta Learning|,Non-data,30
| Let’s start by revisiting the task of inferring the parameter of a Bernoulli distributed random variable given a sequence of private observations We consider two lists of booleans with the same length in the adjacency relation Φ iff they differ in the value of at most one entry We want to ensure differential privacy by perturbing the input A natural way to do this, since the observations are boolean value is by using the exponential mechanism|,Non-data,30
| We can then learn the bias from the perturbed data The post-processing property of differential privacy ensures that we will learn the parameter in a private way Let’s start by considering the quality score function for the exponential mechanism A natural choice is to con- sider a function score:boolÑboolÑ{0,1} mapping equal booleans to 1 and different booleans to 0|,Non-data,30
| Remember that the intended reading is that one of the boolean is the one to which we want to give a quality score, while the other is the one provided by the observation The sensitivity of score is 1 Using this score function we can then create a general function for adding noise to the input list: 1 let rec addNoise db eps = match db with 752|,Non-data,30
| 3 4 || [] Ñ return ([]) || y::yl Ñ mlet yn = (expMech eps score y) in mlet yln = (addNoise yl eps) in return(yn::yln) To this function we can give the following type guaranteeing differential privacy tl :: B list || lŸ Φ lŹu Ñ t :: R` ||“u Ñ M-D,0tb :: B list ||“u where we use tx :: T ||“u as a shorthand for tx :: T || xŸ “ xŹu|,Non-data,30
 We will use this shorthand all along this section The bulk of the example is the following function that recursively updates the prior distribution and learns the final distribution over the parameter 1 let rec learnBias dbn prior = match dbn with 2,Non-data,30
| || [] Ñ prior 3 || d::dbs Ñ observe 4 5 (fun r Ñ mlet z = ran bernoulli(r) in return (d=z)) (learnBias dbs prior) The likelihood given in Line 4 is the formal version of the one we presented in§ 2|,Non-data,30
| The function learnBias can be typed in different ways depending on what is our goal For this example we can assign to it the following type: tl :: B list ||“u Ñ MSD,0tx :: r0, 1s ||“u Ñ MSD,0tx :: r0, 1s ||“u The reading of this type is that if learnBias takes two lists of observations that are equal and two prior that are equal, then we obtain two posterior that are equal Thanks to this we can type the occurrence of observe in line 3-4 using a trivial assertion Here we use the SD divergence but in fact this would also hold for any other f P F|,Non-data,30
| In particular, this type allows us to compose it with addNoise using an mlet This type also reflects the fact that the prior is public We can then compose these two procedures in the following program: 1 let main db a b eps = mlet noisyDB = (addNoise db eps) 2|,Non-data,30
| in return(infer (learnBias noisyDB (ran (beta(a,b))))) Notice that in line 2 we use infer for learning from the noised data We can then assign to main the type tl :: B list || lŸ Φ lŹu Ñ ta :: R` ||“u Ñ tb :: R` ||“u Ñ t :: R` ||“u Ñ M-D,0td :: Drr0, 1ss ||“u which guarantees us that the result is  differentially pri- vate Notice that the result type is a polymonadic type over Drr0, 1ss This because we are releasing the symbolic distribution|,Non-data,30
| Input perturbation: Normal Learning An example similar to the previous one is learning the mean of a gaussian distribution with known variance: kv, from a list of real number observations—for instance some medical parameters like the level of LDL of each patient We consider two lists of reals with the same length adjacent when the (cid:2)1 distance between at most two elements (in the same position) is bounded by 1 To perturb the input we may now want to use a different mechanism, for example we could use the Gaussian mechanisms—this may give reasonable results if we expect the data to come from a normal distribution|,Non-data,30
| Also in this case, the sensitivity is 1 The addNoise function is very similar to the one we used in the previous example: 1 2 3|,Non-data,30
| 4 let rec addNoise db eps delta = match db with || [] Ñ return ([]) || y::yl Ñ mlet yn = (gaussMech (sigma eps delta) y) in mlet yln = (addNoise yl eps delta) in return(yn::yln) The two differences are that now we also have delta as input and that in line 3 instead of the score function we have a function sigma computing the variance as in Definition 34 The inference function become instead the following|,Non-data,30
| || [] Ñ prior || d::dbs Ñ observe (fun (r: real) Ñ 1 let rec learnMean dbn prior = match dbn with 2 3 4|,Non-data,30
 5 6 let main db hMean hVar eps delta = 7 8,Non-data,30
| 9 return(infer (learnMean noisyDB mlet noisyDB = (addNoise db eps delta)in (learnMean dbs prior) in mlet z = ran normal(r, kv) in return (d=z)) (ran (normal(hMean,hVar))))) Composing them we get the following type guaranteeing p, δq-differential privacy tl :: R list || lŸ Φ lŹu Ñ ta :: R` ||“u Ñ tb :: R` ||“u Ñ t :: R` ||“u Ñ tδ :: R` ||“u Ñ M-D,δtd :: DrRs ||“u 62 Noise on Output with (cid:2)1-norm We present examples where the privacy guarantee is achieved by adding noise on the output|,Non-data,30
| For doing this we need to compute the sensitivity of the program In contrast, in the previous section the sensitivity was evident because directly computed on the input As discussed before we can compute the sensitivity with respect to different metrics Here we consider the sensitivity computed over the (cid:2)1-norm on the parameters of the posterior distribution|,Non-data,30
| Output parameters perturbation: Beta Learning The main difference with the example in the previous section is that here we add Laplacian noise to the parameters of the posterior let d = infer (learnBias db (ran beta(a,b))) in 1 let main db a b eps= 2|,Non-data,30
| 3 4 mlet aPn = lapMech(eps, aP) in 5 mlet bPn = lapMech(eps, bP) in 6|,Non-data,30
| let (aP, bP) = getParams d in return beta(aPn, bPn) In line 2 we use the function learnBias from the previous section, while in line 4 and 5 we add Laplace noise The formal sensitivity analysis is based on the fact that the posterior parameters are going to be the counts of true and false in the data respectively summed up to the parameters of the prior This reasoning is performed on each step of observe Then we can prove that the (cid:2)1-norm sensitivity of the whole program is 2 and type the program with a type guaranteeing 2-differentially privacy|,Non-data,30
| tl :: B list || lŸ Φ lŹu Ñ ta :: R` ||“u Ñ tb :: R` ||“u Ñ t :: R` ||“u Ñ M2-D,0td :: Drr0, 1ss ||“u Output parameters perturbation: Normal Learning For this example we use the same adjacency relation of the example with noise on the input where in particular the 76where uk “` hV 2 ` n 1  ̆ ́1 number of observation n is public knowledge The code is very similar to the previous one 1|,Non-data,30
|let main db hM hV kV eps = 2 let mDistr = infer (learnMean db (ran normal(hM,kV))) in 3 4 mlet meanN = lapMech(eps/s mean) in 5|,Non-data,30
| let d = normal(meanN, uk) in return(d) let mean = getMean mDistr in kv2 calculations this can be bound by s “ hV  Notice that we only add noise to the posterior mean parameter and not to the posterior variance parameter since the latter doesn’t depend on the data but only on public information The difficulty for verifying this example is in the sensitivity analysis By some kv`hV where kv is the known variance of the gaussian distribution whose mean we are learning and hV is the prior variance over the mean|,Non-data,30
| We use this information in line 4 when we add noise with the Laplace mechanism By using this information we can give the following type to the previous program: tl :: R list || lŸ Φ lŹu Ñ thM :: R ||“u Ñ thV :: R` ||“u Ñ tkv :: R` ||“u Ñ t :: R` ||“u Ñ Ms-D,0td :: DrRs ||“u 63 Noise on Output using f-divergences We now turn to the approach of calibrating the sensitivity according to f -divergences We will consider once again the example for learning privately the distribution over the parameter of a Bernoulli distribution, but differently from the previous section we will add noise to the output of the inference algorithm using the exponential mechanism with a score function using an f -divergence|,Non-data,30
| So, we perturb the output distribution and not its parameters We will use Hellinger distance as a metric over the output space of our differentially private program, but any other f -divergence could also be used The quality score function for the exponential mechanism can be given a type of the shape: tl :: B list || lŸ Φ lŹu Ñ td :: Drτs ||“u Ñ tr :: R || ||rŸ ́rŹ|| ďρu where the bound ρ express its sensitivity Now we can use as a score function the following program score (db, prior) out = -(H (infer (learnBias db prior)) out) This quality score uses a function H computing the Hellinger distance between the result of the inference and a potential output to assign it a score|,Non-data,30
| The closer out is to the real distribution (using Hellinger distance), the higher the scoring is If we use the exponential mechanism with this score we achieve our goal of using the Hellinger to “calibrate the noise” Indeed we have a program: let main prior obs eps = expMech eps score (obs, prior) To which we can assign type: MHD,0tx :: r0, 1s ||“u Ñ t(cid:2) :: B list || (cid:2)Ÿ Φ (cid:2)Źu Ñ t :: R` ||“u Ñ Mρ-D,0td :: Drr0, 1ss ||“u Concretely, to achieve this we can proceed by considering first the code for learnBias: 1let rec learnBias db prior = match dbn with 2|,Non-data,30
||| [] Ñ prior 3|| d::dbs Ñ mlet rec = (learBias dbs prior) in observe 4 (fun r Ñ mlet z = ran bernoulli(r) in return (d=z)) rec To have a bound for the whole learnBias we need first to give a bound to the difference in Hellinger distance that two distinct observations can generate This is described by the following lemma|,Non-data,30
| a Lemma 61 Let d1, d2 : B with d1Φd2 Let a, b P R` Prpξq “ Betapa, bq|,Non-data,30
| Then ΔHDpPrpξ || d1q, Prpξ || d2qq ď 1 ́ π “ ρ  Let 4 Using this lemma we can then type theobserve statement with the bound ρ We still need to propagate this bound to the whole learnBias|,Non-data,30
| We can do this by using the adja- cency relation which imposes at most one difference in the observations, and the data processing inequality Theorem 31 guaranteeing that for equal observations the Hellinger dis- tance cannot increase Summing up, using the lemma above, the adjacency assumption and the data processing inequality we can give to learnBias the following type: tl :: B list || lŸ Φ lŹu Ñ MHD,0tx :: r0, 1s ||“u Ñ MHD,ρtx :: r0, 1s ||“u This ensures that starting from the same prior and observing l1 and l2 in the two different runs such that l1 Φ l2 we can achieve two beta distributions which are at distance at most ρ Using some additional refinement for infer and H we can guarantee that score has the intended type, and so we can guarantee that overall this program is pρ, 0q-differential privacy|,Non-data,30
| The reasoning above is not limited to the Hellinger distance For instance the following lemma: a Lemma 62 Let d1, d2 : B with d1Φd2|,Non-data,30
| Let a, b P R` Let Prpξq “ Betapa, bq Then ΔSDpPrpξ || d1q, Prpξ || d2qq ď 2p1 ́ π q “ ζ  4 gives a type in term of statistical distance: tl :: B list || lŸ Φ lŹu Ñ MSD,0tx :: r0, 1s ||“u Ñ MSD,ζtx :: r0, 1s ||“u The choice of which metric to use is ultimately left to the user|,Non-data,30
| This example easily extends also to the Dirichlet example Indeed, Lemma 61 can be generalized to arbitrary Dirichlet distributions: Lemma 63|,Non-data,30
| Let k P Ně2, d1, d2 : rks list with d1Φd2 Let a1, a2,   |,Non-data,30
| , ak P R` Then ΔHDpPrpξ || d1q, Prpξ || d2qq ďa  Let Prpξq “ Dirichletpa1, a2,   |,Non-data,30
| , akq 1 ́ π “ ρ 4 Using this lemma we can assign to the following program: 1let rec learnP db prior = match dbn with 2|,Non-data,30
||| [] Ñ prior 3|| d::dbs Ñ mlet rec = (learnP dbs prior) in observe 4 (fun r sÑ mlet z = ran multinomial(r,s) in 5 return (d=z)) rec the type: tl :: r3s list || lŸ Φ lŹu Ñ MHD,0tx :: r0, 1s2 ||“u Ñ MHD,ρtx :: r0, 1s2 ||“u Similarly to the previous example we can now add noise to the output of the inference process using the sensitivity with respect to the Hellinger distance and obtain a pρ, 0q- differential privacy guarantee|,Non-data,30
| 777 RELATED WORK Differential privacy and Bayesian inference Our sys- tem targets programs from the combination of differential privacy and Bayesian inference Both of these topics are ac- tive areas of research, and their intersection is an especially popular research direction today|,Non-data,30
| We briefly summarize the most well-known work, and refer interested readers to sur- veys for a more detailed development (Dwork and Roth [16] for differential privacy, Bishop [9] for Bayesian inference) Blum et al [10] and Dwork et al [17] proposed differential privacy, a worst-case notion of statistical privacy, in a pair of groundbreaking papers, initiating intense research interest in developing differentially private algorithms|,Non-data,30
| The original works propose the Laplace and Gaussian mechanisms that we use, while the seminal paper of McSherry and Talwar [30] introduces the exponential mechanism Recently, researchers have investigated how to guarantee differential privacy when performing Bayesian inference, a foundational technique in machine learning Roughly speaking, works in the literature have explored three different approaches to guaranteeing differential privacy when the samples are private data First, we may add noise directly to the samples, and then perform inference as usual [38]|,Non-data,30
| Second, we may perform inference on the private data, then add noise to the parameters themselves [40] This approach requires bounding the sensitivity of the output parameters when we change a single data sample, relying on specific properties of the model and the prior distribution The final approach involves no noise during inference, but outputs samples from the posterior rather than the entire posterior distribution [15, 40, 41] This last approach is highly specific to the model and prior, and our system does not handle it, yet|,Non-data,30
| Formal verification for differential privacy In parallel with the development of private algorithms, researchers in formal verification have proposed a wide variety of tech- niques for verifying differential privacy For a comprehensive discussion, interested readers can consult the recent survey by Barthe et al [8]|,Non-data,30
| Many of these techniques rely on the composition properties of privacy, though there are some ex- ceptions [7] For a brief survey, the first systems were based on runtime verification of privacy [29] The first systems for static verification of privacy used linear type systems [21, 33] There is also extensive work on relational program logics for differential privacy [2–4], and techniques for verifying privacy in standard Hoare logic using product programs [5]|,Non-data,30
| None of these techniques have been applied to verifying differential privacy of Bayesian inference Our system is most closely related to HOARe2, a relational refinement type system that was recently proposed by Barthe et al [6] This system has been used for verifying differential privacy of algorithms, and more general relational properties like incentive compatibil- ity from the field of mechanism design|,Non-data,30
| However, it cannot model probabilistic inference Probabilistic programming Research in probabilistic pro- gramming has emerged early in the 60s and 70s, and is nowa- days a very active research area Relevant to our work is in particular the research in probabilistic programming for machine learning and statistics which has been very active in recent years|,Non-data,30
| Many probabilistic programming languages have been designed for these applications, including Win- BUGS [28], IBAL [32], Church [22], Infernet [31], Tabu- lar [24], Anglican [36], Dr Bayes [37] Our goal is not to provide a new language but instead is to propose a frame- work where one can reason about differential privacy for such languages|,Non-data,30
| For instance, we compiled programs written in Tabular [24] intoPrivInfer so that differential privacy could be verified Another related work is the one by Adams and Jacobs [1] proposing a type theory for Bayesian inference While technically their work is very different from ours it shares the same goal of providing reasoning principles for Bayesian inference Our work considers a probabilistic PCF for discrete distributions|,Non-data,30
| It would be interesting to extend our techniques to higher-order languages with continuous distributions and conditioning, by building on the rigorous foundations developed in recent work [11, 35] 8 CONCLUSION We have presented PrivInfer, a type-based framework for differentially private Bayesian inference Our framework allows to write data analysis as functional programs for Bayesian inference and to add noise to them in different ways using different metrics|,Non-data,30
| Besides, our framework allows to reason about general f -divergences for Bayesian inference Future directions include exploring the use of this approach to guarantee robustness for Bayesian inference and other ma- chine learning techniques [14], to ensure differential privacy using conditions over the prior and the likelihood similar to the ones studied by Zhang et al [40], Zheng [41], and investigating further uses of f -divergences for improving the utility of differentially private Bayesian learning On the programming language side it would also be interesting to extend our framework to continuous distributions following the approach by Sato [34]|,Non-data,30
|ABSTRACT In-app embedded browsers are commonly used by app developers to display web content without having to redirect the user to heavy- weight web browsers Just like the conventional web browsers, em- bedded browsers can allow the execution of web code In addition, they provide mechanisms (viz, JavaScript bridges) to give web code access to internal app code that might implement critical function- alities and expose device resources|,Non-data,32
| This is intrinsically dangerous since there is currently no means for app developers to perform origin-based access control on the JavaScript bridges, and any web code running in an embedded browser is free to use all the exposed app and device resources Previous work that addresses this prob- lem provided access control solutions that work only for apps that are built using hybrid frameworks Additionally, these solutions fo- cused on protecting only the parts of JavaScript bridges that expose permissions-protected resources In this work, our goal is to provide a generic solution that works for all apps that utilize embedded web browsers and protects all channels that give access to internal app and device resources|,Non-data,32
| Towards realizing this goal, we built Draco, a uniform and fine-grained access control framework for web code running on Android embedded browsers (viz, WebView) Draco provides a declarative policy language that allows developers to define policies to specify the desired access characteristics of web origins in a fine-grained fashion, and a runtime system that dynami- cally enforces the policies In contrast with previous work, we do not assume any modifications to the Android operating system, and implement Draco in the Chromium Android System WebView app to enable seamless deployment|,Non-data,32
| Our evaluation of the the Draco runtime system shows that Draco incurs negligible overhead, which is in the order of microseconds Keywords Android, WebView, access control, origin, JavaScript bridges, ex- ploitation, JavaScript, HTML5 1 INTRODUCTION Mobile application (or "app" for short) developers heavily rely on embedded browsers for displaying content in their apps and Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than ACM must be honored|,Non-data,32
| Abstracting with credit is permitted To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from permissions@acmorg|,Non-data,32
