 6303. DATASETS Our Internet-wide study of key sharing in the HTTPS ecosystem is driven by four datasets: SSL certificates We use SSL certificates from full IPv4 scans as the basis of our measurements,Data,0
| Our SSL scans [30] also contain information on the IP address(es) that advertised each certificate. To obtain in- formation about the entity that controls this IP address, we use full IPv4 reverse DNS scans [29] that are also conducted by Rapid7|,Data,0
| Each AS is assigned an AS Number (ASN): for example, MIT is AS 3 and the Chicago Public Schools are AS 1416 [26]. CAIDA collects and publishes mappings between IP addresses and ASNs via their Route- Views datasets [7]|,Data,0
| For example, AT&T owns 160 unique ASNs. To aggregate these, we use CAIDA’s AS- to-Organization dataset [8] to group together ASes owned by the same organization|,Data,0
| For that, we rely on WHOIS [12], a protocol for querying domain registrars to obtain data on the domain owner. In practice, WHOIS data often contains fields such as the con- tact information for the owner of the domain, the contact for technical issues, where to send abuse complaints, and so on|,Data,0
| Here, we expand upon these prior findings by evaluating whether there is a correlation between centralized management and the quality of the keys chosen. Figure 13 compares several different features of self- managed and outsourced certificates across our entire cor- pus of leaf certificates (3,275,635 self-managed and 1,781,962 outsourced): (a) Key lengths in self-managed certificates are nearly identical to those managed by third-party hosting providers|,Data,0
|1 Combining Packet Capture (PCAP) Files The data set used in this study is a combination of the packet capture files obtained from two main sources. First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour|,Data,1
| First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour. The normal and non-malicious data is obtained from PREDICT internet data set repository [18] under the category of “DARPA Scalable Network Monitoring (SNM) Program Traffic”|,Data,1
| The data collection was performed during April 2016 using ZGrab, an application-layer scanner that operates with ZMap [15]. In the first phase, we performed an Internet-wide scan of all IPv4 addresses on port 500 to determine which hosts were configured 16This defect was corrected quite recently, years after the version of OpenSSL ScreenOS uses was written|,Data,6
|, download from an external source). Running on 71,000 articles collected from 45 leading technical blogs, this new approach demonstrates a remarkable performance: it gener- ated 900K OpenIOC items with a precision of 95% and a coverage over 90%, which is way beyond what the state-of-the-art NLP tech- nique and industry IOC tool can achieve, at a speed of thousands of articles per hour|,Data,7
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
 5. ANALYSIS AND FINDINGS In the following we use the extensive documentation of the 61 minimal exploits to provide insight into how attackers use specific vulnerabilities and features of the Java platform to implement their attacks,Data,11
| We run our event analysis on the top 100 free applications in the Android application store to determine how often this happens. In total, our analysis finds 1060 errors across 88 of the top 100 applications (10|,Data,12
| To our knowledge, AUTOREB is the first work that explores the user review information and utilizes the review semantics to predict the risky behaviors at both review-level and app-level. We crawled a real-world dataset of 2, 614, 186 users, 12, 783 apps and 13, 129, 783 reviews from Google play, and use it to comprehensively evaluate AUTOREB|,Data,14
| 4.1 Data collection For each team, we collected a variety of observed and self- reported data|,Data,16
| To demonstrate this, we scraped greatfire.org for websites in the top 1000 Alexa websites that are blocked by the GFW|,Data,18
| (cid:15) Identifying New Vulnerabilities. Our tool successfully an- alyzed 1,591 service interfaces of all the 80 system services in Android 5|,Data,19
| To understand the scope and magnitude of this new XARA threat, we developed an ana- lyzer for automatically inspecting Apple apps’ binaries to deter- mine their susceptibility to the XARA threat, that is, whether they perform security checks when using vulnerable resource-sharing mechanisms and IPC channels, a necessary step that has never been made clear by Apple. In our study, we ran the analyzer on 1,612 most popular MAC apps and 200 iOS apps, and found that more than 88|,Data,24
| To assist software developers (or secu- rity analysts) in tracking down a memory corruption vulnerability, CREDAL also performs analysis and highlights the code fragments corresponding to data corruption. To demonstrate the utility of CREDAL, we use it to analyze 80 crashes corresponding to 73 memory corruption vulnerabilities archived in Offensive Security Exploit Database|,Data,25
| These techniques may be applicable in other scenarios. We implemented and evaluated the attacks against the popular Gmail and Bing services, in several environments and ethical experiments, taking careful, IRB-approved mea- sures to avoid exposure of personal information|,Data,26
|, CSPAutoGen can handle all the inline and dynamic scripts. We have implemented a prototype of CSPAutoGen, and our eval- uation shows that CSPAutoGen can correctly render all the Alexa Top 50 websites|,Data,27
| 5. EXPERIMENTAL RESULTS This section reports on our evaluation of the moments ac- countant, and results on two popular image datasets: MNIST and CIFAR-10|,Data,28
| 6.1 Mobility Trace Dataset We use the CRAWDAD dataset roma/taxi [2, 3] for our simu- lations|,Data,31
 6.1 Evaluation We evaluated the performance of Σoφoς using 4 data sets of increasing size and also the English Wikipedia,Data,33
|1 Datasets, Metrics, Competitors & Settings Datasets. We test EpicRec on two real-world datasets: MovieLens1: a movie rating dataset collected by the Grou- pLens Research Project at the University of Minnesota through the website movielens|,Data,36
| 1 http://grouplens.org/datasets/movielens 188Yelp2: a business rating data provided by RecSys Chal- lenge 2013, in which Yelp reviews, businesses and users are collected at Phoenix, AZ metropolitan area|,Data,36
| The number of movie categories is 18. We use the MovieLens- 1M, with 1000,209 ratings from 6,040 users on 3,883 movies|,Data,36
| Our goal is to show that an ad- versary can insert an unbounded number of Sybil identities in the SybilLimit protocol, breaking its security guarantees. For our evaluation, we consider a real-world Facebook inter- action graph from the New Orleans regional network [28]|,Data,38
| We utilize these papers to extract Android malware behaviors and to construct the semantic network. From the electronic proceedings distributed to conference participants, we collect the papers from the IEEE Sympo- sium on Security and Privacy (S&P’08–S&P’15)4, the Com- puter Security Foundations Symposium (CSF’00–CSF’14), and USENIX Security (Sec’11)|,Data,39
 We conduct experiments on two publicly available set-valued datasets. • AOL search log dataset [1],Data,45
 90% of the users have fewer than 84 keywords in their logs. • Kosarak dataset [2],Data,45
 We select one month of data for our study. The data logs we used are col- lected from more than 30 machines with various server mod- els and operating systems,Data,46
| This paper rigorously investigates how users’ security beliefs, knowledge, and demographics corre- late with their sources of security advice, and how all these factors influence security behaviors. Using a carefully pre- tested, U|,Data,48
 We have ported Valgrind to iOS and implemented a prototype of iRiS on top of it. We evaluated iRiS with 2019 applications from the official App Store,Data,54
| from manufacturing equipment, as shown in Figure 1. We capture the relevant sensor data by deliberately or accidentally placing an attack-enabled phone close to, on top of, or inside a piece of manu- facturing equipment while the machinery is fabricating the target object|,Data,55
| Our new metric helps us compare in a fair way previously proposed attack-detection mechanisms. (ii) We compare previous attack-detection proposals across three di↵erent experimental settings: a) a testbed operating real-world systems, b) network data we collected from an operational large-scale Supervisory Control and Data Acqui- sition (SCADA) system that manages more than 100 Pro- grammable Logic Controllers (PLCs), and c) simulations|,Data,57
| Evaluation. We ran Oyente on 19, 366 smart contracts from the first 1, 460, 000 blocks in Ethereum network and found that 8, 833 contracts potentially have the documented bugs|,Data,58
| First, we consolidate the eight origin-exposing vectors into one auto- mated origin-exposing system called Cloudpiercer. Then, we assemble a list of clients from five CBSP companies by studying their DNS configurations and obtaining their adop- tion rate across the Alexa top 1 million websites|,Data,59
| The vast majority of them were exposed through their A record, indicating a brief dis- abling of the protection system. SSL certificate exposure In order to find IP addresses hosting SSL certificates associ- ated with the domains in the evaluation set, we made use of the publicly available data of Rapid7’s Project Sonar [42]|,Data,59
| 4. LARGE-SCALE ANALYSIS To assess the magnitude of the origin-exposure problem, we conduct a large-scale analysis in which we attempt to uncover the origin of CBSP-protected domains|,Data,59
|1 Dataset Description The dataset was first presented and used by Keller et al. in [23], and is publicly available in the gene expression om- nibus (GEO) database under reference GSE61741|,Data,61
| Although the cost of stor- age and processing have diminished, the cost of maintaining reliable infrastructure for transaction logs is still noticeable. Figure 1: A plot of transaction fee versus frequency for 1 million transactions in May 2015|,Data,65
| To estimate the cost of producing the preprocessing data (multiplication triples, random bits etc.), we used figures from the recent MASCOT protocol [31], which uses OT ex- tensions to obtain what are currently the best reported triple generation times with active security|,Data,67
| In this section, we validate whether the smartphone’s acoustic data can be utilized to deduce the movements. To conduct the validation, we implement an application on Nexus 5 (Android OS v6|,Data,68
| As seen in Table 4, we found that about half of the servers in Alexa’s top 10 support a large number of requests without rekeying. For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client|,Data,72
| For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client. We identified 11483 different HTTPS servers11, and found that 226 of them (1|,Data,72
| In this paper, we study the possible techniques to detect and measure this fraud and evaluate the real impact of OTT bypass on a small European country. For this, we performed more than 15,000 test calls during 8 months and conducted a user study with more than 8,000 users|,Data,78
|, the server cannot learn their relative order) after some number of queries are performed over real-world data. Specifically, we ran an experiment where we inserted over 2 million public employee salary figures from [1] and then performed 1000 random range queries|,Data,79
| In this study, we are interested in finding answers to security- and privacy-related questions about libraries, such as “How prevalent are third- party libraries in the top apps and how up-to-date are the library versions?”, “Do app developers update the libs included in their apps and how quickly do they update?”, or “How prevalent are vulnerabilities identified in prior research [28, 9] in libraries and how many apps are affected?” To answer these questions, we first built a comprehensive repository of third-party libraries and applications (see Section 5). Our library set contains 164 libraries of different categories (Ad- vertising, Cloud,|,Data,84
|) and a total of 2,065 versions. We then collected and tracked the version histories for the top 50 apps of each category on Play between Sep 2015 and July 2016, accumulating to 96,995 packages from 3,590 apps|,Data,84
|6.1, we found in our sample set 360 affected packages from 23 distinct apps, when only considering exact library matches|,Data,84
|15 for Android, which contained an account hijacking vulnerability, on 06/11/2014. In the histories of our sample set apps, we discovered, in total, 394 affected packages from 51 distinct apps, when only considering packages with exact matches of the vulner- able lib version|,Data,84
| We used LibScout to detect the affected application packages in our data set. In total 2,667 app versions of 296 distinct apps with a cumu- lative install-base of 3|,Data,84
| We observed that there is a significant variance in ACFG size. To reduce the sampling bias, we first collect a dataset which covers ACFGs of different functions from various architec- tures|,Data,89
| This dataset was used for base- line comparison, and all functions in this dataset has known ground truth for metric validation. We prepared this dataset using BusyBox (v1|,Data,89
| Dataset II – Public dataset. Recent work such as Pewny et al [45] and Eschweiler et al [23] used the same public dataset based upon two publicly-available firmware images for baseline comparison [7, 8]|,Data,89
| Dataset III – Firmware image dataset. This dataset of 33,045 firmware images was collected from the wild|,Data,89
| As a result, we created a freely available vulnerability database for this effort and for the broader research community. To build this database, we mined official software websites to collect lists of vulnerabilities with the corresponding CVE num- bers|,Data,89
| We selected OpenSSL for demonstration, since it is widely used in IoT devices. The resulting vulnerability database includes 154 vulnerable functions|,Data,89
| Roughly speaking, our measurement methods can be divided into two kinds: those that could be fully automated and scaled eas- ily, and those that required some manual interaction. For the latter, we used a set of 302938 major email providers and email genera- tors, while for the former, we used a much larger set of a million popular providers occurring in the Adobe leak and the Alexa top million Web sites (as potential email generators)|,Data,90
1.2 Provider List We created the set of popular email providers based on the top 1 million email address domains occurring in the leaked Adobe user data set of September 2013,Data,90
| Using a combination of mea- surement techniques, we determine whether major providers sup- ports TLS at each point in their email message path, and whether they support SPF and DKIM on incoming and outgoing mail. We found that while more than half of the top 20,000 receiving MTAs supported TLS, and support for TLS is increasing, servers do not check certificates, opening the Internet email system up to man- in-the-middle eavesdropping attacks|,Data,90
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
| 3.1 Datasets We use two major types of datasets: (1) packet-level traffic traces collected at various locations in a campus network, and (2) packet-level traces for Tor Pluggable Transport traffic collected in controlled environments|,Data,91
 Evaluation: local mixing time in social graphs. We use 10 various large-scale real-world social network topolo- gies that mainly come from the Stanford Large Network Dataset Collection [23] and other sources [45] to evaluate the local mixing time for nodes in social graphs,Data,92
| Feature Functions and Weights. To learn all feature functions and weights, we downloaded 1784 non-obfuscated Android applications from F-Droid [3], a popular repository for open-source Android applications|,Data,93
2.2 Experiments with Malware Samples We randomly selected one sample from each of the 49 mal- ware families reported in [40],Data,93
1_r1). Apps in our dataset used for the case study are downloaded from the Google official market (Google Play) in May 2016,Data,95
| • Using SInspector, we perform the first study of Unix domain sockets on Android, including the categoriza- tion of usage, existing security measures being en- forced, and common flaws and security implications. We analyze 14,644 apps and 60 system daemons, find- ing that 45 apps, as well as 9 system daemons, have vulnerabilities, some of which are very serious|,Data,98
| We presented SInspector, a tool for discovering potential security vulnerabilities through the process of identifying socket addresses, detecting authen- tication checks, and performing data flow analysis on na- 90tive code. We analyzed 14,644 Android apps and 60 system daemons, finding that some apps, as well as certain system daemons, suffer from serious vulnerabilities, including root privilege escalation, arbitrary file access, and factory reset- ting|,Data,98
| Our results show that many of our attacks succeed with a 100% chance such that the Sound-Proof cor- relation algorithm will accept the attacked audio samples as valid. Third, we collect general population statistics via an online sur- vey to determine the phone usage habits relevant to our attacks|,Data,100
 We find that the larger width of integer types and the increased amount of addressable memory introduce previously non-existent vulnerabilities that often lie dormant in program code. We empirically evaluate the prevalence of these flaws on the source code of Debian stable (“Jessie”) and 200 popular open-source projects hosted on GitHub,Data,104
| We have applied UniSan to the latest Linux kernel and Android kernel and found that UniSan can successfully prevent 43 known uninitialized data leaks, as well as many new ones. In particular, 19 of the new data leak vulnerabilities in the latest kernels have been confirmed by the Linux community and Google|,Data,107
| This allows us to prevent replay attacks, which are possibly the most applicable attack vectors against biometric authentication. Using a gaze tracking device, we build a prototype of our system and perform a series of systematic user experiments with 30 participants from the general public|,Data,108
| If two commits were blamed for the same amount of lines, blame both. Our heuristic maps the 718 CVEs of our dataset to 640 VCCs|,Data,109
| However, improving our blame heuristics further is an interesting avenue for future research. Apart from the 640 VCCs, we have a large set of 169,502 unclassified commits|,Data,109
|9 The SVM detected a high amount of excep- tions, a high number of changed code, inline ASM code, and variables containing user input such as __input and user. 6As previously mentioned we use the years 2011–2014 as the test dataset, since we have ground truth data on which to base the discussion|,Data,109
| When given a source file, Flawfinder returns lines with suspected vul- nerabilities. It offers a short explanation of the finding as well as a link to the Common Weakness Enumeration (CVE) database|,Data,109
| The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database|,Data,109
| Our results show that our approach significantly outperforms the vulner- ability finder Flawfinder. We created a large test database containing 66 C and C++ project with 170,860 commits on which to evaluate and compare our approach|,Data,109
 VoiceLive takes advantages of the user’s unique vocal system and high quality stereo recording of smartphones. • We conduct extensive experiments with 12 participants and three different types of phones under various ex- perimental settings,Data,111
| To test if WebCapsule can successfully record and subsequently replay real-world phishing attacks, we proceeded as follows, us- ing Chromium on our desktop machine. We selected a large and diverse set of recently reported phishing web pages from Phish- Tank8|,Data,112
| 2.4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime|,Data,113
|4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime. The dataset Cal represents the latitude and longitude of about 21,000 intersections in the California road network1 (also used by Mavroforakis et al|,Data,113
294258. The dataset SpitzLoc consists of latitude and longitude coordinates tracking the movement of German Green party politician Malte Spitz over six months,Data,113
| In this section, we aim to explore whether the differences of keystroke wave- forms are large enough to be used for recognizing different keys inputs in the real-world setting. We collected training and testing data from 10 volunteers|,Data,114
 B. Real Attacks MAD uniformly detects attacks more quickly than the PAD; we use the former method to detect the presence of an attack in real Internet traces3,Data,119
 III. DATA SET  changes  The data used was the PREDICT ID USC-Lander!  (- 60  The total  were DNS attack packets,Data,120
|395326000  files IPs. There are total 59,928,920 packet counts out of which there was a total of  DoS_DNS_amplification-20130617 (2013-06-17) (2013-06-17) with anonymized million) 358019 DNS packets|,Data,120
| The maximum number of unique hosts per day we measured was 106,000. To understand these differences, we compared the observations from our network monitor to data collected from DShield (www|,Data,121
| 3.1 From our own transactions We engaged in 344 transactions with a wide variety of services, listed in Table 1, including mining pools, wallet services, bank ex- changes, non-bank exchanges, vendors, gambling sites, and mis- cellaneous services|,Data,122
| Wallets. We kept money with most of the major wallet services (10 in total), and made multiple deposit and withdrawal transac- Bank exchanges|,Data,122
|, in which the exchange rate is not fixed) also function as banks. As such, we tagged these services just as we did the wallets: by depositing into and withdrawing from our accounts (but rarely par- ticipating in any actual currency exchange)|,Data,122
|info/tags, including both addresses provided in users’ signatures for Bitcoin forums, as well as self-submitted tags. We collected all of these tags — over 5,000 in total — keeping in mind that the ones that were not self-submitted (and even the ones that were) could be regarded as less reliable than the ones we collected ourselves|,Data,122
| 3.1 Data analysis overview We use three data sets, summarized in Table 1|,Data,123
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
| We also describe our application of the technique to the IPv6 interface-level graph captured by CAIDA’s Archipelago (Ark) infrastructure [14] for March 2013. The graph consists of all the 52,986 IPv6 interfaces numbered within the 2000::/3 unicast prefix captured from all 27 Ark vantage points (VPs) with IPv6 connectivity|,Data,125
| cause the counters of distinct routers to diverge, and (4) confirm aliases with pairwise probing. Given the absence of velocity in ID counters and the large probes required for the technique to work, we probe at a low rate of 20pps from a single VP, producing 26Kbps of traffic|,Data,125
| 3. METHODOLOGY In this section, we describe the design of our experiment and our data collection methodology, as well as the mitigating steps and proactive measurements we conducted to ensure a minimal im- pact of our covering routes|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| Our IPv6 network telescope results suggest sev- eral important differences (and some similarities) compared to that body of work. To produce a more recent and valid comparison, we analyzed a single week of IPv4 background radiation captured during the course of our ongoing IPv6 packet capture|,Data,126
| 4. DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1|,Data,127
| DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1. Our primary dataset consists of changes made to the |,Data,127
| domains, (2) the removal of existing domains, and (3) changes to existing domains in terms of revisions to their associated name- servers. Our data includes captures of the DNZA files as recorded every five minutes, time periods we refer to as epochs|,Data,127
| Since we lack comprehensive ground truth regarding the ultimate use of domains, to this end we use two proxies: subsequent appearance of a newly registered do- main in: (1) an email spam campaign, or (2) a domain blacklist. For the first of these, we operated a spam trap, i|,Data,127
|com), by restricting our focus to domains recently registered (March–July 2012) we can filter down the do- mains appearing in the spam trap to those very likely used for spam- ming. For the second, we subscribed to three major DNS blacklists, URIBL, SURBL, and Spamhaus DBL|,Data,127
| In this paper, we examine the effectiveness of these inter- ventions in the context of an understudied market niche, counterfeit luxury goods. Using eight months of empirical crawled data, we identify 52 distinct SEO campaigns, document how well they are able to place search results for sixteen luxury brands, how this ca- pability impacts the dynamics of their order volumes and how well existing interventions undermine this business when employed|,Data,128
| For a small number of stores, we were also able to collect user traffic data that directly measures an SEO campaign’s effectiveness in attracting customers to their stores. Specifically, we were able to periodically collect AWStats data for 647 storefronts in 12 cam- paigns|,Data,128
| One issue that undermines coverage is that Google only labels the root of a Web site as “hacked”, and does not label search results that link to sub-pages within the same root domain. In the PSR data set, we found 68,193 “hacked” search results|,Data,128
| We begin by exam- ining the properties of individual darknets and in particular the behavior of source IP addresses. We provide these char- acterizations by looking at data from 14 darknet monitors ranging in size from a /25 monitor to a /17 monitor over a period of 10 days between August 18, 2004 and August 28, 2004|,Data,129
| Figure 10: The number of darknets (of 31) reporting a port in the top 10 ports over a day, week, and month time frame. The analysis is performed for the top 10 destination ports over a day, top 10 destination ports over a week, and top 10 destination ports over a month|,Data,129
| 3.6 Datasets This paper uses DNS datasets from three authorities: one national-level top-level domain, operators of two root servers as shown in Table 1|,Data,130
 JP-DNS operates the .jp country code domain for Japan; we have data from all seven of their anycast sites,Data,130
|) part of the 2014 DITL collection [16] (for B-Root, shortly after 2014 DITL). We also use data for M-Root’s 2015 DITL collection (§ 4|,Data,130
 These root datasets are available to re- searchers through DNS-OARC. For longitudinal analysis we draw on 9 months of data taken at the M-Root server,Data,130
| However, we treat the union of these classes together. We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness|,Data,131
| We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness. The dataset consists of all echo requests that were sent as part of the surveys in this period, as well as all echo responses that were received|,Data,131
|, “host unreachable”); we ignore all probes as- sociated with such responses since the latency of ICMP error responses is not relevant. In later sections, we will complement this dataset with results from Zmap [5] and additional experiments includ- ing more frequent probing with Scamper [13] and Scrip- troute [22]|,Data,131
| 3.2 Milking 3 Methodology To collect the information needed to cluster servers into oper- ations, we have built an infrastructure to track individual exploit servers over time, periodically collecting and classi- fying the malware they distribute|,Data,132
 2. We receive feeds of drive-by download URLs (Sect,Data,132
 2. CHARACTERISTICS OF CHECK-INS We use three different datasets that capture human mobility,Data,133
 First we consider two online location-based social networks. We col- lected all the public check-in data between Feb,Data,133
| There are 196,591 nodes, 950,327 edges in Gowalla and 58,228 nodes, 214,078 edges in Brightkite. To ensure that our observations on human movement are not specific to data based on check-ins from location-based social net- works, we also include a dataset of cell phone location trace data|,Data,133
| Backscatter DDoS is a commonly seen behaviour in darknets where the attacker uses simultaneous bots to generate the actual attack packets to reach the targeted (original) victim. In our study, five publicly available network traffic datasets from CAIDA’s archives are employed|,Data,134
| Datasets Employed In this research, five publicly available real-life network traffic traces (datasets) from CAIDA’s archives are employed. Three of them, which were captured by a passive darknet in 2007, 2008 and 2012 [27][26][28], namely UCSD Network Telescope [21], include mostly one-way malicious traffic while the remaining ones collected in 2008 [29] and 2014 [30] via CAIDA’s Internet backbone links include only normal traffic|,Data,134
| 3 Approach This section presents our approach for the evalua- tion of reputation based blacklists. We evaluated the blacklists by deploying them in a large academic net- work of over 7,000 hosts|,Data,135
| This was a preliminary step to preventing inexperienced and non-serious workers from participating in our survey. Our survey is based on the participants’ actual check-ins on Foursquare posted over the last 24 months (that we collected through a specific application we developed), and it requires a significant amount of time to complete (30-45 minutes)|,Data,136
| The third phase of worm activ- ity is the persistence phase which for the Blaster worm has continued through 2004. In this one-week period of measurement, the IMS system observed over 286,000 unique IP addresses displaying the characteristics of Blaster activity|,Data,137
| published a study in 2011 that focused on the dynamics of leaf cer- tificates and the distribution of certificates among IP addresses, and attempted to roughly classify the overall quality of served certifi- cates. The study was based on regular scans of the Alexa Top 1 Mil- lion Domains [1] and through passive monitoring of TLS traffic on the Munich Scientific Research Network [17]|,Data,138
| Our study is founded on what is, to the best of our knowledge, the most comprehensive dataset of the HTTPS ecosystem to date. Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443|,Data,138
| Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443. Over the course of 14 months, we completed upwards of 400 billion SYN probes and 2|,Data,138
| Content Provider e Service Provider v i t c e p s r e P Content Consumer Addressing Prerequisite IP Functions Routing Naming A1: Address Allocation; A2: Address Advertisement N1: Nameservers; R1: Server Readiness N2: Resolvers N3: Queries A2: Address Advertisement; T1: Topology End-to-End Reachability R1: Server Readiness Operational Characteristics Usage Profile Performance U3: Transition Technologies U1: Traffic Volume; U3: Transition Technologies P1: Network RTT R2: Client Readiness U2: Application Mix; N3: Queries Table 2: Dataset summary showing the time period, scale, and public or new status of the datasets we analyzed. Dataset RIR Address Allocations Routing: Route Views Routing: RIPE Google IPv6 Client Adoption Verisign TLD Zone Files CAIDA Ark Performance Data Arbor Networks ISP Traffic Data Verisign TLD Packets: IPv4 Verisign TLD Packets: IPv6 Alexa Top Host Probing Time Period Metrics Jan 2004 – Jan 2014 A1 Jan 2004 – Jan 2014 A2, T1 Jan 2004 – Jan 2014 A2, T1 Sep 2008 – Dec 2013 R2, U3 Apr 2007 – Jan 2014 N1 P1 Dec 2008 – Dec 2013 U1, U2, U3 Mar 2010 – Dec 2013 Jun 2011 – Dec 2013 N2, N3 N2, N3 Jun 2011 – Dec 2013 Apr 2011 – Dec 2013 R1 Recent Scale ≈18K allocation snapshots (5 daily) 45,271 BGP table snapshots millions of daily global samples daily snapshots of ≈2|,Data,139
com & .net) ≈10 million IPs probed daily ≈33-50% of global Internet traffic; 2013 daily median: 50 terabits/sec (avg,Data,139
| To put the IPv6 allocation data in context, Figure 1 also shows IPv4 prefix allocations over the same period. The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled|,Data,139
| There were less than 30 IPv6 prefixes al- located per month prior to 2007, generally increasing thereafter. In the past several years, we typically find more than 300 prefixes allocated per month, with a high point of 470 prefix allocations in February 2011|,Data,139
| The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled. 1 Overall, we find nearly 69K IPv4 prefix allocations at the beginning of our dataset and just over 136K at the end|,Data,139
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| Table 1 shows the datasets we use in our paper. We use two ICMP surveys taken by USC [12]: IT17ws and IT16ws; IT17ws is the main dataset used in this paper, while we use IT16ws for validation in Section 6|,Data,142
2. We collected VUSC s at our enterprise in order to compare our inferences with network operators as discussed in Section 6,Data,142
| # of Data-Oriented Attacks gives the number of attacks generated by FLOWSTITCH, includ- ing privilege escalation attacks and information leakage attacks. FLOWSTITCH generates 19 data-oriented attacks from 8 vulnerable programs|,Data,144
| Third, this method is not specific to C or C++, and can be applied to any programming language. We collected C++ source of thousands of contestants from the annual international competition “Google Code Jam”|,Data,145
| Finally, we analyze various attributes of programmers, types of programming tasks, and types of features that appear to influence the success of attribution. We identified the most important 928 fea- tures out of 120,000; 44% of them are syntactic, 1% are layout-based and the rest of the features are lexical|,Data,145
|3.1ScalingWecollectedalargerdatasetof1,600programmersfromvariousyears|,Data,145
| ) s y a D n i (    e m T i  7  6  5  4  3  2  1  10  20  30  40  50  60  70  80  90 Time Before Accounts Suspension Number of IP Addresses 2 Motivation: Analysis of Malicious Activ- ity on a Webmail Service We want to understand the way in which cybercrimi- nals abuse accounts on online services, to identify weak points that we could leverage for detection. To this end, we observed the email-sending activity on a large web- mail service|,Data,147
| Following accepted frameworks for qualitative research [18, 30, 35], we focus closely on a small number of participants. We interviewed 15 journalists employed in a range of well-respected journalistic institutions in the United States and France, analyzing these interviews using a grounded theory approach [18, 30]|,Data,146
| 3.1 Datasets We examine 13,345 passwords from four sets created under composition policies ranging from the typical to the currently less common to understand the suc- cess of password-guessing approaches against passwords of different characteristics|,Data,149
| Had we used any major password leak, their analysts would have already been familiar with most or all of the passwords contained in the leak, biasing results. The passwords in these sets were collected using Ama- zon’s Mechanical Turk crowdsourcing service|,Data,149
| The decision for or against pinning is always a trade- off between increasing security and keeping mainte- nance efforts at an acceptable level. In this paper, we present an extensive study on the applicability of pinning for non-browser software by analyzing 639,283 Android apps|,Data,152
| Therefore, we instrument telemetry data from a popular anti-virus software provider. We evaluate the update behaviour of 871,911 unique users from January 2014 to December 2014 and find that only 50% of the users update to a new app version within the first week after release|,Data,152
| Developer View Although pinning is only ap- plicable in relatively few cases, the nominal-actual comparison leaves room for improvement. We there- fore collected feedback from 45 developers of apps for which we would recommend pinning|,Data,152
| Section 4). Altogether we found 20,020,535 calls to network related API calls (cf|,Data,152
| Instability of the routes to the sensor address space can also result in reachability problems, especially given that route flap damping can be triggered during convergence to suppress unstable routes [9]. Using the BGP updates data from RouteViews BGP monitor, we studied the availability of the routes to the sensor blocks in our de- ployment from a large set of ASes|,Data,154
| This section probes these differences using three successively more specific views of traffic to a network of distributed blackhole sensors. The data was recorded over a one month period with SYN responders on TCP port 135, 445, 4444, and 9996 across all sen- sors|,Data,154
|  V. EXPERIMENT RESULTS  In this section, we mainly focus on how our router-to-AS Mapping method and other baseline methods behave on global router-level topology, as discussed above, we use PeeringDB data as ground truth, and apply clustering method on global topology based on CAIDA ITDK project|,Data,155
| It describes the properties that a dataset should have in order to be used for comparison purposes. The dataset used in the paper includes an IRC-based Botnet attack1, but the bot used for the attack was developed by the authors and therefore it may not represent a real botnet behavior|,Data,156
| This dataset may be downloaded with authorization. The Protected Repository for the Defense of Infrastructure Against Cyber Threats (PRE- DICT) indexed three Botnet datasets2 until May 16th, 2013|,Data,156
 None of them are labeled. A custom botnet dataset was created to verify five P2P botnet detection algorithms in Saad et al,Data,156
| Unfortunately, there is only one infected machine for each type of botnet, therefore no synchronization analysis can be done. The Traffic Laboratory at Ericsson Research created a normal dataset that was used in Saad et al|,Data,156
 This is the only normal dataset that is labeled inside the pcap file. A considerable amount of malware traffic in pcap format was published in the Contagio blog9,Data,156
| But since each scenario includes only one infected computer, it should be possible to label them. Another dataset with malware logs and benign logs was collected in NexGinRC (2013)|,Data,156
 Access to this dataset may be granted upon request10. The last dataset analyzed is currently created by the MAWI project described in Cho et al,Data,156
| Methodology and datasets We deployed Paris Traceroute with its Multipath Detection Algorithm (MDA) [29] enabled in 90 PlanetLab nodes. We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]|,Data,158
| We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]. Our dataset contains more than 900 thousand IP-level (multi)routes and 324,313 IP addresses|,Data,158
1 3.1 Address Allocation and BGP Data We analyzed BGP announcements captured by all collectors (24 collectors peering with 184 peers) of the Routeviews [3] and RIPE RIS [52] projects,Data,159
| For each /24 block, we computed the maximum number of peers that saw it reachable at any time within the full observation period of 92 days. To determine which address blocks are available for assignment, we used a dataset compiled by Geoff Hus- ton [23], which merges the extended delegation files from the 5 RIRs [4, 6, 7, 41, 51] with IANA’s published registries [31–36]|,Data,159
| SWITCH. We collected unsampled NetFlow records from all the border routers of SWITCH, a national aca- demic backbone network serving 46 single-homed uni- versities and research institutes in Switzerland [55]|,Data,159
| R-ISP. We collected per-flow logs from a vantage point monitoring traffic of about 25,000 residential ADSL customers of a major European ISP [21]|,Data,159
 UCSD-NT. We collected full packet traces from the /8 network telescope operated at the University of Cal- ifornia San Diego [1],Data,159
| IXP. Our fourth VP is a large European IXP inter- connecting more than 490 networks, exchanging more than 400 PB monthly [5]|,Data,159
|3 Active Measurements ISI. We used the ISI Internet Census dataset it55w- 20130723 [37], obtained by probing the routed IPv4 address space with ICMP echo requests and retaining only those probes that received an ICMP echo reply from an address that matched the one probed (as rec- ommended [38])|,Data,159
| HTTP. We extracted IP addresses from logs of Project Sonar’s HTTP (TCP port 80) scan of the entire IPv4 address space on October 29, 2013 [24]|,Data,159
| Definitions of graph parameters measuring metric tree-likeness of a graph, as well as notions and notations local to a section, are given in appropriate sections. 3 Datasets Our datasets come from different domains like Internet measurements, biological datasets, web graphs, social and collaboration networks|,Data,160
| The experiments were executed as follows. Traces were col- lected by using ICMP, UDP, and TCP Traceroute to probe the paths to a set of 100 destination websites from a source located on the Pennsylvania State University, University Park campus|,Data,161
| For UDP and TCP Traceroute, traces were collected using the default destination port numbers. We also collected traces using other ports and observed similar results|,Data,161
| Realistic Networks Here we compare the merged topologies produced by iTop, MN, and Isomap for realistic topologies. We use the Au- tonomous System (AS) topologies from both the Rocketfuel [20] and the CAIDA [21] projects, which represent IP-level connections between backbone/gateway routers of several ASes from major Internet Service Providers (ISPs) around the globe|,Data,161
| Although the paris-traceroute output of ITDK is more reliable than that of IPlane’s traceroute, the random selection of endpoints implemented by CAIDA hinders the collection of routes between the same vantage- and endpoints. Therefore we used the data of IPlane’s traceroute measurements|,Data,162
| They can also be used for constructing maps of the Internet at the Autonomous Systems level [, ]. In this work we used the CAIDA router-level Internet map from October th,  []|,Data,163
| 3 Table 1: Dataset Description Name BGP Usage AS Geolocation; Detour Detection Date 2016-01 Sources Info RouteViews, RIPE 38,688 RIBS, 416 peers, RIS 30 countries, 55GB Infrastructure IP List AS Geolocation 2016-01 to 2016-03 CAIDA Ark, iPlane, OpenIPMap, RIPE Atlas Measurements 3M Router IPs Infrastructure IPs to AS Mapping Infrastructure IP geolocation 2015-08 CAIDA ITDK, iPlane 6.6M IP to AS mappings AS to IXP Mapping AS Relationship AS Geolocation 2016-01 to 2016-03 Filtering peered paths from detection 2016-01 Traceroute Detour Validation 2016-05-01 IXP websites, PeeringDB, PCH CAIDA AS Relationship RIPE Atlas MaxMind Prefix Geolocation; Detour Validation 2016-01, 2016-03 MaxMind GeoLite City (free and paid) 368 IXP websites crawled 482,657 distinct relationships Used by Netra, 163 traceroutes Paid version used only for geolocating infrastructure IPs and detour validation longest prefix match on the global routing table and map the IP to the AS announcing the longest matching prefix|,Data,164
| As shown in Figure 3, we install LaBrea on a /29 subnetwork and use PlanetLab [9] to probe from multiple vantage points the entire /24 aggre- gate to which the /29 belongs. We scan the /24 network by attempting to establish TCP connections to each IP address in the subnet and capture the packets for further analysis|,Data,165
| • Active IPs in a Subnet: Intuitively, we might ex- pect high-occupancy subnets to be good indicators of pos- sible tarpits. To this end, we initially investigated using a hitlist of probable tarpits as inferred from the /24 subnets with more than 240 responding web hosts in the scans|,Data,165
| To facilitate large-scale scanning and avoid triggering anomaly detectors, degreaser uses permu- tation scanning [7, 12] to pseudo-randomly iterate through the IP address space when probing. Our real-world Internet scan, which probes at least one address in each /24 network in the Internet, discovers 107 different tarpit subnetworks (cid:20)(cid:24)(cid:25) ranging in size from /16 (with up to 216 fake hosts) to /24 (with up to 28 fake hosts)|,Data,165
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
| • Discovering correlations between anomalous traffic types detected with deep inspection techniques and traffic feature entropy variations. • Providing a traffic-type dissection (in-depth and entropy based) of a representative portion of the IBR for three weeks of April, 2012, with a 10-minute time scope|,Data,167
 Following is the summary of information about these data sets:  1. Data set from PREDICT USA [24] which contains traces of a DNS distributed denial of service attack (DDOS),Data,168
  from optical  2. Data set from CAIDA USA [25] which contains internet internet connectivity from 2002 and 2003,Data,168
  3. Data set from our experiment in which a PCAP file is captured from a lab computer which is being used for browsing and software development for the cyber security project,Data,168
| For PassWindow, the security parameter λ specifies the number of positions that a digit can be shown Moreover, two adjacent positions share a common vertical line Thus, a frame of λ positions consist of p(λ) = 5λ + 2 segments Figure 4 gives us an example of PassWindow with λ = 8|,Non-data,74
| The function challengeGen takes as input a key x, and re- turns a sequence of α challenges frames y = (cid:104)y1, y2,    , yα(cid:105)|,Non-data,74
| Challenges are represented in bitstrings The function response defines what are considered as valid responses when given a key x and a challenge y Any response r in the set of valid responses will be accepted In Figure 4, the response function will return the response r = 8596|,Non-data,74
| SVAP Authentication Process An SVAP is used in the following way There are two stakeholders, namely, the server (denoted by Server) and the user who wants to au- thenticate herself to Server (denoted by User) The inter- action proceeds as follows|,Non-data,74
 [Initialization] • Server chooses an SVAP [User registration (Over a secure channel,Non-data,74
|)] • User sends name u to Server • Server generates a secret visual key x using the keyGen function, x $← keyGen(1λ) • Server stores (cid:104)u, x(cid:105) and sends the image image(x) (likely in the form of a transparency) to User [User authentication|,Non-data,74
|] • User sends name u to Server • Server looks up (cid:104)u, x(cid:105), generates challenge frames y $← challengeGen(x), and sends the following chal- lenge frames image(y1),  |,Non-data,74
| , image(yα) to User • User overlays the key transparency on top of each of the challenge images, identifies the symbols di’s, com- putes the response r, and sends r to Server • Server accepts the user’s response and successfully completes the authentication process if and only if r ∈ response(x, y) 2|,Non-data,74
|2 Threat Model and Security Definition We consider an eavesdropping adversary who can eaves- drop on the communications between User and Server, ie, recording the challenges, the responses sent by the user, and the result of whether the responses are correct or not In this paper, we limit ourselves to an eavesdropping adversary because an eavesdropping adversary is sufficient for breaking the security of the concrete SVAPs we discuss here|,Non-data,74
| The ob- jective of the attacker is to successfully impersonate a user, by correctly answering the challenges in one session, without possessing the secret key We adapt the security definition from [25] as follows Definition 1 (((cid:96), p)-break) We say that an algo- rithm ((cid:96), p)-breaks an SVAP if and only if when given (cid:96) match- ing challenge-response pairs and an additional challenge, the algorithm’s response to the additional challenge is accepted with probability p on average|,Non-data,74
 no digitdigit: 2digit: 2123456710111011234567001101012345671011101keychallengewhat user seesframe 3: Pframe 4: 8frame 5: 5frame 6: 9frame 7: 6position 2,Non-data,74
|605If an SVAP is (0, p) breakable, then an adversary without eavesdropping ability can impersonate a user with proba- bility p on average in one attempt When p is above some acceptable threshold, this implies that the protocol is ex- tremely insecure|,Non-data,74
| 3 DESIGN OF SVAPS We now present two classes of concrete SVAPs which we call the uni-symbol SVAPs and multi-symbol SVAPs We as- sume seven-segment LED-style images for displaying digits are used, as in PassWindow 3|,Non-data,74
|1 Uni-Symbol SVAPs In this class of protocols, each frame shows at most one symbol We categorize these protocols along three design dimensions 31|,Non-data,74
|1 Design Dimensions 1 Noisy Frames or Not In PassWindow, each authen- tication session consists of α = 15 frames; however, only 4 of them displays a digit, and the 4-digit sequence is the response These 15 frames are displayed in an animation where each frame displays for around 2 seconds|,Non-data,74
| The an- imated image loops back to the first challenge frame after the last frame is displayed A frame encoding the symbol P alerts the users that the next 4 frames encode the digits The other 10 frames do not display any digit and are known as noisy frames We call this design the NSD (for Noisy-frame Single Digit) scheme, and the other design that does not use noisy frames and simply shows 4 digit-encoding frames with the response being the corresponding 4-digit sequence the BSD (for Basic Single Digit) scheme|,Non-data,74
| In BSD, the 4 challenge frames can be shown one by one, with each new frame dis- played after another digit is entered This way, users can respond at their pace The use of noisy frames aims at increasing the uncertainty for the adversary regarding which frames encode which sym- bols It comes at the usability cost of increasing the time it takes to complete an authentication session and is likely to introduce some stress since users need to decode visually under time pressure|,Non-data,74
 In Section 6 we shall show that using noisy frames decreases the level of security since the noisy frames leak additional information 2 Key and Challenge Generation Algorithms We consider two kinds of generation algorithms,Non-data,74
| In the first kind, keys and challenges are generated by randomly decid- ing whether each segment should be turned on or off In the next kind, keys and challenges are generated by randomly selecting a 7-segment pattern for each position In both cases, additional checks are needed to ensure that the key does not already encode a digit, and the challenges when overlaid with the key, resulting in acceptable images (eg|,Non-data,74
|, not encoding two digits in one frame) In this paper, unless explicitly mentioned, the protocols use a random pattern- based algorithms 3 Shared Edges or Separate Columns|,Non-data,74
| In PassWin- dow, the two adjacent 7-segment positions share a common vertical edge; we call this design Shared Edges (SE, see See Figure 4) The alternative design is not to use shared edges (see Figure 1) In this paper, unless explicitly mentioned, the protocols do not use shared edges As will be shown in the experiment (Section 6), we will show these three choices’ influence on security one by one|,Non-data,74
| Here we describe five concrete schemes They each differ from its previous one in one dimension The first two use noisy frames, and the latter three do not 3|,Non-data,74
12 Protocols 1 [NSD(7%+SE)] Noisy-frame Single Digit ran- domly displaying 7% Shared Edges The first proto- col we want to evaluate is PassWindow,Non-data,74
| NSD (7%+SE) mimics PassWindow by using 15 frames with 10 noisy frames It uses random segment generation algorithms (which produce key and challenge with a density close to the PassWindow exam- ples that are publicly available [4, 1]), noisy frames, and shared edges Specifically, the generation algorithms are as follows: keyGen Segments are turned on randomly with a probability of 25%; further check ensures that no position in the key displays any symbol|,Non-data,74
| challengeGen One first chooses a random symbol σ from Σ, then randomly selects a position p in the challenge frame and then turns segments in position p on so that σ is displayed in position p when combined with the key For the other non- encoding positions, the segments are turned on randomly such that the challenge frame has a density of about 7% Further sanity checks ensure that no other position encodes a symbol when overlaid with the key|,Non-data,74
| [NSD(20%+SE)] Noisy-frame Single Digit ran- 2 domly displaying 20% Shared Edges This protocol is motivated by the observation (from experiments) that chal- lenge frames in NSD (7%+SE) have limited entropy since the position on the challenge frame with the most number of segments turned on is almost certainly the one encoding the digit To mitigate this, NSD (20%+SE) displays 20% segments on the challenge frame|,Non-data,74
 3 [BSD(20%+SE)] Basic Single Digit randomly dis- playing 20% Shared Edges This protocol differs from NSD (20%+SE) in that it does not use any noisy frame There- fore α = 4,Non-data,74
 4 [BSD(20%)] Basic Single Digit randomly display- ing 20% non-shared edges This protocol differs from BSD (20%+SE)in that no shared edge is used 5,Non-data,74
| [BSD] Basic Single Digit randomly displaying valid patterns BSD differs from BSD (20%)in that the gen- eration algorithms are based on randomly choosing a pat- tern, as follows: keyGen A valid key pattern is any seven-segment pattern such that it has between 1 and 6 segments that are turned on, and it does not display any digit For each position of the key, randomly choose a valid key pattern|,Non-data,74
| challengeGen A challenge is generated using the following steps: (1) Randomly choose 4 symbols and 4 patterns such that each pattern is compatible with at least 3 of the chosen symbols A challenge pattern is compatible with a symbol if there exists a valid key pattern that displays the symbol when overlaid with the challenge This step aims to ensure that even if the symbol is known, there are at least 3 possi- bilities for its position|,Non-data,74
| (2) Place these 4 patterns randomly in 4 positions of the frame such that only one position dis- 606plays a symbol when combined with the key The segments in all other positions of the challenge frame are off This step is intended to minimize the amount of information leakage, and will be explained later 3|,Non-data,74
|2 Multi-Symbol SVAPs Section 6 demonstrates a fundamental weakness of uni- symbol SVAPs The information that a frame displays a par- ticular symbol entails that all other symbols are not dis- played at any position To defeat attacks exploiting this insight, we introduce multi-symbol SVAPs, where each chal- lenge frame can generate more than one digit We give three concrete protocols, they all use the same keyGen as in BSD, which is more secure than ones based on random segments|,Non-data,74
| 1 [HDD] Hashing-based Double Digit In HDD, each challenge frame generates two digits d1 and d2 The corre- sponding response is the one’s digit of the sum of d1 and d2, which can be viewed as a simple human computable hashing function: (d1 + d2) mod 10|,Non-data,74
| This scheme makes it more dif- ficult to rule out which digit is not displayed when knowing the response digit However, because 1 is never displayed, this protocol still leaks some deterministic information For example, if 9 is the response digit, then 8 cannot be displayed since it would require an 1 to result in 9 challengeGen|,Non-data,74
| Randomly choose 2 − 9 positions in the chal- lenge frame and randomly choose the same number of valid patterns to show in those positions in the following way: (a) every digit in Σ is compatible with at least 2 patterns; (b) when combined with the key only 2 of the challenge positions display digits 2 [HTD] Hashing-based Triple Digit HTD eliminates deterministic leakage of HDD by having each frame display three digits d1, d2, and d3|,Non-data,74
| The response is the one’s digit of the sum of d1, d2, and d3, that is: (d1 + d2 + d3) mod 10 This scheme requires more computation than other schemes, which may lead to worse usability The challengeGen for HTD is similar to HDD, except that each frame in HTD displays 3 digits instead of 2 3|,Non-data,74
| [EDD] Either of Double Digit EDD prevents the deterministic leakage of BSD without resorting to arithmetic A challenge frame superimposed by the visual key induces two digits d1 and d2; however, instead of responding with d1 + d2 mod 10, one is asked to respond with either d1 or d2 This reduces the entropy of each frame by one bit, but when observing one digit being the response code, any other digit could still be displayed|,Non-data,74
| 4 BREAKING UNI-SYMBOL SVAPS We now present Search, an attack against single-symbol SVAPs For ease of description, we assume that each posi- tion that can display a symbol which consists of 7 segments The attack equally applies to other settings|,Non-data,74
| The attack is based on the idea that after observing each successful authentication session, one can eliminate the keys that are incompatible with the transcript Definition 2 (Compatibility) Given an SVAP AV = (cid:104)PV, α, λ, keyGen, challengeGen, response(cid:105), and a transcript of (cid:96) successful authentications, (cid:104)Y = (cid:104)y1,  |,Non-data,74
|  , y(cid:96)(cid:105), R = (cid:104)r,   |,Non-data,74
| , r(cid:96)(cid:105)(cid:105), we say that a key x is compatible with the if and only if ∀i ∈ {1,    , (cid:96)}, ri ∈ transcript (Y, R), response(x, yi)|,Non-data,74
| 41 Key Universe Representation One straightforward way to exploit compatibility is to ex- plicitly maintain the set of all keys that are compatible with the transcript seen so far This requires maintaining one bit for each possible key (whether it is compatible with the transcript), and is infeasible as the resulting state size will be linear in the size of the key space (eg|,Non-data,74
|, for NSD (7%+SE), this is approximately 25λ+2, which is 277 when λ = 15) Thus the first major challenge to exploit compatibility is to decide how to represent the knowledge about the set of plausible keys A natural approach is to maintain for each segment, whether it must be “on”, must be “off”, or is uncertain Using 2 bits for each segment, this requires 2 × (5λ + 2) bits for NSD (7%+SE), which is 144 bits when λ = 15|,Non-data,74
| However, this fails to capture information regarding the inter-relationship among segments Suppose that from a challenge/response pair, one can conclude that one of two segments must be off (since otherwise it can be decoded into a different response); however, when one maintains informa- tion about segment independently, one can only record that this information Our solution exploits the structured and segmented nature of SVAPs We maintain information regarding each position consisting of 7 segments|,Non-data,74
| For a 7-segment position, there are 128 patterns, and we use one bit for each pattern, indicat- ing whether this pattern is plausible in this position This requires a total space of 128λ bits, which is 1920 bits when λ = 15 More specifically, we maintain the global plausible pat- tern set K = (cid:104)Π1, Π2,  |,Non-data,74
|  , Πλ(cid:105), where Πi : {0, 1}7 → {0, 1} gives the set of compatible patterns in the i’th position The downside of K, of course, is that such a representa- tion loses inter-position information (eg|,Non-data,74
|, if this position takes this pattern, then another position cannot be that pattern) However, such information is still preserved in the transcript, and can be dynamically exploited by searching, as we explain below 42 Deterministic Reduction of Key Universe The critical step in breaking an SVAP using searching is to reduce the global plausible pattern set K = (cid:104)Π1, Π2, |,Non-data,74
|   , Πλ(cid:105); this requires definite knowledge regarding each position This led us to discover a fundamental weakness of uni- symbol SVAPs: Every challenge-response pair leaks deter- ministic information|,Non-data,74
| For example, knowing that a frame encodes the digit 4 means that one knows for certain that none of the other digits is encoded in any position, even if one does not know where 4 occurs Thus, using the challenge pattern at each position, one could reduce the Πi’s Because of such information leakage, the random pattern challenge generation algorithm we introduced for BSD uses empty positions in the challenge whenever possible A few positions must be non-empty, to provide some uncertainty as to where a response digit is displayed|,Non-data,74
| However, each new challenge pattern that appears in a position enables one to eliminate key patterns that when combined with the challenge pattern would display digits not in the response 43 Search Trees While the deterministic information can help prune the 607global plausible pattern set K, combining that with search- ing can help further refine K The key challenge is to limit the total space consumed by the search trees to be below the available resources|,Non-data,74
| Conceptually, we construct one search tree for each ses- sion For each session, there are a number of choices For example, for protocols that use noisy frames, the first choice is which frame is the first frame that encodes a symbol In addition, for each such frame, there is a choice on which position displays a symbol|,Non-data,74
| In each node n, we store Kn, a set of plausible keys consistent with choices made to reach that node The root node uses the global plausible pattern set K Each node starts with the set from its parent, and uses the current choice made to create the current node to eliminate keys that are incompatible with this choice For example, against NSD schemes, once we have made a choice about which one among the α = 15 frames displays P , we know which symbol (if any) each frame encodes; this can be used to reduce Kn|,Non-data,74
| If a node has Kn = (cid:104)Πn λ(cid:105) 2 ,    , Πn i = ∅, we know the current such that one of its component Πn choice is inconsistent with the transcript, as no key pattern can be used on the i’th position; thus the node can be re- moved from the tree|,Non-data,74
| 1 , Πn We construct a tree by maintaining a queue of the frontier nodes to be expanded To avoid exhaust the memory, we will expand a tree only when its frontier consists of no more than N nodes, where N is set to 1000 in our experiments Whenever we expand a node (ie|,Non-data,74
|, making another guess), we simultaneously add all the children nodes into the search tree We know that one of the node in the frontier must be the correct one; however, we do not know which one it is Let F be the forest consisting of all trees For each tree T ∈ F , we use leaves(T ) to denote all the leaf nodes of T |,Non-data,74
| Whether they are fully expanded or not, we observe that the following equality must hold ∀T∈F ,∀i∈[1λ], Πi = Πn i (1) (cid:91) n∈leaves(T ) We use the above relation to prune K as follows|,Non-data,74
| Given a tree T , if Πi contains a pattern not in any leaf node of T , then this pattern can be removed from Πi Furthermore, whenever Πi is reduced, if another tree’s frontier node includes any pattern not in Πi, then that pattern should be removed If any node’s Πi becomes empty, then this node should be removed, which may cause Πi(cid:48) for some i(cid:48) to be updated This process computes a fixpoint that satisfies Eq|,Non-data,74
| (1) We also note that two trees can be combined, by having a cartesian product of their sets of leaf nodes The product of two nodes result in a node with their corresponding Πi’s intersected 4|,Non-data,74
|4 Combining Reduction with Searching Our attack strategy is to combine deterministic reduction with searching in an integrated fashion When given (cid:96) > 1 pairs of challenge/response, we first reduce K using deter- ministic information, and then create a search tree for each authentication session We iteratively perform the following until no change is made to any tree or K (1) Use Eq|,Non-data,74
| (1) to prune K and the search trees as much as possible (2) Expand any search tree if it is not fully expanded and it has no more than N = 1000 leaf nodes (3) If there exist two fully expanded trees in F such that the product of the sizes of their leaf sets is no more than N(cid:48) = 10000, merge the two trees by computing the cartesian product of their sets of leaf nodes The algorithmic description of search over one session is shown in Algorithm 1|,Non-data,74
| 45 Guessing the Response The attack strategy described so far is about how to re- duce K; we now discuss how to respond to a challenge frame The basic strategy is for each symbol, find whether there exists a plausible key in K that results in the symbol when overlaid with the challenge frame For challenges generated by random segment based algorithms, however, a more ef- fective response-guessing strategy is possible|,Non-data,74
| For each chal- lenge frame, order the symbols that are compatible with K by the number of segments that need to be added to form them, and guess the one that requires the fewest number of additional segments based on decreasing probability The intuition is that the symbol(s) that can be formed by adding the least number of segments are the most likely responses It turns out that using this guessing strategy alone, without any searching, one can guess the response for NSD (7%+SE) with probability around 01|,Non-data,74
| That is, this guessing attack can (0, 01)-break NSD (7%+SE) (as shown in Figure 5(c)) Note that such heuristic does not work for random-pattern based challenge generation algorithms 4|,Non-data,74
|6 Effectiveness of Noisy Frames The usage of noisy frames that do not encode any digit might appear to improve security, as it prevents adversary from knowing for certain which frame corresponds to which digit This, however, is not the case First, as each session has only 4 digits, even if one does not know which frame en- codes which digit, one can still exploit the information that none of the other 5 digits are shown on any of the α = 15 frames to reduce the plausible key space This, by itself, may already leak more information when compared with the case of using just 4 frames as in BSD|,Non-data,74
| With noisy frames, for each position, one has more than 15×5 = 75 challenge-digit pairs to eliminate plausible key patterns With just 4 frames, one has 4 × 8 = 32 pairs Second, one can search through each of the α = 15 possibilities, and compute the set of plausi- ble keys for each possibility, and remove key patterns that are incompatible with any possibility Note that after one makes the guess, the noisy frames leak additional informa- tion because they cannot encode any digit|,Non-data,74
| 5 ATTACKING MULTI-SYMBOL SVAPS Search is not effective against multi-symbol SVAPs as they reveal significantly less amount of deterministic information Without such deterministic information, the key reduction step of Search is not effective Without reducing the plau- sible key space, using the search trees is unable to make progress|,Non-data,74
| In this section, we present SolveLP, a class of attacks that model the problem as a pseudo-boolean satis- faction problem 51 Constraint Formulation The key is denoted by x, a challenge frame is denoted by y, and the response is denoted by r We use x[p] and y[p] to denote the key pattern and challenge pattern in position p of x and y, respectively|,Non-data,74
 We use [λ] = [1    λ] to denote the set of positions in a challenge frame or key,Non-data,74
| We use PK 608to denote the set of valid key patterns and PC to denote the set of valid challenge patterns Variables In our pseudo-boolean constraint formulation, the constraints are over the following 0 − 1 variables Xp,k = 1 if and only if x[p] = k, for p ∈ [λ] and k ∈ PK  There are λ ∗ ||PK|| such variables|,Non-data,74
| These variables encode the key we want to attack, and are the variables that we want to solve To make it easier to explain the constraints, we also introduce the following macro variables that can be expressed using Xp,k’s Yp,c,s, where p ∈ [λ], c ∈ PC , and s ∈ Σ Yp,c,s = 1 if and only if for the key we want to attack, using c as the challenge pattern in position p, displays the symbol s; it can be expressed as follows: (cid:88) Yp,c,s = Xp,k (2) k∈compatible(c,s) where compatible(c, s) is the set of all valid key patterns in PK that display s when overlaid with c|,Non-data,74
| Universal Constraints These constraints are applicable to all protocols The following requires that each position of the key takes only one key pattern ∀p ∈ [λ], Xp,k = 1 (3) (cid:88) k∈PK Note that the above represents λ constraints, one for each position|,Non-data,74
| These constraints are conjuncted with the con- straints below for encoding information revealed in each challenge/response pair EDD Specific Constraints For EDD we have the following constraints for each challenge frame y and response r • The challenge y generates exactly two digits: (cid:88) (cid:88) p∈[λ] s∈Σ Yp,y[p],s = 2 (4) This says that knowing the patterns on each position in the challenge, the total number of digits that are displayed is exactly 2|,Non-data,74
| Recall that Yp,y[p],s should be expanded according to Eq (2) • The challenge y generates r: Yp,y[p],r ≥ 1 (5) (cid:88) p∈[λ] Note that the ≤ 2 part is implied by Eq(4)|,Non-data,74
| HDD Specific Constraints For HDD we have the following two constraints for each challenge frame y and response r The first of which states that the response corresponding to frame y is r This constraint is the disjunction of two disjunctions, one corresponding to the case that two different digits r1 and r2 are displayed, and the other corresponding to the case that the same digit is displayed twice|,Non-data,74
| (cid:95) (cid:95) r1(cid:54)=r2∈Σ∧(r1+r2) mod 10=r (cid:95) r1∈Σ∧(r1+r1) mod 10=r  (cid:1) = 2 (cid:88) (cid:88) p∈[λ] p∈[λ] (cid:0)Yp,y[p],r1 + Yp,y[p],r2   = 2 Yp,y[p],r1 The second constraint captures the deterministic informa- tion revealed by HDD, that is, as 1 /∈ Σ, a challenge frame cannot encode a digit d such that (d + 1) mod 10 = r Note this does not apply to HTD (cid:88) (cid:88) Xp,k = 0 (6) p∈[λ] k∈compatible(y[p],(r−1) mod 10) HTD Specific Constraints The constraints for HTD in- clude only the first constraint for HDD, and can be obtained by natural extension|,Non-data,74
| We omit the details 52 Linear Programming Relaxation After obtaining the constraints for multiple challenge- response pairs (cid:104)(y1, r1)  |,Non-data,74
|  (yn, rn)(cid:105), one can attempt to use any off-the-shelf SMT, SAT, or pseudo-boolean solver to solve for the variables of the form Xp,k Such an assign- ment of the Xp,k variables will give us one plausible key which is compatible with the given challenge-response pairs (yi, ri) However, in experiments we have found that all the constraint solvers we have tried fail to scale for λ ≥ 10|,Non-data,74
| To meet this challenge, we apply the linear programming relaxation technique to solving the 0-1 constraint satisfac- tion problem That is, instead of assigning binary values to the variables, we assign fractional values to them, and inter- pret a larger value as more likely to be 1 We further apply the idea of multiplicative updates to solve the resulting lin- ear programming problem Our usage of the multiplicative update method can be roughly viewed as interpreting a fractional value assigned to a variable Xp,k as the probability of x[p] = k|,Non-data,74
| Initially, all the probabilities are uniform, ie, for all p, k, Xp,k = 1||PK||  However, the initial values of Xp,k may not respect the con- straints from challenge-response pairs, which consequently means we have to update the values of Xp,k|,Non-data,74
| We update the variable values in a multiplicative fashion For instance, if we have a constraint Xi +Xj = t, and currently Xi +Xj = s, we update the the values in the following way: Xi ← Xi t and Xj ← Xj s t s  If the constraint is inequality, we will only update the values (taking the RHS of the inequality as the target) when the inequality does not hold As Xp,k can appear in different constraints, updating Xp,k values to sat- isfy one constraint may end up violating another constraint|,Non-data,74
| Hence, we loop through all constraints until the values of the variables converge (if the cumulative change of the variables is below a small threshold, we use 001 in experiments), or a loop threshold (100 in experiments) is reached 53 Attacking EDD To attack EDD, one has a set of linear constraints that are logically and’ed together|,Non-data,74
| We thus can readily apply the above multiplicative update methods When given the chal- lenge frame y, we enumerate all possible response r, encode y and r using assuming r is the response, and just use this summation (LHS of Eq (5)) as the weight corresponding to r The response with highest weight is chosen to be the guess|,Non-data,74
| 54 Attacking HDD and HTD For HDD and HTD, because of the uncertainty of which pairs of digits are displayed, the constraint for each frame con- sists of a disjunction, which cannot be directly handled by 609the multiplicative update approach We now explain our approach for HDD, which can be easily generalized to HTD To attack HDD, we have to combine guessing which pairs are displayed with solving the system of constraints|,Non-data,74
| Given a pair (cid:104)y, r(cid:105), we enumerate through all possible dig- its r1, r2 ∈ Σ such that r1 + r2 = r, and compute vy(r1, r2) = max Yl,y[l],⊥, Yi,y[i],r1 × Yj,y[j],r2 × (cid:89) Xl,k −(cid:80) s∈Σ Yl,y[l],s This esti- mates the probability that r1 and r2 are displayed We then use the pair with the highest value as the weight for this frame k∈PK where Yl,y[l],⊥ = (cid:80) i,j∈[λ] : i(cid:54)=j l(cid:54)=i,j We compute the weight for all frames in the transcript, and start with the frame with highest weight|,Non-data,74
| The two digits that produces this weight are assumed to be shown in this frame, and encoded as a constraint We then update the variables using all constraints, and guess the next frame We do this until we’ve guessed 2/3 of all frames We leave 1/3 frames unguessed because it is more probable to make wrong guesses in the last few frames (as guesses are made in the descending order of confidence), and one wrong guess would make the final guessing inaccurate|,Non-data,74
| This 1/3 ratio is our initial heuristic choice In the experiments, we found that using a ratio of 1/4 performs slightly better, and a ratio of 1/2 is slightly worse We did not attempt to further optimize this ratio When the transcript is updated, the above computation will be repeated|,Non-data,74
| When given the challenge frame, we compute the score for each response r, which is the sum of vy(r1, r2) such that (r1 + r2) mod 10 = r, and choose r with the highest score 6 EXPERIMENTAL EVALUATION In this section, we report experimental results concerning the security analysis of SVAPs under the two attacks: Search and SolveLP 6|,Non-data,74
|1 The Search Attack Experimental setting All the reported results here are averaged over 10, 000 runs For each run, we gener- ate a key and 20 sessions (100 for HDD) of simulated chal- lenge/response Starting from the first session, for each session we do the following (1) generate a challenge/response pair (y, r); (2) without r, use K, the current global set of plausible keys, to guess a response to y, output whether the guess is correct; (3) compute the set of compatible responses, and output the size of this set; (4) output the size of plausible keys in K; (5) add (y, r) to the transcript, and update K|,Non-data,74
| We aggregate the output from step (2) of 10, 000 runs into the probabilities of making a correct guess after using the 1 transcript of a certain number of sessions We use log2 p when plotting the graphs This can be viewed estimating the min-entropies of the guesses We perform steps (3) and (4) to generate data that enable us to better understand the relationship between sizes of plausible keys, numbers of compatible responses, and first-guess success probability|,Non-data,74
| We attack the protocols presented in Section 31, ie, NSD (7%+SE), NSD (20%+SE), BSD (20%+SE), BSD (20%), and BSD|,Non-data,74
| We also attack HDD (α =3), as a comparison of the ef- fectiveness between Search and SolveLP The Search attack is completely ineffective against EDD and HTD We present re- sults for two security parameters, λ = 15 and λ = 30 Larger λ values hurt usability|,Non-data,74
| Effectiveness of the attack Figure 5 shows the results of applying Search against the above protocols From Figure 5, we can observe the following results First, from Figures 5(b) and 5(c), we can observe that Search can (4, 0|,Non-data,74
|25)-break BSD (20%) This means that af- ter 4 sessions, a guess with succeed with probability 024 However, at this point, the number of plausible keys in K is roughly 220|,Non-data,74
| The same trend can be seen for other protocols as well This suggests that it is possible to carry out an impersonation attack even though a large amount of uncer- tainty regarding the key remains Second, from Figures 5(a) and 5(c), we can see that ex- cept for BSD, the level of security indicated by min-entropy is significantly lower than that indicated by the number of possible responses This indicates two things|,Non-data,74
| First, the number of possible responses is not always an accurate indi- cator of security Second, random segment-based challenge generation algorithms are vulnerable to our guessing attacks based on the number of segments needed to display a digit Third, NSD (7%+SE) (ie|,Non-data,74
|, our attempt to duplicate Pass- Window) is extremely insecure Even a stateless adversary, not having access to any response, can succeed in the first guess with probabilities 107% and 82% for λ = 15 and 30, respectively|,Non-data,74
| After intercepting only 3 challenge/response pairs, the success probabilities increase to 669% and 262%, respectively Fourth, while NSD (20%+SE) offers a slightly better secu- rity than NSD (7%+SE) when the attacker has 0 or 1 chal- lenge/response pair, its security degrades to the same level as NSD (7%+SE) with 2 or more intercepted sessions; this is because by using denser segments on non-encoding po- sitions, it also leaks more information regarding the key in each session|,Non-data,74
| Fifth, using noisy frames is a bad idea This is because the noisy frames actually provide more deterministic infor- mation such as which digits are not shown Removing noisy frames improves the level of security significantly Sixth, using shared edges between adjacent positions or not has almost no noticeable impact on the level of security|,Non-data,74
| Seventh, all the uni-symbol SVAP variants we identified are very insecure For λ = 15, Search is able to (8, 025)-break all of them, ie|,Non-data,74
|, after observing 8 challenge-response pairs, an adversary can successfully impersonate a user with prob- ability 025 For λ = 30, Search can (12, 009)-break all uni-symbol SVAP protocols|,Non-data,74
| Finally, Search can (20, 2−984) break HDD with λ = 15, indicating that HDD may be considered to be acceptable if this is the best attack we have 62 The SolveLP Attack Experimental setting|,Non-data,74
| The experimental setting is simi- lar to that of the previous attack; however, we report only the min-entropy For this attack, instead of reporting the cardinality of the set of plausible keys and responses, we only make a guess and thus report only the success proba- bility of the guess In this experiment, we evaluate the effect of applying SolveLP on EDD, HDD, HTD, and BSD Effectiveness of the attack|,Non-data,74
| Figure 6 illustrates the Min- Entropy of our attack against EDD, HDD, and HTD We present results for security parameters λ = 15 and λ = 30 We now highlight some of our findings 610(a) #plausible keys (λ = 15) (b) #possible responses (λ = 15) (c) Min-Entropy of attack (λ = 15) (d) #plausible keys (λ = 30) (e) #possible responses (λ = 30) (f) Min-Entropy of attack (λ = 30) Figure 5: Effect of the Search attack|,Non-data,74
| The X-axes correspond to (cid:96), the number of authentication sessions for which the adversary has intercepted the transcripts The Y-axes in the subfigures 5(a) and 5(d)) correspond to the number of candidate keys The Y-axes in the subfigures 5(b) and 5(e) correspond to the number of plausible responses The Y-axes in the subfigures 5(c) and 5(f )) correspond to the inverse of the probability that the first guess succeeds|,Non-data,74
| The values in Y-axes are in Log Scale (Base 2) First, EDD performs the worst under the attack, even worse than BSD This is because there are two, instead of one, cor- rect responses for each frame SolveLP can (4, 0|,Non-data,74
|25)-break both BSD and EDD Second, SolveLP is more effective than Search on both BSD and HDD For example, when λ = 15, SolveLP can (4, 025)-break and (8, 0|,Non-data,74
|25)-break BSD and HDD, respectively, whereas Search can (8, 025)-break and (8, 2−997)-break BSD and HDD, respectively Third, increasing the key length also increases the secu- rity of the protocol (see Figure 6)|,Non-data,74
| However, increasing the key length to achieve an acceptable level of security yields deployment and usability challenges (eg, the key card size) Finally, all the multi-symbol SVAP protocols are (10, 0|,Non-data,74
|25)- breakable for λ = 15, and (20, 0125)-breakable when λ = 30 The results directly exhibit the potency of the SolveLP against the multi-symbol SVAP protocols and let us draw the conclusion that the concrete SVAPs we have considered are insecure in general Designing protocols that are usable and secure at the same time is a fascinating future research direction|,Non-data,74
| Security of different protocols Table 1 presents the levels of security of different schemes in another way If we view min-entropy of 3 as a point at which we declare a protocol to be broken, Table 1 shows how many sessions a protocol can withstand under an eavesdropping attack Efficiency of the attacks|,Non-data,74
| Now we report the running time of our attacks We measured the wall clock time it takes (ie, using the Linux time utility) to generate and guess 20 sessions using key length 15|,Non-data,74
| All experiments were carried out on a 340GHz Intel(R) Core(TM) i7-3770 CPU running GNU/Linux with 16GB RAM For Search, it takes less than a second to run an attack instance SolveLP takes less than 0|,Non-data,74
|98 minute to run a single instance of EDD whereas it takes 108 and 110 minutes for HDD and HTD, respectively Also, we implemented the attacks in python, and we did not aim at optimizing the attack|,Non-data,74
| The reported times here are just representative examples demonstrating the feasibility of the attacks in real life Note that employing SolveLP using pseudo-boolean, SAT, or SMT solvers are not feasible in practice To the best of our knowledge, the best free solvers for our form of constraints are MiniCard [2, 3] (for EDD), and MiniSat+ [3] (for HDD and HTD) Unfortunately, none of the solvers are efficient, especially when the key length is long (e|,Non-data,74
|g, 15) Specifically, for EDD, it takes MiniCard around 10 minutes to make a guess for one instance, and much longer for HDD 7|,Non-data,74
| USABILITY EVALUATION We have conducted a human subject study to evaluate the usability of several SVAPs Our study was vetted by our in- stitution’s IRB and was given an exemption on the grounds that it is based on survey procedures, and individual partic- ipants cannot be identified from the study 71 Study Design Protocols Studied|,Non-data,74
| We studied the usability of the fol- lowing protocols: NSD (7%+SE), EDD, HDD, and HTD For pre- sentation purposes, we use the alias ASD for NSD (7%+SE) For each of the protocols, we displayed the superimposed image (ie|,Non-data,74
|, the key combined with the challenge frame) on a browser, and let the participants respond according to the protocol’s authentication requirement We measured each  0 20 40 60 80 100 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 0 2 4 6 8 10 12 14 16 18 20 0 50 100 150 200 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 0 2 4 6 8 10 12 14 16 18 20NSD(7%+RS)NSD(20%+RS)BSD(20%+RS)BSD(20%)BSDHDD611(a) Min-Entropy (λ = 15) (b) Min-Entropy (λ = 30) Figure 6: The X-axes in the curves correspond to (cid:96), the number of authentication sessions for which the adversary has intercepted the transcripts The Y-axes in the subfigures 6(a) and 6(b)) correspond to the inverse of the probability that the first guess succeeds The values in Y-axes are in Log Scale (Base 2)|,Non-data,74
| λ 15 30 NSD (20%+SE) BSD (20%+SE) (29, 08) (43, 1|,Non-data,74
|1) (37, 15) (73, 3|,Non-data,74
|1) Search BSD (20%) (38, 19) (71, 3|,Non-data,74
|6) BSD HDD BSD EDD HDD HTD (77, 14) (131, 2|,Non-data,74
|3) (436, 47) (833, 7|,Non-data,74
|5) (32, 09) (57, 1|,Non-data,74
|1) (25, 11) (45, 1|,Non-data,74
|4) (67, 17) (131, 1|,Non-data,74
|6) (114, 36) (160, 2|,Non-data,74
|9) SolveLP Table 1: A value in a cell is of the form (s, k) in which s denotes the average number of sessions for an attack to have min-entropy ≤ 3, ie, the first guess succeeds with probability ≥ 1 8  The value k denotes the standard deviation of s|,Non-data,74
| user’s accuracy and average completion time of each proto- col authentication session An authentication session of EDD, HDD, and HTD consists of four frames (ie, α = 4), whereas an authentication session of ASD is an animation of fifteen frames among which only four frames display digits which constitute the desired 4-digit response (i|,Non-data,74
|e, α = 15) For EDD, HDD, and HTD, we choose α to be 4 so that the response lengths of these protocols are consistent with ASD We also noticed the existence of a few 7-segment LED patterns which closely resemble the 7-segment LED patterns representing digits, but are not valid digit-representing pat- terns|,Non-data,74
| We call these patterns confusing patterns (CP) See Figure 7(a) and Figure 7(b) for these CP and their valid counterparts During the training session, we drew users’ attention to those CP to prevent them from getting con- fused Meanwhile, we also evaluated the influence of CP on authentication, by considering protocol variants which never display these confusing patterns|,Non-data,74
| We identify the protocol variants which exclude CP with a trailing ‘-’ in their name, eg, ASD-, EDD-, HDD-, HTD- Evaluation Process|,Non-data,74
| We ran our user study through Ama- zon’s Mechanical Turk (MTurk) Each study participant is randomly assigned a specific protocol to use and we re- quire the participant to complete five authentication sessions of that protocol We evaluated the accuracy rate and the completion time of each participant Before the evaluation phase, there is a training phase explaining the protocol’s authentication requirement|,Non-data,74
| For ASD and ASD-, the study web page displays an ani- mation of 15 images, with each image lasting two seconds Participants are can enter the four response digits at any time and then click a button to submit For the other pro- tocols, the page displays a static image (a single frame), and the participants need to input the response for that frame in order to proceed to the next frame The training phase contains a single authentication session with explanations describing the requirement of the task|,Non-data,74
| The authentication session used is similar to the ones used in the real study 72 User Study Result Analysis For each of the eight protocols (ASD, EDD, HDD, HTD, ASD-, EDD-, HDD-, HTD-), we recruited fifty participants on MTurk The participants’ ages range from 18 to over 50, with about 80% between 23 and 40|,Non-data,74
| Roughly, 80% of the participants hold bachelor’s or master’s degree Male participants make up around 60% of all participants The distributions are similar for all the eight groups Figure 8 gives the accuracy and completion time of dif- ferent protocols|,Non-data,74
| We now highlight some of our interesting findings below Accuracy In terms of accuracy, EDD performs better than ASD (t = 1361, p = 0|,Non-data,74
|177), HDD (t = 1925, p = 0057), and HTD (t = 3096, p = 0|,Non-data,74
|003) 1 EDD is followed by ASD, which is slightly better than HDD HTD is the worst, with accuracy be- low 80% This ordering is expected|,Non-data,74
| The differences among these three, however, are not statistically significant Completion time EDD is also the clear winner in terms of completion time (t significant at p < 10−8), taking an average of less than 20 seconds per session EDD is followed by ASD and HDD, each taking an average of around 40 sec- onds|,Non-data,74
| Given the animated nature of ASD, taking an average of around 40 seconds appears reasonable If a participant fails to identify a digit, she has to wait for the loop to re- turn to the same frame again With an animation loop of 30 seconds, users who did not succeed in the first time could take close to 60 seconds It is a bit surprising that HDD takes as long as ASD|,Non-data,74
| This is due to a combination of the need to 1t and p are values used in the t-test to indicate whether a result is statistically significant; t denotes how many stan- dard deviations, and p is the probability; larger t and smaller p mean higher significance  0 2 4 6 8 10 12 0 2 4 6 8 10 12 14 16 18 20 0 2 4 6 8 10 12 0 2 4 6 8 10 12 14 16 18 20BSDHDDHTDEDD612(a) Confusing Patterns (b) Valid Patterns Figure 7: Confusing Patterns and their correspond- ing Valid Patterns recognize two digits and to do the extra arithmetic step HTD takes the longest, with around 50 seconds per authentication session Does practice make perfect? From figure 9(a), we can see that, in most cases, the completion time decreases as the evaluation proceeds|,Non-data,74
| The completion time of the last session is less than the first one in all protocols (t significant at p < 02) However, in terms of accuracy (Figure 9(b)), it appears that for EDD and ASD, the accuracy improves For HDD and HTD, the accuracy rates change more erratically|,Non-data,74
| In particular, in the last session of HTD, the accuracy rate drops by a large value One possible reason is that partic- ipants lost their patience at the last authentication session after performing twenty or so additions of three single-digit numbers Overall Assessment Our results suggest that HDD offers similar usability as ASD, but with higher security|,Non-data,74
| However, the level of security offered by HDD is still far from satisfac- tory While HTD offers some security enhancement over HDD, this comes with significant usability cost How “Random” is EDD? Among all the participants as- signed to EDD or its variant, only a small amount (2%) of participants always choose either the left or the right digit displayed in a frame consistently Roughly, half of the par- ticipants (i|,Non-data,74
|e, 44%) randomly choose between the left and right digits They choose the left digit roughly with proba- bility 04 to 0|,Non-data,74
6 The distribution of the probability a partici- pant chooses the left digit is similar to a normal distribution This finding is interesting due to the fact that it contradicts the belief that the probability of the left digit being picked is higher Influence of confusing patterns,Non-data,74
| We evaluated the influ- ence of the confusing patterns (CP) on usability As shown in Figure 8, removing CP helps increase the users’ accuracy for EDD (t = 2982, p = 0004), and at the same time, decrease the completion time for HDD (t = −1|,Non-data,74
|649, p = 01) and EDD (t = −2152, p = 003)|,Non-data,74
| For the rest of the protocols, the impact of removing CP in the context of accuracy or com- pletion time is not substantial 8 RELATED WORK Human Identification Protocols The first theoreti- cal foundation of human identification protocol (HIP) dates back to the work by Matsumoto and Imai [23]|,Non-data,74
| However, their scheme was broken by Wang et al [34] Hopper and Blum [18] proposed to use a NP-hard “learn- ing parity with noisy” (LPN) problem Users can compute inner product of a secret bit-string and a challenge bit-string and uses the result to authenticate herself|,Non-data,74
| Weinshall [35] proposed Cognitive Authentication Scheme (CAS) that uti- lizes human memory and cognition to differentiate two sets of images However, this scheme is later broken by Golle and Wagner [17] They used a SAT solver and recovered the Figure 8: Accuracy and Completion Time of Differ- ent Protocols (a) Completion Time (b) Accuracy Figure 9: Completion time (in second) and accuracy throughout the five sessions|,Non-data,74
| secret after intercepting challenge-response pairs of tens of sessions Bai et al proposed Predicated-based Authentication Ser- vice (PAS) [8] in which the user and the server agree on a predicate During authentication, the server provides the arguments as challenges and the user applies the predicate to obtain the response|,Non-data,74
| One year later, Li et al [21] took advantage of intersection attack to break this scheme Rosener et al proposed GridCode [32] as a layer for obfus- cating passcodes, but it was broken by Molloy and Li [24]|,Non-data,74
| They were able to recover the victim’s password and addi- tional secret after intercepting a small number of challenge- response pairs Human Computable Machine Unbreakable (HCMU ) hash schemes [10, 9] have been proposed for generating textual passwords [10, 9] Since they do not use any external mem- ory aid (such as a key card in SVAPs), such schemes are harder to design and to use For example, the scheme in [10] re- quires a human user to memorize a random mapping from letters to digits, a random permutation of the digits, and to perform single digit mod 10 additions|,Non-data,74
 Yan et al [36] studied the inherent complexities of de- signing authentication mechanisms that do not rely on the underlying computing device They consider those human- computable authentication protocols that depend on the hu- man users’ memory and cognitive capacity They provide two general classes of attacks that can circumvent these au- thentication mechanisms,Non-data,74
 They also provide design princi- ples for leakage-resilient password systems and described a framework for measuring the cognitive overload of a proto- col Their framework and attacks are not directly applicable ASD/ASD-HDD/HDD-HTD/HTD-EDD/EDD-00020,Non-data,74
4060810Accuracy0102030405060Time(s)Accuracyw/CPAccuracyw/oCPTimew/CPTimew/oCP 0 20 40 60 1 2 3 4 5 0,Non-data,74
|7 08 09 1 1 2 3 4 5ASDHDDHTDEDD613to SVAPs as SVAPs allow the use of a key card, which can store a high-entropy secret Visual Cryptography (VC)|,Non-data,74
| Visual cryptography (VC ), introduced by Naor and Shamir [26, 27], can be viewed as an instance of human computable cryptography where de- coding a ciphertext takes advantage of the visual channel of a human user In this model, the ciphertext and the secret key are printed images and revealing the plaintext requires superimposing the two printed images (correspond- ing to the ciphertext and the key) which user can visually observe Note that individually any of the printed images (ciphertext or key) is indistinguishable from a random noise Ateniese et al|,Non-data,74
| [6] and Lu et al [22] considered VC-based sharing scheme in a graph setting in which a secret image is associated with each edge of the graph The problem is to generate shares for each vertex of the graph such that when the shares of two distinct vertices v1 and v2 are combined, it exposes the secret image corresponding to the edge (v1, v2) Ateniese et al|,Non-data,74
 [7] then proposed extended VC schemes that allow the shares to be meaningful figures in order to avoid suspicion and censorship Rijmen and Preneel [31] first pro- posed a VC scheme to use different colors other than black and white The same idea was later improved by Hou et al [19],Non-data,74
| Chavan et al [15] proposed hierarchical VC (HVC), that can be used to derive more on-demand shares Abboud et al [5] proposed to combine steganography and VC, but their scheme requires the human computation to be more involved|,Non-data,74
| Chang and Hu [13] proposed to use VC to distribute shares of a copyright among its stakeholders Watermark schemes based on VC have also been proposed [20, 16] VC can also be used to generate voting receipts [14] that enjoy verifiabil- ity and coercion resistance VC-based authentication|,Non-data,74
| Noar and Pinkas [25] pro- posed the first framework for VC-based authentication and identification Their Visual Identification problem is simi- lar to what we call Visual Authentication Mechanism The security notion we use for SVAPs in our paper is based on the one in [25] In the scheme they propose in [25], the key consists of a number of squares, each is painted with one of 10 colors|,Non-data,74
| A challenge selects d squares, and the re- sponse is the sequence of colors for the d squares, sent in some predefined order Borchert’s note [12] discussed the idea of segment-based VC which can be viewed as a spe- cial case of SVAPs However, in these schemes from each challenge/response pair, an adversary can trivially recover a portion of the key used in the pair 9|,Non-data,74
| CONCLUSIONS In this paper, we presented an abstract framework for designing SVAPs We discussed two classes of protocols: uni- symbol SVAPs and multi-symbol SVAPs Then, we came up with two attacks Search and SolveLP against them Finally, we carried out a user study to evaluate the usability of some of the protocols|,Non-data,74
| Our overall findings are negative; our attacks can break all the protocols we have developed including a commer- cial protocol This points to the following two fundamental weaknesses of SVAPs First, there is not enough entropy in the secret key Second, SVAPs are highly structured; they do not effectively enlarge the search space of the attack|,Non-data,74
| Therefore, it remains an open problem to design a secure yet usable protocol based on visual cryptography 10 ACKNOWLEDGMENTS This material is based upon work supported by the National Science Foundation under Grant No 1314688|,Non-data,74
 Hemanta K Maji’s research is supported by CNS-1566499 We thank Matthew Walker for providing some insights and details of the PassWindow scheme We also thank the anonymous reviewers for their valuable suggestions,Non-data,74
|ABSTRACT We present a highly efficient cryptographic protocol to pro- tect user passwords against server compromise by distribut- ing the capability to verify passwords over multiple servers Password verification is a single-round protocol and requires from each server only one exponentiation in a prime-order group In spite of its simplicity, our scheme boasts security against dynamic and transient corruptions, meaning that servers can be corrupted at any time and can recover from corruption by going through a non-interactive key refresh procedure The users’ passwords remain secure against of- fline dictionary attacks as long as not all servers are cor- rupted within the same time period between refreshes|,Non-data,75
| The only currently known scheme to achieve such strong security guarantees incurs the considerable cost of several hundred exponentiations per server We prove our scheme secure in the universal composability model, which is well-known to offer important benefits for password-based primitives, under the gap one-more Diffie-Hellman assumption in the random-oracle model Server initialization and refresh must take place in a trusted execution environment Initializa- tion additionally requires a secure message to each server, but the refresh procedure is non-interactive|,Non-data,75
 We show that these requirements are easily met in practice by providing an example deployment architecture Categories and Subject Descriptors D46 [Security and Protection]: Cryptographic control; D,Non-data,75
|46 [Security and Protection]: Access controls; D46 [Security and Protection]: Authentication Keywords Password verification, proactive security, UC security|,Non-data,75
| 1 INTRODUCTION In spite of all their shortcomings in terms of security and usability, passwords are still the predominant method of on- Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than the author(s) must be honored Abstracting with credit is permitted|,Non-data,75
| To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from Permissions@acmorg CCS’15, October 12–16, 2015, Denver, Colorado, USA|,Non-data,75
 Copyright is held by the owner/author(s) Publication rights licensed to ACM ACM 978-1-4503-3832-5/15/10 ,Non-data,75
$1500 DOI: http://dx,Non-data,75
doiorg/101145/28101032813722 ,Non-data,75
| line user authentication One of the main threats currently posed to password security is server compromise More than one billion personal data records were reported stolen in 2014 alone [16]; most of these records included user pass- words With more personal and financial data moving into the cloud, a further increase in breaches targeting usernames and passwords is expected for 2015 [14]|,Non-data,75
| Even when properly salted and hashed, the low entropy in human-memorizable passwords is no match for the brute force of modern hardware: already in 2012, a rig of 25 GPUs could test up to 350 billion guesses per second in an of- fline dictionary attack More complicated password hash functions [20, 24] can provide some relief, but at a linear rate at best: the computational effort to verify passwords for an honest server increases by the same factor as for the attacker—while the latter is probably better equipped with dedicated password-cracking hardware The problem of offline dictionary attacks when a server is compromised is inherent whenever that single server can test the correctness of passwords A natural solution, first proposed by Ford and Kaliski [15], is therefore to split up the capability to verify passwords over two or more servers, so that security is preserved as long as less than a thresh- old of them are hacked|,Non-data,75
| This has been the central idea be- hind several threshold password-authenticated key exchange (TPAKE) [17, 22, 2, 12, 25, 21] and threshold password- authenticated secret sharing (TPASS) [3, 10, 9, 18, 6] pro- tocols as well as behind the RSA product Distributed Cre- dential Protection (DCP) [13] Resistance against server compromise is one thing, but knowing how to recover from it is another Without secure recovery, all one can do in case of a detected breach is to re-initialize all servers and request all users to reset their passwords—which is probably exactly what one wanted to avoid by deploying the scheme In cryptographic literature, recovery from compromise is known as proactive security or security against transient corruptions|,Non-data,75
| Of the aforemen- tioned threshold password-authenticated protocols, only Ca- menisch et al [6] describe a recovery procedure and prove their protocol secure against transient corruptions Di Rai- mondo and Gennaro [12] mention the possibility to refresh shares and the RSA DCP product description [13] mentions a re-randomization feature, but neither provides details or a security proof Proactive security in the protocol of Ca- menisch et al|,Non-data,75
| [6] unfortunately comes at a considerable cost: “a few hundred exponentiations” per server may be within practical reach for occasional data retrieval, but not for high- volume password verification 182Our contribution We present two simple and extremely efficient proactively secure distributed password verification protocols, allowing a login server LS and a number of back-end servers S1,  |,Non-data,75
|  ,Sn to jointly determine the correctness of a user’s password, while ruling out offline dictionary attacks unless all servers are corrupted during the same time period A corrupt LS only sees the passwords of user accounts that are created or logged into during the corruption No passwords, pass- word hashes, or any other offline-attackable information is leaked for accounts that are inactive during the corruption|,Non-data,75
| We think this is a reasonable compromise for not requiring user-side software, as it provides adequate protection against “smash-and-grab” attacks and short-term corruptions Login, ie, password verification, is a single-round proto- col requiring just one exponentiation in a prime-order group on each server (two for LS), which is essentially optimal unless schemes without public-key operations can be found|,Non-data,75
| The recovery and key refresh procedure is non-interactive and only involves a couple of additions and pseudo-random function evaluations per server, making it more than efficient enough to perform it preventively on a regular basis instead of just after a detected breach Our first construction works in any prime-order group, including elliptic curves, and in- volves a three-round account creation (password setup) pro- tocol with three exponentiations per server (six for LS) Our second construction is based on elliptic curves with bilinear maps and also offers single-round account creation with one exponentiation per back-end server and one exponentiation and one pairing computation for LS Both our protocols as- sume that the key refresh procedure has access to a special backup tape that is not connected during normal operation|,Non-data,75
| In practice, this can be achieved by using smart cards or by making use of properties of modern cloud platforms, as we will explain Given their extreme efficiency, it is all the more surpris- ing that we managed to prove our constructions secure under a very strong universally composable (UC) [5] notion with transient corruptions Parties can be dynamically corrupted at any point in the protocol, even between communication rounds Transiently corrupted parties leak their full state, but not the content of their backup tape, to the adversary and remain corrupted until the next key refresh|,Non-data,75
| Perma- nently corrupted parties additionally leak the backup tape and cannot be recovered As was argued before [21, 10, 9, 6], universal composabil- ity offers important advantages over traditional game-based definitions in the particular case of password-based proto- cols Namely, UC notions leave the choice of passwords to the environment, so that arbitrary distributions and de- pendencies between passwords are correctly modeled This is crucial to guarantee security in real-life settings where users make typos when entering their passwords, share pass- words, or use the same password for different accounts— none of which are covered by currently known game-based notions|,Non-data,75
| Also, it is very unclear whether protocols can be se- curely composed with the non-negligible attack probabilities that game-based definitions tend to employ We prove our constructions secure in the random-oracle model under the (gap) one-more Diffie-Hellman assumption that was previ- ously used to prove security for blind signature [4], oblivious transfer [11], TPASS protocols [18], and set intersection pro- tocols [19] We achieved this rare combination of strong security and high efficiency by careful proof techniques in the random- oracle model, as well as through some of compromises in security that are very reasonable for practical use, but save on cryptographic machinery in the protocol design First, we assume that the initialization of all servers takes place in a trusted environment where all servers are honest|,Non-data,75
| During initialization, we assume that LS can transmit one secure message to each back-end server Si This secure initializa- tion is not hard to achieve in practice, as we explain in Sec- tion 6 Server refresh, ie|,Non-data,75
|, whereby a server can recover from a transient corruption, does not require any interaction with other servers Second, the back-end servers S1,   |,Non-data,75
| ,Sn do not learn which user is logging in or whether the password was correct This definitely limits their ability to throttle failed login attempts, but since LS can apply clever throttle algorithms based on user id and login results, the natural throttling of back-end servers just by requiring network communication should suf- fice to fend off attacks Finally, we do not cover robust- ness: an adversary can make LS “err on the safe side” and conclude that the password was false while in fact it was correct—but not the other way around This could be fixed by adding the same zero-knowledge or pairing verification as during account registration|,Non-data,75
| This would have a major impact on efficiency, however, so we prefer to accept this rather benign attack in the model As a technical contribution, our scheme employs a novel technique to obtain proactive security that may be of inde- pendent interest In a nutshell, we start off from a basic scheme that is secure under dynamic but non-transient cor- ruptions The basic scheme is secure under the gap one-more Diffie-Hellman assumption, but the security proof requires guessing one server at the beginning of the game that will not get corrupted during the game|,Non-data,75
| This guessing induces a tightness loss in the reduction equal to the number of servers While that loss could still be tolerated, things get worse when moving this scheme into a proactive setting Here one would have to guess an uncorrupted server at the beginning of each epoch, so that the tightness loss blows up exponen- tially in the number of epochs An easy but unsatisfying solution could be to restrict the scheme to a logarithmic number of epochs, or to only model semi-static corruptions where the adversary has to announce all servers that it wants to corrupt at the beginning of each epoch|,Non-data,75
| Instead, we mod- ify the scheme to apply random-oracle-generated blinding factors to all protocol messages, so that protocol messages do not commit servers to their keys, without ruining the overall functioning of the protocol In the simulation, we can therefore choose a server’s keys only at the moment that it is corrupted and carefully program the random oracle to ensure consistency of previous protocol messages, without having to guess anything upfront Related work Our constructions are closely related to the prime-order- group and bilinear-map instantiations of TPASS by Jarecki et al|,Non-data,75
|’s [18] (which they call “PPSS”) In their construc- tion, each server has a key for a verifiable oblivious pseudo- random function (V-OPRF) For each server, the user en- crypts a share of his secret under a key that is the evaluation of the VOPRF of that server on his password The scheme supports thresholds as well as robustness thanks to the ver- 183ifiability of the V-OPRF|,Non-data,75
| In principle, our protocol could be seen as a variant where all servers jointly evaluate a single, distributed V-OPRF, rather than a separate one each, and where servers can update their key shares for the V-OPRF This is not a straightforward change, however, and doesn’t work for any V-OPRF in general Moreover, whereas their protocol requires LS to perform t V-OPRF verifications (ie|,Non-data,75
|, zero-knowledge proofs or pairings) during login, our protocol doesn’t need any at all, which has a tremendous impact on efficiency Even during account creation, our protocol only involves a single verification Finally, we prove our proto- col secure in the UC framework, as opposed to their game- based model, which offers important security improvements as mentioned earlier 2|,Non-data,75
 PRELIMINARIES Let κ ∈ N be a security parameter A polynomial-time algorithm A is an algorithm that takes κ as an implicit input and that has running time bounded by a polynomial in κ A function ν(κ) is said to be negligible if for every polynomial p(κ) there exists a κ(cid:48) ∈ N st,Non-data,75
| ν(κ) < 1/p(κ) for all κ > κ(cid:48) For concrete security, one could typically use κ = 128 Gap One-More Diffie-Hellman G each time it is called|,Non-data,75
| put a group element h ∈ G, returns hx Let G be a multiplicative group of prime order q > 22κ with generator g The gap one-more Diffie-Hellman assump- tion for G says that no polynomial-time adversary A has a non-negligible advantage of winning the following game On input (g, X) where X ← gx for x ←R Zq , the adversary is given access to the following oracles: • A target oracle T that returns a random target point t ←R • A computational Diffie-Hellman oracle CDH that, on in- • A decisional Diffie-Hellman oracle DDH that, on input group elements h, z, returns 1 if z = hx and returns 0 otherwise|,Non-data,75
| Eventually, A outputs a list of tuples ((t1, z1),    , (tn, zn))|,Non-data,75
| It wins the game if t1,    , tn are different target points gen- for all i = 1, |,Non-data,75
|   , n, and A made less erated by T, zi = tx i than n queries to its CDH oracle The adversary’s advantage (κ) is defined as the probability that A wins the Advgomcdh game|,Non-data,75
| Let G1, G2, Gt be multiplicative groups of prime order q > 22κ with generators g1, g2, gt, respectively, and with an efficiently computable pairing function e : G1 × G2 → Gt that is a non-trivial bilinear map, ie, for all a ∈ G1, b ∈ G2, and x, y ∈ Zq , e(ax, by) = e(a, b)xy, and e(g1, g2) = gt The one-more Diffie-Hellman assumption for (G1, G2) is defined analogously to the game above, but now A is given (g1, g2, X = gx 2 ) as input and the T and CDH oracles gener- ate, respectively raise to the x, elements of G1|,Non-data,75
| There is no DDH oracle, but depending on the type of curve, DDH may be easy via the pairing function A,G The one-more DH [4, 11, 18] and the gap one-more DH [18] were used to prove the security of protocols, as well as non- adaptive variants [19] Cheon [8] presented an attack on the (gap) one-more Diffie-Hellman assumptions that reduces the complexity of recovering x from O( √ and gxd reduced by a factor O( attack by increasing the group order with log d bits is given to the adversary|,Non-data,75
| That is, the security is d), so it is prudent to prevent this q) to O((cid:112)q/d) if d||p−1 √ 1 Upon input (SEND, sid, S, R, m) from S, send (SENT, sid, S, R, ||m||)) to A, generate a private delayed output (SENT, sid, S, m) to R and halt 2 Upon receiving (CORRUPT, sid, P) from A, where P ∈ {S, R}, disclose m to A|,Non-data,75
| Next, if the adversary provides a value m(cid:48), and P = S, and no output has been yet written to R, then output (SENT, sid, S, m(cid:48)) to R and halt Figure 1: The functionality Fsmt is to choose shares s2,  |,Non-data,75
|  , sn ←R G and set s1 ← 1/(cid:81)n of one because(cid:81)n Pi computes its share of unity si ← (cid:81)n j=1,j(cid:54)=i s∆i,j{i,j} = (cid:81)n (cid:81)n that (cid:81)n Combinatorial Secret Sharing A straightforward way to create n-out-of-n secret shares of the unity element in a group G among parties P1,  |,Non-data,75
|  ,Pn i=2 si Each party Pi is given secret share si; they are correct shares i=1 si = 1 An alternative way to compute the same shares is by choosing s{i,j} ←R G for all 1 ≤ i < j ≤ n and handing (s{i,j})n j=1,j(cid:54)=i to Pi for i = 1, |,Non-data,75
|   , n Note that each share s{i,j} is known to parties Pi and Pj|,Non-data,75
| Party (cid:81)n j=1,j(cid:54)=i s∆i,j{i,j}, where ∆i,j = 1 if i < j or ∆i,j = −1 otherwise One can easily see j=i+1 s{i,j} · −1{i,j} = 1 This construction is particularly interesting be- s cause it offers a practical way to non-interactively generate arbitrarily many shares of unity by letting s{i,j} be gener- ated pseudorandomly from a seed that is known to parties Pi and Pj only i=1 si = (cid:81)n i=1 i=1 Secure Message Transmission|,Non-data,75
| The ideal functionality for secure message transmission Fsmt depicted in Figure 1 allows a sender S to send a private and integrity-protected message to a receiver R It is the special case of Canetti’s [5] functionality for leakage function l (m) = ||m|| Pseudo-Random Generators A pseudo-random generator (PRG) is a function PRG : D → R where no polynomial-time adversary can distinguish the output of PRG on a random input from a truly random string The advantage AdvprA,PRG(κ) of an adversary A is defined as(cid:12)(cid:12) Pr[1 = A(y) : x ←R D, y ← PRG(x)] − Pr[1 = A(y) : y ←R R](cid:12)(cid:12)|,Non-data,75
| Message Authentication Codes A message authentication code (MAC) is a function MAC : K × {0, 1}∗ → T that on input a key μ and a message m ∈ {0, 1}∗ returns a tag τ  We say that MAC is unforgeable against chosen-message attack if all polynomial-time adver- saries A have negligible advantage AdvufcmaA,MAC(κ) defined as Pr[τ = MAC(μ, m) ∧ m (cid:54)∈ Q : μ ←R K ; (m, τ ) ←R AMAC(μ,·)], where Q is the set of messages that A submitted to its MAC(μ,·) oracle 3|,Non-data,75
| SECURITY DEFINITION In this section we now formally define our distributed password verification scheme by describing its ideal func- tionality in the universal composability (UC) framework [5] Roughly, a protocol is said to securely realize an ideal func- tionality F if an environment E cannot distinguish whether it is interacting with the real protocol π and a real adver- sary A, or with F and a simulator SIM We denote the 184probability that E outputs 1 in both worlds as RealπE,A(κ) and IdealF E,SIM(κ), respectively First, let’s briefly recall the goal of our distributed pass- word verification system, before we present our ideal func- tionality|,Non-data,75
| In our system, a login server LS is the main ac- cess point where users provide their username uid and pass- word pwd Once a user has created an account for such a username-password combination with the LS, he can subse- quently login by providing the correct username and pass- word again Thus, the login server must be able to verify whether a password attempt pwd(cid:48) matches the stored pass- word pwd or not Our goal is to provide that functionality without introducing a single point of failure that, when cor- rupted, leaks all passwords to the adversary or allows offline attacks against them|,Non-data,75
| Therefore, LS is assisted by n servers S1,    ,Sn running in the back-end|,Non-data,75
| Those servers have to actively contribute to allow the verification of a password verification and thus can refuse whenever they notice sus- picious activity that might be aimed at an online password guessing attack Note that password changes are not explic- itly modeled; these can always be implemented by perform- ing a login under the old password followed by an account creation with the new password (if necessary for a new user- name, eg, containing an increased index)|,Non-data,75
| To model a realistic setting, we consider active and adap- tive corruptions, allowing the adversary to take control of any initially honest party at any time We distinguish be- tween transient and permanent corruptions Transiently corrupted parties do not leak the contents of their backup tape and can recover from an attack by going through a refresh procedure In a permanent corruption, the backup tape is leaked to the adversary, and there is no way to re- cover, meaning that the server is corrupted for all future epochs|,Non-data,75
| As long as the adversary does not corrupt all servers LS,S1,    ,Sn in the same epoch, our distributed password verification scheme protects the stored passwords, meaning that the adversary neither learns the passwords nor is able to perform offline attacks on them|,Non-data,75
| 31 Ideal Functionality The detailed description of our ideal functionality Fdpv is given in Figure 2 When describing our functionality, we use the following writing conventions to reduce repetitive notation: • The functionality ignores all inputs other than INIT un- til the instance is active Once the instance is active, it ignores further calls to INIT|,Non-data,75
| • For all interfaces (except INIT), the ideal functionality only considers the first input for each ssid and for each originating party P Subsequent inputs to the same in- terface for the same ssid coming from the same party P are ignored • At each invocation, the functionality checks that sid = (LS,S1,  |,Non-data,75
|  ,Sn, sid(cid:48)) for server identities LS,S1,   |,Non-data,75
| ,Sn, and sid(cid:48) ∈ {0, 1}∗ Also, whenever we say that the func- tionality receives input from or provides output to LS or Si, we mean LS or Si as specified in the sid  • When we say that the functionality “looks up a record”, we implicitly understand that if the record is not found, F ignores the input and returns control to the environment|,Non-data,75
| • We assume that the session identifier sid and sub-session identifiers ssid given as input to our functionality are glob- ally unique, and that honest parties drop any inputs with (sub)session identifiers that are not locally unique We now also describe the behavior of the main interfaces in a somewhat informal manner to clarify the security prop- erties that our functionality provides Account Creation The creation of a new account for username uid and pass- word pwd is initiated by the login server LS and requires the active approval of all n back-end servers S1, |,Non-data,75
  Sn (if LS is honest) Several account creation (and login) sessions can be run in parallel; a unique sub-session identifier ssid is input to all create and login related interfaces and identifies the respective sub-session,Non-data,75
| 2: The CREATE interface allows the login server to trigger the creation of a new user record (setup, ssid , uid , pwd, proceed , finished ) The two flags, proceed and finished , reflect the status of the record and are both initially set to 0 3: The PROCEED interface can be invoked by the back-end servers Si to signal their willingness to continue an account creation (or login) session, identified by the given ssid  Only if all n servers have given the ok to proceed, the setup (or login) account associated with ssid gets activated for finalization, which is modeled by setting proceed ← 1|,Non-data,75
| Awaiting explicit approval of all servers gives each server the opportunity to throttle or block a session if they detect some suspicious behaviour, which is crucial to prevent offline attacks against the password If the login server is corrupt, an activated account creation (or login) session also increases the global guesses counter, giving the adversary one more password guess (via the in- terface PWDGUESS ) 4: The CREATEOK interface can be invoked by the adversary to allow completion of the setup account for ssid , which is realized by setting finished ← 1 However, if the login server is honest, the adversary can only complete records for those ssid ’s to which all servers have already agreed to proceed|,Non-data,75
| This restriction does not hold for a corrupt login server though, as in the real world, the corrupt LS could always create as many (bogus) user records as he wants Whenever the LS gets honest again, the login will most likely fail for such bogus records though This is modeled accordingly in our RESULT interface where the adversary can always make the verification fail for such forged accounts Login|,Non-data,75
| To verify whether a provided username-password combi- nation uid , pwd(cid:48) is correct, the login server LS can initiate a login request Then, if all servers agree to proceed (us- ing the 3PROCEED interface), the adversary can instruct the ideal functionality to inform the LS whether the provided password attempt pwd(cid:48) matches the setup password pwd stored for uid  Again, each login sub-session is identified via a unique ssid(cid:48)|,Non-data,75
| 5: The LOGIN interface is invoked by the LS on input ssid(cid:48), uid , pwd(cid:48) and triggers the creation of a new login record (login, ssid(cid:48), uid , pwd(cid:48), proceed(cid:48)) with proceed ← 0 6: The RESULT interface allows the adversary to instruct Fdpv to release the result of the password verification to the 1851 Initialization On input (INIT, sid) from login server LS: • Record this instance as active, set guesses ← 0 and create • Send (INIT, sid) to A|,Non-data,75
| a record (corrupt, TC, PC) with TC, PC ← ∅ 2 Account Creation Request On input (CREATE, sid, ssid, uid, pwd) from login server LS: • If LS is honest, and a setup record for uid exists, then ignore this input|,Non-data,75
| • Create a new record (setup, ssid, uid, pwd, proceed, finished) with proceed ← 0 and finished ← 0 • Send (CREATE, sid, ssid, uid) to A 3 Server Proceed (used in account creation and login)|,Non-data,75
| On input (PROCEED, sid, ssid) from a server Si: • Look up setup or login record for ssid • If PROCEED messages from all n servers S1,   |,Non-data,75
| , Sn have been received for ssid, update the login or setup record for ssid by setting proceed ← 1, and if LS is corrupt, set guesses ← guesses + 1 • Send (PROCEED, sid, ssid, Si) to A 4 Creation Result|,Non-data,75
| On input (CREATEOK, sid, ssid) from A: • Look up setup record (setup, ssid, uid, pwd, proceed, • If the LS is honest, only proceed if proceed = 1 • Update the record by setting finished ← 1 and output finished) for ssid (CREATEOK, sid, ssid) to LS 5|,Non-data,75
| Login Request On input (LOGIN, sid, ssid(cid:48), uid, pwd(cid:48)) from LS: • Create a new record (login, ssid(cid:48), uid, pwd(cid:48), proceed(cid:48)) with proceed(cid:48) ← 0 and send (LOGIN, sid, ssid(cid:48), uid) to A 6|,Non-data,75
| Login Result On input (RESULT, sid, ssid(cid:48), fail) from adversary A: • Look up login record (login, ssid(cid:48), uid, pwd(cid:48), proceed(cid:48)) record for (setup, ssid, uid, pwd, proceed, finished) for uid Ignore this input if proceed(cid:48) = 0 or finished = 0 corresponding ssid(cid:48) setup and the • If pwd (cid:54)= pwd(cid:48), or if fail = 1 and at least one server from S1, |,Non-data,75
|   , Sn is corrupt or proceed = 0, then set pwdok ← 0 Else, set pwdok ← 1|,Non-data,75
| • If LS is corrupt, set guesses ← guesses − 1 • Delete the login record for ssid(cid:48) and send a delayed output (RESULT, sid, ssid(cid:48), pwdok ) to LS 7 SSID Timeout|,Non-data,75
| On input (TIMEOUT, sid, ssid) from LS: • If a login record for ssid exists, delete the record • If a setup record (setup, ssid, uid, pwd, proceed, finished) for ssid and with finished = 0 exists, then delete the record 8 Server Corruption|,Non-data,75
| On input (CORRUPT, sid, S, mode) from A, where S ∈ {LS, S1,    , Sn} and mode ∈ {trans, perm}: • Look up record (corrupt, TC, PC)|,Non-data,75
