 6303. DATASETS Our Internet-wide study of key sharing in the HTTPS ecosystem is driven by four datasets: SSL certificates We use SSL certificates from full IPv4 scans as the basis of our measurements,Data,0
| Our SSL scans [30] also contain information on the IP address(es) that advertised each certificate. To obtain in- formation about the entity that controls this IP address, we use full IPv4 reverse DNS scans [29] that are also conducted by Rapid7|,Data,0
| Each AS is assigned an AS Number (ASN): for example, MIT is AS 3 and the Chicago Public Schools are AS 1416 [26]. CAIDA collects and publishes mappings between IP addresses and ASNs via their Route- Views datasets [7]|,Data,0
| For example, AT&T owns 160 unique ASNs. To aggregate these, we use CAIDA’s AS- to-Organization dataset [8] to group together ASes owned by the same organization|,Data,0
| For that, we rely on WHOIS [12], a protocol for querying domain registrars to obtain data on the domain owner. In practice, WHOIS data often contains fields such as the con- tact information for the owner of the domain, the contact for technical issues, where to send abuse complaints, and so on|,Data,0
| Here, we expand upon these prior findings by evaluating whether there is a correlation between centralized management and the quality of the keys chosen. Figure 13 compares several different features of self- managed and outsourced certificates across our entire cor- pus of leaf certificates (3,275,635 self-managed and 1,781,962 outsourced): (a) Key lengths in self-managed certificates are nearly identical to those managed by third-party hosting providers|,Data,0
|1 Combining Packet Capture (PCAP) Files The data set used in this study is a combination of the packet capture files obtained from two main sources. First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour|,Data,1
| First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour. The normal and non-malicious data is obtained from PREDICT internet data set repository [18] under the category of “DARPA Scalable Network Monitoring (SNM) Program Traffic”|,Data,1
| The data collection was performed during April 2016 using ZGrab, an application-layer scanner that operates with ZMap [15]. In the first phase, we performed an Internet-wide scan of all IPv4 addresses on port 500 to determine which hosts were configured 16This defect was corrected quite recently, years after the version of OpenSSL ScreenOS uses was written|,Data,6
|, download from an external source). Running on 71,000 articles collected from 45 leading technical blogs, this new approach demonstrates a remarkable performance: it gener- ated 900K OpenIOC items with a precision of 95% and a coverage over 90%, which is way beyond what the state-of-the-art NLP tech- nique and industry IOC tool can achieve, at a speed of thousands of articles per hour|,Data,7
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
 5. ANALYSIS AND FINDINGS In the following we use the extensive documentation of the 61 minimal exploits to provide insight into how attackers use specific vulnerabilities and features of the Java platform to implement their attacks,Data,11
| We run our event analysis on the top 100 free applications in the Android application store to determine how often this happens. In total, our analysis finds 1060 errors across 88 of the top 100 applications (10|,Data,12
| To our knowledge, AUTOREB is the first work that explores the user review information and utilizes the review semantics to predict the risky behaviors at both review-level and app-level. We crawled a real-world dataset of 2, 614, 186 users, 12, 783 apps and 13, 129, 783 reviews from Google play, and use it to comprehensively evaluate AUTOREB|,Data,14
| 4.1 Data collection For each team, we collected a variety of observed and self- reported data|,Data,16
| To demonstrate this, we scraped greatfire.org for websites in the top 1000 Alexa websites that are blocked by the GFW|,Data,18
| (cid:15) Identifying New Vulnerabilities. Our tool successfully an- alyzed 1,591 service interfaces of all the 80 system services in Android 5|,Data,19
| To understand the scope and magnitude of this new XARA threat, we developed an ana- lyzer for automatically inspecting Apple apps’ binaries to deter- mine their susceptibility to the XARA threat, that is, whether they perform security checks when using vulnerable resource-sharing mechanisms and IPC channels, a necessary step that has never been made clear by Apple. In our study, we ran the analyzer on 1,612 most popular MAC apps and 200 iOS apps, and found that more than 88|,Data,24
| To assist software developers (or secu- rity analysts) in tracking down a memory corruption vulnerability, CREDAL also performs analysis and highlights the code fragments corresponding to data corruption. To demonstrate the utility of CREDAL, we use it to analyze 80 crashes corresponding to 73 memory corruption vulnerabilities archived in Offensive Security Exploit Database|,Data,25
| These techniques may be applicable in other scenarios. We implemented and evaluated the attacks against the popular Gmail and Bing services, in several environments and ethical experiments, taking careful, IRB-approved mea- sures to avoid exposure of personal information|,Data,26
|, CSPAutoGen can handle all the inline and dynamic scripts. We have implemented a prototype of CSPAutoGen, and our eval- uation shows that CSPAutoGen can correctly render all the Alexa Top 50 websites|,Data,27
| 5. EXPERIMENTAL RESULTS This section reports on our evaluation of the moments ac- countant, and results on two popular image datasets: MNIST and CIFAR-10|,Data,28
| 6.1 Mobility Trace Dataset We use the CRAWDAD dataset roma/taxi [2, 3] for our simu- lations|,Data,31
 6.1 Evaluation We evaluated the performance of Σoφoς using 4 data sets of increasing size and also the English Wikipedia,Data,33
|1 Datasets, Metrics, Competitors & Settings Datasets. We test EpicRec on two real-world datasets: MovieLens1: a movie rating dataset collected by the Grou- pLens Research Project at the University of Minnesota through the website movielens|,Data,36
| 1 http://grouplens.org/datasets/movielens 188Yelp2: a business rating data provided by RecSys Chal- lenge 2013, in which Yelp reviews, businesses and users are collected at Phoenix, AZ metropolitan area|,Data,36
| The number of movie categories is 18. We use the MovieLens- 1M, with 1000,209 ratings from 6,040 users on 3,883 movies|,Data,36
| Our goal is to show that an ad- versary can insert an unbounded number of Sybil identities in the SybilLimit protocol, breaking its security guarantees. For our evaluation, we consider a real-world Facebook inter- action graph from the New Orleans regional network [28]|,Data,38
| We utilize these papers to extract Android malware behaviors and to construct the semantic network. From the electronic proceedings distributed to conference participants, we collect the papers from the IEEE Sympo- sium on Security and Privacy (S&P’08–S&P’15)4, the Com- puter Security Foundations Symposium (CSF’00–CSF’14), and USENIX Security (Sec’11)|,Data,39
 We conduct experiments on two publicly available set-valued datasets. • AOL search log dataset [1],Data,45
 90% of the users have fewer than 84 keywords in their logs. • Kosarak dataset [2],Data,45
 We select one month of data for our study. The data logs we used are col- lected from more than 30 machines with various server mod- els and operating systems,Data,46
| This paper rigorously investigates how users’ security beliefs, knowledge, and demographics corre- late with their sources of security advice, and how all these factors influence security behaviors. Using a carefully pre- tested, U|,Data,48
 We have ported Valgrind to iOS and implemented a prototype of iRiS on top of it. We evaluated iRiS with 2019 applications from the official App Store,Data,54
| from manufacturing equipment, as shown in Figure 1. We capture the relevant sensor data by deliberately or accidentally placing an attack-enabled phone close to, on top of, or inside a piece of manu- facturing equipment while the machinery is fabricating the target object|,Data,55
| Our new metric helps us compare in a fair way previously proposed attack-detection mechanisms. (ii) We compare previous attack-detection proposals across three di↵erent experimental settings: a) a testbed operating real-world systems, b) network data we collected from an operational large-scale Supervisory Control and Data Acqui- sition (SCADA) system that manages more than 100 Pro- grammable Logic Controllers (PLCs), and c) simulations|,Data,57
| Evaluation. We ran Oyente on 19, 366 smart contracts from the first 1, 460, 000 blocks in Ethereum network and found that 8, 833 contracts potentially have the documented bugs|,Data,58
| First, we consolidate the eight origin-exposing vectors into one auto- mated origin-exposing system called Cloudpiercer. Then, we assemble a list of clients from five CBSP companies by studying their DNS configurations and obtaining their adop- tion rate across the Alexa top 1 million websites|,Data,59
| The vast majority of them were exposed through their A record, indicating a brief dis- abling of the protection system. SSL certificate exposure In order to find IP addresses hosting SSL certificates associ- ated with the domains in the evaluation set, we made use of the publicly available data of Rapid7’s Project Sonar [42]|,Data,59
| 4. LARGE-SCALE ANALYSIS To assess the magnitude of the origin-exposure problem, we conduct a large-scale analysis in which we attempt to uncover the origin of CBSP-protected domains|,Data,59
|1 Dataset Description The dataset was first presented and used by Keller et al. in [23], and is publicly available in the gene expression om- nibus (GEO) database under reference GSE61741|,Data,61
| Although the cost of stor- age and processing have diminished, the cost of maintaining reliable infrastructure for transaction logs is still noticeable. Figure 1: A plot of transaction fee versus frequency for 1 million transactions in May 2015|,Data,65
| To estimate the cost of producing the preprocessing data (multiplication triples, random bits etc.), we used figures from the recent MASCOT protocol [31], which uses OT ex- tensions to obtain what are currently the best reported triple generation times with active security|,Data,67
| In this section, we validate whether the smartphone’s acoustic data can be utilized to deduce the movements. To conduct the validation, we implement an application on Nexus 5 (Android OS v6|,Data,68
| As seen in Table 4, we found that about half of the servers in Alexa’s top 10 support a large number of requests without rekeying. For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client|,Data,72
| For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client. We identified 11483 different HTTPS servers11, and found that 226 of them (1|,Data,72
| In this paper, we study the possible techniques to detect and measure this fraud and evaluate the real impact of OTT bypass on a small European country. For this, we performed more than 15,000 test calls during 8 months and conducted a user study with more than 8,000 users|,Data,78
|, the server cannot learn their relative order) after some number of queries are performed over real-world data. Specifically, we ran an experiment where we inserted over 2 million public employee salary figures from [1] and then performed 1000 random range queries|,Data,79
| In this study, we are interested in finding answers to security- and privacy-related questions about libraries, such as “How prevalent are third- party libraries in the top apps and how up-to-date are the library versions?”, “Do app developers update the libs included in their apps and how quickly do they update?”, or “How prevalent are vulnerabilities identified in prior research [28, 9] in libraries and how many apps are affected?” To answer these questions, we first built a comprehensive repository of third-party libraries and applications (see Section 5). Our library set contains 164 libraries of different categories (Ad- vertising, Cloud,|,Data,84
|) and a total of 2,065 versions. We then collected and tracked the version histories for the top 50 apps of each category on Play between Sep 2015 and July 2016, accumulating to 96,995 packages from 3,590 apps|,Data,84
|6.1, we found in our sample set 360 affected packages from 23 distinct apps, when only considering exact library matches|,Data,84
|15 for Android, which contained an account hijacking vulnerability, on 06/11/2014. In the histories of our sample set apps, we discovered, in total, 394 affected packages from 51 distinct apps, when only considering packages with exact matches of the vulner- able lib version|,Data,84
| We used LibScout to detect the affected application packages in our data set. In total 2,667 app versions of 296 distinct apps with a cumu- lative install-base of 3|,Data,84
| We observed that there is a significant variance in ACFG size. To reduce the sampling bias, we first collect a dataset which covers ACFGs of different functions from various architec- tures|,Data,89
| This dataset was used for base- line comparison, and all functions in this dataset has known ground truth for metric validation. We prepared this dataset using BusyBox (v1|,Data,89
| Dataset II – Public dataset. Recent work such as Pewny et al [45] and Eschweiler et al [23] used the same public dataset based upon two publicly-available firmware images for baseline comparison [7, 8]|,Data,89
| Dataset III – Firmware image dataset. This dataset of 33,045 firmware images was collected from the wild|,Data,89
| As a result, we created a freely available vulnerability database for this effort and for the broader research community. To build this database, we mined official software websites to collect lists of vulnerabilities with the corresponding CVE num- bers|,Data,89
| We selected OpenSSL for demonstration, since it is widely used in IoT devices. The resulting vulnerability database includes 154 vulnerable functions|,Data,89
| Roughly speaking, our measurement methods can be divided into two kinds: those that could be fully automated and scaled eas- ily, and those that required some manual interaction. For the latter, we used a set of 302938 major email providers and email genera- tors, while for the former, we used a much larger set of a million popular providers occurring in the Adobe leak and the Alexa top million Web sites (as potential email generators)|,Data,90
1.2 Provider List We created the set of popular email providers based on the top 1 million email address domains occurring in the leaked Adobe user data set of September 2013,Data,90
| Using a combination of mea- surement techniques, we determine whether major providers sup- ports TLS at each point in their email message path, and whether they support SPF and DKIM on incoming and outgoing mail. We found that while more than half of the top 20,000 receiving MTAs supported TLS, and support for TLS is increasing, servers do not check certificates, opening the Internet email system up to man- in-the-middle eavesdropping attacks|,Data,90
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
| 3.1 Datasets We use two major types of datasets: (1) packet-level traffic traces collected at various locations in a campus network, and (2) packet-level traces for Tor Pluggable Transport traffic collected in controlled environments|,Data,91
 Evaluation: local mixing time in social graphs. We use 10 various large-scale real-world social network topolo- gies that mainly come from the Stanford Large Network Dataset Collection [23] and other sources [45] to evaluate the local mixing time for nodes in social graphs,Data,92
| Feature Functions and Weights. To learn all feature functions and weights, we downloaded 1784 non-obfuscated Android applications from F-Droid [3], a popular repository for open-source Android applications|,Data,93
2.2 Experiments with Malware Samples We randomly selected one sample from each of the 49 mal- ware families reported in [40],Data,93
1_r1). Apps in our dataset used for the case study are downloaded from the Google official market (Google Play) in May 2016,Data,95
| • Using SInspector, we perform the first study of Unix domain sockets on Android, including the categoriza- tion of usage, existing security measures being en- forced, and common flaws and security implications. We analyze 14,644 apps and 60 system daemons, find- ing that 45 apps, as well as 9 system daemons, have vulnerabilities, some of which are very serious|,Data,98
| We presented SInspector, a tool for discovering potential security vulnerabilities through the process of identifying socket addresses, detecting authen- tication checks, and performing data flow analysis on na- 90tive code. We analyzed 14,644 Android apps and 60 system daemons, finding that some apps, as well as certain system daemons, suffer from serious vulnerabilities, including root privilege escalation, arbitrary file access, and factory reset- ting|,Data,98
| Our results show that many of our attacks succeed with a 100% chance such that the Sound-Proof cor- relation algorithm will accept the attacked audio samples as valid. Third, we collect general population statistics via an online sur- vey to determine the phone usage habits relevant to our attacks|,Data,100
 We find that the larger width of integer types and the increased amount of addressable memory introduce previously non-existent vulnerabilities that often lie dormant in program code. We empirically evaluate the prevalence of these flaws on the source code of Debian stable (“Jessie”) and 200 popular open-source projects hosted on GitHub,Data,104
| We have applied UniSan to the latest Linux kernel and Android kernel and found that UniSan can successfully prevent 43 known uninitialized data leaks, as well as many new ones. In particular, 19 of the new data leak vulnerabilities in the latest kernels have been confirmed by the Linux community and Google|,Data,107
| This allows us to prevent replay attacks, which are possibly the most applicable attack vectors against biometric authentication. Using a gaze tracking device, we build a prototype of our system and perform a series of systematic user experiments with 30 participants from the general public|,Data,108
| If two commits were blamed for the same amount of lines, blame both. Our heuristic maps the 718 CVEs of our dataset to 640 VCCs|,Data,109
| However, improving our blame heuristics further is an interesting avenue for future research. Apart from the 640 VCCs, we have a large set of 169,502 unclassified commits|,Data,109
|9 The SVM detected a high amount of excep- tions, a high number of changed code, inline ASM code, and variables containing user input such as __input and user. 6As previously mentioned we use the years 2011–2014 as the test dataset, since we have ground truth data on which to base the discussion|,Data,109
| When given a source file, Flawfinder returns lines with suspected vul- nerabilities. It offers a short explanation of the finding as well as a link to the Common Weakness Enumeration (CVE) database|,Data,109
| The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database|,Data,109
| Our results show that our approach significantly outperforms the vulner- ability finder Flawfinder. We created a large test database containing 66 C and C++ project with 170,860 commits on which to evaluate and compare our approach|,Data,109
 VoiceLive takes advantages of the user’s unique vocal system and high quality stereo recording of smartphones. • We conduct extensive experiments with 12 participants and three different types of phones under various ex- perimental settings,Data,111
| To test if WebCapsule can successfully record and subsequently replay real-world phishing attacks, we proceeded as follows, us- ing Chromium on our desktop machine. We selected a large and diverse set of recently reported phishing web pages from Phish- Tank8|,Data,112
| 2.4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime|,Data,113
|4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime. The dataset Cal represents the latitude and longitude of about 21,000 intersections in the California road network1 (also used by Mavroforakis et al|,Data,113
294258. The dataset SpitzLoc consists of latitude and longitude coordinates tracking the movement of German Green party politician Malte Spitz over six months,Data,113
| In this section, we aim to explore whether the differences of keystroke wave- forms are large enough to be used for recognizing different keys inputs in the real-world setting. We collected training and testing data from 10 volunteers|,Data,114
 B. Real Attacks MAD uniformly detects attacks more quickly than the PAD; we use the former method to detect the presence of an attack in real Internet traces3,Data,119
 III. DATA SET  changes  The data used was the PREDICT ID USC-Lander!  (- 60  The total  were DNS attack packets,Data,120
|395326000  files IPs. There are total 59,928,920 packet counts out of which there was a total of  DoS_DNS_amplification-20130617 (2013-06-17) (2013-06-17) with anonymized million) 358019 DNS packets|,Data,120
| The maximum number of unique hosts per day we measured was 106,000. To understand these differences, we compared the observations from our network monitor to data collected from DShield (www|,Data,121
| 3.1 From our own transactions We engaged in 344 transactions with a wide variety of services, listed in Table 1, including mining pools, wallet services, bank ex- changes, non-bank exchanges, vendors, gambling sites, and mis- cellaneous services|,Data,122
| Wallets. We kept money with most of the major wallet services (10 in total), and made multiple deposit and withdrawal transac- Bank exchanges|,Data,122
|, in which the exchange rate is not fixed) also function as banks. As such, we tagged these services just as we did the wallets: by depositing into and withdrawing from our accounts (but rarely par- ticipating in any actual currency exchange)|,Data,122
|info/tags, including both addresses provided in users’ signatures for Bitcoin forums, as well as self-submitted tags. We collected all of these tags — over 5,000 in total — keeping in mind that the ones that were not self-submitted (and even the ones that were) could be regarded as less reliable than the ones we collected ourselves|,Data,122
| 3.1 Data analysis overview We use three data sets, summarized in Table 1|,Data,123
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
| We also describe our application of the technique to the IPv6 interface-level graph captured by CAIDA’s Archipelago (Ark) infrastructure [14] for March 2013. The graph consists of all the 52,986 IPv6 interfaces numbered within the 2000::/3 unicast prefix captured from all 27 Ark vantage points (VPs) with IPv6 connectivity|,Data,125
| cause the counters of distinct routers to diverge, and (4) confirm aliases with pairwise probing. Given the absence of velocity in ID counters and the large probes required for the technique to work, we probe at a low rate of 20pps from a single VP, producing 26Kbps of traffic|,Data,125
| 3. METHODOLOGY In this section, we describe the design of our experiment and our data collection methodology, as well as the mitigating steps and proactive measurements we conducted to ensure a minimal im- pact of our covering routes|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| Our IPv6 network telescope results suggest sev- eral important differences (and some similarities) compared to that body of work. To produce a more recent and valid comparison, we analyzed a single week of IPv4 background radiation captured during the course of our ongoing IPv6 packet capture|,Data,126
| 4. DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1|,Data,127
| DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1. Our primary dataset consists of changes made to the |,Data,127
| domains, (2) the removal of existing domains, and (3) changes to existing domains in terms of revisions to their associated name- servers. Our data includes captures of the DNZA files as recorded every five minutes, time periods we refer to as epochs|,Data,127
| Since we lack comprehensive ground truth regarding the ultimate use of domains, to this end we use two proxies: subsequent appearance of a newly registered do- main in: (1) an email spam campaign, or (2) a domain blacklist. For the first of these, we operated a spam trap, i|,Data,127
|com), by restricting our focus to domains recently registered (March–July 2012) we can filter down the do- mains appearing in the spam trap to those very likely used for spam- ming. For the second, we subscribed to three major DNS blacklists, URIBL, SURBL, and Spamhaus DBL|,Data,127
| In this paper, we examine the effectiveness of these inter- ventions in the context of an understudied market niche, counterfeit luxury goods. Using eight months of empirical crawled data, we identify 52 distinct SEO campaigns, document how well they are able to place search results for sixteen luxury brands, how this ca- pability impacts the dynamics of their order volumes and how well existing interventions undermine this business when employed|,Data,128
| For a small number of stores, we were also able to collect user traffic data that directly measures an SEO campaign’s effectiveness in attracting customers to their stores. Specifically, we were able to periodically collect AWStats data for 647 storefronts in 12 cam- paigns|,Data,128
| One issue that undermines coverage is that Google only labels the root of a Web site as “hacked”, and does not label search results that link to sub-pages within the same root domain. In the PSR data set, we found 68,193 “hacked” search results|,Data,128
| We begin by exam- ining the properties of individual darknets and in particular the behavior of source IP addresses. We provide these char- acterizations by looking at data from 14 darknet monitors ranging in size from a /25 monitor to a /17 monitor over a period of 10 days between August 18, 2004 and August 28, 2004|,Data,129
| Figure 10: The number of darknets (of 31) reporting a port in the top 10 ports over a day, week, and month time frame. The analysis is performed for the top 10 destination ports over a day, top 10 destination ports over a week, and top 10 destination ports over a month|,Data,129
| 3.6 Datasets This paper uses DNS datasets from three authorities: one national-level top-level domain, operators of two root servers as shown in Table 1|,Data,130
 JP-DNS operates the .jp country code domain for Japan; we have data from all seven of their anycast sites,Data,130
|) part of the 2014 DITL collection [16] (for B-Root, shortly after 2014 DITL). We also use data for M-Root’s 2015 DITL collection (§ 4|,Data,130
 These root datasets are available to re- searchers through DNS-OARC. For longitudinal analysis we draw on 9 months of data taken at the M-Root server,Data,130
| However, we treat the union of these classes together. We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness|,Data,131
| We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness. The dataset consists of all echo requests that were sent as part of the surveys in this period, as well as all echo responses that were received|,Data,131
|, “host unreachable”); we ignore all probes as- sociated with such responses since the latency of ICMP error responses is not relevant. In later sections, we will complement this dataset with results from Zmap [5] and additional experiments includ- ing more frequent probing with Scamper [13] and Scrip- troute [22]|,Data,131
| 3.2 Milking 3 Methodology To collect the information needed to cluster servers into oper- ations, we have built an infrastructure to track individual exploit servers over time, periodically collecting and classi- fying the malware they distribute|,Data,132
 2. We receive feeds of drive-by download URLs (Sect,Data,132
 2. CHARACTERISTICS OF CHECK-INS We use three different datasets that capture human mobility,Data,133
 First we consider two online location-based social networks. We col- lected all the public check-in data between Feb,Data,133
| There are 196,591 nodes, 950,327 edges in Gowalla and 58,228 nodes, 214,078 edges in Brightkite. To ensure that our observations on human movement are not specific to data based on check-ins from location-based social net- works, we also include a dataset of cell phone location trace data|,Data,133
| Backscatter DDoS is a commonly seen behaviour in darknets where the attacker uses simultaneous bots to generate the actual attack packets to reach the targeted (original) victim. In our study, five publicly available network traffic datasets from CAIDA’s archives are employed|,Data,134
| Datasets Employed In this research, five publicly available real-life network traffic traces (datasets) from CAIDA’s archives are employed. Three of them, which were captured by a passive darknet in 2007, 2008 and 2012 [27][26][28], namely UCSD Network Telescope [21], include mostly one-way malicious traffic while the remaining ones collected in 2008 [29] and 2014 [30] via CAIDA’s Internet backbone links include only normal traffic|,Data,134
| 3 Approach This section presents our approach for the evalua- tion of reputation based blacklists. We evaluated the blacklists by deploying them in a large academic net- work of over 7,000 hosts|,Data,135
| This was a preliminary step to preventing inexperienced and non-serious workers from participating in our survey. Our survey is based on the participants’ actual check-ins on Foursquare posted over the last 24 months (that we collected through a specific application we developed), and it requires a significant amount of time to complete (30-45 minutes)|,Data,136
| The third phase of worm activ- ity is the persistence phase which for the Blaster worm has continued through 2004. In this one-week period of measurement, the IMS system observed over 286,000 unique IP addresses displaying the characteristics of Blaster activity|,Data,137
| published a study in 2011 that focused on the dynamics of leaf cer- tificates and the distribution of certificates among IP addresses, and attempted to roughly classify the overall quality of served certifi- cates. The study was based on regular scans of the Alexa Top 1 Mil- lion Domains [1] and through passive monitoring of TLS traffic on the Munich Scientific Research Network [17]|,Data,138
| Our study is founded on what is, to the best of our knowledge, the most comprehensive dataset of the HTTPS ecosystem to date. Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443|,Data,138
| Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443. Over the course of 14 months, we completed upwards of 400 billion SYN probes and 2|,Data,138
| Content Provider e Service Provider v i t c e p s r e P Content Consumer Addressing Prerequisite IP Functions Routing Naming A1: Address Allocation; A2: Address Advertisement N1: Nameservers; R1: Server Readiness N2: Resolvers N3: Queries A2: Address Advertisement; T1: Topology End-to-End Reachability R1: Server Readiness Operational Characteristics Usage Profile Performance U3: Transition Technologies U1: Traffic Volume; U3: Transition Technologies P1: Network RTT R2: Client Readiness U2: Application Mix; N3: Queries Table 2: Dataset summary showing the time period, scale, and public or new status of the datasets we analyzed. Dataset RIR Address Allocations Routing: Route Views Routing: RIPE Google IPv6 Client Adoption Verisign TLD Zone Files CAIDA Ark Performance Data Arbor Networks ISP Traffic Data Verisign TLD Packets: IPv4 Verisign TLD Packets: IPv6 Alexa Top Host Probing Time Period Metrics Jan 2004 – Jan 2014 A1 Jan 2004 – Jan 2014 A2, T1 Jan 2004 – Jan 2014 A2, T1 Sep 2008 – Dec 2013 R2, U3 Apr 2007 – Jan 2014 N1 P1 Dec 2008 – Dec 2013 U1, U2, U3 Mar 2010 – Dec 2013 Jun 2011 – Dec 2013 N2, N3 N2, N3 Jun 2011 – Dec 2013 Apr 2011 – Dec 2013 R1 Recent Scale ≈18K allocation snapshots (5 daily) 45,271 BGP table snapshots millions of daily global samples daily snapshots of ≈2|,Data,139
com & .net) ≈10 million IPs probed daily ≈33-50% of global Internet traffic; 2013 daily median: 50 terabits/sec (avg,Data,139
| To put the IPv6 allocation data in context, Figure 1 also shows IPv4 prefix allocations over the same period. The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled|,Data,139
| There were less than 30 IPv6 prefixes al- located per month prior to 2007, generally increasing thereafter. In the past several years, we typically find more than 300 prefixes allocated per month, with a high point of 470 prefix allocations in February 2011|,Data,139
| The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled. 1 Overall, we find nearly 69K IPv4 prefix allocations at the beginning of our dataset and just over 136K at the end|,Data,139
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| Table 1 shows the datasets we use in our paper. We use two ICMP surveys taken by USC [12]: IT17ws and IT16ws; IT17ws is the main dataset used in this paper, while we use IT16ws for validation in Section 6|,Data,142
2. We collected VUSC s at our enterprise in order to compare our inferences with network operators as discussed in Section 6,Data,142
| # of Data-Oriented Attacks gives the number of attacks generated by FLOWSTITCH, includ- ing privilege escalation attacks and information leakage attacks. FLOWSTITCH generates 19 data-oriented attacks from 8 vulnerable programs|,Data,144
| Third, this method is not specific to C or C++, and can be applied to any programming language. We collected C++ source of thousands of contestants from the annual international competition “Google Code Jam”|,Data,145
| Finally, we analyze various attributes of programmers, types of programming tasks, and types of features that appear to influence the success of attribution. We identified the most important 928 fea- tures out of 120,000; 44% of them are syntactic, 1% are layout-based and the rest of the features are lexical|,Data,145
|3.1ScalingWecollectedalargerdatasetof1,600programmersfromvariousyears|,Data,145
| ) s y a D n i (    e m T i  7  6  5  4  3  2  1  10  20  30  40  50  60  70  80  90 Time Before Accounts Suspension Number of IP Addresses 2 Motivation: Analysis of Malicious Activ- ity on a Webmail Service We want to understand the way in which cybercrimi- nals abuse accounts on online services, to identify weak points that we could leverage for detection. To this end, we observed the email-sending activity on a large web- mail service|,Data,147
| Following accepted frameworks for qualitative research [18, 30, 35], we focus closely on a small number of participants. We interviewed 15 journalists employed in a range of well-respected journalistic institutions in the United States and France, analyzing these interviews using a grounded theory approach [18, 30]|,Data,146
| 3.1 Datasets We examine 13,345 passwords from four sets created under composition policies ranging from the typical to the currently less common to understand the suc- cess of password-guessing approaches against passwords of different characteristics|,Data,149
| Had we used any major password leak, their analysts would have already been familiar with most or all of the passwords contained in the leak, biasing results. The passwords in these sets were collected using Ama- zon’s Mechanical Turk crowdsourcing service|,Data,149
| The decision for or against pinning is always a trade- off between increasing security and keeping mainte- nance efforts at an acceptable level. In this paper, we present an extensive study on the applicability of pinning for non-browser software by analyzing 639,283 Android apps|,Data,152
| Therefore, we instrument telemetry data from a popular anti-virus software provider. We evaluate the update behaviour of 871,911 unique users from January 2014 to December 2014 and find that only 50% of the users update to a new app version within the first week after release|,Data,152
| Developer View Although pinning is only ap- plicable in relatively few cases, the nominal-actual comparison leaves room for improvement. We there- fore collected feedback from 45 developers of apps for which we would recommend pinning|,Data,152
| Section 4). Altogether we found 20,020,535 calls to network related API calls (cf|,Data,152
| Instability of the routes to the sensor address space can also result in reachability problems, especially given that route flap damping can be triggered during convergence to suppress unstable routes [9]. Using the BGP updates data from RouteViews BGP monitor, we studied the availability of the routes to the sensor blocks in our de- ployment from a large set of ASes|,Data,154
| This section probes these differences using three successively more specific views of traffic to a network of distributed blackhole sensors. The data was recorded over a one month period with SYN responders on TCP port 135, 445, 4444, and 9996 across all sen- sors|,Data,154
|  V. EXPERIMENT RESULTS  In this section, we mainly focus on how our router-to-AS Mapping method and other baseline methods behave on global router-level topology, as discussed above, we use PeeringDB data as ground truth, and apply clustering method on global topology based on CAIDA ITDK project|,Data,155
| It describes the properties that a dataset should have in order to be used for comparison purposes. The dataset used in the paper includes an IRC-based Botnet attack1, but the bot used for the attack was developed by the authors and therefore it may not represent a real botnet behavior|,Data,156
| This dataset may be downloaded with authorization. The Protected Repository for the Defense of Infrastructure Against Cyber Threats (PRE- DICT) indexed three Botnet datasets2 until May 16th, 2013|,Data,156
 None of them are labeled. A custom botnet dataset was created to verify five P2P botnet detection algorithms in Saad et al,Data,156
| Unfortunately, there is only one infected machine for each type of botnet, therefore no synchronization analysis can be done. The Traffic Laboratory at Ericsson Research created a normal dataset that was used in Saad et al|,Data,156
 This is the only normal dataset that is labeled inside the pcap file. A considerable amount of malware traffic in pcap format was published in the Contagio blog9,Data,156
| But since each scenario includes only one infected computer, it should be possible to label them. Another dataset with malware logs and benign logs was collected in NexGinRC (2013)|,Data,156
 Access to this dataset may be granted upon request10. The last dataset analyzed is currently created by the MAWI project described in Cho et al,Data,156
| Methodology and datasets We deployed Paris Traceroute with its Multipath Detection Algorithm (MDA) [29] enabled in 90 PlanetLab nodes. We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]|,Data,158
| We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]. Our dataset contains more than 900 thousand IP-level (multi)routes and 324,313 IP addresses|,Data,158
1 3.1 Address Allocation and BGP Data We analyzed BGP announcements captured by all collectors (24 collectors peering with 184 peers) of the Routeviews [3] and RIPE RIS [52] projects,Data,159
| For each /24 block, we computed the maximum number of peers that saw it reachable at any time within the full observation period of 92 days. To determine which address blocks are available for assignment, we used a dataset compiled by Geoff Hus- ton [23], which merges the extended delegation files from the 5 RIRs [4, 6, 7, 41, 51] with IANA’s published registries [31–36]|,Data,159
| SWITCH. We collected unsampled NetFlow records from all the border routers of SWITCH, a national aca- demic backbone network serving 46 single-homed uni- versities and research institutes in Switzerland [55]|,Data,159
| R-ISP. We collected per-flow logs from a vantage point monitoring traffic of about 25,000 residential ADSL customers of a major European ISP [21]|,Data,159
 UCSD-NT. We collected full packet traces from the /8 network telescope operated at the University of Cal- ifornia San Diego [1],Data,159
| IXP. Our fourth VP is a large European IXP inter- connecting more than 490 networks, exchanging more than 400 PB monthly [5]|,Data,159
|3 Active Measurements ISI. We used the ISI Internet Census dataset it55w- 20130723 [37], obtained by probing the routed IPv4 address space with ICMP echo requests and retaining only those probes that received an ICMP echo reply from an address that matched the one probed (as rec- ommended [38])|,Data,159
| HTTP. We extracted IP addresses from logs of Project Sonar’s HTTP (TCP port 80) scan of the entire IPv4 address space on October 29, 2013 [24]|,Data,159
| Definitions of graph parameters measuring metric tree-likeness of a graph, as well as notions and notations local to a section, are given in appropriate sections. 3 Datasets Our datasets come from different domains like Internet measurements, biological datasets, web graphs, social and collaboration networks|,Data,160
| The experiments were executed as follows. Traces were col- lected by using ICMP, UDP, and TCP Traceroute to probe the paths to a set of 100 destination websites from a source located on the Pennsylvania State University, University Park campus|,Data,161
| For UDP and TCP Traceroute, traces were collected using the default destination port numbers. We also collected traces using other ports and observed similar results|,Data,161
| Realistic Networks Here we compare the merged topologies produced by iTop, MN, and Isomap for realistic topologies. We use the Au- tonomous System (AS) topologies from both the Rocketfuel [20] and the CAIDA [21] projects, which represent IP-level connections between backbone/gateway routers of several ASes from major Internet Service Providers (ISPs) around the globe|,Data,161
| Although the paris-traceroute output of ITDK is more reliable than that of IPlane’s traceroute, the random selection of endpoints implemented by CAIDA hinders the collection of routes between the same vantage- and endpoints. Therefore we used the data of IPlane’s traceroute measurements|,Data,162
| They can also be used for constructing maps of the Internet at the Autonomous Systems level [, ]. In this work we used the CAIDA router-level Internet map from October th,  []|,Data,163
| 3 Table 1: Dataset Description Name BGP Usage AS Geolocation; Detour Detection Date 2016-01 Sources Info RouteViews, RIPE 38,688 RIBS, 416 peers, RIS 30 countries, 55GB Infrastructure IP List AS Geolocation 2016-01 to 2016-03 CAIDA Ark, iPlane, OpenIPMap, RIPE Atlas Measurements 3M Router IPs Infrastructure IPs to AS Mapping Infrastructure IP geolocation 2015-08 CAIDA ITDK, iPlane 6.6M IP to AS mappings AS to IXP Mapping AS Relationship AS Geolocation 2016-01 to 2016-03 Filtering peered paths from detection 2016-01 Traceroute Detour Validation 2016-05-01 IXP websites, PeeringDB, PCH CAIDA AS Relationship RIPE Atlas MaxMind Prefix Geolocation; Detour Validation 2016-01, 2016-03 MaxMind GeoLite City (free and paid) 368 IXP websites crawled 482,657 distinct relationships Used by Netra, 163 traceroutes Paid version used only for geolocating infrastructure IPs and detour validation longest prefix match on the global routing table and map the IP to the AS announcing the longest matching prefix|,Data,164
| As shown in Figure 3, we install LaBrea on a /29 subnetwork and use PlanetLab [9] to probe from multiple vantage points the entire /24 aggre- gate to which the /29 belongs. We scan the /24 network by attempting to establish TCP connections to each IP address in the subnet and capture the packets for further analysis|,Data,165
| • Active IPs in a Subnet: Intuitively, we might ex- pect high-occupancy subnets to be good indicators of pos- sible tarpits. To this end, we initially investigated using a hitlist of probable tarpits as inferred from the /24 subnets with more than 240 responding web hosts in the scans|,Data,165
| To facilitate large-scale scanning and avoid triggering anomaly detectors, degreaser uses permu- tation scanning [7, 12] to pseudo-randomly iterate through the IP address space when probing. Our real-world Internet scan, which probes at least one address in each /24 network in the Internet, discovers 107 different tarpit subnetworks (cid:20)(cid:24)(cid:25) ranging in size from /16 (with up to 216 fake hosts) to /24 (with up to 28 fake hosts)|,Data,165
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
| • Discovering correlations between anomalous traffic types detected with deep inspection techniques and traffic feature entropy variations. • Providing a traffic-type dissection (in-depth and entropy based) of a representative portion of the IBR for three weeks of April, 2012, with a 10-minute time scope|,Data,167
 Following is the summary of information about these data sets:  1. Data set from PREDICT USA [24] which contains traces of a DNS distributed denial of service attack (DDOS),Data,168
  from optical  2. Data set from CAIDA USA [25] which contains internet internet connectivity from 2002 and 2003,Data,168
  3. Data set from our experiment in which a PCAP file is captured from a lab computer which is being used for browsing and software development for the cyber security project,Data,168
| Putting these observations together, the total number of encodings in the ciphertext is M = d + d(d + 2) + 3(d + 2) + (κ − 3)(d + 2)2 = d2(κ − 2) + (d + 1)(4κ − 6) (2) The matrix-compressed variant (MC-variant) Note, however, that if we do not apply the matrix pre-multiplication optimization, but instead apply dimension reduction directly to the comparison state machine associated with the normal (not interleaved) ordering of the input digits, then the first matrix is of dimension 1 × d, the second matrix is of dimen- sion d × 3, and all other κ − 2 matrices are of dimension either 3 × (d + 2) or (d + 2) × 3 Putting this together, we have κ = 2n and M = d + 3d + 3(κ − 2)(d + 2) = 3(κ − 2)(d + 2) + 4d|,Non-data,3
| (3) Concretely, for a domain of size N = 1012, if we choose to represent the inputs in base d = 5, then this implies n = 18 (since 12 ≤ log10(518) < 13), and hence with the matrix pre-multiplication optimization along with the Cryptol op- timization for dimension reduction, we have κ = 19 and M = 845 Without applying matrix pre-multiplication and only using dimension reduction with base d = 10, we have n = 12, κ = 24, and M = 832 Experimentally for the mmaps we tested, we found that the matrix pre-multiplication optimization produces shorter 987ciphertexts than applying dimension reduction without ma- trix pre-multiplication However, we only tested this for an input domain of N = 1012 and security parameter λ = 80|,Non-data,3
| As N grows larger, depending on the asymptotic behavior of encoding sizes as κ increases and λ varies, future implemen- tations of the comparison state machine may find that one can produce shorter ciphertexts when applying dimension reduction without matrix pre-multiplication 62 Order-Revealing Encryption To implement order-revealing encryption, we set our plain- text domain to the numbers in the range [N ] By taking N = 1012, we found that selecting the base representation d = 5 and applying the matrix pre-multiplication optimiza- tion resulted in using only κ = 19 levels of the underlying mmap, which achieved the shortest ciphertexts for this do- main|,Non-data,3
| In fact, this construction yields the shortest known ciphertexts for ORE on a domain of size 1012, as explained below An alternative (basic) construction The closest com- petitor to our ORE construction in terms of ciphertext size and overall efficiency is a construction due to Lewi and Wu [33], which we refer to as the “basic” ORE scheme, de- scribed below Let [N ] be the message space|,Non-data,3
| Let F : {0, 1}λ ×{0, 1}λ → {0, 1}λ be a secure pseudorandom function (PRF) and H : {0, 1}λ × {0, 1}λ → {0, 1} be a hash function (modeled as a random oracle) Let cmp be the comparison function, defined as cmp(x, y) = 1 if x < y and cmp(x, y) = 0 if x > y The basic ORE scheme Πore is defined as follows • keygen(1λ) → (pp, sk)|,Non-data,3
| The algorithm samples a PRF r←− {0, 1}λ for F , and a random permutation key k π : [N ] → [N ] The secret key sk is the pair (k, π), and there are no public parameters • encrypt(sk, i, x) → ct Write sk as (k, π)|,Non-data,3
| If i = 1, the ciphertext output is simply ct = (F (k, π(x)), π(x)) If i = 2, then the encryption algorithm samples a nonce r it computes vj = cmp(π−1(j), y) ⊕ H(F (k, j), r) Finally, it out- puts ct = (r, v1, v2  |,Non-data,3
|  , vN ) • eval(pp, ct1, ct2) → {0, 1} The compare algorithm first parses ct1 = (k(cid:48), h) and ct2 = (r, v1, v2, |,Non-data,3
|   , vn), then outputs vh ⊕ H(k(cid:48), r) r←− {0, 1}λ, and for j ∈ [N ], Note that a single ciphertext from this scheme is precisely N + 2λ + (cid:100)log2(N )(cid:101) bits long|,Non-data,3
| For N = 1012 and λ = 80, this amounts to ciphertexts of length 11642 GB5 Choosing the best optimizations Our goal is to con- struct an ORE scheme which achieves shorter ciphertexts than the above construction, without compromising security|,Non-data,3
| To do this, we use our MIFE implementation for the com- parison function, and we apply our optimizations to make the ciphertext as short as possible We compare the ciphertext sizes for four different ORE constructions, obtained from either using the GGHLite or CLT mmap, and by applying either the DC-variant or MC-variant optimizations For each of these options, we fix the input domain size N = 1012 and vary the input base representation d ∈ [2, 25] Using Equations (2) and (3), we can compute the estimated ciphertext size as a function of 5Clearly, increasing λ has a relatively unnoticeable effect on the overall ciphertext size for the settings of N we consider|,Non-data,3
| GGHLite (DC) GGHLite (MC) CLT (DC) CLT (MC) ) B G ( e z i S t x e t r e h p C i 1,000 100 10 2 5 10 15 20 25 Input Base d Figure 61: Estimates of the ciphertext size (in GB ) for ORE with best-possible semantic security at λ = 80, for do- main size N = 1012 and for bases d ∈ [2, 25] We compare GGHLite and CLT, with the DC-variant and MC-variant op- timizations ) B G ( e z i S t x e t r e h p C i 1,000 100 10 1 0|,Non-data,3
|1 001 GGHLite CLT Basic 8 9 10 11 12 13 Domain Size Exponent e Figure 62: Estimates of the ciphertext size (in GB ) for ORE with best-possible semantic security at λ = 80, for varying domain sizes The exponent e on the x-axis denotes support for plaintexts in the range from 1 to N = 10e|,Non-data,3
| We compare GGHLite map (DC-variant), the CLT map, and the basic construction Πore (described in §62) d (since κ is determined by the choice of d and N ) See Fig- ure 6|,Non-data,3
|1 for the results We find that, for N = 1012, the short- est ciphertexts for ORE from GGHLite are obtained when d = 5 using the DC-variant optimization, and the shortest ciphertexts for ORE from CLT are obtained when d = 6 using the DC-variant optimization as well Under these settings, the DC-variant optimization for GGHLite reads the inputs in base 5, requiring κ = 19, to produce a total of 845 encodings per ciphertext, for a total size of 914 GB|,Non-data,3
| For CLT, the DC-variant optimization reads in the inputs in base 6, requiring κ = 19, to produce a total of 974 encodings, for a total size of 681 GB We also measure the ciphertext size as we vary the do- main size; see Figure 62|,Non-data,3
| We measure the estimated ci- phertext size for various domain sizes when using GGHLite, CLT, and the Πore construction described above The re- sults for GGHLite and CLT are using the optimal bases as detailed in Figure 61 We find that for N = 1011 and N = 1012, ORE using the CLT mmap and GGHLite mmap, respectively, produces a smaller ciphertext than Πore|,Non-data,3
| This demonstrates that for certain domain sizes, our ORE con- struction produces the smallest known ciphertexts (versus ORE schemes that do not require mmaps) 9887 PROGRAM OBFUSCATION A program obfuscator [8, 24] is a compiler that aims to make a program “unintelligible” while preserving its func- tionality Formally, an obfuscator O is a tuple of algorithms written as O = (obf, eval), where the obfuscation algorithm obf takes as input a program P (e|,Non-data,3
|g, expressed as a Boolean circuit), and outputs an obfuscated program obf(P), and the evaluation algorithm takes an obfuscated program obf(P) and produces an output An obfuscator is correct for a pro- gram P if, for all valid inputs x accepted by P, we have that eval(obf(P))(x) = P(x) VBB and pseudo-VBB security|,Non-data,3
| An obfuscator O is virtual black-box (VBB) secure 6 for a program P if for any efficient adversary A, there exists an efficient simulator S, given only oracle access to P(·), for which the quantity (cid:104)A(1λ, obf(P)) = 1 (cid:105) − Pr (cid:104)S P(·)(1λ) = 1 (cid:12)(cid:12)(cid:12)Pr (cid:105)(cid:12)(cid:12)(cid:12) is negligible We say that an obfuscator O is pseudo-VBB se- cure (pVBB) for program P and obfuscator O(cid:48) = (obf(cid:48), eval(cid:48)) if O(cid:48) is both VBB secure and for every efficient adversary A, there exists an efficient adversary B for which the quantity (cid:104)A(1λ, obf(P)) = 1 (cid:105) − Pr (cid:104)B(1λ, obf (cid:48) (P)) = 1 (cid:12)(cid:12)(cid:12)Pr (cid:105)(cid:12)(cid:12)(cid:12) is negligible In other words, if an obfuscator O is pVBB secure for a program P and obfuscator O(cid:48), then any efficient attack on the security of O translates directly to an efficient attack on the VBB security of O(cid:48) for the program P In our work, we construct a point function obfuscator that is pVBB secure for the Sahai-Zhandry obfuscator [39]|,Non-data,3
| Our obfuscator operates identically to the Sahai-Zhandry obfus- cator, which is VBB secure, except that we discard half of the ciphertext that corresponds to the second input in the “dual-input” branching programs that obfuscator uses Ef- fectively, our obfuscator can be seen as operating on “single- input” branching programs, which do not obtain VBB se- curity, but do obtain pseudo-VBB security We emphasize that this distinction in security is purely definitional from an attacker’s point of view, as any attack on our obfusca- tor immediately results in an attack on the Sahai-Zhandry obfuscator In this section we show how we use cryfsm and libmmap to build such a program obfuscator|,Non-data,3
| Apon et al [4] gave the first implementation of program obfuscation, using the CLT mmap [19] and a program compiler based on the approaches of Barak et al [7] and Ananth et al [3]|,Non-data,3
| We extend this codebase in the following ways: • Multilinear maps We integrate in libmmap to sup- • Program compilers We support MBPs output by port both the CLT and GGHLite mmaps cryfsm, using the Sahai-Zhandry obfuscator [39]|,Non-data,3
| Point function obfuscation We evaluated our imple- mentation by obfuscating point functions, namely, functions that output 0 on a single (secret) input, and 1 otherwise Previous work [4] also evaluated obfuscation for point func- tions, but was only able to successfully obfuscate 14-bit point functions with an mmap security parameter of λ = 60 As 6The reason we consider VBB versus indistinguishability ob- fuscation is that we consider point functions, for which VBB obfuscators are believed to exist|,Non-data,3
| GGHLite (λ = 80) GGHLite (λ = 40) CLT (λ = 80) CLT (λ = 40) ) B G ( e z i S t x e t r e h p C i 1,000 100 10 1 2 10 20 30 40 50 60 70 80 Input Base d Figure 71: Estimates for the ciphertext size (in GB) for point function obfuscation, for domain sizes N = 280 = 2λ and N = 240 = 2λ In the case of λ = 80, the minimums are achieved at d = 19 for GGHLite and d = 8 for CLT In the case of λ = 40, the minimums are achieved at d = 9 for GGHLite and d = 6 for CLT|,Non-data,3
| noted by Bernstein et al [9], the secret input of an n-bit point function can be recovered by simply enumerating over all 2n possible inputs In our experiments, we set n = λ, and consider point function obfuscation for 40-bit and 80- bit inputs The MBP for a λ-bit point function is of length λ and consists of a total of 2λ matrices, each of dimension 2 × 2|,Non-data,3
| As a small optimization, we can apply dimension reduction to obtain a branching program where the first pair of ma- trices need only be of dimension 1× 2 The more significant optimization comes by condensing the input representation through increasing the input base d The total number of encodings that we must publish in the obfuscation of a λ-bit point function can be computed as M = 2 + 4 · d · (cid:100)λ/ log2(d)(cid:101) We estimate the ciphertext size for various choices of bases in Figure 7|,Non-data,3
|1, which incorporates our estimations for the size of a single encoding in GGHLite and CLT for λ = 40 and λ = 80 • For λ = 40, we find that the minimal ciphertext size for domain size N = 240 is produced using MBPs under base 9 and length 13 for GGHLite, and base 22 and length 9 for CLT • For λ = 80, we find that the minimal ciphertext size for domain size N = 280 is produced using MBPs under base 19 and length 19 for GGHLite, and base 16 and length 20 for CLT Obfuscator implementation|,Non-data,3
| Our implementation is in a mix of Python and C, with Python handling the frontend and with C handling all the computationally expensive por- tions, and provides interfaces to both obfuscate (obf) and evaluate (eval) an MBP We parallelize the encoding of the elements in the MBP by using a threadpool and delegating each encoding operation to a separate thread Once all the threads for a given matrix in the MBP complete, we then write the (encoded) matrix to disk Thus, the threadpool approach has a higher RAM usage (due to keeping multiple encodings in memory as we parallelize) than encoding one element at a time and letting the underlying mmap library handle the parallelization, but is more efficient|,Non-data,3
| Other obfuscators Our obfuscator is built upon improve- ments inspired by the Sahai-Zhandry obfuscator, which is 989built on the general obfuscator described by Barak et al [7] and Ananth et al [3]|,Non-data,3
| In addition to these obfuscators, we also implemented the Zimmerman [40] obfuscator However, because the Zimmerman obfuscator induces a seemingly un- avoidable lower bound on the degree of multilinearity for the inputs we consider, we found that the Zimmerman ob- fuscator was not competitive with the obfuscator we imple- mented More specifically, the Zimmerman obfuscator re- quires that the degree of multilinearity for the obfuscation of any program be at least twice the number of inputs that the circuit accepts—a cost that may be insignificant when obfuscating other programs, but was too high for point func- tions (even when we tried to increase the input base repre- sentation to minimize this cost), and hence unsuitable for our purposes 8|,Non-data,3
| EXPERIMENTAL ANALYSIS With one exception, all of our experiments were performed using the Google Compute Engine servers with a 32-core In- tel Haswell CPU at 25 GHz, 208 GB RAM, and 500 GB disk storage The exception is with the most expensive obfusca- tion experiment we ran (obfuscating an 80-bit point function with CLT at security parameter λ = 80), where we used a Dell PowerEdge R930 server with 4 16-core Intel Xeon CPUs at 25 GHz, 2 TB RAM, and 5 TB disk storage|,Non-data,3
 81 MIFE Experiments We evaluated our multi-input functional encryption con- structions with two applications: order-revealing encryption (ORE) (cf §62) and three-input DNF (3DNF) encryption (see the full version),Non-data,3
| In §6, we showed how we can accu- rately estimate the ciphertext size from parameters derived from the input size and the security parameter λ, and our experiments confirmed that these parameter estimates are reasonably accurate (all within 1–2% of our reported val- ues) Additionally, we assessed the performance of the MIFE interface algorithms keygen, encrypt, and eval, along with memory utilization during the encrypt computation, which was by far the most costly step We note that, since the files that we are working with are so large, a non-trivial amount of time was spent in the reading and writing of these files to disk, and so an exact reproduction of our numbers may also need to mimic the disk storage specification we use As another sidenote, we reiterate that our primary inter- est in selecting the parameters for our MIFE applications is to create the most compact ciphertexts possible given our MIFE implementation|,Non-data,3
| As a result, some of our optimiza- tions come with a cost of increased evaluation time, and hence, we believe that it is possible to reduce our evalu- ation time (potentially at the expense of having larger ci- phertexts) Experimental results We summarize our experiments for MIFE in Table 81|,Non-data,3
| We evaluated the MIFE constructions for ORE with input domain sizes N = 1010 and N = 1012, testing both GGHLite and CLT as the underlying mmap For each experiment, we report the computation wall time for encrypt and eval, the overall ciphertext size ||ct||, along with the memory usage during the encrypt computation We note that for λ = 80, running keygen for all experiments took about 2–3 hours, and about 5–10 minutes for λ = 40 The encryption statistics measured were for generating a λ 40 80 experiment GGH (1012) GGH (1010) CLT (1012) CLT (1010) GGH (1012) GGH (1010) CLT (1012) CLT (1010) encrypt eval 239 m 49 s 198 m 43 s 35 m 81 s 24 m 54 s ||ct|| RAM 9|,Non-data,3
7 GB 8 GB 64 GB 7 GB 16 GB 20 GB 10 GB 17 GB 8 m 92 GB 6 m 61 GB 78 GB 151 h 72 GB 109 h 489 m 8 m 7,Non-data,3
|3 GB 192 GB 312 m 5 m 46 GB 166 GB Table 81: ORE experiments λ denotes the security pa- rameter of the underlying multilinear map; “experiment” denotes the multilinear map used and the domain size; “en- crypt” denotes the running time of encryption; “eval” denotes the running time of evaluation, “||ct||” denotes the size of the ciphertext; and “RAM” denotes the RAM required to en- crypt|,Non-data,3
| We use “h” for hours, “m” for minutes, and “s” for seconds complete ciphertext, containing all components, as opposed to containing only the left or right (or middle) components Since the CLT mmap produces shorter encodings, the en- cryption and evaluation time for the experiments using CLT were much faster than the corresponding experiments for GGHLite This is also partly due to the fact that CLT en- joys much more parallelism than GGHLite|,Non-data,3
| However, since GGHLite has been optimized to use less memory (at the ex- pense of increased computation time), the GGHLite exper- iments did not require as much RAM to run the encryption procedure 82 Program Obfuscation Experiments To evaluate our program obfuscation implementation, we chose a random secret 40-bit and a random secret 80-bit point, and used cryfsm to create the corresponding MBPs for the point functions associated with these points We se- lected the input base representation for these programs with the goal of minimizing the total obfuscation size for each obfuscated point function (see §7 for our calculations)|,Non-data,3
| Like with MIFE, optimizing for obfuscation or evaluation time could lead to different optimal input base representations Experimental results We tested three settings for point function obfuscation: 40-bit inputs with λ = 40, 80-bit in- puts with λ = 40, and finally, 80-bit inputs with λ = 80 We also obfuscated using both CLT and GGHLite for λ = 40, but only used CLT for λ = 80, as the GGHLite experiment was too resource-intensive|,Non-data,3
| Our results are summarized in Table 82 As we observed in the MIFE experiments, we note that GGHLite performs significantly worse when used in obfuscation compared to CLT We also note that while obfuscation takes a huge amount of time and resources, eval- uation is much less resource-intensive, for both GGHLite and CLT—a consequence of the fact that eval only requires mul- tiplying (encoded) matrices, which is highly parallelizable and also much less costly than the encoding operation itself|,Non-data,3
| However, unlike our observations with the MIFE experi- ments, here we note that the GGHLite mmap requires more memory than when using CLT This is because we paral- lelize the encoding process, whereas our MIFE construction avoids the parallelization during this step in order to reduce the amount of encodings in working memory at any given 990λ experiment obf eval 40 GGH (40-bit) CLT (40-bit) GGH (80-bit) CLT (80-bit) 18 h 68 s 5|,Non-data,3
9 m 36 s 297 s 30 h 1,Non-data,3
|3 h 260 s RAM ||obf|| 36 GB 82 GB 04 GB 2|,Non-data,3
7 GB 137 GB 116 GB 25 GB 205 GB 80 CLT (80-bit) 4,Non-data,3
2 h 999 s 117 GB 227 GB Table 82: Program obfuscation experiments,Non-data,3
| λ denotes the security parameter of the underlying multilinear map; “experiment” denotes the multilinear map used and the do- main size; “obf” denotes the obfuscation time; “eval” denotes the evaluation time; “||obf||” denotes the obfuscation size; and “RAM” denotes the RAM required to obfuscate (evaluation RAM usage never exceeded 1 GB) Due to the resource re- quirement for λ = 80, we ran this experiment on a machine with 2 TB of RAM and 128 cores operating at 25 GHz We use “h” for hours, “m” for minutes, and “s” for seconds|,Non-data,3
| stage Since GGHLite encodings are (much) larger than CLT encodings, GGHLite uses much more memory to store these encodings The tradeoff, of course, is an overall faster obfuscation time for all settings These results, while evidently impractical, are a huge im- provement over prior work [4], which took 7 hours to ob- fuscate a 14-bit point function with λ = 60, resulting in an obfuscation of 31 GB|,Non-data,3
| This improvement mainly come from (1) using a much tighter matrix branching program repre- sentation of the program, and (2) operating over different sized bases 9 CONCLUSIONS In this work, we presented 5Gen, a framework for the pro- totyping and evaluation of applications that use multilinear maps (mmaps) and matrix branching programs 5Gen is built as a multi-layer software stack which offers modular- ity and easy integration of new constructions for each com- ponent type|,Non-data,3
| Our framework offers an optimized compiler that converts programs written in the Cryptol language into matrix branching programs, a representation widely used in mmap-based constructions 5Gen includes a library of mmaps available through a common API; we currently sup- port the GGHLite and CLT mmaps, but our library can be easily extended with new candidates Leveraging the capabilities of our compiler and mmap libraries, we imple- mented applications from two computing paradigms based on mmaps: multi-input functional encryption (MIFE) and obfuscation We measured the efficiency of our MIFE and obfuscation applications with various parameter settings using both the GGHLite and CLT mmaps|,Non-data,3
| While the results show efficiency that is clearly not usable in practice, they provide a useful benchmark for the current efficiency of these techniques In the case of MIFE, we considered order-revealing encryption (ORE) and 3-input MIFE for DNF formulas that operate on triples of inputs We chose realistic parameters for ORE for which the implemented mmap construction gives currently the best known secure construction The same holds true for the 3-input MIFE for DNF formulas|,Non-data,3
| Our experiments show that, for 80 bits of security, ORE ciphertexts over a plaintext domain of size N = 1012 are up to 100 GB long with GGHLite and up to 10 GB long with CLT The ORE encryption takes hours while decryption completes in min- utes Likewise, our obfuscation implementation achieves a huge improvement over the only previous work that implemented obfuscation of point functions, primarily due to the opti- mizations we chose for the matrix branching program The size of the obfuscation of an 80-bit secret point with 80-bit security is 11|,Non-data,3
|7 GB, with an obfuscation time of roughly four hours, and an evaluation time of the obfuscated pro- gram which completes in less than two minutes Constructing multilinear maps is an active and rapidly- evolving area of research Our 5Gen framework provides an easy-to-use testbed to evaluate new mmap candidates for various applications and is an open-source tool available to other research groups for experimentation 10|,Non-data,3
| ACKNOWLEDGMENTS The work of Dan Boneh and Kevin Lewi was supported by NSF, DARPA, a grant from ONR, and the Simons Founda- tion The work of Daniel Apon, Jonathan Katz, and Alex J Malozemoff was supported in part by NSF awards #1111599 and #1223623 The work of Alex J|,Non-data,3
| Malozemoff was con- ducted in part with Government support through the Na- tional Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFG 168a, awarded by DoD, Air Force Office of Scientific Research The work of Brent Carmer and Mar- iana Raykova was supported by NSF grants CNS-1633282, 1562888, 1565208, and DARPA SafeWare W911NF-15-C- 0236 This material is based upon work supported by the ARO and DARPA under Contract No W911NF-15-C-0227|,Non-data,3
|ABSTRACT Cryptocurrencies, such as Bitcoin and 250 similar alt-coins, em- body at their core a blockchain protocol — a mechanism for a dis- tributed network of computational nodes to periodically agree on a set of new transactions Designing a secure blockchain protocol relies on an open challenge in security, that of designing a highly- scalable agreement protocol open to manipulation by byzantine or arbitrarily malicious nodes Bitcoin’s blockchain agreement proto- col exhibits security, but does not scale: it processes 3–7 transac- tions per second at present, irrespective of the available computa- tion capacity at hand In this paper, we propose a new distributed agreement proto- col for permission-less blockchains called ELASTICO|,Non-data,4
| ELASTICO scales transaction rates almost linearly with available computation for mining: the more the computation power in the network, the higher the number of transaction blocks selected per unit time ELASTICO is efficient in its network messages and tolerates byzan- tine adversaries of up to one-fourth of the total computational power Technically, ELASTICO uniformly partitions or parallelizes the min- ing network (securely) into smaller committees, each of which pro- cesses a disjoint set of transactions (or “shards”) While sharding is common in non-byzantine settings, ELASTICO is the first candi- date for a secure sharding protocol with presence of byzantine ad- versaries|,Non-data,4
| Our scalability experiments on Amazon EC2 with up to 1, 600 nodes confirm ELASTICO’s theoretical scaling properties 1 INTRODUCTION A blockchain is an append-only distributed database that stores a time-ordered set of facts, also known as transactions Trans- actions are grouped into batches or “blocks” and form a crypto- graphic hash-chain, hence the name blockchain|,Non-data,4
| In 2009, Bitcoin introduced the first blockchain protocol called Nakamoto consen- sus which underlies over 250 cryptocurrencies [1] The blockchain protocol maintains the distributed database in a decentralized net- work, thus aiming to solve what we call as the blockchain agree- ment problem Conceptually, the problem is to allow an arbitrary large network of several processors to agree on the blockchain state (identified by its cryptographic digest), under the assumption that Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page Copyrights for components of this work owned by others than the author(s) must be honored|,Non-data,4
| Abstracting with credit is permitted To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee Request permissions from permissions@acmorg|,Non-data,4
| CCS’16, October 24 - 28, 2016, Vienna, Austria © 2016 Copyright held by the owner/author(s) Publication rights licensed to ACM ISBN 978-1-4503-4139-4/16/10 |,Non-data,4
  $1500 DOI: http://dxdoi,Non-data,4
|org/101145/29767492978389 the fraction of malicious processors is bounded by f (0 ≤ f < 1) Processors have no inherent identities, nor is there any trusted PKI infrastructure to establish identities for processors|,Non-data,4
| Each processor can choose a set (eg, block) of transactions it wishes to commit to the blockchain; the goal of the protocol is to ensure that all honest processors agree on one set of transactions at the end of the proto- col The commit set is appended as a new block to the blockchain|,Non-data,4
| At a high level, the blockchain protocol in Bitcoin randomly selects one processor per epoch (say 10 minutes) which issues a proposal that everyone adopts, thus requiring only a single broad- cast to reach agreement [1] There may be temporary disagree- ment if two proposals occur at the same time; eventually, with very high probability, one proposal will be established by picking the longest blockchain Nakamoto consensus uses a proof-of-work (PoW) mechanism to probabilistically elect the leader, ensuring a fair choice of leaders In terms of scale, Bitcoin employs bil- lions of CPUs worth of computational power today (by observable hashrates [2]), and is one of the largest completely decentralized systems of such scale|,Non-data,4
| Unfortunately, Bitcoin’s transaction throughput does not scale well The Bitcoin network consumes massive computational power and presently processes up to 7 transactions per second [3] Other centralized fiat payment processing systems, like MasterCard or Visa are reported to processing 1, 200 to 56, 000 transactions per second [4, 5] The demand from practical applications is 3 to 4 orders of magnitude higher|,Non-data,4
| Modification to scale up existing pro- tocol is a raging debate in the Bitcoin community [6–9] Recent work shows that these proposals have fundamental scalability lim- its [10] On the other hand, solutions which use classical Byzantine con- sensus protocols [11–14] do not work in an open environment like cryptocurrencies because of two fundamental challenges First, many of these papers assume that the network nodes have pre- established identities or public-key infrastructure in place, which does not exist in open environments like Bitcoin|,Non-data,4
| Second, prac- tical byzantine consensus protocols such as PBFT [13] require at least a quadratic number of messages in the number of participants, thus they are bandwidth-limited — more network identities leads to worse performance Network bandwidth limits the transaction throughputs for a network of even a few hundred nodes severely This raises a fundamental question — are there any blockchain pro- tocols that scale throughput linearly with the increase in the size of the network? Problem & Approach Our goal is to seek a protocol for the open, permissionless network wherein participating processors have no pre-established identities, and where the transaction throughput scales|,Non-data,4
| We provide a new blockchain protocol called ELASTICO, which achieves a sweet spot between classical byzantine consensus 17and Nakamoto consensus protocols The key idea in our approach is to partition the network into smaller committees, each of which processes a disjoint set of transactions (or a “shard") Specifically, the number of committees grows near linearly in the total com- putational power of the network Each committee has a reasonably small number of members so they can run a classical byzantine con- sensus protocol to decide their agreed set of transactions in paral- lel|,Non-data,4
| Sharding protocols are commonly used in distributed databases and in cloud infrastructure, wherein certain network infrastructure can be trusted (eg, see the commonly used two-phase commit protocol) or where the goal is to tolerate crash (non-byzantine) failures [15–17] Note that several commercial and open-source blockchains do not target the permissionless (open) setting, and as a result, promise to scale by relying on trusted infrastructure [18–20] or by using federated identities [21, 22] (see Section 6)|,Non-data,4
| To our knowledge, we provide the first sharding protocol for permission- less blockchains tolerating a constant fraction of byzantine network nodes This is a well-recognized open problem [10] Our protocol makes the same assumptions as those implied in Bitcoin and other cryptocurrencies, and we provide security proofs for key invariants in our protocol Results|,Non-data,4
| Without loss of generality, we assume that the net- work contains n processors which have equivalent computational power ELASTICO exhibits almost linear scalability with computa- tion capacity and does not require quadratic number of messages as the network grows ELASTICO tolerates up to f < n/4 adaptive byzantine adversaries, where f and n are bounds on the adversar- ial and total computational power respectively 1 The protocol can support the same blockchain data structure format (a hash-chain) as Bitcoin; but, for further scalability, we propose a modification that permits better efficiency parameters|,Non-data,4
| From an efficiency perspective, our protocol shards the network into an almost linear number of committees that scales with com- putation capacity Within each committee of size c (a few hundred) identities, we run a secure consensus protocol which has message complexity of O(c2) (best case) to O(c3) (worst case) Overall, this yields a message complexity of at most O(nc3), where mes- sages are of constant size We implement ELASTICO based on the most popular client for Bitcoin [23]|,Non-data,4
| Our implementation adds roughly 5, 000 C++ LoCs on top of Bitcoin The throughput of our prototype scales near lin- early with respect to available computation ie, O(n/ log log(n)), when runs on our network simulation|,Non-data,4
| With the same network im- plementation as in Bitcoin, the scale up (blocks per epoch) for 100, 200, 400, 800 and 1, 600 nodes with equal computational power 2 are as theoretical expectation, namely 1, 189, 361, 698 and 13|,Non-data,4
|5 times respectively Finally, ELASTICO’s clean-slate design decou- ples the consensus from block-data broadcasts, hence the band- width spent by each node remains almost constant, regardless of the size of the network Our simulations are necessarily on a smaller scale than Bitcoin; however, if we project our results to a full de- ployment to a network of Bitcoin’s scale, we can expect a scale up of 10, 000 in the number of agreed values per epoch This agree- ment throughput is 4 orders of magnitude larger than Bitcoin’s|,Non-data,4
| Contributions We claim the following contributions • To our knowledge, ELASTICO is the first secure candidate for a sharding protocol for open blockchains that tolerates byzantine adversaries ELASTICO increases the blockchain’s 1Here, 1/4 is an arbitrary constant bounded away from 1/3, se- lected as such to yield reasonable constant parameters|,Non-data,4
| 2each node is one Amazon EC2 vCPU transaction throughput almost linearly with the computational power of the network • Our experiments on an idealized network simulation on Ama- zon EC2, ranging up to 1, 600 network nodes, confirm a near linear scalability for ELASTICO 2 PROBLEM & CHALLENGES 2|,Non-data,4
|1 Problem Definition We formalize the problem of designing a secure sharding proto- col for blockchains as follows Let there be n identity-less proces- sors having the same computational power, a fraction f of which are controlled by a byzantine adversary The network accepts trans- actions per block, a transaction i in block j is represented by an i ∈ ZN  All processors have access to an externally- integer xj specified constraint function C : ZN (cid:55)→ {0, 1} to determine the validity of each transaction|,Non-data,4
| We seek a protocol Π run between the processors which outputs a set X which contains k separate “shards” or subsets Xi = {xj i} (1 ≤ j ≤ ||Xi||) such that the following conditions hold: • Agreement Honest processors agree on X with a probability of at least 1 − 2−λ, for a given security parameter λ • Validity The agreed shard X satisfies the specified constraint function C, i|,Non-data,4
|e, ∀i ∈ {1k},∀xj i ∈ Xi,C(xj i ) = 1|,Non-data,4
 • Scalability The value of k grows near linearly with the size of the network (or n) • Efficiency The computation and bandwidth used per proces- sor stays constant regardless of n and k,Non-data,4
| Our goal is to split the network into multiple committees, each processes a separate set of transactions (eg, Xi) called a shard The number of shards (k) grows near linearly on the size of the network|,Non-data,4
| The efficiency property represents the sharding advantage, where the cost is localized within a committee Once the network agrees on the set X, it can create a cryptographic digest of X and form a hash-chain with previous agreed sets in the previous runs of Π This serves as a distributed ledger of facts or transactions We point out that the agreement property in our problem is a re- laxation of the original byzantine consensus problem [11, 12]|,Non-data,4
| The first significant distinction is the definition of “agreement” Here, we allow the honest processors to be in “probabilistic agreement” such that processors agree on a value with some high probability, rather than be in exact agreement The second distinction is that the agreed value can be the input of any processor, honest or byzantine The classical definition requires that the agreed value also be the inputs of honest processors|,Non-data,4
| In the blockchain problem, validity can be checked externally — each honest processor can check if the agreed value satisfies an externally-specified constraint C, and accept a solution only if so Remark Notice that this problem does not directly guarantee a double spending check (a problem in cryptocurrency [1]), but im- plementing such a check is possible given the agreement on the transaction set which satisfies the constraints specified in C In Ap- pendix 10|,Non-data,4
|2, we describe how one might build a new cryptocur- rency like Bitcoin based on ELASTICO with all validity checks Threat Model We consider the threat model of a static, round- adaptive adversary Processors controlled by the byzantine adver- sary can be arbitrarily malicious, e|,Non-data,4
|g, deviate from the protocol, 18and/or drop messages from other processors All malicious proces- sors can collude together Further, we consider a round-adaptive adversary, which can select which processors to corrupt at the start of each run Π|,Non-data,4
| The adversary has complete access to the outputs of all previous i−1 runs to make its choices However, once a protocol run begins, the choices of compromised processors are fixed The processors can setup point-to-point communication links between them, and the adversary has full information about the messages transmitted on all links Security Assumptions|,Non-data,4
| We make two assumptions about the underlying network overlay layer as in Bitcoin Explicitly, (a) the network graph between honest processors is connected and (b) the communication channel between honest processors is synchronous, ie, once an honest user broadcasts any message, other honest pro- cessors will receive it within a known bounded delay of δt seconds|,Non-data,4
| Note that such timing and connectivity assumptions are implicit and necessary even in Bitcoin; otherwise, byzantine nodes can de- lay blocks significantly (simulating a gain in computation power) or worse — a fraction of the network can be “eclipsed” by the ad- versary Attacks targeting our assumptions will apply to Bitcoin too However, such assumptions can be achieved with the right de- sign of the underlying network topology [24, 25] — an orthogonal problem of active research On the other hand, we do not make any assumption about a secure broadcast channel or a PKI system or access to external source of randomness|,Non-data,4
| That means, in our threat model, the malicious processors can drop or modify messages, send different messages to honest processors We show in Section 4 that the most that an adversary can do is to delay our consensus process Further, we assume that we know the upper bounds on the true computation power n (say in Gigahash/sec), and that f is less than 1/4 Estimating such a bound is feasible from observing network hashrates, as in Bitcoin, with the caveat that adversaries can pre- tend to control f much lower than they actually do (just as in Bit- coin today)|,Non-data,4
| For this work, we assume such information is exter- nally available We further assume that nodes are reliable during protocol runs, and failed nodes are counted in the f malicious frac- tion Second, we assume that the total computation power of the byzantine adversaries is still confined to standard cryptographic as- sumptions of probabilistic polynomial-time adversaries Lastly, we assume there exists a random oracle H : {0, 1}∗ (cid:55)→ {0, 1}γ which outputs γ random bits given any input string|,Non-data,4
| 22 Challenges Sharding in a permission-less blockchain with the presence of byzantine adversary is a well-recognized open problem [10] due to many challenges First, processors have no inherent identities or external PKI to trust A malicious processor can thus simulate many virtual processors, thereby creating a large set of sybils [26, 27]|,Non-data,4
| Thus, the protocol must prescribe some mechanism to allow processors to establish their identities, yet limiting the number of sybil identities created by malicious processors Once identities are established, the next challenge is to run a sharding protocol among the identities with a fraction f of them are byzantine Our goal is to uniformly split all identities into sev- eral committees with a condition that each committee has the ma- jority as honest with high probability Such a protocol becomes straight-forward if one assumes a shared random coin to do the sharding properly [15–17]|,Non-data,4
| However, achieving a good random- ness in a distributed network is a known hard problem The best solution to date can only tolerate up to 1/6 fraction of malicious, with excessive message complexity [28] Our protocol makes no such assumption Third, we must ensure that an adaptive adversary observing all the protocol runs, does not gain significant advantage in biasing its operations or creating sybil identities|,Non-data,4
| Our protocol must tolerate variable rate of identity creation and inconsistency in views of com- mittee members (ie, they may not agree on who are in the same committee) because of both byzantine failures and network delays in real networks (as in our threat model) 3|,Non-data,4
| ELASTICO DESIGN In this section, we present ELASTICO For the rest of the pa- per, unless otherwise stated, if some probability p is negligible, it means it happens with probability at most O(1/2λ) for some secu- rity parameter λ Similarly, if some event happens with high prob- ability (wh|,Non-data,4
|p), it happens with probability of at least 1−O(1/2λ) If some event happens with non-negligible probability, it happens with probability greater than O(1/2λ) 31 Solution Overview set of values X =(cid:83)2s The algorithm proceeds in epochs, each of which decides on a i=1 Xi where 2s is the number of subsets Xi|,Non-data,4
| In this description, we describe the steps taken during one epoch The key idea is to automatically parallelize the available com- putation power, dividing it into several smaller committees, each processes a disjoint set of transactions (or shards) The number of committees grows proportionally to the total computation power in the network All committees, each of which has a small constant number c of members, run a classical byzantine consensus protocol internally to agree on one value|,Non-data,4
| A final committee called the con- sensus committee is responsible for combining the shards selected by other committees, computing a cryptographic digest and broad- casting it to the whole network As the last step in the epoch, the final committee generates a set of shared public random bit strings, which have a bounded bias These random strings are used in the subsequent epoch as a source of randomness—ensuring that the adversary cannot use observations from previous epoch to simulate any non-negligible gain in computational power In each epoch, processors execute the following 5 steps: 1|,Non-data,4
| Identity Establishment and Committee Formation Each pro- cessor locally generates an identity consisting of a public key, an IP address and a proof-of-work (PoW) solution [1] The processor must solve a PoW puzzle which has publicly verifi- able solutions to generate the final component of the identity A PoW solution also allows others to verify and accept the identity of a processor|,Non-data,4
| Because solving PoW requires com- putation, the number of identities that the malicious proces- sors can create is limited by the fraction of malicious com- putational power f Each processor will then be assigned to a committee corresponding to its established identity 2 Overlay Setup for Committees|,Non-data,4
| In this step, processors com- municate to discover identities of other processors in their committee The overlay of a committee is a fully-connected subgraph containing all the committee members A naïve solution is for every processor to broadcast its identity and committee membership to everyone; however, this solution will result in O(n2) messages, which is not scalable We provide a simple solution that requires a small number of broadcasts, i|,Non-data,4
|e, O(nc), after which identities in same com- mittees can quickly identify each other 3 Intra-committee Consensus|,Non-data,4
| Processors run a standard byzan- tine agreement protocol (eg, PBFT [13]) within their com- mittee to agree on a single set of transactions (or a shard) 19There exist simple solutions to guarantee that all commit- tees propose disjoint shards, e|,Non-data,4
|g, each committee works on a separate shard of transactions based on their committee ID Each committee sends the selected shard, signed by enough members (ie|,Non-data,4
|, c/2 + 1), to a designated final committee 4 Final Consensus Broadcast The final committee computes a final value from all the values received from other commit- tees|,Non-data,4
| To do so, members in final committee run a byzantine consensus protocol to agree on the final result and broadcast it to the network 5 Epoch Randomness Generation The final committee runs a distributed commit-and-xor scheme to generate an exponen- tial biased, but bounded, set of random values|,Non-data,4
| The random values are broadcast to the network and used in the PoW in the next epoch Parameters Throughout this paper, we use the following nota- tion: n is the total number of identities that we expect to be gener- ated in an epoch, f = 1/4 is the fraction of computational power controlled by malicious users, the size of each committee is c, the number of committees is 2s, for some constant s Without loss of generality, s can be picked such that n = c · 2s|,Non-data,4
| Thus, ELASTICO scales up almost linearly because the expected number of PoW so- lutions to have each committee has at least c members is O(n log s) (or O(n log log (n/c))) For a concrete analysis, we refer readers to Appendix 101 Note that picking smaller s leads to lower la- tency and bandwidth consumption, which allows one to tune the network consumption|,Non-data,4
| In addition, the size of committee c is deter- mined by the security parameter λ and the expected network delay δt Efficiency Our protocol requires O(c) broadcasts to the whole network (steps 2, 4 and 5) Each such broadcast can be imple- mented in O(n) message transmissions|,Non-data,4
| Steps 3, 4 and 5 require at most c round of c2 multicasts for each committee of size c There- fore, the total number of messages is O(nc + nc3) or roughly O(n) if we consider c to be a small constant compared to n Security In each epoch, for f = 1/4, our protocol guarantees the following security properties S1–S5, the proofs of which are presented in Section 4|,Non-data,4
| • S1 Given a security parameter λ, there exists n0 such that ∀n(cid:48) ≥ n0, among the first n(cid:48) identities created, at most 1/3 are malicious whp|,Non-data,4
| The gap between f and 1/3 accounts for the variance in the number of PoW solutions found by the adversary Our committee size c is then chosen as based on the value of n0 (eg, c ≥ n0) such that every committee has at most a fraction 1/3 of malicious identities|,Non-data,4
| • S2 After Step 2, all committee members have their own view of at least c members in the committee There may be discrepancies between two views due to network latency and byzantine behaviors This discrepancy, however, is bounded by c/3 w|,Non-data,4
|hp and all honest members have identities of other honest members in their views Further, the number of unique identities in all views is bounded by 3c/2, of which at most 1/3 fraction are malicious wh|,Non-data,4
|p • S3 For each committee, Step 3 yields a consensus on the set of transactions Xi proposed by members in the committee The chosen Xi is signed by at least c/2 + 1 of the identities on the committee|,Non-data,4
 This ensures at least one honest member has verified and agreed on the value • S4 Step 4 yields a valid set X =(cid:83)2s i=1 Xi which combines all proposed sets Xi from other committees X is also signed by at least c/2 + 1 of the members in the final committee,Non-data,4
| • S5 Step 5 will yield a set of random r-bit values with suffi- cient randomness Explicitly, the attacker can predict the ran- dom value, given r is large enough, with a negligible proba- bility in λ Note that we select f = 1/4 in order to achieve a practical value of committee size|,Non-data,4
| Theoretically, ELASTICO can work with any f less than 1/3 by increasing the committee size c accordingly to f The 1/3 bound is because we need to run a consensus protocol (eg, PBFT [13]) at every committee in Step 3, which can tolerate at most 1/3 fraction of malicious committee members|,Non-data,4
| 32 Identity Setup and Committee Formation First, each processor locally chooses its own identity of the form (IP, PK), which are public key and IP address respectively for the authenticated communication later In order for the network to accept an identity, the processor must find a PoW solution corre- sponding to its chosen identity As a “seed” for the PoW, we need a public random string epochRandomness generated at the end of the previous epoch to ensure that the PoW was not precomputed|,Non-data,4
| We discuss how this is generated and verified in Section 36 As- sume, for now, that epochRandomness is a public random string generated in the previous epoch Specifically, each processor lo- cally searches for a valid nonce that satisfies the following con- straint: O = H(epochRandomness||||IP||||PK||||nonce) ≤ 2γ−D|,Non-data,4
| D is a predefined parameter in the network which determines how much work a processor has to do to solves a PoW 3 Note that one can use other mechanisms like Proof of Stake [29], Proof of Space [30,31] instead of PoW to establish identities for processors Next, our protocol assigns each identity to a random commit- tee in 2s, identified by an s-bit committee identity The committee assignment must be random, even for the malicious users: a prob- abilistic polynomial-time adversary should not be able to bias its committee assignment with non- negligible probability|,Non-data,4
| We use a PoW to achieve these goals Specifically, the last s bits of O speci- fies which (s-bit) committee id that the processor belongs to Each committee will process a separate set of values (eg|,Non-data,4
|, a shard) based on this s-bit ID All processors know epochRandomness and choose their iden- tity IP and P K privately For any choice of nonce, H produces a γ-bit random output The probability that a single invocation of H satisfies the constraint for a randomly chosen nonce is thus p = 2−D|,Non-data,4
| No efficient adversary can find a nonce that satis- fies the constraint with non-negligible probability better than p by the cryptographic pre-image resistance assumption We later prove in Lemma 2 (Section 4) that among the first n(cid:48) identities, at most 1/3 · n of identities are created by byzantine processors whp|,Non-data,4
| For establishing S1, we need to examine the number of honest and byzantine identities that map to any given committee Since H is a random oracle, we can treat the bits in its output as unbiased and random Therefore, the s bit strings generated in the solution are random, and an identity is mapped to a given committee with probability 2−s Further, if n = 2sc, then on average it requires O(n log s) PoW solutions to have each of 2s committees has at least c members (see Appendix 10|,Non-data,4
|1) This is why the scale up factor is almost linear 3For example, D = 20 means O has at least 20 leading zeros 20Byzantine adversaries can choose not to broadcast valid solu- tions, thereby denying membership in a committee|,Non-data,4
| However, this does not advantage their membership in any other committee It remains to choose the parameters s, which determines the number of committees, and D, which determines the difficulty of the PoW These are discussed in Section 5 with our experiments 3|,Non-data,4
|3 Overlay Setup for Committees Once identities and their committees are established, committee members need a way to establish point-to-point connections with their committee peers A naıve solution is to ask every proces- sor to broadcast its identity and committee membership to every- one; however, this solution will result in O(n2) messages, which is not scalable Another challenge is that identities are established through a PoW, which is a probabilistic process that occurs over time: new identities are continuously being created at some rate Ideally, we need a mechanism to establish the first c members of the committee so that all honest members have the same view of the member set|,Non-data,4
| One could run any byzantine agreement proto- col here which tolerates up to 1/3 fraction of malicious identities However, this would yield BFT protocol running over the entire network without any parallelization (eg, O(n3) cost in the worst case)|,Non-data,4
| Here we show something more efficient which has O(nc) message complexity To reduce the number of broadcast messages, we have a special committee of size c to serve as a set of “directories" All identities can find their committee peers by contacting the directory commit- tee and then set up point-to-point links Further, we allow commit- tee members (including directory members) to have different views of the member set, a challenge that most previous BFT protocols do not face|,Non-data,4
| Our protocol can tolerate this discrepancy and show that i) all honest members have others’ identities in their view; ii) the difference is bounded by c/2 as in S2 Our algorithm to setup the overlay for committee is depicted in Algorithm 1 More specifically, the directory committee is simply a commit- tee of the first c identities During step one, if a processor finds a valid solution for PoW, and it has not seen c identities, then the processor will broadcast this identity to the whole network|,Non-data,4
| On the other hand, during step one, whenever a processor finds a valid solution for PoW for an identity, the processor will inform all di- rectory members (Line 17) Each processor informs only the first c directories that it sees in the network Note that each processor can have its own view of who are the first c directories In this way, directory members keep track of the committee mem- bership announcements|,Non-data,4
| Once each committee contains at least c identities each, directory members multicast the list of committee members to each committee member (Line 33) To reduce the num- ber of messages, a non-directory committee member only receives the list of members in its own committee Notice that directory members do not have to agree on the same set of members for a committee Each directory member can decide independently which members are in a given committee|,Non-data,4
| For committee members, each will receive at least 2c/3 lists of c committee members (from at least 2c/3 honest directories — due to S2) A malicious directory may not send any list to committee members Worse, malicious directories may send a bad member list to committee members (ie|,Non-data,4
|, a list which favors malicious iden- tities) To prevent that, a committee member takes the union of all the identities that it receives to create a view of at least c committee members (including itself) (Line 19) It is still possible that com- mittee members have different member sets We analyze both the sources and show that the discrepancy is bounded by at most c/2 members in Lemma 3, which proves S2|,Non-data,4
| Algorithm 1 Algorithm to form the directory committee and setup overlay for other committees Input: c: committee size; k = 2s: number of committees Output: Every committee member receives at least c members of (cid:46) Done by everyone (cid:46) Receive a PoW solution (cid:46) Find a PoW solution end if if lV iews ← GetViewsFromDirectories() then (cid:46) Move to next step in the protocol its committee w ← ReceivePoW() if len(curDirectories) < c then myPoWappend(w) Send(curDirectories, w) curDirectoriesappend(w) end if w ← SolvePoW() if len(curDirectories) < c then curDirectories|,Non-data,4
|append(w) Execute RunAsDirectory() BroadcastToNetwork(w) else curDirectories ← empty myPoW ← empty while True do 1: procedure FORMCOMMITTEE 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end if 21: end while 22: 23: end procedure 24: procedure RUNASDIRECTORY 25: 26: 27: 28: 29: 30: 31: 32: 33: v ← union(lV iews) return v while True do to members of committee i end for return True 34: 35: end if 36: end while 37: 38: end procedure (cid:46) Done by directories w ← ReceivePoW() (cid:46) Accept all PoWs in last round i ← GetCommitteeNo(w) if len(commList[i]) < c then commList[i]append(w) end if if ∀i ≤ k, len(commList[i])≥ c then for i = 0, i < k, i ← i + 1 do MulticastCommittee(i) (cid:46) Send commList[i] Overall, security follows from two arguments First, the most that a malicious directory can do is to not send honest identities and favor malicious identities in a committee It is because mali- cious directories cannot create new identities or change committee assignment of new identities due to the PoW|,Non-data,4
| Further, committee members decide who are in their committees based on all views re- ceived from directories As per S1, among the c directories, at least 2c/3 are honest Thus honest members in a committee will know all other honest members 3|,Non-data,4
|4 Intra-committee Consensus Once a committee is established, the protocol to agree on a set of transactions can reuse any existing authenticated byzantine agree- ment protocol [32, 33] As shown in S3, the union of all views will have at most 3c/2 members, and at most 1/3 of them are mali- cious In the worst case, we can assume that each committee has 3c/2 members, of which at most 1/3 of them are malicious Since each honest member knows all other honest members (due to S2), a member can treat all identities which are not in its view as ma- licious|,Non-data,4
| Hence, existing byzantine agreement protocols will work securely in our setting For example, we can use the POLYBYZ algorithm [32], or PBFT [13], or the more recent SYBILSENSUS algorithm [33] 21Once an agreement is reached, the selected transaction set is signed by at least c/2+1 signatures to guarantee some honest mem- ber has verified and accepted the value Each committee member then sends the signed value along with the signatures to the final committee (using the directory, again, to acquire the list of final committee members)|,Non-data,4
| The final committee can verify that a certain value is the selected one by checking that it has sufficient signa- tures Note that this final value can be a small cryptographic digest (eg, a Merkle hash root) representing the set Xi of values xj i (or transactions) that the committee agrees on|,Non-data,4
| 35 Final Consensus Broadcast The next step of the protocol is to merge the agreed values of committees and to create a cryptographic digest (a digital signa- ture) of the final agreed result A final committee (which includes all members with a fixed s-bit committee id) is designated to per- form this step The merge function is simple: each final committee member validates that the values received from the committees are signed by at least c/2 + 1 members of the proper committee, and takes the ordered set union of all inputs|,Non-data,4
| To ensure that the re- sult is indeed correctly composed from the correct inputs, the final committee runs the same intra-committee algorithm described pre- viously This step obtains a verifiable signature by at least c/2 + 1 members of the final committee, which the entire network can ver- ify upon broadcast 36 Generating Epoch Randomness In the final step of the protocol, the final committee (or consensus committee) also generates a set of random strings for use in the next epoch|,Non-data,4
| For the first epoch, a previous approach can be used to generate the epochRandomness [28] However, this solution tolerates at most 1/6 fraction of malicious members and only works for a small network since it requires excessive message complexity Our solution for this step consists of two main phases In the first phase, each member of the final committee chooses a r-bit random string Ri and sends a hash H(Ri) to everyone in the com- mittee|,Non-data,4
| The final committee then runs an interactive consistency protocol to agree on a single set of hash values S [11] The final committee members will broadcast S, along with the final set of X in Section 35 to everyone in the network This set S contains at least 2c/3 hash values and serves as a commitment to the ran- dom strings|,Non-data,4
| This first phase can be done with the previous step in Section 35, but for simplicity, we describe it separately here In the second phase, each member of the final committee broad- casts a message containing the random string Ri itself to everyone (ie|,Non-data,4
|, not just to the final committee) This phase starts only after the agreement of S is done, ie, having 2c/3 signatures on S|,Non-data,4
| This is to guarantee that honest members release their commitments only after they are sure that the committee has agreed on S and the ad- versary cannot change its commitment At this point, each user in the system has received at least 2c/3 and at most 3c/2 pairs of Ri and H(Ri) from members of the final committee: the honest members follow the protocol, while the ma- licious users may choose to not release their commitments Users discard any random strings Ri that do not match the commitments H(Ri) For the purpose of the next epoch, each user takes an XOR of any c/2 + 1 random strings Ri that it receives|,Non-data,4
| Note that users may select different Ri We consider the XOR of any subset of c/2 + 1 valid random strings sent by the final committee to be a valid epochRandomness Recall that these random strings are used as the seed for the PoW in the next epoch In order for others to verify that a PoW is valid, the user should attach to their PoW Step Potential problems 1 2 P1|,Non-data,4
| PoW variance - Too many malicious identities - Bias the committee distribution P2 Inconsistency between views - Adversaries withhold identities - Network latency 3, 4 P3 Cannot reach consensus 5 P4 Bias the random strings State- ment Lemma S1 S2 Lem|,Non-data,4
| 2 Lem 3 S3, S4 Lem 4 Lem 6 S5 Table 1: Sources of byzantine advantage in each step of ELASTICO and the corresponding supporting security statements and lemmas which resolve them|,Non-data,4
 solution the set of c/2+1 strings Ri used to generate the seed Any other user can then verify if these c/2 + 1 random strings match the commitments in S A formal proof which shows the strings generated from this pro- cess are sufficiently random is presented in Section 4 Here we provide an intuition why this works,Non-data,4
| Any combination of c/2 + 1 strings Ri must share at least one Ri from an honest member, and that value is not known to the adversaries before everyone has agreed on the set of commitments S Thus, the value R produced by XOR-ing c/2 + 1 strings Ri is perfectly random The adver- sary, however, can choose which c/2 + 1 values he uses to bias R to be in a set Sa of his choice We show that this probability is bounded by 1/2r−λ−c+log(c)/2|,Non-data,4
| Thus, if we pick r large enough (eg, r > 2λ + c − log(c)/2), the advantage of the adversary is negligible in λ 4|,Non-data,4
| SECURITY ANALYSIS In this section, we provide security analysis for how ELASTICO prevents potential threats and works securely We also discuss how byzantine adversary gain no significant advantage We begin by clarifying several assumptions First, we assume that the network is partially synchronous: any message can reach the destination with a maximum delay of δt, thus network protocol can be assumed to happen in rounds, each lasts for δt time|,Non-data,4
| All our claims for an epoch depend on there being sufficiently random strings generated in the previous epoch Definition 1 (Good randomness) We say that an epoch has a good randomness if: (i) every user has a publicly random string of r bits, verifiably generated in the previous epoch; (ii) no user has access to such a verifiable random string more than δt prior to the beginning of the epoch; (iii) malicious users can bias the randomness with negligible prob- ability We now prove the security properties of ELASTICO|,Non-data,4
| In particu- lar, we start with S1, which states honest identities take a dominate portion in all the generated identities Lemma 2 (S1: Good Majority) In every epoch with good ran- domness, for every sufficiently large integer n(cid:48) ≥ n0: among the first n(cid:48) identities created, at most n(cid:48)/3 − 1 are controlled by the adversary wh|,Non-data,4
|p Proof If all the users start at the same time, each solution gener- ated has a probability 1 − f = 3/4 of being taken by the honest processors Now, let Xi be an indicator random variable which takes value one if the ith identity is generated by an honest proces- i=1 Xi|,Non-data,4
| Then, X follows a binomial distribution sor Let X =(cid:80)n(cid:48) 22Thus we have: Pr(cid:2)X ≤ 2n /3(cid:3) = (cid:48) = (cid:100)2n(cid:48)/3(cid:101)(cid:88) (cid:100)2n(cid:48)/3(cid:101)(cid:88) k=0 k=0 Pr[X = k] (cid:32) (cid:33) n(cid:48) k f n(cid:48)−k(1 − f )k This probability decreases exponentially in n(cid:48) Given a security pa- rameter λ, we can find n0 such that Pr [X ≤ 2n(cid:48)/3] ≤ 2−λ,∀n(cid:48) ≥ n0|,Non-data,4
| The committee size c is at least n0 to guarantee that the fraction of malicious members in a committee is bounded by 1/3, with re- gard to the security parameter λ The value of n0 depends on the security parameter λ For example, if λ = 20, or the probabil- ity that something bad happens is once every 1 million epochs, we have n0 ≈ 600 Lemma 3 (S2: Good views with bounded inconsistency)|,Non-data,4
| In every epoch with good randomness, if the directory size is c, then for each committee, with high probability, we guarantee the following properties (i) Each member has their own view of who are in the commit- tee Two views of two honest members differ by at most 1/3 of the committee size; (ii) All honest members have identities of other honest members in their views; (iii) The total number of unique identities in all views is at most 3c/2 of which less than 1/3 fraction are malicious Proof sketch|,Non-data,4
| The inconsistency between views of member set is because of two reasons: network latency and byzantine behaviors We calculate the difference in views caused by each source Since the network delay and the probability in finding a PoW so- lution, eg|,Non-data,4
|, it is hard to tell who are the last committee members Since honest directories accept all valid PoWs in the last round (before the committee gets filled by at least c members), all hon- est committee members will have other honest identities in their view This leaves the malicious behaviors the main source of the discrepancy Malicious processors can decide to withhold their identities and only publish them in some particular time to cause the maximum discrepancy between views of member set of honest members|,Non-data,4
| How- ever, honest members always share the same view of honest identi- ties, thus the maximum discrepancy that an adversary can cause is the number of identities that he/she can create As per Lemma 2, this amount is always less than 1/3 of the total number of commit- tee members whp|,Non-data,4
| We next prove that the maximum number of identities in all views is 3c/2 and less than c/2 identities are malicious An ad- versary can wait until right before the last identity in a committee is found to publish all his/her identities According to Lemma 2, the maximum number of malicious identities can be created before honest processors find all c identities is c/2 Thus, the total num- ber of identities is bounded by 3c/2 and at most c/2 identities are (cid:3) malicious w|,Non-data,4
|hp We now show S3, S4 which argue that a committee (including the final committee and other commitees) correctly decides a single value (or a set of transactions Xi) Lemma 4 (S3, S4: Consensus)|,Non-data,4
| In every epoch with good random- ness, the honest members agree on a unique set Xi with at least c/2 + 1 signatures, with high probability Proof Although committee members have different views of mem- ber set, we can tolerate the inconsistency by considering a broader committee of size up to 3c/2 As we have established above, the fraction of malicious identities is bounded by 1/3 in any view, and honest members have other honest members in their view|,Non-data,4
| Thus, any Byzantine consensus protocol (eg, POLYBYZ, PBFT) which can tolerate up to 1/3 malicious fraction can guarantee agreement in the committee, ie|,Non-data,4
|, only one value selected Since our network is synchronous, the liveness property is al- ways achieved in these consensus protocols, ie, the committee al- ways reaches consensus in polynomial time|,Non-data,4
| The committee then collects enough number of signatures (eg, c/2 + 1) to guarantee that at least one honest member has signed the chosen value Lastly, we show Lemma 5 and S5, i|,Non-data,4
|e, the shared randomness generated is sufficiently random Specifically, we derive the fol- lowing definition from the standard definitions of bias [28] Definition 1|,Non-data,4
| Let F : {R0, R1, , Rc} (cid:55)→ {0, 1}r, computed over the set of r-bit input strings For all choices of set S ⊆ {0, 1}r, let EF (S) be the expected number of R values that the adversary can generate such that R ∈ S|,Non-data,4
| Assuming the adversary controls some of the inputs, the bias β(F ) is defined as: (cid:26) EF (S) E(S) , E(S) EF (S) (cid:27)(cid:33) , (cid:32) β(F ) = log S ⊆ {0, 1}r max max in which E(S) = ||S||/2r — the value of EF (S) if the adversary does not control any inputs Lemma 5 (Bounded Bias to Randomness) Given a set of c > 3 random r-bit strings of which at most (cid:98)c/2(cid:99) strings are known and generated by the adversary, the computation of R = ⊕Ri has a bias bounded by c − log(c)/2 Proof|,Non-data,4
| We define F : {R0, R1, , Rc} (cid:55)→ {0, 1}r as a function which selects c/2 + 1 strings Ri from c strings and compute R = ⊕Ri The computation of R can be decomposed into F = Fh ⊕ Fd, in which Fd is computed over the set of inputs controlled by the adversary, and Fh is computed over the set of inputs Rh from honest users which the adversary does not know|,Non-data,4
| The bias of Fd, by definition of bias is c−log(c)/2 This follows from the fact that the adversary can pick (cid:0) c (cid:1) combinations of c/2 strings Ri to decide Rd This number has an upper bound of 2c−log(c)/2 computed based on Stirling’s approximation Thus, we have β(Fd) < c − log(c)/2|,Non-data,4
| On the other hand, Rh is not generated by the adversary, so bias of Fh is 0 because EFh (S) = E(S) By composition, the total bias is c − log(c)/2 c/2 Lemma 6 (S5: Good Randomness) In every epoch with good ran- domness, with high probability, at the end of the epoch every user computes a random string of r bits that: (i) can be predicted with a negligible probability; (ii) can be verified as validly generated in that epoch|,Non-data,4
| Moreover, no user knows any of the random strings until at least one round prior to the end of the epoch That is, if epoch e has good randomness, then epoch e + 1 also has good randomness Proof By Lemma 3, there are at least 2c/3 honest members in the final committee w|,Non-data,4
|hp Thus, each user in the network receives a set 23R having from 2c/3 to 3c/2 random strings from members in the final committee in the last round of the epoch The user’s random string R is generated by XOR-ing any c/2 + 1 strings in R|,Non-data,4
| Since there are at most c/2 malicious members in the final com- mittee, at least one random string Ri used by malicious users orig- inated at an honest user Further, this Ri is not known by the ma- licious users before the final committee has agreed on the set S Thus, the r-bit string R generated by XOR-ing any c/2 + 1 is per- fectly random However, the attacker can still pick any of his c/2 values Ri to bias his random string R|,Non-data,4
| As per Lemma 5, the at- tacker’s bias to R is bounded by c − log(c)/2 Thus if we pick r large enough, eg, r > O(λ) + c − log(c)/2, the probability that the attacker can guess R is negligible|,Non-data,4
| Finally, by construction any user can verify that the set of c/2+1 bit strings is valid by checking the commitments sent out previ- ously Similarly, it is immediate that no user knows the random string prior to two rounds before the end of the epoch With the above lemmas in hand, the correctness of the ELAS- TICO protocol follows from the following facts We omit a theo- rem and its formal proof, which is by induction and invokes Lemma 2– Lemma 6 on different cases|,Non-data,4
| Claim 7 For every epoch i ≥ 1, with high probability, the follow- ing properties hold: (i) the final committee will broadcast only one combined value to the network with at least c/2 + 1 signatures and no other combined value will have c/2 + 1 signatures; (ii) this combined value contains 2s sub-values each of which comes from a committee and is verified by at least one honest processor; and (iii) at the end of the epoch, each user has a publicly verifiable random bit string of length r which has sufficient randomness (ie, the following epoch has good randomness)|,Non-data,4
| Notice that we have shown, at this point, that each epoch ends with a correct (combined) value selected Any user that is in the system can, by listening, verify that this combined value is the cor- rect and honest one Often, it is desirable that the correctness by externally verifiable, eg|,Non-data,4
|, by a user that was not in the system at the time In Bitcoin, this is achieved by showing that the chain constructed is the longest chain, with very high probability (ie, exponentially small probability)|,Non-data,4
| In fact, the same property holds here: on average, it will take the malicious users twice as long to generate a signed final value as the honest users Hence, an honest “chain” will grow twice as fast as a malicious “chain” and hence with very high probability it will be externally verifiable (For a more detailed discussion of this issue in Bitcoin, see [34]; the argu- ment here is similar) 5|,Non-data,4
 IMPLEMENTATION & EVALUATION We implement ELASTICO and empirically evaluate the scalabil- ity of ELASTICO and previous solutions The goals of our evalua- tion are twofold We first measure the scalability and efficiency of ELASTICO when the network size increases We aim to establish that the performance of ELASTICO matches its theoretical analysis,Non-data,4
| The second goal is to compare ELASTICO to other related consen- sus protocols including Bitcoin [1], Bitcoin-NG [9] and PBFT [13] 51 Implementation We implemented all components of ELASTICO based on the most popular Bitcoin client version v012|,Non-data,4
|1 [23] Our implementation is in C++ and has roughly 4, 000 LoC above that of Bitcoin’s code base We choose PBFT [13] as the consensus protocol for com- mittees in ELASTICO Since there was no well-maintained open source implementation of PBFT when we started, we built a full- fledged implementation of our own, and the implementation may be of independent interest to the community|,Non-data,4
| We plan to release ELASTICO for public use Recall that once the committee formation is complete, a normal committee runs one instance of PBFT protocol to agree on a data block of size 1 MB The final committee (or consensus committee) runs two consecutive instances of PBFT protocol to agree on one data block and the final block which aggregates all data blocks from other committees Thus, nodes in the consensus committee bear more cost than in normal committees|,Non-data,4
| Network implementation We reuse the network implementa- tion of Bitcoin for our peer-to-peer layer in the overall network The communications within committee are handled separately, in which nodes in a committee form their own peer-to-peer overlay network Messages passed within a committee are not sent to nodes in other committees|,Non-data,4
| The choice of using peer-to-peer instead of point-to-point communications within a committee is because the latter requires much more resource for a node to open a socket con- nection to each of 100 committee nodes Rate-limited mining We artificially limit the mining rate to allow testing ELASTICO with large scale experiments on a the pub- lic EC2 infrastructure (which is expensive for our scale of testing) In order to control the exact fraction of malicious computational power, we limit the number of hash operations (i|,Non-data,4
|e,SHA2) that a node (ie, a processor) can perform to 1 operation per second|,Non-data,4
| Each hash output has a probability 1/600 of finding a valid PoW solu- tion Thus, on average it takes a processor 600 seconds to establish its identity Note that we still faithfully simulate the mining process in Bitcoin, since we use the same implementation for mining 5|,Non-data,4
|2 ELASTICO’s Scalability Experimental Setup We run several experiments with different settings on Amazon EC2 to measure the scalability of ELASTICO We vary the number of nodes in the network from 100 to 1, 600, using up to 800 c4large EC2 instances in two different regions including Oregon and California|,Non-data,4
| Each EC2 instance is shared by two nodes, has 2 Amazon vCPUs and 375 GB of memory Thus, increasing the number of nodes in the network will increase the computation power accordingly We fix our committee size at c = 100, to limit the performance overheads due to PBFT as we discuss in Section 5|,Non-data,4
|3 Scalability of ELASTICO We start with a network of 100 nodes then double the network size 4 times, raising to 1, 600 nodes in the last setting We quantify the number of blocks created in each epoch, time to reach consensus, number of PoWs required to fill all committees|,Non-data,4
| The results are plotted in Figure 1 All numbers are averaged after 10 epochs Figure 1 shows that ELASTICO scales up the block throughput almost linear to the size of the network The number of blocks per epoch increases linearly (from 1 to 16) as we increase more nodes to the network (100 to 1, 600 accordingly)|,Non-data,4
| However, the epoch time is longer (eg, 600 seconds in 100 nodes to 711 seconds in 1, 600 nodes) since it requires relatively more time to find enough PoWs to fill up all committees when the number of committees in- creases In addition, we observe that the time to reach consensus on blocks once the committee formation is complete remains almost constant, regardless of the network size|,Non-data,4
| For example, the laten- cies to reach consensus are 103 and 110 seconds when the network sizes are 400 and 800 nodes respectively In summary, our experi- 24500 400 300 200 100 0 ) s ( y c n e t a L n o i t a g a p o r P 1 Block 2 Blocks 4 Blocks 96 48 24 50 26 13 456 227 114 322 161 80 187 92 46 100 200 400 800 1600 Network size (number of nodes) Figure 3: Latency of Bitcoin-NG with different number of micro blocks 81 809 8|,Non-data,4
11 812 813 1 Block 2 Blocks 4 Blocks 405 4,Non-data,4
05 406 406 407 2,Non-data,4
04 202 203 203 2,Non-data,4
|03 ) B M ( e d o n r e p h t d i w d n a B 8 6 4 2 100 200 400 800 1600 Network size (number of nodes) Figure 4: Bandwidth consumption of Bitcoin-NG with different number of micro blocks and network sizes therefore the number of blocks per epoch that the network can agree on is 10, 000 State differently, ELASTICO scale up the agreement by 4 orders of magnitude when deployed in a network of current Bitcoin’s scale 53 Comparison to Related Systems We show how ELASTICO outperforms existing protocols in many applications where the verification check of a block does not re- quire to scan through the entire history of the blockchain, but only the block itself|,Non-data,4
| A prominent example of such applications is an append-only database for certificate directory, information regis- tration platform and so on In these applications, verifying a data block can be done independently disregard to the current state of the blockchain, thus ELASTICO nodes do not have to download data blocks from other committees We show that only ELASTICO achieves the efficiency property defined in Section 2 On the con- trary, other protocols like Bitcoin, Bitcoin-NG [9] and PBFT re- quire much more network bandwidth and/ or local computation at each node when processing more transactions|,Non-data,4
| Bitcoin Recent work has shown the scalability limit of Bit- coin [10] We also run our own set of experiments and get con- sistent results with the results in [10] Due to the space constraint, we do not include this set of experiments and the results here|,Non-data,4
 Figure 1: ELASTICO scales up the throughput nearly linearly in the computation capacity of the network Figure 2: Cost per node in ELASTICO stays almost constant regard- less of network size ments confirm the expected scalability of ELASTICO’s transaction throughput Efficiency of ELASTICO,Non-data,4
| We next evaluate the efficiency prop- erty (defined in Section 2) of our protocol Figure 2 depicts the number of messages sent/received and the bandwidth consumed at each node for different network sizes for reaching consensus once the committees are formed The number of messages per node re- duces as more nodes join the network, while the bandwidth con- sumed at each node fluctuates around 5 MB per node The decrease in the number of messages exchanged when there are more nodes in the network is because the extra messages re- quired for running the second instance of PBFT in the final commit- tee are amortized|,Non-data,4
| Thus, we see the reduction in the number of mes- sages exchanged per node when the network size increases from 100 nodes (2, 416 messages) to 200 nodes (1, 821 messages) The bandwidth used at each node, on the other hand, remains roughly unchanged, eg, 4|,Non-data,4
|93 MB (400 nodes) and 501 MB (800 nodes) per node This is because the communication costs within commit- tees are localized Thus even when more nodes join the network, existing nodes does not have to bear more costs|,Non-data,4
| Extrapolation to Bitcoin’s scale We extrapolate the scale up that ELASTICO can achieve in a network of current Bitcoin’s scale, if we assume our scalability holds in real network As of February 2016, the hash rate of Bitcoin network is 12× 109 GHash/s|,Non-data,4
| Thus, we assume that the network consists of n = 1, 000, 000 equivalent processors, each of which can perform 12 × 103 GHash/s We consider a setting where the committee size is 100, thus the number of committees is 10, 000 Each committee agrees on a single block, 0 2 4 6 8 10 12 14 16 18 0 100 200 300 400 500 600 700 800 900 100 200 400 800 1600 No|,Non-data,4
 of Block Time (s) Network size (number of nodes) Committee Formation time Consensus Time Blocks Final Scalability 400 440 480 5,Non-data,4
20 560 600 500 1000 1500 2000 2500 3000 3500 100 200 400 800 1600 Bandwidth used per node (MB) Number of messages per node Network size (number of nodes) Number of messages Bandwidth 25Bitcoin-NG Our next set of experiments compare the scalabil- ity of ELASTICO and a recent scalability proposal namely Bitcoin- NG [9],Non-data,4
| Bitcoin-NG slightly modifies Nakamoto consensus proto- col to propose more blocks per epoch At a high level idea, Bitcoin- NG also probabilistically selects a leader, and the leader can pro- pose several micro blocks (ie, data blocks) per epoch|,Non-data,4
| The leader of the next epoch can decide which data block of previous epoch he/ she wants to build on top We measure the bandwidth consumption per node of Bitcoin-NG in different settings where the numbers of nodes are 100, 200, 400, 800, 1, 600 We also quantify the latency to broadcast different number of micro blocks In terms of experimental setup, nodes are distributed equally in two Amazon regions namely Oregon and California|,Non-data,4
 Each node randomly connects with 4 other nodes to form a peer-to-peer network We collect the source code of Bitcoin- NG from the authors through our private communication Figure 3 and Figure 4 report our results Figure 3 shows the latency of Bitcoin-NG for different number of micro blocks,Non-data,4
| Bitcoin-NG performs well (ie, low latency) when the network size is small (100 nodes, 4 blocks, 50 seconds) Its performance worsens when the network size grows larger (1, 600 nodes, 4 blocks, 456 seconds)|,Non-data,4
| Since Bitcoin-NG is a variant of Nakamoto consensus, nodes have to broadcast all blocks to the whole network since they are needed for the consensus protocol of the next epoch Thus, increasing the block throughput entails a longer overall latency, especially when the network grows ELAS- TICO, on the other hand, scales up the throughput as more nodes join the network without expanding the system latency, as we estab- lish in Section 52|,Non-data,4
| ELASTICO does that by decoupling messages needed for consensus from the final data broadcast In terms of bandwidth consumption, the bandwidth used at each node in Bitcoin-NG increases linearly with the throughput, eg, 2|,Non-data,4
|04 and 405 MB per node when there are 1 and 2 data blocks respectively This is because Bitcoin-NG needs to broadcast all blocks to the network for the next consensus step, thus more blocks, more bandwidth used On the other hand, in ELASTICO, only the block headers are broadcast to the whole network since ELAS- TICO decouples data broadcast from consensus metadata|,Non-data,4
| There- fore the bandwidth used at each node for the consensus step re- mains roughly the same, regardless of the throughput, eg, 493 and 5|,Non-data,4
01 MB when there are 4 and 8 blocks per epoch respectively Network constraint in PBFT We next measure the scalability of traditional byzantine consensus protocols The selected candi- date is PBFT [13] with our own implementation,Non-data,4
| Similar to the experiments with Bitcoin-NG, we report the number of messages exchanged per node, the bandwidth consumed at each node and the latency to reach consensus for various network sizes The results of our experiments are in Figure 5 and Figure 6 Note that all com- munications are done via a peer-to-peer network, with each node connects to 4 other peers randomly We observe that the bandwidth cost and the total number of mes- sages exchanged at each node increase linearly with the size of the network|,Non-data,4
| For example, a node sends/ receives 970 and 1, 930 mes- sages when the network is of sizes 80 and 160 respectively Simi- larly, the latency grows quadratically as we introduce more nodes For instance, when network size increases from 40 to 80 nodes (2 times), the latency is 6 times longer (eg|,Non-data,4
|, from 3 seconds to 18 seconds) This linear increase in cost and quadratic increase in la- tency render PBFT inefficient even if the network has only a few hundreds nodes In fact, our experiment when the network has 320 nodes did not terminate after running for 1 hour We remark that in our experiments, there is no faulty (malicious) nodes|,Non-data,4
| Thus, the cost will increase if we introduce faulty nodes in our experiments Figure 5: Cost per node in PBFT increases linearly with the size of the network ) s ( y c n e t a L 150 100 50 0 56 18 1 2 3 10 20 40 Network size (number of nodes) 100 80 165 160 Figure 6: Latency to reach consensus in PBFT with different network sizes 6 RELATED WORK We compare our solution to existing solutions for blockchain scalability in Table 2|,Non-data,4
| The detailed discussions are below 61 Centralized Sharding Protocols ELASTICO is related to other sharding protocols in distributed databases, eg|,Non-data,4
|, Google’s Spanner [15], Scatter [16], RSCoin [39] However, these sharding protocols consider a different model which does not handle byzantine failures, make assumptions of PKI sys- tems and a trusted infrastructure and access to external random seed For example, RSCoin only works for centralized cryptocur- rencies where there is a central point of trust (central bank) Such protocols are inapplicable to deploy in a byzantine environment like blockchains|,Non-data,4
| In fact, sharding is a well-recognized open problem in byzantine environment [10] In this work, we explain all the challenges and propose the first such sharding solution in the par- tially synchronous setting We have established that ELASTICO is secure and cost-efficient even with byzantine adversaries, allowing the transaction throughput to scale up almost linearly with the net- work computation capacity 6|,Non-data,4
|2 Blockchain Scalability Solutions Building a scalable blockchain is an active problem in the Bit- coin and cryptocurrency community There have been several pro- posals from both academia and industry The first approach is to push more blocks to the blockchain, eg|,Non-data,4
|, GHOST [40], Bitcoin-NG [9] GHOST modifies the rule to accept the main valid blockchain to accept not only the earliest block at each epoch, but also other blocks which are found later, eg, “or- 0 500 1000 1500 2000 2500 4|,Non-data,4
50 460 470 480 4,Non-data,4
90 500 510 520 5,Non-data,4
30 10 20 40 80 100 160 No of messages Bandwidth (MB) Network size (number of nodes) Data bandwidth Main messages's bandwidth No of messages 26BFT Tendermint [35] IBM Blockchain [36] Chain OS [19] DigitalAsset [37] Candidate Decentralized Yes (cid:88) Identity-less Bandwidth (per node) Scalability O(n2) No No Nakamoto Bitcoin [1] Ethereum [38] BitcoinNG [9] IntelLedger [18] Yes (cid:88) Yes (cid:88) Constant (cid:88) No Quorum-BFT Two-phase commit ELASTICO Ripple [22] Stellar [21] No No O(n) No Spanner [15] RSCoin [39] Databases No No Constant (cid:88) Yes (cid:88) This work Yes (cid:88) Yes (cid:88) Constant (cid:88) Yes (cid:88) Table 2: Comparison between ELASTICO and existing blockchain protocols in academia and industry ELASTICO is the first solution which can scale up the throughput when the network size increases in a byzantine and decentralized environment,Non-data,4
| phaned blocks" On the other hand, Bitcoin-NG allows each leader in an epoch to propose more blocks to the network Both GHOST and Bitcoin-NG succeed at allowing block parallelism in the net- work, but they do not localize the verification of transactions as in ELASTICO Thus, more transactions in the network, more lo- cal computation is required at each node and delay the consensus process as pointed out in previous work [41]|,Non-data,4
| A different approach for increasing Bitcoin’s transaction through- put is to allow transactions to take place outside of the main Bitcoin blockchain Two prominent examples are lightning-network [42] and Sidechains [43] Lightning-network creates offchain micro- payment channels between users Thus users can send multiple and instant transactions more efficiently, with only a few transactions included in the blockchain|,Non-data,4
| Sidechains takes a different approach to allow users to move coins to different blockchain, thus allowing Bitcoin transactions to happen elsewhere It is widely understood that Sidechains do not solve the scalability problem in Bitcoin [44] Both the techniques, although improve the transaction throughput significantly, are applications running on top of Bitcoin thus still rely on the scalability of the underlying protocol It is worth not- ing that applications enabled by Sidechains do not enjoy the same security guarantee provided by Bitcoin, and micro-payment chan- nels only work for a few applications|,Non-data,4
| ELASTICO, however, allows scaling up the underlying blockchain protocol without degrading any security property Buterin et al also address the scalability problem in blockchain with sampling and challenging techniques [45] Similar to ELAS- TICO, the paper’s approach is to use sharding|,Non-data,4
| However, the pro- tocol “randomly" samples verifiers to verify others’ updates, and allows users to challenge others’ verification results if they ever detect an invalid update The solution relies on a random seed, for which the paper does not provide any security analysis Further, the paper does not consider byzantine adversaries but rational ones in a “cryptoeconomic" threat model, which is different from the threat model that we consider in this paper Recent non-peer-reviewed proposals including Stellar [21], Rip- ple [22], and Tendermint [35] claim to support high transaction rate, but either have weaker threat models or are not as scalable as ELASTICO|,Non-data,4
