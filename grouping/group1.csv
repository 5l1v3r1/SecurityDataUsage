 6303. DATASETS Our Internet-wide study of key sharing in the HTTPS ecosystem is driven by four datasets: SSL certificates We use SSL certificates from full IPv4 scans as the basis of our measurements,Data,0
| Our SSL scans [30] also contain information on the IP address(es) that advertised each certificate. To obtain in- formation about the entity that controls this IP address, we use full IPv4 reverse DNS scans [29] that are also conducted by Rapid7|,Data,0
| Each AS is assigned an AS Number (ASN): for example, MIT is AS 3 and the Chicago Public Schools are AS 1416 [26]. CAIDA collects and publishes mappings between IP addresses and ASNs via their Route- Views datasets [7]|,Data,0
| For example, AT&T owns 160 unique ASNs. To aggregate these, we use CAIDA’s AS- to-Organization dataset [8] to group together ASes owned by the same organization|,Data,0
| For that, we rely on WHOIS [12], a protocol for querying domain registrars to obtain data on the domain owner. In practice, WHOIS data often contains fields such as the con- tact information for the owner of the domain, the contact for technical issues, where to send abuse complaints, and so on|,Data,0
| Here, we expand upon these prior findings by evaluating whether there is a correlation between centralized management and the quality of the keys chosen. Figure 13 compares several different features of self- managed and outsourced certificates across our entire cor- pus of leaf certificates (3,275,635 self-managed and 1,781,962 outsourced): (a) Key lengths in self-managed certificates are nearly identical to those managed by third-party hosting providers|,Data,0
|1 Combining Packet Capture (PCAP) Files The data set used in this study is a combination of the packet capture files obtained from two main sources. First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour|,Data,1
| First of all, the APTs were collected from Contagio malware database [15] contributed by Mila Parkour. The normal and non-malicious data is obtained from PREDICT internet data set repository [18] under the category of “DARPA Scalable Network Monitoring (SNM) Program Traffic”|,Data,1
| The data collection was performed during April 2016 using ZGrab, an application-layer scanner that operates with ZMap [15]. In the first phase, we performed an Internet-wide scan of all IPv4 addresses on port 500 to determine which hosts were configured 16This defect was corrected quite recently, years after the version of OpenSSL ScreenOS uses was written|,Data,6
|, download from an external source). Running on 71,000 articles collected from 45 leading technical blogs, this new approach demonstrates a remarkable performance: it gener- ated 900K OpenIOC items with a precision of 95% and a coverage over 90%, which is way beyond what the state-of-the-art NLP tech- nique and industry IOC tool can achieve, at a speed of thousands of articles per hour|,Data,7
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
| To structure our efforts, we followed a multi- step process. First, we collected exploits from various online databases and exploit frameworks, including Metasploit (22 exploits)3, Exploit-DB (2)4, Packet Storm (5)5, from the security research company Security Explorations (52)6, and an online repository for Java exploits7|,Data,11
 5. ANALYSIS AND FINDINGS In the following we use the extensive documentation of the 61 minimal exploits to provide insight into how attackers use specific vulnerabilities and features of the Java platform to implement their attacks,Data,11
| We run our event analysis on the top 100 free applications in the Android application store to determine how often this happens. In total, our analysis finds 1060 errors across 88 of the top 100 applications (10|,Data,12
| To our knowledge, AUTOREB is the first work that explores the user review information and utilizes the review semantics to predict the risky behaviors at both review-level and app-level. We crawled a real-world dataset of 2, 614, 186 users, 12, 783 apps and 13, 129, 783 reviews from Google play, and use it to comprehensively evaluate AUTOREB|,Data,14
| 4.1 Data collection For each team, we collected a variety of observed and self- reported data|,Data,16
| To demonstrate this, we scraped greatfire.org for websites in the top 1000 Alexa websites that are blocked by the GFW|,Data,18
| (cid:15) Identifying New Vulnerabilities. Our tool successfully an- alyzed 1,591 service interfaces of all the 80 system services in Android 5|,Data,19
| To understand the scope and magnitude of this new XARA threat, we developed an ana- lyzer for automatically inspecting Apple apps’ binaries to deter- mine their susceptibility to the XARA threat, that is, whether they perform security checks when using vulnerable resource-sharing mechanisms and IPC channels, a necessary step that has never been made clear by Apple. In our study, we ran the analyzer on 1,612 most popular MAC apps and 200 iOS apps, and found that more than 88|,Data,24
| To assist software developers (or secu- rity analysts) in tracking down a memory corruption vulnerability, CREDAL also performs analysis and highlights the code fragments corresponding to data corruption. To demonstrate the utility of CREDAL, we use it to analyze 80 crashes corresponding to 73 memory corruption vulnerabilities archived in Offensive Security Exploit Database|,Data,25
| These techniques may be applicable in other scenarios. We implemented and evaluated the attacks against the popular Gmail and Bing services, in several environments and ethical experiments, taking careful, IRB-approved mea- sures to avoid exposure of personal information|,Data,26
|, CSPAutoGen can handle all the inline and dynamic scripts. We have implemented a prototype of CSPAutoGen, and our eval- uation shows that CSPAutoGen can correctly render all the Alexa Top 50 websites|,Data,27
| 5. EXPERIMENTAL RESULTS This section reports on our evaluation of the moments ac- countant, and results on two popular image datasets: MNIST and CIFAR-10|,Data,28
| 6.1 Mobility Trace Dataset We use the CRAWDAD dataset roma/taxi [2, 3] for our simu- lations|,Data,31
 6.1 Evaluation We evaluated the performance of Σoφoς using 4 data sets of increasing size and also the English Wikipedia,Data,33
|1 Datasets, Metrics, Competitors & Settings Datasets. We test EpicRec on two real-world datasets: MovieLens1: a movie rating dataset collected by the Grou- pLens Research Project at the University of Minnesota through the website movielens|,Data,36
| 1 http://grouplens.org/datasets/movielens 188Yelp2: a business rating data provided by RecSys Chal- lenge 2013, in which Yelp reviews, businesses and users are collected at Phoenix, AZ metropolitan area|,Data,36
| The number of movie categories is 18. We use the MovieLens- 1M, with 1000,209 ratings from 6,040 users on 3,883 movies|,Data,36
| Our goal is to show that an ad- versary can insert an unbounded number of Sybil identities in the SybilLimit protocol, breaking its security guarantees. For our evaluation, we consider a real-world Facebook inter- action graph from the New Orleans regional network [28]|,Data,38
| We utilize these papers to extract Android malware behaviors and to construct the semantic network. From the electronic proceedings distributed to conference participants, we collect the papers from the IEEE Sympo- sium on Security and Privacy (S&P’08–S&P’15)4, the Com- puter Security Foundations Symposium (CSF’00–CSF’14), and USENIX Security (Sec’11)|,Data,39
 We conduct experiments on two publicly available set-valued datasets. • AOL search log dataset [1],Data,45
 90% of the users have fewer than 84 keywords in their logs. • Kosarak dataset [2],Data,45
 We select one month of data for our study. The data logs we used are col- lected from more than 30 machines with various server mod- els and operating systems,Data,46
| This paper rigorously investigates how users’ security beliefs, knowledge, and demographics corre- late with their sources of security advice, and how all these factors influence security behaviors. Using a carefully pre- tested, U|,Data,48
 We have ported Valgrind to iOS and implemented a prototype of iRiS on top of it. We evaluated iRiS with 2019 applications from the official App Store,Data,54
| from manufacturing equipment, as shown in Figure 1. We capture the relevant sensor data by deliberately or accidentally placing an attack-enabled phone close to, on top of, or inside a piece of manu- facturing equipment while the machinery is fabricating the target object|,Data,55
| Our new metric helps us compare in a fair way previously proposed attack-detection mechanisms. (ii) We compare previous attack-detection proposals across three di↵erent experimental settings: a) a testbed operating real-world systems, b) network data we collected from an operational large-scale Supervisory Control and Data Acqui- sition (SCADA) system that manages more than 100 Pro- grammable Logic Controllers (PLCs), and c) simulations|,Data,57
| Evaluation. We ran Oyente on 19, 366 smart contracts from the first 1, 460, 000 blocks in Ethereum network and found that 8, 833 contracts potentially have the documented bugs|,Data,58
| First, we consolidate the eight origin-exposing vectors into one auto- mated origin-exposing system called Cloudpiercer. Then, we assemble a list of clients from five CBSP companies by studying their DNS configurations and obtaining their adop- tion rate across the Alexa top 1 million websites|,Data,59
| The vast majority of them were exposed through their A record, indicating a brief dis- abling of the protection system. SSL certificate exposure In order to find IP addresses hosting SSL certificates associ- ated with the domains in the evaluation set, we made use of the publicly available data of Rapid7’s Project Sonar [42]|,Data,59
| 4. LARGE-SCALE ANALYSIS To assess the magnitude of the origin-exposure problem, we conduct a large-scale analysis in which we attempt to uncover the origin of CBSP-protected domains|,Data,59
|1 Dataset Description The dataset was first presented and used by Keller et al. in [23], and is publicly available in the gene expression om- nibus (GEO) database under reference GSE61741|,Data,61
| Although the cost of stor- age and processing have diminished, the cost of maintaining reliable infrastructure for transaction logs is still noticeable. Figure 1: A plot of transaction fee versus frequency for 1 million transactions in May 2015|,Data,65
| To estimate the cost of producing the preprocessing data (multiplication triples, random bits etc.), we used figures from the recent MASCOT protocol [31], which uses OT ex- tensions to obtain what are currently the best reported triple generation times with active security|,Data,67
| In this section, we validate whether the smartphone’s acoustic data can be utilized to deduce the movements. To conduct the validation, we implement an application on Nexus 5 (Android OS v6|,Data,68
| As seen in Table 4, we found that about half of the servers in Alexa’s top 10 support a large number of requests without rekeying. For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client|,Data,72
| For a better estimate of the number of vulnerable servers, we tested servers from Alexa’s top 10k that negotiate 3DES with a modern client. We identified 11483 different HTTPS servers11, and found that 226 of them (1|,Data,72
| In this paper, we study the possible techniques to detect and measure this fraud and evaluate the real impact of OTT bypass on a small European country. For this, we performed more than 15,000 test calls during 8 months and conducted a user study with more than 8,000 users|,Data,78
|, the server cannot learn their relative order) after some number of queries are performed over real-world data. Specifically, we ran an experiment where we inserted over 2 million public employee salary figures from [1] and then performed 1000 random range queries|,Data,79
| In this study, we are interested in finding answers to security- and privacy-related questions about libraries, such as “How prevalent are third- party libraries in the top apps and how up-to-date are the library versions?”, “Do app developers update the libs included in their apps and how quickly do they update?”, or “How prevalent are vulnerabilities identified in prior research [28, 9] in libraries and how many apps are affected?” To answer these questions, we first built a comprehensive repository of third-party libraries and applications (see Section 5). Our library set contains 164 libraries of different categories (Ad- vertising, Cloud,|,Data,84
|) and a total of 2,065 versions. We then collected and tracked the version histories for the top 50 apps of each category on Play between Sep 2015 and July 2016, accumulating to 96,995 packages from 3,590 apps|,Data,84
|6.1, we found in our sample set 360 affected packages from 23 distinct apps, when only considering exact library matches|,Data,84
|15 for Android, which contained an account hijacking vulnerability, on 06/11/2014. In the histories of our sample set apps, we discovered, in total, 394 affected packages from 51 distinct apps, when only considering packages with exact matches of the vulner- able lib version|,Data,84
| We used LibScout to detect the affected application packages in our data set. In total 2,667 app versions of 296 distinct apps with a cumu- lative install-base of 3|,Data,84
| We observed that there is a significant variance in ACFG size. To reduce the sampling bias, we first collect a dataset which covers ACFGs of different functions from various architec- tures|,Data,89
| This dataset was used for base- line comparison, and all functions in this dataset has known ground truth for metric validation. We prepared this dataset using BusyBox (v1|,Data,89
| Dataset II – Public dataset. Recent work such as Pewny et al [45] and Eschweiler et al [23] used the same public dataset based upon two publicly-available firmware images for baseline comparison [7, 8]|,Data,89
| Dataset III – Firmware image dataset. This dataset of 33,045 firmware images was collected from the wild|,Data,89
| As a result, we created a freely available vulnerability database for this effort and for the broader research community. To build this database, we mined official software websites to collect lists of vulnerabilities with the corresponding CVE num- bers|,Data,89
| We selected OpenSSL for demonstration, since it is widely used in IoT devices. The resulting vulnerability database includes 154 vulnerable functions|,Data,89
| Roughly speaking, our measurement methods can be divided into two kinds: those that could be fully automated and scaled eas- ily, and those that required some manual interaction. For the latter, we used a set of 302938 major email providers and email genera- tors, while for the former, we used a much larger set of a million popular providers occurring in the Adobe leak and the Alexa top million Web sites (as potential email generators)|,Data,90
1.2 Provider List We created the set of popular email providers based on the top 1 million email address domains occurring in the leaked Adobe user data set of September 2013,Data,90
| Using a combination of mea- surement techniques, we determine whether major providers sup- ports TLS at each point in their email message path, and whether they support SPF and DKIM on incoming and outgoing mail. We found that while more than half of the top 20,000 receiving MTAs supported TLS, and support for TLS is increasing, servers do not check certificates, opening the Internet email system up to man- in-the-middle eavesdropping attacks|,Data,90
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
|26 and are configured with 4G RAM and 2 virtual processors. The VMs for TorA run on a workstation and are connected to a campus wired network, whereas the VMs for TorB and TorC are run on a laptop and connect to a home wired network, Each of these three datasets contains 30,000 traces collected as follows: (1) For each target obfuscator, we used our trace collection framework to visit Alexa Top 5,000 websites to collect 5,000 traces (labeled as obfs3, obfs4, fte, meekG, and meekA, corresponding to obfsproxy3, obfsproxy4, FTE, meek-google, and meek-amazon respectively); (2) In addition, we visited the same set of websites without Tor and obfuscators to collect 5,000 traces and labeled them as nonTor|,Data,91
| 3.1 Datasets We use two major types of datasets: (1) packet-level traffic traces collected at various locations in a campus network, and (2) packet-level traces for Tor Pluggable Transport traffic collected in controlled environments|,Data,91
 Evaluation: local mixing time in social graphs. We use 10 various large-scale real-world social network topolo- gies that mainly come from the Stanford Large Network Dataset Collection [23] and other sources [45] to evaluate the local mixing time for nodes in social graphs,Data,92
| Feature Functions and Weights. To learn all feature functions and weights, we downloaded 1784 non-obfuscated Android applications from F-Droid [3], a popular repository for open-source Android applications|,Data,93
2.2 Experiments with Malware Samples We randomly selected one sample from each of the 49 mal- ware families reported in [40],Data,93
1_r1). Apps in our dataset used for the case study are downloaded from the Google official market (Google Play) in May 2016,Data,95
| • Using SInspector, we perform the first study of Unix domain sockets on Android, including the categoriza- tion of usage, existing security measures being en- forced, and common flaws and security implications. We analyze 14,644 apps and 60 system daemons, find- ing that 45 apps, as well as 9 system daemons, have vulnerabilities, some of which are very serious|,Data,98
| We presented SInspector, a tool for discovering potential security vulnerabilities through the process of identifying socket addresses, detecting authen- tication checks, and performing data flow analysis on na- 90tive code. We analyzed 14,644 Android apps and 60 system daemons, finding that some apps, as well as certain system daemons, suffer from serious vulnerabilities, including root privilege escalation, arbitrary file access, and factory reset- ting|,Data,98
| Our results show that many of our attacks succeed with a 100% chance such that the Sound-Proof cor- relation algorithm will accept the attacked audio samples as valid. Third, we collect general population statistics via an online sur- vey to determine the phone usage habits relevant to our attacks|,Data,100
 We find that the larger width of integer types and the increased amount of addressable memory introduce previously non-existent vulnerabilities that often lie dormant in program code. We empirically evaluate the prevalence of these flaws on the source code of Debian stable (“Jessie”) and 200 popular open-source projects hosted on GitHub,Data,104
| We have applied UniSan to the latest Linux kernel and Android kernel and found that UniSan can successfully prevent 43 known uninitialized data leaks, as well as many new ones. In particular, 19 of the new data leak vulnerabilities in the latest kernels have been confirmed by the Linux community and Google|,Data,107
| This allows us to prevent replay attacks, which are possibly the most applicable attack vectors against biometric authentication. Using a gaze tracking device, we build a prototype of our system and perform a series of systematic user experiments with 30 participants from the general public|,Data,108
| If two commits were blamed for the same amount of lines, blame both. Our heuristic maps the 718 CVEs of our dataset to 640 VCCs|,Data,109
| However, improving our blame heuristics further is an interesting avenue for future research. Apart from the 640 VCCs, we have a large set of 169,502 unclassified commits|,Data,109
|9 The SVM detected a high amount of excep- tions, a high number of changed code, inline ASM code, and variables containing user input such as __input and user. 6As previously mentioned we use the years 2011–2014 as the test dataset, since we have ground truth data on which to base the discussion|,Data,109
| When given a source file, Flawfinder returns lines with suspected vul- nerabilities. It offers a short explanation of the finding as well as a link to the Common Weakness Enumeration (CVE) database|,Data,109
| The paper makes three contributions. First, we conducted the first large-scale mapping of CVEs to GitHub commits in order to create a vulnerable commit database|,Data,109
| Our results show that our approach significantly outperforms the vulner- ability finder Flawfinder. We created a large test database containing 66 C and C++ project with 170,860 commits on which to evaluate and compare our approach|,Data,109
 VoiceLive takes advantages of the user’s unique vocal system and high quality stereo recording of smartphones. • We conduct extensive experiments with 12 participants and three different types of phones under various ex- perimental settings,Data,111
| To test if WebCapsule can successfully record and subsequently replay real-world phishing attacks, we proceeded as follows, us- ing Chromium on our desktop machine. We selected a large and diverse set of recently reported phishing web pages from Phish- Tank8|,Data,112
| 2.4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime|,Data,113
|4 Datasets and implementation We use two real geographic datasets Cal, SpitzLoc, one synthetic geographic distribution Globe, and one real time- stamp dataset SpitzTime. The dataset Cal represents the latitude and longitude of about 21,000 intersections in the California road network1 (also used by Mavroforakis et al|,Data,113
294258. The dataset SpitzLoc consists of latitude and longitude coordinates tracking the movement of German Green party politician Malte Spitz over six months,Data,113
| In this section, we aim to explore whether the differences of keystroke wave- forms are large enough to be used for recognizing different keys inputs in the real-world setting. We collected training and testing data from 10 volunteers|,Data,114
 B. Real Attacks MAD uniformly detects attacks more quickly than the PAD; we use the former method to detect the presence of an attack in real Internet traces3,Data,119
 III. DATA SET  changes  The data used was the PREDICT ID USC-Lander!  (- 60  The total  were DNS attack packets,Data,120
|395326000  files IPs. There are total 59,928,920 packet counts out of which there was a total of  DoS_DNS_amplification-20130617 (2013-06-17) (2013-06-17) with anonymized million) 358019 DNS packets|,Data,120
| The maximum number of unique hosts per day we measured was 106,000. To understand these differences, we compared the observations from our network monitor to data collected from DShield (www|,Data,121
| 3.1 From our own transactions We engaged in 344 transactions with a wide variety of services, listed in Table 1, including mining pools, wallet services, bank ex- changes, non-bank exchanges, vendors, gambling sites, and mis- cellaneous services|,Data,122
| Wallets. We kept money with most of the major wallet services (10 in total), and made multiple deposit and withdrawal transac- Bank exchanges|,Data,122
|, in which the exchange rate is not fixed) also function as banks. As such, we tagged these services just as we did the wallets: by depositing into and withdrawing from our accounts (but rarely par- ticipating in any actual currency exchange)|,Data,122
|info/tags, including both addresses provided in users’ signatures for Bitcoin forums, as well as self-submitted tags. We collected all of these tags — over 5,000 in total — keeping in mind that the ones that were not self-submitted (and even the ones that were) could be regarded as less reliable than the ones we collected ourselves|,Data,122
| 3.1 Data analysis overview We use three data sets, summarized in Table 1|,Data,123
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
|1 PlanetLab Deployment We deployed tracebox on PlanetLab, using 72 machines as vantage points (VPs). Each VP had a target list of 5,000 items build with the top 5,000 Alexa web sites|,Data,124
| We also describe our application of the technique to the IPv6 interface-level graph captured by CAIDA’s Archipelago (Ark) infrastructure [14] for March 2013. The graph consists of all the 52,986 IPv6 interfaces numbered within the 2000::/3 unicast prefix captured from all 27 Ark vantage points (VPs) with IPv6 connectivity|,Data,125
| cause the counters of distinct routers to diverge, and (4) confirm aliases with pairwise probing. Given the absence of velocity in ID counters and the large probes required for the technique to work, we probe at a low rate of 20pps from a single VP, producing 26Kbps of traffic|,Data,125
| 3. METHODOLOGY In this section, we describe the design of our experiment and our data collection methodology, as well as the mitigating steps and proactive measurements we conducted to ensure a minimal im- pact of our covering routes|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| of IPs 1622 1219 159 9,409 9 12,418 No. of Unique ASNs 603 530 62 3,654 8 4,857 In order to validate minimal impact on data plane connectivity, we performed the following: We collected a set of public IPv6 addresses by querying the Alexa top 1M domains [2] for AAAA records|,Data,126
| Our IPv6 network telescope results suggest sev- eral important differences (and some similarities) compared to that body of work. To produce a more recent and valid comparison, we analyzed a single week of IPv4 background radiation captured during the course of our ongoing IPv6 packet capture|,Data,126
| 4. DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1|,Data,127
| DATA COLLECTION In this section we describe the datasets used in our analysis, which we summarize in Table 1. Our primary dataset consists of changes made to the |,Data,127
| domains, (2) the removal of existing domains, and (3) changes to existing domains in terms of revisions to their associated name- servers. Our data includes captures of the DNZA files as recorded every five minutes, time periods we refer to as epochs|,Data,127
| Since we lack comprehensive ground truth regarding the ultimate use of domains, to this end we use two proxies: subsequent appearance of a newly registered do- main in: (1) an email spam campaign, or (2) a domain blacklist. For the first of these, we operated a spam trap, i|,Data,127
|com), by restricting our focus to domains recently registered (March–July 2012) we can filter down the do- mains appearing in the spam trap to those very likely used for spam- ming. For the second, we subscribed to three major DNS blacklists, URIBL, SURBL, and Spamhaus DBL|,Data,127
| In this paper, we examine the effectiveness of these inter- ventions in the context of an understudied market niche, counterfeit luxury goods. Using eight months of empirical crawled data, we identify 52 distinct SEO campaigns, document how well they are able to place search results for sixteen luxury brands, how this ca- pability impacts the dynamics of their order volumes and how well existing interventions undermine this business when employed|,Data,128
| For a small number of stores, we were also able to collect user traffic data that directly measures an SEO campaign’s effectiveness in attracting customers to their stores. Specifically, we were able to periodically collect AWStats data for 647 storefronts in 12 cam- paigns|,Data,128
| One issue that undermines coverage is that Google only labels the root of a Web site as “hacked”, and does not label search results that link to sub-pages within the same root domain. In the PSR data set, we found 68,193 “hacked” search results|,Data,128
| We begin by exam- ining the properties of individual darknets and in particular the behavior of source IP addresses. We provide these char- acterizations by looking at data from 14 darknet monitors ranging in size from a /25 monitor to a /17 monitor over a period of 10 days between August 18, 2004 and August 28, 2004|,Data,129
| Figure 10: The number of darknets (of 31) reporting a port in the top 10 ports over a day, week, and month time frame. The analysis is performed for the top 10 destination ports over a day, top 10 destination ports over a week, and top 10 destination ports over a month|,Data,129
| 3.6 Datasets This paper uses DNS datasets from three authorities: one national-level top-level domain, operators of two root servers as shown in Table 1|,Data,130
 JP-DNS operates the .jp country code domain for Japan; we have data from all seven of their anycast sites,Data,130
|) part of the 2014 DITL collection [16] (for B-Root, shortly after 2014 DITL). We also use data for M-Root’s 2015 DITL collection (§ 4|,Data,130
 These root datasets are available to re- searchers through DNS-OARC. For longitudinal analysis we draw on 9 months of data taken at the M-Root server,Data,130
| However, we treat the union of these classes together. We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness|,Data,131
| We use data from 103 surveys taken between April 2006 and February 2015, and performed initial studies based on 2011–2013 data, but focus on the most recent of them, in January and February of 2015 for data quality and time- liness. The dataset consists of all echo requests that were sent as part of the surveys in this period, as well as all echo responses that were received|,Data,131
|, “host unreachable”); we ignore all probes as- sociated with such responses since the latency of ICMP error responses is not relevant. In later sections, we will complement this dataset with results from Zmap [5] and additional experiments includ- ing more frequent probing with Scamper [13] and Scrip- troute [22]|,Data,131
| 3.2 Milking 3 Methodology To collect the information needed to cluster servers into oper- ations, we have built an infrastructure to track individual exploit servers over time, periodically collecting and classi- fying the malware they distribute|,Data,132
 2. We receive feeds of drive-by download URLs (Sect,Data,132
 2. CHARACTERISTICS OF CHECK-INS We use three different datasets that capture human mobility,Data,133
 First we consider two online location-based social networks. We col- lected all the public check-in data between Feb,Data,133
| There are 196,591 nodes, 950,327 edges in Gowalla and 58,228 nodes, 214,078 edges in Brightkite. To ensure that our observations on human movement are not specific to data based on check-ins from location-based social net- works, we also include a dataset of cell phone location trace data|,Data,133
| Backscatter DDoS is a commonly seen behaviour in darknets where the attacker uses simultaneous bots to generate the actual attack packets to reach the targeted (original) victim. In our study, five publicly available network traffic datasets from CAIDA’s archives are employed|,Data,134
| Datasets Employed In this research, five publicly available real-life network traffic traces (datasets) from CAIDA’s archives are employed. Three of them, which were captured by a passive darknet in 2007, 2008 and 2012 [27][26][28], namely UCSD Network Telescope [21], include mostly one-way malicious traffic while the remaining ones collected in 2008 [29] and 2014 [30] via CAIDA’s Internet backbone links include only normal traffic|,Data,134
| 3 Approach This section presents our approach for the evalua- tion of reputation based blacklists. We evaluated the blacklists by deploying them in a large academic net- work of over 7,000 hosts|,Data,135
| This was a preliminary step to preventing inexperienced and non-serious workers from participating in our survey. Our survey is based on the participants’ actual check-ins on Foursquare posted over the last 24 months (that we collected through a specific application we developed), and it requires a significant amount of time to complete (30-45 minutes)|,Data,136
| The third phase of worm activ- ity is the persistence phase which for the Blaster worm has continued through 2004. In this one-week period of measurement, the IMS system observed over 286,000 unique IP addresses displaying the characteristics of Blaster activity|,Data,137
| published a study in 2011 that focused on the dynamics of leaf cer- tificates and the distribution of certificates among IP addresses, and attempted to roughly classify the overall quality of served certifi- cates. The study was based on regular scans of the Alexa Top 1 Mil- lion Domains [1] and through passive monitoring of TLS traffic on the Munich Scientific Research Network [17]|,Data,138
| Our study is founded on what is, to the best of our knowledge, the most comprehensive dataset of the HTTPS ecosystem to date. Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443|,Data,138
| Between June 2012 and August 2013, we completed 110 exhaustive scans of the public IPv4 address space in which we performed TLS handshakes with all hosts publicly serving HTTPS on port 443. Over the course of 14 months, we completed upwards of 400 billion SYN probes and 2|,Data,138
| Content Provider e Service Provider v i t c e p s r e P Content Consumer Addressing Prerequisite IP Functions Routing Naming A1: Address Allocation; A2: Address Advertisement N1: Nameservers; R1: Server Readiness N2: Resolvers N3: Queries A2: Address Advertisement; T1: Topology End-to-End Reachability R1: Server Readiness Operational Characteristics Usage Profile Performance U3: Transition Technologies U1: Traffic Volume; U3: Transition Technologies P1: Network RTT R2: Client Readiness U2: Application Mix; N3: Queries Table 2: Dataset summary showing the time period, scale, and public or new status of the datasets we analyzed. Dataset RIR Address Allocations Routing: Route Views Routing: RIPE Google IPv6 Client Adoption Verisign TLD Zone Files CAIDA Ark Performance Data Arbor Networks ISP Traffic Data Verisign TLD Packets: IPv4 Verisign TLD Packets: IPv6 Alexa Top Host Probing Time Period Metrics Jan 2004 – Jan 2014 A1 Jan 2004 – Jan 2014 A2, T1 Jan 2004 – Jan 2014 A2, T1 Sep 2008 – Dec 2013 R2, U3 Apr 2007 – Jan 2014 N1 P1 Dec 2008 – Dec 2013 U1, U2, U3 Mar 2010 – Dec 2013 Jun 2011 – Dec 2013 N2, N3 N2, N3 Jun 2011 – Dec 2013 Apr 2011 – Dec 2013 R1 Recent Scale ≈18K allocation snapshots (5 daily) 45,271 BGP table snapshots millions of daily global samples daily snapshots of ≈2|,Data,139
com & .net) ≈10 million IPs probed daily ≈33-50% of global Internet traffic; 2013 daily median: 50 terabits/sec (avg,Data,139
| To put the IPv6 allocation data in context, Figure 1 also shows IPv4 prefix allocations over the same period. The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled|,Data,139
| There were less than 30 IPv6 prefixes al- located per month prior to 2007, generally increasing thereafter. In the past several years, we typically find more than 300 prefixes allocated per month, with a high point of 470 prefix allocations in February 2011|,Data,139
| The number of IPv4 prefix allocations grows from roughly 300 per month at the begin- ning of our observation period to a peak of 800–1000 per month at the start of 2011, after which it drops to around 500 per month in the last year, as the number of available addresses at RIRs has dwindled. 1 Overall, we find nearly 69K IPv4 prefix allocations at the beginning of our dataset and just over 136K at the end|,Data,139
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| We deployed this detection mechanism on an Alexa top 10 website, Facebook, which terminates connections through a diverse set of network operators across the world. We analyzed 3, 447, 719 real-world SSL connections and successfully discovered at least 6, 845 (0|,Data,140
| Table 1 shows the datasets we use in our paper. We use two ICMP surveys taken by USC [12]: IT17ws and IT16ws; IT17ws is the main dataset used in this paper, while we use IT16ws for validation in Section 6|,Data,142
2. We collected VUSC s at our enterprise in order to compare our inferences with network operators as discussed in Section 6,Data,142
| # of Data-Oriented Attacks gives the number of attacks generated by FLOWSTITCH, includ- ing privilege escalation attacks and information leakage attacks. FLOWSTITCH generates 19 data-oriented attacks from 8 vulnerable programs|,Data,144
| Third, this method is not specific to C or C++, and can be applied to any programming language. We collected C++ source of thousands of contestants from the annual international competition “Google Code Jam”|,Data,145
| Finally, we analyze various attributes of programmers, types of programming tasks, and types of features that appear to influence the success of attribution. We identified the most important 928 fea- tures out of 120,000; 44% of them are syntactic, 1% are layout-based and the rest of the features are lexical|,Data,145
|3.1ScalingWecollectedalargerdatasetof1,600programmersfromvariousyears|,Data,145
| ) s y a D n i (    e m T i  7  6  5  4  3  2  1  10  20  30  40  50  60  70  80  90 Time Before Accounts Suspension Number of IP Addresses 2 Motivation: Analysis of Malicious Activ- ity on a Webmail Service We want to understand the way in which cybercrimi- nals abuse accounts on online services, to identify weak points that we could leverage for detection. To this end, we observed the email-sending activity on a large web- mail service|,Data,147
| Following accepted frameworks for qualitative research [18, 30, 35], we focus closely on a small number of participants. We interviewed 15 journalists employed in a range of well-respected journalistic institutions in the United States and France, analyzing these interviews using a grounded theory approach [18, 30]|,Data,146
| 3.1 Datasets We examine 13,345 passwords from four sets created under composition policies ranging from the typical to the currently less common to understand the suc- cess of password-guessing approaches against passwords of different characteristics|,Data,149
| Had we used any major password leak, their analysts would have already been familiar with most or all of the passwords contained in the leak, biasing results. The passwords in these sets were collected using Ama- zon’s Mechanical Turk crowdsourcing service|,Data,149
| The decision for or against pinning is always a trade- off between increasing security and keeping mainte- nance efforts at an acceptable level. In this paper, we present an extensive study on the applicability of pinning for non-browser software by analyzing 639,283 Android apps|,Data,152
| Therefore, we instrument telemetry data from a popular anti-virus software provider. We evaluate the update behaviour of 871,911 unique users from January 2014 to December 2014 and find that only 50% of the users update to a new app version within the first week after release|,Data,152
| Developer View Although pinning is only ap- plicable in relatively few cases, the nominal-actual comparison leaves room for improvement. We there- fore collected feedback from 45 developers of apps for which we would recommend pinning|,Data,152
| Section 4). Altogether we found 20,020,535 calls to network related API calls (cf|,Data,152
| Instability of the routes to the sensor address space can also result in reachability problems, especially given that route flap damping can be triggered during convergence to suppress unstable routes [9]. Using the BGP updates data from RouteViews BGP monitor, we studied the availability of the routes to the sensor blocks in our de- ployment from a large set of ASes|,Data,154
| This section probes these differences using three successively more specific views of traffic to a network of distributed blackhole sensors. The data was recorded over a one month period with SYN responders on TCP port 135, 445, 4444, and 9996 across all sen- sors|,Data,154
|  V. EXPERIMENT RESULTS  In this section, we mainly focus on how our router-to-AS Mapping method and other baseline methods behave on global router-level topology, as discussed above, we use PeeringDB data as ground truth, and apply clustering method on global topology based on CAIDA ITDK project|,Data,155
| It describes the properties that a dataset should have in order to be used for comparison purposes. The dataset used in the paper includes an IRC-based Botnet attack1, but the bot used for the attack was developed by the authors and therefore it may not represent a real botnet behavior|,Data,156
| This dataset may be downloaded with authorization. The Protected Repository for the Defense of Infrastructure Against Cyber Threats (PRE- DICT) indexed three Botnet datasets2 until May 16th, 2013|,Data,156
 None of them are labeled. A custom botnet dataset was created to verify five P2P botnet detection algorithms in Saad et al,Data,156
| Unfortunately, there is only one infected machine for each type of botnet, therefore no synchronization analysis can be done. The Traffic Laboratory at Ericsson Research created a normal dataset that was used in Saad et al|,Data,156
 This is the only normal dataset that is labeled inside the pcap file. A considerable amount of malware traffic in pcap format was published in the Contagio blog9,Data,156
| But since each scenario includes only one infected computer, it should be possible to label them. Another dataset with malware logs and benign logs was collected in NexGinRC (2013)|,Data,156
 Access to this dataset may be granted upon request10. The last dataset analyzed is currently created by the MAWI project described in Cho et al,Data,156
| Methodology and datasets We deployed Paris Traceroute with its Multipath Detection Algorithm (MDA) [29] enabled in 90 PlanetLab nodes. We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]|,Data,158
| We configured each node to trace IP-level routes toward 10 thou- sand destinations selected at random from a list of 102,404 reachable destinations in different /16 prefixes we obtained from the PREDICT project [11]. Our dataset contains more than 900 thousand IP-level (multi)routes and 324,313 IP addresses|,Data,158
1 3.1 Address Allocation and BGP Data We analyzed BGP announcements captured by all collectors (24 collectors peering with 184 peers) of the Routeviews [3] and RIPE RIS [52] projects,Data,159
| For each /24 block, we computed the maximum number of peers that saw it reachable at any time within the full observation period of 92 days. To determine which address blocks are available for assignment, we used a dataset compiled by Geoff Hus- ton [23], which merges the extended delegation files from the 5 RIRs [4, 6, 7, 41, 51] with IANA’s published registries [31–36]|,Data,159
| SWITCH. We collected unsampled NetFlow records from all the border routers of SWITCH, a national aca- demic backbone network serving 46 single-homed uni- versities and research institutes in Switzerland [55]|,Data,159
| R-ISP. We collected per-flow logs from a vantage point monitoring traffic of about 25,000 residential ADSL customers of a major European ISP [21]|,Data,159
 UCSD-NT. We collected full packet traces from the /8 network telescope operated at the University of Cal- ifornia San Diego [1],Data,159
| IXP. Our fourth VP is a large European IXP inter- connecting more than 490 networks, exchanging more than 400 PB monthly [5]|,Data,159
|3 Active Measurements ISI. We used the ISI Internet Census dataset it55w- 20130723 [37], obtained by probing the routed IPv4 address space with ICMP echo requests and retaining only those probes that received an ICMP echo reply from an address that matched the one probed (as rec- ommended [38])|,Data,159
| HTTP. We extracted IP addresses from logs of Project Sonar’s HTTP (TCP port 80) scan of the entire IPv4 address space on October 29, 2013 [24]|,Data,159
| Definitions of graph parameters measuring metric tree-likeness of a graph, as well as notions and notations local to a section, are given in appropriate sections. 3 Datasets Our datasets come from different domains like Internet measurements, biological datasets, web graphs, social and collaboration networks|,Data,160
| The experiments were executed as follows. Traces were col- lected by using ICMP, UDP, and TCP Traceroute to probe the paths to a set of 100 destination websites from a source located on the Pennsylvania State University, University Park campus|,Data,161
| For UDP and TCP Traceroute, traces were collected using the default destination port numbers. We also collected traces using other ports and observed similar results|,Data,161
| Realistic Networks Here we compare the merged topologies produced by iTop, MN, and Isomap for realistic topologies. We use the Au- tonomous System (AS) topologies from both the Rocketfuel [20] and the CAIDA [21] projects, which represent IP-level connections between backbone/gateway routers of several ASes from major Internet Service Providers (ISPs) around the globe|,Data,161
| Although the paris-traceroute output of ITDK is more reliable than that of IPlane’s traceroute, the random selection of endpoints implemented by CAIDA hinders the collection of routes between the same vantage- and endpoints. Therefore we used the data of IPlane’s traceroute measurements|,Data,162
| They can also be used for constructing maps of the Internet at the Autonomous Systems level [, ]. In this work we used the CAIDA router-level Internet map from October th,  []|,Data,163
| 3 Table 1: Dataset Description Name BGP Usage AS Geolocation; Detour Detection Date 2016-01 Sources Info RouteViews, RIPE 38,688 RIBS, 416 peers, RIS 30 countries, 55GB Infrastructure IP List AS Geolocation 2016-01 to 2016-03 CAIDA Ark, iPlane, OpenIPMap, RIPE Atlas Measurements 3M Router IPs Infrastructure IPs to AS Mapping Infrastructure IP geolocation 2015-08 CAIDA ITDK, iPlane 6.6M IP to AS mappings AS to IXP Mapping AS Relationship AS Geolocation 2016-01 to 2016-03 Filtering peered paths from detection 2016-01 Traceroute Detour Validation 2016-05-01 IXP websites, PeeringDB, PCH CAIDA AS Relationship RIPE Atlas MaxMind Prefix Geolocation; Detour Validation 2016-01, 2016-03 MaxMind GeoLite City (free and paid) 368 IXP websites crawled 482,657 distinct relationships Used by Netra, 163 traceroutes Paid version used only for geolocating infrastructure IPs and detour validation longest prefix match on the global routing table and map the IP to the AS announcing the longest matching prefix|,Data,164
| As shown in Figure 3, we install LaBrea on a /29 subnetwork and use PlanetLab [9] to probe from multiple vantage points the entire /24 aggre- gate to which the /29 belongs. We scan the /24 network by attempting to establish TCP connections to each IP address in the subnet and capture the packets for further analysis|,Data,165
| • Active IPs in a Subnet: Intuitively, we might ex- pect high-occupancy subnets to be good indicators of pos- sible tarpits. To this end, we initially investigated using a hitlist of probable tarpits as inferred from the /24 subnets with more than 240 responding web hosts in the scans|,Data,165
| To facilitate large-scale scanning and avoid triggering anomaly detectors, degreaser uses permu- tation scanning [7, 12] to pseudo-randomly iterate through the IP address space when probing. Our real-world Internet scan, which probes at least one address in each /24 network in the Internet, discovers 107 different tarpit subnetworks (cid:20)(cid:24)(cid:25) ranging in size from /16 (with up to 216 fake hosts) to /24 (with up to 28 fake hosts)|,Data,165
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
  III. DATA SET  The data used in this work was the PREDICT ID USC-Lander/ DoS_DNS_amplification-20130617 (2013- 06-17) to (2013-06-17) [26],Data,166
| • Discovering correlations between anomalous traffic types detected with deep inspection techniques and traffic feature entropy variations. • Providing a traffic-type dissection (in-depth and entropy based) of a representative portion of the IBR for three weeks of April, 2012, with a 10-minute time scope|,Data,167
 Following is the summary of information about these data sets:  1. Data set from PREDICT USA [24] which contains traces of a DNS distributed denial of service attack (DDOS),Data,168
  from optical  2. Data set from CAIDA USA [25] which contains internet internet connectivity from 2002 and 2003,Data,168
  3. Data set from our experiment in which a PCAP file is captured from a lab computer which is being used for browsing and software development for the cyber security project,Data,168
| If it is too long, patterns inside the packet stream may have changed during this long period For the detection of bottleneck traffic, we suggest using a segment length on the order of seconds Here we use a default value of 1 second and inves- tigate the impact of segment length in Section 75|,Non-data,117
| The sampling rate p is another important parameter Given a sampling rate p, the highest frequency that is ob- servable is p 2 according to the Nyquist Theorem If the sam- pling rate is too low, aliasing can occur If it is too high, it will increase both storage and processing overhead unnec- essarily|,Non-data,117
| A compromise has to be made between reducing the overhead and obtaining a better spectral representa- tion In this paper, we select a conservative sampling rate of 200kHz, which is sufficiently high to capture the peri- odic patterns of transmitting 1500-byte packets over an 100Mbps Ethernet link (8333 packets per second) A more thorough exploration of the trade-off for selecting a proper sampling rate is the subject of future work 5|,Non-data,117
| Building Intuition: Spectra of Bottleneck Traffic The final step in Figure 1 is to detect bottleneck links based on the traffic PSD In this section, we apply the techniques described in Section 4 to visually demonstrate the spectral characteristics of bottleneck traffic The goal is to develop intuition behind our automated detection al- gorithm While in this section we focus on graphical rep- resentations of the spectrum, in Section 6, we build upon intuition to design a quantitative algorithm to detect bot- tleneck traffic|,Non-data,117
| We begin with experiments in a simple con- trolled lab environment and then proceed to more complex wide-area experiments 51 Signatures in Controlled Lab Experiments When a link is congested, it sends packets out back-to- back|,Non-data,117
| Assuming for now that all packets are of the same size, this translates to a single periodic pattern in the spectral domain, with a period equal to the packet interarrival time The packet interarrival time is calculated as follows Interarrival T ime = P acket Size Link Bandwidth (3) The frequency of this pattern is the inverse of the inter- arrival time and it can be calculated as follows Base F requency = Link Bandwidth P acket Size (4) Our approach assumes that the bottleneck traffic is dom- inated by packets of a single size|,Non-data,117
| It is well established that a few common packet sizes dominate Internet traffic (see, for example, (Claffy et al, 1998), (Katabi and Blake, 2001), and (Sinha et al, 2006)) We believe bottlenecks will be particular prone to a few packet sizes, since they are typi- cally caused by either denial-of-service attacks or large or x 107 2 1|,Non-data,117
5 D S P 1 05 0 0 x 107 2 15 D S P 1 05 0 0 1 2 3 4 5 6 Frequency (Hz) 7 8 9 10 x 104 (a) Complete Spectrum 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000 Frequency (Hz) (b) Partial Spectrum Fig,Non-data,117
| 3 Spectral signature of a 100Mbps link saturated with a TCP flow many TCP flows Denial-of-service attacks today typically use fixed packet sizes, partially because random packet sizes would make them vulnerable to entropy analysis (Lakhina et al, 2005)|,Non-data,117
| TCP ensures that for bulk traffic, the majority of segments will be as large as the the maximum transmis- sion unit Only a mix of many short TCP flows will show significant variability in packet sizes, but studies show that most Internet traffic (by bytes) is in longer flows (for exam- ple, (Thompson et al, 1997)) For these reasons we focus on detecting the effects of a single dominant packet size, and the experiments in this paper primarily use the packet size 1500-byte for the bottleneck traffic|,Non-data,117
| However, our approach becomes less effective if our assumption about packet size is relaxed, and it would work poorly if the bottleneck traffic consists of packets with random sizes Our experiments use a very simple topology, where the sender and the receiver are directly connected through an Ethernet switch We select a switch instead of a hub be- cause a switch forwards traffic more efficiently and switches are far more widely used today We use tcpdump on the receiver side to gather packet traces|,Non-data,117
| We also use different Ethernet bandwidth (10 or 100Mb/s) and vary the trans- port protocol (TCP or UDP) to get the spectra under dif- ferent scenarios We use Iperf (Tirumala et al, 2003) to generate TCP and UDP packet streams, which mimic TCP file downloads or UDP CBR (Constant-Bit-Rate) traffic Each experiment lasts for 30 seconds|,Non-data,117
| We divide the trace into individual segments, each 1-second long and use the techniques in Section 4 to calculate the PSD for each seg- ment With the exception of the first segment, which in- cludes startup effects, we observe little variation in the spectra among the remaining 29 segments Thus, we only present results from an arbitrarily selected segment from the 29 segments 5|,Non-data,117
|11 TCP Traffic through a 100Mbps Bottleneck In the first experiment, we use a single Iperf TCP flow to saturate a 100Mbps switched Ethernet link with 1500-byte 5 D S P 6 5 4 3 2 1 0 x 105 0 100 200 300 400 500 600 700 800 900 1000 Frequency (Hz) Fig 4|,Non-data,117
| Spectral signature of a 10Mbps link saturated with a TCP flow packets We set the TCP socket buffer size to 128K bytes to ensure there is no window starvation and RTT was less than 1ms Under these conditions, TCP utilizes virtually the entire link bandwidth; we measured actual bit rates of around 999Mbps (including Ethernet header overhead)|,Non-data,117
| Figure 3(a) illustrates the complete measured spectrum of the packet stream along the Ethernet link A complete measured spectrum is the spectrum that shows the energy (or strength) at all observable frequencies from 0Hz up to the Nyquist limit (half the sampling rate), 100KHz in our case since we sample at 200KHz The amplitude at each fre- quency represents the power or strength of the frequency In Figure 3(a), we observe spikes around the 8130Hz base fre- quency and at multiples (harmonics) of this frequency|,Non-data,117
| The amplitude at 8130Hz in the PSD reaches nearly 17,000,000, while the amplitudes at other frequencies are much lower, especially at frequencies other than 8130Hz and its harmon- ics The important conclusion of this simple experiment is that the periodic patterns are very distinct and clearly vis- ible We refer to the 8130Hz base frequency as the bottleneck frequency To fully understand why there is strong energy at 8130Hz, we precisely calculate the frame size on the wire|,Non-data,117
| The Ethernet frame format, specified in IEEE 8023 2002 Standard (IEEE, 2002), specifies that each Ethernet frame has a 38-byte overhead, including an 8-byte preamble, a 6- byte destination MAC address, a 6-byte source MAC ad- dress, a 2-byte type/length field, a 4-byte CRC, and a 12- byte “Inter-Packet Gap” The max length of the Ethernet data payload is 1500-byte, which is what we used with the Iperf TCP flow in our experiment Hence the packet in- terarrival time for the Iperf flow according to Equation 3 is (1500 + 38) * 8 bit / 100Mbps = 0|,Non-data,117
12304ms The in- verse of 012304ms is a frequency of 812744Hz,Non-data,117
| However, since tcpdump (our packet capture tool) has a time reso- lution of 1 microsecond, the majority of the packet inter- arrival times stamped at 0123ms, resulting in strong en- ergy around 8130Hz (the inverse of 0123ms), in the actual spectral representation From this experiment, we can see that high energy around the 8130Hz base frequency and its harmonics is a strong indication of the presence of traffic through a 100Mbps bottleneck link|,Non-data,117
| We explain the presence of har- monics in Section 41 Since harmonics do not provide additional information, we focus mainly on the partial spectrum in the range 0Hz|,Non-data,117
10kHz in Figure 3(b) 512,Non-data,117
| TCP Traffic through a 10Mbps Bottleneck In the second experiment, we repeat the prior experiment but with a 10Mbps switched Ethernet link TCP traffic can also nearly fully utilize the link bandwidth with an actual bit rate very close to 10Mbps (including the Ethernet header overhead) Figure 4 shows the corresponding spectrum of the packet stream As expected, we see strong energy around 813Hz, a tenth of the previous experiment|,Non-data,117
| The PSD amplitude at 813Hz reaches 510,000, which is much lower than the amplitude in the previous experiment The lower amplitude is not surprising since the packet rate here is much lower Note that we excluded harmonics from this graph 5|,Non-data,117
|13 UDP Traffic through a 10Mbps Bottleneck To investigate how the spectrum changes with CBR traf- fic, we use Iperf to saturate the 10Mbps link We configure Iperf to send out 1472-byte UDP packets with a sending rate of 10Mbps|,Non-data,117
| The Ethernet frame data payload length is 1500 bytes after adding 28-byte UDP/IP headers We mea- sure 10Mbps (including Ethernet packet overhead), verify- ing that the UDP flow can fully utilize the link bandwidth Figure 5(a) depicts the spectrum of the Iperf UDP flow we observe a single peak at the 813Hz base frequency (again, we exclude harmonics)|,Non-data,117
| The PSD amplitude at 813Hz is 612,000, which is a bit higher than the amplitude for TCP in Figure 4 We believe the reason to be that packet transmission with UDP is more regular than TCP due to UDP’s lack of a window mechanism However, the difference in our experiments is not significant due to lack of competing cross traffic In the next experiment, we try a different packet size|,Non-data,117
| We again use Iperf to send 772-byte UDP packets at 10Mbps The Ethernet frame data payload length is 800 bytes after adding the 28-byte UDP/IP header The bit rate along the Ethernet link is still 10Mbps (including the Ethernet header overhead) The spectrum in Figure 5(b) shows a spike around 1492Hz, which agrees with Equation 4, with 10Mbps link bandwidth and 838-byte packet size (800-byte data payload plus 38-byte Ethernet overhead)|,Non-data,117
| In the PSD, the amplitude at 1492Hz reaches 1,944,000, which is more than three times the amplitude at 813Hz for the earlier experiment with 1500-byte packets in Figure 5(a) 514|,Non-data,117
| A Multi-Flow Bottleneck In the above experiments the bottleneck was satu- rated with a single Iperf flow In practice, however, high- bandwidth links are often saturated by many flows Our approach detects the characteristics of a saturated link, whether saturated by one flow or many, but becomes less effective if we can only observe some of the bottleneck traf- fic To understand this scenario better we conducted con- trolled lab experiments with the following two scenarios: (a) we observe all flows through the bottleneck, and (b) we observe only a fraction of the flows through the bottleneck|,Non-data,117
 6 x 105 8 6 4 2 D S P 0 0 x 106 2 15 D S P 1 05 0 0 100 200 300 400 500 600 700 800 900 1000 Frequency (Hz) (a) With 1472-byte UDP Packets 200 400 600 800 1000 1200 1400 1600 1800 2000 Frequency (Hz) (b) With 772-byte UDP Packets Fig 5,Non-data,117
| Spectral signature of a 10Mbps link saturated with different size UDP packets Fig 6 Peak Amplitude with increasing percentage of unobserved bottleneck traffic In case (a) the bottleneck traffic spectrum is similar to the case where a single flow saturates the bottleneck link The reason is that the bottleneck shapes packet transmis- sion based on bandwidth and packet size, regardless of how many flows are saturating the link|,Non-data,117
| Experiments supporting this observation are omitted here due to space limitations, but can be found in our technical report (He et al, 2005) Case (b) is more challenging as we do not observe all packets that carry the bottleneck signal We call the flows that traverse the bottleneck link but do not reach the ob- servation point unobserved bottleneck traffic; the fraction of unobserved traffic greatly affects our ability to detect the bottleneck|,Non-data,117
| Figure 6 shows the impact of unobserved bottleneck traf- fic In the test, we use one Iperf TCP flow and multiple web flows generated by a tool called Surge (Barford and Crov- ella, 1998) to saturate a 10Mbps switched Ethernet link Only the Iperf TCP flow reaches the observation point, while the web flows do not reach the observation point and thus serve as unobserved bottleneck traffic We vary the number of simulated web users in Surge to change the un- observed bottleneck traffic volume|,Non-data,117
 The X-axis of the graph shows the percentage of unob- served bottleneck traffic in terms of packet count out of all packets through the bottleneck in a 1-second window The Y-axis shows the peak amplitude of observed bottleneck traffic around the 800Hz frequency The high-level obser- vation is that as the percentage of unobserved bottleneck 7 x 104 15 D S P 10 5 0 0 100 200 300 400 500 600 700 800 900 1000 Frequency (Hz) (a) with an Iperf flow through a 10Mbps bottleneck link x 104 8 6 4 2 D S P 0 0 100 200 300 400 500 600 700 800 900 1000 Frequency (Hz) (b) without the Iperf flow Fig 7,Non-data,117
| Power spectra of aggregate traffic at USC Internet II link traffic increases the bottleneck signal (ie, the peak am- plitude) becomes weaker and hence more difficult to de- tect We provide additional results and more detailed dis- cussion on the effects of unobserved bottleneck traffic else- where (He, 2006)|,Non-data,117
| In this paper, we focus on case (a) where we observe all bottleneck traffic Refinement to improve sensitivity to partial observation is an area of future work 52|,Non-data,117
| Wide-area Network Experiments The previous examples show that strong periodicities of bottleneck traffic can be easily observed from the spectra in simple lab experiments without cross traffic However, the situation is more complicated in wide-area network en- vironments with real Internet traffic Differences include a much more complex network topology, the monitoring point being far away from the actual bottleneck link, and much richer cross traffic, which will interact with the bot- tleneck traffic and affect the spectra We attempt to observe some of these challenges by car- rying out experiments on a live, wide-area network|,Non-data,117
| We place the capture machine close to an Internet II router at the edge of our university’s network We mirror traf- fic at the router to a separate port and capture it using a Endace DAG network monitoring card (Endace, 2005) These purpose-built cards are capable of nanosecond res- olution timestamps and can keep up with the 1Gbps links without dropping any packets We introduce an artificial 10Mb/s bottleneck traversed by an Iperf TCP flow, which flows through the capture point|,Non-data,117
| Figure 7(a) shows the spectrum of aggregate traffic ob- served at the capture machine, which includes the bot- tleneck flow The throughput of the TCP flow is around 903Mbps, and the aggregate traffic volume is 309Mbps|,Non-data,117
| We see a spike around 790Hz in the PSD Compared with the spectrum in Figure 4 we observe three main differ- ences First, the peak amplitude in the spectrum appears at 793Hz, which is close, but different than the 813Hz ob- served for the spectrum without cross traffic Second, the peak amplitude is 140,000, only 27|,Non-data,117
|5% of the peak ampli- tude of the previous case We believe that these differences are due to interference from cross traffic While it is hard to precisely quantify how cross traffic affects the spectrum of the aggregate traffic as the underlying process is non-linear in nature, we have two general observations First, cross traffic competes with the bottleneck traffic and distorts the regularity in the periodic nature of the bottleneck traffic|,Non-data,117
| This results in a lower peak amplitude and a slightly dif- ferent location of the peak frequency Second, background traffic at the monitored link introduces its own frequency components to the observed spectrum resulting into noise We expect that the presence of noise will make if harder to detect the signal from the bottleneck traffic We investigate the effect of noise on detection in later sections|,Non-data,117
| In Figure 7(a) we observe the spectra from a different ex- periment, this one between our university and the Univer- sity of Santa Barbara Here we still see a prominent spike near the 813Hz base frequency despite the the presence of noisy cross traffic This suggests that there are cases where the bottleneck traffic could be detected even when mixed with aggregate traffic Finally, Figure 7(b) shows the spec- trum of aggregate traffic at a time when our artificial TCP bottleneck flow was not present|,Non-data,117
 Note that the strongest energy around 790Hz is only 1/3 of the peak amplitude when the bottleneck flow is present The aggregate traffic volume is about 178Mbps From the above experiments we conclude that despite noise introduced by background traffic the bottleneck traf- fic spectra may still be detected by examining the PSD of the aggregate traffic,Non-data,117
| This is encouraging and motivates the work in the next section, where we present a quantitative approach to detecting the presence if bottleneck traffic by examining the aggregate 6 Detection of Bottleneck Traffic In the previous section we have visually demonstrated the presence of a spectral signature of bottleneck traffic in both controlled lab experiments and wide-area network ex- periments In this section we propose and investigate the performance of an automated process to detect bottleneck traffic based on a classic statistical method, Maximum Like- lihood Detection|,Non-data,117
| We will first describe how to apply Max- imum Likelihood Detection with a generalized framework suitable for any detection feature Then, we will propose a specific algorithm, namely the Top-Frequency Algorithm, which extracts the peak amplitude in a frequency window as the detection feature we will also simplify the matching operation by approximating the data with normal distribu- tions We will explore alternative detection features later, in Section 6|,Non-data,117
|5 8 61 Maximum Likelihood Detection Maximum Likelihood Detection (Trees, 1968) is a mature technique that has been applied to many problems|,Non-data,117
| We ap- ply it to the detection of bottleneck traffic by treating the detection problem as a binary-hypothesis-testing problem where hypothesis HB corresponds to the presence of bot- tleneck traffic of a given type in aggregate traffic, and hy- pothesis H0 corresponds to the absence of such bottleneck traffic For these two hypotheses, we select a feature drawn from the aggregate traffic as the random variable We discuss potential features in Sections 64 and 6|,Non-data,117
|5 For example, one possible feature can be the highest amplitude in a frequency window of the spectrum of the aggregate traffic We denote the PDFs (probability density functions) for the feature under these two hypotheses as p(x||H0) and p(x||HB), re- spectively Formally, if a random variable has a probability density function f (x), then it has a probability of f (x)dx to have a value in the infinitesimal interval [x, x + dx]|,Non-data,117
| We now first introduce Maximum Likelihood Test Rule that determines which hypothesis is more likely to be true for a given trace segment using the PDFs of the designated feature in Section 62 Then, we describe the two phases of the overall scheme, the training phase and the detection phase, in Section 63|,Non-data,117
| 62 Maximum Likelihood Test Rule Assuming we know the PDFs for both hypotheses H0 and HB, we can determine which hypothesis is more likely to be true for a given aggregate trace segment, ie|,Non-data,117
| if the aggregate trace segment contains bottleneck traffic of a given type or not, by comparing the values of the two PDFs at X, where X is the value of the selected feature in the aggregate trace segment The test rule is defined as follows: (5) if p(X||HB) ≥ p(X||H0), select HB if p(X||HB) < p(X||H0), select H0 We do not consider the prior probabilities of the two hypotheses H0 and HB in our test rule because they are hard to obtain If the prior information were available, we could improve the accuracy of the test by forming Maxi- mum a Posteriori test, which compares P [H0]p(X||H0) and P [HB]p(X||HB), where P [H0] and P [HB] are the prior probability for hypothesis H0 and HB 6|,Non-data,117
|3 Two Phases in Maximum Likelihood Detection In order to apply Maximum Likelihood Testing in Sec- tion 62, we need to know the PDFs of the designated feature for each of the two hypotheses Thus, our overall scheme has two phases: training and detection|,Non-data,117
| We describe each of them in detail below Fig 8 Steps in the training phase In the training phase, we estimate the probability den- sity functions for the designated feature for each hypothe- sis|,Non-data,117
| The steps are illustrated in Figure 8 In the first step, T1, we capture a training trace that we know contains no bottleneck traffic and another trace that contains bottle- neck traffic of a given type using existing trace capturing techniques Then, in step T2, we segment both traces and calculate the spectrum for each segment according to the equations in Section 4 Next, we extract the value of the designated feature from the spectrum of each segment in step T3|,Non-data,117
| Our preferred feature is the peak amplitude in a frequency window We present it formally in the next sub- section, and explore alternatives in Section 65 In step T4, we estimate the PDFs of the designated feature based on its values across all segments for each of the two hypotheses|,Non-data,117
| As a result, we get the PDF for H0 and the PDF for HB We then register this pair of PDFs into a database together with the associated bottleneck traffic type in step T5 We repeat the same steps T1 - T5 to get the pairs of PDFs for other types of bottleneck traffic and register them in the database Note that we always form a binary-hypothesis- testing problem for each type of bottleneck traffic, and we may use different features, e|,Non-data,117
|g, peak amplitude in different frequency windows, for different types of bottleneck traffic In the detection phase, the algorithm uses the PDFs ob- tained in the training phase to determine if an unknown trace segment contains bottleneck traffic or not The de- tailed steps are illustrated in Figure 9|,Non-data,117
| We first obtain a trace segment in step D1 Then, we calculate the spectrum for the trace segment in step D2 using the equations in Sec- tion 4 In step D3, we extract the value of the feature from the spectrum of the trace segment Then, in step D4, we match the feature value against a pair of PDFs registered in the database by applying the Maximum Likelihood Test Rule developed in Section 6|,Non-data,117
|2 to determine which hypoth- 9 Fig 9 Steps in the detection phase esis is more likely If HB is more likely, then we declare the presence of bottleneck traffic of the given type in the trace segment|,Non-data,117
| In step D5, we repeat step D3 and D4 to test the trace segment with other pairs of PDFs registered in the database to see if it contains other types of bottleneck traf- fic It is important to note that, theoretically, the detection space for different types of bottleneck traffic can be infinite In practice, however, there are a few common discrete val- ues that variables such as bandwidth and packet size can take For example, link bandwidth is typically discrete with very few possible values (e|,Non-data,117
|g, 10Mbps, 100Mbps, 1Gbps, etc) Common packet sizes include 64B, 576B, or 1500B|,Non-data,117
| Transport protocols are mostly limited to TCP and UDP, although other protocols such as SCTP are beginning to emerge This means that the search space can be greatly reduced by only looking at combinations of common values 64|,Non-data,117
| Top-Frequency Detection Algorithm As alluded to previously, there are a number of possible features upon which to base our detection algorithm Here we describe an algorithm that uses the peak amplitude in a given frequency window as the detection feature We review alternative features briefly in the following subsection Figures 3 to 5 suggest that in the absence of background traffic, the periodicity exhibited by a bottleneck reveals itself by a strong peak near the key frequency determined by the bottleneck traffic bandwidth and packet size|,Non-data,117
| Even when obscured by background traffic, we can still see a modestly sized peak in Figure 7(a) Thus, we conjecture that the peak amplitude near the associated key frequency is a feature of interest In addition, due to the impact of background traffic, the exact location of the peak amplitude varies slightly from time to time, suggesting that we should F D P 14 1|,Non-data,117
2 1 08 06 04 0,Non-data,117
2 0 9 95 10 105 11 115 12 12,Non-data,117
|5 13 Log of Peak Amplitude Fig 10 PDFs of the peak amplitude (after log) in [780Hz, 800Hz] for H0 (left dashed line) and HB (right dashed line) search in a window near the key frequency to catch the bottleneck signal To take advantage of these observations, we design the Top-Frequency Detection Algorithm, which uses the peak amplitude within a frequency window as the feature to dis- tinguish between H0 and HB|,Non-data,117
| The frequency window is se- lected according to a number of factors, most importantly the bottleneck bandwidth and packet size For example, we would check the window around 800Hz for a 10Mbps link saturated by 1500-byte packets as opposed to the 8kHz for the 100Mbps link by the same 1500-byte packets The win- dow size should not be too small or we will miss the strong energy associated with the bottleneck for some instances Neither should it be too wide as this may include strong energy caused by other network phenomena|,Non-data,117
| We will dis- cuss the proper selection of window location and size in Section 73 and 74 using real Internet traffic Rather than directly compare empirical probability den- sity functions, we simplify the operation by first fitting a mathematical distribution to the data|,Non-data,117
| This approach al- lows us to estimate the observed behavior parsimoniously and simplifies the hypothesis testing As we do not have concrete intuition for a particular distribution model, we have tested a number of distribution models to fit them to the actual data The log-normal distribution is selected for a balance between simple description and goodness of fit with the data The log-normal distribution means the log of the peak amplitude follows a Gaussian distribution|,Non-data,117
| Thus, in the log- domain, the parameters which completely characterize the two distributions are the means and standard deviations, ie(μ0, σ0) for H0 and (μB, σB) for HB Formally, the two distributions are given by, p(x||H0) = p(x||HB) = 1 σ0√2π σB√2π 1 e−(x−μ0)2/2σ2 0 e−(x−μB )2/2σ2 B (6) (7) where x is the log of the peak amplitude in the selected frequency window|,Non-data,117
| The mean and standard deviation under each hypothesis are determined from the training data Although the log-normal distribution does not always of- fer the best fit to the data, it provides an excellent trade- off between fit and a parsimonious modeling of the feature Figure 10 shows the distributions of the peak amplitude in the frequency window [780Hz, 800Hz] for the training data 10 associated with the two hypotheses, H0 and HB, in one ex- perimental trace set involving a 10Mbps TCP bottleneck flow All data is presented in the log domain|,Non-data,117
| The dashed lines are the empirical PDFs for H0 and HB, while the solid lines are the normal distributions with the mean and stan- dard deviation derived from the experimental set for H0 and HB We can see the two empirical PDFs can be closely approximated by normal distributions in the log domain represented by the solid lines We have also verified that these two distributions follow normal distributions at the 5% significance level through the Lilliefors test (Lilliefors, 1967) With the log-normal distribution, we can simplify the Maximum Likelihood Test Rule in Section 6|,Non-data,117
|2 by solving the equation p(x||H0) = p(x||HB) first (recall that x here is the log of the data) This equation yields a quadratic function which has two roots if σ0 6= σB, and one root if σ0 = σB In general, we expect trace segments in HB to have higher peak amplitudes than trace segments in H0 as we have seen in Figure 10 Thus, if the quadratic equation has two roots, we discard the root which contradicts this expectation, i|,Non-data,117
|e classifies segments with lower peak ampli- tudes into HB and segments with higher peak amplitudes into H0 We select the root that agrees with the expecta- tion as the cut-off threshold If there is only one root for the quadratic function, then this root should agree with the expectation and it will be selected as the cut-off threshold|,Non-data,117
| In both cases, the detection rule can be simplified to a direct comparison between the cut-off threshold and the peak amplitude of the input trace in the selected window If the latter is larger, then the input trace will be classified as HB, having the associated bottleneck traffic Otherwise, we select H0 and declare that there is no such bottleneck traffic The cut-off threshold can be visually seen in Fig- ure 10 as the point where the two PDFs cross|,Non-data,117
| The computational cost for the Top Frequency Algo- rithm can be calculated as follows Assuming that we build a database of N types of bottleneck traffic, for each type we gather M trace segments with the bottleneck traffic and M trace segments without the bottleneck traffic, the size of the frequency window used in the Top Frequency Algo- rithm is W , then the complexity in the training step would be O(N ∗ (O(M ∗ P SD) + O(M ∗ W ))) = O(N ∗ M ∗ P SD), where PSD is the cost to calculate the PSD of a trace segment which is greater than O(W) The computational cost for detecting bottleneck in a trace segment would be O(N ∗ (P SD + O(W ))) = O(N ∗ P SD) 6|,Non-data,117
|5 Detection Using Other Features We have designed and investigated the performance of a suite of detection algorithms using different kinds of fea- tures selected from the trace spectra Table 1 summarizes their differences In the Single-Frequency Algorithm, we examine the amplitude at a particular frequency (e|,Non-data,117
g 800 Hz) We also use log-normal distribution to model the PDFs Table 1 Comparison of detection features Table 2 Experiment Scenarios Algorithm Feature PDF Model Scenario Bottleneck Traffic Type Background Traffic Vol- Top-Frequency amplitude peak in a frequency window (after log) normal tion distribu- Single-Frequency amplitude at a single fre- quency (after log) normal tion distribu- Top-M-Frequencies M peak amplitudes in a frequency window (after log) multi-variate nor- mal distribution All-Frequencies all amplitudes in a fre- quency window (after log) multi-variate nor- mal distribution of H0 and HB and calculate a cut-off threshold for classi- fying new unknown traces in the same way as in the Top- Frequency Algorithm The Top-M -Frequencies Algorithm is a generalized variation of the Top-Frequency Algorithm,Non-data,117
| It considers the M highest amplitudes in a frequency win- dow The method assumes that the vector comprised of the log of these amplitudes follow a multi-variate Gaussian dis- tribution Finally, we consider all amplitudes within a fre- quency window in the All-Frequencies Algorithm It also assumes a multi-variate Gaussian distribution of the log of these amplitudes|,Non-data,117
| Our experimental results show that the Top-Frequency Algorithm performs much better than the Single-Frequency Algorithm, since it accommodates the possible shift of the peak amplitude associated with the bottleneck traffic in a small frequency window Its performance is also compa- rable to the two multi-variate detection algorithms while having much lower computational overhead since it only models the distribution of a single random variable Due to space limitations we only present the results for the Top- Frequency Algorithm in this paper Details of other meth- ods can be found in (He et al|,Non-data,117
|, 2005) 7 Evaluation with Real Internet Traffic To systematically evaluate the performance of our de- tection algorithm we artificially introduce bottleneck traf- fic into a wide-area network and observe it in packet traces gathered at the Internet-2 access link at our university Overall, our primary goal is to understand the performance of our detection algorithms under different network con- ditions and with different algorithm parameters|,Non-data,117
| We first study the impact of algorithm parameters in Section 73, 74, and 75|,Non-data,117
| We then consider the stability of the results with different transport protocols in Section 76 and the se- lection of a proper frequency window in Section 77 Finally, we investigate the effect of using different training data in Section 7|,Non-data,117
8 and how the signal-to-noise ratio (defined as the ratio of bottleneck traffic volume to background traffic volume) affects algorithm performance in Section 79 11 T10L T10H U10L an Iperf TCP flow through a 10Mbps bottleneck an Iperf TCP flow through a 10Mbps bottleneck an Iperf UDP flow through a 10Mbps bottleneck ume low (24Mbps to 59Mbps) high (94Mbps to 186Mbps) low (19Mbps to 57Mbps) T100H an Iperf TCP flow through a 100Mbps bottleneck high 204Mbps) (112Mbps to 71,Non-data,117
| Experiment Setup In our experiments, we use the same wide-area net- work experiment setup as in Section 52 The actual load measured during our experiments varies from 19Mbps to 204Mbps We create bottleneck traffic from an outside source to a destination inside the university|,Non-data,117
 The source is nine hops away from the destination The link between the source and the first hop is the bottleneck link and is over 90% utilized No other flow shares this link during the experiment The bottleneck flow traverses the Internet-2 link and is observed by the capture machine together with the other background traffic,Non-data,117
| During our experiments there are three variables: the transport protocol of the bottleneck traffic (TCP or UDP), the bandwidth of the bottleneck link (10Mbps or 100Mbps), and the amount of background traffic at the monitored link Among all possible permutations, we have investigated four specific scenarios as shown in Table 2 These four scenar- ios are selected because they capture key differences For example, U10L differs from T10L in the transport protocol of the bottleneck traffic; T10L differ from T10H in back- ground traffic volume; and T10H differs from T100H in the bandwidth of the bottleneck link|,Non-data,117
| For each of the four scenarios we gather a pair of packet traces at the monitored link every two hours for 24 hours Each pair consists of a 5-minute long trace gathered when there is no intentionally introduced bottleneck flow (H0) and another 5-minute long trace gathered when we inten- tionally introduce a bottleneck Iperf flow (HB) In all cases we use a default segment length l of 1 second and a de- fault sampling rate p of 200kHz Thus, each trace pair has 300 trace segments without a bottleneck flow and 300 trace segments with the bottleneck flow|,Non-data,117
| There are many other variations to the above four sce- narios we could try, to evaluate the performance of our de- tection algorithm For example, we could vary the traffic through the bottleneck by using multiple flows with differ- ent packet size distributions We could also vary the portion of bottleneck traffic seen at the observation point Prelimi- nary results show that varying the observed portion of bot- tleneck traffic has a greater effect than varying the number of flows through the bottleneck|,Non-data,117
| We plan to explore these scenarios in future work 72 Detection Accuracy We use detection accuracy to measure the performance of our detection algorithm, defined as the probability that the algorithm gives the correct answer about the existence of bottleneck traffic in the trace|,Non-data,117
| To calculate detection accuracy, we adopt the following procedure First, we select a pair of traces with and without bottle- neck flow as the training data set and train the algorithm on it Next, we use the training result to classify the trace segments in the training data and other trace pairs under the same scenario We compare the answer by the algo- rithm with the ground truth to obtain detection accuracy|,Non-data,117
| In our experiments, the detection accuracy is equal to the average value of the true positive rate and the true neg- ative rate, since we have equal number of trace segments with and without the bottleneck flow We use two types of detection accuracies to better cap- ture the performance of the algorithm on the training data and on other traces under the same scenario The first is the accuracy on the training data, which we call training accu- racy It measures the ability of the algorithm to distinguish trace segments in the training data, where the algorithm knows the truth whether the segment has bottleneck traf- fic or not|,Non-data,117
| The second is the average accuracy on all other trace pairs gathered under the same scenario as the train- ing set, which we call average accuracy in short The aver- age accuracy represents the performance of the algorithm on traces where it has no prior knowledge if the segment has bottleneck traffic or not In this paper, we use training accuracy to tune the al- gorithm parameters, such as selecting the proper detec- tion window We present the corresponding average accu- racy to show the expected performance of the algorithm on unknown traces (traces where the algorithm has no prior knowledge if the segment has bottleneck traffic or not)|,Non-data,117
| As observed in Section 73 and 74, there is a strong correla- tion between these two types of accuracies in their response to the changes in the algorithm parameters This strong correlation gives credence to our methods for parameter tuning based on examining the training accuracy|,Non-data,117
 73 Frequency Window Location The first question we consider is where the frequency win- dow should be located to detect specific bottleneck traffic We expect the algorithm to perform best with the window located around the base frequency calculated according to the Equation 4,Non-data,117
| For example, it should be around 813Hz for 10Mbps bottlenecks congested with 1500-byte packets For 100Mbps bottlenecks with 1500-byte packets it should be around 8130Hz To validate our hypothesis, we run the detection algorithm with the center of the frequency win- dow Wc varying from near 0Hz to near 10kHz Here, we fo- cus on the results under two scenarios: T10L, which targets detecting a 10Mbps bottleneck; and T100H, which targets 1 0|,Non-data,117
8 06 04 02 y c a r u c c A  n o i t c e t e D 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Window Location (Hz) (a) Training Accuracy with 7am trace pair as the training data 1 0,Non-data,117
8 06 04 02 y c a r u c c A  n o i t c e t e D  e g a r e v A 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Window Location (Hz) (b) Average Accuracy on other traces Fig,Non-data,117
| 11 Accuracy in detecting the 10Mbps bottleneck (T10L) as a function of window location, Ws = 20Hz D S P 6 5 4 3 2 1 0 x 104 0 100 200 300 400 500 600 700 800 900 1000 Frequency (Hz) (a) Spectrum 02 015 0|,Non-data,117
1 005 y t i l i b a b o r P 0 1 1 F D C 05 15 2 2,Non-data,117
5 3 4 Inter−arrival times (ms) 35 45 5 55 6 0 1 1,Non-data,117
5 2 25 3 4 Inter−arrival times (ms) 35 45 5 5,Non-data,117
5 6 (b) PDF and CDF of packet interarrival times Fig 12 Spectrum and packet interarrival time distribution of the isolated Iperf TCP flow (T10L) detecting a 100Mbps bottleneck Figure 11 shows the impact of window location on de- tecting a 10Mbps bottleneck under the T10L scenario,Non-data,117
| The top subgraph 11(a) shows the training accuracy using the 7am trace pair as the training data, while the bottom sub- graph 11(b) depicts the corresponding average accuracy on other trace pairs In both subgraphs, we use a fixed win- dow size of 20Hz and vary the window center from 10Hz to 9990Hz We can see that the center of the frequency window has a strong effect on both types of detection accuracies Both types of accuracies reach peak values with windows around 790Hz (close to the predicted 813Hz frequency) and its harmonics|,Non-data,117
| For example, training accuracy can reach as 12 1 08 06 04 0|,Non-data,117
2 y t i l i b a b o r P  n o i t c e t e D 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Frequency (Hz) 1 08 06 04 0,Non-data,117
|2 y c a r u c c A  n o i t c e t e D 0 0 Detection Accuracy on the 7am training set Average Detection Accuracy on other traces 10 20 30 40 50 Window Size (Hz) 60 70 80 90 100 (a) Training Accuracy with 7am trace pair as the training data Fig 14 Detecting the 10Mbps bottleneck (T10L) with varying win- dow sizes, Wc = 790Hz 1 08 0|,Non-data,117
6 04 02 y t i l i b a b o r P  n o i t c e t e D 0 0 1 08 0,Non-data,117
6 04 02 y c a r u c c A  n o i t c e t e D Detection Accuracy on the 7am training set Average Detection Accuracy on other traces 1000 2000 3000 4000 5000 6000 7000 8000 9000 Frequency (Hz) (b) Average Accuracy on other traces 0 0 100 200 300 400 Window Size (Hz) 500 600 700 800 900 1000 Fig 13,Non-data,117
| Accuracy in detecting the 100Mbps bottleneck (T100H) as a function of window location, Ws = 200Hz high as 93% when the window is around 790Hz, but it drops to 50% when the window is around 100Hz Furthermore, there is a strong correlation of the two types of accuracies in terms of their response to the changes in window location As the window location changes, the average accuracy on other trace pairs will increase as train- ing accuracy increases, and decrease as the latter decreases However, in general the former is lower than the latter|,Non-data,117
| For example, average accuracy is only 71% while training accu- racy reaches 93% with the window around 790Hz This in- dicates some mismatch between the statistics of the train- ing trace pair and other trace pairs We have similar observations when we train the algo- rithm with different trace pairs under the T10L scenario The correlation between these two types of accuracies demonstrates that a window with a high training accuracy will generally lead to a high accuracy on other trace pairs|,Non-data,117
| To find out why the detection accuracies peak near 790Hz instead of 813Hz as we have expected, we isolate the Iperf bottleneck flow from the aggregate and plot its spectrum in Figure 12(a) for one trace segment We can see that the spectrum suggests the existence of significant cross traffic which triggers the TCP congestion control for the bottle- neck flow resulting in less periodic packet transmissions In the spectrum the peak amplitude appears around 790Hz, a frequency lower than 813Hz for the case without cross traffic in Figure 4 In addition, the peak amplitude is only 55,000, much weaker than the peak amplitude of 510,000 for the case without cross traffic|,Non-data,117
| The weaker amplitude makes it hard to detect the bottleneck flow The CDF and PDF of packet interarrival times for the isolated Iperf flow in Figure 12(b) show a spike bump pattern, also suggesting that the bottleneck experiences significant cross traffic in a downstream link according to the findings in (Katabi and Blake, 2001) Fig 15|,Non-data,117
| Detecting the 100Mbps bottleneck (T100H) with varying window sizes, Wc = 8100Hz detection accuracy on other trace pairs best detection accuracy on the training set 1 08 06 04 0|,Non-data,117
|5 y c a r u c c A n o   i t c e t e D 0 0 1 2 3 4 5 6 Segment Length (seconds) Fig 16 Detection accuracy as a function of segment length Figure 13 reveals the impact of frequency window loca- tion on the accuracies of detecting a 100Mbps bottleneck under the T100H scenario We fix the window size to 200Hz here, as the predicted base frequency for a 100Mbps bottle- neck link is ten times of the value for a 10Mbps bottleneck link|,Non-data,117
| Again we see a strong correlation of the two types of accuracies in terms of their response to the changes in win- dow location Both types of accuracies peak around 8100Hz, close the predicted 8130Hz base frequency For example, when the window is around 8100Hz, training accuracy with the 7am training trace pair can reach 100%, while the cor- responding average accuracy on other trace pairs reaches 97% These values are significantly higher than the accura- cies for detecting the 10Mbps bottleneck in Figure 11, be- cause we have much higher signal-to-noise ratios|,Non-data,117
 We will discuss the impact of signal-to-noise ratio in more detail in Section 79 Both experiments agree with our expectation of the location of the frequency window for the best detec- tion accuracy 7,Non-data,117
|4 Frequency Window Size In the previous section, we considered the impact of win- dow location with fixed window sizes In this section, we consider the impact of window size while fixing the win- dow location Our intuition behind the choice of window 13 size is that the window should be neither too narrow nor too wide, since narrow windows might fail to capture the strong signal for the bottleneck that is shifted to another location due to noise, and wide windows might result in in- creasing false positives due to noise from unrelated events|,Non-data,117
| Another reason against choosing a large window is due to the increase in the processing overhead as a larger window means more frequencies to be examined To systematically study the impact of window size, we run the Top-Frequency algorithm using the trace pair gath- ered at 7am in scenario T10L as the training set while vary- ing the window size from 1Hz to 100Hz with fixed window center location Wc at 790Hz Figure 14 shows the detection results The upper line is the training accuracy on the 7am training set while the bottom line is the corresponding av- erage accuracy on other trace pairs under Scenario T10L|,Non-data,117
| We see that the two types of accuracies react in a similar way to the changes in window size For both types of accu- racies, we observe that window sizes between 20Hz to 30Hz (about 25% to 4% of the predicted 813Hz base frequency) give the best result In addition, smaller sizes show much lower accuracies because with smaller windows, it becomes possible to miss a signal that is shifted outside the window due to noise|,Non-data,117
| On the other hand, larger sizes also result in lower accuracies but the penalty is not as dramatic as smaller sizes We have also conducted a similar investigation for de- tecting the 100Mbps bottleneck in the T100H scenario with windows centered at 8100Hz Figure 15 shows the detection results using the trace pair gathered at 7am as training set while varying the window size from 1Hz to 1000Hz Again we observe a strong correlation of the two types of accu- racies in terms of their response to the changes in window size|,Non-data,117
| Both types of accuracies reach their peak values with the window size in the range of 100Hz to 200Hz (about 12% to 25% of the predicted 8133Hz base frequency) In addi- tion, smaller sizes show much lower accuracies than larger window sizes|,Non-data,117
 75 Sampling Rate and Segment Length Segment length and sampling frequency are also impor- tant parameters of the algorithm A sampling frequency that is too low will result into aliasing; one that is too high increases processing overhead,Non-data,117
| In future work, we will in- vestigate the optimum balance between the two For this work we choose a conservative frequency of 200kHz As- suming 1500-byte packet size, 200kHz sampling frequency is sufficiently high to capture the signatures of bottleneck links with bandwidth up to 100Mbps We desire a relatively short segment length to allow rapid detection of bottleneck traffic|,Non-data,117
| However, as discussed in Sec- tion 41, segment length cannot be too short or we cannot compute an accurate spectrum In addition, the algorithm may become too sensitive to transient flows 1 second to 5 seconds|,Non-data,117
| Again we use the trace pair gath- ered at 7am in scenario T10L as the training set Here, we use a fixed window size of 20Hz and a fixed window loca- tion at 790Hz since results in Section 73 and 74 show they are good choices for detecting the 10Mbps bottleneck in the T10L scenario|,Non-data,117
 Figure 16 shows that training accu- racy and average accuracy with different segment length We only see small changes for both types of detection accu- racies when we vary the segment length from 1 second to 5 seconds In future work we will explore other trace lengths One practical constraint for using longer trace segment is that we need longer traces so that we have enough number of segments to form a distribution and estimate its param- eters,Non-data,117
| 76 Transport Protocol From the controlled experiments in Section 51, we see that UDP flows are more regular and provide stronger sig- nals than TCP flows|,Non-data,117
 Here we evaluate how that translates into the detection accuracy Figure 17 compares the accuracy for detecting a 10Mbps link saturated by a TCP flow (scenario T10L) and the ac- curacy for detecting the same 10Mbps link saturated by a UDP flow (scenario U10L) Again we vary the window cen- ter from 10Hz to 9990Hz with fixed window size of 20Hz The two top subgraphs 17(a) and 17(b) compare training accuracy on the 7am training trace between TCP and UDP,Non-data,117
| We can see that the spikes for TCP are around 790Hz and its harmonics, and the spikes for UDP are around 810Hz and its harmonics The 810Hz base frequency for UDP is closer to the predicted 813Hz base frequency for 10Mbps bottleneck links than the 790Hz base frequency for TCP In addition, UDP has higher detection accuracies (up to 98%) around its base frequency than TCP (up to 93%) UDP also maintains high detection accuracies around the harmonics, while TCP has a decay of detection accuracies among harmonics as they move further away from the base frequency|,Non-data,117
| The two bottom subgraphs 17(c) and 17(d) compare the corresponding average accuracy on other traces between TCP and UDP Again we see that UDP has spikes around 810Hz and its multiples, while TCP has spikes around 790Hz and its multiples In addition, UDP has higher aver- age accuracies (up to 75%) around its base frequency than TCP (up to 71%) UDP also maintains high detection ac- curacies around the harmonics while TCP has a decay of detection accuracies among harmonics|,Non-data,117
| The differences for the detection accuracies between TCP and UDP are due to the fact that TCP adjusts packet trans- missions by considering feedback from the network while UDP does not in our experiments This results in less pe- riodic packet transmissions for the TCP flow, which trans- lates in lower detection accuracies and a decay of detection accuracies among harmonics To study the impact of segment length, we vary it from To affirm that UDP has more regular packet transmis- 14 1 08 0|,Non-data,117
6 04 02 y c a r u c c A  n o i t c e t e D 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Window Location (Hz) 1 08 0,Non-data,117
|6 04 02 y c a r u c c A  n o i t c e t e D 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Window Location (Hz) (a) Training Accuracy on 7am training set with the bot- tleneck saturated by TCP, Ws = 20Hz (b) Training Accuracy on 7am training set with the bot- tleneck saturated by UDP, Ws = 20Hz 1 08 0|,Non-data,117
6 04 02 y c a r u c c A  n o i t c e t e D  e g a r e v A 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Window Location (Hz) 1 08 0,Non-data,117
|6 04 02 y c a r u c c A  n o i t c e t e D  e g a r e v A 0 0 1000 2000 3000 4000 5000 6000 7000 8000 9000 Window Location (Hz) (c) Average Accuracy on other trace pairs with the bot- tleneck saturated by TCP, Ws = 20Hz (d) Average Accuracy on other trace pairs with the bot- tleneck saturated by UDP, Ws = 20Hz Fig 17|,Non-data,117
 Detecting the 10Mbps bottleneck saturated with different transport protocols 100 200 300 400 500 600 700 800 900 1000 Frequency (Hz) (a) Spectrum x 105 2 15 D S P 1 05 0 0 008 0,Non-data,117
06 004 002 F D P 0 0 1 F D C 05 0,Non-data,117
5 1 15 Inter−arrival times (ms) 2 2 25 25 0 0 0,Non-data,117
|5 1 15 Inter−arrival times (ms) (b) PDF and CDF of packet interarrival times Fig 18 Spectrum and packet interarrival time distribution of the isolated Iperf UDP flow (U10L) sions than TCP, we isolate the UDP bottleneck flow from the aggregate and plot its spectrum in Figure 18(a) and the distribution of its packet interarrival times in Figure 18(b)|,Non-data,117
| We can see that the UDP flow exhibits much less impact from cross traffic compared with the Iperf TCP flow as shown in Figure 12(a) and 12(b) For example, the peak amplitude for the UDP flow is 170,000, more than 2 times stronger than the TCP flow, and it appears around 810Hz instead of the 790Hz for the TCP flow The PDF and CDF of packet interarrival times for UDP show a single spike at 123ms, while for TCP the interarrival times has a spike bump pattern|,Non-data,117
 These figures demonstrate that the UDP flow has more periodic packet transmissions than the TCP 15 e c n e r e f f i D  y c a r u c c A 1 05 0 −05 −1 0 Difference in the accuracy on the training set Difference in the average accuracy on other traces 5 10 15 20 Gathering Time of the Training Set Fig 19,Non-data,117
| Differences between using the fixed [780Hz, 800Hz] window and the best training windows in T10L scenario flow, which makes it easier to detect UDP flows than to detect TCP flows 77 Guidelines for Selection of Frequency Windows Based on our investigation on the window location, win- dow size, and transport protocol, we opt to use the follow- ing fixed detection windows for the four scenarios rather than using the window that yields the best training accu- racy by exhaustively searching all possible window loca- tions and sizes|,Non-data,117
 We call the latter the best training window For scenario T10H and T10L we use the 20Hz wide win- dow centered at 790Hz For scenario U10L we select the same size 20Hz wide window but centered at 810Hz For scenario T100H we choose the 200Hz wide window centered at 8100Hz,Non-data,117
| Results show that the detection accuracies with these fixed windows are comparable to the detection accuracies achieved by the best training window, and in some cases the fixed windows can even yield better average accuracy on other traces than the best training window, because the latter only guarantees the best accuracy on the training set, but not the best average accuracy on other traces For example, Figure 19 shows the differences between using the fixed [780Hz, 800Hz] window and using the best training window in the T10L scenario The x axis repre- sents the time of the day when the training trace pair was gathered For each training trace pair, we find the best training window that yields the best training accuracy by exhaustively searching all possible window locations and sizes|,Non-data,117
| Then we calculate training accuracy and average ac- curacy on other traces using this window In comparison, we also calculate the corresponding detection accuracies using the fixed [780Hz, 800Hz] window The differences plotted in Figure 19 are obtained by deducting the accuracies for the best training windows from the accuracies for the fixed window We can see that for most training traces, the differ- ences between using the fixed window and the best training windows are small|,Non-data,117
| The largest difference happens with the training trace pair gathered at 11pm With the best train- ing window, training accuracy is 87% higher than the value using the fixed window, but the average accuracy on other traces is 18% lower than the value using the fixed window In general, we should follow the following principles in se- lecting the detection window|,Non-data,117
| First, we should use a detec- tion window whose center is located near the predicted base frequency for the bottleneck link (eg 813Hz for 10Mbps links and 8133Hz for 100Mbps links) Second, the detection window size should be around 1% to 5% of the predicted base frequency|,Non-data,117
| Third, the exact location and size of the detection window to be used in real operation should be ad- justed slightly according to the specific network Network operators should do some training to select the proper de- tection window 78|,Non-data,117
| Training Data Variation The accuracy of any training-based detection algorithm is influenced by the quality of the training data In this sec- tion we investigate the impact of using different training data on the two types of detection accuracies, training ac- curacy and average accuracy on other trace pairs We select the T10L scenario for illustration Figure 20 shows the impact of using different training data on the two types of detection accuracies|,Non-data,117
| The top sub- graph shows training accuracy as we vary the trace pair us- ing for training algorithm The x axis is the average aggre- gate traffic volume of the training trace pair The number beside each point in the graph indicates the time of the day when the training trace pair was gathered For example, the point denoted by “9” shows the training accuracy on the 9am training trace pair|,Non-data,117
| In the bottom subgraph, each point represents the average accuracy on all other trace pairs us- ing the cut-off threshold learned from the corresponding training trace pair For example, the point denoted by “9” reflects the average accuracy on all other trace pairs using the cut-off threshold learned from the training trace pair gathered at 9am The x axis for the bottom subgraph is also the average aggregate traffic volume of the training trace 1 08 0|,Non-data,117
6 04 02 y c a r u c c A n o   i t c e t e D 0 0  3 5 7  9  1  19 21  23  11 17  13  15 2 4 6 8 10 12 14 16 18 20 Traffic Volume (kilo−packets per second) (a) Training Accuracy with 7am training set under T10L 1 08 0,Non-data,117
6 04 02 y c a r u c c A n o   i t c e t   e D e g a r e v A 0 0  9  1 23  19 21  3 5 7  17 11  13  15 2 4 6 8 10 12 14 16 18 20 Traffic Volume (kilo−packets per second) (b) Average Accuracy on other traces under T10L Fig 20,Non-data,117
 Impact of training data on detection accuracies y c a r u c c A n o   i t c e e D t 1 08 06 0,Non-data,117
4 02 0 0  3 5 7  9  1  19 21  23  11 17  13  15 2 4 6 8 10 12 14 16 18 20 Traffic Volume (kilo−packets per second) (a) training with low traffic volume trace (7am) 1 08 06 0,Non-data,117
4 02 0 0  3 5 7  9  1  19 21  23  11 17  13  15 2 4 6 8 10 12 14 16 18 20 Traffic Volume (kilo−packets per second) y c a r u c c A n o   i t c e e D t (b) training with medium traffic volume trace (11am) 1 08 06 0,Non-data,117
4 02 y c a r u c c A n o   i t c e e D t 0 0  15  11 17  13  9  3 5 7  1  19 21  23 2 4 6 8 10 12 14 16 18 20 Traffic Volume (kilo−packets per second) (c) training with high traffic volume trace (3pm) Fig 21 Training with different traffic volume traces pair,Non-data,117
| We see that, in general, training on trace pairs with lower aggregate traffic volume yields higher accuracies on the training set itself, with the exception of the points denoted by “19”, “21” and “23” (flow-based analysis suggests that 16 1 08 06 04 0|,Non-data,117
2 y c a r u c c A  n o i t c e t e D 0 0 005 1 08 06 0,Non-data,117
4 02 y c a r u c c A  n o i t c e t e D  e g a r e v A 0 0 005 10Mbps bottleneck with TCP and high background traffic 10Mbps bottleneck with TCP and low background traffic 10Mbps bottleneck with UDP and low background traffic 100Mbps bottleneck with TCP and high background traffic 01 Signal to Noise Ratio 0,Non-data,117
15 02 025 (a) Training Accuracy 10Mbps bottleneck with TCP and high background traffic 10Mbps bottleneck with TCP and low background traffic 10Mbps bottleneck with UDP and low background traffic 100Mbps bottleneck with TCP and high background traffic 01 Signal to Noise Ratio 0,Non-data,117
15 02 025 (b) Average Accuracy on other traces Fig 22,Non-data,117
| Detection accuracy as a function of signal-to-noise ratio using fixed frequency windows specified in Section 77 three points have been tainted with presence of other bot- tleneck traffic when we believe there is no bottleneck traf- fic) An intuition behind the general trend is that lower aggregate traffic means less interference to the bottleneck traffic by the background traffic on the monitored link, and this translates into clearer signal for the bottleneck traffic making it easier to detect the presence of the bottleneck flow On the other hand, the average accuracy on other trace pairs has high values when we train on trace pairs with medium aggregate traffic volume|,Non-data,117
| This agrees with the intuition that it is better to train with the common cases (middle traffic volume) instead of extreme cases (either low or high traffic volume) As the average accuracy reported in Figure 20(b) may hide the variation among the accuracy on individual trace pairs used for evaluation, we dissect it into individual graphs, each representing the result with one training set Figure 21 shows the detection accuracy when training with three different trace pairs corresponding to low, medium, and high aggregate traffic volume at different periods of the day In each graph, a point represents the accuracy on the indicated trace pair using the cut-off threshold learned through the training trace pair|,Non-data,117
| We see a strong correlation between the accuracy on a trace pair and its distance to the training trace pair in terms of traffic volume For example, in Figure 21(a), trace pairs at 3am and 5am have very close traffic volume to the train- ing trace pair gathered at 7am Both have accuracies very close to the accuracy on the training set at 7am As the dis- tance to the training set increases in terms of packet rate, the detection accuracy on the corresponding trace pair de- creases|,Non-data,117
 This observation suggests that we could take ad- vantage of the similarity in terms of traffic volume between the trace pair used for training and the unknown trace to get high accuracies on the unknown trace We could use the cut-off threshold learned through a training set that has similar traffic volume to the unknown trace for detect- ing bottleneck traffic in the unknown trace Further explo- ration along this direction is part of our future work 7,Non-data,117
|9 Effect of Signal-to-noise Ratio Detection theory tells us that the detection accuracy is related to the signal-to-noise ratio (SNR) When detecting bottleneck traffic in aggregate network traffic, the signal is the intensity of the bottleneck traffic, and the noise is the level of background traffic Since the spectral representa- tion is obtained through the processing of packet arrivals, we define signal-to-noise ratio as the ratio of bottleneck traffic packet rate to background traffic packet rate|,Non-data,117
| Al- though it is hard to quantify how SNR affects the detection accuracy as their relationship is non-linear, in general, we expect to see better detection accuracy with higher SNR Figure 22 shows how the detection accuracy varies as a function of signal-to-noise ratio using fixed frequency win- dows specified in Section 77 The top subgraph shows the detection accuracy on the training trace pair, while the bot- tom subgraph shows the detection accuracy on all other trace pairs using the cut-off learned from the corresponding training trace pair|,Non-data,117
| The x axis is the signal-to-noise ratio for the training trace pair Both subgraphs include all trace pairs under the four experiment scenarios listed in Table 2 (different scenarios are indicated with different symbols) As expected, we see a general trend that a higher SNR leads to a better detection accuracy The T100H scenario for 100Mbps TCP bottleneck traffic with high background traffic shows the highest detection accuracies because the bottleneck traffic (around 7|,Non-data,117
5kpps)is quite large relative to background traffic (32 - 74 kpps) for SNRs of (01 - 023) Both training accuracy and average accuracy on other traces can reach over 94% in the T100H scenario,Non-data,117
| The U10L scenario for 10Mbps UDP bottleneck traffic with low background traffic (denoted by “*”) also has pretty good detection accuracies even though the SNRs are lower (0053 - 0125) Training detection accuracy ranges from 75% to 99%, while the average accuracy on other traces ranges from 76% to 86%|,Non-data,117
| The T10L scenario for 10Mbps TCP bottleneck traffic with low background traffic shows generally lower detection accuracies (around 63% to 94%) than the U10L scenario with similar SNRs for the reason that we have discussed in Section 76 Finally, the T10H scenario for 10Mbps TCP bottleneck traffic with high back- ground traffic gets the lowest detection accuracies (around 53% to 61%) as the SNRs are the lowest (001 - 0|,Non-data,117
|02) 8 Related Work Recently, a number of researchers have used spectral techniques to analyze network traffic for various pur- poses Hussain et al|,Non-data,117
| apply spectral techniques to packet arrival time series to distinguish single-source and multi- source DDoS attacks (Hussain et al, 2003) More recently, 17 they have proposed a spectral approach for attack re- identification (Hussain et al, 2006), after detecting and isolating attack packets from background traffic|,Non-data,117
| Barford et al use wavelets to analyze IP flow-level and SNMP information to detect DoS attacks and other network anomalies (Barford et al, 2002) Cheng et al|,Non-data,117
| also apply spectral analysis to separate normal TCP traffic from DDoS traffic as the former exhibits strong periodicities around its round-trip time (Cheng et al, 2002) Magnaghi et al propose a wavelet-based framework to proactively detect network misconfigurations|,Non-data,117
| It utilizes the TCP re- transmission timeout events during the opening phase of the TCP connection (Magnaghi et al, 2004) Partridge et al apply Lomb periodograms to retrieve periodicities in wireless communication, including CBR traffic and FTP traffic (Partridge et al|,Non-data,117
|, 2002) Kim et al apply wavelet denoising to improve the accuracy of detecting congestion among different flows (Kim et al, 2004)|,Non-data,117
| It requires active probing to measure the one-way-delay There are also a number of non-spectral techniques for detecting congestion sharing and estimating link capac- ity and available bandwidth Katabi et al propose to use packet inter-arrival times to infer the path characteris- tics such as bottleneck capacity and bottleneck sharing among flows based on entropy (Katabi and Blake, 2001)|,Non-data,117
| Hu et al (Hu et al, 2004) presented a tool to detect the location of a bottleneck in a path using active probing This tool, however, would be too costly for routine bot- tleneck monitoring|,Non-data,117
| Examples of non-spectral techniques for estimating link capacity and available bandwidth in- clude Pathchar (Jacobson, 1997), Pchar (Mah, 1999), Cap- Probe (Kapoor et al, 2004), Cprobe (Carter and Crovella, 1996), Pathload (Jain and Dovrolis, 2002, 2003), IGI and PTR (Hu and Steenkiste, 2003), and Spruce (Strauss et al, 2003) Unlike the above techniques, our approach is passive (i|,Non-data,117
|e, does not require probing packets), operates on aggregate traffic (so that individual traffic flows do not have to be sep- arated), transforms traffic into a suitable spectral domain representation (which is powerful to reveal periodic pat- terns), and makes use of more rigorous statistical methods rather than relying on qualitative visual evidence 9 Conclusions and Future Work Given the size and complexity of the Internet today, tools that can help understand and diagnose problems with the network are very important|,Non-data,117
| Spectral techniques have great potential in creating powerful tools to extract hidden pat- terns in the network to understand phenomena ranging from network anomalies to application and protocol behav- ior, and detection theory provides the background to trans- late these “pretty pictures” into quantitative algorithms In this work, we presented a methodology to apply these techniques to the detection of bottleneck traffic Appli- cations include troubleshooting, capacity planning, traffic estimation, DDoS detection, application monitoring, etc While we cannot pinpoint the exact location of the bottle- neck, our techniques can be used to determine if the bottle- neck is inside or outside the network|,Non-data,117
| In addition to visually demonstrating the spectral signature imposed by bottle- neck links, we proposed an algorithm based on the Maxi- mum Likelihood Detection to automatically detect the bot- tleneck signature embedded in aggregate traffic, and eval- uated its performance using real-world Internet traces Our results show that we can detect the presence of a bot- tleneck link several hops away from the monitoring point without flow separation, even if the traffic through the bot- tleneck link accounts for less than 10% of the aggregate traffic observed at the monitoring point Our techniques are completely passive, suitable for routine network mon- itoring, and can detect bottlenecks remotely without the need for direct observation, which is useful when bottle- necks are outside our administrative domain Our analysis investigated the effects of several factors on detection per- formance, including the effect of Signal-to-Noise ratio, the selection of the detection window, and the variation using different training data|,Non-data,117
| In the future we plan to strengthen our methodology as follows First, we want to model the underlying processes that govern the generation of bottleneck traffic signature and how it is shaped by competing traffic, and use the model to design more sophisticated detection algorithms that take traffic load and other time-varying factors into considera- tion For example, we can adjust the cut-off threshold ac- cording to the traffic volume to improve the detection accu- racy since higher traffic volume will generally increase the peak amplitude in a window Future work includes deter- mining the relationship between traffic load and the cut- off threshold and use this relationship to improve detection accuracy|,Non-data,117
| Second, we want to apply the detection methods in more including different monitoring diversified environments, points, different bottleneck locations, different types of traffic composition (eg, different packet size distribution and single flow versus multiple flows), and different types of cross traffic, to gain a more thorough understanding of performance Finally, we would like to extend our techniques into a framework that can be applied to study other periodic traf- fic patterns, such as protocol behavior and network anoma- lies|,Non-data,117
 This will expand the applicability of our methodology and help gain insight into other network phenomena 10 Acknowledgments This paper has benefited greatly from the valuable feed- back from Dr Antonio Ortega and Dr,Non-data,117
|Abstract We present new biases in RC4, break the Wi-Fi Protected Access Temporal Key Integrity Protocol (WPA-TKIP), and design a practical plaintext recovery attack against the Transport Layer Security (TLS) protocol To empir- ically find new biases in the RC4 keystream we use sta- tistical hypothesis tests This reveals many new biases in the initial keystream bytes, as well as several new long- term biases Our fixed-plaintext recovery algorithms are capable of using multiple types of biases, and return a list of plaintext candidates in decreasing likelihood|,Non-data,143
| To break WPA-TKIP we introduce a method to gen- erate a large number of identical packets This packet is decrypted by generating its plaintext candidate list, and using redundant packet structure to prune bad candidates From the decrypted packet we derive the TKIP MIC key, which can be used to inject and decrypt packets In prac- tice the attack can be executed within an hour|,Non-data,143
| We also attack TLS as used by HTTPS, where we show how to decrypt a secure cookie with a success rate of 94% using 9· 227 ciphertexts This is done by injecting known data around the cookie, abusing this using Mantin’s ABSAB bias, and brute-forcing the cookie by traversing the plain- text candidates Using our traffic generation technique, we are able to execute the attack in merely 75 hours 1 Introduction RC4 is (still) one of the most widely used stream ciphers|,Non-data,143
| Arguably its most well known usage is in SSL and WEP, and in their successors TLS [8] and WPA-TKIP [19] In particular it was heavily used after attacks against CBC- mode encryption schemes in TLS were published, such as BEAST [9], Lucky 13 [1], and the padding oracle at- tack [7] As a mitigation RC4 was recommended Hence, at one point around 50% of all TLS connections were us- ing RC4 [2], with the current estimate around 30% [18]|,Non-data,143
| This motivated the search for new attacks, relevant ex- amples being [2, 20, 31, 15, 30] Of special interest is the attack proposed by AlFardan et al, where roughly 13· 230 ciphertexts are required to decrypt a cookie sent over HTTPS [2] This corresponds to about 2000 hours of data in their setup, hence the attack is considered close to being practical|,Non-data,143
| Our goal is to see how far these attacks can be pushed by exploring three areas First, we search for new biases in the keystream Second, we improve fixed-plaintext recovery algorithms Third, we demon- strate techniques to perform our attacks in practice|,Non-data,143
| First we empirically search for biases in the keystream This is done by generating a large amount of keystream, and storing statistics about them in several datasets The resulting datasets are then analysed using statistical hy- pothesis tests Our null hypothesis is that a keystream byte is uniformly distributed, or that two bytes are in- dependent|,Non-data,143
| Rejecting the null hypothesis is equivalent to detecting a bias Compared to manually inspecting graphs, this allows for a more large-scale analysis With this approach we found many new biases in the initial keystream bytes, as well as several new long-term biases We break WPA-TKIP by decrypting a complete packet using RC4 biases and deriving the TKIP MIC key|,Non-data,143
| This key can be used to inject and decrypt packets [48] In par- ticular we modify the plaintext recovery attack of Pater- son et al [31, 30] to return a list of candidates in decreas- ing likelihood Bad candidates are detected and pruned based on the (decrypted) CRC of the packet|,Non-data,143
| This in- creases the success rate of simultaneously decrypting all unknown bytes We achieve practicality using a novel method to rapidly inject identical packets into a network In practice the attack can be executed within an hour We also attack RC4 as used in TLS and HTTPS, where we decrypt a secure cookie in realistic conditions|,Non-data,143
| This is done by combining the ABSAB and Fluhrer-McGrew bi- ases using variants of the of Isobe et al and AlFardan et al attack [20, 2] Our technique can easily be extended to include other biases as well|,Non-data,143
| To abuse Mantin’s ABSAB bias we inject known plaintext around the cookie, and ex- ploit this to calculate Bayesian plaintext likelihoods over USENIX Association  24th USENIX Security Symposium 97 1 Listing (1) RC4 Key Scheduling (KSA) 1 j, S = 0, range(256) 2 for i in range(256): 3 j += S[i] + key[i % len(key)] swap(S[i], S[j]) 4 5 return S Listing (2) RC4 Keystream Generation (PRGA) 1 S, i, j = KSA(key), 0, 0 2 while True: 3 i += 1 j += S[i] swap(S[i], S[j]) yield S[S[i] + S[j]] the unknown cookie We then generate a list of (cookie) candidates in decreasing likelihood, and use this to brute- force the cookie in negligible time|,Non-data,143
| The algorithm to gen- erate candidates differs from the WPA-TKIP one due to the reliance on double-byte instead of single-byte likeli- hoods All combined, we need 9 · 227 encryptions of a cookie to decrypt it with a success rate of 94% Finally we show how to make a victim generate this amount within only 75 hours, and execute the attack in practice To summarize, our main contributions are: • We use statistical tests to empirically detect biases in the keystream, revealing large sets of new biases|,Non-data,143
| • We design plaintext recovery algorithms capable of using multiple types of biases, which return a list of plaintext candidates in decreasing likelihood • We demonstrate practical exploitation techniques to break RC4 in both WPA-TKIP and TLS The remainder of this paper is organized as follows Section 2 gives a background on RC4, TKIP, and TLS|,Non-data,143
 In Sect 3 we introduce hypothesis tests and report new biases Plaintext recovery techniques are given in Sect 4,Non-data,143
| Practical attacks on TKIP and TLS are presented in Sect 5 and Sect 6, respectively Finally, we summarize related work in Sect|,Non-data,143
 7 and conclude in Sect 8 2 Background We introduce RC4 and its usage in TLS and WPA-TKIP 2,Non-data,143
|1 The RC4 Algorithm The RC4 algorithm is intriguingly short and known to be very fast in software It consists of a Key Scheduling Algorithm (KSA) and a Pseudo Random Generation Al- gorithm (PRGA), which are both shown in Fig 1 The state consists of a permutation S of the set {0, |,Non-data,143
|   ,255}, a public counter i, and a private index j The KSA takes as input a variable-length key and initializes S|,Non-data,143
| At each round r = 1,2,    of the PRGA, the yield statement out- puts a keystream byte Zr|,Non-data,143
 All additions are performed modulo 256 A plaintext byte Pr is encrypted to cipher- text byte Cr using Cr = Pr ⊕ Zr 21,Non-data,143
1 Short-Term Biases Several biases have been found in the initial RC4 key- stream bytes We call these short-term biases The most significant one was found by Mantin and Shamir They showed that the second keystream byte is twice as likely to be zero compared to uniform [25],Non-data,143
| Or more formally that Pr[Z2 = 0] ≈ 2·2−8, where the probability is over the 4 5 6 2 Figure 1: Implementation of RC4 in Python-like pseudo- code All additions are performed modulo 256 random choice of the key Because zero occurs more of- ten than expected, we call this a positive bias|,Non-data,143
| Similarly, a value occurring less often than expected is called a neg- ative bias This result was extended by Maitra et al [23] and further refined by Sen Gupta et al [38] to show that there is a bias towards zero for most initial keystream bytes|,Non-data,143
| Sen Gupta et al also found key-length dependent biases: if (cid:29) is the key length, keystream byte Z(cid:29) has a pos- itive bias towards 256− (cid:29) [38] AlFardan et al showed that all initial 256 keystream bytes are biased by empiri- cally estimating their probabilities when 16-byte keys are used [2]|,Non-data,143
| While doing this they found additional strong biases, an example being the bias towards value r for all positions 1 ≤ r ≤ 256 This bias was also independently discovered by Isobe et al [20] The bias Pr[Z1 = Z2] = 2−8(1 − 2−8) was found by Isobe et al|,Non-data,143
| refined this result Paul and Preneel [33] for the value zero to Pr[Z1 = Z2 = 0] ≈ 3 · 2−16 [20] In [20] the authors searched for biases of similar strength between initial bytes, but did not find additional ones However, we did manage to find new ones (see Sect|,Non-data,143
 33) 21,Non-data,143
|2 Long-Term Biases In contrast to short-term biases, which occur only in the initial keystream bytes, there are also biases that keep occurring throughout the whole keystream We call these long-term biases For example, Fluhrer and Mc- Grew (FM) found that the probability of certain digraphs, ie|,Non-data,143
|, consecutive keystream bytes (Zr,Zr+1), deviate from uniform throughout the whole keystream [13] These bi- ases depend on the public counter i of the PRGA, and are listed in Table 1 (ignoring the condition on r for now) In their analysis, Fluhrer and McGrew assumed that the in- ternal state of the RC4 algorithm was uniformly random 98 24th USENIX Security Symposium  USENIX Association Digraph (0,0) (0,0) (0,1) (0,i + 1) (i + 1,255) (129,129) (255,i + 1) (255,i + 2) (255,0) (255,1) (255,2) (255,255) Condition i = 1 i (cid:30)= 1,255 i (cid:30)= 0,1 i (cid:30)= 0,255 i (cid:30)= 254∧ r (cid:30)= 1 i = 2,r (cid:30)= 2 i (cid:30)= 1,254 i = 254 i = 255 i = 0,1 i ∈ [1,252]∧ r (cid:30)= 2 i (cid:30)= 254∧ r (cid:30)= 5 Probability 2−16(1 + 2−7) 2−16(1 + 2−8) 2−16(1 + 2−8) 2−16(1− 2−8) 2−16(1 + 2−8) 2−16(1 + 2−8) 2−16(1 + 2−8) 2−16(1 + 2−8) 2−16(1 + 2−8) 2−16(1 + 2−8) 2−16(1 + 2−8) 2−16(1− 2−8) Table 1: Generalized Fluhrer-McGrew (FM) biases|,Non-data,143
| Here i is the public counter in the PRGA and r the posi- tion of the first byte of the digraph Probabilities for long- term biases are shown (for short-term biases see Fig 4) This assumption is only true after a few rounds of the PRGA [13, 26, 38]|,Non-data,143
| Consequently these biases were gen- erally not expected to be present in the initial keystream bytes However, in Sect 33|,Non-data,143
|1 we show that most of these biases do occur in the initial keystream bytes, albeit with different probabilities than their long-term variants Another long-term bias was found by Mantin [24] He discovered a bias towards the pattern ABSAB, where A and B represent byte values, and S a short sequence of bytes called the gap With the length of the gap S de- noted by g, the bias can be written as: Pr[(Zr,Zr+1) = (Zr+g+2,Zr+g+3)] = 2−16(1+2−8e −4−8g 256 ) (1) Hence the bigger the gap, the weaker the bias|,Non-data,143
| Finally, Sen Gupta et al found the long-term bias [38] Pr[(Zw256,Zw256+2) = (0,0)] = 2−16(1 + 2−8) where w ≥ 1 We discovered that a bias towards (128,0) is also present at these positions (see Sect 3|,Non-data,143
|4) 22 TKIP Cryptographic Encapsulation The design goal of WPA-TKIP was for it to be a tem- porary replacement of WEP [19, §114|,Non-data,143
|2] While it is being phased out by the WiFi Alliance, a recent study shows its usage is still widespread [48] Out of 6803 net- works, they found that 71% of protected networks still allow TKIP, with 19% exclusively supporting TKIP Our attack on TKIP relies on two elements of the pro- tocol: its weak Message Integrity Check (MIC) [44, 48], and its faulty per-packet key construction [2, 15, 31, 30]|,Non-data,143
| We briefly introduce both aspects, assuming a 512-bit payload header TSC SNAP IP TCP MIC ICV encrypted Figure 2: Simplified TKIP frame with a TCP payload Pairwise Transient Key (PTK) has already been nego- tiated between the Access Point (AP) and client From this PTK a 128-bit temporal encryption key (TK) and two 64-bit Message Integrity Check (MIC) keys are de- rived The first MIC key is used for AP-to-client commu- nication, and the second for the reverse direction|,Non-data,143
| Some works claim that the PTK, and its derived keys, are re- newed after a user-defined interval, commonly set to 1 hour [44, 48] However, we found that generally only the Groupwise Transient Key (GTK) is periodically re- newed Interestingly, our attack can be executed within an hour, so even networks which renew the PTK every hour can be attacked When the client wants to transmit a payload, it first calculates a MIC value using the appropriate MIC key and the Micheal algorithm (see Fig|,Non-data,143
| Figure 2) Unfortu- nately Micheal is straightforward to invert: given plain- text data and its MIC value, we can efficiently derive the MIC key [44] After appending the MIC value, a CRC checksum called the Integrity Check Value (ICV) is also appended The resulting packet, including MAC header and example TCP payload, is shown in Figure 2|,Non-data,143
| The payload, MIC, and ICV are encrypted using RC4 with a per-packet key This key is calculated by a mixing function that takes as input the TK, the TKIP sequence counter (TSC), and the transmitter MAC address (TA) We write this as K = KM(TA,TK,TSC) The TSC is a 6-byte counter that is incremented after transmitting a packet, and is included unencrypted in the MAC header|,Non-data,143
| In practice the output of KM can be modelled as uni- formly random [2, 31] In an attempt to avoid weak-key attacks that broke WEP [12], the first three bytes of K are set to [19, §1142|,Non-data,143
|11]: K0 = TSC1 K1 = (TSC1 || 0x20) & 0x7f K2 = TSC0 Here, TSC0 and TSC1 are the two least significant bytes of the TSC Since the TSC is public, so are the first three bytes of K Both formally and using simulations, it has been shown this actually weakens security [2, 15, 31, 30]|,Non-data,143
| 23 The TLS Record Protocol We focus on the TLS record protocol when RC4 is se- lected as the symmetric cipher [8] In particular we as- sume the handshake phase is completed, and a 48-byte TLS master secret has been negotiated USENIX Association  24th USENIX Security Symposium 99 3 type version length payload HMAC header RC4 encrypted Figure 3: TLS Record structure when using RC4|,Non-data,143
| To send an encrypted payload, a TLS record of type application data is created It contains the protocol ver- sion, length of the encrypted content, the payload itself, and finally an HMAC The resulting layout is shown in Fig 3|,Non-data,143
| The HMAC is computed over the header, a se- quence number incremented for each transmitted record, and the plaintext payload Both the payload and HMAC are encrypted At the start of a connection, RC4 is ini- tialized with a key derived from the TLS master secret This key can be modelled as being uniformly random [2]|,Non-data,143
| None of the initial keystream bytes are discarded In the context of HTTPS, one TLS connection can be used to handle multiple HTTP requests This is called a persistent connection Slightly simplified, a server indi- cates support for this by setting the HTTP Connection header to keep-alive|,Non-data,143
| This implies RC4 is initialized only once to send all HTTP requests, allowing the usage of long-term biases in attacks Finally, cookies can be marked as being secure, assuring they are transmitted only over a TLS connection 3 Empirically Finding New Biases In this section we explain how to empirically yet soundly detect biases While we discovered many biases, we will not use them in our attacks|,Non-data,143
| This simplifies the descrip- tion of the attacks And, while using the new biases may improve our attacks, using existing ones already sufficed to significantly improve upon existing attacks Hence our focus will mainly be on the most intriguing new biases 3|,Non-data,143
|1 Soundly Detecting Biases In order to empirically detect new biases, we rely on hy- pothesis tests That is, we generate keystream statistics over random RC4 keys, and use statistical tests to un- cover deviations from uniform This allows for a large- scale and automated analysis To detect single-byte bi- ases, our null hypothesis is that the keystream byte values are uniformly distributed|,Non-data,143
| To detect biases between two bytes, one may be tempted to use as null hypothesis that the pair is uniformly distributed However, this falls short if there are already single-byte biases present In this case single-byte biases imply that the pair is also biased, while both bytes may in fact be independent Hence, to detect double-byte biases, our null hypothesis is that they are independent|,Non-data,143
| With this test, we even detected pairs that are actually more uniform than expected Rejecting the null hypothesis is now the same as detecting a bias To test whether values are uniformly distributed, we use a chi-squared goodness-of-fit test A naive approach to test whether two bytes are independent, is using a chi- squared independence test|,Non-data,143
| Although this would work, it is not ideal when only a few biases (outliers) are present Moreover, based on previous work we expect that only a few values between keystream bytes show a clear de- pendency on each other [13, 24, 20, 38, 4] Taking the Fluhrer-McGrew biases as an example, at any position at most 8 out of a total 65536 value pairs show a clear bias [13] When expecting only a few outliers, the M-test of Fuchs and Kenett can be asymptotically more power- ful than the chi-squared test [14]|,Non-data,143
