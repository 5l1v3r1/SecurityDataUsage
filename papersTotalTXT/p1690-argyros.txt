SFADiff: Automated Evasion Attacks and Fingerprinting

Using Black-box Differential Automata Learning

George Argyros
Columbia University

argyros@cs.columbia.edu

Ioannis Stais

University of Athens
i.stais@di.uoa.gr

Suman Jana

Columbia University

suman@cs.columbia.edu

Angelos D. Keromytis

Columbia University

angelos@cs.columbia.edu

Aggelos Kiayias

University of Edinburgh

Aggelos.Kiayias@ed.ac.uk

ABSTRACT
Finding diﬀerences between programs with similar function-
ality is an important security problem as such diﬀerences can
be used for ﬁngerprinting or creating evasion attacks against
security software like Web Application Firewalls (WAFs)
which are designed to detect malicious inputs to web ap-
plications. In this paper, we present SFADiff, a black-box
diﬀerential testing framework based on Symbolic Finite Au-
tomata (SFA) learning. SFADiff can automatically ﬁnd
diﬀerences between a set of programs with comparable func-
tionality. Unlike existing diﬀerential testing techniques, in-
stead of searching for each diﬀerence individually, SFADiff
infers SFA models of the target programs using black-box
queries and systematically enumerates the diﬀerences be-
tween the inferred SFA models. All diﬀerences between the
inferred models are checked against the corresponding pro-
grams. Any diﬀerence between the models, that does not
result in a diﬀerence between the corresponding programs,
is used as a counterexample for further reﬁnement of the in-
ferred models. SFADiff’s model-based approach, unlike ex-
isting diﬀerential testing tools, also support fully automated
root cause analysis in a domain-independent manner.

We evaluate SFADiff in three diﬀerent settings for ﬁnd-
ing discrepancies between: (i) three TCP implementations,
(ii) four WAFs, and (iii) HTML/JavaScript parsing imple-
mentations in WAFs and web browsers. Our results demon-
strate that SFADiff is able to identify and enumerate the
diﬀerences systematically and eﬃciently in all these settings.
We show that SFADiff is able to ﬁnd diﬀerences not only
between diﬀerent WAFs but also between diﬀerent versions
of the same WAF. SFADiff is also able to discover three
previously-unknown diﬀerences between the HTML/Java-
Script parsers of two popular WAFs (PHPIDS 0.7 and Ex-
pose 2.4.0) and the corresponding parsers of Google Chrome,
Firefox, Safari, and Internet Explorer. We conﬁrm that all
these diﬀerences can be used to evade the WAFs and launch
successful cross-site scripting attacks.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978383

1.

INTRODUCTION

Software developers often create diﬀerent programs with
similar functionality for various reasons like supporting dif-
ferent target platforms, resolving conﬂicting licenses, accom-
modating diﬀerent hardware constraints and exploring di-
verse performance trade-oﬀs. However, these programs often
suﬀer from subtle discrepancies that cause them to produce
diﬀerent outputs for the same input due to either implemen-
tation bugs or vagueness of the underlying speciﬁcations.
Besides hurting interoperability of the aﬀected programs,
these diﬀerences can also have serious security implications.
An attacker can leverage these diﬀerences for ﬁngerprint-
ing: That is, to identify the exact version of a program
running on a remote server. As diﬀerent programs suﬀer
from diﬀerent vulnerabilities, such ﬁngerprinting informa-
tion is very useful to an attacker for choosing speciﬁc attack
vectors. Besides ﬁngerprinting, the behavioral discrepancies
can also be used to launch evasion attacks against security
software that detects potentially malicious input to a target
program. In such cases, the security software must faithfully
replicate the relevant parts of the input parsing logic of the
target software in order to minimize false negatives. Any
discrepancy between the input parsing logic of the security
software and that of the target program can be used by an
attacker to evade detection while still successfully delivering
the malicious inputs. For example, Web Application Fire-
walls (WAFs) detect potentially malicious input to web ap-
plications such as cross-site scripting (XSS) attack vectors.
Therefore, a WAF must parse HTML/JavaScript code in the
same way as web browsers do. Any inconsistency between
these two parsers can lead to an evasion attack against the
WAF. However, making the WAF HTML/JavaScript pars-
ing logic similar to that of the web browsers is an extremely
challenging and errorprone task as most web browsers do
not strictly follow the HTML standard.

For the reasons mentioned above, automated detection
of the diﬀerences between a set of test programs providing
similar functionality is a crucial component of security test-
ing. Diﬀerential testing is a way for automatically ﬁnding
such diﬀerences by generating a large number of inputs (ei-
ther through black-box fuzzing or white-box techniques like
symbolic execution) and comparing the outputs of the test
programs against each other for each input. However, exist-
ing diﬀerential testing systems have several drawbacks that
prevent them from scaling to real-world systems with large
input space (e.g., WAFs, web browsers, and network pro-

1690Figure 1: SFADiﬀ archtitecture

tocol implementations). White-box techniques do not scale
to such large systems mostly due to the overhead and com-
plexity of the analysis process. Black-box fuzzing techniques
try to brute-force through the vast input space without any
form of guidance and therefore often fails to focus on the
relevant parts of the input space.

In this paper, we present SFADiff, a black-box diﬀeren-
tial testing framework based on Symbolic Finite Automata
(SFA) learning for automatically ﬁnding diﬀerences between
comparable programs. Unlike existing diﬀerential testing
techniques, instead of searching for each diﬀerence individ-
ually, SFADiff infers SFA models by querying the target
programs in a black-box manner and checks for diﬀerences
in the inferred models. SFADiff also veriﬁes whether the
candidate diﬀerences found from the inferred models indeed
result in diﬀerences in the test programs. If a diﬀerence de-
rived from the inferred models do not result in a diﬀerence
in the actual programs, the corresponding input is reused as
a counterexample to further reﬁne the model.

Comparing two models in order to obtain counterexam-
ples also provides a way to implement an equivalence oracle
which checks the correctness of an inferred model and con-
stitutes an essential component of the learning algorithm. In
practice, simulating such an oracle is a challenging and com-
putationally expensive task (cf.
section 3). Nevertheless,
our diﬀerential testing framework provides an eﬃcient and
elegant way to simulate an equivalence oracle by comparing
the inferred models, thus the term “diﬀerential automata
learning”.

Figure 1 shows an overview of SFADiff architecture. SFAD-

iff has several beneﬁts over the existing approaches: (i) it
explores the diﬀerences between similar programs in a sys-
tematic way and generalizes from the observations through
SFA models; (ii) it can ﬁnd and enumerate diﬀerences be-
tween SFA models eﬃciently; (iii) it can perform root cause
analysis eﬃciently in a domain-independent manner by us-
ing the inferred models; and (iv) it also supports eﬃcient
bootstrapping mechanisms for incremental SFA learning for
programs that only diﬀer slightly (e.g., two versions of the
same program).

We evaluated SFADiff in three diﬀerent settings for ﬁnd-
ing diﬀerences between multiple TCP implementations, be-
tween diﬀerent WAFs, and between the HTML/JavaScript

Figure 2: Types of queries that a learning algorithm
can perform in our learning model.

parsers of WAFs and Web browsers. SFADiff was able to
enumerate a large number of diﬀerences between the TCP
implementations in Linux, FreeBSD, and Mac OSX. In the
WAF setting, SFADiff found multiple diﬀerences between
diﬀerent WAFs as well as between diﬀerent versions of the
same WAF. Finally, SFADiff found three previously-unknown
HTML/JavaScript parsing diﬀerences between two popular
WAFs (PHPIDS 0.7 and Expose 2.4.0) and several major
browsers like Google Chrome, Safari, Firefox, and Internet
Explorer. Our experiments conﬁrmed that all of these diﬀer-
ences can be leveraged to launch successful cross-site script-
ing attacks while evading the vulnerable WAFs.

In summary, our main contributions are as follows:
• In section 4, we describe the design and implemen-
tation of SFADiff, the ﬁrst diﬀerential testing frame-
work based on automata learning techniques. We show
that our framework can be used to perform several se-
curity critical tasks automatically such as ﬁnding eva-
sion attacks, generating ﬁngerprints, and identifying
the root causes of the observed diﬀerences in a domain-
independent manner.
• In section 3, we provide an eﬃcient algorithm to boot-
strap the SFA learning process from an initial model
that allows for eﬃcient incremental inference of similar
programs.
• In section 5, we evaluate SFADiff on eleven appli-
cations from three diﬀerent domains and show that
it is able to ﬁnd a large number of diﬀerences in all
domains, including three previously-unknown evasion
attacks against two popular WAFs, Expose and PH-
PIDS.

2. PRELIMINARIES
2.1 Deﬁnitions

A deterministic ﬁnite automaton (DFA) M over an al-
phabet Σ with set of states Q is speciﬁed by a transition
function δ : Q × Σ → Q. The subset F ⊆ Q is called the
set of accepting states. The language accepted by the au-
tomaton is denoted by L(M ) and contains all those strings
in Σ∗ that, when parsed by the automaton starting from
the initial state q0 ∈ Q, lead to a state in F . Each DFA M
induces a corresponding graph GM = (V, E) where V = Q
and (qi, qj) ∈ E if and only if δ(qi, α) = qj for some a ∈ Σ.

Learning	algorithm	Diﬀerence	analysis		Check	if	the	diﬀerences	are	real	Counterexamples:	refuted	diﬀerences	Bootstrapping	through	ini<aliza<on	Program1	Program2	Program	n	SFA	2	SFA	1	SFA	n	……Stop	if		no	diﬀerence	1691We also denote an edge (qi, qj) ∈ E as qi → qj. We write
∗→ qj to denote that there exists a path in GM between qi
qi
and qj. We say that a path is simple if it does not contain
any loops.
For a given automaton M , string w ∈ Σ∗ and state q ∈
Q we denote by Mq[w] the state that is reached when the
automaton parses the string w, starting from state q. When
the subscript is omitted the initial state q0 is assumed. We
also deﬁne the function l : Q → {0, 1} such that l(q) = 1 if
and only if q ∈ F . It follows that L(M ) = {w | l(Mq0 [w]) =
1}. We denote by  the empty string. For two strings s1,s2
and a set of strings W , we say that s1 ≡ s2 mod W if, for
every w ∈ W it holds that l(M [s1 · w]) = l(M [s2 · w]). A
predicate family P is a set of predicates. The following sets
of strings deﬁned for an automaton M play a fundamental
role in learning algorithms:
• Access strings. We say that a string s access a state q
if M [s] = q. The set of access strings for an automaton
M is a set A such that, for each state q in M there exists
s ∈ A such that s access q.
• Distinguishing strings. The set of distinguishing strings
is a set of strings D for which it holds that for each pair
of states q, q(cid:48) it holds that there is some d ∈ D such that
l(Mq[d]) (cid:54)= l(Mq(cid:48) [d]).

Symbolic Finite Automata. Symbolic ﬁnite automata
(SFA) are ﬁnite state machines that decide an input string
by performing state transitions controlled by predicate mem-
bership. A DFA is a special case of an SFA where the pred-
icate family is restricted to the forms “x = a” for a ∈ Σ.
We will adopt the following deﬁnition that has been used to
formally describe this class of machines [5] :

Deﬁnition 1. A symbolic ﬁnite automaton (SFA) is a tuple
(Q, q0, F,P, ∆), where Q is a ﬁnite set of states, q0 ∈ Q
the initial state, F ⊆ Q is the set of ﬁnal states, P is a
predicate family and ∆ ⊆ Q × P × Q is the move relation.
For each state q, we deﬁne the guard predicate set as follows
guard(q) := {φ : ∃p ∈ Q, (q, φ, p) ∈ ∆}.

Extension to programs with non-binary output. Due
to space constraints, we describe our algorithms for the case
of programs with binary output. Nevertheless, to model
programs with general output, SFAs can be replaced with
symbolic ﬁnite state transducers (SFTs) [30], and the corre-
sponding learning algorithms for transducers [5] can be used.
All of our algorithms can be easily extended to transducers.
2.2 Learning Model

The learning algorithms used in this paper work in an
active learning model called exact learning from member-
ship and equivalence queries. Contrary to the traditional
supervised machine learning setting, where the models are
trained on a given dataset, active learning algorithms are
able to query the target machine with any input of their
choice and obtain the correct label for that input from the
target. Speciﬁcally, in our learning model, we assume that
a learner, who is trying to learn an unknown automaton M ,
has access to an oracle answering two types of queries: (i)
membership queries through which the learner can submit a
string s and obtain whether s ∈ L(M ) or not and (ii) equiva-
lence queries through which the learner can submit a model
H and obtain whether L(H) = L(M ). Figure 2 shows a
pictorial presentation of these queries.

x (cid:54)=<

q0

x (cid:54)= a

x =<

q1

x (cid:54)=>

x = a

true

q2

x =>

q3

aaaaa

S W 
0

0
<
0
<a
<a>
1

a> >

0
1
0
0

0
0
1
0

Figure 3: A Symbolic Finite Automaton (SFA) for
the regular expression .*<a>.* and the correspond-
ing entries for the S, W sets from the observation
table.
2.3 SFA Learning Algorithm

For learning SFAs, we use the ASKK algorithm proposed
by Argyros et al. [5]. We present a brief overview of the al-
gorithm below and encourage the interested readers to check
[5] for more details. At a high level, the algorithm attempts
to reconstruct the set of access and distinguishing strings
for the target automaton, from which it is able to recover a
correct model of the target machine. The transitions of the
SFA are generated using a mechanism called the guardgen()
algorithm that, given a sample set of transitions as input,
generates a set of predicate guards for the SFA model.

The main data structure utilized by the algorithm is the
special observation table SOT = (S, W, Λ, T ), where S and
W are, possibly incomplete, sets of access and distinguishing
strings for the target automaton, Λ ⊆ S· Σ is a set of sample
transitions and T is a table with rows over S∪Λ and columns
in W . Given a row s and column w, the table is populated
with T (s, w) = l(M [sw]). Figure 3 shows a simple SFA
along with the observation table entries for the S and W
sets.
The algorithm initializes the table with S = W = {} and
a set of sample transitions Λ (a single symbol suﬃces). The
SOT is called closed if for every α ∈ Λ, there exists s ∈ S
such that α ≡ s mod W . Once all entries in the table are
populated using membership queries, the table is checked for
closedness. If there exists an α ∈ Λ such that the closedness
condition is not satisﬁed, then α is accessing a previously
undiscovered state in the target automaton. Thus, we move
α into the set S, ﬁll the new entries in the table, and check
again for closedness. Eventually, this process will produce a
closed SOT if the target language is regular.
Updating models. Given a closed SOT , the learning algo-
rithm constructs an SFA model. This model is then tested
for equivalence with the target automaton. In the abstract
learning model this is achieved using a single equivalence
in practice, various testing methods are
query, however,
utilized to simulate an equivalence query.
If the learned
model is not equivalent to the target machine, the equiv-
alence query returns a counterexample input s that causes
the model to produce diﬀerent output than the target ma-
chine. The learning algorithm uses the counterexample to
reﬁne the generated model by either adding a missing state
or correcting an invalid transition.

3. BOOTSTRAPPING SFA LEARNING
Motivation. Consider a user that has invested a signiﬁ-

1692cant time budget to infer an SFA model for a speciﬁc ver-
sion of a program. When a new version of the program is
released, one can expect it to be, in many aspects, similar
with the previous version. In such settings, the ability to
incrementally learn the SFA model for the new version can
be a very useful feature. The learning process will become
signiﬁcantly faster if SFADiff can somehow utilize the old
model for learning the new model. In this section, we pro-
vide an eﬃcient algorithm in order to bootstrap the SFA
learning algorithm by initializing it with an existing model.
Our method ensures that, if the system we are trying to in-
fer is the same as the model used for initializing the learning
algorithm, only a single equivalence query will be made by
the learning algorithm in order to verify the equivalence of
the model with the system. Since simulating equivalence
queries is usually the most expensive part in learning, being
able to save equivalence queries provide a signiﬁcant overall
optimization in the learning process.

Notice that, most popular algorithms for simulating equiv-
alence queries are intractable for large alphabets. For ex-
ample, consider the case of Chow’s W-method [12], that is
used by popular automata inference frameworks like Learn-
Lib [24] for simulating equivalence queries. The W-method
accepts as input a model automaton M with m states and
an upper bound n on the number of states of the target
automaton. The W-method compiles a set of test cases to
verify that, if the target automaton has at most n states,
then it is equivalent to the model automaton. Unfortu-
nately, in order to verify equivalence, the W-method per-
forms O(n2m|Σ|n−m+1) membership queries to the target
system. The exponential term in the alphabet size makes
the method prohibitive for usage in models with large al-
phabets (e.g. all printable characters or even larger sets if
we include Unicode symbols).
Our algorithm. Given an initial SFA model Minit we boot-
strap the ASKK algorithm by creating a special observation
table SOT = (S, W, Λ, T ) with the S, W, Λ sets initialized
from Minit, as described below, while the entries of the table
are ﬁlled using membership queries to the target automa-
ton. This technique allows us to build a correct model if the
initial model and the target system are equivalent. If the
two systems are not equivalent but similar, i.e. they share
certain access and distinguishing strings, then our initial-
ization algorithm will recover those without performing any
equivalence queries. We will now describe how to initialize
each component of the special observation table.

Initializing the SOT

3.1
Initializing S. Initializing S corresponds to the recovery of
all access strings of Minit. This is a straightforward procedure
using a DFS search in the graph induced by Minit. The
procedure starts with an empty access string for the initial
state of the automaton. Every time we exercise a transition
(qs, φ, qt), we check if an access string for qt is already in
S. If no access string exists for qt then, we select a witness
α ∈ φ from the predicate guard of the transition and we
assign the access string sqs α for state qt where sqs ∈ S is an
access string for qs. Once all states are covered, we return
the set of access strings.
Initializing W . Initializing the W set corresponds to the
creation of a set of distinguishing strings for Minit. Algo-
rithms for creating distinguishing sets for DFAs date back to
the development of Chow’s W-method [12]. Adapting these

algorithms in the SFA setting is straightforward by adapt-
ing the SFA minimization algorithms developed recently by
D’Antoni and Veanes [14]. We note that these algorithms
are the most eﬃcient known algorithms for SFA minimiza-
tion and the adaptation for generating a set of distinguish-
ing strings will produce a set of distinguishing strings of size
n − 1 for an SFA with n states.
Initializing Λ. In order to correctly initialize the Λ com-
ponent of the SOT , we have to provide, for every state q of
Minit a set of sample transitions that, when given as input
into the guardgen() algorithm will produce the correct set
of predicate guards for q.

The predicate guards used by the SFA learning algorithm
in [5] are simply sets of symbols from the alphabet. Given a
set of sample transitions for a state q, the guardgen() algo-
rithm from [5] works as follows: All transitions for symbols
from state q already in the Λ set are grouped into predi-
cate guards based on the target of the transition which is
determined as in the original L∗ algorithm [6]. The transi-
tions for symbols which are not part of the Λ set are merged
into the predicate guard with the largest size, i.e. the tran-
sition containing most symbols. The intuition behind this
algorithm is that in most parsers, only a small numbers of
symbols is advancing the automaton towards an accepting
state, while most other symbols are grouped together in a
single transition leading to a rejecting state.

Therefore, given a state q in Minit, in order to construct a
sample set of transitions that will result in producing the cor-
rect predicate guards with the aforementioned guardgen()
algorithm, we proceed as follows: Let {φ1, φ2, . . . , φk} be
the set of predicate guards for the state q such that i <
j =⇒ |φi| ≥ |φj|. Moreover, let sq be the access string for
q and T = ∪i∈{2,...,k}φi. Then, for each α ∈ T , we add the
string sqα in Λ. This will ensure that the predicate guards
for φ2, . . . , φk will be produced correctly by the guardgen()
algorithm. Finally, we have to ensure that enough sample
transitions from φ1 are added in Λ in order for φ1 to get
all implicit transitions which are not part of Λ. To achieve
that, we select l2 = |φ2| + 1 elements αj ∈ φ1, j ∈ {1, . . . , l2}
and add the strings sqαj in Λ. This operation ensures that
if the transitions of the target automaton are the same as
in Minit, they will be generated correctly by the guardgen()
algorithm. Repeating this procedure for all states of Minit
completes the initialization of the Λ set.

4. DIFFERENTIAL SFA LEARNING
4.1 Basic Algorithm

The main idea behind our diﬀerential testing algorithm is
to leverage automata learning in order to infer SFA-based
models for the test programs and then compare the result-
ing models for equivalence as shown in Figure 1. As men-
tioned above, this technique has a number of advantages
such as being able to generalize from comparing individual
input/output pairs and build models for the programs that
are examined.

Algorithm 1 provides the basic algorithmic framework for
diﬀerential testing using automata learning. The algorithm
takes two program implementations as input. The ﬁrst func-
tion calls, to the GetInitialModel function, are responsible
for bootstrapping the models for the two programs. In our
case this function is implemented using the observation ta-
ble initialization algorithm described in Section 3. The ini-

1693Algorithm 1 Diﬀerential SFA Testing Algorithm
Require: P1, P2 are two programs

function GetDifferences(P1, P2)

M1 ← GetInitialModel(P1)
M2 ← GetInitialModel(P2)
while true do
S ← RCADiﬀ(M1, M2)
if S = ∅ then
return ∅

end if
modelUpdated ← F alse
for s ∈ S do

if P1(s) (cid:54)= M1(s) then

M1 ← UpdateModel(M1, s)
modelUpdated ← T rue

end if
if P2(s) (cid:54)= M2(s) then

M2 ← UpdateModel(M2, s)
modelUpdated ← T rue

end if

end for
if modelUpdated = F alse then

return S

end if
end while
end function

tialized models are then checked for diﬀerences using the
RCADiff function call. The internals of this function are
described in detail in Section 4.2. This function is respon-
sible for categorizing the diﬀerences in the two models and
return a sample set of inputs covering all categories that
can cause the two programs to produce diﬀerent outputs.
The algorithm stops if the two models are equivalent. Oth-
erwise, RCADiff returns a set of inputs that cause the two
SFA models to produce diﬀerent output.

However, since these diﬀerences are obtained by compar-
ing the program models and not the actual programs, they
might contain false positives resulting from inaccurate mod-
els. To detect such cases, we verify all diﬀerences obtained
from the RCADiff call using the actual test programs. If any
input is found not to produce a diﬀerence in the implemen-
tations, then that input is used as a counterexample in order
to reﬁne the model through the UpdateModel call. Finally,
when a set of diﬀerences in the two models is veriﬁed to
contain only true positives, the algorithm returns the set of
corresponding inputs back to the user.

The astute reader may notice that, if no candidate dif-
ferences are found between the two models, the algorithm
terminates. For this reason, model initialization plays a sig-
niﬁcant role in our algorithm, since the initialized models
should be expressive enough in order to provide candidate
diﬀerences. It is interesting to point out that the candidate
diﬀerences do not have to be real diﬀerences.
4.2 Difference Analysis

Assume that we found and veriﬁed a number of inputs
that cause the two programs under test to produce diﬀerent
outputs. One fundamental question is whether we can clas-
sify these inputs in certain equivalence classes based on the
cause of the deviant behavior. We will now describe how
we can use the inferred SFAs in order to compute such a
classiﬁcation. Ideally, we would like to assign in two inputs

Algorithm 2 Diﬀerence Categorization Algorithm
Require: M1, M2 are two SFA Models

function RCADiff(M1, M2)

Mprod ← ProductSFA(M1, M2)
S ← ∅
for (qi, qj) ∈ Qprod | l(qi) (cid:54)= l(qj) do

S ← S ∪ SimplePaths(Mprod, (qi, qj))

end for
return Path2Input(S)

end function

that cause a diﬀerence the same root cause if they follow
the same execution paths in the target programs. Since the
program source is unavailable, we trace the execution path
of the inputs in the respective SFA models.
RCADiﬀ algorithm. Given two SFAs M1 and M2, it is
straightforward to compute their intersection by adapting
the classic DFA intersection algorithm [28]. Let Mprod =
(Q1 × Q2, (q0, q0),{(qi, qj) : qi ∈ F1 ∧ qj ∈ F2},P, ∆) be
the, minimal, product automaton of M1, M2. Notice ini-
tially, that the reason a diﬀerence is observed in the output
after processing an input in both SFAs is that the labels of
the states reached in the two machines are diﬀerent. This
motivates our deﬁnition of points of exposure.

Deﬁnition 2. Let Mprod be the intersection SFA of M1, M2
as deﬁned above. We deﬁne the set {(qi, qj)|(qi, qj) ∈ Qprod∧
qi ∈ Q1∧qj ∈ Q2∧l(q1) (cid:54)= l(q2)} to be the points of exposure
for the diﬀerences between M1, M2.

Intuitively, the points of exposure are the reasons the dif-
ferences in the programs are observed through the output of
programs. The path to a point of exposure encodes two dif-
ferent execution paths in machines M1 and M2 respectively
which, under the same input, end up in states producing
diﬀerent output. Thus, we say that any simple path to a
point of exposure is a root cause of a diﬀerence.

Deﬁnition 3. Let M1, M2 be two SFAs and Mprod be the
intersection of M1, M2. Let Qp ⊆ Qprod be the points of
exposure for Mprod. We say that the set of simple paths
∗→ qp|qp ∈ Qp} is the set of root causes for the
S = {q0
diﬀerences between M1 and M2.

Equipped with the set of paths our classiﬁcation algorithm
works as follows: Given two inputs causing a diﬀerence, we
ﬁrst reduce the path followed by each input into a simple
path, i.e. we remove all loops from the path. For example,
an input following the path q0 → q4 → q5 → q4 → q10 will
be reduced to the path q0 → q4 → q10. Afterwards, we
classify the two inputs in the same root cause if the simple
paths followed by the inputs are the same.

Algorithm 2 shows the pseudocode for the RCADiff algo-
rithm. The algorithm works by collecting all the distinct
root causes from the product automaton using the the Sim-
plePaths function call. This function accepts an SFA and
a target state and returns all simple paths from the ini-
tial state to the target state using a BFS search. After-
wards, each path is converted into a sample input through
the function Path2Input. This function works by selecting,
for each edge qi → qj in the path, a symbol α ∈ Σ such that
(qi, φ, qj) ∈ ∆ ∧ φ(α) = 1. Finally, these symbols are con-
catenated in order to form an input that exercise the given
path in the SFA.

16944.3 Differentiating Program Sets

In this section, we describe how our original diﬀerential
testing framework can be generalized into a GetSetDiffer-
ences algorithm which works as follows:
Instead of get-
ting two programs as input, the GetSetDifferences algo-
rithm receives two sets of programs I1 = {P1, . . . , Pn} and
I2 = {P1, . . . , Pm}. Assume that the output of each pro-
gram is a bit b ∈ {0, 1}. The goal of the algorithm is to ﬁnd
a set of inputs S such that, the following condition holds:

∃b ∀P1 ∈ I1, P1(s) = b ∧ ∀P2 ∈ I2, P2(s) = 1 − b

While conceptually simple, this extension provides a num-
ber of nice applications. For example, consider the problem
of ﬁnding diﬀerences between the HTML/JavaScript parsers
of browsers and those of WAFs. While ﬁnding such diﬀer-
ences between a single browser and a WAF will provide us
with an evasion attack against the WAF, the GetSetDif-
ferences algorithm allows us to answer more sophisticated
questions such as: (i) Is there an evasion attack that will
bypass multiple diﬀerent WAFs? and (ii) Is there an eva-
sion attack that will work across diﬀerent browsers? Also,
as we describe in Section 4.4, this extension allows us to pro-
duce succinct ﬁngerprints for distinguishing between multi-
ple similar programs.
GetSetDiﬀerences Algorithm. We extend our basic Get-
Differences algorithm as follows: First, instead of initial-
izing two program models as before, we initialize the SFA
models for all programs in both sets accordingly. Similarly,
when we verify the candidate diﬀerences obtained from the
inferred models, all programs in both sets should be checked.
Besides these changes, the skeleton of the GetDiﬀerences al-
gorithm remains the same.

The most crucial and time-consuming part of our exten-
sion is the extension to the RCADiff functionality in order to
detect diﬀerences between two sets of models. Recall that
RCADiﬀ utilizes the product construction and then ﬁnds
the simple paths leading to the points of exposure. Given
two sets of models, we compute the intersection between all
the models in the two sets. Afterwards, we set the points
of exposure as follows. Let q = (q0, . . . , qm+n) be a state
in the product automaton. Furthermore, assume that state
qi corresponds to automata Mi from one of the input sets
I1,I2. Then, q is a point of exposure if

∀Mi ∈ I1, Mj ∈ I2 =⇒ l(qi) (cid:54)= l(qj)

With this new deﬁnition of the points of exposure, the mod-
iﬁed RCADiff algorithm proceeds as in the original case to
ﬁnd all simple paths in the product automaton that lead to
the points of exposure.

One potential downside of this algorithm is that, its com-
plexity increases exponentially as we add more models in
the sets. For example, computing the intersection of m DFA
with n states each, requires time O(nm) while, in general,
the problem is PSPACE-complete [21]. That being said, we
stress that the number of programs we have to check in prac-
tice will likely be small and many additional heuristics can
be used to reduce the complexity of the intersection compu-
tation.
4.4 Program Fingerprints
Formally, the ﬁngerprinting problem can be described as
follows: given a set I of m diﬀerent programs and black-box
access to a server T which runs a program PT ∈ I, how

Algorithm 3 Fingerprint Tree Building Algorithm
Require: I is a set of Programs

function BuildFingerprintTree(I)

if |I| = 1 then

root.data ← P ∈ I
return root

end if
Pi, Pj ← I
s ← GetDiﬀerences(Pi, Pj)
root.data ← s
root.left ← BuildFingerprintTree(I \ Pi)
root.right ← BuildFingerprintTree(I \ Pj)
return root

end function

can one ﬁnd out which program is running in the server T
by simply querying the program in a black-box manner, i.e.
ﬁnd P ∈ I such that P = PT .

In this section, we present two diﬀerent ﬁngerprinting al-
gorithms that provide diﬀerent trade-oﬀs between computa-
tional and query complexity. Both these algorithms build
a binary tree called ﬁngerprint tree that stores strings that
can distinguish between any two programs in I. Given a
ﬁngerprinting tree, our ﬁrst algorithm requires |I| queries
to the target program. If the user is willing to perform ex-
tra oﬀ-line computation, our second algorithm demonstrates
how the number of queries can be brought down to log m.
Basic ﬁngerprinting algorithm. The BuildFingerprint-
Tree algorithm (shown in Algorithm 3) constructs a binary
tree that we call a ﬁngerprint tree where each internal node
is labeled by a string and each leaf by a program identi-
ﬁer.
In order to build the ﬁngerprint tree recursively, we
start with the set of all programs I, choose any two arbi-
trary programs Pi, Pj from I, and use the diﬀerential testing
framework to ﬁnd diﬀerences between these programs. We
label the current node with the diﬀerences, remove Pi and
Pj from I, and call BuildFingerprintTree recursively until
a single program is left in I. If I has only one program, we
label the leaf node with the program and return.

Given a ﬁngerprint tree, we solve the ﬁngerprinting prob-
lem as follows: Initially, we start at the root node and query
the target program with a string from the set that labels
the root node of the tree.
If the string is accepted (resp.
rejected), we recursively repeat the process along the left
subtree (resp. right subtree), until we reach a leaf node that
identiﬁes the target program.
Time/query complexity. For the following we assume an
input set of programs I of size |I| = m. Our algorithm has

to ﬁnd diﬀerences between all (cid:0)m
time complexity of the algorithm is O(2m−1 +(cid:0)m

The ﬁngerprint tree resulting from the algorithm will be a
full binary of height m. Assuming that the complexity of the
diﬀerential testing algorithm is D, we get that the overall
query complexity of the algorithm is |I|-1 queries, since each
query will discard one candidate program from the list.
Reducing queries using shallow ﬁngerprint trees. No-
tice that, in the previous algorithm, we need m queries to
the target program in order to ﬁnd the correct program be-
cause we discard only one program at each step. We can cut
down the number of queries by shallower ﬁngerprint trees
at the cost of higher oﬀ-line computational complexity for
building such trees.

(cid:1) diﬀerent program pairs.
(cid:1)D). The

2

2

1695Consider the following modiﬁcation in the BuildFinger-
printTree algorithm: First, we partition I into k subsets
I1, . . .Ik of size m/k each. Next, we call BuildFinger-
printTree algorithm with the set IS = {I1, . . . ,Ik} as input
programs and replace the call to GetDifferences with Get-
SetDifferences. This algorithm will generate a full binary
tree of height k that can distinguish between the programs
in the diﬀerent subsets of I. We can recursively apply the
same algorithm on each of the leafs of the resulting ﬁnger-
printing tree, further splitting the subsets of I until each
leaf contains a single program.
Time/query complexity. It is evident that the algorithm
will eventually terminate since each subset is successively
portioned into smaller sets. Let us assume that Dset(k) the
complexity of the GetSetDifferences algorithm when the
input program sets are of size k (see section 4.3 for a com-
plexity analysis of Dset(k)). The number of queries required
for ﬁngerprinting an application with this algorithm will be
equal to the height of the resulting ﬁngerprint tree. Note
that each subset is of size m/k and to distinguish between
the k subsets using our basic algorithm we need k−1 queries.
Therefore we get the equation T (m) = T (m/k) + (k − 1)
describing the query complexity of the algorithm. Solving
the equation we get that T (m) = (k − 1) logk m which is
the query complexity for a given k. When k = 2 we will
need log m queries to identify the target program. Since
each program provides one bit of information per query (ac-
cept/reject), a straightforward decision tree argument [13]
provides a matching lower bound on the query complexity
of the problem.

Regarding the time complexity of the problem, we notice
that, at the i-th recursive call to the modiﬁed BuildFin-
gerprintTree algorithm, we will have an input set of size
m/ki since the initial set is repeatedly partitioned into k
subsets. the overall time complexity of building the tree is

(cid:1)Dset(m/ki)). We omit further de-

(cid:80)logk m

+(cid:0)m/ki

2

(2m/ki

i=1

tails here as the complexity analysis is a straightforward
adaptation of the original analysis.
5. EVALUATION
5.1 Initialization evaluation

Our ﬁrst goal is to evaluate the eﬃciency of our observa-
tion table initialization algorithm as a method to reduce the
number of equivalence queries while inferring similar mod-
els. The experimental setup is motivated by our assumptions
that the initialization model and the target model would
be similar. For that purpose, we utilized 9 regular expres-
sion ﬁlters from two diﬀerent versions of ModSecurity (ver-
sions 3.0.0 and 2.2.7) and PHPIDS WAFs (versions 0.7.0 and
0.6.3). The ﬁlters in the newer versions of the systems have
been reﬁned to either patch evasions or possibly to reduce
false positive rate.

For our ﬁrst experiment we used an alphabet of 92 sym-
bols, the same one used in our next experiments, which con-
tains most printable ASCII characters. Since, in this experi-
ment, we would like to measure the reduction oﬀered by our
initialization algorithm in terms of equivalence queries, we
simulated a complete equivalence oracle by comparing each
inferred model with the target regular expression.
Results. Table 1 shows the results of our experiments.
First, notice that in most cases the updated ﬁlters contain
more states than their previous versions. This is expected,

Figure 4: State machine inferred by SFADiﬀ for
Mac OSX TCP implementation. The TCP ﬂags that
are set for the input packets are abbreviated as fol-
lows: SYN(S), ACK(A), FIN(F), PSH(P), URG(U),
and RST(R).

since most of the times the ﬁlters are patched to cover ad-
ditional attacks, which requires the addition of more states
for covering these extra cases. We can see that, in general,
our algorithm oﬀers a massive reduction of approximately
50× in the number of equivalence queries utilized in order
to infer a correct model. This comes with a trade-oﬀ since
the number of membership queries are increased by a factor
of 1.15×, on average. However, equivalence queries are usu-
ally orders of magnitude slower than membership queries.
Therefore, the initialization algorithm results in signiﬁcant
overall performance gain. We notice that 2/3 cases where
we observed a large increase (more than 1.2×) in member-
ship queries (ﬁlters PHPIDS 50 & PHPIDS 56) are ﬁlters for
which states were removed in the new version of the system.
This is expected since, in that case, SFADiff makes redun-
dant queries for an entry in the observation table that does
not correspond to an access string. Another possible reason
for an increase in the number of the membership queries is
the chance that the distinguishing set obtained by the SFA
learning algorithm is smaller than the one obtained by the
initialization algorithm which is always of size n − 1 where
n is the number of states in a ﬁlter. Exploring ways to ob-
tain a distinguishing set of minimum size is an interesting
direction in order to further develop our initialization algo-
rithm. Nevertheless, in all cases, the new versions of the
ﬁlters were similar in structure with the older versions and
thus, our initialization algorithm was able to reconstruct a
large part of the ﬁlter and massively reduce the number of
equivalence queries required to obtain the correct model.

5.2 TCP state machines

For our experiments with TCP state machines, we run a
simple TCP server on the test machine while the learning
algorithm runs as a client on another machine in the same
LAN. Because the TCP protocol will, possibly, emit output
for each packet sent, the ASKK algorithm is not suited for
this case. Thus, we used the algorithm from [5] for learn-
ing deterministic transducers in order to infer models of the
TCP state machines.
Alphabet. For this set of experiments, we focus on the
eﬀect of TCP ﬂags on the TCP protocol state transitions.

1696IDS Rules

Member

Equiv

Member

Equiv

Without Init

With Init

Learned
States

Init Filter

States

States

Diﬀ

Member
Overhead

Equiv

Speedup

MODSEC 973323
MODSEC 973324
MODSEC 973330

PHPIDS 22
PHPIDS 27
PHPIDS 40
PHPIDS 41
PHPIDS 50
PHPIDS 56

2367
768
887

17195
144759
11119
6635
6206
38768

97
55
62
252
2618
337
318
255
840

2400
892
941

17330
149159
11152
8535
9829
46732

2
19
21
105
437
68
137

1
7

25
15
15
70
66
35
25
25
60

25
12
12
45
59
25
21
27
62

0
3
3
25
7
10
4
-2
-2

1.01
1.16
1.06
1.01
1.03
1.00
1.29
1.58
1.21

48.50
2.89
2.95
2.40
5.99
4.96
2.32

255.00
120.00

Avg= 537.11×

Avg= 88.56×

Avg= 1.15× Avg= 49.45×

Table 1: The performance (no. of equivalence and membership queries) of the SFA learning algorithm with
and without initialization for diﬀerent rules from two WAFs (ModSecurity OWASP CRS and PHPIDS).

OS
OSX Yosemite (version 14.5.0)
Debian Linux (Kernel v3.2.0)
FreeBSD 10.3

States Queries
7
9
9

858
1100
1100

Table 2: Results for diﬀerent TCP implementations:
Number of states in each model and number of mem-
bership queries required to infer the model.

Input
S, S
S, A, F
S, RA, A

Linux
SA, RA

SA, A, FA

SA, R

OSX

FreeBSD

SA, RA, RA

SA

SA, R

SA
SA
SA

Table 3: Some example ﬁngerprinting packet se-
quences found by SFADiﬀ across diﬀerent TCP im-
plementations. The TCP ﬂags that are set for the
input packets are abbreviated as follows: SYN(S),
ACK(A), FIN(F), and RST(R).

Speciﬁcally, we select an alphabet with 11 symbols including
6 TCP ﬂags: SYN(S), ACK(A), FIN(F), PSH(P), URG(U),
and RST(R) along with all possible combinations of these
ﬂags with the ACK ﬂag, i.e., SA, FA, PA, UA, and RA.
Membership queries. Once our learning algorithm for-
mulates a membership query, our client implementation cre-
ates a sequence of TCP packets corresponding to the sym-
bols and sends them to the server.

Our server module is a simple python script which works
as follows: The script is listening for new connections on a
predeﬁned port. Once a connection is established our server
module makes a single recv call and then actively close the
connection. In addition, for each diﬀerent membership query
we spawn a new server process on a diﬀerent port to ensure
that packets belonging to diﬀerent membership queries will
not be mixed together.

The learning algorithm handles the sequence and acknowl-
edgement numbers in the outgoing TCP packets in the fol-
lowing way: a random sequence number is used as long as no
SYN packet is part of a membership query; otherwise, after
sending a SYN packet we set the sequence and acknowledge-
ment numbers of the following packets in manner consistent
with the TCP protocol speciﬁcation. In case the learning
algorithm receives a RST packet during the execution of a
membership query, we also reset the state of the sequence
numbers, i.e. we start sending random sequence numbers
again until the next SYN packet is send.

After sending each packet from a membership query, the
learning algorithm waits for the response for each packet us-
ing a time window. If the learning algorithm receives any re-
transmitted packets during that time, it ignores those pack-
ets. We detect re-transmitted packets by checking for du-
plicate sequence/acknowledgement numbers.
Ignoring the
re-transmitted packets is crucial for the convergence of the
learning algorithm as it helps us avoid any non-determinism
caused by the timing of the packets.
Initialization. As TCP membership queries usually out-
puts more information in terms of packets than one bit, our
algorithm worked eﬃciently for the TCP implementations
even without any initialization. Thus, for this experiment,
we start the learning algorithm without any initial model.
Results. We used SFADiff in order to infer models for the
TCP implementations of three diﬀerent operating systems:
Debian Linux, Mac OSX and FreeBSD. The inferred models
contain all state transitions that are necessary to capture a
full TCP session. Figure 4 shows the inferred state machine
for Mac OSX. States in green color are part of a normal
TCP session while states in red color are reached when an
invalid TCP packet sequence is sent by the client. The path
q0 → q1 → q3 is where the TCP three-way handshake takes
place and it is leading to state q3 where the connection is es-
tablished, while the path q3 → q6 → q0 close the connection
and returns to the initial state (q0). Table 2 shows that the
inferred model for Mac OSX contain fewer states than the
respective FreeBSD and Linux models. Manual inspection
of the models revealed that these additional states are due
to diﬀerent handling of invalid TCP packet sequences. Fi-
nally, in Table 3, we present some sample diﬀerences found
by SFADiff. Note that, even though the state machines
of Linux and FreeBSD contain the same number of states,
they are not equivalent, as we can see in Table 3, since the
two implementations produce diﬀerent outputs for all three
inputs.
5.3 Web Application Firewalls and Browsers
In this setting, we perform two sets of experiments: (i)
we use SFADiff to explore diﬀerences in HTML/JavaScript
signatures used by diﬀerent WAFs for detecting XSS attacks;
and (ii) we use SFADiff to ﬁnd diﬀerences in the JavaScript
parsing implementation of the browsers and the WAFs that
can be exploited to launch XSS attacks while bypassing the
WAFs.

1697reused in the future. The details of our implementation of
membership queries for the browsers is shown in Figure 3.
Membership queries to the WAF. SFADiff sends an
HTTP request to the WAFs containing the corresponding
HTML/JavaScript string as payload to perform a member-
ship request, The WAF analyzes the request, decides whether
to allow/block the payload, and communicates the decision
back to SFADiff. SFADiff caches the results of the mem-
bership queries in order to be reused in the future.
Equivalence queries. We perform equivalence queries in
two ways: ﬁrst, whenever an equivalence query is sent ei-
ther to the browser or to a WAF, we check that the model
complies to the answers of all membership queries made so
far. This ensures that simple model errors will be corrected
before we perform more expensive operations such as cross-
checking the two models against each other. Afterwards,
we proceed to collect candidate diﬀerences and verify them
against the actual test programs as described in Section 4.
Initialization. We initialize the observation tables for both
the browser and the WAF using a small subset of ﬁlters that
come bundled with PHPIDS and ModSecurity, two open-
source WAFs in our test set. However, in the case of the
browser we slightly modify the ﬁlters in order to execute
our JavaScript function call if they are successfully parsed
by the browser.
Fingerprinting WAFs. In order to evaluate the eﬃciency
of our ﬁngerprint generation algorithm we selected 4 diﬀer-
ent WAFs. Furthermore, To demonstrate the ability of our
system to generate ﬁne-grained ﬁngerprints we also include
4 diﬀerent versions of PHPIDS in our test set. As an ad-
ditional way to avoid blowup in the ﬁngerprint tree size we
employ the following optimization: Whenever a ﬁngerprint
is found for a pair of ﬁrewalls, we check whether this ﬁnger-
print is able to distinguish any other ﬁrewalls in the set and
thus further reduce the remaining possibilities. This simple
heuristic signiﬁcantly reduces the size of the tree: Our basic
algorithm creates a full binary tree of height 8 while this
heuristic reduced the size of the tree to just 4 levels.

Figure 5.3 presents the results of our experiment. The re-
sulting ﬁngerprinting tree also provides hints on how restric-
tive each ﬁrewall is compared to the others. An interesting
observation is that we see the diﬀerent versions of PHPIDS
to be increasingly restrictive in newer versions, by rejecting
more of the generated ﬁngerprint strings. This is natural
since newer versions are usually patching vulnerabilities in
the older ﬁlters. Finally, we would like to point out that
some of the ﬁngerprints are also suggesting potential vulner-
abilities in some ﬁlters. For example, the top level string,
union select from, is accepted by all versions of PHPIDS up
to 0.6.5, while being rejected by all other ﬁlters. This may
raise suspicion since this string can be easily extended into
a full SQL injection attack.
Evading WAFs through browser parser inference.
For our last experiment we considered the setting of evaluat-
ing the robustness of WAFs against evasion attacks. Recall,
that, in the context of XSS attacks, WAFs are attempting to
reimplement the parsing logic of a browser in order to detect
inputs that will trigger JavaScript execution. Thus, ﬁnd-
ing discrepancies between the browser parser and the WAF
parser allows us to eﬀectively construct XSS attacks that
will bypass the WAF. In order to accomplish that, we used
the setup described previously. However, instead of cross-
checking the WAFs against each other, we cross-checked

Figure 5: The setup for SFADiﬀ ﬁnding diﬀer-
ences between the HTML/JavaScript parsing in
Web browsers and WAFs.

Figure 6: The implementation of membership
queries for Web browsers.

For these tests, we conﬁgure the WAFs to run as a server
and the learning algorithm executes as a client on the same
machine. The browser instance is also running on the same
machine. The learning algorithm communicates with the
browser instance through WebSockets. The learning algo-
rithm can test whether an HTML page with some JavaScript
code is correctly parsed by the browser and if the embedded
JavaScript is executed or not by exchanging messages with
the browser instance. The overall setup is shown in Figure 5.
Alphabet. We used an alphabet of 92 symbols containing
most printable ASCII characters. This allows us to encode
a wide range of Javscript attack vectors.
Membership queries to the browser. In order to allow
the learning algorithm to drive the browser, we make the
browser connect to a web server controlled by the learning
algorithm. Next, the learning algorithm sends a message to
the browser over WebSockets with the HTML/JavaScript
content corresponding to a membership query as the mes-
sage’s payload. Upon receiving such a message, the browser
sets the query payload as the innerHTML of a DOM element
and waits for the DOM element to be loaded. The user’s
browser dispatches a number of events (such as “click”) on
the DOM element and examines if the provided string led
to JavaScript execution. These events are necessary for trig-
gering the JavaScript execution in certain payloads. In order
to examine if the JavaScript execution was successful, the
browser monitors for any change in the value of a JavaScript
variable located in the page. The payload, when executed,
changes the variable value in order to notify that the exe-
cution was successful. Furthermore, in order to cover more
cases of JavaScript execution, the user’s browser also moni-
tors for any JavaScript errors that indicate JavaScript exe-
cution. After testing the provided string, the user’s browser
sends back a response message containing a boolean value
that indicates the result. The results of the membership
queries are cached by the learning algorithm in order to be

Web	browser	WAF	SFADiﬀ	HTTP		request/	response	Web	Sockets	Membership	queries	Membership	queries	Ini>alize	SFA	for		Web	Browser	&	WAF	True%/%False%Membership%%query%DOM%Element%Insert%string%in%a%DOM%element%Trigger%Events%JS%Variable%Check%%JS%variable%Payload%manipulates%%JS%variable%Web%browser%1698x ∈ {=}

qp
1

qp
0

x ∈ \w

x (cid:54)∈ {=}

x (cid:54)∈ \w

qp
2

true

Figure 7: PHPIDS 0.7 parser (simpliﬁed version).

x (cid:54)∈ {;,-,!} ∪ \w

x ∈ \w

PHPIDS 0.4

Permits

Blocks

PHPIDS 0.6.4

case(

Permits

” background=a

Blocks

Permits

PHPIDS 0.5.0

Permits

union distinct (select

x ∈ {=}

x ∈ {;,-,!}

qc
2

qc
1

qc
0

x ∈ \w

x (cid:54)∈ {=}

x (cid:54)∈ \w

qc
3

true

) when 1 then

Blocks

Permits

Blocks

PHPIDS 0.6.3

PHPIDS 0.6.5

WebCastellum 1.8.4

union select from

Permits

Figure 8: Google Chrome parser (simpliﬁed ver-
sion).

WAFs against the web browser in order to detect inputs
which are successfully executing JavaScript in the browser,
however they are not considered malicious by the WAF.

Table 4 shows the result of a sample execution of our sys-
tem in the setting of detecting evasions. The execution time
of our algorithm was about 6 minutes, in which 53 states
were discovered in the browser parser and 36 states in PH-
PIDS. Our system converged fast into a vulnerability after
improving the generated SFA models using the cached mem-
bership queries. This optimization was very important in
order to correct invalid transitions generated by the learn-
ing algorithm in the inferred models. The number of invalid
attacks that were attempted was 4. Each failed attack led
to the reﬁnement of the SFA models and the generation
of new candidate diﬀerences. At some point the vector “<p
onclick=-a()></p>” was reported as a diﬀerence by SFAD-
iff.

We were able to detect the same vulnerability using all
major browsers and furthermore, the same problem was
found to aﬀect the continuation of PHPIDS, the Expose
WAF. Finally, we point out that our algorithm also found
three more variations of the same attack vector, using the
characters “!”, and “;”.
Evasion analysis. Figures 7 and 8 shows simpliﬁed mod-
els of the parser implemented by the WAF and the browser
respectively. These models contain a minimal number of
states in order to demonstrate the aforementioned evasion
attack. Notice that, intuitively, the cause for the vulnera-
bility is the fact that from state qp
1 the parser of PHPIDS
will return to the initial state with any non alphanumeric
input, while the Google Chrome parser has the choice to
ﬁrst transition to qc
3 using
any alphanumeric character. For example, with an input
“=!a” the product automaton will reach the point of expo-
sure (qp
3). Furthermore, using our root cause analysis,
all diﬀerent evasions we detected are grouped under a single
root cause. This is intuitively correct, since a patch, which
adds the missing state in the PHPIDS parser will address
all evasion attacks at once.

2 and then to an accepting state qc

0 , qc

5.4 Comparison with black-box fuzzing

To the best of our knowledge there is no publicly avail-
able black-box system which is capable of performing black-

select if(a

Permits

Blocks

Blocks

case(

ModSecurity 3.0.0

Expose 2.8.4

Blocks

Permits

‘>

Blocks

PHPIDS 0.7

Figure 9: Fingerprint tree for diﬀerent web applica-
tion ﬁrewalls.

box diﬀerential testing like SFADiff. A straightforward ap-
proach would be to use a black-box fuzzer (e.g. the PEACH
fuzzing platform [1]) and send each input generated by the
fuzzer to both programs. Afterwards, the outputs from both
programs are compared to detect any diﬀerences. Note that,
like SFADiff, fuzzers also start with some initial inputs
(seeds) which they mutate in order to generate more inputs
for the target program. We argue that our approach is more
eﬀective in discovering diﬀerences for two reasons:
Adaptive input generation. Fuzzers incorporate a num-
ber of diﬀerent strategies in order to mutate previous inputs
and generate new ones. For example, PEACH supports more
than 20 diﬀerent strategies for mutating an input. However,
assuming that a new input does not cause a diﬀerence, no
further information is extracted from it; the next inputs are
unrelated to the previous ones. On the contrary, each in-
put submitted by SFADiff to the target program provides
more information about the structure of the program and its
output determines the next input that will be tested. For ex-
ample, in the execution shown in table 4, SFADiff utilized
the initialization model and detected the additional state in
Chrome’s parser (cf. ﬁgures 7, 8). Notice that, the addi-
tional state in Chrome’s parser was not part of the model
used for initialization. This allowed SFADiff to quickly
discover an evasion attack after a few reﬁnements in the
generated models. Each reﬁnement discarded a number of
candidate diﬀerences and drove the generation of new inputs
based on the output of previous ones.

1699Attributes

Browser Model

WAF Model

Membership

Cached Membership

Equivalence

Cached Equivalence

Learned States

Cross-Check Times

6672
448

0
40
53
4

4241
780

3

106
36
4

Provided Browser Model

(<(p|div|form|input) onclick=a()>)

(</(p|div|form|input)>)

Vulnerability Discovered

<p onclick=;a()></p>

Execution Time

382.12 seconds

Table 4: A sample execution that found an evasion
attack for PHPIDS 0.7 and Google Chrome on MAC
OSX.

Root cause analysis. In the presence of a large number
of diﬀerences, black-box fuzzers are unable to categorize the
diﬀerences without some form of white-box access to the
program (e.g. crash dumps). On the other hand, as demon-
strated in the evasion analysis paragraph of section 5.3, our
root cause analysis algorithm provides a meaningful catego-
rization of the diﬀerences based on the execution path they
follow in the generated models.

6. RELATED WORK
Fingerprinting. Nmap [17] is a popular tool for OS ﬁn-
gerprinting that include mechanisms for ﬁngerprinting of
diﬀerent TCP implementations among other things. How-
ever, unlike SFADiff, the signatures of diﬀerent protocols
in nmap are manually crated and tested. Similarly, in the
WAF setting, Henrqiue et al. manually found several ﬁn-
gerprints for distinguishing popular WAFs. Massicotte et
al. [22] quantiﬁed the amount of signature overlap assum-
ing direct white-box access to the signature database of the
analyzed programs. They checked for duplication and in-
tersection across diﬀerent signatures. However, unlike our
approach here, their analysis did not involve any learning
mechanism.
Automated ﬁngerprint generation. Caballero et al.
[10] designed and evaluated an automated ﬁngerprinting sys-
tem for DNS implementations using simple machine learn-
ing classiﬁers like decision trees. They used targeted fuzzing
to ﬁnd diﬀerences between individual protocols. However,
Richardson et al. [25] showed that such techniques do not
tend to perform as good as the hand-crafted signatures for
OS ﬁngerprinting in realistic setting. Unlike these passive
learning-based techniques, we use active learning along with
automata inference for systematically ﬁnding and catego-
rizing the diﬀerences. Moreover, unlike SFADiff, none of
these techniques are capable of performing automated root
cause analysis in a domain-independent way.

Shu et al. [27] explored the problem of automatically ﬁn-
gerprinting TCP implementations. However, instead of ﬁnd-
ing new diﬀerences, they reused the handcrafted Nmap sig-

nature set [17] to create parameterized extended ﬁnite state
machine (PEFSM) models of these signatures for eﬃcient
ﬁngerprinting. By contrast, our technique learns the model
of the TCP implementations without depending on any hand-
crafted signatures. SFADiff is able to ﬁnd such diﬀerences
automatically, including multiple previously-unknown dif-
ferences between TCP implementations.

Brumley et al. [9] describes how to ﬁnd deviations in pro-
grams using symbolic execution that can be used for ﬁnger-
printing. However, such approaches suﬀer from the funda-
mental scalability challenges inherent in symbolic execution
and thus cannot be readily applied in large scale software
such as web browsers.
Diﬀerential testing. Diﬀerential testing is a way of test-
ing a program without any manually crafted speciﬁcations
by comparing its outputs to those of other comparable pro-
grams for the same set of inputs [23]. Diﬀerential testing has
been used successfully for testing a diverse set of systems
including C compilers [32], Java virtual machine implemen-
tations [11], SSL/TLS implementations [8], mobile applica-
tions for privacy leaks [20], PDF malware detectors [31],
and space ﬂight software [18]. However, unlike us, all these
projects simply try to ﬁnd individual diﬀerences in an ad hoc
manner rather than inferring models of the tested programs
and exploring the diﬀerences systematically.
Automata inference. The L∗ algorithm for learning de-
terministic ﬁnite state automata from membership and equiv-
alence queries was described by Angluin [4] and many vari-
ations and optimizations were developed in the following
years. Balcazzar et al. [6] provide an overview of diﬀerent
algorithms under a uniﬁed notation. Initializing the L∗ algo-
rithm was originally described by Groce et al. [19]. Symbolic
ﬁnite automata were introduced by Veanes et al. [29] as an
eﬃcient way to explore regular expression constraints, while
algorithms for SFA minimization were developed recently
by D’Antoni and Veanes [14]. The ASKK algorithm for in-
ferring SFAs was developed recently by Argyros et al. [5].
When access to the source code is provided Botinˇcan and
Babi´c [7] developed an algorithm for inferring SFT models
of programs using symbolic execution. The L∗ algorithm
and variations has being used extensively for inferring mod-
els of protocols such as the TLS protocol [26], security pro-
tocols of EMV bank cards [2] and electronic passport pro-
tocols [3]. While some of these works note that diﬀerences
in the models could be used for the purpose of ﬁngerprint-
ing, no systematic approach to develop and enumerate such
ﬁngerprints was described.

Fiterau-Brostean et al. [15, 16] used automata learning to
infer TCP state machines and then used a model checker
in order to check compliance with a manually created TCP
speciﬁcation. While similar in nature, our approach diﬀers
in the sense that our diﬀerential testing framework does not
require a manual speciﬁcation in order to check for discrep-
ancies between two implementations.
ACKNOWLEDGMENTS
The ﬁrst and fourth authors were supported by the Oﬃce of
Naval Research (ONR) through contract N00014-12-1-0166.
Any opinions, ﬁndings, conclusions, or recommendations ex-
pressed herein are those of the authors, and do not necessar-
ily reﬂect those of the US Government or ONR. Second and
ﬁfth authors were supported by H2020 Project Panoramix
# 653497 and ERC project CODAMODA, # 259152.

1700diﬀerential testing as a prelude to formal veriﬁcation.
In International Conference on Software Engineering
(ICSE), 2007.

[19] A. Groce, D. Peled, and M. Yannakakis. Adaptive

model checking. In Tools and Algorithms for the
Construction and Analysis of Systems, pages 357–370.
2002.

[20] J. Jung, A. Sheth, B. Greenstein, D. Wetherall,

G. Maganis, and T. Kohno. Privacy oracle: a system
for ﬁnding application leaks with black box diﬀerential
testing. In CCS, 2008.

[21] D. Kozen. Lower bounds for natural proof systems. In

FOCS, 1977.

[22] F. Massicotte and Y. Labiche. An analysis of signature

overlaps in Intrusion Detection Systems. In
IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN), 2011.

[23] W. McKeeman. Diﬀerential testing for software.

Digital Technical Journal, 10(1), 1998.

[24] H. Raﬀelt, B. Steﬀen, and T. Berg. Learnlib: A

library for automata learning and experimentation. In
Proceedings of the 10th international workshop on
Formal methods for industrial critical systems
(FMICS), 2005.

[25] D. Richardson, S. Gribble, and T. Kohno. The limits

of automatic OS ﬁngerprint generation. In ACM
workshop on Artiﬁcial intelligence and security
(AISec), 2010.

[26] J. D. Ruiter and E. Poll. Protocol state fuzzing of

TLS implementations. In USENIX Security
Symposium (USENIX Security), 2015.

[27] G. Shu and D. Lee. Network Protocol System
Fingerprinting-A Formal Approach. In IEEE
Conference on Computer Communications
(INFOCOM), 2006.

[28] M. Sipser. Introduction to the Theory of Computation,
volume 2. Thomson Course Technology Boston, 2006.

[29] M. Veanes, P. D. Halleux, and N. Tillmann. Rex:

Symbolic regular expression explorer. In International
Conference on Software Testing, Veriﬁcation and
Validation (ICST), 2010.

[30] M. Veanes, P. Hooimeijer, B. Livshits, D. Molnar, and

N. Bjorner. Symbolic ﬁnite state transducers:
Algorithms and applications. ACM SIGPLAN Notices,
47, 2012.

[31] W. Xu, Y. Qi, and D. Evans. Automatically evading

classiﬁers a case study on PDF malware classiﬁers. In
Proceedings of the 2016 Network and Distributed
Systems Symposium (NDSS), 2016.

[32] X. Yang, Y. Chen, E. Eide, and J. Regehr. Finding

and understanding bugs in C compilers. In PLDI,
2011.

7. REFERENCES
[1] Peach fuzzer. http://www.peachfuzzer.com/.

(Accessed on 08/10/2016).

[2] F. Aarts, J. D. Ruiter, and E. Poll. Formal models of
bank cards for free. In Software Testing, Veriﬁcation
and Validation Workshops (ICSTW), IEEE
International Conference on, 2013.

[3] F. Aarts, J. Schmaltz, and F. Vaandrager. Inference

and abstraction of the biometric passport. In
Leveraging Applications of Formal Methods,
Veriﬁcation, and Validation. 2010.

[4] D. Angluin. Learning regular sets from queries and

counterexamples. Information and computation,
75(2):87–106, 1987.

[5] G. Argyros, I. Stais, A. Keromytis, and A. Kiayias.

Back in black: Towards formal, black-box analysis of
sanitizers and ﬁlters. In Security and privacy (S&P),
2016 IEEE symposium on, 2016.

[6] J. Balc´azar, J. D´ıaz, R. Gavalda, and O. Watanabe.
Algorithms for learning ﬁnite automata from queries:
A uniﬁed view. Springer, 1997.

[7] M. Botinˇcan and D. Babi´c. Sigma*: Symbolic

Learning of Input-Output Speciﬁcations. In POPL,
2013.

[8] C. Brubaker, S. Jana, B. Ray, S. Khurshid, and
V. Shmatikov. Using frankencerts for automated
adversarial testing of certiﬁcate validation in SSL/TLS
implementations. In Security and privacy (S&P), 2016
IEEE symposium on, 2014.

[9] D. Brumley, J. Caballero, Z. Liang, J. Newsome, and

D. Song. Towards automatic discovery of deviations in
binary implementations with applications to error
detection and ﬁngerprint generation. In USENIX
Security Symposium (USENIX Security), 2007.
[10] J. Caballero, S. Venkataraman, P. Poosankam,

M. Kang, D. Song, and A. Blum. FiG: Automatic
ﬁngerprint generation. Department of Electrical and
Computing Engineering, page 27, 2007.

[11] Y. Chen, T. Su, C. Sun, Z. Su, and J. Zhao.
Coverage-directed diﬀerential testing of JVM
implementations. In Proceedings of the 37th ACM
SIGPLAN Conference on Programming Language
Design and Implementation, pages 85–99. ACM, 2016.

[12] T. Chow. Testing software design modeled by

ﬁnite-state machines. IEEE transactions on software
engineering, (3):178–187, 1978.

[13] T. H. Cormen. Introduction to algorithms. MIT press,

2009.

[14] L. D’Antoni and M. Veanes. Minimization of symbolic

automata. In ACM SIGPLAN Notices, volume 49,
pages 541–553. ACM, 2014.

[15] P. Fiter˘au-Bro¸stean, R. Janssen, and F. Vaandrager.
Learning fragments of the TCP network protocol. In
Formal Methods for Industrial Critical Systems. 2014.
[16] P. Fiter˘au-Bro¸stean, R. Janssen, and F. Vaandrager.

Combining model learning and model checking to
analyze TCP implementations. In International
Conference on Computer-Aided Veriﬁcation (CAV).
2016.

[17] Fyodor. Remote OS detection via TCP/IP

ﬁngerprinting (2nd generation).

[18] A. Groce, G. Holzmann, and R. Joshi. Randomized

1701