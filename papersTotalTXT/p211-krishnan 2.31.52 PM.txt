Video Stream Quality Impacts Viewer Behavior: Inferring

Causality Using Quasi-Experimental Designs

S. Shunmuga Krishnan
sarumuga@akamai.com

Akamai Technologies

University of Massachusetts, Amherst

Ramesh K. Sitaraman
& Akamai Technologies

ramesh@cs.umass.edu

ABSTRACT
The distribution of videos over the Internet is drastically
transforming how media is consumed and monetized. Con-
tent providers, such as media outlets and video subscrip-
tion services, would like to ensure that their videos do not
fail, startup quickly, and play without interruptions. In re-
turn for their investment in video stream quality, content
providers expect less viewer abandonment, more viewer en-
gagement, and a greater fraction of repeat viewers, resulting
in greater revenues. The key question for a content provider
or a CDN is whether and to what extent changes in video
quality can cause changes in viewer behavior. Our work
is the ﬁrst to establish a causal relationship between video
quality and viewer behavior, taking a step beyond purely
correlational studies. To establish causality, we use Quasi-
Experimental Designs, a novel technique adapted from the
medical and social sciences.

We study the impact of video stream quality on viewer
behavior in a scientiﬁc data-driven manner by using exten-
sive traces from Akamai’s streaming network that include
23 million views from 6.7 million unique viewers. We show
that viewers start to abandon a video if it takes more than 2
seconds to start up, with each incremental delay of 1 second
resulting in a 5.8% increase in the abandonment rate. Fur-
ther, we show that a moderate amount of interruptions can
decrease the average play time of a viewer by a signiﬁcant
amount. A viewer who experiences a rebuﬀer delay equal to
1% of the video duration plays 5% less of the video in com-
parison to a similar viewer who experienced no rebuﬀering.
Finally, we show that a viewer who experienced failure is
2.32% less likely to revisit the same site within a week than
a similar viewer who did not experience a failure.

Categories and Subject Descriptors
C.4 [Performance of Systems]: Measurement techniques,
Performance attributes; C.2.4 [Computer-Communication
Networks]: Distributed Systems—Client/server

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

Keywords
Video quality, Internet Content Delivery, User Behavior,
Causal Inference, Quasi-Experimental Design, Streaming Video,
Multimedia

1.

INTRODUCTION

The Internet is radically transforming all aspects of human
society by enabling a wide range of applications for business,
commerce, entertainment, news and social networking. Per-
haps no industry has been transformed more radically than
the media and entertainment segment of the economy. As
media such as television and movies migrate to the Internet,
there are twin challenges that content providers face whose
ranks include major media companies (e.g., NBC, CBS),
news outlets (e.g., CNN), sports organizations (e.g., NFL,
MLB), and video subscription services (e.g., Netﬂix, Hulu).
The ﬁrst major challenge for content providers is pro-
viding a high-quality streaming experience for their view-
ers, where videos are available without failure, they startup
quickly, and stream without interruptions [24]. A major
technological innovation of the past decade that allows con-
tent providers to deliver higher-quality video streams to a
global audience of viewers is the content delivery network
(or, CDN for short) [8, 17]. CDNs are large distributed sys-
tems that consist of hundreds of thousands of servers placed
in thousands of ISPs close to end users. CDNs employ sev-
eral techniques for transporting [2, 12] media content from
the content provider’s origin to servers at the “edges” of the
Internet where they are cached and served with higher qual-
ity to the end user. (See [17] for a more detailed description
of a typical CDN architecture.)

The second major challenge of a content provider is to
actually monetize their video content through ad-based or
subscription-based models. Content providers track key met-
rics of viewer behavior that lead to better monetization.
Primary among them relate to viewer abandonment, engage-
ment, and repeat viewership. Content providers know that
reducing the abandonment rate, increasing the play time of
each video watched, and enhancing the rate at which view-
ers return to their site increase opportunities for advertising
and upselling, leading to greater revenues. The key ques-
tion is whether and by how much increased stream quality
can cause changes in viewer behavior that are conducive to
improved monetization. Relatively little is known from a
scientiﬁc standpoint about the all-important causal link be-
tween video stream quality and viewer behavior for online
media. Exploring the causal impact of quality on behavior

211and developing tools for such an exploration are the primary
foci of our work.

While understanding the link between stream quality and
viewer behavior is of paramount importance to the content
provider, it also has profound implications for how a CDN
must be architected. An architect is often faced with trade-
oﬀs on which quality metrics need to be optimized by the
CDN. A scientiﬁc study of which quality metrics have the
most impact on viewer behavior can guide these choices.
As an example of viewer behavior impacting CDN archi-
tecture, we performed small-scale controlled experiments on
viewer behavior a decade ago that established the relative
importance of the video to startup quickly and play without
interruptions. These behavioral studies motivated an archi-
tectural feature called prebursting [12] that was deployed
on Akamai’s live streaming network that enabled the CDN
to deliver streams to a media player at higher than the en-
coded rate for short periods of time to ﬁll the media player’s
buﬀer with more data more quickly, resulting in the stream
starting up faster and playing with fewer interruptions. It
is notable that the folklore on the importance of startup
time and rebuﬀering were conﬁrmed in two recent impor-
tant large-scale scientiﬁc studies [9, 15]. Our current work
sheds further light on the important nexus between stream
quality and viewer behavior and, importantly, provides the
ﬁrst evidence of a causal impact of quality on behavior.
1.1 Measuring quality and viewer behavior

The advent of customizable media players supporting ma-
jor formats such as Adobe Flash, Microsoft Silverlight, and
Apple HTTP streaming has revolutionized our ability to per-
form truly large-scale studies of stream quality and viewer
behavior as we do in this work, in a way not possible even a
few years ago. It has become possible to instrument media
players with an analytics plug-in that accurately measures
and reports both quality and behavioral metrics from every
viewer on a truly planetary scale.
1.2 From correlation to causality

The ability to measure stream quality and viewer behavior
on a global scale allows us to correlate the two in a statis-
tically signiﬁcant way. For each video watched by a viewer,
we are able to measure its quality including whether the
stream was available, how long the stream took to start up,
and how much rebuﬀerring occurred causing interruptions.
We are also able to measure the viewer’s behavior including
whether he/she abandoned the video and how long he/she
watched the video.

As a ﬁrst step, we begin by simply correlating important
quality metrics experienced by the viewers to the behavior
that they exhibit. For instance, we discover a strong cor-
relation between an increase in the delay for the video to
start up and an increase in rate at which viewers abandon
the video. Several of our results are the ﬁrst quantitative
demonstration that certain key streaming quality metrics
are correlated with key behavioral metrics of the viewer.
However, the deeper question is not just whether quality
and behavior are correlated but whether quality can causally
impact viewer behavior. While correlation is an important
ﬁrst step, correlation does not necessarily imply causality.
The holy grail of a content provider or a CDN architect is
to discover causal relationships rather than just correlational
ones, since they would like to know with some certainty that

the signiﬁcant eﬀort expended in improving stream quality
will in fact result in favorable viewer behavior.

In fact, a purely correlational relationship could even lead
one astray, if there is no convincing evidence of causality,
leading to poor business decisions. For instance, both video
quality (say, video bitrates) and viewer behavior (say, play
time) have been steadily improving over the past decade
and are hence correlated in a statistical sense. But, that
fact alone is not suﬃcient to conclude that higher bitrates
cause viewers to watch longer, unless one can account for
other potential “confounding” factors such as the available
video content itself becoming more captivating over time.

While inferring causality is generally diﬃcult, a key tool
widely used in the social and medical sciences to infer causal-
ity from observational data is a Quasi Experimental Design
(QED) [23].
Intuitively, a QED is constructed to infer if
a particular “treatment” (i.e., cause) results in a particular
“outcome” (i.e., eﬀect) by pairing each person in the observa-
tional data who has had treatment with a random untreated
person who is “signiﬁcantly identical” to the treated person
in all other respects. Thus, the pairing eliminates the ef-
fect of the hidden confounding variables by ensuring that
both members of a pair have suﬃciently identical values for
those variables. Thus, evaluating the diﬀerential outcomes
between treated and untreated pairs can either strengthen
or weaken a causal conclusion that the treatment causally
impacts the outcome. While it is impossible to completely
eliminate all hidden factors, our causal analysis using QEDs
should be viewed as strengthening our correlational observa-
tions between treatments and outcomes by eliminating the
common threats to a causal conclusion.
1.3 Our Contributions

Our study is one of the largest of its kind of video stream
quality and viewer behavior that collects and analyzes a data
set consisting of more than 23 million video playbacks from
6.7 million unique viewers who watched an aggregate of 216
million minutes of 102 thousand videos over 10 days.

To our knowledge, our work is the ﬁrst to provide evidence
that video stream quality causally impacts viewer behavior,
a conclusion that is important to both content providers
and CDNs. Further, our adaptation of Quasi-Experimental
Designs (QEDs) is an unique contribution and is of indepen-
dent interest. QEDs have been used extensively in medical
research and the social sciences in the past decades. We ex-
pect that our adaptation of QEDs for measurement research
in networked systems could be key in a variety of other do-
mains that have so far been limited to correlational studies.
Our work is also the ﬁrst to quantitatively explore viewer

abandonment rates and repeat viewership in relation to stream
quality, last-mile connectivity, and video duration. In addi-
tion, we also study viewer engagement (e.g., play time) in
relation to stream quality (e.g., rebuﬀering) that has also
been recently studied in [9] in a correlational setting, but
we take a step beyond correlational analysis to establish a
causal relationship between quality and engagement using
QEDs. Our work makes the following speciﬁc contributions
on the impact stream quality on viewer behavior.

- We show that an increase in the startup delay beyond
2 seconds causes viewers to abandon the video. Using
regression, we show that an additional increase of the
startup delay by 1 second increases the abandonment
rate by 5.8%.

212- Viewers are less tolerant to startup delay for a short
video such as news clip than a long video such an hour-
long TV episode. In a quasi experiment, the likelihood
of a viewer of short video abandoning earlier than a
similar viewer of a long video exceeded the likelihood
that the opposite happens by 11.5%.

- Viewers watching video on a better connected com-
puter or device have less patience for startup delay
and abandon sooner.
In particular, viewers on mo-
bile devices have the most patience and abandon the
least, while those on ﬁber-based broadband abandon
the soonest.
In a quasi experiment, the likelihood
that a viewer on ﬁber abandoned earlier than a similar
viewer on a mobile device exceeded the likelihood that
the opposite happens by 38.25%.

- Viewers who experienced an increase in the normal-
ized rebuﬀer delay, i.e., they experienced more inter-
ruptions in the video, played the video for lesser time.
In a quasi experiment, a viewer who experienced a re-
buﬀer delay that equals or exceeds 1% of the video
duration played 5.02% less of the video in comparison
to a similar viewer who experienced no rebuﬀering.

- A viewer who experienced a failed visit is less likely
to return to the content provider’s site to view more
videos within a speciﬁed time period than a similar
viewer who did not experience the failure. In a quasi
experiment, the likelihood that a viewer who expe-
rienced failure returns to the content provider’s site
within a week is less than the likelihood of a similar
viewer who did not experience failures by 2.32%.

We show that above results are statistically signiﬁcant using
the sign test. Further, these results show a signiﬁcant level of
causal impact of stream quality on viewer behavior. In this
regard, it is important to recall that small changes in viewer
behavior can lead to large changes in monetization, since
the impact of a few percentage points over tens of millions
of viewers can accrue to large impact over a period of time.
Finally, our work on deriving a causal relationship by sys-
tematically accounting for the confounding variables must
not be viewed as a deﬁnitive proof of causality, as indeed
there can be no deﬁnitive proof of causality. But, rather,
our work signiﬁcantly increases the conﬁdence in a causal
conclusion by eliminating the eﬀect of major confounding
factors that could threaten such a conclusion.

2. BACKGROUND

We describe the process of a user watching a stream, deﬁn-

ing terms along the way that we will use in this paper.

Viewer. A viewer is a user who watches one or more
streams using a speciﬁc media player installed on the user’s
device. A viewer is uniquely identiﬁed and distinguished
from other viewers by using a GUID (Globally Unique Iden-
tiﬁer) value that is set as a cookie when the media player is
accessed. To identify the viewer uniquely, the GUID value
is generated to be distinct from other prior values in use.

Views. A view represents an attempt by a viewer to watch
a speciﬁc video stream. A typical view would start with the
viewer initiating the video playback, for instance, by clicking
the play button of the media player1 (see Figure 1). During
a view, the media player begins in the startup state where
1For some content providers, a “pre-roll” advertisement is
shown before the actual content video is requested by the

it connects to the server and downloads a certain speciﬁed
amount of data, before transitioning to the play state. In the
play state, the player uses the data from its buﬀer and ren-
ders the video on the viewer’s screen. Meanwhile, the player
continues to download data from the server and stores it
in the buﬀer. Poor network conditions between the server
and the player could lead to a situation where the buﬀer is
drained faster than it is being ﬁlled. This could lead to a con-
dition where the buﬀer is empty causing the player to enter
the rebuﬀer state where the viewer experiences an interrup-
tion or “freeze” in the video play back. While in the rebuﬀer
state, the player continues to ﬁll its buﬀer from the server.
When the buﬀer has a speciﬁed amount of data, the player
enters the play state and the video starts to play again (see
Figure 1). A view can end in three ways: a successful view
ends normally when the video completes; a failed view ends
with a failure or error due to a problem with the server,
network, or content; and, ﬁnally, an abandoned view ends
with the viewer voluntarily abandoning the stream either
before the video starts up or after watching some portion
of it. Note that a viewer may abandon the view by closing
the browser, stopping the stream, or clicking on a diﬀerent
stream. There are other secondary player-initiated events or
viewer-initiated events that are part of the viewing process.
For instance, a viewer could initiate actions such as pausing,
fast-forwarding, or rewinding the video stream. Further, the
player may switch the bitrate of the encoded media in re-
sponse to network conditions, such as reduce the bandwidth
if there is packet loss. We do not explicitly analyze behav-
iors associated with these secondary events in this paper,
though these could be part of future work.

Figure 1: Views and Visits.

Visits. A visit is intended to capture a single session of
a viewer visiting a content provider’s site to view videos. A
visit is a maximal set of contiguous views from a viewer at a
speciﬁc content provider site such that each visit is separated
from the next visit by at least T minutes of inactivity, where
we choose T = 30 minutes2(see Figure 1).

Stream Quality Metrics. At the level of a view, the key
metrics that measure the quality perceived by the viewer are
shown in Figure 2. Failures address the question of whether

In that case, our view starts at the point

media player.
where the actual video is requested.
2Our deﬁnition is similar to the standard notion of a visit
(also called a session) in web analytics where each visit is
a set of page views separated by a period of idleness of at
least 30 minutes (say) from the next visit.

StartupPlayRebufferPlayStatesEventsViewer clicks "play". Buffer ﬁlled.  Play starts.Buffer empty. Play freezes. Buffer ﬁlled. Play resumes.Video ends. Play stops.ViewVisitVisitMore than T minutes of inactivityViews 213Key Metrics
Failures

Deﬁnition
Number (or, percentage) of views that
fail due to problems with the network,
server, or content.
Total time in startup state.

Startup Delay
Average Bitrate The average bitrate at which the video

Normalized Re-
buﬀer Delay

was watched.
Total time in rebuﬀer state divided by
the total duration of the video.

Figure 2: View-level stream quality metrics.

the stream was available or if viewing of the video initiated
by the viewer failed due to a problem with the network or the
server or the content itself (such as a broken link). A failed
view can be frustrating to the viewer as he/she is unable to
watch the video. A second key metric is startup delay which
is the amount of time the viewer waits for the video to start
up. Once the video starts playing, the average bitrate at
which the video was rendered on the viewer’s screen is a mea-
sure of the richness of the presented content. This metric is
somewhat complex since it is a function of how the video was
encoded, the network connectivity between the server and
the client, and the heuristics for bitrate-switching employed
by the player. Finally, a fourth type of metric quantiﬁes
the extent to which the viewer experienced rebuﬀering. Re-
buﬀering is also frustrating for the viewer because the video
stops playing and “freezes”. We can quantify the rebuﬀering
by computing the rebuﬀer delay which is total time spent
in a rebuﬀer state and normalizing it by dividing it by the
duration of the video.

Many of the above view-level metrics can be easily ex-
tended to visit-level or viewer-level metrics. One key visit-
level metric that we examine in this paper is a failed visit
which is a visit that ends with a failed view. A failed visit
could have had successful views prior to the failed view(s).
However, a failed visit is important because the viewer tries
to play a video one or more times but is unable to do so
and leaves the site right after the failure, presumably with
a level of frustration.

In our paper, we use many of the key metrics in Fig-
ure 2 in our evaluation of the impact of quality on viewer
behavior, though these are by no means the only metrics of
stream quality. It should be noted that many of the above
metrics were incorporated into measurement tools within
Akamai and have been in use for more than a decade [24,
1]. The lack of client-side measurements in the early years
led to measurements based on automated “agents” deployed
around the Internet that simulated synthetic viewers [1, 19]
that were then supplemented with server-side logs. In recent
years, there is a broad consensus among content providers,
CDNs, and analytics providers that these metrics or varia-
tions of these metrics matter.

Metrics for Viewer Behavior. Our metrics are focused on
the key aspects of viewer behavior that are often tracked
closely by content providers which we place in three cate-
gories (see Figure 3). The ﬁrst category is abandonment
where a viewer voluntarily decides to stop watching the
video. Here we are primarily concerned with abandonment
where the viewer abandons the video even before it starts
playing. A viewer can also abandon a stream after watching
a portion of the video which results in a smaller play time,

Type
Abandonment Abandonment

Metric

Engagement

Rate
Play time

Repeat View-
ers

Return Rate

Deﬁnition
% views abandoned
during startup.
Total
state (per view).
Prob. of return to site
within time period

in play

time

Figure 3: Key metrics for viewer behavior.

which we account for in the next category of metrics. The
second category is viewer engagement that can be measured
by play time which is simply the amount of video that the
viewer watches. The ﬁnal category speaks to the behavior
of viewers over longer periods of time. A key metric is the
return rate of viewers measured as the probability that a
viewer returns to the content provider’s site over period of
time, say, returning within a day or returning within a week.

3. DATA SETS

The data sets that we use for our analysis are collected
from a large cross section of actual users around the world
who play videos using media players that incorporate the
widely-deployed Akamai’s client-side media analytics plug
in3. When content providers build their media player, they
can choose to incorporate the plugin that provides an ac-
curate means for measuring a variety of stream quality and
viewer behavioral metrics. When the viewer uses the media
player to play a video, the plugin is loaded at the client-side
and it “listens” and records a variety of events that can then
be used to stitch together an accurate picture of the play-
back. For instance, player transitions between the startup,
rebuﬀering, seek, pause, and play states are recorded so
that one may compute the relevant metrics. Properties of
the playback, such as the current bitrate, bitrate switching,
state of the player’s data buﬀer are also recorded. Further,
viewer-initiated action that lead to abandonment such as
closing the browser or browser tab, clicking on a diﬀerent
link, etc can also be accurately captured. Once the metrics
are captured by the plugin, the information is “beaconed”
to an analytics backend that can process huge volumes of
data. From every media player at the beginning and end of
every view, the relevant measurements are sent to the ana-
lytics backend. Further, incremental updates are sent at a
conﬁgurable periodicity even as the video is playing.
3.1 Data Characteristics

While the Akamai platform serves a signiﬁcant amount
of the world’s enterprise streaming content accounting for
several million concurrent views during the day, we choose
a smaller but representative slice of the data from 12 con-
tent providers that include major enterprises in a variety
of verticals including news, entertainment, and movies. We
consider only on-demand videos in this study, leaving live
videos for future work. We tracked the viewers and views
for the chosen content providers for a period of 10 days (see
Figure 4). Our data set is extensive and captures 23 million
3While all our data is from media players that are instru-
mented with Akamai’s client-side plugin, the actual delivery
of the streams could have used any platform and not neces-
sarily just Akamai’s CDN.

214Total

Views
Minutes
Videos
Bytes

23 million
216 million
102 thousand

1431 TB

Avg Per
Visit
2.39
22.48
1.96
148 MB

Avg Per
Viewer
3.42
32.2
2.59
213 MB

Figure 4: Summary of views, minutes watched, dis-
tinct videos, and bytes downloaded for our data set.

Viewer Geography Percent Views

North America

Asia

Europe
Other

78.85%
12.80%
7.75%
0.60%

Figure 5: The geography of viewers in our trace at
the continent-level.

views from 6.7 million unique viewers, where each viewer
on average made 3.42 visits over the period and viewed a
total of 32.2 minutes of video. In each visit, there were on
average 2.39 views but only 1.96 unique videos viewed, indi-
cating that sometimes the viewer saw the same video twice.
The geography of the viewer was mostly concentrated in
North America, Europe and Asia with small contributions
from other continents (see Figure 5). More than half the
views used cable, though ﬁber, mobile, and DSL were signif-
icant. The ﬁber category consisted mostly of AT&T Uverse
and Verizon FiOS that contributed in roughly in equal pro-
portion. The other connection types such as dialup were
negligible (see Figure 6). Video duration is the total length
(in minutes) of the video (See Figure 7). We divide the
videos into short that have a duration of less than 30 min-
utes and long that have a duration of more than 30 minutes.
Examples of short video include news clips, highlight reels
for sports, and short television episodes. The median dura-
tion was 1.8 minutes, though the mean duration was longer
at 5.95 minutes. In contrast, long video consists of long tele-
vision episodes and movies. The median duration for long
videos was 43.2 minutes and the mean was 47.8 minutes.

4. ANALYSIS TECHNIQUES

A key goal is to establish a causal link between a stream
quality metric X and viewer behavior metric Y . The ﬁrst
key step is to establish a correlational link between X and
Y using the statistical tools for correlation and regression.
Next, in accordance with the maxim that “correlation does
not imply causation”, we do a more careful analysis to es-
tablish causation. We adapt the innovative tool of Quasi
Experimental Design (QED) used extensively in the social
and medical sciences to problem domains such as ours.
4.1 Correlational Analysis

To study the impact of a stream quality metric X (say,
startup delay) with a viewer behavioral metric Y (say aban-
donment rate), we start by visually plotting metric X versus
metric Y in the observed data. The visual representations
are a good initial step to estimating whether or not a cor-
relation exist. As a next step, we also quantify the correla-

Figure 6: Connection type as percent of views.

Figure 7: A CDF of the total video duration. The
median duration is 19.92 minutes over all videos, 1.8
minutes for short, and 43.2 minutes for long videos.

tion between X and Y . There are many diﬀerent ways to
calculate the correlation. Primary among them are Pear-
son’s correlation and Kendall’s correlation that is a type
of rank correlation. As observed in [9], Kendall’s correla-
tion is more suitable for a situation such as ours since it
does not assume any particular distributional relationship
between the two variables. Pearson’s correlation is more ap-
propriate when the correlated variables are approximately
linearly related, unlike the relationships that we explore in
our work. Kendall’s correlation measures the whether the
two variables X and Y are statistically dependent (i.e., cor-
related) without assuming any speciﬁc functional form of
their relationship. Kendall’s correlation coeﬃcient τ takes
values in the interval [−1, 1] where τ = 1 meaning that X
and Y are perfectly concordant, i.e., larger values of X are
always associated with larger values for Y , τ = −1 meaning
that X and Y are perfectly discordant, i.e., larger values of
X are always associated with smaller values of Y , and τ near
0 implying that X and Y are independent.
4.2 Causal Analysis

A correlational analysis of stream quality metric X (say,

215Confounding  

Variables

 
B

C

A

Independent 

Variable X 
(Treatment)

?

Dependent 
Variable Y 
(Outcome)

Figure 8: An example of a QED Model. The con-
founding variables are kept the same while the treat-
ment variable is varied to observe impact on out-
come.

startup delay) and a viewer behavior metric Y (say, aban-
donment rate) could show that X and Y are associated with
each other. A primary threat to a causal conclusion that an
independent variable X causes the dependent variable Y is
the existence of confounding variables that can impact both
X and Y (see Figure 8). To take a recent example from the
medical literature, a study published in Nature [20] made
the causal conclusion that children who sleep with the light
on are more likely to develop myopia later in life. But, as
it turns out, myopic parents tend to leave the light on more
often, as well as pass their genetic predisposition to myopia
to their children. Accounting for the confounding variable
of parent’s myopia, the causal results were subsequently in-
validated or substantially weakened.

More relevant to our own work, lets consider a potential
threat to a causal conclusion that a stream quality metric
X (say, startup delay) results in a viewer behavior Y (say,
abandonment). As a hypothetical example, suppose that
mobile users tend to have less patience for videos to startup
as they tend to be busy and are “on the go”, resulting in
greater abandonment. Further assume that mobile users
tend to have larger startup delays due to poor wireless con-
nectivity.
In this situation, a correlation between startup
delay and abandonment may not imply causality unless we
can account for the confounding variable of how the viewer
is connected to the Internet. In our causal analyses, we sys-
tematically identify and account for all or a subset of the
following three categories of confounding variables as rele-
vant (see Figure 8).

Content. The video4 being watched could itself inﬂuence
both quality and viewer behavior. For instance, some
videos are more captivating than others leading view-
ers to watch more of it. Or, some videos may have
higher perceived value than others, leading viewers to
tolerate more startup delay. The manner in which the
video is encoded and the player heuristic used by the
4Note that our notion of video content is url-based and thus
also incorporates the content provider. If the same movie is
available from two content providers, they would constitute
two diﬀerent pieces of content for our analysis.

media player could also impact stream quality. For
instance, the player heuristics that could diﬀer from
one content provider to another speciﬁes how much of
the video needs to be buﬀered before the stream can
startup or resume play after rebuﬀering.

Connection Type. The manner in which a viewer con-
nects to the Internet, both the device used and typical
connectivity characteristics can inﬂuence both stream
quality and viewer behavior. We use the connection
type of the viewer as a confounding variable, where
the connection type can take discrete values such as
mobile, DSL, cable, and ﬁber (such as AT&T’s Uverse
and Verizon’s FiOS).

Geography. Geography of viewer captures several social,
economic, religious, and cultural aspects that can in-
ﬂuence viewer behavior. For instance, it has been ob-
served by social scientists that the level of patience
that consumers exhibit towards a delay in receiving
a product varies based on geography of the consumer
[5]. Such a phenomena might well be of signiﬁcance
in the extent to which the viewer’s behavior is altered
by stream quality. In our work, we analyze viewer’s
geography at the granularity of a country.

4.2.1 The Quasi Experimental Design (QED) Method
A primary technique for showing that an independent
variable X (called the treatment variable) has a causal im-
pact on a dependent variable Y (called the outcome vari-
able) is to design a controlled experiment. To design a true
experiment in our context, one would have to randomly as-
sign viewers to diﬀering levels of stream quality (i.e., values
of X) and observe the resultant viewer behaviors (values of
Y ). The random assignment in such an experiment removes
any systematic bias due to the confounding variables that
are threats to a causal conclusion. However, the level of
control needed to perform such an experiment at scale for
our problem is either prohibitively hard, expensive, or even
impossible.
In fact, there are legal, ethical, and other is-
sues with intentionally degrading the stream quality of a set
of viewers to do a controlled experiment. However, there
are other domains were a controlled experiment can and are
performed, example, A|B testing of web page layouts [11].
Given the inability to perform true experiments, we adapt
a technique called QED to discover causal relationships from
observational data that already exists. QEDs were devel-
oped by social and medical scientists as a similar inability
to perform controlled experiments is very common in those
domains [23]. In particular, we use a speciﬁc type of QED
called the matched design [18] where a treated individual
(in our case, a view or viewer) is randomly matched with
an untreated individual, where both individuals have identi-
cal values for the confounding variables. Consequently, any
diﬀerence in the outcome for this pair can be attributed to
the treatment. Our population typically consists of views or
viewers and treatment variable is typically is binary. For in-
stance, in Section 7, viewers who experienced “bad” stream
quality in the form a failed visit are deemed to be treated
and viewers who had normal experience are untreated. We
form comparison sets by randomly matching each treated
viewer with an untreated viewer such that both viewers are
as identical as possible on the confounding variables. Need-
less to say, the more identical the viewers are in each pair

216the more eﬀective the matching is in neutralizing the con-
founding variables. Note that matching ensures that the
distributions of the confounding variables in the treated and
untreated set of viewers are identical, much as if viewers were
randomly assigned to treated and untreated sets in a con-
trolled experiment. Now, by studying the behavioral out-
comes of matched pairs one can deduce whether or not the
treatment variable X has a causal eﬀect on variable Y , with
the inﬂuence of the confounding variables neutralized. Note
that treatment variable need not always be stream quality.
Depending on the causal conclusion, we could choose the
treatment variable to content length or connection type, if
we would like to study their impact on viewer behavior.

Statistical Signiﬁcance of the QED Analysis.

As with any statistical analysis, it is important to evalu-
ate whether the results are statistically signiﬁcant or if they
could have occurred by random chance. As is customary in
hypothesis testing [14], we state a null hypothesis Ho that
contradicts the assertion that we want establish. That is,
Ho contradicts the assertion that X impacts Y and states
that the treatment variable X has no impact on the outcome
variable Y . We then compute the “p-value” deﬁned to be the
probability that the null hypothesis Ho is consistent with the
observed results. A “low” p-value lets us reject the null hy-
pothesis, bolstering our conclusions from the QED analysis
as being statistically signiﬁcant. However, a “high” p-value
would not allow us to reject the null hypothesis. That is, the
QED results could have happened through random chance
with a “suﬃciently” high probability that we cannot reject
Ho. In this case, we conclude that the results from the QED
analysis are not statistically signiﬁcant.

The deﬁnition of what constitutes a “low” p-value for a
result to be considered statistically signiﬁcant is somewhat
arbitrary.
It is customary in the medical sciences to con-
clude that a treatment is eﬀective if the p-value is at most
0.05. The choice of 0.05 as the signiﬁcance level is largely
cultural and can be traced back to the classical work of R.A.
Fisher about 90 years ago. Many have recently argued that
the signiﬁcance level must be much smaller. We concur and
choose the much more stringent 0.001 as our signiﬁcance
level, a level achievable in our ﬁeld given the large amount of
experimental subjects (tens of thousands treated-untreated
pairs) but is rarely achievable in medicine with human sub-
jects (usually in the order of hundreds of treated-untreated
pairs). However, our results are unambiguously signiﬁcant
and not very sensitive to the choice of signiﬁcance level. All
our results turned out to be highly signiﬁcant with p-values
of 4×10−5 or smaller, except for one conclusion with a larger
p-value that we deemed statistically insigniﬁcant.
The primary technique that we employ for evaluating sta-
tistical signiﬁcance is the sign test that is a non-parametric
test that makes no distributional assumptions and is par-
ticularly well-suited for evaluating matched pairs in a QED
setting [26]. We sketch the intuition of the technique here,
while deferring the speciﬁcs to the technical sections. For
each matched pair (u, v), where u received treatment and
v did not receive treatment, we deﬁne the diﬀerential out-
come denoted by outcome(u, v) as the numerical diﬀerence
If Ho holds,
in the outcome of u and the outcome of v.
then the outcomes of the treated and untreated individuals
are identically distributed, since the treatment is assumed
to have no impact on the outcome. Thus, the diﬀerential

outcome is equally likely to be a positive number as a neg-
ative number. Thus, for n independently selected matched
pairs, the number of positive values of the diﬀerential out-
come (call it X) follows the binomial distribution with n
trials and probability 1/2. In a measured sample consisting
of a total of n non-zero values of the diﬀerential outcome,
suppose that x have positive values. Given that Ho holds,
the probability (i.e., p-value) of such an occurrence is at
most P rob (|X − n/2|≥| x − n/2|) , which is sum of both
tails of the binomial distribution. Evaluating the above tail
probability provides us the required bound on the p-value.
As an aside, note that a diﬀerent but distribution-speciﬁc
signiﬁcance test called the paired T-test may be applicable in
other QED situations. A paired T-test uses the Student’s T
distribution and requires that the diﬀerential outcome has a
normal distribution. Since our diﬀerential outcome does not
have a normal distribution, we rely on the distribution-free
non-parametric sign test that is more generally applicable.

Some Caveats.

It is important to understand the limitations of our QED
tools, or for that matter any experimental technique of infer-
ence. Care should be taken in designing the quasi-experiment
to ensure that the major confounding variables are explicitly
or implicitly captured in the analysis. If there exists con-
founding variables that are not easily measurable (example,
the gender of the viewer) and/or are not identiﬁed and con-
trolled, these unaccounted dimensions could pose a risk to a
causal conclusion, if indeed they turn out to be signiﬁcant.
Our work on deriving a causal relationship by systematically
accounting for the confounding variables must not be viewed
as a deﬁnitive proof of causality, as indeed there can be no
deﬁnitive proof of causality. But, rather, our work increases
the conﬁdence in a causal conclusion by accounting for po-
tential major sources of confounding. This is of course a
general caveat that holds for all domains across the sciences
that attempt to infer causality from observational data.

5. VIEWER ABANDONMENT

We address the question of how long a viewer will wait
for the stream to start up, a question of great importance
that has not been studied systematically to our knowledge.
However, the analogous problem of how long a user will wait
for web content to download has received much attention.
In 2006, Jupiter Research published a study based on inter-
viewing 1,058 online shoppers and postulated what is known
in the industry as the “4-second rule” that states that an av-
erage online shopper is likely to abandon a web site if a
web page does not download in 4 seconds [21]. But, a recent
study [16] implied that the users have become impatient over
time and that even a 400 ms delay can make users search
less. Our motivation is to derive analogous rules for stream-
ing where startup delay for video is roughly analogous to
download time for web pages.

Assertion 5.1. An increase in startup delay causes more

abandonment of viewers.

To investigate if our assertion holds, we classify each view
into 1-second buckets based on their startup delay. We then
compute for each bucket the percentage of views assigned
to that bucket that were abandoned. From Figure 9, we see
that the percent of abandoned views and startup delay are
positively correlated with a Kendall correlation of 0.72.

217Figure 9: Percentage of abandoned views and
startup delay are positively correlated.

Suppose now that we build a media delivery service that
provides a startup delay of exactly x seconds for every view.
What percent of views delivered by this system will be aban-
doned? To estimate this metric, we deﬁne a function called
AbandonmentRate(x) that equals

100 × Impatient(x)/(Impatient(x) + Patient(x)),

where Impatient(x) is all views that were abandoned af-
ter experiencing less than x seconds of startup delay and
Patient(x) are views where the viewer waited at least x time
without abandoning. That is, Impatient(x) (resp., Patient(x))
corresponds to views where the viewer did not (resp., did)
demonstrate the patience to hold on for x seconds with-
out abandoning. Note that a view in Patient(x) could still
have been abandoned at some time greater than x. Also,
note that a view where the video started to play before x
seconds does not provide any information on whether the
viewer would have waited until x seconds or not, and so
is considered neither patient or impatient. Figure 10 shows
the abandonment rate computed from our data which is near
zero for the ﬁrst 2 seconds, but starts to rise rapidly as the
startup delay increases. Fitting a simple regression to initial
part of the curve shows that abandonment rate increases by
5.8% for each 1-second increase in startup delay.

Assertion 5.2. Viewers are less tolerant of startup delay

for short videos in comparison to longer videos.

Researchers who study the psychology of queuing [13] have
shown that people have more patience for waiting in longer
queues if the perceived value of the service that they are
waiting for is greater. Duration of the service often inﬂu-
ences its perceived value with longer durations often per-
ceived as having greater value. People often tolerate the 30
minute delay for the checkin process for a 4-hour plane ride
but would ﬁnd the same wait excessive for a 10-minute bus
ride. On the same principle, is it true that viewers would

Figure 10: Viewers start to abandon the video if the
startup delay exceeds about 2 seconds. Beyond that
point, a 1-second increase in delay results in roughly
a 5.8% increase in abandonment rate.

be more patient for the video to startup if they expect to be
watching the video for longer period of time?

To investigate our assertion, we ﬁrst classify the views
based on whether the content is short with duration smaller
than 30 minutes (e.g., news clip), or long with duration
longer than 30 minutes (e.g., movies). The Kendall cor-
relation between the two variables, percent of abandoned
videos and startup delay, were 0.68 and 0.90 for short and
long videos respectively, indicating a strong correlation for
each category. Further, Figure 11 shows abandonment rate
for each type of content as a function of the startup delay.
One can see that viewers typically abandon at a larger rate
for short videos than for long videos.

Assertion 5.3. Viewers watching videos on a better con-
nected computer or device have less patience for startup de-
lay and so abandon sooner.

The above assertion is plausible because there is some ev-
idence that users who expect faster service are more likely
to be disappointed when that service is slow. In fact, this
is often touted as a reason for why users are becoming less
and less able to tolerate web pages that download slowly. To
study whether or not this is true in a scientiﬁc manner, we
segment our views into four categories based on their con-
nection type that indicates how the corresponding viewer
is connected to the Internet. The categories in roughly the
increasing order of connectivity are mobile, DSL, cable mo-
dem, and ﬁber (such as Verizon FIOS or AT&T Uverse).
In all four categories, we see a strong correlation between
the two variables, percent of abandoned views and startup
delay. The Kendall correlations for mobile, DSL, cable, and
ﬁber are 0.68, 0.74, 0.71, and 0.75 respectively. Further, in
Figure 12, we show the abandonment rate for each connec-
tion type. We can see that viewers abandon signiﬁcantly
less on mobile in comparison with the other categories, for

218Figure 11: Viewers abandon at a higher rate for
short videos than for long videos.

Figure 12: Viewers who are better connected aban-
don sooner.

a given startup delay. Some diﬀerence in abandonment is
discernible between the other categories in the rough order
of cable, DSL, and ﬁber though they are much smaller.
5.1 QED for Assertion 5.2

First, we devise a QED to study the impact of content
length on abandonment (Assertion 5.2). Therefore, we make
the content length (long or short) the treatment variable and
the outcome measures patience of the viewer to startup de-
lay. The viewer’s patience to startup delay can be inﬂuenced
by both the viewer’s geography and connection type which
we use as the confounding variables. Speciﬁcally, we form
matched pairs (u, v) such that view u is a short video that
was abandoned and view v is a long video that was aban-
doned and u and v are watched by viewers from the same
geography, and the viewers have the same connection type.
The matching algorithm is described as follows.

1. Match step. Let the treated set T be all abandoned
views for short content and let untreated set C be all
the abandoned views for long content. For each u ∈ T
we pick uniformly and randomly a v ∈ C such that
u and v belong to viewers in the same geography and
have the same connection type. The matched set of
pairs M ⊆ T × C have the same attributes for the
confounding variables and diﬀer only on the treatment.

2. Score step. For each pair (u, v) ∈ M , we compute
an outcome(u, v) to be +1 if u was abandoned with a
smaller startup delay than v. If u was abandoned with
a larger startup delay than v, then outcome(u, v) =
−1. And, outcome(u, v) = 0, if the startup delays
when u and v were abandoned are equal. Now,

N et Outcome =!"(u,v)∈M outcome(u, v)

|M|

# × 100.

Note that a positive value for net outcome provides posi-
tive (supporting) evidence for Assertion 5.3, while a nega-
tive value provides negative evidence for the assertion. The
results of the matching algorithm produced a net outcome of
11.5%. The net outcome shows that the matched pairs that
support Assertion 5.2 exceed those that negate the asser-
tion by 11.5%. The positive net outcome provides evidence
of causality that was not provided by the prior correlational
analysis alone by eliminating the threats posed by the iden-
tiﬁed confounding variables.

To derive statistical signiﬁcance of the above QED re-
sult, we formulate a null hypothesis Ho that states that
the treatment (long versus short video) has no impact on
If Ho holds, the outcome(u, v) is equally
abandonment.
likely to be positive (+1) as negative (-1). We now use
the sign test that we described in Section 4.2 to derive a
bound on the p-value. Since we matched n = 78, 840 pairs,
if Ho holds, the expected number pairs with a positive out-
come is n/2 = 78, 840/2 = 39, 420. Our observational data
however had x = 43, 954 pairs with positive scores, i.e.,
x − n/2 = 4534 pairs in excess of the mean. We bound
the p-value by showing that it is extremely unlikely to have
had 4534 positive pairs in excess of the mean by computing
the the two-sided tail of the binomial distribution with n
trials and probability 1/2:

n
2 |≥| x −

n

2% ≤ 3.3 × 10−229

(1)

p-value ≤ Prob$|X −

The above bound for the p-value is much smaller than the
required signiﬁcance level of 0.001 and leads us to reject
the null hypothesis Ho. Thus, we conclude that our QED
analysis is statistically signiﬁcant.
5.2 QED for Assertion 5.3

To investigate a causal conclusion for Assertion 5.3, we
set up a QED where the treatment is the connection type of
the user and the outcome measures the relative tolerance of

219the viewer to startup delay. For each pair of network types
A and B, we run a matching algorithm where the treated
set T is the set of all abandoned views with connection type
A and untreated set is all abandoned views with connection
type B. The matching algorithm used is identical to the
one described earlier except that the match criterion in step
1 is changed to match for identical content and identical
geography. That is, for every matched pair (u, v), view u
has network type A and view v has network type B but
both are views for the same video and belong to viewers in
the same geography.

The results of the matching algorithm are shown in Fig-
ure 13. For instance, our results show that the likelihood
that a mobile viewer exhibited more patience than a ﬁber
viewer is greater than the likelihood that opposite holds by
a margin of 38.25%. Much as in Section 5.1, we use the
sign test to compute the p-value for each QED outcome in
the table. All QED outcomes in Figure 13 turned out to be
statistically signiﬁcant with exceedingly small p-values, ex-
cept the dsl-versus-cable comparison that was inconclusive.
Speciﬁcally, our results show that a that a mobile viewer ex-
hibits more patience than other (non-mobile) viewers, and
the result holds with exceedingly small p-values (< 10−17).
Our results also provide strong evidence for DSL and cable
users being more patient than ﬁber users, though the p-value
for the dsl-versus-ﬁber was somewhat larger (4.6×10−5) but
still statistically signiﬁcant. The dsl-versus-cable compari-
son was however inconclusive and not statistically signiﬁcant
as the p-value of the score was 0.06 that is larger than our
required signiﬁcance level of 0.001.

```````````

Untreated

Treated

mobile

dsl
cable

dsl

cable

ﬁber

33.81

-
-

35.40
-0.75

-

38.25
2.67
3.65

Figure 13: Net QED outcomes support the causal
impact of connection type on viewer patience for
startup delay, though the impact is more pro-
nounced between mobile and the rest. The p-value
for all entrees are very small (< 10−17), except dsl-
versus-cable (0.06) and dsl-versus-ﬁber (4.6 × 10−5).

6. VIEWER ENGAGEMENT

We study the extent to which a viewer is engaged with
the video content of the content provider. A simple metric
that measures engagement is play time. Here we study play
time on a per view basis, though one could study play time
aggregated over all views of a visit (called visit play time) or
play time aggregated over all visits of a viewer (called viewer
play time). Figure 14 shows the CDF of play time over
our entire data set. A noticeable fact is that a signiﬁcant
number of views have very small play time with the median
play time only 35.4 seconds. This is likely caused by “video
surﬁng” where a viewer quickly views a sequence of videos
to see what might of interest to him/her, before settling in
on the videos that he/she wants to watch. The fact that
a viewer watched on average of 22.48 minutes per visit (cf.
Figure 4) is consistent with this observation. Play time is
clearly impacted by both the interest level of the viewer
in the video and the stream quality. Viewer interest could

Figure 14: A signiﬁcant fraction of the views have
small duration.

itself be a function of complex factors, for instance, Italian
viewers might be more interested in soccer world cup videos
than American viewers, even more so if the video is of a
game where Italy is playing. In understanding the impact
of stream quality on viewer engagement, the challenge is to
neutralize the bias from confounding variables not related
to stream quality such as viewer interest, geography, and
connection type. Since more rebuﬀer delay is expected of
videos with a longer duration, we use normalized rebuﬀer
delay5 that equals 100 × (rebuﬀer delay/video duration).

Assertion 6.1. An increase in (normalized) rebuﬀer de-

lay can cause a decrease in play time.

To evaluate the above assertion, we ﬁrst classify views by
bucketing their normalized rebuﬀer delay into 1% buckets.
Then, we compute and plot the average play time for all
views within each bucket (see Figure 15). The decreasing
trend visualizes the negative correlation that exists between
normalized rebuﬀer delay and play time. The Kendall cor-
relation between the two metrics is −0.421, quantifying the
negative correlation.
6.1 QED Analysis

To examine the causality of Assertion 6.1, we devise a
QED where the treatment set T consists of all views that suf-
fered normalized rebuﬀer delay more than a certain thresh-
old γ%. Given a value of γ as input, the treated views are
matched with untreated views that did not experience re-
buﬀering as follows.

1. Match step. We form a set of matched pairs M as
follows. Let T be the set of all views who have a nor-
malized rebuﬀer delay of at least γ%. For each view u
in T , suppose that u reaches the normalized rebuﬀer
delay threshold γ% when viewing the tth second of the
5Note that normalized rebuﬀer delay can go beyond 100% if
we rebuﬀer for longer than the total duration of the video.

220Normalized Rebuﬀer Delay γ Net Outcome

P-Value

(percent)

(percent)

1
2
3
4
5
6
7

5.02
5.54
5.7
6.66
6.27
7.38
7.48

< 10−143
< 10−123
< 10−87
< 10−86
< 10−57
< 10−47
< 10−36

Figure 16: A viewer who experienced more rebuﬀer
delay on average watched less video than an identical
viewer who had no rebuﬀer.

increasing values of γ. Much as in Section 5.1, we use the
sign test to compute the p-values for each QED outcome.
All p-values were extremely small as shown in Figure 16,
making the results statistically signiﬁcant.

7. REPEAT VIEWERSHIP

We study the viewers who after watching videos on a con-
tent provider’s site return after some period of time to watch
more. Repeat viewers are valued highly-valued by media
content providers as these viewers are more engaged and
more loyal to the content provider’s site. Even a small de-
crease (or, increase) in the return rate of viewers can have a
large impact on the business metrics of the content provider.
Clearly, a number of factors, including how captivating the
video content is to the viewer, inﬂuence whether or not a
viewer returns. However, we show that stream quality can
also inﬂuence whether or not a viewer returns.

The most drastic form of quality degradation is failure
when a viewer is unable to play a video successfully. Fail-
ures can be caused by a number of issues, including problems
with the content (broken links, missing video ﬁles, etc), the
client software (media player bugs, etc), or the infrastruc-
ture (network failure, server overload, etc). More frustrating
than a failed view is a failed visit where a viewer tries to play
videos from the content providers site but fails and leaves
the site immediately after the failure, presumably with some
level of frustration. (Note that the deﬁnition of a failed visit
does not preclude successful views earlier in that visit before
the last view(s) that failed.) We focus on the impact of a
failed visit experienced by a viewer on his/her likelihood of
returning to the content provider’s site.

Assertion 7.1. A viewer who experienced a failed visit
is less likely to return to the content provider’s site to view
more videos within a speciﬁed time period than a similar
viewer who did not experience a failed visit.

To examine if the above assertion holds, we classify each
of our views as either failed or normal (i.e., not failed). For
each failed visit (resp., normal visit), we compute the return
time which is deﬁned to be the next time the viewer returns
to the content provider’s site. (Return time could be inﬁnite
if they do not return to the site within our trace window.)
Figure 17 shows the CDF of the return time for both failed
visits and normal visits. It can be seen that there is signiﬁ-
cant reduction in the probability of return following a failed
visit as opposed to a normal one. For instance, the proba-
bility of returning within 1 day after a failed visit is 8.0%

Figure 15: Correlation of normalized rebuﬀer delay
with play time.

video, i.e., view u receives treatment after watching
the ﬁrst t seconds of the video, though more of the
video could have been played after that point. We
pick a view v uniformly and randomly from the set of
all possible views such that

(a) the viewer of v has the same geography, connec-
tion type, and is watching the same video as the
viewer of u.

(b) View v has played at least t seconds of the video

without rebuﬀering till that point.

2. Score step. For each pair (u, v) ∈ M , we compute
play time of v − play time of u

outcome(u, v) =

video duration

.

N et Outcome =!"(u,v)∈M outcome(u, v)

|M|

# × 100.

Note that closer we can make the matched views u and v
in variables other than the treatment, the more accurate
our QED results. Though as a practical matter, adding too
many matching parameters can highly reduce the availabil-
ity of matches, eventually impacting the statistical signiﬁ-
cance of the results. It is worth noting step 1(b) above where
we ensure that v watches the video to at least the same point
as when u ﬁrst received treatment. Thus, at the time both
u and v play the tth second of the video, they have viewed
the same content and the only diﬀerence between them is
one had rebuﬀering and the other did not. The net outcome
of the matching algorithm can be viewed as the diﬀerence in
the play time of u and v expressed as a percent of the video
duration. Figure 16 shows that on average a view that ex-
perienced normalized rebuﬀer delay of 1% or more played
5.02% of less of the video. There is a general upward trend
in the net outcome when the treatment gets harsher with

221versus 11% after a normal one. Likewise, the probability
of returning within 1 week after a failed visit is 25% versus
27% after a normal one.

Figure 17: Probability of the return after a failed
visit and after a normal visit. The probability of
returning within a speciﬁed return time is distinctly
smaller after a failed visit than after a normal one.

7.1 QED Analysis

We perform a QED analysis to strengthen Assertion 7.1
by considering viewers6 with a failed visit to be the treated
set T . For each u ∈ T we ﬁnd a matching viewer v that
is similar to u in all the confounding variables. As before,
we ensure that viewers u and v are from the same geogra-
phy, have the same connection type, and are viewing content
from the same content provider. However, there is a subtle
characteristic that need to be matched. Speciﬁcally, we need
also ensure that the propensity of u to watch videos prior to
when u received treatment is equivalent to the corresponding
propensity of v. This ensures that any diﬀerential behavior
after the treatment can be attributed to the treatment itself.
To reinforce the last point, a viewer who watches more
video at a site is more likely to have had a failed view. There-
fore, the treated set T of viewers has a bias towards contain-
ing more frequent visitors to site who also watch more video.
Figure 18 shows the CDF of the aggregate play time of a
viewer across all visits. It can be seen that the treated set T
has viewers who have watched for more time in aggregate.
To neutralize this eﬀect, we match on the number of prior
visits and aggregate play time in step 1 (c) below and make
them near identical, so that we are comparing two viewers
who have exhibited similar propensity to visit the site prior
to treatment. The use of a similarity metric of this kind for
matching is common in QED analysis and is similar in spirit
to propensity score matching of [22].

6Note that in this matching we are matching viewers and
not views as we are evaluating the repeat viewership of a
viewer over time.

Figure 18: CDF of the viewer play time for all,
treated, and untreated viewers.

The matching algorithm follows.

1. Match step. We produce a matched set of pairs M
as follows. Let T be the set of all viewers who have
had a failed visit. For each u ∈ T we pick the ﬁrst
failed visit of viewer u. We then pair u with a viewer
v picked uniformly and randomly from the set of all
possible viewers such that

(a) Viewer v has the same geography, same connec-
tion type as u, and is watching the content from
the same content provider as u.

(b) Viewer v had a normal visit at about the same
time (within ±3 hours) as the ﬁrst failed visit of
viewer u. We call the failed visit of u and the
corresponding normal visit of v that occurred at
a similar time as matched visits.

(c) Viewer u and v have the same number of visits
and about the same total viewing time (±10 min-
utes) prior to their matched visits.

2. Score step. For each pair (u, v) ∈ M and each return
time δ. We assign outcome(u, v,δ ) to −1 if u returns
within the return time and v does not, +1 if v returns
within the return time and u does not, and 0 otherwise.

N et Outcome(δ) =!"(u,v)∈M outcome(u, v,δ )

#×100.

|M|

Figure 19 shows the outcome of the matching algorithm for
various values of the return time (δ). The positive values
of the outcome provide strong evidence of the causality of
Assertion 7.1 since it shows that viewers who experienced
a normal visit returned more than their identical pair with
a failed visit. To take a numerical example, for δ = 1 day,
458, 621 pairs were created. The pairs where the normal
viewer returned but its identical failed pair did not exceed

222Return Time δ Outcome P-Value

(in days)

(percent)

1
2
3
4
5
6
7

2.38
2.51
2.42
2.35
2.15
1.90
2.32

<10−57
<10−51
<10−44
<10−37
<10−22
<10−11
<10−6

Figure 19: A viewer who experienced a failed visit
is less likely to return within a time period than a
viewer who experienced a normal visit.

the pairs where the opposite happened. The amount of pairs
in excess was 10, 909 pairs, which is 2.38% of the total pairs.
Using the sign test, we show that the p-value is extremely
small (2.2 × 10−58), providing strong evidence of statistical
signiﬁcance for the outcome. Note that as δ increases, the
outcome score remained in a similar range. However, one
would expect that for very large δ values the eﬀect of the
failed event should wear oﬀ, but we did not analyze traces
that were long enough to evaluate if such a phenomenon
occurs. All p-values remain signiﬁcantly smaller than our
threshold of signiﬁcance of 0.001, allowing us to conclude
that the results are statistically signiﬁcant.

8. RELATED WORK

The quality metrics considered here have more than a
dozen years of history within industry where early measure-
ment systems used synthetic “measurement agents” deployed
around the world to measure metrics such as failures, startup
delay, rebuﬀering, and bitrate, example, Akamai’s Stream
Analyzer measurement system [1, 24]. There have been early
studies at Akamai on streaming quality metrics using these
tools [19]. However, truly large-scale studies were made pos-
sible only with the recent advent of client-side measurement
technology that could measure and report detailed quality
and behavioral data from actual viewers. To our knowledge,
the ﬁrst important large-scale study and closest in spirit
to our work is the study of viewer engagement published
last year [9] that shows several correlational relationships
between quality (such as rebuﬀering), content type (such
as live, short/long VoD), and viewer engagement (such as
play time). A recent sequel to the above work [15] studies
the use of quality metrics to enhance video delivery. A key
diﬀerentiation of our work from prior work is our focus on
establishing causal relationships, going a step beyond just
correlation. While our viewer engagement analysis was also
correlationally established in [9], our work takes the next
step in ascertaining the causal impact of rebuﬀering on play
time. Besides our results on viewer engagement, we also es-
tablish key assertions pertaining to viewer abandonment and
repeat viewership that are the ﬁrst quantitative results of its
kind. However, it must be noted that [9] studies a larger set
of quality metrics, including join time, average bitrate, and
rendering quality, and a larger class of videos including live
streaming, albeit without establishing causality.

The work on quasi-experimental design in the social and
medical sciences has a long and distinguished history stretch-
ing several decades that is well documented in [23]. Though

its application to data mining is more recent. In [18], the
authors use QEDs to answer questions about user behavior
in social media such as Stack Overﬂow and Yahoo Answers.
There are a number of other studies on perceived quality
though they tend to be small-scale studies or do not link the
quality to user behavior [10, 7]. There has also been prior
work for other types of systems. For instance, the rela-
tionship between page download times and user satisfaction
[3] for the web and quantifying user satisfaction for Skype
[6]. There has also been work on correlating QoS with QoE
(quality of experience) for multimedia systems using human
subjects [27]. These of course have a very diﬀerent focus
from our work and do not show causal impact. There has
been signiﬁcant amount of work in workload characteriza-
tion of streaming media, P2P, and web workloads [25, 4].
Even though we do characterize the workload to a degree,
our focus is quality and viewer behavior.

9. CONCLUSIONS

Our work is the ﬁrst to demonstrate a causal nexus be-
tween stream quality and viewer behavior. The results pre-
sented in our work are important because they are the ﬁrst
quantitative demonstration that key quality metrics causally
impact viewer behavioral metrics that are key to both con-
tent providers and CDN operators. As all forms of media
migrate to the Internet, both video monetization and the de-
sign of CDNs will increasingly demand a true causal under-
standing of this nexus. Establishing a causal relationship by
systematically eliminating the confounding variables is im-
mensely important, as mere correlational studies have the
potential costly risk of making incorrect conclusions.

Our work breaks new ground in understanding viewer
abandonment and repeat viewership. Further, it sheds more
light on the known correlational impact of quality on viewer
engagement by establishing its causal impact. Our work on
startup delay show that more delay causes more abandon-
ment, for instance, a 1 second increase in delay increases
the abandonment rate by 5.8%. We also showed the strong
impact of rebuﬀering on the video play time. For instance,
we showed that a viewer experiencing a rebuﬀer delay that
equals or exceeds 1% of the video duration played 5.02% less
of the video in comparison with a similar viewer who expe-
rienced no rebuﬀering. Finally, we examined the impact of
failed visits and showed that a viewer who experienced fail-
ures is less likely to return to the content provider’s site in
comparison to a similar viewer who did not experience fail-
ures. In particular, we showed that a failed visit decreased
the likelihood of a viewer returning within a week by 2.32%.
While reviewing these results, it is important to remem-
ber that small changes in viewer behavior can lead to large
changes in monetization, since the impact of a few percent-
age points over tens of millions of viewers can accrue to large
impact over a period of time.

As more and more data become available, we expect that
our QED tools will play an increasing larger role in establish-
ing key causal relationships that are key drivers of both the
content provider’s monetization framework and the CDN’s
next-generation delivery architecture. The increasing scale
of the measured data greatly enhances the statistical sig-
niﬁcance of the derived conclusions and the eﬃcacy of our
tools. Further, we expect that our work provides an impor-
tant tool for establishing causal relationships in other areas

223of measurement research in networked systems that have so
far been limited to correlational studies.

10. ACKNOWLEDGEMENTS

We thank Ethendra Bommaiah, Harish Kammanahalli,
and David Jensen for insightful discussions about the work.
Further, we thank our shepherd Meeyoung Cha and our
anonymous referees for their detailed comments that re-
sulted in signiﬁcant improvements to the paper. Any opin-
ions expressed in this work are solely those of the authors
and not necessarily those of Akamai Technologies.

11. REFERENCES
[1] Akamai. Stream Analyzer Service Description.
http://www.akamai.com/dl/feature_sheets/
Stream_Analyzer_Service_Description.pdf.

[2] K. Andreev, B.M. Maggs, A. Meyerson, and R.K.

Sitaraman. Designing overlay multicast networks for
streaming. In Proceedings of the ﬁfteenth annual ACM
symposium on Parallel algorithms and architectures,
pages 149–158. ACM, 2003.

[3] N. Bhatti, A. Bouch, and A. Kuchinsky. Integrating

user-perceived quality into web server design.
Computer Networks, 33(1):1–16, 2000.

[4] M. Cha, H. Kwak, P. Rodriguez, Y.Y. Ahn, and

S. Moon. I tube, You Tube, Everybody Tubes:
Analyzing the World’s Largest User Generated
Content Video System. In Proceedings of the 7th ACM
SIGCOMM conference on Internet measurement,
pages 1–14, 2007.

[5] H. Chen, S. Ng, and A.R. Rao. Cultural diﬀerences in
consumer impatience. Journal of Marketing Research,
pages 291–301, 2005.

[6] K.T. Chen, C.Y. Huang, P. Huang, and C.L. Lei.

Quantifying skype user satisfaction. In ACM
SIGCOMM Computer Communication Review,
volume 36, pages 399–410. ACM, 2006.

[7] M. Claypool and J. Tanner. The eﬀects of jitter on the

peceptual quality of video. In Proceedings of the
seventh ACM international conference on Multimedia
(Part 2), pages 115–118. ACM, 1999.

[8] John Dilley, Bruce M. Maggs, Jay Parikh, Harald

Prokop, Ramesh K. Sitaraman, and William E. Weihl.
Globally distributed content delivery. IEEE Internet
Computing, 6(5):50–58, 2002.

[9] Florin Dobrian, Vyas Sekar, Asad Awan, Ion Stoica,

Dilip Joseph, Aditya Ganjam, Jibin Zhan, and Hui
Zhang. Understanding the impact of video quality on
user engagement. In Proceedings of the ACM
SIGCOMM Conference on Applications, Technologies,
Architectures, and Protocols for Computer
Communication, pages 362–373, New York, NY, USA,
2011. ACM.

[10] S.R. Gulliver and G. Ghinea. Deﬁning user perception
of distributed multimedia quality. ACM Transactions
on Multimedia Computing, Communications, and
Applications (TOMCCAP), 2(4):241–257, 2006.

[11] R. Kohavi, R. Longbotham, D. Sommerﬁeld, and
R.M. Henne. Controlled experiments on the web:
survey and practical guide. Data Mining and
Knowledge Discovery, 18(1):140–181, 2009.

[12] L. Kontothanassis, R. Sitaraman, J. Wein, D. Hong,

R. Kleinberg, B. Mancuso, D. Shaw, and D. Stodolsky.
A transport layer for live streaming in a content
delivery network. Proceedings of the IEEE,
92(9):1408–1419, 2004.

[13] R.C. Larson. Perspectives on queues: Social justice

and the psychology of queueing. Operations Research,
pages 895–905, 1987.

[14] E.L. Lehmann and J.P. Romano. Testing statistical

hypotheses. Springer Verlag, 2005.

[15] X. Liu, F. Dobrian, H. Milner, J. Jiang, V. Sekar,
I. Stoica, and H. Zhang. A case for a coordinated
internet video control plane. In Proceedings of the
ACM SIGCOMM Conference on Applications,
Technologies, Architectures, and Protocols for
Computer Communication, pages 359–370, 2012.

[16] Steve Lohr. For impatient web users, an eye blink is

just too long to wait. New York Times, February 2012.

[17] E. Nygren, R.K. Sitaraman, and J. Sun. The Akamai

Network: A platform for high-performance Internet
applications. ACM SIGOPS Operating Systems
Review, 44(3):2–19, 2010.

[18] H. Oktay, B.J. Taylor, and D.D. Jensen. Causal

discovery in social media using quasi-experimental
designs. In Proceedings of the First Workshop on
Social Media Analytics, pages 1–9. ACM, 2010.

[19] Akamai White Paper. Akamai Streaming: When

Performance Matters, 2004.
http://www.akamai.com/dl/whitepapers/Akamai_
Streaming_Performance_Whitepaper.pdf.

[20] G.E. Quinn, C.H. Shin, M.G. Maguire, R.A. Stone,
et al. Myopia and ambient lighting at night. Nature,
399(6732):113–113, 1999.

[21] Jupiter Research. Retail Web Site Performance, June

2006. http://www.akamai.com/html/about/press/
releases/2006/press_110606.html.

[22] P.R. Rosenbaum and D.B. Rubin. Constructing a

control group using multivariate matched sampling
methods that incorporate the propensity score.
American Statistician, pages 33–38, 1985.

[23] W.R. Shadish, T.D. Cook, and D.T. Campbell.
Experimental and quasi-experimental designs for
generalized causal inference. Houghton, Miﬄin and
Company, 2002.

[24] R.K. Sitaraman and R.W. Barton. Method and

apparatus for measuring stream availability, quality
and performance, February 2003. US Patent 7,010,598.

[25] K. Sripanidkulchai, B. Maggs, and H. Zhang. An

analysis of live streaming workloads on the internet. In
Proceedings of the 4th ACM SIGCOMM Conference
on Internet Measurement, pages 41–54, 2004.
[26] D.A. Wolfe and M. Hollander. Nonparametric

statistical methods. Nonparametric statistical methods,
1973.

[27] W. Wu, A. Areﬁn, R. Rivas, K. Nahrstedt,

R. Sheppard, and Z. Yang. Quality of experience in
distributed interactive multimedia environments:
toward a theoretical framework. In Proceedings of the
17th ACM international conference on Multimedia,
pages 481–490, 2009.

224