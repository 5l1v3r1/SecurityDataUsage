Transparency Overlays and Applications

Melissa Chase

Microsoft Research Redmond
melissac@microsoft.com

Sarah Meiklejohn

University College London
s.meiklejohn@ucl.ac.uk

ABSTRACT
In this paper, we initiate a formal study of transparency,
which in recent years has become an increasingly critical
requirement for the systems in which people place trust.
We present the abstract concept of a transparency overlay,
which can be used in conjunction with any system to give it
provable transparency guarantees, and then apply the over-
lay to two settings: Certiﬁcate Transparency and Bitcoin.
In the latter setting, we show that the usage of our trans-
parency overlay eliminates the need to engage in mining and
allows users to store a single small value rather than the
entire blockchain. Our transparency overlay is generically
constructed from a signature scheme and a new primitive
we call a dynamic list commitment, which in practice can
be instantiated using a collision-resistant hash function.

1.

INTRODUCTION

In the past decade, the trust that society places in cen-
tralized mechanisms run by government, network operators,
and ﬁnancial institutions has been eroding, with various in-
cidents demonstrating that high integrity cannot be achieved
solely through trust in one or a handful of parties. As a re-
action to this erosion in trust, two alternative architectures
have emerged: users have either taken matters into their own
hands and ﬂocked to systems that have no central point of
trust, or they have increased pressure on central entities to
provide more openness and accountability.

A prominent example of a system with no central point of
trust is Bitcoin [28], which was deployed in January 2009.
Bitcoin is a monetary system that is not backed by any
government and is managed through a consensus mecha-
nism over a peer-to-peer network; there is thus no single
entity that issues bitcoins or validates individual transac-
tions, and users of Bitcoin operate using pseudonyms that
are not inherently tied to their real-world identity. Bitcoin
has achieved staggering success: as of this writing, its mar-
ket capitalization is over 8 billion USD and its underlying
structure has inspired hundreds of alternative cryptocurren-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’16, October 24–28, 2016, Vienna, Austria.
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978404

cies; payment gateways such as Bitpay and Coinbase allow
thousands of vendors to accept it; a number of governments
have taken steps to legitimize Bitcoin via interfaces with
traditional ﬁnancial and regulatory infrastructures; and ma-
jor ﬁnancial institutions such as JPMorgan Chase [30] and
Nasdaq [29] have announced plans to develop Bitcoin-based
technologies.

Bitcoin and its variants have achieved a large degree of
success, but denying all forms of central authority arguably
limits their ability to achieve widespread adoption. Thus,
technological solutions have emerged that instead seek to
provide more visibility into currently centralized systems.
One key example of such a system is Certiﬁcate Trans-
parency (CT) [21], which addresses shortcomings with SSL
certiﬁcate authorities (CAs) — which have ranged from fail-
ing to verify the identity of even major website owners such
as Google before issuing a cryptographic certiﬁcate [10, 19]
to suﬀering major hacks [25] that result in hundreds of forged
certiﬁcates being issued [22] — and empowers users to ver-
ify for themselves the correct functioning of a system with
which they interact many times a day (e.g., any time they
log in to a secure website, such as their email provider). Un-
like Bitcoin’s approach, CT does not substantially alter the
underlying infrastructure (i.e., the issuance of a certiﬁcate is
largely unchanged), but instead provides a way for anyone
to monitor and audit the activities of CAs to ensure that
bad certiﬁcates can be detected quickly, and misbehaving
authorities identiﬁed and excluded.

While Bitcoin and Certiﬁcate Transparency provide solu-
tions in diﬀerent settings, they in fact share some common
features; most notably, they rely on transparency as a means
to achieve integrity. In Bitcoin, the ledger of transactions —
called the blockchain — is completely transparent, meaning
all Bitcoin transactions are globally visible. A similar prop-
erty is provided in Certiﬁcate Transparency, in which a dis-
tributed set of servers each maintain a globally visible log of
all the issued certiﬁcates of which they are aware.

Furthermore, both Bitcoin and CT adopt a distributed so-
lution, which is essential to avoid placing trust in any single
entity. Indeed, relying on one party creates (at worst) a sys-
tem in which this central party has unilateral control over
the information that is released, or (at best) a system with
one central point of failure on which attackers could target
their eﬀorts. By using a solution that is both transparent
and distributed, these systems intuitively provide some no-
tion of public auditability:
individual users can check for
themselves that only “good” events have taken place (e.g.,
in the case of Bitcoin, that all bitcoins have been spent at

168most once) and detect misbehavior on the part of all actors
within the system. Understanding the link between trans-
parency and the types of misbehavior that can be detected
across a variety of settings is one of the main motivations
behind this work.

1.1 Our contributions.

Systems such as Bitcoin and CT seem to provide impor-
tant transparency beneﬁts (namely, the public auditability
mentioned above), but the similarities and diﬀerences be-
tween their beneﬁts are not well understood, and no formal
analysis has demonstrated either the level of transparency
that they provide or how this transparency provides the in-
tended beneﬁts.
In this paper, we initiate such a formal
study.
In doing so, we seek to not only compare the dif-
ferent guarantees provided by these systems (although our
analysis does accomplish this), but more importantly to cre-
ate an abstract transparency overlay that may be used to
provide these guarantees in a variety of applications beyond
ﬁnancial transactions and certiﬁcate issuance.

Before we can analyze these protocols or construct a trans-
parency overlay, we must ﬁrst consider the crucial compo-
nents that make up these systems. Our ﬁrst step is thus
to formalize — in Section 3.2 — a primitive that we call a
dynamic list commitment (DLC); a DLC can be thought
of as a generalization of a rolling hash chain or hash tree,
and serves as the foundation for our construction of a trans-
parency overlay. After deﬁning this underlying primitive,
we then go on to present transparency overlays in Section 4;
here our design is heavily inspired by the design of CT. We
begin with a formal model for transparency overlays, and
then go on to present an abstract transparency overlay and
prove its security.

Armed with this abstract secure transparency overlay, we
go on in Section 5 to demonstrate that CT is a secure trans-
parency overlay. We also demonstrate that our formal no-
tion of security implies more intuitive notions of security in
this setting (i.e., that users should accept only “good” cer-
tiﬁcates) and discuss some practical considerations.

In Section 6, we continue by turning our attention to the
Bitcoin protocol. Here, we do not use the protocol directly
(as we argue that it clearly cannot satisfy our notions of se-
curity), but rather plug crucial components of the protocol
into our abstract transparency overlay. While this allows us
to achieve a provably secure transparency overlay for Bit-
coin, it more importantly also implies that “regular” Bitcoin
users (i.e., users interested only in transacting in bitcoin,
rather than engaging in the mining process) can operate
signiﬁcantly more eﬃciently, provided they are willing to
outsource some trust to a distributed set of parties. This
result demonstrates that, in any setting in which users are
willing to trust any distributed set of parties, the full de-
centralization of Bitcoin is not needed, and the same goals
can in fact be accomplished by a CT-like structure, in which
regular users store signiﬁcantly less information about the
transaction ledger and the mining process is superﬂuous;
i.e., the quadrillion hashes per second expended on Bitcoin
mining (as of March 2016) can be eliminated without sacri-
ﬁcing security. Our formal analysis thus reveals the ﬁne line
separating fully decentralized (and expensive) solutions like
Bitcoin from distributed (and relatively cheap) solutions like
CT, and we hope that our results can help to inform future
decisions about which protocol to adopt.

1.2 Related work.

We consider research that is related both in terms of the
applications of Bitcoin and Certiﬁcate Transparency, and
in terms of the underlying primitives used to construct our
transparency overlay.

An emerging line of work has both formalized some of
the properties provided by the Bitcoin network and boot-
strapped Bitcoin to obtain provably secure guarantees in
other settings. Garay et al. [17] analyzed the so-called “back-
bone” protocol of Bitcoin and prove that it satisﬁes two im-
portant properties as long as the adversary controls some
non-majority percentage of the hashing power. Similarly,
Bentov and Kumaresan [8] provided a two-party compu-
tation built on top of (an abstracted version of) Bitcoin
that provably achieves a notion of fairness, and Andrychow-
icz et al. [3] used Bitcoin to build a provably fair system
for multi-party computation. Andrychowicz and Dziem-
bowski [2] further formalized some of the fairness proper-
ties they require from Bitcoin (and more generally from sys-
tems based on proof-of-work) and used them to construct a
broadcast protocol. Finally, on the privacy side, the Zero-
cash project [6] provides a cryptocurrency that has provable
anonymity guarantees, and Garman et al. [18] showed how
to adapt the decentralized approach of Bitcoin to achieve
anonymous credentials. To the best of our knowledge, ours
is the ﬁrst paper to focus on the transparency property of
Bitcoin, rather than its privacy or fairness guarantees.

Aside from CT, a number of other solutions exist for
changing the way we interact with certiﬁcate authorities;
many of these solutions require a ground-up redesign of the
CA ecosystem, which is why we chose to examine CT in-
stead and use it as our inspiration for an overlay system.
Fromknecht et al. [16] propose a decentralized PKI, based
on Bitcoin and Namecoin, that eliminates the trust in cen-
tralized authorities altogether. CONIKS [24] provides an
approach to logging certiﬁcates that diﬀers from CT in two
key ways: it focuses on user rather than website certiﬁcates,
and largely because of this it provides a privacy-preserving
solution, in which certain aspects of the stored certiﬁcates
(e.g., usernames) are kept hidden. The Accountable Key
Infrastructure [20] and the related ARPKI [4] both require
a distributed infrastructure for not only the storage of is-
sued certiﬁcates (as CT does), but also for their issuance,
thus focusing on the prevention rather than just detection
of misbehavior. Ryan [33] demonstrated how to extend CT
to handle revocation of certiﬁcates. In a concurrent work,
Dowling et al. [15] provided a diﬀerent security model for CT
and demonstrated that if various properties of the underly-
ing Merkle trees are satisﬁed then CT is provably secure in
their model. Although somewhat overlapping with our own
work, their paper is focused ﬁrmly on CT and not on the
abstract properties of transparency overlays and how they
can be applied across a variety of settings.

Finally, the main primitive underlying our transparency
overlay (a dynamic list commitment) is primarily a general-
ization of a Merkle tree [26], and is similar to the deﬁnition of
a tamper-evident log given by Crosby and Wallach [13]. It is
also related to the notion of an authenticated data structure
(ADS) [1, 32, 31] and the notion of a cryptographic accu-
mulator [7, 12, 11, 23]; indeed the application of ADSs to
Bitcoin has already been touched on in previous work [27]
(but from the perspective of programming languages, and
thus without any consideration of security). Dynamic list

169(a) Certiﬁcate issuance.

(b) Bitcoin.

Figure 1: The basic structure for each of the settings in which we
apply transparency.

commitments diﬀer from these related primitives in terms of
the security model, however, and as a result we can provide
more eﬃcient constructions while still satisfying a notion of
provable security.

2. BACKGROUND
2.1 Certiﬁcate Transparency

Certiﬁcate Transparency (CT) was proposed in 2011 by
Ben Laurie and Adam Langley as a way to increase trans-
parency in the process of issuing certiﬁcates, so that cer-
tiﬁcate authorities (CAs) can be held responsible for their
actions and bad certiﬁcates can be caught and revoked early
on. The basic process of issuing certiﬁcates operates as de-
picted in Figure 1a: a CA issues a certiﬁcate to a website
operator, who then publishes this certiﬁcate so that users
can check it. Certiﬁcate Transparency then provides an ex-
tra layer on top of this basic interaction to provide trans-
parency; in fact, as we will see in Section 4, our design of a
transparency overlay is heavily inspired by the CT design.

Brieﬂy, CT introduces three additional actors: a log server,
who is responsible for keeping track of issued certiﬁcates,
an auditor, who is responsible (on behalf of the client) for
keeping track of whether given certiﬁcates are in the log
or not, and a monitor, who is responsible for checking the
quality of the certiﬁcates in the log. As we use these addi-
tional actors in our general transparency overlay, we defer
further discussion of their roles and actions to Section 4.1.
In Section 5, we prove that CT provides a provably secure
transparency overlay, thus (provably) providing the intuitive
security properties that one would hope to achieve.
2.2 Bitcoin

Bitcoin is a decentralized cryptocurrency that was intro-
duced in 2008 [28] and deployed on January 3 2009. We
brieﬂy sketch the main properties of Bitcoin and its under-
lying blockchain technology here, and refer the reader to
Bonneau et al. [9] for a more comprehensive overview.

Brieﬂy, Bitcoin operates as depicted in Figure 1b. A
sender, identiﬁed using a pseudonym or address, has some
number of bitcoins stored with this address; i.e., within the
Bitcoin network, this address is acknowledged as the owner
of these bitcoins. To transfer ownership of these bitcoins to
some receiver, the sender ﬁrst creates a transaction to send
them to the receiver, as identiﬁed by whichever address she
has given to the sender. The transaction is signed to ensure
that only the sender can give away his own bitcoins.

After forming this transaction, the sender broadcasts it
to the Bitcoin network, where it eventually reaches a miner,
who acts to seal the transaction into a block. The miner
broadcasts this block, containing the transaction, to the
network, where it eventually reaches the receiver, who can

conﬁrm the transaction and its position within the Bitcoin
ledger (i.e., the blockchain) to satisfy herself that she is now
the owner of the bitcoins.

Because the Bitcoin blockchain is globally visible, it al-
ready provides a degree of transparency that is higher than
that of traditional ﬁnancial transactions. In Section 6, we
apply a transparency overlay on top of Bitcoin and demon-
strate that it provides a signiﬁcantly more eﬃcient way for
Bitcoin users to participate in transactions and allows hash-
ing to be eliminated from the system.

3. DEFINITIONS AND NOTATION

In this section, we deﬁne various notions that will be used
throughout the rest of the paper. In particular, we formal-
ize dynamic list commitments in Section 3.2, which can be
thought of as a generalization of Merkle trees and allow us
to construct high-integrity logs.
3.1 Preliminaries

If x is a binary string then |x| denotes its bit length. If S
is a ﬁnite set then |S| denotes its size and x r←− S denotes
sampling a member uniformly from S and assigning it to x.
λ ∈ N denotes the security parameter and 1λ denotes its
unary representation. ε denotes the null value.
Algorithms are randomized unless explicitly noted other-
wise. “PT” stands for “polynomial-time.” By y ← A(x1, . . . ,
xn; R) we denote running algorithm A on inputs x1, . . . , xn
and random coins R and assigning its output to y. By
y r←− A(x1, . . . , xn) we denote y ← A(x1, . . . , xn; R) for coins
R sampled uniformly at random. By [A(x1, . . . , xn)] we de-
note the set of values that have positive probability of being
output by A on inputs x1, . . . , xn. Adversaries are algo-
rithms.

For interactive protocols, we use the notation of Bellare
and Keelveedhi [5]. For completeness, we include the for-
mal notion of deﬁning and executing interactive protocols
in the full version of the paper. Brieﬂy, the behavior of
a stateful participant party that is given m during the i-th
round of the j-th execution of a protocol Prot can be deﬁned
r←− Prot[party, i, j](1λ, stateparty, m),
as (stateparty, m(cid:48), p, out)
where p indicates the party to which it is sending m(cid:48); the
execution of the entire interactive protocol can be deﬁned
by outputs r←− Run(1λ, Prot, Parties, inputs); and the message
sent during the protocol (i.e., the transcript) can be deﬁned
by M r←− Msgs(1λ, Prot, Parties, inputs).

We use games in security deﬁnitions and proofs. A game
G has a main procedure whose output is the output of the
game. Pr[G] denotes the probability that this output is true.
3.2 Dynamic list commitments

We deﬁne a dynamic list commitment (DLC), which al-
lows one to commit to a list of elements in such a way that
(1) the list represented by the commitment can be updated
only by having new elements appended to the end, and (2)
given just the list commitment, one can eﬃciently prove
both the append-only property of the list and that a given
element is in the list.

One common example of a DLC is a hash tree, and in
particular a Merkle tree, in which the root hash acts as the
commitment and one can use the hashes of intermediate
nodes to prove the above properties. (Indeed, this is what
CT uses.) Our basic formalization is similar to the deﬁnition

CAsiteusercertcertminersenderreceivertxtxblockCAsiteusercertcertminersenderreceivertxtxblock170of the following algorithms:

of a tamper-evident history system [13], but we also include
an augmented version that considers additional properties
one can use when operating on ordered lists.
3.2.1 A basic formalization for general lists.
We deﬁne a dynamic list commitment DLC as a collection
• c ← Com(list) creates the commitment c and 0/1 ←
CheckCom(c, list) checks that c is a commitment to
list;
• cnew ← Append(list∆, cold) updates the commitment
• π ← ProveAppend(cold, cnew, list) proves that cnew was
obtained from cold solely by appending elements to an
earlier version of list and 0/1 ← CheckAppend(cold,
cnew, π) checks this proof;
• π ← ProveIncl(c, elmt, list) proves that elmt is in list
(as represented by c); and 0/1 ← CheckIncl(c, elmt,
π) checks this proof.
We say that a DLC is compact if Com(list)| (cid:28) |list| for
all suﬃciently long lists list. Formal deﬁnitions of the basic
security properties of a DLC can be found in the full version
of the paper. Informally, a DLC should be

to take into account the new elements in list∆;

1. binding, which means that a commitment cannot rep-
resent two diﬀerent lists, so an adversary should be un-
able to output a commitment c and two lists list1 and
list2 such that c represents both lists (i.e., CheckCom(c,
list1) = CheckCom(c, list2) = 1) but they are not equal;

2. sound, which means that it should be hard to produce
a proof of inclusion for an element not in the list, so an
adversary should be unable to output a commitment
c, list list, element elmt, and proof π such that c rep-
resents list, CheckIncl(c, elmt, π) = 1, but elmt /∈ list;
and

3. append-only, which means that it should be hard to
produce a proof that a list has been only appended to,
so an adversary should be unable to produce a list list2,
two commitments c1 and c2, and a proof π such that
c2 represents list2, CheckAppend(c1, c2, π) = 1, but c1
is not a commitment to any preﬁx of list2.

3.2.2 An augmented formalization for ordered lists.
It will also be useful for us to consider a special type of
DLC, in which the elements in the list have some kind of or-
der imposed on them. In particular, this allows us to more
eﬃciently perform two additional operations: demonstrate
that two DLCs are inconsistent (i.e., that they are commit-
ments to strictly distinct or forking lists), and demonstrate
that a given element is not in the list represented by a given
commitment. As we will see in our applications later on,
these operations are crucial for providing evidence that cer-
tain types of misbehavior have taken place.

In addition to the algorithms required for a basic DLC,
we now require a notion of timing (which may not be the
actual time, but rather any representation that allows us to
for every element elmt in a list, we
impose an ordering):
assume there exists a function time(·) that returns a value
t, and that a global ordering exists for this function, so that
we can also deﬁne a Boolean function 0/1 ← isOrdered(list).
Using this, we deﬁne a notion of consistency for DLCs as
follows:

Definition 3.1. A tuple (c, t, list) is consistent if c is a
commitment to the state of list at time t. Formally, we con-
sider a function isConsistent such that isConsistent(c, t, list) =
1 if and only if there exists a j, 1 ≤ j ≤ len(list), such that
(1) CheckCom(c, list[1 : j]) = 1, (2) time(list[j]) ≤ t, (3)
j = len(list) or time(list[j + 1]) ≥ t), and (4) isOrdered(list).

inconsistent with c(cid:48) at time t(cid:48) and

We can now deﬁne four additional algorithms as follows:
• π ← DemoInconsistent(list, c(cid:48), t(cid:48)) proves that list is
• 0/1 ← CheckInconsistent(c(cid:48), t(cid:48), c, π) checks this proof;
• π ← DemoNotIncl(list, elmt) proves that elmt is not
• 0/1 ← CheckNotIncl(c, elmt, π) checks this proof.
Formal deﬁnitions of the augmented security properties
can be found in the full version of the paper. Informally, in
the augmented setting a DLC should satisfy

in the ordered list list; and

1. provable inconsistency, which means any inconsistent
tuple should be demonstrably inconsistent, so an ad-
versary should be unable to produce a tuple (c, t, list)
such that the tuple is inconsistent but an honestly gen-
erated proof of inconsistency fails veriﬁcation;

2. provable non-inclusion, which means it should be pos-
sible to demonstrate that an element is not in a list, so
an adversary should be unable to produce a list list and
an element elmt such that elmt /∈ list but the honestly
generated proof of non-inclusion fails veriﬁcation;

3. unforgeable inconsistency, which means it should be
impossible to demonstrate an inconsistency that does
not exist, so an adversary should be unable to produce
(c1, t, c2, list2, π) such that c2 represents list2, c1 is con-
sistent with list2 at time t, and CheckInconsistent(c1, t,
c2, π) = 1; and

4. unforgeable non-inclusion, which means it should be
impossible to prove non-inclusion of an element that is
in a list, so an adversary should be unable to produce
(c, list, elmt, π) such that c represents list, elmt ∈ list,
and CheckNotIncl(c, elmt, π) = 1.

3.2.3 Two instantiations of augmented DLCs.
To demonstrate that dynamic list commitments exist, we
provide two instantiations; both can be found in the full ver-
sion of the paper and derive their security from the collision
resistance of a hash function. Brieﬂy, our ﬁrst instantiation
is essentially a rolling hash chain: new elements appended to
the list are folded into the hash (i.e., cnew ← H(cold(cid:107)elmtnew)),
and proofs about (in)consistency and (non-)inclusion reveal
selective parts of the list. This ﬁrst instantiation thus demon-
strates the feasibility of dynamic list commitments (and is
conceptually quite simple), but the proofs are linear in the
size of the list, which is not particularly eﬃcient. Thus, our
second instantiation is essentially a Merkle tree, which al-
lows us to achieve proofs that are logarithmic in the size of
the list.

4. TRANSPARENCY OVERLAYS

In this section, we present our main contributions. First,
in Sections 4.1 and 4.2, we introduce both basic and aug-
mented formal models for reasoning about transparency.

171Then, in Sections 4.3 and 4.4 we present a generic trans-
parency overlay and prove its security. To instantiate this
securely (as we do in Sections 5 and 6), one then need only
provide a simple interface between the underlying system
and the overlay.
4.1 Basic overlays

In order for a system to be made transparent, we must
provide an eﬃcient mechanism for checking that the system
is running correctly. Our setting overlays three additional
parties on top of an existing system Sys: a log server LS,
an auditor Auditor, and a monitor Monitor. The role of the
log server is to take certain events in the system’s operation
and enter them into a publicly available log. The role of the
auditor is to check — crucially, without having to keep the
entire contents of the log — that speciﬁc events are in the log.
Finally, the role of the monitor is to ﬂag any problematic
entries within the log. Collectively then, the auditor and
monitor act to hold actors within the system responsible for
the creation of (potentially conﬂicting) events.

We assume that each of these parties is stateful: the log
server maintains the log as state, so stateLS = log; the audi-
tor maintains a snapshot (i.e., some succinct representation
of the current log) as state, so stateAu = snap; and the mon-
itor maintains a snapshot, a list of bad events, and a list of
all events, so stateMo = (snap, eventsbad, events).

A transparency overlay then requires ﬁve interactive pro-

tocols; these are deﬁned abstractly as follows:1

GenEventSet is an interaction between the actor(s) in the
system that produces the events to be logged. The protocol
is such that eventset r←− Run(1λ, GenEventSet, Sys, aux).
Log is an interaction between one or more of the actors in
the system and LS that is used to enter events into the
r←− Run(1λ, Log,
log. The protocol is such that (b, ε)
(Sys, LS), (eventset, ε)), where b indicates whether or not
the system actor(s) believes the log server behaved hon-
estly.

CheckEntry is an interaction between one or more of the
actors in the system, Auditor, and LS that is used to check
whether or not an event is in the log. The protocol is such
r←− Run(1λ, CheckEntry, (Sys, Auditor, LS),
that (b, b(cid:48), ε)
(event, ε, ε)), where b indicates whether or not the system
actor(s) believes the event to be in the log and b(cid:48) indicates
whether or not the auditor believes the log server behaved
honestly in the interaction.

Inspect is an interaction between Monitor and LS that is
used to allow the monitor to inspect the contents of the log
and ﬂag any suspicious entries. The protocol is such that
r←− Run(1λ, Inspect, (LS, Monitor), (ε, ε)), where b
(b, ε)
indicates whether or not the monitor believes the log server
behaved honestly in the interaction.

Gossip is an interaction between Auditor and Monitor that
is used to compare versions of the log and detect any
inconsistencies.
If any misbehavior on behalf of the log
server is found, then both parties are able to output evi-
dence that this has taken place. The protocol is such that

1In each protocol, we also allow the participants to output
fail, which indicates that they believe they were given im-
properly formatted inputs.

(evidence, evidence)
(ε, ε)).

r←− Run(1λ, Gossip, (Auditor, Monitor),

We also require the following (non-interactive) algorithms:
r←− GenLogID(1λ) is used to generate a public

(pkLS, sk LS)

and secret identiﬁer for the log server; and
0/1 ← CheckEvidence(pkLS, evidence) is used to check if
the evidence against the log server identiﬁed by pkLS is
valid.

From a functionality standpoint, we would like the proto-
cols to be correct, meaning all parties should be satisﬁed by
honest interactions, and compactly auditable, meaning the
size of a snapshot is much smaller than the size of the log.
We deﬁne security for a basic transparency overlay in
terms of two properties: consistency, which says that a
potentially dishonest log server cannot get away with pre-
senting inconsistent versions of the log to the auditor and
monitor, and non-frameability, which says that potentially
dishonest auditors and monitors (and even actors in the orig-
inal system) cannot blame the log server for misbehavior if
it has behaved honestly. Participants can thus be satisﬁed
that they are seeing the same view of the log as all other
participants, and that the interactions they have really are
with the log server.

To formalize consistency, we consider a game in which the
adversary takes on the role of the log server and is allowed to
interact (via the MsgAu and MsgMo oracles, respectively)
with the auditor and monitor. The adversary wins if there is
an event that is not in the list maintained by the monitor but
that the auditor nevertheless perceives as being in the log
(the third winning condition of Deﬁnition 4.1), yet the audi-
tor and monitor are unable to produce valid evidence of this
inconsistency (the ﬁrst two winning conditions). For ease
of formal exposition, we (1) assume that in the CheckEntry
protocol the ﬁrst message sent to the auditor is the event to
be checked and the last message sent by the auditor is a bit
indicating whether the event is in the log, and (2) require
that the monitor must have a newer snapshot than the au-
ditor, but can naturally extend our deﬁnition to cover other
conﬁgurations as well.

Definition 4.1. Deﬁne Advcons
where GconsA (λ) is deﬁned as follows:

trans,A(λ) = Pr[GconsA (λ)],

main GconsA (λ)
events ← ∅; eventspass ← ∅
r←− AMsgAu,MsgMo(1λ)
pkLS
evidence r←− Run(1λ, Gossip, (Auditor, Monitor), (ε, ε))
return ((CheckEvidence(pkLS, evidence) = 0) ∧
(time(stateMo[snap]) ≥ time(stateAu[snap])) ∧
(eventspass \ stateMo[events] (cid:54)= ∅))

MsgAu(i, j, m)
(stateAu, m(cid:48), p, out) r←− CheckEntry[Auditor, i, j](1λ, stateAu, m)
if (i = 1) events[j] ← m
if (out (cid:54)= ⊥) ∧ (m(cid:48) = 1) eventspass ← eventspass ∪ {events[j]}
return m(cid:48)

MsgMo(i, j, m)
(stateMo, m(cid:48), p, out) r←− Inspect[Monitor, i, j](1λ, stateMo, m)
return m(cid:48)

172Then the transparency overlay satisﬁes consistency if for all
PT adversaries A there exists a negligible function ν(·) such
that Advcons

trans,A(λ) < ν(λ).

Next, to formalize non-frameability, we consider an ad-
versary that wants to frame an honest log server; i.e., to
produce evidence of its “misbehavior.” In this case, we con-
sider a game in which the adversary takes on the role of the
auditor, monitor, and any actors in the system, and is al-
lowed to interact (via the Msg oracle) with the honest log
server. The adversary wins if it is able to produce evidence
that passes veriﬁcation.

Definition 4.2. Deﬁne Advframe
(λ) is deﬁned as follows:

where GframeA

trans,A(λ) = Pr[GframeA

(λ)],

(λ)

main GframeA
(pkLS, sk LS) r←− GenLogID(1λ)
evidence r←− AMsg(1λ, pkLS)
return CheckEvidence(pkLS, evidence)

Msg(Prot, i, j, m)
if (Prot /∈ {Log, CheckEntry, Inspect}) return ⊥
(stateLS, m(cid:48), p, out) r←− Prot[LS, i, j](1λ, stateLS, m)
return m(cid:48)

Then the transparency overlay satisﬁes non-frameability if
for all PT adversaries A there exists a negligible function
ν(·) such that Advframe

trans,A(λ) < ν(λ).

We then say that a basic transparency overlay is secure if

it satisﬁes consistency and non-frameability.

Comparison with concurrent work.

With respect to the security model of Dowling et al. [15],
their model requires only that the monitor and auditor pro-
duce evidence of misbehavior in the case where the log fails
to include an event for which it has issued a receipt (which
we consider in the next section). Our model, on the other
hand, also produces evidence in the case where the log has
given inconsistent views to the two parties; this type of ev-
idence seems particularly valuable since this type of misbe-
havior is only detected after the fact. This diﬀerence allows
them to present a simpler deﬁnition of non-frameability, as
they do not have to worry about malicious monitors and
auditors forging this type of evidence.
4.2 Pledged overlays

In the basic setting described, log servers can be held re-
sponsible if they attempt to present diﬀerent views of the
log to the auditor and monitor. If log servers simply fail to
include events in the log in the ﬁrst place, however, then
there is currently no way to capture this type of misbehav-
ior. While in certain settings the log server could plausibly
claim that it never received an event rather than ignoring it,
if the log server issues promises or receipts to include events
in the log then we can in fact enforce inclusion, or at least
blame the log server if it fails to do so.

Formally, we capture this as an additional security prop-
erty, accountability, which says that evidence can also be
used to implicate log servers that promised to include events
but then did not.
In the game (which we defer to the full
version of the paper — included as supplemental material —

due to the formal notational overhead), the adversary then
takes on the role of the log server and is allowed to inter-
act arbitrarily with the actor(s) in the system, auditor, and
monitor.
It wins if there is an event that it has pledged
to include but that the auditor and monitor do not believe
to be in the log, yet the auditor and monitor are unable to
produce evidence of this omission.

We then say that a pledged transparency overlay is secure
if it satisﬁes consistency, non-frameability, and accountabil-
ity.
4.3 A generic pledged transparency overlay

We now present a generic version of a pledged trans-
parency overlay. We begin by introducing algorithms for
performing various operations in the overlay, and then de-
scribe the interactive protocols from Section 4.1 in terms of
these algorithms and the algorithms for a dynamic list com-
mitment (DLC) and a signature scheme (KeyGen, Sign, Verify).
For ease of exposition we assume that various objects (snap-
shots, receipts, etc.) contain only the ﬁelds necessary to
make the protocol work, but could naturally extend our al-
gorithms to cover more general conﬁgurations as well.

To start, an event set eventset contain (at least) a list of
events events; a snapshot snap = (c, t, σ) contains a DLC,
timing information, and an unforgeable signature; a receipt
rcpt = (pk , t, σ) contains a public key, timing information,
and an unforgeable signature; and a log log = (snap, events)
contains a snapshot and a list of events. We denote these
subcomponents using bracket notation; e.g., we use snap[c],
or — where subscripts make it appropriately clear — use ci
to denote snapi[c].

To perform basic operations on these objects, we also in-
troduce algorithms for forming and verifying snapshots and
receipts, and for updating the log. These are deﬁned — with
respect to a notion of timing t and a keypair (pk LS, sk LS) —
as follows:

FormSnap(c, t)
return (c, t, Sign(sk LS, (c, t)))

CheckSnap(snap)
return Verify(pk LS, (snap[c], snap[t]), snap[σ])

FormRcpt(log, event)
return (pk LS, t, Sign(sk LS, (t, event)))

CheckRcpt(event, rcpt)
return Verify(pk LS, (rcpt[t], event), rcpt[σ])

UpdateLog(log, events)
events(cid:48) ← log[events](cid:107)events
c(cid:48) ← Append(events, log[snap][c])
snap(cid:48) ← FormSnap(c(cid:48), t)
return (snap(cid:48), events(cid:48))

Brieﬂy, in the Log protocol (Figure 2), an event set is
given as input to the actor(s) in the system; this is created
by GenEventSet, which we describe for our individual ap-
plications in Sections 5 and 6 but leave here as an abstract
interaction. This event set is sent to the log server, who
ﬁrst checks if it is well formed. The log server then provides
a receipt for every event in the set, and sends the receipts
back to the system actor(s). If any of the receipts are in-
valid, the system rejects the interaction, and otherwise it

173Sys(eventset)

eventset

LS(logLS)

1

2

3

4

5

6

if ∃(event, rcpt) s.t. (CheckRcpt(event, rcpt) = 0) return 0

−→
rcpt

if (isOrdered(eventset[events]) = 0) return fail

if (time(eventset[events][1]) < time(log[events][max])) return fail

rcpt r←− FormRcpt(event) ∀event ∈ eventset[events]

logLS

r←− UpdateLog(logLS, eventset[events])

return 1

return ε

Figure 2: The Log protocol for pledged transparency overlays.

accepts. Either way, the log server updates the log; in our
protocol speciﬁcation here, the log server updates the log
immediately, but we discuss in Section 5.3 how this process
can be batched and the promises of the log server altered
accordingly.

Next, in the CheckEntry protocol (Figure 3), some actor
in the system sends an event and a receipt to the auditor,
who ﬁrst checks that the receipt is valid. If it is, then the
auditor checks if the event already falls within its current
purview; i.e., if it falls within the log that the auditor al-
ready knows about (according to its snapshot). If it does,
then the auditor skips to asking the log server for a proof of
inclusion of this event; if not, the auditor must update its
snapshot and check that the new snapshot is consistent with
the old one. Once the auditor has the proof of inclusion (ei-
ther after updating or not), it returns to the client whether
or not the proof veriﬁes; the client returns whatever it re-
ceives from the auditor, and the auditor returns b = 0 if the
protocol has failed in some way (i.e., the updated snapshot
was inconsistent with the old one) and b = 1 otherwise.

Next, in the Inspect protocol (Figure 4), the monitor
sends its current snapshot to the log server, and the log
server responds with all events that have been logged since
then, along with an updated snapshot.
If this list of ap-
pended events is valid (i.e., ordered and consistent with the
new snapshot), the monitor can update its records and look
for any bad events in this new list. It returns b = 1 if the
protocol has gone smoothly; i.e., if the new list and snapshot
seem to have been formed appropriately.

Finally, in the Gossip protocol (Figure 5), the auditor
and monitor begin by exchanging snapshots, and by en-
suring that each snapshot is valid. The monitor then at-
tempts to demonstrate any inconsistencies between the two
snapshots (i.e., demonstrate that they represent forking or
distinct logs) and — if any inconsistencies do exist — this is
returned as evidence of the log server’s misbehavior.

To augment the protocol for pledged overlays, we include
in Figure 5 a further optional interaction in which the audi-
tor sends to the monitor all events for which the CheckEntry
protocol failed, to see if they are being monitored; these are
stored in a list eventsbad that is now part of the auditor’s
state and updated in the CheckEntry protocol (line 15 of
Figure 3). This allows the auditor and monitor to detect
and provide evidence for the additional type of misbehavior
in which the log server simply drops events from the log.
This means that the auditor and monitor can provide two

types of evidence: evidence that the log server presented
them with forked or distinct views of the log, or evidence
that the log server reneged on the promise it gave in a re-
ceipt. We thus instantiate the algorithm CheckEvidence as
follows:

CheckEvidence(pk LS, evidence)
if (evidence = ⊥) return 0
(snap1, snap2, (event, rcpt), π) ← evidence
if (CheckSnap(snap1) = 0) return 0
if (CheckSnap(snap2) = 0) return 0
if ((event, rcpt) = (⊥,⊥)) return
return (CheckRcpt(event, rcpt) ∧ (rcpt[t] ≤ t2)∧

(CheckInconsistent(c1, t1, c2, π) ∧ (t1 ≤ t2))

CheckNotIncl(c2, event, π)

Finally, our gossip protocol assumes the monitor has a
more up-to-date snapshot than the auditor, which protects
against an adversarial log server trivially winning the con-
sistency game (Deﬁnition 4.1) by ignoring the monitor. One
could also imagine a protocol in which the monitor pauses,
updates (using the Inspect protocol), and then resumes its
interaction with the auditor, in which case the extra winning
condition in Deﬁnition 4.1 could be dropped.

Theorem 4.3. If the DLC is secure in the augmented
setting and the signature scheme is unforgeable (i.e., EUF-
CMA secure), then the protocols presented in Figures 2-5
and the algorithms presented above comprise a secure pledged
transparency overlay, as deﬁned in Section 4.2.

A proof of this theorem can be found in the full version
of the paper. Brieﬂy, consistency follows from three prop-
erties of the dynamic list commitment: provable inconsis-
tency, append-only, and soundness. Together, these ensure
that if the log server presents an inconsistent view of the log
to the auditor and monitor, then the commitment seen by
the auditor in its snapshot — which, crucially, was updated
using ProveAppend and used to demonstrate the inclusion
of events — is inconsistent with the list seen by the moni-
tor. By provable inconsistency, the monitor can thus pro-
vide a proof of inconsistency that comprises valid evidence
of the log server’s misbehavior. Non-frameability, on the
other hand, follows from the unforgeability of the signature
scheme and from the diﬃculty of forging either a proof of
inconsistency or a proof of non-inclusion. Finally, account-
ability follows from the provable non-inclusion of the DLC,

174/
/
o
o
Sys(event)

Auditor(snapAu, eventsbad)

LS(logLS)

1

2

3

4

(if update)

event,rcpt

b

b ← CheckRcpt(event, rcpt)

if (b = 0) return fail

update ← (rcpt[t] > tAu)

b

b ← CheckSnap(snapLS) ∧ CheckAppend(cAu, cLS, π)

snapAu

snapLS,π

π ← ProveAppend(cAu, cLS, eventsLS)

5

6

7

8

9

10

11

12

13

14

15

16

if (b = 0) return 0
snapAu ← snapLS

b ← (rcpt[t] ≤ tAu)
if (b = 0) return fail

b

b

π(cid:48)
b ← CheckIncl(cAu, event, π(cid:48))

event,snapAu

π(cid:48) ← ProveIncl(cAu, event, eventsLS)

return ε

return b

if (b = 0) eventsbad ← eventsbad(cid:107)(event, rcpt)

return 1

Figure 3: The CheckEntry protocol for pledged transparency overlays. The parts of the protocol that may not be carried out (depending
on the ‘if’ clause) are marked with dashed lines.

as if an event is missing from the log then the auditor and
monitor should be able to provide valid evidence of this (in
the form of a receipt promising to include a given event and
a proof of non-inclusion of that event).
4.4 A generic basic transparency overlay

A basic transparency overlay is essentially a simpler ver-
sion of a pledged transparency overlay, so we do not give a
full description of the protocols here, but instead describe
the necessary modiﬁcations that must be made.

The most obvious modiﬁcation is that all of the parts that
involve receipts do not exist in the basic variant. Thus, the
Log protocol for a basic transparency overlay omits lines 4-5
from Figure 2 but otherwise remains the same. Next, in the
CheckEntry protocol, the auditor now cannot use the receipt
to check if it needs to update, so it must use time(event) in-
stead. The Inspect protocol contains no mention or usage
of receipts, and thus is exactly the same in the basic variant.
This leaves the Gossip protocol, in which the only signiﬁcant
modiﬁcation is that basic transparency overlays cannot pro-
vide the second type of evidence (in which the auditor and
monitor use the receipt to prove that the log server promised
to include an event but then did not), so do not attempt to
produce it (lines 9-12 of Figure 5). This also means that
evidence consists only of the two snapshots and a proof.

As the basic transparency overlay thus involves only minor
modiﬁcations to the pledged transparency overlay, we do
not prove its security from scratch, but instead prove the
following theorem as a special case of Theorem 4.3.

Theorem 4.4. If the DLC is secure in the augmented
setting and the signature scheme is unforgeable (i.e., EUF-
CMA secure), then the modiﬁed protocols and algorithms de-
scribed above comprise a secure basic transparency overlay,
as deﬁned in Section 4.1.

5. CERTIFICATE TRANSPARENCY

In this section, we describe how CT instantiates a pledged
transparency overlay (as deﬁned formally in Section 4.2),
discuss how the formal notions of overlay security imply
more intuitive notions of security speciﬁc to the setting of
issuing certiﬁcates, and ﬁnally discuss the requirements of a
practical deployment of CT.
5.1 CT is a secure pledged overlay

As depicted in Section 2, Certiﬁcate Transparency has
three actors in the system Sys: a certiﬁcate authority CA, a
website owner Site, and a client Client. One of the ﬁrst two
actors must participate in the Log protocol,2 to ensure that
the certiﬁcate issued by CA to Site ends up in the log, and
the client participates in the CheckEntry protocol to check
that the certiﬁcate presented to it by a website is in the log.
In the parlance of CT, an event is a (basic) certiﬁcate
cert = (pk name, σCA), where σCA is the CA’s signature on

2This means that either the website obtains the signed cer-
tiﬁcate from the CA and then goes on to enter it into the
log, or the CA signs the certiﬁcate and enters it into the log
before returning the extended certiﬁcate to the website.

175/
/
o
o
/
/
o
o
o
o
o
o
/
/
o
o
o
o
1

2

3

4

5

6

7

8

9

10

Monitor(snapMo, eventsbad, eventsMo)

LS(log)

snapMo

snap,events∆

j ← min{i | CheckCom(cMo, events[1 : i])}

events∆ ← events[j + 1 : ]

if (CheckSnap(snap) = 0) ∨ (isOrdered(events∆) = 0) return 0

return ε

if (time(events∆[1]) < tMo) return 0

c(cid:48) ← Append(events∆, cMo)
if (c(cid:48) (cid:54)= snap[c]) return 0

snapMo ← snap; eventsMo ← eventsMo(cid:107)events∆
update eventsbad using out-of-band checks

return 1

Figure 4: The Inspect protocol.

the site’s public key pk name,3 a receipt is a signed certiﬁ-
cate timestamp (SCT), and a snapshot is a signed tree head
(STH). For the notion of timing needed for snapshots and
receipts, one could pick the current local time of the log
server and either use this value directly as t or incorporate
into it some buﬀer period, which is referred to in the CT
documentation as the maximum merge delay (MMD). We
discuss this further in Section 5.3. Finally, CT instantiates
GenEventSet as follows:

Site(pk name)

pk name

CA

σ r←− Sign(sk CA, pk name)

cert ← (pk name, σ)

return {cert}

cert

return {cert}

The rest of the protocols needed for the transparency over-
lay can be instantiated exactly as in Section 4.3, so Theo-
rem 4.3 carries over directly and we can see that CT provides
a secure pledged transparency overlay.
5.2 Further security implications

We have just demonstrated that CT provides a secure
transparency overlay, but it is not clear what this means for
the speciﬁc setting of certiﬁcate issuance. To explore this,
we ﬁrst remind ourselves of the security of the underlying
system (i.e., the issuance of basic certiﬁcates), in which (1) it
should be diﬃcult to produce a basic certiﬁcate without con-
tacting the CA, and (2) an honest client should accept only
(basic) certiﬁcates that verify. These are clearly satisﬁed as-
suming the correctness and unforgeability of the signature
scheme.

Combining the underlying issuance security with the se-
curity of the overlay, we can argue that three more intuitive
3For simplicity, we include here only the most basic version
of the information that needs to be checked for and included
in a certiﬁcate.

security goals are largely satisﬁed. First, an extended cer-
tiﬁcate (i.e., a certiﬁcate augmented with an SCT)
should not pass veriﬁcation if it has not been jointly
produced by the CA and log server. This holds because
the underlying issuance security implies that it is diﬃcult to
produce cert without the CA, and non-frameability implies
that it is diﬃcult to produce rcpt without the log server, so
it should be diﬃcult to produce (cert, rcpt) without both the
CA and the log server.

Second, honest clients shouldn’t accept “bad” cer-
tiﬁcates; i.e., certiﬁcates that are either improperly
formatted or not being monitored. The underlying is-
suance security says that if cert does not verify then the
client won’t accept. Following this, the honest client ac-
cepts only if the auditor indicates that the certiﬁcate is in
the log. By consistency, the auditor’s view of the log is
consistent with the monitor’s view from the last time they
engaged in the Gossip protocol (unless evidence has been
produced to the contrary, at which point we can assume the
auditor ceases communication with the log server). If the
certiﬁcate is older than this, then the certiﬁcate is deﬁnitely
being monitored; if it is newer, then it is not guaranteed
that the certiﬁcate is being monitored, but if it is not then
the auditor can at least detect this during its next iteration
of the Gossip protocol. Thus, honest clients never accept
improperly formatted certiﬁcates, and are unlikely to ac-
cept unmonitored certiﬁcates provided that the auditor and
monitor are engaging in the Gossip protocol with suﬃcient
frequency.

Finally, if a log server is misbehaving by omitting
certiﬁcates from the log that it promised to include,
it should be possible to blame it. If a log server refuses
to answer queries, then there is little we can do about this
in the context of our overlay (although in a practical setting
with more than one log server this problem could be miti-
gated). If a log server does answer, then it can be formally
blamed by accountability, as the SCT acts as non-repudiable
evidence that the log server has promised to include a cer-
tiﬁcate and the corresponding proof of non-inclusion demon-
strates that it has not done so.

176/
/
o
o
/
/
o
o
bMo ← CheckSnap(snapMo)
if (bMo = 0) return fail

if (tAu > tMo) return fail

π

b ← CheckInconsistent(cAu, tAu, cMo, π)
if (b = 1) return (snapAu, snapMo,⊥, π)

(repeat for all (event, rcpt) ∈ events(Au)
bad )

event,rcpt

π

b ← CheckNotIncl(cMo, event, π)

1

2

3

4

5

6

7

8

9

10

11

12

13

Auditor(snapAu, events(Au)
bad )

Monitor(snapMo, events(Mo)

bad , events)

snapAu

snapMo

bAu ← CheckSnap(snapAu)

if (bAu = 0) return fail

if (tAu > tMo) return fail

π ← DemoInconsistent(events, tAu)
b ← CheckInconsistent(cAu, tAu, cMo, π)
if (b = 1) return (snapAu, snapMo,⊥, π)

π ← DemoNotIncl(cMo, events, event)

b ← CheckNotIncl(cMo, event, π)

if (b = 1) return (snapAu, snapMo, (event, rcpt), π)

if (b = 1) return (snapAu, snapMo, (event, rcpt), π)

return fail

return fail

Figure 5: The Gossip protocol for pledged transparency overlays. The optional part of the protocol is marked with dashed lines.

5.3 Practical considerations

Finally, we discuss some necessary alterations that would

be made to our protocol if used in a real deployment.

Batched additions to the log.

In Figure 2, the log
server currently updates the log during the Log protocol,
and as a result includes the exact current time in the SCT.
To avoid doing this operation every time, this process would
be batched, so the time in the SCT would instead be some
time in the near future (e.g., the end of the current day).
This gap between the current and promised times is referred
to in the CT documentation as the maximum merge delay
(MMD).

Collapsing the overlay into the system. As discussed
in the CT documentation, in a real deployment we expect
auditors to interact with many diﬀerent log servers (as the
certiﬁcates seen by clients may be logged in many diﬀer-
ent places), but expect monitors to focus on one log and
the certiﬁcates it contains. There are therefore two possible
models: in one, the auditors and monitors are operated as
separate services, and monitors can even be used as backup
log servers. In the other, the role of the auditor could col-
lapse into the client (e.g., it could be run as a browser ex-
tension and responses could be cached), and the role of the
monitor could collapse (at least partially) into the website,
who could monitor the log to at least keep track of its own
certiﬁcates.

Privacy concerns. While SSL certiﬁcates are public and
thus storing them in a public log presents no privacy con-
cern, information might be revealed about individual users
through the certiﬁcates queried by the auditor (to both the
log server and monitor), as well as the choice of signed tree
heads and SCTs. We view this as an interesting area for

future research, but mention brieﬂy that some of these con-
cerns can be mitigated — with minimal eﬀect on the security
of the transparency overlay — by omitting the optional part
of Figure 5, in which the auditor reveals to the monitor some
of the certiﬁcates that it has seen.

6. AMPLIFYING BITCOIN’S SECURITY

Although Bitcoin already provides a large degree of trans-
parency — as its transaction ledger, called the blockchain, is
globally visible — it does not satisfy the requirements of a
transparency overlay. In particular, the miners, who play a
role analogous to the log server in producing the blockchain,
are not known entities and thus cannot be held responsible;
this in turn means that consistency and non-frameability
cannot be satisﬁed. In this section, we thus begin by pre-
senting in Section 6.1 a secure basic transparency overlay
for Bitcoin.

One might naturally wonder whether such a distinction is
purely pedantic; i.e., if overlaying transparency on top of a
transparent system provides any actual beneﬁts. To answer
this question in the aﬃrmative, we discuss in Section 6.2
the beneﬁts (in terms of both security and eﬃciency) that
are achieved by applying the transparency overlay. In par-
ticular, we show that the addition of a secure transparency
overlay relieves regular Bitcoin users (i.e., users wishing only
to spend and receive bitcoins) from having to store and ver-
ify the entire Bitcoin blockchain, which as of this writing is
over 80GB.4 To go even further, we argue that if one is will-
ing to adopt a distributed rather than a fully decentralized
solution (i.e., if one is willing to trust any set of named par-
ties), then the entire Bitcoin system collapses into a CT-like
4https://blockchain.info/charts/blocks-size

177/
/
o
o
o
o
/
/
o
o
transparency overlay and the need for hash-based mining is
eliminated.
6.1 A transparency overlay for Bitcoin

As depicted in Section 2, Bitcoin has three actors in the
system Sys: a sender Sender, a receiver Receiver, and a miner
Miner. The sender and the miner must participate in the Log
protocol to enter transactions into the log (although really
this can be done by only the miner, after it has collected all
relevant transactions), and the receiver participates in the
CheckEntry protocol to check that the transaction in which
it should be receiving bitcoins is in the log. Our trans-
parency overlay for Bitcoin then instantiates GenEventSet
as follows:

Receiver Sender(tx) Miner(headold, hgt, txset)

return ε

return ε

tx

txset ← txset ∪ {tx}

head r←− Mine(headold, txset)
return (head, hgt, txset)

An event is a transaction tx, which must have a certain
structure (i.e., lists of input and output addresses) and sat-
isfy certain requirements (i.e., that is does not represent
double-spending). A set of events eventset is a block, which
contains not only a list of transactions txset but also a hash
head, a pointer headprev to the previous block, and a height
hgt; combining events in an event set also allows us to impose
the required notion of timing, which is the block height hgt.
By combining GenEventSet with the modiﬁed protocols de-
scribed in Section 4.4, we can thus apply Theorem 4.4 to get
a secure basic transparency overlay in the setting of Bitcoin.
6.2 Further security implications

By applying a transparency overlay to Bitcoin, we have
provided a method for achieving provable transparency guar-
antees in this setting. We have also achieved (in a manner
similarly observed by Miller et al. [27], although they did
not provide any security guarantees) a much more eﬃcient
version of the system: senders and receivers now store noth-
ing (or, if the auditor collapses into the users as discussed
in Section 5.3 for CT, they store a snapshot), as compared
to the entire blockchain or set of block headers that they
were required to store previously. While this goal was of
course already achievable by Bitcoin senders and receivers
using web solutions (i.e., storing their bitcoins in an online
wallet), our system is the ﬁrst to achieve this goal with any
provable security guarantees, thus minimizing the trust that
such users must place in any third party.

Our analysis also has implications beyond users’ storage of
the blockchain. To go beyond our initial attempt at an over-
lay (which we dub the “na¨ıve overlay” in Table 1), one might
observe that the miner provides no additional value beyond
that of the log server: whereas in CT the CA was necessary
to provide a signature (and more generally is assumed to
perform external functions such as verifying the owner of a
website), here the miner just collates the transactions and
sends them to the log server. By having senders contact log
servers directly, one could therefore eliminate entirely the
role of mining without any adverse eﬀects on security. Thus,

Bitcoin

Na¨ıve overlay CT-like overlay

Hashing
Set of miners
Broadcast
Provable security

decentralized

yes

yes
no

yes

hybrid*

yes
yes*

no

distributed

no
yes

Table 1: The diﬀerent tradeoﬀs between Bitcoin, our na¨ıve over-
lay, and a “CT-like” overlay in which log servers completely re-
place miners. Our na¨ıve solution provides the same openness that
Bitcoin has for miners but also provable security guarantees for
those who make (optional) use of distributed log servers, while
our CT-like solution requires trust in the set of log servers but
achieves both provable security and signiﬁcantly better eﬃciency.

if users are willing to make the trust assumptions necessary
for our transparency overlay — namely, to assume that some
honest majority of a distributed set of log servers provide
the correct response about the inclusion of a transaction —
then the system can collapse into a distributed structure
(the “CT-like overlay” in Table 1) in which no energy is ex-
pended to produce the ledger, and users have minimal stor-
age requirements. Moreover, if users communicate directly
with the log server, then we could add a signed acknowl-
edgment from the log server that would allow us to satisfy
accountability. Interestingly, this solution closely resembles
the recent RSCoin proposal [14] (but with our additional con-
sistency and non-frameability guarantees), which achieves
linear scaling in transaction throughput; this provides addi-
tional validation and suggests that this distributed approach
presents an attractive compromise between the two settings.

7. CONCLUSIONS AND OPEN PROBLEMS
In this paper, we initiated a formal study of transparency
overlays by providing deﬁnitions and a generic secure con-
struction of this new primitive. To demonstrate the broad
applicability of our generic formalization, we proved that
Certiﬁcate Transparency (CT) is a secure transparency over-
lay, and presented a Bitcoin-based transparency overlay that
achieves provable notions of security and signiﬁcantly re-
duces the storage costs of regular Bitcoin users. Our com-
parison reveals that in any settings where distributed trust
is possible (i.e., one is willing to trust any set of known par-
ticipants), Bitcoin can collapse into CT and the need for
both mining and the storage of the blockchain disappears.
On the other hand, if one is not willing to trust anyone, then
on a certain level these requirements seem inevitable.

While our constructions provide provably secure proper-
ties concerning integrity, it is not clear how our transparency
overlay could provide this same value to any system in which
a meaningful notion of privacy is required. It is thus an in-
teresting open problem to explore the interaction between
transparency and privacy, and in particular to provide a
transparency overlay that preserves any privacy guarantees
of the underlying system.

Acknowledgments
Sarah Meiklejohn is supported in part by EPSRC Grant
EP/M029026/1.

8. REFERENCES
[1] A. Anagnostopoulos, M. T. Goodrich, and

R. Tamassia. Persistent authenticated dictionaries and

178/
/
their applications. In G. I. Davida and Y. Frankel,
editors, ISC 2001, volume 2200 of LNCS, pages
379–393, Malaga, Spain, Oct. 1–3, 2001. Springer,
Berlin, Germany.

[2] M. Andrychowicz and S. Dziembowski. Pow-based
distributed cryptography with no trusted setup. In
Proceedings of Crypto 2015, 2015.

[3] M. Andrychowicz, S. Dziembowski, D. Malinowski,

and L. Mazurek. Secure multiparty computations on
Bitcoin. In Proceedings of the IEEE Symposium on
Security and Privacy, 2014.

[4] D. Basin, C. Cremers, T. H.-J. Kim, A. Perrig,
R. Sasse, and P. Szalachowski. ARPKI: Attack
Resilient Public-Key Infrastructure. In Proceedings of
ACM CCS 2014, pages 382–393, 2014.

[5] M. Bellare and S. Keelveedhi. Interactive

message-locked encryption and secure deduplication.
In Proceedings of PKC 2015, volume 9020 of LNCS,
pages 516–538, 2015.

[16] C. Fromknecht, D. Velicanu, and S. Yakoubov. A

decentralized public key infrastructure with identity
retention. IACR Cryptology ePrint Archive, Report
2014/803, 2014. http://eprint.iacr.org/2014/803.pdf.
[17] J. Garay, A. Kiayias, and N. Leonardos. The Bitcoin

backbone protocol: Analysis and applications. In
Proceedings of Eurocrypt 2015, 2015.

[18] C. Garman, M. Green, and I. Miers. Decentralized
anonymous credentials. In Proceedings of the NDSS
Symposium 2014, 2014.

[19] D. Goodin. Fraudulent Google credential found in the

wild, Aug. 2011.

[20] T. H.-J. Kim, L.-S. Huang, A. Perrig, C. Jackson, and

V. Gligor. Accountable key infrastructure (AKI): a
proposal for a public-key validation infrastructure. In
Proceedings of WWW 2013, pages 679–690, 2013.
[21] B. Laurie, A. Langley, and E. Kasper. Certiﬁcate

transparency, 2013.

[22] J. Leyden. Inside ‘Operation Black Tulip’: DigiNotar

[6] E. Ben-Sasson, A. Chiesa, C. Garman, M. Green,

hack analysed, Sept. 2011.

I. Miers, E. Tromer, and M. Virza. Zerocash:
Decentralized anonymous payments from Bitcoin. In
Proceedings of the IEEE Symposium on Security and
Privacy, 2014.

[7] J. C. Benaloh and M. de Mare. One-way

accumulators: A decentralized alternative to digital
sinatures (extended abstract). In T. Helleseth, editor,
EUROCRYPT’93, volume 765 of LNCS, pages
274–285, Lofthus, Norway, May 23–27, 1993. Springer,
Berlin, Germany.

[8] I. Bentov and R. Kumaresan. How to use bitcoin to

design fair protocols. In J. A. Garay and R. Gennaro,
editors, CRYPTO 2014, Part II, volume 8617 of
LNCS, pages 421–439, Santa Barbara, CA, USA,
Aug. 17–21, 2014. Springer, Berlin, Germany.

[9] J. Bonneau, A. Miller, J. Clark, A. Narayanan, J. A.

Kroll, and E. W. Felten. Research perspectives and
challenges for Bitcoin and cryptocurrencies. In
Proceedings of the IEEE Symposium on Security and
Privacy, 2015.

[10] P. Bright. Independent Iranian hacker claims

responsibility for Comodo hack, Mar. 2011.

[11] J. Camenisch, M. Kohlweiss, and C. Soriente. An
accumulator based on bilinear maps and eﬃcient
revocation for anonymous credentials. In S. Jarecki
and G. Tsudik, editors, PKC 2009, volume 5443 of
LNCS, pages 481–500, Irvine, CA, USA, Mar. 18–20,
2009. Springer, Berlin, Germany.

[12] J. Camenisch and A. Lysyanskaya. Dynamic

accumulators and application to eﬃcient revocation of
anonymous credentials. In M. Yung, editor,
CRYPTO 2002, volume 2442 of LNCS, pages 61–76,
Santa Barbara, CA, USA, Aug. 18–22, 2002. Springer,
Berlin, Germany.

[13] S. Crosby and D. Wallach. Eﬃcient data structures for

tamper-evident logging. In Proceedings of the 18th
USENIX Security Symposium, 2009.

[14] G. Danezis and S. Meiklejohn. Centrally banked

cryptocurrencies. In Proceedings of NDSS 2016, 2016.

[15] B. Dowling, F. G¨unther, U. Herath, and D. Stebila.

Secure logging schemes and Certiﬁcate Transparency.
In Proceedings of ESORICS 2016, 2016. To appear.

[23] H. Lipmaa. Secure accumulators from euclidean rings

without trusted setup. In F. Bao, P. Samarati, and
J. Zhou, editors, ACNS 12, volume 7341 of LNCS,
pages 224–240, Singapore, June 26–29, 2012. Springer,
Berlin, Germany.

[24] M. S. Melara, A. Blankstein, J. Bonneau, E. W.

Felten, and M. J. Freedman. CONIKS: Bringing key
transparency to end users. In Proceedings of USENIX
Security 2015, 2015.

[25] J. Menn. Key Internet operator VeriSign hit by

hackers, Feb. 2012.

[26] R. C. Merkle. A certiﬁed digital signature. In

G. Brassard, editor, CRYPTO’89, volume 435 of
LNCS, pages 218–238, Santa Barbara, CA, USA,
Aug. 20–24, 1989. Springer, Berlin, Germany.

[27] A. Miller, M. Hicks, J. Katz, and E. Shi.

Authenticated data structures, generically. In
Proceedings of POPL 2014, 2014.

[28] S. Nakamoto. Bitcoin: A Peer-to-Peer Electronic Cash

System, 2008. bitcoin.org/bitcoin.pdf.

[29] Nasdaq. Nasdaq launches enterprise-wide blockchain

technology initiative, May 2015.

[30] D. O’Leary, V. D’Agostino, S. R. Re, J. Burney, and

A. Hoﬀman. Method and system for processing
Internet payments using the electronic funds transfer
network, Nov. 2013.

[31] C. Papamanthou, E. Shi, R. Tamassia, and K. Yi.

Streaming authenticated data structures. In
T. Johansson and P. Q. Nguyen, editors,
EUROCRYPT 2013, volume 7881 of LNCS, pages
353–370, Athens, Greece, May 26–30, 2013. Springer,
Berlin, Germany.

[32] C. Papamanthou, R. Tamassia, and N. Triandopoulos.
Authenticated hash tables. In P. Ning, P. F. Syverson,
and S. Jha, editors, ACM CCS 08, pages 437–448,
Alexandria, Virginia, USA, Oct. 27–31, 2008. ACM
Press.

[33] M. D. Ryan. Enhanced certiﬁcate transparency and
end-to-end encrypted mail. In Proceedings of NDSS
2014, 2014.

179