Cross-Site Search Attacks

Nethanel Gelernter

Department of Computer Science

Bar Ilan University

nethanel.gelernter@gmail.com

Amir Herzberg

Department of Computer Science

Bar Ilan University

amir.herzberg@gmail.com

ABSTRACT
Cross-site search (XS-search) attacks circumvent the same-
origin policy and extract sensitive information, by using the
time it takes for the browser to receive responses to search
queries. This side-channel is usually considered impractical,
due to the limited attack duration and high variability of
delays. This may be true for naive XS-search attacks; how-
ever, we show that the use of better tools facilitates eﬀective
XS-search attacks, exposing information eﬃciently and pre-
cisely.

We present and evaluate three types of tools:

(1) ap-
propriate statistical tests, (2) ampliﬁcation of the timing
side-channel, by ‘inﬂating’ communication or computation,
and (3) optimized, tailored divide-and-conquer algorithms,
to identify terms from large ‘dictionaries’. These techniques
may be applicable in other scenarios.

We implemented and evaluated the attacks against the
popular Gmail and Bing services, in several environments
and ethical experiments, taking careful, IRB-approved mea-
sures to avoid exposure of personal information.

Categories and Subject Descriptors
J.0 [Computer Applications]: General

Keywords
Side channel attacks; Web; Privacy; Security

1.

INTRODUCTION

Users of cloud and web services depend on the provider for
their privacy. The provider is trusted not to make unautho-
rized use of the user’s data and to protect the data against
unauthorized access by third parties. Extensive research
eﬀorts are directed at reducing or avoiding these poten-
tial privacy exposures, e.g., by encrypting the data kept
by the provider, possibly using searchable encryption [5].
Many works even focus on hiding the access pattern, us-
ing private information retrieval (PIR) [8] or oblivious RAM
(ORAM) [17] for read/write.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3832-5/15/10$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813688.

However, these approaches are irrelevant for search en-
gines, webmail and other Software-as-a-Service sites, to
which users knowingly disclose signiﬁcant private informa-
tion, with consideration of the provider’s reputation and pri-
vacy policy, and the relevant regulatory framework. It is still
critical to protect privacy against rogue third parties, mainly
by using https to protect against eavesdropping and MitM
attackers, and relying on the browser’s Same Origin Policy
(SOP) to prevent exposure to rogue third-party sites.

Previous research [7, 12] showed that eavesdropping and
MitM attackers can still learn information, by observing the
amount of communication. However, these impressive at-
tacks have signiﬁcant limitations: they require eavesdrop-
ping, and attacker must wait for the user to access websites
providing speciﬁc, sensitive services (e.g., medical or ﬁnan-
cial).

We show that by using timing side-channel, information
can be exposed by (weaker, common) cross-site attackers,
which simply control a rogue website to which the user surfs,
and do not have eavesdropping/MitM capabilities. Further-
more, XS-search attacks apply even to general-purpose ser-
vices such as webmail and search-engines, and to queries
initiated by the attacker - no need to wait for user to per-
form sensitive query.

XS-search attacks do not require circumvention of the
SOP or exploiting (known) browser/site vulnerabilities.
This is in contrast to the (very common) known rogue-
site attacks, which mostly exploit browser and/or web-
site vulnerabilities, e.g., Cross-Site Scripting (XSS), Cross-
Site Request Forgery (CSRF) attacks [11, 21, 29] and DNS-
rebinding [20]. These attacks are well-known and hence usu-
ally fail against ‘serious’ sites and updated browsers.

In particular, sites usually prevent CSRF attacks, by
denying cross-site requests that have a ‘sensitive’ state-
changing impact. On the other hand, even important,
security-savvy web-services allow requests from other sites,
which do not involve (sensitive) state-changes, since such
requests may have diﬀerent legitimate purposes.

Security experts know that cross-site queries create a tim-
ing side-channel [4]. However, there are signiﬁcant chal-
lenges in using such side-channel: the attack is limited to
the (short) duration when the user ‘visits’ the rogue web-
site, and must deal with unpredictable delays and imprecise
measurements. Hence, many web security experts consider
it impractical to abuse the timing side-channel, for eﬀective
extraction of sensitive information.
In fact, after we dis-
closed our attacks to vendors, Google referred us to a blog-
post [13] that discusses this threat. In spite of being aware

1394of the theoretical risk, Google and other vendors did not
consider the attack to be practical and to justify adoption
of appropriate defenses. This may be compared to ‘theoret-
ical’ attacks on cryptographic systems, which may indicate
that the cryptosystem does not meet the theoretical deﬁ-
nitions - but usually require signiﬁcant additional research
and development of clever techniques and complex tools, to
turn into a practical attack.
Indeed, there are signiﬁcant
challenges facing XS-search attacks, including:
Noise: Delays depend on several dynamically-changing fac-
tors such as congestion and concurrent processes in
client and server.

Small sample size: the attacking web-site has to take ad-
vantage of the duration of the visit by the user, which
is often just a few minutes. Furthermore, the attacker
must use a limited rate of requests to avoid detection
and blocking (e.g., by server’s anti-DoS defenses).

Measurement: time measurements in scripts are often in-
accurate. We saw diﬀerences of dozens of milliseconds
between the times we measured (in script), and more
precise measurements (e.g., in browser console). Some
browsers provide scripts with a performance object
that records the loading time; attacker may use it to
further improve accuracy [26].

We show that in spite of these limitations, cross-site at-
tackers are still able to eﬃciently and accurately extract a
signiﬁcant amount of sensitive information, even from major
websites. Speciﬁcally, in this manuscript, we demonstrate
the attacks against Gmail, the largest webmail service, and
Bing, the second-largest search engine; see [27] for other
vulnerable major sites, e.g., Facebook, Outlook and Yahoo!.
For example, a visit of a minute suﬃces for the rogue site to
ﬁnd the user’s ﬁrst and last names, with 90% success rate;
see Table 3. Other examples of sensitive information that
may be exposed include:
Search history: ﬁnd out terms searched by the victim

(e.g., in Bing).

Relationships: identify people and organizations with
whom the user corresponds, possibly in relation to spe-
ciﬁc terms (e.g., romantic or professional), dates, etc.

Detect sender/recipient, including bcc recipients:

check if a speciﬁc identity was the sender/recipient of a
particular email, when the message was sent/received,
and identities of sender and (other) recipients (incl.
bcc recipients).

Detect terms in (speciﬁc) messages and folders:

check whether a particular term or phrase appears
in a speciﬁed subset of Gmail messages, e.g.,
in
a speciﬁc folder,
sent to/from speciﬁc identities,
containing some other term, or sent/received during
a speciﬁed time interval. This can also be limited to
frequently-appearing items.

Structured (sensitive) information: for

example,

credit card and phone numbers.

The exposed information can be abused in many ways,
for example, to facilitate spam and phishing, including au-
tomated spear-phishing.

Cross-site search attack. XS-search attacks exploit dif-
ferences in the loading time of responses without results, to
these of responses with few or many results. The diﬀerences
may be due to diﬀerences in the computation time, and/or to
diﬀerences in the transmission and processing times, mainly

depending on response length. Typically, to learn whether
or not a challenge request yields results, the adversary sends
the request with a dummy request, which is known to return
an empty response. The assumption is that the measure-
ments for both the requests will be similar, if and only if
the challenge request also yields an empty response. See
Figure 1.

For example, to test whether the name of a Gmail user is
“Vic”, the attacker will send two search requests: the chal-
lenge request will ask for emails in the Sent Mail folder that
were sent from Vic, and the dummy request will similarly
ask for messages sent from a random dummy string. While
the dummy request is expected to always yield an empty re-
sponse, the challenge request will return a diﬀerent response
if and only if the name of the victim is Vic.

To deal with the challenges facing practical XS-search at-
tacks, as described above, we investigated and developed
several types of tools:
Statistical tests: The right statistical test is critical for ef-
ﬁcient, accurate exposure. We evaluated several tests,
including ‘classical’ tests and ‘tailored’ tests which we
developed, that provided signiﬁcant improvements.

Response-inﬂate mechanisms, where the attacker sig-
niﬁcantly increases the size of one of the two response
options. This may require the term to appear in mul-
tiple records within the private data. See Figure 1(a)
and Section 3.

Compute-inﬂate mechanisms, where the attacker sig-
niﬁcantly increases the computation load of one the
two response options. This usually works, even when
the term appears only once within the private data.
See Figure 1(b) and Section 4.

Divide and conquer XS-search algorithms: A single
search request can be used to answer a Boolean ques-
tion. However, it is desirable to solve more complex
problems, like detecting the name of the victim out
of a list of 2000 names. To further eﬃciently use
these mechanisms to extract information from pri-
vate records, we developed appropriate divide and
conquer algorithms to identify relevant terms among
many. With these algorithms, an attacker can eﬃ-
ciently search within a large set of terms by compar-
ing, each time, between two (or a few) large subsets.
See Section 5.

Contributions. We draw attention to the potential pri-
vacy exposure due to XS-search attacks. XS-search attacks
are not trivial to launch; however, we present several types of
tools that facilitate eﬀective XS-search attacks: XS-search-
optimized statistical tests, response-inﬂate and compute-
inﬂate mechanisms, and divide-and-conquer XS-search al-
gorithms (see brief descriptions above, and details within).
We evaluated these diﬀerent tools extensively, using eth-
ical, IRB-approved experiments, focusing on some of the
most popular web-services.

The tools may be applicable to other scenarios. Speciﬁ-
cally, our evaluation of diﬀerent statistical tests, strengthens
conclusions of [10], but further shows that in some practi-
cal scenarios, simpler tests can be used to eﬃciently extract
information, and may allow the use of less samples.
1.1 Ethics and Disclosure

We implemented all of our attacks (in JavaScript), and
validated them using experiments in diﬀerent environments.

1395(a) Response-inﬂate attack example (Section 3)

(b) Compute-inﬂate attack example (Section 4)

Figure 1: Examples of eﬀective XS-search attack using the Response-inﬂate and Compute-inﬂate techniques. In the end of
the attack, the adversary uses statistical tests to decide whether T (rC ) and T (rD) were sampled from the same distribution.

All experiments were IRB-approved after careful design and
evaluation, to avoid privacy exposures or other ethical issues.
The experiments were carried out on ‘real’ (e.g., Gmail) user
accounts and (e.g., Bing) browsing logs. We focused on the
experiments that we could ethically perform on data from a
signiﬁcant number of (paid) volunteers, including students
as well as users of the Amazon’s Mechanical Turk service.
In these experiments, we only ‘expose’ items that the users
shared with us voluntary, such as their name or a random
value they selected. This allowed us to validate correctness
and to avoid unintentional privacy exposure. We also car-
ried out several experiments on our own accounts to validate
that the techniques expose ‘real information’, e.g., credit
card numbers, passwords and names of contacts, locations,
projects etc. We have alerted both Bing and Google, and
they conﬁrmed the attacks and adopted countermeasures.
We also alerted additional popular websites which we found
vulnerable to the attack.
1.2 Related Work

Timing and other side-channels were eﬀectively used to
circumvent many cryptographic and access-control defenses
[3, 6, 10, 22, 23, 30]. Several works present cross-site attacks
that expose the browsing-history of the user [14,31], or iden-
tify sites to which the user is connected concurrently [16].
Bortz et al. [4] showed that these techniques can also expose
the number of items in shopping cart.

Cross-site side-channel attacks that extract information
using database queries were presented by Futoransky et.
al [15], assuming the ability to perform public queries over
the database. However, their results were very limited with
regards to the amount of information leaked (and that was in
an ‘ideal’ setting of direct access to their own database). A
related, well-known technique is blind SQL injection. How-
ever, this technique depends on SQL injection vulnerabilities
and is not likely to apply to well-protected web services (e.g.,
see [9]). Furthermore, large web-services such as webmail
and search engines rarely use SQL.

Evans discussed XS-search attacks in a blog-post [13],
however, without evaluating its eﬀectiveness or developing
any tools to make the attack practical; indeed, from the re-
sponses of vendors, this attack was not considered practical.

2. XS-Search ATTACK

In this section we brieﬂy introduce the XS-search attack
and its use of timing side-channel. We ﬁrst describe the XS-

search attack, and then discuss techniques for analyzing the
responses’ loading times. In Sections 3 and 4 we discuss two
‘inﬂation’ techniques that improve the attacks by decreasing
the number of requests required for the attacker to learn
information.
2.1 Adversary Model & Attack Process

Despite the large amount of information that can be ob-
tained through it, the XS-search attack can be launched by a
weak adversary. This weak adversary runs a malicious web-
site, but does not require eavesdropping or MitM abilities.
From her website, the adversary can send HTTP requests
and measure the time until a response is received. The mea-
surement is done in the victim’s browser, for example, using
JavaScript. The attacker might attack casual visitors to her
website, or use social engineering techniques to lure speciﬁc
targets to visit the site.

The attacker’s goal is to detect whether a search request
has results or not. We denote a search request for which we
want to learn whether it has results as the challenge request
(rC ). The main idea of XS-search is as follows: the time
it takes until an HTTP response is loaded by the browser
actually leaks information about the response’s content. In
particular, the loading time of a response with search re-
sults (full response) and a response without them (empty
response) is often diﬀerent. The diﬀerence might be due
to the processing time of the request or due to the size of
the response. Hence, we concentrate on measuring the time
required to receive a response to search requests.

Attack process. We assume that the attacker can send
search requests that are replied with an empty response.
This is a reasonable assumption because in most of the ser-
vices, search requests for meaningless terms that a user is un-
likely to ever use, are usually replied with empty responses.
We call such a request a dummy request (rD).

To launch the attack, the attacker sends multiple pairs
of challenge requests (rC ) and dummy requests (rD). To
avoid cached response, the attacker concatenates a random
dummy parameter to each request. We denote the load-
ing time of responses for request r by T (r). Based on the
measured response time values, the attacker has to decide
whether the challenge request rC also resulted in an empty
response (like rD), or if it resulted in a non-empty response,
i.e., there were some matching records.

The assumption behind the analysis of the results is that
the values in T (rC ) will be relatively similar to the values in
T (rD), if rC also receives replies with empty responses. On

1396the other hand, we expect to see a greater diﬀerence between
the distributions, if rC is replied to with a full response,
unlike rD.
2.2 Analyzing the Measured Times

Given T (rC ) and T (rD), the attacker’s challenge is to de-
cide whether rC was replied to with a diﬀerent response
than rD. This problem of deciding whether two sets of val-
ues were sampled from the same distribution is well known.
We ﬁrst describe classical tests for solving this problem and
then additional simple tests we used.

2.2.1 Classical Hypothesis Tests
We build on the seminal work of Crosby, Wallach, and
Riedi [10], who evaluated the use of timing channels as a
means to perform cryptanalysis on a remote system. Crosby
et al. evaluated several well-known hypothesis testing meth-
ods, including the classical and modiﬁed Student’s t-test and
the Mann-Whitney U test, and showed they were all signiﬁ-
cantly inferior to a new and simpler test they developed, the
box test.

D, mi

C )}.

The box test receives as parameters a small interval of
two percentiles [a, b], where 0 < a < b and typically b <
6%, and a pair of sets of measurements {(mi
C )}, where
mi
D are measurements of timing for dummy request rD, and
mi
C are measurements of timing for the challenge request
being evaluated, rC . The box test estimates that the mi
D
measurements are from the same distributions if there is
overlap between the [a, b] percentile values of the two sets
{(mi

D} and {mi
We compared the results for the same set of well-known
tests, together with the box test (slightly modiﬁed, see next
paragraph), when applied to our scenario involving timing
side-channel using remote measurements. However, in our
case, the main impact was for non-cryptographic processing
and communication delays, while for Crosby et al. [10] these
are just noise to the signal (the cryptographic computation
time). Our results show that, as in [10], the box test out-
performed all the well-known statistical tests we compared
against.

Since our results were good enough to allow the use of tiny
samples, we had to slightly modify the box test. Namely,
instead of checking for overlap between the [a, b] percentile
C )}, for some small b < 6%, we
values of sets {(mi
checked for overlapping between the intervals [mi
D] and
C ], for low values i < j (e.g., 0 ≤ i < j ≤ 2). We
[mi
denote the box test between the i-th and j-th lower values
by BXj
i .

D}, {mi

D, mj

C , mj

In addition to the box test, we used the Apache Commons
Mathematics Library [1] to perform the following ‘classical’
hypothesis tests: Student’s t-test, Mann-Whitney U test
(denoted MW), Wilcoxon signed-rank test, Kolmogorov-
Smirnov (denoted KS) two sample test, and one-way
ANOVA test [24]. Throughout the paper, we evaluate the
timing results according to the box test, the KS test, and
the MW test, which achieved better results than the other
‘classical’ hypothesis tests.

2.2.2 Tiny-Sample Tests: MIN and AVG
As Crosby et al. pointed out [10], basic statistical mea-
sures such as the median or the average, do not provide
very good results. However, since we found that even tiny
samples seem to have meaningful diﬀerences in our measure-

ments, we deﬁned and evaluated two very simple tests. In
our experiments, these tests consistently outperformed the
box test, as well as all classical statistical tests we compared
against. Details follow.

Our new tests are based on Crosby et al.’s observation
that the lower values in the samples give a better indica-
tion about the diﬀerences between the distributions. We
deﬁne the AVGt
(1) From
each sample, T (rC ) and T (rD), remove the p% highest val-
ues.
(2) Calculate the average of the remaining samples,
AVG-p(T (rC )) and AVG-p(T (rD)). (3) Return true if and
only if AVG-p(T (rC ))/AVG-p(T (rD)) > t.

-p, which contains three steps:

Similarly, we deﬁne the MINt

i test, where we compare to

t the ratio between the lowest i-th values.

The main advantage of these tests is the fact that they are
applicable and eﬀective even for minimal samples. Another
advantage is their easy evaluation, which allows the attacker
to eﬀectively run them at the client side using JavaScript.
Since all these methods use a threshold, this poses a chal-
lenge of ﬁnding a good (or optimal) threshold. In spite of
this, we found it easy to ﬁnd thresholds that achieve better
results than the box test and the ‘classical’ statistical tests.
See for example results in Section 3 or Table 2 in Section
4.2. In practice, an attacker can ﬁnd the threshold value by
simulating the attack in a controlled environment.

3. Response-Inﬂate XS-Search Attack

The loading time of responses to search requests depends
on the number of search results. It takes longer to prepare
and transmit long, full responses that contain many search
results, than empty responses indicating no results. A longer
response means greater processing time and greater trans-
mission time. Even if the response is sent compressed, there
is still a diﬀerence in the size, and it takes longer to zip and
unzip long responses.

Following the approach outlined in Section 2.1, the dif-
ference in time to receive a response may allow a cross-
site attacker to detect whether the response was empty or
not. Namely, the attacker sends the challenge request to-
gether with the dummy request, which is expected to yield
an empty response, and compares the loading time of the
two responses.

In this section, we present the Response-inﬂate XS-search
attack, a technique that makes it easier to distinguish be-
tween the loading time of empty and full responses, allowing
the attacker to ﬁnd the correct answers to Boolean queries
on the sensitive data of the user in the records of the web-
service.
In Section 5, we use such techniques as building
blocks, to answer more complex queries, e.g., to eﬃciently
ﬁnd out the user’s name, phone-number, and other informa-
tion.

The idea of the attack is to increase the diﬀerence between
the size of empty and full responses. We begin by presenting
some methods of response inﬂating, which we found applica-
ble to many web-services (see [27]), and then we give speciﬁc
details for two example sensitive data services: the Bing his-
tory logs and the Gmail email archive.
3.1 Response-Inﬂate XS-Search Attack:

Methods

In many web-services, each request may contain a param-
eter that is copied, at least once - often more - for each entry
in the response. Furthermore, a response may often contain

1397many entries, typically, one entry per every record which ﬁts
the criteria in the request. By sending a relatively-long pa-
rameter, the length of the response is signiﬁcantly inﬂuenced
by the number of entries, which often allows the attacker
to distinguish between empty responses, i.e., no-match, vs.
responses with signiﬁcant number of matches. This is facil-
itated by the fact that many web-services do not explicitly
limit the length of such parameters, and in particular, many
allow parameters of over 2KB.

The number of times that such parameters are copied,
may depend on properties of the entry in the response, which
may allow an attacker to detect the relevant property. For
example, when the Bing service receives a more request to
Bing, it copies the value of the IID parameters three times
for each query result which the user ‘followed’ (clicked on),
and twice for each ‘orphan’ response (where the user did not
follow on any result); details in subsection 3.2.
3.2 Response-Inﬂate XS-Search Attack on

Bing’s Search History

Bing’s search history is an example of a service vulnera-
ble to the Response-Inﬂate XS-Search Attack. Many web-
services maintain a log containing ‘history-records’ listing
the operations done by each user. Logs often contain sen-
sitive private information and should not be made publicly
available.Services often allow users to view and search within
their search log.
In particular, Bing (and others such as
Google) permits cross-site search requests from a third-party
site, i.e., they do not apply CSRF-restrictions for such re-
quests. In both Bing and Google, the ability to do cross-site
history searches may allow an attacker to learn which search
terms were used in previous searches by the user; we focus
on Bing.1

Terminology: requests vs. queries. To avoid con-
fusion, we use the term request for cross-site requests for
search within the user’s search history as kept by Bing, and
the term query for search queries done by the user (entries
in the search history). Namely, the attacker sends search
requests that return search queries, sent in the past by the
victim.

Bing History Feature. Bing supports two types of re-
quests for searching through its search-history: search re-
quests and more requests. The Bing client-side script sends
‘search’ requests based on the search parameters entered by
the user, and sends ‘more’ requests when the user scrolls-
down the page to see additional results.

Bing responds to both search and more requests with a
list containing up to twenty entries from the user’s search-
history. For fruitful search queries, i.e., queries for which the
user clicked on some of their results, Bing uses one entry for
each clicked-result. For orphan search queries, i.e., queries
for which the user did not click on any result, Bing uses only
a single entry.

The twenty queries returned for search requests are the
most recent search-queries of the user (that ﬁt the search
terms given by the user). To allow Bing to provide the ‘next
recent’ entries, the ‘more’ requests have a parameter t, which
speciﬁes a speciﬁc time; Bing replies with the most recent
history entries that are dated prior to time t. Bing allows

1The attack is not as relevant in Google since Google re-
quires users to re-authenticate in order to search their his-
tory records; we believe that in practice, this would usually
foil the attack.

this parameter to be set to a future (or current) time, and
in this case simply returns the latest results (similar to the
‘search’ query).

The maximal length of both the search and more requests
is 2KB. Each of these two requests contains a parameter
that is copied, at least once, for each entry in the response;
its length can be almost as long as the 2KB length limit.
In the response for a search request, this parameter is the
FORM parameter. The value given in this parameter is
copied once for each query appearing in the response, plus
four more times.
In the response for a more request, the
parameter is the IID parameter. The value given in this
parameter is copied twice for each entry representing an or-
phan query, and three times for each of the multiple entries
of a fruitful query.

Finally, there is a signiﬁcant diﬀerence in the size of empty
responses for search versus more requests. For search re-
quests, the size of empty responses is about 50KB, while
for more requests, the size of such responses is less than
1KB. The smaller size of empty responses and the higher
inﬂation ratio, make the more request more suitable for the
Response-inﬂate XS-search attack.

Attack details. Consider a request that is replied to with
a full response containing X orphan queries and Y entries
for all the fruitful queries together. By using FORM and
IID parameters of 2000 bytes each, the size of the response
for the search request is inﬂated by nearly 2000 · (X + Y )
bytes compared to the empty response. Moreover, the size
of the response for the more request is inﬂated by nearly
2000 · (2X + 3Y ) bytes. This inﬂation is exacerbated since
Bing does not compress (zip) their responses.

3.2.1 Bing-History Response-Inﬂate XS-Search At-

tack: Environments

To analyze the eﬀectiveness of the Bing-history attack,
we applied it on a Bing account we have set-up, in three
diﬀerent environments:

1. High speed wired connection (HS). We used a server in

our lab with measured 95 Mb/s download speed.

2. Home network wireless connection (HN). We used a
laptop connected to a home wireless network. The
measured download speed was 12 − 15 Mb/s.

3. Open wireless network (OW). We used a laptop con-
nected to our university’s wireless network. During the
tests, we measured a download speed that was gener-
ally around 3.5 Mb/s (the highest measured rate was
4.05, and the lowest 1.59).

Note that the HS and the OW environments present some
challenges for the attack. In high-quality high-speed connec-
tions, although the variance of the loading time is expected
to be low, the diﬀerence in the transmission time of two ob-
jects of diﬀerent sizes is also lower. In the OW environment,
although the download speed is low, which makes the diﬀer-
ence in the loading times greater, the variance in the loading
times is higher due to the frequent changes in the load from
other computers.
We tested the detection of term Ti that appears i times
in the Bing history, for i ∈ {1, 3, 5, 10, 20, 50}. For each
environment and each Ti, we took samples of at least 500
pairs of rC and rD, such that rC asks for Ti and rD is a
dummy request (which is certain not to exist in the Bing
search log). Then we separated the measured samples into
groups of n ∈ {5, 10, 15, 20, 25} consecutive pairs, and used

1398Figure 2: False negative rate (%) for BX2
0 and KS tests in
diﬀerent environments, as a function of the number of times
the term (Ti) appears in history (i ∈ {1, 3, 5, 10, 20, 50}).

Figure 4: False negative rate (%) for the AVG1.05−25 and
MIN1.05
tests in diﬀerent environments, as a function of
the number of times the term (Ti) appears in history (i ∈
{1, 3, 5, 10, 20, 50}).

0

Figure 3: False positive rate (%) for the BX2
0 and KS tests
in diﬀerent environments, as a function of the sample size.

several tests to decide whether rC was replied with results
or not. Based on the results, we measured the false-negative
(FN) rate of the methods. Similarly, for each environment,
we sent pairs of rD and rC requests, such that rC is an-
other dummy request, to evaluate the false-positive (FP)
rate. Figure 1(a) depicts the attack.

3.2.2 Results: Classical Hypothesis Tests
For each environment, Ti and n, we tested all the tests
mentioned in Section 2.2.1. The box test [10] and the
Kolmogorov-Smirnov (KS) test with α = 0.01 achieved the
best results. In the box test, we checked for overlapping in
the range between the three shortest times measured in each
sample; namely, we used BX2
0. We checked several ranges,
and this range gave the best results for most of the cases.

Figure 2 shows the FN rate of KS and BX2

0 in each of the
environments, for several sample sizes, as a function of the
frequency of the searched term in the history. The graphs
show that the FN rate decreases for terms that appear more
times in the history. The box test achieves better results
than KS and the other tests. Speciﬁcally, it achieves very
good results for terms that appears at least 5 times in the
history, even when using small samples with 5 pairs of re-
quests.

Figure 3 shows the FP rate of each of the tests in the

diﬀerent environments, as a function of the sample size.

3.2.3 Results: MIN and AVG Tiny-Sample Tests
We next evaluate the new tests which we introduced in
Section 2.2.2; we show that with the right parameters, these
simple tests can achieve excellent results. In Figures 4 and 5
we present the FN and FP rate of the MIN1.05
and AVG1.05−25
tests. The threshold (1.05) was chosen based on a test that
simulated the attack in the HN environment. Although this
threshold is not optimal, we can see that in the HN and OW

0

Figure 5: False positive rate (%) for the AVG1.05−25 and
MIN1.05
tests in diﬀerent environments, as a function of the
sample size.

0

environments both the tests achieve very good results in de-
tecting terms that appear at least 5 times in the history with
samples of 5 requests. In particular, MIN1.05
achieves lower
FN and FP rates than BX2
0. To achieve better results for
the HS environment, we should use a lower threshold (e.g.,
1.02). In practice, the adversary can estimate the connec-
tion quality and speed by downloading a large object, and
choose the threshold accordingly. However, this procedure
increases the time required for the attack. Additional details
and extended analysis appear in [27].

0

3.2.4 Response inﬂation effect
To test the eﬀect of the response inﬂation, we repeated our
evaluation experiment in the OW environment with simple
(not inﬂated) requests. The results showed that although it
is possible to distinguish between empty and full responses,
a larger sample size is required. We also observed that the
response inﬂation decreases the FN when detecting a full
response that contains fewer entries. Figure 6 depicts the
FN diﬀerence according to the box test (BX2
0) for simple
and inﬂating requests. We extend the comparison in [27].
3.3 Response-Inﬂate XS-Search Attack on

Gmail

Google’s Gmail is the most popular email service today.
We observed that it is possible to extract information from
Gmail accounts by measuring the times of cross-site search
requests and further inﬂating the diﬀerence in the sizes of
full and empty responses. Compared to the attack on Bing,
the inﬂating process is weaker due to compression of the
responses. Because the response inﬂation is done similarly to
the attack on Bing, we brieﬂy describe the response inﬂation
and further elaborate in [27].

1399Figure 6: False negative rate (%) for the BX2
0 using simple
and inﬂating requests, for diﬀerent sample sizes as a function
of the number of times the term (Ti) appears in history
(i ∈ {1, 3, 5, 10, 20, 50}).

Type (view mode) No results
Simple (standard)
147 (40.6)
Simple (HTML)
Inﬂated (HTML)

10 (3.4)
106 (11)

20 entries
166 (44.5)
20.2 (5.7)
303 (21.1)

50 entries
166 (44.5)
35.3 (8.5)
575 (25)

Table 1: Comparison of response size for Gmail search query
in the diﬀerent modes. The size after compression appears
in parentheses. The values are in KB.

Information leakage. The diﬀerence in the processing
time of full and empty responses allows an attacker to an-
swer Boolean questions, represented as search queries, about
the data that often appears in Gmail accounts. Among the
things that usually appear many times in Gmail accounts
are the name of the user, frequent contacts, data from the
email signature, and companies and services that send peri-
odic updates or reports. Boolean questions about whether
the name of the user is X, whether she has a contact named
Y, or is a client of bank Z, can be sent as a search query
for emails sent from X, sent or received from Y, or received
from the known ‘no-reply’ email address of bank Z.

Response inﬂation. Similarly to what we did for Bing,
we inﬂated the search query to its maximal size, which, in
Gmail, is 8KB. In Gmail’s basic HTML view [19], the search
query appears once for each entry in the response; this dra-
matically inﬂates the HTTP body of a full response over an
empty response. The default maximal number of entries in a
response is 50. Although Gmail sends the responses zipped,
and hence the improvement is not as signiﬁcant as in the
Bing history attack (see Section 3.2), it takes longer to cre-
ate, zip, and unzip longer responses. A comparison between
the simple and the inﬂating requests for launching eﬀective
timing attacks is discussed in [27].

4. Compute-Inﬂate XS-Search Attack

In contrast,

The Response-inﬂate XS-search attack is based on caus-
ing a signiﬁcant diﬀerence in the length of the response due
to its dependency on the number of terms returned. For
this diﬀerence to be signiﬁcant, the number of terms should
be substantial.
in this section, we present
the Compute-inﬂate XS-search attack, which can eﬀectively
check in the private data for the existence of records that
meet speciﬁc search criteria - even a single record. The
Compute-inﬂate XS-search attack is based on causing a sig-
niﬁcant diﬀerence in the time required to process the search
queries; this diﬀerence can often be signiﬁcant for carefully
engineered queries, even when the queries return roughly the
same amount of information. This exploits the fact that the
time to process queries may vary signiﬁcantly, depending on
speciﬁc conditions.

Speciﬁcally, the Compute-inﬂate XS-search attack is ap-
plicable for the many web services satisfying the following
conditions:
1. Cross-site queries allowed: service can be invoked di-
rectly from a diﬀerent-origin webpage, i.e., does not re-
strict service to calls issued by same-origin pages, using
the referer header or other CSRF-prevention mechanisms.
2. Conjunction allowed: queries may be constructed as

a conjunction of two (or more) terms.
3. Known hard-to-compute terms:

it is easy to ﬁnd

hard-to-compute (computationally intensive) terms.

4. Early abort: When the search query is a conjunction
of easy-to-compute and hard-to-compute terms, the easy
term(s) are computed ﬁrst. Alternatively, processing is
done on all terms in parallel, or sequentially but in a
known order among terms (e.g., from left to right). Fur-
thermore, once any term(s) return false, then processing
is aborted (returning false) without waiting to complete
the computation of the hard-to-compute terms.
Consider a search query containing the conjunction σ ∧ θ,
where σ is an easily-computable search term and θ is a hard-
to-compute term. From the ‘early abort’ condition, if there
is no record matching σ, then the service will immediately re-
turn a negative response (no matching record). Conversely,
if there is a matching record, i.e., σ resolves to true, then
the service would proceed to evaluate the ‘hard’ term θ, and
return results only after that evaluation is completed. Com-
paring the computation time for σ∧θ versus the computation
time for ¯σ ∧ θ, provides a timing side-channel that allows an
attacker to determine whether the private data-set contains
records matching the σ term. Depending on the web-service
and the search terms it supports, this may allow very precise
and privacy-invasive queries.

We demonstrate the eﬀectiveness of the attack for such
precise resolution of queries over the Gmail email-archive
of the user. Gmail allows third-party (cross-site) queries
and supports complex combinations of a rich set of query
terms. Gmail uses diﬀerent optimizations to eﬃciently eval-
uate most typical search queries. This includes searches for
speciﬁc strings within the message body (using inverted in-
dex) and conditions computed over meta-data ﬁelds such
as sender, recipients (including ‘bcc’ recipients for messages
sent by the user), date, and subject. On the other hand,
Gmail allows queries that are rather complex and may result
in signiﬁcant processing, such as the disjunction of multiple
strings or conjunctions of ‘negative’ (exclude) terms, and the
search for exact strings.

Gmail, like most services, does not disclose their search
algorithms. However, as we demonstrate, attackers can ex-
perimentally validate that the above requirements hold, and
in particular ﬁnd appropriate hard-to-compute terms. For
Gmail, we found that one easy way to construct a ‘hard-to-
compute’ term is using a conjunction of many exclude terms.
4.1 Cross-site Existence Queries on Gmail

To be vulnerable to Compute-inﬂate XS-search attack,
a service must meet the four conditions described above.
Gmail satisﬁes the ﬁrst two conditions: it allows cross-site
queries and supports the conjunction of queries. We now
discuss the other two requirements: known hard-to-compute
terms and early abort. Given an easy-to-compute query σ,
where the attacker wants to ﬁnd whether it matches at least
a single email, the attacker needs to send the query σ ∧ θ,

1400such that θ is hard-to-compute. An easy-to-compute query
is simply a query that asks for a single term. For example,
a query that asks for emails that contain some term or that
were sent to/from some person.

Hard-to-compute query. As the hard-to-compute
query θ, we used the ‘has not’ operator [18] for searching
emails without some list of terms. Speciﬁcally, we used a θ
that asks for emails which do not contain a long list of ran-
dom strings that probably do not appear in a typical email.
Following Gmail’s advanced search restrictions, we used a
list of almost 1000 four-character random strings composed
of characters and letters.

Early-abort and hard-to-compute evaluation. Be-
fore we evaluated the use of many-has not query for the
Compute-inﬂate XS-search attack, we veriﬁed two basic
claims: (1) When appending the has not operator with many
values to a query, it is easier to distinguish between a re-
sponse that contains one message and a response that con-
tains no messages. (2) The use of the has not operator with
many values, makes the distinction easier.

To test the claims, we sent an email from a Gmail ac-
count with very low activity and only few sent emails; in
such an account it is faster to ﬁnd emails. We launched the
attack described in Section 2.1 with three diﬀerent types of
queries: (1) complex search query as described above, (2)
simple search query, and (3) complex search query with one
long has not term composed of the concatenation of all the
has not terms in the ﬁrst complex query.

We took measurements of at least 500 pairs of requests (rC
and rD) of each type in the HN environment. We ﬁrst used
an rC that was replied to with a single email to measure
the FN rate; we then repeated the process with an rC as
a dummy query to measure the FP rate. While using the
simple requests, it was hard to detect a single message in
the response; this was the case even when using a sample
of n = 50 requests pairs (almost 50% FN). With complex
queries, several tests achieved zero false positives and zero
false negatives, even with n = 25. This substantiated the
ﬁrst claim. The results using the one-long-has not queries
were better than the simple search queries, yet far from the
zero FN and FP of the complex query. This conﬁrmed our
second claim, as the responses for both the queries were of
the same length. Repeating the comparison between simple
queries and complex queries on real users gave the same
indication; see below.
4.2 Compute-Inﬂate Attack on Gmail: Eval-

uation

To evaluate the Compute-Inﬂate Attack, we conducted
several experiments for participants using their active Gmail
account. These were designed with IRB approval. We de-
scribe here only the ﬁrst experiment, which tested the ef-
fectiveness of answering a Boolean question about whether
there is some email that contains a speciﬁc sentence. We
also repeated the evaluation we did in Section 4.1 to show
that queries of the form σ ∧ θ, where θ is a hard-to-compute
query (see Section 4.1), are much more eﬀective for learning
about the existence of content than simple queries (σ). We
conducted a similar experiment for detecting a single email
by its recipient or subject, with diﬀerent users, and received
similar results; see [27].

Our content-detection experiment had two goals: (1) show
that using Compute-inﬂate XS-search attack it is possible

Simple queries

n = 25

Complex queries
n = 25
n = 50

n = 50
FN FP FN FP FN FP FN FP
6.2 10.5 2.9
6.5 53.6 9.8 54.3 8.7
8.7 59.4 9.1 60.9 7.2
9.1 15.2 5.1
8
9.1 11.2 7.2
8
8.7 56.5 5.8 62.3 5.8
11.6 7.2
8
50
9.4
7.6
17
5.1
13
17
8
8.7
8
47.8 10.1 35.5 14.5
16.7 8.7 12.3 7.2
11.6 36.2 12.3
42
11.2 15.9 7.2 12.3 32.6 15.9 25.4 18.1

60.5 8.3 65.9

10.1 39.1

4

2

Test
Avg1.02−25
Min1.02
Min1.02
Min1.02
6
MW
KS
BX2
0
BX3
1

Table 2: Analyzing false negative (FN) and false positive
(FP) for Compute-inﬂate XS-search attack, to detect a sen-
tence that appears only once in Gmail mailbox; complex vs.
simple queries. See test descriptions in Section 2.2.

to learn about the existence of a sentence in Gmail account
folders, even if it appears only once, and (2) conﬁrm that
the conjunction with a hard-to-compute query makes the
distinction easier. We conducted the experiment with the
participation of 138 Gmail users; 90 of them were from our
university and 48 were Amazon Mechanical Turk workers.
Each participant ran the experiment from her own com-
puter, using the Internet connection available to her.

During the experiment, we asked the participants to send
an email with a single challenge-sentence: “I like random-
animal” concatenated to some random number. For each
participant, we chose a random animal among 32 options.
The concatenated random number was used to make ab-
solutely sure the sentence appears only once in the tested
Gmail account.

To measure the false negative (FN) rate of the attack, we
sent a sequence of 50 pairs of requests; the ﬁrst request asked
for emails in the Sent Mail folder that contain the challenge-
sentence, and the second asked for a similar sentence that did
not exist. To measure false positive (FP) rate, we repeated
the process, but this time, the ﬁrst request in the pair asked
for another non-existent sentence.

We measured the FN and FP for the attack using simple
Gmail queries and complex queries. The complex queries
were created by appending to the simple query a has not
operator with a list of many terms, as discussed above (see
also Figure 1(b)). We used the tests mentioned in Section 2.2
to analyze the times and to conclude whether the participant
had an email containing the challenge-sentence.

We analyzed the times for samples of n = 50 pairs and for
n = 25 pairs. Table 2 presents the analysis of the measure-
ments according to several tests; the results of all the tests
we measured appear in [27].

The analysis shows that complex queries signiﬁcantly im-
prove the eﬀectiveness of the attack compared to simple
queries. The results show that using 25 pairs of search
queries, it is possible to identify a single email by its con-
tent with reasonable false positive (FP) and false negative
(FN) rates. By increasing the sample size to 50, we further
improved the results. The best results were achieved by the
AVG1.02

-25 test, described in Section 2.2.2.

5. EFFICIENT TERM-IDENTIFICATION

In a term-identiﬁcation query, the attacker has a large set
S containing n potential search terms (e.g., person/location/
project/other names, phone-numbers, credit-card numbers,

1401passwords), and wants to identify which, if any, of the terms
appear in the private data records (many times, in speciﬁc
ﬁelds, or even just in one arbitrary record). In this section,
we present three algorithms that allow a cross-site attacker
to eﬃciently perform term-identiﬁcation queries against the
private data records of the user, kept by a web-service. The
algorithms build upon the single-term query algorithms of
Sections 3 and 4, where the attacker only learns if the pri-
vate data contains records matching one given search term.
A naive term-identiﬁcation method, is to perform n single-
term queries. However, n is often large (e.g., the number
of possible names), i.e., this attack may require the user to
remain connected to the rogue site for unreasonably-long
period, and therefore this naive method may have limited
value in practice.

In contrast, we present three divide and conquer algo-
rithms to eﬃciently run term-identiﬁcation queries, when
given large set of potential terms. In order to apply ‘divide
and conquer’, we assume that the attacked web-service al-
lows search terms that are disjunctions (logical or) of other
terms. Namely, if τ and τ(cid:48) are search terms, then τ ∨ τ(cid:48) is
also a search term. In particular, the Gmail webmail ser-
vice, which we used to evaluate our attacks, supports such
composition of terms (using the OR operator), as well as a
rich set of other search operators [18].

We consider

two variants of

the term-identiﬁcation
queries: multiple-term identiﬁcation queries, whose goal is to
identify all or most search terms that have matching records;
and identify any query, whose goal is to identify just one of
the search terms that have matching records. For example,
the multiple-term identiﬁcation query is appropriate when
searching for the set of correspondents or keywords used in
emails sent/received by the user. The identify any query is
appropriate when searching for a speciﬁc value (e.g., to ﬁnd
out the address the user includes in her ‘signature’ appended
to outgoing emails).

In Subsections 5.1 and 5.2, respectively, we present the
MTI and OMTI algorithms, both for the multiple-term iden-
tiﬁcation queries variant. In Subsection 5.3, we present the
ATI algorithm for the identify any query variant. In Sub-
section 5.5, we experimentally evaluate the algorithms; Note
that when the number of terms is small (e.g., two), the any-
term algorithm can also be used by running it twice. (In the
second run, the item identiﬁed in ﬁrst run is excluded.)

However, all three algorithms may fail, when the attacker
searches for terms from a really huge set such as phone-
numbers, credit-card numbers or passwords.
In subsec-
tion 5.4 we show that often, even in such cases, we can ﬁnd
the relevant terms, by taking advantage of common proper-
ties and formats of the relevant terms.
5.1 The Multiple-Term Identiﬁcation (MTI)

Algorithm

The MTI algorithm (Alg 1) is a simple, generic divide-
and-conquer algorithm, whose goal is to identify all or most
of the terms, within a given set of terms S, that have some
property θ.
In this paper, the relevant property is the
existence of records matching θ in the private user’s data
searched by the web service.
To ﬁnd the terms s ∈ S that have the property θ, i.e.,
θ(s) = True, the MTI algorithm is given access to a noisy,
probabilistic test Tθ. Given a set of terms S(cid:48) ⊆ S, the test T
returns True, with a high probability, if and only if some s ∈

S(cid:48) satisﬁes θ. MTI uses Tθ in a modular ‘black-box’ manner;
it receives Tθ as a parameter. MTI also receives parameter
µ, specifying the maximal number of terms allowed in set
S(cid:48), which MTI gives to Tθ. For example, in Gmail, search
queries are limited to about 8KB.

Speciﬁcally, in the XS-search attacks, the test Tθ sends
challenge requests asking for records that match any of the
terms in S(cid:48), and dummy requests that similarly ask for
records that match |S(cid:48)| dummy values, as described in Sec-
tion 2.1. The test then analyzes the diﬀerence between the
loading times of the responses for both the request types
and decides whether their responses are diﬀerent.

Algorithm 1 Given a set of terms S and test T , algorithm
MTI outputs terms in S that appear in the search results.
Parameter µ is the maximal number of terms per query.

MTI(S, T , µ):

Return ←(cid:83)(cid:100)|S|/µ(cid:101)

(cid:16)(cid:16)∪µ·i−1

(cid:17)

(cid:17)

i=1 MTIr

j=µ(i−1)S[j]

, T

MTIr(X, T ):

if T (X) then

if |X| = 1 then return X.
else
XL ← X [1, . . . ,(cid:100)|X|/2(cid:101)]
XR ← X [(cid:100)|X|/2(cid:101) + 1, . . . ,|X|]
return MTIr(XL, T )∪ MTIr(XR, T )

else

return ∅

Comment: Tθ may err on both sides: outputting a term
that does not appear in the private data (false positive), or
failing to output a term which does appear. We found it
best to use a T that uses only a few samples (for eﬃciency),
with a low threshold to avoid false negatives. To also avoid
false positives, we performed further validation before re-
turning S(cid:48) by running T again on each term. We omitted
the validation from Alg 1.
5.2 Optimized Multiple-Term Identiﬁcation

The MTI algorithm uses the single-term test as a ‘black
box’. This results in a simple modular algorithm, but also
in ineﬃciencies. One example is that each invocation of the
single-term test for a set with y terms, causes the sending of
‘dummy’ requests for y dummy values; this results in sending
the same ‘dummy’ requests several times to test several sets
of terms, instead of using the same ‘dummy’ requests for
testing all of them.

We now describe the main ideas of the OMTI algorithm,
an optimized version of the MTI algorithm. OMTI, unlike
MTI, tests multiple terms together to minimize overhead.
For example, it uses the same ‘dummy’ requests for mul-
tiple tests. OMTI also compares between the times mea-
sured for requests with diﬀerent candidate terms, thereby
providing additional useful indication and reducing the er-
rors due to unusually high or low delays to the dummy re-
quests. Namely, even if the times for some set of terms were
not signiﬁcantly higher than those of the dummy requests,
but were signiﬁcantly higher than another set of candidate
terms, the set passes the test. These changes improve eﬃ-
ciency and accuracy, allowing a smaller number of requests.
To avoid cases in which low measured times for the dummy
requests cause all the sets to pass the test, Tθ has a limit on
the number of sets it can return.

14025.3 Any-Term Identiﬁcation Algorithm

The ATI algorithm improves accuracy and eﬃciency for
the goal of ﬁnding (only) one term in S (that appears in
the private data). For simplicity, we present ATI with the
assumption that at least one of the terms does appear in the
private data; this assumption can be avoided by validating
the identiﬁed term.

Assuming set S contains at least one term that has search
results, facilitating the algorithm’s goal to ﬁnd any value
in S for which the search query has results, allows ATI to
avoid the use of dummy queries. Instead, ATI divides the
set S into two subsets S1 and S2, compares their loading
times against each other, and continues the search with the
slower-loading set. ATI continues recursively until one ele-
ment remains. A simple comparative test suﬃces to choose
the more likely option. We do this by comparing the mini-
mal or average times measured for each of the subsets and
taking the subset with the higher value. This relatively sim-
ple comparison task allows ATI to be eﬀective, even in cases
where distinguishing between two distributions is harder,
and hence MTI and OMTI are less eﬀective. See example
in [27].
5.4 Property-based Term Identiﬁcation

The techniques presented in the previous subsections, are
impractical if the number of potential terms is huge, e.g.,
phone numbers, zip codes, credit-card numbers, bank ac-
count numbers, or PIN/passwords. However, we can often
identify (even) such items, by using special properties of the
items, such as typical phone-number formats or credit-card-
number error detection mechanisms.

Speciﬁcally, to identify such items, we usually take advan-
tage of the fact that these complex items are often entered
in records using a simple, known structure, often as multiple
sets of ﬁxed-length numbers, separated by spaces or other
special symbols. We can therefore search each set of dig-
its separately, and then, check for sets that appear in the
same message and in ‘correct’ order. We give two examples:
phone numbers and credit card numbers.

Phone numbers. Consider a victim who has her phone
number in her email signature, as done by many users. The
attacker can launch an eﬀective XS-search attack relying on
the Response-inﬂate technique, as the phone number ap-
pears in every sent email. This task is even more eﬃcient
since phone numbers have few common formats, and fur-
thermore, they are usually broken into well-deﬁned sets of
three or four digits. The attacker can begin with the area
code, and continue to the next short sequence.

Credit card numbers. Unlike phone numbers, credit card
numbers do not appear many times in Gmail accounts,
hence, the Response-inﬂate technique is less eﬀective here;
however, with minor adjustments, the ATI algorithm worked
ﬁne, and found credit card numbers in the Gmail accounts
of both the authors. Speciﬁcally, we use the fact that credit
card numbers have ﬁxed structures such as 4-4-4-4. Each 4-
digits number has one of 10, 000 options. We slightly modi-
ﬁed ATI algorithm: instead of comparing two sets each time,
we separated the 10, 000 options into 11 sets (following the
length limit of Gmail search request), and instead of continu-
ing to the next round with one of the sets, we continued with
5 of them. We chose to continue with more than 4 options,
as we noticed that sometimes the validity year or a wrong
number are returned instead of one of the 4-digits values.

The output of the algorithm was 5 numbers sorted by their
rank. Given the output, we tried to omit the wrong number
based on credit card’s checksum algorithm. We also exam-
ined carefully every two consecutive numbers and numbers
that might be the validity year of the credit card. Relying
on public information about the preﬁxes and the structure
of credit card numbers, we could reduce the number of op-
tions to 2 possible credit card numbers (which can be easily
determined; see Section 4.1). Notice that we could use this
and additional information about the credit card number
structure to improve the eﬃciency of the attack. See [27]
for more details.
For each sampleSize ∈ {10, 20}, we ran the attack three
times on both the accounts, with an average total time of
10.7 and 20.3 minutes respectively. For sampleSize = 10 we
found all the four numbers only once, three of them in three
runs, and two in two runs. However, we noticed, that many
of the wrong numbers were close to the numbers we missed.
For double sample size and time, we succeeded to ﬁnd the
whole credit card number in all the six runs. More details
appear in [27].
5.5 Evaluation of Term-Identiﬁcation Algo-

rithms

In this subsection we validate and evaluate the term-
identiﬁcation attacks using an experiment with Gmail users.
In the experiment, we tried to expose the ﬁrst and the last
names of the users (details below). See [27] for additional
experiment.

Note that to avoid unnecessary loss of privacy, we limited
this experiment to exposing of the user’s name, and further-
more, before using our attack to ‘guess’ the names, we con-
ducted the experiment only with participants who willingly
disclosed their names. We evaluated the use of the three al-
gorithms for detecting keywords that appear in many email
messages, by identifying the ﬁrst and last name of the user
out of a list of 2000 common names. The attacks used in-
ﬂating search requests as described in Section 3.3; we used
inﬂating requests based on the results of subsection 3.3.

Participants and process. Participants were 78 (paid, in-
formed and consenting) students, required to have and con-
nect to their (active) Gmail account. Participants were
asked to visit the experiment webpage. From this webpage,
we launched the algorithms for diﬀerent small sampleSize
values: 1, 2 and 3 only. Namely, when the test T examined
a set in MTI, it sent sampleSize pairs of challenge (rC ) and
dummy (rD) requests alternated; in OMTI, T sent sample-
Size requests for each tested set. In the ATI algorithm, only
challenge requests were sent.

The challenge search requests (rC ) were for messages in
the Sent Mail folder that were sent from one of the names in
S. Similarly, the dummy requests (rD) asked for messages
that were sent by one of |S| dummy names that are likely
to be unrelated to any Gmail account.

Because ATI returns only one term, we ran it twice to ﬁnd
both the ﬁrst and last names; in the second run we excluded
the term identiﬁed in the ﬁrst run. To make a fair compar-
ison with the other algorithms, we ran OMTI‘s veriﬁcation
step on both the received terms. In the veriﬁcation step of
each of the algorithms, we used sampleSize = 3, regardless
of the sampleSize that was used in the regular run.

As a test for MTI and OMTI, we used the minimum test
). For the OMTI algorithm, to compare be-

(T = MIN1.05

0

1403tween two candidate subsets, we used the test MIN1.1
0 . In
the ATI algorithm, we simply compared the minimal values
of the samples, and continued with the set whose sample
had the highest minimal value.

Videos of the attack using each of the algorithms are avail-

able online in [28].

We measured the following criteria:
1. False negative (FN) counter. For each name that was

not identiﬁed, we increased this counter by 1.

2. False positive (FP) counter. For each name that was
returned by the algorithm but was not the name of the
participant, we increased this counter by 1.

3. Total time. The number of seconds until the test was

completed.

4. Requests counter (RC). The total number of requests

sent during the run.

Table 3 summarizes and compares the results for the three
algorithms. The table’s columns contain the FN and FP for
each conﬁguration, as well as the percentage of runs with-
out any FP and FN (perfect runs), the average time of these
runs, and the average number of queries sent in them. All
the algorithms returned good results. The best values in
each category appear in bold and OMTI is almost always
best. OMTI had good results (low FN and FP) even for
sampleSize=1, with 82.1% of the runs being ‘perfect’ in av-
erage time of less than 40 seconds.

The results show that OMTI improves the MTI algorithm
both in the quality of the results and in the performance.
The only advantage of MTI over OMTI, was in FN rate us-
ing sampleSize=3; in MTI, only for one participant no name
was found (FN counter = 2), compared to 5 participants in
OMTI. It seems that the comparison between several can-
didate sets (ATI compares only 2 sets), is the main reason
that OMTI achieved the best results.

Table 3 also contains the results for each of the ATI runs
separately (without the ﬁnal veriﬁcation step). Their results
were impressive, mainly in the time. For example, to ﬁnd
one name (which might be enough in some scenarios), with
sampleSize=1, we achieved 68% success rate in 12.3 seconds.
For double the sampleSize and time, ATI achieved an 88.5%
success rate.

6. DEFENSES

Server-side defenses. XS-Search attacks use cross-site
search requests. Completely blocking cross-site requests
would ‘break the Web’; defenses should block only cross-site
request forgery (CSRF) attacks. While CSRF is often as-
sociated with ‘causing unwanted action’, i.e., state-changing
requests [29], XS-Search attacks expose information and do
not change state or perform actions at the server. Indeed,
the sites we tested with search queries, Gmail and Bing,
did not apply any of the known anti-CSRF defenses such
as anti-CSRF tokens, challenge-response mechanisms (e.g.,
CAPTCHA), relying on the Referer or the Origin HTTP
headers [2], or other defenses (e.g.,

[11, 21, 29]).

Both Gmail and Bing prevented users from sending an ex-
cessive number of search requests within a short time; the
limit is about 200 in Bing and 4500 in Gmail. These numbers
are too high to disrupt our (eﬃcient) XS-search attacks. A
signiﬁcant reduction in these limits could, of course, reduce
the amount of leakage and potentially reduce some legiti-
mate use of cross-site search requests. In particular, websites

could require users to re-authenticate or solve CAPTCHA
when making unusual cross-site requests (e.g., to history),
or when sending too many cross-site queries. Indeed, Google
requires users to re-authenticate to perform queries on their
search-history log.

Websites can block or limit the eﬀectiveness of Response-
inﬂate XS-search attack, for example by limiting the number
of entries returned for a search query or the length of param-
eters duplicated in the response. It seems more diﬃcult to
prevent Compute-inﬂate XS-search attack, without increas-
ing the response time for benign requests.

Client-side defenses. Several works propose client-side
defenses against CSRF attacks. Appropriately designed
client-side defenses, such as of [11], can prevent XS-search
attacks. These defenses require minimal server-side support;
however, all known client-only proposals may not be usable
due to false positives. Furthermore, some more advanced
client-only defenses such as BEAP [25] would also enable
XS-search attacks, since they do not strip persistent cook-
ies.

7. CONCLUSIONS AND FUTURE WORK
We show how even a weak, cross-site attacker, is often able
to extract sensitive user data, even from prestige services
such as Gmail and Bing, by manipulating the web service,
using side-channels, to expose data.

Our attacks used classical and customized statistical tests,
and tailored search algorithms; further research should ex-
plore improvements to both tests and algorithms. Other
directions for further research include measuring the preva-
lence of the problem, designing automated means for de-
tecting and mitigating such weaknesses (beyond Section 6),
and investigating social and legal aspects such as liability to
damages due to cross-site side-channel weaknesses.

8. ACKNOWLEDGMENTS

We would like to thank Amit Klein, Alexei Czeskis, Hemi
Leibowitz, Google Security team and the anonymous review-
ers for useful feedbacks. This research was supported by a
grant from the Ministry of Science and Technology, Israel.

9. REFERENCES
[1] Apache-Commons. Commons Math: The Apache

Commons Mathematics Library . online.

[2] A. Barth, C. Jackson, and J. C. Mitchell. Robust

defenses for cross-site request forgery. In Proceedings
of the 15th ACM conference on Computer and
communications security, pages 75–88. ACM, 2008.

[3] D. Bleichenbacher. Chosen ciphertext attacks against

protocols based on the RSA encryption standard
PKCS #1. In Advances in Cryptology – Crypto 1998,
volume 1462 of Lecture Notes in Computer Science,
pages 1–12, 1998.

[4] A. Bortz and D. Boneh. Exposing private information

by timing web applications. In C. L. Williamson,
M. E. Zurko, P. F. Patel-Schneider, and P. J. Shenoy,
editors, WWW, pages 621–628. ACM, 2007.

[5] C. B¨osch, P. H. Hartel, W. Jonker, and A. Peter. A

survey of provably secure searchable encryption. ACM
Comput. Surv, 47(2):18, 2014.

[6] D. Brumley and D. Boneh. Remote timing attacks are

practical. Computer Networks, 48(5):701–716, 2005.

1404MTI
2

Second ATI run
First ATI run
Algorithm:
2
1
1
sampleSize:
9
32.1 11.5 7.7 43.6
FN counter = 1
−
−
−
FN counter = 2
9
32.1 11.5 7.7 43.6
FP counter = 1
−
−
−
FP counter = 2
67.9 88.5 92.3 56.4
91
Perfect counter
34.6
Perfect: time (sec) 52.2
12.3 24.2 36.5 12.2 24.2
Perfect: avg RC 122.1 207.6 282.6 81.6 130.9 181.1 52.1 95.4 138.6 21.6 43.1 64.8 21.6 43.2

ATI twice
3
3
7.7
2.6
5.1
6.4
6.4
2.6
1.3
0
91
87.2
85.7 28.2 51.4 72.4

3
7.7
−
7.7
−
92.3
34.8
64.7

2
9
7.7
7.7
0

46.2
15.4
11.5

38.5 83.3

2
−
−

1

3

17.9

2
14.1 11.5 5.1
5.1
1.3
7.7
1.3
1.3
0
79.5 82.1 89.7
62.5

9
5.1
1.3
69.2
94.2 130.9 39.8

6.4
7.7
1.3

3
−
−

39.7
21.8

1

9
0

OMTI

1

0

Table 3: Finding two names out of 2000 options by Response-inﬂate XS-search attack with MTI and OMTI, and by running
ATI twice. In bold are the best values per sampleSize. The maximal value for the FN and FP counters in some run, was 2.

[7] S. Chen, R. Wang, X. Wang, and K. Zhang.

Side-channel leaks in web applications: a reality today,
a challenge tomorrow. In Security and Privacy (SP),
2010 IEEE Symposium on, pages 191–206. IEEE,
2010.

[8] B. Chor, E. Kushilevitz, O. Goldreich, and M. Sudan.

Private information retrieval. Journal of the ACM
(JACM), 45(6):965–981, 1998.

[19] Google. Standard view and basic html view. https://
support.google.com/mail/answer/15049?ctx=gmail.

[20] M. Johns, S. Lekies, and B. Stock. Eradicating DNS
rebinding with the extended Same-Origin Policy. In
USENIX Security, pages 621–636, 2013.

[21] N. Jovanovic, E. Kirda, and C. Kruegel. Preventing

cross site request forgery attacks. In Securecomm and
Workshops, 2006, pages 1–10. IEEE, 2006.

[9] J. Clarke. SQL injection attacks and defense. Elsevier,

[22] ¸C. K. Ko¸c. About cryptographic engineering. In ¸C. K.

2012.

[10] S. A. Crosby, D. S. Wallach, and R. H. Riedi.

Opportunities and limits of remote timing attacks.
ACM Transactions on Information and System
Security (TISSEC), 12(3):17, 2009.

[11] A. Czeskis, A. Moshchuk, T. Kohno, and H. J. Wang.

Lightweight server support for browser-based csrf
protection. In Proceedings of the 22nd international
conference on World Wide Web, pages 273–284.
International World Wide Web Conferences Steering
Committee, 2013.

[12] K. P. Dyer, S. E. Coull, T. Ristenpart, and

T. Shrimpton. Peek-a-boo, I still see you: Why
eﬃcient traﬃc analysis countermeasures fail. In IEEE
Symposium on Security and Privacy, pages 332–346.
IEEE Computer Society, 2012.

[13] C. Evans. Cross-domain search timing. blog,

December 2009.
http://scarybeastsecurity.blogspot.co.il/2009/12/cross-
domain-search-timing.html.

[14] E. W. Felten and M. A. Schneider. Timing attacks on

web privacy. In D. Gritzalis, S. Jajodia, and
P. Samarati, editors, ACM Conference on Computer
and Communications Security, pages 25–32. ACM,
2000.

[15] A. Futoransky, D. Saura, and A. Waissbein. The

ND2DB attack: Database content extraction using
timing attacks on the indexing algorithms. In
D. Boneh, T. Garﬁnkel, and D. Song, editors, WOOT.
USENIX Association, 2007.

[16] Y. Gilad and A. Herzberg. Spying in the Dark: TCP

and Tor Traﬃc Analysis. In S. Fischer-H¨ubner and
M. Wright, editors, Privacy Enhancing Technologies
Symposium, volume 7384 of Lecture Notes in
Computer Science, pages 100–119. Springer, 2012.

[17] O. Goldreich and R. Ostrovsky. Software protection

and simulation on oblivious rams. Journal of the ACM
(JACM), 43(3):431–473, 1996.

[18] Google. Advanced search. https:

//support.google.com/mail/answer/7190?hl=en.

Ko¸c, editor, Cryptographic Engineering, pages 1–4.
Springer, 2009.

[23] P. C. Kocher. Timing Attacks on Implementations of

Diﬃe-Hellman, RSA, DSS, and Other Systems. In
N. Koblitz, editor, CRYPTO’96, volume 1109 of
LNCS, pages 104–113. IACR, Springer-Verlag,
Germany, 1996.

[24] E. L. Lehmann and J. P. Romano. Testing statistical

hypotheses. springer, 2006.

[25] Z. Mao, N. Li, and I. Molloy. Defeating cross-site

request forgery attacks with browser-enforced
authenticity protection. In Financial Cryptography
and Data Security, pages 238–255. Springer, 2009.

[26] Microsoft. Internet Explorer Dev Center - Timing and

Performance APIs. http://msdn.microsoft.com/
en-us/library/ie/hh772738(v=vs.85).aspx.

[27] Nethanel Gelernter and Amir Herzberg . Cross-Site

Search Attacks, technical report 15-01. http://u.cs.
biu.ac.il/~herzbea/security/15-01-XSSearch.pdf,
August 2015.

[28] Nethanel Gelernter and Amir Herzberg. Demo:

XS-Search attack on Gmail, May 2015. Online at
http://u.cs.biu.ac.il/~gelernn/xssearch/.
[29] Paul Peteﬁsh, Eric Sheridan, and Dave Wichers.

Cross-Site Request Forgery (CSRF) Prevention Cheat
Sheet.
https://www.owasp.org/index.php/Cross-Site_
Request_Forgery_(CSRF)_Prevention_Cheat_Sheet.

[30] A. Shamir. A top view of side channel attacks. In

Proc. of L-SEC/CALIT IT Security Congress
(October 19-20, 2006), 2011.

[31] Z. Weinberg, E. Y. Chen, P. R. Jayaraman, and
C. Jackson. I Still Know What You Visited Last
Summer: Leaking Browsing History via User
Interaction and Side Channel Attacks. In IEEE
Symposium on Security and Privacy, pages 147–161.
IEEE Computer Society, 2011.

1405