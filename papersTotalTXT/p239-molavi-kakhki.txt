Identifying Trafﬁc Differentiation in Mobile Networks

Arash Molavi Kakhki
Northeastern University

arash@ccs.neu.edu

Abbas Razaghpanah
Stony Brook University

arazaghpanah@cs.stonybrook.edu

Anke Li

Stony Brook University

ankeli@cs.stonybrook.edu

Hyungjoon Koo

Stony Brook University

hykoo@cs.stonybrook.edu

Rajesh Golani

Stony Brook University

rgolani@cs.stonybrook.edu

David Choffnes

Northeastern University
choffnes@ccs.neu.edu

Phillipa Gill

Stony Brook University

phillipa@cs.stonybrook.edu

Abstract
Traﬃc diﬀerentiation—giving better (or worse) performance
to certain classes of Internet traﬃc—is a well-known but
poorly understood traﬃc management policy. There is ac-
tive discussion on whether and how ISPs should be allowed
to diﬀerentiate Internet traﬃc [8, 21], but little data about
current practices to inform this discussion. Previous work
attempted to address this problem for ﬁxed line networks;
however, there is currently no solution that works in the
more challenging mobile environment.

In this paper, we present the design, implementation, and
evaluation of the ﬁrst system and mobile app for identify-
ing traﬃc diﬀerentiation for arbitrary applications in the
mobile environment (i.e., wireless networks such as cellular
and WiFi, used by smartphones and tablets). The key idea
is to use a VPN proxy to record and replay the network traf-
ﬁc generated by arbitrary applications, and compare it with
the network behavior when replaying this traﬃc outside of
an encrypted tunnel. We perform the ﬁrst known testbed
experiments with actual commercial shaping devices to vali-
date our system design and demonstrate how it outperforms
previous work for detecting diﬀerentiation. We released our
app and collected diﬀerentiation results from 12 ISPs in 5
countries. We ﬁnd that diﬀerentiation tends to aﬀect TCP
traﬃc (reducing rates by up to 60%) and that interference
from middleboxes (including video-transcoding devices) is
pervasive. By exposing such behavior, we hope to improve
transparency for users and help inform future policies.

Categories and Subject Descriptors
C.2.2 [Computer-Communication Networks]: Network
Protocols

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
IMC’15, October 28–30, 2015, Tokyo, Japan.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3848-6/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2815675.2815691.

Alan Mislove

Northeastern University
amislove@ccs.neu.edu

Keywords
Traﬃc diﬀerentiation; mobile networks; network neutrality

1.

INTRODUCTION

The rise in popularity of bandwidth-hungry applications
(e.g., Netﬂix) coupled with resource-constrained networks
(e.g., mobile providers) has reignited discussions about
how the diﬀerent applications’ network traﬃc is treated
(or mistreated) by ISPs. One commonly discussed ap-
proach to managing scarce network resources is traﬃc
diﬀerentiation—giving better (or worse) performance to cer-
tain classes of network traﬃc—using devices that selectively
act on network traﬃc (e.g., Sandvine [25], Cisco, and oth-
ers). Diﬀerentiation can enforce policies ranging from pro-
tecting the network from bandwidth-hungry applications, to
limiting services that compete with those oﬀered by the net-
work provider (e.g., providers that sell voice/video services
in addition to IP connectivity).

Recent events have raised the proﬁle of traﬃc diﬀerentia-
tion and network neutrality issues in the wired setting, with
Comcast and Netﬂix engaged in public disputes over conges-
tion on network interconnects [11, 15, 18]. The situation is
more extreme in the mobile setting, where historically reg-
ulatory agencies imposed few restrictions on how a mobile
provider manages its network, with subscribers and regula-
tors generally having a limited (or no) ability to understand
and hold providers accountable for management polices.1

Despite the importance of this issue, the impacts of traf-
ﬁc diﬀerentiation on speciﬁc applications—and the Inter-
net as a whole—are poorly understood. Previous eﬀorts
attempt to detect traﬃc diﬀerentiation (e.g.,
[6, 30, 36, 37]),
but are limited to speciﬁc types of application traﬃc (e.g.,
peer-to-peer traﬃc [6]), or focus on shaping behaviors on
general classes of traﬃc as opposed to speciﬁc applications
[13, 30, 37]. Addressing these limitations is challenging, due
to the wide variety of applications (often closed-source) that
users may wish to test for diﬀerentiation, coupled with the
closed nature of traﬃc shapers. As a result, external ob-
servers struggle to detect if such shapers exist in networks,

1As we discuss throughout this paper, FCC rules that took
eﬀect in June 2015 [9] now prohibit many forms of traﬃc
diﬀerentiation in the US.

239what exactly these devices do, and what is their impact on
traﬃc. What little we do know is concerning. Glasnost [6]
and others [26, 30, 36] found ISPs throttling speciﬁc types of
Internet traﬃc, Cogent admitted to diﬀerentiating Netﬂix
traﬃc [23], and recent work by Google and T-Mobile [12] in-
dicated that unintentional interactions between traﬃc shap-
ing devices has been known to degrade performance.

This paper tackles three key challenges that have held
back prior work on measuring traﬃc diﬀerentiation:
(1)
the inability to test arbitrary classes of applications, (2) a
lack of understanding of how traﬃc shaping devices work in
practice, and (3) the limited ability to measure mobile net-
works (e.g., cellular and WiFi) from end-user devices such as
smartphones and tablets. Designing methods that can mea-
sure traﬃc diﬀerentiation in mobile networks (in addition to
well-studied wired networks) presents unique challenges, as
the approaches must work with highly variable underlying
network performance and within the constraints of mobile
operating systems. By addressing these challenges, we can
provide tools that empower average users to identify diﬀer-
entiation in mobile networks, and use data gathered from
these tools to understand diﬀerentiation in practice.
This paper makes the following key contributions:

• The design and implementation of a system for
detecting traﬃc diﬀerentiation in mobile net-
works (Sec. 3). We design an application-layer trace
record-and-replay system that enables testing of arbi-
trary network applications, and works even if these appli-
cations are closed source. Our solution does not require
special privileges on client devices, making it readily de-
ployable on a global scale. The key idea is to use a VPN
proxy to record arbitrary application traﬃc, and later
replay that traﬃc both with and without the VPN to
identify diﬀerentiation. We implement this both as an
Android and desktop application.
• Validation of our approach using commercial
shaping devices (Sec. 4). To the best of our knowl-
edge, we are the ﬁrst study to validate that our detection
system correctly triggers diﬀerentiation on commercial
shaping devices. Using a testbed consisting of two such
devices, we establish a ground truth to evaluate the ac-
curacy of our approach and identify several pitfalls of
previous approaches.
• Evaluating statistical techniques for identifying
diﬀerentiation (Sec. 5). Previous work uses a variety
of statistical techniques to determine when diﬀerentia-
tion occurs. We use our testbed with commercial shaping
products (with conﬁgurable shaping rates), a controlled
network with no diﬀerentiation, and controlled loss with
Linux Traﬃc Control (tc) to systematically evaluate the
accuracy of diﬀerent statistical techniques for identifying
diﬀerentiation.
• An Android app that conducts our diﬀerentiation
tests and a measurement study of diﬀerentiation
in mobile networks (Sec. 6). We develop an Android
app that conducts our diﬀerentiation tests on unmod-
iﬁed Android OSes, as part of an IRB-approved study
of diﬀerentiation in mobile networks worldwide. We de-
ploy our software in the US and four other countries, and
ﬁnd several instances of shaping, in addition to pervasive
interference from middleboxes.

In addition to being the ﬁrst approach to reliably de-
tect diﬀerentiation from mobile devices without special
privileges, our study also identiﬁes several instances of
middleboxes violating end-to-end principles for a variety
of popular apps. In particular, we ﬁnd evidence of video
transcoding, HTTP header manipulation in requests and
responses, and proxies that modify TCP behavior. While
previous work identiﬁed such behavior for Web traﬃc, we
are the ﬁrst to do so for arbitrary app traﬃc.

The following section discusses related work and deﬁnes
diﬀerentiation that we detect. After detailing our key con-
tributions listed above, we discuss issues with diﬀerentiation
and its detection that are beyond the scope of this work
(Sec. 7), and we conclude in Sec. 8.

2. BACKGROUND

There is an ongoing debate about whether the network
should be neutral with respect to the packets it carries [4];
i.e., not discriminate against or otherwise alter traﬃc except
for lawful purposes (e.g., blocking illegal content). Propo-
nents such as President Obama argue for an open Inter-
net [21] as essential to its continued success, while some
ISPs argue for shaping to manage network resources [6, 26].
The regulatory framework surrounding traﬃc diﬀerentia-
tion is rapidly changing; between submission and publication
of this manuscript, the FCC transitioned from imposing few
restrictions on how mobile providers manage their network,
to new rules that prohibit many forms of diﬀerentiation [9].
In this environment, it is important to monitor traﬃc diﬀer-
entiation in practice, both to inform regulators and enforce
policies.

In the remainder of this section, we overview related work
on measuring traﬃc diﬀerentiation and network neutrality
and then deﬁne the types of traﬃc diﬀerentiation our system
aims to detect.
2.1 Related Work

Comcast’s blocking of BitTorrent [28] in the mid 2000s
spurred research eﬀorts to study traﬃc diﬀerentiation. This
led to several studies that focus on applications or ﬂows
known to be subject to classiﬁcation, and design tests to
detect diﬀerentiation on those ﬂows. For example, Glasnost
focuses on BitTorrent [6] and NetPolice [36] tests ﬁve ap-
plications (HTTP, BitTorrent, SMTP, PPLive and VoIP).
Bonaﬁde [3] takes a similar approach to Glasnost in the
mobile environment, including tests for HTTP, FlashVideo
(YouTube), SIP, RTSP, BitTorrent, and VoIP H323. These
approaches focus on protocols rather than speciﬁc applica-
tion implementations. A key limitation is that the shaping
devices we observed support application/provider granular-
ity, e.g., shape RTSP traﬃc for one content provider, but
not other RTSP applications (see Sec. 4). Such application-
based diﬀerentiation may not be detected by these systems.
They also require manually implementing protocol details
and thus have limited extensibility. We address these limi-
tations in this work.

Furthermore, these projects treat traﬃc diﬀerentiation as
a “black box” which they aim to detect by sending traﬃc
and observing network performance metrics. Notably, these
prior studies lack ground-truth information about how dif-
ferentiation is implemented, and whether or not their de-
tection mechanisms would trigger traﬃc shaping in practice
(an issue we address in this work).

240Another approach avoids testing speciﬁc applications and
focuses on detecting diﬀerentiation for arbitrary traﬃc.
NANO [30] uses Bayesian analysis to detect diﬀerentiation,
while Zhang et al. [37] focus on identifying when it is feasible
to detect and isolate diﬀerentiation. Other related projects
focus on performance issues in ISPs [7, 16, 18, 19] and shap-
ing of all of a subscriber’s traﬃc [13], but do not detect
diﬀerentiation itself.

Other related studies detect proxies on network paths, and
conduct tests to identify a wide range of application-speciﬁc
proxy behaviors [2, 14, 29, 32, 33]. A recent paper [31] specif-
ically focuses on proxies and other middleboxes in mobile
networks, and discusses how these devices vary according to
mobile virtual network operator (MVNO). Zarinni et al. use
Bonaﬁde [3] to test for diﬀerentiation of BitTorrent, VoIP-
H323 and RTSP [35] in MVNOs. The authors did not ob-
serve diﬀerentiation of this limited set of applications; in
contrast, our approach identiﬁed diﬀerentiation in three of
the networks they tested.
2.2 Differentiation considered in our work

In this work, we consider the problem of detecting dif-
ferentiation caused by traﬃc-shaping middleboxes (rather
than interconnection provisioning [11, 15, 18]). We focus on
the mobile environment, in large part due to minimal regu-
lation and scarce network resources in mobile data networks.
However, our techniques also work in the ﬁxed-line environ-
ment.

We focus on diﬀerentiation aﬀecting network performance
as perceived by applications. In our experiments, we observe
that even high-bandwidth mobile applications such as video
streaming do not necessarily exhaust the bandwidth avail-
able, possibly to avoid wasting data transfer in case of viewer
abandonment. This is in contrast to protocols that previ-
ous approaches focused on (e.g., BitTorrent), which aim to
saturate the link bandwidth. For applications that do not
exhaust the available resources, we do not consider traﬃc
shaping where the shaping rate is higher than the bandwidth
demands to be diﬀerentiation. For example, if a shaper lim-
ited a given application to 200 KB/s, but the application
never sent faster than 150 KB/s, we would not consider this
diﬀerentiation (as the application would never be aﬀected
by it). This more conservative approach gives us a clearer
picture of shaping that actually impinges on the network
resource demands of applications.

We make the assumption that traﬃc diﬀerentiation is
likely triggered by at least one of the following factors: IP
addresses, port numbers, payload signatures, number of con-
nections, total bandwidth, and time-of-day.

We conﬁrmed these assumptions are consistent with fea-
tures listed in online manuals of deep packet inspection
(DPI) devices and traﬃc shapers. We also run experiments
with two commercial packet shapers and show that our as-
sumptions hold for these devices (Sec. 4.2).

In this work, there are certain types of traf-
Nongoals.
ﬁc management policies that we are not able to detect.
First, we do not focus on detecting congestion at peering
points, a tactic used by certain ISPs to extract payment
from large bandwidth consumers [11]. Second, we do not
focus on blocking (e.g., censorship) or content modiﬁcation
(e.g., transcoding) by middleboxes. While our methodology
is able detect these types (and we found cases of both), they
are orthogonal to the issue of shaping. Third, we currently

(a) Record

(b) Parse

(c) Replay

(d) Analyze

Figure 1: System overview: (a) Client connects to VPN, that
records traﬃc between the client and app server(s). (b) Parser
produces transcripts for replaying. (c) Client and replay server
use transcripts to replay the trace, once in plaintext and once
through a VPN tunnel. Replay server records packet traces for
each replay. (d) Analyzer uses the traces to detect diﬀerentiation.

do not support detecting diﬀerentiation based on the desti-
nation IP address contacted by clients. Our analysis of com-
mercially deployed traﬃc shapers indicates that IP addresses
are not commonly used to identify applications, since none
of our observed conﬁgurations use them for shaping. We
speculate this is due to the fact that many applications con-
tact servers with IPs shared by many services (e.g., EC2,
or Google frontends that serve both search and YouTube
traﬃc), and the IPs used by any service may change over
time. Thus, IP addresses are a poor classiﬁer for shaping a
speciﬁc application. Regardless, we are investigating how to
use limited forms of IP spooﬁng to address this limitation.

3. METHODOLOGY

We use a trace record-replay methodology to reliably
detect traﬃc diﬀerentiation for arbitrary applications in
the mobile environment. We provide an overview in Fig-
ure 1. We record a packet trace from a target application
(Fig. 1(a)), extract bidirectional application-layer payloads,
and use this to generate a transcript of messages for the
client and server to replay (Fig. 1(b)). To test for diﬀerenti-
ation, the client and server coordinate to replay these tran-
scripts, both in plaintext and through an encrypted channel
(using a VPN tunnel) where payloads are hidden from any
shapers on the path (Fig. 1(c)). Finally, we use validated
statistical tests to identify whether the application traﬃc
being tested was subject to diﬀerentiation (Fig. 1(d)). We
discuss each of these steps below.

2413.1 Recording the trace

Our method requires a packet trace to replay. While it
is straightforward to gather one on desktop operating sys-
tems, recording packet traces on today’s mobile operating
systems requires “rooting” and/or “jailbreaking” the phone
(potentially voiding the phone’s warranty). This is not
a problem for small-scale testbed experiments, but traﬃc
diﬀerentiation—and the applications it aﬀects—is a mov-
ing target that requires running experiments for a variety of
applications on a large set of devices in diﬀerent networks
worldwide. Thus, we need a way to enable end users to cap-
ture traces of mobile application traﬃc without requiring
modiﬁcations to the OS.

To address this challenge, we leverage the fact that all
major mobile operating systems natively support VPN con-
nections, and use the Meddle VPN [22] to facilitate trace
recording. Figure 1(a) illustrates how a client connects to
the Meddle VPN, which relays traﬃc between the client and
destination server(s). In addition, the Meddle VPN collects
packet traces that we use to generate replay transcripts.
When possible, we record traﬃc using the same network
being tested for diﬀerentiation because applications may be-
have diﬀerently based on the network type, e.g., a streaming
video app may select a higher bitrate on WiFi.

Our methodology is extensible to arbitrary applications—
even closed source and proprietary ones—to ensure it works
even as the landscape of popular applications, the network
protocols they use, and their impact on mobile network re-
sources changes over time. Prior work focused on speciﬁc
applications (e.g., BitTorrent [6]) and manually emulated
application ﬂows. This approach, however, does not scale
to large numbers of applications and may even be infeasible
when the target application uses proprietary, or otherwise
closed, protocols.2
3.2 Creating the replay script

After recording a packet trace, our system processes it to
extract the application-layer payloads as client-to-server and
server-to-client byte streams. We then create a replay tran-
script that captures the behavior of the application’s traﬃc,
including port numbers, dependencies between application-
layer messages (if they exist) and any timing properties of
the application (e.g., ﬁxed-rate multimedia).3 Finally, we
use each endpoint’s TCP or UDP stack to replay the traﬃc
as speciﬁed in the transcript. Figure 1(b) shows how we cre-
ate two objects with the necessary information for the client
and server to replay the traﬃc from the recorded trace.

Logical dependencies.
For TCP traﬃc, we preserve
application-layer dependencies using the implicit happens-
before relationship exposed by application-layer communica-
tion in TCP. More speciﬁcally, we extract two unidirectional
byte streams sAB and sBA for each pair of communicating
hosts A and B in the recorded trace. For each sequence of
bytes in sAB, we identify the bytes in sBA that preceded
them in the trace. When replaying, host A sends bytes in
sAB to host B only after it has received the preceding bytes

2The Glasnost paper describes an (unevaluated) tool to sup-
port replaying of arbitrary applications but we were unsuc-
cessful in using it, even with the help of a Glasnost coauthor.
3This approach is similar to Cui et al.’s [5]; however, our
approach perfectly preserves application payloads (to ensure
traﬃc is classiﬁed by shapers) and timing (to prevent false
positives when detecting shaping).

in sBA from host B. We enforce analogous constraints from
B to A. For UDP traﬃc, we do not enforce logical dependen-
cies because the transport layer does explicitly not impose
them.

Timing dependencies. A second challenge is ensuring
the replayed traﬃc maintains the inter-message timing fea-
tures of the initial trace. For example, streaming video apps
commonly download and buﬀer video content in chunks in-
stead of buﬀering the entire video, typically to reduce data-
consumption for viewer abandonment and to save energy by
allowing the radio to sleep between bursts. Other content
servers use packet pacing to minimize packet loss and thus
improve ﬂow completion times.

Our system preserves the timings between packets for
both TCP and UDP traﬃc to capture such behavior when
replaying (this feature can be disabled when timing is not
intrinsic to the application). Preserving timing is a key fea-
ture of our approach, that can have a signiﬁcant impact
on detecting diﬀerentiation, as discussed in Section 2.2. In
short, it prevents us from detecting shaping that does not
impact application-perceived performance.

Speciﬁcally, for each stream of bytes (UDP or TCP) sent
by a host, A, we annotate each byte with time oﬀset from
the ﬁrst byte in the stream in the recorded trace. If the ith
byte of A was sent at t ms from the start of the trace, we
ensure that byte i is not sent before t ms have elapsed in the
replay. For TCP, this constraint is enforced after enforcing
the happens-before relationship.

In the case of UDP, we do not retransmit lost packets.
This closely matches the behavior of real-time applications
such as VoIP and live-streaming video (which often toler-
ate packet losses), but does not work well for more compli-
cated protocols such as BitTorrent’s µTP [20] or Google’s
QUIC [1]. We will investigate how to incorporate this be-
havior in future work.
3.3 Replaying the trace

After steps 1 and 2, we replay the recorded traﬃc on a
target network to test our null hypothesis that there is no
diﬀerentiation in the network. This test requires two sam-
ples of network performance: one that exposes the replay to
diﬀerentiation (if any), and a control that is not exposed to
diﬀerentiation.

A key challenge is how to conduct the control trial. Ini-
tially, we followed prior work [6] and randomized the port
numbers and payload, while maintaining all other features of
the replayed traﬃc. However, using our testbed containing
commercial traﬃc shapers (Section 4.2), we found that some
shapers will by default label “random” traﬃc with high port
numbers as peer-to-peer traﬃc, a common target of diﬀeren-
tiation. Thus, this approach is unreliable to generate control
trials because one can reasonably expect it to be shaped.

Instead, we re-use the Meddle VPN tunnel to conduct
control trials. By sending all recorded traﬃc over an en-
crypted IPSec tunnel, we preserve all application behavior
while simultaneously preventing any DPI-based shaper
from diﬀerentiating based on payload. Thus, each replay
test consists of replaying the trace twice, once in plaintext
(exposed trial), and once over the VPN (control trial),
depicted in Figure 1(c). To detect diﬀerentiation in noisy
environments, we run multiple back-to-back tests (both
control and exposed). Note that we compare exposed and
control trials with each other and not the original recorded

242(a) YouTube

(b) Skype

Figure 2: Plots showing bytes transferred over time, with and
without preserving packet timings for TCP (YouTube) and UDP
(Skype) applications. By preserving inter-packet timings, our re-
play closely resembles the original traﬃc generated by each app.

trace. We explore potential issues of using a VPN as a
control in Sections 4.3 and 7.
3.4 Detecting differentiation

After running the exposed and control trials, we compare
performance metrics (throughput, loss, and delay) to de-
tect diﬀerentiation (Fig. 1(d)). We focus on these metrics
because traﬃc shaping policies typically involve bandwidth
rate-limiting (reﬂected in throughput diﬀerences), and may
have impacts on delay and loss depending on the queu-
ing/shaping discipline. We base our analysis on server-side
packet captures to compute throughput and RTT values
for TCP, because we cannot rely on collecting network-level
traces on mobile clients. For UDP applications, we use jit-
ter as a delay metric and measure it at the client at the
application layer (observing inter-packet timings).

A key challenge is how to automatically detect when dif-
ferences between two traces are caused by diﬀerentiation,
instead of signal strength, congestion, or other confounding
factors.
In Section 5, we ﬁnd that previous techniques to
identify diﬀerentiation are inaccurate when tested against
commercial packet shaping devices with varying packet loss
in our testbed. We describe a novel area test approach to
compare traces that has perfect accuracy with no loss, and
greater than 70% accuracy under high loss (Fig. 9).

4. VALIDATION

We validate our methodology using our replay system
and a testbed comprised of two commercial shaping devices.
First, we verify that our approach captures salient features
of the recorded traﬃc. Next, we validate that our replays
trigger diﬀerentiation and identify the relevant features used
for classiﬁcation. To the best of our knowledge, this is ﬁrst
study to use commercial shapers to validate diﬀerentiation
detection. Finally, we discuss potential overheads of using a
VPN connection as a control and how we mitigate them.

Figure 3: Our testbed for testing our diﬀerentiation detector.

4.1 Record/Replay similarity

Our replay script replicates the original trace, including
payloads, port numbers, and inter-packet timing. We now
show that the replay traﬃc characteristics are essentially
identical to the recorded traﬃc. Figure 2 shows the results
for a TCP application (YouTube (a)) and a UDP application
(Skype (b)). As discussed above, preserving inter-packet
timings is important to produce a replay that closely resem-
bles the original trace (otherwise, we may claim that diﬀer-
entiation exists when the application would never experience
it).

Figure 2(a) shows that our replay captures the behavior
of the application, presenting the cumulative transfer over
time for a YouTube trace collected and replayed over the
Verizon mobile network. The ﬁgure shows the behavior for
the original trace and two replays, one which preserves the
inter-packet timing (overlaps with original) and one which
transfers as fast as possible while preserving application-
layer dependencies. Preserving inter-packet timings results
in a replay that closely follows the recorded application be-
havior. Figure 2(b) shows similar results for Skype traﬃc.
4.2 Trafﬁc shapers detect replayed trafﬁc

We now validate that our replay traﬃc is properly classi-
ﬁed for diﬀerentiation using commercial shaping products.
We acquired traﬃc shaping products from two diﬀerent
vendors and integrated them into a testbed for validating
whether replays trigger diﬀerentiation (Fig. 3). The testbed
consists of a client connected to a router that sits behind a
traﬃc shaper, which exchanges traﬃc with a gateway server
that we control. The gateway presents the illusion (to the
packet shaper) that it routes traﬃc to and from the public
Internet. We conﬁgure the replay server to listen on arbi-
trary IP addresses on the gateway, giving us the ability to
preserve original server IP addresses in replays (by spooﬁng
them inside our testbed). We describe below some of our
key ﬁndings from this testbed.

Diﬀerentiation testing must be extensible. One de-
vice lists more than 700 applications that it uniquely iden-
tiﬁes, the other lists approximately 2,000 application ﬁlters.
Further, both devices routinely update their classiﬁcation
rules, on timescales of months (if not shorter). Thus, testing
only a small number of applications is insuﬃcient. By allow-
ing users to create their own traces to conduct diﬀerentiation
tests, our approach is extensible to evolving diﬀerentiation
targets.

For replays to
Our replays trigger traﬃc shaping.
be eﬀective, they need to “look” like legitimate application
traﬃc from the perspective of the traﬃc shaper, i.e., re-
play traﬃc should be classiﬁed as the application that was
recorded. We validate this for a variety of popular mobile
applications: YouTube, Netﬂix, Skype, Spotify, and Google
Hangouts. Figure 4 shows the YouTube and P2P policies on
the shaper applied to our replay as we vary the application
payload.

 0 5 10 15 20 25 30 35 0 1 2 3 4 5 6Cumulative transfer(Mbits)â€(cid:157)Time (s)OriginalReplay with timingReplay no timing 0 1 2 3 4 5 6 7 8 0 10 20 30 40 50 60Cumulative transfer(Mbits)â€(cid:157)Time (s)OriginalReplay with timingReplay no timing243Row Changes in traﬃc

1
2
3
4
5
6
7
8

No changes
Added a packet with 1 byte of data to the beginning of traﬃc
Added one byte of random data to the beginning of ﬁrst packet
Replaced “GET” with a random string (same size)
Replaced “youtube” string with a random one (ﬁrst packet only)
Replaced “youtube” string with a random one (ﬁrst packet, HOST header only)
Added one byte of random data to the end of ﬁrst packet
Added “GET ” to beginning of ﬁrst packet

Detection result using:

Original ports

Diﬀerent ports

YouTube

YouTube

HTTP
HTTP
HTTP
HTTP

YouTube
YouTube
YouTube

P2P
P2P
P2P
P2P

YouTube
YouTube
YouTube

Table 1: Eﬀect of diﬀerent parameters on YouTube traﬃc detection for a popular commercial shaping device. IP addresses do not aﬀect
traﬃc classiﬁcation, but ports and payloads have eﬀects that vary.

• Server IP addresses do not aﬀect classiﬁcation.
While our shapers support IP-based policies, we found
no evidence of IP-based classiﬁcation rules.

• Non-standard ports may still be subject to diﬀer-
entiation. An important outcome of our testing is that
traﬃc with random high port numbers may be classiﬁed
as P2P. Glasnost’s [6] detection approach (among oth-
ers) assumes that traﬃc sent on random ports will not
be subject to diﬀerentiation, but traﬃc sent on standard
ports will. However, our shaping device classiﬁes traﬃc
on random ports as P2P, which itself is often subject to
diﬀerentiation.

• Traﬃc shaping decisions are made early. For
HTTP requests, the ﬁrst packet with payload, i.e., re-
quest packet from client, is enough for classiﬁcation. For
UDP ﬂows, e.g., Skype, the shaper requires more pack-
ets for classiﬁcation (e.g., ∼10). This means that traf-
ﬁc diﬀerentiation tests can trigger diﬀerentiation using a
small amount of data and only need to run long enough
to observe steady state performance. This observation is
particularly salient for mobile users who may be subject
to data caps.

• HTTPS does not preclude classiﬁcation. We ex-
pected that the shaper would not identify applications
using HTTPS as their transport. However, we found that
in practice the devices do detect applications, speciﬁcally
via the SNI (Server Name Indication) ﬁeld in the TLS
handshake. Modifying this ﬁeld results in detection as
HTTPS (instead of the speciﬁc application).

A key question is whether our replay approach is eﬀective
for all shapers, not just those in our lab. While we can eval-
uate only the shapers we possessed, our replay approach suc-
cessfully detects diﬀerentiation in operational networks (Sec-
tion 6), indicating that our eﬀectiveness is not limited only
to the lab environment. It remains an open question whether
there are other shaper models that are not accounted for in
our design.
4.3 VPN overhead

Our control trials use a VPN tunnel, due to potential is-
sues that arise when randomized ports and payloads are clas-
siﬁed as P2P, or otherwise shaped. We now investigate the
overhead of this approach.

VPN overheads can stem from (1) IPSec encapsulation
and (2) latency added by going through the VPN (e.g., if
the VPN induces a circuitous route to the replay server).
The overhead for IPsec encapsulation varies depending on
the size of the payload, with smaller packets incurring higher
overheads. The impact on throughput is relatively small, as
shown in Table 2. For most applications, the diﬀerence in

Figure 4: The shaper has rules to rate limit YouTube and P2P
at 1Mbps and 512Kbps, respectively. Shaped at 1Mbps line is the
YouTube replay in plaintext where shaper correctly detects as
YouTube and shapes at 1Mbps. Shaped at 512Kbps is YouTube
replay with randomized payload and ports, where shaper detects
as P2P and limits at 512Kbps. The Not shaped line is YouTube
replay with string “youtube” being replaced by a random string,
which the shaper detects as “HTTP” and does not limit (since
there are no rules for generic HTTP traﬃc).

Reverse-engineering classiﬁcation. We use our testbed
to understand which features of the traﬃc were used to trig-
ger shaping, and should be preserved in replays. We replay a
recorded trace in our testbed multiple times, each time mod-
ifying a diﬀerent feature of the traﬃc (destination IP, ports,
and packet payloads) and observe its eﬀect on classiﬁcation.
Table 1 summarizes the results for running tests using
YouTube. We ﬁnd that regular expressions on packet pay-
load are the primary signature, with YouTube being cor-
rectly identiﬁed when payloads were unmodiﬁed, despite
changes to the server IP and ports (row 1). We modify three
diﬀerent aspects of the packet payload: the ﬁrst payload
byte, application-layer protocol details in the ﬁrst packet,
and arbitrary bytes after the ﬁrst packet. The second row
of the table shows that adding a one-byte packet caused the
shaper to classify the replay as HTTP (when using port 80)
or P2P (when using high, random ports). This behavior is
identical if replacing the ﬁrst byte, the “GET” command, or
removing the string “youtube” from the payload of the ﬁrst
packet (rows 3-8). However, if the GET command was un-
modiﬁed and at least one “youtube” string appeared in the
ﬁrst packet, the ﬂow was classiﬁed as YouTube. This indi-
cates that this shaping device is using regular expressions
and falling back to port-based classiﬁcation when regular
expressions do not match. Interestingly, modifying any sub-
sequent packets or payload after the GET command has no
impact on classiﬁcation.

We now summarize other key ﬁndings:

 0 1 2 3 4 5 6 7 8 9 0 2 4 6 8 10 12 14 16 18Cumulative transfer(Mbits)Time (s)Not shapedShaped at 1MbpsShaped at 512Kbps244App

Avg packet size (bytes) Avg throughput diﬀ (%)

Youtube
Netﬂix
Hangout

Skype

705
679
435
234

1.13
0.42
2.06
15.64

Table 2: Minimal eﬀect of VPN on our measurements. Skype
packets are small, leading to larger throughput overhead com-
pared to other apps.

throughput with the VPN is 2% or less. However, for Skype,
which has an average packet size of less than 300 bytes the
throughput overhead is higher (15%). Note that we both
record and replay traﬃc using an MTU that is small enough
to prevent fragmentation in the VPN tunnel.

To minimize the impact of latency, we run the replay
and VPN server on the same machine (currently running
on Amazon EC2), which adds less than 2ms of latency com-
pared to contacting the replay server directly (without a
VPN tunnel). We argue these overheads are acceptably low,
and represent a reasonable lower bound on the amount of
diﬀerentiation we can detect.

5. DETECTING DIFFERENTIATION

In this section, we present the ﬁrst study that uses ground-
truth information to compare the eﬀectiveness of various
techniques for detecting diﬀerentiation, and propose a new
test to address limitations of prior work. We ﬁnd that pre-
vious approaches for detecting diﬀerentiation are inaccurate
when tested against ground-truth data, and characterize un-
der what circumstances these approaches yield correct infor-
mation about traﬃc shaping as perceived by applications.

When determining the accuracy of detection, we sepa-
rately consider three scenarios regarding the shaping rate
and a given trace (shown in Figure 5):
• Region 1. If the shaping rate is less than the average
throughput of the recorded traﬃc, a traﬃc diﬀerentiation
detector should always identify diﬀerentiation because
the shaper is guaranteed to aﬀect the time it takes to
transfer data in the trace.
• Region 3. If the shaping rate is greater than the peak
throughput of the recorded trace, a diﬀerentiation de-
tector should never identify diﬀerentiation because the
shaper will not impact the application’s throughput (as
discussed in Section 2.2 we focus on shaping that will
actually impact the application).
• Region 2. Finally, if the shaping rate is between the
average and peak throughput of the recorded trace, the
application may achieve the same average throughput
but a lower peak throughput. In this case, it is possi-
ble to detect diﬀerentiation, but the impact of this dif-
ferentiation will depend on the application. For exam-
ple, a non-interactive download (e.g., ﬁle transfers for
app updates) may be sensitive only to changes in av-
erage throughput (but not peak transfer rates), while
a real-time app such as video-conferencing may experi-
ence lower QoE for lower peak rates. Given these cases,
there is no single deﬁnition of accuracy that covers all
applications in region 2. Instead, we show that we can
conﬁgure detection techniques to consistently detect dif-
ferentiation or consistently not detect diﬀerentiation in
this region, depending on the application.

Figure 5: Regions considered for determining detection accu-
racy.
If the shaping rate is less than the application’s average
throughput (left), we should always detect diﬀerentiation. Like-
wise, if the shaping rate is greater than peak throughput (right),
we should never detect diﬀerentiation. In the middle region, it
is possible to detect diﬀerentiation, but the performance impact
depends on the application.

Figure 6: KS Test statistic (left) and Area Test statistic (right).
In the former case, the diﬀerence between the distributions is
small in terms of throughput, but the KS Test statistic is large. In
the Area Test statistic, we ﬁnd the area between the distributions
and normalize it by the smaller of the peak throughputs for each
distribution.

In the remainder of this section, we use this region-based
classiﬁcation to evaluate three techniques for detecting dif-
ferentiation. We describe them in the next section, explain
how we calibrate their respective thresholds and parameters,
then discuss how resilient these approaches are to noise (e.g.,
packet loss), and how eﬃcient they are in terms of data con-
sumption.
5.1 Statistical tests

We explore approaches used in Glasnost [6] and NetPo-
lice [36], and propose a new technique that has high accuracy
in regions 1 and 3, and reliably does not detect diﬀerentia-
tion in region 2.

Glasnost: Maximum throughput test. Glasnost [6]
does not preserve the inter-packet timing when replay-
ing traﬃc, and identiﬁes diﬀerentiation if the maximum
throughput for control and exposed ﬂows diﬀer by more than
a threshold. We expect this will always detect diﬀerentia-
tion in region 1, but might generate false positives in regions
2 and 3 (because Glasnost may send traﬃc at a higher rate
than the recorded trace).

NetPolice: Two-sample KS Test.
As discussed in
NetPolice [36], looking at a single summary statistic (e.g.,
maximum) to detect diﬀerentiation can be misleading. The
issue is that two distributions of a metric (e.g., throughput)
may have the same mean, median, or maximum, but vastly
diﬀerent distributions. A diﬀerentiation detection approach
should be robust to this.

To

address

this, NetPolice uses

two-sample
Kolmogorov–Smirnov (KS) test, which compares two empir-
ical distribution functions (i.e., empirical CDFs) using the
maximum distance between two empirical CDF sample sets

the

Region 1(below avg)Region 2(between avg & max)Region 3(above max)Shaping rateAvgMaxReplay transfer rateKS TeststatisticThroughput (KB/s)CDFArea Test = a/wThroughput (KB/s)CDFwa2455.3 Evaluation criteria

We now describe the criteria under which we evaluate the

three diﬀerentiation tests.

Overall accuracy: Using the taxonomy in Figure 5, a
statistical test should always detect diﬀerentiation in region
1, and never detect diﬀerentiation in region 3. We deﬁne ac-
curacy as the fraction of samples for which the test correctly
identiﬁes the presence or absence of diﬀerentiation in these
regions.

Resilience to noise: Diﬀerences between two empirical
CDFs could be explained by a variety of reasons, including
random noise [30]. We need a test that retains high accuracy
in the face of large latencies and packet loss that occur in
the mobile environment. When evaluating and calibrating
our statistical tests, we take this into account by simulating
packet loss.

Data consumption: To account for network variations
over time, we run multiple iterations of control and exposed
trials back to back. We merge all control trials into a single
distribution, and similarly merge all exposed trials. As we
show below, this can improve accuracy compared to running
a single trial, but at the cost of longer/more expensive tests.
When picking a statistical test, we want the one that yields
the highest accuracy with the smallest data consumption.
5.4 Calibration

We identify the most accurate settings of threshold values.

Glasnost: We used the threshold suggested by Glasnost,
δ = 0.2. This yields perfect accuracy in region 1, but gener-
ates 100% false positives in region 3 (as expected). Glasnost
always detects diﬀerentiation in region 2. For applications
that are sensitive to changes in peak throughput, this test
will yield the correct result.

KS Test: We use thresholds suggested by NetPolice, i.e.,
α = 0.95 and β = 0.95. This yields good accuracy in re-
gions 1 and 3, but inconsistent behavior in region 2.
In
other words, this test will sometimes detect diﬀerentiation
in region 2, and sometimes not — making it diﬃcult to use
for detection in this region. We also observed that even for
tests with no added loss in our testbed over well-provisioned
wired network, up to 8% of tests were considered invalid by
KS Test due to the issue shown in Fig. 6, while the Area Test
can correctly detect for diﬀerentiation (or no diﬀerentiation)
in those cases.

Area Test: We ﬁnd that t = 0.1 or t = 0.2 yield the best
accuracy, depending on the application (Fig. 7). This yields
good accuracy in regions 1 and 3, and consistent decisions of
no diﬀerentiation for region 2. For apps that are insensitive
to changes only to peak throughput (average throughput
stays the same), this will yield the correct result.
5.5 Evaluation results
Summary results for low noise. We ﬁrst consider the
accuracy of detecting diﬀerentiation under low loss scenar-
ios. Table 3 presents the accuracy results for four popular
apps in regions 1 and 3. We ﬁnd that the KS Test and
Area Test have similarly high accuracy in both regions 1
and 3, but Glasnost performs poorly in region 3 because it
will detect diﬀerentiation, even if the shaping rate is above
the maximum. We explore region 2 behavior later in this
section; until then we focus on accuracy results for regions

Figure 7: Calibrating Area Test for Youtube. We pick 0.2 as
the threshold.

for a conﬁdence interval α. To validate the KS Test result,
NetPolice uses a resampling method as follows: randomly
select half of the samples from each of the two original in-
put distributions and apply the KS Test on the two sample
subsets, and repeat this r times. If the results of more than
β% of the r tests agree with the original test, they conclude
that the original KS Test statistic is valid.

Our approach: Area Test. The KS Test only consid-
ers the diﬀerence between distributions along the y-axis (as
shown in Figure 6, left) even if the diﬀerence in the x-axis
(in this case, throughput) is small. In our experiments, we
found this makes the test very sensitive to small changes in
performance not due to diﬀerentiation, which can lead to
inaccurate results or labeling valid tests as invalid.

To address this, we propose an Area Test that accounts for
the degree of diﬀerentiation detected: we ﬁnd the area, a,
between the two CDF curves (as shown in Fig. 6, right) and
normalize it by the minimum of peak throughputs for each
distribution, w. This test concludes there is diﬀerentiation if
the KS Test detects diﬀerentiation and the normalized area
between the curves is greater than a threshold t. We discuss
how we select thresholds in Sec. 5.4.
5.2 Testbed environment

Our testbed consists of a commercial traﬃc shaper, a re-
play client, and a replay server (Figure 3). We vary the
shaping rate using the commercial device to shape each ap-
plication to throughput values in regions 1-3. Depending
on the average and maximum throughputs for each trace,
we vary the shaping rate from 0.1 Mbps to 30 Mbps (9% to
300% of peak throughput).

We emulate noisy packet loss in our tests using the Linux
Traﬃc Control (tc) and Network Emulation (netem) tools
to add bursty packet loss according to Gilbert-Elliott (GE)
model [10]. We perform all our evaluations and calibrations
under three conditions: 1) low loss (no added loss), 2) mod-
erate loss (1.08%), and 3) high loss (1.45%). We found that
correlated losses higher than 1.5% had disastrous eﬀects on
TCP performance and all detection techniques were inaccu-
rate.

Evaluation method. We explore the parameter space by
varying the application, loss rate, shaping rate, and number
of replay repetitions for each statistical test. We present re-
sults for a few popular TCP-based applications (YouTube
and Netﬂix) and UDP-based applications (Skype and Hang-
out); we also performed tests on a number of other appli-
cations (omitted for space). We repeat each test 10 times,
where a test is a replay performed with a given set of pa-
rameters, with and without shaping. We present results that
aggregate these tests to produce statistically signiﬁcant re-
sults.

 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Accuracyregions 1 & 3Area Test threshold246App

Netﬂix
YouTube
Hangout
Skype

100
100
100
100

KS Test Area Test Glasnost
R1 R3
R1 R3 R1
100
100
100
100
100
100
100
100
100
100
100
90

R3
100
100
100
100

0
0
0
0

Table 3: Shaping detection accuracy for diﬀerent apps, in regions
1 and 3 (denoted by R1 and R3). We ﬁnd that the KS Test
(NetPolice) and Area Test have similarly high accuracy in both
regions (1 and 3), but Glasnost performs poorly in region 3.

Figure 8: Accuracy against loss.

App

Netﬂix
YouTube
Hangout
Skype

Region 2

KS Test Area Test Glasnost

65
67
40
55

28
0
0
10

100
100
100
92

Table 4: Percent of tests identiﬁed as diﬀerentiation in region 2
for the three detection methods. Glasnost consistently identiﬁes
region 2 as diﬀerentiation, whereas the Area Test is more likely
to not detect diﬀerentiation.

1 and 3 and the two KS Tests (as they do not identify dif-
ferentiation in region 3)

Impact of noise.
Properties of the network unrelated
to shaping (e.g., congested-induced packet loss) may impact
performance and cause false positives for diﬀerentiation de-
tectors. In Fig. 8 we examine the impact of loss on accuracy
for one TCP application (Netﬂix) and one UDP application
(Skype). We ﬁnd that both variants of the KS Test have
high accuracy in the face of congestion, but the Area Test
is more resilient to moderate and high loss for Skype.

Impact of number of replays. One way to account for
noise is to use additional replays to average out any vari-
ations from transient noise in a single replay. To evaluate
the impact of additional replays, we varied the number of
replays combined to detect diﬀerentiation under high loss
and plotted the detection accuracy in Fig. 9.4 In the case
of moderate or high loss, increasing the number of replays
improves accuracy, particularly for a short YouTube trace.
In all cases, the Area Test is more accurate than (or equal
to) the KS Test. Importantly, the accuracy does not signif-
icantly improve past 2 or 3 replays. In our deployment, we
use 2 tests (to minimize data consumption).

Region 2 detection results.
Recall that our goal is
to identify shaping that impinges on the network resources
required by an application. However, in region 2, shap-
ing may or may not impact the application’s performance,
which makes identifying shaping with actual application im-
pact challenging. We report detection results for the three
tests in region 2 in Table 4. We ﬁnd that Glasnost consis-
tently identiﬁes diﬀerentiation, the Area Test consistently
does not detect diﬀerentiation, and the KS Test is inconsis-
tent. When testing an application that is sensitive to shap-
ing of its peak throughput, the Glasnost method may be
preferable, whereas the Area Test is preferred when testing
applications that can tolerate shaping above their average
throughput.

When detecting diﬀerentiation, we conservatively avoid
the potential for false positives and thus do not report dif-

4Additional replays did not help when there is low loss.

Figure 9: Eﬀect of combining replays on accuracy in high loss.

ferentiation for region 2. Thus, we use the Area Test in our
implementation because it will more reliably achieve this
result (Table 4). Determining which apps are aﬀected by
region 2 diﬀerentiation is a topic of future work.

6. MEASUREMENT STUDY

We now use our calibrated detection approach to identify
cases of diﬀerentiation using measurement study of produc-
tion networks. For each network, we focus on detection for a
set of popular apps that we expect might be subject to diﬀer-
entiation: streaming audio and video (high bandwidth) and
voice/video calling (competes with carrier’s line of business).
The data from these experiments was initially collected be-
tween January and May 2015. We then repeated the exper-
iments in August 2015, after the new FCC rules prohibiting
diﬀerentiation took eﬀect.
6.1 System implementation

We now describe the implementation of our client and
server software. To promote transparency and guide pol-
icy for traﬃc diﬀerentiation, our source code and analy-
sis results will be made continuously available at http:
//dd.meddle.mobi.

Client. We implement our replay client in an Android
app called Diﬀerentiation Detector. It does not require any
root privileges, and is available for download in the Google
Play store.5 The app consists of 14,000 lines of source code
(LOC), which includes the Strongswan VPN implementa-
tion [27] (4,600 LOC). The app conducts diﬀerentiation tests
and reports results to our replay servers as part of an IRB-
approved study. Users undergo informed consent when the
app ﬁrst runs, and cannot run any tests unless they consent
to participate in our study. An iOS implementation is under
development.

The app (Fig. 10) is pre-loaded with replay transcripts for
Viber, Netﬂix, Spotify, Skype, YouTube, and Google Hang-
outs, so that users can test for diﬀerentiation of these apps

5https://play.google.com/store/apps/details?id=
com.stonybrook.replay

 0 0.2 0.4 0.6 0.8 1Low lossMod lossHigh lossAccuracyNetflix - AreaNetflix - KS2Skype - AreaSkype - KS20.60.70.80.911234Accuracy#replays combinedyoutube-KS2youtube-Areahangout-KS2hangout-Area247Per-client management. To replay traﬃc for multiple
simultaneous users, our replay server needs to be able to map
each received packet to exactly one app replay. A simple
solution is to put this information in the ﬁrst few bytes of
each ﬂow between client and server. Unfortunately, as shown
in Section 4.2, this can disrupt classiﬁcation and lead to false
negatives. Instead, we use side-channel connections out of
band from the replay to supply information regarding each
replay run.

NAT behavior. We also use the side-channel to identify
which clients will connect from which IPs and ports. For
networks without NAT devices, this works well. However,
many mobile networks use NAT, meaning we can’t use the
side-channel to identify the IP and port that a replay con-
nection will use (because they may be modiﬁed by the NAT).
For such networks, each replay server can reliably support
only one active client per ISP and application. While this
has not been an issue in our initial app release, we are inves-
tigating other work-arounds. Moreover, to scale the system,
we can use multiple replay servers and a load balancer. The
load balancer can be conﬁgured to assign clients with the
same IP address to diﬀerent servers.

“Translucent” HTTP proxies.
It is well known that
ISPs use transparent proxies on Web traﬃc [34]. Our sys-
tem works as intended if a proxy simply relays exactly the
same traﬃc sent by a replay client. In practice, we found
examples of “translucent” proxies that issue the same HTTP
requests as the client but change connection keep-alive be-
havior; e.g., the client uses a persistent connection in the
recorded trace and the proxy does not. To handle this be-
havior, our system must be able to map HTTP requests from
unexpected connections to the same replay.

Another challenge with HTTP proxies is that the proxy
may have a diﬀerent public IP address from other non-
HTTP traﬃc (including the side-channel), which means the
server will not be able to map the HTTP connections to
the client based on IP address.
In these cases the server
replies to the HTTP request with a special message. When
the client receives this message, it restarts the replay. This
time the client adds a custom header (X-) to HTTP requests
with client’s ID, so the server can match HTTP connections
with diﬀerent IP addresses to the corresponding clients. The
server then ignores this extra header for the remainder of the
replay. We have also observed that some mobile providers
exhibit such behavior for other TCP and UDP ports, mean-
ing we cannot rely on an X- header to convey side-channel
information. In these cases we can identify users based on
ISP’s IP subnet, which means we can only support one active
client per replay server and ISP, regardless of the application
they are replaying.

Content-modifying proxies and transcoders. Recent
reports and previous work highlight cases of devices in ISPs
that modify customer traﬃc for reasons such as tracking [17],
caching [34], security [24], and reducing bandwidth [32]. We
also saw several cases of content-modifying proxies. For
example, we saw Sprint modifying HTTP headers, Veri-
zon inserting global tracking cookies, and Boost transcoding
YouTube videos.

In our replay methodology, we assumed that packets ex-
changed between the client and server would not be not
modiﬁed in ﬂight, and trivially detect modiﬁcation because
packet payloads do not match recorded traces. In the case

Figure 10: Screenshot of our Android Diﬀerentiation Detector
app.

without recording traﬃc.6 For each trace, the app follows
the replay procedure described in Section 3 and repeats the
replay tests twice7 back to back.

At the end of each replay, metadata such as carrier name,
OS information, network type, and signal strength, is col-
lected and sent to the server for analysis. To account for
the case where background traﬃc might aﬀect our detec-
tion results, future version of the app will measure data
consumption during replays and discard results where we
detect interference from other traﬃc. Users can also access
their historical results through the app.

Server and analysis code The server coordinates with the
client to replay traces, and records packet traces for analy-
sis. This interaction is managed by side-channel connections
that identify which trace is being replayed and what ports
will be opened in the case of NAT traversal. For networks
that allow it (e.g., in our testbed), we support IP spooﬁng
so our replay server can send packets using the IP addresses
in arbitrary recorded traces. The server logic is 1,850 lines
of code (Python).

Our analysis code implements tests for throughput, RTT,
jitter, and loss diﬀerentiation. We implement KS Test and
Area Test, and use simple scalar metrics (average, max) for
loss. Our parsing script supports TCP and streaming UDP
applications. It uses tshark to extract conversations. To-
gether, these artifacts consist of 1,170 lines of code.
6.2 Challenges in operational networks

In this section we discuss several challenges that we en-
countered when attempting to identify diﬀerentiation in mo-
bile networks. To the best of our knowledge, we are the ﬁrst
to identify these issues for detecting diﬀerentiation and dis-
cuss workarounds that address them.

6While we currently only support Android, the approach
should also work on any mobile OS that supports VPN con-
nectivity, including iOS. We are currently developing sup-
port for users to record their own traces from mobile devices.
7Users can increase the number of back-to-back replays for
more accuracy.

248of header manipulation, we use an edit-distance based ap-
proach to match modiﬁed content to the recorded content
it corresponds to. While our system can tolerate moder-
ate amounts of modiﬁcation (e.g., HTTP header manipu-
lation/substitution), if the content is modiﬁed drastically,
e.g., transcoding an image to reduce its size, it is diﬃcult
to detect shaping because the data from our control and
exposed trials do not match. In our current system, we sim-
ply notify the user that there is content modiﬁcation but do
not attempt to identify shaping. We leave a more detailed
analysis of this behavior to future work.

Caching. Our replay system assumes content is served
from the replay server and not an ISP’s cache. We detect
the latter case by identifying cases where the client receives
data that was not sent by the server. In the few cases where
we observed this behavior, the ISPs were caching small static
objects (e.g., thumbnail images) which were not the domi-
nant portion of the replay traﬃc (e.g., when streaming au-
dio) and had negligible eﬀect on our statistical analysis.8
Going forward, we expect this behavior to be less promi-
nent due to the increased use of encrypted protocols, e.g.,
HTTP/2 and QUIC. In such cases, an ISP may detect and
shape application traﬃc using SNI for classiﬁcation, but
they cannot modify or cache content.
6.3 Differentiation results

In this section, we present results from running our Diﬀer-
entiation Detector Android app on popular mobile networks.
This dataset consists of 4,786 replay tests, covering traces
from six popular apps. We collected test results from most
major cellular providers and MVNOs in the US; further, we
gathered measurements from four international networks.

Our app supports both VPN traﬃc and random payloads
(but not random ports) as control traﬃc. We use the lat-
ter only if a device does not support VPN connectivity or
the cellular provider blocks or diﬀerentiates against VPN
traﬃc. After collecting data, we run our analysis to detect
diﬀerentiation; the results for US carriers are presented in
Table 5.
In other networks such as Idea (India), JazzTel
(Spain), Three (UK), and T-Mobile (Netherlands), our re-
sults based on a subset of the traces (the traces that users
selected) indicated no diﬀerentiation.

Our key ﬁnding is that our approach successfully de-
tect diﬀerentiation in three mobile networks (BlackWireless,
H2O, and SimpleMobile), with the impact of shaping result-
ing in up to 65% diﬀerence in average throughput. These
shaping policies all apply to YouTube (not surprising given
its impact on networks), but not always to Netﬂix and Spo-
tify. We did not identify diﬀerentiation for UDP traﬃc in
any of the carriers.
Interestingly, H2O consistently gives
better performance to port 80 traﬃc with random payloads,
indicating a policy that gives relatively worse performance
to VPN traﬃc and streaming audio/video. We tested these
networks again in August 2015 and did not observe such dif-
ferentiation. We speculate that these networks ceased their
shaping practices in part due to new FCC rules barring this
behavior, eﬀective in June 2015.

We contacted these ISPs we tested for comment on the
behavior we observed in their networks. At the time of pub-
lication, only one ISP had responded (Sprint) and did not
wish to comment.

8Boost was one exception, which we discuss later.

ISP
Verizon
T-Mobile
AT&T
Sprint
Boost
BlackWireless
H2O
SimpleMobile
NET10

YT
m
-
f

m/p
m

60%
37%*
36%

p

NF
m
-
f

m/p
m
-

SF
m
-
f

m/p
m
-

45%*

65%*

-
p

-
p

SK VB HO

-
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-

-
-
-
-
-
-
-
-
-

Table 5: Shaping detection results per ISP in our dataset, for six
popular apps: YouTube (YT), Netﬂix (NF), Spotify (SF), Skype
(SK), Viber (VB), and Google Hangout (HO). When shaping oc-
curs, the table shows the diﬀerence between average through-
put (%) we detected. A dash (-) indicates no diﬀerentiation, (f)
means IP addresses changed for each connection, (p) means a
“translucent” proxy changed connection behavior from the orig-
inal app behavior, and (m) indicates that a middlebox modiﬁed
content in ﬂight between client and server. *For the H2O network,
replays with random payload have better performance than VPN
and exposed replays, indicating a policy that favors non-video
HTTP over VPN and streaming video.

While attempting to detect diﬀerentiation, we identiﬁed
other interesting behavior that prevented us from identifying
shaping. For example, Boost transcodes YouTube video to
a fraction of the original quality, and then caches the result
for future requests. Other networks use proxies to change
TCP behavior. We found such devices to be pervasive in
mobile networks.

7. DISCUSSION

This paper focuses on detecting diﬀerentiation that im-
pacts application performance. While investigating shap-
ing, our technique revealed a number of other issues that
are beyond the scope of this paper.
Assigning blame. When diﬀerentiation is detected, we
generally assign blame to the network from where the test is
launched, i.e., the access network. While in theory it might
be caused by any network along the path between the ac-
cess network and our replay server, we expect that transit
networks will not have incentive to diﬀerentiate and we en-
sure that our replay hosting sites also do not diﬀerentiate
traﬃc. As part of future work, we will investigate how to
combine diﬀerentiation results from multiple vantage points
(e.g., using tomography) to identify cases when diﬀerenti-
ation is not caused by the access network, but rather an
intermediate network.
Diﬀerentiation against VPN traﬃc. A subtle chal-
lenge arises when recording traces of app-generated net-
work traﬃc over a production network: what happens when
recorded traﬃc is subject to diﬀerentiation (e.g., shaping)?
If the recorded and replayed traﬃc are both shaped, a na¨ıve
detector will indicate that the network has no diﬀerentiation.
One simple solution is to allow users to run traces captured
from a network of the same type (e.g., WiFi, 3G, LTE)
where we know the recorded traces were not subject to dif-
ferentiation (our current version of the app ships with traces
from diﬀerent applications recorded over a well-provisioned
network that does not shape traﬃc).

While this solves the problem, it does not tell us when
there is a problem in the ﬁrst place. For this, we need to
detect that the VPN traﬃc itself is subject to diﬀerentia-

249diﬀerent treatment, we can attempt to disguise replay traﬃc,
using diﬀerent servers, ports and control traﬃc techniques
(e.g., diﬀerent payload/port randomization scheme).

Data caps.
Some providers have “unlimited” data plans
with lower data rates after usage exceeds a certain cap. We
do not expect to detect this from a single user, but may be
able to identify it by combining results from multiple users
or by monitoring data usage via Android APIs.

8. CONCLUSION

We presented and evaluated a new tool for accurately de-
tecting diﬀerentiation using a VPN to record and replay
traﬃc generated by apps to improve transparency in mo-
bile networks. We found extensive use of middleboxes that
break end-to-end principles in mobile networks, and identi-
ﬁed their impact on applications.

From a network neutrality perspective, our ﬁndings are
concerning in part due to observed shaping policies and
third-party modiﬁcation of traﬃc, both of which are perva-
sive. There is little research into the impact of such devices,
and currently policymakers have essentially no guidance as
to how to regulate this behavior. As part of our ongoing
work, we are using a Web site to make our results public —
both to improve transparency for users and to guide future
policies. In addition, we are investigating how to ﬁngerprint
middleboxes in mobile networks and developing techniques
to understand their impacts on a wide range of applications.

Acknowledgements
We thank the anonymous reviewers for their helpful com-
ments. This research was partially supported in part by
NSF grants CNS-1054233, CNS-1318396, and CNS-1319019,
CNS-1350720, and a Google Faculty Research Award.

9. REFERENCES
[1] Experimenting with QUIC. http://blog.chromium.

org/2013/06/experimenting-with-quic.html.

[2] Neubot – the network neutrality bot.

http://www.neubot.org.

[3] V. Bashko, N. Melnikov, A. Sehgal, and

J. Schonwalder. Bonaﬁde: A traﬃc shaping detection
tool for mobile networks. In IFIP/IEEE International
Symposium on Integrated Network Management
(IM2013), 2013.

[4] D. Clark. Network neutrality: Words of power and

800-pound gorillas. International Journal of
Communication, 2007.

[5] W. Cui, V. Paxson, N. Weaver, and R. H. Katz.

Protocol-independent adaptive replay of application
dialog. In Proc. of NDSS, 2006.

[6] M. Dischinger, M. Marcon, S. Guha, K. P. Gummadi,

R. Mahajan, and S. Saroiu. Glasnost: Enabling end
users to detect traﬃc diﬀerentiation. In Proc. of
USENIX NSDI, 2010.

[7] FCC announces ”Measuring Mobile America”

program. http://www.fcc.gov/document/fcc-
announces-measuring-mobile-america-program.
[8] FCC. Order 10-201: Preserving the open internet.

Figure 11: Eﬀect of shaping on latency for a TCP stream. Shap-
ing at 1 Mbps exhibits expected queuing delays that identify the
size of the buﬀer (x=75 KB); however, shaping at a lower rate
(512 kbps) leads to bimodal RTTs largely due to TCP never ex-
iting slow start.

tion. If there exists at least one class of traﬃc that is not
shaped in our tests, then it will get higher performance out-
side the VPN tunnel compared to inside the tunnel. In this
case, there will be at least one case where tunneled perfor-
mance will be diﬀerent from non-tunneled. As a result, we
can reliably conclude that there is some kind of diﬀerentia-
tion9; however, we cannot yet directly attribute it to shaping
against a VPN.

One potential way to address this is to exploit the obser-
vation that popular shaping devices use token-bucket queues
to enforce ﬁxed-rate shaping. As shown in Fig. 11 (1 Mbps)
shaping devices exhibit a characteristic queueing delay evi-
dent when plotting bytes in ﬂight vs. RTT. If a traﬃc class
is subject to diﬀerentiation, we will observe increased delays
compared to a case that is not managed via a queue.

If indeed the control traﬃc is shaped, it limits our ability
to test other types of traﬃc for diﬀerentiation—this is true of
any diﬀerentiation detection approach. However, detection
of this behavior itself is important information; for example,
in the US even this behavior should be in violation of FCC
rules eﬀective June 12, 2015 [9].

Shaping conﬁgurations can have unintended conse-
quences for TCP.
In one sample conﬁguration already
used by an ISP (Fig. 11, 512 Kbps), we found that the shap-
ing rate was so low that it prevented TCP from ever exiting
slow start. Importantly, TCP increased its congestion win-
dow exponentially in time while the shaper’s queue drained
linearly in time, leading to substantial queuing and packet
loss.

This is problematic for two reasons. First, this policy
leads to wasted bandwidth at the network edge, which will
cost the ISP running the shaper. Second, it is unclear how
the user is being charged for this lost traﬃc.
If users are
billed based on bytes entering the network before hitting
the shaper, the ISP will over charge the user for packets that
were never delivered. Such policies, and their implications,
merit further investigation as part of a future study.

Evasion.
One of our goals is to improve transparency
for network management practices in cellular data networks.
ISPs wishing to avoid characterization may attempt to do
so by blocking VPN traﬃc or selectively changing policies
for our replay traﬃc.
If ISPs choose to block our traﬃc,
we can easily detect it and report this result to users and
policy makers. For cases where the traﬃc is selectively given

9More generally, we can draw the same conclusion if at least
one class of traﬃc is shaped diﬀerently from VPN traﬃc.

https://apps.fcc.gov/edocs_public/attachmatch/
FCC-10-201A1_Rcd.pdf, December 2010.

 0 0.2 0.4 0.6 0.8 1 1.2 1.4 0 20 40 60 80 100 120 140RTTBytes in flight (KB)1Mbps512Kbps250[9] FCC. Protecting and promoting the open internet.

[26] F. Sarkar. Prevention of bandwidth abuse of a

communications system, Jan. 9 2014. US Patent App.
14/025,213.

[27] Strongswan. www.strongswan.org.
[28] P. Svensson. Comcast blocks some internet traﬃc.

http://www.washingtonpost.com/wp-dyn/content/
article/2007/10/19/AR2007101900842.html,
October 2007.

[29] Switzerland network testing tool.

https://www.eff.org/pages/switzerland-network-
testing-tool.

[30] M. B. Tariq, M. Motiwala, N. Feamster, and

M. Ammar. Detecting network neutrality violations
with causal inference. In CoNEXT, 2009.

[31] N. Vallina-Rodriguez, S. Sundaresan, C. Kreibich,

N. Weaver, and V. Paxson. Beyond the radio:
Illuminating the higher layers of mobile networks. In
Proc. of MobiSys, 2015.

[32] N. Weaver, C. Kreibich, M. Dam, and V. Paxson.

Here Be Web Proxies. In Proc. PAM, 2014.

[33] N. Weaver, R. Sommer, and V. Paxson. Detecting
forged TCP reset packets. In Proc. of NDSS, 2009.

[34] X. Xu, Y. Jiang, T. Flach, E. Katz-Bassett,
D. Choﬀnes, and R. Govindan. Investigating
transparent web proxies in cellular networks. In Proc.
PAM, 2015.

[35] F. Zarinni, A. Chakraborty, V. Sekar, S. Das, and

P. Gill. A ﬁrst look at performance in mobile virtual
network operators. In Proc. of IMC, 2014.

[36] Y. Zhang, Z. M. Mao, and M. Zhang. Detecting

Traﬃc Diﬀerentiation in Backbone ISPs with
NetPolice. In Proc. of IMC, 2009.

[37] Z. Zhang, O. Mara, and K. Argyraki. Network

neutrality inference. In Proc. of ACM SIGCOMM,
2014.

https://www.federalregister.gov/articles/2015/
04/13/2015-07841/protecting-and-promoting-the-
open-internet, April 2015.

[10] G. Hasslinger and O. Hohlfeld. The Gilbert-Elliott

model for packet loss in real time services on the
Internet, 2008.

[11] S. Higginbotham. The Netﬂix–Comcast agreement

isn’t a network neutrality violation, but it is a
problem. http://gigaom.com/2014/02/23/the-
netflix-comcast-agreement-isnt-a-network-
neutrality-violation-but-it-is-a-problem/,
February 2014.

[12] J. Hui, K. Lau, A. Jain, A. Terzis, and J. Smith. How

YouTube performance is improved in T-Mobile
network. http://velocityconf.com/velocity2014/
public/schedule/detail/35350.

[13] P. Kanuparthy and C. Dovrolis. ShaperProbe:

end-to-end detection of ISP traﬃc shaping using
active methods. In Proc. of IMC, 2011.

[14] C. Kreibich, N. Weaver, B. Nechaev, and V. Paxson.
Netalyzr: Illuminating the edge network. In Proc. of
IMC, 2010.

[15] M. Luckie, A. Dhamdhere, D. Clark, B. Huﬀaker, and
k. claﬀy. Challenges in inferring internet interdomain
congestion. In Proc. of IMC, 2014.

[16] R. Mahajan, M. Zhang, L. Poole, and V. Pai.

Uncovering performance diﬀerences among backbone
ISPs with Netdiﬀ. In Proc. of USENIX NSDI, 2008.

[17] J. Mayer. How verizon’s advertising header works.

http://webpolicy.org/2014/10/24/how-verizons-
advertising-header-works/.

[18] Measurement Lab Consortium. Isp interconnection
and its impact on consumer internet performance.
http://www.measurementlab.net/blog/2014_
interconnection_report, October 2014.

[19] A. Nikravesh, H. Yao, S. Xu, D. R. Choﬀnes, and

Z. M. Mao. Mobilyzer: An open platform for
controllable mobile network measurements. In Proc. of
MobiSys, 2015.

[20] A. Norberg. uTorrent transport protocol.

http://www.bittorrent.org/beps/bep_0029.html.

[21] B. Obama. Net neutrality: President Obama’s plan for

a free and open internet.
http://www.whitehouse.gov/net-neutrality#
section-read-the-presidents-statement.
[22] A. Rao, J. Sherry, A. Legout, W. Dabbout,

A. Krishnamurthy, and D. Choﬀnes. Meddle:
Middleboxes for increased transparency and control of
mobile traﬃc. In Proc. of CoNEXT 2012 Student
Workshop, 2012.

[23] D. Rayburn. Cogent now admits they slowed down

netﬂix’s traﬃc, creating a fast lane & slow lane.
http://blog.streamingmedia.com/2014/11/cogent-
now-admits-slowed-netflixs-traffic-creating-
fast-lane-slow-lane.html, November 2014.

[24] C. Reis, S. D. Gribble, T. Kohno, and N. C. Weaver.

Detecting In-Flight Page Changes with Web
Tripwires. In Proc. of USENIX NSDI, 2008.

[25] Sandvine - intelligent broadband networks.

http://www.sandvine.com.

251