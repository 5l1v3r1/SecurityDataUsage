AUTOPROBE: Towards Automatic Active Malicious Server

Probing Using Dynamic Binary Analysis

Zhaoyan Xu
SUCCESS LAB

Texas A&M University
College Station, TX, US

z0x0427@cse.tamu.edu

Guangliang Yang

SUCCESS LAB

Texas A&M University
College Station, TX, US
glyang@cse.tamu.edu

Antonio Nappa

IMDEA Software Institute &
Universidad Politécnica de

Madrid, Spain

antonio.nappa@imdea.org

Juan Caballero

IMDEA Software Institute

Madrid, Spain

juan.caballero@imdea.org

Robert Baykov
SUCCESS LAB

Texas A&M University
College Station, TX, US

baykovr@cse.tamu.edu

Guofei Gu

SUCCESS LAB

Texas A&M University
College Station, TX, US
guofei@cse.tamu.edu

ABSTRACT
Malware continues to be one of the major threats to Internet
security. In the battle against cybercriminals, accurately identifying
the underlying malicious server infrastructure (e.g., C&C servers
for botnet command and control) is of vital importance. Most
existing passive monitoring approaches cannot keep up with the
highly dynamic, ever-evolving malware server infrastructure. As
an effective complementary technique, active probing has recently
attracted attention due to its high accuracy, efﬁciency, and scalabil-
ity (even to the Internet level).

In this paper, we propose AUTOPROBE, a novel system to
automatically generate effective and efﬁcient ﬁngerprints of remote
malicious servers. AUTOPROBE addresses two fundamental limita-
tions of existing active probing approaches: it supports pull-based
C&C protocols, used by the majority of malware, and it generates
ﬁngerprints even in the common case when C&C servers are not
alive during ﬁngerprint generation.

Using real-world malware samples we show that AUTOPROBE
can successfully generate accurate C&C server ﬁngerprints through
novel applications of dynamic binary analysis techniques. By con-
ducting Internet-scale active probing, we show that AUTOPROBE
can successfully uncover hundreds of malicious servers on the
Internet, many of them unknown to existing blacklists. We believe
AUTOPROBE is a great complement to existing defenses, and can
play a unique role in the battle against cybercriminals.

Categories and Subject Descriptors
D.4.6 [Operating Systems]: Security and Protection;

General Terms
Security;

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660352 .

Keywords
Active probing; Malware; Fingerprint generation; C&C servers

1.

INTRODUCTION

The Internet is an essential part of our life, but malware poses a
serious threat to its security. Millions of computers have been com-
promised by various malware families, and they are used to launch
all kinds of attacks and illicit activities such as spam, clickfraud,
DDoS attacks, and information theft. Such malicious activities are
normally initiated, managed, facilitated, and coordinated through
remotely accessible servers, such as exploit servers for malware’s
distribution through drive-by downloads, C&C servers for mal-
ware’s command and control, redirection servers for anonymity,
and payment servers for monetization. These malicious servers
act as the critical infrastructure for cybercrime operations and
are a core component of the malware underground economy.
Undoubtedly, identifying malware’s server infrastructure is of vital
importance to defeat cybercrime.

Traditional approaches for detecting malicious servers mostly
rely on passive monitoring of host and network behaviors in
home/enterprise/ISP networks. However, such passive approaches
are typically slow, incomplete and inefﬁcient because miscreants
use dynamic infrastructures and frequently move their servers (e.g.,
for evasion or as a reaction to takedowns). To solve this issue,
active probing techniques have been proposed to detect malicious
servers and compromised hosts in an active, fast, and efﬁcient
way [25, 41]. The basic idea is to use a network ﬁngerprinting
approach that sends specially crafted packets (i.e., probes) to
remote hosts and examines their responses to determine whether
they are malicious or not. Since probes are sent from a small set of
scanner hosts, active probing is cheap and easy to deploy, and it is
highly scalable, even for the entire Internet.

In this work we describe AUTOPROBE, which implements a
novel approach to the problem of automatically building network
ﬁngerprints that can be used for actively detecting malware servers
on the Internet. Our goal is similar to the recently proposed
CYBERPROBE [25], which demonstrated how active probing can
successfully detect malicious servers at Internet scale. However,
our approach to ﬁngerprint generation radically differs from the
one used by CYBERPROBE. While CYBERPROBE takes as input
network traces and leverages machine learning techniques on
network trafﬁc to generate the ﬁngerprints, AUTOPROBE assumes

179the availability of a sample of the target malware family and applies
dynamic binary analysis on the malware executable. AUTOPROBE
addresses fundamental limitations in CYBERPROBE. First, CY-
BERPROBE is not able to generate ﬁngerprints for malware families
that contain replay protection. In addition, the lack of semantics
available in network trafﬁc and the noise in the input network traces
limit the quality of CYBERPROBE’s ﬁngerprints. Furthermore,
CYBERPROBE cannot generate ﬁngerprints when there is no known
live C&C server to experiment with (thus no network interactions
can be observed) or when the known C&C servers are only alive
for a very short time (thus not enough trafﬁc for building reliable
ﬁngerprints).

Dynamic binary analysis has been previously used by PEER-
PRESS to generate ﬁngerprints for P2P malware [41]. However,
PEERPRESS cannot be used to detect remote malicious servers.
It can only generate ﬁngerprints for malware that embeds some
server-side logic and listens on the network for incoming requests
such as P2P bots. Instead, the majority of malware families use
a pull-based C&C protocol, where bots contain only client-side
logic, send periodic requests for instructions to the remote C&C
servers, and close the communication after the response from the
C&C server is received. Pull-based C&C is the dominant choice
because it avoids incoming probes being blocked by NAT gateways
and ﬁrewalls. To build ﬁngerprints for remote servers PEERPRESS
would require the C&C server software, which is not available.

AUTOPROBE greatly complements PEERPRESS.

It enables
generating ﬁngerprints for identifying C&C servers for malware
that has only client-side logic, extending active probing beyond
P2P bots to also include C&C servers.

AUTOPROBE applies dynamic binary analysis to achieve pro-
found understanding on the packet semantics and deeper insight on
the malware’s logic for request generation (to remote servers) and
response handling (back from the servers) in the following ways.

First, in analyzing (outgoing) request generation logic, AUTO-
PROBE focuses on two tasks: (1) It tracks the generation of variant
bytes, whose value may change in a different environment, and
their semantics. Through re-generating variant bytes in realistic
environments, AUTOPROBE obtains a more accurate probe request.
(2) It analyzes the logic to uncover as many request generation
paths as possible. Thus, AUTOPROBE can generate more probing
requests than existing approaches.

Second, in analyzing (incoming) response handling logic, AU-
TOPROBE employs a novel scheme for detection,
i.e., AUTO-
PROBE identiﬁes speciﬁc response bytes that can affect client-
side malware’s execution as the evidence to detect malicious
servers. More speciﬁcally, AUTOPROBE applies dynamic symbolic
execution to ﬁnd a set of path constraints and generates light-weight
network-level symbolic-constraint-based ﬁngerprints for detection.
Furthermore, AUTOPROBE can generate ﬁngerprints even when a
remote server is not alive, thus no response can be received by the
malware client, an unsolved challenge for existing approaches.

Our paper makes the following contributions:
• We propose a novel approach for automatically generat-
ing active probing ﬁngerprints, which can detect remote
malicious servers. Compared with prior work [25, 41],
our approach leverages dynamic binary analysis,
is able
to generate ﬁngerprints for the large number of malware
families that use pull-based C&C protocols, and works even
when no live C&C server is available for training.
• We have implemented our approach into AUTOPROBE, a tool
that uses a novel combination of dynamic analysis techniques
including taint tracking, dynamic slicing, and symbolic ex-

ploration for producing accurate and high coverage probe
generation, port selection, and classiﬁcation functions.
• We conduct an extensive evaluation of AUTOPROBE with
real-world malware families. We show that AUTOPROBE can
successfully generate on average 2 ﬁngerprints per malware
family (regardless of whether the remote servers are alive
or not). Furthermore, AUTOPROBE has successfully and
quickly found hundreds of live malware servers on the
Internet, most unknown to existing blacklists.

2. PROBLEM STATEMENT AND OVERVIEW

Active probing (or network ﬁngerprinting) is a powerful ap-
proach for classifying hosts that
listen for incoming network
requests into a set of pre-deﬁned classes based on the networking
software they run. In a nutshell, active probing sends a probe to
each host in a set of targets, and applies a classiﬁcation function
on the responses from each of those target hosts, assigning a class
to each host. Given some target network software to detect, a
ﬁngerprint captures how to build the probe to be sent, how to
choose the destination port to send the probe to, and how to classify
the target host based on its response.

The problem of active probing comprises two steps: ﬁngerprint
generation and scanning. This paper focuses on the ﬁngerprint
generation step, proposing a novel approach to automatically build
ﬁngerprints for detecting malware servers. Our approach assumes
the availability of a malware sample and applies dynamic binary
analysis on the malware to build the ﬁngerprint.
2.1 Motivation

Our program analysis approach to ﬁngerprint generation ad-

dresses the following challenges that existing approaches suffer.
Produces valid C&C probes. In existing approaches, the candi-
date probes to be sent to the remote hosts are manually selected
using protocol domain knowledge [7], generated randomly [7],
or selected from prior messages the malware has been observed
to send [25]. However, these three approaches are problematic.
First, domain knowledge is not available for most C&C protocols.
Second, randomly generated probes are most likely invalid because
they do not satisfy the C&C protocol syntax and semantics. A
remote C&C server is likely to refuse responding to invalid probes
and the malware owners may be alerted by the invalid requests.
Third, previously observed malware requests may be invalid when
replayed at a different time and/or machine. For example, Figure 1
shows a Win32/Horst.Proxy malware request that includes the bot’s
IP address and an open port where it runs a Socks proxy. If the
values of these ﬁelds do not match with the sender’s, the C&C
server can detect such inconsistency and refuse to respond.

Figure 1: Request of Win32/Horst.Proxy

In another example, Win32/ZeroAccess [40] encodes the bot’s IP
address and OS information in an obfuscated URL (Figure 2). Iden-
tifying state-dependent ﬁelds, even when obfuscated, represents a
great challenge for existing network-based approaches [7, 25].
Explores the space of valid C&C probes. CYBERPROBE is
limited to using probes that have been previously observed being
sent by the malware. However, those requests are often only a

180181182call.
If it is not inﬂuenced then it keeps processing backwards
until it ﬁnds the next branch. When it ﬁnds a branch that has
been inﬂuenced by the output of a system call (line 3)1 it forces
the system call to generate an alternative result, i.e., it forces
the conditional to take the branch not explored in the trace.
In
our example, if in the original trace RegOpenKeyEx returned
SUCCESS, it forces the function to return FAILURE so that the
other execution branch is executed. This process stops when the
beginning of the execution is reached or a conﬁgurable maximum
number of system-call-inﬂuenced branches has been found (100 by
default). Control-ﬂow-based exploration is detailed in Algorithm 1.

Θ: Trace
ins: instruction in trace
Φ: Set of Instruction of Conditional Branches
Δ: Set of Labeled System Call Output Memory/Register
T : Set of Tainted Memory/Register
F : Set of System Calls Affecting Control Flow
req: Request Sent by Malware
for insi in Θ do

if insi in Φ then
eﬂags → T
Backward Taint eﬂags
if tainted ∈ Δ then

Record System Call into F
Clean eﬂags

end

end

end
for f un in F do

for output:oi of f un’s outputs do
if oi changes control ﬂow then

Rerun malware
Enforce oi for f un along execution
Collect new trace Θi Collect new reqi

end

end

end
Algorithm 1: Algorithm for Control-ﬂow-based Exploration

3.2 Trace Analysis

The analysis of an execution trace that produced a network
request comprises 3 steps: identify the variant bytes in the request
and the target port, recover the semantics of variant bytes in the
request, and generate a regeneration slice for the variant bytes in
the request and the port.
Identify variant parts and their semantics. The request is
commonly a combination of invariant and variant bytes. To identify
variant bytes in the request AUTOPROBE applies dynamic slicing to
each of the bytes in the request starting from the function that sends
the request. Note that while each byte slice is independent they can
be performed in parallel on a single backwards pass on the trace for
efﬁciency. If the slice ends in a ﬁxed constant such as an immediate
value or a constant in the data section then the byte is considered
invariant.
If the slice ends in the output of an API call with
known semantics and whose output is inﬂuenced by a system call
(e.g., rand), it is considered variant. In this case, AUTOPROBE
clusters consecutive bytes inﬂuenced by the same API call (e.g.,
all consecutive bytes in the request inﬂuenced by rand()) into
variant ﬁelds. Then it labels those variant ﬁelds using the semantic
information on the API call collected from public repositories
(e.g., MSDN). Some examples of semantic labels are time, ip,
random, and OS version. Currently, AUTOPROBE has semantics
information for over 200 Windows system and library calls. The
handling of the port selection is similar but it starts at the function

1Or an API call known to perform a system call
gOpenKeyEx

like Re-

that selects the port (e.g., connect, sendto) and since the port
is an integer value, AUTOPROBE slices for all bytes that form the
integer simultaneously.
Reconstruction slices. For each variant ﬁeld in the request the
probe construction function captures how the variant ﬁeld needs
to be updated as a function of the scanner’s environment (e.g., the
current time). For this, AUTOPROBE applies dynamic slicing on
the previously identiﬁed variant bytes. The slice contains both data
and control dependencies. For control dependencies, AUTOPROBE
conservatively includes in the slice the eflags register value
for each branch instruction it encounters that may inﬂuence the
generation of the variant bytes. The slice ends when all variant
bytes are traced back to some semantic-known system calls or the
trace start is reached. The slice is a program that can be re-executed
using the current local environment (e.g., local IP, MAC address, or
time) to reconstruct the ﬁeld value.

4. CLASSIFICATION FUNCTION

CONSTRUCTION

To build the classiﬁcation function, AUTOPROBE conducts dy-
namic binary analysis on the malware’s response handling to ex-
tract a set of symbolic equations. Figure 6 depicts the architecture
of the classiﬁcation function construction. The intuition behind
this phase is that the malware’s processing of a response typically
comprises two widely different logic to handle valid and invalid
responses (without differentiating them the malware could be
controlled by arbitrary messages, which is certainly not desirable
by the malware author).

For example, if the response is considered valid, the malware
may continue its communication with the remote C&C server, but
if considered invalid it may close the communication or re-send
the previous request. To verify the validity of a response, the
malware parses it and checks the values of some selected ﬁelds.
Such validation checks are branches that depend on the content of
the response. Each check can be captured as a symbolic formula
and their conjunction can be used as a classiﬁcation function.

Figure 6 shows the workﬂow of the classiﬁcation function
construction. First, it collects different responses, which can come
from live C&C servers the malware contacted during execution
or be produced by the fuzzing module. Second, it executes the
malware (devil icon) with those responses, applying symbolic
execution and path exploration analysis to identify a valid response.
Last, it generates the classiﬁcation function for each valid request
and response repair.

The remainder of this section describes the classiﬁcation func-
tion construction when a C&C response was obtained during
malware execution, which is illustrated in the left side of Figure 6
(Section 4.1) and when no response is available, which is illustrated
in the right side of Figure 6 (Section 4.2).
4.1 With a C&C Response

To distinguish between valid and invalid responses AUTOPROBE
focuses on the differences between validation checks on both types
of responses. For example, a valid response will successfully go
through all validation checks but an invalid response will fail at
least one of those checks producing an execution trace with a
smaller number of content-dependent branches.

This case comprises 3 steps shown in the left part of Figure 6.
First, AUTOPROBE marks as symbolic each byte in the response
received from the server during the original malware execution
and performs symbolic execution on those symbols along the
execution. For each branch inﬂuenced by the input symbols (i.e.,

183184cious servers. For TCP ﬁngerprints, the scanner ﬁrst performs a
horizontal SYN scan to identify hosts with the target port open.
For each target host listening on that port, the scanner uses the
slices to regenerate the values of the state-dependent ﬁelds in
the request, sends the updated request to the target, and records
its response. UDP ﬁngerprints are handled similarly except that
horizontal scanning is not needed.

Our response classiﬁcation module takes as input the symbolic
equations in the ﬁngerprint and the concrete target response, and
conducts symbolic-equation-based matching.
If the request is
generated from our non-response analysis, the detection result is
a suspicious score,

λ =

# of matched equations

# of equations

The higher λ,
the more likely the target server is malicious.
Otherwise, if the request is generated from concrete (live) server’s
response, we require the response to satisfy all the symbolic
equations to declare detection.

6. EVALUATION

In this section, we ﬁrst evaluate AUTOPROBE for generating
ﬁngerprints of real-world malware samples. Then, we use the
ﬁngerprints to scan for malicious servers.

Malware collection. We collect recent malware from 56 fam-
ilies broken into two datasets. Dataset I contains 37 popular and
notorious malware families including Sality [13], ZeroAccess [40],
Ramnit [30], Bamital [4], and Taidoor [34]. We are able to
collect 10 different variants (with different MD5) for each family
from public malware repositories [22, 27], thus making a total of
370 malware binary samples in Dataset I. Dataset II contains 19
malware families used by CYBERPROBE. We use Dataset II to
compare the accuracy of the ﬁngerprints produced by AUTOPROBE
with the ones produced by CYBERPROBE.

Malware execution. we run the malware for 5 minutes each on
a virtual machine with Intel Core Duo 1.5 GHz CPU and 8 GB
memory. Each run outputs an execution trace that serves as the
starting point for the ﬁngerprint generation.

Scanning setup. We use 5 machines for probing. All machines
run GNU/Linux Ubuntu 12.1 LTS with dual core 2.2 GHz CPUs
and the memory conﬁguration ranges from 2 GB to 16 GB.
6.1 Evaluation of Probe Generation

In Table 1, we summarize the results from probe generation.
We collect malware’s execution/network traces and conduct the
analysis. First, AUTOPROBE analyzes the network traces, extracts
all the malware’s network requests, and ﬁlter out those requests
sent to domains in the Alexa top 10,000 list [2]. The number
of Remaining/Original requests are shown in Table 1 in the R/O
column. Then, for each dataset, the table splits the malware into
two groups corresponding to whether at least one request received
a response from a remote server (ResponseSeen), or all requests
failed to receive a response (NoResponse). For each group, the
table shows the number of requests produced by the malware in the
group during the executions and the number of probes produced
by AUTOPROBE, split into probes that contain some variable parts
and those that have only constant parts. The last column shows the
maximum number of probes that CYBERPROBE can produce for
the group.

All requests are HTTP and on average it takes AUTOPROBE 13.2
minutes to analyze/process one execution trace, relatively slow but
a reasonable cost for off-line analysis tools.

AUTOPROBE generated a total of 105 ﬁngerprints/probes for all
56 malware families in the two datasets. Since multiple requests
may be generated by the same execution path, the total number
of probes is smaller than the number of requests captured on the
network. We also observe that the majority of generated probes
contain some variable parts. This means dynamic binary analysis
enables AUTOPROBE to extract more complete probe generation
functions than network-based approaches, because the variable
parts in the probe generation functions provide higher coverage.

Note that on both datasets, AUTOPROBE can generate ﬁnger-
prints for all the malware, even those with no response, for which
CYBERPROBE cannot. This demonstrates a clear advantage of
AUTOPROBE. For the samples with a response in Dataset II,
CYBERPROBE is able to generate a ﬁngerprint similar to AUTO-
PROBE. However, for 57% of those, AUTOPROBE produces probes
construction functions with variable ﬁelds rather than concrete
probes in CYBERPROBE. Thus, AUTOPROBE probe construction
functions are potentially more accurate. We also ﬁnd 4 cases in
which requests clustered together by CYBERPROBE are indeed
generated by different logic in the malware. Thus, they should have
been considered different as their responses are not guaranteed to
have the same format.

6.2 Evaluation of Classiﬁcation Function

In this section, we ﬁrst verify how our heuristics of classiﬁcation
function work in the real world, i.e., whether malware behaves
differently when fed with valid and invalid responses. To verify
that, we extract all 76 probes that trigger responses from the live
remote servers. We also generate 76 random responses, which
comprise of HTTP 200 response code, a common HTTP header
and some arbitrary bytes in the payload. We feed our generated
responses to the malware and compare the malware execution
with the cases when the valid response from live remote servers
is received. Among all 76 test cases, we ﬁnd that in 71 cases
(93%) malware has noticeable behavior differences (malware will
typically execute over 10 more system calls and over 50 more
code blocks when receiving valid responses). Then we manually
examine the remaining 5 exceptional cases.
It turns out that all
these remotes servers are not malicious any more: four of them are
veriﬁed as sinkhole domains and the last one returns a 404 error
response (possibly server already cleaned). From this experiment,
we reasonably believe that our heuristics work well for most of
malware communications.

In our evaluation, AUTOPROBE generates a total of 70 classi-
ﬁcation functions for all ResponseSeen cases and 31 for the 29
NoResponse cases. The reason why we have more classiﬁcation
functions than the number of cases is because some malware probes
can generate different responses to trigger different malware be-
haviors. This further demonstrates the advantage of AUTOPROBE
because existing work cannot generate such probing.

The matching efﬁciency is important for the classiﬁcation func-
tion. For the ResponseSeen cases,
the detection requires that
all symbolic equations in the classiﬁcation function match, so
AUTOPROBE can ﬁnish matching when any of the equations fails to
match. For the NoResponse cases, it calculates the suspicious score
based on the matching results for all equations. For efﬁciency, our
scanner records the response trafﬁc and conducts ofﬂine matching.
It
shows the time consumed for classifying 1,000 responses. For the
ResponseSeen cases, on average, the classiﬁcation function consists
of 17 equations and takes 251 ms to complete the matching. The
worst case is one classiﬁcation function that consists of 36 equation
comparisons (CP) and takes 757 ms to parse 1,000 responses.

Table 2 summarizes the classiﬁcation function efﬁciency.

185186Matching Scheme Worst (CP) Worst (ms)
757
1,923

ResponseSeen
NoResponse

36
67

Best (CP)
9
37

Best (ms)
102
483

Avg. (CP) Avg. (ms)
251
973

17
50

Table 2: Efﬁciency of Classiﬁcation Functions (time measured when handling 1000 continuous responses). Here CP denotes the
number of equation comparisons.

ID
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28

Scan Date
2013-11-03
2013-11-03
2013-11-03
2013-11-03
2013-11-03
2013-11-03
2013-11-08
2013-11-08
2013-11-08
2013-11-08
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2013-11-10
2014-02-17
2014-02-17
2014-02-17
2014-02-17

DataSet
II
II
II
II
II
II
II
II
II
II
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I
I

Port
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80
80

Time
# Scanners
2.3h
3
2.4h
3
2.4h
3
2.3h
3
2.8h
3
3.2h
3
2.6h
3
2.7h
3
1.2h
3
1.8h
3
3.3h
2
3.8h
2
4.1h
2
3.2h
2
3.8h
2
3.9h
2
3.6h
2
3.2h
2
3.3h
2
3.5h
2
3.3h
2
3.7h
2
3.1h
2
3.0h
2
3.6h
2
3.9h
2
4.1h
2
2
3.8h
TOTALS:

Resp.
64%
64%
64%
64%
64%
64%
63%
63%
63%
63%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%
64%
60%
60%
60%
60%

Found
6
4
5
4
2
9
2
1
0
0
32
12
3
3
17
5
9
11
0
4
3
0
8
1
11
7
4
9
172

Known

4
3
2
2
2
4
2
1
0
0
12
3
0
1
4
4
5
4
0
2
1
0
8
1
3
5
3
5
81

New
2
1
3
2
0
5
0
0
0
0
20
9
3
2
13
1
4
7
0
2
2
0
0
0
8
2
1
4
91

VT MD
2
1
0
0
0
0
0
0
0
0
0
1
1
0
1
1
0
0
0
0
0
1
1
1
0
0
0
1
0
2
0
0
0
0
1
1
0
0
0
0
1
1
0
1
1
1
0
0
0
3
2
1
1
3
2
3
24
11

UQ
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
1
1
0
3

Table 3: Localized Scanning Results of AUTOPROBE.

6.5

Internet-wide Scanning and Comparison
with CYBERPROBE

We next conduct Internet-wide scanning and compare the results
with CYBERPROBE. To minimize the impact to the whole Internet
because of our scanning while still clearly verifying the effective-
ness of AUTOPROBE, instead of scanning all ﬁngerprints, we focus
on three malware families (soft196, ironsource, optinstaller) also
scanned by CYBERPROBE [25].

Since these 3 malware families use HTTP C&C, we ﬁrst perform
an Internet-wide horizontal scan of hosts listening on the target
port 80.
For the horizontal scan, we collect the BGP table
from RouteViews and compute the total number of advertised IP
addresses. We conducted two horizontal scans on November 4,
2013 and February 19th, 2014. Both are summarized in Table 4.
We limit the scan rate to 60,000 packets per second (pps) for good
citizenship. The scan takes 2.9 hours and we ﬁnd over 71 million
live hosts listening on port 80.

After obtaining this 71 million live HTTP server list, we per-
formed 3 scans using the ﬁngerprints from AUTOPROBE and
CYBERPROBE for the three selected malware families. Table 5
summarizes the comparison. The top part of the table has the
results for the CYBERPROBE scans (CP-x) and the bottom part
the results for AUTOPROBE (AP-x). Each row corresponds to one
scan. Similar to the localized scanning, we also compare the results
with popular blacklist databases: VirusTotal (VT) [36], Malware
Domain List (MD) [23] and URLQuery (UQ) [35].

The results show that for every malware family the ﬁngerprints
produced by AUTOPROBE ﬁnd more servers than the one produced

by CYBERPROBE. Overall, AUTOPROBE has found 54 malware
servers, versus 40 malware servers found by CYBERPROBE, which
represents a 35% improvement. Finally, we also conduct ﬁve
additional Internet-wide scans for probes that cannot be generated
by CYBERPROBE, i.e., those from the NoResponse malware
server cases. The results are summarized in Table 6. They show
that AUTOPROBE can detect 83 malware servers and most of them
(80%) are new servers. Compared with CYBERPROBE, which
cannot generate any probe for NoResponse cases, AUTOPROBE has
a unique advantage and complements existing work well.
False positives and false negatives. Given the lack of perfect
ground truth, to measure false positives we check whether the
malware can establish successful communication with the detected
remote servers and whether a server’s response successfully trig-
gers the malware’s malicious logic. In particular, for each detected
server, we conduct another round of veriﬁcation by redirecting
the malware’s request to the detected servers and monitor the
malware’s execution afterwards. If the malware’s execution goes
into the behaviors we found in the analysis phase, we consider it a
true positive. In our test, we do not ﬁnd any false positive.

We cannot properly measure false negatives as the total number
of malicious servers is unknown.
Instead, we use the detection
result of CYBERPROBE as the ground truth to check that AU-
TOPROBE does not miss servers found by CYBERPROBE. The
result shows that AUTOPROBE can correctly detect all the servers
in CYBERPROBE using different signatures for the same families.
We further discuss potential false positives and false negatives in
Section 7.

187HID Type

1
2

I
I

Start Date
2013-11-04
2014-02-19

Port
80
80

Targets
2,528,563,104
2,659,029,804

# Scanners Rate(pps)
60,000
50,000

4
4

Time
2.9h
3.5h

Live Hosts
71,068,585 (2.8%)
71,094,003(2.8%)

Table 4: Horizontal scanning results.

ID
CP-1
CP-2
CP-3

AP-1
AP-2
AP-3

Scan Date
2013-11-06
2013-11-06
2013-11-08

2013-11-08
2013-11-08
2013-11-08

Port
80
80
80

Fingerprint
soft196
ironsource
optinstaller

80
80
80

soft196
ironsource
optinstaller

SC
2
2
2

Time
24.6h
24.6h
24.6h

Resp.
91%
92%
90%

CYBERPROBE TOTALS:

25.3h
25.3h
25.3h

2
2
2
AUTOPROBE TOTALS:

90%
92%
90%

Found Known New
1
4
16
21
1
4
16
21

9
11
20
40
13
17
24
54

8
7
4
19
8
6
5
19

VT MD UQ
0
1
4
0
0
6
0
11
0
3
0
9
9
0
0
21

0
1
0
1
1
2
2
5

Table 5: Comparison of malware servers found using AUTOPROBE and CYBERPROBE for three malware families. Here CP-x denotes
CYBERPROBE and AP-x denotes AUTOPROBE.

7. DISCUSSION

PROBE.

We now discuss limitations and possible evasions of AUTO-

Possible false positives and false negatives. As discussed in
Section 6, we do not ﬁnd any false positives and false negatives in
our detection result. The lack of false positives may be due to our
strict criteria to determine that a server is malicious. For example,
we ensure the response can indeed trigger malware to download
malicious ﬁle or send some response. However, since our criteria of
detecting malicious server purely depends on malware’s behaviors,
lacking of full and precise understanding of malware logic could
introduce inacuraccies. For example, malware may download one
malicious ﬁle from the server and its follow-up logic may depend
on the download success. However, if the malware execution does
not capture the malware using such ﬁle in the limited monitoring
time, AUTOPROBE may directly treat any server hosting this ﬁle
as malicious. The root cause of such false positive/negative is a
fundamental limitation of dynamic analysis:
it can only observe
behaviors executed. To improve and provide more accurate result,
we should provide longer analysis time and improve code coverage.
Malware checks on responses. Our classiﬁcation function
construction assumes that the malware will behave differently when
receiving valid and invalid responses from remote servers. If the
malware violates this assumption, i.e., performs no checks or only
cursory checks on the responses, the generated ﬁngerprints may
produce false positives when probing benign servers. However, this
situation does not arise in our examples and we believe it is unlikely
as it would be extremely easy to inﬁltrate such C&C protocol.

Classiﬁcation function through code reuse. The classiﬁcation
function produced by AUTOPROBE is a logic expression applied
on the response or the output of a parser on the response. Those
expressions are difﬁcult to extract if the variables follow non-
linear relations.
In those cases we could apply binary code
reuse techniques [5, 20] to directly (re)use the malware’s reponse
handling code. In the extreme case, AUTOPROBE could rerun the
malware in the controlled environment on the responses received
from target servers. Obviously, such approaches are expensive,
so they are better used only when our current approach cannot
determine a symbolic expression.

Semantics-guided fuzzing. The ﬁngerprints produced by AU-
TOPROBE use valid probes that satisfy the C&C protocol grammar
because the probe construction functions that generate them have
been extracted from the malware’s request generation logic. How-
ever, for some families it may be possible to generate additional
ﬁngerprints using invalid probes that do not satisfy the C&C

grammar but still trigger a distinctive response from the C&C
servers. Invalid probes are easier to be identiﬁed by the C&C server
managers but may be useful when the C&C masks as a benign
protocol. When a live C&C server is known, AUTOPROBE could be
enhanced with a semantics-guided fuzzing approach that uses the
semantic information extracted during probe generation to modify
valid probes into invalid and test them against the C&C server.

Dynamic analysis limitations. The dynamic analysis tech-
niques used by AUTOPROBE are known to have some limita-
tions.
For example, dynamic taint analysis is known to be
vulnerable to over-tainting and under-tainting [33], which may
introduce inaccuracies in our detection of variable parts during
probe generation. Similarly, symbolic execution is challenging in
the presence of complex loops [32] and implicit ﬂows [18], and
may explore unreachable paths [33]. We admit all these issues can
affect the performance of AUTOPROBE. However, these issues
are not speciﬁc to AUTOPROBE and affect in some degree all
dynamic analysis solutions. More importantly, AUTOPROBE takes
steps to minimize the effect of those challenges. For example,
AUTOPROBE does not need to analyze the complete malware logic
but only its request generation and response handlig logic.
It
can conﬁrm that paths build requests by monitoring that indeed a
request is observed on the network. Furthermore, even if dynamic
analysis marks some request parts as variable, AUTOPROBE still
does backward slicing on those bytes verifying that they are indeed
generated from the output of system/API calls. Clearly, any future
advances in dynamic binary analysis will also beneﬁt our approach.
Handling encrypted trafﬁc. In the evaluation, we ﬁnd around
30% malware samples use encoded packets to communicate with
their remote servers. While in current AUTOPROBE we do no
decode these encrypted trafﬁc (a common research challenge in
this area, and out of the scope of this paper), AUTOPROBE can
observe malware’s logic of handling correctly-encoded response
and incorrectly-encoded response. In particular, we can generate
some random response packet and record the malware execution
path, which represents malware’s logic of handling invalid packet.
If any response packet deviates malware’s execution from this path,
we think the source of the packet is likely suspicious.

Other possible evasions. Among possible evasions, one is to
use some existing exploits as the client request. AUTOPROBE needs
to ﬁlter out all the requests that exploit remote servers and malware
authors could use that to prevent being tampered by AUTOPROBE.
However, using exploits for remote communication increases the
probability of being detected by existing IDS systems. Another
possible evasion is to use coordinated servers since AUTOPROBE

188ID
AP-1
AP-2
AP-3
AP-4
AP-5

Scan Date
2013-11-06
2013-11-06
2013-11-08
2014-02-23
2014-02-23

Port
Fingerprint
80
Sality
80
Taidoor
80
Bamital
80 Vidgrab
80 Horst

SC
5
5
5
5
5

Time
12.1h
13.2h
12.6h
13.4h
13.9h

Resp.
90%
91%
92%
94%
94%

Found Known New
20
10
10
15
11
66

23
14
11
21
13
82

3
4
1
6
2
16

VT MD UQ
0
1
0
2
2
0
0
3
0
2
8
0

0
1
0
1
1
3

AUTOPROBE TOTALS:

Table 6: Additional 5 scanning results of AUTOPROBE for NoResponse cases.

does not correlate trafﬁc to different servers. Malware authors
may allow one server to receive a request, forward it to another
server, and allow the other server to issue commands. This scheme
deﬁnitely increases the maintenance cost for botmasters. Some
existing IDS systems such as BotHunter [15] could complement
AUTOPROBE in some situations.

8. RELATED WORK

Research on Internet-wide probing. Scanning the internet is
one way to ﬁnd large-scale network-level vulnerabilities. Provos et
al. scanned Internet to identify vulnerable SSH servers through
vulnerability signatures [29]. Dagon et al.
[11] scanned DNS
servers on Internet to ﬁnd those providing incorrect resolutions.
Heninger et al.
[16] scanned the Internet to ﬁnd network devices
with weak cryptographic keys. All these studies apply some
widely-known signatures to achieve the purpose.

Different from them, active probing to detect network-based
malware has been proposed in several previous work [3, 14, 25, 28,
41]. In [14], Gu et al. proposed to actively send probing packets
through IRC channels. Zmap [12] is another internet-wide scanner
which is efﬁcient enough to scan the whole internet in less than 45
minutes. However, it targets to test the aliveness of remote hosts
instead of detecting possible malicious servers.

PeerPress [41] is one related work that also adopts dynamic
malware analysis to ﬁnd P2P malware’s network ﬁngerprints.
Nevertheless, as we have stated the difference earlier, the target
of such probing is on the malware samples that actively open the
port for communication, such as P2P malware and Trojan Horse.
AUTOPROBE targets at remote malicious servers and we assume
the server-side logic is not available for analysis in collected
binaries, a different assumption from PeerPress.

Research on network ﬁngerprint generation. Fingerprinting
network applications is a widely studied topic. Botzilla [31] is a
method for detecting malware communication through repetitively
recording network trafﬁc of malware in a controlled environment
and generating network signatures from invariant content pat-
terns. AUTOPROBE has a different goal of ﬁngerprinting malicious
servers and adopts binary-level analysis to ﬁnd the invariant part in
packets.

FiG [7] proposed a framework for automatic ﬁngerprint genera-
tion that produces OS and DNS ﬁngerprints from network trafﬁc.
In contrast, AUTOPROBE applies a different approach for automatic
ﬁngerprint generation that takes as input a malware sample and
applies dynamic binary analysis on the malware’s execution.

Research on malware binary analysis. There are multiple
existing studies that discuss effective and efﬁcient techniques for
malware analysis. Such techniques include taint analysis [19, 26],
enforced execution [38], path exploration [24], program slicing [5],
symbolic execution [37] and trace alignment [17]. AUTOPROBE
applies many of these techniques in our new problem domain in a
novel way to automatically generate network ﬁngerprints.

Among all studies on binary analysis, protocol reverse engi-
neering work, such as [8–10, 21, 39], is also closely related to

AUTOPROBE. We adopt similar approach as in [6] to ﬁgure
out the semantics meanings of malware’s request. However,
one difference between AUTOPROBE and existing work is that
AUTOPROBE does not attempt to understand the complete protocol
of malware’s communication, and AUTOPROBE uses many other
different techniques to aid the generation of ﬁngerprints.

In short, the above studies are complementary to our work.
AUTOPROBE will greatly beneﬁt from the advances in these ﬁelds.

9. CONCLUSION

In this paper, we present AUTOPROBE, a novel tool to gener-
ate active probing ﬁngerprints for Internet-wide malicious server
detection. AUTOPROBE implements a novel dynamic analysis
approach to improve the effectiveness and efﬁciency of existing
work. The dynamic analysis can help expose more requests,
build a classiﬁcation function on a server’s response based on
the malware validation checks, and assist in efﬁcient detection.
Furthermore, AUTOPROBE proposes new solutions for some real-
world challenges such as generating ﬁngeprints when no live C&C
server is known. We also show that AUTOPROBE can generate
more accurate network ﬁngerprints for malicious servers probing.
In our extensive Internet-scale scanning, AUTOPROBE outperforms
the existing state-of-the-art system in discovering more malicious
servers.

10. ACKNOWLEDGMENTS

The authors would like to especially thank the system admin-
istrators at their respective institutions. We also thank Zakir
Durumeric and J. Alex Halderman, as well as VirusTotal, Malware
Domain List, and URLQuery. This material is based upon work
supported in part by the National Science Foundation under Grant
CNS-0954096 and the Air Force Ofﬁce of Scientiﬁc Research
under Grant FA9550-13-1-0077. This research is also partially sup-
ported by the Spanish Government through Grant TIN2012-39391-
C04-01 and a Juan de la Cierva Fellowship for Juan Caballero. Any
opinions, ﬁndings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reﬂect the views of the sponsors.

11. REFERENCES
[1] Dirtjumper. http:

//www.infonomics-society.org/IJICR/DirtJumper.

[2] Alexa Top Domains. http://www.alexa.com/.
[3] Oﬁr Arkin. A remote active os ﬁngerprinting tool using icmp. ;login:

The USENIX Magazine, 27(2), November 2008.

[4] Bamital Malware.

https://now-static.norton.com/now/en/pu/
images/Promotions/2013/Bamital/bamital.html.
[5] Juan Caballero, Noah M. Johnson, Stephen McCamant, and Dawn

Song. Binary code extraction and interface identiﬁcation for security
applications. In Network and Distributed System Security
Symposium, San Diego, CA, February 2010.

[6] Juan Caballero, Pongsin Poosankam, Christian Kreibich, and Dawn
Song. Dispatcher: Enabling active botnet inﬁltration using automatic

189protocol reverse-engineering. In ACM Conference on Computer and
Communications Security, Chicago, IL, November 2009.

[7] Juan Caballero, Shobha Venkataraman, Pongsin Poosankam, Min G.

Kang, Dawn Song, and Avrim Blum. ﬁg: Automatic ﬁngerprint
generation. In Network and Distributed System Security Symposium,
San Diego, CA, February 2007.

[8] Juan Caballero, Heng Yin, Zhenkai Liang, and Dawn Song. Polyglot:

Automatic extraction of protocol message format using dynamic
binary analysis. In ACM Conference on Computer and
Communications Security, Alexandria, VA, October 2007.

[9] Paolo Milani Comparetti, Gilbert Wondracek, Christopher Kruegel,

and Engin Kirda. Prospex: Protocol speciﬁcation extraction. In IEEE
Symposium on Security and Privacy, Oakland, CA, May 2009.

[10] Weidong Cui, Jayanthkumar Kannan, and Helen J. Wang.

Discoverer: Automatic protocol description generation from network
traces. In USENIX Security Symposium, Boston, MA, August 2007.
[11] David Dagon, Chris Lee, Wenke Lee, and Niels Provos. Corrupted
dns resolution paths: The rise of a malicious resolution authority. In
Network and Distributed System Security Symposium, San Diego,
CA, February 2008.

[12] Zakir Durumeric, Eric Wustrow, and J. Alex Halderman. Zmap: Fast

internet-wide scanning and its security applications. In Usenix
Security Symposium, August 2013.

[13] Nicolas Falliere. Sality: Story of a peer-to-peer viral network.

Technical report, 2011.

[14] Guofei Gu, Vinod Yegneswaran, Phillip Porras, Jennifer Stoll, and

Wenke Lee. Active botnet probing to identify obscure command and
control channels. In Proceedings of 2009 Annual Computer Security
Applications Conference (ACSAC’09), December 2009.

[15] Guofei Gu, Junjie Zhang, and Wenke Lee. BotHunter: Detecting

Malware Infection Through IDS-Driven Dialog Correlation. In
Proceedings of USENIX Security’07, 2007.

[16] Nadia Heninger, Zagir Durumeric, Eric Wustrow, and J.Alex

Halderman. Mining your ps and qs: Detection of widespread weak
keys in network devices. In USENIX Security Symposium, 2012.
[17] Noah M. Johnson, Juan Caballero, Kevin Zhijie Chen, Stephen

McCamant, Pongsin Poosankam, Daniel Reynaud, and Dawn Song.
Differential slicing: Identifying causal execution differences for
security applications. In Proceedings of the 2011 IEEE Symposium
on Security and Privacy, 2011.

[18] Min Gyung Kang, Stephen McCamant, Pongsin Poosankam, and

Dawn Song. DTA++: Dynamic taint analysis with targeted
control-ﬂow propagation. In Proceedings of the 18th Annual Network
and Distributed System Security Symposium, San Diego, CA,
February 2011.

[19] Clemens Kolbitsch, Paolo Milani Comparetti, Christopher Kruegel,

Engin Kirda, Xiaoyong Zhou, and Xiaofeng Wang. Effective and
efﬁcient malware detection at the end host. In USENIX Security
Symposium, Montréal, Canada, August 2009.

[20] Clemens Kolbitsch, Thorsten Holz, Christopher Kruegel, and Engin

Kirda. Inspector gadget: Automated extraction of proprietary gadgets
from malware binaries. In IEEE Symposium on Security and Privacy,
Oakland, CA, May 2010.

[21] Zhiqiang Lin, Xuxian Jiang, Dongyan Xu, and Xiangyu Zhang.

Automatic protocol format reverse engineering through
context-aware monitored execution. In Network and Distributed
System Security Symposium, San Diego, CA, February 2008.

[22] Malicia. http://malicia-project.com/.

http://malicia-project.com/.

[23] Malware domain list. http://malwaredomainlist.com/.
[24] Andreas Moser, Christopher Kruegel, and Engin Kirda. Exploring
Multiple Execution Paths for Malware Analysis. In Proceedings of
IEEE Symposium on Security and Privacy, 2007.

[25] Antonio Nappa, Zhaoyan Xu, M. Zubair Raﬁque, Juan Caballero,

and Guofei Gu. Cyberprobe: Towards internet-scale active detection
of malicious servers. In Network and Distributed System Security
Symposium, 2014.

[26] James Newsome and Dawn Song. Dynamic taint analysis for

automatic detection, analysis, and signature generation of exploits on
commodity software. In Network and Distributed System Security
Symposium, San Diego, CA, February 2005.

[27] Offensive Computing. http://www.offensivecomputing.net/.

http://www.offensivecomputing.net/.

[28] Jitendra Padhye and Sally Floyd. Identifying the tcp behavior of web

servers. In SIGCOMM Conference, San Diego, CA, August 2001.
[29] Niels Provos and Peter Honeyman. Scanssh - scanning the internet

for ssh servers. In Technical Report CITI TR 01-13, University of
Michigan, October 2001.

[30] Ramnit Malware.

http://en.wikipedia.org/wiki/Ramnit.

[31] Konrad Rieck, Guido Schwenk, Tobias Limmer, Thorsten Holz, and

Pavel Laskov. Botzilla: Detecting the phoning home of malicious
software. In ACM Symposium on Applied Computing, 2010.

[32] Prateek Saxena, Pongsin Poosankam, Stephen McCamant, and Dawn

Song. Loop-extended symbolic execution on binary programs. In
Proceedings of the ACM/SIGSOFT International Symposium on
Software Testing and Analysis (ISSTA), 2009.

[33] Edward J. Schwartz, Thanassis Avgerinos, and David Brumley. All
you ever wanted to know about dynamic taint analysis and forward
symbolic execution (but might have been afraid to ask). In
Proceedings of IEEE Symposium on Security and Privacy, 2010.

[34] Taidoor Malware. Xpaj.b malware.

http://www.trendmicro.com/cloud-content/us/
pdfs/security-intelligence/white-papers/wp_
the_taidoor_campaign.pdf.

[35] Urlquery. http://urlquery.net/.
[36] Virustotal. http://www.virustotal.com/.
[37] Tielei Wang, Tao Wei, Guofei Gu, and Wei Zou. Taintscope: A

checksum-aware directed fuzzing tool for automatic software
vulnerability detection. In Proc. of IEEE S&P’10, 2010.

[38] Jeffrey Wilhelm and Tzi cker Chiueh. A forced sampled execution
approach to kernel rootkit identiﬁcation. In Proceedings of the 10th
international conference on Recent advances in intrusion detection,
2007.

[39] Gilbert Wondracek, Paolo Milani Comparetti, Christopher Kruegel,

and Engin Kirda. Automatic network protocol analysis. In
Proceedings of the 15th Annual Network and Distributed System
Security Symposium (NDSS), 2008.

[40] James Wyke. The zeroaccess botnet: Mining and fraud for massive

ﬁnancial gain, September 2012. http:
//www.sophos.com/en-us/why-sophos/our-people/
technical-papers/zeroaccess-botnet.asp:x.

[41] Zhaoyan Xu, Lingfeng Chen, Guofei Gu, and Christopher Kruegel.

Peerpress: Utilizing enemies’ p2p strength against them. In ACM
Conference on Computer and Communications Security, Raleigh,

NC, October 2012.

190