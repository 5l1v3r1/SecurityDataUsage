2016 IEEE Symposium on Security and Privacy
2016 IEEE Symposium on Security and Privacy

Prepose: Privacy, Security, and Reliability for

Gesture-Based Programming

∗
Lucas Silva Figueiredo

†
, Benjamin Livshits
∗

Federal University of Pernambuco

†
, David Molnar

†
, and Margus Veanes

†
Microsoft Research

!

Abstract—With the rise of sensors such as the Microsoft Kinect, Leap
Motion, and hand motion sensors in phones (i.e., Samsung Galaxy S6),
gesture-based interfaces have become practical. Unfortunately, today,
to recognize such gestures, applications must have access to depth and
video of the user, exposing sensitive data about the user and her environ-
ment. Besides these privacy concerns, there are also security threats in
sensor-based applications, such as multiple applications registering the
same gesture, leading to a conﬂict (akin to Clickjacking on the web).

We address these security and privacy threats with Prepose, a novel
domain-speciﬁc language (DSL) for easily building gesture recognizers,
combined with a system architecture that protects privacy, security, and
reliability with untrusted applications. We run Prepose code in a trusted
core, and only return speciﬁc gesture events to applications. Prepose
is speciﬁcally designed to enable precise and sound static analysis using
SMT solvers, allowing the system to check security and reliability
properties before running a gesture recognizer. We demonstrate that
Prepose is expressive by creating a total of 28 gestures in three
representative domains: physical therapy, tai-chi, and ballet. We further
show that runtime gesture matching in Prepose is fast, creating no
noticeable lag, as measured on traces from Microsoft Kinect runs.

To show that gesture checking at the time of submission to a gesture
store is fast, we developed a total of four Z3-based static analyses to
test for basic gesture safety and internal validity, to make sure the so-
called protected gestures are not overridden, and to check inter-gesture
conﬂicts. Our static analysis scales well
in practice: safety checking
is under 0.5 seconds per gesture; average validity checking time is
only 188 ms; lastly, for 97% of the cases, the conﬂict detection time
is below 5 seconds, with only one query taking longer than 15 seconds.

Introduction

1
Over 20 million Kinect sensors are in use today, bringing
millions of people in contact with games and other applica-
tions that respond to voice and gestures. Other companies
such as Leap Motion and Prime Sense are bringing low-
cost depth and gesture sensing to consumer electronics.
The newest generation of smart phones such as Samsung
Galaxy S5 supports rudimentary gestures as well.
Context of prior work: The security and privacy com-
munity is starting to pay attention to concerns created by
the emergence of these technologies. Speciﬁcally, we have
seen several proposals on the intersection of augmented
reality, privacy, and security. D’Antoni et al. [6] provides a
high-level overview of the problem space. Darkly [12], like
our work, puts a layer between the untrusted application
and raw sensor data. Unlike us, Darkly lacks a formal
semantics and does not allow precise reasoning about ap-
plication properties. Jana et al. [11] introduces the notion

of an OS abstraction called a recognizer which enables
gesture detection. Yet their approach fails to provide a
way to extend the system with new recognizers in a safe
manner. SurroundWeb [27] demonstrates what a 3D web
browser modiﬁed with new abstractions for input and
output to protect privacy and security would look like. Yet
it also lacks the capacity for precise automatic reasoning.
We are also inspired by world-drive access control [24],
which attempts to restrict applications from accessing
sensitive objects in the environment. Lastly, Proton [15]
is an example of deﬁning a higher-level abstraction for
gestures that enables precise reasoning.

1.1 Background
User demand for sensors such as Kinect is driven by
exciting new applications, ranging from immersive Xbox
games to purpose-built shopping solutions to healthcare
applications for monitoring elders. Each of these sensors
comes with an SDK which allows third-party developers
to build new and compelling applications. Several devices
such as Microsoft Kinect and Leap Motion use the App
Store model to deliver software to the end-user. Examples
of such stores include Leap Motion’s Airspace airspace.
com, Oculus Platform, and Google Glassware http://
glass-apps.org.

These platforms will evolve to support multiple un-
trusted applications provided by third parties, running
on top of a trusted core such as an operating system.
Since such applications are likely to be distributed through
centralized App stores, there is a chance for application
analysis and enforcement of key safety properties. Below
we describe some of the speciﬁc threats posed by applica-
tions to each other and to the user. We refer the reader
to D’Antoni [6] for a more comprehensive discussion of
threats. To address these threats, we introduce Prepose,
a novel domain speciﬁc language and runtime for writing
gesture recognizers. We designed this language with se-
mantics in terms of SMT formulas. This allows us to use
the state of the art SMT solver Z3 both for static analysis
and for runtime matching of gestures to user movements.

1.2 A Case for Controlled Access to Skeletal Data
There is a natural trade-oﬀ between the platform func-
tionality provided to potentially untrusted applications

2375-1207/16 $31.00 © 2016 IEEE
© 2016, Lucas Silva Figueiredo. Under license to IEEE.
DOI 10.1109/SP.2016.16
DOI 10.1109/SP.2016.16

122
122

and possible threats to the end-user. We take a two-
pronged approach to deliver a degree of security, privacy,
and reliability. Privacy is achieved through the use of a
domain-speciﬁc language Prepose, whereas security and
reliability are both achieved through the use of sound
static analysis. By combining system design and sound
static analysis, Prepose improves the security, privacy,
and reliability properties of gesture programming. We
discuss privacy-related issues in this section and security
and reliability in Section 1.3.

Prepose raises the privacy bar, keeping in mind that
perfect privacy is elusive. The degree to which end-users
are comfortable with privacy disclosure varies considerably
as well. Therefore it is important to analyze diﬀerent
points in the design space for untrusted applications that
use gesture recognition.

Figure 1 summarizes three diﬀerent levels of function-
ality for untrusted applications that need gesture recog-
nition. On the bottom, applications can be written in
languages such as C++ and have access to raw video and
depth. Access to the raw video stream is seen as highly
privacy-sensitive [11, 27]. In the middle, applications are
written in memory-safe languages such as C# or Java
and have access only to the skeleton API provided by
Kinect for Windows. What is less obvious is that at the
middle level, the skeleton data also leads to potential loss
of privacy. Speciﬁcally, the following attacks are possible
• The skeleton API reveals how many people are in the
room. This may reveal whether the person is alone
or not. If alone, perhaps she is a target for robbery;
if she’s found to be not alone, that may reveal that
she’s involved with someone illicitly.

• The skeleton API reveals the person’s height (relative
height of joints is exposed, and the Kinect API allows

Prepose 

and 

application logic

C# and skeletal data

C++ and raw data

Fig. 1: Three diﬀerent levels of data access for untrusted applica-
tions that perform gesture recognition. We call out threats to the
user at each levels.

123123

Category Property

Description

Reliability gesture safety

Reliability inner validity

validates that gestures have a basic
measure of physical safety, i.e. do
not require the user to overextend
herself physically in ways that may
be dangerous.

checks for inner contradictions i.e.
do not require the user to both keep
her arms up and down.

Security

protected gestures tests whether a gesture conﬂicts
with a reserved system-wide gesture
such as the Kinect attention gesture
(http://bit.ly/1JlXk79).

Security

conﬂicts

ﬁnds potential conﬂicts within a set
of gestures such as two gestures that
would both be recognized from the
same user movements.

Fig. 2: Properties statically checked by Prepose. The ﬁrst two
properties are reliability properties which aid gesture developers. The
second two are security properties that prevent untrusted applica-
tions from conﬂicting with the OS or with other applications.

mapping from skeleton points to depth space so actual
height as well). The application could distinguish
people by “ﬁngerprinting” skeletons.

• The skeleton API reveals ﬁne grained position of the
person’s hands. The application can in principle learn
something about what they write if they write on a
whiteboard, for example.

1.3 Static Analysis for Security & Reliability
At the heart of Prepose is the idea of compiling gesture
descriptions to formulae for an SMT solver such as Z3 [21].
These formulae capture the semantics of the gestures,
enabling precise analyses that boil down to satisﬁability
queries to the SMT solver. The Prepose language has
been designed to be both expressive enough to support
complex gestures, yet restrictive enough to ensure that
key properties remain decidable. In this paper we focus on
the four properties summarized in Figure 2 and detailed in
Section 3.4. Note that a gesture-based application written
in C++ or Java would generally require an extensive
manual audit to ensure the lack of privacy leaks and
security ﬂaws.

1.4 Threat Model
Prepose, at the top of the pyramid in Figure 1, provides
the next layer of privacy by mediating direct access to
the skeleton API. While the threats emanating from raw
video access and skeleton access are eliminated by design,
in Prepose we worry about higher-level properties such
as inter-gesture conﬂicts and gesture safety.

This is akin to how programming in a memory-safe
language allows one to focus on enforcing semantic security
properties without worrying about buﬀer overruns. As a
matter of security and privacy in depth, Prepose is at
the higher level within the pyramid, following the classic
security principle of least privilege.

As is often the case with privacy mechanisms, there are
some side channels that are harder to protect from. In

GESTURE c r o s s o v e r - l e f t - a r m - s t r e t c h :

POSE r e l a x - a r m s :

point your left arm down ,
point your right arm down .

POSE stretch :

rotate your left arm 90 degrees counter

c l o c k w i s e on the fr o n t a l p l a n e ,

touch your left elbow with your right hand .

E X E C U T I O N :

r e l a x - a r m s ,
slowly stretch and hold for 30 seconds .

Fig. 3: Gesture example: crossover-left-arm-stretch. A gesture
is composed of a sequence of poses. The gesture is completed if the
poses are matched in the sequence speciﬁed in the EXECUTION block.

our scenario, Prepose does not directly protect against
tracking the user by learning which gestures they can
perform (only some users are capable of certain gestures)
or whether, for example, their house is big enough by
testing if the user is able to perform gestures that require
a greater freedom of movement.

While we do not to attempt to catalog all the possible
attacks that may emerge [6], relying on Prepose gives us
conﬁdence that untrusted applications can do less harm
than if they had additional capabilities (lower within the
pyramid).

1.5 Prepose Architecture
The trusted core of Prepose enforces privacy by me-
diating between applications and the raw sensor data.
Inter-application conﬂicts and unsafe gestures are avoided
through static analysis powered by the Z3 SMT solver.
Figure 4 shows our architecture and the security boundary
we draw.

Gesture store: We are also inspired by App Stores for
developer components, such as the Unity 3D Asset store
which oﬀers developers the ability to buy models, object,
and other similar components (https://www.assetstore.
unity3d.com). Today, when developers write their own ges-
ture recognizers from scratch, they use machine learning
methods, or libraries from github and sourceforge. Our
focus in this paper is on gesture recognizers, which are
integral components of AR applications responsible for
detecting gestures performed by users.

As in the case of mobile apps, the App Store central-
ized distribution model provides a unique opportunity to
ensure the security and privacy of gestures before they are
unleashed on unsuspecting users. As such, our approach
in Prepose is to check gestures when they are submitted
to the gesture store.

Figure 5 summarizes our approach. Developers write
gesture recognizers in a high-level domain-speciﬁc lan-
guage, Prepose, then submit them to the gesture store.
Because our domain-speciﬁc language has been carefully
engineered, we can perform precise and sound static anal-
yses for a range of security and privacy properties. The re-
sults of this analysis tell us whether the submitted gesture
is “deﬁnitely OK,”“deﬁnitely not OK,” or, as may happen

Trust 

boundary

Skeleton

Prepose Code

Gesture Events

Prepose 
interpreter 

and 
runtime

MSR Z3 
constraint 

solver

App 0

App 1

App 2

App 3

App 4

Fig. 4: Security architecture of Prepose.

occasionally, “needs attention from a human auditor.” In
our experiments in Section 5, we encountered only one
case of reasoning needing attention. A reasonable approach
would be to reject submissions that do not qualify as
“deﬁnitely OK.”

Improving gesture authoring experience: In addi-
tion to addressing threats from untrusted applications, a
language-based approach can improve gesture authoring.
Gestures are an integral part of sensor-based always-on
application, the equivalent of UI events like left mouse
click, double-click, etc. in regular applications1. While, for
instance, the Kinect SDK already includes a number of
default gestures, developers typically need to add their
own. Diﬀerent applications often require diﬀerent sets of
gestures, and, as such, building new gestures is a funda-
mental part of software development.

Gesture development is a tricky process, which often
depends on machine learning techniques requiring large
volumes of training data [7]. These heavyweight meth-
ods are both expensive and time-consuming for many
developers, resulting in mostly large game studios being
able to aﬀord gesture development. Therefore, making
gesture development easier would unlock the creativity of
a larger class of developers. Prepose aids this with sound
static analyses for reliability properties of gestures, such
as whether the gesture deﬁnition is self-contradictory.
Prepose language and runtime: This paper proposes
Prepose, a language and a runtime for authoring and
checking gesture-based applications. For illustration, a
code snippet supported by our system in shown in Fig-
ure 3. This code is translated into logical formulas which
are checked at runtime against the user’s actual positions
using an SMT solver.

Prepose is built as a library on top of the released
Kinect SDK. Applications link against this library. The
source code of Prepose is available on Github (URL omit-
ted for anonymity). Prepose lowers the cost of developing

1To quote a blog entry: “After further experimenting with the
Kinect SDK, it became obvious what needed to come next. If you
were to create an application using the Kinect SDK, you will want to
be able to control the application using gestures (i.e. waving, swiping,
motions to access menus, etc.).” [25]

124124

.app 

gesture file

Z3 theorem 

prover

static checking

triage 

Fig. 5: Checking submissions to a gesture store. Submissions are marked as safe (green), unsafe (red), or need human attention (blue).

new gestures by exposing new primitives to developers
that can express a wide range of natural gestures.
Application domains implemented in Prepose: To
demonstrate the expressiveness of Prepose, we experi-
ment with three domains that involve diﬀerent styles of
gestures: physical therapy, dance, and tai-chi. Given the
natural syntax of Prepose and a ﬂat learning curve, we
believe that other domains can be added to the system
quite easily. For each of these gestures, we then performed
a series of analyses enabled by Prepose, including conﬂict
detection, as well as safety, security, and privacy checks.
Monitoring applications in Prepose: We discovered
that Prepose is particularly well-suited to what we call
monitoring applications which can be implemented with
Prepose gestures and a small amount of “bookkeeping”
code. For example, Kinect Sports includes a tai-chi trainer,
which instructs users to struck tai-chi poses and gives
real-time feedback on how well they do, which is easily
captured by Prepose and supported by the runtime we
have built. For another example, Atlas5D is a startup
that installs multiple sensors in the homes of seniors
and monitors seniors for any signs of a fall or another
emergency. Another example of such an application for
physical therapy is shown in Figure 8a or can be seen in a
video at http://reflexionhealth.com. These applications
can run, concurrently, for weeks on end, with only minimal
needs to report results (such as completing a certain level
within the tai-chi application) to an external server.

1.6 Contributions
Our paper makes the following contributions:

• Prepose. Proposes a programming language and a
runtime for a broad range of gesture-based immer-
sive applications designed from the ground up with
security and privacy in mind. Prepose follows the
principle of privacy by construction to eliminate the
majority of privacy attacks.

• Static analysis. We propose a set of static anal-
ysis algorithms designed to soundly ﬁnd violations
of important security and reliability properties. This
analysis is designed to be run within a gesture App
Store to prevent malicious third-party applications
from aﬀecting the end-user.

• Expressiveness. To show the expressiveness of Pre-
pose, we encode 28 gestures for 3 useful application
domains: therapy, dance, and tai-chi.

• Performance evaluation. Despite being written in
a domain-speciﬁc language (DSL), Prepose-based
gesture applications pay a minimal price for the extra
security and privacy guarantees in runtime overhead;
tasks like pose matching take milliseconds. Our static
analysis scales well in practice: safety checking is un-
der 0.5 seconds per gesture; average validity checking
time is only 188 ms; lastly, for 97% of the cases, the
conﬂict detection time is below 5 seconds, with only
one query taking longer than 15 seconds.

1.7 Paper Organization

The rest of the paper is organized as follows. Section 2
provides some background on gesture authoring. Section 3
gives an overview of Prepose concepts and provides some
motivating examples. Section 4 describes our analysis for
security and privacy in detail. Section 5 contains the
details of our experimental evaluation. Sections 7 and 8
describe related work and conclude.

2 Background

Today, developers of immersive, sensor-based applications
pursue two major approaches to creating new gesture
recognizers. First, developers write code that explicitly
encodes the gesture’s movements in terms of the Kinect
Skeleton or other similar abstraction exposed by the plat-
form. Second, developers use machine learning approaches
to synthesize gesture recognition code from labeled exam-
ples. We discuss the pros and cons of each approach each
in turn.

Manually written: In this approach, the developer ﬁrst
thinks carefully about the gesture movements in terms of
an abstraction exposed by the platform. For example, the
Kinect for Windows platform exposes a “skeleton” that
encodes a user’s joint positions. The developer then writes
custom code in a general-purpose programming language
such as C++ or C# that checks properties of the user’s
position and then sets a ﬂag if the user moves in a way to
perform the gesture. For example, the Kinect for Windows
white paper on gesture development [16] contains code for
a simple punch gesture, shown in Figure 6.

The code checks that the user’s hand is “far enough”
away from the shoulder, that the hand is moving “fast
enough,” that the elbow is also moving “fast enough,” and
that the angle between the upper and lower arm is greater

125125

// Punch Gesture
if ( vHandPos.z-vShoulderPos.z>fThreshold1 &&

fVelocityOfHand > fThreshold2 ||
fVelocityOfElbow > fThreshold3 &&
DotProduct(vUpperArm, vLowerArm) > fThreshold4)

{

}

bDetect = TRUE;

Fig. 6: A simple punch gesture.

than a threshold. If all these checks pass, the code signals
that a punch gesture has been detected.

Manually-written poses require no special tools, data
collection, or training, which makes them easy to start
with. Unfortunately, they also have signiﬁcant drawbacks.
• First, the code is hard to understand because it
typically reasons about user movements at a low level.
For example, the code uses a dot-product to check the
angle between the lower and upper arm instead of an
abstraction that directly returns the angle.

• Second, building these gestures requires a trained
programmer and maintaining code requires manually
tweaking threshold values, which may or may not
work well for a wider range of users. Third,
it is
diﬃcult to statically analyze this code because it is
written in a general purpose programming language,
so gesture conﬂicts or unsafe gestures must be de-
tected at runtime.

• Finally, the manually coded gesture approach requires
the application to have access to sensor data for the
purpose of recognizing gestures. This raises privacy
problems, as we have discussed: a malicious devel-
oper may directly embed some code to capture video
stream or skeleton data to send it to http://evil.com.

Machine learning: The leading alternative to manually-
coded gesture recognizers is to use machine learning ap-
proaches. In machine learning approaches, the developer
ﬁrst creates a training set consisting of videos of people
performing the gesture. The developer then labels the
videos with which frames and which portions of the depth
or RGB data in the frame correspond to the gesture’s
movements.

Finally, the developer runs an existing machine learning
algorithm, such as AdaBoost, to synthesize gesture recog-
nition code that can be included in a program. Figure 7
shows the overall workﬂow for the Visual Gesture Builder,
a machine learning gesture tool that ships with the Kinect
for Windows SDK. The developer takes recordings of many
diﬀerent people performing the same gesture, then tags the
recordings to provide labeled data. From the labeled data,
the developer synthesizes a classiﬁer for the gesture. The
classiﬁer runs as a library in the application.

Machine learning approaches have important beneﬁts
compared to manually-written poses. If the training set
contains a diverse group of users, such as users of diﬀerent
sizes and ages, the machine learning algorithm can “auto-
matically” discover how to detect the gesture for diﬀerent

users without manual tweaking. In addition, improving the
gesture recognition becomes a problem of data acquisition
and labeling, instead of requiring manual tweaking by a
trained programmer. As a result, many Kinect developers
today use machine learning approaches.

On the other hand, machine learning has drawbacks as
well. Gathering the data and labeling it can be expensive,
especially if the developer wants a wide range of people in
the training set. Training itself requires setting multiple
parameters, where proper settings require familiarity with
the machine learning approach used. The resulting code
created by machine learning may be diﬃcult to interpret
or manually “tweak” to create new gestures. Finally, just
as with manually written gestures, the resulting code is
even more diﬃcult to analyze automatically and requires
access to sensor data to work properly.

3 Overview
We ﬁrst show a motivating example in Section 3.1. Next,
we discuss the architecture of Prepose and how it pro-
vides security and privacy beneﬁts (3.2). We then intro-
duce basic concepts of the Prepose language and discuss
its runtime execution (3.3). Finally, we discuss the security
and privacy issues raised by an App Store for gestures, and
show how static analysis can address them (3.4).

3.1 Motivating Example

Existing application on Kinect: Figure 8a shows a
screen shot from the Reﬂexion Health physical therapy
product. The reader is strongly encouraged to watch the
video at http://reflexionhealth.com for more context.
Here, a Kinect for Windows is pointed at the user. An
on-screen animation demonstrates a target gesture for the
user. Along the top of the screen, the application gives
an English description of the gesture. Also on screen is
an outline that tracks the user’s actual position, enabling
the user to compare against the model. Along the top,
the program also gives feedback in English about what
movements the user must make to properly perform the
therapy gesture.

Reﬂexion is an example of a broader class of trainer
applications that continually monitor a user and give
feedback on the user’s progress toward gestures. The key
point is that trainer applications all need to continuously
monitor the user’s position to judge how well the user
performs a gesture. This monitoring is explicit in Reﬂexion
Health, but in other settings, such as Atlas5D’s eldercare,
the monitoring may be implicit and multiple gestures may
be tracked at once.
Encoding existing poses: We now drill down into an
example to show how applications can encode gesture
recognizers using the Prepose approach. Figure 8b shows
a common ballet pose, taken from an instructional book on
ballet. The illustration is accompanied by text describing
the pose. The text states in words that ankles should be
crossed, that arms should be bent at a certain angle, and
so on.

126126

Fig. 7: Workﬂow for machine-learning based gesture recognition creation in the Kinect Visual Gesture Builder [16].

Gestures in Prepose: Figure 8 shows the Prepose code
which captures the ballet pose. Because of the way we
have designed the Prepose language, this code is close to
the English description of the ballet pose. A ballet trainer
application would include this code, which is then sent to
the Prepose runtime for interpretation.

3.2 Architectural Goals
Figure 4 shows the architecture of Prepose. Multiple
applications run concurrently. Each application has one
or more gestures written in the Prepose language. These
applications are not trusted and do not have access to
raw sensor data. Instead, applications register their gesture
code with a trusted Prepose runtime. This runtime is
responsible for interpreting the gestures given access to
raw depth, video, or other data about the user’s position.
When a gesture is recognized, the runtime calls back to
the application which registered the gesture.

We draw a security boundary between the trusted
component and untrusted applications. Only Prepose
code crosses this boundary from untrusted applications to
trusted components. In our implementation, the trusted
component is written in managed C#, which makes it
diﬃcult for an untrusted application to cause a memory
safety error. Our design therefore provides assurance that
untrusted applications will not be able to access private
sensor data directly, while still being able to deﬁne new
gesture recognizers.

Prepose has been designed for analyzability. Develop-
ers submit code written in the Prepose language to a
gesture App Store. During submission, we can aﬀord to
spend signiﬁcant time (say, an hour or two) on performing

static analyses. We now describe the speciﬁc security and
privacy properties we support, along with the analyses
needed to check them.

3.3 Basic Concepts in Prepose
In contrast to the approaches above, Prepose deﬁnes a
domain speciﬁc language for writing gesture recognizers.
The basic unit of the Prepose language is the pose. A
pose may contain transformations that specify the target
position of the user explicitly, or it may contain restric-
tions that specify a range of allowed positions. A pose
composes these transformations and restrictions to specify
a function that takes a body position and decides if the
position matches the pose. At runtime, Prepose applies
this function to determine if the user’s current body
position matches the pose. For poses that consist solely of
transformations, Prepose also at runtime synthesizes a
target position for the user, enabling Prepose to measure
how close the user is to matching the pose and provide
real time feedback to the user on how to match the pose.
A gesture speciﬁes a sequence of poses. The user must
match each pose in the sequence provided. The gesture is
said to match when the last pose in the sequence matches.
At runtime, Prepose checks the user’s body position to
see if it matches the current pose.

In our current implementation, Prepose poses and
gestures are written in terms of the Kinect skeleton. The
Kinect skeleton is a collection of body joints, which are
distinguished points in a three-dimensional coordinate
space that correspond to the physical location of the user’s
head,
left and right arms, and other body parts. Our
approach, however, could be generalized to other methods

127127

PREPOSE

put your arms down

C#

Z3

public static BodyTransform ArmsDownTransform() {
  return new BodyTransform()
    .Compose(JointType.ElbowLeft, new Direction(0, -1, 0))
    .Compose(JointType.WristLeft, new Direction(0, -1, 0))
    .Compose(JointType.ElbowRight, new Direction(0, -1, 0))
    .Compose(JointType.WristRight, new Direction(0, -1, 0));

joints[‘elbow left’].Y > -1 ∧ 
joints[‘elbow left’].X = 0 ∧ 
joints[‘elbow left’].Z = 0

Fig. 9: Runtime correspondence: Prepose, C#, and Z3.

point your right hand up

In the ﬁrst line, the transformation “rotate” takes as
arguments the name of the user skeleton joint “left wrist,”
the amount of rotation “30 degrees,” and the direction
of rotation. The second line is similar. The third line
is a transformation “point” that takes as arguments the
name of a user skeleton joint and a direction “up.” When
applied to a skeleton position, the eﬀect of all three
transformations is to come up with a single new target
skeleton for the user.

A restriction is a function that takes as input a Kinect
skeleton, checks if the skeleton falls within a range of
allowed positions, and then returns true or false. An
example restriction in Prepose looks like this:

put your right hand on your head

The intuition here is that “on your head” is a restriction
because it does not explicitly specify a single position.
Instead, a range of allowed positions, namely those there
the hand is within a threshold distance from the head, is
denoted by this function. Here, the function “put” takes
as arguments two joints, the “right hand” and the “head.”
The function returns true if the right hand is less than
a threshold distance from the head and false otherwise.
Poses can incorporate multiple transformations and mul-
tiple restrictions. The pose matches if all restrictions are
true and the user’s body position is also closer than a
threshold to the target position.
Gestures: Gestures consist of zero or more pose declara-
tions, followed by an execution sequence. For example, a
gesture for doing “the wave” might contain the following:

E X E C U T I O N :

p o i n t - h a n d s - u p ,
p o i n t - h a n d s - f o r w a r d ,
p o i n t - h a n d s - d o w n .

That is, to do “the wave,” the user needs to put her hands
up, then move her hands from there to pointing forward,
and then ﬁnally point her hands downward. The gesture
matches when the user successfully reaches the end of the
execution sequence.

Our Prepose runtime allows multiple gestures to be
loaded at a time. The execution sequence of a gesture can
use any pose deﬁned by any loaded gesture, which allows
developers to build libraries of poses that can be shared
by diﬀerent gestures.

(a) A physical therapy application. On the right, the appli-
cation displays the user’s current position. Along the top, the
application describes the gesture the user must perform.

(b) Ballet poses.

GESTURE f o u r t h - p o s i t i o n - e n - a v a n t :

POSE c r o s s - l e g s - o n e - b e h i n d - t h e - o t h e r :

put your left ankle behind your right ankle ,
put your left ankle to the right

// do not connect your ankles .

of your right ankle .

POSE h i g h - a r c - a r m s - t o - r i g h t :

point your arms down ,
rotate your right arm 70 degrees up ,
rotate your left elbow 20 degrees to your left ,
rotate your left wrist 25 degrees to your right .

E X E C U T I O N :

// f o u r t h - p o s i t i o n - e n - a v a n t - c o m p o s e d
s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
s t r e t c h - l e g s ,
c r o s s - l e g s - o n e - b e h i n d - t h e - o t h e r ,
h i g h - a r c - a r m s - t o - r i g h t .

(c) A sample ballet gesture written in Prepose.
The gesture deﬁnes two poses, which are spec-
iﬁcations of a body position. Then, the gesture
execution speciﬁes the sequence of poses that must
be matched to perform the gesture.

Fig. 8: Motivating example.

of sensing gestures. For example, the Leap Motion hand
sensor exposes a “hand skeleton” to developers and we
could adapt the Prepose runtime to work with Leap
Motion or other hand sensors.

Poses: A pose contains either transformations or re-
strictions. A transformation is a function that takes as
input a Kinect skeleton and returns a Kinect skeleton.
Transformations in Prepose include “rotate” and “point”,
as in this example Prepose code:

rotate your left wrist 30 degrees to the front
rotate your right wrist 30 degrees to the front

128128

Runtime execution: Figure 9 shows the stages of run-
time processing in Prepose. A high-level Prepose state-
ment is compiled into C# code, which in turn deﬁnes
an SMT formula. The formula is used both for runtime
matching and static analysis.

Declarations

app
gesture
pose

statement
execution

APP id : (gesture . ) + EOF
GESTURE id : pose + execution
POSE id :
statement ( , statement ) * .

::=
::=
::=
::= transf orm | restriction
::=

EXECUTION :
(repeat the following steps number
executionStep( , executionStep) *
| executionStep( , executionStep) * )

executionStep

::= motionConstraint ?

id ( and holdConstraint) ?

Transforms

transf orm

::= pointT o

3.4 Gesture Security and Reliability
At gesture submission time, we apply static analysis to
the submitted Prepose program. This analysis can be
performed within the App store before the user is allowed
to download a new application that contains gestures.
Conﬂict checking may also be done as information about
which applications are installed is already available to the
App store. Conceivably, the analysis may be done on the
client as well. The results of this analysis tell us whether
the submitted gesture is “deﬁnitely OK,” “deﬁnitely not
OK,” or, as may happen occasionally, “needs attention
from a human auditor.” This kind of triage is fairly typical
in the App store context.

We currently perform the four analyses summarized in
Figure 2. As we explain below, this analysis amounts to
queries resolved by the underlying SMT solver, Z3.
Gesture safety: The ﬁrst analysis is for gesture safety.
Just because it’s possible to ask someone to make a gesture
does not mean it is a good idea. A gesture may ask people
to overextend their limbs, make an obscene motion, or
otherwise potentially harm the user. To prevent an unsafe
gesture from being present in the store, we ﬁrst deﬁne
safety restrictions. Safety restrictions are sets of body
positions that are not acceptable. Safety restrictions are
encoded as SMT formulas that specify disallowed positions
for Kinect skeleton joints.
Internal validity: It is possibly in Prepose to write a
gestures that can never be matched. For example, a gesture
that requires the user to keep their arms both up and down
contains an internal contradiction. We analyze Prepose
gestures to ensure they lack internal contradictions.
Reserved gestures: A special case of conﬂict detection
is detecting overlap with reserved gestures. For example,
the Xbox Kinect has a particular attention gesture that
opens the Xbox OS menu even if another game or program
is running. Checking conﬂicts with reserved gestures is
important because applications should not be able to
“shadow” the system’s attention gesture with its own
gestures.
Conﬂict detection: We say that a pair of gestures
conﬂicts if the user’s movements match both gestures
simultaneously. Gesture conﬂicts can happen accidentally,
because gestures are written independently by diﬀerent
application developers. Alternatively, a malicious applica-
tion can intentionally register a gesture that conﬂicts with
another application. In Prepose, because all gestures
have semantics in terms of SMT formulas, we can ask a
solver if there exists a sequence of body positions that
matches both gestures. If the solver completes, then either
it certiﬁes that there is no such sequence or gives an
example.

pointT o

::=

rotateP lane

::=

rotateDirection

::=

your ? bodyP art) *

y our? bodyP art) *

| rotateP lane
| rotateDirection
point your ?
bodyP art(( ,
and your ? bodyP art) ?
( to | to your ) ? direction
rotate your
bodyP art(( ,
and your ? bodyP art) ?
number degrees
angularDirection on the ?
ref erenceP lane
rotate your bodyP art
(( , your ? bodyP art) *
and your ? bodyP art) ?
number degrees
( to | to your ) ?
direction

Restrictions
restriction

::=

touchRestriction

::=

putRestriction

::=

alignRestriction

::=

dont ? touchRestriction
| dont ? putRestriction
| dont ? alignRestriction
touch your ?
bodyP art with your ?
side hand
put your ?
bodyP art(( , your ? bodyP art) *
and your ? bodyP art) ?
relativeDirection bodyP art
align your ?
bodyP art(( , your ? bodyP art) *
and your ? bodyP art) ?

Skeleton

bodyP art

joint
centerJoint

side
sidedJoint

direction
angularDirection
ref erenceP lane

relativeDirection

motionConstraint
holdConstraint
repeat

::= joint | side arm | side leg | spine
legs | shoulders

| back | arms |
| wrists | elbows | hands
| hands tips | thumbs | hips
| knees | ankles |
feet | you
::= centerJoint | side sidedJoint
neck | head | spine m id |
::=
spine base | spine shoulder
left | right
shoulder | elbow | wrist | hand |
hand tip | thumb | hip | knee |
ankle |
foot
up | down |
front | back | side
clockwise | counter clockwise
frontal plane | sagittal plane |
horizontal plane
in front of your | behind your |
(( on top of ) |
above ) your | below your |
to the side of your
slowly | rapidly
hold for number seconds
repeat number times

::=
::=
::=

::=
::=

::=
::=
::=

::=

Fig. 10: BNF for Prepose. The start symbol is app.

4 Techniques

Figure 10 shows a BNF for Prepose which we currently
support. This captures how Prepose applications can be
composed out of gestures, gestures composed out of poses
and execution blocks, execution blocks can be composed

129129

Rotate-Frontal+ Rotate-Frontal(j, a, Clockwise)
j.Y = cos(a) · j.Y + sin(a) · j.Z
j.Z = −sin(a) · j.Y + cos(a) · j.Z

Rotate-Frontal- Rotate-Frontal(j, a, CounterClockwise)

j.Y = cos(a) · j.Y − sin(a) · j.Z
j.Z = sin(a) · j.Y + cos(a) · j.Z

Rotate-Sagittal+ Rotate-Sagittal(j, a, Clockwise)
j.X = cos(a) · j.X + sin(a) · j.Y
j.Y = −sin(a) · j.X + cos(a) · j.Y

Rotate-Sagittal- Rotate-Sagittal(j, a, CounterClockwise)

j.X = cos(a) · j.X − sin(a) · j.Y
j.Y = sin(a) · j.X + cos(a) · j.Y

Rotate-Horizontal+ Rotate-Horizontal(j, a, Clockwise)
j.X = cos(a) · j.X + sin(a) · j.Z
j.Z = −sin(a) · j.X + cos(a) · j.Z

Rotate-Horizontal- Rotate-Horizontal(j, a, CounterClockwise)

j.X = cos(a) · j.X − sin(a) · j.Z
j.Z = sin(a) · j.X + cos(a) · j.Z

Fig. 11: Transformations translated into Z3 terms. j is the joint
position (with X, Y , and Z components); a is the rotational angle.

out of execution steps, etc2.

The grammar is fairly extensible: if one wishes to sup-
port other kinds of transforms or restrictions, one needs
to extend the Prepose grammar, regenerate the parser,
and provide runtime support for the added transform or
restriction. Note also that the Prepose grammar lends
itself naturally to the creation of developer tools such as
context-sensitive auto-complete in an IDE or text editor.

4.1 Prepose to SMT Formulas
Prepose compiles programs written in the Prepose
language to formulae in Z3, a state-of-the-art SMT solver.
Translating basic transforms: Figure 11 captures the
principles of translating Prepose transforms into Z3
terms; the ﬁgure shows the diﬀerent variants of how
rotatePlane from Figure 10 is translated by way of illus-
tration. These are update rules that deﬁne the (cid:2)X, Y, Z(cid:3)
coordinates of the joint j to which the transformation is
applied. Note that rotatePlane transformations take the
plane p and direction d as parameters. Depending on the
type of rotation, namely, the rotation plane, one of these
rules is picked. These coordinate updates generally require
a trigonometric computation3.
Translating basic restrictions: Figure 12 shows how
Prepose restrictions are translated to Z3 constraints.
Auxiliary functions Angle and Distance that are further
compiled down into Z3 terms are used as part of com-
pilation. Additionally, thresholds thangle and thdistance

2For researchers who wish to extend Prepose, we have up-
loaded an a ANTLR version of the Prepose grammar to http:
//binpaste.com/fdsdf

3Because of the lack of support for these functions in Z3, we
have implemented sin and cos applied to a using lookup tables for
commonly used values.

Align

Align(j1, j2)

Γ (cid:2) Angle(j1, j2) < thalign

LowerThan

LowerThan(j)

Γ (cid:2) j.Y < sin(thangle)

Put-Front

Put-Front(j1, j2, InFrontOfYour)

Γ (cid:2) j1.Z > j2.Z + thdistance

Put-Behind

Put-Behind(j1, j2, BehindYour)
Γ (cid:2) j1.Z < j2.Z − thdistance

Put-Right Put-Right(j1, j2, ToTheRightOfYour)

Γ (cid:2) j1.X > j2.X + thdistance

Put-Left

Put-Left(j1, j2, ToTheLeftOfYour)

Γ (cid:2) j1.X < j2.X − thdistance

Put-Top

Put-Top(j1, j2, OnTopOfYour)
Γ (cid:2) j1.Y > j2.Y + thdistance

Put-Below

Put-Below(j1, j2, BelowYour)
Γ (cid:2) j1.Y < j2.Y − thdistance

Touch

Touch(j1, j2)

Γ (cid:2) Distance(j1 < j2) < thdistance

KeepAngle

KeepAngle(j1, j2)

Γ (cid:2) Angle(j1 < j2) < thangle

Fig. 12: Restrictions translated into Z3 terms. Note that thdistance
and thangle are static thresholds: they deﬁne what it means to
perform a speciﬁc pose. For instance, touching a surface does not
mean literally touching it; being very close to it is suﬃcient. As in
Figure 11, j is the joint position (with X, Y , and Z components).

are static thresholds that are part of pose deﬁnition, as
opposed to runtime thresholds used for matching.

Runtime Execution: After a Prepose script is trans-
lated to Z3 constraints, we use the Z3 solver to match
a user’s movements to the gesture. The trusted core of
Prepose registers with the Kinect skeleton tracker to
receive updated skeleton positions of the user.

For each new position, the runtime uses the Z3 term
evaluation mechanism to automatically apply gestures
to the previous user’s position to obtain the target (in
a sense, ideal) position for each potential gesture. This
target position is in turn compared to the current user’s
joints’ position to see if there is a match and to notify
the application. Note that this is an approximate compar-
ison where the level of precision can be speciﬁed by the
application (see, for instance, Figure 13 with a slider for
specifying the accuracy of the match). Note that this is
a very lightweight use of the theorem prover, as we only
evaluate terms without doing satisﬁability checking. One
could also have a custom runtime matching mechanism
instead. Upon receiving a notiﬁcation, the application may
then give feedback to the user, such as encouragement,
badges for completing a gesture, or movement to a more
diﬃcult gesture.

130130

4.2 Security and Reliability
By design, Prepose is amenable to sound static reasoning
by translating queries into Z3 formulae. Below we show
how to convert key security and reliability properties into
Z3 queries. The underlying theory we use is that of reals.
We also use non-recursive data types (tuples) within Z3.
Please remember that these are static analyses that
typically take place before gestures are deployed to the
end-user — there is no runtime checking overhead. The
properties below are also brieﬂy summarized in Figure 2.
Unlike approximate runtime matching described above,
static analysis is about precise, ideal matching. We do not
have a theory of approximate equality that is supported by
the theorem prover. We treat gestures such as G : B → B,
in other words, as functions that transform bodies in set
B to new bodies.

Basic gesture safety: The goal of these restrictions is
to make sure we “don’t break any bones” by allowing
the user to follow this gesture. We deﬁne a collection of
safety restrictions pertaining to the head, spine, shoulders,
elbows, hips, and legs. We denote by RS the compiled
restriction, the set of all states that are allowed under our
safety restrictions. The compiled restriction RS is used to
test whether for a given gesture G

∃b ∈ B : ¬RS(G(b))

in other words, does there exist a body which fails to
satisfy the conditions of RS after applying G. RS restricts
the relative positions of the head, spine, shoulders, elbows,
hips, and legs. The restriction for the head is shown below
to give the reader a sense of what is involved:

var head = new S i m p l e B o d y R e s t r i c t i o n ( body = > {

Z 3 P o i n t 3 D up = new Z 3 P o i n t 3 D (0 , 1 , 0);

return Z3 . Context . MkAnd (

body . Joints [ J o i n t T y p e . Head ]

. I s A n g l e B e t w e e n L e s s T h a n ( up , 45) ,

body . Joints [ J o i n t T y p e . Neck ]

. I s A n g l e B e t w e e n L e s s T h a n ( up , 45));

} );

Inner validity: We also want to ensure that our gesture
are not inherently contradictory, in other words, is it the
case that all sequences of body positions will fail to match
the gesture. An example of a gesture that has an inner
contradiction, consider

put your arms up ;
put your arms down ;

Obviously both of these requirements cannot be sat-
isﬁed at once.
In the Z3 translation, this will give
rise to a contradiction: joint[”rightelbow”].Y = 1 ∧
joint[”rightelbow”].Y = −1. To ﬁnd possible contradic-
tions in gesture deﬁnitions, we use the following query:

¬∃b ∈ B : G(b).

Protected gestures: Several immersive sensor-based sys-
tems include so-called “system attention positions” that
users invoke to get privileged access to the system. These
are the AR equivalent of Ctrl-Alt-Delete on a Windows

131131

system. For example, the Kinect on Xbox has a Kinect
Guide gesture that brings up the home screen no matter
which game is currently being played. The Kinect “Return
to Home” gesture is easily encoded in Prepose and the
reader can see this gesture here: http://bit.ly/1JlXk79.
For Google Glass, a similar utterance is “Okay Glass.”
On Google Now on a Motorola X phone, the utterance
is “Okay Google.”

We want to make sure that Prepose gesture do not

attempt to redeﬁne system attention positions.

∃b ∈ B, s ∈ S : G(b) = s.

where S ⊂ B is the set of pre-deﬁned system attention
positions.
Conﬂict detection: Conﬂict detection, in contrast, in-
volves two possibly interacting gestures G1 and G2.

∃b ∈ B : G1(b) = G2(b).

Optionally, one could also attempt to test whether com-
positions of gestures can yield the same outcome. For

example, is it possible that G1 ◦ G2 = G3 ◦ G4. This can

also be operated as a query on sequences of bodies in B.

5 Experimental Evaluation
We built a visual gesture development and debugging
environment, which we call Prepose Explorer. Figure 13
shows a screen shot of our tool. On the left, a text entry
box allows a developer to write Prepose code with proper
syntax highlighting. On the right, the tool shows the user’s
current position in green and the target position in white.
On the bottom, the tool gives feedback about the current
pose being matched and how close the user’s position is to
the target.

5.1 Dimensions of Evaluation
Given that Prepose provides guarantees about security
and privacy by construction, we focused on making sure
that we are able to program a wide range of applications
that involve gestures, as summarized in Figure 14 and also
partially shown in the Appendix. Beyond that we want to
ensure that the Prepose-based gesture matching scales
well to support interactive games, etc. To summarize

• We used this tool to measure the expressiveness of
Prepose by creating 28 gestures in three diﬀerent
domains.

• We then ran some benchmarks to measure runtime
performance and static analysis performance of Pre-
pose. First, we report runtime performance, including
the amount of time required to match a pose and the
time to synthesize a new target position. Then, we
discuss the results of benchmarks for static analysis.
Prior work has used surveys to evaluate whether the
information revealed by various abstractions is acceptable
to a sample population of users in terms of its privacy.
Here, we are giving the application the least amount of
information required to do its jobs, so these surveys are
not necessary.

Fig. 13: Screenshot of Prepose Explorer in action.

n
o
i
t
a
c
i
l
p
p
A

s
e
r
u
t
s
e
G

s
e
s
o
P

C
O
L

URL

Therapy 12 28 225 http://pastebin.com/ARndNHdu
11 16 156 http://pastebin.com/c9nz6NP8
Ballet
5 32 314 http://pastebin.com/VwTcTYrW
Tai-chi

Fig. 14: We have encoded 28 gestures in Prepose, across three
diﬀerent applications. The table shows the number of total poses and
lines of Prepose code for each application. Each pose may be used
in more than one gesture. The Appendix has one of the Prepose
applications, Ballet, listed as well.

5.2 Expressiveness
Because the Prepose language is not Turing-complete, it
has limitations on the gestures it can express. To determine
if our choices in building the language are suﬃcient to han-
dle useful gestures, we built gestures using the Prepose
Explorer. We picked three distinct areas: therapy, tai-chi,
and ballet, which together cover a wide range of gestures.
Figure 14 shows the breakdown of how many gestures we
created in each area, for 28 in total. These are complex
gestures: the reviewers are encouraged to examine the code
linked to from Figure 14.

For example, Figure 15 shows some of the poses from tai-
chi captured by Prepose code. We chose tai-chi because
it is already present in Kinect for Xbox games such as
Your Shape: Fitness Evolved. In addition, tai-chi poses
require complicated alignment and non-alignment between

Fig. 15: The tai-chi gestures we have encoded using Prepose
(http : //pastebin.com/VwTcTYrW) all come from this illustration.

diﬀerent body parts.

5.3 Pose Matching Performance

We used the Kinect Studio tool that ships with the Kinect
for Windows SDK to record depth and video traces of
one of the authors. We recorded a trace of performing
two representative gestures. Each trace was about 20

132132

500

450

400

350

300

250

200

150

100

50

0

0

2

4

6

8

10

12

14

16

Fig. 16: Time to check for safety, in ms, as a function of the number
of steps in the underlying gesture.

seconds in length and consisted of about 20,000 frames,
occupying about 750 MB on disk. We picked these to be
two representative tai-chi gestures.

Our measurements were performed on an HP Z820 Pen-
tium Xion E52640 Sandy bridge with 6 cores and 32 GB
of memory running Windows 8.1.

For each trace, we measured the matching time: the
time required to evaluate whether the current user posi-
tion matches the current target position. When a match
occurred, we also measured the pose transition time: the
time required to synthesize a new target pose, if applicable.
Our results are encouraging. On the ﬁrst frame, we
observed matching times between 78 ms and 155 ms,
but for all subsequent frames matching time dropped
substantially. For these frames, the median matching time
was 4 ms. with a standard deviation of 1.08 ms. This is
fast enough for real time tracking at 60 FPS (frames per
second).

For pose transition time, we observed a median time
of 89 ms, with a standard deviation of 36.5 ms. While this
leads to a “skipped” frame each time we needed to create
a new pose, this is still fast enough to avoid interrupting
the user’s movements.

While we have made a design decision to use a theorem
prover for runtime matching, one can replace that machin-
ery with a custom runtime matcher that is likely to run
even faster. When deploying Prepose-based applications
on a less powerful platform such as the Xbox, this design
change may be justiﬁed.

5.4 Static Analysis Performance

Safety
linear

checking:
dependency

Figure
between

16

shows

a

the

number

near-
of

600

500

400

300

200

100

0

0

2

4

6

8

10

12

14

16

Fig. 17: Time to check internal validity, in ms, as a function on the
number of steps in the underlying gesture.

gesture

in a
restrictions. Exploring

and time

the

to

check
results

against
further,

steps
safety
we performed a linear re-
gression to see the in-
ﬂuence of other param-
eters such as the num-
ber of negative restric-
tions. The R2 value of the
ﬁt is about 0.9550, and the coeﬃcients are shown in the
table to the right. The median checking time is only 2 ms.
We see that safety checking is practical and, given how
fast it is, could easily be integrated into an IDE to give
developers quick feedback about invalid gestures.

Intercept
NumTransforms
NumRestrictions
NumNegatedRestrictions
NumSteps

-4.44
0.73
-2.42
-6.23
29.48

Validity checking: Figure 17 shows another near-linear
dependency between the number of steps in a gesture and
the time to check if the gesture is internally valid. The
average checking time is 188.63 ms. We see that checking
for internal validity of gestures is practical and, given how
fast it is, could easily be integrated into an IDE to give
developers quick feedback about invalid gestures.

Conﬂict checking: We performed pairwise conﬂict check-
ing between 111 pairs of gestures from our domains.
Figure 18 shows the CDF of conﬂict checking times, with
the x axis in log scale. For 90% of the cases, the checking
time is below 0.170 seconds, while 97% of the cases took
less than 5 seconds and 99% less than 15 seconds. Only
one query out of the 111 took longer than 15 seconds. As a
result, with a timeout of 15 seconds, only one query would
need attention from a human auditor.

6 Limitations
This work is the ﬁrst step in deﬁning a programmable way
to limit the potential for privacy leaks in gesture-based
programming. We are not claiming that we have solved all
the potential privacy issues. In fact, we believe strongly
that the attack model will evolve as this space rapidly
changes.

133133

100%

90%

80%

70%

60%

50%

40%

30%

20%

10%

0%

1

97% of checks 
are faster than 5 

seconds

10

100

1,000

10,000

100,000

Fig. 18: Time to check for conﬂicts for a pair of gestures presented
as a CDF. The x axis is seconds plotted on a log scale.

A major challenge is to deﬁne a precise and easy to
reason about attack model in this space. Our key contribu-
tion lies in going beyond the model that gives application
direct access to hardware and providing an abstraction
layer above that. It is exceedingly diﬃcult to argue that
that abstraction layer cannot be abused by a clever at-
tacker. By way of analogy, consider an operating system
mechanism that allows applications to register keystrokes
(or key chords) such as Ctrl + Alt + P. While this makes
it considerably more diﬃcult to develop a keylogger, it is
diﬃcult to claim that one cannot determine whether the
user is left-handed or possibly to ﬁngerprint diﬀerent users
based on the frequency of their shortcut use. Similarly, in
the context of Prepose, a clever attacker may deﬁne a
“network” of really ﬁne-grained gestures to collect statistics
about the user.

A key advantage of Prepose is that when new at-
tacks are discovered, they can be encoded as satisﬁability
queries, which gives one way to tackle these attacks as well.
We see the following areas as extensions of our current
work:

• We do not explicitly reason about the notion of time;
there could be a pose that is safe for brief periods of
time but is less safe when held for, say, a minute.

• Our current approach reasons about conﬂicts at the
level of entire gestures. This does not preclude con-
ﬂicts at the intermediate, sub-gesture level. A possible
way to alleviate this situation is to automatically
compile the current set of gesture into intermediate,
atomic gestures, which could be validated for lack of
conﬂicts.

• Prepose requires the developer to manually write
gestures. A natural next step is to automatically
synthesize gestures by demonstration.

7 Related Work
Below we ﬁrst describe some gesture-building approaches,
mostly from the HCI community, and then we talk about
privacy in sensing-based applications.

7.1 Gesture Building Tools

Below, we list some of the key projects that focus on
gesture creation. Prepose’s approach is unique in that
it focuses on capturing gestures using English-like com-
mands. This allows gesture deﬁnitions to be modiﬁed more
easily. Prepose diﬀers from the tools below in that it
focuses on security and privacy at the level of system
design.

CrowdLearner [1] provides a crowd-sourcing way to
collect data from mobile devices usage in order to create
recognizers for tasks speciﬁed by the developers. This way
the sampling time during the application development is
shorter and the collected data should represent a better
coverage of real use scenarios in relation to the usual in-lab
sampling procedures. Moreover, it abstracts for developers
the classiﬁer construction and population, requiring no
speciﬁc recognition expertise.

Gesture Script [18] provides a unistroke touch gesture
recognizer which combines training from samples with
explicit description of the gesture structure. By using the
tool, the developer is enabled to divide the input gestures
in core parts, being able to train them separately and
specify by a script language how the core parts are per-
formed by the user. This way, it requires less samples for
compound gestures because the combinations of the core
parts are performed by the classiﬁer. The division in core
parts also eases the recovery of attributes (e.g. number of
repetitions, line length, etc.) which can be speciﬁed by the
developer during the creation of the gestures.

Proton [15] and Proton++ [14] present a tool directed
to multitouch gestures description and recognition. The
gestures are modeled as regular expressions and their
alphabet consists of the main actions (Down, Move and
Up), and related attributes e.g.: direction of the move
action; place or object in which the action was taken;
counter which represents a relative ID; among others. It is
shown that by describing gestures with regular expressions
and a concise alphabet it is possible to easily identify
ambiguity between two gestures previously to the test
phase.

CoGesT [8] presents a scheme to represent hand and
arms gestures. It uses a grammar which generates the pos-
sible descriptions, the descriptions are based on common
textual descriptions and related to the coordinate system
generated by the body aligned planes (sagittal, frontal
and horizontal). The transcription is mainly related to
relative positions and trajectories between them, relying
on the form and not on functional classiﬁcation of the
gesture. Moreover it does not specify the detailed position
but more broad relations between body parts. This way
the speciﬁed gestures are not strongly precise. On the
other hand, it enables users to produce equivalent gestures

134134

by interpreting the description and using their knowledge
about gesture production.

BAP [5] approaches the task of coding body movements
with focus on the study of emotion expression. Actors
trained the system by performing speciﬁc emotion rep-
resentations and these recorded frames were coded into
pose descriptions. The coding was divided into anatomic
(explicating which part of the body was relevant in the
gesture) and form (describing how the body parts were
moving). The movement direction was described adopting
the orthogonal body axis (sagittal, vertical and trans-
verse). Examples of coding: Left arm action to the right;
Up-down head shake; Right hand at waist; etc.

Annotation of Human Gesture [22] proposes an ap-
proach for transcribing gestural movements by overlaying
a 3D body skeleton on the recorded actors’ gestures. This
way, once the skeleton data is aligned with the recorded
data, the annotation can be created automatically.

RATA [23] presents a tool to create recognizers for touch
and stylus gestures. The focus is on the ease and rapidity of
the gesture recognition developing task. The authors claim
that within 20 minutes (and by adding only two lines of
code) developers and interaction designers can add new
gestures to their application.

EventHurdle [13] presents a tool for explorative proto-
typing of gesture use on the application. The tool is pro-
posed as an abstraction of the gathered sensor data, which
can be visualized as a 2D graphic input. The designer also
can specify the gesture in a provided graphical interface.
The main concept is that unistroke touch gestures can be
described as a sequence of trespassed hurdles.

GestureCoder [19] presents a tool for multi-touch ges-
ture creation from performed examples. The recognition is
performed by creating a state machine for the performed
gestures with diﬀerent names. The change of states is
activated by some pre-coded actions: ﬁnger landing; lifting;
moving; and timeout. The ambiguity of recorded gestures
is solved by analyzing the motion between the gestures
using a decision tree.

GestureLab [4] presents a tool for building domain-
speciﬁc gesture recognizers. It focuses on pen unistroke
gestures by considering trajectory but also additional at-
tributes such as timing and pressure.

MAGIC [2] and MAGIC 2.0 [17] are tools to help
developers, which are not experts in pattern recognition,
to create gesture interfaces. Focuses on motion gesture
(using data gathered from motion sensors, targeted to
mobile scenario). MAGIC 2.0 focuses on false-positive pre-
diction for these types of gestures. MAGIC comes with an
“Everyday Gesture Library” (EGL), which contains videos
of people performing gestures. MAGIC uses the EGL to
perform dynamic testing for gesture conﬂicts, which is
complementary to our language-based static approach.

7.2 Sensing and Privacy
The majority of work below focuses on privacy concerns
in sensing applications. In Prepose, we add some security
concerns into the mix, as well.

SurroundWeb [27] presents an immersive browser
which tackles privacy issues by reducing the required
privileges. The concept is based on a context sensing tech-
nology which can render diﬀerent web contents on diﬀerent
parts of the room. In order to prevent the web pages to
access the raw video stream of the room, SurroundWeb
is proposed as a rendering platform through the Room
Skeleton abstraction (which consists on a list of possible
room “screens”). Moreover the SurroundWeb introduces
a Detection Sandbox as a mediator between web pages
and object detection code (never telling the web pages
if objects were detected or not) and natural user inputs
(mapping the inputs into mouse events to the web page).
Darkly [12] proposes a privacy protection system to
prevent access of raw video data from sensors to untrusted
applications. The protection is performed by controlling
mechanisms over the acquired data. In some cases the
privacy enforcement (transformations on the input frames)
may reduce application functionality.

OS Support for AR Apps [6] and AR Apps with Recog-
nizers [11] discusses the access the AR applications usually
have to raw sensors and proposes OS extension to control
the sent data by performing the recognizer tasks itself.
This way the recognizer module is responsible to gather
the sensed data and to process it locally, giving only the
least needed privileges to AR applications.

MockDroid [3] proposes an OS modiﬁcation for smart
phones in which applications always ask the user to access
the needed resources. This way users are aware of which
information are being sent to the application whenever
they run it, and then can decide between the trade-oﬀ of
giving access or using the application functionality.

AppFence [9] proposes a tool for privacy control on
mobile devices, which can block or shadow sent data
to applications in order to keep the application up and
running, but prevent exﬁltration of on-device data. What
You See is What You Get [10] proposes a widget which
alerts users of which sensor is being requested by which
application.

Recent work on world-driven access control restricts sen-
sor input to applications in response to the environment,
e.g. it can be used to disable access to the camera when in a
bathroom [24]. Mazurek et al. surveyed 33 users about how
they think about controlling access to data provided by a
variety of devices, and discovered that many user’s mental
models of access control are incorrect [20]. Vaniea et al.
performed an experiment to determine how users notice
and ﬁx access-control permission errors depending on
where the access-control policy is spatially located on a
web site [26].

8 Conclusions
This paper introduces the Prepose language and runtime.
Prepose allows developers to write high-level gesture
descriptions that have semantics in terms of SMT formu-
las. Our architecture protects the privacy of the user by
preventing untrusted applications from directly accessing

135135

[12] S. Jana, A. Narayanan, and V. Shmatikov. A Scanner Darkly:
Protecting user privacy from perceptual applications. In Pro-
ceedings of IEEE Symposium on Security and Privacy, 2013.

[13] J.-W. Kim and T.-J. Nam. EventHurdle: supporting designers’
exploratory interaction prototyping with gesture-based sensors.
In Proceedings of the Conference on Human Factors in Com-
puting Systems, 2013.

[14] K. Kin, B. Hartmann, T. DeRose, and M. Agrawala. Pro-
ton++: A customizable declarative multitouch framework. In
Proceedings of the Symposium on User Interface Software and
Technology, 2012.

[15] K. Kin, B. Hartmann, T. DeRose, and M. Agrawala. Proton:
Multitouch gestures as regular expressions. In Proceedings of the
Conference on Human Factors in Computing Systems, 2012.

[16] Kinect

for Windows Team at Microsoft.

Visual ges-
ture builder: A data-driven solution to gesture detection,
https://onedrive.live.com/view.aspx?resid=
2014.
1A0C78068E0550B5!77743&app=WordPdf.

[17] D. Kohlsdorf, T. Starner, and D. Ashbrook. MAGIC 2.0: A
web tool for false positive prediction and prevention for gesture
recognition systems.
In Proceedings of Automatic Face &
Gesture Recognition and Workshops, 2011.

[18] H. L¨u, J. Fogarty, and Y. Li. Gesture script: Recognizing
gestures and their structure using rendering scripts and interac-
tively trained parts. 2014.

[19] H. L¨u and Y. Li. Gesture coder: a tool for programming multi-
touch gestures by demonstration. In Proceedings of the ACM
Conference on Human Factors in Computing Systems (CHI),
2012.

[20] M. L. Mazurek, J. P. Arsenault, J. Bresee, N. Gupta, I. Ion,
C. Johns, D. Lee, Y. Liang, J. Olsen, B. Salmon, R. Shay,
K. Vaniea, L. Bauer, L. F. Cranor, G. R. Ganger, and M. K.
Reiter. Access control for home data sharing: Attitudes, needs
and practices.
In Proceedings of the SIGCHI Conference on
Human Factors in Computing Systems, 2010.

[21] L. D. Moura and N. Bjorner. Z3: An Eﬃcient SMT Solver. In
Tools and Algorithms for Construction and Analysis of Systems
(TACAS), 2008.

[22] Q. Nguyen and M. Kipp. Annotation of Human Gesture using

3D Skeleton Controls. In LREC. Citeseer, 2010.

[23] B. Plimmer, R. Blagojevic, S. H.-H. Chang, P. Schmieder, and
J. S. Zhen. Rata: codeless generation of gesture recognizers.
In Proceedings of the Annual BCS Interaction Specialist Group
Conference on People and Computers. British Computer Soci-
ety, 2012.

[24] F. Roesner, D. Molnar, A. Moshchuk, T. Kohno, and H. J.
Wang. World-driven access control. In Proceedings of the ACM
Conference on Computer and Communications Security, 2014.
gesture
2011.

[25] M. Tsikkos
service with
SDK,
http://blogs.msdn.com/b/mcsuksoldev/archive/
2011/08/08/
kinect-for-windows-sdk.aspx.

writing-a-gesture-service-with-the

for Windows

and

J. Glading.

the Kinect

Writing

a

[26] K. Vaniea, L. Bauer, L. F. Cranor, and M. K. Reiter. Out of
sight, out of mind: Eﬀects of displaying access-control infor-
mation near the item it controls. In Proceedings of the IEEE
Conference on Privacy, Security and Trust (PST), 2012.

[27] J. Vilk, D. Molnar, E. Ofek, C. Rossbach, B. Livshits,
A. Moshchuk, H. J. Wang, and R. Gal. SurroundWeb: Miti-
gating Privacy Concerns in a 3D Web Browser. In Proceedings
of the Symposium on Security and Privacy, 2015.

raw sensor data; instead, applications register Prepose
code with a trusted runtime. Sound static analysis helps
eliminate possible security and reliability issues.

To test the expressiveness of Prepose, we have cre-
ated 28 gestures in Prepose across three important and
representative immersive programming domains. We also
showed that Prepose programs can be statically analyzed
quickly to check for safety, pairwise conﬂicts, and conﬂicts
with system gestures.

Runtime matching in Prepose as well as static con-
ﬂict checking, both of which reduce to Z3 queries, are
suﬃciently fast (milliseconds to several seconds) to be
deployed. By writing gesture recognizers in a DSL delib-
erately designed from the ground up to support privacy,
security, and reliability, we obtain strong guarantees with-
out sacriﬁcing either performance or expressiveness. Our
Z3-based approach has more than acceptable performance
in practice. Pose matching in Prepose averages 4 ms. Syn-
thesizing target pose time ranges between 78 and 108 ms.
Safety checking is under 0.5 seconds per gesture. The
average validity checking time is only 188.63 ms. Lastly,
for 97% of the cases, the conﬂict detection time is below 5
seconds, with only one query taking longer than 15 sec-
onds.

References

[1] S. Amini and Y. Li. Crowdlearner: rapidly creating mobile rec-
ognizers using crowdsourcing. In Proceedings of the Symposium
on User Interface Software and Technology, 2013.

[2] D. Ashbrook and T. Starner. Magic: a motion gesture design
In Proceedings of the Conference on Human Factors in

tool.
Computing Systems. ACM, 2010.

[3] A. R. Beresford, A. Rice, N. Skehin, and R. Sohan. MockDroid:
trading privacy for application functionality on smartphones. In
Proceedings of the Workshop on Mobile Computing Systems and
Applications, 2011.

[4] A. Bickerstaﬀe, A. Lane, B. Meyer, and K. Marriott. Developing
domain-speciﬁc gesture recognizers for smart diagram environ-
ments.
In Graphics Recognition. Recent Advances and New
Opportunities. 2008.

[5] N. Dael, M. Mortillaro, and K. R. Scherer. The body action
and posture coding system (BAP): Development and reliability.
Journal of Nonverbal Behavior, 36(2), 2012.

[6] L. D’Antoni, A. Dunn, S. Jana, T. Kohno, B. Livshits, D. Mol-
nar, A. Moshchuk, E. Ofek, F. Roesner, S. Saponas, et al.
Operating system support for augmented reality applications.
Proceedings of Hot Topics in Operating Systems (HotOS), 2013.
[7] S. Fothergill, H. Mentis, P. Kohli, and S. Nowozin. Instructing
people for training gestural interactive systems. In Proceedings
of the Conference on Human Factors in Computing Systems,
2012.

[8] D. Gibbon, R. Thies, and J.-T. Milde. CoGesT: a formal
transcription system for conversational gesture. In Proceedings
of LREC 2004, 2004.

[9] P. Hornyack, S. Han, J. Jung, S. Schechter, and D. Wetherall.
These aren’t the droids you’re looking for: retroﬁtting android to
protect data from imperious applications. In Proceedings of the
Conference on Computer and Communications Security, 2011.
[10] J. Howell and S. Schechter. What you see is what they get:
Protecting users from unwanted use of microphones, camera,
and other sensors.
In Proceedings of Web 2.0 Security and
Privacy Workshop. Citeseer, 2010.

[11] S. Jana, D. Molnar, A. Moshchuk, A. Dunn, B. Livshits, H. J.
Wang, and E. Ofek. Enabling ﬁne-grained permissions for
augmented reality applications with recognizers. In Proceedings
of the USENIX Security Symposium, 2013.

136136

// / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /
// I n i t i a l B a l l e t g e s t u r e s of the C e c c h e t t i M e t h o d
// G e s t u r e s d e s c r i b e d based on the book
// T e c h n i c a l M a n u a l and D i c t i o n a r y of C l a s s i c a l B a l l e t
// By Gail Grant
// From Dover P u b l i c a t i o n s
// This p a r t i c u l a r set can be found
// in the f o l l o w i n g p i c t u r e :
// http : // mysylph . files . w o r d p r e s s . com / 2 0 1 3 / 0 5 /
// c e c c h e t t i - p o r t - d e - b r a . jpg
// / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / / /

APP ballet :

GESTURE f i r s t - p o s i t i o n :

POSE s t a n d - s t r a i g h t :

point your spine , neck and head up .

POSE p o i n t - f e e t - o u t :

point your right foot right ,
point your left foot left .

POSE s t r e t c h - l e g s :

align your left leg ,
align your right leg .

POSE l o w - a r c - a r m s :

point your arms down ,
rotate your elbows 15 degrees up ,
rotate your left wrist 5 degrees to your right ,
rotate your right wrist 5 degrees to your left .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
s t r e t c h - l e g s ,
l o w - a r c - a r m s .

GESTURE s e c o n d - p o s i t i o n :

POSE m i d - a r c - a r m s :

point your arms down ,
rotate your elbows 30 degrees up ,
rotate your wrists 20 degrees up .

POSE h i g h - a r c - a r m s :

point your arms down ,
rotate your arms 70 degrees up .

POSE o p e n - l e g s - f r o n t a l - p l a n e :

point your legs down ,
rotate your right leg 10 degrees to right ,
rotate your left leg 10 degrees to left .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
s t r e t c h - l e g s ,
o p e n - l e g s - f r o n t a l - p l a n e ,
m i d - a r c - a r m s ,
h i g h - a r c - a r m s .

GESTURE t h i r d - p o s i t i o n :

POSE m i d - a r c - a r m s - t o - r i g h t :

point your arms down ,
rotate your right elbow 30 degrees up ,
rotate your right wrist 20 degrees up ,
rotate your left elbow 10 degrees to your left ,
rotate your left wrist 10 degrees to your right .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
s t r e t c h - l e g s ,
m i d - a r c - a r m s - t o - r i g h t .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
s t r e t c h - l e g s ,
c r o s s - l e g s - o n e - b e h i n d - t h e - o t h e r ,
h i g h - a r c - a r m s - t o - r i g h t .

GESTURE f o u r t h - p o s i t i o n - e n - h a u n t :

POSE h i g h - a r c - a r m s - t o - r i g h t - a n d - u p :

point your right arm down ,
rotate your right arm 70 degrees up ,
point your left arm up ,
rotate your left elbow 15 degrees to your left ,
rotate your left wrist 5 degrees to your right .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
s t r e t c h - l e g s ,
c r o s s - l e g s - o n e - b e h i n d - t h e - o t h e r ,
h i g h - a r c - a r m s - t o - r i g h t - a n d - u p .

GESTURE f i f t h - p o s i t i o n - e n - a v a n t :

POSE i n n e r - a r c - a r m s :

point your arms down ,
rotate your right elbow 20 degrees to your right ,
rotate your right wrist 25 degrees to your left ,
rotate your left elbow 20 degrees to your left ,
rotate your left wrist 25 degrees to your right .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
s t r e t c h - l e g s ,
i n n e r - a r c - a r m s .

GESTURE f i f t h - p o s i t i o n - e n - h a u n t :

POSE a r c - a r m s - u p :

point your arms up ,
rotate your right elbow 15 degrees to your right ,
rotate your right wrist 5 degrees to your left ,
rotate your left elbow 15 degrees to your left ,
rotate your left wrist 5 degrees to your right .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
s t r e t c h - l e g s ,
a r c - a r m s - u p .

GESTURE a - l a - q u a t r i e m e - d e v a n t :

POSE q u a t r i e m e - d e v a n t - l e g s :

put your right leg in front of your left leg ,
point your left leg down ,
point your left foot left .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
q u a t r i e m e - d e v a n t - l e g s ,
h i g h - a r c - a r m s .

GESTURE a - l a - q u a t r i e m e - d e r r i e r e :

POSE q u a t r i e m e - d e r r i e r e - l e g s :

put your right leg behind your left leg ,
point your left leg down ,
point your left foot left .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
q u a t r i e m e - d e r r i e r e - l e g s ,
h i g h - a r c - a r m s .

GESTURE a - l a - s e c o n d e :

POSE s e c o n d e - l e g s :

GESTURE f o u r t h - p o s i t i o n - e n - a v a n t :

POSE c r o s s - l e g s - o n e - b e h i n d - t h e - o t h e r :

put your left ankle behind your right ankle ,
put your left ankle to the right of your right ankle .

point your legs down ,
point your left foot left ,
rotate your right leg 20 degrees to your right .

POSE h i g h - a r c - a r m s - t o - r i g h t :

point your arms down ,
rotate your right arm 70 degrees up ,
rotate your left elbow 20 degrees to your left ,
rotate your left wrist 25 degrees to your right .

E X E C U T I O N :

s t a n d - s t r a i g h t ,
p o i n t - f e e t - o u t ,
s e c o n d e - l e g s ,
h i g h - a r c - a r m s .

137137

