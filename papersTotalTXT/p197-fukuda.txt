Detecting Malicious Activity with DNS Backscatter

National Institute of Informatics/Sokendai

Kensuke Fukuda
kensuke@nii.ac.jp

John Heidemann

University of Southern California
Information Sciences Institute

johnh@isi.edu

ABSTRACT
Network-wide activity is when one computer (the origina-
tor ) touches many others (the targets). Motives for activity
may be benign (mailing lists, CDNs, and research scanning),
malicious (spammers and scanners for security vulnerabili-
ties), or perhaps indeterminate (ad trackers). Knowledge
of malicious activity may help anticipate attacks, and un-
derstanding benign activity may set a baseline or charac-
terize growth. This paper identiﬁes DNS backscatter as
a new source of information about network-wide activity.
Backscatter is the reverse DNS queries caused when tar-
gets or middleboxes automatically look up the domain name
of the originator. Queries are visible to the authoritative
DNS servers that handle reverse DNS. While the fraction of
backscatter they see depends on the server’s location in the
DNS hierarchy, we show that activity that touches many tar-
gets appear even in sampled observations. We use informa-
tion about the queriers to classify originator activity using
machine-learning. Our algorithm has reasonable precision
(70–80%) as shown by data from three diﬀerent organiza-
tions operating DNS servers at the root or country-level.
Using this technique we examine nine months of activity
from one authority to identify trends in scanning, identify-
ing bursts corresponding to Heartbleed and broad and con-
tinuous scanning of ssh.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations—Network monitoring

General Terms
Measurement

Keywords
Internet; Domain Name System; DNS; network activity;
scanning

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
IMC’15, October 28–30, 2015, Tokyo, Japan.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3848-6/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2815675.2815706.

1.

INTRODUCTION

Network-wide activity is when one computer (the origina-
tor ) touches many others (the targets). Malicious activity
is a growing problem in the Internet. Spammers exploit
compromised computers to send mail, one part of the gray-
market ecosystem [30]. Scanners walk the IP address space,
for research [25, 19], in academic [18] or commercial [35]
vulnerability scanning, as cyber criminals searching for vul-
nerabilities [17] (or joyriding [6]), or as nation-states espi-
onage [24, 28]. Knowledge of malicious activity may help
anticipate attacks [12]. Non-malicious activity is of concern
as well: ad trackers and content-delivery networks (CDNs)
also interact with many computers. Studies of benign ac-
tivity help set a baseline [54] or characterize growth (results
like Calder et al. [10], but without active probing).

Unfortunately, it is diﬃcult to understand the scope of
these potential threats because of the Internet’s decentral-
ization. While ﬁrewalls observe activity directed at one net-
work, and a few security providers aggregate data from a
few hundreds [3], by design the Internet has no central van-
tage point to detect widespread activity. Prior work has
used Domain Name System (DNS) traﬃc to assess misbe-
havior seen in speciﬁc networks or resolvers [56, 26, 40, 55,
2], but this work does not generalize to network-wide ac-
tivity. Darknets [37, 34, 54, 13, 14, 17] and honeypots (for
example, [41]) are eﬀective at understanding network-wide
activity, but they miss targeted scans (scanning only Alexa
top websites [17]), and new large darknets are unlikely given
IPv4 full allocation and the huge IPv6 space. Search engines
gather information about activity that appears in the public
web, but information is unstructured and may be delayed by
indexing [48]. (§ 6 has detailed related work.)

This paper identiﬁes a new source of information on network-

wide activity: DNS backscatter, the reverse DNS queries
triggered by such activity (Figure 1 as explained in § 2).
Activities of interest are those that touch many Internet
devices, including malicious or potentially malicious activ-
ity such as spamming and scanning, as well as widespread
services such as CDNs, software updates, and web crawl-
ing. These activities trigger reverse DNS queries as ﬁre-
walls, middleboxes, and servers (queriers) resolve map the
IP address of the originator to DNS name in the process
of logging or host-based authentication. Authoritative DNS
servers provide a point of concentration of these queries that
allows detection of large activities. Since backscatter oc-
curs mostly as automated processes, and we consider only
originators with many queriers, our approach avoids traﬃc
from individuals and so has minimal privacy concerns. Since

197emails to (a.example.com and its neighbors; our goal is to
identify an otherwise unknown originator.

An originator is a single IP address that touches many
targets. In the application classes we study, originators in-
teract with their targets. In principle the originator could
be the victim of spoofed traﬃc (such as a DNS server as part
of an ampliﬁcation attack); we have not identiﬁed such orig-
inators in our data. (For a few application classes, such as
software update service, ad-trackers, and CDNs, the activity
is initiated by the target and some other service co-located
with the target does a reverse-DNS query.)

At the Target: The activity prompts the target’s inter-
est in discovering the originator’s domain name from its IP
address: a reverse DNS mapping that causes a querier to
make a reverse query if the result is not already cached.
This query may be for logging (as by ﬁrewalls), to per-
form domain-name based access control, or to characterize
the originator (for example, mail servers that consider the
sender’s hostname as part of anti-spam measures).
The
querier is deﬁned as the computer that does resolution of
the reverse name. That the target and the querier may be
the same computer, or the querier may be a dedicated re-
cursive resolver shared by several targets.

A reverse DNS query will have the query name (QNAME
4.3.2.1.in-addr.arpa in Figure 1) with the reversed IP
address of the originator, and will ask for a reverse Internet
name (QTYPE PTR, QCLASS IN). In this example, this
query returns spam.bad.jp, but we do not use this response
or even depend on the existence of a reverse name for the
originator.

At the Authority: An authoritative DNS nameserver
(the authority) will reply to this query. We analyze DNS
traﬃc at the authority to infer information about the origi-
nator.

DNS is a hierarchical, distributed database with many
authoritative servers that cover diﬀerent levels and parts of
the namespace. In addition, DNS uses aggressive caching at
nearly all levels of the system. These factors mean that there
are multiple authorities that will see diﬀerent reverse-DNS
traﬃc with diﬀerent amounts of sampling due to caching
and coverage.

The ideal place to observe reverse queries is the ﬁnal au-
thority directly responsible for the originator. In typically
the originator’s company or ISP (serving 3.2.1.in-addr.
arpa in our example), the ﬁnal authority will see all queriers.
However, authorities higher in the DNS hierarchy will see
some attenuated fraction of queriers and can discover un-
known originators. We consider several authorities: a national-
level, top-level server (we examine Japan), and root name-
servers (we consider B and M, two of the 14), as well as
a ﬁnal authority. Our national-level view sees traﬃc only
for originators in address space delegated to that country
(perhaps 2.1.in-addr.arpa in our example). Roots poten-
tially see all originators, but caching of the top of the tree
(in-addr.arpa and 1.in-addr.arpa) ﬁlters many queries,
and visibility is aﬀected by selection algorithms that favor
nearby DNS servers. Some recursive servers, such as those
run by large ISPs or large open DNS providers, will also
have broad visibility.

Backscatter Evaluation: We observe reverse queries at
the authority and use this backscatter to identify the origi-
nator’s application. Queries directly provide the IP address
of the originator, but we use that only to group queriers (we

Figure 1: The process behind a DNS backscatter sensor:
an originator sends mail to targets, causing queries that are
observed at the authority.

backscatter is generated by the targets of network activity,
not the originator, an adversarial originator cannot prevent
its generation.

The contribution of this paper is to identify DNS backscat-
ter as a new concentrator of information about network-wide
activity (§ 2). We show that this source is noisy and at-
tenuated by caching, but careful interpretation of aggregate
queriers allows construction of a new type of sensor (§ 3) that
detects large, automated activities that touch many targets.
We use machine learning to classify the originators of each
activity into broad groups (spammers, scanners, and several
types of commercial activity) with reasonable precision (70–
80%) and robustness (§ 4). Finally, we use backscatter and
classiﬁcation to examine world-wide scanning activity from
three sources and up to nine months (§ 5). We characterize
the “natural footprint” of diﬀerent activities for two days at
two sensors at the DNS root and one national-level server.
We also examine nine months of data, showing that there
is a continuous background of scanning, and identifying in-
creased scanning following announcements of vulnerabilities
such as Heartbleed [38]. Through this study, our work helps
characterize scanning and other network-wide activity; our
approach may also serve to support detection and response.

2. DNS BACKSCATTER’S POTENTIAL

DNS backscatter is the set of reverse DNS queries ob-
served by a DNS authority as the result of a network-wide
activity. In such activity, an originator interacts with many
targets. Targets usually do not directly interact with the
authority, but do so through a querier that acts on their be-
half. Figure 1 shows these players. We use DNS backscatter
observations to identify and classify the originators.

Origination of Network Activity: We look for net-
work activity that aﬀects many targets. An originator in-
teracts with many target hosts, for purposes that are le-
gitimate (large mailing lists and web crawlers), malicious
(spam), or perhaps in between (scanning and peer-to-peer
sharing). Our goal is to infer and classify the originator
(§ 3) and to understand the size of its footprint (from [47])—
how many targets it touches.
In Figure 1, the originator
is spam.bad.jp from IP address 1.2.3.4, and it sends spam

Auth serverAuthorityRecursive resolverQuerierSpammerOriginatorMail serverTargetspam.bad.jp(1.2.3.4)a.example.com(192.168.0.1)rdns.example.com(192.168.0.3)Mail serverMail serverb.example.com(192.168.0.2)c.example.jp(172.16.0.1)rdns.example.jp(172.16.0.2)DNS: PTR? 4.3.2.1.in-addr.arpaSMTP (application trafﬁc)Query log @ Auth server192.168.0.3 "PTR? 4.3.2.1.in-addr.arpa" 172.16.0.2 "PTR? 4.3.2.1.in-addr.arpa" m.root-servers.net(1.in-addr.arpa)Recursive resolvera.dns.jp(2.1.in-addr.arpa)bad.jp: the ﬁnal authority(3.2.1.in-addr.arpa and 4.3.2.1.in-addr.arpa)198ignore the originator reverse DNS record, if any). Instead
we use information about queriers to suggest the origina-
tor’s application. Each individual query has little informa-
tion, but the collective behavior of all queriers generating
backscatter indicates the nature of an originator’s activity.
For example, spammers send mail to many mail servers, gen-
erating backscatter from these servers and anti-spam mid-
dleboxes. Similarly, large-scale network scanning causes ﬁre-
walls at targets log the originator’s domain name.

The number of reverse queriers also provides a rough idea
of the size of the originator’s activity. Unfortunately, the
DNS’s caching policies make it extremely diﬃcult to quan-
tify originator traﬃc rates—we caution that querier counts
only approximate activity sizes.

An adversarial originator may seek to hide its activity.
Spreading activity over many originators will reduce detec-
tion, but other aspects of our approach depend on targets
and thus are outside the control of the originator.

Privacy: Analysis of DNS traﬃc raises potential pri-
vacy issues, since much DNS traﬃc originates from activ-
ity by individuals. Our approach minimizes these concerns
for several reasons. First, the data sources we use intrinsi-
cally mask the visibility and identity of individuals. Caching
heavily attenuates all queries seen by the authority, and a
shared cache obscures the identity of any individual. We
see network-wide activity only because of its many targets,
while activity of any given individual is extremely unlikely to
appear. Second, authorities have little or no direct contact
with individuals due to indirection from recursive resolvers.
Finally, while raw data at an authority is a mix of individ-
ual and automated traﬃc, the reverse queries we consider
is nearly all automated. Humans typically use DNS to map
names to addresses; almost all reverse queries are from au-
tomated sources. (As evidence, we examined ten minutes
of B-Root queries in B-post-ditl. Using not-found replies
[NXDomain] as evidence of typos, only 8 of 126,820 reverse
queries against in-addr.arpa are not-found, while about
half of the forward queries for IP addresses are not-found.)
Our analysis and its output also raise minimal concern.

Our automated analysis eﬀectively anonymizes its input through
abstraction and aggregation, as input becomes feature vec-
tors about collective queriers.
It identiﬁes and classiﬁes
originators in bulk, and we consider only originators in the
top fraction of network-wide activity—a set that is neces-
sarily automated to reach that scope. Finally, when exam-
ining speciﬁc originators to validate our results, we would
anonymize any that present personally-identiﬁable informa-
tion (PII). None so far show any PII.

Our work has been institutionally reviewed and approved
as non-human-subjects research (USC IIR00001718). We
see minimal privacy risk in this work, and substantial beneﬁt
in better understanding the scope of network-wide activity
such as scanning.

3. METHODOLOGY: BACKSCATTER TO

ACTIVITY SENSOR

We next summarize our approach to make DNS backscat-
ter a sensor for network-wide activity: collecting DNS data
at the authority, classifying data for each originator with fea-
tures based on the queries and queriers, and then clustering
originators into activities.

3.1 Data Collection: Queries at the Authority
We begin with data collection at the authority. We ob-
serve all DNS queries at arrival and retain only reverse DNS
queries (PTR queries made against in-addr.arpa). Each
query results in an (originator, querier, authority) tuple; we
identify the originator from the QNAME, and the querier
and authority are the source and destination of the DNS
packet.

Queries may be obtained through packet capture on the
network or through logging in DNS server itself. DNS packet
capture techniques are widely used [51, 20]. DNS logging is
supported in most servers, and tools such as dnstap deﬁne
standard logging formats [50]. Centralized DNS information
is available today through services such as SIE [45]. We
describe the datasets used in this paper in § 3.6.
3.2 Interesting and Analyzable Originators

We want to focus our classiﬁcation on only those origina-
tors that are interesting, carrying out activities that aﬀect
large parts of the network, and that are analyzable, provid-
ing enough information from queriers that we can reasonably
classify them.

We identify these originators by analyzing data over time
intervals that are long enough to include a signiﬁcant num-
ber of analyzable originators, and then identifying the most
interesting among them based on how many targets they
touch.

We generate a feature vector vo for each originator o over
some time interval lasting d days. The time interval un-
der consideration must be long enough that the originator
touches enough targets to allow inference of the originator’s
application class. We consider 20 or more unique queriers
per originator to be suﬃcient for analysis. We select the
time interval for each dataset to meet this goal (details of
the datasets are shown in § 3.6). DITL datasets [16] last
1–2 days, and for each we generate one feature vector for
each originator across the whole dataset (d is the dataset
duration, 36 or 50 hours). For the M-sampled dataset we
use d = 7 days, thus generating an array of feature vectors
i where i ∈ [0, 35].
vo
Originators that touch many targets have larger, more in-
teresting activities. To identify the most proliﬁc originators
we record the number of unique (originator, querier) combi-
nations in each time interval. We rank originators by num-
ber of unique queriers in the time interval and retain only
the N originators with the most unique queriers. When we
count originators (§ 5.3) we take all originators with more
than 20 unique queriers.
3.3 Queries to Originator Features

We deﬁne static and dynamic features for each originator.
Static features are derived from the domain names of the
queriers, using diﬀerences in which targets diﬀerent origi-
nators touchto infer the originator’s intentions. Dynamic
features use spatial and temporal patterns in queries.

Querier domain names provide these static features, often

determined by speciﬁc keywords:

home computers with automatically assigned names like
home1-2-3-4.example.com. The name includes dig-
its of the IP address and a keyword: ap, cable, cpe,
customer, dsl, dynamic, ﬁber, ﬂets, home, host, ip,
net, pool, pop, retail, user.

199mail mail servers like mail.example.com. Keywords: mail,
mx, smtp, post, correo, poczta, send*, lists, newsletter,
zimbra, mta, pop, imap.

ns nameservers like ns.example.com. Keywords: cns, dns,

ns, cache, resolv, name.

fw ﬁrewalls like firewall.example.com. Keywords: ﬁre-

wall, wall, fw.

antispam anti-spam services, like spam.example.com. Key-

words: ironport, spam.

www web servers like www.example.com
ntp NTP servers like ntp.example.com
cdn CDN infrastructure, including suﬃx of Akamai, Edge-

cast, CDNetworks , LLNW.

aws Amazon AWS, including suﬃx of amazonaws.
ms Microsft Azure, checked with web page.
google IP addresses assigned to Google, as conﬁrmed by

SPF record in DNS.

other-unclassiﬁed a name not matching the categories above.
unreach cannot reach the authority servers.
nxdomain no reverse name exists.

Domain names may match multiple static features (for
example, mail.google.com is both google and mail). We
match by component, favoring matches by the left-most
component, and taking ﬁrst rule for components with multi-
ple keywords. (Thus both mail.ns.example.com and mail-ns.
example.com are mail ).

The static portion of the feature vector for an originator
is the fraction of queriers that match each of the features.
For example, an originator where all queriers have “mail” in
their domain names would have the mail feature as 1 and all
others as zero. Another where a quarter have “mail” and the
rest have “ﬁrewall” would have feature vector with (mail=
0.25, ﬁrewall= 0.75). We use the fraction of queriers rather
than absolute counts so static features are independent of
query rate.

We use these dynamic features to capture temporal and

spatial aspects of the queries:

queries per querier (temporal) As a rough query rate we

compute the mean number of queries per querier.Diﬀerences
in caching and TTLs prevent generate an exact rate,
but this metric is roughly proportional to query rate.
query persistence (temporal) To show how often an orig-
inator is active we count the number of 10-minute-long
periods that include the originator.

local entropy (spatial) To see if an activity aﬀects many
people we compute the Shannon entropy of /24 preﬁxes
of all queriers.

global entropy (spatial) We compute the Shannon entropy
of the /8 preﬁx of all querier IP addresses. Since /8
preﬁx are assigned geographically, wide variation here
shows global activity.

unique ASes (spatial) the number of ASes across all queriers

normalized by the total number of ASes that appear
in the time interval. (ASes are from IP addresses via
whois.)

unique countries (spatial) the number of countries across
all queriers, normalized by the total number of coun-
tries that appear in the time interval. We deter-
mine country from the IP using MaxMind GeoLiteCity
database [32].

queriers per country (spatial) unique queriers per coun-

try.

queriers per AS (spatial) unique queriers per AS.

To avoid excessive skew of querier rate estimates due to
queriers that do not follow DNS timeout rules [52, 11] we
eliminate duplicate queries from the the same querier in
a 30 s window. Some variation remains due to diﬀerent
caching times (TTLs) for diﬀerent portions of the names-
pace. In some cases, such as fast ﬂux domains, low TTLs
will assist detection.

We selected the features and classes (in this section and
the next) after iteration with our several datasets. Diﬀerent
features and classes will produce diﬀerent results (for exam-
ple, although we omit details due to space, we see higher ac-
curacy with fewer application classes), but our results show
the general approach is sound.
3.4 Originator Features to Application Classes
Finally we classify each originator based on its feature
vector using machine learning. We use several standard
machine-learning algorithms: a decision tree (Classiﬁcation
And Regression Tree; CART) [8], random forest (RF) [7],
and kernel support vector machines (SVM) [43]. For non-
deterministic algorithms (both RF and SVM use random-
ization), we run each 10 times and take the majority classi-
ﬁcation. These algorithms classify each originator into one
of the following classes:

ad-tracker Servers implementing web-bugs (objects in em-

bedded in web pages) to track user for advertising.

cdn Computers that are part of public content delivery net-

works (Akamai, CDNetworks, etc.)

cloud Front-end hosts of cloud service such as Google map,

Goggle drive, and Dropbox.

crawler Web crawlers by Google, Bing, Baidu, etc.
dns Large DNS servers
mail Mail servers that send mail to large mailing lists and

webmail service

ntp Large Network Time Protocol (NTP) servers
p2p End-user computers that participate in peer-to-peer
ﬁle sharing. (We added this late in our work; our anal-
ysis of root servers does not include it.)

push Push-based messaging services for mobile phones (An-

droid and iPhone); typically TCP port 5223.

scan Internet scanners using ICMP, TCP, or UDP.
spam Computers (end-user or mail servers) that send spam

to many destinations.

update Servers for software update distribution run by OS,

computer, and printer vendors

Data over time: For short datasets, each originator has
one feature vector and one class. For longer datasets we see
an array of feature vectors (vo
i ) and a time-speciﬁc class co
i .
Because our observations are incomplete and classiﬁcation is
imperfect, an originator may change class over time, so our
ﬁnal step is to identify the originator as the most common
class seen over all time (the statistical mode of co
i ). We di-
vide the M-sampled data into 35 week-long periods; all other
datasets are analyzed as one period of 1–2 days. Longer ob-
servations for the M-sampled data partially compensate for
attenuation in the data due to its 1:10 downsample (see also
§ 3.6).

200Training: Classiﬁcation requires training. We label a
subset of each dataset as ground truth, manually identifying
the application class of 200–300 originators in each dataset.
Traﬃc seen by each authority is diﬀerent for many reasons:
placement in the DNS hierarchy, size, physical location, use
of anycast, and possible use of sampling (for M-sampled).
For this reason, we currently train for each dataset sepa-
rately, although in some cases originators appear in multiple
datasets and so we can reuse labeled ground-truth. (Study-
ing of training portability and its eﬀects on accuracy is on-
going.)

We do training by generating feature vectors as described
above, then feed this input and known output into each ML
algorithm.
3.5 Constraints in Backscatter as a Data Source
While DNS backscatter provides a new tool to identify
network-wide activity, it is an incomplete source about orig-
inators and their activity.

First, querier domain-names only provide limited informa-
tion about targets. Domain names may be overly general,
unavailable, or never assigned. Currently we see 14–19% of
queriers without reverse names, and the fraction serves as a
classiﬁcation feature, but if all queriers lacked reverse names
our approach would not work.
Other network services
use reverse names, such as anti-spam and network reputa-
tion services, ﬁnding as we do that they are imperfect but
still useful. Our results use only backscatter, but we show
that applications will beneﬁt from combining it with other
sources of information (such as small darknets) to overcome
the limitations of querier names alone.

Second, the signal provided in DNS backscatter is spread
over many authorities by DNS anycast, attenuated by caching
in recursive DNS resolvers, and complicated by caching du-
rations (TTLs) that diﬀer across the DNS hierarchy.These
challenges prevent us from getting strong estimates on the
rates of originator activity, but we show that they do not
prevent their identiﬁcation or classiﬁcation.

Finally, an adversarial originator may try to defeat our
analysis. As with spam, spreading traﬃc from an activity
across many separate originating IP addresses or longer du-
rations disperses the signal. We cannot prevent this coun-
termeasure, but it greatly increases the eﬀort required by
an adversarial originator. Fortunately our classiﬁcation de-
pends on queriers (not originators), and they cannot be ma-
nipulated by the originator.

In this paper we show that querier provides reasonable
classiﬁcation accuracy and backscatter’s signal identiﬁes hun-
dreds of originators. We are working to reﬁne our esti-
mates of accuracy and sensitivity, and of course additional
data sources, larger labeled ground-truth, and more reﬁned
classes may improve on our work.
3.6 Datasets

This paper uses DNS datasets from three authorities: one
national-level top-level domain, operators of two root servers
as shown in Table 1.

JP-DNS operates the .jp country code domain for Japan;
we have data from all seven of their anycast sites. We ob-
tained data from two root server operators: B is a single site
in the US west coast, and M operates 7 anycast sites in Asia,
North America, and Europe. We use root datasets from
the short-term, complete (unsampled) observations taken as

Figure 2: Static features for case studies, derived from
querier domain names. (Dataset: JP-ditl.)

part of the 2014 DITL collection [16] (for B-Root, shortly
after 2014 DITL). We also use data for M-Root’s 2015 DITL
collection (§ 4.4). These root datasets are available to re-
searchers through DNS-OARC.

For longitudinal analysis we draw on 9 months of data
taken at the M-Root server. This dataset (M-sampled) is
sampled with only 1 out of every 10 queries in deterministic
manner; this sampling decreases our sensitivity but is part
of their policy for long-term collection. We also use an 5-
month unsampled dataset (B-long) taken from B-Root to
evaluate for controlled experiments (§ 4.4).

4. VALIDATION

Classiﬁcation with machine learning requires appropriate
features and accurate training data. Here we illustrate our
features with several diﬀerent type of originators, then iden-
tify external sources we use to label the ground truth for
training and validation. Finally, we validate our approach
by training on a random portion of the ground truth and
testing the remainder.
4.1 Distinguishing Different Classes of Origi-

nators

We ﬁrst illustrate how the DNS backscatter allows us to
diﬀerentiate the activity of six diﬀerent originators: scan-
icmp, scan-ssh, ad-tracker, cdn, mail, and spam. Figure 2
and Table 2 show their static and dynamic features.

Static features: Static features (Figure 2) distinguish
several cases. We examine two scanners: scan-icmp is the
Japanese site for a research scanner doing outage detec-
tion [42], while scan-ssh is an unknown party probing ssh
(TCP port 22) on all computers. Scanners trigger many
queries from shared nameservers (NS), as well as nxdomain,
home, and static addresses. We believe this mix results from
scanners walking all IP addresses and triggering ﬁrewall and
logging reverse queries; the large number of NS queriers sug-
gest these services often use an ISP’s shared resolver, but not
always.

Ad-trackers are servers triggered by user’s web behavior.
While similar to scanners, this classes see more shared name-
servers, perhaps because it is queried by end-users’s web
browsers.

Our cdn case is an Akamai server based in Japan.

It
shows a much higher fraction of home queriers than others.
We believe this shift results from heavy CDN use at homes.

 0 0.2 0.4 0.6 0.8 1scn-icmpscn-sshad-trackcdnmailspamfractioncase studyhomensnxdomunclotherhomensnxdomunclotherhomensnxdomunclotherhomensnxdomunclotherhomens  nxdommailunclotherhomens  nxdommailspamunclotherhomensnxdommailspamunclassifiedother201dataset

type
ccTLD JP-ditl
root
root
root
root
root

operator
JP-DNS
B-Root
B-post-ditl
B-Root
B-long
M-ditl
M-Root
M-ditl-2015 M-Root
M-sampled M-Root

start (UTC)
2014-04-15 11:00
2014-04-28 19:56
2015-01-01
2014-04-15 11:00
2015-04-13 11:00
2014-02-16

duration sampling
50 hours
36 hours
5 months
50 hours
50 hours
9 months

no
no
no
no
no
1:10

Table 1: DNS datasets used in this paper.

queries (×109)
(reverse)
(all)
0.3
4.0
2.9
0.04
5.14
290*
0.06
8.3
0.07
9.9
36.2
1.5

qps (×103)

(all)
22
22
22*
46
55
1.6

(reverse)
1.8
0.2
0.39
0.3
0.4
0.07

case

scan-icmp
scan-ssh
ad-track

cdn
mail
spam

queries/
querier

global
entropy

local

entropy

queriers/
country

3.3
4.7
2.3
4.4
1.7
3.4

0.83
0.84
0.85
0.48
0.71
0.85

0.92
0.96
0.94
0.97
0.94
0.95

0.006
0.006
0.017
0.018
0.009
0.005

Table 2: Dynamic features for case studies.

Finally, mail and spam both show a much higher fraction
of the mail feature, consistent with application. Although
hard to see on the graph, spam shows more than twice the
fraction of spam-related queriers (0.1% compared to 0.03%),
indicating heavier involvement of spam-ﬁltering and logging
systems.

Dynamic features: Table 2 lists the distribution of some
dynamic features for the six originators. As described in
§ 3.3, query counts only approximate true rates because of
caching, but we believe relative comparisons are appropriate
for classiﬁcation.

We see that the queries:querier ratio helps distinguish mail
from spam. Global entropy demonstrates geographic dis-
persion of the queriers. Low global entropy for cdn reﬂects
CDN selection algorithms that associate our Japan-based
CDN server with Asian clients. Similarly, the global en-
tropy mail is lower because the language of this speciﬁc list
is Japanese. Queries per country also reﬂects geographic
coverage, with high values of ad-tracker, cdn, and mail sug-
gesting geographically constrained targets.
The examples in § 3.3 illustrate how these features help
identify our application classes, and their overlap suggests
the potential for machine learning as a framework to inter-
pret them optimally. We quantify our approach in § 4.3.
4.2 Conﬁrming Labeled Ground-Truth
We require ground truth with originators labeled into our
twelve application classes (§ 3.4) both to train our classiﬁer
and to evaluate its accuracy (§ 4.3).

In labeling ground truth we strive for accuracy over quan-
tity because a mix of inaccurate originators will mis-train
our classiﬁer. We draw on multiple sources of external data
(blacklists, darknets, and manual investigation) to get rea-
sonable coverage of most application classes, with 180 to 700
samples depending on the dataset (Table 3).

We generate moderate to large lists of potential IP ad-
dresses in each application class from external sources (de-
scribed below), then intersect it with the top-10000 origina-
tors in dataset by the number of queriers. We then verify
the intersection manually. We determine the verifying appli-
cation class of each originator manually using the methods
described below. We added two categories (push and up-

date) after examining all of the top 100 and 1000 largest
originators, respectively.

Validation for each class is as follows:

ad-tracker We identify servers associated with advertising
aﬃliation and tracking systems (such as doubleclick.
net and kauli.net) by crawling around 20 blog pages.
We also registered with ad aﬃliation programs for four
services and added the servers they recommended.

cdn We identiﬁed content-distribution networks for several
large providers (Akamai, Edgecast, CDNetworks, Chi-
naCache, and, CloudFlare) based on their reverse do-
main names and WHOIS on their IP addresses.

cloud We crawled IP addresses of front-end hosts for Google
services such (map, drive, and news) and Dropbox
from Japan and the US.

crawler We conﬁrm web crawler IP addresses from UserA-
gent strings in web server logs of NII, originator reverse
domain names, and http://botvsbrowsers.com.

dns We started with all root DNS servers, country-code
TLD servers, then we added servers for large Japanese
ISPs that had large originator footprints.

mail We conﬁrm mail originators by examining over 100
Japanese mailing lists operated by mail ASPs and com-
panies themselves. We see about 300 IP addresses over
about three months in 2014. We also identify mail
originators for cloud-based mail including Google, Mi-
crosoft, Apple, Facebook, Linkedin, and Amazon, as
conﬁrmed by domain names in e-mail message head-
ers.

ntp We consider well-known Stratum 1 servers and crawled
IP addresses of NTP servers in Open NTP project (at
*.pool.ntp.org).

push We list IP addresses that send packets with TCP port
5223 (Lync mobile client push), as observed in sampled
netﬂow traﬃc from the University of Tokyo.

p2p We identify IP addresses running DHT-based BitTor-
rent from htindex.com in fall 2014 (As of 2015, this
service no longer operates).

scan We conﬁrm scanners by looking at traﬃc in two dark-
nets (one a /17 and the other a /18 preﬁx) located in
Japan. A conﬁrmed scanner sends TCP (SYN only,
not SYN-ACK), UDP, or ICMP packets to more than
1024 addresses in at least one darknet, or is a known
research scanner (such as Trinocular [42] and shad-
owserver [46]).

spam We conﬁrm spammer IP addresses with data from
DNS blacklists (DNSBL) run by 9 organization (badips,
barracuda, dnsbl.sorbs, inps.de, junkemail, openbl, spam-
haus, spamrats, spam.dnsbl.sorbs); we consider only
the spam portion of blacklists that distinguish spam
from others such as ssh-brute force.

202dataset
JP-ditl
B-post-ditl
M-ditl
M-sampled

ad-track
15
13
13
54

cdn cloud crawler dns mail ntp p2p push scan spam update
6
-
-
-

44
46
50
111

64
35
43
136

25
29
33
124

-
17
16
35

8
29
36
81

26
16
17
52

-
16
16
82

-
12
12
73

10
5
8
-

37
-
-
-

total
235
214
240
746

Table 3: Number of examples of each application class in labeled ground-truth, per dataset.

CART

post-
ditl

JP
ditl
B

RF
SVM
CART

dataset algorithm accuracy precision recall F1-score
0.66 (0.05) 0.63 (0.08) 0.60 (0.06) 0.61 (0.06)
0.78 (0.03) 0.82 (0.05) 0.76 (0.06) 0.79 (0.05)
0.73 (0.04) 0.74 (0.05) 0.71 (0.06) 0.73 (0.05)
0.48 (0.05) 0.48 (0.07) 0.45 (0.05) 0.46 (0.05)
0.62 (0.05) 0.66 (0.07) 0.60 (0.07) 0.63 (0.07)
0.38 (0.11) 0.50 (0.14) 0.32 (0.13) 0.39 (0.13)
0.53 (0.06) 0.52 (0.07) 0.49 (0.06) 0.51 (0.06)
0.68 (0.04) 0.74 (0.06) 0.63 (0.05) 0.68 (0.05)
0.60 (0.08) 0.68 (0.10) 0.52 (0.08) 0.59 (0.09)
0.61 (0.03) 0.65 (0.04) 0.58 (0.04) 0.61 (0.04)
0.79 (0.02) 0.82 (0.02) 0.77 (0.03) 0.79 (0.02)
0.72 (0.02) 0.76 (0.03) 0.70 (0.03) 0.73 (0.02)

RF
SVM
CART

RF
SVM
CART

RF
SVM

sampled

M
ditl

M

Table 4: Validating classiﬁcation against labeled ground-
truth.

update We identiﬁed 5 large software update services hosted
in Japan corresponding to computer and printer ven-
dors such as Sony, Ricoh, and Epson.

We identiﬁed these classes after iteration and manual ex-
amination of data, traces, and applications. They share in
common that a single originator touches many targets that
make reverse DNS queries, but activity is sometimes ini-
tiated by the originator (spam, mail, and scan) and other
times likely a side eﬀect of activity at the target (such as
update, ad-tracker, and cdn).
4.3 Classiﬁcation Accuracy and Algorithm Choice

We next carry out cross-validation, using random subsets
of labeled ground-truth to test the classiﬁcation accuracy
and to compare three classiﬁcation algorithms: Classiﬁca-
tion And Regression Tree (CART), Random Forest (RF),
and Kernel Support-Vector Machines (SVM).

Classiﬁcation accuracy: To evaluate a particular machine-

learning algorithm, for each dataset we pick a random 60%
of the labeled ground-truth for training, then test on the
remaining 40% of data. We repeat this process 50 times,
testing each subset with all three algorithms. For each run
we compute accuracy ((tp + tn)/all ), precision (tp/(tp + fp),
recall (tp/(tp +fn), and F1-score (2tp/(2tp +fp +fn)), where
tp: true positive, tn: true negative, fp:
false positive, fn:
false negative. Table 4 shows the mean of each metric over
the 50 iterations, with standard deviations in smaller type.
Overall, we ﬁnd that RF outperforms SVM and CART in
most metrics.The accuracy of the best algorithm over each
dataset is 0.7 to 0.8, suggesting classiﬁcation from our lim-
ited data is not easy, but fairly strong given 12 categories
(thus an expected 0.08 accuracy for guessing). We see mis-
labeling of application classes where the training data is
sparse: ntp, update, ad-tracker, and cdn for JP-ditl. In par-
ticular, the classiﬁcation ratio of update events is very low.
Reducing the number of classes would improve accuracy, at
the cost of less useful results.
Improving the accuracy of

rank

1
2
3
4
5
6

JP-ditl

feature
mail(S)
home(S)
spam(S)

nxdomain(S)
unreach(S)

global entropy(D)

Gini
8.4
7.9
6.3
6.2
5.2
5.0

M-ditl

feature
mail(S)
ns(S)

unreach(S)

query rate(D)

home(S)

nxdomain(S)

Gini
12.5
8.3
7.0
6.2
6.0
5.8

Table 5: Top discriminative features. Classiﬁer: RF.

classiﬁcation across multiple classes with unbalanced train-
ing data is an active area of research in machine learning.
Our current results show promise and will improve as the
ﬁeld progresses.

Similarly, for JP-ditl, p2p is sometimes misclassiﬁed as
scan. Manual inspection shows that misclassiﬁed p2p actu-
ally sent traﬃc to dynamically-assigned ports in our dark-
nets as well. We conclude that they are mis-behaving P2P-
clients [31, 9] and these random probes are triggered by
software bugs or by injection of random IP addresses by
anti-P2P services.

Choice of authority: While accuracy is good, accuracy
for B post-ditl and M-ditl are slightly poorer (60–75%) be-
cause roots are attenuated and these datasets are short. M-
sampled does better because it uses longer observations (7
days, not 2). JP accuracy is highest because it is unsampled
and lower in hierarchy. We conclude that approach can be
strong, but one must be careful to match the quality of the
signal and training data.

Feature choice: In evaluating classiﬁcation, we natu-
rally would like to know what features are most discrimi-
native. Random Forest provides this information, and Ta-
ble 5 lists the top discriminative features for two datasets,
as determined by Gini coeﬃcient [53]. Larger Gini values
indicate features with greater discriminative power. We
see that mail, home, nxdomain, and unreach are important
static features in both datasets. Query rate and global en-
tropy are important in diﬀerent datasets, perhaps reﬂecting
M-ditl’s weaker global signal and JP-ditl’s more regional sig-
nal. These results are consistent with intuitive case studies
(§ 4.1).

Algorithm choice: Since Random Forest provides the
best accuracy and is consistent (small standard deviations),
we use that algorithm to characterize originators in the next
section, after retraining with the full labeled ground-truth
data.

4.4 Controlled Experiments to Evaluate DNS

Caching

Attenuation from DNS caching makes interpreting backscat-

ter challenging. Such caching is diﬃcult to model: ﬁrst, the
DNS resolver infrastructure can be quite complex [44]. Sec-
ond, DNS caches are kept for varying durations and caching

203Figure 3: Size of footprint of random network scans at the
ﬁnal authority. (Datasets: B-long and M-ditl.)

Figure 4: CDF of r, the fraction of the most common
class over all weeks with q or more queriers per originator.
(Dataset: M-sampled.)

of the resolution tree (in-addr.apra) will be caused by
many competing users.

Attenuation: To estimate the number of queriers that
respond to a large network event we conducted a controlled
experiment where we probe a fraction of the IPv4 Internet
from a host where we can monitor queries sent to the ﬁ-
nal reverse DNS server for the prober. We set the TTL of
the reverse DNS record (PTR) to zero to disable or mini-
mize caching (some resolvers force a short minimum caching
period), thus we should observe all queriers triggered in re-
sponse to our scan. We scan several protocols (ICMP, TCP
port 22, 23, 80, and UDP port 53, 123) using ZMap [19],
varying the fraction of the address space we scan from 0.0001%
(4k addresses) to 0.1% (4M addresses) of the whole IPv4
space. The time required for the biggest scan (0.1%) was 13
hours. We run each scan up to 5 times. We also examine
8 full-internet scans (nearly 100% of unicast IP) taken with
Trinocular [42], starting at two diﬀerent months (January
and April 2015) from four diﬀerent sites [49].

Figure 3 shows the number of queries we see as we grow
the size of the scan. Circles represent experimental trials
measured at the ﬁnal authority (slightly jittered). The di-
agonal line represents a best ﬁt: roughly 1 querier per 1000
targets, but actually a power-law ﬁt with power of 0.71. We
also examine M-sampled and B-long, reporting what they
see for 0.1% and 1% ZMap scans and the 100% Trinocular
scans. This large reduction in responses at root servers is
due to both DNS caching and because not all targets are
actually interested in the scanner.

False negatives: Our controlled experiments can evalu-
ate false negatives (missed network-wide events). The hor-
izontal line at 20 queriers is our detection threshold, so we
see that the ﬁnal authority will detect all events scanning
0.001% of the Internet or more.

We expect greater caching at higher levels of the DNS
hierarchy. To measure this, we examined M-ditl-2015 data
for evidence of these trials. Only two trials overlap with
this datasets: one for 0.01% and the other for 0.1%. Of
these, we ﬁnd two queriers for the 0.1% trial (the blue X)
in Figure 3. Greater attenuation at higher levels of DNS
means that it will detect only much larger (space) or longer
(in time) activity. (We are currently extending this study to
trials at larger percentages.)

This experiment shows that backscatter is highly atten-
uated due to disinterested targets and DNS caching, but
responses follow the number of targets.
4.5 Sensitivity of Results

Finally, we use M-sampled to evaluate the sensitivity of
our conclusions by looking at when or if classiﬁcations change
over time. We current vote on classiﬁcations of each origi-
nator over all weeks. To estimate degree of consensus in a
vote, we deﬁne r as the fraction of weeks when the preferred
class (the most common response) occurred of all weeks that
originator appeared. When r is near one, we see consistent
activity, but when r < 0.5 it seems likely that either the orig-
inator is changing activity over time (perhaps diﬀerent ma-
chines behind a NAT, or a botnet node being repurposed),
or doing two things concurrently, or we classify variations in
behavior diﬀerently, suggesting an incomplete training set
or indistinguishable classes.

Figure 4 shows the cumulative distribution of the ratio
r, looking at subsets of data with at least q queriers per
originator. To avoid overly quantized distributions we show
only originators that appear in four or more samples (weeks).
Requiring more queriers per originator (larger q) reduces the
number of eligible originators, but the number of samples
(shown in parenthesis in the legend) are all large enough to
be robust.

We see that more queriers (thus more data for classiﬁca-
tion) provide more consistent results, since with q = 100,
about 60% of originators are strongly consistent. Even with
a threshold of 20 querier per originator, 30% of origina-
tors are consistent. In addition, almost all originators (85–
90%, depending on q) have a class that has strict majority
(r > 0.5). Thus our approach almost always (85–90%) pro-
vides a consistent result.
For the few where the strongest class is only a plurality
(r ≤ 0.5), we wanted to see if there are two nearly equally
strong classes. We examined the entropy for originators in
this case: we ﬁnd that usually there is a single dominant
class and multiple others, not two nearly equally common
classes.

These initial results suggest our approach is consistent for
observers with at least 20 queriers, although noise grows as
queriers approach that threshold.

100101102103104105106103104105106107108109100%10%1%0.1%0.01%0.001%0.0001%full IPv4 spacedetection threshold (20 queriers)M-ditl-2015B-long(censuses)final authority(measurements)(fit)Number of unique queriers (IP addresses)Number of targets (IP addresses)  0 0.2 0.4 0.6 0.8 1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Cumulative distributionRatio of majority classq = 20 (6533)q = 50 (1022)q = 75   (515)q = 100 (308)204diﬀerent authorities see diﬀerent applications, and we see ev-
idence of team of coordinated scanners even with no direct
information from originators.

Size of application classes: There are thousands of
unique originators that touch large parts of the Internet.
Table 6 shows how many originators we see in each originator
class for each dataset, with classes with counts within 10%
of the largest count in bold. We use our preferred classiﬁer
(RF) with per-dataset training over the entire ground-truth.
Classes that lack ground truth for some dataset have no
matches (a “-”).

Applications vary by authority: The classes of appli-
cations seen at diﬀerent authorities vary considerably. For
JP-ditl, spam is the most common class of originator. Al-
though Japan hosts computers for most major CDNs, the
size of the cdn class seen from backscatter is small because
CDNs often use address space assigned by other registrars
(we verify this statement for Akamai and Google with geolo-
cation, whois and prior work [21]). The update class is ex-
actly those in labeled ground-truth. We identiﬁed this class
in examining the data (not from an external source), and
lack of additional examples suggests either class has insuf-
ﬁcient training data to avoid over-ﬁtting, or update servers
are rare.

Both unsampled root servers (B-post-ditl and M-ditl) show
similar distributions of activity, with mail the most common
and spam and cdn both close. The larger number of CDNs
in at M-Root is due to 300 cdn originators located in two
Chinese ISPs and interacting with queriers in China. Classi-
ﬁcation appears correct (they do not send traﬃc to darknets,
nor appear in spam blacklists), but the originators lack do-
main names and we cannot identify them. Such originators
appear only in M-ditl, suggesting that their queriers may be
using DNS resolvers that prefer nearby authorities, since M-
Root is well provisioned in Asia while B-Root is only based
in North America.

Long-term, sampled root data (M-sampled) has some im-
portant diﬀerences from short term (M-ditl). Consider rela-
tive sizes of classes (since absolute counts vary due to dataset
duration), we see many more scanner and spammers in long-
term data. We believe the size of these categories reﬂect
churn in the population carrying out the activity. We expect
churn in spamming where computers known for spamming
are less eﬀective. We measure churn directly for scanners in
§ 5.3.

Big footprints can be unsavory: The mix of appli-
cations varies as we examine originators with smaller foot-
prints, but we see that big footprints are often unsavory ac-
tivity. Figure 6 shows how originator classes change as we
look at more originators with smaller footprints (from Fig-
ure 6a to Figure 6c).

The largest footprints are often spammers (in JP-ditl) or
scanners (for B and M). By contrast, we see that mail ap-
pears only in the top-1000 and top-10000, suggesting that le-
gitimate mail servers may service large mailing lists (to many
targets), but spammers touch many more targets. For B and
M, more spammers rise in Figure 6c, suggesting spreading
of traﬃc over many smaller originators to evade ﬁltering.

By contrast, large but not top originators are often infras-
tructure: cloud, mail, ad-tracker, and crawler. In general,
we ﬁnd that application classes have a “natural” size, with
some favoring origins with large footprints (prominent in

Figure 5: Distribution of originator footprint size.

5. RESULTS

We next study network-wide activities with our method,
identifying large network events and trends in diﬀerent ap-
plications and over time. Since our approach is based on
feedback from targets, our results complement prior studies
(such as darknets) and will observe targeted events will not
appear in darknets.
5.1 Sizes of Originator Footprints

We estimate the footprint of each originator as the number
of unique queriers per originator. Figure 5 shows the fraction
of originators with each footprint size a log-log scale for each
of our three datasets.
Our data suggests there are hundreds of originators that
touch large parts of the Internet. Our controlled trials (§ 4.4)
show high attenuation at root servers, yet hundreds of orig-
inators have footprints suggesting they scan most or all of
the Internet (590 in M-ditl and 298 in B-post-ditl have foot-
prints larger than 102).

The distributions of footprints is consistent across our
datasets. (We cannot directly compare footprint sizes due
to variation in duration and sampling.) As one would ex-
pect, they are a heavy-tailed, with some originators trigger-
ing queries from 10k queriers. We focus the remainder of
our analysis of the originators with the largest footprints,
typically the top-10000 (about 0.5% of each dataset), or the
top-1000 or -100. Considering only large originators will
miss those that are intentionally trying to be stealthy, but
many scanners make no such attempt [17], and we expect
commercial large services to also be open.

The largest footprints here are larger than those we ob-
serve in controlled trials at M-Root (Figure 3). Those scans
were quite short (a few to a dozen hours), while here we
aggregate data over one or two days. In addition, our trials
used random targets, most of which are unoccupied (only
6–8% respond, as seen before [25]); many real-world scans
are targeted, resulting in higher responses rates and thus
greater backscatter.
5.2 Observability and Size of Application Classes

We next classify the top originators. Our goal is to un-
derstand what activity is taking place and approximately
how aggressive they are. Our key observations are: there
are thousands of originators causing network-wide activity,

10-810-710-610-510-410-310-210-1100100101102103104105cumulative distributionfootprint size (number of unique queriers per originator IP)JP-ditl (d=50h)B-post-ditl (d=36h)M-ditl (d=50h)M-sampled (d=1 week)205data
JP-ditl
B-post-ditl
M-ditl
M-sampled

ad-track
210
72
76
1329

cdn cloud crawl
-
361
557
885

-
168
135
2035

49
1782
2692
17,708

dns mail ntp
237
414
76
8
67
258
-
1202

1412
3137
2750
14,752

p2p push
-
2235
318
-
-
119
3652
-

scan
355
1228
983
47,201

spam update
6
5083
-
2849
2353
-
-
34,110

Table 6: Number of originators in each class for all datasets. (Classiﬁer: RF.)

(a) Top 100

(b) Top 1000

(c) Top 10,000

Figure 6: Fraction of originator classes of top-N originators. (Dataset: JP-ditl, B-post-ditl, M-ditl; classiﬁer: RF.)

Figure 6a), while others favor smaller footprints and so are
more common in Figure 6c.

The ad-tracker class is most common in the prominent
in top-most originators (a larger red ad-tracker fraction in
Figure 6a compared to to Figure 6c). There are relatively a
few originators (we see 5 companies as 22 unique originat-
ing addresses for top-100/JP-ditl). Unlike spam, they need
not hide, and are likely prominent because tracking needs
little traﬃc (a few originators can support a network-wide
service), and because they use DNS records with short cache
lifetimes (small TTLs). Cloud follows this pattern as well;
1 company across 21 distinct originating IPs for top-100 in
M-ditl.

The crawler class shows the opposite behavior: most crawlers

appear only in the top-10000, with few in top-1000 (554
vs. 3). This shift is consistent with web crawlers being data
intensive, operating across many distributed IP addresses in
parallel.

We also see that the physical location of the authority in-
ﬂuences what they see. We earlier observed how diﬀerences
in cdn for M-Root and B-Root are explained by their physi-
cal location to CDNs in China. B-Root’s U.S.-only location
may place it closer to more services in cloud (see Figure 6a)
compared to M-Root’s locations mainly in Asia and Europe.
New and old observations: A new observation in our
data is potential teams of scanners. We have manually iden-
tiﬁed several /24 address blocks where many addresses are
engaged in scanning, suggesting possible parallelized scan-
ning. Without direct scan traﬃc, we cannot conﬁrm coordi-
nation, but backscatter suggests networks for closer exami-
nation. To understand with scope of potential collaborative
teams, we start with a a very simple model where a team is
multiple originators in the same /24 IP address block. In M-
sampled we see 5606 unique scan originators (by IP address),
across 2227 unique originating /24 address blocks. Of these,
167 blocks have 4 or more originators, suggesting a potential
team of collaborators. While 128 of these blocks have multi-
ple application classes, suggesting against collaboration (or

possibly mis-classiﬁcation), we see 39 blocks with 4 or more
originators all with the same application class. Such blocks
warrant closer examination.

We also conﬁrmed prior observations that clients linger
on retired services. Originators we ﬁnd include four retired
root DNS servers (B, D, J, L), two prior cloud-based mail
servers, and one prior NTP server. These cases show our
methods can be used to systematically identify overly-sticky,
outdated clients across many services, automating prior re-
ports of clients that stick to retired servers in DNS [29] and
NTP [39].

Classiﬁcation on originator actions: An important
beneﬁt of our approach is that we classify on indirect ac-
tions caused by the originator, with no direct information
from the originator. In fact, about a quarter of the origina-
tors in JP-ditl and half of those in the root datasets have
no reverse domain names, but originator omissions have no
aﬀect on our approach because we do not observe any traf-
ﬁc or reverse names of originators. This separation makes
it more diﬃcult for adversarial originators to conceal their
activities.

5.3 Trends in Network-Wide Activity

We next look for long-term trends in our data. We believe
this is the ﬁrst longitudinal study of network-wide activities
such as scanning (prior work focused on speciﬁc events [17]).
Our goal is to understand the ebb and ﬂow of network-wide
activity, so rather than examine the N largest originators,
we count all originators with footprints of at least 20 queriers
(see also Figure 5). While we see no growth in network-wide
events, we see peaks that respond to security events and a
core of slow-and-steady scanners.

Peaks in numbers of originators: The top all

line
in Figure 7 shows the absolute number of originators over
time, each class (the lower, colored lines) and total (the top,
black line). There are fairly large week-by-week changes,
showing churn in the number of active originator activities,

 0 0.2 0.4 0.6 0.8 1JP-ditlB-post-ditlM-ditlspamscanadspamscancloudcdnadspamscancloudcdnadJP-ditlB-post-ditlM-ditlspamscanp2pmaildnsadspamscanpushcloudcdnadspamscanpushmaildnscloudcdnadJP-ditlB-post-ditlM-ditlspamscanp2pmaildnsspamscanpushmailcloudcdnspamscanmaildnscdnad-trackercdnclouddnsmailp2ppushscanspamother206Figure 7: Number of originators over time. (Dataset: M-
sampled; classiﬁer: RF.)

Figure 9: Three example originators with application class
scan. (Dataset: M-sampled with darknet.)

Figure 8: Box plot of originator footprint (queriers per scan-
ner) over time; whiskers: 10%ile/90%ile.
(Dataset: M-
sampled.)

and peaks that can be explained by reactions to network
security events.

To understand how network activity results from real-
world events we next look the scanner application class.
Our observation period includes public announcement of the
Heartbleed vulnerability on 2014-04-07 [38], and we know
that there were multiple research [1, 18], commercial, and
presumably government scanning activities triggered by that
announcement. The green scanner line in Figure 7 shows
more than a 25% increase in scanning by mid-April, from
1400 originator IPs per week to 1800 at its peak. While
this change is noticeable, it is smaller than we might expect.
Instead, it shows that reaction to Heartbleed is small com-
pared to the large amount of scanning that happens at all
times—the 1200–1400 scanners we saw in March, and the
1000–1200 scanners that are present from June to October.
Churn: To understand long-term scanning, Figure 8 shows
the distribution of footprint sizes over time for class scan.
While the median and quartiles are both stable over these
36 weeks, but the 90th percentile varies considerably. This
variation suggests a few very large scanners that come and
go, while a core of slower scanners are always present.

We illustrate this observation with three diﬀerent scan-
ners that appear in both M-sampled and our darknet data

Figure 10: Five example blocks originating scanning activ-
ity. (Dataset: M-sampled.)

(Figure 9). Two are long-lived (the top “tcp22” line scanning
ssh, and the middle line scanning multiple ports), while the
tcp80 scanner occurs in April and May. Furthermore, two
tcp443 scans only appear in one week in April (shown as
dark squares), suggesting they are Heartbleed-related. We
also see that tcp22 has a bigger footprint than the others,
and it looks a part of a big campaign whose 140 IP addresses
belong to the same /24 block. Using our darknets, we con-
ﬁrm 164 scanners for TCP ports 22, 80, or 443, and while
there is no “typical” scanner, these variations are common.
Our approach also identiﬁes networks supporting scan-
ners. For each /24 block, we count the number of IP ad-
dresses in class scan over time; Figure 10 shows ﬁve of these
blocks. The top dotted line is a block with large scanning
peaks corresponding with Heartbleed and Shellshock, end-
ing in September. The solid line shows a block that scans
continuously, while the three dotted lines are blocks that
start scanning during our observation.

To understand if who scans changes over time, Figure 11
measures week-by-week change in scanner IP addresses. The
bar above the origin shows the number of scanners each
week, showing both new originators (top, dark) and con-

 0 500 1000 1500 2000 2500 3000 3500 4000 45000304050607080910totalscanspammailNumber of originator IPsMonth (2014)ad-trackercdnclouddnsmailp2ppushscanspamother 0 20 40 60 80 100 120 1400304050607080910Number of unique queriersMonth (2014) 10 100 10000304050607080910tcp22tcp80multitcp443Number of unique queriers (logscale)Month (2014) 20 40 60 80 100 120 1400304050607080910scanning from /24 block (# of IP addrs)Month (2014)207Non-DNS Passive sensors: Darknets (or network tele-
scopes) are a commonly used passive technique to character-
ize large-scale network activity [37, 34, 54, 13, 14, 17]. By
monitoring a large, unoccupied blocks of addresses, dark-
nets see active probes from viruses and scanners, queries
from misconﬁguration, and backscatter from spoofed traﬃc;
traﬃc that can predict global malware, and its absence, net-
work outages. Our analysis of DNS backscatter shares the
goal of understanding network-wide activity from a simple,
passive observer, but we observe at DNS authorities rather
than large ranges of addresses. Like Durumeric et al. [17], we
seek to enumerate scanners, but our use of DNS backscat-
ter will see targeted scans that miss their darknet, and our
study considers eight months of activity, not just one.

Some security services use middleboxes with deep-packet
inspection to passively monitor large ISPs [3]. They ob-
serve all traﬃc from multiple points, while we monitor DNS
backscatter from a single provider only.

Staniford monitored network traﬃc for scanners [47], and
Gates emphasized rapid detection with scanner modeling [23].
Rather than protecting a single network, we look for network-
wide activity with a simple observer.

Honeypots (for example, [41]) are a form of application-
level darknet. By interacting with originators they see at-
tacks darknets miss, but they miss attacks that probe spe-
ciﬁc targets (such as Alexa top sites).
Interactivity also
makes them fewer because of deployment expense. DNS
backscatter uses information from existing servers.

Unconstrained endpoint proﬁling [48] uses search engines
to gather information on addresses that leak into the pub-
lic web, possibly complementing network ﬂow data. We
both seek to understand network-wide activity, but we use
diﬀerent data sources and methods. They use largely un-
structured information from the web, while we infer fea-
tures from semi-structured domain names and also traﬃc
patterns. Their work depends on the speed of search engine
indexing, while our work can provide rapid feedback given
data from a DNS authority.

General DNS traﬃc analysis and privacy: Finally,
a wide body of work has explored DNS traﬃc in general
(examples include [15, 52, 11, 22]). Their work seeks to
understand DNS, while we instead study what reverse DNS
tells us about network-wide activities.

Work in DNS privacy focuses on client-to-recursive re-
solvers for end-users (examples include [36, 57], and pro-
posals in the IETF DPRIVE working group). Our use of
reverse queries from automated systems triggered by origina-
tors should see almost no human-triggered, end-user queries
(§ 2). Use of query minimization [5] at the queriers will
constrain the signal to only the local authority (that imme-
diately serving the originator’s reverse address).

7. CONCLUSION

We identiﬁed DNS backscatter as a new source of infor-
mation about benign and malicious network-wide activity,
including originators of mailings list traﬃc, CDN infrastruc-
ture, spammers and scanners. Their activity triggers reverse
DNS queries by or near their targets, and we show that clas-
siﬁcation of these queriers allows us to identify classes of
activity with reasonable precision. We use our approach to
identify trends in scanning across nine months of data from
one data source, and we characterize several kinds of activ-

Figure 11: Week-by-week churn for originators of class scan.
(Dataset: M-sampled.)

tinuing originators (middle, light) The red bar below the
origin shows scanners that were lost from the prior week.
While there are always scanners coming and going (about
20% turnover per week), this data conﬁrms that there is a
stable core of scanners that are consistently probing, week-
after-week.

6. RELATED WORK

We next review prior work in both DNS- and non-DNS-
based sensors and analysis. Overall, our contribution is to
show that reverse DNS queries can identify network-wide
behavior. Prior work instead considers forward DNS traﬃc
and typically applies it to speciﬁc problems, or uses non-
DNS sensors such as darknets and search engines.

DNS-speciﬁc sensors: Several groups use forward DNS
queries to identify spam [56, 27], fast-ﬂux [56, 26], auto-
matically generated domain names [55], and cache poison-
ing [2]. Like our work, several of these approaches use ma-
chine learning to classify activity. However, this prior work
focuses on forward DNS queries, while we consider reverse
queries. Moreover, many use algorithms optimized to de-
tect speciﬁc malicious activities, while we detect a range of
network-wide behavior.

Recent work has used DNS to infer the structure of CDN
networks [4] or internal to DNS resolvers [44]. They infer
speciﬁc services from DNS traﬃc, we search for network-
wide events from reverse queries.

An earlier work uses targeted scan and DNS backscat-
ter for detecting Tor exit routers peeking POP3 authenti-
cation information [33], an earlier use of DNS backscatter
de-anonymization; we generalize this use to detect scanners.
Plonka and Barford use machine-learning-based cluster-
ing and visualization to identify undesirable activity from
local DNS traﬃc [40]. They use DNS traﬃc from an or-
ganization’s recursive resolver to infer activity about that
organization. Overall, our approach provides larger cover-
age, both by using data from authoritative DNS servers that
aggregate queries from many organizations, unlike their sin-
gle organization, and by examining trends in ten months of
data, unlike their week-long analysis.

Antispam software has long used reverse DNS lookups to
directly classify sources of mail. We use the domain names
of queriers to indirectly classify originators.

-500 0 500 1000 1500Number of originator IPsMonth (2014)0304050607080910newcontinuingdeparting208ity for two days over three data sources. Our work provides
a new approach to evaluate classes of network-wide activity.
Acknowledgments: We thank Yuri Pradkin for B-Root data col-
lection, Akira Kato for M-Root data, and Yoshiro Yoneya and Takeshi
Mitamura for JP data. We thank Xun Fan for input about Google
and Akamai sites in Japan. We thank Terry Benzel, Kenjiro Cho,
Ethan Katz-Bassett, Abdul Qadeer, and John Wroclawski comments
on this paper.

Kensuke Fukuda’s work in this paper is partially funded by Young
Researcher Overseas Visit Program by Sokendai, JSPS KAKENHI
Grant Number 15H02699, and the Strategic International Collabora-
tive R&D Promotion Project of the Ministry of Internal Aﬀairs and
Communication in Japan (MIC) and by the European Union Seventh
Framework Programme (FP7/2007- 2013) under grant agreement No.
608533 (NECOMA). The opinions expressed in this paper are those
of the authors and do not necessarily reﬂect the views of the MIC or
of the European Commission.

John Heidemann’s work in this paper is partially sponsored by the
Department of Homeland Security (DHS) Science and Technology Di-
rectorate, HSARPA, Cyber Security Division, via SPAWAR Systems
Center Paciﬁc under Contract No. N66001-13-C-3001, and via BAA
11-01-RIKA and Air Force Research Laboratory, Information Direc-
torate under agreement number FA8750-12-2-0344. The U.S. Gov-
ernment is authorized to make reprints for Governmental purposes
notwithstanding any copyright.The views contained herein are those
of the authors and do not necessarily represent those of DHS or the
U.S. Government.

8. REFERENCES
[1] Mustafa Al-Bassam. Top Alexa 10,000 Heartbleed scan.
https://github.com/musalbas/heartbleed-masstest/
blob/94cd9b6426311f0d20539e696496ed3d7bdd2a94/
top1000.txt, April 14 2014.

[2] Manos Antonakakis, David Dagon, Xiapu Luo, Roberto

Perdisci, and Wenke Lee. A centralized monitoring
infrastructure for improving DNS security. In Proc. of the
13th International Symposium on Recent Advances in
Intrusion Detection, pages 18–37, Ottawa, Ontario,
Canada, September 2010. Springer.

[3] Arbor Networks. Worldwide infrastructure security report.

Technical Report Volume IX, Arbor Networks, January
2014.

[4] Ignacio Bermudez, Marco Mellia, Maurizio M. Munafo,

Ram Keralapura, and Antonio Nucci. DNS to the rescue:
Discerning content and services in a tangled web. In
Proc. of the ACM Internet Measurement Conference, pages
413–426, Boston, MA, November 2012.

[5] S. Bortzmeyer. DNS query name minimisation to improve

privacy. Work in progress (Internet draft
draft-bortzmeyer-dns-qname-minimisation-02), May 2014.

[6] Carna Botnet. Internet census 2012: Port scanning /0 using

insecure embedded devices. web page
http://census2012.sourceforge.net/paper.html, March
2013.

[7] Leo Breiman. Random forests. Machine Learning, 45:5–32,

October 2001.

attacks. In Proc. of the ACM Internet Measurement
Conference, pages 435–448, Vancouver, BC, Canada,
November 2014. ACM.

[13] Jakub Czyz, Kyle Lady, Sam G. Miller, Michael Bailey,

Michael Kallitsis, and Manish Karir. Understanding IPv6
Internet background radiation. In IMC’13, pages 105–118,
Barcelona, Spain, 2013.

[14] Alberto Dainotti, Claudio Squarcell, Emile Aben,

Kimberly C. Claﬀy, Marco Chiesa, Michele Russo, and
Antonio Pescape. Analysis of country-wide internet outages
caused by censorship. In Proc. of the ACM Internet
Measurement Conference, pages 1–18, Berlin, Germany,
November 2011.

[15] Peter B. Danzig, Katia Obraczka, and Anant Kumar. An
analysis of wide-area name server traﬃc: A study of the
Domain Name System. In Proc. of the ACM SIGCOMM
Conference, pages 281–292, January 1992.

[16] DNS-OARC. Day in the life of the internet (DITL) 2014.
https://www.dns-oarc.net/oarc/data/ditl, April 2014.
[17] Zakir Durumeric, Michael Bailey, and J. Alex Halderman.

An Internet-wide view of Internet-wide scanning. In
Proc. of the 23rd USENIX Security Symposium, pages
65–78, San Diego, CA, August 2014. USENIX.

[18] Zakir Durumeric, Frank Li, James Kasten, Johanna

Amann, Jethro Beekman, Mathias Payer, Nicolas Weaver,
David Adrian, Vern Paxson, Michael Bailey, and J. Alex
Halderman. The matter of Heartbleed. In Proc. of the
ACM Internet Measurement Conference, pages 475–488,
Vancouver, BC, Canada, November 2014. ACM.

[19] Zakir Durumeric, Eric Wustrow, and J. Alex Halderman.

ZMap: Fast Internet-wide scanning and its security
applications. In Proc. of the USENIX Security Symposium,
pages 605–620, Washington, DC, USA, August 2013.
USENIX.

[20] Robert Edmonds. ISC passive DNS architecture. Technical

report, Internet Systems Consortium, Inc., March 2012.

[21] Xun Fan, Ethan Katz-Bassett, and John Heidemann.

Assessing aﬃnity between users and CDN sites. In Proc. of
the 7th Workshop on Traﬃc Monitoring and Analysis
(TMA), pages 95–110, Barcelona, Spain, April 2015.
Springer.

[22] Hongyu Gao, Vinod Yegneswaran, Yan Chen, Phillip

Porras, Shalini Ghosh, and Jian Jiang Haixing Duan. An
empirical reexamination of global DNS behavior. In
Proc. of the ACM SIGCOMM Conference, pages 267–278,
Hong Kong, China, 2013.

[23] Carrie Gates. Coordinated scan detection. In Proc. of the

ISOC Network and Distributed System Security
Symposium, San Diego, CA, February 2009. The Internet
Society.

[24] Kenneth Geers, Darien Kindlund, Ned Moran, and Rob

Rachwald. World War C: Understanding nation-state
motives behind today’s advanced cyber attacks. Technical
report, FireEye, September 2014.

[8] Leo Breiman, Jerome Friedman, Richard Olshen, and

[25] John Heidemann, Yuri Pradkin, Ramesh Govindan,

Charles Stone. Classiﬁcation and Regression Trees.
Chapman and Hall, 1984.

[9] Nevil Brownlee. One-way traﬃc monitoring with iatmon. In

Proc. of the Passive and Active Measurement Workshop,
pages 179–188, Vienna, Austria, March 2012.

[10] Matt Calder, Xun Fan, Zi Hu, Ethan Katz-Bassett, John

Heidemann, and Ramesh Govindan. Mapping the
expansion of Google’s serving infrastructure. In Proc. of the
ACM Internet Measurement Conference, pages 313–326,
Barcelona, Spain, October 2013. ACM.

[11] Sebastian Castro, Duane Wessles, Marina Fomenkov, and

kc Claﬀy. A day at the root of the Internet. ACM
SIGCOMM Computer Communication Review,
38(5):41–46, October 2008.

[12] Jakub Czyz, Michael Kallitsis, Manaf Gharaibeh, Christos
Papadopoulos, Michael Bailey, and Manish Karir. Taming
the 800 pound gorilla: The rise and decline of NTP DDoS

Christos Papadopoulos, Genevieve Bartlett, and Joseph
Bannister. Census and survey of the visible Internet. In
Proc. of the ACM Internet Measurement Conference, pages
169–182, Vouliagmeni, Greece, October 2008. ACM.

[26] Thorsten Holz, Christian Gorecki, Konrad Rieck, and Felix

Freiling. Measuring and detecting fast-ﬂux service
networks. In Proc. of the ISOC Network and Distributed
System Security Symposium, San Diego, CA, USA,
February 2008. The Internet Society.

[27] Keisuke Ishibashi, Tsuyoshi Toyono, Katsuyasu Toyama,

Masahiro Ishino, Haruhiko Ohshima, and Ichiro Mizukoshi.
Detecting mass-mailing worm infected hosts by mining DNS
traﬃc data. In Proc. of the ACM SIGCOMM MineNet
Workshop, pages 159–164, Philadelphia, PA, August 2005.

[28] Julian Kirsch, Christian Grothoﬀ, Monika Ermert, Jacob

Appelbaum, Laura Poitras, and Henrik Moltke.

209[48] Ionut Trestian, Supranamaya Ranjan, Aleksandar

Kuzmanovi, and Antonio Nucci. Unconstrained endpoint
proﬁling (Googling the Internet). In Proc. of the ACM
SIGCOMM Conference, pages 279–290, Seattle, WA, Aug
2008.

[49] USC/LANDER project. Internet address census, datasets
internet_address_census it63w, it63c, it63j, it63g, it64w,
it64c, it64j, it64g. web page
http://www.isi.edu/ant/lander, January (it63) and April
(it64) 2015.

[50] Paul Vixie. Passive DNS collection and analysis, the

‘dnstap’ approach. Keynote talk at FloCon, January 2014.

[51] Florian Weimer. Passive DNS replication. In Proc. of the

17th Forum of Incident Response and Security Teams
(FIRST), Singapore, April 2005.

[52] Duane Wessels and Marina Fomenkov. Wow, that’s a lot of

packets. In Proc. of the Passive and Active Measurement
Workshop, La Jolla, CA, April 2003.

[53] Wikipedia. Gini coeﬃcient.

http://en.wikipedia.org/wiki/Gini_coefficient, 2015.

[54] Eric Wustrow, Manish Karir, Michael Bailey, Farnam

Jahanian, and Geoﬀ Houston. Internet background
radiation revisited. In Proc. of the 10th ACM Internet
Measurement Conference, pages 62–73, Melbourne,
Australia, November 2010. ACM.

[55] Sandeep Yadav, Ashwath Kumar, Krishna Reddy,
A.L. Narasimha Reddy, and Supranamaya Ranjan.
Detecting algorithmically generated malicious domain
names. In Proc. of the ACM Internet Measurement
Conference, pages 48–61, Melbourne, Australia, November
2010.

[56] Bojan Zdrnja, Nevil Brownlee, and Duane Wessels. Passive

monitoring of DNS anomalies. In Proc. of the 4th
International Conference on Detection of Intrusions &
Malware, and Vulnerability Assessment (DIMVA), pages
129–139, Lucerne, Switzerland, 2007.

[57] Liang Zhu, Zi Hu, John Heidemann, Duane Wessels,

Allison Mankin, and Nikita Somaiya. Connection-oriented
DNS to improve privacy and security. In Proc. of the 36th
IEEE Symposium on Security and Privacy, pages 171–186,
San Jose, Californa, USA, May 2015. IEEE.

NSA/GCHQ: The HACIENDA program for internet
colonization. C’T Magazine, Aug.15 2014.

[29] Matthew Lentz, Dave Levin, Jason Castonguay, Neil

Spring, and Bobby Bhattacharjee. D-mystifying the D-root
address change. In Proc. of the ACM Internet
Measurement Conference, pages 57–62, Barcelona, Spain,
October 2013. ACM.

[30] Kirill Levchenko, Andreas Pitsillidis, Neha Chachra,

Brandon Enright, M´ark F´elegyh´azi, Chris Grier, Tristan
Halvorson, Chris Kanich, Christian Kreibich, He Liu,
Damon McCoy, Nicholas Weaver, Vern Paxson, Geoﬀrey M.
Voelker, and Stefan Savage. Click trajectories: End-to-end
analysis of the spam value chain. In Proc. of the IEEE
Symposium on Security and Privacy, pages 431–446,
Oakland, CA, USA, May 2011. IEEE.

[31] Zhichun Li, Anup Goyal, Yan Chen, and Aleksandar
Kuzmanovic. Measurement and diagnosis of address
misconﬁgured P2P traﬃc. In INFOCOM’10, pages 1–9,
San Diego, CA, March 2010.

[32] MaxMind LLC. GeoIP. http://www.maxmind.com/geoip.
[33] Damon McCoy, Kevin Bauer, Dirk Grunwald, Tadayoshi
Kohno, and Douglas Sicker. Shining light in dark places:
Understanding the tor network. In Proc. of the IEEE
International Workshop on Performance Evaluation of
Tracking and Surveillance (PETS), pages 63–76, Leuven,
Belgium, July 2008.

[34] Jon Oberheide, Manish Karir, Z. Morley Mao, and Farnam

Jahanian. Characterizing dark DNS behavior. In Proc. of
the 4th International Conference on Detection of Intrusions
& Malware, and Vulnerability Assessment (DIMVA), pages
140–156, Lucerne, Switzerland, July 2007. Springer.

[35] Robert O’Harrow, Jr. Cyber search engine Shodan exposes

industrial control systems to new risks. The Washington
Post, June 3 2012.

[36] OpenDNS. DNSCrypt: Introducing DNSCrypt. web page
http://www.opendns.com/about/innovations/dnscrypt/,
January 2014.

[37] Ruoming Pang, Vinod Yegneswaran, Paul Barford, Vern
Paxson, and Larry Peterson. Characteristics of Internet
background radiation. In Proc. of the ACM Internet
Measurement Conference, pages 27–40, Sicily, Italy, 2004.
[38] Nicole Perlroth. Thought safe, websites ﬁnd the door ajar.

New York Times, page A1, Apr. 9 2014.

[39] Dave Plonka. Flawed routers ﬂood university of wisconsin

internet time server.
http://pages.cs.wisc.edu/~plonka/netgear-sntp, 2003.
[40] David Plonka and Paul Barford. Context-aware clustering

of DNS query traﬃc. In Proc. of the ACM Internet
Measurement Conference, pages 217–229, Vouliagmeni,
Greece, October 2008.

[41] Niels Provos. A virutal honeypot framework. In Usenix
Security Symposium 2004, pages 1–14, San Diego, CA,
August 2004.

[42] Lin Quan, John Heidemann, and Yuri Pradkin. Trinocular:

understanding Internet reliability through adaptive
probing. In Proc. of the ACM SIGCOMM Conference,
pages 255–266, Hong Kong, China, August 2013.

[43] Bernhard Scholkopf and Alexander J. Smola. Learning with

Kernels: Support Vector Machines, Regularization,
Optimization, and Beyond. MIT Press, 2001.

[44] Kyle Schomp, Tom Callahan, Michael Rabinovich, and

Mark Allman. On measuring the client-side DNS
infrastructure. In Proc. of the ACM Internet Measurement
Conference, pages 77–90, Barcelona, Spain, October 2013.

[45] Farsight Security. SIE (Security Information Exchange).

https://www.farsightsecurity.com/Services/SIE/, 2013.
[46] Shadow server foundation. http://www.shadowserver.org/.
[47] Stuart Staniford, James A. Hoagland, and Joseph M.
McAlerney. Practical automated detection of stealthy
portscans. Journal of Comptuer Security, 10(1):105–136,
2002.

210