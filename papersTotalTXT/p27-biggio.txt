Poisoning Behavioral Malware Clustering

Battista Biggio

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy

battista.biggio@diee.unica.it

Konrad Rieck

University of Göttingen
Goldschmidtstraße 7

37077, Göttingen, Germany

konrad.rieck@uni-

goettingen.de

Davide Ariu

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy

davide.ariu@diee.unica.it

Christian Wressnegger

University of Göttingen
Goldschmidtstraße 7

37077, Göttingen, Germany
christian.wressnegger@

cs.uni-goettingen.de

Igino Corona

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy

igino.corona@diee.unica.it

Giorgio Giacinto
Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy

giacinto@diee.unica.it

Fabio Roli

Università di Cagliari

Piazza d’Armi

09123, Cagliari, Italy
roli@diee.unica.it

ABSTRACT
Clustering algorithms have become a popular tool in com-
puter security to analyze the behavior of malware variants,
identify novel malware families, and generate signatures for
antivirus systems. However, the suitability of clustering
algorithms for security-sensitive settings has been recently
questioned by showing that they can be signiﬁcantly com-
promised if an attacker can exercise some control over the in-
put data. In this paper, we revisit this problem by focusing
on behavioral malware clustering approaches, and investi-
gate whether and to what extent an attacker may be able to
subvert these approaches through a careful injection of sam-
ples with poisoning behavior. To this end, we present a case
study on Malheur, an open-source tool for behavioral mal-
ware clustering. Our experiments not only demonstrate that
this tool is vulnerable to poisoning attacks, but also that it
can be signiﬁcantly compromised even if the attacker can
only inject a very small percentage of attacks into the input
data. As a remedy, we discuss possible countermeasures and
highlight the need for more secure clustering algorithms.

Categories and Subject Descriptors
D.4.6 [Security and Protection]: Invasive software (e.g.,
viruses, worms, Trojan horses); G.3 [Probability and Statis-
tics]: Statistical computing; I.5.1 [Models]: Statistical;

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’14, November 7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2953-1/14/11 ...$15.00.
http://dx.doi.org/10.1145/2666652.2666666.

I.5.2 [Design Methodology]: Clustering design and eval-
uation; I.5.3 [Clustering]: Algorithms

General Terms
Security, Clustering.

Keywords
Adversarial Machine Learning; Unsupervised Learning; Clus-
tering; Security Evaluation; Computer Security; Malware
Detection

1.

INTRODUCTION

Automated techniques for behavioral clustering of mal-
ware have been found to be eﬀective for the development of
analysis, detection and mitigation strategies against a broad
spectrum of malicious software. Such techniques can signif-
icantly ease the identiﬁcation of polymorphic instances of
well-known malware as well as novel attack types and infec-
tion strategies, reducing by orders of magnitude the burden
of the analysis task [e.g., 20, 24, 27, 28].

Behavioral clustering is motivated by a key assumption:
albeit malware writers can generate a large number of poly-
morphic variants of the same malware, e.g., using executable
packing and other code obfuscation techniques [12, 15], these
polymorphic variants will eventually perform similar activ-
ities when executed. To expose these behavioral similari-
ties, malware binaries are usually executed in a monitored
sandbox environment, in order to identify malware families
characterized by similar host-level events [e.g., 1, 3, 20, 27]
or network traﬃc patterns [e.g., 13, 14, 24, 25].

However, regardless the behavioral features being used, all
these proposals suﬀer from the same vulnerability: cluster-
ing algorithms have not been originally devised to deal with
data from an adversary. As outlined in recent work [4, 9],
this may allow an attacker to devise carefully-crafted attacks

27that can signiﬁcantly compromise the clustering process it-
self, and invalidate subsequent analyses.

In this work, we also show that the eﬀectiveness of cluster-
ing algorithms — in particular, single-linkage clustering —
can be dramatically reduced by a skilled adversary through a
proper, deliberate manipulation of malware samples, in the
context of a more realistic application scenario that those
considered in [4, 9]. To this end, we ﬁrst review the at-
tacker’s model proposed in [4, 9], as it can also be exploited
as a general threat model for behavioral malware cluster-
ing, and then investigate a worst-case attack against Mal-
heur [28], an open-source malware clustering tool. We emu-
late an attacker who adds specially-crafted poisoning actions
to the original behavior of malware samples, thus leaving in-
tact their original malicious goals. Our experimental results
clearly show that even a small fraction of 3% of poison-
ing samples may completely subvert the clustering process,
leading to poor clustering results. Thus, our case study high-
lights the need for robust malware clustering techniques, ca-
pable of coping with malicious noise. As a consequence, a
safe application of clustering algorithms for malware analy-
sis remains an open research issue. Throughout the paper
we sketch some promising ways of research towards this goal.

Contributions. In summary, the main contribution of this
paper is to extend and adapt the poisoning attacks proposed
in [4, 9] against the single-linkage hierarchical clustering al-
gorithm to target Malheur [28], an open-source tool for be-
havioral malware clustering. In this case, the main diﬃculty
with respect to previous work relies in constructing real mal-
ware samples that correspond to the desired, optimal feature
vectors found by the optimal attack strategy, while account-
ing for application-speciﬁc constraints on the manipulation
of the feature values of each sample. This is a well-known
issue in the ﬁeld of adversarial machine learning, referred
to as the problem of inverting the feature mapping [7, 17].
To assess the eﬀectiveness of poisoning attacks against be-
havioral malware clustering, we ﬁnally report an extensive
set of experiments that highlight the vulnerability of such
approaches to well-crafted attacks, as well as the need for
identifying suitable countermeasures, for which we identify
some interesting ways of research.

Organization. The remainder of this paper is structured
as follows. In Sect. 2, we give an overview of recent work
on behavioral malware clustering. The previously-proposed
framework for the security evaluation of clustering algo-
rithms [4, 9] is discussed in Sect. 3. In Sect. 4, we review
the derivation of (worst-case) poisoning attacks, in which
the attacker has perfect knowledge of the targeted system.
In Sect. 5, we describe Malheur, the malware clustering tool
exploited as a case study to evaluate our poisoning attacks.
The latter are deﬁned as variants of the previously-proposed
poisoning attacks, to deal with the speciﬁc feature represen-
tation exploited by Malheur, in Sect. 6, where we also report
the results of our experimental evaluation. Conclusions are
discussed in Sect. 7, along with possible future research di-
rections.

2. MALWARE CLUSTERING

The urgent need for automated analysis of malware natu-
rally comes with the ever-growing number of malicious codes
on the Internet.
In recent years, machine learning tech-
niques have received attention in this area, as they enable

improving the automation of malware analysis. One promi-
nent representative are clustering algorithms. These algo-
rithms enable grouping similar malware automatically and
can thereby reduce the manual eﬀorts required for develop-
ing mitigation and detection techniques. Several approaches
for such a clustering have been devised in the last years, most
notably, (a) clustering of network traﬃc, and (b) clustering
of program behavior.
Clustering of network traﬃc. Network communication
is a key component of malware and thus several malware
families can be solely characterized by their network traﬃc.
For example, Gu et al. correlate spatial-temporal relations
in botnet communication using clustering [14]. To this end,
the authors make use of hierarchical clustering on the basis
of q-grams over a so-called “activity log” which describes a
botnet’s network communication in terms of diﬀerent types
of responses. This approach is then extended to a more gen-
eral concept of C&C communication [13], where the authors
attempt to be agnostic to the protocol used as well as the
concrete hierarchy of the botnet.

In a similar line of research, Perdisci et al. [25] focus on
HTTP-based malware with the objective to automatically
generate network signatures for malware. In particular, they
use single-linkage clustering over three stages, mainly to
reduce computational complexity: ﬁrst, a “coarse-grained”
clustering is performed; each of the corresponding clusters
is then subdivided into a more “ﬁne-grained” set of clusters;
and, eventually, similar clusters are merged together to avoid
redundant signature generation. An extension by the same
authors [24] focuses more on the scalability of the proposed
approach, in terms of the number of samples that the sys-
tem is able to process in a given amount of time (i.e., the
so-called throughput). The authors utilize an approximate
clustering algorithm for the ﬁrst stage of their approach.
This not only speeds up the initial stage but also decreases
the need of a merging phase, thus yielding a signiﬁcant in-
crease of the overall throughput of the system.
Clustering of program behavior. A second strain of
research has considered program behavior of malware for
identifying related samples. Despite polymorphism and ob-
fuscation, variants of the same malware family often show
similar program behavior. Bailey et al. [1] have been the
ﬁrst to apply clustering algorithms to this information. In
particular, they obtain a single-linkage clustering by com-
puting pairwise distances between sequences of host-level
events. This approach, however, has a quadratic runtime
complexity and therefore quickly reaches its limits in terms
of the possible throughput.

Bayer et al. [3] counter this shortcoming with an approxi-
mate clustering using locality sensitive hashing (LSH). This
makes it possible to scale the analysis to several thousand
malware samples. The behavioral analysis is powered by
the malware analysis system Anubis [18]. Closely related
to this approach is the tool Malheur [28], which we use in
our case study to demonstrate the eﬀectiveness of our at-
tacks. Malheur makes use of program behavior monitored
by CWSandbox in MIST Format [31, 35] and is described
in more detail in Sect. 5.

More recently, several extensions have been proposed for
improving behavioral clustering of malware in practice. For
example, Jang et al. [20] apply feature hashing for clustering
large sets of malware binaries, Perdisci & U [26] propose an
automatic procedure for calibrating clustering algorithms,

28and Hu & Shi [16] combine behavioral clustering with static
code analysis.

Although each of the presented approaches provides ad-
vantages for keeping abreast of malware development, all ap-
proaches employ standard clustering algorithms which have
not been originally designed to explicitly cope with malicious
noise. Consequently, the attacks proposed in this paper can
be potentially adapted to several of these approaches with
minor modiﬁcations.

3. SECURITY EVALUATION OF
CLUSTERING ALGORITHMS

In this section we brieﬂy review the framework proposed
by Biggio et al. [4, 9] for the security evaluation of unsuper-
vised learning algorithms (including clustering) against ad-
versarial attacks. Similarly to previous work on the security
evaluation of supervised learning algorithms [2, 7, 17], this
framework relies on a threat model that consists of deﬁn-
ing the adversary’s goal, knowledge of the attacked system,
and capability of manipulating the input data, in order to
formalize an optimal attack strategy.

In the sequel, we describe this framework using the same
notation deﬁned in Biggio et al. [4, 9]. We refer to any clus-
tering algorithm as a function f that maps a given dataset
D = {xi}n
i=1 to a clustering result C = f (D), without spec-
ifying the structure of C at this stage, as it depends on the
given clustering algorithm.
3.1 Adversary’s Goal

The adversary’s goal can be deﬁned in terms of the de-
sired security violation, and of the so-called attack speci-
ﬁcity [2, 4, 7, 9, 17]. A security violation may compromise
the system integrity, its availability, or the privacy of its
users. Integrity violations, in general, aim to perform some
malicious activity without compromising the normal system
operation. In the unsupervised learning setting, they have
thus been deﬁned as attacks aimed at changing the cluster-
ing of a given set of samples, without signiﬁcantly altering
the clustering result on the rest of the data. Availability
violations aim to compromise system operation, causing a
Denial of Service (DoS). Therefore, in the unsupervised set-
ting, availability attacks have been deﬁned as attacks that
aim to subvert the clustering process by altering its result as
much as possible. By contrast, privacy violations are deﬁned
as attacks that may allow the attacker to gather information
about the system’s users by reverse-engineering the cluster-
ing process. The attack speciﬁcity can be targeted or indis-
criminate, depending on whether the attack aims to modify
the clustering output only on a speciﬁc subset of samples,
or indiscriminately on any sample.
3.2 Adversary’s Knowledge

In order to achieve her goal, the adversary may exploit in-
formation at diﬀerent abstraction levels about the targeted
system. We summarize them in the following. First, the
attacker may know the whole dataset D, a subset of it, or
more realistically, only a surrogate dataset S, that might
be obtained from the same source of D, e.g., publicly avail-
able malware blacklists. Second, the adversary might be
aware of, and reproduce, the extraction process of the whole
feature set, or a portion of it.
Indeed, when it comes to
attacking open-source tools such as Malheur, the adversary

clearly has full knowledge of the feature set. Finally, the ad-
versary might be aware of the targeted clustering algorithm,
as well as of its initialization parameters (if any). In the case
of Malheur, this translates into knowing the user-speciﬁed
conﬁguration of the tool.
Perfect knowledge. The worst-case scenario in which the
attacker has full knowledge of the targeted system is usually
referred to as perfect knowledge [2, 5–8, 10, 17, 22]. In our
case, this amounts to knowing the data, the feature space,
the clustering algorithm and its initialization (if any).
3.3 Adversary’s Capability

The adversary’s capability speciﬁes how and to what ex-
tent the adversary can manipulate the input data to alter the
clustering process. In several cases it is realistic to consider
that the attacker can add a maximum number of (poten-
tially manipulated) samples to the dataset D, without af-
fecting the rest of the data. For instance, anyone, including
a skilled adversary, can submit novel malware samples to
publicly-available malware-analysis services such as Virus-
Total [33] and Anubis [18], which can in turn be used as
sources to collect malware by registered users. If malware
is collected from them, and clustered afterwards, the adver-
sary may actually control a (small) percentage of the input
data given to the clustering algorithm.

An additional constraint may be given in terms of how
malware samples can be manipulated. In fact, to preserve
its malicious functionality, malware code may not be manip-
ulated in an unconstrained manner. Such a constraint can
be often encoded by a suitable distance measure between the
original, non-manipulated attack samples and the manipu-
lated ones, as in [2, 7, 17, 23]. However, this strictly depends
on the speciﬁc application and feature representation.
3.4 Attack Strategy

Based on the presented threat model, consisting of as-
sumptions on the adversary’s goal, knowledge and capabili-
ties, we can ﬁnally deﬁne the optimal strategy for attacking
a clustering algorithm as:

maximize Eθ∼µ[g(A(cid:48); θ)]

s.t. A(cid:48) ∈ Ω(A) .

(1)

In this formulation, as in [4, 9], the adversary’s knowledge
is characterized by a parameter vector θ, whose elements
embed information about the input data D, the clustering
algorithm f , and its parameters (as discussed in Sect. 3.2).
The uncertainty of the adversary about the elements of θ
is captured by a probability distribution µ deﬁned over the
set of all possible conﬁgurations θ. Moreover, the objective
function g(A(cid:48); θ) ∈ R measures the extent to which the ad-
versary’s goal is fulﬁlled by the set of attack samples A(cid:48) used
to taint the initial data D, given the knowledge θ. In the
above formulation, we consider the maximization of the ex-
pected value of this function with respect to θ sampled from
the distribution µ, denoted as Eθ∼µ[·]. Finally, the adver-
sary’s capability is encoded by the set Ω(A), which denotes
the possible manipulations that the attacker can make on a
given a set of attack samples A before adding them to the
original set D. The set A of initial attacks can be empty, e.g.,
if the attack samples can be generated from scratch without
preserving or exhibiting any malicious functionality.

It is ﬁnally worth remarking that the above optimization
problem is formulated in terms of the considered feature

29representation, as many other adversarial machine learning
problems [5–7, 9, 17]. In practice, after solving this prob-
lem, we are given a set of optimal feature vectors for which
we have to subsequently build a set of corresponding real
samples to practically execute the attack. This is clearly an
application-speciﬁc problem that may not be trivial to solve
depending on the given feature representation. However,
it can be mitigated by incorporating speciﬁc constraints on
the manipulation of the feature values of the attack samples,
while deﬁning the set Ω, as we will see in the next sections.

4. POISONING ATTACKS WITH

PERFECT KNOWLEDGE

Following the framework described in the previous sec-
tion, poisoning attacks are deﬁned as indiscriminate avail-
ability violations (i.e., DoS attacks) in which the attacker
aims to maximally alter the clustering result on any of the
input samples through the injection of well-crafted poison-
ing samples. In the case of malware clustering, this amounts
to adding carefully-designed malware samples to the input
data to avoid the correct clustering of malware exhibiting
similar behavior and, thus, the correct identiﬁcation of both
known and novel malware families.

As in previous work [4, 9], we are interested in analyzing
the worst possible performance degradation that the sys-
tem may incur under this attack. We therefore assume that
the attacker has perfect knowledge of the targeted system,
as described in Section 3.2. Accordingly, the expectation in
Eq. (1) vanishes and the objective simply becomes g(A(cid:48); θ0),
being θ0 the set of parameters representing perfect knowl-
edge of the system. Further, for this kind of attack, the
objective function g(A(cid:48); θ0) can be deﬁned as a distance
function between the clustering result C obtained from the
untainted data D and the clustering result C(cid:48) = fD(D(cid:48)) re-
stricted to the same data (through a projection operator
fD), but obtained from the tainted data D(cid:48) = D ∪ A(cid:48) (i.e.,
including the set A(cid:48) of attack samples). The objective can
be thus written as g(A(cid:48); θ0) = dc(C, fD(D∪A(cid:48))), where dc is
a suitable distance function between clusterings. Note that
poisoning samples are excluded from the computation of the
objective function since the attacker’s goal is to maximally
subvert the clustering output on the untainted input data,
and not on the poisoning samples (which may otherwise bias
the evaluation of the attack’s impact).
If the clustering algorithm f assigns each sample to a clus-
ter, the clustering result C can be represented as a matrix
Y ∈ {0, 1}n×k (k being the number of clusters found), where
each (i, j)th component equals 1 if the ith sample is assigned
to the jth cluster, and 0 otherwise. Within this setting, a
possible distance function between clusterings amounts to
counting how many pairs of samples have been clustered
together in one clustering and not in the other, or viceversa:

(cid:48)
dc(Y, Y

(cid:48)(cid:62)(cid:107)F ,
Y

) = (cid:107)YY

(cid:62) − Y

(cid:48)

(cid:62) ∈ {0, 1}n×n (and, similarly, of Y

(2)
where (cid:107) · (cid:107)F is the Frobenius norm, and each element of
the matrix YY
) repre-
sents whether the corresponding pair of samples has been
clustered together (1) or not (0).
As mentioned earlier, to poison the clustering process the
adversary can add a set A(cid:48) of attack samples to the input
data D. We bound the adversary’s capability here by limit-
ing the maximum number of injected poisoning samples to

(cid:48)(cid:62)
Y

(cid:48)

m, i.e. |A(cid:48)| ≤ m. Additional constraints on the set of attack
samples can be identiﬁed depending on the given feature
representation, to facilitate the fabrication of real samples
exhibiting the desired feature values. In general, we denote
the set of constraints to be fulﬁlled by the poisoning at-
tack as A(cid:48) ∈ Ωp. To give a concrete example, consider that
Malheur can be conﬁgured to extract binary feature vectors
that are subsequently normalized to have unitary (cid:96)2-norm.
In this case, the set of constrained attack samples can be
expressed as:
i}m
(cid:48)
i=1 : a

Ωp =
(3)
where d is the number of features, and || · ||2 denotes the
(cid:96)2-norm of a vector.

i||2}d for i = 1,··· , m
(cid:48)

i ∈ {0, 1/||a
(cid:48)

(cid:110){a

(cid:111)

,

In general, the optimal attack strategy for poisoning at-
tacks with perfect knowledge can be therefore derived from
Eq. (1) and written independently from the speciﬁc cluster-
ing algorithm as:

maximize dc(C, fD(D ∪ A(cid:48)))

s.t. A(cid:48) ∈ Ωp .

(4)

Unfortunately, this problem can not be solved analyti-
cally only if the clustering output is analytically predictable,
which is not usually the case. We have thus to resort to suit-
able heuristics depending on the considered clustering algo-
rithm to devise eﬀective attacks. In the next section we in-
vestigate heuristics to solve the above problem [see 4, 9] and
poison the single-linkage hierarchical clustering algorithm,
as we will exploit them in our case study against Malheur.
4.1 Poisoning single-linkage hierarchical

clustering

Before describing the heuristics for poisoning the single-
linkage clustering algorithm, it is worth pointing out that
this algorithm, as any other variant of hierarchical cluster-
ing, outputs a hierarchy of clusterings [19]. Such a hierarchy
is constructed by initially considering each data point as a
single cluster, and iteratively merging the closest clusters
together, until a single cluster containing all data points is
obtained. Clusters are merged according to a given distance
measure, also referred to as linkage criterion. In the single-
linkage variant, the distance between any two clusters (Ci,
Cj) is deﬁned as the minimum Euclidean distance between
all possible pairs of samples in Ci × Cj.

To obtain a given data partitioning into clusters, a suit-
able cutoﬀ distance has to be chosen. This determines the
maximum intra-cluster distance for each cluster, and, thus,
indirectly, the total number of clusters. We follow the ap-
proach of Biggio et al. [4, 9] and select the cutoﬀ distance
that achieves the minimum distance between the clustering
obtained in the absence of attack C and the one obtained in
the presence of poisoning, i.e., min dc(C, fD(D ∪ A(cid:48))). The
reason is that this is the worst-case cutoﬀ criterion for the
attack, which is thus expected to work potentially even bet-
ter under less pessimistic choices of the cutoﬀ distance.

Given a suitable criterion for selecting the cutoﬀ distance,
it is possible to model the clustering output as a binary
matrix Y ∈ {0, 1}n×k indicating the sample-to-cluster as-
signments, and thus use the distance measure dc deﬁned in
Eq. (2) as the objective function in Problem (4). This prob-
lem has then been solved by means of specialized search
heuristics speciﬁcally tailored to the considered clustering

30algorithm.
In particular, we have considered greedy opti-
mization approaches in which the attacker aims to maxi-
mize the objective function by adding one attack sample at
a time, i.e., |A(cid:48)| = m = 1. We have found that the objective
function is often maximized when the attack point is added
in between clusters that are suﬃciently close to each other.
The reason is that such an attack tends to decrease the dis-
tance between the two clusters, thus causing the algorithm
to potentially merge them into a single cluster.

Bridge-based attacks. Based on this observation, we have
thus devised a family of attacks that aim to iteratively bridge
the closest clusters. Let us assume that at each iteration
we are given a set of k clusters, and we have to select the
best attack point to be added to the current dataset. Each
bridge-based attack generates the same set of k−1 candidate
attack points, by considering the k− 1 links between pairs of
points that have been cut to separate the current clustering
from the top of the hierarchy, i.e., the k− 1 shortest connec-
tions between clusters. Each candidate attack point is then
computed as the midpoint between the points in each of the
k − 1 identiﬁed pairs, as conceptually represented in Fig. 1.
The diﬀerence among the bridge-based attacks relies only on
how the best attack point is selected at each iteration.

Bridge (Best). This strategy adds each candidate attack
point to the current dataset, one at a time, re-runs the
clustering algorithm on such data, and chooses the attack
point that maximally increases the objective function. This
is clearly a computationally-intensive procedure, especially
for large datasets.

Bridge (Hard). This strategy aims to improve eﬃciency
by avoiding us to re-run the clustering k times at each attack
iteration. The underlying idea is to approximate the cluster-
(cid:48) on the current dataset including the considered
ing result Y
candidate attack point, without re-computing the cluster-
ing explicitly. To this end, the attack point is assumed to
eﬀectively merge the two adjacent clusters. For each point
belonging to one of the two adjacent clusters, we thus set
(cid:48) corresponding to the ﬁrst (second)
to 1 (0) the value of Y
cluster. This amounts to considering hard clustering assign-
(cid:48) is computed, we evaluate the
ments. Once the estimated Y
(cid:48), and select the at-
objective function using the estimated Y
tack point that maximizes its value.

Bridge (Soft). This is a variant of the latter approach that
(cid:48) using soft clustering assignments instead of hard
estimates Y
(cid:48) is estimated
ones. In particular, the (i, k)th element of Y
as the posterior probability that the ith sample belongs to
the kth cluster, using a Gaussian Kernel Density Estimator
(KDE) with bandwidth parameter h. When h is too small,
the posterior estimates tend to the value of 1/k, i.e., each
point is assigned to any cluster with the same probability.
When h is too high, instead, they tend to hard assignments.
As a rule of thumb, the value of h should be thus comparable
to the average distance between all possible pairs of samples
in the dataset. The rationale of this strategy is to try ﬁnding
connections that can potentially merge large clusters with
more than one attack sample, to mitigate the limitation of
our greedy approach.

5. A CASE STUDY: MALHEUR

To illustrate the eﬀect of the proposed poisoning attacks
in a practical setting, we conduct a case study with the

open-source tool Malheur.1 The tool implements techniques
for clustering and classiﬁcation of program behavior and has
been applied in diﬀerent settings for analyzing malware in
the wild [11, 16, 28]. The analysis realized by Malheur builds
on four basic steps.

1. MIST Representation. As the ﬁrst step, the behavior
of malware binary is monitored in a sandbox environ-
ment and stored as MIST reports [31]. In this format,
the behavior of a program is described as a sequence
of events, where individual execution ﬂows of threads
and processes are grouped in a single, sequential re-
port. Each event encodes one monitored system call
and its arguments, where the arguments are arranged
in diﬀerent levels of blocks, reﬂecting behavior with
diﬀerent degree of granularity. Depending on the con-
ﬁguration of Malheur, the monitored behavior can be
analyzed at these diﬀerent MIST levels [28].

2. Embedding. As the next step, Malheur embeds the
monitored behavior in a high-dimensional vector space,
where each dimension is associated with a short se-
quence of q events—a so called q-gram.
If a q-gram
occurs in the monitored events of a program, the re-
spective dimension is set to 1 in its vector, otherwise
it is set to 0. To enable a fair comparison of pro-
grams that strongly diﬀer in the amount of observed
events, each vector x is additionally normalized, such
that ||x||2 = 1, namely, projecting the vectors onto a
hypersphere of unit radius in the vector space.

3. Clustering. For partitioning the embedded behavior
into groups, Malheur implements an eﬃcient variant of
hierarchical clustering that supports single-linkage and
complete-linkage hierarchical clustering. To alleviate
the quadratic run-time complexity of these clustering
algorithms, the tool can approximate the underlying
data by limiting the analysis to a small subset of proto-
types. For our case study, we disable this functionality
and instead apply Malheur without prototype-based
approximation.

4. Classiﬁcation. Finally, Malheur supports assigning un-
known behavior to previously discovered clusters. This
assignment is realized using a nearest-neighbor classi-
ﬁcation, where a new vector is assigned to a nearby
cluster if it appears within a certain distance to its
members. This nearest-neighbor classiﬁcation can be
approximated by searching for nearest neighbors in a
set of prototypes instead of all cluster members. We
again disable this functionality and operate on the full
data for our case study.

Each of the four steps supports diﬀerent parameters that
can be adapted in the conﬁguration of Malheur. For our case
study, we start with a basic setup by using MIST level 1,
setting the q-gram length to 1 and especially considering
single-linkage clustering by disabling the prototype-based
approximation used by Malheur. The use of the latter would
indeed imply a sort of complete-linkage pre-processing clus-
tering step, which would in turn require us to signiﬁcantly
revisit the derivation of a proper poisoning attack. We there-
fore leave this issue to future work. Finally, although this

1http://www.mlsec.org/malheur

31Figure 1: Bridge-based attacks against single-linkage clustering. The candidate attack samples connecting the k − 1 closest
clusters are highlighted as red hexagons.

setup slightly simpliﬁes our attack, previous work has al-
ready shown that creating artiﬁcial q-grams of system calls,
and similarly the use of diﬀerent MIST levels, is not a chal-
lenge for an attacker [29, 30, 34], especially if she has full
control over the behavior, as in the case of malware.

6. EXPERIMENTAL EVALUATION

In this section we apply the aforementioned evaluation
framework to a concrete case study: we evaluate the worst-
case eﬀects of the poisoning attacks described in Section
4.1 using real malware samples, and against a real-world
tool for behavioral malware clustering.
In Section 6.1 we
present the datasets employed for our investigation. Then,
in Section 6.2 we provide all relevant details about the ex-
perimental setup and evaluation metrics. In Section 6.3 we
summarize the main attack strategies implemented for the
evaluation, including the modiﬁcations to the derivation of
poisoning attacks that allow us to deal with the speciﬁc fea-
ture representation exploited by Malheur. Finally, in Sec-
tion 6.4 we present and discuss our experimental results.
6.1 Datasets

For our experiments and evaluation we make use of two
diﬀerent datasets: ﬁrst, the data that was originally consid-
ered by Rieck et al. in [28], and second, a dataset consisting
of more recent malware samples collected in 2013.

Malheur data. This dataset consists of a selection of 3131
malware samples collected in a period of 3 years up to Au-
gust 2009, and made publicly available in the same year.2
It comprises a reference dataset that was used to calibrate
the clustering algorithm in [28], and 7 application datasets
for evaluating and testing their approach. The latter rep-
resent malware found on the Internet within 24 hours on 7
consecutive days. For our experiments we stick to a similar
setup in order to ensure the comparability with the original
approach and optimally show the practicality of our attack.

Recent Malware data. In addition to the data used for
the Malheur project, we gathered malware samples from
most prominent families in 2013. Similarly to [28] we rely
on the popular antivirus scanner by Kaspersky Lab for this
ranking and labeling of the malware samples. We chose 5 of
the top 10 detections according to a recent threat report [21],

2http://pi1.informatik.uni-mannheim.de/malheur/

Dataset

Number of samples

DangerousObject.Multi.Generic
Trojan.Win32.Generic
Virus.Win32.Sality.gen
Trojan.Win32.Starter.lgb
Virus.Win32.Nimnul.a

Total

129
120
112
150
146

657

Table 1: Summary of the malware families collected in 2013
for the Recent Malware data.

and selected those families for which we were able to gather
more than 100 but at most 150 samples. A summary of the
exact numbers is given in Table 1.

When running Malheur on the aforementioned datasets
using the MIST-level-1 binary embedding discussed in Sect. 5,
we have respectively found 85 and 78 distinct feature values
(i.e., 1-grams).

6.2 Experimental Setup

To fairly evaluate the clustering process, we randomly
split each dataset into two disjunct portions of equal size,
namely, T and S. The T portion is used to calibrate the
clustering algorithm, and, in particular, to estimate the cut-
oﬀ distance (see Sect. 4.1). As suggested in [28], we select
as the optimal cutoﬀ distance the one that maximizes the
F-measure (see below for its deﬁnition). The S split is then
used to evaluate the calibrated clustering on unseen mal-
ware against an increasing percentage of poisoning attacks.
This procedure is repeated ﬁve times and results are aver-
aged over these repetitions. In our experiments, the value of
the cutoﬀ distance has been found to be 0.49 on the Malheur
dataset and 0.63 on the Recent Malware dataset, on average,
with a negligible standard deviation in both cases. Although
in these experiments we assume that the cutoﬀ distance is
known to the attacker, more realistically an attacker can try
estimating it from the data in a more conservative manner
(i.e., essentially underestimating its real value), eventually
poisoning the clustering result at the expense of using more
poisoning samples. Clustering results are evaluated accord-
ing to the three measures given below.

cluster 1cluster 2cluster 3poisoning (bridge)32Figure 2: Computation of a bridge-based attack against
single-linkage clustering, using the feature representation of
Malheur, i.e., a binary embedding with points additionally
projected onto a unit hypersphere (i.e., with unit (cid:96)2 norm).
This example considers a simple two-dimensional feature set,
where we highlighted the only three admissible points, i.e.,
√
the samples x1 = (0, 1) and x2 = (1, 0), and their ideal
2). Besides this simple case,
bridging point a = (1/
the creation of eﬀective bridge-based attacks in this space
is generally much more challenging than that considered in
our previous work [4, 9], due to the restrictions imposed by
the given feature representation.

√
2, 1/

1. The objective function in Eq. (4), with dc given by
Eq. (2), that measures the distance of the current clus-
tering from that obtained in absence of poisoning.

2. The number of clusters, that helps us to gain a better
understanding of how the attacks taint the clustering
process. In particular, since the considered poisoning
attacks are expected to “bridge” clusters, the number
of clusters should decrease as the attack progresses.

(cid:80)

(cid:80)

3. The F-measure [28, 32], deﬁned as the harmonic mean
between precision (π) and recall (ρ), i.e., 2 πρ
π+ρ . The
latter are respectively computed as π = 1
j maxi cij,
n
and ρ = 1
i maxj cij, where cij is the number of
n
malware samples belonging to the i-th family present
in the j-th cluster [28]. Precision reﬂects how well in-
dividual clusters agree with malware families, whereas
recall measures to which extent malware families are
scattered across clusters. Both measures provide com-
plementary information about the quality of clustering
results, summarized by the F-measure.

6.3 Attack Strategies

In this section we explain how we generate the poisoning
samples to attack Malheur. First, as Malheur is conﬁgured
with binary embedding and (cid:96)2 normalization in our case (see
Sect. 5), the feature vectors of poisoning samples have to ful-
ﬁll the constraints given by the set Ωp in Eq. (3). Besides
these constraints, another fundamental pre-requisite that we
impose is that poisoning points have to represent realistic
malware samples. The reason is that a sample that does
not exhibit any malicious or intrusive functionality may not
be included into the set of collected malware to cluster, and
the attack would be trivially defeated.3 Therefore, we gen-
erate every poisoning sample by ﬁrst selecting a malware

3Note however that the main goal of poisoning samples is
not to preserve the malicious functionality of the embedded
malware code, which is required here only to avoid having
such samples discarded by a simple preliminary antivirus
analysis. Instead, their primary goal is to subvert the clus-
tering output on the rest of the data, in order to produce
a less eﬀective characterization of malware families. This

sample from the given S split, and then manipulating its
features by only increasing their value. Note that the value
of a feature can only be increased from 0 to 1/||a||2, being
||a||2 the (cid:96)2-norm of the attack sample. We thus refer in the
following to this kind of manipulation as feature addition,
for short. This manipulation indeed preserves the malicious
functionality of the initially-selected malware, as it does not
compromise the set of instructions required to execute the
original malicious code. Moreover, before adding any candi-
date poisoning point to the data, we verify whether another
point with the same feature values is already present. If this
is the case, we discard the current sample and choose the
next best candidate attack point. This allows us to discard
duplicate attack points, as their presence may worsen the
attack progress.

An important consequence of the particular embedding
used by Malheur is that it aﬀects the way we compute the
bridge between any two points to create our candidate attack
samples. This is a rather important distinction with respect
to our previous work in [4, 9].
In fact, the midpoint in
this case can not be computed as the average of the two
neighboring points, as it is instead possible, for instance,
when real-valued features are used. However, the point that
is as equidistant as possible from each of the two neighboring
points can be found by cloning the neighboring point with
the smaller norm ﬁrst, and then starting adding features to
it that are not null in the other neighboring point, until the
candidate attack point is as equidistant as possible from the
two points. A simple two-dimensional example is given in
Fig. 2. The only drawback of this procedure is that the
candidate attack point may be sometimes farther from the
two neighboring points than they are with respect to each
other. In these cases, the attack may not be eﬀective, as it
may not eﬀectively bridge the two neighboring clusters.

In these experiments we consider six distinct poisoning at-
tack strategies. In addition to the three bridge-based attacks
deﬁned in Sect. 4.1, we consider Random and Random (Best)
as in [9], and a variant of our bridge-based attacks named
F-measure (Best). Random generates any attack point by
cloning a randomly-selected malware from the available set
S, and adding to it a random number of features. Ran-
dom (Best) works similarly, with the diﬀerence that not one
but k − 1 attack points are selected at random, being k
the actual number of clusters at any given attack iteration.
Then, the objective function is evaluated for each candidate
point by re-running the clustering algorithm, and the best
attack point is chosen. F-measure (Best) works as Bridge
(Best), but chooses the best candidate attack point as the
one that minimizes the F-measure instead of maximizing the
(cid:48)). As Random (Best) and Bridge
objective function dc(Y, Y
(Best), this strategy also requires evaluating the clustering
result k− 1 times to determine the best attack at each itera-
tion, while the other strategies are computationally lighter.
As for Bridge (Soft), we set the kernel bandwidth h as the
average distance between each possible pair of samples in the
data, which yielded h ≈ 0.2 in each run. We ﬁnally point
out that, if more than one candidate attack point exhibit
the same value of the desired function (either the objective
function or the F-measure, depending on the attack strat-

may indeed not only lead to lower malware detection rates
or higher false alarm rates, but it also makes more diﬃcult
to identify the proper countermeasures or removal tools in
case of infection.

Nov. 7th, 2012 Poisoning adaptive biometric systems - B. Biggio et al. - SPR 2012 1 x1x2a33egy), we select the one that produces the smaller number of
clusters. If the tie persists, we break it at random.
6.4 Results

Results for the Malheur and the Recent Malware datasets
are presented in Fig. 3. For each dataset, we show how
the value of the objective function, the number of clusters,
and the F-measure change for an increasing percentage of
injected poisoning samples. We observe a similar behavior of
these metrics for both datasets, which is summarized below
in two points.

1. Simply injecting random points does not allow one to
signiﬁcantly worsen the quality of the resulting clus-
tering. We can in fact observe that, for the Random
attack, neither the value of the objective function nor
the F-measure are aﬀected at all. The reason is that
each of the randomly-generated attack points is too
far from the other clusters and it is thus clustered as a
singleton, without aﬀecting the clustering result on the
rest of the samples. Random (Best) performs slightly
better, as it clearly makes k − 1 attempts at each iter-
ation to ﬁnd a better attack point, instead of one.

2. Maximizing the considered objective function actually
allows us to reduce the number of clusters, and, thus,
to compromise the quality of the resulting clustering,
despite it does not incorporate any knowledge of the
problem domain, of the clustering algorithm, and of
the features used. Furthermore,
looking at Fig. 3,
we can observe that the bridge-based strategies that
maximize the objective function achieve similar per-
formances to F-measure (Best), which instead mini-
mizes the F-measure. Whereas the objective function
is general, the F-measure takes into account the ground
truth of the problem. We can therefore reasonably ar-
gue that the proposed objective function and the con-
sequent attack strategies can be successfully employed
to attack also systems diﬀerent from Malheur.

Some further comments can be made separately for the
two data sets. What appears evident from the results on the
Malheur dataset is that injecting an even small percentage of
poisoning points reduces signiﬁcantly the number of clusters.
Bridge (Best) and F-measure (Best) are able to reduce the
number of cluster from an initial value of 40 to a value of 5
with only the 2% of injected samples. If we further increase
such percentage up to 5% a single, large cluster is created,
where all the initial ones are merged. Bridge (Soft) and
Bridge (Hard) appear to be a bit less eﬀective since they
require a slightly higher percentage of injected samples to
achieve similar results. Nevertheless, it is worth pointing out
that, from a computational standpoint, both these strategies
are signiﬁcantly less expensive than the Best strategies.

On the Recent Malware dataset the considered attacks ap-
pear to be less eﬀective. In particular, the bridge-based at-
tacks here are not able to merge all the clusters into a unique
cluster. At some point, instead, it happens that the strate-
gies are no longer able to inﬂict any damage to the current
clustering. The reason is that the candidate bridge points in
this case are selected too far from their corresponding neigh-
boring points, and the former are thus clustered apart in-
stead of successfully merging the desired clusters. We argue
that this may be somehow due to the smaller number of fea-
tures found in this dataset, as this factor limits the number

of manipulations that the attacker can make to ﬁnd a suit-
able attack point. This may be an interesting starting point
for future work to understand how to improve robustness
of clustering algorithms to poisoning attacks by restricting
the feature set and the number of potential manipulation
the attacker can make on the attack samples. Nevertheless,
one should keep in mind that, in this case, the objective
function reaches anyway the value of 250 for Bridge (Best),
which still means that 250 pair of samples out of 329 sam-
ples have changed their clustering assignment with respect
to the clustering in the absence of poisoning.

7. CONCLUSIONS AND FUTURE WORK
A widespread approach for coping with the plethora of
novel malware are clustering algorithms from the area of
machine learning. While these algorithms can help group-
ing similar malware samples automatically, they have not
been originally designed to operate in an adversarial set-
ting. Our work shows that, by leveraging on vulnerabilities
of clustering algorithms, an attacker can signiﬁcantly im-
pact the performance of malware clustering. In our evalua-
tion, only a small fraction of poisoning samples is necessary
to largely destroy the recovery of families in a dataset of
real malware.
In particular, in this work we have consid-
ered Malheur, i.e., a popular malware clustering tool. We
have modiﬁed previously-proposed poisoning attacks to cope
with its speciﬁc feature representation, and to incorporate
the corresponding application-speciﬁc constraints in the cre-
ation of real, poisoning malware samples. Although we have
focused on a particular setup of this tool, we argue that at-
tacks to other setups and clustering systems should not be
considered a major challenge for a sophisticated attacker.
Creating behavioral features artiﬁcially may be more or less
diﬃcult depending on the underlying sandbox environment,
yet the exploited vulnerability resides in the clustering algo-
rithms and thus can hardly be ﬁxed by changing the feature
representation. As a result, our work casts serious doubt
about the security of some clustering algorithms in mal-
ware analysis systems, and there may be considerable need
for novel algorithms that are more robust against poisoning
and malicious noise.

Future extensions of this work may include: investigation
of attacks in which the adversary has only limited knowl-
edge of the system, i.e., attacks in which the input data is
not known to the attacker, who may realistically only collect
surrogate data from the same sources; development of poi-
soning attacks that may target a larger family of clustering
algorithms (instead of considering only specialized heuris-
tics); and development of appropriate countermeasures to
improve security of clustering algorithms against adversar-
ial threats and well-crafted attacks.

It is also worth remarking here that poisoning attacks
are not the only kind of attack that may be incurred by
a clustering-based system operating in an adversarial set-
ting; e.g., if some of the clusters are used to characterize
the behavior of legitimate users or software, an attacker
may aim to manipulate the malware behavior in order to
mimic the legitimate samples, without signiﬁcantly altering
the clustering output on the rest of the data. This attack has
been referred to as obfuscation attack in [9]. We refer the
reader to the same work for a detailed taxonomy of poten-
tial attacks against clustering. However, the implementation
of such attacks for more realistic application scenarios and

34Figure 3: Results for the Malheur dataset (left column) and the Recent Malware dataset (right column).

speciﬁc feature representations remains a non-trivial open
issue, which should be addressed as done in this paper for
poisoning attacks and behavioral malware clustering.

8. ACKNOWLEDGMENTS

We thank Cristian Milia for supporting during tests. This
work has been partly supported by the Regional Adminis-
tration of Sardinia (RAS), Italy, within the projects “Secu-
rity of pattern recognition systems in future internet” (CRP-
18293), and “Advanced and secure sharing of multimedia
data over social networks in the future Internet” (CRP-
17555). Both projects are funded within the framework of
the regional law L.R. 7/2007, Bando 2009. Furthermore, we
acknowledge funding from BMBF under the project PROSEC
(FKZ 01BY1145). The opinions, ﬁndings and conclusions
expressed in this paper are solely those of the authors and
do not necessarily reﬂect the opinions of any sponsor.

9. REFERENCES

1. M. Bailey, J. Oberheide, J. Andersen, Z. M. Mao, F. Ja-
hanian, and J. Nazario. Automated classiﬁcation and
analysis of internet malware. In Recent Adances in In-
trusion Detection (RAID), pages 178–197, 2007.

2. M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and J. D.
Tygar. Can machine learning be secure? In ASIACCS
’06: Proceedings of the 2006 ACM Symposium on Infor-
mation, computer and communications security, pages
16–25, New York, NY, USA, 2006. ACM.

3. U. Bayer, P. Comparetti, C. Hlauschek, C. Kruegel, and
E. Kirda. Scalable, behavior-based malware clustering.
In Proc. of Network and Distributed System Security
Symposium (NDSS), 2009.

4. B. Biggio, S. R. Bul`o,

I. Pillai, M. Mura, E. Z.
Mequanint, M. Pelillo, and F. Roli. Poisoning complete-
linkage hierarchical clustering. In Structural, Syntactic,
and Statistical Pattern Recognition, 2014, In press.

5. B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c,
P. Laskov, G. Giacinto, and F. Roli. Evasion attacks
against machine learning at test time.
In H. Block-
eel, K. Kersting, S. Nijssen, and F. ˇZelezn´y, editors,
European Conference on Machine Learning and Princi-
ples and Practice of Knowledge Discovery in Databases
(ECML PKDD), Part III, volume 8190 of Lecture Notes
in Computer Science, pages 387–402. Springer Berlin
Heidelberg, 2013.

6. B. Biggio,

I. Corona, B. Nelson, B. Rubinstein,
D. Maiorca, G. Fumera, G. Giacinto, and F. Roli. Secu-
rity evaluation of support vector machines in adversarial
environments. In Y. Ma and G. Guo, editors, Support
Vector Machines Applications, pages 105–153. Springer
International Publishing, 2014.

7. B. Biggio, G. Fumera, and F. Roli. Security evaluation of
pattern classiﬁers under attack. IEEE Transactions on
Knowledge and Data Engineering, 26(4):984–996, April
2014.

01020304050607080−1−0.8−0.6−0.4−0.200.20.40.60.81  RandomRandom (Best)Bridge (Best)Bridge (Soft)Bridge (Hard)F−measure (Best)0%2%5%7%9%11%13%15%17%18%20%02004006008001000120014001600Objective function0%2%5%7%9%11%13%15%16%18%20%050100150200250300Objective function0%2%5%7%9%11%13%15%17%18%20%020406080100120140160180200Number of clusters0%2%5%7%9%11%13%15%16%18%20%010203040506070Number of clusters0%2%5%7%9%11%13%15%17%18%20%102030405060708090100F−measureFraction of poisoning attacks0%2%5%7%9%11%13%15%16%18%20%5254565860626466687072F−measureFraction of poisoning attacks358. B. Biggio, B. Nelson, and P. Laskov. Poisoning attacks
against support vector machines.
In J. Langford and
J. Pineau, editors, 29th Int’l Conf. on Machine Learn-
ing. Omnipress, 2012.

22. M. Kloft and P. Laskov. Online anomaly detection under
adversarial impact. In Proceedings of the 13th Interna-
tional Conference on Artiﬁcial Intelligence and Statis-
tics (AISTATS), pages 405–412, 2010.

9. B. Biggio, I. Pillai, S. R. Bul`o, D. Ariu, M. Pelillo, and
F. Roli. Is data clustering in adversarial settings secure?
In Proceedings of the 2013 ACM Workshop on Artiﬁcial
Intelligence and Security, AISec ’13, pages 87–98, New
York, NY, USA, 2013. ACM.

10. M. Br¨uckner, C. Kanzow, and T. Scheﬀer. Static predic-
tion games for adversarial learning problems. J. Mach.
Learn. Res., 13:2617–2654, 2012.

11. M. Brunner, M. Epah, H. Hoﬁnger, C. Roblee, P. Schoo,
and S. Todt. The fraunhofer aisec malware analysis lab-
oratory. Technical report, Fraunhofer Institute AISEC,
2010.

12. P. Fogla, M. Sharif, R. Perdisci, O. Kolesnikov,
In
and W. Lee.
USENIX-SS’06: Proceedings of
the 15th conference
on USENIX Security Symposium, Berkeley, CA, USA,
2006. USENIX Association.

Polymorphic blending attacks.

13. G. Gu, R. Perdisci, J. Zhang, and W. Lee. BotMiner:
Clustering Analysis of Network Traﬃc for Protocol- and
Structure-Independent Botnet Detection.
In Proc. of
USENIX Security Symposium, 2008.

14. G. Gu, J. Zhang, and W. Lee. BotSniﬀer: Detect-
ing Botnet Command and Control Channels in Network
Traﬃc. In Proc. of Network and Distributed System Se-
curity Symposium (NDSS), 2008.

15. F. Guo, P. Ferrie, and T. Chiueh. A study of the packer
problem and its solutions. In Recent Advances in Intru-
sion Detection, 2008.

16. X. Hu and K. G. Shin. DUET: integration of dynamic
and static analyses for malware clustering with cluster
ensembles. In Proc. of Annual Computer Security Ap-
plications Conference (ACSAC), 2013.

17. L. Huang, A. D. Joseph, B. Nelson, B. Rubinstein, and
J. D. Tygar. Adversarial machine learning. In 4th ACM
Workshop on Artiﬁcial Intelligence and Security (AISec
2011), pages 43–57, Chicago, IL, USA, October 2011.

18. iSeclab. Anubis. http://anubis.iseclab.org, visited

April, 2014.

19. A. K. Jain, M. N. Murty, and P. J. Flynn. Data clus-
tering: A review. ACM Comput. Surv., 31(3):264–323,
Sept. 1999.

20. J. Jang, D. Brumley, and S. Venkataraman. Bitshred:
feature hashing malware for scalable triage and semantic
analysis. In Proc. of ACM Conference on Computer and
Communications Security (CCS), pages 309–320, 2011.

21. Kaspersky Lab. KASPERSKY SECURITY BULLETIN
http://media.kaspersky.com/pdf/KSB_2013_

2013.
EN.pdf, 2014.

23. A. Kolcz and C. H. Teo. Feature weighting for im-
proved classiﬁer robustness.
In Sixth Conference on
Email and Anti-Spam (CEAS), Mountain View, CA,
USA, 16/07/2009 2009.

24. R. Perdisci, D. Ariu, and G. Giacinto. Scalable ﬁne-
grained behavioral clustering of http-based malware.
Computer Networks, 57(2):487 – 500, 2013.

25. R. Perdisci, W. Lee, and N. Feamster. Behavioral clus-
tering of HTTP-based malware and signature generation
using malicious network traces.
In Proc. of USENIX
Symposium on Networked Systems Design and Imple-
mentation (NSDI), pages 391–404, 2010.

26. R. Perdisci and M. U. VAMO: towards a fully automated
malware clustering validity analysis. In Proc. of Annual
Computer Security Applications Conference (ACSAC),
2012.

27. K. Rieck, T. Holz, C. Willems, P. D¨ussel, and P. Laskov.
Learning and classiﬁcation of malware behavior. In De-
tection of Intrusions and Malware & Vulnerability As-
sessment (DIMVA), pages 108–125, July 2008.

28. K. Rieck, P. Trinius, C. Willems, and T. Holz. Auto-
matic analysis of malware behavior using machine learn-
ing. J. Comput. Secur., 19(4):639–668, 2011.

29. K. Tan, K. Killourhy, and R. Maxion. Undermining an
anomaly-based intrusion detection system using com-
mon exploits. In Recent Adances in Intrusion Detection
(RAID), pages 54–73, 2002.

30. K. Tan and R. Maxion. “Why 6?” Deﬁning the oper-
ational limits of stide, an anomaly-based intrusion de-
tector.
In Proc. of IEEE Symposium on Security and
Privacy, pages 188–201, 2002.

31. P. Trinius, C. Willems, T. Holz, and K. Rieck. A mal-
ware instruction set for behavior-based analysis. In Proc.
of GI Conference “Sicherheit” (Sicherheit, Schutz und
Verl¨asslichkeit), pages 205–216, Oct. 2010.

32. C. J. van Rijsbergen.

Information Retrieval. Butter-

worth, 1979.

33. VirusTotal. https://www.virustotal.com.

34. D. Wagner and P. Soto. Mimicry attacks on host
based intrusion detection systems.
In Proc. of ACM
Conference on Computer and Communications Security
(CCS), pages 255–264, 2002.

35. C. Willems, T. Holz, and F. Freiling. CWSandbox: To-
wards automated dynamic binary analysis. IEEE Secu-
rity and Privacy, 5(2):32–39, 2007.

36