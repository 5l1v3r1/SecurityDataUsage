Hash First, Argue Later

Adaptive Veriﬁable Computations on Outsourced Data

Dario Fiore

IMDEA Software Institute

Cédric Fournet
Microsoft Research

∗
Esha Ghosh
Brown University

Markulf Kohlweiss
Microsoft Research

Olga Ohrimenko
Microsoft Research

Bryan Parno

Microsoft Research

ABSTRACT
Proof systems for veriﬁable computation (VC) have the po-
tential to make cloud outsourcing more trustworthy. Recent
schemes enable a veriﬁer with limited resources to delegate
large computations and verify their outcome based on suc-
cinct arguments: veriﬁcation complexity is linear in the size
of the inputs and outputs (not the size of the computation).
However, cloud computing also often involves large amounts
of data, which may exceed the local storage and I/O capa-
bilities of the veriﬁer, and thus limit the use of VC.

In this paper, we investigate multi-relation hash & prove
schemes for veriﬁable computations that operate on succinct
data hashes. Hence, the veriﬁer delegates both storage and
computation to an untrusted worker. She uploads data and
keeps hashes; exchanges hashes with other parties; veriﬁes
arguments that consume and produce hashes; and selectively
downloads the actual data she needs to access.

Existing instantiations that ﬁt our deﬁnition either target
restricted classes of computations or employ relatively ineﬃ-
cient techniques. Instead, we propose eﬃcient constructions
that lift classes of existing arguments schemes for ﬁxed rela-
tions to multi-relation hash & prove schemes. Our schemes
(1) rely on hash algorithms that run linearly in the size of
the input; (2) enable constant-time veriﬁcation of arguments
on hashed inputs; (3) incur minimal overhead for the prover.
Their main beneﬁt is to amortize the linear cost for the ver-
iﬁer across all relations with shared I/O. Concretely, com-
pared to solutions that can be obtained from prior work,
our new hash & prove constructions yield a 1,400x speed-
up for provers. We also explain how to further reduce the
linear veriﬁcation costs by partially outsourcing the hash
computation itself, obtaining a 480x speed-up when applied
to existing VC schemes, even on single-relation executions.

∗Work done at Microsoft Research.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978368

1.

INTRODUCTION

Cryptographic proof systems let a veriﬁer check that the
computation executed by an untrusted prover was performed
correctly [28]. These systems are appealing in a variety
of scenarios, such as cloud computing, where a user out-
sources computations and wishes to verify their integrity
given their inputs and outputs (I/O) [2, 36, 27, 25], or
privacy-preserving applications, where a user owns sensitive
data and wishes to release partial information with both
conﬁdentiality and integrity guarantees [42, 24]. Typically,
these systems require the prover to perform considerable ad-
ditional work to produce a proof that can be easily checked
by the veriﬁer.

Recent advances in veriﬁable computations have crossed
an important practical threshold: verifying a proof given
some I/O is faster than performing the computation lo-
cally [40, 7, 43, 45]. While these systems perform well
when delegating computation-intensive algorithms, they do
not help much with data-intensive applications, inasmuch as
veriﬁcation remains linear in the application’s I/O.

Although some linear work is unavoidable when uploading
data, ideally one would like to pay this price just once, rather
than every time one veriﬁes a computation that takes this
data as input. This is particularly relevant for cloud com-
puting on big data, where the veriﬁer may not have enough
local resources to encode and upload the whole database
each time she delegates a query or, more generally, where
many parties contribute data over a long period of time.

Approaches providing amortized veriﬁcation do exist for
limited classes of computations, such as data retrieval. For
instance, the user may keep the root of a Merkle hash tree,
and use it to verify the retrieved content. Unfortunately, as
explained below, embeddings of this approach into generic
proof systems incur large overheads for the prover.

Our goal is to enable practical veriﬁable computation for
data-intensive applications. In particular, we wish to design
schemes where veriﬁcation time is independent of both the
size of the delegated computations and the size of their I/O.
Moreover, we wish to preserve the expressiveness of existing
VC schemes (e.g., supporting NP relations) without adding
to the prover’s burden, which is already several orders of
magnitude higher than the original computation.

Modelling Hash & Prove (HP) We ﬁrst propose a
model that captures the idea of hashing and uploading data
once and then using the resulting hashes across multiple
veriﬁable computations.
In this model, the veriﬁer needs
only keep track of hashes, while the prover stores the cor-

1304responding data. The prover can use the data to perform
computations and then (selectively) return results in plain-
text to the veriﬁer. As described below, hashes yield several
beneﬁts when delegating veriﬁable computations.

Flexible Reuse Hashes depend only on the data and are
not tied to any particular computation. Hence, once
a data hash is computed, it can be used to verify any
computation that uses the corresponding data. It can
also be used with diﬀerent proof systems, as long as
they rely on the same hash format.

Sharing Hashes are a compact representation of the data
that can be easily shared and authenticated. Hence,
veriﬁers can delegate computations on someone else’s
hashes, or chain multiple computations using interme-
diate hashes, without ever seeing or receiving the cor-
responding data.

Provenance A record of an input hash, an output hash,
and a proof can serve as a succinct provenance token
that can be easily and independently veriﬁed.

Conﬁdentiality The veriﬁer checks arguments on hashes
of data that she may never see in plaintext; hence ran-
domized hashes enable zero-knowledge arguments.

Updates If the hash mechanism also supports eﬃcient up-
dates, that is, given hash(x), one can compute hash(x(cid:48))
in time that depends only on the diﬀerence between x
and x(cid:48), then it also enables applications with dynamic
data and streaming. For instance, a hashed database
may be updated by uploading the new data and locally
updating the hash.

Our hash & prove model extends non-interactive proof sys-
tems, with an intermediate hash algorithm between the in-
put and the proof veriﬁcation, and with the possibility of
proving multiple relations. It is inspired by multi-function
veriﬁable computation [41, 23], with relations instead of
functions so that we can capture more general use cases,
notably those where the prover provides its own (private)
input to the computation.

Instantiating Hash & Prove Now equipped with a model
for outsourcing multiple computations on authenticated data,
we survey how existing work could be used to instantiate HP
schemes.
In particular, we observe that existing solutions
have limitations either in eﬃciency or in generality.

Some prior work [11, 26, 5, 15] considers the idea of prov-
ing the correctness of a computation on data succinctly rep-
resented by a hash. This approach consists of encoding the
veriﬁcation of the hash as part of a relation for the under-
lying proof system. Namely, if y = f (x) is the statement to
be proved, then one actually proves an extended statement
of the form y = f (x)∧ σ = hash(x), essentially treating x as
an additional witness. We henceforth refer to this method
as an inner encoding. Inner encodings are simple and gen-
eral, and can also be extended to more general data encod-
ings such as Merkle trees or authenticated data structures
(ADS) [44, 21]. On the other hand, inner encodings incur
a signiﬁcant overhead for the prover—indeed, unless hash is
carefully tuned to the proof system, its veriﬁable evaluation
on large inputs may dominate the prover costs.

Other works address reusability and succinct data repre-
sentation by using diﬀerent data encoding approaches that
we will call outer encodings. The basic idea of outer encod-
ings is that proofs are produced for the original statement,

e.g., y = f (x), and are linked to the encoded data x us-
ing some external mechanism. Works that can be explained
under this approach are commit & prove schemes [33, 16,
19] and homomorphic authenticators [4, 17, 29]. While we
discuss them in detail in §7, the main observation is that all
these works fall short in generality; i.e., they limit the class of
computations that can be executed on an hash value. While
commit & prove schemes can achieve greater generality by
using universal relations (as, e.g., in [5, 7, 9]), this typically
entails a signiﬁcant penalty in concrete eﬃciency.

New Hash & Prove Constructions Our main technical
contributions are eﬃcient, general HP constructions. Com-
pared to general inner encoding solutions, ours incurs mini-
mal overheads for the prover. Compared to prior outer en-
coding solutions, ours is fully general, in the sense that one
can hash data ﬁrst, without any restriction on the functions
that may later be executed and veriﬁed on it.

We instantiate multi-relation hash & prove schemes both
in the public and designated veriﬁer settings. Our solutions
are built in a semi-generic fashion by combining

(1) a veriﬁable computation (VC) or succinct non interac-

tive argument (SNARG) scheme, and

(2) an HP scheme for simple, speciﬁc computations.

At a high level, our construction uses an outer data encod-
ing, where general computation integrity is handled by (1),
whereas data authentication and linking to the computation
is handled by (2). As expected from an outer approach, this
combination does not add any overhead in the use of (1),
and the overhead introduced by (2) can be very low.

i Fi

i Fi

mitment of the form cx = (cid:81)
SNARG veriﬁcation, cx =(cid:81)

More speciﬁcally, for (1) we use any scheme where the
input-processing part of the veriﬁcation consists of a multi-
exponentiation, that is, anything resembling a Pedersen com-
xi , a property of virtu-
ally all modern, eﬃcient SNARGs [40, 7, 19, 9, 31]. Our
generic construction then outsources to the prover the origi-
nal computation of (1) as well as the input-processing part of
xi . We then ask the prover
to show the correctness of cx using the auxiliary HP scheme
(2). To this end, we only need a scheme that handles multi-
exponentiation computations. We propose our own eﬃcient
constructions for such HP schemes. For the designated veri-
ﬁer setting, we adapt a multi-function VC scheme from prior
work [23]. For public veriﬁability, we develop a new scheme,
which requires new techniques to achieve adaptive security.
Our analysis in §6 shows that, in comparison to the inner
encoding solution mentioned earlier, our HP scheme yields
a 1, 400× speed-up for provers, as well as public (proving)
keys that are shorter by the same factor.

Speeding up Hashing and Veriﬁcation As mentioned
above, VC schemes involve a veriﬁcation eﬀort linear in the
size of the I/O. Concretely, this veriﬁcation step is expensive
because it relies on public-key operations (e.g., a few elliptic-
curve multiplications for each word of I/O). With Hash &
Prove, this linear work is ﬁrst shifted to computing the hash,
and then amortized across multiple computations, but the
hash still has to be computed once.

When using inner encodings, one can choose standard,
very eﬃcient hash functions such as SHA2, which consider-
ably reduces the eﬀort of the veriﬁer, at the expenses of the
prover. Other trade-oﬀs between veriﬁer and prover costs

1305are possible, e.g., by using algebraic hash constructions [1,
8, 15]. When using outer encodings, the choice of a hash
function is more constrained. For instance, in Geppetto or
in our HP scheme, the encoding still consists of a multi-
exponentiation (i.e., n elliptic-curve multiplications where n
is the size of the input).

As another contribution, we provide a technique to out-
source such (relatively) expensive data encodings, at a mod-
erate additional cost for the prover, while requiring only
a trivial amount of linear work from the veriﬁer: an ar-
bitrary (fast) hash such as SHA2, and a few cheap ﬁeld
additions and multiplications, instead of elliptic curve oper-
ations. Concretely, this technique saves two orders of mag-
nitude in veriﬁcation time. It applies not only to our HP
scheme, but also to existing VC systems [40, 19, 9].

Other Data Encodings In our presentation, we focus on
plain hashes as a simple data encoding for all I/O, but many
alternatives and variations are possible, depending on the
needs of a given application. As a ﬁrst example, the I/O
can naturally be partitioned into several variables, each in-
dependently hashed and veriﬁed, to separate inputs from
diﬀerent parties, or with diﬀerent live spans.
(In a data-
intensive application, for instance, one may use a hash for
the whole database, and a separate hash for the query and
its response.) More advanced examples include authenti-
cated data structures, and more speciﬁc tools such as accu-
mulators. To illustrate potential extensions of our work, we
show that the HP model, and our generic HP construction,
can be extended to work with such outer encodings. Con-
cretely, we consider accumulators [37] and polynomial com-
mitments [32], with set operations [38] and batch openings
as restricted proof systems, respectively. By adapting our
constructions, we obtain a new accumulate & prove system.
Contents The paper is organized as follows: §2 deﬁnes our
notations, reviews assumptions we rely on, and recalls deﬁ-
nitions of succinct non-interactive argument systems. §3 de-
ﬁnes our hash & prove model, shows that some of the ex-
isting work satisﬁes it, and discusses their overhead for the
prover. §4 presents our eﬃcient HP construction and instan-
tiates it for public and designated veriﬁer settings. §5 presents
the deﬁnition and construction of a hash & prove variant
that supports hash outsourcing. §6 analyze the performance
of our constructions. §7 discusses related work.

The full version [22] also includes auxiliary deﬁnitions,
detailed proofs, and an extension of our work from hashes
to cryptographic accumulators.

2. PRELIMINARIES
Notation. Given two functions f, g : N → [0, 1] we write
f (λ) ≈ g(λ) when |f (λ) − g(λ)| = λ−ω(1). In other words,
for all k, there exists an integer n0 such that for all λ > n0,
we have |f (λ) − g(λ)| < 1
λk . We say that f is negligible
when f (λ) ≈ 0.
Algebraic Tools and Complexity Assumptions. All
our constructions make use of asymmetric bilinear prime-
order groups Gλ = (e, G1, G2, GT , p, g1, g2) with an admissi-
ble bilinear map e : G1 × G2 → GT . We use ﬁxed groups
for every value of the security parameter; this lets us com-
pose schemes that use them without requiring a joint setup
algorithm. Even when pairings are not required, we deﬁne
schemes for group G1 and generator g1 to anticipate their

usage in later constructions. Our constructions are proven
secure under the following assumptions.

Assumption 1 (Strong External Diﬃe-Hellman [39]). The
Strong External Diﬃe-Hellman (SXDH) assumption holds if
every p.p.t. adversary solves the Decisional Diﬃe-Hellman
(DDH) problems in G1 and G2 only with a negligible advan-
tage.

We introduce the Flexible co-CDH assumption and prove

that it is implied by the above SXDH assumption.

Assumption 2 (Flexible co-CDH). The Flexible co-CDH
$←− G2, a $←− Zp,
2 such

assumption holds if, given (g2, g2
every p.p.t. adversary outputs a tuple (h, ha) ∈ G1
that h (cid:54)= 1 only with negligible probability.

a) where g2

Lemma 2.1. Strong External Diﬃe-Hellman implies Flex-
ible co-CDH.
Proof. Given A that solves Flexible co-CDH with non-negli-
gible advantage, we show how to build an adversary A(cid:48) for
DDH in G2. A(cid:48) is given a DDH instance (g, ga, gb, C) ∈ G4
and has to decide if C = gab. A(cid:48) runs A with input (g, ga).
Let A output (h, ha). Then A(cid:48) can check if C = gab by
checking if e(ha, gb) ?= e(h, C) holds. Hence A(cid:48) succeeds in
solving the DDH instance with A’s success probability.

2

For extractability, we optionally require the following as-

sumption parameterized by hash size n:

Assumption 3 (Bilinear n-Knowledge of Exponent). The
Bilinear n-Knowledge of Exponent assumption holds if, for
every p.p.t. adversary A, there exists a p.p.t. extractor E
such that for all large enough λ and ‘benign’ auxiliary input
aux ∈ {0, 1}poly(λ)
pp = Gλ; Hi
(A, B ;(x1, . . . , xn)) ← (A(cid:107)E) (pp,{Hi, H ω

$←− G1;ω $←− Zp;

(cid:20)

Pr

n(cid:89)

Aω = B ∧ A (cid:54)=

i }n

(cid:21)

i=1, aux) :
≈ 0

H xi
i

i=1

In the game above, (u; w) ← (A(cid:107)E) indicates running both
algorithms on the same inputs and random tape, and as-
signing their results to u and to w, respectively. This as-
sumption can been seen as an n-Knowledge of Exponent
Assumption [11] but for the general group model. Indeed
the authors of [11] use the argument by Groth [30] to con-
jecture that their assumption must hold independently of the
bilinear structure. Auxiliary input is required to be drawn
from a ‘benign distribution’ to avoid impossibility of certain
knowledge assumptions [12, 10].
2.1 Online-Ofﬂine SNARKs

We recall the deﬁnition of succinct non-interactive argu-
ments (SNARG) and arguments of knowledge (SNARK) as
used by our constructions.
Let {Rλ}λ be a sequence of families of eﬃciently decidable
relations R ∈ Rλ, with R ⊂ UR× WR. For pairs (u ; w) ∈ R,
we call u the instance and w the witness; we are interested in
producing and verifying arguments that ∃w.R(u ; w) holds.
We require that all instances include some data in a ﬁxed
format. That is, for each R ∈ Rλ, we have UR = X × VR

1306and instances are of the form u = (x, v). For example, u
may consist of the input x and output y of a function with
domain X, i.e., y = f (x). More generally, u may consist
of the inputs x, y and output z of functions whose domains
include X, i.e., z = f (x, y).
For any sequence of families of eﬃciently decidable rela-
tions {Rλ}λ as deﬁned above, SNARGs and SNARKs con-
sist of 3 algorithms VC = (KeyGen, Prove, Verify), as follows.
(EK, VK) ← KeyGen(1λ, R) takes the security parameter and
a relation R ∈ Rλ and computes evaluation and veri-
ﬁcation keys.

Π ← Prove(EK, u ; w) takes an evaluation key for R, an in-
stance u, and a witness w such that R(u ; w) holds,
and returns a proof.

b ← Verify(VK, u, Π) takes a veriﬁcation key and an instance
u, and either accepts (b = 1) or rejects (b = 0) the
proof Π.

(EK, VK) are also referred to as the common reference string.

Deﬁnition 2.1 (Soundness). A VC scheme is sound if, for
all sequences {Rλ}λ∈N in {Rλ}λ∈N and for all p.p.t. adver-
saries A, we have

 EK, VK ← KeyGen(1λ, Rλ);

u, Π ← A(EK, VK, Rλ);
Verify(VK, u, Π) ∧ ¬∃w.Rλ(u ; w)

 ≈ 0.

Pr

Online-Oﬄine Veriﬁcation 1 The veriﬁcation algorithm
of many SNARG constructions can be split into oﬄine and
online computations. Speciﬁcally, for many SNARGs, there
exists algorithms (Online, Oﬄine) such that:

Verify(VK, u, Π) = Online(VK, Oﬄine(VK, x), v, Π).

The oﬄine phase can be seen as the computation of one or
more Pedersen-like commitments cx (here, cx = Oﬄine(VK,
x)), some of which may be computed by the prover, and pos-
sibly never opened by the veriﬁer. On their own, such com-
mitments are not perfectly binding, so this involves mod-
elling adversaries that do not output (u, w) but still must
‘know’ the value they are committing to. For such cases, we
require the existence of an algorithm E that can extract x
and w from a verifying proof.

Deﬁnition 2.2 (Online Knowledge Soundness). A VC scheme
is online knowledge sound if, for all sequences {Rλ}λ∈N in
{Rλ}λ∈N and all p.p.t. adversaries A, there exists a p.p.t. ex-
tractor E such that

 EK, VK ← KeyGen(1λ, Rλ);

(cx, v, Π; x, w) ← (A||E)(EK, VK, Rλ);
Online(VK, cx, v, Π) = 1 ∧ ¬∃w.Rλ(x, v; w)

 ≈ 0

Pr

Instantiations of Online-Oﬄine SNARKs Many suc-
cinct veriﬁable-computation constructions [20, 7, 9, 31] can
be presented in a style that make more apparent their re-
liance on commitments on their inputs, outputs, and internal
witnesses. We may instantiate VC using, for example, the
Geppetto construction [19], which explicitly separates (of-
ﬂine) commitments and (online) proofs and provides online
knowledge soundness.
1The oﬄine phase is not to be confused with input-
independent precomputation steps of the veriﬁer in [8, 9].

Instantiation of Oﬄine Veriﬁcation In our work, we
consider schemes where the oﬄine computations consist pure-
ly of multi-exponentiations in G1 over the instance u, fol-
lowed by online computations that accept or reject the proof.
As mentioned above, we consider the case when UR splits
into X, VR. More speciﬁcally, we assume that X = Zn
p and
from X to G1, where the group el-
ements (F1, F2, . . . , Fn) ∈ Gn
1 are part of the keys. The VC
schemes discussed above follow this format.

Oﬄine(VK, x) =(cid:81) F xi

i

3. MULTI-RELATION HASH & PROVE

SCHEMES (HP)

We deﬁne our schemes for eﬃciently decidable relations
R ∈ Rλ, with R ⊂ UR × WR. Recall that we are interested
in producing and verifying arguments that ∃w.R(u ; w) holds
for pairs (u ; w) ∈ R, where u is the instance and w the wit-
ness. The witness can often speed up veriﬁcation by pro-
viding a non-deterministic hint, as veriﬁcation is often more
eﬃcient than computation, notably in the case of relations
for NP complete languages. We keep the witness implicit
when they can be eﬃciently computed from the instance.
As in §2.1, we consider relations where UR splits into X, VR.
A multi-relation hash & prove scheme consists of 5 algo-
rithms HP = (Setup, Hash, KeyGen, Prove, Verify), as follows.
pp ← Setup(1λ) takes the security parameter and generates

the public parameter for the scheme;

σx ← Hash(pp, x) produces a hash given some data x ∈ X;
EKR, VKR ← KeyGen(pp, R) generates evaluation key EKR

and veriﬁcation key VKR given a relation R ∈ Rλ;

ΠR ← Prove(EKR, x, v ; w) produces a proof of R(x, v ; w) given

an instance and a witness that satisfy the relation.

b ← Verify(VKR, σx, v, ΠR) either accepts (b = 1) or rejects
(b = 0) a proof of R given a hash of x and the rest of
its instance v.

Note that hashes of inputs and the keys of a relation can
be computed independently. In particular, σx can be com-
puted ‘oﬄine’, before generating keys, proving, or verifying
instances of relations; and can be shared between all these
operations.
3.1 Adaptive Soundness

We describe our intended security properties for an HP
scheme, distinguishing two cases. We ﬁrst deﬁne adaptive
soundness with multiple relations and public veriﬁability,
then describe a variant with a single relation.

Deﬁnition 3.1 (Adaptive Soundness). A multi-relation hash
& prove scheme HP is adaptively sound if every p.p.t. ad-
versary with access to oracle KEYGEN wins the game below
with negligible probability.

Adaptive Forgery Game
pp ← Setup(1λ)
R, x, v, Π ← AKEYGEN(1λ, pp)
A wins if VERIFY(R, x, v, Π) = 1 and ¬∃w.R(x, v ; w)

KEYGEN(R)
if VK(R) exists, return ⊥
EK, VK ← KeyGen(pp, R)
VK(R) := VK;
return (EK, VK)

VERIFY(R, x, v, Π)
if VK(R) undeﬁned, return 0
σ ← Hash(pp, x)
return Verify(VK(R), σ, v, Π)

1307The designated-veriﬁer variant of adaptive soundness is ob-
tained by having KEYGEN return only EK, and giving A
oracle access to VERIFY. The single-relation variant is ob-
tained by requesting that the adversary calls KEYGEN once.

Informally, adaptive soundness means that an adversary
that interacts with a veriﬁer on any number of chosen in-
stances of relations supported by HP cannot forge any argu-
ment. Although the VERIFY procedure in the experiment
always recomputes σx, this hash can of course be shared be-
tween veriﬁcations of multiple instances that use the same x.
Unfolding the deﬁnition, the single-relation, public veriﬁ-

ability game is deﬁned by

Adaptive Forgery Game (single relation, public veriﬁability)
pp ← Setup(1λ)
R, state ← A0(1λ, pp)
EK, VK ← KeyGen(pp, R)
x, v, Π ← A1(state, EK, VK)
A wins if Verify(VK, Hash(pp, x), v, Π) = 1 and ¬∃w.R(x, v ; w)

This simpler, single-relation game is still adaptive, in the
sense that the relation R can be chosen by A with knowledge
of pp, and the instance x, v can depend on EK, VK. Using a
standard hybrid argument, we conﬁrm that adaptive single-
relation soundness implies adaptive soundness.

Theorem 3.1 (Security of multi-relation HP). A HP scheme
that is -secure as per Deﬁnition 3.1 for a single relation is
q-secure for multiple relations, where q bounds the number
of calls to KEYGEN made by the adversary.
3.2 Accepting Hashes from the Adversary

In the deﬁnition of adaptive soundness, all hash outputs
need to be trusted: at some point, the veriﬁer is given x and
honestly computes its hash σx, or (equivalently) receives σx
from a trusted party. However, there are cases where the ver-
iﬁer may be given σx but not x. As an example, a composite
argument that there exists an intermediate x ∈ X such that
f (z) = x and g(x) = r may consist of z, σx, r, Πf , Πg where
Πf and Πg prove the two functional relations above. Passing
an ‘opaque’ hash σx may be more eﬃcient than passing x,
and may enable the prover to keep x secret. Similarly, one
may see σx as a binding commitment to some x, received
from the adversary, then later used in arguments that dis-
close some of its contents. Deﬁnition 3.1 does not account
for such arguments.

In order for HP to support arguments on hashes provided
by the adversary, we further require that its Hash algorithm
is an extractable collision-resistant hash function. The ex-
tractability property guarantees that σx was indeed pro-
duced by Hash on some input x. The collision-resistance
property guarantees that it is hard to produce two inputs
for which Hash produces the same output.

Deﬁnition 3.2 (Hash Extractability [11]). A hash function
Hash is extractable when, for any p.p.t. adversary A, there
exists a p.p.t. extractor E such that, for a large enough secu-
rity parameter λ and ‘benign’ auxiliary input aux ∈ {0, 1}poly(λ),
the adversary wins the game below with negligible probability.

Hash Extraction Game
pp ← Setup(1λ)
(σ; xe) ← (A(cid:107)E) (pp, aux)
A wins if ∃x.Hash(pp, x) = σ ∧ σ (cid:54)= Hash(pp, xe)

and there is a p.p.t. algorithm Check(pp, σ) that returns 1 if
∃x.Hash(pp, x) = σ and 0 otherwise.
(In the game above, (A(cid:107)E) indicates running both algo-
rithms on the same inputs and random tape, and assigning
their results to σ and to xe, respectively.) In contrast with
the original deﬁnition of [11], we require the existence of
Check so that our veriﬁers can check the well-formedness of
hashes received from the adversary.
Adaptive soundness for HP schemes guarantees collision-
resistance for Hash as long as, for all x0 (cid:54)= x1, there exists
a relation R ∈ Rλ and v ∈ VR to separate them, that is,
∃w.R(x1, v; w)∧¬∃w.R(x0, v; w). On the other hand, adap-
tive soundness does not guarantee that σ is unique, nor does
it exclude adversaries able to forge σ that pass veriﬁcation.
Complementarily, hash extraction enables us to verify ar-
guments that include opaque hashes provided by the adver-
sary by ﬁrst extracting their content then applying adaptive
soundness. To formalize this idea, we complete our deﬁni-
tions with a more generally useful notion of soundness, called
adaptive hash soundness.

At a high level, an adaptively hash sound HP scheme al-
lows us to verify a composite argument whose instances mix
plaintext values x ∈ X and opaque hashes σ ∈ Σ, where Σ
is a ﬁnite set of hashes; importantly, the same σ can occur
in multiple instances. To verify the argument, the veriﬁer
checks each proof using hashes that are either recomputed
from x ∈ X (once for each x, similar to Deﬁnition 3.1), or
checked for well-formedness.

Our main result for this property is that any scheme HP
that is both adaptively sound and hash extractable is also
adaptively hash sound. This result relies on soundness of
HP, provided that one has access to preimages of the hash
values σ ∈ Σ; in turn, this requirement is guaranteed by the
hash extractability property. See the full version for details.

Stronger Security Notions for HP Our security deﬁ-
nitions for HP schemes model adaptive soundness and ex-
tractability of hash inputs, but not extractability of wit-
nesses, i.e., an equivalent of knowledge soundness for HP
schemes. While adaptive soundness is suﬃcient for appli-
cations such as veriﬁable computation in which the input
data is supplied by the veriﬁer, knowledge soundness can be
useful when using HP schemes in larger cryptographic proto-
cols and in applications where the prover also provides some
input. Elaborating such a deﬁnition of knowledge sound-
ness for HP schemes (and proving a construction using it)
raises subtleties related to deﬁning an extractor for an ad-
versary that has adaptive access to the KEYGEN oracle. We
believe this is an interesting direction, which we leave for
future work. Another useful security notion that may be
considered is zero-knowledge, which intuitively guarantees
that proofs do not reveal any non-trivial information about
the witnesses. A zero-knowledge deﬁnition for HP schemes
is provided in the full version of this paper.

3.3 Hash & Prove Scheme via Inner Encoding
In the introduction, we distinguished between two ways
of embedding data representation inside VC schemes:
in-
ner and outer encodings. Here we describe a construction
proposed in [11, 26, 5, 15] which serves as an example of in-
ner encoding. We call this scheme HPinn. The construction
is presented for completeness (to show that it formally ad-

1308The construction uses a keyed, collision-resistant hash scheme

heres our new deﬁnitions), and to facilitate the comparison
with our new constructions of §4.
with domain X, consisting of two algorithms k ← keygen(1λ)
and σ ← hashk(x), together with a succinct argument VC
for a family of relations R(cid:48), deﬁned next.

Intuitively, we check the computation σ = hashk(x) within
the proof system: to argue on a relation R in HPinn, our
construction uses VC on a relation R(cid:48):

(cid:48)

(σx, v ; x, w) = R(x, v ; w) ∧ (σx = hashk(x)).

R

Compared with R, the relation R(cid:48) uses σx instead of x in
the instance, and takes x as an additional witness. (Pre-
sumably, σx is smaller than x and easier to process in proof
veriﬁcations.) We deﬁne HPinn as follows:
Setup(1λ) samples k ← keygen(1λ) and returns k as pp;
Hash(pp, x) computes σx ← hashpp(x);
KeyGen(pp, R) generates (EKR, VKR) ← VC.KeyGen(1λ, R(cid:48));
Prove(EKR, x, v ; w) returns Π ← VC.Prove(EKR, v, σx ; x, w)

for σx = hashpp(x);

Verify(VKR, σx, v, Π) returns VC.Verify(VKR, σx, v, Π).

Theorem 3.2. If VC is knowledge-sound and hash is collision-
resistant, then HPinn is adaptively sound (Deﬁnition 3.1 for
multiple relations).

Hash Extractability. The above construction naturally
extends to extractable hashes, by applying VC to the relation
that checks the hash computation, deﬁned by

Rk(σ ; x) = (σ = hashk(x)).

We write HPE for the resulting scheme, obtained from
HPinn above by extending the Setup and Hash algorithms
and adding a Check algorithm:
SetupE (1λ) samples k ← keygen(1λ); generates EKpp, VKpp ←
VC.KeyGen(pp, Rk); and returns pp = (k, EKpp, VKpp);
HashE (pp, x) computes σx ← hashk(x); Π ← VC.Prove(EKpp,

σx ; x) and returns σ = (σx, Π).

Check(pp, σ) parses σ as (σx, Π) and returns VC.Verify(VKpp,

σx, v, Π).

Theorem 3.3. If VC is knowledge-sound, then HPE is hash
extractable (Deﬁnition 3.2).

The proof of Theorem 3.3 follows from the existence of the
VC extractor.
By using a separate VC scheme on a new relation Rk,
rather than re-using a VC scheme on one of the relations R(cid:48),
we can use knowledge soundness in a completely standard
manner, taking only the key k as ‘benign’ auxiliary input.

Discussion. The HPinn construction is simple, and can be
extended to Merkle trees [15] to provide logarithmic random
access in data structures. Its main practical drawback is that
the relation to be veriﬁed now includes a hash computation,
which adds tens of thousands of cryptographic operations
to the prover’s workload for each block of input when us-
ing standard algorithms such as SHA2 (§6). To lower this
considerable cost for the prover, one pragmatically chooses
custom, algebraic hash functions, which in turn increases
the cost for the veriﬁer that computes the digest.
In the
following sections we present constructions that are eﬃcient
for both the prover and the veriﬁer.

4. HASH & PROVE CONSTRUCTIONS

In this section we present our main technical contribution:
two eﬃcient multi-relation hash & prove schemes for families
of relations Rλ. We let R(x, v ; w) range over these relations.
Our two schemes are obtained via a generic hash & prove
construction that relies on two main building blocks: (i)
any SNARK scheme that has oﬄine/online veriﬁcation al-
gorithms (cf. §2.1) and where the oﬄine veriﬁcation con-
sists of a multi-exponentiation in a group G1; (ii) any HP
scheme that allows to prove the correctness of such multi-
exponentiations.

i Fi

an element cx = (cid:81)

Before presenting our generic construction in full detail,
we provide some intuition. We start from the observation
that in oﬄine/online SNARKs the veriﬁer already computes
xi . Although cx can be seen as a
hash of the input x, such hash is relation-speciﬁc because
the elements Fi depend on the relation R that was used in
the SNARK’s KeyGen. Our main idea is to outsource the
computation of cx to the prover in order to obtain an HP
scheme where x can be hashed in a relation-independent
manner. Then, we ask the prover to show the correct-
ness of cx using an HP scheme (where hashes are indeed
relation-independent) that supports relations of the form

(x, cx) : cx =(cid:81)

i Fi

xi .

Building an HP scheme from another HP scheme may
look silly at ﬁrst, however the key point is that we require
an HP that supports a speciﬁc class of relations: only multi-
exponentiations. Conversely, our method can be seen as a
way to bootstrap, via SNARKs, an HP scheme that supports
one speciﬁc class of computations into another one that can
support arbitrary computations.

Following the generic HP construction from a hash & prove
scheme for multi-exponentiation, we propose new construc-
tions to instantiate the latter. The ﬁrst, called XP1, is pub-
licly veriﬁable, whereas the second one, called XP2, is in
the designated veriﬁer model but enjoys better eﬃciency.
The two new schemes are signiﬁcantly more eﬃcient than
what could be obtained using known techniques (e.g., the
construction based on inner encoding in §3.3).

As a result, the instantiation of our generic construction
with state-of-the-art SNARKs and our new HP for multi-
exponentiation yields an HP system that, compared to the
solution in §3.3, is at least 1, 400× times faster for the prover
and the key generator (cf. §6).
In §4.1
we describe the generic construction; in §4.2 we give our
publicly veriﬁable HP scheme for multi-exponentiation, and
in §4.3 we give the designated veriﬁer one. Finally, in §4.4 we
outline additional properties of our constructions, including
data updates and extension of HP to accumulators.

The rest of the section is organized as follows.

4.1 Generic Hash & Prove Scheme (HPgen)
Let VC = (KeyGen, Prove, Verify) be a SNARG scheme
that supports a sequence of relations {Rλ}λ and that has
oﬄine/online veriﬁcation, as described in §2.1: we assume
that every veriﬁcation key VK of VC includes group elements
xi com-

F1, . . . , Fn ∈ G1 and that Oﬄine(VK, x) = (cid:81)n
p × G1 is in F iﬀ(cid:81)n

putes a commitment cx.
Let XP = (Setup, Hash, KeyGen, Prove, Verify) be an HP
scheme that supports relations F ⊂ U×∅ where u is Zn
p ×G1,
every F ∈ F is deﬁned by a vector F = (F1, . . . , Fn) ∈ Gn
1 ,
and a pair (x, cx) ∈ Zn

xi = cx.

i=1 Fi

i=1 Fi

1309We use XP and VC to construct a scheme HPgen that sup-
ports any combination of relations R(x, v ; w) supported by
VC. The only requirement is that both schemes have com-
patible (or identical) public parameters. Namely, they share
the same bilinear group setting, and the number of inputs
in x, n, should be the same as in XP.

HPgen is deﬁned as follows:

Setup(1λ) runs XP.Setup(1λ) and returns its public param-

eters pp.

Hash(pp, x) returns σx := XP.Hash(pp, x).
KeyGen(pp, R) takes a relation R and runs

EK, VK ← VC.KeyGen(1λ, R);
Let F := (F1, F2, . . . , Fn) be the ‘oﬄine’ elements in VK;
EKF , VKF ← XP.KeyGen(pp, F );
return EKR := (EK, VK, EKF ), VKR := (VK, VKF ).

Prove(EKR, x, v ; w) parses EKR as (EK, VK, EKF ) then runs

cx ← VC.Oﬄine(VK, x);
Π ← VC.Prove(EK, (x, v) ; w);
Φx ← XP.Prove(EKF , x, cx);
return ΠR := (cx, Π, Φx).

Verify(VKR, σx, v, ΠR) parses VKR as (VK, VKF ) and ΠR as

(cx, Π, Φx), and returns
VC.Online(VK, cx, v, Π) ∧ XP.Verify(VKF , σx, cx, Φx).

Hence, proofs ΠR in HPgen carry three representations of x:
its portable hash σx;
its oﬄine relation-speciﬁc commit-
ment cx; and a multi-exponentiation proof Φx that binds
the two. Compared with VC proofs, and using our instan-
tiations of XP described later in this section, the communi-
cation overhead for HPgen proofs is two group elements (or
three if we want hash extractability).

The following theorem states the security of HPgen.

Theorem 4.1. If XP is adaptively sound in the publicly ver-
iﬁable (resp. designated veriﬁer) setting, and VC is sound,
then the HPgen construction in §4.1 is adaptively sound in
the publicly veriﬁable (resp. designated veriﬁer) setting.

The idea is rather simple: any adversary which breaks
HPgen has to either break the security of the underlying VC
scheme, or cheat on the value of cx, thus breaking the secu-
rity of XP. Our proof shows a reduction for each case.

We also give a corollary that essentially says that, by in-
stantiating our generic construction with a hash extractable
XP scheme, we can handle arguments with untrusted hashes.
It follows by construction of HPgen, observing that this scheme
uses the hashing algorithm of XP.

Corollary 4.1. If XP is hash extractable, then the HPgen
construction in §4.1 is also hash extractable.
4.2 Our Publicly Veriﬁable HP Scheme for

Multi-Exponentiation (XP1)

We present our second key technical contribution: a hash
& prove scheme, called XP1, for the class of multi-exponen-
tiation relations F described above.

For clarity, we write Φ instead of Π for restricted proofs.
$←− G1 for i ∈ [1, n] and returns pp =

Setup(1λ) samples Hi

(Gλ, H) where H = (H1, . . . , Hn).

Hash(pp, (x1, . . . , xn)) returns σx ←(cid:81)

i∈[1,n] Hi

xi .

u, V ← g2

KeyGen(pp, F ) samples u, v, w $←− Z∗

p and computes U ←
$←− G1 and com-
w; samples Ri
g2
w for i ∈ [1, n]; and returns
putes Ti ← Hi
EKF = (F, T, R) and VKF = (U, V, W ) where R =
(R1, . . . , Rn) and T = (T1, . . . , Tn).

Prove(EKF , (x1, . . . , xn), cx) computes Tx ←(cid:81)

v, W ← g2
vFi

Rx ←(cid:81)
(Implicitly we require that cx =(cid:81)

i∈[1,n] Ri

xi ; and returns Φx = (Tx, Rx).

xi , though
the cx part of the instance is not used in the compu-
tation of the proof.)

i∈[1,n] Fi

i∈[1,n] Ti

uRi

xi and

Verify(VKF , σx, cx, Φx) parses Φx = (Tx, Rx) and returns

e(Tx, g2) ?= e(σx, U ) e(Rx, V ) e(cx, W ).

The following theorem states that XP1 scheme is secure.

Correctness follows by inspection.

Theorem 4.2 (Adaptive Soundness of XP1). If the Strong
External DDH Assumption holds, then the XP1 scheme above
is adaptively sound (Deﬁnition 3.1 for multiple relations).

Proof Outline. The proof works by considering the case of
a single relation as the extension to multiple relations is
obtained by applying Theorem 3.1.

Below we provide the outline of the security proof via a

sequence of game hops.

Game 0: this is the adaptive soundness game of Deﬁni-

tion 3.1 restricted to a single relation.

Game 1: this is a modiﬁcation of Game 0 as follows. When
answering the (single) KEYGEN(F ) oracle query, the
challenger sets w = γv + δ for random γ, δ $←− Zp (in-
stead of sampling w $←− Zp). Next, when the adversary
returns the proof (x∗, c∗, Φ∗), with Φ∗ = (T ∗, R∗),
x∗
i and ˆc ←
x∗
i . Then, if (T ∗/ ˆT )(ˆc/c∗)δ = 1 the outcome
of the game is changed so that the adversary does not
win.

the challenger computes ˆT ← (cid:81)
(cid:81)

i∈[1,n] Fi

i∈[1,n] Ti

We claim that Game 0 and Game 1 are statistically in-
distinguishable. The intuition is that δ is information
theoretically hidden from the adversary, which implies
that the only event which changes the game’s outcome
happens with negligible probability.

−γ
i

−α
i F

and Ti ← H β

Game 2: this is a modiﬁcation of Game 1 as follows. When
answering the (single) KEYGEN(F ) oracle query, the
challenger sets u = αv + β for random α, β $←− Zp
(instead of sampling u $←− Zp). Second, the challenger
computes Ri ← H
This game is essentially changing the distribution of
the evaluation keys returned to the adversary. The
distribution in this game however is computationally
indistinguishable from the one in Game 1 under the
Strong External DDH (SXDH) assumption. Finally,
once accounted for this game diﬀerence it is possible to
show that any p.p.t. adversary has negligible probabil-
ity of winning in Game 2, under the Flexible co-CDH
assumption (which in turn reduces to SXDH).

i F δ
i .

1310Detailed proofs for the indistinguishability of the three
games as well as a reduction from winning in Game 2 to
breaking Flexible co-CDH are in the full version.

We can make the XP1 construction hash extractable by
adding a knowledge component. The resulting scheme, XPE ,
consists of algorithms KeyGen and Prove from XP1 together
with the following additional algorithms.
SetupE (1λ,F) samples Hi

$←− G1 for i ∈ [1, n] and ω $←− Zp
2 ,{Hi, H ω

HashE (pp, (x1, . . . xn)) computes Ax ← (cid:81)

and returns pp = (Gλ, gω

i }i∈[1,n]).

xi and

i∈[1,n] Hi
ω)xi . Returns σx = (Ax, Bx).

Bx ←(cid:81)

i∈[1,n](Hi

Check(pp, σx) takes g2 and gω
and checks that e(Ax, gω

2 from pp; parses σx as (Ax, Bx);
2 ) ?= e(Bx, g2).

VerifyE (VKF , σx, cx, Φx) returns
e(Tx, g2) ?= e(Ax, U ) e(Rx, V ) e(cx, W ) ∧ Check(pp, σx) ?= 1.

Lemma 4.1 (Hash Extractability of XPE ). If the Bilinear
n-Knowledge of Exponent Assumption holds, then the XPE
scheme above is hash extractable.

Proof. The existence of an extractor for the Bilinear n-Know-
ledge of Exponent Assumption implies the existence of an
extractor for the XPE construction.
4.3 Our Designated Veriﬁer HP Scheme for

Multi-Exponentiation (XP2)

We present another hash & prove scheme for multi-expon-
entiation, called XP2, which works in the designated veriﬁer
setting. XP2 works similarly to XP1 but has the advantage
of requiring one less element in the proof and one less multi-
exponentiation for the prover.
The XP2 scheme works for the same class of relations F
supported by XP1, and the construction is obtained by adapt-
ing a multi-function veriﬁable computation scheme by Fiore
and Gennaro [23], which works for a similar restricted class
of functions, (f1, . . . , fn) ∈ Zn
p . In the full version we deﬁne
XP2 more generically based on homomorphic weak pseudo-
random functions [23]. For simplicity, we describe below the
instantiation of the scheme based on the SXDH assumption.

The scheme XP2 works as follows:

Setup(1λ) samples Hi

$←− G1 for i ∈ [1, n] and returns pp =

(Gλ, H) where H = (H1, . . . , Hn).

Hash(pp, (x1, . . . , xn)) returns σx ←(cid:81)
Prove(EKF , (x1, . . . , xn), cx) computes Φx ← (cid:81)

δHi
for i ∈ [1, n]; and returns EKF = (F, T ), VKF = (δ, k)
where T = (T1, . . . , Tn).

KeyGen(pp, F ) generates δ, k $←− Z∗

p; computes Ti ← Fi

xi ;
(Implicitly we require that cx =
xi , though the cx part of the instance is not

and returns Φx.

i∈[1,n] Hi

i∈[1,n] Ti

(cid:81)

xi .

k

i∈[1,n] Fi

used in the computation of the proof.)
δ · σx

Verify(VKF , σx, cx, Φx) returns Φx

?= cx

k.

Theorem 4.3 (Adaptive Soundness of XP2). If the SXDH
assumption holds in G1, then the XP2 construction above is
adaptively sound (Deﬁnition 3.1 for multiple relations and a
designated veriﬁer).

We outline the intuition behind the proof of the Theo-
rem. The values (H k
i )i are pseudorandom (by SXDH), and
thus so are (Ti)i. After making a hybrid step where their
distribution is changed to random, the value of δ becomes
information-theoretically hidden from the adversary, making
its probability of cheating negligible.

A publicly veriﬁable variant in the generic group.
Interestingly, the above scheme can be modiﬁed to become
publicly veriﬁable as follows: we publish (gδ
2 ) as part of
VKF , and use these elements with a pairing in the veriﬁca-
tion algorithm. The resulting scheme has the advantage of
being more eﬃcient than XP1. As a drawback, we can only
argue its security in the generic group model, and leave this
analysis for the full version of this work.

2, gk

Hash Extractability. We note that we can make the con-
struction XP2 hash extractable by incorporating a knowl-
edge component, in the same way as we show for XP1.
4.4 Additional Properties of Our Instantiation
By plugging XP1 (or XP2) into the generic HPgen construc-
tion of §4.1 we obtain an eﬃcient HP scheme that can handle
any relation supported by the underlying SNARK system.
A useful property of the hash function of both our con-
structions XP1 and XP2 is its (additive) homomorphism, i.e.,
Hash(x1) · Hash(x2) = Hash(x1 + x2). This property turns
out to have several applications, which we summarize below.

i

Incremental hashing for data streaming applications.
The hash of our construction can be computed incremen-
tally as σi ← σi−1 · H xi
(with σ0 = 1). This is particularly
useful in applications where a resource-constrained device
outsources a data stream x1, x2, . . . to a remote server while
keeping locally only a small digest σi computed as above.
Later, at any point, the client will be able to verify a com-
putation on the stream x1, . . . , xi by only using σi. Fur-
thermore, when hash extractability is not needed, the XP1
construction can be modiﬁed by letting Hi = RO(i) where
RO is a hash function that in the security proof is modeled
as a random oracle (we omit a proof for this case which is
straightforward: simply simulate RO(i) as the Hi in the cur-
rent proof). This simple trick allows for constant-size public
parameters and, more interestingly, to work with a poten-
tially unbounded input size n—a feature particularly useful
in streaming scenarios.

σx = (cid:81)

i Hi

Eﬃcient hash updates. Another application of the ho-
momorphic property is eﬃcient hash updates. Given a hash
xi on a vector x = (x1, . . . , xn), one can easily
update the i-th location from xi to x(cid:48)
i. Instead of recom-
puting the hash from scratch (which would require work
linear in n), one simply does a constant-time computation
x(cid:48)
σx(cid:48) = σx · Hi
i−xi . This trick also generalizes to updat-
ing multiple locations in time linear only in the number of
locations that require an update.

ner. For instance, one user computes σx,k =(cid:81)
second user computes σx,(cid:96) =(cid:81)

Multiple data sources. The homomorphic property also
implies that the hash can be computed in a distributed man-
, a
, and then a ver-
iﬁer who receives σx,k and σx,(cid:96) can reconstruct the full digest
on (x1, . . . , x(cid:96)) with a single multiplication. This feature is
useful in those applications where the data is provided by
multiple trusted sources, in which case only small digests

i∈[k+1,(cid:96)] H xi

i∈[1,k] H xi

i

i

1311have to be communicated. (For example, consider training
a machine learning model using diﬀerent datasets.)

Randomizing hash values.
If one of the xi inputs of
the hash is uniformly random in Zp, then the output of
Hash(pp, x) is a uniformly random element in G1. Show-
ing that SNARK systems randomized in this fashion do not
leak anything about their hashed data is less trivial as the
same randomness is reused by σx and the cx values of dif-
ferent relations. This is akin to randomness reuse in El-
Gamal encryption, which is permissible. However, in most
SNARK systems the group elements used for commitment
randomization have structure, precluding a straightforward
reduction to DDH. A detailed analysis of a multi-relation
zero-knowledge property for speciﬁc VC schemes is thus an
interesting open problem.

From hashes to accumulators. Accumulators are often
used as succinct representations of sets that enable fast, lim-
ited, veriﬁable processing. For example, one can eﬃciently
prove and verify arguments on set operations by exploiting
the structure of accumulators [38], with better performance
than by relying on a general-purpose VC scheme. To this
end, we oﬀer schemes that allow one to transition between
proof systems that operate on hashes and accumulators. In
particular, we introduce Accumulate & Prove scheme which
is a variant of HP that operates on accumulators and builds
on HP and XP (to verify that the hash and the accumulator
were computed from same data). See the full version.

5. OUTSOURCING HASH COMPUTATIONS
In our eﬃcient HP constructions of §4, the Hash algorithm
computes a succinct digest σx using one exponentiation for
every element of x. Hence, when using instantiations with
XP1 or XP2, an HPgen veriﬁer that wishes to relate computa-
tions veriﬁed using σx to their actual inputs x must still per-
form |x| exponentiations, or trust some data provider that
associates σx to x. Though the same σx could be used to
verify many computations that involve x, thereby amortiz-
ing the cost of hash computation, we are looking to further
optimize this cost.

In this section, complementarily, we describe a technique
to outsource hash computations to an untrusted party such
that the veriﬁer (or its trusted data provider) only needs
to perform |x| ﬁeld multiplications and one eﬃcient crypto-
graphic hash on x, say SHA2, typically saving two orders of
magnitude.
We present our construction, called HP∗, as a generic ex-
tension of any HP system to which it adds support for ver-
iﬁable outsourcing of hash computations. The main beneﬁt
of this extension is that the veriﬁer does not need to run the
Hash algorithm:
instead, it can upload x to the untrusted
prover; obtain its hash σx together with a proof of hashing
Πh, verify them; and ﬁnally keep σx. Intuitively, the veriﬁer
can then use σx to refer to x as if it had computed it itself.
5.1 Deﬁnition

We deﬁne HP∗ as an extension of a given hash & prove
scheme HP. In particular, the functionality of the trusted
Hash algorithm is supplemented with a pair of new algo-
rithms, HashProve and HashVerify, run respectively by the
untrusted prover and by the veriﬁer. HashProve computes
a hash of data x and augments it with a proof that the

hash is computed correctly (that is, it is computed accord-
ing to Hash algorithm). HashVerify then accepts σx as the
hash if the proof veriﬁes correctly.
Formally, HP∗ is a multi-relation hash & prove scheme
that supports hash outsourcing and consists of 7 algorithms
HP∗ = (Setup, Hash, HashProve, HashVerify, KeyGen, Prove,
Verify). We omit a description of Setup, Hash, KeyGen, Prove
and Verify, as they are deﬁned identically to those in HP (§3).
Πh ← HashProve(pp, x, σx) produces a proof of RHash(x, σx) =
?= Hash(pp, x)) given some data x ∈ X and hash σx;
bh ← HashVerify(vp, x, σx, Πh) either accepts (bh = 1) or re-

(σx

jects (bh = 0) a proof that σx is a hash of data x.

In addition to being a Hash & Prove scheme (i.e., satisfying
adaptive soundness or adaptive hash soundness), HP∗ must
be secure with regards to outsourcing, as deﬁned below.

Deﬁnition 5.1 (Sound Hash Outsourcing). Outsourcing of
HP∗ hash computation is secure if every p.p.t. adversaries
wins the game below only with negligible probability.

Outsourced Hash Game
pp, vp ← Setup(1λ)
x, σ∗
A wins if HashVerify(vp, x, σ∗

x, Πh ← A(1λ, pp, vp)

x, Πh) = 1 and σ∗

x (cid:54)= Hash(pp, x)

This game is similar to the Hash Extraction game, but it
does not involve extraction, as the veriﬁer is given both x
and σ∗
x. (The designated-veriﬁability variant is obtained by
keeping vp private and, instead, giving the adversary oracle
access to HashVerify.)
Hash outsourcing ensures that, when verifying compos-
ite arguments as in adaptive hash sound schemes (cf. §3.2),
one can safely replace calls to Hash with calls to HashVerify.
In particular, with HP∗, an argument can be passed to a
relation either as data x, as a hash σ or as (x, σ∗). Our def-
inition can be trivially satisﬁed by ignoring Πh and setting
HashVerify(pp, x, σ, Πh) = (σ ?= Hash(pp, x)) but of course
we are looking for more eﬃcient constructions.
5.2 Efﬁcient Construction (HP∗)

We build HP∗ out of any hash & prove scheme HP, and
two additional tools: an almost universal hash function h
(recalled below) and a regular hash function H (that will be
modeled as a random oracle).

hα(x) = (cid:80)n

Almost Universal Hash Functions. An -almost uni-
versal hash function h is such that, for all x (cid:54)= x(cid:48) chosen
before h is sampled, we have Prh[h(x) = h(x(cid:48))] ≤  [13].
We will use such functions from Zn
p to Zp, instantiated by
i=1 xiαi−1 and keyed with a random α ∈ Zp.
These functions can be computed as hα(x) = x1 + α(x2 +
. . . α(xn−1 + αxn))) using n additions and n − 1 multipli-
cations by α, which is particularly eﬃcient in veriﬁable-
computation schemes for arithmetic circuits.
Lemma 5.1. hα is (n − 1)/p-almost universal.

iαi−1, that is,(cid:80)n

Proof. Expanding the collision equality, we get(cid:80)n
(cid:80)n
i=1 x(cid:48)

i=1 xiαi−1 =
i)αi−1 = 0. If x (cid:54)= x(cid:48), we
have a non-zero polynomial in α of degree at most n−1, with
at most n − 1 roots, so this equality holds with probability
at most (n − 1)/p.

i=1(xi − x(cid:48)

1312Before delving into the details of the construction, let us
describe its main ideas. The ﬁrst idea is to build HP∗ by ex-
tending any HP with algorithms HashProve and HashVerify
?= Hash(x).
that allow to prove and verify the correctness of σx
Notably, HashVerify must be signiﬁcantly faster than re-
computing Hash(x). To this end, our second idea is to let
HashProve compute a (freshly sampled) universal hash func-
tion hα(x) and generate a proof Πh that links hα(x) to the
correct σx. Then our HashVerify simply checks Πh (in con-
stant time) and recomputes the universal hash hα(x), which
is much faster than the multi-exponentiation Hash. The se-
curity of universal hash functions relies on their input being
chosen before hα is sampled. To this end, we require that
hα depend on the input x by setting α = H(x, σx) where H
is a hash function.
We are now ready to give our HP∗ construction. Let Rh
be the relation deﬁned by Rh(x, α, µ) = (µ ?= hα(x)), and
let H be a hash function. We build HP∗ using any HP that
supports relation Rh and is hash-extractable.

Setup(1λ) runs setup and generates keys for outsourcing h:

pp(cid:48) ← HP.Setup(1λ);
EKh, VKh ← HP.KeyGen(pp(cid:48), Rh);
return pp = (pp(cid:48), EKh) and vp = VKh;

HashProve(pp, x, σx) computes α = H(x, σx); µ = hα(x);

Πh ← HP.Prove(EKh, x, (α, µ)) and returns Πh;

HashVerify(vp, x, σx, Πh) computes α = H(x, σx); µ = hα(x)

and checks HP.Verify(VKh, σx, (α, µ), Πh).

We omit Hash, KeyGen, Prove and Verify algorithms as they
are simply calls to their counterparts in the HP scheme (for
example, HP∗.KeyGen calls HP.KeyGen(pp(cid:48), R)).

We stress that, even if asymptotically our new construc-
tion is not better than the original one (the veriﬁer performs
Θ(n) operations), in practice, the operations performed by
the veriﬁer in HP∗.HashVerify are orders of magnitude faster
than those in HP.Hash.
Discussion. Applying HP∗ to our eﬃcient constructions
of §4 (either public or designated veriﬁer), our proofs now
carry a fourth representation µ = hα(x) of x in addition to
its hash σx, its commitment cx, and a proof Φx. Note that
we rely on extraction only for the witnesses x of the ﬁxed
relation Rh.
To avoid random oracles, we can use an interactive, des-
ignated veriﬁer variant of HP∗, whereby (1) the prover com-
mits to x and σx; (2) the veriﬁer sends a fresh random α;
(3) the prover produces a proof of Rh; (4) the veriﬁer checks
the proof against x and σx, as above.

Security. We ﬁnally state the security of hash outsourcing:

Theorem 5.1. In the random oracle model for H, if hα is
an -almost universal hash function, HP is adaptively sound
and hash extractable in publicly veriﬁable (resp. designated
veriﬁer) setting, then HP∗ is sound for outsourcing of hash
computations as per Deﬁnition 5.1 in publicly veriﬁable (resp.
designated veriﬁer) setting.

We note that all HP constructions in §4 can be made hash
extractable (meeting requirements of Theorem 5.1) and can
be used for secure hash outsourcing.

6. EVALUATION

In this section, we analyze and measure the performance
of our new HP constructions compared to previous solutions.
Our evaluation is twofold. First we analyze the eﬃciency
of our scheme HPgen from §4 (instantiated with Geppetto [19]
and XP1) and we compare it against the inner encoding con-
struction HPinn of §3.3 (also instantiated with Geppetto and
various choices of the hash function). Second, we report
on the impact of our hash outsourcing technique of §5 in
speeding up hashing and veriﬁcation time.
6.1 Microbenchmarks

We performed a series of microbenchmarks on a single
core of a 2.4 GHz Intel Xeon E5-2620 with 32 GB of RAM.
The table below gives the time for individual operations on
the ﬁelds and elliptic curves used by Geppetto. The cost of
multi-exponentiation and for SHA-256 is reported for each
254-bit word of input.

operation
ﬁeld addition
ﬁeld multiplication
multi-exponentiation
pairing
SHA-256

time
45.2 ns
316.7 ns
231.2 µs
0.7 ms
193.6 ns

6.2 Inner vs. Outer Encodings

We compare the asymptotic performance of inner and

outer encodings and summarize the results in Figure 1.

In our evaluation, we make a distinction between diﬀerent
types of veriﬁer eﬀort, depending on whether the veriﬁer’s
input to the computation is passed by value or by reference
via a hash (referred to as an opaque hash for HP schemes
in §3.2). In the ﬁgure, they are denoted as “Verify IO” and
“Verify Intermediate Commitments”, respectively.

When the veriﬁer’s input is passed by value, she (or some-
one she trusts) must directly handle each IO value, so the
cost depends on the size, n, of the IO. Note that for any
particular veriﬁer, such computation is required only once
for a given IO value, as the computed commitment (or hash)
can be reused in subsequent computations.

When a veriﬁer uses IO values passed by reference, she
veriﬁes a proof using a commitment or hash of the IO values
without handling them directly. Since the commitment/hash
values are constant size, the veriﬁcation eﬀort is also con-
stant. A veriﬁer may use IO values passed by reference when
the corresponding hash comes from a trusted source (e.g.,
the veriﬁer herself), or when it represents intermediate val-
ues in a computation (e.g., between mappers and reducers in
a MapReduce computation) where the veriﬁer merely needs
to check the consistency of the IO, rather than the values
themselves.
HPinn. We consider the construction HPinn given in §3.3 in-
stantiated with Geppetto and either SHA-1, SHA-256, or
Ajtai’s [1] hash function. On the positive side, HPinn has the
same number of elements in the proof as Geppetto; its on-
line veriﬁcation cost is the same as in Geppetto, while oﬄine
veriﬁcation consists of one hash computation plus a multi-
exponentiation on a ﬁxed size word. On the negative side,
to support a relation R, HPinn forces Geppetto to work with
a relation R(cid:48) which (on top of encoding R) encodes hash
computations. The latter adds signiﬁcantly to the evalu-
ation key size and the prover’s work, which scale linearly

1313Generality

Verify
proof

Verify
IO

HPinn (Ajtai)
Geppetto [19]
HPgen
HPgen (extract)
HP∗

Yes
No
Yes
Yes
Yes

12 pairings Ajtai(n) + 1 MultiExp
12 pairings
12 pairings
12 pairings
12 pairings

3n MultiExp
n MultiExp
n MultiExp
SHA(n) + n MulAdd +
16 pairings + 6 MultiExp

Verify
Prover
Interm. Commit Eﬀort
1 MultiExp
5 pairings
4 pairings
6 pairings
4 pairings

O(D log D)
O(d log d)
O(d log d) + 2n MultiExp
O(d log d) + 3n MultiExp
O(d log d) + 2n MultiExp
+ UHash(n)

Proof Size
(group elts.)

8
8
10
11
20

Figure 1: Asymptotic Performance. Comparison of our schemes and prior work. For our schemes, we assume the use
of our publicly veriﬁable XP1 scheme, and HP∗ is instantiated with HPgen. We use n for the size of the inputs/outputs (IO),
d (cid:29) n for the degree of the QAP used for the outsourced computation, and D = d + 350n. MultiExp is the cost of a multi-
exponentiation, and MulAdd is the cost of a simple ﬁeld multiplication and addition. Ajtai(x) and SHA(x) is the time needed
to compute an Ajtai (resp. SHA-256) hash on x words of input, and UHash(x) is O(x log x), i.e., the time necessary to compute
and prove correct a universal hash.

and quasilinearly respectively in the number of quadratic
equations needed to represent the computation. Concretely,
Geppetto includes libraries for veriﬁably computing SHA-1
and SHA-256 hashes. For each 254-bit I/O element, these
libraries require approximately 22,400 equations for SHA-1
or 35,000 for SHA-256. Similar libraries for Ajtai require
only 300–400 equations per word of input, but they increase
the cost for the veriﬁer and may not suﬃce for privacy ap-
plications that require stronger randomness properties from
the hash function [15].

Geppetto. Geppetto is an example of an outer encoding
scheme which avoids the expenses incurred by inner encod-
ings. For example, compared with the hundreds or thou-
sands of equations used for inner encodings, Geppetto only
adds one equation per word of input, and hence they report
improving prover performance by two orders of magnitude
for processing I/Os [19]. However, Geppetto’s approach re-
quires the veriﬁer to compute commitments using a multi-
exponentiation (versus a hash in HPinn) that is linear in the
I/O size. Furthermore, Geppetto must specify which com-
putations will be supported at setup time, before data is
selected for said computations.

Our HPgen Scheme. Unlike Geppetto, which ﬁxes at setup
which computations will be supported for committed data,
our HPgen scheme oﬀers full generality;
i.e., data can be
hashed completely independently of the computations to be
performed, and indeed, new and fully general computations
can be veriﬁed over previously hashed data.

HPgen’s new generality comes at a modest computational
cost relative to Geppetto. In terms of communication, HPgen
proofs include two more elements (three with hash extract-
ability); the evaluation key and the veriﬁcation key of ev-
ery relation contain, respectively, 2n and 3 extra elements.
In terms of computation, our prover has to perform two
additional n-way multi-exponentiations. The veriﬁer’s on-
line cost is the same as in Geppetto, whereas oﬄine ver-
iﬁcation requires one hash computation (i.e., one n-multi-
exponentiation) plus four pairings.
If we wish to support
hash extractability, then this adds an additional group ele-
ment to the proof, an additional multi-exponentiation for the
prover, and an additional pairing for the veriﬁer. Overall,
the additional burden (linear in the I/O size n) that HPgen
adds relative to Geppetto is quite small, since both the size
of the evaluation key and the prover’s eﬀort are typically
dominated by the complexity of the outsourced computa-
tion, which, in most applications, is much larger than n.

Compared with inner encodings like HPinn, however, HPgen
saves the prover signiﬁcant eﬀort. Concretely, if we instanti-
ate HPinn with Ajtai’s hash, then HPgen is 1, 400× faster per
I/O word (e.g., for n = 1, 000, HPinn takes 10 minutes while
HPgen takes half a second), while for SHA-256, the diﬀerence
is closer to 140, 000× (e.g., HPinn takes 18 hours).
Our HP∗ Scheme: Outsourcing Hash Computations.
Compared with HPgen, HP∗ drastically improves the veri-
ﬁer’s I/O processing time. For the veriﬁer, whereas HPgen
required a multi-exponentiation linear in the I/O, with HP∗,
the linear costs consist of (1) a symmetric, fast SHA-256
hash computation to compute the key α; and (2) for each
word, n additions and n−1 multiplications over Zp. A con-
servative comparison based on the results from §6.1 shows
that (2) is 654× cheaper per I/O word than a multi-exponen-
tiation, and that (1) using SHA-256 is even cheaper than
(2). Overall, compared with its current I/O processing,
HP∗ thus reduces the linear costs of the Geppetto veriﬁer
by two orders of magnitude. As a concrete example, with
n = 1, 000, 000, HPgen takes 4 minutes to process the I/O,
while HP∗ needs half a second. Compared with Pantry, (2)
takes one multiplication per word, which is also signiﬁcantly
cheaper than computing Ajtai’s algebraic hash function on
each word. An additional beneﬁt of HP∗ is that the veri-
ﬁer’s key becomes constant size (a few group elements for
encoding α and µ) rather than linear in n.
These beneﬁts come at a low cost: HP∗ increases the size
of the proof from 11 to 20 elements. For the prover, the
proof cost increases by just 2n ﬁeld operations and a SHA-
256 hash computation, plus the cost of generating Πh, which
only depends on n and is independent of the overall relation
to be proven.
6.3 Application Performance

To evaluate the impact of our schemes at the application

level, we evaluated them on two applications.

Statistics has a data generator commit to n 64-bit words.
Later, clients can outsource various statistical calculations
on that data; for example, we experiment with computing
K-bucket histograms.

DNA matching creates a commitment to a string of n nu-
cleotides, against which a client can then outsource queries,
such as looking for a match for a length K substring.

The performance results for both applications appear in
Figure 2. As expected, IO veriﬁcation in HPinn is more ef-
ﬁcient compared to the outer encodings schemes. Among
outer encodings, our HP∗ outperforms others as the size of

1314Verify
proof

Verify Prover
Eﬀort

IO

Statistics (n = 256, K = 8)

HPinn (Ajtai)
Geppetto [19]
HPgen
HP∗

17ms
17ms
17ms
17ms

Statistics (n = 1024, K = 8)

HPinn (Ajtai)
Geppetto [19]
HPgen
HP∗

17ms
17ms
17ms
17ms
DNA Search (n = 600, K = 4)
17ms
17ms
17ms
17ms

HPinn (Ajtai)
Geppetto [19]
HPgen
HP∗

0.070ms
1380ms
557ms
31ms

0.3ms
6,267ms
2,096ms
30ms

0.079ms
1611ms
574ms
31ms

DNA Search (n = 60, 000, K = 4)

HPinn (Ajtai)
Geppetto [19]
HPgen
HP∗

17ms
17ms
17ms
17ms

6.4ms
46,980ms
15,636ms
104ms

117s
113s
114s
114s

2,100s
2,084s
2,085s
2,092s

13.64s
5.00s
5.01s
6.07s

1,695s
706s
710s
931s

Figure 2: Application Performance. Comparison of our
schemes and prior work for two example applications.

the input grows and n multi-exponentiations start dominat-
ing the cost of verifying hash outsourcing in HP∗. On the
other hand, the outer encodings schemes are more prover-
friendly.
In particular, the prover’s total eﬀort (IO plus
computation) is 1.02-2.3x higher for HPinn than for HP∗
(note that even though our schemes signiﬁcantly reduce the
prover’s burden for IO, they do not aﬀect the eﬀort for the
computation itself, and hence Amdahl’s law limits the over-
all impact). Finally, the results for HP∗ show that the addi-
tional computation the scheme imposes on the prover pays
oﬀ: veriﬁcation is 18-150x more eﬃcient than for HPgen with
at most a 30% increase in the prover’s eﬀorts.

7. RELATED WORK

Cryptographic proof systems come in a variety of shapes,
with inherent trade-oﬀs between the eﬃciency of their provers
and veriﬁers and the expressiveness of the statements be-
ing proven. One particularly interesting point in the de-
sign space are computationally-sound non-interactive proof
systems, also known as argument systems [14], that can
be veriﬁed faster than by directly checking NP witnesses.
Starting with the work of Micali [36], there has been much
progress [11, 30, 6, 26, 20, 31] leading to succinct non-
interactive argument systems often referred to as SNARKs
or SNARGs, depending on whether they establish knowledge
rather than just existence of the NP witness. Signiﬁcant the-
oretical improvements have been complemented with nearly-
practical general-purpose implementations [40, 7, 19, 9, 47].
As noted in §1 and §3.3, some prior work ﬁts our hash &
prove model with data veriﬁcation embedded via inner and
outer encodings. Here we review other solutions that follow
the outer encoding approach.

In commit & prove schemes [33, 16], one can create a com-
mitment to the data, and use it in multiple proofs. Costello et
al. [19] and implicitly Lipmaa [35] use this idea for veriﬁable
computation to eﬃciently share data between proofs. How-
ever, in this approach all computations have to be ﬁxed be-
fore one creates commitments to data. In other words, one
has to know a-priori which computations will be executed

on the data, which may not be the case in applications like
MapReduce. This issue can be mitigated by ﬁxing a univer-
sal relation, i.e., a relation which contains all relations that
can be executed within a ﬁxed time bound. However, this
generality comes at a performance cost.

Several works by Ben-Sasson et al. investigate how to ef-
ﬁciently build universal relations for predicates described as
random-access machine algorithms [5, 7, 9]. For instance,
they describe a SNARK scheme [9] supporting bounded-
length executions on a universal von Neumann RISC ma-
chine with support for data dependent memory access, but
this generality comes at a cost [19]. To achieve full general-
ity, the bound on the execution length can be removed via
proof bootstrapping [46]. Despite recent improvements and
innovation [8], such bootstrapping is costly.

Memory delegation [18] also models a scenario where one
outsources memory and only later chooses computations (in-
cluding updates) to be executed on it in a veriﬁable way. In
this model, after a preprocessing phase whose cost is linear
in the memory size, the veriﬁer’s work in the online veriﬁca-
tion phase is sublinear in the memory size. In contrast, with
HP schemes the veriﬁer also needs to do linear work once
to hash the input, but then the veriﬁcation cost is constant
with respect to the the input size.

Another possibility to address computation on previously
outsourced data is to use homomorphic message authentica-
tors [4] or signatures [17, 29]. With the former, data is ﬂex-
ibly authenticated when uploaded and then multiple func-
tions can be executed and proved on it. Homomorphic au-
thenticators share the limitation of commit & prove schemes:
the class of computations has to be ﬁxed before the data
can be authenticated. Moreover, homomorphic authentica-
tor constructions that oﬀer more practical eﬃciency [4] work
only for quite restricted classes of computations (low degree
polynomials). The approach based on leveled homomorphic
signatures [29] is more expressive but still very expensive in
practice, as the size of the proof (i.e., evaluated signature)
is polynomial in the depth of the computation’s circuit.

AD-SNARKs [3] provide a functionality similar to ho-
momorphic authenticators, working eﬃciently for arbitrary
computations, but even in their case the set of computations
has to be ﬁxed a priori. As a further restriction, the model
of both homomorphic authenticators and AD-SNARKs re-
quires a secret key for data outsourcing, and it only supports
append-only data uploading (i.e., it does not support chang-
ing the uploaded data). In contrast, the hash & prove model
considered by this work supports delegating computation on
public data, since hashes are publicly computable.

Finally, TRUESET [34] uses a Merkle hash tree over I/O
commitments in a VC scheme to support computations on
a subset of committed inputs (namely, a collection of sets).
While this adds ﬂexibility as to which inputs can be used in
the computation, these inputs still have to be ﬁxed a-priori.

Acknowledgments We thank the reviewers for their in-
sightful comments and suggestions. The research of Dario
Fiore is partially supported by the European Union’s Hori-
zon 2020 Research and Innovation Programme under grant
agreement 688722 (NEXTLEAP), the Spanish Ministry of
Economy under project reference TIN2015-70713-R (DEDE-
TIS) and a Juan de la Cierva fellowship, and by the Madrid
Regional Government under project N-Greens (ref. S2013/ICE-
2731). The research of Esha Ghosh is supported in part by
the National Science Foundation under grant CNS–1525044.

1315[25] Gennaro, R., Gentry, C., Parno, B.: Non-interactive

veriﬁable computing: Outsourcing computation to
untrusted workers. In: CRYPTO (2010)

[26] Gennaro, R., Gentry, C., Parno, B., Raykova, M.:

Quadratic span programs and succinct NIZKs without
PCPs. In: EUROCRYPT (2013)

[27] Goldwasser, S., Kalai, Y.T., Rothblum, G.N.: Delegating

computation: interactive proofs for muggles. In: STOC
(2008)

[28] Goldwasser, S., Micali, S., Rackoﬀ, C.: The knowledge

complexity of interactive proof systems. SIAM J. Comput.
18(1) (1989)

[29] Gorbunov, S., Vaikuntanathan, V., Wichs, D.: Leveled fully

homomorphic signatures from standard lattices. In: STOC
(2015)

[30] Groth, J.: Short pairing-based non-interactive

zero-knowledge arguments. In: ASIACRYPT (2010)

[31] Groth, J.: On the size of pairing-based non-interactive

arguments. In: EUROCRYPT (2016)

[32] Kate, A., Zaverucha, G.M., Goldberg, I.: Constant-size
commitments to polynomials and their applications. In:
ASIACRYPT (2010)

[33] Kilian, J.: Uses of randomness in algorithms and protocols.

PhD thesis, Massachusetts Institute of Technology (1989)
[34] Kosba, A.E., Papadopoulos, D., Papamanthou, C., Sayed,

M.F., Shi, E., Triandopoulos, N.: TRUESET: Faster
veriﬁable set computations. In: USENIX Security (2014)

[35] Lipmaa, H.: Prover-eﬃcient commit-and-prove

zero-knowledge SNARKs. In: AFRICACRYPT (2016)

[36] Micali, S.: Computationally sound proofs. SIAM J.

Comput. 30(4) (2000)

[37] Nguyen, L.: Accumulators from bilinear pairings and

applications. In: IEEE Symposium on S&P (2005)

[38] Papamanthou, C., Tamassia, R., Triandopoulos, N.:

Optimal veriﬁcation of operations on dynamic sets. In:
CRYPTO (2011)

[39] Pape, S.: Authentication in Insecure Environments - Using
Visual Cryptography and Non-Transferable Credentials in
Practise. Springer (2014)

[40] Parno, B., Gentry, C., Howell, J., Raykova, M.: Pinocchio:

Nearly practical veriﬁable computation. In: IEEE
Symposium on S&P (2013)

[41] Parno, B., Raykova, M., Vaikuntanathan, V.: How to

delegate and verify in public: Veriﬁable computation from
attribute-based encryption. In: TCC (2012)

[42] Rial, A., Danezis, G.: Privacy-preserving smart metering.

In: WPES (2011)

[43] Setty, S., McPherson, R., Blumberg, A.J., Walﬁsh, M.:
Making argument systems for outsourced computation
practical (sometimes). In: Proc. ISOC NDSS (2012)

[44] Tamassia, R.: Authenticated data structures. In: ESA

(2003)

[45] Thaler, J., Roberts, M., Mitzenmacher, M., Pﬁster, H.:

Veriﬁable computation with massively parallel interactive
proofs. In: USENIX HotCloud Workshop (2012)

[46] Valiant, P.: Incrementally veriﬁable computation or proofs
of knowledge imply time/space eﬃciency. In: TCC (2008)
[47] Wahby, R.S., Setty, S., Ren, Z., Blumberg, A.J., Walﬁsh,

M.: Eﬃcient RAM and control ﬂow in veriﬁable outsourced
computation. In: Proc. of the ISOC NDSS (Feb 2015)

8. REFERENCES
[1] Ajtai, M.: Generating hard instances of lattice problems

(extended abstract). In: STOC (1996)

[2] Babai, L., Fortnow, L., Levin, L.A., Szegedy, M.: Checking

computations in polylogarithmic time. In: STOC (1991)

[3] Backes, M., Barbosa, M., Fiore, D., Reischuk, R.M.:

ADSNARK: nearly practical and privacy-preserving proofs
on authenticated data. In: IEEE Symposium on S&P
(2015)

[4] Backes, M., Fiore, D., Reischuk, R.M.: Veriﬁable delegation

of computation on outsourced data. In: CCS (2013)

[5] Ben-Sasson, E., Chiesa, A., Genkin, D., Tromer, E.: Fast
reductions from RAMs to delegatable succinct constraint
satisfaction problems: extended abstract. In: ITCS (2013)

[6] Ben-Sasson, E., Chiesa, A., Genkin, D., Tromer, E.: On the

concrete eﬃciency of probabilistically-checkable proofs. In:
STOC (2013)

[7] Ben-Sasson, E., Chiesa, A., Genkin, D., Tromer, E., Virza,

M.: SNARKs for C: verifying program executions
succinctly and in zero knowledge. In: CRYPTO (2013)

[8] Ben-Sasson, E., Chiesa, A., Tromer, E., Virza, M.: Scalable

zero knowledge via cycles of elliptic curves. In: CRYPTO
(2014)

[9] Ben-Sasson, E., Chiesa, A., Tromer, E., Virza, M.: Succinct

non-interactive zero knowledge for a von Neumann
architecture. In: USENIX Security (2014)

[10] Bitansky, N., Canetti, R., Chiesa, A., Goldwasser, S., Lin,

H., Rubinstein, A., Tromer, E.: The hunting of the
SNARK. Cryptology ePrint Archive, Report 2014/580

[11] Bitansky, N., Canetti, R., Chiesa, A., Tromer, E.: From

extractable collision resistance to succinct non-interactive
arguments of knowledge, and back again. In: ITCS (2012)
[12] Bitansky, N., Canetti, R., Paneth, O., Rosen, A.: On the

existence of extractable one-way functions. In: STOC
(2014)

[13] Black, J., Halevi, S., Krawczyk, H., Krovetz, T., Rogaway,

P.: Umac: Fast and secure message authentication. In:
crypto. vol. 1666 (1999)

[14] Brassard, G., Chaum, D., Cr´epeau, C.: Minimum disclosure

proofs of knowledge. J. Comput. Syst. Sci. 37(2) (1988)
[15] Braun, B., Feldman, A.J., Ren, Z., Setty, S., Blumberg,

A.J., Walﬁsh, M.: Verifying computations with state. In:
Proc. of the ACM SOSP (2013)

[16] Canetti, R., Lindell, Y., Ostrovsky, R., Sahai, A.:

Universally composable two-party and multi-party secure
computation. In: STOC (2002)

[17] Catalano, D., Fiore, D., Warinschi, B.: Homomorphic

signatures with eﬃcient veriﬁcation for polynomial
functions. In: CRYPTO (2014)

[18] Chung, K.M., Kalai, Y.T., Liu, F.H., Raz, R.: Memory

delegation. In: CRYPTO. pp. 151–165 (2011)

[19] Costello, C., Fournet, C., Howell, J., Kohlweiss, M.,

Kreuter, B., Naehrig, M., Parno, B., Zahur, S.: Geppetto:
Versatile veriﬁable computation. In: IEEE Symposium on
S&P (2015)

[20] Danezis, G., Fournet, C., Groth, J., Kohlweiss, M.: Square

span programs with applications to succinct NIZK
arguments. In: ASIACRYPT (2014)

[21] Devanbu, P.T., Gertz, M., Martel, C.U., Stubblebine, S.G.:

Authentic third-party data publication. In: IFIP TC11/
WG11.3 (2001)

[22] Fiore, D., Fournet, C., Gosh, E., Kohlweiss, M.,

Ohrimenko, O., Parno, B.: Hash ﬁrst, argue later:
Adaptive veriﬁable computations on outsourced data.
Cryptology ePrint Archive (2016)

[23] Fiore, D., Gennaro, R.: Publicly veriﬁable delegation of

large polynomials and matrix computations, with
applications. In: CCS (2012)

[24] Fournet, C., Kohlweiss, M., Danezis, G., Luo, Z.: ZQL: A

compiler for privacy-preserving data processing. In:
USENIX Security (2013)

1316