Faster Secure Computation through  

Automatic Parallelization

Niklas Buescher and Stefan Katzenbeisser, Technische Universität Darmstadt

https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/buescher

This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-931971-232Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXFaster Secure Computation through Automatic Parallelization

Niklas Buescher

Technische Universit¨at Darmstadt

Stefan Katzenbeisser

Technische Universit¨at Darmstadt

Abstract
Secure two-party computation (TPC) based on Yao’s gar-
bled circuits has seen a lot of progress over the past
decade. Yet, compared with generic computation, TPC
is still multiple orders of magnitude slower. To improve
the efﬁciency of secure computation based on Yao’s
protocol, we propose a practical parallelization scheme.
Its advances over existing parallelization approaches are
twofold. First, we present a compiler that detects paral-
lelism at the source code level and automatically trans-
forms C code into parallel circuits. Second, by switching
the roles of circuit generator and evaluator between both
computing parties in the semi-honest model, our scheme
makes better use of computation and network resources.
This inter-party parallelization approach leads to signiﬁ-
cant efﬁciency increases already on single-core hardware
without compromising security. Multiple implementa-
tions illustrate the practicality of our approach. For in-
stance, we report speed-ups of up to 2.18 on 2 cores and
4.36 on 4 cores for the example application of parallel
modular exponentiation.

1

Introduction

In the thirty years since Yao’s seminal paper [34], Secure
Multiparty Computation (MPC) and Secure Two-Party
Computation (TPC) have transitioned from purely theo-
retic constructions to practical tools. In TPC, two parties
jointly evaluate a function f over two inputs x and y pro-
vided by the parties in such a way that each party keeps
its input unknown to the other. TPC enables the con-
struction of privacy-enhancing technologies which pro-
tect sensitive data during processing steps in untrusted
environments.

Many privacy enhancing implementations use the ap-
proach of “garbled circuits” introduced by Yao in the
1980s, where f is transformed into a Boolean circuit Cf
and encrypted in a special way. Beginning with the real-

ization of the ﬁrst practical implementation of Yao’s pro-
tocol by Fairplay in 2004 [27], theoretical and practical
advances, including Garbled-row-reduction [31], free-
XOR [21], garbling from ﬁxed-key blockciphers [5] and
others have led to a signiﬁcant speed-up of Yao’s original
protocol. Furthermore, high-level language compilers
[14, 22, 23] that demonstrate the usability of Yao’s pro-
tocol have been developed. Nowadays, millions of gates
can be garbled on off-the-shelf hardware within seconds.
Nonetheless, compared with generic computation, Yao’s
garbled circuits protocol is still multiple orders of mag-
nitudes slower. Even worse, recently Zahur et al. [36]
indicated that the information theoretic lower bound on
the number of ciphertexts for gate-by-gate garbling tech-
niques has been reached. Hence, further simpliﬁcation
of computations is unlikely.

Observing the ongoing trend towards parallel hard-
ware, e.g., many-core architectures on a single chip, we
investigate whether parallelism within Yao’s protocol tar-
geting security against semi-honest adversaries can be
exploited to further enhance its performance. To the
best of our knowledge, all previous parallelization efforts
have focused on Yao’s protocol secure against malicious
adversaries, which is easily parallelizable by “design”, or
explored the parallelization possibilities of semi-honest
Yao to only limited extent (see § 2) by using manually
annotated parallism in handcrafted circuits.
Therefore, in this paper we systematically look at three
different levels of automatic parallelization that have the
potential to signiﬁcantly speed up applications based on
secure computation:
Fine-grained parallelization (FGP). As the ﬁrst step,
we observe that independent gates, i.e., gates that do not
provide input to each other, can be garbled and evaluated
in parallel. Therefore, a straight forward parallelization
approach is to garble gates in parallel that are located at
the same circuit depth, because these are guaranteed to be
independent. We refer to this approach as ﬁne-grained
parallelization (FGP) and show that this approach can

USENIX Association  

24th USENIX Security Symposium  531

be efﬁcient for circuits of suitable shape. For example,
when securely computing a matrix-vector multiplication
with 3 million gates, we report a speed-up of 2.4 on 4
cores and 7.5 on 16 cores. Nevertheless, the achievable
speed-up heavily depends on circuit properties such as
the average circuit width, which can be comparably low
even for larger problems when compiling from a high-
level language, as we show.
Coarse-grained parallelization (CGP). To overcome
the limitations of FGP for inadequately shaped circuits,
we make use of high-level circuit descriptions, such as
program blocks, to automatically detect larger coherent
clusters of gates that can be garbled independently. We
refer to this parallelization as coarse-grained paralleliza-
tion (CGP). As one of our main contributions, we ex-
tend the CBMC-GC compiler of Holzer et al. [14], which
translates functionalities described in ANSI-C into cir-
cuits, with the capability to detect concurrency at the
source code level. This enables the compilation of par-
allel circuits. Hence, one large circuit is automatically
divided into multiple smaller, independently executable
circuits. We show that these circuits lead to more scal-
able and faster execution on parallel hardware. For
example, the matrix-vector multiplication circuit, men-
tioned above, scales with a speed-up of 3.9 on 4 cores
and a speed-up of 12 on 16 cores, thus, signiﬁcantly out-
performing FGP. Furthermore, integrating automatic de-
tection of parallel regions into a circuit compiler gives
potential users the opportunity to exploit parallelism
without knowledge of the internals of Yao’s garbled cir-
cuits and relieves them of writing parallel circuits.
Inter-party parallelization (IPP). Finally, we present
an extension to Yao’s garbled circuit protocol secure
against semi-honest adversaries to balance the compu-
tation costs of both parties. In the original protocol (us-
ing the defacto standard point-and-permute optimization
[4, 27]), the garbling party has to perform four times the
cryptographic work than the evaluating party. Hence, as-
suming similar computational capabilities the overall ex-
ecution time is dominated by the garbling costs. Given
the identiﬁed coarse-grained parallelism, the idea of our
protocol is to divide the work in a symmetric manner be-
tween both parties by switching the roles of the garbling
and evaluating party to achieve better computational re-
source utilization without compromising security in the
semi-honest model. This approach can greatly reduce the
overall runtime. By combining CGP and IPP, we report a
speed-up over a serial implementation of 2.14 when us-
ing 2 cores and a speed-up of 4.34 when using 4 cores
for the example application of a modular exponentiation.
Summarizing our results, the performance of Yao’s
protocols secure against semi-honest adversaries can sig-
niﬁcantly be improved by using automatic paralleliza-
tion.

Outline. Next, we discuss related work. An introduc-
tion of the used primitives and tools is given in § 3. In
§ 4 we discuss FGP, CGP, and present our parallel cir-
cuit compiler. Moreover, we introduce the IPP protocol
in § 5. In § 6 we evaluate our approaches on practical
example applications.

2 Related Work

We give a short overview on parallelization approaches
for Yao’s garbled circuits in the semi-honest model, be-
fore discussing solutions in the malicious model. Fur-
thermore, we discuss parallel compilation approaches for
multi-party computation.
Semi-honest model. Husted et al. [17] showed a CPU
and GPU parallelization with signiﬁcant speed-ups on
both architectures. Their approach is based on the idea of
integrating an additional encryption layer between every
circuit level to enable efﬁcient ﬁne-grained paralleliza-
tion. However, their approach signiﬁcantly increases the
communication costs by sending one additional cipher-
text per XOR gate. Moreover, bandwidth saving opti-
mizations, such as garbled row reduction, are incompati-
ble. This is undesirable, as network bandwidth is already
a signiﬁcant bottleneck.

Barni et al. [3] proposed a parallelization scheme simi-
lar to ours, which distinguishes between ﬁne- and coarse-
grained parallelism. Their approach showed speed-ups
for two example applications. However, their coarse-
grained approach requires manual user interaction to an-
notate parallelism in handcrafted circuits. Unfortunately,
their timing results are hardly comparable with other
work, due to the missing implementation of concurrent
circuit generation and evaluation, which is required to
garble larger circuits.

Most recently, Nayak et al. [30] presented an orthog-
onal and complementary work to ours. Their framework
GraphSC supports the parallel computation of graph ori-
ented applications using RAM based secure computa-
tion. GraphSC shows very good scalability for data in-
tensive computations. Parallelism has to be annotated
manually and has to follow the Pregel [26] pattern. To
exploit further parallelism within different computing
nodes of GraphSC, the ideas presented in this work could
be exploited.
Malicious model. The “Billion gates” framework by
Kreuter et al. [23] was designed to execute large circuits
on cluster architectures. The framework supports paral-
lelization in the malicious model using message passing
technologies. Frederiksen et al. [11] also addressed the
malicious model, yet they targeted the GPU as execu-
tion environment.
In both cases, the protocol is based
upon the idea of cut-and-choose, which consists of mul-

532  24th USENIX Security Symposium 

USENIX Association

2

tiple independent executions of Yao’s protocol secure
against semi-honest adversaries. This independence en-
ables naive parallelization up to the constant number of
circuits required for cut-and-choose. Unfortunately, this
degree of parallelism cannot be transferred to the semi-
honest setting considered in this paper.
Parallel compiler. Zhang et al. [37] presented a com-
piler for distributed secure computation with applications
for parallelization. Their compiler converts manually an-
notated parallelism in an extension of C into secure im-
plementations. Even so the compiler is targeting MPC
and not TPC, it could be used as an additional front-end
to the ideas presented in this work.

In summary, up to now there is no work that ad-
dresses parallelization of Yao’s protocol in the semi-
honest model without making compromises towards the
communication costs or relying on manually annotated
parallism in handcrafted circuits.

3 Preliminaries

In this section, we give a short introduction into existing
tools and techniques required for our parallelization ap-
proach. First, we give a brief overview of Yao’s protocol
(§ 3.1). Next, we introduce the compiler CBMC-GC that
transfers ANSI-C to garbled circuits (§ 3.2), followed by
an introduction of the Par4all framework that detects par-
allelism on source code level (§ 3.3).
3.1 TPC and Yao’s protocol
In the following paragraphs, we give a short introduc-
tion into Yao’s TPC protocol. For a complete descrip-
tion, we refer the reader to the detailed work of Lindell
and Pinkas [25].
Semi-honest adversary model.
In this work, we use
the semi-honest (passive) adversary model. TPC proto-
cols secure against semi-honest adversaries ensure cor-
rectness and guarantee that the participating parties do
not learn more about the other party’s input than they
could already derive from the observed output of the joint
computation. The semi-honest model is opposed the ma-
licious model, where the adversary is allowed to actively
violate the protocol. TPC protocols in the semi-honest
model are used for many privacy-preserving applications
and are therefore interesting on their own. A discussion
on example applications is given in § 5.3.
Oblivious transfers. An Oblivious transfer protocol
(OT) is a protocol in which a sender transfers one of
multiple messages to a receiver, but it remains oblivious
which piece has been transferred. In this paper, we use
1-out-of-2 OTs, where the sender inputs two l-bit strings
m0,m1 and the receiver inputs a bit c ∈ {0,1}. At the

end of the protocol, the receiver obliviously receives mc
such that neither the sender learns the choice c nor the
receiver learns anything about the other message m1−c.
In 2003 Ishai et al. [18] presented the idea of OT Exten-
sion, which signiﬁcantly reduces the computational costs
of OTs for most interesting applications of TPC. We use
OTn
l to denote a number n of 1-out-of-2 oblivious trans-
fers with message bit length l.
Yao’s protocol. Yao’s garbled circuits protocol, pro-
posed in the 1980s [35], is a TPC protocol secure in the
semi-honest model. The protocol is executed by two par-
ties PA, PB and operates on functionality descriptions in
form of Boolean circuits denoted with Cf . A Boolean cir-
cuit consists of n Boolean gates, two sets of inputs wires
(one for each party), and a set of output wires. A gate is
described by two input wires wl,wr, one output wire wo,
and a Boolean function γ = g(α,β ) mapping two input
bits to one output bit. The output of each gate can be
used as input to multiple subsequent gates.

During protocol execution, one party becomes the cir-
cuit generator (the garbling party), the other the circuit
evaluator. The generator initializes the protocol by as-
signing each wire wi in the circuit two random labels w0
i
and w1
i of length κ (the security parameter) representing
the respective values 0 and 1. For each gate the genera-
tor computes a garbled truth table. Each table consists of
four encrypted entries of the output wire labels wγ
o. These
are encrypted according to the gate’s Boolean functional-
ity using the input wire labels wα
r as keys. Thus,
an entry in the table is encrypted as
(wg(α,β )

l and wβ

)).

Ewα
l

(Ewβ

r

o

After their creation, the garbled tables are randomly per-
muted and sent to the evaluator, who, so far, is unable
to decrypt a single row of any garbled table due to the
random choice of wire labels.

To initiate the circuit evaluation, the generator sends
its input bits x in form of input wire labels to the evalu-
ator. Moreover, the evaluator’s input y is transferred via
an OT m
κ protocol with the generator being the OT sender
and m being the number of input bits. After the OT step,
the evaluator is in possession of the garbled circuit and
one input label per input wire. With this information the
evaluator is able to iteratively decrypt the circuit from
input wires to output wires. Once all gates are evalu-
ated, all output wire labels are known to the evaluator. In
the last step of the protocol, the generator sends an out-
put description table (ODT) to the evaluator, containing
a mapping between output label and actual bit value. The
decrypted output is then shared with the generator.
Optimizations. Yao’s original protocol has seen multi-
ple optimizations in the recent past. Most important are
pipe-lining [15], which is nescessary for the evaluation

USENIX Association  

24th USENIX Security Symposium  533

3

of larger circuits and a faster online execution of Yao’s
protocol, garbled-row-reduction (GRR) [31], which re-
duces the number of ciphertexts that are needed to be
transferred per gate, and free-XOR [21], which allows
to evaluate linear gates (XOR/XNOR) essentially for
“free” without any encryption or communication costs.
Most recently, Zahur et al. [36] presented an communi-
cation optimal garbling scheme, which only requires two
ciphertexts per non-linear gate while being compatible
with free-XOR.

3.2 CBMC-GC
In 2012, Holzer et al. [14] presented the ﬁrst compiler for
a large subset of C to garbled circuits, named CBMC-
GC. The compiler unrolls all loops and recursive state-
ments present in the input program up to a given or stati-
cally determined bound. Afterwards, each statement is
transformed to a Boolean formula preserving the bit-
precise semantics of C. The Boolean formula is then
translated into a circuit, which is optimized for Yao’s gar-
bled circuits [10].

The only difference between C code and code for TPC
is a special naming convention introduced by CBMC-
GC. Listing 1 shows example source code for the mil-
lionaires’ problem. The shown procedure is a standard
C procedure, where only the input and output variables
are speciﬁcally marked as designated input of party PA
or PB (Lines 2 and 3) or as output (Line 4). Aside from
this naming convention, arbitrary C computations are al-
lowed to produce the desired result, in this case a simple
comparison (Line 5).

1 void m i l l i o n a i r e s () {
2

int I N P U T _ A _ i n c o m e ;
int I N P U T _ B _ i n c o m e ;
int O U T P U T _ r e s u l t = 0;

3

4

5

if ( I N P U T _ A _ i n c o m e > I N P U T _ B _ i n c o m e )

O U T P U T _ r e s u l t = 1;

6
7 }
Listing 1: CBMC-GC Code for Yao’s Millionaires’ Problem.

either annotates or exports parallel regions. Annota-
tions are realized with the OpenMP language, parallel
executable kernels for Cuda/OpenCL are exported using
static code analysis techniques. In this work, we mainly
build upon the OpenMP output.

4 Parallelizing of Yao’s Garbled Circuits

To exploit parallelism in Yao’s protocol, groups of gates
that can be garbled independently need to be identiﬁed.
Independent gates can be garbled in parallel by the gen-
erator, as well as evaluated in parallel by the evaluator.
However, detecting independent, similar sized groups of
gates is known as the NP-hard graph partitioning prob-
lem [28]. The common approach to circumvent the ex-
pensive search for an optimal solution is to use heuris-
tics. In this section, we ﬁrst discuss sequential and paral-
lel composition of functionalities (§ 4.1) and show how
circuits can be garbled in parallel (§ 4.2), before intro-
ducing the ﬁne-grained parallelization heuristic (§ 4.3)
and the coarse-grained parallelization heuristic (§ 4.4).

this paper, we

4.1 Parallel and sequential decomposition
functionali-
Throughout
ties f (x,y) with two input bit strings x, y and an output
bit string o. Furthermore, we use Cf
to denote the
circuit that represents functionality f . We refer to a
functionality f as sequentially decomposable into sub
functionalities f1 and f2 iff f (x,y) = f2( f1(x,y),x,y).

consider

length,

Moreover, we consider a functionality f (x,y) as
parallel decomposable into sub functionalities f1(x,y)
and f2(x,y) with non-zero output bit
if a
bit string permutation σ f exists such that
f (x,y) =
σ f ( f1(x,y)|| f2(x,y)), where || donates a bitwise concate-
nation operator. Thus, functionality f can directly be
evaluated by independent evaluation of f1 and f2. We
note that f1 and f2 do not necessarily have to be deﬁned
over all bits of x and y. Depending on f they could share
none, some, or all input bits.

3.3 Automatic source code parallelization
In 2012, Amini et al. [1] presented Par4all, an automatic
parallelizing and optimizing compiler for C. It was de-
veloped to integrate several compilation tools into one
single powerful compiler. Par4all is based on the Pips [6]
source-to-source compiler infrastructure that detects par-
allelism and uses the POCC [32] polyhedral loop opti-
mizer to perform memory access optimizations. Par4all
is capable of producing parallel OpenMP [7], Cuda and
OpenCL code. Par4all operates on any ANSI-C code
as input, automatically detects parallel control ﬂow and

We use the operator (cid:29) to express a parallel composi-
tion of two functionalities through the existence of a per-
mutation σ. Thus, we write f (x,y) = f1(x,y, )(cid:29) f2(x,y)
if there exists a permutation σ f such that
f (x,y) =
σ f ( f1(x,y)|| f2(x,y)).
We call a parallelization of f to be efﬁcient if the cir-
cuit size (i.e., number of gates) of the parallelized func-
tionality is roughly equal to the circuit size of the se-
quential functionality: size(Cf ) ≈ size(Cf1) + size(Cf2).
Due to the different garbling methods for linear and non-
linear gates in Yao’s protocol using the free-XOR tech-
nique, size(Cf ) is better measured by the number of non-
linear gates. Furthermore, we refer to a parallelization as

534  24th USENIX Security Symposium 

USENIX Association

4

symmetric if sub functionalities have almost equal circuit
sizes: size(Cf1) ≈ size(Cf2).
Finally, we refer to functionalities that can be de-
composed into a sequential and a parallel part as mixed
functionalities. For example the functionality f (x,y) =
f3( f1(x,y) (cid:30) f2(x,y),x,y) can ﬁrst be decomposed se-
quentially in f3 and f1 (cid:30) f2, where the latter part can then
be further decomposed in f1 and f2.
Without an explicit deﬁnition, we note that all deﬁni-
tions can be extended from the dual case f1 and f2 to the
general case f1, f2, . . . , fn.

4.2 Parallel circuit creation and evaluation
A circuit that consists of annotated sequential and paral-
lel parts can be garbled in parallel as follows. Sequential
regions of a circuit can be garbled using standard tech-
niques by iterating topologically over all gates. Once a
parallel decomposable region of a circuit is reached, par-
allelization is applied. All independent sub circuits in
every parallel region can be garbled in any order by any
available thread (see Figure 1). We note that the gar-
bling order has no impact on the security [25]. After
every parallel region a synchronization between the dif-
ferent threads is needed to guarantee that all wire labels
for the next region of the circuit are computed. Multiple
subsequent parallel regions with different degrees of par-
allelism can be garbled, when ensuring synchronization
in-between.

The circuit evaluation can be parallelized in the same
manner. Sequential regions are computed sequentially,
parallel regions are computed in parallel by different
threads. After every parallel region a thread synchroniza-
tion is required to ensure data consistency.

When using pipe-lining the garbled tables have to be
transmitted in an identiﬁable order to ensure data con-
sistency between generator and evaluator. We propose
three different variants. First, all garbled tables can be
enriched with a numbering, e.g., an index, which allows
a unordered transfer to the evaluator. The evaluator is
then able to reconstruct the original order based on the
introduced numbering. This approach has the disadvan-
tage of an increased communication cost. The second
approach is that garbled tables are sent in a synchronized
and predeﬁned order. This approach functions without
additional communication, yet can lead to an undesir-
able ‘pulsed’ communication pattern. The third approach
functions by strictly separating the communication chan-
nels for every sub circuit. This can either be realized by
multiplexing within the TPC framework or by exploiting
the capabilities of the underlying operating system. Due
to the aforementioned reasons, our implementation of a
parallel framework (presented in § 6.1) builds upon the
latter approach.

Figure 1: Interaction between a parallel circuit generator and
evaluator. The layer n of the presented circuit is garbled and
evaluated in parallel. The independent partitions of the circuit
can be garbled and evaluated by different threads in any order.

4.3 Fine-grained parallelism
A ﬁrst heuristic to decompose a circuit into parallel
parts is the ﬁne-grained gate level approach, described
in the following. Similar to the evaluation of a stan-
dard Boolean circuit, gates in garbled circuits are pro-
cessed in topological execution order. Gates provide in-
put to other gates and hence, can be ordered by the circuit
level (depth) when all their inputs are ready or the level
when their output is required for succeeding gates. Con-
sequently and as proposed by others [3, 17], gates on the
same level can be garbled in parallel. Thus, a circuit is
sequentially decomposable into different levels and each
level is further decomposable in parallel with a granular-
ity up to the number of gates. Figure 2 illustrates ﬁne-
grained decomposition of a circuit into three levels L1,
L2 and L3.

To achieve an efﬁcient distribution of gates onto
threads during protocol execution, it is useful to iden-
tify the circuit levels during the circuit compilation pro-
cess. Furthermore, a reasonable heuristic to symmetri-
cally distribute the workload onto all threads when using
the free-XOR optimization is to divide linear and non-
linear gates independently. Hence, each thread gets as-
signed the same number of linear and non-linear gates
to garble. Therefore, we extended the circuit compiler
CBMC-GC with the capability to mark levels and to
strictly separate linear from non-linear gates within each
level. This information is stored in the circuit descrip-
tion.
Overhead.
In practice, multi-threading introduces an
computational overhead to enable thread management
and thread synchronization. Therefore, it is useful to ex-
perimentally determine a system dependent threshold τ
that describes the minimal number of gates that are re-
quired per level to proﬁt from parallel execution. In prac-
tical settings (see § 6) we observe that at least ∼ 8 non-

USENIX Association  

24th USENIX Security Symposium  535

5

CPUcircuit generatorcircuit evaluatorsync. transfer of labelsparallelgatesnn - 1sync.networkCPUCPUCPUCPUmultiple-data paradigm (SIMD), only one sub circuit per
parallel region is compiled, which reduces the circuit
storage costs. During protocol runtime, the sub circuit
is unrolled and garbled in full extent. The global and sub
circuits are interconnected by explicitly deﬁning inner
input and output wires. These are not exposed as TPC
inputs or outputs, but have to be used by TPC frame-
works to recompose the complete and parallel executable
circuit. The compilation process itself consists of four
different steps:
(1) In the ﬁrst step, parallelism in C code is detected by
Par4all and annotated using the OpenMP notation and
source-to-source transformations.
(2) The annotated C code is parsed by ParCC in the
second step. The source code is decomposed using
source-to-source techniques into a global sequentially
executable part, which is interrupted by one or multiple
parallel executable sub parts. Additionally, functional
OpenMP annotations, such as reduction statements, are
replaced with C code that is compiled into the circuits.
Furthermore, information about the degree of detected
parallelism as well as the interconnection between the
global and sub parts is extracted for later compilation
steps.
(3) Given the decomposed source code, the different
parts are compiled independently with CBMC-GC.
(4) In the ﬁnal step information about the mapping of
wires between gates in the global and the sub circuits is
exported for use in TPC frameworks. For performance
reasons, we distinguish static wires that are shared be-
tween parallel sub circuits and wires that are dedicated
for each individual sub circuit.
Example. To illustrate the functionality of ParCC, we
discuss the source-to-source compilation steps on a small
fork and join task, namely the dot product of two vectors
a and b:

r = a· b = a0 · b0 +··· + an · bn.

The source code of the function dot product() is pre-
sented in Listing 2.

1 int mult ( int a , int b ) {
2
3 }

return a * b ;

4 void d o t _ p r o d u c t () {
5

int I N P U T _ A _ a [100] , I N P U T _ B _ b [100];
int res = 0;

6

7

8

9

for ( i = 0; i < 100; i ++)

res += mult ( I N P U T _ A _ a [ i ] , \

I N P U T _ B _ b [ i ]);

int O U T P U T _ r e s = res ;

10
11 }
Listing 2: Dot vector product written in C with CBMC-GC
input/output notation.

Figure 2: Circuit decomposition. Each level L1, L2 and L3
consists of multiple gates that can be garbled using FGP with
synchronization in-between. The circuit can also be decom-
posed in two coarse-grained partitions P1 and P2.

linear gates per core are required to observe ﬁrst speed-
ups. Achieving a parallelization efﬁciency of 90%, i.e,
a speed up of 1.8 on 2 cores, requires at least 512 non-
linear gates per core. In the next section we present an
approach that overcomes the limitations of FGP.

4.4 Coarse-grained parallelism
Another useful heuristic to partition a circuit is the us-
age of high-level functionality descriptions. Given a cir-
cuit description in a high-level language, parallelizable
regions of the code can be identiﬁed using programming
language techniques. These detected code regions can
then be tracked during the circuit compilation process
to produce parallel decomposable circuits. The differ-
ent sub circuits are guaranteed to be independent of each
other and therefore can be garbled in parallel. We re-
fer to this parallelization scheme as coarse-grained par-
allelization (CGP). Figure 2 illustrates an example de-
composition in two coarse-grained partitions P1 and P2.
Furthermore, we note that FGP and CGP can be com-
bined by utilizing FGP within all coarse partations. In
the following paragraphs, we introduce our CBMC-GC
compiler extension that automatically produces coarse-
grained parallel circuits.

Compiler for parallel circuits. Our parallel circuit
complier ParCC extends the CBMC-GC compiler and
builds on top of the Par4all compiler introduced in § 3.
ParCC takes C code as input, which carries annotations
according the CBMC-GC notation. Hence, TPC input
and output variables of both parties are marked as such.
ParCC detects parallelism within this code with the help
of Par4all and produces one global circuit that is inter-
rupted by one or multiple sub circuits for every parallel
region. If a parallel region follows the single-instruction-

536  24th USENIX Security Symposium 

USENIX Association

6

&&&≥&≥&&≥&&≥≥&≥&≥&≥P2P1L2L1L3Input wiresOutput wiresIn this example code, two parties provide input for
two vectors in form of constant
length integer ar-
rays (Line 5). A loop iterates pairwise over all ar-
ray elements (Line 7), multiplies the elements and
aggregates the result.
In the ﬁrst compilation step,
Par4all detects the parallelism in the loop body and
annotates this parallel region accordingly. Therefore,
Par4all adds the statement #pragma omp parallel
for reduction(+:res) before the for loop in Line 7.
ParCC parses the annotations in the second compila-
tion step to export the loop body, in this case the sub
function mult(), printed in Listing 3.

1 void mult ( int INPUT_A_a , int INPUT_A_b ,
2
3 {
4

int O U T P U T _ r e t u r n )

int a = I N P U T _ A _ a ;
int b = I N P U T _ A _ b ;

5

O U T P U T _ r e t u r n = a * b ;

6
7 }
Listing 3: Exported sub function with CBMC-GC input-
output notation.

The functions arguments are rewritten according the no-
tation of CBMC-GC. Thus, the two arguments a and
b of mult() become inner inputs of the sub circuit,
and the return statement becomes an inner output vari-
able. Note, that during the protocol execution all inner
wires are not assigned to any party, instead they con-
nect global and sub circuits. Yet, to keep compatibility
with CBMC-GC a concrete assignment for the party PA
is speciﬁed. The later exported mapping information is
used to distinguish between inner wires and actual input
wires of both parties. In the same step, the global func-
tion dot product(), printed in Listing 4, is transformed
by ParCC to replace and unroll the loop.

1 void d o t _ p r o d u c t () {
2

int I N P U T _ A _ a [100] , I N P U T _ B _ b [100];
int res = 0;

3

4

5

6

7

8

9

10

11

12

13

int O U T P U T _ S U B _ a [100];
int O U T P U T _ S U B _ b [100];
int i ;
for ( i = 0; i <= 99; i ++) {

O U T P U T _ S U B _ a [ i ] = I N P U T _ A _ a [ i ];
O U T P U T _ S U B _ b [ i ] = I N P U T _ B _ b [ i ];

}

int I N P U T _ A _ S U B _ r e s [100];
for ( i = 0; i <= 99; i ++)

res += I N P U T _ A _ S U B _ r e s [ i ];

int O U T P U T _ r e s = res ;

14
15 }
Listing 4: Rewritten dot product(). The loop has been re-
placed by inner input/output variables (marked with SUB).

Therefore,

the two input arrays INPUT A a and
INPUT B b are exposed as inner output variables begin-
ning in Line 4. Therefore, two new output arrays using
CBMC-GC notation are added based on the statically de-

termined information about parallel variables. Further-
more, an inner input array for the intermediate results is
introduced in Line 11. Finally, the reduction statement is
substituted by synthesized additions over all intermediate
results in Line 13.

In the third and fourth compilation step, the two cir-
cuits are compiled and the mapping of wires between
global and sub circuits is exported.

5

Inter-Party Parallelization (IPP)

In this section, we describe a novel protocol extension
to Yao’s protocol to balance computation between par-
ties, assuming symmetric efﬁciently parallelizable func-
tionalities. We refer to this protocol extension as inter-
party parallelization (IPP). Without compromising secu-
rity, we show in § 6 that the protocol runtime can be re-
duced in practical applications when using IPP. This is
also the case when using only one CPU core per party.

We recap the initial motivation: The computational
costs that each party has to invest in semi-honest Yao
is driven by the encryption and decryption costs of the
garbled tables as well as the communication costs. Con-
sidering the garbling technique with the least number of
cryptographic operations, namely GRR combined with
free-XOR, the generator has to compute four ciphertexts
per non-linear gate, whereas the evaluator has to compute
only one ciphertext per non-linear gate. When consider-
ing the communication optimal half-gate approach [36],
the generator has to compute four and the evaluator two
ciphertexts per non-linear gate. Assuming two parties
that are equipped with similar computational power, a
better overall resource utilization would be achieved, if
both parties could be equally involved in the computa-
tion process. This can be realized by sharing the roles
generator and evaluator. Consequently, the overall pro-
tocol runtime could be decreased. Figure 3 illustrates this
efﬁciency gain.

In the following sections we ﬁrst discuss how to ex-
tend Yao’s protocol to use IPP for purely parallel func-
tionalities. In a second step we generalize this approach
by showing how mixed functionalities proﬁt from IPP.

5.1 Parallel functionalities
We assume that two parties PA and PB agree to compute a
functionality f (x,y) with x being PA’s input and y being
PB’s input. Moreover, we assume f (x,y) to be paralleliz-
able into two (or more) sub functionalities f0, . . . , fn:

f (x,y) = f0(x,y)(cid:30) f1(x,y)(cid:30) . . .(cid:30) fn(x,y).

Given such a decomposition, all sub functionalities
can be computed independently with any TPC proto-
cols (secure against semi-honest adversaries) without

USENIX Association  

24th USENIX Security Symposium  537

7

(a) Original

(b) Independence
analysis

(c) Inter-party
Parallelization

Figure 3: The idea and performance gain of IPP visualized.
The OT phase and output sharing are omitted. In Figure 3(a)
the sequential execution of Yao’s protocol is visualized. Given
a parallel decomposition by two circuits representing parallal
program regions as displayed in Figure 3(b), the protocol run-
time can be reduced when sharing the roles generator and eval-
uator, as displayed in Figure 3(c).

any sacriﬁces towards the security [12]. This obser-
vation allows us to run two independent executions of
Yao’s protocol, each for one half of f ’s sub functional-
ities, instead of computing f with a single execution of
Yao’s protocol. Hence, PA could garble one half of f ’s
sub functionalities, for example feven = f0, f2, . . ., and
PA could evaluate the other half fodd = f1, f3, . . .. Vice
versa, PB could evaluate feven and garble fodd. Follow-
ing this approach, PA and PB have to switch their roles
for the OT phase of Yao’s protocol. In the output phase,
both parties share their output labels and description ta-
bles (ODT) with each other.
Analytical performance gain. As discussed, the com-
putational costs for Yao’s protocol are dominated by en-
crypting and decrypting garbled truth tables. Thus, ide-
alizing and highly abstracted, the time spent to perform
a computation ttotal is dominated by the time to garble a
circuit tgarble. Using GRR with free-XOR and assuming
that tgarble is approximately four times the time to evalu-
ate a circuit teval, by symmetrically sharing this task the
total time could be reduced to:
t(cid:31)total ≈
≈ 2.5·teval.
This result
translates to a theoretical speed-up of
ttotal/t(cid:31)total = 4/2.5 = 1.6. When using the half-gate ap-
proach the approximate computational speed-up is 1.33.

(4·teval +teval)

(tgarble +teval)

2

≈

2

5.2 Mixed functionalities
To exploit IPP in mixed functionalities, a protocol ex-
tension is required, allowing to switch from sequential
(dedicated roles) to IPP (shared roles) without violating
the privacy property of TPC. Therefore, we introduce the
notion of transfering roles to securely interchange be-
tween IPP and sequential execution.
Transferring roles We introduce the idea of transfer-
ring roles in two steps. First we sketch an insecure pro-
tocol, which is then made secure in a second step. To

switch the roles of evaluator and generator during execu-
tion, we consider two parties PA, PB and the sequentially
composed functionality f (x,y) = f2( f1(x,y),x,y). In the
following description, f1 is computed using Yao’s proto-
col with PA being generator and PB being evaluator, f2 is
computed with reversed roles.

The transfer protocol begins by computing f1(x,y)
with Yao’s original protocol. Once f1 is computed, the
roles have to be switched. Na¨ıvely, the parties ‘open’
the circuit by interchanging output wires and the ODT.
This reveals the intermediate result o1 = f1(x,y) to both
parties. In the second phase of the protocol, f2 is com-
puted using Yao’s protocol. This time, PA and PB switch
roles, such that PB garbles f2 and PA evaluates f2. The de-
crypted output bits o1 = f1(x,y) are used by PA as input in
the OT protocol. After garbling f2, the output labels and
the ODT are shared between both parties. This proto-
col resembles a pause/continue pattern and preserves cor-
rectness. However, this protocol leaks o1 to both parties,
which violates the privacy requirement of TPC. There-
fore, we propose to use an XOR-blinding during the role
switch. The full protocol is printed below.

Protocol: Transferring Roles
PA and PB agree to securely compute the se-
quentially decomposable functionality f (x,y) =
f2( f1(x,y),x,y) without revealing the intermediate
result f1(x,y) to either party, where x is PA’s input
bit string, y is PB’s input bit string. The protocol
consists of two phases, one per sub functionality.
Phase 1: Secure computation of f1(x,y)

1.

f1 is extended with a XOR blinding for ev-
ery output bit. Thus, the new output o(cid:31)1 =
f (cid:31)1(x,y||yr) = f1(x,y)⊕ yr is calculated by xor-
ing the output of f1 with additional, randomly
drawn input bits by the evaluator of f1.

2. PA and PB securely compute f (cid:31)1 using Yao’s pro-
tocol. We assume PA to be the generator. Addi-
tional randomly drawn input bits are then input
of PB.

3. The blinded output o(cid:31)1 of the secure computa-
tion is only made visible to the generator PA.
This is realized by transmitting the output wire
labels to PA, but not sharing the ODT with PB.

Phase 2: Secure computation of f2(o1,x,y)

1. The circuit representing f2 is extended with
a XOR unblinding for every input bit of o(cid:31)1.
Hence, f (cid:31)2(o(cid:31)1,x,y,yr) = f2(o(cid:31)1 ⊕ yr,x,y).

538  24th USENIX Security Symposium 

USENIX Association

8

PAPBGENEVLtPAPBGENGENtEVLEVLtPAPBGENEVLGENEVL2. PA and PB securely compute f (cid:31)2 using Yao’s pro-
tocol. We assume PB to be the generator. PA
provides the input o(cid:31)1 and PB provides the input
bits for the blinding with yr.

3. The output of the computation is shared with

both parties.

We observe that, informally speaking the protocol pre-
serves privacy, since the intermediate state o1 is shared
securely between both parties. A detailed formal proof
on sequential decomposed functionalities is given by
Hazay and Lindell [12, page 42ff]. Correctness is pre-
served due to blinding and unblinding with the equal bit
string yr:

f (cid:31)2( f (cid:31)1(x,y||yr),x,y,yr) = f (cid:31)2( f1(x,y)⊕ yr,x,y,yr)
= f2( f1(x,y)⊕ yr ⊕ yr,x,y)
= f2( f1(x,y),x,y).

Finally, we note that transferring roles protocol can
further be improved. Demmler et al. [8] presented an
approach to securely share a state of Yao’s protocol that
uses the point-and-permute bits [27] as a blinding. This
approach has equivalent costs in the number of cryp-
tographic operations, yet removes the need of an ODT
transfer. Our implementation uses this optimization.
Transferring roles for mixed functionalities. With
the idea of transferring roles, IPP can be realized for
mixed functionalities. In the following paragraphs, we
show how to switch from IPP to sequential computation.
Switching into the other direction, namely from sequen-
tial to IPP can be realized analogously. With protocols to
switch in both directions, it is possible to garble and eval-
uate any functionality that consists of an arbitrary num-
ber of sequential and parallel regions.

To show the switch from IPP to sequential computa-
tion, we assume a functionality that is sequentially de-
composable into a parallel and a sequential functionality:

f (x,y) = f3( f1(x,y)(cid:28) f2(x,y),x,y).

Note that f1,
f2 and f3 could further be composed of
any sequential and parallel functionalities. We observe
that f3 can be merged with f1 (or f2) into one combined
functionality fc. Thus, f (x,y) can also be decomposed
as f (x,y) = fc( f2(x,y),x,y) with fc being the sequen-
tial composition of f3 and f1. Given such a decomposi-
tion, fc and f2 can be computed with alternating roles in
Yao’s protocol by following the transferring roles proto-
col. Hence, fc could be garbled by PA while f2 could be
garbled by PB to securely compute f .

Figure 4: The IPP protocol for a mixed functionality with a
switch from parallel to sequential computation. Functionality
f1 (cid:28) f2 is garbled in parallel using IPP, f3 is garbled sequen-
tially in combination with f1. No interaction between parties is
shown. The blinded output o(cid:31)1 of f1 is only made visible to PA
and used as additional input for the computation of f2 using the
transferring roles protocol.

As a second observation we note that the output of f2
is not required to start the computation of fc. Therefore,
the computation of fc can start in parallel to the compu-
tation of f2. This inter-party parallelism can be exploited
to achieve further speed-ups. Figure 4 illustrates this ap-
proach. Party PA garbles fc and PB garbles f2. The ﬁrst
part of fc, namely f1 can be garbled in parallel to f2.
Once the blinded output o(cid:31)1 of f2 is computed, the par-
ties can start computing the second part of fc, namely
f3. Switching from sequential to IPP computation can be
realized in the same manner.

We remark that FGP, CGP and IPP can be combined
to achieve even further speed-ups. Therefore, every par-
allel region has ﬁrst be decomposed in two parts for IPP.
If the parts can further be decomposed in parallel func-
tionalities, these could be garbled following the ideas of
CGP and FGP.

Overhead.
Investigating the overhead of IPP, we ob-
serve that during the cost intensive garbling and evalua-
tion phase, no computational complexity is added. Par-
ticularly, the number of cryptographic operations and
messages is left unchanged. However, when using OT
Extension, a constant one-time overhead for base OTs
in the size of the security parameter k is introduced to
establish bi-directional OT Extension. To switch from
and to IPP in mixed functionalities, additional OTs in the
size of the intermediate state are required. Thus, the per-
formance gain through IPP for mixed functionalities not
only depends on the ratio between parallel and sequen-
tial regions, but also on the ratio of circuit size and shared
state. These ratios are application dependent. A practical
evaluation of the trade-off between overhead and perfor-
mance gain is presented in § 6.5.

USENIX Association  

24th USENIX Security Symposium  539

9

EVLGENEVLGENGENEVLPAPBf1,f2f3xyyro1'OTPOTPOTPOTPInp5.3 Security implications and applications

6 Evaluation

As discussed in the previous section, Yao’s protocol and
IPP are secure against semi-honest adversaries. Never-
theless, semi-honest Yao’s garbled circuits are often used
to bootstrap TPC protocols secure against active adver-
saries. Therefore, in this section, we sketch the security
implications of IPP and its compatibility with the most
common techniques to strengthen Yao’s protocol. Fur-
thermore, we depict applications and protocols the could
proﬁt from IPP.

Yao’s original protocol is already secure against ma-
licious evaluators (when using an OT protocol secure
against malicious adversaries), yet not secure against ma-
licious generators. We note that this one-sided secu-
rity does not longer hold when using IPP because both
parties incorporate the role of a circuit generator. Con-
sequently, cut-and-choose protocols [24], which garble
multiple copies of the same circuit to achieve active se-
curity, are incompatible with IPP because they are built
on the assumption that only one party can actively ma-
nipulate the protocol. A similar observation can be made
for Dual-execution protocols [29, 16] that prevent an ac-
tive adversary from learning more than a small number
of bits during the protocol execution. Even so the con-
cept of Dual-execution is close the the idea of IPP, i.e.,
symmetrically sharing the roles of generator and evalua-
tor, it also requires one-sided security against malicious
evaluators and is therefore incompatible with IPP.

Applications of IPP. IPP can be applied in all appli-
cation scenarios where semi-honest model is sufﬁcient.
These are scenarios where either the behaviour is oth-
erwise restricted, e.g. limited physical access, or where
the parties have sufﬁcient trust into each other. More-
over, IPP can be used in all the scenarios where the par-
ties inputs and seeds could be revealed at a later point
to identify cheating parties. Examples might be nego-
tiations (auctions) or games, such as online poker. An-
other ﬁeld of application is the joint challenge creation,
e.g. RSA factorization. Using secure computation, two
parties could jointly create a problem instance without
already knowing the solution. This allows them to create
a problem and to participate in the challenge at the same
time without a computational advantage. Once a solution
is computed, all parts of the secure computation can be
veriﬁed in hindsight.

For further work, we note that the core idea of IPP
could be applied in other TPC protocols. To proﬁt from
IPP, a state sharing mechanism in the protocols security
model is required as well as an asymmetric workload
between the parties. One example might be the highly
asymmetric STC protocol by Jarecki and Shmatikov [19]
that uses zero-knowledge proofs over every gate.

In this section, we evaluate the three proposed paral-
lelization schemes. We begin by introducing our paral-
lel STC framework named UltraSFE and benchmark its
performance on a single core in § 6.1. The applications
and their circuit descriptions used for benchmarking are
described in § 6.2. We evaluate the ofﬂine garbling per-
formance of the proposed parallelization techniques in
§ 6.3, before integrating and evaluating the promising
coarse-grained parallelization (CGP) in a online setting
in § 6.4. Finally, in § 6.5 we benchmark the inter-party
parallelization (IPP) approach.

6.1 UltraSFE
UltraSFE1 is a parallel framework for Yao’s garbled
circuits built up on the JustGarble (JG) [5] garbling
scheme. To realize efﬁcient parallelization, data struc-
tures and memory layout are optimized with the purpose
of parallelization in mind. This is, to the best of our
knowledge, not the case with existing open source frame-
works. Therefore, we adapted the JustGarble garbling
scheme to support parallelization.

UltraSFE is written in C++ using SSE4, OpenMP and
Pthreads to realize multi-core parallelization. Conceptu-
ally UltraSFE is using ideas from the Java based memory
efﬁcient ME SFE framework [13], which itself is based
on the popular FastGC framework [15]. The fast hard-
ware AES extension in current CPU generations is ex-
ploited by the JustGarble garbling scheme. Oblivious
transfers are realized with the help of the highly efﬁcient
and parallelized OTExtension library written by Asharov
et al. [2]. Moreover, UltraSFE adopts techniques from
many recent theoretical and practical advances of Yao’s
protocol. This includes pipe-lining, point-and-permute,
garbled row reduction, free-XOR and the half-gate ap-
proach [15, 21, 27, 31, 36].
Framework comparison. To illustrate that UltraSFE is
suited to evaluate the scalability of different paralleliza-
tion approaches, we present a comparison of its garbling
performance with other state of the art frameworks using
a CPU architecture in Table 1. Namely, we compare the
single core garbling speed of UltraSFE, which is prac-
tically identical to JG, with the parallel frameworks by
Barni et al. (BCPU) [3], Husted et al. (HCPU) [17] and
Kreuter et al. (KSS) [23]. Note, these results are com-
pared in the ofﬂine setting, i.e., truth tables are written to
memory. This is because circuit garbling is the most cost
intensive part of Yao’s protocol and therefore the most
interesting when comparing the performance of different
frameworks. The previous parallelization efforts HCPU

1UltraSFE will

source
http://www.seceng.informatik.tu-darmstadt.de/research/software/

be made

available

as

open

on

540  24th USENIX Security Symposium 

USENIX Association

10

Ours / JG [5] BCPU [3] HCPU[17]
<0.25M

0.11M
>3500
E5-2609

-

E5-2620

gps
cpg
arch

8.3M
108

E5-2680

KSS [23]

0.1M

>6500 [5]

i7-970

Table 1: Single core garbling speed comparison of different
frameworks on circuits with more than 5 million gates. Metrics
are non-linear gates per second (gps) in millions (M) and clocks
per gate (cpg). All results have been observed on the Intel pro-
cessor speciﬁed in row arch. Note, for HCPU [17] only circuit
evaluation times have been reported on the CPU, the garbling
speed can be assumed to be lower.

and BCPU actually abstained from implementing an on-
line version of Yao’s protocol that supports pipe-lining.
As metrics we use garbled gates per second (gps) and the
average number of CPU clock cycles per gate (cpg), as
proposed in [5]. The numbers are taken from the cited
publications and if not given, the cpq results are calcu-
lated based on the CPU speciﬁcations (arch). Even when
considering theses numbers only as rough estimates, due
to the different CPU types, we observe that UltraSFE per-
forms approximately 1-2 orders of magnitude faster than
existing parallelizations of Yao’s protocol. This is mostly
due to the efﬁcient ﬁxed-key garbling scheme using the
AES-NI hardware extension and a carefully optimized
implementation using SSE4. Summarizing, UltraSFE
shows competitive garbling performance on a single core
and hence, is a very promising candidate for paralleliza-
tion.

6.2 Evaluation methodology
To evaluate the different parallelization approaches we
use three example applications that have been used to
benchmark and compare the performance of Yao’s gar-
bled circuits in the past.
Biometric matching (BioMatch). The ﬁrst application
that we use is privacy-preserving biometric matching. In
this application a party matches one biometric sample
against the other’s party database of biometric templates.
Example scenarios are face-recognition or ﬁngerprint-
matching [9]. One of the encompassing concepts is the
computation of the Euclidean distance between the sam-
ple and all database entries. Once all distances have
been computed, the minimal distance determines the best
match. Thus, the task is to securely compute the minimal

i=1(si,n − ei)2(cid:30) with

si being the sample of degree d provided by the ﬁrst party
and e1, . . . ,e n being the database elements with the same
degree provided by the other party. Following the exam-
ples of [8, 20], the chosen parameters for this circuit are
the number of elements in the database n = 512, the de-
gree of each element d = 4 and the integer size b = 64bit.

distance min(cid:31)∑d

i=1(si,1 − ei)2, . . . ,∑d

Code size
Circuit size
Non-linear gates
# Input bits PA/PB
Ofﬂine garbling time

BioMatch MExp MVMul
10 LOC
22 LOC
3.3M
37%

66M
25%

28 LOC
21.5M
41%
1K/1K
1.136s

131K/256

2.07s

17K/1K
0.154s

Table 2: Circuit properties. Presented are the code size, the
overall circuit size in the number of gates, the fraction of non-
linear gates that determine the majority of computing costs, the
number of input bits as well as the sequential ofﬂine garbling
time with UltraSFE.

Modular exponentiation (MExp).
The second ap-
plication that we benchmark is parallel modular expo-
nentiation. Modular exponentiation has been used be-
fore to benchmark the performance of Yao’s garbled cir-
cuits [5, 8, 23].
It has many applications in privacy-
preserving computation. For example, blind signatures
where the message to be signed should not be revealed
to the signing party. For this application, we differentiate
the circuit by the number of iterated executions k = 32,
as well as the integer width b = 32.
Matrix-vector multiplication (MVMul). Algebraic
operations such a matrix multiplication or the dot vector
product are building blocks for many privacy-preserving
applications and have been used before to benchmark
Yao’s garbled circuits [14, 22]. We use a Matrix-vector
multiplication as required in the learning with errors
(LWE) cryptosystem [33]. We parametrize this task ac-
cording the size of the matrix m×k = 16×16 and vector
k = 16, as well the integer size of each element b = 64 bit.
Circuit creation. All circuits are compiled twice, once
with CBMC-GC and once with ParCC using textbook
C implementations. The time limit for the circuit mini-
mization through CBMC-GC is set to 10 minutes. The
resulting circuits and their properties are shown in Ta-
ble 2. The BioMatch circuit is the largest circuit and
shows the most input bits. The MVMul circuit garbles
in a fraction of a second and thus, ﬁts to evaluated the
performance of parallelelization on smaller circuits. The
MExp circuit shows a large circuit complexity in com-
parison to the number of input bits. Even so not shown
here, we note that the sequential (CBMC-GC) and paral-
lel (ParCC) circuits slightly differ in the overall number
of non-linear gates due to the circuit minimization tech-
niques of CBMC-GC, which proﬁt from decomposition.
Environment. As testing environment we used Amazon
EC2 cloud instances. These provide a scalable number
of CPUs and can be deployed at different sites around
the globe. If not state otherwise, for all experiments in-
stances of type c3.8xlarge have been used. These in-
stances report 16 physical cores on 2 sockets with CPUs

USENIX Association  

24th USENIX Security Symposium  541

11

of type Intel Xeon E5-2680v2, and are equipped with
a 10Gbps ethernet connection. A fresh installation of
Ubuntu 14.04 was used to ensure as little background
noise as possible. UltraSFE was compiled with gcc 4.8
-O2 and numactl was utilized when benchmarking with
only a fraction of the available CPUs. Numactl allows
memory, core and socket binding of processes. Results
have been averaged 10 executions.
Methodology. Circuit garbling is the most expensive
task in Yao’s protocol. Therefore, we begin by evaluating
FGP and CGP for circuit garbling independent of other
parts of Yao’s protocol. This allows an isolated evalua-
tion of the computational performance gains through par-
allelization. Following the ofﬂine circuit garbling phase
is an evaluation of Yao’s full protocol in an online LAN
setting. This evaluation also considers the bandwidth
requirements of Yao’s protocol. Finally, we present an
evaluation of the IPP approach in the same LAN setting.
Therefore, we ﬁrst evaluate the performance of IPP on a
single core, before evaluating its performance in combi-
nation with CGP. The main metric in all experiments is
the overall runtime and the number of non-linear gates
that can be garbled per second.

6.3 Circuit garbling (ofﬂine)
We begin our evaluation of FGP and CGP with the ofﬂine
task of circuit garbling. In practice the efﬁciency of any
parallelization is driven by the ratio between computa-
tional workload per thread and synchronization between
threads. When garbling a circuit with FGP, the workload
is bound by the width of each level, when garbling with
CGP the workload is bound by the size of parallel parti-
tions. Both parameters are circuit and hence, application
dependent.
Artiﬁcial circuits and thread utilization. To get a bet-
ter insight, we ﬁrst empirically evaluate the possible efﬁ-
ciency gain for different sized workloads, independent of
any application. This also allows to observe a system de-
pendent threshold τ, introduced in § 4.3, which describes
the minimal number of gates required per thread to proﬁt
from parallelization. Therefore, we run the following ex-
periment: For every level width w = 24,25, . . . ,2 10 we
created artiﬁcial circuits of depth d = 1000. The width is
kept homogeneous in all levels. Furthermore, the wiring
between gates is randomized and only non-linear gates
are used. Each circuit is garbled using FGP and we mea-
sured the parallelization efﬁciency (speed-up divided by
the number of cores) when computing with a different
numbers of threads. The results are illustrated in Fig-
ure 5.

The experiment shows that on the tested system τ ≈ 8
non-linear gates per thread are sufﬁcient to observe ﬁrst
performance gains through parallelization. To achieve

 1

 0.8

 0.6

 0.4

 0.2

y
c
n
e
c
i
f
f

i

E

2 Cores
4 Cores
8 Cores
16 Cores

 0

 16

 32

 64

 128
Level width

 256

 512

 1024

 1

 0.8

 0.6

 0.4

 0.2

 0

Figure 5: Level-width experiment. Displayed is the efﬁciency
of FGP for different circuit level widths. A larger width in-
creases the efﬁciency of parallelization. The gap between 8
(one socket) and 16 cores (two sockets) is due the communica-
tion latency between two sockets.

an efﬁciency of 90% approximately 512 non-linear gates
per thread are required. Investigating the results for 16
parallel threads, we observe that a signiﬁcantly larger
workload per thread (at least one order of magnitude)
is required to overcome the communication latency be-
tween the sockets on the testing hardware.
Example applications. We evaluated the speed-up of
circuit garbling when using FGP and CGP for the three
applications BioMatch, MExp and MVMul compiled
with CBMC-GC (FGP) and ParCC (CGP). The speed-up
is calculated in relation to the single core garbling per-
formance given in Table 2. The results are presented in
Figure 6. The results have been observed for a security
level of k = 128 bit. We note, that in this experiment
no signiﬁcant differences where observable when using
a smaller security level, e.g., κ = 80 bit, due to the ﬁxed
block size of AES-NI. Discussing the results for FGP,
we observe that all applications proﬁt from paralleliza-
tion. BioMatch and MExp show very limited scalability,
whereas the MVMul circuit is executable with a speed-
up of 7.5 on 16 cores. Analyzing the performance of
CGP, we observe that all applications achieve practically
ideal parallelization when using up to 4 threads. In con-
trast to the FGP approach, scalability with high efﬁciency
is observable with up to 8 threads. Further speed-ups
when using the CPU located on the second socket are no-
ticeable in the MExp and MVMul experiments, achiev-
ing a throughput of more than 100M non-linear gates per
second.

In summary, for all presented applications the CGP ap-
proach signiﬁcantly outperforms the FGP approach re-
garding scalability and efﬁciency due to its coarser gran-
ularity, which implies a better thread utilization.

542  24th USENIX Security Symposium 

USENIX Association

12

ideal parallelization
MExp - CGP
MVMul - CGP
BioMatch - CGP
MVMul - FGP
BioMatch - FGP
MExp - FGP

 16

 14

 12

 10

 8

 6

 4

 2

p
u
-
d
e
e
p
S

 16

 14

 12

 10

 8

 6

 4

 2

 0

 0

 2

 4

 6

 8

 10

 12

 14

 0

 16

Threads

Figure 6: Circuit garbling. The speed-up of circuit garbling
for all three applications when using the FGP, CGP and differ-
ent numbers of computing threads. CGP signiﬁcantly outper-
forms FGP for all applications.

Circuit width analysis. The limited scalability of FGP
is explainable when investigating the different circuit
properties. In Figure 7 the distribution of level widths
for all circuits is illustrated.

10^6

10^5

10^4

10^3

10^2

10^1

10^0

BioMatch

MExp

MVMul

Figure 7: Number of non-linear gates per level and circuit.

For the MVMul application, the CBMC-GC compiler
produces a circuit with a median level width of 2352
non-linear gates per level, whereas the BioMatch and
MExp circuits only show a median width below 100
non-linear gates per level. The major reasons for small
circuit widths in comparison to the overall circuit size
is that high-level TPC compilers such as CBMC-GC or
the compiler by Kreuter et al. [23] have been developed
with a focus on minimizing the number of non-linear
gates. Minimizing the circuit depth or maximizing the
median circuit width barely inﬂuence the sequential run-
time of Yao’s protocol and is therefore not addressed in
the ﬁrst place. Looking at the building blocks that are
used in CBMC-GC, we observe that arithmetic blocks
(e.g. adder, multiplier) show a linear increase in the av-
erage circuit width when increasing the input size. How-
ever, multiplexers, as used for dynamic array accesses
and for ‘if’ statements, show a circuit width that is in-
dependent (constant) of the number of choices. Thus,

a 2-1 multiplexer and a n-1 multiplexer are compiled to
circuits with similar sized levels, yet with different cir-
cuit depths. Moreover, comparisons have a constant cir-
cuit width for any input bit size. Based on these insights
we deduce, that the MVMul circuit shows a signiﬁcantly
larger median circuit width, because of the absence of
any dynamic array access, conditionals or comparisons.
This is not the case with the BioMatch and MExp appli-
cations. Considering that every insufﬁcient saturation of
threads leads to an efﬁciency loss of parallelization, we
conclude that scalability of FGP is not guaranteed when
increasing input sizes.

6.4 Full protocol (online)
To motivate that the parallelization of circuit garbling can
be used in Yao’s full protocol, we evaluated the protocol
for all applications running on two separated cloud in-
stances in the same Amazon region (LAN setting). We
observed an average round trip time of 0.6±0.3ms and a
transfer bandwidth of 5.0±0.4 Gbps using iperf. Fol-
lowing the results of the ofﬂine experiments, we bench-
mark the more promising CGP approach in the online
setting.

To measure the beneﬁts of parallelization, we ﬁrst
benchmark the single core performance of Yao’s protocol
in the described network environment. Table 3 shows the
sequential runtime for all applications using two security
levels κ = 80 bit (short term) and κ = 128 bit (long term).
This runtime includes the time spent on the input as well
as the output phase. Furthermore, the observed through-
put, measured in non-linear gates per second, as well as
the required bandwidth are presented. We observe that
for security levels of κ = 80 and κ = 128 a similar gate
throughput is achieved. Consequently, we deduce that
in this setup the available bandwidth is not stalling the
computation. We also observe that that the time spent on
OTs in all applications is practically negligible (< 5%)
in comparison with the time spent on circuit garbling.

In Figure 8 the performance gain of CGP is presented.
The speed-up is measured in relation to the sequential
total runtime. The timing results show that CGP scales
almost linearly with up to 4 threads when using κ = 80
bit labels. Using κ = 128 bit labels, no further speed-
up beyond 3 threads is noticeable. Thus, the impact of
the network limits is immediately visible. Five (κ = 80
bit), respectively three (κ = 128 bit) threads are sufﬁcient
to saturate the available bandwidth in this experiment.
Achieving further speed-ups is impossible without in-
creasing the available bandwidth or developing new TPC
techniques. However, to the best of our knowledge with
6M non-linear gates per second on a single core, as well
as with approximately 32M non-linear gates per second
on a single socket, we report the fastest garbling speed in

USENIX Association  

24th USENIX Security Symposium  543

13

Circuits

ttotal
[s]
gps
[M]
bw

[Gbps]
tinput
[s]

κ = 128
κ = 80
κ = 128
κ = 80
κ = 128
κ = 80
κ = 128
κ = 80

BioMatch
2.71±0.02
2.56±0.03
6.23±0.04
6.56±0.07
1.48±0.01
0.97±0.01
<0.02s
<0.02s

MExp
1.43±0.01
1.42±0.01
6.17±0.05
6.21±0.04
1.47±0.01
0.92±0.01
<0.01s
<0.01s

MVMul
0.20±0.00
0.19±0.00
6.22±0.00
6.43±0.00
1.48±0.00
0.95±0.00
< 0.01s
< 0.01s

Table 3: Yao’s protocol, single-core performance. The run-
time (ttotal), non-linear gate throughput in million gates per
second (gps), required bandwidth (bw) and time spent in the
input phase (tinput), including the OTs when executing Yao’s
protocol for all applications in a LAN setting.

 8

 7

 6

 5

 4

 3

 2

 1

p
u
-
d
e
e
p
S

 0

 0

ideal parallelization
MExp k=80
BioMatch k=80
MVMul k=80
MVMul k=128
BioMatch k=128
MExp k=128

 1

 2

 3

 4

 5

 6

 7

Threads

 8

 7

 6

 5

 4

 3

 2

 1

 0

 8

Figure 8: Yao’s protocol - CGP The speed-up of all three ap-
plications in the LAN setting with κ = 128 bit and κ = 80 bit
security.

an online setting of Yao’s protocol. We abstain from an
evaluation in a WAN setting due to the high bandwidth
that is required to show the scalability of parallelization.
The best bandwidth that we could observe during our
experiments between two cloud regions was 350 Mbps,
which is insufﬁcient to benchmark parallel scalability.

Inter-party parallelization

6.5
A new application of parallelization in Yao’s protocol is
presented in § 5. We performed two experiments to show
the applicability of IPP in practical settings. The ﬁrst
experiment measures the computational efﬁciency gain
in the same setting as described in § 6.4. In the second
experiment the beneﬁts of IPP in a WAN setting with
limited bandwidth are presented.
Computational efﬁciency gain. In this experiment the
raw IPP performance for all example applications, as
well as the combination of CGP and IPP techniques is
explored. To realize IPP, our implementation uses multi-

ple threads per core to utilize the load balancing capabili-
ties of the underlying OS without implementing a sophis-
ticated load balancer. Due to the heterogeneous hard-
ware environment, e.g. unpredictable caching and net-
working behaviour, we evaluated three different work-
load distribution strategies. The ﬁrst strategy uses one
thread per core and thus only functions with at least two
cores. Then, each party has exactly one garbling and one
evaluating thread. The second and third strategy use two
or four independent threads per core to garble and eval-
uate at the same time. Moreover, to illustrate that IPP
is a modular concept, all circuits are evaluated using a
sequential code block that exposes all inner input and
output wires before and after every parallel region. This
guarantees the evaluation of mixed functionalities. Con-
sequently, all results include the time spent on transfer-
ring all required input bits to and from parallel regions.
Otherwise applications such as the MVMul application,
which is a pure parallel functionality, would proﬁt more
easily from IPP. Even though this weakens the results
for the example applications, we are convinced that this
procedure provides a better insight into the practical per-
formance of IPP.

The results of this experiment are reported in Table 4.
We ﬁrst observe that only the MExp application signif-
icantly proﬁts from IPP. This is due to the small shar-
ing state in comparison to the circuit complexity. For
both security levels IPP outperforms the raw CGP ap-
proach with an additional speed-up of 10-30% on all
cores. The performance of the MVMul applications ac-
tually decreases when using IPP. This is because of the
large state that needs to be transferred. The performance
gain through IPP cannot overcome the newly introduced
overhead of 31ms, which is more than 15% of the se-
quential run-time.

In summary, parallelizable applications that show a
small switching surface (measured in number of bits
compared to the overall circuit size) proﬁt from IPP.
Thus, IPP is a promising extension to Yao’s protocol that
utilizes circuit decomposition beyond naive paralleliza-
tion, independently of other optimization techniques.
Bi-directional bandwidth exploitation. The second
experiment aims towards increasing the available band-
width by exploiting bidirectional data transfers. Com-
monly, Ethernet connections have support for full duplex
(bi-directional) communication. When using standard
Yao’s garbled circuits, only one communication direc-
tion is fully utilized. However, with IPP the available
bandwidth can be doubled by symmetrically exploiting
both communication channels. This practical insight is
evaluated in a WAN setting between two cloud instances
of type m3.xlarge with 100± 10ms latency and a mea-
sured bandwidth of 92± 27Mbps. Each hosts runs two
threads (a garbling and a evaluating thread) using only

544  24th USENIX Security Symposium 

USENIX Association

14

Cores

Environment
IPP
none

1

2
4

2

4

8

none

1
2
4

none

1
2
4

none

1
2
4

BioMatch
time[s] S
2.559 1.000
2.624 0.975
2.556 1.001
1.384 1.849
1.524 1.679
1.472 1.738
1.396 1.833
0.795 3.219
0.996 2.569
0.830 3.083
0.818 3.128
0.652 3.925
0.676 3.786
0.629 4.068
0.636 4.024

κ = 80
MExp
time[s] S
1.423 1.000
1.287 1.106
1.285 1.107
0.734 1.939
0.686 2.074
0.699 2.036
0.654 2.176
0.395 3.603
0.426 3.340
0.336 4.235
0.329 4.325
0.298 4.775
0.239 5.954
0.204 6.975
0.233 6.107

MVMul
time[s] S
0.192 1.000
0.206 0.932
0.208 0.923
0.103 1.864
0.126 1.524
0.132 1.455
0.124 1.548
0.057 3.368
0.084 2.286
0.085 2.259
0.081 2.370
0.045 4.267
0.072 2.667
0.077 2.494
0.070 2.743

BioMatch
time[s] S
2.712 1.000
2.781 0.975
2.707 1.002
1.497 1.812
1.535 1.767
1.516 1.789
1.465 1.851
0.937 2.894
1.041 2.605
0.874 3.103
0.856 3.168
0.872 3.110
0.947 2.864
0.861 3.150
0.871 3.114

κ = 128
MExp
time[s] S
1.485 1.000
1.370 1.084
1.386 1.071
0.780 1.904
0.699 2.124
0.726 2.045
0.693 2.143
0.450 3.300
0.452 3.285
0.356 4.171
0.341 4.355
0.364 4.080
0.303 4.901
0.342 4.342
0.337 4.407

MVMul
time[s] S
0.199 1.000
0.215 0.926
0.218 0.913
0.108 1.844
0.134 1.485
0.137 1.453
0.131 1.519
0.064 3.088
0.087 2.287
0.088 2.261
0.084 2.369
0.048 4.189
0.080 2.488
0.075 2.653
0.073 2.726

Transferring roles

0.231s

0.076s

0.031s

0.257s

0.082s

0.031s

Table 4: Evaluation of IPP in a LAN setting. Column IPP speciﬁes the number of threads used per core for load balancing. The
total protocol run-time is measured in seconds and the speed-up in comparison with CGP is presented in column S. Marked in bold
are settings, where IPP leads to performance gains. The time spent on the transferring roles protocol is presented in the last row.

a single core. The results of this experiment are illus-
trated in Table 5. IPP leads to signiﬁcant speed-ups of
BioMatch and MExp, showing the successful exploita-
tion of bi-directional data transfers. MVMul shows lim-
ited performance gains because the time spent on the
newly introduced communication rounds for the trans-
ferring roles protocol becomes signiﬁcant. Summariz-
ing, IPP can be very useful for TPC in bandwidth limited
environments.

BioMatch
raw 45.02±0.49s
IPP
29.94±0.31s
S
raw 30.34±0.62s
19.13±0.47s
IPP
S

1.50

1.58

MExp

24.13±0.21s
16.05±0.12s

1.50

14.56±0.21s
11.16±0.32s

1.30

MVMul
4.83±0.05s
4.66 ±0.35s

1.03

4.31±0.23s
3.84±0.12s

1.12

κ = 128

κ = 80

Table 5: Evaluation of IPP on a single core with limitted net-
working capabilities. Measured is the total protocol runtime,
when sequentially (raw) computing and with IPP (IPP). Fur-
thermore, the speed-up (S) between the two measurements is
calculated.

approach shows a more efﬁcient parallelization, given
parallel decomposable applications. In contrast to pre-
vious work, a complete compile chain, which takes C
code as input and automatically compiles parallel cir-
cuits, supports the practicability of our parallelization
scheme. Moreover, we proposed the idea of IPP to
achieve a symmetric workload distribution between two
computing parties. With this technique, IPP achieves
speed-ups through parallelization, even when using a sin-
gle physical core. Concluding, in this work we presented
an efﬁcient, versatile and practical parallelization scheme
for Yao’s garbled circuits.

Further work includes the investigation of different
parallel compilation targets for ParCC, such as the GMW
protocol or RAM based secure computation frameworks.
Also worthwhile for future investigations is the compila-
tion of circuits optimized for FGP. Likewise, the applica-
tion of IPP to other protocols is of interest.

8 Acknowledgements

7 Conclusion and Future Work

TPC based on Yao’s garbled circuits protocol can greatly
beneﬁt from automatic parallelization. The FGP ap-
proach can be efﬁcient for some circuits, yet its scala-
bility highly depends on the circuit’s width. The CGP

We thank David Evans and all anonymous reviewers for
their very helpful and constructive comments. This work
has been co-funded by the German Federal Ministry of
Education and Research (BMBF) within EC SPRIDE,
by the DFG as part of project S5 within the CRC 1119
CROSSING, and by the Hessian LOEWE excellence ini-
tiative within CASED.

USENIX Association  

24th USENIX Security Symposium  545

15

References
[1] AMINI, M., CREUSILLET, B., EVEN, S., KERYELL, R., GOU-
BIER, O., GUELTON, S., MCMAHON, J. O., PASQUIER, F.-X.,
P ´EAN, G., AND VILLALON, P. Par4All: From Convex Array Re-
gions to Heterogeneous Computing. In Workshop on Polyhedral
Compilation Techniques (2012).

[2] ASHAROV, G., LINDELL, Y., SCHNEIDER, T., AND ZOHNER,
M. More efﬁcient oblivious transfer and extensions for faster
secure computation. In ACM Conference on Computer and Com-
munications Security CCS (2013).

[3] BARNI, M., BERNASCHI, M., LAZZERETTI, R., PIGNATA, T.,
AND SABELLICO, A. Parallel Implementation of GC-Based
MPC Protocols in the Semi-Honest Setting.
In Data Privacy
Management and Autonomous Spontaneous Security. 2014.

[4] BEAVER, D., MICALI, S., AND ROGAWAY, P. The Round Com-
In ACM Symposium on Theory of

plexity of Secure Protocols.
Computing STOC (1990).

[5] BELLARE, M., HOANG, V. T., KEELVEEDHI, S., AND ROG-
In

AWAY, P. Efﬁcient garbling from a ﬁxed-key blockcipher.
IEEE Symposium on Security and Privacy S&P (2013).

[6] BONDHUGULA, U., HARTONO, A., RAMANUJAM, J., AND SA-
DAYAPPAN, P. A practical automatic polyhedral parallelizer and
locality optimizer. ACM SIGPLAN Notices 43, 6 (2008).

[7] DAGUM, L., AND MENON, R. OpenMP: an industry standard
API for shared-memory programming. IEEE Computational Sci-
ence and Engineering 5, 1 (1998).

[8] DEMMLER, D., SCHNEIDER, T., AND ZOHNER, M. ABY A
Framework for Efﬁcient Mixed-Protocol Secure Two-Party Com-
putation. Network and Distributed System Security NDSS (2015).
[9] ERKIN, Z., FRANZ, M., GUAJARDO, J., KATZENBEISSER, S.,
LAGENDIJK, I., AND TOFT, T. Privacy-preserving face recogni-
tion. In Privacy Enhancing Technologies PETS (2009).

[10] FRANZ, M., HOLZER, A., KATZENBEISSER, S., SCHALL-
HART, C., AND VEITH, H. CBMC-GC: An ANSI C compiler
for secure two-party computations. In Compiler Construction CC
(2014).

[11] FREDERIKSEN, T. K., JAKOBSEN, T. P., AND NIELSEN, J. B.
Faster maliciously secure two-party computation using the GPU.
In Security and Cryptography for Networks SCN. 2014.

[12] HAZAY, C., AND LINDELL, Y. Efﬁcient secure two-party proto-
cols. Information Security and Cryptography. Springer, Heidel-
berg (2010).

[13] HENECKA, W., AND SCHNEIDER, T. Faster secure two-party
computation with less memory. In ACM Conference on Computer
and Communications Security ASIACCS (2013).

[14] HOLZER, A., FRANZ, M., KATZENBEISSER, S., AND VEITH,
H. Secure Two-Party Computations in ANSI C. In ACM Confer-
ence on Computer and Communications Security CCS (2012).

[15] HUANG, Y., EVANS, D., KATZ, J., AND MALKA, L. Faster Se-
cure Two-Party Computation Using Garbled Circuits. In USENIX
Security Symposium (2011).

[16] HUANG, Y., KATZ, J., AND EVANS, D. Quid-pro-quo-tocols:
In

Strengthening semi-honest protocols with dual execution.
IEEE Symposium on Security and Privacy S&P (2012).

[17] HUSTED, N., MYERS, S., SHELAT, A., AND GRUBBS, P. GPU
and CPU parallelization of honest-but-curious secure two-party
computation. In Annual Computer Security Applications Confer-
ence ACSAC (2013).

[18] ISHAI, Y., KILIAN, J., NISSIM, K., AND PETRANK, E. Extend-
In Advances in Cryptology

ing Oblivious Transfer Efﬁciently.
CRYPTO. Springer, 2003.

[19] JARECKI, S., AND SHMATIKOV, V. Efﬁcient Two-Party Secure
Computation on Committed Inputs. In Advances in Cryptology
EUROCRYPT, vol. 4515. Springer, 2007.

[20] KERSCHBAUM, F., SCHNEIDER, T., AND SCHR ¨OPFER, A. Au-
tomatic protocol selection in secure two-party computations. In
Applied Cryptography and Network Security ACNS (2014).

[21] KOLESNIKOV, V., AND SCHNEIDER, T. Improved garbled cir-
cuit: Free XOR gates and applications. In International Confer-
ence on Automata, Languages and Programming ICALP. 2008.
[22] KREUTER, B., MOOD, B., SHELAT, A., AND BUTLER, K. PCF:
A Portable Circuit Format for Scalable Two-Party Secure Com-
putation. In USENIX Security Symposium (2013).

[23] KREUTER, B., SHELAT, A., AND SHEN, C. Billion-Gate Se-
cure Computation with Malicious Adversaries. USENIX Security
Symposium (2012).

[24] LINDELL, Y., AND PINKAS, B. An efﬁcient protocol for secure
two-party computation in the presence of malicious adversaries.
In Advances in Cryptology EUROCRYPT. 2007.

[25] LINDELL, Y., AND PINKAS, B. A proof of security of yao’s
protocol for two-party computation. Journal of Cryptology 22, 2
(2009).

[26] MALEWICZ, G., AUSTERN, M. H., BIK, A. J. C., DEHNERT,
J. C., HORN, I., LEISER, N., AND CZAJKOWSKI, G. Pregel: a
system for large-scale graph processing. In ACM Conference on
Management of Data SIGMOD (2010).

[27] MALKHI, D., NISAN, N., PINKAS, B., AND SELLA, Y.
Fairplay-Secure Two-Party Computation System. In USENIX Se-
curity Symposium (2004).

[28] MCCREARY, C., AND GILL, H. Efﬁcient Exploitation of Con-
currency Using Graph Decomposition. In International Confer-
ence on Parallel Processing ICPP (1990).

[29] MOHASSEL, P., AND FRANKLIN, M. Efﬁciency tradeoffs for
In Public Key Cryptography

malicious two-party computation.
PKC. 2006.

[30] NAYAK, K., WANG, X. S., IOANNIDIS, S., WEINSBERG, U.,
TAFT, N., AND SHI, E. GraphSC: Parallel Secure Computation
Made Easy. In IEEE Symposium on Security and Privacy S&P
(2015).

[31] PINKAS, B., SCHNEIDER, T., SMART, N. P., AND WILLIAMS,
S. C. Secure two-party computation is practical. Advances in
Cryptology ASIACRYPT (2009).

[32] POUCHET, L.-N. Polyhedral Compiler Collection (PoCC), 2012.
[33] REGEV, O. On lattices, learning with errors, random linear codes,

and cryptography. Journal of the ACM 56, 6 (2009).

[34] YAO, A. C. Protocols for secure computations. In Symposium on

Foundations of Computer Science SFCS (1982).

[35] YAO, A. C. How to generate and exchange secrets. In Symposium

on Foundations of Computer Science SFCS (1986).

[36] ZAHUR, S., ROSULEK, M., AND EVANS, D. Two halves make

a whole. Advances in Cryptology - EUROCRYPT (2015).

[37] ZHANG, Y., STEELE, A., AND BLANTON, M. PICCO: A
general-purpose compiler for private distributed computation. In
ACM Conference on Computer and Communications Security
CCS (2013).

546  24th USENIX Security Symposium 

USENIX Association

16

