gregoire.jacob@gmail.com
Christopher Kruegel

Northeastern University

ek@ccs.neu.edu

Giovanni Vigna

PUBCRAWL: Protecting Users and Businesses from CRAWLers
Engin Kirda

Gregoire Jacob

University of California, Santa Barbara / Telecom SudParis

University of California, Santa Barbara

University of California, Santa Barbara

chris@cs.ucsb.edu

vigna @cs.ucsb.edu

Abstract

Web crawlers are automated tools that browse the web
to retrieve and analyze information. Although crawlers
are useful tools that help users to ﬁnd content on the
web, they may also be malicious. Unfortunately, unau-
thorized (malicious) crawlers are increasingly becoming
a threat for service providers because they typically col-
lect information that attackers can abuse for spamming,
phishing, or targeted attacks. In particular, social net-
working sites are frequent targets of malicious crawling,
and there were recent cases of scraped data sold on the
black market and used for blackmailing.

In this paper, we introduce PUBCRAWL, a novel ap-
proach for the detection and containment of crawlers.
Our detection is based on the observation that crawler
trafﬁc signiﬁcantly differs from user trafﬁc, even when
many users are hidden behind a single proxy. Moreover,
we present the ﬁrst technique for crawler campaign attri-
bution that discovers synchronized trafﬁc coming from
multiple hosts. Finally, we introduce a containment
strategy that leverages our detection results to efﬁciently
block crawlers while minimizing the impact on legiti-
mate users. Our experimental results in a large, well-
known social networking site (receiving tens of millions
of requests per day) demonstrate that PUBCRAWL can
distinguish between crawlers and users with high ac-
curacy. We have completed our technology transfer,
and the social networking site is currently running PUB-
CRAWL in production.
1
Introduction
Web crawlers, also called spiders or robots, are tools that
browse the web in an automated and systematic fashion.
Their purpose is to retrieve and analyze information that
is published on the web. Crawlers were originally de-
veloped by search engines to index web pages, but have
since multiplied and diversiﬁed. Crawlers are now used
as link checkers for web site veriﬁcation, as scrapers to
harvest content, or as site analyzers that process the col-
lected data for analytical or archival purposes [9]. While
many crawlers are legitimate and help users ﬁnd relevant

content on the web, unfortunately, there are also crawlers
that are deployed for malicious purposes.

As an example, cyber-criminals perform large-scale
crawls on social networks to collect user proﬁle infor-
mation that can later be sold to online marketers or used
for targeted attacks. In 2009, a hacker who had crawled
the popular student network StudiVZ, blackmailed the
company, threatening to sell the stolen data to gangs in
Eastern Europe [23].
In 2010, Facebook sued an en-
trepreneur who crawled more than 200 million proﬁles,
and who was planning to create a third-party search ser-
vice with the data that he had collected [25].
In gen-
eral, the problem of site scraping is not limited to so-
cial networks. Many sites who advertise goods, ser-
vices, and prices online desire protection against com-
petitors that use crawlers to spy on their inventory. In
several court cases, airlines (e.g., American Airlines [2],
Ryanair [20]) sued companies that scraped the airlines’
sites to be able to offer price comparisons and ﬂights to
their customers. In other cases, attackers scraped content
from victim sites, and then simply offered the cloned in-
formation under their own label.

Many websites explicitly forbid unauthorized scrap-
ing in their terms of services. Unfortunately, such
terms are simply ignored by non-cooperating (mali-
cious) crawlers. The Robot Exclusion Protocol faces
a similar problem: Web sites can specify rules in the
robots.txt ﬁle to limit crawler accesses to certain
parts of their site [14], but a crawler has to voluntarily
follow these restrictions.

Current detection techniques rely on simple crawler
artifacts (e.g., spurious user agent strings, suspicious
referrers) and simple trafﬁc patterns (e.g., inter-arrival
time, volume of trafﬁc) to distinguish between human
and crawler trafﬁc. Unfortunately, better crawler imple-
mentations can remove revealing artifacts, and simple
trafﬁc patterns fail in the presence of proxy servers or
large corporate gateways, which can serve hundreds of
legitimate users from a single IP address. In response to
the perceived lack of effective protection, several com-
mercial anti-scraping services have emerged (e.g., Dis-

til.It, SiteBlackBox, Sentor Assassin). These services
employ “patent pending heuristics” to defend against
unwanted crawlers. Unfortunately, it is not clear from
available descriptions how these services work in detail.
Many sites rely on CAPTCHAs [24] to prevent scrap-
ers from accessing web content. CAPTCHAs use
challenge-response tests that are easy to solve for hu-
mans but hard for computers. Some tests are known
to be vulnerable to automated breaking techniques [4].
Nevertheless, well-designed CAPTCHAs offer a rea-
sonable level of protection against automated attacks.
Unfortunately, their excessive use brings along usabil-
ity problems and severely decreases user satisfaction.
Other prevention techniques are crawler traps. A crawler
trap is a URL that lures crawlers into inﬁnite loops,
using, for example, symbolic links or sets of auto-
generated pages [15]. Unfortunately, legitimate crawlers
or users may also be misled by these traps. Traps and
CAPTCHAs can only be one part of a successful de-
fense strategy, and legitimate users and crawlers must be
exposed as little as possible to them.

In this paper, we introduce a novel approach and a sys-
tem called PUBCRAWL to detect crawlers and automati-
cally conﬁgure a defense strategy. PUBCRAWL’s useful-
ness has been conﬁrmed by a well-known, large social
networking site we have been collaborating with, and
it is now being used in production. Our detection does
not rely on easy-to-detect artifacts or the lack of ﬁdelity
to web standards in crawler implementations. Instead,
we leverage the key observation that crawlers are auto-
mated processes, and as such, their access patterns (web
requests) result in different types of regularities and vari-
ations compared to those of real users. These regularities
and variations form the basis for our detection.

For detection, we use both content-based and timing-
based features to passively model the trafﬁc from dif-
ferent sources. We extract content-based features from
HTTP headers (e.g., referrers, cookies) and URLs (e.g.,
page revisits, access errors). These features are checked
by heuristics to detect values betraying a crawling activ-
ity. For timing-based features, we analyze the time series
produced by the stream of requests. We then use ma-
chine learning to train classiﬁers that can distinguish be-
tween crawler and user trafﬁc. Our system is also able to
identify crawling campaigns led by distributed crawlers
by looking at the synchronization of their trafﬁc.

The aforementioned features work well for detect-
ing crawlers that produce a minimum volume of traf-
ﬁc. However, it is conceivable that some adversaries
have access to a large botnet with hundreds of thousands
of infected machines. In this case, each individual bot
would only need to make a small number of requests to
scrape the entire site, possibly staying below the mini-
mal volume required by our models. An active response

to such attacks must be triggered, such as the injection
of crawler traps or CAPTCHAs. An active response is
only triggered when a single client sends more than a
(small) number of requests. To minimize the impact of
active responses on legitimate users, we leverage our de-
tection results to distinguish between malicious crawlers
and benign sources that produce a lot of trafﬁc (e.g.,
proxy servers or corporate gateways). This allows us
to automatically whitelist benign sources (whose IP ad-
dresses rarely change), minimizing the impact on legiti-
mate users while denying access to unwanted crawlers.
For evaluation, we applied PUBCRAWL to the web
server logs of the social networking site we were work-
ing with. To train the system, we examined 5 days
worth of trafﬁc, comprising 73 million requests from
813 average-volume sources (IP addresses). The logs
were ﬁltered to focus on sources whose trafﬁc volume
was not an obvious indicator of their type. To test the
system, we examined a set of 62 million requests coming
from 763 sources over 5 days. Our experiments demon-
strated that more sophisticated crawlers are often hard
to distinguish from real users, and hence, are difﬁcult
to detect using traditional techniques. Using our sys-
tem, we were able to identify crawlers with high accu-
racy, including crawlers that were previously-unknown
to the social networking site. We also identiﬁed interest-
ing campaigns of distributed crawlers.

Section 2 gives an overview of the system whereas
Sections 3 to 5 provide more technical details for each
part. The conﬁguration and evaluation of the system if
ﬁnally addressed in Sections 6 to 8. Overall, this paper
makes the following contributions:

• We present a novel technique to detect individual
crawlers by time series analysis. To this end, we
use auto-correlation and decomposition techniques
to extract navigation patterns from the trafﬁc of in-
dividual sources.

• We introduce the ﬁrst

strategically emit according to detection results.

technique to detect dis-
tributed crawlers (crawling campaigns). More pre-
cisely, our system can identify coordinated activity
from multiple crawlers.
• We contain crawlers using active responses that we
• We implemented our approach in a tool called
PUBCRAWL, and performed the largest real-world
crawler detection evaluation to date, using tens of
millions of requests from a popular social network.
PUBCRAWL distinguishes crawlers from users with
high accuracy (even users behind a proxy).

2 System Overview
The goal of PUBCRAWL is to analyze the web trafﬁc that
is sent to a destination site, and to automatically classify

the originating sources of this trafﬁc as either crawlers
or users. The initial trafﬁc analysis is passive and per-
formed off-line, using log data that records web site re-
quests from clients. The goal of this analysis is to build
a knowledge base about trafﬁc sources (IP addresses).

This knowledge base is then consulted to respond to
web requests. Requests from known users or accepted
crawlers (e.g., Googlebot) are served directly. Requests
from unwanted crawlers, in contrast, are blocked. Of
course, the knowledge base may not contain an entry for
a source IP. In this case, a small number of requests is
permitted until it exceeds a given threshold: the system
then switches to active containment and, for example,
injects traps or CAPTCHAs into the response.

While the system is active, the requests are recorded
to reﬁne the knowledge base: When PUBCRAWL identi-
ﬁes a previously-unknown source to be a user or a legiti-
mate proxy, requests from this source are no longer sub-
jected to active responses whereas unwanted crawlers
are blacklisted. The key insight is that PUBCRAWL can
successfully identify legitimate, high-volume sources,
and these sources are very stable. This stability of high-
volume sources and the large number of low-volume
sources (tens of requests) ensure that only a small frac-
tion of users will be subjected to active responses.

The architecture of PUBCRAWL is shown in Figure 1.
The server log entries (i.e., the input) are ﬁrst split into
time windows of ﬁxed length. Running over these win-
dows, the system extracts two kinds of information: (i)
HTTP header information, including URLs and (ii) tim-
ing information in the form of time series. The ex-
tracted information is then submitted to two detection
modules (which detect individual crawlers) and an at-
tribution module (which detects crawling campaigns).
These three modules generate the knowledge base that is
leveraged by the proactive containment module for real-
time trafﬁc. The different modules are described below:
Heuristic detection. For a given time window, the
system analyzes the requests coming from distinct
source IP addresses, and extracts different features re-
lated to HTTP header ﬁelds and URL elements. These
features are checked with heuristics to detect suspicious
values that could reveal an ongoing crawling activity
(e.g., suspicious referrers, unhandled cookies, etc.).
Trafﬁc shape detection. When crawlers correctly set
the different HTTP ﬁelds and masquerade the user agent
string, it becomes much more difﬁcult to tell apart slow
crawlers from busy proxies since they cannot be distin-
guished based on request volumes. Trafﬁc shape detec-
tion addresses this problem. For a given source IP ad-
dress, the system analyzes the requests (time stamps)
over a ﬁxed time window to build the associated time
series. Figure 2 depicts time series representing crawler

Figure 1: Architecture overview

Figure 2: YahooSlurp and MSIE 7 time series

and user trafﬁc. One can observe distinctive patterns
that are speciﬁc to each class of trafﬁc. Crawler trafﬁc
tends to exhibit more regularity and stability over time.
User trafﬁc, instead, tends to exhibit more local varia-
tions. However, over time, user trafﬁc also shows repet-
itive patterns whose regularity is related to the “human
time scale” (e.g., daily or weekly patterns).

To determine the long-term stability of a time series
with respect to its quick variations, we leverage auto-
correlation analysis techniques. Furthermore, to sepa-
rate repetitive patterns from slow variations, we lever-
age decomposition analysis techniques. Decomposition
separates a time series into a trend component that cap-
tures slow variations, a seasonal component that cap-
tures repetitive patterns, and a component that captures
the remaining noise. We use the information extracted
from these analyses as input to classiﬁers for detection.
These classiﬁers are used to determine whether an un-
known time series belongs to a crawler or a user.

Campaign attribution. Some malicious crawlers
willingly reduce their volume of requests to remain
stealthy. This comes at a cost for the attacker in terms
of crawling efﬁciency (volume of retrieved data during a
speciﬁc time span). To compensate for these limitations,
malicious crawlers, just like legitimate ones, distribute
their crawling activity over multiple sources. We denote
a set of crawlers, distributed over multiple locations, and
showing synchronized activity, as a crawling campaign.
Figure 3 presents two time series that correspond to
the same crawler distributed over two hosts in different
subnets. One can observe a high level of similarity be-
tween the two time series. The key insight of our ap-
proach is that these similarities can be used to identify
the distributed crawlers that are part of a campaign.

• Low parameter usage: Existing detectors mostly
consider the length and depth of URLs. In our case,
proﬁle URLs show a similar length and the same
level of depth. Instead, parameters can additionally
be appended to URLs (e.g., language selection).
Crawlers typically do not use these parameters.
• Suspicious proﬁle sequence: Crawlers often access
proﬁles in a sorted (alphabetic) order. This is be-
cause the pointers to proﬁles are often obtained
from directory queries.

Heuristics results are combined into a ﬁnal classiﬁca-
tion by a majority vote. That is, if a majority of heuristics
are triggered, we ﬂag the source as a crawler.
3.2 Time series extraction
Our trafﬁc shape detection signiﬁcantly differs from ex-
isting work on crawler detection as we do not divide the
source trafﬁc into sessions. Instead, we model trafﬁc as
counting processes from which some properties are ex-
tracted, replacing the simple timing features (e.g., mean
and deviation of inter-arrival times) computed over traf-
ﬁc sessions as in [18, 21, 22].
Deriving the time series. We model the trafﬁc from a
source over a time window [t0, tn[ as a counting process
(a particular kind of time series). A counting process
X(t) = {X(t)}tn
t0 counts the number of requests that ar-
rive from a given source within n time intervals of ﬁxed
duration, where each interval is a fraction of the entire
time window. Notice that the trafﬁc logs must be split
into windows of at least two days to capture patterns that
repeat at the time scale of one day.

Because most statistical tests are sensitive to the total
(absolute) numbers of requests, we normalize the time
series. To this end, the amplitude of the series is ﬁrst
scaled to capture the ratio of requests per time interval to
the total volume of requests produced by the source. The
time frame of the series is then normalized by setting a
common time origin: the start of all series is set to be
the arrival time of the ﬁrst observed request among all
monitored sources. Formally, the normalized time series
are extracted as follows: Let S be the set of monitored
sources. Let Rs be the set of requests from a source
s ∈ S. Then, its time series Xs(t) is:

(cid:26) Card({r ∈ Rs | r.arrival ∈ [t, t + d [ })

(cid:27)

(1)

(2)

Xs(t) =

Card({r ∈ Rs | r.arrival ∈ [t0, tn[ })

t0 = mins∈S ({minr∈Rs ({r.arrival})})

We chose d = 30 minutes as the length of each time
interval, in order to smooth the shape of the time series.
Shorter intervals made the series too sensitive to pertur-
bations in the network communications that are often in-
dependent of the source. Longer intervals, instead, make
it harder to capture interesting variations in the trafﬁc.

Figure 3: Distributed crawler (sources A and B)

Proactive containment. Our containment approach
uses the detection results to establish a whitelist of legit-
imate crawlers and user sources that are allowed direct
access to the site. We also compile a blacklist of unau-
thorized crawlers that need to be blocked. As mentioned
previously, other sources are granted a small number of
unrestricted accesses per day. For IPs that exceed this
threshold, the system responds by inserting CAPTCHAs
or crawler traps into response pages.

3 Crawler Detection Approach
In this section, we provide a more detailed description
of the crawler detection process in PUBCRAWL.
3.1 Heuristic detection based on headers
The heuristic detection module processes,
for each
source, the HTTP header ﬁelds and URLs extracted from
the requests in the trafﬁc log. Request-based features
have been used in the past by systems that aim to detect
crawlers [10, 11, 17, 18, 21, 22]. We selected the follow-
ing ones for our detection module:

• High error rate: The URL lists used to feed a
crawler often contain entries that belong to invalid
proﬁle names. As a result, crawlers tend to show a
higher rate of errors when accessing proﬁle pages.
• Low page revisit: Crawlers tend to avoid revisiting
the same pages. Users, on the other hand, tend to
regularly revisit the same proﬁles in their network.
• Suspicious referrer: Many crawlers ignore the re-
ferrer ﬁeld in a request, setting it to null instead.
Advanced crawlers do handle the referrer ﬁeld, but
give themselves away by using referrers that point
to the results of directory queries (listings).
• Ignored cookies: Many crawlers ignore cookies.
As a result, a new session identiﬁer cookie is issued
for each request by these crawlers.

In addition to the previously-described heuristics, we
propose a number of additional, novel features:

• Unbalanced trafﬁc: When we see multiple user
agent strings coming from one IP, we expect the re-
quests to be somewhat balanced between these dif-
ferent user agents. If one agent is responsible for
more than 99% of the requests, this is suspicious.

3.3 Detection by trafﬁc shape analysis
In this section, we present how we model the shape
of time series to distinguish users from crawlers. Sec-
tions 3.3.1 and 3.3.2 discuss how characteristic features
of the trafﬁc are extracted using the auto-correlation and
decomposition analyses. Section 3.3.3 describes how
these features are used to train classiﬁers with the goal
of identifying crawler trafﬁc.
3.3.1 Auto-correlation analysis
To characterize the stability of the source trafﬁc, we
compute the Sample Auto-Correlation Function (SAC)
of the source’s time series and analyze its shape [3,
Chapt.9]. The SAC captures the dependency of values
at different points in time on the values observed for the
process at previous times (the time difference between
the two compared values is called lag). This function is a
good indicator for how the request counts vary over time.
A strong auto-correlation at small lags indicates a stable
(regular) process, which is typical for crawlers. Spikes
of auto-correlation at higher lags indicate potential sea-
sonal variations, as in the case of users (for example, a
strong auto-correlation at a lag of one day indicates that
trafﬁc follows a regular, daily pattern).

For a given lag k, the auto-correlation coefﬁcient rk is
computed as in Equation 3, where E denotes the mean
and V ar the variance. The SAC Function captures the
auto-correlation coefﬁcients at different lags.

rk =

E[(X(t) − E[X(t)]) × (X(t + k) − E[X(t)])]

V ar[X(t)]

(3)

To determine the signiﬁcance of the auto-correlation
coefﬁcient at a given lag k, the coefﬁcient is usually
compared to the standard error. If the coefﬁcient is larger
than twice the standard error, it is statistically signiﬁ-
cant. In this case, we say that we observe a spike at lag
k. A spike indicates that counts separated by a lag k are
linearly dependent. We use the Moving Average (MA)
model to compute the standard error at lag k [3]. Un-
like other models, the MA model does not assume that
the values of the time series are uncorrelated, random
variables. This is important, as we expect request counts
from a single source to be correlated.

Figure 4 presents the SAC Functions computed over
the time series from Figure 2. The functions were plot-
ted over 96 lags (time span of two days). The additional
(red) lines correspond to the standard errors under the
MA assumption. If we observe the shape of these SACs,
the crawler SAC shows a strong auto-correlation at small
lags, followed by a slow linear decay, but no consecu-
tive spike. The user SAC shows a less signiﬁcant auto-
correlation at small lags, followed by a fast exponential
decay. However, we observe spikes at lags multiple of
0.5, corresponding to a half-daily and daily seasonality.

linear decay,

one sign alternation

exponential decay, local spikes at

lags multiple of 0.5

Figure 4: YahooSlurp and MSIE 7 SACs

Interpreting

Auto-correlation interpretation.
the
shape of the SAC is traditionally a manual process,
which is left to an analyst [3]. For our system, this pro-
cess needs to be automated. To this end, we introduce
three features to describe the properties of the SAC:
speed of decay, sign alternation, and local spikes.

The speed of decay captures the short-term stability of
the trafﬁc. A slow decay indicates that the trafﬁc is stable
over longer periods whereas a fast decay indicates that
there is little stability. The speed of decay feature can
assume four values: linear decay, exponential decay, cut-
off decay (coefﬁcients reach a cliff and drop), no decay
(coefﬁcients comparable to random noise).

The sign alternation identiﬁes how often the sign of
the SAC changes. Its values are: no alternation, single
alternation, oscillation, or erratic. No or single sign al-
ternations are typical of crawlers, while user trafﬁc po-
tentially shows more alternations.

Local spikes reﬂect periodicity in the trafﬁc. A local
spike implies a repeated activity whose occurrences are
separated by a ﬁxed time difference (lag). This is typical
for user trafﬁc. This feature has two values: a discrete
spikes count plus a Boolean value indicating if spikes
are observed at interesting lags (half day, day).

Computing our features is a two-step process: First,
we compute “runs” over the auto-correlation coefﬁcients
of the SAC. A run is a sequence of zeroes and ones for
each lag k, where a one indicates that a particular prop-
erty of interest holds. An auto-correlation coefﬁcient
is characterized by four properties: positive, signiﬁcant,
null, and larger than the previous value. Runs allow us
to determine how often these properties change. This
can be done by computing various statistics (e.g. mean,
variance, length) over the related runs and their subruns
(a sequence of consecutive, identical values).

In the second step, we apply a number of heuristics to
the different runs. In particular, we compare the statistics
computed for different runs with thresholds that indicate
whether a certain property changes once, frequently, or
never. The details of how we compute the runs, as well
as the heuristics that we apply, are described in detail in
Appendix A. The heuristics provide the actual feature
values: speed of decay, sign alternation and local spikes.

Table 1: Features characterizing trafﬁc time series
Origin
Auto-correlation
analysis (SAC)

Decomposition
analysis

Feature name
coefﬁcients at lags 1 and 2,
decay, sign alternation,
number of local spikes,
daily correlation
differentiated trend IDC,
season over trend ratio

Type
2 x Continuous,
2 x Enumerate(4)
1 x Discrete
1 x Boolean
1 x Continuous,
1 x Continuous

Figure 5: YahooSlurp and MSIE 7 decompositions

3.3.2 Decomposition analysis
The events that constitute a time series are the result
of different factors that are not directly captured by the
time series; only their impact is observed. Part of these
factors slowly change over time. These factors gener-
ate slow shifts within the series that constitute its trend
component T (t). Another part of these factors repeat
over time due to periodic events. These factors gener-
ate repetitive patterns within the series that constitute its
seasonal component S(t). Unexplained short-term, ran-
dom effects constitute the remaining noise R(t).

Decomposition aims to identify the three components
of a time series such as X(t) = T (t) + S(t) + R(t). The
results of decomposition provide valuable insights into
which component has the strongest inﬂuence on the se-
ries. The decomposition is achieved using the Seasonal-
Trend decomposition procedure based on LOESS (STL).
STL moves sliding windows over the analyzed time se-
ries to compute smoothed representations of the series
using the locally weighted regression (LOESS) [6, 7].

The width of the sliding windows is chosen speciﬁ-
cally to extract certain frequencies. In our system, we
set the window width for both the trend and the seasonal
components to 6 hours. This width had to be compara-
ble to the expected seasonality of 12 or 24 hours for user
trafﬁc. The shorter width permits the extraction of com-
ponents that may slightly vary from day to day; a larger
width would not tolerate such variation. Figure 5 shows
the decomposition of the time series from Figure 2.
Trend variation. The trend components for crawlers
are often very stable, and thus, close to a square sig-
nal. To distinguish stable from noisy “signals”, we ap-
ply the differentiation operator ∇ (with a lag 1 and a
distance of 1) to the trend. For crawlers where the trafﬁc
is rather stable, the differentiated trend is very close to
a null series, with the exception of spikes when an am-
plitude shift occurs. For users, the trafﬁc shows quicker
and more frequent variations, which results in a higher
variation of the differentiated trend series. The variation
of the differentiated trend is measured using the Index of
Dispersion for Counts [12], IDC[∇T (t)]. Compared to
other statistical indicators such as the variance, the IDC,
as a ratio, offers the advantage of being normalized.
Season-to-trend ratio. Time series that correspond to
users’ trafﬁc often exhibit repetitive patterns. These pat-

terns can repeat on a daily basis, weekly basis, or other
frequencies that show some signiﬁcance in terms of the
human perception of time. Consequently, the seasonal
component is more important for user trafﬁc and likely
prevails over the trend component. This is no longer
true for crawler trafﬁc. To capture this difference be-
tween user and crawler trafﬁc, we compute the ratio be-
tween the seasonal and trend components as shown in
Equation 4. The amplitude of the seasonality compo-
nent is computed using the difference between its maxi-
mum and minimum values (as the minimum value might
be negative). The amplitude of the trend component is
measured using a quantile operation to remove outliers
resulting from possible errors in the decomposition.

rs/t =

M ax[S(t)] − M in[S(t)]
Quantile[T (t), 0.95]

(4)

3.3.3 Trafﬁc classiﬁcation
The auto-correlation and decomposition provide a set of
features that describe important characteristics related to
the trafﬁc from a given source. These features are sum-
marized in Table 1. The type column shows, for each
feature, the domain of values: continuous or discrete
numbers, Boolean values, or labels drawn from a set of
n possibilities (written as “Enumerate(n)”).

Of course, no single feature alone is sufﬁcient to un-
equivocally distinguish between crawler and user trafﬁc.
Thus, PUBCRAWL trains a combination of three well-
known, simple classiﬁers. The ﬁrst classiﬁer is a naive
Bayes classiﬁer that was trained using the maximum
posterior probability [19]. The conditional probabilities
for continuous attributes were estimated using weighted
local regression. The second classiﬁer is an association
rules classiﬁer [5]. The third classiﬁer is a support vec-
tor machine (SVM) that was trained using a non-linear
kernel function (Gaussian Radial Basis Function - RBF).
To construct an optimal hyperplane, we chose a C-SVM
classiﬁcation error function.

All three classiﬁers require a training phase. For this,
we make use of a labeled training set that contains time
series for both known crawlers and users. Each classiﬁer
is trained separately on the same training data. During
the detection phase, each classiﬁer is invoked in parallel.
To determine whether an individual source is a crawler
or a user, we use majority voting over the outputs of the
three classiﬁers.

Algorithm 1
Require: A time series set χ = X1(t), ..., Xn(t)
1: C = (cid:11)
2: for Xi(t) in χ do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12: end for
13: return clusters set C

cn = new cluster(medoid = Xi(t))
C = C ∪ cn

cm.add time series(Xi(t)

τ = k/Stdv[Xi(t)]
C∗ = candidates(C, V ol[Xi(t)], M ax[Xi(t)], Stdv[Xi(t)])
sm, cm = maxc∈C∗ ({euclidean dist(Xi(t)), c.medoid)})
if sm > τ then

else

end if

4 Crawling Campaign Attribution
In the previous section, we introduced our approach to
detect crawlers based on the analysis of trafﬁc from indi-
vidual sources. However, numerous crawlers do not op-
erate independently, but work together in crawling cam-
paigns, distributed over multiple locations. By identify-
ing such campaigns, we can provide valuable forensic
information to generate the list of blacklisted sources.

Campaign attribution is achieved by identifying
crawlers that exhibit a strong similarity between their
access patterns. To detect sources that exhibit similar
patterns, we use clustering to group similar time series
coming from detected crawlers. During the training pe-
riod, we ﬁrst determine a minimal intra-cluster similarity
required for crawlers to belong to a common campaign.
During detection, clustering results are used to identify
hosts that exhibit similar activity, and hence, are likely
part of a single, distributed crawler.
Time series similarity. A signiﬁcant amount of liter-
ature exists on similarity measures for time series [16].
Most of this body of work aims at providing a similarity
measure that is resilient to distortion, so that time series
of similar shape can be clustered. Distortion in terms of
request volume is already handled by the normalization
that we apply when deriving the time series. On the other
hand, resilience to time distortion is not desirable. The
reason is that we want to detect sources that behave sim-
ilarly, including the time domain. As a consequence, we
leverage the (inverse) Squared Euclidean Distance. This
metric is fast and known to provide good results [13].
Incremental clustering. PUBCRAWL uses an incre-
mental clustering approach to ﬁnd similar time series.
Time series coming from detected crawlers are submit-
ted one by one to our clustering algorithm described in
Algorithm 1. For each new time series, the algorithm
computes the similarity between this time series and
the medo¨ıds of existing clusters (Line 5, Algorithm 1).
When the maximal similarity, found for a given medo¨ıd,
is above a threshold τ (Line 6, Algorithm 1), the time
series is added to the associated cluster. Otherwise, the
time series becomes the medo¨ıd for a new cluster that is

created. τ corresponds to the minimal intra-cluster sim-
ilarity that the algorithm has to enforce. τ is computed
during the learning phase, based on a labeled dataset that
contains time series for distributed crawlers. Note that
τ is not ﬁxed but depends on the standard deviation to
compensate for time series proving to be more “noisy”.
To speed up the process, for each incoming time se-
ries, the function candidates (Line 4, Algorithm 1) se-
lects a subset of comparable clusters. These candidate
clusters are chosen because their medo¨ıds have a volume
of requests, an amplitude and a standard deviation that
are comparable to the new time series. We found that
this selection process signiﬁcantly reduced the number
of necessary computations (but also false positives).

Once all time series are processed, each cluster that
contains a sufﬁcient number of elements is considered to
represent a crawling campaign. Sources that are part of
this cluster are ﬂagged as parts of a distributed crawler.
5 Crawler Proactive Containment
Existing techniques to detect crawlers, including our ap-
proach, often require a non-negligible amount of trafﬁc
before reaching a decision. To address attacks in which
an adversary leverages a large number of bots for crawl-
ing, we require an additional containment mechanism.
In PUBCRAWL, the detection modules are mainly used
to produce two lists: A whitelist of IPs corresponding to
authorized users (proxies) and legitimate crawlers, and
a blacklist of IPs corresponding to suspicious crawlers.
These lists are used to enforce an access policy for the
real-time stream of requests.

Whenever the protected web site receives a request
from a source in the whitelist, the request is granted.
Sources on the blacklist are blocked. Other sources are
considered unknown, and are treated as follows.

For each source, we check the number of requests
that were generated within a given time interval (cur-
rently, one day). If this volume remains below a mini-
mal threshold k1, the source is considered a user, and its
access to the site is granted. If k1 is chosen sufﬁciently
small, the amount of leaked information remains small,
even if an attacker has many machines at their disposal.
If this same volume is above a second threshold k2,
we can use our models to make a meaningful decision
and either whitelist or blacklist this source.

When the number of requests from a source is be-
tween k1 and k2, unknown sources are exposed to ac-
tive responses such as CAPTCHAs and crawler traps.
By modifying k1 and k2, a number of trade-offs can
be made. When k1 increases, fewer users are exposed
to active responses, but it is easier for large-scale, dis-
tributed attackers to steal data. When k2 increases, our
system will be more precise in making decisions be-
tween crawlers and users but we expose more users to

active responses. In Section 6, we show that, for rea-
sonable settings of the thresholds k1 and k2, only a very
small fraction of requests and IP addresses are subjected
to active responses, while the amount of pages that even
large botnets can scrape remains small.

In practice, PUBCRAWL will only be used for “anony-
mous” requests. These are requests from users who do
not have an account on the site or have not logged in.
When a user authenticates (logs in), subsequent requests
will contain a cookie that grants direct access to the site
(and authenticated requests are rate-limited on an indi-
vidual basis). This is important to consider when dis-
cussing why IP addresses form the basis for our white-
and blacklists.
In fact, we are aware that making de-
cisions based on IP addresses can be challenging; IP
addresses are only a weak and imperfect mechanism to
identify the source of requests. However, for anonymous
requests, the IP address is the only reliable information
that is available to the server (since the client completely
controls the request content).

One problem with using IP addresses is that a ma-
licious crawler (or bot) on an infected home computer
might regularly acquire a new IP address through dhcp.
Thus, the blacklist can become stale quickly. We address
this problem by allowing each individual IP address only
a small number k1 of unrestricted accesses (before active
containment is enabled). While each fresh IP address
does allow a bot a new set of requests, IP addresses are
not changing rapidly enough so that attackers can draw
a signiﬁcant advantage. Another problem is that the IP
address of a whitelisted, legitimate proxy could change,
subjecting the users behind it to unwanted, active con-
tainment. Our experimental results (in Section 6) show
that this is typically not the case, and legitimate, high-
trafﬁc sources are relatively stable. Finally, it is pos-
sible that an attacker compromises a machine behind a
whitelisted proxy and abuses it as a crawler. To protect
against this case, our system enforces a maximum trafﬁc
volume after which the whitelist status is revoked and
the IP address is treated as unknown.

To keep the access control lists up-to-date, PUB-
CRAWL continuously re-evaluates unknown sources and
entries on the whitelist. Entries in the blacklist are ex-
pired after some days. Moreover, users ca always au-
thenticate to the site to bypass PUBCRAWL’s checks.

6 Evaluation
We implemented PUBCRAWL in Python, with an inter-
face to R [1] for the time series analysis. The system
was developed and evaluated in cooperation with a large,
well-known social network. More precisely, we used
ten days of real-world request data taken from the web
server logs of the social network, and we received feed-
back from expert practitioners in assessing the quality of

our results. The evaluation yielded a favorable outcome,
and our detection system is now integrated into the pro-
duction environment of a large social networking site.
6.1 Dataset presentation
The ten days of web server logs contain all requests to
public proﬁle pages of the social networking site we
used as a case study. Public proﬁles are accessible by
anyone on the Internet without requiring the client to
be logged in. The social network provider has expe-
rienced that almost all crawling activity to date comes
from clients that are not logged into their system. The
reason is that authenticated users are easy to track and
to throttle. Handling large volumes of non-authenticated
trafﬁc from a single source is most difﬁcult; this trafﬁc
might be the result of anonymous users surﬁng behind a
proxy, or it might be the result of crawling. Making this
distinction is not straightforward.

The log ﬁles contain eight ﬁelds for each request:
time, originating IP address, Class-C subnet, user-
agent, target URL, server response, referrer, cookie. The
IP address and the Class-C subnet ﬁelds were encrypted
to avoid privacy issues. Thus, we can only determine
whether two requests originate from the same client, or
from two clients that are part of the same /24 network.
The remaining information is unmodiﬁed. This allows
us to check for suspicious user agents, and to determine
the proﬁle names that are accessed. The server response
can be used to determine whether the visited proﬁle ex-
ists or not. In addition, the referrer indicates the previ-
ous website visited by the client. The cookie contains,
in our case, a session identiﬁer that is set by the social
networking site to track individual clients.

Data preﬁltering. Given that the social network is
very popular, the log ﬁles are large – they contain tens
of millions of requests per day that originate from mil-
lions of different IP addresses. As a result, we introduce
a preﬁltering process to reduce the data to a volume that
is manageable by our time series analyses. To this end,
we leverage the fact that, by looking at the volume of re-
quests from a single source, certain clients can be imme-
diately discarded: we can safely classify all sources that
generate more than 500,000 requests per day as crawlers.
Sources that generate less than 1,000 requests per day
are also put aside because our time-series-based tech-
niques require a minimum number of data points to pro-
duce statistically meaningful results. These sources are
handled by the active containment policy.

In our experiments, the preﬁltering process reduced
the number of requests that need to be analyzed by a
factor of two. More importantly, however, the number
of clients (IP addresses) that need to be considered is
reduced by about four orders of magnitude.

Ground truth. Obtaining ground truth for any real-
world data is difﬁcult. We followed a semi-automated
approach to establish the ground truth for our datasets.
For the initial labels, we used heuristic detection (Sec-
tion 3.1), which represents the state-of-the-art in crawler
detection. We then contacted the social network site,
which had access to the actual (non-encrypted) IP ad-
dresses. Based on their feedback, we made readjust-
ments to the initial ground truth labels. More precisely,
we ﬁrst marked sources as legitimate crawlers when they
operated from IP ranges associated with popular search
engines. In addition, IP addresses that belong to well-
known companies were labeled as users. For borderline
cases, if an IP address was originating trafﬁc from users
who successfully logged on to the site, we tagged this IP
as a user. Overall, we observed that current heuristics of-
ten incorrectly classify high volume sources as crawlers.
In a next step, we performed an extensive manual
analysis of the sources (by looking at the time series,
checking user agent string values, ...). We made a second
set of adjustments to the ground truth. In particular, we
found a number of crawlers that were missed by heuris-
tic detection. These crawlers were actively attempting to
mimic the behavior of a browser: user agent strings from
known web browsers, cookie and referrer management,
and slow runs at night. Some examples of mimicry are
discussed in Appendix B. These cases are very inter-
esting because they underline the need for more robust
crawler detection approaches such as PUBCRAWL.

Finally, we manually checked for similar time series,
and correlated this similarity with user agent strings and
class-C subnets. This information was used to build a
reference clustering to evaluate the campaign attribution.
6.2 Training detection and attribution
For training, we used a ﬁrst dataset S0, which contained
ﬁve days worth of trafﬁc recorded between the 1st and
the 5th of August 2011. After preﬁltering, this dataset
consists of ∼73 million requests generated by 813 IP
addresses. Given this number of IP addresses, manual
investigation for the ground truth was possible.
Heuristic detection. We used the training set to in-
dividually determine suitable thresholds for the detec-
tion heuristics. We veriﬁed the results of the conﬁgured
heuristics over the training set S0 with the ground truth.
The results are given in Table 2. In this table, a true pos-
itive (TP) means a correctly identiﬁed crawler. A true
negative (TN) is a correctly identiﬁed user. The results
are split between heuristics over features from existing
work and new features introduced in this paper. We can
see that the new features greatly improve the detection
rate when combined with existing ones. Still, the ﬁnal
detection rate of 75.54% illustrated the need for more
robust features.

Table 2: Training: Accuracy for heuristic detection.
Combined features
Rates
75.54%/24.46%
TP/FN
96.00%/04.00%
TN/FP

New features
82.31%/17.69%
84.00%/16.00%

Former features
41.32%/58.68%
100.00%/0.00%

Table 3: Training: Accuracy for trafﬁc shape detection.

Crawlers (TP/FN) Bayes
Cross validation
Two third split
Users (TN/FP)
Cross validation
Two third split

Vote
98.39% 96.36% 98.55% 98.99%/01.01%
97.45% 96.19% 95.11% 96.90%/03.10%
Vote
Bayes
79.09% 78.91% 81.91% 82.91%/17.09%
78.84% 78.22% 80.44% 82.28%/17.72%

Rules

Rules

SVM

SVM

Table 4: Training: Accuracy for campaign attribution.

Precision
99.03%

Recall
85.54%

Accuracy
94.35%

Trafﬁc shape detection. To train the classiﬁers for de-
tection, we used S0 that contained 709 crawler sources
and 104 user sources. To determine the quality of the
training over S0, we used both ﬁve-fold cross validation
and a validation by splitting the data into two thirds for
training and one third for testing. The results are shown
in Table 3. The table shows that our system obtains a
crawler detection rate above 96.9%. It also shows the
beneﬁt of voting, as the ﬁnal output of classiﬁcation is
more accurate than each individual classiﬁer.

Interestingly, the dataset was not evenly balanced be-
tween crawlers and users. The majority of sources
that produce more than thousand requests per day are
crawlers. However, the dataset also contains a non-
trivial amount of user sources. Thus, it is not possible
to simply block all IP addresses that send more than one
thousand requests. In fact, since the user sources are typ-
ically proxies for a large user population, blocking these
nodes would be particularly problematic. We thus veri-
ﬁed the accuracy speciﬁcally for user sources in Table 3.
Given the bias towards crawlers, the accuracy for users
is slightly lower but remains at around 83%.

It must be noticed that trafﬁc shape detection results
show interesting improvements compared to heuristic
detection that is close to the approach deployed in exist-
ing work. This approach produces more accurate results
while using features that are more robust to evasion.

Campaign attribution. For campaign attribution, the
clustering algorithm presented in Section 4 needs to be
conﬁgured with a τ that deﬁnes the minimal, desired
similarity within clusters. To determine this threshold,
we ran a bisection clustering algorithm on the training
dataset S0. The algorithm ﬁrst assumes that all ele-
ments (time series) are part of a single, large cluster.
Then, it iteratively splits clusters until each time series
is part of a single cluster. We analyzed the entire clus-
ter hierarchy to ﬁnd the reference clusters as well as
the necessary cut points to obtain them. The cut points
of reference clusters indicated us the minimal similar-
ity that we related to the deviation to determine that

k = 350 was the linear coefﬁcient giving optimal values
for τ. For the candidate selection, the following thresh-
olds were chosen: volume = 25%, amplitude = 35%,
deviation = 30%, so that these thresholds avoid ad-
ditional false positives while creating no false negatives
compared to the results without candidate selection.

To evaluate the quality of the campaign attribution, we
ran our clustering algorithm on the 709 crawler sources
from S0. We use the precision to measure how well the
clustering algorithm distinguished between time series
that are different, and the recall to measure how well
our technique recognizes similar time series. To evaluate
the successful attribution rate, we use the accuracy to
measure how well the clustering results can be used to
detect distributed crawlers and thus campaigns.

The clustering results are shown in Table 4. One can
see that the algorithm offers a good precision. The recall
is a bit lower: Closer examination revealed that a few
large reference clusters were split into multiple clusters.
For example, Bingbot had its 209 corresponding time
series split into 5 subclusters. Fortunately, recall is less
important in our attribution scenario. The reason is that
split clusters do not prevent us to detect a campaign in
most cases; instead, a campaign is simply split into sev-
eral smaller ones.
6.3 Evaluating detection capabilities
For testing, the social networking site provided an addi-
tional dataset S1, which contained ﬁve extra days worth
of trafﬁc. After preﬁltering, this dataset consisted of
∼62 million requests generated by 763 IP addresses.
Unlike the training, the testing was performed on site at
the social networking site. Hence, the trafﬁc logs could
be analyzed with non-encrypted IPs.
Heuristic detection. We compared the results for the
heuristic detection over the testing set S1 with the
ground truth we had (semi-automatically) established
previously. As shown in Table 5, the detection rate
slightly decreases to 71.60%, but remains comparable
to the rate over the training set.
Trafﬁc shape detection. To test the classiﬁers trained
over S0, we deployed the trafﬁc shape detection ap-
proach over the test set S1. The results for this exper-
iment are presented in Table 6. According to the table,
the global accuracy of 94.89% remains very close to the
95% of accuracy observed for the training set.

Since the goal of the detection module is to build
whitelists and blacklists of sources, we computed indi-
vidual results for the following four subsets: users (5%)
and legitimate crawlers (65%) to be whitelisted, and
unauthorized crawlers (7%) and masquerading crawlers
(23%) to be blacklisted. Unauthorized crawlers are
agents that can be recognized by their user agent string

Table 5: Testing: Accuracy for heuristic detection.
Combined features
Rates
71.60%/28.40%
TP/FN
94.87%/05.13%
TN/FP

New features
86.24%/14.76%
87.18%/12.82%

Former features
31.19%/68.81%
100.00%/0.00%

Table 6: Testing: Accuracy for trafﬁc shape detection.

Rules

Bayes

SVM
93.05% 87.55% 94.36%
Global
Legitimate crawlers
92.54% 87.10% 97.18%
Unauthorized crawlers 88.89% 96.27% 100.00%
Masquerading crawlers 98.27% 86.71% 98.84%
Crawlers (TP/FN)
Users (TN/FP)

Vote
94.89%
93.95%
100.00%
98.84%
93.66% 87.68% 97.79% 95.58%/04.42%
82.50% 85.00% 32.50% 82.50%/17.50%

Table 7: Testing: Accuracy for campaign attribution.

Precision
92.84%

Recall
80.63%

Accuracy
91.89%

but are not supposed to crawl the site. Masquerading
crawlers are malicious crawlers trying to masquerade as
real browsers to remain stealthy and to avoid detection.
We achieve perfect detection for unauthorized
crawlers.
In particular, the system was able to detect
crawlers such as ISA connectivity checker, Yandexbot,
YooiBot, or Exabot. Results are also very good for mas-
querading browsers, with a detection rate close to 99%.
The detection rate slightly drops to 94% for legitimate
crawlers such as Baiduspider, Bingbot, Googlebot or Ya-
hooslurp. But 4% of these false negatives are related
to Google FeedFetcher. In principle, the requests from
FeedFetcher are triggered by user requests. As a result,
its time series are individually recognized as user trafﬁc.
By combining heuristic detection and trafﬁc shape de-
tection, the detection rates were not improved further.
The reason is that the crawlers detected by heuristics
were already included in the set of crawlers detected by
trafﬁc shape. This observations conﬁrms our belief that
trafﬁc shape detection is stronger than heuristic detec-
tion based on HTML and URL features.

To gain a better understanding of our results, we asked
for feedback from the social networking site. The net-
work administrators conﬁrmed that a large number of
crawlers were previously unknown to them (and they
subsequently white- or blacklisted the IPs). Since de-
anonymized IP addresses were available to us, we could
check the sources of these crawlers. Interestingly, sev-
eral sources were proxies of universities, where crawler
trafﬁc was mixed with user activity. Because of the mix
of user and crawler activity, the current detection tech-
niques did not raise alerts. Note that such mix of ac-
tivity must be taken into consideration for blacklisting
(e.g., the university administrators can be warned that
unauthorized crawling is coming from their network and
asked to take appropriate measure). In such cases, it is a
policy decision whether to blacklist the IP or not. Also,
recall that requests from users who are logged-in is not
affected.

Performance. To process the entire dataset S1, several
instances of the modules for heuristic detection, trafﬁc
shape detection and campaign attribution were run over
ﬁve parallel processes that required roughly 45 minutes
to run. This time included loading the data into the
database, generating the time series, and performing the
different analyses. The experiments were run on a single
server (with 4 cores and 24 GB of RAM).
6.4 Evaluation of campaign attribution
To evaluate the quality of the campaign attribution tech-
nique, we ran our clustering algorithm over the crawler
sources from the de-anonymized testing set S1. The re-
sults of the experiment are shown in Table 7. One can
see that the precision and recall have slightly dropped
compared to the training set. The intra-cluster similarity
threshold τ might not be optimal anymore. Nonethe-
less, the attribution accuracy remains at 91.89%, which
is close to the 94% obtained during training.

Overall, we obtain 238 clusters from the 763 distinct
source IPs. Looking at these clusters, we started to ob-
serve interesting campaigns when a cluster contained 3-
4 or more elements (hosts). Table 8 provides a descrip-
tion of these campaigns. The ﬁrst campaigns correspond
to legitimate crawlers. Interestingly, the campaign as-
sociated to Feedfetcher, whose crawlers evaded trafﬁc
shape detection, is successfully identiﬁed. Even if the
Feedfetcher trafﬁc is similar to user trafﬁc, Google dis-
tributes the requests (i.e., load-balances) over multiple
IPs, explaining that their time series are synchronized.
Table 8 also presents ﬁve campaigns showing suspicious
user-agent strings, and ﬁve campaigns masquerading as
legitimate browsers or search engine spiders. Another
interesting case is the campaign where crawlers send
requests as Gecko/20100101 FireFox. This campaign
shows a signiﬁcant number of clusters because it uses
rotating IP addresses over short time periods. However,
it is still detected because the active IP addresses were
operating loosely synchronized. The social network op-
erators showed a particular interest in this case, and they
are now relying on our system to detect such threats that
are difﬁcult to detect otherwise.
6.5 Evaluation of proactive containment
The evaluation was performed over a dataset S2 of non-
ﬁltered trafﬁc. The dataset contained ∼110 million re-
quests from ∼11 million IP sources.

Figure 6 shows the cumulative distribution function
(CDF) for the fraction of IPs (y-axis) that send (at most)
a certain number of requests (x-axis). One can see that
most IPs (more than 98.7%) send only a single request
per day. This is important because it means that most
sources (IPs) will never be affected by active contain-
ment. Figure 7 shows the situation for requests instead

Table 8: Identiﬁed Crawling Campaigns.

#Clust. #ClassC #IP

Agent
Legitimate crawlers
Bingbot
5
Googlebot
5
+ Feedfetcher
4
Yahooslurp
4
Baiduspider
1
Voilabot
3
Facebookexternalhit/1.1
1
Crawlers with suspicious agent strings
” ”
2
Python-urllib/1.17
2
Mozilla(compatible;ICS)
1
EventMachine HTTP Client
1
Gogospider
1
Masquerading crawlers
Gecko/200805906 FireFox
Gecko/20100101 FireFox
MSIE6 NT5.2 TencentTraveler
Mozilla(compatible; Mac OS X)
googlebot(crawl@google.com)

1
9
1
1
1

Req/day

6 million
4 million
–
500 thousand
50 thousand
19 thousand
14 thousand

330 thousand
140 thousand
70 thousand
3 thousand
2 thousand

350 thousand
60 thousand
7 thousand
8 thousand
1 thousand

11
2
4
9
1
3
1

16
51
10
3
2

10
12
1
1
1

211
42
65
71
23
20
8

22
54
10
3
3

73
25
30
4
4

of IPs. That is, the ﬁgure shows the CDF for the fraction
of total requests (y-axis) over the maximum number of
requests per source (x-axis). One can see that roughly
45% of all request are sent by sources that send at most
100 requests. The graph highlights that a little over 40%
of all requests are done by sources that make only a sin-
gle request. One can also see that a signiﬁcant amount
of the total requests are incurred by a few heavy hitter
IPs that make tens of thousands to millions of requests.

Containment impact. We use these two graphs to dis-
cuss the impact of the two containment thresholds: k1,
below which sources have unrestricted access to the site,
and k2, above which sources can be examined by our
analysis (and hence, properly whitelisted or blacklisted).
We can see from Figure 6 that if we choose k1 low
enough, we can guarantee that only a tiny number of
sources will be impacted. For example, by setting k1 =
100, we see that 99.98% of the sources will not be im-
pacted at all. If an attacker wants to take advantage of
this unrestricted access, he would require 5,000 crawlers
running in parallel to reach the crawling rate of a sin-
gle crawler agent from Googlebot. Looking back at
Table 8 for real-world examples, the lowest rate of re-
quests we observed for a distributed crawler was for the
Gecko/20100101 FireFox campaign using rotating IPs.
Even in this case, the amount of requests per agent was
above a few hundreds per day. Thus, we consider values
for k1 between 10 and 100.

To choose k2, we have a trade-off between the amount
of trafﬁc that will be impacted by active responses and
the quality of our detection. We see the fraction of re-
quests that are impacted by plotting k1 and k2 over Fig-
ure 7 and reading the difference over the y-axis. Table 9
lists the proportion of impacted trafﬁc for various con-
crete settings of k1 and k2. If we keep k2 at 1,000 and
choose 20 for k1, we expect active responses to impact
less than 0.1% of all IP sources and only about 3.24%

much negative effect for (non-authenticated) users when
using active containment. Moreover, the 18.2% of non-
stable IPs are mainly due to sources whose volume was
close to the preﬁltering threshold. These sources might
have been stripped from S0 or S1 by preﬁltering.

Overall, our results demonstrate that PUBCRAWL can
thwart large-scale malicious crawlers and botnets while
interfering with a small number of legitimate requests.

7 Limitations
Our system has proven to be useful in the real-world,
and it is currently deployed by the social networking site
as a part of their crawler mitigation efforts. However,
as with many other areas in security, there is no silver
bullet, and sophisticated attackers might try to bypass
our system. Nevertheless, PUBCRAWL signiﬁcantly in-
creases the difﬁculty bar for the attackers.
Detection limitations. An attacker might try to thwart
the heuristics-based and trafﬁc-shape-based detection
modules. The trafﬁc shape detection has two main re-
quirements: 1) a large-enough volume of requests is re-
quired for the time series to be statistically signiﬁcant,
2) at least two days of trafﬁc are required for the auto-
correlation and decomposition analyses.

While trafﬁc shape detection is well-suited for detect-
ing crawlers of sufﬁcient volume, because of require-
ment 1), it is not particularly well-suited to detect “slow”
distributed crawlers that send a very small number of re-
quests from hundreds of thousands of hosts. For this,
campaign attribution is more appropriate. Because of
requirement 2), it is not well suited either to detect “ag-
gressive” (noisy) crawlers in real-time. For this, heuris-
tic detection is more appropriate.

To address the problem of “slow” and “aggressive”
crawlers, PUBCRAWL combines the detection modules
with a containment strategy. Aggressive crawlers are ini-
tially slowed down by active responses, until they are de-
tected and blacklisted. Slow crawlers can at most make
k1 requests before the active response component is ac-
tivated. Moreover, since slow crawlers require a larger
number of machines, the effect of the active response
component is magniﬁed (applied to each crawler).

Another way to avoid detection is trafﬁc reshaping.
That is, an attacker could try to engineer the crawler traf-
ﬁc so that it closely mimics the behavior of actual users.
The attacker would ﬁrst have 1) to craft valid HTTP traf-
ﬁc (headers) and 2) to design a stealthy visiting policy
both in terms of topology and timing. In terms of topol-
ogy, the attacker would have to craft a non-suspicious
sequence of URLs to visit (based on order and revisit
behavior).
In terms of timing, he would have to craft
the volume and distribution of requests over time so that
the trafﬁc shape remains similar to user trafﬁc. Overall,

Figure 6: CDF of the source IPs over trafﬁc volumes

Figure 7: CDF of the requests over trafﬁc volumes

Table 9: Requests (%) impacted by containment
10000
7.88%
7.58%
7.37%
7.19%
6.75%
6.38%

1000
3.75%
3.46%
3.24%
3.07%
2.63%
2.26%

100
1.49%
1.20%
0.99%
0.81%
0.37%
0.00%

500
2.95%
2.66%
2.44%
2.27%
1.83%
1.46%

k1/k2
10
15
20
25
50
100

2000
4.82%
4.53%
4.32%
4.14%
3.70%
3.33%

5000
6.27%
5.98%
5.77%
5.59%
5.15%
4.78%

of the overall requests. Of course, we expect that these
3.24% of requests do contain a non-trivial amount of
trafﬁc from stealthy crawlers. When k1 is increased, the
impact on legitimate users decreases. The downside is
that large botnets can scrape larger parts of the site.
Sources stability. The whitelist approach for legiti-
mate, high-volume sources (over k2) works well only
when IP sources remain stable for user proxies and le-
gitimate crawlers. To verify this assumption, we stud-
ied the IP evolution between the training set S0 and the
testing set S1. Considering crawlers, 66.9% of IPs were
both present in S0 and S1. Looking at the stable IPs, they
correspond either to legitimate crawlers (e.g., Google-
bot, Bingbot) or large crawling campaigns (e.g., Python,
Firefox from Table 8). Unstable crawler IPs correspond
to unauthorized and masquerading crawlers.

Most

importantly,

looking at high-volume users
(proxies), 81.8% of IPs were both present in S0 and
S1 (about one month apart). Thus, whitelisting these
sources would work well and, hence, we would not see

mimicking the behavior of users would require a non-
trivial effort on behalf of the attacker to learn the prop-
erties of user trafﬁc, especially given that only the social
network has a global overview of the incoming trafﬁc.
Attribution limitations.
If an attacker wants to bypass
our campaign attribution method, he has to ensure that
all nodes of his distributed crawler behave differently.
Sets of rotating IPs are already successfully detected.
To break the synchronization between the nodes, simple
time shifts would not be sufﬁcient: Existing similarity
measures for time series (e.g., dynamic time warping)
can be used to recover from shifts. An attacker would
have to completely break the synchronization between
its different crawlers while shaping for each one a dif-
ferent trafﬁc behavior (which needs to be similar to user
trafﬁc to avoid individual detection).
Containment limitations.
If attackers do not succeed
in whitelisting their crawlers, they can willingly main-
tain their trafﬁc volume under the containment threshold
k1. To crawl a good portion of a social network with mil-
lions of pages requires a signiﬁcant crawling infrastruc-
ture, and building or renting botnets over long periods
of time might be prohibitively expensive for most adver-
saries. Attackers can also increase their trafﬁc volume
until the blocking threshold k2. In this case, they would
have to ﬁnd a solution to automatically bypass active re-
sponses (resolve CATPCHAs or identify crawler traps).
8 Related Work
We are not the ﬁrst one to study the problem of detecting
web crawlers. However, we are the ﬁrst to propose a
solution to distributed crawlers, and we are the ﬁrst to
have used an extensive, large-scale real-world dataset to
evaluate and validate our approach.

Similar to [18,21,22], PUBCRAWL relies on machine
learning techniques to extract characteristic properties of
trafﬁc that can help to distinguish crawlers from users.
However, the features we use for the learning process
are different. Compared to [11, 17, 21, 22], the similar
features we extract from the HTTP headers and URLs
are fed to heuristic detection. Our experiments demon-
strate that these features are not reliable.

For trafﬁc shape detection and its learning process, we
used timing features instead. Compared to [18, 21, 22],
the results of the auto-correlation and decomposition
analyses prove to be more robust. That is, the ex-
tracted properties are harder to evade by attackers than
the simple time and volume statistics used by previous
approaches (e.g., the average or the variance of inter-
arrival times between requests).

In our detection approach, most of the features ex-
tracted from the time series are designed to express the
regularity of web trafﬁc.
In [8], the authors already

leveraged the notion of regularity of crawler trafﬁc for
detection. To extract the relevant information, the au-
thors rely on Fast Fourier Transformations. However,
both crawler and user trafﬁcs show regularities, but they
do so at different levels. We thus use decomposition
techniques to distinguish between different types of reg-
ularities: Crawler regularity can be observed within the
trend component, whereas user regularity can be ob-
served within the seasonal component.

Existing crawler detection approaches mainly remain
deployed ofﬂine – just like our detection approach based
on trafﬁc shape. However, a signiﬁcant novelty of our
approach is that we integrate our detection process into a
proactive containment strategy to protect from crawlers
in real-time. This is similar to [18] where the authors ad-
dress real-time containment. The containment approach
they propose relies on the detection results from an in-
cremental classiﬁcation system where crawler models
evolve over time.
Instead, we chose a more realistic,
practical white- and blacklisting approach where sources
are blocked on a per IP basis.

An interesting contribution of our work is the formal-
ization of the trafﬁc by time series, which allows us to
address the problem of crawling campaign attribution
and distributed crawlers detection using clustering. We
have also evaluated our tool on a much larger scale than
previous work, which has used requests that are in the
order of thousands. Finally, in our experiments, we ob-
served and identiﬁed real-world evasion techniques that
target some of the trafﬁc features used in previous work.
Hence, we provide evidence that attackers today are in-
vesting signiﬁcant effort to evade some of the straight-
forward and well-known crawler detection techniques.
9 Conclusion
To defend against malicious crawling activities, many
websites deploy heuristics-based techniques. As a re-
action, crawlers have increased in sophistication. They
now frequently mimic the behavior of browsers, as well
as distribute their activity over multiple hosts.

This paper introduced PUBCRAWL, a novel approach
for the detection and containment of crawlers. The key
insight of our system is that the trafﬁc shape of crawlers
and users differ signiﬁcantly so that they can be auto-
matically identiﬁed using time series analysis. We also
propose the ﬁrst technique that is able to identify crawl-
ing campaigns by detecting the synchronized activity of
distributed crawlers using time series clustering. Fi-
nally, we introduce an active containment mechanism
that strategically uses active responses to maximize pro-
tection and minimize user annoyance.

Our experiments with a large, popular social network-
ing site demonstrate that PUBCRAWL can distinguish
users with accuracy and ﬁlter out crawlers. Our detec-

tion approach is currently deployed in production by the
social networking site we collaborated with.
Acknowledgment
We would like to thank the people working at the social net-
work with whom we collaborated, as well as Secure Business
Austria for their support. This work was supported by the Of-
ﬁce of Naval Research (ONR) under Grant N000140911042
and the National Science Foundation (NSF) under grants CNS-
0845559, CNS-0905537 and CNS-1116777.
References
[1] The R Project for Statistical Computing.

http://www.

r-project.org/.

[2] 67th District Court, Tarrant County, Texas.

Cause NO.
Inc. vs. FareChase,
http://www.fornova.net/documents/

067-194022-02: American Airlines,
Inc.
AAFareChase.pdf, 2003.

[3] B. L. Bowerman, R. T. O’Connell, and A. B. Koehler. Forecast-
ing, Time Series, and Regression – An applied approach – Fourth
edition. Thomson Brook/Cole, 2005.

[4] E. Burzstein, R. Bauxis, H. Paskov, D. Perito, C. Fabry, and
J. Mitchell. The Failure of Noise-Based Non-Continuous Audio
Captchas. In IEEE Security and Privacy, 2011.

[5] P. Clark and T. Niblett. The CN2 Induction Algorithm. Machine

Learning, 3(4):261–283, 1989.

[6] R. B. Cleveland, W. S. Cleveland, J. E. McRae, and I. Terpen-
ning. STL: a Seasonal-Trend Decomposition Procedure based
on Loess. Journal of Ofﬁcial Statistics, 6(1):3–73, 1990.

[7] W. S. Cleveland and S. J. Devlin. Locally Weighted Regression:
an Approach to Regression Analysis by Local Fitting. J. Am.
Stat. Assoc, 83:596–610, 1988.

[8] M. D. Dikaiakosa, A. Stassopouloub, and L. Papageorgioua. An
Investigation of Web Crawler Behavior: Characterization and
Metrics. Computer Networks, 28:880–897, 2005.

[9] D. Doran and S. S. Gokhale. Discovering New Trends in Web
Robot Trafﬁc through Functional Classiﬁcation. In Proc. of the
IEEE International Symposium on Networking Computing and
Applications (NCA), pages 275–278, 2008.

[10] D. Doran and S. S. Gokhale. Web Robot Detection Techniques:
Overview and Limitations. Data Mining and Knowledge Discov-
ery, 22(1-2):183–210, 2011.

[11] W. Guo, S. Ju, and Y. Gu. Web Robot Detection Techniques
based on Statistics of their Requested URL Resources. In Proc.
of the International Conference on Computer Supported Coop-
erative Work in Design (CSCWD), pages 302–306, 2005.

[12] R. Gusella. Characterizing the Variability of Arrival Processes
with Indexes of Dispersion. ”IEEE J. Sel. Areas Commun.,
9(2):203–211, 1991.

[13] E. Keogh and S. Kasetty. On the Need for Time Series Data
Mining Benchmarks: a Survey and Empirical Demonstration. In
Proc. of the ACM International Conference on Knowledge Dis-
covery and Data Mining (SIGKDD), pages 102–111, 2002.

[14] M. Koster. A Method for Web Robots Control. Technical report,

RFC draft, 1996.

[15] M. Lamberton, E. Levy-Abegnoli, and P. Thubert. System and
Method for Enabling a Web Site Robot Trap. United States
Patent No. US 6,925,465 B2, 2005.

[16] T. W. Liao. Clustering of Time Series Data – a Survey. Pattern

Recognition, 38(11):1857–1874, 2005.

[17] X. Lin, L. Quan, and H. Wu. An Automatic Scheme to Catego-
rize User Sessions in Modern HTTP Trafﬁc. In Proc. of GLOBE-
COM, pages 1485–1490, 2008.

[18] A. G. Lourenc¸o and O. O. Belo. Applying Clickstream Data
Mining to Real-Time Web Crawler Detection and Containment
using ClickTips Platform. In Advances in Data Analysis, Proc. of
the 30th Annual Conference of the Gesellschaft f¨ur Klassiﬁkation
e.V., pages 351–358. Springer, 2007.

[19] A. McCallum and K. Nigam. A Comparison of Event Models
for Naive Bayes Text Classiﬁcation. In AAAI FICML Workshop
on Learning for Text Categorization, 1998.

[20] Pinsent Masons.

Ryanair wins German court victory in
screen-scraping injunction. http://www.theregister.
co.uk/2008/07/11/ryanair_screen_scraping_
victory/, 2008.

[21] A. Stassopoulou and M. D. Dikaiakos. Crawler detection: A
bayesian approach. In Proc. of the International Conference on
Internet Surveillance and Protection (ICISP), 2006.

[22] P.-N. Tan and V. Kumar. Discovery of Web Robot Sessions based
on their Navigational Patterns. Data Mining and Knowledge Dis-
covery, 6(1):9–35, 2002.

[23] Tech Crunch.

Hacker

other

ing StudiVZ and
//eu.techcrunch.com/2009/10/21/
hacker-arrested-for-blackmailing-studivz-
and-other-social-networks/, 2009.

social

arrested

for
networks.

blackmail-
http:

[24] L. von Ahn, M. Blum, N. Hopper, and J. Langford. The
CAPTCHA Project. Technical report, Carnegie Mellon Univer-
sity, 2000.

[25] P. Warden.

How I got sued by Facebook.

http:

//petewarden.typepad.com/searchbrowser/
2010/04/how-i-got-sued-by-facebook.html,
2010.

A Interpreting auto-correlation functions
This section describes how we compute the values of the three
features describing the shape of the Sample Auto-Correlation
Function (SAC). We ﬁrst compute runs over four SAC char-
acteristics: trend (increase/decrease), sign (positive/negative),
signiﬁcance (spikes), and null (lags with null correlation). The
run computations are shown in Equations 5-8 for the auto-
correlation coefﬁcient rk that ranges over all k lags.

Trend:

trk =

Sign:

srk =

Signiﬁcance: prk =

Null: nrk =

(cid:40)
(cid:40)
(cid:40)
(cid:40)

1 if k = 1 or |rk| ≥ |rk−1|
0 else
1 if rk ≥ 0
0 else
1 if |rk| ≥ 2 × sk
0 else
1 if |rk| ≤ 0.1 × max{|rk|}
0 else

(5)

(6)

(7)

(8)

Based on the previously-computed runs (as well as ampli-
tude values), we determine the three properties of the SAC: Ta-
ble 10 lists the heuristics to determine the decay, Table 11 lists
the heuristics to determine the sign alternation, and Table 12
lists the heuristics to identify local spikes.
B Crawlers mimicking user behavior
Table 13 shows examples of crawlers that we found in our
dataset. The ﬁrst example in the table represents an easy
crawler to detect: The user-agent points to the Python pro-
gramming framework, which is popular among crawlers, no

Characteristic Metric
Amplitude

Table 10: Heuristics to determine the speed of decay of the SAC

difference between
ﬁrst and last lags
mean of the
runs values
number of runs

mean of the
runs values
difference between
ﬁrst and middle lags
length of the
ﬁrst run
length of the longest
null valued run
mean of the
runs values
lag of the ﬁrst
positive run
difference between
ﬁrst and middle lags
number of runs

length of the
ﬁrst run
length of the
ﬁrst run
mean of the
runs values
-

Expected Value
low

average

high

low

above
average
low

high

above
average
low

below
average
low

above
low
above
low
low

Interpretation
no overall decrease in the
amplitude of the coefﬁcients
balanced proportion between
increases and decreases
high number of inﬂections
points in the function
observed coefﬁcients are
not residuals
quick decrease in the
amplitude of the coefﬁcients
coefﬁcients become insigniﬁcant
after a short number of lags
the function contains long intervals
of insigniﬁcant coefﬁcients
the function coefﬁcients tend
quickly towards 0
the function converges towards
0 in the small lags
slow decrease in the
amplitude of the coefﬁcients
low number of inﬂections
points in the function
coefﬁcients remain signiﬁcant
after a longer number of lags
coefﬁcients remain signiﬁcant
after a longer number of lags
the function coefﬁcients tend
slowly towards 0
all decaying functions that are
neither cut off nor linear

Feature Value
No decay
(white noise)

Cut-off

Linear
decay

Exponential
decay

Trend runs

Null runs

Amplitude

Spike runs

Null runs

Amplitude

Trend runs

Spike runs

Spike runs

Null runs

-

Table 11: Heuristics to determine the sign alternation property of the SAC
Feature Value
No alternation
Single alternation
Oscillations

Characteristic Metric
Sign runs
Sign runs
Sign runs

number of runs
number of runs
number of runs

Expected Value
equal 1
equal 2
above
average
below
average

Interpretation
no sign change
single sign change
important number of
sign changes
sign changes occur
at regular periods
sign changes are
unpredictable

variance of the
runs length
-

-

Erratic
alternation

Table 12: Heuristics to determine the local spikes of the SAC

Characteristic Metric
Spike runs

lags of positive runs
length of positive runs

Expected Values
above 1
low

Interpretation
ﬁrst spike must be ignored
short spikes centered around a lag

Feature Value
Local spike
(lag, length)

iﬁed to look similar to user trafﬁc. Example 6 interrupts its
activity at night to avoid raising suspicion. Examples 6 and 7
show a high fraction of revisited pages, which is more typi-
cal for human behavior. These examples indicate that crawlers
already attempt to bypass existing detection heuristics.

referrer is set, the cookies transmitted by the server are ig-
nored, the dispersion index is low, indicating regular trafﬁc,
and the error rate is too high to be generated by a user fol-
lowing links. The next examples introduce different evasion
techniques by mimicry. Examples 2 to 7 use user-agent string
masquerading. Examples 4 and 5 set their referrer to the result
of a directory query, whereas Example 7 uses mainly proﬁles
URL as referrers, just like during human browsing. Exam-
ples 6 to 8 handle cookies with different policies: Examples 6
and 7 reuse the same cookie for a ﬁxed number of requests,
whereas Example 8 reuses the same cookie for a ﬁxed period of
time. More advanced techniques of trafﬁc shaping can also be
found, where the timing and the targeting of requests is mod-

.
s
r
e
l
w
a
r
c

t
n
e
r
e
f
f
i
d

r
o
f

d
e
v
r
e
s
b
o

s
e
u
q
i
n
h
c
e
t

n
o
i
s
a
v
E

:
3
1

e
l
b
a
T

.
s
t
p
m
e
t
t
a

n
o
i
s
a
v
e

o
t
d
n
o
p
s
e
r
r
o
c

t
h
g
i
m
y
e
h
t

,
c
ﬁ
f
a
r
t

r
e
s
w
o
r
b

o
t

e
s
o
l
c

e
r
a

t
a
h
t

c
ﬁ
f
a
r
t

r
e
l
w
a
r
c

e
h
t

f
o

s
e
i
t
r
e
p
o
r
p

d
e
v
r
e
s
b
o

o
t

d
n
o
p
s
e
r
r
o
c

s
l
l
e
c

y
e
r
G

c
ﬁ
f
a
r
T

s
r
o
r
r
E

p
a
l
r
e
v
O

]
)
t
(

[

X
C
D

I

e
i
k
o
o
C

r
e
r
r
e
f
e
R

e
c
r
u
o
S

%
7

.

2
1

%
0

.

0
0

9
7

.

7
1
5

e
s
u
e
r

e
i
k
o
o
c

o
n

r
e
r
r
e
f
e
r

l
l
u
n

%
4

.

1
0

%
3

.

1
0

9
4

.

0
1

e
s
u
e
r

e
i
k
o
o
c

o
n

r
e
r
r
e
f
e
r

l
l
u
n

%
4

.

0
0

%
0

.

0
0

7
9

.

2
0
5
7

e
s
u
e
r

e
i
k
o
o
c

o
n

r
e
r
r
e
f
e
r

l
l
u
n

%
2

.

0
0

%
2

.

0
0

2
1

.

6
4
2

e
s
u
e
r

e
i
k
o
o
c

o
n

%
3

.

0
0

%
1

.

0
0

2
3

.

0
3
4

e
s
u
e
r

e
i
k
o
o
c

o
n

%
3

.

3
0

%
7

.

1
1

5
7

.

9
6
6
9

%
3

.

0
0

%
3

.

9
2

5
3

.

4

%
2

.

1
0

%
7

.

2
0

2
4

.

9
5

e
i
k
o
o
c

e
s
u
e
r

3

t
x
e
n

r
o
f

s
t
s
e
u
q
e
r

e
i
k
o
o
c

e
s
u
e
r

0
0
1

t
x
e
n

r
o
f

s
t
s
e
u
q
e
r

y
r
e
u
q

y
r
o
t
c
e
r
i
d

l
l
a
(

r
e
r
r
e
f
e
r

s
a

h
t
i

w
g
n
i
t
r
a
t
s

)
r
e
t
t
e
l

e
m
a
s

y
r
e
u
q

y
r
o
t
c
e
r
i
d

l
l
a
(

r
e
r
r
e
f
e
r

s
a

h
t
i

w
g
n
i
t
r
a
t
s

)
r
e
t
t
e
l

e
m
a
s

r
e
r
r
e
f
e
r

l
l
u
n

f
o
%
6

.

1

y
l
n
o

,
s
r
e
r
r
e
f
e
r

l
l
a

s
i

l
l
u
n

t
s
e
r

s
e
l
ﬁ
o
r
p

c
i
l
b
u
p

s
e
i
k
o
o
c

e
s
u
e
r

s
t
s
e
u
q
e
r

r
o
f

n
i
m
5
1

a

n
i

l
a
v
r
e
t
n
i

r
e
r
r
e
f
e
r

l
l
u
n

7
0
1
5
B
0
9
4
0
B
6
E
8
6
F
F
A
8
D
E
0
F
D
3
B
3
8
C
D
4
E
D
7
7
8
1
D
E
2
B

:
g
n
i
r
t
s

t
n
e
g
a
-
r
e
s
U

7
1

.

1
/
b
i
l
l
r
u
-
n
o
h
t
y
P

)
.
.
.

v
r

;

S
U
-
n
e

;
1

.

6
T
N
s
w
o
d
n
i
W

;

U

;
s
w
o
d
n
i
W

(

:
g
n
i
r
t
s

t
n
e
g
a
-
r
e
s
U

0

.

4
/
a
l
l
i
z
o
M

6
8
5
C
2
5
0
9
A
5
5
E
E
1
0
2
5
0
B
F
B
9
E
7
F
F
6
9
7
0
D
5
C
9
E
F
9
B
D
8

8
5
F
B
C
8
2
C
9
4
3
B
C
F
8
3
9
E
5
F
2
1
6
C
4
0
A
F
1
6
2
5
A
F
4
1
B
0
3
A

)
1

.

5
T
N
s
w
o
d
n
i
W

;
0

.

7
E
I
S
M

;
e
l
b
i
t
a
p
m
o
c
(

:
g
n
i
r
t
s

t
n
e
g
a
-
r
e
s
U

0

.

4
/
a
l
l
i
z
o
M

4
E
4
C
E
4
4
B
C
8
7
7
D
B
B
D
2
9
B
3
0
7
6
9
6
5
9
B
A
1
E
8
4
5
4
4
B
D
3
4

)
1

.

5
T
N
s
w
o
d
n
i
W

;
0

.

7
E
I
S
M

;
e
l
b
i
t
a
p
m
o
c
(

:
g
n
i
r
t
s

t
n
e
g
a
-
r
e
s
U

0

.

4
/
a
l
l
i
z
o
M

)
.
.

V.
S

;
1

.

5
T
N
s
w
o
d
n
i
W

;
0

.

6
E
I
S
M

;
e
l
b
i
t
a
p
m
o
c
(

:
g
n
i
r
t
s

t
n
e
g
a
-
r
e
s
U

0

.

4
/
a
l
l
i
z
o
M

B
2
4
D
E
4
C
7
9
E
8
1
4
4
1
6
1
A
1
B
2
A
7
F
C
A
0
0
7
1
C
9
8
0
F
D
7
A
2
6

F
3
8
0
7
B
2
D
A
2
3
3
C
D
6
8
4
1
7
6
8
0
6
B
A
0
7
0
2
9
A
9
B
6
C
0
8
6
E
3

)
1

.

0

.

9

.

1
:
v
r

;

S
U
-
n
e

;
6
8
6
i

x
u
n
i
L

;

U

;
1
1
X

(

:
g
n
i
r
t
s

t
n
e
g
a
-
r
e
s
U

0

.

5
/
a
l
l
i
z
o
M

2
5
A
E
B
9
3
E
5
0
0
E
F
1
1
2
4
C
2
0
0
7
9
7
0
F
1
C
4
9
2
3
5
8
E
9
8
C
D
2

)
.
.
.

T
N
s
w
o
d
n
i
W

;
2
b
0

.

5
E
I
S
M

;
e
l
b
i
t
a
p
m
o
c
(

:
g
n
i
r
t
s

t
n
e
g
a
-
r
e
s
U

0

.

4
/
a
l
l
i
z
o
M

7
A
9
E
9
0
E
F
B
D
E
1
7
3
8
7
D
0
6
2
7
1
5
6
F
7
4
A
A
0
9
A
3
B
4
F
E
B
C
3

)
.
.
.
a
m
b
e
w
/
p
l
e
h
/
e
t
i
s
/

m
o
c
.
a
x
e
l
a
.
w
w
w

/
/
:
p
t
t
h
+
(

r
e
v
i
h
c
r
a

a
i

:
g
n
i
r
t
s

t
n
e
g
a
-
r
e
s
U

:
P
I

:
P
I

:
P
I

:
P
I

:
P
I

:
P
I

:
P
I

:
P
I

