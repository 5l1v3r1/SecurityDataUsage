Principled Sampling for Anomaly Detection

Brendan Juba

Washington University in St. Louis

bjuba@seas.wustl.edu

Christopher Musco, Fan Long,

Stelios Sidiroglou-Douskos and Martin Rinard

Massachusetts Institute of Technology

{fanl,cpmusco,stelios,rinard}@csail.mit.edu

Abstract—Anomaly detection plays an important role in pro-
tecting computer systems from unforeseen attack by automati-
cally recognizing and ﬁlter atypical inputs. However, it can be
difﬁcult to balance the sensitivity of a detector – an aggressive
system can ﬁlter too many benign inputs while a conservative
system can fail to catch anomalies. Accordingly, it is important
to rigorously test anomaly detectors to evaluate potential error
rates before deployment. However, principled systems for doing
so have not been studied – testing is typically ad hoc, making it
difﬁcult to reproduce results or formally compare detectors.

To address this issue we present a technique and implemented
system, Fortuna, for obtaining probabilistic bounds on false
positive rates for anomaly detectors that process Internet data.
Using a probability distribution based on PageRank and an
efﬁcient algorithm to draw samples from the distribution, Fortuna
computes an estimated false positive rate and a probabilistic
bound on the estimate’s accuracy. By drawing test samples from
a well deﬁned distribution that correlates well with data seen in
practice, Fortuna improves on ad hoc methods for estimating false
positive rate, giving bounds that are reproducible, comparable
across different anomaly detectors, and theoretically sound.

Experimental evaluations of three anomaly detectors (SIFT,
SOAP, and JSAND) show that Fortuna is efﬁcient enough to use
in practice — it can sample enough inputs to obtain tight false
positive rate bounds in less than 10 hours for all three detectors.
These results indicate that Fortuna can, in practice, help place
anomaly detection on a stronger theoretical foundation and help
practitioners better understand the behavior and consequences
of the anomaly detectors that they deploy.

As part of our work, we obtain a theoretical result that may
be of independent interest: We give a simple analysis of the
convergence rate of the random surfer process deﬁning PageRank
that guarantees the same rate as the standard, second-eigenvalue
analysis, but does not rely on any assumptions about the link
structure of the web.

I.

INTRODUCTION

Anomaly detection systems are critical components of
many security systems. By recognizing, then discarding, sani-
tizing, or otherwise nullifying outlier inputs that might other-

wise exploit security vulnerabilities, anomaly detectors often
play a central role in many computer security systems.

In general, however, anomaly detectors are not perfect.
Speciﬁcally, anomaly detectors typically navigate a trade off
between two kinds of errors:
False Positives - Type I error A Type I error occurs when
an anomaly detector (incorrectly) rejects a benign input. A
high false positive rate can signiﬁcantly impair the utility
of an anomaly detector — each false positive denies some
part of the functionality of the system to the user.

False Negatives - Type II error A Type II error occurs
when an anomaly detector (incorrectly) accepts a mali-
cious input, leaving the system open to attack.

In general, making an anomaly detector more sensitive in-
creases the false positive rate and decreases the false nega-
tive rate (and vice-versa). Appropriately balancing these two
rates is therefore essential in obtaining an effective anomaly
detector. Practitioners typically tune their anomaly detectors
by running the detector on a (representative) set of inputs to
develop an intuitive understanding of how the anomaly detector
will operate in practice. Current techniques are ad-hoc, not
guided by any theoretically well-founded framework or anal-
ysis, and therefore provide no guarantees on the effectiveness
of the anomaly detector when deployed in production and no
guidance on how to effectively test the anomaly detector to
determine if it will work well in practice.

We present a technique and implemented system, Fortuna,
that, for the ﬁrst time, provides bounds on the number of false
positives that a given anomaly detector will incur in practice.
Fortuna focuses speciﬁcally on anomaly detection systems that
handle data from browsing the Internet. This class of systems is
very well studied and includes, for example, anomaly detectors
that ﬁlter potentially anomalous video, images, and JavaScript
ﬁles [18], [20], [28], [36], [38] or seek to ﬁlter entire malicious
webpages [44], [47], [48]. Nevertheless, our approach is broad
and we provide a general framework for building analysis
systems for other classes of anomaly detectors.

B. Juba was afﬁliated with Harvard University when this work was

performed.

A. False Positive Bounds

Permission to freely reproduce all or part of this paper for noncommercial
purposes is granted provided that copies bear this notice and the full citation
on the ﬁrst page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the ﬁrst-named author
(for reproduction of an entire paper only), and the author’s employer if the
paper was prepared within the scope of employment.
NDSS ’15, 8-11 February 2015, San Diego, CA, USA
Copyright 2015 Internet Society, ISBN 1-891562-38-X
http://dx.doi.org/10.14722/ndss.2015.23268

Speciﬁcally, Fortuna’s sampling algorithm collects a se-
quence of randomly chosen benign inputs that enables Fortuna
to derive probabilistic bounds of the following form:

• One-Sided Bound: For anomaly detectors with no false
positives in the sequence of test inputs that the sampling
algorithm generates, Fortuna provides a bound of the form
P r(err(1) < ) > 1 − δ, where err(1) is the actual Type
I error (false positive rate) that the anomaly detector will

incur in practice,  is an upper bound on this false positive
rate, and 1− δ is the certainty with which Fortuna is able
to provide this bound.
For example, with a sequence of 46,052 sampled inputs
and no false positives, Fortuna can show that the proba-
bility that the false positive rate is less than 0.01% is at
least 99%.
• Two-Sided Bound: For anomaly detectors with some
false positives in the sequence of test inputs that the
sampling algorithm generates, Fortuna provides a bound

of the form P r(|err(1) − (cid:102)err(1)| < ) > 1 − δ, where
the anomaly detector will incur in practice, (cid:102)err(1) is the
(cid:102)err(1), and 1 − δ is the certainty with which Fortuna is

(empirical) Type I error for the sequence of sampled
inputs,  is a bound on the difference between err(1) and

err(1) is the actual Type I error (false positive rate) that

able to provide this bound.
For example, with a sequence of 26,492 sampled inputs,
Fortuna produces an estimate of the false positive rate
that is within 1% of the true false positive rate with 99%
probability.

These bounds are based on standard statistical inequalities
and are a function of the number of inputs that Fortuna’s
sampling algorithm is directed to obtain, with the bounds be-
coming tighter as the number of inputs increases. Speciﬁcally,
for the one-sided bound, the number of required samples is
proportional to (1/) · log(1/δ); for the two-sided bound, the
number of required samples is proportional to (1/2)·log(1/δ).
Fortuna therefore enables practioners to determine, ahead of
time, how many test inputs are required to obtain a desired
bound. Our experimental results show that, in practice, Fortuna
is easily able to obtain enough samples to provide tight bounds
(see Section VI).

B. Accurate Sampling

To provide accurate false positive bounds, Fortuna’s sam-
pling algorithm delivers inputs from a probability distribution
for benign inputs that accurately models the distribution the
anomaly detector will encounter in practice.1 We note that the
standard approach of collecting a large set of inputs from an
available source such as the Internet, then computing the false
positive rate over this input set, does not provide an accurate
estimate of the false positive rate — in general, the anomaly
detector is more likely to encounter some inputs than others,
with signiﬁcant dependence on the ad hoc collection method
selected. To obtain accurate bounds, Fortuna must select inputs
from a concrete probability distribution over benign inputs that
has two properties:

• Reﬂects Production Use: The probability distribution
reﬂects the distribution of inputs that the anomaly detector
will encounter in production use.
• Efﬁcient Sampling Algorithm: The probability distribu-
tion supports an efﬁcient sampling algorithm that Fortuna
can use to generate its random sequence of inputs.

1The false positive rate does not depend on the distribution of malicious
inputs — each malicious input is either a true negative or a false negative.
Each benign input, in contrast, is either a true positive (when the anomaly
detector accepts the input) or a false positive (when the anomaly detector
rejects the input).

2

Fortuna is speciﬁcally designed to work with systems that
process inputs that a typical user would see when browsing
the Internet. Thus, it uses a probability distribution based on
PageRank (ﬁrst deﬁned by Page et al. [45] for ranking web
pages). The original motivation for PageRank was to weight
web pages according to user desirability in order to provide rel-
evant search results to a user (cf. [32, p.4]). At the same time,
PageRank was also designed to be computationally tractable.
A complete justiﬁcation for our selection of PageRank can be
found in Section II.

We are not interested in computing PageRank scores (prob-
abilities) for all pages, but rather in sampling according to the
PageRank distribution. It turns out that generating samples is
even easier than computing the scores, which enables Fortuna
to generate large enough sequences to provide tight false
positive bounds. Speciﬁcally, a characterization of PageRank
distributions that ﬁrst appeared in the work of Andersen et
al. [11] can be interpreted as a simple sampling algorithm. We
develop a system based on this algorithm for rapidly sampling
image ﬁles from a PageRank distribution. The system only
requires access to freely available data, the web, and simple
scripts.

Note that there is a crucial difference between the prob-
ability distributions of malicious and benign inputs. Because
malicious inputs are speciﬁcally designed to target (in many
cases not publicly known) program vulnerabilities, and because
malicious inputs evolve rapidly in response to countermeasures
(such as deployed anomaly detection systems), it is not typi-
cally possible to obtain reasonable probability distributions for
malicious inputs. For this reason, Fortuna does not attempt to
provide bounds about the false negative rate that the anomaly
detector will encounter in production use. Our focus is in
line with previous literature on anomaly detection – in light
of limited access to malicious inputs, systems are typically
designed to handle a ﬁxed set of malicious examples and,
during evaluation, false positive rate is used as the main quality
metric.

C. Experimental Results

trigger target

the input will not

We evaluate Fortuna on three anomaly detectors:

the
SIFT [38], SOAP [37], and JSAND [18] anomaly detectors:
• SIFT: SIFT uses a conservative static program analysis
to obtain a set of constraints on the values of ﬁelds in
JPG and PNG image ﬁles processed by png2swf [8],
jpg2swf [8], and Dillo [5].
If an input satisﬁes the constraints, SIFT guarantees
that
integer overﬂow
errors [38]. Because the SIFT analysis is conservative,
it may reject
trigger the error.
We consider an input ﬁle to be a false positive if it 1)
violates the constraints but 2) does not trigger the integer
overﬂow error. The results show that the SIFT anomaly
detector can effectively guarantee the absence of integer
overﬂow errors at critical memory allocation and block
copy sites in our benchmark applications, including at
least six potentially exploitable errors.
• SOAP: SOAP learns characteristics of ﬁelds in JPG
and PNG image ﬁles that are processed by applications
such as ImageMagick [7] and Dillo [5]. Speciﬁcally,

inputs that would not

SOAP processes input ﬁles that the applications handle
successfully to infer upper bound constraints of integer
ﬁelds, sign constraints of integer ﬁelds, and upper bound
constraints of data ﬁeld lengths [37].
We consider an input ﬁle to be a false positive if it 1)
violates the learned constraints but 2) the application can
process the input ﬁle successfully. SOAP also has the
capability to rectify such ﬁles (i.e., modify the ﬁle so
that it satisﬁes the learned constraints [37], then pass the
rectiﬁed ﬁle along to the application). The results show
that the SOAP anomaly detector/input rectiﬁer effectively
nulliﬁes six potentially exploitable errors in the sample
applications [37].
We trained the SOAP anomaly detector on several thou-
sand input ﬁles (5130 PNG and 3386 JPG) drawn from
Fortuna’s PageRank probability distribution. These train-
ing ﬁles are disjoint from the set of ﬁles used to evaluate
the anomaly detector and compute the false positive rate
bounds.
• JSAND: JSAND is a popular, publicly available anomaly
detector for JavaScript. JSAND uses a variety of machine
learning techniques to detect “Drive-by download” at-
tacks, a common form of JavaScript malware that attempts
to automatically download malicious software onto a
users computer.
JSAND provides a widely used Internet interface that
allows a user to submit a JavaScript program to the
JSAND anomaly detector [9]. The JSAND anomaly de-
tector classiﬁes the submitted program as either normal,
suspicious, or malicious. We analyze two cases: the ﬁrst
considers suspicious ﬁles as false positives (aggressive
ﬁltering), the other considers them benign (conservative
ﬁltering).

We report results for the three anomaly detectors on sample
inputs drawn from the Internet according to the PageRank
probability distribution (see Section V). Over the course of
less than 10 hours, Fortuna was able to sample over 40,000
JPG input ﬁles, over 60,000 PNG input ﬁles, and over 8,000
JavaScript programs from the Internet according to the Page-
Rank distribution. On the resulting JPG and PNG input ﬁles,
SIFT encountered no false positives. SOAP encountered fewer
than 1,000 false positives on these same ﬁles. JSAND encoun-
tered 12 suspicious and 0 malicious JavaScript programs. The
resulting false positive rate bounds are quite tight. Speciﬁcally,
on all programs evaluated, Fortuna guarantees that SIFT has
a false positive rate below 0.011%, with 99% conﬁdence. The
false positive rate for SOAP is 1.99% ± 0.83% for JPEG ﬁles
and 0.29%±0.67% for PNG ﬁles, both two-sided bounds with
conﬁdence 99%. For JSAND, the false positive rate is less
than 0.52% with conﬁdence 99% if conservative ﬁltering is
performed (i.e. suspicious inputs are not rejected). If aggressive
ﬁltering is used (i.e. suspicious inputs are considered malicious
and rejected), it is necessary to use the somewhat weaker
two-sided bound, which guarantees the false positive rate is
0.14%± 1.73% with conﬁdence 99%. These results show that
it is very feasible to use Fortuna in practice to obtain tight
bounds on false positive rates for anomaly detectors.

D. Contributions

This paper makes the following contributions:

• Technique: We present a technique for obtaining tight
probabilistic bounds on the false positive rates that
anomaly detectors will encounter when deployed in pro-
duction use. This technique is, to the best of our knowl-
edge, the ﬁrst technique to provide any bound whatsoever
on anomaly detector false positive rates.
• Distribution and Sampling Algorithm: One of the
keys to obtaining tight bounds is obtaining a probability
distribution that reﬂects the distribution of inputs that
the anomaly detector will encounter in practice. At the
same time, this probability distribution must also support
a sampling algorithm that is efﬁcient enough to use in
practice. This paper presents a combination of probability
distribution and sampling algorithm that satisﬁes both of
these constraints. The distribution models inputs encoun-
tered by a typical user browsing the Internet (currently
one of the most common input sources for anomaly
detectors).
• Analysis: We present a formal analysis of the sampling
algorithm and resulting false positive guarantees. This
analysis includes a review of well known statistical tail
inequalities, which characterize how many sample inputs
are required to obtain a desired false positive bound
(whether one-sided or two-sided).
• Implementation: We present Fortuna, a system that im-
plements the PageRank sampling algorithm using freely
available data and computes resulting probabilistic false
positive rate bounds.
• Experimental Results: We present false positive rate
bounds for three different anomaly detectors: the SOAP
and SIFT anomaly detectors for JPEG and PNG input
ﬁles and the JSAND anomaly detector for JavaScript
programs. These results indicate that Fortuna’s sampling
algorithm is efﬁcient enough to obtain enough inputs for
providing tight bounds for practical anomaly detectors.

Anomaly detectors are a critical component of modern
computer security systems. However, despite the central role
they often play in such systems, there has been little to no
formal analysis that enables practitioners to better understand
the accuracy and effectiveness of their anomaly detectors. By
providing guaranteed probabilistic bounds on the false positive
rates that anomaly detectors will incur in practice, Fortuna
can help practitioners better understand the consequences of
deploying these powerful security tools.

II. THE CASE FOR PAGERANK

Fortuna computes bounds on Type 1 error by testing an
anomaly detector on inputs drawn from a chosen distribution.
The resulting bounds are thus meaningful with respect to data
drawn from that distribution. So to obtain bounds that are
useful in practice, it is important to choose a distribution that
is as consistent as possible with inputs an anomaly detector
will analyze in practice. At the same time, we need to rapidly
obtain many samples from the distribution – obtaining tight
bounds will require tens of thousands of sample inputs.

The tradeoff here is clear: a more sophisticated model or
involved data collection process may provide more accuracy,
but at
the cost of increased sampling difﬁcultly. For our
intended application – anomaly detectors that process web
data – we argue that PageRank balances this tradeoff and

3

thus provides an ideal distribution for testing. Furthermore,
its relative simplicity and ease of use suggests that PageRank
could become a benchmark for web-data anomaly detectors,
replacing ad hoc analysis methods and increasing consistency
and reproducibility across results.

Nevertheless, we stress that PageRank is just one possi-
ble viable distributions for testing anomaly detectors. When
deploying an anomaly detector for use beyond web data,
choosing an alternative distribution would be essential. Addi-
tionally, even within our chosen application domain, depending
on intended application and available computational and data
resources, alternative distributions could be more appropriate
for testing. Our goal is to provide a framework for computing
bounds given any chosen distribution and we offer PageRank
as a simple, easily implemented, yet powerful example.

A. A Brief Introduction to PageRank

A more complete treatment of the PageRank distribution,
including mathematical deﬁnitions and a theoretical discussion
of our sampling algorithm, is included in Section IV. We
present an abbreviated introduction here.

The PageRank distribution was ﬁrst presented by Page et
al. [45] and is the backbone of Google’s search result ranking
algorithm. Seeking to weight pages by importance, PageRank
has been extensively studied in theory and practice thanks
to its relative simplicity, accuracy, and (it turns out) utility
for other purposes such as local graph partitioning and robust
eigenvector approximations, for example [13], [32], [11], [39].
Prior to our work, PageRank distributions had not, however,
previously been used as a test distribution for evaluating ADS
systems.

To understand the PageRank distribution, consider the
following “random surfer” process for randomly accessing
pages on the World Wide Web:

(a) Begin by picking a starting webpage uniformly at random

from all possible pages.

(b) If the current page has no outgoing links, jump to a page
selected uniformly at random from all possible pages.
Otherwise, with probability α choose a link from the page
you are currently on uniformly at random, and follow that
link; otherwise (with probability (1− α)), jump to another
page selected uniformly at random from all possible pages.
Typically α is set to .85.

This Markov process loosely captures the behavior of a typical
Internet user. The PageRank of a URL is then taken to be the
fraction of visits to that page during the random surfer process
as the number of steps of the process goes to inﬁnity, i.e.,
the long-run fraction of time spent on that URL. Intuitively,
webpages with more incoming links tend to have higher
PageRank because the random surfer has more possible ways
of getting to the page. It turns out that moreover, PageRank
also captures a notion of the quality of these links. Speciﬁcally,
links from high PageRank sources are more important because
they are more likely to be taken than links from a page that is
not visited often. This “quality by association” is the important
insight behind PageRank, which has empirically outperformed
other notions of importance on the web.

4

B. Merits of PageRank

We claim that computing Type I errors with respect to
a PageRank distribution is reasonable for two reasons. First,
we argue that PageRank has been successful, in practice, at
capturing the relative importance of webpages (cf. Langville
and Meyer [32, pp.4,25]). It is therefore reasonable to use
PageRank to weight the importance of Type I (false positive)
errors on various webpages: false positives on more important
pages are more serious, as they are more likely to be visited.
Second, we point out that the random surfer process is a
reasonable synthetic model for the behavior of an average
user. The corresponding PageRank distribution, by deﬁnition,
captures precisely the long-run distribution over pages visited
by the random surfer, and therefore is a reasonable synthetic
model for the long-run distribution over pages visited by an
average user.

Fig. 1: PageRank Score (logarithmic scale) vs. Alexa Ranking
up to ∼ 105 pages, out of ∼ 109–1010 total. Warmer (higher)
colors indicate high density and thus reveal the trend curve.

In support of this second claim, we collected data on
approximately 100, 000 of the Web’s most visited webpages
[2], comparing their PageRank score (provided by Google [6])
and their global trafﬁc ranking (provided by Alexa Internet
[1]). Figure 1 presents this data in a 2-dimensional histogram,
which indicates that PageRank decreases in correlation with
reduced trafﬁc rank (a trafﬁc rank of 1 is best). Noise in
the plot can be attributed to several factors – for example,
trafﬁc numbers tend to be quite volatile (signiﬁcant changes
daily) while PageRank captures a more long term measure
of importance. In addition, PageRank numbers as provided
by Google are given on a logarithmic scale – a PageRank
score of x + 1 is twice is good as a PageRank score of x.
This property limits the precision of our y-axis. Finally, Alexa
trafﬁc rankings should only be considered representative of
true trafﬁc numbers in a coarse grained sense [19, pp.38].

Nevertheless, we are interested in the overall

trend –
PageRank correlates positively with trafﬁc ranking and thus

roughly captures visit frequencies while providing a long term
measure of page importance.

have approved the study’s protocol. As such, it highlights the
forbidding challenges inherent in such a large user study.

Of course, this point would be moot if it were as expensive
to sample from the PageRank distribution as it is to conduct,
for example, an large-scale (statistically signiﬁcant) user study
that captures actual visit frequencies for various webpages. As
we explain below, potentially more representative distributions
are (prohibitively) expensive to sample from, especially in
comparison to the efﬁcient method presented in Section IV-B
for PageRank sampling.

C. Alternatives to PageRank

There are several alternatives to our choice of the PageRank
distribution for web data. PageRank is a computed based on
link structure between webpages so it is not generally identical
to the relative rates of trafﬁc over various webpages, and as
we review next, other techniques produce data that may more
closely match actual trafﬁc numbers. While we argue that
sampling from other distributions is generally prohibitively
expensive, the bounds we present in Section III could in theory
be applied to any of these distributions. In particular:

1) Large-scale User Studies: An ideal choice for obtaining
test data for anomaly detectors for the web would be to obtain
data directly from real users. Data from trafﬁc ranking sites,
such as Alexa, unfortunately is not ﬁne grained enough for
our purposes since domains are tracked instead of individual
URLs. Furthermore, such data is typically only available for
the Internet’s most popular sites (e.g. top 1 million domains
out of > 950 million active domains in the case of Alexa [10]).
In order to obtain a representative sample of data, a user
trafﬁc study would have to cover a large population over a
long period of time. Such a study was actually conducted
by Meiss et al. [41] over about seven months during 2006–
07 in their work on the quality of the random surfer as a
user model. Their methodology involved collecting all of the
trafﬁc across the gateway for Indiana University’s Bloomington
campus; needless to say, it would have been difﬁcult to obtain
individualized informed-consent for the monitored population.
Nevertheless, they note that the users’ IP addresses have been
deleted and the study received approval from the university’s
Institutional Review Board.

As effective as their technique is for obtaining a large,
relatively high-quality sample of data, two subsequent devel-
opments cast doubt on whether or not such studies would
still receive IRB approval in the absence of user consent.
The ﬁrst is that work on deanonymization, as developed by
Narayanan and Shmatikov [12], [42], [43], demonstrates that
merely deleting trivial unique identiﬁers (such as IP addresses)
is not an adequate measure for protecting user privacy. The
second is that shortly after the publication of the Meiss et al.
study, whistleblowers at Indiana University alleged numerous
cases of noncompliance by the Bloomington Ofﬁce of Human
Research Protections [34]. In particular, it was alleged that
protocols were misreviewed, personal data had been released
without subjects’ consent, and subjects’ protections were “at
the bottom of the list of [the directors’] concerns.” While
the Meiss et al. study is never mentioned in any of the
released portions of the allegations, the affair clearly does raise
the question of whether a properly functioning IRB would

Furthermore, even with informed consent, several other
issues complicate user study design. For example, it is likely
that users’ browsing habits will change substantially if they
explicitly consent to monitoring (a human Heisenberg effect).
It is also difﬁcult to access an unbiased population of users
willing to participate in a trafﬁc study. Such concerns have
arisen in criticism of Alexa and other trafﬁc ranking ser-
vices who seek to provide much coarser data [19]. Although
user studies still seem likely to provide higher quality data
than modeling techniques like PageRank, even they are only
approximations to the “true” trafﬁc distribution we seek to
address.

We brieﬂy note that these concerns do not arise in situations
where users have weaker than usual expectations of privacy
and necessarily consent to outside monitoring of their surﬁng,
such as in a government or corporate setting. In such settings,
it would be possible to simply monitor the browsing behavior
of the entire population to collect the data for training and
evaluation of an anomaly detector. Again, the bounds we use
still apply in this case, with the actual user data substituted for
the data we sample from PageRank.

Otherwise, the disadvantages of direct data collection via
user studies suggest that it might be desirable to use a rep-
resentative model to generate synthetic trafﬁc data. Although
PageRank is one of the most popular, alternative models exist,
and we discuss one potentially more accurate option next.

2) The ABC Model: In a follow-up to the work on compar-
ing the random surfer model to real user data, Meiss et al. [40]
proposed a more sophisticated user model, the “ABC model.”
This model generates traces in a similar way to PageRank’s
random surfer model, but it simulates features such as back
buttons, user bookmarks, and decaying “user interest.” Meiss et
al. [40] demonstrate that it produces trace data that, in several
ways, agree with real user traces better than PageRank. This
includes prediction of aggregate page trafﬁc: they ﬁnd that
while the distribution of webpage trafﬁc produced by the ABC
model, PageRank, and the empirical data all roughly follow
power-law distributions, the ABC distribution more closely
matches with the empirical power law distribution.

Thus, if data quality is the most important consideration,
the ABC model may be more suited to testing web-data based
anomaly detectors than our PageRank model. The ABC model
would be a good choice in situations where, in addition to
the quality of data being the primary concern,
large-scale
user studies are simply infeasible. As mentioned, it could be
used with the general framework we present for obtaining
quantitative error bounds from a set of samples.

Unfortunately, improved accuracy is achieved at a sig-
niﬁcant cost per sample. Unlike PageRank, the ABC model
is inherently non-Markovian and thus the usual convergence
analyses do not apply. In fact, it is not clear from work on the
model that the long-run distribution over pages ever converges.
Even if the ABC model converges, it is not empirically known
how long one should simulate the ABC model for before
obtaining samples of page visits. Naturally, a “burn in” period
would be required to ensure that samples are sufﬁciently in-
dependent from the chosen start page. PageRank is considered

5

to mix very quickly – nevertheless, if it is sampled by simply
simulating the described random surfer model and taking a
sample after initial burn in, over 120 steps (page downloads)
would be necessary for selecting each unbiased test URL. In
contrast, the optimization we obtain from the reformulation of
Andersen et al. [11], which is unique to PageRank’s deﬁnition,
requires about 7 page downloads on average. It would be an
interesting direction for future theoretical work to see if the
long-run distribution of ABC model (or another similarly close
approximation to the real trafﬁc distribution) could also be
sampled so efﬁciently.

III. STATISTICAL BACKGROUND

Before giving an in-depth theoretical treatment of Page-
Rank and introducing our proposed sampling algorithm, we
review standard probability techniques that Fortuna will use
to compute its false positive rate bounds. In particular, we
explain how to determine the required number of test inputs
for obtaining a bound on the false positive rate (one-sided or
two-sided) to within any desired accuracy.

All of the error bounds described have been known for
quite some time in statistics and machine learning. Our
contribution is applying them to anomaly detection for the
ﬁrst time and, more importantly, providing a distribution and
accompanying sampling algorithm that is efﬁcient enough to
effectively apply the bounds presented.

A. Bounds on Sample Size

Let S be the space of all possible inputs to a program.
We will model
the benign inputs as having been drawn
(independently) from a distribution D, with support X ⊆ S;
that is, X is the space of benign inputs one could possibly
encounter in practice. Although in practice inputs are not
independent, we are only interested in averages over a long-
run distribution over inputs, and not in the order in which
the inputs were encountered. Equivalently, we could consider
deﬁning D to be an average over time of the (correlated) inputs.
We will see later that it is legitimate to treat the examples as
being independently distributed from a ﬁxed distribution for
the actual distribution we sample from—that is, that we can
sample directly from the “long-run” inputs of a synthetic input
distribution.

Let p : S → [0, 1] be the probability density function of D,
and let T : S → {0, 1} be the indicator function for harmful
inputs, i.e., labeling each s ∈ S as either benign (denoted by
0) or harmful (denoted by 1). Then, since X is the subset of
S (with nonzero density under D) containing benign inputs,
T (x) ≡ 0 for all x ∈ X. Now, suppose we have access to an
anomaly detection function F : S → {0, 1}. F is an attempt
to approximate T .

To capture differences between F and T , note that |F (x)−
T (x)| = 0 if the functions agree for an input x and |F (x) −
T (x)| = 1 if they disagree. While T is uniformly 0 on X, the
same is not necessarily true for F . We deﬁne:

Deﬁnition 1 (Type I Error over D):

err(1)D (F ) = E

D[F|X] =

(cid:88)

x∈X

p(x)
p(X)

· F (x)

6

where p(X) denotes(cid:80)

x∈X p(x).
is infeasible to compute err(1)D (F )
Unless X is small,
exactly. However, given access to n independent draws from
D, {x1, . . . , xn}, we can estimate err(1)D (F ) by
Deﬁnition 2 (Empirical Type I Error over D):

it

(cid:102)err(1)D (F, n) =

n(cid:88)

i=1

1
n

F (xi)

(cid:102)err(1)D (F, n) =

n is the empirical frequency of xi from our sample, and thus
1
replaces p(x) in the expectation. Observe that, for a random
variable X(cid:48) denoting the conditional distribution of D given
the input is benign (i.e., lies in X) F (X(cid:48)) is just a Bernoulli
random variable with parameter err(1)D (F ) so, we can rewrite

n(cid:88)
where each yi ∼ B(err(1)D (F )).
When our data is generated from D, (cid:102)err(1)D (F, n) →
err(1)D (F, n) as n → ∞. Accordingly, we can obtain a better
estimate of F ’s false positive rate by selecting a larger set of
test inputs. The exact size of the sample required is given in
the following:

n(cid:88)

F (xi) =

1
n

1
n

Theorem 3 (Hoeffding’s bound [27]): For

(0, 1], if n ≥ ln(2/δ)
from D, then with probability 1 − δ over x1, . . . , xn,

∈
and x1, . . . , xn are drawn independently

any , δ

22

i=1

i=1

yi

(cid:107)(cid:102)err(1)D (F, n) − err(1)D (F )(cid:107) ≤ 

it guarantees

If we set δ = .05 and  = .01, Theorem 3 tells us how to
set n so that we can make the following statement with 95%
conﬁdence: “F ’s true false positive rate on data generated from
D is within an additive 1% of our estimated false positive rate”.
that, with

high probability, (cid:102)err(1)D (F, n) > err(1)D (F ) − δ and that
(cid:102)err(1)D (F, n) < err(1)D (F ) + δ. Unfortunately, obtaining tight

This bound is “two-sided”:

two-sided bounds is expensive. The number of examples
required by Theorem 3 scales as Ω(1/2), which grows rapidly.
Since ADS systems today aim to provide very low false
positive rates of .01% or less, it is often infeasible to collect a
sample of the required size in practice. For example, for 99%
conﬁdence and an error bound of .01%, we would need more
than 264 million examples. Theorem 3 is still very helpful
for estimating the quality of systems with higher false positive
rates. For 99% conﬁdence and an error bound of 1%, a sample
of size n = 39, 120 sufﬁces. We will apply it to such a system
([37]) in Section VI.

To certify the quality of systems with very low false-
positive rates, we switch to a “one-sided” bound, following
a standard calculation from learning theory (e.g., appearing in
Theorem 2.2 of Kearns and Vazirani [29, p.36]):

if we draw n ≥

Theorem 4: For any , δ ∈ (0, 1],

(1/) ln(1/δ) examples from D and
err(1)D (F ) > 

efﬁcient enough to enable Fortuna to obtain tight false positive
bounds based on the theorems from Section III.

Again, while the error bounds for sampling are well known,
the difﬁculty in making use of them is that we require test data
that has been drawn from a distribution that is representative of
the real distribution over benign inputs one would encounter.
As obtaining such data may be expensive, many authors have
had to settle for test data that was selected in some other
manner. As noted by Chandola et al. [16], the use of such
synthetic data may undermine the quality of the evaluation.
As discussed in Section II, we avoid this issue in the case of
anomaly detectors that process data from the web. PageRank
provides a distribution over web data that is both easy to
sample from and representative of the inputs.

A. Formal Properties of PageRank

Building on the overview given in Section II-A, we formal-
ize the random surfer process described by using a “random
walk” matrix, A. (We never construct this matrix explicitly.
It is only used to analyze the process). If n is the number of
pages on the web, A is an n × n real-valued matrix. Every
entry in A lies in [0, 1] and speciﬁcally, Ai,j is the probability
of moving from page j to page i. For pages that j links to,
this probability should equal:

α

degree(j)

+

1 − α

n

If j has links, but the page i is not directly linked to from j,
Ai,j simply equals:

1 − α

n

and if degree(j) is 0, Ai,j should simply equal 1
n for all
i. If we sum up entries in any row of A, the total should
be 1. Each row gives a probability distribution on webpages,
conditioned on our current position. That is, A is a “row
stochastic” matrix.

Actually, the process described above describes one of
many possible PageRank distributions. In practice, it is neither
feasible nor (it turns out) desirable to use precisely this random
surfer process. First, the web is constantly growing and may
contain pages or subsets of pages not linked to by any other
page. And second, it is believed that Google combats attempts
at manipulating the PageRank by explicitly modifying the
distribution (cf. the case of Google vs. SearchKing [32, p.54]).
Thus, in practice one never performs a true random jump to a
uniformly selected page from the web. Instead, one typically
selects a ﬁxed “teleportation” vector v that approximates the
uniform distribution over all (legitimate) webpages. At each
random jump of the surfer process, we move to page i with
probability vi. In such a case, v contains uniform probabilities
over a certain large proportion of webpages known to exist,
with zeros everywhere else. We will use such a teleportation
distribution in our sampling implementation (see Section V).
As long as v is rigorously speciﬁed with a reasonably large
support,
the resulting distribution over web pages is still
representative. Furthermore, we will use a distribution over
“seed” webpages that is public and simple to access, allowing
for consistency with papers that seek to employ a sampling
strategy similar to ours.

It will be helpful to separate v’s contribution to A from
the effect of actual links. Thus, deﬁne P to be another n × n
matrix with entries in [0, 1]. Pi,j = 0 if j does not link to i
and 1/degree(j) is j does link to i.

With v and P chosen, we can deﬁne our random surfer
model as a random walk process. Suppose e is the all ones
vector. Then we can write:

A = αP + (1 − α)ev(cid:62)

(1)

Let st be the probability distribution over webpages after
step t of our walk. So, st(i) is the probability of being at page
i after t steps. Since the walk starts with a standard teleport,
s0 = v.

Furthermore, if A holds the conditional probabilities spec-
iﬁed, st = s(cid:62)
0 At. The
PageRank distribution is the limit of this random walk process.
Speciﬁcally,

t−1A. Expanding this recurrence, st = s(cid:62)

Deﬁnition 8 (PageRank): The PageRank vector s for ran-
dom walk matrix P, teleportation distribution vector v and
α ∈ [0, 1) is given by

s(cid:62) = lim

t→∞ s(cid:62)

(2)

0 At
for s0 = v and A = αP + (1 − α)ev(cid:62).
The actual existence of the limit s(cid:62) is a standard property
of PageRank and follows from noting that, restricted to
the webpages reachable from the none zero elements of v,
the PageRank random walk forms an irreducible, aperiodic
Markov chain (see for example Haveliwala and Kamvar [26]
or Farahat et al. [22]).

One is generally not only interested in whether or not the
Markov chain converges to the stationary distribution, but also
how quickly, in case one is using the power method to compute
the PageRank vector. This is especially important
in case
one is not computing the entire vector, but instead sampling
from the vector by performing a random walk according
to the random surfer distribution; in this case, a bound on
the convergence rate is essential since one cannot test for
numerical convergence (as is usually done when computing
PageRank vectors in practice). Unfortunately, the state of the
literature in this regard is quite poor. The original work of Page
et al. [45] incorrectly references an analysis of an undirected
Markov chain; subsequent work on the convergence rate of
the power method that corrected this error, e.g. Haveliwala and
Kamvar [26], still assumed that the matrix A is diagonalizable,
which in general is not true. (This assumption is expressly
noted by Langville and Meyer [32, p.164].) Speciﬁcally, there
is no guarantee that the transition matrix of a directed Markov
chain has a full basis of eigenvectors.

In spite of these analytical snags, the power method works
rather well in practice, as well as predicted by these “second
eigenvalue” analyses. For completeness, we now offer a direct
proof that is similar to the standard arguments (e.g., [13], [14])
but does not rely on diagonalizability (or even the existence of
a second eigenvector) and obtains the same convergence rate
as a “second-eigenvalue” analysis of the power method. We
therefore ﬁnd that none of these assumptions are necessary to

8

explain the fast rate of convergence of the power method in
practice.

Theorem 9: Suppose s(cid:62) = limt→∞ s(cid:62)

0 At for A = αP +
(1 − α)ev(cid:62) with α ∈ [0, 1), a probability distribution v, and
(cid:62)At is within 2αt of s(cid:62)
a random walk matrix P. Then s0
under the (cid:96)1-norm.

Proof: Expanding our recurrence gives:

t = s(cid:62)
s(cid:62)
= αs(cid:62)
= αs(cid:62)

t−1A
t−1P + (1 − α)s(cid:62)
t−1P + (1 − α)v(cid:62)

t−1ev(cid:62)

(3)
t−1(cid:107)1 = 1 since
The last step follows because s(cid:62)
t−1 is a probability distribution. Similarly, since s(cid:62) = s(cid:62)A,
s(cid:62)
s(cid:62) = αs(cid:62)P + (1 − α)v(cid:62). Thus

t−1e = (cid:107)s(cid:62)

(s − st)(cid:62) = α(s − st−1)(cid:62)P

(4)

Since t was arbitrary, by induction on t we conclude that

(s − st)(cid:62) = αt(s − s0)(cid:62)Pt

(5)
Now, since they are probability distributions, (cid:107)s(cid:107)1 = (cid:107)s0(cid:107)1 =
1. By the triangle inequality, (cid:107)s−s0(cid:107)1 ≤ 2. Furthermore, since
Pt is a row stochastic transition matrix, (cid:107)Pt(cid:107)1 = 1. Combined
with the triangle inequality this gives:

(cid:107)(s − s0)(cid:62)Pt(cid:107)1 ≤ (cid:107)s − s0(cid:107)1(cid:107)Pt(cid:107)1

= (cid:107)s − s0(cid:107)1
≤ 2

It follows that (cid:107)αt(s− s0)(cid:62)Pt(cid:107)1 ≤ 2αt. Thus, from Equation
5, the (cid:96)1 norm of our error from s is bounded by(cid:107)s − st(cid:107)1 ≤
2αt.

The basic paradigm for using PageRank in answering
search queries (described by Page et al. [45]) is to ﬁrst compute
a PageRank vector for the entire web, second ﬁlter out the
pages that do not contain the requested terms, and then output
the remaining pages, ordered by their PageRank. We can view
this ﬁltering step as conditioning the PageRank distribution on
the presence of the requested terms. This is how we will deﬁne
distributions over speciﬁc types of ﬁles:

Deﬁnition 10 (PageRank for a ﬁle type): For a ﬁle type τ
and PageRank vector s, we deﬁne s|τ to be the vector equal
to s(i) if i is of type τ and 0 otherwise. Then the PageRank
distribution over ﬁles of type τ is sτ = s|τ /(cid:107)s|τ(cid:107)1.

B. Sampling from PageRank

Since it is deﬁned as a limit, it seems reasonable to compute
an estimate for s by simply taking t to be a large number
and applying A to an arbitrary starting vector t times. This
process is analogous to the power method for ﬁnding the
top eigenvector of a diagonalizable matrix. It was originally
suggested by Page et al. [45] and has been analyzed and
applied throughout the literature. Since the error (2αt) shrinks
exponentially with the number of steps, the power method is
fast and is the standard means for computing PageRank.

However, it turns out that sampling from the PageRank
distribution is even simpler. In fact, it is possible to sample
from the distribution exactly, with no error. We ﬁrst note that

the PageRank distribution can be reformulated as a random
walk of a random (geometrically-distributed) length, a fact
essentially observed by Andersen et al. [11]. Recall that a
geometrically distributed random variable of parameter p is
a random variable that counts the number of times a p-biased
coin must be tossed before a ‘heads’ is obtained. Precisely,
it takes value i with probability (1 − p)pi, for every integer
i ≥ 0.

Given a parameter α, random walk matrix P, and teleporta-
tion vector v, consider the distribution over webpages sampled
as follows:

1) Sample t from a geometric distribution of parameter α.
2) Sample i from v, and take a t-step random walk started

from i according to P.
Theorem 11: The sampling algorithm produces a webpage
from the PageRank distribution with parameter α, random
walk matrix P and teleportation vector v. Furthermore, it only
requires visiting 1/(1 − α) pages in expectation.

Proof: Our analysis is based on an observation of Ander-
sen et al. [11]. The distribution of web pages output by the
sampling algorithm is given by the vector

∞(cid:88)

s(cid:48)(cid:62)

=

(1 − α)αtv(cid:62)Pt.

t=0

We show s(cid:48) is the PageRank distribution with parameter α,
random walk matrix P and teleportation vector v. As we have
already established existence and uniqueness of the PageRank
vector, we only need to verify that the geometric random walk
distribution s(cid:48) is a ﬁxed point of the map

A(x) = x[αP + (1 − α)ev(cid:62)]

that takes an additional random step according to the random
surfer process. Thus, by linearity:

A(s(cid:48)(cid:62)

) =

=

(1 − α)αtv(cid:62)Pt

[αP + (1 − α)ev(cid:62)]

(1 − α)αtv(cid:62)Pt(αP)

+ (1 − α)v(cid:62)

(1 − α)αtv(cid:62)Pt

+ (1 − α)α0v(cid:62)P0

The expected number of steps of the random walk is given
exactly by

P[#steps ≥ t] =

αt =

α
1 − α

t=1

t=1

Since the number of pages visited is the number of steps of
the random walk plus one initial page from the teleportation
distribution, this is 1/(1 − α) pages in total.

Sampling from the PageRank distribution for a speciﬁc
ﬁle type is easily accomplished as long as the ﬁle type is
sufﬁciently common. We can sample from sτ for a ﬁle type τ
by using the previous algorithm to sample from the PageRank

9

(cid:32) ∞(cid:88)
(cid:32) ∞(cid:88)
(cid:32) ∞(cid:88)

t=0

t=0

=
= s(cid:48)(cid:62)

t=1

∞(cid:88)

(cid:33)

(cid:33)

(cid:33)

∞(cid:88)

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

Parameters:

α: PageRank random jump probability
Fmt: File format to extract

Return:

Downloaded files

random_walk(α,Fmt) {

N ← N + 1

N ← 0
while (rand(0,1) < α):
U ← get_random_url()
cnt ← 0
while (cnt < N):

U ← get_random_outgoing_link(U )
if (U is empty) then:
cnt ← cnt + 1

U ← get_random_url()

return fetch_file_from_url(U ,Fmt)

}

Fig. 3: Random walk algorithm

distribution s repeatedly until we draw a URL for a ﬁle of
type τ.

Theorem 12: The above sampling algorithm produces ﬁles

from the PageRank distribution over ﬁles of type τ.

Proof: Notice, the probability of this algorithm returning
the ith URL is 0 if it is not a ﬁle of type τ and otherwise is
proportional to s(i). Since the algorithm induces a distribution
over ﬁles, the probability that the ith URL is output must
be precisely s(i)/(cid:107)s|τ(cid:107)1, which we see is the PageRank
distribution over ﬁles of type τ.

C. Stability of PageRank

Our PageRank distribution is partially computed using a
random walk on the actual web. The web is not static, however:
its link structure is constantly changing over time. The utility
of our PageRank distribution as a common benchmark for
generating reproducible results would be limited if it were
too sensitive to these changes in the web’s structure. Fortu-
nately, we note that it is quite insensitive to these changes.
Langville and Meyer provide a nice review of the stability
of PageRank [32, Chapter 6]—very roughly, the effect of a
change to the links on a page is proportional to that page’s
PageRank, which is small for most pages. Indeed, PageRank
can be viewed as implicitly being “regularized” [39] and hence
very stable to changes in either the web’s link structure or the
choice of teleportation vector.

V. DESIGN AND IMPLEMENTATION

We next discuss our implementation of Fortuna to estimate
Type I errors by sampling from the PageRank distribution
using the the efﬁcient algorithm of Theorem 11. Fortuna is
implemented in approximately 700 lines of Python and it uses
Common Crawl [3], a regularly updated snapshot of the public
Internet, as the PageRank sampling source.

A. Sampling from PageRank

Common Crawl is a public repository, hosted on Amazon’s
Elastic Cloud, that builds and maintains a snapshot of the web.
To avoid the cost of building an index from scratch (in the
range of $10000 with current Amazon pricing 2), Fortuna uses

2http://aws.amazon.com/s3/pricing

an existing copy of the Common Crawl URL Index provided
by triv.io [4]. The URL Index is implemented as a preﬁxed
B-tree that provides the ability to efﬁciently search the index
by URL, URL preﬁx, subdomain or top-level domain. The
currently available index is approximately 220 GB in size,
representing approximately 2.3x109 URLs.

Fortuna uses the Common Crawl URL index to approxi-
mate an uniform distribution of the entire web (i.e., telepor-
tation distribution v in Section IV-B). As described in Sec-
tion IV-B, Fortuna samples random pages from the URL index
as the starting page for a random walk (i.e, sample random
pages from teleportation distribution v in Section IV-B).

B. Random Walk Algorithm

Figure 3 presents Fortuna’s random walk algorithm. The
algorithm takes as input the PageRank parameter α, and the
desired ﬁle format Fmt. It ﬁrst computes the number of
required walk steps, N, based on a geometric distribution with
parameter α at lines 8-10 in Figure 3 (see Section IV-B). It
simply ﬂips a biased coin (with heads probability (1 − α))
until heads comes up.

The algorithm extracts a random URL from the URL Index
(get_random_url() at line 11) to seed the random walk. Next,
the algorithm downloads and parses the URL to extract all html
links links. To simulate the random walk, a link U is randomly
selected from links , and the walk count cnt is incremented. If
the URL does not contain any link, the algorithm will teleport
to another random URL from the URL Index (lines 15-16). The
process is repeated using U as the seed URL and continues
until the algorithm has performed enough steps (i.e., cnt < N).
Once enough steps have been performed, the algorithm parses
the last URL U and examines the HTML for links pointing to
ﬁle of format Fmt (line 18). If U contains more than one ﬁle
of format Fmt, one is randomly selected.

Note that

this algorithm can be parallelized easily. To
efﬁciently download ﬁles Fortuna uses a distributed crawler
infrastructure where workers (on multiple machines) run the
random walk algorithm and store results to a centralized
database.

VI. EXPERIMENTAL RESULTS

We

evaluate Fortuna on three

anomaly detectors:
SOAP [37], SIFT [38], and JSAND [18] on three input
formats: JPEG and PNG (SOAP and SIFT) and JavaScript
(JSAND).

1) SOAP [37] is an input rectiﬁcation system that learns
input constraints over a set of benign inputs and then
enforces the learned constraints over the incoming inputs.
In our experiments for SOAP, we count a false positive
as occurring if SOAP rectiﬁes a (benign) collected input.
2) SIFT [38] statically analyzes an application and generates
sound input constraints, so that any input that satisﬁes
these constraints will not trigger integer overﬂow errors at
memory allocation and block copy sites of the application.
3) JSAND [18] is a publicly available anomaly detector for
JavaScript. It has a web interface [9] where users can
submit JavaScript programs. The system will generate
a report for each submitted program and classify the
program as normal, suspicious, or malicious.

10

A. Methodology
Collect Sample Inputs: Fortuna uses its sampling algorithm
to collect JPEG, PNG, and JavaScript ﬁles from the Internet.
Using this sampling algorithm, Fortuna collected 42299 JPEG
ﬁles, 64089 PNG ﬁles, and 8853 JavaScript ﬁles in less than
10 hours.
Set up Anomaly Detectors: In our experiments, we applied
SIFT to dillo [5] and png2swf [8] for PNG ﬁles, as well as
jpeg2swf [8] for JPEG ﬁles. We applied SOAP to dillo [5]
for PNG ﬁles, as well as ImageMagick [7] for JPEG ﬁles. We
selected these applications because they were the benchmark
applications in the original papers of SOAP [37] and SIFT [38].
For the SIFT experiments, we ran the SIFT static analysis
on the source code of the applications to obtain our input
constraints. For the SOAP experiments, we randomly selected
5130 PNG ﬁles and 3386 JPEG ﬁles from the collected ﬁles
as training examples to generate our input constraints.
Test Anomaly Detectors: We tested the constraints that SOAP
and SIFT generated on the collected JPEG and PNG ﬁles. We
excluded the ﬁles we selected for training examples in the
SOAP experiments. We also tested JSAND on the collected
JavaScript ﬁles. We report the false positive rate bounds that
Fortuna computed for these anomaly detectors.

Note that the distribution sampled by Fortuna differs in
practice from the ideal PageRank distribution in two ways.
First, we do not visit some pages indicated by /robots.txt
ﬁles. And second, when a website requires a login (e.g., a
social network or a paywall) our “random surfer” cannot follow
the link. These are unavoidable, inherent limitations of any
automated system for collecting inputs from the web.

B. Results

Table I summarizes our experimental results. There is a row
in the table for each combination of anomaly detection system
and application (SIFT) or anomaly detection system and input
ﬁle format (SOAP and JSAND). The ﬁrst column lists the
anomaly detection system we are testing, which is either SIFT,
SOAP, or JSAND. The second column indicates the input
format, which is either JPEG, PNG, or JavaScript. The third
column indicates the application utilizing the inputs (thus, the
target of the analysis) in each SOAP or SIFT experiment. The
fourth column indicates the number of training example inputs
used in each SOAP experiment. The ﬁfth column presents the
number of collected inputs used for testing in each experiment.
The sixth column presents the number of false positives we
encountered in each experiment.

The seventh column presents the corresponding theoretical
bounds on the false positive rate for the anomaly detection
system in each experiment. Each bound is either of the form
“err(1) < X%” (one-sided) or of the form “X% ≤ err(1) ≤
Y %” (two-sided). For the systems with zero observed false
positives – SIFT and JSAND with conservative ﬁltering (i.e.
JavaScript that triggers strong warnings is rejected) – we use
Theorem 5, obtaining a bound of the form, “the Type I error
is < X%.” For the systems for which false positives were
observed – SOAP and JSAND with aggressive ﬁltering (i.e.
JavaScript that triggers any warning is rejected) – we use
Hoeffding’s bound (Theorem 3) to obtain a bound of the form,

“the Type I error is between X% and Y%.” In all cases, we
used a conﬁdence bound of 99% (δ = 1%).

Note that SOAP has a higher false positive rate that SIFT
or JSAND. A higher false positive rate is acceptable for
SIFT because it does not discard false positives. It instead
applies input rectiﬁcation, which changes the input ﬁle but
still presents the rectiﬁed input ﬁle to the application.

C. Comparison to Ad Hoc Methods

To emphasize the importance of using PageRank in place
of an ad hoc crawling method, we also collected data via the
web crawling method used to originally gather test data for the
SOAP anomaly detector [37]. The method initiated a breadth
ﬁrst web crawl from a chosen site, downloading all ﬁles in all
descendent sites. This sort of method has been widely used
for collecting data for testing anomaly detectors in the past.
In a separate test from that used for Table I, we trained SOAP
using 7000 JPEG ﬁles before evaluating on PageRank data
and data collected from the ad hoc method. The later approach
signiﬁcantly underestimated false positive rate in comparison
to testing with PageRank – for ImageMagick we saw a rate of
0.66% in comparison to a rate of 1.15%. Similarly, we trained
SOAP using 3999 PNG ﬁles. Again, testing with the ad hoc
method signiﬁcantly underestimated false positive rate – for
dillo we saw a rate of 0.14% in comparison to 0.41% for
PageRank. Using a standard Pearson’s chi-squared test, these
experiments were determined to be statistically signiﬁcant to
within a p-value of .01.

We conclude that the naive method originally employed
for testing SOAP does not provide an adequate test for false
positive rate. Speciﬁcally, it seems likely that the method does
not collect a wide enough variety of data, thus underestimating
false positive rate. PageRank, on the other hand, ensures wide
coverage of the web.

VII. RELATED WORK

Intrusion Detection: Errors or vulnerabilities in deployed
software are often triggered by atypical inputs that the appli-
cation code does not properly process or reject. Initially intro-
duced by Denning [21], Anomaly Detection techniques [20],
[31], [44], [46], [47], [48], [49], [51] learn over a set of
benign inputs to generate a classiﬁer that can detect potentially
malicious inputs to the system. For example, Kruegel et al. [31]
learn character distributions of benign HTTP trafﬁc to detect
potential malicious HTTP requests. Wang et al. [51] propose
a similar technique that looks at character distributions in
payloads to detect network intrusions. Ntoulas et al. [44]
propose to detect malicious web pages by learning features
like word and anchor text length. Cova et al. [18] statically
analyze benign JavaScript programs to extract features such
as string lengths and character distributions to detect mali-
cious JavaScript programs. SOAP [37] is an automatic input
rectiﬁcation system that learns a set of input constraints from
training inputs and then enforces these learned constraints
by rectifying any input that violates the learned constraints.
Chandola et al. [16] summarizes a good overview on these
anomaly detection systems.

Anomaly detection systems have also been built that rely on
static analysis of application source code [23], [25], [38], [50],

11

System

SIFT
SIFT
SIFT
SOAP
SOAP

File

Format
JPEG
PNG
PNG
JPEG
PNG

JSAND w/ Conservative Filtering
JSAND w/ Aggressive Filtering

JavaScript
JavaScript

Target

Application

# Training
Examples

jpg2swf

dillo

png2swf

ImageMagick

dillo
N/A
N/A

N/A
N/A
N/A
3386
5130
N/A
N/A

# Test
Inputs
42299
64089
64089
38913
58959
8853
8853

# Encountered
False Positives

Bound on False Positive

Rate (with 99% conﬁdence)

0
0
0
775
172
0
12

err(1) < 0.0108%
err(1) < 0.0072%
err(1) < 0.0072%

1.16% ≤ err(1) ≤ 2.82%
0% ≤ err(1) ≤ 0.96%

err(1) < 0.0520%

0% ≤ err(1) ≤ 1.87%

TABLE I: Experimental Results. We obtain strong bounds on Type I error (False Positive Rate) for anomaly detectors processing
several ﬁle types. Each bound holds with 99% conﬁdence.

[15]. Speciﬁcally, researchers have developed techniques that
use pushdown automata (PDA) to model invalid program paths
that can only occur if being exploited [23], [25], [50]. Precon-
dition analysis has also been used to directly synthesize input
constraints that can eliminate certain classes of errors [38],
[15].

In recent years, Anomaly Detection Systems have increas-
ingly been deployed to handle data from the web. Some
common applications include detecting malicious websites
[44], [48], automatically neutralizing malicious JavaScript
code [18], [20], [28], [36], and ﬁltering anomalies in more
general data types that, in today’s world, are usually acquire
from the web [37], [38].

Rigorous ADS Testing:

It has been recognized in the
anomaly detection literature that drawing test data (and training
data) from a well deﬁned distribution over “normal” inputs is
desirable. However, as Chandola et al. [16] point out, while
statistical techniques for ADS are based on the assumption
of an underlying input distribution,
is often difﬁcult or
impossible to deﬁne, let alone sample, from this distribution.
Thus, samples for testing are typically collected with a best
effort approach, with the focus placed on detection techniques.
As recognized by Gao et al. [24]:

it

It is not our goal here to determine how to acquire
adequate training data for a program. Rather, we
simply assume we have adequate training data in
our study; if this is not true, our techniques might
yield false detections, i.e., they may detect anomalies
that are not, in fact, intrusions.

With respect to collecting representative samples from the web,
while we are not aware of any work that samples directly from
the PageRank distribution, several works on anomaly detection
have employed crawl based methods for collecting data [20],
[37], [36], [48]. It has been recognized that simply drawing
webpages from a known list (i.e. search engine indexes) gives
a limited diversity of input samples [18]. Crawling helps
ameliorate that issue.

Ntoulas et al. [44] further recognize that crawling biases
samples towards “high quality,” frequently visited webpages.
This is a key motivation in our work, as a distribution weighted
by such features more accurately represents the pages a
typically user is expected to encounter. While motivated by
similar intuition, Ntoulas et al. [44] do not formalize their input
distribution. They also do not provide any guarantees (such as
bounds on the false positive rate) about the performance of the
anomaly detector.

PageRank: The PageRank distribution has been well stud-
ied in both theoretical and practical literature. Berkhin [13]
provides a valuable survey. Other candidate representative
distributions have been proposed for the web but, largely due
to Google’s success, PageRank has risen as a clear leader
in site ranking algorithms. One of the earliest alternatives
is the Hypertext Induced Topic Selection (HITS) algorithm
introduced by Kleinberg [30]. This algorithm is considered a
precursor to PageRank and was the ﬁrst approach to weight
links by “authority”, something PageRank does implicitly
through its random surfer model.

Part of the early motivation for PageRank was ease of
computation [45]. Several results have focused on accelerated
approaches [17], [33] since a full
list of probabilities is
required in actually using the distribution to rank webpages
(i.e. a set returned from a web search). We are not aware
of prior applications of sampling webpages by PageRank.
Sampling has been considered in a more theoretical context
for collecting a representative group of nodes from a general
network [35].

VIII. CONCLUSION

Anomaly detectors are a critical component of many secu-
rity systems. Despite the central role that they play, anomaly
detectors are often designed and deployed with no precise
understanding of their behavior. We present a novel technique
and implemented system, Fortuna, for obtaining probabilistic
bounds on the actual false positive rate that deployed anomaly
detectors will incur. We demonstrate that Fortuna can provide
tight bounds for three anomaly detectors designed to process
input ﬁles from the Internet. Fortuna takes an important step
towards placing anomaly detection on a ﬁrmer theoretical foun-
dation and is designed to help practitioners better understand
the behavior and consequences of the anomaly detectors that
they build and deploy.

ACKNOWLEDGMENT

The authors would like to thank our anonymous reviewers
for many helpful comments and suggestions. This research was
supported by DARPA grant No. FA8650-11-C-7192. B. Juba
was supported by ONR grant No. N000141210358. C. Musco
was partially supported by NSF Graduate Research Fellowship
grant No. 1122374.

REFERENCES
“Alexa Internet,” http://www.alexa.com/.

[1]

12

[2]

[3]
[4]

“Alexa Top Million Sites,” http://s3.amazonaws.com/alexa-static/
top-1m.csv.zip.
“Common Crawl,” http://commoncrawl.org/.
“Common Crawl URL Index,” https://github.com/trivio/common\
_crawl\_index.
“Dillo,” http://www.dillo.org/.
“Google Toolbar,” http://toolbar.google.com/.
“Imagemagick,” http://www.imagemagick.org/.
“Swftools,” http://www.swftools.org/.
“Wepawet,” https://wepawet.cs.ucsb.edu/.
“Netcraft Web Server Survey,” http://www.alexa.com/, June 2014.

[5]
[6]
[7]
[8]
[9]
[10]
[11] R. Andersen, F. Chung, and K. Lang, “Using PageRank vectors to
locally partition graphs,” Internet Mathematics, vol. 4, no. 1, pp. 35–64,
2007.

[12] V. S. Arvind Narayanan, “Robust de-anonymization of large sparse
datasets (how to break anonymity of the netﬂix prize dataset),” in IEEE
Security and Privacy (Oakland), 2008, pp. 111–125.

[13] P. Berkhin, “A survey on PageRank computing,” Internet Mathematics,

vol. 2, no. 1, pp. 73–120, 2005.

[14] M. Bianchini, M. Gori, and F. Scarselli, “Inside PageRank,” ACM Trans.

Internet Techn., vol. 5, no. 1, pp. 92–128, 2005.

[15] D. Brumley, H. Wang, S. Jha, and D. Song, “Creating vulnerability
signatures using weakest preconditions,” in Proceedings of the 20th
IEEE Computer Security Foundations Symposium, ser. CSF ’07’.
Washington, DC, USA: IEEE Computer Society, 2007, pp. 311–325.
[Online]. Available: http://dx.doi.org/10.1109/CSF.2007.17

[16] V. Chandola, A. Banerjee, and V. Kumar, “Anomaly detection: A
survey,” ACM Comput. Surv., vol. 41, no. 3, pp. 15:1–15:58, Jul. 2009.
[Online]. Available: http://doi.acm.org/10.1145/1541880.1541882

[17] Y.-Y. Chen, Q. Gan, and T. Suel, “I/O-efﬁcient

techniques for
the Eleventh International
computing pagerank,” in Proceedings of
Conference on Information and Knowledge Management, ser. CIKM
’02. ACM, 2002, pp. 549–557. [Online]. Available: http://doi.acm.
org/10.1145/584792.584882

[18] M. Cova, C. Kruegel, and G. Vigna, “Detection and analysis of drive-
by-download attacks and malicious javascript code,” in Proceedings of
the 19th International Conference on World Wide Web, ser. WWW
’10’. New York, NY, USA: ACM, 2010, pp. 281–290. [Online].
Available: http://doi.acm.org/10.1145/1772690.1772720

[19] A. Croll and S. Power, Complete Web Monitoring - Watching your
visitors, performance, communities and competitors. O’Reilly, 2009.
[20] C. Curtsinger, B. Livshits, B. Zorn, and C. Seifert, “Zozzle: Fast
and precise in-browser javascript malware detection,” in Proceedings
of the 20th USENIX Conference on Security, ser. SEC’11. Berkeley,
CA, USA: USENIX Association, 2011, pp. 3–3. [Online]. Available:
http://dl.acm.org/citation.cfm?id=2028067.2028070

[21] D. Denning, “An intrusion-detection model,” Software Engineering,

IEEE Transactions on, vol. SE-13, no. 2, pp. 222–232, Feb 1987.

[22] A. Farahat, T. Lofaro, J. C. Miller, G. Rae, and L. A. Wart, “Authority
rankings from HITS, PageRank, and SALSA: Existence, uniqueness
and effect of initialization,” SIAM J. Sci. Comput., vol. 27, no. 4, pp.
1181–1201, 2006.

[23] H. Feng, J. Gifﬁn, Y. Huang, S. Jha, W. Lee, and B. Miller, “Formalizing
sensitivity in static analysis for intrusion detection,” in Security and
Privacy, 2004. Proceedings. 2004 IEEE Symposium on, May 2004, pp.
194–208.

in Proceedings of

anomaly detection,”

[24] D. Gao, M. K. Reiter, and D. Song, “On gray-box program
the 13th
tracking for
conference on USENIX Security Symposium - Volume 13,
ser.
SSYM’04. USENIX Association, 2004. [Online]. Available: http:
//portal.acm.org/citation.cfm?id=1251375.1251383
J. T. Gifﬁn, S. Jha, and B. P. Miller, “Detecting manipulated remote
call streams,” in Proceedings of the 11th USENIX Security Symposium.
Berkeley, CA, USA: USENIX Association, 2002, pp. 61–79. [Online].
Available: http://dl.acm.org/citation.cfm?id=647253.720282

[25]

[26] T. H. Haveliwala and S. D. Kamvar, “The second eigenvalue of the

Google matrix,” Stanford University, Tech. Rep. 582, 2003.

13

[27] W. Hoeffding, “Probability inequalities for sums of bounded random
variables,” Journal of the American Statistical Association, vol. 58, no.
301, pp. 13–30, 1963.

[28] A. Kapravelos, Y. Shoshitaishvili, M. Cova, C. Kruegel, and G. Vigna,
“Revolver: An automated approach to the detection of evasiveweb-
based malware,” in Proceedings of the 22Nd USENIX Conference on
Security, ser. SEC’13. Berkeley, CA, USA: USENIX Association,
2013, pp. 637–652. [Online]. Available: http://dl.acm.org/citation.cfm?
id=2534766.2534821

[29] M. J. Kearns and U. V. Vazirani, An Introduction to Computational

Learning Theory. Cambridge, MA, USA: MIT Press, 1994.
J. M. Kleinberg, “Authoritative sources in a hyperlinked environment,”
J. ACM, vol. 46, no. 5, pp. 604–632, Sep. 1999. [Online]. Available:
http://doi.acm.org/10.1145/324133.324140

[30]

[31] C. Kruegel and G. Vigna, “Anomaly detection of web-based attacks,”
the 10th ACM conference on Computer and
[Online].

in Proceedings of
communications security, ser. CCS ’03. ACM, 2003.
Available: http://doi.acm.org/10.1145/948109.948144

[32] A. N. Langville and C. D. Meyer, Google’s PageRank and Beyond:
Princeton, NJ: Princeton

The Science of Search Engine Rankings.
University Press, 2006.

[33] C. P. Lee, “A fast two-stage algorithm for computing pagerank and its

extensions,” Tech. Rep., 2003.

[34] M. Leonard, “Flawed process,” Indiana Herald-Times, August 10–12,

2008.
J. Leskovec and C. Faloutsos, “Sampling from large graphs,” in
Proceedings of
the 12th ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining, ser. KDD ’06. New
York, NY, USA: ACM, 2006, pp. 631–636.
[Online]. Available:
http://doi.acm.org/10.1145/1150402.1150479

[35]

[36] Z. Li, S. Alrwais, X. Wang, and E. Alowaisheq, “Hunting the red fox
online: Understanding and detection of mass redirect-script injections,”
in Proceedings of 2014 IEEE Symposium on Security and Privacy.
IEEE Computer Society, 2014.

[37] F. Long, V. Ganesh, M. Carbin, S. Sidiroglou, and M. Rinard,
“Automatic input rectiﬁcation,” in Proceedings of the 2012 International
Conference on Software Engineering, ser. ICSE 2012.
IEEE Press,
2012, pp. 80–90. [Online]. Available: http://dl.acm.org/citation.cfm?
id=2337223.2337233

[38] F. Long, S. Sidiroglou-Douskos, D. Kim, and M. Rinard, “Sound input
ﬁlter generation for integer overﬂow errors,” in Proceedings of the 41st
ACM SIGPLAN-SIGACT Symposium on Principles of Programming
Languages, ser. POPL ’14’. New York, NY, USA: ACM, 2014,
pp. 439–452. [Online]. Available: http://doi.acm.org/10.1145/2535838.
2535888

[39] M. W. Mahoney and L. Orecchia, “Implementing regularization implic-
itly via approximate eigenvector computation,” in Proceedings of the
28th International Conference on Machine Learning (ICML’11), 2011,
pp. 121–128.

[40] M. R. Meiss, B. Gonçalves, J. J. Ramasco, A. Flammini, and
F. Menczer, “Modeling trafﬁc on the web graph,” in Proc. 7th Workshop
on Algorithms and Models for the Web Graph (WAW), R. Kumar and
D. Sivakumar, Eds. Berlin / Heidelberg: Springer, 2010, pp. 50–61.
[41] M. R. Meiss, F. Menczer, S. Fortunato, A. Flammini, and A. Vespignani,
“Ranking web sites with real user trafﬁc,” in Proc. WSDM’08, 2008,
pp. 65–75.

[42] A. Narayanan and V. Shmatikov, “De-anonymizing social networks,” in

IEEE Security and Privacy, 2009, pp. 173–187.

[43] ——, “Myths and fallacies of personally identiﬁable information,”

Communications of the ACM, vol. 53, no. 6, pp. 24–26, 2010.

[44] A. Ntoulas, M. Najork, M. Manasse, and D. Fetterly, “Detecting
spam web pages through content analysis,” in Proceedings of
the
15th International Conference on World Wide Web, ser. WWW ’06’.
New York, NY, USA: ACM, 2006, pp. 83–92. [Online]. Available:
http://doi.acm.org/10.1145/1135777.1135794

[45] L. Page, S. Brin, R. Motwani, and T. Winograd, “The PageRank
citation ranking: Bringing order to the Web,” in Proceedings of the 7th
International World Wide Web Conference, Brisbane, Australia, 1998,
pp. 161–172. [Online]. Available: citeseer.nj.nec.com/page98pagerank.
html

[46] R. Perdisci, D. Ariu, P. Fogla, G. Giacinto, and W. Lee, “Mcpad:
A multiple classiﬁer system for accurate payload-based anomaly
detection,” Computer Networks, vol. 53, no. 6, pp. 864 – 881,
2009, trafﬁc Classiﬁcation and Its Applications to Modern Networks.
[Online]. Available: http://www.sciencedirect.com/science/article/pii/
S1389128608003927

[47] G. Stringhini, C. Kruegel, and G. Vigna, “Shady paths: Leveraging
surﬁng crowds to detect malicious web pages,” in Proceedings of the
2013 ACM SIGSAC Conference on Computer &#38; Communications
Security, ser. CCS ’13. New York, NY, USA: ACM, 2013, pp. 133–
144. [Online]. Available: http://doi.acm.org/10.1145/2508859.2516682
[48] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song, “Design and
evaluation of a real-time url spam ﬁltering service,” in Proceedings
of the 2011 IEEE Symposium on Security and Privacy, ser. SP ’11’.
Washington, DC, USA: IEEE Computer Society, 2011, pp. 447–462.
[Online]. Available: http://dx.doi.org/10.1109/SP.2011.25

[49] F. Valeur, D. Mutz, and G. Vigna, “A learning-based approach to the

detection of sql attacks,” in DIMVA 2005, 2005.

[50] D. Wagner and D. Dean, “Intrusion detection via static analysis,”
in Security and Privacy, 2001. S P 2001. Proceedings. 2001 IEEE
Symposium on, 2001, pp. 156–168.

[51] K. Wang and S. J. Stolfo, “Anomalous payload-based network intrusion

detection,” in RAID, 2004.

14

