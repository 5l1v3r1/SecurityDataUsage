OTO: Online Trust Oracle for

User-Centric Trust Establishment

Tiffany Hyun-Jin Kim† Payas Gupta§

Jun Han† Emmanuel Owusu†

Jason Hong† Adrian Perrig† Debin Gao§

† Carnegie Mellon University

{hyunjin,junhan,eowusu,jasonh,perrig}@cmu.edu

§ Singapore Management University

{payas.gupta.2008,dbgao}@smu.edu.sg

ABSTRACT
Malware continues to thrive on the Internet. Besides auto-
mated mechanisms for detecting malware, we provide users
with trust evidence information to enable them to make in-
formed trust decisions. To scope the problem, we study the
challenge of assisting users with judging the trustworthiness
of software downloaded from the Internet.

Through expert elicitation, we deduce indicators for trust
evidence, then analyze these indicators with respect to scal-
ability and robustness. We design OTO, a system for com-
municating these trust evidence indicators to users, and we
demonstrate through a user study the eﬀectiveness of OTO,
even with respect to IE’s SmartScreen Filter (SSF). The
results from the between-subjects experiment with 58 par-
ticipants conﬁrm that the OTO interface helps people make
correct trust decisions compared to the SSF interface regard-
less of their security knowledge, education level, occupation,
age, or gender.

Categories and Subject Descriptors
K.6.5 [MANAGEMENT OF COMPUTING AND IN-
FORMATION SYSTEMS]: Security and Protection—
Invasive software; H.1.2 [MODELS AND PRINCIPLES]:
User/Machine Systems—Human factors

Keywords
User Interfaces for Security, Human Factors, Trust Evidence,
Software Download, Trust Validation for Uncertiﬁed Soft-
ware

1.

INTRODUCTION

Gauging the authenticity and legitimacy of software on-
line is challenging. For example, novice users do not under-
stand the dangers and generally lack the ability to validate
downloaded software, and even security-conscious users are
often frustrated by their inability to judge the legitimacy of
software.

We observe that useful trust evidence information is avail-
able on the Internet, which can help validate the correctness

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.

and trustworthiness of software. Some examples include
trusted Certiﬁcate Authority-issued public-key certiﬁcates,
social network-based reviews, authority or expert-based re-
ports, PageRank information, etc. However, several factors
complicate successful veriﬁcation by end-users:

• Cumbersome information gathering: users need to

spend time and eﬀort to collect trust evidence.

• Being aware of what evidence exists: most users
(especially non-experts) are unaware of available trust ev-
idence indicators for validation.

• Finding the evidence: users may not even know where

and how to start searching for evidence.

• Assessing the quality of the evidence: even if users
ﬁnd trust evidence, it may be false information (e.g., users
only ﬁnd evidence supporting the trustworthiness of a cer-
tain application which happens to provide malicious con-
tents). Furthermore, contradicting evidence may exist. In
this case, users may face diﬃculty of correctly interpreting
and prioritizing the relevance of trust evidence indicators.
• Limited trust establishment resources: mechanisms
for querying certain resources for trust establishment are
currently unavailable. For example, users can rely on their
online social network (OSN) friends to check if they have
experience with the resources and get personal feedback;
however, such a mechanism is currently unavailable.

One tempting solution would be to automate trust de-
cisions for users based on the available evidence that sys-
tems can gather. In cases where the downloaded software is
clearly malicious, such automated systems are eﬀective, but
in the frequent cases where the malware authors circumvent
the automated system [15], the user is still left alone to make
a trust decision. Indeed, automated protection has not been
100% accurate, due to delays in identifying malware as well
as new and evolving threats.

In such a case, how can we provide useful evidence that
can guide non-expert users to enhance their user experience
in terms of making correct context-dependent trust deci-
sions? To address this question, we explore how to combine
multiple pieces of trust evidence and present them to users in
a usable manner as follows: we furnish users with (1) an ev-
idence list that supports the trustworthiness of the applica-
tion in question, and (2) another evidence list that questions
the legitimacy of the application. Applying this design to
the previous example, a user downloading software would be
presented with evidence showing that the software has low
popularity, in which case the user would be able to weigh
this evidence against the software’s claim to be a commonly
installed application, as is the case in many attacks.

391We further explore how to customize the order of the ev-
idence for each list based on user preferences. However,
novice users may miss the critical evidence before they make
trust decisions. To educate novice users, we compare the be-
havior between security experts and novice users and sug-
gest what evidence experts check that novice users fail to
pay close attention to.

This paper makes the following research contributions:

• We report the observation results of a user study that we
conducted with security experts, to understand how they
make their trust decisions when installing software. We
aggregated what factors these experts take into account,
which was used to inform the design of our user interface.
• We present the design of OTO, short for Online Trust
Oracle, a user interface that is presented before software
is installed. OTO presents two categories of evidence to
assist users in determining software legitimacy: the posi-
tive evidence of why the software is safe to download, and
the negative evidence for potential malware. OTO uses
evidence that resists spooﬁng attacks.

• We present the results of a user study comparing OTO
against SmartScreen Filter (SSF) on Microsoft Internet
Explorer (IE) for software downloads. The results from
the between-subjects experiment with 58 participants con-
ﬁrm that the OTO interface helps people make correct
trust decisions compared to the SSF interface regardless of
their background security knowledge, education level, occu-
pation, age, or gender. OTO especially helps when the un-
derlying operating system (OS) (which controls SSF and
OTO) fails to correctly label the software legitimacy (i.e.,
the OS detects malware as legitimate, and vice versa), and
when the OS correctly labels legitimate software.

2. PROBLEM DEFINITION

We address the following fundamental problem: how can
we gather and present robust trust evidence indicators to as-
sist a novice user to make a correct trust decision on a piece
of software that is about to be downloaded? Numerous sub-
problems exist: Which trust evidence indicators are robust
against adversarial inﬂuence? What trust evidence can be
meaningfully and automatically gathered? How should the
trust evidence be presented to the user?

Our goal is thus to design a dialog box with robust trust
evidence indicators to help novice users avoid malware, even
if the underlying OS fails to correctly label the legitimacy
of software.

2.1 Assumptions

We assume that malware cannot interfere with OTO op-
eration, such as the display of the OTO dialog box, the
detection of software downloads, or the gathering of trust
evidence. Addressing these challenges would go beyond the
scope of this paper.

2.2 Desired Properties

The following properties are desired for the trust evidence

indicators:
Correct. Our trust evidence information should be cor-
rect such that users can trust both the information and the
information source. In other words, users should be able to
rely on it (without searching the Internet extensively them-
selves) when they need to make trust decisions.

Prevalent

security threats

E!ective

design principles

OTO:

trust evidence
user interface

Common

pitfalls

Experts’
feedback

Figure 1: Factors we took into account in our design of Online
Trust Oracle (OTO). We considered the most common threats and
vectors in malware attacks that involve human interaction. We also
drew on design principles, examined common pitfalls of novice
users, and solicited feedback from experts to understand how ex-
perts make decisions regarding software installation.

When the multiple pieces of evidence result in uncertainty
over whether a given piece of software is benign or malicious,
users should still be able to make a correct trust decision
based on how the evidence is presented.
The evidence indicators should be intuitively
Usable.
useful to novice users when they need to make trust deci-
sions. Also, the information should not annoy users.

2.3 Adversary Model

Malware distributors may attempt to manipulate trust
evidence information. For example, they may provide fal-
sifying information, or hide crucial information, misguiding
users to perceive their software as legitimate.

3. DESIGN OVERVIEW AND RATIONALE
Our main goal of this section is to understand the most
common security threats and attack vectors involving some
form of user interaction. This analysis will help us design
(1) a user interface that can eﬀectively present trust evidence
and (2) user study scenarios to validate the eﬀectiveness of
the new interface. To achieve our goal, we consider four
factors: analysis of popular security threats, eﬀective design
principles, common pitfalls of novice users, and feedback
from experts (see Figure 1).

3.1 Analysis of Prevalent Security Threats

An important factor in designing a user interface to sup-
port trust decisions is understanding prevalent security th-
reats. According to SophosLabs, 85% of all malware comes
from the web; in particular, the top threat in 2011 was drive-
by downloads where attackers lure users to malicious sites
that are injected with malicious code, or to legitimate sites
that host the malware [3]. According to Microsoft’s Security
Intelligent Report, 44.8% of successful malware attacks hap-
pen because of some action taken by the end-user [1]. This
indicates that enabling users to defend themselves against
online threats is critical, especially with ever increasing num-
bers of Internet-accessible devices, including laptops, smart-
phones, tablet PCs, etc.

In terms of identifying the threats, SophosLabs has identi-
ﬁed that one of the more persistent threats of Year 2010 was
fake antivirus (also known as ransomware, rogueware and
scareware, mostly with a Trojan horse component) which
resembles or impersonates genuine security solutions and in-
veigles into a victim’s machine [2]. Although declining in
number, possibly due to international law enforcement co-

392operation, fake antivirus continued to be a big problem in
2011 [3]. Other threats include keyloggers to capture per-
sonal information and passwords, and botnet software to
distribute spam, host illegal content, or serve malware.

The reports cited above list the commonly used techniques

to distribute malware, which are as follows:

• Blackhat search engine optimization (SEO) ranks websites

with malware highly in search results.

• Social engineering click-jacking uses social relationships

to trick users into clicking on webpages.

• Malvertising embeds malware in advertisements displayed

on legitimate, high-traﬃc sites.

• Drive-by downloads cause users to install malware simply

by visiting a website.

• Compromised legitimate websites host embedded malware

that spreads to users who access the websites.

In terms of software threats, SophosLabs and Microsoft
have analyzed that malware targets Microsoft products (e.g.,
Microsoft oﬃce, IE, etc.), Java vulnerabilities, HTML and
JavaScript, document parsers, and Adobe products (e.g.,
Flash, PDF) [1, 3].

All reports emphasize that educating users is one of the
most critical steps to reduce the damage caused by attackers.

3.2 Common Pitfalls of Novice Users

Prior research has shown that novice users tend to en-
counter several common pitfalls when they make security
decisions online [5, 8, 9, 18]. Such common pitfalls can help
us design an eﬀective user interface that prevents users from
repeatedly making the same mistakes. We summarize the
common mistakes from past work as follows:
Lack of security and systems knowledge. Users tend
to misinterpret various indicators (e.g., a security lock icon,
false address in the “from” line of an email, broken image,
syntax of domain names, etc.) that could be used to deter-
mine whether an email or a website is trustworthy.
Visual deception. Users tend to be deceived by how an
email or a website visually presents information. For exam-
ple, users tend to trust a website that has professional or
sophisticated layout, or that mimics the legitimate website.
Moreover, users tend to be fooled by “typejacking” homo-
graph attacks that substitute letters of a URL with similar-
looking ones (e.g., www.bankofthevvest.com) or adding
extra repeating letters (e.g., www.gooogle.com).
Even security experts fail to
Psychological pressure.
defend against messages that invoke fear, threat, excitement,
or urgency [18]. For example, scareware and rogueware pro-
vide terrifying messages and force people to download (and
pay for) malware. This is because such urgent cues pres-
sure people and short circuit the available resources which
in other cases help people detect deceptions.
Prior experience.
Users who are familiar with partic-
ular scams seem to be good at spotting similar ones. How-
ever, this characteristic does not hold when these users are
exposed to unfamiliar scams.
Bounded attention. Users tend to separate headers or
URLs from the actual content of an email or a website. Users
also pay insuﬃcient attention to existing security indicators,
and fail to notice when security indicators are absent.

3.3 Design Principles

In this section, we describe design principles that informed
our design for interfaces that can eﬀectively display trust

Table 1: User study scenarios. To observe the mental model
of security experts when they analyze the legitimacy of given soft-
ware, we conducted a user study with 10 software programs, where
5 are legitimate and 5 are malicious. “Difﬁculty” represents how
challenging we believed it would be to determine the legitimacy of
a given software program.
Type
Software
Visual organizer
MindMaple
Antivirus
AhnLab
Spam ﬁlter
SPAMﬁghter Pro
Antivirus
Kaspersky
Rkill
Anti-malware
Windows activation Ransomware
Privacy violation
ActiveX codec
HDD Diagnostic
Adobe ﬂash update

Difﬁculty
Easy
Medium
Medium
Difﬁcult
Difﬁcult
Easy
Scareware
Medium
Trojan malware Medium
Difﬁcult
Rogueware
Trojan malware
Difﬁcult

Malicious

Legitimacy

Legitimate

evidence to users. We follow the design guideline suggested
by Kuo [13] and Egelman [10] to minimize common errors
in the stages of the C-HIP model [20].
Grayed-out background.
Dialog boxes on grayed-out
background is shown to be eﬀective in getting users’ full at-
tention [13]. Users also believe that such dialog boxes orig-
inate from the OS and not from malicious pop-ups. More-
over, this approach ensures that users do not overlook the
dialog boxes, and forces them to make a decision [10].
Mimicked UI of OS vendor.
Professional look and
feel is important to enable users to develop conﬁdence in
the warning messages [13]. Hence, the warning messages
should mimic the user interfaces of the OS vendor.
Detailed explanation.
Users tend to ignore warnings
that they do not understand [13]. Hence, it is important that
the interface provides detailed explanations that people can
easily understand while avoiding jargon. More speciﬁcally,
dialog boxes should provide suﬃcient warning evidence so
that users can easily understand the danger; an overview of
the risks and possible consequences that users can believe
and agree with; and actionable recommendations that users
feel motivated to act upon [10].
Non-uniform UIs.
Users tend to dismiss warnings, of-
ten due to habituation. To prevent this, it is important
to provide diﬀerent dialog-box designs for diﬀerent levels of
severity [10]. Furthermore, randomizing the order of items
in dialog boxes is eﬀective in forcing users to pay attention
to security decisions [7].

3.4 Feedback from Experts

As Bravo-Lillo et al. have shown, novice users have dif-
ferent mental models compared to security experts [6]. In
order to study how security experts determine the legitimacy
of software and what advice they would give to novice users,
we ﬁrst ran a user study on security experts.

Based on the prevalent security threats as described in
Section 3.1, we selected 10 software items for our study: 5
legitimate software programs and 5 malware programs.
Experts’ user study. We created a scenario for each
software in Table 1 based on the commonly used techniques
for malware distribution as described in Section 3.1. We
recruited 9 security experts who have been studying and
working in the computer security ﬁeld for at least 5 years.
Procedure. We created an interactive PowerPoint and
VisualBasic mockup of the Windows 7 OS and IE browser
environment to mimic a user’s typical browsing behavior;

393Table 2: Ten scenarios used in the experts’ user study. We created each scenario based on the commonly used techniques for malware
distribution as mentioned in Section 3.1. For illustration purposes, we name each expert’s best friend either Alice or Bob.

Software

MindMaple

AhnLab

SPAMﬁghter

Pro

Kaspersky

Rkill

Windows
Activation

Privacy
Violation

ActiveX
Codec

HDD

Diagnostic

Adobe Flash

Update

Scenario
Alice has been using ALZip, which is a free ﬁle compression program that she downloaded from the Internet. One day,
Alice notices an advertisement for MindMaple on the ALZip interface and decides to click. As she clicks, a new browser
window opens, showing the webpage of MindMaple – a mind-mapping software to enhance the management of multiple
projects, ideas, etc. Alice skims through the webpage and she is about to click a link to try MindMaple for free.
Bob does not have antivirus software installed and he believes his computer is infected with a virus. Bob searches Google
for ’antivirus’ and navigates to a Wikipedia article that contains a list of antivirus software. After looking at several
options he decides to download AhnLab V3 Antivirus. He clicks the link for AhnLab on the Wikipedia page and clicks
the download link on the AhnLab website.
Bob is tired of getting spam so he decides to download spam blocking software. He searches Google for ’spam ﬁlter
spamﬁghter’ and navigates to the SPAMFighter website. He clicks the ‘Start Download Now’ button and is redirected to
a similar page with instructions to select ‘Run anyway’ if Internet Explorer SmartScreen issues a warning. He clicks on
‘Start Download Now.’
Alice’s computer does not have an antivirus program installed, so she decides to install one. She searches Google for
’antivirus software’ and decides to click on the sponsored link for Kaspersky. She clicks on the link for a free 30-day trial,
ﬁlls in some personal details, and clicks the download button.
Bob believes his computer is infected with malware. He reads two forums on free malware removal software and an
article describing Rkill as the repair tool of the week. He decides to try Rkill and searches Google for ‘rkill download.’
Bob clicks on the link for the CNET download page and then clicks on the ‘Download Now’ button.
Alice’s PC was installed with a pirated copy of Windows 7 which she downloaded from a ﬁle sharing website. She
has had no problems with her PC so far, and is completing a homework assignment that requires a photo editor. Alice
searches Google for ‘free adobe photo editor torrent’ and selects the ﬁrst link in the search results. She is trying to ﬁnd
the download button when a window appears prompting her to call a number to receive an activation code for Windows.
Alice has noticed that her PC has been running slowly, so she searches Google for ’windows 7 is slow.’ After looking
through several options she navigates to a page titled ‘Registry Clean Up.’ While browsing the webpage a window appears
with a warning about privacy violations found on her computer. She clicks on the ’Repair Now!’ button.
Bob is bored at home and decides to watch a movie. He searches Google for ‘batman begins’ and selects a UStream video.
While waiting for the video to load a dialog box appears over the video window stating that there was an error displaying
the video and prompts him to install ActiveX to ﬁx this error.
Alice’s computer has been making some weird noises. Alice decides to search for a solution on Google. She ﬁnds a
webpage titled ‘Windows 7 making weird buzzing noises.’ As she browses the webpage, a window appears claiming that
it is scanning the computer to analyze PC performance. Alice clicks the ‘Pause’ button.
Bob enjoys watching videos by ’Just for laughs TV.’ He searches Google for ‘just for laughs gags’ and looks through
several options before selecting a Youtube video to watch. After Bob ﬁnishes the ﬁrst video, he decides to click on
a related video. While waiting for the video to load a dialog box appears over the video window prompting him to
download an update for Adobe Flash Player.

each scenario is composed of a sequence of Windows 7 desk-
top screenshot slides, each of which shows a web browser
with a diﬀerent website based on the scenario. The next
slide is triggered when a user clicks an appropriate area in-
side the browser. For example, the ActiveX codec scenario
starts with a Google result page of ’batman begins,’ and
asks the user to click a particular link (e.g., UStream link)
to proceed. Such a click triggers the next slide, displaying
the UStream webpage with an active progress bar. Shortly
after, the following slide displays an ActiveX codec error,
asking the user to install ActiveX to ﬁx this error.1

To obtain responses that experts would provide in typical
situations, we asked each expert to pretend to be watch-
ing a close friend who is using a computer and is about to
download software. For this role-playing, one of the authors
pretended to be a close friend of each expert, and we asked
each expert to observe the author’s browsing behavior for 10
diﬀerent scenarios. For each expert, we presented 10 scenar-
ios in randomized order. Table 2 summarizes the scenarios
that we used for our study.

At the end of each scenario, we asked the following ques-
tions to analyze the types of evidence that experts use to
determine the software’s legitimacy:

1Note that our mockup of the Windows 7 environment is indistin-
guishable to the real Windows 7 environment, as commented by
study participants.

• Would you recommend that Alice proceeds and downloads

the software? [Yes / No / Not sure]

• [If Yes or No] Why?
• [If Not sure] What would you do to ﬁnd out the le-

gitimacy of this software?

• What evidence would you present to Alice to convince her

of the legitimacy of this software?

• How well do you know this software? [1 (don’t know at

all) – 5 (know very well)]

The last question is to analyze whether their prior knowl-
edge aﬀects how they recommend software. After all 10 sce-
narios, we asked each expert to draw a ﬂow chart that shows
how (s)he decides about the software legitimacy in general.
We emphasized that this ﬂow chart is to help educate their
close friends in determining the software legitimacy. At the
end of the study, we asked each expert to provide feedback
on the download dialog box that we designed.
In general, the ﬁrst action that all the experts
Results.
took was examining the downloading software’s hosting web-
site carefully, regardless of their prior knowledge. All 9 ex-
perts emphasized the importance of downloading software
from a trusted and reputable website, and indicated that
they would go directly to the publisher of the software. If
users are not on the publisher’s website, the experts would
ascertain if the URLs and the website content looks legit-
imate. If the hosting website does not look reputable, the

394Table 3: Summary of 10 experts’ processing operations. This
table illustrates processing operations that the experts indicated in
their own ﬂowchart, and the number of experts who mentioned the
same processing operation.

Processing operation

Software review
• Are reviews available from reputable sources, experts, or
friends?
• Are the reviews good?
• Do a lot of people use the software?
Hosting site
• Is the hosting site reputable?
• What is the corporate parameter (e.g., number of employees,
age of the company) of the site?
• Is the site related to the software to be downloaded?
• Are you being redirected from advertisements, emails, etc.?
• Does the site have a high rank on Google using general
search terms?
• Does the company have a physical location near where you
live?
User intention
• Did you search for that speciﬁc software?
• Are you familiar with this software?
• Do you really need it?
• Are you downloading from a pop-up?
Securing machines
• Do you run an updated antivirus?
• Is your machine trusted?

#

9

3
1

8
2

1
1
1

1

1
1
1
1

2
1

experts would search for the same software from a reputable
source. Alternatively, 8 experts agreed that they would con-
sider the (unknown) hosting site as reliable if Google’s search
query with general terms (e.g., “antivirus”) provides a high
page rank for that host; on the other hand, 1 expert men-
tioned that it would be “possible to poison Google’s search
results” for very speciﬁc terms (e.g., “AhnLab V3 antivirus”).
All the experts also emphasized user intention to be an
important factor for judging software legitimacy. For ex-
ample, all experts pointed out, throughout the study, that
they would not recommend their friends to download soft-
ware from pop-up windows conveying information that is
irrelevant to the original search. This is because the soft-
ware advertised in the pop-up window is highly unlikely to
be what the user intends to download, and highly unlikely to
be legitimate. On the other hand, if users proactively click
to download the software that they have been searching for,
the experts mentioned that the likelihood of downloading
malware may be low.

All the experts also emphasized that carefully examining
software reviews by trusted authorities or experts is critical.
Before downloading software, all the experts would recom-
mend users to research the downloading software’s reviews
from reputable sources (besides the hosting site). For exam-
ple, the experts suggested checking (1) the total number of
people currently using the software, (2) positive and nega-
tive reviews, and (3) business information for the software
vendor.

Two experts mentioned that securing machines is also im-
portant. For example, one way to reduce malware download
would be running antivirus software that is up-to-date.
Flowchart.
At the end of the study, we asked each ex-
pert to draw a ﬂowchart that their friends could use as a
guideline for future downloads.
In general, every expert’s
detailed thinking process was unique. However, they all il-
lustrated the same main points, in particular hosting site,
user intention, software review, and securing machine as de-
scribed above. Table 3 summarizes the frequency of diﬀer-
ent process operations that the experts drew. We merged all

Did you  

Start

click to download from  

N 
N 

a pop-up window? 

YY
Y 

y sea

Directly search for  
the file   

Do you really  

need to download  

this file? 

Y 

Do you (or  

N 

Stop downloading  
Stop downloading  
S
this file 
this file
t

your close friends, family,  

and security experts) trust the  

N 

website from which you  

clicked to download? 

Search for another 
Search for another 
website that you  
w
website that you  
can trust 
can trust  

Y 

Are the reviews 

Read the reviews  
Read the reviews  
s  
about this file first
t
about this file first 

Not 
Noot 
sure 
surre

 (from reputable sites, security  
experts, or friends and family)  

N 

good for this file? 

Search for a  
S
Search for a  
did fferent file with  
different file with  
good reviews
good reviews 

ff

Y 
YY

Is the  
s the

downloading file  

directly related to the website that  

you are downloading the  

file from? 

Y 
YY

Download 
Download

wnlo

N 

Visit the file 
Visit the file 
publisher’s website 
publishe
p
r’s website 
directly to download 
directly to download 

Figure 2: A merged and simpliﬁed ﬂowchart. This ﬂowchart
preserves the order of the processing operations as the majority of
the experts indicated.

experts’ ﬂowcharts to guide our dialog-box design, with an
additional processing operation: Were you urged to down-
load? This is because many antivirus malware programs,
especially ransomware, scare users into immediately down-
loading the program to recover their computers. Figure 2
shows the resulting ﬂowchart.
Discussion.
An interesting observation from this study
was that the experts did not rely on special knowledge and
their reasoning was simple. Although this study did not re-
veal groundbreaking facts from security experts, such results
are promising since an enhanced user interface may empower
even novice users to make correct security decisions.

4. OTO: USER INTERFACE WITH TRUST

EVIDENCE

In this section, we introduce OTO, which we designed
and modiﬁed based on the four design points and the re-
sults/feedback from the expert study in Section 3. OTO is
an interface that displays clues about the safety of down-
loading ﬁles.

OTO provides both reasons why the downloading ﬁle may
be safe to install, and reasons why the downloading ﬁle may
be harmful to the computer. Figures 3–5 show examples
of the OTO interface. With OTO, users can make informed
decisions about proceeding with or canceling the downloads.
In Section 4.1, we describe the design overview of OTO, and
in Section 4.2, we delineate a list of evidence types that are
suitable for OTO. In Section 4.3, we describe how evidence
is displayed in the OTO interface.

4.1 OTO Interface Overview

As shown in Figure 3, OTO is displayed when a user’s
action leads to the downloading of software; for example, the
user actively clicks to download or (s)he clicks the “cancel”
or “close” button but the download continues.

We assume that a system-level trusted path exists, such
that when a ﬁle is about to be downloaded, the OS can de-
tect it and activate OTO. When OTO is activated, the entire
screen grays out, which will catch the users’ full attention
and ensure that OTO is a legitimate OS-certiﬁed program
and not another malware program.

395Potentially Malicious Software 

Potentially Malicious File

Are you sure you want to download this (cid:127)le? It may damage 
your computer.  We gathered evidence to help you decide.

Are you sure you want to download this (cid:127)le? It may damage 
your computer.  We gathered evidence to help you decide.

Why this !le may be harmful

Why this !le may be safe

Why this !le may be harmful

Why this !le may be safe

Featured Article
ESET Threat Blog has an article 
featuring this !le: . . .

Friends’ Use 
None of your friends and family 
downloaded this !le. 

High Number of Available Reviews
Number of available reviews for this 
!le is high: . . .

High Popularity 
Total number of downloads last week
was high: . . .

See all 3 reasons

See all 5 reasons

File Origin
You attempted to download [install-!ashplayer11x64ax_gtba_alh.exe] while browsing 
http://www.youtube.com. We suggest that you download the (cid:127)le directly 
from the publisher.  

Unknown publisher
This !le is certi!ed by an unknown 
publisher: . . .

Friends’ Use 
None of your friends and family 
downloaded this !le. 

See all 8 reasons

File Origin
You attempted to download [activation.exe] while browsing http://extratorrent.com.  
We suggest that you download the (cid:127)le directly from the publisher.  

Download (I understand the risk)

Cancel

Download (I understand the risk)

Cancel

Change when these noti!cations appear

Change when these noti!cations appear

Figure 3: An example of an OTO interface. OTO presents rea-
sons why the ﬁle may be harmful and why the ﬁle may be safe to
install.

Potentially Malicious Software

Are you sure you want to download this !le? It may damage 
your computer.  We gathered evidence to help you decide.

Why this (cid:127)le may be harmful

Why this (cid:127)le may be safe

Friends’ Use 
24 of your friends and family also 
downloaded this (cid:127)le. Who are they?
Featured Article
New York Times has an article 
featuring this (cid:127)le: . . .

See all 8 reasons

File Origin
You attempted to download [V3.exe] while browsing http://globalahnlab.com.  

Download (I understand the risk)

Cancel

Change when these noti(cid:127)cations appear

Figure 4: An example of an OTO interface for legitimate soft-
ware. This example presents 0 negative reason and 8 positive rea-
sons.

Based on how OTO judges the software’s legitimacy, OTO
shows (1) reasons why this software is safe to download (pos-
itive evidence), and (2) reasons why this software may harm
the computer (negative evidence). Note that these reasons
are primarily based on the security experts’ suggestions that
can be feasibly gathered from the online sources. Users can
also choose to see other reasons that are helpful in determin-
ing the downloading software’s legitimacy. We also envision
that a user can rank how important and critical each piece
of evidence is to him, and OTO will assess the software’s
safety level based on such user-driven evidence ranks.

Following the Windows User Account Control framework,

OTO has three color modes for diﬀerent safety levels:
Blue with “?” shield symbol for legitimate software.
As shown in Figure 4, this applies to software that is highly
likely to be legitimate. For example, software that is dig-
itally signed by Microsoft would be categorized to show
the blue OTO interface. This interface shows the follow-
ing warning statement against a blue background: “Are you
sure you want to download this ﬁle? It may damage your
computer. We gathered evidence to help you decide.” Next
to the statement, Microsoft’s blue security shield symbol
with “?” is displayed. Under this blue message box, a table

Figure 5: An example of an OTO interface for malware. This
example presents 8 negative reasons and 0 positive reason.

is displayed, showing negative and positive reasons. For the
software in this category, a signiﬁcant number of positive
pieces of evidence is displayed, although some negative evi-
dence may be displayed (if it exists). As shown in Figure 4,
OTO shows the top two pieces of evidence, which either
OTO or the user has selected as important, and contains a
link that users can click if they want to see all 8 reasons.
Red with “X” shield symbol for malicious software.
As shown in Figure 5, the red colored OTO interface is for
pieces of software that is highly likely to be malicious. As
with the blue OTO interface, the same warning statement is
shown; however, the statement is against a red background,
with a red “X” security shield symbol. Also, this interface
will display a signiﬁcant number of negative pieces of evi-
dence, although few positive pieces of evidence may be dis-
played.
Yellow with “!” shield symbol for software of un-
certain provenance.
As shown in Figure 3, the yellow
colored OTO interface is for software that the system can-
not clearly determine as either malicious or safe. As a result,
with a yellow “!” security shield symbol, the same warning
statement is shown against a yellow background. For this
software, the interface will display both positive and nega-
tive evidence.

4.2 Retrieving Positive and Negative Evidence
We conjecture that users fail to make informed trust deci-
sions due to (1) unawareness of existing evidence (e.g., they
are simply unaware of the ways to ﬁnd evidence or are re-
luctant to do so), (2) overabundance of evidence (e.g., too
much, and potentially conﬂicting evidence further confuses
them), and (3) the cumbersome nature of searching for evi-
dence. Understanding these issues are part of the objectives
of this paper, and providing a concise list of useful, helpful,
and objective pieces of trust evidence may enable users to
make informed trust decisions.

Existing tools and technology websites provide some ev-
idence that people can refer to. For example, CNET2 pro-
vides reviews and speciﬁcations about software; speciﬁca-
tions include average user rating, the software’s version num-
ber, ﬁle size, release date, number of total downloads, num-
ber of downloads last week, etc. Norton also has antivirus
tools featuring File Insight, the purpose of which is to pro-

2http://www.cnet.com

396Table 4: A list of trust evidence that can be provided to users
when they download software. Each piece of evidence is scalable
for automatic retrieval from the Internet; this table presents which
pieces of evidence that can be made robust against spooﬁng attacks.

Evidence

Robust

General information about software
• Developer
• Version Number
• Date added
• File size
• File name
• Category of software (e.g., video player, game)
• Digitally-signed certiﬁcate
Origin of the downloading software & developer
• Hosting site (that users click to download ﬁles)
• Source address (where ﬁles are downloaded from)
• PageRank of hosting site and source address
• Information about the developer (e.g., business history,
location, number of employees, etc.)
Crowdsourcing
• Total number of people who downloaded the software
• Total number of people who downloaded last week
• Ratings by people who downloaded the software
• Reviews
• Recent changes in ratings
Reputable 3rd-party resource
• (OSN) Expert friends who also downloaded the same
software
• (OSN) Suggestion of more popular/credible software
within the same category
• (Authorities) Credible newspaper/magazine articles
featuring the software







































vide speciﬁcations about the ﬁles that users are about to
download. Similar to CNET, Norton File Insight provides
the information regarding developers, release date, software’s
version number, number of Norton community users who
have used the same software, date when the software was
released, Norton’s rating on the software, where the ﬁle is
being downloaded from, etc.

However, some of the speciﬁcations provided by the exist-
ing tools and technologies may not be robust against spoof-
ing attacks. For example, attackers can easily increase ver-
sion numbers to seem as though the software providers up-
date the software frequently. Similarly, the release date of
the software could potentially be backdated.

As a ﬁrst step, we have analyzed the pieces of trust ev-
idence that are both robust against spooﬁng attacks and
scalable for automatic retrieval from the Internet. Table 4
lists the evidence that satisﬁes our criteria.

Using the evidence that is both robust and scalable, we

describe how OTO displays them in the next section.

4.3 Presenting Positive and Negative Evidence
Each of the pieces of trust evidence, as shown in Table 4,
can be perceived as positive (supporting that a ﬁle may be
safe to download) or negative (conﬁrming that a ﬁle may
be harmful). For example, “Friend’s Use” can be a negative
reason if none of a user’s security-expert friends and family
downloaded the same ﬁle (Figures 3 and 5), but it can be a
positive reason if many friends and family downloaded the
ﬁle (Figure 4).

For the evidence from authorities, we assume that OTO
uses an algorithm that can determine if an article is positive
on a piece of software or whether it declares the software as
malware. Given the recent advances in text understanding,
we assume such analysis to be viable and reliable [4].

Note that the OTO interface displays a summary of each
If users want to examine the evidence in

evidence type.

detail, OTO provides a clickable link that will display more
detailed explanation regarding that evidence.

The OTO interface only displays 2 pieces of supporting
evidence for each column. If there are more than 2 pieces
of evidence, OTO provides a clickable link that can show all
supporting evidence for the particular column.

Furthermore, OTO considers the “File Origin” evidence
as neutral evidence because determining whether the down-
loading ﬁle is legitimate, based on the source address, is a
non-trivial question. As a result, “File Origin” is displayed
below the reasoning table. If OTO is able to consult credible
blacklists of malicious websites, OTO can alert users if the
hosting site was found on the blacklist(s).

5. SECURITY ANALYSIS

In this section, we delineate how OTO detects malware
(Section 5.1) and how OTO handles misclassiﬁcations of
software (Section 5.2). In Section 5.3, we also describe how
OTO mitigates manipulation attacks as mentioned in Sec-
tion 2.3.

5.1 Detecting Malware

In this section, we describe how OTO successfully detects
malware and provides helpful evidence to users. In general,
two types of malware are as follows:
In case of zero-day malware, lack of
Zero-day malware.
available positive evidence should alarm users to be careful
about downloading the ﬁle.
Well-known malware.
If the malware is well-known,
the evidence gathered by the OS is likely to contain sub-
stantially more pieces of negative evidence than positive ev-
idence. Given a high number of negative evidence and the
strength of negative evidence (compared to those of positive
evidence), there is a high probability that users will detect
such malware as harmful.

5.2 Handling False Alarms

There is a possibility that OTO misclassiﬁes software:
(false negative) given undeniably malicious software, OTO
may fail to detect it as malicious when the number and/or
the quality of positive evidence is proportional to that of
negative evidence; (false positive) OTO may misclassify le-
gitimate software as malicious.

In such cases, users can detect false alarms by examin-
ing the evidence. For example, as shown in Figure 3, Alice
can analyze each piece of evidence and judge whether nega-
tive reasons outweigh positive reasons: are (1) a threat blog
discussion on this software and (2) no usage by the user’s
security-expert friends and family (along with a third rea-
son) stronger than (1) high number of available reviews, (2)
high popularity, and 2 other reasons? Since all pieces of evi-
dence are drawn from Alice’s preferences to help her decide,
Alice can make her own informed trust decision based on
the displayed information.

One may argue that Alice may still ﬁnd it diﬃcult to an-
alyze the displayed information. However, what is displayed
is likely to be what she would have searched online her-
self. Given the easy availability of the information increases
the likelihood that she will consider the information. Plus,
the evidence originates from trustworthy resources that she
deﬁnes. Hence, she is safe from reading misguiding informa-
tion that she could have found on the Internet, and OTO
introduces no extra risk to Alice.

3975.3 Mitigating Manipulation Attacks

The OTO system is designed to disrupt malware distribu-
tors from manipulating positive and negative evidence. We
discuss three potential categories of attack.
Generating falsifying evidence.
Attackers can at-
tempt to create fake positive evidence for malware. In Ta-
ble 4, we present the list of pieces of evidence that we believe
can be made robust. For example, the total number of down-
loads can be made robust by analyzing the user population
and temporal aspects of the download – in case the attacker
rents a botnet to inﬂate the number of downloads the sudden
spike in downloads and the population distribution should
appear anomalous.

While it is outside the scope of this paper to ensure the
robustness of each indicator, we would like to emphasize
that a successful attack would need to forge several positive
pieces of evidence, which will likely be negated by a strong
negative piece of evidence that would still alert the user.

If an attacker creates positive articles about the software,
it would need to get them published at an authoritative site
trusted by the user to be considered by OTO.
Hiding harmful evidence.
Attackers may attempt to
prevent the OS from fetching negative evidence or prevent an
online resource from serving negative evidence. While these
attacks are outside the scope of this paper, we believe that it
would be challenging for attackers to prevent authoritative
resources from serving information about the malware.
Impersonation of legitimate software. Attackers may
attempt to impersonate a well-known software system and
claim an update or a free installation. We assume that
OTO can perform secure associations of the pieces of evi-
dence with the software, for example using a cryptographic
hash value of the software. It will, however, be challenging
for written articles to form the correct association with the
correct piece of software, as journalists rarely include the
cryptographic hash of the software they are writing about.
In case OTO is widely deployed, mechanisms for associating
pieces of evidence with software need to be used, for exam-
ple by including the public key of the software distributor or
the cryptographic hash of the software in a written article.

6. EVALUATION

We conducted a user study to test whether OTO achieves

the desired properties as described in Section 2.2.

6.1 Demographics

We recruited 58 participants from a diverse set by adver-
tising on Craigslist, ﬂyers around bus stops, Facebook post-
ings, and university mailing lists. We randomly assigned 29
participants to each of the OTO and SSF conditions. Ta-
ble 5 summarizes the demographics.

6.2 User Study Process

We designed a between-subjects experiment with two con-
ditions, the SSF and OTO conditions, each of which consists
of 10 scenarios as described in Table 2.

We consider the SSF condition as a baseline for the fol-
lowing reason: According to NSS Labs, SSF is the current
state-of-the-art technology3 that is widely used on a modern
browser, which ensures that the users are well aware of IE’s

3http://www.pcmag.com/article2/0,2817,
2391164,00.asp

Table 5: Demographics of study participants for OTO and SSF
conditions.

Group size (N )

Gender

Age

Occupation

Educational background

Security knowledge level

Male
Female
Minimum
Maximum
Average (µ)
Std dev (σ)
Student
Other
High school
Bachelor
Post-graduate
Novice
Intermediate
Expert

OTO SSF
29
15
14
18
59
29.0
10.4
16
13
8
14
7
4
17
8

29
15
14
20
48
26.7
6.5
16
13
3
19
7
2
19
8

Total

58
30
28
18
59
27.8
8.7
32
26
11
33
14
6
36
16

interface from prior experience. SSF checks the software
that a user clicks to download against a known blacklist of
malicious software.
If the software is ﬂagged, then a red-
banner warning appears, intentionally delaying the down-
load procedure, and the user must click on the “Actions”
button to proceed with the download. Figure 6 depicts the
two SSF status bars that appear at the bottom of a browser:
a warning and a normal download deemed legitimate.

For our experiment, we modiﬁed the mockup slides for the
experts’ user study in Section 3.4 as follows:4 two conditions
diﬀer in the last step of each scenario when the interviewer,
who is role-playing as a close friend of the participant, clicks
on the download button. The SSF condition prompts IE’s
SSF dialog box (Figure 6), while OTO prompts the evidence
interface (Figures 3–5). Figures 7 and 8 show the snapshots
of the SSF and OTO interfaces, respectively. Similar to the
expert user study as described in Section 3.4, we asked each
participant to pretend that (s)he was helping a close friend
make a trust decision when downloading a piece of software
from the Internet.5
Pre-study.
Before we started the experiment, we pre-
sented 6 pre-study questions in random order to identify
each participant’s security knowledge level (see Table 6 for
questions). We then classiﬁed the participants as novice, in-
termediate, or expert if they correctly answered 0–2, 3–4, or
5–6 questions, respectively. Table 5 shows the classiﬁcation
of the participants based on their security knowledge.
During study. Before we began, we told the participants
that they would be given 10 scenarios, where some software
may be safe to download and some may be harmful to the
computer. Their task would be to recommend whether the
friend should proceed or cancel the download. We also ex-
plained that each participant would be paid $15, plus an ad-
ditional $1 for each question that they answered correctly,
with a maximum payment of $25. This payment method
was to incentivize the participants to provide the best ed-
ucated guess and mimic reality, rather than be extremely-
conservative or liberal.

4From the end-user’s point of view, only user experience matters,
and participants commented that our study was indistinguishable
from a real environment.
5We observed that participants cared more about providing hon-
est/correct advice to close friends than about protecting the lap-
top of an unrelated person (i.e., the interviewer). Hence, the role-
playing increased participation through passive help. Furthermore,
our experiment setting ensured that all participants were tested us-
ing the same browser and OS settings.

398Table 6: A pre-study questionnaire to identify the participants’ security knowledge.

• While browsing the web, you see a pop-up window showing that your computer is infected with viruses. This warning recommends

that you download antivirus software to delete the viruses from your computer. Do you think this software is safe to download?

• One of your close friends recommend a product called K7 SECURE-WEB for secure online ﬁnancial transactions. You google
the product and click the #1 top search result, which redirects you to http://www.k7computing.com/en/Product/
k7-secureweb.php. You see a link to purchase this product for $19.99. Do you think this software is safe to purchase?

• You log into Facebook and see that your friend posted a video clip entitled: OMG I just hate RIHANNA after watching this video.
When you click the video, you get a prompt asking you to install ActiveX codec player to watch this video clip. Do you think this
video codec is safe to download?

• While browsing the web, you see an advertisement about Google’s super-fast Chrome browser. When you click this advertisement,
you see an instruction page to install Chrome from https://www.google.com/chrome/. Do you think this software is safe to
download?

• You receive an instant message, such as AOL IM, MSN messenger, Google talk, etc., from your friend to download identity protection
software. When you click the link, it redirects you to http://download.cnet.com/ to download the software. Do you think
this software is safe to download?

• You are checking your email and you see that you have an urgent message from your bank. The email message says that the bank is
upgrading their software to improve your safety and security, and asks you to install the software that is attached to the email. Do you
think this software is safe to download?

productkey_setup.exe is not commonly downloaded and could harm your computer.

Delete

Actions

View Downloads

(a) A SSF warning for potentially malicious ﬁle download.

Do you want to run or save DownloadXPro.exe (650KB) from software-!les-a.cnet.com?

Run

Save

Cancel

(b) A SSF interface for normal downloads.

Figure 6: Microsoft IE’s SmartScreen Filter (SSF) interfaces.

Figure 7: A screenshot of a SmartScreen Filter (SSF) interface.
The SSF dialog box is shown at the bottom of the IE browser.

During the experiment, the interviewer walked through
the scenarios, and asked the same questions that we used for
the expert user study in Section 3.4. For this experiment, we
categorized the ten scenarios into the following four cases:

1. True positive (TP): the system properly detects mal-
ware as malicious. In this case, SSF displays a red warning
bar on the bottom of the screen, and OTO displays a red
interface with only negative evidence.

2. True negative (TN): the system properly detects the
legitimate software as legitimate. In this case, SSF dis-
plays a yellow warning bar on the bottom of the screen,
and OTO displays a blue interface with only positive ev-
idence.

3. False positive (FP): the system incorrectly detects the
legitimate software as malicious. In this case, SSF displays
a red warning bar on the bottom of the screen, forcing

Figure 8: A screenshot of an OTO interface. The OTO dia-
log box displays positive and negative pieces of evidence on the
grayed-out screen.

users to stop downloading, and OTO presents a yellow
OTO interface displaying more suspicious evidence than
trustworthy evidence.

4. False negative (FN): the system incorrectly detects mal-
ware as legitimate.
In this case, SSF displays a yellow
download warning bar on the bottom of the screen, and
OTO displays a yellow OTO interface with more positive
evidence than negative evidence.

Among 10 scenarios, we assigned two pieces of software
with easy to medium diﬃculty level to TP and TN cases, and
three pieces of software with medium to diﬃcult level to FP
and FN cases. Table 7 describes the scenario assignments.

When the interface prompted, we asked each participant if
(s)he thought the software was safe to download or harmful

399Table 7: Scenario assignments for TP, TN, FP, and FN cases.

h Legitimate

t
u
r
t

d
n
u
o
r
G

Malicious

System detection outcome

Legitimate

TN

Ahnlab

MindMaple

FN

ActiveX codec
HDD diagnostic

Adobe ﬂash

Malicious

FP
Rkill

Kaspersky

SPAMﬁghter

TP

Windows activation

Privacy violation

to the friend’s computer. Participants were given the fol-
lowing answer choices: (1) legitimate, (2) malicious, and (3)
unsure (each scenario had 1 correct answer and “unsure” was
counted as incorrect). We asked them to think aloud, and
the PowerPoint automatically logged the time duration until
the participants made trust decisions. After they decided,
we asked for justiﬁcations and while the users were thinking
aloud, the interviewer logged the answers for analysis.
Post-study.
After the experiment, we asked the partic-
ipants to answer questions to obtain feedback on the inter-
faces. More speciﬁcally, we asked how much the diﬀerent
types of evidence (as shown in Table 4) would help the par-
ticipants when downloading software.

6.3 General Observations

In this experiment, we observed that many participants
tend to trust their past experience and security knowledge
in certain scenarios that they are familiar with (e.g., TP and
FN scenarios); hence they used the SSF and OTO interfaces
for conﬁrmation of their judgments.

In the SSF condition, many participants were conservative
when prompted with a pop-up that they did not initiate. In
the OTO study, however, we noticed that the participants
began to rely on the evidence that the OTO interface pro-
vided, especially for those scenarios that were unfamiliar to
them. Many participants claimed that the websites looked
legitimate but they were not aware that the websites existed.
This allowed them to focus their attention on the evidence
to gain more knowledge, especially for TN and FP scenarios.
One concern with the OTO interface was that users may
need to spend time to read the evidence. However, we
observed that the participants did not take a signiﬁcantly
longer time with the OTO interface, as compared to the
SSF interface. Figure 9 shows the average response time
from 24 random participants (N = 11 for OTO, N = 13
for SSF) who did not think aloud while making trust deci-
sions, and Table 8 summarizes the average and maximum
time. One interesting observation is that overall, the par-
ticipants took less time to make trust decisions for OTO
compared to SSF. More speciﬁcally, the participants took
approximately the same amount of time for FN cases re-
gardless of the interface they were given, and the partici-
pants given the OTO interface took less time to decide for
FP, TP, and TN cases. This result is due to the follow-
ing observation from the experiment: In the SSF condition,
many participants took some time to make trust decisions
given scenarios that they never experienced before. In the
OTO condition, however, the participants relied on the dis-
played evidence lists to make trust decisions, resulting in
faster decisions compared to SSF.

Next, we analyze in detail the eﬀectiveness of OTO as

compared to SSF.

Figure 9: The average time that the participants spent to make a
trust decision on each scenario for SSF and OTO conditions (N =
13 for SSF, N = 11 for OTO).

Table 8: Mean and maximum time that the participants spent to
make trust decisions (N = 13 for SSF, N = 11 for OTO).

OTO

Mean

10.1 ± 6.9
6.6 ± 3.7
9.5 ± 5.0
11.0 ± 5.8
12.0 ± 9.4

Max
48
14
20
31
48

SSF

Mean

12.3 ± 7.6
12.1 ± 8.5
11.2 ± 6.5
13.3 ± 7.0
12.2 ± 8.4

Max
34
32
26
31
34

Overall

TP
TN
FP
FN

6.4 Effectiveness Analysis

We analyze if the OTO interface helps users make correct
trust decisions compared to Microsoft’s SSF interface (base
case). Based on the number of correct answers provided by
58 participants (29 participants for each condition), we ran a
Repeated Measures ANOVA test. The results conﬁrm that
the OTO interface helps people make more correct trust
decisions compared to the SSF interface regardless of the
participants’ background security knowledge, education level,
occupation, age, or gender. Table 9 summarizes the results.
We designed our study to test whether OTO helps users
make correct trust decisions even if the OS mistakenly cate-
gorizes legitimate software as malicious and vice versa. Be-
low is a list of hypotheses for 4 cases, along with detailed
analysis on how our study participants responded for 4 cases
using Mixed Models. In general, a signiﬁcant main eﬀect ex-
ists for diﬀerent interface conditions with 4 cases taken into
account (F (1, 65) = 18.1, p < .001), and the main eﬀect of 4
diﬀerent cases is signiﬁcant (F (3, 517) = 12.61, p < .0001).
Table 9 summarizes the results for speciﬁc cases.

True Positive (TP).
In the TP case, the OS correctly
identiﬁes malware. We tested two TP scenarios (“Windows
activation” and “Privacy violation”), given the following hy-
pothesis:

Hypothesis 1. When software is malicious and the OS
detects it as malicious, users given the OTO interface detect
the malware at least as well as those with the SSF interface.
The Mixed Model delivered no signiﬁcant diﬀerence be-
tween OTO and SSF for TP scenarios. Hence, Hypothesis 1
is valid and OTO performs at least as well as SSF for the
TP case.
True Negative (TN).
In the TN case, the OS correctly
identiﬁes legitimate software as legitimate. For 2 TN sce-
narios, namely “AhnLab” and “SPAMﬁghter Pro,” our hy-
pothesis is as follows:

Hypothesis 2. When software is indeed legitimate and
the OS detects it as legitimate, users given the OTO interface
detect the legitimacy at least as well as those given the SSF
interface.

400Table 9: Summary of Repeated Measures ANOVA (Overall), Mixed Models (TP, TN, FP, FN), and ANOVA (Usefulness, Annoyance)
results for the effectiveness of OTO compared to SSF (N = 58, 29 for each condition). The higher mean that is statistically signiﬁcant from
the other is highlighted in bold.

Overall

TP

TN

FP

FN

min: 0, max: 1

min: 0, max: 1

min: 0, max: 1

min: 0, max: 1

min: 0, max: 1

µ

.86
.67

σ ¯X
.03
.03

µ
.96
.93

σ ¯X
.05
.05

µ

.91
.64

σ ¯X
.05
.05

µ

.79
.60

σ ¯X
.05
.05

µ

.79
.57

σ ¯X
.05
.05

Usefulness

min:1, max:5

Annoyance

min:1, max:5

µ

4.24
4.10

σ
.58
.72

µ

3.75
3.17

σ

1.02
1.10

F (1, 51) = 4.03

F (1, 425) = .18

F (1, 425) = 12.49

F (1, 294) = 8.86

F (1, 294) = 11.09

F (1, 57) = .64

F (1, 57) = 4.40

p < .0001

p = .67

p = .0005

p = .003

p = .001

p = .43

p = .04

OTO
SSF

Results

µ: mean, σ ¯X : standard error, σ: standard deviation

There was a statistically signiﬁcant diﬀerence between
OTO and SSF conditions for TN scenarios, as shown in Ta-
ble 9. Hence, Hypothesis 2 is valid and users do indeed make
signiﬁcantly better trust decisions with OTO compared to
SSF.
False Positive (FP).
In the FP case the OS identiﬁes
legitimate software as malicious. We tested three FP sce-
narios (“Kaspersky”, “Rkill”, and “MindMaple”) given the
following hypothesis:

Hypothesis 3. Compared to SSF, OTO enables users to
download legitimate software that the OS mistakenly detects
as malicious.

There was a statistically signiﬁcant diﬀerence between
OTO and SSF for FP scenarios, as shown in Table 9. Hence,
for cases where browsers miscategorize legitimate software as
malicious, OTO’s trustworthy and suspicious evidence sig-
niﬁcantly helps users make correct and informed trust deci-
sions.
False Negative (FN).
In the FN case, the OS fails to de-
tect malware. For three FN scenarios (“HDD Diagnostics”,
“ActiveX codec”, and “Adobe ﬂash update”), our hypothesis
is the following:

Hypothesis 4. Compared to SSF, OTO prevents users

from downloading malware that the OS does not detect.

We found a statistically signiﬁcant diﬀerence between OTO
and SSF for FN scenarios (see Table 9). Hence, for cases in
which browsers fail to detect malware, the trustworthy and
suspicious evidence provided in the OTO interface signiﬁ-
cantly helps users make correct trust decisions.
Discussion. The marginal interaction eﬀect (F (3, 517) =
2.04, p = .10) suggests that the TN case was especially
helped by OTO. It also shows that the FN and FP cases
were helped by OTO, but not the TP case. Hence, OTO’s
approach of providing positive and negative trust evidence
helps people make correct trust decisions, even if the inter-
face misclassiﬁes software.

6.5 Usability Analysis
Usefulness. During the post-test of both SSF and OTO
studies, we posed a 5-point Likert scale question to measure
how useful participants found the corresponding interface,
with the following hypothesis:

Hypothesis 5. Users ﬁnd the OTO interface at least as

useful as the SSF interface.

There was no statistically signiﬁcant diﬀerence between
two interfaces for usefulness (see Table 9), hence satisfying
our hypothesis.
Annoyance. We asked participants how much they were
annoyed by given interface. Our hypothesis was as follows:
Hypothesis 6. Since the OTO interface can potentially
contain more information than the SSF interface, users may
ﬁnd the OTO interface more annoying than the SSF inter-
face.

Figure 10: Mean and standard deviation of 20 forms of trust
evidence using 5-point Likert scales. Participants rated how they
consider each evidence as helpful in validating the software legiti-
macy on a 5-point Likert scale.

We were able to ﬁnd a statistically signiﬁcant diﬀerence
between two interfaces, nullifying Hypothesis 6.
In other
words, the participants found OTO more comfortable to use.

6.6 Desired List of Evidence

At the end of each study, we gave participants a list of
pieces of evidence for assessing software legitimacy (same
as what is presented in Table 4), and asked them to rank
the usefulness of each piece of evidence on a 5-point Likert
scale. Figure 10 shows the mean and the standard deviation
of each piece of evidence.

In general, participants reported the robust and scalable
evidence as helpful for trust assessments, except information
about the developer and the category of software. In partic-
ular, participants found the evidence suggested by security
experts helpful.

7. RELATED WORK

In this section we discuss related research covering various
aspects of security warning design and online user behavior:
user mental models of security risks, mitigating the eﬀect of
habituation, and user assessments of content credibility.

7.1 User Mental Models

Prior research has demonstrated the importance of a user’s
mental model of security risks. Sunshine et al. have analyzed
how risk perception plays a large role in how people respond
to SSL warning messages, which are used to notify users
of a potential Man-in-the-Middle attack [17]. Their results
show that, in general, all the warning signs did not prevent
many of the users from exhibiting unsafe behavior, even for
security experts who perceived the warnings as a lower risk
than, for example, a mismatched certiﬁcate. This may be
an even bigger issue for those not familiar with computing
security and privacy because their perception of risk may

401be even lower. For example, these users may be unaware
of the incentives for cyber-criminals or the scalable nature
of online attacks and therefore do not perceive a signiﬁcant
personal risk.

Similarly, Bravo-Lillo et al. have addressed psychological
responses to warning messages in terms of how users per-
ceive and react to computer alerts [6]. Using various warn-
ings from popular operating systems, their study revealed
that many users have the “wrong” mental model for many
computer warnings. For example, understanding SSL warn-
ings is diﬃcult if one does not know about certiﬁcates or
Man-in-the-Middle attacks. The authors argue that warn-
ings should be a third line of defense, after designing out the
risk and guarding against the risk.

Wash illustrates that home users tend to follow some (but
not all) pieces of security advice from experts based on the
identiﬁcation of folk models of security threats [19]. The
author ﬁnds that some users with certain folk models (e.g.,
viruses are generally bad, viruses are buggy software) believe
that the avoidance of intentional downloading and execution
is enough protect them from virus infection. Based on the
analysis of eight folk models on viruses, hackers, and bot-
nets, the author suggests that security technologies should
not only focus on actionable advice, but also clearly explain
potential threats that users may face.

Motiee has explored how to create informative contents to
Microsoft Windows 7 User Account Control warnings [14].
More speciﬁcally, the author focuses on the information con-
tent to help users assess risk and correctly respond to warn-
ings. Based on the user study, the author states that the
most understandable and useful pieces of content for users
are as follows: program name, origin, description, certiﬁ-
cation, changes to apply and result of antivirus scan. The
author also suggests selecting a context-based subset of the
content to avoid habituation. However, the majority of the
contents is not robust against spooﬁng attacks, thus ques-
tioning the eﬀectiveness of the warnings.

We address the requirement of a correct mental model
for security risk assessments by incorporating robust tradi-
tional evidence (e.g., ﬁle origin information) with evidence
that may be more familiar to novice computer users (e.g.,
OSN data and reviews from authorities) and presenting the
information in an intuitive way such that users can make
informed trust decisions without heavily relying on the se-
curity assessment by the underlying operating system.

7.2 Habituation

The research community has studied the eﬀects of ha-
bituation of users to warning dialogs. Egelman et al. have
analyzed the eﬀectiveness of browser warnings [11]. Specif-
ically, they compare the eﬀectiveness of passive and active
warnings in getting users to avoid spear phishing attacks.
They found that the majority of users (97%) were deceived
by least one of the phishing messages. Many users simply
did not notice warning signs. However, 79% of the partic-
ipants presented with active warnings heeded them. This
result conﬁrms that habituation is a major road block for
creating eﬀective warning messages.

To mitigate the tendency of users who ignore security di-
alogs, Brustoloni et al. have explored the use of polymorphic
and audited dialogs to defend against risky email attach-
ments [7]. Since a polymorphic dialog continually changes,
users are forced to examine the dialog even if their goal is

to bypass it as quickly as possible. Auditing involves warn-
ing users that their responses will be forwarded to an au-
ditor and their account may be quarantined based on those
responses. Their results show that untrained users accept
signiﬁcantly less unjustiﬁed risk using polymorphic and au-
dited dialogs as compared to conventional dialogs, but at
the cost of usability: Expert users who already know what
decisions they want to make may ﬁnd a constantly changing
interface annoying. Furthermore, it may be challenging to
enforce an auditing policy in cases where there is no central
authority (e.g., home or public network).

OTO mitigates habituation by varying the color of warn-
ing dialog boxes based on the severity of the risks. Moreover,
every software download generates a unique set of evidence,
encouraging users to read the warnings.

7.3 Assessing Credibility Online

Systems in the past have looked at other domains outside
of software installation. For example, Schwarz and Morris
have explored augmenting web pages and search results with
visualizations presenting credibility features [16]. However,
the credibility features, which were selected by the authors,
are highly subjective and can often times provide security
ﬂaws. For example, the “popularity by experts” feature,
which the study subjects found to be most useful, is ques-
tionable in terms of how the system selects the experts.

Fogg et al. have evaluated the credibility of two web-
sites covering similar topics by applying the Prominence-
Interpretation theory [12]. They found that the design of
the site was the most frequently utilized aspect for evaluat-
ing credibility, followed by information structure and infor-
mation focus. They also found that content is a factor that
aﬀects prominence as people notice diﬀerent aspects when
they examine diﬀerent types of sites.

8. CONCLUSION

Users are often confronted with vexing trust decisions of
whether to download a given piece of software. Unfortu-
nately, automated techniques have proven inadequate so far,
as malware continues to thrive. One solution is to provide
users with additional trust evidence to enable them to make
better trust decisions.
IE’s SmartScreen Filter (SSF) is a
promising step in this direction. As we discover in the two
user studies discussed in this paper, experts generally agree
what additional trust evidence is required for making trust
decisions, and novice users can make better trust decisions
with such information, even improving on SSF with sta-
tistical signiﬁcance. We hope that these results encourage
further research in this important area to ultimately enable
users to make correct trust decisions with high conﬁdence.

9. ACKNOWLEDGMENTS

We gratefully thank Sara Kiesler for her insightful feed-
back and help with statistical analyses, and anonymous re-
viewers for their valuable comments.

This research was supported by CyLab at Carnegie Mellon
under grants DAAD19-02-1-0389, and W911NF-09-1-0273
from the Army Research Oﬃce, by support from NSF un-
der awards CCF-0424422, CNS-1040801, and IGERT Dge-
0903659, and by Singapore National Research Foundation
under its International Research Centre @ Singapore Fund-
ing Initiative and administered by the IDM Programme Of-

402ﬁce. The views and conclusions contained here are those
of the authors and should not be interpreted as necessar-
ily representing the oﬃcial policies or endorsements, either
expressed or implied, of ARO, CMU, NSF, or the U.S. Gov-
ernment or any of its agencies.

10. REFERENCES
[1] Microsoft Security Intelligence Report. http:

//download.microsoft.com/download/0/3/3/
0331766E-3FC4-44E5-B1CA-2BDEB58211B8/
Microsoft_Security_Intelligence_Report_
volume_11_English.pdf, 2011.

Constructing Trustworthy Trust Indicators. PhD
thesis, Carnegie Mellon University, 2009.

[11] S. Egelman, L. F. Cranor, and J. Hong. You’ve been

warned: an empirical study of the eﬀectiveness of web
browser phishing warnings. In Proceedings of the 26th
annual SIGCHI conference on Human factors in
computing systems, 2008.

[12] B. Fogg, C. Soohoo, D. R. Danielson, L. Marable,

J. Stanford, and E. R. Tauber. How Do Users Evaluate
the Credibility of Web Sites? A Study with Over
2,500 Participants. In Proceedings of the Conference
on Designing for User Experiences (DUX), 2003.

[2] Sophos Security Threat Report 2011. http:

[13] C. Kuo. Reduction of End User Errors in the Design

//www.sophos.com/sophos/docs/eng/papers/
sophos-security-threat-report-2011-wpna.
pdf, 2011.

[3] Sophos Security Threat Report 2012.

http://www.sophos.com/medialibrary/PDFs/
other/SophosSecurityThreatReport2012.pdf,
2012.

[4] This Is Watson. IBM Journals of Research and

Development, May/Jul 2012.

[5] P. Ayyavu and C. Jensen. Integrating User Feedback

with Heuristic Security and Privacy Management
Systems. In Proceedings of Proceedings of the annual
SIGCHI conference on Human factors in computing
systems, 2011.

[6] C. Bravo-Lillo, L. F. Cranor, J. S. Downs, and
S. Komanduri. Bridging the Gap in Computer
Security Warnings. IEEE Security and Privacy, 2011.

[7] J. C. Brustoloni and R. Villamarin-Salomon.

Improving Security Decisions with Polymorphic and
Audited Dialogs. In Proceedings of Symposium on
Usable Privacy and Security (SOUPS), 2007.

[8] R. Dhamija, J. Tygar, and M. Hearst. Why Phishing

Works. In Proceedings of the annual SIGCHI
conference on Human factors in computing systems,
2006.

[9] J. S. Downs, M. B. Holbrook, and L. F. Cranor.

Decision Strategies and Susceptibility to Phishing. In
Proceedings of Symposium on Usable Privacy and
Security (SOUPS), 2006.

[10] S. Egelman. Trust Me: Design Patterns for

of Scalable, Secure Communication. PhD thesis,
Carnegie Mellon University, 2008.

[14] S. Motiee. Towards Supporting Users in Assessing the

Risk in Privilege Elevation. Master’s thesis, The
University of British Columbia, 2011.

[15] P. O’Kane, S. Sezer, and K. McLaughlin. Obfuscation:

The Hidden Malware. IEEE Security & Privacy
Magazine, Sept. 2011.

[16] J. Schwarz and M. R. Morris. Augmenting web pages

and search results to help people ﬁnd trustworthy
information online. In Proceedings of the annual
SIGCHI conference on Human factors in computing
systems, 2011.

[17] J. Sunshine, S. Egelman, H. Almuhimedi, N. Atri, and

L. F. Cranor. Crying wolf: an empirical study of ssl
warning eﬀectiveness. In Proceedings of the 18th
conference on USENIX security symposium, 2009.
[18] A. Vishwanath, T. Herath, R. Chen, J. Wang, and

H. R. Rao. Why do people get phished? Testing
individual diﬀerences in phishing vulnerability within
an integrated, information processing model. Decision
Support Systems, 2011.

[19] R. Wash. Folk Models of Home Computer Security. In

Proceedings of the Symposium on Usable Privacy and
Security (SOUPS), 2010.

[20] M. S. Wogalter. Handbook of Warnings, chapter
Communication-Human Information Processing
(C-HIP) Model, pages 51–61. Lawrence Erlbaum
Associates, 2006.

403