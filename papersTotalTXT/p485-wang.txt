Anatomy of a Personalized Livestreaming System

Bolun Wang†, Xinyi Zhang†, Gang Wang† ‡, Haitao Zheng† and Ben Y. Zhao†

†Department of Computer Science, UC Santa Barbara

‡Department of Computer Science, Virginia Tech

{bolunwang, xyzhang, gangw, htzheng, ravenben}@cs.ucsb.edu

ABSTRACT
With smartphones making video recording easier than ever,
new apps like Periscope and Meerkat brought personalized
interactive video streaming to millions. With a touch, view-
ers can switch between ﬁrst person perspectives across the
globe, and interact in real-time with broadcasters. Unlike
traditional video streaming, these services require low-latency
video delivery to support high interactivity between broad-
casters and audiences.

We perform a detailed analysis into the design and perfor-
mance of Periscope, the most popular personal livestream-
ing service with 20 million users. Using detailed measure-
ments of Periscope (3 months, 19M streams, 705M views)
and Meerkat (1 month, 164K streams, 3.8M views), we ask
the critical question: “Can personalized livestreams continue
to scale, while allowing their audiences to experience de-
sired levels of interactivity?” We analyze the network path
of each stream and break down components of its end-to-end
delay. We ﬁnd that much of each stream’s delay is the di-
rect result of decisions to improve scalability, from chunking
video sequences to selective polling for reduced server load.
Our results show a strong link between volume of broadcasts
and stream delivery latency. Finally, we discovered a criti-
cal security ﬂaw during our study, and shared it along with a
scalable solution with Periscope and Meerkat management.

1.

INTRODUCTION

The integration of high quality video cameras in commod-
ity smartphones has made video recording far more conve-
nient and accessible than ever before. In this context, new
mobile apps such as Periscope and Meerkat now oﬀer users
the ability to broadcast themselves and their surroundings
using real-time interactive live streams. With the ﬂick of a
ﬁnger, a user can switch from a ﬁrst person view of Carnival

Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
IMC 2016, November 14-16, 2016, Santa Monica, CA, USA
© 2016 ACM. ISBN 978-1-4503-4526-2/16/11. . . $15.00
DOI: http://dx.doi.org/10.1145/2987443.2987453

in Rio, a guided stroll outside the Burj Khalifa in Dubai, or
a live view of swans in Lake Como, Italy. What makes these
apps compelling is that viewers can interact in real time with
broadcasters, making requests, asking questions and giving
direct feedback via “likes.” Today, popular livestreams cap-
ture thousands of users, and can cover everything from press
conferences, political protests, personal interviews, to back-
stage visits to concerts, TV shows, and sporting events.

Unlike existing video-on-demand streaming services or
live video broadcasts, real time interactivity is critical to the
livestreaming experience for both streamers and their audi-
ence. First, applications like Periscope allow audience to
generate “hearts” or “likes,” which is directly interpreted by
the broadcaster as positive feedback on the content, e.g. “ap-
plause.” Second, many streams involve broadcasters solicit-
ing input on content or otherwise “poll” the audience. Ex-
plicit support for polling is an feature on Twitter that has yet
to be integrated into Periscope. In both cases, the immediacy
of the feedback is critical, and delayed feedback can produce
negative consequences. For example, a “lagging” audience
seeing a delayed version of the stream will produce delayed
“hearts,” which will be misinterpreted by the broadcaster as
positive feedback for a later event in the stream. Similarly, a
delayed user will likely enter her vote after the real-time vote
has concluded, thus discounting her input. Participants in a
recent user study answered that as broadcasters, the imme-
diate interaction with audience members was an authentic,
higher level of engagement, and it was valuable input for
their work and personal branding [45].

Minimizing streaming delay is already a signiﬁcant chal-
lenge for livestreaming services today. To minimize delay
for those commenting on broadcasts, Periscope only allows
100 viewers to comment on a broadcast (usually the ﬁrst
100 to join the stream) [30]. In practice, the ﬁrst 100-200
Periscope users to join a stream are connected to a more di-
rect distribution network with much lower delay (running
Real Time Messaging Protocol (RTMP) [2]), and later ar-
rivals to a high delay CDN for better scalability (running
HTTP Live Streaming (HLS) [3]). While Periscope claims
this is to avoid overwhelming broadcasters, users have al-
ready found this “feature” highly limiting, and have pro-
posed multiple hacks to circumvent the system [22, 19, 20].
Not only does this approach artiﬁcially limit user interac-
tions, but it also prevents more advanced modes of group in-

485teraction, e.g. instantaneous polls for the audience (already
a feature inside Twitter).

In this paper, we study the critical issue, “can personal-
ized livestreaming services like Periscope continue to scale
up in viewers and streams, while allowing large audiences
to experience the interactivity so core to their appeal?” This
leads us to perform an experimental study of Periscope and
its main rival, Meerkat1 2. We perform detailed measure-
ments of Periscope over 3 months in 2015, capturing activ-
ity from 19 million streams generated by 1.85 million broad-
casters, with a total of 705 million views (482M via mobile
and 223M via web). We also gathered measurement data of
Meerkat over 1 month, including 164K streams generated by
57K broadcasters, with a total number of 3.8 million views.
Our work is focused around three speciﬁc questions. First,
we need to understand how popular these systems are, how
they’re used by broadcasters and viewers. We want to un-
derstand current traﬃc loads, as well as longitudinal growth
trends in both users and streams. Second, we need to under-
stand the structure of the livestream delivery infrastructure,
and its contributions of end-to-end delivery delay. More im-
portantly, we are interested in understanding what the trade-
oﬀs are between delay and scalability, and how expected
growth in users is to impact streaming delay. Finally, can
the current system be optimized for improved performance,
and how is continued growth likely to aﬀect future perfor-
mance?

Our study produces several key ﬁndings:

• We ﬁnd that livestream services are growing rapidly,

In contrast, its competitor Meerkat is

with Periscope more than tripling number of daily streams
in 3 months.
rapidly losing popularity, e.g. losing half its daily streams
in a single month. Similar trends are observable in
daily active users. In network structure, Periscope more
resembles the structure of Twitter (likely due to the role
of asymmetric links in both networks), and less that of
Facebook (bidirectional links).

• In Periscope, a combination of RTMP and HLS pro-
tocols are used to host streams. For small streams,
RTMP using server-side push minimizes end-to-end
delay. For the most popular streams, chunking is used
with HLS to reduce server-side overhead, which intro-
duces signiﬁcant delays from both chunking and client-
side polling. In both cases, end-to-end delay is signiﬁ-
cantly exacerbated by aggressive client-side buﬀering.
Barring a change in architecture, more streams will re-
quire servers to increase chunk sizes, improving scala-
bility at the cost of higher delays.

• We use stream traces to drive detailed simulations of
client-side playback, and ﬁnd that current client-side
buﬀering strategies are too aggressive. We believe client-
side buﬀers (and associated latency) can be reduced by
half while still maintaining current playback quality.

1Facebook only recently began a similar service called Face-
book Live, and its user count is far smaller than its two ear-
lier rivals.
2Our study has obtained approval from our local IRB.

• Finally, we ﬁnd a signiﬁcant vulnerability to stream hi-
jacking in the current architectures for both Periscope
and Meerkat, possibly driven by a focus to scale up
number of streams. We propose a lightweight solution
and informed management teams at both companies3.

The results of our work serve to highlight the fundamental

tension between scalability and delay in personalized livestream
services. As Periscope and similar services continue to grow
in popularity (Periscope is now being integrated into hard-
ware devices such as GoPro cameras [7]), it remains to be
seen whether server infrastructure can scale up with demand,
or if they will be forced to increase delivery latency and re-
duce broadcaster and viewer interactivity as a result.

2. BACKGROUND AND RELATED WORK

2.1 The Rise of Periscope and Meerkat

Meerkat was the ﬁrst personalized livestreaming service
to go live, on February 27, 2015 [38]. It was a smartphone
app integrated with Twitter, using Twitter’s social graph to
suggest followers and using Tweets for live comments to
broadcasts. Two weeks later, Twitter announced its acqui-
sition of Periscope [13]. Within a week, Twitter closed its
social graph API to Meerkat, citing its internal policy on
competing apps [32]. Our measurement study began in May
2015, roughly 2 months after, and captured both the rise of
Periscope and the fall of Meerkat. By December 2015, it is
estimated that Periscope has over 20 million users [37].
The initial Periscope app only supported iOS
Periscope.
when it launched in March 2015. The Android version was
released on May 26, 2015. The app gained 10 million users
within four months after launch [15]. On Periscope, any user
can start a live video broadcast as a broadcaster, and other
users can join as viewers. While watching a broadcast, view-
ers can send text-based comments or “hearts” by tapping the
screen. For each broadcast, only the ﬁrst 100 viewers can
post comments, but all viewers can send hearts. Hearts and
comments are visible to the broadcaster and all viewers.

All active broadcasts are visible on a global public list. In
addition, Periscope users can follow other users to form di-
rectional social links. When a user starts a broadcast, all her
followers will receive notiﬁcations. By default, all broad-
casts are public for any users to join. Users do have the
option to start a private broadcast for a speciﬁc set of users.
Facebook Live was initially launched
Facebook Live.
in August 2015 (called “Mentions”) as a feature for celebri-
ties only. In December 2015, Facebook announced that the
app would open to all users. Facebook Live went live on
January 28, 2016. While we have experimented with the
new functionality during its beta period, it is unclear how
far the rollout has reached. Our paper focuses on Periscope
because of its scale and popularity, but we are considering
ways to collect and add Facebook Live data to augment our
measurement study.
3We informed CEOs of both Periscope and Meerkat in Au-
gust, 2015 to ensure they had plenty of time to implement
and deploy ﬁxes before this paper submission.

4862.2 Related Work

Researchers have stud-
Live Streaming Applications.
ied live streaming applications such as CNLive [28], and
Akamai live streaming [43] with focus on user activities and
network traﬃc. Unlike Periscope and Meerkat, these appli-
cations do not support real-time interactivity between users.
Siekkinen et al. studied Periscope with a focus on user ex-
perience and energy consumption, using experiments in a
controlled lab setting [39]. Twitch.tv is a live streaming ser-
vice exclusively for gaming broadcast [21, 16, 48]. Zhang et
al. use controlled experiments to study Twitch.tv’s network
infrastructure and performance [48]. Tang et al. analyzed
content, setting, and other characteristics of a small set of
Meerkat and Periscope broadcasts, and studied broadcasters’
motivation and experience through interviews [45]. Com-
pared to prior work, our work is the ﬁrst large-scale study
on personalized live streaming services (i.e., Periscope and
Meerkat) that support real-time interactivity among users.

A related line of work looks into the peer-to-peer (P2P)
based live streaming applications [42].
In these systems,
users form an overlay structure to distribute video content.
The system is used to stream live video but does not sup-
port user interactivity. Researchers have measured the traﬃc
patterns in popular P2P live streaming systems [17, 40] and
proposed mechanisms to improve scalability [49, 31, 41].
Existing works have studied stream-
Streaming Protocol.
ing protocols under the context of Video-on-Demand sys-
tems. Most studies have focused on HTTP-based protocols
such as DASH, HDS, and HLS including performance anal-
ysis [27, 33, 25] and improvement of designs [47, 18]. Oth-
ers have examined the performance of non-HTTP streaming
protocols such as RTMP in Video-on-demand systems [26].
Fewer studies have examined streaming protocols in the con-
text of live streaming. Related works mostly focus on HTTP
based protocols [29, 11].
Content Distribution Network (CDN). Most existing
literatures focus on general-purpose CDNs that distribute
web content or Video-on-Demand. Su et al. and Huang
et al. measure the CDN performance (latency, availability)
for popular CDNs such as Akamai and LimeLight [44, 6].
Adhikari et al. measure the CDN bandwidth for Video-on-
demand applications such as Netﬂix [10] and Hulu [9]. Kr-
ishnan et al. analyze CDN internal delay to diagnose net-
working issues such as router misconﬁgurations [24]. Kon-
tothanassis et al. explore Akamai’s CDN design for regular
media streaming [23]. Few studies have looked into CDNs
to deliver “real-time” content. In our study, we focus on live
streaming services that also demand a high level of real-time
user interactivity. We analyze streaming protocols and CDN
infrastructures in Periscope to understand the trade-oﬀs be-
tween latency and scalability.

3. BROADCAST MEASUREMENTS

We perform detailed measurements on Periscope and Meerkat

to understand their scale, growth trends and user activities.
Our goal is to set the context for our later analysis on stream

App

Periscope
Meerkat

Months
in 2015

3
1

Broad-
casters

Unique
Broad-
casts
Viewers
19.6M 1.85M 705M 7.65M
183K
164K

Total
Views

57K

3.8M

Table 1: Basic statistics of our broadcast datasets.

Network

Nodes

Edges

Periscope

Facebook [46]

Twitter [36]

231M
12M
1.22M
121M
1.62M 11.3M

Avg.
Degree

38.6
199.6
13.99

Cluster.
Coef.
0.130
0.175
0.065

Avg.
Path
3.74
5.13
6.49

Assort.

-0.057
0.17
-0.19

Table 2: Basic statistics of the social graphs.

delivery infrastructures. In the following, we ﬁrst describe
the methodology of our data collection and the resulting datasets,
and then present the key observations. Our primary mea-
surements focus on Periscope. For comparison, we perform
similar measurements and analysis on Meerkat, Periscope’s
key competitor.

3.1 Data Collection

Our goal is to collect a complete set of broadcasts on the
Periscope and Meerkat networks. For each network, we an-
alyze the network traﬃc between the app and the service,
and identify a set of APIs that allows us to crawl their global
broadcast list. Our study was reviewed and approved by our
local IRB. Both Periscope and Meerkat were aware of our
research, and we shared key results with them to help secure
their systems against potential attack (§7).
To collect a complete
Data Collection on Periscope.
set of broadcasts/streams on Periscope, we build a crawler
to monitor its global broadcast list. The global list shows 50
random selected broadcasts from all active broadcasts. To
obtain the complete list, we use multiple Periscope accounts
to repeatedly query the global list. Each account refreshes
the list every 5 seconds (the same frequency as the Periscope
app) and together we obtain a refreshed list every 0.25 sec-
onds. While our experiments show that a lower refresh rate
(once per 0.5 seconds) can already exhaustively capture all
broadcasts, i.e., capture the same number of broadcasts as
once per 0.25 seconds, we use the higher refresh rate to
accommodate potential burst in the creation of broadcasts.
Whenever a new broadcast appears, our crawler starts a new
thread to join the broadcast and records data until the broad-
cast terminates. For each broadcast, we collect the broad-
castID, starting and ending time of the broadcast, broad-
caster userID, the userID and join time of all the viewers,
and a sequence of timestamped comments and hearts. Only
metadata (no video or message content) is stored, and all
identiﬁers are securely anonymized before analysis.

We ran our crawler for 3+ months and captured all Periscope

broadcasts between May 15, 2015 and August 20, 20154.
As listed in Table 1, our dataset includes 19,596,779 broad-

4We received a whitelisted IP range from Periscope for
active measurements, but our new rate limits were un-
able to keep up with the growing volume of broadcasts on
Periscope.

487e
p
o
c
s
i
r
e
P

 
r
e
p

 
r
e
s
U

 
f

1M

800k

600k

400k

200k

Viewer
Broadcaster

Meerkat

Periscope

25k

20k

15k

10k

5k

t

a
k
r
e
e
M

 1

 0.8

 0.6

 0.4

 0.2

s
t
s
a
c
d
a
o
r
B

 
f

 

o
F
D
C

0
5/15 5/30 6/15 6/30 7/15 7/30 8/15

0

 0

10s

Date

Periscope
Meerkat

1min

10min

1h

6h

24h

Length of Broadcasts

y
a
D

 
r
e
p

 
s
t
s
a
c
d
a
o
r
B

 
f

e
p
o
c
s
i
r
e
P

 

o
#

s
t
s
a
c
d
a
o
r
B

 
f

 

o
F
D
C

350k

300k

250k

200k

150k

100k

50k

Periscope

Crawler Down

Meerkat

10k

8k

6k

4k

2k

t

a
k
r
e
e
M

0
5/15 5/30 6/15 6/30 7/15 7/30 8/15

0

Date

Figure 1: # of daily broadcasts.
 1

 0.8

 0.6

 0.4

 0.2

 0

01

Meetkat
Periscope

100

10
# of Viewers per Broadcast

1k

10k

100k

y
a
D

 

o
#

s
t
s
a
c
d
a
o
r
B

 
f

 

o
F
D
C

Figure 2: # of daily active users.

Figure 3: CDF of broadcast length.

 1

 0.8

 0.6

 0.4

 0.2

 0

Meerkat Heart
Meerkat Comment
Periscope Comment
Periscope Heart

01

10

100

1k

10k

100k

1M

Number of Comments/Hearts per Broadcast

 1

 0.8

 0.6

 0.4

 0.2

s
r
e
s
U

 
f

 

o
F
D
C

 0

0 1

Meerkat Create
Periscope Create
Meerkat View
Periscope View

10

10k
Number of Broadcasts Viewed/Created

100

1k

Figure 4: Total # of viewers per broad-
cast.

Figure 5: Total # of comments (hearts)
per broadcast.

Figure 6: Distribution of broadcasts
views and creation over users.

casts, generated by 1,847,009 unique broadcasters. Alto-
gether, these broadcasts had a total number of 705,751,096
viewers, including both registered users on mobile apps and
anonymous viewers on web browsers. Of these, more than
482 million views were generated by 7,649,303 registered
mobile users, and the rest were from anonymous views on
the Periscope webpage.

Our crawler was interrupted brieﬂy between August 7–9,
2015 due to a communication bug with the Periscope server.
As a result, our dataset is missing roughly 4.5% of the broad-
casts during this period. We believe this missing data is
small enough not to aﬀect our data analysis or conclusions.
Finally, since Periscope assigns userID sequentially5, we can
identify the total number of registered users. As of August
20, 2015 (the last day of our measurement), Periscope had
12 million registered users. For each user, we crawled her
follower and followee lists to analyze the structure of the so-
cial graph.
Data Collection on Meerkat. We use the same method-
ology to collect broadcast data from Meerkat between May
12 and June 15, 2015. After one month of data collection,
Meerkat management informed us our measurements were
introducing noticeable load on their servers, and we termi-
nated our measurements. As shown in Table 1, the Meerkat
dataset includes a complete list of 164,335 broadcasts, in-
volving 189,075 unique users (57K broadcasters, 183K view-
ers). During our measurement period, these broadcasts col-
lected a total of more than 3.8 million views, of which 3.1
million views were generated by 183K viewers. Since Meerkat
does not assign userID sequentially, and the follower/followee
graph was not fully connected at the time of our measure-

5At launch, Periscope assigned userIDs as sequential num-
bers. In September 2015, they switched to a 13-character
hash string.

ments, we were unable to reliably estimate the total number
of registered users.

3.2 Analysis: Broadcasts & Users

Using the collected datasets, we now explore application-
level characteristics of Periscope and Meerkat. We focus on
three aspects: 1) scale and growth trends in terms of num-
ber of users and broadcasts; 2) duration and intensity of user
interactions during broadcasts; and 3) Periscope’s social net-
work and its impact on broadcast popularity.
Figure 1 plots the number of daily
Scale & Growth.
broadcasts over time on both networks. For Periscope, the
daily broadcast volume grew rapidly over time (more than
300% over three months). This rapid growth is consistent
with prior observations of initial network growth [50]. The
biggest leap occurred shortly after May 26, when Periscope
launched their Android app. The broadcasts show a clear
weekly pattern, where peaks in daily broadcasts usually co-
incide with the weekend. Broadcasts drop to their lowest
weekly level on Mondays, then slowly grow during the week.
In contrast, the volume of Meerkat broadcasts is orders of
magnitude lower, weekly patterns are harder to identify, and
daily broadcast volume nearly dropped by half over a single
month to below 4000. It is clear that closing oﬀ the Twitter
social graph had the intended eﬀect on Meerkat.

Similarly, Periscope’s daily number of active users (both

viewers and broadcasters) also grows rapidly (Figure 2). View-
ers jumped from 200K in May to more than a million by Au-
gust. The ratio between the active viewers and broadcasters
is roughly 10:1. In comparison, Meerkat daily viewers ﬂuc-
tuates, but averages around 20K. Meerkat broadcasters show
a clear descending trend over our measurement period, even-
tually dropping to less than 3K (roughly 40 times smaller
than Periscope).

488Viewer
Follower Viewer

105

104

103

102

101

t
s
a
c
d
a
o
r
B

 
f

o

 
s
r
e
w
e
V

i

 
f

o

 

#

1

101

102

103

104

105

106

# of Followers of Broadcaster

Figure 7: Broadcaster’s followers vs. # of viewers
(Periscope).

For both net-
Broadcast Duration & User Interaction.
works, the majority of broadcasts are short: 85% of broad-
casts last <10 minutes (Figure 3). Periscope broadcasts are
more even in length, whereas Meerkat streams are more skewed
by a smaller number of longer broadcasts. In terms of view-
ers, the two systems are quite diﬀerent. For Meerkat, most
(60%) broadcasts have no viewers at all. In contrast, nearly
all Periscope broadcasts received at least one viewer, with
the most popular broadcasts attracting as many as 100K view-
ers. Anecdotal evidence shows that recent Periscope ses-
sions have grown much bigger: a single Periscope of a large
rain puddle collected hundreds of thousands of viewers, and
had more than 20,000 simultaneous viewers at its peak [14].
Within both Periscope and Meerkat broadcasts, viewers
interact with the broadcast by posting real-time comments
and hearts. As shown in Figure 5, while not all broadcasts
received responses, a small portion of popular broadcasts are
highly interactive. As expected, Periscope broadcasts gen-
erate much more user interactions. About 10% Periscope
broadcasts received more than 100 real-time comments and
more than 1000 hearts. The most popular broadcast attracted
1.35 million hearts. Recall that the number of total viewers
per broadcast who can comment is limited by Periscope to
roughly 100 (presumably for scalability reasons). This puts
a strong cap on the total number of comments.

We wanted to take a closer look at the distribution of ac-
tivities over each network’s user population. In Figure 6, we
plot the CDF distribution of broadcast views and broadcast
creation activities over users. Here the trends are roughly
consistent across Periscope and Meerkat: user activity is
highly skewed, and small groups of users are much more
active than average in terms of both broadcast creation and
viewership. For Periscope, viewers are especially skewed,
with the most active 15% of users watching 10x more broad-
casts than the median user.
We found that the social network in
Social Network.
Periscope has a clear impact on the broadcast viewership.
Figure 7 shows the correlation between a broadcaster’s num-
ber of followers and number of viewer per broadcast. It’s
clear that users with more followers are more likely to gen-
erate highly popular broadcasts. This is because when a user
starts a new broadcast, all her followers will receive notiﬁ-
cations on their mobile app. On Periscope, celebrities like
Ellen DeGeneres already have over one million followers,
thus creating built-in audiences for their broadcasts. As we

observed in Table 2, Periscope’s follow graph is similar in
graph structure to social graphs in Twitter and Facebook.
Its follow graph exhibits negative assortativity similar to the
Twitter graph, which likely comes from the prevalence of
asymmetric, one-to-many follow relationships.
Our measurements show that Periscope is ex-
Summary.
periencing astounding growth in users and broadcast streams.
Growth in broadcasts is particularly noteworthy, as it is the
main contributor to issues of scalability in video delivery.
On March 28, 2016, Periscope hit a milestone of 200 mil-
lion total broadcasts [35]. And a quick check at the time of
this submission (May 8, 2016) shows an average of roughly
472K daily broadcasts, and the numbers are still growing
steadily.

At this rate, Periscope will face even bigger scalability
challenges in the near future. We also observe that broad-
casts can attract a large number of viewers (up to 100K).
These broadcasts are also highly interactive, with frequent,
real-time hearts from viewers to broadcasters. Periscope
comments, in contrast, are severely constrained by their limit
of 100 active users. Given the audience size of some broad-
casts, it is clear that more inclusive support for group inter-
action is necessary in these broadcasts.

4. UNDERSTANDING END-TO-END DE-

LAY

Our initial measurements have demonstrated the scale and
real-time features of personalized livestream services. As
they continue to grow, a natural question arises:

“Can personalized livestreams scale to millions of streams,
and what, if any, technical challenges limit their scalabil-
ity?”
To answer this question, we must ﬁrst study how Periscope
delivers its streams over its content distribution network (CDN),
which is responsible for delivering real-time video streams
to viewers and scaling up to a massive number of simulta-
neous live streams. In particular, we seek to understand the
key design choices that Periscope has made to support both
scalability and real-timeness, and the resulting system per-
formance.

Our analysis of Periscope broadcast delivery includes four

elements.

• We explore Periscope’s CDN infrastructure to support mes-
sage delivery and live video streaming services. We reverse-
engineer their streaming protocols and CDN locations, and
compare with Meerkat and Facebook Live (§4).

• We perform detailed measurements on Periscope CDN by
breaking-down the video transmission delay to each step of
the transmission process (§4.2).

• Based on collected data, we perform detailed analysis on
streaming delay and explore key design trade-oﬀs between
delay and scalability (§5).

• We analyze the tradeoﬀ between scalability and latency in
the context of the three largest contributors to delay, in-
cluding chunking and polling (§5.2), geolocation factors in
CDN servers (§5.3), and client-side buﬀering strategies (§6).

489Broadercaster

RTMP

Broadercaster

Broadercaster

Periscope 

Server

Wowza

Fastly

Video

Message

RTMP

HLS

HTTPS

PubNub

HTTPS

Viewers

Viewers

Viewers

Viewers

(Commenter)

(Non-commenter)

(a) Control Channel

(b) Video Channel

(c) Message Channel

Fastly
Wowza

Figure 8: Periscope CDN infrastructure.

Figure 9: Wowza and Fastly server locations.

4.1 Video Streaming CDN and Protocols

We start by describing Periscope’s CDN infrastructure.
By analyzing the network traﬃc between Periscope app and
server, we found that Periscopes uses two independent chan-
nels to deliver live video content and messages (comments/hearts)
and the Periscope server only acts as a control panel (see Fig-
ure 8). Speciﬁcally, when a user starts a broadcast, Periscope
redirects the user to a video server to upload live video, and
a message server to receive real-time comments and hearts.
Viewers receive video frames and messages and combine
them on the client side based on timestamps. For messaging,
Periscope uses a third-party service called PubNub and users
connect to the PubNub via HTTPS. For video, Periscope
leverages collaboration among multiple CDNs.

In the following, we focus on Periscope video CDN and

describe our reverse-engineering eﬀorts to understand its stream-
ing protocols and server deployment.
As shown in Figure 8(b), Periscope
Streaming Protocols.
uses two CDNs, Wowza [8] and Fastly [4], to handle video
uploading (from broadcasters) and downloading (to view-
ers). Wowza uses the RTMP protocol, and Fastly uses the
HLS protocol, which are two fundamentally diﬀerent video
streaming protocols. In RTMP, the client maintains a per-
sistent TCP connection with the server. Whenever a video
frame (≈ 40ms in length) is available, the server “pushes”
it to the client. In HLS, Wowza servers assemble multiple
video frames into a chunk (≈ 3s in length), and create a
chunk list. Over time, Fastly servers poll Wowza servers to
obtain the chunk list. Each HLS viewer periodically “polls”
the chunk list from the (Fastly) server and downloads new
chucks.

When creating a new Periscope broadcast, the broadcaster
connects to a Wowza server to upload live video. Wowza
maintains a persistent RTMP connection with the broadcaster’s
device during the entire broadcast. Wowza also transmits the
live video content to Fastly, and thus both Wowza and Fastly
distribute the video content to end-viewers. The ﬁrst batch
of viewers to join a broadcast directly connect to Wowza to
receive video using RTMP. Our tests estimate the number to
be around 100 viewers per broadcast. Once the threshold is
reached, additional viewers automatically connect to Fastly
to download video using HLS. Recall that Periscope’s de-
fault policy allows only the ﬁrst 100 viewers to post com-
ments in each broadcast [30]. Those commenters are ef-

fectively the ﬁrst batch of viewers who connect directly to
Wowza, and receive their streams via low delay from RTMP.
We believe that Periscope adopts this “hybrid” approach
to improve scalability. Due to its use of persistent TCP con-
nections, RTMP oﬀers low-latency streaming when the num-
ber of viewers is small (<100), while HLS provides scalable
streaming (with higher delay) to a large number of users.
Among the complete set of periscope broadcasts (19.6M)
over the three months, 1.13M broadcasts (5.77%) had at least
one 1 HLS viewer, and 435K had at least 100 HLS viewers.
Both Wowza and Fastly have dis-
Server Distribution.
tributed data centers around the globe. Fastly provides the IP
range and location of each data center on its website [5, 1].
At the time of our measurements, Periscope uses all 23 data
centers of Fastly covering North America, Europe, Asia, and
Oceania 6, as shown in Figure 9. Wowza does not oﬀer any
public information on its data centers thus we obtained both
IPs and locations using a simple experiment. We used 273
PlanetLab nodes (from 6 continents) to join all the broad-
casts as viewers on June 11, 2015. By analyzing the resolved
IPs from diﬀerent PlanetLab nodes, we found that Wowza
runs on 8 Amazon EC2 datacenters, as shown in Figure 9.
Interestingly, for 6 out of 8 Wowza datacenters, there is a
Fastly datacenter co-located in the same city, and 7 out of 8
are co-located in the same continent. The only exception is
South America where Fastly has no site. Later in §5.3 we
will present our measurement study that explores the impact
of data center location on Periscope CDN performance.
We also study the stream-
Meerkat & Facebook Live.
ing protocols in Meerkat and Facebook Live by analyzing
their app-to-server traﬃc. In Meerkat, each broadcaster uses
a single HTTP POST connection to continuously upload live
video to Meerkat server (hosted by Amazon EC2), while
viewers download video chucks from the server using HLS.
For Facebook live, the protocol setting is similar to that of
Periscope. Each broadcaster uses RTMPS (RTMP over TLS/SSL)
to upload live video via an encrypted connection, and view-
ers download video from Facebook’s own CDNs using RTMPS
or HLS. Later in §7, we will discuss the implications of us-
ing RTMPS to system security and performance.

6Fastly deployed 3 new data centers (Perth, Wellington and
Sao Paolo) in December 2015, which are not covered by our
measurement.

4904.2 Breakdown of Broadcast Delays

Figure 10 illustrates the breakdown of Periscope’s end-
to-end delay, which is diﬀerent for viewers who download
video via Wowza (RTMP) and those who download from
Fastly (HLS).
Wowza (RTMP).
video frame) includes 3 parts:

In this case the broadcast delay (per

• Upload delay is for transferring the video frame from

the broadcaster to the Wowza server ( 2 - 1 ).

• Last-mile delay is for downloading the video frame

from Wowza to the viewer’s device ( 3 - 2 ).

• Client-buﬀering delay is the gap between the time
that the video frame arrived at the viewer device to the
time it got played ( 4 - 3 ).

Here the process is more complicated.
Fastly (HLS).
Wowza ﬁrst assembles each video frame into chunks. When
a new chunk is available ( 7 ), Wowza updates the chun-
klist and notiﬁes Fastly to expire its old chunklist ( 8 ). As
viewers poll the Fastly chunklist periodically, the ﬁrst viewer
polling ( 9 ) after the chunklist expiration triggers Fastly to
poll Wowza ( 10 ) and copy the fresh chunk from Wowza
( 11 ) that serves future polling requests. With this process
in mind, we divide each HLS viewer’s broadcast delay (per
video chuck) into 6 parts (Figure 10(b)):

• Upload delay is for the broadcaster uploading a video

frame to Wowza ( 6 - 5 ).

• Chunking delay is the latency involved to form a chunk.

It is equal to the chunk duration ( 7 - 6 ).

• Wowza2Fastly delay is the time gap between a fresh
chunk is ready till the chunk arrives at the Fastly server
( 11 - 7 ). Since transferring a chunk from Wowza to
Fastly is triggered by the ﬁrst HLS viewer polling ( 9 ),
this delay value also includes the contribution of the
ﬁrst viewer’s polling delay.

• Viewer polling delay: Delay between Fastly getting a
local cache and viewers polling cached chunklist from
Fastly ( 14 - 11 ).

• Last-mile delay is between Fastly and the viewer’s

phone ( 12 - 11 , 15 - 14 ).

• Client-buﬀering delay is the gap between the time of
receiving the chunk and the time it got played ( 16 -
12 and 17 - 15 ).

4.3 Experimental Methodology

To measure each delay component, we conduct both ﬁne-
grained passive measurements by crawling a large number of
real-world Periscope broadcasts, and controlled experiments
by monitoring Periscope broadcasts initiated by ourselves.
First, by recording detailed, per-frame (or chunk) events,
the passive measurements allow us to accurately characterize
the delay between broadcasters and Periscope CDN servers,
covering the key processes that Periscope can control via
its CDN design. Second, we use the controlled experiments
to estimate the delay associated with the processes between

viewers and Periscope CDN servers and those on viewer de-
vices. These include the “last-mile” delay which depends
heavily on the viewer’s local network condition, and the la-
tency of “client-buﬀering” and “polling” which depend heav-
ily on the client app conﬁguration.
Using our whitelisted
Passive Broadcast Crawling.
servers, we randomly selected 16,013 broadcasts from all
broadcasts between November 29, 2015 and February 5th,
2016. We carefully engineered our crawlers to capture ﬁne-
grained events within each broadcast. First, since Periscope
provides both RTMP and HLS URLs to RTMP viewers, we
designed our RTMP crawler to join immediately when a broad-
cast starts and obtained both URLs. Second, for each broad-
cast, our HLS crawler acted as an HLS viewer but polled
Fastly at a very high frequency, immediately triggering chunk
transfers between Wowza and Fastly. This way, our crawlers
captured both RTMP and HLS delay performance for each
broadcast regardless of its actual viewer count. Finally, since
Wowza and Fastly have multiple datacenters, we deployed
our crawlers on 8 nearby or co-located Amazon EC2 sites to
collect data from all these datacenters.

More speciﬁcally, for each RTMP channel, our crawler
established an RTMP connection with Wowza to download
video streams. To accurately estimate 2 , we set the stream
buﬀer to 0 so that every video frame is pushed to our crawler
as soon as it becomes available on Wowza. For each Wowza
datacenter, we setup a dedicated crawler co-located at the
same EC-2 site to minimize the delay. In addition, we ob-
tained 1 from the meta data of each video keyframe. This
timestamp is recorded by the broadcaster’s device and em-
bedded into the keyframe. It may not always be a universal
timestamp. Note that this error does not aﬀect the delay mea-
surement of Periscope’s CDN, since all timestamps at server
side are correctly synced.

For each HLS channel, our crawler constantly pulled the
chunklist from Fastly. To accurately estimate Wowza2Fastly
delay ( 11 - 7 ), the crawler used a very high polling fre-
quency (once every 0.1 second) to minimize 9 - 7 , and
recorded the time when a chunk ﬁrst becomes available at
Fastly ( 11 ). Also, for each Fastly datacenter, we had a ded-
icated crawler at the nearest EC2 site. Similar to RTMP, we
obtained 5 from the meta data in the video chuck. Note that
the timestamp 6 was already obtained by the RTMP crawler
(equivalent to 2 ).
We performed
Controlled Broadcast Experiments.
controlled experiments to estimate the rest of the delay com-
ponents. We conﬁgured one smartphone as a broadcaster
and the other two as a RTMP and a HLS viewer7, respec-
tively. All three phones were connected to the Internet via
stable WiFi connections. During each broadcast, we col-
lected the network traﬃc on both viewers’ devices to extract
two key timestamps 3 , 13 , and 15 . At the same time, we

7We forced one smartphone to become a HLS viewer
by intercepting and modifying its message exchange with
Periscope, which contains both RTMP and HLS urls. We
deleted the RTMP url manually, forcing the smartphone to
connect to the HLS server.

491d
a
o
p
U

l

(cid:7555)

Broadcaster

Wowza Server

Wowza Viewer

(cid:7556)

t

(cid:7557) (cid:7558)

e

l
i

M

 
t
s
a
L

g
n
i
r
e
ff
u
B

d
a
o
p
U

l

(cid:7559)

(cid:7560)

y
l
t
s
a
F
2
a
z
w
o
W

i

g
n
k
n
u
h
C

Chunking

(cid:7561)

(cid:7564)

(cid:7562)

(cid:7563)

e

l
i

M

 
t
s
a
L

(cid:7565)

(cid:7566)

Broadcaster

Wowza Server

Fastly Server

Fastly Viewer (1st)

Fastly Viewer (other)

g
n
i
r
e
ff
u
B

(cid:7568)

t

(cid:7570)

(cid:7567)

(cid:7569)

(cid:7571)

g
n

i
l
l

o
P

e

l
i

M

 
t
s
a
L

g
n
i
r
e
ff
u
B

(a) Wowza (RTMP)

(b) Fastly (HLS)

Figure 10: RTMP/HLS end-to-end delay breakdown diagram.

used our crawlers to record the set of timestamps from the
CDN ( 1 , 2 , and 5 , 6 , 7 , 11 ). These measurements al-
lowed us to estimate the last-mile delay for RTMP and HLS,
and the polling delay for HLS.

Estimating the “buﬀering” delay requires knowledge of
4 and 17 , the timestamp of video playing. This cannot be
extracted from network traﬃc.
Instead, we estimated this
value by subtracting other delay components from the end-
to-end delay. To record the end-to-end delay, we used the
broadcaster’s phone camera to videotape a running clock,
and manually recorded the end-to-end delay based on the
diﬀerence of displayed time on broadcaster’s phone screen
and the viewers’. The error is less than 1s. We also repeated
each experiment 10 times and take the average delay to fur-
ther reduce the measurement error.

5. DELAY ANALYSIS

In this section, we analyze the delay measurements of
Periscope broadcasts to identify key contributors to the end-
to-end video streaming delay. By comparing the perfor-
mance of RTMP (which targets low latency delivery) and
HLS (which oﬀers higher scalability), we seek to understand
the tradeoﬀs between latency and scalability.

5.1 Key Delay Contributors

Figure 11 provides a breakdown of the end-to-end broad-
cast delay captured by our controlled experiments. The RTMP
delay is for each video frame, and the HLS delay represents
the chunk-level delay (i.e. the frame-level delay of the ﬁrst
frame in each chunk). The delay value of each component
is averaged across 10 repetitions. As expected, RTMP ses-
sions experienced signiﬁcantly less latency (1.4s) compared
to HLS sessions (11.7s).

The key contributors to HLS’ higher latency are client-

buﬀering (6.9s), chunking (3s), polling (1.2s) and Wowza2Fastly
(0.3s). Chunking and polling delays are direct results of the
protocol diﬀerence between RTMP and HLS; Wowza2Fastly
is caused by the handshaking and data transfer between Wowza
and Fastly servers, a speciﬁc CDN design choice by Periscope.
Finally, HLS’ large buﬀering delay comes from the speciﬁc
buﬀer conﬁguration in the HLS viewer client, which is much

more “aggressive” than that of RTMP8 and will be used to
mitigate heavy delay jitters.

Based on these observations, next we will study the chunk-
ing and polling delay in more detail, and explore how CDN
geolocation aﬀects Wowza2Fastly (and other delay compo-
nents). Later in §6 we will explore approaches to reduce the
buﬀering delay.

5.2 Comparing RTMP and HLS Latency
Targeting two diﬀerent goals (supporting scalability vs.
minimizing latency), RTMP and HLS protocols diﬀer sig-
niﬁcantly in their design. The key design diﬀerences are 1)
the operating data unit (frame vs. chunk) and 2) the com-
munication model (push vs. poll). Next, we investigate how
these choices aﬀect latency and scalability.
RTMP operates on individual video
Frame vs. Chunk.
frames while HLS operates on chunks (a group of video
frames). The chunk size (in terms of the total video dura-
tion) varies across broadcast sessions, but translates directly
into the chunking delay. We extracted the chunk size distri-
bution from our passive crawling of 16,013 broadcasts, and
found that mass majority (>85.9%) of HLS broadcasts used
3s chunks (or 75 video frames of 40ms in length).

We believe that Periscope’s choice of chunk size already
reﬂects the tradeoﬀ between scalability and latency. Us-
ing smaller chunks obviously reduces the chunking delay
but also increases the number of chunks. This translates
into higher server overhead for managing data and handling
client polling (i.e. viewers will send more requests to servers).
Thus to support a large number of users, HLS must conﬁgure
its chunk size with care. As a reference, today’s livestream-
ing services all use ≈3s chunks (3s for Periscope and Face-
book live, 3.6s for Meerkat), while Apple’s video-on-demand
(VoD) HLS operates on 10s chunks.
Another key diﬀerence between RTMP
Push vs. Poll.
and HLS is that when stream videos to viewers, HLS uses
poll-based operations while RTMP is push-based. That is,
HLS viewers must poll Fastly servers periodically to dis-

8Our control experiments show that the HLS client uses a
roughly 9s pre-fetching buﬀer size while RTMP uses 1s.

492Buffering Delay
Polling Delay
Upload Delay

Chunking Delay
Wowza2Fastly Delay
Last Mile Delay

HLS

RTMP

 0

 3

 6

Delay (s)

 9

 12

s
t
s
a
c
d
a
o
r
B

 
f

 

o
F
D
C

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

 1

 0.8

 0.6

 0.4

 0.2

s
t
s
a
c
d
a
o
r
B

 
f

 

o
F
D
C

 0

 0

 0.5

 1

2s
3s
4s

 1.5

 2

2s
3s
4s

 0.5

 1

 1.5

 2

 2.5

 3

Average Polling Delay per Broadcast (s)

Variance of Polling Delay per Broadcast (s)

Figure 11: HLS/RTMP end-to-end de-
lay breakdown.

Figure 12: CDF of average polling de-
lay with diﬀerent polling intervals.

Figure 13: CDF of polling delay vari-
ance with diﬀerent polling intervals.

cover new chunks. Furthermore, after a fresh chunk is avail-
able on Wowza, it is only transferred to Fastly when an HLS
viewer polls Fastly. These extra overheads lead to a consid-
erable amount of polling delay for HLS viewers.

To study the polling delay in detail, we use our passive
measurements to perform trace-driven simulations9. Since
our crawler polls the Fastly server at a very high frequency
of once every 0.1s, it can immediately identify the arrival of
new chunks on Fastly servers. Using the collected chunk ar-
rival timestamps, we simulate periodic polling for each sin-
gle HLS viewer of the broadcast10, and study both the aver-
age and standard deviation of the polling delay within each
broadcast. The results are shown in Figure 12 and 13.

We make two key observations. First, the average polling
delay is large (1-2s) and increases with the polling interval.
Using 2s and 4s polling intervals, the average delay is half of
the polling interval. Yet using 3s interval, the average delay
varies largely between 1s and 2s. This is because the chunk
inter-arrival time varies slightly around 3s, thus polling at the
same frequency creates large delay variations. Second, the
STD results in Figure 13 show that the polling delay varies
largely within each broadcast as viewers are unable to pre-
dict the chunk arrival time. Such high variance translates
into delay jitters at the viewer device which are then handled
by the client-buﬀering.

The choice of polling interval also reﬂects the tradeoﬀ be-
tween scalability and latency. Smaller polling interval re-
duces average polling delay and variance, but comes at the
cost of generating more polling requests to the CDN servers.
Our controlled experiments revealed that in Periscope, the
polling interval varies between 2s and 2.8s.
Scalability. We also compare the cost of supporting scal-
ability when running RTMP and HLS. We set up a Wowza
Stream Engine11 on a laptop, serving as a CDN node. The
laptop has 8GB memory, 2.4GHz Intel Core i7 CPU, and

9As discussed earlier, our crawl of real-world broadcasts
captures the delay from broadcasters to CDNs but not those
from CDNs to viewers.
10Here we assume that the per-viewer polling delay does not
depend on the number of viewers. That is, the CDN servers
have suﬃcient resources to handle all polling requests. We
also assume the communication delay between the viewer
and CDN is negligible compared to the polling delay.
11https://www.wowza.com/products/streaming-engine

1Gbps bandwidth. We start RTMP and HLS viewers (sepa-
rately) on other machines connecting to the laptop via Ether-
net, and measure the laptop’s resource usage including CPU
and memory as we vary the number of viewers. Our re-
sults show that RTMP and HLS result in similar and sta-
ble memory consumption but very diﬀerent CPU usage with
respect to diﬀerent numbers of viewers. Speciﬁcally, Fig-
ure 14 shows that supporting RTMP users requires much
higher CPU power than that for HLS. The gap between the
two elevates with the number of viewers, demonstrating the
higher cost of supporting RTMP scalability. The key rea-
son for RTMP’s higher CPU usage is that it operates on
small frames instead of large chunks, leading to signiﬁcantly
higher processing overhead.

5.3

Impact of CDN Geolocation

We now examine the impact of CDN geolocation on Periscope

performance. By studying our detailed broadcast measure-
ments, we seek to understand how Wowza and Fastly con-
nect Periscope users to their data centers and how such as-
signment aﬀects end-to-end delay. Furthermore, using our
knowledge on the Wowza and Fastly data center locations,
we examine the chunk transfer delay between their data cen-
ters (i.e. the Wowza2Fastly delay).
We discov-
Periscope’s Geolocation Optimization.
ered three types of geolocation optimization that Periscope
uses to improve scalability and latency. First, Periscope
connects each broadcaster to the nearest Wowza data cen-
ter12, which will eﬀectively reduce the upload delay. Sec-
ond, Fastly fetches video chunks from Wowza and copies
them to multiple Fastly data centers, which helps to provide
scalability. Finally, each HLS viewer connects to the near-
est Fastly data center using IP anycast, thus minimizing the
last mile delay. In contrast, RTMP viewers always connect
to the same Wowza data center that the broadcaster connects
to, avoiding data transfer among Wowza data centers.
The CDN geoloca-
Wowza to Fastly Transmission.
tion also directly aﬀects the Wowza-to-Fastly delay, i.e. the
latency for Wowza to distribute video chunks to diﬀerent
Fastly data centers. We estimate this latency from our crawled

12More than half of our crawled broadcasts contain the GPS
coordinates of the broadcaster. Thus we directly compare
each broadcaster’s location to her associated Wowza data
center location.

493)

%

(
 

e
g
a
s
U
U
P
C

 

 100

 80

 60

 40

 20

 0

RTMP
HLS

100

200

300

400

500

# of Viewers

s
t
s
a
c
d
a
o
r
B

 
f
o
 
F
D
C

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

Co-located (0km)
(0, 500km]
(500, 5,000km]
(5,000, 10,000km]
>10,000km

 0.5

 1

 1.5

 2

Wowza2Fastly Delay (s)

Figure 14: CPU usage of server using
RTMP and HLS with diﬀerent num-
ber of viewers.

Figure 15: Wowza-to-Fastly Delay. We
group datacenter pairs based on their
geographic distance. Co-located pairs
are located at the same city.

broadcast traces13. Figure 15 plots the CDF of the Wowza2Fastly
delay across broadcasts, where we group Wowza and Fastly
data center pairs based on their geographical distance (dis-
tance between the cities). Here co-located means that the
two data centers are in the same city. As expected, pairs
with longer distances experience larger delay.

Figure 15 also shows a clear distinction between co-located
pairs and all other pairs. In particular, there is a signiﬁcant
gap (>0.25s) between the latency of co-located pairs and that
of nearby city pairs (<500km), e.g., San Francisco to Los
Angeles. We suspect that this is because each Wowza server
ﬁrst transfers video chunks to the co-located Fastly data cen-
ter, who then behaves as a gateway and distributes the data to
other Fastly data centers. The extra delay is likely the result
of coordination between this gateway and other data centers.

6. OPTIMIZING CLIENT-SIDE BUFFER-

ING

Our delay analysis has shown that client-side buﬀering in-
troduces signiﬁcant delay to both RTMP and HLS viewers.
The resulting video playback delay depends on the broad-
cast channel quality, buﬀering strategies and conﬁgurations
at the mobile app. While buﬀering is necessary to ensure
smooth video replay, setting reasonable parameters for the
buﬀer can dramatically impact streaming delay. In this sec-
tion, we perform trace-driven experiments to understand if
(and by how much) Periscope can reduce this buﬀering de-
lay, and the corresponding impact on end-user video quality.
Buﬀering Strategy. We start by describing the buﬀering
strategy. For HLS, we were able to decompile Periscope’s
Android source code and analyze its media player settings.
We found that Periscope users a naive buﬀering strategy.
Speciﬁcally, when the live streaming starts, the client (viewer)
ﬁrst pre-buﬀers some video content (P seconds of video) in

13Our HLS crawler (as a viewer) polls each Fastly server at
a very high frequency of once per 0.1s, and thus can record
the earliest time that a chunklist is updated at each Fastly
server. Furthermore, operating at a polling frequency 20+
times faster than normal HLS viewers, our crawler will ini-
tiate the ﬁrst poll on the Fastly server that triggers the server
to poll Wowza and obtain a fresh chunklist. Thus we are
able to estimate the Wowza2Fastly delay 11 - 9 (Figure 10)
by measuring 11 - 7 and minimizing 9 - 7 .

its buﬀer. During live streaming playback, the client inserts
newly arrived video content into the buﬀer, which are orga-
nized and played by their sequence numbers to mitigate de-
lay jitter and out-of-order delivery. Arrivals that come later
than their scheduled play time are discarded. This smoothes
out video playback but also introduces extra latency. We
found from the source code that Periscope conﬁgures a suﬃ-
ciently large memory to buﬀer video content and avoid drop-
ping packets.

We were unable to identify the exact buﬀering setting for
RTMP, because Periscope implements a customized RTMP
protocol stack, and the source code is obfuscated. But results
from our controlled experiments suggest that Periscope uses
the same buﬀering strategy for RTMP and HLS.

Next, we implement the above buﬀering strategy in trace-
driven simulations for both RTMP and HLS clients. Our
goal is to understand how Periscope’s buﬀering parameters
impact playback delay, and whether it can be optimized for
better performance. For simplicity, we do not put any hard
constraint on the physical buﬀer size, but vary the pre-buﬀer
size (P) to study its impact.
We perform trace-driven simulations us-
Experiments.
ing our measurements on our 16,013 real-world broadcasts.
For each broadcast, we extract from our measurements a se-
quence of video frame/chunk arrival times (at the Wowza/Fastly
server). This covers the path from the broadcaster to the
CDN. We then simulate the path from the CDN to the view-
ers. For RTMP viewers, we implement client-side buﬀering,
and for HLS viewers, we implement polling (at an interval
of 2.8s) and buﬀering. While we cannot capture possible de-
lay variances from last mile links, we assume it is under one
second in our simulations.

We evaluate the viewer experience via two metrics. The
ﬁrst is the video play smoothness, represented by the stalling
ratio, i.e. the duration of stalling moment (no video to play)
divided by the duration of the broadcast. The second metric
is the buﬀering delay, the time gap between a video frame/chunk
arriving at the viewer till the time it got played. For each
broadcast, we compute the average buﬀering delay across
all the frames/chunks.

Figure 16(a)-(b) plot the stalling ratio and average buﬀer-
ing delay for RTMP users by varying P from 0s to 1s. As
expected, pre-buﬀering more video content (i.e.
larger P)

494s
t
s
a
c
d
a
o
r
B

 
f

 

o
F
D
C

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

1s
0.5s
0s

 0.02

 0.04

 0.06

 0.08

 0.1

RTMP Stalling Ratio

(a) Stalling Time Ratio

s
t
s
a
c
d
a
o
r
B

 
f

 

o
F
D
C

 1

 0.8

 0.6

 0.4

 0.2

 0

0s
0.5s
1s

 0

 1

 2

 3

 4

 5

 6

 7

 8

 9  10

RTMP Buffering Delay (s)
(b) Buﬀering Delay

Figure 16: RTMP: the impact of diﬀerent buﬀer size (for pre-download) to
buﬀering delay and stalling.

s
t
s
a
c
d
a
o
r
B

 
f

 

o
F
D
C

 1

 0.8

 0.6

 0.4

 0.2

 0

 0

9s
6s
3s
0s

 0.1
 0.2
HLS Stalling Ratio

(a) Stalling Time Ratio

 1

 0.8

 0.6

 0.4

 0.2

s
t
s
a
c
d
a
o
r
B

 
f

 

o
F
D
C

 0.3

 0

 0

0s
3s
6s
9s

 2

 4

 6

 8

 10

HLS Buffering Delay (s)
(b) Buﬀering Delay

Figure 17: HLS: the impact of diﬀerent buﬀer sizes (for pre-download) to buﬀer-
ing delay and video stalling.

increases the smoothness of video playing but also incurs
(slightly) higher buﬀering delay. Here the beneﬁt of pre-
buﬀering is not signiﬁcant because RTMP streaming is al-
ready smooth. We also observe that in Figure 16(b), a small
portion (10%) of broadcasts experience long buﬀering delay
(>5s). This is caused by the bursty arrival of video frames
during uploading from the broadcaster.

HLS viewers, on the other hand, experience much more
stalling due to high variance in polling latency, and the sys-
tem needs to pre-buﬀer much more video content (6-9s) to
smooth out playback (Figure 17(a)). From analyzing our
controlled experiment traces, we ﬁnd that Periscope conﬁg-
ures P=9s for HLS viewers. However, Figure 17 (a)-(b)
show that a much less conservative value of P=6s already
provides similar results on stalling, but reduces buﬀering
delay by 50% (3s). These results suggest that Periscope’s
buﬀering conﬁguration is conservative, and can be optimized
today to signiﬁcantly reduce buﬀering delay. Over time,
we expect that buﬀering will increase as higher volume of
broadcasts force servers to increase chunk size and decrease
polling frequency.

Although we do not consider last-mile delay in our ex-
periments, our result stands as long as the last mile delay
is stable. In cases when viewers have stable last-mile con-
nection, e.g., good WiFi/LTE, smaller buﬀer size could be
applied to reduce the buﬀering delay. In other cases of bad
connection, Periscope could always fall back to the default
9s buﬀer to provide smooth playback.

7. SECURING LIVESTREAM BROAD-

CASTS

During the course of our investigation into livestream-
ing services, we discovered that Periscope and Meerkat both

shared a common security vulnerability. Neither service au-
thenticated video streams after the initial connection setup,
and it is not diﬃcult for attackers to silently “alter” part or
all of an ongoing broadcast for their own purposes. More
speciﬁcally, the attacker can overwrite selected portions of
an ongoing broadcast (video or audio or both), by tapping
into the transmission at the source (in the broadcaster’s net-
work) or at the last-mile network of one or more viewers. In
this section, we describe this vulnerability in the Periscope
protocol, the results of our proof-of-concept experiments con-
ducted on our own streams, and simple countermeasures to
defend against this attack.
Our primary concern was to the security of
Ethics.
livestream broadcasters on these services. With this attack,
any video stream could be altered in undetectable ways. For
example, a broadcast coming from the White House can be
altered in an unsecured edge wireless network in a foreign
country, so that a selected group of users would see an al-
tered version of the stream while any alterations remained
invisible to the broadcaster.

Validating the vulnerability required that we perform ex-
periments to alter a Periscope stream at both the broadcaster’s
local network and the viewer’s network. We took all possi-
ble precautions to ensure our tests did not aﬀect any other
users. First, we conducted the proof-of-concept experiments
on broadcasts created by ourselves, set up so that they were
only accessible to our own viewer. All involved parties (at-
tacker and victim) were our own Periscope accounts. Sec-
ond, our Periscope accounts have no followers and thus our
experiments did not push notiﬁcations to other users. Third,
we made small changes that should have zero impact on any
server side operations. Finally, once we conﬁrmed the vul-
nerability, we notiﬁed both Periscope and Meerkat about this

495vulnerability and our proposed countermeasure (directly via
phone to their respective CEOs). We also promised any dis-
closures of the attack would be delayed for months to ensure
they had suﬃcient time to implement and deploy a ﬁx.

7.1 Validating the Attack

The broadcast tampering attack is possible because Periscope

uses unencrypted and unauthenticated connections for video
transmission. As shown in Figure 8(a), when a user starts a
broadcast, she ﬁrst obtains a unique broadcast token from a
Periscope server via a HTTPS connection. Next, she sends
the broadcast token to Wowza and sets up a RTMP connec-
tion to upload video frames. This second step introduces
two critical issues: (1) the broadcast token is sent to Wowza
via RTMP in plaintext; (2) the RTMP video itself is unen-
crypted. As a result, an attacker can easily launch a man-in-
the-middle attack to hijack and modify the video content.
An attacker in the
Tampering on Broadcaster Side.
same edge network as the broadcaster can alter the stream
before it reaches the upload server. This is a common sce-
nario when users connect to public WiFi networks at work,
coﬀee shop or airport. To launch the attack, the attacker sim-
ply connects to the same WiFi network and sniﬀs the vic-
tim’s traﬃc. There is no need to take control of the WiFi
access point.

Speciﬁcally, the attacker ﬁrst performs a simple ARP spoof-

ing attack to redirect the victim’s traﬃc to the attacker14. The
attacker then parses the unencrypted RTMP packet, replaces
the video frame with arbitrary content, and uploads modiﬁed
video frames to Wowza servers, which are then broadcast to
all viewers. This attack can commence anytime during the
broadcast, and is not noticeable by the victim because her
phone will only display the original video captured by the
camera.
An attacker can
Tampering at the Viewer Network.
also selectively tamper with the broadcast to aﬀect only a
speciﬁc group of viewers, by connecting to the viewers’ WiFi
network. When the viewer (the victim) downloads video
content via WiFi, the attacker can modify the RTMP packets
or HLS chunks using a similar approach. The broadcaster
remains unaware of the attack.
Experimental Validation. We performed proof-of-concept
experiments to validate both attack models. Since both at-
tacks are similar, for brevity we only describe the experiment
to tamper at the broadcaster. We set up a victim broadcaster
as a smartphone connected to a WiFi network, an attacker as
a laptop connected to the same WiFi, and a viewer as another
smartphone connected via cellular. The broadcaster begins
a broadcast and points the camera to a running stopwatch
(to demonstrate the “liveness” of the broadcast). When the
attack commences, the attacker replaces the video content
with a black screen.

14ARP spooﬁng is a known technique to spoof another host
in a local area network. This is done by sending out falsiﬁed
ARP (Address Resolution Protocol) messages to manipulate
the mapping between IP and MAC address in the local net-
work.

Before Attack

After Attack

r
e
t
s
a
c
d
a
o
r
B

r
e
w
e
V

i

Figure 18: Screenshots of the broadcaster and viewer
before and after the attack. After the attack, the
viewer sees a black screen (tampered), while the
broadcaster sees the original video.

The attacker runs ARP spooﬁng to perform the man-in-
the-middle attack (using the Ettercap library), and replaces
video frames in the RTMP packet with the desired frames.
Our proof of concept uses simple black frames. We wrote
our own RTMP parser to decode the original packet and
make the replacements. Finally, we open the broadcast as
viewer from another phone to examine the attack impact.
The viewer does not need connect to this WiFi network.

Figure 18 shows the screenshot results of the broadcaster
and the viewer before and after the attack. Following the
attack, the viewer’s screen turns into a black frame while
the broadcaster sees no change. In practice, comments from
viewers may alert the broadcaster to the tampered stream,
but by then the damage is likely done.

7.2 Defense

The most straightforward defense is to replace RTMP with
RTMPS, which performs full TLS/SSL encryption (this is
the approach chosen by Facebook Live). Yet encrypting
video streams in real time is computationally costly, espe-
cially as smartphone apps with limited computation and en-
ergy resources. Thus for scalability, Periscope uses RTMP/HLS
for all public broadcasts and only uses RTMPS for private
broadcasts.

Another simple countermeasure would protect (video) data
integrity by embedding a simple periodic signature into the
video stream. After a broadcaster obtains a broadcast to-
ken from the Periscope server (via HTTPS), she connects to
Wowza using this broadcast token and securely exchanges
a private-public key pair (TLS/SSL) with the server. When
uploading video to Wowza (using RTMP), the broadcaster
signs a secure one-way hash of each frame, and embeds the
signature into the metadata. The Wowza server veriﬁes the
signatures to validate video frames have not been modiﬁed.
To mitigate viewer-side attacks, Wowza can securely for-
ward the broadcaster’s public key to each viewer, and they
can verify the integrity of the video stream. Our solution is
simple and lightweight, and we can further reduce overhead
by signing only selective frames or signing hashes across
multiple frames.

We reported this attack and countermeasure to the man-
agement teams at both Periscope and Meerkat in September

4962015. To the best of our knowledge, Periscope is taking ac-
tive steps to mitigate this threat.

8. DISCUSSION AND CONCLUSION

Our work shows a clear tension between scalability and
delivery delay on today’s personalized livestreams services.
We also recognize that these systems are quite young in their
development, and ongoing engineering eﬀorts can signiﬁ-
cantly reduce per-broadcast overheads and improve scalabil-
ity. Our results suggest, however, that services like Periscope
are already limiting user interactions to ensure minimal lag
between the audience and broadcaster. Moving forward, these
services will have to make a diﬃcult decision between main-
taining hard limits on user interactivity (limiting comments
to the ﬁrst 100 users connected to RTMP servers), or ad-
dressing issues of scale in order to support more inclusive
and richer modes of audience interaction.

One potential alternative is to build a dramatically dif-
ferent delivery infrastructure for interactive livestreams. To
avoid the costs of managing persistent connections to each
viewer, we can leverage a hierarchy of geographically clus-
tered forwarding servers. To access a broadcast, a viewer
would forward a request through their local leaf server and
up the hierarchy, setting up a reverse forwarding path in
the process. Once built, the forwarding path can eﬃciently
forward video frames without per-viewer state or periodic
polling. The result is eﬀectively a receiver-driven overlay
multicast tree (similar to Scribe [12] and Akamai’s stream-
ing CDN [34, 23]) layered on top of a collection of CDN or
forwarding servers. We note that Akamai’s CDN is focused
on scalability, and uses a two-layer multicast tree that fo-
cuses on optimizing the transmission path from broadcaster
to receiver [23]. Since its audience does not directly interact
with the broadcaster, streams do not need to support real-
time interactions.

Moving forward, we believe user-generated livestreams
will continue to gain popularity as the next generation of
user-generated content. Novel methods of interaction be-
tween broadcasters and their audience will be a diﬀerentiat-
ing factor between competing services, and issues of scal-
able, low-latency video delivery must be addressed.

Finally, following consultations with the Periscope team,
we will make parts of our measurement datasets available to
the research community at http://sandlab.cs.ucsb.edu/periscope/.

Acknowledgments
The authors wish to thank the anonymous reviewers and
our shepherd Fabian Bustamante for their helpful comments.
This project was supported by NSF grants CNS-1527939
and IIS-1321083. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are those of
the authors and do not necessarily reﬂect the views of any
funding agencies.

9. REFERENCES
[1] Accessing fastly’s ip ranges. https://docs.fastly.com/guides/

securing-communications/accessing-fastlys-ip-ranges.

[2] Adobe rtmp speciﬁcation. http://www.adobe.com/devnet/

rtmp.html.

[3] Apple hls speciﬁcation. https://developer.apple.com/

streaming/.

[4] Fastly. https://www.fastly.com/.
[5] Fastly network map. https://www.fastly.com/network.
[6] Huang, C., Wang, A., Li, J., Ross, K. W. Measuring and

evaluating large-scale CDNs. http://dl.acm.org/citation.cfm?
id=1455517 (2008).

[7] Periscope - live streaming with your gopro. https://gopro.
com/help/articles/Block/Periscope-Live-Streaming-with-
your-GoPro.

[8] Wowza stream engine. https://www.wowza.com/products/

streaming-engine.

[9] Adhikari, V., Guo, Y., Hao, F., Hilt, V.,and Zhang, Z.-L. A
tale of three cdns: An active measurement study of hulu and
its cdns. In INFOCOM Workshops (2012).

[10] Adhikari, V. K., Guo, Y., Hao, F., Varvello, M., Hilt, V.,

Steiner, M.,and Zhang, Z.-L. Unreeling netﬂix:
Understanding and improving multi-cdn movie delivery. In
Proc. of INFOCOM (2012).

[11] Bouzakaria, N., Concolato, C., and Le Feuvre, J. Overhead

and performance of low latency live streaming using
mpeg-dash. In Proc. of IISA (2014).

[12] Castro, M., et al. Scribe: A large-scale and decentralized
application-level multicast infrastructure. IEEE JSAC 20, 8
(2002).

[13] Constine, J. Twitter conﬁrms periscope acquisition, and

here’s how the livestreaming app works. TechCrunch, March
2015.

[14] Cresci, E.,and Halliday, J. How a puddle in newcastle
became a national talking point. The Guardian, January
2016.

[15] Dredge, S. Twitter’s periscope video app has signed up 10m

people in four months. The Guardian, August 2015.

[16] Hamilton, W. A., Garretson, O., and Kerne, A. Streaming

on twitch: fostering participatory communities of play within
live mixed media. In Proc. of CHI (2014), ACM.

[17] Hei, X., Liang, C., Liang, J., Liu, Y., and Ross, K. W. A

measurement study of a large-scale p2p iptv system. IEEE
Transactions on Multimedia 9, 8 (2007).

[18] Huang, T.-Y., Johari, R., McKeown, N., Trunnell, M., and

Watson, M. A buﬀer-based approach to rate adaptation:
Evidence from a large video streaming service. In Proc. of
SIGCOMM (2014).

[19] Jackson, R. How to avoid periscopes broadcast too full

message. Phandroid Blog, August 2015.

[20] Jill, J. ’broadcast is too full’? how to share your periscope

comments. Scope Tips Blog, October 2015.

[21] Kaytoue, M., Silva, A., Cerf, L., Meira Jr, W., and Ra¨issi,
C. Watch me playing, i am a professional: a ﬁrst study on
video game live streaming. In MSND@WWW (2012).

[22] Khan, A. Broadcast too full & you can’t comment? here are

3 ways to get your message out anyway. Personal Blog,
August 2015.

[23] Kontothanassis, L., Sitaraman, R., Wein, J., Hong, D.,

Kleinberg, R., Mancuso, B., Shaw, D., and Stodolsky, D. A
transport layer for live streaming in a content delivery
network. Proc. of the IEEE 92, 9 (2004).

[24] Krishnan, R., Madhyastha, H. V., Srinivasan, S., Jain, S.,

Krishnamurthy, A., Anderson, T.,and Gao, J. Moving
beyond end-to-end path information to optimize CDN
performance. In Proc. of SIGCOMM (2009).

497[25] Kupka, T., Griwodz, C., Halvorsen, P., Johansen, D., and

[39] Siekkinen, M., Masala, E., and K¨am¨ar¨ainen, T. Anatomy of

Hovden, T. Analysis of a real-world http segment streaming
case. In Proc. of EuroITV (2013).

a mobile live streaming service: the case of periscope. In
Proc. of IMC (2016).

[26] Laine, S.,and Hakala, I. H.264 qos and application

[40] Silverston, T.,and Fourmaux, O. Measuring p2p iptv

performance with diﬀerent streaming protocols. In
MobiMedia (2015).

[27] Lederer, S., M¨uller, C., and Timmerer, C. Dynamic

adaptive streaming over http dataset. In Proc. of MMSys
(2012).

[28] Li, Y., Zhang, Y.,and Yuan, R. Measurement and analysis of

a large scale commercial mobile internet tv system. InProc.
of IMC (2011).

[29] Lohmar, T., Einarsson, T., Fr¨ojdh, P., Gabin, F., and

Kampmann, M. Dynamic adaptive http streaming of live
content. In Proc. of WoWMoM (2011).

systems. In Proc. of NOSSDAV (2007).

[41] Small, T., Liang, B., and Li, B. Scaling laws and tradeoﬀs in

peer-to-peer live multimedia streaming. In Proc. of MM
(2006).

[42] Sripanidkulchai, K., Ganjam, A., Maggs, B., and Zhang, H.

The feasibility of supporting large-scale live streaming
applications with dynamic application end-points. In Proc. of
SIGCOMM (2004).

[43] Sripanidkulchai, K., Maggs, B.,and Zhang, H. An analysis
of live streaming workloads on the internet. In Proc. of IMC
(2004).

[30] Madrigal, A. C. The interesting problem with periscope and

[44] Su, A.-J., Choffnes, D. R., Kuzmanovic, A., and

meerkat. Fusion, March 2015.

[31] Magharei, N., and Rejaie, R. Prime: Peer-to-peer

receiver-driven mesh-based streaming. IEEE/ACM TON 17, 4
(2009), 1052–1065.

[32] Mediati, N. Twitter cuts oﬀ meerkat, won’t let it import who

you follow on twitter. PCWorld, March 2015.

[33] M¨uller, C., Lederer, S., and Timmerer, C. An evaluation of

dynamic adaptive streaming over http in vehicular
environments. In Proc. of MoVid (2012).

[34] Nygren, E., Sitaraman, R. K., and Sun, J. The akamai

network: a platform for high-performance internet
applications. SIGOPS OSR 44, 3 (2010).

[35] Perez, S. Live streaming app periscope touts 200 million

broadcasts in its ﬁrst year. TechCrunch, March 2016.

[36] Poblete, B., Garcia, R., Mendoza, M., and Jaimes, A. Do all

birds tweet the same?: characterizing twitter around the
world. In Proc. of CIKM (2011).

[37] Pramuk, J. Periscope ceo: How we’re growing

live-streaming. CNBC, December 2015.

[38] Pullen, J. P. You asked: What is the meerkat app? Time,

March 2015.

Bustamante, F. E. Drafting behind akamai (travelocity-based
detouring). In Proc. of SIGCOMM (2006).

[45] Tang, J. C., Venolia, G., and Inkpen, K. M. Meerkat and

periscope: I stream, you stream, apps stream for live streams.
In Proc. of CHI (2016).

[46] Wilson, C., Boe, B., Sala, A., Puttaswamy, K. P. N., and
Zhao, B. Y. User interactions in social networks and their
implications. In Proc. of EuroSys (2009).

[47] Yin, X., Jindal, A., Sekar, V., and Sinopoli, B. A

control-theoretic approach for dynamic adaptive video
streaming over http. In Proc. of SIGCOMM (2015).

[48] Zhang, C.,and Liu, J. On crowdsourced interactive live

streaming: a twitch. tv-based measurement study. In Proc. of
NOSSDAV (2015).

[49] Zhang, X., Liu, J., Li, B.,and Yum, T.-S. P.

Coolstreaming/donet: a data-driven overlay network for
peer-to-peer live media streaming. In Proc. of INFOCOM
(2005).

[50] Zhao, X., Sala, A., Wilson, C., Wang, X., Gaito, S., Zheng,
H., and Zhao, B. Y. Multi-scale dynamics in a massive online
social network. In Proc. of IMC (2012), pp. 171–184.

498