Range Tomography: Combining the Practicality of Boolean

Tomography with the Resolution of Analog Tomography

Sajjad Zarifzadeh

Madhwaraj Gowdagere

szarifzadeh@gmail.com

madhwarajgk@gmail.com

Georgia Tech

Georgia Tech

Constantine Dovrolis

constantine@gatech.edu

Georgia Tech

ABSTRACT
The objective of early network tomography approaches was
to produce a point estimate for the performance of each
network link (Analog tomography). When it became clear
that the previous approach is error-prone in practice, re-
search shifted to Boolean tomography where each link is
estimated as either “good” or “bad”. The Boolean approach
is more practical but its resolution is too coarse. We pro-
pose a new tomography framework that combines the best
of both worlds: we still distinguish between good and bad
links (for practicality reasons) but we also infer a range es-
timate for the performance of each bad link. We apply the
Range tomography framework in two path performance met-
ric functions (Min and Sum) and propose an eﬃcient algo-
rithm for each problem. Together with simulations, we have
also applied Range tomography in three operational net-
works allowing us to identify the location of bad links and
to estimate their performance during congestion episodes.
We also compare the proposed method with existing Analog
and Boolean tomography algorithms.

Categories and Subject Descriptors
C.2.3 [Computer-communication Networks]: Network
Operations—Network management, Network monitoring; C.4
[Performance of Systems]: Measurement techniques

Keywords
Network tomography, localization, performance metric

1.

INTRODUCTION

The objective of network tomography is to infer the in-
ternal characteristics of a network (e.g., link delay and loss
rate, topology) from end-to-end path measurements. As a
research area, network tomography is both interesting and
signiﬁcant. It is interesting because the underlying estima-
tion problem is typically under-constrained (more unknowns
than equations) and so its solution requires creative and

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

domain-speciﬁc additional constraints and objectives. Fur-
ther, network tomography combines the art of active probing
with the science of statistical inference. Network tomogra-
phy is also signiﬁcant in practice because it allows users and
network administrators to monitor and troubleshoot a set
of network paths of interest without having direct access to
every router and switch in those paths. This is particularly
important in the context of interdomain paths, i.e., paths
that traverse more than one network provider. Addition-
ally, as IP-based networks move towards performance-based
Service Level Agreements (SLAs) tomographic methods will
become even more valuable. In fact, the motivation for this
work resulted from an actual system that is used to monitor
in real-time the performance of network paths that inter-
connect a large number of scientiﬁc research labs around
the world using end-to-end measurements.

The ﬁrst eﬀorts in network tomography, about 15 years
ago, focused on the most challenging instance of the prob-
lem: estimate the actual performance (e.g., loss rate or delay
variation) of every link in the monitored network [1, 2]. We
refer to those tomographic methods as Analog because they
provide a real-valued performance estimate for each link.
Analog tomography is a very challenging problem for sev-
eral reasons, some of which are related to the underlying
path measurements. For instance, those early methods as-
sume that if two paths go through a single and shared lossy
link, their measured path loss rates will be equal. This may
be true if the measurements last for a very long period, as-
suming that the network load is stationary and the routes
do not change.
In practice, however, network conditions
change signiﬁcantly with time, and so the path measure-
ments must be performed in short time intervals (e.g., one
minute) before they become stale [3]. To make things worse,
the probing frequency cannot be too high because the path
measurements can then aﬀect the underlying network per-
formance [4]. So, Analog tomography methods need to work
with noisy path estimates that result from short-term active
measurements.

In 2003, Duﬃeld proposed a paradigm shift in network to-
mography [5]. Instead of trying to estimate the actual per-
formance of a link, it is more practical to only infer whether
the link’s performance is “good” versus “bad”. This approach
is referred to as Boolean tomography and it has also been fol-
lowed by other researchers [6, 8]. Here, path measurements
become much simpler (just detect congested paths) and so
they can take place in shorter time periods and with fewer
probes. The problem, of course, is that the result of the
tomographic process has low resolution: just a single bit for

385each link. This may be suﬃcient if the objective is to simply
identify severe congestion. In practice, however, the conges-
tion level of network links varies widely and even though an
application or operator may be satisﬁed with a loss rate of
0.1% or a queueing delay of 50msec, another may ﬁnd these
congestion levels unacceptable.

In this work, we propose a new approach in network to-
mography that attempts to combine the higher resolution
of Analog tomography with the practicality of Boolean to-
mography. The path measurements are still classiﬁed as
good or bad, to exploit the fact that most network paths
typically operate without congestion [15]. This allows us to
quickly remove a large number of links from the problem,
as long as they only appear in good paths. For the remain-
ing paths, we aim not only to identify bad links, but also
to estimate a range (interval) for their actual performance.
The width of the resulting range depends on the variability
in the underlying path measurements: more accurate path
measurements will result in narrower range estimates. We
refer to the proposed framework as Range Tomography.

We apply the Range tomography framework in two path
performance formulations: a Min function in which a path’s
performance is determined by the link with the minimum
performance, and a Sum function in which a path’s perfor-
mance is determined by the sum of its links’ performance
values. Min is more appropriate for metrics such as avail-
able bandwidth, while Sum can be used for delay and (as
an approximation) for loss rate. For each formulation we
propose an eﬃcient algorithm to identify bad links and es-
timate their performance range. The algorithms are ﬁrst
evaluated with simulations in three real topologies. Sim-
ulations are not ideal, but they allow us to examine the
false positives, false negatives and accuracy of Range tomog-
raphy under controllable and repeatable experiments. We
performed these simulations under both random and bursty
loss processes, showing the signiﬁcant impact of the latter
in network tomography methods. Further, we compare the
proposed algorithms with “canonical” methods from Analog
and Boolean tomography.

We have also applied the proposed methods in practice
to detect and localize congestion events in three operational
large networks, where the number of monitored paths varies
between 70-3600. Even though we cannot use those real-
world experiments for validation (we do not have access to
routers and switches in those networks), they show some
interesting points. First, in practice, there is rarely more
than one bad link in the monitored paths and so the false
positive/negative probability is close to zero. Second, the
actual congestion level of those bad links can vary widely,
and so it is important to also estimate a congestion range
instead of a single bit.

The rest of the paper is structured as follows. In Section
2, we describe the Range Tomography problem. In Sections
3 and 4, we consider the Min and Sum formulations, respec-
tively. In Section 5, we evaluate tomography methods using
simulations. We present a short summary of the experimen-
tal results in Section 6. Section 7 discusses the related work
and we conclude in Section 8.

2.1 Problem statement

We are given a set of N measurement points or hosts,
referred to as sensors S = {s1, ..., sN}. Each sensor mea-
sures the end-to-end path performance, with respect to a
certain metric such as loss rate, one-way delay or available
bandwidth, to all other sensors. We are interested to local-
ize performance problems, with respect to the corresponding
metric, in the set Π of N × (N − 1) paths between sensors.
The forwarding paths between sensors can be “mapped” at
the IP-layer using traceroute-like tools (such as Paris tracer-
oute [22]). Each path is represented by a sequence of links
and each link is represented by the IP address of the cor-
responding router or host interface. Let G = (V, E) be the
directed graph constructed from the union of all paths be-
tween sensors.

Suppose that pi,j denotes the path from sensor si to sensor
sj, and mi,j is the actual performance of pi,j for a given per-
formance metric in the time interval of interest. We assume
that there is a function f that expresses the performance of
a path based on the performance of its constituent links,

mi,j = f ({xl|∀l ∈ pi,j})

(1)

where xl is the actual performance of link l. We refer to
f as the path metric function. For instance, in the case
of available bandwidth, the performance of the end-to-end
path is determined by the link with the minimum available
bandwidth. Thus, the function f for available bandwidth is
based on the MIN operator. In the case of delay, jitter and
(as an approximation) loss rate, the path performance can
be estimated by the sum of link performances. Hence, the
function f would be the SUM operator for those metrics. In
network tomography, we attempt to invert function f and
estimate the performance of some links given the measured
performance of a set of paths that traverse those links. As
in other instances of tomography, this “inversion problem”
is typically ill-deﬁned and it can be solved only if we impose
additional constraints, optimization objectives and assump-
tions [10].

In practice, the performance of a network path must be
measured with low frequency probing (i.e., the probing traf-
ﬁc load should be low compared to the capacity of the path),
and in short time intervals (due to dynamic network condi-
tions). Hence, the measured path performance can be sig-
niﬁcantly diﬀerent than the actual path performance.
In
other words, the measured performance of two paths shar-
ing the same bottleneck link can be diﬀerent even though
their actual performance is the same. Let ˜mi,j be the mea-
sured performance of path pi,j in the time interval of interest.
Because of the previous limitations of the measurement pro-
cess, we typically have that mi,j 6= ˜mi,j, while the diﬀerence
between the two metrics can vary signiﬁcantly over diﬀerent
paths. To illustrate this issue, consider the following simple
example. Assume that the packet drops at a link l follow a
Bernoulli random process with probability xl = 0.01, while
the measurement process sends n = 1000 probing packets
on that link. The variance of the measured packet losses is

n xl (1 − xl) and so the standard deviation (≈ √10) is sig-

niﬁcant compared to the expected number of packet losses
at link l (10 packets).

2. RANGE TOMOGRAPHY

In this section, we motivate, state and explain the Range

Tomography problem.

In the following, we introduce the Range Tomography
framework as a way to deal with the previous uncertainty in
path measurements. First, we inherit from Boolean tomog-
raphy the notion that paths and links can be distinguished

386s2

2

s1

1

4

5

s4

3

s3

Figure 1: A simple network example with four sensors. The
dashed lines indicate lossy links.

as “good” versus “bad”. Formally, a path pi,j is called bad if
its measured performance ˜mi,j is worse than a threshold δ
(δ may be zero, as in the case of loss rate for example). Sim-
ilarly, a link is bad if its performance metric is worse than δ,
otherwise it is good. After we infer the bad links from path
measurements, we also estimate the performance of every
bad link with a “range estimate” for its metric.
In other
words, we start with path point measurements and end up
with link range estimates. To this end, we ﬁrst identify and
group paths that are likely aﬀected by the same bad link
and then use the measured performance of these paths to
determine a narrow performance range for that link.

Deﬁnition 1. Two paths pi,j and pk,l are α-similar (rep-
resented by pi,j k pk,l), if their relative performance diﬀer-
ence is within a parameter α,

bad link are not α-similar, we can conclude that there are
additional bad links (as in the previous example).

Let ˜xl = [sl..el] denote the performance range assigned to
link l (i.e., the actual performance of l is estimated to be
between sl and el). |˜xl| denotes the width of range ˜xl. We
deﬁne next how to examine whether the assigned link per-
formance ranges are consistent with the path performance
measurements.

Deﬁnition 2. The performance ranges assigned to bad
links on a path pi,j are consistent with the measured per-
formance of pi,j if there is at least one value in each of these
ranges that would satisfy the path performance constraint
in (1). We denote the consistency relation as:

˜mi,j ≃ f ({˜xl|∀l ∈ pi,j}) if:
∀l ∈ pi,j,∃tl ∈ ˜xl : ˜mi,j = f ({tl|∀l ∈ pi,j})

(3)

To illustrate the previous deﬁnition, consider the following
example. Suppose that the measured loss rate of a path pi,j
is 1% and there are two bad links l1 and l2 in that path. The
performance range of both links is estimated as [1%-2%].
Obviously, these range estimates are not consistent with the
path measurement. On the contrary, if the performance
ranges are both [0.4%-0.6%], then they are consistent with
the measured path loss rate. In this case, we also say that
the performance ranges assigned to l1 and l2 justify the bad
path pi,j (pi,j is then called justiﬁed path).

Before stating the Range Tomography problem more for-

mally, we need to make the following assumptions:

(1) There are no spatial correlations between diﬀerent bad

| ˜mi,j − ˜mk,l|
min( ˜mi,j, ˜mk,l) ≤ α

(2)

links.1

where α is a positive parameter that quantiﬁes the accuracy
of the measurement process (we also say that the measure-
ments ˜mi,j and ˜mk,l are α-similar ). α is related to the
measurement process (e.g., size of probing packets, prob-
ing frequency, probing method) and to the characteristics of
the estimated path performance process. The parameter α
can be estimated experimentally, measuring paths that are
known to be limited by the same bottleneck link. The rela-
tive diﬀerence of the resulting path measurements should be
less than the chosen value of α. In the rest of this section,
we assume that α is given as an input to the network tomog-
raphy process. In Section 5, we discuss how we estimated α
in our experiments.

Let us consider the example of Figure 1. pi,j is the short-
est path from si to sj. End-to-end measurements indicate
that the paths p1,4, p2,4, p3,4 are lossy; their measured loss
rates are 15%, 5% and 7%, respectively. All remaining paths
are good. Boolean tomography would choose the link from
node-4 to node-5, represented by (4,5), as the only bad link
because it appears on all lossy paths. However, choosing a
single lossy link cannot explain why these paths have so dif-
ferent loss rates. If α = 0.5, paths p2,4 and p3,4 are α-similar.
Then, one plausible solution is that link (4,5) (shared by
the two α-similar paths) has a loss rate between 5% and
7% while link (1,4) on p2,3 has a loss rate between 8% and
10%. In general, the notion of α-similar paths improves the
network tomography process in two ways: 1) As opposed to
Analog tomography, we can avoid the incorrect detection of
multiple bad links in paths that show diﬀerent performance
even though they are aﬀected by the same bad link. 2) Un-
like Boolean tomography, when the paths sharing the same

(2) The routing paths between sensors remain constant

during the measurement process.2

(3) A path is bad, if and only if, there is at least one bad

link in that path.

Given the directed graph G and a measurement for each
path in Π, the Range Tomography problem can be stated as
follows:
Range Tomography (RT) problem: Infer all bad links
in G and assign a performance range to each bad link so
that:

1. The number of bad links is minimized.

2. The assigned performance ranges to bad links are con-
sistent with the path performance measurements, ∀pi,j ∈
Π : ˜mi,j ≃ f ({˜xl|∀l ∈ pi,j}).

In practice, it is not common to have multiple bad links
at the same time (this is also observed in our experimental
results in Section 6). So, as it is common in Boolean to-
mography, we consider an optimization objective that aims
to minimize the number of detected bad links. Without

1To the extent of our knowledge, there is no prior evidence
that such correlations are common or signiﬁcant in practice.
We make this assumption so that we can calculate the per-
formance of a path from the performance of its constituent
links through a simple function.
2If the network deploys multipath routing, we need to make
sure that both the Paris-traceroute packets and the probing
packets sent from one sensor to another use the same port
numbers so that they follow the same forwarding path.

387this objective, the solution of the RT problem can result in
several false positives. The complexity of the RT problem
depends on the metric function f . In the following two sec-
tions, we consider two speciﬁc instances of the function f
that can be used to capture diﬀerent performance metrics.

3. MIN METRIC FUNCTION

We ﬁrst consider a function that determines the perfor-
mance of path pi,j as the minimum performance among all
links in that path,

mi,j = min∀l∈pi,j{xl}

(4)

We refer to this instance of the RT problem as RT-Min.
The previous function can be used for performance metrics
such as capacity or available bandwidth, because these met-
rics are mostly determined by a single “bottleneck” link. In
the rest of this section, we will refer to available bandwidth
as the performance metric of interest (even though the pro-
posed algorithm is applicable to any metric that satisﬁes the
previous path performance constraint). We say a path (or a
link) is bad if its available bandwidth is less than a threshold
δ.

3.1 Solution and analysis for RT-Min

Deﬁnition 3. A bad link l is detectable if there is at
least one path pi,j traversing l such that (I) there is no other
bad link in pi,j OR (II) other bad links in pi,j have higher
available bandwidth than link l.

The above condition implies that for every bad link l, there
should be at least one path pi,j such that l is the bottleneck
link of that path.

Deﬁnition 4. A good link l is removable, if there is at

least one good path traversing l.

Lemma 1. Assume that there is no measurement error,
i.e. ˜mi,j = mi,j ,∀pi,j ∈ Π. In that ideal case, the available
bandwidth of every bad link can be determined if (I) all good
links are removable, AND (II) all bad links are detectable.
The proof is straightforward and it is omitted. If both con-
ditions of Lemma-1 hold, we can use the following method
(referred to as Min-Ideal) to estimate the available band-
width of the bottleneck link for every bad path. First,
“mark” all links in good paths as good; the rest of the links
are “unmarked”. Then, select the bad path pb with the high-
est available bandwidth among the remaining paths and as-
sign the available bandwidth of pb to all unmarked links
on pb (pb is then labeled as justiﬁed and all links in pb are
marked). We repeat the previous step until there is no un-
justiﬁed path in Π.

However, the two conditions of Lemma-1 (good links are
removable and bad links are detectable) may not be true
in practice.
In addition, the measurement process is al-
ways error-prone. Therefore, we need to consider an algo-
rithm that can infer bad links, even when the assumptions
of Lemma-1 are not met.

Based on the complexity analysis of the Boolean tomogra-
phy problem [8], it is easy to see that the RT-Min problem is
also NP-hard3. So, we need to approximate solution of RT-
Min with heuristics. Our solution to this problem, referred

3To prove this we have to consider the simpliﬁed case where
the available bandwidth of all bad links is the same. In that
case, the RT-Min problem is the same with the Boolean
tomography problem, which is NP-hard [8].

to as Min-Tomo, is based on the following two principles:
I) As in the case of Min-Ideal, the Min-Tomo algorithm runs
iteratively, and in each iteration it considers the bad path
pb that has the highest available bandwidth among all re-
maining paths. Paths that are α-similar with pb are then
grouped into the set Ω.
II) Since good links are not necessarily removable, it is pos-
sible that we consider a link as bad even though it is good,
and vice versa. To reduce this classiﬁcation error, Min-Tomo
greedily selects the link l that is traversed by the largest
number of bad paths in Ω and marks it as bad. The intu-
ition is that it is unlikely that a large number of bad paths
traverse a good link, without any good path traversing that
link.4 Additionally, this greedy heuristic aims to minimize
the number of links identiﬁed as bad.

The pseudo-code of Min-Tomo is presented in Algorithm 1.
We ﬁrst mark all links on every good path as good (line 3).
Then, we consider the path with the highest available band-
width among all bad paths. Let pb be such a path. Instead
of considering only path pb, we consider the set of paths
that are α-similar with pb (this set is referred to by Ω in
line 7). We then select link lm as the link that is shared
most between paths in Ω. In the case of a tie, we select the
link which is traversed by most unjustiﬁed paths (lines 17
and 18). We also check in line 11 that there is at least one
path that is α-similar with pb and traverses lm. Taking ¯r
as the average available bandwidth of paths going over lm
(line 19), the performance range of lm is determined in line
20 so that every value in the range ˜xlm is α-similar with ¯r.
We label all paths traversing lm as justiﬁed (line 21), if their
available bandwidth falls in the performance range ˜xlm . We
repeat this process until there is no unjustiﬁed path.

The estimated ranges produced by Min-Tomo satisfy the
consistency requirement for the following reason. Let h(lm)
be the highest available bandwidth path traversing lm. Sup-
pose there are several paths that are α-similar with h(lm)
and traverse lm. Let Φ(lm) be the set of these paths. In the
RT-Min problem, the range ˜xlm assigned to a link lm is con-
sistent with the measured performance of path pi,j ∈ Φ(lm)
if the measurement ˜mi,j belongs in the range ˜xlm . It can be
shown that this is true for every path in Φ(lm) if we select
the range ˜xlm as shown in line 20.

The run-time complexity of Min-Tomo is O(|Π|2+|Π||E|).
The running time for a network with about 1000 bad paths
and 4600 links is less than 5.5msec on a workstation with a
2.6GHz Intel i5 processor.

4. SUM METRIC FUNCTION

In this section we consider the following path metric func-

tion,

mi,j = X
∀l∈pi,j

xl

(5)

The corresponding range tomography problem is referred to
as RT-Sum. The Sum metric function can capture perfor-
mance metrics such as delay and jitter. Under the previous
spatial independence assumption, the path loss rate is given
by

mi,j = 1 − Y

∀l∈pi,j

(1 − xl)

(6)

4The analysis of three real topologies, described in Section 5,
conﬁrms this intuition.

388Algorithm 1 Min-Tomo

Require: Set of all links E
Require: Set of all paths Π
Require: The measured available bandwidth of every path

in Π, ˜mi,j,∀pi,j ∈ Π

1: Initialize R = Π {set of unjustiﬁed paths}
2: Initialize U = E {set of unmarked links}
3: Remove from R and U all good paths and all links on

good paths, respectively.

available bandwidth going over l}

4: h(l) = argmax∀pi,j∈R{ ˜mi,j}, ∀l ∈ U {path with highest
5: while R 6= ∅ and U 6= ∅ do
6:

pb = argmax∀pi,j∈R{ ˜mi,j} {path with highest avail-
able bandwidth in R}
Ω = {pi,j|pi,j ∈ R, pi,j k pb} {set of paths which are
α-similar with pb}
num(l) = |{pi,j|l ∈ pi,j, pi,j ∈ R}| {number of unjus-
tiﬁed paths going over l}
for each link l in U do

7:

8:

9:
10:
11:
12:
13:
14:
15:
16:
17:

C(l) = set of paths in Ω containing l
if h(l) k pb then
else

score(l) = |C(l)| {number of paths in C(l)}
score(l) = 0

18:

19:

end if
end for
Lm = argmaxl∈Uscore(l) {set of links with the max-
imum score}
lm = argmaxl∈Lmnum(l) {link with the maximum
score and maximum number of unjustiﬁed paths going
over it}
¯r = avg{ ˜mi,j|pi,j ∈ Ω, lm ∈ pi,j} {average available
bandwidth of paths in Ω going over lm }
˜xlm = [¯r( 1
1+α ), ¯r(1 + α)] {performance range of link
lm }
21: R = R − {pi,j|lm ∈ pi,j, pi,j ∈ R, ˜mi,j ∈ xlm} {Now,
some paths going over lm are justiﬁed}
22:
U = U − {lm}
23: end while
24: return ˜xl, ∀l ∈ E

20:

However, if the link loss rates are low and there are few
lossy links in each path, the two formulas give similar results
(e.g., in the case of three lossy links each with 1% loss rate,
the actual loss rate is about 2.97% even though the Sum
approximation gives 3%).
In the rest of this section, we
consider loss rate as the performance metric of interest, and
we refer to bad links as “lossy” (i.e., δ=0).

4.1 Solution and analysis for RT-Sum

We ﬁrst present a necessary condition for the identiﬁcation

of lossy links assuming error-free measurements.

Deﬁnition 5. A lossy link l is detectable-1 if there is at
least one path pi,j traversing l such that there is no other
lossy link in pi,j . A lossy link l is detectable-n (n > 1) if
it is either detectable-(n-1) OR there is at least one path
pi,j traversing l such that all other lossy links on pi,j are
detectable-(n-1).

Lemma 2. Assume there is no measurement error, i.e.,
˜mi,j = mi,j,∀pi,j ∈ Π. The loss rate of every lossy link can

be determined if (I) all good links are removable AND (II)
all lossy links are detectable-n (n ≥ 1).
Proof: Under the previous conditions, we can use the
following method (referred to as Sum-Ideal) to estimate the
loss rate of lossy links. Let U be the set of links identiﬁed as
lossy. Based on the recursive deﬁnition of detectable-n links,
there should exist at least one path that traverses only one
link in U . Suppose pi,j is such a path, traversing link l ∈ U .
Hence, the loss rate of l is the same as the loss rate of pi,j
(xl = mi,j). We remove l from U and then subtract the loss
rate of l from the loss rate of every path traversing l.
In
the next iteration, there should again exist at least one path
that traverses only one link of U ; otherwise, the remaining
links in U would not be detectable-n (n ≥ 1). We repeat
this process until the set U is empty. In each iteration, one
link is removed from U , and so this algorithm is guaranteed
to determine the loss rate of every bad link.

However, the two conditions of Lemma-2 may not be true
in practice. We conducted a simple experiment to evaluate
how often those two conditions are violated in three real
topologies (described in Section 5). We randomly select a
number of lossy links and assign a loss rate to them from
a uniform distribution. The percentage of lossy links that
are not detectable-n is quite low in all three topologies (less
than 2% on average when up to 10% of links in the ESNet
topology are lossy). But, the percentage of good links that
are not removable is higher (around 7% when up to 10%
of links in ESNet are lossy). In other words, it is possible
that all paths traversing a good link are bad. Therefore,
we need to consider an algorithm that can infer lossy links,
even when the conditions of Lemma-2 are not met.

The RT-Sum problem is also NP-hard. To prove this, it
is enough to consider the case where all lossy paths have
the same loss rate. The solution in that case is the same
with the solution of the Boolean tomography problem. We
approximate the solution of the RT-Sum problem with a
heuristic, referred to as Sum-Tomo, which is based on the
following three principles:
I) In each iteration, we only consider the set of paths that
are α-similar with the path that has the lowest loss rate.
This set is referred to as Ω.
II) Similar to Min-Tomo, we greedily choose the link l that
is traversed by the maximum number of lossy paths in Ω.
III) We subtract the assigned loss rate of a detected bad link
from all paths traversing that link.

The pseudo-code of Sum-Tomo is presented in Algorithm 2.
The input arguments are the same with Min-Tomo in Algo-
rithm 1 and they are omitted. After removing good paths
and their links (in line 3), we consider the path pb with the
lowest loss rate; let Ω be the set of paths that are α-similar
with pb (line 6 and 7). In line 14, we greedily choose link
lm as the link that is traversed by most paths in Ω (if there
are multiple such links, the link that is traversed by most
unjustiﬁed paths is chosen). We calculate ¯r as the average
loss rate of paths in Ω that traverse lm (line 15), and so the
performance range of lm (line 16) is such that every point
in ˜xlm is α-similar with ¯r. We then mark all paths (travers-
ing lm) as justiﬁed (line 17), if their loss rate is within the
performance range of lm.
In that case, we also subtract
the average loss rate ¯r from the loss rate of all paths that
traverse lm (line 18).

The run-time complexity of Sum-Tomo is also O(|Π|2 +
|Π||E|). The Sum-Tomo algorithm can only approximate

389Algorithm 2 Sum-Tomo
1: Initialize R = Π {set of unjustiﬁed paths}
2: Initialize U = E {set of unmarked links}
3: Remove from R and U all good paths and all links on

good paths, respectively.

5. EVALUATION

This section presents an evaluation study of diﬀerent to-
mography algorithms using simulations on real topologies.
For brevity, we only consider the RT-Sum problem and the
loss rate metric.

which has not been justiﬁed yet}

4: Initialize ri,j = ˜mi,j,∀pi,j ∈ R {the loss rate of pi,j
5: while R 6= ∅ and U 6= ∅ do
6:

pb = argmin∀pi,j ∈R{ri,j} {path with lowest loss rate
in R}
Ω = {pi,j|pi,j ∈ R, pi,j k pb} {set of paths which are
α-similar with pb, using ri,j instead of mi,j}
num(l) = |{pi,j|l ∈ pi,j, pi,j ∈ R}| {number of lossy
paths going over l}
for each link l in U do

7:

8:

C(l) = set of paths in Ω containing l
score(l) = |C(l)| {number of paths in C(l)}

9:
10:
11:
12:
13:

15:

14:

end for
Lm = argmaxl∈Uscore(l) {set of links with the max-
imum score}
lm = argmaxl∈Lmnum(l) {link with the maximum
score and maximum number of lossy paths going over
it}
¯r = avg{ri,j|lm ∈ pi,j , pi,j ∈ Ω} {average loss rate of
paths in Ω going over lm }
˜xlm = [¯r( 1
1+α ), ¯r(1 + α)] {performance range of link
lm }
17: R = R−{pi,j|pi,j ∈ R, ri,j ∈ ˜xlm} {Some paths going
over lm are now justiﬁed}
18:
ri,j = max{0, ri,j − ¯r}, ∀pi,j ∈ R, lm ∈ pi,j {Update
loss rate of paths going over lm}
U = U − {lm}

16:

19:
20: end while
21: return ˜xl, ∀l ∈ E

the optimal solution to the RT-Sum problem for two reasons.
First, as in the case of Min-Tomo, it may not return the
minimum number of bad links. Second, as opposed to Min-
Tomo, Sum-Tomo may violate the consistency constraint.
To understand how this can happen consider the following
example. Assume we have three links l1, l2 and l3, and three
lossy paths p1 =< l1 >, p2 =< l1, l2 > and p3 =< l2, l3 >.
The measured path loss rates are 3%, 4% and 2%, respec-
tively. Suppose that α = 0.1. The Sum-Tomo algorithm
would only detect l1 and l2 as lossy links, with the same loss
rate range [1.8%-2.2%] for both links. Note that these ranges
are not consistent with the measurement for path p1. A con-
sistent solution in this example would be that all three links
are lossy and their loss rate ranges are xl1 = [2.7% − 3.3%],
xl2 = [0.9%−1.1%] and xl3 = [0.9%−1.1%]. Notice however
that this solution results in more lossy links that the solu-
tion provided by Sum-Tomo. In other words, there can be
a trade-oﬀ between minimizing the number of inferred lossy
links and satisfying the consistency requirement.
In fact,
in the RT-Sum problem, we can show that ﬁnding a solu-
tion that satisﬁes the consistency requirement is NP-hard5
(assuming that the performance ranges cannot contain zero
loss rate).

5.1 Simulation model

We have developed a packet-level event-driven simulator
to evaluate tomography algorithms under various network
conditions. In each simulation run, we ﬁrst set the number
of lossy links to c. We vary c to examine how diﬀerent to-
mography algorithms perform as the likelihood of lossy links
increases. We then select the lossy links in the underlying
network and assign an average loss rate to each of them. In
the following results, the link loss rates are drawn from a
Lognormal distribution with mean 0.04 and standard devia-
tion 0.16 (these parameters were estimated from a large set
of loss rate measurements in about 3600 Planetlab paths).
The loss rate of the remaining links is set to zero.

We consider two loss processes:

(I) A Bernoulli random process where each arriving packet
at a link is dropped with a given probability (equal to that
link’s loss rate).
(II) A Gilbert random process where the link’s state varies
between good and congested. In the good state, the link does
not drop packets. In the congested state, the link drops ar-
riving packets with a certain probability. The duration of the
good and congested states follows an exponential distribu-
tion (with average duration Tgood and Tcong, respectively).
The loss probability in the congested state is calculated so
that the long-term loss rate, across both good and congested
time periods, is equal to the assigned average loss rate for
that link.

The Gilbert process can better capture the bursty con-
gestion events that are commonly observed in practice, es-
pecially when Tcong is much shorter than Tgood. In our sim-
ulations, we have examined two cases for Tgood and Tcong:
1) they are both set to 10sec and, 2) Tgood is set to 100sec
and Tcong = 10sec. The results with the ﬁrst pair of values
are similar to the Bernoulli case, and so we only report the
results with the second pair of values.

To measure path loss rates, we simulate sending 4000
probing packets in each path at a rate of ten packets per
second (i.e., the measurement duration is 400 seconds). If
the measured path loss rate is less than 0.1%, the path is
good (i.e. δ = 0.1%). The actual loss rate of a lossy link l
is the ratio of probing packets (across all paths traversing l)
that have been dropped at l.

We consider three IP-layer network topologies. Two of
them (Internet2 and ESNet) have been provided to us by the
corresponding operators. The third was obtained running
Paris-traceroute [22] between 100 PlanetLab hosts. In more
detail, the three topologies are:

1. The Internet2 topology,

interconnecting 9 Internet2
sensors (located at the major PoPs of Internet2). It
includes 44 links (all with known IP addresses) and 72
paths. The average path length is around 4 hops.

2. The ESNet topology, interconnecting 22 ESNet sensors
(located at the major PoPs of ESNet). It includes 117

5The proof uses a reduction to the Exact-Set-Cover problem
[9].

6But, we limit the maximum loss rate to 20%.

390links (all with known IP addresses) and 382 paths. The
average path length is around 6 hops.

3. The “PlanetLab topology”, resulting from full-mesh
Paris-traceroute measurements between 100 PlanetLab
hosts at diﬀerent sites. This topology includes 4672
links and 5917 paths. The average path length is about
15 hops.

In each of the previous topologies, there are some links
that have the same path cover set (the path cover set of a
link is the set of paths traversing that link). When two or
more links have the same path cover set, it is impossible to
distinguish those links and detect which of them (if any) is
bad. Hence, as in previous work [15, 24], we group links
with the same path cover set together7 and localize bad link
groups instead of bad links. The size of such groups is a
topological property of the underlying network. The average
group size is around 1.8 in the PlanetLab topology, while it
is almost 1.0 in ESNet and Internet2.

In each simulation run, we select a topology and then
select lossy links either randomly or based on a distribution
that favors links close to the edge of the network (about
80% of the lossy links are at most three hops away from
at least one sensor). This second approach allows us to
simulate scenarios where congestion takes place mostly at
the periphery of the network rather than the core.

We consider the following metrics. Let I be the set of
lossy links and O be the set of lossy links inferred by a
tomography algorithm. The precision is the ratio of links
that are correctly identiﬁed as lossy to the total number
of inferred lossy links, precision = |I T O|
. The recall is the
ratio of links that are detected correctly as lossy to the actual
number of lossy links, recall = |I T O|
. The precision and
recall parameters capture the frequency of false positives and
false negatives, respectively.8 Let Q be the set of lossy links
whose performance range is accurately estimated, i.e., the
performance range assigned to a link in Q includes the actual
loss rate of that link. The accuracy is deﬁned as the ratio
of inferred lossy links whose loss rate range is accurately
estimated, accuracy = |Q|

|O|

|I|

|I T O| .

We consider two methods to set the α parameter. The
ﬁrst method (Method-1 ) relies on prior measurements in
which the ground truth about the identity of bad links is
known (e.g., using SNMP data from routers). The second
method (Method-2 ) does not require any prior knowledge.
Speciﬁcally, in Method-1 we calculate the variation of the
measured end-to-end loss rates in all paths that traverse a
single lossy link, and set α to the minimum value with which
all these paths are α-similar. In the simulation experiments,
Method-1 gives α ≈ 0.3 for Bernoulli losses, and about 0.5
for Gilbert losses. In Method-2, we ﬁrst remove links that
appear in good paths, as well as links that are traversed by
only a small number of bad paths (say 3). For each remain-
ing link, we compute the average loss rate ¯r of all paths
traversing that link, and determine the minimum value of α
with which the loss rate of every such path is α-similar with
¯r. We ﬁnally use the median of the previous α values. The
rationale in Method-2 is that if a link is traversed by multi-

7These groups of links over a path correspond to the Mini-
mal Identiﬁable Link Sequence (MILS), deﬁned in [25].
8In tomography, a false positive refers to detecting a link as
bad while it is good.

ple bad paths it is probably a bad link, and so we set α so
that most such links are traversed by α-similar paths. There
are two cases in which this assumption is incorrect: when
a good link is identiﬁed as bad (false positive) and when a
path traverses more than one bad link. However, as shown
in the simulation results of this section and the experimental
results of the next section, the likelihood of these two events
is low in practice. In addition, by taking the median of α
values across all links traversed by multiple bad paths, we
reduce the impact of these events.

We compare Sum-Tomo with two other tomography al-
gorithms: the Boolean Tomo method [8] and the Analog
Norm method (“L1-norm minimization with non-negativity
constraints”) [23]. Norm starts with similar constraints as in
(1) and solves them heuristically using a certain error min-
imization approach (the estimated link loss rates may not
satisfy the path equations exactly, but they minimize the
corresponding inference error favoring solutions that involve
fewer lossy links). Some recent tomography algorithms, such
as Netscope [15] and LIA [24], apply the previous norm min-
imization method but additionally they rely on the variance
of link loss rates over multiple measurement snapshots. We
do not consider such methods because they require multiple
measurement snapshots (with 1,000-10,000 probing packets
in each snapshot), and so they are less robust to load varia-
tions and routing changes. Our focus is on short-term mea-
surements, and so we only compare our method with the
original Norm algorithm. The precision and recall of Norm
are computed by comparing the returned point estimates
with the threshold δ (so that we can determine the set of
bad links according to that method). To calculate the accu-
racy of Norm, we ﬁrst convert the reported point estimates
to range estimate using the relation shown in line-16 of Al-
gorithm 2.

The results of each simulation are repeated 200 times.
The standard error for all results in this section is negligible
(mostly between 0.005 and 0.02) and so we simply show the
average across all runs.

5.2 Results

We start with the ESNet topology. Figure 2 shows the re-
sults with three tomography algorithms in ESNet, when the
number of lossy links varies from 1 to 20 (i.e., up to about
20% of the total links in that network). Lossy links are se-
lected randomly and the loss process follows the Bernoulli
model. The value of α is set based on Method-1; a compar-
ison with Method-2 is given later in this section (Figure 5).
Since Tomo is not able to determine the loss rate of links,
Figure 2c only compares the results of Sum-Tomo and Norm.
As the number of lossy links increases, we see a degradation
in the performance of all tomography algorithms. This is
because more lossy links create more lossy paths, and hence
fewer links can be accounted as good at the initial step of
the algorithms. Therefore, the tomography methods have
to opt their candidate lossy links from a larger set, which
increases the localization errors.

As illustrated in Figure 2a, the precision of Tomo and
Sum-Tomo is very close. However, the recall of Sum-Tomo
is higher than of Tomo by up to 13% (Figure 2b). The rea-
son is that Tomo does not consider the loss rate of lossy
paths and it simply aims to ﬁnd the minimum set of links
shared between such paths. This approach can be clearly
wrong, however, because a link can justify diﬀerent paths

391i

i

n
o
s
c
e
r
p
e
g
a
r
e
v
A

 

i

i

n
o
s
c
e
r
p
e
g
a
r
e
v
A

 

i

i

n
o
s
c
e
r
p
 
e
g
a
r
e
v
A

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

Sum-Tomo
Tomo
Norm

l
l

a
c
e
r
 

e
g
a
r
e
v
A

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

Sum-Tomo
Tomo
Norm

y
c
a
r
u
c
c
a
e
g
a
r
e
v
A

 

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

Sum-Tomo
Norm

 2

 4

 6

 8  10  12  14  16  18  20

 2

 4

 6

 8  10  12  14  16  18  20

 2

 4

 6

 8  10  12  14  16  18  20

Number of lossy links

(a) Precision

Number of lossy links
(b) Recall

Number of lossy links

(c) Accuracy

Figure 2: Results of diﬀerent algorithms obtained in ESNet when actual link loss rates follow Bernoulli process.

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

Sum-Tomo
Tomo
Norm

l
l

a
c
e
r
 
e
g
a
r
e
v
A

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

Sum-Tomo
Tomo
Norm

y
c
a
r
u
c
c
a
e
g
a
r
e
v
A

 

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

Sum-Tomo
Norm

 2

 4

 6

 8  10  12  14  16  18  20

 2

 4

 6

 8  10  12  14  16  18  20

 2

 4

 6

 8  10  12  14  16  18  20

Number of lossy links

(a) Precision

Number of lossy links
(b) Recall

Number of lossy links

(c) Accuracy

Figure 3: Results of diﬀerent algorithms obtained in ESNet when actual link loss rates follow Gilbert process.

Sum-Tomo
Tomo
Norm

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

l
l

a
c
e
r
 
e
g
a
r
e
v
A

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

Sum-Tomo
Tomo
Norm

y
c
a
r
u
c
c
a
 
e
g
a
r
e
v
A

 1

 0.95

 0.9

 0.85

 0.8

 0.75

 0.7

 0.65

 0.6

 0.55

 0.5

Sum-Tomo
Norm

 10

 20

 30

 40

 50

 60

 10

 20

 30

 40

 50

 60

 10

 20

 30

 40

 50

 60

Number of lossy links

(a) Precision

Number of lossy links
(b) Recall

Number of lossy links

(c) Accuracy

Figure 4: Results of diﬀerent algorithms obtained in PlanetLab when actual link loss rates follow Gilbert process.

even though their loss rates are very diﬀerent. Hence, Tomo
often identiﬁes fewer lossy links. On the contrary, Norm usu-
ally selects more lossy links to justify the loss rate of every
lossy path (the number of links reported by Norm as lossy is
30% more than Sum-Tomo). This is the reason that Norm
gives low precision and high recall. Moreover, the accuracy
results in Figure 2c show that Sum-Tomo can estimate a
loss rate range for lossy links quite well. Speciﬁcally, if a
link is correctly detected by Sum-Tomo as lossy, then with
a high probability (above 95%) its loss rate range includes
the actual loss rate of that link.

The results of the previous algorithms in the Internet2
topology are very similar with the results in the ESNet topol-
ogy (when considering the same fraction of lossy links), and
so they are omitted.

Figure 3 shows the results of the three tomography al-
gorithms in ESNet, when lossy links are selected randomly
and their loss process follows the Gilbert model. Comparing
the results of Figures 2 and 3, note that the precision of all
algorithms does not change much. In addition, the recall of
Tomo remains the same because the binary state of a path
remains the same under these two loss processes. However,
we observe a small decrease (by about 2%) in the recall of
Sum-Tomo and Norm, compared to Figure 2b. With the
Gilbert process, the measured loss rate of a path can be
signiﬁcantly diﬀerent from the actual loss rate of its con-
stituent links, and so the value of α should be higher. This
means that each link that is detected by Sum-Tomo as lossy
can justify more lossy paths, and so Sum-Tomo may fail to
identify some lossy links. Nonetheless, the recall of Sum-

392e
c
n
e
r
e

f
f
i

 

d
e
g
a
r
e
v
A

 0.1

 0.05

 0

-0.05

-0.1

Recall
Precision
Accuracy

t
l

u
s
e
r
 

e
g
a
r
e
v
A

 1

 0.96

 0.92

 0.88

 0.84

 0.8

Recall
Precision
Accuracy

 2

 4

 6

 8  10  12  14  16  18  20

Number of lossy links

 0.2

 0.25

 0.3

 0.35

 0.4

 0.45

 0.5

α

Figure 5: The diﬀerence in the results of Sum-Tomo with
Method-1 versus Method-2 for the selection of α.

Figure 6: The sensitivity of various parameters to the value
of α.

Tomo is still considerably higher than that of its Boolean
counterpart (by up to 10%), while it remains close to the re-
call of Norm. Because the Gilbert process introduces more
error in the path measurements, the accuracy of both Sum-
Tomo and Norm algorithms declines (Figure 2c). However,
because the Analog approach is more sensitive to measure-
ment errors, we see a larger accuracy decrease in Norm (by
about 25%).

Similarly, Figure 4 shows results obtained with the Plan-
etLab topology. Here, the loss process follows the Gilbert
model while the selection of lossy links has the previously
mentioned edge-network bias (α is the same as before). Be-
cause the PlanetLab topology is much larger, we consider up
to 60 lossy links (about 5% of the total number of links in
that network). Figure 4a shows that the precision of Tomo
and Sum-Tomo are close. However, as shown in Figure 4b,
the recall of Sum-Tomo is higher than that of Tomo. This
is again because Tomo selects fewer links to justify lossy
paths (e.g. about 10% fewer links than Sum-Tomo). On the
other hand, the precision of Norm is quite low, as it does
not capture that paths sharing the same lossy link may mea-
sure diﬀerent loss rates. So, it reports about 40% more lossy
links than Sum-Tomo. The accuracy of the loss rate ranges
assigned to lossy links by Sum-Tomo is always higher than
94%.

The width of the loss rate ranges assigned to bad links
depends on the parameter α. When we use the Bernoulli
loss model (i.e. α ≈ 0.3), the width of the resulting loss
rate range is about 50% of the center value. For example,
when the center of the loss rate range is 0.04, the reported
range is from 0.03 to 0.05. With the Gilbert loss model (i.e.
α ≈ 0.5), the width of the loss rate range is about 80% of
the center value.
In comparison to a random selection of lossy links, the
edge-network bias results in a small decrease (between 2-
3%) in precision and recall for Tomo and Sum-Tomo. This is
because fewer paths traverse edge links than core links, and
so in some cases these two greedy tomography algorithms fail
to select the right lossy link because they prefer to select the
most shared link (which is often in the network core).

We now focus on the selection of α, comparing Method-
1 with Method-2. Here, we use the ESNet topology, while
lossy links are selected randomly and the link loss rates fol-

low the Gilbert process. Figure 5 shows the diﬀerences in
precision, recall and accuracy between Method-1 and Method-
2. Method-1 provides better estimates for α, as it relies on
prior measurements in which the ground truth is known.
The diﬀerence between the two methods is negligible when
the number of lossy links is not high (e.g., less than 10%
of all network links). With more lossy links, the recall ob-
tained with Method-2 is worse (by up to 6%), while the
accuracy improves (by at most 5%). The precision remains
roughly the same. The reason is that, when there are many
lossy links, Method-2 over-estimates the value of α since
each bad path is more likely to traverse more than one lossy
link. Then, Sum-Tomo becomes more similar to the Boolean
Tomo algorithm, which yields lower recall, as noted earlier.
On the other hand, Method-2 gives higher accuracy because
the performance ranges become wider as a result of a larger
α.

Finally, we investigate the sensitivity of the precision, re-
call and accuracy metrics to the value of α. Here, we use
the ESNet topology, while losses follow the Bernoulli pro-
cess. We ﬁx the number of lossy links to 10 (i.e. about 10%
of the network’s links are lossy). The value of α resulting
from Method-1 and Method-2 is around 0.3 and 0.4, respec-
tively. We vary α around these two values, from 0.2 to 0.5.
As Figure 6 shows, the accuracy is always improved with
increasing α because the estimated loss rate range becomes
wider. The precision has an increasing trend with α because
each link that is detected as lossy can potentially justify
more bad paths, and so Sum-Tomo detects fewer links as
lossy. For the same reason, the recall drops signiﬁcantly as
we increase α above the value recommended by Method-1 or
Method-2. In fact, when α is suﬃciently high the Sum-Tomo
algorithm behaves similar with the Boolean tomography al-
gorithm Tomo.

In summary, the errors with Sum-Tomo increase as the
number of lossy links increases, their locations are moved
closer to the edge of the network, and their loss rate varies
rapidly over time.

6. EXPERIMENTAL RESULTS

We have conducted extensive experiments on real net-
works to understand the practical issues involved in network
tomography and to examine the behavior of Range Tomog-
raphy algorithms in practice. In fact, the main purpose of

393this section is to show the characteristics of actual congestion
events in Internet paths and to illustrate how the Sum-Tomo
algorithm localizes such events. We ﬁrst present our method
to detect congestion events, and analyze the results of that
detection phase. We then use the Sum-Tomo algorithm to
localize those congestion events.

6.1 Detection of congestion events

We performed our experiments over three real networks:
Internet2, ESNet, and PlanetLab (the same topologies that
were described in Section 5, except that the PlanetLab topol-
ogy here consists of 60 sensors). The measurement data for
Internet2 and ESNet have been provided to us by the cor-
responding network operators, while the measurement data
for PlanetLab were obtained by us running the same mea-
surement tools that are used in Internet2 and ESNet.

The forwarding paths between sensors were measured us-
ing Paris-traceroute [22] every 30 minutes. Because multi-
path forwarding is common in practice (as reported by [26]),
it is necessary to use tools such as Paris-traceroute to avoid
false inference of paths that do not actually exist in the
network. We measured the one-way delay variations and
the loss rate in each path with active measurements (using
OWAMP), sending ten UDP 60-byte packets per second. In
the following results, we analyze a 3-day PlanetLab data set,
a 35-day Internet2 data set, and a 14-day ESNet data set.

Generally speaking, we deﬁne a congestion event as a sig-
niﬁcant increase in the path one-way delays and/or as the
appearance of packet losses. Speciﬁcally, if we observe a
signiﬁcant increase in the path one-way delay for at least k
consecutive packets, relative to the path’s base delay, we de-
tect a congestion event in that path. This approach falsely
detects congestion events in two cases: I) when the forward-
ing path between the two sensors changes (e.g. due to a
routing change or because of an NTP clock adjustment),
or II) when there is a CPU context-switch event in either
sensor, causing a short-term disruption in the measurement
process.
In the former, we observe a level-shift (rising or
falling) in the one-way packet delays. In the latter, we ob-
serve a sudden substantial increase in the one-way packet
delays, followed by a linear decrease towards the base de-
lay. Speciﬁcally, if we observe an increase of at least 80ms
in the one-way delay of two consecutive packets, we detect
the start of a context-switch event; the event lasts until the
delay returns back to its base level9. We preprocess the data
set to remove all detected context-switches and level-shifts
before applying the tomography algorithm.

6.2 Detection results

Table 1 shows the congestion event detection results for
the three data sets. The frequency of level-shifts is quite low
in all three networks (especially, in ESNet), suggesting that
the paths in these networks are stable over several hours.
On the other hand, the frequency of context-switches is rel-
atively high in ESNet and PlanetLab paths, indicating that
sensors are often highly-loaded (Internet2 sensors are much
better in this aspect). The frequency of congestion events
is much higher in Internet paths between PlanetLab hosts,
while this frequency is quite small in ESNet.

Table 1: The results of the detection process on three net-
works. The value in each cell represents the average occur-
rence frequency per day and per path.

Level-shift Context-switch Congestion

Internet2

ESNet

PlanetLab

0.01

0.0002
0.007

0.011
0.48
0.61

0.002
0.0002
0.005

Figure 7a shows the distribution of congestion event dura-
tion in the PlanetLab network. About 95% of the congestion
events last less than 30 seconds. These durations become
even shorter in Internet2 and ESNet (the maximum dura-
tion is 27sec and only 5% of congestion events last more than
20 seconds in those two networks). This result clearly con-
tradicts the assumption made by several tomography meth-
ods that the network conditions (e.g., congestion level at
diﬀerent links) remain constant for long periods of time. It
becomes clear that tomography methods should be able to
produce results based on short-term measurements.

Table 2 summarizes statistics about lossy congestion events
(congestion events with at least one lost packet)10. The
frequency of lossy congestion events is negligible in Inter-
net2 and ESNet, but it is large in PlanetLab paths. More
than 50% of the lossy congestion events include just one lost
packet, while there are at most 4 packet losses in about 80%
of such events (the loss rate is below 1% in about 80% of
lossy congestion events).

It is important to note that several tomography methods
rely on loss rate to infer and localize congestion events. The
previous results imply that most congestion events do not
introduce any packet losses, and if there are losses the loss
rate is often very low (making it diﬃcult to estimate the loss
rate in practice with a limited frequency of probing packets).
For these reasons, we deﬁne congestion events based on one-
way delay variations in the rest of this section.

Table 2: The number and frequency of lossy congestion
events (over all congestion events) in the three data sets.

lossy

#
events
6
0
535

Frequency of
lossy events
0.2%
0%
16.5%

lost

#
packets
1
0
[1-320]

Internet2

ESNet

PlanetLab

Assume we have detected a congestion event in a path
between the kth and lth probing packets. We use the average
one-way delay increase during that period to quantify the
congestion magnitude M ,
M = Pl

i=k (di − dbase)

where dbase is the base one-way delay (estimated as the me-
dian one-way delay of the last 1000 probing packets before
the kth packet). A higher value of M indicates heavier con-
gestion in that path. Figure 7b shows the congestion mag-
nitude distribution in the PlanetLab network. The average
delay increase during half of the congestion events is less

l − k

(7)

9The base delay of a path at time t is set to the median of
the one-way delays among the last 100 packets received in
that path before t.

10Note that we have ignored cases where all packets sent (or
received) over every path to (or from) a speciﬁc host are
dropped because the problem then can be easily associated
with the corresponding sensor and its local-area network.

394(a) CDF of congestion duration

(b) CDF of congestion magnitude

(c) Congestion magnitude vs. congestion dura-
tion

Figure 7: Results of the detection process in PlanetLab.

than 20ms, and it does not exceed 50ms in 90% of those
events. The distribution of M in the other two networks
has the same trend, but the maximum value of M is at
most 60ms there. Finally, Figure 7c shows the magnitude
of congestion events versus their duration.
It is interest-
ing to see that heavier congestion events usually last for
shorter durations, emphasizing again that a practical to-
mography method should be able to work with short-term
measurements.
It should be noted here that network to-
mography methods cannot localize congestion events that
are very short in duration because it is hard to reliably de-
tect such events, and further to estimate their magnitude,
with active measurements. For instance, even if the probing
frequency is 10 packets per second, a congestion event that
lasts for three seconds introducing a loss rate of 1% would
probably not be observed by the probing stream. However,
because the Range Tomography methods do not require a
long measurement history or precise estimates of path per-
formance, they are able to localize congestion events that
last for at least few tens of seconds, as shown in the follow-
ing results.

6.3 Localization results

We run the Sum-Tomo algorithm on the detected conges-
tion events to localize congested links.
Ideally, we expect
that a congested link causes the same congestion event (in
terms of duration and congestion magnitude) to paths that
traverse that link. However, these congestion periods do
not exactly overlap in practice due to lack of perfect clock

synchronization in the measurement hosts. Therefore, we
consider the time interval that covers all overlapping con-
gestion periods as the designated congestion period that the
tomography method analyzes.

We then compute the congestion magnitude M in every
path during that designated period. To distinguish good
paths from bad paths, we set the threshold δ as the minimum
congestion magnitude over all detected congestion events in
that network (the value of δ is around 2ms in practice). The
α parameter is determined based on Method-2, introduced
in Section 5.1. This process results in α=2 in Internet2 and
ESNet, and α=1.2 in PlanetLab.
In our experiments, we
ignore paths containing links with unknown IP addresses
(only happens in the PlanetLab data set).

It is not possible to directly validate the results of the
Sum-Tomo algorithm because we do not have access to the
routers and switches along each path. Instead, we use the in-
direct validation method introduced in [19]. In this method,
the measured paths in each period are divided in two groups,
inference paths and validation paths. Each link should be
included in both sets. The inference paths are then used
to identify congested links and to estimate their congestion
magnitude range using the Sum-Tomo algorithm. Then, we
use the validation paths to examine if the ranges that were
computed based on the inference paths are consistent with
the measured congestion magnitude of the validation paths.
Finally, we measure the validation error as the fraction of
bad links in which the estimated congestion magnitude range

395l

 

)
s
m
n
i
(
 
y
a
e
D
 
y
a
W
e
n
O

 

 100

 80

 60

 40

 20

 0

 0

 5

 10

 15

 20

 25

l

 

)
s
m
n
i
(
 
y
a
e
D
 
y
a
W
e
n
O

 

 100

 80

 60

 40

 20

 0

 0

 5

 10

 15

 20

 25

l

 

)
s
m
n
i
(
 
y
a
e
D
 
y
a
W
e
n
O

 

 100

 80

 60

 40

 20

 0

 0

 5

 10

 15

 20

 25

Time (in seconds)

Time (in seconds)

Time (in seconds)

(a) Delay timeseries of a path traversing a
congested link l1.

(b) Delay timeseries of a path traversing
a congested link l2.

(c) Delay timeseries of a path traversing
both congested links l1 and l2.

Figure 8: The timeseries of one-way packet delays measured in three paths during a congestion event in Internet2. The
delays are relative to the base delay in that path (i.e., they represent one-way delay increase due to queueing). There are two
congested links during this episode. Paths in (a) and (b) traverse two diﬀerent congested links, but the path in (c) traverses
both links. Note that the sum of the delay increase in (a) and (b) results to roughly the same delay increase we observe in
(c).

for a link is not consistent with the measured congestion
magnitude in the corresponding validation path.

Overall, we analyzed 254 congestion events in Internet2,
336 events in ESNet, and 159 events in PlanetLab. In almost
all events, only one congested link was identiﬁed as the root
cause of the congestion event. Speciﬁcally, more than 97%
of congested paths in all three networks include just one
congested link, and only 0.5% of paths contain more than
two congested links (we did not ﬁnd any path with four
congested links in our data sets).

The validation error in all three data sets was zero.

In
addition, the time-series of one-way delay variations in the
inference and validation paths are highly correlated in the
time domain when the corresponding congested paths tra-
verse the same congested link. For instance, the time-series
in Figure 8 show an interesting example of a congestion event
in Internet2 that involves two congested links. The conges-
tion magnitude range assigned to links is mostly below 70ms
in PlanetLab, and mostly less than 40ms in Internet2 and
ESNet.

7. RELATED WORK

We refer the reader to a thorough review [10] for a cover-

age of the prior work until 2004.

In Analog tomography, a typically under-constrained lin-
ear system of equations models the relation between path
and link parameters (assuming that the topology is known).
Techniques from parametric or non-parametric statistical
inference, jointly with additional constraints, assumptions
and optimization objectives, are then used to infer the most
likely values of the link parameters from the measured path
parameters [10, 11]. For instance, Shavitt et al.
[12] es-
timate link delays using a least-squares method, Bu et al.
[13] use expectation-maximization to infer link loss rates,
while Chen et al.
[14] use Fourier domain analysis to infer
link delays. NetScope [15] is a recent method that estimates
link loss rates also considering the observed loss rate vari-
ances. Ghita et al. study the case that diﬀerent links can
be correlated (all other related work assumes independent
links) [16]. The application of Analog tomography in prac-
tice has been rather limited for several reasons. First, they

require accurate end-to-end path measurements, which are
hard to get in short timescales and without intrusive prob-
ing. Second, some link-level parameters may not be sta-
tistically identiﬁable, meaning that diﬀerent assignments of
link-level metrics produce the same statistical distribution
of path measurements [5, 6, 7]. Third, Analog methods can
be computationally intensive [7].

Duﬃeld introduced the Boolean tomography framework
in 2003 [5, 7], while Nguyen and Thiran compared Analog
with Boolean tomography [17]. NetDiagnoser [8] extends
Boolean tomography to multiple sources and destinations.
Kompella et al.
[18] consider a similar approach to detect
“silent failures” in MPLS/IP networks using active measure-
ments between edge routers. Nguyen and Thiran introduce
a Boolean method to infer link state probabilities from mul-
tiple measurements over time, and then using those proba-
bilities to identify congested links [6]. Bayesian approaches
to infer lossy links have also been proposed [19]. The au-
thors in [20] use prior link state probabilities to diagnose the
underlying cause behind the faulty state of links. Barford et
al. have proposed a framework to detect and localize per-
formance anomalies using active probe-based measurements
[21]. To reduce probing overhead, they give an algorithm to
select the paths that should be probed at any point in time.

8. CONCLUSIONS

We proposed a new tomography framework that combines
the best features of Analog and Boolean tomography. Range
tomography estimates a range estimate for each bad link, in-
stead of aiming to infer a point estimate or a binary estimate
for every link. We applied the range tomography framework
in two path performance metric functions (Min and Sum)
and presented an eﬃcient heuristic for each function. The
Min-Tomo algorithm considers only the lowest-performance
link over a path, while the Sum-Tomo algorithm considers
the sum of the performance metrics for all bad links in that
path.

Simulation results show that the proposed tomography
method performs much better than earlier Boolean and Ana-
log techniques in terms of precision, recall and accuracy. For
instance, Sum-Tomo generates up to 35% less false positives

396than the Analog Norm-minimization method, while its false
negative error is less than the Boolean Tomo method by
up to 15%. Moreover, the accuracy of the resulting range
estimates is always high (more than 93%).

We have also applied the Sum-Tomo method in three op-
erational networks to detect and localize congested links and
to estimate their congestion magnitude. According to an in-
direct validation method, the resulting link range estimate
is consistent with the measured path congestion magnitude
in every congestion event we have analyzed. The experimen-
tal results also emphasize that congestion events in Internet
paths are often short-lived, and so any practical tomography
methods should be accurate even if the path measurements
result from few probes and they are error-prone. Finally,
at least for the networks we measured in this study, we of-
ten see only one bad link during any congestion event, and
rarely more than 2-3 bad links.

Acknowledgements
We are grateful to Partha Kanuparthy for his help with the
detection logic and data management. We also thank Ja-
son Zurawski from Internet2, and Joe Metzger, Brian Tier-
ney and Andrew Lake from ESnet for providing us with the
OWAMP data sets. We are also grateful to the anonymous
reviewers and our “shepherd”, Matthias Grossglauser, for
their constructive comments.

This research was supported by the U.S. Department of

Energy under grant number DE-FG02-10ER26021.

9. REFERENCES
[1] R. Caceres, N.G. Duﬃeld, J. Horowitz, D. Towsley.
Multicast-Based Inference of Network Internal Loss
Characteristics. IEEE Trans. on Information Theory,
45(7), 2462-2480, 1999.

[2] M. Coates, R. Nowak. Network loss inference using

unicast end-to-end measurement, In Proc. ITC Conf.
IP Traﬃc, Modeling and Management, 2000.

[3] Y. Zhang, N. Duﬃeld, V. Paxson, S. Shenker. On the

Constancy of Internet Path Properties. In ACM
SIGCOMM Workshop on Internet Measurement, 2001.
[4] M. Roughan. Fundamental Bounds on the Accuracy of

Network Performance Measurements. In ACM
SIGMETRICS, 2005.

[5] N. Duﬃeld. Simple network performance tomography.

In Proc. ACM IMC, 2003.

[6] H. Nguyen, and P. Thiran. The Boolean Solution to

the Congested IP Link Location Problem: Theory and
Practice. In Proc. IEEE INFOCOM, 2007.

[7] N. Duﬃeld. Network Tomography of Binary Network

Performance Characteristics. In IEEE Trans.
Information Theory, 52, 2006.

[8] A. Dhamdhere, R. Teixeira, C. Dovrolis, C. Diot.

NetDiagnoser: Troubleshooting network
unreachabilities using end-to-end probes and routing
data. In Proc. ACM CoNEXT, 2007.

[9] M. R. Garey, and D. S. Johnson. Computers and

Intractability: A Guide to the Theory of
NP-Completeness. W. H. Freeman and Co., 1979.
[10] R. Castro, M. Coates, G. Liang, R. Nowak, B. Yu.

Network Tomography: Recent Developments.
Statistical Science, 19(3):499-517, 2004.

[11] R. Caceres, N. Duﬃeld, S. Moon, D. Towsley.

Inference of Internal Loss Rates in the MBone. In
Proc. IEEE Global Internet, 1999.

[12] Y. Shavitt, X. Sun, A. Wool, B. Yener. Computing the

unmeasured: An algebraic approach to internet
mapping. In Proc. IEEE INFOCOM, 2001.

[13] T. Bu, N. Duﬃeld, F. L. Presti, D. Towsley. Network

tomography on general topologies. In Proc. ACM
SIGMETRICS, 2002.

[14] A. Chen, J. Cao, T. Bu. Network tomography:

Identiﬁability and fourier domain estimation. In Proc.
IEEE INFOCOM, 2007.

[15] D. Ghita, H. Nguyen, M. Kurant, K. Argyraki, P.

Thiran. Netscope: Practical Network Loss
Tomography. In Proc. IEEE INFOCOM, 2010.

[16] D. Ghita, K. Argyraki, P Thiran. Network

Tomography on Correlated Links. In Proc. ACM IMC,
2010.

[17] H. X. Nguyen, and P. Thiran. Binary versus analogue
path monitoring in IP networks. In LNCS, Vol. 3431,
Jan. 2005, p. 97.

[18] R. R. Kompella, J. Yates, A. Greenberg, A. C.

Snoeren. Detection and Localization of Network
Blackholes. In Proc. IEEE INFOCOM, 2007.

[19] V. Padmanabhan, L. Qiu, H. Wang. Server-based
Inference of Internet Link lossiness. In Proc. IEEE
INFOCOM, 2003.

[20] S. Kandula, D. Katabi, J.-P. Vasseur. Shrink: A Tool

for Failure Diagnosis in IP Networks. In Proc. ACM
SIGCOMM MineNet Workshop, 2005.

[21] P. Barford, N. Duﬃeld, A. Ron, and J. Sommers.

Network Performance Anomaly Detection and
Localization. In Proc. IEEE INFOCOM, 2009.

[22] B. Augustin et al. Avoiding traceroute anomalies with

Paris traceroute, In Proc. ACM IMC, 2006.

[23] H. H. Song, L. Qiu, Y. Zhang. NetQuest: A Flexible
Framework for Large-Scale Network Measurement. In
Proc. ACM SIGMETRICS, 2006.

[24] H. X. Nguyen, and P. Thiran. Network Loss Inference
with Second Order Statistics of End-to-End Flows. In
Proc. ACM IMC, 2007.

[25] Y. Zhao, Y. Chen, D. Bindel, Towards Unbiased

End-to-End Network Diagnosis, In Proc. ACM
SIGCOMM, 2006.

[26] M. Luckie, A. Dhamdhere, K. Claﬀy, D. Murrell.
Measured impact of crooked traceroute. In ACM
SIGCOMM CCR, Vol. 41, Issue 1, 2011.

397