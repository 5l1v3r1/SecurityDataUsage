Differential Privacy with Bounded Priors:

Reconciling Utility and Privacy in Genome-Wide

Association Studies

Florian Tramèr

Zhicong Huang

Jean-Pierre Hubaux

School of IC, EPFL

ﬁrstname.lastname@epﬂ.ch

Erman Ayday

∗

Computer Engineering Department

Bilkent University

erman@cs.bilkent.edu.tr

ABSTRACT
Diﬀerential privacy (DP) has become widely accepted as a
rigorous deﬁnition of data privacy, with stronger privacy
guarantees than traditional statistical methods. However,
recent studies have shown that for reasonable privacy bud-
gets, diﬀerential privacy signiﬁcantly aﬀects the expected
utility. Many alternative privacy notions which aim at relax-
ing DP have since been proposed, with the hope of providing
a better tradeoﬀ between privacy and utility.

At CCS’13, Li et al. introduced the membership privacy
framework, wherein they aim at protecting against set mem-
bership disclosure by adversaries whose prior knowledge is
captured by a family of probability distributions. In the con-
text of this framework, we investigate a relaxation of DP, by
considering prior distributions that capture more reasonable
amounts of background knowledge. We show that for diﬀer-
ent privacy budgets, DP can be used to achieve membership
privacy for various adversarial settings, thus leading to an
interesting tradeoﬀ between privacy guarantees and utility.
We re-evaluate methods for releasing diﬀerentially private
χ2-statistics in genome-wide association studies and show
that we can achieve a higher utility than in previous works,
while still guaranteeing membership privacy in a relevant
adversarial setting.

Categories and Subject Descriptors
K.4.1 [Computer and Society]: Public Policy Issues—
Privacy; C.2.0 [Computer-Communication Networks]:
General—Security and protection; J.3 [Life and Medical
Sciences]: Biology and genetics

Keywords
Diﬀerential Privacy; Membership Privacy; GWAS; Genomic
Privacy; Data-Driven Medicine

∗Part of this work was done while the author was at EPFL.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813610.

1.

INTRODUCTION

The notion of diﬀerential privacy, introduced by Dwork
et al. [4, 5], provides a strong and rigorous deﬁnition of
data privacy. A probabilistic mechanism A is said to sat-
isfy -diﬀerential privacy (-DP), if for any two neighboring
datasets T and T (cid:48), the probability distribution of the out-
puts A(T ) and A(T (cid:48)) diﬀer at most by a multiplicative fac-
tor e. Depending on the deﬁnition of neighboring datasets,
we refer to either unbounded -DP or bounded -DP. Informally,
satisfying diﬀerential privacy ensures that an adversary can-
not tell with high conﬁdence whether an entity t is part of
a dataset or not, even if the adversary has complete knowl-
edge over t’s data, as well as over all the other entities in
the dataset. The relevance of such a strong adversarial set-
ting has been put into question because it seems unlikely,
in a practical data setting, for an adversary to have such a
high certainty about all entities. Alternative privacy deﬁni-
tions such as diﬀerential-privacy under sampling [13], crowd-
blending privacy [8], coupled-worlds privacy [2], outlier pri-
vacy [15], -privacy [16], or diﬀerential identiﬁability [12]
relax the adversarial setting of DP, with the goal of achiev-
ing higher utility.

This line of work is partially in response to the ﬂow of re-
cent results, for example in medical research, which show
that satisfying diﬀerential privacy for reasonable privacy
budgets leads to an signiﬁcant drop in utility. For instance,
Fredrikson et al. [6] investigate personalized warfarin dosing
and demonstrate that for privacy budgets eﬀective against a
certain type of inference attacks, satisfying DP exposes pa-
tients to highly increased mortality risks. Similarly, studies
on privacy in genome-wide association studies (GWAS) [10,
19, 22] consider diﬀerential privacy as a protective measure
against an inference attack discovered by Homer et al. [9,
20]. These works show that for reasonably small values of ,
the medical utility is essentially null under DP, unless there
is an access to impractically large patient datasets.

Membership Privacy.

We present an alternative characterization of diﬀerential
privacy, by considering weaker adversarial models in the con-
text of the positive membership-privacy (PMP) framework
introduced by Li et al. [14]. Their privacy notion aims at
preventing positive membership disclosure, meaning that an
adversary should not be able to signiﬁcantly increase his be-
lief that an entity belongs to a dataset. The privacy guar-
antee is with respect to a distribution family D, that cap-
tures an adversary’s prior knowledge about the dataset. If

1286a mechanism A satisﬁes γ-positive membership-privacy un-
der a family of distributions D, denoted (γ, D)-PMP, then
any adversary with a prior in D has a posterior belief upper-
bounded in terms of the prior and the privacy parameter
γ. The power of this framework lies in the ability to model
diﬀerent privacy notions, by considering diﬀerent families of
distributions capturing the adversary’s prior knowledge. For
instance, Li et al. show that -DP is equivalent to e-PMP
under a family of ‘mutually independent distributions’ (de-
noted either DI for unbounded -DP or DB for bounded -DP).
Similarly, privacy notions such as diﬀerential identiﬁability
or diﬀerential-privacy under sampling can also be seen as
instantiations of the PMP framework for particular distri-
bution families.

Bounded Adversarial Priors.

Our approach at relaxing the adversarial setting of DP is
based on the observation that the families of mutually inde-
pendent distributions DI and DB contain priors that assign
arbitrarily high or low probabilities to all entities. This cap-
tures the fact that DP protects the privacy of an entity, even
against adversaries with complete certainty about all other
entities in the dataset, as well as some arbitrary (but not
complete) certainty about the entity itself.

A natural relaxation we consider is to limit our adversar-
ial model to mutually independent distributions that assign
bounded prior probabilities to each entity. More formally, for
constants 0 < a ≤ b < 1, we concentrate on adversaries with
priors pt ∈ [a, b] ∪ {0, 1} about the presence of each entity t
in the dataset. In this setting, there are some entities (called
known entities) for which the adversary knows apriori with
absolute certainty whether they are in the dataset or not.
For the remaining entities however (called uncertain enti-
ties), the adversary has some level of uncertainty about the
entity’s presence or absence from the dataset. In a sense, we
consider what privacy guarantees a mechanism can provide
for an uncertain entity, if the adversary has some limited
amount of background knowledge about that entity. In con-
trast, DP asks for something much stronger, as it provides
the same privacy guarantees for an entity, even if the ad-
versary already has an arbitrarily high certainty about the
entity’s presence in the dataset.

Our main result shows that for a ﬁxed privacy parameter
, satisfying e-PMP for adversaries with bounded priors re-
quires less data perturbation than for the general families
DB and DI . More precisely, we prove that although -DP
is necessary to guarantee e-PMP for DI and DB (see [14]),
a weaker level of (cid:48)-DP (where (cid:48) > ) suﬃces to satisfy e-
PMP if the priors are bounded. Therefore, we introduce an
alternative privacy-utility tradeoﬀ, in which the data pertur-
bation, and the utility loss, depend on the range of priors for
which we guarantee a given level of PMP. This leads to an
interesting model for the selection of the DP privacy param-
eter, in which we ﬁrst identify a relevant adversarial setting
and corresponding level of PMP, and then select the value 
such that these speciﬁc privacy guarantees hold.

Let’s consider an interesting sub-case of our model of
bounded prior distributions, where we let a get close to b;
this corresponds to a setting where an adversary’s prior be-
lief about an entity’s presence in the dataset tends to uni-
form, for those entities whose privacy is not already breached
apriori. Although this adversarial model seems simplistic,
we argue that certain relevant privacy threats, such as the

Figure 1: Level of -DP guaranteeing 2-PMP for the family
of mutually independent distributions with priors bounded
between a and b.

attack on genomic studies by Homer et al. [9, 20], can be
seen as particular instantiations of it. We show that pro-
tecting against such adversaries is, quite intuitively, much
easier than against adversaries with unbounded priors. In
Figure 1, we illustrate how the DP budget  evolves, if our
goal is to satisfy 2-PMP for priors ranging from a uniform
belief of 1
2 for each uncertain entity, to a general unbounded
prior (DB or DI ). The ﬁgure should be read as follows: If
the priors are arbitrary (pt ∈ [0, 1]), then 2-PMP is guaran-
teed by satisfying (ln 2)-DP. If the priors are uniformly 1
2 ,
then satisfying (ln 3)-DP suﬃces. Note that for a prior of 1
2 ,
the deﬁnition of 2-PMP (see Deﬁnition 5) guarantees that
the adversary’s posterior belief that an uncertain entity is
in the dataset is at most 3
4 .

Result Assessment and Implications.

To assess the potential gain in utility of our relaxation, we
focus on a particular application of DP, by re-evaluating the
privacy protecting mechanisms in genome-wide association
studies [10, 19, 22] for the release of SNPs with high χ2-
statistics. Our results show that, for a bounded adversarial
model, we require up to 2500 fewer patients in the study, in
order to reach an acceptable tradeoﬀ between privacy and
medical utility. As patient data is usually expensive and
hard to obtain, this shows that a more careful analysis of
the adversarial setting in a GWAS can signiﬁcantly increase
the practicality of known privacy preserving mechanisms.

As our theoretical results are not limited to the case of
genomic studies, we believe that our characterization of DP
for bounded adversarial models could be applied to many
other scenarios, where bounded- or unbounded-DP has been
considered as a privacy notion.

2. NOTATIONS AND PRELIMINARIES

We will retain most of the notation introduced for the
membership-privacy framework in [14]. The universe of en-
tities is denoted U. An entity t ∈ U corresponds to a physical
entity for which we want to provide some privacy-protection

22.22.42.62.83exp(ǫ)[12,12][38,58][14,34][18,78][0,1][a,b]1287A
U
t
T
D

T

D
DI
DB

Da

γ

List of symbols

A privacy preserving mechanism
The universe of entities
An entity in the universe U
A subset of entities in U that make up the dataset
A probability distribution over 2U , representing
the prior belief of some adversary about T
A random variable drawn from D (the adversary’s
prior belief about T )
A set of probability distributions
The set of mutually independent distributions
The set of bounded mutually independent distri-
butions
priors in [a, b] ∪ {0, 1} to all entities
Equivalent to D[a,a]
Privacy parameter for DP
Privacy parameter for PMP

D[a,b] A subset of D, in which all distributions assign

guarantees. A dataset is generated from the data associ-
ated with a subset of entities T ⊆ U. By abuse of notation,
we will usually simply denote the dataset as T .
In order
to model an adversary’s prior belief about the contents of
the dataset, we consider probability distributions D over 2U
(the powerset of U). From the point of view of the adver-
sary, the dataset is a random variable T drawn from D.
Its prior belief that some entity t is in the dataset is then
given by PrD[t ∈ T].
In order to capture a range of ad-
versarial prior beliefs, we consider a family of probability
distributions. We denote a set of probability distributions
by D. Each distribution D ∈ D corresponds to a particular
adversarial prior we protect against. We denote a proba-
bilistic privacy-preserving mechanism as A. On a particular
dataset T , the mechanism’s output A(T ) is thus a random
variable. We denote by range(A) the set of possible values
taken by A(T ), for any T ⊆ U .
2.1 Differential Privacy

Diﬀerential privacy provides privacy guarantees that de-
pend solely on the privacy mechanism considered, and not on
the particular dataset to be protected. Informally, DP guar-
antees that an entity’s decision to add its data to a dataset
(or to remove it) does not signiﬁcantly alter the output dis-
tribution of the privacy mechanism.

Deﬁnition 1 (Diﬀerential Privacy [4, 5]). A mechanism
A provides -diﬀerential privacy if and only if for any two
datasets T1 and T2 diﬀering in a single element, and any
S ⊆ range(A), we have

Pr [A(T1) ∈ S] ≤ e · Pr [A(T2) ∈ S] .

(1)

Note that the above deﬁnition relies on the notion of
datasets diﬀering in a single element, also known as neigh-
boring datasets. There exist two main deﬁnitions of neigh-
boring datasets, corresponding to the notions of unbounded
and bounded diﬀerential-privacy.

Deﬁnition 2 (Bounded DP [4]). In bounded diﬀerential-
privacy, datasets T1 and T2 are neighbors if and only if
|T1| = |T2| = k and |T1 ∩ T2| = k − 1. Informally, T1 is
obtained from T2 by replacing one data entry by another.

Deﬁnition 3 (Unbounded Diﬀerential-Privacy [5]). In un-
bounded diﬀerential-privacy, datasets T1 and T2 are neigh-
bors if and only if T1 = T2 ∪ {t} or T1 = T2 \ {t}, for some
entity t. Informally, T1 is obtained by either adding to, or
removing an data entry from T2.

In this work, we consider two standard methods to achieve
-DP, the so-called Laplace and exponential mechanisms.
We ﬁrst introduce the sensitivity of a function f : 2U → Rn;
it characterizes the largest possible change in the value of f ,
when one data element is replaced.

Deﬁnition 4 (l1-sensitivity [5]). The l1-sensitivity of a
function f : 2U → Rn is ∆f = maxT1,T2 ||f (T1) − f (T2)||1,
where T1 and T2 are neighboring datasets.

Laplace Mechanism.

If the mechanism A produces outputs in Rn, the most
straightforward method to satisfy DP consists in perturbing
the output with noise drawn from the Laplace distribution.
Let A be a mechanism computing a function f : 2U → Rn.
Then, if on dataset T , A outputs f (T )+µ, where µ is drawn
from a Laplace distribution with mean 0 and scale ∆f
 , then
A satisﬁes -diﬀerential privacy [5].

Exponential Mechanism.

If A does not produce a numerical output, the addition
of noise usually does not make sense. A more general mech-
anism guaranteeing -DP consists in deﬁning a score func-
tion q : T × range(A) → R that assigns a value to each
input-output pair of A. On a dataset T , the exponential
mechanism samples an output r ∈ range(A) with probabil-
ity proportional to exp( q(T,r)
2∆q ), which guarantees -DP [17].
2.2 Positive Membership-Privacy

In this subsection, we give a review of the membership-
privacy framework from [14] and its relation to diﬀerential-
privacy. Readers familiar with this work can skip directly
to Section 3, where we introduce and discuss our relaxed
adversarial setting.

The original membership-privacy framework is comprised
of both positive and negative membership-privacy. In this
work, we are solely concerned with positive membership-
privacy (PMP). This notion protects against a type of re-
identiﬁcation attack called positive membership disclosure,
where the output of the mechanism A signiﬁcantly increases
an adversary’s belief that some entity belongs to the dataset.
Adversaries are characterized by their prior belief over the
contents of the dataset T . A mechanism A is said to satisfy
positive membership-privacy for a given prior distribution, if
after the adversary sees the output of A, its posterior belief
about an entity belonging to a dataset is not signiﬁcantly
larger than its prior belief.

Note that although diﬀerential privacy provides seemingly
strong privacy guarantees, it does not provide PMP for ad-
versaries with arbitrary prior beliefs. It is well known that
data privacy against arbitrary priors cannot be guaranteed if
some reasonable level of utility is to be achieved. This fact,
known as the no-free-lunch-theorem, was ﬁrst introduced by
Kifer and Machanavajjhala [11], and reformulated by Li et
al. [14] as part of their framework. We now give the formal
deﬁnition of γ-positive membership-privacy under a family
of prior distributions D, which we denote as (γ, D)-PMP.

1288Deﬁnition 5 (Positive Membership-Privacy [14]). A mech-
anism A satisﬁes γ-PMP under a distribution family D,
where γ ≥ 1, if and only if for any S ⊆ range(A), any
distribution D ∈ D, and any entity t ∈ U , we have

PrD|A [t ∈ T | A(T) ∈ S] ≤ γ PrD [t ∈ T]
PrD [t /∈ T] .
PrD|A [t /∈ T | A(T) ∈ S] ≥ 1

γ

(2)

(3)

By some abuse of notation, we denote by S the event
A(T) ∈ S and by t the event t ∈ T. When D and A are
obvious from context, we reformulate (2), (3) as

Pr [t | S] ≤ γ Pr [t]
Pr [¬t | S] ≥ 1
γ

Pr [¬t] .

Together, theses inequalities are equivalent to
γ − 1 + Pr[t]

Pr [t | S] ≤ min

γ Pr[t],

γ

(cid:18)

(4)

(5)

.

(6)

(cid:19)

The privacy parameter γ in PMP is somewhat analogous
to the parameter e in DP (we will see that the two pri-
vacy notions are equivalent for a particular family of prior
distributions). Note that the smaller γ is, the closer the
adversary’s posterior belief is to its prior belief, implying a
small knowledge gain. Thus, the strongest privacy guaran-
tees correspond to values of γ close to 1.

Having deﬁned positive membership-privacy, we now con-
sider eﬃcient methods to guarantee this notion of privacy,
for various distribution families. A simple suﬃcient condi-
tion on the output of the mechanism A, which implies PMP,
is given by Li et al. in the following lemma.
Lemma 1 ([14]). If for any distribution D ∈ D, any output
S ⊆ range(A) and any entity t for which 0 < PrD[t] < 1,
the mechanism A satisﬁes

Pr[S | t] ≤ γ · Pr[S | ¬t] ,

then A provides (γ, D)-PMP.

Notice the analogy to diﬀerential privacy here, in the sense
that the above condition ensures that the probabilities of A
producing an output, given the presence or absence of a
particularly data entry, should be close to each other.

Relation to Differential Privacy.

One of the main results of [14] shows that diﬀerential pri-
vacy is equivalent to PMP under a particular distribution
family. We will be primarily concerned with bounded DP,
as it is the privacy notion generally used for the genome-
wide association studies we consider in Section 4. Our main
results also apply to unbounded DP and we discuss this re-
lation in Section 5. Before presenting the main theorem
linking the two privacy notions, we introduce the necessary
distribution families.

Deﬁnition 6 (Mutually-Independent Distributions (MI) [14]).
The family DI contains all distributions characterized by as-
signing a probability pt to each entity t such that the proba-
bility of a dataset T is given by

Pr[T ] =

(1 − pt) .

(7)

(cid:89)

t∈T

pt ·(cid:89)

t /∈T

Deﬁnition 7 (Bounded MI Distributions (BMI) [14]). A
BMI distribution is the conditional distribution of a MI dis-
tribution, given that all datasets with non-zero probability
have the same size. The family DB contains all such distri-
butions.

The following result, used in the proof of Theorem 4.8 in
[14] will be useful when we consider relaxations of the family
DB in Section 3.
Lemma 2 ([14]). If A satisﬁes -bounded DP, then for any
D ∈ DB we have

Pr[S | t]
Pr[S | ¬t]

≤ e .

Note that together with Lemma 1, this result shows that -
bounded diﬀerential-privacy implies e-positive membership-
privacy under DB. Li et al. prove that the two notions are
actually equivalent.
Theorem 1 ([14]). A mechanism A satisﬁes -bounded DP
if and only if it satisﬁes (e, DB)-PMP.

This equivalence between -bounded DP and e-PMP un-
der DB will be the starting point of our relaxation of dif-
ferential privacy. Indeed, we will show that for certain sub-
families of DB, we can achieve e-PMP even if we only pro-
In this sense,
vide a weaker level of diﬀerential privacy.
we will provide a full characterization of the relationship
between the privacy budget of DP and the range of prior
beliefs for which we can achieve e-PMP.

3. PMP FOR BOUNDED PRIORS

The result of Theorem 1 provides us with a clear charac-
terization of positive membership-privacy under the family
DB. We now consider the problem of satisfying PMP for dif-
ferent distribution families. In particular, we are interested
in protecting our dataset against adversaries weaker than
those captured by DB, meaning adversaries with less back-
ground knowledge about the dataset’s contents. Indeed, as
the prior belief of adversaries considered by DP has been ar-
gued to be unreasonably strong for most practical settings,
our goal is to consider a restricted adversary, with a more
plausible level of background knowledge.

One reason to consider a weaker setting than DP’s adver-
sarial model, is that mechanisms that satisfy DP for small
values of  have been shown to provide rather disappointing
utility in practice. Examples of studies, where DP oﬀers a
poor privacy-utility tradeoﬀ, are numerous in medical appli-
cations such as genome-wide association studies [10, 19, 22]
or personalized medicine [6].
Indeed, many recent results
have shown that the amount of perturbation introduced by
appropriate levels of DP on such datasets renders most sta-
tistical queries useless. We will show that when considering
more reasonable adversarial settings, we can achieve strong
membership-privacy guarantees with less data perturbation,
thus leading to a possibly better privacy-utility tradeoﬀ.
3.1 A Relaxed Threat Model

As illustrated by Theorem 1, diﬀerential privacy guaran-
tees positive-membership privacy against adversaries with a
prior in DB. Thus, in the context of protection against mem-
bership disclosure, the threat model of diﬀerential privacy
considers adversaries with the following capabilities.

12891. The adversary knows the size of the dataset N .

2. All entities are considered independent, conditioned on

the dataset having size N .

3. There are some entities for which the adversary knows
with absolute certainty whether they are in the dataset
or not (Pr[t] ∈ {0, 1}).

4. For all other entities, the adversary may have an arbi-
trary prior belief 0 < Pr[t] < 1 that the entity belongs
to the dataset.

In our threat model, we relax capability 4). We ﬁrst con-
sider each capability separately and discuss why it is (or is
not) a reasonable assumption for realistic adversaries.

Knowledge of N.

Bounded-DP inherently considers neighboring datasets of
ﬁxed size. It is preferably used in situations where the size
of the dataset is public and ﬁxed, an example being the
genome-wide association studies we discuss in Section 4. In
contrast, unbounded-DP is used in situations where the size
of the dataset is itself private. Our results apply in both
cases (see Section 5 for a discussion of unbounded-DP).

Independence of Entities.

As we have seen in Theorem 1 (and will see in Theo-
rem 3 for unbounded-DP), a diﬀerentially-private mecha-
nism guarantees that an adversary’s posterior belief will be
within a given multiplicative factor of its prior, exactly when
the adversary’s prior is a (bounded) mutually independent
distribution. In this work, we focus on a relaxation of DP
within the PMP framework, and thus model our adversary’s
prior belief as a subfamily of either DB or DI .

Known Entities.

It is reasonable to assume that an adversary may know
with certainty whether some entities belong to the dataset
or not, because these entities either willingly or unwillingly
disclosed their (non)-membership (the adversary itself may
be an entity of the universe). Note that for such entities
with prior 0 or 1, perfect PMP with γ = 1 is trivially satis-
ﬁed, since the adversary’s posterior does not diﬀer from its
prior. As the privacy of these entities is already breached
a priori, the privacy guarantees of A should be considered
only with respect to those entities whose privacy still can
be protected. Because all entities are considered indepen-
dent, we may assume that the adversary knows about some
entities’ presence in the dataset, but that some uncertainty
remains about others.

Unknown Entities.

A distribution D ∈ DB can assign to each uncertain entity
a prior probability arbitrarily close to 0 or 1. This means
that when providing positive membership-privacy under DB,
we are considering adversaries that might have an extremely
high prior conﬁdence about whether each user’s data is con-
tained in the dataset or not. In this sense, the family DB
corresponds to an extremely strong adversarial setting, as
it allows for adversaries with arbitrarily high prior beliefs
about the contents of a dataset.

Yet, while it is reasonable to assume that the adversary
may know for certain whether some entities are part of the
dataset or not, it seems unrealistic for an adversary to have

high conﬁdence about its belief for all entities, a priori. As
we will see, guaranteeing membership privacy for those en-
tities for which an adversary has high conﬁdence a priori
(Pr[t] close to 0 or 1), requires the most data perturbation.
Thus, when protecting against adversaries with priors in DB,
we are degrading our utility in favor of protection for enti-
ties whose membership privacy was already severely com-
promised to begin with.
In our alternative threat model,
we focus on protecting those entities whose presence in the
dataset remains highly uncertain to the adversary prior to
releasing the output of A. As we will see in Section 3.4, our
mechanisms still guarantee some weaker level of protection
against the full set of adversaries with priors in DB.
3.2 Our Results

Our natural relaxation of DP’s adversarial model con-
sists in restricting ourselves to adversaries with a prior belief
about uncertain entities bounded away from 0 and 1. Such
an adversary thus may know for certain whether some en-
tities are in the dataset or not, because they unwillingly or
willingly disclosed this information to the adversary. For the
remaining entities however, the adversary has some minimal
level of uncertainty about the entity’s presence or absence
from the dataset, which appears to be a reasonable assump-
tion to make in practice. We will consider the subfamily of
DB, consisting of all BMI distributions for which the priors
Pr[t] are either 0, 1 or bounded away from 0 and 1. This
distribution family is deﬁned as follows.
Deﬁnition 8 (Restricted1 BMI Distributions). For 0 < a ≤
B ⊂ DB contains all BMI distributions
b < 1, the family D[a,b]
for which Pr[t] ∈ [a, b] ∪ {0, 1}, for all entities t. If a = b,
we simply denote the family as Da
B.

Our goal is to show that in this weaker adversarial setting,
we can guarantee PMP with parameter γ, while satisfying a
weaker form of privacy than (ln γ)-DP.

We ﬁrst show that the adversaries with arbitrarily low
or high priors are, rather intuitively, the hardest to protect
against. More formally, we show that when guaranteeing
(γ, DB)-PMP, inequalities (2) and (3) are only tight for pri-
ors approaching 0 or 1. For each entity t, we can compute
a tight privacy parameter γ(t) ≤ γ, whose value depends
on the prior Pr[t]. When considering an adversary with a
prior belief in D[a,b]
B , we will see that γ(t) < γ for all en-
tities t, which shows that we can achieve tighter positive
membership-privacy guarantees in our relaxed adversarial
model. We formalize these results in the following lemma.
Lemma 3. If a mechanism A satisﬁes (γ, DB)-PMP, then
Pr [t | S] ≤ γ(t) · Pr[t] and Pr [¬t | S] ≥ Pr[¬t]

γ(t) , where

(cid:40)

γ(t) =

1
max{(γ−1) Pr[t]+1,

if Pr[t] ∈ {0, 1}

(γ−1) Pr[t]+1} otherwise.

γ

Proof. If Pr[t] ∈ {0, 1}, γ(t) = 1 and the lemma trivially
holds. If 0 < Pr[t] < 1, Bayes’ theorem gives us

Pr[t | S] =

1

1 + Pr[S|¬t]
Pr[S|t]

.

Pr[¬t]
Pr[t]

(8)

1Although we talk about adversaries with bounded priors,
we use the term restricted instead of bounded here, as DB al-
ready denotes the family of bounded MI distributions in [14].

1290Theorem 2. A mechanism A satisﬁes (γ, D[a,b]
some 0 < a ≤ b < 1, if A satisﬁes -bounded DP, where

B )-PMP, for

(cid:40)

(cid:16) (1−a)γ

e =

min
γ+b−1

1−aγ , γ+b−1

b

(cid:17)

Proof. Recall that satisfying -bounded diﬀerential privacy
is equivalent to satisfying (e, DB)-PMP. Using (11), we want

γ = max

(e − 1)b + 1,

e

(e − 1)a + 1

.

(12)

b

(cid:18)

if aγ < 1,

otherwise .

(cid:19)

Solving for  yields the desired result.

γ

Note that when aγ ≥ 1, the ﬁrst condition of PMP, namely
Pr [t | S] ≤ γ Pr[t] is trivially satisﬁed. Thus, in this case
we have to satisfy only the second condition, Pr [¬t | S] ≤
, which is satisﬁed if γ = (e − 1)b + 1. We thus arrive
Pr[¬t]
at a full characterization of the level of diﬀerential privacy
to satisfy, if we wish to guarantee a certain level of positive
membership-privacy for subfamilies of DB. For a ﬁxed level
of privacy γ and 0 < a ≤ b < 1, protecting against adver-
saries from a family D[a,b]
B will correspond to a weaker level
of diﬀerential privacy, and thus to less perturbation of the
mechanism’s outputs, compared to the distribution family
DB. Therefore, by considering a more restricted adversarial
setting, we could indeed reach a higher utility for a constant
level of protection against positive membership disclosure.

These results lead to the following simple model for the
selection of an appropriate level of diﬀerential privacy, in a
restricted adversarial setting.

Selecting a level of DP
1: Identify a practically signiﬁcant adversarial model cap-

tured by some distribution family D[a,b]
B .

2: Select a level γ of PMP, providing appropriate bounds

on the adversary’s posterior belief.

3: Use Theorem 2 to get the corresponding level of DP.

As an example, assume a PMP parameter of 2 is consid-
ered to be a suitable privacy guarantee. If our adversarial
model is captured by the family DB, then (ln 2)-DP provides
the necessary privacy. However, if a reasonable adversarial
setting is the family D0.5
B , then the same privacy guaran-
tees against membership disclosure are obtained by satisfy-
ing (ln 3)-DP, with signiﬁcantly less data perturbation.
3.3 Selecting the Bounds [a, b] in Practice

Selecting appropriate bounds [a, b] on an adversary’s prior
belief (about an individual’s presence in a dataset) is pri-
mordial for our approach, yet might prove to be a diﬃcult
task in practice. One possibility is to focus on privacy guar-
antees in the presence of a particular identiﬁed adversarial
threat. In Section 4.2, we will consider a famous attack on
genome-wide association studies, and show how we can de-
ﬁne bounds on the adversary’s prior, in the presumed threat
model. Such bounds are inherently heuristic, as they derive
from a particular set of assumptions about the adversary’s
power, that might fail to be met in practice. However, we
will show in Section 3.4, that our methods also guarantee
some (smaller) level of privacy against adversaries whose
prior beliefs fall outside of the selected range.

Finally, another use-case of our approach is for obtaining
upper bounds on the utility that a mechanism may achieve,

Figure 2: Bounds on an adversary’s posterior belief when
satisfying (2, DB)-PMP.

By Theorem 1, we know that providing (γ, DB)-PMP is
equivalent to satisfying (ln γ)-bounded DP. By Lemma 2,
we then get

Pr[t | S] ≤

1

1 + γ−1 1−Pr[t]

Pr[t]

=

Pr[¬t | S] ≥

1

1 + γ Pr[t]
Pr[¬t]

=

γ · Pr[t]

(γ − 1) Pr[t] + 1
Pr[¬t]

(γ − 1) Pr[t] + 1

.

(9)

(10)

From this lemma, we get that γ(t) < γ for all entities
for which 0 < Pr[t] < 1. Thus, as mentioned previously,
(γ, DB)-PMP actually gives us a privacy guarantee stronger
than the bounds (2) and (3), for all priors bounded away
from 0 or 1. To illustrate this, Figure 2 plots the two dif-
ferent bounds on the posterior probability, when satisfying
(2, DB)-PMP.
Let A be a mechanism satisfying (γ, DB)-PMP. If we were
to consider only those distributions in DB corresponding to
prior beliefs bounded away from 0 and 1, then A would
essentially satisfy PMP for some privacy parameter larger
than γ. This privacy gain can be quantiﬁed as follows. From
Lemma 3, we immediately see that if we satisfy (γ, DB)-
PMP, then we also satisfy (γ(cid:48), D[a,b]
(γ − 1)b + 1,

B )-PMP, where

= max

(cid:18)

(cid:19)

. (11)

(cid:48)

γ

γ

t∈U γ(t) = max

(γ − 1)a + 1

As γ(cid:48) < γ, this result shows (quite unsurprisingly) that if
we consider a weaker adversarial model, our privacy guaran-
tee increases. Conversely, we now show that for a ﬁxed pri-
vacy level, the relaxed adversarial model requires less data
perturbation. Suppose we ﬁx some positive membership-
privacy parameter γ. We know that to provide (γ, DB)-
PMP, we have to satisfy (ln γ)-DP. However, our goal here
is to satisfy (γ, D[a,b]
B )-PMP for a tight value of γ. The fol-
lowing theorem shows that a suﬃcient condition to protect
positive membership-privacy against a bounded adversary is
to provide a weaker level of diﬀerential privacy.

00.20.40.60.8100.10.20.30.40.50.60.70.80.91priorposterior  posteriorboundwithγposteriorboundwithγ(t)1291when guaranteeing γ-PMP against a so-called uninformed
adversary.
If the dataset size N and the size of the uni-
verse U are known, such an adversary a priori considers all
individuals as part of the dataset with equal probability N|U| .

3.4 Risk-Utility Tradeoff

We have shown that focusing on a weaker adversary leads
to higher utility, yet we must also consider the increased pri-
vacy risk introduced by this relaxation. Suppose our goal is
to guarantee e-PMP. If we consider the adversarial model
captured by the full family DB, A must satisfy -DP. If we
instead focus on the relaxed family D[a,b]
B , it suﬃces to guar-
antee (cid:48)-DP, where (cid:48) is obtained from Theorem 2.

Now suppose our mechanism satisﬁes (e, D[a,b]

B )-PMP, but
there actually is an entity for which the adversary has a
prior Pr[t] /∈ ([a, b] ∪ {0, 1}). Although our mechanism will
not guarantee that conditions (2) and (3) hold for this en-
tity, a weaker protection against membership disclosure still
holds. Indeed, since our mechanism satisﬁes (cid:48)-DP, it also
satisﬁes (e(cid:48)
, DB)-PMP by Theorem 1, and thus guarantees
that bounds (2) and (3) will hold with a factor of e(cid:48)
, rather
than e. In conclusion, satisfying -DP corresponds to sat-
isfying e-PMP for all entities, regardless of the adversary’s
prior. Alternatively, satisfying (cid:48)-DP is suﬃcient to guaran-
tee e-PMP for those entities for which the adversary has a
bounded prior Pr[t] ∈ [a, b] ∪ {0, 1}, and a weaker level of
e(cid:48)
-PMP for entities whose membership privacy was already
severely compromised to begin with.

3.5 Relation to Prior Work

A number of previous relaxations of diﬀerential privacy’s
adversarial model have been considered. We discuss the re-
lations between some of these works and ours in this section.
A popular line of work considers distributional variants
of diﬀerential privacy, where the dataset is assumed to be
randomly sampled from some distribution known to the ad-
versary. Works on Diﬀerential-Privacy under Sampling [13],
Crowd-Blending Privacy [8], Coupled-Worlds Privacy [2] or
Outlier Privacy [15] have shown that if suﬃciently many
users are indistinguishable by a mechanism, and this mech-
anism operates on a dataset obtained through a robust sam-
pling procedure, diﬀerential privacy can be satisﬁed with
only little data perturbation. Our work diﬀers in that we
make no assumptions on the indistinguishability of diﬀer-
ent entities, and that our aim is to guarantee membership
privacy rather than diﬀerential privacy. Another main dif-
ference is in the prior distributions of the adversaries that we
consider. Previous works mainly focus on the unbounded-
DP case, and thus are not directly applicable to situations
where the size of the dataset is public. Furthermore, pre-
viously considered adversarial priors are either uniform [13,
2] or only allow for a ﬁxed number of known entities [8, 15].
Finally, very few results are known on how to design general
mechanisms satisfying distributional variants of DP. In our
work, we show how diﬀerent levels of DP, for which eﬃcient
mechanisms are known, can be used to guarantee PMP for
various adversarial models. Alternatively, Diﬀerential Iden-
tiﬁability [12] was shown in [14] to be equivalent to PMP
under a family of prior distributions slightly weaker than
the ones we introduce here, namely where all entities have
a prior Pr[t] ∈ {0, β} for some ﬁxed β.

4. EVALUATION

Having provided a theoretical model for the characteri-
zation of DP for adversaries with bounded priors, we now
evaluate the new tradeoﬀ between privacy and utility that
we introduce when considering adversarial models captured
by a family D[a,b]
B . We can view an adversary with a prior
in this family as having complete certainty about the size
of the dataset, as well as some degree of uncertainty about
its contents. Scenarios that nicely ﬁt this model, and have
been gaining a lot of privacy-focused attention recently, are
genome-wide association studies (GWAS). We will use this
setting as a case study for the model we propose for the
selection of an appropriate DP parameter.
4.1 Genome-Wide Association Studies

Let us begin with some genetic background. The human
genome consists of about 3.2 billion base pairs, where each
base pair is composed of two nucleobases (A,C,G or T).
Approximately 99.5% of our genome is common to all hu-
man beings. In the remaining part of our DNA, a single nu-
cleotide polymorphism (SNP) denotes a type of genetic
variation occurring commonly in a population. A SNP typ-
ically consists of a certain number of possible nucleobases,
also called alleles. An important goal of genetic research is
to understand how these variations in our genotypes (our
genetic material), aﬀect our phenotypes (any observable
trait or characteristic, a particular disease for instance).

We are concerned with SNPs that consist of only two alle-
les and occur on a particular chromosome. Each such SNP
thus consists of two nucleobases, one on each chromosome.
An example of a SNP is given in Figure 3. In a given pop-
ulation, the minor allele frequency (MAF) denotes the
frequency at which the least common of the two alleles oc-
curs on a particular SNP.

Figure 3: Example of a SNP. Alice has two G alleles on this
fragment and Bob has one G allele and one A allele.

We use the standard convention to encode the value of
a SNP as the number of minor alleles it contains. As an
example, if a SNP has alleles A and G, and A is the minor
allele, then we encode SNP GG as 0, SNPs AG and GA as 1,
and SNP AA as 2. The MAF corresponds to the frequency
at which SNP values 1 or 2 occur.

Genome-wide association studies (GWAS) are a partic-
ular type of case-control studies. Participants are divided
into two groups, a case group containing patients with a
particular phenotype (a disease for instance) and a control
group, containing participants without the attribute. For
each patient, we record the values of some particular SNPs,
in order to determine if any DNA variation is associated
with the presence of the studied phenotype. If the value of
a SNP appears to be correlated (negatively or positively)
to the phenotype, we say that the SNP is causative, or
associated with the phenotype.

A standard way to represent this information is through a
contingency table for each of the considered SNPs. For a
particular SNP, this table records the number of cases and

1292controls having a particular SNP value. An example of such
a table is given hereafter, for a GWAS involving 100 cases
and 100 controls. From this table, we can, for instance, read
that 70% of the cases have no copy of the minor allele. We
can also compute the MAF of the SNP as 40+2·50

2·200 = 0.35.

SNP value

Cases

Controls

Total

0
1
2

Total

70
10
20
100

40
30
30
100

110
40
50
200

Table 1: Contingency table of one SNP, for a GWAS with
100 cases and 100 controls.

The interested reader may ﬁnd additional information on
genomics as well as on genome privacy and security research
at the community website2 maintained by our group.
4.2 Homer’s Attack and Adversarial Model

The ongoing research on applying diﬀerential privacy to
GWAS has been primarily motivated by an attack proposed
by Homer et al. [9]. In this attack, the adversary is assumed
to have some knowledge about an entity’s genetic proﬁle,
and wants to determine if this entity belongs to the case
group or not. Towards this end, the adversary measures
the distance between the entity’s SNP values and the allele
frequencies reported for the case group, or some reference
population. It has been shown that other aggregate statis-
tics, such as p-values or SNP correlation scores, could be
used to construct similar or even stronger attacks [20].

Unsurprisingly, privacy mechanisms based on DP have
been proposed to counter these attacks [10, 19, 22], as they
guarantee that an entity’s presence in the dataset will not
signiﬁcantly aﬀect the output statistics. However, the adver-
sarial model considered here is quite diﬀerent from the one
DP protects against. Indeed, all these attacks assume some
prior knowledge about an entity’s genomic proﬁle, but not
about the entity’s presence or absence from the case group.
Actually, the adversary makes no assumptions on the pres-
ence or absence of any entity from the case group, and it is
absolutely not assumed to have complete knowledge about
the data of all but one of the entities. This attack thus ap-
propriately ﬁts into our relaxed adversarial setting, where we
consider an adversary with bounded prior knowledge. From
the results of Section 3, we know that protecting member-
ship disclosure against such adversaries can be achieved with
much weaker levels of DP.

2 cases and N

In the following, we consider a genome-wide association
study with N patients. It is generally recommended ([18])
that the number of cases and controls be similar. We thus
focus on studies with N
2 controls. The cases
suﬀer from a particular genetic disease, and the goal of the
study is to ﬁnd associated SNPs by releasing some aggregate
statistics over all participants. We assume that the adver-
sary knows the value N (which is usually reported by the
study). In the adversarial model considered by DP, we would
assume the adversary to have complete knowledge about all
but one of the entities in the case group. We will consider a
weaker setting here, which includes the adversarial model of
Homer’s attack [9]. The adversary is assumed to know the
2https://genomeprivacy.org

identity of the study participants, and possibly the disease
status of some of them, but has no additional information
on whether other entities were part of the case or control
group. In regard to the attacks discussed previously, we will
limit the adversary’s capability of asserting the membership
of an entity to the case group, and thus his disease status.
Suppose the adversary already breached the privacy of a
small number m1 of the cases and m2 of the controls. In this
case, the adversary’s prior belief about some other entity’s
presence in the case group is N/2−m1
. In the following,
N−m1−m2
we assume that m1 ≈ m2 and thus that the adversary’s
prior can be modeled by the family D0.5
B . As we discussed in
Section 3.4, our mechanisms will still provide some smaller
level of security against adversaries with more general priors.
More generally, if we have N1 cases and N2 controls, we
can consider a similar adversarial model with a prior belief
of
4.3 A Simple Counting Query

that an entity belongs to the case group.

N1+N2

N1

We ﬁrst consider a simple counting query. While the fol-
lowing example has little practical signiﬁcance in a GWAS,
it is an interesting and simple toy-example illustrating the
usability and power of the model we derived in Section 3.
Let A and A(cid:48) be mechanisms computing the number of
patients in the case group whose SNP value is 0. Under
bounded-DP, the sensitivity of this query is 1. Suppose we
want to guarantee (γ, DB)-PMP for A, and (γ, D0.5
B )-PMP
for A(cid:48). In the ﬁrst case, this is equivalent to satisfying -DP,
for  = ln(γ). In the bounded adversarial model, we have
shown in Theorem 2 that it is suﬃcient to satisfy (cid:48)-DP, for
an (cid:48) > ln(γ).

To satisfy DP, and therefore PMP, we add Laplacian noise
to the true count value. We deﬁne the utility of our mech-
anism as the precision of the count, after application of the
privacy mechanism. More formally, if the true count is C
and the noisy output count is ˆC, then we are interested in
the expected error E[| ˆC − C|]. When satisfying -DP, we
have that ˆC = C + µ, where µ is drawn from a Laplace
distribution with mean 0 and scale −1. Thus, we have that
(13)

E[| ˆC − C|] = E[|µ|] = 

−1 .

As a concrete example of the diﬀerences in utility be-
tween A and A(cid:48), we vary the PMP parameter γ and plot
the expected error of the count in Figure 4. As we can see,
A(cid:48) gives signiﬁcantly more precise outputs than A, when
the two mechanisms provide the same positive membership-
privacy guarantees in their respective adversarial settings.
Note that for an adversary with prior D0.5
B , and PMP param-
eter λ = 2, seeing the output of A(cid:48) yields a posterior belief
of at most 3
4 , that a particular entity is in the case group.
This simple example shows that by focusing on a bounded
adversarial model, protecting against membership disclosure
can be achieved while retaining signiﬁcantly higher utility,
compared to the original adversarial setting.
4.4 Releasing Causative SNPs

A typical GWAS aims at uncovering SNPs that are associ-
ated with some disease. A standard and simple method con-
sists in computing the χ2-statistics of the contingency table
of each SNP. Assume that the genomes of people participat-
ing in the GWAS are uncorrelated (a necessary assumption
for χ2-statistics). For a SNP unrelated to the disease, we
expect any SNP value to appear in the case group as often

1293function in the exponential mechanism. Algorithm 1 is -
diﬀerentially private [3, 22]. Note that as the number of
output SNPs M grows large, the sampling probabilities tend
to be uniform. Thus, it is not necessarily beneﬁcial to output
more SNPs, in the hope that the SNPs with the highest true
statistics will be output.

Algorithm 1 Diﬀerentially private release of associated
SNPs, using the exponential mechanism [22].
Input: The privacy budget , the sensitivity s of the χ2-

statistic, the number of SNPs M to release.

Output: M SNPs
1: For i ∈ {1, . . . , M(cid:48)}, compute the score qi as the χ2-

statistic of the ith SNP.

has probability proportional to exp(cid:0) ·qi

(cid:1) .

2: Sample M SNPs (without replacement), where SNP i

2·M·s

Johnson and Shmatikov [10] propose a general framework
that performs multiple queries used in typical GWAS and
guarantees diﬀerential privacy. They use the exponential
mechanism with a speciﬁc distance score function. We will
focus on their LocSig mechanism that outputs M signiﬁcant
SNPs similarly to Algorithm 1. The sole diﬀerence is that
they use a diﬀerent score function than the χ2-statistic.

Let the distance-to-signiﬁcance of a contingency table be
deﬁned as the minimal number of SNP values to be modiﬁed,
in order to obtain a contingency table with a p-value or
χ2-statistic deemed as signiﬁcant (beyond some pre-deﬁned
threshold). Their algorithm for outputting M signiﬁcant
SNPs is then the same as Algorithm 1, where the scores
qi are replaced by the distance-to-signiﬁcance score, whose
sensitivity s can easily be seen to be 1.
As noted by Yu et al. [22], computing these distance scores
exactly can be a daunting task for 3 × 2 contingency tables.
They suggest instead to approximate the true distance-to-
signiﬁcance by a greedy approach that only considers edits
introducing a maximal change in the χ2-statistic or p-value.
In our experiments, we follow the same approach.

Both of the mechanisms we discussed are subject to a stan-
dard tradeoﬀ between privacy, utility and dataset size. We
illustrate this tradeoﬀ for Algorithm 1 (see [19] and [22] for
details). The tradeoﬀ between privacy and utility is straight-
forward as the sampling probabilities depend on . For the
dependency on the dataset size, note that by deﬁnition, an
unassociated SNP is expected to have a χ2-statistic of 0, re-
gardless of N (this is the null hypothesis). However, if the
SNP is correlated to the disease status, we can verify that
the value of the χ2-statistic grows linearly with N . Thus,
as N grows, the gap between the χ2-statistics of associated
and unassociated SNPs grows as well. Nevertheless, the sen-
sitivity ∆χ2 remains bounded above by 4. Combining both
observations, we see that the larger N gets, the less probable
it is that the algorithm outputs unassociated SNPs. Thus
Algorithm 1 achieves high utility for very large datasets.

We show that by considering a weaker but practically sig-
niﬁcant adversarial model, we require much less patient data
in order to achieve high medical utility, thus rendering such
privacy protecting mechanisms more attractive and appli-
cable for medical research. Spencer et al. [18] note that a
GWAS with 2000 cases and controls necessitates a budget
of about $2,000,000 for standard genotyping chips. Obtain-
ing an acceptable utility-privacy tradeoﬀ even for reasonably
large studies is thus an interesting goal.

Figure 4: Expected error of the counting query, for privacy
mechanisms A and A(cid:48) satisfying (λ, DB)-PMP and (λ, D0.5
B )-
PMP respectively.

as in the control group. The χ2-statistic measures how much
the true values diverge from this expected null hypothe-
sis. The higher the statistic is, the more likely it is that the
SNP and disease status are correlated. Equivalently, we can
compute the p-values that correspond to the χ2-statistics.
Consider the following generic contingency table for a
SNP, in a GWAS with N
2 controls. The table
should be read as follows. There are α cases with SNP value
0 and β cases with value 1. The total number of patients
with SNP values 0 and 1 are, respectively, m and n.

2 cases and N

SNP value

Cases

0
1
2

α
β

N

2 − α − β

Controls
m − α
n − β

N

2 − m + α − n + β

In a typical GWAS, only SNPs with a MAF larger than
some threshold (e.g. 0.05) are considered. Thus, it is reason-
able to assume that the margins of the contingency table
are positive (m > 0, n > 0, N − m− n > 0). Uhler et al. [19]
show that the χ2-statistic of a SNP is then given by

χ2 =

(2α − m)2

m

(2β − n)2

n

(2α − m + 2β − n)2

N − m − n

.

+

+

Existing Techniques.

Methods for the diﬀerentially-private release of SNPs with
high χ2-statistics have been studied by Uhler et al. [19],
Johnson and Shmatikov [10], and more recently Yu et al.
[22]. When the number of cases and controls are equal,
the sensitivity of the χ2-statistic is
4N
N +2 [19]. For the gen-
eral case where the size of the case and control groups are
not necessarily equal, the χ2-statistic and its sensitivity are
given in [22]. We consider two exponential mechanisms for
outputting M SNPs with high χ2-statistics and satisfying
DP. As noted in [10], the value M of signiﬁcant SNPs (with
a χ2 score above a given threshold) can also be computed in
a diﬀerentially private manner. In the following, we assume
the total number of SNPs in the study to be M(cid:48).

Yu et al. propose a very simple algorithm (Algorithm 1)
that directly uses the χ2-statistics of the SNPs as the score

1.11.21.31.41.51.61.71.81.92024681012λE[|ˆC−C|]  AA′1294(a) γ = 1.3

(b) γ = 1.5

(a) γ = 1.3

(b) γ = 1.5

Figure 5: Utility of mechanisms A and A(cid:48), when outputting
2 SNPs using Algorithm 1 from [22].

Figure 6: Utility of mechanisms A and A(cid:48), when outputting
2 SNPs using the LocSig mechanism from [10].

Results.

We evaluate diﬀerent privacy mechanisms on the GWAS
simulations from [19], obtained from the Hap-Sample simu-
lator [21]. The studies consist of 8532 SNPs per participant,
typed on chromosomes 9 and 13 using the AFFY 100k ar-
ray. There are two causative SNPs with an additive eﬀect.
We consider mechanisms that use either Algorithm 1 or the
LocSig mechanism to output 2 SNPs. As a measure of util-
ity, we use the probability (averaged over 1000 runs) that a
mechanism outputs either 1 or both of the causative SNPs.
We do not compare the mechanisms from Yu et al. and
Johnson and Shmatikov directly (see [22] for a full compar-
ison). Instead, we evaluate how the utility of these mecha-
nisms behave, for a bounded adversarial model close to those
models used in the attacks we described in Section 4.2. To
this end, we ﬁx a level γ of positive membership-privacy and
consider mechanisms that protect against arbitrary priors in
DB (equivalent to (ln γ)-DP) or bounded priors in D0.5
B (cor-
responds to a weaker level of DP).
We begin with two privacy mechanisms A and A(cid:48) that
use Algorithm 1 to release 2 SNPs and satisfy PMP un-
B , respectively. For datasets of sizes N ∈
der DB and D0.5
{5000, 7500, 10000} and PMP parameters γ ∈ {1.3, 1.5}, we
compare the utility of A and A(cid:48), and display the results in
Figure 5. We see that for a ﬁxed level of PMP, the bounded
adversarial model leads to signiﬁcantly higher utility. Con-
sider the results for γ = 1.5. Mechanism A, which satisﬁes
(1.5, DB)-PMP, requires at least 10000 patients to achieve
signiﬁcant utility. Even in such a large study, the mechanism
fails to output any of the causative SNPs in about 25% of
the experiments. For A(cid:48), which satisﬁes (1.5, D0.5
B )-PMP, we
achieve a better utility with only 7500 patients, and quasi-
perfect utility for 10000 patients. By focusing on a more
reasonable adversarial threat, we thus achieve a good trade-
oﬀ between privacy and utility, for much smaller datasets.
This is as an attractive feature for medical research, where
large patient datasets are typically expensive to obtain.
We now consider two privacy mechanisms A and A(cid:48) that
use the LocSig mechanism to release 2 SNPs. To compute
the distance scores, we ﬁx a threshold of 10−10 on the p-
values, such that exactly 2 SNPs reach this threshold. As
before, the mechanisms satisfy positive membership-privacy
under DB and D0.5
In our particular exam-
ple, LocSig provides better results than Algorithm 1, and
we actually achieve similar utility for smaller datasets. For
datasets of sizes N ∈ {1500, 2000, 2500} and PMP parame-
ters γ ∈ {1.3, 1.5}, we compare the utility of A and A(cid:48), and
we display the results in Figure 6.

B , respectively.

(a) M = 1

(b) M = 3

Figure 7: Utility of mechanisms A and A(cid:48), when outputting
M SNPs using LocSig [10] with γ = 1.5.

Again, there is a signiﬁcant improvement in utility if we
consider a bounded adversarial model. Although the Loc-
Sig mechanism yields higher accuracy than the exponential
method from Algorithm 1 in this case, we re-emphasize that
computing the distance scores has a much higher complex-
ity than the computation of the χ2-statistics [22]. Deciding
upon which method to use in practice is thus subject to a
tradeoﬀ between utility and computational cost.

Alternatively, we could consider increasing our utility by
releasing M > 2 SNPs. However, as the exponential mecha-
nisms we considered associate probabilities proportional to
M to each SNP, it is unclear whether we should expect
higher utility by increasing M . Obviously, if we were to let
M approach the total number of SNPs, the recall would be
maximized. Hence, we also consider the precision (ratio of
output SNPs that are signiﬁcant). In Figure 7, we evaluate
the utility of LocSig with γ = 1.5, for M = 1 and M = 3.
We see that for M = 3, the utility is worse than for M = 2,
therefore conﬁrming that the increased data perturbation
eliminates the potential gain in recall. Also, in this case the
precision is naturally upper bounded by 2
3 . An interesting
tradeoﬀ is given by selecting M = 1. Although recall can
2 , we see that for small datasets (N ≤ 2000),
not exceed 1
the utility actually is higher than for M = 2.

Finally, we compare the privacy-utility tradeoﬀ for a range
of bounds [a, b] on the adversary’s prior belief. In Figure 8,
we display the probability that Algorithm 1 outputs at least
one or both of the causative SNPs in a GWAS with N =
7500, while providing PMP with γ = 1.5. As we can see,
even if the considered adversary has only a small degree of
a priori uncertainty about an individual’s presence in the
dataset, we still obtain a signiﬁcant gain in utility compared
to the setting where the adversary’s prior is unbounded.

5000 7500 1000000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPs5000 7500 1000000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPs15002000250000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPs15002000250000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPs15002000250000.250.50.751SampleSizeProbability  Areturned1associatedSNPA′returned1associatedSNP15002000250000.250.50.751SampleSizeProbability  Areturned1associatedSNPAreturnedbothassociatedSNPsA′returned1associatedSNPA′returnedbothassociatedSNPs1295In this deﬁnition, we consider only neighboring datasets
obtained by adding a new entity (and not by removing an
entity). Note that satisfying -unbounded DP trivially im-
plies -positive unbounded-DP.

For this deﬁnition, the results we obtained for bounded-
DP can be applied rather straightforwardly to (positive)
unbounded-DP. Li et al. [14] provide results analogous to
Lemma 2 and Theorem 1, by replacing the family DB, by
the family DI of mutually-independent distributions.
Lemma 4 ([14]). If A satisﬁes -positive unbounded DP,
then for any D ∈ DI we have Pr[S|t]
Theorem 3 ([14]). A mechanism A satisﬁes -positive un-
bounded DP if and only if it satisﬁes (e, DI )-PMP.

Pr[S|¬t] ≤ e .

From here on, our analysis from Section 3 can be directly
applied to the case of unbounded-DP. We ﬁrst deﬁne a family
of bounded prior distributions.
Deﬁnition 10 (Restricted MI Distributions). For 0 < a ≤
⊂ DI contains all MI distributions
b < 1, the family D[a,b]
for which Pr[t] ∈ [a, b] ∪ {0, 1}, for all entities t. If a = b,
we simply denote the family as Da
I .

I

Finally, we obtain an analogous result to Theorem 2, by
characterizing the level of (positive) unbounded-DP that
guarantees a level γ of PMP under a restricted MI distri-
bution family.
Theorem 4. A mechanism A satisﬁes (γ, D[a,b]
)-PMP, for
0 < a ≤ b < 1, if A satisﬁes -positive unbounded-DP, where

I

(cid:40)

(cid:16) (1−a)γ

(cid:17)

1−aγ , γ+b−1

b

e =

min
γ+b−1

b

if aγ < 1,

otherwise .

6. CONCLUSION AND FUTURE WORK

We have investigated possible relaxations of the adver-
sarial model of diﬀerential privacy, the strength of which
has been questioned by recent works. By considering the
problem of protecting against set membership disclosure,
we have provided a complete characterization of the rela-
tionship between DP and PMP for adversaries with limited
prior knowledge. We have argued about the practical signif-
icance of these weaker adversarial settings and have shown
that we can achieve a signiﬁcantly higher utility when pro-
tecting against such bounded adversaries.

We have proposed a simple model for the selection of the
DP parameter, that consists in identifying a practically sig-
niﬁcant adversarial setting, as well as an appropriate bound
on an adversary’s posterior belief. We have illustrated these
points with a speciﬁc example on genome-wide association
studies and have shown that privacy threats identiﬁed in the
literature can be re-cast into our bounded adversarial model,
which leads to a better tradeoﬀ between privacy guaran-
tees and medical utility. Evaluating the applicability of our
model to other privacy domains, as well as the corresponding
utility gain, is an interesting direction for future work.

Our results from Theorems 1 and 4 show that when we
consider an adversary with limited prior knowledge, satisfy-
ing DP provides a suﬃcient condition for satisfying PMP.
An interesting direction for future work is to investigate
whether PMP under distribution families D[a,b]
and D[a,b]

B

I

Figure 8: Probability that Algorithm 1 outputs both, or at
least one of the causative SNPs, when guaranteeing PMP
with γ = 1.5 against adversaries with prior D[a,b]
B .

Discussion.

For both of the exponential mechanisms we considered,
our results show that by focusing on an adversarial setting
with bounded prior knowledge, we can attain the same PMP
guarantees as for adversaries with arbitrary priors and re-
tain a signiﬁcantly higher utility. As we argued that the
adversarial model with priors in D0.5
B is relevant in regard
to attacks against GWAS, this shows that we can achieve a
reasonable level of protection against these attacks and also
guarantee an acceptable level of medical utility for datasets
smaller (and thus cheaper) than previously reported.

We stress that the applicability of our results need not be
limited to GWAS or even to genomic privacy in general. In-
deed, we could consider applications in other domains where
DP has been proposed as a privacy notion, as a bounded
adversarial setting makes sense in many practical scenarios.
As we will see in Section 5, our results can also be adapted
to cover the case of unbounded-DP, thus further extending
their applicability to other use-cases of diﬀerential privacy.
Examples of settings where DP mechanisms have been pro-
posed, and yet an adversary with incomplete background
knowledge appears reasonable, can be found in location pri-
vacy [1] or data mining [7] for instance.

In scenarios where DP is applied to protect membership
disclosure, we would beneﬁt from considering whether the
adversarial setting of DP is reasonable, or whether a bound
on an adversary’s prior belief is practically signiﬁcant. De-
pending on the identiﬁed adversaries, we can select an ap-
propriate level of noise to guarantee PMP, according to the
model derived in Section 3.

5. THE CASE OF UNBOUNDED-DP

The characterization of unbounded-DP in the PMP frame-
work is a little more subtle than for bounded-DP. Li et al.
introduce a uni-directional deﬁnition of unbounded-DP.

Deﬁnition 9 (Positive Unbounded-DP [14]). A mechanism
A satisﬁes -positive unbounded-DP if and only if for any
dataset T , any entity t not in T , and any S ⊆ range(A),

Pr [A(T ∪ {t}) ∈ S] ≤ e · Pr [A(T ) ∈ S] .

(14)

00.20.40.60.81Probability  [0.0,1.0][0.1,0.9][0.2,0.8][0.3,0.7][0.4,0.6][0.5,0.5][a,b]AtleastoneassociatedSNPisoutputBothassociatedSNPsareoutput1296can be attained by other means than through DP. For in-
stance, in their work on membership privacy, Li et al. pro-
pose a simple mechanism for outputting the maximum of a
set of values, that satisﬁes PMP for the family D0.5
I but does
not satisfy any level of DP [14]. It is unknown whether sim-
ilar mechanisms could be designed for other queries (such
as those we considered in our GWAS scenario), in order to
potentially improve upon the privacy-utility tradeoﬀ of DP.

Acknowledgments
We thank Mathias Humbert and Huang Lin for helpful com-
ments.

7. REFERENCES
[1] M. E. Andr´es, N. E. Bordenabe, K. Chatzikokolakis,

and C. Palamidessi. Geo-indistinguishability:
Diﬀerential privacy for location-based systems. In
Proceedings of the 2013 ACM SIGSAC Conference on
Computer & Communications Security, CCS ’13,
pages 901–914, New York, NY, USA, 2013. ACM.

[2] R. Bassily, A. Groce, J. Katz, and A. Smith.

Coupled-worlds privacy: Exploiting adversarial
uncertainty in statistical data privacy. In Foundations
of Computer Science (FOCS), 2013 IEEE 54th
Annual Symposium on, pages 439–448. IEEE, 2013.

[3] R. Bhaskar, S. Laxman, A. Smith, and A. Thakurta.

Discovering frequent patterns in sensitive data. In
Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining,
pages 503–512. ACM, 2010.

[4] C. Dwork. Diﬀerential privacy. In Automata, languages

and programming, pages 1–12. Springer, 2006.

[5] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data
analysis. In Proceedings of the Third Conference on
Theory of Cryptography, TCC’06, pages 265–284,
Berlin, Heidelberg, 2006. Springer-Verlag.

[6] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and

T. Ristenpart. Privacy in pharmacogenetics: An
end-to-end case study of personalized warfarin dosing.
In 23rd USENIX Security Symposium (USENIX
Security 14), pages 17–32, San Diego, CA, Aug. 2014.
USENIX Association.

[7] A. Friedman and A. Schuster. Data mining with

diﬀerential privacy. In Proceedings of the 16th ACM
SIGKDD international conference on Knowledge
discovery and data mining, pages 493–502. ACM,
2010.

[8] J. Gehrke, M. Hay, E. Lui, and R. Pass.
Crowd-blending privacy. In Advances in
Cryptology–CRYPTO 2012, pages 479–496. Springer,
2012.

[9] N. Homer, S. Szelinger, M. Redman, D. Duggan,

W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan,
S. F. Nelson, and D. W. Craig. Resolving individuals
contributing trace amounts of dna to highly complex
mixtures using high-density snp genotyping
microarrays. PLoS genetics, 4(8):e1000167, 2008.

[10] A. Johnson and V. Shmatikov. Privacy-preserving

data exploration in genome-wide association studies.
In Proceedings of the 19th ACM SIGKDD

International Conference on Knowledge Discovery and
Data Mining, KDD ’13, pages 1079–1087, New York,
NY, USA, 2013. ACM.

[11] D. Kifer and A. Machanavajjhala. No free lunch in

data privacy. In Proceedings of the 2011 ACM
SIGMOD International Conference on Management of
Data, SIGMOD ’11, pages 193–204, New York, NY,
USA, 2011. ACM.

[12] J. Lee and C. Clifton. Diﬀerential identiﬁability. In

Proceedings of the 18th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining,
KDD ’12, pages 1041–1049, New York, NY, USA,
2012. ACM.

[13] N. Li, W. Qardaji, and D. Su. On sampling,

anonymization, and diﬀerential privacy or,
k-anonymization meets diﬀerential privacy. In
Proceedings of the 7th ACM Symposium on
Information, Computer and Communications Security,
ASIACCS ’12, pages 32–33, New York, NY, USA,
2012. ACM.

[14] N. Li, W. Qardaji, D. Su, Y. Wu, and W. Yang.

Membership privacy: a unifying framework for privacy
deﬁnitions. In Proceedings of the 2013 ACM SIGSAC
conference on Computer & communications security,
CCS ’13, pages 889–900, New York, NY, USA, 2013.
ACM.

[15] E. Lui and R. Pass. Outlier privacy. In Y. Dodis and

J. Nielsen, editors, Theory of Cryptography, volume
9015 of Lecture Notes in Computer Science, pages
277–305. Springer Berlin Heidelberg, 2015.

[16] A. Machanavajjhala, J. Gehrke, and M. G¨otz. Data
publishing against realistic adversaries. Proc. VLDB
Endow., 2(1):790–801, Aug. 2009.

[17] F. McSherry and K. Talwar. Mechanism design via

diﬀerential privacy. In Foundations of Computer
Science, 2007. FOCS’07. 48th Annual IEEE
Symposium on, pages 94–103. IEEE, 2007.

[18] C. C. Spencer, Z. Su, P. Donnelly, and J. Marchini.
Designing genome-wide association studies: sample
size, power, imputation, and the choice of genotyping
chip. PLoS genetics, 5(5):e1000477, 2009.

[19] C. Uhler, A. Slavkovic, and S. E. Fienberg.

Privacy-preserving data sharing for genome-wide
association studies. Journal of Privacy and
Conﬁdentiality, 5(1), 2013.

[20] R. Wang, Y. F. Li, X. Wang, H. Tang, and X. Zhou.

Learning your identity and disease from research
papers: Information leaks in genome wide association
study. In Proceedings of the 16th ACM Conference on
Computer and Communications Security, CCS ’09,
pages 534–544, New York, NY, USA, 2009. ACM.

[21] F. A. Wright, H. Huang, X. Guan, K. Gamiel,

C. Jeﬀries, W. T. Barry, F. P.-M. de Villena, P. F.
Sullivan, K. C. Wilhelmsen, and F. Zou. Simulating
association studies: a data-based resampling method
for candidate regions or whole genome scans.
Bioinformatics, 23(19):2581–2588, 2007.

[22] F. Yu, S. E. Fienberg, A. B. Slavkovi´c, and C. Uhler.
Scalable privacy-preserving data sharing methodology
for genome-wide association studies. Journal of
biomedical informatics, 2014.

1297