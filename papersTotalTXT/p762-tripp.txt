ALETHEIA: Improving the Usability

of Static Security Analysis

Omer Tripp

Salvatore Guarnieri

IBM T. J. Watson Research Center, Yorktown Heights, New York, USA

{otripp,sguarni,pistoia,saravkin}@us.ibm.com

Marco Pistoia

Aleksandr Aravkin

ABSTRACT
The scale and complexity of modern software systems complicate
manual security auditing. Automated analysis tools are gradually
becoming a necessity. Speciﬁcally, static security analyses carry
the promise of efﬁciently verifying large code bases. Yet, a critical
usability barrier, hindering the adoption of static security analysis
by developers, is the excess of false reports. Current tools do not
offer the user any direct means of customizing or cleansing the re-
port. The user is thus left to review hundreds, if not thousands, of
potential warnings, and classify them as either actionable or spuri-
ous. This is both burdensome and error prone, leaving developers
disenchanted by static security checkers.

We address this challenge by introducing a general technique
to reﬁne the output of static security checkers. The key idea is
to apply statistical learning to the warnings output by the analysis
based on user feedback on a small set of warnings. This leads to an
interactive solution, whereby the user classiﬁes a small fragment
of the issues reported by the analysis, and the learning algorithm
then classiﬁes the remaining warnings automatically. An important
aspect of our solution is that it is user centric. The user can express
different classiﬁcation policies, ranging from strong bias toward
elimination of false warnings to strong bias toward preservation of
true warnings, which our ﬁltering system then executes.

We have implemented our approach as the ALETHEIA tool. Our
evaluation of ALETHEIA on a diversiﬁed set of nearly 4,000 client-
side JavaScript benchmarks, extracted from 675 popular Web sites,
is highly encouraging. As an example, based only on 200 classi-
ﬁed warnings, and with a policy biased toward preservation of true
warnings, ALETHEIA is able to boost precision by a threefold fac-
tor (×2.868), while reducing recall by a negligible factor (×1.006).
Other policies are enforced with a similarly high level of efﬁcacy.

Categories and Subject Descriptors
D.2.4 [Software Engineering]: Software/Program Veriﬁcation—
Correctness proofs, Statistical methods; D.2.2 [Software Engineer-
ing]: Design Tools and Techniques—User interfaces

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660339.

General Terms
Security, Veriﬁcation, Human Factors, Algorithms, Measurement

Keywords
Usable security; False alarms; Static analysis; Information-ﬂow se-
curity; Classiﬁcation; Machine learning

1.

INTRODUCTION

The scale and complexity of modern software systems often lead
to missed security vulnerabilities. Static analysis has emerged as
a promising solution for automated security auditing, which can
scale to millions of lines of code while accounting for nontrivial
program behaviors, resulting in the detection of severe and some-
times also subtle vulnerabilities [29, 7, 28, 8].

integrity and conﬁdentiality violations.

In particular, static analysis has shown great value when applied
to information-ﬂow vulnerabilities [22, 5]. These are the most se-
rious and prevalent forms of vulnerability in today’s landscape of
Web and mobile applications,1 which divide into two broad cat-
egories:
Integrity viola-
tions include cross-site scripting (XSS), whereby unvalidated input
may execute on the victim’s browser; cross-application scripting
(XAS), whereby tags injected into widgets can alter their appear-
ance and behavior; and SQL injection (SQLi), whereby unintended
SQL commands are executed. Conﬁdentiality is the dual problem.
It arises when sensitive data emanating from the application is re-
leased to unauthorized observers.

Static Information-ﬂow Analysis.

A popular method of detecting information-ﬂow vulnerabilities
using static analysis, known as taint analysis, is by solving a “reach-
ability” problem. Statements reading untrusted user input like the
URL query string (or dually, sensitive information like the user’s lo-
cation) are deﬁned as the sources. Statements that perform security-
critical functionality like updating a database (or dually, release of
information) are deﬁned as the sinks. The third and ﬁnal category
is statements that sanitize or validate the input (or dually, declassify
sensitive data), which are dubbed downgraders.

The analysis checks for reachability between source and sink
statements via a downgrader-free path. Such paths, which we re-
fer to as witnesses or counterexamples, are reported as potentially
vulnerable. As an illustration, we provide in Figure 1 an exem-
plary witness reported by a commercial taint analysis for client-
side JavaScript, which we have also used for our experiments.2

1owasp.org.
2We have anonymized the actual ﬁle names to avoid from disclos-
ing the Web site’s identity.

762interviews with professional developers, false alarms are the most
signiﬁcant barrier to adoption of tools based on static program anal-
ysis by developers [11]. As things now stand, developers prefer to
release and deploy insecure software rather than ﬁnd and ﬁx latent
vulnerabilities using off-the-shelf static security checkers. This is
unfortunate.

Scope and Approach.

Our goal in this paper is to improve the usability of static secu-
rity checkers by cleansing their output. We put forward two basic
requirements:
• Generality: The cleansing technique should not be speciﬁc
to a given tool or analysis technique. It should rather treat
the analysis algorithm as opaque for wide applicability and
ease of integration into legacy as well as new analysis tools.
• Customizability: Different users have different preferences
when reviewing security warnings. Some prefer to aggres-
sively suppress false alarms, even at the expense of eliminat-
ing certain true issues, whereas others may opt for complete-
ness at the price of more false warnings.

Driven by these requirements, we have developed a method for
ﬁltering false warnings that combines lightweight user interaction
with heavyweight automation. In our approach, the user classiﬁes
a small portion of the raw warnings output by the analysis. The
user also speciﬁes a tradeoff between elimination of false alarms
and preservation of true ﬁndings. These two inputs are fed into a
statistical learning engine, which abstracts the warnings into fea-
ture vectors that it uses to automatically build a ﬁlter. The ﬁlter,
which is instantiated according to the user-speciﬁed policy, is next
applied to the (vast majority of) remaining warnings, resulting in
(many) less warnings than those initially reported by the security
checker. Importantly, our learning-based approach is guided solely
by the warnings themselves.
It does not make any assumptions
about, or access to, the internals of the analysis tool.

We have implemented our approach as the ALETHEIA system.
To build the ﬁlter, ALETHEIA searches through a library of clas-
siﬁcation algorithms. It computes a competency score for each of
these algorithms based on the user-classiﬁed warnings. The most
favorable candidate is then applied to the remaining warnings.

To evaluate ALETHEIA, we ran a commercial static JavaScript
security checker on a set of 1,700 HTML pages, taken from a di-
versiﬁed set of 675 top-popular Web sites, which resulted in a total
of 3,758 warnings. These were then classiﬁed by a security spe-
cialist as either true or false warnings. We report on a wide set
of experiments over this dataset. The results are highly encourag-
ing. As an example, for a policy biased toward preservation of true
positives, given only 200 classiﬁed warnings, ALETHEIA is able
to boost precision by a factor of 2.868 while reducing recall by a
negligible factor (×1.006). Conversely, if the user is more biased
toward elimination of false alarms, still based on only 200 classiﬁed
warnings, ALETHEIA achieves a recall-degradation factor of only
2.212 and a precision-improvement factor of 9.014. In all cases,
and across all policies, ALETHEIA is able to improve precision by
a factor ranging between ×2.868 and ×16.556 in return to user
classiﬁcation of only 200 warnings.

Contributions.

This paper makes the following principal contributions:
1. Boosting Usability via Learning: We propose a novel and

general technique to boost the usability of static security check-
ers. In our approach, the users invest tolerable effort in clas-
sifying a small portion of the warnings, and in return a sta-
tistical learning engine computes a ﬁlter over the remaining

Figure 1: Security Witness Reported by a Commercial Static
Security Checker for JavaScript

The reported vulnerability in this case is open redirect, which oc-
curs when the user is able to inﬂuence the target URL of a redi-
rection operation. The analysis bases this warning on information
ﬂow between a statement that obtains the URL query string (stored
as location.search) and a statement performing redirection
(location.replace(. . .)). The intermediate statement con-
catenates the return value from the source (pointed-to by variable
search) with other strings, and thus the taint tag is carried across
to the resulting url string.

Generalizations and extensions of taint analysis include features
such as string sensitivity, whereby the analysis explicitly tracks
string values and their structure for increased precision [26]; typestate-
based tracking rules to reﬁne the security speciﬁcation [13]; as well
as quantitative notions of information ﬂow for more accurate and
informative warnings [14]. In all of these cases, however, the key
concept of verifying disjointness between sources and sinks mod-
ulo permitted exceptions remains the same.

Static security veriﬁcation is not a silver bullet. To handle industry-

scale applications, the analysis must apply aggressive approxima-
tions. Notable dimensions of precision loss include ﬂow insensitiv-
ity, whereby the analysis does not track the order in which memory
updates occur and instead conservatively accounts for all possible
update orders; path insensitivity, whereby the analysis ignores path
conditions, thereby traversing infeasible execution paths; and con-
text insensitivity, whereby the analysis refrains from modeling the
calling context of a method, thereby analyzing infeasible invoca-
tion scenarios.

These (and other) sources of inaccuracy, which we discuss in
more detail and illustrate in Section 2.1, shrink the analysis’ state
space (also known as the abstract state) by an exponential factor,
and hence contribute signiﬁcantly to the scalability of the analysis.
As an example, if path conditions are accounted for, then the analy-
sis has to track two different runtime program states when reaching
a branching condition. Path insensitivity saves the analysis from
this type of state-space blowup.

Usability of Static Analysis Tools.

Approximations applied by the analysis can result in false alarms.
These may be due to various reasons, including e.g. infeasible con-
trol ﬂow, if the witness contains invalid branching decisions or call
sites that are resolved incorrectly; ignoring of downgrading opera-
tions, if a proprietary downgrader is applied or downgrading occurs
inline; and imprecise tracking of data ﬂow, e.g. due to coarse mod-
eling of string operations and/or aliasing between variables.

Indeed, while approximation enables scalability, it often also re-
sults in an excess of false alarms.These plague the reports by static
analysis tools [15], thereby hindering their usability. According to

763warnings. The ﬁlter is parameterized by the user’s preference
regarding the tradeoff between true and false alarms.

2. Characterization of Security Warnings: We characterize
different features of static security warnings, and draw gen-
eral conclusions about their correlation with the correctness
of a reported warning. This insight is of independent value,
and can serve for other studies as well. We also discuss and
analyze the relative merits and weaknesses of different clas-
siﬁcation algorithms, which is again of general applicability.
3. Experimental Evaluation: We have implemented our ap-
proach as the ALETHEIA system, which is to be featured in
a commercial product: IBM Security AppScan Source. Ex-
perimental evaluation of ALETHEIA on a diversiﬁed set of
benchmarks has yielded highly encouraging results.

2. OVERVIEW

In this section, we motivate the need for the ALETHEIA system
and provide a high-level description of its main components and
properties.
2.1 Limitations of Static Analysis

As highlighted above, static program analysis has inherent limi-
tations in precisely modeling runtime behaviors of the subject pro-
gram. Added to these, the analysis often deliberately sacriﬁces
precision (even when a precise model is possible) in favor of scal-
ability. Following are design choices that are often made for the
analysis to scale to large codes:
• Flow insensitivity: The analysis does not track the order in
which memory updates occur, and instead conservatively ac-
counts for all possible update orders. A simple example is
the following:
x. f = read(); x. f = "" ; write(x. f );

Even though the value of x.f is benign when it ﬂows into
the write(. . .) sink, the analysis abstracts away the order
of updates to x.f and simply records that at some point that
ﬁeld was assigned an untrusted value.
• Path insensitivity: Path conditions are ignored. Instead, the
analysis assumes that all paths through the control-ﬂow graph
(CFG) are feasible. Here is an example:
x. f = "" ;

if (b) { x. f = read(); } if

(! b) { write(x. f ); }

The above code is not vulnerable, as the conditions govern-
ing the source and sink statements are mutually exclusive.
Yet a path-insensitive analysis would report the infeasible
trace through both the source and the sink as a warning.
• Context insensitivity: The analysis abstracts away the con-
text governing the invocation of a method, thereby merging
together different execution contexts of the same method, as
this example illustrates:
y1 = id(x); y2 = id(read ()); write(y1);

Here id(. . .) is simply the identity method, which echoes
back its input.
In the ﬁrst invocation the input is trusted,
while the second invocation is with an untrusted argument.
Smushing together the two contexts, the analysis conserva-
tively judges that id(. . .) may return an untrusted value,
and therefore y1 is treated as untrusted and a vulnerability is
reported when write(. . .) is called.

Design choices like the above, which are pertinent for performance
and scalability, each contribute to the analysis’ imprecision. Worse
yet, there are signiﬁcant interaction effects between the different
sources of imprecision. This deﬁnes the need for complementary

machinery to cleanse the analysis’ output. In our approach, this
step is carried out interactively, with help from the user, by cast-
ing the warnings reported by the analysis into a statistical learning
framework.
2.2 System Architecture

We describe the high-level architecture of ALETHEIA with ref-
erence to Figure 2. The input to ALETHEIA is the raw warnings
{w1, . . . , wn} output by the static security checker. Next, the user
is asked to classify a subset of the warnings. This subset is se-
lected at random to avoid biases. The result is a classiﬁed subset
{(wi1 , bi1 ), . . . , (wik , bik )} of the warnings, where the labels bij are
boolean values (indicating whether the warning is true of false).
Naturally, the accuracy of the ﬁlter computed by ALETHEIA is pro-
portionate to the number of warnings reviewed by the user. How-
ever, as we demonstrate experimentally in Section 5, even a rel-
atively small sample of 100 warnings sufﬁces for ALETHEIA to
construct a highly precise ﬁlter.

To cast security warnings into a statistical setting, we need to de-
rive simple-structured features from the warnings, which are com-
plex objects that cannot tractably be learned directly. This part of
the ﬂow is visualized in Figure 2 as the “feature mapping” box. A
given warning is abstracted as a set of attributes, including e.g., the
respective line numbers of the source and sink statements, the time
required by the analysis to compute the witness, the number of ﬂow
steps along the witness, etc.

The feature vectors, combined with the user-provided true/false
tags and policy, provide the necessary data to learn and evaluate
ﬁlters. Given training data of the following form:

[length = 14, time = 2.5, srcline = 10, . . .]
[length = 6, time = 1.1, srcline = 38, . . .]
[length = 18, time = 3.6, srcline = 26, . . .]

(cid:55)→ false
(cid:55)→ true
(cid:55)→ false

. . .

the ALETHEIA system partitions the data into training and testing
sets of equal cardinality. ALETHEIA then generates a set F of
candidate ﬁlters by training different classiﬁcation algorithms on
the training set. ALETHEIA also converts the policy into a scor-
ing function SCORE. Next, each of the candidate ﬁlters is applied
to the testing set, and the resulting classiﬁcations are reduced to a
score via the SCORE function based on the rate of true positives,
false positives and false negatives. Finally, the ﬁlter that achieves
the highest score is applied to the remaining warnings. The user is
presented with the ﬁndings surviving the ﬁlter.
2.3 Features and Learning Algorithms

Naturally, the efﬁcacy of ALETHEIA is dependent on the avail-
able features and learning algorithms. We describe both in detail
in Sections 3 and 4, respectively. Here we give a brief informal
description of both.

Features.

As explained earlier, features are an abstraction of witnesses re-
ported by the analysis tool. Most of the features that we have
deﬁned reﬂect basic characteristics of the witness, such as (i) its
length (i.e., the number of statements along the path), (ii) the syn-
tactic location of the source and sink statements, or (iii) the context
manipulated by the JavaScript code (plain DOM elements, Flash,
JavaScript, etc). We have also deﬁned general features that are not
derivable directly from the witness itself, but rather reﬂect meta-
data associated with the witness, such as the time required by the
analysis to detect the respective violation.

764Figure 2: Visual description of the workﬂow of the ALETHEIA system

the features we have selected. We divide the discussion according
to feature categories.

3.1 Lexical Features

We note, importantly, that simply deﬁning a large set of arbitrary
features may lead the learning algorithm to discover false corre-
lations. For this reason, we have made a careful selection of fea-
tures. Behind each of the features is a justiﬁcation for why it may
correlate with true/false alarms. As an example, many ﬁndings in-
volve imported code, such as the swfobject library for embed-
ding Flash content in HTML ﬁles.3 These often invoke sources
and/or sinks (e.g., reading or writing of the document’s URL). The
developer may deem ﬂows emanating from, passing through or ar-
riving at such libraries as spurious. This may happen, e.g., if the
library applies inline sanitization that the analysis misses. Thus,
the source/sink location may become an important feature, which
correlates well with user classiﬁcations.

Learning Algorithms.

There is a wide spectrum of classiﬁcation techniques [30, 9].
These range between tree-based classiﬁcation (e.g., decision trees
[21]); rule-based algorithms [17]; functional methods, which com-
pute a geometrical boundary between the instances [20]; Bayesian
techniques, which compute the probability of events (actual vs spu-
rious vulnerabilities in our case) as a function of feature attributes
[18]; and even (unsupervised) clustering techniques like K-means
that form clusters and tag new instances according to their assigned
cluster [4].

An important property of ALETHEIA is that it lets the user spec-
ify the ﬁltering policy. As we explain in Section 4.5, and validate
experimentally in Section 5.3, different policies are best served by
different classiﬁcation algorithms. As an example, if the user is
strongly biased toward elimination of false alarms, then a rule-
based classiﬁer may identify a feature f and a threshold value v,
such that (almost) any alarm for which [[f ]] ≥ v is false. This
would satisfy the user’s preference perfectly, but at the same the
classiﬁer may also eliminate true vulnerabilities. Striking a non-
trivial balance between precision and recall, on the other hand, may
leads toward a more sophisticated classiﬁer ((e.g., a decision tree
or a kernel SVM). In light of this observation, we have linked into
ALETHEIA 8 popular classiﬁers that together all the algorithmic
categories above (functional, rule based, tree based, etc).

uated in the sink statement (e.g., window.open(. . .)).

A natural category of features are those recording syntactic prop-
erties of the witness. Speciﬁcally, we have deﬁned seven such fea-
tures, as follows:
• Source identiﬁer (srcid): The name of the ﬁeld or function
evaluated in the source statement. An example of a source
identiﬁer is document.location.
• Sink identiﬁer (srcid): The name of the ﬁeld or function eval-
• Source line number (srcline): The line number of the source
• Sink line number (srcline): The line number of the sink
• Source URL (srcurl): The URL of the JavaScript function
• Sink URL (sinkurl): The URL of the JavaScript function
• External objects (extobjs): Flags indicating whether the wit-
ness is performing mailto or embed functionality (e.g.,
Flash).

containing the source statement.

containing the sink statement.

statement.

statement.

Syntactic features are effective in uncovering patterns due to third-
party libraries or usage of frameworks. They also assist in localiz-
ing noisy sources and sinks.

Indeed, our reason for recording source and sink line numbers as
well as the URLs of the ﬁles enclosing the source and sink state-
ments, which we exemplify in Section 2.3, is to capture instances
where the ﬂow either emanates from, or arrives at, third-party li-
braries. Another meaningful characterization of the ﬂow is the con-
text(s) it manipulates via mailto and embed statements. Certain
attack vectors fail in such special contexts, which could be a source
of false alarms.

Finally, for source and sink identiﬁers, the motivation is to detect
“noisy” operations (e.g., document.location.url, which can
cause an open-redirect attack if assigned an untrusted value, but this
happens very rarely [27]).

3. LEARNING FEATURES

As we highlighted earlier, in Section 2, the choice of which fea-
tures to map a security witness to has important bearing on the
overall quality of the ﬁltering algorithm. The main concern with
introducing arbitrary features is that the algorithm could be led to
discover false correlations between such features and witness cor-
rectness. We dedicate this section to explaining the rationale behind
3https://code.google.com/p/swfobject/

3.2 Quantitative Features

A second category of features are those that record quantitative
measures of the witness. These are not to be confused with nu-
merical yet nonquantitative features like line numbers. The latter
are meaningful only inasmuch as equality checking is concerned,
whereas quantitative measures can meaningfully be subjected to
less-/greater-than comparisons. We deﬁne the following ﬁve quan-
titative features:

11: x=document.location; 12: y=x.search; 13: document.location=y;11: x=document.location; 12: y=x.search; 13: document.location=y;11: x=document.location; 12: y=x.search; 13: document.location=y;11: x=document.location; 12: y=x.search; 13: document.location=y;11: x=document.location; 12: y=x.search; 13: document.location=y;11: x=document.location; 12: y=x.search; 13: document.location=y;11: x=document.location; 12: y=x.search; 13: document.location=y;analysis engineraw output11: x=document.location; 12: y=x.search; 13: document.location=y;11: x=document.location; 12: y=x.search; 13: document.location=y;11: x=document.location; 12: y=x.search; 13: document.location=y;feature mapping[length=3,srcline=11,snkline=13,…]11: x=document.location; 12: y=x.search; 13: document.location=y;11: x=document.location; 12: y=x.search; 13: document.location=y;[…]user classiﬁcationcleansed output11: x=document.location; 12: y=x.search; 13: document.location=y;classiﬁer11: x=document.location; 12: y=x.search; 13: document.location=y;765comprising the witness path.

reported on the ﬁle containing the sink statement.

• Total results on (results): The overall number of ﬁndings
• Number of steps (steps): The number of ﬂow milestones
• Time (time): The total time spent by the analysis on the
• Number of path conditions (conditions): The number of
branching statements (either loops or conditions) along the
witness path.
• Number of functions (functions): The number of functions

scope containing the witness.

enclosing statements along the witness path.

Intuitively, all ﬁve of these features satisfy that the greater their
value is, the less likely it is for the witness to be correct. First, for
overall number of results, it is unlikely (albeit not impossible) for
a single ﬁle to contain a large number of vulnerabilities. A more
likely hypothesis is that the functions in the ﬁle are complicated,
leading to their conservative and thus imprecise analysis. The time
feature captures a similar pathology. Often imprecision leads the
analysis to explore dead code or infeasible execution paths, which
in turn lead to further imprecision, and so on. Thus, if the analysis
has spent a long time on a given scope, then it is likely a symp-
tom indicating that the model it has created is overly conservative.
Finally, for methods, steps and path conditions, the higher these
counts are, the more the analysis is exposed to errors due to ap-
proximations (e.g., infeasible branching, incorrect resolution of call
sites, etc).
3.3 Security-speciﬁc Features

to the security domain. We have identiﬁed two such features:

The third and ﬁnal category relates to features that are intrinsic
• Rule name (rname): The name of the violated security rule.
• Severity (severity): The severity of the violation as deter-

mined by the analysis tool.

There are various other security properties that are reported by in-
dustrial security checkers, but these change across tools, and so for
generality we avoided from including them.

Similarly to the source and sink identiﬁers, the involved security
rule may prove “noisy.” This could be either because (i) the rule is
perceived as less relevant by the user or (ii) the sources and sinks
deﬁned by the rules are noisy or (iii) there are defense measures
(like framework-level sanitizers) that suppress the given type of
vulnerabilities, which the analysis is not aware of. The severity of
a ﬁnding also hints toward its correctness. In speciﬁc, low-severity
witnesses are less likely to be accepted by the user as actionable.
3.4 Discussion

and how our approach generalizes to other domains:

Our division of the features into three categories clariﬁes where
• Syntactic features are of wide applicability. Our speciﬁc
choice of features, which mostly refer to sources and sinks,
assumes a data-ﬂow analysis. Deﬁning other syntactic fea-
tures, both general and speciﬁc to other forms of analysis, is
straightforward.
• For quantitative features, some of the features we selected
are strictly general. Measuring quantities like time and num-
ber of results is useful across virtually all forms of analysis,
including even AST-level bug-ﬁnding tools like FindBugs.4
Other quantitative properties, like the number of ﬂow steps,
assume a data-ﬂow analysis whose results are in the form of
traces through the code.
• Last, the security-speciﬁc features naturally carry values that
are inherent to our domain. However, the features themselves

4findbugs.sourceforge.net/

— rule name and severity — are general, and likely remain
useful in other domains.

To summarize, then, many of the features that we have deﬁned are
of general usefulness. Most of the features remain relevant under
other forms of data-ﬂow analysis (e.g., data-race detection or type-
state veriﬁcation), and some, like time and number of results, even
apply to a wider range of analyses.

As for generalization to other languages, beyond the boundaries
of JavaScript, a pleasing property of our approach is that it largely
treats the analysis as a black box, instead focusing on observable
outputs (warnings, running time, etc). This also renders any depen-
dence on the underlying programming language minimal. While
there are certain speciﬁc properties that relate to language libraries,
extobjs being an example, ALETHEIA can be adapted to support
the same style of security analysis in other languages with modest
effort.

4. LEARNING ALGORITHMS

Our approach to static security analysis reduces the problem to
binary classiﬁcation, either in the online or ofﬂine setting. For a
set of reports, we have feature data (given in Section 3) along with
user-generated labels. Binary classiﬁcation is a classic problem in
machine learning, and a variety of methods from the ﬁeld can be
applied (for a survey, see [30, 3, 9]). In this section, we give a brief
overview of methods we use to evaluate the approach in Section 5,
and discuss their suitability for static security analysis. We discuss
four categories of methods: functional, clustering, tree/rule based,
and Bayesian. Methods in different classes can be combined, and
some state of the art methods borrow ideas from several categories.
For example, the NBTree method builds a decision tree with Naive
Bayes’ classiﬁers at the leaves. We provide a general overview,
rather than delving too deeply into the details of the methods we
compared. We close with a comparative discussion of the methods,
identifying potential advantages of some categories, and presenting
strategies for comparison between methods useful from the user’s
perspective.
4.1 Functional Methods

Functional methods include logistic regression (see, e.g., [3]),
linear support vector machines (see, e.g. [20, 24]), and generaliza-
tions, such as neural nets and kernel SVMs (see, e.g., [9]). For
convenience, we refer to these models as functional classiﬁcation.
These methods classify by learning a boundary either in feature
space, or in a derived space related to features space by particular
mappings. Once trained, the model can be used for prediction by
noting where in the decision space an incoming feature would fall.
For example, SVMs search for a hyperplane in feature space that
best separates labeled data (according to a maximum margin con-
dition). Given a set of features xi with labels yi ∈ {−1, 1}, SVMs
solve for a hyperplane w that solves a strictly convex optimization
problem:

minw,γ

(cid:107)w(cid:107)2 + λ

1
2

max(0, 1 − yi(xT

i w − γ))

(1)

By inspection, the method weights a regularization term, (cid:107)w(cid:107)2,
against a robust classiﬁcation measure, which is 0 for example i
i w − γ has the correct sign, and grows
when the predicted label xT
linearly if the sign is incorrect. The problem is strictly convex, so
always guaranteed to have a unique solution, which is easily found
using iterative methods.

One weakness of SVMs, and other other linear methods (e.g.,
logistic regression) is the richness of the model space — there are

(cid:88)

i

766limits to how well a linear classiﬁer can perform. To address this
issue, kernel SVMs neural net models train multiple layers of de-
rived features from the data, while kernel SVMs make use of a
kernel function Φ that maps the features to a different space, yet
allows simple evaluation of inner products Φ(xi)T Φ(xj). Kernels
and their applications have a rich literature, see, e.g., [1, 23].
4.2 Instance-based Classiﬁcation

Instance based learning requires a distance function to measure
the distance of an incoming measurement to existing instances.
Given such a function, one can simply ﬁnd the nearest labeled in-
stance to an unknown datapoint, and use that label to predict the
class of the input.

For example, the Kstar algorithm uses a generalized distance
function that models the distance between two instances through
a series of possible transformations. Considering the entire set of
possible transformations gives a probabilistic measure that takes
into account inﬂuence from several labeled points [4].
4.3 Tree- and Rule-based Methods

Tree- and rule-based methods are divide and conquer algorithms

that try to efﬁciently partition data instances according to labels.

For example, decision trees partition data into branches accord-
ing to attribute values and labels. In order to build the tree, a fea-
ture attribute must be chosen at every branch point. These choices
are made in a way that maximizes the so-called ‘information gain’,
which is a data dependent measure that can be quickly computed.
Decision trees work top-down, ﬁnding an attribute to split on at
each point, and continuing recursively into the branches [21].

Rule-based methods attempt to ﬁnd covering rules that describe
each class, excluding others. For discrete attributes, rules can in-
clude membership in subsets of domain values (e.g., use of frame-
works is binary and can be used in a split or a rule). For continuous
or ordinal variables, rules can include thresholds (e.g., time taken
to complete analysis larger than a set value). For example, the 1R
classiﬁer generates a one-level decision tree, with each rule in the
set testing only one particular attribute [17].
4.4 Bayesian Methods

Bayesian methods include Naive Bayes, and Bayesian Networks,
and directly model the probability of class events as a function of
feature attributes. Naive Bayes assumes independence of attributes,
and uses Bayes’ rule to compute the probability of class given fea-
ture as:

P(C = c | X = x) =

P(X = x | C = c)P(C = c)

,

P(X = x)

where the probabilities on the right hand side are learned from
the data [18].
It is important to note that Naive Bayes also as-
sumes quantitative features have a Gaussian distribution, but there
are generalizations that relax this assumption. More importantly,
independence of attributes can be relaxed as well, and more so-
phisticated methods such as Bayesian Networks are able to learn
covariance structure from the data, e.g., by maximum likelihood
estimation [10].
4.5 Discussion

In the context of static security analysis, geometric methods such
as support vector machines and logistic regression models (which
ﬁt a hyperplane through feature space) and instance-based learning
that rely on Euclidean distance between features have disadvan-
tages, since they rely on the usefulness of the numerical quantities
present in feature space (or related space). This assumption may

not hold for applications such as static security analysis, where the
numerical values of the features may not have directly interpretable
meaning, and methods from non-geometric categories may be a
better choice. Our numeric experiments support this hypothesis.

Consider the list of features in Section 3. Two reports with sig-
niﬁcantly different source line numbers may not be signiﬁcantly
different (whether they are or not depends on the code). While
examples that took similar time to complete may be similar, exam-
ples that took different but long times to complete may be similar
as well. These features of the data may limit the performance of ge-
ometric classiﬁers, and so we may expect that tree and rule-based
methods may outperform functional learning. The performance of
instance-based learning is harder to predict, since a a generalized
notion of distance may not directly rely on numerical quantities.
4.6 Performance Measurement

The performance of classiﬁcation methods can be understood
through a range of statistics, such as precision, recall, and accuracy.
Every method may have a whole range of values associated to it.
For example, precision is the ratio of positives correctly identiﬁed
to all the instances labeled positive, while recall is the proportion
of positive instances correctly identiﬁed. There is a natural trade-
off between recall and false positives. If one tries to catch a higher
portion of positive instances, one will necessarily also mislabel a
greater portion of negative ones. Thus, increasing recall leads to
decreased precision.

This suggests two ways of comparing binary classiﬁcation algo-
rithms. One way is to compute an aggregate measure of quality
across a range of possible policies. This is typically done using re-
ceiver operator characteristic (ROC) curves, see, e.g., [19]. ROC
curves plot true positive rate (recall) as a function of false positive
rate for each classiﬁer. The area under and ROC curve, called AUC,
serves as an aggregate measure of quality; the higher the AUC, the
better the classiﬁer (overall). An AUC of 1 means that the classi-
ﬁer can perfectly separate the two classes; an AUC of 0.5 means
that the classiﬁer is essentially no better than a random guess. Two
classiﬁers with the same AUC may perform differently in different
regions of the 2D space deﬁned by true positives and false positives.
A second way is to prescribe a policy choice, for example, to
require high recall. This is appropriate for static security analysis,
since we want to miss as few real issues as possible. Then, for
high recall, one can compare classiﬁers using their false positive
rates (the lower the better), or similar measures such as precision
(the higher the better). Different classiﬁers may be superior for
different policy choices.

5.

IMPLEMENTATION AND EVALUATION
In this section, we describe our prototype implementation of the
ALETHEIA system. We then present experiments that we have con-
ducted to evaluate the efﬁcacy of ALETHEIA and the viability of its
underlying usage scenario.
5.1 Prototype Implementation

ALETHEIA is implemented as a Java library. Its public interface
consists of a main operation that accepts as input (i) a set of security
warnings, (ii) a partial mapping from warnings to the label assigned
to them by the user (true vs false) and (iii) a ﬁltering policy (spec-
ifying a precision/recall tradeoff). To compute a highest-ranking
classiﬁer per the policy, ALETHEIA partitions the mapped warnings
into training and testing subsets. (By default, these are obtained by
splitting the set in half at random.) Each of the candidate classiﬁ-
cation algorithms is then trained on the training subset. A rank is
assigned to the resulting classiﬁer based on the test instances. The

767output is the highest-ranking classiﬁer. It accepts a warning as its
argument and returns a boolean value. The lifetime of the ﬁlter is
decided by the client analysis tool, which may compute the ﬁlter
from scratch across analysis tasks, different applications, etc.

The current ALETHEIA prototype is built atop version 3.6.10 of
the Weka data-mining library [30].5 Weka provides a rich set of
machine-learning algorithms, which are all implemented in Java
and expose a consistent interface. ALETHEIA utilizes both the clas-
siﬁcation algorithms provided by Weka and the statistical analysis
tools built into it. In particular, ALETHEIA makes use of the ZeroR
and OneR rule-based classiﬁers; the NaiveBayes and BayesNet-
work Bayesian classiﬁers; the K-Star lazy classiﬁcation algorithm;
the NB-Tree and J48 tree-based classiﬁers; and the SVM functional
classiﬁcation method.

For the ﬁltering policy, ALETHEIA lets the user express the trade-
off between precision and recall, which are calculated as follows:

p =

r =

tp

tp + fp

tp

tp + fn

(precision)

(recall)

(2)

(3)

where tp, fp, and fn indicate the number of true positives, false posi-
tives, and false negatives reported by the analysis, respectively. The
balance between precision and recall is achieved via a normalized
weight w. This leads to the following formula:

w × r + (1 − w) × p

(4)

where the current ALETHEIA implementation ﬁxes the discrete val-
ues w ∈ {0, 1
5.2 Experimental Setup

4 , 1} to simplify user interaction.

4 , 1

2 , 3

To evaluate ALETHEIA, we collected an extensive set of 3,758

security warnings output by a commercial JavaScript security checker
when applied to 1,706 HTML pages taken from 675 top-popular
Web sites. These Web sites include all Fortune 500 companies,
the top 100 Web sites,6 as well as several handpicked sites of IT
and security vendors. Overall, the source ﬁles are highly diversi-
ﬁed, reﬂecting a variety of software categories and coding idioms.
All 3,758 of the reported warnings were carefully reviewed by a
professional ethical hacker, who determined for each of the issues
whether or not it is an actual vulnerability.

3,758 classiﬁed ﬁndings.

Based on this set of classiﬁed ﬁndings, we have created an ex-
perimental harness that performs the following series of actions for
a given policy w and training-set size n:
• A sample of n warnings is obtained at random out of the
• Next, ALETHEIA is applied: Each of the candidate classiﬁers
(ZeroR, OneR, etc) is trained on the n
2 training-set instances,
and then tested on the remaining n
• The ﬁlter output by ALETHEIA is then applied to the remain-
ing 3, 758−n warnings. Since the entire set has already been
classiﬁed in advance, we can compute precision and recall
per w and n.

2 test instances.

This experimental setup provides an accurate and measurable sim-
ulation of user interaction with ALETHEIA. In reality, n and w are
determined by the user.
We report in Table 1 (under Appendix A) on the results we ob-
tained across the ﬁve policies built into ALETHEIA as well as 100 ≤
5http://www.cs.waikato.ac.nz/ml/weka/
6http://www.web100.com/category/web-100/

n ≤ 900. In the interest of robust methodology and reproducible
results, we have executed the harness in each of the w/n conﬁgu-
rations a total of 15 times, where the reported data is the average
across all runs.
5.3 Experimental Hypotheses

Before we present the raw experimental data output by our testbed,

we lay out the experimental hypotheses guiding our research.

H1: Feasibility.

Our ﬁrst research hypothesis asserts the overall viability of our
approach. Concretely, we consider manual classiﬁcation of up to
200 warnings as tolerable and a ﬁlter that achieves at least 95%
accuracy as effective. This leasds to the following statement of vi-
ability:

H1: Based on tolerable user effort, it is possible — for
a given precision/recall policy — to ﬁlter the remain-
ing warnings effectively with respect to the speciﬁed
policy.

H2: Learning Framework.

The second hypothesis concerns the need for an extensive and
diversiﬁed set of classiﬁers. The claim is that there isn’t a “one size
ﬁts all” ﬁlter that could replace the learning approach embodied in
ALETHEIA. A space of candidate classiﬁers needs to be searched
for an optimal ﬁlter per the reported warnings and policy at hand:

H2: None of the classiﬁcation algorithms in our suite
satisﬁes that across all the different ﬁltering policies
(i.e., precision/recall tradeoffs), it achieves a score that
is at most 10% below the best classiﬁer for the given
policy.

That is, the choice of which classiﬁer to use is context sensitive,
and so a general ﬁltering strategy would not be appropriate.

H3: Diminishing Returns.

The third and ﬁnal hypothesis with which we went into the ex-
periments is that ﬁlter quality, as a function of user burden, gradu-
ally plateaus. If so, then this is reason for encouragement, because
the ﬁlter computed in return to tolerable user effort is a nearly op-
timal one given the diminishing return thanks to classiﬁcation of
additional warnings:

H3: Improvement in ﬁlter quality, measured in terms
of policy score, diminishes as the user contributes in-
creasingly more warning classiﬁcations.

The hope, beyond merely conﬁrming H3, is for the plateauing trend
to arise already at a tolerable threshold of 200 warnings per H1.
5.4 Experimental Results

The results of the experiments are listed in numeric form in Ta-
ble 1 (under Appendix A). To make the trends easier to appreciate,
we present the results pertaining to data points of interest — 100
and 200 user classiﬁcations, which we perceive as reasonable man-
ual effort — in visual form in Figures 3 through 10 and 13.

In Figures 3 and 4, we visualize the scores of different classiﬁers
as a continuous function of the policy — which is deﬁned as the
linear expression w× r + (1− w)× p — for 100 and 200 classiﬁed
instances respectively. Recall that r denotes recall and p denotes
precision. The policy is, therefore, a linear function of w (which
determines the relative weight of precision vs recall), and hence

768Figure 3: Scores Achieved by the Different Classiﬁers As a
Function of the Policy Given 100 Classiﬁed Warnings

Figure 4: Scores Achieved by the Different Classiﬁers As a
Function of the Policy Given 200 Classiﬁed Warnings

Figure 5: Precision and Recall for the Different Classiﬁers
Given 100 Classiﬁed Warnings

Figure 6: Precision and Recall for the Different Classiﬁers
Given 200 Classiﬁed Warnings

Figure 7: Precision As a Function of Classiﬁed-set Size

Figure 8: Recall As a Function of Classiﬁed-set Size

 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4ValuePolicyUser classified 100ONERNAIVEBAYESKSTARJ48NBTREESVMBAYESNETZEROR 0 0.2 0.4 0.6 0.8 1 0 1 2 3 4ValuePolicyUser classified 200J48ONERKSTARSVMNAIVEBAYESBAYESNETNBTREEZEROR 0 0.2 0.4 0.6 0.8 1OriginalONERNAIVEBAYESKSTARJ48NBTREESVMBAYESNETZERORValuePolicyUser classified 100PrecisionRecall 0 0.2 0.4 0.6 0.8 1OriginalJ48ONERKSTARSVMNAIVEBAYESBAYESNETNBTREEZERORValuePolicyUser classified 200PrecisionRecall 0 0.2 0.4 0.6 0.8 1100200300400500600700800900ValueClassified SizePrecisionbayesnetj48kstarnbtreenaivebayesonersvmzeror 0 0.2 0.4 0.6 0.8 1100200300400500600700800900ValueClassified SizeRecallbayesnetj48kstarnbtreenaivebayesonersvmzeror769Figure 9: Number of Findings the User Has to Review with
ALETHEIA (by Policy: 1-4) and without ALETHEIA Given 100
Initial Classiﬁcations

Figure 10: Number of Findings the User Has to Review with
ALETHEIA (by Policy: 1-4) and without ALETHEIA Given 200
Initial Classiﬁcations

the straight trend lines in 3 and 4 (and in general). The horizontal
labels, i ∈ {0, . . . , 4}, are the discrete policy points i
4 , as discussed
in Section 5.1.

The bar graphs in Figures 5 and 6 depict precision and recall
information for the different classiﬁers, again given 100 and 200
classiﬁed instances respectively. To clarify, these were calculated
for the remaining 3, 758 − n warnings (i.e., all warnings excluding
those classiﬁed by the user).

Notice that the ZeroR classiﬁer has both precision and recall val-
ues of 0. This results from the simplicity of this classiﬁer, which
merely predicts the majority category. Since true warnings consti-
tute only 5% of all the reports, ZeroR predicts that all warnings are
false, the result being 0 precision and recall values. Also note that
the original algorithm (without ﬁltering) has perfect recall. Thus,
the relative improvement in precision and degradation in recall due
to different classiﬁers can be quantiﬁed by comparison with the re-
spective “Original” bars.

The graphs in Figures 7 and 8 plot changes in precision and
recall, respectively, as a function of the classiﬁed-set size. Intu-
itively, classiﬁer quality improves as the number of classiﬁed in-
stances increases. The graphs in Figures 9 and 10 depict user effort,
measured as number of manually classiﬁed issues, under differ-
ent ALETHEIA conﬁgurations as well as without ALETHEIA. Last,
Figure 13 displays the trend line for policy score for the ﬁve dis-
crete policy points above a function of classiﬁed-set size.

In the following, we evaluate our experimental hypotheses.

Evaluation of H1.

First, for H1, we begin by reviewing Figure 5 that corresponds
to a relatively modest effort by the user of classifying only 100 out
of the full set of 3,758 warnings reported by the security checker.
Driven by this data alone, ALETHEIA achieves precision improve-
ments ranging from 417% (OneR) all the way up to 1274% (Naive-
Bayes). If the user prefers to preserve true alarms, then ALETHEIA
is able to comply with this requirement with 76% recall alongside
×4.17 improvement in accuracy (OneR).

We now switch to Figure 6, which reﬂects data for 200 classiﬁed
instances (still a tolerable amount of work by the user in our view
and experience). Here we observe across all policies signiﬁcant re-

duction in false alarms, ranging between ×3.4 (OneR) and ×18.4
(SVM) according to the bias induced by the policy.
In particu-
lar, if the user has a strong bias toward preserving true warnings,
then ALETHEIA satisﬁes this bias with recall as high as 97% while
simultaneously improving precision by 340% (OneR), which is a
very dramatic improvement in user experience. Similar trends can
be gleaned from review of the additional data in Table 1 (for 300-
900 classiﬁed instances).

Viewed from the perspective of user effort, ALETHEIA enables
visible saving thanks to ﬁltering out false alarms, as we depict
in Figures 9 and 10. In these ﬁgures, we quantify developer ef-
fort in “warning units”, counting how many reports the user has
to review under the different policies after the ﬁltering applying
by ALETHEIA given classiﬁed-set sizes of 100 and 200, respec-
tively. The reference, appearing as the rightmost column, is the
3,758 warnings reported by default. We visualize within each col-
umn the proportion of true vs false positives. We also visualize the
proportion of reports classiﬁed by the user where appropriate.

The difference in height from the reference column, which cap-
tures the gap in user effort, is signiﬁcant. This is true even of the
more conservative policies given only 100 user-classiﬁed instances
(Figure 9), but moving to 200 user-classiﬁed instances has a much
more dramatic effect (Figure 10): For a conservative user, approx-
imately 75% of the initial workload is eliminated. For a less con-
servative user, the saving becomes about 90% with very few issues
to review beyond the initial 200 classiﬁcations.

The conclusion we derive from the data is the H1 is conﬁrmed.
That is, given tolerable classiﬁcation effort by the user (up to 200
warnings), ALETHEIA is able to signiﬁcantly decrease the rate of
false warnings while also achieving high recall rates if a bias for
preservation of true positives is expressed. As a consequence, there
is substantial reduction in user effort.

As a natural extension, given a rich dataset like ours, an of-
ﬂine learning setting — whereby per-policy universal classiﬁers
are computed once for all users and software systems — could
completely eliminate the end user’s classiﬁcation effort. Though
seemingly attractive, this solution is complicated by the fact that
static security checkers are often conﬁgured by the user (e.g., by
modifying the security rules, severity levels, analysis bounds, etc).

 0 500 1000 1500 2000 2500 3000 3500 4000 0 1 2 3 4ValuePolicyUser classified 100ClassifiedTPFPOrg. TPOrg. FPWithout AssistanceUsing ALETHEIA 0 500 1000 1500 2000 2500 3000 3500 4000 0 1 2 3 4ValuePolicyUser classified 200ClassifiedTPFPOrg. TPOrg. FPWithout AssistanceUsing ALETHEIA770Such customizations bear direct impact on the resulting classiﬁer,
and so ofﬂine learning is less adequate.

In our evaluation, this was not a concern since we retained the
same (default) tool conﬁguration across all benchmarks. Indeed, if
a given conﬁguration is used across an entire organization or devel-
opment team, then classiﬁers can be computed and shared based on
the effort of a single user (or by distributing the 200 classiﬁcations
across multiple users).

Evaluation of H2.

4 , 1

2 , 3

Moving to H2, we refer to Figures 3 and 4. These indicate the
trend followed by each of the classiﬁer across the continuum of
policies for w ∈ [0, 1]. For the discrete data points corresponding
4 , 1}, we provide in the second column of Table 1,
to w ∈ {0, 1
titled “Model”, the classiﬁer achieving the highest score.
Concentrating on Figures 3 and 4, we derive the clear observa-
tion that none of the classiﬁers dominates, or is even close to dom-
inating, the other classiﬁers. This would have corresponded to that
hypothetical classiﬁer’s graph being above (or near to) all the other
graphs. In reality, all the graphs intersect with each other, mostly
far from the boundaries, which indicates that different policies re-
quire different classiﬁers and there is no “one size ﬁts all” candi-
date. The trend lines for other classiﬁed-set sizes, which we omit
for lack of space, echo the same pattern.

The data in Table 1 across the entire range of classiﬁed-set sizes
is also consistent with this analysis. Across all policies and sec-
tions, ﬁve out of the eight algorithms plugged into ALETHEIA come
out as best at least once. Somewhat unsurprisingly, OneR — which
generates a one-level decision tree, thus being a simple rule-based
classiﬁer — ﬁts well with the “extreme” policy that biases toward
preservation of true positives. At the other extreme, there is more
variance with J48 and SVM being the most effective candidates
overall. The asymmetry between the two extremes (w = 0 and
w = 1) is due to the disproportion between true and false alarms in
the dataset (which is an accurate representation of warnings in the
wild).

The in-between policies are enforced effectively by the tree-based
classiﬁers. Our conjecture is that this follows from the non-geometric
interpretation of several important witness features, such as the lex-
ical location and line number of sources and sinks. SVM and other
functional classiﬁers are confused by such features, and rule-based
techniques are often too naive to capture the complexity of the de-
cision process. (See also Section 4.5.) To illustrate this, we refer
the reader to Figures 11 and 12. The former depicts an NB-Tree
instance learned by ALETHEIA, whereas the latter presents a rep-
resentative fragment of a OneR model. The difference between the
two in granularity of judgment is obvious, favoring tree-based clas-
siﬁers for weakly biased policies.

Our conclusion, in summary, is that the decision of which clas-
siﬁcation algorithm to apply to a given ﬁltering scenario is context
sensitive, depending both on the policy expressed by the user and
on the warnings at hand. This conﬁrms H2, motivating why simpler
ﬁltering strategies are unlikely to achieve comparable performance
results.

Evaluation of H3.

Finally, to evaluate H3 we examine Figures 7 and 8. These rep-
resent data for up to 900 classiﬁed instances, which is well beyond
reasonable user effort in our opinion, the only purpose being to
validate whether there is indeed noticeable gain in classifying more
instances than the user can reasonably be asked to classify.

The graphs — for both precision and recall — provide a clear
negative answer: Roughly speaking, up to 200 classiﬁed instances

Figure 13: Policy Score as a Function of the Training-set Size,
where Policies Are Represented as Their Respective w Value

there is sharp improvement in classiﬁer quality, where the improve-
ment trend drops sharply in the range of 300 to 900 user classiﬁca-
tions. While there is still gain from additional classiﬁed instances
in some of the cases, returns are clearly diminishing.

As another source of conﬁrmation, we examine vertical slices
of Table 1. The answer, according to the data, is strictly negative.
As one data point, for w = 1 recall remains within the range of
0.97 − 0.99 and precision is restricted to the range 3.4 − 4.81 all
the way between 200 and 900 classiﬁed instances. The trend of
improvement is sublinear at best.
Indeed, 200 is ﬁguring as the
sweet spot, which also reafﬁrms H1.

Finally, we refer the reader to the graph in Figure 13. This graph
visualizes the highest score achieved per each of the policies as
a function of sample size. Somewhat unintuitively, the trend is not
monotonic, let alone linear. A key reason for this, in our analysis, is
that the data is unbalanced (many more false warnings compared to
true alarms). Hence, a naive classiﬁer can perform well by simply
classifying all of the alarms as false. Still, aside from the nonlinear
trends for small samples, all of the graphs in Figure 13 are visibly
plateauing beyond the cutoff value of 200. This again suggests
that returns (i.e., ﬁlter quality) fast diminish thanks to enlarging the
sample size.

6. RELATED WORK

The challenge of eliminating as many false positives as possible
from the report of a static analysis, without introducing an exces-
sive number of false negatives, has been the subject of numerous
research studies.

Junker, et al. [12] cast static-analysis clients as syntactic model-
checking problems. Violations of the veriﬁcation property result in
a counterexample (or path), which is checked for feasibility using
an SMT solver. If the path is infeasible, then the causes for its in-
feasibility are extracted via path slicing and an observer automaton
is constructed to exclude all paths sharing the same cause. Junker,
et al. have integrated their approach into the Goanna tool for static
analysis of C/C++ programs. In another study [6], Fehnker, et al.
share their experience in applying Goanna to large software sys-
tems consisting of 106 and more lines of code, including the Fire-
fox browser. They report on an excessive number of false alarms,
and address this usability issue by reﬁning the security rules of
Goanna. In contrast with many other analyses, the Goanna rules
are phrased as automata, which are translated into a Kripke struc-
ture for model checking. Ulike ALETHEIA, these works are strictly
concerned with reducing the number of false positives, without giv-
ing the user of the analysis the ﬂexibility to express a preference
that reﬂects a bias over true versus false positives.

Muske, et al. [15] tackle the problem of false warnings via a par-
titioning approach. They assume two partitioning phases: The ﬁrst

 0 0.2 0.4 0.6 0.8 1 50 100 150 200 250 300 350 400 450Policy ScoreTraining-set SizePolicy Score vs Training-set Sizew = 0w = 0.33w = 0.5w = 0.66w = 1.0771time <= 1862.5: NB 4
time > 1862.5
|

ruleName = jsCrossSiteRequestForgery : NB 2
ruleName = jsDOMXSSandOpenRedirect
|
|
|
time <= 4252 ? NB 6 : NB 7
ruleName = jsDOMCrossSiteScripting
|
time <= 1216.5 ? NB 9 : NB10
ruleName = MethodOverwrite: NB 11
ruleName = jsOpenRedirect: NB 12
ruleName = jsCodeInjection : NB 13
ruleName = jsPortManipulation : NB 14
ruleName = jsProtocolManipulation : NB 15

time <= 5042
|
|
|
|
|
|
|
|
|
|
|
|
time > 5042
|
|
|
|
|
|
|
|
|
|

sinkLineNo <= 89.5
|
|
|
|
|
sinkLineNo > 89.5: NB 25

sourceLineNo <= 27.5: NB 19
sourceLineNo > 27.5
numSteps <= 8.5
|
|
|
|
numSteps > 8.5: NB 24

numSteps <= 14.5
|
|
|
|
|
|
|
numSteps > 14.5
|

time <= 393223 ? NB 27 : NB 28

time <= 27839.5 ? NB 22 : NB 23

Figure 11: An NB-Tree Instance Computed by ALETHEIA as a
Candidate Filter (in Textual Weka Format)

divides the warnings into equivalence classes, assigning a leader to
each class, such that if the leader is a false warning, then all other
warnings are also false. The second step is to partition the leader
warnings. This is done based on the variables modiﬁed along the
path and their modiﬁcation points. The entire process is meant to
facilitate manual review of the warnings generated by a static anal-
ysis tool. The authors report on 50%-60% reduction in review ef-
fort corresponding to 60% redundant warnings on average, which
are more readily eliminated thanks to their technique. Just like for
ALETHEIA, the purpose of this work is to reduce the burden on the
analyst reviewing the results of a static analysis. The difference is
that this work does that by partitioning the analysis results, in such
a way that results with the same characteristics are grouped in the
same equivalence class, whereas ALETHEIA is based on an algo-
rithm that learns the characteristics of false positives and prevents
from presenting to the analyst ﬂows that are highly likely to lead to
other false positives.

An orthogonal approach that Muske, et al. propose [16] is to
ﬁrst apply a scalable yet imprecise abstract interpretation, and then
remove false warnings using a bounded model-checking technique,
which features the opposite tradeoff. The main goal is the elimina-
tion of false reports. This approach consists of two distinct tech-
niques to identify equivalence between assertions, thereby obviat-
ing the need to verify all but one of the assertions, and a third tech-
nique for skipping veriﬁcation of assertions that do not inﬂuence
any of the false alarms. However, this work does not allow the ana-
lyst to choose whether the ﬁlter applied to the result of an analysis
should err more towards the false-positive or true-positive side.

Johnson, et al. report on interviews with developers to learn the
reasons why static analysis tools are underused [11]. They con-
clude that the two main barriers to adoption of such tools are (i)
false positives and (ii) the way in which warnings are presented to
the developer. These are related: if the warning is hard to under-
stand, then false alarms become harder to identify and eliminate.

EFindBugs, developed by Shen, et al. [25], is an improved ver-
sion of the popular FindBugs tool [2] that addresses the excess of
false positives commonly reported by FindBugs. This is achieved
via a two-staged error ranking strategy. First, EFindBugs is applied

sinkURL:

−> NO

js

ﬁle :/.../52311/152673.
js −> NO
ﬁle :/.../3170/646.
js −> YES
ﬁle :/.../1065/183.
js −> YES
ﬁle :/.../644/51458.
js −> YES
ﬁle :/.../2751/183.
−> YES
ﬁle :/.../4978/4978. html
ﬁle :/.../449/449. html −> YES
−> NO
ﬁle :/.../35320/35320. html
−> NO
ﬁle :/.../7967/7967. html
ﬁle :/.../402/1501.
ﬁle :/.../1451995/1451995. html −> NO
...

js −> YES

Figure 12: Fragment of a OneR Instance Computed by
ALETHEIA as a Candidate Filter (in Textual Weka Format)

to a sample program. The resulting warnings are classiﬁed manu-
ally, and an approximate defect likelihood is assigned to each bug
category and bug kind. This determines the initial ranking of re-
ported bugs. Defect likelihood is later tuned in a self-adaptively
manner when EFindBugs is run on the user’s application thanks
to users’ feedback on reported warnings. This optimization pro-
cess is executed automatically and based on the correlations among
error reports with the same bug pattern. Unlike ALETHEIA, this ap-
proach does not offer the analyst the ability to ﬁnely tune the results
according to the speciﬁc needs of the analyst.

7. CONCLUSION AND FUTURE WORK

We have taken a step toward bridging the usability gap separat-
ing between developers and automated security checkers based on
static program analysis. The strongest detractor for developers is
the excess of false warnings and the difﬁculty to understand the
ﬁndings reported by the analysis tool (often because they are spu-
rious and represent infeasible program behaviors). Thus, in aiming
for completeness, static security checkers end up discouraging the
user to the point that the tool is not used at all even at the cost of
missing actual vulnerabilities that the tool is capable of ﬁnding.

In our approach, the report from ALETHEIA is cleansed by com-
bining user interaction with statistical learning techniques. The
user speciﬁes a policy, or preference, that reﬂects a bias over true
versus false warnings. The user also contributes classiﬁcations for
a small fragment of the overall warnings. These trigger the cre-
ation and evaluation of multiple candidate ﬁlters, which are each
evaluated according to the user-provided policy. The best ﬁlter is
then applied to the remaining warnings, leaving the user to review
only a (small) subset of the raw warnings output by the tool. Ex-
periments that we have performed over ALETHEIA are highly en-
couraging. For example, given only 200 classiﬁed warnings, if the
user expresses a preference toward preservation of true positives,
ALETHEIA is able to improve precision by a factor of 2.868 while
reducing recall by a negligible factor (×1.006). Conversely, if the
user favors elimination of false alarms, still based on only 200 clas-
siﬁed warnings, ALETHEIA achieves a recall-degradation factor of
only 2.212 and a precision-improvement factor of 9.014. Other

772policies are enforced in an equality effective manner. In all cases,
and across all policies, ALETHEIA is able to improve precision by
a factor ranging between ×2.868 and ×16.556 in return to user
classiﬁcation of only 200 warnings.

In the future, we intend to extend ALETHEIA to consider not
only different classiﬁcation algorithms but also different instantia-
tions of the same algorithm. This should ﬁne tune the resulting ﬁl-
ter, making it more effective. The challenge is to efﬁciently search
through the unbounded space of parameter conﬁgurations for each
of the candidate algorithms. We hope to address this challenge by
applying local search strategies guided by the shape of the AUC
curve computed for each of the algorithms. Another enhancement
that we plan to investigate is online reﬁnement: Instead of ask-
ing the user to eagerly tag several hundred warnings, the goal is to
apply ﬁltering incrementally, on the ﬂy, as the user classiﬁes warn-
ings. This offers immediate beneﬁt from every user labeling. An
important research question that the online setting raises is how to
order the warnings presented to the user for effective online ﬁlter-
ing and how sensitive the computation of the online ﬁlter is to the
choice and order of warnings.
8. REFERENCES
[1] N. Aronszajn. Theory of reproducing kernels. Transactions of the

American Mathematical Society, 68, 1950.

[2] N. Ayewah, W. Pugh, J. D. Morgenthaler, J. Penix, and Y. Zhou.
Using ﬁndbugs on production software. In OOPSLA Companion,
2007.

[3] C. M. Bishop. Pattern recognition and machine learning, volume 1.

Springer, 2006.

[4] J. G. Cleary, L. E. Trigg, et al. Kˆ*: An instance-based learner using

an entropic distance measure. In ICML, 1995.

[5] D. D. E. Denning and P. J. Denning. Certiﬁcation of programs for

secure information ﬂow. Commun. ACM, 20(7), 1977.

[6] A. Fehnker, R. Huuck, S. Seefried, and M. Tapp. Fade to grey:

Tuning static program analysis. ENTCS, 266, 2010.

[7] S. Guarnieri, M. Pistoia, O. Tripp, J. Dolby, S. Teilhet, and R. Berg.

Saving the world wide web from vulnerable javascript. In ISSTA,
2011.

[8] A. Guha, S. Krishnamurthi, and T. Jim. Using Static Analysis for

Ajax Intrusion Detection. In WWW, 2009.

[9] T. Hastie, R. Tibshirani, J. Friedman, T. Hastie, J. Friedman, and

R. Tibshirani. The elements of statistical learning, volume 2.
Springer, 2009.

[10] D. Heckerman, D. Geiger, and D. M. Chickering. Learning bayesian

networks: The combination of knowledge and statistical data.
Machine learning, 20(3), 1995.

[11] B. Johnson, Y. Song, E. Murphy-Hill, and R. Bowdidge. Why don’t
software developers use static analysis tools to ﬁnd bugs? In ICSE,
2013.

[12] M. Junker, R. Huuck, A. Fehnker, and A. Knapp. Smt-based false
positive elimination in static program analysis. In ICFEM, 2012.

[13] B. Livshits and M. S. Lam. Finding security vulnerabilities in java

applications with static analysis. In USENIX Security, 2005.

[14] S. McCamant and M. D. Ernst. Quantitative information ﬂow as

network ﬂow capacity. In PLDI, 2008.

[15] T. B. Muske, A. Baid, and T. Sanas. Review efforts reduction by

partitioning of static analysis warnings. In SCAM, 2013.

[16] T. B. Muske, A. Datar, M. Khanzode, and K. Madhukar. Efﬁcient
elimination of false positives using bounded model checking. In
VALID, 2013.

[17] C. G. Nevill-Manning, G. Holmes, and I. H. Witten. The

development of holte’s 1r classiﬁer. In Artiﬁcial Neural Networks
and Expert Systems, 1995. Proceedings., Second New Zealand
International Two-Stream Conference on. IEEE, 1995.

[18] A. Y. Ng and M. I. Jordan. On discriminative vs. generative

classiﬁers: A comparison of logistic regression and naive bayes.
Advances in neural information processing systems, 2, 2002.

[19] M. S. Pepe. The statistical evaluation of medical tests for

classiﬁcation and prediction. Oxford University Press, 2003.

[20] M. Pontil and A. Verri. Properties of support vector machines. Neural

Computation, 10, 1998.

[21] J. R. Quinlan. C4. 5: programs for machine learning, volume 1.

Morgan kaufmann, 1993.

[22] A. Sabelfeld and A. C. Myers. Language-based information-ﬂow

security. J-SAC, 21(1), 2006.

[23] S. Saitoh. Theory of reproducing kernels and its applications.

Longman, 1988.

[24] B. Schölkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. New

support vector algorithms. Neural Computation, 12, 2000.

[25] H. Shen, J. Fang, and J. Zhao. Eﬁndbugs: Effective error ranking for

ﬁndbugs. In ICST, 2011.

[26] T. Tateishi, M. Pistoia, and O. Tripp. Path- and index-sensitive string
analysis based on monadic second-order logic. TOSEM, 22(4), 2013.
[27] O. Tripp, P. Ferrara, and M. Pistoia. Hybrid security analysis of web

javascript code via dynamic partial evaluation. In ISSTA, 2014.

[28] O. Tripp, M. Pistoia, P. Cousot, R. Cousot, and S. Guarnieri.
Andromeda: Accurate and scalable security analysis of web
applications. In FASE, 2013.

[29] O. Tripp, M. Pistoia, S. J. Fink, M. Sridharan, and O. Weisman. TAJ:

Effective Taint Analysis of Web Applications. In PLDI, 2009.

[30] I. H. Witten and E. Frank. Data Mining: Practical machine learning

tools and techniques. Morgan Kaufmann, 2005.

APPENDIX
A. DETAILED EXPERIMENTAL RESULTS
The raw results of the experiments are summarized in Table 1.
For readability, the table is partitioned into sections according to the
number n of classiﬁed instances used by ALETHEIA to converge on
the best ﬁlter. These correspond to user effort (in performing man-
ual classiﬁcation of warnings), and so the table is organized around
this parameter. Precision and recall improvement are measured rel-
ative to those of the original security checker, without ALETHEIA,
which has perfect recall (being that it is conservative) yet low pre-
cision of 5% on the set of 3,758 warnings.

In a given section of Table 1, each row reﬂects the statistics for a
particular policy, governed by the value of w. The “Test Recall” and
“Test Precision” columns provide recall and precision statistics for
the tested instances (forming half of the classiﬁed instances made
available to ALETHEIA), whereas “Full Recall” and “Full Preci-
sion” reﬂect these measures for the remaining 3, 758− n warnings.
We also indicate the highest-ranking algorithm per policy (under
the “Model” column).

773w (Policy: w × r + (1 − w) × p)

Model

Test Recall Test Precision

Full Recall

Full Precision

0

1/4
1/2
3/4

1

0

1/4
1/2
3/4

1

0

1/4
1/2
3/4

1

0

1/4
1/2
3/4

1

0

1/4
1/2
3/4

1

0

1/4
1/2
3/4

1

0

1/4
1/2
3/4

1

0

1/4
1/2
3/4

1

0

1/4
1/2
3/4

1

Size of Classiﬁed Set: 100

NAIVEBAYES
NAIVEBAYES
ONER
ONER
ONER

0.31
0.31
0.93
0.93
0.93

Size of Classiﬁed Set: 200

SVM
SVM
J48
ONER
ONER

0.05
0.05
0.51
0.94
0.94

Size of Classiﬁed Set: 300

NBTREE
NBTREE
NBTREE
ONER
ONER

0.58
0.58
0.58
0.99
0.99

Size of Classiﬁed Set: 400

J48
J48
NBTREE
NBTREE
ONER

0.75
0.75
0.78
0.78
0.99

Size of Classiﬁed Set: 500

SVM
NBTREE
NBTREE
ONER
ONER

0.21
0.75
0.75
0.99
0.99

Size of Classiﬁed Set: 600

J48
J48
J48
NBTREE
ONER

0.77
0.77
0.77
0.79
0.99

Size of Classiﬁed Set: 700

J48
J48
NBTREE
NBTREE
ONER

0.75
0.75
0.78
0.78
0.99

Size of Classiﬁed Set: 800

SVM
J48
J48
NBTREE
ONER

0.3
0.75
0.75
0.81
0.98

Size of Classiﬁed Set: 900

SVM
J48
J48
NBTREE
ONER

0.32
0.8
0.8
0.83
0.98

10.83
10.83

2
2
2

16.86
16.86
12.84
4.05
4.05

16.7
16.7
16.7
3.37
3.37

19.17
19.17
18.78
18.78
4.33

19.65
18.07
18.07
3.97
3.97

18.4
18.4
18.4
17.85
4.24

19.17
19.17
18.78
18.78
4.33

19.18
18.85
18.85
16.84
4.42

19.12
19.09
19.09
17.6
4.83

0.36
0.36
0.76
0.76
0.76

0.12
0.12
0.42
0.97
0.97

0.58
0.58
0.58
0.99
0.99

0.77
0.77
0.76
0.76
0.98

0.23
0.79
0.79
0.99
0.99

0.69
0.69
0.69
0.8
0.98

0.77
0.77
0.76
0.76
0.98

0.29
0.84
0.84
0.83
0.98

0.34
0.8
0.8
0.77
0.98

12.74
12.74
4.17
4.17
4.17

18.4
18.4
9.44
3.4
3.4

15.16
15.16
15.16
3.34
3.34

18.33
18.33
18.49
18.49
4.36

18.79
18.69
18.69
3.95
3.95

18.05
18.05
18.05
18.85
4.22

18.33
18.33
18.49
18.49
4.36

18.4
18.79
18.79
17.65
4.53

19.05
19.27
19.27
18.09
4.81

Table 1: Per-policy Precision and Recall Statistics as a Function of Number of Classiﬁed Instances

774