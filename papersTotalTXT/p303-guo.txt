Understanding On-device Bufferbloat

For Cellular Upload

Yihua Guo†, Feng Qian(cid:92), Qi Alfred Chen†, Z. Morley Mao†, Subhabrata Sen‡

†University of Michigan

(cid:92)Indiana University

‡AT&T Labs – Research

{yhguo,alfchen,zmao}@umich.edu

fengqian@indiana.edu

sen@research.att.com

ABSTRACT
Despite the extensive characterization of the growth of cellu-
lar network trafﬁc, we observe two important trends not yet
thoroughly investigated. First, fueled by the LTE technology
and applications involving wearable devices and device-to-
device (D2D) communication, device upload trafﬁc is in-
creasingly popular. Second, the multi-tasking and multi-
window features of modern mobile devices allow many con-
current TCP connections, resulting in potentially complex
interactions. Motivated by these new observations, we con-
duct to our knowledge the ﬁrst comprehensive characteriza-
tion of cellular upload trafﬁc and investigate its interaction
with other concurrent trafﬁc. In particular, we reveal rather
poor performance associated with applications running con-
currently with cellular upload trafﬁc, due to excessive on-
device buffering (i.e., on-device bufferbloat). This leads to
signiﬁcant performance degradation on real mobile appli-
cations, e.g., 66% of download throughput degradation and
more than doubling of page load times. We further system-
atically study a wide range of solutions for mitigating on-
device bufferbloat, and provide concrete recommendations
by proposing a system called QCUT to control the ﬁrmware
buffer occupancy from the OS kernel.

Keywords
Upload; Bufferbloat; Cellular Networks; Radio Firmware

1.

INTRODUCTION

The explosive growth of mobile devices and cellular net-
works shows no sign of slowing down. We notice two im-
portant trends not well explored in previous work, namely
user-generated trafﬁc and multi-tasking.

Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
IMC 2016, November 14-16, 2016, Santa Monica, CA, USA
c(cid:13) 2016 ACM. ISBN 978-1-4503-4526-2/16/11. . . $15.00
DOI: http://dx.doi.org/10.1145/2987443.2987490

On one hand, the mobile trafﬁc paradigm is undergoing
a shift from being dominated by download to a mix of both
download and upload, due to a wide range of emerging apps
enabling user-generated trafﬁc such as media content up-
load to social networks (e.g., Facebook videos), background
synchronization, cloud-based ofﬂoading, HD video chat,
machine-to-machine (M2M), and device-to-device (D2D)
communication, etc. The prevalence of upload is further
fueled by increasingly ubiquitous access to LTE networks,
which provides uplink bandwidth of up to 20Mbps.

On the other hand, the frequent use and rich function
of mobile devices give rise to multi-tasking, which en-
ables users to interact with multiple applications at the same
time. Previously, due to the limitation of mobile operat-
ing system and the device processing power, older phones
and Android systems only support a single application at
foreground interacting with the user. Newer phones allow
users to use multiple apps simultaneously (a.k.a. “multi-
window”). Even without multi-tasking, a single foreground
app can still trigger multiple concurrent TCP ﬂows. A re-
cent study [20] shows that around 28% of the time for each
mobile user there are concurrent TCP ﬂows.

Motivated by the above, in this paper, we conduct to our
knowledge the ﬁrst comprehensive, quantitative, and cross-
layer measurement study of cellular upload trafﬁc and its in-
teraction with concurrent trafﬁc, using combined approaches
of analyzing large network traces and conducting controlled
lab experiments. Our contributions consist of the following.
Characterization of upload trafﬁc (§3). We character-
ized the cellular upload trafﬁc by measuring its volume, du-
ration, rate, concurrency, and impact on latency from a large
network trace collected from an IRB-approved user study1
involving 15 users for 33 months. We found that although
the majority of today’s smartphone trafﬁc is still download,
the upload trafﬁc can be signiﬁcant. Upload can last for up
to 11 minutes with 226 MB of data transferred. In particu-
lar, the upload speed can achieve up to 12.8Mbps (median
2.2Mbps for 10MB+ ﬂows) in today’s LTE networks, fa-

1This study was conducted entirely with data collected
from active and passive measurements at the University of
Michigan and was approved by the University of Michi-
gan IRB (Institutional Review Board) approval number
HUM00111075.

303cilitating many applications that upload rich user-generated
trafﬁc. We also found that large upload tends to have higher
RTT, and upload trafﬁc may also increase the RTT experi-
enced by concurrent download.

An anatomy of on-device queuing for upload trafﬁc
(§4.1-§4.3). For cellular upload, we found a signiﬁcant frac-
tion of the end-to-end latency occurs on the end-host device
instead of in the network, due to the large buffer inside the
mobile device. Speciﬁcally, we made two key observations.
First, contrary to the common understanding [41, 22] that
(i) excessive queuing delay (a.k.a. “bufferbloat”) happens
mostly inside the network (or near-edge network elements),
and (ii) cellular upload trafﬁc is less likely to incur queu-
ing delay due to its low data rate, our ﬁndings suggest that
bufferbloat for upload trafﬁc can frequently occur on mobile
devices accessing diverse types of networks (HSPA, LTE,
and even Wi-Fi), across different devices. On-device buffer-
bloat can cause signiﬁcant latency increase up to 4 seconds,
or 100x of the network RTT, on off-the-shelf Android de-
vices. This implies that when an upload is in progress, the
on-device queuing delay in fact eclipses the network delay.
Second, we identiﬁed the root cause of such excessive on-
device queuing. It can happen at different layers including
application buffer, OS TCP buffer, and the Queuing Dis-
cipline (Qdisc) in the OS kernel.
In particular, we found
excessive queuing also frequently happens at the cellular
ﬁrmware buffer, whose occupancy can, for example, reach
up to 300KB while accounting for 49% of the end-to-end
delay when the uplink bandwidth is 2Mbps. More impor-
tantly, the cellular ﬁrmware buffer distinguishes itself from
other on-device buffers in that its occupancy plays a role in
the cellular control plane which in turn affects base station’s
scheduling decision and achievable uplink throughput.

Accurate achievable uplink throughput estimation
(§4.4). The excessive buffering at various layers makes ac-
curate estimation of achievable cellular uplink throughput
challenging. We found that surprisingly, using the same al-
gorithm, the throughput estimated at upper layers (TCP and
Qdisc) deviates from the lower-layer throughput estimation
by 136% and 70% on average. We proposed a method that
accurately infers the uplink throughput by leveraging lower-
layer information from cellular control-plane messages.

Quantifying the impact of uplink bufferbloat (§5). We
illustrated that large upload trafﬁc signiﬁcantly affects the
performance of concurrent TCP download, whose average
RTT is increased by 91% and average throughput is reduced
by 66%. We found such severe performance degradation
is predominantly caused by on-device buffers, because the
uplink ACK stream of TCP download shares the same Qdisc
and cellular ﬁrmware buffers with concurrent upload, and
is thus delayed mainly due to the on-device bufferbloat. We
further quantitatively demonstrated concurrent upload incurs
signiﬁcant user experience degradation on real applications,
including web browsing (219% to 607% increase of page
load time with concurrent upload), video streaming (57%
reduction of playback bitrate and frequent stalls), and VoIP.
Mitigating on-device bufferbloat (§6,§7). Due to the
uniqueness of on-device bufferbloat, we found existing mit-

igation solutions, such as changing TCP congestion control,
tuning TCP buffer size, reducing Qdisc sizes [7], prioritiz-
ing delay-sensitive trafﬁc, and applying Active Queue Man-
agement (e.g., CoDel [30] and PIE [34]) are not capable of
effectively mitigating the excessive buffering. This is be-
cause (i) they cannot be directly implemented at the cellular
device driver, and (ii) they are unaware of the interplay be-
tween the driver buffer and the cellular control plane. We
therefore designed and implemented a new solution called
QCUT to control the ﬁrmware buffer occupancy from the
OS kernel. QCUT is general (independent of a particular
driver implementation), lightweight, and effective. Our lab
experiments show that QCUT effectively reduces the cellular
ﬁrmware queuing delay by more than 96% while incurring
little degradation of uplink throughput. We deploy QCUT
on a user study involving 5 users for one week. The results
indicate QCUT signiﬁcantly improves the application QoE
when concurrent upload is present.

Although we identiﬁed the on-device bufferbloat problem
in today’s HSPA/LTE networks, we anticipate it continues to
affect future wireless technologies, whose uplink bandwidth
will remain below the downlink bandwidth (e.g., 50Mbps vs.
150Mbps for LTE Advanced [3]) causing the cellular uplink
to often remain the end-to-end bottleneck link. More impor-
tantly, higher network speed and cheaper memory facilitate
device vendors and cellular carriers to deploy larger buffers
that exacerbate on-device (and in-network) bufferbloat.

Paper Organization. After describing the experimental
methodology in §2, we conduct a measurement study of to-
day’s upload trafﬁc in §3. We reveal the on-device queuing
problem in §4, and quantify the impact of upload on mobile
apps in §5. We then describe how QCUT mitigates on-device
queuing in §6 and evaluate QCUT as well as existing mitiga-
tion strategies in §7. We discuss related work in §8 before
concluding the paper in §9.

2. EXPERIMENTAL METHODOLOGY
To comprehensively study cellular upload trafﬁc, we car-
ried out controlled lab experiments (§2.1) and analyzed data
collected from a user study with 15 participants (§2.2).
2.1 Controlled Local Experiments

We conduct controlled experiments using off-the-shelf
smartphones and commercial cellular networks. Our devices
consist of the following: (i) Samsung Galaxy S3 running An-
droid 4.4.4 with Linux kernel version 3.4.104, using Carrier
1’s LTE network2, (ii) Samsung Galaxy S4 running Android
4.2.2 with Linux kernel version 3.4.0, using Carrier 1’s LTE
network, (iii) Samsung Galaxy S3 running Android 4.0.4
with Linux kernel version 3.0.8, with access to Carrier 2’s
3G network and (iv) Nexus 5 running Android 6.0.1 with
Linux kernel version 3.4, using Carrier 1’s LTE network.
We also set up dedicated servers located at the University of
Michigan with 64-core 2.6GHz CPU, 128GB memory, 64-
bit Ubuntu 14.04 OS for experiments. Both mobile phones
2We anonymized three large U.S. cellular carriers’ names as
Carrier 1, 2, and 3.

304Figure 1: Trafﬁc volume distribution of user sessions.

and the servers use TCP CUBIC [19], the default TCP vari-
ant for Linux/Android, unless otherwise mentioned. We
conducted the experiments during off-peak hours. For each
setting, we repeat the experiment for at least 10 times and
report the average metrics unless otherwise noted.

For TCP throughput and RTT measurement, the mobile
device ﬁrst establishes a TCP connection to one of the ded-
icated servers. Then this TCP connection is used to trans-
fer random data without interruption. For bidirectional data
transfer (i.e., simultaneous upload and download) experi-
ments, the mobile device establishes two TCP connections
to two servers, one for download and the other for upload to
eliminate server-speciﬁc bottlenecks. To measure through-
put, we ignore the ﬁrst 10 seconds to skip the slow start pe-
riod and calculate the throughput every 500ms from trans-
ferred data.
2.2 Network Traces from a User Study

We also leveraged network traces collected from an
IRB-approved smartphone user study by distributing instru-
mented Samsung Galaxy S3 phones to 15 students. Each of
the 15 participants was also given unlimited LTE data plan.
The phones were instrumented with data collection software
(with very low runtime overhead). It continuously runs in
the background and collects full packet traces in tcpdump
format including both headers and payload. We collected
900GB of data in total from January 2013 to October 2015.
We used an idle timing gap of 1 minute to separate user ses-
sions of the same device.

3. UPLOAD TRAFFIC

MEASUREMENT

In this section, we perform a measurement study of upload
trafﬁc in today’s cellular networks, using the traces collected
from our user study (§2.2).

Trafﬁc volume. Figure 1 plots the distributions of down-
load, upload, and overall trafﬁc volume of user sessions. We
only consider TCP/UDP payload size when computing the
session size. Today’s mobile trafﬁc is dominated by down-
load, whose average size is about one order of magnitude
larger than that of upload. About 2.2% of user sessions carry
more than 1MB of downlink bytes, while only 0.4% upload
more than 1MB data in the user study trace. We notice a ma-
jor source of user-consumed trafﬁc is video, which accounts
for about half of download trafﬁc.

(a) Large upload

(b) Large download

Figure 2: Flow duration distributions.

(a) UL vs. DL

(b) Large upload

Figure 3: Flow rate distributions.

However, we observed that the fraction of upload bytes
is indeed non-trivial. Within the top 20% of user sessions
(in terms of their overall transferred bytes), the 25th, 50th,
and 75th percentiles of the fractions of upload trafﬁc are 9%,
20%, and 42%. Across all user sessions, the corresponding
fractions are higher, i.e., 19%, 50%, and 57%. The upload of
one user session even lasts for 11 minutes with 226 MB of
data transferred. We expect that in the future, the fraction of
upload trafﬁc will keep increasing because of the increas-
ingly popular user-generated trafﬁc. Compared to smart-
phones, wearable and IoT devices may incur even more up-
load trafﬁc, due to their ubiquitous sensing capabilities.

Flow duration. We use inter-packet arrival time to divide
a TCP ﬂow into multiple segments with a threshold of 1 sec-
ond. We also use the threshold of 1 second to eliminate idle
period of a TCP ﬂow. We then divide these segments into
upload bursts that only have TCP payload in uplink, and
download bursts with only TCP downlink payload, based
on the direction of transferred TCP payload. Given a TCP
ﬂow, we deﬁne its upload duration as the total duration of
all uplink bursts. Similarly, the download duration is the
total duration of all downlink bursts. Figure 2(a) plots the
upload durations for ﬂows with large upload trafﬁc volume
(100KB to 1MB, 1MB to 10MB, and at least 10MB). As ex-
pected, larger ﬂows tend to be longer in duration. For ﬂows
with large download trafﬁc volume, as shown in Figure 2(b),
their download duration exhibits distributions qualitatively
similar to those of upload duration, yet with the main differ-
ence being that the download duration is statistically shorter,
largely due to the higher downlink bandwidth compared to
the uplink bandwidth, as to be measured next.

Flow rate. We compute the upload (download) rate of

 0 0.25 0.5 0.75 1 0.001 0.01 0.1 1 10 100CDFSession size (MB)UploadDownloadUpload + Download 0 0.25 0.5 0.75 1 0.01 0.1 1 10 100 1000CDFUpload duration (s)0.1-1 MB1-10 MB> 10 MB 0 0.25 0.5 0.75 1 0.01 0.1 1 10 100 1000CDFDownload duration (s)0.1-1 MB1-10 MB> 10 MB 0 0.25 0.5 0.75 1 0.001 0.01 0.1 1 10 100CDFFlow rate (Mbps)UploadDownload 0 0.25 0.5 0.75 1 0.001 0.01 0.1 1 10CDFUpload rate (Mbps)0.1-1 MB1-10 MB> 10 MB305upload stream of the TCP ﬂow that p belongs to. Third, we
plot in Figure 4(a) the distributions of I(s) across all slots
grouped by their upload size. As shown, the upload RTT is
indeed highly “inﬂatable”, and larger upload tends to incur
much higher RTT. This resembles the “bufferbloat” effect
that is well studied for download [41, 22], and motivates us
to conduct a comprehensive investigation of bufferbloat for
cellular upload in §4.

Impact of Upload on Download Latency. We are also
interested in how upload impacts download latency, which
is quantiﬁed as follows. We ﬁrst generate one-second slots
using a similar way as employed in Figure 4(a), but this time
we only keep slots with both upload and download trafﬁc.
Then for every slot, we compute the average on-device de-
lay of download. Note that since our user study traces were
collected on client devices, we are only able to measure the
on-device component of the download RTT i.e., t1 shown in
Figure 6(b). Next, in Figure 4(b), we plot the distributions
of the on-device download delay grouped by the size of per-
slot upload size as is also done in Figure 4(a). We clearly
observe that concurrent upload affects the on-device down-
load delay because the ACK stream (for download) and the
data stream (for upload) share several on-device buffers. We
will conduct an in-depth investigation on this in §4 and §5.
Summary. Overall, we found that although the majority
of today’s smartphone trafﬁc remains to be download, the
upload trafﬁc can still be large. In particular, the median up-
load speed is 2.2Mbps for 10MB+ ﬂows and can achieve up
to 12.8Mbps in today’s LTE networks, enabling many appli-
cations to upload rich user-generated trafﬁc. We also found
that large upload tends to have higher RTT, and upload trafﬁc
may also increase the RTT experienced by concurrent down-
load. Furthermore, it is quite common that multiple TCP
ﬂows are transferring data concurrently on a mobile device,
leading to complex interactions possibly among uplink and
downlink ﬂows to be investigated soon.

4. ON-DEVICE QUEUING DELAY OF

UPLOAD TRAFFIC

We conduct a thorough analysis of the latency character-
istics for cellular upload trafﬁc. We found a signiﬁcant frac-
tion of the latency happens on the end-host device instead
of in the network (§4.1). In particular, in §4.2, we discover
the root cause of large Qdisc and ﬁrmware buffers playing
major roles in causing the excessive on-device delay, whose
prevalence across devices and carriers are shown in §4.3. We
also found in §4.4 that on-device queuing may signiﬁcantly
impact accurate uplink throughput estimation.
4.1 Overall Delay Characterization

When a mobile device is uploading data, its packets will
traverse various buffers in the protocol stack, as illustrated
in Figure 6: TCP buffers, link-layer buffers (Linux queuing
discipline), radio ﬁrmware buffers. Each buffer may incur
queuing delay. As a result, we may get different RTT values
if we conduct measurements at different layers. In this work,
we focus on three RTT measurements deﬁned bellow.

(a) RTT increase of upload (b) On-device delay of

download

Figure 4: Delay distributions.

a TCP ﬂow by dividing the total bytes of all upload (down-
load) bursts by its upload (download) duration that is deﬁned
above. We only consider ﬂows whose upload/download du-
ration are longer than a threshold, which is empirically cho-
sen to be 3 seconds, as the “rate” of a very short ﬂow is not
very meaningful. Figure 3(a) compares upload and down-
load rates for the user study trace. Statistically, download
is faster than upload, largely due to their differences in the
underlying channel rates of the LTE radio access network.
On the other hand, Figure 3(b) indicates larger upload ﬂows
(larger than 1MB) tend to have higher rates.
In the user
study dataset, for ﬂows that upload 1 to 10 MB data, their
25%, 50%, and 75% percentiles of upload rates are about
1.4Mbps, 2.4Mbps, and 3.8Mbps, which are comparable or
even higher than those of today’s many residential broad-
band networks. For 10MB+ ﬂows, the maximum achieved
throughput are 12.8Mbps for the user study traces. Such
high upload speed provides the infrastructural support for
user-generated trafﬁc.

Flow concurrency. We explore the concurrency of TCP
ﬂows per user. The result is shown in Figure 5. For every
one-second slot in each user session, we count the number
of TCP ﬂows that are transferring data. For the user study
trace, for 28.2% of the time (i.e., 28.2% of the one-second
slots across all user sessions), there exist at least two TCP
connections that perform either upload or download. The
results indicate that concurrent TCP transfers are quite com-
mon on today’s mobile devices. Motivated by this, we study
the interplay between uplink and downlink trafﬁc, a previ-
ously under-explored type of concurrency, in §5.

RTT Dynamics. We next study the RTT dynamics of cel-
lular upload from the user study trace. The RTT is mea-
sured by timing the timestamp difference between each up-
link TCP data packet and its corresponding ACK packet cap-
tured by tcpdump. We then study the ﬂuctuation of upload
RTT using the following methodology. First, we split each
user session into one-second slots and discard slots without
uplink trafﬁc. We also discard slots whose download trafﬁc
volume is non-trivial (using 5KB as a threshold). The pur-
pose is to eliminate the impact of concurrent download on
upload. Second, for each one-second slot s, we compute its
RTT increase I(s) = meanp{RTT(p) − MinRTT(p.f low)}
over all data packets {p} within the slot. RTT(p) is the mea-
sured RTT, and MinRTT(p.f low) is the minimum RTT of

 0 0.25 0.5 0.75 1 0.01 0.1 1CDFRTT increase (s)UL < 5 KBUL 5-100 KBUL > 100 KB 0.8 0.85 0.9 0.95 1 0.01 0.1 1CDFOn-device Delay (s)UL < 5 KBUL 5-100 KBUL > 100 KB306Figure 5: TCP concurrency distribution.

Figure 6: On-device bufferbloat (in thick blue).

calculated by subtracting RT TF measured from tcpdump
trace by the estimated queuing delay. The results of two
representative experiments with different uplink bandwidth
(2Mbps and 8Mbps, which are 50% and 98% percentiles of
upload rates measured from Figure 3(b), respectively) are
shown in Figure 7. Note the Y axes are in log scale.

As shown in both plots of Figure 7, at the beginning
of the TCP upload, RT TF increases steadily, and quickly
outweighs RT TB. When the uplink bandwidth is 2Mbps
(8Mbps), after 2MB (4MB) of data has been sent out, RT TF
is increased to around 1.3s (330ms), which is much larger
than RT TB maintaining stably at around 50ms. Meanwhile,
RT TQ starts to exceed RT TF , and becomes twice as large
as RT TF after another 2MB of data is uploaded. The abso-
lute difference between RT TQ and RT TF is as high as 3s
and 680ms in Figure 7(a) and 7(b), respectively.

Overall, we found that for cellular upload, surprisingly,
the RTT observed by mobile devices’ TCP stack (RT TQ)
can be signiﬁcantly larger than the RTT perceived by
tcpdump (RT TF ), which further far exceeds the pure net-
work RTT (RT TB). Depending on the uplink bandwidth,
RT TQ and RT TF can be 22x∼100x and 6x∼24x of RT TB,
respectively, during the steady phase of TCP bulk upload.
We call such a phenomenon on-device bufferbloat since it
is caused by excessive queuing delay on the mobile de-
vice, as opposed to the network, which is regarded as the
main source of excessive queuing for cellular downlink traf-
ﬁc [22]. As we will demonstrate later, on-device bufferbloat
has deep implications on, for example, uplink bandwidth es-
timation, multi-tasking performance, uplink scheduling al-
gorithms, and on-device buffer management.
4.2 Root Cause of On-device Queuing

We now explore the root cause of the excessive on-device
queuing delay. We begin with an overview of how outgo-
ing TCP packets on the sending path traverse the Linux ker-
nel (also used by Android) and radio chipset. As shown
in Figure 8, an application invokes the send() system call
at time tsA and the data is put into TCP buffer by kernel
through TCP sockets at time tsT . Note tsT may be later
than tsA if the socket is blocked. In Linux, a packet is stored

(a) Uplink BW 2Mbps

(b) Uplink BW 8Mbps

Figure 7: Overall latency characterization for a single
TCP upload ﬂow under two network conditions.

• RT TB consists of only the delay a packet experiencing
in the network. It does not include any delay caused by on-
device buffer (B stands for “base”).
• RT TF includes RT TB and the delay incurred by the
buffer in the radio ﬁrmware, which usually resides on the
cellular chipset of a mobile device (F stands for “ﬁrmware”).
• RT TQ includes RT TF , plus the delay incurred by the
queuing discipline (Qdisc), the link-layer buffer in the main
memory managed by the OS (Q stands for “Qdisc”).

Similarly, we can also deﬁne RTTs measured at higher
layers (TCP, application). Nevertheless, RT TB, RT TF , and
RT TQ are our particular interests, because their correspond-
ing network path or buffers are shared by multiple applica-
tions. As we will show in §5, if upload and delay sensitive
trafﬁc coexist, the former may severely interfere with the
latter due to the shared nature of lower-layer buffers. In con-
trast, the higher-layer buffers are usually not shared.

We now measure RT TB, RT TF , and RT TQ by perform-
ing bulk data upload over TCP on Samsung Galaxy S3, us-
ing Carrier 1’s LTE network. RT TQ and RT TF can be di-
rectly measured by tcp_probe [8] and tcpdump, respec-
tively. RT TB can only be indirectly estimated. For a given
upload trace, we keep track of the buffer occupancy and en-
queue/dequeue rates of the ﬁrmware buffer. We then use
them to estimate the ﬁrmware queuing delay3. RT TB is then

3The detailed methodology of ﬁrmware buffer occupancy
estimation is described in §6.2. Since the radio ﬁrmware we
use only reports ﬁrmware buffer occupancy of up to 150KB,

we validate our methodology when the occupancy is smaller
than 150KB and then use it to infer the buffer occupancy at
any time in the trace.

 0 0.25 0.5 0.75 1 0 2 4 6 8 10CDFTCP Concurrency (# of conns)TCPAppli-cationQdiscRadio FirmwareUE/NetworkBoundaryServerRTTBRTTFRTTQTFTQTTTADataACK(a) Bulk uploadTCPUE/NetworkBoundaryDataACK(b) Download when another bulk upload  flow exists in backgroundServertsAtsTtsQtsFtsNt1t2 1 0.04 0.4 4 0 2 4 6 8 10RTT (s) (log scale)Data sent (MB)RTTQRTTFRTTB 0.6 1 0.04 0.4 4 0 2 4 6 8 10RTT (s) (log scale)Data sent (MB)RTTQRTTFRTTB307Figure 9: On-device queuing delay on diverse devices
and cellular carriers.

a strong correlation (around 0.86) between the queuing delay
and the amount of trafﬁc in Qdisc.

Firmware Queuing. In LTE uplink, the data to be trans-
mitted from applications is processed and queued in the RLC
(Radio Link Control) buffer4, which is physically located in
the cellular chipset ﬁrmware. The amount of data available
for transmission on the UE (i.e., the ﬁrmware buffer occu-
pancy) is provided to the eNodeB through control messages
called Buffer Status Reports (BSR). BSR can report 64 lev-
els of buffer size with each level representing a buffer size
range [9]. The highest level of BSR is 150KB or above.
Based on BSR from all UEs, the eNodeB uplink scheduler
assigns network resources to each UE for uplink transmis-
sion in a centralized manner. The eNodeB sends control
messages called Scheduling Grants to inform a UE of the
scheduling decision. A UE is only allowed to transmit on
the physical uplink shared channel (PUSCH) with a valid
grant.

We observed that the BSR quickly increases to the high-
est level (150KB+) when there is large LTE upload trafﬁc.
Also there exists a strong correlation (around 0.73) between
RT TF and buffer level in BSR. Leveraging the BSR infor-
mation, we measured that the actual ﬁrmware buffer occu-
pancy can reach several hundreds of KBs (using a more ac-
curate algorithm described in §6.2), and the ﬁrmware queu-
ing delay (TF ) can reach 400ms with 8Mbps uplink. By
subtracting the RT TF by the ﬁrmware queuing delay, we
can estimate RT TB, which is constantly low (e.g., around
50ms for Carrier 1), as shown in Figure 7.

Overall, the above ﬁndings have two important implica-
tions. First, cellular uplink scheduling is performed in a
centralized manner, different from that in Wi-Fi networks
where clients autonomously sense the wireless channel to
transmit data and avoid collision in a distributed way. Sec-
ond, the ﬁrmware buffer distinguishes itself from other on-
device buffers in that its occupancy plays a role in the cellu-
lar control plane which in turn affects eNodeB’s scheduling
decisions and the achievable uplink throughput.
4.3 Prevalence across Carriers & Devices
We show the prevalence of on-device bufferbloat in Fig-
ure 9 by repeating the upload experiments on various net-
4In the remainder of this paper, we use the general term
“ﬁrmware buffer” to refer to the RLC buffer.

Figure 8: Packet processing and transmission on An-
droid devices.

in a data structure called skb. A TCP packet is encapsu-
lated into skb (with its TCP header being added) and sent
to IP layer in tcp_transmit_skb() at time tsQ. After be-
ing processed at the IP layer, the skb with TCP/IP header
is subsequently injected to the queuing discipline (Qdisc)
when dev_queue_xmit() is called. When the driver is
ready to transmit more data, dev_hard_start_xmit() is
called by the kernel to dequeue the packet from Qdisc to the
driver buffer at time tsD. Similarly, when the radio ﬁrmware
of the chipset is ready to receive a packet from the driver,
ndo_start_xmit() will be called to enqueue the packet to
the ﬁrmware buffer at time tsF . The kernel usually does not
have direct control over the logic of radio ﬁrmware, which
determines when to actually transmit the packet at time tsN .
As mentioned in §4.1, in this study we focus on the queu-
ing delay below the transport layer, as lower-layer buffers are
usually shared, causing potential interference across multi-
ple apps. We now describe lower-layer queuing in details.

In-kernel Queuing. To identify where on-device queuing
occurs exactly in the kernel, we use Linux kernel debugging
tool jprobe to log timestamps of the aforementioned func-
tion calls. We found that the queuing delay in the queuing
discipline (Qdisc), denoted as TQ, is almost identical to the
difference between RT TQ and RT TF . Besides, the delay
between tcp_transmit_skb() and dev_queue_xmit(),
as well as the driver queuing delay (TD) are negligible. This
indicates that when sending out trafﬁc in cellular networks,
packet queuing in Qdisc dominates the on-device queuing
delay in the kernel. Also, as another validation, we observe

App1App2AppNApplication LayerTransportLayerApplication data into TCP sockets.  .  .tcp_transmit_skb()ACKLinuxQdiscQdisc enqueue: dev_queue_xmit()Queueing Disciplinedev_hard_start_xmit()TQ =RTTQ - RTTFDeviceDriverRadioFirmwarendo_start_xmit()Driver buffertcpdumpTD ≈ 0 PHY Modulation and CodingtsTtsQtsDtsFtsNtTCP buffer(per connection)TT       Firmware bufferTF =RTTF - RTTBKernel spaceUserspaceRadiochipsetNetworktsA 1 10 100 1000SGS3 Carrier 1LTESGS3Carrier 1HSPA+SGS3Carrier 2LTESGS3Carrier 2HSPA+SGS3Carrier 3LTEHTC One SCarrier 1HSPA+Delay (ms)TQRTTF308Throughput

RTT

% AVG
decrease

% RSD % AVG % RSD
increase
increase

increase

(a) Measurement error with
20ms interval

(b) Measurement error with
100ms interval

Figure 10: Uplink throughput measurement error at dif-
ferent layers.

works using two different devices. For each setting, we re-
port the 5th, 25th, 50th, 75th, and 95th percentiles of TQ
and RT TF . We observe on-device bufferbloat on all set-
tings in LTE, with median TQ larger than 200ms. Regarding
the HSPA+ network, TQ is small (around 20ms) for SGS3
using Carrier 2. This is because the TCP sending buffer size
is conﬁgured to be small by Carrier 2 on this device (we will
discuss the impact of TCP buffer size in §6.1). Yet across all
settings, we found that the RT TF is much larger than the es-
timated RT TB, indicating that excessive ﬁrmware queuing
happens on all devices and carriers.
4.4 Uplink Throughput Measurement

Often applications (e.g., real-time multimedia apps) need
to know the instantaneous network throughput. The lower-
layer information provided by ﬁrmware enables accurate
cellular throughput measurement. Recall in §4.2 that the
UE can only send the amount of data up to the scheduling
grant.
If a portion of the grant is not used, the ﬁrmware
uses padding to indicate the unused part. The padding size
is also reported by the ﬁrmware. Therefore, by subtracting
the scheduling grant by the padding size, we can calculate
the amount of data sent out from the device, as well as the
uplink throughput (the padding is not transmitted).

Since the above approach directly utilizes lower-layer in-
formation from the cellular control plane, it gives the ground
truth of cellular uplink throughput. An interesting ques-
tion is, compared to this ground truth, how accurate is the
throughput measured at upper layers? We quantify this in
Figure 10, which plots the measurement error at Qdisc, TCP,
and application layer where we use a slide window of 100ms
and 20ms to estimate uplink throughput during a bulk up-
load. The results indicate that the throughput estimation
at higher layers are highly inaccurate, with the root mean
square being 141%, 136%, and 70% at the application layer,
the transport layer, and the Qdisc, respectively, when the
estimation interval is 100ms. Reducing the interval further
worsens the accuracy. The root cause of such inaccuracy is
again the on-device bufferbloat: when a higher layer delivers
a potentially large chunk of data into large low-layer buffers,
the higher layer thinks the data is sent out but the data will
stay in the buffer for a long time. In fact the higher layer has
no way to know when the data actually leaves the device.

SGS3 C1 LTE

SGS3 C1 HSPA+

SGS3 C2 LTE
SGS3 C3 LTE
HTC One S
C1 HSPA+

66
8
10
80
22

253
25
36
192
260

91
7
10
86
10

37
9
20
42
91

Table 1: Impact of upload on download performance on
different devices, vendors, and networks (C1, C2, and C3
refer to Carrier 1, 2, and 3 respectively).

As indicated in Figure 10, as the location of measurement
moves to higher layers, the overall on-device buffer size in-
creases, leading to worse estimation accuracy.

5.

IMPACT OF UPLOAD ON MOBILE
APPLICATION PERFORMANCE

This section quantiﬁes the impact of upload on some pop-
ular applications’ performance: ﬁle download, web brows-
ing, video streaming, and VoIP. We compare user-perceived
application performance in two scenarios: without and with
concurrent upload. The experiments in this section were
conducted on a Samsung Galaxy S3 phone using Carrier 1’s
LTE network unless otherwise mentioned. We use a single
TCP connection to generate upload trafﬁc in controlled ex-
periments.
5.1 Impact of Upload on Bulk Download
When upload and download exist concurrently, upload
trafﬁc can affect download trafﬁc in two ways: in-network
and on-device. The former is well-known [47]: upload
data shares the same network link with TCP ACK pack-
ets of download data, leading to potentially delayed uplink
ACK for download. This can cause the server to retransmit
download data and reduce the congestion window size, ulti-
mately leading to lower download throughput. On the other
hand, the on-device queuing delay triggered by upload can
also severely affect download by delaying its ACK packets
(shown as t1 in Figure 6(b)), since when download and up-
load trafﬁc coexist, uplink TCP ACKs share the same queues
(e.g., Qdisc and ﬁrmware buffers) with uplink data, as de-
tailed in §4.2.

We carried out experiments of running a one-minute TCP
download ﬂow with and without a concurrent TCP upload
ﬂow on different devices and carriers with the setup de-
scribed in §2.1. Table 1 quantiﬁes the impact of upload
on download in four aspects, using bulk download in ab-
sence of upload as the baseline: (i) decrease of the aver-
age (AVG) download throughput, (ii) increase of the rela-
tive standard deviation (RSD)5 of download throughput, (iii)
increase of AVG RTT, and (iv) increase of RSD of RTT.
5Relative standard deviation (RSD) = standard deviation /
mean.

 0 0.25 0.5 0.75 1-100-50 0 50 100CDFEstimation error (%)AppTCPQdisc 0 0.25 0.5 0.75 1-100-50 0 50 100CDFEstimation error (%)AppTCPQdisc309Figure 11:
TCP/UDP throughput.

Impact of Uplink trafﬁc on downlink

All carriers exhibit performance degradations in various de-
grees. In particular, large ﬂuctuation of throughput and RTT
exists when there is background upload, posing challenges
for user-interactive applications. We also compare the in-
network and the on-device impact of upload on download
trafﬁc, by computing t1/t2 in Figure 6(b). The mean and
median values of the fractions of t1 in t2 are as high as 63%
and 74%, indicating the on-device queuing delay dominates
the overall RTT of download trafﬁc.

Next, we show that when uplink and downlink trafﬁc are
both present, the uplink ACK packets being delayed is the
dominating cause of degraded download performance. Fig-
ure 11 plots the download throughput distributions in three
scenarios using Carrier 1’s LTE network: (i) TCP download
only, (ii) TCP download with concurrent UDP upload, and
(iii) UDP download with concurrent UDP upload. Figure 11
indicates that (i) and (iii) exhibit similar download perfor-
mance (scenario (iii) is even slightly better because it uses
UDP for download) while the throughput in Scenario (ii) is
much lower. Since the key difference between (ii) and (iii)
is whether the uplink ACK stream exists, the results indicate
that the degraded download performance is almost solely as-
sociated with TCP’s upstream ACKs, whereas in the under-
lying radio layer, uplink and downlink use different channels
and can be performed independently. Similar results are ob-
served for upload performance (ﬁgure not shown).
5.2

Impact on Web Browsing

We next examine the impact of upload trafﬁc on web
browsing. We picked ten popular websites from Alexa top
sites, and loaded each of them in Google Chrome browser
on a Samsung Galaxy S3 phone in two settings: without and
with concurrent upload. We repeat the test of each website
for 5 times in a row and report the average results. We per-
formed cold-cache loadings for all sites, and measured the
page load time (PLT) using QoE Doctor [14, 32].

We found that upload trafﬁc signiﬁcantly inﬂates most de-
lay components. For example, the connection setup delay,
which usually takes only one round-trip, increases by 64%
to 509% due to on-device bufferbloat as the dominating fac-
tor. A similar case happens to HTTP requests, which can
typically ﬁt into one single TCP packet. HTTP responses
that carry downlink data are also affected due to the explana-
tions described in §5.1. The response duration inﬂates by up
to 3464%. Overall, the increase of PLT across the 10 web-

(a) δt = 0

(b) δt = 4s

(c) δt = 8s

Figure 12: Impact of upload on PLT. The web browsing
session begins δt after upload starts. “X” indicates the
upload is completed before the web page is fully loaded.

sites ranges from 219% to 607%. The results indicate when
concurrent upload is in progress, on-device bufferbloat can
signiﬁcantly affect short-lived ﬂows.

Next, we show that even a medium-sized upload can cause
signiﬁcant degradation of user experience. Figure 12 repeats
the above experiments but uses a ﬁnite size of upload start-
ing at δt seconds before the web browsing session begins. In
each subﬁgure, a heatmap block (x, y) visualizes the PLT in-
ﬂation caused by an upload of size x for website y. An “X”
mark indicates the upload is completed before the page is
fully loaded or even started to load so the measured PLT in-
crease is an under-estimation. We observe two trends. First,
a larger upload incurs a higher impact on PLT. Second, the
PLT impact also becomes higher as δt increases (for blocks
without “X”). This is because a larger δt allows more time
for the on-device queue to build up, and thus worsens the on-
device bufferbloat condition when the web browsing session
starts.
5.3 Impact on Video Streaming and VoIP
Video Streaming. We randomly chose 10 popular videos
of various lengths (from 40 seconds to 6 minutes) from You-
Tube and played them over LTE on a Samsung Galaxy S3
phone. The playback software is ExoPlayer [2], which uses
the standard DASH streaming algorithm. When the sig-
nal strength is above -98dBm, the average playback bitrate
across 10 videos is 0.93Mbps without any stall when no con-
current trafﬁc is present. With concurrent TCP upload, the
average bitrate is reduced by 57% to only 0.39Mbps, with
15.3 stalls (total stall duration 103s) for each video on av-
erage. Even when periodical upload is in progress (upload
5MB data every with 5s idle time between consecutive up-
loads), half of the videos exhibit playback bitrate degrada-
tion by up to 19%.

VoIP. We make Skype voice calls from a Samsung Galaxy
S3 phone to a desktop in three settings (3 runs each setting):
(i) Skype call only, (ii) Skype call with concurrent TCP up-
load, and (iii) Skype call with periodical TCP upload of 5MB
with 5s idle time between uploads. The experiments were
conducted over Carrier 1’s LTE network. For each call, we
play the same pre-recorded audio (90 seconds) as the base-
line and record the audio at the receiver. To quantify the

 0 0.25 0.5 0.75 1 0 5 10 15 20 25CDFDownload Throughput (Mbps)TCP DL+UDP ULUDP DL+UDP ULTCP DL only#1#2#3#4#5#6#7#8#9#10124816Site #Background uploadsize (MB)XXXXXXXXX124816Background uploadsize (MB)XXXXXXXXXXXXXXXXXXXXXXXXXXXX124816Background uploadsize (MB)XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX 0 50 100 150 200Page Load TimeIncrease (%)310Reducing queuing delay
Qdisc Driver Firmware
()

()

()

()

()

()

Cross-ﬂow

control



Congestion
[19,

Bufferbloat Mitiga-
tion Solution
Change TCP buffer
size
TCP
Control (CC)
12, 11]
TCP Small Queue
(TSQ) [7]
Trafﬁc
tion (TP)
Active
Management
(AQM) [30, 34]
Byte Queue Limit
(BQL) [1]
QCUT

prioritiza-

Queue

















Table 2: Summary of solutions for reducing queuing de-
lay for upload trafﬁc. “()” means only limited support.

user experience, we use an existing tool [4] to compute the
PESQ MOS (Perceptual Evaluation of Speech Quality, Mean
Opinion Score) [5] metric. When upload is not present, the
average PESQ MOS score is 4.08. With continuous TCP up-
load, the average PESQ MOS score drops to 1.80. Even for
scenario (iii), the average PESQ MOS score is only 1.77.

6. QCUT: SOLUTION FOR

ON-DEVICE BUFFERBLOAT

Given the severity of on-device bufferbloat, we propose

our solution called QCUT to mitigate it.
6.1

Inadequateness of Existing Solutions
In the literature, numerous solutions have been proposed
to mitigate in-network bufferbloat, and some do work with
on-device buffers. Table 2 lists representative solutions:
changing TCP buffer size, changing TCP congestion con-
trol (CC), TCP Small Queue (TSQ), Trafﬁc Prioritization
(TP), and Active Queue Management (AQM). However,
they all have limitations on reducing on-device queuing de-
lay. Changing TCP buffer size and CC are transport layer so-
lutions that adjust TCP behaviors to reduce the delay. How-
ever, they do not work with buffers below the transport layer;
also they do not provide cross-ﬂow control as each TCP
connection has a separate buffer. TSQ, a newly introduced
Linux kernel patch, only limits the Qdisc occupancy on a
per-connection basis. TP works across ﬂows and improves
user experience by prioritizing delay-sensitive trafﬁc. How-
ever, it only partially reduces the queuing delay as will be
evaluated in §7.1. The AQM approaches (e.g., CoDel and
PIE) also work across ﬂows. But they do not help reduce
the buffer occupancy at the ﬁrmware buffer.
In §7.1, we
quantitatively compare all above approaches. We also show
that jointly applying them may further incur unexpected con-
ﬂicts, causing additional performance degradation.

We emphasize that none of the above solutions can be re-

Figure 13: The QCut design.

alized at the ﬁrmware buffer, which is usually proprietary
hardware making it difﬁcult to incorporate different queue
management algorithms. As new wireless technologies and
radio chipsets emerge (e.g., 5G and IoT devices), modiﬁca-
tion to all ﬁrmware to solve the on-device queuing is imprac-
tical. Also, as shown in §4.2, the cellular ﬁrmware buffer
differs from upper-layer buffers in that it plays a role in the
cellular control plane (i.e., the BSR affects uplink scheduling
and the LTE uplink throughput). Therefore, even ignoring
the implementation issues, naïvely applying existing buffer-
bloat mitigation solutions on the ﬁrmware buffer may lead
to unexpected results or performance degradation.
6.2 QCUT Design

Motivated by the above, we designed and implemented a
new approach called QCUT to reduce the on-device queu-
ing delay. Here we focus on optimizing cellular uplink but
the general concept of QCUT applies to other networks. As
illustrated in Figure 13, QCUT has three prominent features.
• Realized as a general OS service, QCUT is independent of
ﬁrmware implementation. Therefore it can address the on-
device queuing problem on any radio ﬁrmware, where no
modiﬁcation is needed. QCUT operates in the kernel space
and takes as input only information of buffer occupancy and
transmission statistics, which is exposed by most cellular ra-
dio ﬁrmware from Qualcomm and likely other vendors.
• Since directly limiting the ﬁrmware buffer occupancy is
difﬁcult, QCUT controls the ﬁrmware queuing delay indi-
rectly in the kernel by controlling how fast packets from
Qdisc ﬂow into the ﬁrmware buffer. QCUT estimates the ra-
dio ﬁrmware buffer occupancy and queuing delay to decide
the transmission of packets to the ﬁrmware dynamically.
• QCUT is ﬂexible on trafﬁc classiﬁcation and prioritiza-
tion. By (indirectly) limiting the amount of data in the
ﬁrmware, packets are queued in the Linux Qdisc, where
QCUT can ﬂexibly prioritize packets based on the applica-
tion requirements. For example, when background upload
and interactive trafﬁc co-exist, the latter can be prioritized
and transmitted without Qdisc queuing. By contrast, directly
realizing ﬁne-grained trafﬁc prioritization in the ﬁrmware is
impractical and inﬂexible.

QCUT aims at reducing the on-device queuing delay.
When there is no on-device queuing, QCUT does not incur
additional delay to RT TB or other runtime overhead.

KernelCellular FirmwareQCutTraffic ShapingTraffic DifferentiationThroughput EstimationBuffer EstimationPrioriti-zationClassifi-cationScheduling GrantPadding StatisticsMAC & PHY TXCPBuffer Status ReportFrom eNBTo eNBQdisc311(a) Signal strength -110dBm

(b) Signal strength -98dBm

(c) Signal strength -85dBm

Figure 14: Uplink throughput
prediction error at different lay-
ers with 20ms prediction interval.

Figure 15: Impact of ﬁrmware buffer occupancy threshold of QCUT-B.
The best threshold in each plot is in bold blue text.

As shown in Figure 13, QCUT comprises of two compo-
nents: trafﬁc differentiation and trafﬁc shaping. Trafﬁc dif-
ferentiation classiﬁes packets from applications, and prior-
itizes certain trafﬁc in the Qdisc (e.g., delay-sensitive traf-
ﬁc) based on applications’ requirement. The trafﬁc shaping
module (i) performs accurate throughput prediction, which
is then used to (ii) estimate the buffer occupancy in the
ﬁrmware. Based on that, the module (iii) controls how fast
packets from Qdisc ﬂow into the ﬁrmware buffer, in order
to limit the ﬁrmware buffer occupancy. We describe each
component in details below.

Achievable physical

layer throughput prediction.
Based on recent lower-layer throughput values measured
from scheduling grant and padding (§4.4), we perform
throughput prediction using Exponentially Weighted Mov-
ing Average (EWMA) with α = 0.25 (empirically cho-
sen). The prediction interval is 20ms. Note that we need
to predict the throughput because the lower-layer ﬁrmware
information is not provided in real time so the through-
put measurement is delayed, as we explain shortly. The
“ﬁrmware” curve in Figure 14 plots the prediction error dis-
tributions under 20ms prediction interval, in our controlled
bulk upload experiments with -95dBm RSRP (8Mbps uplink
bandwidth). The ground truth is the lower-layer through-
put measured with a delay (∼100ms later). The results in-
dicate that compared to other curves in Figure 14 where
we perform throughput estimation at higher layers using the
same EWMA algorithm, using lower-layer information for
throughput prediction is much more accurate.

Buffer occupancy estimation. For a wide range of cel-
lular ﬁrmware, their buffer occupancy level can be directly
read from the buffer status report (BSR). However, a practi-
cal issue we found is that, BSR is not reported in real time to
allow accurate buffer occupancy estimation. On both Sam-
sung Galaxy S3 and Nexus 5 devices, although BSR is re-
ported to eNodeB every 5ms, there is on average around
100ms delay before this information is reported to the kernel
due to various overheads. During this period, the ﬁrmware
buffer dynamics may ﬂuctuate considerably.

To overcome this issue, we propose to combine the BSR
and the predicted throughput to derive accurate ﬁrmware
buffer occupancy. The basic idea is the following: since we

Figure 16: Radio ﬁrmware buffer occupancy estimation.

know both the accurate enqueue rate (measured from Qdisc)
and dequeue rate (from uplink throughput) of the ﬁrmware
buffer, we can use them to reﬁne the rough buffer occupancy
estimation from delayed BSR. More speciﬁcally, let S0 be
the most recently reported BSR generated by the ﬁrmware
at t0, which can be obtained from a BSR’s timestamp ﬁeld.
Let Ruplink be the predicted uplink throughput at t0. Also
we keep track of packets {Pi} (i=1,2,...) leaving Qdisc after
t0 by recording their sizes {Si} and timestamps {ti} of leav-
ing Qdisc. Given the above information, the ﬁrmware buffer
occupancy B(tcurr) at timestamp tcurr can be calculated as
follows:

B(tcurr) = B(tn+1)

B(ti+1) = B(ti) + Si+1 − ST X (i), i ∈ [0, n]

(1)
(2)
(3)

ST X (i) = min(B(ti), Ruplink × (ti+1 − ti)), i ∈ [0, n];
where t0 < t1 < .. < tn ≤ tn+1 = tcurr, Sn+1 = 0.
The buffer occupancy is estimated in an iterative manner
as shown in Equation (2), where Si+1 and ST X (i) are the
number of bytes enter and leave the ﬁrmware buffer since ti,
respectively. ST X (i) is computed in Equation (3) using the
predicted throughput. The process is illustrated in Figure 16.
Qdisc dequeue control. QCUT limits the queuing de-
lay in the radio ﬁrmware by throttling the Qdisc in the ker-
nel, i.e., strategically controlling whether a packet should

 0 0.25 0.5 0.75 1-100-50 0 50 100CDFEstimation error (%)AppTCPQdiscFirmware 0 0.5 1 1.5 60 1000 2000 3000 4000BetterUplink throughput(Mbps)RTTF (ms)2KB5KB10KB20KB50KB100KBNo limit 0 5 60 200 350 500BetterUplink throughput(Mbps)RTTF (ms)2KB5KB10KB20KB50KB100KBNo limit 0 5 10 15 60 120 200 280Uplink throughput(Mbps)RTTF (ms)2KB5KB10KB20KB50KB100KBNo limitBettertRadio firmware bufferoccupancy B(t)Timestamp of most recent BSRCurrent timestampPackets transmitted from Linux QdiscEstimatedBufferOccupancyt0tcurr=tn+1tnt1tiSi+1STX(i)312be dequeued from Qdisc into the radio ﬁrmware. To re-
alize this, a simple way is to use a ﬁxed threshold of the
ﬁrmware buffer occupancy, which we refer as QCUT-B (B
stands for “bytes”). We evaluated this approach by repeat-
ing one-minute TCP uploads ﬁve times with different thresh-
olds on a Nexus 5 phone using Carrier 1’s LTE network,
under different signal strength conditions. As shown in Fig-
ure 15, different QCUT-B thresholds incur different trade-
offs between throughput and latency (quantiﬁed by RT TF ).
However, it is difﬁcult to ﬁnd a threshold that works for all
network conditions. The best threshold that achieves low
latency without sacriﬁcing the throughput depends on the
signal strength: 2KB for -110dBm, 20KB for -98dBm, and
In particular, a small threshold (e.g.,
50KB for -85dBm.
2KB) works well when the signal strength is low. However,
at high signal strength, it causes bandwidth under-utilization.
As described in §4.2, this is attributed to the very nature of
cellular uplink scheduling: the ﬁrmware buffer occupancy
reported in BSR is used for determining uplink bandwidth
allocation; the base station thus regards a small buffer occu-
pancy as an indicator that the client does not have much data
to transmit, thus allocating small uplink bandwidth for the
mobile client.

To overcome the above limitation, we propose another
scheme called QCUT-D (D stands for “delay”). It instead
uses the ﬁrmware queuing delay (TF ) as a threshold. The
queuing delay is computed from the estimated throughput
and the buffer occupancy. If the delay is above the thresh-
old, QCUT-D does not allow a packet to be dequeued to the
ﬁrmware from Qdisc. Thus, QCUT-D is adaptive to diverse
network conditions by dynamically adjusting the ﬁrmware
buffer occupancy. We empirically found that using 20ms
as the delay threshold on LTE networks works reasonably
well in diverse network conditions: it leads to low ﬁrmware
buffer queuing while incurring very small impact on the up-
link throughput, as to be evaluated in §7.2. This threshold
can also be empirically chosen for other types of networks.
Trafﬁc differentiation. To meet the performance require-
ment of different applications, QCUT uses the priority queu-
ing in Linux Qdisc for trafﬁc prioritization. For example,
the background upload and interactive trafﬁc such as web
browsing are put into different queues in Qdisc. As a re-
sult, interactive trafﬁc does not experience high queuing de-
lay in Qdisc caused by bulk upload. Also, thanks to the
aforementioned trafﬁc shaping module in QCUT, the delay-
sensitive trafﬁc also undergoes very low queuing delay in the
ﬁrmware, thus leading to an overall small on-device queuing
delay and thus good user experience. QCUT uses existing
trafﬁc classiﬁcation mechanism on Linux to allow applica-
tions and users to ﬂexibly conﬁgure priorities for different
trafﬁc through the standard tc interface.
6.3 QCUT Implementation

We implemented QCUT on Android Linux kernel. Our
testing devices consist of Samsung Galaxy S3 and Nexus
5 running Android 4.4.4 and 6.0.1 with Qualcomm radio
chipset. We expect QCUT to also work with other phones
and tablets with cellular ﬁrmware from the same vendor.

Note that QCUT does not require any special equipment such
as QXDM [6].

Trafﬁc shaping and differentiation are implemented as a
Linux packet scheduler module in 600 LoC. QCUT keeps
track of transmitted packets from Qdisc since the most re-
cent BSR. The trafﬁc shaping module is implemented in
the function call enqueue() of the Qdisc operation data
structure Qdisc_ops. In enqueue(), the queuing delay in
ﬁrmware is estimated based on the information from the ra-
dio ﬁrmware. More speciﬁcally, we use the /dev/diag in-
terface on the Android phones with Qualcomm radio chipset
to extract the uplink scheduling grant, padding statistics, and
BSR from the logs of LTE uplink transport blocks. The on-
line parsing of the logs is implemented in a C++ program
in the user space. Each log record has a timestamp of the
ﬁrmware. The timestamps between kernel and the ﬁrmware
need to be synchronized. The user-space program sends time
request periodically and uses the response, which contains
the ﬁrmware timestamp, to perform the synchronization.

7. EVALUATION

We comprehensively assess how a wide range of solutions
help mitigate the on-device bufferbloat problem, focusing on
existing solutions (§7.1) and then QCUT (§7.2). For all the
following experiments, we conducted on a Samsung Galaxy
S3 on Carrier 1’s LTE network unless otherwise mentioned.
We expect the experimental ﬁndings to be general as none of
the solutions depends on a speciﬁc carrier or vendor.
7.1 Existing Solutions

We consider existing bufferbloat-mitigation solutions dis-
cussed in §6.1. We demonstrate in this section that they
can reduce excessive on-device queuing to various degrees.
However, they suffer from various limitations, and are all in-
capable of reducing the ﬁrmware buffer occupancy. We con-
duct bulk upload experiments at two locations with different
signal strengths measured by RSRP (Reference Signal Re-
ceived Power): good signal (RSRP of -69 to -75 dBm) and
fair signal (RSRP of -89 to -95), using a Samsung Galaxy S3
on Carrier 1’s LTE network.

Changing TCP buffer sizes. The TCP send buffer
(tcp_wmem) on device imposes a limit on the TCP con-
gestion window (cwnd). As shown in Figure 17, under
good signal, shrinking the send buffer effectively reduces
RT TQ that is dominated by device-side queuing at Qdisc
and ﬁrmware. However, the penalty is severely degraded
upload throughput, in particular when tcp_wmem is smaller
than the bandwidth-delay product (BDP). Since BDP con-
stantly ﬂuctuates in cellular networks [43], a ﬁxed conﬁgu-
ration of TCP buffer size does not ﬁt all network conditions.
Changing TCP small queue (TSQ) size. As a newly in-
troduced Linux kernel patch, TSQ [7] limits per-connection
data in Qdisc using a ﬁxed threshold. By reducing the
threshold, we observe smaller Qdisc queuing delay (TQ =
RT TQ − RT TF in Figure 8) under both network conditions,
as shown in Figure 18. Yet Linux’s default TSQ threshold
is too large to eliminate the Qdisc queuing. However, Fig-

313Figure 17:
Impact of TCP send
buffer sizes on upload performance.

Figure 18: Impact of different TCP
small queue (TSQ) sizes on upload.

Figure 19: Impact of different TCP
CC on upload (with TSQ=128KB).

ure 18 also indicates that TSQ has negligible impact on the
ﬁrmware queuing delay (TF = RT TF − RT TB), because
TSQ only controls the bytes in Qdisc. Further, TSQ lim-
its Qdisc occupancy on a per-connection basis so the Qdisc
occupancy can still be high when concurrent ﬂows exist.

Changing TCP congestion control. Congestion con-
trol (CC) affects the aggressiveness of TCP. We consider
two representative CC categories: loss-based CC (TCP CU-
BIC[19] and Westwood[29]) and delay-based CC (TCP Ve-
gas[12] and LP[24]). Generally speaking, lost-based CC,
which uses packet loss as congestion indicator, is more ag-
gressive than delay-based CC that treats increased delay as
a signal of congestion. We found even with TSQ enabled,
loss-based CC incurs severe on-device queuing, measured
by RT TQ, as shown in Figure 19. For delay-based CC, re-
gardless of TSQ setting, on-device queuing is almost always
negligible. However, such low on-device queuing delays are
achieved by sacriﬁcing up to 80% of the throughput.

Active Queue Management is a major in-network so-
lution to reduce queuing delay and network congestion by
strategically dropping packets in a queue. We considered
two well-known and recently proposed AQM algorithms,
CoDel [30] and PIE [34]. Both approaches use a target
threshold to control the queuing delay. Under both signal
strengths, CoDel effectively keeps the Qdisc queuing delay
below the target threshold. However, Since CoDel does not
apply to the ﬁrmware buffer, it only slightly reduces RT TF
by 10% to 20%, as indicated in Figure 20. This is the re-
sult of TCP cwnd reduction triggered by packet losses in-
jected by CoDel. The performance of PIE is even worse
than CoDel.

Jointly applying multiple strategies. In many case, sev-
eral mitigation strategies can be jointly applied to better
balance various tradeoffs. However, we ﬁnd that jointly
using several approaches may also incur unexpected con-
ﬂicts, causing performance degradation. For example, when
CoDel (with target threshold 5ms) and TSQ (with queue size
4KB) are jointly applied to a single upload ﬂow, RT TF ac-
tually increases by 37% compared to using CoDel alone (ﬁg-
ure not shown). This is explained as follows. A small Qdisc
achieved by TSQ can reduce the effectiveness of CoDel,
since the small on-device queuing delay allows CoDel to
drop very few packets compared to a large queue does. This

causes TCP cwnd to increase faster, leading to more notice-
able in-network queuing delay.

Trafﬁc prioritization. All above solutions focus on re-
ducing on-device queuing for bulk upload. When concur-
rent upload and download exist, an alternative approach is to
prioritize uplink ACK packets over upload data trafﬁc to mit-
igate the impact of upload on download (§5.1). Our experi-
ments indicate that when uplink ACKs are prioritized, their
Qdisc queuing delay is reduced signiﬁcantly from 1363ms
to 86ms at -95dBm. However, prioritization can only be re-
alized at Qdisc, causing the uplink ACK stream still to in-
terfere with uplink data at the ﬁrmware buffer. As a result,
compared to the case where no concurrent upload exists, ap-
plying prioritization still increases RT TF by 112ms. We
will demonstrate in §7.2 that by combining Qdisc prioriti-
zation with ﬁrmware queuing delay reduction, QCUT can
effectively mitigate on-device bufferbloat.

7.2 Evaluation of QCut

We conduct a thorough evaluation of QCUT to demon-
strate that it outperforms existing solutions. First, we show
QCUT can signiﬁcantly reduce RT TF that mainly consists
of the ﬁrmware queuing delay. We then conduct a crowd-
sourced user study to assess the effectiveness of QCUT un-
der real workload (web browsing and video streaming) when
bulk upload is present.

Reducing excessive ﬁrmware queuing. Using the work-
load of a single TCP upload, we compare the performance
of ﬁve schemes: TCP CUBIC, TCP Vegas, TSQ, CoDel, and
QCUT. Each experiment thus consists of ﬁve back-to-back
TCP uploads (one minute each) using Carrier 1’s LTE net-
work. We repeat the experiment for 10 times at two loca-
tions with stable signal strength of -95dBm and -110dBm,
respectively. We calculate the throughput every 500ms and
measure RT TF using tcpdump traces. For each scheme, we
report the average result at each location.

Since the ﬁve schemes achieve different tradeoffs between
throughput and latency, we visualize the results on a two-
dimensional plane in Figure 21. The X and Y axes cor-
respond to RT TF and measured throughput, respectively.
A good solution should appear in the upper-right corner of
the plane. The results indicate that except for QCUT and
TCP Vegas, none of the ﬁve solutions is capable of reduc-

 0 0.5 1 1.5 2 2.5default(1192)166425610243072 0 2 4 6 8 10Y1: RTT (s)Y2: Upload throughput(Mbps)Device-side TCP write buffer limit (KB)RTTQ (Y1)Throughput (Y2) 0 0.5 1 1.5 22832128(default)512Delay (s)TSQ size (KB)TQ / GoodTQ / FairRTTF / GoodRTTF / Fair 0 0.2 0.4 0.6 0.8 1 1.2 1.4CubicWestwoodVegasLP 0 2 4 6 8 10 12 14Y1: RTT (s)Y2: Upload throughput(Mbps)RTTQ / Good (Y1)RTTQ / Fair (Y1)Throughput / Good (Y2)Throughput / Fair (Y2)314(a) Signal Strength -110dBm

(b) Signal Strength -95dBm

Figure 20: Effectiveness of CoDel
on reducing latency.

Figure 21: Compare TCP upload performance of different schemes.

(a) Web browsing

(b) Video streaming: initial loading time

(c) Video streaming: bitrate

Figure 22: Improvement of application performance brought by QCUT.

ing RT TF because they do not work at the ﬁrmware layer.
For TCP Vegas, in Figure 21(b), it achieves low latency at
the cost of very low throughput, with the reason explained
in §7.1. On the other hand, QCUT effectively reduces the
ﬁrmware queuing with little or small sacriﬁce of the through-
put. Recall in §6.2 that we devised two QCUT schemes:
QCUT-B and QCUT-D, which use the ﬁrmware buffer occu-
pancy and delay as the threshold to limit the ﬁrmware buffer
occupancy. We found QCUT-D works reasonably well at
both locations since it is adaptive to different throughput,
while it is a bit difﬁcult to pick a ﬁxed threshold for QCUT-
B for different throughput.

Improving application performance. To assess how
QCUT improves real applications’ performance, we de-
ployed QCUT on ﬁve Samsung Galaxy S3 phones used by
real users. The phones run crowd-sourced measurements
supported by Mobilyzer[33] for a week under diverse net-
work conditions. This user study has been approved by IRB.
We consider two workloads: (1) load ﬁve popular web-
pages, and (2) stream a 2-min YouTube video. For each
workload, we run back-to-back measurements under four
different settings: (i) no background upload, (ii) concurrent
upload without bufferbloat mitigation, (iii) concurrent up-
load with CoDel on Qdisc, and (iv) concurrent upload with
QCUT-D. For web browsing, we collect the page load time
(PLT) of each webpage; for video streaming, we record ini-
tial loading time, playback bitrate and rebuffering events.

Note we only triggered these measurements when the phone
is idle, so the experiment is not interfered with other user
trafﬁc. To mitigate the impact of the varying signal strength
within the same experiment that consists of four back-to-
back measurements, we discard the entire experiment if the
LTE RSRP changes by more than 4dBm. Overall we con-
ducted 1266 and 549 successful experiments for web brows-
ing and video streaming, respectively.

The results are shown in Figure 22.

In each plot, we
show two groups of results corresponding to weak signal
strength (LTE RSRP<-99dBm) and strong signal strength
(LTE RSRP≥-99dBm), respectively. As shown in Fig-
ure 22(a), due to concurrent upload, the median PLT across 5
sites increases by 78% and 159% for strong and weak signal
strength, respectively, leading to signiﬁcantly degraded user
QoE. Applying CoDel does not help mitigate the additional
ACK delay (of webpage download) incurred by the bulk up-
load, in particular when the signal strength is weak: the me-
dian PLT increases are still as large as 75% and 171% for
strong and weak signal strength, respectively, compared to
the no-upload cases. QCUT-D, on the other hand, effectively
reduces the PLT to the baseline (i.e., no-upload). For video
streaming, we consider two QoE metrics: initial buffering
time and playback bitrate, whose results are shown in Fig-
ure 22(b) and 22(c), respectively. Again, QCUT signiﬁcantly
outperforms CoDel on improving the video streaming QoE
when concurrent bulk upload is present.

 0 200 400 600 800 1000NoCoDel1525RTTF (ms)Target threshold (ms)Good signalFair signal 0 1 2 3 60 120 240 480 960 1920BetterUplink throughput(Mbps)RTTF (ms)CUBICVegasCoDelTSQQCut-B(5KB)QCut-B(20KB)QCut-D(20ms) 0 2 4 6 8 60 120 240 480BetterUplink throughput(Mbps)RTTF (ms)CUBICVegasCoDelTSQQCut-B(5KB)QCut-B(20KB)QCut-D(20ms) 1 10 100Weak SignalStrengthStrong SignalStrengthPage load time (s)No UploadUploadUpload + CoDelUpload + QCut 1 10Weak SignalStrengthStrong SignalStrengthInitial loading time (s)No UploadUploadUpload + CoDelUpload + QCut 0 0.5 1 1.5 2 2.5 3Weak SignalStrengthStrong SignalStrengthVideo bitrate (Mbps)No UploadUploadUpload + CoDelUpload + QCut315As described in §6.2, the effectiveness of QCUT is at-
tributed to two reasons. First, it reduces the ﬁrmware buffer
occupancy (TF ). But doing that alone is not sufﬁcient be-
cause delay sensitive trafﬁc can still be interfered by upload
trafﬁc at Qdisc. QCUT addresses this by performing priori-
tization at Qdisc, resulting in reduced TQ for delay sensitive
trafﬁc.

8. RELATED WORK

Measuring cellular performance. Several prior efforts
focus on characterizing cellular performance at various as-
pects. Studies [15, 39, 37, 31] collect data from deployed
user trials to understand smartphone performance at dif-
ferent layers. To name a few, Sommers et al.
leveraged
speedtest data to compare cellular versus Wi-Fi perfor-
mance for metro area mobile connections [40]. Huang et
al. examined LTE bandwidth utilization and its interaction
with TCP [20]. Shaﬁq et al. conducted a study of cellular
network performance during crowded events [38]. Liu et al.
measured performance of several TCP variants on 3G EvDO
networks [28]. Rosen et al.
studied the impact of RRC
state timers on network and application performance [35,
36]. Jia et al. performed a systematic characterization and
problem diagnosis of commercially deployed VoLTE (Voice
over LTE) [21]. None of the above studies deeply examined
cellular upload trafﬁc that is becoming increasingly popular.
Improving transport protocols. Over the past 30 years,
researchers have produced a large body work on improving
TCP. We already mentioned many TCP congestion control
algorithms in §6.1, such as [19, 29, 16, 13, 25, 23, 12, 24,
11, 17, 27]. Some recent proposals such as TCP-RRE [26],
Sprout [43], Verus [46] and TCP ex Machina [42] leverage
throughput forecasts or machine learning to ﬁnd the opti-
mal data transmission strategy. All these approaches face
the problem of balancing between throughput and latency,
which is a key factor to be considered when selecting the
desired CC given the application requirement. Other solu-
tions like RSFC [44] and DRWA [22] uses receive buffer
to limit the queuing impact. However, transport-layer solu-
tions do not provide cross-ﬂow control, which may not fully
eliminate interference between ﬂows. Compared to these
transport-layer solutions, QCUT uses accurate throughput
estimation based on the information from the ﬁrmware and
explicitly reduces on-device queuing in cellular networks.

Understanding excessive queuing delay (“buffer-
bloat”). The bufferbloat problem is known in both wired
and wireless networks. Gettys et al. presented an anecdo-
tal study [18] on large queuing delay of interactive trafﬁc.
The study focuses on the scenario of concurrent bulk data
transfers in cable and DSL networks. Using real network
traces, Allman argued that although bufferbloat can happen,
the problem happens more in residential than non-residential
networks and the magnitude of the problem is modest [10].
However, the issue is indeed severe in cellular networks that
usually employ deep buffers, as shown in a study conducted
by Jiang et al., who explored the bufferbloat problem of
downlink trafﬁc in 3G and LTE networks [22]. Recent work

by Xu et al. indicates that some newer smartphones seem to
buffer packets in the kernel when UDP packets are transmit-
ted continuously [45]. However, they did not study TCP or
consider how application is affected. In contrast, we carry
out comprehensive measurements to quantitatively under-
stand (i) on-device bufferbloat caused by TCP upload trafﬁc
and its impact on applications, (ii) the interaction between
upload and other trafﬁc patterns, (iii) the interplay between
TCP and lower layer queues, and (iv) the effectiveness of a
wide range of mitigation strategies at different layers.

Mitigating bufferbloat. Besides those evaluated in §7,
there exist other proposals for reducing excessive queuing
delay. Dynamic Receive Window Adjustment (DRWA) [22]
is a receiver-side solution to reduce the queuing delay by ad-
justing the TCP receive window. Originally it is deployed
on mobile devices to reduce the latency for downlink traf-
ﬁc. For the uplink case, DRWA needs to be deployed at
server side that serves both cellular and non-cellular clients,
thus posing deployment challenges. Byte Queue Limits
(BQL) [1] is another proposal that puts a cap on the amount
of data waiting in the device driver queue. It does not ap-
ply to Qdisc that contributes the majority of the on-device
latency. Moreover, BQL needs driver support.

9. CONCLUDING REMARKS

We carried out to our knowledge the ﬁrst comprehensive
investigation of cellular upload trafﬁc and its interaction with
concurrent trafﬁc. Our extensive measurement using 33-
month crowd-sourced data indicates the contribution of up-
load is large, and the upload speed is high enough to enable
applications to upload user-generated trafﬁc. We then com-
prehensively investigated the on-device bufferbloat problem
that incurs severe performance impact on applications. We
identiﬁed a major source of on-device bufferbloat to be the
large ﬁrmware buffer, on which existing bufferbloat mitiga-
tion solutions are ineffective. We then propose a general
and lightweight solution called QCUT, which controls the
ﬁrmware buffer occupancy from the OS kernel. We demon-
strate the effectiveness of QCUT through in-lab experiments
and real deployment.

Acknowledgements
We would like to thank our shepherd, Mark Allman,
and the anonymous reviewers for their valuable comments
and suggestions. We would also like to thank Shichang
Xu for his assistance in obtaining information from ra-
dio ﬁrmware. This research was supported in part by the
National Science Foundation under grants CNS-1059372,
CNS-1345226, CNS-1566331 and CNS-1629894.

10. REFERENCES
[1] Byte Queue Limit.

https://lwn.net/Articles/454390/.

[2] ExoPlayer. http://developer.android.com/guide/

topics/media/exoplayer.html.

[3] Introduction to LTE Advanced. http://www.

androidauthority.com/lte-advanced-176714/.

316[4] OPTICOM, PESQ - perceptual evaluation of speech quality.

http://www.opticom.de/technology/pesq.php.
[5] P.862: Perceptual evaluation of speech quality (PESQ).

https://www.itu.int/rec/T-REC-P.862-200102-I/en.

[6] Qualcomm eXtensible Diagnostic Monitor.

https://goo.gl/LODgRY.

[7] TCP Small Queues.

https://lwn.net/Articles/507065.

[8] tcp_probe. http://www.linuxfoundation.org/

collaborate/workgroups/networking/tcpprobe/.

[9] 3GPP TS 36.321: Medium Access Control (MAC) protocol

speciﬁcation (V10.3.0), 2011.

[10] M. Allman. Comments on bufferbloat. ACM SIGCOMM

CCR, 2012.

[11] A. Baiocchi, A. P. Castellani, and F. Vacirca. Yeah-tcp: yet

another highspeed tcp. In PFLDnet, 2007.

[12] L. S. Brakmo and L. L. Peterson. Tcp vegas: End to end

congestion avoidance on a global internet. Selected Areas in
Communications, IEEE Journal on, 13(8):1465–1480, 1995.
[13] C. Caini and R. Firrincieli. Tcp hybla: a tcp enhancement for

heterogeneous networks. International Journal of Satellite
Communications and Networking, 22(5):547–566, 2004.
[14] Q. A. Chen, H. Luo, S. Rosen, Z. M. Mao, K. Iyer, J. Hui,

K. Sontineni, , and K. Lau. QoE Doctor: Diagnosing Mobile
App QoE with Automated UI Control and Cross-layer
Analysis. In ACM IMC, 2014.

[15] H. Falaki, R. Mahajan, S. Kandula, D. Lymberopoulos, and

R. G. D. Estrin. Diversity in Smartphone Usage. In ACM
Mobisys, 2010.

[16] S. Floyd. Highspeed tcp for large congestion windows. RFC

3649, 2003.

[17] C. P. Fu and S. C. Liew. Tcp veno: Tcp enhancement for

transmission over wireless access networks. Selected Areas
in Communications, IEEE Journal on, 21(2):216–228, 2003.

[18] J. Gettys. Bufferbloat: Dark buffers in the internet. IEEE

Internet Computing, 15(3):96, 2011.

[19] S. Ha, I. Rhee, and L. Xu. Cubic: a new tcp-friendly

high-speed tcp variant. ACM SIGOPS Operating Systems
Review, 42(5):64–74, 2008.

[20] J. Huang, F. Qian, Y. Guo, Y. Zhou, Q. Xu, Z. M. Mao,

S. Sen, and O. Spatscheck. An in-depth study of LTE: effect
of network protocol and application behavior on
performance. In ACM SIGCOMM, 2013.

[21] Y. J. Jia, Q. A. Chen, Z. M. Mao, J. Hui, K. Sontineni,

A. Yoon, S. Kwong, and K. Lau. Performance
Characterization and Call Reliability Problem Diagnosis for
Voice over LTE. In ACM MobiCom, 2015.

[22] H. Jiang, Y. Wang, K. Lee, and I. Rhee. Tackling Bufferbloat

in 3G/4G Networks. In IMC, 2012.

[23] T. Kelly. Scalable tcp: Improving performance in highspeed

wide area networks. ACM SIGCOMM CCR, 2003.

[24] A. Kuzmanovic and E. W. Knightly. Tcp-lp: A distributed

algorithm for low priority data transfer. In IEEE INFOCOM,
2003.

[25] D. Leith and R. Shorten. H-tcp: Tcp for high-speed and

long-distance networks. In PFLDnet, 2004.

[26] W. K. Leong, Y. Xu, B. Leong, and Z. Wang. Mitigating

egregious ack delays in cellular data networks by eliminating
tcp ack clocking. In IEEE ICNP, 2013.

[27] S. Liu, T. Ba¸sar, and R. Srikant. Tcp-illinois: A loss-and
delay-based congestion control algorithm for high-speed
networks. Performance Evaluation, 65(6):417–440, 2008.

[28] X. Liu, A. Sridharan, S. Machiraju, M. Seshadri, and

H. Zang. Experiences in a 3G Network: Interplay between

the Wireless Channel and Applications. In ACM MobiCom,
2008.

[29] S. Mascolo, C. Casetti, M. Gerla, M. Y. Sanadidi, and

R. Wang. Tcp westwood: Bandwidth estimation for
enhanced transport over wireless links. In ACM MobiCom,
2001.

[30] K. Nichols and V. Jacobson. Controlling queue delay. ACM

Queue, 10(5), 2012.

[31] A. Nikravesh, Y. Guo, F. Qian, Z. M. Mao, and S. Sen. An

In-depth Understanding of Multipath TCP on Mobile
Devices: Measurement and System Design. In ACM
MobiCom, 2016.

[32] A. Nikravesh, D. K. Hong, Q. A. Chen, H. V. Madhyastha,

and Z. M. Mao. QoE Inference Without Application Control.
In ACM SIGCOMM Internet-QoE Workshop, 2016.

[33] A. Nikravesh, H. Yao, S. Xu, D. Choffnes, and Z. M. Mao.

Mobilyzer: An open platform for controllable mobile
network measurements. In ACM Mobisys, 2015.

[34] R. Pan, P. Natarajan, C. Piglione, M. Prabhu,

V. Subramanian, F. Baker, and B. VerSteeg. Pie: A
lightweight control scheme to address the bufferbloat
problem. In IEEE HPSR, 2013.

[35] S. Rosen, H. Luo, Q. A. Chen, Z. M. Mao, J. Hui, A. Drake,
and K. Lau. Discovering Fine-grained RRC State Dynamics
and Performance Impacts in Cellular Networks. In ACM
MobiCom, 2014.

[36] S. Rosen, H. Luo, Q. A. Chen, Z. M. Mao, J. Hui, A. Drake,

and K. Lau. Understanding RRC State Dynamics through
Client Measurements with Mobilyzer. In ACM MobiCom S3
Workshop, 2014.

[37] S. Rosen, A. Nikravesh, Y. Guo, Z. M. Mao, F. Qian, and
S. Sen. Revisiting Network Energy Efﬁciency of Mobile
Apps: Performance in the Wild. In ACM IMC, 2015.

[38] Z. Shaﬁq, L. Ji, A. Liu, J. Pang, S. Venkataraman, , and
J. Wang. A First Look at Cellular Network Performance
during Crowded Events. In ACM SIGMETRICS, 2013.

[39] C. Shepard, A. Rahmati, C. Tossell, L. Zhong, and

P. Kortum. LiveLab: Measuring Wireless Networks and
Smartphone Users in the Field. In HotMetrics, 2010.

[40] J. Sommers and P. Barford. Cell vs. WiFi: On the

Performance of Metro Area Mobile Connections. In IMC,
2012.

[41] S. Sundaresan, W. de Donato, N. Feamster, R. Teixeira,

S. Crawford, and A. Pescape. Broadband Internet
Performance: A View From the Gateway . In ACM
SIGCOMM, 2011.

[42] K. Winstein and H. Balakrishnan. TCP ex machina:

computer-generated congestion control. In ACM SIGCOMM,
2013.

[43] K. Winstein, A. Sivaraman, and H. Balakrishnan. Stochastic
forecasts achieve high throughput and low delay over cellular
networks. In USENIX NSDI, 2013.

[44] Y. Xu, W. K. Leong, B. Leong, and A. Razeen. Dynamic

regulation of mobile 3g/hspa uplink buffer with receiver-side
ﬂow control. In IEEE ICNP, 2012.

[45] Y. Xu, Z. Wang, W. K. Leong, and B. Leong. An end-to-end

measurement study of modern cellular data networks. In
PAM, 2014.

[46] Y. Zaki, T. Pötsch, J. Chen, L. Subramanian, and C. Görg.

Adaptive congestion control for unpredictable cellular
networks. In ACM SIGCOMM, 2015.

[47] L. Zhang, S. Shenker, and D. D. Clark. Observations on the
dynamics of a congestion control algorithm: The effects of
two-way trafﬁc. ACM SIGCOMM CCR, 1991.

317