Measuring Video QoE from Encrypted Trafﬁc

Giorgos Dimopoulos
UPC BarcelonaTech

gd@ac.upc.edu

Ilias Leontiadis

Telefonica Research

ilias.leontiadis@telefonica.com

Pere Barlet-Ros
UPC BarcelonaTech
pbarlet@ac.upc.edu

Konstantina
Papagiannaki

Telefonica Research

dina.papagiannaki@telefonica.com

ABSTRACT
Tracking and maintaining satisfactory QoE for video
streaming services is becoming a greater challenge for
mobile network operators than ever before. Download-
ing and watching video content on mobile devices is
currently a growing trend among users, that is caus-
ing a demand for higher bandwidth and better provi-
sioning throughout the network infrastructure. At the
same time, popular demand for privacy has led many
online streaming services to adopt end-to-end encryp-
tion, leaving providers with only a handful of indicators
for identifying QoE issues.

In order to address these challenges, we propose a
novel methodology for detecting video streaming QoE
issues from encrypted traﬃc. We develop predictive
models for detecting diﬀerent levels of QoE degrada-
tion that is caused by three key inﬂuence factors, i.e.
stalling, the average video quality and the quality vari-
ations. The models are then evaluated on the produc-
tion network of a large scale mobile operator, where we
show that despite encryption our methodology is able
to accurately detect QoE problems with 72%-92% ac-
curacy, while even higher performance is achieved when
dealing with cleartext traﬃc.

1.

INTRODUCTION

Mobile video will increase 11-fold by 2020, account-
ing for 75% percent of total mobile data traﬃc [1]. Such
rapid growth asserts signiﬁcant pressure to mobile op-
erators who have to radically rethink and optimize their
network.

Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
IMC ’16, November 14–16, Santa Monica, CA, USA.
c(cid:13) 2016 ACM. ISBN 978-1-4503-4526-2/16/11. . . $15.00
DOI: http://dx.doi.org/10.1145/2987443.2987459

To perform such optimizations and capacity planning,
operators have to deeply understand and monitor the
oﬀered Quality of Experience (QoE) on video delivery.
Currently, most operators have made signiﬁcant ef-
forts to facilitate the delivery of media-rich content us-
ing techniques such as caching, transcoding, compres-
sion and radio resource allocation across users.

At the same time, a signiﬁcant number of major In-
ternet services have begun to encrypt their traﬃc. Cur-
rently more than 60% of mobile traﬃc is encrypted, a
number that is rapidly rising [2]. Popular video providers
such as YouTube, Netﬂix and Hulu now encrypt a large
part of their video content and the trend indicates that
most of video traﬃc will be encrypted soon [3].

While encryption of video content ensures the users’
privacy, it signiﬁcantly impacts the ability of operators
to monitor or optimize their network [4]. Practically,
with encrypted traﬃc network operators cannot fulﬁll
essential tasks such as to inspect, protect, prioritize,
optimize, compress or balance traﬃc eﬀectively.

In this paper we present a framework that is able to
extract key QoE metrics such as i) stall detection, ii)
average representation (resolution), and iii) representa-
tion ﬂuctuations in encrypted traﬃc. More speciﬁcally,
our contributions are the following:

• We analyze more than 390,000 unique non-encrypted
video sessions collected by a web proxy that is de-
ployed on the cellular network of a large provider
with more than 10M customers in order to extract
insights about video delivery mechanisms and QoE
issues.

• We use the insights and the ground truth from
the non-encrypted traﬃc to build a uniﬁed QoE
measurement method for both adaptive and tradi-
tional video streaming over HTTP.

• We then validate our work on encrypted traﬃc
collected from the same network. First we com-
pare the similarities and the diﬀerences to the non-
encrypted traﬃc delivery. Furthermore, we setup
controlled experiments to verify the accuracy of

513the developed model. We demonstrate that the
models we developed can identify quality issues
from unencrypted data with accuracies between
78% and 93.5% and from encrypted traﬃc with
accuracies between 76% and 91.8%.

• We provide important insights about the informa-
tion that can be extracted from encrypted traﬃc.
Our results indicate that i) passive measurements
from a single vantage point are enough to accu-
rately detect the key factors that aﬀect the users’
experience ii) we discuss on the features that are
the most signiﬁcant for detecting each particular
problem iii) we demonstrate that client instrumen-
tation is not required.

2. BACKGROUND AND MOTIVATION
2.1 Video Streaming Background

For many content providers HTTP has become the
preferred protocol for video delivery over the last few
years. HTTP streaming combines advantages such as
ﬁrewall pass-through and easy network address trans-
lation, but also the beneﬁts of TCP, i.e.
congestion
control mechanisms and reliable packet delivery.

Traditional HTTP Video Streaming
In traditional HTTP video streaming, the video is down-
loaded as a single continuous ﬁle which represents a
single quality setting. Moreover, video buﬀering is em-
ployed as an additional measure to compensate for jitter
and short-term bandwidth variations.

Typically, each video session can be divided into two
buﬀering phases, i.e. the start-up phase and the steady
state [5]. During the start-up phase the player will
download the ﬁrst part of the video as fast as possi-
ble to quickly ﬁll the buﬀer and minimize the initial
delay before the playback begins.

Once the buﬀer has been ﬁlled up to a speciﬁc thresh-
old and the playback has started, the video session goes
into the steady state. This phase is characterized by
ON-OFF cycles, also referred to as pacing, where the
download is paused as soon as the buﬀer has been ﬁlled
and resumes when it is reaching depletion.

HTTP Adaptive Streaming (HAS)
In contrast to traditional streaming, HAS videos are
split on the server in multiple segments, each one cor-
responding to a few seconds of playback time. Each
segment is encoded in a range of diﬀerent quality pro-
ﬁles which are deﬁned by the content provider.

Instead of requesting the entire video, the player per-
forms HTTP requests to fetch consecutive segments.
The quality proﬁle of the next segment is determined
as a function of the throughput with which the previ-
ous segment was downloaded and the available seconds
of playback in the buﬀer. In this way, the representa-

tion of the video can change dynamically to adapt to
changes in the network and minimize stalls.
2.2 Factors that Affect Video QoE
Initial Delay
The initial delay refers to the time spent from the mo-
ment the user requests the video until the playback be-
gins. This delay has two components, the network de-
lay and the initial buﬀering delay. The former can be
attributed to factors such as network latency, longer
server response times, DNS lookups and/or CDN redi-
rections. The later is caused by the time required to
perform the initial ﬁll of the buﬀer with suﬃcient video
data to allow a smooth playback.

Both Mok et al. [6] and Etoh et al. [7] agree that this
factor has the lowest impact on the QoE as users tend
to be more tolerant to longer initial delays than other
impairments such as stalls or quality changes.

Stalls
Whenever the network throughput is not suﬃcient for
the content to be downloaded faster than the rate that
it is consumed, the buﬀer is depleted and the playback
is forced to pause until more data are downloaded and
the buﬀer is ﬁlled again.

Hoßfeld et al. [8] showed that not only the frequency
but also the duration of the playback stalls which occur
due to buﬀer outages, have a high correlation with poor
QoE. Speciﬁcally, the authors conclude that a video
with 2 stalls of 3 seconds duration each, will lead to
signiﬁcantly lower Mean Opinion Score (MOS).

Moreover, Mok et al.

[9] found that the rebuﬀering
frequency has the highest impact on QoE and that a
medium rebuﬀering frequency can result in a MOS lower
by 2 points.

In this work, we measure the stalls using the Rebuﬀer-
ing Ratio which is expressed as the time spent stalling
over the total duration of the video session.

Average Representation Quality
The average quality can be applied only to HAS video
sessions, since only in these cases quality representation
changes may occur. It is calculated as the average of all
the individual qualities of the segments which belong to
a video session.

Multiple related works have shown a high correlation
between the video representation quality and the user’s
QoE. In one of these studies [10], the subjective exper-
iments performed in mobile networks have shown that
video streams with higher quality representations are
linked to better overall QoE.

Representation Quality Variation
Another factor that aﬀects the QoE of adaptive video
streaming, is the changes in quality variation. The vari-
ation in this case has two dimensions, the frequency of
the changes and their amplitude. The frequency is the
absolute number of changes that occurred in a video ses-

514sion, while the amplitude corresponds to the diﬀerence
in magnitude between two consecutive qualities.

In [11], the authors investigate how the representa-
tion switching amplitude and the switching frequency
aﬀect the QoE. Their results show that the switching
amplitude has a very high impact on the user experi-
ence.

2.3 Problem Statement

Adaptive streaming and encryption are nowadays the
default technologies used by the majority of the popu-
lar content providers. The widespread adoption of these
new technologies has given rise to a new set of challenges
for identifying video QoE issues and has rendered pre-
vious solutions obsolete.

Deep Packet Inspection (DPI) solutions for extracting
quality metrics, such as the video resolution and stall
characteristics [12], [13], do not work anymore with en-
crypted traﬃc. Moreover, adaptive quality switching
has introduced new factors that aﬀect the user’s expe-
rience, i.e. quality switching amplitude and frequency.
However, these factors were not included in previous
models for video QoE.

These changes in video streaming technologies, have
caused a high demand, not only by network operators
but also the by research community, for updated tools
and methods for detecting and quantifying quality is-
sues.

Towards this end, this work aims to provide new
methods for assessing the diﬀerent types of impairments
that aﬀect the users’ QoE from encrypted traﬃc.

2.4 Challenges

Although many services have already made the mi-
gration towards adaptive streaming, their platforms con-
tinue to maintain backward compatibility with tradi-
tional static streaming. Therefore, one of the main
challenges in this work, is to provide a solution which
will be compatible with current but also previous video
streaming technologies.

Moreover, with end-to-end encryption enabled, a great
part of the metrics that were previously available in the
network traﬃc for detecting QoE issues is now becom-
ing inaccessible. For this reason, one of our challenges
is to identify the right metrics from the limited amount
of information that is provided by encrypted traﬃc and
build the models to detect quality impairments. In or-
der to accomplish that, we need to reverse engineer the
video services and rely on machine learning and time
series analysis.

Finally, in order to preserve the user’s privacy but at
the same to make our solution as generalizable as pos-
sible, we focus on developing a methodology that will
be capable of detecting problems from network traﬃc
alone and will not depend on the instrumentation of
devices or video players and therefore it can be easily
deployed by operators.

3. DATASET

The set which is presented in this section is con-
structed from unencrypted data that contains the ground
truth for the QoE impairments of each video session.
This information is then used to create the predictive
models for identifying each impairment type. We then
move to a set of encrypted data to validate the previ-
ously constructed models using controlled experiments.

3.1 Weblogs

The data is collected from a web proxy that is de-

ployed on the cellular network of a large European provider.
The proxy is capable of registering all unencrypted HTTP
traﬃc including IP-port tuples, URI’s, object sizes, trans-
action times, request time-stamps and more. Moreover,
each log is annotated with a set of transport layer per-
formance metrics, i.e. bandwidth-delay product (BDP),
bytes-in-ﬂight (BIF), packet loss, packet retransmissions
and RTT. The BDP is equal to the link’s capacity di-
vided by its round-trip delay and represents the max-
imum amount of bytes that can be transferred by the
link at any given time.

The dataset is created from YouTube traﬃc weblogs
which are collected over a period of 45 days spanning
from February to April 2016. From all the HTTP traﬃc
that is generated by the service, we keep the weblogs
that correspond to video and audio segment downloads
and the signalling exchanged between the video player
and the service during playback.

All the data is anonymized before the extraction by
removing all private information such as user agents,
subscriber and handset identiﬁers, MAC and IP ad-
dresses and so on. The only identiﬁer which is pre-
served is the unique 16-character video session ID which
is generated by YouTube. This parameter is described
in more detail in Section 3.2.

We ﬁnd that YouTube is the most suitable candidate
among the currently popular video streaming services
for developing and evaluating our methodology. The
main reasons for this are i) the service’s huge popular-
ity which allows the generation of a very rich dataset
in a short time window, ii) the diversity of the pro-
vided content in terms of video formats, qualities and
durations, iii) its popularity among mobile users and
iv) the adoption of modern technologies i.e. Dynamic
Adaptive Streaming over HTTP (DASH), HTML-based
video playback and pacing.

Moreover, most of the popular video sharing services
are currently following YouTube’s streaming paradigm,
adopting adaptive streaming, a variety of supported
codecs and HTML-based players.

Note that although Google has in the recent years de-
ployed HTTPS for all of its services including YouTube,
we can still observe signiﬁcant amount of video sessions
in cleartext HTTP in our dataset. This is attributed to
the fact that many users use legacy devices or players

515that either do not support TLS encryption or do not
have it enabled by default.

Nevertheless, we veriﬁed through experiments in the
lab that apart from the encryption which is enabled by
default, the delivery mechanism and overall behaviour
of the app remains the same with newer devices with
modern browsers and the latest version of the app.

In the weblogs, each segment download is generated
from the client with a separate HTTP request and there-
fore we obtain a new entry for each new video chunk.
From the list of metrics mentioned above, we also com-
pute the chunk size and the chunk time that indicates
the time when a video chunk arrives at the client, since
in our experiments we found they bring relevant infor-
mation to model the QoE impairments. The complete
list of the metrics extracted from the traﬃc can be found
in Table 1.

The ﬁnal set consists of approximately 390,000 unique
video sessions. However, only 3% of these are adaptive
streaming sessions. This imbalance is expected since
we are able to observe traﬃc from mainly legacy de-
vices and video players which do not support the more
recently adopted adaptive technology.

For the methodology of the stall detection we take
the entire dataset, while for the development of the
average representation and the representation quality
switch detection we only keep the videos that made use
of adaptive streaming.

Network Features

Ground Truth (URI)

chunk resolution

stall count

stall duration

video session ID

minimum RTT
average RTT

maximum RTT

Bandwidth-delay product

average bytes-in-ﬂight

maximum bytes-in-ﬂight

% packet loss

% packet retransmissions

chunk size
chunk time

Table 1: Metrics that we extract from the opera-
tor’s web logs (left column) and the ones that are
reverse engineered from the request URIs (right
column). The features (left) are available for
encrypted and non-encrypted ﬂows whereas the
ground truth is only available for non-encrypted
sessions.

3.2 Ground Truth

From the meta-data that are passed as parameters in
the URIs of the HTTP requests we are able to collect the
ground truth that will be used in the evaluation phase.
In more detail, these parameters carry three main types
of statistics, i.e. generic device and player stats, content
stats and playback stats [13].

The generic stats include information about the user’s
device such as OS, locale, screen resolution, player type
and so on. One of the most important parameters here,
is the unique video session ID. This ID is a 16-character
hash that is randomly generated and it is unique to each
session. We use it to identify and group together all the
weblogs that belong to the same video session.

The content stats are extracted from the HTTP re-
quests for downloading the individual video segments.
One of the the parameters in this group is the ‘content
type’, which indicates if the segment contains video or
audio content and the multimedia container that was
used to encode it, e.g. MP4, FLV or WebM. ‘Itag’ is
another parameter which is used to specify the bit-rate,
frame-rate and resolution of the segment, which we use
to obtain the ground truth for the changes in represen-
tation quality throughout the session.

Finally, the playback statistics are included in the
statistical reports that are periodically sent from the
player to Google servers during the playback. Each re-
port contains information that summarizes the progress
of the playback since the previous report was gener-
ated. Diﬀerent ﬂags are used in the reports to specify
if the video has successfully loaded, if the playback has
started, paused or stopped and if there was a stall and
how long it lasted. These indicators allow us to dis-
cover if a video was played throughout or abandoned
and more important, identify the frequency and dura-
tion of stalls.

Out of the information that is available in the unen-
crypted data, we only use the chunk resolution, the stall
count and duration and the video session ID (Table 1).
These features will be used as the ground truth for
training the detection models in Section 4. After the
completion of training phase, the access to the ground
truth from unencrypted traﬃc will no longer be required
and even if YouTube removes this information or de-
ploys encryption for all sessions, the methodology will
still be applicable.
3.3 Data Preparation

Before starting the analysis, we ensure that any logs
that correspond to cached and/or compressed content
by the proxy are removed from the dataset.

Next, after the ground truth for the stalls and repre-
sentation switches is extracted, all the logs that belong
to the same video session are identiﬁed by the common
session ID and are then grouped together.

Thus, each entry in the dataset corresponds to a unique

video session which includes information about the total
number of stalls and their duration, as well as the char-
acteristics of each chunk such as the quality representa-
tion, size, download time-stamp, but also the transport
layer statistics like RTT, loss, re-transmissions, BDP
and bytes-in-ﬂight for each chunk download.

5164. BUILDING THE DETECTION

FRAMEWORK

Our approach involves ﬁrst the development and test-
ing of the detection framework with unencrypted data.
As soon as we verify that the constructed models can
leverage the cleartext dataset, we can proceed to test
the framework with data from encrypted video streams.
As mentioned in Section 2, there are three main types
of impairments which may cause the degradation of
poor video QoE, the frequency and duration of stalls,
the session’s Average Representation Quality and the
Representation Quality Variation [10].

The initial delay is not considered as part of our video
QoE model given its small contribution on the overall
user experience as explained in 2.1.

In this section we describe the process of identifying
from the limited number of metrics that are oﬀered by
the encrypted traﬃc, those that are the most signiﬁcant
for creating predictive models to detect each of the three
types of impairments. An important part of this process
is the feature construction that allows the generation of
new more powerful features from the already existing
ones.

Next, we show that there is a diﬀerent set of met-
rics that better describes each type of impairment and
contributes more information to the detection model.

In order to generate predictive models for detecting
the level of stalling and the average representation, we
use Machine Learning (ML) and in particular the Ran-
dom Forest algorithm and 10-fold cross-validation.

Classiﬁcation is preferred over regression given that
we divide the data in discrete classes in both scenar-
ios and the models are required to identify in which
class each video session belongs based on the amount of
stalling or the level of the average representation.

4.1 Stall Detection
Feature Construction
From the traﬃc features described in Section 3 (Ta-
ble 1), we generate summary statistics, i.e. max, min,
mean, standard deviation, 25th, 50th and 75th per-
centiles for each of the metrics, resulting in 70 new met-
rics.

Among all the performance metrics that we take into
consideration, the chunk size is one of the most impor-
tant for detecting stalls.
If we take an example of a
video session were stalling has occurred (Figure 1), we
can see the signiﬁcant changes in the chunk size when
the two events take place at the third and the seven-
teenth second of the video session.

More speciﬁcally, whenever there is an outage on the
player’s buﬀer that results in a stall, the player will
request small chunks which can be downloaded much
faster so that the buﬀer will be ﬁlled as soon as possi-
ble and the video playback can resume. Then the size
of the chunks will gradually increase and remain at a

maximum value during the steady state as long as no
further issues occur.

Therefore, we understand that we can signiﬁcantly
improve the accuracy of the stall detection model by
including the sizes of the chunks in our feature set.

Figure 1: Changes in chunk sizes in a video ses-
sion with stalls.

After all the required features have been generated,
the dataset is then split into sessions without stalls and
sessions where at least one stall has occurred. The infor-
mation regarding the number of stalls observed during
a video session and their duration, is the ground truth
which is extracted from the meta-data of URIs as men-
tioned in Section 3.

Figure 2 (left) illustrates the distribution of the num-
ber of stalls that occurred per video session. We observe
that 12% of all the sessions have suﬀered from rebuﬀer-
ing events, while about 8% was aﬀected by more than
1 event.

Figure 2: ECDF of number of stalls (left) and
rebuﬀering ratio (right) per session

Labelling
Next, we use the information from the ground truth
to label the data and create a predictive model. To
do this, ﬁrst we calculate the re-buﬀering ratio (RR)
for each video session as the ratio of the sum of the
duration tstall k of each of the total K stalls over the
duration of the entire session ttotal (eq. 1)

(cid:80)K

k=1 tstall k

ttotal

RR =

(1)

The sessions are then labelled according to the rule
below. The deﬁnition of three levels of stalling, i.e. no

05101520253035relative time (s)02004006008001000120014001600segment size (KB)360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls517stalling, mild and severe, allows a more detailed view of
the degree to which the stalls aﬀect the user.

 “no stalling” :

“mild stalling” :
“severe stalling” :

Stall labels :

RR = 0

0 > RR ≥ 0.1

RR > 0.1

The RR threshold for distinguishing mild and severe
stalling is set to 0.1, since in their work [14] Krishnan
et al. have shown that when the RR is over 0.1, the
severity of the stalling causes such a quality degradation
that leads the users to abandon the video.

Figure 2 (right) shows the distribution of the RR per
video session. We can observe that the sessions with RR
equal or greater thatn 0.1 correspond to approximately
10% of the distribution.

Feature Selection
We then proceed to apply Feature Selection (FS) us-
ing the Correlation-based Feature Subset Selection (Cf-
sSubsetEval) with the Best First search algorithm to
reduce the number of features from 70 to the following
four, BDP mean, packet re-transmissions max, chunk
size min and the chunk size standard deviation.

The output of the feature selection algorithm reveals
that there are three important factors that are corre-
lated with stalling, BDP which is equivalent to through-
put, number of retransmissions and chunk size. The
limited throughput and increased number of retrans-
mitted packets are QoS metrics which are performance
indicators of congested networks and/or networks with
limited bandwidth where stalling is more likely to occur.
Table 2 shows the gain of each of the features that
were obtained after FS was applied and their respective
information gains. The information gain represents the
contribution of each feature in the construction of the
predictive model. Features with higher information gain
have a higher correlation with the problems that we
want the model to detect and are used more frequently
by the classiﬁer.

The higher gains for the minimum and standard devi-
ation of the chunk size indicate that both these features
carry important information for detecting if a video suf-
fered from stalls or not. Smaller chunk sizes correspond
to lower quality streams that are frequently selected by
the user or the adaptive algorithm in the presence of
poor network conditions and limited bandwidth.

On the other hand, larger deviation of the size of
chunks is related to sudden changes in the network’s
performance that in turn lead to quality switches during
playback. In both cases the videos which are streamed
under these conditions are more prone to stalling due
to buﬀer outages.

The BDP and number of packet retransmissions have
a more clear and direct correlation to low bandwidth
and congestion scenarios where the speed at which the
video buﬀer is ﬁlled is limitted and therefore there is a
much higher probability of stalling. These metrics can

be beneﬁcial specially for cases of traditional streaming
where the video is downloaded over a single connection.

info. gain

feature

0.45
0.25
0.18
0.12

chunk size minimum

chunk size std. deviation

BDP mean

packet retransmissions max

Table 2: Features and respective gains for the
stall detection model.

Training and Testing the Predictive Model
In order to avoid biasing the results during the test
phase, we balance the number of instances among the
three classes before training the classiﬁer. The instances
in the classes are then restored to their original numbers
for testing.

Overall, the classiﬁer is able to make predictions with
93.5% accuracy. The proposed stall detection model is
a signiﬁcant improvement over previous approaches [15]
where the achieved accuracy was approximately 84% for
a binary classiﬁcation. In contrast, our model not only
achieves much higher accuracy but it also can predict
the severity of the stalling that aﬀected the user.

The output of the test phase of the model in terms of
True Positives (TP), False Positives (FP), Precision and
Recall can be found in Table 3, while the corresponding
confusion matrix is shown in Table 4.

Precision is calculated as the ratio of TP over TP
and FP and corresponds to the accuracy with which a
certain problem is predicted. Recall is equal to the ratio
of TP divided by the total instances in this class and
measures the models’s ability to correctly identify the
QoE issue of a video session from the data set.

From the confusion matrix we can see that the clas-
siﬁcation errors occur between instances without stalls
and those with mild stalls but also between mild and
severe. However, signiﬁcantly fewer misclassiﬁcations
happen between the severe and “no stall” classes.

Therefore, it is straightforward that the errors oc-
cur due to the classiﬁer’s inability to correctly identify
marginal cases where the RR is close to the RR thresh-
olds we deﬁned for labelling the instances. Hence, in-
stances with RR slightly over zero can be falsely pre-
dicted as healthy sessions without stalls and thus in-
creasing the number of FP. The same applies for cases
where the RR is marginally over 10%, which can be
identiﬁed as mildly problematic and vice versa.

In more detail, although some marginal instances be-
long to diﬀerent classes, they often have similar charac-
teristics, such as throughput delay and loss. The sim-
ilarity between instances of diﬀerent classes can cause
confusion to the classiﬁer resulting to the generation of
FP.

From Table 3, we can see that the healthy sessions are
predicted with higher Precision and Recall when com-

518pared to the other two classes. Moreover, the confusion
matrix in Table 4 indicates that very few sessions have
been misclassiﬁed as mildly or severly problematic.

These indicators show that healthy video sessions are
streamed in signiﬁcantly better network conditions as
opposed to the problematic ones. This is translated to
higher BDP and close to zero packet retransmissions for
the vast majority of the instances. Additionally, healthy
conditions allow higher quality streams with fewer or no
quality switches. The combination of these character-
istics allow the algorithm to easily distinguish healthy
videos from problematic ones.

The separation of problematic sessions can be more
challenging however, which can be veriﬁed from respec-
tive values in the confusion matrix. Here, in contrast
to the healthy cases, there is a much higher number
of misclassiﬁcations between the videos with mild stalls
and those with severe stalls. In these cases, the chunk
size often is not suﬃcient to indicate the amount of
stalling. The reason for this is that frequently the min-
imum video quality is already selected due to limited
bandwidth and therefore the minimum chunk size or its
standard deviation will not contribute signiﬁcant infor-
mation for detecting the amount of stalling that took
place during a video session.

percentiles. As a result, the total number of features we
end up with is equal to 210.

The chunk average size is calculated from the sizes of
all the individual chunks in a video. The size of a chunk
has a strong correlation with the respective quality of
the video segment. The chunk size delta represents the
diﬀerence in the size of consecutive chunks while the
chunk time delta corresponds to the inter-arrival time
of video chunks. These parameters are indicators of
representation switches which in turn aﬀect the average
representation of the session and will be discussed in
more detail in Section 4.3.

Figure 3 presents a video session with a representa-
tion switch from 144p to 480p. Each point in the plot
represents a video chunk, while the labels above the
points indicate the segments’ resolutions. The x axis
corresponds to the video session relative time and the
y axis to the size of the video segments. In this exam-
ple there is a representation switch from 144p to 480p
at t = 22 of the time line. This is translated to a sig-
niﬁcant increase for both chunk ∆t and chunk ∆size,
which indicates that they can be relevant indicatiors of
quality switches.

Class

no stalls
mild stalls
severe stalls
weighted avg.

TP Rate FP Rate Precision Recall
0.977
0.809
0.793
0.935

0.977
0.809
0.793
0.935

0.111
0.035
0.009
0.09

0.965
0.816
0.887
0.934

Table 3: Classiﬁer’s output for the stall detec-
tion model

original label

predicted label

no stalls
mild stalls
severe stalls

no stalls mild stalls
97.76%
14.7%
4.2%

2.06%
80.9%
16.5%

severe stalls

0.18%
4.4%
79.3%

Table 4: Stall detection confusion matrix

4.2 Average Representation Detection
Feature Construction
In order to detect the average representation of videos
with higher accuracy, in addition to the 10 features that
are already available in the dataset, we construct ﬁve
new ones, i.e. the chunk average size, the chunk size
delta, the chunk time delta, the average throughput
and the throughput cumulative sum. The chunk res-
olution is only used for the ground truth and labelling
of the instances and not for the construction of the pre-
dictive model. Hence, we have a total of 14 features
from which we extract the following statistics, mini-
mum, mean, maximum, std. deviation and 5th, 10th,
15th, 20th, 25th, 50th, 75th, 80th, 85th, 90th and 95th

Figure 3: ∆t and ∆size in a video session with a
representation switch

The average throughput is calculated from the indi-
vidual throughputs of all the chunks, while the cusum is
their cumulative sum. The later is used as an indicator
of variations in throughput.

Labelling
For the detection of the average representation of a
video session, it is necessary to categorize the videos
in three main categories based on their average resolu-
tion, low (LD), standard (SD) and high deﬁnition (HD).
Given that in our dataset all the observed resolutions
take only a few standard values, i.e. 144p, 240p, 360p,
480p, 720p and 1080p, we label all videos with resolu-
tions 144p and 240p as LD, 360p and 480p as SD and
all videos with higher resolution as HD.

In the dataset 57% of the videos have LD average
quality, 38% have SD quality and only 5% have HD.
This is an expected ﬁnding in our case where videos

020406080100120140160180relative time (s)0100200300400500600700800segment size (KB)240p∆t∆size240p∆t∆size240p∆t∆size240p∆t∆size240p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size360p∆t∆size519are streamed using limited mobile data plans and on
handheld devices that often come whith smaller screens
which leads users to opt for LD and SD video qualities.
However, we need to also account for cases where
there are representation changes during the playback.
For these videos, we calculate the average representa-
tion µ from the resolutions of all the segments. We
proceed to label the instances in the dataset following
the rule below for calculating the Representation Qual-
ity RQ.

 HD :

SD :
LD :

RQ =

µ > 480
480 ≥ µ ≥ 360
µ < 360

Feature Selection
The FS is again performed with the aid of CfsSubsetE-
val and Best First. After the selection there are 15
features remaining out of the initial 210. These fea-
tures are listed in Table 5, ranked by their respective
information gain.

We observe that statistics derived from the chunk size
are the ones with the highest rank and represent the
vast majority of the 15 features. This is a meaning-
ful and expected result since the chunk sizes are highly
correlated with the diﬀerent representation qualities.

Moreover, the list of features also contains the BDP
and the BIF which are proportional to the amount of
bytes that can be delivered by the network but also the
throughput cusum which is related to the throughput
variations throughout the video session.

info. gain

0.41
0.39
0.38
0.37
0.33
0.32
0.22
0.21
0.2
0.19
0.16
0.15
0.06
0.05
0.03

feature

chunk size 75%
chunk size 85%
chunk size 90%
chunk size 50%
chunk size max

chunk avg size mean

BIF avg max

cumsum throughput min

chunk ∆size max

chunk size std

chunk ∆size std
chunk ∆t 25%

BDP 90%

BIF maximum min
RTT minimum min

Table 5: Features used for the Average Repre-
sentation detection.

Training and Testing the Predictive Model
The model to predict the average representation quality
is again built using ML and Random Forest. The train-
ing is done with balanced classes and then the trained
model is tested on the entire set. The obtained over-
all accuracy in this case is 84.5%. The accuracy for
each class is provided in Table 6 and the corresponding
confusion matrix in Table 7.

Class

TP Rate FP Rate Precision Recall

LD
SD
HD

weighted avg.

0.9

0.768
0.756
0.841

0.206
0.106
0.003
0.156

0.845
0.82
0.945
0.841

0.9

0.768
0.756
0.841

Table 6: Classiﬁer’s output for the average rep-
resentation model

original label

predicted label

LD
SD
HD

SD
HD
LD
90%
9.9% 0.1%
22.7% 76.8% 0.5%
6.8% 18.2% 75%

Table 7: Average representation confusion ma-
trix

The accuracies in the later table reveal that our model
is able to predict the average quality of LD videos with
very high accuracy but with slightly reduced accuracy in
the case of SD and HD videos. Nevertheless, the overall
but also the individual accuracies remain in high levels,
which verify the model’s good performance.

When further investigating the accuracy loss how-
ever, we identify that its caused by the increased num-
ber of misclassiﬁcations that occur in the SD and HD
classes. More speciﬁcally, a considerable amount of SD
video sessions is falsely detected as LD, while 18% of
HD videos are identiﬁed as SD.

This behavior is attributed to the quality downscales
that happen during a video session. As a result one
part of the video is streamed in higher quality and the
part after the downscale is streamed with lower quality.
The diﬀerences in chunk sizes between the two qualities
of a session lead to the incorrect classiﬁcation of the
video. Of course the eﬀects of this phenomenon cannot
be observed for LD videos since there is no lower qual-
ity to downgrade to and chunk sizes remain consistent
throughout the session.
4.3 Representation Quality Switch Detec-

tion

Adaptive streaming can adjust the representation of
the video during playback in order to compensate for
changes in the network conditions and reduce the likeli-
hood of playback buﬀer outages that lead to stalls. The
duration and frequency of the representation changes,
also known as Presentation Quality Switch Rate (PQSR),
as well as the amplitude of the switch can have a nega-
tive impact on the perceived QoE.

Filtering
During the start-up phase, many content providers em-
ploy a fast start mechanism that allows them to ﬁll the
playout buﬀer and start the playback as fast as possible,
eﬀectively reducing the start-up delay. This short ini-
tial part of a video session may have very diﬀerent char-

520acteristics in terms of segment sizes, inter-segment ar-
rival times and throughput when compared to the much
longer steady phase.

To reduce the noise introduced by the start-up phase
in the detection of resolution variations, we remove the
ﬁrst ten seconds of all video sessions in our dataset.
Given that this initial section represents a very small
fraction of the entire video session (the average session
duration is approximately 180 seconds), we can safely
remove it to reduce the noise introduced by the start-up
phase while maintaining more than 95% of the session.

Labelling
In order to build a model for quality switching detec-
tion, it is necessary to ﬁrst quantify the switches in
terms of frequency and amplitude. To this end, we de-
ﬁne two metrics, the time spent in each representation
tr, the frequency of representation switches F and the
switch amplitude A.

The switching frequency F is simply calculated as
the total number of switches that were observed in a
video. The lower the value this metric has, the better
the quality of the corresponding video is.

Finally, equation 2 which is based on the work of
Yin et al.[16], expresses the switch amplitude A as the
normalized sum of all the amplitudes of representation
switches between consecutive segments rk and rk+1.
Again, A is analogous to the degradation of QoE since
large representation changes which lead to poor QoE
will return higher values of A.

K−1(cid:88)

k=1

A =

1

K − 1

|rk+1 − rk|

(2)

The two metrics are then combined to a single indi-
cator of the representation variation Var using linear
combination. Next, each instance in the dataset is clas-
siﬁed in one of three main categories, no variation, mild
variation and high variation, based on the value of Var.

Change Detection
During the study of the sessions with many represen-
tation changes, we observe that whenever the adaptive
algorithm enforces a change in the representation of the
video, a new start-up phase is initiated for the new rep-
resentation. During this phase, the size and inter-arrival
times of the segments are reduced signiﬁcantly until a
certain threshold in the playout buﬀer has been reached
and the video download returns to the steady phase.

In the video session in Figure 3, we can see there is a
steady state in terms of size and inter-arrival times for
the ﬁrst quality. When the representation switch occurs
however, the chunk time delta and size delta are grad-
ually increasing until a steady state is reached again.

Therefore, for the purpose of more accurately captur-
ing the representation changes we use the two features

that were used in section 4.2, the segment size delta
∆size and segment time delta ∆t.

The most suitable approach to detect representation

changes, is to perform a time-series analysis. This method
allows the identiﬁcation of abrupt changes in the values
of diﬀerent metrics in the dimension of time that are
correlated with the switches of representations.

In more detail, our analysis of video sessions with
quality switches showed that whenever a change in res-
olution takes place, a new start-up phase is initiated
in order to ﬁll the buﬀer with data from the new rep-
resentation as fast as possible. This phase is charac-
terized by video segments with small sizes and small
inter-arrival times which will increase gradually until
the steady state is reached once again.

We ﬁnd that the metric which better captures the
changes in both the size and the inter-arrival of the
video segments, is the product ∆size× ∆t. Speciﬁcally,
the multiplication of the two parameters will combine
but at the same time emphasize the eﬀects of each one.
Therefore, for each video session in the dataset, we cal-
culate a new time series where each point corresponds
to the aforementioned product.

While there are many tools and algorithms for de-
tecting abrupt changes in a time series, we ﬁnd that
the most suitable for the purposes of this work is the
Cumulative Sum Control Chart (CUSUM) which was
developed by E.S. Page [17].

CUSUM is a change detection monitoring technique
which allows the detection of shifts from the mean of a
given sample of points in a time series. When a point
exceeds an upper or lower threshold then a change is
found.
In our case, instead of thresholds we use the
standard deviation of the output of the change detec-
tion algorithm. The standard deviation is capable of
capturing the magnitude of the changes that occurred
and is an indicator of high variance.

Figure 4, shows the distributions of the standard de-
viation of the change detection output for sessions with
and without variance. We observe that there is sig-
niﬁcant separation between the two distributions and
by deﬁning a threshold at value 500 on the horizontal
axis, we are capable of correctly identifying 78% of the
sessions without variance and 76% of those that have
representation variations.

Apart from the time-series analysis, ML was also con-
sidered to develop a model for the detection of represen-
tation switches. However, it did not perform as well as
the proposed methodology did and for this reason that
approach was not considered.

5. EVALUATION WITH

ENCRYPTED TRAFFIC

In this section we present and discuss the ﬁndings
from the evaluation of the models that were developed
in Section 4 with encrypted data. This step is impor-
tant for verifying that the proposed methodology can

521application then ‘hooks’ each invocation of this method
and extracts its result, which in this case is the full
URL of the HTTP request. The URL is then parsed to
extract the required ground truth.

Finally, our app will periodically aggregate and send
the collected information from the videos to a remote
server. The local copy of this information is then deleted
from the device to free up space.
5.2 Dataset

Next, the app was installed on a Samsung Galaxy S2
device with a SIM card with an unlimited 3G data plan.
The instrumented phone was given to a user who was
instructed to carry it at all times for a period of 25 days.
The user was motivated to launch the application when
moving to increase the probability of QoE issues.

As a result, we generated a dataset for the ground
truth and a dataset from the encrypted traﬃc corre-
sponding to 722 video sessions. Each entry in the ground
truth dataset corresponds to a unique segment and the
video session ID which the segment belongs to, the
timestamp that marks the beginning of the chunk down-
load, a ﬁeld to indicate if it is an audio or video segment,
the total number and duration of the stalls observed in
the session and ﬁnally its quality representation.

The encrypted traﬃc data is collected again from the
proxy in the form of weblogs. However, since the ﬂows
are encrypted, information such as the session ID, the
stall characteristics and the quality level of each chunk
are not available. Therefore, we only extract the times-
tamp of the HTTP request, the server IP address and
port, the size of the requested object and the TCP
statistics which were described in detail in Section 3.1.
Although the session ID is available in the ground
truth dataset and it is used to group the video segment
statistics in unique sessions, this parameter is missing
from the encrypted data. Even so, we ﬁnd that it is
possible to identify the encrypted segments that belong
to the same session and group them together.

To achieve this we go through the following steps:
• Identify the traﬃc that corresponds to a single
subscriber and remove all requests that do not be-
long to YouTube by ﬁltering out those that have
domain names not related to the service.

• Next, we look for the unique HTTP traﬃc patterns
that take place at the beginning of a new video
session but also after the completion of the play-
back. These include requests to m.youtube.com
and i.ytimg.com which are responsible for down-
loading multiple web objects such as HTML, scripts
and images to construct the video’s web page.

• Longer periods without traﬃc that correspond to
the time between consecutive sessions are identi-
ﬁed in order to clearly deﬁne the beginning and
ending of each session.

Figure 4: CDF of change detection output for
videos with and without resolution changes.

perform with similar accuracy when dealing with en-
crypted traﬃc.
5.1 Ground Truth

For the collection of the encrypted traﬃc, we devel-
oped an Android application which is responsible for
automatically launching YouTube videos which are ran-
domly selected from the list of the 100 most popular
videos on the website [18]. All videos are played using
the latest version of the stock YouTube app for Android,
where encryption is enabled by default.

Apart from handling the playback of videos, the app
has also the capability to extract performance measure-
ments related to the video that is being played.
In
more detail, by accessing the device’s log, it can iden-
tify and log the playback status of a video, i.e.
if the
playback has started, paused, stopped or if a stall has
occurred. Therefore, we do not only detect if the video
was watched throughout its full length or abandoned
earlier, but also identify any stalling events and their
duration. This information is used as the ground truth
for labeling the data and evaluating the accuracy of the
stall detection model.

In order to capture the ground truth related to the
representation quality switches we need access to the
metadata in the HTTP requests that are responsible for
the download of the individual video chunks. However,
these requests are encrypted by default by the YouTube
application and the required information cannot be cap-
tured by means of traﬃc monitoring.

Although solutions such as Man-in-the-middle (MITM)

proxies are common in such use cases for decrypting the
traﬃc generated by the device, we believe that they are
not practical since they alter the path between the client
and the server, but also change the encryption scheme
by establishing two separate TLS connections instead
of one.

To make sure that the ground truth for the quality
switches is obtained without tampering with the en-
cryption scheme or the traﬃc between the player and
the content server, we reverse engineer the YouTube ap-
plication and pinpoint the method which is responsible
for constructing and performing HTTP requests. Our

522This methodology has high accuracy as it successfully
identiﬁed the vast majority of the sessions that were
launched during the entire period of the measurements.
However, it can be limited in scenarios were the same
subscriber launches multiple videos in parallel and not
sequentially. Although such cases are quite rare, it can
be challenging to identify the segments that belong to
the same video session.

Then the two datasets can be easily joined by match-
ing the respective timestamps and the chunk count per
session. As a result, the ﬁnal dataset contains the same
metrics that were described in the left column of Table
1. Having the exact same set of features in both datasets
is necessary to allow the evaluation of the trained mod-
els that were created in the previous section with the
new data from the encrypted traﬃc.

5.3 Dataset Comparison

In this section we characterize the two datasets and
make a comparison of the key features. This will help
verify that the encrypted YouTube service behaves sim-
ilarly to the unecrypted and the model built for plain
traﬃc works for encrypted traﬃc as well.

More speciﬁcally, in Figure 5 we present the distribu-
tions of the segment size (left) for encrypted and clear-
text. The right ﬁgure shows the comparison between
the two distributions for the segment inter-arrival times.
In the case of the segment size, there is a signiﬁcant
overlap between the two distributions. This indicates
that there is a common pattern with respect to the
downloaded chunk sizes of the videos in both datasets
which can be translated to videos streamed with similar
qualities. Only 10% of the segments were larger than
1MB which can be found in HD videos, while the ma-
jority of the segment sizes are consentrated at or below
500KB which corresponds to SD video quality.

The distributions for the segment inter-arrival times
also have very common characteristics. However, 60%
of the encrypted chunks have slightly lower values in
comparison with the respective unencrypted data. The
shorter times between chunks are indicative of lower
bandwidth availability that results in faster depletion of
the playout buﬀer and a more frequent request of new
segments. This observation is expected since a large
part of the encrypted videos was downloaded while the
user was commuting where network conditions can sig-
niﬁcantly deteriorate.

5.4 Stall Detection

Before evaluating the model for detecting stalls, we
repeat the feature construction process described in Sec-
tion 4.1. However, an automated feature selection like
the one employed in the previous section is no longer
necessary since we already know the important features
that are required to make predictions and the rest are
safely removed. Next, the trained model from Section
4.1 is directly tested with encrypted traﬃc.

Figure 5: CDF of the segment size (left) and
segment inter-arrival time (right) for encrypted
and unencrypted traﬃc.

The resulting accuracy is 91.8% which corresponds to
only 1.7% lower performance than the evaluation with
unencrypted data. Nevertheless, this is still an excellent
result which demonstrates that the training set that we
used created a very accurate model that can be applied
to encrypted traﬃc with equal success.

Table 8 shows the evaluation results in terms of Preci-
sion and Recall and Table 9 the corresponding confusion
matrix. Here we can see that the performance has im-
proved for the videos without stalls, it remained roughly
the same for sessions aﬀected by mild stalling but has
decreased for the case of videos with severe stalls.

Class

TP Rate FP Rate Precision Recall

no stalls
mild stalls
severe stalls
weighted avg.

0.97
0.75
0.64
0.92

0.19
0.04
0.02
0.16

0.96
0.79
0.6
0.92

0.97
0.75
0.54
0.92

Table 8: Classiﬁer’s output for the stall detec-
tion evaluation

original label

predicted label

no stalls mild stalls

severe stalls

no stalls
mild stalls
severe stalls

97.2%
18.6%

2%

2.5%
75.2%
32.4%

0.3%
6.2%
65.6%

Table 9: Stall detection confusion matrix

The detection of non-problematic videos is done with
higher accuracy than the one observed in Section 4 be-
cause there is smaller diversity in the network conditions
where the healthy sessions occur. This is attributed to
the fact that the majority of these sessions are generated
when the user is static either at the oﬃce or at home,
where the network conditions have a constant perfor-
mance and as a result, the classiﬁer can more easily
identify that these sessions did not have any issues.

The main source of the overall accuracy loss in this
evaluation however, is the class of videos with sever
stalls. From the confusion matrix it is apparent that

523this is a result of the increased number of videos with
severe stalls that were falsely detected as mild stalls.
This is a problem that was also observed to a lesser
extent in the training and evaluation with the unen-
crypted dataset (Section 4.1).

Although the low performance for the severe stalls
class is attributed to the same reasons that were de-
scribed in the previous section, the further decrease
in accuracy originates from the fact that in the new
dataset most of the sessions with severe stalls have a
Rebuﬀering Ratio slightly higher than 0.1. Remember
that 0.1 is the borderline that was deﬁned to separate
sessions with mild and severe stalls. Therefore, it be-
comes more diﬃcult for the classiﬁer to distinguish to
which class these videos belong to.
5.5 Average Representation Detection

The evaluation of the second model for the detection
of the average representation is done following the same
process as previously. The extended set of features is
generated by means of feature construction, followed
by the manual removal of the features which do not
contribute to the model. This results in the same 15
parameters that were presented in Table 5.

The evaluation is performed with the same approach
as previously, where the encrypted dataset is used as a
test set for the trained model. The process returns an
overall accuracy equal to 81.9% which is approximately
2.5% less than the respective result we got when using
the unencrypted dataset in Section 4.2. Again, this is
an overall good indicator that the model can perform
the detection with almost equally good accuracy when
dealing with encrypted traﬃc.

In Tables 10 and 11, we can see more details regard-
ing the performance of the evaluation per label. Specif-
ically, although the detection of LD and SD videos is
done with slightly reduced accuracy, we still get satis-
factory performance as we can see from the Precision
and Recall values. If we look at the confusion matrix
below however, we observe that there is an increase in
the LD videos which were misclassiﬁed as SD. This is
attributed to the fact that in the current dataset the
number of 240p videos in the LD category is signiﬁ-
cantly higher than the 144p. This causes a shift in the
distribution of the average quality for this category to-
ward the higher end, which in turn causes the incorrect
classiﬁcation of a percentage of these videos as SD.

Another reason behind the reduction of the accuracy
is the reduced detection capabilities for the HD videos.
In this case, the Precision and Recall for this class have
both reduced signiﬁcantly. At the same time, from the
confusion matrix we see that a signiﬁcant amount of
videos have been incorrectly identiﬁed as SD quality.
This poor performance is a result of the very small num-
ber of videos that are available in the HD class. When
combined with the also relatively small number of HD
videos that were used to train the model, this results
in a class where the training and testing was done with

small number of samples and therefore reduced detec-
tion capabilities for this class.

This problem can be easily alleviated by introducing
a training set that is much richer in HD videos. This
will allow the creation of a predictive model which will
be based on a more diverse dataset that will be capable
of a more accurate detection of the average quality of
HD videos with diﬀerent characteristics.

Class

LD
SD
HD

weighted avg.

TP Rate FP Rate Precision Recall
0.845
0.789
0.513
0.819

0.853
0.775
0.641
0.819

0.203
0.157
0.003
0.183

0.845
0.789
0.513
0.819

Table 10: Accuracies from the evaluation for the
average representation detection

original label

predicted label

LD
SD
HD

LD

SD

84.5% 15.4%
20.4% 78.9%
15%

HD
0.1%
0.7%

33.75% 51.25%

Table 11: The confusion matrix from the average
representation evaluation

5.6 Representation Quality Switch Detec-

tion

The last phase of the evaluation is done for detect-
ing quality switches. In this case, there is no trained
model that can be directly applied to the encrypted
data.
In contrast, the methodology relies on the de-
tection of changes that happen in the time intervals
between segment downloads and the diﬀerence in size
between consecutive segments.

In this evaluation there is no requirement for feature
construction or feature selection. We only need to cal-
culate the time series of the products ∆size × ∆t for
each video in the dataset which is going to be used as
input for the change detection algorithm. Next, we ap-
ply the change detection on each session and from that
we take the standard deviation.

In order to validate the methodology from Section
4.3, we use the same value that was proposed in that
section as a threshold for the standard deviation of the
change detection output.

ST D(CU SU M (∆size × ∆t)) = 500

(3)

According to the proposed methodology, all sessions
below the threshold should represent approximately 78%
of the sessions without quality switches and the sessions
above the threshold should represent 76% of the sessions
with quality switches (Figure 4).

Next, the dataset is split into two parts, i.e. the ses-
sions with score below the threshold and those with a

524score above it. From the ground truth from the en-
crypted data, we are able to evaluate if the predeﬁned
threshold allows the detection of variance with accuracy
equal to the one demonstrated in Section 4.3.

Our analysis reveals that the ﬁrst part of the dataset
consists of 76.9% of videos without any quality change,
while in the second part we ﬁnd 71.7% of the sessions
with quality switches. These accuracies are lower by
1.1% and 4.3% respectively as compared to the results
from the evaluation with unencrypted data.

The decrease in accuracy for detecting videos with
quality switches indicates that the encrypted data con-
sists of videos where the average quality variance is
smaller than the one that was observed in the previ-
ous section. As a result, the distribution of (3) shifted
towards the smaller values and after the threshold was
applied, lower percentage of problematic sessions was
correctly identiﬁed.

6. RELATED WORK

Prometheus [15] uses passive measurements on a mo-
bile network to estimate the QoE of two applications,
Video on Demand and VoIP. For the video QoE only
Buﬀering Ratio is considered as a QoE indicator, while
the system is evaluated only on unencrypted traﬃc us-
ing binary classiﬁcation to detect buﬀering issues with
84% accuracy.

Using similar approaches, OneClick [19] and HostView
[20] develop predictive models to detect the QoE of mul-
tiple applications including video streaming, using net-
work performance metrics. However, both approaches
are limited by the requirement of instrumented devices
to capture the feedback from the users.

Hossfeld et al. [11] study the impact of the amplitude
and frequency of representation switches on the user
experience. The authors re-encoded a video in multiple
qualities and introduced diﬀerent levels and frequencies
of switching and performed crowd-sourced experiments
to detect correlations with the received MOS from the
users. In this work only a single short video was used,
which can be considered a very limited representation
of the diverse content found in popular services.

In [10] the authors perform subjective tests in mobile
networks to assess the impact that the video quality
level and quality switching among other factors has on
the users’ experience. The experiments were conducted
with a very limited sample of very short videos, while
only the direction of quality switching, i.e. resolution
upscaling or downscaling was taken into consideration
but not the eﬀects of the amplitude or the frequency.

Finally, the work of Liu et al. [21] investigates three
factors that inﬂuence the user perceived quality, initial
delay, stalling and quality level variation. The authors
conducted experiments in the lab with diﬀerent network
conditions in order to derive functions for calculating
each of the three impairment factors. The fact that the
tests were performed in the lab however, minimizes the

generalization of the results to real network conditions
and to real streaming services where CDNs and diﬀerent
quality adaptation logics can create diﬀerent eﬀects in
terms of initial delay and quality switches respectively.
Overall, although signiﬁcant work has been done pre-
viously in detecting and quantifying the factors that af-
fect the quality of video streaming, our work is the ﬁrst
that extensively studies these factors in a large scale
network using encrypted traﬃc.

7. LIMITATIONS

The methodology presented in this paper was devel-
oped using information from YouTube video sessions
that were streamed with the service’s current conﬁgu-
ration. However, the predictive power of the models re-
sponsible for detecting QoE impairments can be limited
in the case YouTube changes its video delivery scheme.
In such a scenario, the models that were aﬀected by the
changes need to be trained and evaluated again with an
updated dataset.

Moreover, we do not study the evaluation of the method-

ology with other video streaming services in order to
verify to what extent this approach can be generalized.
However, our analysis of other popular video streaming
services such as Vevo, Vimeo, Dailymotion and so on,
has revealed that they have adopted the same technolo-
gies that YouTube is using for content delivery such as
adaptive streaming, rate limiting, wide range of codecs
and qualities and HTML5-based playback. This com-
mon set of characteristics is a strong indicator that our
methodology can be generalized to a number of other
streaming services and motivates us to include it in the
future steps of this work.

8. CONCLUSIONS

In this work we presented a novel framework for de-
tecting from encrypted traﬃc the 3 key factors that im-
pact both adaptive and classical video streaming QoE,
i.e. stalls, average quality and quality switching.

Next, we demonstrated through evaluations on en-
crypted and unencrypted traﬃc from a large mobile
network, that the proposed models can detect diﬀerent
levels of impairments with accuracies as high as 93.5%.
One of the main ﬁndings of the paper is that the
changes in size and inter-arrival times of video segments
are among the most important indicators of quality
impairments. The incorporation of these features in
our detection framework resulted in signiﬁcant improve-
ments in accuracy.

We showed that the framework can perform very well
on a real production network using a few key perfor-
mance metrics from a single vantage point and without
the requirement of instrumented clients or additional
vantage points, so it can easily be deployed by network
operators. The trained models can be then directly ap-
plied on the passively monitored traﬃc and report issues
in real time.

5259. ACKNOWLEDGMENTS

This work was supported by the Spanish Ministry of
Economy and Competitiveness and EU FEDER under
grant TEC2014-59583-C2-2-R (SUNSET project) and
by the Catalan Government (ref. 2014SGR-1427).

10. REFERENCES

[1] Cisco. “Cisco Visual Networking Index: Global

Mobile Data Traﬃc Forecast Update”. White
Paper, February 2016.

[2] Sandvine. “Global Internet Phenomena Report”.

December 2015.

[3] A. Finamore et al. “Is there a case for mobile

phone content pre-staging?”. In 9th ACM
conference on Emerging networking experiments
and technologies (CoNEXT), pages 321–326.
ACM, 2013.

[4] Vasona. “How encryption threatens mobile
operators, and what they can do about it”.
http://goo.gl/fe3xpB. (Accessed on 05/11/2016).

[5] A. Rao et al. “Network Characteristics of Video

Streaming Traﬃc”. 7th ACM conference on
Emerging networking experiments and
technologies (CoNEXT), 2011.

[6] R. Mok et al. “Inferring the QoE of HTTP video
streaming from user-viewing activities”. 1st ACM
SIGCOMM workshop on Measurements up the
stack (W-MUST), 2011.

[13] G. Dimopoulos et al. “Analysis of YouTube user

experience from passive measurements”. In 9th
International Conference on Network and Service
Management (CNSM), pages 260–267. IEEE,
2013.

[14] S. Krishnan et al. “Video stream quality impacts

viewer behavior: inferring causality using
quasi-experimental designs”. Networking,
IEEE/ACM Transactions on, 21(6):2001–2014,
2013.

[15] V. Aggarwal et al. “Prometheus: toward

quality-of-experience estimation for mobile apps
from passive network measurements”. In
Proceedings of the 15th Workshop on Mobile
Computing Systems and Applications, page 18.
ACM, 2014.

[16] X. Yin et al. “A Control-Theoretic Approach for

Dynamic Adaptive Video Streaming over HTTP”.
In Proceedings of the 2015 ACM Conference on
Special Interest Group on Data Communication,
pages 325–338. ACM, 2015.

[17] ES Page. “Continuous inspection schemes”.

Biometrika, 41(1/2):100–115, 1954.

[18] “YouTube: Most Viewed Videos of All Time”.

https://www.youtube.com/playlist?list=
PLirAqAtl h2r5g8xGajEwdXd3x1sZh8hC.
[19] K. Chen et al. “OneClick: A framework for

measuring network quality of experience”. In
INFOCOM 2009, IEEE, pages 702–710. IEEE,
2009.

[7] Z. Guangtao et al. “Cross-Dimensional Perceptual

[20] D. Joumblatt et al. “Predicting user

Quality Assessment for Low Bit-Rate Videos”.
IEEE Transactions on Multimedia,
10(7):1316–1324, 2008.

dissatisfaction with internet application
performance at end-hosts”. In INFOCOM, pages
235–239. IEEE, 2013.

[8] T. Hoßfeld et al. “Quantiﬁcation of YouTube QoE

[21] Y. Liu et al. User experience modeling for dash

video. In 20th International Packet Video
Workshop (PV), pages 1–8. IEEE, 2013.

[22] A. Balachandran et al. “Developing a predictive

model of quality of experience for internet video”.
In ACM SIGCOMM Computer Communication
Review, volume 43, pages 339–350. ACM, 2013.

[23] Z. M. Shaﬁq et al. “Understanding the impact of

network dynamics on mobile video user
engagement”. In ACM SIGMETRICS
Performance Evaluation Review, volume 42, pages
367–379. ACM, 2014.

via crowdsourcing”. In IEEE International
Symposium on Multimedia (ISM), pages 494–499.
IEEE, 2011.

[9] R. Mok et al. “Measuring the quality of experience

of HTTP video streaming”. In IFIP/IEEE
International Symposium on Integrated Network
Management (IM), pages 485–492. IEEE, 2011.

[10] B. Lewcio et al. “Video quality in next generation

mobile networks – perception of time-varying
transmission”. IEEE International Workshop
Technical Committee on Communications Quality
and Reliability (CQR), pages 1–6, 2011.

[11] T. Hoßfeld et al. “Assessing eﬀect sizes of

inﬂuence factors towards a QoE model for HTTP
adaptive streaming”. In 6th International
Workshop on Quality of Multimedia Experience
(QoMEX), pages 111–116. IEEE, 2014.

[12] R. Schatz et al. “Passive youtube QoE monitoring

for ISPs”. In Innovative Mobile and Internet
Services in Ubiquitous Computing (IMIS), 2012
Sixth International Conference on, pages 358–364.
IEEE, 2012.

526