Timing is Everything: Accurate, Minimum Overhead,
Available Bandwidth Estimation in High-speed Wired

Networks

Han Wang1

hwang@cs.cornell.edu

Ki Suh Lee1

kslee@cs.cornell.edu

Chiun Lin Lim2

cl377@cornell.edu
1Department of Computer Science

Cornell University
Ithaca, NY 14850

Ao Tang2

atang@ece.cornell.edu

Erluo Li1

erluoli@cs.cornell.edu
Hakim Weatherspoon1

hweather@cs.cornell.edu

2Department of Electrical and Computer Engineering

Cornell University
Ithaca, NY 14850

ABSTRACT
Active end-to-end available bandwidth estimation is intru-
sive, expensive, inaccurate, and does not work well with
bursty cross traﬃc or on high capacity links. Yet, it is im-
portant for designing high performant networked systems,
improving network protocols, building distributed systems,
and improving application performance. In this paper, we
present minProbe which addresses unsolved issues that have
plagued available bandwidth estimation. As a middlebox,
minProbe measures and estimates available bandwidth with
high-ﬁdelity, minimal-cost, and in userspace; thus, enabling
cheaper (virtually no overhead) and more accurate avail-
able bandwidth estimation. MinProbe performs accurately
on high capacity networks up to 10 Gbps and with bursty
cross traﬃc. We evaluated the performance and accuracy of
minProbe over a wide-area network, the National Lambda
Rail (NLR), and within our own network testbed. Results
indicate that minProbe can estimate available bandwidth
with error typically no more than 0.4 Gbps in a 10 Gbps
network.

1.

INTRODUCTION

Active end-to-end available bandwidth estimation is in-
trusive, expensive, inaccurate, and does not work well with
bursty cross traﬃc or on high capacity links [13, 33]. Yet, it
is important [17, 43, 45]. It is necessary for designing high
performant networked systems, improving network proto-
cols, building distributed systems, and improving applica-
tion performance.

The problems associated with available bandwidth esti-
mation stem from a simple concept: Send a train of probe
packets through a network path to momentarily congest the
bottleneck link, then infer the available bandwidth at the

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
IMC’14, November 5–7, 2014, Vancouver, BC, Canada.
Copyright 2014 ACM 978-1-4503-3213-2/14/11 ...$15.00.
http://dx.doi.org/10.1145/2663716.2663746.

receiving end from probe packet timestamps [20, 36, 40, 46].
There are three signiﬁcant problems with current bandwidth
estimation approaches. First, such approaches are intrusive
since load is being added to the network. Current measure-
ment methods require creating explicit probe packets that
consume bandwidth and CPU cycles. Second, current ap-
proaches often do not have enough ﬁdelity to accurately gen-
erate or timestamp probe packets. By operating in userspace
to maximize programming ﬂexibility, precise control and
measurement of packet timings is sacriﬁced. As a result,
the probe packet timings are often perturbed by operating
systems (OS) level activities. Without precise timestamps,
accurate estimation becomes very challenging, especially in
high-speed networks. Finally, Internet traﬃc is known to
be bursty at a range of time scales [22], but previous stud-
ies have shown that existing estimation approaches perform
poorly under bursty traﬃc conditions [48].

In this paper, we present a middlebox approach that
addresses the issues described above. MinProbe performs
available bandwidth estimation in real-time with minimal-
cost, high-ﬁdelity for high-speed networks (10 Gigabit Eth-
ernet), while maintaining the ﬂexibility of userspace control
and compatibility with existing bandwidth estimation algo-
rithms. MinProbe minimizes explicit probing costs by us-
ing application packets as probe packets whenever possible,
while generating explicit packets only when necessary. Min-
Probe greatly increases measurement ﬁdelity via software
access to the wire and maintains wire-time timestamping
of packets before they enter the host. Finally, a userspace
process controls the entire available bandwidth estimation
process giving ﬂexibility to implement new algorithms as
well as reuse existing estimation algorithms. Importantly,
minProbe operates in real-time, such that it is useful as a
networked systems building block.

MinProbe was designed with commodity components and
achieves results that advance the state of the art. It can
be built from a commodity server and an FPGA (ﬁeld pro-
grammable gate array) PCIe (peripheral component inter-
connect express) pluggable board, SoNIC [26]. As a result,
any server can be turned into a minProbe middlebox. In-
deed, we have turned several nodes and sites in the GENI
(global environment for networking innovations) network
into minProbe middleboxes. It has been evaluated on a

407Low Fidelity
Timestamping
Pathload [20],
Pathchirp [40],

Spruce [46],
TOPP [36]

tcpdump

High-Fidelity
Timestamping

minProbe

DAG [1]

Active
Probe

Passive
Monitor

Table 1: Bandwidth Estimation Design Space

testbed that consists of multiple 10 Gigabit Ethernet (10
GbE) switches and on the wide-area Internet via the Na-
tional Lambda Rail (NLR). It achieves high accuracy: Re-
sults illustrate available bandwidth estimation with errors
typically no more than 0.4 Gbps in a 10 Gbps network.
It estimates available bandwidth with minimal overhead:
Available bandwidth estimates use existing network traﬃc
as probes. Overall, minProbe allows software programs and
end-hosts to accurately measure the available bandwidth of
high speed, 10 gigabit per second (Gbps), network paths
even with bursty cross traﬃc, which is a unique contribu-
tion.

Our contributions are as follows:

• High performance: We are the ﬁrst to demonstrate a
system that can accurately estimate available bandwidth in
10 Gbps network.
• Minimal overhead: Albeit not the ﬁrst middlebox ap-
proach to available bandwidth estimation(e.g. MGRP [37]),
our system performs accurate bandwidth estimation with
minimal overhead.
• Ease of use: Our system provides ﬂexible userspace pro-
gramming interface and is compatible with existing algo-
rithms.
• Sensitivity study: We conduct a sensitivity study on
our estimation algorithm with respect to internal parameter
selection and network condition.

2. BACKGROUND

Available bandwidth estimation is motivated by a simple
problem: What is the maximum data rate that a user could
send down a network path without going over capacity? This
data rate is equal to the available bandwidth on the tight
link, which has the minimum available bandwidth among
all links on the network path. While the problem sounds
simple, there are four main challenges to available band-
width estimation—timeliness, accuracy, non-intrusiveness,
and consistency. In particular, an available bandwidth es-
timation methodology and tool would ideally add as little
overhead as possible and return timely and accurate estima-
tion on the available bandwidth consistently across a wide
range of Internet traﬃc conditions.

In this section, we ﬁrst discuss many of the key ideas and
assumptions underlying existing available bandwidth esti-
mation methods and tools. Then, motivate the development
of minProbe by illustrating the limitations faced by these
current methods and tools.
2.1 Methodology

Many existing available bandwidth estimation tools take
an end-to-end approach. A typical setup sends probe pack-
ets with a predeﬁned interval along the path under measure-
ment, and observe change in certain packet characteristics

Figure 1: Usage of bandwidth estimation

at the receiver to infer the amount of cross traﬃc in the
network. The key idea to such inferences: When the probing
rate exceeds the available bandwidth, the observed packet
characteristics undergo a signiﬁcant change. The turning
point where the change occurs is then the estimated avail-
able bandwidth. See Figure 9, for example, the turning point
where queuing delay variance increases is the available band-
width estimate. Most existing available bandwidth estima-
tion tools can be classiﬁed according to the packet charac-
teristics and metrics used to observing a turning point.

Bandwidth estimation tools such as Spruce [46] and IGI
[16] operate by observing the change in the output prob-
ing rate. The idea is that when the probing rate is below
the available bandwidth, probe packets should emerge at
the receiver with the same probing rate. However, once the
probing rate exceeds the available bandwidth, the gap be-
tween packets will be stretched due to queuing and conges-
tion, resulting in a lower output probing rate. On the other
hand, bandwidth estimation tools such as Pathload [20],
PathChirp [40], TOPP [36], and Yaz [42] operate by ob-
serving the change in one-way delay (OWD), i.e. the time
required to send a packet from source to destination. OWD
increases when the probing rate exceeds the available band-
width, as packets have to spend extra time in buﬀers.

An alternative methodology to the end-to-end approach
would be to query every network element (switch/router)
along a network path. A network administrator could collect
the statistical counters from all related ports, via protocols
such as sFlow [5]; or infer from Openﬂow control messages
such as PacketIn and FlowRemoved messages [50]. However,
obtaining an accurate, consistent, and timely reading from
multiple switches in an end-to-end manner can be very diﬃ-
cult [2]. Further, collecting counters from switches is done at
per-second granularity and requires network administrative
privileges, which makes the approach less timely and useful
for building distributed systems, improving network proto-
cols, or improving application performance. As a result, we
assume and compare to other end-to-end approaches for the
rest of this paper.
2.2 Current Limitations

While useful, the end-to-end approach for available band-
width estimation has inherent issues that limit its applica-
bility, especially in high-capacity links. We outline a few of
these challenges.
• High-ﬁdelity measurements require high-ﬁdelity
instruments. Generating packet trains in userspace typi-
cally require a CPU-intensive busy-wait loop, which is prone
to operating system level noise, such as OS scheduling, in-
terrupt coalescing, and batching. As a result, generated in-
terpacket gaps of a packet train rarely match the interpacket

MPMPsisi+1si+1sigihi408gaps speciﬁed by the userspace program. Moreover, times-
tamping in user or kernel space adds considerable error to
the available bandwidth estimation. Even timestamping in
hardware adds noise when timestamping packet (albeit, less
noise than kernel space, but still enough to make available
bandwidth estimation unusable in many situations [27]).
The takeaway: Without high-ﬁdelity capabilities, available
bandwidth estimation lacks accuracy.
• Non-negligible overhead. Bandwidth estimation tools
add traﬃc to the network path under measurement. This
may adversely aﬀect application traﬃc and measurement
accuracy [37]. The amount of probe traﬃc is proportional
to the rate of sampling and the number of concurrent mea-
surement sessions. As a result, the eﬀect of probe packets on
cross traﬃc exacerbates with traﬃc increase.
• Traﬃc burstiness impacts measurement results.
Bandwidth estimation results are sensitive to traﬃc bursti-
ness. However, existing tools typically ignore the burstiness
of cross traﬃc in favor of simple estimation algorithms. Limi-
tations of batching and instrumentation further mask bursti-
ness to some degree to userspace tools. As a result, existing
tools may not be able to reﬂect true available bandwidth
with bursty traﬃc.

In this paper, we introduce a new tool that can be ap-
plied to existing available bandwidth estimation algorithms
and tools while overcoming the limitations discussed above.
Indeed, we demonstrate that we can accurately estimate
available bandwidth in high-speed networks with minimal
overhead with existing algorithms. In the next section, we
discuss the details of the design and implementation of our
methodology.
3. MinProbe DESIGN

MinProbe measures the available bandwidth with high-
ﬁdelity, minimal-cost and in userspace; thus, enabling
cheaper (virtually no overhead) and more accurate available
bandwidth estimation. It achieves these goals through three
main features:

First, minProbe is a middlebox architecture that uses ap-
plication network traﬃc as probe traﬃc to eliminate the
need for explicit probe packets. When an application packet
arrives at minProbe, minProbe decides whether the packet
should be used as a probe; if so, minProbe modulates the
timings between application packets chosen as probe pack-
ets before forwarding them to their destination. A pro-
grammable ﬂow table accessible from userspace controls the
selection of an application packet as a probe packet. Thus,
by using application traﬃc implicitly as available bandwidth
probes, we are able to remove all the traditional costs and
overheads. A similar idea was proposed in MGRP [37], which
has the same goal of lower overhead, but MGRP lacks high-
ﬁdelity measurement capability.

Second, minProbe is a high-ﬁdelity network measurement
substrate that is capable of modulating and capturing traﬃc
timings with sub-nanosecond precision. The high-precision
is achieved by enabling software access to the physical layer
of the network protocol stack. When minProbe modulates
probe packets (application packets), it adds and removes
minute spacings between packets through direct access to
the physical layer.

Finally, minProbe is accessible from userspace. From
userspace, users can control sub-nanosecond modulations

between packets
to generate probes and obtain sub-
nanosecond timings between received packets to estimate
available bandwidth. Further, all probes can be generated
and timing measurements can be received in real-time from
userspace.

We envision minProbe to be deployed in an architecture
where a separate control plane manages a rich set of mid-
dlebox functionalities with event trigger support(e.g [9]). In
particular, we believe the separation of measurement hard-
ware and production hardware enables high-ﬁdelity mea-
surement in high-speed networks (and in real-time) that is
diﬃcult to achieve otherwise.
3.1 Precise Probe Control

10 GbE Physical Layer

MinProbe oﬀers enhanced network measurement capabil-
ities: High-ﬁdelity packet pacing to generate probe packets
and high-ﬁdelity packet timestamping to measure received
packet times. We achieve high-ﬁdelity capabilities via direct
access to the physical layer from software and in real-time.
To see how minProbe works, we ﬁrst describe the physi-
cal layer of 10 Gigabit Ethernet (GbE). Then, we describe
how minProbe takes advantage of the software access to the
physical layer to achieve high-ﬁdelity network measurement.
3.1.1
According to the IEEE 802.3 standard [3], when Ethernet
frames are passed to the physical layer (PHY), they are re-
formatted before being sent across the physical medium. On
the transmit path, the PHY encodes every 64bits of an Eth-
ernet frame into a 66bit block, which consists of a two bit syn-
chronization header (sync-header) and a 64bit payload. As
a result, a 10 GbE link actually operates at 10.3125 Gbaud
(10G× 66
64 ). The PHY also scrambles each block before pass-
ing it down the network stack to be transmitted. The entire
66bit block is transmitted as a continuous stream of symbols,
which a 10 GbE network transmits over a physical medium.
As 10 GbE always sends 10.3125 gigabits per second (Gbps),
each bit in the PHY is about 97 pico-seconds wide. On the
receive path, the PHY layer removes the two-bit header and
de-scrambles each 64bit block before decoding it.

Idle symbols (/I/) are special characters that ﬁll the gaps
between any two packets in the PHY. When there is no Eth-
ernet frame to transmit, the PHY continuously inserts /I/
symbols until the next frame is available. The standard re-
quires at least twelve /I/s after every packet. Depending on
Ethernet frame and PHY alignment, an /I/ character can
be 7 or 8 bits, thus it takes about 700∼800 pico-seconds to
transmit one /I/ character [3]. Importantly, controlling or
simply counting the number of /I/s between packets is what
enables high-ﬁdelity. However, before the SoNIC [26] plat-
form was created, /I/ characters were typically inaccessible
from higher layers (L2 or above), because they are discarded
by hardware, and deﬁnitely were not accessible in software.
3.1.2 High-Fidelity Measurement
The key insight and capability of how minProbe achieves
high-ﬁdelity is from its direct access to the /I/ characters
in the physical layer. In particular, minProbe can measure
(count) and generate (insert or remove) an exact number
of /I/ characters between each subsequent probe packet to
measure the relative time elapsed between packets or gen-
erate a desired interpacket gap. Further, if two subsequent
probe packets are separated by packets from a diﬀerent ﬂow

409dlebox. The x-axis is the distance in time between the ﬁrst
byte of subsequent packets (or interpacket delay), and y-axis
is the frequency of occurrences. We generated two ﬂows of
application traﬃc at 1 Gbps, and a minProbe middlebox
was conﬁgured to only modulate one of the ﬂows. As can be
seen in Figure 2b, after minProbe, the packets of the prob-
ing ﬂow have minimal spacing (interpacket delay) between
subsequent packets, and exhibit a much higher probe rate:
The peak just to the right of 0 ns interpacket delay was the
modulated probing traﬃc. Even though the overall applica-
tion traﬃc rate was 1 Gbps, we were able to increase the
instantaneous probe rate up to 10 Gbps for short periods
of time by creating short packet trains with minimal inter-
packet delay. The packets of the other ﬂows, on the other
hand, were forwarded as-is, with no changes in timing.

How would minProbe impact the application traﬃc per-
formance? Obviously, if any traﬃc were paced, then min-
Probe would change the pacing. But, for general TCP traf-
ﬁc, the minute modulation that minProbe causes does not
aﬀect the rate or TCP throughput. Similar results have been
demonstrated in prior art MGRP [37]. One problem may oc-
cur when a TCP cwnd (congestion window) is small, e.g.,
when cwnd = 1. In this case, the application does not create
enough traﬃc and dummy probe needs to be created.
3.2 Generalized Probe Model

Figure 3: Generalized probe train model

Figure 3 shows a generalized probe train model we devel-
oped to emulate a number of existing bandwidth estimation
algorithms and used for the rest of the paper. The horizontal
dimension is time. The vertical dimension is the correspond-
ing (instantaneous) probe rate. Each pulse (we call it a train)
contains multiple probe packets sent at a particular rate, as
depicted by the height. In particular, the parameter N rep-
resents the number of packets in each train and parameter
R represents the (instantaneous) probe rate of the train (i.e.
we are able to change the interpacket gap between N packets
to match a target probe rate R). Packet sizes are considered
in computing the probe rate. For a mixed-size packet train,
minProbe is able to adjust the space between all adjacent
packets within the train to achieve the desired probe rate
R. The gaps between successive probe trains are speciﬁed
with parameter G (gap). Finally, each measurement sample
consists of a set of probe trains with increasing probe rate.
The distance in time between each measurement sample is
identiﬁed with parameter D (distance). With these four pa-
rameters, we can emulate most probe traﬃc used in prior
work, as shown in Table 2 and discussed in Section 2.1. For
example, to emulate Spruce probes, we can set the param-
eters (N, R, G, D) to the values (2, 500Mbps, 1.2ns, 48us),
which would generate a pair of probe packets every 48 us
with minimal inter-packet gap (12 /I/ characters or 1.2ns)
at 500Mbps (5% of the capacity of a 10Gbps link), as in-

(a) original traﬃc

(b) modulated traﬃc

Figure 2: Comparison of traﬃc pattern before and after
passed through middlebox

(cross traﬃc), the gap between probe packets will include
/I/ characters as well as data characters of the cross traﬃc
packets, but the measurement will still be exact (i.e. /I/ and
data characters represent time with sub-nanosecond preci-
sion). This level of access from software is unprecedented.
Traditionally, however, an end host may timestamp packets
in userspace, kernel, or network interface hardware which
all add signiﬁcant noise. None of these methods provide
enough precision for high-ﬁdelity network measurements;
consequently, many existing bandwidth estimation tools re-
port signiﬁcant estimation error (large variation and low ac-
curacy) [27].

In a similar fashion to measuring the space between pack-
ets, minProbe generates probe packets through high-ﬁdelity
pacing, by inserting an exact spacing between packets. By
accessing the physical layer of 10 GbE in software in real-
time, minProbe can insert or remove /I/ characters from ap-
plication traﬃc as needed. In particular, two variables of the
probe traﬃc are of interest: The gap between packets and the
overall rate of packet trains. Users can program minProbe
via command line calls or API calls to specify the number of
/I/ characters (i.e. the number of 100s of pico-seconds) to
be maintained between subsequent probe packets, allowing
userspace programs to perform high-ﬁdelity pacing.

Direct access to /I/ characters from software and in real-
time diﬀerentiates minProbe from other measurement tools.
Speciﬁcally, minProbe is able to characterize and generate
an exact spacing (interpacket gap) between probe packets.
3.1.3 Probing without a Probe
Explicit probes are not necessary for minProbe. Similar
to MGRP [37], minProbe can use application traﬃc as
probe packets. It has a programmable ﬂow table that
performs ﬂow matching on pass-through traﬃc. Users can
insert entries into the ﬂow table to specify which ﬂows are
probes. Traﬃc that have a match in the ﬂow table are
modulated before they are forwarded. Other ﬂows that
do not have a match are simply forwarded without any
change in timing. With minProbe, we are able to perform
line-rate forwarding at 10 Gbps, even for the minimum size
packets. Moreover, minProbe has the ability to generate
explicit probe packets, especially when there is insuﬃcient
application traﬃc. Dummy probing packets are padded
with zero bytes and transmitted at pre-deﬁned intervals as
programmed by the users.

Figure 2 illustrates minProbe’s ability to perform high-
ﬁdelity measurement and pacing as a middlebox with only
application packets as probes. Figure 2a shows the distri-
bution of incoming application traﬃc to a minProbe mid-

10-410-310-210-1100 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000Probability DistributionInterpacket Delay (ns)10-410-310-210-1100 0 2000 4000 6000 8000 10000Probability DistributionInterpacket Delay (ns)NRGDProbes410Algorithm N
Pathload 20 [0.1:0.1:9.6] Gbps
Variable
Pathchirp 1 [0.1:0.1:9.6] Gbps Exponential decrease Variable

Variable

G

D

R

Spruce

IGI

2
60 [0.1:0.1:9.6] Gbps

500Mbps

1.2ns
30s

48us
30s

Table 2: Parameter setting for existing algorithms. G is the
gap between packet trains. R is the rate of probe. N is the
number of probe packets in each sub-train. D is the gap
between each sub-train.

tended by the original Spruce Algorithm [46]. Similarly, to
reproduce the IGI experiment results [16], we set the param-
eters (N, R, G, D) to the values (60, [0.1:0.1:9.6]Gbps, 30s,
30s) to generate probe trains of 60 packets every 30 seconds
with rate ranging from 100Mbps to 9.6Gbps at 100Mbps in-
crements. In summary, most types of existing probe trains
can be emulated with the generalized model we developed
for minProbe, where diﬀerent points in the parameteriza-
tion space can represent diﬀerent points in the entire design
space covering prior art and possibly new algorithms.

In the following sections, we use a parameterization that
is similar to Pathload, which estimates available bandwidth
based on the increasing one-way delay (OWD) in probe
trains. If the rate of a probe train is larger than the available
bandwidth of the bottleneck link, the probe train will induce
congestion in the network. As a result, the last packet of the
probe train will experience longer queuing delays compared
to the ﬁrst packet of the same probe train. The diﬀerence
between the OWD of the last packet and the ﬁrst packet of
the same probe train can be used to compute the increasing
OWD in the probe train. On the other hand, if a probe train
does not induce congestion, then there will be no change in
the OWD between the ﬁrst and last packet. Thus, if an avail-
able bandwidth estimation algorithm sends probe trains at
diﬀerent rates R, it will estimate the available bandwidth to
be the lowest probe train rate R where the OWD (queuing
delay) increases.

A bandwidth estimation algorithm based on Pathload
needs to measure the change (increase) in OWD between
the ﬁrst and last packet in a probe train. Since we can mea-
sure the gaps between subsequent packets, we show that the
interpacket gap information is suﬃcient to compute the in-
crease in the OWD of a probe packet train.

Proof. Consider sending a train of n packets with packet
sizes s1, s2, . . ., sn and interpacket gaps g1, g2, . . ., gn−1 from
host A to host B through a network path. The received pack-
ets at host B experiences one-way delays of q1, q2, . . ., qn
through the network and now have interpacket gaps h1, h2,
. . ., hn−1. Assume no packet losses due to network conges-
tion, and no packet reordering in the network. Then we want
to show that the diﬀerence in one-way delay of the ﬁrst and
last packets is equal to the total increase in interpacket gaps,

i.e. qn − q1 =(cid:80) hi −(cid:80) gi.

Initially, the length of the packet train measured from the
ﬁrst bit of the ﬁrst packet to the last bit of the nth packet
is given

n−1(cid:88)

i=1

n−1(cid:88)

i=1

n(cid:88)

j=1

n(cid:88)

j=1

sj

sj

(1)

(2)

Similarly at the receiving end

lA =

gi +

lB =

hi +

Figure 4: MinProbe Architecture

Additionally, the diﬀerence in one-way delay tells us that

lB = lA + (qn − q1)

(3)
Substitute the relationship for lA and lB and we can see that
the diﬀerence in one-way delay is equivalent to the diﬀerence
of interpacket gap.
3.3 Implementation

We built a prototype of minProbe on the software network
interface, SoNIC, platform with two 10 GbE ports [26]. The
detailed design of the SoNIC platform has been described in
prior work [26]. Here, we brieﬂy recount the main features of
SoNIC. SoNIC consists of two components: a software stack
that runs on commodity multi-core processors, and a hard-
ware PCIe pluggable board. The software implements all of
the functionality in the 10GbE physical layer that manip-
ulates bits to enable access to /I/ in software. The hard-
ware performs line-speed data transfer between the 10 GbE
transceiver and the host. To enable the high-ﬁdelity network
measurements required by minProbe, we extended SoNIC’s
capabilities with three new features:
Packet Filtering and Forwarding: We extended SoNIC
to support packet forwarding and ﬁltering at line rate.
Packet forwarding preserves the timing characteristics of the
pass-through traﬃc exactly: Data is copied from an incom-
ing port to an outgoing port, including the exact number
of /I/ characters in between each packet. To ﬁlter for probe
packets, we use an in-kernel ﬂow table that matches on the 5-
tuples of a packet IP header. The packets that have a match
are temporarily buﬀered in a queue, interpacket gap modu-
lated, and sent once enough packets have been buﬀered for
a single packet train (a measurement sample).
Packet Gap Extraction and Manipulation: MinProbe
has two operational modes: a modulation mode for send-
ing probes and an extraction mode for receiving probes. In
the modulation mode, we extended SoNIC with additional
packet queues for temporarily buﬀering of probe packets,
and modiﬁed the amount of /I/ characters in front of the
buﬀered packet to change the packet spacing. If /I/ char-
acters are removed from the probe packet trains to increase
the probe rate, we compensate for the loss of /I/ characters
at the end of the probe packet train to conform to the 10
GbE standard, and vice versa. In the extraction mode, the
number of /I/ characters are extracted from the matched
probe packet trains and sent to the userspace programs via
kernel upcalls.

forwarding pathmiddlebox daemongap extractionuserspacekernel space!ow tablemeasurement    algorithmmbox-ctl411Function
set mode
set gap
ﬂow add

Description
set middlebox mode
set gap between probes
add ﬂow used as probes

Example
set mode
set gap(120,120,120)
ﬂow add(srcip,dstip)

Table 3: Application Programming Interface

Application Programming Interface: The last feature
we added was the userspace accessibility. Users can in-
sert/remove ﬂow entries, dynamically adjust probe packet
gaps, and retrieve probe packet gap counts from minProbe.
It is required to implement a ﬂexible user and kernel space
communication channel for passing measurement data and
control messages between user and kernel space. We used
the netlink [4] protocol as the underlying mechanism and
implemented the exchange of information using netlink mes-
sages. For example, we encapsulate the interpacket gaps in
a special netlink message, and transmitted it from kernel to
userspace, and vice versa.

Userspace daemons control minProbe. Userspace daemons
provide a programmable interface for the kernel-to-user dat-
apath. Table 3 presents a few API calls that are supported
by minProbe. For example, as shown in Figure 4, users can
program the ﬂow table through the daemon or directly by
flow-add commands. In addition, users can retrieve infor-
mation from the kernel through the userspace daemon. In
particular, a kernel module captures the interpacket gap /I/
counts from the probe traﬃc and passes the information
to userspace through the userspace daemon. All communi-
cation consumes a reasonably small amount of bandwidth.
For instance, the most expensive action is the transfer of
interpacket gap /I/ counts from kernel to userspace, which
requires a few hundred mega-bytes per second of bandwidth
for minimum sized 64 byte packets at 10 Gbps.

4. EVALUATION

MinProbe can accurately estimate the available band-
width in high-speed networks with minimal overhead. To
demonstrate its capabilities and accuracy, we evaluated min-
Probe over three diﬀerent physical networks: Two topolo-
gies in our controlled environment and the National Lambda
Rail (NLR). We used a controlled environment to study the
sensitivity of estimation accuracy to various network condi-
tions. We also evaluated minProbe on NLR to study its per-
formance in wide area networks. To highlight the accuracy
minProbe is capable of, Figure 8 in Section 4.2 shows that
minProbe can accurately estimate the available bandwidth
within 1% of the actual available bandwidth in a 10Gbps
network with topology shown in Figure 6a.

In the remainder of this section, we evaluate minProbe in
depth and seek to answer the following questions:
• How sensitive is minProbe to application and cross traﬃc
characteristics (Section 4.3 and 4.4)?
• Does minProbe work in the wild (Section 4.5)?
• Can other software middleboxes accurately measure avail-
able bandwidth (Section 4.6)?
4.1 Experimental setup

Our evaluation consists of two parts: Controlled environ-
ment experiments and wide area network (WAN) experi-
ments. For the WAN experiments, we evaluate minProbe
over the National Lambda Rail (NLR). NLR is a transcon-
tinental production 10Gbps Ethernet network shared by re-
search universities and laboratories with no restriction on

Figure 5: National Lambda Rail Experiment

(a) Dumb-bell

(b) Parking lot

Figure 6: Controlled Experiment Topologies

usage or bandwidth. As a result, traﬃc in the NLR is typi-
cally bursty, yet persistent, and similar to the Internet. We
provisioned a dedicated virtual route that spans nine rout-
ing hops over 2500 miles, as shown in Figure 5. Since the
underlying physical network is also shared with other provi-
sioned routes, we observed persistent cross traﬃc between 1
Gbps to 3.5 Gbps.

For the controlled environment experiments, we set up two
diﬀerent network topologies in a 10 Gbps network testbed:
Dumb-bell (DB) and parking-lot (PL), as illustrated in Fig-
ure 6 and described in [49]. We used one minProbe mid-
dlebox (MP0) to modulate the application traﬃc generated
from a server directly connected to MP0 . We used the other
minProbe middlebox (MP1) as the receiver of the modulated
probes to capture the timing information from the applica-
tion traﬃc. A network of one or more hops separated the
two middleboxes, where one link was a bottleneck link (i.e.
a tight link with the least available bandwidth). Our ex-
periments attempted to measure and estimate the available
bandwidth of this bottleneck (tight) link. The testbeds were
built with commercial 10 GbE switches, represented by cir-
cles in Figure 6. MinProbe middleboxes and hosts that gen-
erated cross traﬃc were separately connected to the switches
using 10 GbE links, illustrated with the solid line. In the DB
topology, the link between the two switches is the bottleneck
(tight) link. In the PL topology, the bottleneck (tight) link
depends on the pattern of the cross traﬃc.

MP0 and MP1 were Dell PowerEdge T710 servers. Each
server had two Intel Xeon Westmere [19] X5670 2.93GHz
processors, with six cores in each CPU and a total of 24 GB
RAM. The Westmere architecture of the processor is well-
known for its capability of processing packets in a multi-
threaded environment [11, 14, 35]. The switches used in the
environment consisted of an IBM G8264 RackSwitch and

Cornell (NYC)NYCChicagoClevelandBostonCornell (Ithaca)MP0MP1App1App0N0N1MP0MP1APP0APP1N0N2MP0MP1APP0APP1N4N6N7N1N3N5412[Bytes]

[Gbps]

Packet size Data Rate Packet Rate
[pps]
156250
467814
935628
1250000

792
792
792
792

1
3
6
8

IPD IPG
[/I/]
[ns]
7200
6400
2137
1872
536
1068
800
200

(a) CAIDA OC-192 trace

(b) Energy plot for CAIDA
trace

(c) Synthesized traﬃc

(d) Energy plot for synthe-
sized traﬃc

Figure 7: The time series and wavelet energy plots for cross
traﬃc used in controlled experiments. Figure 7a shows a
time series of a CAIDA trace in three diﬀerent time scales:
10ms, 100ms and 1s. Coarser time scale means longer av-
eraging period, hence less burstiness. Figure 7b shows the
corresponding wavelet energy plot for the trace in Figure 7a.
Figure 7c shows three diﬀerent traces with diﬀerent traﬃc
burstiness of the same time scale. Figure 7d shows the corre-
sponding wavelet energy plot, with higher energy indicating
more burstiness.

a Dell Force10 switch, with network divided into separate
logical areas using VLANs.

We used three diﬀerent classes of cross traﬃc for our con-
trolled experiments. Figure 7 illustrates the time series char-
acteristics of all three traﬃc classes. The diﬀerent classes of
cross traﬃc are also described below.

Constant bit rate cross traﬃc (CBR1): CBR1 cross
traﬃc consists of ﬁxed-sized and uniformly distributed pack-
ets. In particular, CBR1 traﬃc is a stationary traﬃc with
constant average data rate over long time scale (millisec-
onds), and interpacket gaps are uniform (i.e., variance is
zero).

Stationary, but bursty cross traﬃc (CBR2): The
second class of cross traﬃc was generated from stationary
but bursty distributions, CBR2. CBR2 is created by varying
both the packet size and packet chain length distributions
with two parameters Dsize and Dlen. Both parameters range
from 0.0 (zero variance) to 1.0 (high variance). We draw the
packet size from a log-normal distribution which has been
demonstrated to closely resemble the distribution of Inter-
net traﬃc in [28]. The mean of the log-normal distribution
is ﬁxed, while the variance is controlled by the parameter
Dsize to be Dsize ·Vsize, where Vsize is a ﬁxed high variance
value. A packet chain is a series of consecutive packets with
minimal interpacket gap in between, and from [12], we know
its length takes on a geometric distribution. The parameter
of the geometric distribution is taken to be 1 − Dlen. Thus,
CBR1 is a special case of CBR2 where both Dsize and Dlen
equals 0.0.

Table 4: IPD and IPG of uniformly spaced packet streams.

CAIDA: In the third scheme, we generated cross traﬃc
from a real internet trace. In particular, we used the CAIDA
OC-192 [6] trace which was recorded using a DAG card with
nano-second scale timestamps.

While there is not one commonly accepted deﬁnition for
traﬃc burstiness in the literature, burstiness generally refers
to the statistical variability of the traﬃc. In other words,
high variability in traﬃc data rates implies very bursty traf-
ﬁc. Variability, and by extension, traﬃc burstiness could be
captured by wavelet-based energy plot [47]. Speciﬁcally, the
energy of the traﬃc represents the level of burstiness at a
particular timescale (e.g. at the microsecond timescale). The
higher the energy, the burstier is the traﬃc. Figure 7b and
7d illustrate this wavelet-based energy plot when applied to
the byte arrivals for CAIDA Internet traces and synthet-
ically generated traﬃc, respectively. The x-axis represents
diﬀerent time scale, ranging from nano-second to second in
log scale. The y-axis represents the abstracted energy level
at a particular time scale. We were mostly interested in the
micro-second timescale, which correspond to x-axis values
around 15. Notice that the synthetic traﬃc behave very
much like CAIDA internet traces for energy levels for these
values.

For most experiments, we used 792 bytes as the applica-
tion traﬃc packet size, which according to [6], is close to the
average packet size observed in the Internet. We adjusted the
traﬃc data rate by varying interpacket gaps (IPG). In par-
ticular, we insert a speciﬁc number of /I/’s between packets
to generate a speciﬁc data rate. We controlled the exact data
rate of CBR1 and CBR2 since we control the exact number
of /I/’s inserted between packets. However, for the CAIDA
traces, we selected portions of the trace recording times to
obtain diﬀerent average data rates. In addition, when we
replay a trace, we only need to preserve the timing informa-
tion of the trace, not the actual payload. We used SoNIC
to replay the CAIDA trace. As described in [26], SoNIC can
regenerate the precise timing characteristics of a trace with
no deviation from the original.
4.2 Baseline Estimation

How does available bandwidth estimation perform using
minProbe in a base case: A simple topology with a couple
to several routing hops (Figure 6.a and b) and uniform cross
traﬃc (uniform packet size and interpacket gaps)? We illus-
trate the result with the following experiments.

In the ﬁrst experimental network setup, we use a dumb-
bell topology with two routing hops and a single bottleneck
link (Figure 6a). Cross traﬃc is generated with a constant
bit rate that has uniform packet sizes and interpacket gaps
(CBR1). Node N0 sends cross traﬃc to node N1 according
to the CBR1 uniform distribution.

The second experimental setup is similar to the ﬁrst ex-
cept that we use a parking-lot topology (Figure 6b). Further,
cross traﬃc is generated with the CBR1 uniform distribu-
tion between neighboring nodes: Neven to Nodd (e.g. N0 to

1.5G2.0G2.5G3.0G3.5G4.0G 0 10 20 30 40 50 60Throughput (Gbps)Time (seconds)10ms100ms1000ms 0.01 0.1 1 10 100 1000 10000 5 10 15 20 25 306.4ns1.6us26us1.6ms53msEnergytimescaleEnergy2.6G2.8G3.0G3.2G3.4G0.0Throughput (Gbps)Time (seconds)highly bursty CBR2moderately bursty CBR2CBR1 0.01 0.1 1 10 100 1000 5 10 15 20 25 306.4ns819ns26us3.4ms53msEnergytimescalehighly bursty p=0.1moderately bursty, p=0.4constant bitrate, p=1.0413Figure 8: Available bandwidth estimation in a dumb-bell
and parking-lot topology under CBR traﬃc. Both cross traf-
ﬁc and probe traﬃc share one bottleneck with the capacity
of 10Gbps. The x-axis represents the actual available band-
width of the bottleneck link. The y-axis represents the es-
timation by minProbe. This evaluation demonstrates min-
Probe’s ability to accurately measure the available band-
width and achieve the estimation with minimal probing over-
head.

N1, N2 to N3, etc). Thus, we can control the cross traﬃc
separately on each link.

In all experiments, we varied the data rate of the CBR1
cross traﬃc from 1 Gbps to 8 Gbps with an increment of 1
Gbps each time. We conﬁgured MP0 to modulate application
traﬃc from APP0 destined to APP1 to create probe packet
samples. Note that there is no overhead since we are not in-
troducing any new packets. Further, we conﬁgured MP1 to
capture the timing information of the probe packet samples
(i.e. application traﬃc destined to APP1 from source APP0).
The probe packets modulated by MP0 were parameterized
using the model introduced in Section 3.2. We used the pa-
rameters (N, R, G, D) = (20, [0.1 : 0.1 : 9.6] Gbps, 10 us,
4 ms) where MP0 sends a probe packet sample every 4 ms
enabling us to collect 250 samples per second. Each probe
sample consists of 96 constant bit rate (CBR) trains with
a 10 us gap between trains. Each train runs at an increas-
ing data rate ranging from 0.1 Gbps to 9.6 Gbps, with an
increment of 0.1 Gbps. Recall that we are able to precisely
control the data rate of trains by controlling the interpacket
gap between probe packets within the train (e.g. See Ta-
ble 4). We assume node APP0 generates enough application
traﬃc to create probe packet samples.

Figure 8 shows the result. It shows the actual available
bandwidth on the x-axis and estimated available bandwidth
on the y-axis for the the dumb-bell and parking-lot topolo-
gies. We estimated the available bandwidth with the mean of
10 measurement samples. The estimations in the dumb-bell
topology were within 0.1 Gbps of the actual available band-
width. The estimations in parking-lot topology tended to
under-estimate the available bandwidth, due to the multiple
bottleneck links in the network path. Higher available band-
width scenarios were more diﬃcult to measure as accurately
in a multiple-hop network path. Alternatively, the maximum
value of a measurement could be used to estimate available
bandwidth if the estimation was always overly conservative;
this would actually increase the estimation accuracy.

Figure 9 shows the raw probe sample data that was used
to estimate the available bandwidth of diﬀerent cross traﬃc

(a) 1 Gbps

(b) 3 Gbps

(c) 6 Gbps

(d) 8 Gbps

Figure 9: Scatter-plot showing the queuing delay variance of
probe packets versus the probe rate. The cross traﬃc rate
are constant at 1Gbps, 3Gbps, 6Gbps and 8Gbps. We used
probe with N=20, R=[0.1:0.1:9.6]Gbps, G=10us, D=4ms.

rates in the dumb-bell topology. The x-axis is the rate (R) at
which the probe trains were sent. The y-axis is the variance
of the queuing delay, which was computed from the one-way
delay (OWD) experienced by the packets within the same
probe train. We kept the cross traﬃc rate constant at 1Gbps,
3Gbps, 6Gbps and 8Gbps. The available bandwidth was es-
timated to be the turning point where the delay variance
shows an signiﬁcant trend of increasing.
4.3 Impact of Application Trafﬁc

Next, we study whether minProbe is impacted by charac-
teristics of application traﬃc. In particular, we focus on two
characteristics that matter the most for available bandwidth
estimation: the length of a probe train and the distribution
of probe packet sizes. The length of a probe train aﬀects the
delay experienced by application traﬃc (See the description
of probe train length in Section 3.2 and equation 1). The
longer the probe train, the longer the application traﬃc has
to be buﬀered in the middlebox, resulting in longer delays
the application traﬃc experiences. Typically, a probe train
consists of around 100 or more packets [20,40]. A probe train
of two packets is essentially similar to the packet pair scheme
described by Spruce [46].

Creating probe packet trains from variable sized packets
is diﬃcult because packets with diﬀerent sizes suﬀer dif-
ferent delays through a network. Because minProbe does
not have control over the application traﬃc that is used as
packet probes, it is important to understand how the ac-
curacy of minProbe changes with diﬀerent distributions of
probe packet sizes. Another consideration is that many esti-
mation algorithms prefer large probe packets to small pack-
ets [20, 40].

We evaluate the eﬀect of the length of a probe packet
train (i.e. the number of packets in a train) to the accu-
racy of available bandwidth estimation in Section 4.3.1 and
the eﬀect of the distributions of probe packet sizes in Sec-
tion 4.3.2 and 4.3.3. For these experiments, we always used
CBR1 as cross traﬃc.

 0 2 4 6 8 10 0 2 4 6 8 10Estimated Available Bandwidth(Gbps)Actual Available Bandwidth(Gbps)Dumb-bell topologyParking-lot topology 0 500 1000 1500 2000 2500 3000 0 2 4 6 8 10Queuing Delay VarianceProbe Rate(Gbps) 0 1000 2000 3000 4000 5000 6000 0 2 4 6 8 10Queuing Delay VarianceProbe Rate(Gbps) 0 1000 2000 3000 4000 5000 6000 7000 8000 0 2 4 6 8 10Queuing Delay VarianceProbe Rate(Gbps) 0 2000 4000 6000 8000 10000 0 2 4 6 8 10Queuing Delay VarianceProbe Rate(Gbps)414Actual
Length

5
20
40
60
80
100

1.9

2.57
2.07
1.9
1.85
1.86
1.83

Available Bandwidth [Gbps]

8.9

1.9

Parking-lot
Dumb-bell
3.9
6.9
3.9
6.9
Estimated Available Bandwidth [Gbps]
6.57
5.57
6.90
3.96
6.70
3.87
6.76
3.79
3.79
6.78
6.56
3.96

8.54
6.97
6.94
6.79
6.90
6.79

9.5
8.8
8.68
8.70
8.70
8.55

1.99
1.80
1.80
1.80
1.80
1.80

4.41
3.80
3.86
3.80
3.75
3.70

Table 5: Estimation with diﬀerent probe train length.

Size [B]

64
512
792
1024
1518

1.9

9.47
2.06
2.07
1.90
1.81

Available Bandwidth [Gbps]

Dumb-bell
6.9
3.9

8.9

1.9

Parking-lot
6.9
3.9

Estimated Bandwidth [Gbps]

9.50
4.51
3.96
3.97
3.88

9.50
7.53
6.97
7.01
6.91

9.50
8.9
8.8
8.83
8.84

9.50
1.85
1.80
1.80
1.80

9.50
3.76
3.80
3.75
3.81

9.50
6.64
6.90
6.72
6.83

8.9

8.59
8.30
8.50
8.56
8.44
8.02

8.9

9.50
8.09
8.30
8.54
8.48

Figure 10: The distribution of probe packet sizes from the
CAIDA trace.

Table 6: Estimation results with diﬀerent probe packet size.
4.3.1 Impact of Varying Probe Train Length
In order to understand how the number of packets in a
probe packet train aﬀects the accuracy of minProbe, we
varied the number of packets in each train while using the
same parameters as the baseline experiment (Section 4.2).
In particular, we used (N, [0.1 : 0.1 : 9.6] Gbps, 10 us, 4 ms)
while increasing N from 5 to 100. Table 5 illustrates the
estimated available bandwidth when diﬀerent train lengths
were used. The actual available bandwidth is shown in the
row marked “Actual” and the estimated available bandwidth
is in the rows below “Length” in Table 5. As shown in the
table, increasing the number of probe packets per ﬂow yields
diminishing returns. A probe train with ﬁve packets is not as
accurate as the baseline experiment with 20 packets regard-
less of the actual available bandwidth. However, trains with
larger than 20 packets result in similar estimation accuracy.
The take-away: Increasing the number of packets in a probe
packet train does not necessarily result in more accurate esti-
mation. The minimum number of packets to obtain accurate
estimation results was about 20 packets. More packets in a
probe train elongate the delay of applications traﬃc due to
buﬀering, but do not improve estimation accuracy.

What happens when there is not enough packets between
a source and destination pair? This highlights the fundamen-
tal trade-oﬀ between application latency and probing over-
head. MinProbe has a timeout mechanism to deal with this
corner case. If the oldest intercepted packet has exceeded a
pre-deﬁned timeout value, minProbe will generate dummy
probe packets to fulﬁll the need for probe packets and send
out the probe train. Since minProbe requires much shorter
probe trains compared to existing tools, opportunistically
inserting additional dummy packets add minimal overhead.
4.3.2 Impact of Varying Probe Size
Next, we evaluated minProbe with diﬀerent packet sizes
for probes. All the parameters were the same as in the base-
line experiment except that we varied the size of packets for
probes from 64 bytes to 1518 bytes (the minimum to max-
imum sized packets allowed by Ethernet). In particular, we
used (20, [0.1 : 0.1 : 9.6] Gbps, 10 us, 4 ms) while increasing
probe packet sizes from 64 to 1518 bytes. As can be seen
from Table 6, larger packet sizes (more than 512 bytes) per-

Figure 11: Estimation with probe packets drawn from the
CAIDA trace.

form better than smaller packet sizes in general. Note that
we used 792 byte packets in our baseline experiment, which
resulted in similar available bandwidth estimate accuracy as
1518 byte probe packets. On the other hand, smaller probe
packets, such as 64 byte packets resulted in poor accuracy.
This result and observation is consistent with the previous
results from literature such as [40]. The take-away is that
probe trains with medium to large packet sizes perform bet-
ter than trains of smaller packet sizes.
4.3.3 Mixed Probe Packet Sizes
In a real network, the distribution of packet sizes is usually
mixed with large and small packets. Figure 10 shows the
distribution of packet sizes in a recorded Internet trace. We
repeat the previous experiment using the baseline setting,
except that in this case, we use APP0 to replay the Internet
trace. We let MP0 modulate the probe train in presence of
mixed-sized probe traﬃc. For each rate R ∈ [0.1, 9.6], MP0
ensures that the interpacket gaps between packets of the
same ﬂow are uniform. Figure 11 shows the result for CBR1
cross traﬃc. As can be seen, even though a probe train has
mixed packet sizes, the estimation result is still accurate.
The reason is that small packets only take a small fraction
of total probe train length with respect to large packets.
The estimation accuracy is largely determined by the large
packets, thus remains accurate.
4.4 Impact of Cross Trafﬁc Characteristics

So far, we have evaluated minProbe under the assumption
of constant bit-rate cross traﬃc. In this section, we no longer
make this assumption and instead examine minProbe under
bursty cross traﬃc. In particular, we evaluate minProbe us-
ing modeled synthetic bursty traﬃc (CBR2) and realistic
Internet traces.

 0 0.2 0.4 0.6 0.8 1 0 200 400 600 800 1000 1200 1400 1600Packet size CDF 0 2 4 6 8 10 0 2 4 6 8 10Estimated Available Bandwidth(Gbps)Actual Available Bandwidth(Gbps)415(a) 1 Gbps

(b) 3 Gbps

(c) 6 Gbps

(d) 8 Gbps

Figure 12: Bandwidth estimation accuracy with diﬀerent
cross traﬃc burstiness. On y-axis, we turn the knob from no
clustering to batching. On x-axis, we turn the knob on cross
traﬃc packet size distribution from uniform distribution to
log-normal distribution. We plot the graph for diﬀerent cross
traﬃc rate: 1Gbps, 3Gbps, 6Gbps and 8Gbps.
4.4.1 Cross Trafﬁc Burstiness
Synthesized Traﬃc: We ﬁrst show the results of synthe-
sized bursty cross traﬃc (CBR2). We generated CBR2 with
rate ∈ [1, 3, 6, 8] Gbps. We set Vsize to be a ﬁxed large vari-
ance value, e.g. 8 × 104 bytes2, as speciﬁed in [28]. Next, we
varied the distribution Dsize of packet sizes in CBR2 from
0 to 1 with 0.1 step size. 0 means uniform distribution, 1
means distribution with large variance. Similarly, we varied
the distribution Dlen of packet chain length in CBR2 from
between 0 to 1 with 0.1 step size. Note that even though
CBR2 is bursty in terms of the variation of packet distri-
bution, the long term average data rate of CBR2 remains
constant.

Figure 12 shows the estimation error (in Gbps) over a
range of cross traﬃc burstiness for four diﬀerent cross traf-
ﬁc data rates. In each ﬁgure, we plot the diﬀerence between
the actual available bandwidth and the estimated available
bandwidth for the cross traﬃc burstiness identiﬁed by the
(Dsize,Dlen) parameters. Traﬃc close to the bottom left cor-
ner was more uniformly distributed in both packet sizes and
train lengths. Traﬃc on the top right corner was burstier.
Dark gray colors mean under estimation, and light gray col-
ors mean over estimation. As shown in the ﬁgure, the esti-
mation error by minProbe is typically within 0.4 Gbps of
the true value except when the link utilization is low, or the
cross traﬃc is bursty.
Real
Internet Trace: Next, we evaluate minProbe
with traces extracted from CAIDA anonymized OC192

(a) raw measurement

(b) after moving average

Figure 13: Bandwidth estimation of CAIDA trace, the ﬁgure
on the left is the raw data trace, the ﬁgure on the right is
the moving average data.

dataset [6], and replayed by SoNIC as a traﬃc generator.
Figure 13a shows an example of the raw data captured
by minProbe. We observed that real the traﬃc traces were
burstier than the synthesized traﬃc in the previous section.
To compensate, we used standard exponential moving aver-
age (EMA) method to smooth out the data in Figure 13a.
For each data points in Figure 13a, the value was replaced
by the weighted average of the ﬁve data points preceding the
current data point. Figure 13b shows the result. Similar to
the highly bursty synthesized cross traﬃc, we achieved a sim-
ilarly accurate estimation result for the real traﬃc trace via
using the EMA. The estimation error was typically within
0.4Gbps of the true available bandwidth.

In summary, while bursty cross traﬃc resulted in noisier
measurement data than CBR, we were able to compensate
for the noise by performing additional statistical processing
in the bandwidth estimation. Speciﬁcally, we found using
the EMA smoothed the measurement data and improved
the estimation accuracy. As a result, we observed that cross
traﬃc burstiness had limited impact on minProbe.
4.5 MinProbe In The Wild

In this section, we demonstrate that minProbe works in
a wide area Internet network, the National Lambda Rail
(NLR). Figure 5 illustrates the experimental network topol-
ogy. We setup a dedicated route on the NLR with both ends
terminating at Cornell University. Node APP0 generated ap-
plication traﬃc, which was modulated by MP0 using param-
eter (N, R, G, D) = (20, [0.1 : 0.1 : 9.6] Gbps, 10 us, 4 ms).
The traﬃc was routed through the path shown in Figure 5
across over 2500 miles. Since it was diﬃcult to obtain accu-
rate readings of cross traﬃc at each hop, we relied on the
router port statistics to obtain a 30-second average of cross
traﬃc. We observed that the link between Cleveland and
NYC experienced the most cross traﬃc compared to other
links, and was therefore the bottleneck (tight) link of the
path. The amount of cross traﬃc was 1.87 Gbps on aver-
age, with a maximum of 3.58 Gbps during the times of our
experiments.

Figure 14 shows the result of minProbe in the wild. We
highlight two important takeaways: First, the average of our
estimation is close to 2 Gbps, which was consistent with
the router port statistics collected every 30 seconds. Sec-
ond, we observed that the readings were very bursty, which
means the actual cross traﬃc on NLR exhibited a good deal
of burstiness. In summary, minProbe performed well in the
wild, in the wide area. The available bandwidth estimation
by minProbe agreed with the link utilization computed from
switch port statistical counters: The mean of the minProbe
estimation was 1.7Gbps, which was close to the 30-second

1.00.90.80.70.60.50.40.30.20.1Packet Size Distribution (Dsize)1.00.90.80.70.60.50.40.30.20.1Packet Chain Length Distribution (Dlen)-0.4-0.2 0 0.2 0.41.00.90.80.70.60.50.40.30.20.1Packet Size Distribution (Dsize)1.00.90.80.70.60.50.40.30.20.1Packet Chain Length Distribution (Dlen)-0.4-0.2 0 0.2 0.41.00.90.80.70.60.50.40.30.20.1Packet Size Distribution (Dsize)1.00.90.80.70.60.50.40.30.20.1Packet Chain Length Distribution (Dlen)-0.4-0.2 0 0.2 0.41.00.90.80.70.60.50.40.30.20.1Packet Size Distribution (Dsize)1.00.90.80.70.60.50.40.30.20.1Packet Chain Length Distribution (Dlen)-0.4-0.2 0 0.2 0.4 0 1000 2000 3000 4000 5000 6000 7000 8000 0 2 4 6 8 10Queuing Delay VarianceProbe Rate(Gbps) 0 1000 2000 3000 4000 5000 6000 7000 8000 0 2 4 6 8 10Queuing Delay VarianceProbe Rate(Gbps)416Figure 14: Measurement result in NLR.

average 1.8Gbps computed from switch port statistical coun-
ters. Moreover, minProbe estimated available bandwidth at
higher sample rate and with ﬁner resolution.

Many cloud datacenters and federated testbeds enforce
rate limits on their tenants. For example, the tenants are re-
quired to specify the amount of bandwidth to be reserved at
the time of provisioning. However, many bandwidth estima-
tion algorithms need to send probe traﬃc at high data rates
to temporarily saturate the bottleneck link. As a result, rate
limiting may aﬀect the applicability of the algorithms. We
tested the minProbe middlebox inside a federated testbed,
ExoGENI, with a virtual network provisioned at 1Gbps on a
10Gbps physical network. We used minProbe to send probe
packet trains with diﬀerent lengths at line rate to ﬁnd the
maximum length of packet train that could be sent before
throttled by rate limiters. We found that, in the provisioned
1Gbps virtual network, if the probe train was less than 1200
packets, then there was no observable packet loss. As has
been shown in Section 4.3, minProbe only uses probe trains
with less than 100 probe packets. As a result, and interest-
ingly, rate limiters do not impact the applicability of min-
Probe in federate testbed environments.
4.6 Software Router and Other Hardware

Finally, we evaluate whether other middleboxes, such as
software routers, can be used to perform high-ﬁdelity net-
work measurements required to accurately estimate avail-
able bandwidth without any overhead. In particular, we eval-
uated two software routers, one based on the Linux kernel
network datapath, and another based on the Intel DPDK
driver [18]. We conﬁgured the software routers to perform
L2 forwarding from one port to another. Again, we used the
baseline setup, with the exception that software routers were
placed between minProbe and the switch. The network path
started at the application. After minProbe modulated the
application traﬃc to create packet probe trains, the probe
trains were passed through a software router before being
sent to a 10 GbE switches.

The experiment was generated from probe packet trains
with data rates ranging according to (N, R, G, D) = (20,
[0.5 : 0.5 : 9.5] Gbps, 10 us, 4 ms). We show two ﬁgures
from the this experiment to illustrate the result. First, Fig-
ure 15a shows the CDF of the measured instantaneous data
rate of probe packet pairs passing through a software router
(i.e. we measured the interpacket gap of probe packet pairs
after passing through a software router). We conﬁgured each
software router to forward each packet as soon as it received
the packet to minimize any batching. Since we generated an
equal number of probe packets (20) for a probe train and

(a)

(b)

Figure 15: Software Routers do not exhibit the same ﬁdelity
as minProbe

increased the probe packet train data rate by 0.5 Gbps, the
ideal CDF curve should increase along a diagonal frp, 0.5
Gbps to 9.5 Gbps at 0.5 Gbps increments (the red line).
However, as seen in the ﬁgure, over 25% of the probe packet
pairs in the Intel DPDK-based software router were batched
with minimum interpacket gap: Batching can be seen via the
large increase in the CDF near the highest data rate close
to 10 Gbps. Worse, the vanilla kernel datapath (with ixgbe
kernel driver) batched over 40% of packets. These results
were consistent with the existing research ﬁndings that show
that network interface cards (NICs) generate bursty traﬃc
at sub-100 microsecond timescale. [25]

Second, Figure 15b highlights the same result but slightly
diﬀerently. The x-axis is the packet number and the y-axis
is the measured instantaneous probe packet pair rate (in
Gbps). The ﬁgure shows that minProbe was able to mod-
ulate interpacket gaps to maintain the required probe data
rate (all 20 packets in a probe train exhibited the target
data rate), while the software routers were not able to con-
trol the probe rate at all. To summarize, due to batching,
software routers are not suitable for performing the required
high-ﬁdelity available bandwidth estimation on high-speed
networks.

Next, we investigated the question if prior work such as
MGRP [37] could perform on high-speed networks? To an-
swer this, we setup MGRP to perform the same baseline es-
timation experiment in Figure 8. Unfortunately, the result
was consistent with our ﬁndings above and in Figures 15a
and 15b. In particular, we found that MGRP could not re-
port valid results when it was used in high-speed networks
(10Gbps Ethernet). The accuracy of MGRP was limited
by its capability to precisely timestamp the received probe
packets and its ability to control the probe packet gaps. Both
of these capabilities were performed at kernel level in MGRP
and were prone to operating system level noise. The accu-

 0 1 2 3 4 5 6 7 8 0 1000 2000 3000 4000 5000 6000 7000Estimated Cross Traffic(Gb/s)Time (ms)raw measurement samplesmoving average over 250 msmoving average over 1 s 0 0.2 0.4 0.6 0.8 1 0 2 4 6 8 10CDFBitrate (Gbps)Intel DPDK L3 forward, 1us burstLinux kernel ixgbe L3 forwardminProbe forward 0 2 4 6 8 10 0 50 100 150 200 250 300 350 400Instantaneous Probe Packet Pair Rate(Gpbs)Packet NumberIntel DPDK L3 forward, 1us burstLinux kernel ixgbe L3 forwardminProbe forward417extra layer of indirection makes it hard to optimize appli-
cation performance. MinProbe could potentially bridge the
gap. It can measure the underlying network and provide per-
formance metrics to the virtualized applications to optimize
tasks such as VM placement and route selection.
5.3 Generalization to 1GbE and 40GbE

At the time of writing, minProbe operated only in 10 GbE
networks. However, the high-ﬁdelity capability of minProbe
can be extended to support 1GbE and 40GbE networks as
well. The architecture of the PHY for 40GbE networks is
very similar to the PHY in a 10GbE network. As a result,
as future work, we intend to extend minProbe to support
40GbE. Support for 40GbE networks will require paralleliz-
ing the processing the PHY layer functionality using modern
multi-core architecture. Moreover, as future work, we intend
to extend minProbe to support 1GbE networks. The PHY
for 1GbE networks are less complicated than 10GbE PHY,
which makes it possible to port minProbe to 1GbE. For ex-
ample, the encoding and decoding schemes at physical cod-
ing layer in 1GbE is simpler than the 10GbE physical coding
layer.
5.4 GENI

MinProbe is available on ExoGENI [10]. Users can provi-
sion compute nodes that has SoNIC cards installed and use
it as a minProbe to run network measurement experiments.
Currently, at the time of writing, minProbe was enabled at
two ExoGENI sites: RENCI (Renaissance Computing In-
stitute) and University of California, Davis. We are in the
process of expanding to more sites in the near future.

6. RELATED WORK

Prior work in MAD [43], MGRP [37] and Periscope [15]
provide a thin measurement layer (via userspace daemon and
kernel module) for network measurement in a shared envi-
ronment. These works complement minProbe. MinProbe is
applicable in a shared environment. We also advance the
conversation of available bandwidth estimation to 10Gbps,
unlike most of the prior work, which were only demonstrated
to operate in 1Gbps or less.

We use an algorithm similar to Pathload [20] and
Pathchirp [40] to estimate available bandwidth. There are
many related works on both theoretical and practical aspect
of available bandwidth estimation [7, 20, 23, 29, 30, 32, 38, 40,
42, 44]. Our work contributes more on the practice of avail-
able bandwidth estimation in high speed network (10Gbps).
We found that existing probing parameterizations such as
probe trains are sound and still applicable in high-speed
networks when supported by the appropriate instrumenta-
tion. Furthermore,
[21, 48] mentioned that burstiness of the
cross traﬃc can negatively aﬀect the accuracy of estimation.
While we found that bursty traﬃc introduces noise into the
measurement data, the noise can be ﬁltered out using simple
statistical processing, such as the moving average.

Middleboxes are popular building blocks for current net-
work architecture [41]. MinProbe, when operating as a mid-
dlebox, is similar to software routers in a sense that min-
Probe consists of a data forwarding path and programmable
ﬂow table. Unlike software routers, which typically focus on
high throughput, minProbe has the capability to precisely
control the distance between packets; this capability is ab-
sent in most existing software routers. From the system de-

Figure 16: Estimating Available Bandwidth on diﬀerent
switches

racy of MGRP was better than Pathload [20], it eliminated
the overhead of extra probing traﬃc, but the fundamental
accuracy was still constrained by the measurement platform.
Finally, we studied the sensitivity of bandwidth estima-
tion with respect to network hardware. In particular, we
experimented with three diﬀerent 10 GbE switches: IBM
G8264T, Dell S4810, and NetFPGA-10G reference switch
design [31]. We used the baseline setup for all experiments
and conﬁgured all three switches to perform L2 forward-
ing. Figure 16 illustrates an example of captured queuing
delay after each hardware. As can be seen from the ﬁgure,
the three switches can be used nearly interchangeably since
there was no signiﬁcant diﬀerence in queuing delay. As a
result, we have found that the underlying hardware itself
may not contribute to the available bandwidth estimation.
Instead, as earlier results in the paper indicate, the ﬁdelity
of the available bandwidth estimation depend more on the
ability to control generated interpacket gaps and measure
received interpacket gaps.

5. DISCUSSION

In this section, we discuss the issues regarding the de-
ployment of minProbe, the applications of available band-
width estimation, and whether minProbe can be generalized
to measure bandwidth in 1GbE and 40GbE.
5.1 Deployment

Middleboxes are widely deployed in datacenter networks
to perform a wide spectrum of advanced network process-
ing functions ranging from security (ﬁrewalls, intrusion de-
tection), traﬃc shaping (rate limiters, load balancers), to
improving application performance (accelerators, proxies,
caches), to name a few. Similarly, minProbe deployment
scenarios include middleboxes with enhanced measurement
capabilities potentially located in the same rack as other
servers, or even integrated into each server directly, possi-
bly using hypervisor-based network virtualization, provided
that there exists proper support for accessing the physical
layer of the network in software. In the future, we will ex-
plore the possibility of deploying minProbe as a platform
for providing high-ﬁdelity network measurement as a service
(NMaaS).
5.2 Virtualization and Availability Bandwidth

Estimation

Because of virtualization, datacenter tenants no longer
have direct access to the underlying physical network. The

 0 10 20 30 40 50 60 70 808.008.108.208.308.408.508.608.708.808.909.009.109.209.309.40Probe Packet Queueing Delay (bytes)Probe Traffic Rate(Gbps)NetFPGADellIBM418sign perspective, ICIM [34] has proposed a similar inline net-
work measurement mechanism as our middlebox approach.
However, ICIM has only simulated their proposal, while we
implemented a prototype and performed experiment in real
network. Others have investigated the eﬀect of interrupt co-
alescence [39] and memory I/O subsystem [24], but they are
orthogonal to our eﬀorts. MinProbe, albeit a software tool,
avoids the noise of OS and network stack completely by op-
erating at the physical layer of the network stack.

Another area of related work is precise traﬃc pacing and
precise packet timestamping. They are useful system build-
ing blocks for designing a high-ﬁdelity network measurement
platform. Traditionally, traﬃc pacing [8] is used to smooth
out the burstiness of the traﬃc to improve system perfor-
mance. In minProbe, we use traﬃc pacing in the opposite
way to generate micro-burst of traﬃc to serve as probe pack-
ets. Precise timestamping has been used widely in passive
network monitoring. Typically, this is achieved by dedicated
hardware platform, such as Endace Data Acquisition and
Generation (DAG) card [1]. With minProbe, we achieve the
same nanosecond timestamping precision, and we are able to
use the precise timestamp for active network measurement,
which the traditional hardware platform are incapable of.

7. CONCLUSION

Available bandwidth estimation as a research topic has
been studied extensively, but there still lacks an accurate and
low cost solution in high-speed networks. We present min-
Probe, which is a high-ﬁdelity, minimal-cost and userspace
accessible network measurement approach that is able to
accurately measure available bandwidth in 10 gigabit-per-
second (Gbps) networks. We provided a systematic study of
minProbe’s performance with respect to various cross traf-
ﬁc and probe traﬃc conditions and patterns. We found that
minProbe performs well even under bursty traﬃc, and the
estimation is typically within 0.4 Gbps of the true value in
a 10 Gbps network.

8. AVAILABILITY

The minProbe and SoNIC source code is published un-
der a BSD license and is freely available for download at
http://sonic.cs.cornell.edu. SoNIC nodes are available
for experimentation as bare-metal nodes at the following
ExoGENI sites: RENCI and UCDavis.

9. ACKNOWLEDGMENTS

This work was partially funded and supported by an Intel
Early Career and IBM Faculty Award received by Hakim
Weatherspoon, DARPA (No. D11AP00266), DARPA MRC
(No. FA8750-11-2-0256), NSF CAREER (No. 1053757),
NSF TRUST (No. 0424422), NSF FIA (No. 1040689), NSF
CiC (No. 1047540), and NSF EAGER (No. 1151268). We
would like to thank our shepherd, Oliver Spatsch, and the
anonymous reviewers for their comments. We further rec-
ognize the people helped establish and maintain the net-
work infrastructure on which we performed our experiments:
Eric Cronise and Laurie Collinsworth (Cornell Information
Technologies); Scott Yoest, and his team (Cornell Com-
puting and Information Science Technical Staﬀ). We also
thank the people who provided support for us to perform
experiments on GENI and ExoGENI: Ilya Baldin (RENCI)
and his team (RENCI Network Research and Infrastruture

Program), Jonathan Mills (RENCI), and Ken Gribble (UC
Davis).

10. REFERENCES
[1] Endace DAG Network Cards.

http://www.endace.com/
endace-dag-high-speed-packet-capture-cards.
html.

[2] High Frequency sFlow v5 Counter Sampling.

ftp://ftp.netperf.org/papers/high_freq_sflow/
hf_sflow_counters.pdf.

[3] IEEE Standard 802.3-2008. http:

//standards.ieee.org/about/get/802/802.3.html.

[4] Linux programmer’s manual. http:

//man7.org/linux/man-pages/man7/netlink.7.html.

[5] sﬂow, version 5.

http://www.sflow.org/sflow_version_5.txt.

[6] The CAIDA UCSD Anonymized Internet Traces.

http://www.caida.org/datasets/.

[7] Ahmed Ait Ali, Fabien Michaut, and Francis Lepage.
End-to-End Available Bandwidth Measurement Tools
: A Comparative Evaluation of Performances.
arXiv.org, June 2007.

[8] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall,

Balaji Prabhakar, Amin Vahdat, and Masato Yasuda.
Less is more: Trading a little bandwidth for ultra-low
latency in the data center. In Procs. NSDI, 2012.

[9] Bilal Anwer, Theophilus Benson, Nick Feamster, Dave
Levin, and Jennifer Rexford. A slick control plane for
network middleboxes. In HotSDN, 2013.

[10] Mark Berman, Jeﬀrey S. Chase, Lawrence Landweber,

Akihiro Nakao, Max Ott, Dipankar Raychaudhuri,
Robert Ricci, and Ivan Seskar. Geni: A federated
testbed for innovative network experiments. Comput.
Netw., 61:5–23, March 2014.

[11] Mihai Dobrescu, Norbert Egi, Katerina Argyraki,

Byung-Gon Chun, Kevin Fall, Gianluca Iannaccone,
Allan Knies, Maziar Manesh, and Sylvia Ratnasamy.
Routebricks: exploiting parallelism to scale software
routers. In Proc. SOSP, 2009.

[12] Daniel A. Freedman, Tudor Marian, Jennifer H. Lee,

Ken Birman, Hakim Weatherspoon, and Chris Xu.
Exact temporal characterization of 10 gbps optical
wide-area network. In Proc. IMC, 2010.

[13] Cesar D. Guerrero and Miguel A. Labrador. On the

applicability of available bandwidth estimation
techniques and tools. Comput. Commun., 33(1):11–22,
January 2010.

[14] Sangjin Han, Keon Jang, KyongSoo Park, and Sue

Moon. Packetshader, a gpu-accelerated software
router. In Proc. SIGCOMM, 2010.

[15] Khaled Harfoush, Azer Bestavros, and John Byers.

Periscope: An active internet probing and
measurement api. Technical report, Boston University
Computer Science Department, 2002.

[16] Ningning Hu, Li Erran Li, Zhuoqing Morley Mao,
Peter Steenkiste, and Jia Wang. Locating internet
bottlenecks: algorithms, measurements, and
implications. In Proc. SIGCOMM, 2004, 2004.

[17] P´eter H´aga, Attila P´asztor, Darryl Veitch, and Istv´an

Csabai. Pathsensor: Towards eﬃcient available

419bandwidth measurement. In Proc. IPS-MoMe 2005,
2005.

[18] Intel DPDK. http://www.dpdk.com.
[19] Intel Westmere. http://ark.intel.com/products/

codename/33174/Westmere-EP.

[20] M Jain and C Dovrolis. Pathload: A Measurement

Tool for End-to-End Available Bandwidth. 2002.

[21] Manish Jain and Constantinos Dovrolis. Ten fallacies

and pitfalls on end-to-end available bandwidth
estimation. In Proc. IMC, 2004.

[22] Hao Jiang and Constantinos Dovrolis. Why is the

internet traﬃc bursty in short time scales? In Proc.
SIGMETRICS, 2005.

[23] Guojun Jin and Brian L. Tierney. System capability

eﬀects on algorithms for network bandwidth
measurement. Proc. IMC, 2003, New York, NY, USA,
2003. ACM.

[24] Guojun Jin and Brian L. Tierney. System capability

eﬀects on algorithms for network bandwidth
measurement. In Proc. IMC, 2003.

[25] Rishi Kapoor, Alex C. Snoeren, Geoﬀrey M. Voelker,
and George Porter. Bullet trains: A study of nic burst
behavior at microsecond timescales. In Proc.
CoNEXT, 2013.

[26] Ki Suh Lee, Han Wang, and Hakim Weatherspoon.

Sonic: Precise realtime software access and control of
wired networks. In Proc. NSDI, 2013.

[27] Ki Suh Lee, Han Wang, and Hakim Weatherspoon.

Phy covert channels: Can you see the idles? In Proc.
NSDI, 2014.

[28] ChiunLin Lim, KiSuh Lee, Han Wang, Hakim

Weatherspoon, and Ao Tang. Packet clustering
introduced by routers: modelling, analysis and
experiments. In Proc. CISS, 2014.

[29] Xiliang Liu, Kaliappa Ravindran, Benyuan Liu, and
Dmitri Loguinov. Single-hop probing asymptotics in
available bandwidth estimation: sample-path analysis.
In Proc. IMC, 2004.

[30] Xiliang Liu, Kaliappa Ravindran, and Dmitri

Loguinov. Multi-hop probing asymptotics in available
bandwidth estimation: stochastic analysis. In Proc.
IMC, October 2005.

[31] John W. Lockwood, Nick McKeown, Greg Watson,

Glen Gibb, Paul Hartke, Jad Naous, Ramanan
Raghuraman, and Jianying Luo. NetFPGA–An Open
Platform for Gigabit-Rate Network Switching and
Routing. In Proceedings of Microelectronics Systems
Education, 2007.

[32] Sridhar Machiraju. Theory and Practice of

Non-Intrusive Active Network Measurements. PhD
thesis, EECS Department, University of California,
Berkeley, May 2006.

[33] Sridhar Machiraju and Darryl Veitch. A

measurement-friendly network (mfn) architecture. In
Proc. SIGCOMM INM, Pisa, Italy, 2006.

[34] Cao Le Thanh Man, G. Hasegawa, and M. Murata.

Icim: An inline network measurement mechanism for
highspeed networks. In Proceedings of the 4th
IEEE/IFIP Workshop on End-to-End Monitoring
Techniques and Services, April 2006.

[35] Tudor Marian, Ki Suh Lee, and Hakim Weatherspoon.

Netslices: scalable multi-core packet processing in
user-space. In Proc. ANCS, 2012.

[36] B. Melander, M. Bjorkman, and P. Gunningberg. A

new end-to-end probing and analysis method for
estimating bandwidth bottlenecks. In IEEE Global
Telecommunications Conference, 2000.

[37] Pavlos Papageorge, Justin McCann, and Michael

Hicks. Passive aggressive measurement with mgrp.
SIGCOMM Comput. Commun. Rev., 39(4):279–290,
August 2009.

[38] R Prasad, C Dovrolis, M Murray, and K Claﬀy.

Bandwidth estimation: metrics, measurement
techniques, and tools. Network, 2003.

[39] Ravi Prasad, Manish Jain, and Constantinos Dovrolis.

Eﬀects of interrupt coalescence on network
measurements. In Proceedings of Passive and Active
Measurements Workshop, 2004.

[40] V Ribeiro, R Riedi, R Baraniuk, and J Navratil.

pathchirp: Eﬃcient available bandwidth estimation for
network paths. In Proc. PAM, 2003.

[41] Vyas Sekar, Sylvia Ratnasamy, Michael K. Reiter,

Norbert Egi, and Guangyu Shi. The middlebox
manifesto: Enabling innovation in middlebox
deployment. In Proc. HotNet, 2011.

[42] J Sommers, P Barford, and W Willinger.

Laboratory-based calibration of available bandwidth
estimation tools. Microprocessors and Microsystems,
31(4):222–235, 2007.

[43] Joel Sommers and Paul Barford. An active

measurement system for shared environments. In
IMC, 2007.

[44] Joel Sommers and Paul Barford. An active

measurement system for shared environments. In
Proc. IMC, 2007, October 2007.

[45] Joel Sommers, Paul Barford, and Mark Crovella.

Router primitives for programmable active
measurement. In In Proc. PRESTO, 2009.

[46] Jacob Strauss, Dina Katabi, and Frans Kaashoek. A

Measurement Study of Available Bandwidth
Estimation Tools. In Proc. SIGCOMM, 2003.

[47] Kashi Venkatesh Vishwanath and Amin Vahdat.

Realistic and responsive network traﬃc generation. In
Proc. SIGCOMM, 2006.

[48] Kashi Venkatesh Vishwanath and Amin Vahdat.
Evaluating distributed systems: Does background
traﬃc matter? In Proc. USENIX ATC, 2008.

[49] Yong Xia, Lakshminarayanan Subramanian, Ion

Stoica, and Shivkumar Kalyanaraman. One more bit
is enough. SIGCOMM Comput. Commun. Rev.,
35(4):37–48, August 2005.

[50] Curtis Yu, Cristian Lumezanu, Yueping Zhang, Vishal

Singh, Guofei Jiang, and Harsha V. Madhyastha.
Flowsense: Monitoring network utilization with zero
measurement cost. In In Proc. PAM, 2013.

420