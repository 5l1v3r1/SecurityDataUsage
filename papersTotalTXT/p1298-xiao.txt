Protecting Locations with Differential Privacy under

Temporal Correlations

Dept. of Math and Computer Science

Dept. of Math and Computer Science

Li Xiong

Emory University

lxiong@emory.edu

Yonghui Xiao

Emory University

yonghui.xiao@emory.edu

ABSTRACT
Concerns on location privacy frequently arise with the rapid
development of GPS enabled devices and location-based ap-
plications. While spatial transformation techniques such as
location perturbation or generalization have been studied
extensively, most techniques rely on syntactic privacy mod-
els without rigorous privacy guarantee. Many of them only
consider static scenarios or perturb the location at single
timestamps without considering temporal correlations of a
moving user’s locations, and hence are vulnerable to vari-
ous inference attacks. While diﬀerential privacy has been
accepted as a standard for privacy protection, applying dif-
ferential privacy in location based applications presents new
challenges, as the protection needs to be enforced on the ﬂy
for a single user and needs to incorporate temporal correla-
tions between a user’s locations.

In this paper, we propose a systematic solution to preserve
location privacy with rigorous privacy guarantee. First, we
propose a new deﬁnition, “δ-location set” based diﬀerential
privacy, to account for the temporal correlations in location
data. Second, we show that the well known (cid:96)1-norm sensi-
tivity fails to capture the geometric sensitivity in multidi-
mensional space and propose a new notion, sensitivity hull,
based on which the error of diﬀerential privacy is bound-
ed. Third, to obtain the optimal utility we present a pla-
nar isotropic mechanism (PIM) for location perturbation,
which is the ﬁrst mechanism achieving the lower bound of
diﬀerential privacy. Experiments on real-world datasets al-
so demonstrate that PIM signiﬁcantly outperforms baseline
approaches in data utility.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: Gener-
al—Security and protection; K.4.1 [Computers and Soci-
ety]: Public Policy Issues—Privacy

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813640.

Keywords
Location privacy; Location-based services; Diﬀerential pri-
vacy; Sensitivity hull; Planar isotropic mechanism

1.

INTRODUCTION

Technology and usage advances in smartphones with lo-
calization capabilities have provided tremendous opportu-
nities for location based applications. Location-based ser-
vices (LBS) [20, 8] range from searching points of inter-
est to location-based games and location-based commerce.
Location-based social networks allow users to share location-
s with friends, to ﬁnd friends, and to provide recommenda-
tions about points of interest based on their locations.

One major concern of location based applications is lo-
cation privacy [3]. To use these applications, users have to
provide their locations to the respective service providers or
other third parties. This location disclosure raises important
privacy concerns since digital traces of users’ whereabouts
can expose them to attacks ranging from unwanted location
based spams/scams to blackmail or even physical danger.

Gaps in Existing Works and New Challenges. Many
location privacy protection mechanisms have been proposed
during the last decade [23, 15] in the setting of LBS or con-
tinual location sharing.
In such setting, a user sends her
location to untrusted service providers or other parties in
order to obtain some services (e.g. to ﬁnd the nearest restau-
rant). One solution is Private Information Retrieval (PIR)
technique, based on cryptography instead of revealing indi-
vidual locations (e.g. [32]). However, such technique tends
to be computationally expensive and not practical in ad-
dition to requiring diﬀerent query plans to be designed for
diﬀerent query types.

Most solutions proposed in the literature are based on lo-
cation obfuscation which transforms the exact location of a
user to an area (location generalization) or a perturbed lo-
cation (location perturbation) (e.g. [14, 1]). Unfortunately,
most spatial transformation techniques proposed so far rely
on syntactic privacy models such as k-anonymity, or ad-hoc
uncertainty models, and do not provide rigorous privacy.
Many of them only consider static scenarios or perturb the
location at single timestamps without considering the tem-
poral correlations of a moving user’s locations, and hence
are vulnerable to various inference attacks. Consider the
following examples.

I Suppose a user moved from school to the cafeteria (where
“(cid:63)” is) in Figure 1 (left). Three perturbed locations were
released by selecting a point probabilistically in each of

1298Figure 1: Examples of privacy breach caused by temporal corre-
lations of user locations

the three circles (by some spatial cloaking methods).
Even though the individual locations were seemingly
protected at each timestamp, considering them togeth-
er with road constraints or the user’s moving pattern
will enable an adversary to accurately ﬁgure out the
user is in the cafeteria, resulting in privacy breach.

II Suppose a user’s location “(cid:63)” is protected in a circle
as shown in Figure 1 (right). If by estimation based on
previous locations the user can only be in the ﬁve places
at current timestamp as shown in the ﬁgure, then the
obfuscated location actually exposes the true location.
Thus technically, the radius of the circle (in location
obfuscation) should be subject to temporal correlations.

While such temporal correlations can be commonly modeled
by Markov chain [37, 17, 25], and few works have considered
such Markov models [37, 17], it remains a challenge to pro-
vide rigorous privacy protection under temporal correlations
for continual location sharing.

Diﬀerential privacy [9] has been accepted as a standard for
privacy preservation. It was originally proposed to protect
aggregated statistics of a dataset by bounding the knowledge
gain of an adversary whether a user opts in or out of a
dataset. Applying diﬀerential privacy for location protection
is still at an early stage. In particular, several works (e.g.
[6, 33, 12]) have applied diﬀerential privacy on location or
trajectory data but in a data publishing or data aggregation
setting. In this setting, a trusted data publisher with access
to a set of location snapshots or user trajectories publishes
an aggregate or synthetic view of the original data while
guaranteeing user-level diﬀerential privacy, i.e. protecting
the presence of a user’s location or entire trajectory in the
aggregated data.

There are several challenges in applying diﬀerential priva-
cy in the new setting of continual location sharing. First,
standard diﬀerential privacy only protects user-level privacy
(whether a user opts in or out of a dataset); while in our
setting, the protection needs to be enforced on the ﬂy for a
single user. Second, as shown in Figure 1, temporal correla-
tions based on road networks or the user’s moving patterns
exist and the privacy guarantee needs to account for such
correlations. Finally, there is no eﬀective location release
mechanism with diﬀerential privacy under such model.

Contributions. In this paper, we propose a systematic so-
lution to preserve location privacy with diﬀerential privacy
guarantee. As shown in Figure 2, we consider a moving user
with sensitive location stream who needs to share her lo-
cations to an untrusted location-based application host or
other parties. A user’s true locations are only known by
the user. The “sanitized” locations released by the priva-

Figure 2: Problem setting

cy mechanisms are observable to the service providers, as
well as adversaries. To enable private location sharing, we
address (and take advantage of) the temporal correlations,
which can not be concealed from adversaries and hence are
assumed to be public. Our contributions are summarized as
follows.

First, we propose “δ-location set” based diﬀerential pri-
vacy to protect the true location at every timestamp. The
“neighboring databases” in standard diﬀerential privacy are
any two databases under one operation: adding or removing
a record (or a user). However, this is not applicable in a
variety of settings [21, 5], which leads to new and extended
notions such as δ-neighborhood [13] or event-level [11] diﬀer-
ential privacy. In our problem, location changes between two
consecutive timestamps are determined by temporal correla-
tions modeled through a Markov chain [37, 17]. Accordingly
we propose a “δ-location set” to include all probable location-
s (where the user might appear). Intuitively, to protect the
true location, we only need to “hide” it in the δ-location set
in which any pairs of locations are not distinguishable.

Second, we show that the well known (cid:96)1-norm sensitivity
in standard diﬀerential privacy fails to capture the geomet-
ric sensitivity in multidimensional space. Thus we propose a
new notion, sensitivity hull, to capture the geometric mean-
ing of sensitivity. We also prove that the lower bound of
error is determined by the sensitivity hull.

Third, we present an eﬃcient location perturbation mech-
anism, called planar isotropic mechanism (PIM), to achieve
δ-location set based diﬀerential privacy.

I To our knowledge, PIM is the ﬁrst optimal mechanism
that can achieve the lower bound of diﬀerential priva-
cy1. The novelty is that in two-dimensional space we
eﬃciently transform the sensitivity hull to its isotropic
position such that the optimality is guaranteed.

II We also implement PIM on real-world datasets, show-
ing that it preserves location utility for location based
queries and signiﬁcantly outperforms the baseline Laplace
mechanism (LM).

2. PRELIMINARIES

We denote scalar variables by normal letters, vectors by
bold lowercase letters, and matrices by bold capital letters.
We use || · ||p to denote the (cid:96)p norm, x[i] to denote the ith
1The state-of-art diﬀerentially private mechanisms [18, 4]
for linear queries can be O(log(d)) approximately optimal
where d is the number of dimensions.

(East)(North)?123(East)(North)?timet1t2t3t4Location Based ApplicationsPrivacy MechanismsPerturbedlocationPerturbedlocationTruelocationTruelocationTruelocationTruelocationTruelocationuntrustedusert51299si

u, x
u∗, x∗

z
p−
t
p+
t
∆X
K

a cell in a partitioned map, i = 1, 2, · · · , m
location in state and map coordinates

true location of the user

the released location in map coordinate
prior probability (vector) at timestamp t

posterior probability (vector) at timestamp t

δ-location set
sensitivity hull

Table 1: Denotation

Figure 3: Two coordinate systems

element of x, E() to denote the expectation, xT to denote the
transpose of vector x. Table 1 summarizes some important
symbols for convenience.
2.1 Two Coordinate Systems

We use two coordinate systems, state coordinate and map
coordinate, to represent a location for the Markov model and
map model respectively. Denote S the domain of space. If
we partition S into the ﬁnest granularity, denoted by “cell”,
then S = {s1, s2,··· , sm} where each si is a unit vector with
the ith element being 1 and other m − 1 elements being 0.
Each cell can represent a state (location) of a user. On the
other hand, If we view the space as a map with longitude
and latitude, then a 2 × 1 vector can be used to represent a
user’s location x with two components x[1] and x[2]. Figure
3 shows an example using these two coordinate systems. If
a user is in s7, the state coordinate and map coordinate are
shown as follows. Note that the two coordinate systems can
be transformed to each other. We skip how to transform
them and treat u and x interchangeable.

u = s7 =(cid:2) 0 0 0 0 0 0 1 0 ··· 0 (cid:3)

x = [2, 4]T with x[1] = 2 and x[2] = 4

As time evolves, the trace of a user can be represented by
a series of locations, x1, x2,··· , xt in map coordinate or
u1, u2,··· , ut in state coordinate.
2.2 Mobility and Inference Model

Our approach uses Markov chain [37, 17, 25] to model
the temporal correlations between user’s locations. Other
constraints, such as road network, can also be captured by
it. However, we note that Markov model, as well as any
mobility models, may have limits in terms of predicability
[38]. And we will discuss our solution to address these limits
later.

In our problem setting, a user’s true locations are unob-
servable, i.e. only known by the user. The “sanitized” loca-
tions released by the perturbation mechanism are observable
to the service provider, as well as adversaries. Thus from an
adversarial point of view, this process is a Hidden Markov
Model (HMM).

At timestamp t, we use a vector pt to denote the probabil-
ity distribution of a user’s location (in each cell). Formally,

∗
∗
t = si) = P r(x
pt[i] = P r(u
t = the coordinate of si)

where pt[i] is the ith element in pt and si ∈ S. In the exam-
ple of Figure 3, if the user is located in cells {s2, s3, s7, s8}
with a uniform distribution, the probability vector can be
expressed as follows.

p =(cid:2) 0 0.25 0.25 0 0 0 0.25 0.25 0 ··· 0 (cid:3)

Transition Probability. We use a matrix M to denote the
probabilities that a user moves from one location to another.
Let mij be the element in M at ith row and jth column.
Then mij represents the probability that a user moves from
cell i to cell j. Given probability vector pt−1, the probabil-
ity at timestamp t becomes pt = pt−1M. We assume the
transition matrix M is given in our framework.
Emission Probability. If given a true location u∗
t , a mech-
anism releases a perturbed location zt, then the probabili-
ty P r(zt|u∗
t = si) is called “emission probability” in HMM.
This probability is determined by the release mechanism and
should be transparent to adversaries.
Inference and Evolution. At timestamp t, we use p−
t
and p+
to denote the prior and posterior probabilities of
t
a user’s location before and after observing the released zt
respectively. The prior probability can be derived by the
posterior probability at previous timestamp t − 1 and the
Markov transition matrix as p−
t−1M. Given zt, the
posterior probability can be computed using Bayesian infer-
ence as follows. For each cell si:

t = p+

t = si|zt) =
∗
t [i] = P r(u

p+

(cid:80)

j

P r(zt|u∗
P r(zt|u∗

t = si)p−
t [i]
−
t = sj)p
t [j]

(1)

The inference at each timestamp can be eﬃciently com-
puted by forward-backward algorithm in HMM, which will
be incorporated in our framework.
2.3 Differential Privacy and Laplace Mecha-

nism

Definition 2.1

(Differential Privacy). A mechanis-

m A satisﬁes -diﬀerential privacy if for any output z and
any neighboring databases x1 and x2 where x2 can be ob-
tained from x1 by either adding or removing one record2, the
following holds

P r(A(x1) = z)
P r(A(x2) = z)

≤ e

Laplace mechanism [10] is commonly used in the literature
It is built on the (cid:96)1-norm

to achieve diﬀerential privacy.
sensitivity, deﬁned as follows.

2This is the deﬁnition of unbounded diﬀerential privacy [21].
Bounded neighboring databases can be obtained by chang-
ing the value of exactly one record.

123456891011121314151617181920212223123451234571300Definition 2.2

((cid:96)1-norm Sensitivity). For any query
f (x): x → Rd, (cid:96)1-norm sensitivity is the maximum (cid:96)1 norm
of f (x1) − f (x2) where x1 and x2 are any two instances in
neighboring databases.

Sf =

x1,x2∈ neighboring databases

max

where || · ||1 denotes (cid:96)1 norm.

||f (x1) − f (x2)||1

A query can be answered by f (x) + Lap(Sf /) to achieve
-diﬀerential privacy, where Lap() ∈ Rd are i.i.d. random
noises drawn from Laplace distribution.
2.4 Utility Metrics

To measure the utility of the perturbed locations, we fol-
low the analysis of metrics in [37] and adopt the expected
distance (called “correctness” in [37]) between the true loca-
tion x∗ and the released location z as our utility metric.

(cid:113)E||z − x∗||2

2

Error =

(2)

In addition, we also study the utility of released locations
in the context of location based queries such as ﬁnding n-
earest k Points of Interest (POI). We will use precision and
recall as our utility metrics in this context which we will
explain later in the experiment section.
2.5 Convex Hull

Our proposed sensitivity hull is based on the well studied
notion of convex hull in computational geometry. We brieﬂy
provide the deﬁnition here.

Definition 2.3

(Convex Hull). Given a set of points
X = {x1, x2,··· , xn}, the convex hull of X is the smallest
convex set that contains X.

Note that a convex hull in two-dimensional space is a poly-
gon (also called “convex polygon” or “bounding polygon”).
Because it is well-studied and implementations are also avail-
able [31], we skip the details and only use Conv(X) to denote
the function of ﬁnding the convex hull of X.

3. PRIVACY DEFINITION

To apply diﬀerential privacy in the new setting of contin-
ual location sharing, we conduct a rigorous privacy analysis
and propose δ-location set based diﬀerential privacy in this
section.
3.1

δ-Location Set

The nature of diﬀerential privacy is to “hide” a true database

in “neighboring databases” when releasing a noisy answer
from the database. In standard diﬀerential privacy, neigh-
boring databases are obtained by either adding or removing
a record (or a user) in a database. However, this is not
applicable in our problem. Thus we propose a new notion,
δ-location set, to hide the true location at every timestamp.

Motivations. We ﬁrst discuss the intuitions that motivates
our deﬁnition.

First, because the Markov model is assumed to be pub-
lic, adversaries can make inference using previously released
locations. Thus we, as data custodians in a privacy mecha-
nism, can also track the temporal inference at every times-
tamp. At any timestamp, say t, a prior probability of the

user’s current location can be derived, denoted by p−
follows.

t as

t = si|zt−1,··· , z1)
∗
−
t [i] = P r(u

p

Similar to hiding a database in its neighboring databases,
we can hide the user’s true location in possible locations
(where p−
t [i] > 0). On the other hand, hiding the true
location in any impossible locations (where p−
t [i] = 0) is
a lost cause because the adversary already knows the user
cannot be there.

Second, a potential shortcoming of Markov model is that
the probability distribution may converge to a stationary
distribution after a long time (e.g. an ergodic Markov chain).
Intuitively, a user’s possible locations can eventually cover
the entire map given enough time. Hiding a location in a
large area may yield a signiﬁcantly perturbed location that
is not useful at all.

According to [16], moving patterns of human have a “high
degree” of temporal and spatial regularity. Hence if people
tend to go to a number of highly frequented locations, our
privacy notion should also emphasize protecting the more
probable locations in Markov model.

δ-Location Set. With above motivations, we deﬁne δ-
location set at any timestamp t, denoted as ∆Xt. Essen-
tially, δ-location set reﬂects a set of probable locations the
user might appear (by leaving out the locations of small
probabilities).

Definition 3.1

(δ-Location Set). Let p−

t be the pri-
or probability of a user’s location at timestamp t. δ-location
set is a set containing minimum number of locations that
have prior probability sum no less than 1 − δ.
t [i] ≥ 1 − δ}
−
p

∆Xt = min{si|(cid:88)

si

For example, if p−
t = [0.3, 0.4, 0.05, 0.2, 0.03, 0.02] correspond-
ing to [s1, s2, s3, s4, s5, s6], then ∆X = {s2, s1, s4} when
δ = 0.1; ∆X = {s2, s1, s4, s3} when δ = 0.05.

Note that if δ = 0 the location set contains all possible lo-
cations. Thus 0-location set preserves the strongest privacy.

Drift. Because δ-location set represents the most proba-
ble locations, a drawback is that the true location may be
ﬁltered out with a small probability (technically, P r(x∗ /∈
∆X) = δ). Same situation may also occur if the Markov
model is not accurate enough in practice due to its limit in
predicability, as we mentioned earlier. Therefore, we denote
this phenomenon as “drift” and handle it with the following
surrogate approach.

Surrogate. When a drift happens, we use a surrogate lo-
cation in ∆X as if it is the “true” location in the release
mechanism.

Definition 3.2

in ∆X with the shortest distance to the true location x∗.

(Surrogate). A surrogate ˜x is the cell

˜x = argmin

s∈∆X

∗
dist(s, x

)

where function dist() denotes the distance between two cells.

Note that the surrogate approach does not leak any informa-
tion of the true location, explained as follows. If x∗ ∈ ∆X,
then x∗ is protected in ∆X; if not, ˜x is protected in ∆X.

1301Using surrogate does not reveal whether x∗ is in ∆X or not.
Because in any location release mechanisms x∗ is treated as
a black box (oblivious to adversaries), replacing x∗ with ˜x is
also a black box. We formally prove the privacy guarantee
in Theorem 5.1.

In some cases, a surrogate may be far from the true loca-
tion. Then the released location may not be useful. There-
fore, we also measure the distance between released location
and true location in our experiment to reﬂect the long-term
eﬀect of surrogate.
3.2 Differential Privacy on δ-Location Set

We deﬁne diﬀerential privacy based on δ-location set, with
the intuition that the released location zt will not help an
adversary to diﬀerentiate any instances in the δ-location set.

Definition 3.3

(Differential Privacy). At any times-

tamp t, a randomized mechanism A satisﬁes -diﬀerential
privacy on δ-location set ∆Xt if, for any output zt and any
two locations x1 and x2 in ∆Xt, the following holds:

P r(A(x1) = zt)
P r(A(x2) = zt)

≤ e

(3)

Above deﬁnition guarantees the true location is always
protected in δ-location set at every timestamp. In anoth-
er word, the released location zt is diﬀerentially private at
timestamp t for continual location sharing under temporal
correlations. For other application settings, like protecting
the trace or trajectory of a user, we defer the investigation
to future works.
3.3 Adversarial Knowledge

In reality, there may be a variety of adversaries with all
kinds of prior knowledge. Accordingly, we prove that for the
problem of continual location sharing diﬀerential privacy is
equivalent to adversarial privacy, ﬁrst studied in [35].

Definition 3.4

(Adversarial Privacy). A mechanis-
m is -adversarially private if for any location si ∈ S, any
output z and any adversaries knowing the true location is in
∆X, the following holds:
P r(u∗
P r(u∗

≤ e

t = si|zt)
t = si)
t = si) and P r(u∗

where P r(u∗
posterior probabilities of any adversaries.

(4)
t = si|zt) are the prior and

We can show Deﬁnition 3.3 is equivalent to adversarial pri-
vacy for continual location sharing, which can be derived
from the PTLM property [35].

Theorem 3.1. For the problem of continual location shar-

ing, Deﬁnition 3.3 is equivalent to Deﬁnition 3.4.

Deﬁnition 3.4 limits the information gain for adversaries
t /∈ ∆X, our frame-
knowing the condition x∗
work reveals no extra information due to the surrogate ap-
proach. Thus adversarial knowledge can be bounded, dis-
cussed as follows.

t ∈ ∆X. If x∗

Standard Adversary. For adversaries who have exactly
the same Markov model and keep tracking all the released
locations, their knowledge is also the same as our model

(with location inference in Section 5.3). In this case, diﬀer-
ential privacy and adversarial privacy are guaranteed, and
we know exactly the adversarial knowledge, which in fact
can be controlled by adjusting .

Weak Adversary. For adversaries who have little knowl-
edge about the user, the released locations may help them
obtain more information. With enough time to evolve, they
may converge to standard adversaries eventually. But their
adversarial knowledge will not exceed standard adversaries.

Strong Adversary. For adversaries who have additional
information, the released location from diﬀerential privacy
may not be very helpful. Speciﬁcally, a strong adversary
with auxiliary information may have more accurate prior
knowledge. However, if the adversary cannot identify the
true location so that P r(u∗ = si) = 1 for any si ∈ S, Deﬁ-
nition 3.4 is always satisﬁed. On the other hand, if an “om-
nipotent” adversary already knows the true location, then
no mechanism can actually protect location privacy.
3.4 Comparison with Other Deﬁnitions
Diﬀerential Privacy. Since the concept of neighboring
databases is not generally applicable (as discussed earlier),
induced neighborhood [21], metric based neighborhood [5]
and δ-neighborhood [13] were proposed. The general idea
is that the neighborhood can be formulated by some con-
straints of data or distance (metric) functions instead of
adding or removing a record. However, applying these neigh-
borhood based diﬀerential privacy is not feasible in our mod-
el because there is only one sole tuple (location) at each
timestamp without any “neighbors”. Hence we deﬁne δ-
location set to extend the notion of “neighborhood”.

Geo-indistinguishability. Another closely related deﬁ-
nition is the Geo-indistinguishability [1], which protects a
user’s location within a radius (circle) with a “generalized
diﬀerential privacy” guarantee. In other words, the neigh-
borhood is deﬁned with Euclidian distance. Nevertheless,
such spatial perturbation technique may not be reasonable
in reality. For example, as shown in Figure 1, the “gener-
alized diﬀerential privacy” can still be breached given the
road network constraint or user’s moving pattern (which is
represented by Markov model). Thus location privacy must
be protected under temporal correlations.

Blowﬁsh privacy. Our privacy deﬁnition shares the same
insight as the unconstrained Blowﬁsh privacy framework [19]
in statistical data release context, which uses secret pairs
and privacy policy to build a subset of possible database
instances as “neighbors”. We show that δ-location set based
diﬀerential privacy can be instantiated as a special case of
unconstrained Blowﬁsh privacy at each timestamp.

Theorem 3.2. Let S be the domain of all possible lo-
cations. Let G be a complete graph where each node de-
notes a location in S. Let ∆X be a condition such that
x∗ ∈ ∆X. At each timestamp, Deﬁnition 3.3 is equivalent
to {,{S, G, ∆X}}-Blowﬁsh privacy.
3.5 Discussion
Learning Markov Model. Existing methods such as the
knowledge construction module in [37] or EM method in
HMM can be used to acquire the transition matrix M, which
will not be discussed in this paper. However, depending on
the power of adversaries, two typical M can be learned.

1302I Popular M can be learned from public transit data.

II Personal M can be derived with personal transit data3.

No matter which M is adopted in our framework, the ad-
versarial knowledge is always bounded, as discussed before.
However, the usefulness of released locations may vary for
diﬀerent adversaries. We also compare the two models in
our experiments.

Composibility. Since we only need to release one per-
turbed location at a timestamp, the sequential composition
[28] is not applicable. Otherwise, for multiple releases at a
timestamp the composition of  holds. On the other hand,
given a series of perturbed locations {z1, z2,··· , zt} released
from timestamp 1 to t, a new problem is how to protect and
measure the overall privacy guarantee of the entire trace.
We defer this to future work.

4. SENSITIVITY HULL

The notion of sensitivity indicates the diﬀerences between
any two query answers from two instances in neighboring
databases. However, in multidimensional space, we show
that (cid:96)1-norm sensitivity (in Deﬁnition 2.2) fails to capture
the exact sensitivity. Thus we propose a new notion, sensi-
tivity hull. Note that sensitivity hull is an independent no-
tion from the context of location privacy and can be plugged
in any data-independent perturbation mechanisms.
4.1 Sensitivity Hull

To derive the meaning of sensitivity, let us consider the
following example in traditional setting of diﬀerential priva-
cy.

Example 4.1. Assume we have an employee table T with
attributes gender and income. Then we answer the following
query workload f :
f1 : Select count(∗) f rom T where gender = “f emale”
f2 : Select count(∗) f rom T where income > 50000

Let x1 and x2 be neighboring databases so that x1 is equal
to x2 adding or removing a random user. Suppose f (x2) =
[10, 20]T . Then the possible answers for f (x1) could be one
of the following columns, from which ∆f can be derived.

(cid:20) 11

21

f (x1) =

(cid:21)

(cid:21)

10
21

10
20

9
20

9
19

10
19

11
20

(cid:20) 1

1

0
0

0
1

1 −1 −1
0

0
0 −1 −1

∆f = f (x1) − f (x2) =
Sf = max||∆f||1 = 2 ((cid:96)1-norm sensitivity)
In Figure 4, the dashed lines form the set of ||∆f||1 = 2
because the (cid:96)1-norm sensitivity is 2. However, ∆f only con-
sists of all the “•” points.
It is obvious that the (cid:96)1-norm
sensitivity exaggerates the “real sensitivity”. To capture the
geometric representation of ∆f in multidimensional space,
we deﬁne sensitivity hull (the solid lines in Figure 4) as fol-
lows.

Definition 4.1

(Sensitivity Hull). The sensitivity
hull of a query f is the convex hull of ∆f where ∆f is the

3For example, mobile apps, like Google Now, may have a
user’s location history to derive the user’s moving pattern.

Figure 4: Sensitivity hull of Example 4.1. Solid lines denote the
sensitivity hull K; dashed lines are the (cid:96)1-norm sensitivity.

set of f (x1) − f (x2) for any pair x1 and x2 in δ-location set
∆X.

K = Conv (∆f )

∆f =

x1,x2∈∆X

∪

(f (x1) − f (x2))

Theorem 4.1. A sensitivity hull K is centrally symmet-

ric: if v ∈ K then −v ∈ K.

Theorem 4.2. If data x is in discrete domain, then for
any f : x → Rd, the sensitivity hull of f is a polytope in Rd.
4.2 Error Bound of Differential Privacy

We extend the error bound of diﬀerential privacy in database

context [18] to our location setting using sensitivity hull.

Lemma 4.1. Suppose F : RN → Rd is a linear function.
When neighboring databases are obtained by adding or re-
moving a record, the sensitivity hull K of F is a polytope
FBN

1 is the N -dimensional unit (cid:96)1 ball.

1 where BN

Theorem 4.3

(Lower Bound). Let K be the sensitiv-
ity hull of δ-location set ∆X. To satisfy Deﬁnition 3.3, every
mechanism must have

(cid:19)
(cid:112)Area(K)

(cid:18) 1



Error ≥ Ω

where Area(K) is the area of K.

5. LOCATION RELEASE ALGORITHM
5.1 Framework

The framework of our proposed location release algorithm
is shown in Algorithm 1. At each timestamp, say t, we com-
pute the prior probability vector p−
t . If the location needs
to be released, we construct a δ-location set ∆Xt. Then if
the true location x∗ is excluded in ∆Xt (a drift), we use
surrogate to replace x∗. Next a diﬀerentially private mecha-
nism (like Algorithm 2 which will be presented next) can be
adopted to release a perturbed location zt. In the meantime,
the released zt will also be used to update the posterior prob-
ability p+
t (in the equation below) by Equation (1), which
subsequently will be used to compute the prior probability
for the next timestamp t + 1. Then at timestamp t + 1, the
above process is repeated.
t = si|zt, zt−1,··· , z1)
∗
t [i] = P r(u

p+

f1f2o1−1−11K‘1-normsensitivity2−22−21303(cid:46) Markov transition

t

t−1M;

t−1, x∗

t ← p+

Algorithm 1 Framework
Require: t, δ, M, p+
1: p−
2: if location needs to be released then
3:
4:
5:
6:
7:
8:
9: end if
10: return Algorithm 1(t+1, δ, M, p+

Construct ∆Xt;
t /∈ ∆Xt then
if x∗
t ← surrogate;
x∗
end if
zt ←Algorithm 2(t, ∆Xt, x∗
t );
Derive posterior probability p+
t by Equation (1);

t , x∗

t+1);

(cid:46) δ-location set
(cid:46) a drift

(cid:46) release zt

(cid:46) go to next timestamp

Theorem 5.1. At any timestamp t, Algorithm 1 is t-

diﬀerentially private on 0-location set.

Proof. It is equivalent to prove adversarial privacy on
t ∈
0-location set, which includes all possible locations. If x∗
∆Xt, then zt is generated by x∗
t . By Theorem 5.3, zt is
t-diﬀerentially private. So P r(u∗
t =si|zt)
t =si) ≤ e. When x∗
t /∈
P r(u∗
∆Xt, then a surrogate ˜xt replaces x∗
t = si|˜xt = sk)P r(˜xt = sk|zt)
P r(u∗
t = si|˜xt = sk)P r(˜xt = sk)
P r(u∗
Therefore, by equivalence (Theorem 3.1) Algorithm 1 is t-
diﬀerentially private on 0-location set.

(cid:80)
(cid:80)
k P r(u∗
k P r(u∗

t = si|zt)
t = si)

t . Then

=

≤ e

Laplace Mechanism. With the (cid:96)1-norm sensitivity in Def-
inition 2.2, Laplace mechanism (LM) can be adopted in Line
7 of Algorithm 1. The problem of this approach is that it will
over-perturb a location because (cid:96)1-norm sensitivity could be
much larger than the sensitivity hull, as discussed in Section
4. We use LM with δ-location set as a baseline in our ex-
periment.
5.2 Planar Isotropic Mechanism

Because we showed (in Lemma 4.1) that the sensitivity
hull of a query matrix is a polytope (polygon in our two-
dimensional location setting), the state-of-art K-norm based
mechanism [18, 4, 30] can be used.

Definition 5.1

(K-norm Mechanism [18]). Given a
linear function F : RN → Rd and its sensitivity hull K,
a mechanism is K-norm mechanism if for any output z, the
following holds:

1

P r(z) =

Γ(d + 1)Vol(K/)

(5)
where Fx∗ is the true answer, ||·||K is the (Minkowski) norm
of K, Γ() is Gamma function and Vol() indicates volume.

exp (−||z − Fx

∗||K )

However, standard K-norm mechanism was designed for
high-dimensional structure of sensitivity hull, whereas in our
problem a location is only two-dimensional. Thus we can
further optimize K-norm mechanism to achieve the lower
bound of diﬀerential privacy. We propose a Planar Isotropic
Mechanism (PIM) based on K-norm mechanism as follows.

Rationale. The rationale of PIM is that in two-dimensional
space we eﬃciently transform the sensitivity hull to its isotrop-
ic position4 so that the optimality is guaranteed.
4We refer readers to [2, 29] for a detailed study of isotropic
position.

Theorem 5.2. [18] If the sensitivity hull K is in C-appro-
ximately isotropic position, then K-norm mechanism has er-
ror O(C)LB(K) where LB(K) is the lower bound of diﬀer-
ential privacy.

From Theorem 5.2, we know that K-norm mechanism
would be the optimal solution if the sensitivity hull K is
in isotropic position, denoted by KI . Although in high-
dimensional space transforming a convex body to its isotrop-
ic position is extremely expensive, it is feasible in two-dimensional
space. To this end, we need the following corollary (which
can be derived from [36, 27]).

Corollary 5.1

(Isotropic Transformation). For any

convex body K in R2, any integer p ≥ 1, there is an abso-
lute constant c such that if l ≥ 4cp2, with probability at least
1 − 2−p, KI = TK is in isotropic position.

(cid:33)− 1

2

(cid:32)

l(cid:88)

i=1

1
l

T =

yiyT

i

(6)

where y1, y2,··· , yl are independent random points uniform-
ly distributed in K.

Therefore, the isotropic transformation of any sensitivity
hull K can be fulﬁlled by sampling, which is a trivial task
in two-dimensional space. For instance, a hit-and-run algo-
rithm [26] only takes O(log3(1/δ)) time where δ is an error
parameter. We skip the sampling details and refer readers
to the survey paper of Santosh Vempala [39] for a complete
study.

Algorithm. As an overview, PIM involves the following
steps:

(1) Compute sensitivity hull K from ∆X;
(2) Transform K to isotropic position KI ;
(3) generating a noise in the space of KI by K-norm mech-

anism;

(4) Transform to the original space.
We ﬁrst describe how to compute sensitivity hull K. Sup-
pose we have a δ-location set ∆X at a timestamp. We
can ﬁrst derive the convex hull of ∆X, denoted by K(cid:48) =
Conv(∆X). For example, in Figure 5a, the convex hull K(cid:48)
is shown by the black lines given δ-location set as “•” and
“(cid:63)” where “(cid:63)” is the true location. Denote v1, v2,··· , vh the
vertices of K(cid:48). Then we use a set ∆V to store vi−vj for any
vi and vj from the vertices of K(cid:48) as the equation below. In
Figure 5b, for instance, the polygon “(cid:52) · · · (cid:52)” denotes vi−v1

for all vi. Then Conv(∆V) will be the sensitivity hull K of
the δ-location set, as shown by the polygon with solid lines
in Figure 5b.

K = Conv(∆V)
∪

v1,v2∈ vertices of K(cid:48)(v1 − v2)

∆V =

Next we transform K to its isotropic position KI . We
sample y1, y2,··· , yl uniformly from K. Then a matrix T
can be derived by Equation (6). To verify if T is stable, we
can derive another T(cid:48). If the Frobenius norm ||T(cid:48) − T||F is
small enough (e.g. < 10−3), then we accept T. Otherwise
we repeat above process with larger l. In the end, KI = TK
is the isotropic position of K, as shown in Figure 5c.
Next a point z(cid:48) can be uniformly sampled from KI . We
generate a random variable r from Gamma distribution Γ(3, −1).

1304(a)

(b)

(c)

Figure 5: (a) Convex hull of ∆X. (b) Finding the sensitivity hull K. (c) Transform K to isotropic position KI . Sample a point z(cid:48).

Let z(cid:48) = rz(cid:48). Then we transform the point z(cid:48) to the original
space by z(cid:48) = T−1z(cid:48). The released location is z = x∗ + z(cid:48).
Algorithm 2 summarizes the process of PIM. Lines 5∼6
can be iterated until T is stable, whereas the computation-
al complexity is not aﬀected by the iterations because the
number of samples is bounded by a constant (by Corollary
5.1).

Algorithm 2 Planar Isotropic Mechanism
Require: , ∆X, x∗
1: K(cid:48) ← Conv(∆X);
2: ∆V ←−
∪
3: K ← Conv(∆V);
4: Repeat lines 5,6 with larger l if T is not stable:
5:

v1,v2∈ vertices of K(cid:48)(v1 − v2);
(cid:80)l

Sample y1, y2,··· , yl uniformly from K;

T ←(cid:16) 1

(cid:17)− 1

2 ;

(cid:46) convex hull of ∆X

(cid:46) sensitivity hull

l

i

i=1 yiyT

6:
7: KI = TK;
8: Uniformly sample z(cid:48) from KI ;
9: Sample r ∼ Γ(3, −1);
10: return z = x∗ + rT−1z(cid:48);

(cid:46) isotropic transformation

(cid:46) release z

Privacy and Performance Analysis. We now present
the privacy property, complexity, and the error of PIM.

Theorem 5.3. Algorithm 2 is -diﬀerentially private on

δ-location set ∆X.

Theorem 5.4. Algorithm 2 takes O(nlog(h) + h2log(h))
time where n is the size of ∆X and h is number of vertices
on Conv(∆X).

(cid:16) 1



(cid:112)Area(K)

(cid:17)

Theorem 5.5. Algorithm 2 has error O

at most, which means it achieves the lower bound in Theorem
4.3.

5.3 Location Inference

The inference of line 8 in Algorithm 1 is a general state-
ment because inference methods depend on speciﬁc release
algorithms. To implement the inference for PIM, we need to
transform the location si and the released location zt to the
isotropic space of KI . Then in Equation (1), the probability
P r(zt|u∗
t = si) can be computed as follows. This completes

the whole algorithm.

P r(zt|u
∗
t = si) =
(cid:48)
(cid:48)
t = Tz; s
z
i = Tsi

2Area(KI )

2

exp(−||z
t − s
i||KI )
(cid:48)
(cid:48)

6. EXPERIMENTAL EVALUATION

In this section we present experimental evaluation of our
method. All algorithms were implemented in Matlab on a
PC with 2.9 GHz Intel i7 CPU and 8 GB Memory.

Datasets. We used two real-world datasets.

I Geolife data. Geolife data [40] was collected from 182
users in a period of over three years.
It recorded a
wide range of users’ outdoor movements, represented
by a series of tuples containing latitude, longitude and
timestamp. The trajectories were updated in a high
frequency, e.g. every 1 ∼ 60 seconds. We extracted all
the trajectories within the 3rd ring of Beijing to train
the Markov model, with the map partitioned into cells
of 0.34 × 0.34 km2.

II Gowalla data. Gowalla data [7] contains 6, 442, 890
check-in locations of 196, 586 users over the period of
Feb. 2009 to Oct. 2010. We extracted all the check-
ins in Los Angeles to train the Markov model, with the
map partitioned into cells of 0.89 × 0.89 km2. Because
check-ins were logged in a relatively low frequency, e.g.
every 1 ∼ 50 minutes, we can examine the diﬀerence of
the results from Gowalla and Geolife.

Metrics. We used the following metrics in our experiment,
including two internal metrics: size of ∆X, drift ratio, and
two sets of utility metrics: distance, precision and recall. We
skip the runtime report because most locations were released
within 0.3 second by PIM.

I Since our privacy deﬁnition is based on δ-location set
∆X, we evaluated the size of ∆X to understand how
∆X grows or changes.

II The deﬁnition of ∆X and the potential limit of Markov
model may cause the true location to fall outside ∆X
(drift). Thus we measured the drift ratio computed as
the number of timestamps the true location is excluded
in ∆X over total number of timestamps.

III We measured the distance between the released location
and the true location, which can be considered as a

?K0v1v2v3v4v5oK(cid:5)(cid:5)(cid:5)(cid:5)∆∆∆∆oz0KI1305(a) True trace

(b) Size of ∆X

(a) Size vs. 

(b) Size vs. δ

(c) LM released trace

(d) Drift ratio

(c) Drift Ratio vs. 

(d) Drift Ratio vs. δ

(e) PIM released trace

(f) Distance

Figure 6: Performance over time: (a) The true (original) trace;
(c)(e) Released traces; (b) Size of ∆X over time; (d) Drift ratio
over time; (f) Distance over time.

(e) Distance vs. 

(f) Distance vs. δ

Figure 7: Impact of parameters on GeoLife data with popular
M:
(a)(b) Impact of  and δ on size of ∆X; (c)(d) Impact of 
and δ on drift ratio; (e)(f) Impact of  and δ on distance.

general utility metric independent of speciﬁc location
based applications.

IV We also run k nearest neighbor (kNN) queries using
the released locations and report its precision and re-
call compared to the true kNN set using the original
location. Suppose the true kNN set is R, the returned
k(cid:48)NN set (we set k(cid:48) ≥ k) is R(cid:48), precision is deﬁned as
|R ∩ R(cid:48)|/k(cid:48), and recall is deﬁned as |R ∩ R(cid:48)|/k.

6.1 Performance Over Time

In order to show the performance of a release mechanis-
m as a user moves over time, including how ∆X changes,
how often drift happens and how accurate is the perturbed
location, we ﬁrst run a set of experiments for a single test
trajectory with popular M learned from all users. We select-
ed a random test trajectory from Geolife dataset consisting
of 500 timestamps. We tested both PIM and LM at each
timestamp with  = 1 and δ = 0.01. Each method was run
20 times and the average is reported. Figure 6a shows the
original trajectory in map and state (grid) coordinates; Fig-
ures 6c and 6e show the released (perturbed) locations at
each timestamp. We can see that the released locations of
PIM is closer to the true location, compared with LM.

Size of ∆X. From Figure 6b we see that the size of ∆X
does not increase dramatically, instead it maintains at stable
level after a few timestamps. The reason is that by selecting
the δ-location set the inference mechanism only boost prob-
abilities of locations in ∆X. Then the probabilities of other
locations decay gradually. Thus a stable δ-location set can
be maintained.

Drift Ratio. In Figure 6d, the peak of drift ratio happened
in timestamp 200 ∼ 300. This can be explained by the fact
that the true trajectory has a turning corner as in Figure
6a, and the transition probability of making this right turn
is relatively small in the Markov model.

When a drift happens, we use surrogate for release mech-
anisms. Because the surrogate is the nearest cell to the true
location in ∆X and the release mechanism is based on the
surrogate, the posterior probability of the surrogate will be
boosted. Consequently, in the next timestamp the probabil-
ity that ∆X includes the previous true location rises. This
“lagged catch-up” can be veriﬁed by Figures 6f, 6c and 6e.

Distance. The distance is reported in Figure 6f. We can
see that PIM provided more accurate locations than LM for
two reasons. First, because PIM is optimal, the posterior
probability distribution is more accurate than LM. Second,
with such distribution a better (Bayesian) inference can be

10152025303540455044454647484950100200300400500  map coordinatestate coordinate1002003004005002468101214TimestampSize of ∆X  LMPIM10203040504045505510020030040050000.20.40.60.81TimestampDrift ratio  LMPIM1020304050444546474849505110020030040050002468TimestampDistance (km)  LMPIM0.20.40.60.815678910εSize of ∆X  LMPIM−3−2.5−2−1.5−1102030log10(δ)Size of ∆X  LMPIM0.20.40.60.810.380.40.420.440.460.480.5εDrift Ratio  LMPIM−3−2.5−2−1.5−10.20.40.60.8log10(δ)Drift Ratio  LMPIM0.20.40.60.815101520εDistance (km)  LMPIM−3−2.5−2−1.5−124681012log10(δ)Distance (km)  LMPIM1306(a) Size vs. 

(b) Size vs. δ

(a) Size vs. 

(b) Size vs. δ

(c) Drift Ratio vs. 

(d) Drift Ratio vs. δ

(c) Drift Ratio vs. 

(d) Drift Ratio vs. 

(e) Distance vs. 

(f) Distance vs. δ

(e) Distance vs. 

(f) Distance vs. δ

Figure 8: Impact of parameters on GeoLife data with personal
M:
(a)(b) Impact of  and δ on size of ∆X; (c)(d) Impact of 
and δ on drift ratio; (e)(f) Impact of  and δ on distance.

Figure 9: Impact of parameters on Gowalla data with popular
M:
(a)(b) Impact of  and δ on size of ∆X; (c)(d) Impact of 
and δ on drift ratio; (e)(f) Impact of  and δ on distance.

obtained, making ∆X more accurate for the coming times-
tamp.
6.2 Impact of Parameters

Since the performance may vary for diﬀerent trajectories,
we chose 100 trajectories from 100 users, each of which has
500 timestamps, to evaluate the overall performance and
the impact of parameters. The default values are  = 1 and
δ = 0.01 if not mentioned. The average performances for
both datasets are reported in Figures 7 (on GeoLife data
with popular M), Figure 8 (on GeoLife data with personal
M) and Figure 9 (on Gowalla data with popular M).

Size of ∆X vs. . In Figures 7a and 8a (Geolife data),
size of ∆X shrinks with larger  because the inference result
is enhanced by big . On the other hand, impact of  would
be negligible in Gowalla data because one-step transition in
Markov model has limited predictability (check-ins are not
frequent), as in Figure 9a.

Size of ∆X vs. δ. Size of ∆X is mainly determined by
δ as shown in Figures 7b, 8b and 9b. Note that LM and
PIM have similar size of ∆X, meaning the true location is
hidden in the similar size of candidates. When δ grows,
size of ∆X reduces dramatically because more improbable
locations are truncated. However, δ cannot be too large
because it preserves nearly no privacy if size of ∆X is close

to 1. Thus we use δ = 0.01 by default, which guarantees the
sizes of ∆X are larger than 4 in the three settings.

Drift Ratio vs. . Figures 7c and 9c show that drift ratio
declines with larger , which is easy to understand because
larger  provides more accurate release. However, the impact
of  is not obvious in Figure 8c. The reason is that the size
of ∆X is already small as in Figure 8b, hence the increase
of  does not help much in improving the accuracy of the
inference.

Drift Ratio vs. δ. Figures 7d, 8d and 9d show that drift
ratio rises when δ increases due to reduced ∆X, and PIM is
slightly better than LM. However, due to the phenomenon
of “lagged catch-up”, we will see next that the accuracy of
the released locations was still improved with increasing δ.

Distance vs. . Figures 7e, 8e and 9e show the distance
with varying . We can see that PIM performed better than
LM. In Gowalla data, because check-in locations are far away
from each other, the distance is larger than Geolife.

Distance vs. δ. Because bigger δ will result in less can-
didates in δ-location set, the distance declines when δ in-
creases. Figures 7f, 8f and 7f show that PIM achieves better
accuracy than LM. However, from 10−1.5 to 10−1, the im-
provement on distance is very small while privacy guarantee
drops signiﬁcantly as in Figures 7b, 8b and 9b. Especially in

0.20.40.60.8145678εSize of ∆X  LMPIM−3−2.5−2−1.5−151015log10(δ)Size of ∆X  LMPIM0.20.40.60.810.220.240.260.280.30.32εDrift Ratio  LMPIM−3−2.5−2−1.5−10.10.20.30.40.50.6log10(δ)Drift Ratio  LMPIM0.20.40.60.8151015εDistance (km)  LMPIM−3−2.5−2−1.5−1123456log10(δ)Distance (km)  LMPIM0.20.40.60.817.888.28.48.68.8εSize of ∆X  LMPIM−3−2.5−2−1.5−1510152025log10(δ)Size of ∆X  LMPIM0.20.40.60.810.380.40.420.440.460.48εDrift Ratio  LMPIM−3−2.5−2−1.5−10.30.40.50.6log10(δ)Drift Ratio  LMPIM0.20.40.60.811020304050εDistance (km)  LMPIM−3−2.5−2−1.5−16810121416log10(δ)Distance (km)  LMPIM1307Figure 8b, size of ∆X is 1 when δ = 0.1. Therefore, choosing
a high value of δ (like δ > 0.03) does not provide the best
trade-oﬀ of privacy and utility.

Impact of Markov model. Comparing Figures 7 on pop-
ular M and Figure 8 on personal M, we can see the impact
of diﬀerent Markov model. With more accurate (personal)
model, better utility can be achieved, including smaller size
of ∆X, lower drift ratio and less distance. However, the
same privacy level (-diﬀerential privacy) is maintained (on
diﬀerent ∆X) regardless of M.
6.3 Utility for Location Based Queries

To demonstrate the utility of released locations, we also
measured the precision and recall of kNN queries at each
of the 500 timestamps in the 100 trajectories with popular
M. The average results of kNN from original locations and
k(cid:48)NN from released locations are reported in Figure 10 with
 = 1 and δ = 0.01.
In Figures 10a and 10b, we show the precision and recall
with k = k(cid:48). Note that in this case precision is equal to
recall. We can see that when k grows precision and recall
also increase because the nearest neighbors have to be found
in larger areas. PIM is consistently better than LM.
Next we ﬁxed k = 5 and varied k(cid:48). Figures 10c and 10d
show the precision drops when k(cid:48) rises because of a larger
returned set. On the other hand, Figures 10e and 10f indi-
cate recall increases with large k(cid:48). Overall, PIM has better
precision and recall than LM.

7. RELATED WORKS
7.1 Location Privacy

There is a rich set of literature related to location priva-
cy. A few recent books and surveys [23, 15] provide an up-
to-date review of Location Privacy Preserving Mechanisms
(LPPMs).

LPPMs generally use obfuscation methods, such as spatial
cloaking, cell merging, location precision reduction or dum-
my cells, to achieve anonymity based privacy or uncertainty
based privacy. However, anonymity or ad hoc uncertain-
ty based techniques do not always provide suﬃcient privacy
protection [22, 37]. Most of them do not consider the tempo-
ral correlations between locations and are subject to various
inference attacks. The recent work [1] proposed a notion
of geo-indistinguishability which extends diﬀerential priva-
cy. However, a fundamental diﬀerence is that neighboring
pairs or secrets are deﬁned based on a radius and it does not
consider the temporal correlations of multiple locations.

Several works use Markov models for modeling users’ mo-
bility and inferring user locations or trajectories [25, 34].
[17] proposed an insightful technique with a provable privacy
guarantee to ﬁlter a user context stream even if the adver-
saries are powerful and have knowledge about the temporal
correlations but it used suppression instead of perturbation.
[37] investigated the question of how to formally quantify
the privacy of existing LPPMs and assumed that an adver-
sary can model users’ mobility using a Markov chain learned
from a population.
7.2 Differential Privacy

Several variants or generalizations of diﬀerential privacy
have been studied, as discussed in Section 3.4. However,

(a) Precision/Recall (Geolife)

(b) Precision/Recall (Gowalla)

(c) Precision (Geolife)

(d) Precision (Gowalla)

(e) Recall (Geolife)

(f) Recall (Gowalla)

Figure 10: kNN results:
k(cid:48); (c)(d) precision vs. k(cid:48); (e)(f) recall vs. k(cid:48).

(a)(b) precision and recall under k =

applying diﬀerential privacy for location protection has not
been investigated in depth. Several recent works have ap-
plied diﬀerential privacy to publish aggregate information
from a large volume of location, trajectory or spatiotempo-
ral data (e.g. [6, 33, 12, 24]). Our contribution is to extend
diﬀerential privacy in a new setting of continual location
sharing of only one user whose locations are temporally cor-
related.

Optimal query answering under diﬀerential privacy has
been studied recently. Hardt and Talwar [18] studied the
theoretical lower bound for any diﬀerentially private mecha-
nisms and proposed K-norm mechanism. Bhaskara et al [4]
studied another K-norm based method to project the sen-
sitivity hull onto orthogonal subspaces. Nikolov et al [30]
also improved the eﬃciency of K-norm mechanism by ﬁnd-
ing the minimal enclosing ellipsoid to release multivariate
Gaussian noises. So far the best utility of existing mecha-
nisms can be log(d) approximately optimal. We extended
the K-norm mechanism to location data by examining the
two-dimensional sensitivity hull and designing its isotropic
transformation so that optimal utility can be achieved.

8. CONCLUSION AND FUTURE WORK

In this paper we proposed δ-location set based diﬀerential
privacy to protect a user’s true location at every timestamp
under temporal correlations. We generalized the notion of
“neighboring databases” to δ-location set for the new setting

5101520250.60.650.70.750.8kPrecision/Recall  LMPIM5101520250.30.350.40.450.50.55kPrecision/Recall  LMPIM5101520250.20.30.40.50.6k(cid:146)Precision  LMPIM5101520250.150.20.250.3k(cid:146)Precision  LMPIM5101520250.60.70.80.9k(cid:146)Recall  LMPIM5101520250.30.40.50.6k(cid:146)Recall  LMPIM1308and extended the well known (cid:96)1-norm sensitivity to sensitivi-
ty hull in order to capture the geometric meaning of sensitiv-
ity. Then with sensitivity hull we derived the lower bound of
δ-location set based diﬀerential privacy. To achieve the lower
bound, we designed the optimal planar isotropic mechanism
to release diﬀerentially private locations with signiﬁcantly
high eﬃciency and utility.

The framework of δ-location set based diﬀerential privacy
can work with any mobility models (besides Markov chain).
As a future work direction, we are interested in instantiating
it with diﬀerent and more advanced mobility models and
studying the impact.

9. ACKNOWLEDGEMENT

This research is supported by NSF under grant No. 1117763

and the AFOSR DDDAS program under grant FA9550-12-1-
0240. We thank the anonymous reviewers for their valuable
comments that helped improve the ﬁnal version of this pa-
per.

10. REFERENCES
[1] M. E. Andr´es, N. E. Bordenabe, K. Chatzikokolakis, and

C. Palamidessi. Geo-indistinguishability: Diﬀerential
privacy for location-based systems. CCS ’13, pages
901–914. ACM, 2013.

[2] G. Apostolos. Notes on isotropic convex bodies. 2003.
[3] A. R. Beresford and F. Stajano. Location privacy in

pervasive computing. Pervasive Computing, IEEE,
2(1):46–55, 2003.

[4] A. Bhaskara, D. Dadush, R. Krishnaswamy, and K. Talwar.

Unconditional diﬀerentially private mechanisms for linear
queries. STOC ’12, New York, NY, USA, 2012.

[5] K. Chatzikokolakis, M. Andr´es, N. Bordenabe, and

C. Palamidessi. Broadening the scope of diﬀerential privacy
using metrics. Lecture Notes in Computer Science, pages
82–102. Springer Berlin Heidelberg, 2013.

[6] R. Chen, B. C. Fung, B. C. Desai, and N. M. Sossou.

Diﬀerentially private transit data publication: a case study
on the montreal transportation system. KDD ’12, pages
213–221, New York, NY, USA, 2012. ACM.

[7] E. Cho, S. A. Myers, and J. Leskovec. Friendship and

mobility: User movement in location-based social networks.
KDD ’11, pages 1082–1090, New York, NY, USA, 2011.

[8] A. Dey, J. Hightower, E. de Lara, and N. Davies.

Location-based services. Pervasive Computing, IEEE,
9(1):11–12, 2010.

[9] C. Dwork. Diﬀerential privacy. In Automata, languages and

programming, pages 1–12. Springer, 2006.

[10] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data analysis.
Proceedings of the 3rd Theory of Cryptography
Conference, 2006.

[11] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum.

Diﬀerential privacy under continual observation. STOC ’10,
pages 715–724, New York, NY, USA, 2010. ACM.

[12] L. Fan, L. Xiong, and V. S. Sunderam. Diﬀerentially

private multi-dimensional time series release for traﬃc
monitoring. In DBSec, pages 33–48, 2013.

[13] C. Fang and E.-C. Chang. Diﬀerential privacy with

δ-neighbourhood for spatial and dynamic datasets. ASIA
CCS ’14, pages 159–170, New York, NY, USA, 2014. ACM.

[14] B. Gedik and L. Liu. Protecting location privacy with

personalized k-anonymity: Architecture and algorithms.
Mobile Computing, IEEE Transactions on, 7(1):1–18, 2008.
[15] G. Ghinita. Privacy for Location-Based Services. Synthesis

Lectures on Information Security, Privacy, and Tru.
Morgan & Claypool, 2013.

[16] M. C. Gonzalez, C. A. Hidalgo, and A.-L. Barab´asi.

Understanding individual human mobility patterns. Nature,
453(7196):779–782, June 2008.

[17] M. G¨otz, S. Nath, and J. Gehrke. Maskit: Privately

releasing user context streams for personalized mobile
applications. SIGMOD ’12, New York, NY, USA, 2012.

[18] M. Hardt and K. Talwar. On the geometry of diﬀerential

privacy. In STOC, pages 705–714. ACM, 2010.

[19] X. He, A. Machanavajjhala, and B. Ding. Blowﬁsh privacy:

Tuning privacy-utility trade-oﬀs using policies. SIGMOD
’14, pages 1447–1458, New York, NY, USA, 2014. ACM.
[20] I. A. Junglas and R. T. Watson. Location-based services.

Communications of the ACM, 51(3):65–69, 2008.

[21] D. Kifer and A. Machanavajjhala. No free lunch in data

privacy. In SIGMOD, pages 193–204. ACM, 2011.

[22] J. Krumm. Inference attacks on location tracks.

PERVASIVE’07, pages 127–143, Berlin, Heidelberg, 2007.
Springer-Verlag.

[23] J. Krumm. A survey of computational location privacy.

Personal and Ubiquitous Computing, 13(6):391–399, 2009.

[24] H. Li, L. Xiong, and X. Jiang. Diﬀerentially private

synthesization of multi-dimensional data using copula
functions. EDBT’14, pages 475–486, 2014.

[25] L. Liao, D. J. Patterson, D. Fox, and H. Kautz. Learning

and inferring transportation routines. Artif. Intell.,
171(5-6):311–331, Apr. 2007.

[26] L. Lov´asz and S. Vempala. Hit-and-run from a corner.
STOC ’04, pages 310–314, New York, NY, USA, 2004.
ACM.

[27] L. Lov´asz and S. Vempala. Simulated annealing in convex
bodies and an o*(n4) volume algorithm. J. Comput. Syst.
Sci., 72(2):392–417, Mar. 2006.

[28] McSherry. Privacy integrated queries: an extensible

platform for privacy-preserving data analysis. In SIGMOD
’09, pages 19–30, New York, NY, USA, 2009. ACM.

[29] V. Milman and A. Pajor. Isotropic position and inertia

ellipsoids and zonoids of the unit ball of a normed
n-dimensional space. Lecture Notes in Mathematics, pages
64–104. Springer Berlin Heidelberg, 1989.

[30] A. Nikolov, K. Talwar, and L. Zhang. The geometry of
diﬀerential privacy: The sparse and approximate cases.
ACM STOC ’13, pages 351–360, NY, USA, 2013.

[31] J. O’Rourke. Computational Geometry in C. Cambridge

University Press, New York, NY, USA, 2nd edition, 1998.

[32] S. Papadopoulos, S. Bakiras, and D. Papadias. Nearest

neighbor search with strong location privacy. Proceedings of
the VLDB Endowment, 3(1-2):619–629, 2010.

[33] W. H. Qardaji, W. Yang, and N. Li. Diﬀerentially private

grids for geospatial data. In ICDE, pages 757–768, 2013.

[34] S. Qiao, C. Tang, H. Jin, T. Long, S. Dai, Y. Ku, and

M. Chau. Putmode: prediction of uncertain trajectories in
moving objects databases. Applied Intelligence,
33(3):370–386, Dec. 2010.

[35] V. Rastogi, M. Hay, G. Miklau, and D. Suciu. Relationship
privacy: output perturbation for queries with joins. PODS
’09, pages 107–116, New York, NY, USA, 2009. ACM.

[36] M. Rudelson. Random vectors in the isotropic position. J.

Funct. Anal, pages 60–72, 1999.

[37] R. Shokri, G. Theodorakopoulos, J.-Y. Le Boudec, and

J.-P. Hubaux. Quantifying location privacy. IEEE SP ’11,
pages 247–262, Washington, DC, USA, 2011.

[38] C. Song, Z. Qu, N. Blumm, and A.-L. Barab´asi. Limits of

predictability in human mobility. Science,
327(5968):1018–1021, 2010.

[39] S. Vempala. Geometric random walks: a survey.

Combinatorial and Computational Geometry, pages
573–612, 2005.

[40] Y. Zheng, X. Xie, and W.-Y. Ma. Geolife: A collaborative

social networking service among user, location and
trajectory. IEEE Data Eng. Bull., 33(2):32–39, 2010.

1309