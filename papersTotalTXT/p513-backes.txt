(Nothing else) MATor(s):

Monitoring the Anonymity of Tor’s Path Selection

Michael Backes

Saarland University, CISPA

Saarbrücken, Germany

backes@cs.uni-saarland.de

Sebastian Meiser

Saarland University, CISPA

Saarbrücken, Germany

meiser@cs.uni-saarland.de

Aniket Kate

Saarland University, MMCI

Saarbrücken, Germany

aniket@mmci.uni-saarland.de
Esfandiar Mohammadi
Saarland University, CISPA

Saarbrücken, Germany
mohammadi@cs.uni-

saarland.de

ABSTRACT
In this paper we present MATOR: a framework for rigorously as-
sessing the degree of anonymity in the Tor network. The framework
explicitly addresses how user anonymity is impacted by real-life
characteristics of actually deployed Tor, such as its path selection
algorithm, Tor consensus data, and the preferences and the connec-
tions of the user. The anonymity assessment is based on rigorous
anonymity bounds that are derived in an extension of the ANOA
framework (IEEE CSF 2013). We show how to apply MATOR
on Tor’s publicly available consensus and server descriptor data,
thereby realizing the ﬁrst real-time anonymity monitor. Based on
experimental evaluations of this anonymity monitor on Tor Metrics
data, we propose an alternative path selection algorithm that pro-
vides stronger anonymity guarantees without decreasing the overall
performance of the Tor network.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—secu-
rity and protection

Keywords
Tor; Tor path selection; provable privacy; anonymity monitor

1.

INTRODUCTION

The onion routing network Tor is a widely employed low-latency
anonymous communication service [34]. To provide anonymity
Tor routes a user’s trafﬁc through anonymizing proxies.
In Tor
the trust in these anonymizing proxies (also called nodes) is dis-
tributed over three nodes, which are chosen from more than 5000
volunteer nodes. Using these anonymizing proxies, Tor creates an
anonymous channel for the user, which leads to the following cen-
tral question from a user perspective:
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660371.

How anonymous is this channel that Tor creates, i.e.,
how likely is it that an adversary can deanonymize me?

Deriving the degree of a user’s anonymity is challenging for
such a complex system where each of the 5000 ﬂuctuating nodes
is entrusted with different bandwidth, and each node offers a dif-
ferent set of ports for a communication. Previous mathematically
founded analyses abstract the Tor network by ignoring character-
istics of Tor, such as the path selection algorithm, the varying en-
trusted bandwidth of different Tor nodes, or the user’s requested
ports [6, 15–18, 21]. However, these real-life characteristics of Tor
signiﬁcantly inﬂuence a user’s anonymity, which renders the previ-
ously proven bounds inaccurate.
Contribution.
In this paper, we present MATOR: the ﬁrst sys-
tem to derive realistic sender, recipient and relationship anonymity
guarantees based on Tor’s real-life characteristics, such as its ac-
tual path selection strategy. Our anonymity deﬁnitions are founded
in the ANOA framework [6] and are thus modular. MATOR entails
light-weight real-time monitors that compute sender, recipient and
relationship anonymity guarantees based on the actual Tor consen-
sus data and the user requested ports.

As ANOA, the theoretical framework upon which we base MA-
TOR fails to model adversaries that resemble the worst case for a
realistic scenario, we extend ANOA with the concepts of adver-
sary classes and adaptive user behavior. These adversary classes
allow for restricting the strong ANOA adversary to the scenario
of interest, whereas extending ANOA with adaptive user behav-
ior enables interactive communication scenarios. We show that se-
quential composition does not hold for some adversary classes and
characterize those adversary classes for which we show sequential
composition. We consider this extension of the ANOA framework
to be of independent interest for the analysis of other anonymous
communication networks.

Using MATOR, we conduct experiments on Tor Metrics [33]
data that span the last 24 months. These experiments illustrate that
the anonymity guarantees ﬂuctuate substantially on an hourly basis,
which further underscores the value of MATOR’s novel method-
ology to monitor anonymity in real-time. The experiments addi-
tionally highlight the inﬂuence of the entrusted bandwidth on a
user’s anonymity, as Tor’s path selection algorithm places a large
amount of trust into single high-bandwidth nodes. Based on our
ﬁndings from the experiments, we propose DISTRIBUTOR: a novel
path selection algorithm that improves Tor for all three considered
anonymity notions (sender, recipient and relationship anonymity)

513by redistributing the trust in single high-bandwidth nodes to a larger
set of Tor nodes. In contrast to previous proposals for path selec-
tion algorithms that only improve the users’ anonymity [4, 13], our
path selection algorithm also completely preserves the overall per-
formance of the Tor network.
Related work. In the literature, there are two lines of work about
estimating the degree of anonymity of Tor users.

The ﬁrst line of work assumes a worst case adversary and proves
rigorous anonymity bounds for Tor users [6, 15–18, 21]. These
pieces of work, however, abstract the Tor network by ignoring char-
acteristics of Tor that signiﬁcantly inﬂuence a user’s anonymity,
such as the path selection algorithm, the varying entrusted band-
width of different Tor nodes, or the user’s requested ports. Never-
theless, these previous approaches for proving anonymity degrees
for Tor offer a valuable foundation for our work: in particular, the
rigorous ANOA framework. ANOA, however, has several severe
limitations: it only considers a worst-case adversary that has full
control over all actions of all users, which is too strong for many
realistic and interesting scenarios. However, there are important
scenarios in which the adversary should not have full control over
all actions of all users and that cannot be modeled in ANOA. There-
fore, we extend ANOA such that it also allows much more ﬁne-
grained scenarios.

The second line of work models Tor’s anonymity relevant char-
acteristics more accurately [25,35], e.g., they explicitly model Tor’s
path selection. Johnson et al. [25] give estimates for the anonymity
that Tor provides for typical users based on simulation experiments
that only assume some experimentally estimated adversarial strat-
egy for compromising relays, which, however, is not the worst case
adversary. Moreover, they characterize the adversary by its band-
width and split this bandwidth up between entry and exit nodes
(i.e., the ﬁrst and third proxy), which neglects the following case:
a relay that offers its bandwidth as entry node will with signiﬁcant
probability be chosen as a middle node (i.e., the second proxy) or as
an exit node (i.e., the third proxy) and can still learn some informa-
tion. These cases are ignored by their analysis, which signiﬁcantly
reduces the estimated de-anonymization probability of adversarial
relays. The work by Wacek et al. [35] goes into a similar direc-
tion. They construct a “scale” model of the Tor network, on which
they perform extensive simulations, using the real tor client code,
to evaluate the impact of path selection algorithms on Tor’s per-
formance and anonymity. However, as both analyses are based on
simulations, they, in contrast to our work, do not yield provable
bounds on the degree of a user’s anonymity. Moreover, unlike
MATOR, their analyses cannot be performed in real time on typi-
cal user devices.

With respect to the improvement on Tor’s path selection algo-
rithm, there is an extensive line of work. Wang et al. [36] focus
on improving Tor’s latency by using a congestion-aware path se-
lection. This proposed path selection would improve the overall
performance of Tor, but the authors show that their path selection
does not improve a user’s anonymity and even reduces it slightly.
In contrast, there are several proposals to improve the anonymity of
single user’s in Tor signiﬁcantly [4,13,26], but these path selection
algorithms solely aim to improve the anonymity of Tor and ignore
the inﬂuence on the overall performance of the Tor network. If one
of these latter path selection algorithms was to be widely deployed
and used by the majority of Tor users, the overall performance of
the Tor network would signiﬁcantly decrease, as the path selection
would no longer allow the trafﬁc to be distributed according to the
bandwidth of the nodes.
In this work, we propose a path selec-
tion algorithm that both signiﬁcantly improves the anonymity of
users and at the same time is completely performance-preserving;

i.e., even if every Tor user applied our technique, the overall per-
formance of Tor would not change at all.

2. OVERVIEW

The Tor network is an overlay network consisting of OR nodes
that relay user trafﬁc and of a distributed directory service that
maintains and provides the users with cryptographic information
and with routing information about the OR nodes in form of a con-
sensus. On a regular basis (typically every 10 minutes) users select
a sequence of (three) OR nodes based on the directory information,
such as bandwidth and accepted ports, and create a path, called a
circuit over the selected set. The users then employ these circuits
to send trafﬁc to their destinations and to receive responses from
those. The selection of nodes is done by a random choice. How-
ever, to improve the performance of the Tor network, this choice
is not uniformly at random, but a weighted random choice over all
possible nodes, where the weight depends on the node’s bandwidth.
Our goal is to perform a rigorous real-time analysis of a user’s
anonymity bounds in the presence of real-life parameters such as
the current path selection algorithm in a mathematically founded
framework.

Due to the lack of a suitable theoretical framework that we could
use off-the-shelf, we chose to extend the ANOA framework [6],
a general framework for anonymity guarantees, such that we can
prove our analysis secure.

ANOA is a mathematical framework that is conceptually based
on differential privacy in three ways: ﬁrst, it considers the user be-
havior as a database of user actions, hence statically determines the
users’ behaviors; second, it leaves the choice over the user’s be-
havior to the adversary; and third, it allows for deriving guarantees
for all sufﬁciently similar user behaviors. By statically determin-
ing the behavior of all users (in a database) before this behavior is
executed, ANOA excludes interactive communication scenarios.

In Section 3 we extend the ANOA framework to such interac-
tive communication scenarios by allowing the adversary to specify
user actions as atomic steps instead of providing databases. In ad-
dition, we restrict the adversary’s power in ANOA by introducing
wrapper machines that we coin adversary classes for including set-
tings with uncompromised servers. With these adversary classes
we additionally are able to describe usage patterns and to restrict
the ports that an adversary may choose for a user’s connection and
the inﬂuence the adversary may have on user preferences. With ad-
versary classes a variety of user proﬁles and realistic restrictions
for the adversary can be deﬁned and they can modularly be com-
bined with anonymity notions, such as sender anonymity, recipient
anonymity and relationship anonymity.

Before we discuss how a user’s anonymity in Tor is assessed in
MATOR, we describe in Section 4 Tor and its path selection to il-
lustrate that there is a variety of parameters that need to be carefully
considered. Then, we ﬁnally deﬁne, on top of ANOA, MATOR as
a framework for assessing a user’s anonymity in the Tor network.
We describe how to derive bounds on the degree of anonymity de-
pending on the path selection algorithm and actual Tor consensus
data. From this data we derive the impact of individual relays on
anonymity – which is the likelihood that they are used as entry,
middle or exit node in a user’s circuit – depending on the user’s
preferences, the ports she wants to connect to and the structure of
the Tor network, such as the active Tor nodes with their tags, en-
trusted bandwidths and family relations [33]. We then base the
computation of bounds for sender anonymity, recipient anonymity
and relationship anonymity on these calculations and show these
bounds to be founded in ANOA. To allow users to compute their

514Figure 1: Output of MATOR on historical data from January 2012 until the end of July 2014: HTTPS + IRC vs. HTTPS, The graph
shows a bound on the success probability of a worst case adversary in deanonymizing a user, where the adversary may compromise
up to 0.5% of the nodes.

anonymity we present light-weight live-monitors that implement
MATOR for sender, recipient and relationship anonymity.

In Section 6 we conduct experiments by applying MATOR to
Tor network data taken from the Tor Metrics archive [33]. These
experiments give several interesting insights into Tor’s anonymity.
The guarantees heavily depend on the user’s preferences, Tor’s path
selection algorithm and the status of all active Tor nodes. Con-
sequently, the guarantees vary over time, ﬂuctuating signiﬁcantly
with every new consensus ﬁle, as new nodes become active, are
entrusted with more or less bandwidth and receive different tags.
Moreover, our experiments show that the current path selection al-
gorithm does not properly distribute the trust over Tor nodes and
extremely prioritizes a few high-bandwidth nodes. This leads to
single points of failure and thus decreases the anonymity guaran-
tees we can derive. Based on this insight, we propose an alternative
path selection algorithm, coined DISTRIBUTOR, that maximally
distributes the trust without decreasing the overall performance of
the Tor network, thereby achieving signiﬁcantly better anonymity
guarantees while preserving Tor’s throughput and latency.

Figure 1 depicts the result of applying our monitor to Tor’s
archive data, from Tor Metrics [33], of the last two years, where
the anonymity guarantees are averaged for each day for the sake
of readability. In the experiment the monitor assumes that 0.5% of
all nodes are compromised, that the user requested the ports 443
(HTTPS) and 194 (IRC) and wants to compare itself against the set
of users that solely request port 443, i.e., the probability that it is in
the anonymity set including the set of users that solely surf1. The
ﬁgure compares Tor’s path selection algorithm against DISTRIBU-
TOR for sender anonymity and recipient anonymity and shows that
DISTRIBUTOR leads to signiﬁcant improvements. The ﬁgure ad-
ditionally illustrates that a user’s degree of anonymity highly ﬂuc-
tuates. This ﬂuctuation underscores the value of providing a real-
time, user-centric, anonymity monitor.

3. THE EXTENDED AnoA FRAMEWORK
The theoretical framework upon which we base MATOR is an
extension of the ANOA framework for proving anonymity guaran-

1We choose port 443 as the Tor browser uses by default the add-on
HTTPS Everywhere, which uses HTTPS whenever possible.

tees of anonymous communication networks [6]. ANOA is a useful
building block for analyzing anonymity, but it lacks the generality
to model the scenarios in MATOR. As a consequence, we extend
the ANOA framework in this section such that it is suitable for prov-
ing that the anonymity bounds of our MATOR monitors are secure.
In this section, we ﬁrst present the extended ANOA challenger for
stronger, adaptive adversaries and deﬁne adversary classes as re-
alistic restrictions for this strong adversary. We then analyze how
the anonymity guarantees provided by our extended ANOA frame-
work compose under sequential composition. Finally, we present
instantiations of the three anonymity notions we need for MATOR:
sender anonymity, recipient anonymity and relationship anonymity.
3.1 The extended AnoA challenger

ANOA generalizes the notion of adjacency from differential pri-
vacy [12] to anonymity deﬁnitions. The challenger of ANOA de-
ﬁnes at its heart an indistinguishability game in which a challenge
bit b is chosen, and depending on this bit b one of two settings is
executed. The goal of the adversary is to guess this bit b.

The challenger for AnoA (for n challenges) expects two kinds
of messages: input messages of the form (input, m), that are pro-
cessed by the protocol by running the protocol and letting a user S
send the message m to a recipient R, and challenge messages of the
form (challenge, r1, r2, Ψ), where r1 and r1 correspond to two dif-
ferent inputs and where Ψ is an identiﬁer for the challenge. In each
challenge message (challenge, r1, r2, Ψ), the two messages r1, r2
have to be adjacent in a generalized sense. To simplify the notation
of our adjacency functions, they get the bit b from the challenger
as an additional input. Consequently, the adjacency functions can
model the anonymity challenges on their own and simply output a
message r∗ that is sent to the challenger. We assume an anonymity
function α, with the following interface:

α(r0 = (S0,R0, m0, sid0), r1 = (S1,R1, m1, sid1), b)

For every challenge with tag Ψ, the challenger maintains a state
sΨ that can be either fresh, over, or contain some information about
an ongoing challenge. For every challenge that is not in the state
over, we apply the anonymity function with its correct state – fresh
for newly started challenges, some other state sΨ if the challenge is
already active – and simulate the protocol on the output of the ad-

 0 0.2 0.4 0.6 0.8 12012-012012-042012-072012-102013-012013-042013-072013-102014-012014-042014-07Anonymity bound δRecipient Anonymity PSTorSender Anonymity PSTorRecipient Anonymity DistribuTorSender Anonymity DistribuTor515Adaptive ANOA Challenger CH(P, α, n, b)
Upon message(input, r = (S, R, m, sid))
1: RunProtocol(r, 0)

Upon message (challenge, r0, r1, Ψ)
1: if Ψ /∈ {1, . . . , n} then abort
2: else if Ψ ∈ T then
3:
4:
5: else s := fresh and add Ψ to T
6: Compute (r∗, sΨ) ← α(s, r0, r1, b)
7: RunProtocol(r, Ψ)

Retrieve s := sΨ
if s = over then abort

Let sidreal ← {0, 1}k; Store (sid, sidreal, Ψ) in S.

RunProtocol(r = (S, R, m, sid), Ψ)
1: if ¬∃y such that (sid, y, Ψ) ∈ S then
2:
3: else sidreal := y
4: Run P on r = (S, R, m, sidreal) and forward all messages that
are sent by P to the adversary A and send all messages by the
adversary to P.

Figure 2: Extended ANOA Challenger

jacency function. The challenger only allows n challenges. This
restriction is implemented by restricting the set of possible chal-
lenge tags Ψ, which are used by the adversary to distinguish the
challenges. We store all tags of already started challenges Ψ in a
set T.

To model more complex anonymity notions, such as anonymity
for a session, the adjacency function is allowed to keep state for
each challenge, i.e., the sessions created by challenge messages are
isolated from sessions created by input messages. Thus, an adver-
sary cannot use the challenger to hijack sessions. This is done by
choosing and storing session IDs sidreal for every challenge sepa-
rately as follows. Whenever a message with a new session ID sid is
to be sent to the protocol, randomly pick a fresh session ID sidreal
that is sent instead, and store sidreal together with sid and the chal-
lenge tag Ψ (if it is a challenge) or zero (if it is not a challenge).
We store all mappings of sessions (sid, sidreal, Ψ) in a set S.

By deﬁnition, the sessions created by challenge messages are
isolated from sessions created by input messages. Thus, an adver-
sary cannot use the challenger to hijack sessions.
Multiplicative factor. ANOA introduces a multiplicative factor eε
to the indistinguishability based deﬁnition, which gives the notion
a ﬂavor of differential privacy. Although they do not illustrate the
usefulness of this factor in their analysis, we ﬁnd that such a factor
can in some cases be surprisingly helpful in describing anonymity
guarantees. The most prominent case we have found is the analysis
of an adversary that compromises no, or only a very limited amount
of nodes. Before describing the details of this aspect we present our
generalization of ANOA and then later elaborate on the impact of a
multiplicative factor in Section 6.4.

The full description of the session challenger is available in Fig-

ure 2.

DEFINITION 1

((n, ε, δ)-α-IND-CDP). A protocol P is
(n, ε, δ)-α-IND-CDP for a class of adversaries A, with ε ≥ 0
and 0 ≤ δ ≤ 1, if for all PPT machines A,

Pr [0 = (cid:104)A(A(n))||CH(P, α, n, 0)(cid:105)]

≤enε Pr [0 = (cid:104)A(A(n))||CH(P, α, n, 1)(cid:105)] + enεnδ

where the challenger CH is deﬁned in Figure 2.

Example 1: Single message sender anonymity. For modeling the
simplest version of sender anonymity, where only a single mes-
sage is sent by one of two possible senders, the adjacency func-
tion αSA depending on the challenge bit b simply chooses which
of the two senders sends the message. As the adjacency func-
tion models sender anonymity, it makes sure that no informa-
tion is leaked by the message itself or the recipients identity.
Therefore it chooses the same message and the same recipient in
both scenarios, e.g., by requiring that they are equal or by sim-
ply always choosing the message and recipient that are sent for
scenario 1. Moreover, since here we only allow a single mes-
sage per session, for each message a fresh session is created
and this session is terminated by setting the state to over, i.e.,
αSA(s, (S0,R0, m0), (S1, _, _), b) = ((Sb,R0, m0), over).
In
Section 3.3 we formally deﬁne the anonymity functions for sender
(cid:5)
anonymity, recipient anonymity and relationship anonymity.
Modiﬁcations made to AnoA.
In ANOA only static, i.e., non-
interactive, scenarios can be modeled, which excludes two-way
communication. Moreover the adversary is not restricted in its
choices to determine the actions of the challenge users, which
excludes many interesting and realistic scenarios.
In practice, it
makes a tremendous difference to which server a user wants to con-
nect, as this might require a different port or different settings. The
worst-case adversary in ANOA could always choose the ports and
settings for which the weakest guarantees can be given (c.f. Section
6 for the inﬂuence of ports and settings on anonymity). However,
such a adversary is meaningless in practice because a user is rather
interested in its real anonymity (e.g., the difﬁculty to distinguish its
behavior from a regular user that only uses port 443 for HTTPS)
and not in a superﬁcial worst-case behavior.
3.2 Adversary classes

So far, we assumed an adversary that can completely determine
the users’s behavior. Even though this assumption captures that the
adversary can have auxiliary information about the user in ques-
tion and receives all messages that are sent to any server, for many
interesting scenarios this assumption is far too strong. For recipi-
ent anonymity, where the adversary can observe the link from the
user to the Tor network, we cannot assume that the adversary con-
trols the server; otherwise the adversary trivially breaks recipient
anonymity. Hence, we need to restrict the adversary.

In this section, we discuss how to model weaker adversarial set-
tings by introducing so-called adversary classes. It turns out that
the sequential composition result of ANOA does not always hold
for general adversary classes. In this section, we characterize the
adversary classes for which sequential composition holds.

Consider a setting where the adversary is the ISP of the user and
wants to ﬁnd out to which server this user communicates. Since the
adversary in our adaptive extension of ANOA controls all servers
and users, it can always determine the recipient, which is of course
unrealistic. In the spirit of universal composability [8, 10, 22, 27],
we can, however, restrict the inputs that an adversary can send to
the parties by placing a machine between the adversary and the
challenger. We call such a machine an adversary class.
An adversary class A(·) is a wrapper that restricts the adversary
A in its possible output behavior, and thus, in its knowledge about
the world. Technically, it is a PPT machine A(A) that internally
runs the adversary A and forwards all messages that are sent from
a compromised node to the adversary A and vice versa.
Example 2: Hiding recipients. With such a notion of an adversary
class we can model an interactive scenario in which the adver-

516sary solely learns the information that an ISP would learn (and
the information from its additionally compromised Tor nodes). The
adversary class A(·) computes the behavior of the servers without
informing the adversary A about the communication. Thus, the ad-
versary class would respond for the servers and the adversary will
not be directly informed about the messages by the servers them-
selves. In the same way, the adversary class can additionally con-
trol which responses from the servers the adversary can see and,
possibly, allow very few choices and solely inform the adversary
(cid:5)
whether a challenge session has ended.
Sequential composability. It turns out that for some combinations
of adversary classes and protocols proving an anonymity property
for a single session does not imply that this anonymity property
holds for multiple session, i.e., sequential composition does not
hold. As an example, consider a protocol that leaks all secrets once
the second message is sent, and consider an adversary class A(A)
that only forwards challenge-messages to the challenger and that
blocks all input-messages. This combination of a protocol and an
adversary class is insecure from the second challenge on; hence,
for this combination sequential composability does not hold.

We show that sequential composability holds for adversary
classes where additional challenges do not enable qualitatively new
attacks, i.e., the adversary does not gain any conceptually new
power. This can be ensured by requiring the following three con-
ditions from an adversary class A(A) with respect to α. It does
not initiate challenges on its own, it does not behave differently de-
pending on the challenge tag that is associated with a challenge,
and all its challenges are simulatable by the adversary. We call a
class with these three properties composable. The proof (see [7])
is omitted due to space constraints.

THEOREM 1. For every protocol P, every anonymity function
α, every n ∈ N and every adversary class A(A) that is composable
for α. Whenever P is (1, ε, δ)-α-IND-CDP for A(A), with ε ≥ 0
and 0 ≤ δ ≤ 1, then P is (n, n · ε, n · enε · δ)-α-IND-CDP for
A(A).

PROOF OUTLINE. We show the composability theorem by in-
duction over n. We assume the theorem holds for n and compute
the anonymity loss between the games ACREAL(0, n + 1), where
we have b = 0 and ACREAL(1, n + 1), where we have b = 1 via
a transition of indistinguishable, or differentially private games.

We start with ACREAL(0, n + 1) and introduce a simulator that
simulates one of the challenges for the correct bit b = 0. We ap-
ply the induction hypothesis for the remaining n challenges (this
introduces an anonymity loss of (n · ε, n · enε · δ)). The simulator
still simulates one challenge for b = 0, but the bit of the challenger
is now b = 1. We then simulate all remaining n challenges for
b = 1 and thus introduce a game in which all challenges are sim-
ulated. As the bit of the challenger is never used in the game, we
can switch it back to b = 0 again and remove the simulation of the
ﬁrst challenge. We can apply the induction hypothesis again (we
loose (ε, δ)) and switch the bit of the challenger to b = 1 again. In
this game, we have one real challenge (for b = 1) and n simulated
challenges (also for b = 1). Finally, we remove the simulator again
and yield ACREAL(1, n + 1).

We refer to [7] for a formal proof.
3.3 Anonymity Notions

Finally, we are ready to present the anonymity notions for sender

anonymity, recipient anonymity and relationship anonymity.
Sender anonymity.
Sender anonymity models the scenario in
which a malicious recipient, that might additionally control some

Tor nodes, tries to determine the sender of a message. Since we are
not interested in modeling the (semantic) privacy of messages, we
model that the messages do not depend on the sender.

Since we allow challenges that span a whole session (consisting
of several messages) we require that within this session the sender
does not change (i.e., the adversary cannot choose a new pair of
potential senders per message, but only per challenge). We deﬁne
the anonymity function for (Session) Sender Anonymity with the
following anonymity function.
αSSA(s, r0 = (S0,R0, m0, _), r1 = (S1, _, _, _), b)

if s = fresh ∨ s = (S0,S1) then

output ((Sb,R0, m0, 1), s := (S0,S1))

Recipient Anonymity.
In recipient anonymity the adversary is
assumed to control the link to the Tor network, e.g., by compro-
mising the ISP of the user. The goal of the adversary is thus to ﬁnd
out which web pages a user visits. Similar to sender anonymity, the
adversary additionally controls some Tor nodes.

Recall that in Example 2, adversary classes are needed for mod-
eling recipient anonymity for representing the servers and poten-
tially the user proﬁles. Hence, recipient anonymity can only be
proven for certain adversary classes. In addition to requiring that
the servers are machines and the users follow certain user pro-
ﬁles, as in Example 2, there is an inherent insecurity for recipient
anonymity.

Various so-called website ﬁngerprinting attacks [9, 20, 32] are
known against the anonymous communication service Tor that
directly break recipient anonymity.
In these attacks, the adver-
sary recognizes recipient by their trafﬁc patterns, such as direction
changes or size. These attacks, however, are not speciﬁc to Tor.
Every low-latency anonymous channel is prone to these kinds of
attacks. Hence, for a settings in which such website ﬁngerprinting
attacks are possible, recipient anonymity cannot hold.

As a consequence, we only consider settings in which such at-
tacks are not possible. As a ﬁrst step, we deﬁne an anonymity func-
tion αRA for recipient anonymity that excludes challenge messages
that are of different size. Similar to sender anonymity, we model
the requirement that the sender is constant by always choosing the
sender S0 from the ﬁrst message.
αSRA(s, r0 = (S0,R0, m0, _), r1 = (_,R1, m1, _), b)

if (s = fresh ∨ s = S0) ∧ |m0| = |m1| then

output ((S0,Rb, mb, 1), s := S0)

These restrictions in the anonymity function, however, do not
sufﬁce. We can only deﬁne recipient anonymity for servers, i.e., re-
cipients, in which the response of the recipients has the same length
for all messages that are of the same length, i.e., the length of the
response of a server solely depends on the length of the message.
Relationship Anonymity.
In relationship anonymity the adver-
sary is an observer that might control some Tor nodes and that tries
to deanonymize a communication between a sender and a recipi-
ent. As both sender and recipient of this communication are previ-
ously unknown to the adversary (otherwise relationship anonymity
collapses to either sender anonymity or recipient anonymity), we
require a slightly more complex anonymity function:
αSRel(s, r0 = (S0,R0, m0, _), r1 = (S1,R1, _, _), b)

a ← {0, 1}

if s = fresh then
else if ∃x. s = (S0,S1, x) then

a := x

if b=0 then

517output ((Sa,Ra, m0, 1), s := (S0,S1, a))
output ((Sa,R1−a, m0, 1), s := (S0,S1, a))

else

In this function there are four possible scenarios for a challenge
session: Each of the senders could send its messages to each re-
cipient and again the choice is made depending on the bit of the
challenger. If b = 0, then one message is sent, by a randomly cho-
sen sender Sa (that is then used for the whole challenge) to the
recipient Ra that was speciﬁed for this sender by the adversary. If
b = 1, the (still randomly chosen) sender Sa sends its messages
to the other recipient R1−a. The goal of the adversary is to ﬁg-
ure out whether one of its challenges was chosen or whether the
combination of sender and recipient was swapped.
4. MODELING TOR IN AnoA

To ensure the correctness of the MATOR output we need to
model Tor in our extension of ANOA. First, we brieﬂy recall how
Tor and its path selection works and then we explain how they are
modeled.
Tor’s path selection algorithm (PSTOR). As mentioned in Sec-
tion 2, nodes on circuits (or paths) in Tor are not selected (uni-
form) randomly. To improve the performance, Tor’s current path
selection algorithm makes a weighted random choice over all nodes
that support the user’s connections and preferences, and bases the
weights on information that is retrieved from a periodically pub-
lished server descriptor and an hourly published consensus docu-
ment. These documents are generated and maintained by a small
set of semi-trusted directory authorities, and contain up-to-date in-
formation about each node.

In a server descriptor, a Tor node publishes its bandwidth, the
ports it would allow as an exit node, its so-called family (used for
distributing trust), its uptime, its operating system, and its version
of Tor. In order to prevent malicious Tor nodes from equivocating
(i.e., sending different information to different users), the nodes are
required to periodically upload (every 12 to 18 hours) their current
server descriptor to all directory authorities.

The consensus document is computed hourly by the directory
authorities, and it contains for each node information such as the
node’s availability, its entrusted bandwidth, a pointer to the up-to-
date server descriptor, and whether this node should be used as an
exit node and/or an entry node. Moreover, the consensus document
contains entry, middle, and exit scaling factors for every node in
order to balance the bandwidth. This scaling is necessary since
there are fewer nodes that are marked as exit nodes (∼1000 in May
2014) than as entry (∼4000 in May 2014) or middle nodes (∼5000
in May 2014).

The PSTOR algorithm computes the weight of a node based on
the retrieved node’s entrusted bandwidth. Since a circuit establish-
ment is expensive, the path selection tries to include as many of
the requested ports into one circuit as possible. Given a list of re-
quested ports by the user, PSTOR determines the maximal set of
ports that is supported by any exit node, and then excludes all nodes
that do not support this maximal set of ports and that are not marked
as exit nodes. Then, the algorithm assigns a weight to every remain-
ing node by dividing its entrusted bandwidth node.bw by the sum
of the entrusted bandwidths s of all not excluded nodes and mul-
tiplies this with the corresponding exit scaling factor scEx(node)
from the consensus document: node.bw/s∗ scEx(node). Finally,
the algorithm performs a weighted random choice over these nodes.
As Tor is built upon the principle of distributing trust, the path
selection excludes circuits with nodes that are related, i.e., that are
in the same /16 subnet and nodes that are in each other’s family.

After having chosen the exit node, the path selection chooses an
entry node in two steps: ﬁrst, the algorithm excludes all nodes that
are related to the exit node and all nodes that are not marked as
entry nodes in the current consensus; second, the algorithm com-
putes the weight of each of the remaining nodes by dividing their
entrusted bandwidth by the sum of all not excluded nodes and per-
forms a weighted random choice over these nodes. For the middle
node the path selection proceeds as for the entry nodes except that
middle nodes do not require speciﬁc tags. However, all relatives of
both the exit node and the entry node are excluded.

This path selection algorithm is adapted to the preferences of the
user, who can, e.g., decide to only use nodes that have the ‘stable’
tag or to build circuits that only use ‘fast’ nodes. It is also possible,
although strongly recommended against, to allow non-valid entry
or exit nodes as well as entry nodes that are not considered to be
entry guards.
4.1 The Tor protocol in extended AnoA

We base our model of Tor on our previous work that models Tor
as a UC protocol ΠOR [5]. ΠOR is based on Tor’s speciﬁcation and
accurately models the key exchange, the circuit establishment, and
the process of relaying message cells over Tor.

However, ΠOR abstracts from Tor’s path selection by consider-
ing a uniform path selection.
In our work, we use an extension
of ΠOR, where instead of the uniform path selection the above de-
scribed PSTOR algorithm is used. This extension of ΠOR gives us
a protocol on which we can base our analysis of MATOR.

5. ANONYMITY MONITORS

In this section, we estimate these anonymity bounds of Tor for
a given snapshot of the Tor network computed from a given con-
sensus document. We devise anonymity monitors that a client can
run along with the Tor client to estimate sound anonymity bounds
for the PSTOR algorithm. First we present the anonymity monitor
for sender anonymity that gives a bound for the probability that a
malicious server manages to deanonymize the user . Second, we
present the anonymity monitor for recipient anonymity that bounds
the probability that the user’s ISP can determine the recipient of a
communication . Third, we present the anonymity monitor for re-
lationship anonymity that bounds the probability that an observer
can relate trafﬁc to a sender and a recipient .

The output of these three monitors provides a bound of the max-
imal success probability of a worst-case adversary to deanonymize
a Tor user at a given point in time. The success probability is calcu-
lated for an individual circuit creation, i.e., whenever a new circuit
is created the adversary may deanonymize the user for this circuit
with the success probability.
Adversary model. For our monitors we consider a worst-case ad-
versary that statically compromises a given number k of Tor nodes
of its choice.
It may use any auxiliary information it possesses;
however, we assume that it is has no prior knowledge of the entry
guards a user will choose or has chosen. In practice the adversary
can gather its k nodes by attacking the existing nodes or (rather
easily) by adding new nodes to the Tor network.
5.1 Modeling MATor in AnoA

In MATOR we analyze the anonymity of a real Tor user depend-
ing on her preferences pf and connections conn and the veriﬁed
Tor consensus ﬁle and the latest server descriptors.

Technically, we instantiate MATOR in ANOA by deﬁning an ad-
versary class for the user that restricts the ANOA adversary as fol-
lows. As we are interested in the anonymity of a speciﬁc user, we

518for en ∈ N do

if ps.allows(en, ex , pf ) then

SAMonitor(N , pf , ps)
1: for ex ∈ N do
2:
3:
4:
5:
6: sort all nodes in N by their δ(·) value in a list sorted
7: for node ∈ sorted and 1 ≤ i ≤ k do
8:
9: return δ

exitP := ps.exP(ex ); entryP = ps.enP(en, ex )
δ(en)+=exitP · entryP

δ += δ(node); i := i + 1

Figure 3: Sender Anonymity Monitor

overwrite the adversary’s choice of preferences for the challenge
users with the ones from the real user. The adversary may still
choose the preferences of other users in the network

DEFINITION 2

(MATOR proﬁle). For a set of user pref-
erences pf and two sets of ports ports1, ports2 the proﬁle
MATOR(pf ,ports1,ports2) replaces all messages of the form

(challenge,(S1,⊥, (initialize, _, _), sid1),

(S2,⊥, (initialize, _, _), sid1), Ψ)

with (challenge,(S1,⊥, (initialize, pf , ports1), sid1),

(S2,⊥, (initialize, pf , ports2), sid2), Ψ)

and by blocking all challenge messages in which only one user
sends initialize. Consequently, only the preferences pf and ports
ports1 and ports2 that are speciﬁed for the proﬁle can be used for
challenges.

In our monitors’ implementations, we prepare the anonymity anal-
ysis as follows.
Monitor—Preparing the scenario. The monitor ﬁrst prepares
the scenario, consisting of user preferences pf , a list of connections
conn that describes to which servers (particularly over which ports)
the user wants to connect, and information about the current con-
sensus ﬁle and the respective server descriptors. The monitor then
uses a simulation of the PSTOR algorithm to compute the weights
of all nodes for these connections, each depending on the possible
position of the node within the circuit (entry, middle, exit) and de-
pending on the node-speciﬁc data from the consensus ﬁle (e.g., its
tags, bandwidth and IP address).
5.2 Computing Sender Anonymity Bounds

Sender anonymity considers an adversary that wants to
deanonymize a Tor user communicating to a given server that is
under adversarial control.

A long history of prior work on trafﬁc correlation attacks shows
that whenever an adversary controls the entry and the exit node
of a circuit, or the entry node and the server, it may deanonymize
the user [11, 14, 19, 23, 24, 28–30]. Directly following the analysis
from [6] we notice that a (local) adversary with control over the
server can deanonymize the user only if it manages to compromise
the entry node of a circuit. Our monitor thus checks the probability
that an entry node is compromised, which depends on the user’s
preferences and connections as well as on the active nodes. We give
guarantees even against a worst-case adversary that compromises
the k most probable entry nodes.

Figure 3 depicts the pseudocode for computing the bounds for
sender anonymity. Here, the adversary compromises the k most

likely entry nodes for the given connection. Since the probability
that an entry node is chosen is independent of the user (for a ﬁxed
connection), the sender anonymity bound is the sum that any of
these nodes is chosen. Thus, the bounds computed by the sender
anonymity monitor are the worst case bounds. The proof (see [7])
is omitted due to space constraints.

THEOREM 2

(SENDER ANONYMITY MONITOR). Given

a
consensus and a corresponding set of server descriptors, let N
be a set of nodes with bandwidth weights, the preferences pf
of the user, the ports ports to which the user wants to connect,
and a path selection algorithm ps that uses these informations.
Then for the output δ of the algorithm SAMonitor(N , pf , ps)
the following holds:
against passive local adversaries Tor
(1, 0, δ) − αSSA-IND-CDPMATOR(pf ,ports,∅) , where
satisﬁes
αSSA-IND-CDPMATOR(pf ,ports,∅) denotes session sender anonymity
(see Section 3) with the MATOR proﬁle as in Deﬁnition 2.

The sender anonymity monitor and the guard mechanism. The
sender anonymity monitor outputs a bound on the probability that
the adversary might deanonymize a user’s communication using
Tor. This deanonymization is heavily based on the probability of
compromising the user’s entry node, and thus, sender anonymity
only changes whenever a fresh entry node is chosen. If the user
makes use of the guard mechanism [31, 38] she will not choose a
fresh entry node for every communication, but only choose from a
small set of entry nodes that is kept for a long time.

The guard mechanism modiﬁes the adversaries possibilities to
deanonymize the user as follows: As the user rarely choses a fresh
entry node it is less likely that the adversary manages to compro-
mise the entry node, even if the user builds many circuits. How-
ever, if it manages to compromise a user’s guard, it will be able to
deanonymize this user very often and consistently until she changes
her guards again.
5.3 Computing Recipient Anonymity Bounds
Recipient anonymity considers the setting with a malicious ISP
of the user that wants to ﬁnd out which web pages a user visits
and additionally controls some Tor nodes. As presented earlier, in
AnoA recipient anonymity is formalized by comparing two chal-
lenge settings in which the user is the same but the recipients dif-
fer.

For this analysis we exclude website ﬁngerprinting attacks, as
those attacks are attacks on the content of messages and present an
orthogonal attack vector that is not related to the communication
protocol. A malicious ISP could always additionally host ﬁnger-
printing attacks, independent of the protocol we analyze. Techni-
cally, we assume that the adversary cannot learn information from
the answers to challenge messages.2 Recently, some provably se-
cure defenses have been developed against these ﬁngerprinting at-
tacks [37].

For computing the recipient anonymity bound the probability of
three scenarios has to be considered. The ﬁrst and most relevant
scenario encompasses compromising the exit node in the user’s cir-
cuit, where recipient anonymity is immediately broken. The sec-
ond scenario considers a compromised middle node: in this case,
the adversary knows the circuit’s exit node and can check whether
this exit node does not offer a port that is requested in one of the
settings. The third scenario considers a compromised entry node:
2For Tor this formally presents a form of length-regularity:
the
lengths of the responses the two possible recipients of a challenge
message give have to be equally distributed for input messages of
equal length.

519even in this case, the adversary can learn which middle node is
chosen in the circuit and thereby check whether this middle node
is more probable in one of the settings, e.g., because the middle
node is related to a heavy-weighted (i.e., very probable) exit node
that does not offer one of the ports in one of the two challenge set-
tings. Additionally, the adversary might learn something by seeing
the entry node that has been chosen, e.g., it might be less proba-
ble to choose an entry node that is related to a very heavy-weighted
exit node.

A precise estimation of the recipient anonymity bounds has to
take into account how much an adversary could learn from any
given node and then add these values up for the k most advan-
tageous nodes. For each node and each of the three positions
(entry, middle, and exit), the increase in the distinguishing prob-
ability has to be computed. For the exit position,
the adver-
sary can immediately distinguish the challenge settings by com-
promising the exit node; hence this increase δEx is the proba-
bility that the node n is chosen as an exit node: δEx(node) :=

node.bw/((cid:80)

node(cid:48)∈bSi

node(cid:48).bw).

For the entry position, the adversary can not directly learn which
setting is chosen, but it can gain evidence that one of the settings
is chosen, e.g., because some entry nodes are more probable to be
chosen as entry nodes in the ﬁrst setting since in the other settings
these entry nodes are in the family of a heavy-weighted exit node
that is only in the best support of the second setting. Hence, this
increase can be characterized by the difference in the probabilities
for every triple of nodes.

(cid:88)

(cid:88)

δEn(node) =

1 − w
(cid:48)
w

(cid:48)
2

node1∈bS1
node,node1
unrelated

node2∈N
unrelated

node,node1,node2

node,node1,node2

distinguishing

where node, node 1, node 2 distinguishing ⇔ w(cid:48)
and where w(cid:48)
ps.miP(node 2, node, node 1).3

2 > eε,
i = ps.enP(node, node 1) · ps.exPi(node 1) ·

1/w(cid:48)

Iterating over all node triples, however, is not sufﬁciently efﬁ-
cient for a live recipient anonymity monitor. To improve the ef-
ﬁciency, we over-approximate the inﬂuence of the exit node on
δEn(node) and omit the exit nodes in the computation.

These approximations inﬂuence relevant but rather minor factors
of the anonymity guarantee, as the δ mostly depends on the prob-
ability to compromise exit nodes. A detailed description of our
approximations and the proof (see [7]) are omitted due to space
constraints.

THEOREM 3

(RECIPIENT ANONYMITY MONITOR). For a
consensus document, a set of server descriptors, a user’s prefer-
ences pf and two sets of ports ports1 and ports2, let N be the
set of nodes in the consensus together with their weights. Let
ε > 0 be a real value . Then for the output δ of the algorithm
RAMonitor(N , pf , ports1,ports2,ps,ε)
holds
against passive local adversaries:

following

the

Tor satisﬁes (1, ε, δ) − αSRA-IND-CDPMATOR(pf ,ports1 ,ports2) ,

where αSRA-IND-CDPMATOR(pf ,ports1,ports2) denotes session recipi-
ent anonymity (see Section 3) .

The recipient anonymity monitor and the guard mechanism. In
recipient anonymity we assume that the adversary has knowledge
3We calculate a bound on δ for a given multiplicative factor of
ε ≥ 0. Intuitively, we take a bound for ε as input and compute
the probability that this bound cannot be achieved.

about the user’s connection to the Tor network. For our bounds
we model both the case where the user does not use entry guards,
i.e., uses a fresh entry node for every circuit and the case in which
the user uses guards. To this end we allow the user to specify her
entry guards (we envision that they are read from the Tor source-
code if MATOR is integrated into Tor). In this case we allow the
adversary to even have auxiliary information about the guards as
we can assume that they have been used before. This in particular
allows the adversary to target the entry guards if that helps it to
break recipient anonymity. Note, however, that the entry node plays
only a minor role in breaking recipient anonymity.
5.4 Relationship Anonymity Bounds

Relationship anonymity considers a setting in which an observer
tries to deanonymize a communication between a sender and a re-
cipient. The adversary does not already control the ISP of the
sender or the server, but corrupts or controls some Tor nodes. As
presented earlier, in AnoA, relationship anonymity is formalized by
comparing two challenge settings in which there are two possible
senders and two possible recipients. As for recipient anonymity,
we exclude ﬁngerprinting attacks for this analysis, as those attacks
are based on the content of messages.

For computing the relationship anonymity bound we ﬁrst ob-
serve that the adversary has to break the anonymity of the sender
in order to succeed. Consequently, the scenarios in which an ad-
versary gains an advantage are a combination of the scenarios from
sender anonymity and from recipient anonymity: As long as the
probability for choosing entry nodes is not different for different
senders, the entry node has to be compromised. Still, the probabil-
ity of three scenarios associated with recipient anonymity has to be
considered. The most intuitive and most severe scenario is a com-
promised entry node in combination with a compromised exit node.
In this case an adversary can immediately see both the sender and
the recipient of a message, and thus, break relationship anonymity.
The second scenario is a compromised entry node in combination
with a compromised middle node. Here, depending on the ports
that are used, an observed exit node might be more likely in one
of the scenarios or even impossible, in some other scenarios. The
third scenario is a compromised entry node without other compro-
mised nodes. Even now the adversary might learn something by
seeing the middle node, which might be more or less likely to be
chosen, depending on which exit node was chosen (which, again,
might depend on the scenario).

Similarly to recipient anonymity, computing a precise bound
for relationship anonymity directly is infeasible for a light-weight
anonymity monitor. Consequently we give a sound approximation
for the anonymity guarantee. We give estimates for the advantage
of the adversary for each combination of nodes, as well as for each
individual entry node. To overcome the necessity to compute the
best subset of all nodes such that their combinations yield the op-
timal advantage for the adversary, we observe that by compromis-
ing k nodes an adversary can only compromise at most k·(k−1)
combinations of nodes. Our bound comprises of the top k values
from compromising entry nodes only and the top k·(k−1)
values
that arise from combinations of two nodes.

2

2

THEOREM 4

(RELATIONSHIP ANONYMITY MONITOR).

For a consensus document, a set of server descriptors, a
user’s preferences pf
and two sets of ports ports1 and
let N be the set of nodes in the consensus to-
ports2,
gether with their weights.
Let ε > 0 be a real value
the algorithm RelMonitor(N ,
.
pf , ports1, ports2,ps,ε) Tor satisﬁes Tor satisﬁes (1, ε, δ) − αSRel

Then for the output δ of

520Figure 4: Relationship anonymity guarantees (value of δ) de-
pending on the number of compromised nodes. We used ε = 0
and as settings the ports HTTPS+IRC vs. HTTPS.

Figure 6: Comparison between PSTOR and DISTRIBUTOR. The
graph shows the value for δ with ε = 0 for 0 to 50 compromised
nodes of the adversary’s choice.

Figure 5: Bandwidth and weights chosen by Tor (from top to
bottom: total bandwidths, entry bandwidths, exit bandwidths,
one point for each node; the nodes are sorted by bandwidth for
each line)

Figure 7: Anonymity guarantees (value of δ) over the course of
Feb.2014 for 0.5% compromised nodes. We used ε = 0 and as
settings the ports HTTPS+IRC vs. HTTPS.

-IND-CDPMATOR(pf ,ports1,ports2 )
local adver-
saries, where αSRel-IND-CDPMATOR(pf ,ports1,ports2 ) denotes session
relationship anonymity (see Section 3).

against passive

6. EXPERIMENTAL RESULTS

Our anonymity monitors described in Section 5 allow us to per-
form an analysis on the real consensus-data published by Tor [33].
In this section we present a selection of the guarantees that the mon-
itors computed.
6.1 Implementation and Data collection

We implement our sender, recipient, and relationship anonymity
monitors as multi-threaded C++ programs. The code comprises
of approximately 3000 lines codes and employs SQLite [2] and
Boost [3] libraries. The monitor programs are available on our web-
site [1].

For our analysis we process the server descriptions of Tor that
are released every month to construct a database of family relation-
ships between nodes [7]. Processing the server descriptors takes
a signiﬁcant amount of time (around 15 minutes), but we require
this computation only once per month. Moreover, the information
does not depend on the settings of the user, which means that the
database could be precomputed and downloaded once per month.

Performance. We measured the performance of the monitors on a
standard notebook (MacBook Air 2 GHz Intel Core i7, 4 GB 1600
MHz DDR3 RAM). Our monitors start by processing a consensus
ﬁle and by computing the weights of all nodes depending on the
path selection algorithm, the connections of the user (and of the
scenario we want to compare the user with). In our performance
evaluation we called this part of the computation “preparing the
weights”. Afterwards the anonymity guarantees are computed. The
exact computation times are given in Figure 8 (averaged over 100
runs on consensus ﬁles from Feb. 2014).

 0 0.1 0.2 0.3 0.4 0.5 0.6 0 50 100 150 200Anonymity bound δPSTor 443+194 (IRC) vs 443PSTor 443 vs 443DistribuTor 443+194 (IRC) vs 443DistribuTor 443 vs 443 1 10 100 1000 10000 100000 0 1000 2000 3000 4000 5000Bandwidths of all nodesEntry bandwidthsExit bandwidths 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50Anonymity bound δRecipient Anonymity PSTorSender Anonymity PSTorRelationship Anonymity PSTorRecipient Anonymity DistribuTorSender Anonymity DistribuTorRelationship Anonymity DistribuTor 0 0.2 0.4 0.6 0.8 114-02-0114-02-0814-02-1514-02-2214-03-01Anonymity bound δRecipient Anonymity PSTorSender Anonymity PSTorRelationship Anonymity PSTorRecipient Anonymity DistribuTorSender Anonymity DistribuTorRelationship Anonymity DistribuTor521Preparing the weights
Computing sender anonymity guarantee
Computing recipient anonymity guarantee
Computing relationship anonymity guarantee

3.39 sec.
0.73 sec.
6.07 sec.
9.10 sec.

Figure 8: Performance of our anonymity monitors

6.2 Path Selection Strategies

The bandwidth of Tor nodes is not uniformly distributed as Tor
tries improve its performance by selecting nodes depending on their
bandwidth. As a result, a node with twice the bandwidth is twice as
likely to be used for a circuit. The real-life bandwidth distribution,
however, contains nodes that are several hundred times as likely as
other nodes. (See the log-scale graph in Figure 5.) Consequently, a
small number of nodes with a very high bandwidth is used in a large
percentage of circuits. If these nodes get compromised or similar
new nodes are added to the network by an adversary, this adversary
can deanonymize many connections. Consequently the current path
selection of Tor produces obvious targets such that an attacker that
compromises these points can deanonymize a signiﬁcant part of the
network.
Novel loss-less path selection: Re-balancing the workload. To
reduce the risk posed by such high bandwidth nodes we propose
DISTRIBUTOR, a path selection algorithm that distributes the trust
amongst exit and entry nodes as evenly as possible. We observe
that the exit bandwidth inherently is a bottleneck as only few nodes
wish to be used as exit nodes. Consequently, we ﬁrst focus on the
exit nodes.

1. Distributing the bandwidth for exit nodes: We compute the
exit bandwidth that Tor supports at a given moment by summing
over the bandwidth of all possible exit nodes and weighting them
according to their tags and the weights from the consensus. We
then compute how evenly we can distribute the bandwidth by using
small exit nodes solely as exit nodes and restricting the usage of
the largest exit nodes accordingly. In this process we make sure
that the previous exit bandwidth is still provided by our improved
path selection algorithm.

2. Distributing the bandwidth for entry nodes: After the
weight of nodes for their position as an exit node has been set we
compute the weights for entry nodes. We proceed just as before,
by trying to preserve the entry bandwidth and still distributing the
trust in entry nodes as widely as possible.
Anonymity improvement. As we put a bound for the maximal
weight of exit and entry nodes, we use their remaining bandwidth
by increasing their weight to be used as middle node, as this po-
sition is considered least critical. The details of our redistribution
can be found in Figure 10. In the following section we present ex-
perimental results computed on the real consensus data of Tor and
evaluate DISTRIBUTOR against Tor’s path selection algorithm.

Naturally it would be possible to sacriﬁce performance of the Tor
network for a much stronger improvement in anonymity by reduc-
ing the targeted total bandwidth.
In an extreme case one could
weight all nodes uniformly, which would allow much stronger
anonymity guarantees.

Note that we did not consider the case that the entry bandwidth
poses a bottleneck for Tor. In this case, one should change the order
in which these calculations are made.
6.3 Lessons learned
Advantages of DistribuTor over PSTor. As expected, our DIS-
TRIBUTOR algorithm signiﬁcantly improves sender anonymity and

Figure 9: Impact of the choice of ports on recipient anonymity.
We chose ε = 0. The graph depicts the value for δ depending
on the number of compromised nodes (0 to 50). We compared
all settings against HTTPS only (port 443).

also moderately improves recipient anonymity in all experiments.
The only moderate improvement of recipient anonymity is to be
expected as the exit bandwidth inherently is a bottleneck of Tor.
The improvement in sender anonymity, however, is more signiﬁ-
cant (see Figure 6). As the re-balancing path selection algorithm
does not affect Tor’s overall performance, it presents a possibility
to improve the anonymity guarantees at virtually no cost.
Change in the anonymity guarantees over time. Our monitors
also allow to analyze whether and how the anonymity guarantees
vary over time. Such variations are the result of changes in the
number of available servers, their bandwidth and their exit policies.
Figure 7 shows how the guarantees change over the course of a
month (February 2014).
Anonymity guarantees over the last years. As a long-time study
analyzed the guarantees for the last 24 Months in Figure 1 (c.f.
Section 2). We smoothed the graph by computing the average
anonymity for each day in order to improve the readability. inter-
estingly, the guarantees improve slightly over time, even though we
allowed the adversary to compromise a ﬁxed percentage of nodes,
and thus, to compromise more nodes of its choice as the Tor net-
work grows in size.
Anonymity guarantees depending on the ports. The ports re-
quested by the user signiﬁcantly impact the (recipient) anonymity
guarantees. In Figure 9 we show the recipient anonymity guaran-
tees depending on the number of compromised nodes for the 5’th
of February. As settings we chose a multiplicative value of ε = 0
and we disabled guards and did not restrict the path selection to fast
or stable nodes.
6.4 The impact of a multiplicative factor

The deﬁnition of AnoA introduces a multiplicative factor in ad-
dition to the normal additive factor (that often sufﬁces to describe
the success probability of an adversary). This factor allows for ac-
counting for various events in which an adversary might gain infor-
mation that may even lead to a non-negligible advantage without
overestimating these events.

The experiments show that such a factor often only plays a mi-
nor role, as the probability to completely deanonymize a user is for

 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50Anonymity bound δRecipient Anonymity 443+22+23+194 vs 443Recipient Anonymity 443+23 (Telnet) vs 443Recipient Anonymity 443+194 (IRC) vs 443Recipient Anonymity 443+22 (SSH) vs 443Recipient Anonymity 443 vs 443522if node.bw < maxExitBW then

node.exitBW
1: if node can be used as exit then
2:
3:
4:
5: else return 0

else return maxExitBW

return node.bw

if node.bw < maxExitBW then

node.entryBW
1: if node can be used as entry then
2:
if node can be used as exit then
3:
4:
5:
6:
7:
8:
9:
10: else return 0

else return 0

else return maxEntryBW

else

return 0
if node.bw− maxExitBW < maxEntryBW then

return node.bw − maxExitBW

node.middleBW
1: bw := node.bw
2: if node can be used as exit then
bw := bw − maxExitBW
3:
4: if node can be used as entry then
bw := bw − maxEntryBW
5:
6: if bw > 0 then return bw
7: else return 0

Figure 10: DistribuTor: Our redistribution of the bandwidths

most settings higher than the probability to just learn some infor-
mation about them. Recipient anonymity in a setting with a weaker
adversary, that compromises no, or only a very limited amount of
nodes presents a noteworthy exception. Recall that for recipient
anonymity we assume that the ISP of the user is compromised,
which means that the adversary can see which entry node the user
connects to. For different ports the probability of choosing these
entry nodes, however, will be different, because they might also be
possible exit nodes, or related to possible exit nodes. For PSTOR,
an adversary that compromises no (only a very limited number
of) nodes can have already a non-negligible advantage in guess-
ing which port a user might choose, which can either be expressed
by a multiplicative factor and a δ of zero, or by a non-negligible
δ. (See Figure 11.) Notice that for DISTRIBUTOR the value for δ
does not reach zero, as the redistribution of exit bandwidth intro-
duces a small (distinguishing) event in which the adversary can win
without compromising nodes.

7. CONCLUSION & FUTURE WORK

This work presents a framework for rigorously assessing the de-
gree of anonymity in the Tor network: MATOR. We carefully ad-
dress the impact of user anonymity by real-life characteristics of
Tor, such as its path selection algorithm, Tor consensus data, and
the preferences and the connections of the user. The anonymity as-
sessment is derived from a theoretical framework for anonymous
communication networks. To obtain such a theoretical framework
that suits our needs, we extended the ANOA framework [6]: a
general framework for anonymous communication networks. Us-
ing MATOR together with Tor’s publicly available consensus and
server descriptor data, we developed the ﬁrst real-time anonymity
monitor. We apply this real-time monitor to archived data of the

Figure 11:
Impact of a multiplicative factor on recipient
anonymity. We chose values for ε of ε = 0.25 for one setting
and ε = 0 for the other setting. The graph depicts the value for
δ computed by the monitor for both path selection algorithms.
The scenarios are: HTTPS + IRC vs. HTTPS

Tor network, using Tor Metrics data [33]. Based on the evalua-
tion of these experiments, we propose an alternative path selec-
tion algorithm DISTRIBUTOR. We illustrate by our experiments
that DISTRIBUTOR provides stronger anonymity guarantees with-
out decreasing the overall performance of the Tor network.

A natural next step is the integration of MATOR to the actual Tor
code. An interesting direction for future research is modeling the
recently proposed congestion-aware path selection algorithm [36].
This path selection improves the overall performance of Tor, but
reduces the anonymity guarantees of Tor. It would be great to see
whether it is possible to compute in real-time a bound on the loss
of anonymity loss of this more efﬁcient path selection.

For future work, we also leave the application of the MATOR
framework to Tor hidden services. A real-time anonymity monitor
for a hidden service could be used to automatically disconnect from
Tor whenever the anonymity bounds drop below a certain thresh-
old.
Acknowledgments. We would like to thank the anonymous re-
viewers for their helpful comments and Marcin Slowik for revising
the code and implementing MATOR for relationship anonymity.
This work was supported by the German Ministry for Educa-
tion and Research (BMBF) through funding for the Center for IT-
Security, Privacy and Accountability (CISPA) and the German Uni-
versities Excellence Initiative.
8. REFERENCES
[1] Source-code of MATor. available at

https://www.infsec.cs.uni-saarland.de/
projects/anonymity-guarantees/mator.html.

[2] SQLite. http://www.sqlite.org/.
[3] The Boost C+ Libraries. http://www.boost.org.
[4] M. Akhoondi, C. Yu, and H. V. Madhyastha. LASTor: A
Low-Latency AS-Aware Tor Client. In Proc. of the 2012
IEEE Symposium on Security and Privacy (S& P), pages
476–490. IEEE Computer Society, 2012.

[5] M. Backes, I. Goldberg, A. Kate, and E. Mohammadi.

Provably Secure and Practical Onion Routing. In Proc. 26st
IEEE Symposium on Computer Security Foundations (CSF),
pages 369–385, 2012.

[6] M. Backes, A. Kate, P. Manoharan, S. Meiser, and
E. Mohammadi. Anoa: A framework for analyzing

 0 0.2 0.4 0.6 0.8 1 0 10 20 30 40 50Anonymity bound δRecipient Anonymity PSTorRecipient Anonymity with eps PSTorRecipient Anonymity DistribuTorRecipient Anonymity with eps DistribuTor523anonymous communication protocols. In Computer Security
Foundations Symposium (CSF), 2013 IEEE 26th, pages
163–178. IEEE, 2013.

[7] M. Backes, A. Kate, S. Meiser, and E. Mohammadi.

(Nothing else) MATor(s): Monitoring the Anonymity of
Tor’s Path Selection. Cryptology ePrint Archive, Report
2014/621, 2014. http://eprint.iacr.org/.

[8] M. Backes, B. Pﬁtzmann, and M. Waidner. The Reactive

Simulatability (RSIM) Framework for Asynchronous
Systems. Information and Computation, 205(12):1685–1720,
2007.

[9] X. Cai, X. C. Zhang, B. Joshi, and R. Johnson. Touching

From a Distance: Website Fingerprinting Attacks and
Defenses. In Proceedings of the 19th ACM Conference on
Computer and Communications Security (CCS), pages
605–616, 2012.

[10] R. Canetti. Universally Composable Security: A New

Paradigm for Cryptographic Protocols. Cryptology ePrint
Archive, Report 2000/067, 2013.

[11] S. Chakravarty, A. Stavrou, and A. D. Keromytis. Trafﬁc

Analysis against Low-Latency Anonymity Networks Using
Available Bandwidth Estimation. In Proceedings of the 15th
European Symposium on Research in Computer Security
(ESORICS), pages 249–267, 2010.

[12] C. Dwork. Differential Privacy. In ICALP (2), pages 1–12,

2006.

[13] M. Edman and P. Syverson. As-awareness in tor path

selection. In Proceedings of the 16th ACM Conference on
Computer and Communications Security, CCS ’09, pages
380–389, New York, NY, USA, 2009. ACM.

[14] N. S. Evans, R. Dingledine, and C. Grothoff. A Practical

Congestion Attack on Tor Using Long Paths. In Proceedings
of the 18th USENIX Security Symposium (USENIX), pages
33–50, 2009.

[15] J. Feigenbaum, A. Johnson, and P. F. Syverson. A Model of

Onion Routing with Provable Anonymity. In Proc. 11th
Conference on Financial Cryptography and Data Security
(FC), pages 57–71, 2007.

[16] J. Feigenbaum, A. Johnson, and P. F. Syverson. Probabilistic
Analysis of Onion Routing in a Black-Box Model. In Proc.
6th ACM Workshop on Privacy in the Electronic Society
(WPES), pages 1–10, 2007.

[17] J. Feigenbaum, A. Johnson, and P. F. Syverson. Probabilistic

Analysis of Onion Routing in a Black-Box Model. ACM
Transactions on Information and System Security (TISSEC),
15(3):14, 2012.

[18] N. Gelernter and A. Herzberg. On the limits of provable

anonymity. In Proc. 12th ACM Workshop on Privacy in the
Electronic Society (WPES), pages 225–236, 2013.

[19] Y. Gilad and A. Herzberg. Spying in the Dark: TCP and Tor

Trafﬁc Analysis. In Proceedings of the 12th Privacy
Enhancing Technologies Symposiun (PETS), pages 100–119,
2012.

[20] X. Gong, N. Kiyavash, and N. Borisov. Fingerprinting

Websites using Remote Trafﬁc Analysis. In Proceedings of
the 17th ACM Conference on Computer and
Communications Security (CCS), pages 684–686, 2010.

[21] A. Hevia and D. Micciancio. An Indistinguishability-Based

Characterization of Anonymous Channels. In Proc. 8th
Privacy Enhancing Technologies Symposium (PETS), pages
24–43, 2008.

[22] D. Hofheinz and V. Shoup. Gnuc: A new universal

composability framework. Journal of Cryptology, pages
1–86, 2013.

[23] N. Hopper, E. Y. Vasserman, and E. Chan-Tin. How much

anonymity does network latency leak? ACM Transactions on
Information and Systems Security (TISSEC), 13(2):13, 2010.

[24] A. Houmansadr and N. Borisov. SWIRL: A Scalable
Watermark to Detect Correlated Network Flows. In
Proceedings of the 18th Annual Network & Distributed
System Security Symposium (NDSS), 2011.

[25] A. Johnson, C. Wacek, R. Jansen, M. Sherr, and P. Syverson.

Users get routed: Trafﬁc correlation on tor by realistic
adversaries. In Proceedings of the 2013 ACM SIGSAC
conference on Computer & communications security, pages
337–348. ACM, 2013.

[26] A. M. Johnson, P. Syverson, R. Dingledine, and

N. Mathewson. Trust-based anonymous communication:
Adversary models and routing algorithms. In Proceedings of
the 18th ACM conference on Computer and communications
security, pages 175–186. ACM, 2011.

[27] R. Küsters and M. Tuengerthal. The IITM Model: a Simple
and Expressive Model for Universal Composability. IACR
Cryptology ePrint Archive, 2013:25, 2013.

[28] Z. Ling, J. Luo, Y. Zhang, M. Yang, X. Fu, and W. Yu. A

Novel Network Delay based Side-Channel Attack: Modeling
and Defense. In Proceedings of the 31st Annual IEEE
International Conference on Computer Communications
(INFOCOM), pages 2390–2398, 2012.

[29] P. Mittal, A. Khurshid, J. Juen, M. Caesar, and N. Borisov.

Stealthy Trafﬁc Analysis of Low-Latency Anonymous
Communication Using Throughput Fingerprinting. In
Proceedings of the 18th ACM Conference on Computer and
Communications Security (CCS), pages 215–226, 2011.

[30] G. O’Gorman and S. Blott. Improving Stream Correlation
Attacks on Anonymous Networks. In Proceedings of the
24th ACM Symposium on Applied Computing (SAC), pages
2024–2028, 2009.

[31] L. Øverlier and P. Syverson. Locating hidden servers. In

Proceedings of the 2006 IEEE Symposium on Security and
Privacy, May 2006.

[32] A. Panchenko, L. Niessen, A. Zinnen, and T. Engel. Website

Fingerprinting in Onion Routing based Anonymization
Networks. In Proceedings of the 10th ACM Workshop on
Privacy in the Electronic Society (WPES), pages 103–114,
2011.

[33] Tor Metrics Portal.

https://metrics.torproject.org/. Accessed in
May 2014.

[34] The Tor Project. https://www.torproject.org/.

Accessed in May 2014.

[35] C. Wacek, H. Tan, K. S. Bauer, and M. Sherr. An empirical

evaluation of relay selection in tor. In Proc. 20th Annual
Network & Distributed System Security Symposium (NDSS),
2013.

[36] T. Wang, K. Bauer, C. Forero, and I. Goldberg.

Congestion-aware path selection for tor. In Financial
Cryptography and Data Security, pages 98–113. Springer,
2012.

[37] T. Wang, X. Cai, R. Nithyanand, R. Johnson, and

I. Goldberg. Effective Attacks and Provable Defenses for
Website Fingerprinting. In Proc. 23th USENIX Security
Symposium (USENIX), 2014.

[38] M. Wright, M. Adler, B. N. Levine, and C. Shields.

Defending Anonymous Communication Against Passive
Logging Attacks. In Proc. 24th IEEE Symposium on Security
and Privacy, pages 28–43, 2003.

524