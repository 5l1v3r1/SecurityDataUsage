On the Security of Picture Gesture Authentication

Ziming Zhao and Gail-Joon Ahn, Arizona State University and GFS Technology, Inc.;  

Jeong-Jin Seo, Arizona State University; Hongxin Hu, Delaware State University

Open access to the Proceedings of the 22nd USENIX Security Symposium is sponsored by USENIXThis paper is included in the Proceedings of the 22nd USENIX Security Symposium.August 14–16, 2013 • Washington, D.C., USAISBN 978-1-931971-03-4On the Security of Picture Gesture Authentication

†Arizona State University

‡GFS Technology, Inc.

§Delaware State University

Ziming Zhao†‡ Gail-Joon Ahn†‡

Jeong-Jin Seo† Hongxin Hu§

{zzhao30,gahn,jseo15}@asu.edu

hhu@desu.edu

Abstract

Computing devices with touch-screens have experi-
enced unprecedented growth in recent years. Such an
evolutionary advance has been facilitated by various ap-
plications that are heavily relying on multi-touch ges-
tures. In addition, picture gesture authentication has been
recently introduced as an alternative login experience to
text-based password on such devices. In particular, the
new Microsoft Windows 8TM operating system adopts
such an alternative authentication to complement tradi-
tional text-based authentication. In this paper, we present
an empirical analysis of picture gesture authentication on
more than 10,000 picture passwords collected from over
800 subjects through online user studies. Based on the
ﬁndings of our user studies, we also propose a novel at-
tack framework that is capable of cracking passwords on
previously unseen pictures in a picture gesture authen-
tication system. Our approach is based on the concept
of selection function that models users’ password selec-
tion processes. Our evaluation results show the proposed
approach could crack a considerable portion of collected
picture passwords under different settings.

1 Introduction

Using text-based passwords that include alphanumerics
and symbols on touch-screen devices is unwieldy and
time-consuming due to small-sized screens and the ab-
sence of physical keyboards. Consequently, mobile op-
erating systems, such as iOS and Android, integrate a
numeric PIN and a draw pattern as alternative authenti-
cation schemes to provide user-friendly login services.
However, the password spaces of these schemes are sig-
niﬁcantly smaller than text-based passwords, rendering
them less secure and easy to break with some knowledge
of device owners [8].

All correspondences should be addressed to Dr. Gail-Joon Ahn at

gahn@asu.edu.

Many

password

To bring a fast and ﬂuid login experience on touch-
the Windows 8TM operating system
screen devices,
comes with a picture password authentication system,
namely picture gesture authentication (PGA) [25], which
is also an instance of background draw-a-secret (BDAS)
schemes [18]. This new authentication mechanism hit
the market with miscellaneous computing devices in-
cluding personal computers and tablets. At the time of
writing, over 60 million Windows 8TM licenses have been
sold [21] and it is estimated that 400 million computers
and tablets will run Windows 8TM with this newly intro-
duced authentication scheme in one year [28]. Conse-
quently, it is imperative to examine and explore potential
attacks on picture gesture authentication in such a preva-
lent operating system for further understanding user ex-
periences and enhancing this commercially popular pic-
ture password system.
graphical

schemes–including
DAS [24], Face [9], Story [15], PassPoints [41] and
BDAS [18]–have been proposed in the past decade
(for more, please refer to [6, 7, 13, 14, 16, 23, 34, 37]).
Amongst these schemes, click-based schemes, such as
PassPoints, have attracted considerable attention and
some research has analyzed the patterns and predictable
characteristics shown in their passwords [12, 39]. Fur-
thermore, harvesting characteristics from passwords of
a target picture and exploiting hot-spots and geometric
patterns on the target picture have been proven effective
for attacking click-based schemes [17, 32, 38]. However,
PGA allows complex gestures other than a simple
click. Moreover, a new feature in PGA, autonomous
picture selection by users, makes it unrealistic to harvest
passwords from the target pictures for learning.
In
other words,
the target picture is previously unseen
to any attack models. All existing attack approaches
lack a generic knowledge representation of user choice
in password selection that should be abstracted from
speciﬁc pictures. The absence of this abstraction makes
existing attack approaches impossible or abysmal (if

USENIX Association  

22nd USENIX Security Symposium  383

possible) to work on previously unseen target pictures.

In this paper, we provide an empirical analysis of user
choice in PGA based on real-world usage data, show-
ing interesting ﬁndings on user choice in selecting back-
ground picture, gesture location, gesture order, and ges-
ture type. In addition, we propose a new attack frame-
work that represents and learns users’ password selec-
tion patterns from training datasets and generates ranked
password dictionaries for previously unseen target pic-
tures. To achieve this, it is imperative to build generic
knowledge of user choice from the abstraction of hot-
spots in pictures. The core of our framework is the con-
cept of a selection function that simulates users’ selection
processes in choosing their picture passwords. Our ap-
proach is not coupled with any speciﬁc pictures. Hence,
the generation of a ranked password list is then trans-
formed into the generation of a ranked selection function
list which is then executed on the target pictures. We
present two algorithms for generating the selection func-
tion list: one algorithm is to appropriately develop an op-
timal guessing strategy for a large-scale training dataset
and the other deals with the construction of high-quality
dictionaries even when the size of the training dataset is
small. We also discuss the implementation of our attack
framework over PGA, and evaluate the efﬁcacy of our
proposed approach with the collected datasets.

The contributions of this paper are summarized as fol-

lows:

• We compile two datasets of PGA usage from user
studies2 and perform an empirical analysis on col-
lected data to understand user choice in background
picture, gesture location, gesture order, and gesture
type;

• We introduce the concept of a selection function
that abstracts and models users’ selection processes
when selecting their picture passwords. We demon-
strate how selection functions can be automatically
identiﬁed from training datasets; and

• We propose and implement a novel attack frame-
work which could be potentially redesigned as
a picture-password-strength meter for PGA. Our
evaluation results show that our approach cracked
48.8% passwords for previously unseen pictures in
one of our datasets and 24.0% in the other within
fewer than 219 guesses (the entire password space is
230.1).

The rest of this paper is organized as follows. Sec-
tion 2 gives an overview of picture gesture authentica-
tion. Section 3 discusses our empirical analysis on pic-
ture gesture authentication.
In Section 4, we illustrate

2These datasets with the detailed information will be available at

http://sefcom.asu.edu/pga/.

our attack framework. Section 5 presents the implemen-
tation details and evaluation results of the proposed at-
tack framework. We discuss several research issues in
Section 6 followed by the related work in Section 7. Sec-
tion 8 concludes the paper.

2 Picture Gesture Authentication: An

Overview

Like other login systems, Windows 8TM PGA has two
independent phases, namely registration and authentica-
tion. In the registration stage, a user chooses a picture
from his or her local storage as the background. PGA
does not force users to choose pictures from a predeﬁned
repository. Even though users may choose pictures from
common folders, such as the Picture Library folder
in Windows 8TM, the probability for different users to
choose an identical picture as the background for their
passwords is low. This phenomenon requires potential
attack approaches to have the ability to perform attacks
on previously unseen pictures. PGA then asks the user
to draw exactly three gestures on the picture with his or
her ﬁnger, mouse, stylus, or other input devices depend-
ing on the equipment he or she is using. A gesture could
be viewed as the cursor movements between a pair of
‘ﬁnger-down’ and ‘ﬁnger-up’ events. PGA does not al-
low free-style gestures, but only accepts tap (indicating
a location), line (connecting areas or highlighting paths),
and circle (enclosing areas) [29]. If the user draws a free-
style gesture, PGA will convert it to one of the three rec-
ognized gestures. For instance, a curve would be con-
verted to a line and a triangle or oval will be stored as a
circle. To record these gestures, PGA divides the longest
dimension of the background image into 100 segments
and the short dimension on the same scale to create a
grid, then stores the coordinates of the gestures. The line
and circle gestures are also associated with additional in-
formation such as directions of the ﬁnger movements.

Once a picture password is successfully registered,
the user may login the system by drawing correspond-
ing gestures instead of typing his or her text-based pass-
word. In other words, PGA ﬁrst brings the background
image on the screen that the user chose in the registration
stage. Then, the user should reproduce the drawings he
or she set up as his or her password. PGA compares the
input gestures with the previously stored ones from the
registration stage. The comparison is not strictly rigid
but shows tolerance to some extent.
If any of gesture
type, ordering, or directionality is wrong, the authenti-
cation fails. When they are all correct, an operation is
further taken to measure the distance between the input
password and the stored one. For tapping, the gesture
passes authentication if the predicate 12− d2 ≥ 0 satis-
ﬁes, where d denotes the distance between the tap coordi-

384  22nd USENIX Security Symposium 

USENIX Association

2

nates and the stored coordinates. The starting and ending
points of line gestures and the center of circle gestures
are measured with the same predicate [29].

The differences between PGA and the ﬁrst BDAS
scheme proposed in [18] include: i) in PGA, a user up-
loads his or her picture as the background instead of
choosing one from a predeﬁned picture repository; ii) a
user is only allowed to draw three speciﬁc types of ges-
tures in PGA, while BDAS takes any form of strokes.
The ﬁrst difference makes PGA more secure than the pre-
vious scheme, because a password dictionary could only
be generated after the background picture is acquired.
However, the second characteristic reduces the theoret-
ical password space from its counterpart. Pace et al. [29]
quantiﬁed the size of the theoretical password space of
PGA which is 230.1 with current length-three conﬁgu-
ration in Windows 8TM. For more details, please refer
to [29].

3 An Empirical Analysis of Picture Ges-

ture Authentication

In this section, we present an empirical analysis on user
choice in PGA by analyzing data collected from our user
studies. Our empirical study is based on human cognitive
capabilities. Since human cognition of pictures is limited
in a similar way to their cognition of texts, the picture
passwords selected by users are probably constrained by
human cognitive limits which would be similar to the
ones in text-based passwords [42].
3.1 Experiment Design
For the empirical study, we developed a web-based PGA
system for conducting user studies. The developed sys-
tem resembles Windows 8TM PGA in terms of its work-
ﬂow and appearance. The differences between our im-
plementation and Windows 8TM PGA include:
i) our
system works with major browsers in desktop PCs and
tablets whereas Windows 8TM PGA is a stand-alone pro-
gram; ii) some information, such as the criterion for cir-
cle radius comparison, is not disclosed. In other words,
our implementation and Windows 8TM PGA differ in
some criteria (we regard radiuses the same if their dif-
ference is smaller than 6 segments in grid). In addition,
our developed system has a tutorial page that includes
a video clip educating how to use the system and a test
page on which users can practice gesture drawings.

Our study protocol, including the type of data we plan
to collect and the questionnaire we plan to use, was re-
viewed by our institution’s IRB. The questionnaire con-
sisted of four sections: i) general information of the sub-
ject (gender, age, level of education received, and race);
ii) general feeling toward PGA (is it easier to remem-
ber, faster to input, harder to guess, and easier to observe

than text-based password); iii) selection of background
picture (preferred picture type); and iv) selection of pass-
word (preferred gesture location and type).

We started user studies after receiving the IRB ap-
proval letter in August 2012 and compiled two datasets
from August 2012 to January 2013 using this system.
Dataset-1 was acquired from a testbed of picture pass-
word used by an undergraduate computer science class.
Dataset-2 was produced by advertising our studies in
schools of engineering and business in two universities
and Amazon’s Mechanical Turk crowdsourcing service
that has been used in security-related research work [26].
Turkers who had ﬁnished more than 50 tasks and had
an approval rate greater than 60% were qualiﬁed for our
user study.

For registration, subjects in Dataset-1 were asked to
provide their student IDs for a simple veriﬁcation af-
ter which they were guided to upload a picture, regis-
ter a password and then use the password to access class
materials including slides, homework, assignments, and
projects. Subjects used this system for the Fall 2012
semester which lasted three and a half months at our
university. If subjects forgot their passwords during the
semester, they would inform the teaching assistant who
reset their passwords. Subjects were allowed to change
their passwords by clicking a change password link af-
ter login. There were 56 subjects involved in Dataset-1
resulting in 58 unique pictures, 86 registered passwords,
and 2,536 login attempts.

Instead of asking subjects to upload pictures for
Dataset-2, we chose 15 pictures (please refer to Ap-
pendix B for the pictures) in advance from the PAS-
CAL Visual Object Classes Challenge 2007 dataset [19].
We chose these pictures because they represent a diverse
range of pictures in terms of category (portrait, wedding,
party, bicycle, train, airplane and car) and complexity
(pictures with few and plentiful stand-out regions). Sub-
jects were asked to choose one password for each pic-
ture by pretending that it was protecting their bank in-
formation. The 15 pictures were presented to subjects in
a random order to reduce the dependency of password
selection upon the picture presentation order. 762 sub-
jects participated in the Dataset-2 collection resulting in
10,039 passwords. The number of passwords for each
picture in the Dataset-2 varies slightly, with an average
of 669, because some subjects quit the study without set-
ting up passwords for all pictures.

For both datasets, subjects were asked to ﬁnish the
aforementioned questionnaire to help us understand their
experiences. We collected 685 (33 for Dataset-1, 652 for
Dataset-2) copies of survey answers in total. According
to the demographic-related inquiries in the exit survey,
81.8% subjects in Dataset-1 are self-reported male and
63.6% are between 18 and 24 years old. While partic-

USENIX Association  

22nd USENIX Security Symposium  385

3

Table 1: Survey Question: Which of the following best
describes what you are considering when you choose lo-
cations to perform gestures?

Multi-choice Answers

I try to ﬁnd locations where special
objects are.
I try to ﬁnd locations where some spe-
cial shapes are.
I try to ﬁnd locations where colors are
different from their surroundings.
I
randomly choose a location to
draw without thinking about the back-
ground picture.

1
24
(72.7%)
8
(24.2%)
0
(0%)
1
(3.0%)

Dataset
2
389
(59.6%)
143
(21.9%)
57
(8.7%)
66
(10.1%)

Overall
413
(60.3%)
151
(22.1%)
57
(8.3%)
67
(9.8%)

ipants in Dataset-2 are more diverse with 64.4% male,
37.2% among 18 to 24 years old, 45.4% among 25 - 34,
and 15.0% among 35 - 50. Even though the subjects in
our studies do not represent all possible demographics,
the data collected from them represents the most com-
prehensive PGA usage so far. Their tendencies could
provide us with signiﬁcant insights into the user choice
in PGA.
3.2 Results
This section summarizes our empirical analysis on the
above-mentioned datasets by presenting ﬁve ﬁndings.

3.2.1 Finding 1: Relationship Between Background
Picture and User’s Identity, Personality, or In-
terests

We analyzed all unique pictures3 in Dataset-1, and
the background pictures chosen by subjects range from
celebrity to system screenshot. We categorize them into
six classes:
ii) civilization (7/58),
iii) landscape (3/58),
iv) computer-generated picture
(14/58), v) animals (6/58), and vi) others (1/58).

i) people (27/58),

For the category of ‘people’, 6 pictures were catego-
rized as ‘me’; 12 pictures were subjects’ families; 4 were
pictures of subjects’ friends; and 5 were celebrities. The
analysis of answers to the survey question “Could you ex-
plain why you choose such types of pictures?” revealed
two opposite attitudes towards using picture of people.
The advocates for such pictures considered: i) it is more
friendly. e.g. “The image was special to me so I enjoy
seeing it when I log in”; ii) it is easier for remembering
passwords. e.g. “Marking points on a person is easier to
remember”; and iii) it makes password more secure. e.g.
“The picture is personal so it should be much harder for
someone to guess the password”. However, other partic-
ipants believed it may leak his or her identity or privacy.
e.g. “revealing myself or my family to anyone who picks
up the device”. They preferred other types of pictures

3Due to the conﬁdentiality agreement with the subjects, we are not
able to share pictures that are marked having personally identiﬁable
information.

Table 2: Attributes of Most Frequently Used PoIs

Attributes

# Gesture

# Password

# Subject

Eye
Nose

Hand/Finger

Jaw

Face (Head)

36
21
6
5
4

20
13
5
3
2

19
10
4
3
2

because “less personal if someone gets my picture” and
“landscape usually doesn’t have any information about
who you are”.

14 pictures in Dataset-1 could be categorized as
computer-generated pictures including computer game
posters, cartoons, and some geometrical graphs. 24.1%
(14/58) of such pictures were observed in Dataset-1 but
the survey results indicated 6.4% (42/652) of partici-
pants were in such a usage pattern in Dataset-2 based
on the following survey question: “Please indicate the
type of pictures you prefer to use as the background”.
We concluded the population characteristics (male, age
18-24, college students) in Dataset-1 were the major rea-
son behind this phenomenon. The answers to “Could
you explain why you choose such types of pictures?” in
Dataset-1 supported this conjecture: “computer game is
something I am interested [in] it” and “computer games
picture is personalized to my interests and enjoyable to
look at”.

It is obvious that pictures with personally identiﬁable
information may leak personal information. However, it
is less obvious that even pictures with no personally iden-
tiﬁable information may provide some clues which may
reveal the identity or persona of a device owner. Tra-
ditional text-based password does not have this concern
as long as the password is kept secure. Previous graph-
ical password schemes, such as Face and PassPoints, do
not have this concern either because pictures are selected
from a predeﬁned repository.

3.2.2 Finding 2: Gestures on Points of Interest

The security of background draw-a-secret schemes
mostly relies on the location distribution of users’ ges-
tures.
It is the most secure if the locations of users’
gestures follow a uniform distribution on any picture.
However, such passwords would be difﬁcult to remem-
ber and may not be preferable by users. By analyz-
ing the collected passwords, we notice that subjects fre-
quently chose standout regions (points of interest, PoIs)
on which to draw. As shown in Table 1, only 9.8% sub-
jects claimed to choose locations randomly without car-
ing about the background picture. The observation is
supported by survey answers to “Could you explain the
way you choose locations to perform gestures?”: “If I
have to remember it; it [would] better stand out.” and
“Something that would make it easier to remember”.

Even though the theoretical password space of PGA is

386  22nd USENIX Security Symposium 

USENIX Association

4

Table 3: Numbers of Gesture Type Combinations and Average Time Spent on Creating Them
2×c+l
N/A
442
10.19

2×t+c
21.56
380
6.14

2×c+t
N/A
192
8.78

2×l+c
17.51
622
9.98

7

2×l+t
11.17
1000
7.72

9

2×t+l
10.12
1211
6.02

3×l
3
12.39
1447
7.11

3×t
60
5.74
3438
4.33

Average Time (Seconds)

1

1

0

0

3×c
0
N/A
253
9.96

Dataset-1

Dataset-2

#

#

Average Time (Seconds)

t+l+c

5

11.22
1054
9.37

Table 4: Numbers of Gesture-order Patterns

Dataset-1

Dataset-2

H+
43
50.0%
3144
31.3%

H-
5
5.8%
1303
12.9%

V+
16
18.6%
1479
14.7%

V-
4
4.6%
887
8.8%

DIAG Others
22
25.5%
2621
26.1%

18
20.9%
3326
33.1%

larger than text-based passwords with the same length,
a background picture affects user choice in gesture loca-
tion, reducing the feasible password space tremendously.
We summarize three popular ways that subjects used to
identify standout regions: i) ﬁnding regions with objects.
e.g. “I chose eyes and other notable features” and “I
chose locations such as nose, mouth or whole face”; ii)
ﬁnding regions with remarkable shapes. e.g. “if there is
a circle there I would draw a circle around that”; and
iii) ﬁnding regions with outstanding colors. The detailed
distribution of these selection processes is shown in Ta-
ble 1. 60.3% of subjects prefer to ﬁnd locations where
special objects catch their eyes while 22.1% of subjects
would rather draw on some special shapes.
3.2.3 Finding 3: Similarities Across Points of Inter-

est

We analyzed the attributes of PoIs that users preferred to
draw on. We paid more attention to the pictures of people
because it was the most popular category. In the 31 regis-
tered passwords for the 27 pictures of people uploaded by
22 subjects in Dataset-1, we analyzed the patterns of PoI
choice. As shown in Table 2, 36 gestures were drawn on
eyes and 21 gestures were drawn on noses. Other loca-
tions that attracted subjects to draw included hand/ﬁnger,
jaw, face (head), and ear. Interestingly, 19 subjects out of
22 (86.3%) drew on eyes at least once, while 10 subjects
(45.4%) performed gestures on noses. The tendencies
to choose similar PoIs by different subjects are common
in other picture categories as well. Figure 1 shows an-
other example where two subjects uploaded two versions
of Starry Night in Dataset-1. The passwords they chose
show strikingly similar patterns with three taps on stars,
even if there is no single gesture location overlap.
3.2.4 Finding 4: Directional Patterns in PGA Pass-

word

Salehi-Abari et al. [32] suggest many passwords in click-
based systems follow some directional patterns. We are
interested in whether PGA passwords show similar char-
acteristics. For simplicity, we consider the coordinates of
tap and circle gestures as their locations and the middle









Figure 1: Two Versions of Starry Night and Correspond-
ing Passwords

point of the starting and ending points of line as its loca-
tion. If the x or y coordinate of a gesture sequence fol-
lows a consistent direction regardless of the other coor-
dinate, we say the sequence follows a LINE pattern. We
divide LINE patterns into four categories: i) H+, denot-
ing left-to-right (xi ≤ xi+1); ii) H-, denoting right-to-left
(xi ≥ xi+1); iii) V+, denoting top-to-bottom (yi ≤ yi+1);
and iv) V-, denoting bottom-to-top (yi ≥ yi+1). If a se-
quence of gestures follows a horizontal pattern and a ver-
tical pattern at the same time, we say it follows a DIAG
pattern.

We examined the occurrence of each LINE and DIAG
pattern in the collected data. As shown in Table 4,
more than half passwords in both datasets exhibited some
LINE patterns, and a quarter of them exhibited some
DIAG patterns. Among four LINE patterns, H+ (drawing
from left to right) was the most popular one with 50.0%
and 31.3% occurrences in Dataset-1 and Dataset-2, re-
spectively. And, V+ (drawing from top to bottom) was
the second most popular with 18.6% and 14.7% occur-
rences in two datasets, respectively. This ﬁnding shows it
is reasonable to use gesture-order patterns as one heuris-
tic factor to prioritize generated passwords.

3.2.5 Finding 5: Time Disparity among Different

Combinations of Gesture Types

We analyzed all registered passwords to understand the
gesture patterns and the relationship between gesture
type and input time. For 86 registered passwords (258
gestures) in Dataset-1, 212 (82.1%) gesture types were
taps, 39 (15.1%) were lines, and only 7 (2.7%) were cir-
cles. However, the corresponding occurrences for 10,039
registered passwords (30,117 gestures) in Dataset-2 were
15,742 (52.2%), 10,292 (34.2%), and 4,083 (13.5%), re-
spectively. Obviously, subjects in Dataset-2 chose more
diverse gesture types than subjects in Dataset-1. As
shown in Table 3, there was a strong connection between
the time subjects spent on reproducing passwords and

USENIX Association  

22nd USENIX Security Symposium  387

5

the gesture types they chose. Three taps, the most com-
mon gesture combination, appeared in both datasets with
the lowest average time (5.74 seconds and 4.33 seconds
in corresponding dataset). On the other hand, the pass-
words with two circles and one line took the longest av-
erage input time (10.19 seconds in Dataset-2).
In the
user studies, subjects in Dataset-2 were asked to set up
the passwords by pretending they were protecting their
bank information. However, subjects in Dataset-1 actu-
ally used these passwords to access the class materials
which they accessed more than four times a week on av-
erage. This may be a reason why subjects in Dataset-1
prefer passwords with simpler gesture type combinations
that are easier to reproduce in a timely manner.
4 Attack Framework

In this section, we present an attack framework on Win-
dows 8TM picture gesture authentication, leveraging the
ﬁndings addressed in Section 3. Our attack framework
takes the target picture’s PoIs, a set of learning pictures’
PoIs and corresponding password pairs as input, and pro-
duces a list of possible passwords, which is ranked in the
descending order of the password probabilities.

Next, we ﬁrst discuss the attack models followed by
the representations of picture password and PoI. We then
illustrate the idea of a selection function and its auto-
matic identiﬁcation. We also present two algorithms for
generating a selection function sequence list and describe
how it can generate picture password dictionaries for pre-
viously unseen target pictures.

4.1 Attack Models
Depending on the resources an attacker possesses, we ar-
ticulate three different attack models: i) Pure Brute-force
Attack: an attacker blindly guesses the picture password
without knowing any information of the background pic-
ture and the users’ tendencies. The password space in
this model is 230.1 in PGA [29]. ii) PoI-assisted Brute-
force Attack: an attacker assumes the user only performs
drawings on PoIs of the background picture and this
model randomly guesses passwords on identiﬁed PoIs.
The password space for a picture with 20 PoIs in this
model is 227.7 [29]. Salehi-Abari et al. [32] designed an
approach to automatically identify hot-spots in a picture
and generate passwords on them. iii) Knowledge-based
PoI-assisted Attack:
in addition to the assumption for
PoI-assisted brute-force attack, an attacker ought to have
some knowledge about the password patterns learned
from collected picture and password pairs (not necessar-
ily from the target user or picture). The guessing space
in this model is the same as the one in PoI-assisted brute-
force attack. However, the generated dictionaries in this
model are ranked with the higher possibility passwords

on the top of the list.

Attack schemes could also be divided into two cate-
gories based on whether or not an attacker has the ability
to attack previously unseen pictures. The method pre-
sented in [32] is able to attack previously unseen pic-
tures for click-based graphical password. It uses click-
order heuristics to generate partially ranked dictionar-
ies. However, this approach cannot be applied directly to
background draw-a-secret schemes because the gestures
allowed in such schemes are much more complex and
the order-based heuristics could not capture users’ selec-
tion processes accurately. In contrast, our attack frame-
work could abstract generic knowledge of user choice
in picture password schemes. In addition, as a working
knowledge-based PoI-assisted model, it is able to gener-
ate ranked dictionaries for previously unseen pictures.

4.2 Password and PoI Representations
We ﬁrst formalize the representation of a password in
PGA with the deﬁnition of a location-dependent gesture
which represents a single gesture on some locations in a
picture.

Deﬁnition 1 A location-dependent gesture (LdG) de-
noted as π is a 7-tuple ⟨g,x1,y1,x2,y2,r,d⟩ that consists
of gesture’s type, location, and other attributes.
In this deﬁnition, g denotes the type of LdG that must
be one of tap, line, and circle. A tap LdG is further rep-
resented by the coordinates of a gesture ⟨x1,y1⟩. A line
LdG is denoted by the coordinates of the starting and
ending points of a gesture ⟨x1,y1⟩ and ⟨x2,y2⟩. A circle
LdG is denoted by the coordinates of its center ⟨x1,y1⟩,
radius r, and direction d ∈{+,−} (clockwise or not). We
deﬁne the password space of location-dependent gesture
as Π = Πtap ∪Πline ∪Πcircle. A valid PGA password is

a length-three sequence of LdGs denoted as ⃗π, and the
PGA password space could be denoted as ⃗Π.

A point of interest is a standout region in a picture.
PoIs could be regions with semantic-rich meanings,
such as face (head), eye, car, clock, etc. Also, they
could stand out in terms of their shapes (line, rectangle,
circle, etc.)
or colors (red, green, blue, etc.). We
denote a PoI by the coordinates of its circumscribed
rectangle and some describing attributes. A PoI is a
5-tuple ⟨x1,y1,x2,y2,D⟩, where ⟨x1,y1⟩ and ⟨x2,y2⟩
are the coordinates of the top-left and bottom-right
points of the circumscribed rectangle, and D ⊆ 2D
is a set of attributes that describe this PoI. D has
three sub-categories Do, Ds and Dc and four wildcards
∗o,∗s,∗c, and ∗, where Do = {head, eye, nose, ...},
Ds = {line, rectangle, circle, ...},
and Dc =
{red, blue, yellow, ...}. Wildcards are used when
no speciﬁc information is available. For example, if a
PoI is identiﬁed with objectness measure [3] that gives

388  22nd USENIX Security Symposium 

USENIX Association

6









Gesture 1: Circle my father’s head

Gesture 2: Connect my little 
sister’s nose to my older sister’s 
nose

Gesture 3: Tap my mother’s nose


LdGSF 1: Circle a head

i.e., (circle, {head}, Ф)

LdGSF 2: Line two noses

i.e., (line, {nose}, {nose})

LdGSF 3: Tap a nose

i.e., (tap, {nose}, Ф)

(b)

(a)
Figure 2:
(a) Background picture and password (b)
User’s selection processes that were taken from [30]
(c) Corresponding LdGSFs that simulate user’s selection
processes

(c)

no semantics about the identiﬁed region, we mark the
PoI’s describing attribute as ∗.
4.3 Location-dependent Gesture Selection

Functions

in our

A key concept
framework is the location-
dependent gesture selection function (LdGSF) which
models and simulates the ways of thinking that users go
through when they select a gesture on a picture. The
motivation behind this abstraction is that the set of PoIs
and their locations differ from picture to picture, but the
ways that users think to choose locations for drawing a
gesture exhibit certain patterns. This conjecture is sup-
ported by our observations from collected data and sur-
veys discussed in Section 3. With the help of LdGSF,
the PoIs and corresponding passwords in training pic-
tures are used to generalize picture-independent knowl-
edge that describes how users choose passwords.

Deﬁnition 2 A location-dependent gesture selection
function (LdGSF) is a mapping s : G×2D ×2D ×Θ→ 2Π
which takes a gesture, two sets of PoI attributes, and a set
of PoIs in the learning picture as input to produce a set
of location-dependent gestures.

The universal set of LdGSF is deﬁned as S. A
length-three sequence of LdGSF is denoted as ⃗s, and a
set of length-three LdGSF sequences is denoted as ⃗S.
s(tap,{red, apple},∅,θk) is interpreted as ‘tap a red ap-
ple in the picture pk’ and s(circle,{head},∅,θk) as ‘cir-
cle a head in pk’. Note that, no speciﬁc information of
the locations of ‘red apple’ and ‘head’ is provided here
which makes the representations independent from ac-
tual locations of objects in the picture.

One challenge we face is some PoIs may be big
enough to take several unique gestures. Let us consider
a picture with a big car image in it. Simply saying ‘tap
a car’ could result in lots of distinct tap gestures in the
circumscribed rectangle of the car. One solution to this
problem is to divide the circumscribed rectangle into a
grid with the scale of toleration threshold. However, this
solution would result in too many password entries in
the generated dictionary. For simplicity, we introduce
ﬁve inner points for one PoI, namely center, top, bot-
tom, left, and right that denote the center of the PoI and







(a)





PoI 1: <4, 23, 21, 46, {head}>
PoI 2: <23, 3, 43, 28, {head}>
PoI 3: <46, 19, 63, 43, {head}>
PoI 4: <71, 12, 90, 35, {head}>
PoI 5: <13, 33, 18, 37, {nose}>
PoI 6: <32, 17, 34, 19, {nose}>
PoI 7: <51, 31, 56, 35, {nose}>
PoI 8: <76, 24, 81, 28, {nose}>

  ...

(b)

LdG 1: <circle, 33, 15, 0, 0, 9,  >

LdG 2: <line, 54, 34, 79, 27, 0, 0>

LdG 3: <tap, 16, 35, 0, 0, 0, 0>

(c)

Figure 3: (a) Background picture and identiﬁed PoIs (b)
Identiﬁed PoIs (c) Password representations (Colors are
used to indicate the connections between the PoIs in (b)
and LdGs in (c))

four points of the center of two consecutive corners. Any
gesture that falls into the proximities of these ﬁve points
of a PoI would be considered as an action on this PoI.
For some PoIs that are big enough to take an inner line
gesture, we put ∅ as the input of the second set of PoI
attributes. s(line,{mouth},∅,θk) denotes ‘line from the
left(right) to the right(left) on the same mouth’. While,
s(line,{mouth},{mouth},θk) means ‘connect two dif-
ferent mouths’.
Figure 2 shows an example demonstrating how
LdGSF simulates a user’s selection processes that were
taken from [30]. In reality, a user’s selection process on
a PoI and gesture selection may be determined by some
subjective knowledge and cognition. For example, ‘cir-
cle my father’s head’ and ‘tap my mother’s nose’ may
involve some undecidable computing problems. One so-
lution to handle this issue is to approximate subjective
selection processes in objective ways by including some
modiﬁers. ‘circle my father’s head’ may be transformed
into ‘circle the uppermost head’ or ‘circle the biggest
head’. However, it is extremely difﬁcult, if not impossi-
ble, to accurately approximate subjective selection pro-
cesses in this way, and it may bring serious over-ﬁtting
problems in the learning stage.
Instead, we choose to
ignore subjective information by abstracting ‘circle my
father’s head’ to ‘circle a head’. A drawback of this
abstraction is that an LdGSF may return more than one
LdG and we have no knowledge to rank them directly, as
they come from the same LdGSF. Using Figure 2(a) as
an example, ‘circle a head’ outputs four different LdGs
on each head in the picture. The LdGSF sequence shown
in Figure 2(c) generates 4× (4×3)×4 = 192 passwords.
To cope with this issue, we use gesture-order to rank
the passwords generated by the same LdGSF sequence
that will be detailed in Section 4.5. Next, we present an
automated approach to extract users’ selection processes
from the collected data and represent them with LdGSFs.
Figure 3 shows an example demonstrating that how
to extract users’ selection processes from PoIs automat-
ically. First, PoIs in the background picture are iden-
tiﬁed using mature computer vision techniques such as
object detection, feature detection and objectness mea-
sure. Then, each LdG in a password is compared with

USENIX Association  

22nd USENIX Security Symposium  389

7

PoIs based on their coordinates and sizes. If a match be-
tween PoIs and LdGs is found, a new LdGSF is created
as the combination of the LdG’s gesture type and PoI’s
attributes. For instance, the location and size of LdG 1 in
Figure 3(c) matches PoI 2 in Figure 3(b) (the locations
of the circle gesture and PoI center are compared ﬁrst;
then, the radius of the circle is compared with 1/2 of PoI’s
height and width). Then, an LdGSF s(circle,{head},∅)
is created which is equivalent to the LdG shown in Fig-
ure 2(c).

To choose a password in PGA, the user selects a
length-three LdGSF sequence. With the deﬁnition of
LdGSF, the generation of ranked password list is simpli-
ﬁed into the generation of the ranked LdGSF sequence
list. Let order: ⃗S → {1..|⃗S|} be a bijection which indi-
cates the order LdGSF sequences should be performed.
The objective of generating ranked LdGSF sequence list
is to ﬁnd such a bijection.

4.4 LdGSF Sequence List Generation and

Ordering

Now we present our approach to ﬁnd the aforementioned
bijection that indicates the order that the LdGSF se-
quences should be performed on a target picture for gen-
erating the password dictionary. Our framework is not
dependent on certain rules, but is adaptive to the tenden-
cies shown by users who participate in the training set.
The characteristic of adaptiveness helps our framework
generate dedicated guessing paths for different training
data. Next, we present two algorithms for obtaining such
a feature.

4.4.1 BestCover LdGSF Sequence List Generation

We ﬁrst propose an LdGSF sequence list genera-
tion algorithm named BestCover that is derived from
Bemts [44]. The objective of BestCover LdGSF se-
quence list generation is to optimize the guessing order
for the sequences in the list by minimizing the expected
number of sequences that need to be tested on a random
choice of picture in the training dataset.

The problem is formalized as follows: Instance: The
collection of LdGSF sequences ⃗s1, ...,⃗sn and correspond-
ing picture password ⃗π1,...,⃗πn, for which ⃗si(θi) ∋ ⃗πi,i ∈
{1..n} and θ1, ..,θn are the sets of PoIs in pictures
p1, .., pn. Question: Expected Min Selection Search
(emss): The objective is to ﬁnd order so as to minimize
E(min{i : ⃗si(θr) ∋ ⃗πr}, where ⃗si = order−1(i) and the
expectation is taken with respect to a random choice of
r ← {1..n}.
The hardness of this problem is that different LdGSFs
and LdGSF sequences may generate the same list of
LdGs and passwords. For instance, ‘tap a red object’
and ‘tap an apple’ turn out the same result on a picture

in which there is a red apple. An overlap in different
LdGSF results is similar to the coverage characteristics
in the set cover problem. We can prove the NP-hardness
of emss by reducing from emts [44]. Due to space lim-
itations, we omit the corresponding proof. We give an
approximation algorithm for emss in Algorithm 1 that is
a modiﬁcation from Bmssc [20]. The time complexity of
BestCover is O(n2 +|⃗S′|log(|⃗S′|)).
Algorithm 1: BestCover((⃗s1, ..,⃗sn),(⃗π1,...,⃗πn))
for i = 1..n do

T⃗si ← {k : ⃗si(θk) ∋ ⃗πk};

end
⃗S′ ← {⃗s : |T⃗s| > 0};
for i = 1..|⃗S′| do

order−1(i)← ⃗sk, that T⃗sk has most elements that are not
included in∪i′<iorder−1(i′);

end
return order

BestCover is good for a training dataset that consists
of comprehensive and large scale password samples, be-
cause it assumes the target passwords exhibit same or at
least very similar distributions to the training data. How-
ever, if the training dataset is small and biased, the results
from BestCover may over-ﬁt the training data and fail in
testing data.

4.4.2 Unbiased LdGSF Sequence List Generation

The over-ﬁtting problem in BestCover is brought about
by the biased PoI attribute distributions in training data.
For example, we have a training set with 9 pictures of
apples and 1 picture of a car, and 5 corresponding pass-
words have circles on apples and 1 has a circle on car. In
the generated LdGSF sequence list, BestCover will put
sequences with ‘circle an apple’ prior to the ones with
‘circle a car’, because the former ones have an LdGSF
that was used in more passwords. However, we can see
the probability for users to circle car (1/1) is higher than
apples (5/9) if we consider the occurrences of apple and
car in pictures.

Unbiased LdGSF sequence list generation copes with
this issue by considering the PoI attribute distributions. It
removes the biases from the training dataset by normal-
izing the occurrences of LdGSFs with the occurrences of
their corresponding PoIs. Let D⃗sk ⊆ θ denote the event
that θ contains enough PoIs that have attributes speciﬁed
in ⃗sk. If a PoI with a speciﬁc type of attributes does not
exist in a picture, the probability that a user select the PoI
with such an attribute on this picture to draw a password
is 0, denoted as Pr(⃗sk|D⃗sk ⊆ θ) = 0, e.g. a user would not
think and perform ‘tap a red apple’ on a picture without
the existence of the red apple. We assume each LdGSF
in a sequence is independent of each other and approxi-
mately compute Pr(⃗sk|D⃗sk ⊆ θ) with Equation 1.

390  22nd USENIX Security Symposium 

USENIX Association

8

Pr(⃗sk|D⃗sk ⊆ θ)
= Pr(s1s2s3|Ds1 ⊆ θ∧ Ds2 ⊆ θ∧ Ds3 ⊆ θ)
= Pr(s1|Ds1 ⊆ θ)× Pr(s2|Ds2 ⊆ θ)× Pr(s3|Ds3 ⊆ θ)
(1)
For each si ∈ S, we compute Pr(si|Dsi ⊆ θ) with Equa-
tion 2:

(2)

∑n
∑n

j=1 count(Dsi , ⃗πj)
j=1 count(Dsi,θj)

Pr(si|Dsi ⊆ θ) =
where ∑n
j=1 count(Dsi , ⃗πj) denotes the number of LdGs
in passwords of the training set that share the same
attributes with si, and ∑n
j=1 count(Dsi,θj) denotes the
number of PoIs in the training set that share the same
attributes with si. Pr(si|Dsi ⊆ θ) describes the probabil-
ity of using a certain LdGSF when there are enough PoIs
with the required attributes.

The Unbiased algorithm generates an LdGSF se-
quence list by ranking Pr(⃗sk|D⃗sk ⊆ θ) instead of Pr(⃗sk)
in descending order as shown in Algorithm 2. The time
complexity of Unbiased is O(n|S|+|⃗S|log(|⃗S|)). The Un-
biased algorithm would be better for the scenarios where
fewer samples are available or samples are highly biased.

Algorithm 2: Unbiased(S)
for s ∈ S do
end
for ⃗s ∈ ⃗S do
end
for i = 1..|⃗S| do

Compute Pr(s|Ds ⊆ θ) with Equation 2;

Compute Pr(⃗s|D⃗s ⊆ θ) with Equation 1;

order−1(i)← ⃗sk, that Pr(⃗sk|D⃗sk ⊆ θ) holds the i-th position
in the descending ordered Pr(⃗s|D⃗s ⊆ θ) list;

end
return order

4.5 Password Dictionary Generation
The last step in our attack framework is to generate the
password dictionary for a previously unseen target pic-
ture. First, the PoIs in the previously unseen picture are
identiﬁed. Then, a dictionary is acquired by applying
the LdGSF sequences on the PoIs, following the order
created by the BestCover or Unbiased algorithm. Obvi-
ously, the passwords generated by an LdGSF sequence
that holds a higher position in the LdGSF sequence list
will also be in higher positions in the dictionary. How-
ever, as addressed earlier, BestCover and Unbiased al-
gorithms do not provide extra information to rank the
passwords generated by the same LdGSF sequence. In-
spired by using the click-order patterns as the heuris-
tics for dictionary generation [32], we propose to rank

9

such passwords generated by the same LdGSF sequence
with gesture-orders. In the training stage, we record the
gesture-order occurrence of each LINE and DIAG pat-
tern and rank the patterns in descending order.
In the
attack stage, for the passwords generated by the same
LdGSF sequence, we reorder them with their gesture-
orders in the order of LINE and DIAG patterns. Pass-
words that do not belong to any LINE or DIAG pattern
hold lower positions.

5 Implementation and Evaluation
5.1 PoI Identiﬁcation
We chose OpenCV [1] as the computer vision framework
for our implementation and collected several feature de-
tection tools for automatically identifying PoIs in back-
ground pictures. The computer vision techniques we
adopted include: i) object detection: the goal of object
detection is to ﬁnd the locations and sizes of semantic
objects of a certain class in a digital image. Viola-Jones
object detection framework [40] is the ﬁrst computation-
ally affordable online object detection framework that
utilizes Haar-like features instead of image intensities.
Each learned classiﬁer is represented and stored as a haar
cascade. We collected 30 proven haar cascades from [31]
for 8 different object classes including face (head), eye,
nose, mouth, ear, head, body, and clock. ii) low-level fea-
ture detection: due to the high positive and high negative
rates of object detection, we also resorted to some low-
level feature detection algorithms that identify standout
regions without extracting semantics. To identify regions
whose colors are different from their surroundings, we
ﬁrst converted the color pictures to black and white, then
found the contours using algorithms in [35]. For the
circle detection, we used Canny edge detector [10] and
Hough transform algorithms [5].
iii) objectness mea-
sure: objectness measure [3] deals with class-generic
object detection. Different from detecting objects in a
speciﬁc class, the objectness measure ﬁnds the locations
and sizes of class-generic objects whose colors and tex-
tures are opposed to the background images. Objectness
measure could be considered as a technique combining
several low-level feature detectors together. We used an
objectness measure library from [2] that is able to locate
objects and give numerical conﬁdence values with its re-
sults.

Figure 4 displays the PoI detection results on four
example pictures in Dataset-2. As we can see in Fig-
ure 4(b), circle detection could identify both bicycle
wheels and car badge, but its false positive rate is a lit-
tle high. Contour detection is the most robust algorithm
with a low false positive rate which could locate regions
whose colors are different as shown in Figure 4(c). Ob-
jectness measure shown in Figure 4(d) could also iden-

USENIX Association  

22nd USENIX Security Symposium  391

A−40

L−15

BestCover P2
BestCover P2
Unbiased P2
Unbiased P2
L−15
BestCover P1
Unbiased P1

A−40

A−40

A−40

0

2

4

Number of Password Guesses (Log−2 Scale)

8

10

12

14

6

(a)

A−40

L−15

BestCover P2
BestCover P2
Unbiased P2
Unbiased P2
L−15
BestCover P1
Unbiased P1

A−40

A−40

A−40

d
e
k
c
a
r
C
 
s
d
r
o
w
s
s
a
P

 
f

o

 

e
g
a

t

n
e
c
r
e
P

50%
45%
40%
35%
30%
25%
20%
15%
10%
 5%
 0%

70%

60%

50%

40%

30%

20%

10%

d
e
k
c
a
r
C
 
s
G
d
L

 
f

o

 

e
g
a

t

n
e
c
r
e
P

 0%

0

2

4

Number of Password Guesses (Log−2 Scale)

8

10

12

14

6

16

18

16

18

(b)

Figure 5: (a) Percentage of passwords cracked vs. num-
ber of password guesses, per condition. (b) Percentage
of LdGs cracked vs. number of password guesses, per
condition. For Dataset-1, there are 86 passwords that
include 258 LdGs. For Dataset-2, there are 10,039 pass-
words that have 30,117 LdGs.

could perform ofﬂine dictionary attacks like cracking
text-based password systems.
In the second scenario,
picture passwords could be used for other purposes be-
sides logging into Windows 8TM, where no constraint on
the number of attempts is enforced. For example, a reg-
istered picture password could be transformed and used
as a key to encrypt a ﬁle. An attacker who acquires the
encrypted ﬁle would like to perform an ofﬂine attack.

In order to attack passwords from a previously unseen
picture, the training dataset excluded passwords from the
target picture. More speciﬁcally, to evaluate Dataset-1
(58 unique pictures), we used passwords from 57 pic-
tures as the training data and attacked the passwords for
the last picture. To evaluate Dataset-2 (15 unique pic-
tures), we used passwords for 14 pictures as training
data, learned the patterns exhibited in the training data,
and generated a password dictionary for the last picture.
The same process was carried out 58 and 15 times for
Dataset-1 and Dataset-2, respectively, in which the tar-
get picture was different in each round. The size of the
dictionary was set as 219 which is 11-bit smaller than the
theoretical password space. We compared all collected
passwords for the target picture with the generated dic-
tionary for the picture, and recorded the number of pass-
word guesses.

The ofﬂine attack results within 219 guesses in differ-
ent settings are shown in Figure 5. There are 86 pass-
words in Dataset-1, which have a total of 258 LdGs.











Figure 4: PoI Identiﬁcation on Example Pictures in
Dataset-2: (a) Original pictures (b) Circle detection with
Hough transform (c) Contour detection (d) Objectness
measure (e) Object detection
tify regions whose colors and textures are different from
their surroundings. Since most haar cascades we used
are designed for facial landmarks, they work smoothly
on portraits as does the second picture in Figure 4(e).
However, the results show relatively high false positive
rates on pictures from other categories. In order to iden-
tify more PoIs as accurate as possible, our approach in
PoI identiﬁcation leveraged two steps. In the ﬁrst step,
all possible PoIs were identiﬁed using different kinds of
tools. In the second step, we examined all identiﬁed PoIs
and removed duplicates by comparing their locations,
sizes and attributes. Then, our approach generated a PoI
set called P1
A-40 for each picture in Dataset-1
and Dataset-2, respectively. Those PoI sets consisted of
at most 40 PoIs with the highest conﬁdences.

A-40 and P2

Since our attack algorithms are independent from the
PoI identiﬁcation algorithms, we are also interested in
examining how our attack framework performs with
ideal PoI annotations for pictures. Besides using the au-
tomated PoI identiﬁcation techniques, we manually an-
notated pictures in Dataset-2 for some outstanding PoIs
as well. To annotate the pictures, we simply recorded the
locations and attributes of at most ﬁfteen most appealing
regions in the pictures without referring to any password
in the collected dataset. We call this annotated PoI set
P2
L-15.
5.2 Attack Evaluation
Ofﬂine Attacks. Due to the introduction of a tolerance
threshold, picture passwords may be more difﬁcult to
store securely compared with text-based passwords that
are normally saved after salted hashing. Even though the
approach that Windows 8TM is adopting to store picture
passwords remains undisclosed, we could consider two
attack scenarios where picture passwords are prone to
ofﬂine attacks. In the ﬁrst scenario, all passwords which
fall into the vicinity (deﬁned by the threshold) of cho-
sen passwords could be stored in a ﬁle with salted hashes
for comparison. An attacker who has access to this ﬁle

392  22nd USENIX Security Symposium 

USENIX Association

10

d
e
k
c
a
r
C
 
s
d
r
o
w
s
s
a
P

 
f

o

 

e
g
a

t

n
e
c
r
e
P

A−40

L−15

BestCover P2
BestCover P2
Unbiased P2
Unbiased P2

A−40

L−15

25%

20%

15%

10%

 5%

 0%

0

2

4

Number of Password Guesses (Log−2 Scale)

8

10

12

14

6

d
e
k
c
a
r
C
 
s
d
r
o
w
s
s
a
P

 
f

o

 

e
g
a

t

n
e
c
r
e
P

25%

20%

15%

10%

 5%

 0%

0

2

16

18

A−40

L−15

BestCover P2
BestCover P2
Unbiased P2
Unbiased P2

A−40

L−15

4

Number of Password Guesses (Log−2 Scale)

8

10

12

14

6

(a)

(a)

50%

40%

30%

20%

10%

d
e
k
c
a
r
C
 
s
G
d
L

 
f

o

 

e
g
a

t

n
e
c
r
e
P

 0%

0

2

A−40

L−15

BestCover P2
BestCover P2
Unbiased P2
Unbiased P2

A−40

L−15

4

Number of Password Guesses (Log−2 Scale)

8

10

12

14

6

50%

40%

30%

20%

10%

d
e
k
c
a
r
C
 
s
G
d
L

 
f

o

 

e
g
a

t

n
e
c
r
e
P

16

18

 0%

0

2

A−40

L−15

BestCover P2
BestCover P2
Unbiased P2
Unbiased P2

A−40

L−15

4

Number of Password Guesses (Log−2 Scale)

8

10

12

14

6

16

18

16

18

(b)

(b)

Figure 6: (a) Percentage of passwords cracked vs. num-
ber of password guesses, per condition. (b) Percentage of
LdGs cracked vs. number of password guesses, per con-
dition. Only the ﬁrst chosen password by each subject in
Dataset-2 was considered. There are 762 passwords that
have 2,286 LdGs.

Figure 7: (a) Percentage of passwords cracked vs. num-
ber of password guesses, per condition. (b) Percentage of
LdGs cracked vs. number of password guesses, per con-
dition. Only passwords for pictures 243, 1116, 2057,
4054, 6467, and 9899 were considered. There are 4,003
passwords that have 12,009 LdGs.

And 10,039 passwords were collected in Dataset-2, con-
taining a total of 30,117 LdGs. For Dataset-1, Best-
Cover cracks 42 (48.8%) passwords out of 86 while Un-
biased cracks 40 (46.5%) passwords for the same dataset
with P1
A-40. For Dataset-1, 178 LdGs (68.9%) out of
258 are cracked with Unbiased and 171 (66.2%) are bro-
ken with BestCover. On the other hand, Unbiased with
P2
L-15 breaks 2,953 passwords (29.4%) out of 10,039 for
Dataset-2. This implies Unbiased with P2
A-40 cracking
2,418 passwords (24.0%) is the best result for all purely
automated attacks on Dataset-2. As Figure 5 suggests,
BestCover outperforms Unbiased slightly when ample
training data is available. The better performance of both
algorithms on Dataset-1 is because the password gesture
combinations in Dataset-1 are relatively simpler than the
ones in Dataset-2 as we discussed in Section 3.2.5.

In Dataset-2, subjects may not choose all 15 pass-
words with the same care as they were eager to ﬁnish
the process. To reduce this effect, we ran another analy-
sis in which only the ﬁrst chosen password by each sub-
ject was considered. There are 762 passwords that have
2,286 LdGs. Like previous analysis, the training dataset
excluded passwords from the target picture. As shown in
Figure 6, results of this analysis are not as good as pre-
vious ones. Unbiased with P2
L-15 breaks 160 passwords
(21.0%) out of 762. Unbiased with P2
A-40 cracking 123
passwords (16.1%). BestCover cracks 108 (14.2%) and
116 (15.2%) with P2

A-40, respectively.

L-15 and P2

Since some pictures in Dataset-2 are similar, we ran an
additional analysis in which only passwords for pictures
243 (airplane), 1116 (portrait), 2057 (car), 4054 (wed-
ding), 6467 (bicycle), and 9899 (dog) were considered.
There are 4,003 passwords that have 12,009 LdGs. Un-
biased with P2
L-15 breaks 1,147 passwords (28.6%) while
803 passwords (20.1%) are cracked by Unbiased with
P2
A-40. BestCover cracks 829 (20.7%) and 875 (21.8%)
with P2
A-40 respectively. Results of this anal-
ysis are not as good as results with passwords from all
pictures.

L-15 and P2

Online Attacks. The current Windows 8TM allows
ﬁve failure attempts before it forces users to enter their
text-based passwords. Therefore, breaking a password
under ﬁve guesses implies the feasibility for launching
an online attack. Figure 8 shows a reﬁned view of the
number of passwords and LdGs cracked with the ﬁrst
ﬁve guesses per condition. Purely automated attack Un-
biased with P2
A-40 breaks 83 passwords (0.8%) with the
ﬁrst guess and cracks 94 passwords (0.9%) within the
ﬁrst ﬁve guesses, while BestCover with P2
A-40 cracked
20 passwords (0.2%) for the ﬁrst guess and 38 pass-
words (0.4%) within ﬁve guesses. Additionally, Unbi-
ased with P2
A-40 breaks 1,723 LdGs (5.7%) with the ﬁrst
guess. With the help of manually labeled PoI set P2
L-15,
the results are even better. For example, Unbiased breaks
195 passwords (1.9%) for the ﬁrst guess and 266 (2.6%)
within the ﬁrst ﬁve guesses.
In the meantime, Unbi-

USENIX Association  

22nd USENIX Security Symposium  393

11

d
e
k
c
a
r
C
 
s
d
r
o
w
s
s
a
P

 
f

o

 
r
e
b
m
u
N

500

450

400

350

300

250

200

150

100

50

0

A−40

L−15

BestCover P2
BestCover P2
Unbiased P2
Unbiased P2

A−40

L−15

A−40

L−15

BestCover P2
BestCover P2
Unbiased P2
Unbiased P2

A−40

L−15

7000

6000

5000

4000

3000

2000

1000

d
e
k
c
a
r
C
 
s
G
d
L

 
f

o

 
r
e
b
m
u
N

1
3
Number of Password Guesses

2

0
1
3
Number of Password Guesses

2

(a)

(b)

Figure 8: (a) Number of passwords cracked within ﬁve
guesses, per condition.
(b) Number of LdGs cracked
within ﬁve guesses, per condition.

 

d
e
k
c
a
r
C
s
d
r
o
w
s
s
a
P

 
f
o
 
r
e
b
m
u
N
e
g
a
r
e
v
A

 

2500

2000

1500

1000

500

0

60
600
~9400

BestCover

Unbiased

(a)

 

d
e
k
c
a
r
C
s
G
d
L
 
f
o
 
r
e
b
m
u
N
e
g
a
r
e
v
A

 

16000

14000

12000

10000

8000

6000

4000

2000

0

60
600
~9400

BestCover

Unbiased

(b)

Figure 9: (a) Average number of passwords cracked vs.
different training data sizes. (b) Average number of LdGs
cracked vs. different training data sizes. P2
A-40 is used for
this analysis. Average over 3 analyses, with one standard
deviation shown.

ased with P2
guess and 4,090 LdGs (13.5%) with ﬁve guesses.

L-15 breaks 3,022 LdGs (10.0%) with the ﬁrst

Effects of Training Data Size. In Figure 9, we show
the password and LdG cracking results with different
sizes of training datasets. For each algorithm, we used
P2
A-40 as the PoI set and performed three analyses with 60,
600, and all available passwords (about 9,400) as train-
ing data, respectively. The sizes of 60 and 600 represent
two cases: i) a training set (60) is ten times smaller than
the target set (about 669); and ii) a training set (600) is
almost the same size as the target set (about 669). For
training datasets with the sizes of 60 and 600, we ran-
domly selected these training passwords and performed
each analysis three times to get the averages and standard
deviations.

As Figure 9 shows, BestCover with 60 training sam-
ples could only break an average of 888 passwords
(8.8%) out of 10,039. And the standard deviation is as
strong as 673. While Unbiased with 60 training sam-
ples can crack 2,352 passwords (23.4%) that is almost
the same as the results generated from all available train-
ing samples. Also, the standard deviation for three trials
is as low as 62. The results from BestCover with 600
training samples are much better than the counterparts
with 60 training samples. All these observations are ex-
pected as Unbiased could eliminate the biases considered

300

250

200

150

100

50

d
e
k
c
a
r
C
 
s
d
r
o
w
s
s
a
P

 
f
o
 
r
e
b
m
u
N

0

0

300

250

200

150

100

50

d
e
k
c
a
r
C
 
s
d
r
o
w
s
s
a
P

 
f
o
 
r
e
b
m
u
N

0

0

243.jpg
316.jpg

4

2
Number of Password Guesses (Log−2 Scale)

10

12

14

16

6

8

300

250

200

150

100

50

d
e
k
c
a
r
C
 
s
d
r
o
w
s
s
a
P

 
f
o
 
r
e
b
m
u
N

18

0

0

1116.jpg
7628.jpg

4

2
Number of Password Guesses (Log−2 Scale)

10

12

14

16

6

8

(a)

(b)

1358.jpg
3026.jpg
3731.jpg
4054.jpg
5570.jpg

4

2
Number of Password Guesses (Log−2 Scale)

10

12

14

16

6

8

300

250

200

150

100

50

d
e
k
c
a
r
C
 
s
d
r
o
w
s
s
a
P

 
f
o
 
r
e
b
m
u
N

18

0

0

2057.jpg
2840.jpg
6412.jpg
6467.jpg

4

2
Number of Password Guesses (Log−2 Scale)

10

12

14

16

6

8

18

18

(c)

(d)

Figure 10: (a) pictures with fewer PoIs (b) portraits (c)
pictures with people in them (d) pictures with lots of
PoIs. Unbiased algorithm on P2
A-40 is used for this analy-
sis. (Please refer to Appendix B for the pictures).

in BestCover. The results clearly demonstrate the beneﬁt
of using the Unbiased algorithm when a training dataset
is small.

Effects on Different Picture Categories. We mea-
sured the attack results on different picture categories
as shown in Figure 10 where each subﬁgure depicts the
number of passwords cracked versus the number of pass-
word guesses. Each curve in a subﬁgure corresponds to
a picture as shown in the legend. Our approach cracks
more passwords for a picture, if the curve is skewed up-
ward. And the cracking is faster (with fewer guesses), if
the curve is leaned toward the left.

Figure 10(a) provides a view of the attack results on
target pictures 243 and 316, each of which has only one
airplane ﬂying in the sky. Fewer PoIs in these two pic-
tures make subjects choose more similar passwords. Un-
biased with P2
A-40 breaks 261 passwords (39.0%) for the
picture 243 and 209 (31.2%) for the picture 316. The
cracking success rates are much higher than the average
success rate in Dataset-2 under the same condition. Note
that the size of generated dictionaries for these two pic-
tures are smaller than 219 due to the number of available
PoIs.

In Figure 10(b), we show the results on two portrait
pictures where Unbiased with P2
A-40 cracks 389 pass-
words (29.0%) for both in total. The attack success rate is
much higher than the average success rate in Dataset-2.
This is due to the fact that state-of-the-art computer vi-
sion algorithms work well on facial landmarks and sub-
jects’ tendencies of drawing on these features are high.
The results show that passwords on simple pictures with
fewer PoIs or portraits, for which state-of-the-art com-

394  22nd USENIX Security Symposium 

USENIX Association

12

e
m

i
t

 

n
u
R
e
g
a
r
e
v
A

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

Ordering LdGSF sequences
Generating password dictionary

2

P

L-15

2

P

A-40

e
m

i
t

 

n
u
R
e
g
a
r
e
v
A

20

18

16

14

12

10

8

6

4

2

0

Ordering LdGSF sequences
Generating password dictionary

2

P

L-15

2

P

A-40

(a) BestCover algorithm

(b) Unbiased algorithm

Figure 11: Average runtime in seconds to order LdGSF
sequences using BestCover and Unbiased. Average over
15 pictures in Dataset-2 with one standard deviation
shown.
puter vision techniques could detect PoIs with high ac-
curacy, are easier for attackers to break.

Figure 10(c) shows the attack results on 5 pictures of
people. Some of these pictures only have very small ﬁg-
ures of people and others have larger ﬁgures but not big
enough to be considered as a portrait. Unbiased with
P2
A-40 cracks 726 passwords (21.7%) for these 5 pictures
in total, which is lower than the average success rate in
Dataset-2.

Figure 10(d) shows the attack results on 4 miscella-
neous pictures, two of which are bicycle pictures and the
other two are car pictures. The picture, 6412.jpg, has
a bicycle leaning against the wall. Different colors on
the bicycle and wall in this picture make it cluttered and
have lots of PoIs. Unbiased with P2
A-40 only cracks 68
passwords (10.1%) for this picture. However, Unbiased
with P2

A-40 cracked 458 (17.1%) for all 4 pictures.

Performance. We also evaluated the performance of
our attack approach. Our analyses were carried out on
a computer with dual-core processor and 4GB of RAM.
In Figure 11, we show the average runtime for our algo-
rithms to order the LdGSF sequences and generate dic-
tionary for a picture in Dataset-2. Each bar represents the
average time in seconds over 15 pictures with the stan-
dard deviation using different algorithms and PoI sets.
The results show that BestCover is much faster than Un-
biased under the same condition. The average runtime
for BestCover on P2
A-40 to order LdGSF sequences is only
0.06 seconds and to generate a dictionary is 2.68 seconds,
while Unbiased spends 18.36 and 3.96 seconds, respec-
tively. As we analyzed in Section 4.4, such a difference
is caused by the complexity of each algorithm. With such
a prompt response, BestCover could be used for online
queries.

6 Discussion

6.1 Picture-Password-Strength Meter
Our framework could enhance the security of PGA so it
would eventually protect users and their devices by pro-

13

viding a picture-password-strength meter. One way to
help users choose secure passwords is to enforce some
composition policies, such as ‘three taps are not al-
lowed’. However, a recent effort [26] on text-based pass-
word found that rule-based password compositions are
ineffective because they can allow weak passwords and
reject strong ones. The cornerstone of accurate strength
measurement is to quantify the strength of a password.
With a ranked password dictionary, our framework, as
the ﬁrst potential picture-password-strength meter, is ca-
pable of quantifying the strength of selected picture pass-
words. More intuitively, a user could be informed of the
potential number of guesses for breaking a selected pass-
word through executing our attack framework.

6.2 Other Attacks on PGA
Besides keyloggers that record users’ ﬁnger movements,
there are some other attack methods that may affect the
security of PGA and other background draw-a-secret
schemes. Shoulder surﬁng, an attack where attackers
simply observe the user’s ﬁnger movements, is one of
them.
In our survey, 54.3% participants believe the
picture password scheme is easier for attackers to ob-
serve when they are providing their credentials than text-
based password. Several new shoulder surﬁng resistant
schemes [22, 43] were proposed recently. However, the
usability is always a major concern for these approaches.
The smudge attack [4] which recovers passwords from
the oily residues on a touch-screen has also been proven
feasible to the background draw-a-secret schemes and
could pose threats to PGA.

6.3 Limitations of Our Study
While we took great efforts to maintain our studies’ va-
lidity, some design aspects of our studies and developed
system may have caused subjects to behave differently
from what they do on Windows 8TM PGA. Subjects in
Dataset-2 pretended to access their bank information but
did not have anything at risk. Schechter et al. [33] sug-
gest that role playing like this affects subjects’ security
behavior, so passwords in Dataset-2 may not be repre-
sentative of real passwords chosen by real users. Be-
sides, we did not record whether a subject used a tablet
with touch-screen or a desktop with mouse. The different
ways of input may affect the composition of passwords.
Moreover, Dataset-2 includes multiple passwords per
user and this may have impacted the results. In our anal-
yses, training password datasets include passwords from
the targeted subject. Even though this may have affected
the results, we believe it is less inﬂuential. Because,
for each analysis, there were around 9,400 training pass-
words for which only 14 came from the targeted user.

USENIX Association  

22nd USENIX Security Symposium  395

Since all training passwords were treated equally, the in-
ﬂuence brought by the 0.14% training data is low. As
discussed in Section 5.2, even though our online attack
results showed the feasibility of our approach, it still re-
quires more realistic and signiﬁcant attack cases. As part
of future work, we plan to integrate smudge attacks [4]
into our framework to improve the efﬁcacy of our online
attacks.

7 Related Work

The security and vulnerability of text-based password
have attracted considerable attention because of several
infamous password leakage incidents in recent years.
Zhang et al. [44] studied the password choices over time
and proposed an approach to attack new passwords from
old ones. Castelluccia et al. [11] proposed an adap-
tive Markov-based password strength meter by estimat-
ing the probability of password using training data. Kel-
ley et al. [26] developed a distributed method to calcu-
late how effectively password-guessing algorithms could
guess passwords. Even though the attack framework we
presented is dedicated to cracking background draw-a-
secret passwords, the idea of abstracting users’ selection
processes of password construction introduced in this pa-
per could also be applicable to cracking and measuring
text-based passwords.

The basic idea of attacking graphical password
schemes is to generate dictionaries that consist of poten-
tial passwords [36]. However, the lack of sophisticated
mechanisms for dictionary construction affects the attack
capabilities of existing approaches. Thorpe et al. [38]
proposed a method to harvest the locations of training
subjects’ clicks on pictures in click-based passwords to
attack other users’ passwords on the same pictures. In the
same paper [38], they presented another approach which
creates dictionaries by predicting hot-spots using image
processing methods. Oorschot et al. [27] cracked DAS
using some password complexity factors, such as reﬂec-
tive symmetry and stroke-count. Salehi-Abari et al. [32]
proposed an automated attack on the PassPoints scheme
by ranking passwords with click-order patterns. How-
ever, the click-order patterns introduced in their approach
could not capture users’ selection processes accurately,
especially when a background image signiﬁcantly affects
user choice.

8 Conclusion

We have presented a novel attack framework against
background draw-a-secret schemes with special attention
on picture gesture authentication. We have described an
empirical analysis of Windows 8TM picture gesture au-
thentication based on our user studies. Using the pro-

posed attack framework, we have demonstrated that our
approach was able to crack a considerable portion of
picture passwords in various situations. We believe the
ﬁndings and attack results discussed in this paper could
advance the understanding of background draw-a-secret
and its potential attacks.

Acknowledgements

The authors are grateful to Lujo Bauer of Carnegie Mel-
lon University and Sonia Chiasson of Carleton Univer-
sity for useful comments while this work was in progress.
The authors also thank the anonymous reviewers whose
comments and suggestions have signiﬁcantly improved
the paper.

References
[1] Opencv. http://opencv.willowgarage.com.
[2] ALEXE,

DESELAERS,

B.,
V.

T.,

AND
measure

FER-
v1.5.

RARI,
http://groups.inf.ed.ac.uk/calvin/objectness/objectness-release-
v1.5.tar.gz.

Objectness

[3] ALEXE, B., DESELAERS, T., AND FERRARI, V. Measuring the
objectness of image windows. IEEE Transactions Pattern Analy-
sis and Machine Intelligence (2012).

[4] AVIV, A., GIBSON, K., MOSSOP, E., BLAZE, M., AND SMITH,
J. Smudge attacks on smartphone touch screens. In Proceedings
of the 4th USENIX conference on Offensive technologies (2010),
USENIX Association, pp. 1–7.

[5] BALLARD, D. Generalizing the hough transform to detect arbi-

trary shapes. Pattern recognition 13, 2 (1981), 111–122.

[6] BICAKCI, K., ATALAY, N., YUCEEL, M., GURBASLAR, H.,
AND ERDENIZ, B. Towards usable solutions to graphical pass-
word hotspot problem. In Proceedings of the 33rd Annual IEEE
International on Computer Software and Applications Confer-
ence (2009), vol. 2, IEEE, pp. 318–323.

[7] BIDDLE, R., CHIASSON, S., AND VAN OORSCHOT, P. Graphi-
cal passwords: Learning from the ﬁrst twelve years. ACM Com-
puting Surveys 44, 4 (2011), 2012.

[8] BONNEAU, J., PREIBUSCH, S., AND ANDERSON, R. A birthday
present every eleven wallets? the security of customer-chosen
banking pins. Financial Cryptography and Data Security (2012),
25–40.

[9] BROSTOFF, S., AND SASSE, M. Are passfaces more usable than
passwords? a ﬁeld trial investigation. People And Computers
(2000), 405–424.

[10] CANNY, J. A computational approach to edge detection. IEEE
Transactions on Pattern Analysis and Machine Intelligence, 6
(1986), 679–698.

[11] CASTELLUCCIA, C., D ¨URMUTH, M., AND PERITO, D. Adap-
tive password-strength meters from markov models. In Proceed-
ings of the 19th Network and Distributed System Security Sympo-
sium (2012), vol. 2012.

[12] CHIASSON, S., FORGET, A., BIDDLE, R., AND VAN
OORSCHOT, P. User interface design affects security: Patterns
in click-based graphical passwords. International Journal of In-
formation Security 8, 6 (2009), 387–398.

396  22nd USENIX Security Symposium 

USENIX Association

14

[13] CHIASSON, S., STOBERT, E., FORGET, A., BIDDLE, R., AND
VAN OORSCHOT, P. Persuasive cued click-points: Design, im-
plementation, and evaluation of a knowledge-based authentica-
tion mechanism. IEEE Transactions on Dependable and Secure
Computing 9, 2 (2012), 222–235.

[14] CHIASSON, S., VAN OORSCHOT, P., AND BIDDLE, R. Graph-
ical password authentication using cued click points. Springer,
pp. 359–374.

[15] DAVIS, D., MONROSE, F., AND REITER, M. On user choice in
graphical password schemes. In Proceedings of the 13th confer-
ence on USENIX Security Symposium (2004), USENIX Associa-
tion, pp. 11–11.

[16] DHAMIJA, R., AND PERRIG, A. D ´ej `a vu: A user study using
images for authentication. In Proceedings of the 9th conference
on USENIX Security Symposium (2000), USENIX Association.

[17] DIRIK, A. E., MEMON, N., AND BIRGET, J.-C. Modeling user
choice in the passpoints graphical password scheme. In Proceed-
ings of the 3rd symposium on Usable privacy and security (2007),
ACM, pp. 20–28.

[18] DUNPHY, P., AND YAN, J. Do background images improve draw
a secret graphical passwords? In Proceedings of the 14th ACM
conference on Computer and communications security (2007),
ACM, pp. 36–47.

[19] EVERINGHAM, M., VAN GOOL, L., WILLIAMS, C. K. I.,
WINN, J., AND ZISSERMAN, A. The PASCAL Visual Object
Classes Challenge 2007 (VOC2007) Results. http://www.pascal-
network.org/challenges/VOC/voc2007/workshop/index.html.

[20] FEIGE, U., LOV ´ASZ, L., AND TETALI, P. Approximating min

sum set cover. Algorithmica 40, 4 (2004), 219–234.

[21] FOLEY, M. J. Microsoft: 60 million windows 8 licenses sold
to date. http://www.zdnet.com/microsoft-60-million-windows-8-
licenses-sold-to-date-7000009549/, 2013.

[22] FORGET, A., CHIASSON, S., AND BIDDLE, R.

Shoulder-
surﬁng resistance with eye-gaze entry in cued-recall graphical
passwords. In Proceedings of the 28th international conference
on Human factors in computing systems (2010), ACM, pp. 1107–
1110.

[23] GAO, H., GUO, X., CHEN, X., WANG, L., AND LIU, X. Yagp:
Yet another graphical password strategy. In Proceedings of the
24th Annual Computer Security Applications Conference (2008),
IEEE, pp. 121–129.

[24] JERMYN, I., MAYER, A., MONROSE, F., REITER, M., AND
RUBIN, A. The design and analysis of graphical passwords.
In Proceedings of the 8th USENIX Security Symposium (1999),
Washington DC, pp. 1–14.

[25] JOHNSON, J. Picture gesture authentication, US Patent 163201,

2012.

[26] KELLEY, P., KOMANDURI, S., MAZUREK, M., SHAY, R., VI-
DAS, T., BAUER, L., CHRISTIN, N., CRANOR, L., AND LOPEZ,
J. Guess again (and again and again): Measuring password
strength by simulating password-cracking algorithms.
In Pro-
ceedings of the IEEE Symposium on Security and Privacy (2012),
IEEE, pp. 523–537.

[27] OORSCHOT, P., AND THORPE, J. On predictive models and user-
drawn graphical passwords. ACM Transactions on Information
and system Security (TISSEC) 10, 4 (2008), 5.

[28] OVIDE, S. Microsoft’s windows 8 test: Courting consumers.
http://online.wsj.com/article/SB1000142405297020453050457-
8078743616727514.html.
Signing

[29] PACE,

password.

Z.

http://blogs.msdn.com/b/b8/archive/2011/12/16/signing-in-
with-a-picture-password.aspx.

in with

a

picture

[30] PACE, Z. Signing into windows 8 with a picture password.

http://www.youtube.com/watch?v=Ek9N2tQzHOA.

[31] REIMONDO, A.

ip.org/OpenCV/34.

Haar cascades.

http://alereimondo.no-

[32] SALEHI-ABARI, A., THORPE, J., AND VAN OORSCHOT, P. On
purely automated attacks and click-based graphical passwords. In
Proceedings of the 24th Annual Computer Security Applications
Conference (2008), IEEE, pp. 111–120.

[33] SCHECHTER, S. E., DHAMIJA, R., OZMENT, A., AND FIS-
CHER, I. The emperor’s new security indicators. In Proceedings
of the 2007 IEEE Symposium on Security and Privacy (2007),
IEEE, pp. 51–65.

[34] SUO, X., ZHU, Y., AND OWEN, G. Graphical passwords: A
In Proceedings of the 21st Annual Computer Security

survey.
Applications Conference (2005), IEEE, pp. 10–19.

[35] SUZUKI, S., ET AL. Topological structural analysis of digitized
binary images by border following. Computer Vision, Graphics,
and Image Processing 30, 1 (1985), 32–46.

[36] THORPE, J., AND VAN OORSCHOT, P. Graphical dictionaries
and the memorable space of graphical passwords. In Proceedings
of the 13th conference on USENIX Security Symposium (2004),
USENIX Association, pp. 10–10.

[37] THORPE, J., AND VAN OORSCHOT, P. Towards secure design
choices for implementing graphical passwords. In Proceedings
of the 20th Annual Computer Security Applications Conference
(2004), IEEE, pp. 50–60.

[38] THORPE, J., AND VAN OORSCHOT, P. Human-seeded attacks
and exploiting hot-spots in graphical passwords. In Proceedings
of 16th USENIX Security Symposium (2007), USENIX Associa-
tion, p. 8.

[39] VAN OORSCHOT, P., AND THORPE, J. Exploiting predictability
in click-based graphical passwords. Journal of Computer Secu-
rity 19, 4 (2011), 669–702.

[40] VIOLA, P., AND JONES, M. Robust real-time face detection.
International journal of computer vision 57, 2 (2004), 137–154.
[41] WIEDENBECK, S., WATERS, J., BIRGET, J., BRODSKIY, A.,
AND MEMON, N. Authentication using graphical passwords: ef-
fects of tolerance and image choice. In Proceedings of the Sym-
posium on Usable privacy and security (2005), ACM, pp. 1–12.
[42] YUILLE, J. C. Imagery, memory, and cognition. Lawrence Erl-

baum Assoc Inc, 1983.

[43] ZAKARIA, N., GRIFFITHS, D., BROSTOFF, S., AND YAN, J.
Shoulder surﬁng defence for recall-based graphical passwords.
In Proceedings of the 7th Symposium on Usable Privacy and Se-
curity (2011), ACM, p. 6.

[44] ZHANG, Y., MONROSE, F., AND REITER, M. The security of
modern password expiration: An algorithmic framework and em-
pirical analysis. In Proceedings of the 17th ACM conference on
Computer and communications security (2010), ACM, pp. 176–
186.

A Memorability and Usability Analysis

The tolerance introduced in PGA is a trade-off between
security and usability. In order to quantify this tradeoff,
we calculate the distance between input PGA passwords
with the registered ones. When the types or directions of
gestures do not match, we regard input passwords incom-
parable with the registered ones. Otherwise, the distance

USENIX Association  

22nd USENIX Security Symposium  397

15

)
s
d
n
o
c
e
S

(
 
t
n
e
p
S
 
e
m
T
 
e
g
a
r
e
v
A

i

12

10

8

6

4

2

0

P2
L-15 and P2
A-40 using our LdGSF identiﬁcation algorithm
discussed in Section 4.3. The results from PL are closer to
users’ actual selection processes, while the results from
PA are the best approximations to users’ selection pro-
cesses we could get in a purely automated way with state-
of-the-art computer vision techniques.

s
d
r
o
w
s
s
a
P

 
f
o
 
r
e
b
m
u
N

550
500
450
400
350
300
250
200
150
100
50
0

0

2
Password Distance (<10)

4

6

8

10

(a) Password Distance His-
togram

0

2

4

6

Registration(1) Confirmation(2) Successful Logins(3−12)
(b) Average Time Spent (Seconds)

8

10

12

Figure 12: Memorability and Usability

is deﬁned as the average distance of all gestures. We de-
note the password presented for the i-th attempt ⃗π(i) and
⃗π(0) as the password registered for the same picture.

In the 2,536 login attempts collected in Dataset-1, 422
are unsuccessful in which 146 are type or direction er-
rors and 276 are distance errors. Figure 12(a) shows the
distance distribution for the password whose distance is
less than 10 and the red line denotes the threshold for be-
ing classiﬁed as successful. The result shows the current
setup in our system is quite reasonable to capture most
closely presented passwords.

Figure 12(b) shows the average time in seconds that
subjects spent on registering, conﬁrming, and reproduc-
ing passwords. x = 1 denotes the registration, x = 2 de-
notes the conformation, and all others denote the later
login attempts. As we can notice, the average time for
the registration is 7.43 seconds while 4.53 seconds are
taken for the conﬁrmation. With subjects getting used to
the picture password system, the average time spent for
successful logins is reduced to as low as 2.51 seconds.
On the other hand, the average time spent on all unsuc-
cessful login attempts is 5.86 seconds.

B Dataset-2 Pictures

Figure 13 shows 15 images that are used in Dataset-2 as
the background pictures for password selection.































Figure 13: Background Pictures Used in Dataset-2

C LdGSF Identiﬁcation

We discuss the identiﬁed LdGSFs by linking PoIs and
passwords in Dataset-2 with the help of two PoI sets

16

L-15

Pr(sk)

Table 5: Top 10 Identiﬁed LdGSFs using P2
Pr(sk|Dsk ⊆ θ)
Rank
(tap,{nose},∅)
(tap,{mouth},∅)
(tap,{circle},∅)
(tap,{eye},∅)
(tap,{∗c},∅)
(tap,{head},∅)
(circle,{circle},∅)
(tap,{ear},∅)

(tap,{head},∅)
(tap,{∗c},∅)
(tap,{circle},∅)
(tap,{eye},∅)
(circle,{head},∅)
(tap,{nose},∅)
(circle,{circle},∅)
(circle,{eye},∅)
(line,{∗c},{∗c})
(line,{eye},{eye})

1
2
3
4
5
6
7
8
9
10

(tap,{forehead},∅)

(line,{mouth},{mouth})

The top ten identiﬁed LdGSFs using P2

L-15 are shown
in Table 5 ordered by their Pr(sk) and Pr(sk|Dsk ⊆ θ).
It also suggests that ‘tap a head’ is found the most times
in the passwords, while ‘tap a nose’ is the most popular
one when there is a nose in the picture. The result seems
unreasonable at the ﬁrst glance since there is always a
nose in a head. Actually, it is because if the head in the
picture is really small, we simply annotate the circum-
scribed rectangle as head instead of marking the inner
rectangles with more speciﬁc attributes. Table 5 indi-
cates that gestures on human organs are the most popular
selection functions adopted by subjects.

A-40

Rank

Pr(sk)

Table 6: Top 10 Identiﬁed LdGSFs using P2
Pr(sk|Dsk ⊆ θ)
(tap,{clock},∅)
(circle,{clock},∅)
(tap,{shoulder},∅)
(tap,{eye},∅)
(tap,{head},∅)
(tap,{body},∅)
(tap,{mouth},∅)
(tap,{circle},∅)
(tap,{∗},∅)
(tap,{∗c},∅)

(tap,{circle},∅)
(tap,{mouth},∅)
(tap,{eye},∅)
(tap,{head},∅)
(tap,{∗c},∅)
(tap,{∗},∅)
(circle,{eye},∅)
(tap,{body},∅)
(circle,{circle},∅)
(circle,{head},∅)

1
2
3
4
5
6
7
8
9
10

The top ten identiﬁed LdGSFs using P2

A-40 are shown
in Table 6. By comparing Table 5 and Table 6, we could
notice differences caused by using annotated PoI set and
automated detected PoI set. The fact that s(tap,{∗},∅)
is among the top ten LdGSFs is an indicator that the au-
tomatic PoI identiﬁcation could not classify many PoIs
and simply mark them as ∗.
It is surprising to ﬁnd
out there are two LdGs on clock in top ten ordered by
Pr(sk|Dsk ⊆ θ) at ﬁrst, because there is no clock in
any picture in Dataset-2. The closest guess is OpenCV
falsely identiﬁed some circle shape objects as clocks, but
the number is not very big since there is no LdG on a
clock in the top ten ordered by Pr(sk).

398  22nd USENIX Security Symposium 

USENIX Association

