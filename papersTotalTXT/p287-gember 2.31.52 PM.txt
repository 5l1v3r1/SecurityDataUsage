Obtaining In-Context Measurements of

Cellular Network Performance

Aaron Gember, Aditya Akella
University of Wisconsin – Madison
{agember,akella}@cs.wisc.edu

Jeffrey Pang, Alexander Varshavsky,

Ramon Caceres

AT&T Labs – Research

{jeffpang,varshavsky,ramon}@research.att.com

ABSTRACT
Network service providers, and other parties, require an ac-
curate understanding of the performance cellular networks
deliver to users.
In particular, they often seek a measure
of the network performance users experience solely when
they are interacting with their device—a measure we call
in-context. Acquiring such measures is challenging due to
the many factors, including time and physical context, that
inﬂuence cellular network performance. This paper makes
two contributions. First, we conduct a large scale measure-
ment study, based on data collected from a large cellular
provider and from hundreds of controlled experiments, to
shed light on the issues underlying in-context measurements.
Our novel observations show that measurements must be
conducted on devices which (i) recently used the network as
a result of user interaction with the device, (ii) remain in
the same macro-environment (e.g., indoors and stationary),
and in some cases the same micro-environment (e.g., in the
user’s hand), during the period between normal usage and
a subsequent measurement, and (iii) are currently sending/
receiving little or no user-generated traﬃc. Second, we de-
sign and deploy a prototype active measurement service for
Android phones based on these key insights. Our analysis
of 1650 measurements gathered from 12 volunteer devices
shows that the system is able to obtain average throughput
measurements that accurately quantify the performance ex-
perienced during times of active device and network usage.

Categories and Subject Descriptors
C.4 [Performance of Systems]: Measurement techniques;
C.2.3 [Computer-Communication Networks]: Network
Operations—Network monitoring

Keywords
Active measurement, Cellular network performance, Device
context

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

INTRODUCTION

Cellular data networks have become a main source of In-
ternet access for hundreds of millions of users. Consequently,
many parties require an accurate understanding of the per-
formance that cellular networks deliver to users, e.g., the
range of throughput, latency, and loss that users can expe-
rience in practice. Content providers need this to optimize
application decisions; regulatory bodies need this to validate
wireless broadband speeds; and network service providers
need this for eﬀective network management and eﬃcient de-
bugging.

Although many passive analysis techniques [17, 20] and
mobile device tools for crowdsourcing active measurements [1,
2, 5, 6, 19, 33] have been developed, they mostly quantify the
performance the cellular network can oﬀer at random times,
irrespective of whether users interact with their devices at
these times and actually experience the consequences of the
measured performance. For example, the speed of a back-
ground synchronization task conducted at night may not be
relevant to most users, in contrast to the download speed of
a movie a user is watching on-demand. We argue that in
many important use-cases there is a need to obtain a mea-
sure of the network performance users experience solely when
they are interacting with their device. Moreover, some par-
ties may want to narrow their view of performance further,
to focus on a speciﬁc scope of interest (e.g., morning com-
muters driving in Los Angeles). We refer to such measures of
performance as in-context measurements, where context may
refer to, e.g., whether or not a user is actively using their
device, a device’s position relative to the user (in pocket, in
hand, etc.), or any of several other factors that may inﬂu-
ence the cellular network performance a user experiences. §2
provides several example use-cases and discusses limitations
of prior approaches in more detail.

Despite the need for in-context performance measures (as
deﬁned above), it has yet to be shown how one should go
about capturing them, or even whether existing approaches
are good enough. To this end, our paper makes two con-
tributions: First, we use anonymized cellular network data
from 20,000 subscribers of a large US-based cellular carrier
and empirical results from 100s of controlled end-to-end ex-
periments to examine the extent to which users’ interac-
tions with their devices quantitatively impact cellular net-
work performance. Crucially, we ﬁnd that measurements
must be conducted in a manner that considers whether a
user is actually using the device, what position a user puts
the device in, and what activities a user is running on the
device. Second, we leverage our empirical insights to de-

287sign a crowdsourcing-based active measurement system for
deriving in-context performance measures. We evaluate it
using a real-world deployment spanning 12 volunteer users.
Empirical Study. (§3) Our empirical analysis addresses
three questions that are central to obtaining in-context per-
formance measurements: (i) How does performance diﬀer
when measured at the times users actually use their device
as opposed to when devices are idle? (ii) What are the
reasons for performance diﬀerences? In particular, to what
extent do various contextual factors that manifest during ac-
tive use, e.g., device position, motion, etc., impact measured
performance? (iii) How should measurements be conducted
to avoid conﬂating factors? In particular, what types of
measurements and user traﬃc can overlap?

Active vs.

Idle: Using network data from 20,000 sub-
scribers, we ﬁnd that, on average, latency is 16ms higher
and loss rate is 6%-age points worse (∼17% vs ∼12%) near
the times users actively use their device. This ﬁnding implies
that a performance measurement scheme must take into ac-
count whether a user is actively using their device, otherwise
measurements may overestimate network performance.

Impact of context: We conduct 100s of controlled end-to-
end experiments in varying environments to identify the key
contextual factors that aﬀect measured performance. We
observe that small changes in a device’s position–e.g., mov-
ing a device from hand to pocket–can cause up to a 79%
diﬀerence in measured throughput and, to a lesser extent, a
diﬀerence in measured latency. We also conﬁrm that changes
in location by a few 100m and changes in whether a device is
stationary or moving may cause >1Mbps diﬀerence in mea-
sured throughput and ∼60ms diﬀerence in measured latency.
Measurement interference: Using our controlled end-to-
end experiments, we quantify the impact of simultaneous de-
vice usage and measurement gathering—an important con-
sideration for maximizing measurement opportunities. We
observe that, despite the limited bandwidth and high latency
of cellular links, web browsing or low-rate streaming can oc-
cur concurrently with measurements, while still obtaining
accurate measurement results and suitable user experience.
Measurement System. (§4) A system for in-context mea-
surement must decide when and on which devices to conduct
measurements such that the results convey the cellular net-
work performance users likely experience when they are in-
teracting with their device. Our empirical study shows that
measurements must be conducted only on devices which are
active (for low-bandwidth measurements) or were recently-
active (for measurements that may interfere with user ac-
tions). In the latter case, we must ensure that the device’s
context has not changed compared to its last active use.

To address these challenges, we design a measurement ser-
vice for Android phones that leverages crowdsourcing of ac-
tive measurements. We describe how to address key chal-
lenges, e.g., inferring user activity and monitoring the phys-
ical environment. We deployed the service across 12 volun-
teer cellular subscribers1 over a three month period, gather-
ing over 1650 measurements. Our analysis of these measure-
ments shows that the system is able to obtain measures of
mean throughput equivalent to the performance experienced
during times of active device usage.

With a growing focus on the performance experienced by
cellular subscribers from a variety of stakeholders, there is

1The volunteers include both customers and employees of a
large US-based cellular carrier.

an urgent need for systems that provide the needed perfor-
mance measures. Our work ﬁlls this gap both by showing
what factors impact measurements to what degree, and by
designing and evaluating an accurate measurement system.

2. TOWARD CROWDSOURCED

ACTIVE MEASUREMENTS

In this section, we present several use cases for cellular net-
work performance measurements taken from the perspective
of mobile devices, and then discuss the requirements mea-
surements must meet to serve these use cases. We argue
that existing approaches for measuring end-to-end perfor-
mance meet the needs of some use cases, but they lack the
ability to accurately measure speciﬁc metrics in situations
where users actually use their device.

2.1 Use Cases and Requirements

Network service providers, content providers, regulatory
agencies, and researchers all have a vested interest in mea-
suring cellular network performance; we refer to these par-
ties as measurement admins.
Network management. When customers complain, the
failures they experienced may be a result of any combina-
tion of the mobile device, the network, user behavior (e.g.,
mobility), etc. Understanding the performance experienced
by a certain device in similar contexts can help network
providers eliminate confounding factors and narrow down
the root cause.

A variety of issues may lead to network alarms being
raised, e.g., RTT estimates or radio link control failures
crossing key thresholds.
It is often unclear if the events
impact users, because they manifest as soft failures rather
than loss of connectivity. Understanding the performance
impact across a range of users is important for prioritizing
alarm analysis.

Network providers continuously upgrade their networks
but have diﬃculty understanding the impact on performance
because of complex interactions between protocol layers [12,
29]. They beneﬁt from conducting before-and-after user ex-
perience measurements in the areas where changes occurred.
Evaluating network providers. Some third parties col-
lect measurements to compare diﬀerent network providers
(e.g., [5, 6]). In addition, regulatory bodies require accurate
network speed measurements to evaluate the ubiquity of ac-
cess to broadband capacities [3, 4]. In both these cases, mea-
surements must accurately represent network performance
in order to provide a fair and unbiased view for the public.
Application decision making. Time-sensitive applica-
tions (e.g., video streaming, multiplayer games, etc.)
re-
quire accurate estimates of end-to-end performance to tune
parameters such as buﬀer sizes and streaming rates. Since
performance characteristics like delay change over time and
space [25], estimating performance on demand is essential.
Measurement Requirements. Several common require-
ments emerge from these use cases.

1. Measurements should convey performance experienced
in environments, and on devices, where users actually
send/receive traﬃc, and more speciﬁcally, when users
are interacting with their device (as opposed to perfor-
mance at random times or when a device is unused).
2. Admins should be able to control the measurement and
obtain speciﬁc metrics of interest. Only characteristics

288of the cellular network which impact end-to-end per-
formance should be depicted in measurement results.
3. Admins should be able to schedule measurements in
a controlled scope of interest, e.g., devices used in an
area where new infrastructure was deployed, or devices
used indoors during peak times. Limiting measure-
ments to controlled contexts helps admins identify the
root cause of speciﬁc performance.

2.2 Need for a New Approach

Admins currently leverage a combination of passive anal-
ysis, ﬁeld testing, and self-initiated reporting to understand
cellular network performance. While these tools provide
valuable views of performance, they do not entirely meet
the requirements outlined above.
Network-based passive analysis. Network providers can
instrument their infrastructure to passively estimate perfor-
mance statistics based on user traﬃc [17, 20, 32]. However,
it is diﬃcult to determine or control what context mobile
devices are in when they originate or receive the network
traﬃc being measured, e.g., whether the device is indoors
or outdoors. This limits the ability for administrators to fo-
cus their analysis on speciﬁc usage contexts. In addition, it
can be diﬃcult to eliminate the confounding factors of appli-
cation variability [17]. Lastly, large scale passive analysis is
only viable for admins with access to network infrastructure.
Field testing. Due to the manpower required and restric-
tions on access, it is impractical for ﬁeld testing to cover the
full range of physical contexts in which the network is used,
e.g., to measure throughput in all buildings at many dif-
ferent times of day. Even if we crowdsource measurements
from user devices for better coverage in time and space [33],
we are unlikely to quantify the performance users experience
when they are interacting with their device (§3.2, §3.3.1).
Self-initiated reporting. Admins also can rely on users
to manually initiate active measurements and report per-
formance anomalies [1, 2, 5, 6, 19]. However, these results
will only capture a small subset of situations in which users
interact with their device and the network: partially due to
the need for manual initiation, and partially due to most
users running these tools only when problems occur.

This paper advocates a new approach: context-aware crowd-

sourcing of active measurements. This measurement method
enables a user’s interactions with their device, along with the
physical context of the device, to be considered when con-
ducting measurements. This method also enables controlled
metrics to be collected, without conﬂating factors external
to the cellular network in our understanding of performance.
In speciﬁc cases, passive analysis of application traﬃc may
be an appropriate substitute for active measurements—e.g.,
it may be possible to use connections to Facebook to measure
latency—but these measurements must be carefully ﬁltered
to only include passive observations from speciﬁc contexts.
To delineate the issues that determine the design of our
approach, we ﬁrst conduct a detailed measurement study of
how users’ interactions with their devices impact measured
performance (§3). Then, we use the observations to design
a measurement service prototype (§4).

Identiﬁer Release Android
Version

Phone A May ’11 2.3
Phone B Jul ’10
2.2
Phone C Nov ’10 2.1
Phone D Jan ’10
2.1

CPU
HSDPA/HSUPA
(GHz)
3G Speed (Mbps)
Down 21, Up 3.6
1.2
Down 7.2, Up 5.7 1.0
0.8
Down 7.2, Up 2
Down 7.2, Up 2
1.0

RAM
(MB)
512
512
512
512

Table 1: Speciﬁcations for phones used in experiments

in-context cellular network performance measurements. We
analyze anonymized ﬂow records and radio signaling data
from 20,000 subscribers of a large US-based cellular carrier,
and conduct 100s of end-to-end experiments from mobile de-
vices under our control (§3.1). We start by looking at diﬀer-
ences in performance between actual usage and idle periods
(§3.2). We then investigate the impact of various contextual
factors on the observed diﬀerences (§3.3). Finally, we exam-
ine the impact of overlap between measurement probes and
user traﬃc (§3.4).

3.1 Datasets
Cellular Network Data. Our network dataset consists of
anonymized ﬂow records and radio resource control events
from approximately 20,000 devices connected to a large US-
based cellular provider. The devices are randomly sampled
from a major metropolitan area, and only devices of a single
device family are sampled (i.e., same OS and manufacturer).
All device and user identiﬁers are anonymized to protect
privacy without aﬀecting the usefulness of our analysis.

Flow records for all traﬃc were collected on December
5–11, 2010 on the logical links between Serving GPRS Sup-
port Nodes (SGSNs) in the target metropolitan area and
Gateway GPRS Support Nodes (GGSNs) in the UMTS core
network. The data contains the following details, at one
minute intervals, for each IP ﬂow:
initial cell sector, start
and end timestamps, TCP header information, TCP hand-
shake times [20], device identiﬁers, domain names, and ap-
plication identiﬁers.2

Radio resource control (RRC) messages were collected on
December 5–6, 2010 from the Radio Network Controllers
(RNCs) in the target metropolitan area. RRC measure-
ment and intra-frequency handover events were collected,
with their relevant parameters. We post-process the data to
extract: event timestamps, a list of cell sectors in the ac-
tive set, and received signal code power (RSCP), i.e., signal
strength.
Controlled Experiments. Our context3 and activity datasets
consist of latency and throughput measurements gathered
using servers and devices under our control. Latency is mea-
sured using ping, with 1 KB (context) or 64 byte (activity)
packets spaced 1 second apart. Downlink throughput is mea-
sured using iPerf, with observed bandwidth reported every 2
seconds (context) or 10 seconds (activity). All measurements
are conducted on Android phones (Table 1) connected to the
3G UMTS network of the same carrier as above.

For the context dataset, latency and throughput measure-
ments are conducted with the device in a variety of diﬀer-
ent positions and environments. The device positions in-

3. EMPIRICAL STUDY

In this section, we examine the extent to which user’s in-
teractions with their device (or lack thereof), and other rel-
evant factors (e.g., device position when not in use), impact

2Domain names are extracted from HTTP headers or DNS
responses. Applications are identiﬁed using a combination of
port information, HTTP headers, and other heuristics [15].
3The context dataset is available for download at: http:
//cs.wisc.edu/~agember/go/imc2012dataset

289clude a user’s hand, pocket, backpack, and desk/table. The
environments vary in terms of geographic location, device
movement speed (stationary, walking, city driving, highway
driving), and place (indoors, outdoors, vehicle). For each
environment, we conduct measurements on a single device
model4, and each measurement is run for either 1 or 5 min-
utes, depending on the environment. We conduct at least 5
measurements in each device position in each environment
(in walking environments we only consider hand and pocket
positions and in driving environments we do not consider
desk/table), and we change device positions in a round-robin
order to avoid bias due to temporal variation.

For the activity dataset, latency and throughput measure-
ments are conducted on devices with and without user ac-
tivities running simultaneously. The user activities include:
web browsing, which loads 15 popular websites5; bulk down-
loads, which downloads a single large ﬁle 0.5-3MB in size;
and streaming video, which models YouTube’s streaming
mechanism [9], sending a 4 minute video encoded at a spe-
ciﬁc bitrate. All measurements are conducted on devices
in the same position and environment—stationary on a ta-
ble indoors in a single geographic location. We conducted
some measurements in a second location at diﬀerent times,
aﬃrming that results from other environments are similar.

All ﬁgures in the paper identify the dataset used.

3.2 Active vs. Idle

The ﬁrst step towards obtaining in-context cellular net-
work performance measurements is to understand how per-
formance diﬀers between the times users actually use their
devices versus the times the devices are unused. Speciﬁcally,
we consider a device to be active when a user is interacting
with the device (e.g., browsing the web, using a navigation
application, playing music, etc.) and the activity requires
sending/receiving data over the cellular network. A device
is idle when the user is not interacting with the device (e.g.,
screen is oﬀ and no audio is playing), although background
traﬃc (e.g., email sync) may still be present.

We compare the network performance of active devices to
the performance of idle devices using the network dataset,
accounting for the well-known eﬀects of time [25], space [33],
and resource allocation [28]. We ﬁnd signiﬁcant diﬀerences
between the performance experienced by active and idle de-
vices, implying that random measurements, conducted ir-
respective of whether a user is actually using their device,
insuﬃciently capture an in-context view of performance.

3.2.1 Methodology
Estimating performance. To estimate network perfor-
mance at diﬀerent times and locations, we leverage the ob-
servation that each device in the network dataset has a TCP
connection on which it sends and receives keep-alive mes-
sages approximately once every 30 minutes. This occurs
whether the device is active or idle, and all devices connect
to the same data center. We deﬁne each keep-alive event as
a measurement.6

4We manually validated that multiple phones of the same
model deliver similar performance under identical condi-
tions.
5Based on Alexa, http://www.alexa.com/topsites/countries/US.
6While this connection is between the device and one of sev-
eral diﬀerent server IP addresses in the data center, the av-
erage performance of any subset of these connections should

We estimate two performance metrics for each measure-
ment: downlink loss rate and RTT. We estimate loss as the
ratio of the number of bytes retransmitted over the number
of bytes received: (tcp datalen−seqno range)/seqno range,
where tcp datalen is the total number of TCP payload bytes
observed during the measurement and seqno range is the
diﬀerence between the minimum and maximum TCP se-
quence numbers observed. We estimate RTT using TCP
handshake times when the measurement is preceded by a
new TCP connection [20], as the connection is periodically
reestablished by the device. There are approximately 2 mil-
lion measurements in the network dataset.
Identifying active times. The network dataset does not
give us direct indications of when a user is interacting with
their device, so we infer a device is active based on the pres-
ence of network traﬃc that is likely to be triggered by user
input. More speciﬁcally, we deﬁne an active range as the
time range between the start and end times of ﬂows that
indicate user activity; active ranges are calculated on a per-
user basis.

To diﬀerentiate ﬂows that indicate user activity from back-
ground traﬃc ﬂows (e.g., email polling and keep-alives), we
analyze the periodicity of traﬃc for each <domain name,
application> pair. Many applications send periodic traﬃc
frequently while in active use (e.g., Facebook sends keep-
alives every 1 minute), but the periodicity is much lower
than background traﬃc (e.g., email polling every 10+ min-
utes). Based on manual inspection of the traﬃc patterns of
the most used applications, we chose a periodicity thresh-
old of 5 minutes to diﬀerentiate between the two categories
of ﬂows. If one of the top three periodicities for a <domain
name, application> pair (excluding the keep-alive traﬃc de-
scribed above) is longer than 5 minutes and appears as a
spike in the frequency domain, we label the time ranges for
ﬂows belong to this pair as unknown ranges; this traﬃc is
likely just background traﬃc but may indicate user activity.
All other ﬂows indicate active ranges.

We deﬁne any measurement that occurs within 10 min-
utes of an active range to be a near active measurement, and
any measurement that occurs > 30 minutes from all active
and unknown ranges to be an idle measurement. We ignore
measurements that overlap or occur within 17 seconds of an
active or unknown range to avoid measuring the eﬀects of
self-interference and to ensure near active and idle measure-
ments both start in the same cellular radio state [28]. While
our deﬁnitions of idle and near active times are inexact, we
conjecture that the near active measurements are more likely
to overlap actual active times than the idle measurements.
Thus, these deﬁnitions are suﬃcient to demonstrate diﬀer-
ences in active and idle measurements on average.

3.2.2 Results
Performance diﬀerences. We expect that performance
will diﬀer during diﬀerent times of the day, as the traﬃc load
on the network varies. So the ﬁrst question we answer is: If
we control for time of day, is the performance experienced by
idle devices the same as active devices? Figures 1a and 1b
show the mean latency and loss during each hour of the day
for near active and idle devices; the error bars show 95%
conﬁdence intervals. We observe a clear diﬀerence in the

not be biased by the server selection algorithm. We statis-
tically validated the server is chosen uniformly at random
and all servers are co-located.

290)
s
m

(
 
T
T
R

)
s
m

(
 
T
T
R

230

210

190

170

240

220

200

180

160

near active
idle

0

4

8

12

16

20

Hour of day (localtime)
(a) RTT by time

near active
idle

0 1 2 3 4 5 6 7 8 9

Zip code

(c) RTT by region

d
e
t
t
i

m
s
n
a
r
t
e
R
 
s
e
t
y
B

 
.
c
a
r
F

d
e
t
t
i

m
s
n
a
r
t
e
R
 
s
e
t
y
B

 
.
c
a
r
F

0.2

0.15

0.1

0.05

near active
idle

0

0

4

8

12

16

20

Hour of day (localtime)
(b) Loss by time

near active
idle

0.25

0.2

0.15

0.1

0.05

0 1 2 3 4 5 6 7 8 9

Zip code

(d) Loss by region

Figure 1: Distribution of latency and loss for active and idle
devices in a large metropolitan area [N etwork]

near active and idle performance for most hours of the day.
Measurements on idle devices have up to 16ms lower RTT;
loss rates are lower by up to 6%-age points (or, 40% lower,
relatively speaking) compared to near active measurements.
It has been shown that performance also diﬀers between
If we control for coarse-grain
coarse-grain locations [33].
geographic location, is the performance experienced by idle
devices the same as active devices? Figures 1c and 1d show
the mean latency and loss for the 10 zipcodes with the most
measurements for near active and idle devices. Again we see
that measurements on idle devices have up to a 30ms lower
RTT, and loss rates lower by up to 4%-age points (or, 26%
on a relative scale) compared to near active measurements.
Thus, we conclude that the average performance of idle
devices is not representative of the average performance of
active devices, either at the same point in time, or within the
same coarse geographic area. Including measurements from
idle devices would overestimate performance, on average.
Avoiding even such small overestimation is important, as
minor diﬀerences in latency and loss can become magniﬁed
over the lifetime of a TCP connection. Moreover, studies
have shown that even small increases in latency—as little as
100ms—can impact user retention [21].
Cause of diﬀerences. Two common causes of cellular
performance variation—time of day and coarse geographic
location—have already been accounted for in our examina-
tion of the performance diﬀerences between active and idle
devices. We now consider three other possible causes.

First, we consider the duration of time a device has been
idle. We calculate the Pearson’s correlation coeﬃcient be-
tween the idle duration and the RTT or loss. For both
metrics, there is no correlation (0.0057 and -0.0045, respec-
tively), indicating the duration of time since a device was
last active does not impact its network performance.

Next, we consider signal strength. Figure 2a shows the
mean signal strength across all near active and idle devices
during each hour of the day; the error bars show 95% conﬁ-
dence intervals. We observe no signiﬁcant diﬀerence in av-

)

m
B
d
(
 

P
C
S
R

−98
−99
−100
−101
−102
−103
−104
−105
−106
−107
−108

e
g
n
a
h
C

 
r
o
t
c
e
S
 
h
t
i

 

w
%

70

60

50

40

30

20

near active
idle

near active
idle

0

4

8

12

16

20

Hour of day (localtime)

(a) Average signal strength

0

4

8

12

16

20

Hour of day (localtime)

(b) Cell sector changes

Figure 2: Possible causes of performance diﬀerences between
active and idle devices [N etwork]

erage signal strength for most hours of the day, indicating
variations in signal strength are unlikely to cause the dif-
ferences in latency and loss between active and idle devices.
Our measurement system evaluation (§4.1) shows the eﬀect
of signal strength on measured throughput is also minimal.
Finally, we consider diﬀerences between cell sectors as a
possible cause of the performance diﬀerences. This consid-
eration is prompted by Manweiler et al.’s observation that
latency diﬀers between cell sectors managed by diﬀerent
RNCs [25]. Figure 2b shows, for each hour of the day, the
fraction of near active and idle devices which have changed
cell sector since they were last active. We observe that a
larger faction of idle devices are using diﬀerent cell sectors,
implying that diﬀerences between cell sectors—frequency,
back-haul capacity, load, etc. [8]—are a possible cause of
the performance diﬀerences between active and idle devices.
Summary. Our analysis indicates that the average net-
work performance of active devices is worse than that of idle
devices—the latency experienced by active devices is 16ms
higher and the loss rate is 26% higher (∼17% loss vs ∼12%
loss)—requiring measurements to be cognizant of whether
devices are active to avoid overestimating network perfor-
mance. Such diﬀerences in network performance cannot be
attributed to idle duration or signal strength, but may be
caused by diﬀerences between cell sectors. In the next sec-
tion, we explore several other contextual factors, e.g., device
position, that also likely cause the performance diﬀerence.

3.3 Impact of Physical Context

There are many factors that can contribute to diﬀerences
in the end-to-end performance the network is capable of de-
livering to a device, including signal strength, signal fading
and interference, handover rate, cell sector traﬃc load, and
back-haul capacity. Several of these factors change over time
as a result of what a user physically does with their mobile
device—e.g., drives in a car with their device. In the previ-
ous section, we alluded to changes in these factors as possible
explanations for the diﬀerence in achievable performance be-
tween active and idle devices. However, precisely accounting
for changes in these low-level factors is extremely diﬃcult,
if not impossible.

We focus instead on the extent to which various aspects
of a device’s physical context—which inﬂuences these low-
level factors—impacts measured performance. For exam-
ple, changing a device’s position from your hand to your
pocket can change the signal characteristics, as can mov-
ing from indoors to outdoors. We use the context dataset
(described in §3.1) to quantify the diﬀerences in perfor-
mance between device positions—e.g., hand versus pocket—

291Hand
Other

)
s
p
b
K

(
 
t

u
p
h
g
u
o
r
h
T

2500

2000

1500

1000

500

0

In−1a In−1b In−1c In−2a In−2b In−3a In−4a In−5a Out−2a

(a) Stationary

)
s
p
b
K

(
 
t

u
p
h
g
u
o
r
h
T

2500

2000

1500

1000

500

0

Hand
Other

Hand
Other

)
s
p
b
K

(
 
t
u
p
h
g
u
o
r
h
T

2500

2000

1500

1000

500

0

In−6 Out−2 Out−5

In−2
(b) Walking

Veh−7 Veh−8
(c) Driving

Figure 3: Diﬀerence in mean throughput (with 95% con-
ﬁdence intervals) between hand and all other positions for
various environments. Environments are named by place
(Indoors, Outdoors, Vehicle), geographic location (1-8), and
stationary spot (a-c). [Context]

and validate prior observations on performance diﬀerences
associated with some changes in environment—e.g., moving
from one room to another. We ﬁnd that a device’s posi-
tion, location, and movement signiﬁcantly and consistently
inﬂuence measured performance.

3.3.1 Impact of Device Position

Users typically hold their mobile device in their hand when
they are actively using the device, whereas they may place it
on their desk or in their pocket or backpack when not in use.
The device’s signal characteristics will vary with each posi-
tion, potentially resulting in diﬀerent network performance.
We quantify the diﬀerences in performance between several
positions—a user’s hand, pocket, backpack, and desk—to
determine what eﬀect, if any, minor position variation has
on measurements. We observe diﬀerent trends with diﬀer-
ent types of measurements, so we discuss throughput and
latency measurements separately.
Throughput. In most environments, we observe a signiﬁ-
cant diﬀerence in throughput between device positions. Fig-
ure 3 shows the mean throughput measured with a device
in a user’s hand versus in other positions across various en-
vironments; the error bars show 95% conﬁdence intervals.
Mean throughput diﬀers by up to 875Kbps between posi-
tions in some environments, but it may diﬀer by less than
50Kbps.

Diﬀerences in measured throughput between device posi-
tions occur across all types of environments. Stationary de-
vices may exhibit 35-660Kbps diﬀerences in throughput be-
tween positions, and moving devices may exhibit 20-875Kbps
diﬀerences between positions. Similar variation in through-
put diﬀerences exists for indoor and outdoor environments.
Furthermore, there is no consistent trend as to whether per-
formance in hand is always better or worse: mean through-
put can be up to 47% better when the device is in the user’s
hand, or it can be up to 79% worse. Hence, throughput
measurements must be cognizant of device position.

We look in detail at measured throughput across posi-
tions for a few of the environments to determine if we can
attribute diﬀerences between positions to speciﬁc low-level
factors. We consider two indoor stationary environments

1

0.8

0.6

0.4

0.2

F
D
C

0

0

1

0.8

0.6

0.4

0.2

F
D
C

Hand
Desk
Backpack
Pocket

Hand
Desk
Backpack
Pocket

1000

2000

3000

4000

0

0

1000

2000

3000

4000

Throughput (Kbps)
(a) In-1a

Throughput (Kbps)
(b) In-1b

Figure 4: Throughput in two oﬃces in a building [Context]

Hand
Desk
Backpack
Pocket

1

0.8

0.6

0.4

0.2

F
D
C

1

0.8

0.6

0.4

0.2

F
D
C

0
−115

−105

−95
RSSI

(a) In-1a

−85

−75

0
−115

−105

−95
RSSI

(b) In-1b

Hand
Desk
Backpack
Pocket

−85

−75

Figure 5: RSSI during throughput measurements [Context]

(In-1a and In-1b), which are diﬀerent oﬃces in the same
building. However, similar trends in measurements and low-
level factors hold for a user sitting at two diﬀerent desks in
a single oﬃce in a diﬀerent building (indoor stationary envi-
ronments In-2a and In-2b) and for users walking with their
devices (environments In-2, In-6, Out-2, and Out-5).

Figure 4 shows CDFs of throughput across several mea-
surements conducted in each device position in the two of-
ﬁces. In both oﬃces, there are signiﬁcant diﬀerences in the
throughput distributions for each device position: the me-
dian throughput is 50% higher in pocket than in hand in
In-1a and 917% higher on desk than in hand in In-1b. Fur-
thermore, the ordering of throughput curves by device posi-
tion is not consistent across the two oﬃces.

We ﬁrst consider signal strength as a possible explanation
for the throughput diﬀerences between positions. Figure 5
shows the distribution of signal strength (RSSI) during our
measurements in the two oﬃces for each device position.
There is a noticeable diﬀerence in median RSSI between po-
sitions: up to 18dB for In-1a and 12dB for In-1b. However,
the ordering of the throughput curves does not match the
ordering of the RSSI curves for either oﬃce. This suggests
signal strength is not a primary factor.

We also consider cell sector, and its associated low-level
factors, as a possible cause of throughput diﬀerences. How-
ever, we observe that all measurements conducted in In-1a
occur over a single cell sector, and most measurements con-
ducted in In-1b occur over that same cell sector, along with
one other cell sector. This implies that cell sector is not a
primary cause of the throughput diﬀerences between posi-
tions.

Our hypothesis is that small scale fading is the primary
low-level factor causing diﬀerences in measured throughput
across positions. Due to constructive and destructive inter-
ference of a signal’s reﬂections oﬀ diﬀerent objects in the
environment, the signal power can be time and space vary-
ing even over small regions of space. This means that a
device being in a user’s hand versus their pocket, when the
user is stationary, can result in signiﬁcant diﬀerences in net-
work performance. We would expect that the small scale
fading would average out in moving environments, but since

2921000

)
s
m

(
 
T
T
R

800

600

400

200

0

I n − 1 a

I n − 1 c

I n − 1 b

I n − 3 a
(a) Stationary

1000

)
s
m

(
 
T
T
R

800

600

400

200

0

Hand
Other

I n − 4 a

I n − 9 a

Hand
Other

O u t − 2

V e h − 7

V e h − 1 0

(b) Moving

)
s
m

(
 
T
T
R

270

250

230

210

190

170

Figure 6: Diﬀerence in latency between hand and all other
positions for various environments; boxes show 25th, 50th,
and 75th %-tile, error bars show min and max [Context]

some objects do not move randomly—e.g., your pocket and
a device within it moves in unison when you are walking—
some regions of space may be consistently stuck in a null
(destructive interference). Hence, we would still observe the
eﬀects of small scaling fading even when walking or driving.
Latency. In contrast to throughput measurements, latency
measurements are less aﬀected by device position. Figure 6
shows the median latency measured with a device in a user’s
hand versus in other positions across some of the same envi-
ronments. We ﬁnd that median latency tends to be equiva-
lent across most environments, diﬀering by more than 15ms
in only one third of the environments. Furthermore, the
worst-case (e.g., 75th percentile) measured latencies diﬀer
substantially between positions in only two environments.

Like throughput measurements, there is no consistent trend
across environments as to when latency measurements will
diﬀer between device positions. However, our observation
that median latency tends to be similar between device po-
sitions in the same environment agrees with previous ﬁnd-
ings [25] that latency is similar amongst mobile devices con-
nected to the same cell sector. Overall, our ﬁndings suggest
that median latency can be accurately measured irrespective
of device position, but full distributions of latency require
considering device positions.

3.3.2 Impact of Environment

In addition to changing the position of their mobile de-
vice, users may also change locations or transition between
stationary and moving. These changes in environment have
previously been shown to cause diﬀerences in end-to-end net-
work performance [25, 33, 37]. We brieﬂy discuss how our
results compare with these ﬁndings.
Location. Measurements conducted using WiScape [33]
showed that throughput diﬀerences of a few 100Kbps exist
between locations as small as 250m apart. Similarly, Fig-
ure 3a shows a diﬀerence in measured throughput between
three oﬃces in the same building (environments In-1a, In-
1b, and In-1c): the mean throughput of a device held in each
of the three oﬃces is 1491Kbps, 98Kbps, and 1842Kbps, re-
spectively. All measurements are conducted in these oﬃces
within a timespan of about 2 hours and most measurements
occur over the same cell sector, so these factors are unlikely
to be major contributors to the measurement diﬀerences.

There also exists a diﬀerence, albeit less pronounced, in
measured latency between locations (Figure 6a): median la-
tency measured on a device held in each of the three oﬃces
is 416ms, 475ms, and 412ms, respectively. This discrepancy
occurs despite most measurements occurring over the same
cell sector, which others have shown to be the primary con-

)
s
m

(
 
T
T
R

300

280

260

240

220

200

180

active
near active

0 1 2 3 4 5 6 7 8 9

Zip code

(b) RTT by region

active
near active

0

4

8

12

16

20

Hour of day (localtime)
(a) RTT by time

Figure 7: RTTs with/without competing traﬃc [N etwork]

tributor of latency diﬀerences [25]. In summary, changes in
location cause diﬀerences in measured performance.
Stationary versus moving. Tso et al. [37] showed that
downlink throughput can decrease by over 1Mbps between
a stationary environment and a public transportation envi-
ronment (e.g., bus). Figure 3a shows a diﬀerence in mea-
sured throughput between stationary and moving devices in
the same location (environment 2): held devices moving at
walking speeds indoors and outdoors have mean through-
put of about 950Kbps, while stationary devices have mean
throughput of 1540Kbps outdoors and a range of 533Kbps
to 1351Kbps indoors. Thus, measurements conducted at
even slow speeds can exhibit diﬀerences compared to mea-
surements on stationary devices.
Summary. Our ﬁndings indicate that both device posi-
tion and environment can impact measurement results. For
example, an active device in a user’s hand will experience
diﬀerent performance than an idle device in a user’s pocket
due to small scale fading. Additionally, diﬀerences in per-
formance between environments can aﬀect the accuracy of
measurements if a device is typically active in some envi-
ronments and idle in others. Therefore, in-context measure-
ments should not be conducted in positions or environments
where devices are not active.

3.4 Measurement Interference

The ﬁnal factor we consider in conducting in-context per-
formance measurements is the allowable overlap in mea-
surements and user traﬃc. Many use-cases require speciﬁc
metrics and results that depict only how characteristics of
the cellular network impact end-to-end performance (§2.1).
Hence, we want to avoid the concomitant eﬀects of, and
on, other traﬃc sent/received by devices under considera-
tion. Additionally, we want to maximize the opportunities
for conducting in-context measurements, since our earlier
observations already require limiting measurements to de-
vices which are active and in certain physical contexts.

We use the network and activity datasets to examine how
user traﬃc—traﬃc resulting from user interactions with the
device—impacts measurement results. We also study how
active measurements may impact a user’s experience.

3.4.1 Impact on Measurement Results
Latency measurements. We ﬁrst examine the eﬀect of
self-interference caused by real ﬂows, which encompass a
wide range of traﬃc triggered by user interactions. We use
the network dataset and the same methodology described in
§3.2.1. Figure 7 shows the mean RTT over time and in dif-
ferent regions for measurements that overlap user-initiated
ﬂows (active) and nearby measurements that do not overlap
user ﬂows (near active); the error bars show 95% conﬁdence

2931

0.8

0.6

0.4

0.2

F
D
C

0

0

1

0.8

0.6

0.4

0.2

F
D
C

0

0

w/Browsing
No User Traffic

500

1000

1500

2000

RTT (ms)
(a) Ping

w/Browsing
No User Traffic

1000

2000

3000

4000

Bandwidth (Kbps)
(b) iPerf

Figure 8: Measurements with web browsing [Activity]

1

0.8

0.6

0.4

0.2

F
D
C

0

0

1

0.8

0.6

0.4

0.2

F
D
C

0

0

w/0.5MB File
w/1MB File
w/3M File
No User Traffic

500

1000

1500

2000

RTT (ms)
(a) Ping

w/0.5MB File
w/1MB File
w/3MB File
No User Traffic

500

1000

1500

2000

Bandwidth (Kbps)
(b) iPerf

Figure 9: Measurements with bulk downloads [Activity]

1

0.8

0.6

0.4

0.2

F
D
C

0

0

1

0.8

0.6

0.4

0.2

F
D
C

0

0

w/600K Stream
w/800K Stream
w/1000K Stream
w/1200K Stream
No User Traffic

500

1000

1500

2000

RTT (ms)
(a) Ping

w/600K Stream
w/800K Stream
w/1000K Stream
w/1200K Stream
No User Traffic

500

1000 1500 2000 2500

Bandwidth (Kbps)
(b) iPerf

Figure 10: Measurements with video streaming [Activity]

intervals. We observe that mean RTT is about 20% (40ms)
higher, during peak hours and in many regions, when mea-
surements overlap with user traﬃc. This suggests that at
least some types of concurrent user traﬃc will bias measure-
ments.

We use the activity dataset to narrow the potential causes
of this bias. We compare the RTTs reported by ping with
and without three common types of user traﬃc: web brows-
ing, bulk downloads, and video streaming (Figures 8a, 9a
and 10a). We make three observations.

First, the lowest RTT measurements without user traﬃc
are ≈150ms higher than the lowest RTTs reported during
activities. This is due to eﬀects from the radio resource
control (RRC) state machine, which causes devices with low
traﬃc volumes to stay in the CELL FACH state, utilizing
a low-bandwidth shared channel [29]. Operators need to
adjust RTT measurements to account for this RRC eﬀect.

Second, some user traﬃc signiﬁcantly increases the tail
In the presence of web browsing, 8%
of measured RTTs.
of measured RTTs are greater than 450ms, versus only 4%
without web browsing (Figure 8a). The tail increase is even
more pronounced for bulk downloads ≥ 1MB (Figure 9a).

Finally, the median RTT provides an accurate measure of
latency only during short transfers and constant rate trans-
fers. The median RTT with and without short web transfers
is about 360ms (Figure 8a), as is the median RTT with and
without video streaming (Figure 10a), due to the constant
data transfer rate used after the initial video buﬀering pe-
riod.
In contrast, the median RTT measured during bulk
downloads (Figure 9a) is only accurate for a small 0.5MB
ﬁle, not the larger 1MB and 3MB downloads.
Throughput measurements. We also examine the bias

)
c
e
s
(
 
e
m
T
 
d
a
o
L

i

15

10

5

0

o

G

Page Load Only
w/ Ping
w/ iPerf

g l e
c

a

e

o
F

o

o

b

k

Y

a

o

u

o

o

h

Y

u

T

b

e
A m a

n
o
z
W i k i p

e

d i a

T

w itt e r

N

M S

g

B i n

N
N
C
W o r d

p r e

s

s

E
W e

o m

N
e r. c

s

B

Y   T i m e

I M D
N

P
S
a t h

Figure 11: Page load times on Phone A; boxes show 25th,
50th, & 75th %-tile, error bars show min and max [Activity]

i

 

)
s
(
 
e
m
T
d
a
o
n
w
o
D

l

Download Only
w/ Ping
w/ iPerf

0
4

0
3

0
2

0
1

0

e   B

n

o

h

e   A

n

o

h

P

P
(a) 0.5MB

e   C

n

o

h

P

e   A

e   C

e   B

n

o

h

P

n

o

h

P

n

o

h

P

e   A

e   C

e   B

n

o

h

P

n

o

h

P

n

o

h

P

(b) 1MB

(c) 3MB

Figure 12: File download times; boxes show 25th, 50th, &
75th %-tile, error bars show min and max [Activity]

user traﬃc introduces in throughput measurements. Fig-
ures 8b, 9b and 10b show that measured throughput distri-
butions are aﬀected by all three categories of user traﬃc. As
the peak bandwidth used by the user interaction increases,
the impact on throughput measurements increases. For ex-
ample, 1MB and 3MB bulk downloads are able to achieve
higher peak bandwidths due to an increase in connection du-
ration and a corresponding opportunity for TCP to probe for
more bandwidth. A similar, though less pronounced eﬀect,
occurs as video streaming rates increase.

3.4.2 Impact on User Experience

We also consider the impact of active measurements on

user experience, again using the activity dataset.
Web browsing. The median page load times—which in-
cludes both content retrieval and page rendering time—of 15
popular websites are unaﬀected by active latency or through-
put measurements (Figure 11). We attribute the small vari-
ations (< 0.5s) for a few pages to transient network eﬀects.
Bulk downloads. Applications relying on downloading
data in bulk are aﬀected by bandwidth-intensive throughput
measurements. For ﬁles 0.5-3MB in size, throughput mea-
surements cause download times to increase by 0.5s to 7s,
with a 9% to 223% increase in variation (Figure 12). This
time increase does not occur with web browsing because
several small ﬁles (a few 100KB in total) are downloaded,
keeping TCP congestion windows small and preventing the
opportunity to achieve throughput levels that would be af-
fected by co-occurring throughput measurements. Latency
measurements have a much less pronounced impact on ﬁle
download time: at most a 0.8s increase for a 1MB ﬁle.
Video streaming. Streaming is aﬀected by both latency
and throughput measurements. Table 2 shows the fraction
of playback time spent buﬀering, which is a good indication
of user experience [14]. Throughput measurements cause up
to a 157% increase in the median buﬀering ratio, due to the
high-bandwidth streaming requires. Latency measurements

294Median

Relative Increase
Bit Rate Only w/Ping w/iPerf w/Ping w/iPerf
600Kbps
800Kbps
1000Kbps
1200Kbps

0%
12%
114%
157%

0%
11%
57%
24%

0.00
0.00
0.04
0.07

0.00
0.01
0.06
0.09

0.00
0.09
0.09
0.19

Table 2: Streaming buﬀering ratios on Phone B [Activity]

Figure 14: Measurement service decision process

Figure 13: System architecture

cause up to a 57% increase in the median buﬀering ratio, as
ping packets add jitter that impacts the ability to maintain
a constant streaming ratio. Thus, any measurement activity
could impact user experience for jitter-sensitive applications.
Summary. Our ﬁndings indicate the measurements con-
ducted in the presence of low-bandwidth user activities (e.g.,
web browsing or low-rate streaming) accurately portray me-
dian latency and throughput, while high-bandwidth user
traﬃc causes overestimates of worst-case latency and under-
estimates of throughput. Measurement systems (passive or
active) must consider these eﬀects for in-context results to be
accurate. Additionally, active measurement systems should
avoid high-bandwidth measurements during user activities
that heavily use the network, and no active measurements
should be conducted during jitter-sensitive activities.

3.5 Summary of Observations

With a goal of obtaining a measure of the performance
users experience solely when they are interacting with their
device, we have analyzed cellular network performance across
active and idle devices, various physical contexts, and sev-
eral combinations of measurements and user traﬃc. Our
ﬁndings indicate that measurements must be conducted:

• Only on active devices, otherwise measurements may

overestimate network performance.

• On devices which are in the same positions and en-
vironments where the devices are actively used, oth-
erwise mean throughput and worst-case latency mea-
surements will be inaccurate.

• At times when only low-bandwidth, non-jitter-sensitive
user traﬃc is present, otherwise measurement results
will be skewed and user experience will be degraded.

These ﬁndings inform the design of our system for context-
aware crowdsourcing of active measurements, discussed next.
However, these observations can also be used to ﬁlter pas-
sive measurements to quantify the performance the network
delivers solely when users are interacting with their device.

4. MEASUREMENT SYSTEM DESIGN

We now present the detailed design of a cellular network
measurement system capable of crowdsourcing in-context
active measurements from user devices. Our system con-
sists of a logically centralized measurement controller and
users’ mobile devices running a local measurement service
(Figure 13).

Admins submit requests to the controller, specifying the
metric of interest, the time frame and any constraints on
which mobile devices to use. For instance, an administrator
can request to measure latency on iPhones in Los Angeles
during morning rush hour. The controller forwards the re-
quest to any viable participating mobile devices. After mo-
bile devices conduct measurements, their results are received
by the controller and aggregated for analysis by admins.

Users install our measurement service on their mobile de-
vice to conduct active measurements. The service (i) re-
ceives measurement requests, reports results, and sends coarse
context updates to the controller, (ii) monitors network us-
age, screen state, and device sensors to identify opportuni-
ties to conduct an in-context measurement, and (iii) initiates
the appropriate measurement when an opportunity arises.
The service currently supports latency (ping), throughput
(iPerf or bulk ﬁle download), streaming, and web page load
time measurements, but it can be easily extended to gather
other metrics. The local decision process used to initiate
in-context measurements is shown in Figure 14.

Measurements are only conducted on devices which are
active or were recently-active, to avoid overestimating net-
work performance (§3.2). We consider a device to be active
when a user is interacting with the device and the activity
sends/receives data over the cellular network. Monitoring
for active use is triggered by the receipt of a measurement
request from the controller; we discuss the details below.
When a device becomes idle, or if the device’s usage of the
network will not negatively impact the measurement (§3.4),
we consider conducting a measurement.

A device’s context is sensed and recorded while the de-
vice is active and while it is conducting a measurement. We
check the context of an active device against the admin-
speciﬁed scope of interest, conducting a measurement only
if it matches. After the measurement ﬁnishes, we check if the
position or environment changed in a way that impacts per-
formance (§3.3), discarding the measurement if necessary;
otherwise, the result is sent to the controller. When the
speciﬁed measurement time frame ends, the service stops
monitoring for measurement opportunities and returns to
idle. We discuss the details of context monitoring below.

Our system conducts active measurements to avoid the
confounding factors of application variability [17] and over-
lapping traﬃc (§3.4). However, our design could be easily
modiﬁed to passively measure existing application traﬃc in-
stead of conducting active measurements. The measurement
decision process (Figure 14) would remain the same.

We now discuss how we address the challenges of active de-
tection and context sensing in an energy-aware manner and
describe their implementation on the Android platform.7

7Although our service is implemented for Android devices,
we believe similar solutions could be ported to other mobile
platforms.

295Detecting Active. The Android platform generates an
event when the screen turns on, which presumably means
the user is interacting with their device. Subsequently, we
can poll traﬃc counters for the cellular network interface to
identify the presence of traﬃc from a user activity. However,
some interactions may occur when the screen is oﬀ: listening
to network-streamed audio or speaking into the microphone.
Thus, we extend our monitoring to detect (using standard
Android APIs) when the network is being used and either
the screen is on, the microphone is on, or audio is playing.
Context Monitoring. Environment context can be ob-
tained using existing solutions: cell sector identiﬁer is often
a suﬃcient indication of geo-location; mobility can be de-
tected using cellular signals [34], GPS (used in our imple-
mentation), and accelerometers [31]; and indoors/outdoors
detection can be based on the ability to obtain a GPS signal
[30] or ambience ﬁngerprinting [10].

Position changes are much more subtle. Existing accel-
erometer-based approaches for detecting a device’s position
on the human body are promising [11, 16], but no one has
investigated the position changes we are interested in, e.g.,
moving a device from hand to backpack. We design a new
approach that stems from a key observation: we do not need
to identify the precise position of the device, only whether
or not it has changed.

We detect changes in device position by monitoring for
increases in the standard deviation of device acceleration.
If the increase is signiﬁcant enough, then signiﬁcant device
movement must have occurred—e.g., picking up the device
from your desk—versus a slight device shift—e.g., tilting
the device in your hand to see the screen better. Based
on experimentation, we chose to sample the accelerometer
at a rate of 4Hz, combine the acceleration along all three
axis into a single acceleration vector, calculate the standard
deviation over the last 15 samples, and signal changes in
micro-environment when standard deviation exceeds 1m/s2.
To keep energy use low, we only sense the environment
and position changes from the start of active use to the end
of a corresponding measurement. If at any point we detect
a change that aﬀects whether a measurement is in-context
(§3.3), we stop sensing until the next active use.

4.1 Evaluation

We evaluate our measurement system’s accuracy in gath-
ering in-context measurements, along with its position change
detection accuracy and energy consumption. We show that
measurements conducted under the constraints listed in §3.5
result in throughput metrics that closely match the perfor-
mance users experience while interacting with their device.
Furthermore, the mobile device service is able to detect po-
sition changes with a low false negative rate, and the energy
burden imposed on devices is usually minimal.

4.1.1 In-context Measurement Accuracy

We deployed our measurement service prototype to the
mobile devices of 12 volunteers over a three month period
to evaluate how closely measurements gathered by our sys-
tem match the cellular network performance these users ex-
perienced while interacting with their device. We focused
on downlink TCP throughput, a common metric of interest
which inherently captures other metrics (latency, loss, etc.).
Dataset. We measure downlink TCP throughput using a
20 second bulk ﬁle transfer from a server under our control.

Throughput is calculated at one second intervals. Measure-
ments were conducted (i) when the device’s screen was on
but no traﬃc was present, (ii) 20s after a device was ac-
tive and the environment remained unchanged, and (iii) at
random times regardless of current device usage or context.
We refer to the three types of measurements as ground truth,
in-context, and random, respectively.

We use measurements conducted under condition (i) as
our ground truth for in-context performance, because times
when the screen is on are times the user may engage in activ-
ities that use the network. We do not use passive measure-
ments of actual application traﬃc because of the variability
caused by application-speciﬁc factors (e.g., server location)
[17] and the inﬂuence of cross-traﬃc from multiple applica-
tions (§3.4). Measurements conducted under condition (ii)
are measurements we expect to be in-context, as the criteria
for conducting these measurements satisfy the requirements
outlined in §3.5. Hence, we compare these measurements
against the ground-truth to verify our system can gather
accurate in-context measurements of network performance.
We obtained 320 pairs of ground truth/in-context mea-
surements and 607 random measurements. A few of these
measurements (12 ground truth/in-context pairs, 14 ran-
dom) were excluded because the measurement failed to start
or complete. In 156 instances, the environment changed af-
ter a ground truth measurement was conducted; however,
we still conducted a measurement in the idle period that
followed (referred to as environ-diﬀ measurements) to serve
as a comparison to in-context measurements. Additionally,
we obtained 284 ground truth measurements that did not
have a corresponding in-context measurement, due to the
lack of a suﬃcient idle period (<20s) between the device’s
screen turning oﬀ and then turning back on. These are likely
caused by short power-saving screen timeouts. We use these
unpaired-active measurements only for examining how vari-
ous environment factors aﬀect mean throughput.
Measurement Opportunities. Each of the volunteers
contributed a diﬀerent number of measurements to the dataset.
Due to data plan constraints, the volunteers only ran our
prototype for part of the three month period. The top of
Figure 15 shows how many days each volunteer participated.
Furthermore, the number of measurements contributed by a
volunteer each day varies based on their device usage habits.
The bars in Figure 15 show the average number of ground
truth/in-context pairs, ground truth/environ-diﬀ pairs, and
random measurements conducted daily on each volunteer’s
device. The average number of measurement opportunities
per day for each volunteer is the sum of the in-context and
environ-diﬀ counts, since these measurements are conducted
when a user interacts with their device.

We make two important observations from Figure 15. First,
the number of times per day volunteers interact with their
device varies signiﬁcantly. Some devices are used over 12
times per day, on average (e.g., volunteer 4), while other
are used less than 3 times per day (e.g., volunteer 2).8 Sec-
ond, for most volunteers, 20% to 60% of measurements must
be discarded due to changes in environment. This suggests

8This only includes instances when the device’s screen is on
for more than 10 seconds with no network traﬃc present;
short device usage periods and background network usage
also occur, but these are not adequate measurement oppor-
tunities.

29627

37

8

6

10

10

3

21

14

8

11

14

Days Participated

In−Context
Environ−Diff
Random

n
a
e
M
n

 

i
 

e
c
n
e
r
e

f
f

D

1250

1000

)
s
p
b
K

(
 
t

u
p
h
g
u
o
r
h
T

750

500

250

0

Same Cell Sector
Changed Cell Sector

0 0 0 1 0 2 0 6 0 7 0 8 0 9 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 0 2 1 2 2 2 3

Hour of Day

y
a
D

9
8
7
6
5
4
3
2
1
0

 
r
e
P
 
s
e
r
u
s
a
e
M
g
v
A

 

1

2

3

4

5

6

7

8

9

10

11

12

Volunteer

Figure 15: Measurement frequency by volunteer [Eval]

Figure 18: Eﬀect of cell sector changes [Eval]

i

s
e
c
v
e
D

 
f

o

 
t

n
e
c
r
e
P

20

10

0

Weekday
Weekend

0

2

4

6

8 10 12 14 16 18 20 22 24

Time (hour of day in local time)

Figure 16: Measurement opportunities at scale [N etwork]

that analyses which do not account for context will include
a signiﬁcant number of results that are not in-context.

Since there are few measurement opportunities per-device
per-day, we use the network dataset to examine whether
there are suﬃcient opportunities across a large number of
users. We consider a measurement opportunity to exist if
a device has an active ﬂow (according to our deﬁnition in
§3.2.1) within a given 5-minute interval. As shown in Fig-
ure 16, measurement opportunities exist throughout the day
on at least 2.5% of devices, with opportunities on as many
as 8% of devices during peak hours on weekdays. Although
these percentages are low, they translate to several tens of
thousands of devices within a metro area.
Measurement Accuracy. We compare throughput per-
formance across all volunteers for each hour of the day. Due
to the sparsity of our collected dataset, we aggregate mea-
surements from the entire three month evaluation period.
Figure 17 shows the mean throughput, with 95% conﬁdence
intervals, for ground truth, in-context, and random mea-
surements for each hour of the day. The mean throughput
reported by ground truth and in-context measurements is
approximately equivalent for 18 (75%) of the hours in a day.
Two of the hours (4am, 5am) have zero or one measurement
pairs, so a fair evaluation cannot be made. For three (11am,
1pm, 8pm) of the four hours when mean throughput diﬀers
between ground truth and in-context measurements, the dif-
ference can be attributed to a single measurement pair. The
cause of these outlier pairs is unknown, but excluding them
results in equivalent mean throughput between ground truth
and in-context measurements.

Our data also conﬁrms our earlier observation that ran-
dom measurements provide inaccurate metric values (§3.2).
The mean throughput reported by random measurements
signiﬁcantly diﬀers from ground truth throughput for half
of the hours. For several hours, the diﬀerence is more than
1 Mbps.

In practice, cellular networks oﬀer a range of throughput
performance, even within tightly controlled measurement
conditions. We compare CDFs of throughput (graphs ex-
cluded for brevity) for the three types of measurements for
each hour of the day to conﬁrm that our system can accu-

rately quantify this range of performance. We observe that,
in addition to diﬀerences in mean throughput, the shape and
range of the CDF curves for ground truth and in-context
measurements are similar and the curves for in-context and
random measurements are signiﬁcantly diﬀerent.
Environment Impact. We show the eﬀect of changes
in environment by comparing the mean throughput during
ground truth measurements to both in-context and environ-
diﬀ measurements. Speciﬁcally, we use environ-diﬀ mea-
surements where the cell sector changes between the ground
truth and environ-diﬀ measurement. We aggregate the mea-
surements by hour of day, and we exclude the three out-
lier ground truth/in-context measurement pairs discussed
above. Figure 18 shows the absolute diﬀerence in average
throughput between ground truth and in-context and be-
tween ground truth and environ-diﬀ; the hours 3am-5am
are excluded due to a lack of environ-diﬀ measurements.
We observe a signiﬁcant inaccuracy in average throughput
measurements for 15 of 21 hours (71%), conﬁrming that
changes in cell sector have an impact on measurement accu-
racy (§3.3). In several hours, the diﬀerence is over 0.5Mbps.
We further examine the impact of environment on mea-
surement accuracy by examining which pieces of physical
context are the most predictive of mean throughput. We
consider nine factors: cell sector (LACID), location area
(LAC), speed, indoors/outdoors, network connection type,
signal strength, phone model, hour of day, and month. These
factors are recorded for all measurements we obtained (in-
cluding ground truth, in-context, environ-diﬀ, and unpaired-
active), along with the measured mean throughput.

We determine which factors are most inﬂuential by run-
ning the Reduced Error Pruning Tree (REPTree) algorithm
in Weka [7]. REPTree builds a decision tree based on infor-
mation gain, pruning the tree in a way that reduces decision
errors. The accuracy of the tree is evaluated using 10-fold
cross validation on our dataset. We limit the tree depth to 1,
to determine the most predictive factor. Then, we exclude
that factor from the dataset and re-run the REPTree algo-
rithm to ﬁnd the second most predictive factor; the process
is repeated until all factors have been excluded.

We ﬁnd that cell sector (LACID) is the most predictive of
average throughput (with a raw absolute error of 619Kbps),
indicating a change a cell sector has the biggest impact on
measurement accuracy. The second most predictive factor
is phone model, which we suspect is a result of diﬀerences in
support for various 3G enhancements (e.g., HSDPA+) be-
tween devices. The ordering of the remaining seven factors
from most predictive to least is: location area, hour of day,
month, network connection type, indoors/outdoors, speed,
and signal strength.
Summary. These ﬁndings indicate that our system is able

297)
s
p
b
K

(
 
t

u
p
h
g
u
o
r
h
T

3500

3000

2500

2000

1500

1000

500

0

Ground Truth
In−Context
Random

00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23

Hour of Day

Figure 17: Mean throughput by hour of day; error bars show 95% conﬁdence intervals [Eval]

Event
Desk→Hand
Web browsing*
Hand→Pocket
In pocket*
Pocket→Hand
Hand→Desk

Correct False Negatives False Positives

Functionality

Energy Consumed

7
5
7
7
7
6

0
-
0
-
0
1

-
2
-
0
-
-

Idle
Active Monitoring
Environment Monitoring (with GPS)
Environment Monitoring (no GPS)

in 1 Minute

0 Joules

0.44 Joules
16.85 Joules
0.15 Joules

Table 4: Energy consumed on Phone B

Table 3: Number of individuals for whom the service cor-
rectly/incorrectly detected position changes; *correct means
not detecting any change in position

to accurately crowdsource in-context throughput measure-
ments. Additionally, our ﬁndings on which environment
factors are most predictive of throughput can be used to
improve passive analysis techniques or other mobile appli-
cations seeking to approximate network performance, e.g.,
[25].

4.1.2 Measurement Service Benchmarks
Detecting Position Changes. To evaluate our accelerometer-
based approach for detecting device position changes (§4),
we asked 7 volunteers to perform a sequence of activities
with their mobile devices. Users sitting at a desk were asked
to pick up the device, browse to a website, then put the de-
vice in their pocket. After a minute, users were asked to
remove the device from their pocket, make a phone call, and
put the device back on the desk. Throughout the experi-
ment, the service monitored for changes in position.

Table 3 shows the detection results from our user study.
Our system correctly detected all but one position change,
for a low false negative rate of 3%. However, some false pos-
itives occurred during the periods with no position change.
No error in measurement accuracy occurs with false posi-
tives, but measurement opportunities are unnecessarily skipped.
Energy Usage. The exact energy used by the service de-
pends on how frequently the device is active and how often
measurements are requested. We broke down the service’s
process (Figure 14) into three pieces—idle, active monitor-
ing, and context monitoring—and measured the energy con-
sumed by each piece over a one minute interval. We ran the
service on Phone B (Table 1 lists speciﬁcations), and read
coarse-grained energy values from system counters every sec-
ond. The device LCD was always oﬀ for consistency, and we
subtracted the baseline energy the device consumes (20.5
Joules) when not running our service. We present results
averaged over 30 one-minute runs for each function.

The energy consumed in one minute by each piece of the
measurement service process is shown in Table 4. When the

device is not active and no measurements are in progress,
i.e., the service is idle, there is no noticeable energy con-
sumption. The energy consumption slightly increases (to
0.44 Joules per minute) when the user interacts with the
device and cellular interface traﬃc counters are monitored
to detect network usage. Finally, energy consumption is
the highest when using the GPS, accelerometer, and other
events to detect environment changes. An optimized service
could reduce this energy consumption signiﬁcantly, by up to
about 16 Joules, if movement speed and indoors/outdoors
were detected without using the GPS.

4.2 Additional Issues
Scope of interest. As discussed in §2, admins often seek
performance measurements for a narrower set of users cor-
responding to a speciﬁc usage environment, device speciﬁca-
tion etc. Our measurement service already tracks usage en-
vironment, and device speciﬁcations can be easily obtained,
enabling the device itself to determine if it is currently in-
scope. However, this suﬀers from scaling issues, as having
each device make this decision means the measurement con-
troller must broadcast measurement requests. To avoid this,
we can conduct a preliminary ﬁltering of devices at the mea-
surement controller. Elements in the cellular network are al-
ready aware of some environment context and device details
(e.g., cell location), enabling them to inform this preliminary
ﬁltering. The remaining context (e.g., precise geo-location)
and speciﬁcations still require on-device ﬁltering. Exactly
which parameters to ﬁlter at the controller and which to
ﬁlter on device requires trade-oﬀs in scalability, device over-
head, and potentially missed measurement opportunities, a
subject we leave for future work.
Security. Mechanisms must be in place to prevent devices
from reporting phony measurements and to protect the sys-
tem from being compromised. Since the threat model is not
fundamentally diﬀerent from existing active measurement
testbeds, ideas can be borrowed from prior techniques [27,
35] to help address these issues.
Scheduling. An admin may want to run multiple concur-
rent experiments, so scheduling them such that they do not

298interfere is important. The bottleneck and complex proto-
col interactions in cellular networks is typically in the “last
mile” [8], so limiting geographic overlap may be suﬃcient.
Incentives for deployment. To lure users to adopt mea-
surement crowdsourcing, admins may use application fea-
tures or monetary incentives. Existing measurement tools
have successfully recruited thousands of volunteers with these
approaches [1, 5, 18]. Monetary incentives is a feasible ap-
proach for network service providers since they control both
billing and measurements.

5. RELATED WORK

Our analysis and measurement system is related to work
in cellular performance studies and measurement testbeds.
Cellular performance studies. Numerous measurement
studies of cellular network performance have been conducted,
including studies on delay [23, 25], TCP performance [24,
26], fairness and performance under load [8, 36], and appli-
cation speciﬁc performance [19]. While these studies shed
light on performance trends, cellular network performance
is constantly changing as network providers upgrade and re-
conﬁgure their networks. Instead, we designed a system for
measuring performance on demand as network infrastruc-
ture and user devices continuously evolve. Moreover, our
analysis oﬀers previously unrecognized guidelines for per-
forming in-context measurement of cellular networks.
Measurement testbeds. Automatically initiating active
measurements from vantage points of interest has received
prior attention in both the wired and wireless domains. NIMI [27]
and Scriptroute [35] pioneered the idea of using voluntary
hosts on the Internet to conduct active network measure-
ments and addressed many important issues regarding secu-
rity and sharing of resources. DipZoom [38], ATMEN [22],
and CEM [13] extended these wired testbeds with match-
making services, event-based measurement triggers, and end-
point crowdsourcing. WiScape [33] uses measurements con-
ducted by mobile devices to evaluate the performance of
wide area networks across time and space.
It focuses on
collecting a small number of measurement samples to ade-
quately characterize performance in a given epoch and zone.
Similar to our approach of opportunistic crowdsourcing, these
systems conduct measurements from voluntary endpoints.
However, our system is the ﬁrst to address the question of
where and when those measurements should be conducted to
obtain results that quantify the network performance users
experience solely when they are interacting with their de-
vice. Thus, the techniques we designed in this paper com-
plement these previous systems, which mostly focused on
policy and scheduling issues to avoid interference and limit
network disruption.

6. CONCLUSION

We argue that there are several important scenarios where
there is a need to obtain in-context performance measures,
i.e., the performance the network can deliver when users
are interacting with their mobile devices. We conducted a
large-scale empirical study using data collected across cell
subscribers and controlled experiments, and concluded that
such measurements must be conducted on devices that are
(i) actively used during the measurement time frame, (ii)
currently exchanging limited user traﬃc, and (iii) in the
same position and usage environment since the device was

last used. Approaches that don’t take these guidelines into
account are very likely to give skewed results. Based on our
ﬁndings, we designed a crowdsourcing-based active measure-
ment system that allows various parties to obtain in-context
measurements in a controlled fashion. We showed, using a
three month study of our system deployed across 12 users,
that our measurement results are very accurate.

7. ACKNOWLEDGEMENTS

We would like to thank our 12 volunteers for running our
measurement system on their mobile devices. We would
also like to thank our shepherd Dina Papagiannaki and the
anonymous reviewers for their insightful feedback. This
work is partially supported by the National Science Foun-
dation under grants CNS-1050170, CNS-1017545 and CNS-
0746531.

8. REFERENCES

[1] AT&T mark the spot. research.att.com/articles/

featured_stories/2010_09/201009_MTS.html.

[2] MobiPerf. http://mobiperf.com.
[3] The national broadband plan. http://broadband.gov.
[4] Open internet. http://openinternet.gov.
[5] Root metrics. http://rootmetrics.com.
[6] Speedtest.net. http://speedtest.net.
[7] Weka. http://www.cs.waikato.ac.nz/ml/weka.
[8] V. Aggarwal, R. Jana, J. Pang, K. K. Ramakrishnan,
and N. K. Shankaranarayanan. Characterizing fairness
for 3g wireless networks. In LANMAN, 2011.

[9] S. Alcock and R. Nelson. Application ﬂow control in

YouTube video streams. SIGCOMM CCR, 41(2), 2011.
[10] M. Azizyan, I. Constandache, and R. Roy Choudhury.

SurroundSense: mobile phone localization via
ambience ﬁngerprinting. In MobiCom, 2009.

[11] L. Bao and S. S. Intille. Activity recognition from
user-annotated acceleration data. In PERVASIVE,
2004.

[12] M. C. Chan and R. Ramjee. TCP/IP performance

over 3G wireless links with rate and delay variation. In
Mobicom, 2002.

[13] D. R. Choﬀnes, F. E. Bustamante, and Z. Ge.

Crowdsourcing service- level network event
monitoring. SIGCOMM CCR, 40(4), 2010.

[14] F. Dobrian, A. Awan, D. Joseph, A. Ganjam, J. Zhan,

V. Sekar, I. Stoica, and H. Zhang. Understanding the
impact of video quality on user engagement. In
SIGCOMM, 2011.

[15] J. Erman, A. Gerber, M. T. Hajiaghayi, D. Pei, and

O. Spatscheck. Network-aware forward caching. In
WWW, 2009.

[16] R. K. Ganti, P. Jayachandran, T. F. Abdelzaher, and

J. A. Stankovic. Satire: a software architecture for
smart attire. In MobiSys, 2006.

[17] A. Gerber, J. Pang, O. Spatscheck, and

S. Venkataraman. Speed testing without speed tests:
estimating achievable download speed from passive
measurements. In IMC, 2010.

[18] D. Han and S. Seshan. A case for world-wide network

measurements using smartphones and open
marketplaces. Technical report, CMU, 2011.

299[19] J. Huang, Q. Xu, B. Tiwana, Z. M. Mao, M. Zhang,

and P. Bahl. Anatomizing application performance
diﬀerences on smartphones. In MobiSys, 2010.

[30] L. Ravindranath, C. Newport, H. Balakrishnan, and
S. Madden. Improving wireless network performance
using sensor hints. In NSDI, 2011.

[20] H. Jiang and C. Dovrolis. Passive estimation of tcp

[31] S. Reddy, M. Mun, J. Burke, D. Estrin, M. Hansen,

round-trip times. SIGCOMM CCR, 32(3), 2002.

[21] R. Kohavi and R. Longbotham. Online experiments:

Lessons learned. Computer, 40, 2007.

[22] B. Krishnamurthy, H. V. Madhyastha, and
O. Spatscheck. Atmen: a triggered network
measurement infrastructure. In WWW, 2005.
[23] M. Laner, P. Svoboda, E. Hasenleithner, and

M. Rupp. Dissecting 3g uplink delay by measuring in
an operational hspa network. In PAM, 2011.

[24] X. Liu, A. Sridharan, S. Machiraju, M. Seshadri, and

H. Zang. Experiences in a 3g network: interplay
between the wireless channel and applications. In
MobiCom, 2008.

[25] J. Manweiler, S. Agarwal, M. Zhang,

R. Roy Choudhury, and P. Bahl. Switchboard: a
matchmaking system for multiplayer mobile games. In
MobiSys, 2011.

[26] K. Mattar, A. Sridharan, H. Zang, I. Matta, and

A. Bestavros. Tcp over cdma2000 networks: a
cross-layer measurement study. In PAM, 2007.

and M. Srivastava. Using mobile phones to determine
transportation modes. ACM Trans. Sensor Networks,
6(2), 2010.

[32] F. Ricciato. Traﬃc monitoring and analysis for the

optimization of a 3g network. IEEE Wireless
Communications, 13(6), 2006.

[33] S. Sen, J. Yoon, J. Hare, J. Ormont, and S. Banerjee.

Can they hear me now?: a case for a client-assisted
approach to monitoring wide-area wireless networks.
In IMC, 2011.

[34] T. Sohn, A. Varshavsky, A. Lamarca, M. Y. Chen,
T. Choudhury, I. Smith, S. Consolvo, J. Hightower,
W. G. Griswold, and E. D. Lara. Mobility detection
using everyday gsm traces. In Ubicomp, 2006.

[35] N. Spring, D. Wetherall, and T. Anderson.

Scriptroute: a public internet measurement facility. In
USITS, 2003.

[36] W. L. Tan, F. Lam, and W. C. Lau. An empirical

study on the capacity and performance of 3g networks.
IEEE Trans. on Mobile Computing, 7(6), 2008.

[27] V. Paxson, J. Mahdavi, A. Adams, and M. Mathis. An

[37] F. P. Tso, J. Teng, W. Jia, and D. Xuan. Mobility: a

architecture for large scale internet measurement. In
IEEE Comm. Magazine, 1998.

double-edged sword for hspa networks. In MobiHoc,
2010.

[28] F. Qian, Z. Wang, A. Gerber, Z. M. Mao, S. Sen, and

[38] Z. Wen, S. Triukose, and M. Rabinovich. Facilitating

O. Spatscheck. Characterizing radio resource
allocation for 3g networks. In IMC, 2010.

[29] F. Qian, Z. Wang, A. Gerber, Z. M. Mao, S. Sen, and

O. Spatscheck. Proﬁling resource usage for mobile
applications: A cross-layer approach. In MobiSys,
2011.

focused internet measurements. SIGMETRICS
Perform. Eval. Rev., 35(1), 2007.

300