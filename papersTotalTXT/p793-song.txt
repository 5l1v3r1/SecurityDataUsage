CrowdTarget: Target-based Detection of Crowdturﬁng in

Online Social Networks

Jonghyuk Song

Dept. of CSE, POSTECH
Pohang, Republic of Korea
freestar@postech.ac.kr

Sangho Lee

Dept. of CSE, POSTECH
Pohang, Republic of Korea
sangho2@postech.ac.kr

Jong Kim

Dept. of CSE, POSTECH
Pohang, Republic of Korea
jkim@postech.ac.kr

Abstract
Malicious crowdsourcing, also known as crowdturﬁng, has
become an important security problem. However, detect-
ing accounts performing crowdturﬁng tasks is challenging
because human workers manage the crowdturﬁng accounts
such that their characteristics are similar with the charac-
teristics of normal accounts.
In this paper, we propose a
novel crowdturﬁng detection method, called CrowdTarget,
that aims to detect target objects of crowdturﬁng tasks (e.g.,
post, page, and URL) not accounts performing the tasks.
We identify that the manipulation patterns of target objects
by crowdturﬁng workers are unique features to distinguish
them from normal objects. We apply CrowdTarget to detect
collusion-based crowdturﬁng services to manipulate account
popularity on Twitter with artiﬁcial retweets. Evaluation
results show that CrowdTarget can accurately distinguish
tweets receiving crowdturﬁng retweets from normal tweets.
When we ﬁx the false-positive rate at 0.01, the best true-
positive rate is up to 0.98.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—
Security and protection; K.4.1 [Computers and Society]:
Public Policy Issues—Abuse and crime involving computers

General Terms
Security

Keywords
Malicious crowdsourcing; Online social networks; Twitter;
Underground services

1.

INTRODUCTION

According to the characteristics of tasks, people can do
certain tasks better than computers in terms of accuracy,
cost, and speed. Crowdsourcing is the process of outsourcing

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813661.

tasks to human workers to exploit such observations while
paying them for the tasks. Various crowdsourcing sites ex-
ist, such as Amazon Mechanical Turk, Microworkers, and
Crowdsource.

Unfortunately, adversaries have become major customers
of crowdsourcing services. They use the services for mali-
cious purposes because human workers can easily circumvent
conventional security systems to detect automated activities
performed by bots. Adversaries can leave various malicious
tasks to human workers belonging to crowdsourcing sites,
such as spreading spam URLs, searching speciﬁc keywords
to manipulate search results, and boosting the popularity
of their accounts in online social networks (OSNs). This
malicious crowdsourcing has both characteristics of crowd-
sourcing and astroturﬁng, so researchers name it crowdturf-
ing [33].

Although researchers propose numerous methods of mali-
cious account detection using account-based features or syn-
chronized group activities, they are inappropriate to detect
crowdturﬁng accounts. First, detection methods based on
account-based features [12, 17, 23, 34, 35] inspect the charac-
teristic of individual account, e.g., the number of friends, the
number of posts, and age. However, recent studies [29, 32]
show that applying the techniques to detect crowdturﬁng
accounts is vulnerable to simple evasion techniques, such as
performing malicious tasks while doing normal behaviors.
Interestingly, our analysis of account popularity, which is
computed by using account features and behaviors, shows
that crowdturﬁng accounts are more popular than normal
accounts (Section 4.1).

Next, identifying synchronized group activities of mali-
cious accounts is state-of-the-art methods of detecting mali-
cious accounts managed by bots [8, 11, 15, 16, 31]. However,
we empirically identify that crowdturﬁng tasks have weak
correlation because human workers perform the tasks ei-
ther without schedule or with ﬂexible schedule (Section 4.2).
Consequently, we demand a novel detection method that re-
lies on neither account characteristics nor program-controlled
behaviors.

In this paper, we propose a novel method of detecting
crowdturﬁng, called CrowdTarget. CrowdTarget aims to dis-
cover target objects that crowdturﬁng customers attempt to
manipulate, e.g., URL, search keyword, and post, by us-
ing their manipulation patterns. Unlike conventional detec-
tion methods using account characteristics, CrowdTarget is
(i) robust against evasive techniques to manipulate account-
based features. Also, it can detect crowdturﬁng tasks per-

793formed by (ii) new accounts or (iii) casual workers who oc-
casionally participate in crowdturﬁng tasks.

Among numerous crowdturﬁng services aiming at various
services, we apply CrowdTarget to collusion-based crowd-
turﬁng services that manipulate account popularity on Twit-
ter by using artiﬁcial retweets. Our goal is to distinguish be-
tween tweets receiving retweets from crowdturﬁng accounts
(we name them crowdturﬁng tweets) and tweets receiving
retweets from normal accounts.

We ﬁrst analyze the diﬀerences in retweet patterns of
the three tweet groups: normal, crowdturﬁng, and black-
market tweet groups. From the analysis, we ﬁnd four new
retweet-based features that allow us to distinguish crowd-
turﬁng tweets from others: (i) retweet time distribution,
(ii) the ratio of the most dominant application, (iii) the
number of unreachable retweeters, and (iv) the number of
received clicks. The ﬁrst feature, retweet time distribu-
tion, consists of four sub-features: mean, standard devia-
tion, skewness, and kurtosis.

Next, we build three classiﬁcation models, Ada Boost,
Gaussian na¨ıve Bayes, and k-nearest neighbors, by using the
retweet-based features and evaluate them with our ground-
truth dataset. Evaluation results show that CrowdTarget
can accurately distinguish crowdturﬁng tweets from normal
tweets; the true-positive rate (TPR) is 0.98 when the false-
positive rate (FPR) is 0.01 with the k-nearest neighbor al-
gorithm.

In summary, the main contributions of this paper are as

follows:

• New detection approach. We detect crowdturﬁng
by analyzing not the characteristics of its accounts but
the characteristics of its targets.
In this paper, the
targets are tweets and the crowdturﬁng task retweets
the tweets. To the best of our knowledge, this is the
ﬁrst approach that detects crowdturﬁng by using the
targets.

• In-depth analysis. We analyze retweets generated
by three account groups: normal, crowdturﬁng, and
black market. This analysis provides insight to under-
stand each group’s behaviors.

• High accuracy. The accuracy of our method is very
high. When we ﬁx the false-positive rate at 0.01, the
true-positive rate is up to 0.98.

The remainder of this paper is organized as follows. In
Section 2 we compare black-market sites and crowdturﬁng
sites.
In Section 3 we explain the details of our dataset.
In Section 4 we analyze the characteristics of crowdturﬁng
workers. In Section 5 we introduce the unique features of
crowdturﬁng targets. In Section 6 we explain how we use
the features to construct our classiﬁers and evaluate their
accuracy. In Section 7 we discuss the robustness of our fea-
tures. In Section 8 we introduce related studies. Lastly, we
conclude this paper in Section 9.

2. BACKGROUND

In this section, we explain black-market sites and crowd-
turﬁng sites for OSNs. Their main diﬀerence is that the
black-market sites only sell malicious services, whereas the
crowdturﬁng sites not only sell malicious services but also
encourage the participation of users in conducting malicious
activities.

Figure 1: Procedure of OSN boosting in a collusion-
based crowdturﬁng service. A customer C posts a
task on the service S. A worker W performs the task
on S and S relays W ’s actions to the target OSN. C
ﬁnally pays virtual money for the tasks that W has
conducted.

2.1 Black-market Site for OSNs

Black-market sites are proposed to satisfy people’s desire:
promoting their popularity in OSNs. The sites provide var-
ious services for the goal, e.g., increasing the number of fol-
lowers, likes, and comments. According to the price, they
oﬀer various plans with deadlines, e.g., $39 for gaining 2,500
Twitter followers within 48 hours.

To provide malicious services, black-market sites usually
operate a large number of bots to perform many tasks by
deadlines. They strive to develop bot accounts that closely
resemble normal accounts because (i) they want to prevent
security teams of OSNs from suspending their accounts and
(ii) their customers want to have human-like followers to
make the popularity of their accounts more realistic.

Although bot accounts resemble normal accounts, they
inevitably have synchronized group activities because they
should perform the same tasks by deadlines. Therefore, re-
cent studies try to detect bot accounts in OSNs by discov-
ering their synchronized group activities [8, 11, 15, 16, 31]. In
Section 4, we also observe synchronized group activities of
black-market accounts.
2.2 Crowdturﬁng Sites for OSNs

Recently, collusion-based crowdturﬁng services specialized
for OSN boosting have appeared, e.g., addmefast.com [1]
and traffup.net [6]. In these services, users exchange their
eﬀorts to achieve their goals, such as increasing the num-
ber of Twitter followers and retweets, the number of Insta-
gram comments, and the number of Facebook likes. Figure 1
shows the procedure of OSN boosting in such services.

x A customer C posts an object (e.g., tweet and page) to
be manipulated to a crowdturﬁng service and speciﬁes
a reward (e.g., an amount of virtual money).

y A worker W performs boosting tasks on the crowd-

turﬁng service (e.g., click an RT button).

z The crowdturﬁng service relays the boosting tasks to

the target OSN.

{ The crowdturﬁng service transfers C’s virtual money

to W .

The collusion-based crowdturﬁng service simpliﬁes the pro-
cess of boosting for both workers and customers. In a con-

WCCrowdturfing service(cid:164)Boosting(cid:163)Crowdturfing task(cid:165)Boosting(cid:166)Payment(virtual money)Online Social NetworkWCpost1post2…postnRT794Table 1: The dataset

Dataset

#Tweets #Retweets #Retweeters

Normal
Without URL
With URL
Total
Crowdturﬁng
Without URL
With URL
Total
Black-market
Total

10,318
15,248
25,566

4,531
14,867
19,398

914,974
1,941,482
2,856,456

576,033
1,866,843
2,442,876

390,275
1,149,563
1,412,632

115,657
110,295
190,800

282

71,858

41,829

ventional crowdsourcing service, a worker performs the boost-
ing in the target OSN and customers should examine whether
the worker has done the task properly. However, the collusion-
based crowdturﬁng service automates the procedures of work-
ers and customers. When a user signs up the crowdturﬁng
service, the user authorizes the crowdturﬁng service’s ap-
plication that manages and monitors overall boosting tasks.
The application monitors how crowdturﬁng workers perform
certain boosting tasks at the crowdturﬁng service and re-
lays the tasks to the target OSN. Thus, the service can be
convinced that the boosting tasks are done properly. This
convenient procedure makes workers easily perform many
crowdturﬁng tasks.

Based on the analysis results in Section 4, we are con-
vinced that workers of collusion-based crowdturﬁng services
are either real humans or advanced human-like bots. Un-
like casual bots, the crowdturﬁng workers are more popular
than normal accounts and do not have synchronized group
activities. Therefore, the conventional bot detectors cannot
detect the crowdturﬁng accounts.

3. DATA COLLECTION

In this section, we explain our ground-truth tweets col-
lected from three sources: Twitter, crowdturﬁng sites, and
black-market sites. We only consider tweets that received
≥ 50 retweets because a small number of retweets cannot
manipulate the popularity of accounts. Note that every
black-market site analyzed assures ≥ 50 retweets, so this
treatment is acceptable. Also, every tweet we collected was
created between November 2014 and February 2015. Table 1
summarizes our dataset.
3.1 Ground-truth Tweets
Normal tweets on Twitter. We collected normal tweets
from Twitter. We regarded a tweet as a normal tweet if it
was created by a veriﬁed Twitter account that has ≥ 100,000
followers. We randomly selected 1,044 veriﬁed Twitter ac-
counts that satisfy the requirements and monitored their
timeline to collect tweets and retweets.
Crowdturﬁng tweets. We collected crowdturﬁng tweets
from nine diﬀerent crowdturﬁng sites. We registered at the
crowdturﬁng sites and retrieved tasks requesting retweets
posted on the sites.
Black-market tweets. We collected black-market tweets
from ﬁve diﬀerent black-market sites, e.g., retweets.pro [4]
and socialshop.co [5]. We ﬁrst wrote 282 tweets containing
URLs by using our fake Twitter accounts. Then, we regis-

tered at the black-market sites and purchased retweets for
our tweets. On average, we paid $5.6 for 100 retweets and
$13.4 for 1,000 retweets. All black-market sites provided the
retweets about a day.
3.2 Methods to Collect Retweets

We explain our approach to collect retweets. Although
Twitter provides a REST API to retrieve retweets that a
tweet received (statuses/retweets), this API only returns
up to 100 latest retweets. Our objective is to collect as
many retweets for each target tweet as possible. We take two
approaches to achieve it. First, for a target tweet recently
posted, we use a streaming API to monitor retweets it will
receive in the next three days. Second, for a target tweet
posted in the past, we use a Twitter search function to ﬁnd
as many retweets of the target tweet as possible.
3.3 Ethics

In this study, we have encountered several legal and eth-
ical problems on experimenting and collecting data. We
referenced Thomas et al. [25]’s approach to ethically study
underground services. We designed our data collection and
subsequent experiments to follow the exemption guideline
from a formal review of the institutional review board (IRB)
of our institute.

First, we have not collected any data that can be used
to distinguish individual subjects. We deleted detailed per-
sonal information (e.g., names and proﬁles) that were unre-
lated to our experiments.

Second, to minimize our eﬀects to underground services,
we only retrieved public tasks posted on crowdturﬁng sites
and purchased a small number of retweets from black-market
sites. Further, we neither attempted to inspect who operate
such services nor contacted them via other channels.

Third, to avoid the negative eﬀects of using black-market
services against Twitter and its users, we deleted our fake ac-
counts right after receiving and collecting purchased retweets.
Since we made our fake accounts only follow each other and
post tweets with harmless and meaningless contents, legiti-
mate users would rarely see or retweet our tweets.

4. CROWDTURFING WORKERS

In this section, we analyze crowdturﬁng workers to know
whether they are humans, bots, or something else. We check
two sets of features: account popularity and synchronized
group activity.
4.1 Account Popularity

We compare the popularity of crowdturﬁng accounts on
Twitter with other account groups by using three features:
follower to following ratio, the number of received retweets
per tweet, and Klout score [3] (Figure 2). First, we mea-
sure the ratio of the number of followers to the number of
followings in each account group. Figure 2a shows that ap-
proximately 70% of the crowdturﬁng accounts have a larger
number of followers than followings; this ratio is much higher
than the normal (37%) and black-market account groups
(20%).

Second, we check the tweets of each account group to
know how many times they are retweeted (Figure 2b). We
observe that tweets posted by crowdturﬁng accounts are
more frequently retweeted than tweets posted by normal
or black-market account groups. Approximately 43% of

795(a) The ratio of the number of followers to the
number of followings

(a) Following similarity between every two ac-
counts

(b) The number of retweets for each account’s
tweets

(b) Retweet similarity between every two accounts

Figure 3: Synchronized group activities of the three
account groups: normal, crowdturﬁng, and black-
market account groups. Crowdturﬁng and normal
accounts have similar patterns.

counts and 4% of tweets posted by black-market accounts
are retweeted more than once.

Third, we query Klout scores of the three account groups,
which is a popular OSN inﬂuence score. Figure 2c shows that
crowdturﬁng accounts have a higher Klout score than those
of other groups. The median Klout score of the crowdturﬁng
accounts is 41. In contrast, the median Klout scores of the
normal accounts and black-market accounts are 33 and 20,
respectively.

Consequently, we are convinced that crowdturﬁng accounts
successfully boost their popularity by gaining followers and
retweets from crowdturﬁng services. They diﬀer from black-
market accounts and resemble inﬂuential users in OSNs.

4.2 Synchronized Group Activity

Next, we aim to identify whether crowdturﬁng accounts
have synchronized group activities. We use two measures to
check it: following similarity and retweet similarity.

(c) Klout score

Figure 2: Social popularities of the three account
groups: normal, crowdturﬁng, and black-market ac-
count groups. Crowdturﬁng accounts are more pop-
ular than normal and black-market accounts.

tweets posted by crowdturﬁng accounts are retweeted more
than once. In contrast, 5% of tweets posted by normal ac-

796Following similarity. We deﬁne the following similarity
Fsim between two accounts ui and uj as follows:

Fsim(ui, uj) =

|F (ui) ∩ F (uj)|
|F (ui) ∪ F (uj)| ,

where F (ui) is a set of ui’s followings. We compute the fol-
lowing similarity between two accounts only when at least
one of their retweets originate from the same tweets. Fig-
ure 3a shows that the crowdturﬁng and normal account
groups have the same pattern:
low following similarities.
In contrast, the black-market account group has the highest
following similarity.
Retweet similarity. To compute the retweet similarity,
we ﬁrst deﬁne a set of retweets of ui, RT (ui), as follows:
RT (ui) = {(ui, T1, tid1), (ui, T2, tid2), . . . , (ui, Tn, tidn)},
where Ti represents retweet time and tidi is the ID of a
tweet retweeted by ui. A retweet (ui, Tk, tidk) in RT (ui) is
matched with another retweet (uj, Tl, tidl) in RT (uj) if they
satisfy the following two properties:

1. The two retweets are for the same tweet: tidk = tidl.

2. The two retweets are created within a threshold time

window: |Tk − Tl| ≤ Tthreshold.

Based on the deﬁnitions, we compute the retweet similar-

ity RTsim between two accounts ui and uj as follows:

RTsim(ui, uj) =

|RT (ui) ∩ RT (uj)|
|RT (ui) ∪ RT (uj)| .

Figure 3b shows the statistics of the retweet similarities
of the three account groups. We observe that the crowdturf-
ing and normal account groups have the same pattern: low
retweet similarities. In contrast, the black-market account
group has the highest retweet similarity.

Consequently, we conﬁrm that the crowdturﬁng account
group shows no or weakly synchronized group activity. Thus,
we should not rely on conventional detection methods using
synchronized group activity to detect them.

5. CROWDTURFING TARGETS

In this section, we analyze the characteristics of crowdturf-
ing targets on Twitter: tweets receiving artiﬁcial retweets
generated by crowdturﬁng workers. Note that all charac-
teristics explained in this section were never considered in
previous work and we will use all of them to build our clas-
siﬁers explained in Section 6.
5.1 Retweet Time Distribution

We ﬁrst consider the time distribution of retweets that a
tweet received. Our key insight is that the time pattern of
artiﬁcial retweets diﬀers from that of normal retweets. Fig-
ure 4 shows example retweet time distributions of normal,
crowdturﬁng, and black-market tweets. We have counted
the number of retweets generated every hour from the cre-
ation of the individual tweets. Figure 4a shows that the nor-
mal tweet is intensively retweeted within a few hours after
posting, and number of retweets decreases as time goes on.
In contrast, Figure 4b shows that the crowdturﬁng tweet
is constantly retweeted because the tweet is continuously
exposed to crowdturﬁng workers as long as it is posted on
crowdturﬁng services. In the black-market case (Figure 4c),

(a) Normal tweet

(b) Crowdturﬁng tweet

(c) Black-market tweet

Figure 4: Retweet time distributions of normal,
crowdturﬁng, and black-market tweets. They diﬀer
from each other.

a large number of retweets are generated within a certain
time period, and no other retweet is generated later.

To extract features from the retweet time distribution,
we use four popular measures to ﬁgure out the shape of a
distribution.

797(a) Mean

(b) Standard deviation

(c) Skewness

(d) Kurtosis

Figure 5: Statistical characteristics of retweet time distribution. The characteristics of normal, crowdturﬁng,
and black-market retweets diﬀer from each other.

5.1.1 Mean
We use the mean of a retweet time distribution to know
the average time diﬀerence between posting and retweeting.
The mean retweet time of a normal tweet is usually smaller
than that of a crowdturﬁng tweet. The mean retweet time
of a black-market tweet depends on when bots begin to op-
erate; usually, they perform retweets as soon as possible to
satisfy their customers.

Figure 5a shows the mean retweet time of the three groups.
The mean retweet time of the crowdturﬁng tweets is larger
than other groups, since they are continuously retweeted.
Also, approximately 90% of the black-market tweets, 60%
of the normal tweets, and 20% of the crowdturﬁng tweets
receive 50% of their retweets within 24 hours. Therefore, we
decide to use the mean of a retweet time distribution as a
feature.

5.1.2 Standard deviation
We use the standard deviation of a retweet time distribu-
tion to know how many retweets are generated around the
mean time. Crowdturﬁng retweets are evenly distributed
such that their standard deviation is larger than those of
normal and black-market tweets.

Figure 5b shows the standard deviation of the retweet time
distribution of the three groups. The crowdturﬁng tweets
have higher standard deviation than other groups. Further,
the smallest standard deviation of the black-market tweets
shows that most of them are retweeted around the mean
time. Therefore, we decide to use the standard deviation of
a retweet time distribution as a feature.

Skewness

5.1.3
We use the skewness of a retweet time distribution to know
when a tweet is mostly retweeted. Skewness is a measure of
the asymmetry of the distribution. Positive skewness means
that the right side tail of the distribution is longer than the
left side. In contrast, negative skewness means that the tail
on the left side is longer than the right side.

Figure 5c shows the skewness of retweet time distribu-
tions of the three groups. Most of the crowdturﬁng tweets
have near-zero skewness, which implies that they are evenly
retweeted. In contrast, the skewness of the normal tweets is
larger than zero, which implies that the number of retweets
they receive gradually decreases as time goes on. Skewness
of black-market tweets depends on how the black-market
services operate their bots. Most of the black-market tweets
collected have negative skewness, implying that the num-

798ber of retweets gradually increases at ﬁrst, but suddenly
decreases later. Thus, we decide to use the skewness of a
retweet time distribution as a feature.

5.1.4 Kurtosis
We use the kurtosis of a retweet time distribution to know
the intensity of retweets within a short time period. Kur-
tosis is a measure of the peakedness of the distribution. If
a distribution is sharper than the normal distribution, its
kurtosis is positive. In contrast, if a distribution is ﬂatter
than the normal distribution, its kurtosis is negative. Note
that the kurtosis of the normal distribution is zero.

Figure 5d shows that the crowdturﬁng tweets have the
lowest kurtosis among the three groups, i.e., their retweets
are evenly distributed. The kurtosis of the normal tweets
is much higher than that of the crowdturﬁng tweets be-
cause, usually, a normal retweet time distribution has a peak
around the posting time. The black-market tweets have the
highest kurtosis because black-market services should gener-
ate a number of retweets within a given deadline [11]. Con-
sequently, we decide to use the kurtosis of a retweet time
distribution as a feature.
5.2 Twitter Application

We ﬁnd that most of the collusion-based crowdturﬁng
services have third-party Twitter applications to generate
retweets. Their web sites provide custom interfaces for work-
ers to easily create retweets for tweets of crowdturﬁng cus-
tomers. Therefore, for each tweet receiving retweets, we
compute the ratio of the number of the retweets generated
by the most dominant application to the total number of
retweets.

Figure 6 shows the ratio distributions of the dominant
applications used to generate retweets. We found that domi-
nant applications generated approximately 90% of the crowd-
turﬁng retweets and approximately 99% of the black-market
retweets on average. In contrast, dominant applications gen-
erated approximately 40% of the normal retweets on aver-
age. Therefore, the ratio of the dominant applications can
be a feature of crowdturﬁng tweets.
5.3 Unreachable Retweeter

We observe that most retweeters of a crowdturﬁng tweet
do not follow the user who posts the tweet because crowd-
turﬁng services promote the tweet to unspeciﬁed individuals
without considering their friendships on Twitter. But, in
general, a tweet is propagated between users who are con-
nected with each other on Twitter. Thus, retweeters are
usually connected to a posting user by follower-following re-
lationships.

To attest the observation, we measure how many retweet-
ers are unreachable to posting users on Twitter. Figure 7
shows that approximately 80% of the crowdturﬁng tweets
have over 80% of unreachable retweeters. In contrast, less
than 10% of normal tweets have over 80% of unreachable
retweeters. Hence, the ratio of the unreachable retweeters is
another feature of crowdturﬁng tweets.
5.4 Click Information

One of the main purpose of malicious accounts in OSNs is
spreading links to many OSN users to promote their websites
or spread malwares. When malicious accounts post tweets
with malicious links, they abnormally boost the tweets to

expose the links to as many users as possible. Thus, de-
tecting URL tweets retweeted by crowdturﬁng services is an
important problem.

Our hypothesis is that when retweeting tweets that con-
tain links, crowdturﬁng accounts are not willing to click on
the links because it is not their duty. Therefore, even if a
tweet with a link is heavily retweeted by such services, the
number of clicks that the link receives could be small.

To conﬁrm our hypothesis, we should measure how many
times a link in a tweet is clicked on. Fortunately, many
Twitter users use URL shortening services (e.g., bit.ly and
goo.gl) to share URLs via Twitter and the services provide
the click analytics for each shortened URL [22]. This allows
us to count the number of clicks that each link receives.

We extract tweets that contain bit.ly and goo.gl short-
ened URLs from our dataset: 6,024 normal tweets, 3,093
crowdturﬁng tweets, and 282 black-market tweets (when we
purchased retweets from black markets, all our tweets con-
tained shortened URLs.) We crawl the click analytics of
each shortened URL and extract the number of clicks via
Twitter according to the referrer information.

Figure 8 shows the ratio of the number of clicks to the
number of retweets per tweet. Over 80% of links in the
normal tweets receive a larger number of clicks than the
number of retweets. However, approximately 90% of links
in the crowdturﬁng tweets receive a smaller number of clicks
than the number of retweets. Furthermore, most of the
links in the black-market tweets are never clicked on. From
the results, we conﬁrm that most crowdturﬁng and black-
market accounts perform retweets without clicking on con-
tained links because they have no reason to visit the links
to retweet them. Therefore, we use the click information as
the ﬁnal feature of crowdturﬁng tweets.

6. DETECTION OF CROWDTURFING TAR-

GETS

In this section, we explain how we build our classiﬁers,
CrowdTarget, to detect crowdturﬁng targets and evaluate
their accuracy. We treat both crowdturﬁng tweets and black-
market tweets as malicious tweets and attempt to distin-
guish them from normal tweets.
6.1 Building Classiﬁers

We ﬁrst explain how we prepared training and testing data
using the dataset in Section 3. Note that in real-world ser-
vices, the number of malicious messages is fairly smaller than
the number of normal messages. For example, Twitter has
announced that the portion of spam tweets is approximately
1% of the total tweets [26]. Therefore, we decided to set the
ratio of malicious tweets as 1% of the total tweets. We over-
sampled normal tweets to satisfy the requirement. I.e., we
randomly duplicated normal tweets until their number be-
came 99 times larger than the number of malicious tweets.
We built classiﬁers by using the seven features of retweets
explained in Section 5: (i) mean, (ii) standard deviation,
(iii) skewness, and (iv) kurtosis of retweet time distribution,
(v) the ratio of dominant applications used for retweets,
(vi) the ratio of unreachable retweeters, and (vii) the ra-
tio of the number of clicks to the number of retweets for
tweets containing URLs. We normalized all feature values
to be lie between 0 and 1. With these features, we tested
several classiﬁers provided by the scikit-learn library (a

799Figure 6: Ratio of the most dominant application
performing retweets. Almost the same applications
generate crowdturﬁng and black-market retweets
unlike normal retweets.

Figure 9: ROC curve showing TPRs and FPRs of
CrowdTarget. We test Ada boost, Gaussian Na¨ıve
Bayes, and k-nearest neighbors algorithms with 10-
fold cross validation.

Python machine-learning library) [21] and then selected top
three classiﬁers showing good accuracy: Ada Boost, Gaus-
sian na¨ıve Bayes, and k-nearest neighbors. We validated
classiﬁcation results with 10-fold cross-validation.
6.2 Basic Classiﬁcation

First, we distinguish malicious tweets from normal tweets
without using click information to deal with both tweets
with and without URLs. Figure 9 shows receiver operating
characteristics (ROC) curves of the algorithms that draw
how TPRs change according to the changes of FPRs. We
deﬁne TPR and FPR are as follows:

T P R =

#T P

#T P + #F N

and F P R =

#F P

#F P + #T N

,

where TP stands for true positive and FP stands for false
positive.

We aim to build a classiﬁer whose target FPR is 0.01
while increasing TPR as high as possible. When the FPR
was 0.01, the TPR of the k-nearest neighbors algorithm was
0.96, the TPR of the Ada Boost algorithm was 0.95, and
the TPR of the Gaussian na¨ıve Bayes algorithm was 0.87.
Therefore, we selected the k-nearest neighbors algorithm as
our classiﬁer.

We also measured the area under the ROC curve (AUC)
values of the three algorithms. The AUC of the Ada Boost
algorithm was 0.994, the AUC of the k-nearest neighbors
algorithm was 0.991, and the AUC of the Gaussian na¨ıve
Bayes algorithm was 0.99.
6.3 Classiﬁcation with Click Information

Next, we distinguish the malicious tweets containing URLs
from the normal tweets containing URLs by additionally
considering how many times the URLs are clicked on. We
extracted tweets containing bit.ly and goo.gl links from
our dataset. Then, we classiﬁed them with a link-based fea-
ture: the ratio of the number of clicks to the number of
retweets. Since the k-nearest neighbors algorithm showed
the best results in Section 6.2, we only tested the algorithm
in this experiment for simplicity.

Figure 7: Ratio of unreachable retweeters per tweet.
Most crowdturﬁng and black-market retweets are
generated by unreachable retweeters who do not fol-
low the posting users.

Figure 8: Ratio of the number of clicks to the num-
ber of retweets per tweet. Unlike normal retweet-
ers, crowdturﬁng and black-market retweeters do
not click the URLs included in the retweeted tweets.

800Figure 10: ROC curve showing TPRs and FPRs of
CrowdTarget in distinguishing with click informa-
tion and without click information. We only test
k-nearest neighbors algorithm with 10-fold cross val-
idation.

Figure 10 compares the classiﬁcation results with and with-
out click information. CrowdTarget increased accuracy by
additionally considering click information. The TPR in-
creased from 0.95 to 0.98 at FPR of 0.01, and the AUC
increased from 0.989 to 0.993. Therefore, we conclude that
the click information is useful to detect the malicious tweets
with links.

The main shortcoming of this evaluation is that we cannot
check other links that do not associated with bit.ly and
goo.gl because we have no mechanism to obtain their click
information. We can solve the problem if we can access the
click information of t.co links in future (Section 7.4).
6.4 Error Analysis

In this section, we analyze the reasons of false negatives

and false positives.

6.4.1 False-negative analysis
We analyzed the malicious tweets that CrowdTarget could
not detect (i.e., false negatives) and ﬁgured out the follow-
ing three reasons. First, we observed that CrowdTarget
misjudged certain crowdturﬁng tweets that received a small
number of retweets. Figure 11a compares the number of
retweets of the detected crowdturﬁng tweets and that of the
undetected crowdturﬁng tweets. The undetected crowdturf-
ing tweets had a smaller number of retweets than that of
the detected crowdturﬁng tweets. Approximately 75% of
the undetected tweets were retweeted less than 100 times.
Although CrowdTarget cannot detect crowdturﬁng tweets
with a small number of retweets, it is not a serious problem
because their negative eﬀects against normal Twitter users
are limited.

Next, we discovered that the ratio of unreachable retweet-
ers led to more errors than other features in CrowdTarget.
Figure 11b shows that approximately 50% of the undetected
crowdturﬁng tweets were mostly retweeted by reachable ac-
counts; the ratio of unreachable retweeters were approxi-
mately 17%. We expect that the posting users of such un-
detected crowdturﬁng tweets bought followers on the same

(a) The number of retweets of detected and unde-
tected crowdturﬁng tweets

(b) The ratio of unreachable retweeters of de-
tected and undetected crowdturﬁng tweets

(c) The click ratio of detected and undetected
crowdturﬁng tweets

Figure 11: Comparisons between detected and un-
detected crowdturﬁng tweets

crowdturﬁng service, so that their tweets will be frequently
retweeted by shared followers.

Lastly, on the analysis of false negatives in the classiﬁca-
tion with click information, we recognized that a few links in

801the undetected crowdturﬁng tweets receive a larger number
of clicks than retweets (Figure 11c). We searched those links
on Twitter and found that they were distributed via many
other tweets. Therefore, we expect that the number of clicks
we measured is the aggregated number of clicks originated
from every tweet containing the same links. Unfortunately,
we cannot diﬀerentiate the number of clicks per tweet be-
cause bit.ly and goo.gl APIs only return domain name
when retrieving referrer information (e.g., t.co and twit-
ter.com). If we can access private data of bit.ly, goo.gl,
or Twitter, we can exclude clicks from other tweets such that
we can decrease the false-negative rate of CrowdTarget.

6.4.2 False-positive analysis
We manually analyzed the normal tweets classiﬁed as ma-
licious by CrowdTarget (i.e., false positives). Most of the
false positives are due to automated applications or embed-
ded tweets [2].

First, we found that tweets of a few veriﬁed accounts were
retweeted by automated applications. Table 2 shows ex-
amples of veriﬁed accounts that received retweets from the
automated applications. We visited homepages of the appli-
cations to know their purposes and identiﬁed that they are
automatic retweet applications. For example, TweetAdder
is a famous automated application that was sued by Twitter
due to its creation of many spam tweets [7]. Therefore, in
fact, these are not false positives.

Second, CrowdTarget classiﬁed the embedded tweets in
websites as malicious. Twitter oﬀer an application, “Twit-
ter Web Client”, to allow a user to embed his or her tweets
into a website. Any visitors of the website can retweet em-
bedded tweets. However, we cannot guarantee that the visi-
tors who have retweeted the embedded tweets are the user’s
followers. Consequently, the ratio of unreachable retweeters
of embedded tweets is higher than normal tweets such that
they can be misclassiﬁed. We think that if we can access
the private date of Twitter, e.g., IP addresses of retweeters,
we can avoid this problem.

7. FEATURE ROBUSTNESS

In this section, we discuss the robustness of our features

against feature fabrication attempts.
7.1 Retweet Time Distribution

Retweeters can cooperate each other to artiﬁcially manip-
ulate retweet time distributions. For the goal, they should
arrange a retweet time schedule similar with a normal retweet
time distribution and perform retweets as scheduled. How-
ever, it is diﬃcult to do that by themselves because crowd-
turﬁng workers act independently.

The crowdturﬁng services also can attempt to manipulate
the retweet time distributions. First, the services can manip-
ulate every boosting task of a worker by installing a program
at the worker’s device. However, it is a strong assumption
because the services need to persuade workers to install a
program or install the software without the perception of
workers.

Second, the services can handle every boosting task at the
server. The services collect the tasks of workers and trans-
mit the tasks to the target OSN when they wants. However,
OSNs can recognize such activities because the same IP ad-
dresses are frequently used.

Third, the services can use bot accounts to secretly per-
form tasks. CrowdTarget may not work correctly if the ser-
vices prepare an enough number of bot accounts to simulate
the retweet time distribution of normal tweets. However,
due to extra costs, we expect that the services would not
take this approach.
7.2 Twitter Application

The crowdturﬁng services can use a large number of Twit-
ter applications for evasion. By assigning diﬀerent appli-
cations to diﬀerent groups of workers, they can eliminate
dominant applications. However, they cannot arbitrary cre-
ate a large number of Twitter applications because Twitter
restricts the number of application creation per day and per
account. Furthermore, it is diﬃcult to exactly control the
ratio of the most dominant application, since workers can
retweet any tweet at any time.
7.3 Unreachable Retweeters

To reduce the number of unreachable retweeters, the crowd-
turﬁng services would request crowdturﬁng workers to fol-
low the posting user of a tweet they want to retweet. How-
ever, due to three important reasons, it is impractical. First,
workers should receive future tweets of the posting user even
if they do not want it. Second, increasing the number of fol-
lowings can decrease the popularity of workers on Twitter,
which is exactly opposite to their goal. Third, workers can-
not follow the posting user when the number of their follow-
ers is small or when they recently follow many accounts [27].
7.4 Click Information

To manipulate the number of clicks, the crowdturﬁng ser-
vices can request crowdturﬁng workers to click on a link
in a tweet while retweeting it. This approach could evade
CrowdTarget, but it has two problems. First, crowdturﬁng
workers unwilling to click on such a link because it may be
a malicious link (e.g., spam, phishing, and drive-by down-
loads). Second, we expect that the distributions of artiﬁcial
clicks in terms of time, geographical location, user agents,
and referrers diﬀer from those of real clicks. Note that all
links shared on Twitter are automatically shortened to t.co
links [28]. This allows Twitter to obtain detailed click in-
formation of all links on Twitter. Thus, generating realistic
click patterns by using crowdturﬁng workers would be a dif-
ﬁcult task. Unfortunately, to the best of our knowledge, no
crowdturﬁng service currently manipulates the number of
click such that we cannot conﬁrm our expectation. There-
fore, in future, we will show how much eﬀort is necessary to
produce realistic click distributions.

8. RELATED WORK

In this section, we explain related studies of our work.

8.1 Detection of Crowdturﬁng Accounts

Malicious crowdsourcing has recently received consider-
able attention. Motoyama et al. [20] analyze various types
of abuse tasks in Freelancer, one of the most popular crowd-
sourcing site. Wang et al. [33] collect data from crowdturf-
ing sites based in China, Zhubajie and Sandaha, and analyze
their structures, scale, and the amount of money involved in
it.

Several researchers propose methods to detect crowdturf-
ing aiming at OSNs. Lee et al. [18] and Wang et al. [32] aim

802Table 2: Example of veriﬁed accounts that received retweets from accounts using automated applications

Veriﬁed accounts Application name

Application homepage

PopWrapped
m bukairy
ODEONCinemas
alweeamnews
CaesarsPalace
Almatraﬁ
MohammadMamou KLILK API RETWEET http://www.klilk.com

http://tweetadder.com
http://www.rtwity.com
http://twitaculous.com
http://twittretweet.com
http://web.socialrewards.com
http://rettwwet.net

TweetAdder
rtwity
Twitaculous
twittretweet EEE
Social Rewards
rettwwet net

to detect OSN accounts performing crowdturﬁng tasks on
Twitter and Weibo, respectively. These studies use account-
based features introduced in conventional spam detection
studies, such as the ratio of tweets including links, the num-
ber of tweets per day, and the number of retweets per tweet.
Lee et al. [19] detect malicious tasks targeting Twitter in
Fiverr, one of the popular crowdsourcing site.
8.2 Detection of Malicious Accounts

There are a large number of studies of detecting mali-
cious accounts in OSNs. We classify them into three types:
account-based methods, graph-based methods, and behavior-
based methods. First, account-based methods [12, 17, 23, 34,
35] extract various features from user proﬁles and postings,
and use them to build machine-learning classiﬁers. Second,
graph-based methods [9, 10, 13, 30, 36, 37] detect malicious
accounts by using the observation that malicious accounts
usually have few connections with normal accounts. Third,
recent researchers detect malicious accounts by monitoring
their synchronized group activity. For example, COMPA [15]
detects compromised accounts by catching similar changes
of account behavior within a short time. Clickstream [31]
classiﬁes accounts based on the similarity of clickstream se-
quences. CopyCatch [8] and SynchroTrap [11] detect ma-
licious accounts that have synchronized Facebook like pat-
terns. CatchSync [16] uses synchronicity and normality of
accounts to detect malicious accounts.
8.3 Detection of Black-market Accounts

Some researchers focus on black markets for OSNs. Stringh-

ini et al. [24] analyze Twitter follower markets. They de-
scribe characteristics of Twitter follower markets and clas-
sify customers of the markets. Thomas et al. [25] inves-
tigate black-market accounts used for distributing Twitter
spams. Cristofaro et al. [14] analyze Facebook like farms
by deploying honeypot pages. Viswanath et al. [29] detect
black-market Facebook accounts based on their like behav-
iors.

9. CONCLUSION

In this paper, we proposed a novel crowdturﬁng detection
method using target objects of crowdturﬁng tasks, Crowd-
Target. We observed that the manipulation patterns of
the target objects maintained, regardless of what evasion
techniques crowdturﬁng accounts used. Through the ob-
servation, we distinguished tweets that received retweets by
crowdturﬁng sites from tweets that received retweets by nor-
mal Twitter users. Evaluation results showed that Crowd-
Target could detect crowdturﬁng retweets on Twitter with
TPR of 0.98 at FPR of 0.01.

Acknowledgments
We would like to appreciate our shepherd Guoliang Xue and
anonymous reviewers for their invaluable comments and sug-
gestions. This work was supported by ICT R&D program of
MSIP/IITP. [14-824-09-013, Resilient Cyber-Physical Sys-
tems Research]

10. REFERENCES
[1] Addmefast. http://addmefast.com/.
[2] Embedded tweets.

https://dev.twitter.com/web/embedded-tweets/.

[3] Klout. https://klout.com/.
[4] Retweets.pro. http://retweets.pro/.
[5] Socialshop. http://socialshop.co/.
[6] Traﬀup. http://traffup.net/.
[7] Twitter reaches spam lawsuit settlement with tweet

adder.
http://marketingland.com/twitter-reaches-spam-
lawsuit-settlement-with-tweet-adder-45890/.
[8] A. Beutel, W. Xu, V. Guruswami, C. Palow, and

C. Faloutsos. CopyCatch: Stopping group attacks by
spotting lockstep behavior in social networks. In
International World Wide Web Conference (WWW),
2013.

[9] Y. Boshmaf, D. Logothetis, G. Siganos, J. Ler´ıa,

J. Lorenzo, M. Ripeanu, and K. Beznosov. ´Integro:
Leveraging victim prediction for robust fake account
detection in OSNs. In Network and Distributed System
Security Symposium (NDSS), 2015.

[10] Q. Cao, M. Sirivianos, X. Yang, and T. Pregueiro.
Aiding the detection of fake accounts in large scale
social online services. In USENIX Symposium on
Networked Systems Design and Implementation
(NSDI), 2012.

[11] Q. Cao, X. Yang, J. Yu, and C. Palow. Uncovering
large groups of active malicious accounts in online
social networks. In ACM Conference on Computer and
Communications Security (CCS), 2014.

[12] Z. Chu, S. Gianvecchio, H. Wang, and S. Jajodia.

Who is tweeting on Twitter: Human, bot, or cyborg?
In Annual Computer Security Applications Conference
(ACSAC), 2010.

[13] G. Danezis and P. Mittal. SybilInfer: Detecting Sybil

nodes using social networks. In Network and
Distributed System Security Symposium (NDSS), 2009.

[14] E. De Cristofaro, A. Friedman, G. Jourjon, M. A.

Kaafar, and M. Z. Shaﬁq. Paying for likes?:
Understanding Facebook like fraud using honeypots.
In Internet Measurement Conference (IMC), 2014.

803[15] M. Egele, G. Stringhini, C. Kruegel, and G. Vigna.

[26] Twitter. State of Twitter spam.

COMPA: Detecting compromised accounts on social
networks. In Network and Distributed System Security
Symposium (NDSS), 2013.

[16] M. Jiang, P. Cui, A. Beutel, C. Faloutsos, and

S. Yang. CatchSync: Catching synchronized behavior
in large directed graphs. In ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining (KDD), 2014.

[17] K. Lee, J. Caverlee, and S. Webb. Uncovering social
spammers: Social honeypots + machine learning. In
International ACM SIGIR Conference on Research
and Development in Information Retrieval, 2010.

[18] K. Lee, P. Tamilarasan, and J. Caverlee.

Crowdturfers, campaigns, and social media: Tracking
and revealing crowdsourced manipulation of social
media. In International AAAI Conference on Web and
Social Media (ICWSM), 2013.

[19] K. Lee, S. Webb, and H. Ge. The dark side of

micro-task marketplaces: Characterizing Fiverr and
automatically detecting crowdturﬁng. In International
AAAI Conference on Web and Social Media
(ICWSM), 2014.

[20] M. Motoyama, D. McCoy, K. Levchenko, S. Savage,
and G. M. Voelker. Dirty jobs: The role of freelance
labor in web service abuse. In USENIX Security
Symposium, 2011.

[21] Scikit-learn. https://http://scikit-learn.org.
[22] J. Song, S. Lee, and J. Kim. I know the shortened

URLs you clicked on Twitter: Inference attack using
public click analytics and Twitter metadata. In
International World Wide Web Conference (WWW),
2013.

[23] G. Stringhini, C. Kruegel, and G. Vigna. Detecting
spammers on social networks. In Annual Computer
Security Applications Conference (ACSAC), 2010.

[24] G. Stringhini, G. Wang, M. Egele, C. Kruegel,

G. Vigna, H. Zheng, and B. Y. Zhao. Follow the green:
growth and dynamics in Twitter follower markets. In
Internet Measurement Conference (IMC), 2013.
[25] K. Thomas, D. McCoy, C. Grier, A. Kolcz, and

V. Paxson. Traﬃcking fraudulent accounts: The role
of the underground market in Twitter spam and
abuse. In USENIX Security Symposium, 2013.

https://blog.twitter.com/2010/state-twitter-
spam.

[27] Twitter Blogs. Following rules and best practices.
https://support.twitter.com/entries/68916-
following-rules-and-best-practices.

[28] Twitter Blogs. Next steps with the t.co link wrapper,
2011. https://blog.twitter.com/2011/next-steps-
with-the-tco-link-wrapper.

[29] B. Viswanath, M. A. Bashir, M. Crovella, S. Guha,
K. P. Gummadi, B. Krishnamurthy, and A. Mislove.
Towards detecting anomalous user behavior in online
social networks. In USENIX Security Symposium,
2014.

[30] B. Viswanath, A. Post, K. P. Gummadi, and

A. Mislove. An analysis of social network-based Sybil
defenses. In ACM SIGCOMM, 2010.

[31] G. Wang, T. Konolige, C. Wilson, X. Wang, H. Zheng,

and B. Y. Zhao. You are how you click: Clickstream
analysis for Sybil detection. In USENIX Security
Symposium, 2013.

[32] G. Wang, T. Wang, H. Zheng, and B. Y. Zhao. Man

vs. machine: Practical adversarial detection of
malicious crowdsourcing workers. In USENIX Security
Symposium, 2014.

[33] G. Wang, C. Wilson, X. Zhao, Y. Zhu, M. Mohanlal,

H. Zheng, and B. Y. Zhao. Serf and turf:
Crowdturﬁng for fun and proﬁt. In International
World Wide Web Conference (WWW), 2012.

[34] C. Yang, R. C. Harkreader, and G. Gu. Die free or live
hard? empirical evaluation and new design for ﬁghting
evolving Twitter spammers. In Recent Advances in
Intrusion Detection, pages 318–337. Springer, 2011.

[35] Z. Yang, C. Wilson, X. Wang, T. Gao, B. Y. Zhao, and

Y. Dai. Uncovering social network Sybils in the wild.
In Internet Measurement Conference (IMC), 2011.
[36] H. Yu, P. B. Gibbons, M. Kaminsky, and F. Xiao.
SybilLimit: A near-optimal social network defense
against Sybil attacks. In IEEE Symposium on Security
and Privacy (Oakland), 2008.

[37] H. Yu, M. Kaminsky, P. B. Gibbons, and A. Flaxman.
SybilGuard: Defending against Sybil attacks via social
networks. In ACM SIGCOMM, 2006.

804