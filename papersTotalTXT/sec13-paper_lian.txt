Measuring the Practical Impact  

of DNSSEC Deployment

Wilson Lian, University of California, San Diego; Eric Rescorla, RTFM, Inc.;  
Hovav Shacham and Stefan Savage, University of California, San Diego

Open access to the Proceedings of the 22nd USENIX Security Symposium is sponsored by USENIXThis paper is included in the Proceedings of the 22nd USENIX Security Symposium.August 14–16, 2013 • Washington, D.C., USAISBN 978-1-931971-03-4Measuring the practical impact of DNSSEC Deployment

Wilson Lian
UC San Diego

Eric Rescorla
RTFM, Inc.

Hovav Shacham
UC San Diego

Stefan Savage
UC San Diego

Abstract
DNSSEC extends DNS with a public-key infras-
tructure, providing compatible clients with crypto-
graphic assurance for DNS records they obtain, even
in the presence of an active network attacker. As
with many Internet protocol deployments, admin-
istrators deciding whether to deploy DNSSEC for
their DNS zones must perform cost/beneﬁt analy-
sis. For some fraction of clients — those that perform
DNSSEC validation — the zone will be protected
from malicious hijacking. But another fraction of
clients — those whose DNS resolvers are buggy and
incompatible with DNSSEC — will no longer be able
to connect to the zone. Deploying DNSSEC requires
making a cost-beneﬁt decision, balancing security for
some users with denial of service for others.

We have performed a large-scale measurement of
the eﬀects of DNSSEC on client name resolution us-
ing an ad network to collect results from over 500,000
geographically-distributed clients. Our ﬁndings cor-
roborate those of previous researchers in showing
that a relatively small fraction of users are protected
by DNSSEC-validating resolvers. And we show, for
the ﬁrst time, that enabling DNSSEC measurably in-
creases end-to-end resolution failures. For every 10
clients that are protected from DNS tampering when
a domain deploys DNSSEC, approximately one or-
dinary client (primarily in Asia) becomes unable to
access the domain.

Introduction

1
The Domain Name System (DNS) [32], used to map
names to IP addresses, is notoriously insecure; any
active attacker can inject fake responses to DNS
queries, thus corrupting the name → address map-
ping. In order to prevent attacks on DNS integrity,
the Internet Engineering Task Force (IETF) has
developed DNSSEC [4], a set of DNS extensions
which allows DNS records to be digitally signed, thus
preventing—or at least detecting—tampering.

Over the past several years, public enthusiasm for
DNSSEC has increased signiﬁcantly. In July 2010,
the DNSSEC root zone (containing all top level do-
mains) was signed; in March 2011, .com, the largest
top level domain, was signed; in January 2012, Com-
cast announced that they had switched all of their
DNS resolvers to do DNSSEC validation and that

they had DNSSEC-signed all customer domains they
were serving [30]. Moreover, protocol designs which
depend on DNSSEC have started to emerge. For
instance, DANE [20] is a DNS extension that uses
DNS to authenticate the name → public key binding
for SSL/TLS connections. Obviously, DANE is not
secure in the absence of DNSSEC, since an attacker
who can man-in-the-middle the SSL/TLS connec-
tion can also forge DNS responses.

Despite the eﬀort being poured into DNSSEC,
actual deployment of signed records at the end-
system level has remained quite limited. As of
February 2013, VeriSign Labs’ Scoreboard1 mea-
sured 158,676 (.15%) of .com domains as secured
with DNSSEC. As with many Internet protocol de-
ployments, there is a classic collective action prob-
lem: because the vast majority of browser clients do
not verify DNSSEC records or use resolvers which
do, the value to a server administrator of deploying
a DNSSEC-signed zone is limited. Similarly, because
zones are unsigned, client applications and resolvers
have very little incentive to perform DNSSEC vali-
dation.

A zone administrator deciding whether to deploy

DNSSEC must weigh the costs and beneﬁts of:

• The fraction of clients whose resolvers validate
DNSSEC records and therefore would be able
to detect tampering if it were occurring and
DNSSEC were deployed.

• The fraction of clients which fail with valid
DNSSEC records and therefore will be unable
to reach the server whether or not tampering is
occurring.

In this paper, we measure these values by means of a
large-scale study using Web browser clients recruited
via an advertising network. This technique allows us
to sample a cross-section of browsers behind a vari-
ety of network conﬁgurations without having to de-
ploy our own sensors. Overall, we surveyed 529,294
unique clients over a period of one week. Because of
the scale of our study and the relatively small error
rates we were attempting to quantify, we encoun-
tered several pitfalls that can arise in ad-recruited

1Online: http://scoreboard.verisignlabs.com/. Vis-

ited 20 February 2013.

USENIX Association  

22nd USENIX Security Symposium  573

1

browser measurement studies. Our experience may
be relevant to others who wish to use browsers for
measurements, and we describe some of these results
in Section 4.2.

Ethics. Our experiment runs automatically with-
out user interaction and is intended to measure the
behavior and properties of hosts along the paths
from users to our servers rather than the users them-
selves. We worked with the director of UC San
Diego’s Human Research Protections Program, who
certiﬁed our study as exempt from IRB review.

2 Overview of DNS and DNSSEC

A DNS name is a dot-separated concatenation of
labels; for example, the name cs.ucsd.edu is com-
prised of the labels cs, ucsd, and edu. The DNS
namespace is organized as a tree whose nodes are
the labels and whose root node is the empty string
label. The name corresponding to a given node in
the tree is the concatenation of the labels on the path
from the node to the root, separated by periods.

Associated with each node are zero or more re-
source records (RRs) specifying information of dif-
ferent types about that node. For example, IP ad-
dresses can be stored with type A or AAAA RRs,
and the name of the node’s authoritative name
servers can be stored in type NS RRs. The set of
all RRs of a certain type2 for a given name is re-
ferred to as a resource record set (RRset).

2.1 Delegation

DNS is a distributed system, eliminating the need
for a central entity to maintain an authoritative
database of all names. The DNS namespace tree
is broken up into zones, each of which is owned
by a particular entity. Authority over a subtree
in the domain namespace can be delegated by the
owner of that subtree’s parent. These delegations
form zone boundaries. For example, a name reg-
istrar might delegate ownership of example.com
to a customer, forming a zone boundary between
.com and example.com while making that customer
the authoritative source for RRsets associated with
example.com and its subdomains. The customer can
further delegate subdomains of example.com to an-
other entity. Figure 1 depicts an example DNS tree.

2.2 Address resolution

The most important DNS functionality is the reso-
lution of domain names to IP addresses (retrieving

2And class, but for our purposes class is always IN, for

“Internet.”

.

(Root)

org

edu

com

ucsd.edu

google.com

cs.ucsd.edu

sysnet.ucsd.edu

mail.google.com

www.google.com

dnsstudy.sysnet.ucsd.edu

Figure 1: Example DNS name tree. Shaded boxes
represent zone boundaries. Edges that cross zone
boundaries are delegations.

(1) cs.ucsd.edu IN A?

(10) cs.ucsd.edu IN A 

1.2.3.4

LAN
Router

Desktop PC

(2) cs.ucsd.edu IN A?

(3) cs.ucsd.edu IN A?

(9) cs.ucsd.edu IN A

1.2.3.4

(4) edu IN NS a.edu-servers.net
a.edu-servers.net IN A x.x.x.x

Root DNS Server

(5) cs.ucsd.e d u IN A ?

ISP's Recursive
DNS Resolver

(6) ucsd.edu IN NS ns0.ucsd.edu

ns0.ucsd.edu IN A y.y.y.y

edu DNS Server
a.edu-servers.net

x.x.x.x

(8) cs.ucsd.edu IN A

1.2.3.4

(7) cs.ucsd.e

d

u IN A

?

ucsd.edu DNS Server

ns0.ucsd.edu

y.y.y.y

Figure 2: Simpliﬁed DNS address resolution pro-
cedure for cs.example.tld. In this example, there
are at most one nameserver and one IP address per
name.

type A or AAAA RRsets). Domain name resolu-
tion is performed in a distributed, recursive fashion
starting from the root zone, as shown in Figure 2.
Typically, end hosts do not perform resolution them-
selves but instead create DNS queries and send them
to recursive resolvers, which carry out the resolution
to completion on their behalfs. When a nonrecursive
DNS server receives a query that it cannot answer, it
returns the name and IP address of an authoritative
name server as far down as possible along the path
to the target domain name. The recursive resolver
then proceeds to ask that server.
In this fashion,
the query eventually reaches a server that can an-
swer the query, and the resolution is complete. This
recursive process is bootstrapped by hardcoding the
names and IP addresses of root nameservers into end
hosts and recursive resolvers.

574  22nd USENIX Security Symposium 

USENIX Association

2

2.3 DNS (in)security

The original DNS design did not provide any mecha-
nisms to protect the integrity of DNS response mes-
sages. Thus, an active network attacker can launch
a woman-in-the-middle attack to inject her own re-
sponses which would be accepted as if they were le-
gitimate. This attack is known as DNS spooﬁng.
Moreover, because recursive resolvers typically cache
responses, a single spoofed response can be used to
perform a DNS cache poisoning attack, which re-
sults in future responses to requests for the same
RRset returning the bogus spoofed response. The
mechanisms by which DNS cache poisoning is car-
ried out are outside the scope of this work but have
been studied more formally in [38]. DNS spooﬁng
and cache poisoning may be used to compromise any
type of DNS RR.

2.4 DNSSEC to the rescue

The Domain Name System Security Extensions
(DNSSEC) [4], aim to protect against DNS spoof-
ing attacks by allowing authoritative nameservers to
use public key cryptography to digitally sign RRsets.
Security-aware recipients of a signed RRset are able
to verify that the RRset was signed by the holder of
a particular private key, and a chain of trust from
the root zone downwards ensures that a trusted key
is used to validate signatures.

While DNSSEC adds a number of new RR types,
the DNSKEY, RRSIG, DS only the records are rele-
vant for our purposes; we describe them brieﬂy here.
DNSKEY: DNSKEY records are used to hold
public keys. Each zone authority generates at least
one public/private key pair, using the private keys
to sign RRsets and publishing the public keys in
Domain Name System Key (DNSKEY) resource
records.

RRSIG: When a zone is signed, a resource record
signature (RRSIG) resource record is generated for
each RRset-public key pair. In addition to contain-
ing a cryptographic signature and the name and type
of the RRset being signed, the RRSIG RR speciﬁes
a validity window and the name of the signing key’s
owner.

DS: Lastly, the Delegation Signer (DS) RR type
links signed zones to establish the chain of trust.
Each DS RR contains the digest of one of the sub-
zone’s DNSKEY RRs.

DNSSEC’s security is built on the chain of trust
model. Starting from a “trust anchor,” a validator
attempts to trace a chain of endorsements from the
root all the way to the RRset being validated; I.e.,
that each DNSKEY or DS record along the path and
the ﬁnal RRSet is correctly signed by the parent’s

If a chain of trust can be constructed
public key.
all the way to the trust anchor, then the validating
resolver can have conﬁdence that the information in
that RR is correct — or at least that it is crypto-
graphically authenticated.

Because DNSSEC is a retroﬁt onto the exist-
ing insecure DNS, it is explicitly designed for in-
cremental deployment, and insecure (i.e., unsigned)
domains can coexist with secure domains. Thus,
DNSSEC-capable resolvers should be able to re-
solve unsigned domains, and non-DNSSEC resolvers
should be able to resolve DNSSEC-signed domains,
though of course they will not gain any security
value. In order to make this work, DNSSEC records
are designed to be backwards-compatible with ex-
isting resolvers, and DNSSEC resolvers are able to
distinguish zones which simply are not signed from
those which are signed but from which an attacker
has stripped the signatures (the DS record is used
for this purpose).

Unfortunately, while DNSSEC is designed to be
it is known [9] that there
backwards compatible,
are some network elements which do not process
DNSSEC records properly. The purpose of this work
is to determine the frequency of such elements and in
particular their relative frequency to elements which
actually validate DNSSEC signatures and thus ben-
eﬁt from its deployment.

3 Methodology
In order to address this question, we conducted a
large-scale measurement study of web browsers in
the wild. In particular, we sought to measure two
quantities:

• What fraction of clients validate DNSSEC
records and therefore would be able to detect
tampering if it were occurring and DNSSEC
were deployed?

• What fraction of clients fail with valid DNSSEC
records and therefore will be unable to reach the
server whether or not tampering is occurring?

Answering these questions requires taking mea-
surements from a large number of clients. We gath-
ered our clients by purchasing ad space from an on-
line advertising network; the ad network enabled us
to host an ad at a ﬁxed URL which would be loaded
in an iframe on various publishers’ web sites. Our
ad included JavaScript code to drive the experiment
and was executed without any user interaction upon
the loading of the ad iframe in clients’ browsers. In
order to minimize sampling bias, our ad campaign
did not target any particular keywords or countries.

USENIX Association  

22nd USENIX Security Symposium  575

3

However, because our measurements were sensitive
to the reliability of the participants’ Internet con-
nections, we conﬁgured our ad campaign to target
desktop operating systems, to the exclusion of mo-
bile users.

Publisher's web page
Ad-network iframe 1
Ad-network iframe 2
Static ad URL iframe
Measurement page

Our client-side “driver script” (discussed in detail

in § 3.1) induces participants’ browsers to load 1×1-

pixel images (“test resources”) from various domains.
This is a standard technique for inducing the browser
to load resources from diﬀerent origins than the con-
taining document. These domains fall into the fol-
lowing three classes:

Dummy 
image

Driver 
script

jQuery.js

JSON lib

Test 
resource

Test 
resource

...

Test 
resource

• nosec — without DNSSEC

• goodsec — with correctly-conﬁgured DNSSEC

• badsec — with DNSSEC against which we simu-
late misconﬁguration or tampering by an active
network attacker

The goodsec and badsec zones were signed with

1024-bit keys3 using RSA-SHA1.

If we observe an HTTP request for a test resource,
we conclude that the participant’s browser was able
to resolve that type of domain. Otherwise, we con-
clude that it was not.

These three domain classes allow us to assess the
client/resolver’s DNSSEC behavior. The nosec do-
main class serves as a control, representing the state
of the majority of the sites on the web. Failed loads
from the goodsec domain class allow us to measure
the fraction of clients which would not be able to
reach a DNSSEC-enabled site, even in the absence
of an attack. Failed loads from the badsec domain
class tell us about the fraction of clients which detect
and react to DNSSEC tampering.

During each ad impression, the driver script at-
tempts to resolve and load a total of 27 test re-
sources. They are distributed as follows: one nosec
domain, one goodsec domain, and 25 diﬀerent badsec
domains. Each badsec variant simulates an attack
against DNSSEC at a diﬀerent point in the chain of
trust, and as we will see in Section 4, certain vali-
dating resolvers exhibit bugs that cause some badsec
domains to be treated as correctly-signed.

3.1 Client-side experiment setup

Figure 3 shows how our driver script is embedded in
an ad in a publisher’s web page. We provide the ad
network with an ad located at a static URL which is
wrapped in an iframe by the ad network. The pub-
lisher places an iframe in its web page whose source

3We attempted to use 2048-bit keys, but at the time of the
experiment, our domain registrar, GoDaddy, did not support
keys that large.

Figure 3: Client-side experiment setup

points to the iframe wrapping the ad. Our ad page
residing at the static URL iframes the measure-
ment page, which contains the JavaScript driver
program. Each instance of the measurement page
and all requests generated by it are linked by a ver-
sion 4 UUID [29] placed in both the URL query
string and the domain name (with the exception
of the measurement page, which only has it in the
query string).

The measurement page loads a dummy ad image
and 3 pieces of JavaScript which are the following:

• A miniﬁed jQuery4 [26]

library hosted by

jquery.com

• A JSON encoding and decoding library hosted

on our servers

• The experiment’s JavaScript “driver script”

The measurement page body’s onLoad handler
commences the experiment by invoking the driver
script. The driver script randomizes the order of a
list of nosec, goodsec, and badsec domains then it-
erates over that list, creating for each domain an
image tag whose source property points to an image
hosted on that domain. The creation of the image
tag causes the participant’s browser to attempt to
resolve the domain name and load an image from it.
Because we need to gather data for all domains in the
list before the participant navigates away from the
web page containing the ad, the driver script does
not wait for each image to complete its load attempt
before proceeding to the next domain.
Instead, it
creates all of the image tags in rapid succession. The
driver script also registers onLoad and onError call-
backs on each image tag created to monitor whether
each load succeeds or fails. When a callback for
an image ﬁres, the outcome of the load, along with

4We used jQuery to minimize browser compatibility issues.

576  22nd USENIX Security Symposium 

USENIX Association

4

info about the browser, are sent via jQuery’s AJAX
POST mechanism to a PHP logging script on our
servers. Once the driver script detects that all im-
age tags have invoked either an onLoad or onError
callback, it creates a ﬁnal image tag whose source
domain is a unique nosec domain (UUID.complete.
dnsstudy.ucsd.edu). A DNS query for such a do-
main serves as a “completion signal” and allows us
to identify UUIDs where the user did not navigate
away from the ad publisher’s page before completing
the trial. We discarded the data from any page load
which did not generate the completion signal.

3.2 Identifying successful DNS resolu-

tion

Our original intent was to use onLoad and onError
handlers attached to each test resource’s image tag
to measure the outcome of the HTTP requests for
test resources. If the onLoad handler was called, we
would record a successful HTTP request; if instead
the onError handler was called, we would record
a failed HTTP request. These results are reported
back to our servers via AJAX POST. However, we
found 9754 instances of the onError handler ﬁr-
ing, the test resource subsequently being loaded, and
then the onLoad handler ﬁring. For another 1058
test resource loads, the onLoad handler ﬁred, despite
our receiving neither the corresponding DNS lookups
nor the HTTP requests for the test resources in ques-
tion. Consequently, we looked to diﬀerent avenues
for identifying resolution success.

Because we are not able to ascertain the result of
a DNS lookup attempt via direct inspection of the
DNS caches of our participants and their recursive
resolvers, we must infer it from the presence of an
HTTP request whose Host header or request line
speciﬁes a particular test resource’s domain name as
an indicator of DNS resolution success. Thus, if we
observed a completion signal for a particular UUID
but did not observe an HTTP request associated
with that UUID for a certain test resource type, we
infer that the DNS resolution for that UUID-test re-
source pair failed. Note however that we can record
a completion signal after observing just a DNS query
for it: what matters is whether the driver script at-
tempted to load the completion signal resource, not
whether it succeeded in doing so.

This strategy has the potential to over-estimate
the number of DNS resolution failures due to TCP
connections that are attempted and are dropped or
aborted before the HTTP request is received by our
servers. The only source of this type of error that we
are able to control is our HTTP servers’ ability to
accept the oﬀered TCP-connection load at all times

throughout the experiment. We describe our serving
infrastructure in Section 3.4. We believe it is suﬃ-
ciently robust against introducing this type of error.

3.3 Cache control

Because requests fulﬁlled by cache hits do not gen-
erate HTTP and DNS logs that we can analyze, we
took measures, described in Table 1, to discourage
caching. Most importantly, the use of a fresh, ran-
dom UUID for each ad impression serves as a cache-
buster, preventing cache hits in both DNS resolvers
and browsers.

If, despite our eﬀorts, our static ad page is cached,
causing the measurement page to be requested with
a cached UUID, we must detect it and give the cur-
rent participant a fresh UUID. To this end, we used
a memcached cache as a UUID dictionary to detect
when the measurement page was loaded with a stale
UUID. If this occured, the stale measurement page
was redirected to one with a fresh UUID.

3.4 Serving infrastructure

To run our study, which generates large bursts of
traﬃc, we rented 5 m1.large instances running
Ubuntu 10.04 on Amazon’s Elastic Compute Cloud
(EC2). All 5 instances hosted identical BIND 9
(DNS), nginx (HTTP), and beanstalkd (work queue)
servers. The nginx servers supported PHP 5 CGI
scripts via FastCGI. Tables 2 and 3 show the adjust-
ments made to the servers’ conﬁguration parameters
to ensure a low rate of dropped connections.

One instance ran a MySQL server, another ran a
memcached server. To increase our EC2 instances’
ability to accept large quantities of short TCP con-
nections, we conﬁgured our machines to timeout con-
nections in the FIN-WAIT-2 state after only a frac-
tion of the default time and to quickly recycle con-
nections in the TIME-WAIT state. This was accom-
plished by setting the sysctl variables tcp ﬁn timeout
and tcp tw recycle to 3 and 1, respectively.

3.4.1 DNS & BIND 9

All 5 EC2 instances ran identical BIND 9 DNS
servers providing authoritative DNS resolution for
all nosec, goodsec, and badsec domains. We used
Round Robin DNS to distribute load across all 5
DNS and web servers. In order to reduce the chance
of load failures due to large reply packets, our DNS
servers were conﬁgured (using BIND’s minimal-
responses option) to refrain from sending unso-
licited RRs that are not mandated by the DNS
speciﬁcation. Speciﬁcally, we only send the extra
DNSSEC RRs in response to queries which include
the DNSSEC OK option (approximately two thirds
of all queries).

USENIX Association  

22nd USENIX Security Symposium  577

5

Type

Value

Used on

HTTP header

Cache-Control: no-cache, must-revalidate

HTTP header

Expires: Sat, 26 Jul 1997 00:00:00 GMT

HTML <meta>
HTML <meta>

http-equiv="Pragma" content="no-cache"

http-equiv="Expires" content="−1"

static ad page, measurement page,
driver script
static ad page, measurement page,
driver script
static ad page, measurement page
static ad page, measurement page

Table 1: Description of the HTTP and HTML anti-caching measures and their uses.

worker processes
worker rlimit noﬁle
worker connections

8
65,535
65,000

Table 2: Non-default nginx server conﬁg params.

PHP FCGI CHILDREN
PHP FCGI MAX REQUESTS

50
65,000

Table 3: Non-default PHP FastCGI conﬁg params.

Our 5 BIND servers are authoritative for all do-
main names used in our study except for the domain
of the static ad URL that iframes the measurement
page. Because we were not interested in measuring
resolution of those domains we hosted their author-
itative servers on Amazon Route 53 DNS service.

3.5 Data gathering

Our analysis of
the behavior of participants’
browsers and resolvers is based on the following 3
data sources: nginx access logs, BIND request logs,
and MySQL tables containing the outcomes and
browser info reported by AJAX POST messages.

Nginx was conﬁgured to use its default “common
log format” [34], which includes a timestamp of each
request, the URL requested, the user agent string,
among other details about the request and its cor-
responding response. However, BIND’s log format
is hardcoded and compiled into the binary. Its de-
fault logging behavior only provides basic informa-
tion about queries (e.g., a timestamp, the domain
name, the source IP and port).
It does not pro-
vide information about replies and excludes certain
important diagnostic ﬁelds. We modiﬁed and re-
compiled BIND to add enhanced logging of requests
and replies. Log lines for requests were modiﬁed to
include the request’s transaction ID and the value
of the sender’s UDP payload size from the EDNS0
OPT RR (if present) [39]. We added support for re-
ply logs that include the transport-layer protocol in
use, the size of the reply, and the transaction ID.

With these additional log details, we are able to
link requests to replies and determine if a lookup
fell back to TCP due to truncation of the UDP re-
ply. BIND logs are also used to identify the UUIDs
for which a completion signal was sent as well as
to determine which resolvers were associated with a
particular UUID.

The client-side driver script AJAX POSTs the out-
come of each test resource load along with additional
metadata regarding the experiment and the state of
the browser environment under which it is running.
These data are logged by our servers.

3.6 Experiment scheduling

In our preliminary test runs of the study, we found
that the successful load rates for test resources varied
depending on the time of day at which the experi-
ment was conducted. To account for this variability,
we conducted an extended study lasting for a full
week. Every two hours, we paid ad network enough
for 10,000 impressions to be shown.

4 Results

In this section we describe the results of our measure-
ments. We begin by providing an overview of our
data. Then, in Section 4.1 we describe our measure-
ments of the diﬀerential impact of DNSSEC on reso-
lution success. Finally, in Section 4.2, we describe a
number of confounding network artifacts that plague
any attempt to use advertisement surveys to mea-
sure small signals against the background of a noisy
Web environment.

Over the course of the 84 segments of our week-
long experiment, we collected data from 529,294
ad impressions, receiving DNS queries from 35,010
unique DNS resolvers. Figure 4 shows the distri-
bution of unique resolvers performing resolution for
each UUID. The distribution has a long tail, al-
though 98% of UUIDs used at most 25 resolvers. We
mapped each resolver’s IP address to its ASN and
found that 92.75% of the clients surveyed were ob-
served using recursive resolvers whose IP addresses
resided in the same ASN, and 99.12% used resolvers

578  22nd USENIX Security Symposium 

USENIX Association

6

I

s
D
U
U

 
f
o
 
n
o
i
t
c
a
r
F

4
.
0

3
.
0

2
.
0

1
.
0

0
.
0

Distribution of unique resolvers per UUID

i

d
e
v
e
c
e
r
 
s
t
s
e
u
q
e
r
 

P
T
T
H
 
c
e
s
o
n

0
0
0
0
8

0
0
0
0
6

0
0
0
0
4

0
0
0
0
2

0

2

3

4

6

5
7
# resolvers

8

9

10

280

0

10

30

20
50
Minutes since run started

40

60

70

Figure 4: Distribution of the number of unique
resolvers observed performing DNS resolution per
UUID. Tail not shown.

Figure 5: Plot showing total number of requests
received during each minute after the start of a run,
aggregated over all runs.

in two or fewer ASNs. This is consistent with our
expectation that most users use their default DNS
resolvers provided by their ISPs, while a small per-
centage of “power users” might conﬁgure their sys-
tems to take advantage of open resolvers such as
Google Public DNS.5

As shown in Figure 5 each ad buy results in a
delay of approximately 20 minutes from the time we
released funds to the ad network, at which point im-
pressions start to appear. Incoming traﬃc spikes for
15 minutes, peaking around 25 minutes into the run
and tapering oﬀ for the remainder of the ﬁrst hour.
We also witnessed considerable drop-oﬀ at each
stage of executing experiment code in the partic-
pants’ browsers. Figure 6 illustrates the number of
UUIDs observed reaching each stage of the experi-
ment. 15.88% of the ad impressions that we paid for
did not even manage to load the driver script and
only 63.02% of the impressions we paid for actually
resulted in a completed experiment. This compares
favorably with past studies. For instance, prior work
by Huang et al. [22], which also used ad networks
to recruit participants to run experiment code, had
only a 10.97% total completion rate.

4.1 DNSSEC Resolution Rates

The ﬁrst question we are interested in answering
is the impact on load failure rates of introducing
DNSSEC for a given domain. Table 4 shows the

5https://developers.google.com/speed/public-dns/

Class

Failure rate CI 0.99

nosec
goodsec
badsec

0.7846%
1.006%
2.661%

0.7539% - 0.8166%
0.9716% - 1.042%
2.649% - 2.672%

Table 4: Failure rates for each class of test re-
source.

raw failure rates across each class of test resource,
where the failure rate is deﬁned as one minus the
quotient of the number of successful test resource
loads and the number of attempted resource loads
across all UUIDs for which we received a comple-
tion signal. This table is suﬃcient to draw some
initial conclusions. First, as evidenced by the low
failure rate of badsec domains the vast majority of
end hosts and their recursive resolvers do not per-
form DNSSEC validation.
If all end hosts or re-
cursive resolvers veriﬁed DNSSEC, we would expect
a badsec failure rate of 100%,
instead of the ob-
served value of 2.661%. Thus, the increased security
value of DNSSEC-signing a domain is relatively low,
as most resolvers will not detect tampering against
DNSSEC-signed domains.

Second, DNSSEC-signed domains—even validly
signed domains—have a higher failure rate than non-
DNSSEC-signed domains:
just DNSSEC-signing
a domain increases the failure rate from around

7

USENIX Association  

22nd USENIX Security Symposium  579

Experiment Loading Dropoff

5
0
+
e
8

5
0
+
e
6

5
0
+
e
4

i

s
n
o
s
s
e
r
p
m

i
 
f
o
 
r
e
b
m
u
N

5
0
+
e
2

0
0
+
e
0

0
0
1

0
9

0
8

0
7

0
6

0
5

0
4

0
3

0
2

0
1

0

i

s
n
o
s
s
e
r
p
m

i
 

%

nosec
goodsec
badsec

e
t
a
r
 
e
r
u

l
i

a
F

5
3
0
.
0

0
3
0
.
0

5
2
0
.
0

0
2
0
.
0

5
1
0
.
0

0
1
0
.
0

5
0
0
.
0

Static ad Measurement

page

Driver
script

Completion

AfriNIC

APNIC

ARIN
Regional Internet Registry

LACNIC

RIPE

Figure 6: Plot of UUIDs that reached each stage
of the experiment.

Figure 7: Failure rates broken down by resolver
IP RIR. Error bars indicate a 95 percent binomial
proportion conﬁdence interval.

0.7846% to 1.006% (though this value is very sen-
sitive to geographic factors, as discussed in the fol-
lowing section). While this is not a huge diﬀerence,
it must be compared to the detection rate of bad do-
mains, which is also very small. Moreover, because
resolvers which cannot process DNSSEC at all ap-
pear to “detect” bogus DNSSEC records, the badsec
failure rate in Table 4 is actually an overestimate of
clients behind DNSSEC-validating resolvers, which
is probably closer to 1.655% (the diﬀerence between
the badsec and goodsec rates).

4.1.1 Geographic Eﬀects

As mentioned above, the raw numbers are somewhat
misleading because the failure rates are very geo-
graphically dependent. In order to explore this de-
pendence we categorized each test case (UUID) by
geographic area based on the resolver IP observed
performing resolution for a domain containing the
UUID.6 We used the CAIDA preﬁx-to-AS mapping
dataset [11] to determine the Autonomous System
Number (ASN) for each for client’s resolver IP ad-
dress and then assigned each client to the Regional
Internet Registry (RIR) which is responsible for that
AS, as listed in Table 5.

6If there was more than one resolver associated with a
particular UUID, our analytics package chose one arbitrar-
ily during the process of merging the records. If we restrict
our analysis to clients which only use one resolver, the overall
error rate goes down, but our results are qualitatively simi-
lar, with the error rates being 0.0046, 0.0055, and 0.0119, for
nosec, goodsec, and badsec, respectively.

As shown in Figure 7, resolution failure rates vary
widely by region, as does the diﬀerence in resolution
rates between nosec, goodsec, and badsec. In partic-
ular, while all ﬁve regions show a signiﬁcant diﬀer-
ence (2-proportion z-test, p < 0.0001) between ag-
gregate badsec-domain outcomes and nosec & good-
sec outcomes, only APNIC (Asia Paciﬁc) shows
a signiﬁcant diﬀerence between nosec and goodsec
(McNemar’s test, p < 0.0001). While AfriNIC
(Africa) shows a qualitative diﬀerence, we do not
have enough data points to determine whether it is
statistically signiﬁcant. Note that in general APNIC
seems to have an elevated resolution failure rate;
LACNIC (Latin America) does as well but still does
not show a signiﬁcant diﬀerence between nosec and
goodsec. We drilled down into the resolvers responsi-
ble for anomalous failure rates and present our ﬁnd-
ings in Sections 4.1.3, & 4.1.4.

4.1.2 The Impact of Packet Size and TCP

Fallback

One commonly-expressed concern with DNSSEC is
that it increases the size of DNS responses and, con-
sequently, failure rates. Ordinarily, DNS requests
and responses are carried over UDP, which limits
the maximum size of the responses. DNS has two
mechanisms to allow responses larger than the 512-
byte limit deﬁned in RFC 1035 [33]:

• Resolution can fall back to TCP if the server

supports it.

580  22nd USENIX Security Symposium 

USENIX Association

8

Name

Abbreviation Frequency Percentage

African Network Information Centre
American Registry for Internet Numbers
Asia-Paciﬁc Network Information Centre
Latin America and Caribbean Network Information Centre
R´eseaux IP Europ´eens Network Coordination Centre
Unclassiﬁable

AfriNIC
ARIN
APNIC
LACNIC
RIPE NCC

10,914
75,577
200,366
62,925
179,492
20

2.062%
14.28%
37.86%
11.89%
33.91%
< 0.001%

Table 5: Table listing the 5 Regional Internet Registries (RIRs). The Frequency and Percentage columns
indicate the number and relative prevalence of UUIDs for which at least one DNS query originated from each
region.

Failure rates vs. Transport Protocol

UDP
TCP

nosec

goodsec

badsec

e

t

a
r
 
e
r
u

l
i

a
F

6
0
.
0

5
0
0

.

4
0
0

.

3
0
0

.

2
0
0

.

1
0
0

.

0
0
0

.

Figure 8: Failure rates broken down by DNS trans-
port protocol. Error bars indicate a 95 percent bi-
nomial proportion conﬁdence interval.

• Clients can advertise a larger maximum UDP
datagram size via the EDNS0 OPT pseudo-
RR [39].

Unfortunately, both of these mechanisms can cause
problems for some intermediaries [7, 8, 10]. Because
the resolver behavior is observable on the server, we
can directly measure the impact of these strategies
on test resource load failures.

In order to look more closely at these eﬀects, we
ﬁrst ﬁltered out the data for the 4,739,669 (33.25%)
lookup requests we received which did not have the
DNSSEC OK ﬂag set. The DNSSEC OK ﬂag an-
nounces the query source’s willingness to receive
DNSSEC RRs, and thus when it is not set, our
resolver simply sends the requested records with-

out the DNSSEC RRs.7 Non-DNSSEC OK lookups
for DNSSEC resources appear to have similar suc-
cess rates to nosec resources. Out of the remaining
9,516,394 (66.75%) transactions where DNSSEC OK
was indicated, 4.22% of goodsec and 4.064% of badsec
lookups fell back to TCP. These TCP lookups had
dramatically higher failure rates: 6.011% for good-
sec and 6.531% for badsec compared to 0.6742% for
goodsec and 3.249% for badsec when UDP was used.
For nosec, resolution never fell back to TCP, and the
failure rate of 0.6% 8. was similar to that for goodsec
with UDP. Figure 8 summarizes these ﬁndings.

The similar UDP failure rates for nosec and good-
sec suggest that it is the TCP fallback that results
from DNSSEC’s increased response sizes, and not
the bigger responses themselves, that is the major
contributor to the elevated goodsec failure rate.

TCP fallback in the DNS resolution for one com-
ponent of a web page can have a negative impact on
the load rate of other components on the page, even
if their DNS lookups do not themselves fall back to
TCP. If we partition the UUIDs into those that fall
back to TCP for at least one test resource and those
that never fall back to TCP, we ﬁnd that the nosec
failure rates are 1.0791% for the former and 0.7617%
for the latter. We have not explored these eﬀects in
detail, but it seems likely that the failed resolution
slows down the retrieval of the rest of the resources,
thus causing failures.

We also found that accurate path MTU prediction
is crucial for maintaining high resolution success.
For 13,623 test resources (0.0953% of the 14,291,174
total), we observed that recursive resolvers overesti-
mated the UDP path MTU, advertised an inﬂated

7If multiple queries were present we considered the
DNSSEC OK ﬂag to be set if any of the queries had it. 2.771%
of test resources exhibited variation in this ﬂag.

8This nosec failure rate is lower than the one found in
Table 4 because, to be consistent with the goodsec and badsec
failure rate calculations in this section, it excludes failed test
resource loads for which we did not observe a DNS lookup
attempt.

USENIX Association  

22nd USENIX Security Symposium  581

9

value via EDNS0, and subsequently had to retry the
lookup with a smaller advertised value. Test re-
sources whose lookups included this path MTU dis-
covery behavior failed to load 14.09% of the time
compared to 2.519% for those that did not.

4.1.3 Case Study:

badsec-b8 validation

anomaly

We compared the failure rates of the badsec domains
and observed that the badsec-b8 variant exhibited
a signiﬁcantly lower failure rate (1.480%) than all
other badsec types (McNemar’s test applied pair-
wise against each other badsec variant, p < 0.01). In
badsec-b8, we simulated an invalid DNSKEY RRSIG
RR by incrementing the labels ﬁeld of the RR data
and signing it with a correctly-authenticated key.
The labels ﬁeld in an RRSIG RR is used for match-
ing RRSIGs to the RRsets they authenticate when
wildcards are involved. For example, if a zone de-
clares the *.foo.com wildcard name, then RRSIGs
for the RRsets of names matching *.foo.com (e.g.,
www.foo.com) would have a labels ﬁeld value of 2.
Section 5.3.1 of RFC 4035 [5] stipulates that an
RRSIG RR must have a labels ﬁeld value less than
or equal to the number of labels in the owner name
of the RRset that it authenticates.

To identify resolvers responsible for this valida-
tion anomaly, we ﬁrst partitioned the set of UUIDs
by the IP address of the resolver associated with
that UUID. Using the partitioned dataset, we iden-
tiﬁed 124 resolvers whose failure rate for badsec-b8
was signiﬁcantly lower than that of each of the other
badsec variants (McNemar’s test, p < 0.01). More-
over, for 123 of these resolvers, the badsec-b8 and
goodsec failure rates did not signiﬁcantly diﬀer at
the .01 level (McNemar’s test).

With the cooperation of one of the ISPs whose
resolvers exhibited the validation anomaly, we were
granted access to query their closed resolvers and
were able to manually reproduce the errant valida-
tion. We also added -1, +2, and +100 to the RRSIG
labels ﬁeld values and found that the resolvers in-
correctly accepted all of the increased values, but
not the decreased value, suggesting that the DNS
server implementation in use reversed the inequality
for testing the labels ﬁeld.

We were unable to devise a cache-poisoning at-
tack that leverages this validation error under any
reasonable threat model.

4.1.4 Case Study:

badsec-c12 validation

anomaly

The failure rate of the badsec-c12 variant (2.521%)
diﬀered signiﬁcantly from those of all other badsec

domains and was the second lowest, after badsec-b8
(McNemar’s test, p < 0.01). The badsec-c12 subdo-
main attacks the DNSSEC chain of trust by not pro-
viding the RRSIG RR for the test resource’s type A
RRset. A properly-validating server should not con-
sider the aﬀected A RRset validated unless it were
able to retrieve and validate its RRSIG.

All 32 of the resolvers in our dataset that exhib-
ited this validation anomaly belonged to the same
/22 subnetwork controlled by one particular ISP,
as did 45 of the 49 resolvers for which McNemar’s
test showed signiﬁcantly-diﬀerent (p < 0.01) failure
rates between nosec and goodsec. Customers using
this ISP’s recursive resolvers suﬀer from the worst
of both worlds. They are not only more vulnera-
ble to a man-in-the-middle attack against DNSSEC,
but also less likely to be able to access a domain
with DNSSEC enabled than one without. We were
unable to obtain access to the ISP’s closed recursive
resolvers to try to manually reproduce the incorrect
validation behavior.

Due to the reduced size of responses omitting
RRSIG RRs for type A queries, no badsec-c12 DNS
resolutions fell back to TCP, and for the 32 resolvers
exhibiting the validation anomaly, badsec-c12 ’s fail-
ure rate (1.245%) was signiﬁcantly less than that
of goodsec (13.81%) (McNemar’s test applied sepa-
rately for each resolver, p < 0.01).

4.1.5 Case Study: Comcast

In January 2012, Comcast announced that it had
ﬁnished deploying DNSSEC within its network and
that its residential customers would thenceforth be
protected by DNSSEC-validating DNS resolvers [30].
We identiﬁed dynamic Comcast IP end hosts in our
dataset using a list of IP preﬁxes published by Com-
cast [13]. One should expect that Comcast end hosts
in our dataset would fail on goodsec at a lower than
average rate and badsec at a higher than average
rate. Indeed, the 582 Comcast end hosts observed
exhibited a 0.1718% failure rate for goodsec and a
92.5636% failure rate for badsec. For comparison,
Comcast end hosts failed on nosec domains 0.1718%
of the time. This result is consistent with the ex-
pectation that a network that is properly conﬁgured
for DNSSEC will have identical behavior for nosec
and goodsec. Our measurements indicate that the
majority of the diﬀerence between Comcast’s badsec
failure rate and 100% is caused by users who are not
using Comcast for recursive resolution and therefore
do not beneﬁt from Comcast’s DNSSEC veriﬁcation;
if we exclude end-hosts that use resolvers outside of
Comcast’s AS the badsec failure rate improves to
98.6544%.

582  22nd USENIX Security Symposium 

USENIX Association

10

e
t
a
r
 
e
r
u

l
i

a
F

7
2
0
.
0

6
2
0
.
0

5
2
0
.
0

4
2
0
.
0

3
2
0
.
0

2
2
0
.
0

1
2
0
.
0

All test resources

Windows

Firefox
Chrome
Safari
IE

e
t
a
r
 
e
r
u

l
i

a
F

5
3
0
.
0

0
3
0
.
0

5
2
0
.
0

0
2
0
.
0

5
1
0
.
0

0
1
0
.
0

0

5

10
15
Load position

20

25

0

5

10
15
Load position

all test resource classes

20

25

Figure 9: Plot of failure rate across all test re-
source types versus load order.

Figure 10: Failure rate versus load order across
all test resources for Windows clients.

Percentile Test duration (seconds)

50%
90%
95%
98%

9
31
50
100

Table 6: Percentiles from the distribution of test
durations, measured from the time of the measure-
ment page load to the time of the completion signal.

4.2 Measurement Diﬃculties

Because our primary measurement endpoint is the
browser’s failure to retrieve a resource, we are very
sensitive to any other sources of failure other than
the ones we are attempting to measure; by con-
trast, many previous studies such as [22] measured
between multiple diﬀerent success outcomes, which
were distinguishable from failures.
In order to
minimize these eﬀects, we investigated other po-
tential sources of failure closely, as described be-
low.

4.2.1 Resource Load Sequence

Recall from Section 3.1, that test resource loads are
initiated one after another in a random order. Be-
cause the test takes some time (see Table 6) to com-
plete, there are a variety of conditions which can
cause the test to abort prematurely. This suggests
that the order in which resources are loaded may
impact the error rate.

Figure 9 shows the overall failure rate versus load
position (note that the ﬁrst resource is at position 0).
While the overall trend seems consistent with fail-
ures getting progressively worse with later resources,
the sharp spike and then subsequent decline between
positions 5 and 9 seems anomalous. In order to ex-
plore this further, we broke down the the failure rate
by browser and operating system.

As Figures 10 and 11 make clear, Chrome and
Firefox on Mac and Windows both show the same
pattern of a failure spike around resources 5-8,
whereas the same browsers on Linux (Figure 12) as
well as both Safari and Internet Explorer show a
generally linear trend (though the break around re-
source 9 for Internet Explorer is also puzzling). We
leave the explanation of these anomalies for future
work.

4.2.2 Latent UUIDs

Our analysis uncovered 3616 UUIDs for which we
received completion signals without corresponding
measurement page loads during the one-week exper-
iment window. We refer to these UUIDs as latent
UUIDs because we observed DNS and HTTP re-
quests for FQDNS that included them in our logs
prior to the start of our experiment window. There
are two plausible explanations for the existence of
latent UUIDs:

1. Browser caching. Modern web browsers
cache users’ recent and open tabs to allow for
restoration of the browsing session in case of

USENIX Association  

22nd USENIX Security Symposium  583

11

Mac

Linux

Firefox
Chrome
Safari

e
t
a
r
 
e
r
u

l
i

a
F

5
3
0
.
0

0
3
0
.
0

5
2
0
.
0

0
2
0
.
0

5
1
0
.
0

0
1
0
.
0

Firefox
Chrome

e
t
a
r
 
e
r
u

l
i

a
F

5
3
0
.
0

0
3
0
.
0

5
2
0
.
0

0
2
0
.
0

5
1
0
.
0

0
1
0
.
0

0

5

10
15
Load position

all test resource classes

20

25

0

5

10
15
Load position

all test resource classes

20

25

Figure 11: Failure rate versus load order across
all test resources for Mac clients.

Figure 12: Failure rate versus load order across
all test resources for Linux clients.

a crash, browser termination, or accidental tab
closure. Half of the 18 latent UUIDs that had
HTTP requests during the experiment window
appeared within the ﬁrst 33 hours of the exper-
iment window, and 11 of them loaded the mea-
surement page within the 24 hours leading up
to the start of the experiment window. Thus, it
is plausible that browser caching explains some
of the latent UUIDs.

2. DNS caching with eager renewal. To im-
prove DNS cache hit rates and, consequently,
reduce client latency, Cohen and Kaplan [12]
proposed a caching scheme wherein DNS caches
issue unsolicited queries to authoritative name-
servers for cached RRs whose TTLs have ex-
pired, even if no client queried for the RR at
the time of the renewal. This mechanism is
a documented feature in the Cisco IOS Dis-
tributed Director [1] and has been implemented
by others [45]. Our log data strongly supports
this explanation, as all latent UUIDs (by deﬁni-
tion) appeared in the DNS logs, but only 18 had
HTTP requests during the experiment window.

Latent UUIDs are not included in our analysis,
as we cannot guarantee that our log data extends
far enough into the past to cover them. Further-
more, our analysis only includes UUIDs for which
we observed both the measurement page load and
completion signal within the experiment window.

5 Discussion

The beneﬁt from DNSSEC-signing a domain is
upper-bounded by the number of clients which actu-
ally validate DNSSEC-signed records. As our mea-
surements show, the fraction of clients which do so
is less than 3%.9 Moreover, this beneﬁt is only ob-
tained if DNSSEC either deters attacks or allows
detection of attacks. By contrast, for a site with
worldwide users, our results indicate that deploying
DNSSEC in the current environment amounts to a
self-inﬂicted partial attack on one’s own site on the
order of 0.2214% (the diﬀerence between the goodsec
and nosec failure rates). For a site without signiﬁ-
cant Asian usage, the tradeoﬀ looks more attractive,
and for a site with largely Asian usage it looks less
attractive.

The major source of increased failure rates from
DNSSEC deployment appears to be that increased
packet sizes force clients into DNS over TCP rather
than DNS over UDP. The failure rate for DNS over
TCP is approximately 10 times larger than DNS
over UDP. This phenomenon is strongly localized to
Asia/Paciﬁc browsers.

Some potential future developments could change
this calculation. First, a signiﬁcant number of ISPs
could deploy validating resolvers. As shown by the

9Here we interpret the badsec failure rate as an upper
bound on the fraction of end users protected by DNSSEC
validation because some fraction of the failures may be due
to the fact that DNSSEC was enabled rather than validation
failure

584  22nd USENIX Security Symposium 

USENIX Association

12

Comcast data in Section 4.1.5, unilateral deploy-
ment of DNSSEC by ISPs can have a very large im-
pact on the behavior of their customers. If a few of
the large ISPs were to deploy validating resolvers,
our measured badsec failure rate would no doubt
have been much higher and their customers would
have obtained some level of defense against attackers
outside the ISP’s network. (Validation at the ISP re-
solver level does not provide defense against attack-
ers located between the resolver and the customer.)
Second, there could be widespread deployment
of a technology such as DANE that depends on
DNSSEC. As mentioned above, DNSSEC for A
records does not provide security against on-path
attackers, who can intercept the traﬃc between
the client and the server. Defending against such
attackers
cryptographic
protocol such as SSL/TLS. By contrast, if DANE
is used to attest to end-user certiﬁcates,
then
DNSSEC combined with DANE-based certiﬁcates
can provide security against on-path attackers and
thus a signiﬁcantly greater beneﬁt.
Even with
DANE, however, the collective action problem of
simultaneous client and server deployment persists.
In fact, it is worse since DANE’s security requires
that the client do DNSSEC validation — ISP-level
validation is not suﬃcient.

requires

some

sort of

Our results also serve as a caution for future re-
searchers: Advertisement network based studies —
especially those which attempt to measure success
or failure — are very sensitive to variation in client
and network behavior.
In particular, there is sig-
niﬁcant variation both between browsers and oper-
ating systems and by request order within the same
browser/operating system pair. These variations are
of the same order of magnitude as the signal we are
trying to measure and thus present a signiﬁcant chal-
lenge. Additionally, they may indicate actual prob-
lems with the browsers — or at least opportunities
for improvement. We are currently working with
browser vendors to attempt to determine the reason
for these anomalies.

6 Related Work

Several research groups have performed measure-
ments related to DNSSEC.

The SecSpider project [35, 36, 43] has surveyed
DNSSEC-secured zones since the DNSSEC rollout,
quantifying the rollout using metrics of availability,
veriﬁability, and validity. Deccio et al. [14, 15] sur-
veyed representative DNSSEC domains for miscon-
ﬁguration. Both of these projects focus on properties
of the authoritative DNS server zone data, rather
than the behavior of resolvers or caches.

Several research groups have attempted to charac-
terize the overhead (to clients, servers, and the net-
work) from deploying DNSSEC. Ager, Dreger, and
Feldmann [2] used a trace of DNS traﬃc as the basis
for a testbed experiment. They noted the possibility
of overhead arising from packet fragmentation. Wi-
jngaards and Overeinder [42] described the design of
a DNSSEC resolver and validator and compared its
performance to an ordinary DNS resolver. Migault,
Girard, and Laurent [31] measured the overhead of
DNSSEC resolution in a lab setting, including the
NSEC3 option.

Gudmundsson and Crocker [18] measured the de-
ployment of DNSSEC-capable resolvers using traces
of DNS queries made to the .org servers. Glynn [17]
surveyed DNSSEC deployment in Ireland, highlight-
ing the possibilty that large responses would suﬀer
fragmentation, and noting the geographic variation
in client path MTU.

Dietrich [16] reports on a study of one component
in DNSSEC resolution: users’ home DSL routers.
With the cooperating of network operators, the
study’s authors tested the behavior of 36 routers in
a testbed environment. The DNS proxies in more
than half of these routers were incompatible with
DNSSEC; several of the tested routers could not be
used with DNSSEC even if their internal DNS proxy
were bypassed.

Herzberg and Shulman [19] describe several chal-
lenges to wide-scale DNSSEC deployment. They ob-
serve that large-response fragmentation not only re-
duces performance but can be the basis of down-
grade and forgery attacks on permissive resolvers.

Pappas and Keromytis [37] performed a dis-
tributed measurement of resolution failures in the
aftermath of the May 5, 2010 signing of the DNS
root. They made resolution attempts from hundreds
of geographically dispersed nodes (e.g., Tor exit
nodes), which allowed them to observe the behavior
of many DNS resolvers. Whereas their conclusions
focused on the eﬀects of rolling out DNSSEC on the
DNS root-level servers, our measurements target the
DNS in its steady state behavior nearly two years
later.

Krishnan and Monrose [28] performed a large-
scale measurement of browser DNS prefetching and
characterized its security and privacy implications.
Using a trace-based cache simulator, they showed
that the additional overheads induced by prefetching
would increase the overhead of deploying DNSSEC.
The Netalyzr platform [27] allows interested users
to measure and report on properties of their Inter-
net connection. Netalyzr has uncovered widespread
DNS manipulation by ISPs [40, 41].

USENIX Association  

22nd USENIX Security Symposium  585

13

Zhang et al. [44] included client-side DNS mea-
surement code in a software package used by mil-
lions. They identiﬁed several ISPs that manipulate
DNS results, allowing them to proxy and modify
Web searches.

Ager et al. [3] asked friends to run DNS measure-
ment code on their systems. Their data anlysis fo-
cused on DNS performance.

Honda et al. [21] asked IETF colleagues to run
a measurement tool, TCPExposure; this tool gener-
ated TCP segments with various properties, allowing
Honda et al. to observe how middleboxes between
clients and their servers handle diﬀerent TCP exten-
sions. Of all the related work, Honda et al.’s is the
closest in spirit to ours. They sought to measure the
compatibility of hypothetical future protocols with
deployed middleboxes; we measure the interaction
between DNSSEC and today’s network infrastruc-
ture.

Rather than deploy custom software to users, we
wrote JavaScript code that triggers DNS resolution,
and served this code in an ad we placed with a dis-
play ad network. This strategy for enlisting users
was pioneered by Barth, Jackson, and their coau-
thors [6, 25], who used it to measure the Web plat-
form. This strategy was also recently used by Hus-
ton and Michaelson [23, 24] to measure the deploy-
ment of DNSSEC-capable resolvers and to describe
their geographic distribution. Unlike our study, Hus-
ton did not also measure the prevalence of DNSSEC-
intolerant resolvers.

7 Summary

While DNS name resolution is a key part of the In-
ternet infrastructure, it has long been known that it
is seriously insecure. DNSSEC is designed to repair
that insecurity. We report on a large ad network
based study designed to measure both the current
state of deployment and the extent to which deploy-
ing DNSSEC-signed domains creates collateral dam-
age in the form of failed resolutions of valid domains.
Our measurements conﬁrm previous reports that
DNSSEC deployment is proceeding quite slowly.
Less than 3% of clients failed to retrieve resources
hosted on DNSSEC-signed domains with broken sig-
natures. This indicates that either these clients — or
their resolvers — are not doing DNSSEC validation
or they are not hard-failing on broken validations,
which is eﬀectively the same as not validating at all.
Moreover, about 1.006% of clients fail to retrieve
validly DNSSEC-signed resources (as compared to
0.7846% of unsigned resources. In other words, for
every ten clients a site protects by using DNSSEC, it
self-DoSes about one client. This eﬀect is principally

due to TCP fallback to accomodate larger DNSSEC
packet sizes and is strongly localized to Asian users.
Finally, we report on a number of new measure-
ment artifacts that can aﬀect the results of advertis-
ing network based studies, including some browser-
speciﬁc anomalies which may reveal opportunities
for improvement in those browsers. In future work,
we hope to explore further the speciﬁc causes of these
anomalies.

Acknowledgements

The authors thank the anonymous reviewers and our
shepherd, Tara Whalen. We also thank Duane Wes-
sels, Casey Deccio, Cynthia Taylor, and Stephen
Checkoway for their feedback on the paper, Philip
Stark for suggestions about the analysis, and Collin
Jackson for his help in acquiring an advertising net-
work advertiser account. This material
is based
upon work supported by the MURI program under
AFOSR Grant No. FA9550-08-1-0352.

References

[1] Distributed

director

cache

auto

refresh.

https:

//www.cisco.com/en/US/docs/ios/12_2t/12_2t8/feature/
guide/ftrefrsh.pdf.

[2] B. Ager, H. Dreger, and A. Feldmann. Predicting the
DNSSEC overhead using DNS traces.
In R. Calderbank
and H. Kobayashi, editors, Proceedings of CISS 2006, pages
1484–89. IEEE Information Theory Society, Mar. 2006.

[3] B. Ager, W. M¨uhlbauer, G. Smaragdakis, and S. Uhlig.
Comparing DNS resolvers in the wild.
In M. Allman, ed-
itor, Proceedings of IMC 2010, pages 15–21. ACM Press,
Nov. 2010.

[4] R. Arends, R. Austein, M. Larson, D. Massey, and S. Rose.
DNS Security Introduction and Requirements. RFC 4033
(Proposed Standard), Mar. 2005. Updated by RFC 6014.

[5] R. Arends, R. Austein, M. Larson, D. Massey, and S. Rose.
Protocol Modiﬁcations for the DNS Security Extensions.
RFC 4035 (Proposed Standard), Mar. 2005. Updated by
RFCs 4470, 6014.

[6] A. Barth, C. Jackson, and J. C. Mitchell. Robust defenses
for cross-site request forgery.
In P. Syverson and S. Jha,
editors, Proceedings of CCS 2008, pages 75–88. ACM Press,
Oct. 2008.

[7] R. Bellis. DNS Proxy Implementation Guidelines. RFC 5625

(Best Current Practice), Aug. 2009.

[8] R. Bellis. DNS Transport over TCP - Implementation Re-

quirements. RFC 5966 (Proposed Standard), Aug. 2010.

[9] R. Bellis and L. Phifer.

pact on broadband routers and ﬁrewalls.
https://www.dnssec-deployment.org/wp-content/uploads/
2010/03/DNSSEC-CPE-Report.pdf, Sept. 2008.

Test report: DNSSEC im-
Online:

[10] R. Braden. Requirements for Internet Hosts - Application
and Support. RFC 1123 (Standard), Oct. 1989. Updated by
RFCs 1349, 2181, 5321, 5966.

[11] Caida routeviews preﬁx to as mappings dataset (pfx2as).
http://www.caida.org/data/routing/routeviews-prefix2as.
xml.

14

586  22nd USENIX Security Symposium 

USENIX Association

[12] E. Cohen and H. Kaplan. Proactive caching of DNS records:
Addressing a performance bottleneck. Computer Networks,
41(6):707–26, 2003.

[29] P. Leach, M. Mealling, and R. Salz. A Universally Unique
IDentiﬁer (UUID) URN Namespace. RFC 4122 (Proposed
Standard), July 2005.

[13] What are comcast’s dynamic ip ranges? http://postmaster.

comcast.net/dynamic-IP-ranges.html.

[14] C. Deccio, J. Sedayao, K. Kant, and P. Mohapatra. A
case for comprehensive DNSSEC monitoring and analysis
tools. In R. Clayton, editor, Proceedings of SATIN 2011,
Apr. 2011. Online: http://conferences.npl.co.uk/satin/
agenda2011.html.

[15] C. Deccio, J. Sedayao, K. Kant, and P. Mohapatra. Quan-
tifying and improving DNSSEC availability. In G. Rouskas
and X. Zhou, editors, Proceedings of ICCCN 2011. IEEE
Communications Society, July 2011.

[16] T. Dietrich. DNSSEC support by home routers in Ger-
many. Presented at RIPE 60, May 2010. Online slides:
http://ripe60.ripe.net/presentations/Dietrich-DNSSEC_
Support_by_Home_Routers_in_Germany.pdf.

[17] W. J. Glynn. Measuring DNS vulnerabilities and DNSSEC
challenges from an irish perspective. In R. Clayton, editor,
Proceedings of SATIN 2011, Apr. 2011. Online: http://
conferences.npl.co.uk/satin/agenda2011.html.

[18] ´O. Gudmundsson and S. D. Crocker. Observing DNSSEC
validation in the wild. In R. Clayton, editor, Proceedings of
SATIN 2011, Apr. 2011. Online: http://conferences.npl.
co.uk/satin/agenda2011.html.

[19] A. Herzberg and H. Shulman. Towards adoption of dnssec:
Availability and security challenges. Cryptology ePrint
Archive, Report 2013/254, 2013. http://eprint.iacr.org/.

[20] P. Hoﬀman and J. Schlyter. The DNS-Based Authentica-
tion of Named Entities (DANE) Transport Layer Security
(TLS) Protocol: TLSA. RFC 6698 (Proposed Standard),
Aug. 2012.

[21] M. Honda, Y. Nishida, C. Raiciu, A. Greenhalgh, M. Han-
dley, and H. Tokuda. Is it still possible to extend TCP? In
P. Thiran and W. Willinger, editors, Proceedings of IMC
2011, pages 181–94. ACM Press, Nov. 2011.

[30] J.

Livingood.

Comcast

ployment.
comcast-completes-dnssec-deployment.html.

de-
http://blog.comcast.com/2012/01/

completes

dnssec

[31] D. Migault, C. Girard, and M. Laurent. A performance view
on dnssec migration. In H. Lutﬁyya and Y. Diao, editors,
Proceedings of CNSM 2010, pages 469–74. IEEE Commu-
nications Society, Oct. 2010.

[32] P. Mockapetris. Domain names - concepts and facilities.
RFC 1034 (Standard), Nov. 1987. Updated by RFCs 1101,
1183, 1348, 1876, 1982, 2065, 2181, 2308, 2535, 4033, 4034,
4035, 4343, 4035, 4592, 5936.

[33] P. Mockapetris. Domain names - implementation and spec-
iﬁcation. RFC 1035 (Standard), Nov. 1987. Updated by
RFCs 1101, 1183, 1348, 1876, 1982, 1995, 1996, 2065, 2136,
2181, 2137, 2308, 2535, 2845, 3425, 3658, 4033, 4034, 4035,
4343, 5936, 5966, 6604.

[34] nginx httplogmodule. http://wiki.nginx.org/HttpLogModule.

[35] E. Osterweil, D. Massey, and L. Zhang. Deploying and mon-
itoring DNS security (DNSSEC). In C. Payne and M. Franz,
editors, Proceedings of ACSAC 2009, pages 429–38. ACM
Press, Dec. 2009.

[36] E. Osterweil, M. Ryan, D. Massey, and L. Zhang. Quanti-
fying the operational status of the DNSSEC deployment. In
K. Papagiannaki and Z.-L. Zhang, editors, Proceedings of
IMC 2008, pages 231–42. ACM Press, Oct. 2008.

[37] V. Pappas and A. D. Keromytis. Measuring the deploy-
ment hiccups of DNSSEC. In J. L. Mauri, T. Strufe, and
G. Martinez, editors, Proceedings of ACC 2011, volume 192
of CCIS, pages 44–53. Springer-Verlag, July 2011.

[38] S. Son and V. Shmatikov. The hitchhiker’s guide to DNS
cache poisoning. In S. Jajodia and J. Zhou, editors, Proceed-
ings of SecureComm 2010, volume 50 of LNICST, pages
466–83. Springer-Verlag, Sept. 2010.

[39] P. Vixie. Extension Mechanisms for DNS (EDNS0). RFC

2671 (Proposed Standard), Aug. 1999.

[22] L.-S. Huang, E. Y. Chen, A. Barth, E. Rescorla, and C. Jack-
son. Talking to yourself for fun and proﬁt. In H. J. Wang,
editor, Proceedings of W2SP 2011. IEEE Computer Society,
May 2011.

[40] N. Weaver, C. Kreibich, B. Nechaev, and V. Paxson. Impli-
cations of Netalyzr’s DNS measurements.
In R. Clayton,
editor, Proceedings of SATIN 2011, Apr. 2011. Online:
http://conferences.npl.co.uk/satin/agenda2011.html.

[23] G. Huston. Counting DNSSEC. Online: https://labs.ripe.

net/Members/gih/counting-dnssec, Sept. 2012.

[24] G. Huston and G. Michaelson. Measuring DNSSEC per-
formance. Online: http://www.potaroo.net/ispcol/2013-05/
dnssec-performance.html, May 2013.

[25] C. Jackson, A. Barth, A. Bortz, W. Shao, and D. Boneh.
Protecting browsers from DNS rebinding attacks. ACM
Trans. Web, 3(1), Jan. 2009.

[26]

jquery: The write less, do more, javascript library. http:
//jquery.com.

[27] C. Kreibich, B. Nechaev, N. Weaver, and V. Paxson. Net-
alyzr: Illuminating the edge network. In M. Allman, editor,
Proceedings of IMC 2010, pages 246–59. ACM Press, Nov.
2010.

[28] S. Krishnan and F. Monrose. An empirical study of the
performance, security and privacy implications of domain
name prefetching. In S. Bagchi, editor, Proceedings of DSN
2011, pages 61–72. IEEE Computer Society and IFIP, June
2011.

[41] N. Weaver, C. Kreibich, and V. Paxson. Redirecting DNS
In N. Feamster and W. Lee, editors,

for ads and proﬁt.
Proceedings of FOCI 2011. USENIX, Aug. 2011.

[42] W. C. Wijngaards and B. J. Overeinder. Securing DNS:
Extending DNS servers with a DNSSEC validator. Security
& Privacy, 7(5):36–43, Sept.–Oct. 2009.

[43] H. Yang, E. Osterweil, D. Massey, S. Lu, and L. Zhang.
Deploying cryptography in Internet-scale systems: A case
study on DNSSEC. IEEE Trans. Dependable and Secure
Computing, 8(5):656–69, Sept.–Oct. 2011.

[44] C. Zhang, C. Huang, K. W. Ross, D. A. Maltz, and J. Li.
Inﬂight modiﬁcations of content: Who are the culprits? In
C. Kruegel, editor, Proceedings of LEET 2011. USENIX,
Mar. 2011.

[45] Z. Zhang, L. Zhang, D.-E. Xie, H. Xu, and H. Hu. A novel
dns accelerator design and implementation. In C. S. Hong,
T. Tonouchi, Y. Ma, and C.-S. Chao, editors, Proceedings
of APNOMS 2009, volume 5787 of LNCS, pages 458–61.
Springer-Verlag, Sept. 2009.

USENIX Association  

22nd USENIX Security Symposium  587

15

