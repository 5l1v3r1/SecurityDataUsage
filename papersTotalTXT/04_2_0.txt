Preventing Side-Channel Leaks in Web Traﬃc: A Formal Approach

Michael Backes

Saarland University and MPI-SWS

backes@mpi-sws.org

Goran Doychev

IMDEA Software Institute
goran.doychev@imdea.org

Boris K¨opf

IMDEA Software Institute

boris.koepf@imdea.org

Abstract

Internet traﬃc is exposed to potential eavesdroppers.
Standard encryption mechanisms do not provide suﬃcient
protection: Features such as packet sizes and numbers re-
main visible, opening the door to so-called side-channel at-
tacks against web traﬃc.

This paper develops a framework for the derivation of
formal guarantees against traﬃc side-channels. We present
a model which captures important characteristics of web
traﬃc, and we deﬁne measures of security based on quanti-
tative information ﬂow. Leaning on the well-studied proper-
ties of these measures, we provide an assembly kit for coun-
termeasures and their security guarantees, and we show
that security guarantees are preserved on lower levels of
the protocol stack.

We further propose a novel technique for the eﬃcient
derivation of security guarantees for web applications. The
key novelty of this technique is that it provides guaran-
tees that cover all execution paths in a web application,
i.e.
it achieves completeness. We demonstrate the utility
of our techniques in two case studies, where we derive for-
mal guarantees for the security of a medium-sized regional-
language Wikipedia and an auto-complete input ﬁeld.

1

Introduction

Internet traﬃc is exposed to potential eavesdroppers. To
limit disclosure of secret information, security-aware users
can protect their traﬃc by accessing web services which of-
fer TLS encryption, or by sending their data through en-
crypted tunnels. While today’s encryption mechanisms hide
the secret payload from unauthorised parties, they cannot
hide lower-level traﬃc features such as packet sizes, num-
bers, and delays. These features contain information about
the payload which can be extracted by traﬃc proﬁling,
side-stepping the protection oﬀered by cryptography. The
relevance of this threat is demonstrated by a large num-
ber of side-channel attacks against encrypted web traﬃc,
e.g. [4, 7, 8, 10, 12, 22, 23, 30, 32, 40].

A number of approaches for the mitigation and analysis
of side-channel leaks have been proposed, e.g. [30, 33, 40,
41]. A common pattern in these approaches is the following
relationship between attacks, countermeasures, and their se-
curity analysis: An attack corresponds to a classiﬁcation of
a sample of network traﬃc, where classes correspond to se-
cret aspects of user behavior. A correct classiﬁcation corre-
sponds to a successful attack, i.e. one in which the secret is
correctly recovered. A countermeasure modiﬁes the shape
of the network traﬃc with the goal of making the classiﬁca-
tion of samples more diﬃcult or even impossible. A secu-
rity analysis is based on an evaluation of the performance
of a particular classiﬁcation algorithm.

A security analysis following this pattern enables one
to assess a system’s vulnerability to a particular classiﬁer;
however, the analysis does not make immediate assertions
about the vulnerability to attackers using more sophisticated
techniques for mining their observations. This limitation is
not only unsatisfactory from a theoretical viewpoint, but it
also raises signiﬁcant problems in practice: A recent com-
prehensive study [17] of traﬃc analysis countermeasures
exhibits that the incompleteness of existing security anal-
yses indeed leaves room for highly eﬀective attacks. It is
clear that, ultimately, one strives for security guarantees that
hold for all realistic adversaries who can observe the web
application’s traﬃc, i.e. formally backed-up security guar-
antees.

There are two key challenges in deriving formal guaran-
tees against side-channel attacks against web traﬃc. The
ﬁrst challenge is to devise a mathematical model of the web
application’s traﬃc, which is indispensable for expressing
and deriving security guarantees. Such a model has to be
accurate enough to encompass all relevant traﬃc features,
and at the same time, the model must be simplistic enough
to allow for tractable reasoning about realistic web applica-
tions.

The second challenge is to develop techniques for the
computation of security guarantees for a given web appli-
cation. The main obstacle is that the derivation of secu-
rity guarantees requires considering all execution paths of a
web application. As the number of paths may grow expo-

nentially with their length, their naive enumeration is com-
putationally infeasible. Previous approaches deal with this
problem by resorting to a subset of the possible execution
paths [5, 42], introducing incompleteness into the analysis.
In this paper, we develop a novel framework for the
derivation of formal guarantees against traﬃc side-channels
in web applications. We ﬁrst provide a model that captures
the eﬀect of user actions on web-traﬃc. In particular, we
cast web applications as labeled, directed graphs where ver-
tices correspond to the states of the web application, edges
correspond to possible user actions, and (vertex) labels are
observable features of encrypted traﬃc induced by mov-
ing to the corresponding state. These features can include
packet sizes and numbers, and their source and destinations,
which is suﬃcient to encompass relevant examples.

We then interpret this graph as an information-theoretic
channel mapping sequences of (secret) user actions to (pub-
lic) traﬃc observations. Casting the graph as a channel
allows us to apply established measures of conﬁdentiality
(Shannon entropy, min-entropy, g-leakage) from quantita-
tive information-ﬂow analysis and make use of their well-
understood properties. Leveraging those properties, we ob-
tain the following results.

First, we provide an assembly kit for countermeasures
and their security guarantees. In particular, we identify a
number of basic countermeasures, and we demonstrate how
representative countermeasures from the literature can be
obtained by their composition. The information that is re-
vealed by a composed countermeasure is upper-bounded
by the information that is revealed by underlying compo-
nents (formalized as the so-called data processing inequal-
ity), which allows us to show how composed countermea-
sures relate in terms of the security they provide. Second,
we show that the security guarantees derived in our model
(which capture attackers who observe encrypted traﬃc at
a speciﬁc layer of the protocol stack) also hold for attack-
ers who observe the traﬃc at lower layers of the stack. We
obtain this result by casting segmentation of packets and
packet headers in our assembly kit, and by applying the data
processing inequality. Third, we show that recent work on
predictive mitigation of timing channels [2] can be applied
in our setting, allowing us to deal with leaks induced by
traﬃc patterns and timing in a modular fashion.

To put our model into action, we propose a novel tech-
nique for the eﬃcient derivation of security guarantees for
web applications. The key advantage of this technique is
that it allows considering all execution paths of the web ap-
plication, which is fundamental for deriving formal security
guarantees. We achieve this result for the important special
case of a user following the random surfer model [36] and
for Shannon entropy as a measure: As a ﬁrst step, we use
PageRank to compute the stationary distribution of a ran-
dom surfer, which we take as the a priori probability of a

user visiting a vertex. As a second step, we apply the chain
rule of entropy to reduce the computation of the uncertainty
about a path of length (cid:96) to that about path of length (cid:96) − 1,
and transitively to that about a single transition. As com-
puting the uncertainty about a single transition can be done
eﬃciently, this is the key to avoiding the enumeration of all
(exponentially many) paths.

We use this algorithm to study the trade-oﬀ between
security and performance (in terms of overhead) of dif-
ferent countermeasures. A key observation is that coun-
termeasures based on making individual vertices indistin-
guishable (e.g. [41]) fail to protect paths of vertices. To
ensure protection of paths, we devise path-aware counter-
measures, which strengthen countermeasures for vertices
by making sure that indistinguishable vertices also have in-
distinguishable sets of successors. Formally, we achieve
this by coarsening indistinguishability on vertices to a prob-
abilistic bisimulation relation, which we compute using a
novel algorithm based on randomization and partition re-
ﬁnement.

We demonstrate the applicability of the proposed tech-
niques in two case studies, where we analyze the traﬃc of
(1) a regional-language Wikipedia consisting of 5,000 arti-
cles, and that of (2) an auto-complete input ﬁeld for a dic-
tionary of 1,100 medical terms. Our approach delivers a
number of countermeasures that provide diﬀerent trade-oﬀs
between security guarantees and traﬃc overhead, spreading
from good security at the price of a high overhead, to low
overhead at the price of lower security guarantees.

In summary, our main contributions are twofold. First,
we present a novel model for formal reasoning about side-
channel attacks in web applications. We show that this
model is expressive enough to capture relevant aspects of
web applications and countermeasures, and that it provides
a clean interface to information theory and quantitative
information-ﬂow analysis. Second, we present algorithms
and techniques that enable the eﬃcient derivation of such
guarantees for real systems and we demonstrate how they
can be used for adjusting the trade-oﬀ between security and
performance.

2 Web-traﬃc as an information-theoretic

channel

In this section, we cast web-traﬃc as an information-
theoretic channel (i.e. a conditional probability distribu-
tion) from user input to observable traﬃc patterns. We spec-
ify the threat scenario in Section 2.1 and present our basic
model of web applications and their traﬃc in Section 2.2.

(a) An Ethernet
without encryption

frame

(b) An Ethernet
frame
with HTTPS encryption

(c) An Ethernet
frame
with an encrypted tunnel
to a proxy server

(d) A WLAN frame with
WPA2 encryption

Figure 1. Encrypted packets with commonly used protection mechanisms. When HTTPS is applied
(Figure 1(b)), the attacker may use the information contained in the TCP header to reassemble the
packets and infer the approximate size of the web-object. When trafﬁc is forced through an SSH
tunnel (Figure 1(c)) and when using an encrypted wireless connection (Figure 1(d)), the original
headers are encrypted and direct reassembly is not possible any more.

2.1 Threat scenario

The threat scenario we are considering is a user perform-
ing conﬁdential actions (such as following hyperlinks, or
typing characters) in a web application, and a passive at-
tacker who inspects network traﬃc and wants to obtain in-
formation about the user’s actions.

When a user performs an action on the web applica-
tion, this causes messages (which we call web-objects) to
be exchanged between the client- and the server-side of
the web application, and the attacker observes bursts of
network packets corresponding to each web-object. Traf-
ﬁc is protected by encrypting the data at a certain layer of
the protocol stack. Commonly used protection mechanisms
are HTTPS, an encrypted connection to proxies (e.g. an
SSH tunnel), and encryption of wireless traﬃc (e.g. using
WPA2). Depending on the used protection mechanism, at-
tackers will have a diﬀerent view of the traﬃc, as illustrated
in Figure 1.

2.2 Basic model

We model the network traﬃc corresponding to a web ap-
plication using a directed labeled graph. In this graph, ver-
tices correspond to the states of the web application, edges
correspond to possible user actions, and vertex labels corre-
spond to induced traﬃc patterns.

Deﬁnition 1. A web application is a directed graph G =
(V, E), together with an application ﬁngerprint fapp : V → O
that maps vertices to observations O ⊆ (A × {↑,↓})∗, where

A is the set of observable objects. We denote the set of paths
in G by Paths(G).

An application ﬁngerprint of a vertex is intended to cap-
ture an eavesdropper’s view of transmitted packets, requests
and responses, which necessarily includes the packets’ di-
rections. If traﬃc is not encrypted, we set the observable
objects to be bit-strings, i.e. A = {0, 1}∗. If (a part of) an
object is encrypted, only its size remains visible, i.e., we as-
sume that encryption is perfect and length-preserving. For-
mally, encryption is deﬁned as a function enc : {0, 1}∗ → N,
and if all objects are encrypted, then we set A = N.
For example, a sequence (10,↑), (10,↑), (20,↓), (32,↓)
captures an exchange of encrypted objects, two of size 10
sent from the client to the server, and two of size 20 and 32,
respectively, sent from the server to the client.

We next show how Deﬁnition 1 can be instantiated to
capture two representative scenarios, which we use as run-
ning examples throughout the paper. The ﬁrst scenario
models web navigation, where user actions correspond to
following hyperlinks. The second scenario models an auto-
complete form, where user actions correspond to typing
characters into an input ﬁeld. For ease of presentation, in
the following examples we cast the states of a web applica-
tion as the requested web-objects, and assume that all ob-
jects are encrypted.

Example 1. Consider a user who is navigating the Web by
following hyperlinks. The web application is a web-graph
G = (V, E) , where each vertex v = (w1, w2, w3, . . . ) is a
sequence of web-objects, i.e V ⊆ W∗ for a set W of web-
objects. For example, we may have a vertex v = (a.html,

Deﬁnition 2. A network ﬁngerprint is a function

such that for each o ∈ O,(cid:80)

fnet : O → (O → [0, 1]),

o(cid:48)∈O fnet(o)(o(cid:48)) = 1.

(cid:96)(cid:89)

A network ﬁngerprint fnet(o)(o(cid:48)) models the conditional
probability of outputting a burst of network packets o(cid:48),
given as input a burst of network packets o. We cast network
ﬁngerprints using function notation because this allows for
a clean combination with application ﬁngerprints.

We now show how the combination of the web ap-
fnet can be cast as
plication with the network ﬁngerprint
an information-theoretic channel mapping execution paths
Paths(G) to sequences of network observations O∗.
Deﬁnition 3. Let G be a web application with ﬁngerprints
fapp and fnet. Let X be a random variable with range(X) =
Paths(G), and Y a random variable with range(Y) = O∗.
Then the traﬃc channel induced by (G, fapp, fnet) is the con-
ditional distribution

P[Y = o1 . . . o(cid:96)|X = v1 . . . v(cid:96)] =

fnet( fapp(vi))(oi)

i=1

With Deﬁnition 3, we make two implicit assumptions:
First, we assume that when a user traverses a path of length
(cid:96) in a web application, the attacker can see a sequence
o1, . . . , o(cid:96) ∈ O(cid:96) of sequences of packet sizes and directions.
This corresponds to an attacker that can distinguish between
bursts of traﬃc corresponding to diﬀerent vertices, but that
cannot observe the timing of the packets (See Section 5 for a
treatment of timing). Second, by multiplying the probabili-
ties of observations, we assume that the network bursts cor-
responding to individual vertices are pairwise independent.
Note that this assumption can be weakened by condition-
ing on multiple vertices, which we forgo for simplicity of
presentation. Finally, note that Deﬁnition 3 does not make
any assumptions on the distribution of X, which models the
user’s behavior. We will only make such assumptions in
Section 6, where we need them for the algorithmic deriva-
tion of security guarantees.

3

Information leaks in web-traﬃc

In this section, we present techniques for quantifying the
information that a web application’s encrypted traﬃc re-
veals about the user’s behavior. We base our development
on the notion of an information-theoretic channel, which
enables us to rely on established notions of conﬁdentiality
with their well-understood theory.

3.1 Quantifying information leaks

We model the (secret) behavior of a user as a random
variable X and the observable traﬃc of a web-application

Figure 2. An auto-complete input ﬁeld

style.css, script.js, image.jpg, video.ﬂv), which corresponds
to a particular webpage. An edge (u, v) models that the web-
page v can be reached from webpage u by following a hy-
perlink. If we additionally allow users to jump to arbitrary
webpages, the resulting graph will be complete. For the
deﬁnition of the application ﬁngerprint fapp, let the size of
a web-request corresponding to a web-object, and the size
of the web-object be given as functions r : W → N and
s: W → N, respectively. Then, the application ﬁngerprint
of a webpage v = (w1, . . . , wn) is given by
fapp(v) = ((r(w1),↑), (s(w1),↓), . . . , (r(wn),↑), (s(wn),↓)) .
Example 2. Consider an auto-complete input ﬁeld where,
after each typed character, a list of suggestions is generated
by the server and displayed to the user (see Figure 2). Let
Σ be the input alphabet (i.e. the set of possible user actions)
and Σ∗ the set of possible words. We deﬁne the correspond-
ing web application as a graph G = (V, E) with V = Σ∗,
the root r = ε is the empty word, and (u, v) ∈ E if and
only if v = uσ, for some σ ∈ Σ. Note that G is in fact
a preﬁx tree [9], and the leafs of the tree form the input
dictionary D ⊆ Σ∗. Let the auto-complete functionality be
implemented by a function suggest : V → S ∗ returning a list
of suggestions from the dictionary of possible suggestions
S ⊆ Σ∗. Let the sizes of a suggestion request and the cor-
responding suggestion list be given as functions r : V → N
and s: S ∗ → N, respectively. Then, given a word v ∈ V, we
deﬁne its application ﬁngerprint as

fapp(v) = ((r(v),↑), (s(suggest(v)),↓)).

To capture modiﬁcations of observations when traﬃc
passes through diﬀerent network protocols, as well as the
eﬀect of (potentially randomized) countermeasures, we in-
troduce the notion of a network ﬁngerprint.

as a random variable Y. The dependency between X and Y
is given in terms of a traﬃc channel P[Y|X] (formalized in
Deﬁnition 3); Section 6 discusses possible instantiations for
P[X].

We characterize the security of this system in terms of
the diﬃculty of guessing the value of X when only the value
of Y is known. This diﬃculty can be captured in terms
of information-theoretic entropy, where diﬀerent notions of
entropy correspond to diﬀerent kinds of guessing. We be-
gin by introducing Shannon entropy and discuss alternative
notions below.
Deﬁnition 4. The (Shannon) entropy of X is deﬁned as

P[X = x] log2 P[X = x],

H(X) = −(cid:88)
(cid:88)

H(X|Y) =

x

and captures the initial uncertainty about the value of X.
The conditional entropy H(X|Y) of X given Y is deﬁned by

P[Y = y]H(X|Y = y),

y

and captures the remaining uncertainty about the value of X
when the outcome of Y is known. The leakage from X to Y
is the reduction in uncertainty about X when Y is observed,
i.e. H(X) − H(X|Y).

Shannon entropy is interesting as a measure of conﬁ-
dentiality because one can use it to give a lower bound
for the expected number of steps that are required for de-
termining the value of X by brute-force search. Observe
that the optimal strategy for this is to try all values of X
in order of their decreasing probabilities. For this, let X
be indexed accordingly: P[X = x1] ≥ P[X = x2] ≥ . . . .
Then the expected number of guesses (also called guess-
ing entropy [34]) required to determine X is deﬁned by
1≤i≤|X| i P[X = xi], and the conditional version
G(X|Y) is deﬁned in the natural way [28]. The following
result [28, 34] bounds G(X|Y) in terms of Shannon entropy.
Proposition 1. G(X|Y) ≥ 1

G(X) = (cid:80)

42H(X|Y)

Note however that for heavily skewed X, the expected
number of guesses required to determine X can be large,
even if an attacker has a considerable chance of correctly
guessing the value of X in one attempt. To see this, consider
a random variable X distributed by P[X = x1] = 1
2 and
2n, for i ∈ {2, . . . , n}: the expected number
P[X = xi] = 1
of guesses to determine X grows linearly with n, but the
probability of correct guessing in one shot remains 1
2.

The min-entropy H∞ (see, e.g., [39] for a deﬁnition) ac-
counts for such scenarios by capturing the probability of
correctly guessing the secret in one attempt, i.e., it deliv-
ers worst-case rather than average-case guarantees. The g-
entropy [1] Hg generalizes min-entropy by parameterizing
the notion of an attacker’s gain.

The model presented in Section 4 can be used in conjunc-
tion with all H ∈ {H, H∞, Hg}, and we will keep our presen-
tation parametric. The algorithmic results in Section 6 rely
on the so-called chain rule, which is only satisﬁed by Shan-
non entropy. I.e., there we trade the strength of the security
guarantees for the tractability of the analysis.

4 Network ﬁngerprints

In this section, we take a closer look at network ﬁnger-
prints.
In particular, we show that their composition de-
creases the side-channel leakage of a web application. This
result has two implications: First, it enables us to compose
countermeasures against side-channel attacks, obtaining as-
sertions about their relative strengths for free. We utilize
this result by giving a toolkit of basic countermeasures, and
we show how they can be combined to encompass pro-
posals for countermeasures that appear in the recent liter-
ature. Second, it enables us to establish the soundness of
our bounds with respect to observers of lower layers of the
protocol stack.

4.1 Network ﬁngerprint composition

We now show how network ﬁngerprints can be com-
posed, and how this composition aﬀects the security of a
web application.
Deﬁnition 5. Given network ﬁngerprints fnet : O → (O(cid:48) →
net : O(cid:48) → (O(cid:48)(cid:48) → [0, 1]), we deﬁne the com-
[0, 1]) and f (cid:48)
net ◦ fnet): O → (O(cid:48)(cid:48) → [0, 1])
posed network ﬁngerprint ( f (cid:48)
(cid:88)
by

net ◦ fnet)(o)(o(cid:48)(cid:48)) =
( f (cid:48)

fnet(o)(o(cid:48)) f (cid:48)

net(o(cid:48))(o(cid:48)(cid:48)) .

o(cid:48)∈O(cid:48)

Note that composition of network ﬁngerprint corre-
sponds to concatenating two conditional distributions and
marginalizing the intermediate results. In the literature, the
resulting conditional distribution has been referred to as a
channel cascade [20].

The following result states that ﬁngerprint composition
does not decrease a web application’s security. For the for-
mal statement, let fnet and f (cid:48)
net as in Deﬁnition 5, let X be
a random variable with range(X) = Paths(G), Y a random
variable with range(Y(cid:48)) = (O(cid:48))∗ induced by fnet, and Y(cid:48)(cid:48) a
random variable with range(Y(cid:48)(cid:48)) = (O(cid:48)(cid:48))∗ induced by the
composition f (cid:48)
Theorem 1. H(X|Y(cid:48)(cid:48)) ≥ H(X|Y(cid:48)), for H ∈ {H, H∞, Hg}.

net ◦ fnet.

For the proof of Theorem 1, we use the data process-
ing inequality. Intuitively, this inequality states that com-
putation on data does not increase the amount of informa-
tion contained in this data. This inequality is a well-known

result for Shannon-entropy [11], and also holds for min-
entropy [20] and g-leakage [1].
Lemma 1 (Data Processing Inequality). Let X, Y, Z be ran-
dom variables such that P[Z|XY] = P[Z|Y], i.e. X, Y, Z form
a Markov chain. Let H ∈ {H, H∞, Hg}. Then

H(X|Z) ≥ H(X|Y) .

Proof of Theorem 1. By deﬁnition of ﬁngerprint composi-
tion, the output of Y(cid:48)(cid:48) only depends on the output of Y(cid:48), i.e
X, Y(cid:48), and Y(cid:48)(cid:48) form a Markov chain. This allows application
of Lemma 1, which concludes the proof.

(cid:3)

From Theorem 1 it follows that given a ﬁngerprint fnet,
composing other ﬁngerprints to the left always yields better
security. We conclude this section with the observation that
this is not always the case for composition to the right, i.e.
there are cases when fnet ◦ f (cid:48)
net induces a lower remaining
uncertainty than fnet (see also [20]).

4.2.2 Dummy

The network ﬁngerprint dummy adds redundant ﬁles to ob-
servations. Formally, we deﬁne dummy as a conditional dis-
tribution with
P[F(cid:48)

= (cid:126)o(cid:48)|Fdum = (cid:126)o] > 0 =⇒ (cid:126)o (cid:118) (cid:126)o(cid:48)

,

dum

i.e. the inputs (cid:126)o form a subsequence of the outputs (cid:126)o(cid:48).

4.2.3 Split

The network ﬁngerprint split causes splitting a ﬁle into
smaller ﬁles. Formally, we deﬁne split as a conditional dis-
tribution with

P[F(cid:48)

split

= (m1, d1), . . . ,(mn, dn)|Fsplit = (s, d)] > 0

=⇒ ∀i.di = d ∧ n(cid:88)

i=1

|mi| = |m| .

4.2 Basic network ﬁngerprints

4.2.4 Shuﬄe

In this section, we deﬁne a number of basic network ﬁn-
gerprints that will form the basis for the construction of
countermeasures by composition, and for the modeling of
the attacker observations at diﬀerent layers of the proto-
col stack. In Appendix A, we discuss practical aspects of
the implementation of those network ﬁngerprints as coun-
termeasures.

By Deﬁnition 2, a network ﬁngerprint is applied to an
observation corresponding to the currently visited vertex,
which is a sequence of observable objects and directions in
O = (A × {↑,↓})∗. For convenience of notation, in this sec-
tion we denote sequences of observable objects as (cid:126)o, and
deﬁne some basic ﬁngerprints on single observable objects
o = (m, d) from A × {↑,↓}, where m is the observed mes-
sage and d is its direction. Single observable objects are ex-
tended to sequences of observable objects by applying them
independently to each object in the sequence. Formally,
this corresponds to taking the product of the correspond-
ing distributions. Furthermore, we denote the probability
fnet(o)(o(cid:48)) in random variable notation: P[F(cid:48) = o(cid:48)|F = o].
In the following, we elaborate on each of the basic network
ﬁngerprints.

4.2.1 Padding

Padding is the most commonly considered countermeasure
in the literature (see, e.g., [7, 30, 40]). It changes observa-
tions o = (m, d) by (randomly) increasing the size of the
message |m| and retaining its direction d; formally, it can be
deﬁned as a conditional distribution P[F(cid:48)
P[F(cid:48)

, d(cid:48))|Fpad = (m, d)] > 0 =⇒ d(cid:48) = d∧|m(cid:48)| ≥ |m| .

pad|Fpad] with

= (m(cid:48)

pad

In a webpage containing more than one objects, the base
.html ﬁle is always requested ﬁrst, and the remaining web-
objects are usually downloaded in the order in which they
are included into the .html ﬁle. Ordered sequences of packet
sizes can be used for identifying websites, e.g. see [32]. In
the following, we propose the shuﬄe network ﬁngerprint,
where objects will be requested in an arbitrary order. For-
mally, shuﬄe is deﬁned as a conditional distribution with

P[F(cid:48)

shuﬄe

= (cid:126)o(cid:48)|Fshuﬄe = (cid:126)o] > 0 =⇒ (cid:126)o(cid:48) ∈ Π((cid:126)o) ,

where Π((cid:126)o) is the set of all permutations of the sequence (cid:126)o.
For an example implementation, see Appendix A.

4.2.5

k-parallelism

We deﬁne the k-parallelism (or k-par) network ﬁngerprint
that captures the addition of redundant sequences of obser-
vations, corresponding to k− 1 vertices from G; thus, an ob-
server does not know which of the observations correspond
to the actual vertices, and which to the redundant ones. We
formally deﬁne k-par as a conditional distribution with
P[F(cid:48)

= (o1, . . . , ok)|Fk−par = o] > 0 =⇒ ∃i ∈ [k].oi = o .

k−par

4.2.6

Interleave

When inspecting k concurrent streams of observations, e.g.
corresponding to traﬃc of k users or the result of the k-par
countermeasure, an observer may be able to directly infer
which sequence of observations corresponds to which se-
quence of downloaded ﬁles (e.g. by inspecting TCP headers
of TLS-encrypted data). The interleave network ﬁngerprint

(or inter) turns k consecutive sequences of observations into
one sequence in which the observations of k sequences ap-
pear interleaved, but without changing the relative order of
observations within each sequence. We forgo a formal def-
inition of inter.

4.3 Building composed countermeasures

The basic network ﬁngerprints deﬁned above can be used
as building blocks for complex countermeasures, which
can be obtained using network ﬁngerprint composition (see
Deﬁnition 5). We illustrate this using two examples from
the recent literature: The ﬁrst one corresponds to one of the
modules used in the HTTPOS system [33], the second one
corresponds to the traﬃc morphing countermeasure [41].

4.3.1 HTTPOS

As part of HTTPOS, Luo et al. [33] propose a method which
(1) injects requests for objects, and (2) uses the TCP maxi-
mum segmentation size (MSS) and advertising window op-
tions to reduce packet sizes.

Part (2) works in two stages: First, the ﬁrst m bytes of
an object are split into equal-sized packets. Second, the re-
maining n bytes of the object are split into two packets of
size r and n − r, respectively, for a random r < n. Part
(2) can be cast in terms of the split network ﬁngerprint, and
part (1) can be formally captured by the dummy network ﬁn-
gerprint. Combining both countermeasures by ﬁngerprint
composition, we obtain

dummy ◦ split

as a formalization of the proposed HTTPOS module. A
simple consequence of applying Theorem 1 to this formal-
ization is that that the module indeed oﬀers better security
than using no protection, or split alone.

4.3.2 Traﬃc morphing

Wright et al. [41] propose traﬃc morphing as a countermea-
sure against side-channel attacks. The idea behind traﬃc
morphing is to modify a website’s proﬁle (i.e., distribution
of packet sizes and numbers) by splitting or padding pack-
ets with the goal of mimicking another website’s proﬁle.
This countermeasure can be cast in our model by a suitable
combination of the split and pad countermeasures.

4.4 Soundness with respect to the protocol stack

By choosing a particular application ﬁngerprint, we
model an adversary who can monitor traﬃc at a speciﬁc
protocol layer (e.g. TCP packets), and we assume that the
payload of the corresponding messages is encrypted at some

higher layer. From the perspective of physical security and
veriﬁcation, it is not immediately clear whether the secu-
rity guarantees based on this model extend to adversaries
who can monitor lower layers of the protocol stack, e.g., the
physical layer: For example, proofs of semantic security of
a cryptosystem do not imply security against an adversary
who can monitor timing and power consumption [26]. Like-
wise, in program veriﬁcation, formal proofs of correctness
and security at the level of source code do not immediately
imply correctness or security of the corresponding machine
code.

In this section, we show that the situation is diﬀerent for
the traﬃc analysis problem we consider. There, the secu-
rity guarantees obtained in our model are preserved when
moving to a lower layer of the protocol stack. We obtain
this result by casting the standard operations required for
this (i.e. segmentation, adding packet headers, and tunnel-
ing) as ﬁngerprints according to Deﬁnition 2. Theorem 1
then implies that our security guarantees translate to lower
layers of the stack.

Packet headers When passing through diﬀerent network
layers, each layer adds its own headers. Such headers can
be interpreted as a padding

header = pad .

If the header is of ﬁxed size, then pad will be deterministic.

Packet segmentation Packet segmentation is the process
of dividing a packet into smaller chunks, and each of those
chunks obtains a new header. For example, segmentation is
used by the TCP protocol for congestion control. Formally,
packet segmentation corresponds to

segment = header ◦ split ,

where split is usually deterministic.

Tunneling Tunneling encapsulates traﬃc from one proto-
col into another protocol. It may add a new layer of encryp-
tion (e.g. in the case of a SSH tunnel), in which case none of
the original packet headers will be transmitted in the clear
(see Section 2.1). As a consequence, an attacker may lose
the ability to distinguish which packets correspond to which
web object; additionally, background traﬃc, which is eas-
ily ﬁltered out in the presence of unencrypted headers, may
now appear to be part of the downloaded content. Thus we
model encrypted tunneling as

tunnel = inter ◦ segment ◦ k-par ,

which corresponds to an interleaving of concurrent streams
of segmented packets.

5 Advanced features

The model presented so far does not encompass caching
or timing behavior and, consequently, does not oﬀer pro-
tection against side-channels induced by these features. In
this section we discuss how our model can be extended
with these advanced features. Moreover, we show how our
model allows for specifying that only a part of the user’s
input is sensitive.

5.1 Timing behavior

For closing information leaks due to packet delays, one
can make use of the techniques for predictive mitigation of
timing channels proposed in [2]. The mitigation scheme
buﬀers packets and permits them to enter the network only
at multiples of a particular time quantum q. With this re-
striction, the only source of timing leakage are the points in
time at which no packet is sent. The core idea of predictive
mitigation is to double the time delay between packets af-
ter such time points, which ensures that their number (and
hence the leakage) remains bounded within each time inter-
val. The uncertainty bounds obtained by timing mitigation
are of the form

H(X|Z) ≥ H(X) − O(log2(T + 1)) ,

where X is secret user input, Z is an abstraction of the ob-
servable timing behavior, T is the number of observed time
units, and H ∈ {H, H∞, Hg}. For Shannon entropy, the guar-
antees obtained for the timing channel compose with the
guarantees obtained for the traﬃc channel, as shown in [2]:

H(X|YZ) ≥ H(X|Y) − O(log2(T + 1)) ,

where H(X|YZ) denotes the remaining uncertainty from
user behavior X to an adversary who can simultaneously
observe packet sequences Y and their timing Z. This result
enables us to combine our bounds for traﬃc channels with
bounds for timing channels and obtain a guarantee that ac-
counts for both. The proof of this result relies on the chain
rule and does not immediately extend to H∞ and Hg.

5.2 Modeling browser caches

A browser cache stores downloaded web-objects so that
if an object is requested a second time, it can be fetched
from the cache, avoiding costly communication with the
server. We now show how the deﬁnitions presented in Sec-
tion 2.2 can be extended to take into account the eﬀect
of caching. As in Example 1, we assume vertices are se-
quences of web-objects, i.e. V ⊆ W∗.

For the modeling of the traﬃc induced by a webpage on a
given execution path, a browser cache has the following ef-
fects: (1) the sequence of downloaded web-objects is a sub-
sequence of the web-objects contained in the webpage, and

(2) all web-objects contained in the page have been down-
loaded at some point in the execution path. Both eﬀects are
captured by the following deﬁnition.
Deﬁnition 6. A cached request is a function req: V + → W∗
that takes execution paths v1, . . . , v(cid:96) ∈ Paths(G) and returns
a subset of the web-objects of the terminal vertex v(cid:96).
1. req(v1, . . . , v(cid:96)) (cid:118) v(cid:96), and

2. set(v(cid:96)) ⊆ (cid:83)(cid:96)

i=1 set(req(v1, . . . , vi)) ,

where (cid:118) denotes the subsequence relation, and set(·) de-
notes the set representation of a sequence.

Notice that conditions 1 and 2 deﬁne a family of cache
behaviors (rather than a unique one) for which we implicitly
assume that the cache is empty when the browsing starts.
We can now extend Deﬁnition 3 to capture traﬃc channels
with cache.
Deﬁnition 7. Let G be a web application with ﬁngerprints
fapp and fnet and a browser cache modeled by req. Let X
be a random variable range(X) = Paths(G), and Y a ran-
dom variable with range(Y) = O∗. Then the traﬃc channel
induced by (G, fapp, fnet, req) is the conditional distribution

(cid:96)(cid:89)
P[Y = o1. . .o(cid:96)|X = v1. . .v(cid:96)] =

fnet( fapp(req(v1, . . . , vi))(oi)

i=1

Informally, Deﬁnition 7 states that the traﬃc induced by
each vertex depends on the entire input path, but that the
outputs of the individual network are mutually independent,
given the web-objects requested by the browser.

5.3 Specifying secrets

The measures of security introduced in Section 3.1 cap-
ture bounds on the diﬃculty of guessing the exact value of
X, which corresponds to the entire path taken by the user. In
many cases, however, one is not interested in protecting the
entire path, but rather some sensitive property of the path,
e.g. containment in a critical subset of vertices.
To capture such properties in our model, we specify the
secret as a function sec: Paths(G) → range(sec) of arbi-
trary range that takes as input an execution path and returns
the sensitive information contained in it. From the perspec-
tive of information-ﬂow security, sec can be thought of as
a speciﬁcation of the high parts of a path. The model from
Section 2 can be straightforwardly extended to account for a
speciﬁcation of secrets sec by replacing X with the random
variable Xsec = sec(X).

Examples of relevant instances of sec are the identity
function sec = idPaths(G) (which models that the whole path
is sensitive), a function sec(v1, . . . , v(cid:96)) = {v1, . . . , v(cid:96)} return-
ing a set-representation of a path (which models that the set

of visited vertices is sensitive, but not their ordering), or ar-
bitrary binary predicates, such as sec(v1 . . . , v(cid:96)) = true iﬀ
v ∈ {v1, . . . , v(cid:96)} (which models that it is sensitive whether or
not a vertex v has been visited). Secret speciﬁcations can
be also used to model the scenario where a user wants to
hide the currently visited website, rather than pages within
this website, e.g.
if traﬃc is forced through an SSH tun-
nel (see also Section 2.1). Then, we deﬁne G = (V, E)
as the Internet-graph, and V1, . . . , Vk as the partition of V
into disjoint blocks corresponding to k diﬀerent websites.
The sensitive information in this scenario is captured by the
function sec, which returns the (cid:96) blocks corresponding to
the visited websites.

6 Algorithms for practical evaluation of web

applications

So far we have introduced techniques for reasoning
about the relative strength of countermeasures (Theorem 1).
In this section we devise techniques for the derivation of ab-
solute security guarantees, i.e. concrete numbers for the re-
maining uncertainty H(X|Y) about the user input. For this,
two challenges need to be addressed.

The ﬁrst is that the derivation of absolute guarantees re-
quires making assumptions about (the attacker’s initial un-
certainty H(X) about) the user’s behavior. The second is
that the direct computation of H(X|Y) requires enumerat-
ing the values of X (i.e. all paths) and Y (i.e. all possi-
ble traﬃc patterns), which quickly becomes infeasible. In
this section we present algorithms that enable the eﬃcient
computation of formal security guarantees. Our algorithms
assume Markov models of user behavior and rely on the
chain rule of entropy, which is only satisﬁed by Shannon
entropy H(X). As a consequence, their application is re-
stricted to scenarios where such assumptions can be justi-
ﬁed and where guarantees based on Shannon entropy (see
Section 3) are adequate.

We begin by showing how, under these assumptions, the
initial uncertainty about a user’s navigation in a website
can be computed. We then present a generic approach for
constructing countermeasures based on bisimulations and
for computing the uncertainty induced by such countermea-
sures.

6.1 Modeling user behavior as a Markov chain

We capture user behavior by a random variable X, where
P[X = (v1, . . . , v(cid:96))] describes the probability of the user
taking execution path (v1, . . . , v(cid:96)), see Section 2. The ran-
dom variable X can be decomposed into the components
X1, . . . , X(cid:96) corresponding to the vertices on the path. When
the user’s choice of the next vertex depends only on the cur-
rently visited vertex, then X1, . . . , X(cid:96) form a Markov chain.

Formally: P[Xi+1 = vi+1|Xi = vi, . . . , X1 = v1] = P[Xi+1 =
vi+1|Xi = vi] for i ∈ {1, . . . , (cid:96) − 1}. Then, we obtain

(cid:96)−1(cid:89)

P[X = (v1, . . . , v(cid:96))] = P[X1 = v1]

P[Xi+1 = vi+1|Xi = vi] .

i=1

This decomposition enables one to specify the probabil-
ity of execution paths in terms of probabilities of indi-
vidual transitions. The Markov property is clearly valid
in the auto-complete input ﬁelds from Example 2 where
the execution paths form a tree, and it also is a com-
monly made assumption for predicting navigation behavior
of users [19, 38].

6.2 Computing the initial uncertainty

We next devise an algorithm for computing the initial
uncertainty H(X) about the behavior of a user, based on the
stationary distribution of X and the chain rule of entropy.
We then show how X can be instantiated and how its sta-
tionary distribution can be computed using the PageRank
algorithm.

6.2.1

Initial uncertainty based on stationary distribu-
tions

From the Fundamental theorem of Markov chains [35] it
follows that each ﬁnite, irreducible, and aperiodic Markov
chain converges to a unique stationary distribution. For-
mally, if the Markov chain is given in terms of a transition
matrix Q, where qi, j denotes the probability of moving from
vi to v j, there is a row vector (the stationary distribution) π
with π = πQ.

Under the above conditions, the user’s choice of a next
vertex will converge to π, which we use to capture the prob-
ability of the user choosing the ﬁrst vertex P[X1 = v1]. The
following theorem gives a handle on eﬃciently computing
the initial uncertainty about the user’s execution path.

Theorem 2. Let X1, . . . , X(cid:96) be a Markov chain with P[X1] =
π, where π is the stationary distribution. Then,

H(X1, . . . , X(cid:96)) = H(X1) + ((cid:96) − 1)H(X2|X1).

Proof.

H(X1, . . . , X(cid:96))

(∗)= H(X(cid:96)|X(cid:96)−1, . . . , X1) + ··· + H(X2|X1) + H(X1)
(∗∗)= H(X(cid:96)|X(cid:96)−1) + H(X(cid:96)−1|X(cid:96)−2) + ··· + H(X2|X1) + H(X1)
Here (∗) follows from the chain rule for Shannon entropy
and (∗∗) follows from the Markov property. As P[X1] is the
stationary distribution, we have H(Xk|Xk−1) = H(X j|X j−1)
for all j, k ∈ {2, . . . , n}, which concludes the proof.
(cid:3)

Given the stationary distribution, Theorem 2 enables the
computation of the initial uncertainty H(X) about the user’s
navigation in time O(|V|2), for any ﬁxed (cid:96). We next show
how the Markov chain representation of X can be instan-
tiated, and how its stationary distribution can be obtained,
using the PageRank algorithm.

6.2.2 Using PageRank for practical computation of the

initial uncertainty

PageRank [36] is a link analysis algorithm that has been ap-
plied for predicting user behavior in web usage mining [19].
It relies on the random surfer model, which captures a user
who either follows a link on the currently visited webpage,
or jumps to a random webpage.
Formally, the probability of following links is given by a
transition matrix Q(cid:48), and the probability of random jumping
is 1−α, where the damping factor α ∈ [0, 1] is usually set to
0.85. The user’s behavior is then captured by the transition
matrix Q = αQ(cid:48) + (1 − α)p111 , where p is a row vector rep-
resenting the probability of jumping to pages, and 111 is the
column vector with all 1 entries. One typically assumes uni-
form distributions for the rows q(cid:48)
i of Q(cid:48) and for p, however
the probabilities can also be biased according to additional
knowledge, e.g. obtained by mining usage logs [18].

Given a transition matrix Q of a website, the PageRank
algorithm computes the corresponding stationary distribu-
tion π, and πi is called the PageRank of a webpage i. No-
tice that the conditions for the existence of π are usually
fulﬁlled: irreducibility is guaranteed by a damping factor
α < 1, and in practice websites are aperiodic.

6.3 Constructing path-aware countermeasures

Most traﬃc analysis countermeasures aim to provide
protection by making multiple states of a web-application
indistinguishable for an attacker who can only observe their
traﬃc ﬁngerprints (e.g. see [41]). In our model, we say that
two vertices v1 and v2 are ( f )-indistinguishable for some
countermeasure f whenever f (v1) = f (v2), i.e. the observa-
tions produced by v1 and v2 have the same probability dis-
tribution. Clearly, f -indistinguishability induces a partition
P = {B1, . . . , Bm} on the state space V of a web-application,
where the blocks Bi correspond to the equivalence classes
of f -indistinguishability.

Unfortunately, indistinguishability of individual states
does not protect information gained by observing a se-
quence of states. Consider, e.g., a scenario where an at-
tacker observes traﬃc produced by typing a word in En-
glish. Assume that the ﬁrst typed character is known to be t
and the second character is either h or s, which we assume to
be f -indistinguishable. From the ﬁrst action t, the attacker
can deduce that the second user action is most likely h, i.e.
secret information is revealed.

6.3.1 Ensuring indistinguishability of paths

To protect
the information contained in the transition
between states, we devise path-aware countermeasures,
which require that the induced partition be in fact a bisim-
ulation [29] (also called lumping [25]), a property which
ensures behavioral equivalence of states.
Deﬁnition 8. A partition P = {B1, . . . , Bm} of the state
space V of a Markov chain X = (X1, . . . , X(cid:96)) is a (proba-
bilistic) bisimulation if for any blocks Bi, B j ∈ P and for
v j∈B j qh, j. We deﬁne the cor-
responding quotient process Y = (Y1, . . . , Y(cid:96)) as the random
variable with state space P, such that Yk = Bi iﬀ Xk = v and
v ∈ Bi.

any vi, vh ∈ Bi,(cid:80)

v j∈B j qi, j =(cid:80)

A fundamental characteristic of bisimulations is that they
preserve the Markov property of the resulting quotient pro-
cess.

Theorem 3 (Kemeny-Snell [25]). A partition of the state
space of a Markov chain is a probabilistic bisimulation iﬀ
the corresponding quotient process is a Markov chain.

To measure the strength of a path-aware countermeasure,
we need to calculate the remaining uncertainty H(X|Y), for
which we have

H(X|Y) ≥ H(X) − H(Y),

(1)

with equality whenever Y is determined by X (e.g., if the
corresponding countermeasure is deterministic). By Theo-
rem 3, the resulting quotient process Y is a Markov chain.
This enables us to use Theorem 2 for computing H(Y), lead-
ing to an eﬃcient algorithm returning a lower bound for
H(X|Y).

6.3.2

Implementing path-aware countermeasures

For a given countermeasure f , we seek to group together
vertices in V to blocks of indistinguishable vertices, such
that the resulting partition is path-aware, i.e. a bisimula-
tion. This could be trivially achieved by padding all possi-
ble observations to a maximal element o∗. While the cor-
responding (1-block) partition achieves maximal security, it
may also induce an unacceptably large traﬃc overhead. Our
goal is hence to ﬁnd a bisimulation that coarsens the parti-
tion induced by f and that oﬀers a more attractive trade-oﬀ
between security and performance.

While there are eﬃcient algorithms for computing bisim-
ulations that reﬁne a given partition to a bisimulation, we are
not aware of existing algorithms for obtaining bisimulations
by coarsening. We tackle the problem by the following two-
step approach.

In a ﬁrst step, we compute a set of random bisimulations
on V. To achieve this, we select random initial partitions

of V with only two blocks, e.g. by ﬂipping a coin for each
vertex and selecting a block accordingly. For each of those
two-block partitions, we compute the coarsest bisimulation
that reﬁnes it using the eﬃcient algorithm by Derisavi et
al. [13], which we call CoarsestBisimulation in the re-
mainder of this paper.
In a second step, we coarsen the partition given by f to
the partition P = {B1, . . . , Bm} given by each bisimulation
computed in the previous step. We achieve this by modify-
ing f to a new countermeasure that is a constant function on
each Bi. We consider two such modiﬁcations:

(cid:80)

1. The ﬁrst countermeasure, which we call flimit, returns
for each vertex v ∈ Bi the maximum over all vertices
w ∈ Bi of the sum of the sizes of the observable objects
i |oi,w| ,
for each countermeasure flimit(v) = maxw∈Bi
where f (w) = (o1,w, . . . , ot,w).
If observations con-
sist of one object each, flimit can be implemented by
padding. If observations consist of multiple objects, an
implementation may not be possible without additional
overhead, and thus flimit is a theoretical concept repre-
senting the smallest possible size that can be achieved
without losing information about f , and hence repre-
sents a lower limit for the cost required by any coun-
termeasure inducing a partition {B1, . . . , Bm}.

2. The second countermeasure, which we call forder, ﬁrst
orders for each vertex v ∈ Bi the components in f (v)
according to their (descending) size, and then pads the
k-th component to the maximum size of the k-th com-
ponents over all w ∈ Bi. This countermeasure can be
built from the basic countermeasures pad, dummy, and
shuﬄe (see Section 4.2).

In our experiments in Section 7, we demonstrate the
overhead induced by the practical forder, as well as by the
cost-limiting flimit countermeasures. The results show that
there is room for designing more cost-eﬀective countermea-
sures which induce an overhead closer to flimit; for this to be
achieved, further basic countermeasures can be utilized, e.g.
splitting of objects.

7 Case studies

In this section we report on two case studies in which
we apply the approach presented in this paper to evaluate
the security of countermeasures for a website and an auto-
complete ﬁeld. Moreover, we analyze the overhead in traﬃc
induced by instantiations of the countermeasure to diﬀerent
strengths, which gives a formal account of the ubiquitous
trade-oﬀ between security and performance in side-channel
analysis.

7.1 Web navigation

We analyze a web navigation scenario as described
Example 1. As target we use the Bavarian-language
Wikipedia1 of more than 5,000 articles. We instantiate user
models and network ﬁngerprints as follows.

User model We assume the random surfer model (see
Section 6.2), where a user is navigating through Wikipedia
articles. To instantiate the web-graph G, we crawled the
website only following links leading to the Wikipedia arti-
cle namespace, where the crawling was performed using a
customized wget2. Excluding redirect pages, we obtained a
graph with 3,496 vertices.

Application ﬁngerprint For each vertex v, we took the
application ﬁngerprint fapp(v) to be the tuple containing the
size of the source .html ﬁle and the sizes of the contained
image ﬁles. On average, the total size of each vertex was
104 kilobytes, with a standard deviation of 85 kilobytes.
Taking sizes of web-objects instead of sizes of packets is
a safe approximation whenever encryption happens at the
object-level, see Section 4.4. We did not account for the
sizes of requests because they are almost of equal size in
our example, and padding them to a uniform constant size
is cheap.

Results without countermeasure We compute the initial
uncertainty H(X) about the user’s navigation using Theo-
rem 2, where we obtain the stationary distribution of the
user’s starting point using PageRank, as described in Sec-
tion 6.2. The ﬁrst row in Figure 3 gives the values of H(X)
for varying path lengths (cid:96). An interpretation of the data in
the sense of Proposition 1 shows that for (cid:96) = 5 we already
obtain a lower bound of around 230 for the expected number
of guesses to determine the correct execution path. For the
corresponding sequence of network ﬁngerprints Y (without
any countermeasure applied) and (cid:96) > 1, we consistently
obtain a remaining uncertainty H(X|Y) of 0. This follows
because in our example almost every page is determined by
its ﬁngerprint, and whenever there is ambiguity, the page
can be recovered from observing the path on which it oc-
curs. Also note that a naive computation of H(X) quickly
becomes infeasible, because the number of paths is |V|(cid:96),
due to the completeness of the graph induced by the ran-
dom surfer model.

Results with countermeasure As described in Sec-
tion 6.3, we obtain a number of path-aware countermea-
sures by reﬁning randomly chosen initial partitions. For

1http://bar.wikipedia.org
2http://www.gnu.org/software/wget/

(cid:96)

H(X)
# paths

1

10.1
3496

3
21
236.5

5

31.8
259.8

9

53.4
2106

15
85.9
2176

25

139.9
2295

40
221
2472

Figure 3. Web navigation:
the initial uncer-
tainty H(X) about the user’s navigation in
the regional-language Wikipedia for a varying
path length (cid:96). # paths denotes the number of
execution paths.

each of them, we evaluate the eﬀect on security and perfor-
mance: (1) we compute the remaining uncertainty H(X|Y)
using Equation 1 and Theorem 2; (2) we compute the
relative expected overhead of a countermeasure fnet as
Ev[#( fnet(v))−#( fapp(v))]/Ev[#( fapp(v))], where #(·) denotes
the size of the payload, i.e. the sum of the number of bytes
used for the individual observations. The results below are
obtained for paths of length (cid:96) = 5 and the practical forder
and cost-limiting flimit countermeasures (see Section 6.3).

As reference point we use the countermeasure that makes
all states indistinguishable and induces maximal security,
i.e. H(X|Y) = H(X). On this countermeasure, forder pro-
duces a relative expected overhead of 73.5, while flimit pro-
duces a relative expected overhead of 9.7. Trading secu-
rity for performance, we randomly chose 500 initial parti-
tions, and reﬁned them to path-aware countermeasures us-
ing CoarsestBisimulation (see Section 6.3). The re-
maining uncertainty and the relative expected overhead for
each of those countermeasures are depicted in Figure 4(a)
(for forder), and Figure 4(b) (for flimit). The results spread
from partitions delivering strong security at the price of a
high overhead, to smaller overhead at the price of weaker
security.

Figure 5 details on 8 of the 500 computed bisimulation
partitions. The ﬁrst and the last lines present the trivial
coarsest and ﬁnest partitions, respectively. The data shows
that, when performance is mandatory, one can still achieve
6.63 bits of uncertainty (corresponding a lower bound of 25
expected guesses, according to Proposition 1) with an over-
head of factor 2.75. On the other hand, stronger security
guarantees come at the price of overhead of more than fac-
tor 10. The high required overheads in the web-navigation
scenario can be explained by the large diversity of the sizes
and numbers of the web-objects in the studied pages. As
the next case study shows, the required overheads are much
lower for web applications with more homogeneous web-
objects.

7.2 Auto-complete input ﬁeld

We analyze the scenario of an auto-complete input ﬁeld
as described in Example 2. Unlike graphs corresponding

H(X|Y)

overhead

overhead

31.77
26.7
24.4
16.35
12.32
9.35
6.63

0

forder
73.47
40.01
25.68
13.29
7.31
5.04
2.78

0

flimit
9.71
7.83
5.86
5.11
4.11
3.18
1.97

0

Figure 5. Web navigation: the remaining un-
certainty H(X|Y) about the user’s navigation
and the relative expected overhead for 8 se-
lected bisimulation partitions.

to websites, the graph corresponding to an auto-complete
ﬁeld is a tree with a designated root and a small number of
paths. This allows us to compute the remaining uncertainty
by enumerating the paths in the tree, which is why we can
also consider min-entropy H∞ (see Section 3). In this case
study, we instantiate user models and network ﬁngerprints
as follows.

User model As described in Section 6.1, user behav-
ior is characterized by a probability distribution P[X =
(v1, . . . , v(cid:96))] of possible paths of length (cid:96), which for auto-
complete input ﬁelds corresponds to the set of words of
length (cid:96). To obtain such a distribution, we take as an in-
put dictionary D the 1,183 hyponyms of (i.e., terms in an
is-a relationship with) the word “illness”, contained in the
WordNet English lexical database [21]. We use the number
of results of a corresponding Google query as an approx-
imation of the probability for each term in the dictionary:
We issue such queries for all 1,183 words in the dictionary
D and count their relative frequencies, thereby establishing
probabilities for the leaves in the preﬁx tree corresponding
to D. We traverse the tree towards the root, instantiating the
probability of each vertex as the sum of the probabilities of
its children. Then we instantiate the probabilities of the out-
going edges of each vertex with values proportional to the
probabilities of its children.

Application ﬁngerprint We instantiate the application
ﬁngerprints as follows. The size of the request r(v) (see Ex-
ample 2) is given by the length of the typed word v. The size
of the response s(suggest(v)) is characterized by the size of
the suggestion list given by the Google’s auto-complete ser-
vice on query v. We issued queries for all 11,678 vertices in
the preﬁx tree to instantiate these numbers. The responses
in all cases consisted of only one packet with average size
of 243 bytes and a standard deviation of 97 bytes.

(a) Results with the forder countermeasure

(b) Results with the flimit countermeasure

the remaining uncertainty H(X|Y) about the user’s navigation in the
Figure 4. Web navigation:
regional-language Wikipedia versus the relative expected overhead, using the flimit and forder counter-
measures, for 500 randomly chosen initial partitions. Red stars represent favored resulting partitions,
i.e. resulting in a better security-versus-overhead trade-off than close-by points.

Results without countermeasure We compute initial un-
certainty about the typed word as described in Section 3.
For a varying path length (cid:96) = 1, . . . , 7, the initial uncer-
tainty is between 3.79 and 5.65 bits of Shannon Entropy,
and between 1.61 and 2.85 of min-entropy (see Figure 6).
Unlike graphs corresponding to websites, the graph corre-
sponding to an auto-complete ﬁeld has a tree-structure, and
the number of paths is bounded by the number of terms in
the dictionary D. The initial uncertainty is highest for paths
of a smaller length, and the decrease of the uncertainty for
longer paths occurs because the transition relation reveals
there are less words in D sharing the
more information:
same preﬁx.
In all cases the remaining uncertainty after
observing the traﬃc was 0, meaning that in those cases the
web application is highly vulnerable to attacks: an attacker
can infer the secret input word with probability 1.

(cid:96)

H(X)
H∞(X)
# paths

1

4.22
2.7
47

2

5.09
2.85
238

3

5.52
2.65
538

4

5.65
2.39
657

5

5.52
1.96
710

6

5.51
1.71
773

7

5.32
1.61
804

the initial uncer-
Figure 6. Auto-complete:
tainty (in terms of Shannon entropy H(X) and
min-entropy H(X|Y)) about the typed word in
the auto-complete input ﬁeld for a varying
path length (cid:96). # paths denotes the number
of execution paths.

flimit, which here can be practically implemented because
the observations consist of single ﬁles (see Section 6.3).
In the following, we report on our experiments for a path
length of (cid:96) = 4, where we obtain the following results:

First, padding all vertices to a uniform size results in
a maximal uncertainty of 5.65 bits of Shannon entropy
and 2.39 bits of min-entropy, respectively, with an rela-
tive expected overhead of 2.9. Second, padding only ver-
tices at the same depth to a uniform size, the relative ex-
pected overhead drops to 0.52. The resulting uncertainty
remains maximal because, in our model, the depth of a ver-
tex is common knowledge. Finally, trading security for per-
formance, we construct path-aware countermeasures using
CoarsestBisimulation, for 500 randomly chosen initial
partitions (see Section 6.3). Figure 7 depicts the trade-oﬀ
between remaining uncertainty and relative expected over-
head in this case. For example, the data show that the over-
head needed for maximal protection can be decreased by
46%, for the price of leaking of 1.44 bits of Shannon en-
tropy (0.54 bits of min-entropy), and by 65%, for the price
of leaking 1.97 bits of Shannon entropy (0.78 bits of min-
entropy). Note that in most cases the two considered en-
tropy measures favor the same partitions, as depicted by the
partitions corresponding to the red stars in Figure 7(a) and
Figure 7(b) coincide. This is not always the case: the par-
tition denoted as a green × is only favored by the Shannon
entropy, and the partition denoted as a yellow triangle is
only favored by the min-entropy.

8 Related work

Results with countermeasure We also measured the vul-
nerability of the auto-complete ﬁeld with countermeasures
applied. States were made indistinguishable by applying

There is a long history of attacks that exploit visible pat-
terns in encrypted web traﬃc. The ﬁrst attacks for extract-

(a) Shannon entropy

(b) Min-entropy

Figure 7. Auto-complete:
the remaining uncertainty about the typed word in the auto-complete
input ﬁeld versus the relative expected overhead, for 500 randomly chosen initial parititions, using
Shannon entropy and min-entropy as a measure. Red stars represent favored resulting partitions, i.e.
resulting in a better security-versus-overhead trade-off than close-by points. The green × denotes a
partition that is only favored by the Shannon entropy; the yellow triangle denotes a partition that is
only favored by the min-entropy.

ing user information from the volume of encrypted web
traﬃc were proposed in by Cheng and Avnur [8]; since
then there have been several published attacks of this kind,
e.g. [4, 7, 10, 12, 17, 22, 23, 30, 32, 40]. There has been an
evolution of the goals which such attacks pursue, and the
of settings of those attacks. While the goal of the attack
presented in [8] is to identify a webpage within a website
accessed through HTTPS, later attacks aim at identifying
the website a user is visiting when browsing through an en-
crypted tunnel. Those attacks target HTTPS connections to
anonymizing proxies [12, 23, 40], SSH or a VPN connec-
tions to anonymizing proxies [4, 22, 30, 32], data contained
in anonymized NetFlow records [10], or onion routing and
web mixes [22, 37]. Chen et al. [7] turn the focus of their
attacks to web applications accessed through HTTPS and
WPA, and the goal of those attacks is the extraction of sensi-
tive information about user’s health conditions and ﬁnancial
status.

Several works propose countermeasures against those at-
tacks. Padding, i.e., adding noise to observations, is the
standard countermeasure and has been proposed by a large
number of authors [7, 8, 12, 23, 30, 32, 40]. A diﬀerent ap-
proach proposed by [40] and implemented by [41] changes
patterns of observations corresponding to one website to
look like patterns corresponding to another website, which
allows sensitive traﬃc to be camouﬂaged as traﬃc com-
ing from popular services. Features of the TCP and HTTP
protocols were utilized in the techniques proposed by Luo
et al. [33] used to build the HTTPOS system which of-
fers browser-side protection from traﬃc analysis. We have
shown how parts of HTTPOS and other previously pro-
posed countermeasures can be cast in our model, and how

we can use it to formally reason about them.

A number of works empirically evaluate the security of
proposed countermeasures [8, 30, 33, 37, 40, 41]. There, the
security is measured by comparing the performance of at-
tacks before and after the application of the countermea-
sures.
In recent work, Dyer et al. [17] exhibit shortcom-
ings in this kind of security evaluation. In particular, they
demonstrate the inability of nine previously known coun-
termeasures to mitigate information leaks, even when only
coarse traﬃc features are exploited, such as total bandwidth
or total time. In contrast, we propose a formal framework
which does not assume the use of particular attacks, but
rather measures security in terms of the amount of infor-
mation leaked from the produced observations.

Sidebuster [42] is a language-based approach to coun-
tering side-channel attacks in web applications.
It uses a
combination of taint-based analysis and repeated sampling
to estimate the information revealed through traﬃc patterns,
however without an explicitly deﬁned system model. Our
goal for future work is to use our model for providing a
semantic basis for the security guarantees derived by such
language-based approaches. Liu et al. [31] present a traf-
ﬁc padding technique that is backed up by formal guaran-
tees. In contrast to our work, their model does not account
for an attacker’s prior knowledge or for the probabilistic
nature of traﬃc patterns. Finally, [5] propose a black-box
web application crawling system that interacts with the web
application as an actual user while logging network traﬃc.
They view an attacker as a classiﬁer and quantify informa-
tion leaks in terms of classiﬁer performance, using metrics
based on entropy and on the Fisher criterion. Their metrics
are computed from a small number of samples, which can

deliver imprecise results for entropy [3]. In contrast, we ap-
ply our metric to the entirety of paths, which is possible due
to the random surfer assumption. Finally, work in quantita-
tive information-ﬂow analysis has been applied for assess-
ing the information leakage of anonymity protocols [6] and
cryptographic algorithms [27].

9 Conclusions and future work

We have presented a formal model that enables reasoning
about side-channel attacks against web applications. In our
model, the web application’s encrypted traﬃc is cast as an
information-theoretic channel, which gives a clean interface
to the growing body of research in quantitative information-
ﬂow analysis and allows us to use established notions of
quantitative security. We have demonstrated that our model
is expressive enough to encompass web browsing, simple
web applications, and several countermeasures from the re-
cent literature. Furthermore, we have demonstrated algo-
rithms allowing the eﬃcient derivation of security guaran-
tees for real systems.

Our long-term goal is to use the presented model as a se-
mantic basis for language-based approaches for reasoning
about side-channel attacks against web applications, such
as [5,42]. Progress along these lines could lead to principled
analysis techniques with soundness guarantees that hold for
the whole protocol stack. As intermediate steps, we will in-
vestigate methods for approximating fnet by sampling, and
we will investigate how quality guarantees for the sampling
aﬀect the ﬁnal security guarantees. Moreover, we will in-
vestigate alternative notions of conﬁdentiality [16] for sce-
narios in which we cannot justify assumptions about the un-
certainty about the user’s behavior.

Acknowledgments This work was partially funded by
European projects FP7-256980 NESSoS, FP7-229599
AMAROUT, and Spanish project TIN2009-14599 DE-
SAFIOS 10.

References

[1] M. Alvim, K. Chatzikokolakis, C. Palamidessi, and
G. Smith. Measuring Information Leakage using Gener-
In Proc. IEEE Computer Secu-
alized Gain Functions.
rity Foundations Symposium (CSF),, pages 265–279. IEEE,
2012.

[2] A. Askarov, D. Zhang, and A. C. Myers. Predictive black-
In Proceedings of the
box mitigation of timing channels.
17th ACM Conference on Computer and Communications
Security (CCS), pages 297–307. ACM, 2010.

[3] T. Batu, S. Dasgupta, R. Kumar, and R. Rubinfeld. The com-
plexity of approximating entropy. In Proc. ACM Symp. on
Theory of Computing (STOC), pages 678–687. ACM, 2002.

[4] G. Bissias, M. Liberatore, D. Jensen, and B. N. Levine.
Privacy Vulnerabilities in Encrypted HTTP Streams.
In
Proc. Privacy Enhancing Technologies (PET), pages 1–11.
Springer, 2005.

[5] P. Chapman and D. Evans. Automated black-box detection
of side-channel vulnerabilities in web applications. In Proc.
18th ACM Conference on Computer and Communications
Security (CCS), pages 263–274. ACM, 2011.

[6] K. Chatzikokolakis, C. Palamidessi, and P. Panangaden.
Inf. Comput.,

Anonymity protocols as noisy channels.
206(2-4):378–401, 2008.

[7] S. Chen, R. Wang, X. Wang, and K. Zhang. Side-channel
leaks in web applications: a reality today, a challenge to-
morrow. In IEEE Symposium on Security and Privacy (SSP),
pages 191–206. IEEE, 2010.

[8] H. Cheng, H. Cheng, and R. Avnur. Traﬃc analysis of ssl

encrypted web browsing, 1998.

[9] T. H. Cormen, C. Stein, R. L. Rivest, and C. E. Leiserson.
Introduction to Algorithms. McGraw-Hill Higher Education,
2nd edition, 2001.

[10] S. E. Coull, M. P. Collins, C. V. Wright, F. Monrose, and
M. K. Reiter. On web browsing privacy in anonymized net-
In Proc. 16th USENIX Security Symposium, pages
ﬂows.
23:1–23:14. USENIX Association, 2007.

[11] T. M. Cover and J. A. Thomas. Elements of Information

Theory. Wiley, second edition, 2006.

[12] G. Danezis. Traﬃc analysis of the http protocol over tls,

2007.

[13] S. Derisavi, H. Hermanns, and W. H. Sanders. Optimal
Inf. Process. Lett.,

state-space lumping in markov chains.
87(6):309–315, 2003.

[14] T. Dierks and C. Allen. The TLS protocol version 1.0, 1999.
[15] G. Doychev. Analysis and mitigation of information leaks
in web applications. Master’s thesis, Saarland University,
Germany, 2012.

[16] C. Dwork. Diﬀerential Privacy. In Proc. 33rd Intl. Collo-
quium on Automata, Languages and Programming (ICALP),
pages 1–12. Springer, 2006.

[17] K. Dyer, S. Coull, T. Ristenpart, and T. Shrimpton. Peek-a-
Boo, I still see you: Why Traﬃc Analysis Countermeasures
In IEEE Symposium on Security and Privacy (SSP),
Fail.
pages 332–346. IEEE, 2012.

[18] M. Eirinaki and M. Vazirgiannis. Usage-Based PageRank
for Web Personalization. In Proc. 5th IEEE Intl. Conference
on Data Mining (ICDM), pages 130–137. IEEE, 2005.

[19] M. Eirinaki, M. Vazirgiannis, and D. Kapogiannis. Web path
recommendations based on page ranking and markov mod-
els. In Proc. 7th ACM Workshop on Web Information and
Data Management (WIDM), pages 2–9. ACM, 2005.

[20] B. Espinoza and G. Smith. Min-entropy leakage of chan-
nels in cascade. In 8th Intl. Workshop on Formal Aspects in
Security and Trust (FAST), pages 70–84. Springer, 2011.

[21] C. Fellbaum, editor. WordNet:

an electronic lexical

database. MIT Press, 1998.

[22] D. Herrmann, R. Wendolsky, and H. Federrath. Website ﬁn-
gerprinting: attacking popular privacy enhancing technolo-
In Proc.
gies with the multinomial naive-bayes classiﬁer.
ACM Workshop on Cloud Computing Security (CCSW),
pages 31–42. ACM, 2009.

[23] A. Hintz. Fingerprinting websites using traﬃc analysis. In

Privacy Enhancing Technologies (PET), 2002.

[24] D. C. Howe and H. Nissenbaum. TrackMeNot: Resisting
surveillance in web search. In I. Kerr, V. Steeves, and C. Lu-
cock, editors, Lessons from the Identity Trail: Anonymity,
Privacy, and Identity in a Networked Society, chapter 23,
pages 417–436. Oxford University Press, Oxford, UK, 2009.
[25] J. Kemeny and J. Snell. Finite Markov Chains. Undergrad-

uate Texts in Mathematics. Springer-Verlag, 1960.

[26] P. Kocher. Timing Attacks on Implementations of Diﬃe-
In Proc. An-
Hellman, RSA, DSS, and Other Systems.
nual Intl. Cryptology Conference (CRYPTO), pages 104–
113. Springer, 1996.

[27] B. K¨opf and D. Basin. An Information-Theoretic Model
for Adaptive Side-Channel Attacks. In Proc. ACM Conf. on
Computer and Communications Security (CCS), pages 286–
296. ACM, 2007.

[28] B. K¨opf and M. D¨urmuth. A Provably Secure and Eﬃcient
In Proc. IEEE
Countermeasure against Timing Attacks.
Computer Security Foundations Symposium (CSF), pages
324–335. IEEE, 2009.

[29] K. G. Larsen and A. Skou. Bisimulation through probabilis-

tic testing. Inf. Comput., 94(1):1–28, 1991.

[30] M. Liberatore and B. N. Levine.

Inferring the Source of
In Proc. ACM Conference
Encrypted HTTP Connections.
on Computer and Communications Security (CCS), pages
255–263. ACM, 2006.

[31] W. M. Liu, L. Wang, K. Ren, P. Cheng, and M. Debbabi.
k-indistinguishable traﬃc padding in web applications. In
Proc. Privacy Enhancing Technologies (PET), pages 79–99.
Springer, 2012.

[32] L. Lu, E.-C. Chang, and M. C. Chan. Website Fingerprint-
ing and Identiﬁcation Using Ordered Feature Sequences. In
Proc. 15th European Symposium on Research in Computer
Security (ESORICS). Springer, 2010.

[33] X. Luo, P. Zhou, E. W. W. Chan, W. Lee, and R. K. C.
Chang. HTTPOS: Sealing information leaks with browser-
side obfuscation of encrypted ﬂows. In Proc. Network and
Distributed Systems Symposium (NDSS). The Internet Soci-
ety, 2011.

[34] J. L. Massey. Guessing and Entropy.

In Proc. IEEE Intl.
Symp. on Information Theory (ISIT), page 204. IEEE, 1994.
[35] R. Motwani and P. Raghavan. Randomized Algorithms.

Cambridge University Press, 1995.

[36] L. Page, S. Brin, R. Motwani, and T. Winograd. The pager-
ank citation ranking: Bringing order to the web. Technical
Report 1999-66, Stanford InfoLab, November 1999.

[37] A. Panchenko, L. Niessen, A. Zinnen, and T. Engel. Website
Fingerprinting in Onion Routing Based Anonymization Net-
works. In Proc. ACM Workshop on Privacy in the Electronic
Society (WPES). ACM, 2011.

[38] R. Sarukkai. Link prediction and path analysis using markov

chains. Computer Networks, 33(1-6):377–386, 2000.

[39] G. Smith. On the foundations of quantitative information
ﬂow. In Proc. Foundations of Software Science and Compu-
tation Structures (FoSSaCS ’09), pages 288–302. Springer,
2009.

[40] Q. Sun, D. R. Simon, Y.-M. Wang, W. Russell, V. N. Pad-
manabhan, and L. Qiu. Statistical identiﬁcation of encrypted
web browsing traﬃc. In IEEE Symposium on Security and
Privacy (SSP), pages 19–30. IEEE, 2002.

[41] C. V. Wright, S. E. Coull, and F. Monrose. Traﬃc morphing:
An eﬃcient defense against statistical traﬃc analysis.
In
Proc. Network and Distributed Systems Symposium (NDSS).
The Internet Society, 2009.

[42] K. Zhang, Z. Li, R. Wang, X. Wang, and S. Chen. Side-
automated detection and quantiﬁcation of side-
buster:
In Proc.
channel leaks in web application development.
ACM Conference on Computer and Communication Secu-
rity (CCS), pages 595–606. ACM, 2010.

A Implementing basic countermeasures

In the following, we discuss implementation considera-
tions for each of the basic network ﬁngerprints presented
in Section 4. We distinguish between two kinds of ﬁnger-
prints: We say that a ﬁngerprint is deployed in a protocol-
independent manner if the traﬃc is modiﬁed before enter-
ing the protocol stack, and in a protocol-dependent manner
if the traﬃc is modiﬁed within the protocol stack.

Padding The countermeasure of up to 255 bytes padding
of packets has been speciﬁed in the TLS protocol [14].
Padding can also be deployed independently of the proto-
col: On the server side, it can be implemented by augment-
ing web-objects with variable-length comments or special
characters that will not be displayed by the browser, which
is possible in most languages (HTML, JavaScript), and in
ﬁle formats (JPEG, FLV). On the client-side, protocol de-
pendent approaches are possible, e.g. by deploying the
HTTP Range option, or by causing TCP retransmissions,
see [33].

Dummy On the server, protocol-independent deployment
of dummy as a countermeasure is possible e.g. by includ-
ing hidden objects in an HTML ﬁle. On the client side,
protocol-independent deployment is possible by requesting
existing ﬁles from the server. For this, the client should
know which ﬁles reside on the server. On the client side,
a protocol-dependent approach for implementing dummy is
to use HTTP range and TCP retransmission (see [33]).

Split On the server-side, a protocol-dependent deploy-
ment of split as a countermeasure is possible on the TCP
layer, by segmenting a packet before it is sent on the net-
work. On the client side, a protocol-dependent deployment
is possible, e.g. by setting TCP advertising window, or by
utilizing HTTP Range (see [33]). Splitting a ﬁle will in-
cur an overhead because of additional packet headers and
control messages.

Shuﬄe On the server side, deployment of shuﬄe is possi-
ble by preloading all web-objects in random order and us-
ing JavaScript to display the content of the HTML in or-
dered form, see [15] for an example implementation. On
the server and client side, straightforward implementations
are possible where all but the ﬁrst (.html) ﬁle are requested
in shuﬄed order.

k-parallelism On the client side, a protocol-independent
deployment of k-par is possible, e.g. by requesting k− 1 ad-
ditional vertices for each visited vertex (similar to the func-
tionality oﬀered by [24]). On the server-side, a protocol-
independent implementation is possible by including pages
in hidden iframes.

Interleave To implement this countermeasure, it should
become impossible to tell which ﬁles belong to which
stream. A protocol-dependent approach to achieve this is
by utilizing the HTTP pipelining option (see [33]), or by
tunneling traﬃc through an encrypted proxy.

