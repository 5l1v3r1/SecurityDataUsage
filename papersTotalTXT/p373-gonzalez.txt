User Proﬁling in the Time of HTTPS

Roberto Gonzalez
NEC Labs. Europe
ﬁrst.last@neclab.eu

Claudio Soriente
Telefonica Research

ﬁrst.last@telefonica.com

Nikolaos Laoutaris
Telefonica Research

ﬁrst.last@telefonica.com

ABSTRACT
Tracking users within and across websites is the base
for proﬁling their interests, demographic types, and
other information that can be monetised through tar-
geted advertising and big data analytics. The advent
of HTTPS was supposed to make proﬁling harder for
anyone beyond the communicating end-points.
In
this paper we examine to what extent the above is
true. We ﬁrst show that by knowing the domain that
a user visits, either through the Server Name Indica-
tion of the TLS protocol or through DNS, an eaves-
dropper can already derive basic proﬁling informa-
tion, especially for domains whose content is homo-
geneous. For domains carrying a variety of categories
that depend on the particular page that a user visits,
e.g., news portals, e-commerce sites, etc., the basic
proﬁling technique fails. Still, accurate proﬁling re-
mains possible through traﬃc ﬁngerprinting that uses
network traﬃc signatures to infer the exact page that
a user is browsing, even under HTTPS. We demon-
strate that transport-layer ﬁngerprinting remains ro-
bust and scalable despite hurdles such as caching, dy-
namic content for diﬀerent device types etc. Overall
our results indicate that although HTTPS makes pro-
ﬁling more diﬃcult, it does not eradicate it by any
means.
1.

INTRODUCTION

Online user proﬁling is a proﬁtable business exten-
sively carried out by third parties such as search en-
gines, ad networks and network providers. It lever-
ages browsing activities to infer user interests and
intentions. Since HTTP traﬃc has no privacy pro-
visions, any third party can pry on the connections

Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
IMC 2016, November 14-16, 2016, Santa Monica, CA, USA
c(cid:13) 2016 ACM. ISBN 978-1-4503-4526-2/16/11. . . $15.00
DOI: http://dx.doi.org/10.1145/2987443.2987451

to a websever and proﬁle users. HTTPS enhances
online user privacy by encrypting the communication
between a browser and a webserver. Major internet
stakeholders are pushing for an HTTPS everywhere
web with the promise of increased security and pri-
vacy and, therefore, of mitigating the problem of user
proﬁling by third parties.

In this paper we assess the extent by which HTTPS
prevents third parties from proﬁling users based on
the websites they visit. We show that the widely used
Server Name Indication (SNI) extension of the TLS
protocol leaks user interests to third parties which
eavesdrop on the (encrypted) connection between a
client and an HTTPS webserver. The SNI exten-
sion improves address-space utilization as it allows
to consolidate several HTTPS webservers at a given
IP address. However, SNI also hinders user privacy
as it leaks the domain requested by a user, despite the
HTTPS pledge of a secure and private connection.

The privacy leakage due to SNI is especially se-
vere in websites with homogenous content across its
pages. For example, a connection request to www.
foxsports.com/ tells a lot about the user interests, re-
gardless of the actual page the user is browsing within
that website. For a website with more variety across
its pages, the domain requested by a user may not
tell enough about the interests of that user. For ex-
ample, a connection to www.amazon.com/ may not
tell much about a user. However a connection to
www.amazon.com/books/ would reveal interests in
books and a connection to www.amazon.com/baby/
may indicate an intent to buy baby items. We show
that by using traﬃc ﬁngerprinting techniques, a net-
work eavesdropper can accurately tell the page a user
is browsing within a domain and, therefore, build a
reﬁned user proﬁle.

Users proﬁling despite HTTPS is achievable, given
enough bandwidth to ﬁngerprint the websites under
observation. In case bandwidth is an issue, we also
deﬁne an optimization problem that allows an eaves-
dropper to periodically pick the websites to ﬁnger-
print in order to maximize the number of users that
are accurately proﬁled over time.

373Overall, our ﬁndings show that HTTPS, while be-
ing a formidable tool to strengthen the security of
web applications, cannot protect users against online
proﬁling by third parties.
2. BACKGROUND AND MODEL

User proﬁling.

Proﬁling systems often use a closed-source map-
ping between URLs and interest categories. We fol-
low the approach of previous work [1] and instantiate
the mapping using the Display Planner of Google Ad-
Words [2] – an online tool that given a URL returns
the set of categories assigned by AdWords to that
URL. Categories are arranged in a hierarchy and each
URL has, on average, 10 assigned categories. The
Display Planner also provides the inverse mapping,
i.e., given a category it provides a list of websites
that belong to that category.

HTTPS and Server Name Indication.

HTTPS enhances HTTP with the Transport Layer
Security (TLS) protocol. TLS provides a secure pipe
to a server that is usually authenticated via an X.509
certiﬁcate. The secure pipe is established via a TLS
handshake – a procedure that allows the client and
the server to establish cryptographic keys to encrypt
and authenticate data exchanged through the pipe.

Given the ever increasing awareness on the privacy
issues of HTTP, major web stakeholders are mandat-
ing secure (i.e., HTTPS) connections to serve their
websites [3, 4]. Furthermore the Tor Project and EFF
promote the HTTPS Everywhere extension [5, 6],
that automatically redirects browsers to the HTTPS
version of a website when available. One goal of this
collective eﬀort towards an HTTPS web is to increase
online privacy with respect to network eavesdroppers.
HTTPS ensures that a user is connected to the legit-
imate webserver and that the exchanged information
cannot be eavesdropped by third parties.1

Server Name Indication (SNI) is an extension of the
TLS protocol by which a client speciﬁes the hostname
it is attempting to connect in the client_hello mes-
sage (the ﬁrst message of a TLS handshake). The ex-
tension is widely used by modern browsers and allows
a server to serve multiple HTTPS websites, each with
its own X.509 certiﬁcate, from the same IP address.
The SNI is, therefore, sent in cleartext and can be
eavesdropped by any party tapping on the network
between the client and the server.

System Model.

We consider a network eavesdropper that tries to
proﬁle users by tapping on their network connections.

We assume an HTTPS everywhere web where users
connect to any website via HTTPS. The network eaves-
dropper, therefore, does not have access to the clear-
text traﬃc exchanged between the user browser and
the webservers but only sees encrypted ﬂows. How-
ever, we assume the eavesdropper can infer the host-
name requested by the user by looking at the SNI
in the client_hello message.
In case SNI is not
used, client queries to DNS (recall that DNS has no
provisions for conﬁdentiality) or simply a whois on
the destination IP address may reveal the hostname
requested by the user.

We simplify the structure of a website and the user
browsing behaviour as follows. Each website has a
main page and a set of 1-st level pages (i.e., the pages
linked on the main page). We do not consider pages of
the website beyond the ones linked on the main page,
but our results can be easily generalized to account for
more complex website structures. Similar to previous
work [7, 8, 9, 10, 11], we assume a user visits one page
at a time for each domain.2 This could be either
the main page, or any of the 1-st level pages. The
eavesdropper tries to infer the page visited by the
user and assigns to her proﬁle the corresponding set
of categories according to Google AdWords.

3. USER PROFILING BY SNI

Looking at the SNI in the client_hello message,
a basic eavesdropper learns the website a user is brows-
ing and assigns the categories of the main page to the
user proﬁle. If the user were actually browsing a page
diﬀerent from the main one, the proﬁle built by the
basic eavesdropper may not be accurate.

A ﬁrst step towards understanding the accuracy
of user proﬁling in an HTTPS everywhere web con-
sists in assessing the diﬀerence between the categories
of a website main page (e.g., the categories of www.
nbcnews.com/) and the ones of any of its 1-st level
pages (e.g, the categories of www.nbcnews.com/politics/).

In this experiment we have collected the list of top
websites returned by AdWords for each of its 24 ﬁrst
level categories.Within each list, we have selected the
100 most popular websites based on their rank in
Alexa [12]. For each of the resulting 2.4K websites
we have fetched the URL of all the links available on
the main page that remains within the same host. We
did not consider external links like the ones to CDNs.
Each of the collected URLs (totalling to more than
110K URLs) was submitted to the AdWords Display
Planner to obtain its set of categories.

For each of the 24 top level categories of AdWords,
Figure 1 shows the distribution of the Jaccard index
among the categories assigned to the main page of a
website and the categories assigned to its 1-st level

1Security guarantees of HTTPS do not take into ac-
count phishing attacks or ﬂaws in the public key cer-
tiﬁcation system.

2Discerning traﬃc when multiple pages of a domain
are fetched simultaneously using HTTPS, remains an
open problem.

374Figure 1: Distribution of the Jaccard index among the categories of the main page and the 1st-level pages.

pages. A Jaccard index close to 1 means that simply
assigning the categories of the main page to a user
creates a quite accurate proﬁle, regardless of the ac-
tual page the user is browsing within that website. A
Jaccard index close to 0 means that the same proﬁle
technique may lead to a less accurate user proﬁle.

Figure 1 shows a great variance depending on the
main category of the website. Users visiting Sports,
Real Estate or Games websites could be proﬁled very
accurately only by knowing the website their are con-
nected to. However, when a user visits any page
within a website related to, e.g., Shopping, Comput-
ers & Electronics or News, the user proﬁle built by
assigning her the categories of the main page is likely
to be inaccurate.
4. TRAFFIC FINGERPRINTING

In this section we show how to improve proﬁling ac-
curacy by inferring the exact page a user is browsing
using traﬃc ﬁngerprinting. Traﬃc ﬁngerprinting [7,
8, 9, 10] is an active research area on techniques to
infer information (such as the visited page on an en-
crypted connection) by solely observing traﬃc pat-
terns at the network/transport level.

Fingerprinting involves a training phase during which

the adversary builds a ﬁngerprint of each of the mon-
itored pages. This is accomplished by fetching multi-
ple times the monitored pages and recording features
of the generated traﬃc such as packet size or inter-
arrival times. Later, the adversary eavesdrops on the
client’s connection, extracts the same features from
the client’s traﬃc, and tries to match the client trace
to one of the ﬁngerprints computed during the train-
ing phase. Diﬀerences between the training data and
the client (or test) data, due to, e.g., diﬀerent routes
or congestions are mitigated using statistical meth-
ods.

We use and adapt to our scenario the ﬁngerprinting
technique of [11] – the most accurate web ﬁngerprint-
ing framework to date – that uses as features the size
and the direction of each packet of a TCP connection.
The classiﬁer is, therefore, robust against diﬀerences
in bandwidth or congestions along the route. The au-
thors of [11] show that page ﬁngerprinting is hard in
an open-world scenario in which the client can browse

Figure 2: Accuracy of the classiﬁer when fetching
pages from a PC (with and without cache) and from
a Mobile Device.

any page outside of the set monitored by the eaves-
dropper. We show that webpage ﬁngerprinting can
be reasonably accurate in a closed-world scenario in
which the eavesdropper monitors all the pages that
the client can possibly visit. This assumption is real-
istic in our settings because the eavesdropper knows
the website requested by the user (by looking at the
SNI in the client_hello message) and must infer
which page she is browsing within that website.

The features we extract from the traﬃc generated
by downloading a page include: the number of in-
coming packets and the number of outgoing ones, the
total size of incoming packets and the total size of out-
going ones, and a trace deﬁned over the size and the
order of the observed packets.3 We use an SVM clas-
siﬁer with an RBF kernel with γ, c ∈ [0.001, 10000].
For each website, we capture with tcpdump the traﬃc
generated by fetching each of the 1-st level pages 50
times and measure the accuracy of the classiﬁer using
10-fold cross validation.

Classiﬁer Accuracy.

For this experiment we pick 9 websites that have
low Jaccard index between the main page and the 1-st

3Given the space constraints, we refer the reader
to [11] for the details on the feature selection process.

ShoppingScienceComputers&ElectronicsBusiness&IndustrialOnlineCommunitiesNewsInternet&TelecomPets&AnimalsBeauty&FitnessFinanceArts&EntertainmentTravelHobbies&LeisureBooks&LiteratureHome&GardenFood&DrinkLaw&GovernmentJobs&EducationAutos&VehiclesReferencePeople&SocietySportsRealEstateGames0.00.20.40.60.81.0JaccardIndexamazon.comrakuten.comaarp.comwonderhowto.comabout.commashable.comslashdot.orgnbcnews.comreuters.com0.00.20.40.60.81.0ClassiﬁerAccuracyPC(NoCache)PC(Cache)MobileDevice375papers, retailers and other websites such as the main
railway company in Spain (www.renfe.com/).

We have trained a classiﬁer for each one of the se-
lected website in the “PC, No-cache” scenario. As
shown in Figure 3, proﬁling accuracy is higher than
90% for 18 of the 33 websites, and higher than 70%
for 29 of them. We experienced worse results on
4 websites: the proﬁling accuracy on www.antena3.
com/, www.tripadvisor.es/, www.fnac.es/ and www.
ebay.es/ was 13, 41, 45 and 49% respectively. Those
websites use a common template to show most of the
internal pages and leverage CDNs to deliver content.
Since our system does not consider data fetched from
other domains (i.e., CDNs) the common template of
the internal pages make webpage prediction a hard
task. We speculate that jointly considering the data
fetched from CDNs will increase the proﬁling accu-
racy but defer this to future work.

From Page Prediction to User Proﬁling.

In this experiment we take a closer look at the eﬀect
of the classiﬁer accuracy on the quality of the user
proﬁles built by the eavesdropper.

Figure 4a shows the confusion matrix for edition.
cnn.com/ where pages are sorted lexicographically
based on their URL. For the same website and the
same sorting of its pages, the matrix in Figure 4b
shows the Jaccard index between any pair of 1st-
level pages. Due to the sorting, pages under the same
branch of the website, say edition.cnn.com/style/ ap-
pear sequentially, in both matrices. Figure 4a shows
that when the classiﬁer makes a mistake, the output
page tends to be “close” to the correct one. For exam-
ple edition.cnn.com/style/arts/ is often mis-classiﬁed
as edition.cnn.com/style/fashion/ and viceversa. This
is because the features we use to train the classiﬁer
look at the structure of a page (e.g., the number and
position of textboxes) rather than its content (e.g.,
the actual text). Therefore, when pages within the
same branch of a website share a similar structure,
we experience classiﬁcation mistakes similar to the
ones of Figure 4a.

When mis-classiﬁcation happens, the amount of
damage to user proﬁling accuracy depends on whether
the categories of the true page and the categories of
the page output by the classiﬁer overlap or not. For
example, because of their similar structure, edition.
cnn.com/asia/ is likely to be predicted as edition.cnn.
com/africa/ by the classiﬁer (see box 1 in Figure 4a);
however, given that the set of their categories is very
similar (see box 1 in Figure 4b), the mistake of the
classiﬁer has very little impact on the quality of user
proﬁling. Of course this is not always the case. For
example the pages under edition.cnn.com/style/ (see
box 2 in Figure 4a) are likely to be confused with
one an other by the classiﬁer. This, however, leads
to high proﬁling error because diﬀerent pages under

Figure 3: Accuracy of the classiﬁer for the most pop-
ular pages in Spain.

level pages (see Figure 1). For each website we train
the classiﬁer and test its accuracy in three diﬀerent
scenarios. We use a PC with Mozilla Firefox with
and without cache, and a mobile device with Google
Chrome with cache enabled.
In the latter scenario
we use the Android emulator to fetch the pages from
an emulated Nexus 5 using the built-in feature of the
emulator to simulate the conditions of a 3G network.
Figure 2 shows the accuracy of the classiﬁer for
the 9 websites in each one of the aforementioned sce-
narios. We found the lower accuracy for the PC with
cache scenario when predicting pages of aarp.com (0.79)
while we experienced the highest accuracy for ama-
zon.com (0.97). Caching inevitably hinders the ac-
curacy of the classiﬁer by 10.3% on average, but the
average accuracy never drops below 0.48. The ac-
curacy decreases because when parts of a page are
in the local cache, the traﬃc trace available to the
classiﬁer becomes shorter and, therefore, more likely
to be confused with that of another page.4 The mo-
bile phone scenario suﬀers from a similar issue, not
due to caching only, but also because mobile versions
of a website are typically simpler than their desktop
counterparts, and thus they end up producing more
similar traﬃc traces.

In order to conﬁrm the results of the previous ex-
periment on a larger number of websites, we selected
the 100 most visited websites from the traces collected
by the main network operator in Spain. After manu-
ally ﬁltering search engines (i.e., www.google.com/),
websites that require the user to login (i.e., www.
facebook.com/) and those ones dedicated to content
delivery or advertising (i.e., www.doubleclick.net/),
we were left with 33 websites including online news-

4In the extreme case of a page whose elements are
all in the cache, the resulting trace becomes totally
indistinguishable from that of any other fully cached
page.

antena3.comtripadvisor.esfnac.esebay.escarrefour.esidealista.comredcoon.eselpais.comforocoches.comeleconomista.estelecinco.eslavanguardia.commilanuncios.compccomponentes.commediamarkt.eselconﬁdencial.comabc.esbooking.comas.comapple.commsn.comes.aliexpress.comelmundo.eshuﬃngtonpost.esmarca.comes.ccm.netrtve.es20minutos.esamazon.comwordreference.comexpansion.comtiendas.mediamarkt.esrenfe.com0.00.20.40.60.81.0ClassiﬁerAccuracy376(a)

(b)

Figure 4: Confussion matrix (a) of the classiﬁer
and the Jaccard index between the categories as-
signed to two diﬀerent pages (b) for edition.cnn.com.
Box number 1 highlights URLs of the type edi-
tion.cnn.com/[region]. Box number 2 shows URLs
under the branch edition.cnn.com/style/

the “style” branch of the website have little overlap in
term of categories (see box 2 in Figure 4b).

Figure 5 depicts the performance of the basic and
the advanced proﬁling techniques when monitoring
the 9 websites of Figure 2. Dashed bars show the
precision and recall of the basic proﬁling technique
described in Section 3. Solid bars show the precision
and recall of the advanced proﬁling mechanism that
leverages the web ﬁngerprinting technique described
above. User proﬁling leveraging web ﬁngerprinting
clearly outperforms the basic proﬁling technique.

Classiﬁer Freshness.

The diﬀerence between the time when the classiﬁer
is trained and the time when pages are predicted may
aﬀect the prediction accuracy. This is especially true
for very dynamic websites (e.g., news or online com-
munity websites).
In this experiment we discretize

Figure 5: Precision and recall of the baseline eaves-
dropper and the eavesdropper leveraging website ﬁn-
gerprinting.

Figure 6: Accuracy of the classiﬁer days after training
for a dynamic website (nbcnews.com) and a static one
(bu.edu).

time in epochs and we assume that website content
only change from one epoch to the next one. If the
train and the test data are collected in the same
epoch, we say that the classiﬁer is fresh; otherwise
we say that the classiﬁer is stale. We deﬁne epochs
as days. We train the classiﬁer over a snapshot of the
website on a given day, and we try to predict pages
fetched throughout the following 6 days.

We expect a detectable diﬀerence in accuracy be-
tween a stale classiﬁer and a fresh one for dynamic
pages where content changes every day (e.g., news
websites). In the case of websites with static content,
the diﬀerence between a stale classiﬁer and a fresh
one are expected to be less pronounced. To verify
this, we add 4 websites with mostly static content (2
corporate and 2 academic ones) to the the 9 websites
of the previous experiments.

Figure 6 shows the eﬀect of staleness on the accu-
racy of the classiﬁer for both a static and a dynamic
website. The dashed lines represent the percentage of
1st-level pages that remain linked in the main page
across days, while the solid ones represent the accu-
racy of the classiﬁer. We observe the accuracy of the
classiﬁer for the dynamic website decreases rapidly

PredictedURLActualURL210102030405060708090100%ofpredictionsPredictedURLActualURL210.00.10.20.30.40.50.60.70.80.91.0JacardIndexamazon.comrakuten.comaarp.comwonderhowto.comabout.commashable.comslashdot.orgnbcnews.comreuters.com0.00.20.40.60.81.0BasicRecallBasicPrecisionAdvancedRecallAdvancedPrecision0123456Daysaftertraining0.00.20.40.60.81.0ClassiﬁerAccuracynbcnews.comClassiﬁerAccuracyStableURLsbu.eduClassiﬁerAccuracyStableURLs377while the accuracy for the static one decreases slowly
during the ﬁrst two days and then stabilizes around
80% accuracy. For both lines, the shadows denote the
minimum and the maximum of the statistics.

5. OPTIMIZING BANDWIDTH USE

In a real-world deployment, the eavesdropper may
not have the bandwidth required to refresh the classi-
ﬁer of each monitored website at every epoch. In the
following we formulate an optimization problem for
maximizing the proﬁling quality given a bandwidth
constraint.

1, . . . , pi

We consider an eavesdropper that monitors a cor-
pus of n websites w1, . . . , wn. Website wi has a main
0 and si 1-st level pages pi
page pi
si. We also
use c(pi
j) to denote the set of categories of page pi
j.
When browsing website wi, the user may visit any
page pi
j, with j = 0, . . . , si. Since the connection is
encrypted, we do not make any assumption on which
are the most popular pages within wi.
If the user
visits page pi
j, the correct categories that should be
assigned to that user when browsing wi are, therefore,
c(pi
j) that the pro-
ﬁler assigns to that user as a true positive. Similarly,
any category not in c(pi
j) that the proﬁler assigns to
that user is a false positive.

j). We consider any category in c(pi

The basic eavesdropper of Section 3 learns wi from
the client_hello message issued by the user browser
and assigns the categories of the main page c(pi
0) to
that user. However, because of HTTPS, the baseline
proﬁling system cannot tell which page pi
j was visited.
If we denoted with T i and F i the true positive and
the false positive, respectively, we have:
0) ∩ c(pi
j)|
|c(pj) \ c(p0)|

• T i = 1
• F i = 1

(cid:80)
(cid:80)

|c(pi

j=0..si

si+1

si

j=1..si

The advanced eavesdropper of Section 4 tries to
infer the page pi
j the user has fetched by looking at
the encrypted traﬃc trace. This is done by means
of a classiﬁer trained on a snapshot of wi. As shown
in the previous section, the freshness of the snapshot
used to train the classiﬁer impacts its accuracy.

We denote the expected number of true positives
and false positives with a classiﬁer that is ti epochs
stale by T i

ti, respectively. Thus we have:

ti and F i
(cid:80)

ti =(cid:80)
(cid:80)
ti =(cid:80)

j=0..si

j=0..si

j=0..si

• T i

• F i

(cid:80)

π(pi

j)|+
j)|c(pi
l)|(c(pi
l=0..si, l(cid:54)=j π(pi
j, pi

j, pi

l=0..si, l(cid:54)=j π(pi

j, pi

j) ∩ c(pi
l)|c(pi

l))|
l)\c(pi

j)|,

where π(pi

j, pi

l) denotes the probability of predict-
l (depends on the freshness of the

j as pi

ing page pi
classiﬁer).

Given the expected number of true and false posi-
tives, we set B as the bandwidth budget made avail-
able to eavesdropper at every epoch, and bi as the
bandwidth required to refresh the classiﬁer for we-
biste wi. We also denote by ui the popularity of web-
site wi (i.e., the number of users that visit wi in an
epoch).

Upon every epoch, the eavesdropper decides to spend

the budget B by training classiﬁers on a fresh snap-
shots of a subset X of the monitored websites.
If
website wi is included in X, the available budget is
reduces by bi and the expected number of correct cat-
egories assigned is ui ·T i
0 , while the expected number
of categories miss-assigned is ui · F i
0. If website wi
is not included in X, the budget remains untouched
and the expected number of correctly assigned and
mis-assigned categories is ui · T i
ti, respec-
tively, assuming the most recent classiﬁer for wi is ti
epochs stale.

ti and ui · F i

The selection of X, therefore, tries to maximize the
number of true positive and to minimize the number
of false negative, while respecting the available bud-
get.

Select X ⊆ {1, . . . , n}
s.t. Max

ui(T i

0 − F i

0) +

(cid:88)
(cid:88)

i∈X

i∈X
bi ≤ B

Where

(cid:88)

i /∈X

ui(T i

ti

− F i
ti)

If a classiﬁer were never trained on wi we fall-back
to the proﬁling technique of the na¨ıve eavesdropper
so that the number of true positives is ui · c(pi
0) and
the number of false positive is |c(pi
0)|.

j) \ c(pi

The above problem resembles the well-known 0/1
knapsack problem with the only diﬀerence that items
that are not selected add a non-zero value to the total
gain.

A toy example.

To illustrate the workings and the value of the
above optimization we have conducted a simulation
based on the 15 websites from previous sections. We
empirically assessed the training bandwidth require-
ment and probabilities of the confusion matrices, while
we used Alexa to obtain the popularity of each web-
site.

In Figure 7 we use a black box to mark a domain
that is being selected for re-training on a particular
day. We show which domains get to be classiﬁed every
day under two diﬀerent budgets – 500mb and 2Gb,
representing 10% and 40%, respectively of the budget
needed to re-classify all sites every day.

We observe that bandwidth availability can strongly
aﬀect the daily classiﬁcation pattern. In case of 500mb
budget, the same set of websites gets to be picked for

3787. REFERENCES
[1] J. M. Carrascosa, J. Mikians, R. Cuevas,

V. Erramilli, and N. Laoutaris, “I always feel
like somebody’s watching me measuring online
behavioural advertising,” in Proc. of ACM
CoNEXT’15.

[2] “Display Planner basics.” https://support.

google.com/adwords/answer/3056115?hl=en.
”[Online; accessed 12-May-2016]”.

[3] “SSL compliance.” https://support.google.com/

richmedia/answer/6015286?hl=en. ”[Online;
accessed 12-May-2016]”.

[4] M. Belshe, R. Peon, and M. Thomson,
“Hypertext transfer protocol version 2
(http/2),” RFC 7540, RFC Editor, May 2015.
http://www.rfc-editor.org/rfc/rfc7540.txt.

[5] “HTTPS Everywhere.” https://addons.mozilla.

org/en-US/ﬁrefox/addon/https-everywhere/.
”[Online; accessed 12-May-2016]”.

[6] R. Dingledine, N. Mathewson, and P. Syverson,

“Tor: The second-generation onion router,”
tech. rep., DTIC Document, 2004.

[7] T.-F. Yen, Y. Xie, F. Yu, R. P. Yu, and

M. Abadi, “Host ﬁngerprinting and tracking on
the web: Privacy and security implications.,” in
Proc. of NDSS’12.

[8] A. Hintz, “Fingerprinting websites using traﬃc

analysis,” in Proc. of PETS’02.

[9] D. Herrmann, R. Wendolsky, and H. Federrath,

“Website ﬁngerprinting: attacking popular
privacy enhancing technologies with the
multinomial na¨ıve-bayes classiﬁer,” in Proc. of
CCSW’09.

[10] A. Panchenko, L. Niessen, A. Zinnen, and
T. Engel, “Website ﬁngerprinting in onion
routing based anonymization networks,” in
Proc. of ACM WPES’11.

[11] A. Panchenko, F. Lanze, A. Zinnen, M. Henze,

J. Pennekamp, K. Wehrle, and T. Engel,
“Website ﬁngerprinting at internet scale,” in
Proc. of NDSS’16.
[12] “Alexa Top Sites.”

http://www.alexa.com/topsites. ”[Online;
accessed 12-May-2016]”.

Figure 7: Output of the optimization problem across
15 days for 2 diﬀerent bandwidth budgets (500mb
and 2Gb). A black box represents a website which
classiﬁer must be refreshed on that day.

classiﬁcation every day.
In the case of 2Gb, how-
ever, diﬀerent websites get to compete for the avail-
able budget and thus end up being picked or skipped
on diﬀerent days. The actual resulting pattern de-
pends on the interplay between website popularity,
size, and dynamicity of content.

The small number of websites in the above exam-
ple does not leave a lot of margin for proﬁling per-
formance diﬀerence between optimizing only once vs
optimizing every day. We have, however, simulated a
larger example that includes 200 pages with a mix of
popularities, content dynamicity, and size and have
observed that in more complex settings the diﬀerence
between optimizing only once vs. every day is sub-
stantial.

6. CONCLUSIONS

To the best of our knowledge our study is the ﬁrst
one to demonstrate that network eavesdroppers can
proﬁle user interests despite HTTPS. We have shown
that even oﬀ-the-shelf traﬃc classiﬁcation algorithms
can guess the page that a user is viewing. Caching
and dynamic content tailored to the device capabil-
ities make the whole eﬀort harder but the obtained
accuracy remains high. We believe that more spe-
cialized classiﬁcation algorithms, coupled with care-
ful optimisation of classiﬁcation bandwidth can yield
accurate and scalable user proﬁling even in more com-
plex settings than the ones we have considered. We
plan to prove our claim by developing a fully func-
tioning prototype.

Acknowledgments
We are grateful to Juan Miguel Carrascosa, Ruben
Cuevas and Costas Iordanou for helpful comments
and discussions. We also thank anonymous reviewers
and our shepherd for their valuable comments and
suggestions. This work has been partially supported
by the European Union through the FP7 METRICS
(607728), H2020 TYPES (653449) and ReCRED(653417)
Projects.

0.00.20.40.60.81.0Days0.00.20.40.60.81.0051015aarp.comabout.comamazon.comchron.comcnn.comnbcnews.comrakuten.comreuters.comwonderhowto.commashable.comslashdot.orgbu.edunec.comtelefonica.esuc3m.es500mb0510152Gb379