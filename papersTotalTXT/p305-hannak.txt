Measuring Price Discrimination and Steering

on E-commerce Web Sites

Aniko Hannak

Northeastern University

Boston, MA

ancsaaa@ccs.neu.edu

Gary Soeller

Northeastern University

Boston, MA

soelgary@ccs.neu.edu

David Lazer

Northeastern University
d.lazer@neu.edu

Boston, MA

Alan Mislove

Northeastern University

Boston, MA

amislove@ccs.neu.edu

Christo Wilson

Northeastern University
cbw@ccs.neu.edu

Boston, MA

ABSTRACT
Today, many e-commerce websites personalize their content,
including Netﬂix (movie recommendations), Amazon (prod-
uct suggestions), and Yelp (business reviews).
In many
cases, personalization provides advantages for users: for ex-
ample, when a user searches for an ambiguous query such as
“router,” Amazon may be able to suggest the woodworking
tool instead of the networking device. However, personaliza-
tion on e-commerce sites may also be used to the user’s dis-
advantage by manipulating the products shown (price steer-
ing) or by customizing the prices of products (price discrim-
ination). Unfortunately, today, we lack the tools and tech-
niques necessary to be able to detect such behavior.

In this paper, we make three contributions towards ad-
dressing this problem. First, we develop a methodology for
accurately measuring when price steering and discrimina-
tion occur and implement it for a variety of e-commerce web
sites. While it may seem conceptually simple to detect dif-
ferences between users’ results, accurately attributing these
diﬀerences to price discrimination and steering requires cor-
rectly addressing a number of sources of noise. Second, we
use the accounts and cookies of over 300 real-world users
to detect price steering and discrimination on 16 popular
e-commerce sites. We ﬁnd evidence for some form of per-
sonalization on nine of these e-commerce sites. Third, we
investigate the eﬀect of user behaviors on personalization.
We create fake accounts to simulate diﬀerent user features
including web browser/OS choice, owning an account, and
history of purchased or viewed products. Overall, we ﬁnd
numerous instances of price steering and discrimination on
a variety of top e-commerce sites.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
IMC’14, November 5–7, 2014, Vancouver, BC, Canada.
Copyright 2014 ACM 978-1-4503-3213-2/14/11 ...$15.00.
http://dx.doi.org/10.1145/2663716.2663744 .

Categories and Subject Descriptors
H.3.5 [Information Systems]: Online Services—commer-
cial services, web-based services; H.5.2 [Information in-
terfaces and presentation]: User Interfaces—evalua-
tion/methodology

Keywords
Search; Personalization; E-commerce; Price discrimination

1.

INTRODUCTION

Personalization is a ubiquitous feature on today’s top web
destinations. Search engines such as Google, streaming-
media services such as Netﬂix, and recommendation sites
such as Yelp all use sophisticated algorithms to tailor con-
tent to each individual user. In many cases, personalization
provides advantages for users:
for example, when a user
searches on Google with an ambiguous query such as “apple,”
there are multiple potential interpretations. By personaliz-
ing the search results (e.g., by taking the user’s history of
prior searches into account), Google is able to return results
that are potentially more relevant (e.g., computer products,
rather than orchards).

Recently, researchers and Internet users have uncovered
evidence of personalization on e-commerce sites [1, 29, 30].
On such sites, the beneﬁts of personalization for users are
less clear; e-commerce sites have an economic incentive
to use personalization to induce users into spending more
money. For example, the travel website Orbitz was found
to be personalizing the results of hotel searches [28]. Unbe-
knownst to users, Orbitz “steered” Mac OS X users towards
more expensive hotels in select locations by placing them at
higher ranks in search results. Orbitz discontinued the use
of this personalization algorithm after one month [7].

At ﬁrst blush, detecting personalization on e-commerce
sites seems conceptually simple: have two users run the same
search, and any diﬀerences in the results indicate personal-
ization. Unfortunately, this approach is likely to have many
false positives, as diﬀerences between users’ results may exist
for a number of reasons not related to personalization. For
example, results may diﬀer due to changes in product inven-
tory, regional tax diﬀerences, or inconsistencies across data
centers. As a result, accurately detecting personalization on
e-commerce sites remains an open challenge.

305In this paper, we ﬁrst develop a methodology that can ac-
curately measure personalization on e-commerce sites. We
then use this methodology to address two questions: ﬁrst,
how widespread is personalization on today’s e-commerce
web sites? This includes price discrimination (customizing
prices for some users) as well as price steering (changing the
order of search results to highlight speciﬁc products). Sec-
ond, how are e-commerce retailers choosing to implement
personalization? Although there is anecdotal evidence of
these eﬀects [46, 48] and speciﬁc instances where retailers
have been exposed doing so [1, 28], the frequency and mech-
anisms of e-commerce personalization remain poorly under-
stood.

Our paper represents the ﬁrst comprehensive study of e-
commerce personalization that examines price discrimina-
tion and price steering for 300 real-world users1, as well as
synthetically generated fake accounts. We develop a mea-
surement infrastructure that is able to distinguish genuine
personalization of e-commerce sites from other sources of
noise; this methodology is based on previous work on mea-
suring personalization of web search services [17]. Using
this methodology, we examine 16 top e-commerce sites cov-
ering general retailers as well as hotel and rental car booking
sites. Our real-world data indicates that eight of these sites
implement personalization, while our controlled tests based
on fake accounts allow us to identify speciﬁc user features
that trigger personalization on seven sites. Speciﬁcally, we
observe the following personalization strategies:

• Cheaptickets and Orbitz implement price discrimina-
tion by oﬀering reduced prices on hotels to “members”.
• Expedia and Hotels.com engage in A/B testing that
steers a subset of users towards more expensive hotels.
• Home Depot and Travelocity personalize search results
• Priceline personalizes search results based on a user’s

for users on mobile devices.

history of clicks and purchases.

In addition to positively identifying price discrimination
and steering on several well-known e-commerce sites, we also
make the following four speciﬁc contributions. First, we in-
troduce control accounts into all of our experiments, which
allows us to diﬀerentiate between inherent noise and actual
personalization. Second, we develop a novel methodology us-
ing information retrieval metrics to identify price steering.
Third, we examine the impact of purchase history on per-
sonalization by reserving hotel rooms and rental cars, then
comparing the search results received by these users to users
with no history. Fourth, we identify a never-before-seen form
of e-commerce personalization based on A/B testing, and
show that it leads to price steering. Finally, we make all of
our crawling scripts, parsers, and raw data available to the
research community.

2. BACKGROUND AND MOTIVATION

We now provide background on e-commerce personaliza-

tion and overview the terminology used in this paper.
2.1 A Brief History of Personalization

Online businesses have long used personalized recommen-
dations as a way to boost sales. Retailers like Amazon and
1Our study is conducted under Northeastern Institutional
Review Board protocol #13-04-12.

Target leverage the search and purchase histories of users
to identify products that users may be interested in.
In
some cases, companies go to great lengths to obfuscate the
fact that recommendations are personalized, because users
sometimes ﬁnd these practices to be creepy [10].

In several cases, e-commerce sites have been observed per-
forming price discrimination: the practice of showing diﬀer-
ent prices to diﬀerent people for the same item. Several years
ago, Amazon brieﬂy tested an algorithm that personalized
prices for frequent shoppers [1]. Although many consumers
erroneously believe that price discrimination on the Inter-
net is illegal and are against the practice [8, 36], consumers
routinely accept real-world price discrimination in the form
of coupons, student discounts, or members-only prices [5].

Similarly, e-commerce sites have been observed perform-
ing price steering: the practice of re-ordering search results
to place expensive items towards the top of the page. For ex-
ample, the travel web site Orbitz was found to be promoting
high-value hotels speciﬁcally to Apple users [28]. Although
the prices of individual items do not change in this case, cus-
tomers are likely to purchase items that are placed towards
the top of search results [25], and thus users can be nudged
towards more expensive items.

2.2 Scope of This Study

Throughout the paper, we survey a wide variety of e-
commerce web sites, ranging from large-scale retailers like
Walmart to travel sites like Expedia. To make the re-
sults comparable, we only consider products returned via
searches—as opposed to “departments”, home page oﬀers,
and other mechanisms by which e-commerce sites oﬀer prod-
ucts to users—as searching is a functionality supported by
most large retailers. We leave detecting price discrimination
and steering via other mechanisms to future work. Addi-
tionally, we use products and their advertised price on the
search result page (e.g., a speciﬁc item on Walmart or hotel
on Expedia) as the basic unit of measurement; we leave the
investigation of eﬀects such as bundle discounts, coupons,
sponsored listings, or hidden prices to future work as well.

2.3 Deﬁnitions

Personalization on web services comes in many forms (e.g.,
“localization”, per-account customization, etc.), and it is not
entirely straightforward to declare that an inconsistency be-
tween the product search results observed by two users is
due to personalization. For example, the two users’ search
queries may have been directed to diﬀerent data centers,
and the diﬀerences are a result of data center inconsistency
rather than intentional personalization.

For the purposes of this paper, we deﬁne personalization
to be taking place when an inconsistency in product search
results is due to client-side state associated with the request.
For example, a client’s request often includes tracking cook-
ies, a User-Agent identifying the browser and Operating Sys-
tem (OS), and the client’s source IP address. If any of these
lead to an inconsistency in the results, we declare the incon-
sistency to be personalization.
In the diﬀerent-datacenter
example from above, the inconsistency between the two re-
sults is not due to any client-side state, and we therefore
declare it not to be personalization.

More so than other web services [17], e-commerce retailers
have a number of diﬀerent dimensions available to personal-

306ize on. In this paper, we focus on two of the primary vectors
for e-commerce personalization:

Price steering occurs when two users receive diﬀerent
product results (or the same products in a diﬀerent order)
for the same query (e.g., Best Buy showing more-expensive
products to user A than user B when they both query for
“laptops”). Price steering can be similar to personalization
in web search [17], i.e., the e-commerce provider may be try-
ing to give the user more relevant products (or, they may
be trying to extract more money from the user). Steering is
possible because e-commerce sites often do not sort search
results by an objective metric like price or user reviews by
default; instead, results can be sorted using an ambiguous
metric like “Best Match” or “Most Relevant”.

Price discrimination occurs when two users are shown
inconsistent prices for the same product (e.g., Travelocity
showing a select user a higher price for a particular hotel).
Contrary to popular belief, price discrimination in general
is not illegal in the United States [13], as the Robinson–
Patman Act of 1936 (a.k.a. the Anti-Price Discrimination
Act) is written to control the behavior of product manu-
facturers and distributors, not consumer-facing enterprises.
It is unclear whether price discrimination targeted against
protected classes (e.g., race, religion, gender) is legal.

Although the term “price discrimination” evokes nega-
tive connotations, it is actually a fundamental concept in
economic theory, and it is widely practiced (and accepted
by consumers) in everyday life.
In economic theory, per-
fect price discrimination refers to a pricing strategy where
each consumer is charged the maximum amount that they
are willing to pay for each item [37]. Elastic consumers
can aﬀord to pay higher prices, whereas inelastic (price-
constrained) consumers are charged less. In practice, strate-
gies like direct and indirect segmentation are employed by
companies to charge diﬀerent prices to diﬀerent segments of
consumers [32]. One classic (and extremely popular [5]) ex-
ample of indirect segmentation is coupons: consumers that
take the time to seek out and use coupons explicitly reveal
to the retailer that they are price-inelastic. Another ac-
cepted form of indirect segmentation is Saturday-night stay
discount for airfares. Accepted forms of direct segmentation
include discounts for students and seniors.

3. METHODOLOGY

Both price steering and discrimination are diﬃcult to re-
liably detect in practice, as diﬀerences in e-commerce search
results across users may be due to factors other than person-
alization. In this section, we describe the methodology we
develop to address these measurement challenges. First, we
give the high-level intuition that guides the design of our ex-
periments, and describe how we accurately distinguish per-
sonalization of product search results from other causes of
inconsistency. Second, we describe the implementation of
our experiments. Third, we detail the sites that we choose
to study and the queries we select to test for personalization.
3.1 Measuring Personalization

The ﬁrst challenge we face when trying to measure per-
sonalization on e-commerce sites is dealing with the variety
of sites that exist. Few of these sites oﬀer programmatic
APIs, and each uses substantially diﬀerent HTML markup

to implement their site. As a result, we collect data by visit-
ing the various sites’ web pages, and we write custom HTML
parsers to extract the products and prices from the search re-
sult page for each site that we study. We make these scripts
and parsers available to the research community.

The second challenge we face is noise, or inconsistencies
in search results that are not due to personalization. Noise
can be caused by a variety of factors:

• Distributed

Large-scale

• Updates to the e-commerce site: E-commerce
services are known to update their inventory often,
as products sell out, become available, or prices are
changed. This means that the results for a query may
change even over short timescales.
infrastructure:

e-
commerce sites are often spread across geographically
diverse datacenters. Our tests show that diﬀerent
datacenters may return diﬀerent results for the same
search, due to inconsistencies across data centers.
• Unknown: As an e-commerce site is eﬀectively a
black-box (i.e., we do not know their internal archi-
tecture or algorithms), there may be other, unknown
sources of noise that we are unaware of.

To control for all sources of noise, we include a control in
each experiment that is conﬁgured identically to one other
treatment (i.e., we run one of the experimental treatments
twice). Doing so allows us to measure the noise as the level of
inconsistency between the control account and its twin; since
these two treatments are conﬁgured identically, any incon-
sistencies between them must be due to noise, not personal-
ization. Then, we can measure the level of inconsistency be-
tween the diﬀerent experimental treatments; if this is higher
than the baseline noise, the increased inconsistencies are due
to personalization. As a result, we cannot declare any par-
ticular inconsistency to be due to personalization (or noise),
but we can report the overall rate.

To see why this works, suppose we want to determine if
Firefox users receive diﬀerent prices than Safari users on a
given site. The na¨ıve experiment would be to send a pair of
identical, simultaneous searches—one with a Firefox User-
Agent and one with a Safari User-Agent—and then look for
inconsistencies. However, the site may be performing A/B
testing, and the diﬀerences may be due to requests given
diﬀerent A/B treatments.
Instead, we run an additional
control (e.g., a third request with a Firefox User-Agent),
and can measure the frequency of diﬀerences due to noise.
Of course, running a single query is insuﬃcient to accu-
rately measure noise and personalization. Instead, we run a
large set of searches on each site over multiple days and re-
port the aggregate level of noise and personalization across
all results.
3.2 Implementation
Except for when we use real users’ accounts (our Ama-
zon Mechanical Turk users in § 4), we collect data from
the various e-commerce sites using custom scripts for Phan-
tomJS [35]. We chose PhantomJS because it is a full imple-
mentation of the WebKit browser, meaning that it executes
JavaScript, manages cookies, etc. Thus, using PhantomJS
is signiﬁcantly more realistic than using custom code that
does not execute JavaScript, and it is more scalable than
automating a full web browser (e.g., Selenium [41]). Phan-

307Retailer
Best Buy
CDW
HomeDepot
JCPenney
Macy’s
Newegg
Oﬃce Depot
Sears
Staples
Walmart

Category
Electronics
Computers
Home-improvement
Clothes, housewares
Clothes, housewares
Computers

Site
http://bestbuy.com
http://cdw.com
http://homedepot.com
http://jcp.com
http://macys.com
http://newegg.com
http://officedepot.com Oﬃce supplies
http://sears.com
http://sears.com
http://walmart.com

Clothes, housewares
Oﬃce supplies
General retailer

Retailer
Cheaptickets
Expedia
Hotels.com
Orbitz
Priceline
Travelocity

Site
http://cheaptickets.com
http://expedia.com
http://hotels.com
http://orbitz.com
http://priceline.com
http://travelocity.com

Hotels Cars

(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)

(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)
(cid:50)(cid:8)
(cid:50)(cid:8)
(cid:50)(cid:8)

Table 2: The travel retailers we measured in this study.

Table 1: The general retailers we measured in this study.

tomJS has been proven to allow large-scale measurements
in previous work [17].

IP Address Geolocation.
In our experiments, all
PhantomJS instances issued queries from a /24 block of IP
addresses located in Boston. Controlling the source IP ad-
dress is critical since e-commerce sites may personalize re-
sults based on a user’s geolocation. We do not examine
geolocation-based personalization in this paper; instead, we
refer interested readers to work by Mikians et al. that thor-
oughly examines this topic [29, 30].

Browser Fingerprinting.
All of our PhantomJS
instances are conﬁgured identically, and present identi-
cal HTTP request headers and DOM properties (e.g.,
screen.width) to web servers. The only exceptions to this
rule are in cases where we explicitly test for the eﬀect of dif-
ferent User-Agent strings, or when an e-commerce site stores
state on the client (e.g., with a cookie). Although we do not
test for e-commerce personalization based on browser ﬁnger-
prints [34], any website attempting to do so would observe
identical ﬁngerprints from all of our PhantomJS instances
(barring the two previous exceptions). Thus, personalization
based on currently known browser ﬁngerprinting techniques
is unlikely to impact our results.
3.3 E-commerce Sites

We focus on two classes of e-commerce web sites: general
e-commerce retailers (e.g., Best Buy) and travel retailers
(e.g., Expedia). We choose to include travel retailers be-
cause there is anecdotal evidence of price steering among
such sites [28]. Of course, our methodology can be applied
to other categories of e-commerce sites as well.

General Retailers. We select 10 of the largest e-commerce
retailers, according to the Top500 e-commerce database [43],
for our study, shown in Table 1. We exclude Amazon, as
Amazon hosts a large number of diﬀerent merchants, making
it diﬃcult to measure Amazon itself. We also exclude sites
like apple.com that only sell their own brand.

Travel Retailers. We select six of the most popular web-
based travel retailers [44] to study, shown in Table 2. For
these retailers, we focus on searches for hotels and rental
cars. We do not include airline tickets, as airline ticket pric-
ing is done transparently through a set of Global Distribu-
tion Systems (GDSes) [6]. Furthermore, a recent study by
Vissers et al. looked for, but was unable to ﬁnd, evidence of
price discrimination on the websites of 25 major airlines [45].

applied helps to avoid diﬀerences in pricing that are due to
the location of the business and/or the customer.
3.4 Searches

We select 20 searches to send to each target e-commerce
site; it is the results of these searches that we use to look for
personalization. We select the searches to cover a variety of
product types, and tailor the searches to the type of products
each retailer sells. For example, for JCPenney, our searches
include “pillows”, “sunglasses”, and “chairs”; for Newegg, our
searches include “ﬂash drives”, “LCD TVs”, and “phones”.

For travel web sites, we select 20 searches (location and
date range) that we send to each site when searching for
hotels or rental cars. We select 10 diﬀerent cities across
the globe (Miami, Honolulu, Las Vegas, London, Paris, Flo-
rence, Bangkok, Cairo, Cancun, and Montreal), and choose
date ranges that are both short (4-day stays/rentals) and
long (11-day stays/rentals).

4. REAL-WORLD PERSONALIZATION

We begin by addressing our ﬁrst question: how widespread
are price discrimination and steering on today’s e-commerce
web sites? To do so, we have a large set of real-world users
run our experimental searches and examine the results that
they receive.
4.1 Data Collection

To obtain a diverse set of users, we recruited from Ama-
zon’s Mechanical Turk (AMT) [3]. We posted three Human
Intelligence Tasks (HITs) to AMT, with each HIT focus-
ing on e-commerce, hotels, or rental cars. In the HIT, we
explained our study and oﬀered each user $1.00 to partici-
pate.2 Users were required to live in the United States, and
could only complete the HIT once.

Users who accepted the HIT were instructed to conﬁgure
their web browser to use a Proxy Auto-Conﬁg (PAC) ﬁle pro-
vided by us. The PAC ﬁle routes all traﬃc to the sites under
study to an HTTP proxy controlled by us. Then, users were
directed to visit a web page containing JavaScript that per-
formed our set of searches in an iframe. After each search,
the Javascript grabs the HTML in the iframe and uploads it
to our server, allowing us to view the results of the search.
By having the user run the searches within their browser,
any cookies that the user’s browser had previously been as-
signed would automatically be forwarded in our searches.
This allows us to examine the results that the user would
have received. We waited 15 seconds between each search,
and the overall experiment took ≈45 minutes to complete
(between ﬁve and 10 sites, each with 20 searches).

In all cases, the prices of products returned in search re-
sults are in US dollars, are pre-tax, and do not include ship-
ping fees. Examining prices before taxes and shipping are

2This study was conducted under Northeastern University
IRB protocol #13-04-12; all personally identiﬁable informa-
tion was removed from our collected data.

308Figure 1: Previous usage (i.e., having an account and making a purchase) of diﬀerent e-commerce sites by our AMT users.

The HTTP proxy serves two important functions. First,
it allows us to quantify the baseline amount of noise in
search results. Whenever the proxy observes a search re-
quest, it ﬁres oﬀ two identical searches using PhantomJS
(with no cookies) and saves the resulting pages. The results
from PhantomJS serve as a comparison and a control result.
As outlined in § 3.1, we compare the results served to the
comparison and control to determine the underlying level
of noise in the search results. We also compare the results
served to the comparison and the real user; any diﬀerences
between the real user and the comparison above the level
observed between the comparison and the control can be
attributed to personalization.

Second, the proxy reduces the amount of noise by sending
the experimental, comparison, and control searches to the
web site at the same time and from the same IP address. As
stated in § 3.2, sending all queries from the same IP address
controls for personalization due to geolocation, which we
are speciﬁcally not studying in this paper. Furthermore,
we hard-coded a DNS mapping for each of the sites on the
proxy to avoid discrepancies that might come from round-
robin DNS sending requests to diﬀerent data centers.

In total, we recruited 100 AMT users in each of our retail,
hotel, and car experiments. In each of the experiments, the
participants ﬁrst answered a brief survey about whether they
had an account and/or had purchased something from each
site. We present the results of this survey in Figure 1. We
observe that many of our users have accounts and a purchase
history on a large number of the sites we study.3
4.2 Price Steering

We begin by looking for price steering, or personalizing
search results to place more- or less-expensive products at
the top of the list. We do not examine rental car results
for price steering because travel sites tend to present these
results in a deterministically ordered grid of car types (e.g.,
economy, SUV) and car companies (with the least expensive
car in the upper left). This arrangement prevents travel sites
from personalizing the order of rental cars.

To measure price steering, we use three metrics:

Jaccard Index. To measure the overlap between two dif-
ferent sets of results, we use Jaccard index, which is the size
of the intersection over the size of the union. A Jaccard In-
dex of 0 represents no overlap between the results, while 1
indicates they contain the same results (although not neces-
sarily in the same order).

3Note that the fraction of users having made purchases can
be higher than the fraction with an account, as many sites
allow purchases as a “guest”.

Kendall’s τ . To measure reordering between two lists of
result, we use Kendall’s τ rank correlation coeﬃcient. This
metric is commonly used in the Information Retrieval (IR)
literature to compare ranked lists [23]. The metric ranges
between -1 and 1, with 1 representing the same order, 0
signifying no correlation, and -1 being inverse ordering.

then calculated as DCG(R) = g(r1) +(cid:80)k

nDGG. To measure how strongly the ordering of search
results is correlated with product prices, we use Normal-
ized Discounted Cumulative Gain (nDCG). nDCG is a met-
ric from the IR literature [20] for calculating how “close”
a given list of search results is to an ideal ordering of re-
sults. Each possible search result r is assigned a “gain” score
g(r). The DCG of a page of results R = [r1, r2, . . . , rk] is
i=2(g(ri)/ log2(i)).
Normalized DCG is simply DCG(R)/DCG(R(cid:48)), where R(cid:48) is
the list of search results with the highest gain scores, sorted
from greatest to least. Thus, nDCG of 1 means the ob-
served search results are the same as the ideal results, while
0 means no useful results were returned.
In our context, we use product prices as gain scores, and
construct R(cid:48) by aggregating all of the observed products
for a given query. For example, to create R(cid:48) for the query
“ladders” on Home Depot, we construct the union of the re-
sults for the AMT user, the comparison, and the control,
then sort the union from most- to least-expensive.
Intu-
itively, R(cid:48) is the most expensive possible ordering of prod-
ucts for the given query. We can then calculate the DCG
for the AMT user’s results and normalize it using R(cid:48). Ef-
fectively, if nDCG(AM T user) > nDCG(control), then the
AMT user’s search results include more-expensive products
towards the top of the page than the control.

For each site, Figure 2 presents the average Jaccard index,
Kendall’s τ , and nDCG across all queries. The results are
presented comparing the comparison to the control searches
(Control), and the comparison to the AMT user searches
(User). We observe several interesting trends. First, Sears,
Walmart, and Priceline all have a lower Jaccard index for
AMT users relative to the control. This indicates that the
AMT users are receiving diﬀerent products at a higher rate
than the control searches (again, note that we are not com-
paring AMT users’ results to each other; we only compare
each user’s result to the corresponding comparison result).
Other sites like Orbitz show a Jaccard of 0.85 for Control
and User, meaning that the set of results shows inconsisten-
cies, but that AMT users are not seeing a higher level of
inconsistency than the control and comparison searches.

Second, we observe that on Newegg, Sears, Walmart, and
Priceline, Kendall’s τ is at least 0.1 lower for AMT users,
i.e., AMT users are consistently receiving results in a diﬀer-

 0 10 20 30 40 50BestbuyCDWHomeDepotJCPMacysNeweggOfficeDepotSearsStaplesWalmartCheapticketsExpediaHotelsOrbitzPricelineCheapticketsExpediaOrbitzPricelineTravelocity% of Users72%E-commerceHotelsRental CarsAccountPurchase309Figure 2: Average Jaccard index (top), Kendall’s τ (middle), and nDCG (bottom) across all users and searches for each web site.

Figure 3: Percent of products with inconsistent prices (bottom), and the distribution of price diﬀerences for sites with ≥0.5% of
products showing diﬀerences (top), across all users and searches for each web site. The top plot shows the mean (thick line), 25th and
75th percentile (box), and 5th and 95th percentile (whisker).

ent order than the controls. This observation is especially
true for Sears, where the ordering of results for AMT users
is markedly diﬀerent. Third, we observe that Sears alone
appears to be ordering products for AMT users in a price-
biased manner. The nDCG results show that AMT users
tend to have cheaper products near the top of their search
results relative to the controls. Note that the results in
Figure 2 are only useful for uncovering price steering; we
examine whether the target sites are performing price dis-
crimination in § 4.3.

Besides Priceline, the other four travel sites do not show
signiﬁcant diﬀerences between the AMT users and the con-
trols. However, these four sites do exhibit signiﬁcant noise:
Kendall’s τ is ≤0.83 in all four cases. On Cheaptickets and
Orbitz, we manually conﬁrm that this noise is due to ran-
domness in the order of search results. In contrast, on Ex-
pedia and Hotels.com this noise is due to systematic A/B
testing on users (see § 5.2 for details), which explains why
we see equally low Kendall’s τ values on both sites for all
users. Unfortunately, it also means that we cannot draw

any conclusions about personalization on Expedia and Ho-
tels.com from the AMT experiment, since the search results
for the comparison and the control rarely match.
4.3 Price Discrimination

So far, we have only looked at the set of products returned.
We now turn to investigate whether sites are altering the
prices of products for diﬀerent users, i.e., price discrimina-
tion. In the bottom plot of Figure 3, we present the frac-
tion of products that show price inconsistencies between the
user’s and comparison searches (User) and between the com-
parison and control searches (Control). Overall, we observe
that most sites show few inconsistencies (typically <0.5% of
products), but a small set of sites (Home Depot, Sears, and
many of the travel sites) show both a signiﬁcant fraction
of price inconsistencies and a signiﬁcantly higher fraction of
inconsistencies for the AMT users.

To investigate this phenomenon further,

in the top of
Figure 3, we plot the distribution of price diﬀerentials for
all sites where >0.5% of the products show inconsistency.
We plot the mean price diﬀerential (thick line), 25th and

 0 0.2 0.4 0.6 0.8BestbuyCDWHomeDepotJCPMacysNeweggOfficeDepotSearsStaplesWalmartCheapticketsExpediaHotelsOrbitzPricelineAvg. nDCGControlUser 0 0.2 0.4 0.6 0.8 1Avg. Kendall’s tau 0 0.2 0.4 0.6 0.8 1Avg. JaccardE-commerceHotels 0 1 2BestbuyCDWHomeDepotJCPMacysNeweggOfficeDepotSearsStaplesWalmartCheapticketsExpediaHotelsOrbitzPricelineCheapticketsExpediaOrbitzPricelineTravelocity% Products w/different prices3.6%Threshold 0.5%ControlUser$1$10$100$1000Distribution ofinconsistenciesE-commerceHotelsRental Cars310Figure 5: AMT users that receive highly personalized search results on general retail, hotels, and car rental sites.

75th percentile (box), and 5th and 95th percentile (whisker).
Note that in our data, AMT users always receive higher
prices than the controls (on average), thus all diﬀerentials
are positive. We observe that the price diﬀerentials on many
sites are quite large (up to hundreds of dollars). As an ex-
ample, in Figure 4, we show a screenshot of a price inconsis-
tency that we observed. Both the control and comparison
searches returned a price of $565 for a hotel, while our AMT
user was returned a price of $633.
4.4 Per-User Personalization

Next, we take a closer look at the subset of AMT users
who experience high levels of personalization on one or more
of the e-commerce sites. Our goal is to investigate whether
these AMT users share any observable features that may il-
luminate why they are receiving personalized search results.
We deﬁne highly personalized users as the set of users who
see products with inconsistent pricing >0.5% of the time.
After ﬁltering we are left with between 2-12% of our AMT
users depending on the site.

First, we map the AMT users’ IP addresses to their ge-
olocations and compare the locations of personalized and
non-personalized users. We ﬁnd no discernible correlation
between location and personalization. However, as men-
tioned above, in this experiment all searches originate from
a proxy in Boston. Thus, it is not surprising that we do not
observe any eﬀects due to location, since the sites did not
observe users’ true IP addresses.

Next, we examine the AMT users’ browser and OS choices.
We are able to infer their platform based on the HTTP head-

Figure 4: Example of price discrimination. The top result was
served to the AMT user, while the bottom result was served to
the comparison and control.

ers sent by their browser through our proxy. Again, we ﬁnd
no correlation between browser/OS choice and high person-
In § 5, we do uncover personalization linked to
alization.
the use of mobile browsers, however none of the AMT users
in our study did the HIT from a mobile device.

Finally, we ask the question: are there AMT users who
receive personalized results on multiple e-commerce sites?
Figure 5 lists the 100 users in our experiments along the
x-axis of each plot; a dot highlights cases were a site person-
alized search results for a particular user. Although some
dots are randomly dispersed, there are many AMT users
that receive personalized results from several e-commerce
sites. We highlight users who see personalized results on
more than one site with vertical bars. More users fall into
this category on travel sites than on general retailers.

The takeaway from Figure 5 is that we observe many AMT
users who receive personalized results across multiple sites.
This suggests that these users share feature(s) that all of
these sites use for personalization. Unfortunately, we are
unable infer the speciﬁc characteristics of these users that
are triggering personalization.

Cookies.
Although we logged the cookies sent by AMT
users to the target e-commerce sites, it is not possible to
use them to determine why some users receive personalized
search results. First, cookies are typically random alphanu-
meric strings; they do not encode useful information about
a user’s history of interactions with a website (e.g., items
clicked on, purchases, etc.). Second, cookies can be set by
content embedded in third-party websites. This means that
a user with a cookie from e-commerce site S may never
have consciously visited S, let alone made purchases from
S. These reasons motivate why we rely on survey results
(see Figure 1) to determine AMT users’ history of interac-
tions with the target e-commerce sites.
4.5 Summary

To summarize our ﬁndings in this section: we ﬁnd ev-
idence for price steering and price discrimination on four
general retailers and ﬁve travel sites. Overall, travel sites
show price inconsistencies in a higher percentage of cases,
relative to the controls, with prices increasing for AMT users
by hundreds of dollars. Finally, we observe that many AMT
users experience personalization across multiple sites.

5. PERSONALIZATION FEATURES

In § 4, we demonstrated that e-commerce sites personalize
results for real users. However, we cannot determine why
results are being personalized based on the data from real-
world users, since there are too many confounding variables
attached to each AMT user (e.g., their location, choice of
browser, purchase history, etc.).

BestbuyCDWHomeDepotJCPMacysNeweggOfficeDepotSearsStaplesWalmartAMT UsersCheapticketsExpediaHotelsOrbitzPricelineAMT UsersCheapticketsExpediaOrbitzPricelineTravelocityAMT Users311Category Feature Tested Values

Account

Cookies No Account, Logged In, No Cookies

User-
Agent

Account
History

OS Win. XP, Win. 7, OS X, Linux

Browser

Click
Purchase

Chrome 33, Android Chrome 34, IE 8,
Firefox 25, Safari 7, iOS Safari 6
Low Prices, High Prices
Low Prices, High Prices

Table 3: User features evaluated for eﬀects on personalization.

In this section, we conduct controlled experiments with
fake accounts created by us to examine the impact of spe-
ciﬁc features on e-commerce personalization. Although we
cannot test all possible features, we examine ﬁve likely can-
didates: browser, OS, account log-in, click history, and pur-
chase history. We chose these features because e-commerce
sites have been observed personalizing results based on these
features in the past [1, 28].

We begin with an overview of the design of our synthetic
user experiments. Next, we highlight examples of person-
alization on hotel sites and general retailers. None of our
experiments triggered personalization on rental car sites, so
we omit these results.
5.1 Experimental Overview

The goal of our synthetic experiments is to determine
whether speciﬁc user features trigger personalization on e-
commerce sites. To assess the impact of feature X that can
take on values x1, x2, . . . , xn, we execute n + 1 PhantomJS
instances, with each value of X assigned to one instance.
The n + 1th instance serves as the control by duplicating
the value of another instance. All PhantomJS instances ex-
ecute 20 queries (see § 3.4) on each e-commerce site per day,
with queries spaced one minute apart to avoid tripping secu-
rity countermeasures. PhantomJS downloads the ﬁrst page
of results for each query. Unless otherwise speciﬁed, Phan-
tomJS persists all cookies between experiments. All of our
experiments are designed to complete in <24 hours.
To mitigate measurements errors due to noise (see § 3.1),
we perform three steps (some borrowed from previous
work [16, 17]): ﬁrst, all searches for a given query are ex-
ecuted at the same time. This eliminates diﬀerences in re-
sults due to temporal eﬀects. This also means that each of
our treatments has exactly the same search history at the
same time. Second, we use static DNS entries to direct all
of our query traﬃc to speciﬁc IP addresses of the retailers.
This eliminates errors arising from diﬀerences between dat-
acenters. Third, although all PhantomJS instances execute
on one machine, we use SSH tunnels to forward the traﬃc
of each treatment to a unique IP address in a /24 subnet.
This process ensures that any eﬀects due to IP geolocation
will aﬀect all results equally.

Static Features.
Table 3 lists the ﬁve features that
we evaluate in our experiments. In the cookie experiment,
the goal is to determine whether e-commerce sites person-
alize results for users who are logged-in to the site. Thus,
two PhantomJS instances query the given e-commerce site
without logging-in, one logs-in before querying, and the ﬁnal
account clears its cookies after every HTTP request.

In two sets of experiments, we vary the User-Agent sent
by PhantomJS to simulate diﬀerent OSes and browsers. The
goal of these tests is to see if e-commerce sites personalize
based on the user’s choice of OS and browser. In the OS

experiment, all instances report using Chrome 33, and Win-
dows 7 serves as the control.
In the browser experiment,
Chrome 33 serves as the control, and all instances report
using Windows 7, except for Safari 7 (which reports OS X
Mavericks), Safari on iOS 6, and Chrome on Android 4.4.2.

Historical Features.
In our historical experiments,
the goal is to examine whether e-commerce sites personal-
ize results based on users’ history of viewed and purchased
items. Unfortunately, we are unable to create purchase his-
tory on general retail sites because this would entail buying
and then returning physical goods. However, it is possi-
ble for us to create purchase history on travel sites. On
Expedia, Hotels.com, Priceline, and Travelocity, some hotel
rooms feature “pay at the hotel” reservations where you pay
at check-in. A valid credit card must still be associated with
“pay at the hotel” reservations. Similarly, all ﬁve travel sites
allow rental cars to be reserved without up-front payment.
These no-payment reservations allow us to book reservations
on travel sites and build up purchase history.

To conduct our historical experiments, we created six ac-
counts on the four hotel sites and all ﬁve rental car sites.
Two accounts on each site serve as controls: they do not
click on search results or make reservations. Every night
for one week, we manually logged-in to the remaining four
accounts on each site and performed speciﬁc actions. Two
accounts searched for a hotel/car and clicked on the high-
est and lowest priced results, respectively. The remaining
two accounts searched for the same hotel/car and booked
the highest and lowest priced results, respectively. Sepa-
rate credit cards were used for high- and low-priced reser-
vations, and neither card had ever been used to book travel
before. Although it is possible to imagine other treatments
for account history (e.g., a person who always travels to
a speciﬁc country), price-constrained (inelastic) and uncon-
strained (elastic) users are a natural starting point for exam-
ining the impact of account history. Furthermore, although
these treatments may not embody realistic user behavior,
they do present unambiguous signals that could be observed
and acted upon by personalization algorithms.

We pre-selected a destination and travel dates for each
night, so the click and purchase accounts all used the same
search parameters. Destinations varied across major US,
European, and Asian cities, and dates ranged over the last
six months of 2014. All trips were for one or two night
stays/rentals. On average, the high- and low-price pur-
chasers reserved rooms for $329 and $108 per night, respec-
tively, while the high- and low-price clickers selected rooms
for $404 and $99 per night. The four rental car accounts
were able to click and reserve the exact same vehicles, with
$184 and $43 being the average high- and low-prices per day.
Each night, after we ﬁnished manually creating account
histories, we used PhantomJS to run our standard list of 20
queries from all six accounts on all nine travel sites. To main-
tain consistency, manual history creation and automated
tests all used the same set of IP addresses and Firefox.

Ethics.
We took several precautions to minimize any
negative impact of our purchase history experiments on
travel retailers, hotels, and rental car agencies. We reserved,
at most, one room from any speciﬁc hotel. All reservations
were made for hotel rooms and cars at least one month into
the future, and all reservations were canceled at the conclu-
sion of our experiments.

312Figure 6: Examining the impact of user accounts and cookies on hotel searches on Cheaptickets.

Analyzing Results.
To analyze the data from our
feature experiments, we leverage the same ﬁve metrics used
in § 4. Figure 6 exempliﬁes the analysis we conduct for each
user feature on each e-commerce site. In this example, we
examine whether Cheaptickets personalizes results for users
that are logged-in. The x-axis of each subplot is time in
days. The plots in the top row use Jaccard Index, Kendall’s
τ , and nDCG to analyze steering, while the plots in the
bottom row use percent of items with inconsistent prices
and average price diﬀerence to analyze discrimination.

All of our analysis is always conducted relative to a con-
trol.
In all of the ﬁgures in this section, the starred (*)
feature in the key is the control. For example, in Figure 6,
all analysis is done relative to a PhantomJS instance that
does not have a user account. Each point is an average of
the given metric across all 20 queries on that day.

In total, our analysis produced >360 plots for the various
features across all 16 e-commerce cites. Overall, most of the
experiments do not reveal evidence of steering or discrimina-
tion. Thus, for the remainder of this section, we focus on the
particular features and sites where we do observe personal-

Figure 7: Price discrimination on Cheaptickets. The top result
is shown to users that are not logged-in. The bottom result is a
“Members Only” price shown to logged-in users.

ization. None of our feature tests revealed personalization
on rental car sites, so we omit them entirely.

5.2 Hotels

We begin by analyzing personalization on hotel sites. We
observe hotel sites implementing a variety of personalization
strategies, so we discuss each case separately.

Cheaptickets and Orbitz.
The ﬁrst sites that we
examine are Cheaptickets and Orbitz. These sites are ac-
tually one company, and appear to be implemented using
the same HTML structure and server-side logic. In our ex-
periments, we observe both sites personalizing hotel results
based on user accounts; for brevity we present the analysis
of Cheaptickets and omit Orbitz.

Figures 6(a) and (b) reveal that Cheaptickets serves
slightly diﬀerent sets of results to users who are logged-in
to an account, versus users who do not have an account or
who do not store cookies. Speciﬁcally, out of 25 results per
page, ≈2 are new and ≈1 is moved to a diﬀerent location
on average for logged-in users. In some cases (e.g., hotels in
Bangkok and Montreal) the diﬀerences are much larger: up
to 11 new and 11 moved results. However, the nDCG analy-
sis in Figure 6(c) indicates that these alterations do not have
an appreciable impact on the price of highly-ranked search
results. Thus, we do not observe Cheaptickets or Orbitz
steering results based on user accounts.
However, Figure 6(d) shows that logged-in users receive
diﬀerent prices on ≈5% of hotels. As shown in Figure 6(e),
the hotels with inconsistent prices are $12 cheaper on aver-
age. This demonstrates that Cheaptickets and Orbitz im-
plement price discrimination,
in favor of users who have
accounts on these sites. Manual examination reveals that
these sites oﬀer “Members Only” price reductions on certain
hotels to logged-in users. Figure 7 shows an example of this
on Cheaptickets.

Although it is not surprising that some e-commerce sites
give deals to members, our results on Cheaptickets (and
Orbitz) are important for several reasons. First, although
members-only prices may be an accepted practice, it still
qualiﬁes as price discrimination based on direct segmenta-
tion (with members being the target segment). Second, this
result conﬁrms the eﬃcacy of our methodology, i.e., we are
able to accurately identify price discrimination based on au-
tomated probes of e-commerce sites. Finally, our results

 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. Jaccard(a)-1-0.5 0 0.5 104/0504/1204/1904/2605/0305/10Avg. Kendall’s Tau(b) 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. nDCG(c) 0 5 10 15 2004/0504/1204/1904/2605/0305/10% of Items w/ Diff. Prices(d)Logged-in-20-15-10-5 004/0504/1204/1904/2605/0305/10Avg. Price Difference ($)(e)No Account*Logged-InNo Cookies313Browser

OS

0.4

FX IE8 Chr* Ctrl OSX Lin XP Win7*
0.4 0.4 0.4
0.4
0.9 1.0 1.0 0.9
1.0 0.9 0.9 1.0
1.0 0.9 0.9 1.0
1.0 0.9 0.9 1.0
1.0 0.9 0.9
0.9 1.0
0.9

0.4 0.4 0.4
0.9 0.9 0.9
1.0 1.0
1.0

Account

In No* Ctrl
0.4 0.4 0.3
1.0 1.0 1.0
0.9 0.9 0.9
0.9 0.9 0.9
0.9 0.9 0.9
0.9 0.9 0.9
1.0 1.0 1.0
1.0 1.0 1.0
0.9 0.9 0.9
1.0 1.0
1.0

Ctrl

Win7*

S
O

XP
Lin
OSX
r Ctrl
e
Chr*
s
w
IE8
o
r
FX
B
t Ctrl
c
c
No*
A

Table 4: Jaccard overlap between pairs of user feature experiments on
Expedia.

Figure 8: Clearing cookies causes a user to be placed in
a random bucket on Expedia.

Figure 9: Users in certain buckets are steered towards higher priced hotels on Expedia.

reveal the actual diﬀerences in prices oﬀered to members,
which may not otherwise be public information.

Hotels.com and Expedia.
Our analysis reveals that
Hotels.com and Expedia implement the same personaliza-
tion strategy: randomized A/B tests on users. Since these
sites are similar, we focus on Expedia and omit the details
of Hotels.com.

Initially, when we analyzed the results of our feature tests
for Expedia, we noticed that the search results received by
the control and its twin never matched. More oddly, we
also noticed that 1) the control results did match the results
received by other speciﬁc treatments, and 2) these matches
were consistent over time.

These anomalous results led us to suspect that Expedia
was randomly assigning each of our treatments to a “bucket”.
This is common practice on sites that use A/B testing: users
are randomly assigned to buckets based on their cookie,
and the characteristics of the site change depending on the
bucket you are placed in. Crucially, the mapping from cook-
ies to buckets is deterministic: a user with cookie C will be
placed into bucket B whenever they visit the site unless their
cookie changes.

To determine whether our treatments are being placed
in buckets, we generate Table 4, which shows the Jaccard
Index for 12 pairs of feature experiments on Expedia. Each
table entry is averaged over 20 queries and 10 days. For a
typical website, we would expect the control (Ctrl) in each
category to have perfect overlap (1.0) with its twin (marked
with a *). However, in this case the perfect overlaps occur
between random pairs of tests. For example, the results
for Chrome and Firefox perfectly overlap, but Chrome has
low overlap with the control, which was also Chrome. This
strongly suggests that the tests with perfect overlap have
been randomly assigned to the same bucket. In this case,
we observe three buckets: 1) {Windows 7, account control,

no account, logged-in, IE 8, Chrome}, 2) {XP, Linux, OS X,
browser control, Firefox}, and 3) {OS control}.

To conﬁrm our suspicion about Expedia, we examine the
behavior of the experimental treatment that clears its cook-
ies after every query. We propose the following hypothesis: if
Expedia is assigning users to buckets based on cookies, then
the clear cookie treatment should randomly change buckets
after each query. Our assumption is that this treatment will
receive a new, random cookie each time it queries Expedia,
and thus its corresponding bucket will change.

To test this hypothesis we plot Figure 8, which shows
the Jaccard overlap between search results received by the
clear cookie treatment, and results received by treatments
in other buckets. The x-axis corresponds to the search re-
sults from the clear cookie treatment over time; for each
page of results, we plot a point in the bucket (y-axis) that
has >0.97 Jaccard overlap with the clear cookie treatment.
If the clear cookie treatment’s results do not overlap with
results from any of the buckets, the point is placed on the
“Unknown” row. In no cases did the search results from the
clear cookie treatment have >0.97 Jaccard with more than
a single bucket, conﬁrming that the set of results returned
to each bucket are highly disjoint (see Table 4).

Figure 8 conﬁrms that the clear cookie treatment is ran-
domly assigned to a new bucket on each request. 62% of
results over time align with bucket 1, while 21% and 9%
match with buckets 2 and 3, respectively. Only 7% do not
match any known bucket. These results suggest that Expe-
dia does not assign users to buckets with equal probability.
There also appear to be time ranges where some buckets
are not assigned, e.g., bucket 3 in between 04/12 and 04/15.
We found that Hotels.com also assigns users to one of three
buckets, that the assignments are weighted, and that the
weights change over time.

Now that we understand how Expedia (and Hotels.com)
assign users to buckets, we can analyze whether users in dif-
ferent buckets receive personalized results. Figure 9 presents

Bucket 1Bucket 2Bucket 3Unknown04/0604/1304/2004/27Result Overlap (>0.97)Search Results Over Timewith Cleared Cookies 0 0.2 0.4 0.6 0.8 103/2904/0504/1204/1904/2605/0305/10Avg. Jaccard(a)-1-0.5 0 0.5 103/2904/0504/1204/1904/2605/0305/10Avg. Kendall’s Tau(b) 0 0.2 0.4 0.6 0.8 103/2904/0504/1204/1904/2605/0305/10Avg. nDCG(c)Bucket 1*Bucket 2Bucket 3314Figure 10: Priceline alters hotel search results based on a user’s click and purchase history.

the results of this analysis. We choose an account from
bucket 1 to use as a control, since bucket 1 is the most fre-
quently assigned bucket.

Two conclusions can be drawn from Figure 9. First, we
see that users are periodically shuﬄed into diﬀerent buckets.
Between 04/01 and 04/20, the control results are consistent,
i.e., Jaccard and Kendall’s τ for bucket 1 are ≈1. However,
on 04/21 the three lines change positions, implying that the
accounts have been shuﬄed to diﬀerent buckets. It is not
clear from our data how often or why this shuﬄing occurs.
The second conclusion that can be drawn from Figure 9 is
that Expedia is steering users in some buckets towards more
expensive hotels. Figures 9(a) and (b) show that users in
diﬀerent buckets receive diﬀerent results in diﬀerent orders.
For example, users in bucket 3 see >60% diﬀerent search re-
sults compared to users in other buckets. Figure 9(c) high-
lights the net eﬀect of these changes: results served to users
in buckets 1 and 2 have higher nDCG values, meaning that
the hotels at the top of the page have higher prices. We do
not observe price discrimination on Expedia or Hotels.com.

Priceline.
As depicted in Figure 10, Priceline alters
hotel search results based on the user’s history of clicks and
purchases. Figures 10(a) and (b) show that users who clicked
on or reserved low-price hotel rooms receive slightly diﬀerent
results in a much diﬀerent order, compared to users who
click on nothing, or click/reserve expensive hotel rooms. We
manually examined these search results but could not locate
any clear reasons for this reordering. The nDCG results in
Figure 10(c) conﬁrm that the reordering is not correlated
with prices. Thus, although it is clear that account history
impacts search results on Priceline, we cannot classify the
changes as steering. Furthermore, we observe no evidence of
price discrimination based on account history on Priceline.

Travelocity.
As shown in Figure 11, Travelocity alters
hotel search results for users who browse from iOS devices.
Figures 11(a) and (b) show that users browsing with Safari
on iOS receive slightly diﬀerent hotels, and in a much dif-
ferent order, than users browsing from Chrome on Android,
Safari on OS X, or other desktop browsers. Note that we
started our Android treatment at a later date than the other
treatments, speciﬁcally to determine if the observed changes
on Travelocity occurred on all mobile platforms or just iOS.
Although Figure 11(c) shows that this reordering does not
result in price steering, Figures 11(d) and (e) indicate that
Travelocity does modify prices for iOS users. Speciﬁcally,
prices fall by ≈$15 on ≈5% of hotels (or 5 out 50 per page)
for iOS users. The noise in Figure 11(e) (e.g., prices increas-
ing by $50 for Chrome and IE 8 users) is not signiﬁcant: this
result is due to a single hotel that changed price.

The takeaway from Figure 11 is that we observe evidence
consistent with price discrimination in favor of iOS users on
Travelocity. Unlike Cheaptickets and Orbitz, which clearly
mark sale-price “Members Only” deals, there is no visual
cue on Travelocity’s results that indicates prices have been
changed for iOS users. Online travel retailers have pub-
licly stated that mobile users are a high-growth customer
segment, which may explain why Travelocity oﬀers price-
incentives to iOS users [26].
5.3 General Retailers
Home Depot.
We now turn our attention to general
retail sites. Among the 10 retailers we examined, only Home
Depot revealed evidence of personalization. Similar to our
ﬁndings on Travelocity, Home Depot personalizes results for
users with mobile browsers. In fact, the Home Depot website
serves HTML with diﬀerent structure and CSS to desktop
browsers, Safari on iOS, and Chrome on Android.

Figure 11: Travelocity alters hotel search results for users of Safari on iOS, but not Chrome on Android.

 0 0.2 0.4 0.6 0.8 105/0305/1005/1705/24Avg. Jaccard(a)Control*Click LowClick HighBuy LowBuy High-1-0.5 0 0.5 105/0305/1005/1705/24Avg. Kendall’s Tau(b)Click Low and Buy Low 0 0.2 0.4 0.6 0.8 105/0305/1005/1705/24Avg. nDCG(c) 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. Jaccard(a)-1-0.5 0 0.5 104/0504/1204/1904/2605/0305/10Avg. Kendall’s Tau(b) 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. nDCG(c) 0 5 10 15 2004/0504/1204/1904/2605/0305/10% of Items w/ Diff. Prices(d)iOS Safari-20 0 20 40 6004/0504/1204/1904/2605/0305/10Avg. Price Difference ($)(e)Chrome*IE 8FirefoxSafari on OS XSafari on iOSChrome on Android315Figure 12: Home Depot alters product searches for users of mobile browsers.

Figure 12 depicts the results of our browser experiments
on Home Depot. Strangely, Home Depot serves 24 search re-
sults per page to desktop browsers and Android, but serves
48 to iOS. As shown in Figure 12(a), on most days there is
close to zero overlap between the results served to desktop
and mobile browsers. Oddly, there are days when Home De-
pot brieﬂy serves identical results to all browsers (e.g., the
spike in Figure 12(a) on 4/22). The pool of results served
to mobile browsers contains more expensive products over-
all, leading to higher nDCG scores for mobile browsers in
Figure 12(c). Note that nDCG is calculated using the top
k results on the page, which in this case is 24 to preserve
fairness between iOS and the other browsers. Thus, Home
Depot is steering users on mobile browsers towards more
expensive products.

In addition to steering, Home Depot also discriminates
against Android users. As shown in Figure 12(d), the
Android treatment consistently sees diﬀerences on ≈6% of
prices (one or two products out of 24). However, the prac-
tical impact of this discrimination is low: the average price
diﬀerential in Figure 12(e) for Android is ≈$0.41. We manu-
ally examined the search results from Home Depot and could
not determine why the Android treatment receives slightly
increased prices. Prior work has linked price discrimination
on Home Depot to changes in geolocation [30], but we con-
trol for this eﬀect in our experiments.

It is possible that the diﬀerences we observe on Home
Depot may be artifacts caused by diﬀerent server-side im-
plementations of the website for desktop and mobile users,
rather than an explicit personalization algorithm. However,
even if this is true, it still qualiﬁes as personalization ac-
cording to our deﬁnition (see § 2.3) since the diﬀerences are
deterministic and triggered by client-side state.

6. RELATED WORK

In this section, we brieﬂy overview the academic literature

on implementing and measuring personalization.

Improving Personalization.
Personalizing search re-
sults to improve Information Retrieval (IR) accuracy has
been extensively studied in the literature [33, 42]. While
these techniques typically use click histories for personaliza-
tion, other features have also been used, including geolo-
cation [2, 53, 54] and demographics (typically inferred from
search and browsing histories) [18, 47]. To our knowledge,

only one study has investigated privacy-preserving person-
alized search [52]. Dou et al. provide a comprehensive
overview of techniques for personalizing search [12]. Several
studies have looked at improving personalization on systems
other than search, including targeted ads on the web [16,49],
news aggregators [9, 24], and even discriminatory pricing on
travel search engines [15].

Training and compar-
Comparing Search Results.
ing IR systems requires being able to compare ranked lists
of search results. Thus, metrics for comparing ranked lists
are an active area of research in the IR community. Classi-
cal metrics such as Spearman’s footrule and ρ [11, 38] and
Kendall’s τ [22] both calculate pairwise disagreement be-
tween ordered lists. Several studies improve on Kendall’s τ
by adding per-rank weights [14,40], and by taking item sim-
ilarity into account [21,39]. DCG and nDCG use a logarith-
mic scale to reduce the scores of bottom ranked items [19].

The Filter Bubble.
Activist Eli Pariser brought
widespread attention to the potential for web personal-
ization to lead to harmful social outcomes; a problem he
dubbed The Internet Filter Bubble [31]. This has motivated
researchers to begin measuring the personalization present
in deployed systems, such as web search engines [17, 27, 50]
and recommender systems [4].

Exploiting Personalization.
Recent work has shown
that it is possible to exploit personalization algorithms for
nefarious purposes. Xing et al. [51] demonstrate that re-
peatedly clicking on speciﬁc search results can cause search
engines to rank those results higher. An attacker can exploit
this to promote speciﬁc results to targeted users. Thus, fully
understanding the presence and extent of personalization to-
day can aid in understanding the potential impact of these
attacks on e-commerce sites.

Personalization of E-commerce.
Two recent stud-
ies by Mikians et al. that measure personalization on e-
commerce sites serve as the inspiration for our own work.
The ﬁrst study examines price steering (referred to as search
discrimination) and price discrimination across a large num-
ber of sites using fake user proﬁles [29]. The second paper
extends the ﬁrst work by leveraging crowdsourced workers to
help detect price discrimination [30]. The authors identify
several e-commerce sites that personalize content, mostly
based on the user’s geolocation. We improve upon these

 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. Jaccard(a)-1-0.5 0 0.5 104/0504/1204/1904/2605/0305/10Avg. Kendall’s Tau(b)iOS SafariAndroid 0 0.2 0.4 0.6 0.8 104/0504/1204/1904/2605/0305/10Avg. nDCG(c) 0 5 10 15 2004/0504/1204/1904/2605/0305/10% of Items w/ Diff. Prices(d) 0 20 40 60 8004/0504/1204/1904/2605/0305/10Avg. Price Difference ($)(e)Chrome*IE 8FirefoxSafari on OS XSafari on iOSChrome on Android316studies by introducing duplicated control accounts into all
of our measurements. These additional control accounts are
necessary to conclusively diﬀerentiate inherent noise from
actual personalization.
7. CONCLUDING DISCUSSION

Personalization has become a important feature of many
web services in recent years. However, there is mounting
evidence that e-commerce sites are using personalization al-
gorithms to implement price steering and discrimination.

In this paper, we build a measurement infrastructure to
study price discrimination and steering on 16 top online re-
tailers and travel websites. Our method places great empha-
sis on controlling for various sources of noise in our exper-
iments, since we have to ensure that the diﬀerences we see
are actually a result of personalization algorithms and not
just noise. First, we collect real-world data from 300 AMT
users to determine the extent of personalization that they
experience. This data revealed evidence of personalization
on four general retailers and ﬁve travel sites, including cases
where sites altered prices by hundreds of dollars.

Second, we ran controlled experiments to investigate what
features e-commerce personalization algorithms take into ac-
count when shaping content. We found cases of sites altering
results based on the user’s OS/browser, account on the site,
and history of clicked/purchased products. We also observe
two travel sites conducting A/B tests that steer users to-
wards more expensive hotel reservations.

Comments from Companies.
We reached out to
the six companies we identiﬁed in this study as implement-
ing some form of personalization (Orbitz and Cheaptick-
ets are run by a single company, as are Expedia and Ho-
tels.com) asking for comments on a pre-publication draft of
this manuscript. We received responses from Orbitz and
Expedia. The Vice President for Corporate Aﬀairs at Or-
bitz provided a response conﬁrming that Cheaptickets and
Orbitz oﬀer members-only deals on hotels. However, their
response took issue with our characterization of price dis-
crimination as “anti-consumer”; we removed these assertions
from the ﬁnal draft of this manuscript. The Orbitz repre-
sentative kindly agreed to allow us to publish their letter on
the Web [7].

We also spoke on the phone with the Chief Product Of-
ﬁcer and the Senior Director of Stats Optimization at Ex-
pedia. They conﬁrmed our ﬁndings that Expedia and Ho-
tels.com perform extensive A/B testing on users. However,
they claimed that Expedia does not implement price dis-
crimination on rental cars, and could not explain our results
to the contrary (see Figure 3).

Scope.
In this study we focus on US e-commerce sites.
All queries are made from IP addresses in the US, all re-
tailers and searches are in English, and real world data is
collected from users in the US. We leave the examination of
personalization on e-commerce sites in other countries and
other languages to future work.

Incompleteness.
As a result of our methodology, we
are only able to identify positive instances of price discrimi-
nation and steering; we cannot claim the absence of person-
alization, as we may not have considered other dimensions
along which e-commerce sites might personalize content. We
observe personalization in some of the AMT results (e.g., on
Newegg and Sears) that we cannot explain with the ﬁndings

from our feature-based experiments. These eﬀects might be
explained by measuring the impact of other features, such
as geolocation, HTTP Referer, browsing history, or purchase
history. Given the generality of our methodology, it would
be straightforward to apply it to these additional features,
as well as to other e-commerce sites.

All of our experiments were conducted
Open Source.
in spring of 2014. Although our results are representative for
this time period, they may not hold in the future, as the sites
may change their personalization algorithms. We encourage
other researchers to repeat our measurements by making all
of our crawling and parsing code, as well as the raw data
from § 4 and § 5, available to the research community at

http://personalization.ccs.neu.edu/

Acknowledgements
We thank the anonymous reviewers and our shepherd, Vijay
Erramilli, for their helpful comments. This research was
supported in part by NSF grants CNS-1054233 and CHS-
1408345, ARO grant W911NF-12-1-0556, and an Amazon
Web Services in Education grant.
8. REFERENCES
[1] Bezos calls Amazon experiment ’a mistake’. Puget

Sound Business Journal, 2000.
http://www.bizjournals.com/seattle/stories/
2000/09/25/daily21.html.

[2] L. Andrade and M. J. Silva. Relevance Ranking for

Geographic IR. GIR, 2006.

[3] Amazon mechanical turk. http://mturk.com/.
[4] F. Bakalov, M.-J. Meurs, B. K¨onig-Ries, B. Satel,

R. G. Butler, and A. Tsang. An Approach to
Controlling User Models and Personalization Eﬀects
in Recommender Systems. IUI, 2013.

[5] K. Bhasin. JCPenney Execs Admit They Didn’t

Realize How Much Customers Were Into Coupons.
Business Insider, 2012.
http://www.businessinsider.com/jcpenney-didnt-
realize-how-much-customers-\\were-into-
coupons-2012-5.

[6] P. Belobaba, A. Odoni, and C. Barnhart. The Global

Airline Industry. Wiley, 2009.

[7] C. Chiames. Correspondence with the authors, in

reference to a pre-publication version of this
manuscript, 2014. http://personalization.ccs.neu.
edu/orbitz_letter.pdf.

[8] R. Calo. Digital Market Manipulation. The George

Washington Law Review, 82, 2014.

[9] A. Das, M. Datar, A. Garg, and S. Rajaram. Google
News Personalization: Scalable Online Collaborative
Filtering. WWW, 2007.

[10] C. Duhigg. How Companies Learn Your Secrets. The

New York Times, 2012. http://www.nytimes.com/
2012/02/19/magazine/shopping-habits.html.

[11] P. Diaconis and R. L. Graham. Spearman’s Footrule

as a Measure of Disarray. J. Roy. Stat. B, 39(2), 1977.

[12] Z. Dou, R. Song, and J.-R. Wen. A Large-scale
Evaluation and Analysis of Personalized Search
Strategies. WWW, 2007.

[13] J. H. Dorfman. Economics and Management of the

Food Industry. Routledge, 2013.

317[14] R. Fagin, R. Kumar, and D. Sivakumar. Comparing

top k lists. SODA, 2003.

[15] A. Ghose, P. G. Ipeirotis, and B. Li. Designing

Ranking Systems for Hotels on Travel Search Engines
by Mining User-Generated and Crowdsourced
Content. Marketing Science, 31(3), 2012.

[16] S. Guha, B. Cheng, and P. Francis. Challenges in

Measuring Online Advertising Systems. IMC, 2010.

[17] A. Hannak, P. Sapiezy´nski, A. M. Kakhki, B.

Krishnamurthy, D. Lazer, A. Mislove, and C. Wilson.
Measuring Personalization of Web Search. WWW,
2013.

[18] J. Hu, H.-J. Zeng, H. Li, C. Niu, and Z. Chen.

Demographic Prediction Based on User’s Browsing
Behavior. WWW, 2007.

[19] K. J¨arvelin and J. Kek¨al¨ainen. IR evaluation methods
for retrieving highly relevant documents. SIGIR, 2000.
[20] K. J¨arvelin and J. Kek¨al¨ainen. Cumulated Gain-based
Evaluation of IR Techniques. ACM TOIS, 20(4), 2002.

[34] Panopticlick. https://panopticlick.eff.org.
[35] PhantomJS. 2013. http://phantomjs.org.
[36] A. Ramasastry. Web sites change prices based on

customers’ habits. CNN, 2005. http://edition.cnn.
com/2005/LAW/06/24/ramasastry.website.prices/.

[37] C. Shapiro and H. R. Varian. Information Rules: A

Strategic Guide to the Network Economy. Harvard
Business School Press, 1999.

[38] C. Spearman. The Proof and Measurement of

Association between Two Things. Am J Psychol, 15,
1904.

[39] D. Sculley. Rank Aggregation for Similar Items. SDM,

2007.

[40] G. S. Shieh, Z. Bai, and W.-Y. Tsai. Rank Tests for

Independence—With a Weighted Contamination
Alternative. Stat. Sinica, 10, 2000.

[41] Selenium. 2013. http://selenium.org.
[42] B. Tan, X. Shen, and C. Zhai. Mining long-term

search history to improve search accuracy. KDD, 2006.

[21] R. Kumar and S. Vassilvitskii. Generalized Distances

[43] Top 500 e-retailers.

Between Rankings. WWW, 2010.

http://www.top500guide.com/top-500/.

[22] M. G. Kendall. A New Measure of Rank Correlation.

[44] Top booking sites.

Biometrika, 30(1/2), 1938.

[23] W. H. Kruskal. Ordinal Measures of Association.
Journal of the American Statistical Association,
53(284), 1958.

[24] L. Li, W. Chu, J. Langford, and R. E. Schapire. A
Contextual-Bandit Approach to Personalized News
Article Recommendation. WWW, 2010.

[25] B. D. Lollis. Orbitz: Mac users book fancier hotels

than PC users. USA Today Travel Blog, 2012.
http://travel.usatoday.com/hotels/post/2012/
05/orbitz-hotel-booking-mac-pc-/690633/1.

http://skift.com/2013/11/11/top-25-online-
booking-sites-in-travel/.

[45] T. Vissers, N. Nikiforakis, N. Bielova, and W. Joosen.

Crying Wolf? On the Price Discrimination of Online
Airline Tickets. HotPETs, 2014.

[46] J. Valentino-Devries, J. Singer-Vine, and A. Soltani.

Websites Vary Prices, Deals Based on Users’
Information. Wall Street Journal, 2012.
http://online.wsj.com/news/articles/
SB10001424127887323777204578189391813881534.
[47] I. Weber and A. Jaimes. Who Uses Web Search for

[26] B. D. Lollis. Orbitz: Mobile searches may yield better

What? And How? WSDM, 2011.

hotel deals. USA Today Travel Blog, 2012.
http://travel.usatoday.com/hotels/post/2012/
05/orbitz-mobile-hotel-deals/691470/1.

[27] A. Majumder and N. Shrivastava. Know your

personalization: learning topic level personalization in
online services. WWW, 2013.

[48] T. Wadhwa. How Advertisers Can Use Your Personal

Information to Make You Pay Higher Prices.
Huﬃngton Post, 2014.
http://www.huffingtonpost.com/tarun-wadhwa/
how-advertisers-can-use-y_b_4703013.html.

[49] C. E. Wills and C. Tatar. Understanding What They

[28] D. Mattioli. On Orbitz, Mac Users Steered to Pricier

Do with What They Know. WPES, 2012.

Hotels. The Wall Street Journal, 2012.
http://on.wsj.com/LwTnPH.

[29] J. Mikians, L. Gyarmati, V. Erramilli, and N.

Laoutaris. Detecting Price and Search Discrimination
on the Internet. HotNets, 2012.

[30] J. Mikians, L. Gyarmati, V. Erramilli, and N.

Laoutaris. Crowd-assisted Search for Price
Discrimination in E-Commerce: First results.
CoNEXT, 2013.

[31] E. Pariser. The Filter Bubble: What the Internet is

[50] X. Xing, W. Meng, D. Doozan, N. Feamster, W. Lee,
and A. C. Snoeren. Exposing Inconsistent Web Search
Results with Bobble. PAM, 2014.

[51] X. Xing, W. Meng, D. Doozan, A. C. Snoeren, N.

Feamster, and W. Lee. Take This Personally:
Pollution Attacks on Personalized Services. USENIX
Security, 2013.

[52] Y. Xu, B. Zhang, Z. Chen, and K. Wang.

Privacy-Enhancing Personalized Web Search. WWW,
2007.

Hiding from You. Penguin Press, 2011.

[53] B. Yu and G. Cai. A query-aware document ranking

[32] I. Png. Managerial Economics. Routledge, 2012.
[33] J. Pitkow, H. Sch¨utze, T. Cass, R. Cooley, D.

Turnbull, A. Edmonds, E. Adar, and T. Breuel.
Personalized search. CACM, 45(9), 2002.

method for geographic information retrieval. GIR,
2007.

[54] X. Yi, H. Raghavan, and C. Leggetter. Discovering

Users’ Speciﬁc Geo Intention in Web Search. WWW,
2009.

318