Preserving Link Privacy in Social Network Based Systems

Prateek Mittal

Charalampos Papamanthou

Dawn Song

University of California,

University of California,

University of California,

Berkeley

Berkeley

Berkeley

pmittal@eecs.berkeley.edu

cpap@eecs.berkeley.edu

dawnsong@cs.berkeley.edu

Abstract

A growing body of research leverages social network
based trust relationships to improve the functionality of
the system. However, these systems expose users’ trust
relationships, which is considered sensitive information
in today’s society, to an adversary.

In this work, we make the following contributions.
First, we propose an algorithm that perturbs the struc-
ture of a social graph in order to provide link privacy,
at the cost of slight reduction in the utility of the so-
cial graph. Second we deﬁne general metrics for char-
acterizing the utility and privacy of perturbed graphs.
Third, we evaluate the utility and privacy of our pro-
posed algorithm using real world social graphs. Finally,
we demonstrate the applicability of our perturbation al-
gorithm on a broad range of secure systems, including
Sybil defenses and secure routing.

1 Introduction

In recent years, several proposals have been put for-
ward that leverage user’s social network trust relation-
ships to improve system security and privacy. So-
cial networks have been used for Sybil defense [36,
35, 6, 23, 31], secure routing [14, 21, 17], secure
reputation systems [32], mitigating spam [19], cen-
sorship resistance [30], and anonymous communica-
tion [5, 25, 20, 22].

A signiﬁcant barrier to the deployment of these sys-
tems is that they do not protect the privacy of user’s
trusted social contacts. Information about user’s trust
relationships is considered sensitive in today’s society;
in fact, existing online social networks such as Face-
book, Google+ and Linkedin provide explicit mecha-
nisms to limit access to this information. A recent
study by Dey et al. [7] found that more than 52% of
Facebook users hide their social contact information.

Most protocols that leverage social networks for sys-
tem security and privacy either explicitly reveal users’
trust relationships to an adversary [6] or allow the ad-
versary to easily perform traﬃc analysis and infer these
trust relationships [35]. Thus the design of these sys-
tems is fundamentally in conﬂict with the current on-
line social network paradigm, and hinders deployment.
In this work, we focus on protecting the privacy of
users’ trusted contacts (edge/link privacy, not vertex
privacy) while still maintaining the utility of higher
level systems and applications that leverage the social
graph. Our key insight in this work is that for a large
class of security applications that leverage social rela-
tionships, preserving the exact set of edges in the graph
is not as important as preserving the graph-theoretic
structural diﬀerences between the honest and dishonest
users in the system.

This insight motivates a paradigm of structured
graph perturbation, in which we introduce noise in the
social graph (by deleting real edges and introducing
fake edges) such that the local structures in the orig-
inal social graph are preserved. We believe that for
many applications, introducing a high level of noise in
such a structured fashion does not reduce the overall
system utility.

1.1 Contributions

In this work, we make the following contributions.
• First, we propose a mechanism based on random
walks for perturbing the structure of the social
graph that provides link privacy at the cost of a
slight reduction in application utility (Section 4).
• We deﬁne a general metric for characterizing the
utility of perturbed graphs. Our utility deﬁnition
considers the change in graph structure from the
perspective of a vertex. We formally relate our no-
tion of utility to global properties of social graphs,
such as mixing times and second largest eigenvalue

modulus of graphs, and analyze the utility prop-
erties of our perturbation mechanism using real
world social networks (Section 5).

• We deﬁne several metrics for characterizing link
privacy, and consider prior information that an
adversary may have for de-anonymizing links. We
also formalize the relationship between utility and
privacy of perturbed graphs, and analyze the pri-
vacy properties of our perturbation mechanism us-
ing real world social networks (Section 6).

• Finally, we experimentally demonstrate the real
world applicability of our perturbation mechanism
on a broad range of secure systems, including Sybil
defenses and secure routing (Section 7). In fact,
we ﬁnd that for Sybil defenses, our techniques are
of interest even outside the context of link privacy.

2 Related Work

Work in this space can be broadly classiﬁed into two
categories: (a) mechanisms for protecting the privacy
of links between labeled vertices, and (b) mechanisms
for protecting node/graph privacy when vertices are
unlabeled.

For many real world applications relying on social
networks, such as reputation systems, recommendation
systems, Sybil defenses, secure routing, and anonymous
communications, it is critical for a user to know the
identities of the vertices in the social graph that it in-
teracts with. Protocols that preserve vertex privacy
(unlabeled vertices) would not be appropriate for such
applications. On the other hand, these applications
could reveal sensitive social contacts to an adversary;
thus the focus of this work is on protecting the privacy
of links between labeled vertices.

2.1 Link privacy between labeled vertices

There are two main mechanisms for preserving link
privacy between labeled vertices. The ﬁrst approach is
to perform clustering of vertices and edges, and aggre-
gate them into super vertices (e.g., [11] and [37]). In
this way, information about corresponding sub-graphs
can be anonymized. While these clustering approaches
permit analysis of some macro-level graph properties,
they are not suitable for black-box application of ex-
isting social network based applications, such as Sybil
defenses. The second class of approaches aim to intro-
duce perturbation in the social graph by adding and
deleting edges and vertices. Next, we discuss this line
of research in more detail.

Hay et al. [12] propose a perturbation algorithm
which applies a sequence of k edge deletions followed
by k random edge insertions. Candidates for edge dele-
tion are sampled uniformly at random from the space
of existing edges in graph G, while candidates for edge
insertion are sampled uniformly at random from the
space of edges not in G. The key diﬀerence between
our perturbation mechanism and that of Hay et al. is
that we sample edges for insertion based on the struc-
ture of the original graph (as opposed to random selec-
tion). We will compare our approach with that of Hay
et al. in Section 6.

Ying and Wu [34] study the impact of Hay et al.’s
perturbation algorithms [12] on the spectral properties
of graphs, as well as on link privacy. They also propose
a new perturbation algorithm that aims to preserve the
spectral properties of graphs, but do not analyze its
privacy properties.

Korolova et al. [13] show that link privacy of the
overall social network can be breached even if infor-
mation about the local neighborhood of social network
nodes is leaked (for example, via a look-ahead feature
for friend discovery).

2.2 Anonymizing the vertices

Although the techniques described above reveal the
identity of the vertices in the social graph but add noise
to the relationships between them, there have been var-
ious works in the literature that aim at anonymizing
the identities of the nodes in the social network. This
line of research is orthogonal to our goals, but we de-
scribe them for completeness.

The straightforward approach of just removing the
identiﬁers of the nodes before publishing the social
graph does not always guarantee privacy, as shown
by Backstrom et. al. [2]. To deal with this problem,
Liu and Terzi [15] propose a systematic framework for
identity anonymization on graphs, where they intro-
duce the notion of k-degree anonymity. Their goal is
to minimally modify the graph by changing the degrees
of specially-chosen nodes so that the identity of each
individual involved is protected. An eﬃcient version
of their algorithm was recently implemented by Lu et
al. [16].

Another notion of graph anonymity in social net-
works is presented by Pei and Zhou [38]: A graph is
k-anonymous if for every node there exist at least k− 1
other nodes that share isomorphic neighborhoods. This
is a stronger deﬁnition than the one in [15], where only
vertex degrees are considered.

Zhou and Pei [39] recently introduced another no-
tion called l-diversity for social network anonymiza-

tion. In this case, each vertex is associated with some
non-sensitive attributes and some sensitive attributes.
Maintaining the privacy of the individual in this sce-
nario is based on the adversary not being able (with
high probability) to re-identify the sensitive attribute
values of the individual.

Finally, Narayanan and Shmatikov [26] show some
of the weaknesses of the above anonymization tech-
niques, propose a generic way for modeling the release
of anonymized social networks and report on successful
de-anonymization attacks on popular networks such as
Flickr and Twitter.

2.3 Differential privacy and social networks

Sala et al. [29] use diﬀerential privacy (a more elab-
orate tool of adding noise) to publish social networks
with privacy guarantees. Given a social network and a
desired level of diﬀerential privacy guarantee, they ex-
tract a detailed structure into degree correlation statis-
tics, introduce noise into these statistics, and use them
to generate a new synthetic social network with diﬀer-
ential privacy. However, their approach does not pre-
serve utility of the social graph from the perspective of
a vertex (since vertices in their graph are unlabeled),
and thus cannot be used for many real world applica-
tions such as Sybil defenses.

Also, Rastogi et al. [28] introduce a relaxed notion of
diﬀerential privacy for data with relationships so that
more expressive queries (e.g., joins) can be supported
without signiﬁcant harm to utility.

2.4 Link privacy preserving applications

X-Vine [21] proposes to perform DHT routing us-
ing social links, in a manner that preserves the privacy
of social links. However, the threat model in X-Vine
excludes adversaries that have prior information about
the social graph. Thus in real world settings, X-Vine
is vulnerable to the Narayanan-Shmatikov attack [26].
Moreover the techniques in X-Vine are speciﬁc to DHT
routing, and cannot be used to design a general pur-
pose defense mechanism for social network based ap-
plications, which is the focus of this work.

3 Basic Theory

|V | = n and |E| = m. The focus of this paper is on
undirected graphs, where the edges are symmetric. Let
AG denote the n × n adjacency matrix corresponding
to the graph G, namely if (i, j) ∈ E, then Aij = 1,
otherwise Aij = 0.

A random walk on a graph G starting at a vertex v
is a sequence of vertices comprising a random neighbor
v1 of v, then a random neighbor v2 of v1 and so on. A
random walk on a graph can be viewed as a Markov
chain. We denote the transition probability matrix of
the random walk/Markov chain as P , given by:

(cid:40) 1

Pij =

deg(i)
0

if (i, j) is an edge in G ,

otherwise .

(1)

where deg(i) denotes the degree of the vertex i. At
any given iteration t of the random walk, let us de-
note with π(t) the probability distribution of the ran-
dom walk state at that iteration (π(t) is a vector of
n entries). The state distribution after t iterations is
given by π(t) = π(0)· P t, where π(0) is the initial state
distribution. The probability of a t-hop random walk
starting from i and ending at j is given by (P t)ij. From
now on, to simplify the notation, we write (P t)ij as P t
ij
in order to denote the element (i, j) of the matrix P t.
For irreducible and aperiodic graphs (which undi-
rected and connected social graphs are), the corre-
sponding Markov chain is ergodic, and the state distri-
bution of the random walk π(t) converges to a unique
stationary distribution denoted by π. The stationary
distribution π satisﬁes π = π · P .

For undirected and connected social graphs, we can
see that the probability distribution πi = deg(i)
2·m satisﬁes
the equation π = π·P , and is thus the unique stationary
distribution of the random walk.
Let us denote the eigenvalues of A as λ1 ≥ λ2 ≥
. . . ≥ λn, and the eigenvalues of P as ν1 ≥ ν2 ≥ . . . ≥
νn. The eigenvalues of both A and P are real. We
denote the second largest eigenvalue modulus (SLEM)
of the transition matrix P as µ = max(|ν2|,|νn|). The
eigenvalues of matrices A and P are closely related to
structural properties of graphs, and are considered util-
ity metrics in the literature.

4 Structured Perturbation

4.1 System model and goals

Before we introduce our mechanism, we present
some necessary notation and background on graph the-
ory.

Let us denote the social graph as G = (V, E), com-
prising the set of vertices V (wlog assume the vertices
have labels 1, . . . , n), and the set of edges E, where

For the deployment of secure applications that lever-
age user’s trust relationships, we envision a scenario
where these applications bootstrap user’s trust rela-
tionships using existing online social networks (OSNs)
such as Facebook or Google+.

However, most applications that leverage this infor-
mation do not make any attempt to hide it; thus an
adversary can exploit protocol messages to learn the
entire social graph.

Our vision is that OSNs can support these appli-
cations while protecting the link privacy of users by
introducing noise in the social graph. Of course the
addition of noise must be done in a manner that still
preserves application utility. Moreover the mechanism
for introducing noise should be computationally eﬃ-
cient, and must not present undue burden to the OSN
operator.

We need a mechanism that takes the social graph
G as an input, and produces a transformed graph
G(cid:48) = (V, E(cid:48)), such that the vertices in G(cid:48) remain the
same as the original input graph G, but the set of edges
is perturbed to protect link privacy. The constraint
on the mechanism is that application utility of systems
that leverage the perturbed graph should be preserved.
Conventional metrics of utility include degree sequence
and graph eigenvalues; we will shortly deﬁne a general
metric for utility of perturbed graphs in the following
section. There is a tradeoﬀ between privacy of links
in the social graph and the utility derived out of per-
turbed graphs. As more and more noise is added to
the social graph, the link privacy increases, but the
corresponding utility decreases.

4.2 Perturbation algorithm

We propose the paradigm of structured graph pertur-
bation, in which we introduce noise in the social graph
(by deleting real edges and introducing fake edges) such
that the local structures in the original social graph are
preserved.

Let t be the parameter that governs how much noise
we wish to inject in the social graph. We propose that
for each node u in graph G, we perturb all of u’s con-
tacts as follows. Suppose that node v is a social contact
of node u. Then we perform a random walk of length
t − 1 starting from node v. Let node z denote the ter-
minus point of the random walk. Our main idea is that
instead of the edge (u, v), we will introduce the edge
(u, z) in the graph G(cid:48). It is possible that the random
walk terminates at either node u itself, or that node z is
already a social contact of u in the transformed graph
G(cid:48) (due to a previously added edge). To avoid self loops
and duplicate edges, we perform another random walk
from vertex v until a suitable terminus vertex is found,
or we reach a threshold number of tries, denoted by
parameter M .

For undirected graphs, the algorithm described so
far would double the number of edges in the perturbed

graphs: for each edge (u, v) in the original graph, an
edge would be added between a vertex u and the ter-
minus point of the random walk from vertex v, as well
as between vertex v and the terminus point of the ran-
dom walk from vertex u. To preserve the degree dis-
tribution, we could add an edge between vertex u and
vertex z in the transformed graph with probability 0.5.
However this could lead to low degree nodes becom-
ing disconnected from the social graph with non-trivial
probability. To account for this case, we add the ﬁrst
edge corresponding to the vertex u with probability
1, while the remaining edges are accepted with a re-
duced probability to preserve the degree distribution.
The overall algorithm is depicted in Algorithm 1. The
computational complexity of our algorithm is O(m).

Algorithm 1 Transform(G, t, M ): Perturb undirected
graph G using perturbation t and maximum loop count
M .

G(cid:48) = null;
foreach vertex u in G

let count = 1;
foreach neighbor v of vertex u
let loop = 1;
do
perform t − 1 hop random walk from vertex v;
let z denote the terminal vertex of the random

walk;

loop + +;
until (u = z ∨ (u, z) ∈ G(cid:48)) ∧ (loop ≤ M )
if loop ≤ M
if count = 1
add edge (u, z) in G(cid:48)
else
let deg(u) denote degree of u in G;

in G(cid:48) with probability

(u, z)

add edge

0.5×deg(u)−1

;
deg(u)−1
count + +;
return G(cid:48);

4.3 Visual depiction of algorithm

For our evaluation, we consider two real world so-
cial network topologies (a) Facebook friendship graph
from the New Orleans regional network [33] :
the
dataset comprises 63,392 users that have 816,886 edges
amongst them, and (b) Facebook interaction graph
from the New Orleans regional network [33] :
the
dataset comprises 43,953 users that have 182,384 edges
amongst them. Mohaisen et al. [24] found that pre-
processing social graphs to exclude low degree nodes
signiﬁcantly changes the graph theoretic characteris-

(a)

(b)

(c)

(d)

(e)

Figure 1. Facebook dataset link topology (a) Original graph (b) Perturbed, t = 5 (c) t = 10 (d) t = 15,
and (e) t = 20. The color coding in (a) is derived using a modularity based community detection
algorithm. For the remaining ﬁgures, the color coding of vertices is same as in (a). We can see
that short random walks preserve the community structure of the social graph, while introducing a
signiﬁcant amount of noise.

tics. Therefore, we did not pre-process the datasets at
all.

Figure 1 depicts the original Facebook friendship
graph, and the perturbed graphs generated by our al-
gorithm for varying perturbation parameters, using a
force directed algorithm for depicting the graph [3].
The color coding of nodes in the ﬁgure was obtained by
running a modularity based community detection al-
gorithm [3] on the original Facebook friendship graph,
which yielded three communities. For the perturbed
graphs, we used the same color for the vertices as in
the original graph. This representation allows us to vi-
sually see the perturbation in the community structure
of the social graph. We can see that for small values of
the perturbation parameter, the community structure
(related to utility) is strongly preserved, even though
the edges between vertices are randomized. As the per-
turbation parameter is increased, the graph looses its
community structure, and eventually begins to resem-
ble a random graph.

Figure 2 depicts a similar visualization for the Face-
book interaction graph. In this setting, we found two

communities using a modularity based community de-
tection algorithm in the original graph. We can see a
similar trend in the Facebook interaction graph as well:
for small values of perturbation algorithm, the com-
munity structure is somewhat preserved, even though
signiﬁcant randomization has been introduced in the
links. In the following sections, we formally quantify
the utility and privacy properties of our perturbation
mechanism.

5 Utility

In this section, we develop formal metrics to charac-
terize the utility of perturbed graphs, and then analyze
the utility of our perturbation algorithm.

5.1 Metrics

One approach to measure utility would be to con-
sider global graph theoretic metrics, such as the second
largest eigenvalue modulus of the graph transition ma-
trix P . However, from a user perspective, it may be the

(a)

(b)

(c)

(d)

(e)

Figure 2. Facebook dataset interaction topology (a) Original graph (b) Perturbed t = 5 (c) t = 10 (d)
t = 15, and (e) t = 20. We can see that short random walks preserve the community structure of the
social graph, while introducing a signiﬁcant amount of noise.

case that the users’ position in the perturbed graph rel-
ative to malicious users is much worse, even though the
global graph properties remain the same. This moti-
vates our ﬁrst deﬁnition of utility of a perturbed graph
from the perspective of a single user.

Deﬁnition 1. The vertex utility of a perturbed graph
G(cid:48) for a vertex v, with respect to the original graph
G and an application parameter l, is deﬁned as the
statistical distance between the probability distributions
induced by l hop random walks starting from v in graphs
G and G(cid:48), i.e.,

VU(v, G, G(cid:48), l) = distance(P l

v(G), P l

v(G(cid:48))) ,

where P l

v denotes the v-th row of the matrix P l.

In the above deﬁnition, the parameter l is linked to
higher level applications that leverage social graphs.
For example, Sybil defense mechanisms exploit large
scale community structure of social networks, where
the application parameter l ≥ 10. For applications
such as recommendation systems, it may be more im-
portant to preserve the local community characteris-
tics, where l could be set to a smaller value.

Random walks are intimately linked to the structure
of communities and graphs, so it is natural to consider
their use when deﬁning utility of perturbed graphs.
In fact, a lot of security applications directly exploit
the trust relationships in social graphs by performing
random walks themselves, such as Sybil defenses and
anonymous communication.

There are several ways to deﬁne statistical distance
between probability distributions P and Q [4] (assume
P and Q are vectors of n entries summing up to one).
In this work, we consider the following three notions.
The total variation distance between two probability
distributions is a measure of the maximum diﬀerence
between the probability distributions for any individual
element. Namely

distancev(P, Q) = ||P − Q||tvd = sup

i

|pi − qi| .

As we will discuss shortly, the total variation distance
is closely related to the computation of several graph
properties such as mixing time and second largest
eigenvalue modulus. However, the total variation dis-
tance only considers the maximum diﬀerence between
probability distributions corresponding to an element,

and not the diﬀerences in probabilities corresponding
to other elements of the distribution. This motivates
the use of Hellinger distance, deﬁned as

(cid:118)(cid:117)(cid:117)(cid:116) n(cid:88)

i=1

distanceh(P, Q) =

·

1√
2

√
(

pi − √

qi)2 .

The Hellinger distance is related to the Euclidean
distance between the square root vectors of P and
Q. Finally, we also consider the Jenson-Shannon
distance measure, which takes an information theo-
retic approach of averaging the Kullback-Leibler diver-
gence between P and Q, and between Q and P (since
Kullback-Leibler divergence by itself is not symmetric)
and is deﬁned as

· n(cid:88)

i=1

(cid:18) pi

(cid:19)

qi

+

1
2

· n(cid:88)

i=1

(cid:18) qi

(cid:19)

pi

pi log

qi log

distancej(P, Q) =

1
2

Using these notions, we can compute the utility
of the perturbed graph with respect to an individual
vertex (vertex utility). Note that a lower value of
VU(v, G, G(cid:48), l) corresponds to higher utility (we want
distance between probability distributions over original
graph and perturbed graph to be low). Using the con-
cept of vertex utility, we can deﬁne metrics for overall
utility of a perturbed graph.

Deﬁnition 2. The overall mean vertex utility of a per-
turbed graph G(cid:48) with respect to the original graph G,
and an application parameter l is deﬁned as the mean
utility for all vertices in G, i.e.,

VUmean(G, G(cid:48), l) =

distance(P l

v(G(cid:48)))

.

v(G), P l
|V |

(cid:88)

v∈V

Similarly the maximum vertex utility (worst case) of a
perturbed graph G(cid:48) is deﬁned by computing the maxi-
mum of the utility values over all vertices in G, i.e.,
v(G(cid:48)))} .

VUmax(G, G(cid:48), l) = max
v∈V

{distance(P l

v(G), P l

The notion of maximum vertex utility is particularly
interesting, specially in conjunction with the use of to-
tal variation distance. This is because of its relation-
ship to global graph metrics such as mixing times and
second largest eigenvalue modulus, which we demon-
strate next. Our analysis shows the generality of our
formal deﬁnition for utility.

5.2 Metrics analysis

Towards this end, we ﬁrst introduce the notion of
mixing time of a Markov process. The mixing time of a

Markov process is a measure of the minimum number
of steps needed to converge to its unique stationary
distribution. Formally, the mixing time of a graph G =
(V, E) is deﬁned as

τG() = max
v∈V

{min

t>0

{t : |P t

v(G) − π| < }} ,

where π is the stationary distribution deﬁned in Sec-
tion 3. The following two theorems illustrate the bound
on global properties of the perturbed graph, using the
global properties of the original graph, and the utility
metric. We defer the proofs of these theorems to the
Appendix.
Theorem 1. Let VUmax(G, G(cid:48), l) be the maximum
vertex utility distance between the perturbed graph
G(cid:48) and the original graph G.
Then τG(cid:48)( +
VUmax(G, G(cid:48), τG()) ≤ τG().

Theorem 1 relates the mixing time of the perturbed
graph using the mixing time of the original graph, and
the max vertex utility metric, for application parame-
ter l = τG().

.

Theorem 2. Let µG be the second largest eigenvalue
modulus (SLEM) of transition matrix PG of graph
G. We can bound the SLEM µG(cid:48) of a perturbed
graph G(cid:48) using the mixing time τG() of the origi-
nal graph and the worst case vertex utility distance
χ = VUmax(G, G(cid:48), τG()) between G and G(cid:48). Namely
we have

µG(cid:48) ≤

2τG()

(cid:16) 1

(cid:17) .

2τG() + log

2+2χ

Theorem 2 relates the second largest eigenvalue
modulus of the perturbed graph, using the mixing time
of the original graph, and the worst case vertex utility
metric for application parameter l = τG().

These theorems show the generality of our util-
ity deﬁnitions. Mechanisms that provide good utility
(have low values of VUmax), introduce only a small
change in the mixing time and SLEM of perturbed
graphs.

5.3 Algorithm analysis

Our above results show the general relationship be-
tween our utility metrics and global graph properties
(which hold for any perturbation algorithm). Next, we
analyze the properties of our proposed perturbation al-
gorithm.

First, we empirically compute the mean vertex util-
ity of the perturbed graphs (VUmean), for varying per-
turbation parameters and varying application param-
eters. Figure 3 depicts the mean vertex utility for the

(a)

(b)

Figure 3. Jenson-Shannon distance between transient probability distributions for original graph and trans-
formed graph using (a) Facebook interaction graph (b) Facebook link graph. We can see that as
the original graph is perturbed to a larger degree, the distance between original and transformed
transient distributions increases, decreasing application utility.

(a)

(b)

Figure 4. Hellinger distance between transient probability distributions for original graph and transformed
graph using (a) Facebook interaction graph (b) Facebook link graph. We can see that even with differ-
ent notions of distance between probability distributions (Hellinger/Jenson-Shannon), the distance
between original graph and perturbed graph monotonically increases depending on the perturbation
degree.

Facebook interaction and friendship graphs using the
Jenson-Shannon information theoretic distance metric.
We can see that as the perturbation parameter in-
creases, the distance metric increases. This is not sur-
prising, since additional noise will increase the distance
between probability distributions computed from orig-
inal and perturbed graphs. We can also see that as the
application parameter l increases, the distance metric
decreases. This illustrates that our perturbation algo-
rithm is ideally suited for security applications that rely
on local or global community structures, as opposed to
applications that require exact information about one
or two hop neighborhoods. We can see a similar trend
when using Hellinger distance to compute the distance
between probability distributions, as shown in Figure 4.

Theorem 3. The expected degree of each node after
the perturbation algorithm is the same as in the original
graph, i.e., ∀v ∈ V , it holds that E(deg
(v)) = deg(v),
where deg

(v) denotes the degree of vertex v in G(cid:48).

(cid:48)

(cid:48)

Proof. On an expectation, half the degree of any node

v is preserved via outgoing random walks from v in the
perturbation process. To prove the theorem, we need to
show that each node v is the terminal point of deg(v)/2
random walks in the perturbation mechanisms (on av-
erage). From the time reversibility property of the
random walks, we have that P t
jiπj. Thus for
any node i, the incoming probability of a random walk
deg(i)
starting from node j is P t
deg(j) , i.e., it is pro-
portional to the node degree of i. Thus the expected
number of random walks terminating at node i in the
v∈V deg(v)P t−1
.
iv deg(i) = deg(i). Since
half of these walks will be added to the graph G(cid:48) on
average, we have that E(deg

perturbation algorithm is given by(cid:80)
This is equivalent to(cid:80)

v∈V P t−1

(v)) = deg(v).

vi

ijπi = P t

ji = P t
ij

(cid:48)

√

Corollary 1. The expected value of
the largest
eigenvalue of the transformed graph is bounded as
max{davg,
1) ≤ dmax, where davg and
dmax are the average and maximum degrees respec-
tively.

dmax} ≤ E(λ(cid:48)

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 2 4 6 8 10 12 14 16 18 20 22Jenson-Shannon Distance  wrt Original GraphTransient Random Walk Lengtht=2t=3t=4t=5t=10 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 2 4 6 8 10 12 14 16 18 20 22Jenson-Shannon Distance  wrt Original GraphTransient Random Walk Lengtht=2t=3t=4t=5t=10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 2 4 6 8 10 12 14 16 18 20 22Hellinger Distance  wrt Original GraphTransient Random Walk Lengtht=2t=3t=4t=5t=10 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 2 4 6 8 10 12 14 16 18 20 22Hellinger Distance  wrt Original GraphTransient Random Walk Lengtht=2t=3t=4t=5t=10(a)

(b)

Figure 5. Degree distribution of nodes using (a) Facebook interaction graph (b) Facebook link graph.
We can see that the expected degree of each node after the perturbation process remains the same
as in the original graph.

(a)

(b)

Figure 6. Total variation distance as a function of random walk length using (a) Facebook interaction graph
(b) Facebook link graph. We can see that increasing the perturbation parameter of our algorithm
reduces the mixing time of the graph.

From the Perron-Frobenius theorem, we have that
the largest eigenvalue of the graph is related to the
notion of average graph degree as follows,
1 ≤ d(cid:48)

avg,(cid:112)d(cid:48)

max{d(cid:48)

max} ≤ λ(cid:48)

max .

Taking expectation on the above equation, and using
the previous theorem yields the corollary.

Next, we show experimental results validating our
theorem. Figure 5 depicts the node degrees of the origi-
nal graphs, and expected node degrees of the perturbed
graphs, corresponding to all nodes in the Facebook
interaction and friendship graphs. In this ﬁgure, the
points in a vertical line for diﬀerent perturbed graphs
correspond to the same node index. We can see that
the degree distributions are nearly identical, validating
our theoretical results.

Theorem 4. Using our perturbation algorithm, the
mixing time of the perturbed graphs is related to the
mixing time of the original graph as follows: τG()
E(τG(cid:48)()) ≤ τG().

t ≤

Theorem 4 bounds the mixing time of the perturbed
graph using the mixing time of the original graph and

the perturbation parameter t. We defer the proof to
the Appendix. Finally, we compute the mixing time of
the original and perturbed graphs using simulations.
Figure 6 depicts the total variation distance between
random walks of length x and the stationary distri-
bution, for the original and perturbed graphs.1 We
can see that as the perturbation parameter increases,
the total variation distance (and the mixing time) de-
creases. Moreover, for small values of the perturbation
parameter, the diﬀerence from the original topology is
small. As an aside, it is interesting to note that the
variation distance for the Facebook friendship graph is
orders of magnitude smaller that the Facebook inter-
action graph. This is because the Facebook interaction
graphs are a lot sparser, resulting in slow mixing.

6 Privacy

We now address the question of understanding link
privacy of our perturbation algorithm. We use sev-

1Variation distance has a slight oscillating behavior at odd
and even steps of the random walk; this phenomenon is also
observed in Figure 8.

 1 10 100 1000 0 10000 20000 30000 40000Node DegreeNode indexOriginalt=5t=10 1 10 100 1000 10000 0 20000 40000 60000Node DegreeNode indexOriginalt=5t=10 0.001 0.01 0.1 1 0 5 10 15 20 25 30 35 40 45 50Variation DistanceRandom Walk Lengtht=Originalt=3t=5t=15t=20 1e-06 1e-05 0.0001 0.001 0.01 0.1 1 0 5 10 15 20 25 30 35 40 45 50Variation DistanceRandom Walk Lengtht=Originalt=3t=5t=15t=20eral notions for quantifying link privacy, which fall
into two categories (a) quantifying exact probabilities
of de-anonymizing a link given speciﬁc adversarial pri-
ors, and (b) quantifying risk of de-anonymizing a link
without making speciﬁc assumptions about adversarial
priors. We also characterize the relationship between
utility and privacy of a perturbed graph.

6.1 Bayesian formulation for link privacy

Deﬁnition 3 (Link privacy). Let G be the original
graph and L be a link (edge) belonging to G. Let G(cid:48)
be the perturbed graph, as computed by our algorithm.
The adversary is given G(cid:48) and some prior (auxiliary)
information H. The privacy of link L is the probability
Pr[L ∈ G|G(cid:48), H] . Namely, we deﬁne the privacy of a
link L (or a subgraph) as the probability of existence
of the link (or a subgraph) in the original graph G un-
der the assumption that the adversary has access to the
perturbed graph G(cid:48) and prior information H. We write
this probability as Pr[L|G(cid:48), H].

Note that low values of link probability Pr[L|G(cid:48), H]
correspond to high privacy. We cast the problem of
computing the link probability as a Bayesian inference
problem. Using Bayes theorem, we have that:
Pr[G(cid:48)|L, H] · Pr[L|H]

Pr[L|G(cid:48), H] =

Pr[G(cid:48)|H]

.

In the above expression, Pr[L|H] is the prior prob-
ability of the link. In Bayesian inference, Pr[G(cid:48)|H] is
a normalization constant that is typically diﬃcult to
compute, but this is not an impediment for the analysis
since sampling techniques can be used (as long as the
numerator of the Bayesian formulation is computable
upto a constant factor [18, 10]). Our key theoretical
challenge is to compute Pr[G(cid:48)|L, H].
To compute Pr[G(cid:48)|L, H], the adversary has to con-
sider all possible graphs Gp, which have the link L, and
are consistent with background information H. Thus,
we have that:

(cid:88)

Pr[G(cid:48)|L, H] =

Pr[G(cid:48)|Gp] · Pr[Gp|L, H] .

(2)

Gp

The adversary can compute Pr[G(cid:48)|Gp] using the
knowledge of the perturbation algorithm; we assume
that the adversary knows the full details of our pertur-
bation algorithm, including the perturbation parame-
ter t. Observe that given Gp, edges in G(cid:48) can be mod-
eled as samples from the probability distribution of t
hop random walks from vertices in Gp. Thus we can

Using G− L as the adversarial prior constraints the
set of possible Gp to a polynomial number. However,
even in this setting, we found that the above compu-
tation is computationally expensive (> O(n3)) using
our real world social networks. Thus to get an un-
derstanding of link privacy in this setting,we gener-
ated a 500 node synthetic scale-free topology using the
preferential attachment methodology of Nagaraja [25].
The parameters of the scale free topology was set us-
ing the average degree in the Facebook interaction

Figure 7. Cumulative distribution of link probabil-
ity Pr[L|G(cid:48), H] (x-axis is logscale) under worst
case prior H = G − L using a synthetic scale
free topology. Small probabilities offer higher
privacy protection.

compute Pr[G(cid:48)|Gp] by using the t hop transition prob-
abilities of vertices in Gp as:

(cid:19)2m ·

(cid:18) 1

2

(cid:18)2m
(cid:19)

m(cid:48)

· (cid:89)

(i,j)∈E(G(cid:48))

P t
ij(Gp) + P t

ji(Gp)

2

.

In general, the number of possible graphs Gp that
have L as a link and are consistent with the adversary’s
background information can be very large, and the
computation in Equation 2 then becomes intractable.
For evaluation, we consider a special case of this deﬁ-
nition: the adversary’s prior is the entire original graph
without the link L (which is the link for which we want
to quantify privacy). Observe that this is a very pow-
erful adversarial prior; we use this prior to shed light
on the worst-case link privacy using our perturbation
algorithm. Under this prior, we have that:

Pr[G(cid:48)|L, G − L] · Pr[L|G − L]

Pr[L|G(cid:48), G − L] =

=

=

Pr[G(cid:48)|G − L]
Pr[G(cid:48)|G] · Pr[L|G − L]

Pr[G(cid:48)|G − L]

Pr[G(cid:48)|G] · Pr[L|G − L]
l Pr[G(cid:48)|G − L + l]

(cid:80)

.

(3)

 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 0.0001 0.001 0.01 0.1 1CDFLink Probabilityt=2t=3t=4t=5t=10graph. Figure 7 depicts the cumulative distribution for
link probability (probability of de-anonymizing a link,
Pr[L|G(cid:48), H]) in this setting (worst case prior) for the
synthetic scale-free topology. We can see that there is
signiﬁcant variance in the privacy protection received
by links in the topology:
for example, using pertur-
bation parameter t = 2, 40% of the links have a link
probability less than 0.1 (small is better), while 30% of
the links have have a probability of 1 (can be fully de-
anonymized). Note that this is a worst case analysis,
since we assume that an attacker knows the entire orig-
inal graph except the link in question. Furthermore,
even in this setting, as the perturbation parameter t
increases, the privacy protection received by links sub-
stantially improves: for example, using t = 5, all of the
links have a link probability less than 0.01, while 70%
of the links have a link probability of less than 0.001.
Thus we can see that even in this worst case analysis,
our perturbation mechanism oﬀers good privacy pro-
tection.

We now compare with the work of Hay et al [12],
which proposes a perturbation approach where k real
edges are deleted and k fake edges are introduced at
random. Even considering k = m/2 (which would sub-
stantially hurt utility), 50% of the edges in the per-
turbed graph are real edges between users; for these
edges, Pr[L|G(cid:48)] = 0.5. Thus we can see that our pertur-
bation mechanism signiﬁcantly improves privacy com-
pared with the work of Hay et al.

6.2 Relationship between privacy and utility

Intuitively, there is a relationship between link pri-
vacy and the utility of the perturbed graphs. Next, we
formally quantify this relationship.

Theorem 5. Let the maximum vertex utility of the
graph (over all vertices) corresponding to an applica-
tion parameter l be VUmax(G, G(cid:48), l). Then for any two
vertices A and B, we have that Pr[LAB|G(cid:48)] ≥ f (δ),
where f (δ) denotes the prior probability of two vertices
being friends given that they are both contained in a δ
hop neighborhood, and δ is computed as δ = min{k :
AB(G(cid:48)) − VUmax(G, G(cid:48), k) > 0}.
P k
Utility measures the change in graph structure be-
tween original and perturbed graphs. If this change is
small (high utility), then an adversary can infer infor-
mation about the original graph given the perturbed
graph. For a given level of utility, the above theorem
demonstrates a lower bound on link privacy.

We defer the proof of the above theorem to the Ap-
pendix. The above theorem is a general theorem that
holds for all perturbed graphs. To shed some intuition,

we speciﬁcally analyze the lower bounds on privacy for
our perturbation algorithm (where parameter t gov-
erns utility), using the transition probability between
A and B in the perturbed graph G(cid:48) as a feature to as-
sign probabilities to nodes A and B of being friends in
the original graph G. We are interested in the quan-
tity Pr[LAB | P k
AB(G(cid:48)) > x], for diﬀerent values of
k. Using Bayes’ theorem, we have that the probability
Pr[LAB | P k

AB(G(cid:48)) > x] can be written as
AB(G(cid:48)) > x | LAB] · Pr[LAB]
Pr[P k

.

(4)

Pr[P k

AB(G(cid:48)) > x]

Moreover, we have that

Pr[P k

AB(G(cid:48)) > x] = Pr[P k
+ Pr[P k

AB(G(cid:48)) > x | LAB] · Pr[LAB]
AB(G(cid:48)) > x | LAB] · Pr[LAB] ,

where LAB denotes the event when vertices A and B
don’t have a link.

AB(G(cid:48))|LAB] and Pr[P k

To get some insight, we computed the probability
AB(G(cid:48))|LAB]
distributions Pr[P k
using simulations on our real world social network
topologies. Figure 8 depicts the median value of the
respective probability distributions, as a function of
parameter k, for diﬀerent perturbation parameters t,
using the Facebook interaction graph. We can see that
the median transition probabilities are higher for the
scenario where two users are originally friends (as op-
posed to the setting where they are not friends). We
can also see that the diﬀerence between median transi-
tion values in the two scenarios is higher when (a) the
perturbation parameter t is small, and (b) the parame-
ter K (random walk length) is small. This diﬀerence is
related to the privacy of the link—the larger the diﬀer-
ence, the greater the loss in privacy. The insight from
this ﬁgure is that for small perturbation parameters,
the closer two nodes are to each other in the perturbed
graph, the higher their chances of being friends in the
original graph.

Next, we consider the full distribution of transition
probabilities (as opposed to only the median values dis-
cussed above). Figure 9(a) depicts the complimentary
AB(G(cid:48)) > x) using pertur-
cumulative distribution (P k
bation parameter t = 2 for the Facebook interaction
graph. Again, we can see that when A and B are
friends, they have higher transition probability to each
other in the perturbed graph, compared to the scenario
when A and B are not friends. Moreover, as the value
of k increases, the gap between the distributions be-
comes smaller. Similarly, as the value of t increases,
the gap between the distributions becomes smaller, as
depicted in Figures 9(b-c). We can use these simula-
tion results to compute the probabilities in Equation 4.

(a)

(b)

Figure 8. Median transition probability between two vertices in transformed graph when (a) two vertices
were neighbors (friends) in the original graph and (b) two vertices were not neighbors in the original
graph, for the Facebook interaction graph.

(a)

AB(G(cid:48)) using (a) perturbation parameter t = 2, and
Figure 9. Complimentary cumulative distribution for P k
(b) perturbation parameter t = 5, and (c) perturbation parameter t = 10 for the Facebook interaction
graph.

(b)

(b)

probability of less than 0.1.
Increasing perturbation
parameter t signiﬁcantly improves performance; using
t = 3, 95% of the links have a link probability less than
0.1, and 90% of the links have a link probability less
than 0.01. In summary, our analysis shows that a given
level of utility translates into a lowerbound on privacy
oﬀered by the perturbation mechanism.

6.3 Risk based formulation for link privacy

The dependence of the Bayesian inference based pri-
vacy deﬁnitions on the prior of the adversary motivates
the formulation of new metrics that are not speciﬁc to
adversarial priors. We ﬁrst illustrate a preliminary def-
inition of link privacy (which is unable to account for
links to degree 1 vertices), and then subsequently im-
prove it.

Deﬁnition 4 (-SI link privacy). We deﬁne the struc-
tural impact (SI) of a link L in graph G with respect to a
perturbation mechanism M, as the statistical distance
between probability distributions of the output M(G)
of the perturbation mechanism (i.e., the set of possible
perturbed graphs) when using (a) the original graph G
as an input to the perturbation mechanism, and (b) the
graph G−L as an input to the perturbation mechanism.

Figure 10. Cumulative distribution of link proba-
bility Pr[L|G(cid:48), H] for the Facebook interaction
graph. Smaller probabilities offer higher pri-
vacy protection.

One way to analyzing the lower bound on link privacy
would be to choose a uniform prior for vertices being
friends in the original graph, i.e., Pr[LAB] = m
. Cor-
(n
2)
respondingly, Pr[LAB] = 1 − Pr[LAB].

Figure 10 depicts the cumulative distribution of link
probability computed using the above methodology,
for the Facebook interaction graph. We can see the
variance in privacy protection received by links in the
topology. Using t = 2, 80% of the links have a link

 1e-07 1e-06 1e-05 0.0001 0.001 2 4 6 8 10 12 14 16 18 20Median Transition ProbabilityTransient Random Walk LengthFriends, t=2Friends, t=3Friends, t=4Friends, t=5Friends, t=10 1e-07 1e-06 1e-05 0.0001 0.001 2 4 6 8 10 12 14 16 18 20Median Transition ProbabilityTransient Random Walk LengthRandom, t=2Random, t=3Random, t=4Random, t=5Random, t=10 1e-05 0.0001 0.001 0.01 0.1 1 1e-05 0.0001 0.001 0.01 0.1 1Complimentary CDFTransition Probabilityk=1, friendsk=1, not friendsk=2, friendsk=2, not friends 1e-05 0.0001 0.001 0.01 0.1 1 1e-05 0.0001 0.001 0.01 0.1 1Complimentary CDFTransition Probabilityk=1, friendsk=1, not friendsk=2, friendsk=2, not friends 1e-05 0.0001 0.001 0.01 0.1 1e-05 0.0001 0.001 0.01 0.1 1Complimentary CDFTransition Probabilityk=1, friendsk=1, not friendsk=2, friendsk=2, not friends 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 0.0001 0.001 0.01 0.1 1CDFLink Probabilityt=2t=3t=4t=5t=10(a)

(b)

Figure 11. Cumulative distribution of -SI link privacy for (a) Facebook interaction graph (b) Facebook
link graph. Note that the SI privacy deﬁnition is not applicable to links for which either of the vertex
has degree of 1, since removing that link disconnects the graph.

Figure 12. Cumulative distribution of K-anonymity set size for  = 0.1 SE link privacy using (a) Facebook
interaction graph (b) Facebook link graph.

(a)

(b)

Let Pr[G(cid:48) = M(G)] denote the probability distribution
of perturbed graphs G(cid:48) using the perturbation mecha-
nism M and input graph G. A link has -SI privacy if
the statistical distance

|| Pr[G(cid:48) = M(G)] − Pr[G(cid:48) = M(G − L)]|| <  .

Intuitively, if the SI privacy of a link is high, then
the perturbation process leaks more information about
that link. On the other hand, if the SI privacy of a link
is low, then the perturbed graph G(cid:48) leaks less informa-
tion about the link.

As before, we consider the total variation distance as
our distance metric between probability distributions.
Observe that the links in graphs G(cid:48) are samples from
v(G), for v ∈ V . So we
the probability distribution P t
can bound the diﬀerence in probability distributions of
perturbed graphs generated from G and G − L, by the
worst case total variation distance between probability
v(G− L), over all v ∈ V , i.e.,
distributions P t
by VUmax(G, G − L, t).

v(G) and P t

Note that our preliminary attempt at deﬁning link
privacy above does not accommodate links where ei-
ther of its endpoints have degree 1, since removal of
that link disconnects the graph. This is illustrated in
Figure 11, which depicts the cumulative distribution
of -SI link privacy values. We can see that approx-

imately 30% and 20% of links in the Facebook inter-
action graph and friendship graphs respectively do not
receive any privacy protection under this deﬁnition (be-
cause they are connected to degree 1 vertices). For
the remaining links, we can see a similar qualitative
trend as before: increasing the perturbation parameter
t signiﬁcantly improves link privacy. To overcome the
limitations of this deﬁnition, we propose an alternate
formulation based on the notion of link equivalence.

Deﬁnition 5 (K-anonymous -SE link privacy). We
deﬁne the structural equivalence (SE) between a link
L(cid:48) with a link L in graph G with respect to a perturba-
tion mechanism M, as the statistical distance between
probability distributions of the output of the perturba-
tion mechanism, when using (a) the original graph G
as an input to the perturbation mechanism, and (b) the
graph G− L + L(cid:48) as the input to the perturbation mech-
anism. A link L has K-anonymous -SE privacy, if
there exist at least K links L(cid:48), such that
|| Pr[G(cid:48) = M(G)] − Pr[G(cid:48) = M(G − L + L(cid:48))]|| <  .

Observe that this deﬁnition of privacy is able to ac-
count for degree 1 vertices, since they can become con-
nected to the graph via the addition of other alternate
links L(cid:48). For our experiments, we limited the number

 0 0.2 0.4 0.6 0.8 1 0.0001 0.001 0.01 0.1 1CDFε SI link privacyt=2t=4t=6t=10 0 0.2 0.4 0.6 0.8 1 0.0001 0.001 0.01 0.1 1CDFε SI link privacyt=2t=4t=6t=10 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 500 600 700 800 900 1000CDF K anonymity (0.1  SE link privacy)t=2t=4t=6t=10 0 0.2 0.4 0.6 0.8 1 0 100 200 300 400 500 600 700 800 900 1000CDF K anonymity (0.1  SE link privacy)t=2t=4t=6t=10of alternate links explored to 1000 links (for compu-
tational tractability). Figure 12 depicts the cumula-
tive distribution of anonymity set sizes for links using
 = 0.1 for the Facebook interaction and friendship
graphs. For t = 2 we see a very similar trend as in
the previous deﬁnition, where a non-trivial fraction of
links do not receive much privacy. Unlike the previous
setting however, as we increase the perturbation pa-
rameter t, the anonymity set size for even these links
improves signiﬁcantly. Using t = 10, 50% and 70%
of the links in the interaction and friendship graphs
respectively, achieved the maximum tested anonymity
set size of 1000 links.

6.4 Relation to differential privacy

There is an interesting relation between our risk-
based privacy deﬁnitions and diﬀerential privacy [9]. A
diﬀerentially-private mechanism adaptively adds noise
to the system to ensure that all user records in a
database (links in our setting) receive a threshold pri-
vacy protection.
In our mechanism, we are adding
a ﬁxed amount of noise (governed by the perturba-
tion parameter t), and observing the variance in  and
anonymity set size values.

7 Applications

In this section, we demonstrate the applicability of
our perturbation mechanism to social network based
systems.

Table 1. Path reliability using Sprout for a lin-
ear trust decay model.

Reliability

Mechanism

Reliability

Original

Facebook interaction graph Facebook friendship graph
Mechanism
Original
t = 3
t = 5
t = 10
Chord

0.110
0.101
0.101
0.096
0.075

0.140
0.126
0.121
0.118
0.072

t = 3
t = 5
t = 10
Chord

decrement is bounded by a value reﬂecting the prob-
ability of a random user in the network being trusted
(set to 0.6 in [17]).

The reliability of a DHT lookup in Sprout is deﬁned
as the probability of all users in the path being trusted.
Table 1 depicts the reliability of routing using a single
DHT lookup in Sprout, for the original and the per-
turbed topologies. We used Chord as the underlying
DHT system. For each perturbation parameter, our re-
sults were averaged over 100 perturbed topologies. We
can see that as the perturbation parameter increases,
the utility of application decreases. For example, using
the original Facebook interaction topology, the relia-
bility of a single DHT path in sprout is 0.11, which
drops to 0.10 and 0.096 when using perturbed topolo-
gies with parameters t = 5 and t = 10 respectively.
However, even when using t = 10, the performance is
better as compared with the scenario where social links
are not used for routing (Chord’s baseline performance
of 0.075). We can see similar results for the Facebook
friendship graph as well.

7.1 Secure routing

7.2 Sybil detection

Several peer-to-peer systems perform routing over
social graph to improve performance and security,
such as Sprout [17], Tribler [27], Whanau [14] and X-
Vine [21]. Next, we analyze the impact of our pertur-
bation algorithm on the utility of Sprout.

7.1.1 Sprout

Sprout is a routing system that enhances the security
of conventional DHT routing by leveraging trusted so-
cial links when available. For example, when routing
towards a DHT key, if leveraging a social link makes
forward progress in the DHT namespace, then the so-
cial link is used for routing. The authors of Sprout
considered a linear trust decay model, where a user’s
social contacts are trusted with probability f (set to
0.95 in [17]), and the trust in other users decreases as
a linear function of the shortest path distance between
the users (a decrement of 0.05 is used in [17]). The

In a Sybil attack [8], a single user or an entity em-
ulates the behavior of multiple identities in a system.
Many systems are built on the assumption that there
is a bound on the fraction of malicious nodes in the
systems. By being able to insert a large number of
malicious identities, an attacker can compromise the
security properties of such systems. Sybil attacks are a
powerful threat against both centralized as well as dis-
tributed systems, such as reputation systems, consen-
sus and leader election protocols, peer-to-peer systems,
anonymity systems, and recommendation systems.

A wide body of recent work has proposed to leverage
trust relationships embedded in social networks for de-
tecting Sybil attacks [36, 35, 6, 23, 31]. However, in all
of these mechanisms, an adversary can learn the trust
relationships in the social network. Next, we show
that our perturbation algorithm preserves the ability
of above mechanisms to detect Sybils, while protecting
the privacy of the social network trust relationships.

(a)

(b)

Figure 13. SybilLimit % validated honest nodes as a function of SybilLimit random route length for (a)
Facebook interaction graph (b) Facebook link graph. We can see that for a false positive rate of
1 − 2%, the required random route length for our perturbed topologies is a factor of 2 − 3 smaller
as compared with the original topology. Random route length is directly proportional to number of
Sybil identities that can be inserted in the system.

(a)

(b)

Figure 14. Attack edges in perturbed topologies as a function of attack edges in the original topology. We
can see that there is a marginal increase in the number of attack edges in the perturbed topologies.
The attack edges are directly proportional to the number of Sybil identities that can be inserted in
the system.

(a)

(b)

Figure 15. Number of Sybil identities accepted by SybilInfer as a function of number of compromised
nodes in the original topology. We can see that there is a signiﬁcant decline in the number of Sybil
identities using our perturbation algorithms.

 0 10 20 30 40 50 60 70 80 90 100 2 4 6 8 10 12 14 16 18 20% Accepted nodesRandom Walk LengthOriginalt=5t=10 0 10 20 30 40 50 60 70 80 90 100 2 4 6 8 10 12 14 16 18 20% Accepted nodesRandom Walk LengthOriginalt=5t=10 0 500 1000 1500 2000 2500 3000 3500 4000 4500 0 500 1000 1500 2000 2500 3000 3500 4000Final Attack EdgesAttack EdgesOriginalt=5t=10 0 1000 2000 3000 4000 5000 6000 7000 0 1000 2000 3000 4000 5000 6000Final Attack EdgesAttack EdgesOriginalt=5t=10 0 5000 10000 15000 20000 25000 30000 0 500 1000 1500 2000Number of Sybil identitiesNumber of compromised nodesOriginalt=5t=10 0 5000 10000 15000 20000 25000 30000 0 500 1000 1500 2000Number of Sybil identitiesNumber of compromised nodesOriginalt=5t=107.2.1 SybilLimit

7.2.2 SybilInfer

We use SybilLimit [35] as a representative protocol for
Sybil detection, since it is the most popular and well
understood mechanism in the literature. SybilLimit
is a probabilistic defense, and has both false positives
(honest users misclassiﬁed as Sybils) and false nega-
tives (Sybils misclassiﬁed) as honest users.

We compared the performance of running Sybil-
Limit on original graph, and on the transformed graph,
for varying perturbation parameters. For each pertur-
bation parameter, we averaged the results over 100 per-
turbed topologies. Figure 13 depicts the percentage of
honest users validated by SybilLimit using the original
graph and perturbed graphs, as a function of the Sybil-
Limit random route length (application parameter w).
For any value of the SybilLimit random route length,
the percentage of honest nodes accepted by SybilLimit
is higher when using perturbed graphs. This is because
our perturbation algorithms reduce the mixing time of
the graph.
In fact, for a false positive percentage of
1-2% (99-98% accepted nodes), the required length of
the SybilLimit random routes is a factor of 2-3 smaller
as compared to the original topology. SybilLimit ran-
dom routes are directly proportional to the number of
Sybil identities that can be inserted in the system.

This improvement in the number of accepted honest
nodes (reduction in false positives) comes at the cost of
increase in the number of attack edges between honest
users and the attacker. Figure 14 depicts the number
of attack edges in the perturbed topologies, for varying
values of attack edges in the original graph. We can
see that as expected, there is a marginal increase in the
number of attack edges in the perturbed topologies.

Remark: The number of Sybil identities that an ad-
versary can insert is given by S = g(cid:48) · w(cid:48). We note that
the marginal increase in the number of attack edges g(cid:48)
is oﬀset by the reduced length of the SybilLimit ran-
dom route parameter w (for any desired false positive
rate), thus achieving comparable performance with the
original social graph. In fact, for perturbed topologies,
since the required random route length in SybilLimit is
halved for a false positive rate of 1-2%, and the increase
in the number of attack edges is less than a factor of
two, the Sybil defense performance has improved using
our perturbation mechanism. Thus for Sybil defenses,
our perturbation mechanism is of independent inter-
est, even without considering the beneﬁt of link pri-
vacy. We further validate this conclusion using another
state-of-art detection mechanism called SybilInfer.

We compared the performance of running SybilInfer
on real and perturbed topologies. Figure 15 depicts
the optimal number of Sybil identities that an adver-
sary can insert before being detected by SybilInfer, as
a function of real compromised and colluding users.
Again, we can see that the performance of perturbed
graphs is better than using original graphs. This is
due to the interplay between mixing time of graphs
and the number of attack edges in the Sybil defense
application. Our perturbation mechanism signiﬁcantly
reduces the mixing time of the graphs, while suﬀering
only a marginal increase in the number of attack edges.
It is interesting to see that the advantage of using our
perturbation mechanism is less in Figure 15(b), as com-
pared to Figure 15(a). This is because the mixing time
of the Facebook friendship graph is much better (as
compared with the the mixing time of the Facebook
interaction graph). Thus we conclude that our pertur-
bation mechanism improves the overall Sybil detection
performance of existing approaches, especially for in-
teraction based topologies that exhibit relatively poor
mixing characteristics.

8 Conclusion and Future Work

In this work, we proposed a random walk based
perturbation algorithm that anonymizes links in a so-
cial graph while preserving the graph theoretic prop-
erties of the original graph. We provided formal deﬁ-
nitions for utility of a perturbed graph from the per-
spective of vertices, related our deﬁnitions to global
graph properties, and empirically analyzed the prop-
erties of our perturbation algorithm using real world
social networks. Furthermore, we analyzed the privacy
of our perturbation mechanism from several perspec-
tives (a) a Bayesian viewpoint that takes into consid-
eration speciﬁc adversarial prior, and (b) a risk based
view point that is independent of the adversary’s prior.
We also formalized the relationship between utility and
privacy of perturbed graphs. Finally, we experimen-
tally demonstrated the applicability of our techniques
on applications such as Sybil defenses and secure rout-
ing. For Sybil defenses, we found that our techniques
are of independent interest.

Our work opens several directions for future re-
search, including (a) investigating the applicability of
our techniques on directed graphs (b) modeling closed
form expressions for computing link privacy using the
Bayesian framework (c) investigating tighter bounds on
 for computing link privacy in the risk-based frame-
work, and (d) modeling temporal dynamics of social

networks in quantifying link privacy.

[9] C. Dwork. Diﬀerential privacy: a survey of results. In

By protecting the privacy of trust relationships, we
believe that our perturbation mechanism can act as a
key enabler for real world deployment of secure systems
that leverage social links.

Acknowledgments

We are very grateful to Satish Rao for helpful discus-
sions on graph theory, including insights on utility met-
rics for perturbed graphs, and relating mixing times
of original and perturbed graphs. We would also like
to thank Ling Huang, Adrian Mettler, Nguyen Tran,
and Mario Frank for helpful discussions on analyzing
link privacy, as well as their feedback on early versions
of this work. Our analysis of SybilLimit is based on
extending a simulator written by Bimal Vishwanath.
This work was supported by Intel through the ISTC
for Secure Computing, by the National Science Foun-
dation under grant numbers CCF-0424422, 0842695
and 0831501 CT-L, by the Air Force Oﬃce of Scientiﬁc
Research (AFOSR) under MURI award FA9550-09-1-
0539 and grant number FA9550-08-1-0352 and by the
Oﬃce of Naval Research under MURI grant number
N000140911081.

References

[1] D.

Aldous

Chains

Markov
Graphs.
dous/RWG/book.html.

and

J.
and

Fill.
Random Walks
http://www.stat.berkeley.edu/

Reversible
on
al-

[2] L. Backstrom, C. Dwork, and J. Kleinberg. Wherefore
art thou r3579x?: anonymized social networks, hidden
patterns, and structural steganography.
In WWW,
2007.

[3] M. Bastian, S. Heymann, and M. Jacomy. Gephi: An
open source software for exploring and manipulating
networks. In International AAAI Conference on We-
blogs and Social Media, 2009.

[4] S.-H. Cha.

Comprehensive

dis-
tance/similarity measures
probability
density functions.
International Journal of Math-
ematical Models and Methods in Applied Sciences,
1(4), 2007.

between

survey

of

[5] G. Danezis, C. Diaz, C. Troncoso, and B. Laurie. Drac:
an architecture for anonymous low-volume communi-
cations. In PETS, 2010.

[6] G. Danezis and P. Mittal. Sybilinfer: Detecting Sybil

nodes using social networks. In NDSS, 2009.

[7] R. Dey, Z. Jelveh, and K. Ross. Facebook users have
become much more private: a large scale study. Tech-
nical report, New York University, 2011.

[8] J. Douceur. The Sybil Attack. In IPTPS, 2002.

TAMC, 2008.

[10] W. K. Hastings. Monte carlo sampling methods us-
ing markov chains and their applications. Biometrika,
57(1), 1970.

[11] M. Hay, G. Miklau, D. Jensen, D. Towsley, and
P. Weis.
Resisting structural re-identiﬁcation in
anonymized social networks. Proc. VLDB Endow.,
1(1), 2008.

[12] M. Hay, G. Miklau, D. Jensen, P. Weis, and S. Srivas-
tava. Anonymizing social networks. Technical Report
07-09, University of Massachusetts, Amherst, 2007.

[13] A. Korolova, R. Motwani, S. U. Nabar, and Y. Xu.

Link privacy in social networks. In CIKM, 2008.

[14] C. Lesniewski-Laas and M. F. Kaashoek. Whanaun-
In

gatanga: A Sybil-proof distributed hash table.
NSDI, 2010.

[15] K. Liu and E. Terzi. Towards identity anonymization

on graphs. In SIGMOD, 2008.

[16] X. Lu, Y. Song, and S. Bressan.

Fast identity

anonymization on graphs. In DEXA, 2012.

[17] S. Marti, P. Ganesan,

SPROUT: P2P routing with social networks.
P2P&DB, 2004.

and H. Garcia-Molina.
In

[18] N. Metropolis, A. W. Rosenbluth, M. N. Rosenbluth,
A. H. Teller, and E. Teller. Equation of state calcu-
lations by fast computing machines. The Journal of
Chemical Physics, 21(6), 1953.

[19] A. Mislove, A. Post, P. Druschel, and K. P. Gummadi.
Ostra: leveraging trust to thwart unwanted communi-
cation. In NSDI, 2008.

[20] P. Mittal, N. Borisov, C. Troncoso, and A. Rial. Scal-
able anonymous communication with provable secu-
rity. In USENIX HotSec, 2010.

[21] P. Mittal, M. Caesar, and N. Borisov. X-vine: Secure
and pseudonymous routing using social networks. In
NDSS, 2012.

[22] P. Mittal, M. Wright, and N. Borisov. Pisces: Anony-
mous communication using social networks. In NDSS,
2013.

[23] A. Mohaisen, N. Hopper, and Y. Kim. Keep your
friends close: Incorporating trust into social network-
based sybil defenses. In INFOCOM, 2011.

[24] A. Mohaisen, A. Yun, and Y. Kim. Measuring the

mixing time of social graphs. In IMC, 2010.

[25] S. Nagaraja. Anonymity in the wild: mixes on un-

structured networks. In PETS, 2007.

[26] A. Narayanan and V. Shmatikov. De-anonymizing so-

cial networks. In IEEE SSP, 2009.

[27] J. A. Pouwelse, P. Garbacki, J. Wang, A. Bakker,
J. Yang, A. Iosup, D. H. J. Epema, M. Reinders, M. R.
van Steen, and H. J. Sips. Tribler: a social-based peer-
to-peer system: Research articles. Concurr. Comput.
: Pract. Exper., 20(2), 2008.

[28] V. Rastogi, M. Hay, G. Miklau, and D. Suciu. Rela-
tionship privacy: output perturbation for queries with
joins. In PODS, 2009.

[29] A. Sala, X. Zhao, C. Wilson, H. Zheng, and B. Y.
Zhao. Sharing graphs using diﬀerentially private graph
models. In IMC, 2011.

[30] Y. Sovran, J. Li, and L. Subramanian. Unblocking
the internet: Social networks foil censors. Technical
report, NYU, 2008.

[31] N. Tran, J. Li, L. Subramanian, and S. Chow. Optimal
sybil-resilient node admission control. In INFOCOM,
2011.

[32] N. Tran, B. Min, J. Li, and L. Subramanian. Sybil-

resilient online content voting. In NSDI, 2009.

[33] B. Viswanath, A. Mislove, M. Cha, and K. P. Gum-
madi. On the evolution of user interaction in Face-
book. In WOSN, 2009.

[34] X. Ying and X. Wu. Randomizing social networks: a

spectrum preserving approach. In SDM, 2008.

[35] H. Yu, P. B. Gibbons, M. Kaminsky, and F. Xiao.
Sybillimit: A near-optimal social network defense
against Sybil attacks. In IEEE S&P, 2008.

[36] H. Yu, M. Kaminsky, P. Gibbons, and A. Flaxman.
SybilGuard: Defending against Sybil attacks via social
networks. In SIGCOMM, 2006.

[37] E. Zheleva and L. Getoor. Preserving the privacy
of sensitive relationships in graph data. In PinKDD,
2008.

[38] B. Zhou and J. Pei. Preserving privacy in social net-
works against neighborhood attacks. In ICDE, 2008.
[39] B. Zhou and J. Pei. The k-anonymity and l-diversity
approaches for privacy preservation in social networks
against neighborhood attacks. Knowl. Inf. Syst.,
28(1), 2011.

B Proof of Theorem 2: Relating vertex

utility and SLEM.

We sketch the proof of the theorem. It is known that
for undirected graphs, the second largest eigenvalue
modulus is related to the mixing time of the graph
as follows [1]:

(cid:18) 1

(cid:19)

2

≤ τG(cid:48)() ≤ log n + log(cid:0) 1



(cid:1)

1 − µG(cid:48)

.

µG(cid:48)

2(1 − µG(cid:48))

log

From the above equation, we can bound the SLEM in
terms of the mixing time as follows:

1 − log n + log( 1
 )

τG(cid:48)()

≤ µG(cid:48) ≤

2τG(cid:48)()

2τG(cid:48)() + log( 1

2 )

.

Set χ = VUmax(G, G(cid:48), τG()). Replacing  with  + χ,
we have that

1− log n + log( 1
τG(cid:48)( + χ)

+χ )

≤ µG(cid:48) ≤

2τG(cid:48)( + χ)
2τG(cid:48)( + χ) + log(

2+2χ )
Finally, from Theorem 1, we leverage τG(cid:48)(+χ) ≤ τG()
in the above equation to get

.

1

µG(cid:48) ≤

2τG()
2τG() + log(

.

1

2+2χ )

A Proof of Theorem 1: Relating vertex

C Proof of Theorem 4: Bounding mix-

utility and mixing time.

ing time.

We now sketch the proof of the above theorem.
From the deﬁnition of total variation distance, we can
see that:
||P t

v(G)−π||tvd .
v(G(cid:48))−π||tvd ≤ ||P t
From the deﬁnition of mixing time, we have that for

v(G)||tvd+||P t

v(G(cid:48))−P t

all t ≥ τG():

||P t

v(G(cid:48)) − π||tvd ≤ ||P t

v(G(cid:48)) − P t

v(G)||tvd +  .

Substituting t = τG() in the above equation, and tak-
ing the maximum over all vertices, we have that:

v

v

max

||P τG()
||P τG()

(G(cid:48)) − π||tvd ≤
(G(cid:48)) − P τG()
max
VUmax(G, G(cid:48), τG()) +  .

v

v

v

(G)||tvd +  ≤

Finally, we have that

τG(cid:48)( + VUmax(G, G(cid:48), τG()) ≤ τG() .

t

Observe that the edges in graph G(cid:48) can be mod-
eled as samples from the t-hop probability distribution
of random walks starting from vertices in G. We will
prove the lower bound on the mixing time of the per-
turbed graph G(cid:48) by contradiction: let us suppose that
the mixing time of the graph G(cid:48) is k < τG()
. Then
in the original graph G, a user could have performed
random walks of length k · t and achieve a variation
distance less than . But k · t < τG(), which is a con-
tradiction. Thus, we have that τG()

t ≤ τG(cid:48)().

We prove an upper bound on mixing time of the per-
turbed graph using the notion of graph conductance.
Let us denote the number of edges across the bottle-
neck cut (say S) of the original topology as g. Observe
that the t-hop conductance between the sets S and S
is strictly larger than the corresponding one hop con-
ductance (since S is the bottleneck cut in the origi-
nal topology). Thus, E(G(cid:48)) ≥ g. Hence the expected
graph conductance is an increasing function of the per-
turbation parameter t, and thus E(τG(cid:48)()) ≤ τG().

D Proof of Theorem 5: Relating utility

and privacy.

From the deﬁnition of maximum vertex utility, we
AB(G(cid:48))| ≤ VUmax(G, G(cid:48), l).
AB(G) as follows:
AB(G) ≤ P l

have that |P l
AB(G) − P l
Thus, we can bound P l
AB(G(cid:48)) − χ ≤ P l
P l

AB(G(cid:48)) + χ ,

where χ = VUmax(G, G(cid:48), l). Thus for any value of k, if
AB(G(cid:48))− VUmax(G, G(cid:48), k) > 0, then we have that the
P k
lower bound on the probability P k
AB(G) > 0, which re-
veals the information that A and B are within an k hop

neighborhood of each other. Thus the maximum infor-
mation is revealed when the value of k is minimized,
AB(G(cid:48))−VUmax(G, G(cid:48), k) > 0, i.e.,
while maintaining P k
k = δ. This gives us a lower bound on the probability
of A and B being friends in the original graph: the
prior probability that two vertices in a δ hop neigh-
borhood are friends: f (δ). Let mδ denote the average
number of links in a δ hop neighborhood, and let nδ
denote the average number of vertices in a δ hop neigh-
borhood. In the special case of a null prior, we have

that f (δ) = mδ/(cid:0)nδ

(cid:1).

2

