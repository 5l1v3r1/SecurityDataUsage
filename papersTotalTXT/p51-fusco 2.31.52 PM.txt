RasterZip: Compressing Network Monitoring Data

with Support for Partial Decompression

Francesco Fusco

ETH Zurich
Switzerland

fusco@tik.ee.ethz.ch

Michail Vlachos

IBM Research - Zurich

Switzerland

mvl@zurich.ibm.com

Xenofontas

Dimitropoulos

ETH Zurich
Switzerland

fontas@tik.ee.ethz.ch

ABSTRACT
Network traﬃc archival solutions are fundamental for a num-
ber of emerging applications that require: a) eﬃcient stor-
age of high-speed streams of traﬃc records and b) support
for interactive exploration of massive datasets. Compres-
sion is a fundamental building block for any traﬃc archival
solution. However, present solutions are tied to general-
purpose compressors, which do not exploit patterns of net-
work traﬃc data and require to decompress a lot of redun-
dant data for high selectivity queries. In this work we in-
troduce RasterZIP, a novel domain-speciﬁc compressor de-
signed for network traﬃc monitoring data. RasterZIP uses
an optimized lossless encoding that exploits patterns of traf-
ﬁc data, like the fact that IP addresses tend to share a com-
mon preﬁx. RasterZIP also introduces a novel decompres-
sion scheme that accelerates highly selective queries target-
ing a small portion of the dataset. With our solution we can
achieve high-speed on-the-ﬂy compression of more than half
a million traﬃc records per second. We compare RasterZIP
with the fastest Lempel-Ziv-based compressor and show that
our solution improves the state-of-the-art both in terms of
compression ratios and query response times without intro-
ducing penalty in any other performance metric.

Categories and Subject Descriptors
D.4.1 [Data]: Coding and Information Theory— Data com-
paction and compression

General Terms
Algorithms

Keywords
Network monitoring, network traﬃc archives, data compres-
sion, NetFlow

 
Permission to make digital or hard copies of all or part of this work for 
personal or classroom use is granted without fee provided that copies are 
not  made  or  distributed  for  profit  or  commercial  advantage  and  that 
copies  bear  this  notice  and  the  full  citation  on  the  first  page.  To  copy 
otherwise,  or  republish,  to  post  on  servers  or  to  redistribute  to  lists, 
requires prior specific permission and/or a fee. 
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA. 
Copyright 2012 ACM  978-1-4503-1705-4/12/11...$15.00. 
 

1.

INTRODUCTION

In many network monitoring tasks, it would be extremely
useful to “travel back in time” and trace the causal nexus
of events that led to important outcomes, like failures or
security breaches. For this reason, an increasing number of
applications need to eﬃciently search massive network traﬃc
archives. For example, when a breach of credit card numbers
is detected, analysts strive to ﬁnd past traﬃc connections
that are linked to the adversarial action to determine which
systems and data were aﬀected. Large-scale traﬃc reposito-
ries are also vital for network operators in validating claims
against Service Level Agreements (SLAs), for scientists in
studying Internet traﬃc, and for law enforcement agencies
in lawful interception. All these applications need to sup-
port very eﬃcient drill-down queries to ﬁnd “needles in a
haystack”.

In the context of long term traﬃc archiving, compres-
sion is an essential enabling technology. It substantially re-
duces the size of an archive, while it directly relates with
the eﬃciency of drill-down queries. Domain-optimized com-
pression technologies have been researched, developed and
sometimes standardized in several information technology
ﬁelds, including, but not limited to bioinformatics [6, 21],
biochemistry, and web information retrieval [26]. Previous
experience shows that substantial gains can be obtained by
designing compressors tailored to the data and requirements
posed by speciﬁc domains.

In contrast, the de-facto approach to reduce the vol-
ume of network traﬃc repositories consists of applying
general-purpose compression utilities, such as gzip, bzip2,
and lzop [12, 17]. However, general-purpose compressors
present limitations when used in the context of large-scale
traﬃc repositories. The main shortcoming is that network
traﬃc data archives exhibit distinct data patterns and pose
speciﬁc requirements, which are not taken into account and
exploited by general-purpose compressors.

In particular, the values stored in a traﬃc archive often
share a common preﬁx (e.g., ﬁrst few bits of IP addresses
and timestamps), typically lie within a particular range (e.g.,
port numbers and protocols), and exhibit a large number of
repetitions within a relatively short time window. A com-
pressor optimized for network traﬃc data should exploit
these patterns to substantially reduce data volumes.

Second, traﬃc data have to be compressed without in-
troducing severe performance penalties when performing
drill-down operations. Therefore, supporting eﬃcient data
retrieval from large-scale compressed repositories is neces-
sary. Traﬃc archival solutions should provide mechanisms

51enabling selective, ﬁne-grained and high-speed data decom-
pression operations that minimize the amount of redundant
data to be decompressed, especially in the case of drill-down
operations targeting a small subset of the collected data.
In contrast, general purpose compressors are optimized for
compressing data in large blocks, which need to be retrieved,
and fully decompressed, even for queries targeting a small
part of the datasets.

Third, traﬃc archival solutions need to handle high rates
of input streams, which are only expected to increase in
the future. For example, present commercial ﬂow collec-
tors are designed to process up to hundreds of thousands
ﬂow records per second [1]. However, even higher rates can
be observed caused by denial of service attacks, oversub-
scription of streams from many probes to a single collector
or very high traﬃc rates on gateway routers of large insti-
tutions. Therefore, a compression algorithm designed for
network traﬃc archival solutions has to provide high-speed
compression of data streams.

Our work introduces ‘RasterZip’, a compressor optimized
for the requirements and data patterns of network traﬃc
archives. RasterZip compresses blocks of homogeneous net-
work attribute types. It intelligently combines and adapts
a number of compression techniques to exploit the shared
preﬁxes, bounded ranges, and frequent value repetitions com-
monly observed in network traﬃc attributes. RasterZip ﬁrst
homogenizes each block based on a transposition that ex-
ploits the preﬁx structure and bounded range of traﬃc at-
tributes and ﬁnally compresses it using an eﬃcient online
run-length encoding (RLE). Our encoding can sustain high-
speed (de)compression exploiting features of modern CPUs
instruction sets. More importantly, RasterZip introduces a
novel decompression scheme based on partial and adap-
tive decompression to accelerate highly selective queries.
With partial decompression, attribute values stored at spec-
iﬁed positions can be retrieved from a RasterZip-compressed
data block without requiring the entire block to be decom-
pressed. Our adaptive decompression scheme dynamically
selects at runtime between full or partial decompression
based on a very light-weight decision classiﬁer that predicts
which strategy is expected to be the fastest.

We conduct an extensive evaluation of RasterZip using
traﬃc traces from two diﬀerent networks. The results show
that: ﬁrst, RasterZip can reach stream compression rates of
more than half a million traﬃc ﬂows per second, which is
in par with the fastest general-purpose compressor (lzop).
Second, decompression speed is better than lzop, which is
one of the fastest decompressors. Third, RasterZip produces
signiﬁcantly more compact archives than lzop. Our experi-
ments suggests an expected data reduction of approximately
22-24%. This directly suggests that compared to the state-
of-the-art high-speed compressor, RasterZip reduces stor-
age expenses for network traﬃc archives by an analogous
amount.

In summary, our work makes the following contributions:

• Domain-Speciﬁc Compressor. RasterZip leverages
network data patterns; enables interactive queries over
archived data; supports online high-speed stream com-
pression; and exploits features of modern hardware.

• Novel Decompression. RasterZip introduces a
novel decompression scheme that provides ﬁne-grained

decompression granularity at the attribute level in or-
der to accelerate queries for “needles in a haystack”.

• Outperform General-Purpose Compression.
Compared to the widely-used state-of-the-art lzop
compressor, RasterZip accelerates queries, reduces ex-
penses for storage by 22-24% due to more compact
compression, and does not incur any penalty.

Although our design is focused on network traﬃc data,
there are several monitoring domains and applications that
demonstrate similar traits (shared preﬁxes, bounded data-
ranges, and frequent data repetitions), and, therefore, can
also beneﬁt from our compressor. For example, data cen-
ters and cloud computing environments typically produce
voluminous monitoring data to capture environmental (e.g.
temperature) and performance metrics [12]. Such resource
monitoring streams inherently contain numerous repetitions
and shared preﬁxes and therefore can also beneﬁt from
RasterZip.

The remainder of the paper is structured as follows. In
Section 2, we describe common strategies, recent trends and
unsolved challeges in archiving network traﬃc ﬂow records.
Section 3 describes the internals of RasterZip. We evalu-
ate our compressor in Section 5 and we discuss the cost of
storage for large-scale traﬃc archives in Section 7. Finally,
Section 8 concludes our paper.

2. BACKGROUND ON FLOW RECORD

ARCHIVING AND SCOPE OF WORK

Modern network infrastructures come with built-in traf-
ﬁc monitoring functionalities.
Switches and routers are
equipped with embedded meters, referred to as ﬂow meters,
that analyze the network traﬃc to produce network ﬂow
records, which are then sent towards a centralized ﬂow col-
lector using an export protocol, such as Cisco’s NetFlow [7].
A network ﬂow is typically deﬁned as the set of packets
that have ﬁve header ﬁelds in common: source and destina-
tion IP addresses, source and destination port numbers, and
the layer-4 protocol. Flow records carry detailed statistics
describing each observed network ﬂow, such as transferred
bytes, number of packets, TCP ﬂags, and timestamps.

Flow record analysis has many applications,

Existing Relational Database Management

including
billing [10], traﬃc engineering [11], and anomaly detec-
tion [20]. Large-scale historical network ﬂow record reposi-
tories are also used for forensics operations, for intrusion de-
tection, and for conducting long-term network traﬃc stud-
ies. Such applications require ﬂow storage infrastructures
capable of storing high-speed data streams in a compressed
format, while enabling highly eﬃcient drill-down operations.
Sys-
tems (RDBMSs) present severe performance degradations
when dealing with multi-gigabyte datasets [18] and oﬀer
limited support for data compression. A more commonly
used strategy for collecting ﬂow records consists of storing
ﬂow records or even raw NetFlow packets on disk as multiple
ﬁles. This approach provides signiﬁcantly higher insertion
rates than RDBMSs, and allows users to compress their
repositories using oﬀ-the-shelf general-purpose compression
utilities. In this way, however, accessing the data involves
time consuming decompression operations. NfDump [17], a
widely-used ﬂow processing tool, trades compression ratios
for decompression speed by compressing data with the

52general-purpose compression utility lzop (based on the LZO
algorithm), which is up to ﬁve times faster in decompression
than gzip, but provides lower compression ratios [25].

Recent research has shown that advances in database tech-
nologies, stream compression and stream indexing, enable
the development of more advanced ﬂow record storage in-
frastructures that are more eﬀective in compressing data
and that are more suitable for drill-down operations.

Difference of consecutive values

with on‐line LSH sorting (oLSH)

 Destination Port Number values

original sequence (without oLSH)

14 times larger

• Columnar databases have been identiﬁed as a signiﬁ-
cantly better alternative to relational databases in the
context of ﬂow record collection [9, 14, 16]. By storing
distinct attributes of ﬂow records as separate physi-
cal columns, columnar databases oﬀer better oppor-
tunities for compression [2] and are suited for queries
that are highly selective at the attribute level (e.g., for
queries that require just a few ﬁelds of a ﬂow record
to be accessed). These queries are common for after-
the-fact analysis, where network operators are rarely
interested in accessing the entire set of ﬂow record at-
tributes.

• Compressed bitmap indexes have been shown to be ef-
fective for indexing network traﬃc attributes [9, 13,
3]. They are a companion technology of columnar
databases. Given a columnar database with k rows,
a bitmap index is a binary array of length k that in-
dicates the row positions, where an attribute assumes
a speciﬁc value, e.g., srcPort=443. Bitmap indexes
are intelligently compressed to allow for performing
boolean operations in the compressed space between
multiple compressed bitmap indexes. This enables to
eﬃciently support multi-attribute queries, e.g., src-
Port=X AND srcIP=Y , by combining the compressed
bitmap indexes corresponding to columns of distinct
attributes. Given a single- or multi-attribute query,
compressed bitmap indexing enables to ﬁnd very fast
the n (n ≤ k) row positions j1 .. jn that match the
query. Compared to tree-based indexes (e.g., B-Trees),
compressed bitmap indexes:
i) are more compact, ii)
provide high-speed indexing and iii) are optimized for
read-only data.
If data columns are compressed in
small data blocks, the matching row positions can be
exploited to selectively decompress the data blocks
that store the requested data. This capability is very
desirable for accelerating drill-down queries targeting
a small portion of the dataset.

• Stream-based reordering techniques have been pro-
posed in the context of network ﬂow record collection
to introduce better opportunities for compression [14].
The techniques proﬁt from the fact that it is not neces-
sary to strictly order ﬂow records by time. Therefore,
the order of incoming ﬂow records can be altered to
improve their compression.

In our previous work [14], we introduced a ﬂow-record
reordering technique, called online Locality Sensitive Hash-
ing (oLSH), which is optimized for columnar databases.
oLSH treats each multi-attribute ﬂow-record as a multi-
dimensional numerical vector and uses Locality Sensitive
Hash functions [8] to bring ﬂow records that are close to
each other according to their euclidean distance to nearby
stream positions.

Figure 1: Eﬀect of record reordering on column data
homogenity. Storing the streaming records without
reordering (bottom) results in a diﬀerence of con-
secutive values that is up to 14 times larger than
when the online-LSH approach is used (top).

.

Figure 1 illustrates an example of the beneﬁts of oLSH,
which is described in detail in [14]. For one hour of network
traﬃc we create a columnar archive for a multi-attribute
ﬂow record stream with and without the oLSH reordering
enabled. For the destination port attribute, we compute the
sum of diﬀerences between consecutive values for ﬁxed-size
blocks. The unordered stream shown on the bottom depicts
signiﬁcantly lower homogeneity: the cumulative diﬀerence
between consecutive values is up to 14 times larger compared
to the reordered stream using oLSH on top. It is evident that
the reordering process is very eﬀective in clustering values
that belong to adjacent ranges and, thus, that share common
preﬁxes.

In our previous work [14], we have also shown that by
applying this transformation to a ﬂow-record stream, the
performance of the speed-optimized compressor LZO used
to compress individual attributes separately in a colum-
nar manner is substantially improved and in fact, LZO can
match gzip in terms of compression ratio, while providing
substantially higher decompression speed.

The main goal and outcome of our research (illustrated
in Figure 2) is to move beyond general-purpose compres-
sors by designing a domain-optimized compression algorithm
that oﬀers better compression ratios than gzip and bzip2
while at least matching LZO, which was used in our previous
work [14], both in terms of compression and decompression
speed in the context of ﬂow record archival.

Compared to general-purpose compressors, our compres-
sor can fully proﬁt from recent advances in ﬂow record col-
lection research including columnar databases, compressed
bitmap indexes and stream-based ﬂow record sorting tech-
niques:

1. It is designed to exploit speciﬁc data patterns com-

monly present in network traﬃc attributes.

2. It is optimized for compressing data columns of colum-

53r
e
t
s
a
F

i

d
e
e
p
S
 
n
o
s
s
e
r
p
m
o
C

RasterZIP

LZO

(cid:2)

(cid:2)

GZIP

BZIP2

Compressed size 

Smaller

(cid:3)

(cid:1)

(cid:1)

(cid:3)

(cid:1)

(cid:1)

Figure 2: RasterZip versus general-purpose com-
pressors.

nar databases in small data blocks allowing ﬁne-
grained decompression operations.

3. It can exploit bitmap indexes to retrieve attribute
values stored at speciﬁc positions from a compressed
block without performing a full decompression.

3. RASTERZIP COMPRESSION

RasterZip is a block-based compressor optimized for small
data blocks intended to be used in columnar-databases. This
means that the compression routine operates on blocks of
homogeneous attribute values (e.g., timestamps). Our com-
pressor has been designed to leverage the common preﬁx
structure that is present in many network attributes (e.g.,
timestamps) and also produced by the oLSH transformation.
RasterZip takes as input a column, e.g., of IP addresses,
and scans its bytes column-wise (as shown in Figure 3) to
take advantage of the observation that in this manner adja-
cent bytes are often identical. The column-wise order forms
the input to our compressor, which is based on run-length
encoding (RLE) principles. We call the new compression
algorithm RasterZip, because we scan data in a raster-like
fashion. In this section, we describe our compression tech-
niques in detail.

3.1 Overview

The compression routine receives as input data blocks of
homogeneous attribute values. Each block consists of m
values of size n bytes.
In our setting n corresponds, for
example, to two bytes for port numbers, or four bytes for
IPv4 addresses. Each data block of every attribute is treated
as an m × n matrix of bytes, stored in row-major order.

Traditional data archiving approaches process the data
row by row for reasons of eﬃciency and simplicity. Be-
cause of the data reordering, consecutive row-records share
similar/adjacent values and hence have common preﬁxes.
RasterZip compresses data eﬃciently by processing them
column by column, where each cell represents one byte of
the currently processed data attribute. This conceptual and
practical distinction of our approach is shown in Figure 3.
Note, that since rows share common preﬁxes, we expect very
high compression for the ﬁrst byte-columns. For the last
byte-columns where higher variability is generally observed,
lower compression ratios are to be expected.

Based on the above discussion, compression consists of
two logical steps. First, the matrix is logically transposed
and then the resulting matrix is traversed row-by-row to
compress long sequences of repeated symbols. We provide

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:7)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:8)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:7)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:8)

Figure 3: Left: Traditional compression approaches
process data bytes in a row-wise fashion. Right:
RasterZip scans the data bytes of an attribute in
a columnar manner, in order to exploit the preﬁx
structure of the input data.

an in-depth description of the two steps and the resulting
encoding.

1. Transposition. The m × n matrix is transposed,
i.e., the ith row becomes the ith column for all i ∈
{1, . . . , m}. Given that the ﬁrst bytes of subsequent
data items (i.e., subsequent entries in each of the ﬁrst
few columns in the original matrix) are often the same,
transposing the matrix creates rows with many identi-
cal bytes in a row. As a small example, given an input
of three IPv4 addresses (3 × 4 byte matrix), stored in
memory as

10.4.20.22|10.4.20.23|10.4.21.24

the transposition yields the following representation:

10.10.10.4.4.4.20.20.21.22.23.24

This transformation results in long sequences of iden-
tical bytes not only for IP addresses, but also for other
ﬁelds such as timestamps, ﬂow duration, and so on.

We emphasize, that the above transposition is merely
logical. One does not need to carry out the matrix
transposition, but only traverse the data in a diﬀerent
order.

2. Repeated symbol compression: RasterZip eﬃ-
ciently compresses long sequences of repeated symbols
(runs) using a technique based on run-length encod-
ing (RLE) principles. RLE encodes the input stream
as a list of pairs (r, l) where r is the symbol and l is
the length. For example, RLE encodes the sequence
9,9,9,4,4,4,3 as (9,3),(4,3),(3,1).

RasterZip encodes runs in a more structured and eﬃ-
cient way that provides higher decompression perfor-
mance compared to the standard RLE encoding, par-
ticularly when highly compressible and incompressible
content are interleaved in the input stream. In addi-
tion, our encoding has been designed to enable random
access on the compressed data. This is particularly
useful for supporting partial data decompression.

54/ŶƉƵƚ
^ƚƌĞĂŵ

ZƵŶƐ

;ƌ͕ůͿ

ŶĐŽĚĞƌ

ůŽĐŬ

ŶĐŽĚĞƌ

ďůŽĐŬ

B

V

sďůŽĐŬ

KƵƚƉƵƚ^ƚƌĞĂŵ

B V

VB

V B

"(cid:29)(cid:17)#(cid:18)(cid:2)$(cid:18)(cid:28)(cid:24) (cid:31)%
(cid:12)(cid:13)&(cid:2)(cid:12)(cid:13)&(cid:2)(cid:12)(cid:13)&(cid:2)(cid:12)(cid:13)&(cid:2)(cid:14)&(cid:2)(cid:14)&(cid:2)(cid:14)&(cid:2)(cid:8) &(cid:9) &(cid:10) &(cid:11)&(cid:2)(cid:12)(cid:13)&(cid:2)(cid:15)&(cid:2)(cid:15)&(cid:2)(cid:15)&(cid:2)(cid:15)&(cid:2)(cid:15)&(cid:2)(cid:9)&(cid:2)(cid:9)&(cid:2)(cid:9)&(cid:2)(cid:9)&(cid:2)(cid:9)&(cid:2)(cid:9)&(cid:2)(cid:9)&(cid:2)(cid:9)&(cid:2)(cid:9)&(cid:2)(cid:9)&(cid:2)(cid:9)

Figure 4: Flowgraph of RasterZip compression. Se-
quences of repeated symbols in the input stream
are detected by the Runs Encoder and encoded as
Bitmap (B) or Verbatim (V) blocks by the Block En-
coder.

 (cid:22)(cid:17)(cid:21)(cid:18)(cid:2)!(cid:18)(cid:20)(cid:25)"#$
(cid:8)%(cid:12)%(cid:13)%(cid:14)%(cid:8)(cid:8)%(cid:8)(cid:15)%(cid:8)(cid:9)%(cid:8)(cid:12)%(cid:9)%(cid:8)(cid:11)%(cid:8)(cid:11)%(cid:15)

(cid:16)(cid:17)(cid:2)(cid:18)(cid:5)(cid:2)(cid:15)(cid:19)(cid:2)(cid:20)(cid:21)(cid:22)(cid:23)(cid:2)(cid:5)(cid:24)(cid:2)(cid:4)(cid:25)(cid:22)(cid:26)(cid:18)(cid:27)(cid:2)(cid:8)(cid:2)

(cid:28)(cid:29)(cid:15)(cid:19)(cid:2)(cid:3)(cid:30)(cid:18)(cid:25)(cid:23)(cid:31)

V

(cid:8)
(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)

(cid:12)

(cid:13)

(cid:14) (cid:8)(cid:8) (cid:8)(cid:15)

(cid:8)(cid:9) (cid:8)(cid:10) (cid:9) (cid:8)(cid:11) (cid:8)(cid:11)

(cid:15)

Figure 5: V-blocks are used to store content that
is incompressible with RLE. V blocks store value
repetitions of length 1. Runs of length 2 are split in
two runs of length 1.

Figure 4 depicts a ﬂowgraph of the compressor. The input
stream for the compressor is the matrix given in column-
major order. This is the currently processed data window,
representing the output of the online-LSH component. The
Runs Encoder scans the input stream and outputs to the
Block Encoder pairs of the form (r, l) representing sequences
of repeated symbols. The Block Encoder groups together
runs in multiples of 32 and encodes each group by dynami-
cally selecting between two diﬀerent subblock types: a Ver-
batim (V) block and a Bitmap (B) block, which are de-
scribed in the next section.

3.2 The Encoding

The

encoding includes

two diﬀerent block types:
Verbatim (V) blocks store RLE-incompressible content
whereas Bitmap (B) blocks store RLE-compressible con-
tent.1 Speciﬁcally:

1. V-blocks store a group of up to 32 runs whose length
is exactly one. Runs of length 2 are stored as two runs
of length 1 (i.e., the block encoder never receives runs
of length 2 from the runs encoder). Therefore, such a
block stores data that is hard to compress using RLE.
A one-byte header is prepended. The header speciﬁes
the block type using 1 bit (i.e. that it is a V-block) and
the number of runs which are stored in the block using
ﬁve bits (25 = 32 runs). Because this block-type only
stores runs of length 1, lengths need not be explicitly
stored. Figure 5 depicts how an incompressible input
stream is encoded with a V-block.

2. B-blocks encode up to 32 runs where at least one run

has length greater than 1. B-blocks consist of:

• A header (1 byte) specifying the block type and

the number of runs (similarly to V-blocks).

1A block of attribute values is encoded into multiple sub-
blocks of type V or B. The terms V-block and B-block refer
to subblocks. The proper interpretation of the term block
should be clear from the context.

(cid:27)(cid:28)(cid:24)(cid:25)(cid:24)(cid:29)(cid:6)(cid:24)(cid:2)(cid:1)(cid:30)(cid:18)(cid:31) (cid:17)

(cid:20)(cid:11)(cid:19)(cid:2)(cid:3)(cid:30)(cid:18)(cid:25)(cid:26)(cid:2)

(cid:16)(cid:17)(cid:2)(cid:18)(cid:5)(cid:2)(cid:11)(cid:19)(cid:2)(cid:1)(cid:2)(cid:3)(cid:4)

(cid:20)(cid:21)(cid:22)(cid:2)(cid:11)(cid:19)(cid:2)(cid:3)(cid:23)(cid:18)(cid:24)(cid:25)(cid:26)

Up to 32 lengths

(<= 32 bytes)

B

1100000110…0

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)

(cid:12)(cid:13)

(cid:14)

(cid:8)

(cid:9)

(cid:10)

(cid:11)

(cid:12)(cid:13)

(cid:15)

(cid:9)

(cid:10)

(cid:11)

!

(cid:12)(cid:12)

Figure 6: B-blocks store RLE-compressible content.
B blocks encode up to 32 consecutive runs where
at least one run has a length greater than 1. Only
lengths greater than 1 are explicitly stored.

• A presence bitmap (32 bits) indicating which runs

have length greater than 1.

• A Runs part storing up to 32 runs.

• A Lengths part storing up to 32 lengths.

Therefore, unlike plain RLE, RasterZip does not store
runs and lengths as pairs. Instead, runs are stored se-
quentially, followed by the sequence of the correspond-
ing lengths. The reason for this becomes apparent in
the following, where we describe how the V- and B-
blocks are created in an online fashion.

Note that a B-block stores mixed content of compress-
ible and incompressible content. In fact, the presence
bitmap (32 bits) exactly indicates which of the runs
have length greater than 1. Recall that runs of length
2 are always treated as 2 consecutive runs of length
1. So, length is only explicitly stored for runs with
length greater than 2. This is indicated by setting the
corresponding bit in the presence bitmap to one.

An example of a B-block is given in Figure 6. For
the initial sequences of 10’s and 9’s the lengths are
explicitly stored (4 and 3, respectively), whereas for
the singular values that follow, no explicit length is
recorded. This is captured in the presence bitmap, by
setting the appropriate bits to 1 or 0.

We also emphasize that, having allocated one byte for
each length, we could have represented sequences of up
to 28 = 256 consecutive identical symbols. However,
by not storing explicitly lengths 1 and 2, we can in-
crease the maximum sequence length that can be rep-
resented to 258. In fact, since runs of length 1 are just
marked in the bitmap and runs of length 2 are always
treated as two runs of length 1, each length-byte in the
B-block can store lengths in the range [3, 258]. This
allows us to pack long sequences of the same symbol
even more eﬀectively.

Online Blocks Creation Now we describe how the two
types of blocks can be created by the Block Encoder in a
streaming fashion. First, note that a V-block can be treated
as a special case of a B-block, as it is a B-block whose
presence bitmap does not have any bits set to 1. Therefore,
the block creation always commences by assuming that the
current block is a B-block. Whenever a run of length greater
than 2 is observed, the presence bitmap is updated and the
corresponding length is appended as well. After 32 runs are
processed, we check whether the completed block contains
at least one run of length greater than 2 by examining if at

55least one bit is set in the presence bitmap. In that case, the
block is ﬁnalized and a new B-block is created. Otherwise,
the presence bitmap is dropped and the block is changed
into a V-block by simply ﬂipping the ﬁrst bit in the header
byte. Pseudocode of this process is given in Algorithm 1.

{Update the bitmap}
if l > 1 then

set bit(bitmap, curr run)
lengths[curr len++] := l

end if
{Add the current run}
runs[curr run++] := r
if curr run = 32 then

Algorithm 1 Compress(input)
Input: Uncompressed input stream
Output: Compressed output stream
1: {split runs of length two}
2: while (r, l) := getN extRun(input) do
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27: end while

end if
curr run := 0
curr len := 0
bitmap := 0

end if

{Output a V or a B block}
if bitmap = 0 then

h := create header(V, curr run)
output.appendHeader(h)
output.appendBytes(runs, curr run)

else

h := create header(B, curr run)
output.appendHeader(h)
output.appendBitmap(bitmap)
output.appendBytes(runs, curr run)
output.appendBytes(lengths, curr len)

3.3 Encoding properties and advantages

The proposed encoding presents several desirable prop-
erties with an emphasis on exploiting features of modern
CPU architectures for enhanced performance. In particular
RasterZip provides:

Low memory footprint during compression:
Through the whole compression process, the algorithm is
only required to allocate memory for a single RasterZip sub-
block. A sub-block is very small: at most 69 bytes (header
+ bitmap + runs + lengths = 1 + 4 + 32 + 32). We
also note that the average sub-block size is even smaller in
practice. We observed an average of 40 bytes in our ex-
periments. The cache-line size on modern processors is 64
bytes; therefore, at most 2 cache-lines are used during the
compression. This leads to a low number of cache-misses
and enhanced performance of the proposed compression al-
gorithm. In contrast, popular compression algorithms, such
as the Lempel-Ziv based compression schemes, use hash ta-
bles or dictionaries to store previously seen data [23], and as
such, result in many cache misses in practice.

Eﬃcient handling of

incompressible content:
RasterZip encodes data that is hard to compress using RLE
(i.e., the length of each run is less than 3) with a V-block.
This block-type enables fast decompression of incompress-
ible data, because decompressing a V-block merely requires
copying up to 32 bytes to the output, which can be im-
plemented using the memcpy() function. On the contrary,

decompressing hard to compress data encoded with the orig-
inal RLE encoding requires a conditional branch (if state-
ment) for every symbol, even if the length is as short as 1
or 2.

The next two points show how RasterZip exploits features
oﬀered by modern CPU architectures. These characteristics
are useful primarily during the decompression process, as
they allow the eﬃcient traversal of RLE subblocks and the
computation of the span of each subblock.

Fast traversing of compressed data: The simple struc-
ture of V and B blocks allows compressed data to be traversed
quickly. A V-block storing n runs always consists of 1 + n
bytes, and n can be read from the ﬁrst byte after masking
the three most signiﬁcant bits to 0. Similarly, the size of
a B-block is 1 + 4 + n + b bytes, where b corresponds to
the number of bits set to one in the bitmap. The value of b
can be eﬃciently computed using specialized bit counting in-
structions oﬀered on modern processors. For example, Intel
introduced POPCNT to the SSE4.2 instruction sets, which
oﬀers a dedicated assembly instruction to perform the bit
counting operation.2

Eﬃcient computation of the block span: given a
block, its span is the number of bytes of uncompressed data
stored in it. For both block types, the span can be eﬃciently
computed: the span of a V-block is the number of runs that
it stores. Computing the span of a B-block requires sum-
ming up the lengths greater than 2 and adding the number
of runs with length 1. More formally:

SpanB = (n − b) + Pb

i=1 l[i]

where n is the number of runs, b denotes the number of
bits set to 1 in the bitmap, and l[i] is the length of the
ith run whose length exceeds 2. Since b is at most 32, and
therefore the array of lengths cannot be longer than 32, it
is possible to unroll the loop for this computation using a
switch statement with 32 entries. This optimization enables
the fast computation of the block span, which is an essential
component of the decompression process.

Additional properties of the encoding scheme: The
described encoding represents runs in batches of 32. How-
ever, the number of runs encoded in V- or B-blocks is
in principle a tunable parameter3.
In practice, there are
i) a smaller number of runs
trade-oﬀs to be considered:
corresponds to ﬁner-grained decompression granularity and
lower performance when copying individual V-blocks to the
output, and, ii) a larger number of runs favors B- over V-
blocks, making the compressor less capable of mixing RLE-
compressible and incompressible content eﬃciently. We ﬁx
the number of runs to 32, which resulted to be a good com-
promise between compression ratio and decompression per-
formance.

2Modern compilers, such as GCC, oﬀer the intrinsic func-
tion
builtin bitcount(unsigned int) for using POPCNT in
C. The -msse4 compilation ﬂag instructs the GNU GCC
compiler to use the POPCNT instruction.
3The 1-byte header of both V- and B-blocks can be accom-
modated for up to 128 runs by using 7 instead of 5 bits to
store the number of runs belonging to the block (27 = 128
runs).
In the case of B-blocks the maximum number of
runs has to be the same as the length, expressed in bits, of
the presence bitmap.

564. DECOMPRESSION

Recall that for a certain attribute, a columnar archive con-
tains multiple RasterZip-compressed blocks, which in turn
are made of multiple RLE-subblocks (V or B). At runtime,
user queries are evaluated using an index (e.g., a compressed
bitmap index), which returns the row positions that match
within the data column. Therefore, the query processor re-
ceives as input a list of row positions from which it deter-
mines the blocks that need to be decompressed.

The row positions list also speciﬁes the positions of the
attribute values stored by a speciﬁc RasterZip-compressed
block that belongs to the result set. Compared to general-
purpose compressors, our approach can exploit this informa-
tion to provide partial data decompression: it decompresses
only the RLE subblocks that are part of the result set.

We ﬁrst start by examining two block decompression
strategies: a full decompression and a partial decompression.
The latter approach decompresses only the RLE-subblocks
that contain parts of the result set. Intuitively the ﬁrst ap-
proach is better when a block contains many results; the
second approach when only few results need to be retrieved.
Finally, we show how to adaptively select between full and
partial decompression at runtime. We introduce an adaptive
decompression component that intelligently shifts between
full- or partial decompression, according to which approach
is expected to result in lower decompression time.

4.1 Full-Decompression (FD)

Assume that a compressed block is marked for decompres-
sion because it contains results for a given query. The block
contains m data items each with size of n bytes and the index
shows the row positions r1, . . . , rk (k ≤ m) within the block.
The Full-Decompression strategy is shown in Figure 7. It de-
compresses the entire block (i.e., all RLE-subblocks) which
is then stored in column-major order. Block transposition
back to row-major order does not need to be performed, be-
cause the required data rows can be retrieved directly, as we
explain below.

Compressed block: 13 subblocks

Un-Compressed block (n=4):

Requested rows:

1 2

3 4 5 6 7 8 9 10 11 12 13

Row positions: {r1

, r2, r3 , r4, }

1

3

2

4

5

8

9

10

6

11

12

7

13

r1

r2

r3

r4

192 168 0

10

10 155 1

192 143 1

5

5

192 155 0

10

Decompressed 

subblocks

Figure 7: Full-Decompression (FD) decompresses
the entire set of RLE subblocks and then uses the
row positions to compute the indices of the bytes
that need to be retrieved in the uncompressed ma-
trix stored in column-major order.

Consider each decompressed archive block as an m × n
matrix. Each required row position r can be retrieved by
accessing the bytes at the oﬀsets r, r + m, .., r + (n − 1)m,
since the oﬀset between the bytes that belong to a certain
row is exactly m.

An example of the retrieval procedure is given in Figure 8,
which on the left illustrates a block with three IPv4 ad-
dresses and 4 bytes per row. Because the decompressed ma-
trix is in column-major order, as shown in the right of Fig-
ure 8, to access a row r at position x, the decompressor tra-
verses the data every 4 bytes at oﬀsets: x, x +4, x +8, x +12.

(cid:13)(cid:14)(cid:15)(cid:16)(cid:15)(cid:9)(cid:17)(cid:18)(cid:10)(cid:19)(cid:17)(cid:20)(cid:17)(cid:21)

(cid:24)(cid:25)(cid:20)(cid:14)(cid:15)(cid:25)(cid:26)(cid:15)(cid:9)(cid:16)(cid:10)(cid:14)(cid:27)(cid:28)(cid:29) (cid:22) (cid:30)(cid:14)(cid:27)(cid:12)(cid:10)(cid:20)(cid:31)(cid:25)(cid:10)(cid:12)(cid:17)(cid:20)(cid:14)(cid:15)(cid:22)(cid:10)(cid:15)(cid:9)(cid:10) (cid:27)(cid:18)!(cid:12)(cid:9)"(cid:12)(cid:17)#(cid:27)(cid:14)(cid:10)(cid:27)(cid:14)$(cid:25)(cid:14)(cid:21)

(cid:9)(cid:10)(cid:11)(cid:10)(cid:2)

(cid:1)(cid:6)(cid:7)

(cid:1)(cid:4)(cid:5)

(cid:1)(cid:7)

(cid:1)(cid:8)

(cid:1)(cid:6)(cid:7)

(cid:1)(cid:4)(cid:5)

(cid:1)(cid:7)

(cid:7)(cid:3)(cid:2)

(cid:1)(cid:6)(cid:7)

(cid:1)(cid:4)(cid:5)

(cid:1)(cid:3)

(cid:1)(cid:2)

(cid:22)

(cid:22)(cid:23)(cid:12)

(cid:22)(cid:23)(cid:7)(cid:12)

(cid:22)(cid:23)(cid:3)(cid:12)

(cid:1)(cid:6)(cid:7)

(cid:1)(cid:6)(cid:7)

(cid:1)(cid:6)(cid:7)

(cid:1)(cid:4)(cid:5)

(cid:1)(cid:4)(cid:5)

(cid:1)(cid:4)(cid:5)

(cid:1)(cid:7)

(cid:1)(cid:7)

(cid:1)(cid:3)

(cid:1)(cid:8)

(cid:7)(cid:3)(cid:2)

(cid:1)(cid:2)

(cid:12)

(cid:10)

(cid:3)
(cid:11)
(cid:12)

(cid:10)

Figure 8: The row at position x from a 3 ×
4 matrix stored in column-major order is re-
trieved by accessing the four bytes at the oﬀsets
x, x + m, x + 2m, x + 3m.

4.2 Partial-Decompression (PD)

The Full-Decompression decompresses all RLE-subblocks,
even if they contain no results. Partial-Decompression de-
compresses only subblocks that contain query results. An
illustration of this approach is given in Figure 9.

Compressed block: 13 subblocks

Un-Compressed block (n=4):

Requested rows:

1 2

3 4 5 6 7 8 9 10 11 12 13

Row positions: {r1

, r2

}

r1
r2

1

3

2

4

5

8

9

10

6

11

12

7

13

192 168 0

10

10 155 0

5

Decompressed 

subblocks

Figure 9: Partial Decompression (PD) decompress
only RLE subblocks containing at least a byte that
is required for retrieving the requested rows.

The partial decompression algorithm ﬁrst calculates the
set of oﬀsets O in the matrix (stored in column-major order)
that should be accessed to retrieve the requested rows. The
oﬀsets O are used to determine whether an RLE-subblock
(V- or B-block) must be decompressed: a subblock b that
compresses the matrix byte oﬀset i to j needs to be decom-
pressed if there is an oﬀset k ∈ O, where i ≤ k ≤ j. The
decompressor can eﬃciently ﬁnd which subblocks need to be
decompressed using the procedure we outline below. After
decompressing the required subblocks, the requested rows
are extracted using the same byte-traversal technique de-
scribed for the Full-Decompression. The process described
above is summarized in Algorithm 2.

Implementing Partial-Decompression: A naive ap-
proach to determine which RLE-subblock needs to be de-
compressed is to test, for each subblock, whether it contains
at least one byte of the rows in the query (line 6 of Algo-
rithm 2). For this purpose, we need to compute the subblock
range, which is the range of byte oﬀsets stored in an RLE
subblock. If the span of a subblock is σ and the sum of the

57Algorithm 2 PartialDecompress(Rows, B, n)
Input: A list of row positions Rows

A compressed block B
The size n of each attribute value

Output: The attribute values stored in the requested row
positions

1: O := ∅

✁ start with an empty set of oﬀsets

2: foreach r ∈ Rows
3: O := O ∪ getOﬀsets(r, n)

✁ build the set of oﬀsets

✁ linear subblock scan

4: foreach b ∈ SubBlocks(B)
5:
(i, j) := subBlockRange(b)
if ∃ k ∈ O : k ≤ j ∧ k ≥ i then
6:
7:
8:
9:
10: return retrieveRowsFromColunMajor(Rows,output,n)

{Byte-traversal technique as in Figure 8}

decompressSubBlock(b, i, output)

end if

spans of the preceding subblocks is S, then the range of a
subblock is [S +1, S +σ]. Recall that the RasterZip encoding
allows to eﬃciently compute the span of a subblock. In order
to test whether a sublock range contains a byte of a queried
row, the naive approach would require to map the oﬀsets of
the queried row bytes into the transposed matrix and then
to test each subblock. If at least one of the new oﬀsets lies
in the range between S + 1 and S + σ, then the correspond-
ing subblock should be decompressed. This naive strategy
is very simple, but requires many comparisons. In practice,
it leads to very slow performance. Even if only a small frac-
tion of rows have to be extracted, the decompression speed
is considerably worse than performing a full-decompression.
We next describe a practical and eﬃcient implementation
of Algorithm 2 that uses a ﬁxed amount of memory to store
the set of oﬀsets and substantially reduces the number of
needed comparisons. The main idea of the implementation
is to represent the set of oﬀsets as a bitmap instead of a
list of integers. Each bitmap entry corresponds to a certain
range of bytes. A bitmap entry is set to 1 if and only if at
least one byte in the corresponding range belongs to one of
the queried rows.
In the next paragraph, we describe the
technique in more detail.

The transposed m × n matrix (in column-major order) is
partitioned into s ﬁxed size cells, where s is a small constant.
Intuitively, a grid of s slots is logically superimposed on the
matrix. A bitmap of length s, called grid bitmap, is used to
mark each cell that contains at least one byte that is part
of the query result (see Figure 10 where s = 16). Given the
set of rows to be retrieved, the bits that need to be set to 1
can be determined very eﬃciently. First, the oﬀsets of the
desired row bytes are mapped into the transposed matrix.
The mapped oﬀsets are (still) in the range [1, m · n]. Then,
by linearly rescaling the oﬀsets in the range [1, s] we ﬁnd the
positions of the grid bitmap that need to be set.

Given the grid bitmap, it can be determined eﬃciently
whether a subblock needs to be decompressed: its subblock
range simply needs to be linearly rescaled as well by a factor
of s/(mr). If any bit is set in the grid bitmap in this rescaled
range, the block needs to be decompressed. This process can
exploit the bit counting assembly instruction and does not
need any comparisons. The number s of cells is a tunable
parameter that allows to trade oﬀ precision for performance.
More detailed grids (i.e. with larger values of s) are more

Row positions:

{r

1, r

2}

Un-Compressed 

Grid Bitmap

Un-Compressed 

block (n=4)

block (n=4)

r
1

r
2

1 1 1 1

1 1 1 1

0 0 0 0

0 0 0 0

1

3

2

4

5

8

9

10

6

11

12

7

13

Required bytes

Decompressed 
subblocks

Figure 10: A Grid Bitmap is used to partition the
uncompressed m×n matrix into 16 distinct cells (s =
4 × 4). Given a query, the bit corresponding to each
cell is set to 1 if there is at least a byte in the cell
that has to be retrieved from the compressed data.
A subblock has to be decompressed if and only if it
overlaps with a cell that has been marked with a 1
in the corresponding Grid Bitmap.

precise in estimating the subblocks to be decompressed, but
require more space to be stored, and consequently, are slower
to query in average due to cache misses.

4.3 Adaptive Block Decompression

We have so far described two algorithms for block de-
compression. In this section we examine how to adaptively
choose between the two algorithms at runtime. Partial
decompression is preferred when a small fraction of RLE-
subblocks need to be decompressed. Otherwise, the cost for
computing the span of multiple RLE-subblocks makes full
decompression a better strategy.

We introduce an adaptive decompression component that
uses two runtime parameters, namely the selectivity of a
query (within a block) and the compression ratio of a block,
to predict which decompression strategy will result in faster
decompression. Below, we describe how the two parameters
relate to the decompression strategy:

Selectivity: If the selectivity within a block is low (i.e.,
a large fraction of rows have to be retrieved), then many
RLE subblocks need to be decompressed. In this case, full-
decompression is preferable. In contrast, for high selectivity
queries, a small fraction of subblocks is decompressed. In
this case partial-decompression is faster.

Compression ratio: The compression ratio is deﬁned
as the ratio between the size of the compressed data and
the size of data before compression [23]. When the com-
pression is better (lower compression ratio), then a block
is compressed into fewer RLE-subblocks, and a larger frac-
tion of the subblocks needs to be decompressed. Therefore,
better compressed blocks beneﬁt more from full decompres-
sion. Conversely, when RasterZip achieves worse compres-
sion ratios, a small fraction of RLE-subblocks have to be
decompressed.

The selectivity of a query and the compression ratio of a
block largely determine the fraction of RLE-subblocks that

58need to be decompressed and therefore can be used to pre-
dict which of the two strategies will perform better.
In
addition, the selectivity (s) and compression ratio (c) can
be eﬃciently computed at runtime before accessing the com-
pressed subblocks. We exploit these two parameters to build
an adaptive decompression component shown in Figure 11.
The adaptive decompression component accepts as input a
compressed block b and a set R of row positions and, then,
it uses a model to decide if partial or full decompression will
provide a lower response time.

…
…

Header:CompSIZE,UncSIZE

Block i

RasterZip SubBlocks

…
…

Data for

attribute N 

of size AttWIDTH

Row Positions:

Rp = {r

1, … , r

k }

k

Header

- compression ratio

- selectivity

Adaptive

Decompression

Strategy

Partial 

Full

Decompression

Decompression

Figure 11: The adaptive decompression component
decides at runtime the more eﬃcient decompression
strategy (partial or full) to be used for answering
a query where k rows of the data block have to be
accessed. It accepts as input the row selectivity s
and the block compression ratio c.

Given a block b, the compression ratio can be eﬃciently
computed from the block header, which stores the size
of the block before and after compression (U ncSIZE and
CompSIZE in Figure 11). The selectivity is computed by
dividing the number of rows to be decompressed by the to-
tal number of attribute values packed in b. The adaptive de-
compression component, chooses between the two strategies
using a simple and yet eﬀective model built from proﬁling
RasterZip. The model is a l × l matrix, where the two di-
mensions are selectivity and compression ratio. The value in
a cell indicates the decompression strategy that is expected
to provide faster decompression for the corresponding range
of selectivity and compression ratio values.

A proﬁled model is data- and attribute-agnostic: once
built using real or synthetic data, it can be applied for dif-
ferent datasets and queries. To proﬁle RasterZip we exe-
cuted queries of diﬀerent selectivity on compressed traﬃc
ﬂow records and measured the time required to decompress
the queried rows using both the partial and full decompres-
sion strategies. For each cell of the model matrix, we com-
puted the average response time for the two decompression
strategies. We ﬁnally selected the decompression strategy
with the lowest average decompression time.

An actual example of the model is shown in Figure 12.
Note that partial-decompression is preferable for blocks that
exhibit poor compression and/or high selectivity. For the
remaining cases, full-decompression should be chosen.
In
summary, the adaptive decompression component allows our
methodology to combine the power of both full- and partial-
decompression strategies.

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:2)(cid:6)(cid:7)

(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:3)(cid:9)(cid:14)(cid:14)(cid:5)(cid:11)(cid:15)

(cid:16)(cid:19)(cid:6)(cid:6)

(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:3)(cid:9)(cid:14)(cid:14)(cid:5)(cid:11)(cid:15)

(cid:20)(cid:21)(cid:22)

x
x

(cid:16)(cid:9)(cid:17)(cid:9)(cid:3)(cid:7)(cid:3)(cid:11)(cid:17)(cid:14)

Selectivity (%)

(cid:9)
(cid:10)
(cid:2)
(cid:13)
(cid:14)
(cid:7)
(cid:14)
(cid:14)
(cid:9)
(cid:18)

)

%

i

(
 
o
i
t
a
r
 
n
o
s
s
e
r
p
m
o
C

0

10

20

30

40

50

60

70

80

90

100

10

20

30

40

50

60

70

80

90

xx
xx
xx
xx

xx
xx
xx
xx

xx
x
xxxxxxxxxxxxxxxx
xx
xx
x
xx
xx
xx
x
xx
x
xx

x
xx
x
xx
xx
x
x
xx

xx
xx
xx
xx

xx
xx
xx
xx

xx
xx
xx
xx

100

Figure 12: A 10 × 10 grid model built using a proﬁl-
ing dataset for deciding when to decompress using
Partial-Decompression (P D) or Full-Decompression
(F D). We indicate with N/A, cells that were
not ﬁlled using the proﬁling dataset.
Partial-
Decompression is preferable for blocks that exhibit
low compression ratios and/or high block selectivi-
ties.

4.4 Error detection

Contrary to block-based compression utilities, such as
bzip2, RasterZip does not deﬁne a framing format and does
not come with built-in support for error detection. In fact,
RasterZip delegates this task to the upper layers where error
detection mechanisms can be implemented using well-known
approaches.

For example, RasterZip-encoded data blocks can be inter-
leaved with a ﬁxed size bit pattern, the block delimiter, that
allows data block boundaries to be recognized and a check-
sum can be prepended to each RasterZip-encoded block.
Data corruption can be detected by comparing the stored
checksum with the checksum computed over the data de-
compressed using the full decompression algorithm. This
approach is not applicable when the partial decompression
strategy is used. However RasterZip could be modiﬁed to in-
clude an additional checksum at the end of V- and B-blocks.
We leave this extension to future work.

5. EVALUATION

To evaluate the performance of RasterZip we built the
archival solution shown in Figure 13 and use it to archive
and query traﬃc ﬂow records collected from real networks.
The streaming ﬂow records (13.A) are hashed into hash-
buckets containing chains of ‘similar’ records (13.B). Data
blocks are created from each hash chain and then indexed
(13.C) and archived (13.D) in compressed data blocks using
a columnar-approach. At query time, the index returns the
row positions of interest in the archive. This information is
used by the query processor to identify for each requested
attribute the compressed data blocks within the correspond-
ing data column that contain the result set.

59(cid:3)(cid:2)(cid:13)&(cid:28)(cid:26)(cid:11)(cid:8)(cid:16)(cid:10)(cid:11)(cid:11)(cid:5)(cid:8)(cid:17)(cid:28)(cid:11)(cid:12)(cid:13)’(cid:11)(cid:5)(cid:12)(cid:10)(cid:9)(cid:13)(cid:30)(cid:12)(cid:19)(cid:6)(cid:5)(cid:18)(cid:15)

(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:7)(cid:16)(cid:17)(cid:13)(cid:18)
(cid:25)(cid:21)

(cid:21)(cid:22)(cid:23)(cid:25)(cid:22)(cid:23)(cid:21)(cid:23)(cid:21)(cid:22)(cid:22)

(cid:21)(cid:22)(cid:23)(cid:24)(cid:23)(cid:21)(cid:21)(cid:23)(cid:21)

(cid:26)(cid:22)

(cid:6)(cid:20)(cid:18)(cid:15)(cid:16)

(cid:19)

(cid:19)

(cid:19)

(cid:19)

(cid:1)(cid:2) (cid:3)(cid:4)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:12)(cid:13)(cid:14)(cid:10)(cid:15)(cid:14)(cid:16)(cid:17)(cid:10)(cid:15)(cid:12)(cid:18)(cid:13)

(cid:5)(cid:12)(cid:19)(cid:6)(cid:5)(cid:18)(cid:13)(cid:5)(cid:12)(cid:6)(cid:5)(cid:18)(cid:12)(cid:5)(cid:8)(cid:20)(cid:21)

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:3)
(cid:6)(cid:4)(cid:5)(cid:4)(cid:7)
(cid:8)(cid:9)(cid:10)(cid:1)(cid:11)

(cid:22)(cid:2)(cid:13)(cid:23)(cid:20)(cid:18)(cid:12)(cid:7)(cid:13)(cid:22)(cid:5)(cid:12)(cid:10)(cid:11)(cid:8)(cid:6)(cid:20)

(cid:8)(cid:15)(cid:5)(cid:27)(cid:4)(cid:16)(cid:7)(cid:15)(cid:28)(cid:6)(cid:3)(cid:29)

(cid:24)(cid:2)(cid:13)(cid:3)(cid:5)(cid:19)(cid:14)(cid:8)(cid:25)(cid:10)(cid:26)(cid:13)
(cid:27)(cid:22)(cid:6)(cid:26)(cid:28)(cid:9)(cid:20)(cid:16)(cid:29)(cid:8)(cid:15)(cid:12)(cid:13)(cid:30)(cid:28)(cid:20)(cid:13)(cid:31)(cid:12)(cid:20)(cid:21)(cid:11)(cid:14)(cid:13) (cid:20)(cid:19)(cid:6)(cid:18)(cid:8)(cid:20)(cid:21)!

(cid:8)(cid:9)(cid:10)(cid:1)(cid:11)(cid:7)-.(cid:21)

!

"

…

%
&
(cid:20)
(cid:20)
&
(cid:13)
+
*
(cid:17)
(cid:14)
$
,

(cid:21)

(cid:25)

 

(cid:24)

(cid:8)(cid:9)(cid:10)(cid:1)(cid:11)(cid:7)-

(cid:20)(cid:14)#$(cid:7)(cid:17)(cid:13)%&(cid:13)
(cid:17)’(cid:7)()(cid:18)&(cid:20)

(cid:14)(cid:17)*+(cid:13)&(cid:20)(cid:20)&%

(cid:1)(cid:10)(cid:27)(cid:16)(cid:2)(cid:3)(cid:12)(cid:12)(cid:3)(cid:6)(cid:7)(cid:4)(cid:2)(cid:1)(cid:30)(cid:15)(cid:31)(cid:3)

(cid:3)(cid:11)(cid:11)(cid:5)(cid:8)(cid:17)(cid:28)(cid:11)(cid:12)(cid:13)"(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)#(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)$(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)(cid:13)

%

5.1 Compression Ratio

For our comparison experiments, we compress attribute
blocks in the component D of Figure 13 both with LZO
and RasterZip.
In this manner, both LZO and RasterZip
beneﬁt from the online record rearrangements provided by
the component 13.B. The reordering is realized using oLSH,
a ﬂow record sorting methodology that we introduced in our
previous work [14].

We choose a block size of 4000 records, which we found
empirically to provide the best trade-oﬀ between compres-
sion ratio and response time. The indexing (13.C) is realized
using the COMPAX compressed bitmap indexing encoding
introduced in our previous work [14]. The only evaluation
metric that depends on the selected index is the query re-
sponse time. As shown in our previous work, the index
lookup makes a negligible fraction of the query response
time, which is dominated by the time for decompressing
blocks even when a high-speed compressor such as LZO is
used. In addition, the evaluated compressors are used with
the same index to yield a fare comparison.

Figure 13: An overview of the compression method-
ology for streaming records.

Our experiments show that the column-major RLE-based
compression signiﬁcantly improves the compression ratio
achieved with LZO. Additionally, because RasterZip pro-
vides subblock decompression it has better response times
for high selectivity queries.

All experiments have been conducted on a commod-
ity desktop machine equipped with 2 GB of DDR3 mem-
ory and an Intel Core 2 Quad processor (Q9400) running
GNU/Linux (2.6.28 kernel) in 32-bit mode. The processor
has four cores running at 2.66 GHz and 6 MB of L2 cache.
We store the compressed archives on a 320 GB desktop hard
drive4.

As input to the system we provided uncompressed ﬂow
traces stored on a commodity solid state drive5. The drive
provides a sustained reading speed of 170 MB/s, which cor-
responds to more than 5 Million ﬂows/second (f/s). The

4The hard drive is a 7200 rpm Hitachi HDP725032GLA380
equipped with 8 MB of cache. The system is capable of per-
forming cached reading at 2400 MB/s and unbuﬀered disk
reads at 80 MB/s (measured with hdparm).
5Intel X-25M G1, 80 GB model

system has been conﬁgured to fetch ﬂows sequentially from
the solid state drive and to store both indexes and com-
pressed columns to the mechanical desktop hard drive. This
simple setup allows us to reproduce ﬂow rates that can only
be observed in large ISP networks. We use two data sets in
the evaluation:

• Six days of NetFlow traces of access traﬃc from a large

hosting environment (HE).

• A two month NetFlow trace of internal and external
traﬃc in an medium-sized enterprise production net-
work (PN).

Each ﬂow record includes the following attributes: source
and destination IP addresses and L3 ports, the L3 protocol,
TCP ﬂags, source and destination Autonomous System (AS)
number, number of packets, number of bytes, the time of the
ﬁrst packet and the duration of the ﬂow. Information about
the datasets are summarized in Table 1. The gzip and bzip2
columns report the storage footprint when compressing the
raw data with the gzip and bzip2, respectively.

Table 1: Utilized Datasets

Data # ﬂows Length Raw
HE
PN

231.9 M 6 days
1.2 B

62 days

gzip

bzip2
6.9 GB 2.5 GB 2.2 GB
37 GB
8.1 GB 6.9 GB

In Table 2, we report the size of the archives compressed
with RasterZip and LZO. First, we note that the approxi-
mate ﬂow reordering scheme allows LZO, which is optimized
for compression speed rather than for achieving high com-
pression ratios, to almost match the compression ratio of
gzip (shown in Table 1). By properly exploiting the pre-
ﬁx structure of partially reordered traﬃc records, RasterZip
can further reduce the disk consumption for both datasets.
In fact, we observe that RasterZip uses 22% and 24% less
space than LZO for the HE and PN datasets, respectively.
It also uses less space than the simple approach of compress-
ing raw data with gzip or bzip2. Compared to the values in
Table 1, RasterZip oﬀers 20% compression than gzip and
even 9-10% better compression than the slow, but heavily
space-optimized bzip2.

Table 2: Disk space requirements for diﬀerent com-
pression algorithms.

Dataset
HE
PN

LZO RasterZip
2.0 GB
6.2 GB

2.6 GB
8.2 GB

5.2 Insertion Rate

Our system has been designed for compressing and in-
dexing high-speed streams of ﬂow records in real-time.
RasterZip aims at reducing the disk usage of compressed
archives and at supporting very high insertion rates. Having
showed that RasterZip reduces the disk usage of two refer-
ence compressors, we evaluate the insertion rate it can sup-
port. For this purpose, we use the two datasets and compare
the insertion rate our archive achieves when data columns
are compressed on-the-ﬂy using LZO and RasterZip.

60Table 3: Record processing rates when using
RasterZip or LZO for compressing data columns.

Dataset
HE
PN

LZO RasterZip
471K f/s
512K f/s

474K f/s
513K f/s

As shown in Table 3, RasterZip realizes almost identical
insertion rates with LZO, which is the fastest known com-
pressor of the reference Lempel-Ziv family. The average in-
sertion rate is close to 500 thousand ﬂows/second (f/s) for
both datasets regardless of the compression algorithm used.
To put this number into perspective, medium-large networks
exhibit peak rates of 50 thousand f/s. Therefore, we learn
that RasterZip substantially reduces the disk consumption
of LZO (and gzip) and supports very high insertion rates.

5.3 Adaptive Decompression Selection

Next, we evaluate the adaptive decompression component
to see if it can properly select between the full and partial
decompression strategies at run-time.

Proﬁling stage: We constructed the model for the adap-
tive decompression component by proﬁling the decompres-
sion speed of the two decompression strategies using real
network data and queries of diﬀerent selectivity. Speciﬁ-
cally, we created a set P containing the distinct IP addresses
present in one hour of traﬃc from the production network
dataset (PN). Then, we performed a query for every element
in P and for every data block that was decompressed to an-
swer the query we computed the following four metrics: the
compression ratio of the block (percentage), the selectivity
of the query (percentage), and the time required to decom-
press the block using full and partial decompression. We
used these metrics to proﬁle a very compact grid model of
size 8 × 8 bits as described in Section 4.3.

Evaluation stage: We evaluate the adaptive decompres-
sion component by comparing the block decompression time
it provides with the decompression time oﬀered by the full
and partial decompression strategies. For this purpose, we
issue the IP queries described above over a diﬀerent time
frame. In Figure 14 we illustrate the query response time
using full, partial, and adaptive decompression. The ﬁgure
on the bottom shows how the response time varies with se-
lectivity for a ﬁxed compression ratio (20%), while the one
on the top shows how the response time varies with the com-
pression ratio when the selectivity is ﬁxed to 20%. The line
corresponding to adaptive decompression is at the bottom
for the vast majority of the queries. This illustrates that the
adaptive decompression component eﬀectively selects the de-
compression strategy that is faster. This is feasible simply
by relying on a very compact model stored in a 8×8 bitmap.
We use this model for the rest of our evaluation experiments.

5.4 Overall Performance

Finally, we measure the overall query response time of the
archives compressed using RasterZip with the adaptive de-
compression component enabled. We compare the response
time of this setup with the response times that the system
oﬀers when LZO is used for data compression. Note that
the response time for answering queries, as we have shown
in our previous work [14], is dominated by the time spend

Figure 14: The Adaptive strategy chosen on-the-ﬂy
intelligently shifts between Full-Decompression and
Partial-Decompression. Shown for a ﬁxed compres-
sion ratio (bottom) and selectivity (top).

in decompressing data blocks rather than by the time spend
in looking up the compressed bitmap indexes to ﬁnd blocks
that need to be decompressed.

To compare the two compressors, we query for ﬂow records
using ports corresponding to well-known applications.6 For
this purpose, we extracted the 311 ports listed in the ﬁle
/etc/services of the test machine and executed destination
port queries over 7 days of each dataset (PN and HE).
This corresponds to 2,177 diﬀerent queries, each selecting
all the existing ﬂow records attributes (i.e.,"SELECT * FROM
flows WHERE dstPort=X" in SQL parlance). Note that us-
ing single- or multi-attribute queries, e.g., dstPort=X AND
dstIP=Y , does not play a role in the comparison between
LZO and RasterZip because it would only aﬀect the index
lookup time of a query equally for both compressors. What
is important is to compare the two compressors over a range
of query selectivity values. Varying the selectivity can be
done in multiple ways. In our experiments, we selected to
do this by issuing single-attribute queries as this is a simple

6The Internet Assigned Numbers Authority (IANA) assigns
port numbers to applications.

61way to do it. We ordered queries by the number of rows
retrieved (the selectivity) and, for each query, we plot in
Figures 15 and 16 the ratio between the response time when
the query is executed over a RasterZip compressed archive
and the response time for the same query executed over an
LZO compressed archive. Values lower than one correspond
to queries where RasterZip oﬀers better response times than
LZO.

Figure 15: RasterZip over LZO query response time
for the Production Network dataset (PN).

Figure 16: RasterZip over LZO query response
times for the Hosting Environment dataset (HE).

RasterZip provides better query response time than LZO
for more than 87% − 96% of the queries depending on the
dataset (see Figure 15 and 16 for the PN and HE dataset, re-
spectively). For high selectivity queries, the query response
time is in the order of milliseconds. The few cases where
RasterZip oﬀers worse response time than LZO correspond
to: a) a small fraction of the high selectivity queries (left top
side of the two ﬁgures) and b) to extremely low selectivity

queries. Examples of the second type of queries can be ob-
served on the extreme right side of Figure 16, which is based
on the data set from the hosting environment. These points
correspond to queries on port 80 (HTTP protocol), which, in
the hosting environment, match an extremely large fraction
of the ﬂows. Such queries of extremely low selectivity are
not typically useful for network administrators, since they
match too many ﬂows. Our system is optimized for high se-
lectivity queries, i.e., searching for “needles in a haystack”,
which are shown in the middle and left side of Figures 15
and 16.

6. RELATED WORK

General-purpose compression has been extensively stud-
ied in the previous decades, which led to a set of state-of-
the-art algorithms and associated utilities that are presently
extensively-used. The well-known Lempel-Ziv family of al-
gorithms [28] includes gzip and LZO, which is the fastest
member of the family [27]. Due to the high compression
and decompression speed, LZO is a compressor widely used
to accelerate I/O bound data-intensive workloads (e.g., in
Hadoop clusters [19]).

Although the available compression gain margins have be-
come extremely tight, in this work we show that by ex-
ploiting the patterns of network traﬃc data we can im-
prove the compression ratio of LZO (and also of gzip and
bzip2 ) by a signiﬁcant fraction of 22-24%, while reducing
query response times and without incuring any penalty. The
RadixZip Transform [24] is similar to our approach in that it
transposes bytes by the sort order of their preﬁxes to boost
the compression of token based streams, such as urls and
logs. RadixZIP oﬀers up to 20% better compression ratios
and up to 10% higher decompression speed than bzip2, the
widely used block based compressor based on the Burrows-
Wheeler Transform (BWT). In contrast with RadixZip and
bzip2, which are optimized for achieving high compression
ratios, RasterZip is optimized for speed and achieves much
faster (de)compression speeds. Additionally, it has been de-
signed to support ﬁne-grained partial decompression opera-
tions, which are hard to accommodate in bzip2 even with the
help of an index. In fact, bzip2 -compressed data is made of
data blocks that can be independently decompressed. How-
ever, the block size is large (between 100Kb and 900Kb),
and as a result, even highly selective queries can require the
decompression of the entire archive.

Silk [15], nfdump [17], and ﬂowtools [22] are commonly-
used tools for storing streams of ﬂows. They all store ﬂow
records as ﬂat ﬁles, which are eventually compressed using
general purpose compressors and compression algorithms,
such as gzip, bzip2, and LZO.

Recently, compression algorithms have been used to boost
the performance of columnar databases. A comparison of
the performance of diﬀerent general-puropose compression
algorithms for columnar databases can be found in [2]. In
our previous work [14], we built a columnar database that
uses a novel indexing scheme optimized for network ﬂow
records and the LZO general-puropose compressor. In this,
work we visit the subject of compression and introduce a
domain-optimized compression scheme that provides ﬁne-
grained decompression granularity and high compression ef-
ﬁciency. Recently, Giura et al [16] optimized a columnar
database called NetStore for storing ﬂows. NetStore pro-
vides the capability to select, at runtime, among diﬀerent

62standard compression tools. However, when deployed on a
signiﬁcantly less powerful machine (4 cores instead of 8, 2
Gb of RAM instead of 6 Gb), our solution provides insertion
rates that are two orders of magnitude higher.

7. WHAT IS THE COST OF STORAGE?

The costs for storing and maintaining large repositories is
commonly underestimated, mostly because the price of desk-
top hard drives dropped signiﬁcantly in the last few years.
Unfortunately, the disk space is cheap motto only holds
at small scales. Historical network traﬃc repositories can
be massive. For example, large traﬃc archives maintained
by academic and research institutions, like the ﬂow record
repository of the Communication Systems Group at ETH
and the UCSD Network Telescope data of CAIDA, are both
close to 100 Terabytes [4]. Organizations, like medium and
large ISPs, cloud providers, and law enforcement agencies,
monitor much more traﬃc and need to handle even bigger
traﬃc data. Large-scale repositories must be highly avail-
able and fault-tolerant to prevent data loss. Fault-tolerance
can be achieved by replicating data in a distributed set-
ting (e.g., the Google approach) or by using enterprise class
storage products (e.g, disk arrays with RAID support). In
both cases, the operational costs include additional expenses
for energy consumption and for performing backups over
tape drives. Based on current storage prices [5], the cost
for storing 100 Terabytes is, for example, between $40,000
and $72,000 per year for diﬀerent types of service. High
storage costs prevent the wider availability of rich Internet
measurement data, and are, in fact, the sole reason why even
existing traﬃc repositories that have been very inﬂuencial in
studying Internet threats are threatened with deletion [4]. A
compression technology that can substantially reduce data
volume has a direct and very tangible economical impact,
which can be measured in several thousands of dollars per
year for few dozens of Terabytes. RasterZip, by compressing
more than 20% better than gzip, reduces costs accordingly.

8. CONCLUSION

In this work we introduce RasterZip, the ﬁrst high-
performance compressor speciﬁcally designed for network
traﬃc archives that exploits the preﬁx structure of par-
tially reordered data to provide ﬁne-grained decompression
granularity. RasterZip supports online high-speed stream
compression, enables interactive queries over archived data,
leverages network data patterns, and exploits features of
modern hardware.
In addition, we introduce a novel de-
compression scheme that provides ﬁne-grained decompres-
sion granularity at the attribute level in order to accel-
erate queries for “needles in a haystack”. Our evaluation
shows that RasterZip outperforms general-purpose compres-
sors on traﬃc ﬂow data from two diﬀerent networks. Com-
pared to the widely-used state-of-the-art lzop compressor,
RasterZip accelerates queries, reduces expenses for storage
by 22-24% due to more compact compression, and does not
incur penalty in any other performance metric.

9. REFERENCES
[1] StealthWatch FlowCollector.

http://www.lancope.com/products/stealthwatch-
ﬂowcollector/.

[2] D. Abadi, S. Madden, and M. Ferreira. Integrating

Compression and Execution in Column-Oriented

Database Systems. In Proc. 32nd ACM SIGMOD Int.
Conf. on Management of Data, pages 671–682, 2006.

[3] E. W. Bethel, S. Campbell, E. Dart, K. Stockinger,

and K. Wu. Accelerating Network Traﬃc Analysis
Using Query-Driven Visualization. In Proc. of IEEE
Symposium on Visual Analytics Science and
Technology, pages 115–122, 2006.

[4] CAIDA. Targeted serendipity: the search for storage.

http://blog.caida.org/best_available_data/
2012/04/04/, 2012.

[5] S. D. S. Center. Sdsc project storage pricing options.

http://project.sdsc.edu/pricing.php.

[6] X. Chen, M. Li, B. Ma, and J. Tromp. DNACompress:

fast and eﬀective DNA sequence compression.
Bioinformatics, 18(12):1696–1698, 2002.

[7] B. Claise. RFC 3954: Cisco Systems NetFlow Services

Export Version 9, 2004.

[8] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni.

Locality-Sensitive Hashing Scheme Based on p-Stable
Distributions. In Proc. of the 20th Annual Symposium
on Computational Geometry, pages 253–262, 2004.
[9] L. Deri, V. Lorenzetti, and S. Mortimer. Collection

and Exploration of Large Data Monitoring Sets Using
Bitmap Databases. In Proc. of the 2nd Int. Workshop
on Traﬃc Monitoring and Analysis, TMA’10, pages
73–86, 2010.

[10] N. Duﬃeld, C. Lund, and M. Thorup. Charging from

Sampled Network Usage. In Proc. of the 1st ACM
SIGCOMM Workshop on Internet Measurement
(IMW), pages 245–256, 2001.

[11] A. Feldmann, A. Greenberg, C. Lund, N. Reingold,

J. Rexford, and F. True. Deriving Traﬃc Demands for
Operational IP Networks: Methodology and
Experience. IEEE/ACM Transactions on Networking
(ToN), 9:265–280, 2001.

[12] A. Friedl and S. Ubik. Perfmon and Servmon:

Monitoring Operational Status and Resources of
Distributed Computing Systems. Technical Report 10,
CESNET, Prague, Czech Republic, 2008.

[13] F. Fusco, X. Dimitropoulos, M. Vlachos, and L. Deri.

pcapindex: an index for network packet traces with
legacy compatibility. SIGCOMM Comput. Commun.
Rev., 42(1):47–53.

[14] F. Fusco, M. Vlachos, and M. P. Stoecklin. Real-time

creation of bitmap indexes on streaming network data.
The VLDB Journal, 21(3):287–307, June 2012.

[15] C. Gates, M. Collins, M. Duggan, A. Kompanek, and
M. Thomas. More Netﬂow Tools for Performance and
Security. In Proc. of the Conf. on Large Installation
Systems Administration, pages 121–132, 2004.

[16] P. Giura and N. Memon. Netstore: an eﬃcient storage
infrastructure for network forensics and monitoring. In
Proc. of the 13th Int. Conf. on Recent advances in
intrusion detection, RAID’10, pages 277–296, 2010.
[17] P. Haag. NFDump. http://nfdump.sourceforge.net/.
[18] R. Hofstede, A. Sperotto, T. Fioreze, and A. Pras. The

network data handling war: MySQL vs. NfDump. In
Proc. of the 16th EUNICE/IFIP Conf. on Networked
services and applications: engineering, control and
management, EUNICE’10, pages 167–176, 2010.
[19] S. B. Joshi. Apache hadoop performance-tuning

methodologies and best practices. In Proc. of the 3rd
joint WOSP/SIPEW Int. Conf. on Performance
Engineering, ICPE ’12, pages 241–242, 2012.

[20] A. Lakhina, M. Crovella, and C. Diot. Mining

Anomalies Using Traﬃc Feature Distributions. In
Proc. ACM SIGCOMM Conf. on Applications,
Technologies, Architectures, and Protocols for
Computer Communications, pages 217–228, 2005.

[21] T. W. Lam, W.-K. Sung, S.-L. Tam, C.-K. Wong, and

S.-M. Yiu. Compressed indexing and local alignment
of DNA. Bioinformatics, 24(6):791–797, 2008.

63[22] S. Romig, M. Fullmer, and R. Luman. The OSU

[26] H. Yan, S. Ding, and T. Suel. Compressing term

Flow-tools Package and Cisco NetFlow Logs. In Proc.
Conference on Large Installation Systems
Administration (LISA), pages 291–303, 2000.

[23] D. Salomon. Data Compression: The Complete

Reference. Springer-Verlag, 2nd edition, 2000.

[24] B. D. Vo and G. S. Manku. RadixZip: Linear Time

Compression of Token Streams. In Proc. Int. Conf. on
Very Large Data Bases, pages 1162–1172, 2007.

[25] A. Wagner. Entropy-Based Worm Detection for Fast

IP Networks. PhD thesis, ETH Zurich, 2008.

positions in web indexes. In Proc. of the 32nd ACM
SIGIR Conf. on Research and development in
information retrieval, SIGIR ’09, pages 147–154, 2009.
[27] L. Yang, R. P. Dick, H. Lekatsas, and S. Chakradhar.

Online memory compression for embedded systems.
ACM Trans. Embed. Comput. Syst., 9:27:1–27:30,
2010.

[28] J. Ziv and A. Lempel. A Universal Algorithm for

Sequential Data Compression. IEEE Transactions on
Information Theory, 23(3):337–343, 1977.

64