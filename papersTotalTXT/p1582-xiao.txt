Mitigating Storage Side Channels

Using Statistical Privacy Mechanisms

Qiuyu Xiao

Michael K. Reiter

University of North Carolina

University of North Carolina

Chapel Hill, NC, USA
qiuyu@cs.unc.edu

Chapel Hill, NC, USA
reiter@cs.unc.edu

Yinqian Zhang

The Ohio State University

Columbus, OH, USA

yinqian@cse.osu.edu

ABSTRACT
A storage side channel occurs when an adversary accesses
data objects inﬂuenced by another, victim computation and
infers information about the victim that it is not permitted
to learn directly. We bring advances in privacy for statisti-
cal databases to bear on storage side-channel defense, and
speciﬁcally demonstrate the feasibility of applying diﬀeren-
tially private mechanisms to mitigate storage side channels
in procfs, a pseudo ﬁle system broadly used in Linux and
Android kernels. Using a principled design with quantiﬁ-
able security, our approach injects noise into kernel data-
structure values that are used to generate procfs contents,
but also reestablishes invariants on these noised values so
as to not violate assumptions on which procfs or its clients
depend. We show that our modiﬁcations to procfs can be
conﬁgured to mitigate known storage side channels while
preserving its utility for monitoring and diagnosis.

Categories and Subject Descriptors
D.4.6 [OPERATING SYSTEMS]: Security and Protec-
tion—Information ﬂow controls

General Terms
Security

Keywords
Side channels; diﬀerential privacy

1.

INTRODUCTION

Side-channel attacks aim at disclosing data in computer
systems by exﬁltrating sensitive information through inter-
faces that are not designed for this purpose. In recent years,
the scope of side-channel attacks has been extended beyond
their traditional use to attack cryptographic keys, and tech-
niques utilized in side-channel analysis have also increased
in variety and sophistication.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the Owner/Author(s).
Copyright is held by the owner/author(s).
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
ACM 978-1-4503-3832-5/15/10.
DOI: http://dx.doi.org/10.1145/2810103.2813645.

In this paper, we examine one particular type of side-
channel attack vector, which we call storage side channels.
Storage side channels occur when an adversary accesses data
objects associated with a victim computation and makes in-
ferences about the victim based on the contents of the data
objects themselves or their metadata. As we use the term
here, storage side channels form a subclass of storage covert
channels [35] that gleans information from an unwitting vic-
tim, versus receiving information inconspicuously from an
accomplice. Storage side (and covert) channels diﬀer from
legitimate communication channels since the data value or
the metadata exploited by the side channel is not considered
sensitive by itself; yet, it still leaks information that may be
exploited to infer victim secrets.

A generic approach to mitigate storage side (and covert)
channels is to reduce the accuracy of the data or its meta-
data being reported by adding random noise to disturb side-
channel observations [35]. A challenge in this approach is to
develop principled mechanisms to perturb the side channels
with provable security guarantees, and to do so while pre-
serving the utility of the data and metadata in the system.
In this paper, we present a novel approach to doing so by
leveraging privacy concepts in storage side-channel defense.
By limiting data reporting to conform to diﬀerential privacy
and generalizations thereof, we show how to introduce noise
into the data reporting so as to bound information leakage
mathematically. The diﬃculties in doing so, however, stem
from the challenges in (i) modeling these storage channels
as statistical databases, where diﬀerential privacy was pre-
viously applied; (ii) designing privacy mechanisms to add
noise so that side channels are provably mitigated; and (iii)
designing these mechanisms so as to minimize the loss of util-
ity of the released data. We will discuss methods to address
these challenges in the remaining sections of this paper. In
theory, these methods can be applied to mitigating a variety
of storage side channels. However, in this paper we illus-
trate the idea by focusing only on storage channels based on
procfs, a ﬁle-system interface for reporting resource usage
information on Linux and Android systems.

Toward this end, we propose a modiﬁed procfs, dubbed
dpprocfs, that provides guarantees about the inferences pos-
sible from values reported through the procfs interfaces. In
doing so, dpprocfs defends against a variety of storage side
channels recently exploited in procfs on both Linux and
Android (see Sec. 3.1 for a summary of these attacks). Our
work builds on the works of Dwork et al. [20, 21] and Chan et
al. [12], which consider diﬀerential privacy under continuous
observations, but we are forced to extend from this starting

1582point in multiple ways. First, diﬀerential privacy itself is
not a good match for side-channel mitigation in the procfs
context; rather, we turn to a recent generalization called
d-privacy [13] that is parameterized by a distance metric
d. By deﬁning a suitable distance metric d and expressing
side-channel mitigation goals in terms of the distance be-
tween two series of procfs observations, we prove that the
diﬀerentially private mechanism of Chan et al. [12] general-
izes to mitigate storage side channels. Second, however, the
naive application of this mechanism to noise procfs out-
puts would risk correctness of applications that depend on
invariants that procfs outputs satisfy in practice. To re-
tain the utility of procfs, dpprocfs therefore extracts and
reestablishes invariants on the noised outputs so as to assist
applications that depend on them.

We implemented dpprocfs for Linux as a suite that con-
sists of an extension of the Linux kernel, a userspace daemon
process, and a software tool that is used for generating in-
variants on the values of kernel data structures oﬄine. The
kernel extension alters the functionality of procfs to enforce
d-privacy on the exported data values while preserving the
standard procfs interfaces. The userspace daemon inter-
acts with the kernel extension to reestablish the invariants
procfs satisﬁes. We will elaborate on our implementation
choices in later sections.

We evaluate our prototype for both its security and utility.
For security, we demonstrate conﬁgurations that eﬀectively
mitigate existing procfs side-channel attacks from the liter-
ature. We speciﬁcally demonstrate preventing two attacks,
one that uses procfs data to measure keystroke behavior as
a means to recover a typed input, and another that mon-
itors the resource usage of a browser process to determine
the website it is accessing [25]. We evaluate the utility of
dpprocfs by measuring the relative error of protected ﬁelds
and the similarity of the resource-use rankings of processes
by the popular top utility to those rankings without noise.

In summary, our contributions are as follows:

• We bring advances in privacy for statistical databases
to bear on storage side-channel defense. Speciﬁcally, we
show that an existing mechanism due to Chan et al. [12]
for enforcing diﬀerential privacy under continuous binary
data release extends to implement d-privacy for a dis-
tance metric d∗ that can quantify storage side channels
in procfs. We deﬁne this distance metric d∗, argue its
utility for capturing storage side channels, and prove that
the Chan et al. mechanism implements d∗-privacy.

• We identify a challenge in inserting noise into procfs
outputs, namely the violation of invariants that procfs
clients (and procfs code itself) might depend. Drawing
from previous research in invariant identiﬁcation, we de-
velop a tool for extracting invariants and imposing them
upon noised values prior to returning procfs outputs. In
doing so, we ensure that procfs outputs are consistent,
even while being noised to interfere with side channels.

• We develop a working implementation of dpprocfs, our
variant of procfs that implements storage side-channel
defense, and evaluate both the protection it oﬀers against
previously published attacks and the utility it oﬀers for
monitoring and diagnosis. Our results illustrate that
side-channel defense can be accomplished while still main-
taining the utility of procfs for its intended purposes.

The remainder of this paper is organized as follows. Sec. 2
summarizes related work. Sec. 3 provides an overview of
storage side channel attacks via procfs, and the theoretical
basis of d-privacy. Sec. 4 presents our design of dpprocfs,
which is followed by details of its implementation in Sec. 5.
We evaluate both the security and utility of dpprocfs in
Sec. 6 and discuss remaining challenges in Sec. 7. We con-
clude the paper in Sec. 8. The proofs of propositions stated
in this paper can be found in App. A.

2. RELATED WORK

Relevant to our work is privacy in the context of statistical
databases. Statistical database systems allow users to query
aggregate statistics of subsets of entities in the database.
Privacy concerns arise when a database client learns infor-
mation about individuals represented in the database through
one or multiple queries to the database [4]. This concern has
driven decades of innovation in stronger privacy deﬁnitions
in this context and algorithms to achieve them (e.g., [4, 38,
41, 29, 18, 27, 36, 8]). Of particular interest here is diﬀeren-
tial privacy and extensions thereof; please see the works due
to Dwork [19] and Fung et al. [23] that survey advances in
diﬀerential privacy and compare it to other privacy models,
respectively.

Prior to our work, diﬀerential privacy has been imple-
mented in practical systems, e.g., to support privacy for
data accessed through SQL-like queries [32] or MapReduce
computations [37]. The security scenarios we consider, how-
ever, diﬀer from the statistical database privacy model in
two dimensions: First, storage side channels revolve around
information leakage due to an attacker continuously mon-
itoring the same data as it changes over time. Statistical
databases are typically static, however. Second, database
indistinguishability is not well deﬁned under our security
model, and hence we need to adapt the deﬁnition of diﬀer-
ential privacy for our intended purposes.

We build our work upon two lines of research in the lit-
erature. The ﬁrst line is concerned with diﬀerential privacy
with continuous data release [20, 21, 12].
In these works,
the continuous data release takes the form of a sequence of
binary values, and only sequences that diﬀer in a single bi-
nary value are rendered indistinguishable to the attacker.
In the model we consider, in contrast, the continuous data
release can be characterized as a sequence of integers, and
even sequences that diﬀer in multiple values might need to
be rendered indistinguishable. The second line of research
generalizes the deﬁnition of diﬀerential privacy for statistical
databases. In particular, Chatzikokolakis et al. [13] broad-
ened the deﬁnition of diﬀerential privacy by parameterizing
the deﬁnition with a distance metric d, and requiring that
the degree of indistinguishability of two databases be a func-
tion of their distance. (The original deﬁnition of diﬀerential
privacy can be viewed as a special case for Hamming dis-
tance [13].) We build from this approach, deﬁning a metric
d that applies to storage side channels and implementing
this defense in a working system.

While several prior works also extend the deﬁnition of dif-
ferential privacy to settings that are not statistical databases
(e.g., geo-location services [5, 9] and smart metering [3, 2, 17,
26, 31, 42, 7]), our work is the ﬁrst to our knowledge to ap-
ply diﬀerential privacy concepts in operating system security
and side-channel defense. Moreover, the domain of storage
side-channel defense introduces important diﬀerences that

1583require innovation. In particular, since the values that our
system must perturb to interfere with side channels are ones
that are used by other software, it is important that our
modiﬁcations do not violate invariants on which that soft-
ware depends. To our knowledge, this aspect distinguishes
the problem we address from work in geo-location and smart
metering and drives us to a novel design as discussed in the
balance of the paper.

3. BACKGROUND

3.1 Side Channel Attacks via PROCFS

procfs is a pseudo ﬁle system implemented in Linux, An-
droid, and a few other UNIX-like operating systems to fa-
cilitate userspace applications’ accesses to kernel-space in-
formation. Two types of information are typically shared
through procfs: per-process information and system-wide
information. Per-process information reveals conﬁguration
and state information about a process, including path of the
executable, environment variables, size of virtual and phys-
ical memory, CPU and network usage, and so on. While
some of the information should only be consumed by the
process itself, other information, especially statistics about
resource usage, is required for performance monitoring and
diagnosis. For instance, in Linux, top, ps, iostat, netstat,
pidstat, and others rely on procfs to function. In Android,
procfs is used for apps to monitor the resource usage, e.g.,
transferred network data, of other apps.

This useful facility has been exploited to conduct side-
channel attacks by several prior works. Particularly of in-
terest in this paper are the attacks exploiting publicly avail-
able per-process information to infer secrets of the targeted
process; see Table 1 for examples. The techniques under-
lying these attacks are similar. Jana et al. [25] introduced
an attack that, by reading from a ﬁle in procfs, /proc/
<pid>/statm, and learning the data resident size (drs) of
a Chrome browser, enables a malicious co-located applica-
tion to infer the website it is visiting. The feature used to
diﬀerentiate multiple websites being browsed is the snap-
shot of the application’s memory footprint. Zhou et al. [44]
explored ways in Android to infer a victim app’s activity
by monitoring its network communications. Speciﬁcally,
by sampling the ﬁles /proc/uid_stat/<uid>/tcp_rcv and
/proc/uid_stat/<uid>/tcp_snd, an adversary is able to
learn the packet sizes sent and received by the victim app
with high accuracy. Chen et al. [14] extracted the victim
app’s CPU utilization time, memory usage, and network us-
age from various procfs ﬁles to classify the application’s
behaviors. Lin et al. [28] also used utime to recognize a
user’s operation of the software keyboard on Android.

3.2 d-Privacy

In this paper we leverage a generalization of diﬀerential
privacy due to Chatzikokolakis et al. [13] called d-privacy,
which we summarize here brieﬂy. (Our summary is not of
the most general form of d-privacy, however.) A metric d on
a set X is a function d : X 2 → [0, ∞) satisfying d(x, x) = 0,
d(x, x′) = d(x′, x), and d(x, x′′) ≤ d(x, x′) + d(x′, x′′) for all
x, x′, x′′ ∈ X . A randomized algorithm A : X → Z satisﬁes
(d, ǫ)-privacy if

P (A(x) ∈ Z) ≤ exp(ǫ × d(x, x′)) × P¡A(x′) ∈ Z¢

for all Z ⊆ Z.

We leverage the following composition property of d-privacy:

Proposition 1. If A : X → Z is (d, ǫ)-private and A′ :
X → Z ′ is (d, ǫ′)-private, then A′′ : X 2 → Z × Z ′ deﬁned
by A′′(x, x′) = (A(x), A′(x′)) satisﬁes

P¡A′′(x, x′) ∈ Z × Z ′¢ ≤ exp(ǫ × d(x, x′′) + ǫ′ × d(x′, x′′′))

× P¡A′′(x′′, x′′′) ∈ Z × Z ′¢

for any Z ⊆ Z, any Z ′ ⊆ Z ′, and any x, x′, x′′, x′′′ ∈ X .

Let Z and R denote the integers and reals, respectively.
In the case X = Zn, a metric that will be of interest for our
purposes is L1 distance, deﬁned by

dL1(x, x′) =

n

Xi=1

|x[i] − x′[i] |

where x = hx[1] , . . . , x[n]i.

Proposition 2. Let A : Zn → Rn be the algorithm that
$←

returns A(x) = hx[1] + r1, . . . , x[n] + rni, where each ri

Lap¡ 1

ǫ¢. Then, for any x, x′ ∈ Zn and Z ⊆ Rn,
P (A(x) ∈ Z) ≤ exp(ǫ × dL1(x, x′)) × P¡A(x′) ∈ Z¢

4. DESIGN OF A d-PRIVATE PROCFS

In an eﬀort to suppress information leakages in procfs
such as those described in Sec. 3.1, we devise a new procfs-
like ﬁle system, called dpprocfs, that leverages diﬀerential
privacy principles. In this section, we describe how we apply
these principles in the design of dpprocfs.

4.1 Threat Model

This paper considers side-channel attacks exploiting statis-
tics values exported by procfs from co-located applications
running within the same OS. In particular, we consider the
default settings of procfs, which do not restrict accesses to
a process’ private directories in procfs by other processes.
Such settings are very typical in traditional desktop envi-
ronments or shared server hosting environments running all
kinds of Linux distributions, and mobile devices running An-
droid. We assume the OS kernel and the root user of the
system are not compromised. Accordingly, security attacks
due to software vulnerabilities are beyond the scope of con-
sideration.

4.2 Design Overview

When a procfs ﬁle is open and read, the data read are cre-
ated on-the-ﬂy by the Linux kernel. To create the ﬁle data,
the kernel draws information from several data structures.
Examples include the task_struct structure that describes
a process or task in the system, and the mm_struct structure
that describes the virtual memory of a process.

One option to interfere with adversary inferences about
victim processes using values obtained from procfs would
be to add noise to those values directly, just before out-
putting them. Unfortunately, there are numerous outputs
from procfs with complex relationships among them, and
so we determined that adding noise to the underlying kernel
data-structure ﬁeld values used to calculate procfs outputs
would be a more manageable design choice. In particular,
there are fewer such ﬁelds, and while there remain relation-
ships among them (more on that below), they are reduced
in number and complexity.

1584Reference

Description

procfs ﬁles used

Underlying kernel data-structure ﬁelds

Jana et al. [25] Memory footprint and
context switches of a
browser process leak
website it visits

/proc/<pid>/statm
/proc/<pid>/status
/proc/<pid>/schedstat

mm_struct.total_vm
mm_struct.shared_vm
task_struct.nvcsw
task_struct.nivcsw

/proc/uid_stat/<uid>/tcp_rcv
/proc/uid_stat/<uid>/tcp_snd

uid_stat.tcp_rcv
uid_stat.tcp_snd

/proc/<pid>/statm
/proc/<pid>/stat
/proc/uid_stat/<uid>/tcp_rcv
/proc/uid_stat/<uid>/tcp_snd

mm_struct.shared_vm
mm_struct.rss_stat.count[MM_FILEPAGES]
mm_struct.rss_stat.count[MM_ANONPAGES]
uid_stat.tcp_rcv
uid_stat.tcp_snd
task_struct.utime
task_struct.utime

Zhou et al. [44]

Sizes of network pack-
ets to/from Android
app leaks its activity

foreground
Chen et al. [14] Android
activity
identiﬁed
using shared memory,
CPU utilization time
and network activity

Lin et al. [28]

Use of software key-
board detected using
CPU utilization time

/proc/<pid>/stat

Table 1: Selected attacks leveraging storage side channels in the procfs ﬁle system

So, in the design of dpprocfs, we treat updates to the
relevant per-process kernel data structures as constituting
a “database” x that represents the evolution of the process
since its inception. That is, consider a conceptual database
x to which a record is added each time one or more of a pro-
cess’ kernel data-structure ﬁelds changes. The columns of
x correspond to the numeric ﬁelds of the per-process kernel
data structures consulted by procfs. So, for example, the
mm_struct.total_vm ﬁeld, which indicates the total num-
ber of virtual memory pages of a process, is represented by
a column in x. As the process executes, a new record is ap-
pended to x anytime the value in one of these ﬁelds changes.
Each time a procfs ﬁle is read, the values returned are as-
sembled from what is, in eﬀect, the most recently added row
of the database x. We stress, however, that this database is
conceptual only, and does not actually exist in dpprocfs.

We design an algorithm to implement d-privacy per col-
umn of x (i.e., per data-structure ﬁeld), relying on Prop. 1 to
bound the information leaked from multiple columns simul-
taneously. Since each column of the database x corresponds
to a speciﬁc ﬁeld in a kernel data structure, our mechanism
is applied each time a ﬁeld in a protected data structure is
read by procfs code. For the remainder of this paper, we
adjust our notation so that the database x represents a sin-
gle column corresponding to that data-structure ﬁeld. We
refer to x[i] as the value of the last element of that column
(i.e., the ﬁeld in the kernel data structure corresponding to
the column) when the i-th access occurs (i.e., i = 1 is the
ﬁrst access to the data-structure ﬁeld).

Even to limit leakage from a single column, it is necessary
to decide on a distance metric d for which to implement
d-privacy. While we might not know exactly how the ad-
versary uses the procfs outputs to infer information about
a victim process, we can glean guidance from known at-
tacks. For example, Zhou et al. [44] discuss how they used
procfs output based on the uid_stat.tcp_snd ﬁeld to in-
fer when a victim sent a tweet (a la Twitter) as follows:
“a tweet is considered to be sent when the increment se-
quence is either (420|150, 314, 580–720) or (420|150, 894–
1034).” [44, Sec. 3.2] That is, their attack works by reading
from procfs four times in a short interval to obtain values
x[1], x[2], x[3], x[4] where x denotes the uid_stat.tcp_snd
ﬁeld, and deciding that a tweet was sent if either x[2]−x[1] ∈

{150, 420}, x[3]−x[2] = 314, and x[4]−x[3] ∈ {580, . . . , 720}
or x[2] − x[1] ∈ {150, 420} and x[3] − x[2] ∈ {894, . . . , 1034}.
So, to interfere with this attack, it is necessary to render
these readings from the “database” x indistinguishable from
readings from an alternative “database” x′ that reﬂects a run
in which no tweet was sent. This insight led us to choose
the following metric d∗ for enforcing privacy:

d∗(x, x′) = Xi≥1

|(x[i] − x[i − 1]) − (x′[i] − x′[i − 1])|

Proposition 3. d∗ is a metric.

The distance d∗ captures the distinguishability of consecu-
tive pairs of observations of a data-structure ﬁeld via procfs,
and so by deﬁning d∗ in this way (and choosing ǫ appropri-
ately), we ensure that a (d∗, ǫ)-private mechanism can hide
the diﬀerences between x and x′ that, e.g., enabled Zhou et
al. to identify a tweet being sent in their attack.

Moreover, adopting d∗ is plausibly of use in defending
against a much broader range of attacks, since d∗-privacy
implies dL1-privacy:

Proposition 4. If A is (d∗, ǫ)-private, then A is (dL1, 2ǫ)-

private.

Since any p-point metric space can be embedded in L1 dis-
tance with O(log p) distortion [1], making it diﬃcult to dis-
tinguish x and x′ with low d∗ (and hence L1) distance should
make it more diﬃcult to distinguish them via other distance
metrics, too.

One challenge of using d-privacy to protect information
from kernel data structures used in responding to procfs
reads is that the information obtained through procfs might
become inconsistent. That is, our mechanism might break
data-structure invariants on which the procfs code or the
clients of procfs rely. dpprocfs therefore reestablishes these
invariants on the d-private values prior to providing them to
procfs code. So, for example, since enforcing d-privacy adds
noise to the mm_struct.total_vm and mm_struct.shared_vm
values, the resulting values might fail to satisfy the invariant
mm_struct.total_vm ≥ mm_struct.shared_vm. dpprocfs
thus adjusts mm_struct.total_vm and mm_struct.shared_vm
to reestablish this invariant before permitting them to be

1585used by the procfs code.
In Sec. 4.4, we describe how
we generate the invariants for these kernel data structures
and how we reestablish those invariants on d-private values.
Note that these invariants are public information: they can
be extracted statically or dynamically via the same meth-
ods we obtain them, and post-processing d-private values
to reestablish these invariants does not impinge on their d-
privacy (cf., [24]).

4.3 d∗-Private Mechanism Design

In this section we describe the mechanism we use to imple-
ment d∗-privacy for the conceptual single-column database x
described above. This mechanism is due to Chan et al. [12],
though they considered only the case where x[i + 1] − x[i] ∈
{0, 1} and, moreover, diﬀerential privacy (so that x[i + 1] −
x[i] 6= x′[i + 1] − x′[i] for only one i), rather than d∗-privacy
as we do here. As such, our primary contribution is in prov-
ing that this mechanism generalizes to implement d∗-privacy
and does so for vectors over the natural numbers.

Let N denote the natural numbers and D(i) ∈ N denote
the largest power of two that divides i; i.e., D(i) = 2j if and
only if 2j |i and 2j+16 | i. Note that i = D(i) if and only if i
is a power of two. The mechanism A computes a value ˜x[i]
that is used in place of x[i] in the procfs code using the
recurrence

˜x[i] = ˜x[G(i)] + (x[i] − x[G(i)]) + ri

(1)

where x[0] = ˜x[0] = 0, Lap (b) denotes the Laplace distribu-
tion with scale b and location µ = 0, and

G(i) = 

ri ∼ 


0

i/2

if i = 1

if i = D(i) ≥ 2

i − D(i)

if i > D(i)

Lap¡ 1
ǫ¢
Lap³ ⌊log2 i⌋

ǫ

if i = D(i)

´ otherwise

(2)

(3)

So, for example, the ﬁrst eight queries to x result in the
$← Lap (b) denotes sam-

following return values, where ri
pling randomly according to the distribution Lap (b).

˜x[1] ← x[1] + r1

where r1

˜x[2] ← ˜x[1] + (x[2] − x[1]) + r2 where r2

˜x[3] ← ˜x[2] + (x[3] − x[2]) + r3 where r3

˜x[4] ← ˜x[2] + (x[4] − x[2]) + r4 where r4

˜x[5] ← ˜x[4] + (x[5] − x[4]) + r5 where r5

˜x[6] ← ˜x[4] + (x[6] − x[4]) + r6 where r6

˜x[7] ← ˜x[6] + (x[7] − x[6]) + r7 where r7

˜x[8] ← ˜x[4] + (x[8] − x[4]) + r8 where r8

$← Lap¡ 1
ǫ¢
$← Lap¡ 1
ǫ¢
$← Lap¡ 1
ǫ¢
$← Lap¡ 1
ǫ¢
$← Lap¡ 2
ǫ¢
$← Lap¡ 2
ǫ¢
$← Lap¡ 2
ǫ¢
$← Lap¡ 1
ǫ¢

Chan et al. characterize the amount of noise introduced
by the mechanism described above, which grows only loga-
rithmically in i, speciﬁcally:

Proposition 5

([12]). With probability at least 1 − δ,

|˜x[i] − x[i] | = O³(log 1

δ ) × (⌊log i⌋)3/2 × ǫ−1´.

Our main contribution as it relates to this mechanism de-

sign lies in showing the following result:

Proposition 6. The algorithm in Eqns. 1–3 is (d∗, 2ǫ)-

private.

4.4 Consistency Enforcement

The values provided to procfs code, once rendered d∗-
private by the mechanism described in Sec. 4.3, are pro-
cessed as usual by the procfs code to produce the values
served as the contents of the queried procfs ﬁles. By adding
noise to these values, however, it is possible that we cause
them to violate invariants on which the procfs code or the
reader of the procfs ﬁles depends. As such, prior to pro-
viding the d∗-private values to the procfs code, we process
these values to re-establish invariants on which this code
might depend.

Speciﬁcally, the invariants we reestablish are of two types,
namely one-ﬁeld or multiple-ﬁeld. A one-ﬁeld invariant holds
between the values of the same data-structure ﬁeld when
queried at two diﬀerent times. For example, the fact that
the task_struct.utime ﬁeld is monotonically nondecreasing
is a one-ﬁeld invariant. In contrast, a multiple-ﬁeld invariant
holds among the values of two or more data-structure ﬁelds
accessed at the same time, e.g., mm_struct.hiwater_rss <
mm_struct.shared_vm. There could also be invariants that
hold among the values of two or more data-structure ﬁelds
accessed at diﬀerent times, though we do not consider such
invariants here.

Techniques for invariant identiﬁcation range from static
(e.g., [45]) to dynamic (e.g., [22]) and combinations thereof
(e.g., [16]). While dpprocfs is agnostic to the method of
invariant generation, the type we explored for our proto-
type is dynamic.
Intuitively, in this approach we execute
the system under a variety of workloads, taking snapshots
of the relevant kernel data structures after they are updated.
We then post-process these snapshots to identify properties
that held consistently in all executions. Obviously we can-
not detect all such properties (there are inﬁnitely many that
could be inferred from ﬁnitely many traces), nor is identify-
ing all of them strictly necessary. (We return to this issue in
Sec. 7.) In Sec. 5.2, we detail the invariants that dpprocfs
enforces in our current implementation, though we stress
that these invariants can be generated through a combina-
tion of techniques—including manually.

Enforcing these invariants involves processing the data-
structure ﬁeld values output by the d∗-private mechanism
described in Sec. 4.3 to satisfy these invariants. More specif-
ically, any attempt to read from a procfs ﬁle will cause an
access to certain data-structure ﬁelds. The values in these
ﬁelds and in any other ﬁelds related to them by multi-ﬁeld
invariants (even transitively) are each subjected to the d∗-
private mechanism of Sec. 4.3, producing a noised value ˜x[i]
to replace the actual value x[i] in this, the i-th, access to
this ﬁeld. These outputs are then altered to satisfy relevant
single-ﬁeld and multiple-ﬁeld invariants, resulting in a ﬁnal
output ˆx[i] for further processing by the kernel routine that
produces the contents of the accessed procfs ﬁle.

In Sec. 5.3, we explore two ways of manipulating these
outputs to satisfy invariants. In the ﬁrst, to which we refer
as computing a heuristic solution to the invariants, dpprocfs
leverages a hand-implemented algorithm to deterministically
modify the outputs to conform. This method is very eﬃ-
cient, but might alter the outputs more than other ways of
satisfying the invariants might. In the second approach, to
which we refer as computing the nearest solution to the in-

1586variants, we generate an integer programming problem with
the invariants as constraints and an objective of minimizing
the total magnitude of the changes to the d∗-private out-
puts to conform to the invariants. We then feed this integer
program to a commercial solver (in our current implementa-
tion, CPLEX1) to compute an optimal solution. We stress
that both the heuristic and nearest solutions are computed
using invariants that an adversary can compute himself (i.e.,
are public), and so this post-processing does not erode the
d∗-privacy of these outputs.

5.

IMPLEMENTATION

We implemented dpprocfs as a suite of software tools in
Ubuntu Linux LTS 14.04 with kernel version 3.13.11. dp-
procfs consists of three components: a kernel extension,
which we call privfs, that enhances the procfs with d∗-
private mechanisms (as discussed in Sec. 4.3) without al-
tering its existing program interfaces; a software tool, in-
vgen, that automatically searches for invariants in kernel
data structures for maintaining procfs value consistency (as
discussed in Sec. 4.4); and a userspace daemon, privfsd,
that interacts with the kernel extension and facilitates con-
sistency enforcement in real time.

5.1 d∗-Private Mechanism Implementation

When a ﬁle in procfs is read by a userspace process, a ker-
nel function is invoked to serve the request, and the return
values are sent to the process as if it is reading a ﬁle. The
values reported by procfs are computed from ﬁelds in cer-
tain kernel data structures. To generate d∗-private outputs,
a kernel extension privfs computes noised versions of those
protected ﬁelds for use by the kernel function computing the
procfs output.

Speciﬁcally, privfs introduces a kernel data structure of
type privfs_struct per kernel data-structure ﬁeld x that is
protected (rendered d∗-private) by dpprocfs. This structure
includes two arrays of ﬂoating-point values. After access i
to the data-structure ﬁeld x to which the privfs_struct
structure is associated, position log2 D(i) in these arrays
are updated to hold x[i] − x[G(i)] and ri, respectively. To-

gether with xh2⌊log2 i⌋i and ˜xh2⌊log2 i⌋i, which the struc-

ture also stores, these arrays permit the eﬃcient computa-
tion of ˜x[i + 1]. Also to speed up this computation, the
privfs_struct structure maintains a buﬀer of 32B to store
precomputed random values ri+1, ri+2,
. . . following the
speciﬁed Laplace distributions. Buﬀer reﬁlling is imple-
mented as a tasklet, a type of software IRQ in Linux kernels.
The arrays in privfs_struct in our present implementa-
tion are of ﬁxed length, speciﬁcally 32 ﬂoating-point values,
which limits the number of queries to the protected data-
structure ﬁeld to 232 − 1. These arrays might instead be
made arbitrarily extensible so as to allow an unlimited num-
ber of queries. That said, as the query count i grows, the
accuracy of the returned ˜x[i] value decays. As such, alterna-
tive designs might limit (or rate-limit) the number of queries
to any protected data-structure ﬁeld by each userspace pro-
cess or its associated user. Another implementation choice
might be to maintain separate arrays for each user of the
system, so that queries from one user would not decrease
the utility of queries from other users.

1http://www.ibm.com/software/commerce/
optimization/cplex-optimizer/

privfs does not return ˜x[i] directly for use in computing
the procfs output. Instead, it sends this value to privfsd
for enforcing invariants across all noised values. privfsd will
be discussed in Sec. 5.3, after we discuss how data-structure
invariants are identiﬁed in Sec. 5.2.

5.2 Invariant Generation

Kernel data-structure invariants are generated by a com-
ponent called invgen. invgen generates two types of in-
variants, namely one-ﬁeld and multiple-ﬁeld invariants as
discussed in Sec. 4.4. One-ﬁeld invariants are relationships
between a ﬁeld’s current and previous values. Multiple-ﬁeld
invariants are relationships between diﬀerent variables when
accessed at the same time.

As discussed in Sec. 4.4, our system generates invariants
from traces of data-structure values captured during exe-
cution. Speciﬁcally, invgen does so by collecting execution
traces of all numerical data-structure ﬁelds that are relevant
to procfs outputs. To do so, we patch an OS kernel by
adding one more ﬁle in the procfs to directly export all nu-
meric kernel data-structure ﬁelds of interest. invgen then
repeatedly reads the extended procfs ﬁle, sampling the val-
ues of these ﬁelds frequently and writing them into trace
ﬁles. For this paper, traces were collected by monitoring
the data-structure ﬁelds during the execution of a variety of
software programs, including Google Chrome and a set of
benchmark applications from Phoronix Test Suite2. By exe-
cuting each benchmark application three times, we collected
22.6MB of trace ﬁles.

We then used Daikon [22] to extract invariants from these
trace ﬁles. To use Daikon, we ﬁrst conﬁgured it with in-
variant templates, or ﬁlters, that the tool uses to search
for invariants. For one-ﬁeld invariants, Daikon was conﬁg-
ured with ﬁlters to locate ﬁelds that do not change, that are
monotonically nonincreasing, or that are monotonically non-
decreasing. For multiple-ﬁeld invariants, we implemented a
ﬁlter that Daikon uses to search for linear invariants among a

set X of ﬁelds, i.e., a property of the form Px∈X cx ×x[i] ≥ 0

that holds for all i, for some constant cx ∈ {−1, 0, 1}. We
ran Daikon with this ﬁlter for two sets X , one for memory-
related ﬁelds and one for scheduler-related ﬁelds. After using
Daikon to extract likely invariants in this way, we manually
inspected the outputs and discarded those that were either
implied by others or that we believed to be spurious.

The invariants produced in this way are shown in Table 2.
(We also include invariants that all ﬁelds are integral, but
we do not show those, for brevity.) The right half of the ta-
ble shows the invariants expressed using the labels for kernel
data-structure ﬁelds indicated in the left half of the table.
The ﬁelds marked “Protected” in the left half of the table
are those that dpprocfs renders d∗-private in our present
implementation. Those ﬁelds marked with a “z” were se-
lected based on their use in existing attacks (see Sec. 3.1),
and those marked with a “checkmark” were selected for pro-
tection because they are included in invariants with such
ﬁelds. One ﬁeld, namely uptime, is not protected in our
present implementation despite being included in invariants,
simply because the information it carries (the time since the
machine was booted) seems unlikely to carry information
useful to a side-channel attack. That said, it could also be
protected with minimal additional cost.

2http://www.phoronix-test-suite.com

1587Data-structure ﬁeld

Protected

Label

Invariants

z

mm_struct.total_vm
mm_struct.shared_vm
mm_struct.stack_vm
X
mm_struct.exec_vm
mm_struct.rss_stat.count[MM_FILEPAGES] z
mm_struct.rss_stat.count[MM_ANONPAGES] z
mm_struct.rss_stat.count[MM_SWAPENTS] X
X
mm_struct.hiwater_rss
mm_struct.hiwater_vm

z
X

X

task_struct.utime
task_struct.stime
task_struct.gtime
task_struct.signal->cstime
task_struct.signal->cutime
task_struct.real_start_time
task_struct.nvcsw
task_struct.nivcsw
get monotonic boottime()

z

X

X
X

X

X
z

z

totalVM
sharedVM
stackVM
execVM
ﬁlePages
anonPages
swapEnts
hiwaterRSS
hiwaterVM

utime
stime
gtime
cstime
cutime
starttime
nvcsw
nivcsw
uptime

swapEnts ≥ 0

cstime ≥ 0 utime[i] ≥ utime[i − 1]
totalVM ≥ 0
sharedVM ≥ 0 hiwaterRSS ≥ 0 cutime ≥ 0 stime[i] ≥ stime[i − 1]
hiwaterVM ≥ 0 nvcsw ≥ 0 gtime[i] ≥ gtime[i − 1]
stackVM ≥ 0
utime ≥ 0
execVM ≥ 0
ﬁlePages ≥ 0
stime ≥ 0
anonPages ≥ 0 gtime ≥ 0

nivcsw ≥ 0 cstime[i] ≥ cstime[i − 1]
cutime[i] ≥ cutime[i − 1]
nvcsw[i] ≥ nvcsw[i − 1]
nivcsw[i] ≥ nivcsw[i − 1]
starttime[i] = starttime[i − 1]

hiwaterRSS < sharedVM
hiwaterVM ≥ ﬁlePages
execVM ≥ ﬁlePages + swapEnts
sharedVM + ﬁlePages ≥ anonPages + swapEnts
sharedVM + execVM ≥ ﬁlePages + anonPages + swapEnts
sharedVM ≥ execVM + ﬁlePages + swapEnts
totalVM ≥ execVM + stackVM + ﬁlePages + anonPages + swapEnts
totalVM ≥ sharedVM + stackVM + swapEnts
totalVM + ﬁlePages ≥ sharedVM + anonPages + swapEnts
totalVM + execVM ≥ sharedVM + stackVM + ﬁlePages
+ anonPages + swapEnts

uptime ≥ starttime + utime + stime + gtime + cutime + cstime

Table 2: Selected kernel data-structure ﬁelds (Linux kernel 3.13) and generated invariants (Sec. 5.2) that
reference them. “Protected” ﬁelds are rendered d∗-private as described in Sec. 5.1, either because they have
been utilized in published side-channel attacks (z) or because they are involved in invariants that include
such ﬁelds (X).

The upper right corner of the right half of Table 2 lists
one-ﬁeld invariants, e.g., that task_struct.utime[i] (the
i-th access to task_struct.utime) is at least as large as
task_struct.utime[i − 1]. That is, task_struct.utime[i]
is nondecreasing. The other invariants hold for all simulta-
neous accesses to the indicated ﬁelds.

Our chosen method of invariant generation is admittedly
in that like any method of invariant generation
limited,
based on an incomplete set of recorded traces,
it allows
for false positives and false negatives. False positives—i.e.,
found “invariants” that are not actually invariants—will pre-
sumably not cause diﬃculties for the procfs code or appli-
cations when dpprocfs enforces them, since even if not in-
variant, the identiﬁed behavior is evidently common. False
negatives (i.e., missed invariants) might cause such prob-
lems, however, and so it would be prudent to augment our
dynamic approach with static analysis (e.g., [16, 45]) and
additional manual inspection. That said, we have not iden-
tiﬁed applications (or kernel routines that respond to procfs
reads) that appear to depend on behaviors other than those
identiﬁed in Table 2.

5.3 Reestablishing Invariants

Upon producing ˜x[i] for each protected ﬁeld x needed by a
kernel routine to respond to a procfs query,3 privfs needs
to reestablish the invariants among those ﬁeld values be-
fore submitting them to the kernel routine. For our pro-
totype, we implemented this step in a userspace daemon
process, which we call privfsd, that receives requests from
the privfs via Netlink sockets. This implementation choice
allows us to sidestep the need to port more complex op-
erations (e.g., ﬂoating-point operations, constraint-solving
algorithms) to run in the kernel.

privfs produces inputs for privfsd by ﬁrst identifying
the set X of protected ﬁelds to be accessed by the kernel rou-

3We are abusing notation here slightly, in that the access
index i might be diﬀerent per ﬁeld x.

tine serving the procfs query (i.e., from those ﬁelds marked
“protected” in Table 2). privfs forms the set of relevant

invariants from Table 2, namely I(X ) = Sx∈X I(x) where

I(x) for any x is deﬁned using the following inductive def-
inition: (i) I(x) is initialized to include any constraint in
Table 2 that includes ﬁeld x; and (ii) if any protected ﬁeld
x′ from Table 2 is named in an invariant already in I(x),
then I(x′) is added to I(x). privfs instantiates each pro-
tected ﬁeld x named in I(X ) with a variable ˆx[i] and, if
uptime ∈ I(X ), instantiates uptime with its current value.
privfs then produces the relevant value ˜x[i] for each ﬁeld
x ∈ X and sends I(X ) and the noised values {˜x[i]}x∈X to
privfsd.

privfsd operates in one of two modes, computing either
a nearest compliant assignment to each ˆx[i] or a heuris-
tic assignment to each ˆx[i]. The nearest assignment is cal-
culated by taking the instantiated invariants I(X ) as con-
straints in an integer programming (IP) problem, with vari-
ables {ˆx[i]}x∈X and objective being to minimize the cumula-

tive relative error, i.e., to minimize Px∈X |˜x[i] − ˆx[i] |/|˜x[i] |.

Our current implementation invokes CPLEX to solve this IP
problem. In contrast, the heuristic approach simply calcu-
lates any values for {ˆx[i]}x∈X that satisfy I(X ) using man-
ually coded heuristics to adjust the {˜x[i]}x∈X values.
In
Sec. 6, we will evaluate both modes of operation. Regard-
less of its mode of operation, privfsd returns the computed
values {ˆx[i]}x∈X to privfs to pass along to the kernel rou-
tine for preparing the procfs output to the waiting client.

6. EVALUATION

In this section we evaluate the eﬃcacy of dpprocfs de-
sign. While our design is provably d∗-private (and hence
dL1-private by Prop. 4), we perform an empirical security
evaluation of our design in Sec. 6.1 to better illustrate set-
tings of ǫ that suﬃce to interfere with known attacks. With
greater clarity as to reasonable settings of ǫ, we then evalu-
ate the utility of procfs for these ǫ values in Sec. 6.2.

15886.1 Security Evaluation

In this section, we evaluate the capability of dpprocfs
to defend against side-channel attacks discussed in Sec. 3.1.
Speciﬁcally, we measure the extent to which the procfs fea-
tures used by the attacker in selected attacks are still ef-
fective attack features in dpprocfs. Rather than trying to
replicate each attack from previous work exactly, we adopt
a more general framework for evaluation in which the at-
tacker’s task is detecting one of m classes of activities.

We perform this measurement of the attacker’s likely suc-
cess by building a multiclass classiﬁer for classifying procfs
features (which are attack-dependent) into one of m classes.
We use the scikit-learn4 support-vector-machine (SVM)
implementation to build the multiclass classiﬁer. We then
report the accuracy of the classiﬁer in a testing phase, namely
the fraction of test instances that it classiﬁes correctly.

6.1.1 Defending Against Keystroke Timing Attacks

The voluntary context switch counter (nvcsw in Table 2)
can be exploited to identify a user’s keystroke actions and
hence the timing characteristics of those keystrokes. These
timing characteristics can then leak information about what
those keystrokes were (e.g., [40]). To approximate the de-
fense that dpprocfs oﬀers against this attack, we consider
an adversary that consecutively reads the nvcsw ﬁeld from
procfs six times, and then the adversary classiﬁes this vec-
tor of readings to determine when the keystroke occurred.
(We only inject one keystroke during these six readings.) As
such, we model the attacker as a multi-class classiﬁer, which
classiﬁes the vector of six readings (i.e., a vector in N6) into
m = 5 classes; classifying a vector as class i indicates that
the keystroke occurred between reads i and i + 1.

To perform these experiments, we used a tool called xdo-
tool to simulate the keystroke actions at a speciﬁed time.
During each experiment run, we started a bash terminal and
injected one keystroke, at a time distributed normally with
mean 2.5s and standard deviation 0.83s (i.e., 0.0s is three
standard deviations from the mean). Beginning with the
launch of the bash process, the attacker process read the
/proc/<pid>/status ﬁle of the bash process every second5
to obtain the voluntary context switch counter nvcsw, yield-
ing six readings (a vector in N6).
Invariant enforcement
(Sec. 5.3) provided the nearest solution to the needed in-
variants. To allow for a powerful attacker, we provided to it
the underlying normal distribution imposed on the keystroke
timing. The attacker used this distribution to estimate the
true (unnoised) nvcsw value corresponding to each vector el-
ement (adapting [34, Eqn. 10]), yielding an estimated true
vector per collected vector. We repeated this experiment
440 times to get 440 estimated true vectors. When training
and testing the SVM classiﬁer, we used 75% of the vectors
from each class for training and 25% for testing.

The accuracy of the resulting classiﬁer on the testing ex-
amples is shown in Fig. 1(a). The horizontal axis shows var-
ious values of ǫ; the vertical axis shows classiﬁer accuracy.
Because of the form of the distribution imposed on keystroke

4http://scikit-learn.org/dev/index.html
5This interval is much longer than in the demonstrated at-
tack of Jana et al. [25], but we lengthened this interval to
minimize ambiguity regarding the class i ∈ {1 . . . 5} to which
each vector should be assigned for training. By increasing
this interval, we believe we produced classiﬁcation results
that are conservative (i.e., advantageous for the attacker).

y
c
a
r
u
c
c
A

1.0

0.8

0.6

0.4

0.2

0.0

0

1.0

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
A

3

6

9

12

15

0.0

0.00

0.02

0.04

0.06

0.08

0.10

ǫ

ǫ

(a) Keystroke timing attack

(b) Website inference attack

Figure 1: Multi-class classiﬁer accuracy under dif-
ferent ǫ settings; dashed horizontal lines show accu-
racies of blind guesses based only on knowledge of
the likelihood of each class

timings, the most likely class occurred roughly 44% of the
time, and so this baseline (shown by the horizontal dashed
line) is the accuracy that the adversary could achieve simply
by blindly guessing based on that distribution. As shown in
the graph, setting ǫ ∈ [1, 3] suﬃces to reduce the classiﬁer
to this baseline accuracy. By comparison, the classiﬁer was
perfect (an accuracy of 1.0) when no noise was added.

6.1.2 Mitigating Website Inference

The memory footprint of a browser can leak the website
it visits (as discussed in Sec. 3.1). In this experiment, we
instrumented the Google Chrome browser with a script to
visit a target website, chosen uniformly from the Alexa top-
10 websites. While this occurred, an attacker process re-
peatedly sampled the data resident size ﬁeld drs, calculated
as totalVM − sharedVM (using the labels deﬁned in Table 2),
by reading the /proc/<pid>/statm of the browser process
every 500µs. To support this rate of sampling, dpprocfs
employed the heuristic method of invariant reestablishment
(Sec. 5.3), which returned results in roughly 50µs (in com-
parison to 8ms for the nearest solution). The sampling pe-
riod lasted for 3s, during which the attack process recorded
all the drs ﬁeld values read. As in Sec. 6.1.1, the attacker
estimated the true (unnoised) drs value corresponding to the
j-th read value in each 3s interval, using an empirical distri-
bution observed for these j-th values gathered by accessing
each of these 10 websites an equal number of times. The at-
tacker then constructed a histogram of these estimated drs
values binned into seven equal-width bins, and the vector of
bin counts (in N7) was used as a feature vector for classiﬁ-
cation. Each of the Alexa top-10 websites were visited 100
times; when used to train and test the SVM classiﬁer (with
m = 10 classes), 70% were used for training and 30% were
used for testing.

The resulting accuracy of the classiﬁer is shown in Fig. 1(b).

The most important distinction from the graph in Fig. 1(a)
is that the values of ǫ needed to interfere with the website
inference attack are much smaller, meaning that the noise
added was greater. This is primarily a function of the size
diﬀerences between drs readings from the m classes, which
were generally much greater than the diﬀerences between the
readings of the voluntary context switch counter nvcsw with
and without a keystroke. In terms of d∗, the distances be-
tween the classes in the website inference attack were much
greater than the distances between classes in the keystroke
attack. This is noteworthy because it implies that the set-

1589tings of ǫ needed for privacy will diﬀer per-ﬁeld and per-
application and, to some extent, will need to be informed
by known attacks. Still, however, several values tested for ǫ
decayed classiﬁcation accuracy to a signiﬁcant extent; with
no noise added, the classiﬁer reached 0.915 accuracy.

6.2 Utility Evaluation

We evaluate the utility of dpprocfs in two ways. First, we
measure the relative error of selected procfs outputs that
are calculated using ﬁelds protected by dpprocfs, under the
two methods discussed in Sec. 5.3 for enforcing invariants,
namely producing a heuristic solution and a nearest solution
to the invariants. Second, we report the impact of dpprocfs
to the ranking of processes according to certain features by
top, a common utility for monitoring and diagnosis. Here
we focus on dpprocfs outputs such as memory and CPU
usage, as these are generally useful systems diagnostics.

6.2.1 Relative Error

We begin our utility evaluation by measuring the relative
error of the drs ﬁeld, the same ﬁeld exploited by website in-
ference attackers (see Sec. 6.1.2). To calculate the relative
error of this ﬁeld under dpprocfs, we preserved access to an
unprotected version of procfs alongside the protected ver-
sion. Then, we extended our setup described in Sec. 6.1.2
to simultaneously query both the protected and unprotected
versions of the drs ﬁeld while the browser process was run-
ning. During the evaluation, the browser was instrumented
to repeatedly visit https://www.youtube.com, and the drs
ﬁeld was queried every 50ms for a total of 500 queries. We
repeated this experiment 200 times.

0.8

0.6

0.4

Queries

0.2

0.0

r
o
r
r
E

e
v
i
t
a
l
e
R

heuristic

nearest

invariant

1-100 101-200 201-300 301-400 401-500

Figure 2: Comparison be-
tween nearest and heuris-
tic invariant reestablish-
ment for drs ﬁeld; ǫ = 0.005

Fig. 2 shows the dis-
tribution of relative er-
ror for both the nearest
and heuristic solutions
for
reestab-
lishment, computed on
the same noised values
˜x produced by privfs,
for a parameter setting
(ǫ = 0.005) that pro-
vided good security for
the side-channel attack
tested in Sec. 6.1 (see
Fig. 1(b)). Each query
range on the horizontal
axis has two box-and-whiskers plots, one for nearest and
one for heuristic. The three horizontal lines forming each
box indicate the ﬁrst, second (median), and third quartiles,
and the whiskers extend to cover all points within 1.5×
the interquartile range. Outliers are indicated using plus
(“+”) symbols. A diﬀerent box-and-whiskers plot is shown
per 100-query block across the 200 runs (i.e., each boxplot
represents 20,000 points) because the noise increases as the
number of queries grows. The diﬀerences between the near-
est and heuristic distributions are nearly imperceptible, and
this trend holds for other parameter and procfs ﬁelds we
have explored, as well. That said, the heuristic solution re-
lies on hand-tuned algorithms and by default provides no
guarantees, and so in cases where the speed of computing
the nearest solution is acceptable—the nearest solution took
an average of 8ms to return, whereas our heuristic approach
completed in an average of 50µs—it might be preferable.

r
o
r
r
E

e
v
i
t
a
l
e
R

0.5

0.4

0.3

0.2

0.1

0.0

ǫ = 0.005

ǫ = 0.01

ǫ = 0.02

ǫ = 0.04

1-100

101-200

201-300
Queries

301-400

401-500

Figure 3: Relative error for drs ﬁeld under nearest
invariant reestablishment

r
o
r
r
E

e
v
i
t
a
l
e
R

0.8

0.6

0.4

0.2

0.0

ǫ = 1

ǫ = 2

ǫ = 4

ǫ = 5

1-100

101-200

201-300
Queries

301-400

401-500

Figure 4: Relative error for utime ﬁeld under nearest
invariant reestablishment

Fig. 3 and Fig. 4 represent the relative error in readings of
the drs ﬁeld and of the utime ﬁeld from the /proc/<pid>/stat
ﬁle, respectively, for various values of ǫ. The values of ǫ
in Fig. 3 were chosen to overlap those used in the secu-
rity evaluation depicted in Fig. 1(b). The ǫ values tested in
Fig. 4 were chosen based on our simulation of the software-
keyboard side-channel attack of Lin et al. [28], which we
conducted on a Nexus 4 smartphone running Android 5.1
with kernel 3.4.0; based on this simulation, we estimated
that ranging ǫ over 1/2 ≤ ǫ ≤ 5 would result in curve simi-
lar to or better (with lower accuracy) than that in Fig. 1(a).6
In the tests in Fig. 4, the utime ﬁeld was queried every 50ms
while a video game was running. These graphs suggest that
the relative error is typically modest, e.g., with a third quar-
tile of < 15% in Fig. 3 and < 30% in Fig. 4, though outliers
can be large.

6.2.2 Rank Accuracy of top

The utility top is used by Linux administrators for per-
formance monitoring and diagnosis. By reading procfs, top
displays system information like memory and CPU usage of
running processes. The processes are ranked by top accord-
ing to a chosen ﬁeld. In this section, we evaluate the utility
of dpprocfs by measuring the rank accuracy of top when
run using dpprocfs in place of the original procfs.

To measure the rank accuracy, we ran two top processes
on one computer. These two top processes were started at
the same time and updated information with the same fre-
quency (every two seconds in our tests). The only diﬀerence
was that one top process read from dpprocfs (with heuristic
invariant reestablishment), and the other read from procfs
in its original form. To control the test workload in each ex-
periment, we ran a set of ten processes doing ﬂoating-point

6Lin et al. reported querying the utime ﬁeld of the software
keyboard process every 100ms to detect its increase. With
very rapid typing, the utime ﬁeld in our tests increased less
than 3 (jiﬃes) per 100ms interval, on average.

1590y
c
a
r
u
c
c
A
e
g
a
r
e
v
A

1.0

0.8

0.6

0.4

0.2

0.0

y
c
a
r
u
c
c
A
e
g
a
r
e
v
A

1.0

0.8

0.6

0.4

0.2

0.0

Top 1

Top 3

Top 5

Top 10

50 100 150 200 250 300 350 400 450 500

Queries

(a) ǫ = 0.005

Top 1

Top 3

Top 5

Top 10

50 100 150 200 250 300 350 400 450 500

Queries
(c) ǫ = 0.02

y
c
a
r
u
c
c
A
e
g
a
r
e
v
A

1.0

0.8

0.6

0.4

0.2

0.0

y
c
a
r
u
c
c
A
e
g
a
r
e
v
A

1.0

0.8

0.6

0.4

0.2

0.0

Top 1

Top 3

Top 5

Top 10

50 100 150 200 250 300 350 400 450 500

Queries

(b) ǫ = 0.01

Top 1

Top 3

Top 5

Top 10

50 100 150 200 250 300 350 400 450 500

Queries

(d) ǫ = 0.04

y
c
a
r
u
c
c
A
e
g
a
r
e
v
A

1.0

0.8

0.6

0.4

0.2

0.0

y
c
a
r
u
c
c
A
e
g
a
r
e
v
A

1.0

0.8

0.6

0.4

0.2

0.0

Top 1

Top 3

Top 5

Top 10

50 100 150 200 250 300 350 400 450 500

Queries
(a) ǫ = 1

Top 1

Top 3

Top 5

Top 10

50 100 150 200 250 300 350 400 450 500

Queries
(c) ǫ = 4

y
c
a
r
u
c
c
A
e
g
a
r
e
v
A

1.0

0.8

0.6

0.4

0.2

0.0

y
c
a
r
u
c
c
A
e
g
a
r
e
v
A

1.0

0.8

0.6

0.4

0.2

0.0

Top 1

Top 3

Top 5

Top 10

50 100 150 200 250 300 350 400 450 500

Queries
(b) ǫ = 2

Top 1

Top 3

Top 5

Top 10

50 100 150 200 250 300 350 400 450 500

Queries
(d) ǫ = 5

Figure 5: Average rank accuracy based on RES ﬁeld

Figure 6: Average rank accuracy based on %CPU ﬁeld

computations continually during each test. The number of
memory pages allocated by each process to store its array of
ﬂoats was scaled linearly across the ten processes: the ﬁrst
process allocated an 80MB array, the next process allocated
a 95MB array, and so on up to the tenth process, which
allocated a 215MB array. Similarly, the processes were con-
ﬁgured with linearly scaled nice values ranging from −19
(highest priority) through −1 (lowest).

Let R(k) and R′(k) be the set of top k processes displayed
by the two top programs. The top-k accuracy is deﬁned as
k |R(k) ∩ R′(k)|. Fig. 5 shows the average rank accuracy for
1
various values of k when processes were ranked by the RES
ﬁeld. The RES ﬁeld is read from /proc/<pid>/statm, calcu-
lated as ﬁlePages + anonPages, and represents the physical
memory usage of the process. Fig. 6 shows the average rank
accuracy when processes were ranked by the %CPU ﬁeld, cal-
culated as (utime[i]−utime[i − 1])/(uptime[i]−uptime[i − 1]).
Several observations from Fig. 5 and Fig. 6 are worth not-
ing. First, top retains much of its ability to rank processes
by these measures; e.g., even for the lowest values of ǫ tested
(Fig. 5(a) and Fig. 6(a)), the top-5 ranks remained roughly
80% correct on average through the tests. Second, whereas
the top-10 rank is generally more accurate than the top-1
rank in Fig. 5, the reverse is true in Fig. 6. This occurs be-
cause while the memory usage of the ten test processes was
scaled linearly, our linear scaling of nice values caused the
actual %CPU to drop oﬀ super-linearly. So, for example, the
average diﬀerence in %CPU values for the processes with nice
values −19 and −18 was much larger than the average %CPU
diﬀerence between processes with nice values −3 and −1.

7. DISCUSSION

Security limitations. Since we do not noise every ker-
nel data-structure ﬁeld that is used to serve procfs queries,
there remains the possibility that such ﬁelds might reveal in-
formation about the true values of noised ﬁelds. This could
occur either because the unprotected ﬁelds are related to
those noised ﬁelds by invariants that our techniques did not

ﬁnd (see Sec. 5.2) or because those relationships are only
statistical (but not invariant).
It will therefore be neces-
sary to extend the scope of our protections to other ﬁelds
as new procfs storage side-channel attacks are discovered
or, in the limit, that all kernel data-structures used to gen-
erate procfs contents be protected. As additional ﬁelds are
brought under the protections of dpprocfs, the invariants
that are reestablished on those values will need to be ex-
panded appropriately.

Similarly, the value of ǫ used to protect a ﬁeld might need
to be updated as new attacks involving that ﬁeld are dis-
covered. As shown in Sec. 6.1, the value of ǫ may need to
diﬀer from one ﬁeld to another. The magnitude of ǫ needed
for a ﬁeld will be correlated with the variation of that ﬁeld
and the number of queries over which protection needs to be
provided, since as the number of queries grow, presumably
so might d∗ (between the actual ﬁeld values and another
from which it should remain indistinguishable).

Utility limitations. As the number of procfs queries
grows, the amount of noise added to the kernel data-structure
ﬁelds used to generate the procfs outputs grows (see Sec. 4.3
and Sec. 5.1). As such, the utility of procfs outputs gener-
ated from those ﬁelds will decay. To slow this decay, it may
be necessary to rate-limit the queries that involve each ﬁeld
or to limit the number of such queries from any one user. Or,
as suggested in Sec. 5.1, a separate privfs_struct could be
maintained per querying user, though that obviously weak-
ens the mechanism against colluding users.

It may eventually be necessary to “reset” the d∗-private
mechanism associated with a ﬁeld, particularly for a ﬁeld
associated with a long-running process. A natural way to
do so would be to restart the process itself, since restarting
a process also refreshes its associated kernel data structures.
(This is also beneﬁcial for performance and reliability [15].)

Extensions to other storage side channels. We be-
lieve our proposed method can be extended to other storage
side channels such as those associated with mobile sensors
(e.g., [10, 30, 43, 11, 6, 33, 39]). However, it is unclear how

1591adding noise, e.g., to smartphone gyroscopes, will aﬀect the
usability of the apps that rely on their readings.

Alternative solutions to procfs side channels. An al-
ternative to adding noise in procfs outputs is to isolate mu-
tually distrusting processes into diﬀerent namespaces so that
they cannot read each others’ private procfs ﬁles. For ex-
ample, Linux containers7 isolate multiple applications from
each other using PID namespaces in the kernel. While useful
in hosting services such as modern PaaS clouds, Linux con-
tainers are less suitable in personal computing environments
(e.g., Android devices and desktop computers) since shar-
ing between diﬀerent software applications are necessary in
these single-user settings. Without a shared procfs, appli-
cations that needs accesses to these system statistics—e.g.,
most traﬃc- and system-monitoring apps on Google Play,
as well as the sysstat utilities8—will no longer work.

8. CONCLUSION

In this paper we have reported on the design, implemen-
tation, and evaluation of dpprocfs, a modiﬁcation to the
procfs pseudo ﬁle system that suppresses storage side chan-
nels. The innovations that are central to our design include:
(i) framing the side-channel problem as one of achieving d-
privacy for continual data release, and deﬁning an appropri-
ate distance d∗ for instantiating d-privacy for this scenario;
(ii) generalizing a diﬀerentially private mechanism for the
continuous release of binary values to the d∗-privacy goal
we set forth; (iii) recognition of the systems diﬃculties that
can arise when adding noise to procfs outputs, and an in-
variant reestablishment framework to address those diﬃcul-
ties; and (iv) a working implementation of dpprocfs, cou-
pled with an evaluation that shows it can simultaneously
defend against known storage side-channel attacks while re-
taining the utility of procfs for monitoring and diagnosis.
Our solution provides a conﬁgurable framework to suppress
new storage side channels as they are discovered, through
adding protection to additional kernel data-structure ﬁelds
or updating the ǫ values associated with each ﬁeld and ap-
plication. We further believe that the mechanisms we have
developed within our solution might be applicable to other
storage side channels, and we plan to explore this direction
in future work.

Acknowledgments
This work was supported in part by grant 1330599 from
the National Science Foundation. We are grateful to the
anonymous reviewers for their helpful comments.

9. REFERENCES

[1] I. Abraham, Y. Bartal, and O. Neimany. Advances in

metric embedding theory. In 38th ACM Symposium on
Theory of Computing, May 2006.

[2] G. ´Acs and C. Castelluccia. I have a DREAM!
(diﬀerentially private smart metering). In 13th
International Conference on Information Hiding, 2011.

[3] G. Acs, C. Castelluccia, and W. Lecat. Protecting
against physical resource monitoring. In 10th ACM
Workshop on Privacy in the Electronic Society, 2011.

7https://linuxcontainers.org/
8http://sebastien.godard.pagesperso-orange.fr/

[4] N. R. Adam and J. C. Worthmann. Security-control

methods for statistical databases: A comparative
study. ACM Computing Surveys, 21(4), Dec. 1989.

[5] M. E. Andr´es, N. E. Bordenabe, K. Chatzikokolakis,

and C. Palamidessi. Geo-indistinguishability:
Diﬀerential privacy for location-based systems. In
ACM Conference on Computer and Communications
Security, 2013.

[6] A. J. Aviv, B. Sapp, M. Blaze, and J. M. Smith.

Practicality of accelerometer side channels on
smartphones. In 28th Annual Computer Security
Applications Conference, 2012.

[7] M. Backes and S. Meiser. Diﬀerentially private smart

metering with battery recharging. In Data Privacy
Management and Autonomous Spontaneous Security,
8th International Workshop, volume 8247 of Lecture
Notes in Computer Science. 2014.

[8] A. Blum, K. Ligett, and A. Roth. A learning theory

approach to non-interactive database privacy. In 40th
ACM Symposium on Theory of Computing, 2008.

[9] N. E. Bordenabe, K. Chatzikokolakis, and

C. Palamidessi. Optimal geo-indistinguishable
mechanisms for location privacy. In ACM Conference
on Computer and Communications Security, 2014.

[10] L. Cai and H. Chen. TouchLogger: Inferring

keystrokes on touch screen from smartphone motion.
In 6th USENIX Conference on Hot Topics in Security,
2011.

[11] L. Cai and H. Chen. On the practicality of motion

based keystroke inference attack. In Trust and
Trustworthy Computing, 5th International Conference,
volume 7344 of Lecture Notes in Computer Science.
June 2012.

[12] T.-H. H. Chan, E. Shi, and D. Song. Private and

continual release of statistics. ACM Transactions on
Information and System Security, 14(3), Nov. 2011.
[13] K. Chatzikokolakis, M. E. Andr´es, N. E. Bordenabe,

and C. Palamidessi. Broadening the scope of
diﬀerential privacy using metrics. In 13th Privacy
Enhancing Technologies Symposium, July 2013.

[14] Q. A. Chen, Z. Qian, and Z. M. Mao. Peeking into

your app without actually seeing it: UI state inference
and novel Android attacks. In 23th USENIX Security
Symposium, 2014.

[15] D. Cotroneo, R. Natella, R. Pietrantuono, and

S. Russo. A survey of software aging and rejuvenation
studies. ACM Journal on Emerging Technologies in
Computing Systems, 10(1), Jan. 2014.

[16] C. Csallner, N. Tillmann, and Y. Smaragdakis. DySy:
Dynamic symbolic execution for invariant inference. In
30th International Conference on Software
Engineering, 2008.

[17] G. Danezis, M. Kohlweiss, and A. Rial. Diﬀerentially

private billing with rebates. In 13th International
Conference on Information Hiding, 2011.

[18] C. Dwork. Diﬀerential privacy. In Automata,

Languages and Programming, volume 4052 of Lecture
Notes in Computer Science, 2006.

[19] C. Dwork. Diﬀerential privacy: A survey of results. In

Theory and Applications of Models of Computation,
5th International Conference, volume 4978 of Lecture
Notes in Computer Science, Apr. 2008.

1592[20] C. Dwork. Diﬀerential privacy in new settings. In 21st
ACM-SIAM Symposium on Discrete Algorithms, 2010.

[21] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum.

Diﬀerential privacy under continual observation. In
42nd ACM Symposium on Theory of Computing, 2010.

[22] M. D. Ernst, J. H. Perkins, P. J. Guo, S. McCamant,
C. Pacheco, M. S. Tschantz, and C. Xiao. The Daikon
system for dynamic detection of likely invariants.
Science of Computer Programming, 69, 2007.

[23] B. C. M. Fung, K. Wang, R. Chen, and P. S. Yu.

Privacy-preserving data publishing: A survey of recent
developments. ACM Computing Surveys, 42(4), June
2010.

[24] M. Hay, V. Rastogi, G. Miklau, and D. Suciu.
Boosting the accuracy of diﬀerentially private
histograms through consistency. Proceedings of the
VLDB Endowment, 3(1-2), Sept. 2010.

[25] S. Jana and V. Shmatikov. Memento: Learning secrets
from process footprints. In 2012 IEEE Symposium on
Security and Privacy, May 2012.

[36] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth
sensitivity and sampling in private data analysis. In
39th ACM Symposium on Theory of Computing, 2007.

[37] I. Roy, S. T. V. Setty, A. Kilzer, V. Shmatikov, and

E. Witchel. Airavat: Security and privacy for
MapReduce. In 7th USENIX Conference on
Networked Systems Design and Implementation, 2010.

[38] P. Samarati. Protecting respondents’ identities in

microdata release. IEEE Transactions on Knowledge
and Data Engineering, 13(6), Nov. 2001.

[39] L. Simon and R. Anderson. PIN skimmer: Inferring

PINs through the camera and microphone. In 3rd
ACM Workshop on Security and Privacy in
Smartphones and Mobile Devices, 2013.

[40] D. X. Song, D. Wagner, and X. Tian. Timing analysis
of keystrokes and timing attacks on SSH. In USENIX
Security Symposium, 2001.

[41] L. Sweeney. k-anonymity: A model for protecting

privacy. International Journal on Uncertainty,
Fuzziness and Knowledge-based Systems, 10(5), 2002.

[26] K. Kursawe, G. Danezis, and M. Kohlweiss.

[42] D. Varodayan and A. Khisti. Smart meter privacy

Privacy-friendly aggregation for the smart-grid. In
11th International Conference on Privacy Enhancing
Technologies, 2011.

[27] N. Li, T. Li, and S. Venkatasubramanian. t-closeness:
Privacy beyond k-anonymity and ℓ-diversity. In 23rd
International Conference on Data Engineering, Apr.
2007.

[28] C.-C. Lin, H. Li, X. Zhou, and X. Wang. Screenmilker:

How to milk your Android screen for secrets. In 21st
ISOC Network and Distributed System Security
Symposium, 2014.

[29] A. Machanavajjhala, J. Gehrke, D. Kifer, and

M. Venkitasubramanian. ℓ-diversity: Privacy beyond
k-anonymity. In 22nd International Conference on
Data Engineering, 2006.

[30] P. Marquardt, A. Verma, H. Carter, and P. Traynor.

(Sp)iPhone: Decoding vibrations from nearby
keyboards using mobile phone accelerometers. In 18th
ACM Conference on Computer and Communications
Security, 2011.

[31] S. McLaughlin, P. McDaniel, and W. Aiello.

Protecting consumer privacy from electric load
monitoring. In 18th ACM Conference on Computer
and Communications Security, 2011.

[32] F. D. McSherry. Privacy integrated queries: An
extensible platform for privacy-preserving data
analysis. In ACM SIGMOD International Conference
on Management of Data, 2009.

[33] E. Miluzzo, A. Varshavsky, S. Balakrishnan, and R. R.

Choudhury. Tapprints: Your ﬁnger taps have
ﬁngerprints. In 10th International Conference on
Mobile Systems, Applications, and Services, 2012.

[34] M. Naldi and G. D’Acquisto. Diﬀerential privacy for
counting queries: Can Bayes estimation help uncover
the true value? CoRR, abs/1407.0116, July 2014.

[35] National Computer Security Center. A Guide to

Understanding Covert Channel Analysis of Trusted
Systems, volume NCSC-TG-030 of NSA/NCSC
Rainbow Series. Nov. 1993.

using a rechargeable battery: Minimizing the rate of
information leakage. In IEEE International Conference
on Acoustics, Speech and Signal Processing, May 2011.

[43] Z. Xu, K. Bai, and S. Zhu. Taplogger: Inferring user

inputs on smartphone touchscreens using on-board
motion sensors. In 5th ACM Conference on Security
and Privacy in Wireless and Mobile Networks, 2012.

[44] X. Zhou, S. Demetriou, D. He, M. Naveed, X. Pan,
X. Wang, C. A. Gunter, and K. Nahrstedt. Identity,
location, disease and more: Inferring your secrets from
Android public resources. In 20th ACM Conference on
Computer and Communications Security, 2013.

[45] F. Zhu and J. Wei. Static analysis based invariant

detection for commodity operating systems.
Computers & Security, 43, June 2014.

APPENDIX

A. PROOFS

Proof of Prop. 1.

P¡A′′(x, x′) ∈ Z × Z ′¢
= P (A(x) ∈ Z) × P¡A′(x′) ∈ Z′¢
≤ exp(ǫ × d(x, x′′)) × P¡A(x′′) ∈ Z¢
× exp(ǫ′ × d(x′, x′′′)) × P¡A′(x′′′) ∈ Z′¢
× P¡A′′(x′′, x′′′) ∈ Z × Z ′¢

= exp(ǫ × d(x, x′′) + ǫ′ × d(x′, x′′′))

Proof of Prop. 2. First note that for any i and any

z ∈ Z,

P (x[i] + ri = z)
P (x′[i] + ri = z)

=

exp(−ǫ × |x[i] − z|)
exp(−ǫ × |x′[i] − z|)

= exp(−ǫ × (|x[i] − z| − |x′[i] − z|))
= exp(ǫ × (|x′[i] − z| − |x[i] − z|))
≤ exp(ǫ × (|x[i] − x′[i] |))

The result then follows from Prop. 1.

1593≤ exp


And so,

Yi:i>D(i)
= Yk>0 Yj∈[0,k)

where k = ⌊log2 i⌋. For any ﬁxed j and k,

P ((x[i] − x[i − D(i)]) + ri = zi)
P ((x′[i] − x′[i − D(i)]) + ri = zi)

Yi ∈ (2k, 2k+1)

: D(i) = 2j

≤ exp









ǫ
k

ǫ
k

× Xi ∈ (2k, 2k+1)

: D(i) = 2j

× Xi∈(2k ,2k+1)

¯¯¯¯

(x[i] − x[i − D(i)])

− (x′[i] − x′[i − D(i)]) ¯¯¯¯
¯¯¯¯

− (x′[i] − x′[i − 1]) ¯¯¯¯


(x[i] − x[i − 1])

P ((x[i] − x[i − D(i)]) + ri = zi)
P ((x′[i] − x′[i − D(i)]) + ri = zi)

Yi ∈ (2k, 2k+1)

P ((x[i] − x[i − D(i)]) + ri = zi)
P ((x′[i] − x′[i − D(i)]) + ri = zi)

: D(i) = 2j

≤ Yk>0 Yj∈[0,k)
exp

= Yk>0
≤ exp

ǫ
k

(x[i] − x[i − 1])

exp


× Xi∈(2k,2k+1)
¯¯¯¯


− (x′[i] − x′[i − 1]) ¯¯¯¯
¯¯¯¯


− (x′[i] − x′[i − 1]) ¯¯¯¯

ǫ ×Xi≥2 ¯¯(x[i] − x[i − 1]) −¡x′[i] − x′[i − 1]¢¯¯

ǫ × Xi∈(2k,2k+1)

(x[i] − x[i − 1])




(5)

Finally, note that

P ((x[1] − x[0]) + ri = zi)
P ((x′[1] − x′[0]) + ri = zi)

≤ exp¡ǫ × |(x[1] − x[0]) − (x′[1] − x′[0])|¢

by Prop. 2. So, for any x, x′ ∈ X n and any hz1, . . . , zni,

(6)

Proof of Prop. 3. For any x, x′ ∈ Zn, the properties
d∗(x, x) = 0 and d∗(x, x′) = d∗(x′, x) are evident. The
triangle property results as follows, for x, x′, x′′ ∈ Zn:

n

d∗(x, x′′) =

=

≤

n

Xi=1
Xi=1
Xi=1

n

|(x[i] − x[i − 1]) − (x′′[i] − x′′[i − 1])|

¯¯¯¯

(x[i] − x[i − 1]) − (x′[i] − x′[i − 1])

+(x′[i] − x′[i − 1]) − (x′′[i] − x′′[i − 1]) ¯¯¯¯

|(x[i] − x[i − 1]) − (x′[i] − x′[i − 1])|

+

n

Xi=1

|(x′[i] − x′[i − 1]) − (x′′[i] − x′′[i − 1])|

= d∗(x, x′) + d∗(x′, x′′)

Proof of Prop. 4. First note that

n

d∗(x, x′) =

|(x[i] − x[i − 1]) − (x′[i] − x′[i − 1])|

n

Xi=1
Xi=1
≤ 2 × dL1(x, x′)

≤

|x[i] − x′[i] | +

n

Xi=1

|x[i − 1] − x′[i − 1] |

Therefore, if A : X → Z is (d∗, ǫ)-private, then we have that
for any x, x′ ∈ X and any Z ⊆ Z,

P (A(x) ∈ Z)
P (A(x′) ∈ Z)

≤ exp(ǫ × d∗(x, x′))

≤ exp(ǫ × 2dL1(x, x′))
= exp(2ǫ × dL1(x, x′))

Proof of Prop. 6. For any i such that i = D(i) ≥ 2

and any zi,

2¤) + ri = zi¢
2¤) + ri = zi¢
µx[i] − x· i

by Prop. 2, and so

P¡(x[i] − x£ i
P¡(x′[i] − x′£ i
≤ expµǫ ×¯¯¯¯
Yi:i=D(i)≥2
≤ exp

2¤) + ri = zi¢
2¤) + ri = zi¢

P¡(x[i] − x£ i
P¡(x′[i] − x′£ i
¯¯¯¯
2¸¶¯¯¯¯
µx[i] − x· i
2¸¶ −µx′[i] − x′· i
ǫ × Xi:i=D(i)≥2

ǫ ×Xi≥2¯¯(x[i] − x[i − 1]) −¡x′[i] − x′[i − 1]¢¯¯


≤ exp

Similarly, for any i > D(i) and any zi,

P ((x[i] − x[i − D(i)]) + ri = zi)
P ((x′[i] − x′[i − D(i)]) + ri = zi)

≤ exp³ ǫ

k

×¯¯(x[i] − x[i − D(i)]) −¡x′[i] − x′[i − D(i)]¢¯¯´

2¸¶¯¯¯¯
2¸¶ −µx′[i] − x′· i

¶

P (A(x) = hz1, . . . , zni)
P (A(x′) = hz1, . . . , zni)

n

=

Yi=1
≤ exp

P¡(x[i] − x[G(i)]) + ri = zi − zG(i)¢
P¡(x′[i] − x′[G(i)]) + ri = zi − zG(i)¢
2ǫ ×Xi≥1 ¯¯(x[i] − x[i − 1]) −¡x′[i] − x′[i − 1]¢¯¯




where the last step follows by multiplying Eqn. 6, Eqn. 4,
and Eqn. 5.




(4)

1594