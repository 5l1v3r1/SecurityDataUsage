Differentially Private Online Active Learning with

Applications to Anomaly Detection

Mohsen Ghassemi
Department of ECE
Rutgers University

Piscataway, New Jersey

m.ghassemi@rutgers.edu

Anand D. Sarwate
Department of ECE
Rutgers University

Piscataway, New Jersey

anand.sarwate@rutgers.edu

Rebecca N. Wright

Department of CS
Rutgers University

Piscataway, New Jersey

rebecca.wright@rutgers.edu

ABSTRACT
In settings where data instances are generated sequentially
or in streaming fashion, online learning algorithms can learn
predictors using incremental training algorithms such as sto-
chastic gradient descent. In some security applications such
as training anomaly detectors, the data streams may con-
sist of private information or transactions and the output
of the learning algorithms may reveal information about the
training data. Diﬀerential privacy is a framework for quanti-
fying the privacy risk in such settings. This paper proposes
two diﬀerentially private strategies to mitigate privacy risk
when training a classiﬁer for anomaly detection in an on-
line setting. The ﬁrst is to use a randomized active learn-
ing heuristic to screen out uninformative data points in the
stream. The second is to use mini-batching to improve clas-
siﬁer performance. Experimental results show how these
two strategies can trade oﬀ privacy, label complexity, and
generalization performance.

Keywords
diﬀerential privacy; online learning; active learning; anomaly
detection; stochastic gradient descent

1.

INTRODUCTION

Anomaly detection is an important application of statis-
tical testing. Classical hypothesis testing approaches for
detecting anomalous data samples often rely on known or
partially known statistical models of anomalous vs. non-
anomalous instances. However, in many real-world appli-
cations, these models may not be known and so must be
learned from data before deployment or may be reﬁned in
an online manner after deployment. For example, spam ﬁl-
ters for email may be trained on an existing corpus and
then tuned based on user feedback [1]. Data-driven machine
learning approaches can yield eﬀective anomaly detectors in
a wide range of applications [2].

Privacy can be a signiﬁcant concern in several emerg-
ing domains for anomaly detection. For example, audit-

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’16, October 28 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4573-6/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2996758.2996766

ing of ﬁnancial transactions, fraud investigation in medical
billing, and various national security applications entail sift-
ing through sensitive information about individuals in order
to ﬁnd anomalous behaviors or entities. Diﬀerential privacy
was proposed a decade ago by Dwork et al. [3] to address
the fundamental question of the privacy risk incurred by
publishing functions of private data. Since its introduction,
diﬀerentially private algorithms have been proposed for a
wide range of applications by several diﬀerent research com-
munities that work with information processing systems.

In this paper, we look at two additional challenges that ap-
pear in the context of anomaly detection methods and design
diﬀerentially private learning algorithms that can address
these challenges. The ﬁrst challenge is in the online adapta-
tion of the anomaly detector in the face of new data. We use
the private stochastic gradient framework (SGD) proposed
recently [4–7] to perform stream-based online learning.

The second, more important, challenge we address is label
eﬃciency; in anomaly detection problems where anomalies
are relatively rare or when the labeling of anomalies requires
an expert, the cost of acquiring a large labeled training set
may be high. We use selective screening and active learning
to limit the number of labels used by the algorithm. The
algorithm publishes updates to its detection rule as well as
the time of the updates; the key privacy challenge here is
that the selection of points and timing of updates can reveal
information about the training stream.

1.1 Our Contributions

We adapt general-purpose machine learning approaches to
create diﬀerentially private anomaly detectors. The general
strategy of our algorithm is shown in Figure 1. The input
data consists of a stream of unlabeled feature vectors whose
labels (anomalous or non-anomalous) are known to an ora-
cle (an expert who can label the points). At each time, the
algorithm chooses whether to request a label from the ora-
cle. Labeled points are collected over time and processed in
batches, possibly at a slower rate. When a batch of labeled
points is processed, the classiﬁcation rule is updated and the
time and the current classiﬁcation rule are published.

This abstract structure encompasses many fraud or ano-
maly detection systems of interest. The privacy challenge
enters because the classiﬁcation rule is revealed when it is
updated. Both the selection rule and the update rule may
reveal information about the training data. We apply stan-
dard diﬀerentially privacy techniques to control the privacy
risk incurred by each of these steps.

To select points for screening, the algorithm can use ran-

117Figure 1: Block diagram of stream-based active screening and private updates. The dashed line indicates what is published
by the algorithm.

domized response [8] or the exponential mechanism [9] to
decide whether to select a given training sample. Our con-
tribution here is to use a modiﬁed version of the active learn-
ing heuristic of Tong and Koller [10] within the exponen-
tial mechanism [9] to improve the usefulness of the selected
points for the learning task.

With randomized selection, the classiﬁer update step could
immediately use diﬀerentially private stochastic gradient de-
scent (SGD) [4–7]. However, due to the noise needed for dif-
ferential privacy, the algorithm might potentially converge
much more slowly in spite of the improved point selection
process.
Instead, to obtain better performance while still
achieving privacy, we propose two mini-batching strategies,
which we call ﬁxed-length selection windows (FLSW) and
ﬁxed-length mini-batch (FLMB). The FLSW policy updates
after ﬁxed time intervals: the update uses a variable-sized
mini-batch update of points selected within the previous
window. The FLMB strategy updates after a pre-speciﬁed
number of points has been selected. FLSW uses variable
memory and batch size, whereas FLMB uses ﬁxed memory
and batch size.

We provide privacy proofs for these algorithms, but util-
ity bounds are diﬃcult to prove under the active learning
strategy for point selection. An analysis of the Tong-Koller
method [10] is not available even in the non-private case,
and it is likely more challenging to prove such a result in the
diﬀerentially private case. We demonstrate empirically that
our algorithms allow the designer or other decision makers to
trade oﬀ label complexity, privacy, and error during online
classiﬁer training for anomaly detection.

1.2 Related Work

Anomaly detection is a key application area for machine
learning techniques. In this paper, in contrast to studying
a particular application for anomaly detection, we focus on
the privacy issues that may arise in learning an anomaly
detector using sensitive streaming data. A 2009 survey by
Chandola et al. [2] discusses several approaches for anomaly
detection as well as applications. Hodge and Austin [11]
survey many outlier detection techniques, some using ma-
chine learning methods. We evaluate our method on the
KDD Cup ’99 dataset [12], which is a network intrusion de-
tection problem widely studied by the machine learning com-
munity (for example, Tang and Cao [13]).

The online learning framework we use has been stud-
ied extensively in the machine learning community, start-
ing with the work of Zinkevich [14]. Shalev-Shwartz’s 2011
survey [15] gives a comprehensive introduction to the on-
line learning problem; our algorithm is an instance of online
learning in that framework. McMahan’s 2014 survey [16]

covers more recent work on adaptive regularizers. We do
not incorporate this type of adaptivity in our work, but
these ideas combined with our approach could be used to
yield anomaly detection methods for scenarios in which the
data distribution changes over time.

Active learning has received signiﬁcant attention in the
recent years. In active learning, the learning algorithm can
adaptively select training examples based on previous data;
this adaptivity can lead to signiﬁcant improvements in sam-
ple complexity over “passive learning” approaches that use
a random sample of the data [17, 18]. There is a rich body
of work on active learning; we refer the reader to the sur-
vey by Settles [17], Fu et al. [19], and the monograph by
Hanneke [20] for more details.

Our approach uses an active learning heuristic proposed
by Tong and Koller [10], which is a simple “pool-based” ac-
tive learning algorithm using a support vector machine clas-
siﬁer. In pool-based active learning [20], the learning algo-
rithm selects samples to be labeled from a pool of unlabeled
data. We propose a “stream-based” version of this heuristic.
In stream-based active learning, in each iteration t the al-
gorithm is given point xt and has to decide whether or not
to request the label yt from an oracle. Sabato and Hess [21]
recently identiﬁed conditions under which stream-based and
pool-based active learning can simulate each other. A diﬀer-
ent line of work in active learning considers the case where
the algorithm can choose x as well. Such active query al-
gorithms [22] are related to probabilistic bisection meth-
ods [23].

Diﬀerential privacy has been widely studied since its intro-
duction in 2006 [3]; Dwork and Roth’s recent book [24] pro-
vides an inclusive review. Duchi et al. [25] study statistical
learning problems under local privacy conditions measured
by mutual information between private observations and
non-private original data, as well as by diﬀerential privacy.
Dwork et al. formalize the notion of pan privacy [26, 27]
for streaming algorithms. Roughly speaking, pan-private
algorithms guarantee diﬀerential privacy even when their
internal memory state becomes available to an adversary.
We partially expose the current state of the algorithm by
publishing the learned classiﬁer over time. However, we do
assume the buﬀer of labeled points is kept secret and so do
not guarantee pan privacy.

Online learning has also been studied with diﬀerential pri-
vacy [28]. Balcan and Feldman [29] describe a framework for
designing oﬄine active learning algorithms that are tolerant
to classiﬁcation noise and show that these algorithms are
also diﬀerentially private. Our algorithms operate in an on-
line manner and use a speciﬁc diﬀerentially private online
active learning algorithm.

SelectorCollectorwTbatchUpdate(DP-SGD)T(T,wT)publicreleaseprivate data streamOracle...,xt+2,xt+1,xttytxt1182. PRELIMINARIES
We consider a model, shown in Figure 1, in which a col-
lection Dx = {xt ∈ Rd : t = 1, 2, . . .} of samples is presented
to a learning algorithm in an online manner. We assume all
data points are bounded, so

(cid:107)xt(cid:107) ≤ M.

(1)

Associated with each sample point xt is a (hidden) label
yt, which is known to an oracle O. In the anomaly detec-
tion context, the vector xt speciﬁes the value of measured
features about an event or individual observed at time t,
and the label yt ∈ {−1, +1} indicates whether the sample is
anomalous. Let D = {(xt, yt)} be the labeled dataset. The
feature vectors, but not the labels, are given to the learning
algorithm. The online algorithm must decide at each time
t whether to query the oracle for the label of xt or discard
it before observing the next sample. The goal of the learn-
ing algorithm is to learn a classiﬁer w that assigns a correct
label ˆy to a feature vector x. The labels are assigned by
applying the sign function to the inner product of w and x,
i.e. ˆy = sgn((cid:104)w, x(cid:105)).

The state of the algorithm is given by the current classi-
ﬁer wt. At each time t = 1, 2, . . . , N , the state is wt and
the algorithm makes two choices. First, it selects whether
or not to request the label yt from the oracle. Second, it can
choose to update the classiﬁer/state wt and publish the re-
sulting update. We consider models in which wt is updated
based on new labeled data; updates are either based on a
ﬁxed schedule (if there is no new labeled data the algorithm
publishes the previous state) or based on the number of new
labeled points. We call the time between updates a selection
window .
Thus the overall output of the algorithm over n iterations
is the sequence of classiﬁer estimates {wt : t = 1, 2, . . . , n} or
alternatively the pairs of update times and updates {(Ti, wTi ) :
i = 1, 2, . . . , T}, where the algorithm chooses to update the
classiﬁer at times {Ti}. The algorithm requires a buﬀer that
keeps the labeled points during each selection window until
the end of the window.

Support vector machines.

We use support vector machine training to ﬁnd a classi-
ﬁer by approximately solving the following regularized risk
minimization problem:

min
w∈Rd

f (w) = R(w) + EP [(cid:96)h(w; (x, y))] ,

(2)
where (x, y) is a sample-label pair such that x ∈ Rd and
y ∈ Y = {−1, 1}, P is the underlying joint distribution of
2 (cid:107)w(cid:107)2 is a regularizer. The
the pair (x, y), and R(w) = λ
loss function (cid:96)h is deﬁned as the hinge loss (cid:96)h(w; (x, y)) =
[1 − y (cid:104)w, x(cid:105)]+, which is convex. Throughout this paper, we
use words sample, data point, and instance interchangeably
to refer to x.
For a training set of labeled points S (which in our case
will be the subset of the data points whose points have been
labeled), the SVM algorithm minimizes the regularized em-
pirical risk as a proxy for (2):

min
w∈Rd

f (w) = R(w) +

1
|S|

(cid:96)h(w; (x, y)).

(3)

(cid:88)

((x,y)∈S

is unknown to the learning algorithm. Instead, the learner
is presented sequentially with samples {xi : i = 1, 2, . . .}
drawn from the marginal distribution PX . The regularized
empirical risk minimization framework includes many classi-
ﬁcation and regression problems; we use SVM for simplicity
of exposition.

Differential privacy.

Diﬀerential privacy [3] is a method of measuring the pri-
vacy risk from publishing functions of private data. In our
setting the private data is the set of pairs {(xt, yt) : t =
1, 2, . . .}. For a ﬁnite time horizon n, consider two data
streams that diﬀer in a single point, say at time τ :
D = (x1, y1), (x2, y2), . . . , (xτ , yτ ), (xτ +1, yτ +1), (xn, yn)
D = (x1, y1), (x2, y2), . . . , (x
(cid:48)
τ ), (xτ +1, yτ +1), (xn, yn),
where all data vectors satisfy (1). We call such databases
neighboring. We call a randomized algorithm A(D) oper-
ating on D -diﬀerentially private if the presence of any
individual data point does not change the probability of
A(D) ∈ C by much, for any set C. Formally, we say algo-
rithm A provides -diﬀerential privacy if for any two neigh-
boring databases D and D’ and any set of outputs C,

(cid:48)
τ , y

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:12)(cid:12)(cid:12)(cid:12) ≤ ,

P(A(D) ∈ C
P(A(D(cid:48)) ∈ C

(4)

where A(D) is the output of A on dataset D.
For the algorithms described above, the output set would
consist of n-tuples of classiﬁers {wt : t = 1, 2, . . . , n}. Thus
we are interested in controlling the total amount of privacy
risk  due to both the selection of points as well as the classi-
ﬁer update rule. In the diﬀerential privacy threat model, an
adversary observes the output of the algorithm and attempts
to infer whether the outcome came from input D or D(cid:48);
successful inference would mean that the adversary would
learn whether (xτ , yt) or (x(cid:48)
t) was in the data stream.
The parameter  controls how diﬃcult this hypothesis test
is, bounding the tradeoﬀ between false alarm and missed
detection (Type I and Type II) errors [30, 31].

τ , y(cid:48)

Privacy-preserving stream-based learning.

Because our procedure reveals the classiﬁers over time,
an adversary observes both the updates and the timing of
updates. This means that potentially, the selection of points
whose labels we query as well as the classiﬁer updates must
guarantee a total -diﬀerential privacy. We can consider two
extremes for some intuition.

Algorithm 1 Batch SVM
Input: , stream {xt}.
Initialize: w1 = 0, S0 = ∅.
for t = 1 to n do

Ask oracle O for label yt of xt.
St ← St−1 ∪ {(xt, yt)}.
Set wt+1 = wt.
Output wt.

end for
Output DPERM(Sn, ).

In the online setting, we assume that the true distribution P

The ﬁrst example in Algorithm 1 simply asks for all la-
bels and then performs one update at time n using a batch -

119diﬀerentially private SVM training method such as objective
perturbation [32]; we call this DPERM in Algorithm 1. Since
the algorithm selects all points, the point selection process
trivially guarantees diﬀerential privacy, and the SVM train-
ing is diﬀerentially private from previous results [32]. This
approach has high label complexity and high computation
cost from training an SVM on the entire dataset at time n.

Algorithm 2 Private Stochastic Gradient Descent

Input: , stream {xt}, step sizes {ηt}
Initialize: w1 = 0
for t = 1 to n do

Ask oracle O for label yt of xt.
Update wt+1 using DPSGD(wt, xt, yt, ηt, ).
Output wt.

end for

On the other end of the update-frequency spectrum, we
have Algorithm 2, which simply performs noisy stochas-
tic gradient updates that guarantee diﬀerential privacy [7].
Again, this algorithm asks for the label of every data point
and the updates are performed in a diﬀerentially private
way, so the overall algorithm guarantees -diﬀerential pri-
vacy. This approach has low per-iteration complexity.

In settings where the learning algorithm is processing pri-
vate or sensitive data, we would like to limit the privacy risk
by not querying the labels of many points. In addition, it
is also beneﬁcial to limit the number of labels that must be
queried in cases where training labels for anomaly detection
must be generated by costly experts. In the next section, we
describe how to use ideas from active learning to design algo-
rithms that trade oﬀ label complexity, privacy, and classiﬁer
accuracy in this setting.

3. SVM PRIVATE ACTIVE LEARNING

In our system model, both the sample selection and classi-
ﬁer update must be made diﬀerentially private. In anomaly
detection problems we are concerned with label complex-
ity. In selecting samples for labeling, we would like to select
points that are informative. We use a heuristic introduced
by Tong and Koller [10] for training a support vector ma-
chine (SVM) using active learning. Their approach uses an
informativeness measure based on the closeness to a hyper-
plane estimate w. In our model, the informativeness of point
x with respect to a hyperplane w is measured by its close-
ness to the hyperplane. Let

d(x, w) (cid:44) |(cid:104)w, x(cid:105)|
(cid:107)w(cid:107)

(5)

The informativeness is

c(x, w) = exp (−d(x, w)) ∈ [0, 1].

Tong and Koller originally suggested this method for active
learning in the pool-based setting: at iteration t the learner
would select the point xi with the largest informativeness
c(xi, w) and ask the oracle for the label. The algorithm
then estimates a new w by retraining an SVM on the entire
data set.

In our model, samples come in sequentially, so we deﬁne

c(t) = c(xt, wt)

(6)

at time t. In a non-private selection strategy, if the infor-

mativeness of sample xt meets a certain threshold τ , then
the learner queries the oracle to obtain yi and updates the
classiﬁer. Otherwise, it discards xt and waits for the next
instance. Note that the condition c(t) > τ is equivalent to
d(xt, wt) < log 1
τ , which means that the sample xt is within
a selection slab of width 2 log 1
τ around wt. We call this an
online active learning strategy.
3.1 Differentially Private Point Selection

As mentioned earlier, the active learning algorithms we
propose here make two decisions at each time t: ﬁrst, whether
to request the label yt and add (xt, yt) to the training set,
and secondly, whether to update the classiﬁer so that wt+1 (cid:54)=
wt. We refer to our selection strategy as an online T-K
(Tong-Koller) strategy. The original T-K heuristic’s deci-
sion to query was deterministic, so to guarantee privacy we
must randomize both the decision to query and the gradient
step. At time t, based on the current state wt, the algorithm
selects xt to be labeled based on whether it passes the in-
formativeness threshold. We can therefore use randomized
mechanisms to select the whether or not to label the point.
When the algorithm decides to update wt we can use private
stochastic gradient descent (SGD) [4, 7].

Bernoulli selection.

The simplest approach is to compare the informativeness
c(t) to a threshold and then use randomized response [8] to
select the point. More formally, for parameters p < 1/2 and
τ , the selection variable st ∼ Bern(1 − p) if c(t) ≥ τ and
st ∼ Bern(p) if c(t) < τ . Standard arguments imply that
this provides Bern = log 1−p
p diﬀerential privacy (Lemma 1).

Exponential selection.

A strategy that is more in the spirit of the Tong-Koller
method uses the exponential mechanism [9]. Consider a
threshold on d(·,·), so that we consider d(·,·) ≤ b and d(·,·) >
b as separate cases. Within the selection slab deﬁned by b,
the algorithm selects points with constant probability. Out-
side the slab it selects with probability that decays exponen-
tially with the distance.

Deﬁne

(cid:40)

q(t) =

d(xi, wt) ≤ b
e−b/∆
e−d(xi,wt)/∆ d(xi, wt) > b
= exp (− max{b, d(xi, wt)} · /∆)

(7)

M )M and  > 0. For this strategy, st ∼
where ∆ = (1 − b
Bern(q(t)). In this way, every point has a chance of being
selected, so the adversary cannot infer whether an observed
point is inside the selection slab or not. However, more
informative points are still more likely to be selected. The
privacy guarantee of this method is given in Lemma 2.
3.2 Differentially Private Update

In addition to making the point selection private, we must
also make the update step private. We consider two strate-
gies based on mini-batching: the ﬁxed-length selection win-
dow (FLSW) policy, which updates after a ﬁxed number of
time steps, and the ﬁxed-length mini-batch (FLMB) policy,
which updates after a ﬁxed number of selected points. Both
strategies use a mini-batch update rule. Mini-batching is a
well-known method in stochastic optimization for machine
learning to control the variance of the subgradient estimates.

120(cid:32)

(cid:88)

t∈B

(cid:33)

z

(10)

In a privacy preserving algorithm, it also provides ambigu-
ity in the contribution of each member of the mini-batch to
the approximate gradient [6]. Moreover, in our model, this
technique can also provide ambiguity in sample selection.
To keep the privacy of the users whom data are present
in D, during gradient update step, one method is to follow
the diﬀerentially private SGD update [7]. In this method,
a controlled noise term zt is added to each update. More
speciﬁcally, in order to make updates g-diﬀerentially pri-
vate, we add a random noise vector zt ∈ Rd drawn i.i.d.
from the following probability distribution:

P(z) = γe

−(g /2M )(cid:107)z(cid:107)

(8)
where M is an upper bound on every xi ∈ DX and γ is a
normalizing constant.

For a labeled point (x, y) let

u = 1((cid:96)h(w; (x, y)) > 0)

(9)

be the indicator that the hinge loss is positive. For a batch
B = {(xt, yt)} of B = |B| points, the diﬀerentially-private
mini-batch update rule is given by

(cid:48)

= w − η

w

λw − 1
B

ytxtut +

1
B

where z ∼ P(z) in (8) and ut is given by (9).

We consider two variants of the mini-batch update. In the
ﬁrst method, the algorithm updates the classiﬁer after every
N observations, resulting in a variable batch size due to
the randomized label queries. The second method suggests
waiting until L samples are selected to be labeled, resulting
in a ﬁxed batch size.

Fixed-Length Selection Windows (FLSW).

Algorithm 3 FLSW update with randomized screening

Input: , stream {xt}, step sizes {ηt}, batch B.
Initialize: w1 = 0
for t = 1 to n/N do

Use s-DP method to decide whether to ask oracle for
label yt of xt.
If yt known, then add (xt, yt) to B
if n = 0 mod N then

Update wt+1 using (10) with batch B.
Set B = ∅.
Output wt.

end if
end for

In this update method, the learner collects labeled sam-
ples into a batch B during an interval of length N . It updates
the classiﬁer at a rate slower than it observes incoming sam-
ples: once every N observations. Since the adversary can
only see the updates in w(k), we can take advantage of the
ambiguity in the number of the samples selected during the
kth interval, which samples are selected, and the individual
contribution of selected ones to the update. An extreme ver-
sion of the FLSW rule is the immediate update rule, which
takes N = 1.

Fixed-Length Mini-Batches (FLMB).

Here, before updating the classiﬁer, the algorithm waits

Algorithm 4 FLMB update with randomized screening

Input: , stream {xt}, step sizes {ηt}, batch B.
Initialize: w1 = 0
for t = 1 to n/N do

Use s-DP method to decide whether to ask oracle for
label yt of xt.
If yt known, then add (xt, yt) to B
if |B| = L then

Update wt+1 using (10) with batch B.
Set B = ∅.
Output wt.

end if
end for

until a mini-batch of L labeled instances are collected. Again,
the algorithm needs to decide whether to select a given sam-
ple or not for labeling immediately after it observes the sam-
ple. Thus the batch B has size B = L in update rule (10).
Unlike the ﬁxed-length selection windows method, the num-
ber of labeled samples in a window is ﬁxed. However, the
adversary can observe the number of samples that are ob-
served by the algorithm in each interval before L samples
are selected. This, generally, adds ambiguity to the selec-
tion process since the selection result for each observation,
in most cases, is not readily available to the adversary. How-
ever, when number of these observations equals L, the adver-
sary can deduce that all points in the interval are selected.

4. PRIVACY ANALYSIS

We now quantify the diﬀerential privacy guarantees under
our models and update procedures. We compare the utility
performance of the diﬀerent methods empirically.

4.1 Privacy analysis for individual steps
In the proofs of the theorems in this section, let st ∈
{0, 1} be the indicator whether the algorithm queries the
label of the tth point. We begin with privacy guarantees for
the selection steps and mini-batch steps that make up the
algorithm.

Bernoulli selection strategy.

Lemma 1 bounds the privacy risk of the Bernoulli selection

strategy.

Lemma 1. Consider an online active learning algorithm
with the Bernoulli selection strategy as described in Section
3.1 with p > 1
2 . Assuming that the most recent value of wt is
public, this query strategy is ber-diﬀerentially private, where
ber = log

(cid:16) p

(cid:17)

1−p

.

Proof. Consider two databases Dx and D(cid:48)

x that diﬀer in
a single point: for some j, the jth point of Dx is xp and the
jth point of D(cid:48)
x is xq. The selection procedure reveals wt
at each iteration t. Since the samples are presented online,
until time τ the iterations have the same distribution.

121Then,(cid:12)(cid:12)(cid:12)(cid:12)log

P (st = 1|D, wt)
P (st = 1|D(cid:48), wt)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ log

≤ log
≤ log

P (st = 1|it = p, wt)
P (st = 1|it = q, wt)
P (st = 1|cp(t) > τ )
P (st = 1|cq(t) < τ )
p
1 − p

.

(11)

Similarly, the same result can be shown for the case st = 0.
This concludes the proof.

Exponential selection strategy.

Lemma 2 presents the privacy guarantee provided by the

exponential mechanism.

Lemma 2. Consider an online active learning algorithm
with the exponential selection strategy as described in Section
3.1. Suppose that b in (7) satisﬁes exp(−bexp/∆) ≤ 1/2.
Assuming that the most recent value of wt is public, this
query strategy is exp-diﬀerentially private.

and thus

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:12)(cid:12)(cid:12)(cid:12) = 0 ≤ g.

P (w(cid:48)|D, w)
P (w(cid:48)|D(cid:48), w)

(15)

so the result is diﬀerentially private for two neighboring
dataset. This concludes the proof.
4.2 Immediate Update

As a baseline, we analyze the FLSW scheme with win-
dow size N = 1, which corresponds to immediate updates.
Standard composition theorems [31, 33] allows us to ﬁnd the
overall diﬀerential privacy guarantee under each scenario un-
der either Bernoulli or exponential sampling.

Corollary 1. Each update in Algorithm 3 with N = 1

is g-diﬀerentially private.

Proof. The proof follows from Lemma 3 with batch size

B = 1.

Theorem 1. The FLSW algorithm in Algorithm 3 with

N = 1 is (s + g)-diﬀerentially private.

Proof. Let xp and xq be the jth entries of two neigh-

x, respectively. Then,

boring datasets Dx and D(cid:48)
P (st = 1|D, wt)
P (st = 1|D(cid:48), wt)

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)log

exp (− max{b, d(xp, wt)}s/∆)
exp (− max{b, d(xq, wt)}s/∆)

≤
≤ |max{b, d(xq, wt)} − max{b, d(xp, wt)}| exp
∆
≤ (M − b)

exp
∆

(cid:12)(cid:12)(cid:12)(cid:12)

we have(cid:12)(cid:12)(cid:12)(cid:12)log

= exp.

(12)
Similarly, considering the assumption exp(−bexp/∆) ≤ 1/2,

P (st = 0|D, wt)
P (st = 0|D(cid:48), wt)

1 − exp(−M exp/∆)
1 − exp(−bexp/∆)

(13)

(cid:12)(cid:12)(cid:12)(cid:12) ≤ log

≤ exp.

This concludes the proof.

Mini-batch update.

Lemma 3. The gradient step in (10) with batch B is g-

diﬀerentially private.

Proof. let xp is the jth entry of Dx and xq is the jth
entry of Dx. Furthermore, assume without loss of generality
that |(cid:104)wt, xp(cid:105)| ≤ |(cid:104)wt, xq(cid:105)|.

(cid:80)
i∈B:i(cid:54)=p ηyixiui− η
(cid:80)
i∈B:i(cid:54)=q ηyixiui− η

B ypxpup(cid:107)
B yq xq uq(cid:107)

P (w(cid:48)|D, w)
P (w(cid:48)|D(cid:48), w)
− g B

2ηM (cid:107)w(cid:48)−w+ηλw− 1
2ηM (cid:107)w(cid:48)−w+ηλw− 1
− g B

B

B

≤ γe
γe
≤ e(g /2M )(cid:107)xp−xq(cid:107)
≤ eg .
P(cid:0)w

P(WT , QT|D)
P(WT , QT|D(cid:48))

P(wt, st|D, Wt−1)

P(wt, st|D(cid:48), Wt−1)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

t=1

T(cid:81)
T(cid:81)
T(cid:81)
T(cid:81)

t=1

t=1

t=1

T(cid:81)
T(cid:81)

t=1

t=1

=

=

(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)log

(a)
=

(b)
=

≤
≤ g + s,

Proof. Each selection step incurs s privacy risk. If we
show that each update incurs at most g privacy risk, so the
risk per sample (by composition) is at most s + g.
Suppose the algorithm runs for T iterations and deﬁne the
T ](cid:62) of revealed values.
matrix WT = [w(cid:62)
From this an adversary can infer QT = [s1, s2, . . . , sT ], the
vector of selection results. Let D and D(cid:48) be two neighboring
databases that diﬀer in the jth element: xj = xp in D and
j = xq in D(cid:48). We calculate the log-likelihood ratio:
x(cid:48)

2 , . . . , w(cid:62)

0 , w(cid:62)

1 , w(cid:62)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12) .
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

P(wt|D, Wt−1, st)P(st|D, Wt−1)

P(wt|D(cid:48), Wt−1, st)P(st|D(cid:48), Wt−1)

(16)

Now, we use the fact that up until time j, the distribution
of the two algorithms’ decisions are identical:

P(WT , QT|D)
P(WT , QT|D(cid:48))

P(wt|D, wt−1, st)P(st|D, wt−1)

P(wt|D(cid:48), wt−1, st)P(st|D(cid:48), wt−1)

P(wj|D, wj−1, sj)P(sj|D, wj−1)
P(wj|D(cid:48), wj−1, sj)P(sj|D(cid:48), wj−1)
P(wj|D, wj−1, sj)
P(wj|D(cid:48), wj−1, sj)

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:12)(cid:12)(cid:12)(cid:12) +

P(sj|D, wj−1)
P(sj|D(cid:48), wj−1)

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

(17)

Similarly, we can show that if B = ∅,

(cid:48)|D, w(cid:1) = P(cid:0)w

(cid:48)|D(cid:48)

(14)

, w(cid:1)

Equality (a) above results from the fact that given the most
recent classiﬁer wt, the probability distributions of wt and
st are independent of the previous classiﬁers. Equality (b)

122follows from the assumption the observed samples do not
appear again in the stream, so except for the iteration where
the streams are diﬀerent, the conditional probabilities are
identical.

The immediate update strategy provides a baseline perfor-
mance against which to compare the two mini-batch strate-
gies in Algorithms 3 and 4.
4.3 Mini-batch Update with FLSW

Under the FLSW policy, updates happen every N obser-
vations. Since the individual point selections within this
interval of length N are hidden (only the updates {wkN}
are revealed), this can possibly save in terms of the privacy
properties.

Selection step.

Each update of the classiﬁer in the FLSW algorithm con-
sists of N diﬀerentially private selection steps. The only con-
clusion an adversary can make with regards to the selection
process by observing the decision vectors {wkN}, is whether
any samples have been selected in the kth window or not.
Therefore, we consider only two events: one where no sam-
ple has been selected at all, i.e., Sk = 0, and the other where
at least one sample is selected to be queried for its label, i.e.,
Sk = 1. First, consider the case where w(k+1)N +1 = wkN +1,
which means that st = 0 for all kN < t ≤ (k + 1)N . In
this worst case scenario, there is no ambiguity as to which
points have been selected during the given window. This
means that the privacy guarantee when Sk = 0 selected is
the same as when we use the immediate update.

As for when Sk = 1, the information that is revealed in
this case is always less than when the selection results for
every observation is available to the adversary, which hap-
pens in the immediate update scenario. Consequently, the
privacy guarantee here is also no worse than that of the
selection strategies in the immediate update method, which
means that the entire selection step in the FLSW mini-batch
method is s-diﬀerentially private.

Gradient step.

The following Corollary states the privacy guarantee pro-

vided for a mini-batch gradient update.

Corollary 2. The gradient update step in Algorithm 3

is g-diﬀerentially private.

Proof. The proof follows from Lemma 3 with batch size

Bk.

Note that the guarantee in this Corollary is g by design
of the gradient step. Because each step averages Bk points,
the eﬀective noise per sample is reduced by a factor of Bk.
Since the gradient step is less noisy, the performance of the
algorithm should improve, empirically. However, because
the batch size varies in each length-N epoch, the noise level
will also be variable. As we will see, this variability can aﬀect
the empirical performance of the FLSW method. For larger
N the expected batch size E[Bk] will be larger, so longer
windows can help improve performance at the expense of a
(possibly) larger buﬀer size for the learning algorithm.
4.3.1 Overall Privacy Guarantees

Theorem 2. The FLSW algorithm in Algorithm 3 with
general N guarantees at most (s + g)-diﬀerential privacy.

Proof. Suppose that we run the algorithm for K it-
erations each consisting of N sample observations and a
gradient update. Similar to Theorem 1, deﬁne the collec-
tion of updates WK = (w0, wN , w2N . . . , wKN ) and also
QK = (S1, S2, . . . , SK ) where Sk indicates if any samples
have been labeled during the kth interval. We have

P(WKN , QK|D)
P(WKN , QK|D(cid:48))

(cid:12)(cid:12)(cid:12)(cid:12)

P(wkN , Qk|D, W(k−1)N )

P(wkN , Qk|D(cid:48), W(k−1)N )

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

k=1

K(cid:81)
K(cid:81)
T(cid:81)
T(cid:81)

k=1

t=1

t=1

=

(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)log

(a)
=

(b)
=

≤
≤ s + g,

P(wkN|D, w(k−1)N , st)P(st|D, w(k−1)N )

P(wkN|D(cid:48), w(k−1)N , st)P(st|D(cid:48), w(k−1)N )

P(wj|D, wj−1, sj)P(sj|D, wj−1)
P(wj|D(cid:48), wj−1, sj)P(sj|D(cid:48), wj−1)
P(wj|D, wj−1, sj)
P(wj|D(cid:48), wj−1, sj)

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:12)(cid:12)(cid:12)(cid:12) +

P(sj|D, wj−1)
P(sj|D(cid:48), wj−1)

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

(18)

where j here indicates the window in which the jth entries
of the datasets appear. Equalities (a) and (b) above are
explained in Theorem 1.
4.4 Mini-batch Update with FLMB

As mentioned earlier, the adversary can see the number
of data points observed by the learner before it collects L
labeled points. By randomizing the selection process and the
gradient update rule, we guarantee diﬀerential privacy of the
FLMB mini-batch algorithm. In this section, we discuss this
guarantee.

Selection step.

We use the same query strategies that are introduced in
the previous sections of this paper. However, the informa-
tion revealed to an adversary is not about whether any point
is selected or not, but about how many points are observed
before L points are selected. In the worst case, the number
of points in a given interval is also L, so the adversary could
infer that all points in that interval were selected. Thus
the privacy guarantee is the same as that of the selection
strategy of the immediate update method when a point is
selected. Any other event leaks less information than when
the adversary is able to learn about the results of all indi-
vidual selection decisions. Hence, the selection step in an
FLMB mini-batch method is s-diﬀerentially private.

Gradient Step.

The FLMB method guarantees a ﬁxed batch size of L
samples per iteration. Thus for the same privacy guarantee
g, it uses a factor L less noise.

Corollary 3. The gradient update step in in Algorithm

4 is g-diﬀerentially private.

Proof. The proof follows from Lemma 3 with batch size

L.
4.4.1 Overall Privacy Guarantees

123Theorem 3. The FLSW algorithm in Algorithm 4 with
batch size L guarantees at most (s +g)-diﬀerential privacy.
Proof. Suppose that we run the algorithm for T itera-
tions each consisting of Tl sample observations and a (pos-
sible) gradient update. As before, deﬁne the sets of outputs
WT = (w0, w1, w2, . . . , wT ) and QT = (T1, T2, . . . , TT ). We

(cid:12)(cid:12)(cid:12)(cid:12)

P(WT , QT|D)
P(WT , QT|D(cid:48))

P(wt, Tt|D, Wt−1)

P(wt, Tt|D(cid:48), Wt−1)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

t=1

T(cid:81)
T(cid:81)
T(cid:81)
T(cid:81)

t=1

t=1

t=1

=

have(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)log
(cid:12)(cid:12)(cid:12)(cid:12)log

(a)
=

(b)
=

≤
≤ s + g.

P(wt|D, wt−1, Tt)P(Tt|D, wt−1)

P(wt|D(cid:48), wt−1, Tt)P(Tt|D(cid:48), wt−1)

P(wj|D, wj−1, Tj)P(Tj|D, wj−1)
P(wj|D(cid:48), wj−1, Tj)P(Tj|D(cid:48), wj−1)
P(wj|D, wj−1, Tj)
P(wj|D(cid:48), wj−1, Tj)

(cid:12)(cid:12)(cid:12)(cid:12)log

(cid:12)(cid:12)(cid:12)(cid:12) +

P(Tj|D, wj−1)
P(Tj|D(cid:48), wj−1)

(cid:12)(cid:12)(cid:12)(cid:12)

(cid:12)(cid:12)(cid:12)(cid:12)

(19)

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(a) Error versus number of iterations for MNIST

where j indicates the window in which the jth entries of the
datasets appear. Equalities (a) and (b) above are explained
in Theorem 1.

5. NUMERICAL RESULTS

Datasets and preprocessing.

We apply our classiﬁcation methods on two real-world

datasets:

• The MNIST dataset contains images of handwritten dig-
its [34]. The task we deﬁne here is two separate digit
0 from digit 9. The training dataset consists of around
12000 images of handwritten digits 0 and 9 and the
test set contains less than 200 instances.

• The KDD Cup 1999 dataset [12] is about network intru-
sion detection and more closely matches our anomaly
detection motivation. This dataset consists of around
half a million examples. The task here is to build an in-
trusion detection model that can diﬀerentiate “normal”
network connections from “intrusions” or “attacks”.

For the MNIST dataset, we apply PCA to reduce the dimen-
tionality from the original 784 to 50. For the KDD Cup 1999
dataset we remove to irrelevant features (numoutbound_cmds
and is_host_login) and convert 7 remaining categorical
features into binary vectors. This process leaves us with
120 features. We then project all entries of both data sets
onto the unit ball.

Procedure.

In all experiments we set diﬀerential privacy guarantees
g = 1 and s = 1. We set p = e/(1+e) to make the Bernoulli
selection processes also 1-diﬀerentially private. Parameter
b in (7) is set to 0.2. We used step sizes ηt = η/t and
found values of η and the regularization parameter λ using

(b) Error versus number of iterations for KDD Cup 1999

Figure 2: Classiﬁer performance as a function of number of
labels requested.

cross-validation. Unless stated otherwise, the threshold τ in
Bernoulli strategies is set to e−0.2.

At every classiﬁer update iteration we add noise according

to (8), and the iterate is projected onto the uniform ball.

In all experiments, we averaged the results obtained over
10 random permutations of training data streams. The error
bars in Figures 4 are 1-standard deviation error bars.

Error rate over time.

Figures 2a and 2b show the misclassiﬁcation error rates
of FLSW and FLMB update methods for each dataset. For
both FLSW and FLMB, we set the selection window size to
5. We observe that even though all of the methods show fast
convergence to their limit value, due to the added gradient
noise we see a small gap between the non-private batch error
rate and the limit error rates of our proposed methods. After
processing many samples, progress slows due to the small
step size in the SGD iterations. This shows the trade-oﬀ
between privacy and accuracy in our algorithms.

A comparison between diﬀerent methods based on these
plots would not be fair as the number of labeled points that

050100150200250300350400450500Number of Iterations012345678910Error RateMNIST, Error over timeBerrnoulli-FLSWExponential-FLSWBerrnoulli-FLMBExponential-FLMBNonPrivate-Batch050100150200250300Number of Iterations01020304050607080Error RateKDD Cup 99, Error over timeBerrnoulli-FLSWExponential-FLSWBerrnoulli-FLMBExponential-FLMBNonPrivate-Batch124(a) Error versus labels requested for MNIST

(b) Error versus labels requested for KDD Cup 1999

Figure 3: Classiﬁer performance as a function of number of
labels requested.

the algorithms use diﬀers from one another. Therefore, In
the next section, we compare the error rates against number
of labeled points. However, from these experiments, we can
see that in general the FLMB performance tends to be better
than the FLSW performance. We conjecture that the ﬁxed
batch size controls the variance of the subgradient steps in
the SGD iterations.

Error rate versus label cost.

Figures 3a and 3b show the misclassiﬁcation error rates
as a function of label costs, assuming that each label costs
1 to acquire. Both results on MNIST and KDD Cup 99 show
that the FLMB method is more eﬃcient than FLSW in the
sense that it achieves better error rates for a given label bud-
get. We believe this is because for our choice of parameters,
the number of labeled points per iteration, denoted by B in
(10), in FLSW is smaller than or equal that of FLMB. This
means, according to update rule (10), larger mini-batches
are selected and smaller noise is added to the iterate during
an FLMB update.

We note that in our FLSW experiment in this section, the
values on the label complexity axis (x-axis) are not equally
spaced and the spaces also diﬀer across experiments over
diﬀerent permutations. In order to address this issue, we use
piece-wise linear interpolation and sample the interpolated
iterate values at the desired equally spaced points.

Figure 4: Classiﬁer performance as a function of batch size
for FLMB and FLSW with Bernoulli strategy

Selection method.

Surprisingly, we see from the experiments in Figures 2 and
3 that the diﬀerence between the Bernoulli and Exponential
selection methods is negligible in terms of overall perfor-
mance. We conjecture two reasons for this. First, by choos-
ing a large value of s for the selection step, the diﬀerence
between the two sampling distributions is not too signiﬁcant.
Second, the active learning heuristic provides the most im-
provements in the early stages of the classiﬁer training pro-
cess: once the classiﬁer has stabilized, additional training
samples do not improve the performance signiﬁcantly.

Mini-Batching.
Figure 4 shows the error rate of the FLMB update method
with Bernoulli selection strategy for batch sizes {1, 2, 5, 10, 20}
over both datasets. For the MNIST dataset the error rate is
reported after using 100 labeled data points, while for the
KDD Cup 99 the results are from 200 labeled examples. We
observe that mini-batching, as expected, reduces the vari-
ance in results.

Time-varying selection strategies.

Label costs are not necessarily constant over time. It can
be the case that the more the learner queries for labels, the
more the oracle charges per label. Moreover, as the classi-
ﬁer is updated using more labeled points, it becomes more
accurate and points far from the classiﬁer are more likely
to be noisy. Therefore, we would like to lower the chances
of labeling uninformative, noisy data points by making the
selection policy stricter.

As an example, we study the performance of both FLSW
and FLMB methods with time-varying Bernoulli selection
strategy over the KDD Cup 99 dataset. We compare using
a ﬁxed threshold τ = e−0.2 versus a time-varying thresh-
old τ = e− 1
t . The latter case uses a selection slab that
shrinks linearly over time. We remind the reader that t is
the counter for classiﬁer updates and not the data stream
counter. The results of this experiment are given in Figures
5a and 5b. Figure 5a shows a considerable reduction in the
label complexity in the time-varying query method. In fact,
we observe that the selection rate is not linear anymore and
becomes sublinear.

020406080100120140160180200Number of Labeled Points05101520253035404550Error rateMNIST -- Error Rate vs. Number of QueriesBerrnoulli-FLSWExponential-FLSWBerrnoulli-FLMBExponential-FLMBNonPrivate-Batch0200400600800100012001400160018002000Number of Labeled Points05101520253035404550Error rateKDD Cup 99 -- Error Rate vs. Number of QueriesBerrnoulli-FLSWExponential-FLSWBerrnoulli-FLMBExponential-FLMBNonPrivate-Batch02468101214161820Batch Size020406080Error RateKDD -- Error rate After 200 Labeled Samples02468101214161820Batch Size020406080Error RateMNIST -- Error rate After 100 Labeled Samples125quantify the diﬀerential privacy guarantees of these methods
and experimental results showing classiﬁcation performance
of our methods. Our experiments show that the proposed al-
gorithms have acceptable generalization performance while
preserving diﬀerential privacy of the users whose data is
given as input to the algorithms.

Our experiments also show that mini-batching reduces
the variance in the results and improves the convergence
of our algorithms. Although the diﬀerential privacy guar-
antees for our mini-batch methods are the same as the im-
mediate update method, the worst case scenarios are less
likely to happen as the selection window size gets larger.
We also demonstrate empirically that a time-varying selec-
tion strategy where the informativeness criterion becomes
stricter with time can substantially reduce label costs while
showing acceptable generalization performance, as long as
the mini-batching structure is not compromised.

Our procedure used a ﬁxed cost function over time; de-
signing cost-aware algorithms for settings where label cost
increases with the number of queries seems challenging. In-
vestigating tradeoﬀs with respect to  and sample size could
shed light on how stronger privacy guarantees aﬀect the
speed of learning and label complexity.
Identifying how
privacy risk and label complexity interact could yield algo-
rithms which can switch between more and less aggressive
active learning techniques.

Finally, because this work is motivated by problems in
anomaly detection, evaluating and adapting the private on-
line active learning framework here to domain-speciﬁc prob-
lems could give more guidance on how to set  in practical
scenarios. Implementing practical diﬀerentially private ma-
chine learning algorithms (as opposed to statistical aggre-
gates/estimators) can tell us when reasonable privacy guar-
antees are or are not possible. Another related problem
in this vein is online learning of feature maps in a diﬀer-
entially private manner. This could help characterize sta-
tistical properties of the anomalous class in an online and
sample-eﬃcient manner.

Acknowledgment
This material is based upon work supported by the U.S. De-
partment of Homeland Security under Award Number 2009-
ST-061-CCI002, by the Defense Advanced Research Projects
Agency (DARPA) and Space and Naval Warfare Systems
Center, Paciﬁc (SSC Paciﬁc) under contract No. N66001-
15-C-4070, and by the National Science Foundation under
award 1453432.

References
[1] Joaquin Quionero-Candela, Masashi Sugiyama, Anton
Schwaighofer, and Neil D. Lawrence. Dataset Shift in
Machine Learning. MIT Press, Cambridge, MA, 2009.
ISBN 0262170051, 9780262170055.

[2] Varun Chandola, Arindam Banerjee, and Vipin

Kumar. Anomaly detection: A survey. ACM
computing surveys (CSUR), 41(3):15:1–15:58, 2009.
URL http://doi.acm.org/10.1145/1541880.1541882.

[3] Cynthia Dwork, Frank McSherry, Kobbi Nissim, and

Adam Smith. Calibrating noise to sensitivity in
private data analysis. In Theory of Cryptography,
2006. URL http://dx.doi.org/10.1007/11681878 14.

(a) Number of labels requested vs number of iterations for ﬁxed
and time-varying Bernoulli strategy in FLMB for KDD Cup 1999

(b) Error versus labels for ﬁxed and shrinking slabs on KDD Cup
1999

Figure 5: Comparing ﬁxed and shrinking slabs for diﬀerent
strategies in terms of labels complexity and error perfor-
mance.

Interestingly, the generalization performance of the FLMB
method, for given number of labeled points, is not com-
promised. This is not surprising since each update uses a
ﬁxed mini-batch size and the learner is simply more selective
about informativeness and needs to wait for a longer time to
collect a batch. The results show that this selection strat-
egy is actually querying more informative points and does
not lose much by discarding more points. We expect that
for some datasets this strategy actually could improve the
performance by using its labeling budget more judiciously.
The story for the FLSW method is diﬀerent. The classi-
ﬁcation performance of this algorithm is considerably com-
promised. We suspect that with a shrinking selection slab
the bar for selection is too high; so few instances can pass
the informativeness threshold that this method (in later it-
erations) eﬀectively loses the “mini-batch” property.

6. CONCLUSION

We propose two randomized query strategies for privacy-
preserving selection of informative instances in a stream, and
explore two mini-batching techniques that improve the clas-
siﬁer performance of the Tong-Koller-based active learning
heuristic in our context. We provide theoretical analysis to

01002003004005006007008009001000Number of Observed Instances0200400600800Number of Labeled InstancesKDD -- Fixed SlabBernoulli FLSWBernoulli FLMB01002003004005006007008009001000Number of Observed Instances0200400600800Number of Labeled InstancesKDD -- Shrinking SlabBernoulli FLSWBernoulli FLMB0200400600800100012001400160018002000Number of Labeled Points01020304050Error rateKDD -- Shrinking SlabBerrnoulli-FLSWBerrnoulli-FLMBNonPrivate-Batch0200400600800100012001400160018002000Number of Labeled Points01020304050Error rateKDD -- Fixed SlabBerrnoulli-FLSWBerrnoulli-FLMBNonPrivate-Batch126[4] Raef Bassily, Adam Smith, and Abhradeep Thakurta.

Private empirical risk minimization: Eﬃcient
algorithms and tight error bounds. In 2014 IEEE 55th
Annual Symposium on Foundations of Computer
Science (FOCS), pages 464–473, 2014. URL
http://dx.doi.org/10.1109/FOCS.2014.56.

[5] John C. Duchi, Michael I. Jordan, and Martin J.

Wainwright. Local privacy and statistical minimax
rates. In 2013 IEEE 54th Annual Symposium on
Foundations of Computer Science (FOCS), pages
429–438, 2013. URL
http://dx.doi.org/10.1109/FOCS.2013.53.

on Machine Learning (ICML), pages 928–936, 2003.
URL http://www.aaai.org/Papers/ICML/2003/
ICML03-120.pdf.

[15] Shai Shalev-Shwartz. Online learning and online
convex optimization. Foundations and Trends in
Machine Learning, 4(2):107–194, 2011. URL
http://dx.doi.org/10.1561/2200000018.

[16] H. Brendan McMahan. A survey of algorithms and
analysis for adaptive online learning. arXiv preprint
arXiv:1403.3465, 2014. URL
http://arxiv.org/abs/1403.3465.

[6] Shuang Song, Kamalika Chaudhuri, and Anand D.

[17] Burr Settles. Active learning literature survey.

Sarwate. Stochastic gradient descent with
diﬀerentially private updates. In Proceedings of the
2013 Global Conference on Signal and Information
Processing (GlobalSIP 2013), pages 245–248, 2013.
URL
http://dx.doi.org/10.1109/GlobalSIP.2013.6736861.

[7] Shuang Song, Kamalika Chaudhuri, and Anand D.

Sarwate. Learning from data with heterogeneous noise
using sgd. In Proceedings of the Eighteenth
International Conference on Artiﬁcial Intelligence and
Statistics, pages 894–902, 2015. URL
http://jmlr.org/proceedings/papers/v38/song15.html.

[8] Stanley L. Warner. Randomized response: a survey

technique for eliminating evasive answer bias. Journal
of the American Statistical Association, 60(309):63–69,
1965. URL
http://dx.doi.org/10.1080/01621459.1965.10480775.

[9] Frank McSherry and Kunal Talwar. Mechanism

design via diﬀerential privacy. In 48th Annual IEEE
Symposium on Foundations of Computer Science
(FOCS), pages 94–103, October 2007. URL
http://dx.doi.org/10.1109/FOCS.2007.41.

[10] Simon Tong and Daphne Koller. Support vector
machine active learning with applications to text
classiﬁcation. Journal of Machine Learning Research,
2:45–66, November 2001. URL
http://www.jmlr.org/papers/v2/tong01a.html.

[11] Victoria J. Hodge and Jim Austin. A survey of outlier

detection methodologies. Artiﬁcial Intelligence
Review, 22(2):85–126, 2004. URL
http://dx.doi.org/10.1007/s10462-004-4304-y.

[12] Saharon Rosset and Aron Inger. KDD-cup 99:

knowledge discovery in a charitable organization’s
donor database. ACM SIGKDD Explorations
Newsletter, 1(2):85–90, 2000. . URL
http://doi.acm.org/10.1145/846183.846204.

[13] Hua Tang and Zhuolin Cao. Machine learning-based

intrusion detection algorithms. Journal of
Computational Information Systems, 5(6):1825–1831,
2009.

[14] Martin Zinkevich. Online convex programming and

generalized inﬁnitesimal gradient ascent. In
Proceedings of the Twentieth International Conference

Computer Sciences Technical Report 1648. 2010. URL
http://burrsettles.com/pub/settles.activelearning.pdf.

[18] Maria-Florina Balcan, Alina Beygelzimer, and John

Langford. Agnostic active learning. In Proceedings of
the 23rd international conference on Machine learning,
pages 65–72, 2006. URL
http://doi.acm.org/10.1145/1143844.1143853.

[19] Yifan Fu, Xingquan Zhu, and Bin Li. A survey on

instance selection for active learning. Knowledge and
Information Systems, 35(2):249–283, 2013. URL
http://dx.doi.org/10.1007/s10115-012-0507-8.

[20] Steve Hanneke. Rates of convergence in active

learning. The Annals of Statistics, 39(1):333–361,
2011. URL http://dx.doi.org/10.1214/10-AOS843.

[21] Sivan Sabato and Tom Hess. Interactive algorithms:

from pool to stream. In Proceedings of the 2016
Conference On Learning Theory (COLT 2016), pages
1419–1439, 2016. URL http:
//www.jmlr.org/proceedings/papers/v49/sabato16.

[22] Rui M. Castro and Robert D. Nowak. Minimax

bounds for active learning. IEEE Transactions on
Information Theory, 54(5):2339–2353, 2008. URL
http://dx.doi.org/10.1109/TIT.2008.920189.

[23] Michael Horstein. Sequential transmission using

noiseless feedback. IEEE Transactions on Information
Theory, 9(3):136–143, 1963. URL
http://dx.doi.org/10.1109/TIT.1963.1057832.

[24] Cynthia Dwork and Aaron Roth. The algorithmic

foundations of diﬀerential privacy. Foundations and
Trends in Theoretical Computer Science, 9(3-4):
211–407, 2014. URL
http://dx.doi.org/10.1561/0400000042.

[25] John C. Duchi, Michael I. Jordan, and Martin J.

Wainwright. Privacy aware learning. Journal of the
ACM (JACM), 61(6):38:1–38:57, 2014. URL
http://doi.acm.org/10.1145/2666468.

[26] Cynthia Dwork, Moni Naor, Toniann Pitassi, and

Guy N. Rothblum. Diﬀerential privacy under
continual observation. In Proceedings of the
forty-second ACM symposium on Theory of
computing, pages 715–724, 2010. URL
http://doi.acm.org/10.1145/1806689.1806787.

127[27] Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N.

[31] Peter Kairouz, Sewoong Oh, and Pramod Viswanath.

Rothblum, and Sergey Yekhanin. Pan-private
streaming algorithms. In Proceedings of The First
Symposium on Innovations in Computer Science (ICS
2010), 2010. URL http://www.wisdom.weizmann.ac.
il/mathusers/naor/PAPERS/pan private.pdf.

The composition theorem for diﬀerential privacy. In
Proceedings of The 32nd International Conference on
Machine Learning, pages 1376–1385, 2015. URL http:
//www.jmlr.org/proceedings/papers/v37/kairouz15.

[28] Prateek Jain, Pravesh Kothari, and Abhradeep

[32] Kamalika Chaudhuri, Claire Monteleoni, and

Thakurta. Diﬀerentially private online learning. In
Proceedings of the 25th Annual Conference on
Learning Theory (COLT 2012), volume 23 of JMLR
Workshop and Conference Proceedings, pages
24.1–24.34, 2012. URL http://www.jmlr.org/
proceedings/papers/v23/jain12/jain12.

[29] Maria-Florina Balcan and Vitaly Feldman. Statistical

active learning algorithms. In Advances in Neural
Information Processing Systems, pages 1295–1303,
2013. URL http://papers.nips.cc/paper/
5101-statistical-active-learning-algorithms.pdf.

Anand D. Sarwate. Diﬀerentially private empirical
risk minimization. Journal of Machine Learning
Research, 12:1069–1109, 2011. URL
http://www.jmlr.org/papers/v12/chaudhuri11a.

[33] Frank McSherry. Privacy integrated queries: an
extensible platform for privacy-preserving data
analysis. Communications of the ACM, 53(9):89–97,
2010. URL
http://doi.acm.org/10.1145/1810891.1810916.

[30] Larry Wasserman and Shuheng Zhou. A statistical

[34] Yann Lecun, L´eon Bottou, Yoshua Bengio, and

framework for diﬀerential privacy. Journal of the
American Statistical Association, 105(489):375–389,
2010. URL
http://dx.doi.org/10.1198/jasa.2009.tm08651.

Patrick Haﬀner. Gradient-based learning applied to
document recognition. Proceedings of the IEEE, 86
(11):2278–2324, 1998. URL
http://dx.doi.org/10.1109/5.726791.

128