2013 IEEE Symposium on Security and Privacy

Protecting User Privacy From Perceptual Applications

A Scanner Darkly:

Suman Jana‚àó

‚àóThe University of Texas at Austin

Arvind Narayanan‚Ä†
‚Ä†Princeton University

Vitaly Shmatikov‚àó

programmable robots, even moving around‚Äîraises interest-
ing privacy issues for their users. Many people are already
uncomfortable with law enforcement agencies conducting
large-scale face recognition [2, 17]. Perceptual applications
running in one‚Äôs home or a public area may conduct
unauthorized surveillance, intentionally or unintentionally
overcollect information (e.g., keep track of other people
present in a room), and capture sensitive data such as credit
card numbers, license plates, contents of computer monitors,
etc. that accidentally end up in their Ô¨Åeld of vision.

Abstract‚ÄîPerceptual, ‚Äúcontext-aware‚Äù applications that ob-
serve their environment and interact with users via cameras
and other sensors are becoming ubiquitous on personal com-
puters, mobile phones, gaming platforms, household robots,
and augmented-reality devices. This raises new privacy risks.
We describe the design and implementation of DARKLY, a
practical privacy protection system for the increasingly com-
mon scenario where an untrusted, third-party perceptual ap-
plication is running on a trusted device. DARKLY is integrated
with OpenCV, a popular computer vision library used by such
applications to access visual inputs. It deploys multiple privacy
protection mechanisms, including access control, algorithmic
privacy transforms, and user audit.

We evaluate DARKLY on 20 perceptual applications that per-
form diverse tasks such as image recognition, object tracking,
security surveillance, and face detection. These applications
run on DARKLY unmodiÔ¨Åed or with very few modiÔ¨Åcations
and minimal performance overheads vs. native OpenCV. In
most cases, privacy enforcement does not reduce the appli-
cations‚Äô functionality or accuracy. For the rest, we quantify
the tradeoff between privacy and utility and demonstrate that
utility remains acceptable even with strong privacy protection.

I. INTRODUCTION

Modern software programs increasingly include percep-
tual functionality that takes advantage of high-resolution
cameras and other sensors to observe their users and physi-
cal environment. Perceptual software includes ‚Äúnatural user
interface‚Äù systems that
interact with users via gestures
and sounds, image recognition applications such as Google
Goggles, security software such as motion detectors and
face recognizers, augmented reality applications, ‚Äúambient
computing‚Äù frameworks, a variety of video-chat and tele-
presence programs, and other context-aware software.

Hardware platforms for perceptual applications include
mobile phones, programmable robotic pets and household
robots (e.g., iRobot Create platform), gaming devices (e.g.,
Kinect), augmented reality displays (e.g., Google Glass),
and conventional computers equipped with webcams. Many
platforms provide app stores‚Äîfor example robotappstore.
com (‚Äúyour robots are always up-to-date with the coolest
apps‚Äù)‚Äîenabling consumers to download and execute thou-
sands of third-party perceptual applications.

The growing availability and popularity of potentially
untrusted perceptual applications capable of scanning their
surroundings at Ô¨Åne level of detail‚Äîand, in the case of

¬© 2012, Suman Jana. Under license to IEEE.
DOI 10.1109/SP.2013.31

349

General-purpose, data-agnostic privacy technologies such
as access control and privacy-preserving statistical analysis
are fairly blunt tools. Instead, we develop a domain-speciÔ¨Åc
solution, informed by the structure of perceptual applications
and the computations they perform on their inputs, and ca-
pable of applying protection at the right level of abstraction.
Our system, DARKLY, is a privacy protection layer for
untrusted perceptual applications operating on trusted de-
vices. Such applications typically access input data from
the device‚Äôs perceptual sensors via special-purpose software
libraries. DARKLY is integrated with OpenCV, a popular
computer vision library which is available on Windows,
Linux, MacOS, iOS, and Android and supports a diverse
array of input sensors including webcams, Kinects, and
smart cameras. OpenCV is the default vision library of the
Robot Operating System (ROS); our prototype of DARKLY
has been evaluated on a Segway RMP-50 robot running
ROS Fuerte. DARKLY is language-agnostic and can work
with OpenCV programs writen in C, C++, or Python. The
architecture of DARKLY is not speciÔ¨Åc to OpenCV and can
potentially be adapted to another perceptual software library
with a sufÔ¨Åciently rich API.

We evaluate DARKLY on 20 existing OpenCV applica-
tions chosen for the diversity of their features and perceptual
tasks they perform,
including security surveillance with
motion detection, handwriting recognition, object tracking,
shape detection, face recognition, background-scenery re-
moval from video chat, and others.

18 applications run on DARKLY unmodiÔ¨Åed, while 2 re-
quired minor modiÔ¨Åcations. The functionality and accuracy
of most applications are not degraded even with maximum
privacy protection. In all cases, performance with DARKLY
is close to performance on ‚Äúnative‚Äù OpenCV.

II. THREAT MODEL AND DESIGN OF DARKLY

We focus on the scenario where the device, its operating
system, and the hardware of its perceptual sensors are
trusted, but the device is executing an untrusted third-party
application. The application can be arbitrarily malicious, but
it runs with user-level privileges and can only access the
system, including perceptual sensors, through a trusted API
such as the OpenCV computer vision library.

	 
	 

$$
$$








"
"




%!




"
"








"
"
!

!

 
 



"
"



Figure 1. System architecture of DARKLY.

The system model of DARKLY is shown in Fig. 1 with
the trusted components shaded. DARKLY itself consists of
two parts, a trusted local server and an untrusted client
library. We leverage standard user-based isolation provided
by the OS: the DARKLY server is a privileged process with
direct access to the perceptual sensors, while applications
run as unpriviliged processes that can only access the
sensors through DARKLY. Furthermore, we assume that no
information about DARKLY operation (e.g.,
side-channel
screenshots of its console) can be obtained via system calls.
The untrusted DARKLY client library runs as part of each
application process and communicates with the DARKLY
server. This is merely a utility for helping applications access
the perceptual API and the system remains secure even if a
malicious application modiÔ¨Åes this library.

A major challenge in this design is Ô¨Åguring out which
parts of the input should be revealed to the application and
in what form, while protecting ‚Äúprivacy‚Äù in some fashion.
Visual data in particular are extremely rich and diverse,
making it difÔ¨Åcult to isolate and identify individual objects.
Existing methods for automated image segmentation are too
computationally expensive to be applied in real time and
suffer from high false positives and false negatives.

DARKLY applies multiple layers of privacy protection
to solve the problem: access control, algorithmic transfor-
mation, and user audit. First, it replaces raw perceptual
inputs with opaque references. Opaque references cannot
be dereferenced by an application, but can be passed to and
from trusted library functions which thus operate on true per-
ceptual data without loss of Ô¨Ådelity. This allows applications
to operate on perceptual inputs without directly accessing
them. This approach is so natural that privacy protection is

completely transparent to many existing applications: they
work on DARKLY without any modiÔ¨Åcations to their code
and without any loss of accuracy or functionality.

Second, some applications such as security cameras and
object trackers require access to certain high-level features of
the perceptual inputs. To support such applications, DARKLY
substitutes the corresponding library API with declassiÔ¨Åer
functions that apply appropriate feature- or object-speciÔ¨Åc
(but application-independent!) privacy transforms before re-
turning the data to the application. Example of transforms
include sketching (a combination of low-pass Ô¨Åltering and
contour detection) and generalization (mapping the object to
a generic representative from a predeÔ¨Åned dictionary).

To help balance utility and privacy, the results of applying
a privacy transform are shown to the user in the DARKLY
console window. The user can control the level of transfor-
mation via a dial and immediately see the results. In our
experience, most applications do not need declassiÔ¨Åers, in
which case DARKLY protects privacy without any loss of
accuracy and the DARKLY console is not used. For those
of our benchmark applications that use declassiÔ¨Åers, we
quantitatively evaluate the degradation in their functionality
depending on the amount of transformation.

DARKLY provides built-in trusted services, including a
trusted GUI‚Äîwhich enables a perceptual application to
show the result of computation to the user without accessing
it directly‚Äîand trusted storage. For example, after the
security camera detects motion, it can store the actual images
in the user‚Äôs Google Drive without ‚Äúseeing‚Äù them.

A few applications, such as eigenface-based face rec-
ognizers, need to operate directly on perceptual
inputs.
DARKLY provides a domain-speciÔ¨Åc ibc language based
on GNU bc. Isolating domain-speciÔ¨Åc programs is much
easier than isolating arbitrary code. Untrusted ibc programs
are executed on the raw inputs, but have no access to the
network, system calls, or even system time. Furthermore,
DARKLY only allows each invocation to return a single
32-bit value to the application. We show that legitimate
computations can be ported to ibc with little difÔ¨Åculty.

...
// Grab a frame from camera
img=cvQueryFrame(..);
// Process the image to filter out unrelated stuff
...
// Extract a binary image based on the ball‚Äôs

color

cvInRangeS(img, ...);
...
// Process the image to filter out unrelated stuff
...
// Compute the moment
cvMoments(...);

// Compute ball‚Äôs coordinates using moment
...
// Move robot towards the calculated coordinates
...

Listing 1. Outline of the ball-tracking robot application.

350

To illustrate how DARKLY works on a concrete example,
Listing II shows a simpliÔ¨Åed ball-tracking application for a
robotic dog. The code on the light gray background does
not need direct access to image contents and can operate on
opaque references. The code on the dark gray background
invokes a DARKLY declassiÔ¨Åer, which applies a suitable
privacy transform to the output of the cvMoments OpenCV
function. The rest of the code operates on this transformed
data. DARKLY thus ensures that the application ‚Äúsees‚Äù only
the position of the ball. The accuracy of this position
depends on the privacy transform and can be adjusted by
the user via the privacy dial.

III. PRIVACY RISKS OF PERCEPTUAL APPLICATIONS

What does a scanner see? Into the head? Down into the
heart? Does it see into me, into us? Clearly or darkly?

A Scanner Darkly (2006)

Perceptual applications present unique privacy risks. For
example, a security-cam application, intended to detect mo-
tion in a room and raise an alarm, can leak collected video
feeds. A shape detector can read credit card numbers, text on
drug labels and computer screens, etc. An object or gesture
tracker‚Äîfor example, a robot dog programmed to follow
hand signals and catch thrown balls‚Äîcan be turned into
a roving spy camera. A face detector, which hibernates the
computer when nobody is in front of it, or a face recognizer,
designed to identify its owner, can surreptitiously gather
information about people in the room. A QR code scanner, in
addition to decoding bar codes, can record information about
its surroundings. App stores may have policing mechanisms
to remove truly malicious applications, but these mecha-
nisms tend to be ineffective against applications that collect
privacy-sensitive information about their users.
Overcollection and aggregation. The privacy risks of
perceptual applications fall into several hierarchical cate-
gories. The Ô¨Årst is overcollection of raw visual data and
the closely related issue of aggregation. The problem of
aggregation is similar to that of public surveillance: a single
photograph of a subject
in a public place might make
that individual uncomfortable, but it is the accumulation of
these across time and space that is truly worrying. Even
ignoring speciÔ¨Åc inferential privacy breaches made possible
by this accumulation, aggregation itself may inherently be
considered a privacy violation. For example, Ryan Calo
argues that ‚ÄúOne of the well-documented effects of interfaces
and devices that emulate people is the sensation of being
observed and evaluated. Their presence can alter our attitude,
behavior, and physiological state. Widespread adoption of
such technology may accordingly lessen opportunities for
solitude and chill curiosity and self-development.‚Äù [4]

Many applications in DARKLY work exclusively on
opaque references (Section VI-B), in which case the ap-
plication gets no information and the aggregation risk does

not arise. For applications that do access some objects and
features of the image, we address aggregation risks with
the DARKLY console (Section VIII). The DARKLY console
is an auxiliary protection mechanism that visually shows
the outputs of privacy transforms to the user, who has the
option to adjust the privacy dial, shut down the application,
or simply change his or her behavior. A small amount of
leakage may happen before the user has time to notice
and react to the application‚Äôs behavior, but we see this as
categorically different from the problem of aggregation. The
DARKLY console is rougly analogous to the well-established
privacy indicators in smartphones that appear when location
and other sensory channels are accessed by applications.
Inference. The Ô¨Årst category of inference-based privacy
risks is speciÔ¨Åc, sensitive pieces of information‚Äîanything
from a credit card number to objects in a room to a person‚Äôs
identity‚Äîthat are leaked by individual frames.

DARKLY addresses such threats by being domain- and
data-dependent, unlike most privacy technologies. Privacy
transforms (see Section VII), speciÔ¨Åcally sketching, mini-
mize leakage at a frame-by-frame level by interposing on
calls that return speciÔ¨Åc features of individual images (see
examples in Figs. 2 and 3). Privacy protection is thus speciÔ¨Åc
to the domain and perceptual modality in question, and
some privacy decisions are made by actually examining the
perceptual inputs. In contrast to basic access control, this
domain-speciÔ¨Åc design sacriÔ¨Åces the simplicity of imple-
mentation and reasoning. In exchange, we gain the ability to
provide the far more nuanced privacy properties that users
intuitively expect from perceptual applications.

The last category in the hierarchy of privacy risks is
semantic inference. For example, even a sketch may al-
low inference of potentially sensitive gestures, movements,
proximity of faces, bodies, etc. It is unlikely these risks
can be mitigated completely except for speciÔ¨Åc categories
of applications, mainly those that can function solely with
opaque references or require only numerical features such as
histograms where techniques like differential privacy [9, 10]
may apply. Unless the transformed data released to the
application is sufÔ¨Åciently simple to reason about analytically,
the semantic inference risk will exist, especially due to the
continual nature of perceptual observation.

That said, a machine-learning-based, data-dependent ap-
proach to privacy transforms offers some hope. For example,
in Section VII-B, we describe how to use facial identiÔ¨Åcation
technology to transform a face into a privacy-preserving
‚Äúcanonical representation.‚Äù The key idea here is to take a
technology that leads to the inference risk, namely facial
recognition, and turns it on its head for privacy protection.
It is plausible that this paradigm can be extended to handle
other types of inference, and as more complex inference
techniques are developed, privacy transforms will co-evolve
to address them. This is left to future work.

351

IV. STRUCTURE OF PERCEPTUAL APPLICATIONS

DARKLY is based on the observation that most legiti-
mate applications do not need unrestricted access to raw
inputs. This is reÔ¨Çected in their design. For
perceptual
example, most existing OpenCV applications do not access
raw images (see Section IX) because implementing complex
computer vision algorithms is difÔ¨Åcult even for experienced
developers. Fortunately, the OpenCV API is at the right
level of abstraction: it provides domain-speciÔ¨Åc functions
for common image-processing tasks that applications use
as building blocks. This enables applications to focus on
speciÔ¨Åc objects or features, leaving low-level image analysis
to OpenCV functions and combining them in various ways.
DARKLY ensures that these functions return the information
that applications need to function‚Äîbut no more!

Perceptual applications can be classiÔ¨Åed into three general
categories: (1) those that do not access the perceptual inputs
apart from invoking standard library functions; (2) those that
access speciÔ¨Åc, library-provided features of the inputs; and
(3) those that must execute their own code on raw inputs. For
applications in the Ô¨Årst category, DARKLY completely blocks
access to the raw data. For the second category, DARKLY
provides declassiÔ¨Åer functions that apply privacy transforms
to the features before releasing them to the application. For
the third category, DARKLY isolates untrusted code to limit
the leakage of sensitive information.

For example, a security camera only needs to detect
changes in the scene and invoke a trusted service to store
the image (and maybe raise an alarm). This requires the
approximate contours of objects, but not their raw pixels.
Trackers need objects‚Äô moments to compute trajectories, but
not objects themselves. A QR scanner works correctly with
only a thresholded binary representation of the image, etc.
DARKLY is designed to support more sophisticated func-
tionalities, too. For example, applications dealing with hu-
man faces can be classiÔ¨Åed into ‚Äúdetectors‚Äù and ‚Äúrecogniz-
ers.‚Äù Face detectors are useful for non-individualized tasks
such as emotion detection or face tracking‚Äîfor example,
a robotic pet might continually turn to face the user‚Äîand
need to know only whether there is a rectangle containing
a face in their Ô¨Åeld of vision. To support such applications,
DARKLY provides a privacy transform that returns a generic
representation of the actual face.

Face recognizers, on the other hand, must identify speciÔ¨Åc
faces, e.g., for visual authentication. Even in this case, a
recognizer may run an algorithm comparing faces in the
image with a predeÔ¨Åned face but only ask for a single-bit
answer (match or no match). To support such applications,
DARKLY allows execution of arbitrary image analysis code,
but rigorously controls the information it can export.

V. DESIGN PRINCIPLES OF DARKLY

Block direct access to perceptual inputs. DARKLY inter-
poses on all accesses by applications to cameras and other

352

perceptual sensors. As shown in Fig. 1, this privacy protec-
tion layer is implemented as a DARKLY server that runs as
a privileged ‚Äúuser‚Äù on the same device as the applications;
only this user can access the sensors. Applications interact
with the DARKLY server via inter-process sockets (UNIX
domain sockets) and standard OS user isolation mechanisms
prevent them from accessing the state of DARKLY.

The key concept in DARKLY is opaque reference. Opaque
references are handles to image data and low-level rep-
resentations returned by OpenCV functions. An applica-
tion cannot dereference them, but can pass them to other
OpenCV functions, which internally operate on unmodiÔ¨Åed
data without any loss of Ô¨Ådelity. Applications can thus per-
form sophisticated perceptual tasks by ‚Äúchaining together‚Äù
multiple OpenCV functions. In Section IX, we show that
many existing applications produce exactly the same output
when executed on DARKLY vs. unmodiÔ¨Åed OpenCV.

A similar architectural approach is used by PINQ [18], a
system for privacy-preserving data analysis. PINQ provides
an API for basic data-analysis queries such as sums and
counts. Untrusted applications receive opaque handles to
the raw data (PINQueryable objects) which they cannot
dereference, but can pass to and from trusted API functions
thus constructing complex queries.

DARKLY also provides trusted services which an applica-
tion can use to ‚Äúobliviously‚Äù export data from the system,
if needed. For example, after a security-camera application
detects motion in the room, it can use a trusted remote-
storage service to store the captured image in the user‚Äôs
Google Drive‚Äîwithout accessing its pixels!
Support unmodiÔ¨Åed applications, whenever possible.
DARKLY is language-independent and works equally well
with OpenCV applications written in C, C++, or Python. It
changes neither the API of the existing OpenCV functions,
nor OpenCV‚Äôs types and data structures. Instead, opaque
references replace pointers to raw pixels in the meta-data of
OpenCV objects. DARKLY is thus completely transparent to
applications that do not access raw image data, which are the
majority of the existing OpenCV applications (Section IX).
Use multiple layers of privacy protection. Applications
that do not access raw inputs assemble their functionality by
passing opaque references to and from OpenCV functions.
For applications that work with high-level features, DARKLY
provides declassiÔ¨Åers that replace these features with safe
representations generated by the appropriate privacy trans-
forms (Section VII). Privacy transforms keep the information
that applications need for their legitimate functionality while
removing the details that may violate privacy.
Inform the user. To help the user balance utility and
privacy, our system includes a trusted DARKLY console. For
applications that operate solely on opaque references, this
window is blank. For applications that use declassiÔ¨Åers to
access certain input features, it shows to the user the outputs

of the privacy transforms being used by the application at
any point in time (Section VIII).

The DARKLY console window also contains a privacy dial
that goes from 0 to 11. By adjusting the dial, the user can
increase or decrease the degree of privacy transformation.
the setting of 0, DARKLY provides signiÔ¨Åcant
Even at
privacy protection;
in particular, applications are always
blocked from directly accessing raw image data.
Be Ô¨Çexible. In rare cases, applications may need to execute
arbitrary code on raw inputs. For example, one of our
benchmark applications runs the eigenface algorithm [26]
to match a face against a database (see Section VI-F).

For such applications, DARKLY provides a special ibc
language inspired by GNU bc [1]. Applications can supply
arbitrary ibc programs which DARKLY executes internally.
These programs are almost pure computations and have no
access to the network, system calls, or even system time
(Section VI-F). Furthermore, DARKLY restricts their output
to 32 bits, thus blocking high-bandwidth covert channels.

VI. IMPLEMENTATION

The prototype implementation of DARKLY consists of
approximately 10,000 lines of C/C++ code, not counting the
ported ibc compiler and VM.
A. OpenCV

OpenCV provides C, C++, and Python interfaces [20]
on Windows, Linux, MacOS, iOS and Android. OpenCV
is also the default vision library of the Robot Operating
System (ROS), a popular platform that runs on 27 robots
ranging from the large Willow Garage PR2 to the small
iRobot Create or Lego NXT. OpenCV supports diverse input
sensors including webcams, Kinects and smart cameras like
VC nano 3D1 or PicSight Smart GigE.2

The OpenCV API has more than 500 functions that
applications‚Äîranging from interactive art to robotics‚Äîuse
for image-processing and analysis tasks. Our prototype cur-
rently supports 145 of these functions (see Section IX for
a survey of OpenCV usage in existing applications). Our
design exploits both the richness of this API and the fact
that individual OpenCV functions encapsulate the minutiae
of image processing, relieving applications of the need to
access raw image data and helping DARKLY interpose pri-
vacy protection in a natural way. That said, the architecture
of DARKLY is not speciÔ¨Åc to OpenCV and can be applied
to any perceptual platform with a sufÔ¨Åciently rich API.

OpenCV comprises several components: libcxcore imple-
ments internal data structures, drawing functions, clustering
algorithms, etc.; libcv ‚Äì image processing and computer
vision tasks such as image transformations, Ô¨Ålters, motion
analysis, feature detection, camera calibration, and object

1http://www.vision-components.com/en/products/smart-cameras/

vc-nano-3d/

2http://www.leutron.com/cameras/smart-gige-cameras/

detection; libhighgui ‚Äì functions for creating user interfaces;
libml ‚Äì machine learning algorithms; libcvaux ‚Äì auxiliary
algorithms such as principal component analysis, hidden
markov models, view morphing, etc.

OpenCV deÔ¨Ånes data structures for image data (IplImage,
CvMat, CvMatND, etc.), helper data structures (CvPoint,
CvRect, CvScalar, etc.), and dynamic data structures (CvSeq,
CvSet, CvTree, CvGraph, etc.). OpenCV also provides
functions for creating, manipulating, and destroying these
objects. For example, cvLoadImage creates an IplImage
structure and Ô¨Ålls it with the image‚Äôs pixels and meta-data,
while cvQueryFrame fetches a frame from a camera or video
Ô¨Åle and creates an IplImage structure with the frame‚Äôs pixels.
The OpenCV API thus helps developers to program their
applications at a higher level. For example, the following 8
lines of C code invert the image and display it to the user
until she hits a key:
1
2
3
4
5
6
7
8
9
10
11
12
13
14

IplImage* img = 0;
// load an image
img=cvLoadImage(argv[1]);
// create a window
cvNamedWindow("mainWin", CV_WINDOW_AUTOSIZE);
cvMoveWindow("mainWin", 100, 100);
// invert the image
cvNot(img, img);
// show the image
cvShowImage("mainWin", img );
// wait for a key
cvWaitKey(0);
// release the image
cvReleaseImage(&img );

OpenCV permits only one process at a time to access the
camera, thus DARKLY does not allow concurrent execution
of multiple applications.

B. Opaque references

To block direct access to raw images, DARKLY replaces
pointers to image data with opaque references that cannot
be dereferenced by applications. Applications can still pass
them as arguments into OpenCV functions, which derefer-
ence them internally and access the data.

To distinguish opaque references and real pointers,
DARKLY exploits the fact that the lower part of the address
space is typically reserved for the OS code, and therefore
all valid pointers must be greater than a certain value. For
example, in standard 32-bit Linux binaries, all valid stack
and heap addresses are higher than 0x804800. The values
of all opaque references are below this address.

DARKLY cannot simply return an opaque reference in lieu
of a pointer to an OpenCV object. Some existing, benign
applications do dereference pointers, but only read the meta-
data stored in the object, not the image data. For example,
consider this fragment of an existing application:
surfer = cvLoadImage("surfer.jpg",

CV_LOAD_IMAGE_COLOR);

...
size = cvGetSize(surfer);

353

/* create an empty image, same size, depth and

mprotect(page, getpagesize(), PROT_WRITE|

result = cvCreateImage(size, surfer->depth, surfer

vptr[vt_index] = (int)our_func;

channels of others */

->nChannels);

PROT_READ))

}

Here, surfer is an instance of IplImage whose meta-data
includes the number of channels and the depth of the image.
Even though this code does not access the pixel values,
it would crash if DARKLY returned an opaque reference
instead of the expected pointer to an IplImage object.

DARKLY exploits the fact that most OpenCV data struc-
tures for images and video include a separate pointer to the
actual pixel data. For example, IplImage‚Äôs data pointer is
stored in the imageData Ô¨Åeld; CvMat‚Äôs data pointer is in
the data Ô¨Åeld. For these objects, DARKLY creates a copy of
the data structure, Ô¨Ålls the meta-data, but puts the opaque
reference in place of the data pointer. Existing applications
can thus run without any modiÔ¨Åcations as long as they do
not dereference the pointer to the pixels.

C. Interposition

To support unmodiÔ¨Åed applications, DARKLY must in-
terpose on their calls to the OpenCV library. All of the
applications we tested use the dynamically linked version
of OpenCV. We implemented DARKLY‚Äôs interposition layer
as a dynamically loaded library and set the LD PRELOAD
shell variable to instruct Linux‚Äôs dynamic linker to load it
before OpenCV. The functions in the interposition library
have the same names as the OpenCV functions, thus the
linker redirects OpenCV calls made by the application.

This approach works for C functions, but there are several
complications when interposing on C++ functions. First, the
types of the arguments to DARKLY‚Äôs wrapper functions must
be exactly the same as those of their OpenCV counterparts
because the C++ compiler creates new mangled symbols
based on both the function name and argument types.

The second, more challenging issue is C++ virtual func-
tions. Because their bindings are resolved at runtime, they
are not exported as symbols for the linker to link against.
Instead, their addresses are stored in per-object vtables. To
interpose on calls to a virtual function, DARKLY overrides
the constructor of the class deÔ¨Åning the function. The new
constructor overwrites the appropriate entries in the vtables
of newly created objects with pointers to DARKLY wrappers
instead of the genuine OpenCV functions. The formats of
objects and vtables are compiler-dependent: for example,
GCC stores the vtable address in the object‚Äôs Ô¨Årst 4 bytes.
Our code for hooking vtables is as follows:3
extern "C" void patch_vtable(void *obj, int

vt_index, void *our_func) {

int* vptr = *(int**)obj;
// align to page size:
void* page = (void*)(int(vptr) & Àú(getpagesize()

-1));

// make the page with the vtable writable

The vt index parameter speciÔ¨Åes the index of the vtable
entry to be hooked. GCC creates vtable entries in the order
of the virtual function declarations in the class source Ô¨Åle.
Dispatching OpenCV functions. For each call made by an
application to an OpenCV function, the interposition library
must decide whether to execute it within the application or
forward it to the trusted DARKLY server running as a sepa-
rate ‚Äúuser‚Äù on the same device (only this server has access
to camera inputs). To complicate matters, certain OpenCV
functions accept variable-type arguments, e.g., cvNot accepts
either IplImage, or CvMat. OpenCV detects the actual type
at runtime by looking at the object‚Äôs header.

After intercepting a call

to an OpenCV function,

the
interposition library determines the type of each argument
and checks whether it contains an opaque reference (the
actual check depends on the object‚Äôs type). If there is at
least one argument with an opaque reference, executing
the function requires access to the image. The interposition
library marshals the local arguments and opaque references,
and forwards the call to DARKLY for execution.

If none of the arguments contain an opaque reference,
the function does not access the image and the interposition
library simply calls the function in the local OpenCV library.

D. Privacy transforms

For

to image

that need access
to detect motion,

fea-
applications
tures‚Äîfor example,
track certain ob-
jects, etc.‚ÄîDARKLY provides declassiÔ¨Åer functions. Our
prototype includes the following declassiÔ¨Åers: cvMoments
returns moments, cvFindContours ‚Äì contours, cvGoodFea-
turesToTrack ‚Äì sets of corner points, cvCalcHist ‚Äì pixel
histograms, cvHaarDetectObjects ‚Äì bounding rectangles for
objects detected using a particular model (DARKLY restricts
applications to predeÔ¨Åned models shipped with OpenCV),
cvMatchTemplate ‚Äì a map of comparison results between
the input image and a template, cvGetImageContent ‚Äì image
contents (transformed to protect privacy).

DeclassiÔ¨Åer
cvMoments
cvFindContours
cvGoodFeaturesToTrack
cvCalcHist
cvHaarDetectObjects
cvMatchTemplate
cvGetImageContent

Privacy transform
Sketching
Sketching
Increasing feature threshold
Sketching
Generalization
Thresholding match values
Thresholding binary image

3Cf. http://www.yosefk.com/blog/machine-code-monkey-patching.html

TRANSFORMS USED FOR EACH DARKLY DECLASSIFIER.

Table I

354

DeclassiÔ¨Åers apply an appropriate privacy transform (see
Section VII) to the input, as shown in Table I. For example,
cvGetImageContent returns a thresholded binary represen-
tation of the actual image. Furthermore, these outputs are
displayed on the DARKLY console to inform the user.

E. Trusted services

Trusted services in DARKLY enable the application to

send data to the user without actually ‚Äúseeing‚Äù it.
Trusted display. The trusted display serves a dual purpose:
(1) an application can use it to show images to which it
does not have direct access, and (2) it shows to the user
the privacy-transformed features and objects released to the
application by declassiÔ¨Åers (see Section VIII).

We assume that the OS blocks the application from read-
ing the contents of the trusted display via ‚Äúprint screen‚Äù and
similar system calls. These contents may also be observed
and recaptured by the device‚Äôs own camera. We treat this
like any other sensitive item in the camera‚Äôs Ô¨Åeld of vision
(e.g., contents of an unrelated computer monitor).

To enable applications to display images without access
interpose on HighGUI,
to their contents, DARKLY must
OpenCV‚Äôs user interface (UI) component [13]. HighGUI is
not as extensive as some other UI libraries such as Qt, but
the general principles of our design are applicable to any UI
library as long as it is part of the trusted code base. Among
other things, HighGUI supports the creation and destruction
of windows via its CvNamedWindow and CvDestroyWindow
functions. Applications can also use cvWaitKey to receive
keys pressed by the user, cvSetMouseCallback to set custom
callback functions for mouse events, and cvCreateTrackbar
to create sliders and set custom handlers.

The interposition library forwards calls to any of these
functions to DARKLY. For functions like CvNamedWindow,
DARKLY simply calls the corresponding OpenCV func-
tion, but for the callback-setting functions such as cvSet-
MouseCallback and cvCreateTrackbar, DARKLY replaces
the application-deÔ¨Åned callback with its own function. When
the DARKLY callback is activated by a mouse or tracker
event, it forwards these events to the interposition library,
which in turns invokes the application-deÔ¨Åned callback.

User input may be privacy-sensitive. For example, our
benchmark OCR application recognizes characters drawn by
the user using the mouse cursor. DARKLY replaces the actual
mouse coordinates with opaque references before they are
passed to the application-deÔ¨Åned callback.

HighGUI event handling is usually synchronous:

the
application calls cvWaitKey, which processes pending
mouse and tracker events and checks if any key has
been pressed. This presents a technical challenge because
most application-deÔ¨Åned callbacks invoke multiple OpenCV
drawing functions. If callback interposition is implemented
synchronously, i.e., if the DARKLY callback handler for-
wards the event to the application-deÔ¨Åned callback and waits

for it to Ô¨Ånish, the overhead of interposition (about 9%
per each call forwarded over an interprocess socket, in our
experiments) increases linearly with the number of OpenCV
functions invoked from the application-deÔ¨Åned callback. In
practice, this causes the OpenCV event buffer to overÔ¨Çow
and start dropping events.

Instead, our callback handler runs in a separate thread
in the DARKLY server. The interposed callbacks forward
GUI events asynchronously to a thread in the interposition
library, which then invokes the application-deÔ¨Åned callbacks.
Because most OpenCV functions are not thread-safe, we
serialize access with a lock in the interposition library.

void on_mouse( int event, int x, int y, int flags,

void* param ) {

...
cvCircle(imagen, cvPoint(x,y), r, CV_RGB(red,

green,blue), -1, 4, 0);

// Get clean copy of image
screenBuffer=cvCloneImage(imagen);
cvShowImage( "Demo", screenBuffer );
...

}

}

int main(int argc, char** argv ) {

...
cvSetMouseCallback("Demo",&on_mouse, 0 );
for (;;) { ... c = cvWaitKey(10); ... } }

Listing 2. Sample callback code.

Trusted storage. To store images and video without ac-
cessing their contents, applications can invoke cvSaveImage
or cvCreateVideoWriter. The interposition library forwards
these calls to DARKLY, which redirects them to system-
conÔ¨Ågured Ô¨Åles that are owned and accessible only by the
user who is running DARKLY. Dropbox or Google Drive can
be mounted as (user-controlled) remote Ô¨Åle systems.

With this design, an application cannot store data into its
own Ô¨Åles, while standard OS Ô¨Åle permissions block it from
reading the user‚Äôs Ô¨Åles.

F. Support for application-provided code

Even though the OpenCV API is very rich, some ap-
plications may need to run their own computations on
raw images rather than chain together existing OpenCV
functions. DARKLY provides a special-purpose language that
application developers can use for custom image-processing
programs. DARKLY executes these programs inside the
library on the true image data (as opposed to privacy-
preserving representations returned by the declassiÔ¨Åers), but
treats them as untrusted, potentially malicious code. Isolating
arbitrary untrusted programs is difÔ¨Åcult, but our design takes
advantage of the fact that, in our case, these domain-speciÔ¨Åc
programs deal solely with image processing.

The DARKLY language for application-supplied untrusted
computations is called ibc. It is based on the GNU bc
language [1]. We chose bc for our prototype because it (1)

355

supports arbitrary numerical computations but has no OS
interface, (2) there is an existing open-source implementa-
tion, and (3) its C-like syntax is familiar to developers. ibc
programs cannot access DARKLY‚Äôs or OpenCV‚Äôs internal
state, and can only read or write through the DARKLY
functions described below. They do not have access to the
network or system timers, minimizing the risk of covert
channels, and are allowed to return a single 32-bit value.4
ibc compiler. The GNU bc compiler takes a source Ô¨Åle
as input, generates bytecode, and executes it in a bytecode
VM. DARKLY cannot pay the cost of bytecode generation
every time an application executes the same program (for
example, for each frame in a video). Therefore, we separated
the bytecode generator and the VM.

DARKLY adds a bcCompile function to the OpenCV API.
It takes as input a string with ibc source code and returns a
string with compiled bytecode. DARKLY also adds a cvExe-
cuteUntrustedCode function, which takes a bytecode string
and pointers to OpenCV objects, executes the bytecode on
these objects, and returns a 32-bit value to the application.
The latter required a VM modiÔ¨Åcation because GNU bc
does not allow the main program to return a value.

To support computations on images and matrices,
DARKLY adds iimport and iexport functions. iimport takes
the id of an OpenCV object (i.e., the order in which it was
passed to cvExecuteUntrustedCode), x and y coordinates,
and the byte number, and returns the value of the requested
byte of the pixel at the x/y position in the image. Similarly,
iexport lets an ibc program to set pixel values.
Using custom ibc programs. To illustrate how to write
custom image-processing code in ibc, we modiÔ¨Åed an
existing application that inverts an image by subtracting each
pixel value from 255 (this can be done by calling OpenCV‚Äôs
cvNot function, but this application does not use it):
img = cvLoadImage(argv[1], 1);
data = (uchar *)img->imageData;
// invert the image
for(i=0;i<img->height;i++)

for(j=0;j<img->width;j++)
for(k=0;k<channels;k++)

data[i*step+j*channels+k]=255-data[i*step+

j*channels+k];

Listing 3. Application code for inverting an image.

bc_invrt_tmpl =
"for (i=0; i<%d;i++) {

for (j=0; j<%d; j++) {

for (k=0; k<4; k++) {

v = iimport(0, i, j, k);
iexport(0, i, j, k, 255-v); } } }

return 0;";

img = cvLoadImage(argv[1], 1);
snprintf(bc_invert_code, MAX_SIZE, bc_invert_tmpl,

img->height, img->width);

4The current DARKLY prototype allows an application to gain more
information by invoking ibc programs multiple times, but it is easy to
restrict the number of invocations if needed.

bc_bytecode = bcCompile(bc_invert_code);
ret = cvExecuteUntrustedCode(bc_bytecode, img, 0,

0);

Listing 4. Using ibc code for inverting an image.

The iimport/iexport interface can also be used to access
any 1-, 2- or 3-D array. For example, we took an existing
face recognition application (see Section IX) and wrote
an ibc program to Ô¨Ånd the closest match between the
input face‚Äôs eigen-decomposition coefÔ¨Åcients computed by
cvEigenDecomposite and a dataset of faces. Running this
program inside DARKLY allows the application to determine
whether a match exists without access to the actual eigen-
decomposition of the input face. The code is shown below.
int findNearestNeighbor( const Eigenface& data,

float * projectedTestFace ) {
double leastDistSq = 999999999; //DBL_MAX;
int iNearest = 0;

for( int iTrain = 0; iTrain < data.nTrainFaces

; iTrain++ ) {
double distSq = 0;
for( int i = 0; i < data.nEigens; ++i ) {

float d_i = projectedTestFace[i] -

data.projectedTrainFaceMat->data.
fl[iTrain * data.nEigens + i];

distSq += d_i * d_i / data.eigenValMat

->data.fl[i]; }

if( distSq < leastDistSq ) {

leastDistSq = distSq;
iNearest = iTrain; } }

return iNearest;

}

cvEigenDecomposite(image,

data.nEigens,
&(*( data.eigenVectVec.begin())),
0, 0, data.pAvgTrainImg,
projectedTestFace);

int iNearest = findNearestNeighbor(data,

projectedTestFace);

Listing 5.
closest match to the input image.

Part of face-recognition application code for calculating the

bc_dist_tmpl =
"fscale=2;
leastdistsq = 999999999
inearest = -1
for( itrain = 0; itrain < %d; itrain++ ) {

distsq = 0.0;

for( i = 0; i < %d; ++i ) {

a = iimport(0, i, 0, 0)
b = iimport(1, itrain * 2 + i, 0, 0)
di = a-b
c = iimport(2,i,0,0);
distsq += di * di / c ;

}
if( distsq < leastdistsq ) {

leastdistsq = distsq;
inearest = itrain;

}

}
return inearest;";

cvEigenDecomposite(image,

data.nEigens,

356

Figure 2. Output of the sketching transform on a female face image at different privacy levels.

Figure 3. Output of the sketching transform on a credit card image at different privacy levels.

&(*( data.eigenVectVec.begin())),
0, 0, data.pAvgTrainImg,
projectedTestFace);

snprintf(bc_dist_code, MAX_SIZE, bc_invert_tmpl,

data.nTrainFaces, data.nEigens);

bc_bytecode = bcCompile(bc_dist_code);
int iNearest = cvExecuteUntrustedCode(bc_bytecode,
projectedTestFace, data.projectedTrainFaceMat

, data.eigenValMat);

Listing 6. ModiÔ¨Åed face-recognition application code using ibc for
calculating the closest match to the input image.

VII. PRIVACY TRANSFORMS

In Section IX, we show that many OpenCV applications
can work, without any modiÔ¨Åcations, on opaque refer-
ences. Some applications, however, call OpenCV functions
like cvMoments, cvFindContours, or cvGoodFeaturesTo-
Track which return information about certain features of the
image. We call these functions declassiÔ¨Åers (Section VI-D).
To protect privacy, declassiÔ¨Åers transform the features
before releasing them to the application. The results of the
transformation are shown to the user in the DARKLY console
window (Section VIII). The user can control the level of
transformation by adjusting the privacy dial on this screen.
The transformations are speciÔ¨Åc to the declassiÔ¨Åer but
application-independent. For example, the declassiÔ¨Åer for
cvGetImageContent replaces the actual image with a thresh-
olded binary representation (see Fig. 7). The declassiÔ¨Åer
for cvGoodFeaturesToTrack, which returns a set of corner
points, applies a higher qualitylevel threshold as the dial

setting increases,
corner points are released to the application.

thus only the strongest candidates for

The declassiÔ¨Åers for cvFindContours, cvMoments, and cv-
CalcHist apply the sketching transform from Section VII-A
to the image before performing their main operation (e.g.,
Ô¨Ånding contours) on the transformed image. The application
thus obtains only the features such as contours or moments
and not any other information about the image.

Applying a privacy transform does not affect the accuracy
of OpenCV functions other than the declassiÔ¨Åers because
these functions operate on true, unmodiÔ¨Åed data.
A. Sketching

The sketch of an image is intended to convey its high-level
features while hiding more speciÔ¨Åc, privacy-sensitive details.
A loose analogy is publicly releasing statistical aggregates
of a dataset while withholding individual records.

The key to creating sketches is to Ô¨Ånd the contours of the
image, i.e., the points whose greyscale color value is equal to
a Ô¨Åxed number. In our prototype we use a hardcoded value of
50% (e.g., 127 for 8-bit color). Contours by themselves don‚Äôt
always ensure the privacy properties we want. For example,
in Fig. 3, contours reveal a credit card number. Therefore,
the sketching transform uses contours in combination with
two types of low-pass Ô¨Ålters.

First,

the image is blurred5 before contour detection.
Blurring removes small-scale details while preserving large-
scale features. The privacy dial controls the size of the Ô¨Ålter

5We use a box Ô¨Ålter because it is fast: it averages the pixels in a box
surrounding the target pixel. We could also use a Gaussian or another Ô¨Ålter.

357

kernel. Higher kernel values correspond to more blurring
and fewer details remaining after contour detection.

Just as contour detection alone is insufÔ¨Åcient, low-pass
Ô¨Åltering alone would have been insufÔ¨Åcient. For example,
image deblurring algorithms can undo the effect of box
Ô¨Ålter and other types of blur; in theory, this can be achieved
exactly as long as the resolution of the output image is not
decreased [15]. By returning only the contours of the blurred
image, our sketching transform ensures that blurring cannot
be undone (it also removes all contextual information).

Another low-pass Ô¨Ålter is applied after contour detection.
The transform computes the mean radius of curvature of
each contour (suitably deÔ¨Åned for nondifferentiable curves
on discrete spaces) and Ô¨Ålters out the contours whose mean
radius of curvature is greater than a threshold. The threshold
value is controlled by the privacy dial. Intuitively,
this
removes the contours that are either too small or have too
much entropy due to having many ‚Äúwrinkles.‚Äù

Reducing an image to its contours, combined with low-
pass Ô¨Åltering, ensures that not much information remains in
the output of the transform. Due to blurring, no two contour
lines are too close to each other, which upper-bounds the
total perimeter of the contours in an image of a given size.
Fig. 4 illustrates how sketching reduces information avail-
able to the application, as a function of the user-selected
privacy level. We also experimentally estimated the entropy
of sketches on a dataset of 30 frontal face images sampled
from the Color FERET database.6 These were cropped to the
face regions, resulting in roughly 220x220 images. We can
derive an upper bound on entropy by representing contours
as sequences of differences between consecutive points,
which is a more compact representation. Fig. 5 shows that,
for reasonable values of the privacy dial (3‚Äì6), the resulting
sketches can be represented in 500-800 bytes.

i

s
t
n
o
p
r
u
o
n
o
c

t

f
o
o
n

l

a
t
o
T

10000

8000

6000

4000

2000

0
0

1

2

3

face
credit card

4

7

8

9

10 11

5

6

Privacy dial value

Figure 4. Sketching: reduction in information available to the application
for images from Figs. 2 and 3.

B. Generalization

In addition to generic image manipulation and feature
extraction functions like cvFindContours, OpenCV also pro-
vides model-based object detectors. An application can load
a Haar classiÔ¨Åer using cvLoadHaarClassiÔ¨ÅerCascade and

6http://www.nist.gov/itl/iad/ig/colorferet.cfm

358

s
r
u
o
t
n
o
c

f
o
#

s
e
t
y
b
f
o
#

16
14
12
10
8
6
4
2
0
0
1200
1000
800
600
400
200
0
0

1

2

3

4

5

6

7

8

9

10 11

1

2

3

5

8
4
Privacy dial value

6

7

9

10 11

Figure 5.
Sketching: reduction in average information available to the
application for facial images in FERET database (size roughly 220x220).

detect objects of a certain class (for example, faces) by
calling cvHaarDetectObjects with a class-speciÔ¨Åc model. To
prevent applications from inferring information via mali-
cious models, the current DARKLY prototype only allows
predeÔ¨Åned models that ship with OpenCV.

If a match is found, cvHaarDetectObjects returns a rect-
angular bounding box containing the object, but not the
pixels inside the box. This still carries privacy risks. For
example, an application that only has an opaque reference
to the box containing a face can use OpenCV calls to
detect the location of the nose, mouth, etc. and learn enough
information to identify the face. To prevent this, DARKLY
applies a generalization-based privacy transform.
Face generalization. Generalization has a long history in
privacy protection; we explain our approach using face
detection as an example. Our privacy transform replaces
the actual face returned by cvHaarDetectObjects with a
‚Äúgeneric‚Äù face selected from a predeÔ¨Åned, model-speciÔ¨Åc
dictionary of canonical face images. We call our face gen-
eralization algorithm cluster‚Äìmorph.

The generalization idiom is already familiar to users from
‚Äúavatars‚Äù in video games, online forums, etc. Sometimes
avatars are picked arbitrarily, but often users choose an
avatar that best represents their own physical characteristics.
In the same way, the generalized face in DARKLY is intended
to be perceptually similar to the actual face, although, unlike
an avatar, it is programmatically generated.

There are two components to generalization: Ô¨Årst, Ô¨Åxing
(and if necessary, pre-processing) the canonical dictionary,
and second, choosing a representative from this dictionary
for a given input face. The former is a one-time process, the
latter is part of the transform. For the Ô¨Årst component, one
straightforward approach is to simply pick a small dictionary
of (say) 20 faces and run a face detector on the actual face
to Ô¨Ånd and return its closest match from the dictionary.

Our proposed cluster‚Äìmorph technique is a promising
but more complex approach to generalization. It works as
follows: start from a large database of images and compute
its eigenfaces by applying a well-known algorithm [26] that
uses Principal Component Analysis to calculate a set of

Finally, to Ô¨Ånd the canonical face ‚Äúrepresenting‚Äù each
cluster, morph the faces in the cluster using standard morph-
ing algorithms [3]. Fig. 6 shows an example from a cluster
of size 2 obtained by hierarchical clustering on a 40-person
ORL dataset [22]. Clustering and morphing are done once
to produce a Ô¨Åxed dictionary of canonical faces.

We propose to use hierarchical agglomerative clustering.
It offers the key advantage that the level of generalization
can be adjusted based on the setting of the privacy dial: as
the dial value increases, the transform selects clusters higher
in the hierarchy. If all clusters have at least k elements, then
the number of clusters is no more than 2N
k where N is the
total number of faces in the database.

basis vectors for the set of all faces. Then compute the
eigen-decomposition of each face,
it as a
linear combination of the basis vectors, and truncate each
decomposition to the Ô¨Årst (say) 30 principal components.
Next, cluster the set of faces using the Euclidean distance
between decompositions as the distance function.

i.e., represent

At runtime, to generalize a given input face, compute its
eigen-decomposition, calculate its distance to each cluster
center,7 and pick the closest. The transform then returns the
morphed image representing this cluster to the application.
Our DARKLY prototype includes a basic implementation
of cluster‚Äìmorph. Evaluating the algorithm on the Color
FERET database is work in progress. There are at least three
challenges: measuring the effectiveness of face clustering,
Ô¨Ånding a mapping between privacy dial values and cluster
hierarchy levels (e.g., dial values can be pegged to either
cluster sizes or cluster cohesion thresholds), and developing
metrics for quantifying privacy protection.

canonical representation w.r.t. a globally predeÔ¨Åned dataset
(in particular, the input image is not drawn from this dataset).
Further, Newton et al.‚Äôs algorithm has some weaknesses
for our purposes: it uses greedy clustering instead of more
principled methods, requires re-clustering if the privacy dial
changes, and, Ô¨Ånally, in our experiments averaging of faces
produced results that were visually inferior to morphing.

Figure 7. Output of the thresholding binary transform on an image of
a street scene with a QR code. QR decoding application works correctly
with the transformed image.

VIII. DARKLY CONSOLE

The DARKLY console is a DARKLY-controlled window
that shows a visual representation of the features and ob-
jects returned to the application by the declassiÔ¨Åers. For
applications that operate exclusively on opaque references,
the DARKLY console is blank. For applications that use
declassiÔ¨Åers, the DARKLY console shows the outputs of the
corresponding privacy transforms‚Äîsee examples in Figs. 8
and 9. We assume that this window cannot be spoofed by
the application. In general, constructing trusted UI is a well-
known problem in OS design and not speciÔ¨Åc to DARKLY.

Figure 6. Face morphing for generalization. The left and right faces belong
to the same cluster; the morph ‚Äúrepresenting‚Äù this cluster is in the center.
Our cluster‚Äìmorph algorithm is inspired in part by New-
ton et al.‚Äôs algorithm for k-anonymity-based facial de-
identiÔ¨Åcation [19], which works as follows: given a database
of images, repeatedly pick a yet-unclustered image from the
database and put it in a cluster with k ‚àí 1 of its ‚Äúclosest‚Äù
images, according to an eigenface-based distance measure.
For each face in the input database, the average of the faces
in its cluster constitutes its de-identiÔ¨Åed version.

The salient differences in our case are as follows: our
goal is not k-anonymity within a database, but Ô¨Ånding a

7A cluster center is the mean of the eigen-decomposites of each image
in the cluster. It does not correspond to the morphed image. Since eigen-
decomposition of a face is a linear transformation, averaging in the
eigenspace is the same as averaging in the original space; thus, the image
corresponding to the cluster center is a plain pixelwise average of the faces
in the cluster. This average would be unsuitable as a canonical representative
due to artifacts such as ghosting, which is why we use the morphed image.

Figure 8. Motion detector: actual image and the DARKLY console view.
Application works correctly with the transformed image.

The DARKLY console is implemented as a separate pro-
cess communicating with DARKLY over UNIX domain sock-
ets. With this design, the application‚Äôs declassiÔ¨Åer function
calls need not be blocked until the DARKLY console has
Ô¨Ånished rendering. We did not
the DARKLY
console as a thread inside the DARKLY server because both
use OpenCV, and OpenCV functions are not thread-safe.

implement

Consecutive DARKLY console views are stored as a movie
Ô¨Åle in AVI or MPG format. If storage is limited, they can be
compressed and/or stored at reduced resolution. The user can

359

Figure 9.
Application works correctly with the transformed image.

Ball tracker: actual image and the DARKLY console view.

play back the movie and see how the information released
to the application by privacy transforms evolved over time.
Privacy dial. The DARKLY console includes a slider for
adjusting the level of transformation applied by the pri-
vacy transforms. The values on the slider range from 0 to
11. Absolute values are interpreted differently by different
transforms, but higher values correspond to coarser outputs
(more abstract representations, simpler contours, etc.). For
example, higher values cause the sketching declassiÔ¨Åer to
apply a larger box Ô¨Ålter to smoothen the image before Ô¨Ånding
the contours, thus removing more information (see Fig. 3).

IX. EVALUATION

We evaluated DARKLY on 20 OpenCV applications, listed
in Table II along with their source URLs. These applications
have been selected from Google Code, GitHub, blogs, and
OpenCV samples for the variety and diversity of their
features and the OpenCV functionality they exercise. With
the exception of OCR, which uses the C++ interface for
nearest-neighbor clustering, they use OpenCV‚Äôs C interface.
Our DARKLY prototype is based on OpenCV release
2.1.0. Applications were evaluated on a Segway RMP-50
robot running ROS Fuerte and/or a laptop with a quad-core
2.40GHz Intel Core i3 CPU and 4 GB of RAM running 32
bit Ubuntu 11.10 desktop edition.

Results are summarized in Table III. 18 out of 20 applica-
tions required no modiÔ¨Åcations to run on DARKLY, except
very minor formatting tweaks in a couple of cases (removing
some header Ô¨Åles so that the program compiles in Linux).
For the face recognizer, we re-implemented the eigenface
matching algorithm in our ibc language (see Section VI-F)
so that it can run on true images inside the library, returning
only the match/no match answer to the application.

For all tests, we used either a benchmark video dataset of
a person talking,8 or the sample images and videos that came
with the applications, including OpenCV sample programs.9
Depending on the application, frame rates were computed
for the video or over the input images.

8http://www-prima.inrialpes.fr/FGnet/data/01-TalkingFace/talking face.

9https://code.ros.org/trac/opencv/browser/trunk/opencv/samples/c?rev=

html

27

hand-drawn

Application
OCR for
digits
Security cam
Ball tracker

QR decoder
PrivVideo, video back-
ground subtractor
and
streamer
Facial features detector

Face recognizer

Histogram calculator
(RGB)
Histogram calculator
(Hue-Saturation)
Square detector

for

images/

Morphological
transformer
Intensity/contrast
changer
histograms
Pyramidal
downsampler + Canny
edge detector
Image adder

H-S histogram back-
projector
Template matcher

Corner Ô¨Ånder

Hand detector

Laplace edge detector

Ellipse Ô¨Åtter

URL
http://blog.damiles.com/2008/11/
basic-ocr-in-opencv/
http://code.google.com/p/camsecure/
https://github.com/liquidmetal/
AI-Shack--Tracking-with-OpenCV/blob/master/
TrackColour.cpp
https://github.com/josephholsten/libdecodeqr
http://theembeddedsystems.blogspot.com/2011/
05/background-subtraction-using-opencv.html

http://opencvfacedetect.blogspot.com/2010/10/
face-detectionfollowed-by-eyesnose.html
http://www.cognotics.com/opencv/servo 2007
series/index.html
http://www.aishack.in/2010/07/
drawing-histograms-in-opencv/
http://opencv.willowgarage.com/documentation/
cpp/histograms.html
https://code.ros.org/trac/opencv/browser/trunk/
opencv/samples/c/squares.c?rev=27
https://code.ros.org/trac/opencv/browser/trunk/
opencv/samples/c/morphology.c?rev=27
https://code.ros.org/trac/opencv/browser/trunk/
opencv/samples/c/demhist.c?rev=1429
http://dasl.mem.drexel.edu/‚àºnoahKuntz/
openCVTut1.html

http://silveiraneto.net/2009/12/08/
opencv-adding-two-images/
http://dasl.mem.drexel.edu/‚àºnoahKuntz/
openCVTut6.html
http://opencv.willowgarage.com/wiki/
FastMatchTemplate?action=AttachFile&do=
view&target=FastMatchTemplate.tar.gz
http://www.aishack.in/2010/05/
corner-detection-in-opencv/
http://code.google.com/p/
wpi-rbe595-2011-machineshop/source/browse/
trunk/handdetection.cpp
https://code.ros.org/trac/opencv/browser/trunk/
opencv/samples/c/laplace.c?rev=27
https://code.ros.org/trac/opencv/browser/trunk/
opencv/samples/c/Ô¨Åtellipse.c?rev=1429

Table II

BENCHMARK OPENCV APPLICATIONS.

Performance. Performance is critically important for per-
ceptual applications that deal with visual data. If the over-
head of privacy protection caused frame rates to drop
too much, applications would become unusable. Figure 10
shows that the performance overhead of DARKLY is very
minor and, in most cases, not perceptible by a human user.
The effect of a given privacy transform depends on the set-
ting of the privacy dial, aka the privacy level. For example,
sketching, the transform for the cvFindContours declassiÔ¨Åer,
applies different amounts of blurring before Ô¨Ånding contours.
Fig. 11 shows that the performance variation of the security
camera application at different privacy levels is minimal
(within 3%). Interestingly, in this case performance does not
change monotonically with the privacy level. The reason is
that the OpenCV function used by the sketching transform
switches algorithms depending on the parameters.
Tradeoffs between privacy and utility. Table III shows that
for most applications, there is no change of functionality and
no loss of accuracy even at the maximum privacy setting.

360

Application

QR decoder

Face recognizer
OCR
Template matcher
Security cam
Facial features detector
Square detector
Ellipse Ô¨Åtter
Intensity/contrast changer for images/his-
tograms
Ball tracker
PrivVideo
Morphological transformer
H-S histogram backprojector
Laplace edge detector
RGB histogram calculator
H-S histogram calculator
Hand detector
Corner Ô¨Ånder
Image adder
Downsampler + Canny edge detector

4700

851
513
483
312
258
238
134
127

114
96
91
81
73
70
58
48
42
37
36

0
0
0
0
0
0
0
0
0
0
0

LoC ModiÔ¨Åed

LoC
19

in

Information accessed

Change
functionality
Works only at
privacy level 0 ‚àó Contours, thresholded image

1 + 19 (ibc) No change
0
No change
No change
0
See Fig. 12
0
No change ‚àó‚àó
0
0
See Fig. 12
See Fig. 12
0
0
No change

Match/no match
Output digit
Match matrix
Contours
Rectangular bounding boxes
Contours
Contours
Histograms

See Fig. 12
No change
No change
See Fig. 12
No change
See Fig. 12
See Fig. 12
No change
See Fig. 12
No change
No change

Moments
None
None
Histogram
None
Histogram
Histogram
Yes/no
Corner coordinates
None
None

‚àó Even at level 0, privacy from the QR decoder is protected by the thresholding binary transform.

‚àó‚àó Feature detection is performed on privacy-transformed faces (Section VII-B).

EVALUATION OF DARKLY ON OPENCV APPLICATIONS.

Table III

with Darkly
w/o Darkly

7.25

7.20

c
e
s
/
s
e
m
a
r
F

60

50

40

30

20

10

0

R
C
O

m
a
c

y
t
i
r
u
c
e
S

r
e
k
c
a
r
t

l
l

a
B

r
e
d
o
c
e
d
R
Q

i

o
e
d
V
v
i
r

P

s
e
r
u
t
a
e
f

l

i

a
c
a
F

d
d
A

y
n
n
a
C

i

t
s
H
S
H

e
r
a
u
q
S

m
a
r
g
o
t
s
H

i

l

y
g
o
o
h
p
r
o
M

i

r
e
z
n
g
o
c
e
r
e
c
a
F

t
s
a
r
t
n
o
c
/
y
t
i
s
n
e
t
n
I

j

t
c
e
o
r
p
k
c
a
B
t
s
H

i

l

e
c
a
p
a
L

r
e
d
n
Ô¨Å

r
e
n
r
o
C

r
o
t
c
e
t
e
d
d
n
a
H

i

g
n
h
c
t
a
m
e
t
a
p
m
e
T

l

e
s
p

F

i
l
l

e
t
i

Figure 10. Frame rates with and without DARKLY.

The reason is that these applications do not access raw
images and can operate solely on opaque references.

One application,

the QR decoder, works correctly at
privacy level 0, but not at higher settings. Even at privacy
level 0, signiÔ¨Åcant protection is provided by the thresholding
binary transform (see Fig. 7). For the remaining applications,

361

c
e
s
/
s
e
m
a
r
F

7.15

7.10

7.05

7.00
0

1

2

3

5

4
8
Privacy dial value

6

7

9 10 11

Figure 11.
Frame rate of the security-camera application as a function
of the privacy level. At levels above 4, OpenCV switches from directly
calculating the convolution to a DFT-based algorithm optimized for larger
kernels. Furthermore, as privacy level increases, smaller motions are not
detected and the application has to process fewer motions.

the tradeoff between their accuracy and user-selected privacy
level is shown in Fig. 12.
Support for other OpenCV applications. We found 281
GitHub projects mentioning ‚Äúvision,‚Äù ‚Äúapplications,‚Äù and
‚Äúopencv.‚Äù10 Filtering out empty projects and clones with the
same name and codebase reduced the set to 77 projects.

10A simple search for ‚Äúopencv‚Äù returns different parts of the OpenCV

library itself and does not work for Ô¨Ånding OpenCV applications.

1.0

0.8

0.6

0.4

0.2

s
e
h
c
a
e
r
b

)
d
e
z

i
l

a
m
r
o
n
(

y
t
i
r
u
c
e
s
d
e
t
c
e
e
d

t

1.0

0.8

0.6

0.4

0.2

s
e
r
a
u
q
s
d
e
t
c
e
e
d

t

)
d
e
z

i
l

a
m
r
o
n
(

f

o
#

1.0

0.8

0.6

0.4

0.2

t

s
r
u
o
n
o
c
d
e
t
c
e
e
d

t

)
d
e
z

i
l

a
m
r
o
n
(

f

o
#

f

o
#

0.0
0

1

2

3

5

4
8
Privacy dial value

6

7

9 10 11

0.0
0

1

2

3

5

4
8
Privacy dial value

6

7

9 10 11

0.0
0

1

2

3

5

4
8
Privacy dial value

6

7

s
r
e
n
r
o
c
d
e
t
c
e
e
d

t

)
d
e
z

i
l

a
m
r
o
n
(

f

o
#

1.0

0.8

0.6

0.4

0.2

0.0
0

1

2

3

1.0

0.8

0.6

0.4

0.2

y
c
a
r
u
c
c
a
g
n
k
c
a
r
t

i

)
d
e
z

i
l

a
m
r
o
n
(

.

g
v
A

1.0

0.8

0.6

0.4

0.2

n
o

i
t

)
d
e
z

i
l

a
m
r
o
n
(

l

a
e
r
r
o
C

.

g
v
A

5

4
8
Privacy dial value

6

7

9 10 11

0.0
0

1

2

3

5

4
8
Privacy dial value

6

7

9 10 11

0.0
0

1

2

3

5

4
8
Privacy dial value

6

7

9 10 11

9 10 11

Figure 12. Change in the number of detected security breaches (Security cam), detected squares (Square detector), detected contours (Ellipse Ô¨Åtter), moments
(Ball tracker), and histograms (RGB and H-S histogram calculators, Intensity/contrast changer for images/histograms, and H-S histogram backprojector) as
the privacy level increases. Correlation between histograms was calculated using the cvHistCompare function. Accuracy for tracking was measured using
the Euclidean distance between the object‚Äôs original position and the reported position after applying privacy transforms.

We scanned these 77 projects for invocations of cvGet2D,
cvGetAt, or cvGetRawData, and direct accesses to the im-
ageData Ô¨Åeld of the image data structure. After removing
the spurious matches caused by included OpenCV header
Ô¨Åles, we found that 70% of the projects (54 out of 77) do
not access raw pixels. Furthermore, only 11 projects access
the network, and only 2 access audio inputs.

These 77 projects call a total of 291 OpenCV functions, of
which 145 are already supported by our DARKLY prototype,
118 can be supported with opaque references, 15 can be sup-
ported with the sketching-based declassiÔ¨Åer, and 3 require
porting application code to ibc. These 281 functions are
sufÔ¨Åcient to support 68 of the 77 surveyed projects.

The remaining 9 projects make calls to unsupported
OpenCV functions (10 in total) that perform tasks such as
optical Ô¨Çow (cvCalcOpticalFlowBM, cvCalcOpticalFlowHS
cvCalcOpticalFlowLK, and cvCalcOpticalFlowPyrLK), ob-
ject
tracking (cvCamShift, cvMeanShift, and cvSnakeIm-
age), camera calibration (ComputeCorrespondEpilines), mo-
tion analysis (cvSegmentMotion), and image segmenta-
tion (cvWatershed). Supporting these functions in DARKLY
would require new, task-speciÔ¨Åc privacy transforms and is
an interesting topic for future research.

X. RELATED WORK

Denning et al. [7] showed that many off-the-shelf con-
sumer robots do not use proper encryption and authenti-
cation, thus a network attacker can control the robot or
extract sensitive data. By contrast, DARKLY protects users
from untrusted applications running on a trusted robot.
PlaceRaider [25] is a hypothetical mobile malware that

can construct a 3-D model of its environment from phone-
camera images. DARKLY prevents this and similar attacks.
SciFi [21] uses secure multiparty computation to match
faces against a database. Matching takes around 10 seconds
per image, thus SciFi is unusable for real-time applications.
The threat model of DARKLY is different (protecting images
from untrusted applications), it handles many more percep-
tual tasks, and can protect real-time video feeds.

Ad-hoc methods for protecting speciÔ¨Åc sensitive items
include the blurring of faces and license plates in Google
Maps‚Äô Street View [24]. Senior et al. [23] suggested im-
age segmentation to detect sensitive objects in surveillance
videos and transform them according to user-provided poli-
cies. To protect surveillance videos on the network, Dufaux
and Ebrahimi [8] proposed to encrypt regions of interest.
This requires computationally expensive, ofÔ¨Çine image seg-
mentation and it is not clear whether perceptual applications
would work with the modiÔ¨Åed videos. Chan et al. [5]
developed a method for counting the number of pedestrians
in surveillance videos without tracking any single individual.
Sweeney et al. published several papers [11, 12, 19] on
‚Äúde-identifying‚Äù datasets of face images. Many of their tech-
niques, especially in the k-same-Eigen algorithm, are similar
to the generalization transform described in Section VII-B.
They do a ‚Äúgreedy‚Äù version of clustering and their model-
based face averaging has similarities with face morphing.

Showing the outputs of privacy transforms to the user on
the DARKLY console is conceptually similar to the sensor-
access widgets by Howell and Schechter [14]. Their widgets,
however, display the entire camera feed because applications
in their system have unrestricted access to visual inputs.

362

[4] M. R. Calo. People can be so fake: A new dimension to
privacy and technology scholarship. Penn St. L. Rev., 114:809,
2009.

[5] A. Chan, Z. Liang, and N. Vasconcelos. Privacy preserving
crowd monitoring: Counting people without people models
or tracking. In CVPR, 2008.

[6] L. D‚ÄôAntoni, A. Dunn, S. Jana, T. Kohno, B. Livshits,
D. Molnar, A. Moshchuk, E. Ofek, F. Roesner, S. Saponas,
M. Veanes, and H. Wang. Operating system support for
augmented reality applications. Technical Report MSR-TR-
2013-12, Microsoft Research.

[7] T. Denning, C. Matuszek, K. Koscher, J. Smith, and T. Kohno.
A Spotlight on Security and Privacy Risks with Future
Household Robots: Attacks and Lessons. In Ubicomp, 2009.
[8] F. Dufaux and T. Ebrahimi. Scrambling for video surveillance

with privacy. In CVPRW, 2006.

[9] C. Dwork. Differential privacy. In ICALP, 2006.
[10] C. Dwork. A Ô¨Årm foundation for private data analysis.

In

CACM, 2011.

[11] R. Gross, E. Airoldi, B. Malin, and L. Sweeney. Integrating

utility into face de-identiÔ¨Åcation. In PET, 2006.

[12] R. Gross, L. Sweeney, F. De la Torre, and S. Baker. Model-

based face de-identiÔ¨Åcation. In CVPRW, 2006.

[13] HighGUI: High-level GUI

and Media

I/O.

//opencv.willowgarage.com/documentation/python/highgui
high-level gui and media i o.html.

[14] J. Howell and S. Schechter. What you see is what they get:
Protecting users from unwanted use of microphones, camera,
and other sensors. In W2SP, 2010.

[15] R. Hummel, B. Kimia, and S. Zucker. Deblurring gaussian

http:

blur. CVGI, 1987.

[16] S. Jana, D. Molnar, A. Moshchuk, A. Dunn, B. Livshits,
H. Wang, and E. Ofek. Enabling Ô¨Åne-grained permissions for
augmented reality applications with recognizers. Technical
Report MSR-TR-2013-11, Microsoft Research.

[17] D. McCullagh. Call it Super Bowl Face Scan I. http://www.

wired.com/politics/law/news/2001/02/41571, 2001.

[18] F. McSherry. Privacy integrated queries: an extensible plat-
form for privacy-preserving data analysis. In SIGMOD, 2009.
[19] E. Newton, L. Sweeney, and B. Malin. Preserving privacy by

de-identifying face images. TKDE, 2005.

[20] OpenCV Wiki. http://opencv.willowgarage.com/wiki/.
[21] M. Osadchy, B. Pinkas, A. Jarrous, and B. Moskovich. SCiFI

- A System for Secure Face IdentiÔ¨Åcation. In S&P, 2010.

[22] F. Samaria and A. Harter. Parameterisation of a stochastic
In Applications of

model for human face identiÔ¨Åcation.
Computer Vision, 1994.

[23] A. Senior, S. Pankanti, A. Hampapur, L. Brown, Y. Tian, and
A. Ekin. Blinkering Surveillance: Enabling Video Privacy
through Computer Vision. IBM Research Report, 2003.

[24] Google Maps Street View - Privacy. http://maps.google.com/

help/maps/streetview/privacy.html.

[25] R. Templeman, Z. Rahman, D. Crandall, and A. Kapadia.
in physical spaces with smart-

theft

PlaceRaider: Virtual
phones. In NDSS, 2013.

[26] M. Turk and A. Pentland. Eigenfaces for recognition.

In

Augmented Reality (AR) applications are a special subset
of perceptual applications that not only read perceptual data
but also modify and display some parts of the input back
to the user. To protect user privacy from such applications,
D‚ÄôAntoni et al. [6] argue that the OS should provide new
higher-level abstractions for accessing perceptual data in-
stead of the current low-level sensor API. Jana et al. [16]
built a new OS abstraction (recognizers) and a permission
system for enforcing Ô¨Åne-grained, least-privilege access to
perceptual data by AR applications. This permission-based
approach is complementary to DARKLY.

XI. FUTURE WORK

DARKLY is the Ô¨Årst step towards privacy protection for
perceptual applicatons. Topics for future research include:
(1) evaluation of functionality and usability on a variety of
computer-vision tasks, (2) support for application-provided,
potentially untrusted object recognition models (the cur-
rent
transform for cvHaarDetectObjects is based on the
face detection model shipped with OpenCV) and third-
party object recognition services such as Dextro Robotics,
and (3) development of privacy transforms for untrusted,
application-provided image-processing code. The latter may
obviate the restriction on the outputs of untrusted code,
but would also require a new visualization technique for
displaying these outputs to the user on the DARKLY console.
Longer-term research includes: (4) preventing inferential
leaks by using large-scale, supervised machine learning to
construct detectors and Ô¨Ålters for privacy-sensitive objects
and scenes, such as certain text strings, gestures, patterns of
movement and physical proximity, etc., and (5) extending
the system to other perceptual inputs such as audio.
Acknowledgments. We are grateful to David Molnar, Scott
Saponas, and Ryan Calo for helpful discussions during the
early part of this work and to Piyush Khandelwal for helping
us evaluate DARKLY on the Segway RMP-50 robot.

This work was supported by the NSF grant CNS-0746888,
the MURI program under AFOSR Grant No. FA9550-08-1-
0352, and Google PhD Fellowship to Suman Jana.

REFERENCES

[1] bc Command Manual.

http://www.gnu.org/software/bc/

manual/html chapter/bc toc.html.

[2] R. V. Bruegge. Facial Recognition and IdentiÔ¨Åcation Initia-
tives. http://biometrics.org/bc2010/presentations/DOJ/vorder
bruegge-Facial-Recognition-and-IdentiÔ¨Åcation-Initiatives.
pdf, 2010.

[3] T. Bui, M. Poel, D. Heylen, and A. Nijholt. Automatic face
morphing for transferring facial animation. In CGIM, 2003.

363

CVPR, 1991.

