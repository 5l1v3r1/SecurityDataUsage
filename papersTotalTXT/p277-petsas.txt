Rise of the Planet of the Apps:

A Systematic Study of the Mobile App Ecosystem

Thanasis Petsas
FORTH-ICS, Greece
petsas@ics.forth.gr

Antonis Papadogiannakis

FORTH-ICS, Greece

papadog@ics.forth.gr

Michalis Polychronakis
Columbia University, USA

mikepo@cs.columbia.edu

Evangelos P. Markatos

FORTH-ICS, Greece

markatos@ics.forth.gr

Thomas Karagiannis
Microsoft Research, UK

thomkar@microsoft.com

ABSTRACT
Mobile applications (apps) have been gaining rising popularity due
to the advances in mobile technologies and the large increase in
the number of mobile users. Consequently, several app distribution
platforms, which provide a new way for developing, downloading,
and updating software applications in modern mobile devices, have
recently emerged. To better understand the download patterns, pop-
ularity trends, and development strategies in this rapidly evolving
mobile app ecosystem, we systematically monitored and analyzed
four popular third-party Android app marketplaces. Our study fo-
cuses on measuring, analyzing, and modeling the app popularity
distribution, and explores how pricing and revenue strategies affect
app popularity and developers’ income.

Our results indicate that unlike web and peer-to-peer ﬁle shar-
ing workloads, the app popularity distribution deviates from com-
monly observed Zipf-like models. We verify that these deviations
can be mainly attributed to a new download pattern, to which we re-
fer as the clustering effect. We validate the existence of this effect
by revealing a strong temporal afﬁnity of user downloads to app
categories. Based on these observations, we propose a new formal
clustering model for the distribution of app downloads, and demon-
strate that it closely ﬁts measured data. Moreover, we observe that
paid apps follow a different popularity distribution than free apps,
and show how free apps with an ad-based revenue strategy may re-
sult in higher ﬁnancial beneﬁts than paid apps. We believe that this
study can be useful to appstore designers for improving content de-
livery and recommendation systems, as well as to app developers
for selecting proper pricing policies to increase their income.

Categories and Subject Descriptors
D.4.8 [Operating Systems]: Performance—Measurements; Mod-
eling and prediction; Simulation

Keywords
Appstores; Mobile Apps; Workload Characterization; App Popu-
larity; Clustering effect; App Pricing; Revenue

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
IMC’13, October 23–25, 2013, Barcelona, Spain.
Copyright 2013 ACM 978-1-4503-1953-9/13/10 ...$15.00.
http://dx.doi.org/10.1145/2504730.2504749.

1.

INTRODUCTION

With more than 640 million active mobile devices as of July
2012 [9], the market of mobile devices is one of the fastest growing.
Along with this growth, recent advances in mobile technologies
facilitated a plethora of mobile applications (apps); for example,
the Android Market experienced a sevenfold increase in the num-
ber of offered apps from 100,000 to 700,000 within the last two
years [8]. Typically, apps are hosted and distributed to end users
through an increasing number of mobile application stores (app-
stores). Appstores provide a new way of developing, downloading,
and updating software applications for mobile devices, and create
new business opportunities for application developers.

Despite this rapid evolution, there have been only a few large-
scale studies of mobile appstores, mostly focusing on security and
privacy aspects of mobile apps [32, 33, 44, 45]. However, appstores
provide essentially a public marketplace matching mobile users and
developers, and hence can offer valuable insights across a diverse
set of dimensions ranging from app download patterns and popu-
larity trends, to developers’ revenue strategies. To this end, through
a study of four popular third-party Android appstores over several
months, our work aims to better understand appstore properties by
focusing on two main aspects:

Q1: How can we characterize the popularity of different mobile

applications?

Q2: What are the pricing and revenue strategies followed by app

developers?

To examine Q1, we ﬁrst characterize the app popularity distri-
bution. Our results highlight a typical “Pareto effect,” with 10%
of the apps accounting for 70–90% of the total downloads. While
such phenomena have also been observed in the popularity of web
objects [25] and video content [21], the app popularity distribu-
tion appears truncated at both head and tail, and this observation is
consistent across all examined appstores. Our analysis shows that
truncation at the head of the distribution is caused by the “fetch-
at-most-once” property, also found in peer-to-peer ﬁle sharing sys-
tems [34]. Contrary, we attribute tail truncation to a phenomenon
we refer to as the “clustering effect,” which captures the tendency
of users to show a strong temporal afﬁnity to app categories, by
selectively downloading apps from certain categories over time.
These insights lead us to propose a new model to approximate the
popularity distributions observed in appstores, and through simula-
tion results we show that it closely captures the observed behaviors.
Typically, appstores offer two categories of mobile apps: paid
and free. In Q2, we examine whether offering a paid or free ad-
based app is a better revenue-making strategy for developers. To
this end, we ﬁrst study the differences between paid and free apps.

277While as expected free apps are far more popular, interestingly,
paid apps appear to follow a different popularity distribution, which
more closely resembles a Zipf distribution. We conjecture that this
is the result of users being more selective when paying for apps.

Looking at developers’ income from paid apps, we ﬁnd that the
majority of them earn a very small income, and offering a large
number of apps does not lead to larger income; quality is more im-
portant than quantity. In contrast, by inspecting application bina-
ries, we ﬁnd that 67.7% of the free apps include at least one library
that belongs to the 20 most popular advertising networks. Our anal-
ysis suggests that, on average, a free app needs to make just $0.21
per download through ads to match the income offered by a paid
app. For popular apps, this number drops to $0.033. Hence, opting
for a free app appears to be a more lucrative strategy.

Overall, we believe that our study offers multiple insights to both
appstore designers and app developers. By understanding app pop-
ularity patterns, appstore and mobile operators can improve per-
formance (e.g., by caching the most popular applications as shown
by the Pareto effect), usability (e.g., by pre-installing very popular
applications to mobile devices), and the relevance of recommenda-
tion and search systems (e.g., by taking into account the clustering
effect). Similarly, insights on app monetization can be crucial for
developers. Indeed, our study further shows that choosing an ap-
propriate app category signiﬁcantly affects the expected income.

The main contributions of this work are:

• We study the popularity distribution of mobile apps and com-
pare it to the popularity distributions seen in web, peer-to-
peer ﬁle sharing, and user-generated content. We show that
app downloads follow a power-law distribution with trun-
cated edges and have a highly-skewed Pareto effect, with
10% of apps accounting for 70–90% of total downloads.

• We provide a model of app download distribution and vali-
date it using data from four Android appstores. We show that
our model ﬁts the measured data much more closely than pre-
viously proposed models for web and peer-to-peer ﬁle shar-
ing. We attribute this difference to the “clustering effect,”
which we validate with a study of user behavior patterns.

• We study the differences between paid and free apps and
show that (i) they have different download distributions, and
(ii) they lead to fundamentally different revenue patterns for
developers. We show that, in most cases, free apps that fol-
low an ad-based revenue strategy may result in higher ﬁnan-
cial beneﬁts than paid apps.

2. DATA COLLECTION

In this section we describe the data sources we used, our data
collection architecture, and summarize the collected data, which
are analyzed in the rest of the paper.

2.1 Monitored Appstores

For our study we systematically collected information from four
popular Android app marketplaces: SlideMe [16], 1Mobile [13],
AppChina [15], and Anzhi [14]. SlideMe is one of the oldest An-
droid marketplaces, founded in 2008, containing more than 20,000
apps. 1Mobile is one of the largest third-party appstores, with more
than 150,000 apps. AppChina and Anzhi are very popular app-
stores located in China, with more than 50,000 apps each. All
marketplaces provide a website through which users can browse,
search, download, and buy apps. They also provide an application
manager tool that allows users to manage, search, and download
apps directly from their Android device,
in a similar way as the
ofﬁcial Google Play Store App. Anzhi is often found as the pre-

Crawler
program

Selenium

Firefox
Headless
browser

HTTP Proxy

App statistics

APKs

Local crawler

host

p
p
A

s
c

i
t
s

i
t
a
t
s

s
K
P
A

HTTP Proxy

App statistics

APKs

HTTP Proxy

App statistics

APKs

SlideMe

1Mobile

AppChina

Anzhi

Monitored Android

Marketplaces

Chinese nodes

PlanetLab

nodes

Local crawler

Local Database

hosts

Figure 1: Data collection architecture. Local crawler hosts use
Planetlab nodes as proxies to download per-app statistics and
APKs, which are stored in a local database.

conﬁgured appstore on HTC smartphones in China. Many tablet
manufacturing companies like Disgo, as well as many telecommu-
nication providers like Vodafone, offer their devices pre-installed
with SlideMe [5, 17]. More than 120 OEM device models use
SlideMe as their default appstore [35].

To explore similarities and differences among various appstores,
we chose a representative and diverse sample of the available mar-
ketplaces.
In terms of geographical distribution, Anzhi and Ap-
pChina target the quickly rising mobile app market of China, while
1Mobile and SlideMe are mostly used by developers and clients lo-
cated in Europe and the United States. In terms of the number of
available apps, 1Mobile is one of the largest appstores, Anzhi and
AppChina are considered as medium-size appstores, while SlideMe
is a smaller one. The appstores also differ in the degree of popu-
larity in terms of daily downloads. SlideMe provides both free and
paid apps, while the rest provide only free apps.

One of the main reasons for choosing these appstores is that
all of them provide the accurate number of downloads for each
app, an aspect that is of great interest for our study. On the other
hand, the ofﬁcial Android market Google Play [6], as well as other
third-party marketplaces [1, 3, 4], do not provide a precise num-
ber of downloads or installations per app. Instead, they give only
range approximations, which make any kind of systematic anal-
ysis very difﬁcult. Besides the coarse-grained available informa-
tion, Google’s terms of service do not allow access to Google Play
through any type of automated means.

2.2 Data Collection Architecture

For each appstore, we collect information in two phases. Ini-
tially, we gather a snapshot of the available apps by crawling the
appstore through its web interface, and store all data into a local
database. Then, the crawling process is repeated on a daily basis.
Each day, our crawling system re-visits the web pages for the al-
ready indexed apps to update their daily statistics. It also collects
information for all the newly added apps, which are also stored into
the database expanding the initial dataset. For each app, we collect
statistics such as the number of downloads, user ratings and com-
ments, current version, category, price, and information about the
developer. We also download any updated APK ﬁles (i.e., the app
itself), collecting this way all versions of every app in the market-
place. Note that we download each app version only once, so we
do not affect the actual number of downloads.

Figure 1 shows our data collection architecture. The system con-
sists of a set of local machines, each hosting several crawler in-
stances. We also use a set of nodes (roughly around 100) from
the PlanetLab infrastructure [23], which act as HTTP proxies. Be-
fore sending an HTTP request to an appstore, each crawler instance

278Appstore

Crawling period

Total apps

New apps per day

Total downloads

Daily downloads

(ﬁrst day / last day)

(average)

Anzhi

AppChina
1Mobile

SlideMe (free)
SlideMe (paid)

2012-06-04 – 2012-08-03
2012-03-30 – 2012-06-03
2012-03-21 – 2012-08-01
2012-03-01 – 2012-08-01
2012-03-01 – 2012-08-01

58,423 / 60,196
33,183 / 55,357

128,455 / 156,221

12,296 / 16,578

4,606 / 5,606

29.6
336.0
210.4
28.0
6.5

(ﬁrst day / last day)
1,396 M / 2,816 M
1,033 M / 2,623 M

367 M / 453 M

63 M / 96 M
111 K / 914 K

(average)
23.7 M
24.1 M
651.5 K
215.7 K
5.2 K

Table 1: Summary of collected data. We collected data about more than 300,000 apps while monitoring four different appstores for
2–5 months. AppChina has the largest number of daily downloads and new apps per day.

randomly selects one of these proxies. This distributed crawling
scheme was necessary to avoid any IP address blacklisting. The
Chinese appstores apply rate limiting to hosts away from China,
so we used only the PlanetLab nodes that are located in China for
proxying their crawling connections.

We implemented a speciﬁc crawler for each appstore using the
Scrapy framework [10]. We designed our crawlers to comply with
the thresholds set by each appstore in terms of requests per time
unit. To gather data from pages with dynamically generated content
with JavaScript, we used the Selenium Remote Control (RC) [11], a
browser automation tool, combined with a headless Firefox browser
running on an X virtual framebuffer. For such pages, the crawler
was altered to make the HTTP requests through Selenium with the
controlled headless browser, so that Scrapy collects all the required
information from the already rendered HTML page.

2.3 Data Set

Table 1 summarizes the collected data. For SlideMe, 74.7% of
the apps we collected are free, while the rest 25.3% are paid. As
of August 2013, the examined appstores had 316,143 apps and 7
billion downloads in total; this is roughly 52.7% of the 600,000
apps and 35% of the 20 billion downloads featured in Google Play
at the same time [7]. This implies that our dataset comprises a
signiﬁcant part of the mobile app ecosystem.

We see that a small number of new apps are added to SlideMe
and Anzhi by app developers every day. On the other hand, a
higher number of new apps are added every day to AppChina and
1Mobile. This is probably due to the increasing popularity of the
AppChina marketplace, while 1Mobile hosts the largest number
of apps among these appstores. Indeed, we see that AppChina is
the most popular appstore in terms of user downloads, with more
than 24 million of downloads per day. It seems that this popularity
attracts an increasing number of developers that choose this mar-
ketplace to distribute their applications. Although 1Mobile hosts
the largest number of apps, it has a lower number of downloads
than AppChina and Anzhi, which target smartphone users in China.
We also observe that paid apps receive signiﬁcantly less downloads
than free apps, and fewer paid apps are added per day to SlideMe.

3. APP POPULARITY

First, we study app popularity based on the distribution of down-

loads of each app among the different appstores.

3.1

Is There a Pareto Effect?

Previous studies have shown that web content [25] and video
downloads [21] usually follow the Pareto Principle: 20% of the ob-
jects are responsible for 80% of the downloads. Figure 2 shows
the CDF of the percentage of app downloads as a function of the
app ranking (ranked from the most popular to the least popular) for
the different appstores. The results conﬁrm that a small percent-
age of apps is responsible for a large percentage of downloads. For
example, in AppChina and Anzhi, about 10% of the apps are re-
sponsible for close to 90% of all downloads. Similarly, 10% of the

)
F
D
C

l

(
 
s
d
a
o
n
w
o
d
 
f
o
 
e
g
a
t
n
e
c
r
e
P

 100

 90

 80

 70

 60

 50

 40

 30

 20

 10

 0

10% of the apps account for 90% of the downloads

1% of the apps accounts for 70% of the downloads

 90
 80
 70
 60
 50
 40
 30
 20
 10
 0

 1

 2

 3

 4

 5

Anzhi

AppChina

1Mobile

SlideMe

 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

Normalized app ranking (%)

Figure 2: A few apps account for most of the downloads. The
CDF of downloads per app shows that 10% of the apps account
for 90% of the total downloads.

1Mobile apps are responsible for more than 85% of the downloads,
and 10% of the SlideMe apps are responsible for more than 70% of
all downloads in this appstore. We see that this uneven distribution
of popularity goes all the way into the top 1% of the applications,
which are responsible for more than 70% of downloads in Anzhi,
more than 60% in AppChina, more than 55% in 1Mobile, and more
than 30% in SlideMe. This evidence for the existence of the Pareto
effect is signiﬁcant for the design of efﬁcient caching mechanisms
for application delivery to end users. For example, a cache large
enough to hold 1% of the apps will have a hit rate as high as 70%.

3.2 Is There a Power-law Behavior?

Although it is clear that app downloads follow a pareto princi-
ple, we would like to explore whether the app downloads follow a
power law distribution much like web downloads do [20]. Figure 3
shows the number of downloads per app, when all apps are sorted
in the x axis according to their rank based on their total down-
loads. We see that all distributions share a similar pattern: their
main “trunk” has a linear slope indicating a Zipf distribution, which
is truncated at both ends (i.e., for small and large x values).

The truncation for small x values (i.e., most popular apps) seems
to follow similar patterns shown in the downloads of ﬁle sharing
systems [34] and video downloads in YouTube [21]. This trunca-
tion on these systems was attributed to the fetch-at-most-one princi-
ple. That is, objects shared in a ﬁle sharing system, such as music,
videos, movies, etc., tend to be downloaded at most once by each
user. Therefore, the curve for very popular content (i.e., small val-
ues in the x axis) tends to ﬂatten out and reach a value close to
the number of users in the system. We see the same behavior in
Figure 3: for very small x the number of downloads stays almost
horizontal (especially in AppChina and Anzhi appstores), which is
probably due to the fetch-at-most-one principle. It is reasonable to
expect that smartphone users will also download each app at most
once, apart from apps which are updated.

279108

107

106

105

104

103

102

l

s
d
a
o
n
w
o
D

Anzhi
1.42

108

107

106

105

104

103

102

101

l

s
d
a
o
n
w
o
D

AppChina
1.51

107

106

105

104

103

102

101

l

s
d
a
o
n
w
o
D

1Mobile
0.92

106

105

104

103

102

101

l

s
d
a
o
n
w
o
D

SlideMe
0.90

101

100

101

102

103

104

105

100

100

101

102

103

104

105

100

100

101

102

103

104

105

100

100

App rank

(a) Anzhi

App rank

(b) AppChina

App rank

(c) 1Mobile

103

104

101

102

App rank

(d) SlideMe

Figure 3: App popularity distribution deviates from Zipf at both ends. The distribution of total downloads per app as a function
of app’s rank shows a main trunk that follows a Zipf distribution with truncated ends. The truncation for small x values can be
attributed to the fetch-at-most-once property observed in most popular apps. We explain the truncation for large x values with
another phemomenon we call as clustering effect.

 1

 0.8

 0.6

 0.4

 0.2

)
F
D
C

(
 
s
p
p
A

 0

 0

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0

 1

 2

 3

 4

Anzhi
AppChina
1Mobile
Slideme

 5

 10

 15

 20

 25

Number of updates

Figure 4: Apps are not updated often. CDF of the number of
updates per app within two months shows that a small percent-
age of apps was updated in this period. This implies that users
download most apps just once, or very few times, verifying the
fetch-at-most-once property.

To validate that the fetch-at-most-once property exists in mobile
appstores as well, and it is not affected by application updates, we
plot in Figure 4 the CDF of the number of updates per app in the
four appstores for a period of two months. We see that more than
80% of the apps are not updated at all within this period. Even
the apps that are updated, they have a very small number of up-
dates, e.g., the 99% of the apps have less than four updates. Since
the fetch-at-most-once property affects only the most popular apps,
limiting their downloads to the number of users in the market, we
focused only on the top 10% most popular apps. We found that
these apps are also rarely updated: 60%-75% of them have no up-
dates in this period, while 99% of these apps have up to six updates.
These results imply that the fetch-at-most-once property applies to
mobile app market, as apps are not updated often.

Moreover, Figure 3 clearly shows that there is a signiﬁcant curva-
ture for large x values (less popular apps) as well. Observed in user-
generated content downloads [21], but not in ﬁle sharing [34], this
curvature has been thought to be attributed to search engines and
recommendation systems. Previous works argue that these systems
tend to favor the most popular content, due to information ﬁlter-
ing, which results to the observed truncation of power law [22, 37].
However, we feel that this curvature is an instance of a more gen-
eral phenomenon, to which we refer as the clustering effect.

The clustering effect suggests that apps are grouped into (static
or dynamic) sets. Apps within the same set (cluster) are correlated:
if a user downloads one of them, then the same user will proba-

bly download another app of the same set rather than switching to
another set, or to a totally unrelated app. These clusters can be
formed, for example, by the semantic classiﬁcation of apps to cat-
egories, by user communities as a result of positive comments, by
the appstore as a result of the recommendation algorithm used, or
by other grouping forces. To validate our clustering effect hypothe-
sis, we deﬁne and measure the afﬁnity of user downloads among the
several app categories in Section 4. Then, we propose a model for
user downloads based on clustering effect and we validate it with
simulations comparing to the actual gathered data in Section 5.

4. THE CLUSTERING EFFECT

In our next set of measurements we explore whether the pre-
viously suggested “clustering effect” hypothesis can be validated.
One way to validate the hypothesis would be to show that (A) apps
are categorized into clusters, and (B) once users have downloaded
an app from one cluster, it is more likely to download another app
from the same cluster, rather than switching to another one.

Fortunately, point (A) above is easy to show. Most appstores al-
ready group apps into clusters that are usually thematically related
to the app’s content. For example, there exists a cluster for “games,”
a cluster for “e-books,” a cluster for “business and ﬁnance,” and so
on. To validate point (B), we would like to study the download pat-
terns of individual users and see if they tend to concentrate around
a few clusters or not. Unfortunately, appstores tend not to reveal
the download patterns of individual users, mostly due to privacy
concerns, and thus it is not easy to study them. Instead, we use
publicly available user comments to approximate their download
patterns and interests. If a user posts a publicly available comment
for a particular app, we assume that the user has download the app.
Thus, publicly available comments provide us with access to a sub-
set of the download patterns of individual users.

4.1 Users Focus on a Few Categories

In this set of measurements we focused only on data from the
Anzhi appstore, because it provides user comments with precise
timestamps. We considered only user comments that are accom-
panied by a rating of the app, which is a strong indication of actual
downloads. During the crawling process of the Anzhi appstore, we
recorded and stored into our database the comments of each user on
each app. The crawling process of user comments from the Anzhi
marketplace resulted in a dataset of 361,282 users to 60,196 apps
in 34 categories. Figure 5(a) shows the distribution of the number
of comments per user. We see that 99% of the users made up to 30
comments. In many categories, there were a few users with a very
large number of comments. We found that these users were posting
spam, possibly using an automated script.

280 1

 0.8

 0.6

 0.4

 0.2

)
F
D
C

(
 
s
r
e
s
U

 0

 1

)
F
D
C

(
 
s
r
e
s
U

0.93

0.70

0.53

0.40

0.20

0.00

 10

 100

 1000

 10000

 1

 5

 10

 15

 20

 25

 30

s
t
n
e
m
m
o
c
 
’
s
r
e
s
u
 
f
o
 
e
g
a
t
n
e
c
r
e
p
 
e
g
a
r
e
v
A

 100

s
e
i
r
o
g
e
t
a
c
 
k
 
p
o
t
 
n
i
 
s
p
p
a
 
r
o
f
 
e
d
a
m

 95

 90

 85

 80

 75

 70

 65

)

%

(
 
e
g
a
t
n
e
c
r
e
p
 
s
d
a
o
n
w
o
D

l

 14

 12

 10

 8

 6

 4

 2

 0

 1

 5

 10

 15

 20

 25

 30

 0

 5

 10

 15

 20

 25

 30

 35

Comments per user

Unique categories per user

Number of top k categories

Category rank

(a) Comments per user

(b) Unique categories per user

(c) Comments on top k categories

(d) Downloads per app category

Figure 5: Users focus on a few categories. (a) shows that 92% of the users made up to 10 comments, while 2% of the users made more
than 20 comments. (b) shows that 53% of the users leave feedback on apps from a single category, while 94% comment on up to ﬁve
categories. (c) shows that 66% of the comments are for a single category, while 95% are made for up to three categories. (d) shows
that there is no dominant category in terms of downloads, as the most popular app category has 12% of the total downloads.

Figure 5(b) shows the CDF of unique categories for which each
user comments on, for users that posted at least one comment. We
see that 53% of the users commented on apps from a single cate-
gory, and 94% of the users commented on apps from up to just ﬁve
categories. Thus, only 6% of the users made comments in more
than ﬁve categories, which is signiﬁcantly lower than the percent-
age of users that made more than 5 comments in general (20%), as
shown in Figure 5(a). Similarly, in Figure 5(c) we present the aver-
age percentage of user comments made for the top k categories, as
a function of k. We have excluded users that made comments on a
single app in this ﬁgure. We observe that the 66% of the comments
of an average user were made for a single category, while 95% of
the user comments were made for no more than ﬁve categories.

These ﬁndings indicate that most users tend to download apps
from a single or very few categories, which implies a clustering
effect. One could argue that the popularity distribution of app
categories can inﬂuence user downloads. For example, games are
more likely to be downloaded than apps belonging to the tools cat-
egory, as the games category has a higher number of total down-
loads than tools. Therefore, a few popular categories could attract
most of the users downloads. Figure 5(d) shows the distribution of
app downloads across the different app categories. We see that the
most popular category has just 12% of the total downloads, while
the majority of categories have less than 4% of the total downloads.
This fact indicates that there are no dominant categories in terms of
downloads, and the popularity of app categories does not seem to
have a considerable inﬂuence on users downloads. Consequently,
the main reason for the small number of categories observed in each
user’s comments appears to be the clustering effect.

4.2 Temporal Afﬁnity to App Categories

To study the download patterns in more detail and validate the
existence of the clustering effect, we will ﬁrst deﬁne and then mea-
sure the temporal afﬁnity metric of users to app categories. During
the crawling process of the Anzhi appstore we have recorded the
stream of comments for each user on each app. We suppressed suc-
cessive comments of the same user on the same app. For example,
if a user commented on apps a1a2a3a3a1a4 we kept the sequence
a1a2a3a4, which we refer to as app string. In addition, we know
that appstore has already categorized each app ai into a category
c(ai). Using this knowledge, for each app string a1a2a3a3a4 we
construct the category string: c(a1)c(a2)c(a3)c(a4).

Given a category string S of n elements c1c2c3...cn with the re-
spective categories of a user’s n comments in chronological order,
we deﬁne the temporal afﬁnity metric Aff as the number of ele-
ments that are in the same category with their previous element,
divided by n − 1, using the following formula:

n

Pi=2

Aff =

Afﬁnity(ci, ci−1)

n − 1

(1)

where Afﬁnity(ci, cj) equals to one if ci and cj are the same, and
zero otherwise. For example, a user with a category string c1c1c1c1
will have temporal afﬁnity equal to 3/3, a user with a category
string c1c1c1c2 will have temporal afﬁnity equal to 2/3, and a user
with a category string c1c1c2c3 will have afﬁnity equal to 1/3.

This afﬁnity metric is an indication of whether users tend to com-
ment on apps from the same category or on apps from different cat-
egories. Indeed, if afﬁnity approaches the highest value (i.e., one),
then users tend to repeatedly comment on apps from the same cat-
egory, indicating that the users may tend to successively download
apps from the same category. On the contrary, if the afﬁnity is low,
then users tend to switch from one category to another.

Note that if we have C categories of roughly equal volume, then
if users randomly wander from one category to another, the afﬁnity
will be around 1/C. However, in practice, the apps are not evenly
distributed among the different categories. To calculate the accu-
rate afﬁnity probability of a random walk in the Anzhi marketplace
we used the actual distribution of apps to the C different categories
in this appstore from our dataset. Let A be the total number of apps
in the appstore and A(i) the number of apps that belong to cate-
gory i. Given this distribution, the random walk afﬁnity probability
Affrandom walk, i.e., the probability that two random app choices be-
long to the same category, is equal to:

Affrandom walk =

C

Pi=1

A(i) × (A(i) − 1)

A × (A − 1)

(2)

since out of the A × (A − 1) possible random app choices, the
number of app choices for which the two apps belong to the same
category are equal to the sum of A(i)×(A(i)−1) for all categories.
We need the afﬁnity probability of a random walk as a base case to
compare with the actual afﬁnity we measure from Anzhi.

So far we have deﬁned the afﬁnity metric as the percentage of
consecutive app pairs that belong to the same category. In this way,
however, we miss users that may oscillate between few categories.
For instance, the category string c1c2c1c2 has afﬁnity equal to zero
according to our previous deﬁnition, but we can see a clear afﬁnity
of this user to c1 and c2 categories. To address this issue, we deﬁne
the afﬁnity metric using the depth notion. That is, the afﬁnity Aff
with depth d for a category string with n elements is deﬁned as
the number of elements in the category string for which at least

28195% CI
Average (depth level 1)
Random wandering (0.14)

)
i
(

G
 
p
u
o
r
g
 
f
o
 
y
t
i
l
i

b
a
b
o
r
p
 
y
t
i
n
i
f
f

A

 1

 0.8

 0.6

 0.4

 0.2

 0

)
i
(

G
 
p
u
o
r
g
 
f
o
 
y
t
i
l
i

b
a
b
o
r
p
 
y
t
i
n
i
f
f

A

 1

 0.8

 0.6

 0.4

 0.2

 0

95% CI
Average (depth level 2)
Random wandering (0.28)

)
i
(

G
 
p
u
o
r
g
 
f
o
 
y
t
i
l
i

b
a
b
o
r
p
 
y
t
i
n
i
f
f

A

 1

 0.8

 0.6

 0.4

 0.2

 0

95% CI
Average (depth level 3)
Random wandering (0.42)

 0

 10

 20

 30

 40

 50

 60

 70

 80

 0

 10

 20

 30

 40

 50

 60

 70

 80

 0

 10

 20

 30

 40

 50

 60

 70

 80

i : Number of comments

(a) Depth=1

i : Number of comments

(b) Depth=2

i : Number of comments

(c) Depth=3

Figure 6: Successive user selections are from the same category with higher probability. Temporal afﬁnity for depth levels up to 3 for
users grouped by their number of comments. We see that for depth=1, users select an app of the same category as the previous one
with probability 0.55, which is 3.9 times higher than the base case of random walk. As expected, afﬁnity increases with depth level.

one element among its previous d elements belongs to the same
category, divided by n − d:

n

Pi=d

Aff =

Afﬁnity(ci, {ci−1, ci−2, ..., ci−d})

n − d

(3)

where Afﬁnity(ci, {ci−1, ci−2, ..., ci−d}) is equal to one if at least
one of the {ci−1, ci−2, ..., ci−d} belongs to the same category with
ci, and zero otherwise. For example, the afﬁnity of depth equal to
two is deﬁned as the percentage of app selections that belong to the
same category with one of the previous two selections. To calculate
the afﬁnity metric for this depth, we ﬁrst divide the n apps of the
category string into n − 2 triplets. Then we check which of these
triplets have afﬁnity, i.e., if the third element belongs to the same
category with the second or with the ﬁrst one in the triplet. Finally,
we sum the triplets that have afﬁnity and divide them with the total
number of triplets, as shown in Equation 3.

Using Equation 2 we ﬁnd the afﬁnity probability of a random
walk for depth equal to one. To calculate the afﬁnity for an arbitrary
depth d we use the following equation:

Affrandom walk =

C

Pi=1

A(i) × (A(i) − 1) × d ×

(A − k)

d

Qk=0

d

(A − k)

Qk=2

(4)

4.3 Users Exhibit a Strong Temporal Afﬁnity

to App Categories

We measure the temporal afﬁnity metric as deﬁned in Equation 3,
based on the actual app distribution of the 34 categories of the
Anzhi appstrore. Figure 6 shows the measured afﬁnity for depth
levels from 1 up to 3, as a function of the number of comments
per user. We have grouped together all users that made the same
number of comments, and we plot the average values and the 95th
conﬁdence intervals from each group. We plotted only the groups
that had more than 10 samples, excluding, in this way, the spam
users as well. In case of depth equal to one, we see that the av-
erage afﬁnity for most groups is around 0.55. This implies that
once a user comments on an app from one category, the same user
will comment on another app of the same category with probability
close to 0.55. To place these numbers in context, we also calculated
the estimated afﬁnity of the base case as well, i.e., the case where
a user wanders from one app to another randomly. The random
walk afﬁnity probability in our dataset for depth one is 0.14, shown
as a horizontal line in Figure 6(a). Thus, we see that Anzhi users

Depth level 1
Depth level 2
Depth level 3

Random
walk
depth
level 1

Random
walk
depth
level 2

Random
walk
depth
level 3

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

)
F
D
C

(
 
s
r
e
s
U

 0

 0.1

 0.2

 0.3

 0.4

 0.5

 0.6

 0.7

 0.8

 0.9

 1

Affinity probability

Figure 7: Most users exhibit a strong temporal afﬁnity to app
categories. The CDF of the afﬁnity metric for each depth level
shows that 50% of the users have signiﬁcantly higher afﬁnity
than the base case of random walk.

are 3.9 times more likely to stay in the same category compared
to the base case of random walk. This outcome indicates a strong
temporal afﬁnity between users and categories.

To explore how the temporal afﬁnity increases for higher depth
levels, we plot in Figures 6(b) and 6(c) the afﬁnity metric of the
same user groups, as well as the random walk afﬁnity probability,
for depths equal to two and three respectively. We see that user
afﬁnity to app categories increases with depth, as it was expected,
and remains higher than the respective random walk afﬁnity prob-
ability. To explore how the afﬁnity varies among users, we plot
the CDF of temporal afﬁnity among all Anzhi users for the three
different levels in Figure 7. We observe that the median value for
depth one is 0.5, while for depths two and three the median values
are 0.58 and 0.67 respectively. Overall, our results show that there
is a clear temporal afﬁnity of users’ comments to app categories,
which is a strong indication that users tend to download successive
apps from the same categories.These observations validate our hy-
pothesis about the existence of clustering effect in user downloads.

5. A MODEL FOR APPSTORE WORKLOADS

We now deﬁne a model of appstore usage called APP-CLUSTER-
ING, based on fetch-at-most-once and clustering effect properties.
The model will enable us to further explore our hypothesis and val-
idate the effect of app clustering on app popularity distribution.

282Measured
ZIPF (zr=1.4)
ZIPF-at-most-once (zr=1.6)
APP-CLUSTERING (zr=1.7, p=0.9, zc=1.4)

l

s
d
a
o
n
w
o
D

109
108
107
106
105
104
103
102
101
100

100

104

103

102

101

100

  1x104

  2x104   3x104

101

102

103

App rank

(a) AppChina

109

108

107

106

105

104

103

102

l

s
d
a
o
n
w
o
D

Measured
ZIPF (zr=1.2)
ZIPF-at-most-once (zr=1.3)
APP-CLUSTERING (zr=1.4, p=0.9, zc=1.4)

104

103

102

101

  2x104

  4x104

  6x104

104

105

101

100

101

102

103

104

105

App rank
(b) Anzhi

l

s
d
a
o
n
w
o
D

109
108
107
106
105
104
103
102
101
100

100

Measured
ZIPF (zr=1.4)
ZIPF-at-most-once (zr=1.6)
APP-CLUSTERING (zr=1.7, p=0.95, zc=1.5)

102

101

100

  5x104

  1x105

101

102

103

104

105

106

App rank

(c) 1Mobile

Figure 8: Predicted versus measured app popularity for each appstore. We see that APP-CLUSTERING ﬁts very closely the measured
data. ZIPF-at-most-once ﬁts the data better than a pure ZIPF distribution, but diverges for large x values.

Symbol

A
U
D
d
zr
ZG
C
p
zc
Zc

D(i, j)

Parameter Description
Number of apps
Number of users
Total downloads
Downloads per user (average)
Zipf exponent for overall app ranking
Overall Zipf distribution of all apps
Number of clusters
Percentage of downloads based on the clustering effect
Zipf exponent for cluster’s app ranking
Zipf distribution of apps in cluster c
Predicted downloads for app with total rank i and rank j in its cluster

Table 2: APP-CLUSTERING model parameters.

5.1 Model Description and Analysis

Table 2 summarizes the key parameters used in our model. We
assume that all apps are categorised in C clusters so that each app
belongs to exactly one cluster. When a user downloads the ﬁrst app,
this download is drawn from a Zipf-like distribution ZG. Once a
user has downloaded at least one app, subsequent downloads will
be from the same category of a previous download with probability
p, and from a different category with probability 1−p. Downloads
from the same category of a previous download also follow a Zipf-
like distribution Zc.
Therefore, each app has two rankings: its
overall ranking i, taking values between 1 and A, and its ranking j
within its category, taking values between 1 and the number of apps
in that category. Thus, the APP-CLUSTERING model suggests
that each user behaves as follows:

1. Download the ﬁrst app according to the ZG distribution.

2. Download another app:

2.1. with probability p the app will be downloaded from the
same cluster c of a previously downloaded app. The
cluster c is randomly chosen from previous downloads
with a uniform probability. The app from cluster c is
drawn from distribution Zc. If the app has been down-
loaded go to 2.1.

2.2. with probability 1 − p the app will be drawn from ZG.

If the app has been downloaded go to 2.2.

3. If user’s downloads are less than d go to 2.

Analysis. We assume an appstore with U users, where each user
downloads d apps. Out of the d downloads per user, (1 − p) × d
are app selections based on a pure Zipf distribution. That is, an app
with rank i has a probability to be selected for downloading equal
to (1/izr )/(PA
k=1 1/kzr ). Similarly, when apps are selected from

a speciﬁc cluster Ca, an app with rank j in this cluster will be
SCa
selected with probability equal to (1/jzc )/(P
l=1 1/lzc ), where
SCa the size of cluster Ca. For simplicity we assume that all C
clusters have the same size SC . Overall, the expected downloads
D(i, j) for an app with overall rank i and rank j in its respective
cluster with the APP-CLUSTERING model are:

D(i, j) =

1− 1−

U

Xu=1

1/izr
A

1/kzr !(1−p)×d

Pk=1

!p×d

(5)

× 1−

1/jzc

1/lzc

SC

Pl=1

The expected downloads per app are estimated by adding the
probability of all users to download this app. The probability of a
single user to download this app is one minus the probability for
not downloading this app with (1 − p) × d Zipf-based selections
and p × d clustering-based selections. We see that the number of
downloads per app is limited by the number of users U .

5.2 Simulation-based Model Validation

To validate our model and understand the impact of clustering ef-
fect on the app downloads distribution, we developed three Monte
Carlo simulators of an appstore, using ZIPF, ZIPF-at-most-once,
and APP-CLUSTERING models. In the ZIPF simulator all down-
loads are drawn from ZG. In the ZIPF-at-most-once simulator all
downloads are drawn from ZG, but each user can never download
the same app more than once. Then we ran these simulators for all
appstores in our dataset while varying their key parameters, in order
to approximate the observed distribution of app downloads as close
as possible. To measure how close each simulation approaches the
actual downloads distribution of A apps we calculate the distance
between the observed and simulated downloads of each app using
the mean relative error:

distance =

1
A

A

Xi=1

|Do(i) − Ds(i)|

Do(i)

(6)

where Do(i) and Ds(i) are the observed and simulated downloads,
respectively, for the app with overall rank i.

5.2.1 Comparing Modeled and Actual Downloads

Figure 8 compares the simulation results of the ZIPF, ZIPF-
at-most-once, and APP-CLUSTERING models with the measured
downloads in AppChina, Anzhi, and 1Mobile. We tuned the pa-
rameters of each model to produce the best data ﬁt, by running
simulations with all parameter combinations, and measuring the
distance from actual data. We plot the results of each model using

283t

 

a
a
d
d
e
r
u
s
a
e
m
m
o
r
f
 

 

e
c
n
a
t
s
D

i

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

ZIPF
ZIPF-at-most-once
APP-CLUSTERING

a
h i n
C
3 - 3
2 - 0

0

a
h i n
C
6 - 0
2 - 0

3

p

1

p

0

A
2

p

1

p

0

A
2

4

6 - 0

h i
2 - 0

z

1

n

0

A
2

z

1

n

0

A
2

3

8 - 0

h i
2 - 0

1

3 - 2

b il e
2 - 0

1 M o
1
0
2

1 M o
1
0
2

1

8 - 0

b il e
2 - 0

a
t
a
d
 
d
e
r
u
s
a
e
m
m
o
r
f
 
e
c
n
a
t
s
D

 

i

 4

 3.5

 3

 2.5

 2

 1.5

 1

 0.5

 0

0.1

AppChina 2012-03-30
AppChina 2012-06-03
Anzhi 2012-06-04
Anzhi 2012-08-03
1Mobile 2012-03-21
1Mobile 2012-08-01

0.25

0.5

1

2

5

10

20

50

Number of users (as a fraction of the downloads of the most popular app)

Figure 9: APP-CLUSTERING has the smallest distance from
measured data. APP-CLUSTERING approximates the actual
downloads up to 7.2 times closer than ZIPF and up to 6.4 times
closer than ZIPF-at-most-once.

Figure 10: Downloads of the most popular app are a good esti-
mate of the number of users. We see the minimum distance from
the actual downloads when the number of users is close to the
downloads of the most popular app.

the parameters that produced the minimum distance from actual
downloads. We ﬁrst see that in all ﬁgures the pure ZIPF distribu-
tion (dashed red line) clearly deviates from the measured data. The
deviation is obvious: ZIPF is a straight line (in log-log scale) while
the measured data follow a curved line at both ends of the x spec-
trum. Indeed, for small x (i.e., popular apps) ZIPF overshoots the
measured data by more than an order of magnitude. ZIPF-at-most-
once (dashed blue line) ﬁts the data better than ZIPF, especially for
small x (i.e., for popular apps), but deviates from the actual data
mainly for large x values. On the other hand, APP-CLUSTERING
matches the data very close, better than any other model, both for
small x and especially for large x values (i.e., least popular apps).
As we increase p, i.e., the percentage of simulated downloads
based on clustering, we see that the distance of APP-CLUSTER-
ING from the actual data is reduced, and data ﬁtting is improved.
The best approximations of the actual data are achieved when the
percentage p of clustering-based downloads is 90–95%. This out-
come provides strong evidence that clustering affects app down-
loads distribution and is responsible for the observed tail behavior.
Figure 9 shows the distance of the three models from the mea-
sured data for the ﬁrst and last day of our crawling period in Ap-
pChina, Anzhi, and 1Mobile. We see that APP-CLUSTERING has
the smallest distance from measured data for all appstores. For in-
stance, in the ﬁrst day AppChina dataset, APP-CLUSTERING pre-
dictions result in 0.15 distance from the actual downloads, which is
4.7 times better than ZIPF-at-most-once and 5.1 times better than
ZIPF. We see similar results from simulations of other appstores
as well. Overall, we observe that APP-CLUSTERING model is
able to approximate the actual downloads of all appstores very well
from the ﬁrst day up to the last day of our measurement period.

5.2.2 Choosing the Right Number of Users

Using the APP-CLUSTERING simulator we also explore how
the number of users U inﬂuences the simulation results. Since we
do not have access to the logs of the appstores, we do not know the
actual number of users who have accessed each appstore. We know
the total number of downloads, the total number of apps, but we
do not know the total number of users. Nevertheless, we conduct
simulations in order to explore whether there is a reasonable range
for the number of users of the appstores studied that results to very
close approximations of the actual downloads distribution per app.
Given that different appstores have a different actual number of

users, we express the number of users as a function of the total
number of downloads of the most popular app.

Figure 10 shows our simulation results while varying the num-
ber of users U and setting the rest of the simulation parameters
to the values that produce the minimum distance from the actual
data. The x-axis is the number of the simulated users, as a ratio
of the total number of downloads of the most popular app, and the
y-axis reports the distance between the simulation results and the
measured downloads. We plot the simulation results for the app
downloads of the ﬁrst and last day of the AppChina, Anzhi, and
1Mobile appstores. We see that in all cases the minimum distance
from the measured data is achieved when the number of users is
very close to the number of downloads of the most popular app.

6. APP PRICING AND INCOME

Among the four appstores we studied, AppChina, Anzhi, and
1Mobile offer solely free apps—only SlideMe provides both free
and paid apps. One might wonder why developers choose third-
party marketplaces to upload paid apps instead of Google Play,
which could result in increased app visibility. There are several
reasons to do so. First, many developers upload their apps in as
many alternative markets as possible in order to gain more popular-
ity, increase their income, and attract users with devices that have
third-party appstores pre-installed, such as SlideMe, as explained in
Section 2. Also, Google Play supports only speciﬁc locations for
merchants [12]. Since developers living in countries not supported
by Google cannot upload paid apps to Google Play, they usually
choose an alternative market like SlideMe. Finally, third-party mar-
ketplaces may provide beneﬁts to developers, e.g., SlideMe does
not charge transaction fees besides a payment processing fee [35].
Paid apps usually have more advanced functionality and do not
include advertisements. In this section, we focus on the SlideMe
appstore to study how price affects app popularity, how revenue
is distributed among developers and categories, and how different
pricing strategies may affect developers’ income. The main ques-
tions that drive our analysis regarding app pricing and developers’
income are the following: (Q1) Which are the main differences be-
tween paid and free apps? (Q2) What is the developers’ income
range? (Q3) Which are the common strategies when offering an
app, and how do these strategies affect revenue? In the rest of this
section we explore these questions in more detail using the dataset
of the SlideMe appstore.

284106

105

104

103

102

101

l

s
d
a
o
n
w
o
D

SlideMe - free apps
0.85

104

103

102

101

l

s
d
a
o
n
w
o
D

SlideMe - paid apps
1.72

100

100

101

102

App rank

103

104

100

100

101

102

App rank

103

104

(a) SlideMe - free apps

(b) SlideMe - paid apps

Figure 11: Paid apps follow a clear Zipf distribution. The downloads distribution
of free and paid apps shows that paid apps clearly follow a power-law distribu-
tion, while free apps follow the same distribution we observed in Section 3.2.

)
p
p
a
 
r
e
p
(
 
s
d
a
o
n
w
o
D

l

s
p
p
A

104
103
102
101
100

104

103

102

101

100

Pearson’s correlation coefficient: -0.229

Pearson’s correlation coefficient: -0.240

 0

 10

 20
 30
Price (dollars)

 40

 50

Figure 12: Expensive apps are less popular. Num-
ber of downloads and apps as a function of price.
We see that the number of downloads and num-
ber of apps are negatively correlated with price.

6.1 The Inﬂuence of Cost on App Popularity
First, we explore how cost affects app popularity. To under-
stand the different download patterns between paid and free apps
(Q1), Figure 11 shows the distribution of downloads for free and
paid apps separately. Free apps follow a similar distribution as the
one observed in Section 3.2. Interestingly, we see that paid apps
clearly follow a pure power-law distribution, with no signiﬁcant
deviations. This is probably due to the fact that users are more se-
lective when downloading paid apps: they are less inﬂuenced by
recommendation systems and more considerate about cost. Thus,
less popular apps tend to stay that way and are deprived from any
casual downloads. Contrary, free apps enjoy much more downloads
and even less popular apps get a decent number of downloads.

Figure 12 shows the average number of downloads per app, as
well as the number of apps as a function of price. Since the app
price may change, we use the average price of each app we ob-
served within our measurement period. We group together all apps
that their price is within a range of one dollar, e.g., apps with price
between two and three dollars belong to the same bin, and we plot
the number of apps for each bin (in the lower part) and the average
number of downloads among the apps of each bin (upper part). It
is expected that apps with higher prices will get less downloads.
Indeed, we observe a negative correlation between price and down-
loads, with Pearson’s correlation coefﬁcient equal to -0.229. This
means that apps with higher prices are less likely to become popu-
lar. Similarly, the results indicate that there is a negative correlation
between price and number of apps as well, i.e., we see more apps
with lower prices, with a correlation coefﬁcient equal to -0.240.
This means that most developers offer their apps in lower prices,
hoping for increased popularity.

6.2 Developers’ Income

Next, we set out to explore how the app market’s revenue from
paid apps are distributed among the developers, and which param-
eters affect the developers’ income (Q2). To estimate the income
of each developer we rely on the total number of downloads (pur-
chases) of each paid app offered by this developer, multiplied by the
average price of this app during our measurement period. The av-
erage revenue of a single paid app is $3.9. Appstores usually get a
commission from purchases, typically around 20–30% of the app’s
price, and the remaining amount is the actual developer’s income.
However, SlideMe has one of the lowest commissions on app pur-
chases:
it gets 5% of the app’s price for each download. From
the total revenue, which is close to 4 million dollars in our dataset,
SlideMe should get around 200 thousand dollars as commission.

For simplicity in our measurements we assume that developers get
the whole amount from each download of a paid app.

Figure 13 shows the CDF of total income per developer. We see
that half of the developers made less than $10, while 27% of them
had no income from their paid apps. The 80% of the developers
made less than $100, and 95% of them less than $1,500. However,
we observe that there is a small percentage of developers, roughly
around 1%, with income higher than 2 million dollars from very
popular applications. Overall, it is clear that the majority of devel-
opers in the SlideMe appstore had a negligible income by selling
paid apps, while only a very small percentage of them made a sig-
niﬁcant income.

An interesting question that comes up is whether the total income
of a developer is correlated with the number of apps that this devel-
oper creates. In other words, can a developer earn more by creating
more applications? To answer this question we plot in Figure 14
the number of paid apps per developer versus the average total in-
come of the developers with this number of paid apps. We see
that the developer’s income does not increase with the number of
apps, and there is no clear relation between them. The Pearson’s
correlation coefﬁcient between income and number of apps per de-
veloper is equal to 0.008, which validates that these values are not
correlated. Therefore, it seems that a developer will not necessar-
ily reach a wider set of users by simply offering more apps. This
is very interesting, because it shows that quality is more important
than quantity for the developers’ income.

We also measure how the revenue from paid app installations is
distributed to app categories, and whether the percentage of apps
and developers per category are related to the category’s revenue
fraction. Figure 15 shows the percentage of the total revenue per
app category, along with the percentage of apps and developers for
the same categories. We see that 67.7% of the revenue comes from
paid apps of the music category, 19.7% from games, while 3.9%
and 3.7% of the revenue comes from utilities and productivity cate-
gories respectively. Overall, 95% of the revenue comes from these
four categories, while the rest 16 categories contribute only with
5% of the developers’ income. These four categories include the
30.4% of the apps and 72% of the developers. Despite the fact that
music produces 67.7% of the total revenue, we see that it contains
just 1.6% of the paid apps, and only 4.3% of the developers have
made at least one app in this category. We also observe that 33.2%
of the apps are in the e-books category, created by only 1.6% of the
developers, and they result to just 0.1% of the total revenue. On
the other hand, 18.3% of the apps are games, offered by 36.2% of
the developers, resulting to 19.7% of the total revenue, which is a
reasonable ratio. Overall, we see that the fraction of revenue per

285 1

 0.8

 0.6

 0.4

 0.2

)
F
D
C

(
 
s
r
e
p
o
e
v
e
D

l

 0
10-1

100

101

102

103

104

105

106

107

Total income per developer (dollars)

l

r
e
p
o
e
v
e
d

 
r
e
p

 
s
p
p
a
d
a
p

 

i

 
f

o

 
r
e
b
m
u
N

104

103

102

101

100

100

line fit: slope = 0.00364

Pearson’s correlation coefficient: 0.008

101
104
Total income per developer (dollars)

102

103

105

Figure 13: Most developers have negligible income from paid apps.
80% of the developers made less than $100, while 1% of them
gained more than 2 million dollars.

Figure 14: Quality is more important than quantity. We see that
developer’s income is not correlated with the number of apps that
this developer offers.

Revenue
Apps
Developers

 100

e
g
a

t

n
e
c
r
e
P

 10

 1

 0.1

 0.01

i

c
s
u
m

s
e

i
t
i
l
i
t

u

s
e
m
a
g

/

n
u

f

y
t
i
v
i
t
c
u
d
o
r
p

l

e
v
a
r
t

n
o
g

i

i
l

e
r

l

i

a
c
o
s

l

a
n
o

i
t

a
c
u
d
e

t

n
e
m
n
a
t
r
e

i

t

n
e

s
n
o

i
t

i

a
c
n
u
m
m
o
c

s
k
o
o
b
-
e

l

e
y
t
s
e

f
i
l

r
e
h

t

o

s
r
e
p
a
p

l
l

a
w

s
s
e
n

t
i
f
/

h

t
l

a
e
h

n
o

i
t

a
r
o
b
a

l
l

o
c

s
p
a
m
n
o

/

i
t

a
c
o

l

y
b
b
o
h

/

e
m
o
h

e
s
i
r
p
r
e

t

n
e

l

r
e
p
o
e
v
e
d

Figure 15: Revenue comes from few categories. We see that 95%
of the total revenue comes from the music, games, utilities, and
productivity categories. The revenue per category is not corre-
lated with the number of apps per category, and it is slightly
correlated with the number of developers in these categories.

category is not related with the percentage of apps in the same cat-
egory, with Pearson’s correlation coefﬁcient equal to 0.014, and it
is slightly related with the percentage of developers in these cate-
gories, with correlation coefﬁcient equal to 0.198. The percentage
of apps and developers per category are more correlated, with Pear-
son’s correlation coefﬁcient equal to 0.378.

6.3 Revenue Stategies

In this section we set out to explore the common developers’
strategies, and how these strategies may affect the developers’ in-
come (Q3). From our dataset we see that there exist 5,106 develop-
ers in the SlideMe marketplace for 22,184 applications. Thus, each
developer builds 4.3 apps on average. Figure 16(a) shows the CDF
of the number of free and paid apps offered by each developer. We
see that 60% of the free app developers, and 70% of the paid app
developers, create only a single app. Overall, 95% of the develop-
ers offer less than 10 apps. We also observe that there is a small
fraction of developers that release a large number of apps, e.g., one
developer with 1,402 apps and another with 592 apps. We found
that these developer accounts correspond to two different compa-

)
F
D
C

(
 
s
r
e
p
o
e
v
e
D

l

 1

 0.8

 0.6

 0.4

 0.2

 0

 1

 0.8

 0.6

 0.4

 0.2

)
F
D
C

(
 
s
r
e
p
o
e
v
e
D

l

 0

 0

free apps
paid apps

 2

 4

 6

 8

 10

 12

 14

 16

 1

 10

 100

 1000

 10000

free apps
paid apps

Number of apps made by developer

Number of unique categories

(a) Number of apps

(b) Number of categories

Figure 16: Developers create a small number of apps focused on
few categories. CDF of the number of free and paid apps per
developer shows that 95% of the developers offer less than 10
apps. CDF of the number of unique categories per developer
shows that 99% of the developers focus on 1–5 categories.

nies that develop many mobile apps systematically. From these
5,106 developers we see that 75% of them offer only free apps,
15% of them offer only paid apps, while there is a 10% of devel-
opers that offer both free and paid apps. This outcome implies that
most developers choose a single pricing strategy: either they offer
their apps as free, aiming at income from advertisements or in-app
billing, or they choose to sell their apps at a pre-deﬁned price.

Figure 16(b) shows the CDF of the number of unique categories
for which each developer create apps. We see that developers usu-
ally focus on one or just few categories: 75% of the free app devel-
opers and 85% of the paid app developers create apps belonging to
a single category, while 99% of the developers in both cases focus
on 1 up to 5 categories. This outcome shows that most developers
are also interested in speciﬁc app categories.

In Section 6.2 we focused on the income made by paid apps in
SlideMe, based on the apps’ price and downloads. However, free
apps also give opportunities for ﬁnancial beneﬁts to their devel-
opers, mainly due to advertisements (ads) and in-app billing. To
validate this argument, we measured the percentage of free apps in
SlideMe that include at least one of the 20 most popular advertising
networks [33], using the Androguard [2] reverse engineering anal-
ysis tool, which attempts to detect advertising libraries within APK
ﬁles. The results of this analysis show that 67% of the free apps
in SlideMe include ads, i.e., at least one of the 20 most popular ad
networks. The SlideMe web page also indicates whether each app

286 
)
s
r
a

l
l

o
d
(
 
e
m
o
c
n

i
 

d
a

 
y
r
a
s
s
e
c
e
N

0.25
0.24
0.23
0.22
0.21
0.20
0.19
0.18
1.60
1.40
1.20
1.00
0.80
0.60
0.40
0.20
0.00

Average free app

Unpopular free apps
Free apps with medium popularity
Most popular free apps

1

0

1 . 6

)
s
r
a

l
l

o
d
(
 

e
m
o
c
n

i
 

d
a
 
y
r
a
s
s
e
c
e
N

 10

 1

 0.1

 0.01

 0.001

1

7

0 . 0

4

6

0 . 0

9

4

0 . 0

2

4

0 . 0

3

3

0 . 0

8

2

0 . 0

0 . 0

8

2

6

1

0 . 0

6

1

0 . 0

0 . 0

6

1

1

1

0 . 0

0 . 0

1

1

7

0

0 . 0

5

0

0 . 0

5

0

0 . 0

3

0

0 . 0

3

0

0 . 0

2

0

0 . 0

2

0

0 . 0

 0

 10

 20

 30

 40

 50

 60

 70

 80

 90

 100

Day

n
o
g

i

i
l

e
r

s
e

i
t
i
l
i
t

u

i

c
s
u
m

y
t
i
v
i
t
c
u
d
o
r
p

l

e
v
a
r
t

s
e
m
a
g

/

n
u

f

n
o

i
t

a
r
o
b
a

l
l

o
c

t

n
e
m
n
a

i

t
r
e
n
e

t

l

i

a
c
o
s

l

a
n
o

i
t

a
c
u
d
e

l

e
y
t
s
e

f
i
l

e
s
i
r
p
r
e
n
e

t

y
b
b
o
h

/

e
m
o
h

s
s
e
n

t
i
f
/

h

t
l

a
e
h

s
p
a
m
n
o

/

i
t

a
c
o

l

r
e
h

t

o

s
k
o
o
b
-
e

l

r
e
p
o
e
v
e
d

s
r
e
p
a
p

l
l

a
w

s
n
o

i
t

i

a
c
n
u
m
m
o
c
 

Figure 17: Free apps with ads may have higher ﬁnancial bene-
ﬁts than paid apps. An average free app needs about $0.21 per
download to match the income of an average paid app. Break-
even ad income drops over time because downloads increase
faster for free apps. Popular free apps have a lower break-even
ad income, thus better chances for increased revenue.

Figure 18: Some app categories may be more proﬁtable for devel-
opers that focus on free apps with ads. A free app in categories
like wallpapers or e-books needs to make just $0.002 per down-
load to match the income of a paid app in the same category.
In contrast, free apps in the music category need to make more
than $1.6 to gain a higher proﬁt.

(free or paid) contains ads. We observed that this information is
generally true, compared with our analysis on the actual APK ﬁles
for free apps, with just a few exceptions. We also observed that
very few paid apps state that they use ads, which implies that there
are two different revenue strategies: either paid apps with no ads,
or free apps with advertisements.

Using our dataset, we evaluate the two different revenue strate-
gies: (i) paid apps, which require a payment before downloading,
and (ii) free apps using advertisements, which make proﬁt after in-
stallation. However, we do not have any data about the usage of
free apps upon installation, i.e., clicks and impressions, to approxi-
mate the actual income from those apps. Therefore, to compare the
two alternative revenue strategies, we estimate the ad income that a
free app needs to make per download to match the income of a paid
app. We can estimate this break-even ad income per download for
a free app based on the average number of downloads measured for
free and paid apps in the SlideMe appstore, and the average price
of paid apps, using the following formula:

Npaid

Ad Income =

Pi=1

Downloads(i) × P rice(i)/Npaid

Nf ree

Pj=1

Downloads(j)/Nf ree

(7)

where Npaid and Nf ree the number of paid and free apps respec-
tively. We consider only free apps with ads in this analysis.

Figure 17 shows the break-even ad income per download for an
average free app as a function of time, for the last three months
of our measurement period. We see that overall, an average free
app needs to make just $0.21 per download from ads in order to
match the income of an average paid app. Moreover, we see that
the break-even ad income is dropping over time. This happens be-
cause the average downloads of free apps increase much faster than
the average downloads of paid apps. Thus, for many apps, where
users are expected to spend some time using the application, the
ad-based revenue strategy seems more promising for increased in-
come. While the paid apps collect money once per download, a
free app may continue to earn money for a long period after the
download, depending on app usage.

In Figure 17 we also see the break-even ad income for the most
popular free apps, which are the 20% of apps with most downloads,
for free apps with medium popularity, which are the next 50% of
apps, and for the less popular free apps, which are the last 30% of
apps with the fewer downloads. We see that a popular app needs an
even lower ad income, just $0.033 per download, to outreach the in-
come of a paid app. Even for the less popular apps, the break-even
ad income is $1.56 per download, which is still 2.7 times lower
than the average price of a paid app. Therefore, especially for free
apps with high and medium popularity, the ad-based revenue strat-
egy seems to be able to offer signiﬁcantly higher ﬁnancial beneﬁts
than asking for payment only once before downloading.

We performed the same analysis for each app category, as pric-
ing depends on several factors that may vary across different cate-
gories. Such factors, for example, include the effort of developing
an app (e.g., games in general are more complex to be developed
than e-books or wallpaper apps) or the popularity of the app cate-
gory. Figure 18 shows the break-even ad income per app category.
We see that break-even ad income differs among categories. For in-
stance, the highest break-even ad income, roughly $1.6, is observed
for music apps, while the lowest is observed for the wallpaper and
e-book categories (close to $0.002). This is because music is the
dominant category of paid apps in terms of revenue, as we see in
Figure 15. Moreover, it is interesting to note that a free app of the
fun or games category, which is the second more lucrative cate-
gory, needs to make only $0.04 per download on average to reach
the revenue of a category-equivalent paid app.

7.

IMPLICATIONS

We believe that our study can be useful for both appstore opera-
tors and app developers in several ways. In this section, we brieﬂy
discuss some practical implications.
New replacement policies for improved app caching. The exis-
tence of locality in user downloads, as manifested by the Pareto ef-
fect, can help appstore operators to design efﬁcient caching mecha-
nisms for improving the speed of app delivery to end users, e.g., by
caching the most popular apps in fast memory or local network
caches. To explore the beneﬁts of this locality, we simulated a
typical cache with a Least Recently Used (LRU) replacement pol-

287)

%

(
 
o

i
t

a
r
 
t
i

 

h
e
h
c
a
C

 100

 95

 90

 85

 80

 75

 70

 65

ZIPF
ZIPF-at-most-once
APP-CLUSTERING

 0

 2

 4

 6

 8

 10  12  14  16  18  20

Cache size (% of total apps)

Figure 19: Clustering-based user behavior has a negative im-
pact on LRU cache performance. Cache hit ratio as a function
of cache size for an LRU app cache. Although the LRU cache
achieves a high hit ratio, APP-CLUSTERING results in a sig-
niﬁcantly lower hit ratio than ZIPF and ZIPF-at-most-once.

icy. We varied the cache size in terms of apps, assuming that all
apps have the same size. In our dataset, the average app size is
3.5 MB. For each cache size, the cache was initialized with the re-
spective number of most popular apps. To examine the effect of the
observed clustering-based user behavior on a typical LRU cache,
we simulated user downloads based on the three different models:
ZIPF, ZIPF-at-most-once, and APP-CLUSTERING. We simulated
an appstore similar to Anzhi, i.e., with 60,000 apps divided into
30 categories, 600,000 users, and 2 million downloads. We used a
ZIPF exponent zr = 1.7 for overall app ranking, and a ZIPF ex-
ponent zc = 1.4 for each cluster’s app ranking. The percentage of
downloads based on clustering was set to 90% (p = 0.9). In case
of a cache miss, the requested app was replacing the least recently
downloaded app from the cache.

Figure 19 shows the cache hit ratio as a function of cache size,
which varies from 1% to 20% of the number of total apps (i.e.,
from 6,000 up to 120,000 apps, which roughly corresponds to 2.1–
42 GB of actual cache size), when generating user downloads with
each one of the three different models. In general, we see that app
caching with an LRU policy achieves a high hit ratio. For example,
when caching 10% of the apps, the cache hit ratio is higher than
90% for all models.

However, we see that the workload generated by the APP-CLUS-
TERING model, which simulates the observed clustering-based
user behavior, results in a signiﬁcantly reduced hit ratio compared
to the other two models. While ZIPF-based workload generation
(e.g., like web downloads) leads to a hit ratio higher than 99% for
all cache sizes, and ZIPF-at-most-once workloads (e.g., like peer-
to-peer ﬁle sharing downloads) result in a high hit ratio starting at
94.5% and exceeding 99% for cache sizes larger than 10% of the
total apps, we see a signiﬁcantly lower hit ratio for APP-CLUS-
TERING: from 67.1% up to 96.3% when the cache size varies from
1% to 20% of total apps. Thus, we conclude that although an LRU
cache can exploit locality of user downloads to improve the perfor-
mance of app delivery, the clustering behavior we have identiﬁed
in this work has a negative effect on the performance of a typical
LRU cache. To improve cache performance in the context of app
delivery, we believe that new replacement policies should be used,
taking into account the clustering-based user behavior.
Effective prefetching. Understanding the temporary afﬁnity of
users to app categories can help the design of more efﬁcient prefetch-
ing methods. As we showed in Section 4, a user that downloads an

app from a given category is more likely to download the next few
apps from the same category. Thus, the most popular apps from
this category that have not been downloaded by the user can be
prefetched to a local place in order to improve user experience and
app delivery performance.
Better recommendation systems. The understanding of user down-
load patterns, like the clustering effect, can help appstore opera-
tors to design better recommendation systems, which can beneﬁt
(i) apps and developers by providing better opportunities for more
suggestions and downloads, and (ii) end users by providing more
useful suggestions. A recommendation system can beneﬁt apps and
developers by identifying the apps that need to be recommended
to increase their popularity, and by identifying users interested in
the respective app category. Also, end users can beneﬁt from a
better recommendation system by improving their user experience.
A typical recommendation system follows a collaborative ﬁltering
method [18]. The main idea of this method is to ﬁnd groups of
users who share similar interests. These users can be determined
based on their similar app downloads. If an app is downloaded by
most users in a group, then it is likely to be of interest for another
user in the same group that has not yet downloaded it. Thus, this is
a possible suggestion from the recommendation system.

The clustering effect, however, can also suggest apps that have
not necessarily been downloaded by users who share similar inter-
ests. For example, a user who downloaded an app from a given
category may be interested in other popular apps of the same cat-
egory. Thus, the clustering effect provides a richer set of choices
than the limited choices of current recommendation systems, which
are mainly based on the common apps downloaded by a set of
users. Moreover, users who prefer a large variety of choices will
have a better experience with recommendation systems that do not
bombard them with the same set of popular apps. In addition, cap-
italizing on the temporal afﬁnity of users to app categories, as we
explained in Section 4, the recommendation system can suggest
apps related to the most recent interests of a user, instead of apps
related to older downloads.
Identify and help problematic apps. Our model of app down-
loads can be used by appstores to estimate future app downloads
based on app popularity. This will enable appstores to pinpoint
problematic apps and favor them through better recommendations.
Larger category diversity. Another possible implication of the
clustering effect is that, currently, users appear to be adherent to
one speciﬁc category. This may be because they were initially in-
terested in a speciﬁc app and they ended up downloading more apps
from a single category, based on appstore’s suggestions. Hence,
there might be a need to expose apps from more categories through
recommendations. This may also incentivize developers to offer a
wider set of apps across multiple categories.
Understand and improve app popularity. Understanding the pa-
rameters that affect app popularity is of high interest for developers
who want to predict, understand, and most importantly, increase
the popularity of their apps. To this end, we provide insights on
how user behavior, app category, and other parameters affect app
downloads in four popular third-party Android appstores.
Maximize income. Understanding which pricing models result in
higher revenue can help developers to choose the appropriate pric-
ing policies for increasing app popularity and, eventually, their in-
come. To this end, we examine how cost, app popularity, and app
categories affect a developer’s income in our dataset. We also com-
pare two popular revenue strategies: paid apps versus free apps
with advertisements. Our results indicate that the most proﬁtable
revenue strategy depends on app popularity, app category, and ad
income per download.

2888. RELATED WORK

Workload Characterization. There are numerous studies fo-
cusing on the workload characterization of networks and distributed
systems, aiming to improve the understanding of their complex
behavior by identifying high-level trends and summarizing statis-
tics that explain signiﬁcant properties of their workloads. Many
of these studies concentrate on web workloads. Breslau et al. [20]
show that web requests follow a Zipf-like distribution, and pro-
pose a simple model based on this distribution to explain the ob-
served behavior. Crovella and Bestavros [25] show that web trafﬁc
exhibits self-similar characteristics, which can be modeled using
heavy-tailed distributions. Based on web workload characteristics,
Barford and Crovella [19] propose a model for representative web
workload generation. In a similar spirit, we show that mobile app
popularity follows a Zipf-like distribution with truncated ends, and
we propose a model that approximates the observed distribution.

Besides the web, there are also several studies that focus on other
systems’ workload characterization. Gummadi et al. [34] show that
popularity distribution in peer-to-peer ﬁle sharing systems deviates
from Zipf for very popular objects, which is also observed in our
study, and introduce a model that explains this behavior. Cha et
al. [21] study YouTube and ﬁnd that the popularity distribution of
user-generated video content follows power-law with an exponen-
tial cutoff, which is similar to the app popularity distribution we ob-
serve in our study. Power-law distributions with exponential cutoffs
have been also identiﬁed in other networks, such as live streaming
media networks [24], protein, e-mail, actor, and collaboration net-
works [31]. There are also many works focusing on the explanation
and understanding of the power-law distributions [37, 38].

Systematic studies of mobile apps. Recently, mobile systems
and applications have attracted the interest of researchers that try
to understand the behavior and functionality of this emerging tech-
nology. Several research efforts were made for collecting and ana-
lyzing large sets of mobile apps from multiple marketplaces. These
efforts are mainly focused on security and privacy-related analysis,
such as malware detection [45], malware analysis [44], overpriv-
ilege identiﬁcation [30], detection of privacy leaks [26, 32], mali-
cious ad libraries [33], and vulnerability assessment [27]. In this
work we collected and analyzed a similar large-scale dataset, but
our analysis was focused on characterizing and modeling the work-
load of the monitored marketplaces.

In a work closely related to ours, Xu et al. [42] study the usage
behavior of smartphone apps by analyzing IP-level traces from a
tier-1 cellular network provider. Their analysis is mostly focused
on spatial and temporal locality, geographic coverage, and diurnal
usage patterns. On the other hand, our analysis focuses on app
popularity and pricing strategies. We also use a different dataset,
by systematically crawling four third-party appstores.

Other related approaches focus on mobile trafﬁc analysis as well,
but they do not study their relation with mobile applications. Maier
et al. [36] perform a study of residential DSL lines of a European
ISP and ﬁnd that mobile devices’ trafﬁc is dominated by multime-
dia content and applications’ downloads. Falaki et al. [28] con-
duct a trafﬁc analysis on 43 different smartphones. They show
that browsing contributes most of the trafﬁc, and lower layer proto-
cols impose high overheads due to small transfer sizes. They also
study the factors that impact performance and power consumption
on smartphone communications and propose improvements.

Falaki et al. [29] analyze user behavior in two different smart-
phone platforms in order to understand and characterize user activ-
ities and their impact on network and battery. They observe a diver-
sity in user patterns, which implies that techniques for improving
user experience or power consumption on the average case may be

inefﬁcient for a large fraction of users. Wei et al. [41] present a
multi-layer system for monitoring and proﬁling Android apps.

Vallina-Rodriguez et al. [40] characterize the mobile ad trafﬁc by
analyzing a large network trace of mobile users. They focus on the
volume, frequency, and content of the mobile ad trafﬁc, and they
show the distribution of ad trafﬁc among the different platforms,
ad networks, and mobile apps. They also study the energy impli-
cations of mobile advertising in smartphone devices. Tongaonkar
et al. [39] study mobile apps’ usage patterns based on mobile ad
trafﬁc found in network traces. On the other hand, we study the
app download patterns, the app popularity based on the number of
downloads from each marketplace, and the relation of app down-
loads with developers’ income.

Another related work by Zhong and Michahelles [43] study app
popularity patterns using data from appaware, an application that
is optionally installed by Android users in their devices. This ap-
plication captures information regarding installations, updates and
removal of apps at real time. Their main outcome is that 10% of the
apps account for the 90% of the total downloads, very similar to our
results. Our work presents a more detailed study on app popular-
ity distribution, along with a model for app downloads. Moreover,
we gathered and analyzed data from all the apps of four different
third-party Android appstores, while Zhong and Michahelles have
a limited view using data only from the users that have installed the
appaware application. To the best of our knowledge, our work is
the ﬁrst systematic and large-scale study that focuses on the work-
load characterization of mobile application marketplaces.

9. CONCLUSION

The mobile application market has recently gained increasing
popularity, yet the main characteristics and parameters affecting its
workload are still poorly understood. In this paper we presented a
systematic study of the mobile app ecosystem by collecting and an-
alyzing data from four popular third-party Android appstores. The
analysis was focused on app popularity distribution and app pric-
ing. We also brieﬂy discuss possible implications of our ﬁndings.
Our results show a highly-skewed Pareto effect where 10% of the
apps account for 70-90% of the total downloads. The app popular-
ity follows a Zipf-like distribution with truncated ends, unlike web
and peer-to-peer workloads, but similar to user-generated video
content. We attributed the observed distribution to a new down-
load pattern we refer to as “clustering effect.” The clustering effect
implies that users will download the next apps from the same cat-
egories as their previous downloads. We validated the existence of
clustering effect with a user behavior study, which shows a strong
temporal afﬁnity of users to app categories. Then, we introduced
a new model for app downloads based on clustering effect, and we
validated with simulations that it approximates very close the actual
downloads. Our model is able to approximate very well the down-
loads distributions of all appstores, which implies that the proposed
clustering effect is a more general phenomenon rather than a ran-
dom property of a single appstore.

We also studied the effect of cost on app popularity. We observed
that paid apps follow a clear power law distribution, probably due
to the more selective behavior of users when they are paying to
download an app. We measured how the revenue of paid apps are
distributed to developers. The results show that the large majority
of developers have very low income, while a very small percentage
of them make a signiﬁcantly higher income. We also found that the
number of developer’s apps do not increase developer’s income.
Finally, we found that free apps with ads need to make just $0.21
per download to match the income of paid apps, which seems a
more promising strategy especially for popular apps.

289Acknowledgements
We would like to thank our shepherd Chen-Nee Chuah and the
anonymous reviewers for their valuable feedback. We would also
like to thank Daniel Song for helping with the implementation of
crawler prototypes for AppChina and Anzhi. This work was sup-
ported in part by the FP7 project SysSec and the FP7-PEOPLE-
2009-IOF project MALCODE, funded by the European Commis-
sion under Grant Agreements No. 254116 and No. 257007. An-
tonis Papadogiannakis and Evangelos Markatos are also with the
University of Crete.

10. REFERENCES
[1] Amazon Appstore. http://www.amazon.com/appstore/.
[2] Androguard. http://code.google.com/p/androguard/.
[3] AndroLib. http://www.androlib.com/.
[4] AppBrain. http://www.appbrain.com/.
[5] Disgo, FAQ. http://www.mydisgo.info/index.php/

faq?faqid=5.

[6] Google Play. http://en.wikipedia.org/wiki/Google_

Play.

[7] Google Play hits 600,000 apps, 20 billion total installs. http://

www.engadget.com/2012/06/27/google-play-hits-
600000-apps/.

[8] Google Says 700,000 Applications Available for Android.

http://www.businessweek.com/news/2012-10-29/
google-says-700-000-applications-available-
for-android-devices.

[9] iOS and Android Adoption Explodes Internationally. http://
blog.flurry.com/bid/88867/iOS-and-Android-
Adoption-Explodes-Internationally.

[10] Scrapy framework. http://scrapy.org/.
[11] Selenium Remote Control (RC), a web application testing system.

http://seleniumhq.org/projects/remote-
control/.

[12] Supported locations for merchants, Google Play. https://

support.google.com/googleplay/android-
developer/answer/150324?hl=en&ref_topic=15867.

[13] The 1Mobile Marketplace website. http://www.1mobile.

com/.

[14] The Anzhi Marketplace website. http://www.anzhi.com/.
[15] The AppChina Marketplace website. http://www.appchina.

com/.

[16] The SlideMe Marketplace website. http://slideme.org/.
[17] Vodafone Egypt Ships Android Phones With SlideME App Store.

http://www.distimo.com/blog/2009_11_vodafone-
egypt-ships-android-phones-with-slideme-app-
store/.

[18] G. Adomavicius and A. Tuzhilin. Toward the Next Generation of

Recommender Systems: A Survey of the State-of-the-Art and
Possible Extensions. IEEE Transactions on Knowledge and Data
Engineering, 17(6), 2005.

[19] P. Barford and M. Crovella. Generating Representative Web

Workloads for Network and Server Performance Evaluation. In ACM
International Conference on Measurement and Modeling of
Computer Systems (SIGMETRICS), 1998.

[20] L. Breslau, P. Cao, L. Fan, G. Phillips, and S. Shenker. Web Caching

and Zipf-like Distributions: Evidence and Implications. In IEEE
International Conference on Computer Communications
(INFOCOM), 1999.

[21] M. Cha, H. Kwak, P. Rodriguez, Y.-Y. Ahn, and S. Moon. I Tube,
You Tube, Everybody Tubes: Analyzing the World’s largest User
Generated Content Video System. In ACM SIGCOMM Conference
on Internet Measurement (IMC), 2007.

[22] J. Cho and S. Roy. Impact of Search Engines on Page Popularity. In

International Conference on World wide web (WWW), 2004.

[23] B. Chun, D. Culler, T. Roscoe, A. Bavier, L. Peterson,

M. Wawrzoniak, and M. Bowman. PlanetLab: An Overlay Testbed
for Broad-Coverage Services. ACM SIGCOMM Computer
Communication Review (CCR), 33(3), 2003.

[24] C. P. Costa, I. S. Cunha, A. Borges, C. V. Ramos, M. M. Rocha,

J. M. Almeida, and B. Ribeiro-Neto. Analyzing Client Interactivity

in Streaming Media. In International Conference on World wide web
(WWW), 2004.

[25] M. E. Crovella and A. Bestavros. Self-Similarity in World Wide Web

Trafﬁc: Evidence and Possible Causes. IEEE/ACM Transactions on
Networking, 5(6), 1997.

[26] W. Enck, P. Gilbert, B.-G. Chun, L. P. Cox, J. Jung, P. McDaniel,

and A. N. Sheth. TaintDroid: An Information-Flow Tracking System
for Realtime Privacy Monitoring on Smartphones. In USENIX
Symposium on Operating System Design and Implementation
(OSDI), 2010.

[27] W. Enck, D. Octeau, P. McDaniel, and S. Chaudhuri. A Study of
Android Application Security. In USENIX Security Symposium,
2011.

[28] H. Falaki, D. Lymberopoulos, R. Mahajan, S. Kandula, and
D. Estrin. A First Look at Trafﬁc on Smartphones. In ACM
SIGCOMM Conference on Internet Measurement (IMC), 2010.

[29] H. Falaki, R. Mahajan, S. Kandula, D. Lymberopoulos,

R. Govindan, and D. Estrin. Diversity in Smartphone Usage. In ACM
International Conference on Mobile Systems, Applications, and
Services (MobiSys), 2010.

[30] A. P. Felt, E. Chin, S. Hanna, D. Song, and D. Wagner. Android
Permissions Demystiﬁed. In ACM Conference on Computer and
Communications Security (CCS), 2011.

[31] T. Fenner, M. Levene, and G. Loizou. A Stochastic Evolutionary

Model Exhibiting Power-Law Behaviour with an Exponential
Cutoff. Physica A: Statistical Mechanics and its Applications,
355(2), 2005.

[32] M. Grace, Y. Zhou, Z. Wang, and X. Jiang. Systematic Detection of
Capability Leaks in Stock Android Smartphones. In ISOC Network
and Distributed System Security Symposium (NDSS), 2012.
[33] M. C. Grace, W. Zhou, X. Jiang, and A.-R. Sadeghi. Unsafe

Exposure Analysis of Mobile In-App Advertisements. In ACM
Conference on Security and Privacy in Wireless and Mobile
Networks (WISEC), 2012.

[34] K. P. Gummadi, R. J. Dunn, S. Saroiu, S. D. Gribble, H. M. Levy,

and J. Zahorjan. Measurement, Modeling, and Analysis of a
Peer-to-Peer File-Sharing Workload. In ACM Symposium on
Operating Systems Principles (SOSP), pages 314–329, 2003.

[35] S. Jansen and E. Bloemendal. Deﬁning App Stores: The Role of
Curated Marketplaces in Software Ecosystems. In International
Conference on Software Business (ICSOB), 2013.

[36] G. Maier, F. Schneider, and A. Feldmann. A First Look at Mobile
Hand-Held Device Trafﬁc. In International Conference on Passive
and Active Measurement (PAM), 2010.

[37] S. Mossa, M. Barthelemy, H. E. Stanley, and L. A. N. Amaral.

Truncation of Power Law Behavior in "Scale-Free" Network Models
due to Information Filtering. PHYS.REV.LETT, 2002.

[38] M. E. J. Newman. Power Laws, Pareto Distributions and Zipf’s Law.

Contemporary Physics, 2005.

[39] A. Tongaonkar, S. Dai, A. Nucci, and D. Song. Understanding
Mobile App Usage Patterns Using In-App Advertisements. In
International Conference on Passive and Active Measurement
(PAM), 2013.

[40] N. Vallina-Rodriguez, J. Shah, A. Finamore, Y. Grunenberger,
K. Papagiannaki, H. Haddadi, and J. Crowcroft. Breaking for
Commercials: Characterizing Mobile Advertising. In ACM
SIGCOMM Conference on Internet Measurement (IMC), 2012.
[41] X. Wei, L. Gomez, I. Neamtiu, and M. Faloutsos. ProﬁleDroid:

Multi-Layer Proﬁling of Android Applications. In ACM
International Conference on Mobile Computing and Networking
(MobiCom), 2012.

[42] Q. Xu, J. Erman, A. Gerber, Z. Mao, J. Pang, and S. Venkataraman.
Identifying Diverse Usage Behaviors of Smartphone Apps. In ACM
SIGCOMM Conference on Internet Measurement (IMC), 2011.
[43] N. Zhong and F. Michahelles. Google Play Is Not A Long Tail

Market: An Empirical Analysis of App Adoption on the Google Play
App Market. In ACM Symposium on Applied Computing (SAC),
2013.

[44] Y. Zhou and X. Jiang. Dissecting Android Malware:

Characterization and Evolution. In IEEE Symposium on Security and
Privacy, 2012.

[45] Y. Zhou, Z. Wang, W. Zhou, and X. Jiang. Hey, You, Get Off of My

Market: Detecting Malicious Apps in Ofﬁcial and Alternative
Android Markets. In ISOC Network and Distributed System Security
Symposium (NDSS), 2012.

290