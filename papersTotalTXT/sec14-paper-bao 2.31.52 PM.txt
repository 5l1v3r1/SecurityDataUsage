ByteWeight: Learning to Recognize Functions  

in Binary Code

Tiffany Bao, Jonathan Burket, and Maverick Woo, Carnegie Mellon University;  
Rafael Turner, University of Chicago; David Brumley, Carnegie Mellon University

https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/bao

This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXBYTEWEIGHT: Learning to Recognize Functions in Binary Code

Tiffany Bao

Carnegie Mellon University

tiffanybao@cmu.edu

Jonathan Burket

Carnegie Mellon University

jburket@cmu.edu

Maverick Woo

Carnegie Mellon University

pooh@cmu.edu

Rafael Turner

University of Chicago

turnersr@uchicago.edu

David Brumley

Carnegie Mellon University

dbrumley@cmu.edu

Abstract
Function identiﬁcation is a fundamental challenge in re-
verse engineering and binary program analysis. For in-
stance, binary rewriting and control ﬂow integrity rely on
accurate function detection and identiﬁcation in binaries.
Although many binary program analyses assume func-
tions can be identiﬁed a priori, identifying functions in
stripped binaries remains a challenge.

In this paper, we propose BYTEWEIGHT, a new au-
tomatic function identiﬁcation algorithm. Our approach
automatically learns key features for recognizing func-
tions and can therefore easily be adapted to different
platforms, new compilers, and new optimizations. We
evaluated our tool against three well-known tools that
feature function identiﬁcation: IDA, BAP, and Dyninst.
Our data set consists of 2,200 binaries created with three
different compilers, with four different optimization lev-
els, and across two different operating systems. In our
experiments with 2,200 binaries, we found that BYTE-
WEIGHT missed 44,621 functions in comparison with the
266,672 functions missed by the industry-leading tool
IDA. Furthermore, while IDA misidentiﬁed 459,247 func-
tions, BYTEWEIGHT misidentiﬁed only 43,992 functions.
1
Binary analysis is an essential security capability with
extensive applications, including protecting binaries with
control ﬂow integrity (CFI) [1], extracting binary code
sequences from malware [9], and hot patching vulnerabil-
ities [25]. Research interest in binary analysis shows no
sign of waning. In 2013 alone, several papers such as CFI
for COTS [34] (referred to as COTS-CFI in this paper),
the Rendezvous search engine for binaries [21], and the
Phoenix decompiler [28] focus on developing new binary
analysis techniques.

Introduction

Function identiﬁcation is a preliminary and necessary
step in many binary analysis techniques and applications.
For example, one property of CFI is to constrain inter-
function control ﬂow to valid paths. In order to reason

about such paths, however, binary-only CFI infrastruc-
tures need to be able to identify functions accurately. In
particular, COTS-CFI [34], CCFIR [33], MoCFI [12],
Abadi et al. [1], and extensions like XFI [15] all depend
on accurate function identiﬁcation to be effective.

CFI is not the only consumer of binary-level function
identiﬁcation techniques. For example, Rendezvous [21]
is a search engine that operates at the granularity of func-
tion binaries; incorrect function identiﬁcation can there-
fore result in incomplete or even incorrect search results.
Decompilers such as Phoenix [28], Boomerang [32], and
Hex-Rays [18] recover high-level source code from bi-
nary code. Naturally, decompilation occurs on only those
functions that have been identiﬁed in the input binary.

Given the foundational impact of accurate function
identiﬁcation in so many security applications, is this
problem easy and can thus be regarded as “solved”? Inter-
estingly, recent security research papers seem to have con-
ﬂicting opinions on this issue. On one side, Kruegel et al.
argued in 2004 that function start identiﬁcation can be
solved “very well” [23, §4.1] in regular binaries and even
some obfuscated ones. On the other side, Perkins et al.
described static function start identiﬁcation as “a complex
task in a stripped x86 executable” [25, §2.2.3] and there-
fore applied a dynamic approach in their ClearView sys-
tem. A similar opinion is also shared by Zhang et al., who
stated that “it is difﬁcult to identify all function bound-
aries” [34, §3.2] and used a set of heuristics for this task.
So how good are the current tools at identifying func-
tions from stripped, non-malicious binaries? To ﬁnd
out, we collected a dataset of 2,200 Linux and Win-
dows binaries generated by GNU gcc, Intel icc, and
Microsoft Visual Studio (VS) with multiple optimization
levels. We then use our dataset to evaluate the most
recent release of three popular off-the-shelf solutions
for function identiﬁcation: (i) IDA (v6.5 at submission),
used in CodeSurfer/x86 [2], Choi et al.’s work on stati-
cally determining binary similarity [11], BinDiff [4], and
BinNavi [5]; (ii) the CMU Binary Analysis Platform

USENIX Association  

23rd USENIX Security Symposium  845

(BAP, v0.7), used in the Phoenix decompiler [28] and
the vulnerability analysis tool Mayhem [10]; and (iii) the
unstrip utility in Dyninst (dated 2012-11-30), used in
BinSlayer [7], Sharif et al.’s work on dynamic malware
analysis [29], and Sidiroglou et al.’s work on software
recovery navigation [30].

Our ﬁnding is that while IDA performs better than BAP
and Dyninst on our dataset, its result can still be quite
alarming—in our experiment, IDA returned 521,648 true
positives (41.81%), 266,672 false negatives (21.38%),
and 459,247 false positives (36.81%). While there is no
doubt that such failures can have a negative impact on
downstream security analyses, a real issue is in setting
the right expectation on the subject within the security
research community. If there is a publicly-available func-
tion identiﬁcation solution where both its mechanism and
limitations are well-understood by researchers, then re-
searchers may be come up with creative strategies to cope
with the limitations in their own projects. The goal of
this paper is to explain our process of developing such a
solution and to establish its quality through evaluating it
against the aforementioned solutions.

We draw inspirations from how BAP and Dyninst per-
form function identiﬁcation since their source code is
available. Both solutions rely on ﬁxed, manually-curated
signatures. Dyninst, at the version we tested, uses the
byte signature 0x55 (push %ebp in assembly) to recog-
nize function starts in ELF x86 binaries [14]. BAP v0.7
uses a more complex signature, but it is also manually
generated. Unfortunately, the process of manually gen-
erating such signatures do not scale well. For example,
each new compiler release may introduce new idioms that
require new signatures to capture. The myriad of different
optimization settings, such as omit frame pointers, may
also demand even more signatures. Clearly, we cannot
expect to manually catch up.

One approach to recognizing functions is to automati-
cally learn key features and patterns. For example, semi-
nal work by Rosenblum et al. proposed binary function
start identiﬁcation as a supervised machine learning clas-
siﬁcation problem [27]. They model function start identi-
ﬁcation as a Conditional Random Field (CRF) in which
binary offsets and a number of selected idioms (patterns)
appear in the CRF. Since standard inference methods for
CRF on large, highly-connected graphs are expensive,
Rosenblum et al. adopted feature selection and approxi-
mate inference to speed up their model. However, using
hardware available in 2008, they needed 150 compute-
days just for the feature selection phase on 1,171 binaries.
In this paper, we propose a new automated analysis
for inferring functions and implemented it in our BYTE-
WEIGHT system. A key aspect of BYTEWEIGHT is the
ability to learn signatures for new compilers and opti-
mizations at least one order of magnitude faster than as

reported by Rosenblum et al. [27], even after generously
accounting for CPU speed increase since 2008. In par-
ticular, we avoid using CRFs and feature selection, and
instead opt for a simpler model based on learning preﬁx
trees. Our simpler model is scalable using current comput-
ing hardware: we ﬁnish training 2,064 binaries in under
587 compute-hours. BYTEWEIGHT also does not require
compiler information of testing binaries, which makes the
tool more powerful in practice. In the interest of open
science, we also make our tools and datasets available to
seed future improvements.

At a high level, we learn signatures for function starts
using a weighted preﬁx tree, and recognize function starts
by matching binary fragments with the signatures. Each
node in the tree corresponds to either a byte or an instruc-
tion, with the path from the root node to any given node
representing a possible sequence of bytes or instructions.
The weights, which can be learned with a single linear
pass over the data set, express the conﬁdence that a se-
quence of bytes or instructions corresponds to a function
start. After function start identiﬁcation, we then use value
set analysis (VSA) [2] with an incremental control ﬂow re-
covery algorithm to ﬁnd function bodies with instructions,
and extract function boundaries.

To evaluate our techniques, we perform a large-scale
experiment and provide empirical numbers on how well
these tools work in practice. Based on 2,200 binaries
across operating systems, compilers and optimization op-
tions, our results show that BYTEWEIGHT has a precision
and recall of 97.30% and 97.44% respectively for function
start identiﬁcation. BYTEWEIGHT also has a precision
and recall of 92.84% and 92.96% for function boundary
identiﬁcation. Our tool is adaptive for varying compilers
and therefore more general than current pattern matching
methods.

Contributions. This paper makes the following contri-
butions:

• We enumerate the challenges we faced and imple-
ment a new function start identiﬁcation algorithm
based on preﬁx trees. Our approach is automatic
and does not require a priori compiler information
(see §4). Our approach models the function start
identiﬁcation problem in a novel way that makes it
amenable to much faster learning algorithms.

• We evaluate our method on a large test suite across
operating systems, compilers, and compiling opti-
mizations. Our model achieves better accuracy than
previously available tools.

• We make our test infrastructure, data set, implemen-
tation, and results public in an effort to promote open
science (see §5).

846  23rd USENIX Security Symposium 

USENIX Association

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33

#include <stdio.h>
#include <string.h>
#define MAX 10
void sum(char *a, char *b)
{

printf("%s + %s = %d\n",

}
void sub(char *a, char *b)
{

printf("%s - %s = %d\n",

a, b, atoi(a) + atoi(b));

a, b, atoi(a) - atoi(b));

}
void assign(char *a, char *b)
{

char pre_b[MAX];
strcpy(pre_b, b);
strcpy(b, a);
printf("b is changed from %s to %s\n",

pre_b, b);

}
int main(int argc, char **argv)
{

void (*funcs[3])(char *x, char *y);
int f;
char a[MAX], b[MAX];
funcs[0] = sum;
funcs[1] = sub;
funcs[2] = assign;
scanf("%d %s %s", &f, a, b);
(*funcs[f])(a, b);
return 0;

}

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

%rbx,-0x10(%rsp)
%rbp,-0x8(%rsp)
$0x28,%rsp
%rdi,%rbp
0xf(%rsp),%rdi

00400660 <assign>:
mov
mov
sub
mov
lea
...
004006b0 <sub>:
mov
mov
mov
mov
xor
sub
...
00400710 <sum>:
mov
mov
mov
mov
xor
sub
...

%rbx,-0x18(%rsp)
%rbp,-0x10(%rsp)
%rsi,%rbx
%r12,-0x8(%rsp)
%eax,%eax
$0x18,%rsp

%rbx,-0x18(%rsp)
%rbp,-0x10(%rsp)
%rsi,%rbx
%r12,-0x8(%rsp)
%eax,%eax
$0x18,%rsp

(a) Source code

(b) Assembly compiled by gcc -O3

Figure 1: Example C Code. IDA fails to identify functions sum, sub, and assign in the compiled binary.

2 Running Example
We start with a simple example written in C, shown in
Figure 1. In this program, three functions are stored as
function pointers in the array funcs. When the program
is run, input from the user dictates which function gets
called, as well as the function arguments. We compiled
this example code on Linux Debian 7.2 x86-64 using gcc
with -O3, and stripped the binary using the command
strip. We then used IDA to disassemble the binary
and perform function identiﬁcation. Many security tools
use IDA in this way as a ﬁrst step before performing
additional analysis [9, 20, 24]. Unfortunately, for our
example program IDA failed to identify the functions
sum, sub, and assign.

IDA’s failure to identify these three critical functions
has signiﬁcant implications for security analyses that rely
on accurate function boundary identiﬁcation. Recall that
the CFI security policy dictates that runtime execution
must follow a path of the static control ﬂow graph (CFG).
In this case, when the CFG is recovered by ﬁrst iden-
tifying functions using IDA, any call to sum, sub, or
assign would be incorrectly disallowed, breaking legit-
imate program behavior. Indeed, any indirect jump to

an unidentiﬁed or mis-identiﬁed function will be blocked
by CFI. The greater the number of functions missed, the
more legitimate software functionality incorrectly lost.
Secondly, suppose we are checking code for potential
security-critical bugs. In our sample program, the func-
tion assign is vulnerable to a buffer overﬂow attack, but
is not identiﬁed by IDA as a function. For tools like
ClearView [25] that operate on binaries at the function
level, missing functions can mean missing vulnerabilities.
In our analysis of 1,171 binaries, we observed that
that IDA failed to identify 266,672 functions. BYTE-
WEIGHT improves on this number, missing only 44,621.
BYTEWEIGHT also makes fewer mistakes, incorrectly
identifying functions 43,992 times compared to 459,247
with IDA. While these results are not perfect, they demon-
strate that our automated machine learning approach can
outperform years of manual hand-tuning that has gone
into IDA.
3 Problem Deﬁnition and Challenges
The goal of function identiﬁcation is to faithfully deter-
mine the set of functions that exist in binary code. Deter-
mining what functions exist and which bytes belong to
which functions is trivial if debug information is present.

USENIX Association  

23rd USENIX Security Symposium  847

For example, “unstripped” Linux binaries contain a sym-
bol table that maps function names to locations in a bi-
nary, and Microsoft program database (PDB) information
contains similar information for Windows binaries. We
start with notation to make our problem deﬁnition pre-
cise and then formally deﬁne three function identiﬁcation
problems. We then describe several challenges to any
approach or algorithm that addresses the function identiﬁ-
cation problems. In subsequent sections we provide our
approach.

3.1 Notation and Deﬁnitions
A binary program is divided into a number of sections.
Each section is given a type, such as code, data, read-only
data, and so on. In this paper we only consider executable
code, which we treat as a binary string.

Let B denote a binary string. For concreteness, think
of this as a binary string from the .text section in a
Linux executable. Let B[i] denote the ith byte of a binary
string, and B[i : i + j] refer to the list of contiguous bytes
B[i],B[i + 1], . . . ,B[i + j− 1]. Thus, B[i : i + j] is j-bytes
long (with j ≥ 0).
Each byte in an executable is associated with an ad-
dress. The address of byte i is calculated with respect to
a ﬁxed section offset, i.e., if the section offset is ω, the
address of byte i is i + ω. For convenience, we omit the
offset, and refer to i as the ith address. Since the real ad-
dress can always be calculated by adding the ﬁxed offset,
this can be done without loss of generality.

A function Fi in a binary B is a list of addresses cor-
responding to statements in either a function from the
original compiled language or a function introduced di-
rectly by the compiler, denoted as

F = {B[i],B[ j], . . . ,B[k]}

Note that function bytes need not be a set of contiguous
addresses. We elaborate in §3.3 on real optimizations that
result in high-level functions being compiled to a set of
non-contiguous intervals of instructions.

Towards our goal of determining which bytes of a bi-
nary belong to which functions, we deﬁne the set of func-
tions in a binary

FUNCS(B) ={F 1,F2, . . . ,Fk}.

Note that functions may share bytes, i.e., it may be that
F1 ∩ F2 �= /0. We give examples in §3.3 where this is the
case.
We call the lowest address of a function Fi the func-
tion start address si, i.e., si = min(Fi). The function end
address ei is the maximum byte in a function body, i.e.,
ei = max(Fi). We deﬁne the function boundary (si,ei) as
the function start and end addresses for Fi.

In order to evaluate function identiﬁcation algorithms,
we deﬁne ground truth in terms of oracles, which may
have a number of implementations:
Function Oracle. Ofunc is an oracle that, given a binary
B, returns a list of functions FUNCS(B) where each
Fi is a set of bytes representing higher-level function
i, as deﬁned above.

returns

the

Boundary Oracle. Obound

is an oracle that, given
function boundaries

set of

Start Oracle. Ostart is an oracle that, given B, returns the

B,
{(s1,e1), (s2,e2), . . . , (sk,ek)}.
set of function start addresses {s1,s2, . . . ,sk}.

These oracles are successively less powerful. For ex-
ample, implementing a boundary oracle Obound from a
function oracle Ofunc requires simply taking the minimum
and maximum element of each Fi. Similarly, a start oracle
Ostart can be implemented from either Ofunc or Obound by
ﬁnding the minimum element of each Fi.

We do not restrict ourselves to a speciﬁc oracle imple-
mentation, as realizable oracles may vary across operating
system and compiler. For example, the boundary oracle
can be implemented by retaining debug information for
Windows or Linux binaries. The function oracle can be
implemented by instrumenting a compiler to output a
list of instruction addresses included in each compiled
function.
3.2 Problem Deﬁnition
With the above deﬁnitions, we are now ready to state
our problem deﬁnitions. We start with the least powerful
identiﬁcation (function start) and build up to the most
difﬁcult one (entire function).
Deﬁnition 3.1. The Function Start Identiﬁcation (FSI)
problem is to output the complete list of function starts
{s1,s2, . . . ,sk} given a binary B compiled from a source
with k functions.

Suppose there is an algorithm AFSI(B) for the FSI prob-

lem which outputs S = {s1,s2, . . . ,s k}. Then:

• The set of true positives, TP, is S∩ Ostart(B).
• The set of false positives, FP, is S− Ostart(B).
• The set of false negatives, FN, is Ostart(B)− S.
We also deﬁne precision and recall. Roughly speak-
ing, precision reﬂects the number of times an identiﬁed
function start is really a function start. A high precision
means that most identiﬁed functions are indeed functions,
whereas a low precision means that some sequences are
incorrectly identiﬁed as functions. Recall is the mea-
surement describing how many functions were identiﬁed
within a binary. A high recall means an algorithm detected
most functions, whereas a low recall means most func-
tions were missed. Mathematically, they can be expressed
as

Precision =

|TP|

|TP| +|FP|

848  23rd USENIX Security Symposium 

USENIX Association

and

Recall =

|TP|

.

|TP| +|FN|

A more difﬁcult problem is to identify both the start

and end addresses for a function:

Deﬁnition 3.2. The Function Boundary Identiﬁcation
(FBI) problem is to identify the start and end bytes
(si,ei) for each function i
i.e., S =
{(s1,e1), (s2,e2), . . . ,(s k,ek)}, given a binary B compiled
from a source with k identiﬁed functions.

in a binary,

Suppose there is an algorithm AFBI(B) for the FBI prob-
lem which outputs S = {(s1,e1), (s2,e2), . . . ,(s k,ek)}. We
then deﬁne true positives, false positives, and false nega-
tives similarly to above with the additional requirement
that both the start and end addresses must match the out-
put of the boundary oracle, i.e., for oracle output (sgt ,egt )
and algorithm output (sA ,eA ), a positive match requires
sgt = sA and egt = eA . A false negative occurs if either
the start or end address is wrong. Precision and recall are
deﬁned analogously to the FSI problem.

Finally, we deﬁne the general function identiﬁcation

problem:

Deﬁnition 3.3. The Function Identiﬁcation (FI) problem
is to output a set {F1,F2, . . . ,Fk} where each Fi is a list
of bytes corresponding to high-level function i given a
binary B with k identiﬁed functions.

We deﬁne true positives, false positives, false negatives,
precision, and recall for the FI problem in the same ways
as FSI and FBI but add the requirement that all bytes of a
function must be matched between agorithm and oracle.
The above problem deﬁnitions form a natural hierarchy,
where function start identiﬁcation is the easiest and full
function identiﬁcation is the most difﬁcult. For exam-
ple, an algorithm AFBI for function boundaries can solve
the function start problem by returning the start element
of each tuple. Similarly, an algorithm for the function
identiﬁcation problem needs only return the maximum
and minimum element to solve the function boundary
identiﬁcation problem.
3.3 Challenges
Identifying functions in binary code is made difﬁcult by
optimizing compilers, which can manipulate functions
in unexpected ways. In this section we highlight several
challenges posed by the behavior of optimizing compilers.
Not every byte belongs to a function. Compilers may
introduce extra instructions for alignment and padding
between or within a function. This means that not ev-
ery instruction or byte must belong to a function. For
example, suppose we have symbol table information for a
binary B. One naive algorithm is to ﬁrst sort symbol-table

1
2
3
4
5
6
7
8
9

<_func1>:
100000e20:
100000e21:
100000e24:
100000e2b:
100000e2c:
100000e31:
100000e38:
<func2>:

push
mov
lea
pop
jmpq
nopl
nopl

%rbp
%rsp,%rbp
0x69(%rip),%rdi
%rbp
100000e5e <_puts$stub>
0x0(%rax)
0x0(%rax,%rax,1)

Figure 2: Unreachable function example: source code
and assembly.

entries by address, and then ascribe each byte between en-
try fi and fi+1 as belonging to function fi. This algorithm
has appeared in several binary analysis platforms used in
security research, such as versions of BAP [3] and Bit-
Blaze [6]. This heuristic is ﬂawed, however. For example,
in Figure 2 lines 7–8 are not owned by any function.

Functions may be non-contiguous. Functions may
have gaps. The gaps can be jump tables, data, or
even instructions for completely different functions [26].
As noted by Harris and Miller [19], function sharing
code can also lead to non-contiguous functions. Fig-
ure 3 shows code that starts out with the function
ConvertDefaultLocale. Midway through the function
at lines 17–21, however, the compiler decided to include
a few lines of code for FindNextFileW as an optimiza-
tion. Many binary analysis platforms, such as BAP [3]
and BitBlaze [6], are not able to handle non-contiguous
functions.

Functions may not be reachable. A function may be
dead code and never called, but nonetheless appear in
the binary. Recognizing such functions is still important
in many security scenarios. For example, suppose two
malware samples both contain a unique, identifying, yet
uncalled function. Then the two malware samples are
likely related even though the function is never called.
One consequence of this is that techniques based solely
on recursive disassembling from program start are not
well-suited to solve the function identiﬁcation problem. A
recursive disassembler only disassembles bytes that occur
along some control ﬂow path, and thus by deﬁnition will
miss functions that are not called.

Unreachability may occur for several reasons, includ-
ing compiler optimizations. For example, Figure 4 shows
a function for computing factorials called fac. When
compiled by gcc -O3, the result of the call to fac is pre-
computed and inlined. Although the code of fac appears,
it is never called in the binary code.

Security policies such as CFI and XFI must be aware
of all low-level functions, not just those in the original
code.

USENIX Association  

23rd USENIX Security Symposium  849

<ConvertDefaultLocale>

%edi,%edi
%ebp

mov
push

jz
test
jz
mov
cmp
jz
test
jnz
mov

7c848556
%eax, %eax
7c83965c
$1024,%ecx
%ecx,%eax
7c83965c
$252,%ah
7c838442
%eax,%edx

7c8383ff:
7c838401:
...
7c83840c:
7c838412:
7c838414:
7c83841a:
7c83841f:
7c838421:
7c838427:
7c83842a:
7c83842c:
...
7c838442:
7c838443:
; chunk of different function FindNextFileW
7c838446:
7c838448:
7c83844d:
; end of chunk
...
7c83965c:
7c890661:
...
7c848556:
7c84855b:

GetUserDefaultLCID
7c838442

6
sub_7c80935e

pop
ret

push
call

call
jmp

mov
jmp

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27

%ebp
4

$8,%eax
7c838442

Figure 3: Lines 17–21 show code from FindNextFileW
included in the middle of ConvertDefaultLocale.

Functions may have multiple entries. High-level lan-
guages use functions as an abstraction with a single entry.
When compiled, however, functions may have multiple
entries as a result of specialization. For example, the icc
compiler with -O1 specialized the chown_failure_ok
function in GNU LIBC. As shown in Figure 5, a new
function entry chown_failure_ok. (note the period) is
added for use when invoking chown_failure_ok with
NULL. The compiled binary has both symbol table entries.
Unlike shared code for two functions that were originally
separate, the compiler here has introduced shared code
via multiple entries as an optimization.

Identifying both functions is necessary in many security
scenarios, e.g., CFI needs to identify each function entry
point for safety, and realize that both are possible targets.
More generally, any binary rewriting for protection (e.g.,
memory safety, control safety, etc.) would need to reason
about both entry points.
Functions may be removed. Functions can be re-
moved by function inlining, especially small functions.
Compilers perform function-inlining to reduce function
call overhead and expose more optimization opportuni-
ties. For example, the function utimens_symlink is in-
lined into the function copy_internal when compiled
by gcc with -O2. The source code and assembly code
are shown in Figure 6. Note that function inlining does
not have to be explicitly declared with inline annota-
tion in source code. Many compilers inline functions
by default unless explicitly disabled with options such

as -fno-deault-inline [17]. This indicates that for
those binary analysis techniques which need function in-
formation, even though source code is accessible, a robust
function identiﬁcation technique should still operate on
the program binary. If using source code, function iden-
tiﬁcation may be less precise due to functions that are
inlined during compilation.
Each compilation is different. Binary code is not only
heavily inﬂuenced by the compiler but also the compiler
version and speciﬁc optimizations employed. For exam-
ple, icc does not pre-compute the result of fac in Fig-
ure 4, but gcc does. Even different versions of a compiler
may change code. For example, traditionally gcc (e.g.,
version 3) would only omit the use of the frame pointer
register %ebp when given the -fomit-frame-pointer
option. Recent versions of gcc (such as version 4.2),
however, opportunistically omit the frame pointer when
compiled with -O1 and -O2. As a result several tools that
identiﬁed functions by scanning for push %ebp break.
For example, Dyninst, used for instrumentation in sev-
eral security projects, relies on this heuristic to identify
functions and breaks on recent versions of gcc.
4 BYTEWEIGHT
In this section, we detail the design and algorithms used
by BYTEWEIGHT to solve the function identiﬁcation
problems. We ﬁrst start with the FSI problem, and then
move to the more general function identiﬁcation problem.
We cast FSI as a machine learning classiﬁcation prob-
lem where the goal is to label each byte of a binary as
either a function start or not. We use machine learning
to automatically generate literal patterns so that BYTE-
WEIGHT can handle new compilers and new optimiza-
tions without relying on manually generated patterns or
heuristics. Our algorithm works with both byte sequences
and disassembled instruction sequences.

Our overall system is shown in Figure 7. Like any clas-
siﬁcation problem, we have a training phase followed by
a classiﬁcation phase. During training, we ﬁrst compile a
reference corpus of source code to produce binaries where
the start addresses are known. At a high level, our algo-
rithm creates a weighted preﬁx tree of known function
start byte or instruction sequences. We weight vertices
in the preﬁx tree by computing the ratio of true positives
to the sum of true and false positives for each sequence
in the reference data set. We have designed and imple-
mented two variations of BYTEWEIGHT: one working
with raw bytes and one with normalized disassembled
instructions. Both use the same overall algorithm and
data structures. We show in our evaluation that the nor-
malization approach provides higher precision and recall,
and costs less time (experiment 5.2).

In the classiﬁcation phase, we use the weighted preﬁx
tree to determine whether a given sequence of bytes or

850  23rd USENIX Security Symposium 

USENIX Association

1
2
3
4
5
6
7
8
9
10

1
2
3
4
5
6

int fac(int x)
{

if (x == 1) return 1;
else return x * fac(x - 1);

}

}

void main(int argc, char **argv)
{

printf("%d", fac(10));

1
2
3
4
5
6
7
8
9
10
11

080483f0 <fac>:
...
08048410 <main>:
...
movl
movl
call
xor
add
pop
ret

$0x375f00 ,0x4(%esp)
$0x8048510 ,(%esp)
8048300
%eax,%eax
$0x8,%esp
%ebp

;call printf without fac

(a) Source code

(b) Assembly compiled by gcc -O2

Figure 4: Unreachable code: source code and assembly.

extern bool
chown_failure_ok (struct cp_options const *x)
{

return ((errno == EPERM || errno == EINVAL)

&& !x->chown_privileges);

}

1
2
3
4
5
6
7

<chown_failure_ok>:
804f544:
mov
<chown_failure_ok.>:
push
804f548:
804f549:
push
804f54a:
push
...

0x4(%esp),%eax

%esi
%esi
%esi

(a) Source Code

(b) Assembly compiled by icc -O1

Figure 5: chown_failure_ok is specialized: source code and assembly.

instructions corresponds to a function start. We say that a
sequence corresponds to a function start if the correspond-
ing terminal node in the preﬁx tree has a weight value
larger than the threshold t. In the case where the sequence
exactly matches a path in the preﬁx tree, the terminal node
is the ﬁnal node in this path. If the sequence does not
exactly match a path in the tree, the terminal node is the
last matched node in the sequence.

Once we identify function starts, we infer the remaining
bytes (and instructions) that belong to a function using
a CFG recovery algorithm. The algorithm incrementally
determines the CFG using a variant of VSA [2]. If an
indirect jump depends on the value of a register, then we
over-approximate a solution to the function identiﬁcation
problem by adding edges that correspond to locations
approximated using VSA.
4.1 Learning Phase
The input to the learning phase is a corpus of training
binaries T, and a maximum sequence length � > 0. �
serves as a bound on the maximum tree height.

In BYTEWEIGHT, we ﬁrst generate the oracle Obound
by compiling known source using a variety of optimiza-
tion levels while retaining debug information. The debug
information gives us the required (si,ei) pair for each
function i in the binary.

In this paper, we consider two possibilities: learning
over raw bytes and learning over normalized instructions.
We refer to both raw bytes and instructions as a sequence
of elements. The sequence length � determines how many

raw sequential bytes or instructions we consider for train-
ing.
Step 1: Extract ﬁrst � elements for each function (Ex-
traction).
In the ﬁrst step, we iterate over all (si,ei)
pairs and extract the ﬁrst � elements. If there are fewer
than � elements in the function, we extract the maximum
number of elements. For raw bytes, this is B[s : s + �]
bytes, and for instructions, it is the ﬁrst � instructions
disassembled linearly starting from B[s].
Step 2: Generate a preﬁx tree (Tree Generation).
In
step 2, we generate a preﬁx tree from the extracted se-
quences to represent all possible function start sequences
up to � elements.

A preﬁx tree, also called a trie, is a data structure en-
abling efﬁcient information retrieval. In the tree, each
non-root node has an associated byte or instruction. The
sequence for a node n is represented by the elements that
appear on the path from the root to n. Note that the tree
represents all strings up to � elements, not just exactly �
elements.

Figure 8a shows an example tree on instructions,
where node callq 0x43a28 represents the instruction
sequence:

%ebp
%esp,%ebp
0x43a28

;saved stack pointer
;establish new frame
;call another function

push
mov
callq
If the sequence is over bytes, the preﬁx tree is calcu-
lated directly, although our experiments indicate that a
preﬁx tree calculated over normalized instructions fairs

USENIX Association  

23rd USENIX Security Symposium  851

static inline int
utimens_symlink (char const *file,

struct timespec const *timespec)

int err = lutimens (file, timespec);
if (err && errno == ENOSYS)

err = 0;
return err;

{

}

{

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23

static bool
copy_internal (char const *src_name,
char const *dst_name,
...)

...
if ((dest_is_symlink

?utimens_symlink (dst_name,
timespec)

:utimens (dst_name, timespec))
!= 0)

...

}

1
2
3
4
5
6
7
8
9
10
11
12
13
14

<_copy_internal>:
100003170:
100003171:
100003174:
100003176:
...
10000468c:
10000468f:
100004695:
10000469c:
1000046a3:
1000046a8:
1000046aa:
...

push
mov
push
push

test
je
lea
mov
callq
test
mov

%rbp
%rsp,%rbp
%r15
%r14

%r14b,%r14b
100005bfd
-0x738(%rbp),%rsi
-0x750(%rbp),%rdi
10000d020 <_lutimens>
%eax,%eax
%eax,%ebx

(a) Source Code

(b) Assembly compiled by gcc -O2

Figure 6: Example of function being removed due to function inlining optimization.

Testing(cid:1)
Binary(cid:1)

Extraction &(cid:1)
Tree Generation(cid:1)

Weight 
Weight 
Calculation
Calculation(cid:1)

CFG Recovery(cid:1)

Training 
Binaries(cid:1)

Prex 
Tree(cid:1)

Training(cid:1)

Weighted 
Prex Tree(cid:1)

Function 
Start(cid:1)

Function 
Bytes(cid:1)

Function 
Boundary(cid:1)

Classication(cid:1)

RFCR(cid:1)

Function 

Identication(cid:1)

Function Boundary 

Identication(cid:1)

Figure 7: The BYTEWEIGHT function boundary inference approach.

better. We perform two types of normalization: imme-
diate number normalization and call & jump instruction
normalization. As shown in Table 1, normalization takes
an instruction as input and generalizes it so that it can
match against very similar, but not identical instructions.
These two types of normalization help us improve recall
at the cost of a little precision (Table 2). In our running
example, only the function assign is recognized as a
function start when matched against the unnormalized
preﬁx tree (Figure 8a), while functions assign, sub, and
sum can all be recognized when matched against the nor-
malized preﬁx tree (Figure 8b).

Step 3: Calculate tree weights (Weight Calculation).
The preﬁx tree represents possible function start se-
quences up to (cid:31) elements. For each node, we assign a
weight that represents the likelihood that the sequence

corresponding to the path from the root node to this node
is a function start in the training set. For example, ac-
cording to Figure 8, the weight of node push %ebp is
0.1445, which means that during training, 14.45% of all
sequences with preﬁx of push %ebp were truly function
starts, while 85.55% were not.

To calculate the weight, we ﬁrst count the number of
occurrences T+ in which each preﬁx in the tree matches
a true function start with respect to the ground truth Ostart
for the entire training set T.

Second, we lower the weight of a preﬁx if it occurs
in a binary, but is not a function start. We do this by
performing an exhaustive disassembly starting from every
address that is not a function start [23]. We match each
exhaustive disassembly sequence of (cid:31) elements against
the tree. We call these false matches. The number of false
matches T− is the number of times a preﬁx represented

852  23rd USENIX Security Symposium 

USENIX Association

0.0000(cid:1)
Ø (cid:1)

0.0000(cid:1)
Ø (cid:1)

0.1445(cid:1)
push %ebp(cid:1)

0.9883(cid:1)

mov %esp,%ebp(cid:1)

0.8459(cid:1)

0.9694(cid:1)

…(cid:1)
0.0159(cid:1)

callq 0x43a28 (cid:1)

0.0320(cid:1)

callq 0x401320 (cid:1)

0.9728(cid:1)

sub $0x20,%rsp (cid:1)

0.9419(cid:1)

mov %rbx,-0x10(%rsp) (cid:1)

mov %rbp,-0x8(%rsp) (cid:1)

mov %rsi,%rbx (cid:1)

…(cid:1)

(a) Unnormalized

0.1445(cid:1)
push %ebp(cid:1)

0.9883(cid:1)

mov %esp,%ebp(cid:1)

0.8459(cid:1)

mov %rbx,-0x[1-9a-
f][0-9a-f]*\(%rsp\) (cid:1)

0.9694(cid:1)

mov %rbp,-0x[1-9a-
f][0-9a-f]*\(%rsp\) (cid:1)

(b) Normalized

call[q]* +0x[0-9a-

…(cid:1)
0.0219(cid:1)

f]*(cid:1)
0.9728(cid:1)

sub (?<! -)0x[1-9a-
f][0-9a-f]*,%rsp (cid:1)

0.9419(cid:1)

mov %rsi,%rbx (cid:1)

…(cid:1)

Figure 8: Example of unnormalized (a) and normalized (b) preﬁx tree. Weight is shown above its corresponding node.

in the tree is not a function start in the training set T. The
weight for each node n is then the ratio of true positives
to overall matches

Wn =

T+

T+ + T−

.

(1)

Since the preﬁx tree can end up being quite large, it
is beneﬁcial to prune the tree of unnecessary nodes. For
each node in the tree, we remove all its child nodes if
the value of T− for this node is 0. For any child node,
the value of T− is never negative and never larger than
the value of T− for the parent node. Hence, if T− is 0
for a parent node, then the value must be 0 for all of the
child nodes as well. The intuition here is that if a child
node matches a sequence that is not a function start, then
so must the parent node. Thus, if the parent node does
not have any false matches, then neither can a child node.
Based on Equation 1, if T− = 0 and T+ > 0, then the
weight of the node is 1. Since the child nodes of such a
node also have a T− value of 0 and are not included in
the tree if T+ = 0, they must also have a weight of 1. As
discussed more in Section 4.2, child nodes with identical
weights are redundant and can safely be removed without
affecting classiﬁcation.

This pruning optimization helps us greatly reduce the
space needed by the tree. For example, pruning reduced

the number of nodes in the preﬁx tree from 2,483 to
1,447 for our Windows x86 dataset. Moreover, pruning
increases the speed of matching, since we can determine
the weight of test sequences after traversing fewer nodes
in the tree.
4.2 Classiﬁcation Phase Using a Weighted Preﬁx

Tree

The output of the learning phase is a weighted preﬁx tree
(e.g., Figure 8). The input to the classiﬁcation step is a
binary B, the weighted preﬁx tree, and a weight threshold
t.

To classify instructions, we perform exhaustive disas-
sembly of the input binary B and match against the tree.
Matching is done by tokenizing the disassembled stream,
performing normalization as done during learning, and
walking the tree. To classify bytes rather than instructions,
we again start at every offset but instead match the raw
bytes instead of normalized instructions.

The weight of a sequence is determined by last match-
ing node (the terminal node) during the walk. For exam-
ple, given the tree in Figure 8a, and our running example
with sequences

mov
mov
sub

%rbx,-0x10(%rsp)
%rbp,-0x8(%rsp)
%0x28,%rsp

USENIX Association  

23rd USENIX Security Symposium  853

Type

all

zero

Immediate

positive

negative

npz

Call & Jump

Unnormalized Signature
mov $0xaa,%eax
mov %gs:0x0,%eax
mov 0x80502c0,%eax
mov $0xaa,%eax
mov %gs:0x0,%eax
mov 0x80502c0,%eax
mov $0xaa,%eax
mov %gs:0x0,%eax
mov 0x80502c0,%eax
mov $0xaa,%eax
mov %gs:0x0,%eax
mov 0x80502c0,%eax
movzwl -0x6c(%ebp),%eax
mov $0xaa,%eax
mov %gs:0x0,%eax
mov 0x80502c0,%eax
movzwl -0x6c(%ebp),%eax
call 0x804cf32

Normalized Signature

mov \$-*0x[0-9a-f]+,%eax
mov %gs:-*0x[0-9a-f]+,%eax
mov -*0x[0-9a-f]+,%eax
mov \$-*0x[1-9a-f][0-9a-f]*,%eax
mov %gs:0x0+,%eax
mov -*0x[1-9a-f][0-9a-f]*,%eax
mov \$(?<! -)0x[1-9a-f][0-9a-f]*,%eax
mov %gs:-0x[0-9a-f]+|0x0+,%eax
mov (?<! -)0x[1-9a-f][0-9a-f]*,%eax
mov \$(?<! -)0x[0-9a-f]+,%eax
mov %gs:(?<! -)0x[0-9a-f]+,%eax
mov (?<! -)0x[0-9a-f]+,%eax
movzl -0x[1-9a-f][0-9a-f]*\(%ebp\),%eax
mov \$(?<! -)0x[1-9a-f][0-9a-f]*,%eax
mov %gs:0x0+,%eax
mov (?<! -)0x[1-9a-f][0-9a-f]*,%eax
movzl -0x[1-9a-f][0-9a-f]*\(%ebp\),%eax
call[q]* +0x[0-9a-f]*

For immediate normalization, we generalize immediate operands. There are ﬁve kinds of generalization: all, zero,
positive, negative, and npz. For jump and call instruction normalization, we generalize callee and jump addresses.

Table 1: Normalizations in signature.

the matching node will be mov%rbp,-0x8(%rsp), giving
a weight of 0.9694. However, for another sequence

push
and

%ebp
$0x2,%esp

we would have weight 0.1445. We say the sequence is the
beginning of a function if the output weight w is not less
than the threshold t.
4.3 The Function Identiﬁcation Problem
At a high level, we address the function identiﬁcation
problem by ﬁrst determining the start addresses for func-
tions, and then performing static analysis to recover the
CFG of instructions that are reachable from the start. Di-
rect control transfers (e.g., direct jumps and calls) are
followed using recursive disassembly. Indirect control
transfers, e.g., from indirect calls or jump tables, are enu-
merated using VSA [2]. The ﬁnal CFG then represents
all instructions (and corresponding bytes) that are owned
by the function starting at the given address.

CFG recovery starts at a given address and recursively
ﬁnds new nodes that are connected to found nodes. The
process ends when no more vertices are added into graph.
Starting at the addresses classiﬁed for FSI, CFG recovery
recursively adds instructions that are reachable from these
starts. A ﬁrst-in-ﬁrst-out vertex array is maintained during
CFG recovery.

At the beginning, there is only one element – the start
address in the array. In each round, we process the ﬁrst

element by exploring new reachable instructions. If the
new instruction is not in the array, it will be appended
to the end. Elements in the array are handled accord-
ingly until all elements have been processed and no more
instructions are added.

If

the instruction being processed is a branch
mnemonic, the reachable instruction is the branch ref-
erence. If it is a call mnemonic, the reachable instructions
include both the call reference and the instruction directly
following the call instruction. If it is an exit instruction,
there will be no new instruction. For the rest of mnemon-
ics, the new instruction is the next one by address. We
handle indirect control transfer instruction by VSA: we
infer a set that over-approximates the destination of the
indirect jump and thus over-approximate the function
identiﬁcation problem.

Note that functions can exit by calling a no-return func-
tion such as exit. This means that some call instructions
in fact never return. To detect these instances, we check
the call reference to see if it represents a known no-return
function such as abort or exit.
4.4 Recursive Function Call Resolution
Pattern matching can miss functions; for example, a func-
tion that is written directly in assembly may not obey
calling conventions. To catch these kinds of missed func-
tions, we continue to supplement the function start list
during CFG recovery. If a call instruction has its callee in

854  23rd USENIX Security Symposium 

USENIX Association

the .text section, we consider the callee to be a function
start. We then do CFG recovery again, starting at the
new function start until there are no more functions added
into the function start list. We will refer to this strategy
as recursive function call resolution (RFCR). In §5.3, we
discuss the effectiveness of this technique in function start
identiﬁcation.

4.5 Addressing Challenges
In this section, we describe how BYTEWEIGHT addresses
the challenges raised in §3.3.

First, BYTEWEIGHT recovers functions that are un-
reachable via calls because it does not depend on calls to
identify functions. In particular, BYTEWEIGHT recovers
any function start that matches the learned weighted pre-
ﬁx tree as described above. Similarly, our approach will
also learn functions that have multiple entries, provided
a similar specialization occurs in the training set. This
seems realistic in many scenarios since the number of
compiler optimizations that create multiple entry func-
tions are relatively few and can be enumerated during
training.

BYTEWEIGHT also deals with overlapping byte or in-
struction sequences provided that there is a unique start
address. Consider two functions that start at different
addresses, but contain the same bytes. During CFG recov-
ery, BYTEWEIGHT will discover that both functions use
the same bytes, and attribute the bytes to both functions.
BYTEWEIGHT can successfully avoid false identiﬁcation
for inlined functions when inlined function does not be-
have like an empirical function start (does not weighted
over threshold in training).

Finally, note that BYTEWEIGHT does not need to at-
tribute every byte or instruction to a function. In particular,
only bytes (or instructions) that are reachable from the
recovered function entries will be owned by a function in
the ﬁnal output.

5 Evaluation
In this section, we discuss our experiments and perfor-
mance. BYTEWEIGHT is a cross-platform tool which can
be run on both Linux and Windows. We used BAP [3] to
construct CFGs. The rest of the implementation consists
of 1988 lines of OCaml code and 222 lines of shell code.
We set up BYTEWEIGHT on one desktop machine with a
quad-core 3.5GHz i7-3770K CPU and 16GB RAM. Our
experiments aimed to address three questions:

1. Does BYTEWEIGHT’s pattern matching model per-
form better than known models for function start
identiﬁcation? (§5.2)

2. Does BYTEWEIGHT perform function start identi-
ﬁcation better than existing binary analysis tools?
(§5.3)

3. Does BYTEWEIGHT perform function boundary
identiﬁcation better than existing binary analysis
tools? (§5.4)

In this section, we ﬁrst describe our data set and ground
truth (the oracle), then describe the results of our exper-
iments. We performed three experiments answering the
above three questions. In each experiment, we compared
BYTEWEIGHT against existing tools in terms of both
accuracy and speed.

Because BYTEWEIGHT needs training, we divided the
data into training and testing sets. We used standard 10-
fold validation, dividing the element set into 10 sub-sets,
applying 1 of the 10 on testing, and using the remaining
9 for training. The overall precision and recall represent
the average of each test.
5.1 Data Set and Ground Truth
Our data set consisted of 2,200 different binaries compiled
with four variables:
Operating System. Our evaluation used both Linux and

Windows binaries.

Instruction Set Architecture (ISA). Our binaries con-
sisted of both x86 and x86-64 binaries. One reason
for varying the ISA is that the calling convention is
different, e.g., parameters are passed by default on
the stack in Linux on x86, but in registers on x86-64.
Compiler. We used GNU gcc, Intel icc, and Microsoft

VS.

Optimization Level. We experimented with the four op-
timization levels from no optimization to full opti-
mization.

On Linux, our data set consisted of 2,064 binaries in
total. The data set contained programs from coreutils,
binutils, and findutils compiled with both gcc
4.7.2 and icc 14.0.1. On Windows, we used VS 2010, VS
2012, and VS 2013 (depending on the requirements of the
program) to compile 68 binaries for x86 and x86-64 each.
These binaries came from popular open-source projects:
putty, 7zip, vim, libsodium, libetpan, HID API, and pbc (a
library for protocol buffers). Note that because Microsoft
Symbol Server releases only public symbols which do
not contain information of private functions, we were un-
able to use Microsoft Symbol Server for ground truth and
include Windows system applications in our experiment.
We obtained ground truth for function boundaries from
the symbol table and PDB ﬁle for Linux and Windows
binaries, respectively. We used objdump to parse symbol
tables, and Dia2dump [13] to parse PDB ﬁles. Addi-
tionally, we extracted “thunk” addresses from PDB ﬁles.
While most tools do not take thunks into account, IDA
considers thunks in Windows binaries to be special func-
tions. To get a fair result, we ﬁltered out thunks from
IDA’s output using the list of thunks extracted from PDB
ﬁles.

USENIX Association  

23rd USENIX Security Symposium  855

5.2 Signature Matching Model
Our ﬁrst experiment evaluated the signature matching
model for function start identiﬁcation. We compared
BYTEWEIGHT and Rosenblum et al.’s implementation in
terms of both accuracy and speed. In order to equally eval-
uate the signature matching models, recursive function
call resolution was not used in this experiment.

The implementation of Rosenblum et al. is available
as a matching tool with 12 hard-coded signatures for gcc
and 41 hard-coded signatures for icc. Their learning
code was not available, nor was their dataset. Although
they evaluated VS in their paper, the version of the im-
plementation that we had did not support VS and was
limited to x86. Each signature has a weight, which is also
hard-coded. After calculating the probability for each
sequence match, it uses a threshold of 0.5 to ﬁlter out
function starts. Taking a binary and a compiler name
(gcc or icc), it generates a list of function start addresses.
To adapt to their requirements, we divide Linux x86 bi-
naries into two groups by compiler, where each group
consists of 516 binaries. We did 10-fold cross valida-
tion for BYTEWEIGHT, and use the same threshold as
Rosenblum et al.’s implementation.

We also evaluated another two varieties of our model:
one without normalization, and one with a maximum
tree height of 3, which is same as the model used by
Rosenblum et al. and BYTEWEIGHT (3), respectively.

Table 2 shows precision, recall, and runtime for each
compiler and each function start identiﬁcation model.
From the table we can see that Rosenblum et al.’s imple-
mentation had an accuracy below 70%, while both BYTE-
WEIGHT-series models achieved an accuracy of more
than 85%. Note that BYTEWEIGHT with 10-length and
normalized signatures (the last row in table) performed
particularly well, with an accuracy of approximately 97%,
a more than 35% improvement over Rosenblum et al.’s
implementation.

Table 2 also details the accuracy and performance dif-
ferences among BYTEWEIGHT with different conﬁgura-
tions. Comparing against the full conﬁguration model
(BYTEWEIGHT), the model with a smaller maximum sig-
nature length (BYTEWEIGHT (3)) performs slightly faster
(3% improvement), but sacriﬁces 7% in accuracy. The
model without signature normalization (BYTEWEIGHT
(no-norm)) has only 1% higher precision but 6.68% lower
recall, and the testing time is ten times longer than that of
the normalized model.
5.3 Function Start Identiﬁcation
The second experiment evaluated our full function start
identiﬁcation against existing static analysis tools. We
compared BYTEWEIGHT (no-RFCR)—a version without
recursive function call resolution, BYTEWEIGHT, and the
following tools:

IDA. We used IDA 6.5, build 140116 along with the
default FLIRT signatures. All function identiﬁcation
options were enabled.

BAP. We

used BAP 0.7, which

provides

a
invoked

can be

get_function utility that
directly.

Dyninst. Dyninst offers the tool unstrip [31] to identify

functions in binaries without debug information.

Naive Method. This matched simple 0x55 (push %ebp
or push %rbp) and 0xc3 (ret or retq) signatures
only.

We divided our data set into four categories: ELF x86,
ELF x86-64, PE x86, and PE x86-64. Unlike the previous
experiment, binaries from various compilers but the same
target were grouped together. Overall, we had 1032 ELF
x86 and ELF x86-64 binaries, and 68 PE x86 and PE x86-
64 binaries. We evaluated these categories separately, and
again applied 10-fold validation. During testing, we used
the same score threshold t = 0.5 as in the ﬁrst experiment.
Note that not every tool in our experiment supports all
binary targets. For example, Dyninst does not support
ELF x86-64, PE x86, or PE x86-64 binaries. We use “-”
to indicate when the target is not supported by the tool.
Also, we omitted 3 failures in BYTEWEIGHT, and 10
failures in Dyninst during this experiment. Due to a bug
in BAP, BYTEWEIGHT failed in 3 icc compiled ELF
x86-64 binaries: ranlib with -O3, ld_new with -O2, and
ld_new with -O3. Dyninst failed in 8 icc compiled ELF
x86-64 binaries and 2 gcc compiled ELF x86-64 binaries.
The results of our experiment are shown in Table 3.

As evident in Table 3, BYTEWEIGHT achieved a higher
precision and recall than BYTEWEIGHT without recursive
function call resolution. BYTEWEIGHT performed above
96% in Linux, while all other tools all performed below
90%. In Windows, we have comparable performance to
IDA in terms of precision, but improved results in terms
of recall.

Interestingly, we found that the naive method was not
able to identify any functions in PE x86-64. This is mainly
because VS does not use push %rbp to begin a function;
instead, it uses move instructions.
5.4 Function Boundary Identiﬁcation
The third experiment evaluated our function boundary
identiﬁcation against existing static analysis tools. As in
the last experiment, we compared BYTEWEIGHT, BYTE-
WEIGHT (no-RFCR), IDA, BAP, and Dyninst, classiﬁed
binaries by their target, and applied 10-fold validation
on each of the classes. The results of our experiment are
shown in Table 4.

Our tool performed the best in Linux, and was compa-
rable to IDA in Windows. In particular, for Linux binaries,
BYTEWEIGHT and BYTEWEIGHT (no-RFCR) have both
precision and recall above 90%, while IDA is below 73%.

856  23rd USENIX Security Symposium 

USENIX Association

Rosenblum et al.
BYTEWEIGHT (3)

BYTEWEIGHT (no-norm)

BYTEWEIGHT

Precision
0.4909
0.9103
0.9877
0.9726

GCC
Recall
0.4312
0.8711
0.9302
0.9599

Time(sec)
1172.41
1417.51
19994.18
1468.75

Precision
0.6080
0.8948
0.9727
0.9725

ICC
Recall
0.6749
0.8592
0.9132
0.9800

Time(sec)
2178.14
1905.34
20894.45
1927.90

Table 2: Precision/Recall of different pattern matching models for function start identiﬁcation.

Naive
Dyninst

BAP
IDA

BYTEWEIGHT (no-RFCR)

BYTEWEIGHT

ELF x86

0.4217/0.3089
0.8877/0.5159
0.8910/0.8003
0.7097/0.5834
0.9836/0.9617
0.9841/0.9794

ELF x86-64
0.2606/0.2506

PE x86

PE x86-64

0.6413/0.4999

0.0000/0.0000

−
−

0.7420/0.5550
0.9911/0.9757
0.9914/0.9847

−

0.3912/0.0795
0.9467/0.8780
0.9675/0.9213
0.9378/0.9537

−
−

0.9822/0.9334
0.9774/0.9622
0.9788/0.9798

Table 3: Precision/Recall for different function start identiﬁcation tools.

For Windows binaries, IDA achieves better results than
BYTEWEIGHT with x86-64 binaries, but is slightly worse
for x86 binaries.
5.5 Performance
Training. We compare BYTEWEIGHT against Rosen-
blum et al.’s work in terms of time required for training.
Since we do not have access to either their training code
or their training data, we instead compare the results
based on the performance reported in paper. There are
two main steps in Rosenblum et al.’s work. First, they
conduct feature selection to determine the most informa-
tive idioms – patterns that either immediately precede
a function start, or immediately follow a function start.
Second, they train parameters of these idioms using a
logistic regression model. While they did not provide the
time for parameter learning, they did describe that feature
selection required 150 compute days for 1,171 binaries.
Our tool, however, spent only 586.44 compute hours to
train on 2,064 binaries, including overhead required to
setup cross-validation.
Testing. We list the performance of BYTEWEIGHT,
IDA, BAP, and Dyninst for testing. As described in sec-
tion 4, BYTEWEIGHT has three steps in testing: function
start identiﬁcation by pattern matching, function boundary
identiﬁcation by CFG and VSA, and recursive function
call resolution (RFCR). We report our time performance
separately, as shown in Table 5.

IDA is clearly the fastest tool for PE ﬁles. For ELF
binaries, it takes a similar amount of time to use IDA and
BYTEWEIGHT to identify function starts, however our
measured times for IDA also include the time required

to run other automatic analyses. BAP and Dyninst have
better performance on ELF x86 binaries, mainly because
they match fewer patterns than BYTEWEIGHT and do
not normalize instructions. This table also shows that
function boundary identiﬁcation and recursive function
call resolution are expensive to compute. This is mainly
because we use VSA to resolve indirect calls during CFG
recovery, which costs more than typical CFG recovery by
recursive disassembly. Thus while BYTEWEIGHT with
RFCR enabled has improved recall, it is also considerably
slower.
6 Discussion
Recall that our tool considers a sequence of bytes or in-
structions to be a function start if the weight of the cor-
responding terminal node in the learned preﬁx tree is
greater than 0.5. The choice to use 0.5 as the threshold
was largely dictated by Rosenblum et al., who also used
0.5 as a threshold in their implementation. While this
appears to be a good choice for achieving high precision
and recall in our system, it is not necessarily the optimal
value. In the future, we plan to experiment with differ-
ent thresholds to better understand how this affects the
accuracy of BYTEWEIGHT.

While there are similarities betwen Rosenblum et al.’s
approach and ours, there are also several key differences
that are worth highlighting:

• Rosenblum et al. considered sequences of bytes or
instructions immediately preceding functions, called
preﬁx idioms, as well the entry idioms that start a
function. Our present model does not include preﬁx
idioms. Rosenblumet al.’s experiments show preﬁx

USENIX Association  

23rd USENIX Security Symposium  857

Naive
Dyninst

BAP
IDA

BYTEWEIGHT (no-RFCR)

BYTEWEIGHT

ELF x86

0.4127/0.3013
0.8737/0.5071
0.6038/0.6300
0.7063/0.5653
0.9285/0.9058
0.9278/0.9229

ELF x86-64
0.2472/0.2429

PE x86

PE x86-64

0.5880/0.4701

0.0000/0.0000

−
−

0.7284/0.5346
0.9317/0.9159
0.9322/0.9252

−

0.1003/0.0219
0.9393/0.8710
0.9503/0.9048
0.9230/0.9391

−
−

0.9811/0.9324
0.9287/0.9135
0.9304/0.9313

Table 4: Precision/Recall for different function boundary identiﬁcation tools.

Dyninst

BAP
IDA*

BYTEWEIGHT-Function Start

BYTEWEIGHT-Function Boundary

BYTEWEIGHT-RFCR

ELF x86
2566.90
1928.40
5157.85
3296.98
367018.53
457997.09

ELF x86-64

PE x86

PE x86-64

−
−
5705.13
5718.84
412223.55
593169.73

−

3849.27
318.27
10269.19
54482.30
84602.56

−
−
371.59
11904.06
87661.01
97627.44

* For IDA, performance represents the total time needed to complete disassembly and auto-analysis.

Table 5: Performance for different function identiﬁcation tools (in seconds).

idioms increase accuracy in their model. In the fu-
ture, we plan to investigate whether adding preﬁx
matching to our model can increase its accuracy as
well.

• Rosenblum et al.’s idioms are limited to at most
4 instructions [27, p. 800] due to scalability issues
with forward feature selection. With our preﬁx tree
model, we can comfortably handle longer instruction
sequences. At present, we settle on a length of 10.
In the future, we plan to optimize the length to strike
a balance between training speed and recognition
accuracy.

• Rosenblum et al.’s CRF model considers both posi-
tive and negative features. For example, their algo-
rithm is designed to avoid identifying two function
starts where the second function begins within the
ﬁrst instruction of the ﬁrst function (the so-called
“overlapping disassembly”). Although we consider
both positive and negative features as well, in con-
strast the above outcome is feasible with our algo-
rithm.

While our technique is not compiler-speciﬁc, it is based
on supervised learning. As such, obtaining representative
training data is key to achieving good results with BYTE-
WEIGHT. Since compilers and optimizations do change
over time, BYTEWEIGHT may need to be retrained in
order to accurately identify functions in this new environ-
ment. Of course, the need for retraining is a common re-
quirement for every system based on supervised learning.
This is applicable to both BYTEWEIGHT and Rosenblum

et al.’s work, and underscores the importance of having a
computationally efﬁcient training phase.

Despite our tool’s success, there is still room for im-
provement. As shown in Section 5, over 80% of BYTE-
WEIGHT failures are due to the misclassiﬁcation of the
end instruction for a function, among which more than
half are functions that do not return and functions that
call such no-return functions. To mitigate this, we could
backward propagate information about functions that do
not return to the functions that call them. For example, if
function f always calls function g, and g is identiﬁed as
a no-return function, then f should also be considered a
no-return function. We could also use other abstract do-
mains along with the strided intervals of VSA to increase
the precision of our indirect jump analysis [2], which can
in turn help us identify more functions more accurately.
One other scenario where BYTEWEIGHT currently
struggles is with Windows binaries compiled with hot
patching enabled. With such binaries, functions will start
with an extra mov %edi,%edi instruction, which is ef-
fectively a 2-byte nop. A training set that includes bina-
ries with hot patching can reduce the accuracy of BYTE-
WEIGHT. Because the extra instruction mov %edi,%edi
is treated as the function start in binaries with hot patch-
ing, any subsequent instructions are treated as false
matches. Thus, any sequence of instructions that would
normally constitute a function start but now follows a
mov %edi,%edi is considered to be a false match. Con-
sider a hypothetical dataset where all functions start with
push %ebp; mov %esp,%ebp, but half of the binaries

858  23rd USENIX Security Symposium 

USENIX Association

are compiled with hot patching and thus start functions
with an extra mov %edi,%edi. Half of the time, the se-
quence push %ebp; mov %esp,%ebp will be treated as
a function start, but in the other half it will not be treated
as such, thus leaving the sequence with a weight of 0.5
in our preﬁx tree. In order to deal with this compiler
peculiarity, we would need give special consideration to
mov %edi,%edi, treating both this instruction and the
instruction following it as a function start for the sake of
training.

Although training BYTEWEIGHT for function start
identiﬁcation is relatively fast, training for function bound-
ary identiﬁcation is still quite slow. Proﬁling reveals that
most of the time is spent building CFGs, and in particular
resolving indirect jumps using VSA. In future work, we
plan to explore alternative approaches that avoid VSA
altogether.

Finally, obfuscated or malicious binaries which inten-
tionally obscure function start information are out of
scope of this paper.
7 Related Work
In addition to the already discussed Rosenblum et al. [27],
there are a variety of existing binary analysis platforms
tackle the binary identiﬁcation problem. BitBlaze [6]
assumes debug information. If no debug information is
present, it treats the entire section as one function. Bit-
Blaze also provides an interface for incorporating Hex
Rays function identiﬁcation information.

Dyninst [19] also offers tools, such as unstrip [31],
to identify functions in binaries without debug informa-
tion. Within the Dyninst framework, potential functions
in the .text section are identiﬁed using the hex pattern
0x55 representing push %ebp. First, Dyninst will start at
the entry address and traverse inter- and intra-procedural
control ﬂow. The algorithm will scan the gaps between
functions and check if push %ebp is present. This does
not preform well across different optimizations and oper-
ating systems.

IDA using proprietary heuristics and FLIRT [16] tech-
nique attempts to help security researchers recover pro-
cedural abstractions. However, updating the signature
database requires an amount of manual effort that does
not scale.
In addition, because FLIRT uses a pattern
matching algorithm to search for signatures, small varia-
tions in libraries such as different compiler optimizations
or the use of different compiler versions, prevent FLIRT
from recognizing important functions in a disassembled
program. The Binary Analysis Platform (BAP) also at-
tempts to provide a reliable identiﬁcation of functions
using custom-written signatures [8].

Kruegel et al. perform exhaustive disassembly, then
use unigram and bigram instruction models, along with
patterns, to identify functions [23]. Jakstab uses two pre-

deﬁned patterns to identify functions for x86 code [22,
§6.2].
8 Conclusion
In this paper, we introduce BYTEWEIGHT, a system for
automatically learning to identify functions in stripped
binaries. In our evaluation, we show on a test suite of
2,200 binaries that BYTEWEIGHT outperforms previous
work across two operating systems, two compilers, and
four different optimizations. In particular, BYTEWEIGHT
misses only 44,621 functions in comparison with the
266,672 functions missed by the industry-leading tool
IDA. Furthermore, while IDA misidentiﬁes 459,247 func-
tions, BYTEWEIGHT misidentiﬁes only 43,992 functions.
To seed future improvements to the function identiﬁcation
problem, we are making our tools and dataset available in
support of open science.
Acknowledgments
This material is based upon work supported by DARPA
under Contract No. HR00111220009. Any opinions, ﬁnd-
ings and conclusions or recommendations expressed in
this material are those of the author(s) and do not neces-
sarily reﬂect the views of DARPA.
References
[1] ABADI, M., BUDIU, M., ERLINGSSON, U., AND LIGATTI, J.
Control-ﬂow integrity—principles, implementations, and applica-
tions. ACM Transactions on Information and System Security 13,
1 (2009), 1–40.

[2] BALAKRISHNAN, G. WYSINWYX: What You See Is Not What You

Execute. PhD thesis, University of Wisconsin-Madison, 2007.

[3] BAP: Binary analysis platform. http://bap.ece.cmu.edu/.

[4] BinDiff. http://www.zynamics.com/bindiff.html.

[5] BinNavi. http://www.zynamics.com/binnavi.html.

[6] BitBlaze: Binary analysis for computer security.

bitblaze.cs.berkeley.edu/.

http://

[7] BOURQUIN, M., KING, A., AND ROBBINS, E. BinSlayer: Ac-
curate comparison of binary executables. In Proceedings of the
2nd ACM Program Protection and Reverse Engineering Workshop
(2013), ACM.

[8] BRUMLEY, D., JAGER, I., AVGERINOS, T., AND SCHWARTZ,
E. J. BAP: A binary analysis platform. In Proceedings of the 23rd
International Conference on Computer Aided Veriﬁcation (2011),
Springer, pp. 463–469.

[9] CABALLERO, J., JOHNSON, N. M., MCCAMANT, S., AND
SONG, D. Binary code extraction and interface identiﬁcation
for security applications. In Proceedings of the 17th Network
and Distributed System Security Symposium (2010), The Internet
Society.

[10] CHA, S. K., AVGERINOS, T., REBERT, A., AND BRUMLEY,
D. Unleashing mayhem on binary code. In Proceedings of the
2012 IEEE Symposium on Security and Privacy (2012), IEEE,
pp. 380–394.

USENIX Association  

23rd USENIX Security Symposium  859

[11] CHOI, S., PARK, H., LIM, H.-I., AND HAN, T. A static birthmark
of binary executables based on API call structure. In Proceeding
of the 12th Asian Computing Science Conference (2007), Springer,
pp. 2–16.

[12] DAVI, L., DMITRIENKO, A., EGELE, M., FISCHER, T., HOLZ,
T., HUND, R., STEFAN, N., AND SADEGHI, A.-R. MoCFI: A
framework to mitigate control-ﬂow attacks on smartphones. In
Proceedings of the 19th Network and Distributed System Security
Symposium (2012), The Internet Society.

[13] Dia2dump Sample. http://msdn.microsoft.com/en-us/

library/b5ke49f5.aspx.

[14] Dyninst API. http://www.dyninst.org/.

[24] PAPPAS, V., POLYCHRONAKIS, M., AND KEROMYTIS, A. D.
Smashing the gadgets: Hindering return-oriented programming
using in-place code randomization. In Proceedings of the 2012
IEEE Symposium on Security and Privacy (2012), IEEE, pp. 601–
615.

[25] PERKINS, J. H., KIM, S., LARSEN, S., AMARASINGHE, S.,
BACHRACH, J., CARBIN, M., PACHECO, C., SHERWOOD, F.,
SIDIROGLOU, S., SULLIVAN, G., WONG, W.-F., ZIBIN, Y.,
ERNST, M. D., AND RINARD, M. Automatically patching errors
in deployed software. In Proceedings of the ACM 22nd Symposium
on Operating Systems Principles (2009), ACM, pp. 87–102.

[26] ROSENBLUM, N. The new Dyninst code parser: Binary code isn’t

as simple as it used to be, 2006.

[15] ERLINGSSON, U., ABADI, M., VRABLE, M., BUDIU, M., AND
NECULA, G. C. XFI: Software guards for system address spaces.
In Proceedins of the 7th Symposium on Operating Systems Design
and Implementation (2006), USENIX, pp. 75–88.

[27] ROSENBLUM, N. E., ZHU, X., MILLER, B. P., AND HUNT, K.
Learning to analyze binary computer code. In Proceedings of the
23rd National Conference on Artiﬁcial Intelligence (2008), AAAI,
pp. 798–804.

[16] IDA FLIRT Technology.

https://www.hex-rays.com/

products/ida/tech/flirt/in_depth.shtml.

[17] GCC—Function Inline. http://gcc.gnu.org/onlinedocs/

gcc/Inline.html.

[18] GUILFANOV, I. Decompilers and beyond.

(2008).

In BlackHat USA

[19] HARRIS, L. C., AND MILLER, B. P. Practical analysis of stripped
binary code. ACM SIGARCH Computer Architecture News 33, 5
(2005), 63–68.

[20] HU, X., CHIUEH, T.-C., AND SHIN, K. G. Large-scale malware
indexing using function-call graphs. In Proceedings of the 16th
ACM Conference on Computer and Communications Security
(2009), ACM, pp. 611–620.

[28] SCHWARTZ, E., LEE, J., WOO, M., AND BRUMLEY, D. Native
x86 decompilation using semantics-preserving structural analysis
and iterative control-ﬂow structuring. In Proceedings of the 22nd
USENIX Security Symposium (2013), USENIX, pp. 353–368.

[29] SHARIF, M., LANZI, A., GIFFIN, J., AND LEE, W. Impeding
malware analysis using conditional code obfuscation.
In Pro-
ceedings of the 16th Network and Distributed System Security
Symposium (2008), Internet Society.

[30] SIDIROGLOU, S., LAADAN, O., KEROMYTIS, A. D., AND NIEH,
J. Using rescue points to navigate software recovery. In Proceed-
ings of the 2007 IEEE Symposium on Security and Privacy (2007),
IEEE, pp. 273–280.

[31] Unstrip. http://www.paradyn.org/html/tools/unstrip.

html.

[21] KHOO, W. M., MYCROFT, A., AND ANDERSON, R. Ren-
dezvous: A search engine for binary code. In Proceedings of
the 10th IEEE Working Conference on Mining Software Reposito-
ries (2013), IEEE, pp. 329–338.

[32] VAN EMMERIK, M. J., AND WADDINGTON, T. Using a de-
compiler for real-world source recovery. In Proceedings of the
11th Working Conference on Reverse Engineering (2004), IEEE,
pp. 27–36.

[22] KINDER, J. Static Analysis of x86 Executables. PhD thesis,

Technische Universität Darmstadt, 2010.

[23] KRUEGEL, C., ROBERTSON, W., VALEUR, F., AND VIGNA, G.
Static disassembly of obfuscated binaries. In Proceedings of the
13th USENIX Security Symposium (2004), USENIX, pp. 255–270.

[33] ZHANG, C., WEI, T., CHEN, Z., DUAN, L., SZEKERES, L.,
MCCAMANT, S., SONG, D., AND ZOU, W. Practical control ﬂow
integrity & randomization for binary executables. In Proceedings
of the 2013 IEEE Symposium on Security and Privacy (2013),
IEEE, pp. 559–573.

[34] ZHANG, M., AND SEKAR, R. Control ﬂow integrity for COTS
binaries. In Proceedings of the 22nd USENIX Security Symposium
(2013), pp. 337–352.

860  23rd USENIX Security Symposium 

USENIX Association

