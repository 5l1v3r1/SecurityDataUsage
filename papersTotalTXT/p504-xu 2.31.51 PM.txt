High Fidelity Data Reduction for Big Data Security

Dependency Analyses

Zhang Xu†1

zhang@nofutznetworks.com

Zhenyu Wu‡

adamwu@nec-labs.com

Kangkook Jee‡
kjee@nec-labs.com
Fengyuan Xu(cid:5)

fengyuan.x@gmail.com

Junghwan Rhee‡
rhee@nec-labs.com
Haining Wang∗
hnw@udel.edu

Zhichun Li‡

zhichun@nec-labs.com

Xusheng Xiao‡

xsxiao@nec-labs.com

Guofei Jiang‡
gfj@nec-labs.com

† NofutzNetworks Inc., Croton-on-hudson, NY, USA

‡ NEC Labs America, Inc., Princeton, NJ, USA

(cid:5) Nanjing University, Nanjing, China

∗ University of Delaware, Newark, DE, USA

ABSTRACT
Intrusive multi-step attacks, such as Advanced Persistent
Threat (APT) attacks, have plagued enterprises with signif-
icant ﬁnancial losses and are the top reason for enterprises
to increase their security budgets. Since these attacks are
sophisticated and stealthy, they can remain undetected for
years if individual steps are buried in background “noise.”
Thus, enterprises are seeking solutions to “connect the sus-
picious dots” across multiple activities. This requires ubiq-
uitous system auditing for long periods of time, which in
turn causes overwhelmingly large amount of system audit
events. Given a limited system budget, how to eﬃciently
handle ever-increasing system audit logs is a great challenge.
This paper proposes a new approach that exploits the de-
pendency among system events to reduce the number of log
entries while still supporting high-quality forensic analysis.
In particular, we ﬁrst propose an aggregation algorithm that
preserves the dependency of events during data reduction to
ensure the high quality of forensic analysis. Then we pro-
pose an aggressive reduction algorithm and exploit domain
knowledge for further data reduction. To validate the eﬃ-
cacy of our proposed approach, we conduct a comprehensive
evaluation on real-world auditing systems using log traces of
more than one month. Our evaluation results demonstrate
that our approach can signiﬁcantly reduce the size of system
logs and improve the eﬃciency of forensic analysis without
losing accuracy.

1Work done during an internship in NEC Labs America, Inc.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978378

1.

INTRODUCTION

Today’s enterprises are facing serious cyber-threat posed
by intrusive multi-step attacks such as Advanced Persistent
Threat (APT) attacks.
It takes much time for attackers
to gradually penetrate into an enterprise, to understand its
infrastructure, to own the high-value targets, and to steal
important information [1, 6, 2, 9, 10, 12] or to sabotage
mission critical infrastructures [11]. Compared with conven-
tional attacks, sophisticated multi-step attacks such as APT
attacks usually inﬂict much more severe damage upon the
enterprises’ business. The recent DARPA Transparent Com-
puting (TC) program [13] emphasizes that the challenges of
multi-step attacks come from modern computing systems be-
ing large, complex and opaque, and the threats can remain
undetected for years if individual steps are buried in back-
ground “noise.” Thus, to counter these sophisticated attacks,
enterprises are in great need of solutions to “connect the dots”
across multiple activities that individually might not be sus-
picious enough. Even though these attacks can be powerful
and stealthy, one typical constraint from the attacker side is
that such an attack needs multiple steps to succeed, such as
infrastructure reconnaissance and target discovery, as illus-
trated in the cyber kill chain [5]. Therefore, multiple attack
footprints might be left as “dots.”

In order to achieve the vision of connecting the dots, the
ﬁrst challenge is to collect and store the dots (attack prove-
nance). Since the attackers have the potential to reach each
host, we need to monitor and collect attack provenance from
every single host. According to [14], in 2014 APT attacks
penetrated enterprises and remained stealthy for 188 days on
average and even years before launching harmful harvesting
actions. This implies that in order to detect and understand
the impact of such attacks, the enterprises need to keep a
half year to one year worth of data. However, monitoring at-
tack provenance on every host in an enterprise for more than
a half year is burdensome and laborious. The system level
audit data collected from Linux Audit system [8] or Window
Event Tracing framework [7] alone can easily reach 0.5 to
1GB per host.
In a real-world scenario, for a commercial

504bank with 200,000 hosts, the required data is around 17PB
(half year 0.5GB per host) to 70PB (one year 1GB per host).
Furthermore, to achieve fast dependency correlation anal-
ysis or to promptly connect the dots even interleaved with
multiple normal activities, the data has to be eﬃciently stored
and indexed. Therefore, meeting these storage and retrieval
requirements for such a big-data system is a daunting task.
Very little previous research focuses on how to mitigate the
problem of overwhelmingly big-data of attack provenance de-
rived from system-level audit events for dependency analy-
sis. LogGC [31] observes that some system objects such as
temporary ﬁles are isolated, have short life-spans, and have
little impact on the dependency analysis; such objects and
events can be garbage collected to save space.
In our ap-
proach, we take a diﬀerent perspective and primarily study
the impact of diﬀerent events. We found that some events
have identical dependency impact scope, and therefore they
can be safely aggregated. Based on this observation, we
propose an algorithm called Causality Preserved Reduction
(CPR) for event aggregation. Furthermore, we found that
certain popular system behaviors lead to densely connected
dependency graphs of objects and their related neighbors.
Thus, we devise a causality-preserving one-hop graph reduc-
tion technique called Process-centric Causality Approxima-
tion Reduction (PCAR), which can further improve data re-
duction with very few false positives.

To validate the eﬃcacy of our proposed approach, we con-
duct a comprehensive evaluation on real-world auditing sys-
tems using log traces of more than one month. Our evalua-
tion results demonstrate that the CPR algorithm is general
and does not lose any accuracy by design, which can improve
the capacity of a big-data system by up to 3.4 times. In other
words, the same system can store 3.4 times more data with-
out aﬀecting storage and data processing eﬃciency. With a
trade oﬀ of introducing very few false positives (in our evalu-
ation at the rate of 0.2%), the PCAR algorithm can enlarge
the capacity of the system by up to 5.6 times. We also com-
pared our approach with a naive time-window-based data
aggregation. We show that the naive approach introduces
many more false positives at the rate of 13.9%. Without
considering manually tuning our approach for each applica-
tion in an enterprise environment, we achieve a similar re-
duction ratio as LogGC, but our solution can be combined
with LogGC to achieve more signiﬁcant data reduction.

The major contributions of this paper can be summarized

as follows:

• Through intensive data collection and analysis, we ob-
serve that some events have identical contributions to
dependency analysis and thus can be aggregated. Based
on this observation, we propose the CPR algorithm for
data reduction.

• We further observe that some high-level system behav-
iors such as PCI scanning will result in dense one-hop
dependency graphs, and such a subgraph as a whole
is more useful than its internal structures. Thus, we
propose the PCAR algorithm, which can achieve even
higher data reduction rates with few false positives.

• We conduct a comprehensive evaluation over extensive
datasets collected from a real enterprise environment
for one month. The evaluation results demonstrate
that our approach can improve the eﬃciency of forensic
analysis up to 5.6 times without losing accuracy.

• Our work will facilitate the detection of multi-step at-
tack behaviors that involve multiple steps of malicious
behaviors (i.e., dots) whose footprints are buried inside
data gathered from multiple hosts over a long period
of time.

The remainder of this paper is organized as follows. Sec-
tion 2 introduces the background and motivates our work.
Section 3 provides the deﬁnitions and design details of our
work. Section 4 presents our real-world data driven evalua-
tion. Section 5 discusses attack resilience and generality of
our approach. Section 6 surveys related works, and ﬁnally,
Section 7 draws the conclusion.

2. BACKGROUND AND MOTIVATION

In this section, we brieﬂy introduce the basic concept of
a system dependency graph and causality tracking. Then,
we present our observation of data characteristics of low-
level system event traces. Finally, we provide an intuition-
based description of how to reduce event trace data while
preserving its embedded causal dependencies.
2.1 System Dependency Analysis

A system event trace is a sequence of recorded interactions
among multiple system objects (processes, ﬁles, network con-
nections, etc.). It contains information, such as timing, type
of operation, and information ﬂow directions, which can be
used to reconstruct causal dependencies between historical
events. For ease of discussion, we use the terms causality
and dependency interchangeably in this paper.

Intuitively, we present a system event as an edge between
two nodes, each representing the initiator or the target of
the interaction. For example, a process named “/bin/bash”
reads a ﬁle named “/etc/bashrc”, is represented as node A,
B and edge eBA−1, shown in Figure 1 denoting that the read
operation from A to B occurred during the time interval of
[10, 20].

While each node and edge carries many pieces of informa-
tion, such as process ID and ﬁle permissions, for simplicity,
we only show the ones critical to causality analysis. This
includes the type of event, presented as text under the edge
name; the window of time the event took place, presented
as a pair of numbers in square brackets, denoting the start
and end timestamps, respectively; and the information ﬂow
direction, presented by the direction of the edge. Note that
the dependency graph allows multiple edges for a node pair
to distinguish events that happened at diﬀerent time inter-
vals. For instance, edges of eBA−1 and eBA−2 represent the
same type of event that occurred in diﬀerent time durations.
Illustrated by Figure 1, a trace of system events form
networks of causality dependencies, (i.e., the dependency
graph). The dependency graph is essential to many forensic
analysis applications, such as root cause diagnosis, intrusion
recovery, attack impact analysis and forward tracking [28],
which performs causality tracking on the dependency graph.
Causality tracking is an recursive graph traversal procedure,
which follows the causal relationship of edges either in the
forward or backward direction. For example, in Figure 1,
to examine the root cause of event eAD−1 (“/bin/bash” exe-
cutes “/bin/wget”), we apply backtracking [27] on our Point-
of-Interest (POI) edge, which recursively follows all edges
that could have contributed to the occurrence of POI. Edges
of eBA−1, eCA−1, eCA−2, and eEC−1 have an earlier starting

505that is, the presence or absence of “shadowed events” does
not alter any causality analysis. In multi-step attack forensic
analysis, shadow events describe the same attacker activities
that have already been revealed by key events. Therefore,
we could signiﬁcantly reduce the data volume while keeping
the causal dependencies intact, by merging or summarizing
information in “shadowed events” into “key events” while pre-
serving causal relevant information in the latter.

Figure 1: Dependency Graph and Backtracking

time than their predecessors and are therefore identiﬁed as
relevant events.
2.2 Data Characteristics

The accuracy of system dependency analysis heavily de-
pends on the granularity of event trace – the more ﬁnely
grained the data, the more accurate the analysis results.

We built a ubiquitous monitoring system for enterprise se-
curity analyses, which collects low-level system events from
Linux and Windows machines using kernel audit [8] and
ETW kernel event tracing [7], respectively. We observe that
an average desktop computer produces more than 1 million
events per day, while a server could yield 10 to 100 times
the volume. Each day, a rather small system of 100 comput-
ers generated more than 200 million events, which requires
mid- to high-end server to process and produces databases
over 200GB. A large enterprise can easily have more than
100,000 computers, and may require them to store the moni-
tored data for several months or even years. Such a workload
made it impractical to deploy an accurate dependency anal-
ysis to large enterprises. Reducing the data volume is key to
solving the scalability problem.

While data reduction is a well-studied topic, the most com-
monly used techniques, spatial and temporal sampling, are
not applicable to dependency analysis based on system event
traces. Because sampling techniques do not have the inherit
concept of causal relations, they are prone to introducing
random causal errors. On the other hand, the recursive
nature of causality tracking exponentially magniﬁes errors
on the causal path – a single falsely introduced dependency
when tracked forward or backward several hops could easily
leads to hundreds of false positives. Therefore, any practical
data reduction on system event traces must take great care
to limit the introduction of causal errors.
2.3 Data Reduction Insights

Recognizing their critical importance, we ﬁrst explore all
possibilities to perform data reduction while perfectly pre-
serving the causal dependencies. By studying causal rela-
tions of every event in our system traces, we discover that
only a small fraction of events, which we call “key events”,
bear causal signiﬁcances to other events. When performing
forensic analysis on multi-step attacks, the key events reveal
the footprint of an attacker (i.e., the sequences of activities
an attacker triggers). Moreover, for each “key event” there
exist a series of “shadowed events” whose causal relations to
other events are negligible in the presence of the “key event”,

Figure 2: Unequal Dependencies in Backtracking

Figure 2 shows an example of unequal dependencies re-
In
visiting our backtracking example shown in Figure 1.
this ﬁgure, we backtrack POI event eAD−1 to eBA−1, eCA−1,
eCA−2, and eEC−1. However, note that eCA−2 took place
after eCA−1, while no event involves either A or C hap-
pening in between, and both events are of the same type
(Read ). As a result, the existence of eCA−1 in this graph has
no causal impact to the backward dependency analysis. In
other words, the presence of the “key event” eCA−2 shadows
the event eCA−1. Therefore, eCA−1 can be removed by com-
bining its information with eCA−2, preserving the end time
of the latter.

Figure 3: Unequal Dependencies in Forward-tracking

In Figure 2, we apply similar logic to identify unequal
dependencies in forward-tracking analysis [28] where edge
eEC−1 represents a POI event. For two edges that connects
nodes C and A, event eCA−1 took place before eCA−2, while
no event involves either A or C happening in between; both
events are of the same type and therefore “key event” eCA−1
shadows event eCA−2.

When bi-directional tracking is concerned, the condition
for event shadowing become more strict. Figure 4 combines
backward and forward tracking into a single data dependency
graph deﬁning POI event as eHB−1 and eAD−1, respectively.
While “key event” eCA−2 shadows the event eCA−1 for both

 A B eBA-1  Read [10,20] A: /bin/bash B: /etc/bashrc C: /etc/inputrc D: /bin/wget E: /usr/bin/vi D eBA-2 Read [40, 42] C eCA-1   Read [15,23] eCA-2  Read [28,32]  eAD-1  Exec [36, 37]  POI Event Relevant Event Irrelevant Event E eEC-1   Write [2,8]  A B eBA-1  Read [10,20] A: /bin/bash B: /etc/bashrc C: /etc/inputrc D: /bin/wget E: /usr/bin/vi D eBA-2 Read [40, 42] C eCA-1   Read [15,23] eCA-2  Read [28,32]  eAD-1  Exec [36, 37]  POI Event Key Event Irrelevant Event E eEC-1   Write [2,8]  Shadowed Event   eCF-2 Read [20, 22] eCA-1   Read [15,23] eCA-2 Read [28,32] eCF-1  Write [24, 26] C eEC-1 Write [2,8] A  E F POI Event Key Event Shadowed Event Irrelevant Event A: /bin/bash C: /etc/inputrc E: /usr/bin/vi F: /bin/sed 506ensures that causality loss only impacts events within the
iBurst. Shown in Figure 5, the “/bin/pcscd” process gener-
ates an iBurst involving many ﬁles, including “/dev/sda0,”
“/dev/ppp0,” and “/dev/tty1,” highlighted by the dashed cir-
cle. By ignoring the causal relationship among all events
within the iBurst, event eCA−2 is considered approximately
shadowed by eCA−1, even though they are interleaved by
eAD−1. However, eAB−1 and eAB−2 must be kept as inde-
pendent key events because they are interleaved by eBF−1,
which does not belong to the iBurst.

3. DESIGN

In this section, we ﬁrst present a formal deﬁnition of im-
portant terms and concepts used in our design. Then we
detail our algorithm used in data reduction. Our approach
provides two schemes that serve to diﬀerent reduction goals.
While the primary scheme prioritizes perfect dependency
preservation, the optional secondary scheme performs causal-
ity approximating reduction to gain better reduction rates
at the expense of limited dependency accuracy loss. Finally,
we describe an extension to incorporate domain knowledge-
based data reduction.
3.1 Deﬁnitions

The formal deﬁnitions of three key concepts that are key

to our data reduction design are given below.

For the rest of the paper, tw(e) is used to represent the
time window associated with an event, ts(e) and te(e) deﬁnes
the start and end times of event e, respectively. We use the
terms event and edge, entity and node interchangeably.
3.1.1 Causality Dependency
Two events have causality dependency with each other if
an event has information ﬂow that can aﬀect the other event.
DEFINITION 1: Causality Dependency.

causality dependency, if

For two event edges e1=(u1,v1) and e2=(u2,v2), they have
• v1=u2,
• te(e1) < te(e2).

If e1 has information ﬂow to e2 and e2 has information ﬂow
to e3, then e1 and e3 have causality dependency.
3.1.2 Event Trackability
While each event has causality with other events in the
system, we use a term called trackability to summarize the
causality dependency information contained in an event. We
deﬁne trackability as the forensic analysis results that can
be derived from an event (i.e. the backtracking and forward-
tracking results with the event as an POI event). A “key
event” and its “shadowed events” have equal trackability.

DEFINITION 2: Causality Shadowing (Partial Track-
ability Equivalence).

Given two event edges across the same pair of nodes e1 =

(u1, v1) and e2 = (u1, v1), where te(e2) > te(e1):

• e2 shadows the backward causality of e1, denoted as
e2 ≈B e1, if and only if there exists no event edge
e = (u2, v2) that satisﬁes all of:

– u2 = u1 and v2 (cid:54)= v1,
– te(e) > te(e1) and ts(e) < te(e2).

We further explain this deﬁnition in Section 3.2.1.

Figure 4: Dependency Reduction in Bi-directional Tracking

analyses, event eBA−1 and eBA−2 must be kept as indepen-
dent “key events,” since each event deﬁnes causal relation
from diﬀerent analyses.
2.3.1 Low-loss Dependency Approximation
With further study of our data, we discover that several
applications (mostly system daemons) tend to produce in-
tense bursts of events that appears semantically similar, but
are not reducible by the perfect causality preserving reduc-
tion, due to interleaved dependencies. For example, process
“pcscd” repeatedly interleaves read/write access to a set of
ﬁles. We name this type of workload “iBurst.”

We performed in-depth analysis of those applications and
found that each iBurst is generated by an application per-
forming a single high-level operation, such as checking for
the existence of PCI devices, scanning ﬁles in a directory, etc.
Those high-level operations are not complex, but they trans-
late to highly repetitive low-level operations. With iBurst,
data reduction is only possible with certain levels of loss
in causality. We then analyse whether and to what extent
causality loss is acceptable. From the causality analysis ap-
plications’ perspective, tracking down to the high-level op-
erations usually yields enough information to aid the under-
standing of the results. For example, “pcscd” checks for all
PCI devices, or “dropbox” scans the directory. Although ob-
taining precise low-level operation dependencies does yield
more information, the extra data usually do not add more
value. Therefore, accuracy loss seems acceptable as long as
we contain the impact of the errors so they do not aﬀect
events that do not belong to the burst.

Figure 5: Dependency Approximation Reduction Example

We thus devise a method to detect an iBurst and apply a
well-controlled dependency approximation reduction, which

  A B eBA-1  Read [10,20] A: /bin/bash B: /etc/bashrc C: /etc/inputrc D: /bin/wget G: /bin/scp H: /bin/pico  D eBA-2 Read [40, 42] C eCA-1   Read [15,23] eCA-2  Read [28,32]  eAD-1  Exec [36, 37]  POI Event Key Event Shadowed Event G eGB-1   Write [1,5]  H eHB-1   Write [36,38]     EBF-1 Read [70, 80] eCA-1   Read [10,20] eAB-1 Write [50,60]  eAD-1  Write [15, 25]   B eCA-2 Read [30,40]  A  C D Event of Interest Key Event Approx. Shadowed Event A: /bin/pcscd B: /dev/sda0 C: /dev/ppp0 D: /dev/tty1 E: /bin/dd F: /bin/mount   E eEC-1  Write [1, 5]   F eAB-2Write [75,78]507Algorithm 1 Causality Preserved Aggregation

Require: E is event stream sorted by start time in ascend-

ing order
function CPR AGGREGATE(E)

for each e in E do

u←src(e)
v←dst(e)
Let S(u,v,R(e)) be a stack of events from u to v

that are aggregable

if S.Empty then

else

S.push(e)
ep←S.pop
if FORWARD CHECK(ep,e,v) and

then

BACKWARD CHECK(ep,e,u)
ep←MERGE(ep,e)
S.push(ep)

else

S.push(e)

end if

end if

end for

end function
function MERGE((ep, el))

te(ep)←te(el)
Tune attributes of ep
DELETE(el)
return ep
end function

• e1 shadows the forward causality of e2, denoted as
e1 ≈F e2, if and only if there exists no event edge
e = (u2, v2) that satisﬁes all of:

– u2 (cid:54)= u1 and v2 = v1,
– te(e) > ts(e1) and ts(e) < ts(e2),

We further explain this deﬁnition in Section 3.2.2.

DEFINITION 3: Full Trackability Equivalence.
Given two event edges e1 and e2, they are fully equivalent
in trackability, denoted as e1 ≈ e2, if and only if e2 ≈B e1
and e1 ≈F e2.

In other words, for two events e1 and e2 that have both
backward and forward trackability equivalence, with either
e1 or e2 removed from the dependency graph, forward or
backward causality tracking from any POI event (other than
e1 and e2) would yield identical results (except e1 and e2).
As a result, e1 and e2 are completely equivalent from the
causality analyses’ point-of-view.
3.1.3 Event Aggregability
Two events are aggregable only if they have the same type
and share the same source and destination entities. For cer-
tain types of events such as ﬁle read/write, we also require
that the two events share the same certain attributes, (e.g.,
the ﬁle open descriptor). A set of aggregable events is a su-
perset of a “key event” and its “shadowed events.”

DEFINITION 4: Event Aggregability.
E, where ∀ei, ei ≈ e:

Given an event e and a set of trackbility equivalent events
• Deﬁne a set of attributes A to identify the aggregated

event with acceptable level of granularity.
For example, A can include subject and object at-
tributes (i.e., process executable name, ﬁle path name),
the event type (i.e., ﬁle access, IP channel access), and

Algorithm 2 Backward Trackability Check
function BACKWARD CHECK(ep,el,u)
for each e of u.INCOMING EVENT do

if tw(e) overlap with [te(ep),te(el)] then

return false

end if

end for
return true

end function

Algorithm 3 Forward Trackability Check
function FORWARD CHECK(ep,el,v)

for each e of v.OUTGOING EVENT do

if tw(e) overlap with [ts(ep),ts(el)] then

return false

end if

end for
return true

end function

type-speciﬁc attributes, such as ﬁle access operation
(i.e., read, write or execute).
• Event ε is aggregable with e, denoted as ε ∼ e, if and
only if ε ∈ E and ai(ε) = ai(e),∀ai ∈ A.
For a given deﬁnition, without loss of generality, if e1∼e2

and e2∼e3, then e1∼e3.
3.2 Causality Preserved Reduction

Our primary reduction scheme aims at preserving causality
dependency while performing the data reduction. Therefore,
we name it Causality Preserved Reduction (CPR). The core
idea is to aggregate all those events that are aggregable and
share the same trackability.

Algorithm 1 shows the working procedure of CPR, in which
a stream of events sorted by start time are taken as input.
We maintain a stack for each type of event between a pair of
entities. Every time we observe an event between the same
pair of entities with same type, we check whether the events
can be aggregated with the event in stack. This aggrega-
tion checking includes the examination of both forward and
backward trackability.

Algorithm 2 describes the procedure to check the backward
trackability. For two events e1 and e2 from entity u to entity
v, they have the same backward trackability if all the time
windows of incoming events of u do not overlap with the
time window of [te(e1),te(e2)]. Algorithm 3 describes the
procedure to check the forward-trackability. For two events
e1 and e2 from entity u to entity v, they have the same
forward trackability if none of outgoing events of v has an
end time between the start times of e1 and e2.

If two events can be aggregated, we aggregate the event
with a later start time (i.e., the later event) to the event with
an earlier start time (i.e., the former event) by extending the
end time of the former event to the end time of the later event
and then discard the later event.
3.2.1 Backward trackability
According to our backward algorithm, for two aggregat-
able events e1 and e2 from u to v, they have the same back-
ward trackability only if none of the incoming events of u has
a time window overlapped with the time window between the
end times of e1 and e2.

For any incoming event e of entity u, its timing can fall
into one of the following three cases, illustrated in Figure 6:

5083. ts(e)>ts(e2) (i.e., the time window only overlaps with

the blue shadow of Area F).

In the ﬁrst case, e has no dependency with e1 or e2, so it
will not be included in forward-tracking results. The sec-
ond case also has two sub-cases: a) te(e)<ts(e2) and b)
te(e)>ts(e2).
In a), the event only has dependency with
e1, breaking the trackability equivalence of e1 and e2.
In
b), the threshold populated with e1 as a POI event will be
max{ts(e1), ts(e)} while the threshold populated with e2 as a
POI event will be ts(e2). Diﬀerent populated thresholds will
also break the trackability equivalence.
In the third case,
e has dependency with both e1 and e2 and the populated
threshold is ts(e).

As we can see, only case 2 can break the forward tracka-
bility equivalence. Thus, our algorithm will stop aggregating
the two events if case 2 is reached.

3.2.3 Tracking with an aggregated event
Given that e1 and e2 pass the backward and forward track-
ability equivalence checking of our algorithms, they will be
aggregated and replaced with a new event e(cid:48). All the at-
tributes of e1 and e2 will be copied to e(cid:48) except that the time
window of e(cid:48) will become [ts(e1), te(e2)]. If e(cid:48) is selected as
an POI event, the backtracking threshold will be te(e2) and
the forward tracking threshold will be ts(e1). Because all in-
coming events of entity u and all outgoing events of v satisfy
the conditions set by our algorithms, the backtracking and
forward-tracking results with e(cid:48) as the POI event should be
the same as those with e1 or e2 as the POI event.

Impact on tracking with other POI events

3.2.4
When we apply our data reduction, we attempt to aggre-
gate all the events that share the same trackability. Before
an attack is revealed, we cannot know what events will be
selected as POI events; therefore, our approach should work
equally on events no matter whether they will be selected as
POI events.

To continue the example above, assume we start in another
POI event to perform backtracking. While reaching the step
we need to check the incoming events of entity v, we have
a threshold th. Using data without reduction, we need to
check events e1 and e2, but with the reduced data we need
to check event e(cid:48). Since no outgoing events of v have the end
time falling into the range of [ts(e1), ts(e2)], according to the
threshold population algorithm, th should not fall into this
range either. This leaves two cases for th: th <ts(e1) and
th >ts(e2). In the ﬁrst case, with the raw data both e1 and
e2 will fail the dependency check and will not be included in
the backtracking graph. With the reduced data, e(cid:48) will not
pass the dependency check either, yielding the same results
with the data without reduction: entity u and the events
from u to v will not appear in the backtracking results. In
the second case, with the raw data, both e1 and e2 will be
included in the backtracking graph and the threshold will be
populated.

Note that since all events that happen before e1 will also
happen before e2, it means e1 will be shadowed by e2 while
performing backtracking with other POI events in this case.
Therefore we only need to populate the threshold with e2,
which would be min{th, te(e2)}. With the reduced data, e(cid:48)
will be included with the populated threshold min{th, te(e2)}.
Therefore, entity u will be included in the backtracking re-

Figure 6: Determine whether to aggregate two events be-
tween u and v.

1. te(e)<te(e1) (i.e., the time window of e only overlaps

with the yellow shadow of Area B).

2. te(e)>te(e1) and ts(e)<te(e2) (i.e., the time window of

e has an overlap with the red shadow of Area B).

3. ts(e)>te(e2) (i.e., the time window of e only overlaps
with the blue shadow of Area B). By selecting e1 as
a POI event, the threshold will be set to te(e1); by
selecting e2 as a POI event, the threshold will be set to
te(e2).

In the ﬁrst case, e has dependency with both e1 and e2.
According to the backtracking algorithm, the event will be
included into the result set no matter whether we choose e1
or e2 as a POI event. Moreover, the threshold will be up-
dated to the end time of the incoming event of u in both
backtracking situations. The second case has another two
sub-cases: a) ts(e)>te(e1) and b) ts(e)<te(e1).
In a), the
event only has dependency with e2, and thus choosing e1
or e2 as a POI event generates diﬀerent backtracking re-
sults, which will break the trackability equivalence of e1 and
e2. In b), although the event has dependency with both e1
and e2, selecting e1 as a POI event will make the populated
threshold te(e1) while selecting e2 as a detection point will
make the populated threshold min{te(e1), te(e)}. Such a sit-
uation will possibly make backtracking results diﬀerent in
the subsequent iterations, which also breaks the trackabil-
ity equivalence of e1 and e2.
In the third case, the event
has no dependency with e1 or e2, implying that it will not
be included in backtracking results anyway. As we can see,
only case 2 can break the backward trackability equivalence.
Thus, our algorithm will stop aggregating the two events if
case 2 is reached.
3.2.2 Forward trackability
According to our forward algorithm, for two aggregatable
events e1 and e2 from u to v, they have the same forward
trackability only if none of the outgoing events of entity v has
a time window overlapped with the time window between the
start times of e1 and e2.

For any outgoing event e of entity v, its timing can fall
into one of the following three cases, illustrated in Figure 6:

1. te(e)<ts(e1) (i.e., the time window only overlaps with

the yellow shadow of Area F).

2. te(e)>ts(e1) and ts(e)<ts(e2) (i.e., the time window

overlaps with the red shadow of Area F).

e1 e2 u v ts(e1) ts(e2) te(e1) te(e2) B F 509Algorithm 4 Process-centric approximation reduction

Algorithm 5 Aggregate hot process events

Require: E is a stream of events involving hot process and

its neighbours N(u)

Require: E is sorted by start time

function PCAR AGGREGATE(E)

Let Q be a queue holding events in N(u)
Q←∅
for each e do
s←src(e)
t←dst(e)
if s /∈N(u) then
else if t /∈N(u) then

N.INDEADLIN E←te(e)
N.OUTDEADLIN E←te(e)

else

CLEARSTATE(N(u),e)

end if

end for
return N(u)

end function

function CLEARSTATE(N(u),e)

Let S(s,t,R(e)) be a stack of events aggregable with e

in Q

if S.IS EMPTY then

else

S.PUSH(e)
ep←S.POP
if s==u then

else if t==u then

ﬂag←PCAR CHECK(ep,e,N.INDEADLIN E,out)
ﬂag←PCAR CHECK(ep,e,N.OUTDEADLIN E,in)
ﬂag←true

else

end if
if ﬂag then

MERGE(ep,e)

end if

end if
return

end function

sults with both raw data and reduced data. The connectivity
between u and v will also be kept with the data reduction.

The same procedure can be applied to forward tracking.
Since no incoming events of u can have the start time in
[te(e1), te(e2)],
in the tracking process while reaching u,
the threshold cannot fall into [te(e1), te(e2)]. Therefore, in
forward-tracking of such a case, e2 will be shadowed by e1,
and aggregating e1 and e2 will preserve the entities and con-
nectivity in tracking results.
3.3 Process-centric Causality Approximation

Reduction

Our secondary data reduction scheme, Process-centric Causal-

ity Approximation Reduction (PCAR), aims to reduce data
from intensive bursts of events with interleaved dependen-
cies, which are otherwise not reducible without dependency
loss. PCAR constrains the causal dependencies compromised
within the events within the burst, and achieves data reduc-
tion with a very limited impact to the dependency analyses.
We deﬁne a process that interacts with a large number of
objects in a short time a hot process. Hot processes can be
detected using a simple statistics calculation with a sliding
time window If the number of events related to a process
in a time window exceeds a certain threshold, the process
is marked as a hot process.
In our evaluation, we set the
threshold to 20 events per 5 seconds. Once a hot process is
detected, we collect all objects involved in the interactions,
and form a neighbour set N (u), where u is the hot process.
We name the set ego-net. Illustrated in Algorithms 4, 5, and
6, instead of equally checking the trackability on all aggre-
gation candidates based on the events, we only check the
trackability with the information ﬂow into and out of the
neighbour set N (u). The checking procedure is the same as
CPR in checking the time window overlap.
It can ensure
that as long as the events inside the ego-net are not selected
as a POI event, we can achieve high-quality tracking results.
It is noteworthy that PCAR algorithm does not reserve the
trackability inside the ego-net. In a rare case that an event
inside the ego-net is selected as a POI event, it has a chance
that PCAR has aggregated the event, which results in an
enlarged time window of the POI event. Due to the nature
of forensic analysis, an enlarged time window can introduce
false positives in tracking results.

Algorithm 6 Trackability check for hot processes

function PCAR CHECK(ep,el,t,ﬂag)

if t>te(ep) then
return false

else if ﬂag==out then

return FORWARD CHECK(ep,el, ep.dst)

else if ﬂag==in then

return BACKWARD CHECK(ep,el, ep.src)
return true

end if

end function

3.4 Domain Knowledge Reduction Extension
Besides exploiting causality, our work also utilizes domain

Special ﬁles

knowledge to enhance our data reduction.
3.4.1
In a Linux system, many system entities are treated as ﬁles
and interact with processes as ﬁle reads/writes. For instance,
all device interfaces are regarded as ﬁles and can be accessed
via /dev directory. Shared memory can also be accessed as
ﬁles under /shm directory. All the ﬁles in a Linux system can
be classiﬁed into two categories: plain ﬁles and special ﬁles.
Plain ﬁles refer to the ﬁles that have data stored in the disk,
and special ﬁles refer to the ﬁles that are only abstractions
of system resources.

For a plain ﬁle, when a process reads from it or writes to
it, an explicit information ﬂow occurs, which will be further
used in backtracking or forward-tracking. By contrast, the
interactions between processes and special ﬁles may involve
more complex behaviours and implicit information ﬂows. For
instance, if process A writes to a plain ﬁle and process B
reads from the same ﬁle later, there is an explicit information
ﬂow from A to B. However, if process A writes to something
under /proc and process B reads from the /proc ﬁle later, it
is unlikely that there is an information ﬂow from A to B. The
reason is that the ﬁles under /proc are mappings of kernel
information, and writing to them or reading from them in-
volves complex kernel activities that cannot be treated as a
simple read/write, resulting in no explicit information ﬂow.
Based on such domain knowledge, we further integrate a
special ﬁles ﬁlter into our approach to remove events related
to those special ﬁles that will not introduce any explicit in-

510formation ﬂow. Note that for some special ﬁles, such as ﬁles
under /shm, the processes interacting with them can still in-
troduce information ﬂows. Therefore, we will not ﬁlter them.
3.4.2 Temporary ﬁles
In a system, there are many ﬁles that only serve temporary
purposes and will be deleted afterwards. Many processes
need to store temporary information during execution, and
hence generate ﬁles that only exist for a short period of time.
Thus, these temporary ﬁles only interact with the process
that creates them.

Similar to previous work [31], we deﬁne a temporary ﬁle as
a ﬁle that is only touched by one process during its lifetime.
Since a temporary ﬁle only has information exchange with
one process, it does not introduce any explicit information
ﬂow in attack forensics either, and therefore we can remove
all the events of temporary ﬁles from the data.

4. EVALUATION

We implement a prototype of our proposed approach, which
consists of CPR, PCAR, and domain knowledge extension
(which will be referred to as DOM). Then we conduct a se-
ries of experiments based on real-world data collected from a
corporate R&D laboratory to demonstrate the eﬀectiveness
of our data reduction approach, in terms of data processing,
storage capacity improvement and the support for forensic
analysis.

We also perform a break-down analysis to show the ef-
fectiveness of our approach on diﬀerent workload patterns,
which therefore beneﬁts enterprises with diﬀerent workloads.
Moreover, to fully evaluate our approach, we also implement
a na¨ıve event aggregation and conduct experiments for com-
parison. Finally, we measure the runtime overhead of our
data reduction system.
4.1 Data collection

In the corporate research laboratory from which our data
is collected, we have deployed an enterprise-wide audit sys-
tem. There are monitoring agents deployed across servers,
desktops and laptops to collect system events. We select one
month of data for our study. The data logs we used are col-
lected from more than 30 machines with various server mod-
els and operating systems. All the data collected is stored in
a daily basis (i.e., the data of each day is stored in a sepa-
rate database) and there will be a separate dependency graph
generated from each individual day’s data. There are more
than 10 billion events captured during our data collection.
4.2 Data reduction

We ﬁrst evaluate the overall eﬀectiveness of our approach
in data reduction, and then we present a break-down anal-
ysis to demonstrate how our solution works under diﬀerent
workloads. Finally, we compare our approach with a na¨ıve
aggregation in data reduction.

Our reduction system records how many events are aggre-
gated and then reports the reduction statistics. A reduced
data volume has multi-fold signiﬁcance. Most intuitively, it
results in saved storage capacity and reduced storage cost.
Moreover, since our reduction system is a module in the data
processing stream, the reduced data will save bandwidth and
improve the data processing capacity for the following mod-
ules. For instance, if our system can reduce data by 80%
and the following data processing module needs to perform

Figure 7: The eﬀectiveness of our system on improving data
processing and storage capacities

event queries on our output, our reduction can increase the
processing capacity of the query module by 5 times.
4.2.1 Overall effectiveness
For comparison reasons, the log data is processed in three
phrases.
In the ﬁrst phrase, we only apply CPR. In the
second phrase, we further apply PCAR. Finally, we apply
domain knowledge reduction extension in the third phrase.
Figure 7 shows the data reduction results of our system
(i.e., the improvements on the data processing and storage
capacities after applying our data reduction techniques). In
total we collected data from 31 hosts, 18 of which have Linux
as operating systems and 13 of which have Windows as op-
erating systems. For CPR, on average it can achieve the
reduction ratio of 56% (i.e., it can reduce the data size by
56%) thus increasing the data processing and storage ca-
pacities by 2.27 times (2.63 in Linux and 1.61 in Windows,
respectively). Next, if we apply CPR+PCAR, the overall
reduction ratio will be raised to 70%, which achieves 3.33
times growth in the data processing and storage capacities
(4 in Linux and 2.44 in Windows, respectively). Finally, af-
ter we apply our domain knowledge reduction extension, the
reduction ratio can reach 77%, which increases the data pro-
cessing and storage capacities by 4.35 times (5.26 in Linux
and 2.56 in Windows, respectively).

It is evident that our system can eﬀectively reduce data
logs and improve data process and storage capacities in a
signiﬁcant fashion. Even with CPR alone, the data size can
be reduced by more than half, increasing the data process-
ing/storage capacities by 2 times; on certain hosts, our sys-
tem can help to increase the data processing and storage
capacities by more than 20 times. From Figure 7 we can
see that at diﬀerent hosts, the beneﬁts gained by our system
vary. This is because diﬀerent hosts run diﬀerent workloads,
and the eﬀectiveness of our system is aﬀected by diﬀerent
workloads.
4.2.2 Break-down analysis
Diﬀerent workloads introduce diﬀerent system activities,
resulting in diﬀerent process behaviours and diﬀerent amounts
of event redundancy. Since the eﬀectiveness of our system is
sensitive to diﬀerent workloads, we need to conduct a break-
down analysis to scrutinize how our system works on diﬀerent
workloads.

(cid:2)(cid:3)(cid:4)(cid:2)(cid:4)(cid:3)(cid:5)(cid:2)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:7)(cid:8)(cid:12)(cid:13)(cid:14)(cid:15)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:9)(cid:16)(cid:17)(cid:18)(cid:19)(cid:5)(cid:20)(cid:14)(cid:9)(cid:21)(cid:15)(cid:3)(cid:7)(cid:16)(cid:17)(cid:18)(cid:16)(cid:17)(cid:18)(cid:19)(cid:17)(cid:16)(cid:20)(cid:18)(cid:16)(cid:17)(cid:18)(cid:19)(cid:17)(cid:16)(cid:20)(cid:18)(cid:19)(cid:21)(cid:22)(cid:23)511Figure 8: The workload distribution of our collected data

We categorize the common workloads in an enterprise en-
vironment to the following groups: system monitor, system
utility, system daemon, ﬁle sharing, browser, oﬃce, commu-
nication, develop tools, programming, web server, database,
and others. System monitoring includes the auditing and
monitoring tools. System utility includes common utilities
in Linux or Windows such as mv, cp, ls. System daemons
are daemon processes running in the background, such as pc-
scd, sshd and security scanning in Windows. File sharing
represents the applications used to share ﬁles such as SVN,
CVS, and DropBox. Browser includes all types of browsers
like Chrome, IE and FireFox. All the applications used
for writing documents are categorized to oﬃce (e.g., vim,
OpenOﬃce, PowerPoint, excel). Communication includes
mailing tools such as GMail, voice tools like Skype and re-
mote access tools like SSH. Develop tools are those dedicated
to production development such as Matlab, R and Eclipse.
Programming refers to all kinds of programming language
compilers like GCC, Java, python. Web server includes tom-
cat, Apache and JBoss. Database includes these processes
running as database services like MySQL, MangoDB, etc.
Finally, the rest of the processes are categorized to others.

Figure 8 illustrates the workload pattern distribution in
our collected data. Because this work is conducted in a re-
search institute, the workload pattern is skewed to system
monitoring, system utilities, system daemons, and program-
ming. We can see that those processes contribute to the
majority of the events we captured.

Table 1 lists the breakdown analysis under diﬀerent work-
load patterns, showing the percentages of data reduction
and the speed-ups on data processing and storage capacities.
From the table we can see that CPR works well on most
workloads. However, it works poorly on system daemons,
and the reason has been explained previously: the daemon
processes generally perform tasks that generate intensively
interleaving events. That is also why PCAR works very well
on this type of workload. Some system utilities exhibit sim-
ilar behaviours, and thus applying PCAR can improve their
reduction ratios signiﬁcantly. Oﬃce workloads generate con-
siderable temporary ﬁles, which is why our domain knowl-
edge reduction extension works best on them. File sharing
generates many interactions with temporary ﬁles (logs), and
thus domain knowledge ﬁltering can help to improve the re-
duction ratio signiﬁcantly.

Figure 9: The comparison of na¨ıve aggregation with a 10-
seconds time window, CPR, and na¨ıve aggregation with an
unlimited time window

The workload on which our approach is least eﬀective is
communication applications. However, their workloads do
not contribute much to the entire workload in an enterprise
environment. Database, on the other hand, could be one of
the majority workloads in certain circumstances, but our ap-
proach works less eﬀectively on them. However, our system
can still achieve an reduction ratio of more than 40% and
increase the data processing and storage capacities by 1.67
times.
4.2.3 Naïve aggregation
The na¨ıve approach is based on the heuristics that events
appeared in a short period of time between two system en-
tities that tend to share similar behaviours. Thus, in this
na¨ıve approach, we blindly aggregate events in a ﬁxed time
window, without considering any dependency. Such a na¨ıve
approach can be regarded as a state-of-art data reduction ap-
proach, which will provide us a baseline of reduction power
to compare with.

We implement the na¨ıve aggregation approach in two ways.
First, we ﬁx the time window to 10 seconds (i.e., we aggre-
gate the events within a 10-second interval). Second, we set
the time window to unlimited (i.e., we will aggregate all ag-
gregatable events in the data). The second na¨ıve aggregation
should be an upper bound of data reduction power of any
reduction approach that does not remove any entities and
has the same event aggregability deﬁnition as our approach.
Figure 9 shows the comparisons among the na¨ıve aggre-
gation with a 10-seconds time window, CPR, and the na¨ıve
aggregation with an unlimited time window. On average,
the na¨ıve aggregation with a 10-seconds time window can
improve the data processing/storage capacity by 1.85 times,
while CPR can increase the data processing/storage capacity
by 2.18. Thus, on average, CPR clearly outperforms the 10-
second na¨ıve aggregation; in many cases, its improvement is
close to that of the upper bound (i.e., the na¨ıve aggregation
with an unlimited time window).
4.3 Support of forensic analysis

Our data reduction approach is designed to support foren-
sic analysis. Therefore, we evaluate the quality of forensic
analysis after applying our data reduction (i.e., whether the
entities and connectivity can be well preserved in tracking
results after we apply our data reduction techniques on the

(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:4)(cid:5)(cid:9)(cid:7)(cid:5)(cid:10)(cid:4)(cid:9)(cid:11)(cid:7)(cid:5)(cid:10)(cid:4)(cid:3)(cid:12)(cid:7)(cid:9)(cid:4)(cid:5)(cid:9)(cid:7)(cid:8)(cid:4)(cid:6)(cid:10)(cid:7)(cid:3)(cid:4)(cid:11)(cid:8)(cid:7)(cid:3)(cid:4)(cid:12)(cid:6)(cid:7)(cid:8)(cid:4)(cid:2)(cid:12)(cid:7)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:1)(cid:19)(cid:20)(cid:21)(cid:22)(cid:16)(cid:20)(cid:23)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:1)(cid:24)(cid:25)(cid:17)(cid:18)(cid:20)(cid:21)(cid:26)(cid:23)(cid:20)(cid:27)(cid:15)(cid:17)(cid:23)(cid:28)(cid:23)(cid:20)(cid:29)(cid:23)(cid:25)(cid:18)(cid:18)(cid:22)(cid:21)(cid:29)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:1)(cid:30)(cid:16)(cid:22)(cid:31)(cid:15) (cid:22)(cid:31)(cid:17)(cid:1)(cid:13)!(cid:25)(cid:23)(cid:22)(cid:21)(cid:29)(cid:24)(cid:17)"(cid:17)(cid:31)(cid:20)#(cid:24)(cid:25)(cid:16)(cid:25)$(cid:25)(cid:15)(cid:17)%(cid:22)(cid:23)(cid:16)&(cid:25)(cid:31)(cid:22)’(cid:25)(cid:16)(cid:22)(cid:20)(cid:21)((cid:16)!(cid:17)(cid:23))(cid:20)(cid:18)(cid:18)&(cid:21)(cid:22)*(cid:25)(cid:16)(cid:22)(cid:20)(cid:21)(++(cid:22)*(cid:17),(cid:17)$(cid:1)(cid:13)(cid:17)(cid:23)"(cid:17)(cid:23)(cid:2)(cid:3)(cid:4)(cid:2)(cid:4)(cid:3)(cid:5)(cid:2)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:7)(cid:8)(cid:12)(cid:13)(cid:14)(cid:15)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:9)(cid:10)(cid:11)(cid:3)(cid:11)(cid:12)(cid:13)(cid:14)(cid:15)(cid:16)(cid:17)(cid:18)(cid:19)(cid:5)(cid:20)(cid:14)(cid:9)(cid:21)(cid:15)(cid:3)(cid:7)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)(cid:1)(cid:21)(cid:4)(cid:2)(cid:15)(cid:1)(cid:22)(cid:7)(cid:23)(cid:20)(cid:1)(cid:14)(cid:7)(cid:8)(cid:12)(cid:13)(cid:14)(cid:24)(cid:25)(cid:26)(cid:16)(cid:17)(cid:18)(cid:19)(cid:20)(cid:21)(cid:9)(cid:8)(cid:27)(cid:13)(cid:9)(cid:8)(cid:12)(cid:1)(cid:22)(cid:7)(cid:23)(cid:20)(cid:1)(cid:14)(cid:7)(cid:8)(cid:12)(cid:13)(cid:14)512Table 1: The eﬀectiveness of our data reduction system under diﬀerent workloads

CPR

CPR+PCAR

Linux

Windows

Linux

Windows

CPR+PCAR+DOM
Linux

Windows

Impro Reduc

Impro Reduc

Impro Reduc

Impro Reduc

Impro Reduc

Reduc
Impro
55.2% 2.22× 32.1% 1.47× 71.7% 3.53× 62.2% 2.65× 73.5% 3.77× 63.6% 2.75×
45.3% 1.83× 33.3% 1.50× 77.2% 4.39× 45.8% 1.85× 82.5% 5.71× 57.6% 2.36×
30.6% 1.44× 37.8% 1.61× 82.1% 5.59× 55.4% 2.24× 90.2% 10.20× 58.3% 2.40×
58.0% 2.38× 47.0% 1.89× 71.1% 3.46× 61.9% 2.62× 82.2% 5.62× 81.9% 5.52×
70.4% 3.38× 41.8% 1.72× 72.6% 3.65× 48.8% 1.95× 74.2% 3.88× 61.1% 2.57×
66.8% 3.01× 32.2% 1.47× 71.2% 3.47× 44.4% 1.80× 82.5% 5.71× 70.7% 3.41×
22.2% 1.29× 18.2% 1.22× 31.8% 1.47× 30.0% 1.43× 33.5% 1.50× 31.5% 1.46×
56.5% 2.30× 54.5% 2.20× 75.2% 4.03× 77.4% 4.42× 78.1% 4.57× 78.1% 4.57×
68.4% 3.16× 38.4% 1.62× 71.5% 3.51× 55.4% 2.24× 82.2% 5.62× 60.1% 2.51×
55.1% 2.23× 51.1% 2.04× 72.1% 3.58× 62.6% 2.67× 80.3% 5.08× 70.7% 3.41×
21.2% 1.27× 31.1% 1.45× 33.2% 1.50× 42.5% 1.74× 34.8% 1.53× 47.6% 1.91×
65.2% 2.87× 71.9% 3.56× 66.7% 3.00× 72.8% 3.68× 80.2% 5.05× 73.0% 3.70×
42.1% 1.73× 45.1% 1.82× 48.9% 1.96× 47.0% 1.89× 49.1% 1.96× 47.2% 1.89×

System Monitor
System Util
System Daemon
File Sharing
Browser
Oﬃce
Communication
Develop
Programming
Web Server
Database
Virtualization
Other

Table 2: Backtracking results before and after reduction

Table 3: Backtracking results on random POI events

Test Case
distcc_nmap
distcc_passwd
Freesweep
PHPCgi_nmap
Gzip
netcat_bashrc
Passwd
Pbzip
Useradd
Wget_gcc

Raw
84
72
38
56
5
432
17
25
7
19

CPR
84
72
38
56
5
432
17
25
7
19

CPR+PCAR
84
74
38
56
5
441
17
25
7
19

auditing data). Since forward-tracking is the opposite of
backtracking, in our evaluation we focus on backtracking.

We manually produce some test cases during data collec-
tion to generate traces for backtracking. Since we have con-
trol over the test environment of these test cases, we are able
to ﬁgure out the ground truth backtracking results that re-
view the attack/system activity sequences These test cases
are listed in column 1 of Table 2. We ﬁrst perform some
multi-step attacks that exploit system vulnerabilities as test
cases. The ﬁrst two cases are the attacks on Distcc where
the vulnerability (CVE-2004-2687 [3]) allows a remote at-
tacker to execute an arbitrary command, such as nmap and
passwd. The third case is the attack on Freesweep, where a
user downloads a malicious package that will create a back-
door to the user’s system. The fourth case is the attack on
PHPCgi, where the vulnerability (CVE-2012-1823 [4]) allows
a remote attacker to inject malicious commands in a query.
The rest of the test cases are a series of normal system
operations, such as downloading a program with wget and
using gcc to compile and execute it afterwards. These test
cases can also be involved in various multi-step attacks.

For comparison reasons, we perform backtracking on three
copies of the data: the raw data (i.e., the data without any
reduction), the data only reduced by CPR, and the data
reduced by CPR+PCAR.

False
Positives
0

Reduction
Method
CPR
CPR+PCAR 45
Na¨ıve-10s

2778

False Con-
nectivities
0%
3.7%
17%

Additional
Entities
0
1.4%
8.8%

Table 2 shows the connectivity change of backtracking on
data before and after reduction. Since the entities are un-
touched anyway, we do not show the statistics of the entities
here. The connectivity reﬂects the quality of the backtrack-
ing. Multiple aggregatable events only contribute to one
connectivity. As we can see, for these test cases, CPR can
perfectly preserve the connectivity and therefore maintains
a high-quality tracking results. PCAR is a more aggressive
approach, and in two cases it introduces false positive con-
nectivity. A false positive connectivity is caused by the na-
ture that PCAR will enlarge the time window of POI events.
False positives will introduce noise nodes and connectivity in
the resulted dependency graph from backtracking. The en-
larged resulted graph will increase the diﬃculty of tracing the
root cause of anomaly. However, as the false positive rate of
our approach is very low, the impact on forensic analysis is
negligible.

Since in all cases, both CPR and PCAR do not introduce
any missing connectivity in the tracking results, we do not
present the statistics here.

To further investigate the impact upon attack forensics
and compare with the na¨ıve aggregation, we randomly select
20,000 POI events from the data and apply backtracking.
Table 3 illustrates the evaluation results for the na¨ıve aggre-
gation with a 10-seconds time window, CPR and CPR+PCAR.
The false positive column indicates how many cases showed
that, the backtracking results introduce additional connec-
tivity.

The additional connectivity/entities indicate that for the
false positive cases, what is the ratio of the extra connec-
tivity and entities the false positive will introduce. Column
2 is calculated by dividing the number of extra connectiv-
ity introduced in all false positive cases with the number of

513low-level, even simple activities consist of events chained in
multiple dependency hops (e.g., a single ssh login consists
of 4 hops of dependencies). PCAR guarantees that any de-
pendency loss is conﬁned within a single hop from a hot
process. Therefore, although attackers could intentionally
trigger PCAR by injecting multiple hot processes during the
attack, each hot process is usually well-separated from oth-
ers in term of low-level dependencies, and thus PCAR will
unlikely lead to confusion in tracking the attack steps. In
addition, it is also undesirable for an attacker to generate
multiple hot processes, since doing so signiﬁcantly enlarges
its attack footprint, which defeats the intention of evading
detections.
5.2 Generality

Our approach is designed to work with generic system level
event traces, and reduces data volume by removing redun-
dant dependencies within the data. We make little assump-
tion on the speciﬁcs of data, such as system platforms, in-
strumentation techniques, and semantic levels, and therefore
our approach is applicable to a large variety of workloads and
data sources.

We demonstrated the capability of our technique using
data collected from an enterprise security monitoring system
which employs no custom instrumentation. The data covers
Windows and Linux operating systems, and are sourced from
oﬀ-the-shelf kernel auditing and event tracing. Moreover,
our technique is complementary to other existing data re-
duction techniques. CPR and PCAR do not alter data char-
acteristics with respect to platform, instrumentations and
high-level semantics, and thus can function as an indepen-
dent data reduction layer, applied either before or after other
data reductions.

6. RELATED WORK

Very limited research directly addresses the issue of the re-
duction of big-data of attack provenance derived from system-
level audit events. LogGC [31] is the closest work, and it has
three major ideas.
(i) Some system objects such as tem-
porary ﬁles are isolated, have a short life-span, and have
little impact on the dependency analysis, so that such ob-
jects and events can be garbage collected to save space. (ii)
Application-speciﬁc adaptation by using the existing appli-
cation logs. (iii) Application source modiﬁcation to output
ﬁner-grained dependency information.
Ideas (ii) and (iii)
are application speciﬁc adaptation, need human in the loop
to understand and change approach for the speciﬁc applica-
tions. The (i) and our approach are toward a general scheme.
In our study with an enterprise of a few hundred hosts, we
found a large number of diverse applications, so it is not
easy to adopt an application-speciﬁc approach. For general
approaches, we found that our approach oﬀers comparable
reduction as LogGC. Furthermore, the two approaches are
orthogonal. Their approach focuses on object life-span, while
we focus on event causality equivalence. The two approaches
compliment one another and can be further combined.

ProTracer [34] proposes a new system that aims at reduc-
ing log volume for provenance tracing. While their approach
is to build an audit system that is dedicated to provenance
tracing, our approach can be applied on existing audit sys-
tems without any modiﬁcation and our reduced data also
retains the potential to be used by applications other than
forensic analysis.

Figure 10: The runtime memory consumption of our system

ground truth connectivity. Ground truth connectivity is one
that exists in the tracking results on data without reduc-
tion. Similar statistics are calculated for entities and listed
in column 3.

From the results we can see, while CPR can preserve the
high quality of tracking, PCAR will introduce very few false
positives in rare cases (around 0.2%) and the impact is lim-
ited. By contrast, the na¨ıve aggregation introduces false pos-
itives in more than 10% of the cases, which will introduce
considerable noises into the tracking results and therefore
signiﬁcantly degrade the quality of forensic analysis.
4.4 Runtime Performance

While the main purpose of data reduction is to trade data
processing and storage capacities with pre-processing over-
head, it is important to keep the runtime overhead low. Ac-
cording to our algorithm design, both CPR and PCAR are
linear in time complexity. We only need to scan all events
once and update the states with hash tables. We also proﬁle
the runtime memory and CPU consumption of our approach.
Since the data is stored/processed on a daily basis, our pro-
ﬁling lasts one day as well. Figure 10 illustrates the memory
consumption of our complete system (CPR+PCAR+domain
knowledge extension) over time. The data rate is around
4,000 events per second. As we can see from the ﬁgure, the
memory consumption ﬂuctuates. The reason is two-fold. On
one hand, our system continues storing new events and up-
dating new states; on the other hand, it continues to output
the events that cannot be aggregated and drops events that
are either aggregated or discarded. Overall, the memory con-
sumption remains under 2GB, which is a small overhead for
any commercial server. As for the CPU consumption, a sin-
gle 2.5G Hz core can easily handle such a data rate. Thus,
the CPU overhead is also minor.

5. DISCUSSION
5.1 Attack Resilience

Serving as the underlying support for forensic analysis, it
is important that our technique is resilient to attacks, such
as data evasions.

Both CPR and PCAR are resilient against evasion attacks.
Since CPR guarantees that no causal dependency is lost after
data reduction, an attacker simply cannot exploit CPR to
distort the reality by any means. Although PCAR does
result in dependency loss during data reduction, it is very
diﬃcult to exploit such a loss to cover any meaningful mali-
cious activities. Because our system monitors events at fairly

(cid:2)(cid:3)(cid:2)(cid:2)(cid:4)(cid:2)(cid:2)(cid:5)(cid:2)(cid:2)(cid:6)(cid:7)(cid:2)(cid:2)(cid:6)(cid:8)(cid:2)(cid:2)(cid:6)(cid:9)(cid:2)(cid:2)(cid:7)(cid:6)(cid:2)(cid:2)(cid:2)(cid:7)(cid:10)(cid:4)(cid:9)(cid:6)(cid:2)(cid:6)(cid:7)(cid:6)(cid:10)(cid:6)(cid:4)(cid:6)(cid:9)(cid:7)(cid:2)(cid:7)(cid:7)(cid:7)(cid:10)(cid:1)(cid:2)(cid:3)(cid:4)(cid:5)(cid:6)(cid:7)(cid:8)(cid:4)(cid:9)(cid:10)(cid:11)(cid:3)(cid:12)(cid:13)(cid:14)(cid:4)(cid:9)(cid:7)(cid:15)(cid:1)(cid:16)(cid:17)(cid:18)(cid:11)(cid:9)(cid:7)(cid:19)(cid:14)(cid:3)(cid:2)(cid:7)(cid:15)(cid:20)(cid:4)(cid:11)(cid:5)(cid:10)(cid:17)514The other data reduction works do not consider preserv-
ing the dependency path and thus are not applicable to our
problem. Some works on data aggregation aim to reduce
communication cost and improving data privacy [17, 19, 20,
21, 26, 24, 38]. In contrast to these works, we focus on reduc-
ing system auditing data while supporting forensic analysis,
particularly backtracking and forward-tracking. There are
also other studies exploiting graph techniques for data reduc-
tion [18, 37]. However, none of them leverages dependency
in data reduction.

Since the assumptions and usage scenarios in our approach
are very diﬀerent from these previous data reduction, it is
hard to compare them side-by-side. Our work has its unique
contribution in the novelty of the key techniques and is com-
plementary to other reduction techniques.

Forensic analysis plays an important role in security. King
and Chen [27] proposed a backtracking technique to perform
intrusion analysis by automatically reconstructing a chain
of events in a dependency graph. Following this approach,
various forensic analysis techniques have been developed in
various scenarios such as recovering an intrusion [23], worm
break-in detection [25], forensic analysis in virtualized envi-
ronments [29], binary level attack provenance [30], risk anal-
ysis in networks [35], and ﬁle system intrusion detection [36].
Ma et al. [33] proposed a Windows audit logging technique
that can provide accurate traces for forensic analysis

Although there are diﬀerent tracking techniques, the key
concept of exploiting the dependency between system events
for analysis remains the same. Dependency graphs have
been widely used in system and security studies. Besides
the usage of a dependency graph in forensic analysis [27,
23], researchers have leveraged dependency graphs to per-
form code generation on multiprocessor systems [15], identify
user clicks in http requests [32], predict system failures [41],
diagnose networking failures [40], assess attacks in enter-
prises [22], perform malware classiﬁcation [39], and detect
security failures in the cloud environment [16].

7. CONCLUSION

We presented a novel approach that exploits dependency
among system events to reduce the size of data without com-
promising the accuracy of the forensic analysis. Core to our
contribution, we proposed the concept of trackability, which
determines causality relation among system events. By ag-
gregating events under the same trackability, our approach
could reduce a large potion of data while preserving events
relevant to a forensic analysis.
Incorporated with domain
knowledges speciﬁc to system process behaviours, our pro-
totype implemented data reduction for two types of forensic
analyses: backtracking and forward-tracking.

Evaluated over datasets gathered from a real-world enter-
prise environment, our results show that our approach im-
proves space capacity by 3.4 times with no or little accuracy
compromise. Although the overall space and computational
requirements for large-scale data analysis remain challeng-
ing, we hope our data reduction approach will bring forensic
analysis in a big data context closer to become practical.

8. ACKNOWLEDGEMENT

We would like to thank our shepherd Tudor Dumitras
and the anonymous reviewers for their insightful and de-
tailed comments. Zhang Xu and Haining Wang were par-

tially supported by ONR grant N00014-13-1-0088 and NSF
grant CNS-1618117.

9. REFERENCES
[1] Anthem cyber attack.

http://abcnews.go.com/Business/
anthem-cyber-attack-things-happen-personal-information/
story?id=28747729.

[2] Case study: The Home Depot data breach. https://

www.sans.org/reading-room/whitepapers/casestudies/
case-study-home-depot-data-breach-36367.

[3] CVE-2004-2687. https://cve.mitre.org/cgi-bin/

cvename.cgi?name=CVE-2004-2687.

[4] CVE-2012-1823. https://cve.mitre.org/cgi-bin/

cvename.cgi?name=CVE-2012-1823.

[5] Cyber kill chain. http://www.lockheedmartin.com/us/

what-we-do/information-technology/cybersecurity/
tradecraft/cyber-kill-chain.html.

[6] Ebay inc. to ask Ebay users to change passwords.

http://blog.ebay.com/
ebay-inc-ask-ebay-users-change-passwords/.

[7] Etw events in the common language runtime.

https://msdn.microsoft.com/en-us/library/
ﬀ357719(v=vs.110).aspx.

[8] The Linux audit framework.

https://www.suse.com/documentation/sled10/audit
sp1/data/book sle audit.html.

[9] OPM government data breach impacted 21.5 million.

http://www.cnn.com/2015/07/09/politics/
oﬃce-of-personnel-management-data-breach-20-million/.

[10] Sony reports 24.5 million more accounts hacked.

http://www.darkreading.com/attacks-and-breaches/
sony-reports-245-million-more-accounts-hacked/d/
d-id/1097499?

[11] Stuxnet. https://en.wikipedia.org/wiki/Stuxnet.
[12] Target hit by credit-card breach.

http://online.wsj.com/news/articles/
SB10001424052702304773104579266743230242538.

[13] Transparent computing. http:

//www.darpa.mil/program/transparent-computing.

[14] Trustwave Global Security Report, 2015.

https://www2.trustwave.com/rs/815-RFM-693/
images/2015 TrustwaveGlobalSecurityReport.pdf.

[15] J. A. Ambrose, J. Peddersen, S. Parameswaran,

A. Labios, and Y. Yachide. Sdg2kpn: System
dependency graph to function-level kpn generation of
legacy code for mpsocs. In Proceedings of IEEE
ASP-DAC’14, pages 267–273.

[16] S. Bleikertz, C. Vogel, and T. Groß. Cloud radar: near

real-time detection of security failures in dynamic
virtualized infrastructures. In Proceedings of ACM
ACSAC’14, pages 26–35.

[17] C. Castelluccia, E. Mykletun, and G. Tsudik. Eﬃcient

aggregation of encrypted data in wireless sensor
networks. In Proceedings of IEEE MobiQuitous’05,
pages 109–117.

[18] A. P. Chapman, H. V. Jagadish, and P. Ramanan.
Eﬃcient provenance storage. In Proceedings of the
ACM SIGMOD’08, pages 993–1006.

[19] S. Cheng and J. Li. Sampling based (epsilon,

delta)-approximate aggregation algorithm in sensor

515networks. In Proceedings of IEEE ICDCS’09, pages
273–280.

[20] S. Cheng, J. Li, Q. Ren, and L. Yu. Bernoulli sampling

based (ε, δ)-approximate aggregation in large-scale
sensor networks. In Proceedings of IEEE
INFOCOM’10, pages 1181–1189.

[31] K. H. Lee, X. Zhang, and D. Xu. Loggc: garbage

collecting audit log. In Proceedings of ACM CCS’13,
pages 1005–1016.

[32] J. Liu, C. Fang, and N. Ansari. Identifying user clicks

based on dependency graph. In Proceedings of IEEE
WOCC’14, pages 1–5.

[21] G. Cormode and K. Yi. Tracking distributed

[33] S. Ma, K. H. Lee, C. H. Kim, J. Rhee, X. Zhang, and

aggregates over time-based sliding windows. In
Proceedings of SSDBM’12, pages 416–430.

[22] N. Ghosh, I. Chokshi, M. Sarkar, S. K. Ghosh, A. K.
Kaushik, and S. K. Das. Netsecuritas: An integrated
attack graph-based security assessment tool for
enterprise networks. In Proceedings of ACM
ICDCN’15.

[23] A. Goel, K. Po, K. Farhadi, Z. Li, and E. De Lara.

The taser intrusion recovery system. In Proceedings of
ACM SOSP’05, pages 163–176.

[24] M. Gupta, J. Gao, X. Yan, H. Cam, and J. Han. Top-k

interesting subgraph discovery in information
networks. In Proceedings of IEEE ICDE’14, pages
820–831.

[25] X. Jiang, A. Walters, D. Xu, E. H. Spaﬀord,

F. Buchholz, and Y.-M. Wang. Provenance-aware
tracing ofworm break-in and contaminations: A
process coloring approach. In Proceedings of IEEE
ICDCS’06.

[26] J. Jose and S. Manoj Kumar. Energy eﬃcient

recoverable concealed data aggregation in wireless
sensor networks. In Proceedings of IEEE ICE-CCN’13,
pages 322–329.

[27] S. T. King and P. M. Chen. Backtracking intrusions.

In Proceedings of ACM SOSP’03.

[28] S. T. King, Z. M. Mao, D. G. Lucchetti, and P. M.
Chen. Enriching intrusion alerts through multi-host
causality. In Proceedings of the Network and
Distributed System Security Symposium, NDSS 2005,
San Diego, California, USA, 2005.

[29] S. Krishnan, K. Z. Snow, and F. Monrose. Trail of

bytes: eﬃcient support for forensic analysis. In
Proceedings of ACM CCS’10, pages 50–60.

[30] K. H. Lee, X. Zhang, and D. Xu. High accuracy attack

provenance via binary-based execution partition. In
Proceedings of NDSS’13.

D. Xu. Accurate, low cost and instrumentation-free
security audit logging for windows. In Proceedings of
ACM ACSAC’15.

[34] S. Ma, X. Zhang, and D. Xu. Protracer: Towards

practical provenance tracing by alternating between
logging and tainting. In Proceedings of NDSS’16.

[35] M. Rezvani, A. Ignjatovic, E. Bertino, and S. Jha.

Provenance-aware security risk analysis for hosts and
network ﬂows. In Proceedings of IEEE NOMS’14,
pages 1–8.

[36] S. Sitaraman and S. Venkatesan. Forensic analysis of
ﬁle system intrusions using improved backtracking. In
Proceedings of IEEE IWIA’05, pages 154–163.

[37] Y. Xie, D. Feng, Z. Tan, L. Chen, K.-K.

Muniswamy-Reddy, Y. Li, and D. D. Long. A hybrid
approach for eﬃcient provenance storage. In
Proceedings of ACM CIKM’12, pages 1752–1756.

[38] X. Xu, R. Ansari, A. Khokhar, and A. V. Vasilakos.

Hierarchical data aggregation using compressive
sensing (hdacs) in wsns. ACM Transactions on Sensor
Networks, 11(3):45, 2015.

[39] M. Zhang, Y. Duan, H. Yin, and Z. Zhao.

Semantics-aware android malware classiﬁcation using
weighted contextual api dependency graphs. In
Proceedings of ACM SIGSAC’14, pages 1105–1116.

[40] M. Zibaeenejad and J. Thistle. Dependency graph: an

algorithm for analysis of generalized parameterized
networks. In Proceedings of IEEE ACC’15, pages
696–702.

[41] T. Zimmermann and N. Nagappan. Predicting

subsystem failures using dependency graph
complexities. In Proceedings of ISSRE’07, pages
227–236.

516