2014 IEEE Symposium on Security and Privacy

Automating Efﬁcient RAM-Model Secure

Computation

Chang Liu

Yan Huang

Elaine Shi

Jonathan Katz Michael Hicks

University of Maryland

Email: {liuchang, yhuang, elaine, jkatz, mwh}@cs.umd.edu

College Park, Maryland 20742

Abstract—RAM-model secure computation addresses the in-
herent limitations of circuit-model secure computation considered
in almost all previous work. Here, we describe the ﬁrst automated
approach for RAM-model secure computation in the semi-
honest model. We deﬁne an intermediate representation called
SCVM and a corresponding type system suited for RAM-model
secure computation. Leveraging compile-time optimizations, our
approach achieves order-of-magnitude speedups compared to
both circuit-model secure computation and the state-of-art RAM-
model secure computation.

I.

INTRODUCTION

Secure computation allows mutually distrusting parties to
make collaborative use of their local data without harming pri-
vacy of their individual inputs. Since Yao’s seminal paper [31],
research on secure two-party computation—especially in the
semi-honest model we consider here—has ﬂourished, resulting
in ever more efﬁcient protocols [4], [10], [15], [32] as well
as several practical implementations [6], [11]–[13], [16], [21].
Since the ﬁrst system for general-purpose secure two-party
computation was built in 2004 [21], efﬁciency has improved
substantially [4], [13].

Almost all previous implementations of general-purpose
secure computation assume the underlying computation is
represented as a circuit. While theoretical developments using
circuits are sensible (and common), compiling typical pro-
grams, which assume a von Neumann-style Random Access
Machine (RAM) model, to efﬁcient circuits can be challeng-
ing. One signiﬁcant challenge is handling dynamic memory
accesses to an array in which the memory location being
read/written depends on secret inputs. A typical program-to-
circuit compiler typically makes an entire copy of the array
upon every dynamic memory access, thus resulting in a huge
circuit when the data size is large. Theoretically speaking,
generic approaches for translating RAM programs into circuits
incur, in general, O(T N ) blowup in efﬁciency, where T is an
upper bound on the program’s running time, and N is the
memory size.

To address the above limitations, researchers have more re-
cently considered secure computation that works directly in the
RAM model [10], [20]. The key insight is to rely on Oblivious
RAM (ORAM) [8] to enable dynamic memory access with
poly-logarithmic cost, while preventing information leakage
through memory-access patterns. Gordon et al. [10] observed
a signiﬁcant advantage of RAM-model secure computation

(RAM-SC) in the setting of repeated sublinear-time queries
(e.g., binary search) on a large database. By amortizing the
setup cost over many queries, RAM-SC can achieve amortized
cost asymptotically close to the run-time of the underlying
program in the insecure setting.

A. Our Contributions

We continue work on secure computation in the RAM
model, with the goal of providing a complete system that takes
a program written in a high-level language and compiles it to
a protocol for secure two-party computation of that program.1
In particular, we

• Deﬁne an intermediate representation (which we call
SCVM) suitable for efﬁcient two-party RAM-model se-
cure computation;

• Develop a type system ensuring that any well-typed pro-
gram will generate a RAM-SC protocol secure in the
semi-honest model, if all subroutines are implemented
with a protocol secure in the semi-honest model.

• Build an automated compiler that transforms programs
written in a high-level language into a secure two-party
computation protocol, and integrate compile-time opti-
mizations crucial for improving performance.

We use our compiler to compile several programs includ-
ing Dijkstra’s shortest-path algorithm, KMP string matching,
binary search, and more. For moderate data sizes (up to the
order of a million elements), our evaluation shows a speedup
of 1–2 orders of magnitude as compared to standard circuit-
based approaches for securely computing these programs. We
expect the speedup to be even greater for larger data sizes.

B. Techniques

As explained in Sections II-A and III, the standard imple-
mentation of RAM-SC entails placing all data and instructions
inside a single Oblivious RAM. The secure evaluation of one
instruction then requires i) fetching instruction and data from
ORAM; and ii) securely executing the instruction using a
universal next-instruction circuit (similar to a machine’s ALU

1Note that Gordon et al. [10] do not provide such a compiler; they only
implement RAM-model secure computation for the particular case of binary
search.

© 2014, Chang Liu. Under license to IEEE.
DOI 10.1109/SP.2014.46

623

unit). This approach is costly since each step must be done
using a secure-computation sub-protocol.
An efﬁcient representation for RAM-SC. Our type system
and SCVM intermediate representation are capable of express-
ing RAM-SC tasks more efﬁciently by avoiding expensive
next-instruction circuits and minimizing ORAM operations
when there is no risk to security. These language-level capabil-
ities allow our compiler to apply compile-time optimizations
that would otherwise not be possible. Thus, we not only
obtain better efﬁciency than circuit-based approaches, but we
also achieve order-of-magnitude performance improvements in
comparison with straightforward implementations of RAM-SC
(see Section VI-C).
Program-trace simulatability. A well-typed program in our
language is guaranteed to be both instruction-trace oblivious
and memory-trace oblivious. Instruction-trace obliviousness
ensures that the values of the program counter during execution
of the protocol do not leak information about secret inputs
other than what is revealed by the output of the program.
As such, the parties can avoid securely evaluating a universal
next-instruction circuit, but can instead simply evaluate a
circuit corresponding to the current instruction. Memory-trace
obliviousness ensures that memory accesses observed by one
party during the protocol’s execution similarly do not leak
information about secret inputs other than what is revealed
by the output. In particular, if access to some array does not
depend on secret information (e.g., it is part of a linear scan
of the array), then the array need not be placed into ORAM.
We formally deﬁne the security property ensured by our
type system as program-trace simulatability. We deﬁne a
mechanism for compiling programs to protocols that rely on
certain ideal functionalities. We prove that if every such ideal
functionality is instantiated with a semi-honest secure protocol
computing that functionality,
then any well-typed program
compiles to a semi-honest secure protocol computing that
program.
Additional language features. SCVM supports several other
it permits reactive computations by
useful features. First,
allowing output not only at the end of the program’s execution,
but also while it is in progress. Our notation of program-trace
simulatability also ﬁts this reactive model of computation.

SCVM also integrates state-of-the-art optimization tech-
niques that have been suggested previously in the literature.
For example, we support public,
local, and secure modes
of computation, a technique recently explored (in the circuit
model) by Kerschbaum [15] and Rastogi et al. [25] Our
compiler can identify and encode portions of computation that
can be safely performed in the clear or locally by one of the
parties, without incurring the cost of a secure-computation sub-
protocol.

Our SCVM intermediate representation generalizes circuit-
model approaches. For programs that do not rely on ORAM,
our compiler effectively generates an efﬁcient circuit-model
secure-computation protocol. This paper focuses on the design
of the intermediate representation language and type system for
RAM-model secure computation, as well as the compile-time
optimization techniques we apply. Our work is complementary

to several independent, ongoing efforts focused on improving
the cryptographic back end.

II. BACKGROUND AND RELATED WORK

A. RAM-Model Secure Computation

In this section, we review some background for RAM-
model secure computation. Our treatment is adapted from that
of Gordon et al. [10], with notation adjusted for our purposes.
We compare our scheme against the one presented here in
Section VI-C.

A key underlying building block of RAM-model secure
computation is Oblivious RAM (ORAM) which was initially
proposed by Goldreich and Ostrovsky [8] and later improved
in a sequence of works [9], [17], [27], [28], [30]. ORAM is
a cryptographic primitive that hides memory-access patterns
by randomly reshufﬂing data in memory. With ORAM, each
memory read or write operation incurs poly log n actual mem-
ory accesses.

We introduce some notation to describe the execution of a
RAM program. We let mem refer to the memory maintained
by the program. We let (pc, raddr, waddr, wdata, reg) ←
U (I, reg, rdata) denote a single application of the next-
instruction circuit (like a CPU’s ALU), taking as input the
current instruction I, the current register contents reg, and a
value rdata (representing a value just fetched from memory),
and outputting the next value of the program counter pc, an
updated register ﬁle reg, a read address raddr, a write address
waddr, and a value wdata to write to location mem[waddr].

mem[i]

pc
reg

I
U

rdata
wdata
raddr
waddr

the memory value at index i
the current program counter
an O(1)-sized set of registers
an instruction
the next-instruction circuit
the last value read from memory
the value to write to memory
a read address
a write address

Existing RAM-model secure computation proceeds as in
Figure 1. The entire memory denoted mem, containing both
program instructions and data, is placed in ORAM, and the
ORAM is secret-shared between the two participating parties
as discussed above, e.g., using a simple XOR-based secret-
sharing scheme. With ORAM, a memory access thus requires
each party to access the elements of their respective arrays
at pseudorandom locations (the addresses are dictated by the
ORAM algorithm), and the value stored at each position is then
obtained by XORing the values read by each of the parties.
Alternatively, the server can hold an encryption of the ORAM
array, and the client holds the key. The latter was done by
Gordon et al. to ensure that one party holds only O(1) state.
All CPU states, including pc, reg, I, rdata, wdata, raddr,
and waddr, are also secret-shared between the two parties.

In Figure 1, each step of the computation must be done
using some secure computation subprotocol. In particular, SC-
U is a secure computation protocol that securely evaluates
the universal next
instruction circuit, and SC-ORAM is a
secure computation protocol that securely evaluates the ORAM

624

/* All variables, including mem, pc, I, rdata, reg, wdata, raddr, and waddr are secret-shared between the two parties. */
For i = 1, 2, . . . , t where t is the maximum run-time of the program:

//Protocol SC-ORAM
//Protocol SC-U
//Protocol SC-ORAM
//Protocol SC-ORAM

instr. fetch phase:
CPU phase:
data read phase:
data write phase:

I ← ORAM.Read(mem, pc)
(pc, raddr, waddr, wdata, reg) ← U(I, reg, rdata)
rdata ← ORAM.Read(mem, raddr)
ORAM.Write(mem, waddr, wdata)

Fig. 1: Generic RAM-model secure computation. The parties repeatedly perform secure computation to obtain the next
instruction I, execute that instruction, and then read/write from/to main memory. All data are secret-shared.

Scenario

1 Repeated sublinear queries over a large dataset (e.g.,

binary search, range query, shortest path query)

2 One-time computation over a large dataset

Potential beneﬁts of RAM-model secure computation
• Amortize preprocessing cost over multiple queries
• Achieve sublinear amortized cost per query
Avoid paying O(n) cost per dynamic memory access

TABLE I: Two main scenarios and advantages of RAM-model secure computation

algorithm. For ORAM.Read, each party supplies a secret share
of the raddr, and during the course of the protocol,
the
ORAM.Read protocol will emit obfuscated addresses for each
party to read from. At the end of the protocol, each party
obtains a share of the fetched data. For ORAM.Write, each
party supplies a secret share of waddr and wdata, and during
the course of the protocol, the ORAM.Read protocol will emit
obfuscated addresses for each party to write to, and secret
shares of values to write to those addresses.

Scenarios for RAM-model secure computation. While Gor-
don et al. describe RAM-model secure computation mainly for
the amortized setting, where repeated computations are carried
out starting from a single initial dataset, we note that RAM-
model secure computation can also be meaningful for one-time
computation on large datasets, since a straightforward RAM-
to-circuit compiler would incur linear (in the size of dataset)
overhead for every dynamic memory access whose address
depends on sensitive inputs. Table I summarizes the two main
scenarios for RAM-model secure computation, and potential
advantages of using the RAM model in these cases.

B. Other Related Work

Automating and optimizing circuit-model secure computa-
tion. As mentioned earlier, a number of recent efforts have
focused on automating and optimizing secure computation
in the circuit model. Intermediate representations for secure
computation have been developed in the circuit model, e.g.,
[16]. Mardziel et al. [22] proposed a way to reason about the
amount of information declassiﬁed by the result of a secure
computation, and Rastogi et al. [25] used a similar analysis to
infer intermediate values that can be safely declassiﬁed without
revealing further information beyond what is also revealed by
the output. These analyses can be applied to our setting as well
(though their results would not necessarily be accepted by our
type system, whose improved precision would be future work).
Concurrently with our work, Rastogi et al. [24] developed
Wysteria, a programming language for mixed mode secure
multiparty computations, which consist of local computations
intermixed with joint, secure ones. While this high-level idea is

similar to our work, the details are very different. For example,
they do not provide a simulatability theorem (they propose to
accept results from the analysis of Rastogi et al. [25] ) and are
focused more at usability.

Zahur and Evans [32] also attempted to address some
of the drawbacks of circuit-model secure computation. Their
approach, however, focuses on designing efﬁcient circuit struc-
tures for speciﬁc data structures, such as stacks and queues,
and do not generalize for arbitrary programs. Many of the
programs we use in our experiments are not supported by their
approach.

Trace-oblivious type systems. Our type system is trace-
oblivious. Liu et al. [18] propose a memory-trace oblivious
type system for a secure-processor application. In compar-
ison, our program trace also includes instructions. Further,
Liu et al. propose an indistinguishability-based trace-oblivious
notion which is equivalent to a simulation-based notion in
their setting. In the secure computation setting, however, an
indistinguishability-based trace-oblivious notion is not equiv-
alent to simulation-based trace obliviousness due to the de-
classiﬁcation of computation outputs. We therefore deﬁne a
simulation-based trace-oblivious notion in our paper which is
necessary to ensure the security of the compiled two-party
protocol. Other work has proposed type systems that track side
channels as traces. For example, Agat’s work traces operations
in order to avoid timing leaks [1].

III. TECHNICAL OVERVIEW: COMPILING FOR

RAM-MODEL SECURE COMPUTATION

This section describes our approach to optimize RAM-
model secure computation. Our key idea is use static anal-
ysis during compilation to minimize the use of heavyweight
cryptographic primitives such as garbled circuits and ORAM.

A. Instruction-Trace Obliviousness

The standard RAM-model secure computation protocol
described in Section II-A is relatively inefﬁcient because it
requires a secure-computation sub-protocol to compute the

625

universal next-instruction circuit U. This circuit has large
size, since it must interpret every possible instruction. In our
solution, we will avoid relying on a universal next-instruction
circuit, and will instead arrange things so that we can securely
evaluate instruction-speciﬁc circuits.

Note that

it

is not secure,

to reveal what
instruction is being carried out at each step in the execution of
some program. As a simple example, consider a branch over
a secret value s:

in general,

if(s) x[i]:=a+b; else x[i]:=a-b

Depending on the value of s, a different instruction (i.e.,
add or subtract) will be executed. To mitigate such an
implicit information leak, our compiler transforms a program
to an instruction-trace oblivious counterpart, i.e., a program
whose program-counter value (which determines which in-
struction will be executed next) does not depend on secret
information. The key idea there is to use a mux operation to
rewrite a secret if-statement. For example, the above code can
be re-factored to the following:

t1 := s;
t2 := a+b;
t3 := a-b;
t4 := mux(t1, t2, t3);
x[i] := t4

At every point during the above computation, the instruc-
tion being executed is pre-determined, and so does not leak in-
formation about sensitive data. Instruction-trace obliviousness
is similar to program-counter security proposed by Molnar et
al. [23] (for a different application).

B. Memory-Trace Obliviousness

Using ORAM for memory accesses is also a heavyweight
operation in RAM-model secure computation. The standard
approach is to place all memory in a single ORAM, thus
incurring O(poly log n) cost per data operation, where n is
a bound on the size of the memory.

In the context of securing remote execution against physical
attacks, Liu et al. [18] recently observe that not all access
patterns of a program are sensitive. For example, a findmax
program that sequentially scans through an array to ﬁnd
the maximum element has predictable access patterns that
do not depend on sensitive inputs. We propose to apply a
similar idea to the context of RAM-model secure computation.
Our compiler performs static analysis to detect safe memory
accesses that do not depend on secret inputs. In this way, we
can avoid using ORAM when the access pattern is independent
of sensitive inputs. It is also possible to store various subsets
of memory (e.g., different arrays) in different ORAMs, when
information about which portion of memory (e.g., which array)
is being accessed does not depend on sensitive information.

C. Mixed-Mode Execution

We also use static analysis to partition a program into code
blocks, and then for each code block use either a public, local,
or secure mode of execution (described next). Computation in

626

public or local modes avoids heavyweight secure computation.
In the intermediate language, each statement is labeled with
its mode of execution.

Public mode. Statements computing on publicly-known vari-
ables or variables that have been declassiﬁed in the middle
of program execution can be performed by both parties inde-
pendently, without having to resort to a secure-computation
protocol. Such statements are labeled P. For example, the
loop iterators (in lines 1, 3, 10) in Dijkstra’s algorithm (see
Figure 2) do not depend on secret data, and so each party can
independently compute them.

Local mode. For statements computing over Alice’s variables,
public variables, or previously declassiﬁed variables, Alice
can perform the computation independently without interacting
with Bob (and vice versa). Here we crucially rely on the fact
that we assume semi-honest behavior. Alice-local statements
are labeled A, and Bob-local statements are labeled B.

Secure mode. All other statements that depend on variables
that must be kept secret from both Alice and Bob will be
computed using secure computation, making ORAM accesses
along the way if necessary. Such statements are labeled O (for
“oblivious”).

D. Example: Dijkstra’s Algorithm

In Figure 2, we present a complete compilation example for
part of Dijkstra’s algorithm. Here one party, Alice, has a private
graph represented by a pairwise edge-weight array e[][] and
the other party, Bob, has a private source/destination pair. Bob
wishes to compute the shortest path between his source and
destination in Alice’s graph. The ﬁgure shows the code that
computes shortest paths (Bob’s inputs are elided).

Our speciﬁc implementation of Dijkstra’s algorithm uses
three arrays, a dis array which keeps track of the current
shortest distance from the source to any other node; an edge-
weight array orame which is initialized by Alice’s local array
e, and an indicator array vis, denoting whether each node has
been visited. In this case, our compiler places arrays vis and
e in separate ORAMs, but does not place array dis in ORAM
since access to dis always follows a sequential pattern.

Note that parts of the algorithm can be computed publicly.
For example, all the loop iterators are public values; therefore,
loop iterators need not be secret-shared, and each party can
independently compute the current loop iteration. The remain-
ing parts of program all require ORAM accesses; therefore,
our compiler annotates these instructions to be run in secure
mode, and generates equivalent instruction- and memory-trace
oblivious target code.

IV. SCVM LANGUAGE

This section presents SCVM, our language for RAM-model

secure computation, and presents our formal results.

In Section IV-A, we present SCVM’s formal syntax. In
Section IV-B, we give a formal, ideal world semantics for
SCVM that forms the basis of our security theorem. Informally,
each party provides their inputs to an ideal functionality F

1 for(i = 0; i < n; ++i) {
2

int bestj = -1; bestdis = -1;
for(int j=0; j<n; ++j) {

if( ! vis[j] && (bestj < 0

|| dis[j] < bestdis))

bestj = j;
bestdis = dis[j];

}
vis[bestj] = 1;
for(int j=0; j<n; ++j) {

if( !vis[j] && (bestdis +
e[bestj][j] < dis[j]))
dis[j] = bestdis + e[bestj][j];

3

4

5

6

7

8

9

10

11

12

13

}

14
15 }

O: orame :=oram( e );
P: i :=0; P: cond1 := i < n ;
P:while( cond1 ) do

O:bestj:=-1; O:bestdis:=-1;
P: j :=0; P: cond2 := j < n ;
P:while( cond2 ) do

O:t1:= vis [ j ]; O:t2:=!t1; O:t3:=best<0;
O:t4:= dis [ j ]; O:t5:=t4<bestdis;
O:t6:=t3||t5; O:cond3:=t2 && t6;
O:best
:=mux(cond3, j , best);
O:bestdis:=mux(cond3, t4, bestdis);
P: j := j +1; P: cond2 := j < n ;;

O: vis [ bestj ]:=1;
P: j :=0; P: cond2 := j < n ;
P:while(cond2) do

O:t7:= vis [ j ]; O:t8:=!t7;
O:idx:=bestj*n; O:idx:=idx+ j ; O:t9:= orame [idx];
O:t10:=bestdis + t9; O:t11:= dis [ j ];
O:t12:=t10 < t11; O:cond4:=t8 && t12
O:t13:=mux(cond4, t10, t11); O: dis [ j ]:=t13;
P: j := j +1; P: cond2 := j < n ;

Fig. 2: Compilation example: Part of Dijkstra’s shortest-path algorithm. The code on the left is compiled to the annotated code
on the right. Array variable e is Alice’s local input array containing the graph’s edge weights; Bob’s input, a source/destination
pair, is not used in this part of the algorithm. Array variables vis and orame are placed in ORAMs. Array variable dis
is placed in non-oblivious (but secret-shared) memory. (Prior to the shown code, vis is initialized to all zeroes except that
vis[source]—where source is Bob’s input—is initialized to 1, and dis[i] is initialized to e[source][i].) Variables n ,
i , j and others boxed in white background are public variables. All other variables are secret-shared between the two parties.

is allowed to see;

that computes the result and returns to each party its result
and a trace of events it
these events
include instruction fetches, memory accesses, and declassiﬁ-
cation events, which are results computed from both parties’
data. Section IV-C formally deﬁnes our security property, Γ-
simulatability. Informally, a program is secure if each party,
starting with its own inputs, memory, the program code, and
its trace of declassiﬁcation events, can simulate (in polynomial
time) its observed instruction traces and memory traces without
knowing the other party’s data. We present a type system
for SCVM programs in Section IV-D, and in Theorem 1
prove that well-typed programs are Γ-simulatable. Theorem 2
additionally shows that well-typed programs will not get stuck,
e.g., because one party tries to access memory unavailable
in Section IV-E we deﬁne a hybrid world
to it. Finally,
functionality that more closely models SCVM’s implemented
semantics using ORAM, garbled circuits, etc. and prove that
for Γ-simulatable programs, the hybrid-world protocol securely
implements the ideal functionality. The formal results are
summarized in Figure 3.

Fig. 3: Formal results.

A. Syntax

The syntax of SCVM is given in Figure 4. In SCVM, each
variable and statement has a security label from the lattice
{P,A,B,O}, where (cid:118) is deﬁned to be the smallest partial order
such that P (cid:118) l (cid:118) O for l ∈ {A, B}. The label of each variable
indicates whether its memory location should be public, known
to either Alice or Bob (only), or secret. For readability, we
do not distinguish between oblivious secret arrays and non-
oblivious secret arrays at this point, and simply assume that
all secret arrays are oblivious. Support for non-oblivious, secret
arrays will be added in Section V.

627

Type checking	
Γ, pc S S is	
 Γ-progressive	
Thm 1	
Thm 2	
S is 	
Γ-simulatable	
πG implements F Thm 3	
[Canetti ‘00]	
If ρ implements G,     then πρ implements F πG:  hybrid-world protocol	
πρ :  real-world protocol	
F :  ideal-world functionality	
Variables
Security Labels
Numbers
Operation
Expressions

Statements

x, y, z
l
n
op
e

s

Labeled Statements S

Vars
SecLabels = {P, A, B, O}
Nat

∈
∈
∈
::= + | − | ...
::= x | n | x op x |
::= skip | x := e | x[x] := x |

x[x] | mux(x, x, x)
if (x) then S else S |
while (x) do S |
x := declassl(y) |
x := oram(y)

::= l : s | S; S

Fig. 4: Syntax of SCVM

An information-ﬂow control type system, which we discuss
in Section IV-D, enforces that information can only ﬂow from
low (i.e., lower in the partial order) security variables to high
security variables. For example, for a statement x := y to be
secure, y’s security label should be less than or equal to x’s
security label. An exception is the declassiﬁcation statement
x := declassl(y) which may declassify a variable y labeled O
to a variable x with lower security label l.

The label of each statement indicates the statement’s mode
of execution. A statement with the label P is executed in public
mode, where both Alice and Bob can see its execution. A
statement with the label A or B is executed in local mode, and
is visible to only Alice or Bob, respectively. A statement with
the label O is executed securely, so both Alice and Bob know
the statement was executed but do not learn the underlying
values that were used.

Most SCVM language features are standard. We highlight
the statement x := oram(y), by which variable x is assigned
to an ORAM initialized with array y’s contents, and the
expression mux(x0, x1, x2), which evaluates to either x1 or
x2, depending on whether x0 is 0 or 1.

B. Semantics

We deﬁne a formal semantics for SCVM programs which
we think of as deﬁning a computation carried out, on Alice and
Bob’s behalf, by an ideal functionality F. However, as we fore-
shadow throughout, the semantics is endowed with sufﬁcient
structure that it can be interpreted as using the mechanisms
(like ORAM and garbled circuits) described in Sections II
and III. We discuss such a hybrid world interpretation more
carefully in Section IV-E and prove it also satisﬁes our security
properties.

Memories and types. Before we begin, we consider a few
auxiliary deﬁnitions given in Figure 5. A memory M is a
partial map from variables to value-label pairs. The value is
either a natural number n or an array m, which is a partial map
from naturals to naturals. The security labels l ∈ {P,A,B,O}
indicate the conceptual visibility of the value as described
earlier. Note that in a real-world implementation, data labeled
O is stored in ORAM and secret-shared between Alice and
Bob, while other data is stored locally by Alice or Bob. We

sometimes ﬁnd it convenient to project memories whose values
are visible at particular labels:
Deﬁnition 1 (L-projection). Given memory M and a set of
security labels L, we write M [L] as M’s L-projection, which
is itself a memory such that for all x, M [L](x) = (v, l) if and
only if M (x) = (v, l) and l ∈ L.

We deﬁne types Nat l and Array l, for numbers and arrays,
respectively, where l is a security label. A type environment Γ
associates variables with types, and we interpret it as a partial
map. We sometimes consider when a memory is consistent
with a type environment Γ:
Deﬁnition 2 (Γ-compatibility). We say a memory M is Γ-
compatible if and only if for all x, when M (x) = (v, l), then
v ∈ Nat ⇔ Γ(x) = Nat l and v ∈ Array ⇔ Γ(x) = Array l.

Ideal functionality. Once Alice and Bob have agreed on a pro-
gram S, we imagine an ideal functionality F that executes S.
Alice and Bob send to F memories MA and MB, respectively.
Alice’s memory contains data labeled A and P, while Bob’s
memory contains data labeled B and P. (Data labeled O is only
constructed during execution.) F then proceeds as follows:
1) It checks that MA and MB agree on P-labeled values,
i.e., that MA[{P}] = MB[{P}]. It also checks that they
do not share any A/B-labeled values, i.e., that the domain
of MA[{A}] and the domain of MB[{B}] do not intersect.
If either of these conditions fail, F notiﬁes both parties
and aborts the execution. Otherwise, it constructs memory
M from MA and MB:

M = {x (cid:55)→ (v, l) | MA[{A,P}](x) = (v, l) ∨

MB[{B}](x) = (v, l)}

2) F executes S according to semantics rules having the
form (cid:104)M, S(cid:105) (ia,ta,ib,tb)
−−−−−−−→ (cid:104)M(cid:48), S(cid:48)(cid:105) : D. This judgment
states that starting in memory M, statement S runs,
producing a new memory M(cid:48) and a new statement S(cid:48)
(representing the partially executed program) along with
instruction traces ia and ib, memory traces ta and tb, and
declassiﬁcation event D. We discuss these traces/events
shortly. The ideal execution will produce one of three
outcomes (or fail to terminate):
• (cid:104)M, S(cid:105) (ia,ta,ib,tb)

−−−−−−−→ (cid:104)M(cid:48), S(cid:48)(cid:105) : D, where D = (da, db).
In this case, F outputs da to Alice, and db to Bob. Then
F sets M to M(cid:48) and S to S(cid:48) and restarts step 2.

• (cid:104)M, S(cid:105) (ia,ta,ib,tb)

−−−−−−−→ (cid:104)M(cid:48), l : skip(cid:105) : . In this case, F
notiﬁes both parties that computation ﬁnished success-
fully.

• (cid:104)M, S(cid:105) (ia,ta,ib,tb)

−−−−−−−→ (cid:104)M(cid:48), S(cid:48)(cid:105) : , where S(cid:48) (cid:54)= l : skip,
and no rules further reduce (cid:104)M(cid:48), S(cid:48)(cid:105). In this case, F
aborts and notiﬁes both parties.

Notice that the only communications between F and each
party about the computation are declassiﬁcations da and db (to
Alice and Bob, respectively) and notiﬁcation of termination.
This is because we assume that secure programs will always
explicitly declassify their ﬁnal output (and perhaps interme-
diate outputs, e.g., when processing multiple queries), while

628

m ∈
Arrays
M ∈
Memory
Type
τ
Type Environment Γ
Instruction Traces
i

Array = Nat (cid:42) Nat
Vars (cid:42) (Array ∪ Nat) × SecLabels

::= Nat l | Array l
::= x : τ | ·
::= l : x := e | l : x[x] := x |

t

l : declass(x, y) | l : init(x, y)
l : if(x) | l : while(x) | i@i | 
::= read(x, n) | readarr(x, n, n) |
write(x, n) | writearr(x, n, n) |
x | t@t | 
::= (x, n) | 
d
D ::= (d, d) | 

Memory Traces

Declassiﬁcation
Declass. event

(t1, )
(, t1)
(t2, t2)

if l = P
if l = A
if l = B
if l = O

 (t1, t1)
(cid:26) m(i) 0 ≤ i < |m|
(cid:26) m[i (cid:55)→ v] 0 ≤ i < |m|

otherwise

0

m

otherwise

= select(l, l : i, l : i)

select(l, t1, t2)

=

inst(l, i)

get(m, i)

set(m, i, v)

=

=

t1 ≡ t2

t ≡ t

t@ ≡ @t ≡ t

t1 ≡ t(cid:48)
t1@t2 ≡ t(cid:48)

t2 ≡ t(cid:48)
1@t(cid:48)

2

1

2

Fig. 5: Auxiliary syntax and functions for semantics

all other variables in memory are not of consequence. The
memory and instruction traces, though not explicitly communi-
cated by F, will be visible in a real implementation (described
later), but we prove that they provide no additional information
beyond that provided by the declassiﬁcation events.

Traces and events. The formal semantics incorporate the
concept of traces to deﬁne information leakage. There are
three types of traces, all given in Figure 5. The ﬁrst
is
an instruction trace i. The instruction trace generated by an
assignment statement is the statement itself (e.g., x := e); the
instruction trace generated by a branching statement is denoted
if(x) or while(x). Declassiﬁcation and ORAM initialization
will generate instruction traces declass(x, y) and init(x, y),
respectively. The trace  indicates an unobservable statement
execution (e.g., Bob cannot observe Alice executing her local
code). Trace equivalence (i.e. t1 ≡ t2) is deﬁned in Figure 5.
The second sort of trace is a memory trace t, which
captures reads or writes of variables visible to one or the other
party. Here are the different memory trace events:

• P: Operations on public arrays generate memory event
readarr(x, n, v) or writearr(x, n, v) visible to both par-
ties, including the variable name x, the index n, and the
value v read or written. Operations on public variables
generate memory event read(x, v) or write(x, v).

• A/B: Operations on Alice’s secret arrays generate mem-
ory event readarr(x, n, v) or writearr(x, n, v) visible
to Alice only. Operations on Alice’s secret variables

629

generate memory event read(x, v) or write(x, v) visible
to Alice only. Operations on Bob’s secret arrays/variables
are handled similarly.

• O: Operations on a secret array generate memory event x
visible to both Alice and Bob, containing only the variable
name, but not the index or the value. A special case is the
initialization of ORAM bank x with y’s value: a memory
trace y, but not its content, is observed.

Memory-trace equivalence is deﬁned similarly to instruction-
trace equivalence.

Finally, each declassiﬁcation executed by the program pro-
duces a declassiﬁcation event (da, db), where Alice learns the
declassiﬁcation da and Bob learns db. There is also an empty
declassiﬁcation event , which is used for non-declassiﬁcation
statements. Given a declassiﬁcation event D = (da, db), we
write D[A] to denote Alice’s declassiﬁcation da and D[B] to
denote Bob’s declassiﬁcation db.

Semantics rules. Now we turn to the semantics, which consists
of two judgments. Figure 6 deﬁnes rules for the judgment
l (cid:96) (cid:104)M, e(cid:105) ⇓(ta,tb) v, which states that in mode l, under
memory M, expression e evaluates to v. This evaluation
produces memory trace ta (resp., tb) for Alice (resp., Bob).
Which memory trace event to emit is chosen using the function
select, which is deﬁned in Figure 5. The security label l
is passed in by the corresponding assignment statement (i.e.
l : x := e or l : y[x1] := x2). If l is A or B, then the accesses to
public variables are not observable to the other party, whereas
if l is O then both parties know that an access took place;
the l(cid:63) label deﬁned in E-Var and E-Array ensures the proper
visibility of such events. Note the E-Array rule uses the get()
function to retrieve an element from an array; this function
will return a default value 0 if the index is out of bounds.
Most elements of the rules are otherwise straightforward.

Figure 7 deﬁnes rules for the judgment (cid:104)M, S(cid:105) (ia,ta,ib,tb)
−−−−−−−→
(cid:104)M(cid:48), S(cid:48)(cid:105) : D, which says that under memory M, the statement
S reduces to memory M(cid:48) and statement S(cid:48), while producing
instruction trace ia (resp., ib) and memory trace ta (resp., tb)
for Alice (resp., Bob), and generating declassiﬁcation D. Most
rules are standard, except for handling memory traces and
instruction traces. Instruction traces are handled using function
inst deﬁned in Figure 5. This function is deﬁned such that
if the label l of a statement is A or B, then the other party
cannot observe the statement; otherwise, both parties observe
the statement.

A skip statement generates empty instruction traces and
memory traces for both parties regardless of its label. An
assignment statement ﬁrst evaluates the expression to assign,
and its trace and the write event constitute the memory trace
for this statement. Note that expression is evaluated using the
label l of the assignment statement as per the discussion of
E-Var and E-Array above.

Declassiﬁcation x := declassl(y) declassiﬁes a secret
variable y (labeled O) to a non-secret variable x (not labeled O).
Both Alice and Bob will observe that y is accessed (as deﬁned
by ta and tb), whereas the label l of variable x determines who

E-Const

l (cid:96) (cid:104)M, n(cid:105) ⇓(,) n

E-Var

l(cid:48) (cid:118) l
v ∈ Nat
M (x) = (v, l(cid:48))
l = O ⇒ l(cid:63) = l(cid:48)
l (cid:54)= O ⇒ l(cid:63) = l
(ta, tb) = select(l(cid:63), read(x, v), x)

l (cid:96) (cid:104)M, x(cid:105) ⇓(ta,tb) v

l (cid:96) (cid:104)M, xi(cid:105) ⇓(tia,tib) vi
v = v1 op v2

i = 1, 2

E-Array

E-Op

ta = t1a@t2a
tb = t1b@t2b
l (cid:96) (cid:104)M, x1 op x2(cid:105) ⇓(ta,tb) v

E-Mux

l(cid:48) (cid:118) l
M (x) = (m, l(cid:48)) m ∈ Array
l (cid:54)= O ⇒ l(cid:63) = l
l = O ⇒ l(cid:63) = l(cid:48)
l (cid:96) (cid:104)M, y(cid:105) ⇓(t(cid:48)
v(cid:48) = get(m, v)
a,t(cid:48)
a, t(cid:48)(cid:48)
b ) = select(l(cid:63), readarr(x, v, v(cid:48)), x)
(t(cid:48)(cid:48)
tb = t(cid:48)
ta = t(cid:48)
a@t(cid:48)(cid:48)
l (cid:96) (cid:104)M, x[y](cid:105) ⇓(ta,tb) v(cid:48)

b@t(cid:48)(cid:48)

b) v

a

b

l (cid:96) (cid:104)M, xi) ⇓(tia,tib) vi
i = 1, 2, 3
v1 = 0 ⇒ v = v2
v1 (cid:54)= 0 ⇒ v = v3
tb = t1b@t2b@t3b
ta = t1a@t2a@t3a
l (cid:96) (cid:104)M, mux(x1, x2, x3)(cid:105) ⇓(ta,tb) v

Fig. 6: Operational semantics for expressions in SCVM l (cid:96) (cid:104)M, e(cid:105) ⇓(ta,tb) v

sees the declassiﬁed value as indicated by the declassiﬁcation
event D.

ORAM initialization produces a shared, secret array x from
an array y provided by one party. Thus, the security label of
x must be O, and the security label of y must not be O. This
rule implies that the party who holds y will observe accesses
to y, and then both parties can observe accesses to x.

Rule S-ArrAss handles an array assignment. Similar to
rule E-Array, out-of-bounds indices are ignored (cf. the set()
function in Figure 5). For if-statements and while-statements,
no memory traces are observed other than those observed from
evaluating the guard x.

Rule S-Seq sequences execution of

two statements
if
in the obvious way. Finally,
(cid:104)M, S(cid:105) (ia,ta,ib,tb)
−−−−−−−→ (cid:104)M(cid:48)(cid:48), S(cid:48)(cid:48)(cid:105) : D, the transformation may
perform one or more small-step transformations that generate
no declassiﬁcation.

rule S-Concat says that

statement semantics:

(cid:104)M, P(cid:105) Γ,(ia,ta,ib,tb)

−−−−−−−−−→(cid:63) (cid:104)Mn, Pn(cid:105) : D1, ..., Dn
a,t(cid:48)
(cid:104)Mn, Pn(cid:105) (i(cid:48)
−−−−−−−→ (cid:104)M(cid:48), P (cid:48)(cid:105) : D(cid:48)

b,t(cid:48)
b)

a,i(cid:48)

D(cid:48) (cid:54)=  ∨ P (cid:48) = l : skip M and M(cid:48) are both Γ-compatible

n ≥ 0

(cid:104)M, P(cid:105) Γ,(i(cid:48)

−−−−−−−−−→(cid:63) (cid:104)M(cid:48), P (cid:48)(cid:105) : D1, ..., Dn, D(cid:48)

b,t(cid:48)
b)

a,t(cid:48)

a,i(cid:48)

This allows programs to make multiple declassiﬁcations, ac-
cumulating them as a trace, while remembering only the
most recent instruction and memory traces and ensuring that
intermediate memories are Γ-compatible.
Deﬁnition 3 (Γ-simulatability). Let Γ be a type environment,
and P a program. We say P is Γ-simulatable if there exist
simulators simA and simB, which run polynomial time in the
data size, such that for all M, ia, ta, ib, tb, M(cid:48), P (cid:48), D1, ..., Dn,
if (cid:104)M, P(cid:105)
(cid:104)M(cid:48), P (cid:48)(cid:105)
then
and
simA(M [{P,A}], D1[A], ..., Dn−1[A])
simB(M [{P,B}], D1[B], ..., Dn−1[B]) ≡ (ib, tb).

: D1, ..., Dn,
≡
(ia, ta)

Γ,(ia,ta,ib,tb)

−−−−−−−−−→(cid:63)

C. Security

The ideal functionality F deﬁnes the baseline of security,
emulating a trusted third party that runs the program using
Alice and Bob’s data, directly revealing to them only the
explicitly declassiﬁed values. In a real implementation run
directly by Alice and Bob, however, each party will see
additional events of interest, in particular an instruction trace
and a memory trace (as deﬁned by the semantics). Importantly,
we want
these traces provide no additional
information about the opposite party’s data beyond what each
party could learn from observing F. We do this by proving that
in fact these traces can be simulated by Alice and Bob using
their local data and the list of declassiﬁcation events provided
by F. As such, revealing the instruction and memory traces
(as in a real implementation) provides no additional useful
information.

to show that

We call our security property Γ-simulatability. To state this
property formally, we ﬁrst deﬁne a multi-step version of our

Intuitively, if P is Γ-simulatable there exists a simulator
simA that, given public data M [{P}], Alice’s secret data
M [{A}], and all outputs D1[A], ..., Dn−1[A] declassiﬁed to
Alice so far, can compute the instruction traces ia and memory
traces ta produced by the ideal semantics up until the next
declassiﬁcation event Dn, regardless of the values of Bob’s
secret data.

Note that Γ-simulatability is termination insensitive, and
information may be leaked based upon whether a program
terminates or not [3]. However, as long as all runs of a program
are guaranteed to terminate (as is typical for programs run in
secure-computation scenarios), no information leakage occurs.

D. Type System

This section presents our type system, which we prove en-
sures Γ-simulatability. There are two judgments, both deﬁned
in Figure 8. The ﬁrst, written Γ (cid:96) e : τ, states that under
environment Γ, expression e evaluates to type τ. The second
judgment, written Γ, pc (cid:96) S, states that under environment Γ
and a label context pc, a labeled statement S is type-correct.

630

−−−−−→ (cid:104)M, S(cid:105) : 

S-Skip (cid:104)M, l : skip; S(cid:105) (,,,)
l (cid:96) (cid:104)M, e(cid:105) ⇓(t(cid:48)

M(cid:48) = M [x (cid:55)→ (v, l)]

a,t(cid:48)

b) v

(ia, ib) = inst(l, x := e)

(t(cid:48)(cid:48)
a, t(cid:48)(cid:48)
ta = t(cid:48)

b ) = select(l, write(x, v), x)
a@t(cid:48)(cid:48)
b@t(cid:48)(cid:48)

tb = t(cid:48)

a

b

S-ArrAss

S-Assign

−−−−−−−−→ (cid:104)M(cid:48), l : skip(cid:105) : 

(cid:104)M, l : x := e(cid:105) (ia,ta,ib,tb)
l (cid:54)= O

M (y) = (v, O)
M(cid:48) = M [x (cid:55)→ (v, l)]

ta = tb = y
i = O : declass(x, y)

S-Declass

D = select(l, (x, v), )

(cid:104)M, O : x := declassl(y)(cid:105) (i,ta,i,tb)

−−−−−−→ (cid:104)M(cid:48), O : skip(cid:105) : D

(ta, tb) = select(l, read(x, v), x)

(ia, ib) = inst(l, if(x)) M (x) = (v, l)
v (cid:54)= 1 ⇒ c = 2
v = 1 ⇒ c = 1
−−−−−−−−→ (cid:104)M, Sc(cid:105) : 

(cid:104)M, l : if(x)then S1else S2(cid:105) (ia,ta,ib,tb)

S-Cond

M (y) = (m, l)
M(cid:48) = M [x (cid:55)→ (m, O)]
(t(cid:48)
a, t(cid:48)
b) = select(l, y, )
ta = t(cid:48)
i = O : init(x, y)
a@x
b@x
(cid:104)M, O : x := oram(y)(cid:105) (i,ta,i,t)
−−−−−→ (cid:104)M(cid:48), O : skip(cid:105) : 

l (cid:54)= O
tb = t(cid:48)

S-ORAM

M (y) = (m, l)
m(cid:48) = set(m, v1, v2)
ta = t1a@t2a@t(cid:48)

(t(cid:48)
a, t(cid:48)

a

l (cid:96) (cid:104)M, xi(cid:105) ⇓(tia,tib) vi

i = 1, 2
M(cid:48) = M [y (cid:55)→ (m(cid:48), l)]

b) = select(l, writearr(y, v1, v2), y)
tb = t1b@t2b@t(cid:48)

b

(ia, ib) = inst(l, y[x1] := x2)

(cid:104)M, l : y[x1] := x2(cid:105) (ia,ta,ib,tb)

−−−−−−−−→ (cid:104)M(cid:48), l : skip(cid:105) : 
(ia, ib) = inst(l, while(x))

M (x) = (0, l)

S-While-False

S-While-True

(ta, tb) = select(l, read(x, 0), x)

S = l : while(x)do S(cid:48)
−−−−−−−−→ (cid:104)M, l : skip(cid:105) : 

(cid:104)M, S(cid:105) (ia,ta,ib,tb)

v (cid:54)= 0

M (x) = (v, l)

(ta, tb) = select(l, read(x, v), x)

S = l : while(x)do S(cid:48)
−−−−−−−−→ (cid:104)M, S(cid:48); S(cid:105) : 

(cid:104)M, S(cid:105) (ia,ta,ib,tb)

S-Seq

S-Concat

1(cid:105) : D
1; S2(cid:105) : D

−−−−−−−−→ (cid:104)M(cid:48), S(cid:48)
−−−−−−−−→ (cid:104)M(cid:48), S(cid:48)

(cid:104)M, S1(cid:105) (ia,ta,ib,ib)
(cid:104)M, S1; S2(cid:105) (ia,ta,ib,tb)
−−−−−−−−→ (cid:104)M(cid:48), S(cid:48)(cid:105) : 
(cid:104)M, S(cid:105) (ia,ta,ib,ib)
a,t(cid:48)
(cid:104)M(cid:48), S(cid:48)(cid:105) (i(cid:48)
b,i(cid:48)
a,i(cid:48)
−−−−−−−−→ (cid:104)M(cid:48)(cid:48), S(cid:48)(cid:48)(cid:105) : D
b)
b,tb@t(cid:48)
a,ib@i(cid:48)
a,ta@t(cid:48)
−−−−−−−−−−−−−−−−−→ (cid:104)M(cid:48)(cid:48), S(cid:48)(cid:105) : D
b)

(cid:104)M, S(cid:105) (ia@i(cid:48)

Fig. 7: Operational semantics for statements in SCVM (cid:104)M, S(cid:105) (ia,ta,ib,tb)

−−−−−−−→ (cid:104)M(cid:48), S(cid:48)(cid:105) : D

Here, pc is a label that describes the ambient control context;
pc is set according to the guards of enclosing conditionals or
loops. Note that since a program cannot execute an if-statement
or a while-statement whose guard is secret, pc can be one of
P, A, or B, but not O. Intuitively, if pc is A (resp., B), then
the statement is part of Alice’s (resp., Bob’s) local code. In
general, for a labeled statement S = l : s we enforce the
invariant pc (cid:118) l, and if pc (cid:54)= P, then pc = l. In so doing, we
ensure that if the security label of a statement is A (including
if-statements and while-statements), then all nested statements
also have security label A, thus ensuring they are only visible to
Alice. On the other hand, under a public context, the statement
label is unrestricted.

Now we consider some interesting aspects of the rules.
Rule T-Assign requires pc (cid:116) l(cid:48) (cid:118) l, as is standard: pc (cid:118) l
prevents implicit ﬂows, and l(cid:48) (cid:118) l prevents explicit ones. We
further restrict that Γ(x) = Nat l, i.e., the assigned variable
should have the same security label as the instruction label.
Rule T-ArrAss and rule T-Array require that for an array
expression y[x], the security label of x should be lower than
the security label of y. For example, if x is Alice’s secret
variable, then y should be either Alice’s local array, or an
ORAM shared between Alice and Bob. If y is Bob’s secret
variable, or a public variable, then Bob can observe which
indices are accessed, and then infer the value of x. In the
example from Figure 2, the array access vis[bestj] on line
9 requires that vis be an ORAM variable since bestj is.

For rules T-Declass and T-ORAM, since declassiﬁcation
and ORAM initialization statements both require secure com-
putation, we restrict the statement label to be O. Since these
two statements cannot be executed in Alice’s or Bob’s local
mode, we restrict that pc = P.

to be equal

Rule T-Cond deals with if-statements; T-While handles
while loops similarly. First of all, we restrict pc (cid:118) l and
Γ(x) = Nat l
for the same reason as above. Further,
the rule forbids l
to O to avoid an implicit
ﬂow revealed by the program’s control ﬂow. An alternative
way to achieve instruction- and memory- trace oblivious-
ness is through padding [18]. However,
in the setting of
secure-computation, padding achieves the same performance
as rewriting a secret-branching statement into a mux (or a se-
quence of them). And, using padding would require reasoning
about trace patterns, a complication our type system avoids.

A well-typed program is Γ-simulatable:

Theorem 1. If Γ, P (cid:96) S, then S is Γ-simulatable.

Notice that some rules allow a program to get stuck. For
example, in rule S-ORAM, if the statement is l : x := oram(y)
but l (cid:54)= O, then the program will not progress. We deﬁne
a property called Γ-progress that formalizes the notion of a
program that never gets stuck.
Deﬁnition 4 (Γ-progress). Let Γ be a type environment, and
let P = P0 be a program. We say P enjoys Γ-progress

631

Γ (cid:96) e : τ

T-Var

Γ(x) = Nat l
Γ (cid:96) x : Nat l

T-Const

Γ (cid:96) n : Nat P

T-Op

Γ(x1) = Nat l1

Γ(x2) = Nat l2

Γ (cid:96) x1 op x2 : Nat l1 (cid:116) l2

Γ(x) = Nat l2

Γ(y) = Array l1
Γ (cid:96) y[x] : Nat l1

l2 (cid:118) l1

T-Array

Γ(xi) = Nat li
i = 1, 2, 3
Γ (cid:96) mux(x1, x2, x3) : Nat l

l = l1 (cid:116) l2 (cid:116) l3

T-Mux

Γ, pc (cid:96) S

T-Skip

Γ(x) = Nat l
pc (cid:116) l(cid:48) (cid:118) l

T-Assign

Γ, pc (cid:96) l : x := e

pc (cid:118) l

pc (cid:54)= P ⇒ pc = l
Γ, pc (cid:96) l : skip
Γ (cid:96) e : Nat l(cid:48)
pc (cid:54)= P ⇒ l = pc

T-Declass

T-ORAM

Γ(y) = Nat O
pc = P
l (cid:54)= O
Γ(x) = Nat l
Γ, pc (cid:96) O : x := declassl(y)
Γ(x) = Array O
pc = P
l (cid:54)= O
Γ(y) = Array l
Γ, pc (cid:96) O : x := oram(y)

Γ(y) = Array l
Γ(x2) = Nat l2

Γ(x1) = Nat l1
pc (cid:116) l1 (cid:116) l2 (cid:118) l

pc (cid:54)= P ⇒ l = pc

Γ, pc (cid:96) l : y[x1] := x2

T-ArrAss

Γ(x) = Nat l
pc (cid:54)= P ⇒ l = pc

pc (cid:118) l
Γ, l (cid:96) Si

l (cid:54)= O
i = 1, 2

T-Cond

Γ, pc (cid:96) l : if(x)then S1else S2
l (cid:54)= O
Γ(x) = Nat l
pc (cid:54)= P ⇒ l = pc
Γ, l (cid:96) S

pc (cid:118) l

Γ, pc (cid:96) l : while(x)do S
Γ, pc (cid:96) S1
Γ, pc (cid:96) S2

Γ, pc (cid:96) S1; S2

T-While

T-Seq

Fig. 8: Type System for SCVM

a,ij

for any Γ-compatible memories M0, . . . , Mn for which
−−−−−−−→ (cid:104)Mj+1, Pj+1(cid:105) : Dj for j = 0, ..., n − 1,
a,tj
b, M(cid:48), P (cid:48) such

if
(cid:104)Mj, Pj(cid:105) (ij
either Pn = l : skip, or there exist i(cid:48)
a, t(cid:48)
a,t(cid:48)
that (cid:104)Mn, Pn(cid:105) (i(cid:48)
−−−−−−−→ (cid:104)M(cid:48), P (cid:48)(cid:105) : D(cid:48).

a, i(cid:48)

b, t(cid:48)

b,t(cid:48)
b)

b,tj
b)

a,i(cid:48)

Γ-progress means, in particular, that the third bullet in
step (2) of the ideal functionality (Section IV-B) does not occur
for type-correct programs.

A well-typed program never gets stuck:

Theorem 2. If Γ, P (cid:96) S, then S enjoys Γ-progress.

Proofs of both theorems above can be found in our sup-

plemental technical report [19].

E. From SCVM Programs to Secure Protocols

Let P be a program, and let F be the ideal function-
ality based on this program as described earlier. Here we
deﬁne a hybrid-world protocol πG based on P , where G =
(Fop,Fmux,Foram,Fdeclass) is a ﬁxed set of ideal functionali-
ties that implement simple binary operations (Fop), a MUX
operation (Fmux), ORAM access (Foram), and declassiﬁcation
(Fdeclass). Input
to each of these ideal functionalities can
either be Alice or Bob’s local inputs, public inputs, and/or
the shares of secret
inputs (each share supplied by Alice
and Bob respectively). Each ideal functionality is explicitly
parameterized by the types of the inputs. Further, except
for Fdeclass which performs an explicit declassiﬁcation, all
other ideal functionalities return shares of the computation or
memory fetch result to Alice and Bob, respectively. Further
details of the ideal functionalities are given in our supplemental
technical report [19], along with formal deﬁnitions of the
simulator and hybrid world semantics.

Informally, the hybrid world protocol πG runs as follows:
1) Alice and Bob ﬁrst agree on public values, ensuring
that MA[{P}] = MB[{P}]. During the protocol each
maintains a declassiﬁcation list, for keeping track of
previously declassiﬁed values, and a secret memory that
contains shares of secret (non-ORAM) variables. To start,
both the lists and memories are empty, i.e., declsA :=
declsB :=  and M S

A = M S

B = [].

2) Alice runs her simulator (locally) on her initial memory
to obtain (ia, ta) = simA(MA, declsA), where ia and ta
cover the portion of the execution starting from just after
the last provided declassiﬁcation (i.e., the ﬁnal da in the
list declsA) up to the next declassiﬁcation instruction or
the terminating skip statement. Bob does likewise to get
(ib, tb) = simB(MB, declsB).

3) Alice executes the instructions in ia using the hybrid-
world semantics, which reads (and writes) secret shares
from (to) M S
A and obtains the values of other reads from
events observed in ta. Bob does similarly with ib, M S
B
and tb. The semantics covers three cases:
• If an instruction in ia is labeled P, then so is the
corresponding instruction in ib. Both parties execute
the instruction.

• If an instruction in ia is labeled A, then Alice executes
this instruction locally. Bob does similarly for instruc-
tions labeled B.

• If an instruction in ia is labeled O, then so is the
corresponding instruction in ib. Alice and Bob call the
appropriate ideal-world functionality from G to execute
this instruction. If the instruction is a declassiﬁcation,
then Fdeclass will generate an event (da, db).

4) If the last instruction executed in step 3 is a declassiﬁca-
tion, then Alice appends her declassiﬁcation to her local

632

declassiﬁcation list (i.e., declsA := declsA++[da]), and
Bob does likewise; then both repeat step 2. Otherwise,
the protocol completes.
We have proved that if P is Γ-simulatable, then πG securely

implements F against semi-honest adversaries.
Theorem 3. (Informally) Let P be a program, F the ideal
functionality corresponding to P , and πG the protocol corre-
sponding to P as described above. If P is Γ-simulatable, then
πG securely implements F against semi-honest adversaries in
the G-hybrid model.

Using standard composition results for cryptographic pro-
tocols, we obtain as a corollary that if all ideal functionalities
in G are implemented by semi-honest secure protocols, the
resulting (real-world) protocol securely implements F against
semi-honest adversaries.

A formal deﬁnition of πG, formal theorem statement, and
a proof of the theorem can be found in our supplemental
technical report [19].

V. COMPILATION

We informally discuss how to compile an annotated C-like
source language into a SCVM program. An example of our
source language is:

int sum(alice int x, bob int y) {

return x<y ? 1 : 0;

}

The program’s two input variables, x and y, are annotated
as Alice’s and Bob’s data, respectively, while the unannotated
return type int indicates the result will be known to both
Alice and Bob. Programmers need not annotate any local
variables. To compile such a program into a SCVM program,
the compiler takes the following steps.

Typing the source language. As just mentioned, source level
types and initial security label annotations are assumed given.
With these, the type checker infers security labels for local
variables using a standard security type system [26] using
our lattice (Section IV-D). If no such labeling is possible
without violating security (e.g., due to a conﬂict in the initial
annotation), the program is rejected.

Labeling statements. The second task is to assign a security
label to each statement. For assignment statements and array
assignment statements, the label is the least upper bound of
all security labels of the variables occurring in the statement.
For an if-statement or a while-statement, the label is the least
upper bound of all security labels of the guard variables, and
all security labels in the branches or loop body.

On secret branching. The type system deﬁned in Sec-
tion IV-D will reject an if-statement whose guard has security
label O. As such, if the program branches on secret data,
into if-free SCVM code, using mux
we must compile it
instructions. The idea is to execute both branches, and use
mux to activate the relevant effects, based on the guard. To do
this, we convert the code into Static-Single-Assignment form

(SSA) [2], and then replace occurrences of the φ-operator with
a mux. The following example demonstrates this process:

if(s) then x:=1; else x:=2;

The SSA form of the above code is

if(s) then x1:=1; else x2:=2; x:=phi(x1, x2);

Then we eliminate the if-structure and substitute the φ-

operator to achieve the ﬁnal code:

x1:=1; x2:=2; x:=mux(s, x1, x2)

(Note that, for simplicity, we have omitted the security

labels on the statements in the example.)

On secret while loops. The type system requires that while
loop guards only reference public data, so that the number
of iterations does not leak information. A programmer can
work around this restriction by imposing a constant bound
on the loop; e.g., manually translating while (s) do S to
while (p) do if (s) S else skip, where p deﬁnes an
upper bound on the number of iterations.

Declassiﬁcation. The compiler will emit a declassiﬁcation
statement for each return statement in the source program.
To avoid declassifying in the middle of local code, the type
checker in the ﬁrst phase will check for this possibility and
relabel statements accordingly.

Extension for non-oblivious secret RAM. The discussion so
far supports only secret ORAMs. To support non-oblivious
secret RAM in SCVM, we add an additional security label
N such that P (cid:118) N (cid:118) O. To incorporate such a change, the
memory trace for the semantics should include two more kinds
of trace event, nread(x, i) and nwrite(x, i), which represent
that only the index of an access is leaked, but not the content.
Since label N only applies to arrays, we allow types Array N
but not types Nat N. The rules T-Array and T-ArrAss should
be revised to deal with the non-oblivious RAM. For example,
for rule T-ArrAss, where l is the security label for the array, l1
is the security label of the index variable and l2 is the security
label of the value variable, the type system should still restrict
l1 (cid:118) l, but if l = N, the type system accepts l2 = O, but
requires l1 = P.
Correctness. We do not prove the correctness of our com-
piler, but instead can use a SCVM type checker (using the
above extension) for the generated SCVM code, ensuring it
is Γ-simulatable. Ensuring the correctness of compilers is
orthogonal and outside the scope of this work, and existing
techniques [7] can potentially be adapted to our setting.

Compiling Dijkstra’s algorithm. We explain how compi-
lation works for Dijkstra’s algorithm, previously shown in
Figure 2. First,
the type checker for the source program
determines how memory should be labeled. It determines that
the security labels for bestj and bestdis should be O, and
the arrays dis and vis should be secret-shared between Alice
and Bob, since their values depend on both Alice’s input (i.e.,
the graph’s edge weights) and Bob’s input (i.e., the source).

633

Then, since on line 9 array vis is indexed with bestj, variable
vis should also be put in an ORAM. Similarly, on line 12,
array e is indexed by bestj so it must also be secret; as such
we must promote e, owned by Alice, to be in ORAM, which
we do by initializing a new ORAM-allocated variable orame
to e at the start of the program.

The type checker then uses the variable labeling to deter-
mine the statement labeling. Statements on lines 4–7, 9, and
11–13, require secure computation and thus are labeled as O.
Loop control-ﬂow statements are computed publicly, so they
are labeled as P.

The two if-statements both branch on ORAM-allocated
data, so they must be converted to mux operations. Lines 4–7
are transformed (in source-level syntax) as follows

cond3 := !vis[j] && (bestj<0||dis[j]<bestdis);
bestj := mux(cond3, j, bestj);
bestdis := mux(cond3, dis[j], bestdis);

Lines 11-13 are similarly transformed

tmp := bestdis + orame[bestj*n+j];
cond4 := !vis[j] && (tmp<dis[j]);
dis[j] := mux(cond4, tmp, dis[j]);

Finally, the code is translated into SCVM’s three-address

code style syntax.

VI. EVALUATION

Programs. We have built several secure two-party computation
applications. As run-once tasks, we implemented both the
Knuth-Morris-Pratt (KMP) string-matching algorithm as well
as Dijkstra’s shortest-path algorithm. For repeated sublinear-
time database queries, we considered binary search and the
heap data structure. All applications are listed in Table II.

Compilation time. All programs took little time (e.g., under
1 second) to compile. In comparison, some earlier circuit-
model compilers involve copying datasets into circuits, and
therefore the compile-time can be large [16], [21] (e.g., Kreuter
et al. [16] report a compilation time of roughly 1000 seconds
for an implementation of an algorithm to compute graph
isomorphism on 16-node graphs).

In our experiments, we manually checked the correctness
of compiled programs (we have not yet implemented a type
checker for SCVM, though doing so should be straightfor-
ward).

A. Evaluation Methodology

Although our techniques are compatible with any cryp-
tographic back-end secure in the semi-honest model by the
deﬁnition of Canetti [5], we use the garbled circuit approach
in our evaluation [13].

We measure the computational cost by calculating the
number of encryptions required by the party running as the
circuit generator (the party running as the evaluator does
less work). For every non-XOR binary gate, the generator
makes 3 block-cipher calls; for every oblivious transfer (OT),

2 block-cipher operations are required since we rely on OT
extension [14]. For the run-once applications (i.e., Dijkstra
shortest distance, KMP-matching, aggregation,
inverse per-
mutation), we count in the ORAM initialization cost when
comparing to the automated circuit approach (which doesn’t
require RAM initialization). The ORAM initialization can
be done using a Waksman shufﬂing network [29]. For the
applications expecting multiple executions we do not count the
ORAM initialization cost since this one-time overhead will be
amortized to (nearly) 0 over many executions.

We implemented the binary tree-based ORAM of Shi et
al. [27] using garbled circuits, so that array accesses reveal
nothing about the (logical) addresses nor the outcomes. Fol-
lowing Gordon et al.’s ORAM encryption technique [10],
every block is XOR-shared (i.e., the client stores secret key
k while the server stores (r, fk(r)⊕ m) where f is a family of
psuedorandom permutations and m the data block). This adds
one additional cipher operation per block (when the length of
an ORAM block is less than the width of the cipher). We note
speciﬁc choices of the ORAM parameters in related discussion
of each application.

Metrics. We use the number of block-cipher evaluations
as our performance metric. Measuring the performance by
the number of symmetric encryptions (instead of wall clock
times) makes it easier to compare with other systems since
the numbers can be independent of the underlying hardware
and ciphering algorithms. Additionally, in our experiments
these numbers represent bandwidth consumption since every
encryption is sent over the network. Therefore, we do not
report separately the bandwidth used. Modern processors with
AES support can compute 108 AES-128 operations per second.

B. Comparison with Automated Circuits

Presently, automated secure computation implementations
largely focus on the circuit-model of computation, handling
array accesses by linearly scanning the entire array with
a circuit every time an array lookup happens;
this incurs
prohibitive overhead when the dataset is large. In this section,
we compare our approach with the existing compiled circuits,
and demonstrate that our approach scales much better with
respect to dataset size.

1) Repeated sublinear-time queries:

scenario,
ORAM initialization is a one-time operation whose cost can
be amortized over multiple subsequent queries, achieving
sublinear amortized cost per query.

In this

Binary search. One example application we tested is binary
search, where one party owns a conﬁdential (sorted) array of
size n, and the other party searches for (secret) values stored
in that array.

In our experiments, we set the ORAM bucket size to 32
(i.e., each tree-node can store up to 32 blocks). For binary
search, we aligned our experimental settings with those of
Gordon et al. [10], namely, assuming the size of each data item
is 512 bits. We set the recursion factor to 8 (i.e., each block can
store up to 8 indices for the data in the upper level recursion
tree) and the recursion cut-off threshold to 1000 (namely no
more recursion once fewer than 1000 units are to be stored).

634

Name
Dijkstra’s shortest path
Knuth-Morris-Pratt string matching
Aggregation over sliding windows
Inverse permutation
Binary search
Heap (insertion/extraction)

a key-value table

share of permutation

an array of keys

share of permutation

sorted array

search key

Alice’s Input

a graph

a sequence

Bob’s Input

a (src, dest) pair

a pattern

share of the heap

share of the heap
TABLE II: Programs used in our evaluation

Setting
run-once
run-once
run-once
run-once

repeated sublinear-time query
repeated sublinear-time query

(a) Our approach vs. automated circuit-based approach

(b) Our approach vs. hand-constructed linear scan circuit

Fig. 9: Binary search

Comparing to a circuit-model implementation—which uses a
circuit of size O(n log n) that implements binary search—our
approach is faster for all RAM sizes tested (see Figure 9(a)).
For n = 220, our approach achieves a 100× speedup.

Note it is also possible to use a smaller circuit of size O(n)
that just performs a linear scan over the data. However, such
a circuit would have to be “hand-crafted,” and would not be
output by automated compilation of a binary-search program.
Our approach runs faster for large n even when compared
to such an implementation (see Figure 9(b)). On data of size
n = 220, our approach achieves a 5× speedup even when
compared to this “hand-crafted” circuit-based solution.
Heap. Besides binary search, we also implemented an oblivi-
ous heap data structure (with 32-bit payload, i.e., size of each
item). The costs of insertion and extraction respecting various
heap sizes are given in Figure 10(a) and 10(b), respectively.
The basic shapes of the performance curves are very similar
to that for binary search (except that heap extraction is twice
as slow as insertion because two comparisons are needed per
level). We can observe an 18× speedup for both heap insertion
and heap extraction when the heap size is 220.

The speedup of our heap implementation over automated
circuits is even greater when the size of the payload is bigger.
At 512-bit payload, we have an 100× speedup for data size
220. This is due to the extra work incurred from realizing the
ORAM mechanism, which grows (in poly-logarithmic scale)
with the size of the RAM but independent of the size of each
data item.

2) Faster one-time executions: We present two applica-
tions: the Knuth-Morris-Pratt string-matching algorithm (rep-

resentative of
linear-time RAM programs) and Dijkstra’s
shortest-path algorithm (representative of super-linear time
RAM programs). We compare our approach with a naive
program-to-circuit compiler which copies the entire array for
every dynamic memory access.

The Knuth-Morris-Pratt algorithm. Alice has a secret string
T (of length n) while Bob has a secret pattern P (of length
m) and wants to scan through Alice’s string looking for this
pattern. The original KMP algorithm runs in O(n + m) time
when T and P are in plaintext. Our compiler compiles an
implementation of KMP into a secure string matching protocol
preserving its linear efﬁciency up to a polylogarithmic factor
(due to the ORAM technique).

We assume the string T and the pattern P both consist of
16-bit characters. The recursion factor of the ORAM is set to
16. Figure 11(a) and 11(b) show our results compared to those
when a circuit-model compiler is used. From Figure 11(a), we
can observe that our approach is slower than the circuit-based
approach on small datasets, since the overhead of the ORAM
protocol dominates in such cases. However, the circuit-based
approach’s running time increases more quickly as the dataset’s
size increases. When m = 50 and n = 2 × 106, our program
runs 21× faster.
Dijkstra’s algorithm. Here Alice has a secret graph while Bob
has a secret source/destination pair and wishes to compute the
shortest distance between them. Compiling from a standard
Dijkstra shortest-path algorithm, we obtain an O(n2 log3 n)-
overhead RAM-model protocol.

In our experiment, Alice’s graph is represented by an n×n

635

(a) Heap insertion

(b) Heap extraction

Fig. 10: Heap operations

(a) Median n (ﬁxing m = 50)

(b) Large n (ﬁxing m = 50)

Fig. 11: KMP string matching

adjacency matrix (of 32-bit integers) where n is the number of
vertices in the graph. The distances associated with the edges
are denoted by 32-bit integers. We set ORAM recursion factor
to 8. The results (Figure 12(a)) show that our scheme runs
faster for all sizes of graphs tested. As the performance of our
protocol is barely noticeable in Figure 12(a), the performance
gaps between the two protocols for various n is explicitly
plotted in Figure 12(b). Note the shape of the speedup curve
is roughly quadratic.

Aggregation over sliding windows. Alice has a key-value
table, and Bob has a (size-n) array of keys. The secure
computation task is the following: for every size-k window
on the key array, look up k values corresponding to Bob’s
k keys within the window, and output the minimum value.
Our compiler outputs a O(n log3 n) protocol to accomplish
the task. The optimized protocol performs signiﬁcantly better,
as shown in Figure 13 (we ﬁxed the window size k to 1000
and set recursion factor to 8, while varying the dataset from 1
to 6 million pairs).

C. Comparison with RAM-SC Baselines

Beneﬁts of instruction-trace obliviousness. The RAM-SC
technique of Gordon et al. [10], described in Section II-A,
uses a universal next-instruction circuit to hide the program
counter and the instructions executed. Each instruction in-

volves ORAM operations for instruction and data fetches,
and the next-instruction circuit must effectively execute all
possible instructions and use an n-to-1 multiplexer to select the
right outcome. Despite the lack of concrete implementation for
their general approach, we show through back-of-the-envelope
calculations that our approach should be orders-of-magnitude
faster.

Consider the problem of binary search over a 1-million
item dataset: in each iteration, there are roughly 10 instructions
to run, hence 200 instructions in total to complete the search.
To run every instruction, a universal-circuit-based implemen-
tation has to execute every possible instruction deﬁned in its
instruction set. Even if we conservatively assume a RISC-
style instruction set, we would require over 9 million (non-
free) binary gates to execute just a memory read/write over
a 512M bit RAM. Plus, an extra ORAM read is required to
obliviously fetch every instruction. Thus, at least a total of
3600 million binary gates are needed, which is more than
20 times slower than our result exploiting instruction trace
obliviousness. Furthermore, notice that binary search is merely
a case where the program traces are very short (with only
logarithmic length). Due to the overwhelming cost of ORAM
read/write instructions, we stress that the performance gap will
be much greater with respect to programs that have relatively
fewer memory read/write instructions (comparing to binary
search, 1 out of 10 instructions is a memory read instruction).

636

(a) Performance

(b) Speedup

Fig. 12: Dijkstra’s shortest-path algorithm

(a) Performance

(b) Speedup

Fig. 13: Aggregation over sliding windows

(a) Inverse permutation

(b) Dijkstra

Fig. 14: Savings by memory-trace obliviousness optimization. In (a), the non-linearity (around 60) of the curve is due to the

increase of the ORAM recursion level at that point.

Beneﬁts of memory-trace obliviousness. In addition to avoid-
ing the overhead of a next-instruction circuit, SCVM avoids
the overhead of storing all arrays in a single, large ORAM.
Instead, SCVM can store some arrays as non-oblivious secret
shared memory, and others in separate ORAM banks, rather
than one large ORAM. Doing so does not compromise security
because the type system ensures memory-trace obliviousness.
Here we assess the advantages of these optimizations by
comparing against SCVM programs compiled without
the
optimizations enabled. The results for two applications are
given in Figure 14.

• Inverse permutation. Consider a permutation of size n,
represented by an array a of n distinct numbers from 1
to n, i.e., the permutation maps the i-th object to the
a[i]-th object. One common computation would be to
compute its inverse, e.g., to do an inverse table lookup
using secret indices. The inverse permutation (with result
stored in array b) can be computed with the loop:
while (i < n) { b[a[i]]=i; i=i+1;}

The memory-trace obliviousness optimization automat-
ically identiﬁes that
the array a doesn’t need to be
put in ORAM though its content should remain secret
(because the access pattern to a is entirely public known).

637

This yields 50% savings, which is corroborated by our
experiment results (Figure 14(a)).
• Dijkstra’s shortest path. We discussed the advantages
of memory-trace obliviousness in Section III with respect
to Dijkstra’s algorithm. Our experiments show that we
consistently save 15 ∼ 20% for all graph sizes. The
savings rates for smaller graphs are in fact higher even
though it is barely noticeable in the chart because of the
fast (super-quadratic) growth of overall cost.

VII. CONCLUSIONS

We describe the ﬁrst automated approach for RAM-model
secure computation. Directions for future work include ex-
tending our framework to support malicious security; applying
orthogonal techniques (e.g., [7]) to ensure correctness of the
compiler; incorporating other cryptographic backends into our
framework; and adding additional language features such as
higher-dimensional arrays and structured data types.

Acknowledgments. We thank Hubert Chan, Dov Gordon,
Feng-Hao Liu, Emil Stefanov, and Hong-Sheng Zhou for
helpful discussions. We also thank the anonymous reviewers
and our shepherd for their insightful feedback and comments.
This research was funded by NSF awards CNS-1111599,
CNS-1223623, and CNS-1314857, a Google Faculty Research
Award, and by the US Army Research Laboratory and the UK
Ministry of Defence under Agreement Number W911NF-06-
3-0001. The views and conclusions contained herein are those
of the authors and should not be interpreted as representing the
ofﬁcial policies, either expressed or implied, of the US Army
Research Laboratory, the U.S. Government, the UK Ministry
of Defense, or the UK Government. The US and UK Govern-
ments are authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copyright notation
hereon.

REFERENCES

[1] J. Agat. Transforming out timing leaks. In POPL, 2000.
[2] B. Alpern, M. N. Wegman, and F. K. Zadeck. Detecting equality

of variables in programs. In In POPL, 1988.

[3] A. Askarov, S. Hunt, A. Sabelfeld, and D. Sands. Termination-
In

insensitive noninterference leaks more than just a bit.
ESORICS, 2008.

[4] M. Bellare, V. T. Hoang, S. Keelveedhi, and P. Rogaway.
Efﬁcient garbling from a ﬁxed-key blockcipher. In IEEE S &
P, 2013.

[5] R. Canetti. Security and composition of multiparty crypto-

graphic protocols. Journal of Cryptology, 2000.

[6] H. Carter, B. Mood, P. Traynor, and K. Butler.

outsourced garbled circuit evaluation for mobile devices.
USENIX Security, 2013.

[7] COMPCERT: Compilers you can formally trust. http://compcert.

Secure
In

inria.fr/.

Privacy-preserving
In

[9] M. T. Goodrich and M. Mitzenmacher.

access of outsourced data via oblivious RAM simulation.
ICALP, 2011.

[10] S. D. Gordon, J. Katz, V. Kolesnikov, F. Krell, T. Malkin,
M. Raykova, and Y. Vahlis. Secure two-party computation in
sublinear (amortized) time. In CCS, 2012.

[11] W. Henecka, S. K¨ogl, A.-R. Sadeghi, T. Schneider, and
I. Wehrenberg. Tasty: Tool for automating secure two-party
computations. In CCS, 2010.

[12] A. Holzer, M. Franz, S. Katzenbeisser, and H. Veith. Secure

two-party computations in ANSI C. In CCS, 2012.

[13] Y. Huang, D. Evans, J. Katz, and L. Malka. Faster secure two-
party computation using garbled circuits. In USENIX Security,
2011.

[14] Y. Ishai, J. Kilian, K. Nissim, and E. Petrank. Extending

oblivious transfers efﬁciently. In CRYPTO, 2003.

[15] F. Kerschbaum. Automatically optimizing secure computation.

In CCS, 2011.

[16] B. Kreuter, B. Mood, A. Shelat, and K. Butler. PCF: A portable
In

circuit format for scalable two-party secure computation.
USENIX Security, 2013.

[17] E. Kushilevitz, S. Lu, and R. Ostrovsky. On the (in)security
of hash-based oblivious RAM and a new balancing scheme. In
SODA, 2012.

[18] C. Liu, M. Hicks, and E. Shi. Memory trace oblivious program

execution. In CSF, 2013.

[19] C. Liu, Y. Huang, E. Shi, J. Katz, and M. Hicks. Automating
efﬁcient RAM-model secure computation. Technical Report
CS-TR-5033, University of Maryland, Department of Computer
Science, Mar. 2014.

[20] S. Lu and R. Ostrovsky. How to garble RAM programs.

In

EUROCRYPT, 2013.

[21] D. Malkhi, N. Nisan, B. Pinkas, and Y. Sella. Fairplay: A secure

two-party computation system. In USENIX Security, 2004.

[22] P. Mardziel, M. Hicks, J. Katz, and M. Srivatsa. Knowledge-

oriented secure multiparty computation. In PLAS, 2012.

[23] D. Molnar, M. Piotrowski, D. Schultz, and D. Wagner. The pro-
gram counter security model: Automatic detection and removal
of control-ﬂow side channel attacks. In ICISC, 2005.

[24] A. Rastogi, M. A. Hammer, and M. Hicks. Wysteria: A
programming language for generic, mixed-mode multiparty
computations. In IEEE S & P, 2014.

[25] A. Rastogi, P. Mardziel, M. Hammer, and M. Hicks. Knowledge
In

inference for optimizing secure multi-party computation.
PLAS, 2013.

[26] A. Sabelfeld and A. C. Myers. Language-based information-
ﬂow security. IEEE Journal on Selected Areas in Communica-
tions, 2003.

[27] E. Shi, T.-H. H. Chan, E. Stefanov, and M. Li. Oblivious RAM

with O((log N )3) worst-case cost. In ASIACRYPT, 2011.

[28] E. Stefanov, M. van Dijk, E. Shi, C. Fletcher, L. Ren, X. Yu,
and S. Devadas. Path ORAM: an extremely simple oblivious
ram protocol. In In CCS, 2013.

[29] A. Waksman. A permutation network. J. ACM, 15, 1968.
[30] P. Williams, R. Sion, and B. Carbunar. Building castles out
of mud: Practical access pattern privacy and correctness on
untrusted storage. In CCS, 2008.

[31] A. C.-C. Yao. How to generate and exchange secrets. In FOCS,

[8] O. Goldreich and R. Ostrovsky.

Software protection and

simulation on oblivious RAMs. J. ACM, May 1996.

1986.

[32] S. Zahur and D. Evans. Circuit structures for improving

efﬁciency of security and privacy tools. In S & P, 2013.

638

