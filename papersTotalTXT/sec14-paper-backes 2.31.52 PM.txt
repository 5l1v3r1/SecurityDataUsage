Oxymoron: Making Fine-Grained Memory 

Randomization Practical by Allowing Code Sharing

Michael Backes, Saarland University and Max Planck Institute for Software Systems  

(MPI-SWS); Stefan Nürnberger, Saarland University

https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/backes

This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXOxymoron

Making Fine-Grained Memory Randomization

Practical by Allowing Code Sharing

Michael Backes

Saarland University, Germany

Max-Planck-Institute for

Software Systems, Germany
backes@ mpi-sws. org

Stefan N¨urnberger

Saarland University, Germany

nuernberger@ cs. uni-saarland. de

Abstract

The latest effective defense against code reuse attacks is
ﬁne-grained, per-process memory randomization. How-
ever, such process randomization prevents code shar-
ing since there is no longer any identical code to share
between processes. Without shared libraries, however,
tremendous memory savings are forfeit. This drawback
may hinder the adoption of ﬁne-grained memory ran-
domization.

We present Oxymoron, a secure ﬁne-grained memory
randomization technique on a per-process level that does
not interfere with code sharing. Executables and libraries
built with Oxymoron feature ‘memory-layout-agnostic
code’, which runs on a commodity Linux. Our theoreti-
cal and practical evaluations show that Oxymoron is the
ﬁrst solution to be secure against just-in-time code reuse
attacks and demonstrate that ﬁne-grained memory ran-
domization is feasible without forfeiting the enormous
memory savings of shared libraries.

1

Introduction

Code reuse attacks manage to re-direct control ﬂow
through a program with the intent of imposing malicious
behavior on an otherwise benign program. Despite be-
ing introduced more than 20 years ago, code reuse is still
one of the three most prevalent attack vectors [1, 28],
e.g., through vulnerable PDF viewers, browsers, or op-
erating system services. Several code reuse mitigations
have been proposed. They either detect the redirection
of control ﬂow [7, 12], or randomize a process’s address
space. Randomizations jumble the whole address space,
with the intent of preventing code reuse attacks by mak-
ing it impossible to predict where speciﬁc code resides.

Especially Address
Space Layout Randomization
(ASLR [23, 22]) has become widespread, but meanwhile
has been shown to be ineffective [24, 25]. A promising

avenue is the use of even ﬁner randomization techniques
that randomize at the granularity of functions, basic
blocks or even instructions [18, 10, 16, 19, 17].

To be effective, ﬁne-grained memory randomization
must prevent an attacker from using information about
the memory layout of one process to infer the layout
of another process. This is a particular threat in the
light of shared code originating from shared libraries.
Hence, most recent ﬁne-grained memory randomization
solutions also randomize shared libraries for every sin-
gle process [13, 21, 29]. As a result, there is no identical
code in any two processes, which makes sharing impos-
sible. A dysfunctional code sharing, however, increases
the memory footprint of the entire system, likely on the
order of Gigabytes, as we elaborate in Section 2.

To summarize: ﬁne-grained randomization solutions pre-
sented so far come at the expense of tremendous memory
overhead, which renders them impractical.

Oxymoron /,6k.sI’mO:.r6n/ (noun)

Greek. A ﬁgure of speech that combines contradic-
tory terms.

We present Oxymoron, which combines two seemingly
contradictory methods: a secure ﬁne-grained memory
randomization with the ability to share the entire code
among other processes. At the heart of Oxymoron is a
new x86 calling convention we propose: Position-and-
Layout-Agnostic CodE (PALACE). This code uses no in-
structions that reference other code or data directly, but
instead the instructions use a layer of indirection referred
to by an index. This index uniquely identiﬁes a target and
hence remains identical when targets are randomized in
memory. Consequently, the memory in which those in-
structions are stored does not change, thereby making it
available to be shared with other processes.

USENIX Association  

23rd USENIX Security Symposium  433

1

Oxymoron cuts program code into the smallest sharable
piece: a memory page. We randomize those pages and
share them individually among processes. Each shared
page appears at a different, random address in each pro-
cess. We use the x86 processor’s segmentation feature to
disable access to the unique indices, which we organized
in a translation table. This unique property of Oxymoron
makes our solution more secure than ﬁne-grained mem-
ory randomization solutions published so far.

To demonstrate the effectiveness and efﬁciency of Oxy-
moron, we implemented and evaluated a static binary
rewriter for the Intel x86 architecture that emits PALACE
executables and libraries with a very low run-time over-
head of only 2.7%. By re-enabling code sharing, Oxy-
moron is the ﬁrst memory randomization technique that
reduces the total system memory overhead back to lev-
els it was before ﬁne-grained memory randomization,
while simultaneously being the ﬁrst solution that is se-
cure against the just-in-time code reuse attacks recently
proposed by Snow et al [26].

2 Problem Description

Before we describe our idea, we want to explain in more
detail why any traditional ﬁne-grained memory random-
ization necessarily makes sharing libraries impossible.
The goal of ﬁne-grained randomization is for every pro-
cess to feature a memory layout that is as varied as pos-
sible from any other process. If we treat program code,
which usually is en bloc, as a puzzle and shufﬂe the puz-
zle pieces throughout the entire address space, their com-
binatorial possibilities provide a high entropy. It is only
possible to share those puzzle pieces individually as a
memory page with other processes if the content of each
piece is identical in each process. With traditional code,
the content of those piece necessarily changes when their
order in memory is rearranged, as we explain in the fol-
lowing:

Code references other code or other data using either ab-
solute memory addresses, e.g., call 0x804bd32, or rela-
tive addresses, e.g., call +42. For absolute addresses it
is obvious that different randomizations necessarily lead
to different code and data addresses. As a result, the en-
coding of instructions that hold such addresses changes
as well, thereby forfeiting the sharing with other pro-
cesses. Relative addresses, in turn, make code indepen-
dent of its load address in memory. However, in case of
using code pieces that are randomized, the relative dis-
tances change as well. Here, for the same reason, those
pieces cannot be shared across processes as they feature
different relative addresses. Consequently, ﬁne-grained
memory randomization impedes common code sharing,
which is a fundamental concept of all modern OSes.

Severity. Modern operating systems use code sharing
automatically, and it is in effect because the running pro-
grams use the same libraries (C library, threading library
etc.), i.e. their address spaces have identical code loaded.

To verify this claim, we conducted a simple experiment
that shows the impact of code sharing and lack thereof.
We used an unmodiﬁed Ubuntu 13.10 x86 operating sys-
tem on a machine with 4 GB of RAM and evaluated how
much RAM is saved due to code sharing. After booting
to an idle desktop, the 234 running processes consumed
a total 679 MB. Our analysis of memory page map-
pings in each process obtained from /proc/<PID>/maps
revealed that most of the processes used the same set of
shared libraries. As expected, most frequently the stan-
dard C-library libc.2.17.so was shared between all of
the 234 processes. All mapped portions of libc sum up
to 207,028 KB while only 885 KB of real memory are
consumed. This is a savings of 206 MB for libc alone.

Figure 1 illustrates the top ten savings by library. In total,
sharing instead of duplicating saved 1,388 MB of RAM
on the idle Ubuntu desktop. When additionally starting
the Firefox browser, the memory consumption was in-
creased from 679 MB to 817 MB. The total amount of
savings by sharing summed up to 1,435 MB of RAM,
which is an additional savings of 47 MB.

7	  

	  

6
0
2

18	  

	  

4
7
1

200	  
(MB)	  

150	  

100	  

50	  

0	  

Firefox	  started,	  Σ	  =	  1435	  MiB	  
Idle	  Desktop,	  Σ=1388	  MiB	  

4	  

	  

7
9

	  

4
1
1

3	  

	  

2
7

4	  

	  

8
4

4	  

	  

9
3

2	  

	  

8
2

	  

6
3

3	  

	  

2
2

1	  

	  

1
2

Figure 1: Savings due to sharing of libraries.
saves 1388 MB, with Firefox 47 MB more is saved.

Idle desktop

2.1 Threat Model

We assume a Linux operating system that runs a user
mode process, which contains a memory corruption vul-
nerability. The attacker’s goal is to exploit this vulner-
ability in order to divert the control ﬂow and execute
arbitrary code on her behalf. To this end, the attacker
knows the process’ binary executable and can precom-
pute potential gadget chains in advance. The attacker can
control the input of all communication channels to the
process, especially including ﬁle content, network traf-
ﬁc, and user input. However, we assume that the attacker

434  23rd USENIX Security Symposium 

USENIX Association

2

has not gained prior access to the operating system’s ker-
nel and that the program’s binary is not modiﬁed. Apart
from that, the computational power of the attacker is not
limited.

Moreover, for JIT-ROP attacks [26] to work, we assume
that the process has at least one memory disclosure vul-
nerability, which makes the process read from an arbi-
trary memory location chosen by the attacker and report
the value at that location. This vulnerability can be ex-
ploited any number of times during the runtime of the
process. Note that the process itself performs the read
attempt: both address space and permissions are implied
to belong to the process.

3 High-Level Design of Oxymoron

To beneﬁt from the best of both worlds – ﬁne-grained
memory randomization and code sharing – the challenge
is to create a form of code that does not incorporate abso-
lute or relative addresses, as we have already shown that
both addressing schemes by deﬁnition suffer from being
dependent on their randomization. An additional layer
of indirection that translates unique labels to current ran-
domized addresses allows the byte representation of code
to remain the same, which enables code page sharing.
However, this approach is difﬁcult to realize as it is ac-
companied by four key challenges:

C) Randomization: At program load time, the pieces
p1|p2| . . .| pn are shufﬂed by the ASLR part of the
operating system loader. In memory, their order is
completely random and the pieces may have empty
gaps of arbitrary size between them.

The ﬁrst two steps only have to be done once, while the
third step is performed at load-time of the executable PE.

Executable	  E 

CODE	  

DATA	  

A	  B	  C	  D	  E	  F	  G	  H	  

1	  2	  3	  4	  5	  6	  

A 
PALACE	  Program	  

CODE	  

DATA	  

Split	  Program	  Pieces	  
CODE	  
F	  G	   E	  A	   D	  B	  

DATA	  

1	  2	  3	  4	  5	  6	  

C	  H	  F	  G	  E	  A	  D	  B	  

1	  2	  3	  4	  5	  6	  

B 

Process	  1	  

C	  H	  

C 

F	  G	  

C	  H	  

1	  2	  3	  4	  5	  6	  

D	  B	  

E	  A	  

Process	  2	  

D	  B	  

C	  H	  

F	  G	  

1	  2	  3	  4	  5	  6	  

e	  a	  

Figure 2: The program is transformed and split once (A and
B), then randomized at every process start-up (C).

1. keeping the size of the translation table small in or-

der not to increase the memory that we saved,

3.1 Code Transformation

2. developing an efﬁcient layer of indirection so that it

is practical,

3. making the translation inaccessible by adversaries,

4. making the solution run on a commodity, unmodi-

ﬁed Linux OS.

Overall Procedure. Oxymoron prevents code reuse at-
tacks by shufﬂing every instruction of a program to a
completely different position in memory so that no in-
struction stays at a known address, thereby making it
infeasible for an adversary to guess or brute-force ad-
dresses. We use a three-step procedure (cf. Figure 2):

A) Code Transformation:

The executable E is
transformed to Position-and-Layout-Agnostic CodE
(PALACE). The result is a PALACE-code executable
PE. The same applies to shared libraries, which can
be treated like executables.

B) Splitting: The PE code is then split into the smallest
possible piece that can be shared among processes: a
memory page. The code of PE now consists of code
pieces PE = p1|p2| . . .| pn.

To enable layout-agnostic code, all references to code
and data are replaced with a unique label. Such a unique
label is an assigned index into a translation table. This
Randomization-agnostic Translation Table (RaTTle) in
turn refers to the actual target (see Figure 3). This is the
key to code sharing among processes, since the byte rep-
resentation of the PALACE code does not change in the
next step, when it is split and individual pieces are shuf-
ﬂed in memory.

0x1000: 

RaTTle	  
α: 0x1200 
β: 0x15F9 
γ: 0x3FFA 

instrA1 
instrA2 
Call α() 
 

Code	  A 
ret B 

0x1200:  instrB1	  
 
instrB2 
ret 

Figure 3: Control-ﬂow is redirected through the RaTTle rather
than jumping to addresses directly.

USENIX Association  

23rd USENIX Security Symposium  435

3

3.2 Splitting

3.4 Addressing the RaTTle

Splitting ensures that the resulting pieces can be mapped
into different processes at different addresses. As
PALACE code references every target through a unique
label in the RaTTle, it can be split without the need
for traditional relocation, which rewrites addresses that
hence have changed.

The PALACE code is split into page-sized pieces.
If
those pieces are later shufﬂed, it must be assured that the
original semantics of the program are kept intact. This is
essential when control ﬂows from the end of one piece to
the piece that was adjacent to it in the original program
code. Thus, we need to insert explicit control ﬂows be-
tween consecutive code pieces that might be moved away
in a later stage of randomization. These explicit links
only need to be inserted as the last instruction of a piece
to ensure that control indeed ﬂows to the intended suc-
cessor (see Figure 4). After the links have been inserted,
the code pieces can be randomized in memory without
violating the original program semantics.

0x11F7 

0x1200 

0x12C9 

Memory	  Page	  

mov  add  push jump 

Memory	  Page	  
mov 

add  mov 

jump 

Memory	  Page	  
pop  jne 
mov 

RaTTle	  
0: 0x12C9 
4: 0x1200 
8: 0x148D 

Figure 4: Filling a page with instructions and linking them
with explicit control ﬂow transfers.

3.3 Randomization

Modern OS loaders for shared libraries already support
Address Space Layout Randomization (ASLR), i.e. they
load the code, data, and stack segments at random base
addresses. We leverage this fact by putting every mem-
ory page in its own loadable segment of the executable
ﬁle or of the shared library. As the page-sized code
pieces are already transformed to PALACE code, no re-
location of addresses is needed. An ASLR-enabled com-
modity loader can blindly load all pieces at random ad-
dresses. Consequently, each process can have its own
permutation of the randomization. Only the RaTTle
needs to be kept up to date with a per-process random-
ization (see “Populating the RaTTle”).

At ﬁrst glance, it might seem we have only shifted the
problem of addressing functions in code to securely ad-
dressing the RaTTle. However, our approach enables se-
cure access to the RaTTle without access for adversaries.
We ﬁrst explain why we chose the more involved realiza-
tion of the RaTTle and not existing approaches, such as a
ﬁxed address, a ﬁxed register or the Global Offset Table
(GOT). As already alluded to by Shacham et al. [25], the
following techniques have drawbacks:
Fixed. Storing the RaTTle at a ﬁxed address in memory
allows for its address to be hard-coded in the in-
structions themselves. Unfortunately, a hard-coded
address restricts the table to a ﬁxed position. This
fact can be exploited by an attacker.

GOT. Accessing the GOT is realized by using relative
addresses, which forfeits sharing as discussed ear-
lier. Moreover, several attacks are known that deref-
erence the GOT [5].

Register. A dynamic address that is randomly chosen
for every process could be stored in a dedicated ma-
chine register. However, this register would need
to be sacriﬁced and every original use of that reg-
ister must then be simulated with other registers or
the stack. Moreover, a leakage vulnerability could
reveal the address of the RaTTle.

Our Approach. Our RaTTle does not suffer from the
aforementioned drawbacks. We use the x86 feature of
memory segmentation to address and at the same time
hide the RaTTle from adversaries. X86’s segmentation
is disused today because it has been superseded by mem-
ory paging. Memory paging, also called virtual mem-
ory, allows a ﬁne-grained mapping of memory on a per-
process basis and is much more versatile than segmen-
tation. However, segmentation is still available in mod-
ern processors and in combination with paging allows
for the security we need for the RaTTle. Additionally,
as segmentation is a hardware feature and we can use
it to implement the translation table, it is very efﬁcient.
Segmentation allows the memory to be divided in user-
deﬁned segments that may overlap. Segmentation is re-
alized in the processor by adding a user-deﬁned offset to
all addresses the code handles (see Figure 5).
Segmentation allows different so-called segment descrip-
tors to be created, each with their own base address and
limit, i.e. the start and length of that segment. The list of
these segment descriptors is kept in the Global Descrip-
tor Table (GDT, see Figure 5). Segment selectors must
then point to exactly one segment entry in that GDT. Seg-
ment selection is done using dedicated segment selector
registers such as CS (Code Segment), DS (Data Segment),

436  23rd USENIX Security Symposium 

USENIX Association

4

Code	  

instrA1 
instrA2 
 
Call 0xABC() 

A 
ret B 

instrB1	  
instrB2 
ret 

0x1A06: 

0x1ABC: 

GDT	  

CS 

Oﬀset:	  0x1000 

+

Figure 5: Code using segments as offsets for addresses.

SS (Stack Segment) and three general purpose segment
selectors ES, FS and GS.

Position-and-Layout-Agnostic CodE (PALACE). The
trick we use is the fact that segments can be selectively
overridden on a per-instruction basis. In this way, a sin-
gle instruction may use an addressing that is relative to
the RaTTle, thereby indexing the RaTTle to change con-
trol ﬂow or to access data. For example, call *%fs:0x4
dereferences the double-word stored at %fs:4 and calls
the function stored at that double-word. If we let the seg-
ment selector FS point to the randomly chosen address of
the RaTTle, we effectively index the RaTTle by an offset
of 4 (see Figure 6).

0x245A: 

 
0x87CD:  instrB1	  
instrB2 
ret 

Code	  

instrA1 
instrA2 
 
Call %fs:*0x4 
ret 

A 
B 

GDT	  

FS 

Oﬀset:	  0x6F9B 

0x6F9B:	  
*

RaTTle	  
0: 0x2AB9 
4: 0x87CD 
8: 0x1A34 

+	  

Figure 6: The RaTTle in Action: Indexed through the GDT and
dereferenced using an indirect call; all in one instruction.

In PALACE code, we substitute each branch and jump
instruction with an %fs segment override and a unique in-
dex. When not using the FS segment override, code does
not have access to the RaTTle because it uses a differ-
ent segment. The address of a segment, and hence of the
RaTTle, cannot be read from user space because the local
and global descriptor tables point to kernel space mem-
ory which is inaccessible from user space. This makes
the address of the RaTTle inaccessible.

As a segment selector for the RaTTle, we chose the gen-
eral purpose segment selector register FS, as already used
in the example above. To the best of our knowledge, this
register is unused. The only use we found is in the Win-
dows emulator Wine that uses segmentation for its 16-bit
Windows emulation.

Efﬁcient Data Access. Data can be accessed in a sim-
ilar way, but through the Global Offset Table (GOT).
The GOT is used in position-independent code such as
libraries anyway. We just need to substitute the way
the address of the GOT is calculated with an indirection
through the RaTTle. Further access is done through the
GOT as in traditional position-independent code. This is
explained in more detail in Section 4.5.
Populating the RaTTle. The RaTTle is the only part of
the code that needs rewriting at load time. The RaTTle is
empty in the ELF executable ﬁle on disk and its memory
gets initialized by the loader with the help of relocation
information. This relocation information points to the ac-
tual symbols that each RaTTle index refers to. The Linux
loader automatically takes the relocation information to
rewrite the RaTTle at program load [6].

4 Design Details

With the ingredients described earlier, we can put to-
gether our mitigation against code reuse attacks that is
efﬁcient, lightweight and shares code and data between
processes.

4.1 Design Decisions

There are several ways to implement PALACE. A
PALACE executable can be produced by a compiler, or
it can be transformed from a traditional executable using
static or load-time translation.
Compiler Support. The same way contemporary com-
pilers support PIC,
they can be augmented to emit
PALACE code. Based on the principles of PALACE
code introduced in the previous Idea section, the com-
piler needs to generate PALACE code and put it in subse-
quent memory-page-sized chunks. It is then ready to be
loaded by a traditional loader that permutes the chunks
prior to execution of the code.
Static Translation.
If the source is not available, an
existing executable can be transformed to PALACE by
means of static translation [14, 15]. Static translation
reads an executable or shared library ﬁle from disk,
disassembles it, transforms the instructions, and writes
a modiﬁed executable ﬁle back to disk.
In our sce-
nario, static translation keeps most of the instructions
untouched while only replacing code and data references
with the appropriate indirection through the RaTTle.
Load-time Translation. Load-time Translation can be
regarded as a static translation that happens automati-
cally at very load-time, after the executable or library has
been read from disk into memory but before it starts exe-
cution. This method is often referred to as binary rewrit-

USENIX Association  

23rd USENIX Security Symposium  437

5

ing. Its advantage is that a process can be randomized at
every startup. In our scenario, however, we do not need
load-time translation as we can achieve a randomization
at load-time with the specially crafted PALACE chunks
in the executable ﬁle.

Our Choice. We want to stress that Oxymoron can be
implemented by a compiler that simply emits PALACE
code in the ﬁrst place instead of traditional code. We
could have implemented Oxymoron as a compiler solu-
tion. However, this would have required us to modify ex-
isting compilers. Instead, we built a legacy-compatible
solution that uses static translation and can be built on
an existing ﬁne-grained memory randomization frame-
work, which already uses static translation. We built
Oxymoron on the existing framework Xifer provided by
Davi et al. [13].

In theory, a static translation approach may seem frag-
ile because it needs a perfect disassembly. However,
static translation can be tuned to reliably disassemble
code generated by a particular compiler with known and
carefully chosen parameters. Besides, in this paper we
use the translation from traditional x86 code to PALACE
code as a comprehensible running example that demon-
strates how PALACE code looks in contrast to traditional
x86 code.

In both cases, compiler and static translation, the gener-
ated PALACE code of the executables and libraries can
be read by a commodity Linux. The Linux OS loader
will detect the executable as being ASLR-enabled and
will randomize its base address. Unfortunately the com-
modity loader does not randomize the program segments
individually but keeps their relative distances. For tradi-
tional position-independent code that was necessary so
that code in the .text section can still reference objects
in the .data section by their relative distance to the cur-
rent instruction pointer. However, for PALACE this lim-
itation is not required. We want to achieve a more ﬁne-
grained randomization by allowing an individual ran-
domization of each program segment, which could be as
small as a memory page. This can be achieved by re-
questing a special linker in the program header, which
randomizes the segments individually.

4.2 Setting up the RaTTle

The RaTTle needs to be populated with all references
in the executable and the table needs to be loaded at a
random address. Moreover, one table does not sufﬁce
for the interaction of several shared libraries. Before we
can use PALACE code, we need to set up the RaTTle as
follows:

1. Assign every reference in code a unique number that

will act as an index into the RaTTle,

2. Fill the RaTTle with the actual, current, random ad-

dresses of the original targets, and

3. Set up segmentation so that a free segment selector

points to the RaTTle and we can index the RaTTle.

In step 1, the absolute addresses of the original pro-
gram are saved in a hash set. Then, every address is
assigned an ascending index. This ensures that the ta-
ble does not grow unnecessarily large. Because the ﬁ-
nal, random addresses are unknown before the process is
started, the RaTTle cannot be ﬁlled until start-up of the
process. As we want to avoid modiﬁcation of the operat-
ing system loader, we chose a method that is able to ﬁll
the RaTTle using only traditional features of the loader.
Such a feature is relocation. Relocation information tells
the loader which objects in the executable ﬁle or in the li-
brary must be overwritten with current addresses at load
time. Therefore, we add relocation information for each
RaTTle index to the ﬁnal executable/library ﬁle. This en-
sures that the loader rewrites each index so that it points
to the corresponding position of code or data that this in-
dex represents. As a result, the randomized addresses of
the code pieces are automatically written into the RaTTle
by the operating system loader.

4.3 Setting up Segmentation

In order to ﬁnd the RaTTle in memory, we need to set
up segmentation so that a pre-deﬁned segment points to
the beginning of the table. Unfortunately, we cannot use
relocation information for this purpose, because neither
setting up segmentation nor setting segment selectors is
supported by relocation information. Setting up segmen-
tation via the Global Descriptor Table (GDT) would re-
quire kernel modiﬁcations. Since the goal is to avoid
operating system modiﬁcations in order to stay legacy
compatible, this is not an option. Luckily, the x86 archi-
tecture additionally supports a so-called Local Descrip-
tor Table (LDT). The LDT can be switched for every ad-
dress space, so that Linux emulates a per-process LDT.
This is a perfect feature for enabling Oxymoron on a per-
process basis.

The set-up of the LDT and the segment selector that
points into the LDT is done in initialization code. To
this end, we leverage the ELF executable format’s ini-
tialization code that resides in the .init section. Code
in this section is ensures to be executed before any other
code. This init code ﬁgures out the address at which the
RaTTle has been randomly loaded by the loader and sets
up the LDT accordingly. After the initialization code has
run, the segment selector FS points to the random address

438  23rd USENIX Security Symposium 

USENIX Association

6

of the RaTTle. The PALACE code can now work as in-
tended.

4.4 Control Flow and Data

Code. Control ﬂow branches or function calls that target
another memory page need to be replaced with an indi-
rection through the RaTTle. The simplest case is a direct
call or an unconditional jmp to a different place in code:

Address

Before

8050512:

call 0x8050c08 

RaTTle:

After
call %fs:4
[0] ..........
[4] 0x8050c08 

Only branches that reference code outside of the current
memory page must go through the RaTTle. Code and
data access within one memory page may be encoded
position-relative (e.g., call +90).

two levels of indirection.

If the to-be-replaced instruction is an indirect jump,
the translation is slightly larger due to the fact that
x86 does not support
It
is either possible to use the RaTTle to get the ad-
dress of the second indirection and then dereference
that using an indirect jump or to use a trampoline.
is slightly faster:
We use a trampoline because it
After
jmp %fs:4
8050c08 

Before
jmp *0x80a00012 
8050c08 

Address
8050512:
80a00012:

RaTTle:

[0] ..........
[4] jmp *80a00012 
A slightly more involved case is a conditional
jump because there is no equivalent conditional
in-
jump. Our solution is a bit more involved:
direct

Before
cmp %eax, %ebx 
jne 0x8050590 

Address
8050512:
8050514:
8050516:
8050518:
RaTTle:

After
cmp %eax, %ebx
jne 0x8050518 
jmp 0x805051a 
jmp *%fs:4 
[0] ..........
[4] 0x8050590 

An indirect jump, such as jmp *%eax does not need to be
replaced at all. However, the used register (in this exam-
ple %eax) must point to the correct randomized position
in memory. This is either ensured by the compiler that
generated PALACE code or by the translation from tra-
ditional code.
In either case, a register is loaded with
a code address. Optionally, this address is modiﬁed to
mimic jump tables or C++ vTables, and then the indi-

rect jump transfers control ﬂow to the address stored in
the register. To load a code address to the register before
it is modiﬁed, a ﬁxed address is copied to the register.
This is similar to mov $0x8402dbc, %eax.
In the case
of PALACE, this step needs an indirection to conceal
the actual address and to make the address exchangeable
by the RaTTle. In PALACE code this register loading
looks like this: mov %fs:$0x4, %eax. This copies an
address stored as an entry in the RaTTle to the register
%eax. Then, some mathematical operations can be per-
formed, such as adding the offset into C++ vTables and
ﬁnally the indirect jump is performed as in traditional
x86: jmp *%eax.

Data Access. Accessing data through the RaTTle is done
in exactly the same way. An indirect memory operation
is used to read or write data from or to an address stored
in the RaTTle. mov %fs:$0x4, %ebx is used to read the
ﬁrst entry (4 bytes) of the RaTTle into register %ebx and
vice versa the operation mov %ebx, %fs:$0x8 copies the
register %ebx to the second entry (8 bytes) of the RaTTle.

4.5

Inter-Library Calls and Data

Control ﬂow and access to data is not restricted to one
library or executable. Naturally, these code elements fre-
quently use each other’s functions and data. Some oper-
ating systems, like Windows, use relocation information
to directly patch the control ﬂow so that it points into
a library after it has been loaded. Linux, on the other
hand, uses the procedure linkage table (PLT) to link calls
to libraries with the advantage of lazy loading.1 In con-
trast, we use an indirection through the RaTTle for ev-
ery library call or access to global library data because
this approach conceals the actual address of the loaded
library and has only minor performance impact.

Inter-Library Data. Libraries can export data to be used
by the executable main process or other shared libraries.
Since it is known a priori which data is accessed in an-
other library, each reference gets a place-holder in the
GOT which can be accessed as described above. When
the appropriate library is loaded by the loader, it auto-
matically updates the GOT thanks to the relocation info
pointing to this entry in the GOT.

The following is an example of
typical position-
independent code that uses a GOT to access data: The
code is ﬁrst calling the next instruction, thereby pushing
its own address as a return address to the stack. Follow-
ing, this very address is popped off the stack to get the

1First, the PLT entries do not point to the actual procedure inside a
library because it has not been loaded yet. Instead, they point to code
that loads the library and then rewrites the PLT to link the call to the
actual target procedure.

USENIX Association  

23rd USENIX Security Symposium  439

7

absolute address of the currently running code. The ad-
dress of the GOT is calculated by adding a known offset.

Address

Before

8050512: call 0x8050517 
8050517: pop %ebx 
8050518: add $1234, %ebx 
805051e: mov 4(%ebx), $1 

Call next instruction

ebx ß 8050517

ebx ß GOT

GOT[4] ß 1

Figure 7 ﬁrst sets FS to point to RaTTle2. RaTTle2 is
the second selector in the LDT. Hence, the trampoline
code in RaTTle1 assigns 10111bin = 23 to FS, which cor-
responds to a segment selector of “2” (see Appendix A).
The trampoline code then jumps to index 0, which now
corresponds to currently active RaTTle2. Because the
trampoline uses a call instruction to ﬁnally call into the
other library, control ﬂow returns to the trampoline where
FS is restored to its former value.

When transforming this piece of code to PALACE,
only the calculation of the GOT needs to be substi-
tuted.
In this case, the three former instructions get
compressed to a single instruction with segment over-
ride. Interestingly, this is a faster method of accessing
the GOT than the currently used PC-relative addressing.

Address

After

8050512: mov %fs:4, %ebx 
805051e: mov 4(%ebx), $1 
RaTTle: 0x805174B 

ebx ß GOT
GOT[4] ß 1

Points to GOT

Inter-Library Calls.
Inter-library calls are calls from
one loaded library to another or from the main executable
to a library. In theory, these calls are no different from
a call within the same library or executable because the
RaTTle can simply point to code in another library. How-
ever, in practice, this would require the RaTTle to re-
ﬂect all possible combinations of loaded libraries. There-
fore, we resort to a solution in which every loaded library
brings its own RaTTle and an inter-library call acts as a
trampoline that changes the segment selector FS to point
to the corresponding RaTTle of another library prior to
jumping into that library (see in Figure 7).

0x1200: 

Code1	  

instr 
instr 
 
call %fs:8 
ret 

Code2	  

0x6721: 

instr 
instr 
 
ret 

4.6 Debugging

Debugging information augments the executable or li-
brary ﬁle with annotations describing which memory ad-
dresses correspond to which variables or lines of code.
These stored addresses must be compatible with Oxy-
moron randomized addresses. Since Oxymoron is imple-
mented as a static translation tool, the original debugging
information needs to be translated as well. Currently
Oxymoron supports the common DWARF [3] ﬁle format
which can be read by the gdb or other debuggers. This
way, it is possible to teach gdb the randomized addresses
so that gdb can still step through the code, inspect vari-
ables etc. like for the non-randomized executable.

5 Evaluation

In this section, we evaluate the effectiveness of Oxy-
moron empirically as well as theoretically. In order to
demonstrate the efﬁciency, we used the de-facto standard
performance benchmark SPEC CPU2006 as well as mi-
cro benchmarks to measure cache hit/miss effects.

First, we inspect the security of the RaTTle itself to ver-
ify that it did not open the ﬂood gates for other attack
vectors. Then, we compare the slightly different random-
ization of memory pages that this solution entails to the
more classical memory randomization solutions in order
to get an understanding of the implied security.

RaTTle2	  

5.1 Practical Security Evaluation

RaTTle1	  

0: 0x1200 
4: 0x12C9 
8: mov LIB2, %fs 
   call *%fs:0 
   mov LIB1, %fs 
   ret 

LDT	  
Oﬀset:	  0x8F9B 
Oﬀset:	  0x97A2 

LIB1:	  

LIB2:	  

Figure 7: Inter-Library Calls: Because the indices overlap, a
new RaTTle needs to be set up before those calls.

Please note the missing “*” in the call %fs:8 of Fig-
ure 7, which means the RaTTle is not de-referenced
rather than used as a trampoline. This trampoline then
lets FS point to the index of the other library’s RaTTle
without the need to know the exact address. Suppose
the function that we want to call is stored at index 0 in
RaTTle2, but RaTTle1 is currently active. The code in

We tested our randomization solution against real-life
vulnerabilities and exploits. The documented vulnerabil-
ities CVE-2013-0249 and CVE-2008-2950 both allow ar-
bitrary code execution by means of return-oriented pro-
gramming [2]. CVE-2013-0249 targets the libcurl li-
brary which handles web requests and is used in dozens
of popular programs, including ClamAntiVirus, Libre-
Ofﬁce, and the Git versioning system. The exploit for
this vulnerability is crafted in such a way that it trig-
gers a buffer overﬂow in libcurl with the ability to over-
write a return address and ultimately execute a chain of
ROP gadgets. The severity of this bug lies in the fact
that it can be triggered remotely when libcurl accesses

440  23rd USENIX Security Symposium 

USENIX Association

8

a prepared resource that is under the control of the ad-
versary. In order to test the exploit, we used the ‘curl’
downloader executable in version 7.28.1, which inter-
nally uses libcurl. We could successfully run arbitrary
code by assembling ROP gadgets at our discretion. Af-
ter curl had been rewritten to use Oxymoron, the exploit
was no longer possible as the addresses that are needed
to successfully mount the attack are unknown due to the
randomization at every program start.

Similarly, the vulnerability CVE-2008-2950 allows for ar-
bitrary code reuse in the PDF library poppler, which
is used by many popular programs such as LibreOfﬁce,
Evince and Inkscape. A specially prepared PDF ﬁle can
trigger an arbitrary memory reference in the poppler li-
brary, ultimately leading to a code reuse attack. After our
attacks against pdftotext using libpoppler 0.8.4 were
successful, we applied Oxymoron. Since the memory ad-
dress of the PALACE-protected process were no longer
known, the exploit was rendered unsuccessful after ap-
plying Oxymoron to the pdftotext executable.

5.2 Security of the RaTTle

Because processes are protected by W ⊕ X (stack execu-
tion prevention), no code can be injected by an attacker.
Hence, the only possibility is to reuse existing code. This
existing (PALACE) code is littered with %fs-preﬁxed in-
structions that implicitly point to the RaTTle due to the
sheer fact they incorporate a reference to %fs. However,
the situation is identical to ﬁnding ROP gadgets in a clas-
sical program, as an attacker needs to know their ran-
domized position in memory in order to chain them to-
gether. The fact that this address is not known to an at-
tacker prevents the reuse of code. In fact, the probability
of guessing a correct address is negligible (see subsection
“Theoretical Security Evaluation”).

The RaTTle holds lots of random addresses and, at ﬁrst
glance, seems like a valuable target for an attacker. The
security of the RaTTle originates from the fact that its
address is unknown and that its content cannot be ac-
cessed. All %fs-instructions are mere replacements for
control ﬂow branches and as such only use the RaTTle
as a layer of indirection without ever knowing the actual
address of the landing position. If an %fs-instruction is
a replacement for data access, the same holds true: The
RaTTle is only used for indirect access of the actual data.
In general, the x86 architecture does not support reveal-
ing addresses that segments point to. The only way to
read the address is to parse the GDT or LDT which both
reside in kernel space. To access the LDT, a user mode
program needs to issue a special syscall. Even if a pro-
gram would consist of ROP gadgets to issue this syscall,
he would still need to know the addresses of the required

ROP gadgets. So this can be reduced to ﬁnding special
instructions that can be used as ROP gadgets. This has a
negligible probability as explained in “Theoretical Secu-
rity Evaluation”.

5.3 Enhanced Security of the RaTTle

It is possible to further enhance the security of the
RaTTle by making it completely inaccessible. The seg-
mentation principle of the x86 architecture allows to dis-
tinguish code access from data access. This way, it is
possible to set up two different RaTTles, one for code
going through %fs and one for data going through %gs.
First of all, in a program without self-modifying code,
there should be no instructions that read data using the
%fs code segment selector. Even if there were, the pro-
cessor would prohibit such access. Further, it is possible
to move the RaTTle completely outside of the normal,
otherwise ﬂat2 data segment (%ds). This results in the
inability for code to ever access the RaTTle without us-
ing proper segment selectors, because it no longer resides
in the accessible segment. This is an effective protec-
tion against leakage and disclosure attacks (see subsec-
tion “Disclosure Attacks”). Also, the call stack could
be protected using this method. If return addresses are
not saved on the regular stack, but rather on a side stack
in a reserved area inside the RaTTle, there is no way
for memory disclosure vulnerabilities to ever read return
addresses and thus they cannot gain information about
function addresses.

5.4 Theoretical Security Evaluation

In this subsection we elaborate on why the entropy of
memory page granularity randomization is still sufﬁcient
for ﬁne-grained randomization and why it is much higher
than traditional ASLR.

First, we show that the entropy induced by a page-
granular randomization is high enough in the sense that
the adversary has only negligible probability of success-
fully guessing an address. We model the adversary’s goal
as mounting a code reuse attack against a running pro-
gram consisting of the executable and its loaded libraries.
Hence, his goal is to know the address of either a par-
ticular function f of interest (return-into-libc attack) or
of several particular instructions i1 . . .i k to build gadgets
from (ROP attack). Since the contents of a memory page
can be extracted from the executable ﬁle, the attacker can
determine in which memory page the instruction in ques-
tion resides. Therefore, the success of the adversary re-

2A ﬂat segment is a segment that covers the entire address space,
i.e. 0x00000000 to 0xFFFFFFFF on a 32-bit system. This is the
default for Windows, Linux and MacOS.

USENIX Association  

23rd USENIX Security Symposium  441

9

n!

lies on the probability of knowing the address of a par-
ticular memory page.
Every memory page is assigned a random address at
load-time. Thus, the ﬁrst page can choose 1 out of n
possible page-aligned address slots. The second 1 out
of n − 1 and so forth. For p total process pages to lay
(n−p)! combina-
out in memory, this yields a total of
tions. The adversary’s probability of correctly guessing
one address is hence the reciprocal (n−p)!
. In a 32 bit ad-
dress space, we have n = 219 = 524,288 possible page
addresses. The probability of guessing one page cor-
rectly therefore is 2−19. That scenario is intuitively iden-
tical to ASLR which only randomizes the base address
of the code. However, when ﬁnding ROP gadget chains,
the page granularity drastically lowers the chance of suc-
cess compared to ASLR because several pages have to
be guessed correctly. For a 128 kB (p = 32 pages) exe-
cutable to lay out in memory, the adversary’s probability
of guessing the correct memory layout therefore is:

n!

Pr(cid:31)Advlayout(cid:30) =

(n− p)!

n!

=

(219 − 25)!

219!

= 2−608

Leakage Attacks in ASLR. A leakage vulnerability in-
advertently reveals a valid, current address inside the
running program.
If the adversary additionally knows
which object or function has been leaked, he knows the
address of that object/function. In the case of ASLR, he
can then infer the current addresses of all other objects
or functions because ASLR has shifted the entire code
segment in memory by changing its base address. Con-
sequently, the relative distances between functions stay
exactly the same.
To model the leakage attack, we assume the adversary
exploits an existing leakage vulnerability thereby learn-
ing a valid address. We assume that this address depicts
the beginning of a particular function that the adversary
knows. That such a leaked address actually constitutes
a function pointer is not very likely but here it models
the best-case scenario for the adversary. Hence, the fol-
lowing calculations give a upper bound of success for an
adversary.
More formally, the adversary has access to an oracle that
can tell which function f has leaked and the adversary
can use the leakage vulnerability to learn the current ad-
dress of A f of the function f . The adversary can then
calculate their difference in memory by calculating their
difference in the executable ﬁle. As their relative posi-
tions did not change in ASLR, the adversary can infer the
current address of f (cid:30) by calculating the difference to the
leaked function f . In the case of traditional ASLR, the
address of any function f (cid:30) can be calculated with proba-
bility 1. Ultimately, the success probability of the adver-

sary entirely depends on the likelihood of ﬁnding such a
leakage vulnerability.
Leakage Attacks in Oxymoron. In our case of memory
page granularity shufﬂing, the relative distance between
functions varies in general since the code segment is not
just shifted en bloc. For any leaked pointer f , there is a
chance that it resides in the same memory pages as the
desired function f (cid:30). For an equal distribution of f (cid:30) in p
pages, the likelihood of f (cid:30) being in the same page as f is
1
p. For a program of a total size of only one memory page
(4kB), both functions f and f (cid:30) must reside in the same
memory page. Under the assumption that both functions
are uniformly distributed, the probability for both to ap-
pear in the same memory page is 1
p for a program size of
p pages. Hence

Pr[AdvPALACE

ret2libc ] ≤

1
p

and Pr[AdvPALACE

ROP

1
pk

] ≤

Disclosure Attack. We distinguish between a leakage
and a disclosure vulnerability. A disclosure vulnerabil-
ity allows an attacker to read arbitrary memory content
given its address. Snow et al. proposed just-in-time code
reuse, which showed that a disclosure vulnerability can
signiﬁcantly reduce the security of ﬁne-grained memory
randomization [26]. Just-in-time code reuse repeatedly
exploits a memory disclosure vulnerability to map por-
tions of a process’ address space with the objective of
reusing the so-discovered code in a malicious way. In a
ﬁne-grained randomization, the memory pages are scat-
tered across the address space and scanning with arbi-
trary memory addresses is very likely to end up in un-
mapped memory.
In order not to trap into unmapped
memory, they rely on a leakage attack to learn a valid
address and then start from this address by disassem-
bling the code in order to follow control ﬂow instruc-
tions. Even ﬁne-grained randomization can be reversed
using their technique by transitively following the con-
trol ﬂow.
However, in our setting of PALACE code, no control
ﬂow branch can be followed by reading memory as such
a branch only constitutes an offsets into the RaTTle. In
order to resolve branches such as call *%fs:4, the at-
tacker would need to know the address of the RaTTle or
%fs, which is not possible, as alluded to earlier. The only
chance an attacker has is to rely on a leakage vulnerabil-
ity to get a valid address. If that address points to data it
is useless to the attacker. If it points to code, the attacker
can only use a disclosure vulnerability to get the contents
of up to a whole memory page (4KB). Otherwise, he is
likely to overrun the page and end up in unmapped mem-
ory which triggers a page fault that kills the program.

442  23rd USENIX Security Symposium 

USENIX Association

10

5.5 Effectiveness of Memory Page Sharing

To have a set basic programs one would typically ﬁnd
on a Linux machine, we used the busybox project, which
incorporates 298 standard Linux commands. Those com-
mand line programs were started and their memory foot-
print was measured using /proc/<PID>/maps. On av-
erage, they mapped 14.9% more code pages than their
unmodiﬁed original. Their data pages were unmodi-
ﬁed. Only the RaTTle consumes memory (see Subsec-
tion 6.1). Compared to ﬁne-grained memory randomiza-
tion solutions that impede code page sharing, Oxymoron
on average saves about 85% of program memory.

The number of instructions per benchmark reﬂect the to-
tal number of instructions from the executable ﬁle itself
plus its dependent libraries. Note, that this measurement
rewrites the entire C-library and other dependent libraries
again for each benchmark and is hence slower than just
translating the main executable.
Run-Time Overhead. The run-time overhead intro-
duced by the translation through the RaTTle as well
as the introduction of jmp instructions to connect pages
(cf. section 3) is measured in Figure 8. The average
run-time overhead of all benchmarks is only 2.7% for
the PALACE code and 0.1% for the additionally needed
chunking in memory page-sized pieces (4096 bytes).

6 Performance Evaluation

To evaluate the efﬁciency of Oxymoron, we did not
only use standard command line tools from busybox but
conducted CPU benchmarks with PALACE-enabled pro-
grams using the de facto standard SPEC CPU2006 inte-
ger benchmark suite. All benchmarks were performed
on an Intel Core i7-2600 CPU running at 3.4 GHz with 8
GB of RAM.

12.00% 

10.00% 

8.00% 

6.00% 

4.00% 

2.00% 

0.00% 

Performance Run-Time Overhead 

Page	  Jump	  (avg.	  0.5%)	  
RaTTle	  (avg.	  2.7%)	  

Static Translation Overhead. Before the executable
and libraries can be shufﬂed in memory, they either need
to be compiled with an PALACE-enabled compiler or
they must be converted using static translation (cf. sec-
tion 3). Even though the translation only needs to be
performed once, it must be efﬁcient. We measured the
rewriting time for all benchmark programs of the Spec
CPU suite. The rewriting process is not exactly linear,
but on average achieves between 35,000 and 700,000 in-
structions per second. An overview of the timings of sev-
eral programs is given in Table 1.
Total #

Benchmark
483.xalancbmk
403.gcc
471.omnetpp
400.perlbench
445.gobmk
464.h264ref
456.hmmer
458.sjeng
473.astar
401.bzip2
462.libquantum
429.mcf

of Instructions
1,111,779
942,244
238,978
322,084
226,661
170,942
54,582
40,438
32,502
28,087
15,788
12,268

Figure 8: SPEC CPU2006 integer benchmark results.

Cache Miss Penalty. We also evaluated the cache ef-
fects of our randomization. This is important, since mod-
ern processors assume locality of code, which might be
thwarted by wild jumping in the code due to the random-
ization. Keeping cache effects in mind, our implemen-
tation optimizes jumping behavior in order to optimize
performance under real-life conditions. Our cache ex-
periments showed that PALACE and the randomization
have no measurable cache effect.

For this impact to be measured, we handcrafted code
consisting of interdependent add instructions with a to-
tal length of one L1 cache line. These instructions are
aligned in memory in such a way that they start at the
beginning of a cache line and re-occur such that every
cache set and every cache line is ﬁlled after execution.
We inserted equidistant jmp instructions and measured
the overhead of 100,000 runs on an Intel Core i7-2600
(32 KB L1 cache, 64 bytes per line). Our results show
that the performance impact is not measurable up to ev-
ery seventh instruction being a jmp.
If every sixth in-
struction is a jmp, a negligible overhead of 0.4% is in-
troduced. Our analysis of the busybox code showed that
after translating it to PALACE, on average indeed every
6th instruction was a branch or jump.

Rewriting
Time (s)
4.321
3.667
0.316
1.084
6.744
0.396
0.116
0.101
0.032
0.056
0.024
0.024

Table 1: Timings for static rewriting that needs to be done at
least once. The total # of instructions include the executable
and all its shared libraries.

USENIX Association  

23rd USENIX Security Symposium  443

11

6.1 Memory and Instruction Overhead

7 Related Work

Compared to a traditional program, the introduction of
PALACE code replaced control ﬂow branches with other,
%fs-relative, instructions. For all SPEC2006 benchmark
executables, on average 9% +-1.7% of all instructions are
calls that needed to be replaced by indirections through
the RaTTle. GOT indirect calls through the RaTTle are
only 0.03% of all instructions.

Additionally, a PALACE binary executable ﬁle is slightly
larger than a traditional executable ﬁle because each code
page (4 KB) is a separate ASLR-enabled section in the
executable ﬁle.

During run-time, the memory footprint also slightly in-
creases because the RaTTle has to be kept in memory. Of
course, this run-time memory usage is accompanied with
the achieved goal of memory savings due to the sharing
of code pages with other processes.

Encapsulating each memory page in a separate segment
in the ELF ﬁle requires the allocation of one section
header and one program header per page. A section
header is 40 bytes and the ELF program header is 32
bytes which leads to an overhead of 72 bytes per 4096
byte memory page, or ≈ 1.76%. Figure 9 depicts both
the increase of instructions due the static translation as
well as the increase of the ELF section and program
headers.

File Size Overhead 

ELF	  File	  (avg.	  1.76%)	  
Instruc9ons	  (avg.	  12%)	  

20.0% 
18.0% 
16.0% 
14.0% 
12.0% 
10.0% 
8.0% 
6.0% 
4.0% 
2.0% 
0.0% 

Figure 9: Memory overhead after static translation.

Run-Time. The size of the RaTTle depends on how
many references the code has. If a target is referenced
more then once, e.g., the GOT, only one index is saved
in the RaTTle. For all ﬁles that belong to the SPECint
CPU2006 benchmark, on average 19% of the code seg-
ment had to be added in the form of a RaTTle.

Over the course of the last several years, code reuse at-
tacks and their mitigation has been an ongoing cat and
mouse game. Some of the code reuse mitigation tech-
niques address the problem at its roots by preventing
buffer overruns or by conﬁning the control ﬂow to the
destined control-ﬂow graph. Other mitigation techniques
make it hard for the adversary to guess or brute-force
addresses that are necessary for successful execution of
malicious code.

In this section, we focus on approaches that use ﬁne-
grained memory randomization as a means to mitigate
code reuse attacks and work that nulliﬁes memory ran-
domization or even ﬁne-grained memory randomization.

One way to categorize ﬁne-grained memory random-
ization solutions is by their implementation: There ex-
ist compiler-based solutions, static or load-time transla-
tions, and dynamic translations. Another category di-
mension is whether they randomize only once, every
time the program starts, or even continuously during pro-
gram execution.

Compiler-Based Solutions. If a program is not random-
ized, an adversary can learn the layout, i.e. addresses, of
all functions and gadgets and hence use them in a ret2libc
or ROP attack. The idea of compiler-based approaches is
to randomize the layout of a program and to install differ-
ently randomized copies on different computers so that
the program layout is not predictable for an adversary.

Cohen et al. [10] suggested compiling different versions
of the same program. In a modern setting this technique
can be applied within an AppStore to distribute individu-
ally randomized software. Similarly, Franz et al. [16, 19]
have suggested automating this compiler process and
generate a different version of a program for every cus-
tomer. The authors suggest that app store providers in-
tegrate a multicompiler in the code production process.
However, those approaches have several shortcomings:
First, app store providers have no access to the app
source code. This requires the multicompiler to be de-
ployed on the developer side, who has to deliver possibly
millions3 of app copies to the app store. Second, the pro-
posed scheme requires software update processes to cor-
rectly patch app instances that in turn differ from each
other. Finally, the most severe drawback of compiler-
based solutions is the fact that the diversiﬁed program
remains unchanged until an update is provided, which
increases the chance of an adversary compromising this
particular instance over time.

3According to Gartner [4], the number of app downloads is about

102 billion in 2013.

444  23rd USENIX Security Symposium 

USENIX Association

12

Similar to Oxymoron is the idea of using a compiler-
based solution to divide a shared library into even more
fragments. Code Islands [30] follows this path and com-
piles groups of functions to several shared libraries in-
stead of one shared library containing all the functions.
These (potentially thousands of shared library ﬁles) are
then put in a container whose format is understood by a
modiﬁed loader which maps the libraries in the particular
process. However, their solution needs a modiﬁed loader
to support the proprietary format. Executables then need
to load literally thousands of shared libraries, while each
library constitutes a single function.

In contrast, Bhatkar et al. [8] presented a source code
transformer and its implementation for x86/Linux. The
main idea is to augment any source code with the capa-
bility of self-diversiﬁcation for each run. In particular,
features are added to the source code that allow the pro-
gram to re-order its functions in memory in order to mit-
igate code reuse attacks. Their tool can also be applied
to shared libraries if their source code is available. How-
ever, their solution induces a run-time overhead of 11%
and apparently needs access to the source code.

Static Translation.
Static translation reads an exe-
cutable or shared library ﬁle from disk, disassembles
it and transforms the instructions according to a pre-
deﬁned pattern within the executable ﬁle itself. Kil
et al. [20] use static translation for their Address Space
Layout Permutation (ASLP). ASLP performs function
permutation without requiring access to source code.
The proposed scheme statically rewrites ELF executables
to permute all functions and data objects of an applica-
tion. The presented scheme is efﬁcient and also supports
re-diversiﬁcation for each run. However, only the func-
tions themselves are permuted, not their content.

Pappas et al. proposed randomizing instructions and reg-
isters within a basic block to mitigate return-oriented
programming attacks [21]. However, the proposed solu-
tion cannot prevent return-into-libc attacks (which have
been shown to be Turing-complete [27]), since all func-
tions remain at their original position.

Load-Time Translation. Load-time translation solu-
tions are similar to static translation but apply the transla-
tion at load time in order for the processes to beneﬁt from
a re-randomization at each run. This can be achieved
by several means, such as rewriting the binary ﬁle after
it has been loaded but before execution [29, 13]. Such
solutions usually suffer from the fact that each execu-
tion either needs a translation/rewriting phase each time
a process is started or they need a prior static analysis
phase [29].

Dynamic Translation. Dynamic translation leaves the
original ﬁle untouched and does not apply binary rewrit-
ing but the program undergoes a dynamic translation,
i.e. the instructions are transformed as they are executed.
Dynamic translation is very similar to Just-in-Time (JIT)
compilation but usually translates from and to the same
instruction set architecture. For example, Bruening pro-
posed the DynamoRIO framework in his PhD thesis [9].
DynamoRIO is able to perform run-time code manipula-
tion. ILR (instruction location randomization) [18] ran-
domizes the location of each single instruction in the vir-
tual address space. For this, a program needs to be an-
alyzed and re-assembled during a static analysis phase.
This is why ILR induces a signiﬁcant performance over-
head (on average 13%), and suffers from a high space
overhead, i.e., the rewriting rules reserve on average 104
MB for only one benchmark of the SPEC CPU bench-
mark suite. For direct calls, ILR can only randomize
the return address in 58% of the calls, meaning that for
a large number of return instructions, ILR needs to do
a live translation for un-randomized return addresses to
runtime addresses.

Constant Re-Randomization.
To the best of our
knowledge,
there are only two papers that actu-
ally implemented and benchmarked re-randomization.
Curtsinger et al. [11] have implemented an LLVM com-
piler modiﬁcation that injects code, which adds the func-
tionality to re-randomize the address of functions every
500 ms. According to [11], their overhead of code, heap
and stack (re-)randomization is 7%.

Giuffrida et al. [17] changed the Minix microkernel to
re-randomize itself every x seconds. This is achieved
by maintaining the intermediate language of the LLVM
compiler for the compiled kernel modules. However, this
procedure has a signiﬁcant run-time overhead of 10% for
a randomization every x = 5 seconds or even 50% over-
head when applied every second.

Common Shortcomings and Nulliﬁcation. All the re-
lated work on ﬁne-grained memory randomization has
in common that they either do not randomize shared li-
braries, or if they do, the difference introduced in the
shared libraries prohibits code sharing.

Furthermore, it is unclear whether ﬁne-grained memory
randomization alone is enough to protect against code
reuse attacks. Recently, Snow et al. [26] showed that
given a memory disclosure vulnerability it is possible to
assemble ROP gadgets on-demand without knowing the
layout or randomization of a process. They explore the
address space of the vulnerable process step by step by
following the control ﬂow from an arbitrary start posi-
tion. After they have discovered enough ROP gadgets

USENIX Association  

23rd USENIX Security Symposium  445

13

they compile the payload so that it incorporates the ac-
tual current addresses that were discovered on-site.

Snow et al. also proposed potential mitigations of their
own attack. However, the proposed solutions are either
very speciﬁc to their heap spraying exploitation or are as
general and slow as frequent re-randomization of a whole
process. The latter is not even secure if the attack takes
place between two randomization phases.

To the best of our knowledge, in this paper we present
the ﬁrst solution that addresses both problems: (1) It is
secure against the new just-in-time ROP by Snow et al.
(2) It proﬁts from code sharing despite secure random-
ization.

8 Discussion

In this section we would like to discuss the general ap-
plicability of Oxymoron but also its limitations.

The PALACE code presented in this paper only relies on
segmentation as an additional hardware feature. Hence,
Oxymoron also works in virtualized environments. We
successfully tested Oxymoron in software and hardware
virtual machines as well as on a para-virtualized Linux
using the Xen hypervisor.

The solution presented herein was implemented for the
32 bit x86 architecture. While its 64 bit successor has
limited supported for segmentation, the necessary offset
functionality of %fs segment registers is still available.
However, in 64 bit mode, segments do no longer support
to set a limit, which makes the RaTTle accessible as data
if its address is known.

Another interesting avenue that we did not investigate is
just-in-time (JIT) compiled code, such as the Java run-
time environment. Those JIT-compilers would need to
be adapted in order to emit PALACE-enabled code, oth-
erwise the traditional code they emit is not protected.

9 Conclusion

We presented a novel technique for ﬁne-grained memory
randomization that still allows sharing of code among
processes. This makes ﬁne-grained memory randomiza-
tion practical as the memory overhead is signiﬁcantly re-
duced in contrast to other randomization solutions. Oxy-
moron is effective, i.e., code reuse attacks can be mit-
igated, memory leakage vulnerabilities can no longer
be used to revert the randomization, and we presented
the ﬁrst solution to be secure against just-in-time code
reuse attacks. The randomized addresses are protected
by hardware means, which is an unprecedented security
level with a run-time overhead of only 2.7%.

An interesting side effect of our PALACE code is that
accessing the Global Offset Table (GOT) uses fewer in-
structions than the state-of-the-art technique of using
PC-relative addressing. Maybe our method could be a
slightly faster alternative for accessing the GOT.

References

[1] Common Weakness Enumeration – Top Software Vulnerabilities.

http://cwe.mitre.org/top25/index.html.

[2] Database of Common Security Vulnerabilities and Exposures.

http://cve.mitre.org.

[3] Dwarf 2.0 debugging format standard. http://www.dwarfstd.

org/doc/dwarf-2.0.0.pdf.

[4] Gartner Says Mobile App Stores Will See Annual Downloads
http://www.gartner.com/

Reach 102 Billion in 2013.
newsroom/id/2592315.

[5] How to hijack the Global Offset Table with pointers for root

shells. http://www.open-security.org/texts/6.

[6] Executable and Linking Format (ELF). Tool Interface Standards

Committee, May 1995.

[7] ABADI, M., BUDIU, M., ERLINGSSON, U., AND LIGATTI, J.
In ACM Conference on Computer and

Control-ﬂow integrity.
Communications Security (CCS) (2005), ACM, pp. 340–353.

[8] BHATKAR, S., SEKAR, R., AND DUVARNEY, D. C. Efﬁcient
techniques for comprehensive protection from memory error ex-
ploits. In USENIX Security Symposium (2005), USENIX Associ-
ation.

[9] BRUENNING, D. Efﬁcient, Transparent and Comprehensive Run-
time Code Manipulation. PhD thesis, Massachusetts Institute of
Technology, 2004.

[10] COHEN, F. B. Operating system protection through program evo-

lution. Computer & Security 12, 6 (Oct. 1993), 565–584.

[11] CURTSINGER, C., AND BERGER, E. D. Stabilizer: statistically
sound performance evaluation. In ACM SIGARCH Computer Ar-
chitecture News (2013), vol. 41, ACM, pp. 219–228.

[12] DAVI, L., DMITRIENKO, A., EGELE, M., FISCHER, T., HOLZ,
T., HUND, R., N ¨URNBERGER, S., AND SADEGHI, A.-R.
MoCFI: A Framework to Mitigate Control-Flow Attacks on
Smartphones. In Symposium on Network and Distributed System
Security (NDSS) (2012).

[13] DAVI, L. V., DMITRIENKO, A., N ¨URNBERGER, S., AND
SADEGHI, A.-R. Gadge me if you can: Secure and efﬁcient ad-
hoc instruction-level randomization for x86 and arm. In 8th ACM
SIGSAC symposium on Information, computer and communica-
tions security (ACM ASIACCS 2013) (2013), ACM, pp. 299–310.
[14] DE SUTTER, B., DE BUS, B., AND DE BOSSCHERE, K.
Link-time binary rewriting techniques for program compaction.
ACM Transactions on Programming Languages and Systems
(TOPLAS) 27, 5 (2005), 882–945.

[15] EUSTACE, A., AND SRIVASTAVA, A. Atom: A ﬂexible interface
for building high performance program analysis tools.
In Pro-
ceedings of the USENIX 1995 Technical Conference Proceedings
(1995), USENIX Association, pp. 25–25.

[16] FRANZ, M. E unibus pluram: massive-scale software diversity
as a defense mechanism. In Proceedings of the 2010 workshop
on New security paradigms (2010), ACM, pp. 7–16.

[17] GIUFFRIDA, C., KUIJSTEN, A., AND TANENBAUM, A. S.
Enhanced operating system security through efﬁcient and ﬁne-
grained address space randomization. In Proceedings of the 21st
USENIX conference on Security symposium (2012), USENIX As-
sociation, pp. 40–40.

446  23rd USENIX Security Symposium 

USENIX Association

14

[18] HISER, J. D., NGUYEN-TUONG, A., CO, M., HALL, M., AND
In IEEE

ILR: Where’d My Gadgets Go?

DAVIDSON, J. W.
Symposium on Security and Privacy (2012).

[19] JACKSON, T., SALAMAT, B., HOMESCU, A., MANIVANNAN,
K., WAGNER, G., GAL, A., BRUNTHALER, S., WIMMER, C.,
AND FRANZ, M. Compiler-generated software diversity. In Mov-
ing Target Defense, vol. 54 of Advances in Information Security.
Springer New York, 2011, pp. 77–98.

[20] KIL, C., JUN, J., BOOKHOLT, C., XU, J., AND NING, P. Ad-
dress space layout permutation (ASLP): Towards ﬁne-grained
randomization of commodity software. In ACSAC (2006).

[21] PAPPAS, V., POLYCHRONAKIS, M., AND KEROMYTIS, A. D.
Smashing the gadgets: Hindering return-oriented programming
using in-place code randomization. In IEEE Symposium on Secu-
rity and Privacy (2012).

[22] PAX TEAM. http://pax.grsecurity.net/.
[23] PAX TEAM. PaX Address Space Layout Randomization (ASLR).

http://pax.grsecurity.net/docs/aslr.txt.

[24] SHACHAM, H. The Geometry of Innocent Flesh on the Bone:
In ACM
Return-into-libc Without Function Calls (on the x86).
Conference on Computer and Communications Security (CCS)
(2007).

[25] SHACHAM, H., JIN GOH, E., MODADUGU, N., PFAFF, B., AND
BONEH, D. On the Effectiveness of Address-space Randomiza-
tion. In ACM Conference on Computer and Communications Se-
curity (CCS) (2004).

[26] SNOW, K. Z., MONROSE, F., DAVI, L., DMITRIENKO, A.,
LIEBCHEN, C., AND SADEGHI, A.-R. Just-in-time code reuse:
On the effectiveness of ﬁne-grained address space layout random-
ization. In IEEE Symposium on Security and Privacy (2013).

[27] TRAN, M., ETHERIDGE, M., BLETSCH, T., JIANG, X., FREEH,
V., AND NING, P. On the expressiveness of return-into-libc at-
tacks.
In Proceedings of the 14th international conference on
Recent Advances in Intrusion Detection (2011), Springer-Verlag.
[28] VAN DER VEEN, V., CAVALLARO, L., BOS, H., ET AL. Mem-
ory errors: the past, the present, and the future. In Research in
Attacks, Intrusions, and Defenses. Springer, 2012, pp. 86–106.

[29] WARTELL, R., MOHAN, V., HAMLEN, K. W., AND LIN, Z. Bi-
nary Stirring: Self-randomizing Instruction Addresses of Legacy
x86 Binary Code. In ACM Conference on Computer and Com-
munications Security (CCS) (2012).

[30] XU, H., AND CHAPIN, S. Address-space layout randomization
using code islands. In Journal of Computer Security (2009), IOS
Press, pp. 331–362.

A LDT Selector Bits

The actual value that a segment selector must hold is not
merely an index to the GDT/LDT, but is deﬁned by the
architecture set as follows:

Bits 15 - 3

Number of the entry

Bit 2

0=GDT, 1=LDT

Bit 1 - 0

Privilege Level

As user mode is in Ring 3, bits 0 and 1 must be set to
11bin. The use of the LDT forces us to set bit 2 to 1bin.
The index “0” of the LDT yields a valid value for the
segment selector of 111bin or 7 in decimal.

USENIX Association  

23rd USENIX Security Symposium  447

15

