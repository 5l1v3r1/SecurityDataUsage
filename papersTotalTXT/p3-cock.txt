Fast, Privacy Preserving Linear Regression over

Distributed Datasets based on Pre-Distributed Data

Martine De Cock

University of Washington

Tacoma

martine@uw.edu

Rafael Dowsley
Karlsruhe Institute of

Technology

rafael.dowsley@kit.edu

Anderson C. A.

Nascimento

University of Washington

Tacoma

andclay@uw.edu

Stacey C. Newman
University of Washington
newmsc8@uw.edu

Tacoma

ABSTRACT
This work proposes a protocol for performing linear regres-
sion over a dataset that is distributed over multiple parties.
The parties will jointly compute a linear regression model
without actually sharing their own private datasets. We
provide security deﬁnitions, a protocol, and security proofs.
Our solution is information-theoretically secure and is based
on the assumption that a Trusted Initializer pre-distributes
random, correlated data to the parties during a setup phase.
The actual computation happens later on, during an on-
line phase, and does not involve the trusted initializer. Our
online protocol is orders of magnitude faster than previous
solutions. In the case where a trusted initializer is not avail-
able, we propose a computationally secure two-party proto-
col based on additive homomorphic encryption that substi-
tutes the trusted initializer. In this case, the online phase
remains the same and the oﬄine phase is computationally
heavy. However, because the computations in the oﬄine
phase happen over random data, the overall problem is em-
barrassingly parallelizable, making it faster than existing so-
lutions for processors with an appropriate number of cores.

Keywords
Secure Machine Learning, Private Linear Regression, Un-
conditional Security, Commodity Based Model

Introduction

1
Linear regression is a technique for modelling the relation-
ship between one or more input variables – often called ex-
planatory variables – and a real valued outcome. It is widely
used as a tool for statistical analysis and is very popular as
a technique to build predictive models in machine learning

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
AISec’15, October 16, 2015, Denver, Colorado, USA.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3826-4/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2808769.2808774.

(see e.g. [30]). Linear regression models owe their popularity
to various factors, including the fact that they can be trained
eﬃciently, are intuitive, and ﬁt the data reasonably well for
many learning problems. In addition, the high inductive bias
caused by the simplicity of the model helps prevent it from
overﬁtting to a speciﬁc set of training examples. Further-
more, linear regression models do not require the outcome
variable to be linear in terms of the input variables; they are
simply linear in terms of the weights associated to each of
the input variables.

Techniques for constructing a linear regression model, like
other traditional machine learning (ML) methods, require
that all training attributes and tuples be accessible to the
learning algorithm. As storage and computing becomes in-
creasingly distributed and heterogeneous, data movement
and transformations become non-trivial, making conventional
ML algorithms diﬃcult to apply. Moreover, multiple parties
often cannot or will not share their data due to economic
incentives or privacy legislation, creating a dire need for
privacy-preserving ML techniques.

The importance of such techniques is immediately appar-
ent in ML applications in security-sensitive industries such
as the ﬁnancial sector and electronic surveillance. In the for-
mer, a bank may want to hire an analytics company to mine
the data of its customers but, being bound by the customer
agreement, cannot simply hand over the data to that com-
pany.
In the latter, Internet service providers wanting to
have a consulting ﬁrm do traﬃc analysis on their logs may
be unwilling to disclose details about their customer base
in the process. Another prominent example is the health-
care ecosystem. In addition, it is widely acknowledged that
big data analytics can revolutionize the healthcare industry,
among other things, by optimizing healthcare spending at
all levels from patients to hospitals to governments. Impor-
tant challenges for this reform are leveraging existing large
and varied clinical and claims datasets to estimate future
healthcare costs and taking measures in care-management
that reduce such costs while improving overall population
health. However, in practice a major roadblock is that ML
for many healthcare tasks (e.g., estimating the risk of hospi-
tal readmission) needs data that is split over many diﬀerent
owners – healthcare providers, hospitals, and medical insur-

3ance companies – who do not want to or legally cannot share
their data with outside entities.

In this paper, we attack the problem of securely com-
puting a linear regression model between two parties that
are not allowed to share their data. We propose protocols
that securely compute a linear regression model over two
separate datasets according to our deﬁnition (we later show
that our solution can be extended to the case of multiple
parties). Our results are information-theoretically secure
and work in the commodity based model [3, 5]. This model
can provide us with unconditionally secure protocols, that
is protocols that do not rely on computational assumptions
for ensuring their security. It has been proven that commit-
ments [28, 7, 24], oblivious transfer [3, 2], distributed inner
product [12], veriﬁable secret sharing [25, 13], and oblivious
polynomial evaluation [31] are implementable in this set-
ting. [20] presents recent general results on the power of the
commodity based model. In the commodity based model,
Alice and Bob have correlated data that was pre-distributed
at the beginning of the protocol. The pre-distributed data
can be seen as data provided by a trusted initializer during
an oﬄine setup phase. Note that, in this case, this trusted
party does not engage in the protocol after the setup phase
and never learns Alice and Bob’s inputs.

If a trusted initializer is not available or desirable, we
present a protocol where Alice and Bob can simulate the
trusted initializer by running a two-party secure computa-
tion protocol by themselves during an oﬄine setup phase.
The remaining online phase remains the same. In this case,
the overall protocol becomes computationally secure.

The online phase of our protocol is extremely eﬃcient,
having solely modular additions and multiplications. The
oﬄine, pre-processing phase, in the case of the computation-
ally secure protocol, is based on any additive homomorphic
public key cryptosystem. Because all the data used during
the pre-processing is random, the computations to be per-
formed become embarrassingly parallelizable and gains pro-
portional to the number of available cores can be obtained,
making even the costly pre-processing phase practical.

We improve the running time of previous algorithms for
secure linear regression from days [19] to seconds in the on-
line phase. Even when considering the time needed for Alice
and Bob to perform their pre-processing during the oﬄine
phase (in the case of the computationally secure protocol)
the overall computing time (oﬄine and online phase) is in
the order of minutes.

This paper is structured as follows: after discussing re-
lated work (Section 2) and giving preliminaries on the secu-
rity model in Section 3, we present a high level overview of
both our protocols for secure linear regression in Section 4.
Next, we provide details on our secure computation of mul-
tiplication and inverse of matrices (Section 5 and 7), as well
as how we deal with real numbers (Section 6). In Section
8, we summarize how these building blocks ﬁt together in
our information-theoretically secure protocol, while in Sec-
tion 9 we explain how to substitute the trusted initializer
and obtain a computationally secure protocol. In Section 10
we present runtime results of both protocols on ten diﬀerent
datasets, with a varying number of instances and features,
showing a substantial speed-up compared to existing work.
Finally, we sketch how to extend the protocols to more than
two parties (Section 11) and how to obtain security against
malicious adversaries (Section 12).

2 Related Work on Secure Linear Regression
There are many attempts in the literature at obtaining se-
cure linear regression protocols over distributed databases.
Most of them clearly do not even aim at obtaining the level
of privacy usually required by modern cryptographic proto-
cols (such as Karr et al. [21] and Du et al.[14], see also [29,
22]).

The pioneering work of Hall et al. [19] actually presents
a protocol that achieves cryptographic security within the
framework of secure two-party protocols and simulation based
deﬁnitions of security, as proposed by Goldreich in [17].
However, we remark that as some of the protocols they pro-
pose rely on approximations, rather than exact computa-
tions, the correct framework to be used is that of Feigen-
[15, 16], instead of Goldreich’s. Additionally,
baum et al.
Hall et al.
[19] uses operations in a ﬁnite ﬁeld and homo-
morphic encryption as a building block. However, the (in-
teresting version of the) linear regression problem deals with
numbers in R, or at least in Q. To cope with this problem,
a ﬁxed-point data type and its representation in the ﬁnite
ﬁeld are deﬁned in [19]. In such an approach, it is neces-
sary to perform a truncation after each multiplication, but
the description of the truncation protocol of [19] has a small
(correctable) problem as explained in Appendix A. Finally,
the overall computing time for solving the linear regression
problem for 51K input vectors, each with 22 features, is two
days [19]. The online phase of our protocol solves this prob-
lem in a few seconds. Even when considering the running
time of the oﬄine phase of our computationally secure pro-
tocol, by exploiting its embarrassingly parallelization prop-
erty, the overall running time is still in the order of minutes
for such a number of features and vectors.

In [26], a solution is proposed based on homomorphic en-
cryption and garbled circuits for a scenario where many
parties upload their data to a third party responsible for
obtaining the regression model (with the help of a Crypto
Service Provider, responsible for performing heavy crypto-
graphic operations). The Crypto Service Provider is a semi-
honest trusted party that is assumed to not collude with
other players and actively engages in the protocol during
its execution.
In our information theoretical solution the
trusted initializer does not engage in the protocol after the
setup phase and our online phase is still much faster than
the protocol presented in [26]. Even when we add up the
oﬄine phase and the online phase running times, in the case
of our computationally secure protocol, when multiple cores
are available for the oﬄine phase computations, the overall
running time is less for our protocol.

Finally, we assessed our secure linear regression by imple-
menting and analyzing the results using ten real datasets.
We chose a variety of diﬀerent datasets based on their num-
ber of features and instances. Some of our datasets have
millions of vectors. We are unaware of any other work on
secure linear regression where real datasets of this size have
been analysed before. For example, in [19] and in [26], the
real datasets used had thousands of vectors.
3 Security Model
3.1 Secure Two-Party Computation
We consider honest-but-curious adversaries (i.e., adversaries
that follow the protocol instructions but try to learn ad-
ditional information about the other parties’ inputs) and

4deﬁne (exact) secure two-party computation following Gol-
dreich [17].
A two-party computation is speciﬁed by an ideal function-
ality that is a (possibly randomized) mapping f : {0, 1}∗ ×
{0, 1}∗→{0, 1}∗×{0, 1}∗ from inputs (a, b) to outputs (c, d).
Let f1(a, b) denote the ﬁrst output of f and f2(a, b) the sec-
ond. A two-party protocol is a pair of polynomial-time in-
teractive algorithms π = (πAlice, πBob). Alice executes πAlice
with input a and randomness rAlice and Bob executes πBob
with input b and randomness rBob. The execution proceeds
in rounds, each party is able to send one message in each
round to the other party. The messages are speciﬁed by
π, given the party’s view, which consists of his input, ran-
domness, and messages exchanged so far. Each party can
also terminate, at any point, outputting some value based
on his view. Let viewπ
Alice(a, b) denote Alice’s view of the
protocol execution, i.e, her input, her randomness, and all
the exchanged messages. Let viewπ
Bob(a, b) similarly denote
Bob’s view of the protocol execution. Let outputπ
Alice(a, b)
and outputπ
Bob(a, b) denote Alice’s and Bob’s outputs respec-
tively.

Definition 3.1. A protocol π privately computes f with
statistical security if for all possible inputs (a, b) the follow-
ing properties hold:
• Correctness:
{outputπ

Bob(a, b)} ≡ {f (a, b)}

Alice(a, b), outputπ

{SAlice(a, c), f2(a, b)} s≈ {viewπ

• Privacy: There are simulators SAlice and SBob such that:
Bob(a, b)}
Bob(a, b)}

{f1(a, b),SBob(b, d)} s≈ {outputπ

Alice(a, b), viewπ
s≈ denotes statistical indistinguishability.

Alice(a, b), outputπ

where

The privacy requirement ensures that whatever an honest-
but-curious adversary learns from interactions within the
protocol can also be learned by an ideal adversary that only
learns the input and output of that party.

A very useful paradigm for building private protocols is
designing them in a modular way, using the following com-
position theorem [17]:

Theorem 3.2. Let f, g be two-party functionalities. Let
πf|g be a private protocol for computing f using oracle calls
to g and suppose that there is a private protocol πg computing
g. Let πf be the protocol obtained from πf|g by independently
using one instance of πg for implementing each oracle call
to g. Then πf privately computes f .
3.2 Secure Approximations
Note that the private computation of an approximation f
of a target functionality f can reveal more information than
the target functionality itself.
Imagine, for instance, the
case where the output of f is equal to the output of f in
all bits except the least signiﬁcant one, in which f encodes
one bit of the input of the other party. To ensure that the
approximation f does not leak additional information, we
use the framework of Feigenbaum et al. [15, 16] for private
approximations, using the notation of Kiltz et al. [23]. Only
deterministic target functionalities f are considered, but the
approximations f can be randomized.

Definition 3.3. The functionality f is an ε-approximation

of f if for all possible inputs (a, b), |f (a, b) − f (a, b)| < ε.

Definition 3.4. The functionality f is functionally pri-
vate with respect to f if there is a simulator S such that for
all possible inputs (a, b), {S(f (a, b))} s≈ {f (a, b)}.

Note that functional privacy is a property of the function-
ality f itself, and not of any protocol π implementing it. It
captures the fact that the approximation error is indepen-
dent from the inputs when conditioned on the output of the
exact functionality.

Definition 3.5. A protocol π is a private computation of
an ε-approximation with respect to f if π privately computes
a (possibly randomized) function f such that f is function-
ally private with respect to f and is an ε-approximation of
f .
3.3 Commodity Based Cryptography
In the commodity based cryptography model [3, 2], a trusted
initializer (TI) distributes values (i.e., the commodities) to
the parties before the start of the protocol execution. The
TI has no access to the parties’ secret inputs and does not
communicate with the parties except for delivering the pre-
distributed values during the setup. One main advantage of
this model is the high computational eﬃciency that arises
from the fact that the parties often only need to derandom-
ize the pre-computed instances to match their own inputs.
Another advantage is the computations are pre-distributed
by a trusted initializer, and therefore most protocols yield
perfect security. The trusted initializer functionality FD
T I is
parametrized by an algorithm D, which is executed upon ini-
tialization to generate the correlated randomness (PAlice, PBob)
that is distributed to Alice and Bob respectively.

4 Overview of Our Protocols
Assume that we have a set of training examples (real vectors)

(a1(xi), a2(xi), . . . , am(xi), yi)

where aj(xi) is the value of the input attribute aj for the
training example xi (i = 1, . . . , n) and yi is the associated
output. The goal is to leverage these training examples to
predict the unknown outcome for a previously unseen input
as accurately as possible. To this end, we want to learn a
linear function

y = β1a1(x) + β2a2(x) + . . . + βmam(x) + b

that best approximates the relation between the input vari-
ables a1(x), a2(x), . . . , am(x) and the response variable y.
Throughout this paper we assume that all variables are real
numbers and that we aim to ﬁnd real values for the param-
eters β1, β2, . . . , βm and b that minimize the empirical risk
function

((β1a1(xi) + β2a2(xi) + . . . + βmam(xi) + b)− yi)2 (1)

n(cid:88)

i=1

1
n

which is the mean squared error over the training instances.
For notational convenience, we switch to the homogenous
version of the linear function and we use vector notation,
i.e. let

5• xi = (a0(xi), a1(xi), a2(xi), . . . , am(xi)), with a0(xi) =

1 for all i ∈ {1, . . . , n}

• β = (β0, β1, . . . , βm), with β0 = b

Using (cid:104)β, xi(cid:105) to denote the dot product of β and xi, minimiz-
ing (1) amounts to calculating the gradient and comparing
it to zero, i.e. solving

n(cid:88)

i=1

2
n

((cid:104)β, xi(cid:105) − yi)xi = 0

The solution to (2) is1

with

X =

β = (X T X)

−1X T y

 x1

x2
. . .
xn

 and y =

 y1

y2
. . .
yn

(2)

(3)



The scenarios that we are interested in are those in which
the training data is not owned by a single party but is instead
distributed across multiple parties who are not willing to
disclose it. Our experiments in Section 10 correspond to
scenarios in which X is partitioned column-wise across two
parties, i.e. Alice and Bob have information about diﬀerent
features of the same instances. However, as will become
clear below, our protocols work in all scenarios in which
Alice has a share XAlice and Bob has a share XBob such that
XAlice + XBob = X, regardless of whether the dataset X is
sliced column-wise, row-wise, or a mixture of the two. In
our experiments we also assume that Bob has the vector Y .
However, our protocol can also handle the case when Y is
distributed over two or more players.

Here we give an overview of our solution. The basic idea
is to reduce the problem of securely computing linear re-
gression to the problem of securely computing products of
matrices. The protocol for computing products of matrices
works only for elements of the matrices belonging to a ﬁnite
ﬁeld. Thus, Alice and Bob should be able to map their real
valued ﬁxed precision inputs to elements of a ﬁnite ﬁeld (as
described in Section 6). Our protocol works in a shared in-
put model in which each party holds some elements of the
design matrix. Each party creates its share of the design
matrix by mapping their respective real valued inputs to el-
ements of a ﬁnite ﬁeld and putting them on the respective
position of the matrix and then ﬁlling the remaining posi-
tions of the matrix’s share with zeros. I.e., the shares XAlice
and XBob are such that XAlice + XBob = X where X is the
design matrix mapped into the ﬁnite ﬁeld.

1. Oﬄine phase:

in the information-theoretically secure
protocol, Alice and Bob receive correlated data from
the Trusted Initializer. In the case of the computation-
ally secure protocol, they run the protocol described in
Section 9.

2. Online Phase:

(a) The players map their ﬁxed precision real valued
inputs to elements of a ﬁnite ﬁeld as described in
Section 6 and create the shares of X as described
above.

1Assuming that X T X is invertible

(b) The players compute over their shares using the
protocols for matrix multiplication (described in
Section 5) and for computing the inverse of a Co-
variance Matrix (described in Section 7) in order
to obtain shares of the estimated regression coef-
ﬁcient vector.

(c) The players exchange their shares of the estimated

regression coeﬃcient vector and reconstruct it.

After presenting the building blocks in Sections 5, 6 and
7, we reiterate the information-theoretically secure and the
computationally secure protocol for linear regression at a
more concrete level of detail in Sections 8 and 9 respectively.
In presenting our protocols, we ﬁrst introduce an ideal
functionality that captures the behaviour of a secure in-
stance of the protocol in question. Ideal functionalities are
always represented by calligraphic letters (e.g. FDMM in the
case of distributed the matrix multiplication functionality).
We then present the protocol and prove that the protocol
is as secure as the ideal functionality. Protocols are repre-
sented by capital greek letters (e.g. ΠDMM in the case of the
distributed matrix multiplication protocol).

5 Secure Distributed Matrix Multiplication Pro-

tocol

q

q

q

q

q

q

and Y ∈ Zn2×n3

Let’s ﬁrst take a look at a straightforward extension of Beaver’s
protocol for secure multiplication in the commodity based
model [4] for matrices. Alice and Bob hold shares of matrices
X ∈ Zn1×n2
to be multiplied and the goal is
to obtain shares of the multiplication result X · Y ∈ Zn1×n3
in such a way that the result remains hidden from both of
them individually. Let XAlice ∈ Zn1×n2
and YAlice ∈ Zn2×n3
be Alice’s shares of the inputs and XBob ∈ Zn1×n2
and
YBob ∈ Zn2×n3
be Bob’s shares. Note that XY = (XAlice +
XBob)(YAlice + YBob) = XAliceYAlice + XAliceYBob + XBobYAlice +
XBobYBob. The terms XAliceYAlice and XBobYBob can be com-
puted locally, but the computation of the terms XAliceYBob
and XBobYAlice is more complex. Beaver’s protocol solves
the problem of computing the last two terms by having
the trusted initializer distribute random values AAlice, ABob,
BAlice, BBob to the parties and also random shares of the value
AAliceBBob + ABobBAlice. Then the parties only need to de-
randomize this pre-distributed instance to the actual input
values. The protocol ΠDMM is parametrized by the size q of
the ﬁeld and by the dimensions n1, n2, n3 of the matrices to
be multiplied and works as follows:

q

1. At the setup, the trusted initializer chooses uniformly
, BAlice, BBob ∈ Zn2×n3
random AAlice, ABob ∈ Zn1×n2
and T ∈ Zn1×n3
, and distributes the values AAlice, BAlice,
T to Alice and the values ABob, BBob, C = (AAliceBBob +
ABobBAlice − T ) to Bob.

q

q

q

2. Bob sends (XBob − ABob) and (YBob − BBob) to Alice.
3. Alice chooses a random T (cid:48) ∈ Zn1×n3

, computes W =
AAlice(YBob−BBob)+(XBob−ABob)BAlice +XAliceYAlice−T (cid:48)
and sends W , (XAlice−AAlice) and (YAlice−BAlice) to Bob.
Alice outputs T + T (cid:48).

q

4. Bob outputs U = (XAlice − AAlice)YBob + XBob(YAlice −

BAlice) + XBobYBob + W + C.

6This protocol securely implements the ideal distributed
matrix multiplication functionality FDMM that works as fol-
lows: FDMM is parametrized by the size q of the ﬁeld and the
dimensions n1, n2, n3 of the matrices to be multiplied. Given
Alice’s input shares XAlice ∈ Zn1×n2
, and
Bob’s input shares XBob ∈ Zn1×n2
, it
computes V = (XAlice + XBob)(YAlice + YBob), chooses a ran-
dom matrix R ∈ Zn1×n3
and sends R to Alice and V − R to
Bob.

and YAlice ∈ Zn2×n3
and YBob ∈ Zn2×n3

q

q

q

q

q

Theorem 5.1. The distributed matrix multiplication pro-
tocol ΠDMM is correct and securely implements the distributed
matrix multiplication functionality FDMM against honest but
curious adversaries in the commodity based model.

The correctness of the protocol can be trivially veriﬁed
by inspecting the value of T + T (cid:48) + U . The security of this
protocol lies in the fact that all values exchanged between
the parties are blinded by a value which is completely ran-
dom in the underlying ﬁeld from the point of view of the
message receiver. This protocol can in fact be proved secure
even against malicious parties and in the stronger Univer-
sal Composability (UC) framework [8]. The idea is that the
simulator simulates an instance of the adversary and an in-
stance of the protocol execution with the adversary, allowing
the adversary to communicate with the environment. The
leverage used by the simulator is the fact that, in the ideal
execution, he is the one simulating the trusted initializer. By
simulating the TI, he is able, at the same time, to generate
a protocol transcript for the adversary that is indistinguish-
able from the real protocol execution and also to extract the
input of the corrupted parties in order to forward them to
the ideal functionality.

6 Dealing with Real Numbers
The security proof of the (matrix) multiplication protocol
presented in the last section essentially relies on the fact
that the blinding factors are uniformly random in Zq.
If
one tries to design similar protocols working directly with
integers or real numbers there would be a problem, since it
is not possible to sample uniformly in Z or R. Similarly,
in protocols that use homomorphic encryption as building
blocks, the encryption is normally done for messages which
are members of a ﬁnite group. But in secure protocols for
functionalities such as linear regression, one needs to deal
with inputs which are real numbers. Thus it is necessary
to develop a way to approximate the computations on real
numbers by using building blocks which work on ﬁelds Zq.
We adapt the method of Catrina and Saxena [9] with a
ﬁxed-point representation. Let k, e and f be integers such
that k > 0, f ≥ 0 and e = k − f ≥ 0. Let Z(cid:104)k(cid:105) denote the
set {x ∈ Z : −2k−1 + 1 ≤ x ≤ 2k−1 − 1}. The ﬁxed-point
data type with k bits, resolution 2−f , and range 2e is the set
Q(cid:104)k,f(cid:105) = {˜x ∈ Q : ˜x = ˆx2−f , ˆx ∈ Z(cid:104)k(cid:105)}. The signed integers
in Z(cid:104)k(cid:105) are then encoded in the ﬁeld Zq (with q > 2k) using
the function

g: Z(cid:104)k(cid:105)→Zq, g(ˆx) = ˆx mod q.

In secure computation protocols using secret sharing tech-
niques, the values in Zq are actually shared between the
two parties. Using this encoding, we have that ˆx + ˆy =
g−1(g(ˆx) + g(ˆy)), where the second addition is in Zq, i.e.,
we can compute the addition for signed integers in Z(cid:104)k(cid:105) by

using the arithmetic in Zq. The same holds for subtraction
and multiplication.
For the ﬁxed-point data type we can do additions using the
fact that ˜w = ˜x + ˜y = (ˆx + ˆy)2−f , so we can trivially obtain
the representation of ˜w with resolution 2−f by computing
ˆw = ˆx+ ˆy, i.e., we can do the addition of the ﬁxed-point type
by using the addition in Z(cid:104)k(cid:105), which itself can be done by
performing the addition in Zq. The same holds for subtrac-
tion. But for multiplication we have that ˜w = ˜x˜y = ˆxˆy2−2f ,
and therefore if we perform the multiplication in Zq, we will
obtain (if no overﬂow occurred) the representation of ˜w with
resolution 2−2f . Such representation uses Z(cid:104)k+f(cid:105) instead of
the original Z(cid:104)k(cid:105). In order to have the size of the signed inte-
gers representation be independent from the amount of mul-
tiplication operations performed with the ﬁxed-point data,
we need to scale the resolution of ˜w down to 2−f . For that
purpose we use a slightly modiﬁed version of the truncation
protocol of Catrina and Saxena [9].

The central idea of this truncation protocol is to reveal the
number w to be truncated to one of the parties, but blinded
by a factor r which is from a domain exponentially bigger
than the domain of the value w and thus statistically hides
w. The value r is generated so that the parties have shares
of both r and r(cid:48) (which represents the f least signiﬁcant
bits of r). Then Bob can reveal w + r to Alice and they
can compute shares of the truncated value by using local
computations. In more detail, let λ be a security parameter
and let the ﬁeld Zq in which the signed integers are encoded
be such that q > 2k+f +λ+1. Note that the multiplicative
inverse of 2f in Zq is ((q + 1)/2)f and let F −1 denote it.
The parties have, as inputs, shares wAlice, wBob ∈ Zq such
that w = (wAlice + wBob) ∈ {0, 1, . . . , 2k+f−1 − 1} ∪ {q −
2k+f−1 + 1, . . . , q − 1}. The protocol ΠTrunc for truncating
the output works as follows:

1. At the setup, the trusted initializer chooses uniformly
random r(cid:48) ∈ {0, 1}f and r(cid:48)(cid:48) ∈ {0, 1}λ+k and com-
putes r = r(cid:48)(cid:48)2f + r(cid:48). He also chooses uniformly ran-
dom r(cid:48)
and rAlice = r − rBob. He sends r(cid:48)
r(cid:48)
Bob, rBob to Bob.

Bob, rBob ∈ Zq and then sets r(cid:48)

Bob
Alice, rAlice to Alice and

Alice = r(cid:48) − r(cid:48)

2. Bob sends zBob = (wBob + rBob) to Alice and outputs

(wBob + r(cid:48)

Bob)F −1.

c mod 2f . Then she outputs (wAlice + r(cid:48)

3. Alice computes c = zBob+wAlice+rAlice+2k+f−1 and c(cid:48) =
Alice − c(cid:48))F −1.
This protocol securely implements the functionality FTrunc
that captures the approximate truncation without leakage.
FTrunc is parametrized by the size q of the ﬁeld and reduces
the resolution by 2−f . Given Alice and Bob’s shares of the
input, wAlice, wBob ∈ Zq, and a blinding factor r(cid:48)
Bob ∈ Zq
from Bob, it computes ˆw = g−1(wAlice + wBob mod q) and
samples u such that Pr [ u = 1 ] = ( ˆw mod 2f )/2f . Then it
computes v = (wAlice−(wAlice +wBob mod 2f )−r(cid:48)
Bob)F −1 +u
Bob)F −1 and
and sends it to Alice (Bob’s output is (wBob + r(cid:48)
can be locally computed).

Theorem 6.1. The truncation protocol ΠTrunc privately

computes the approximate truncation functionality FTrunc.

Proof. Correctness: Let ˆw = g−1(wAlice+wBob mod q).
We have that the value ˆw ∈ {−2k+f−1 + 1,−2k+f−1 +
2, . . . , 2k+f−1 − 1}. Let b = ˆw + 2k+f−1 and let b(cid:48) = b

7mod 2f . We have that b ∈ {1, . . . , 2k+f − 1} and since k > 0
also that

(cid:48)

b

= b mod 2f = ˆw + 2k+f−1 mod 2f = ˆw mod 2f .

Since r < 2k+f +λ and q > 2k+f +λ+1 we have that c = b + r
and thus

where u ∈ {0, 1} and Pr [ u = 1 ] = Pr(cid:2) r(cid:48) ≥ 2f − b(cid:48)(cid:3) = ( ˆw

) mod 2f = b

(cid:48) − u2f

= (b

+ r

+ r

c

(cid:48)

(cid:48)

(cid:48)

(cid:48)

mod 2f )/2f with the probability over the choices of random
r(cid:48). Hence

(cid:48) − r

c

Alice − r
(cid:48)
(cid:48)
Alice + r

Bob = g( ˆw mod 2f − u2f ),
(cid:48)
Bob − c
(cid:48)

= g( ˆw − ( ˆw mod 2f ) + u2f )

(cid:48)

wAlice + wBob + r

(cid:18)(cid:22) ˆw

(cid:23)

(cid:19)

2f + u2f

,

= g

2f

(cid:19)
(cid:23)
,
2f
Alice−c(cid:48))F −1
and so the shares output by the parties (wAlice+r(cid:48)
and (wBob + r(cid:48)

Bob)F −1 are correct.

(cid:18)(cid:22) ˆw

(wAlice + wBob + r

(cid:48)
Alice + r

Bob − c
(cid:48)

−1 = g

+ u

)F

(cid:48)

Security: The only message exchanged is zBob = (wBob +
rBob) that reveals c = b + r to Alice, but since r is a uni-
formly random (k + f + λ)-bits integer and b is a (k + f )-bits
integer, we have that the statistical distance between c and
r is at most 2−λ, i.e., c is statistically indistinguishable from
a uniformly random value.

Theorem 6.2. FTrunc is an 1-approximation2 and is func-
tionally private with respect to an exact truncation function-
ality that computes the truncation using the ﬂoor function.

Proof. The only diﬀerence between the two functionali-
ties is that in the approximate truncation an error factor u is
present in the shared output. But note that u ∈ {0, 1} and
Pr [ u = 1 ] = ( ˆw mod 2f )/2f , but u is independent from
the speciﬁc shares wAlice, wBob used to encode g( ˆw). Thus
the protocol rounds ˆw/2f to the nearest integer with prob-
ability 1− α, where α is the distance between ˆw/2f and the
nearest integer.

We should mention that in the case of matrix multipli-
cation the truncations only have to be performed in the el-
ements of the resulting matrix instead of once per element
multiplication, which would be less eﬃcient and also increase
the error due to truncation.

7 Computing the Inverse of a Covariance Ma-

trix

In order to be able to compute the linear regression model
from a design matrix and the response vector we need to
compute the inverse of the covariance matrix. Let A be
the covariance matrix. In order to compute A−1 we use a
generalization for matrices of the Newton-Raphson division
method.

The algorithms for division of ﬁxed-point numbers are di-
vided in two main classes: digit recurrence (subtractive divi-
sion) and functional iteration (multiplicative division). The
2in the representation, 2−f in the ﬁxed-point data type

Newton-Raphson division method is from the functional it-
eration class, which is more amenable to secure implemen-
tation and converges faster. Additionally this method is self
correcting, i.e., truncation errors in one iteration decrease
quadratically in the next iterations. The inverse of a num-
ber a is computed by deﬁning the function h(x) = x−1 − a
and then applying the Newton-Raphson method for ﬁnding
successively better approximations to the roots of h(x). The
iterations follow the form:

xs+1 = xs(2 − axs).

This algorithm can be generalized for computing the inverse
of the matrix A. A numerical stable iteration for computing
A−1 is [19, 18]:

c = trace(A)

X0 = c

−1I

Xs+1 = Xs(2 − AXs)

where I is the identity matrix with the same dimensions as
A. Note that A is a covariance matrix and thus it is posi-
tive semi-deﬁnite and the trace of A dominates the largest
eigenvalue of A. It is convenient to use c = trace(A) because
the trace of A can be computed locally by parties that have
shares of A. To compute c−1 the Newton-Raphson is also
used with x0 set to an arbitrarily small value, as the conver-
gence happens if the magnitude of the initial value is smaller
than that of c−1.

Note that, in our case, we use this method to securely com-
pute the inverse of the covariance matrix, i.e, each party has
a share of the covariance matrix as input and should receive,
as output, random shares of its inverse, but no additional
information should be learned by the parties. Hence we can-
not perform a test after each iteration in order to check if
the values already converged with resolution 2−f (and thus
stop the iterations at the optimal point) because this would
leak information about the input based on how many iter-
ations were performed. We have to use an upper bound (cid:96)
on the number of iterations such that all possible covariance
matrices converge with resolution 2−f in (cid:96) iterations. A very
conservative upper bound is (cid:96) = 2k [19]; further studies will
be done to try to determine a tighter upper bound.

q

The protocol to securely compute the inverse of a shared
covariance matrix is parametrized by the size q of the ﬁeld.
Let A ∈ Zn×n
be the encoding in Zq of a covariance matrix
where the elements are ﬁxed-point numbers. Alice has input
and Bob has input ABob ∈ Zn×n
AAlice ∈ Zn×n
such that
AAlice + ABob = A. Then the protocol proceeds as follows:

1. Alice and Bob locally compute shares of c = trace(A),
i=1 ABob[i, i]

i=1 AAlice[i, i] and cBob =(cid:80)n

i.e., cAlice =(cid:80)n

q

q

−1
Alice and c

2. Alice and Bob use the Newton-Raphson division method
−1
Bob of c−1 with resolu-
to compute shares c
tion 2−f . The subtractions can be performed locally
and the multiplications using the distributed (matrix)
multiplication functionality FDMM followed by the ap-
proximate truncation functionality FTrunc.

3. Alice and Bob use the generalized Newton-Raphson
−1
Bob of A−1 with
method to obtain shares A
resolution 2−f for the elements. The subtractions can
be performed locally and the multiplications using the
distributed matrix functionality FDMM followed by the
approximate truncation functionality FTrunc.

−1
Alice and A

8We emphasize that the truncation used has some intrin-
sic error, but the Newton-Raphson method’s self-correcting
properties compensate for that.

8 Computing the Linear Regression Coefﬁcients
We consider the setting in which there is a design matrix ˜X
and a response vector ˜y. We are interested in analyzing the
statistical regression model

˜y = ˜X ˜β + ,

and therefore want to compute the estimated regression co-
eﬃcient vector

β = ( ˜X T ˜X)

−1 ˜X T ˜y

The design matrix is a n × m matrix where the elements
are of the ﬁxed-point data type with precision 2−f and range
2k−f (i.e., ˜X ∈ Qn×m(cid:104)k,f(cid:105)) and the response vector ˜y ∈ Qn(cid:104)k,f(cid:105).
Let ˆX ∈ Zn×m(cid:104)k(cid:105)
be the element-wise representation of ˜X
as signed integers and let X ∈ Zn×m
be the element-wise
encoding of ˆX as elements of the ﬁeld Zq. Deﬁne ˆy and y in
the same way from ˜y.

q

It is assumed that the parties Alice and Bob hold shares
of X and y. Alice and Bob can then use the protocols for
matrix multiplication, truncation and covariance matrix in-
version that were described in the previous sections in order
to compute shares of

β = (X T X)

−1X T y

Then they only need to reveal their ﬁnal shares and con-
vert the result back to the ﬁxed-point data type in order to
get β.

In more details:

1. Online Phase:

(a) The players map their ﬁxed precision real valued
inputs to elements of a ﬁnite ﬁeld as described in
Section 6 and create the shares of X as described
above.

(b) The players compute X T X by using the matrix
multiplication protocol ΠDMM (described in Sec-
tion 5). Once the multiplication is ﬁnished they
ran the truncation protocol ΠTrunc for each ele-
ment of the matrix X T X.

(c) Alice and Bob compute the inverse of (X T X) by
running the protocol for computing the inverse
of a covariance matrix (described in Section 7).
Within the covariance matrix inversion protocol
there are several calls to the matrix multiplication
and truncation protocols.

(d) Alice and Bob run the matrix multiplication and

the truncation protocols twice to obtain (X T X)−1X T
and ﬁnally (X T X)−1X T y.

(e) The players exchange their shares of the estimated

regression coeﬃcient vector and reconstruct it.

(f) The coeﬃcients β obtained by the players are
mapped back from ﬁnite ﬁeld elements to real val-
ues with ﬁnite precision.

The security of the composed protocol follows from the com-
position theorem 3.2 using the facts that ΠDMM securely im-
plements the distributed matrix multiplication functionality
FDMM and ΠTrunc privately computes the approximate trun-
cation functionality FTrunc. It is assumed that a big enough
k is used so that no overﬂow occurs and hence the correct-
ness of the protocol follows. The ﬁnal protocol implements
the linear regression functionality FReg that upon getting the
shares XAlice and XBob of the design matrix X and yAlice and
yBob of the response vector y, compute β = (X T X)−1X T y
and output β to Alice and Bob.

9 Substituting the Trusted Initializer
If a trusted initializer is not desired, it is possible to obtain a
solution where the parties, during the setup phase, compute
the correlated data themselves. The idea is to use the homo-
morphic properties of the Paillier’s encryption scheme [27].
For two large prime numbers p and q, the secret key of Pail-
lier’s cryptosystem is sk = (p, q). The corresponding pub-
lic key is pk = N = pq and the encryption of a message
x ∈ ZN is done by picking a random r ∈ Z∗
N 2 and com-
puting Enc(pk, x) = (N + 1)xrN mod N 2. The following
homomorphic properties of Paillier’s encryption scheme are
used:

Enc(pk, x) · Enc(pk, y) = Enc(pk, x + y mod N )

Enc(pk, x)y = Enc(pk, xy mod N ).

Given two vectors x = (x1, . . . , xn) and y = (y1, . . . , yn)
where the second is given in clear and the ﬁrst is encrypted
element-wise (i.e., Enc(pk, xi) are revealed), one can com-
pute a ciphertext corresponding to the inner product:

n(cid:89)

Enc(pk,(cid:104)x, y(cid:105) mod N ) =

Enc(pk, xi)yi .

i=1

The idea for computing the necessary correlated data for
the distributed matrix multiplication protocol is to use the
above fact in order to compute the non-local multiplication
terms. Bob has a pair of public/secret keys for Paillier’s
encryption scheme and sends to Alice the element-wise en-
cryption under his own public key of the elements of the
column/row that needs to get multiplied. Alice, having the
plaintext corresponding to her own values on the appropri-
ate column/row, can compute an encryption of the inner
product under Bob’s public key. She then adds a random
blinding factor and sends the ciphertext to Bob, who can de-
crypt it, thus yielding distributed shares of the inner product
between Alice and Bob.

The protocol is parametrized by the dimensions n1, n2, n3
of the matrices to be multiplied. Bob holds a Paillier’s secret
key sk, whose corresponding public-key is pk. For a matrix
X, x[i, j] denote the element in the i-th row and j-th column.
The protocol works as follows:

1. Alice chooses uniformly random AAlice ∈ Zn1×n2

, BAlice ∈

N

Zn2×n3

N

and T ∈ Zn1×n3

.

N

2. Bob chooses uniformly random ABob ∈ Zn1×n2

BBob ∈ Zn2×n3
own public key and send the ciphertexts to Alice.

and
, element-wise encrypts them under his

N

N

93. For i = 1, . . . , n1, j = 1, . . . , n3, Alice computes the

ciphertext

˜c[i, j] = Enc (pk, t[i, j]) · n2(cid:89)

(cid:16)
· Enc (pk, aBob[i, k])bAlice[k,j](cid:17)

Enc (pk, bBob[k, j])aAlice[i,k] ·

k=1

and sends them to Bob. Alice outputs AAlice, BAlice and
T .

4. Bob decrypts the ciphertexts in order to get the matrix
C = (AAliceBBob + ABobBAlice + T ). Bob outputs ABob,
BBob and C.

IND-CPA security of Paillier’s encryption scheme [27].

The security of this protocol follows trivially from the
Note that the values r and r(cid:48) that are distributed by the
trusted initializer for performing the truncation protocol can
be trivially computed by the parties themselves using dis-
tributed multiplications.

10 Experiments
We assessed our secure linear regression algorithm by imple-
menting it and analyzing the results using ten real datasets.
We chose a variety of diﬀerent datasets based on the number
of features and the number of instances (see Section 10.1).
We used C++ as our programming language which we aug-
mented with the BOOST libraries for functionality such as
lexical cast for type casting and asio for work with sockets.
We also made use of the GMP and NTL libraries within
C++ to implement our protocols. We built our system on
top of a Microsoft Azure G4 series machine with Intel Xeon
processor E5 v3 family, 224GB RAM size, 3072GB of disk
size and 16 cores. Finally, we chose Ubuntu 12.04 as our op-
erating system. We have merged the matrix multiplication
and truncation protocols within one protocol for implemen-
tation purposes.

The online phase (Section 10.2) is very fast and capable of
handling millions of records within less than an hour, which
is a huge improvement to the previous results. We only use
addition and multiplication of matrices on our online phase
which makes it simple and easy to manage.

In the case when a trusted initializer is not desired one
can use our computationally secure protocol, at the cost of
having a costier oﬄine phase (Section 10.3). However, be-
cause Alice and Bob only work over random inputs during
the oﬄine phase, the encryption, decryption and mathemat-
ical operations are all embarassingly parallelizable.
10.1 Datasets
All our datasets are contained within the UCI repository3,
with the exception of the State Inpatient Database (WA)
which is provided by HCUP4. The UCI repository includes
48 regression task datasets from which we chose 9. Our
datasets range in size from 395 instances to over 4 million
and from 7 attributes to 367, and are summarized in Table
1.

Gas Sensor Array Under Dynamic Gas Mixtures

This dataset represents data from 16 chemical sensors
exposed to ethylene and CO mixtures at various con-
centration levels in the air. We added together the con-
centration of ethylene and the concentration of CO to
create one continuous response variable of gas concen-
tration and removed the time feature from the dataset.
We then designated the ﬁrst 8 sensor readings to Alice
and the second 8 to Bob. This left us with a total of
4,208,261 sets of 16 sensor readings to diﬀerent total
concentrations of ethylene and CO.

Communities and Crime We used 122 attributes describ-
ing 1,993 communities and their law enforcement de-
partments in 1990 to create this dataset. The goal with
this dataset is to predict the number of violent crimes
per capita for each community. All missing values
present in the dataset (of which there were 36,850 dis-
tributed throughout 1,675 diﬀerent communities and
22 diﬀerent attributes) were replaced with 0s. These
missing values were largely relevant to the communi-
ties’ police departments. We also removed 5 variables
that were present in the original data but described
by the UCI documentation as non-predictive, namely
state, county, community, community name, and fold.
The ﬁnal 122 attributes were then divided in half be-
tween Alice and Bob.

Auto MPG This dataset contains attributes describing 398
automobiles in attempt to predict MPG (miles per gal-
lon) for each. We removed the car name attribute
which was present in the original data and were left
with 7 predictive features. We then replaced the 6
missing horsepower values with 0s. In the end we des-
ignated the cylinders, displacement, and horsepower
features to Alice and the weight, acceleration, model
year, and origin features to Bob.

BlogFeedback In an attempt to predict the number of
comments a blog post will receive in the upcoming 24
hours, this dataset originally had 280 attributes. Since
our complete dataset must be linearly independent, to
enable the inversion of X T X required in our proto-
col, we removed 57 of these original attributes leaving
us with 223 predictors describing 52,397 blog posts.
An example of such a feature would be the binary in-
dicator of whether or not a blog post was published
on a Sunday. There are binary indicators of whether
publication occurred on any of the other days of the
week and therefore this feature, publication on a Sun-
day, is linearly dependent on the other six. Finally,
the dataset was divided column wise, designating 111
attributes to Alice and the other 112 to Bob.

Wine Quality This dataset takes 11 attributes related to
the white variant of Portuguese “Vinho Verde” wine
which are used to predict the quality score of 4,897
wines. We designated the ﬁxed acidity, volatile acidity,
citric acid, residual sugar, chlorides, and free sulfur
dioxide features to Alice and the total sulfur dioxide,
density, pH, sulphates, and alcohol features to Bob.

3UC Irvine Machine Learning Repository
https://archive.ics.uci.edu/ml/datasets.html
4http://www.ahrq.gov/research/data/hcup/

Bike Sharing In this dataset we took attributes describing
a certain hour and day and attempted to predict the
number of users for a bike sharing system. We removed

10the record index which was present in the original data
as well as the count of casual users and the count of
registered users and targeted the total rental bikes used
(casual + registered) for our prediction. We were left
with 13 predictors of 17,379 hour/day combinations
which were used to model bike use. Alice received in-
formation on the dates, seasons, years, months, hours,
and holidays while Bob was given information on week-
days, working days, weather situations, temperatures,
feel temperatures, humidities, and windspeeds.

Student Performance We used 30 attributes describing
395 students across two schools to create this dataset.
The goal with this dataset is to predict the ﬁnal grade
of each student in their math class. We removed two
columns from the original dataset – one detailing stu-
dents’ performances in the ﬁrst period and one de-
tailing their performances in the second period. We
identiﬁed the student’s ﬁnal grade as our sole response
variable. The ﬁnal 30 attributes were then divided
evenly between Alice and Bob.

YearPredictionMSD In this dataset we have attributes
describing audio features of 515,344 songs and we aim
to predict the release year of each song. We kept all 90
features that were present in the original data provided
by the UCI repository. In allocating the data we gave
Alice the ﬁrst 45 features and the second 45 to Bob.

State Inpatient Database (WA) From the HCUP State
Inpatient Database (WA) we extracted attributes de-
scribing 25,180 beneﬁciaries who had at least one hos-
pital admission within the state of Washington during
the ﬁrst nine months of the year between the years
2009 and 2012. The goal with this data is to predict
the cost each beneﬁciary will incur in the ﬁnal three
months of the same year. We extracted demographic,
medical, and previous cost information from the orig-
inal data and replaced any missing values with a 0
value. We then designated the age, gender, race, num-
ber of chronic conditions, length of stay, and number
of admits attributes to Alice. Bob was given a Boolean
matrix of comorbidities as well as previous cost infor-
mation.

Relative Location of CT Slices on Axial Axis In an at-

tempt to predict the relative location of a CT slice on
the axial axis of the human body, the original dataset
had 384 attributes describing CT images. Since our
complete dataset must be linearly independent, we re-
moved 17 of these original attributes leaving us with
367 predictors describing 53,500 CT images. We then
divided this dataset column wise, designating 183 at-
tributes to Alice and the other 184 to Bob.

10.2 Online Phase
We present in Table 1 the running times for the online phase
of our protocol building a predictive linear regression model.
Our online phase is very fast, computing a linear regression
model for a matrix of over 4 million rows and 16 columns
in under one hour. The regression coeﬃcients computed
with our secure protocol agree to the 5th decimal digit with
regression coeﬃcients computed without any security.
We brieﬂy work out the theoretical computational com-
plexity of computing β = (X T X)−1X T y with our online

protocol. If our dataset (which is denoted by X in this for-
mula), has n features and m records, then the total runtime
for computing the β values is O(mn2) which means that the
number of records in the dataset has only a linear eﬀect on
the run time of our implementation.

We used NTL for matrix multiplication with modular
arithmetic. We also used GMP (the GNU Multi-Precision li-
brary) in conjunction with NTL to increase our performance.
In the NTL library, the basic algorithm is used (with time
complexity O(n3) when all matrix dimensions are n).

Note that (X T X) is a square matrix with both dimen-
sions equal to n, and for datasets in which the number of
features is small relative to the number of records, com-
puting (X T X)−1 is very fast and negligible in respect to,
for example, computing X T X. Our online phase is faster
and independent from the semi-honest trusted party unlike
similar implementations, such as Nikolaenko et al.’s imple-
mentation [26].
10.3 Computationally Secure Ofﬂine Phase
In the pre-processing of the computationally secure oﬄine
phase of the matrix multiplication protocol ΠDMM, we use
Paillier for encryption and decryption, but any additive ho-
momorphic encryption scheme can be used. The down side
of these schemes is that their encryption and decryption
times are computationally intensive and, if the given dataset
is large, the pre-processing phase can take a long time. This
issue can be tackled by noticing that Alice and Bob, during
this phase, only work over random inputs and thus one can
use heavy parallelization to speed-up the running time.

For a dataset with m records and n features, in order
to get coeﬃcients securely and correctly, we use i = 50 it-
erations in the computation of inversion. Overall, we need
5mn+(3i+3)n2+n+3i encryptions and mn+(i+1)n2+n+i
decryptions. We also have two matrix multiplications and 3
matrix additions between encryption and decryption. Each
encryption in an Azure VM takes about 0.005 seconds for
each core. It is then easy to see that the encryption phase is
the bottle-neck of the pre-processing phase and easily par-
allelizable. Since we have 5mn number of encryptions, by
multiplying this number to the runtime of a single encryp-
tion time divided by number of cores, a good estimate of the
pre-processing phase is achievable.

The estimated running time for the oﬄine phase is given
in Table 2. These estimated results are a huge improvement
when compared to the previous result [19] which took two
days given a dataset with only about 50,000 records and are
comparable to the total running time presented in [26] in
the case of 256 cores.

11 Extending the Protocol to Multiple Parties
It is easy to generalize the previous protocol to the case in
which the design matrix ˜X and the response vector ˜y are
shared among more than two parties.

Let A, B and C be random matrices such that C = AB
and let the party Pi have shares Ai, Bi, Ci of these matrices.
Such triples of shares will be called matrix multiplication
triples. Given a pre-distributed matrix multiplication triple
and two shared matrices X and Y , it is possible to compute
shares of Z = XY without leaking any information about
these values. The idea is for the parties to locally compute
shares of D = X − A and E = Y − B and then open the

11Table 1: Actual Time Required (in seconds) for Online Phase of Secure Protocol to Build a Predictive Linear
Regression Model

Dataset
Name

Student Performance

Auto MPG

Communities and Crime

Wine Quality
Bike Sharing

State Inpatient Database (WA)

BlogFeedback

Relative Location of CT Slices on Axial Axis

YearPredictionMSD

Gas Sensor Array Under Dynamic Gas Mixtures

Number Number of
of Rows
Columns

Train Time: Data
Shared in the Clear Proposed Secure Protocol

Train Time: Using

395
398
1,993
4,897
17,379
25,180
52,397
53,500
515,344
4,208,261

30
7

122
11
13
36
223
367
90
16

0.3 sec
0.09 sec

9 sec
0.9 sec
3.7 sec
21 sec

1,800 sec
6,000 sec
3,800 sec
1,100 sec

11.7 sec
1.2 sec
147 sec
5.2 sec
16.5 sec
93 sec

9,000 sec
30,000 sec
18,000 sec
4,500 sec

Table 2: Estimated Time Required (in seconds) for Oﬄine Phase of Secure Protocol to Build a Predictive
Linear Regression Model
Dataset
Name

Number Number of
of Rows

Columns With 16 Cores With 64 Cores With 256 Cores

Oﬄine Time

Oﬄine Time

Oﬄine Time

Student Performance

Auto MPG

Communities and Crime

Wine Quality
Bike Sharing

State Inpatient Database (WA)

BlogFeedback

Relative Location of CT Slices on Axial Axis

YearPredictionMSD

Gas Sensor Array Under Dynamic Gas Mixtures

395
398
1,993
4,897
17,379
25,180
52,397
53,500
515,344
4,208,261

30
7

122
11
13
36
223
367
90
16

20 sec
4 sec

400 sec
100 sec
350 sec
1,500 sec
15,000 sec
30,000 sec
70,000 sec
100,000 sec

6 sec
1 sec

100 sec
30 sec
100 sec
400 sec
4,000 sec
10,000 sec
20,000 sec
30,000 sec

2 sec
0.3 sec
30 sec
10 sec
30 sec
100 sec
1,000 sec
3,000 sec
6,000 sec
10,000 sec

values D and E. The parties can compute the shares of Z
using the fact that

Z = AE + BD + AB + DE = AE + BD + C + DE.

For instance, P1 can output Z1 = A1E + B1D + C1 + DE
and all the other parties Zi = AiE + BiD + Ci. The correct-
ness can trivially be veriﬁed by inspection. The security of
this method follows from the fact that, when D and E are
opened, the values of X and Y are masked by the random
factors A and B from the matrix multiplication triple (and
all other operations are local). For the truncation protocol,
one of the parties, lets say P1, learns the value to be trun-
cated w blinded by a factor r, the other parties only use
their shares of w and r(cid:48) (which represents the least signiﬁ-
cant bits of r) in order to get their outputs. The remaining
protocols can be trivially generalized to the case of multiple
parties.

12 Securing Against Malicious Adversaries
One possibility for obtaining security against malicious ad-
versaries is to use the compute with MACs approach [6,
11, 10] to protect the shared values from being manipu-
lated by a malicious adversary. In this technique there is an
unconditionally secure message authentication code (MAC)
associated to each shared secret value. At the beginning of
the protocol the parties commit to their inputs by opening
the diﬀerence between their inputs and some random shared
value (that has an associated MAC). Whenever an operation
is performed over some shared values, the MAC for the out-

put value is also computed (using the MACs of the input val-
ues). This approach prevents the malicious adversary from
successfully deviating from the speciﬁed protocol instruc-
tions (if he deviates, the honest parties will notice it during
the checking of the MACs). The only problem for applying
this technique is that the truncation protocol used before is
probabilistic and so it would not allow propagation of the
MACs, but this can be solved by using a deterministic trun-
cation procedure [1] (at the cost of having to perform one
execution of a comparison protocol per truncation). More
details about security against malicious adversaries will be
presented in the full version. See [6, 11, 10] for further de-
tails about the compute with MACs technique.

13 Conclusion
In this paper we have presented an information-theoretically
secure protocol for privacy preserving linear regression in the
commodity-based model. The protocol has an oﬄine phase
where a trusted authority pre-distributes random correlated
data to Alice and Bob and never again engages in the pro-
tocol and/or the online phase. The online phase is orders
of magnitude faster when compared to previous solutions in
the literature [26, 19].

When a trusted authority is not desirable, we propose an
oﬄine phase that is computationally secure and based on
any additive homomorphic public key cryptosystem. This
oﬄine phase is computationally heavy but completely par-
allelizable, making it practical when a large number of cores
is available.

1214 References

[1] M. Aliasgari, M. Blanton, Y. Zhang, and A. Steele.

Secure computation on ﬂoating point numbers. In
ISOC Network and Distributed System Security
Symposium – NDSS 2013, San Diego, California, USA,
Feb. 24–27, 2013. The Internet Society.

[2] D. Beaver. Precomputing oblivious transfer. In

D. Coppersmith, editor, Advances in Cryptology –
CRYPTO’95, volume 963 of Lecture Notes in
Computer Science, pages 97–109, Santa Barbara, CA,
USA, Aug. 27–31, 1995. Springer, Berlin, Germany.

[3] D. Beaver. Commodity-based cryptography (extended
abstract). In 29th Annual ACM Symposium on Theory
of Computing, pages 446–455, El Paso, Texas, USA,
May 4–6, 1997. ACM Press.

[4] D. Beaver. One-time tables for two-party

computation. In Computing and Combinatorics, pages
361–370. Springer, 1998.

[5] D. Beaver. Server-assisted cryptography. In

Proceedings of the 1998 workshop on New security
paradigms, NSPW ’98, pages 92–106, New York, NY,
USA, 1998. ACM.

[6] R. Bendlin, I. Damg˚ard, C. Orlandi, and S. Zakarias.

Semi-homomorphic encryption and multiparty
computation. In K. G. Paterson, editor, Advances in
Cryptology – EUROCRYPT 2011, volume 6632 of
Lecture Notes in Computer Science, pages 169–188,
Tallinn, Estonia, May 15–19, 2011. Springer, Berlin,
Germany.

[7] C. Blundo, B. Masucci, D. R. Stinson, and R. Wei.

Constructions and bounds for unconditionally secure
non-interactive commitment schemes. Des. Codes
Cryptography, 26(1-3):97–110, June 2002.

[8] R. Canetti. Universally composable security: A new

paradigm for cryptographic protocols. In 42nd Annual
Symposium on Foundations of Computer Science,
pages 136–145, Las Vegas, Nevada, USA, Oct. 14–17,
2001. IEEE Computer Society Press.

[9] O. Catrina and A. Saxena. Secure computation with

ﬁxed-point numbers. In R. Sion, editor, FC 2010: 14th
International Conference on Financial Cryptography
and Data Security, volume 6052 of Lecture Notes in
Computer Science, pages 35–50, Tenerife, Canary
Islands, Spain, Jan. 25–28, 2010. Springer, Berlin,
Germany.

[10] I. Damg˚ard, M. Keller, E. Larraia, V. Pastro,

P. Scholl, and N. P. Smart. Practical covertly secure
MPC for dishonest majority - or: Breaking the SPDZ
limits. In J. Crampton, S. Jajodia, and K. Mayes,
editors, ESORICS 2013: 18th European Symposium
on Research in Computer Security, volume 8134 of
Lecture Notes in Computer Science, pages 1–18,
Egham, UK, Sept. 9–13, 2013. Springer, Berlin,
Germany.

[11] I. Damg˚ard, V. Pastro, N. P. Smart, and S. Zakarias.

Multiparty computation from somewhat homomorphic
encryption. In R. Safavi-Naini and R. Canetti, editors,
Advances in Cryptology – CRYPTO 2012, volume
7417 of Lecture Notes in Computer Science, pages
643–662, Santa Barbara, CA, USA, Aug. 19–23, 2012.
Springer, Berlin, Germany.

[12] R. Dowsley, J. Graaf, D. Marques, and A. C. A.
Nascimento. A two-party protocol with trusted
initializer for computing the inner product. In
Y. Chung and M. Yung, editors, WISA 10: 11th
International Workshop on Information Security
Applications, volume 6513 of Lecture Notes in
Computer Science, pages 337–350, Jeju Island, Korea,
Aug. 24–26, 2010. Springer, Berlin, Germany.

[13] R. Dowsley, J. M¨uller-Quade, A. Otsuka, G. Hanaoka,

H. Imai, and A. C. A. Nascimento. Universally
composable and statistically secure veriﬁable secret
sharing scheme based on pre-distributed data. IEICE
Transactions, 94-A(2):725–734, 2011.

[14] W. Du, Y. S. Han, and S. Chen. Privacy-preserving

multivariate statistical analysis: Linear regression and
classiﬁcation. In In Proceedings of the 4th SIAM
International Conference on Data Mining, pages
222–233, 2004.

[15] J. Feigenbaum, Y. Ishai, T. Malkin, K. Nissim,

M. Strauss, and R. N. Wright. Secure multiparty
computation of approximations. In Automata,
Languages and Programming, 28th International
Colloquium, ICALP 2001, Crete, Greece, July 8-12,
2001, Proceedings, pages 927–938, 2001.

[16] J. Feigenbaum, Y. Ishai, T. Malkin, K. Nissim, M. J.

Strauss, and R. N. Wright. Secure multiparty
computation of approximations. ACM Transactions on
Algorithms, 2(3):435–472, 2006.

[17] O. Goldreich. Foundations of Cryptography: Basic

Applications, volume 2. Cambridge University Press,
Cambridge, UK, 2004.

[18] C.-H. Guo and N. J. Higham. A schur-newton method
for the matrix p’th root and its inverse. SIAM Journal
On Matrix Analysis and Applications, 28(3):788–804,
oct 2006.

[19] R. Hall, S. E. Fienberg, and Y. Nardi. Secure multiple

linear regression based on homomorphic encryption.
Journal of Oﬃcial Statistics, 27(4):669–691, 2011.

[20] Y. Ishai, E. Kushilevitz, S. Meldgaard, C. Orlandi, and

A. Paskin-Cherniavsky. On the power of correlated
randomness in secure computation. In Theory of
Cryptography, pages 600–620. Springer, 2013.

[21] A. F. Karr, X. Lin, A. P. Sanil, and J. P. Reiter.

Secure regression on distributed databases. Journal of
Computational and Graphical Statistics,
14(2):263–279, 2005.

[22] A. F. Karr, X. Lin, A. P. Sanil, and J. P. Reiter.

Privacy-preserving analysis of vertically partitioned
data using secure matrix products. Journal of Oﬃcial
Statistics, 25(1):125, 2009.

[23] E. Kiltz, G. Leander, and J. Malone-Lee. Secure

computation of the mean and related statistics. In
J. Kilian, editor, TCC 2005: 2nd Theory of
Cryptography Conference, volume 3378 of Lecture
Notes in Computer Science, pages 283–302,
Cambridge, MA, USA, Feb. 10–12, 2005. Springer,
Berlin, Germany.

[24] A. C. A. Nascimento, J. M¨uller-Quade, A. Otsuka,

G. Hanaoka, and H. Imai. Unconditionally secure
homomorphic pre-distributed bit commitment and
secure two-party computations. In C. Boyd and
W. Mao, editors, ISC 2003: 6th International

13Conference on Information Security, volume 2851 of
Lecture Notes in Computer Science, pages 151–164,
Bristol, UK, Oct. 1–3, 2003. Springer, Berlin,
Germany.

[25] A. C. A. Nascimento, J. M¨uller-Quade, A. Otsuka,

G. Hanaoka, and H. Imai. Unconditionally
non-interactive veriﬁable secret sharing secure against
faulty majorities in the commodity based model. In
M. Jakobsson, M. Yung, and J. Zhou, editors, ACNS
04: 2nd International Conference on Applied
Cryptography and Network Security, volume 3089 of
Lecture Notes in Computer Science, pages 355–368,
Yellow Mountain, China, June 8–11, 2004. Springer,
Berlin, Germany.

[26] V. Nikolaenko, U. Weinsberg, S. Ioannidis, M. Joye,

D. Boneh, and N. Taft. Privacy-preserving ridge
regression on hundreds of millions of records. In
Security and Privacy (SP), 2013 IEEE Symposium on,
pages 334–348. IEEE, 2013.

[27] P. Paillier. Public-key cryptosystems based on

composite degree residuosity classes. In J. Stern,
editor, Advances in Cryptology – EUROCRYPT’99,
volume 1592 of Lecture Notes in Computer Science,
pages 223–238, Prague, Czech Republic, May 2–6,
1999. Springer, Berlin, Germany.

[28] R. L. Rivest. Unconditionally secure commitment and
oblivious transfer schemes using private channels and
a trusted initializer. Preprint available at
http://people.csail.mit.edu/rivest/Rivest-
commitment.pdf, 1999.

[29] A. P. Sanil, A. F. Karr, X. Lin, and J. P. Reiter.

Privacy preserving regression modelling via
distributed computation. In Proceedings of the tenth
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 677–682. ACM,
2004.

[30] S. Shalev-Shwartz and S. Ben-David. Understanding

Machine Learning: From Theory to Algorithms.
Cambridge University Press, 2014.

[31] R. Tonicelli, A. C. Nascimento, R. Dowsley,
J. M¨uller-Quade, H. Imai, G. Hanaoka, and
A. Otsuka. Information-theoretically secure oblivious
polynomial evaluation in the commodity-based model.
International Journal of Information Security, pages
1–12, 2014.

APPENDIX
A Problems with the Truncation Protocol of

Hall et al. [19]

The protocol of Hall et al.
[19] uses operations in a ﬁnite
ﬁeld and homomorphic encryption as a building block. A
ﬁxed-point data type is deﬁned and also its representation in
the ﬁnite ﬁeld. In such approach it is necessary to perform
a truncation after each multiplication, but the truncation
protocol of [19] is not correct. The truncation protocol of
[19] works in two phases. First, for P (cid:28) q and ˆx such
that |ˆx| < P , it takes the encoding x of ˆx in Zq (i.e., x ∈
{0, . . . , P/2 − 1} ∪ {q − P/2 + 1, . . . , q − 1}) and generates
shares of ˆx encoded in the smaller ﬁeld ZP . Then using the
encodings of ˆx in both Zq and ZP it performs the truncation.
In their protocol Alice has the private key to an instance of
Paillier’s encryption scheme, Bob knows the corresponding
public key q, and has the encryption under q of a value ˆx
that is encoded as x ∈ Zq, denoted E(x). It is assumed that
|ˆx| < P for P (cid:28) q. The parties want to truncate the input
that have an additional factor M , each obtaining a share of
the truncated output. It works as follows:

1. Bob draws r uniformly at random from the set {P, . . . , q−
1}. He then sets xBob = r mod P , computes E(x − r)
using the homomorphic properties of the Paillier’s en-
cryption scheme and sends it to Alice.

2. Alice decrypts the value to obtain s = x− r mod q and
sets xAlice = (s−q) mod P . She computes E(xAlice) and
sends the ciphertext to Bob.

3. Bob uses the homomorphic properties of the Paillier’s
encryption scheme to compute E((xAlice+xBob−x)M−1−
r(cid:48)) for a random r(cid:48) ∈ Zq, sends it to Alice and outputs

4. Alice decrypts s(cid:48) = (xAlice + xBob − x)M−1 − r(cid:48) and

(cid:5) − r(cid:48) mod q.

(cid:4) xBob
outputs(cid:4) xAlice

M

(cid:5) − s(cid:48) mod q.

M

This protocol is not correct since the subprotocol for gen-
erating the shares of the secret ˆx encoded in ZP is not cor-
rect. If the blinding factor r is such that P < x < r then
equation 5 used in the correctness proof (see [19]) does not
hold since the equality s + r = x + q (in the integers) does
not hold. This can be solved by adding P/2 to x before ex-
ecuting the P -sharing subprotocol and then removing P/2
accordingly in the end of the subprotocol.

14