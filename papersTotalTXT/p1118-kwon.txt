The Dropper Effect: Insights into Malware Distribution with

Downloader Graph Analytics

Bum Jun Kwon

University of Maryland
College Park, MD, USA
bkwon@umd.edu

Jayanta Mondal

University of Maryland
College Park, MD, USA
jayanta@cs.umd.edu

Jiyong Jang
IBM Research

Yorktown Heights, NY, USA

jjang@us.ibm.com

Leyla Bilge

Symantec Research Labs

France

leyla_bilge@symantec.com

Tudor Dumitras,

University of Maryland
College Park, MD, USA

tdumitra@umiacs.umd.edu

ABSTRACT
Malware remains an important security threat, as miscreants con-
tinue to deliver a variety of malicious programs to hosts around
the world. At the heart of all the malware delivery techniques are
executable ﬁles (known as downloader trojans or droppers) that
download other malware. Because the act of downloading software
components from the Internet is not inherently malicious, benign
and malicious downloaders are difﬁcult to distinguish based only
on their content and behavior.
In this paper, we introduce the
downloader-graph abstraction, which captures the download acti-
vity on end hosts, and we explore the growth patterns of benign and
malicious graphs. Downloader graphs have the potential of expo-
sing large parts of the malware download activity, which may othe-
rwise remain undetected. By combining telemetry from anti-virus
and intrusion-prevention systems, we reconstruct and analyze 19
million downloader graphs from 5 million real hosts. We identify
several strong indicators of malicious activity, such as the growth
rate, the diameter, and the Internet access patterns of downloader
graphs. Building on these insights, we implement and evaluate a
machine learning system for malware detection. Our system achie-
ves a 96.0% true-positive rate, with a 1.0% false-positive rate, and
detects malware an average of 9.24 days earlier than existing anti-
virus products. We also perform an external validation by exami-
ning a sample of unlabeled ﬁles that our system detects as mali-
cious, and we ﬁnd that 41.41% are blocked by anti-virus products.
Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—Secu-
rity and protection
General Terms
Security
Keywords
Downloader Graph; Malware classiﬁcation; Early detection
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813724.

1.

INTRODUCTION

Because cyber criminals constantly re-pack and obfuscate ma-
licious software (malware) to evade detection, the security com-
munity has turned its attention to content-agnostic techniques for
detecting malware [3, 5, 10, 14, 20, 28, 30, 31]. For example, re-
search has generally focused on understanding the properties of the
global malware distribution networks, such as their business mo-
dels [3], their network-level behavior [10, 30], or their server-side
infrastructure [14,31]. Prior research has also investigated how ma-
lware samples are downloaded on end hosts, e.g., through drive-by-
download attacks [20] (which exploit vulnerabilities in web brow-
sers when users are surﬁng the Web), pay-per-install infrastructu-
res [3] (which are paid services that distribute malware on behalf
of their afﬁliates), or self-updating malware (e.g., worker bots from
recent botnets [35]). Comparatively, less attention has been given
to the executable ﬁles (colloquially called downloader trojans or
droppers) that download a variety of supplementary malware (cal-
led payloads) at the heart of malware distribution techniques.

It is not trivial to distinguish benign and malicious downloa-
ders simply based on their content and behavior because the act of
downloading software components from the Internet is not a sign
of inherently malicious intent. For example, many benign applica-
tions download legitimate installers and software updates. Owing
to social engineering or drive-by-download attacks, benign appli-
cations (e.g., web browsers) download malicious programs, which
may then download additional malware. Some malware droppers
are known to be active for over two years [23] due to the difﬁculty
of determining their maliciousness.

However, the downloader-payload relationship of executable ﬁ-
les on a host, and the downloader graphs generated by the rela-
tionship, can provide unique insights into malware distribution. Fi-
gure 1 shows a real downloader graph example that illustrates this
opportunity. A benign web browser (node A) downloads two ﬁles
from the same domain: nodes B and D that are unlabeled (i.e., not
known to be malicious or benign). However, node B downloads
additional ﬁles, some of which (nodes C and F) are detected as ma-
licious, suggesting that node B, and potentially node D downloaded
from the same domain as node B, as well as all of the nodes rea-
chable from them in the downloader graph, are likely involved in
malware distribution. Consequently, by analyzing the downloader
graphs on a host we can identify large parts of the malware down-
load activity, which may otherwise remain undetected.

In this paper, we conduct a systematic analysis of downloader
graphs in the wild, exposing the differences between the growth

1118to detect unknown malware an average of 9.24 days earlier than
the anti-virus products employed by VirusTotal. Finally, we per-
form an external validation of the classiﬁer by querying VirusTotal
for some of the unlabeled samples predicted to be malicious, and
ﬁnd that 41.41% of them are reported as malicious by anti-virus
products.
In summary, the paper makes the following contributions:
• We build a large data set of malicious and benign download
activities on 5 million real hosts by reconstructing download
events from IPS and AV telemetry. We also build a ground
truth of malicious and benign downloaders by combining three
data sources.

• We propose a graph-based abstraction to model the download
activity on end hosts, and perform a large measurement study
to expose the differences in the growth patterns between benign
and malicious downloader graphs.

• We use insights from our measurements to build a malware de-
tection system, using machine learning on downloader graph
features, and evaluate it using both internal and external perfor-
mance metrics.

Outline. The rest of the paper is organized as follows. In Section 2
we overview the threat of malicious downloaders and we state our
goals. In Section 3 we formally introduce the downloader graph
abstraction, and we discuss our data sources and our method for
constructing downloader graphs. We then present our measurement
results in Section 4, followed by the design and evaluation of our
classiﬁer in Section 5. In Section 6 we discuss the implications of
our results, and in Section 7 we review the related work.
2. THREAT MODEL

In this section, we present an overview of the security threats

imposed by downloaders and articulate the goals of our research.
Overview of Downloaders. Applications often have a particu-
lar component that is responsible for determining which software
components are required to be installed or updated, and for down-
loading them from remote servers. In this work, we refer to this
component as the downloader.

The software delivery process beneﬁts from the ubiquitous ac-
cess to the Internet, e.g., efﬁcient updating mechanisms, and cus-
tomizable software installation. As a matter of fact, these beneﬁts
make the distribution of both legitimate and malicious software re-
liable and easy. Multi-phase malware ﬁrst gains a foothold on a sys-
tem, and installs a small executable—called droppers or downloa-
der trojans—which then downloads additional components, such
as the rest of the malicious payload. An early example of such
multi-phase malware was the Sobig email worm discovered in
2003, which downloaded additional ﬁles to set up spam relay ser-
vers on infected machines and to update itself [27].

The next evolution step was the emergence of general-purpose
droppers that could be conﬁgured to propagate different malware
families (e.g., Bredolab trojan [29]). Some general-purpose dro-
ppers represent the client-side part of pay-per-install (PPI) infras-
tructures, which allows malware authors to install arbitrary execu-
tables on thousands of hosts and to select their targets based on
the properties of those hosts (e.g., their geographical locations) [3].
These infrastructures also have server-side systems designed to be
resilient to takedown efforts [3,14,18], which may continue to ope-
rate for over two years [23]. They employ sophisticated techniques
to disseminate malicious payloads to a large number of hosts, e.g.,
drive-by-downloads [20], social engineering, search engine poiso-
ning [13], and provide this functionality as a service [3]. PPI down-
loaders represent only a fraction of the current population of down-

Figure 1: Example of a real downloader graph and the inﬂu-
ence graphs of two selected downloaders.

patterns of benign and malicious graphs. We construct downloader
graphs by combining telemetry collected by Symantec’s intrusion
prevention systems (IPS) and anti-virus (AV) products on real hosts
targeted by malware distribution networks. This data is collected
from users who opt in for Symantec’s data sharing program and
is available on the WINE analytics platform [7]. Speciﬁcally, we
correlate reports of Portable Executable (PE) ﬁle downloads over
HTTP with records of executables present on end hosts to recon-
struct the download activity on 5 million Windows hosts around the
world. We further introduce the notion of inﬂuence graph (marked
by dotted outlines in Figure 1), which is the subgraph reachable
from a given downloader on the downloader graph. Intuitively, the
inﬂuence graph represents the download activity that a downloa-
der has caused on a host. We identify 19 million inﬂuence graphs
in our data. We label 15 million of these graphs as benign and
0.25 million as malicious, using data from VirusTotal, the Natio-
nal Software Reference Library (NSRL), and an additional ground
truth data set we received from Symantec.

We demonstrate that the growth rate of inﬂuence graphs is a
strong indicator of malicious activity, as successful malware cam-
paigns deliver their payloads slowly to evade detection. Speciﬁca-
lly, almost 88% of inﬂuence graphs with average inter-download
times greater than 1.5 days/node are malicious, and 65% of the
malicious inﬂuence graphs have inter-download times above this
threshold. Additionally, 84% of the inﬂuence graphs with diameter
≥ 3 are malicious, as competition and arbitrage opportunities in
the underground economy [3] result in situations where malicious
droppers download other droppers. We also ﬁnd that the average
number of downloaders accessing a domain and the average nu-
mber of ﬁles downloaded from a domain, computed per inﬂuence
graph, are relevant to certain classes of benign and malicious down-
loaders. Surprisingly, 55.5% of malicious downloaders are digita-
lly signed (22.4% have valid signatures), suggesting that the or-
ganizations responsible for a large part of the malware download
activity are not trying to evade attack attribution.

We use the properties of inﬂuence graphs as features to learn a
classiﬁer for identifying malicious executables. Because of the di-
fferences in the inﬂuence graphs depending on malware classes and
maturity stages produced by the slow growth rates (which could be
perceived as a noise by a learning algorithm), we choose to employ
a random forest classiﬁer that is known to perform well with such
variability in the data. Our classiﬁer achieves a 96.0% true positive
rate, with a 1.0% false positive rate, using 10-fold cross validation.
We also estimate, conservatively, that our classiﬁer would be able

ABCDEGIExplore.exechrome_setup.exechrome_setup[2].exesizlsearch.adFsetup.exesizlsearchuninstall.exed3rmfxfj03.cloudfront.netinstall-cdn.sizlsearch.netinstall-cdn.sizlsearch.netchrome_setup[1].exetheappcenter.comsecure.2-pn-installer.comBenignMaliciousUnlabeledsecure.2-pn-installer.comIG(B)IG(C)1119Inﬂuence graphs
Files (graph nodes)
Total Downloaders
Benign downloaders
Malicious downloaders
Download events (graph edges)
Domains accessed
Hosts

19 million
24 million
0.9 million
87,906
67,609
50.5 million
0.5 million
5 million

Table 1: Summary of our data sets and ground truth.

loaders, as simpler and older forms continue to operate [23]. In
consequence, malware samples often rely on the functionality pro-
vided by downloaders.
Problem Statement. In this paper, we conduct a systematic ana-
lysis of downloaders in the wild with a special focus on the rela-
tionship between the downloaders and the supplementary execu-
tables they download. We represent the transitive closure of this
relationship using a graph abstraction. Our ﬁrst goal is to uncover
the differences between the graph structures constructed from be-
nign and malicious download activities. By leveraging the insights
obtained from the analysis, our second goal is to propose a new me-
thod to detect malicious downloaders. Our method could improve
the overall performance of malware detectors by providing earlier
warnings for malicious download activities. This approach com-
plements existing host-based malware detection systems as it helps
comprehend how various ﬁles are delivered to end-hosts using the
dropper-payload relationship to connect the dots.

We also have some non-goals: in this paper, we do not aim to
analyze the server-side infrastructure and network-level behavior
of malware delivery networks, to determine for how long these ne-
tworks remain operational, to proﬁle the organizations involved in
malware distribution, or to improve network security. Rather, our
main goal is to complement existing approaches focusing on server-
side infrastructures [14, 31] and network-level behaviors [10, 30].
3. DOWNLOADER GRAPH ANALYTICS

We start our discussion by describing our data sets, followed by
a formal description of the downloader graph abstraction and how
we construct the downloader graphs from the collected data.
3.1 Data Sources

We infer the download activities on 5 million end-hosts using
data available on the Worldwide Intelligence Network Environment
(WINE) [7], a platform for data intensive experiments in cyber se-
curity provided by Symantec. WINE contains security telemetry
collected on real hosts that are often targeted by cyber attacks. The
data is collected from the users who opt in for Symantec’s data
sharing program, and does not include personally identiﬁable in-
formation. Speciﬁcally, we use two WINE data sets: (a) binary
reputation, and (b) intrusion prevention system telemetry. The data
sets are stored as SQL relations consisting of multiple columns. We
extract relevant columns from each relation, and join them to gene-
rate the data sets we require for our study. Table 1 summarizes our
data.

The binary reputation data includes summarized information
about all binary download activities on the Symantec’s customers’
machines. From this data we collect the following information:
the server-side timestamp of the event, the unique identiﬁer for the
machine that initiated the download, the name and SHA2 hash of
the downloaded ﬁle, and the SHA2 hash of the parent ﬁle (an ar-
chive including the downloaded ﬁle or the actual downloader). We
extract information about 24 million distinct ﬁles, including 0.4 mi-
llion parent ﬁles, from the binary reputation data set.

The intrusion prevention system (IPS) telemetry data provides
information about malicious activities detected on network streams.
In addition, the IPS telemetry includes logs of network activity on
the host that might not be necessarily tied to any malicious activi-
ties. The IPS telemetry reports downloads of Portable Executable
(PE) ﬁles over HTTP. From this data set, we extract the unique
identiﬁer of the host, the MD5 hash of the process initiated the ne-
twork activity (portal in IPS jargon), the server-side timestamp and
the URL from which the portal downloads the binary. From the IPS
telemetry, we extract information about 0.5 million portals.
3.2 Ground Truth Data For Executables

Our ground truth consists of a large number of known-malicious
and known-benign ﬁles, recorded in VirusTotal,
the National
Software Reference Library (NSRL), and an additional data set re-
ceived from Symantec.
VirusTotal. VirusTotal1 is a service that provides a public API for
scanning ﬁles with up to 54 different anti-virus (AV) products, and
for querying hashes to retrieve their previous analysis reports. We
query VirusTotal for each downloader in our data set to obtain its
ﬁrst-seen timestamp, the number of AV products that ﬂagged the
binary as malicious, the total number of AV products that scanned
the binary, and the corresponding ﬁle signer information. We then
compute the percentage rmal of products that ﬂagged the binary
as malicious. We consider that a ﬁle is malicious if rmal ≥ 30%;
because AV vendors are typically worried about false positives, we
believe that this represents a conservative threshold. We veriﬁed
that approximately 80% of the ﬁles above this threshold are also
labeled as malicious in the Symantec ground truth described below.
National Software Reference Library. NSRL2 is a project that
collects software installers from leading software vendors with the
purpose of creating a reference data set (RDS) of benign software.
RDS is a collection of digital signatures (mainly SHA1, MD5, and
other metadata) of known, traceable software applications. NSRL
releases the RDS four times per year. We used the RDS 2.47 ver-
sion that was released in December 2014. We treat all the down-
loaders with matching hashes in NSRL as benign.
Additional Ground Truth for Files. Due to the limitations impo-
sed by the VirusTotal API, we were unable to query all the 24 mi-
llion ﬁle hashes. We therefore complement our ground truth with
an additional ground truth maintained by Symantec. This step in-
creases the coverage of our ground truth.

After combining all these sources of ground truth, we identify
87,906 benign and 67,609 malicious downloaders; the rest of the
downloaders remain unlabeled.
3.3 Downloader Graph

In this section, we introduce the notion of downloader graph
(DG) and inﬂuence graph (IG). A DG is a directed graph, deﬁned
for each host machine, where a node represents a downloaded ﬁle
and an edge from downloader da to db indicates that da has down-
loaded db on the corresponding host machine. On the other hand,
an IG is deﬁned for each individual downloader (called root) on a
given host machine, and is a subgraph of the DG on that host. In-
ﬂuence graph captures the impact of its download root by encoding
the downloads (both direct and indirect) caused by the root.

Figure 1 illustrates an example of a DG and two IGs (in loo-
ped dotted lines) in it. In this example, the nodes are annotated
with their ﬁle names, and edges are annotated with the domains of

1
2

https://www.virustotal.com/
http://www.nsrl.nist.gov/

1120the download URLs. We discuss additional properties of the no-
des and edges of the downloader graph in Section 3.4. Note that
the downloader graph per machine could be disconnected, unlike a
connected one in this example. Another observation is that the in-
ﬂuence graph of a downloader could be contained in the inﬂuence
graph of another downloader; in particular, the inﬂuence graph of a
malicious dropper can be a part of the inﬂuence graph of a benign
downloader.
Now we provide the formal deﬁnition of the DG abstraction.
Considering V to be the set of all executables in our data set, and
M to be the set of all host machines, downloader graph Gi for ma-
chine Mi is deﬁned as Gi = (Vi, Ei, α, β), where:
• Vi ⊆ V denotes the set of executable ﬁles downloaded on ma-
chine Mi. Note that the same executable could be downloaded
across multiple machines.

• Ei ⊆ Vi X Vi denotes a set of directed edges that correspond to

the download relations between executables.

• α denotes a set of properties deﬁned on the nodes of the
downloader graphs. Node properties could be: (a) machine-
dependent (unique across all machines), and (b) machine-
independent (unique to a host machine) properties.

• β denotes a set of properties deﬁned on the edges of the down-
loader graphs.
Now, we deﬁne inﬂuence graph. The inﬂuence graph Ig(dMi )
of a download root d in machine Mi is deﬁned as the subgraph of
Gi which is reachable by traversing Gi starting at d. Note that, the
inﬂuence graphs are deﬁned for every downloader (i.e., an execu-
table that has downloaded at least one executable) in a DG, but not
for the other executables in a DG.

3.4 Constructing DGs and Labeling IGs

the

this

IPS telemetry data

In this section, we start by describing how we construct the DGs
for each machine using the data described in Section 3.1. Then
for each possible root of the DGs, we extract the IGs and label as
benign, malicious, or unlabeled using the ground truth data for the
individual downloaders.
Constructing Downloader Graphs. We start by extrac-
from the URL
ting the names of
the downloaded ﬁles
column of
(example
entry:
set
http://somedomain.com/file_name.exe);
corres-
ponds to the name of the ﬁle created on a disk. 95% of the URLs
from which users downloaded PE ﬁles include the name of the
ﬁle downloaded. We then search for these ﬁles in the binary
reputation data set, which reports ﬁle creation events and includes
the corresponding SHA2 hashes.
If a matching ﬁlename and
source domain appear in the binary reputation data set within ±2
days from the IPS event timestamp, we create a graph node for
the ﬁle, add the ﬁle hash as a node attribute, and add an incoming
edge from the portal reported in the IPS event. In consequence,
we may miss certain graph edges in some rare cases (e.g., the user
changes the ﬁlename, the malware renames itself), but the edges
we create correspond to genuine download events. We look 2 days
before or after the IPS event because server-side timestamps may
be affected by transmission delays and different submission orders
between the two data sets. We employ an approximate ﬁle name
matching algorithm by computing the edit distance between ﬁle
names and by accepting pairs with distance below 3 as matches.
This allows us to handle differences in the ﬁle name caused by
duplicate downloads (e.g., setup.exe and setup[1].exe). We
also extract the domain name from the URL and add it as an
attribute to the edge.

We also analyze the relationship between ﬁles and their parents
in the binary reputation data set.
If there exists a parent for the
downloaded ﬁle, we consider that the parent downloaded the ﬁle
and add a directed edge from the parent to the ﬁle; then we assign
the domain name extracted from the downloaded ﬁle’s source URL
as an attribute of the new edge. The step was motivated by our ob-
servation that many of the parents recorded in the binary reputation
data set are the same as the portals in the IPS telemetry.

of a downloader.

ble is downloaded on a host.

During the downloader graph construction, we add the following
properties to each node in the graph:
• Number of outgoing edges: This captures the download volume
• Number of incoming edges: This captures how often an executa-
• Time interval between a node and its out-neighbors: This captu-
res how quickly, on an average, a downloader tends to download
other executables after it was downloaded on a host.
• File score (based on digital signatures): We assign a ﬁle score in
the range 0–3 for every downloader in our data set based on their
digital signatures. Speciﬁcally, we take into account the availa-
bility of the following information: (a) signature, (b) publisher
information, and (c) reputation of the certiﬁcation authorities.
The executables without ﬁle signatures or records in VirusTotal
will be assigned with score 0. Otherwise, if the signature con-
tains information about the publisher, we assign score 1. Then,
if the certiﬁcate validation chain contains a certiﬁcate authority
among Comodo, VeriSign, Go Daddy, GlobalSign, and DigiCert,
we add 1 to the score. Finally, if the signature is valid, we also
add 1 to the score.
• Number of out-neighbors with score 0 or 1: This represents a ru-
dimentary quantiﬁcation of the malicious intent of a downloader.
We add the following properties to each edge in the graph:

download URL had a domain name or an IP address.

• URL has IP instead of domain: denotes if the corresponding
• URL is Localhost: denotes if the corresponding download URL
• URL is in Alexa top 1 million: denotes if the download URL is

was relative to a localhost address.

hosts it appears on, in the binary reputation data set.

in Alexa3 top 1 million websites.
We also extract some aggregated properties, which we will leve-
rage to derive some features of inﬂuence graphs:
• File prevalence (FP): For every ﬁle, we count the number of
• Number of unique downloaders accessing given URL (UDPL):
For every URL (domain), we count the number of unique down-
loaders that used the domain to download new executables,
aggregated across all machines.
• Number of unique downloads from a given URL (UDFL): For
every URL (domain), we count the number of unique executa-
bles downloaded from the domain, aggregated across all machi-
nes.
Figure 2 illustrates the distribution of nodes, edges, and life span
for our inﬂuence graphs. There are 3.66 nodes on average per in-
ﬂuence graph, with a minimum 2 nodes and a maximum of 66,573
nodes. Inﬂuence graphs have between 1–66572 edges, with an ave-
rage of 2.66 edges. These graphs have an average life span (diffe-
rence between the timestamps of the ﬁrst and the last node in the
graph) of 75.3 days, ranging from 0 to 2199.9 days. Because this
paper focuses on analyzing the properties of downloader graphs, in
the rest of our analysis we exclude the inﬂuence graphs with fewer

3

http://www.alexa.com/

1121Figure 2: Distributions of inﬂuence-graph properties: (a) Number of nodes, (b) Number of edges, (c) Life span.

than 3 nodes, which might not provide sufﬁcient insight for our
problem.
Labeling Inﬂuence Graphs. We label the inﬂuence graphs, using
the benign and malicious labels of downloaders, determined as des-
cribed in Section 3.2. We label only the IGs whose root downloader
is known to our ground truth. Speciﬁcally, we consider that an IG
is malicious if its root downloader is labeled as malicious. On the
other hand, we consider that an IG is benign if one of the following
three conditions is true: (1) the root is in NSRL, (2) the digital sig-
nature of the root is from a well known publisher and veriﬁed by
VirusTotal, or (3) the root is rmal = 0 and is benign in Symantec
ground truth, and the next two are also true: (1) none of the other
nodes in the IG have rmal > 0, (2) none of the inﬂuence graph no-
des is labeled as malicious in Symantec ground truth. This results
in 14,918,097 benign and 274,126 malicious inﬂuence graphs.

4.

INSIGHTS INTO INFLUENCE GRAPHS
We now present our insights into the ecosystem of benign and
malicious inﬂuence graphs. We ﬁrst investigate the ability of mali-
cious downloaders to stay under the radar of the security commu-
nity, while delivering malware. This allows us to assess the mag-
nitude of this threat and to create a data set of malicious download
graphs, unbiased by interference from anti-virus products, for fur-
ther empirical analysis. We then analyze the properties of inﬂuence
graphs that correspond to malicious and benign downloaders, and
identify properties that can be utilized in malware detection.
4.1 Unknown Droppers

The functionality of benign and malicious downloaders is simi-
lar: both retrieve software components from the Internet, someti-
mes in response to commands received from a remote server. We
therefore expect that it takes a while until the security community
recognizes that a downloader is involved in malicious activities.

For each downloader labeled as malicious in our data set, we
compute the earliest ﬁle timestamp in the binary reputation data
from WINE, which approximates the date when the sample appea-
red in the wild. We compare this timestamp with the time when
the ﬁle was ﬁrst submitted to VirusTotal, which approximates the
date when the malicious dropper became known to the security co-
mmunity. These are not perfect approximations. Antivirus com-
panies change their settings speciﬁcally for VirusTotal compared
to their commercial versions4, so we cannot conclude that the dro-
ppers it fails to label as malicious are undetectable. Similarly, be-
cause the dropper may be delivered initially to a host not covered
by WINE, the interval when the dropper remains unknown may be
under-estimated. However, given the large number of hosts where
data is collected (5 million) and the fact that several anti-virus pro-
ducts (up to 54) must agree, unanimously, that a downloader is not
known to be malicious, we believe that the structure of downloader
graphs analyzed in this section is representative of the way mali-

4

https://www.virustotal.com/en/faq/

Figure 3: Distribution of the interval between the earliest ﬁle
timestamp in WINE and the ﬁrst seen time from VirusTotal,
for all the malicious downloaders.

cious droppers operate in the wild before they become known to
the security community.

Figure 3 shows the distribution of the time interval between the
timestamp of the downloader on each host and the ﬁrst-seen times-
tamp in VirusTotal. Negative time intervals correspond to unknown
droppers. We identify 140,062 inﬂuence graphs rooted in unknown
malicious droppers. Among the 67,609 malicious downloaders,
36,801 appear at least once before VirusTotal ﬁrst seen timestamp.
In 27.1% of these cases, the dropper appears in the wild one day
before it is uploaded to VirusTotal, suggesting that many of these
malicious downloaders rouse suspicion. Nevertheless, the distribu-
tion has a long tail, with an average of 80.6 days (approximately
2.7 months) before discovery. This suggests that many downloa-
ders are able to deliver malware stealthily for several months.

These results illustrate the magnitude of the downloader threat
and the opportunities for improving malware detection. For the
empirical results presented in this section, we focus on malicious
graphs rooted in an unknown dropper, in order to minimize the
bias caused by anti-virus products that may block the growth of
the graphs.
4.2 Dynamics of Malware Delivery
Web browsers, updaters and instant messengers. To understand
how malware is delivered to end hosts, we ﬁrst analyze the pro-
grams responsible for most downloads in our data set. The top
downloaders are well-known programs, which appear in our benign
ground truth and which have valid digital signatures. For example,
we identify the top-3 browsers by searching the digital signatures
for the following <publisher, product> pairs: <Microsoft Corpora-
tion, Internet Explorer>, <Google Inc, Google Chrome>, <Mozilla
Corporation, Firefox>; we also check that the ﬁle name contains
the keywords chrome, firefox or explore. In addition to brow-
sers, the top downloaders include software updaters and Skype (an
instant messaging program).

Who Drops the Droppers? By analyzing the incoming edges
for all the malicious droppers, we determine that 94.8% of them
are downloaded using the top-3 Web browsers. This illustrates the

# of Inﬂuence Graphs (Log Scale)1102104106Number of Nodes in the inﬂuence Graph (Log Scale)101102103104105# of Inﬂuence Graphs (Log Scale)1102104106Number of Edges in the inﬂuence Graph (Log Scale)1101102103104105# of Inﬂuence Graphs (Log Scale)1102104106Life Span (i.e., age) of the Inﬂuence Graphs (days)0500100015002000time difference (before VT)time difference (after VT)Number of Roots (Log Scale)102103104Time Difference (days)−100−500501001122Downloader ﬁle name
CSRSS.EXE
EXPLORER.EXE
JAVA.EXE
DAP.EXE
OPERAUPGRADER.EXE
SVCHOST.EXE
WMPLAYER.EXE
IDMAN.EXE
CBSIDLM-CBSI145-
SUBTITLESSYNCH-ORG-10445104.EXE
MODELMANAGERSTANDALONE.EXE
KMPLAYER.EXE
JAVAW.EXE

Payloads
14801
1717
892
749
584
547
247
237
209
187
140
105

Table 2: Top benign downloaders dropping malware.

fact that download graphs may include a mix of benign and mali-
cious software and emphasizes the importance of focusing on the
inﬂuence graph rooted in each downloader in our analysis.
Benign Programs Dropping Malware. In addition to browsers,
we observe a number of benign programs that drop malicious exe-
cutables (see Table 2). These include three Windows process,
EXPLORER.EXE, CSRSS.EXE and SVCHOST.EXE. In the case of
CSRSS.EXE, the 10 most frequently downloaded executables are
adware programs (detected by several AV products); 8 out of 10
are from Mindspark Interactive Network. For EXPLORER.EXE, the
10 most frequently downloaded executables are adware programs
from Conduit, Mindspark, Funweb, and Somoto.
In the case of
SVCHOST.EXE most of top 10 payloads are generic trojan dro-
ppers. Executables downloaded from JAVA.EXE are speciﬁc tro-
jans (Zlob, Genome, Qbot, Zbot, Mufanom, Cycbot, Gbot and Fa-
keAV), and a hack tool (passview). DAP.EXE and IDMAN.EXE are
downloader managers, and the top 5 executables they drop are pro-
ducts signed by Mindspark. For JAVAW.EXE, Bitcoin mining exe-
cutables made the top of the list. Another Java related process,
MODELMANAGERSTANDALONE.EXE, is also used for dropping tro-
jans.
In many of these cases it is difﬁcult to pinpoint the exact
program that is responsible for delivering malware; for example,
SVCHOST.EXE is a process that can contain a variety of Windows
services, while JAVA.EXE is an interpreter that runs Java programs.
However, this illustrates the fact that malware programs, spanning
a broad functionality range, often try to remain undetected by inﬁl-
trating benign software ecosystems.
Signed Malicious Downloaders. Surprisingly, among the 67,609
malicious downloaders, 22.4% (15,115 downloaders) have a va-
lid digital signature. Moreover, the droppers may be uploaded to
VirusTotal after the expiration of their signing certiﬁcate; if we co-
unt the invalid signatures, 55.5% of malicious downloaders are sig-
ned. These signed malicious downloaders form 128,436 inﬂuence
graphs, accounting for 46.9% of all the malicious inﬂuence gra-
phs. The top-5 downloaders with valid signatures are from Softo-
nic International, Amonetize Ltd, InstallX, Mindspark Interactive
Network, SecureInstall. All of these downloaders are known to de-
liver third-party software. Among these, Amonetize is known to be
a pay-per-install provider5. These software publishers release 1.70,
1.55, 1.98, 0.35, and 1.79 new droppers per day, respectively. This
practice likely stems from a desire to evade detection, as benign
software is usually signed. However, this also illustrates that the
organizations responsible for almost half of the malware download
activity identify themselves voluntarily by signing their droppers.

5

http://www.amonetize.com/about-us/

4.3 Properties of Malicious Inﬂuence Graphs
We now turn our attention to the question: How do malicious in-
ﬂuence graphs differ from benign inﬂuence graphs? We compared
multiple features and feature combinations; for brevity, we report
only the strongest indicators of malicious activity.
Large diameter IGs are mostly malicious. The diameter of inﬂu-
ence graphs (the maximum length of the shortest path between two
nodes) ranges between 2–5. Figure 4(a) shows the distribution. A
graph with diameter 2 (a single downloader with multiple payloads)
is equally likely to be benign or malicious. However, when the dia-
meter is 3 and above a high percentage (84% in our case) of graphs
are malicious. More importantly, almost 12% of the malicious in-
ﬂuence graphs have diameter of 3 and larger. These ﬁndings are
consistent with prior observations of pricing arbitrage in the under-
ground economy [3], where PPI providers distribute their droppers
through competing PPI infrastructures.
IGs with slow growth rates are mostly malicious. Figure 4(b)
shows the distribution of the average inter-download time (AIT) of
the inﬂuence graphs. We deﬁne AIT to be the average amount of
time taken to grow by one node. Almost 88% of the inﬂuence gra-
phs that grow slowly (AIT > 1.5 days/node) are malicious. The ra-
tio of malicious graphs further increases with slower growth rates.
We also observe that over 65% of the malicious inﬂuence graphs
have AIT > 1.5 days/node. This suggests that successful malware
campaigns, which are able to evade detection for a long time, tend
to deliver their payloads slowly.
URL access patterns vary across subclasses of malicious/benign
downloaders. Figure 4(c) shows the distribution of the average
number of distinct downloaders accessing an Internet domain. We
compute this number by determining the set of source domains for
the nodes in an inﬂuence graph and by taking the average of the
number of downloaders accessing them, across all the hosts. Even
though the distribution does not clearly separates malicious and be-
nign behavior, we found some interesting patterns. The large nu-
mber of IGs with between 1,100–1,200 downloaders per domain,
towards the right side of the plot, is mostly caused by adware. In
fact, three adware programs, LUCKYLEAP.EXE, LINKSWIFT.EXE,
and BATBROWSER.EXE comprise 26% of the IGs in that distribu-
tion bucket. This suggests that the organizations behind these pro-
grams have resilient server-side infrastructures (and the domains
used to host the adware do not have to change very often) and
that they frequently re-pack their droppers (in order to evade de-
tection on the client side). Figure 4(d) zooms in on the left side
of the same distribution. Most of the benign IGs have up to 10
downloaders per domain. However, we also identiﬁed several ma-
licious IGs in this bucket; the top-3 are fake antivirus programs
(EASYVACCINESETUP.EXE, LITEVACCINESETUP.EXE, and BOAN-
DEFENDERSETUP.EXE), which access Korean domains and seem to
be part of the same campaign. Windows Update IGs have between
60–70 downloaders per domain in our data set.
Malware tend to download fewer ﬁles per domain. Figure 4(e)
shows the distribution of the average number ﬁles downloaded
from the domains accessed from an inﬂuence graph. The ﬁgure
also illustrates the diversity in malicious behavior. Most of the
malicious droppers that download large number of ﬁles per do-
main correspond to adware. For example, 40% of the IGs in
the 4000–5000 ﬁles-per-domain histogram bucket (the tallest mali-
cious bar) correspond to three adware programs (LUCKYLEAP.EXE,
LINKSWIFT.EXE, and BATBROWSER.EXE). Apart from the adware,
most of the other malicious droppers (around 40% of all malware)
download 1–5 ﬁles per domain, as they have to move to new do-
mains after the old ones are blacklisted. Another interesting obser-

1123Figure 4: (a) Distribution of inﬂuence graph diameter (b) Growth rate of inﬂuence graphs (expressed as the average inter-download
time) (c) Distribution of the average number distinct portals accessing the domains of an inﬂuence graph (d) Zoomed in version of
the previous plot (e) Distribution of the number of executables downloaded from the source domains of an inﬂuence graph.

vation is that benign downloaders also exhibit diverse behaviors.
For example, the inﬂuence graphs of Apple Software Update are
also in the 4000–5000 histogram bucket, while DivXInstaller’s in-
ﬂuence graphs download around 10000 ﬁles per domain.

5. MALWARE CLASSIFICATION

While in the previous section we identify several features that
indicate malicious download activity, none of these features, ta-
ken individually, are sufﬁcient for detecting most of the malware in
our data set. We therefore build a malware detection system that
employs supervised machine learning techniques for automatically
selecting the best combination of features to separate the malicious
and benign inﬂuence graphs. Speciﬁcally, we train a random-forest
classiﬁer using inﬂuence graphs labeled as described in Section 3.4;
unlike in Section 4, we employ malicious graphs that correspond to
both known and unknown droppers, as this data set is more repre-
sentative of the state of malicious downloaders in the wild. We test
our classiﬁer using both internal (cross-fold validation and early
detection) and external (VirusTotal results for some of the unlabe-
led samples predicted to be malicious) performance metrics.
5.1 Handling Data Skew

The ground data consists of 274,126 malicious and 14,918,097
benign inﬂuence graphs. Training a classiﬁer on such a skewed data
set may bias the model toward the abundant class (the benign IGs)
and may focus on tuning the unimportant features, i.e., the ones
that do not contribute to the classiﬁcation but rather model noise in
the dataset. We address this problem through stratiﬁed sampling:
we sample the abundant and rare classes separately to select appro-
ximately equal numbers of IGs for the training set. In practice, we
do not need to exclude any examples from the rare class, and some
examples from the abundant class can be identiﬁed easily and ﬁlte-
red to reduce the variability within that class. We therefore ﬁlter out
all the Web browsers (identiﬁed as described in Section 4.2) from
the benign set of IGs, as the set of ﬁles they download is not predic-
table and includes both benign and malicious executables. We then
keep all the malicious graphs and we downsample the remaining
set of benign graphs (sampling uniformly at random). We manu-
ally examined the properties of several random samples created in

this manner and observed that they closely match the properties of
the abundant class. Our balanced training set consists of 43,668
malicious and 44,546 benign inﬂuence graphs.
5.2 Feature Engineering

Table 3 provides the features that we compute based on the pro-
perties of the inﬂuence graphs. We organize the features into ﬁve
semantic categories:
internal dynamics, life cycle, properties of
downloaders, properties of domains, and globally aggregated be-
havior. For each feature, we also provide the high level intuition
for why we expect it would separate malicious and benign graphs.
Depending on how we compute the features, we distinguish two
feature types. Local features (LF) use information contained in the
inﬂuence graph. Global features (GF) are also computed for each
inﬂuence graph; however, they use properties aggregated across all
the hosts. An example of a GF is the Average Distinct File Afﬁnity,
illustrated in Figure 4(e), which reﬂects the tendency of an inﬂuence
graph to download ﬁles from domains that are known to serve a
large number of distinct ﬁles.

We quantify the worth of each feature in terms from distingu-
ishing benign and malicious inﬂuence graphs using the gain ratio6
with 10-fold cross validation. We select this metric because it is
known to be more robust than alternative metrics, such as the infor-
mation gain or the Gini index, when the features differ greatly with
respect to their range [21]. Table 4 shows a summary of the top-10
features in descending order of their gain ratio. The most useful fe-
atures are the average ﬁle prevalence and the features illustrated in
Figures 4(c)–(e). We emphasize that, because the features are com-
puted per inﬂuence graph, the power of these global features helps
classify all the downloaders in the graph. For example, a dropper
that normally evades detection because it is present on many hosts,
but that always downloads unique ﬁles, will likely be classiﬁed as
malicious because of the low average prevalence of the ﬁles in its
inﬂuence graph.
5.3 Choosing the Classiﬁer

Our data set presents several challenges for supervised machine
learning. Some classiﬁcation algorithms work best on data sets
6

http://www.csee.wvu.edu/~timm/cs591o/old/Lecture6.html

BenignMalicious% of Inﬂuence Graphs0.011.00100.00Diameter2345% of Inﬂuence graph Histogram bucket size = 16000 minutesMaliciousBenign0.011.00100.00Rate of Inﬂuence Graph growth (minutes/nodes)01234×105% of Inﬂuence GraphsBenignMaliciousMostly AdwareHistogram Bucket width = 10002040Average #distinct portals accessing an URL050010001500MaliciousBenign% of Inﬂuence GraphsWindows UpdateHistogram Bucket width = 10Fake AVs and more02040Average #distinct portals accessing an URL - Zoomed In050100150200250% of Inﬂuence GraphsMaliciousBenignHistogram Bucket width = 500DivXInstaller.exeApple software updateMostly AdwareGabpath Adware3+ Adware02040Average #Files per URL for Inﬂeunce Graphs05000100001124Categories

Name
f1. Diameter

Type
LF

Internal
Dynamics

(FI)

Domain
Properties

(FU)

Downloader

(FD)

Score

Properties

Life
Cycle
(FL)

Gloablly
Aggregated
Behavior

(FA)

f2. Clustering Coefﬁ-
cient

f3. Density

f4. Total Download
f5. Number of Uni-
que Domains
f6. Domain Name Si-
milarity

f7. Alexa top-1M
f8. Average Score

f9. Standard Devia-
tion of the Score
f10. Inﬂuence Graph
Life Span
f11. Growth Rate

Average File

f12. Children Spread
f13.
Intra-children
Time Spread
f14.
Prevalence
f15. Average Distinct
File Afﬁnity
f16. Average Distinct
Dropper Afﬁnity

LF

LF

LF
LF

LF

LF
LF

LF

LF

LF

LF
LF

GF

GF

GF

Explanation

Diameter capture a chain of download relations (e.g., A → B, B → C, and so on). High diameter
could imply malicious behavior such as droppers or Pay-per-install ecosystem where there is an afﬁliate.
Clustering coefﬁcient is a measure of the degree to which nodes in a graph tend to cluster together, i.e.,
create triangles (e.g., (A → B, A → C, B → C ) or (A → B, B → C, C → A)). Intuitively, we would
expect the benign software to show low clustering as compared to malware.
A graph with high density means that the binaries are downloading each other actively, and there are
binaries that are downloaded by multiple downloaders.
Total number of downloads made by the inﬂuence graph.
Some malware droppers may access more domains, e.g. if they employ domain generation algorithms to
bypass blacklists.
We compute the similarity between all pairs of domains accessed from the graph:

similarity = 1 −

EditDistance(D1, D2)

min(length(D1), length(D2))

Percentage of domains in the inﬂuence graph that appear in Alexa top 1 Million list
Average score (based on signatures—see Section 3.4) of all the downloaders in an inﬂuence graph. In-
tuitively, even if the root has high score (signed malware) it might download low score downloaders,
indicating that the root might be malicious.
A malicious inﬂuence graph can achieve high average score by downloading known high score downloa-
ders. Standard deviation of downloader score in the IG might be relatively robust in that regard.
The life span of an inﬂuence graph is deﬁned as the time interval between the newest and oldest node. The
life span of malicious IGs tends to be shorter, as A/V programs eventually start detecting the droppers.
Average inter-download time for the nodes in an inﬂuence graph (= IGlif espan
#N odes ). Malicious IG trying
to remain stealthy tend to grow slowly, as shown in Figure 4(b).
Average time difference between the root and the children of an inﬂuence graph.
Average difference in download timestamp among the children of the root. Intuitively, we would expect
this value to be smaller for malware, as they tend to be more aggressive as downloaders.
Average FP (see Section 3.4) of the executables that appear in an inﬂuence graph. Benign binaries are
expected to show high prevalence.
Intuitively, this depicts whether an inﬂuence graph tends to prefer/avoid domains that download less/more
distinct binaries. This feature is illustrated in Figure 4(e).
Intuitively, this depicts the bias of a dropper toward a speciﬁc set of domains in order download new
binaries. This feature is illustrated in Figures 4(c)–(d).

Table 3: Feature categories and the high-level intuition behind some of the important features

Features

f14. Avg. File Prevalence
f15. Avg. Distinct File Afﬁnity
f16. Avg. Distinct Dropper Afﬁnity
f10. IG Life Span
f7. Alexa Top-1M
f11. Growth Rate
f13. Children Spread
f1. Diameter
f12. Intra-children Time Spread

Gain Ratio
.16 ± 0
.16 ± 0
.12 ± 0
.1 ± 0
.1 ± 0
.09 ± 0
.08 ± 0
.06 ± 0
.06 ± 0

Avg. Rank
1.1 ± .3
1.9 ± .3
3 ± 0
4.3 ± .64
5 ± 0.5
5.7 ± 0.6
7.4 ± .66
8.5 ± 1.1
11.4 ± 1.1

Table 4: Gain ratio of top 10 features of inﬂuence graphs.

with low variability and few outliers [25]. These assumptions may
not hold true for download graphs, for several reasons:
• Fitting multiple malware types in a two-class model: The subcla-
sses of malicious and benign IGs have different properties (e.g.,
the average distinct ﬁle afﬁnity, illustrated in Figure 4(e)). From
the point of view of a classiﬁer trying to label each example as
benign or malicious, this means that the training data has high
variability or many outliers. It is also difﬁcult to establish how
many subclasses of malware exist. Moreover, one subclass of
benign downloaders may exhibit similar properties of that of a
subclass of malicious downloaders (e.g., software components
that update themselves), confusing the classiﬁer even further.
• Inﬂuence graph evolution: Inﬂuence graphs evolve over time.
Depending on when a downloader was dropped on different ma-
chines, the corresponding inﬂuence graphs may be different. For
example, a malicious IG might initially have diameter 2, and the
value of this feature might increase as the graph grows.
• Cross-machine behavioral differences: Even for similar ages, the
inﬂuence graphs of a downloader may depend on the host envi-

ronment and user behavior. In fact, downloaders may also en-
code host-speciﬁc download instructions [3].
Given these properties, the classes of malicious and benign in-
ﬂuence graphs may not be linearly separable in the feature space,
which raises a challenge for Support Vector Machine (SVM) cla-
ssiﬁers. Additionally, we want our classiﬁer to be scalable and
parameter-free, as tuning parameters for such diverse data could be
difﬁcult. After experimenting with several algorithms, including
SVM and k-nearest neighbor (kNN), we found that ensemble clas-
siﬁcation algorithms provide the best ﬁt for our problem. We select
a random forest classiﬁer (RFC) [2], for the following reasons:
• Bias-variance trade-off: RFCs are known to provide a good bias-
variance trade-off, especially if the data is diverse (i.e., noisy
from the point of the view of a learning algorithm). An RFC
consists of a set of decision trees (DT), where each DT is trained
using only a subset of features (randomly chosen). The ﬁnal
class prediction is made by aggregating the votes from all the
individual DTs. Thus, an RFC is less prone to overﬁtting as
compared to a single DT. In other words, the bias of the entire
forest is bounded by the bias of the individual DTs [2]. Moreo-
ver, since the class predication is made by aggregating the votes
from a number of smaller decision trees, RFCs exhibit low vari-
ance as well. Intuitively, we can expect that different DTs will
specialize on the different subclasses of the malicious/benign bi-
naries and champion their cause in the ﬁnal classiﬁcation.
• Parameter-free: RFCs do not require parameter tuning7 and are
robust to outliers. While we need to specify the number of deci-

7In contrast, SVM requires selecting: (a) the kernel, (b) the kernel’s pa-
rameters, (c) a soft margin parameter that encodes the penalty factor for
non-separable points.

1125sion trees (Nt) used and the number of features (Nf ) per deci-
sion tree, these parameters are independent of the nuances of the
data and have standard selection rules8. Moreover, they exhibit
monotonicity and diminishing returns, i.e., increasing them leads
to an increase in accuracy but only to a certain limit.
• Scalability: Random forest classiﬁers are very fast to train, com-
pared to SVM and kNN. Our experiments show that training an
RFC on the same dataset is 10-15 times faster than SVM.

5.4

Internal Validation of the Classiﬁer

In this section we evaluate the performance of our RFC classiﬁer
in three ways: (a) using 10-fold cross validation on the balanced
training dataset, (b) using all the labeled data as a test set, and (c)
computing the detection lead time, compared to the anti-virus pro-
ducts employed by VirusTotal.
10-fold Cross Validation. We choose Nt = 40 and Nf =
log2(# of features) + 1, for our experiments. We observe that in-
creasing Nt and Nf beyond these numbers results in very small
improvement in accuracy; however, the training cost goes up sig-
niﬁcantly. We use the RandomForestClassifier module from
Python’s scikit-learn package for the experiment. We run a
10-fold cross validation on the balanced set. We use all the features
described on Table 3 for training the classiﬁer. We report the clas-
siﬁcation result at 1.0% FP rate, which achieves 96.0% TP rate9.

These results with a default scikit-learn threshold are listed

in the ﬁrst row of Table 5, labeled “All Features”.
Feature Evaluation. While in Table 4 lists the most useful features
for our classiﬁer, we also want to know how using different feature
combinations would affect the performance of the classiﬁer. Star-
ting from only using FI features from Table 3, we combine other
feature categories one by one (FL, FD, FU, then FA) and evaluate
how the classiﬁcation performance increases. The result is shown
as a Receiver Operating Characteristic (ROC)10 plot in Figure 5,
where the X axis is the false positive rate and the Y axis is the
true positive rate. ROC plots show the TP/FP trade-off: the top-
left corner corresponds to a perfect classiﬁer, which never makes
mistakes. The curves for the different feature combinations are ob-
tained by varying internal thresholds of the random forest classiﬁer.
We observe large jumps in performance at two points, ﬁrst is when
we add the FL features and them when we add the FA features.
We believe the classiﬁer is capturing the insights we discussed in
Section 4.3, such as the slow growth rate and speciﬁc numbers of
distinct downloaders accessing a domain for the malicious inﬂu-
ence graphs. At FP rate 1%, the corresponding TP rates are 19.9%,
13.4%, 3%, 17.6%, 28.5% and 96.0%. Interestingly, only using FI
rate was performing better than adding FL and FS at this point. This
is consistent with our observation that almost 12% of the malicious
IGs have diameter 3 and more and 84% of the IGs are malicious at
diameter 3 and beyond (Section 4.3). At FP rate 3%, the numbers
for TP rate are 24.7%, 33.5%, 42.1%, 59.6%, 98.7%. At 10%, TP
rates are 41.7%, 64.5%, 71.2%, 84.8%, and 99.8%.
Evaluation of the False Positives. We perform a manual inves-
tigation of the misclassiﬁed IGs. We take the average of all the
feature values, for the TP, TN, FP sets and compared the difference
between <TP, FP> and <TN, FP>, to see which feature is closer
to the TP set. The number of distinct ﬁles downloaded by a sin-
gle URL, density, distribution of the number of incoming/outgoing
edges, prevalence of the executables, and the children time spread
8Nt is typically data size independent and few applications go beyond 100
trees. For Nf , standard value used is Log(Total number of features) + 1
F P +T N , F1 = 2 ∗ P recision∗Recall
9
T Prate = T P
P recision+Recall
http://en.wikipedia.org/wiki/Receiver_operating_characteristic

T P +F N , F Prate = F P

10

Figure 5: ROC curve for different feature groups

Algorithms
All Features
FI+FL+FD+FU
FI+FL+FD
FI+FL
Only FI

TP rate
0.980
0.868
0.831
0.811
0.602

FP rate
0.020
0.124
0.184
0.211
0.261

F-score
0.980
0.870
0.823
0.801
0.644

ROC Area
0.998
0.939
0.902
0.876
0.752

Table 5: Classiﬁer performance on malicious class.

features are showing closer value to the TP set. The IGs in the
FP set have a smaller number of ﬁles downloaded per URL, den-
ser structure, smaller number of outgoing edges, lower prevalence,
and faster dropping rate. Most prevalent downloaders in this TP set
turned out to be P2P downloaders and download managers.
Testing the Classiﬁer on the Entire Labeled Data. We apply the
RFC trained on the balanced labeled data to the entire labeled data,
which is skewed toward the benign class. There are 1,433,670 IGs
in this set, where 43,668 are malicious and 1,390,092 are benign.
The classiﬁcation result showed 100% TP rate and 2% FP rate on
the malicious class. From the ROC curve, we get 91.8% TP rate at
FP rate 1%. We also got 99% G-mean11score [12] which is a stan-
dard way of measuring the classiﬁcation accuracy on unbalanced
data where both classes are important [26].
Early Detection. We also evaluate how early we can detect mali-
cious executables that are previously unknown. We deﬁne “early
detection” as “we are able to ﬂag unknown executables as mali-
cious before their ﬁrst submission to VirusTotal”. As discussed in
Section 4.1, we approximate the date when malware samples be-
comes known to the security community using the VirusTotal ﬁrst-
submission time. We estimate our detection time in three ways:
(a) an executable is detected at its earliest timestamp in the TP set
(Lower bound), (b) an executable is detected at the timestamp when
the last node was added to the newest inﬂuence graph it resides as
a node (Upper bound) , and (c) the average timestamp of the exe-
cutables in the TP set (Average).

We apply random forest with 10 fold cross validation on our
balanced labelled data set using all 58 features. The outcome
of 10 fold cross validation is TP=42,683, FP=822, FN=985 and
TN=43,724, in terms of the number of inﬂuence graphs. For dis-
tinct executables in the TP set excluding the ones in the FN set, we
compared the ﬁrst seen timestamps in VirusTotal and our lower-
bound/upper-bound/average detection timestamps. Among 31,104
distinct executables that are in TP set but not in FN set, 20,452 ﬁles
had scanning records in VirusTotal. Among them, 17,462 executa-
bles had at least one detection in VirusTotal (rmal > 0), and 10,323
executables had detection rates over 30% (rmal ≥ 30).
11G − mean =

T P × T N

√

All featuresFI+FL+FD+FUFI+FL+FDFI+FLFITrue positive rate0.10.20.30.40.50.60.70.80.91.0False positive rate0.10.20.30.40.50.60.70.80.91.01126rmal > 0
Distinct Executables
Early detection avg.
rmal ≥ 30
Distinct Executables
Early detection avg.

3,344 (19.2%)
-23.73

Lower bound Upper bound Average
6,515 (37.3%)
20.91
Lower bound Upper bound Average
3,939 (38.2%)
35.86

2,041 (19.8%)
-7.69

4,871 (27.9%)
9.24

3,002 (29.1%)
25.24

Table 6: Early detection.

Figure 6: Detection rate vs. Early detection ratio / Early detec-
tion avg. (days)

Table 6 lists our results. For rmal > 0, the time difference be-
tween the VirusTotal ﬁrst seen timestamp and our lower bound de-
tection timestamp is 20.91 days on average, and we are able to
detect 6,515 executables earlier than VirusTotal. Our upper bound
is 23.73 after VirusTotal, with 3,344 executables detected early. On
average, we detect malware 9.24 days before the ﬁrst VirusTotal
detection. Interestingly, for rmal ≥ 30, we detect malware on ave-
rage 25.24 days earlier than VirusTotal. We further investigate this
trend in Figure 6, where we plot the portion of executables that we
are able to detect early, in our average detection scenario, against
the VirusTotal detection rate rmal. Up to rmal = 80%, our early
detection lead time increases with rmal. This suggests that exe-
cutables that are more likely to be malicious present stronger indi-
cations of maliciousness in their downloader graphs, allowing our
classiﬁer to detect them earlier than current anti-virus products.
5.5 External Validation of the Classiﬁer

In this section, we evaluate the performance of our RFC classi-
ﬁer by drawing three random samples from the unlabeled IGs and
querying the corresponding ﬁle hashes in VirusTotal. Out of the
580,210 unlabeled IGs, our classiﬁer identiﬁes 116,787 as mali-
cious.

As discussed in Section 3.2, we were unable to query VirusTotal
for all the ﬁle hashes, which leaves many of the leaf nodes in our
graphs unlabeled. We draw three random samples, of approxima-
tely 3000 inﬂuence graphs each, from the set of unlabeled graphs
and try to estimate the accuracy of our predictions by presenting
the results of each set. We consider that an IG labeled as malicious
is misclassiﬁed if none of the AV products in VirusTotal detect it as
malicious. We consider that an IG labeled as benign is misclassi-
ﬁed if its nodes were detected by more than 30% of the AV vendors,
to account for the fact that AV products may also produce false po-
sitives in gray-area situations, such as benign executables that are
sometimes involved in malware delivery.

Table 7 shows our results. On average 41.41% of the binaries
that construct the IGs are known to be malicious also by other AV
vendors, while only 0.53% of the binaries in benign IGs were labe-
led as malicious. Moreover, on average 78% of the IGs labeled as
malicious have at least one internal node that AV products detect
as malware, and only 1.58% of the IGs labeled as benign carry a

malicious node. As many malware samples are discovered late by
the security community (see Section 4.1), we expect that the num-
ber of samples considered malicious by VirusTotal will grow in the
future.

#Run

Predictions

#IGs

#1

#2

#3

Malicious
Benign
Malicious
Benign
Malicious
Benign

582
2456
590
2454
592
2495

by

#IGmal
VT
444 (76%)
43 (1.7%)
471 (80%)
30 (1.2%)
466 (79%)
44 (1.7%)

#Binarymal
VT
1093(41%)
60 (0.5%)
1249 (43%)
38 (0.3%)
1041 (41%)
67 (0.6%)

by

Table 7: Testing Classiﬁer on the Unlabeled Inﬂuence Graphs.
5.6 Online Detection Experiment

Finally, we perform an experiment where we simulate the way
our classiﬁer would be employed operationally, for detecting ma-
lware in an online manner. We prepare the training set of 21,543
malicious and 21,755 benign IGs from the data before 2014. For
the testing set, we build IGs based on the data from the year 2014.
There are 12,299 malicious and 12,594 benign IGs for the test set.
Since we are trying online detection, we assume here that the data
from 2014 may not had enough time to collect the prevalence of the
executables, so we disregard the prevalence features for the classi-
ﬁcation. Note that prevalence is one of the top features that perform
well in the classiﬁcation. We train the RFC on the training set, with
the same parameters used on section 5.4. Then we apply the trained
classiﬁer on the testing set. We obtain 99.8% TP rate, 1.9% FP rate
and 0.2% FN rate, with 99.0% F1-score. This suggests that our fe-
atures do not exhibit a signiﬁcant concept drift over time and show
the potential for using our classiﬁer as a robust online detection
technique.
6. DISCUSSION

We now discuss the lessons learned from our experiments, focu-
sing on the implications for malware detection and for attack attri-
bution.
Opportunity for Improving Malware Detection. Our research
represents a ﬁrst step towards understanding the properties of
downloader graphs in the wild. We demonstrate that some of these
properties, such as the large graph diameter, high growth interval
of the inﬂuence graphs, large number of distinct droppers acces-
sing a domain, represent strong indication of malicious activities.
This insight can lead to deterministic detection techniques that help
existing anti-virus products to block certain classes of malware in
early phases of their lifecycle. The intuition behind these beneﬁts
is that the properties of downloader graphs can provide evidence of
malicious activity before new malware samples can be investigated
by the security community. Moreover, our results highlight the be-
neﬁts of incorporating downloader graph features into probabilistic
detection techniques along with existing host-based and network-
based features. The information extracted from downloader graphs
complements the existing approaches as it (a) captures the client-
side activity of malware delivery networks, (b) reﬂects the relation-
ships among malware families, and (c) helps increase the detection
performance and reduce the detection latency. We note that our te-
chniques operate on end hosts. Although the analysis of network
trafﬁc may indicate when downloading is in progress [10, 30], it
cannot determine which executable triggers the download; thus it
may not be possible to construct a complete downloader graph.
Blocking Malicious Droppers. Our results raise the question, Sho-
uld we require user approval for all downloads of executable pro-

Average Early Detection (days)Early Detection Rate (%)Avg. Early Detection (days)Early Detection Rate (%)0204022242628303234VirusTotal Detection Rate (%)0501001127grams? If an operating system, or an anti-virus program, quaranti-
nes every executable downloaded and presents a user with a dialog
to ask for the user’s approval, the operation of malicious downloa-
ders would be severely impaired. Unfortunately, this approach wo-
uld also harm security by inhibiting the deployment of security pat-
ches, as manual or semi-automated software updating approaches
are considerably less effective than silent updates which download
and install security patches without user interactions [6, 16]. A
more effective approach would be to require digital signatures for
programs that download other executables, as is currently done for
device drivers [15]. This approach would also be more efﬁcient
than the attempts to whitelist all benign software [24], because only
a few benign programs (87,906 in our data set) download other exe-
cutables.
Implications for Attack Attribution. Attack attribution to iden-
tify attackers is generally considered as a hard problem because
attackers may employ various methods to conceal their identi-
ties, e.g., obfuscating or re-packing binaries, changing the URLs
and domains of servers, launching attacks from geographically-
distributed compromised machines. However, 55.5% of malicious
downloaders are signed, accounting for 73.8% of the malicious in-
ﬂuence graphs. For example, one PPI provider consistently speci-
ﬁes “Amonetize LTD” in the publisher ﬁeld of the digital signature.
Attackers may distribute signed programs to avoid raising suspi-
cion and to remain undetected for possibly longer periods of time.
This suggests that, for the miscreants operating malware dissemi-
nation networks, the beneﬁts of being able to remain stealthy for
a while outweigh the beneﬁts of thwarting attribution efforts. Si-
milarly, attackers may transmit malicious payloads over the HTTP
protocol, which is one of the most common trafﬁc crossing orga-
nizations network perimeters, to go around ﬁrewall restrictions. In
fact, this has facilitated several efforts to proﬁle the organizations
involved in malware distribution [3, 8, 17]. Our detection approach
based on the downloader graph analytics may force attackers to em-
ploy stealthier techniques to download malicious payloads, such as
utilizing custom ports and protocols, but these techniques are more
likely to raise alarms in ﬁrewalls and intrusion-prevention systems.
Limitation. Droppers with rootkit functionality would evade our
technique. However, rootkits are typically employed to hide more
incriminating functionality, while our technique relies mainly on
the ability to track downloader-payload relationships. This provi-
des a new signal, complementary to current AV engines, and our
experiments suggest that it can detect malware that are not curren-
tly being detected by existing AVs.
7. RELATED WORK

Several research efforts have focused on characterizing the pro-
perties of malware delivery networks and on taking advantage of
these properties for detecting malware. Provos et al. [20] described
drive-by-download attacks caused by exploiting vulnerabilities in
web browsers, analyzed the tree-like structure of redirection paths
leading to the main malware distribution sites, and identiﬁed seve-
ral mechanisms to inject malicious web content into popular pages
(e.g., comments on blogs or syndication in Ad serving networks).
Li et al. [14] further analyzed URL redirection graphs and identi-
ﬁed a set of topologically dedicated malicious hosts (e.g., Trafﬁc
Distribution Systems) that are long-lived and receive trafﬁc from
new attack campaigns over time. Perdisci et al. [19] analyzed the
structural similarities among malicious HTTP trafﬁc traces which
include a variety of activities, such as receiving commands from
C&C servers, sending spam, exﬁltrating private data, and downloa-
ding updates. Xu et al. [31] ﬁngerprinted several types of malicious
servers, including exploit servers (for malware distribution through

drive-by downloads), C&C servers (for command and control), re-
direction servers (anonymity), and payment servers (for moneti-
zation).
Inspired by these studies, several techniques were pro-
posed for detecting malware download events from network tra-
fﬁc [10, 30]. Unlike our work, these studies focus on the server
side of malware distribution networks, and they are unable to iden-
tify which program triggered the download, and cannot be used to
reconstruct complete downloader graphs.

Prior research on the behavior of downloaders [3, 17, 23] focu-
sed on executing malware droppers in lab environments, typically
for short periods of time (e.g., up to one hour), in order to observe
the communication protocols they employ and to milk their server-
side infrastructures (i.e., to download the payloads). For example,
Caballero et al. [3] described pay-per-install infrastructures, which
distributed malware on behalf of their afﬁliates, and analyzed their
structure and business model. They also provided an example of
a download tree, which reached diameter 4 within 10 minutes.
Follow-up work reported that downloaders might remain active for
over 2 years [23], and characterized several malware distribution
operations [17]. In contrast, we analyze malicious droppers that re-
main undetected for 80.6 days on average in the wild. We compare
the properties of malicious and benign downloader graphs, in order
to assess their real impact on end-user security and the potential
beneﬁts of downloader graph analytics. This approach allows us
to determine that some features, such as high graph diameter, slow
growth rate of the inﬂuence graphs, large number of distinct portal
accessing a domain are strong indication of malicious behavior, and
to train a generic classiﬁer for malware detection using information
extracted from downloader graphs.

Closest to our work are recent techniques for assigning reputa-
tion scores to executable ﬁles by performing belief propagation on
bipartite graphs [5, 28]. Chau et al. [5] constructed a graph that
encoded the relationship between ﬁles and the hosts they are pre-
sent on, building on the intuition that hosts with poor cyber-hygiene
tended to contain more malware. Tamersoy et al. [28] constructed
a graph that encoded the relationship between different ﬁles that
were present on the same host, building on the intuition that se-
veral malware samples were often distributed together (guilt-by-
association). In our work, we construct graphs that encode a se-
mantic relationship between ﬁles—the fact that one ﬁle downloads
another ﬁle—which can provide deeper insights into malware dis-
tribution activities.

There is a rich literature on graph mining techniques [4]. We cite
here the Oddball approach [1], which consists of extracting features
from the k-hop neighborhoods of every node, identifying patterns
for these features and analyzing the outliers that do not seem to
conform to these patterns, for its similarity with the technique we
employ in Section 4. Graph analytics have also been employed
for analyzing function call graphs in malware samples [9, 11, 33],
spamming operations [32, 34], and vote gaming attacks [22].
In
contrast to these approaches, we propose downloader graph analy-
tics for analyzing the malware delivery activities on the client side.

8. CONCLUSIONS

We analyze downloader graphs in the wild and uncover the di-
fferences in growth patterns between benign and malicious gra-
phs. Because downloader graphs capture the relationships between
downloaders and the supplementary executables they download, we
can identify large parts of the malware download activity on each
host by analyzing the upstream download chain. We identify deter-
ministic techniques for detecting certain classes of malware based
on properties of the downloader graphs, including their growth rate,
diameter and the diversity in the set of Internet domains accessed.

1128We also describe a generic malware detection system, which uses
machine learning techniques for automatically learning models of
malicious download graphs. We evaluate our system on 19 million
graphs and show that it detects malware with high accuracy and
earlier than existing anti-virus systems.
Acknowledgments
We thank Amol Deshpande, Jonathan Katz, and Michel Cukier for
early feedback. We also thank VirusTotal for access to their service
and Symantec for access to WINE (the data analyzed in this paper
corresponds to the reference data set WINE-2015-002). This re-
search was partially supported by the Maryland Procurement Ofﬁce
(contract H98230-14-C-0127) and by the Department of Defense.
9. REFERENCES
[1] L. Akoglu, M. McGlohon, and C. Faloutsos. Oddball:
Spotting anomalies in weighted graphs. In KDD. 2010.

[2] L. Breiman. Random forests. 2001.
[3] J. Caballero, C. Grier, C. Kreibich, and V. Paxson.

Measuring pay-per-install: The commoditization of malware
distribution. In USENIX Security Symposium, 2011.

[4] D. Chakrabarti and C. Faloutsos. Graph mining: Laws,

generators, and algorithms. ACM Computing Surveys
(CSUR), 2006.

[5] D. H. Chau, C. Nachenberg, J. Wilhelm, A. Wright, and

C. Faloutsos. Polonium: Tera-scale graph mining and
inference for malware detection. In SDM, 2011.

[6] T. Dübendorfer and S. Frei. Web browser security update

effectiveness. In CRITIS Workshop, September 2009.

[7] T. Dumitras, and D. Shou. Toward a standard benchmark for

computer security research: The Worldwide Intelligence
Network Environment (WINE). In EuroSys BADGERS
Workshop, Salzburg, Austria, 2011.

[8] C. Grier, L. Ballard, J. Caballero, N. Chachra, C. J. Dietrich,

K. Levchenko, P. Mavrommatis, D. McCoy, A. Nappa,
A. Pitsillidis, N. Provos, M. Z. Raﬁque, M. A. Rajab,
C. Rossow, K. Thomas, V. Paxson, S. Savage, and G. M.
Voelker. Manufacturing compromise: the emergence of
exploit-as-a-service. In CCS, 2012.

[9] X. Hu, T. Chiueh, and K. G. Shin. Large-scale malware

indexing using function-call graphs. In CCS, 2009.

[10] L. Invernizzi, S.-J. Lee, S. Miskovic, M. Mellia, R. Torres,

C. Kruegel, S. Saha, and G. Vigna. Nazca: Detecting
Malware Distribution in Large-Scale Networks. In NDSS,
2014.

[11] D. Kong and G. Yan. Discriminant malware distance

learning on structural information for automated malware
classiﬁcation. In KDD, 2013.

[12] M. Kubat, R. C. Holte, and S. Matwin. Machine learning for
the detection of oil spills in satellite radar images. Machine
learning, 1998.

[13] N. Leontiadis, T. Moore, and N. Christin. A Nearly

Four-Year Longitudinal Study of Search-Engine Poisoning.
In CCS, 2014.

[14] Z. Li, S. Alrwais, Y. Xie, F. Yu, and X. Wang. Finding the

linchpins of the dark web: a study on topologically dedicated
hosts on malicious web infrastructures. In IEEE S&P, 2013.

[15] Microsoft. Driver signing policy.

https://msdn.microsoft.com/en-us/library/
windows/hardware/ff548231(v=vs.85).aspx.

[16] A. Nappa, R. Johnson, L. Bilge, J. Caballero, and

T. Dumitras,. The Attack of the Clones: A Study of the

Impact of Shared Code on Vulnerability Patching. In IEEE
S&P, San Jose, CA, 2015.

[17] A. Nappa, M. Z. Raﬁque, and J. Caballero. The MALICIA

dataset: identiﬁcation and analysis of drive-by download
operations. IJIS, 2015.

[18] A. Nappa, Z. Xu, M. Z. Raﬁque, J. Caballero, and G. Gu.

Cyberprobe: Towards internet-scale active detection of
malicious servers. In NDSS. The Internet Society, 2014.

[19] R. Perdisci, W. Lee, and N. Feamster. Behavioral clustering

of http-based malware and signature generation using
malicious network traces. In NSDI, 2010.

[20] N. Provos, P. Mavrommatis, M. A. Rajab, and F. Monrose.

All Your iFRAMEs Point to Us. In USENIX Security
Symposium, 2008.

[21] J. R. Quinlan. Induction of decision trees. Machine learning,

1986.

[22] A. Ramachandran, A. Dasgupta, N. Feamster, and

K. Weinberger. Spam or ham?: characterizing and detecting
fraudulent not spam reports in web mail systems. In CEAS,
2011.

[23] C. Rossow, C. Dietrich, and H. Bos. Large-scale analysis of

malware downloaders. In DIVMA. 2013.

[24] A. Sedgewick, M. Souppaya, and K. Scarfone. Guide to

application whitelisting. Technical Report Special
Publication 800-167 (Draft), National Institute of Standards
and Technology, 2014.

[25] R. Sommer and V. Paxson. Outside the closed world: On
using machine learning for network intrusion detection. In
IEEE S&P, 2010.

[26] Y. Sun, A. K. Wong, and M. S. Kamel. Classiﬁcation of

imbalanced data: A review. IJPRAI, 2009.

[27] Symantec. W32.Sobig.F.

http://www.symantec.com/security_response/
writeup.jsp?docid=2003-081909-2118-99, 2003.

[28] A. Tamersoy, K. Roundy, and D. H. Chau. Guilt by

association: large scale malware detection by mining
ﬁle-relation graphs. In KDD, 2014.

[29] G. Tenebro. The Bredolab Files. Symantec Whitepaper.

http://securityresponse.symantec.com/content/
en/us/enterprise/media/security_response/
whitepapers/the_bredolab_files.pdf, 2009.

[30] P. Vadrevu, B. Rahbarinia, R. Perdisci, K. Li, and

M. Antonakakis. Measuring and Detecting Malware
Downloads in Live Network Trafﬁc. In ESORICS, 2013.
[31] Z. Xu, A. Nappa, R. Baykov, G. Yang, J. Caballero, and

G. Gu. AUTOPROBE: Towards Automatic Active Malicious
Server Probing Using Dynamic Binary Analysis. In CCS,
2014.

[32] C. Yang, R. C. Harkreader, and G. Gu. Die Free or Live

Hard? Empirical Evaluation and New Design for Fighting
Evolving Twitter Spammers. In RAID, 2011.

[33] M. Zhang, Y. Duan, H. Yin, and Z. Zhao. Semantics-aware

android malware classiﬁcation using weighted contextual api
dependency graphs. In CCS, 2014.

[34] Y. Zhao, Y. Xie, F. Yu, Q. Ke, Y. Yu, Y. Chen, and E. Gillum.

BotGraph: Large Scale Spamming Botnet Detection. In
NSDI, 2009.

[35] C. C. Zou and R. Cunningham. Honeypot-aware advanced

botnet construction and maintenance. In DSN, 2006.

1129