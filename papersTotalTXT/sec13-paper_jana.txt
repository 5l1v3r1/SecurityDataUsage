Enabling Fine-Grained Permissions for Augmented 

Reality Applications with Recognizers

Suman Jana, The University of Texas at Austin;  

David Molnar and Alexander Moshchuk, Microsoft Research;  

Alan Dunn, The University of Texas at Austin;  

Benjamin Livshits, Helen J. Wang, and Eyal Ofek, Microsoft Research

Open access to the Proceedings of the 22nd USENIX Security Symposium is sponsored by USENIXThis paper is included in the Proceedings of the 22nd USENIX Security Symposium.August 14–16, 2013 • Washington, D.C., USAISBN 978-1-931971-03-4Enabling Fine-Grained Permissions for Augmented Reality

Applications With Recognizers

Suman Jana1, David Molnar2, Alexander Moshchuk2, Alan Dunn1, Benjamin Livshits2,

Helen J. Wang2, and Eyal Ofek2

1University of Texas at Austin

2Microsoft Research

Abstract

1 Introduction

Augmented reality (AR) applications sense the en-
vironment, then render virtual objects on human
senses. Examples include smartphone applications
that annotate storefronts with reviews and XBox
Kinect games that show “avatars” mimicking human
movements. No current OS has special support for
such applications. As a result, permissions for AR
applications are necessarily coarse-grained : applica-
tions must ask for access to raw sensor feeds, such
as video and audio. These raw feeds expose signif-
icant additional information beyond what applica-
tions need, including sensitive information such as
the user’s location, face, or surroundings.

Instead of exposing raw sensor data to applica-
tions directly, we introduce a new OS abstraction:
the recognizer. A recognizer takes raw sensor data
as input and exposes higher-level objects, such as
a skeleton or a face, to applications. We propose
a ﬁne-grained permission system where applications
request permissions at the granularity of recognizer
objects. We analyze 87 shipping AR applications
and ﬁnd that a set of four core recognizers covers
almost all current apps. We also introduce privacy
goggles, a visualization of sensitive data exposed to
an application. Surveys of 962 people establish a
clear “privacy ordering” over recognizers and demon-
strate that privacy goggles are eﬀective at commu-
nicating application capabilities. We build a proto-
type on Windows that exposes nine recognizers to
applications, including the Kinect skeleton tracker.
Our prototype incurs negligible overhead for single
applications, while improving performance of con-
current applications and enabling secure oﬄoading
of heavyweight recognizer computation.

An augmented reality (AR) application takes nat-
ural user interactions (such as gestures, voice, and
eye gaze) as input and overlays digital content on
top of the real world seen, heard, and experienced
by the user. For example, on mobile phones, aug-
mented reality “browsers” such as Layar and Junaio
allow users to look through the phone and see an-
notations about a magazine article or a storefront.
Furniture applications on the iPad allow users to
preview what a couch would look like in the context
of a real room before buying [17]. The Xbox Kinect
has sold over 19 million units and allows application
developers to overlay avatars on top of a user’s pose,
creating new kinds of games and natural user inter-
faces. Microsoft has released a Windows SDK for
Kinect and helped incubate multiple startup com-
panies delivering AR experiences on the PC. Even
heads-up displays, previously restricted to academic
and limited military/industrial use, are set to reach
consumers with Google Glass [25].

Today’s AR applications are monolithic. The
application itself performs sensing, rendering, and
user input interpretation (e.g., for gestures), aided
by user-space libraries, such as the Kinect SDK,
OpenCV [6, 12], or cloud object recognition ser-
vices, such as Lambda Labs or IQ Engines. Because
today’s OSes are built without AR applications in
mind, they oﬀer only coarse-grained access to sensor
streams, such as video or audio data. This raises
a privacy challenge:
it is diﬃcult to build applica-
tions that follow the principle of least privilege, hav-
ing access to only the information they need and no
more. Today’s systems also do not have any AR-
speciﬁc permissions, relying instead on careful pre-
publication vetting of applications [5].

Motivating Example. Figure 1 illustrates the
problem with coarse-grained abstractions in today’s

USENIX Association  

22nd USENIX Security Symposium  415

1

416  22nd USENIX Security Symposium 

USENIX Association

Figure1:Givingrawsensordatatoapplicationscancompromiseuserprivacy.ThisvideoframecapturedfromaKinectcontainstheuser’sface,privatewhite-boarddrawings,andabottleofmedicine.Figure2:ARapplicationsoftenneedonlyspeciﬁcob-jectsratherthantheentiresensorstreams.The“KinectAdventures!”gameonlyneedsbodypositiontorenderanavatarandsimulategamephysics.ARapplications.Theﬁgureshowsavideoframecapturedfromacamera.Today,applicationsmustaskforrawcameraaccessiftheywanttodovideo-basedAR,whichmeanstheapplicationwillseeallsensitiveinformationintheframe.Inthisframe,thatinformationincludestheuser’sface,(pri-vate)drawingsonthewhiteboard,andabottleofmedicinewithalabelthatrevealsamedicalcondi-tion.Anapplication,however,maynotneedanyofthissensitiveinformationtodoitsjob.Forexample,Figure2showsascreenshotfromthe“KinectAd-ventures!”gamethatshipswiththeMicrosoftXboxKinect.First,thegameestimatesthebodypositionoftheplayerfromthevideoanddepthstreamoftheKinect.Next,thegameoverlaysanavatarontopoftheplayer’sbodyposition.Finallythegamesim-ulatesinteractionbetweentheavatarandavirtualworld,includingaballthatbouncesbackandforthtohitblocks.Todoitsjob,thegameneedsonlybodyposition,andnotanyotherinformationfromthevideoanddepthstream.KinectisjustoneexampleofanARsystem;thisFigure3:TwoexamplesofmobileARapplicationsthatonlyneedspeciﬁcobjectsinasensorstream.Ontheleft,Macy’sBelieve-O-Magiconlyneedsthelocationintheframeofaspecialmarker,ontopofwhichitrendersacartooncharacter.Ontheright,LayaronlyneedstoknowtheGPSlocationandcompasspositiontoshowgeo-taggedtweets.ApplicationObjectsrecognizedYourShape2012skeleton,persontextureDanceCentral3skeleton,persontextureNike+Kinectskeleton,persontextureJustDance4skeleton,videoclipNBA2K13voicecommandsXboxDashboardpointer,voicecommandsLayarGPS“pointsofinterest”RedBullRacingRedBullCansMacy’sBelieve-O-MagicMacy’sstoredisplayFigure4:SampleARapplicationsandtheobjectstheyrecognize.Kinectappsareabovetheline,mobilebelow.principleofARapplicationsbeneﬁtingfrom“leastprivilege”ismoregeneral.WeshowtwomobilephoneexamplesinFigure3.Ontheleft,theMacy’sBelieve-O-Magicapplicationshowsaviewofachildstandingnexttoaholiday-themedcartooncharac-ter.Whiletheapplicationtodaymustaskforrawvideoaccess,whichincludesthefaceofthechildandofallbystanders,theonlyinformationtheap-plicationneedsisthelocationofaspecialmarkertoenablerenderingthecartooninthecorrectplace.Ontheright,Layarisan“ARbrowser”formobilephones,hereshowingavisualizationofwhererecenttweetshaveoriginatedneartheuser.Again,Layarmustaskforrawvideoandlocationaccess,butinfactitneedstoonlyknowtheGPSpositionofthetweetrelativetotheuser.Beyondtheseexamples,Figure4showsthetop5Amazonbest-sellingKinect-enabledapplicationsfortheXbox360,alongwiththeXboxDashboardandrepresentativeARappsonmobilephones.Foreachapplication,aswellastheXboxDashboard,weenu-meratetheobjectsrecognized;inSection5wecarryoutasimilaranalysisforallshippingXboxKinectapplications.Noneoftheseapplicationsneedcon-2tinuous access to raw video and depth data, but no
current OS allows a user to restrict access at ﬁner
granularity.
The Recognizer Abstraction. To address this
problem, we introduce a new least-privilege OS ab-
straction called a recognizer. A recognizer takes as
input a sensor stream and creates events when ob-
jects are recognized. These events contain informa-
tion about the recognized object, such as its position
in the video frame, but not the raw sensor informa-
tion. By making access to recognizer-exposed ob-
jects a ﬁrst-class permission in an operating system,
we enable least privilege for AR applications. We
assume a ﬁxed set of system-provided recognizers in
this work. This is justiﬁed by our analysis of over 87
shipping applications, which shows a set of four “core
recognizers” is suﬃcient for the vast majority of such
applications (Section 5).

Supporting recognizers in the OS incurs several
beneﬁts. Besides enabling least privilege, recogniz-
ers lead to a performance improvement, as heavy-
weight object recognition can be shared among mul-
tiple applications. We show how an OS can com-
pose recognizers in a dataﬂow graph, which enables
precise reasoning about which recognizers should be
run, depending on the set of running applications.
Finally, we show how making dataﬂow explicit al-
lows us to prune spurious permission requests. These
beneﬁts extend beyond AR applications and to any
set of applications that must interpret higher-level
objects from raw sensor data, such as building moni-
toring, stored video analysis, and health monitoring.
Challenges. We faced several challenges designing
our recognizer-based AR platform. First, other ﬁne-
grained permission systems, such as Android, have
been shown to be diﬃcult to interpret for users [11].
To address this problem, we introduce privacy gog-
gles: an “application’s-eye view” of the world that
shows users which recognizers are available to an
application. Users see a video representation of sen-
sitive data that will be shown to the application
(Figure 9). This, in turn, lays the foundation for
informed permission granting or permission revoca-
tion. Our surveys of 462 people show that privacy
goggles are eﬀective at communicating capabilities
to users.

Another challenge concerned recognizer errors.
For example, an application may have permission
for a skeleton recognizer. If that recognizer mistak-
enly ﬁnds a skeleton in a frame, the application may
obtain information even though there is no person
present. This information leakage violates a user’s
expectations, even though the application sees only
a higher-level object such as the skeleton.

We address recognizer errors with a new OS
component, recognizer error correction. We evalu-
ate three approaches: blurring, frame subtraction,
and recognizer combination. The ﬁrst two manipu-
late raw sensor data to reduce false positives in a
recognizer-independent way. The last reduces false
positives by using context information available to
the OS from its use of multiple recognizers that could
not be available to any individual recognizer author.
We show that our techniques reduce false positives
across a set of seven recognizers implemented in the
OpenCV library [12].

Our ﬁnal challenge concerned recognizers that
require heavyweight object recognition algorithms
which may run poorly or not at all on performance-
constrained mobile devices [23, 21]. We thus build
and evaluate support for oﬄoading of particularly
heavyweight recognizers to a remote machine.

We have implemented a prototype of our system
on Windows, using the Kinect for Windows SDK.
Our system includes nine recognizers, including face
detection, skeleton detection, and a “plane recog-
nizer” built on top of KinectFusion [23].

Contributions. We make the following contribu-
tions:

• We introduce a new OS abstraction, the rec-
ognizer, which captures the core object recog-
nition capabilities of AR applications. Our
novel ﬁne-grained permission system for recog-
nizers enables least privilege for AR applica-
tions. We show that all shipping Kinect applica-
tions would beneﬁt from least privilege. Based
on surveys of 500 people, we determine a pri-
vacy ordering on common recognizers.

• We introduce a novel visualization of sensitive
data provided to AR applications, which we call
privacy goggles. Privacy goggles let users in-
spect sensitive information ﬂowing to an appli-
cation, to aid in permission granting, inspec-
tion, and revocation. Our surveys of 462 people
show that privacy goggles are eﬀective at com-
municating capabilities to users.

• We recognize the problem of granting permis-
sions in the presence of object recognition errors
and propose techniques to mitigate it.

• We demonstrate that raising the level of ab-
straction to the “recognizer” enables the OS
to oﬀer services such as oﬄoading and cross-
application recognizer sharing that improve per-
formance. Our implementation has negligible

USENIX Association  

22nd USENIX Security Symposium  417

3

Figure 5: AR application pipeline: (1) reading raw data from hardware, (2) parsing raw data into recognized objects,
(3) manipulating these objects to add augmentations to the scene, and (4) resolving conﬂicts and rendering.

overhead for single applications, yet greatly in-
creases performance for concurrent applications
and allows the OS to oﬄoad heavyweight rec-
ognizer computation.

In the rest of the paper, Section 2 provides back-
ground on AR, Section 3 discusses our recognizer
abstraction, and Section 4 describes our implemen-
tation. Section 5 evaluates privacy goggles, recogniz-
ers required for shipping AR applications, recognizer
error correction, and performance of our prototype.
Sections 6 and 7 present related and future work,
and Section 8 concludes.

2 AR Overview

We characterize AR applications using a pipeline
shown in Figure 5. First, the sensing stage acquires
raw video, audio, and other sensor data from plat-
form hardware. In the ﬁgure, we show an RGB video
frame as an example. The frame was captured from
a Kinect, which also exposes a depth stream and a
high-quality microphone.

Next, the recognition stage applies object recog-
nition algorithms to the raw sensor data. For ex-
ample, voice recognition may run on audio to look
for speciﬁc keywords spoken, or a skeleton detector
may estimate the presence and pose of a human in
the video. As new advances in computer vision and
machine learning make it possible to reliably recog-
nize diﬀerent objects, the resulting algorithms can
be added to this stage. The code performing object
recognition is similar to drivers in traditional operat-
ing systems: code running with high privilege main-
tains an abstraction between “bare sensing” and ap-
plications. Just as with devices in traditional OSes,
an OS with support for AR could multiplex applica-
tions across multiple object recognition components;
we will describe a new OS abstraction that enables
this in the next section. In the ﬁgure, a face and two
areas of text are recognized, one on the whiteboard
and another on a bottle of medicine. The output of
the recognition stage is a set of software objects that

“mirror” recognized real-world objects.

In the transformation stage, applications consume
the recognized objects and add virtual objects of
their own. Finally, the presentation stage creates
the ﬁnal view for the user, taking as input all cur-
rent software objects and the current state of the
world. This stage must resolve any remaining logi-
cal conﬂicts, as well as check that desired placement
of objects is feasible. Today, this rendering is done
using standard OS abstractions, such as DirectX or
OpenGL.

3 The Recognizer OS Abstraction

We propose a new OS abstraction called a recog-
nizer. A recognizer is an OS component that takes
a sensor stream, such as video or audio, and “rec-
ognizes” objects in the sensor stream. For example,
Figure 6 shows a recognizer that wraps face detec-
tion logic. This recognizer takes a raw RGB image
and outputs a face object if a face is present. The
recognizer abstraction lets us capture that most AR
applications operate on speciﬁc entities with high-
level semantics, such as the face or the skeleton. To
enable least privilege, the OS exposes higher level
entities through recognizers.

Recognizers create events when objects are recog-
nized. A recognizer event contains structured data
that encodes information about the objects. Each
recognizer declares a public type for this structured
data that is available to applications. Applications
register callbacks with the OS that ﬁre for events
from a particular recognizer; the callbacks accept
arguments of the speciﬁed type. For example, the
recognizer in Figure 6 declares that it will return a
list of points corresponding to facial features, plus
an RGB texture for the face itself. A callback for
an application receives the points and texture in its
arguments, but not the rest of the raw RGB frame.
The recognizer is the unit of permission granting.
Every time an application attempts to register a call-
back with the OS for a speciﬁc recognizer, the ap-
plication must be authorized by the user. Diﬀerent

418  22nd USENIX Security Symposium 

USENIX Association

4

Figure 6: Example of a recognizer for face detection.
The input is a feed of raw RGB video plus a region within
that video. The recognizer outputs an event if a face is
recognized in the region. Applications register callbacks
that ﬁre on the event and are called with a list of points
outlining the face plus an RGB texture, but not the rest
of the video frame.

Figure 7: A sample directed acyclic graph of recogniz-
ers. Arrows denote how recognizers subscribe to events
from other recognizers.

applications can, depending on the user’s authoriza-
tion, have access to diﬀerent recognizers.This gives
us a ﬁne-grained permission mechanism.

Users can restrict applications to only “see” a sub-
set of the raw data stream. For example, Figure 6
shows a bounding box in the raw RGB frame that
can be associated with a speciﬁc application.
If a
face happened to be present outside this bounding
box, that application would not see the resulting
event. Such regions are useful to (1) prevent an
application from seeing sensitive information in the
environment, and (2) improve eﬃciency and accu-
racy of recognizers (e.g., by skipping a region that
generates false positives). This bounding box works
for sensors where the data is spatial, such as RGB,
depth, or skeleton feeds. Other cutoﬀs would work
for other sensors, such as ﬁltering audio to a certain
frequency range to ensure voice data is not leaked
while other sounds are kept.

Recognizers can also subscribe to events from
other recognizers, just like applications. The OS in-
cludes recognizers for raw sensor streams, such as
RGB frames from a camera. Because subscribing
to events is an explicit call to the OS, the OS can
construct a dataﬂow graph showing how raw sensor
streams are progressively reﬁned into objects. Fig-
ure 7 shows an example. Having explicit data ﬂow

Figure 8: Recognizer-based OS architecture. Applica-
tions request subscriptions to sets of recognizers, which
the OS then conﬁrms with the user using privacy gog-
gles (Figure 9). Once the user grants permission, the OS
delivers recognizer events to subscribed applications.

helps the OS with both security and performance,
as we describe below.
Architecture and Threat Model: Figure 8 shows
the core architecture of an OS with multiple appli-
cations and multiple recognizers. “Root” recognizers
acquire raw input from sensors such as the Kinect,
then raise events that are consumed by other rec-
ognizers. An application may request a subscription
for a set of recognizers. The OS conﬁrms this request
with the user using our “Privacy Goggles” visualiza-
tion (Section 3.3). If the user agrees to the request,
the OS then delivers events from appropriate recog-
nizers to the application. While our implementation
and example focuses on the Kinect, our architecture
applies to all forms of object recognition across dif-
ferent platforms such as mobile phones.

The applications are not

trusted, while the
OS, recognizer implementations, and hardware are
trusted. This is similar to the threat model in to-
day’s mobile devices. Third-party recognizer imple-
mentations are out of scope of this paper, but we
describe in Section 7 key new challenges they raise.

3.1 Security Beneﬁts

The recognizer abstraction has two key security ben-
eﬁts:
Least privilege: Applications can be given access
only to the recognizers they need, instead of to raw
sensor streams. Before recognizers, OSes could ex-
pose permissions only at a coarse granularity. As
we will see in Section 5, a small set of recognizers is
suﬃcient to cover most shipping AR applications.
If an applica-
Reducing permission requests:
tion requests access to the skeleton and hand rec-
ognizers from the DAG shown in Figure 7, a user
only needs to grant access to the skeleton recognizer.

USENIX Association  

22nd USENIX Security Symposium  419

5

More generally, the recognizer DAG allows us to ﬁnd
such dependencies eﬃciently. This helps with warn-
ing fatigue, which is one of the major problems with
existing permission systems [11].

3.2 Performance Beneﬁts

Besides the security beneﬁts described above, recog-
nizer DAGs also allow us to achieve signiﬁcant per-
formance gains.
Sharing recognizer output: Most computer vi-
sion algorithms used in recognizers are computation-
ally intensive. Since concurrently running AR appli-
cations may access the same recognizers, our recog-
nizer DAG allows us to run such shared recognizers
only once and send the output to all subscribed ap-
plications. Our experiments show that this results
in signiﬁcant performance gains for concurrent ap-
plications.
On-demand invocation:
The recognizer DAG
allows us to ﬁnd all recognizers being accessed by
currently active applications at all times. We can
then prevent scheduling inactive recognizers.
Concurrent execution:
The recognizer DAG
also allows us to ﬁnd true data dependencies between
the recognizers. We leverage this to schedule inde-
pendent recognizers in multiple threads/cores and
thus minimize inter-thread/core communication.
Oﬄoading:
Some recognizers require special-
purpose hardware such as a powerful GPU that may
not be available in mobile devices. These recognizers
must be outsourced to a remote server. For exam-
ple, the real-time 3D model generation of KinectFu-
sion [23] requires a high-end nVidia desktop graphics
card, such as a GeForce GTX 680. Therefore, if we
want to use a commodity tablet with a Kinect at-
tached to scan objects and create models, we must
run the recognizer on a remote machine. While
applications could implement oﬄoading themselves,
adding oﬄoading support to the OS preserves least
privilege. For example, the OS can oﬄoad KinectFu-
sion without giving applications access to raw RGB
and depth inputs, which would be required if an ap-
plication were to oﬄoad it manually.

3.3 Privacy Goggles

We introduce privacy goggles, an “application-eye
view” of the world for running applications. For
example, if the application has access to a skele-
ton recognizer, a stick ﬁgure in the “privacy goggles
view” mirrors the movements of any person in view
of the system, as shown in Figure 9. A trusted visu-
alization method for each recognizer communicates

the capabilities of applications that have access to
this recognizer. If an application requests access to
more than one recognizer, the OS will compose the
appropriate visualizations.
In Section 5 we survey
462 people to demonstrate that privacy goggles do
eﬀectively communicate capabilities for “core recog-
nizers” derived from analyzing shipping AR applica-
tions. Privacy goggles are complementary to exist-
ing permission widgets, such as those of Howell and
Schechter [16], which allow users to understand how
apps perceive them in real time.
Permission Granting and Revocation. Privacy
goggles lay a foundation for permission granting, in-
spection, and revocation experiences. For example,
we can generalize existing install-time manifests to
use privacy goggles visualizations. At installation
time, a short prepared video could play showing a
“raw” data stream side by side with the privacy gog-
gles view. The user can then decide to allow access
to all, some, or none of the recognizers. We are cur-
rently evaluating this approach. Because manifest-
based systems have known problems with user at-
tention [11], we are also exploring how access-control
gadgets might interact with privacy goggles [27].

A major diﬀerence between privacy goggles and
existing permission granting systems like Android
manifests is the visual representation of the sen-
sitive data. The visual representation helps users
to make informed decisions about granting and re-
voking an application’s access to diﬀerent recogniz-
ers. Traditional systems do not need this represen-
tation because they ask for permissions about well-
understood low-level hardware, such as the camera
and microphone. Because we are ﬁne-grained and
must consider higher-level semantics, we need pri-
vacy goggles to show the impact of allowing applica-
tions access to speciﬁc recognizers.

After installation, privacy goggles are a natural
way to inspect sensitive data exposed to applica-
tions. The user can trigger a “privacy goggles control
panel” to zero in on a particular application or view
a composite for all applications at once. From the
control panel, a user can then turn oﬀ an applica-
tion’s access to a recognizer or even uninstall the
application.

3.4 Handling Recognizer Errors

Because our permission system depends on recog-
nizer outputs, we have a new challenge: recognizer
errors. Object recognition algorithms inside recog-
nizers have both false positives and false negatives.
A false negative means that applications will not
“see” an object in the world, impacting functionality.

420  22nd USENIX Security Symposium 

USENIX Association

6

False negatives, however, do not concern privacy.

A false positive, on the other hand, means that
an application will see more information than was
intended. In some cases the damage will be limited,
because the recognizer will return information that
is not sensitive. For example, a false positive from a
recognizer for hand positions is unlikely to be a prob-
lem. In others, false positives could leak portions of
raw RGB frames or other more sensitive data.

To address recognizer errors, we introduce a new
OS component for recognizer error correction. While
recognizers themselves implement various techniques
to decrease errors, in our setting false positives are
damaging, while false negatives are less important.
Therefore, we are willing to tolerate more false neg-
atives and fewer false positives than a recognizer de-
veloper who is not concerned with basing permission
decisions on a recognizer’s output.

For recognizer error correction, we ﬁrst consid-
ered two techniques: blurring and frame subtraction,
both of which are well-known graphics techniques
that can be applied in a recognizer-independent way.
We apply these techniques to recognizer inputs to
reduce potential false positives, accepting that they
may raise false negatives. We discuss the results and
show data in Section 5.

In addition, the OS has information not available
to an individual recognizer developer: results from
other recognizers in the same system on the same
environment. Recognizer error correction can there-
fore employ recognizer combination to reduce false
positives. For example, if a depth camera is avail-
able, the OS can use the depth camera to modify the
input to a face detection recognizer. By blanking
out all pixels past a certain depth, the OS can en-
sure a face recognizer focuses only on possible faces
near the system. While combination does require
knowing something about what a recognizer does,
it is independent of the internals of the recognizer
implementation. For another example, the OS can
combine a skeleton recognizer and a face recognizer
to release a face image only if there is also a skeleton
with its head in the appropriate place.

3.5 Adding New Recognizers.

Today’s AR platforms ship with a small ﬁxed set of
recognizers. Applications that want capabilities out-
side that set need to both innovate on object recog-
nition and on app experience, which is rare. As the
platforms mature, we expect additional recognizers
to appear. The main incremental costs for new rec-
ognizers are 1) coming up with a privacy goggles
visualization, 2) measuring the eﬀectiveness of this

Figure 9: Example of “privacy goggles.” The user sees
the “application-eye view” for a skeleton recognizer.

API

Purpose

init
destruct
event_generate
visualize
filter
cache_compare

Register
Clean up
Notify apps of recognized objects
Render recognized objects
Restrict domain for recognition
Compare to previous inputs

Figure 10: The APIs implemented by each recog-
nizer. The ﬁrst four are required, while filter and
cache_compare are optional.

visualization at informing users (and re-designing if
not eﬀective), and 3) deﬁning relationships with ex-
isting recognizers to support recognizer error correc-
tion. For example, a new “eye recognizer” would
have the invariant that every eye detected should
be on a head detected by the skeleton recognizer.
Third-party recognizers raise additional security is-
sues outside the scope of this paper; we discuss them
brieﬂy in Section 7.

4 Implementation

We have built a prototype implementation of our ar-
chitecture. Our prototype consists of a multiplexer,
which plays the role of an OS “kernel”, and ARLib, a
library used by AR applications to communicate to
the multiplexer. Our system uses the Kinect RGB
and depth cameras for its sensor inputs.

Multiplexer. The multiplexer handles access to
the sensors and also contains implementations of
all recognizers in the system. Our applications no
longer have direct access to Kinect sensor data and
must instead interact with the multiplexer and re-
trieve this data from recognizers. The multiplexer
supports simultaneous connections from multiple ap-
plications. To simplify implementation, we built
the multiplexer as a user-space program in Windows
that links against the Kinect for Windows SDK.

The multiplexer registers each recognizer using
a static, well-known name. Applications use these
names to request access to one or more recogniz-

USENIX Association  

22nd USENIX Security Symposium  421

7

var client = new MultiplexerClient();
client.Connect();
client.OnFace += new FaceEventCallback(ProcessFace);
...
public void ProcessFace(FTPoint[] points)
{

if (points.Length > 0) {

DrawFace(points);

} else {

RemoveFace();

}

}
Figure 12: Code used by a sample C# application to
connect to the multiplexer, subscribe to events from the
face recognizer, and use those events to update its face
visualization.

ers. When the multiplexer receives such an access
request,
it asks the user whether or not permis-
sion should be granted using privacy goggles (Sec-
tion 3.3).
If the user grants permission, the mul-
tiplexer will forward future recognizer events, such
as face mesh points from a face recognizer, to the
application.

The multiplexer interacts with recognizers via an
API shown in Figure 10. All recognizers must im-
plement the ﬁrst four API calls. The multiplexer
calls init to initialize a recognizer and destruct
to let a recognizer release its resources.
In our
current implementation, the multiplexer calls the
event_generate function of each recognizer in a
loop, providing prerequisite recognizer inputs as pa-
rameters, to check if any new objects have been rec-
ognized. If so, the recognizer will return data that
the multiplexer will then package in an event data
structure and pass to all subscribed applications. We
plan to implement a more eﬃcient interrupt-driven
multiplexer in the future.

The next two API calls are optional. The fil-
ter call allows the multiplexer to tell the recog-
nizer that only a speciﬁc subset of the raw inputs
should be used for recognition. For example, only a
sub-rectangle of the video frame should be consid-
ered for a face detector. Finally, cache_compare is
a recognizer-speciﬁc comparator function that takes
two sets of recognizer inputs and determines whether
they are considered equal. The multiplexer uses this
comparator to implement per-recognizer caching.
For example, the multiplexer may pass the previ-
ous and current RGB frames to the cache_compare
function of the face recognizer and potentially avoid
a recomputation of the face model if the two frames
have not suﬃciently changed.

Our multiplexer and recognizers consisted of
about 3,000 lines of C++ code. We wrote a total of
nine recognizers, which we summarize in Figure 11.

targeting
Application support. Applications
our multiplexer run in separate Windows processes.
Each application links against the ARLib library we
have built. ARLib communicates with the multi-
plexer over local sockets and handles marshaling and
unmarshaling of recognizer event data. By calling
ARLib functions, an application can request access
to speciﬁc recognizers and register callbacks to han-
dle recognizer events. ARLib provides two kinds
of interfaces: a low-level interface for applications
written in C++ and higher-level wrappers for .NET
applications written in C# or other managed lan-
guages. ARLib consists of about 500 lines of C++
code and 400 lines of C# code.

Sample code in Figure 12 shows a part of a test
application we wrote that detects faces and draws
pictures on the screen which follow face movements.
The application connects to the multiplexer and sub-
scribes to face recognizer events. In our implementa-
tion, these events contain approximately 100 points
corresponding to diﬀerent parts of the face, or 0
points if a face is not present. The application han-
dles these events in the ProcessFace callback by
checking if a face is present and calling a separate
function (not shown) that updates the display.

In addition to face visualization, we ported a few
other sample applications bundled with the Kinect
SDK to our system. These included a skeleton vi-
sualizer and raw RGB and depth visualizers. We
found the porting eﬀort to be modest, aided in part
by the fact that we modeled our event data formats
on existing Kinect SDK APIs. In each case, we only
changed a handful of lines dealing with event sub-
scription. We additionally wrote two applications
from scratch: a 500-line C++ application that trans-
lates hand gestures into mouse cursor movements,
and a 300-line C# application that uses face recog-
nition to annotate people with their names. Overall,
we found our multiplexer interface simple and intu-
itive to use for building AR applications.

5 Evaluation

We ﬁrst evaluate how recognizers are used by an
analysis of 87 shipping AR applications and users’
mental models of AR applications. A survey of 462
respondents shows that users expect AR applica-
tions to have limited access to raw data. Fur-
thermore, no shipping application needs continuous
RGB access, and in fact a set of four recognizers is
suﬃcient for almost all applications. For these “core”
recognizers, we design privacy goggles visualizations
and evaluate how well users understand them. Next,
we look at how the OS can mitigate recognizer er-

422  22nd USENIX Security Symposium 

USENIX Association

8

Recognizer

Input dependencies

Output

RGB
Depth
Skeleton
Hand
FaceDetect
PersonTexture
Plane
FaceRecognize
CameraMotion

Kinect
Kinect
Kinect
Skeleton
RGB
Depth, Skeleton
RGB, Depth
RGB, FaceDetect
Kinect

RGB camera frames
Depth camera frames
Computed skeleton model(s)
Hand positions
2D face models for faces in current view
Depth “cutout” of a person
3D polygon coordinates constructed with KinectFusion (see Section 5.3)
Name of person in current view (see Section 5.3)
Camera movements detected using an accelerometer/gyro

Figure 11: The nine recognizers implemented by our multiplexer. A “Kinect” input dependency means that the
recognizer obtains data directly from the Kinect rather than other recognizers.

rors once an application has access to recognizers.
Finally, we show that our abstraction enables perfor-
mance improvements, making this a rare case when
improved privacy leads to improved performance.

5.1 Recognizers

Core Recognizers. We analyzed 87 AR applica-
tions on the Xbox Kinect platform,
including all
applications sold on Amazon.com. We focused on
Kinect because it is widely adopted and sits in a
user’s home. For each application, we manually re-
viewed their functionality, either through reading re-
views or by using the application. From this, we
extracted “recognizers” that would be suﬃcient to
support the application’s functionality.

Figure 13 shows the results. Four core recogniz-
ers are suﬃcient to support around 89% of ship-
ping AR applications. The set consists of skeleton
tracking, hand position, person texture, and keyword
voice commands. Person texture reveals a portion of
RGB video around a person detected through skele-
ton tracking, but with the image blurred or other-
wise transformed to hide all details. Fitness appli-
cations, in particular, use person texture when in-
structing the user on proper form.

After the core set, there is a “long tail” of seven
recognizers. For example, the Alvin and the Chip-
munks game uses voice modulation to “Alvin-ize”
the player’s voice, and NBA Baller Beats actually
tracks the location of a basketball to check that the
player dribbles in time to music. None of the ap-
plications in our set, however, require continuous
access to RGB data.
Instead, applications take a
short video or photo of the player so that she can
share how silly she looks with friends; this could be
handled via user-driven access control [27]. Only 3
applications require audio access beyond voice com-
mand triggers. There is plenty of room to improve
privacy with least privilege enabled by the recognizer
abstraction.
Privacy Expectations for Applications. To

Recognizer

% Apps

Skeleton
Person Texture (PT)
Voice Commands (VC)
Hand Position (HP)
Video Clip
Picture Snap
Voice Intensity
Voice Modulation
Speaker Recognition
Sound Recognition
Basketball Tracking

Skeleton+PT+VC
Skeleton+PT+VC+HP

94.3%
25.3%
3.44%
5.74%
3.4%
1.1%
1.1%
1.1%
1.1%
1.1%
1.1%

82.75%
89.65%

Figure 13: Analysis of all recognizers used by 87 ship-
ping Xbox applications. For each recognizer, we show
what percentage of apps use that recognizer (and possi-
bly others). We also show two sets of recognizers, and
for each set, the percentage of apps that use recogniz-
ers in this set and no others. A set of four recognizers
covers 89.65% of all applications. No application needs
continuous raw RGB access, and only 3 need audio access
beyond voice commands.

learn users’ mental models of AR application capa-
bilities, we showed 462 survey respondents a video
of a Kinect “foot piano” application in action: the
Kinect tracks foot positions and plays music. We
then asked about the capabilities of the application.
Figure 17(A) shows the results. Over 86% of all users
responded that the application could see the foot po-
sitions, while a much smaller number believed this
application had other capabilities. Overall, users ex-
pect applications will not see the entire raw sensor
stream.
Privacy Goggles for Core Recognizers. As we
discussed in Section 3, every recognizer must im-
plement a visualization method to enable the pri-
vacy goggles view. The OS uses these visualizations
to display to the user what information is obtained
by each application. We developed privacy goggles
visualizations for three of the four core recogniz-
ers:
skeleton, hand position, and person texture.
While voice commands are also a core recognizer,

USENIX Association  

22nd USENIX Security Symposium  423

9

424  22nd USENIX Security Symposium 

USENIX Association

Figure14:Examplesurveyquestionforprivacygog-gles.Anembeddedwarningvideoshowstwoviews:therawvideoontheright,andwhattheapplicationwillseeontheleft.Surveyrespondentswatchedthewarn-ingvideo,thenansweredquestionsaboutwhattheappcouldorcouldnotdoafterinstallation.Outof152re-spondents,80%correctlyidentiﬁedthattheappcouldseebodyposition,and47%correctlydeterminedtheappcouldseehandpositions.Figure15:Examplesurveyonrelativesensitivity.Re-spondentsindicatedwhichpictureismoresensitive:the“raw”RGBvideoframeoranimageshowingonlytheoutputofafacedetector.Outof50respondents,86%indicatedtherawimagewasmoresensitive.wedecidedtofocusﬁrstonthevisualrecognizersandleavevisualizationofvoicecommandsforfuturework.PrivacyAttitudesforCoreRecognizers.Wethenconductedsurveystomeasuretherelativesen-sitivityoftheinformationreleasedbythecorerecog-nizers.Wealsoaddedthe“facedetector”recognizer,becauseintuitivelythefaceisprivateinformation,anda“Raw”videorecognizerthatrepresentsgivingallinformationtotheapplication.Foreachpairofrecognizers,weshowedavisualizationfromthesameunderlyingvideoframe,thenaskedtheparticipanttostatewhichpicturewas“moresensitive”andwhy.Figure15showsanexamplecomparingrawRGBandfacedetectorrecognizers.Foreachpairofrecognizers,weasked50peopletoratewhichpicturecontainedinformationthatwas“moresensitive.”Figure16showstheresults.Intotalwehad500surveyrespondents,allfromtheUnitedStates.Asexpected,respondentsﬁndthattherawRGBframeismoresensitivethananyotherRecognizersLeftmore95%LeftRightsensitiveCIRawFace86%±9.6%RawSkeleton78%±11.48%RawTexture88%±9.01%RawHand88%±9.01%TextureSkeleton82%±10.65%TextureFace35%±13.22%TextureHand84%±10.16%SkeletonFace24%±11.84%SkeletonHand84%±10.16%HandFace22%±11.48%Figure16:Resultsfromrelativesensitivitysurveys.Userswereshowntwopictures,onefromeachrecognizer,hereshownasthe“left”andthe“right”recognizer.Thetablereportswhichpicturerespondentsthoughtcon-tained“moresensitive”informationandthe95%con-ﬁdenceinterval.Forexample,intheﬁrstline,86%ofpeoplethoughtthattheviewfromthe“Raw”RGBrec-ognizerwasmoresensitivethantheviewfromafacedetector,witha95%conﬁdenceintervalof±9.6%.recognizer.Basedontheresponses,wecanorderrecognizersfrom“mostsensitive”to“leastsensitive”,asfollows:Raw,Face,PersonTexture,Skeleton,andﬁnallytheleastsensitiveisHandPosition.EﬀectivenessofPrivacyGoggles.Finally,weevaluatedwhetherour“privacygoggles”visualiza-tionssuccessfullycommunicatethecapabilitiesofapplications.Wecreatedthreesurveys,oneforeachoftheskeleton,persontexture,andhandrecogniz-ers.Wehadatleast150respondentstoeachsurvey,withatotalof462respondents.Oursurveysarein-spiredbyFeltetal.’sAndroidpermission“quiz.”[11]Weshowedashortvideoclipoftheprivacygog-glesvisualizationforthetargetrecognizer.Figure14showsanexamplefortheskeletonrecognizer.TherighthalfshowstherawRGBvideoofapersonwrit-ingonawhiteboardandhandlingasmallceramiccatﬁgurine.Thelefthalfshowsthe“application-eyeview”showingthedetectedskeleton.Wethenaskeduserswhattheybelievedthecapabilitiesoftheappli-cationwouldbeifinstalled.Figure17showsthere-sults,withacheckmarknexttocorrectanswers.Weseethatalargenumberofrespondents(over80%)pickedthecorrectresultandrelativelyfewpickedin-correctresults.Thisshowsthatprivacygogglesareeﬀectiveatcommunicatingapplicationcapabilitiestotheuser.RespondentDemographics.Oursurveypartic-ipantswererecruitedfromtheU.S.throughuS-ample[30],aprofessionalsurveyservice,viatheInstant.lywebsite.Wedidnotspecifyanyre-strictionsondemographicstorecruit.AsreportedbyuSample,participantsare66%femaleand33%10USENIX Association  

22nd USENIX Security Symposium  425

A.FootPiano(462respondents)Seemybodyposition76(16%)Seemyfootpositions(cid:31)400(86%)SeewhatIlooklike28(6%)Seetheentirevideo52(11%)Learnmyheartrate21(4%)Noneoftheabove20(4%)Idon’tknow20(4%)B.Skeleton(152respondents)SeewhatIlooklike17(11%)Seemybodyposition(cid:31)122(80%)Seemylocation24(16%)Readthecontentsofthewhiteboard14(9%)SendpremiumSMSmessagesonmybehalf4(3%)Trackthepositionofmyhands(cid:31)71(47%)Noneoftheabove4(3%)Idon’tknow1(1%)C.PersonTexture(156respondents)SeewhatIlooklike36(23%)Seemybodyposition(cid:31)137(88%)Seemylocation25(16%)Seetheceramiccat19(12%)Readthecontentsofthewhiteboard5(3%)SendpremiumSMSmessagesonmybehalf0(0%)Trackthepositionofmyhands(cid:31)60(38%)Noneoftheabove2(1%)Idon’tknow5(3%)D.HandPosition(154respondents)SeewhatIlooklike17(11%)Seemybodyposition32(21%)Seemylocation14(9%)Seetheceramiccat12(8%)Readthecontentsofthewhiteboard7(5%)SendpremiumSMSmessagesonmybehalf2(1%)Trackthepositionofmyhands(cid:31)125(81%)Noneoftheabove3(2%)Idon’tknow4(3%)Figure17:Resultsfromprivacygoggleseﬀectivenesssurveys.Foreachofourthreecorerecognizers,weﬁrstaskedrespondentstoanswerquestionsaboutthecapabilitiesofaKinect“footpiano”applicationbasedonashortvideooftheapplicationinuse(A).Wenextshowedaprivacygoggles“permissionwarningvideo”andaskedquestionsaboutwhattheapplicationcoulddoifinstalled(B-D).male,with10.2%inthe0–22agerange,12.9%22–26,21.2%26–34,16.8%34–42,13.5%42–50,15.1%50–60,8.1%60–70,and1.8%70orolder.HumanEthicsStatement.Ourexperimentsin-cludesurveysofanonymoushumanparticipants.OurinstitutiondoesnothaveanInstitutionalRe-viewBoard(IRB),butitdoeshaveadedicatedteamwhosefocusisprivacyandhumanprotection.Thisteamhaspre-approvedsurveyparticipantvendorstoensurethattheyhaveprivacypolicieswhichpro-tectparticipants.Wefollowedtheguidelinesofthisteaminchoosingoursurveyvendor.Wealsodis-cussedoursurveyswithamemberoftheteamtoensurethatourquestionsdidnotaskforpersonallyidentiﬁableinformation,thattheywerenotoverlyintrusive,andthatnootherissueswerepresent.5.2NoisyPermissionsWhileprivacygogglesareeﬀectiveatcommunicat-ingwhatanappshouldandshouldnotseetotheuser,therecognizersweusecanhavefalsepositives.Thesecouldleakinformationtoapplications.Weﬁrstevaluatedarepresentativesetofrecognizersonwell-knownvisiondatasetstoquantifytheprob-lem.Next,weevaluatedOS-levelmitigationsforfalsepositives.RecognizerAccuracy.Wepickedthreewell-knowndatasetsforourevaluations:(1)aBerkeleydatasetconsistingofpicturesofobjects,(2)anIN-RIAdatasetcontainingpicturesofatalkinghead,and(3)asetofpicturesofafaceturningtowardtheFigure19:Recognizercombinationinaction.TheleftﬁgureshowsresultsofrunningafacedetectoronarawRGBvideoframe.Twofacesaredetected,butonlyonebelongstoarealperson.Ontheright,facedetectionisrunaftercombiningRGBanddepth.Onlytherealpersonisdetected.cameraandthenaway.Wethenevaluatedbaselinefalsepositiveandfalsenegativeratesforsevenob-jectrecognitionalgorithmscontainedinthewidelyadoptedOpenCVlibrary.Allsevenhadfalseposi-tivesonatleastoneofthedatasets.InputMassaging.Wethenimplementedpre-permissionblurring,inwhichframesareputthroughablurringprocessusingaboxﬁlterbeforebeingpassedtothefacedetectionalgorithm.Weuseda12×12boxﬁlter.Wealsousedframesubtractionasaheuristictosuppressrecognizerfalsepositives.Inframesubtraction,whenarecognizerdetectsanobjectwithaboundingboxbinaframeF1thatitdidnotdetectinthepreviousframeF0,wecomputethediﬀerenceCrop(F1,b)−Crop(F0,b)andcheckthenumberofpixelsthathaveadiﬀerence.Ifthisnum-11Recognizer Data Set

False Positive False Negative BlurFP BlurFN SubFP SubFN

Face
Face
Face

FullBody
FullBody
FullBody

LowBody
LowBody
LowBody

UpperBody
UpperBody
UpperBody

Eye
Eye
Eye

Nose
Nose
Nose

Mouth
Mouth
Mouth

Objects
Talking Head
Turning Face

Objects
Talking Head
Turning Face

Objects
Talking Head
Turning Face

Objects
Talking Head
Turning Face

Object
Talking Head
Turning Face

Object
Talking Head
Turning Face

Object
Talking Head
Turning Face

10.6%
0.2%
19.1%

14.8%
0.2 %
24.6%

19.5%
6.2%
33%

41%
5.3%
86%

35%
64 %
23 %

17.8%
90 %
24.5 %

61%
100 %
75 %

0%
0%
16.1%

0%
0%
0%

0%
0%
0%

0%
0%
0%

0%
0 %
5%

0%
0%
0%

0%
0%
0%

6%
0%
15%

3.5%
0%
22.7 %

4.6%
0.3%
25%

10%
0.1%
0%

83%
100 %
100%

57%
86%
43%

75%
75%
82%

0%
0%

9.6%
0%
16.1% 17.64 %

0%
0%
16.1%

0%
0%
0%

0%
0%
0%

0%
0%
0%

0%
0%
0%

0%
0%
7%

0%
0%
0%

9.6%
0%
20%

17.9%
0%
28.3%

38.1%
0.2%
19.9%

32 %
30%
9%

17.1 %
90%
24%

59 %
100%
74%

0%
0%
0%

0%
0%
0%

0%
0%
0%

0%
2%
10%

0%
0%
0%

0%
0%
0%

Figure 18: False positive and false negative rates for OpenCV recognizers on common data sets. False positives are
important because they could leak unintended information to an application. We also show the eﬀect of blurring and
frame subtraction. For blurring we used a 12x12 box ﬁlter.

ber does not exceed a threshold, we ignore the de-
tected object as a false positive.

For three out of our seven recognizers, blurring
decreases false positives with no eﬀect on false neg-
atives, with a maximum reduction for our lower
body recognizer from 19.5% false positives to 4.6%
false positives. For the remaining recognizers, false
positives decrease but false negatives also increase.
Frame subtraction decreases false positives for six
out of seven recognizers and has no eﬀect on the sev-
enth, with no impact on false negatives. This is in
line with our goals, because false positives are more
damaging to privacy than false negatives. The full
results are in Figure 18.
Recognizer Combination. Finally, we imple-
mented recognizer combination, in which the OS can
take advantage of the fact that multiple recognizers
are available. Speciﬁcally, we combined the OpenCV
face detector with the Kinect depth sensor. We chose
the OpenCV face detector because its developers
could depend only on the presence of RGB video
data. We ran an experiment that ﬁrst acquires an
RGB and depth frame, then blanks out all pixels
with depth data that is further away than a thresh-
old. Next, we fed the resulting frame to the face
detector. An example result is shown in Figure 19.
On the left, the original frame shows a false posi-
tive detected behind the real person. On the right,
recognizer combination successfully avoids the false

Recognizers

Kinect SDK

Our framework

RGB Video
Skeleton
Face

29.87 fps
29.59 fps
28.24 fps

30.02 fps
28.65 fps
28.00 fps

Figure 20: Frame rates for a single application using
the Kinect SDK vs. using recognizers from our system.
Our system incurs negligible overhead.

positive.

5.3 Performance

In our performance evaluation, we (1) measure the
overhead of using our system compared to using the
Kinect SDK directly, (2) quantify the beneﬁts of rec-
ognizer sharing for multiple concurrent applications,
and (3) evaluate the beneﬁt of recognizer oﬄoading.

Overhead over Kinect SDK. Compared to di-
rectly using the Kinect SDK, an application that
uses our multiplexer will face extra overhead due
to recognizer event processing in the multiplexer as
well as data marshaling and transfer over local sock-
ets. To quantify this overhead, we wrote two identi-
cally functioning applications to obtain and display
a raw 640x480 RGB video feed, a skeleton model,
and points from a face model. The ﬁrst application
used the Kinect SDK APIs directly, while the second

426  22nd USENIX Security Symposium 

USENIX Association

12

 
)
c
e
s
/
s
e
m
a
r
f
(
 
e
t
a
r
 
e
m
a
r
F

30

20

10

0

1

 With recognizer sharing
 No recognizer sharing

5

Number of concurrent applications 

10

15

Figure 21: Eﬀect of sharing a concurrent RGB video
stream between applications. Our framework enables 25
frames per second or higher for up to six applications,
while without sharing the frame rate drops.

used our multiplexer with RGB, skeleton, and face
detection recognizers.

Figure 20 shows the frame rates when running
these two applications on a desktop HP xw8600 ma-
chine with a 4-core Core i5 processor and 4 GB of
RAM. We see that, fortunately, our current proto-
type incurs negligible overhead over the Kinect SDK
when used by a single application.

Recognizer Sharing. Next, we ran multiple con-
current copies of the two applications above to eval-
uate the beneﬁts of recognizer sharing as well as the
scalability of our prototype. Since the Kinect SDK
does not permit concurrent applications, we wrote
a simple wrapper for simulating that functionality,
i.e., allowing multiple applications as if they were
linking to independent copies of the Kinect SDK.

Figure 21 shows the average frame rate for mul-
tiple concurrent applications using the RGB recog-
nizer. We see that without recognizer sharing, frame
rates quickly stall as the number of concurrent ap-
plications increases, becoming unusable beyond ﬁve
applications.
In contrast, our approach maintains
at least 25 frames per second up to six concurrent
applications and degrades gracefully thereafter. We
experienced similar recognizer sharing beneﬁts for
skeleton and face recognizers.

While currently shipping AR platforms do not yet
support multiple concurrent applications, the above
experiment demonstrates that our system is ready to
eﬃciently embrace such support. Indeed, we believe
this to be the future of AR platforms. Mobile phone
“AR Browsers” such as Layar already expose APIs
for third-party developers, with over 5,000 applica-
tions written for Layar alone [20]. Users will beneﬁt
from running these applications concurrently; for ex-
ample, looking at a store front, one application may
show reviews of the store, while another shows in-
formation about its online catalog, and yet a third

Recognizer

Tablet

Oﬄoaded

Server

Throughput (frames/sec)

Plane detection
Face recognition

0
2.04

4.17
2.73

4.46
2.84

Figure 22: Frames processed per second when running
recognizers (1) locally on a client tablet, (2) oﬄoaded to
the server and shipping results back to the tablet, and
(3) locally on the server.

application attaches a name to the face of someone
walking by.
Recognizer Oﬄoading. We evaluated oﬄoad-
ing of two resource-intensive recognizers:
plane
and face recognition. The plane recognizer recon-
structs planes in the current scene using KinectFu-
sion, which computes 3D models from Kinect depth
data [23]. The face recognizer uses the Microsoft
Face SDK [21] to identify the name of the person in
the scene using a small database of known faces.

We implemented oﬄoading across two devices
linked by an 802.11g wireless network. For face
recognition, the client sends RGB bitmaps of the
current scene to the server as often as possible; the
client additionally includes the depth bitmap for the
plane recognizer.

Our client device was a Samsung Series 7 tablet
running Windows 8 Pro 64-bit with a 2-core Core i5
processor and 4 GB of RAM, hooked up to a Kinect.
Our server device was a desktop HP Z800 ma-
chine running Windows 8 Pro 64-bit with two 4-
core Xeon E5530 processors, 48 GB of RAM, and an
Nvidia GTX 680 GPU.

The ﬁrst two columns of Figure 22 show through-
puts experienced by the client when running rec-
ognizers locally and when oﬄoading them to the
server. The plane recognizer requires a high-end
Nvidia GPU, which prevented it from running on our
client at all; we report this as zero frames per second.
With oﬄoading, however, the client is able to detect
planes 4.2 times per second. For face recognition,
the client processed 2.73 frames per second when of-
ﬂoading, a 34% improvement in response time com-
pared to running face recognition locally. In addi-
tion, when run locally, face recognition placed heavy
CPU load on the client, completely consuming one of
its two cores. With oﬄoading, the client’s CPU con-
sumption dropped to 15% required to send bitmaps,
saving battery and freeing resources for processing
other recognizers. Note that our setup allows the
oﬄoading server to service multiple clients in paral-
lel. For example, the server was able to handle eight
concurrent face recognition clients before saturating.
We also considered the overhead of our oﬄoad-

USENIX Association  

22nd USENIX Security Symposium  427

13

ing mechanism by plugging a Kinect into our server
and running the recognizer framework directly on
it. Column 3 of Figure 22 shows these results. We
see that oﬄoading with the Kinect on the client
is only 4–7% slower than running the Kinect on
the server, meaning that the oﬄoading overhead of
transferring bitmaps and recognition results is rea-
sonable.

6 Related Work

Augmented Reality. Azuma surveyed augmented
reality, deﬁning it as real-time registration of 3-D
overlays on the real world [1], later broadening it
to include audio and other senses [2]. We take a
broader view and also consider systems that take in-
put from the world. Qualcomm now has an SDK
for augmented reality that includes features such as
marker-based tracking for mobile phones [26]. Previ-
ous work by our group has laid out a case for adding
OS support for augmented reality applications and
highlighted key challenges [7].

Common shipping object recognition algorithms
include skeleton detection [29], face and headpose
detection [31, 21], and speech recognition [22]. More
recently, Poh et al. showed that heart rate can be ex-
tracted from RGB video [24]. Our recognizer graph
and simple API allow quickly adding new recogniz-
ers to our system.
Sensor Privacy. There are several parts to sen-
sor privacy: access control on sensors, sensor data
usage control once an application obtains access to
sensor data, and access visualization; we discuss re-
lated work for each.

Access control can take the form of user permis-
sions.
iOS’s permission system is to prompt a user
at the ﬁrst time of the sensor access (such as a
map application ﬁrst accessing GPS). Android and
latest Windows OSes use manifests at application
installation time to inform the user of sensor us-
age among other things; the installation proceeds
only if the user permits the application to perma-
nently access all the requested permissions. These
existing permission systems are either disruptive or
ask users’ permissions out-of-context. They are not
least-privilege; permanent access is often granted un-
necessarily. Felt et al [11] has shown that most peo-
ple ignore manifests, and the few who do read man-
ifests do not understand them. To address these is-
sues, access control gadgets (ACGs) [27] were intro-
duced to be trusted UI elements for sensors, which
are embeddable by applications; users’ authentic ac-
tions on an ACG (e.g., a camera trusted UI) grants
the embedding application permission to access the

represented sensor. In this paper, we argue that even
the ACG style of permission granting is too coarse-
grained for augmented reality systems because most
AR applications only require speciﬁc objects rather
than the entire RGB streams (Section 5.1).

Another form of access control

is to reduce
the sensitivity of private data (e.g., GPS coordi-
nates) available to applications. MockDroid [3]
and AppFence [14] allow using fake sensor data.
Krumm [19] surveys methods of reducing sensitive
information conveyed by location readings. Diﬀeren-
tial privacy [9] uses well-known methods for comput-
ing the amount of noise to add to give strong guar-
antees against an adversary’s ability to learn about
any speciﬁc individual. Similarly, we proposed mod-
ifying sensor inputs to recognizers in speciﬁc ways
to reduce false positives that could result in privacy
leaks. Darkly [18] transforms output from computer
vision algorithms (such as contours, moments, or
recognized objects) to blur the identity of the out-
put. Darkly can be applied to the output of our
recognizers.

Once an application obtains access to sensors, in-
formation ﬂow control approaches can be used to
control or monitor an application’s usage of the sen-
sitive data as in TaintDroid [10] and AppFence [14].
In access visualization, sensor-access widgets [15]
were proposed to reside within an application’s dis-
play with an animation to show sensor data being
collected by the application. Darkly [18] also gives
a visualization on its transforms (see above). Our
privacy goggles apply similar ideas to the AR envi-
ronment, allowing a user to visualize an application’s
eye view of the user’s world.
Abstractions for Privacy. Our notion of taking
raw sensor data and providing the higher-level ab-
straction of recognizers is similar to CondOS [4]’s
notion of Contextual Data Units. However, they nei-
ther choose a set of concrete Contextual Data Units
that are suitable for a wide variety of real-world ap-
plications nor address privacy concerns that arise
from applications having access to Contextual Data
Unit values. Koi [13] provides a location matching
abstraction to replace raw GPS coordinates in appli-
cations. The approach in Koi is limited to location
data and may require signiﬁcant work to integrate
into real applications, while our recognizers cover
many types of sensor data and were speciﬁcally cho-
sen to match application needs.

7 Future Work

Further Recognizer Visualization. The recog-
nizers we evaluated had straightforward visualiza-

428  22nd USENIX Security Symposium 

USENIX Association

14

tions, such as the Kinect skeleton. As we noted,
some recognizers, such as voice commands, do not
have obvious visualizations. Other recognizers might
extract features from raw video or audio for use by
a variety of object recognition algorithms, but not
in themselves have an easily understood semantics,
such as a fast Fourier transform of audio. One key
challenge here is to design visualizations for privacy
goggles that clearly communicate to users the impact
of allowing application access to the recognizer. For
example, with voice commands we might try show-
ing a video with sound where detected words are
highlighted with subtitles. A second key challenge
is characterizing the privacy impact of algorithmic
transforms on raw data, especially in the case of
computer vision features that have not been con-
sidered from a privacy perspective.

Third-Party Recognizers. All the recognizers in
this paper are assumed trusted. To enable new ex-
periences, we would like to support extension of the
platform with third-party recognizers. Supporting
third-party recognizers raises challenges, including
permissions for recognizers as well as sandboxing un-
trusted GPU code without sacriﬁcing performance.
We have developed recognizers in a domain-speciﬁc
language that enables precise analysis [8]. Dealing
with such challenges is intriguing future work, sim-
ilar in spirit to research on third-party driver isola-
tion in an OS. For example, we might require such
recognizers to go through a vetting program and
then have their code signed, similar to drivers in
Windows or applications on mobile phone platforms.

Sensing Applications. Besides traditional AR ap-
plications, other applications employ rich sensing
but do not necessarily render on human senses. For
example, robots today use the Kinect sensor for nav-
igating environment, and video conferencing can use
the “person texture” recognizer we describe. One of
our colleagues has also suggested that video confer-
encing can beneﬁt from a depth-limited camera [28].
These applications may also beneﬁt from recogniz-
ers.

Bystander Privacy. Our focus is on protecting
a user’s privacy against untrusted applications. Mo-
bile AR systems such as Google Glass, however, have
already raised signiﬁcant discussion of bystander pri-
vacy — the ability of people around the user to opt
out of recording and object recognition. Our archi-
tecture allows explicitly identifying all applications
that might have access to bystander information, but
it does not tell us when and how to stop sending rec-
ognizer events to applications. Making the system
aware of these issues is important future work.

8 Conclusions

We introduced a new abstraction, the recognizer, for
operating systems to support augmented reality ap-
plications. Recognizers allow applications to raise
the level of abstraction from raw sensor data, such
as audio and video streams, to ask for access to spe-
ciﬁc recognized objects. This enables applications
to act with the least privilege needed. Our analy-
sis of existing applications shows that all of them
would beneﬁt from least privilege enabled by an OS
with support for recognizers. We then introduced
a “privacy goggles” visualization for recognizers to
communicate the impact of allowing access to users.
Our surveys establish a clear privacy ordering on
core recognizers, show that users expect AR apps to
have limited capabilities, and demonstrate privacy
goggles are eﬀective at communicating capabilities
of apps that access recognizers. We built a prototype
on top of the Kinect for Windows SDK. Our imple-
mentation has negligible overhead for single appli-
cations, enables secure OS-level oﬄoading of heavy-
weight recognizer computation, and improves per-
formance for concurrent applications. In short, the
recognizer abstraction improves privacy and perfor-
mance for AR applications, laying the groundwork
for future OS support of rich sensing and AR appli-
cation rendering.

9 Acknowledgements

We thank Janice Tsai, our Privacy Manager, for re-
viewing our survey. We thank Doug Burger, Loris
D’Antoni, Yoshi Kohno, Franziska Roesner, Stuart
Schechter, Margus Veanes, and John Vilk for help-
ful discussions and review of drafts. Stuart Schechter
suggested the idea of a depth-limited camera for tele-
conferencing scenarios. This work was carried out
while the ﬁrst and fourth author were interning at
Microsoft Research.

References

[1] R. T. Azuma. A survey of augmented reality. Presence:
Teleoperators and Virtual Environments, 6(4):355–385,
August 1997.

[2] R. T. Azuma, Y. Baillot, R. Behringer, S. Feiner,
S. Julier, and B. MacIntyre. Recent advances in aug-
mented reality. Computer Graphics and Applications,
21(6):34–47, 2001.

[3] A. R. Beresford, A. Rice, N. Skehin, and R. Sohan.
MockDroid: Trading privacy for application functional-
ity on smartphones. In Workshop on Mobile Computing
Systems and Applications (HotMobile), 2011.

[4] D. Chu, A. Kansal, J. Liu, and F. Zhao. Mo-
It’s time to move up to condOS. May

bile apps:

USENIX Association  

22nd USENIX Security Symposium  429

15

ments using a webcam. IEEE Trans Biomed Engineer-
ing, 58(1):7–11, 2011.

[25] Project

Glass.

https://plus.google.com/

+projectglass/posts.

[26] Qualcomm.

SDK,
http://www.qualcomm.com/products_services/
augmented_reality.html.

Augmented Reality

2011.

[27] F. Roesner, T. Kohno, A. Moshchuk, B. Parno, H. J.
Wang, and C. Cowan. User-driven access control: Re-
thinking permission granting in modern operating sys-
tems.
In IEEE Symposium on Security and Privacy,
2011.

[28] S. Schecter. Depth-limited camera for skype - personal

communication, 2012.

[29] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finoc-
chio, R. Moore, A. Kipman, and A. Blake. Real-time
human pose recognition in parts from a single depth im-
age. In Computer Vision and Pattern Recognition, June
2011.

[30] uSample.

Instant.ly survey creator, 2013.

http://

instant.ly.

[31] P. Viola and M. Jones. Robust Real-time Object De-
tection. In International Journal of Computer Vision,
2001.

2011.
default.aspx?id=147238.

http://research.microsoft.com/apps/pubs/

[5] M. Corporation. Kinect for xbox 360 privacy consid-
http://www.microsoft.com/privacy/

erations, 2012.
technologies/kinect.aspx.

[6] M. Corporation. Kinect for Windows SDK, 2013. http:

//www.microsoft.com/en-us/kinectforwindows/.

[7] L. D’Antoni, A. Dunn, S. Jana, T. Kohno, B. Livshits,
D. Molnar, A. Moshchuk, E. Ofek, F. Roesner,
S. Saponas, M. Veanes, and H. J. Wang. Operating sys-
tem support for augmented reality applications. In Hot
Topics in Operating Systems (HotOS), 2013.

FAST: A transducer-based language

[8] L. D’Antoni, M. Veanes, B. Livshits, and D. Mol-
nar.
for
tree manipulation, 2012. MSR Technical Report
2012-123 http://research.microsoft.com/apps/pubs/
default.aspx?id=179252.

[9] C. Dwork. The diﬀerential privacy frontier. In 6th The-

ory of Cryptography Conference (TCC), 2009.

[10] W. Enck, P. Gilbert, B.-G. Chun, L. P. Cox, J. Jung,
P. McDaniel, and A. N. Sheth.
TaintDroid: An
information-ﬂow tracking system for realtime privacy
monitoring on smartphones. In Conference on Operating
System Design and Implementation, 2010.

[11] A. P. Felt, E. Ha, S. Egelman, A. Haney, E. Chin, and
D. Wagner. Android permissions: User attention, com-
prehension, and behavior. In Symposium on Usable Pri-
vacy and Security (SOUPS), 2012.

[12] W. Garage. OpenCV, 2013. http://opencv.org/.
[13] S. Guha, M. Jain, and V. N. Padmanabhan. Koi: A
location-privacy platform for smartphone apps. In NSDI,
2012.

[14] P. Hornyack, S. Han, J. Jung, S. Schechter, and
D. Wetherall. These aren’t the droids you’re looking
for: retroﬁtting android to protect data from imperious
applications. In Conference on Computer and Commu-
nications Security, 2011.

[15] J. Howell and S. Schechter. What You See is What
They Get: Protecting users from unwanted use of micro-
phones, cameras, and other sensors. In Web 2.0 Security
and Privacy, IEEE, 2010.

[16] J. Howell and S. Schecter. What you see is what they
get: Protecting users from unwanted use of microphones,
cameras, and other sensors.
In Web 2.0 Security and
Privacy Workshop, 2010.

[17] E. Hutchings.

see

how new furniture would

pers
home,
augmented-reality-furniture-app.html.

2012.

Augmented reality

shop-
at
http://www.psfk.com/2012/05/

look

lets

[18] S. Jana, A. Narayanan, and V. Shmatikov. DARKLY:
Privacy for perceptual applications. In IEEE Symposium
on Security and Privacy, 2013.

[19] J. Krumm. A survey of computational location pri-
vacy. Personal Ubiquitous Computing, 13(6):391–399,
Aug 2009.

[20] Layar. Layar catalogue, 2013. http://www.layar.com/

layers.

[21] Microsoft Research Face SDK Beta. http://research.

microsoft.com/en-us/projects/facesdk/.

[22] Microsoft Speech Platform.

http://msdn.microsoft.

com/en-us/library/hh361572(v=office.14).aspx.

[23] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges,
and A. Fitzgibbon. KinectFusion: Real-time dense sur-
face mapping and tracking. In 10th IEEE International
Symposium on Mixed and Augmented Reality, 2011.

[24] M. Poh, D. MacDuﬀ, and R. Picard. Advancements
in non-contact, multiparameter physiological measure-

430  22nd USENIX Security Symposium 

USENIX Association

16

