Man vs. Machine: Practical Adversarial Detection  

of Malicious Crowdsourcing Workers

Gang Wang, University of California, Santa Barbara; Tianyi Wang, University of California, 

Santa Barbara  and Tsinghua University; Haitao Zheng and Ben Y. Zhao,  

University of California, Santa Barbara

https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/wang

This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXMan vs. Machine: Practical Adversarial Detection of Malicious

Crowdsourcing Workers

†Computer Science, UC Santa Barbara

‡Electronic Engineering, Tsinghua University

Gang Wang†, Tianyi Wang†‡, Haitao Zheng† and Ben Y. Zhao†

{gangw, tianyi, htzheng, ravenben}@cs.ucsb.edu

Abstract

Recent work in security and systems has embraced the
use of machine learning (ML) techniques for identify-
ing misbehavior, e.g. email spam and fake (Sybil) users
in social networks. However, ML models are typically
derived from ﬁxed datasets, and must be periodically
retrained.
In adversarial environments, attackers can
adapt by modifying their behavior or even sabotaging
ML models by polluting training data.
In this paper1, we perform an empirical study of ad-
versarial attacks against machine learning models in the
context of detecting malicious crowdsourcing systems,
where sites connect paying users with workers willing to
carry out malicious campaigns. By using human work-
ers, these systems can easily circumvent deployed se-
curity mechanisms, e.g. CAPTCHAs. We collect a
dataset of malicious workers actively performing tasks
on Weibo, China’s Twitter, and use it to develop ML-
based detectors. We show that traditional ML techniques
are accurate (95%–99%) in detection but can be highly
vulnerable to adversarial attacks, including simple eva-
sion attacks (workers modify their behavior) and power-
ful poisoning attacks (where administrators tamper with
the training set). We quantify the robustness of ML clas-
siﬁers by evaluating them in a range of practical adver-
sarial models using ground truth data. Our analysis pro-
vides a detailed look at practical adversarial attacks on
ML models, and helps defenders make informed deci-
sions in the design and conﬁguration of ML detectors.

1 Introduction
Today’s computing networks and services are extremely
complex systems with unpredictable interactions be-
tween numerous moving parts.
In the absence of ac-
curate deterministic models, applying Machine Learning

1Our work received approval from our local IRB review board.

(ML) techniques such as decision trees and support vec-
tor machines (SVMs) produces practical solutions to a
variety of problems. In the security context, ML tech-
niques can extract statistical models from large noisy
datasets, which have proven accurate in detecting mis-
behavior and attacks, e.g. email spam [35, 36], network
intrusion attacks [22, 54], and Internet worms [29]. More
recently, researchers have used them to model and detect
malicious users in online services, e.g. Sybils in social
networks [42, 52], scammers in e-commerce sites [53]
and fraudulent reviewers on online review sites [31].

Despite a wide range of successful applications, ma-
chine learning systems have a weakness: they are vulner-
able to adversarial countermeasures by attackers aware
of their use. First, through either reading publications
or self-experimentation, attackers may become aware of
details of the ML detector, e.g. choice of classiﬁer and
parameters used, and modify their behavior to evade de-
tection. Second, more powerful attackers can actively
tamper with the ML models by polluting the training set,
reducing or eliminating its efﬁcacy. Adversarial machine
learning has been studied by prior work from a theoreti-
cal perspective [6, 12, 27], using simplistic all-or-nothing
assumptions about adversaries’ knowledge about the ML
system in use. In reality, however, attackers are likely to
gain incomplete information or have partial control over
the system. An accurate assessment of the robustness of
ML techniques requires evaluation under realistic threat
models.

In this work, we study the robustness of machine
learning models against practical adversarial attacks, in
the context of detecting malicious crowdsourcing activ-
ity. Malicious crowdsourcing, also called crowdturﬁng,
occurs when an attacker pays a group of Internet users
to carry out malicious campaigns. Recent crowdturf-
ing attacks ranged from “artiﬁcial grassroots” political
campaigns [32, 38], product promotions that spread false
rumors [10], to spam dissemination [13, 39]. Today,
these campaigns are growing in popularity in dedicated

USENIX Association  

23rd USENIX Security Symposium  239

crowdturﬁng sites, e.g. ZhuBaJie (ZBJ)2 and SanDaHa
(SDH)3, and generic crowdsourcing sites [26, 48].
The detection of crowdturﬁng activity is an ideal con-
text to study the impact of adversarial attacks on ma-
chine learning tools. First, crowdturﬁng is a growing
threat to today’s online services. Because tasks are per-
formed by intelligent individuals, these attacks are unde-
tectable by normal measures such as CAPTCHAs or rate
limits. The results of these tasks, fake blogs, slander-
ous reviews, fake social network accounts, are often in-
distinguishable from the real thing. Second, centralized
crowdturﬁng sites like ZBJ and SDH proﬁt directly from
malicious crowdsourcing campaigns, and therefore have
strong monetary incentive and the capability to launch
adversarial attacks. These sites have the capability to
modify aggregate behavior of their users through inter-
face changes or explicit policies, thereby either helping
attackers evade detection or polluting data used as train-
ing input to ML models.
Datasets.
For our analysis, we focus on Sina Weibo,
China’s microblogging network with more than 500 mil-
lion users, and a frequent target of crowdturﬁng cam-
paigns. Most campaigns involve paying users to retweet
spam messages or to follow a speciﬁc Weibo account.
We extract records of 20,416 crowdturﬁng campaigns
(1,012,923 tasks) published on conﬁrmed crowdturﬁng
sites over the last 3 years. We then extract a 28,947
Weibo accounts belonging to crowdturﬁng workers. We
analyze distinguishing features of these accounts, and
build detectors using multiple ML models, including
SVMs, Bayesian, Decision Trees and Random Forests.
We seek answers to several key questions. First, can
machine learning models detect crowdturﬁng activity?
Second, once detectors are active, what are possible
countermeasures available to attackers? Third, can ad-
versaries successfully manipulate ML models by tamper-
ing with training data, and if so, can such efforts succeed
in practice, and which models are most vulnerable?
Adversarial Attack Models. We consider two types of
practical adversarial models against ML systems: those
launched by individual crowd-workers, and those in-
volving coordinated behavior driven by administrators of
centralized crowdturﬁng sites. First, individual workers
can perform evasion attacks, by adapting behavior based
on their knowledge of the target classiﬁer (e.g. ML al-
gorithms, feature space, trained models). We identify a
range of threat models that vary the amount of knowl-
edge by the adversary. The results should provide a com-
prehensive view of how vulnerable ML systems to eva-
sion, ranging from the worst case (total knowledge by at-
tacker) to more practically scenarios. Second, more pow-

2http://www.zhubajie.com/c-tuiguang/
3http://www.sandaha.com/

erful attacks are possible with the help of crowdturﬁng
site administrators, who can manipulate ML detectors by
poisoning or polluting training data. We study the im-
pact on different ML algorithms from two pollution at-
tacks: injecting false data samples, and altering existing
data samples.

Our study makes four key contributions:
• We demonstrate the efﬁcacy of ML models for de-
tecting crowdturﬁng activity. We ﬁnd that Random
Forests perform best out of multiple classiﬁers, with
95% detection accuracy overall and 99% for “pro-
fessional” workers.
• We develop adversarial models for evasion at-
tacks ranging from optimal evasion to more prac-
tical/limited strategies. We ﬁnd while such attacks
can be very powerful in the optimal scenario (at-
tacker has total knowledge), practical attacks are
signiﬁcantly less effective.
• We evaluate a powerful class of poison attacks on
ML training data and ﬁnd that injecting carefully
crafted data into training data can signiﬁcantly re-
duce detection efﬁcacy.
• We observe a consistent tradeoff between ﬁtting ac-
curacy and robustness to adversarial attacks. More
accurate ﬁts (especially to smaller, homogeneous
populations) make models more vulnerable to de-
viations introduced by adversaries. The exception
is Random Forests, which naturally supports ﬁtting
to multiple populations, thus allowing it to maintain
both accuracy and robustness in our tests.

To the best of our knowledge, this is the ﬁrst study to
examine automated detection of large-scale crowdturf-
ing activity, and the ﬁrst to evaluate adversarial attacks
against machine learning models in this context. Our
results show that accurate models are often vulnerable
to adversarial attacks, and that robustness against attacks
should be a primary concern when selecting ML models.
2 Datasets and Methodology
In this section, we provide background on crowdturﬁng,
and introduce our datasets and methodology.
2.1 Background: Crowdturﬁng Systems
Malicious crowdsourcing (crowdturﬁng) sites are web
services where attackers pay groups of human workers to
perform questionable (and often malicious) tasks. While
these services are growing rapidly world-wide, two of the
largest are Chinese sites ZhuBaJie (ZBJ) and SanDaHa
(SDH) [48]. Both sites leave records of campaigns pub-
licly visible to recruit new workers, making it possible
for us to crawl their data for analysis.

240  23rd USENIX Security Symposium 

USENIX Association

Crowdturfing Sites

Target Social Networks

.
.
.

.
.
.

Customer

Worker

Crowdturf Account

Normal User

Figure 1: Crowdturﬁng process.

Figure 1 illustrates how crowdturﬁng campaigns work.
Initially, a customer posts a campaign onto the crowd-
turﬁng site, and pays the site to carry it out. Each cam-
paign is a collection of small tasks, e.g.
tasks to send
or retweet messages advertising a malware site. Workers
accept the task, and use their fake accounts in the target
social network(s) (e.g. Twitter) to carry out the tasks.
Today, crowdturﬁng campaigns often spam web services
such as social networks, online review sites, and instant-
messaging networks [48]. While workers can be any In-
ternet user willing to spam for proﬁt, customers often re-
quire workers to use “high quality” accounts (i.e. estab-
lished accounts with real friends) to perform tasks [48].
In the rest of the paper, we refer workers’ social network
accounts as crowdturf accounts.
Crowdturﬁng on Weibo. Sina Weibo is China’s most
popular microblogging social network with over 500 mil-
lion users [30]. Like Twitter, Weibo users post 140-
character tweets, which can be retweeted by other users.
Users can also follow each other to form asymmetric so-
cial relationships. Unlike Twitter, Weibo allows users to
have conversations via comments on a tweet.
Given its large user population, Weibo is a popular tar-
get for crowdturﬁng systems. There are two major types
of crowdturﬁng campaigns. One type asks workers to
follow a customer’s Weibo account to boost their per-
ceived popularity and visibility in Weibo’s ranked social
search. A second type pays crowd-workers to retweet
spam messages or URLs to reach a large audience. Both
types of campaigns directly violate Weibo’s ToS [2]. A
recent statement (April 2014) from a Weibo administra-
tor shows that Weibo has already begun to take action
against crowdturﬁng spam [1].

2.2 Ground Truth and Baseline Datasets
Our study utilizes a large ground-truth dataset of crowd-
turﬁng worker accounts. We extract these accounts by
ﬁltering through records of all campaigns and tasks tar-
geting Weibo from ZBJ and SDH, and extracting all

Category
Turﬁng
Authent.
Active

# Weibo IDs
28,947
71,890
371,588

# (Re) Tweets
18,473,903
7,600,715
34,164,885

# Comments
15,970,215
13,985,118
75,335,276

Table 1: Dataset summary.

Weibo accounts that accepted these tasks. This is possi-
ble because ZBJ and SDH keep complete records of cam-
paigns and transaction details (i.e. workers who com-
pleted tasks, and their Weibo identities) visible.
As of March 2013, we collected a total of 20,416
Weibo campaigns (over 3 years for ZBJ and SDH), with a
total of 1,012,923 individual tasks. We extracted 34,505
unique Weibo account IDs from these records. 5,558 of
which have already been blocked by Weibo. We col-
lected user proﬁles for the remaining 28,947 active ac-
counts, including social relationships and the latest 2000
tweets from each account. These accounts have per-
formed at least one crowdturﬁng task. We refer to this
as the Turﬁng dataset.
Baseline Datasets for Comparison. We need a base-
line dataset of “normal” users for comparison. We start
by snowball sampling a large collection of Weibo ac-
counts4. We ran breadth-ﬁrst search (BFS) in November
2012 using 100 Seeds randomly chosen from Weibo’s
public tweet stream, giving us 723K accounts. Because
these crawled accounts can include malicious accounts,
we need to do further ﬁltering to obtain a real set of “nor-
mal” users.
We extract two different baseline datasets. First, we
construct a conservative Authenticated dataset, by in-
cluding only Weibo users who have undergone an op-
tional identity veriﬁcation by phone number or Chinese
national ID (equivalent to US drivers license). A user
who has bound her Weibo account to her real-world iden-
tity can be held legally liable for her actions, making
these authenticated accounts highly unlikely to be used
as crowdturﬁng activity. Our Authenticated dataset in-
cludes 71,890 accounts from our snowball sample. Sec-
ond, we construct a larger, more inclusive baseline set of
Active users. We deﬁne this set as users with at least 50
followers and 10 tweets (ﬁltering out dormant accounts5
and Sybil accounts with no followers). We also cross ref-
erence these users against all known crowdturﬁng sites
to remove any worker accounts. The resulting dataset
includes 371,588 accounts. While it is not guaranteed to
be 100% legitimate users, it provides a broader user sam-
ple that is more representative of average user behavior.
4Snowball crawls start from an initial set of seed nodes, and runs
breadth-ﬁrst search to ﬁnd all reachable nodes in the social graph [3].
5Dormant accounts are unlikely to be workers. To qualify for jobs,
ZBJ/SDH workers must meet minimum number of followers/tweets.

USENIX Association  

23rd USENIX Security Symposium  241

 100

 80

 60

 40

 20

)

%

(
 
s
r
e
s
U

 
f
o
 
F
D
C

 0
 0.01

)

%

(
 
s
r
e
s
U

 
f
o
 
F
D
C

 100

 80

 60

 40

 20

 0

Turfing
Active
Authenticated
 1
 0.1
 10
Followee to Follower Ratio

 100

Turfing
Active
Authenticated

 0

 0.2

 0.4
 0.6
Reciprocity

 0.8

 1

)

%

(
 
s
r
e
s
U

 
f
o
 
F
D
C

 100

 80

 60

 40

 20

 0

 0

Turfing
Active
Authenticated

 0.4

 0.2
 0.8
Ratio of Tweets with Comments

 0.6

 1

Figure 2: Followee-to-Follower ratio.

Figure 3: Reciprocity.

Figure 4: Ratio of commented tweets.

This is likely to provide a lower bound for detector accu-
racy, since more carefully curated baselines would pro-
duce higher detection accuracy. Our datasets are listed in
Table 1.
2.3 Our Methodology
We have two goals: evaluating the efﬁcacy of ML clas-
siﬁers to detect crowdturﬁng workers, and evaluating the
practical impact of adversarial attacks on ML classiﬁers.
• We analyze ground-truth data to identify key behav-
ioral features that distinguish crowdturﬁng worker
accounts from normal users (§3).
• We use these features to build a number of pop-
ular ML models, including Bayesian probabilistic
models via Bayes’ theorem (i.e. conditional prob-
ability), Support Vector Machines (SVMs), and al-
gorithms based on single or multiple decision trees
(e.g. Decision Trees, Random Forests) (§4).
• We evaluate ML models against adversarial attacks
ranging from weak to strong based on level of
knowledge by attackers (typically evasion attacks),
and coordinated attacks potentially guided by cen-
tralized administrators (possible poison or pollution
of training data).

3 Proﬁling Crowdturf Workers
We begin our study by searching for behavioral fea-
tures that distinguish worker accounts from normal users.
These features will be used to build ML detectors in §4.
User Proﬁle Fields. We start with user proﬁle fea-
tures commonly used as indicators of abnormal behav-
ior. These features include followee-to-follower ratio
(FFRatio), reciprocity (i.e. portion of user’s followees
who follow back), user tweets per day, account age, and
ratio of tweets with URLs and mentions.
Unfortunately, our data shows that most of these fea-
tures alone cannot effectively distinguish worker ac-
counts from normal users. First, FFRatio and reci-
procity are commonly used to identify malicious spam-

mers [4, 43, 50].
Intuitively, spammers follow a large
number of random users and hope for them to follow
back, thus they have high FFRatio and low reciprocity.
However, our analysis shows worker accounts have bal-
anced FFRatios, the majority of them even have more
followers than followees (Figure 2), and their reciprocity
is very close to those of normal users (Figure 3). Other
proﬁle features are also ineffective, including account
age, tweets per day, ratio of tweets with URLs and men-
tions. For example, existing detectors usually assume
attackers create many “fresh” accounts to spam [4, 43],
thus account age has potential. But we ﬁnd that more
than 75% of worker accounts in our dataset have been
active for at least one year.

These results show that crowd-worker accounts in
many respects resemble normal users, and are not eas-
ily detected by proﬁle features alone [47].
User Interactions. Next, we move on to features re-
lated to user interactions. The intuition is that crowdturf
workers are task-driven, and log on to work on tasks, but
spend minimal time interacting with others. User interac-
tions in Weibo are dominated by comments and retweets.
We perform analysis on both of them and ﬁnd consistent
results which show they are good metrics to distinguish
workers from non-workers. For brevity, we limit our dis-
cussion to results on comment interactions.

Figure 4 shows crowdturf accounts are less likely to
receive comments on their tweets. For 80% of crowdturf
accounts, less than 20% of their tweets are commented;
while for 70% of normal users, their ratio of commented
tweets exceeds 20%. This makes sense, as the fake con-
tent posted by crowdturf workers may not be interesting
enough for others to comment on. We also examine the
number of people that each user has bidirectional com-
ments with (bi-commentors). Crowdturf workers rarely
interact with other users, with 66% of accounts having at
most one bi-commentor.
Tweeting Clients. Next we look at the use of tweeting
clients (devices). We can use the “device” ﬁeld associ-
ated with each tweet to infer how tweets are sent. Tweet
clients fall into four categories: web-based browsers,
apps on mobile devices, third-party account management

242  23rd USENIX Security Symposium 

USENIX Association

s
t
e
e
w
T

 
l
a
t
o
T

 
f
o
 
%

 100

 80

 60

 40

 20

 0

Turfing Active

Auth.

)

%

(
 
s
r
e
s
U

 
f
o
 
F
D
C

 100

 80

 60

 40

 20

 0

Others
Share
Auto
Mobile
Web

Authenticated
Active
Turfing

 1
Max # consecutive tweets with all intervals < d

 100

 10

)

%

(
 
s
r
e
s
U

 
f

 

o
F
D
C

 100

 80

 60

 40

 20

 0

 0

 0.1

 0.2

Turfing
Active
Authenticated
 0.3

 0.4

Entropy

 0.5

 0.6

Figure 5: Tweet client usage.

Figure 6: Max size of tweeting burst
(threshold d = 1 minute).

Figure 7: Normalized entropy of
tweeting inter-arrival time.

Categ.
Web
Mobile
Auto
Share

Top Tweet Clients
Weibo Web, Weibo PC, 360Browser, Weibo Pro.
iPhone, Android, iPad, XiaoMi
PiPi, Good Nanny, AiTuiBao, Treasure Box
Taobao, Youku, Sina Blog, Baidu

Table 2: High-level categories for tweeting clients.

tools, and third-party websites via “share” buttons (Ta-
ble 2). Figure 5 shows key differences in how differ-
ent users use tweet clients. First, crowdturf workers
use mobile (10%) much less than normal users (36% −
46%). One reason is that crowdturf workers rely on web
browsers to interact with crowdturﬁng sites to get (sub-
mit) tasks and process payment, actions not supported by
most mobile platforms.
We also observe that crowdturf workers are more
likely to use automated tools. A close inspection shows
that workers use these tools to automatically post non-
spam tweets retrieved from a central content repository
(e.g. a collection of hot topics). Essentially, crowdturf
accounts use these generic tweets as cover trafﬁc for their
crowdturﬁng content. Third, crowdturf accounts “share”
from third-party websites more often, since that is a com-
mon request in crowdturﬁng tasks [48].
Temporal Behavior. Finally, we look at temporal char-
acteristics of tweeting behavior: tweet burstiness and pe-
riodicity. First, we expect task-driven workers to send
many tweets in a short time period. We look for poten-
tial bursts, where each burst is deﬁned as m consecutive
tweets with inter-arrival times < d. We examine each
user’s maximum burst size (m) with different time thresh-
olds d, e.g. Figure 6 depicts the result for d is set to 1
minute. We ﬁnd that crowdturf accounts are more likely
to post consecutive tweets within one-minute, something
rarely seen from normal users.
In addition, crowdturf
workers are more likely to produce big bursts (e.g. 10
consecutive tweets with less than one-minute interval).
Second, workers accept tasks periodically, which can
leave regular patterns in the timing of their tweets. We
use entropy to characterize this regularity [16], where

low entropy indicates a regular process while high en-
tropy indicates randomness of tweeting. We treat each
user’s tweeting inter-arrival time as a random variable,
and compute the ﬁrst-order entropy [16]. Figure 7 plots
user’s entropy, normalized by the largest entropy in our
dataset. Compared to normal users, crowdturf accounts
in general have lower entropy, indicating their tweeting
behaviors have stronger periodic patterns.
4 Detecting Crowdturﬁng Workers
We now use the features we identiﬁed to build a number
of crowdturﬁng detectors using machine learning mod-
els. Here, we summarize the set of features we use
for detection, and then build and evaluate a number of
machine-learning detectors using our ground-truth data.
4.1 Key Features
We chose for our ML detectors a set of 35 features across
ﬁve categories shown below.

total

reciprocity,

• Proﬁle Fields (9). We use 9 user proﬁle ﬁelds6 as
features: follower count, followee count, followee-
tweet count,
to-follower ratio,
tweets per day, mentions per tweet, percent of
tweets with mentions, and percent of tweets with
embedded URLs.
• User Interactions (8). We use 8 features based on
user interactions, i.e. comments and retweets. 4
features are based on user comments: percent of
tweets with comments, percent of all comments that
are outgoing, number of bi-commentors, and com-
ment h-index (a user with h-index of h has at least h
tweets each with at least h comments). We include
4 analogous retweet features.
• Tweet Clients (5). We compute and use the % of
tweets sent from each tweet client type (web, mo-
bile, automated tools, third-party shares and others)
as a feature.

6Although proﬁle ﬁelds alone cannot effectively detect crowdturf
accounts (§3), they are still useful when combined with other features.

USENIX Association  

23rd USENIX Security Symposium  243

Alg.
NB
BN
SVMr
SVMp Kernel degree d =3, Cost parameter C =50
J48
RF

Settings
Default
Default, K2 function
Kernel γ =1, Cost parameter C =100
Conﬁdence factor C =0.25, Instance/leaf M =2
20 trees, 30 features/tree

)

%

(
 

e

t

a
R

 
r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

 50
 40
 30
 20
 10
 0

False Positive
False Negative

(Auth.+Turfing)

(Active+Turfing)

R

F

J48

S

V
Mr

S

V

B

N

M
p

N

B

R

F

Algorithms

J48

S

V
Mr

M
p

S

V

B

N

N

B

Table 3: Classiﬁer conﬁgurations.

• Tweet Burstiness (12). These 12 features capture
the size and number of tweet bursts. A burst is m
consecutive tweets where gaps between consecutive
tweets are at most d minutes. For each user, we ﬁrst
compute the maximum burst size (m) while varying
threshold d from 0.5 to 1, 30, 60, 120, 1440. Then
we set d to 1 minute, and compute the number of
bursts while varying size m over 2, 5, 10, 50, 100,
and 500.
• Tweeting Regularity (1). This is the entropy value
computed from each user’s tweeting time-intervals.

4.2 Classiﬁcation Algorithms
With these features, we now build classiﬁers to detect
crowdturf accounts. We utilize a number of popular
algorithms widely used in security contexts, including
two Bayesian methods: Naive Bayesian (NB) [20] and
BayesNet (BN) [18]; two Support Vector Machine meth-
ods [33]: SVM with radial basis function kernel (SVMr)
and SVM with polynomial kernel (SVMp); and two
Tree-based methods: C4.5 Decision Tree (J487) [34] and
Random Forests (RF) [7]. We leverage existing imple-
mentations of these algorithms in WEKA [17] toolkits.
Classiﬁer and Experimental Setup.
We start by
constructing two experimental datasets, each contain-
ing all 28K turﬁng accounts, plus 28K randomly sam-
pled baseline users from the “authenticated” and “active”
sets. We refer to them as Authenticated+Turﬁng and Ac-
tive+Turﬁng.
We use a small sample of ground-truth data to tune the
parameters of different classiﬁers. At a high-level, we
use grid search to locate the optimized parameters based
on cross-validation accuracy. For brevity, we omit the
details of the parameter tuning process and give the ﬁnal
conﬁgurations in Table 3. Note that features are normal-
ized for SVM algorithms (we tested unnormalized ap-
proach which produced higher errors). We use this con-
ﬁguration for the rest of our experiments.
Basic Classiﬁcation Performance. We run each clas-
siﬁcation algorithm on both experimental datasets with

7J48 is WEKA’s C4.5 implementation.

Figure 8: Classiﬁcation error rates. Tree-based algo-
rithms and SVMs outperform Bayesian methods.

10-fold cross-validation.8 Figure 8 presents their classi-
ﬁcation error rates, including false positives (classifying
normal users as crowdturf workers) and false negatives
(classifying crowdturf accounts as normal users).
We make four key observations. First, the two sim-
ple Bayesian methods generally perform worse than
other algorithms. Second, Decision Tree (J48) and Ran-
dom Forests (RF) are more accurate than SVMs. This
is consistent with prior results that show SVMs excel
in addressing high-dimension problems, while Tree al-
gorithms usually perform better when feature dimen-
sionality is low (35 in our case) [8]. Third, Random
Forests outperform Decision Tree. Intuitively, Random
Forests construct multiple decision trees from training
data, which can more accurately model the behaviors of
multiple types of crowdturf workers [7]. In contrast, de-
cision tree would have trouble ﬁtting distinct types of
worker behaviors into a single tree. Finally, we observe
that the two experiment datasets show consistent results
in terms of relative accuracy across classiﬁers.
Comparing the two datasets, it is harder to differen-
tiate crowdturf workers from active users than from au-
thenticated users. This is unsurprising, since authenti-
cated accounts often represent accounts of public ﬁgures,
while active users are more likely to be representative
of the normal user population. In the rest of the experi-
ments, wherever the two datasets show consistent results,
we only present the results on Active+Turﬁng for brevity,
which captures the worse case accuracy for detectors.
4.3 Detecting Professional Workers
Our machine learning detectors are generally effective in
identifying worker accounts. However, the contribution
of tasks per worker is quite skewed, i.e. 90% of all tasks
are completed by the top 10% most active “professional”
workers (Figure 9). Intuitively, these “professional work-
ers” are easier to detect than one-time workers. By focus-
8Cross-validation is used to compare the performance of different
algorithms. We will split the data for training and testing the detectors
later.

244  23rd USENIX Security Symposium 

USENIX Association

)

%

(
 
s
k
s
a
T

 
f
o
 
F
D
C

 100

 80

 60

 40

 20

 0

 0

)

%

(
 
e
t
a
R

 
r
o
r
r

E
 
n
o
i
t
a
c
i
f
i
s
s
a
C

l

 12
 10
 8
 6
 4
 2
 0

0

 100

 40

 20
 80
Fraction of Top Workers (%)

 60

False Positive
False Negative
SVMp
SVMr
J48
RF

1

10

Workers that Finished > n Tasks

e
t
a
R
 
e
v
i
t
i
s
o
P
 
e
u
r
T

 1

 0.98

 0.96

 0.94

 0.92

 0.9

 0

 0.02

100

RF
SVMp
SVMr
J48

 0.04

 0.06

 0.08

 0.1

False Positive Rate

Figure 9: % of Tasks ﬁnished by top
% of workers. The majority of spams
were produced by top active workers.

Figure 10: Classifying different levels
of workers. Workers are ﬁltered by #
of crowdturﬁng tasks ﬁnished.

Figure 11: ROC curves of classi-
fying professional workers (workers
who ﬁnished more than 100 tasks).

ing on them, we can potentially improve detection accu-
racy while still effectively eliminate the largest majority
of crowdturf output.
We evaluate classiﬁer accuracy in detecting profes-
sional workers, by setting up a series of datasets each
consisting of workers who performed more than n tasks
(with n set to 1, 10, and 100). Each dataset also contains
an equal number of randomly sampled normal users. We
focus on the most accurate algorithms: Random Forests
(RF), Decision Tree (J48) and SVM (SVMr and SVMp),
and run 10-fold cross-validation on each of the datasets.
Figure 10 shows the classiﬁcation results on Ac-
tive+Turﬁng. As expected, our classiﬁers are more ac-
curate in identifying “professional” workers. Different
algorithms converge in accuracy as we raise the mini-
mum productivity of professional workers. Accuracy is
high for crowdturf workers who performed >100 tasks:
Random Forests only produce 1.2% false positive rate
and 1.1% false negative rate (99% accuracy). Note that
while these top workers are only 8.9% of the worker pop-
ulation, they are responsible for completing 90% of all
tasks. In the rest of the paper, we use “professional work-
ers” to refer to workers who have completed >100 tasks.
False Positives vs. False Negatives.
In practice, differ-
ent application scenarios will seek different tradeoffs be-
tween false positives (FP) and false negatives (FN). For
example, a system used as a pre-ﬁlter before more so-
phisticated tools (e.g. manual examination) will want to
minimize FN, while an independent system without ad-
ditional checks will want to minimize false positives to
avoid hurting good users.
Figure 11 shows the ROC9 curves of the four algo-
rithms on the dataset of professional workers. Again,
Random Forests perform best:
they achieve extremely
low false positive rate of <0.1% with only 8% false neg-
ative rate, or <0.1% false negative rate with only 7%
false positive rate. We note that SVMs provide better
false positive and false negative tradeoffs than J48, even
9ROC (receiver operating characteristic) is a plot that illustrates
classiﬁer’s false positives and true positives versus detection threshold.

though they have similar accuracy rates.
Imbalanced Data. We check our results on imbalanced
data, since in practice there are more normal users than
crowdturf workers. More speciﬁcally, we run our clas-
siﬁer (RF, professional) on imbalanced testing data with
turﬁng-to-normal ratio ranging from 0.1 to 1. Note that
we can still train our classiﬁers on balanced training data
since we use supervised learning (we make sure training
and testing data have no overlap). We ﬁnd all the classi-
ﬁers have accuracy above 98% (maximum FP 1.5%, FN
1.3%) against imbalanced testing data. We omit the plot
for brevity.
Summary.
Our results show that current ML sys-
tems can be used to effectively detect crowdturf workers.
While this is a positive result, it assumes no adversarial
response from the crowdturﬁng system. The following
sections will examine detection efﬁcacy under different
levels of adversarial attacks.
5 Adversarial Attack: Evasion
We show that ML detectors can effectively identify “pas-
sive” crowdturf accounts in Weibo. In practice, however,
crowdturﬁng adversaries can be highly adaptive:
they
will change their behaviors over time or can even in-
tentionally attack the ML detectors to escape detection.
We now re-evaluate the robustness of ML detectors un-
der different adversarial environments, focusing on two
types of adversaries:
1. Evasion Attack:

individual crowd-workers adjust
their behavior patterns to evade detection by trained
ML detectors.
2. Poisoning Attack: administrators of crowdturﬁng
sites participate, manipulating the ML detector
training process by poisoning the training data.

We focus on evasion attacks in this section, and de-
lay the study of poisoning attacks to §6. First, we deﬁne
the evasion attack model. We then implement evasion
attacks of different strengths, and study the performance

USENIX Association  

23rd USENIX Security Symposium  245

of ML detectors accordingly. Speciﬁcally, we consider
“optimal evasion” attacks, where adversaries have full
knowledge about the ML detectors and the Weibo sys-
tem, and more “practical” evasion attacks, where adver-
saries have limited knowledge about the detectors and
the Weibo system.
5.1 Basic Evasion Attack Model
Evasion attacks refer to individual crowdturﬁng workers
seeking to escape detection by altering their own behav-
ior to mimic normal users. For example, given knowl-
edge of a deployed machine learning classiﬁer, a worker
may attempt to evade detection by selecting a subset of
user features, and replacing their values with the median
of the observed normal user values. Since mimicking
normal users reduces crowdturﬁng efﬁciency, workers
are motivated to minimize the number of features they
modify. This means they need to identify a minimal core
set of features enabling their detection.10
This attack makes two assumptions. First, it assumes
that adversaries, i.e. workers, know the list of features
used by the classiﬁers. Technical publications, e.g. on
spam detection [4, 43, 50], make it possible for adver-
saries to make reasonable guesses on the feature space.
Second, it assumes that adversaries understand the char-
acteristics of normal users in terms of these features. In
practice, this knowledge can be obtained by crawling a
signiﬁcant portion of Weibo accounts.
Depending on their knowledge of the ML features and
of normal user behavior, adversaries can launch evasion
attacks of different strengths. We implement and evalu-
ate ML models on a range of threat models with vary-
ing levels of adversary knowledge and computational ca-
pabilities. We start from the optimal evasion scenario,
where adversaries have complete knowledge of the fea-
ture set. The corresponding ML detector results repre-
sent worst-case performance (or lower bound) under eva-
sion attacks. We also study a set of practical evasion
models where adversaries have limited (and often noisy)
knowledge, and constrained resources.
5.2 Optimal Evasion Attack
In this ideal case, adversaries have perfect knowledge
about the set of features they need to modify. To un-
derstand the impact of the feature choices, we look at
multiple variants of the optimal evasion models. These
include the per-worker optimal evasion model, where
each worker ﬁnds her own optimal set of features to alter,
the global optimal evasion where all workers follow the
same optimal set of features to alter, and feature-aware
evasion where workers alter the most important features.
10For simplicity, we consider features to be independent.

this optimal evasion model

We implement these evasion models on our ground-truth
dataset, and evaluate ML detector accuracy. Note that
these attacks we identify are not necessarily practical,
but are designed to explore worse-case scenarios for ML
models.
Per-worker Optimal Evasion.
Intuitively, each
worker should have her own optimal strategy to alter
features, e.g. some workers need to add followers ﬁrst,
while others need to reduce tweeting burstiness. Doing
so is hard in practice: each worker has to apply exhaus-
tive search to identify its optimal strategy that minimizes
the set of features to modify.
We implement this scenario on our Active+Turﬁng
dataset. We ﬁrst split the data into equal-sized training
and testing datasets, and use the top-4 most accurate al-
gorithms to build classiﬁers with authentic training data.
We then run detection on worker accounts in the testing
dataset. Here for each worker, we exhaustively test all
combinatorial combinations of possible features to mod-
ify until the classiﬁer classiﬁes this worker as normal. In
this way, we ﬁnd the minimal set of features each user
must modify to avoid detection.
Figure 12(a) plots the evasion rate for the four ML
algorithms. Clearly,
is
highly effective. By simply altering one feature, 20-50%
of workers can evade detection (different workers can
choose to alter different features). And by altering ﬁve
features, 99% of workers can evade all four classiﬁers.
We also observe that the Random Forests (RF) algorithm
achieves the best robustness, since it requires the most
number of features to be altered.
Global Optimal Evasion.
The per-worker model
makes a strong assumption that each worker can iden-
tify her own optimal feature set. Next, we loosen this
assumption and only assume that all workers exercise a
uniform strategy. This is possible if a third-party (e.g.
site admin) guides workers in altering their features.
To identify the global optimal strategy, we search ex-
haustively through all possible feature combinations, and
locate the feature set (for a given size) that allows the ma-
jority of workers to achieve evasion. The corresponding
evasion rate result is in Figure 12(b). 99% of workers
can successfully evade all four detectors by altering 15
features, which is much larger than the per-worker case
(5 features). This is because any one-size-ﬁts-all strat-
egy is unlikely to be ideal for individual workers, thus
the feature set must be large enough to cover all workers.
Feature-aware Evasion. Achieving optimal evasion is
difﬁcult, since it requires adversaries to have knowledge
of the trained classiﬁers. Instead, this model assumes that
adversaries can accurately identify the relatively “impor-
tance” of the features. Thus workers alter the most im-
portant features to try to avoid detection.

246  23rd USENIX Security Symposium 

USENIX Association

)

%

(
 

o

i
t

 

a
R
n
o
s
a
v
E

i

 
r
e
k
r
o
W

 100

 80

 60

 40

 20

 0

 0

 1

J48
SVMr
SVMp
RF
 4

# of Features Changed

 2

 3

)

%

(
 

o

i
t

 

a
R
n
o
s
a
v
E

i

 
r
e
k
r
o
W

 5

 100

 80

 60

 40

 20

 0

 0

 5

J48
RF
SVMr
SVMp
 25
 10
# of Features Changed

 15

 20

)

%

(
 

o

i
t

 

a
R
n
o
s
a
v
E

i

 
r
e
k
r
o
W

 30

 35

 100

 80

 60

 40

 20

 0

 0

 5

RF
J48
SVMr
SVMp
 25
 10
# of Features Changed

 15

 20

 30

 35

(a) Per-worker Optimal Evasion

(b) Global Optimal Evasion

(c) Feature Importance Aware Evasion

Figure 12: Evasion rate of optimal evasion strategies (all workers).

We implement this attack by building the classiﬁers
and then computing the feature importance. For this we
use the χ2 (Chi Squared) statistic [51], a classic metric
to measure feature’s discriminative power in separating
data instances of different classes11. During detection,
workers alter features based on their rank.
Figure 12(c) plots evasion results for the four classi-
ﬁers. We make two key observations. First, this feature-
aware strategy is still far away from the per-worker op-
timal case (Figure 12(a)), mostly because it is trying
to approximate global optimal evasion. Second, perfor-
mance depends heavily on the underlying classiﬁer. For
RF and J48, performance is already very close to that
of the global optimal case, while the two SVM algo-
rithms are more resilient. A possible explanation is that
the χ2 statistic failed to catch the true feature importance
for SVM, since SVM normalizes feature values before
training the classiﬁer. These results suggest that without
knowing the speciﬁc ML algorithm used by the defend-
ers, it is hard to avoid detection even knowing the impor-
tance of features.

5.3 Evasion under Practical Constraints
Our results show workers can evade detection given com-
plete knowledge of the feature set and ML classiﬁers.
However, obtaining complete knowledge is very difﬁcult
in practice. Thus we examine practical evasion threat
models to understand their efﬁcacy compared to optimal
evasion models. We identify practical constraints facing
adversaries, present several practical threat models and
evaluate their impact on our detectors.
Practical Constraints.
In practice, adversaries face
two key resource constraints. First, they cannot reverse-
engineer the trained classiﬁer (i.e.
the ML algorithm
used or its model parameters) by querying the classiﬁer
and analyzing the output – it is too costly to establish
millions of proﬁles with controlled features and wait for
some of them to get banned. Thus workers cannot per-
11We also tested information gain to rank features, which produced

similar ranking results (i.e. the same top-10 as using χ2).

form exhaustive search to launch optimal evasion attacks,
but have to reply on their partial knowledge for evasion.
Second, it is difﬁcult for adversaries to obtain complete
statistics of normal users. They can estimate normal user
statistics via a (small) sampling of user proﬁles, but esti-
mation errors are likely to reduce their ability to precisely
mimic normal users.
Next, we will examine each constraint separately, and
evaluate the likely effectiveness of attacks under the
more realistic conditions.
Distance-aware Evasion. We consider the ﬁrst con-
straint which forces workers to rely on partial knowl-
edge to guide their evasion efforts. In this case, individ-
ual workers are only aware of their own accounts and
normal user statistics. When choosing features to alter,
they can prioritize features with the largest differential
between them and normal users. They must quantify the
“distance” between each crowdturf account and normal
users on a given feature. Here, we pick two very intu-
itive distance metrics and examine the effectiveness of
the corresponding evasion attacks. For now, we ignore
the second constraint by assuming workers have perfect
knowledge of average user behaviors.

• Value Distance (VD): Given a feature k, this cap-
tures the distance between a crowd-worker i and
|Fk (i)−Median(Nk )|
normal user statistics byVD(i,k) =
Max(Nk )−Min(Nk )
where Fk(i) is the value of feature k in worker i, and
Nk is normal user statistical distribution on feature
k. When altering feature k, worker i replaces Fk(i)
with Median(Nk).
• Distribution Distance (DD): Here the distance
depends on where Fk(i) is positioned within
if Fk(i) is around 50%-
Nk.
tile of Nk,
is similar to a nor-
mal user. Therefore, we deﬁne the distance by
DD(i,k) = |Percentile(Nk ,Fk(i)) − 50|/100 where
Percentile(Nk ,Fk(i)) is the percentile of Fk(i) in the
normal user CDF Nk. Note that when Fk(i) exceeds
the range of Nk, this distance metric becomes in-
valid. However, our data suggests that this rarely
happens (<1%).

then worker i

For example,

USENIX Association  

23rd USENIX Security Symposium  247

)

%

(
 

o

i
t

 

a
R
n
o
s
a
v
E

i

 
r
e
k
r
o
W

 100

 80

 60

 40

 20

 0

 0

 5

RF
J48
SVMr
SVMp
 25
 10
# of Features Changed

 15

 20

)

%

(
 

o

i
t

 

a
R
n
o
s
a
v
E

i

 
r
e
k
r
o
W

 30

 35

 100

 80

 60

 40

 20

 0

 0

 5

SVMp
SVMr
J48
RF

 15

 10
 25
# of Features Changed

 20

)

%

(
 

o

i
t

 

a
R
n
o
s
a
v
E

i

 
r
e
k
r
o
W

 30

 35

 100

 80

 60

 40

 20

 0

 0

 5

SVMp
SVMr
RF
J48
 25
 10
# of Features Changed

 15

 20

 30

 35

(a) Random Evasion Strategy (Random)

(b) Value Distance Aware Strategy (VD)

(c) Distribution Distance Aware Strategy (DD)

Figure 13: Evasion rate of practical evasion strategies (all workers).

)

%

(
 
o
i
t
a
R
 
n
o
s
a
v
E

i

 
r
e
k
r
o
W

 100

 80

 60

 40

 20

 0

 0

 5

l

e
u
a
V

 
.
d
e
M
 
d
e
t
a
m

i
t
s
E

 
f
o
 
e

l
i
t
n
e
c
r
e
P

 100

 80

 60

 40

 20

 0

 30

 35

0.001% sample
0.01% sample
0.1% sample

 0

 5

 10

 20

 15
Feature Index

 25

 30

 35

)

%

(
 
o
i
t
a
R
 
n
o
s
a
v
E

i

 
r
e
k
r
o
W

 100

 80

 60

 40

 20

 0

 0

 5

delta=5%
delta=10%
delta=15%
delta=20%
delta=25%
 20

 15

 10
 25
# of Features Changed

 30

 35

J48
SVMp
RF
SVMr
 25
 10
# of Features Changed

 15

 20

Figure 14: Evasion rate using distri-
bution distance aware strategy (DD)
for professional workers.

Figure 15: The percentile of esti-
mated median value in the true nor-
mal user CDF.

Figure 16: Impact of median value es-
timation error on evasion rate, using
DD evasion on SVMp.

To evaluate the impact of practical evasion attacks,
we split the Active+Turﬁng data into equal-sized train-
ing and testing sets. After classiﬁer training, we sim-
ulate the distance-aware evasion attacks on the testing
data. Figure 13(b) and 13(c) show evasion rates based on
VD and DD respectively. As a baseline, we also show
Figure 13(a) where adversaries randomly select features
to alter. Compared to random evasion, distance-based
approaches require much less feature altering. For ex-
ample, when altering 15 features, random approach only
saves <40% of workers, while distance strategies pro-
vide as high as 91% (VD-SVMp) and 98% (DD-SVMp).
The four classiﬁers perform very differently. RF and
J48 classiﬁers are much more vulnerable to DD based
evasion than to VD based evasion. While SVMs perform
similarly in both strategies.
In general, Tree-based al-
gorithms are more robust than SVM classiﬁers against
distance-aware evasions. This is very different to what
we observed in the optimal evasion cases (Figure 12(a)–
12(b)), where SVMs are generally more robust. This
suggests that theoretical bounds on ML algorithms may
not truly reﬂect their performance in practice.

Consistently, the impact of practical evasion attacks
is much weaker than that of optimal evasion (i.e. per-
worker optimal). Adversaries are severely constrained by
lack of knowledge of detection boundaries of the classi-
ﬁers, and have to guess based on “distance” information.
The implication is that the less adversaries know about
classiﬁers, the harder it is for them to evade detection.

We also evaluate the attack impact on classiﬁers to de-
tect professional workers. We ﬁnd the general trends are
similar and only show the results of DD-based attack in
Figure 14. We note that it is easier to evade classiﬁers
dedicated to detect professionals (compared with Fig-
ure 13(c)). This is because when trained to a smaller,
more homogeneous worker population, classiﬁers expect
strong malicious behaviors from crowd-workers. Thus
even a small deviation away from the model towards nor-
mal users will help achieve evasion.
Impact of Normal User Estimation Errors. We ex-
tend the above model by accounting for possible errors in
estimating normal user behaviors (the second constraint).
These errors exist because adversaries can only sample
a limited number of users, leading to noisy estimations.
Here, we investigate the impact of sampling strategies on
the attack efﬁcacy.

For all 35 features, we vary the sampling rate, i.e. the
ratio of normal users sampled by adversaries, by taking
random samples of 0.001%, 0.01% to 0.1% of the Active
dataset. We repeat each instance 100 times, and com-
pute the mean and standard deviation of the estimated
median feature values (Figure 15). We show each fea-
ture’s percentile in the true CDF of the Active dataset. In
this case, the optimal value is 50%. Clearly sampling rate
does impact feature estimation. With the 0.001% sam-
pling rate, the estimated value varies signiﬁcantly across
instances. Raising the sample rate to 0.1% means attack-
ers can accurately estimate the median value using only

248  23rd USENIX Security Symposium 

USENIX Association

a few instances. Furthermore, we see that burstiness fea-
tures (e.g. features 30-34) are easy to sample, since nor-
mal user values are highly skewed to zero.
Finally, we evaluate the impact of estimation errors
on practical evasion attacks. This time we run distance-
aware evasions based on the estimated median feature
values. For each worker’s feature k, we estimate the me-
dian value M′(k) with a given bound of error ∆. That
is, M′(k) is randomly picked from the percentiles within
[50% − ∆,50% + ∆] on the true CDF of normal user be-
haviors. By iterating through different ∆ (from 5% to
25%), our results show that ∆ only has a minor impact.
The most noticeable impact is on SVMp using DD dis-
tance (Figure 16). Overall, we conclude that as long as
adversaries can get a decent guess on normal user be-
haviors, the residual noise in the estimation ∆ should not
affect the efﬁcacy of evasion attacks.
Summary. Our work produces two key observations.
• Given complete knowledge, evasion attacks are
very effective. However, adversaries under more re-
alistic constraints are signiﬁcantly less effective.
• While no classiﬁer is robust against all attack sce-
narios, there is a consistent inverse relationship be-
tween single model ﬁtting accuracy and robustness
to adversarial evasion. Highly accurate ﬁt to a
smaller, more homogeneous population (e.g. pro-
fessionals) makes models more vulnerable to eva-
sion attacks.

6 Adversarial Attack: Poisoning
After examining evasion attacks, we now look at how
centralized crowdturﬁng sites can launch more powerful
attacks to manipulate machine learning models. Speciﬁ-
cally, we consider the poisoning attack where administra-
tors of crowdturﬁng sites intentionally pollute the train-
ing dataset used to build ML classiﬁers, forcing defend-
ers to produce inaccurate classiﬁers. Since the training
data (i.e. crowdturﬁng accounts) actually comes from
these crowdturﬁng sites, administrators are indeed capa-
ble of launching these attacks.
In the following, we examine the impact of poison-
ing attacks on ML detection accuracy. We consider two
mechanisms for polluting training data. The ﬁrst method
directly adds misleading/synthetic samples to the train-
ing set. Adversaries in the second method simply alter
data records, or modify operational policies to alter the
composition of the training data used by ML models.
6.1
Perhaps the simplest way to pollute any training data is
to add misleading or false samples. In our case, since

Injecting Misleading Samples

the training data has two classes (groups) of accounts,
this can be done by mixing normal user samples into the
“turﬁng” class, i.e. poisoning the turﬁng class, or mix-
ing crowdturf samples into the “normal” user class, i.e.
poisoning the normal class. Both introduce incorrectly
labeled training data to mislead the classiﬁer.
Poisoning Turﬁng Class. To poison the turﬁng class,
adversaries (e.g. ZBJ and SDH administrators) add nor-
mal Weibo accounts to the public submission records in
their own systems. Since ML classiﬁers take ground-
truth crowdturf accounts from those public records, these
benign accounts will then be mixed into the training data
and labeled as “turﬁng.” The result is a model that marks
some characteristics of normal users as crowdturﬁng be-
havior, likely increasing false positive rate in detection.
We simulate the attack with our ground-truth dataset.
At a high level, we train the classiﬁers on “polluted”
training data, and then examine changes in classiﬁers’
detection accuracy. Here we experiment with two strate-
gies to pollute the turﬁng class. First, as a baseline strat-
egy, adversaries randomly select normal users as poison
samples to inject into the turﬁng class. Second, adver-
saries can inject speciﬁc types of normal users, causing
the classiﬁers to produce targeted mistakes.
We simulate this poisoning at-
Random Poisoning:
tack with Active+Turﬁng dataset, where adversaries in-
ject random normal accounts to the turﬁng class. Specif-
ically, for training, the turﬁng class (14K accounts) is a
mixture of crowdturf accounts and poison samples ran-
domly selected from Active, with a mixing ratio of p.
The normal class is another 14K normal accounts from
Active. Then we use 28K of the rest accounts (14K turf-
ing and 14K normal users) for testing. For any given p,
we repeat the experiment 10 times with different random
poison samples and training-testing partitions to compute
average detection rates.
Results are shown in Figure 17(b). As a baseline com-
parison, we also present the results of the classiﬁers for
professional workers in Figure 17(a). We have three ob-
servations. First, as poison-to-turﬁng ratio p increases,
false positive rates go up for all four algorithms. False
negative rates are not much affected by this attack, thus
are omitted from the plot.12 Second, we ﬁnd that the
SVM classiﬁers are more resilient: SVMp’s false posi-
tive rate increases <5% as p approaching 1.0, while the
analogous increases exceed 10% for Random Forests and
J48. Particularly, J48 experiences more drastic ﬂuctua-
tions around average, indicating it is very sensitive to the
choice of poison samples. This is consistent with our
prior observation that more accurate single model ﬁtting
(i.e. J48 is more accurate than SVM) is more vulnerable
to adversarial attacks. Similarly, highly accurate detec-
12False negative rates increase < 2% when p approaches 1.0.

USENIX Association  

23rd USENIX Security Symposium  249

J48
RF
SVMr
SVMp

)

%

(
 

e

t

 

a
R
e
v
i
t
i
s
o
P
e
s
a
F

 

l

 25
 20
 15
 10
 5
 0

 0.01

 0.1

 1

Ratio of Poison-to-Turfing
(a) Professional Workers

)

%

(
 

e

t

 

a
R
e
v
i
t
i
s
o
P
e
s
a
F

 

l

 25
 20
 15
 10
 5
 0

J48
RF
SVMr
SVMp

)

%

(
 

e

t

 

a
R
e
v
i
t

 

a
g
e
N
e
s
a
F

l

 1

 0.01

 0.1

Ratio of Poison-to-Turfing
(b) All Workers

 30
 25
 20
 15
 10
 5
 0

SVMr
SVMp
J48
RF

)

%

(
 

e

t

 

a
R
e
v
i
t

 

a
g
e
N
e
s
a
F

l

 0.01

 0.1

 1

Ratio of Poison-to-Normal
(a) Professional Workers

 30
 25
 20
 15
 10
 5
 0

SVMr
SVMp
J48
RF

 0.01

 0.1

Ratio of Poison-to-Normal
(b) All Workers

 1

Figure 17: Poisoning training dataset by injecting random
normal user samples to the turﬁng class.

Figure 19: Poisoning training dataset by adding turﬁng
samples to normal class.

)

%

(
 
e

t

 

a
R
e
v
i
t
i
s
o
P
e
s
a
F

 

l

 45
 40
 35
 30
 25
 20
 15
 10
 5
 0

J48
RF
SVMr
SVMp

 0.01

 0.1

Ratio of Poison-to-Turfing

)

%

(
 
e

t

 

a
R
e
v
i
t
i
s
o
P
e
s
a
F

 

l

 45
 40
 35
 30
 25
 20
 15
 10
 5
 0

 1

J48
RF
SVMr
SVMp

 0.01

 0.1

Ratio of Poison-to-Turfing

 1

(a) Injecting Accounts with > 50%
tweets commented
Figure 18: Targeted poisoning. Adversaries inject speciﬁc
type of normal users to the turﬁng class (all workers).

(b) Injecting Accounts with < 150
followers

tion of the more homogeneous population of professional
workers (§4) means the models experience larger rela-
tive impacts from attacks compared to classiﬁers over all
workers.
Note that we limited the poison-to-turﬁng ratio <1,
since in practice adversaries cannot inject unlimited poi-
son samples to defender’s training data. First, injecting
noise causes inconvenience to their own customers in lo-
cating qualiﬁed workers. Second, defenders may collect
ground-truth records from multiple crowdturﬁng sites.
Next, we explore targeted poi-
Targeted Poisoning:
soning to the turﬁng class. Here the adversaries want
to carefully inject selected poison samples so classiﬁers
make targeted mistakes. For example, our classiﬁer uses
“ratio of commented tweets” as a feature with the intu-
ition that worker’s tweets rarely receive comments (§3).
Once adversaries gain this knowledge, they can inten-
tionally select accounts whose tweets often receive com-
ments as the poison samples. As a result, the trained
classiﬁer will mistakenly learn that users with high com-
ment ratio can be malicious, thus are likely to misclassify
this kind of normal users as crowd-workers.
To evaluate the impact of targeted poisoning, we per-
form similar experiments, except that we select poison
samples based on speciﬁc feature. Figure 18 shows the
attacking results on two example features: ratio of tweets
with comments and follower count. Compared with Fig-
ure 17, targeted poisoning can trigger higher false posi-

tives than randomly selecting poison samples. Also, the
previous observations still hold with SVM being more
robust and J48 experiencing unstable performance (large
deviation from average).
Poisoning Normal User Class. Next, we analyze the
other direction where adversaries inject turﬁng samples
into the “normal” class to boost the false negative rate
of classiﬁers. This may be challenging in practice since
the normal user pool – Weibo’s whole user population –
is extremely large. Hence it requires injecting a signiﬁ-
cant amount of misleading samples in order to make an
impact. Here from defender’s perspective, we aim to un-
derstand how well different classiﬁers cope with “noisy”
normal user data.
We repeat the previous “Random Poisoning” attack on
the normal class. Figure 19(a) and Figure 19(b) show
the attack results on classiﬁers for professional workers
and all workers respectively. As we increase the ratio of
poison samples, the false negatives of all four classiﬁers
increase. This is expected as the classiﬁers will mistak-
enly learn crowdturf characteristics when modeling nor-
mal users, thus are likely to misclassify turﬁng accounts
as benign later. In addition, we ﬁnd the robustness of dif-
ferent classiﬁers varies, with Random Forests algorithm
producing the lowest overall false negatives. Finally, we
again observe that the more accurate classiﬁer for profes-
sional workers suffers larger relative impacts from adver-
saries than classiﬁers for all-workers.

6.2 Altering Training Data
The above poisoning attacks focus on misleading classi-
ﬁers to catch the wrong target. However, it does not fun-
damentally prevent crowd-workers from detection, since
workers’ behavior patterns are still very differently from
normal users. To this end, we explore a second poison-
ing attack, where adversaries directly alter the training
data by tuning crowd-workers’ behavior to mimic normal
users. The goal is to make it difﬁcult (or even impossi-
ble) to train an accurate classiﬁer that isolates crowdturf
accounts with normal accounts.

250  23rd USENIX Security Symposium 

USENIX Association

)

%

(
 

t

e
a
R

 
r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

 16
 14
 12
 10
 8
 6
 4
 2
 0

False Positive
False Negative

)

%

(
 

t

e
a
R

 
r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

Entropy

Burst

None

Interact

Profile
Client
Features Attacked
(a) Random Forests

 16
 14
 12
 10
 8
 6
 4
 2
 0

False Positive
False Negative

)

%

(
 

t

e
a
R

 
r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

Burst

Entropy

None

Interact

Profile
Client
Features Attacked
(b) J48

 16
 14
 12
 10
 8
 6
 4
 2
 0

False Positive
False Negative

)

%

(
 

t

e
a
R

 
r
o
r
r

 

E
n
o

i
t

a
c
i
f
i
s
s
a
C

l

Burst

Entropy

None

Interact

Profile
Client
Features Attacked
(c) SVMr

 16
 14
 12
 10
 8
 6
 4
 2
 0

False Positive
False Negative

Burst

Entropy

None

Interact

Profile
Client
Features Attacked
(d) SVMp

Figure 20: Performance of different classiﬁers when adversaries alter crowd-workers’ features to mimic normal users.
The horizontal lines represent the baseline false positive (false negative) rates when no feature is altered.

To carry out this attack, adversaries (e.g. administra-
tors of ZBJ and SDH) need to modify the behaviors of
numerous crowdturf workers. This can be done by cen-
trally enforcing operational policies to their own system.
For example, enforcing minimal time interval between
taking tasks to reduce the tweeting burstiness or enforc-
ing screening mechanisms to reject worker accounts with
“malicious” proﬁle features. In the following, we evalu-
ate the attack impact using simulations, followed by the
discussion of practical costs.
Feature Altering Attack. To simulate this attack, we
let adversaries select a set of features F of crowdturf ac-
counts and alter F to mimic the corresponding features
of normal users. Unlike evasion attacks that can sim-
ply mimic normal users’ median values, here we need
to mimic the whole distribution in order to make the two
classes indistinguishable on these features. Since the fea-
ture altering is for all workers in the crowdturﬁng sys-
tem, thus it actually applies to crowdturf accounts in both
training and testing datasets. Finally, note that features
are not completely independent, i.e. changing one fea-
ture may cause changes in others. To mitigate this, we
tune features under the same category simultaneously.

Figure 20 shows the attack results on Turﬁng+Active
dataset. We attack each feature category and repeat the
experiment for 10 times. Here we simulate attacking one
category at a time, and will discuss attacking category
combinations later. In general, the attack makes all clas-
siﬁers produce higher error rates compared with baseline
where no feature is altered (the horizontal lines). How-
ever the impact is mild compared to injection-based poi-
soning attacks. For example, the most effective attack is
on J48 when altering interaction features, which causes
error rate increased by 4%, while injection-based attack
can boost error rate by more than 20% (Figure 18). One
possible reason is that unlike injection-based poisoning,
altering-based poisoning does not cause inconsistencies
in training and testing data, but only make the two classes
harder to separate.
Costs of Altering.
In practice, feature altering comes
with costs, and some features may be impossible to ma-

Features
Attacked
None
C+B
B+E
C+E
C+B+E

RF

(6.2, 3.4)
(5.7, 4.4)
(6.5, 3.9)
(6.4, 4.5)
(5.8, 4.2)

J48

Error Rate (FP %, FN %)
SVMr
(7.7, 10.1)
(6.7, 6.8)
(7.9, 8.7)
(8.7, 12.2)
(7.1, 7.8)
(8.7, 12.5)
(7.9, 8.2)
(7.5, 11.8)
(8.6, 13.2)
(8.3, 8.5)

SVMp
(7.9, 12.1)
(8.0, 14.0)
(7.3, 13.1)
(6.3, 13.8)
(7.7, 15.2)

Table 4: Error rates when features are altered in combi-
nations. We focus on attacking low-cost features: Tweet
Client (C), Burstiness (B) and Entropy (E).

nipulate even by crowdturﬁng administrators. For in-
stance, Tweeting Regularity (Entropy) and Burstiness
features are easier to alter. Recall that crowdturﬁng sys-
tems can enforce minimal (random) time delay between
workers taking on new tasks, or use delays to increase
entropy. Changing the Tweet Client feature is also pos-
sible, since crowdturﬁng systems can develop mobile
client software for their workers, or simply release tools
for workers to fake their tweeting clients.
Proﬁle and Interaction features are more difﬁcult to al-
ter. Some features are mandatory for common tasks. For
example, workers need to maintain a certain number of
followers in order to spread spam to reach large enough
audiences. In addition, some features are rooted in the
fact that crowd-workers don’t use their accounts organ-
ically, which, making it hard to generate normal user
interactions. Although, crowdturﬁng systems could po-
tentially use screening mechanisms to reject obviously-
malicious crowdturf accounts from their system. How-
ever, this high bar will greatly shrink the potential worker
population, and likely harm the system’s spam capacity.
Considering practical costs, we consider whether it is
more impactful to alter the combinations of features from
different categories. Here we focus on altering the low
cost features in Tweet Client (C), Burstiness (B) and En-
tropy (E). As shown in Table 4, attacking feature combi-
nations produces slightly higher error rates than attack-
ing a single feature category, but the overall effect is still
small (less than 4% error rate increase).

USENIX Association  

23rd USENIX Security Symposium  251

Summary and Discussion. Through our analysis, we
ﬁnd that injecting misleading samples into training data
causes more signiﬁcant errors than uniformly altering
worker behavior. In addition, we again observe the in-
verse relationship between single model ﬁtting accuracy
and robustness.
To protect their workers, crowdturﬁng sites may also
try to apply stronger access control to their public records
in order to make training data unavailable for ML detec-
tors13. However, this creates obvious inconvenience for
crowdturﬁng sites, since they rely on these records to at-
tract new workers. Moreover, even if records were pri-
vate, defenders can still obtain training data by joining as
“customers,” offering tasks, and identifying accounts of
participating workers.

7 Related Work
Crowdturﬁng.
Prior works used measurements on
crowdturﬁng sites to understand their operation and eco-
nomic structure [23, 24, 26, 48].
Some systems have
been developed to detect paid human spammers in on-
line review sites [31] and Q&A systems [9, 45]. To the
best of our knowledge, our work is the ﬁrst to explore de-
tection of crowdturﬁng behaviors in adversarial settings.
OSN Spammer Detection.
Researchers have de-
veloped mechanisms to detect fake accounts (Sybil)
and spam campaigns in online social networks, includ-
ing Facebook [15, 49], Twitter [43], Renren [52] and
LinkedIn [46]. Most prior works develop ML models
using features of spammer proﬁles (e.g. FFRatio, black-
listed URLs) or bot-like behaviors [4, 11, 42, 47, 50].
However, a recent study shows dedicated spam bots
can still inﬁltrate social networks without being de-
tected [14].
In our case, crowdturf accounts are care-
fully maintained by human users, and their questionable
activities are camouﬂaged with synthetic cover trafﬁc.
This makes their detection challenging, until we add ad-
ditional behavioral features (e.g. user-interaction, task-
driven behavior).
Adversarial Machine Learning.
In an early
study [19], researchers classify ML adversarial attacks
into two high-level categories: causative attacks where
adversaries alter the training process to damage the clas-
siﬁer performance, and exploratory attacks where ad-
versaries try to circumvent an already-trained classi-
ﬁer. Much of existing work focuses on exploratory at-
tacks [5, 12, 25, 28] with less focusing on causative at-
tacks [6, 37], since it’s usually more difﬁcult for adver-
saries to access training data in practice. In this paper, we
13As of late 2013, some crowdturﬁng sites (e.g. ZBJ) have already
started to follow this direction, by limiting access to public transaction
records to veriﬁed active participants.

studied both angles as both attacks are practically feasi-
ble from crowdturﬁng adversaries.
Several studies have examined attacks on speciﬁc ML-
based applications, from email spam detection [12] to
network intuition detection [37, 40, 44] to malicious
(PDF) ﬁle classiﬁcation [5, 25, 41] and malware detec-
tion [21]. Our work focuses on crowdturﬁng and ex-
plores a wider range of adversarial attacks, including ac-
tive evasion and more powerful poison attacks against the
model training process.

8 Conclusion and Discussion
We use a large-scale ground truth dataset to develop ma-
chine learning models to detect malicious crowdsourcing
workers. We show that while crowdturﬁng workers re-
semble normal users in their proﬁles, ML models can ef-
fectively detect regular workers (95% accuracy) or “pro-
fessionals” (99% accuracy) using distinguishing features
such as user interactions and tweet dynamics.
More importantly, we use crowdturﬁng defense as
context to explore the robustness of ML algorithms
against adversarial attacks. We evaluate multiple adver-
sarial attack models targeting both training and testing
phases of ML detectors. We ﬁnd that these attacks are
effective against all machine learning algorithms, and co-
ordinated attacks (such as those possible in crowdturﬁng
sites) are particularly effective. We also note a consistent
tradeoff where more accurate ﬁts (especially to a smaller,
more homogeneous population) result in higher vulner-
ability to adversarial attacks. The exception appears to
be Random Forests, which often achieves both high ac-
curacy and robustness to adversaries, possibly due to its
natural support for multiple populations.
Limitations and Future Work.
We note that our
study has several limitations. First, our analysis focuses
on Weibo, and our adversary scenarios may not gener-
alize fully to other platforms (e.g. review sites, instant
message networks). However, more work is necessary
to validate our ﬁndings on other platforms. Second, our
adversarial models use simplifying assumptions, i.e. fea-
tures are independent and costs for feature modiﬁcation
are uniform.
In addition, attackers may behave differ-
ently to disrupt the operation of ML detectors.
Moving forward, one goal is to validate our adversar-
ial models in practice, perhaps by carrying out a user-
study on crowdturﬁng sites where we ask workers to ac-
tively evade and disrupt ML detectors. In addition, our
results show we must explore approaches to improve the
robustness of ML-based systems. Our analysis showed
that ML algorithms react differently to different adver-
sarial attacks. Thus one possible direction is to develop
hybrid systems that integrate input from multiple classi-

252  23rd USENIX Security Symposium 

USENIX Association

ﬁers, ideally without affecting overall accuracy. We also
observe that limiting adversaries’ knowledge of the tar-
get system can greatly reduce their attack abilities. How
to effectively prevent adversaries from gaining knowl-
edge or reverse-engineering models is also a topic for
future work.
9 Acknowledgments
We would like to thank the anonymous reviewers for
their helpful feedback, and Xifeng Yan for insightful
discussions. This work is supported in part by NSF
grants IIS-1321083, CNS-1224100, IIS-0916307, by the
DARPA GRAPHS program (BAA-12-01), and by the
Department of State. Any opinions, ﬁndings, and con-
clusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the
views of the funding agencies.
References
[1] Sina Weibo Admin Statement on Spam. http://www.

weibo.com/p/1001603697836242954625,
April 2014.

[2] Sina Weibo Terms of Service. http://service.
2014.

account.weibo.com/roles/guiding,
(The link is accessible after login).

[3] AHN, Y.-Y., HAN, S., KWAK, H., MOON, S., AND
JEONG, H. Analysis of topological characteristics of
huge online social networking services. In Proc. of WWW
(2007).

[4] BENEVENUTO, F., MAGNO, G., RODRIGUES, T., AND
ALMEIDA, V. Detecting spammers on twitter. In Proc. of
CEAS (2010).

[5] BIGGIO, B., CORONA, I., MAIORCA, D., NELSON, B.,
SRNDIC, N., LASKOV, P., GIACINTO, G., AND ROLI, F.
Evasion attacks against machine learning at test time. In
Proc. of ECML PKDD (2013).

[6] BIGGIO, B., NELSON, B., AND LASKOV, P. Poisoning
attacks against support vector machines. In Proc. of ICML
(2012).

[7] BREIMAN, L. Random forests. Machine Learning 45, 1

(2001), 5–32.

[8] CARUANA, R., KARAMPATZIAKIS, N., AND YESSE-
NALINA, A. An empirical evaluation of supervised learn-
ing in high dimensions. In Proc. of ICML (2008).

[9] CHEN, C., WU, K., SRINIVASAN, V., AND R, K. B.
The best answers? think twice: Online detection of com-
mercial campaigns in the cqa forums. CoRR (2012).

[10] CHEN, X. Dairy giant mengniu in smear scandal. China

Daily, October 2010.

[11] CHU, Z., GIANVECCHIO, S., WANG, H., AND JAJO-
DIA, S. Who is tweeting on twitter: human, bot, or cy-
borg? In Proc. of ACSAC (2010).

[12] DALVI, N., DOMINGOS, P., MAUSAM, SANGHAI, S.,
AND VERMA, D. Adversarial classiﬁcation. In Proc. of
KDD (2004).

[13] EATON, K. Mechanical turk’s unsavory side effect: Mas-

sive spam generation. Fast Company, December 2010.

[14] FREITAS, C. A., BENEVENUTO, F., GHOSH, S., AND
VELOSO, A. Reverse engineering socialbot inﬁltration
strategies in twitter. CoRR abs/1405.4927 (2014).

[15] GAO, H., HU, J., WILSON, C., LI, Z., CHEN, Y., AND
ZHAO, B. Y. Detecting and characterizing social spam
campaigns. In Proc. of IMC (2010).

[16] GIANVECCHIO, S., AND WANG, H. Detecting covert
timing channels: an entropy-based approach. In Proc. of
CCS (2007).

[17] HALL, M., FRANK, E., HOLMES, G., PFAHRINGER,
B., REUTEMANN, P., AND WITTEN, I. H. The weka
data mining software: an update. SIGKDD Explor. Newsl.
11, 1 (2009).

[18] HECKERMAN, D., GEIGER, D., AND CHICKERING,
D. M. Learning bayesian networks: The combination
of knowledge and statistical data. Mach. Learn. 20, 3
(1995), 197–243.

[19] HUANG, L., JOSEPH, A. D., NELSON, B., RUBIN-
STEIN, B. I., AND TYGAR, J. D. Adversarial machine
learning. In Proc. of AISec (2011).

[20] JOHN, G. H., AND LANGLEY, P. Estimating continu-
ous distributions in bayesian classiﬁers. In Proc. of UAI
(1995).

[21] KANTCHELIAN, A., AFROZ, S., HUANG, L., ISLAM,
A. C., MILLER, B., TSCHANTZ, M. C., GREENSTADT,
R., JOSEPH, A. D., AND TYGAR, J. D. Approaches to
adversarial drift. In Proc. of AISec (2013).

[22] LAKHINA, A., CROVELLA, M., AND DIOT, C. Diag-
nosing network-wide trafﬁc anomalies. In Proc. of SIG-
COMM (2004).

[23] LEE, K., TAMILARASAN, P., AND CAVERLEE, J.
Crowdturfers, campaigns, and social media: Tracking and
revealing crowdsourced manipulation of social media. In
Proc. of ICWSM (2013).

[24] LEE, K., WEBB, S., AND GE, H. The dark side of
micro-task marketplaces: Characterizing ﬁverr and au-
tomatically detecting crowdturﬁng. In Proc. of ICWSM
(2014).

[25] MAIORCA, D., CORONA, I., AND GIACINTO, G. Look-
ing at the bag is not enough to ﬁnd the bomb: an evasion
of structural methods for malicious pdf ﬁles detection. In
Proc. of ASIACCS (2013).

[26] MOTOYAMA, M., MCCOY, D., LEVCHENKO, K., SAV-
AGE, S., AND VOELKER, G. M. Dirty jobs: The role of
freelance labor in web service abuse. In Proc. of Usenix
Security (2011).

USENIX Association  

23rd USENIX Security Symposium  253

[27] NELSON, B., RUBINSTEIN, B. I. P., HUANG, L.,
JOSEPH, A. D., HON LAU, S., LEE, S. J., RAO, S.,
TRAN, A., AND TYGAR, J. D. Near-optimal evasion of
convex-inducing classiﬁers. In Proc. of AISTATS (2010).
[28] NELSON, B., RUBINSTEIN, B. I. P., HUANG, L.,
JOSEPH, A. D., LEE, S. J., RAO, S., AND TYGAR, J. D.
Query strategies for evading convex-inducing classiﬁers.
J. Mach. Learn. Res. 13, 1 (2012).

[29] NEWSOME, J., KARP, B., AND SONG, D.

Poly-
graph: Automatically generating signatures for polymor-
phic worms. In Proc. of IEEE S&P (2005).

[30] ONG, J. China’s sina weibo grew 73% in 2012, pass-
ing 500 million registered accounts. The Next Web, Feb.
2013.

[31] OTT, M., CHOI, Y., CARDIE, C., AND HANCOCK, J. T.
Finding deceptive opinion spam by any stretch of the
imagination. In Proc. of ACL (2011).

[32] PHAM, N. Vietnam admits deploying bloggers to support

government. BBC News, January 2013.

[33] PLATT, J. C. Fast training of support vector machines
In Advances in

using sequential minimal optimization.
Kernel Methods - Support Vector Learning (1998).

[34] QUINLAN, J. R. C4.5: programs for machine learning.

Morgan Kaufmann Publishers Inc., 1993.

[35] RAMACHANDRAN, A., FEAMSTER, N., AND VEM-
PALA, S. Filtering spam with behavioral blacklisting. In
Proc. of CCS (2007).

[36] ROBINSON, G. A statistical approach to the spam prob-

lem. Linux J. 2003, 107 (2003).

[37] RUBINSTEIN, B. I., NELSON, B., HUANG, L., JOSEPH,
A. D., LAU, S.-H., RAO, S., TAFT, N., AND TYGAR,
J. D. Antidote: understanding and defending against poi-
soning of anomaly detectors. In Proc. of IMC (2009).

[38] SHEAR, M. D. Republicans use crowdsourcing to attack

obama campaign. The New York Times, May 2012.

[39] SIMONITE, T. Hidden industry dupes social media users.

MIT Review, December 2011.

[40] SOMMER, R., AND PAXSON, V. Outside the closed
world: On using machine learning for network intrusion
detection. In Proc. of IEEE S&P (2010).

[41] SRNDIC, N., AND LASKOV, P. Practical evasion of a
learning-based classiﬁer: A case study. In Proc. of IEEE
S&P (2014).

[42] STRINGHINI, G., KRUEGEL, C., AND VIGNA, G. De-
tecting spammers on social networks. In Proc. of ACSAC
(2010).

[43] THOMASY, K., GRIERY, C., PAXSONY, V., AND
SONGY, D. Suspended accounts in retrospect: An analy-
sis of twitter spam. In Proc. of IMC (2011).

[44] VENKATARAMAN, S., BLUM, A., AND SONG, D. Lim-
its of learning-based signature generation with adver-
saries. In Proc. of NDSS (2008).

[45] WANG, G., GILL, K., MOHANLAL, M., ZHENG, H.,
AND ZHAO, B. Y. Wisdom in the social crowd: an anal-
ysis of quora. In Proc. of WWW (2012).

[46] WANG, G., KONOLIGE, T., WILSON, C., WANG, X.,
ZHENG, H., AND ZHAO, B. Y. You are how you click:
In Proc. of
Clickstream analysis for sybil detection.
USENIX Security (2013).

[47] WANG, G., MOHANLAL, M., WILSON, C., WANG, X.,
METZGER, M., ZHENG, H., AND ZHAO, B. Y. Social
turing tests: Crowdsourcing sybil detection. In Proc. of
NDSS (2013).

[48] WANG, G., WILSON, C., ZHAO, X., ZHU, Y., MOHAN-
LAL, M., ZHENG, H., AND ZHAO, B. Y. Serf and turf:
crowdturﬁng for fun and proﬁt. In Proc. of WWW (2012).
[49] WILSON, C., SALA, A., PUTTASWAMY, K. P. N., AND
ZHAO, B. Y. Beyond social graphs: User interactions
in online social networks and their implications. ACM
Transactions on the Web 6, 4 (November 2012).

[50] YANG, C., HARKREADER, R. C., AND GU, G. Die
free or live hard? empirical evaluation and new design
for ﬁghting evolving twitter spammers. In Proc. of RAID
(2011).

[51] YANG, Y., AND PEDERSEN, J. O. A comparative study
In Proc. of

on feature selection in text categorization.
ICML (1997).

[52] YANG, Z., WILSON, C., WANG, X., GAO, T., ZHAO,
B. Y., AND DAI, Y. Uncovering social network sybils in
the wild. In IMC (2011).

[53] ZHANG, L., YANG, J., AND TSENG, B. Online model-
ing of proactive moderation system for auction fraud de-
tection. In Proc. of WWW (2012).

[54] ZHANG, Y., GE, Z., GREENBERG, A., AND ROUGHAN,

M. Network anomography. In Proc. of IMC (2005).

254  23rd USENIX Security Symposium 

USENIX Association

