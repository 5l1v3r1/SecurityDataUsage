Finding Unknown Malice in 10 Seconds:  

Mass Vetting for New Threats at  

the Google-Play Scale

Kai Chen, Chinese Academy of Sciences and Indiana University; Peng Wang,  

Yeonjoon Lee, Xiaofeng Wang, and Nan Zhang, Indiana University;  

Heqing Huang, The Pennsylvania State University; Wei Zou, Chinese Academy of Sciences; 

Peng Liu, The Pennsylvania State University

https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/chen-kai

This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-931971-232Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXFinding Unknown Malice in 10 Seconds: Mass Vetting for New Threats at

the Google-Play Scale

Kai Chen‡,†, Peng Wang†, Yeonjoon Lee†, XiaoFeng Wang†, Nan Zhang†, Heqing Huang§, Wei Zou‡ and Peng Liu§

{chenkai, zouwei}@iie.ac.cn, {pw7, yl52, xw7, nz3}@indiana.edu, hhuang@cse.psu.edu, pliu@ist.psu.edu
‡State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences

†Indiana University, Bloomington

§College of IST, Penn State University

Abstract

An app market’s vetting process is expected to be scal-
able and effective. However, today’s vetting mechanisms
are slow and less capable of catching new threats.
In
our research, we found that a more powerful solution
can be found by exploiting the way Android malware is
constructed and disseminated, which is typically through
repackaging legitimate apps with similar malicious com-
ponents. As a result, such attack payloads often stand out
from those of the same repackaging origin and also show
up in the apps not supposed to relate to each other.

Based upon this observation, we developed a new
technique, called MassVet, for vetting apps at a mas-
sive scale, without knowing what malware looks like
and how it behaves. Unlike existing detection mecha-
nisms, which often utilize heavyweight program analy-
sis techniques, our approach simply compares a submit-
ted app with all those already on a market, focusing on
the difference between those sharing a similar UI struc-
ture (indicating a possible repackaging relation), and the
commonality among those seemingly unrelated. Once
public libraries and other legitimate code reuse are re-
moved, such diff/common program components become
highly suspicious. In our research, we built this “Diff-
Com” analysis on top of an efﬁcient similarity compar-
ison algorithm, which maps the salient features of an
app’s UI structure or a method’s control-ﬂow graph to
a value for a fast comparison. We implemented MassVet
over a stream processing engine and evaluated it nearly
1.2 million apps from 33 app markets around the world,
the scale of Google Play. Our study shows that the tech-
nique can vet an app within 10 seconds at a low false
detection rate. Also, it outperformed all 54 scanners in
VirusTotal (NOD32, Symantec, McAfee, etc.) in terms
of detection coverage, capturing over a hundred thou-
sand malicious apps, including over 20 likely zero-day
malware and those installed millions of times. A close
look at these apps brings to light intriguing new obser-

vations: e.g., Google’s detection strategy and malware
authors’ countermoves that cause the mysterious disap-
pearance and reappearance of some Google Play apps.

1 Introduction
The phenomenal growth of Android devices brings in
a vibrant application ecosystem. Millions of applica-
tions (app for short) have been installed by Android users
around the world from various app markets. Prominent
examples include Google Play, Amazon Appstore, Sam-
sung Galaxy Apps, and tens of smaller third-party mar-
kets. With this prosperity, the ecosystem is tainted by the
rampancy of Android malware, which masquerades as a
useful program, often through repackaging a legitimate
app, to wreak havoc, e.g., intercepting one’s messages,
stealing personal data, sending premium SMS messages,
etc. Countering this menace primarily relies on the effort
from the app markets, since they are at a unique position
to stop the spread of malware in the ﬁrst place. Accom-
plishing this mission, however, is by no means trivial,
as highlighted by a recent report [8] that 99% of mobile
malware runs on Android devices.
Challenges in app vetting. More speciﬁcally, the pro-
tection today’s app market puts in place is a vetting pro-
cess, which screens uploaded apps by analyzing their
code and operations for suspicious activities. Particu-
larly, Google Play operates Bouncer [24], a security ser-
vice that statically scans an app for known malicious
code and then executes it within a simulated environ-
ment on Google’s cloud to detect hidden malicious be-
havior. The problem here is that the static approach does
not work on new threats (i.e., zero-day malware), while
the dynamic one can be circumvented by an app capable
of ﬁngerprinting the testing environment, as discovered
by a prior study [30]. Also the dynamic analysis can be
heavyweight, which makes it hard to explore all execu-
tion paths of an app.

New designs of vetting techniques have recently been

USENIX Association  

24th USENIX Security Symposium  659

proposed by the research community [57, 28] for captur-
ing new apps associated with known suspicious behavior,
such as dynamic loading of binary code from a remote
untrusted website [57], operations related to component
hijacking [28], Intent injection [12], etc. All these ap-
proaches involve a heavyweight information-ﬂow anal-
ysis and require a set of heuristics that characterize the
known threats. They often need a dynamic analysis in ad-
dition to the static inspection performed on app code [57]
and further human interventions to annotate the code or
even participate in the analysis [14]. Moreover, emula-
tors that most dynamic analysis tools employ can be de-
tected and evaded by malware [23]. Also importantly,
none of them has been put to a market-scale test to un-
derstand their effectiveness, nor has their performance
been clearly measured.
Catching unknown malice. Actually, a vast majority of
Android malware are repackaged apps [56], whose au-
thors typically attach the same attack payload to different
legitimate apps. In this way, not only do they hide their
malicious program logic behind the useful functionali-
ties of these apps, but they can also automate the repack-
aging process to quickly produce and distribute a large
number of Trojans1. On the other hand, this practice
makes such malware stand out from other repackaged
apps, which typically incorporate nothing but advertis-
ing libraries [2]. Also as a result of the approach, similar
code (typically in terms of Java methods) shows up in
unrelated apps that are not supposed to share anything
except popular libraries.

These observations present a new opportunity to catch
malicious repackaged apps,
the mainstay of Android
malware, without using any heuristics to model their be-
havior. What we can do is to simply compare the code
of related apps (an app and its repackaged versions, or
those repackaged from the same app) to check their dif-
ferent part, and unrelated apps (those of different ori-
gins, signed by different parties) to inspect their com-
mon part to identify suspicious code segments (at the
method level). These segments, once found to be in-
explicable (e.g., not common libraries), are almost cer-
tain to be malicious, as discovered in our study (Sec-
tion 4.2). This DiffCom analysis is well suited for ﬁnd-
ing previously unknown malicious behavior and also can
be done efﬁciently, without resorting to any heavyweight
information-ﬂow technique.
Mass vetting at scale. Based on this simple idea, we de-
veloped a novel, highly-scalable vetting mechanism for
detecting repackaged Android malware on one market
or cross markets. We call the approach mass vetting or
simply MassVet, as it does not use malware signatures

1Those Trojans are typically signed by different keys to avoid

blocking of a speciﬁc signer.

and any models of expected malicious operations, and
instead, solely relies on the features of existing apps on
a market to vet new ones uploaded there. More specif-
ically, to inspect a new app, MassVet runs a highly ef-
ﬁcient DiffCom analysis on it against the whole market.
Any existing app related to the new one (i.e., sharing the
same repackaging origin) is quickly identiﬁed from the
structural similarity of their user interfaces (aka., views),
which are known to be largely preserved during repack-
aging (Section 2). Then, a differential analysis happens
to those sharing the similar view structure (indicating a
repackaging relation between them) when a match has
been found. Also, an intersection analysis is performed
to compare the new app against those with different view
structures and signed by different certiﬁcates. The code
components of interest discovered in this way, either the
common (or similar) methods (through the intersection
analysis) or different ones (by the differential analysis),
are further inspected to remove common code reuses
(libraries, sample code, etc.) and collect evidence for
their security risks (dependence on other code, resource-
access API calls, etc.), before a red ﬂag is raised.

Supporting this mass vetting mechanism are a suite
of techniques for high-performance view/code compar-
isons. Particularly, innovations are made to achieve a
scalable analysis of different apps’ user interfaces (Sec-
tion 3.2). The idea is to project a set of salient features of
an app’s view graph (i.e., the interconnections between
its user interfaces), such as types of widgets and events,
to a single dimension, using a unique index to represent
the app’s location within the dimension and the similar-
ity of its interface structure to those of others.
In our
research, we calculated this index as a geometric center
of a view graph, called v-core. The v-cores of all the
apps on the market are sorted to enable a binary search
during the vetting of a new app, which makes this step
highly scalable. The high-level idea here was applied
to application clone detection [7], a technique that has
been utilized in our research (mapping the features of a
Java method to an index, called m-core in our research)
for ﬁnding common methods across different apps (Sec-
tion 3.3). It is important to note that for the view-graph
comparison, new tricks need to be played to handle the
structural changes caused by repackaging, e.g., when ad-
vertisement interfaces are added (Section 3.2).
Our ﬁndings. We implemented MassVet on a cloud
platform, nearly 1.2 million real-world apps collected
from 33 app markets around the world. Our experimen-
tal study demonstrates that MassVet vetted apps within
ten seconds, with a low false positive rate. Most impor-
tantly, from the 1.2 million apps, our approach discov-
ered 127,429 malware: among them at least 20 are likely
zero-day and 34,026 were missed by the majority of the
malware scanners run by VirusTotal, a website that syn-

660  24th USENIX Security Symposium 

USENIX Association

2

dicates 54 different antivirus products [43]. Our study
further shows that MassVet achieved a better detection
coverage than any individual scanner within VirusTotal,
such as Kaspersky, Symantec, McAfee, etc. Other high-
lights of our ﬁndings include the discovery of malicious
apps in leading app markets (30,552 from Google Play),
and Google’s strategies to remove malware and malware
authors’ countermoves, which cause mysterious disap-
pearance and reappearance of apps on the Play Store.
Contributions. The contributions of the paper are sum-
marized as follows:
• New techniques. We developed a novel mass vet-
ting approach that detects new threats using nothing but
the code of the apps already on a market. An innova-
tive differential-intersection analysis (i.e., DiffCom) is
designed to exploit the unique features of repackaging
malware, catching the malicious apps even when their
behavior has not been proﬁled a priori. This analysis
is made scalable by its simple, static nature and the fea-
ture projection techniques that enable a cloud-based, fast
search for view/code differences and similarities. Note
that when the v-core and m-core datasets (only 100 GB
for 1.2 million apps) are shared among multiple markets,
MassVet can help one market to detect malicious submis-
sions using the apps hosted by all these markets.
• New discoveries. We implemented MassVet and eval-
uated it using nearly 1.2 million apps, a scale unparal-
leled in any prior study on Android malware detection,
up to our knowledge, and on a par with that of Google
Play, the largest app market in the world with 1.3 million
apps [39]. Our system captured tens of thousands of mal-
ware, including those slipping under the radar of most or
all existing scanners, achieved a higher detection cover-
age than all popular malware scanners within VirusTotal
and vetted new apps within ten seconds. Some malware
have over millions of installs. 5,000 malware were in-
stalled over 10,000 times each, impacting hundreds of
millions of mobile devices. A measurement study fur-
ther sheds light on such important issues as how effective
Google Play is in screening submissions, how malware
authors hide and distribute their attack payloads, etc.

2 Background
Android App markets. Publishing an app on a market
needs to go through an approval process. A submission
will be inspected for purposes such as quality control,
censorship, and also security protection. Since 2012,
Google Play has been under the protection of Bouncer.
This mechanism apparently contributes to the reduction
of malware on the Play store, about 0.1% of all apps
there as discovered by F-Secure [15]. On the other hand,
this security vetting mechanism was successfully cir-
cumvented by an app that ﬁngerprints its simulator and

strategically adjusts its behavior [33]. Compared with
the Android ofﬁcial market, how third-party markets re-
view submitted apps is less clear. The picture painted by
F-Secure, however, is quite dark: notable markets like
Mumayi, AnZhi, Baidu, etc. were all found riddled with
malware inﬁltrations [16].

Attempts to enhance the current secure vetting mecha-
nisms mainly resort to conventional malware detection
techniques. Most of these approaches, such as Vet-
Droid [52], rely on tracking information ﬂows within
an app and the malicious behavior modeling for detect-
ing malware. In the case that what the malware will do
is less clear to the market, these approaches no longer
help. Further, analyzing information ﬂows requires se-
mantically interpreting each instruction and carefully-
designed techniques to avoid false positives, which are
often heavyweight. This casts doubt on the feasibility of
applying these techniques to a large-scale app vetting.
Repackaging. App repackaging is a process that mod-
iﬁes an app developed by another party and already re-
leased on markets to add in some new functionalities be-
fore redistributing the new app to the Android users. Ac-
cording to Trend Micro (July 15, 2014), nearly 80% of
the top 50 free apps on Google Play have repackaged
versions [49]. Even the Play store itself is reported to
host 1.2% repackaged apps [58]. This ratio becomes
5% to 13% for third-party markets, according to a prior
study [55]. These bogus apps are built for two purposes:
either for getting advertisement revenues or for distribut-
ing malware [7]. For example, one can wrap Angry-
Bird with ad libraries, including his own adverting ID
to beneﬁt from its advertising revenue. Malware authors
also found that leveraging those popular legitimate apps
is the most effective and convenient avenue to distribute
their attack payloads: repackaging saves them a lot of ef-
fort to build the useful functionalities of a Trojan and the
process can also be automated using the tools like smal-
i/baksmali [36]; more importantly, they can free-ride the
popularity of these apps to quickly infect a large number
of victims. Indeed, research shows that the vast majority
of Android malware is repackaged apps, about 86% ac-
cording to a study [56]. A prominent feature shared by
all these repackaged apps, malicious or not, is that they
tend to keep the original user interfaces intact, so as to
impersonate popular legitimate apps.
Scope and assumptions. MassVet is designed to detect
repackaged Android malware. We do not consider the
situation that the malware author makes his malicious
payload an inseparable part of the repackaged app, which
needs much more effort to understand the legitimate app
than he does today. Also, MassVet can handle typi-
cal code obfuscation used in most Android apps (Sec-
tion 3). However, we assume that the code has not been

USENIX Association  

24th USENIX Security Symposium  661

3

















































































Figure 1: The Architecture of MassVet.

obfuscated to the extent that even disassembly cannot go
through. When this happens, not only our approach but
also most of other static analyses will fail. Finally, we
assume that the app market under protection accommo-
dates a large number of highly-diverse apps, so that for
the malicious repackaged app uploaded, on the market
there will be either another app sharing its repackaging
origin or the one incorporating the same attack payload.
To make this more likely to happen, different markets
can share the feature datasets of their apps (i.e., v-cores
and m-cores) with each other. Note that such datasets are
very compact, only 100 GB for 1.2 million apps.

3 MassVet: Design and Implementation
3.1 Overview
Design and architecture. To detect unknown malware
at a large scale, we come up with a design illustrated in
Figure 1. It includes three key components: a prepro-
cessing module, a feature database system and a Diff-
Com module. The preprocessing module automatically
analyzes a submitted app, which includes extracting the
features of its view structure and methods, and then sum-
marizing them into the app’s v-cores and m-cores respec-
tively. The DiffCom component then works on these fea-
tures, searching for them within the app market’s v-core
and m-core databases. Matches found there are used to
identify suspicious different or common methods, which
are further screened to remove false positives.
How it works. Here we use an example to walk through
the work ﬂow of the system. MassVet ﬁrst processes all
the apps on a market to create a v-core database for view
structures and an m-core database for Java methods (Sec-
tion 3.4). Both databases are sorted to support a binary
search and are used for vetting new apps submitted to
the market. Consider a repackaged AngryBird. Once up-
loaded to the market, it is ﬁrst automatically disassem-
bled at the preprocessing stage into a smali represen-
tation, from which its interface structures and methods
are identiﬁed. Their features (for views, user interfaces,
types of widgets and events, and for methods, control
ﬂow graphs and bytecode) are mapped to a set of v-cores
(Section 3.2) and m-cores (Section 3.3) through calculat-

ing the geometric centers of the view graphs and control-
ﬂow graphs respectively. The app’s v-cores are ﬁrst used
to query the database through a binary search. Once a
match is found, which happens when there exists another
app with a similar AngryBird user interface structure, the
repackaged app is compared with the app already on the
market at the method level to identify their difference.
Such different methods (diff for short) are then automat-
ically analyzed to ensure that they are not ads libraries
and indeed suspicious, and if so, are reported to the mar-
ket (Section 3.2). When the search on the v-core database
comes back with nothing2, MassVet continues to look
for the AngryBird’s m-cores in the method database. If
a similar method has been found, our approach tries to
conﬁrm that indeed the app including the method is un-
related to the submitted AngryBird and it is not a legit-
imate code reuse (Section 3.3).
In this case, MassVet
reports that a suspicious app is found. All these steps are
fully automated, without human intervention.

3.2 Fast User-Interface Analysis
As discussed before, the way MassVet vets an app de-
pends on whether it is related to any other app already
on the market. Such a relation is established in our re-
search through a quick inspection of apps’ user inter-
faces (UI) to identify those with similar view structures.
When such apps are not “ofﬁcially” connected, e.g., pro-
duced by the same party, the chance is that they are of
the same repackaging origin, and therefore their diffs
become interesting for malicious code detection. This
interface-based relation identiﬁcation is an alternative to
code-based identiﬁcation: a malicious repackaged app
can be obfuscated and junk code can be easily added
to make it look very different from the original version
in terms of the similarity between their code (e.g., per-
centage of similar methods shared between them). On
the other hand, a signiﬁcant change to the user interface
needs more effort and most importantly affects user ex-
perience, making it more difﬁcult for the adversary to
free ride the popularity of the original app. Therefore,
most repackaged apps preserve their original UI struc-
tures, as found by the prior research [50]. In our research,
we further discovered that many repackaged apps incor-
porate a large amount of new code, even more than that
in their original versions, but still keep those apps’ UI
structures largely intact.

The idea of using view structures to detect repack-
aged apps has been preliminarily explored in prior re-
search [50], which utilizes subgraph isomorphism algo-
rithms to measure the similarity between two apps. How-
ever, the approach is less effective for the apps with rel-
atively simple user-interface structures, and most impor-

2The market can also choose to perform both differential and inter-

action analyses for all new apps (Section 3.3).

662  24th USENIX Security Symposium 

USENIX Association

4

tantly, agonizingly slow: it took 11 seconds to compare
a pair of apps [50], which would need 165 days to ana-
lyze one submission against all 1.3 million apps on the
Google Play store.

Following we elaborate our new solution designed for

an accurate and high performance app-view analysis.
Feature extraction. An app’s user interface consists of a
set of views. Each view contains one or more visual wid-
gets such as Button, ListView, TextView, etc. These UI
components respond to users’ input events (e.g., tapping
and swiping) with the operations speciﬁed by the app de-
veloper. Such responses may cause visible changes to the
current view or transitions to other views. This intercon-
nection structure, together with the layouts and function-
alities of individual views, was found to be sufﬁciently
unique for characterizing each app [50].

In our research, we model such a UI structure as a view
graph, which is a directed weighted graph including all
views within an app and the navigation relations (that
is, the transition from one view to another) among them.
On such a graph, each node is a view, with the number of
its active widgets (those with proper event-response op-
erations) as its weight, and the arcs connecting the nodes
describe the navigation (triggered by the input events) re-
lations among them. According to the types of the events
(e.g., onClick, onFocusChange, onTouch, etc.),
edges can be differentiated from each other.

Such a view graph can effectively describe an app with
a reasonably complicated UI structure. However, it be-
comes less effective for the small apps with only a couple
of views and a rather straightforward connection struc-
ture. To address this issue, we enrich the view graph with
additional features, including other UIs and the types of
widgets that show up in a view. Speciﬁcally, in addi-
tion to view, which is displayed through invocation of
an Android Activity, the UIs such as AlertDialog are
also treated as nodes for the graph. Custom dialogs can
be handled by analyzing class inheritance. Further, each
type of widgets is given a unique value, with a sole pur-
pose of differentiating it from other types. In this way,
we can calculate a UI node’s weight by adding together
the values associated with the widgets it carries to make
a small view graph more distinctive from others. An ex-
ample is illustrated in Figure 2.

Note that we avoid using text labels on UI elements
or other attributes like size or color. All the features se-
lected here, including UIs, types of widgets and events
that cause transitions among UIs, are less manipulable:
in the absence of serious effort, any signiﬁcant change
to them (e.g., adding junk widgets, modifying the widget
types, altering the transitions among views) will perceiv-
ably affect user experience, making it more difﬁcult for
the adversary to use them to impersonate popular apps.
the view graph for a submitted

To construct





















































Figure 2: A View-graph example.

Ac: Activity; Da: AlertDialog; Dt: TimePickerDialog

Dp: ProgressDialog; Dd: DatePickerDialog

its

code

For

IPC calls

the preprocessing module automatically an-
app,
all UI-related inter-
alyzes
to recover
the channel
through
process communication (IPC),
interfaces.
which an Android app invokes user
include startActivity and
Such
startActivityForResult.
each call,
our approach locates it within a UI and further identiﬁes
the UI it triggers. Speciﬁcally, the program location of
the IPC is examined to determine whether it is inside a
UI-related class v. Its parameter is parsed to ﬁnd out the
class it calls (v′). In this case, nodes are created on the
view graph for both classes (UIs) and an edge is added
to link v to v′. Also, the type of the edge is determined
by the event handler at which the IPC is located: for
example, when the call is found inside the onClick
function for a button, we know that this widget is used to
cause a view transition. All such widgets are identiﬁed
from each class for determining the weight of its node.
Design for scale. Once a view graph is recovered from
an app, we want to quickly compare it across a market (or
markets) to identify those related to the app. This opera-
tion needs to be of high-performance, capable of process-
ing over one million apps within seconds. To this end, we
applied a recently proposed similarity comparison algo-
rithm, called Centroids [7], to the view-graph analysis.
Centroid maps the features of a program’s control-ﬂow
graph (CFG) into a value, which is calculated as the ge-
ometric center of the program. This value has a mono-
tonicity property: that is, whenever a minor change hap-
pens to the CFG, the value changes accordingly at a small
scale, reﬂecting the level of the difference made to the
program. This property localizes the global comparison
to a small number of “neighbors” to achieve high scal-
ability without losing accuracy. The approach was used
for the method comparison in our research (Section 3.3).
However, it cannot be directly adopted for analyzing the
UI structure, as the view graph is quite different from the
CFG. Also, an app’s graph is often fragmented due to the
unusual ways to trigger some of its modules: e.g., most
advertisement views are invoked through callbacks using
the APIs of their library; as a result, their graph becomes
separated from that of the main program. Here we de-
scribe how we address these issues.

USENIX Association  

24th USENIX Security Symposium  663

5

Given a set of subgraphs for an app UI, Gi=1···n, our
preprocess module analyzes them one by one to calcu-
late their individual geometric centers, i.e., v-cores. For
a subgraph Gi, the ﬁrst thing that needs to be done is to
convert the features of each of its nodes (i.e., view) into
a three-dimensional vector ⃗c = {α,β,γ}. Here α is a
sequence number assigned to each node in Gi, which is
done through an ordered deep-ﬁrst traversal of Gi: start-
ing from its main view, we select nodes to visit in the
order of the sizes of their subtrees, and use their individ-
ual weights to break a tie; each node traversed in this way
gets the number based upon its order of being visited. If
two subtrees have the same size, we select the one ac-
cording to their node types. In this way, we ensure that
the assignment of sequence numbers is unique, which
only depends on the structure of the directed weighted
graph. The second element, β, in the vector is the out de-
gree of the node: that is, the number of UIs the node can
lead to. Finally, γ is the number of “transition loops” the
current node is involved: i.e., the structure that from the
node, there exists a navigation path that by visiting each
node on the path only once, the user is able to navigate
back to the current view. Figure 2 presents an example
that show how such a vector is constructed.

After every node k on Gi has been given a vector ⃗ck,
we can calculate its geometric center, i.e., v-core vci, as
follows:

vci =

∑e(p,q)∈Gi (wp⃗cp + wq⃗cq)
∑e(p,q)∈Gi(wp + wq)

where e(p,q) denotes an edge in Gi from node p to q
and wp is the weight of node p. With the monotonicity
of v-cores, we can sort them for a large number of apps
to support a binary search.
In this way, the subgraph
Gi can be quickly compared with millions of graphs to
identify similar ones. Speciﬁcally, given another graph
Gt with a v-core vct, we consider that it matches Gi if
|vci − vct| ≤ τ, where τ is a threshold. Further, given
two apps sharing a subset of their view-graphs Gi(l=1···m),
we consider that these two apps are similar in their UI
structure when the following happens to at least one app:
∑l |Gi(l)|/∑i|Gi| ≥ θ:
that is, most of the app’s view
structures also appear in the other app (with θ being a
threshold). This ensures that even when the adversary
adds many new views to an app (e.g., through fake ad-
vertisements), the relation between the repackaged app
and the original one can still be identiﬁed.

In our research, such thresholds were determined from
a training process using 50,000 randomly selected apps
(Section 3.3). We set different thresholds and measured
the corresponding false positive/negative rates. For false
positives, we randomly sampled 50 app pairs detected
by our approach under each threshold and manually
checked their relations. For false negatives, we utilized

100 app pairs known to have repackaging relations as the
ground truth to ﬁnd out the number of pairs our approach
identiﬁed with different thresholds. The study shows that
when τ = 0 and θ = 0.8, we got both a low false posi-
tive rate (4%) and a low false negative rate (6%). Among
these 50,000 apps, we found that 26,317 app pairs had
repackaging relations, involving 3,742 apps in total.
Effectiveness of the view-graph analysis. Compared
with existing code-based approaches [7], the view-graph
analysis turns out to be more effective at detecting apps
of the same repackaging origin. Speciﬁcally, we ran-
domly selected 10,000 app pairs (involving 17,964 apps)
from those repackaged from the same programs, as dis-
covered from 1.2 million apps we collected (Section 4.1).
Many of these repackaging pairs involve the apps whose
code signiﬁcantly differ from each other. Particularly,
in 14% of these pairs, two apps were found to have less
than 50% of their individual code in common. This could
be caused by a large library (often malicious) added to an
app during repackaging or junk code inserted for the pur-
pose of obfuscation. Since these apps look so different
based upon their code, their repackaging relations can-
not be easily determined by program analysis. However,
they were all caught by our approach, simply because the
apps’ view-graphs were almost identical.

3.3 DiffCom Analysis at Scale
For an app going through the mass vetting process, the
view-graph analysis ﬁrst determines whether it is related
to any app already on the market. If so, these two apps
will be further compared to identify their diffs for a mal-
ware analysis. Otherwise, the app is checked against
the whole market at the method level, in an attempt to
ﬁnd the program component it shares with other apps.
The diffs and common component are further inspected
to remove common code reuse (libraries, sample code,
etc.) and collect evidence for their security risks. This
“difference-commonality” analysis is performed by the
DiffCom module. We also present the brick and mor-
tar for efﬁcient code-similarity analyzer and discuss the
evasion of DiffCom.
The brick and mortar. To vet apps at the market
scale, DiffCom needs a highly efﬁcient code-similarity
analyzer.
In our research, we chose Centroids [7] as
this building block. As discussed before, this approach
projects the CFG of a program to its geometric center, in
a way similar to the view-graph analysis. More specif-
ically, the algorithm takes a basic program block as a
node, which includes a sequence of consecutive state-
ments with only a single input and output. The weight of
the block is the number of the statements it contains. For
each node on the CFG, a sequence number is assigned,
together with the counts of the branches it connects and

664  24th USENIX Security Symposium 

USENIX Association

6

the number of loops it is involved in. These parameters
are used to calculate the geometric center of the program.
To prepare for mass vetting, our approach ﬁrst goes
through all the apps on a market and breaks them into
methods. After removing common libraries, the pre-
processing module analyzes their code, calculates the
geometric centers for individual methods (i.e., the m-
cores) and then sorts them before storing the results in
the database. During the vetting process, if a submitted
app is found to share the view graph with another app,
their diffs are quickly identiﬁed by comparing the m-
cores of their individual methods. When the app needs to
go through the intersection step, its methods are used for
a binary search on the m-core database, which quickly
discovers those also included in existing apps. Here we
elaborate how these operations are performed. Their
overhead is measured in Section 4.2.
Analyzing diffs. Whenever an app is found to relate to
another one from their common view graph, we want to
inspect the difference part of their code to identify sus-
picious activities. The rationale is that repackaged apps
are the mainstay of Android malware, and the malicious
payloads are often injected automatically (using tools
like smali/baksmali) without any signiﬁcant changes to
the code of the original app, which can therefore be lo-
cated by looking at the diffs between the apps of the same
repackaging origin. Such diffs are quickly identiﬁed by
comparing these two apps’ m-cores: given two ordered
sequences of m-cores L and L′, the diff between the apps
at the method level is found by merging these two lists
according to the orders of their elements and then re-
moving those matching their counterparts on the other
list; this can be done within min (|L|,|L′|) steps.
However, similarity of apps’ UIs does not always indi-
cate a repackaging relation between them. The problem
happens to the apps produced by the same party, indi-
vidual developers or an organization. In this case, it is
understandable that the same libraries and UIs could be
reused among their different products. It is even possible
that one app is actually an updated version of the other.
Also, among different developers, open UI SDKs such as
Appcelerator [3] and templates like Envatomarket [13]
are popular, which could cause the view structures of
unrelated apps to look similar. Further, even when the
apps are indeed repackaged, the difference between them
could be just advertisement (ad) libraries instead of mali-
cious payloads. A challenge here is how to identify these
situations and avoid bringing in false alarms.

To address these issues, MassVet ﬁrst cleans up a sub-
mitted app’s methods, removing ad and other libraries,
before vetting the app against the market. Speciﬁcally,
we maintain a white list of legitimate ad libraries based
on [6], which includes popular mobile ad platforms such
as MobWin, Admob, etc. To identify less known ones,

we analyzed a training set of 50,000 apps randomly sam-
pled from three app markets, with half of them from
Google Play. From these apps, our analysis discovered
34,886 methods shared by at least 27,057 apps signed
by different parties. For each of these methods, we fur-
ther scanned its hosting apps using VirusTotal. If none of
them were found to be malicious, we placed the method
on the white list. In a similar way, popular view graphs
among these apps were identiﬁed and the libraries as-
sociated with these views are white-listed to avoid de-
tecting false repackaging relations during the view-graph
analysis. Also, other common libraries such as Admob
were also removed during this process, which we elab-
orate later. Given the signiﬁcant size of the training set
(50,000 randomly selected apps), most if not all legiti-
mate libraries are almost certain to be identiﬁed. This
is particularly true for those associated with advertising,
as they need certain popularity to remain proﬁtable. On
the other hand, it is possible that the approach may let
some zero-day malware fall through the cracks. In our re-
search, we further randomly selected 50 ad-related meth-
ods on the list and searched for them on the Web, and
conﬁrmed that all of them were indeed legitimate. With
this false-negative risk, still our approach achieved a high
detection coverage, higher than any scanner integrated in
VirusTotal (Section 4.2).

When it comes to the apps produced by the same party,
the code they share is less popular and therefore may not
be identiﬁed by the approach. The simplest solution here
is to look at similar apps’ signatures: those signed by the
same party are not considered to be suspicious because
they do have a good reason to be related. This simple
treatment works most of time, since legitimate app ven-
dors typically sign their products using the same certiﬁ-
cate. However, there are situations when two legitimate
apps are signed by different certiﬁcates but actually come
from the same source. When this happens, the diffs of
the apps will be reported and investigated as suspicious
code. To avoid the false alarm, we took a close look at
the legitimate diffs, which are characterized by their in-
tensive connections with other part of the app. They are
invoked by other methods and in the meantime call the
common part of the code between the apps. On the other
hand, the malicious payload packaged to a legitimate app
tends to stand alone, and can only be triggered from a few
(typically just one) program locations and rarely call the
components within the original program.

In our research, we leveraged this observation to dif-
ferentiate suspicious diffs from those likely to be legiti-
mate. For each diff detected, the DiffCom analyzer looks
for the calls it makes toward the rest of the program and
inspects the smali code of the app to identify the ref-
erences to the methods within the diff. These methods
will go through a further analysis only when such inter-

USENIX Association  

24th USENIX Security Symposium  665

7

actions are very limited, typically just an inward invoca-
tion, without any outbound call. Note that current mal-
ware authors do not make their code more connected to
the legitimate app they repackage, mainly because more
effort is needed to understand the code of the app and
carefully construct the attack. A further study is needed
to understand the additional cost required to build more
sophisticated malware to evade our detection.

For the diff found in this way, DiffCom takes fur-
ther measures to determine its risk. A simple approach
used in our implementation is to check the presence
of API calls (either Android framework APIs or those
associated with popular libraries) related to the oper-
ations considered to be dangerous. Examples include
getSimSerialNumber, sendTextMessage and
getLastKnownLocation. The ﬁndings here indi-
cate that the diff code indeed has the means to cause dam-
age to the mobile user’s information assets, though how
exactly this can happen is not speciﬁed. This is differ-
ent from existing behavior-based detection [27], which
looks for much more speciﬁc operation sequences such
as “reading from the contact list and then sending it to the
Internet”. Such a treatment helps suppress false alarms
and still preserves the generality of our design, which
aims at detecting unknown malicious activities.
Analyzing intersections. When no apparent connection
has been found between an app and those already on the
market, the vetting process needs to go through an in-
tersection analysis. This also happens when DiffCom is
conﬁgured to perform the analysis on the app that has not
been found to be malicious at the differential step. Identi-
ﬁcation of common methods a newly submitted app car-
ries is rather straightforward: each method of the app is
mapped to its m-core, which is used to search against the
m-core database. As discussed before, this can be done
through a binary search. Once a match is found, Diff-
Com further inspects it, removing legitimate connections
between the apps, and reports the ﬁnding to the market.
the main challenge here is to determine
whether two apps are indeed unrelated. A simple sig-
nature check removes most of such connections but not
all. The “stand-alone” test, which checks whether a set
of methods intensively interact with the rest of an app,
does not work for the intersection test. The problem here
is that the common methods between two repackaged
apps may not be the complete picture of a malicious pay-
load, making them different from the diff identiﬁed in
the differential-analysis step: different malware authors
often use some common toolkits in their attack payloads,
which show up in the intersection between their apps;
these modules still include heavy interactions with other
components of the malware that are not found inside the
intersection. As a result, this feature, which works well
on diffs, cannot help to capture suspicious common code

Again,

among apps.

An alternative solution here is to look at how the seem-
ingly unrelated apps are actually connected. As dis-
cussed before, what causes the problem is the developers
or organizations that reuse code internally (e.g., a propri-
etary SDK) but sign the apps using different certiﬁcates.
Once such a relation is also identiﬁed, we will be more
conﬁdent about whether two apps sharing code are inde-
pendent from each other. In this case, the common code
becomes suspicious after all public libraries (e.g., those
on the list used in the prior research [6]) and code tem-
plates have been removed. Here we describe a simple
technique for detecting such a hidden relation.

From our training dataset, we found that most code
reused legitimately in this situation involves user inter-
faces: the developers tend to leverage existing view de-
signs to quickly build up new apps. With this practice,
even though two apps may not appear similar enough in
terms of their complete UI structures (therefore they are
considered to be “unrelated” by the view-graph analy-
sis), a close look at the subgraphs of their views may re-
veal that they actually share a signiﬁcant portion of their
views and even subgraphs. Speciﬁcally, from the 50,000
apps in our training set, after removing public libraries,
we found 30,286 sharing at least 30% of their views with
other apps, 16,500 sharing 50% and 8,683 containing no
less than 80% common views. By randomly sampling
these apps (10 each time) and analyzing them manually,
we conﬁrmed that when the portion goes above 50%, al-
most all the apps and their counterparts are either from
the same developers or organizations, or having the same
repackaging origins. Also, once the shared views be-
come 80% or more, almost always the apps are involved
in repackaging. Based upon this observation, we run an
additional correlation check on a pair of apps with com-
mon code: DiffCom compares their individual subgraphs
again and if a signiﬁcant portion (50%) is found to be
similar, they are considered related and therefore their
intersection will not be reported to the market.

After the correlation check, all the apps going through
the intersection analysis are very likely to be unrelated.
Therefore, legitimate code shared between them, if any,
is almost always public libraries or templates. As de-
scribed before, we removed such common code through
white-listing popular libraries and further complemented
the list with those discovered from the training set: meth-
ods in at least 2,363 apps were considered legitimate
public resources if all these apps were cleared by Virus-
Total. Such code was further sampled and manually an-
alyzed in our study to ensure that it indeed did not in-
volve any suspicious activities. With all such libraries
removed, the shared code, particularly the method with
dangerous APIs (e.g., getSimSerialNumber), is re-
ported as possible malicious payload.

666  24th USENIX Security Symposium 

USENIX Association

8

Evading MassVet. To evade MassVet, the adversary
could try to obfuscate app code, which can be tolerated
to some degree by the similarity comparison algorithm
we use [7]. For example, common obfuscation tech-
niques, such as variable/method renaming, do not affect
centroids. Also the commonality analysis can only be
defeated when the adversary is willing to signiﬁcantly
alter the attack payload (e.g., a malicious SDK) each
time when he uses it for repackaging. This is not sup-
ported by existing automatic tools like ADAM [53] and
DroidChameleon [32], which always convert the same
code to the same obfuscated form. Further, a deep obfus-
cation will arouse suspicion when the app is compared
with its repackaging origin that has not been obfuscated.
The adversary may also attempt to obfuscate the app’s
view graphs. This, however, is harder than obfuscat-
ing code, as key elements in the graph, like event func-
tions OnClick, OnDrag, etc., are hardcoded within
the Android framework and cannot be modiﬁed. Also
adding junk views can be more difﬁcult than it appears
to be: the adversary cannot simply throw in views dis-
connected from existing sub-graphs, as they will not af-
fect how MassVet determines whether two view-graphs
match (Section 3.2); otherwise, he may connect such
views to existing sub-graphs (potentially allowing them
to be visited from the existing UI), which requires under-
standing a legitimate app’s UI structures to avoid affect-
ing user experience.

We further analyzed the effectiveness of existing ob-
fuscation techniques against our view-graph approach
over 100 randomly selected Google-Play apps. Popu-
lar obfuscation tools such as DexGuard [37] and Pro-
Guard [38] only work on Java bytecode, not the Dalvik
bytecode of these commercial apps. In our research, we
utilized ADAM [53] and DroidChameleon [32], which
are designed for Dalvik bytecode, and are highly effec-
tive according to prior studies [53, 32]. Supposedly they
can also work on view-related code within those apps.
However after running them on the apps, we found that
their v-cores, compared with those before the obfusca-
tion, did not change at all. This demonstrates that such
obfuscation is not effective on our view-graph approach.
On the other hand, we acknowledge that a new obfus-
cation tool could be built to defeat MassVet, particularly
its view-graph search and the Com step. The cost for do-
ing this, however, is less clear and needs further effort to
understand (Section 5).

3.4 System Building
In our research, we implemented a prototype of MassVet
using C and Python, nearly 1.2 million apps collected
from 33 markets, including over 400,000 from Google
Play (Section 4.1). Before these apps can be used to
vet new submissions, they need to be inspected to de-

tect malicious code already there. Analyzing apps of this
scale and utilizing them for a real-time vetting require
carefully designed techniques and a proper infrastructure
support, which we elaborate in this section.
System bootstrapping and malware detection. To
bootstrap our system, a market ﬁrst needs to go through
all its existing apps using our techniques in an efﬁ-
cient way. The APKs of these apps are decompiled into
smali (using the tool baksmali [36]3) to extract their
view graphs and individual methods, which are further
converted into v-cores and m-cores respectively. We use
NetworkX [29] to handle graphs and ﬁnd loops. Then
these features (i.e., cores) are sorted and indexed before
stored into their individual databases. In our implemen-
tation, such data are preserved using the Sqlite database
system, which is characterized by its small size and ef-
ﬁciency. For all these apps, 1.5 GB was generated for
v-cores and 97 GB for m-cores.

The next step is to scan all these 1.2 million apps for
malicious content. A straightforward approach is to in-
spect them one by one through the binary search. This
will take tens of millions of steps for comparisons and
analysis. Our implementation includes an efﬁcient al-
ternative. Speciﬁcally, on the v-core database, our sys-
tem goes through the whole sequence from one end (the
smallest element) to the other end, evaluating the ele-
ments along the way to form equivalent groups: all those
with identical v-cores are assigned to the same group4.

All the subgraphs within the same group match each
other. However, assembling them together to determine
the similarity between two apps turns out to be tricky.
This is because the UI of each app is typically broken
into around 20 subgraphs distributed across the whole
ordered v-core sequence. As such, any attempt to make
the comparison between two apps requires to go through
all equivalent groups. The fastest way to do that is to
maintain a table for the number of view subgraphs shared
between any two apps. However, this simple approach
requires a huge table, half of 1.2 million by 1.2 million in
the worst case, which cannot be completely loaded into
the memory. In our implementation, a trade-off is made
to save space by only inspecting 20,000 apps against the
rest 1.2 million each time, which requires going through
the equivalent groups for 60 times and uses about 100
GB memory at each round.

The inspection on m-cores is much simpler and does
not need to compare one app against all others. This
is because all we care about here are just the common
methods that already show up within individual equiva-

3Very few apps (0.01%) cannot be decompiled in our dataset due to

the limitation of the tool.

4In our implementation, we set the threshold τ to zero, which can
certainly be adjusted to tolerate some minor variations among similar
methods.

USENIX Association  

24th USENIX Security Symposium  667

9








 










Figure 3: Cloud framework for MassVet.

lent groups. Those methods are then further analyzed to
detect suspicious ones.
Cloud support. To support a high-performance vetting
of apps, MassVet is designed to work on the cloud, run-
ning on top of a stream processing framework (Figure 3).
Speciﬁcally, our implementation was built on Storm [40],
an open-source stream-processing engine that also pow-
ers leading web services such as WebMD, Alibaba, Yelp,
etc. Storm supports a large-scale analysis of a data
stream by a set of worker units that connect to each
other, forming a topology.
In our implementation, the
work ﬂow of the whole vetting process is converted into
such a topology: a submitted app is ﬁrst disassembled
to extract view graphs and methods, which are checked
against the white list to remove legitimate libraries and
templates; then, the app’s v-cores and m-cores are calcu-
lated, and a binary search on the v-core database is per-
formed; depending on the results of the search, the differ-
ential analysis is ﬁrst run, which can be followed by the
intersection analysis. Each operation here is delegated to
a worker unit on the topology and all the data associated
with the app are in a single stream. The Storm engine
is designed to support concurrently processing multiple
streams, which enables a market to efﬁciently vet a large
number of submissions.

4 Evaluation and Measurement
4.1 Setting the Stage
App collection. We collected 1.2 million apps from 33
Android markets, including over 400,000 from Google
Play, 596,437 from 28 app stores in China, 61,866 from
European stores and 27,047 from other US stores as elab-
orated in Table 5 in Appendix. We removed duplicated
apps according to their MD5. All the apps we down-
loaded from Google Play have complete meta data (up-
load date, size, number of downloads, developer, etc.),
while all those from third-party markets do not.

The apps from Google Play were selected from 42 cat-
egories, including Entertainment, Tools, Social, Com-
munication, etc. From each category, we ﬁrst went for
its top 500 popular ones (in terms of number of installs)
and then randomly picked up 1000 to 30,000 across the

whole category. For each third-party market, we just ran-
domly collected a set of apps (Table 5) (190 to 108,736,
depending on market sizes). Our collection includes
high-proﬁled apps such as Facebook, Skype, Yelp, Pin-
terest, WeChat, etc. and those less popular ones. Their
sizes range from less than 1 MB to more than 100 MB.
Validation. For the suspicious apps reported by our pro-
totype, we validated them through VirusTotal and man-
ual evaluations. Virustotal is the most powerful public
malware detection system, which is a collection of 54
anti-malware products, including the most high-proﬁle
commercial scanners. It also provides the scanning ser-
vice on mobile apps [44]. VirusTotal has two modes,
complete scanning (which we call new scan) and using
cached results (called cached scan). The latter is fast,
capable of going through 200 apps every minute, but
only covers those that have been scanned before. For
the programs it has never seen or uploaded only recently,
the outcome is just “unknown”. The former determines
whether an app is malicious by running on it all 54 scan-
ners integrated within VirusTotal. The result is more up-
to-date but the operation is much slower, taking 5 min-
utes for each app.

To validate tens of thousands suspicious cases detected
from the 1.2 million apps (Section 4.2), we ﬁrst per-
formed the cached scan to conﬁrm that most of our ﬁnd-
ings were indeed malicious. The apps reported to be “un-
known” were further randomly sampled for a new scan.
For all the apps that VirusTotal did not ﬁnd malicious,
we further picked up a few samples for a manual anal-
ysis. Particularly, for all suspicious apps identiﬁed by
the intersection analysis, we clustered them according to
their shared code. Within each cluster, whenever we ob-
served that most members were conﬁrmed malware, we
concluded that highly likely the remaining apps there are
also suspicious, even if they were not ﬂagged by Virus-
Total. The common code of these apps were further
inspected for suspicious activities such as information
leaks. A similar approach was employed to understand
the diff code extracted during the differential analysis.
We manually identiﬁed the activities performed by the
code and labeled it as suspicious when they could lead to
damages to the user’s information assets.

4.2 Effectiveness and Performance
Malware found and coverage. From our collection,
MassVet reported 127,429 suspicious apps (10.93%).
10,202 of them were caught by “Diff” and the rest
were discovered by “Com”. These suspicious apps are
from different markets: 30,552 from Google Play and
96,877 from the third-party markets, as illustrated in Ta-
ble 5. We ﬁrst validated these ﬁndings by uploading them
to VirusTotal for a cached scan (i.e., quickly checking
the apps against the checksums cached by VirusTotal),

668  24th USENIX Security Symposium 

USENIX Association

10

AV Name
Ours (MassVet)
ESET-NOD32
VIPRE
NANO-Antivirus
AVware
Avira
Fortinet
AntiVir
Ikarus
TrendMicro-HouseCall
F-Prot
Sophos
McAfee

# of Detection
197
171
136
120
87
79
71
60
60
59
47
46
45

% Percentage
70.11
60.85
48.40
42.70
30.96
28.11
25.27
21.35
21.35
21.00
16.73
16.37
16.01

Table 1: The coverages of other leading AV scanners.

which came back with 91,648 conﬁrmed cases (72%),
17,061 presumably false positives (13.38%, that is, the
apps whose checksums were in the cache but not found
to be malicious when they were scanned) and 13,492
unknown (10.59%, that is, the apps whose checksums
were not in VirusTotal’s cache). We further randomly
selected 2,486 samples from the unknown set and 1,045
from the “false-positive” set, and submitted to VirusTotal
again for a new scan (i.e., running all the scanners, with
the most up-to-date malware signatures, on the apps). It
turned out that 2,340 (94.12%) of unknown cases and
349 (33.40%) of “false positives” are actually malicious
apps, according to the new analysis. This gives us a false
detection rate (FDR: false positives vs. all detected) of
9.46% and a false positive rate (FPR: false positives vs.
all apps analyzed) of 1%, solely based upon VirusTotal’s
scan results. Note that the Com step found more mal-
ware than Diff, as Diff relies on the presence of two apps
of same repackaging origins in the dataset, while Com
only looks for common attack payloads shared among
apps. It turns out that many malicious-apps utilize same
malicious SDKs, which make them easier to catch.

We further randomly sampled 40 false positives re-
ported by the new scan for a manual validation and found
that 20 of them actually are highly suspicious. Specif-
ically, three of them load and execute suspicious code
dynamically; one takes pictures stealthily; one performs
sensitive operation to modify the booting sequence of
other apps; seven of them get sensitive user information
such as SIM card SN number and telephone number/ID;
several aggressive adware turn out to add phishing plug-
ins and app installation bars without the user’s consent.
The presence of these activities makes us believe that
very likely they are actually zero-day malware. We have
reported all of them to four Antivirus software vendors
such as Norton and F-Secure for a further analysis. If
all these cases are conﬁrmed, then the FDR of MassVet
could further be reduced to 4.73%.

To understand the coverage of our approach, we
randomly sampled 2,700 apps from Google Play and
scanned them using MassVet and the 54 scanners within
VirusTotal. All together, VirusTotal detected 281 apps

# Apps Pre-Processing v-core database differential m-core database
search (Intersection)

analysis

sum

search
0.15
0.15
0.14
0.16
0.16

10
50
100
200
500

1.80
1.99
2.23
3.13
3.56

0.33
0.34
0.35
0.35
0.35

5.84
5.85
5.85
5.88
5.88

8.12
8.33
8.57
9.52
9.95
Table 2: Performance: “Apps” here refers to the number of
concurrently submitted apps.
and among them our approach got 197 apps. The cover-
age of MassVet, with regard to the collective result of all
54 scanners, is 70.1%, better than what could be achieved
by any individual scanner integrated within VirusTo-
tal, including such top-of-the-line antivirus systems as
NOD32 (60.8%), Trend (21.0%), Symantec (5.3%) and
McAfee (16%). Most importantly, MassVet caught at
least 11% malware those scanners missed. The details
of the study are presented in Table 1 (top 12).
Vetting delay. We measured the performance of our
technique, on a server with 260 GB memory, 40 cores
at 2.8 GHz and 28 TB hard drives. Running on top of the
Storm stream processor, our prototype was tested against
1 to 500 concurrently submitted apps. The average delay
we observed is 9 seconds, from the submission of the app
to the completion of the whole process on it. This vetting
operation was performed against all 1.2 million apps.

Table 2 further shows the breakdown of the vetting
time at different vetting stages, including preprocessing
(v-core and m-core generation), search across the v-core
database, the differential analysis, search over the m-core
database and the intersection analysis. Overall, we show
that MassVet is indeed capable of scaling to the level of
real-world markets to provide a real-time vetting service.

4.3 Measurement and Findings
Over the 127,429 malicious apps detected in our study,
we performed a measurement study that brings to light
a few interesting observations important for understand-
ing the seriousness of the malware threat to the Android
ecosystem, as elaborated below.
Landscape.
The malware we found are distributed
across the world: over 35,473 from North America,
4,852 from Europe and 87,104 from Asia. In terms of
the portion of malicious code within all apps, Chinese
app markets take the lead with 12.90%, which is fol-
lowed by US, with 8.28%. This observation points to
a possible lack of regulations and proper security protec-
tion in many Chinese markets, compared with those in
other countries. Even among the apps downloaded from
Google Play, over 7.61% are malicious, which is differ-
ent from a prior report of only 0.1% malware discovered
there [15]. Note that most of the malware here has been
conﬁrmed by VirusTotal. This indicates that indeed the
portion of the apps with suspicious activities on leading
app stores could be higher than previously thought. De-

USENIX Association  

24th USENIX Security Symposium  669

11






















































































































































































































































Figure 4: The distribution of down-
loads for malicious or suspicious apps
in Google Play.

Figure 5: The distribution of rating for
malicious or suspicious apps in Google
Play.

Figure 6: The distribution of average
number of downloads for malicious or
suspicious apps in Google Play.

tailed numbers of malicious apps are shown in Appendix
(Table 5).

We observed that most scanners react slowly to the
emergence of new malware. For all 91,648 malicious
apps conﬁrmed by VirusTotal, only 4.1% were alarmed
by at least 25 out of 54 scanners it hosts. The results are
present in Figure 7. This ﬁnding also demonstrates the
capability of MassVet to capture new malicious content
missed by most commercial scanners.















             



Figure 7: Number of malware detected by VirusTotal.
The impacts of those malicious apps are signiﬁcant.
Over 5,000 such apps have already been installed over
10,000 times each (Figure 4). Also, there are a few
extremely popular ones, with the install count reaching
1 million or even more. Also, the Google-Play ratings
of the suspicious APKs are high (most of them ranging
from 3.6 to 4.6, Figure 5), with each being downloaded
for many times (100,000 to 250,000) on average (Fig-
ure 6). This suggests that hundreds of millions of mobile
devices might have already been infected.
Existing defense and disappeared apps. Apparently,
Google Play indeed makes effort to mitigate the malware
threat. However, our measurement study also shows the
challenge of this mission. As Figure 8 illustrates, most
malware we discovered were uploaded in the past 14
months. Also the more recently an app shows up, the
more likely it is problematic. This indicates that Google
Play continuously inspects the apps it hosts to remove
the suspicious ones. For the apps that have already been
there for a while, the chance is that they are quite legiti-
mate, with only 4.5% found to be malicious. On the other
hand, the newly released apps are much less trustworthy,
with 10.69% of them being suspicious. Also, these mali-
cious apps have a pretty long shelf time, as Google needs
up to 14 months to remove most of them. Among the
malware we discovered, 3 apps uploaded in Dec. 2010
are still there in Google Play.

Interestingly, 40 days after uploading 3,711 apps
(those we asked VirusTotal to run new scan upon, as
mentioned earlier) to VirusTotal, we found that 250 of
them disappeared from Google Play. 90 days later, an-
other 129 apps disappeared. Among the 379 disappeared
apps, 54 apps (14%) were detected by VirusTotal. Ap-
parently, Google does not run VirusTotal for its vetting
but pays close attention to the new malware it ﬁnds.

We further identiﬁed 2,265 developers of the 3,711
suspicious apps, using the apps’ meta data, and moni-
tored all their apps in the follow-up 15 weeks (November
2014 to February 2015). Within this period, we observed
that additional 204 apps under these developers disap-
peared, all of which were detected by MassVet, due to
the suspicious methods they shared with the malware we
caught before that period. The interesting part is that we
did not scan these apps within VirusTotal, which indi-
cates that it is likely that Google Play also looked into
their malicious components and utilized them to check
all other apps under the same developers. However, ap-
parently, Google did not do this across the whole market-
place, because we found that other apps carrying those
methods were still there on Google Play. If these apps
were missed due to the cost for scanning all the apps on
the Play Store, MassVet might actually be useful here:
our prototype is able to compare a method across all 1.2
million apps within 0.1 second.

Another interesting ﬁnding is that we saw that some of
these developers uploaded the same or similar malicious
apps again after they were removed. Actually, among the
2,125 reappeared apps, 604 conﬁrmed malware (28.4%)
showed up in the Play Store unchanged, with the same
MD5 and same names. Further, those developers also
published 829 apps with the same malicious code (as that
of the malware) but under different names. The fact that
the apps with known malicious payloads still got slipped
in suggests that Google might not pay adequate attention
to even known malware.
Repackaging malware
and malicious payload.
Among the small set of repackaging malware captured
by the differential analysis, most are from third-party
stores (92.35%).
Interestingly, rarely did we observe
that malware authors repackaged Google Play apps

670  24th USENIX Security Symposium 

USENIX Association

12





































                          



Figure 8: Number of malicious apps overtime.

Figure 9: The distribution of common code across malware.

and distributed them to the third-party stores in China.
Instead, malware repackaging appears to be quite
localized, mostly between the app stores in the same
region or even on the same store. A possible explanation
could be the effort that malware authors need to make
on the original app so that it works for a new audience,
which is certainly higher than simply repackaging the
popular one in the local markets.

Figure 9 illustrates the distribution of common code
across malware, as discovered from the intersection anal-
ysis. A relatively small set of methods have been reused
by a large number of malicious apps. The leading one
has been utilized by 9,438 Google-Play malware and by
144 suspicious apps in the third-party markets. This
method turns out to be part of the library (“com/star-
tapp”) extensively used by malware. Over 98% of the
apps integrating this library were ﬂagged as malicious
by VirusTotal and the rest were also found to be suspi-
cious through our manual validation. This method sends
users’ ﬁne-grained location information to a suspicious
website. Similarly, all other popular methods are appar-
ently also part of malware-building toolkits. Examples
include “guohead”, “purchasesdk” and “SDKUtils”. The
malware integrating such libraries are signed by thou-
sands of different parties. An observation is that the use
of these malicious SDKs is pretty regional: in Chinese
markets, “purchasesdk” is popular, while “startapp” is
widely used in the US markets. We also noticed that a
number of libraries have been obfuscated. A close look
at these attack SDKs shows that they are used for getting
sensitive information like phone numbers, downloading
ﬁles and loading code dynamically.
Signatures and identities. For each conﬁrmed mali-
cious app, we took a look at its “signature”, that is, the
public key on its X.509 certiﬁcate for verifying the in-
tegrity of the app. Some signatures have been utilized by
more than 1,000 malware each: apparently, some mal-
ware authors have produced a large number of malicious
apps and successfully disseminated them across differ-
ent markets (Table 3). Further, when we checked the
meta data for the malware discovered on Google Play,
we found that a few signatures have been associated with
many identities (e.g., the creator ﬁeld in the meta-
data). Particularly, one signature has been linked to 604
identities, which indicates that the adversary might have

Signature

# of malicious apps

c673c8a5f021a5bdc5c036ee30541dde
a2993eaecf1e3c2bcad4769cb79f1556
3be7d6ee0dca7e8d76ec68cf0ccd3a4a
f8956f66b67be5490ba6ac24b5c26997
86c2331f1d3bb4af2e88f485ca5a4b3d

1644
1258
615
559
469

Table 3: Top 5 signatures used in apps.

created many accounts to distribute his app (Table 4).
Case studies. Among the suspicious apps MassVet re-
ported are a set of APKs not even VirusTotal found to
be malicious. We analyzed 40 samples randomly cho-
sen from this set and concluded that 20 of them were
indeed problematic through manual analysis, likely to be
zero-day malware. We have reported them to 4 malware
companies (F-Secure, Norton, Kaspersky, Trend Micro)
for further validations. The behaviors of these apps in-
clude installing apps without user’s consent, collecting
user’s private data (e.g., take screen shots of other apps)
even though such information does not serve apps’ stated
functionalities, loading and executing native binary for
command and control.

These apps use various techniques to evade detec-
tion. For example, some hide the suspicious function-
ality for weeks before starting to run it. “Durak card
game” is such an game, which has been downloaded over
5,000,000 times. It was on Google Play before BBC re-
ported it on February 4th 2015 [25]. So far, only two
scanners hosted by VirusTotal can detect it. This mal-
ware disguises as warning messages when the user un-
lock her Android smartphone. It waits for several weeks
before performing malicious activities.
Its advertise-
ments also do not show up until at least one reboot. Al-
though Google removes “Durak card game”, other apps
with similar functionalities are still on the Play Store
now. We also found that some malicious apps conceal
their program logic inside native binaries. Some even
encrypt the binaries and dynamically decrypt them for
execution. Further some utilize Java reﬂection and other
obfuscation techniques to cover their malicious code.

Signature

# of different identities

02d98ddfbcd202b13c49330182129e05
a2993eaecf1e3c2bcad4769cb79f1556
82fd3091310ce901a889676eb4531f1e
9187c187a43b469fa1f995833080e7c3
c0520c6e71446f9ebdf8047705b7bda9
Table 4: Top 5 signatures used by different identities.

604
447
321
294
145

USENIX Association  

24th USENIX Security Symposium  671

13

5 Discussion
As discussed before, MassVet aims at repackaging mal-
ware, the mainstay of potentially harmful mobile apps:
this is because malware authors typically cannot afford
to spend a lot of time and money to build a popular app
just for spreading malware, only to be forced to do this
all over again once it gets caught. Our technique ex-
ploits a weakness of their business model, which relies
on repackaging popular apps with a similar attack pay-
load to keep the cost for malware distribution low. With
the fundamentality of the issue and the effectiveness of
the technique on such malware, our current implementa-
tion, however, is still limited, particularly when it comes
to the defense against evasion.

Speciﬁcally, though simply adding the junk views con-
nected to an existing app’s view graph can affect user ex-
perience and therefore may not work well (Section 3.3),
a more effective alternative is to obfuscate the links be-
tween views (calls like StartActivity). However,
this treatment renders an app’s UI structure less clear
to our analyzer, which is highly suspicious, as the vast
majority of apps’ view graphs can be directly extracted.
What we could do is to perform a dynamic analysis on
such an app, using the tools like Monkey to explore the
connections between different views. Note that the over-
all performance impact here can still be limited, simply
because most apps submitted to an app store are legiti-
mate and their UI structures can be statically determined.
Further, to evade the commonality analysis, the ad-
versary could obfuscate the malicious methods. As dis-
cussed earlier (Section 3.3), this attempt itself is nontriv-
ial, as the m-cores of those methods can only be moved
signiﬁcantly away from their original values through
substantial changes to their CFGs each time when a le-
gitimate app is repackaged. This can be done by adding
a large amount of junk code on the CFGs. Our current
implementation does not detect such an attack, since it is
still no there in real-world malware code. On the other
hand, further studies are certainly needed to better under-
stand and mitigate such a threat.

Critical to the success of our DiffCom analysis is re-
moval of legitimate libraries. As an example, we could
utilize a crawler to periodically gather shared libraries
and code templates from the web to update our whitelists.
Further, a set of high-proﬁle legitimate apps can be ana-
lyzed to identify the shared code missed by the crawler.
What can also be leveraged here is a few unique re-
sources in the possession of the app market. For exam-
ple, it knows the account from which the apps are up-
loaded, even though they are signed by different certiﬁ-
cates. It is likely that legitimate organizations are only
maintaining one account and even when they do have
multiple ones, they will not conceal the relations among
them. Using such information, the market can ﬁnd out

whether two apps are actually related to identify the in-
ternal libraries they share. In general, given the fact that
MassVet uses a large number of existing apps (most of
which are legitimate) to vet a small set of submissions, it
is at the right position to identify and remove most if not
all legitimate shared code.

6 Related Work
Malicious app detection. App vetting largely relies on
the techniques for detecting Android malware. Most ex-
isting approaches identify malicious apps either based
upon how they look like (i.e., content-based signa-
ture) [20, 27, 21, 45, 51, 57, 19, 54, 17, 22, 4] or how
they act (behavior-based signature) [11, 31, 48, 47, 42,
18, 34]. Those approaches typically rely on heavyweight
static or dynamic analysis techniques, and cannot detect
the unknown malware whose behavior has not been mod-
eled a priori. MassVet is designed to address these is-
sues by leveraging unique properties of repackaging mal-
ware. Most related to our work is PiggyApp [54], which
utilizes the features (permissions, APIs, etc.) identiﬁed
from a major component shared between two apps to ﬁnd
other apps also including this component, then clusters
the rest part of these apps’ code, called piggybacked pay-
loads, and samples from individual clusters to manually
determine whether the payloads there are indeed mali-
cious. In contrast, MassVet automatically detects mal-
ware through inspecting the code diff among apps with a
similar UI structure and the common methods shared be-
tween those unrelated. When it comes to the scale of our
study, ANDRUBIS [26, 46] dynamically examined the
operations of over 1 million apps in four years. Different
from ANDRUBIS, which is an off-line analyzer for re-
covering detailed behavior of individual malicious apps,
MassVet is meant to be a fast online scanner for iden-
tifying malware without knowing its behavior. It went
through 1.2 million of apps within a short period of time.
Repackaging and code reuse detection. Related to our
work is repackaging and code reuse detection [55, 21,
1, 9, 10, 41, 35, 5]. Most relevant to MassVet is the
Centroids similarity comparison [7], which is also pro-
posed for detecting code reuse. Although it is a building
block for our technique, the approach itself does not de-
tect malicious apps. Signiﬁcant effort was made in our
research to build view-graph and code analysis on top of
it to achieve an accurate malware scan. Also, to defeat
code obfuscation, a recent proposal leverages the simi-
larity between repackaged apps’ UIs to detect their rela-
tions [50]. However, it is too slow, requiring 11 seconds
to process a pair of apps. In our research, we come up
with a more effective UI comparison technique, through
mapping the features of view graphs to their geometric
centers, as Centroids does. This signiﬁcantly improves
the performance of the UI-based approach, enabling it to

672  24th USENIX Security Symposium 

USENIX Association

14

help vet a large number of apps in real time.

7 Conclusion
We present MassVet, an innovative malware detection
technique that compares a submitted app with all other
apps on a market, focusing on its diffs with those hav-
ing a similar UI structure and intersections with others.
Our implementation was used to analyze nearly 1.2 mil-
lion apps, a scale on par with that of Google Play, and
discovered 127,429 malicious apps, with 20 likely to be
zero-day. The approach also achieves a higher coverage
than leading anti-malware products in the market.

Acknowledgement

We thank our shepherd Adam Doup´e and anonymous re-
viewers for their valuable comments. We also thank Dr.
Sencun Zhu and Dr. Fangfang Zhang for sharing their
ViewDroid code, which allows us to understand how
their system works, and VirusTotal for the help in val-
idating over 100,000 apps discovered in our study. IU
authors were supported in part by the NSF 1117106,
1223477 and 1223495. Kai Chen was supported in part
by NSFC 61100226, 61170281 and strategic priority re-
search program of CAS (XDA06030600). Peng Liu was
supported by NSF CCF-1320605 and ARO W911NF-09-
1-0525 (MURI).

References
[1] ANDROGUARD.

goodware analysis of android applications
http://code.google.com/p/androguard/, 2013.

Reverse

engineering, malware

and
... and more.

[2] APPBRAIN. Ad networks - android library statistics — app-
brain.com. http://www.appbrain.com/stats/libraries/ad. (Visited
on 11/11/2014).

[3] APPCELERATOR.

6

steps

to

great mobile

apps.

http://www.appcelerator.com/. 2014.

[4] ARZT, S., RASTHOFER, S., FRITZ, C., BODDEN, E., BARTEL,
A., KLEIN, J., LE TRAON, Y., OCTEAU, D., AND MCDANIEL,
P. Flowdroid: Precise context, ﬂow, ﬁeld, object-sensitive and
lifecycle-aware taint analysis for android apps. In PLDI (2014),
ACM, p. 29.

[5] BAYER, U., COMPARETTI, P. M., HLAUSCHEK, C., KRUEGEL,
C., AND KIRDA, E. Scalable, behavior-based malware cluster-
ing. In NDSS (2009), vol. 9, Citeseer, pp. 8–11.

[6] CHEN, K. A list of shared libraries and ad libraries used in
android apps. http://sites.psu.edu/kaichen/ 2014/02/20/a-list-of-
shared-libraries-and-ad-libraries-used-in-android-apps.

[7] CHEN, K., LIU, P., AND ZHANG, Y. Achieving accuracy and
scalability simultaneously in detecting application clones on an-
droid markets. In ICSE (2014).
2014

[8] CISCO.

report,”.

security

annual

“cisco

http://www.cisco.com/web/offer/gist ty2 asset/
Cisco 2014 ASR.pdf, 2014.

[9] CRUSSELL, J., GIBLER, C., AND CHEN, H. Attack of the
clones: Detecting cloned applications on android markets. ES-
ORICS (2012), 37–54.

[10] CRUSSELL, J., GIBLER, C., AND CHEN, H. Scalable semantics-
In ESORICS

based detection of similar android applications.
(2013).

[11] ENCK, W., GILBERT, P., CHUN, B.-G., COX, L. P., JUNG, J.,
MCDANIEL, P., AND SHETH, A. Taintdroid: An information-
ﬂow tracking system for realtime privacy monitoring on smart-
phones. In OSDI (2010), vol. 10, pp. 1–6.

[12] ENCK, W., OCTEAU, D., MCDANIEL, P., AND CHAUDHURI,
S. A study of android application security. In USENIX security
symposium (2011), vol. 2, p. 2.

[13] ENVATOMARKET.

Android notiﬁcation templates library.

http://codecanyon.net/item/android-notiﬁcation-templates-
library/5292884. 2014.

[14] ERNST, M. D., JUST, R., MILLSTEIN, S., DIETL, W. M.,
PERNSTEINER, S., ROESNER, F., KOSCHER, K., BARROS, P.,
BHORASKAR, R., HAN, S., ET AL. Collaborative veriﬁcation of
information ﬂow for a high-assurance app store.

[15] F-SECURE. F-secure : Internet security for all devices. http://f-

secure.com, 2014.

[16] F-SECURE.

Threat report h2 2013.

Tech. rep., f-secure,

http://www.f-secure.com/documents/996508/1030743/
Threat Report H2 2013.pdf, 2014.

[17] FENG, Y., ANAND, S., DILLIG, I., AND AIKEN, A. Ap-
poscopy: Semantics-based detection of android malware through
static analysis. In SIGSOFT FSE (2014).

[18] GILBERT, P., CHUN, B.-G., COX, L. P., AND JUNG, J. Vi-
sion: automated security validation of mobile apps at app mar-
kets.
In Proceedings of the second international workshop on
Mobile cloud computing and services (2011), ACM, pp. 21–26.
[19] GRACE, M., ZHOU, Y., ZHANG, Q., ZOU, S., AND JIANG,
X. Riskranker: scalable and accurate zero-day android mal-
ware detection. In Proceedings of the 10th international confer-
ence on Mobile systems, applications, and services (2012), ACM,
pp. 281–294.

[20] GRIFFIN, K., SCHNEIDER, S., HU, X., AND CHIUEH, T.-C.
Automatic generation of string signatures for malware detec-
tion. In Recent Advances in Intrusion Detection (2009), Springer,
pp. 101–120.

[21] HANNA, S., HUANG, L., WU, E., LI, S., CHEN, C., AND
SONG, D. Juxtapp: A scalable system for detecting code reuse
among android applications. In DIMVA (2012).

[22] HUANG, H., CHEN, K., REN, C., LIU, P., ZHU, S., AND WU,
D. Towards discovering and understanding unexpected hazards
in tailoring antivirus software for android. In AsiaCCS (2015),
ACM, pp. 7–18.

[23] JING, Y., ZHAO, Z., AHN, G.-J., AND HU, H. Morpheus: au-
tomatically generating heuristics to detect android emulators. In
Proceedings of the 30th Annual Computer Security Applications
Conference (2014), ACM, pp. 216–225.

[24] KASSNER, M.

Google play: Android’s bouncer can be
http://www.techrepublic.com/blog/it-security/-google-

pwned.
play-androids-bouncer-can-be-pwned/, 2012.

[25] KELION, L. Android adware ’infects millions’ of phones and
tablets. http://www.bbc.com/news/technology-31129797, 2015.
[26] LINDORFER, M., NEUGSCHWANDTNER, M., WEICHSEL-
BAUM, L., FRATANTONIO, Y., VAN DER VEEN, V., AND
PLATZER, C. Andrubis-1,000,000 apps later: A view on current
android malware behaviors. In Proceedings of the the 3rd Inter-
national Workshop on Building Analysis Datasets and Gathering
Experience Returns for Security (BADGERS) (2014).

[27] LINDORFER, M., VOLANIS, S., SISTO, A., NEUGSCHWANDT-
NER, M., ATHANASOPOULOS, E., MAGGI, F., PLATZER, C.,
ZANERO, S., AND IOANNIDIS, S. Andradar: Fast discovery of
android applications in alternative markets. In DIMVA (2014).

[28] LU, L., LI, Z., WU, Z., LEE, W., AND JIANG, G. Chex: stat-
ically vetting android apps for component hijacking vulnerabili-
ties. In Proceedings of the 2012 ACM conference on Computer
and communications security (2012), ACM, pp. 229–240.

[29] NETWORKX.

Python

package

and

ing
and
https://pypi.python.org/pypi/networkx/1.9.1, 2015.

manipulating

graphs

for

creat-
networks.

USENIX Association  

24th USENIX Security Symposium  673

15

[30] OBERHEIDE, J., AND MILLER, C. Dissecting the android

bouncer. SummerCon2012, New York (2012).

[31] RASTOGI, V., CHEN, Y., AND ENCK, W. Appsplayground: Au-
tomatic security analysis of smartphone applications. In Proceed-
ings of the third ACM conference on Data and application secu-
rity and privacy (2013), ACM, pp. 209–220.

[32] RASTOGI, V., CHEN, Y., AND JIANG, X. Catch me if you can:
Evaluating android anti-malware against transformation attacks.
Information Forensics and Security, IEEE Transactions on 9, 1
(2014), 99–108.

I.

. D.

[33] READING,

Google play exploits bypass malware
checks. http://www.darkreading.com/risk-management/google-
play-exploits-bypass-malware-checks/d/d-id/1104730?, 6 2012.
[34] REINA, A., FATTORI, A., AND CAVALLARO, L. A system call-
centric analysis and stimulation technique to automatically recon-
struct android malware behaviors. EuroSec, April (2013).

[35] REN, C., CHEN, K., AND LIU, P. Droidmarking: resilient soft-
ware watermarking for impeding android application repackag-
ing. In ASE (2014), ACM, pp. 635–646.

[36] SMALI. An assembler/disassembler for android’s dex format.

http://code.google.com/p/smali/, 2013.

[37] SQUARE, G. Dexguard.

https://www.saikoa.com/dexguard,

2015.

[38] SQUARE, G. Proguard. https://www.saikoa.com/proguard, 2015.
[39] STATISTA.
portal.

statistics

Statista

The

:

http://www.statista.com/, 2014.

[40] STORM, A. Storm, distributed and fault-tolerant realtime com-

putation. https://storm.apache.org/.

[41] VIDAS, T., AND CHRISTIN, N. Sweetening android lemon mar-
kets: measuring and combating malware in application market-
places. In Proceedings of the third ACM conference on Data and
application security and privacy (2013), ACM, pp. 197–208.

[42] VIDAS, T., TAN, J., NAHATA, J., TAN, C. L., CHRISTIN, N.,
AND TAGUE, P. A5: Automated analysis of adversarial android
applications. In Proceedings of the 4th ACM Workshop on Secu-
rity and Privacy in Smartphones & Mobile Devices (2014), ACM,
pp. 39–50.

[43] VIRUSTOTAL. Virustotal - free online virus, malware and url

scanner. https://www.virustotal.com/, 2014.

https://www.virustotal.com/en/documentation/mobile-
applications/, 2015.

[45] WALENSTEIN, A., AND LAKHOTIA, A. The software simi-
Internat. Begegnungs-und

larity problem in malware analysis.
Forschungszentrum f¨ur Informatik, 2007.

[46] WEICHSELBAUM, L., NEUGSCHWANDTNER, M., LINDORFER,
M., FRATANTONIO, Y., VAN DER VEEN, V., AND PLATZER,
C. Andrubis: Android malware under the magnifying glass. Vi-
enna University of Technology, Tech. Rep. TRISECLAB-0414-001
(2014).

[47] WU, C., ZHOU, Y., PATEL, K., LIANG, Z., AND JIANG, X.
Airbag: Boosting smartphone resistance to malware infection. In
NDSS (2014).

[48] YAN, L. K., AND YIN, H. Droidscope: seamlessly reconstruct-
ing the os and dalvik semantic views for dynamic android mal-
ware analysis. In In USENIX rSecurity 12’.

[49] YAN, P. A look at repackaged apps and their effect on the
mobile threat landscape. http://blog.trendmicro.com/trendlabs-
security-intelligence/a-look-into-repackaged-apps-and-its-role-
in-the-mobile-threat-landscape/, 7 2014. Visited on 11/10/2014.
[50] ZHANG, F., HUANG, H., ZHU, S., WU, D., AND LIU, P. View-
droid: Towards obfuscation-resilient mobile application repack-
aging detection. In Proceedings of the 7th ACM Conference on
Security and Privacy in Wireless and Mobile Networks (WiSec
2014). ACM (2014).

[51] ZHANG, Q., AND REEVES, D. S. Metaaware: Identifying meta-
In Computer Security Applications Confer-

morphic malware.

[44] VIRUSTOTAL.

Virustotal

for

android.

AndroidLeyuan

ence, 2007. ACSAC 2007. Twenty-Third Annual (2007), IEEE,
pp. 411–420.

[52] ZHANG, Y., YANG, M., XU, B., YANG, Z., GU, G., NING, P.,
WANG, X. S., AND ZANG, B. Vetting undesirable behaviors in
android apps with permission use analysis. In Proceedings of the
2013 ACM SIGSAC conference on Computer & communications
security (2013), ACM, pp. 611–622.

[53] ZHENG, M., LEE, P. P., AND LUI, J. C. Adam: An automatic
and extensible platform to stress test android anti-virus systems.
In Detection of Intrusions and Malware, and Vulnerability As-
sessment (2013), pp. 82–101.

[54] ZHOU, W., ZHOU, Y., GRACE, M., JIANG, X., AND ZOU, S.
Fast, scalable detection of piggybacked mobile applications. In
CODASPY (2013).

[55] ZHOU, W., ZHOU, Y., JIANG, X., AND NING, P. Detecting
repackaged smartphone applications in third-party android mar-
ketplaces. In Proceedings of the second ACM conference on Data
and Application Security and Privacy (2012), ACM, pp. 317–
326.

[56] ZHOU, Y., AND JIANG, X. Dissecting android malware: Char-
In Security and Privacy (SP), 2012

acterization and evolution.
IEEE Symposium on (2012), IEEE, pp. 95–109.

[57] ZHOU, Y., WANG, Z., ZHOU, W., AND JIANG, X. Hey, you,
get off of my market: Detecting malicious apps in ofﬁcial and
alternative android markets. In NDSS (2012).

[58] ZORZ,

Z.

1.2info.

security.org/secworld.php?id=15976, 11 2013.
11/10/2014).

http://www.net-
(Visited on

8 Appendix

Appstore

Anzhi
Yidong
yy138
Anfen
Slideme

gfun
16apk

Pandaapp
Lenovo
Haozhuo
Dangle

3533 world
Appchina
Wangyi
Youyi
Nduo
Sogou
Huawei

Opera
Mumayi
Google
Xiaomi
others
Amazon
Baidu
7xiazi
Liqu
Gezila

Yingyongbao

AndroidRuanjian

Anji

AndroidMarket

# of malicious apps # of total apps studied Percentage Country
China
China
China
China
US
China
China
China
US
China
China
China
China
China
China
China
China
China
China
China
China
China
China
Europe
China
US
China
China
US
China
China
China
China

46055
3026
2950
1572
15367
6053
108736
25714
10679
68839
8052
22183
9886
62449
663
3628
190
23774
1466
2812
2308
41607
24332
61866
79594
401549
12139
38648
1001
21122
26195
26392
5000

17921
1088
828
365
3285
997
17779
4008
1577
9799
1100
2992
1331
8396
85
408
20
2414
148
272
198
3467
1997
4852
6129
30552
832
2377
59
831
898
394
30

38.91
35.96
28.07
23.22
21.38
16.47
16.35
15.59
14.77
14.23
13.66
13.49
13.46
13.44
12.82
11.25
10.53
10.15
10.1
9.67
8.58
8.33
8.21
7.84
7.7
7.61
6.85
6.15
5.89
3.93
3.43
1.49
0.6

Table 5: App Collection & Malware in Different Markets.

674  24th USENIX Security Symposium 

USENIX Association

16

