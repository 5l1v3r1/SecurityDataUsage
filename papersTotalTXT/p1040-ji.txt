Structural Data De-anonymization: Quantiﬁcation,

Practice, and Implications

School of Electrical and Computer Engineering

School of Electrical and Computer Engineering

Shouling Ji

Weiqing Li

Georgia Institute of Technology

sji@gatech.edu

Mudhakar Srivatsa

IBM T. J. Watson Research Center

msrivats@us.ibm.com

Georgia Institute of Technology

wli64@gatech.edu

Raheem Beyah

Georgia Institute of Technology
rbeyah@ece.gatech.edu

School of Electrical and Computer Engineering

ABSTRACT
In this paper, we study the quantiﬁcation, practice, and
implications of structural data (e.g., social data, mobility
traces) De-Anonymization (DA). First, we address several
open problems in structural data DA by quantifying per-
fect and (1 − ϵ)-perfect structural data DA, where ϵ is the
error tolerated by a DA scheme. To the best of our knowl-
edge, this is the ﬁrst work on quantifying structural data DA
under a general data model, which closes the gap between
structural data DA practice and theory. Second, we con-
duct the ﬁrst large-scale study on the de-anonymizability of
26 real world structural datasets, including Social Networks
(SNs), Collaborations Networks, Communication Networks,
Autonomous Systems, and Peer-to-Peer networks. We also
quantitatively show the conditions for perfect and (1 − ϵ)-
perfect DA of the 26 datasets. Third, following our quantiﬁ-
cation, we design a practical and novel single-phase cold start
Optimization based DA (ODA) algorithm. Experimental
analysis of ODA shows that about 77.7%−83.3% of the users
in Gowalla (.2M users and 1M edges) and 86.9% − 95.5% of
the users in Google+ (4.7M users and 90.8M edges) are de-
anonymizable in diﬀerent scenarios, which implies optimiza-
tion based DA is implementable and powerful in practice.
Finally, we discuss the implications of our DA quantiﬁcation
and ODA and provide some general suggestions for future
secure data publishing.

Categories and Subject Descriptors
C.2.0 [General]: Security and protection; H.4 [Information
Systems Applications]: Miscellaneous; G.3 [Probability
and Statistics]: Stochastic processes

General Terms
Security, Privacy, Theory, Management

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
Copyright 2014 ACM 978-1-4503-2957-6/14/11 ...$15.00.
http://dx.doi.org/10.1145/2660267.2660278.

Keywords
De-anonymization; structural data; quantiﬁcation; evalua-
tion; social networks; mobility traces

1.

INTRODUCTION

Nowadays, a large amount of data generated by comput-
er networks and services have a graph structure, which is
referred to as structural data. For instance, it is straight-
forward to model Social Networks (SNs), network topolo-
gies, etc. by graphs [2][5][6][26]. Additionally, mobility
traces (e.g., WiFi contacts, Instant Message contacts) can
also be modeled as graphs (structural data) [3]. Even gen-
eral spatiotemporal data (mobility traces) with the classi-
cal (latitude, longitude, timestamp) format can be convert-
ed to structural data by applying sophisticated techniques
[27]. Since these structural data have huge commercial value
to businesses and potentially signiﬁcant impacts to society
[28][29], the security and privacy issues that arise during da-
ta release to the public, sharing with commercial partners,
and/or transferring to third parties are attracting increasing
interest [1][2][3].

Currently, to protect structural data’s privacy, the most
common technique used is to anonymize data by remov-
ing the “Personally Identiﬁable Information (PII)” before
releasing data. Unfortunately, this naive method is shown
to be vulnerable to many De-Anonymization (DA) attacks
[6][7][8]. Latterly, some sophisticated anonymization schemes
to protect structural data privacy, e.g., k-anonymity and its
variants [6][7][8], were designed1. They can protect the pri-
vacy of structural data to some extent. However, they are
susceptible to emerging structure based DA attacks due to
the limitations of the schemes (e.g., they are syntactic prop-
erties based) and the rich amount of information available
to adversaries [1][2][3] (see the detailed analysis in Section
2).

In structure based DA attacks, some auxiliary data (graph-
s) are employed to break the privacy of anonymized struc-
tural data based only on the structural information. The fac-
t that the auxiliary data may come from either the same or
a diﬀerent domain/context with the anonymized data makes

1Also, diﬀerential privacy [9] was developed to protect
the privacy of interactive data release. However, it cannot
defend against structural data DA attacks which breach the
privacy of non-interactive data release [2][3][8][9].

1040the attack powerful, e.g., using Flickr to de-anonymize Twit-
ter [2], using Facebook to de-anonymize WiFi mobility traces
[3]. Furthermore, the wide availability of auxiliary data
makes the attack applicable and practical [2][3].

Structure based DA attacks were initially presented in [1],
where Backstrom et al. designed both active and passive at-
tacks to break the privacy of SN users. However, since the
attacks in [1] leverage the success of a “sybil” attack before
actual anonymized data publication, they are less practical.
Later, Narayanan and Shmatikov designed a new structure
based DA attack in [2], which successfully de-anonymized
a large scale directed social network by applying several
heuristics such as eccentricity, edge directionality, reverse
match.
In [3], Srivatsa and Hicks demonstrated that the
privacy of three kinds of mobility traces can be compro-
mised by structure based DA attacks. However, the attack-
s presented in [3] are only suitable for small datasets due
to the computational infeasibility of ﬁnding a proper land-
mark mappings for large datasets. Furthermore, each of the
aforementioned attacks consist of two phases: a landmark
identiﬁcation phase and a DA propagation phase.

Although we already have some successful structure based
DA techniques [1][2][3], we do not have any rigorous theo-
retical analysis under a general model yet that explains why
structure based DA attacks work.
In [5], Pedarsani and
Grossglauser quantiﬁed the privacy of anonymized struc-
tural data under the Erd¨os-R´enyi (ER) random graph mod-
el G(n, p) (every edge exits with identical probability p).
However, this quantiﬁcation is not suitable in practice since
most, if not all, observed real world structural data (e.g.,
SNs, collaboration networks [21][22][26]) do not follow the
ER model. Actually, they may follow the power-law mod-
el, exponential model, etc.
[21][22][26]. Therefore, under a
practical general data model, there are still some open ques-
tions in DA research, including: why can structural data
be de-anonymized ? what are the conditions for successful
structural data DA? and what portion of users can be de-
anonymized in a structural dataset? To close the practice-
theory gap, we study the quantiﬁcation, practice, and im-
plications of structural data DA in this paper. Speciﬁcally,
our contributions are as follows.

• To the best of our knowledge, this is the ﬁrst work on
quantifying structural data DA under a general data
model. In our quantiﬁcation, we answer several fun-
damental open questions: why can structural data be
de-anonymized based only on the topological informa-
tion (the inherent reason for the success of existing
structure based DA practices)? what are the condi-
tions for perfect and (1 − ϵ)-perfect DA, where ϵ is the
error tolerated by a DA scheme? what portion of users
can be de-anonymized in a structural dataset? Thus,
we close the gap between structural data DA practice
and theory.

• We conduct the ﬁrst large-scale study on the de-anonymiz-

ability of 26 real world structural datasets, including
SNs, location based mobility traces and SNs, collabo-
ration networks, communication networks, autonomous
systems, peer-to-peer networks, etc. Based on our s-
tudy, we ﬁnd that all the structural datasets that we
considered are perfectly or partially de-anonymizable.
We also quantitatively show the conditions for perfect

and (1 − ϵ)-perfect DA and what portion of users can
be de-anonymized for the 26 datasets.

• Following our quantiﬁcation, we present a novel Opti-
mization based DA (ODA) attack. Diﬀerent from ex-
isting structure based DA attacks [1][2][3], ODA is a
single-phase cold start algorithm without any require-
ment on priori knowledge, e.g., landmark mappings.
We also examine ODA on real datasets Gowalla (.2M
users and 1M edges) and Google+ (4.7M users and
90.8M edges). The results demonstrate that about
77.7% − 83.3% of the users in Gowalla and 86.9% −
95.5% of the users in Google+ are de-anonymizable,
which illustrates that optimization based DA is imple-
mentable and powerful in practice.

• Finally, we discuss some implications of this work ac-
cording to our structural DA quantiﬁcation and the
ODA attack. We further provide some general sugges-
tions for future secure data publishing.

The rest of this paper is organized as follows. In Section
In Section 3, we give
2, we summarize the related work.
the data and attack models. In Section 4, we theoretically
quantify perfect and (1− ϵ)-perfect DA attacks under a gen-
eral model, followed by a large-scale evaluation in Section
5. In Section 6, we present a novel optimization based DA
attack. We discuss the implications in Section 7. The paper
is concluded and future work is addressed in Section 8.

2. RELATED WORK

In this section, we ﬁrst brieﬂy survey the state-of-the-
art advances of security issues related to structural data.
Subsequently, we discuss the status quo on structural data
anonymization and DA followed by remarking the charac-
teristics that distinguish this paper from existing works.
2.1 State-of-the-Art Advances

Recently, the security and privacy issues related to struc-
tural data have attracted the interest of many researchers
[1]-[20]. In [10], Korolova et al. presented an attack on link
privacy in SNs. Another attack using link-based and group-
based classiﬁcation to study privacy implications in SNs is
discussed in [11] by Zheleva and Getoor. Based on four
previously unrecognized implicit identiﬁers, Pang et al. de-
veloped an automated procedure to identify users in 802.11
traces [12].
In [13], Backstrom proposed an algorithm to
predict users’ location using SN information. On the oth-
er hand, multiple strategies have been developed to protec-
t people’s privacy in SN systems and related applications,
e.g., pseudonym abstraction [14], decentralized protocols for
anonymous communications [15], guaranteed data lifetime
[16], compromised accounts detection [17], privacy preserv-
ing SN applications [18].
In addition, location based ser-
vices, especially those in smartphone-based SN applications,
have created big commercial beneﬁts. However, on the other
hand, the publicly availability of users’ mobility trace data
causes a potentially serious threat to users’ privacy and even
themselves [3][19][20].
2.2 Structural Data Anonymization and DA
2.2.1 Anonymization Schemes

1041To protect the privacy of structural data, the most com-
mon method is removing the PII [1][2][3]. However, this
widely used naive solution is proven to be vulnerable to
many DA attacks [6][7][8]. Later, researchers proposed some
sophisticated data anonymization solutions, e.g., k-anonymity
and its many variants [6][7][8]. These solutions do work to
protect users’ privacy against semantics based DA attacks to
some extent. However, according to recent empirical studies
[2][19], these solutions fail against emerging structure based
DA attacks. Some of the reasons are as follows. First, the
adversaries may obtain much richer auxiliary information
through multiple channels, e.g., data mining, advertising,
third-party applications, data aggregation. [1][2][3][4]. Sec-
ond, the k-anonymity scheme is applicable to low-average-
degree datasets. Nevertheless, many datasets, e.g., Google+
[25], Facebook [26], tend to have a large average degree and
still increasing. Finally, the k-anonymity idea lies on data’s
syntactic property, which may not work on protecting data
privacy even if this property is satisﬁed [2].
2.2.2 DA Schemes
Several successful DA attacks on structural data were pro-
posed recently, including structure based schemes [1][2][3]
and semantics based schemes [4]. In [4], Wondracek et al.
designed a DA attack to SN users based on the group mem-
bership information. To implement this attack, the adver-
sary should collect enough group membership information
(semantics information) by “history stealing” on browsers,
and then try to uniquely identify a user.

Instead of leveraging semantic information, we focus on
structure based DA attacks in this paper. In [1], Backstrom
et al.
introduced the structure based DA attacks by de-
signing both active and passive attacks in SNs. Since the
designed attacks in [1] leverage a “sybil” attack before the
actual anonymized data release, they are not scalable as
SNs increase in size [2]. Later, Narayanan and Shmatikov
in [2] designed a two-phase heuristics based DA attack a-
gainst large scale directed SNs. Through experiments on real
datasets, they demonstrated the feasibility of large scale DA
in terms of structure information. In [3], Srivatsa and Hicks
presented three DA attacks to mobility traces, which are all
two-phase schemes. In all the three attacks, to achieve high
DA accuracy, the DA propagation (the second phase) must
be repeated for all k! possible landmark mappings (k is the
number of landmarks), which is very time-consuming. For
instance, to de-anonymize a small dataset having 125 user-
s with 5 landmarks, the three attacks take 6.7 hours, 6.2
hours, and 0.5 hours, respectively. Furthermore, the attacks
in [3] are not suitable for large scale DA since in that sce-
nario more landmarks are required (k > 30, [2]), while when
k ≥ 20, the attacks in [3] are computationally infeasible.
2.2.3 Remarks
In this paper, we study the quantiﬁcation, practice, and
implications of structural data DA. The main aspects dis-
tinguishing this paper from existing works are as follows.
(i) To the best of our knowledge, this is the ﬁrst work that
quantiﬁes structural data DA under a general model. By our
quantiﬁcation, we answered several open questions, e.g., why
can structural data be de-anonymized? what are the condi-
tions for structural data DA? what portion of users can be
de-anonymized? and thus we bridge the gap between struc-
tural data DA practice and theoretical quantiﬁcation. (ii)

Following our theoretical quantiﬁcation, we conduct a large
scale study on 26 real world structural datasets. We also
conduct comprehensive and in-depth analysis on the evalua-
tion results. (iii) Following our quantiﬁcation, we propose a
novel single-phase optimization based DA algorithm (ODA).
ODA is a cold start algorithm without any requirement on
priori knowledge. By conducting experiments on large scale
real datasets, we demonstrate the eﬀectiveness of ODA.

3. SYSTEM MODEL

In this paper, we focus on quantifying the DA attack (vul-
nerability) on anonymized structural data, which could be
social data released by SN operators, (e.g., Google+ [25])
and/or mobility data generated by mobile devices (e.g., clas-
sical longitude-latitude spatiotemporal traces [26][27]).
3.1 Data Model

It is straightforward to model social data using graphs,
where nodes represent users and edges indicate the social
relationships (friendship, contact, following) among users.
Mobility data generated by users (users’ devices) can al-
so be modeled by contact graphs [3][27]. Furthermore, it
has been shown that a contact graph derived from mobili-
ty data has strong correlation with the social graph of the
same group of users that generated them [3][27]. There-
fore, we model the anonymized structural data by a graph
Ga = (V a, Ea), where V a = {i|i is an anonymized user} is
the user set and Ea = {ea
i,j| there is a relationship between
i ∈ V a and j ∈ V a} is the edge/relationship set. In reali-
ty, it is possible that a structural dataset corresponds to a
directed graph, e.g., Twitter. However, for simplicity and
without loss of generality, we assume Ga as an undirected
graph. Note that, the designed algorithm in this paper can
be extended to the directed scenario directly. For i ∈ V a,
i = {j|∃ea
i,j ∈ Ea} and we
its neighborhood is deﬁned as N a
i |, i.e., the degree of i.
i as |N a
denote the cardinality of N a

The auxiliary data is also assumed to be structural da-
ta, e.g., a SN compromising users overlapped with that in
Ga [2][3]. Furthermore, the auxiliary data is easily obtain-
able by multiple means such as academic and government
data mining, advertising, third-party applications, data ag-
gregation, online crawling, etc. Successful examples can be
found in [2][3][4][27]. Consequently, the auxiliary data is al-
so modeled by a graph Gu = (V u, Eu), where V u = {i is a
known user} and Eu = {eu
i,j| there is a relationship between
i ∈ V u and j ∈ V u}. Similarly, the neighborhood of i ∈ V u
i,j ∈ Eu}.
i = {j|∃eu
is deﬁned as N u
3.2 DA Attack
Given Ga and Gu, a DA attack can be formally deﬁned
as a mapping: σ : V a → V u. For ∀i ∈ V a, its mapping
under σ is σ(i) ∈ V u∪{⊥}, where ⊥ is a special not existing
indicator. Similarly, for ∀ea
∈
Eu ∪ {⊥}. Under σ, a successful DA on i ∈ V a is deﬁned as
′ ∈ V u and i and i
′
correspond to the same user;
σ(i) = i
or σ(i) =⊥, otherwise. For other cases, the DA on i fails.
Consequently, the objective of a DA attack is to successfully
de-anonymize as many users in V a as possible.

i,j ∈ Ea, σ(ea

, if i

i,j) = eu

σ(i),σ(j)

′

4. DA QUANTIFICATION

In this section, given Ga and Gu, we quantify a DA attack
under an arbitrary graph distribution in multiple scenarios.

1042(a) G

(b) Ga

(c) Gu

Figure 1: Edge/relationship projection. Only black
edges appear in Ga/Gu.

Note that, our quantiﬁcation aims to provide a theoretical
foundation for understanding the success of recent heuristic
structure-based DA practices [2][3].
4.1 Preliminaries

To make the quantiﬁcation and proof tractable and conve-
nient, we make some assumptions and deﬁnitions. First, we
assume V a = V u, i.e., the auxiliary data and the anonymized
data are corresponding to the same group of users [2][3][5].
This does not mean that we know any priori correct mapping
from V a to V u. Furthermore, this assumption is reasonable
since one cannot be expected to use Gu to de-anonymize Ga
if they correspond to diﬀerent groups of users. It is possi-
ble that the auxiliary data only has some overlap with the
anonymized data instead of corresponding to the exactly
same group of users. This fact does not limit our theoretical
analysis since we can either (i) apply the quantiﬁcation to
new = V a ∪ (V u \ V a)
the overlapping part, or (ii) redeﬁne V a
new = V u ∪ (V a \ V u), i.e. adding the non-overlapped
and V u
users to V a and V u respectively as isolated users (with de-
gree 0), and apply the analysis to Ga = (V a
new, Ea) and
Gu = (V u
new, Eu). Without of causing any confusion, we
assume V a = V u in the rest of this section.

Second, similar to [5], for the users in V a (or, V u), we
assume that there exists a conceptual underlying graph G =
(V, E) with V = V a = V u and E consisting of the true
relationships among users in V . Consequently, Ga and Gu
can be viewed as the physically observable projections of
G on particular relationships, e.g., “circle” relationship on
Google+, “co-occurrence” relationship in Gowalla. The pro-
jection from G to Ga is characterized by an edge/relationship
projection process [5]: (i) V a = V ; and (ii) ∀ei,j ∈ E, ei,j is
appeared in Ea with probability pa, i.e., Pr(ei,j ∈ Ea|ei,j ∈
E) = pa. Similarly, the projection from G to Gu can be
characterized by another edge/relationship projection pro-
cess with probability pu. For instance, we show a projec-
tion from G to Ga/Gu in Fig. 1. Furthermore, we assume
both projection processes are independent and identically
distributed (i.i.d.). Note that, (i) although the assumption
on the existence of a conceptual underlying graph and the
projection process makes the quantiﬁcation problem theo-
retically tractable, it is still a challenging issue in practice;
and (ii) assuming Ga and Gu are projected from an under-
lying network implies Ga and Gu have a strong structural
correlation. Intuitively, this assumption is reasonable since
they correspond to the same group of users and the em-
pirical results in [2][3] also supports such strong structural
correlation.
Based on the above assumptions, we have n! possible DA
schemes σ : V a → V u to de-anonymize Ga, among which
the only one perfect DA scheme (∀i ∈ V a, i is successfully
de-anonymized) is denoted by σ0.

4.2 Model and Formalization
Now, given G, we denote |V | = n and |E| = m. Let
V = {1, 2,··· , n} and di be the degree of i ∈ V . Then,
we deﬁne D =< d1, d2,··· , dn > as the degree sequence of
the nodes (users) in V . Furthermore, let ∆1 and ∆2 (resp.,
δ1 and δ2) be the maximum and second maximum (resp.,
minimum and second minimum) degrees of G, respectively.
In [5], Pedarsani and Grossglauser quantiﬁed the privacy of
G when G is an ER random graph G(n, p) 2. The G(n, p)
model is very useful as a source of insight into the study
of structural data, e.g., SNs [5][21]. However, the degree
distribution of G(n, p) tends to follow the Poisson distribu-
tion, which is quite diﬀerent from the degree distribution-
s of most, if not all, observed real world structural data
(e.g., SNs) [21][22]. Actually, the degree distribution of re-
al world structural data may follow any distribution such
as the power-law distribution, exponential distribution, etc.
[21][22]. Therefore, it is signiﬁcant to understand and quan-
tify a DA attack for structural data under an arbitrary degree
distribution. To this end, we characterize G by a generalized
graph model, the conﬁguration model [21]. Under the con-
ﬁguration model, a graph is speciﬁed by an arbitrary degree
sequence D rather than a particular degree distribution. S-
ince D is an arbitrary degree sequence, D can follow an
arbitrary distribution observed in real world data [21].
Let pi,j be the probability of an edge existing between
i, j ∈ V . Then, we have pi,j = didj
didj
2m−1
2m , which is
a key property of the conﬁguration model [21]. From pi,j, it
is more likely of an existing edge between two users with high
degrees. Based on pi,j, we deﬁne l = min{pi,j|i, j ∈ V, i ̸= j}
and h = max{pi,j|i, j ∈ V, i ̸= j}, i.e., l and h are the
lower and upper bounds of pi,j respectively. Then, given G
with arbitrary degree distribution, we have l ≥ δ1δ2
2m−1 and
h ≤ ∆1∆2
2m−1 .
′ ≤ n, i ∈
′ ∈ V u} ⊆ V a × V u, we deﬁne the DA Error (DE) on
V a, i
i′ \ N a
i |,
a user mapping (i, i
which measures the neighborhoods’ diﬀerence between i in
in Gu under the particular σ. Then, we deﬁne the
Ga and i
ψi,i′ . Taking
overall DE for a particular σ as Ψσ =

Finally, given any DA scheme σ = {(i, i
) ∈ σ as ψi,i′ = |N a

as m→∞

≃

′

)|1 ≤ i, i
i′| + |N u
i \ N u
∑

(i,i′)∈σ

′

′

Ga and Gu shown in Fig. 1 as an example, the DE of the
perfect DA scheme σ0 is Ψσ0 = 20. For another DA scheme
σ = (σ0 \ {(4, 4), (5, 5)}) ∪ {(4, 5), (5, 4)} (users 4 and 5 are
incorrectly de-anonymized to each other), its DE is Ψσ =
28. In the following subsections, we quantify a DA attack
by studying the conditions on G and the projection process
under which perfect and (1 − ϵ)-perfect DA attacks can be
conducted.
4.3 Perfect DA Quantiﬁcation

Now, we quantify the conditions for perfect DA attacks.
Some useful properties of the binomial distribution that will
be used in the proofs are as follows.

Lemma 1. (i) Let X ∼ B(n1, p) and Y ∼ B(n2, p) be
independent binomial variables. Then, X + Y is again a
binomial variable and X + Y ∼ B(n1 + n2, p); (ii) [5] Let X
and Y be two binomial random variables with means λx and

2Based on the projection process, Ga and Gu are also ER

random graphs G(n, p · pa) and G(n, p · pu), respectively.

1(cid:13)2(cid:13)3(cid:13)4(cid:13)5(cid:13)6(cid:13)7(cid:13)8(cid:13)9(cid:13)10(cid:13)1(cid:13)2(cid:13)3(cid:13)4(cid:13)5(cid:13)6(cid:13)7(cid:13)8(cid:13)9(cid:13)10(cid:13)1(cid:13)2(cid:13)3(cid:13)4(cid:13)5(cid:13)6(cid:13)7(cid:13)8(cid:13)9(cid:13)10(cid:13)1043λy, respectively. Then, when λx > λy, Pr(X − Y ≤ 0) ≤
2 exp(− (λx−λy )2
8(λx+λy ) ).

4.3.1 Same Projection Probability
First, we consider the scenario that the projection pro-
cesses from G to Ga and Gu are characterized by the same
probability ℘, i.e., pa = pu = ℘. Let f℘ = ℘[l(1−h℘)−h(1−℘)]2
2(l(1−h℘)+h(1−℘))
be a variable depending on ℘. Then, we have the follow-
ing Theorem 1 which indicates the conditions on ℘ and f℘
such that it is asymptotically almost surely (a.a.s.)3
that
Ψσ ≥ Ψσ0 for any DA scheme σ ̸= σ0. We defer the proof

to Appendix A for readability.

Theorem 1. For any σ ̸= σ0, let k be the number of
diﬀerent mappings between σ and σ0, i.e., the number of
incorrect mappings in σ. Then, 2 ≤ k ≤ n and Pr(Ψσ ≥
Ψσ0 ) →

h−hl and f℘ = Ω( 2 ln n+1

n→∞ 1 when ℘ > h−l

).

kn

In Theorem 1, we quantiﬁed the condition on ℘, l, and h
under which the perfect DA scheme σ0 will cause less DE
than any other given DA scheme σ ̸= σ0. To guarantee
the uniqueness of σ0 (i.e., σ0 is the one and the only one
DA scheme introducing the least DE), intuitively, stronger
conditions on ℘, l, and h are required. We quantify such
conditions in Theorem 2. We defer the proof to Appendix
B for readability.

Theorem 2. Let E be the event that there exists at least
one DA scheme σ ̸= σ0 such that Ψσ ≤ Ψσ0 . When ℘ >
), where 2 ≤ k ≤ n, Pr(E) →
h−l
h−hl and f℘ = Ω( (k+3) ln n+1
0, i.e., it is a.a.s. that @σ s.t. σ ̸= σ0 and Ψσ ≤ Ψσ0 .

kn

kn

h−hl and f℘ = Ω( (k+3) ln n+1

From Theorem 2, although we seek a stronger result, the
condition on ℘ is the same as in Theorem 1 and the condition
on f℘ only has an increase of order Θ(k). Based on Theorem
2, if ℘ > h−l
), the perfect DA
scheme causes the least DE. Furthermore, the number of
possible DA schemes is upper-bounded. Therefore, when the
conditions on ℘ and f℘ are satisﬁed, Ga can mathematically
be perfectly de-anonymized by Gu based on the structure
information only.
4.3.2 Different Projection Probabilities
Now, we quantify the conditions on pa, pu, l, and h when
pa ̸= pu for structure based perfect DA attacks. Let gpa,pu =
papu
4(l(pa+pu−2hpapu)+h(pa+pu−2papu)) be t-
pa+pu
wo variables depending on pa and pu. Then, we have the fol-
lowing theorems quantifying the conditions on gpa,pu , fpa,pu , l,
and h under which it is a.a.s. Ψσ ≥ Ψσ0 for any σ ̸= σ0.

and fpa,pu = (l(pa+pu−2hpapu)−h(pa+pu−2papu))2

We omit the proofs due to space limitation.

Theorem 3. Pr(Ψσ ≥ Ψσ0 ) → 1 for any σ ̸= σ0 when

gpa,pu > h−l

2(h−lh) and fpa,pu = Ω( 2 ln n+1

kn

).

Theorem 4. It is a.a.s. that @σ s.t. σ ̸= σ0 and Ψσ ≤
2(h−lh) and fpa,pu = Ω( (k+3) ln n+1
),

Ψσ0 when gpa,pu > h−l
where 2 ≤ k ≤ n.

kn

From Theorem 4, to guarantee the uniqueness of inducing
the least DE of σ0, which is a stronger conclusion compared
n → ∞, with probability goes to 1 an event happens.

3Asymptotically almost surely (a.a.s.)

implies that as

with that in Theorem 3, the condition on gpa,pu is the same
as in Theorem 3 and the condition on fpa,pu has an increase
of Θ(k). Furthermore, Theorem 4 quantiﬁes the conditions
under which the anonymized structural data can be mathe-
matically perfectly de-anonymized when pa ̸= pu.
(1 − ϵ)-Perfect DA Quantiﬁcation
4.4
Formally, we deﬁne a (1 − ϵ)-perfect DA, denoted by σϵ,
as a DA scheme under which at most ϵ|V a| = ϵn users are
tolerated to be incorrectly de-anonymized, where 0 ≤ ϵ ≤ 1.
Under the (1 − ϵ)-perfect DA assumption, any σk is prop-
er as long as k ≤ ϵn, i.e., we take it as a satisﬁable de-
anonymizatoin solution. Theoretically, the conditions on
(1 − ϵ)-perfect DA are quantiﬁed in Theorems 5 and 6.
Again, we omit the proofs due to space limitation. Note
that, when we quantify the conditions for (1− ϵ)-perfect de-
aonymization, we do not distinguish σ0 and σk with k ≤ ϵn,
since they are all proper solutions. Hence, as in the scenario
of perfect DA, our quantiﬁcation takes σ0 as the reference
point.

ϵn2

Theorem 5. (i) When pa = pu = ℘, ℘ > h−l

h−hl , and
≥ Ψσ0 ) for any σk with k >
2(h−lh) , and fpa,pu =

), Pr(Ψσk

f℘ = Ω( 2 ln n+1
ϵn; (ii) When pa ̸= pu, gpa,pu > h−l
Ω( 2 ln n+1

≥ Ψσ0 ) for any σk with k > ϵn.
Theorem 6. (i) When pa = pu = ℘, ℘ > h−l

), Pr(Ψσk

ϵn2

), it is a.a.s.

ϵn2

f℘ = Ω( (ϵn+3) ln n+1
such that k > ϵn and Ψσk
gpa,pu > h−l
that @σk s.t. k > ϵn and Ψσk

2(h−lh) , and fpa,pu = Ω( (ϵn+3) ln n+1

h−hl , and
that there exists no σk
≤ Ψσ0 ; (ii) When pa ̸= pu,
≤ Ψσ0 .

), it is a.a.s.

ϵn2

kn ) to Ω( ln n

From Theorem 5, we can see that (i) for any DA scheme
σk, if it has more than ϵn incorrect mappings, with proba-
bility 1, it will cause more DE than σ0. On the other hand,
if σk is a (1 − ϵ)-perfect DA scheme, i.e., k ≤ ϵn, we cannot
a.a.s. distinguish σk and σ0 based on DE under the quan-
tiﬁed conditions; (ii) compared with the quantiﬁcations in
Theorems 1 and 3, the conditions on f℘ and fpa,pu change
from Ω( ln n
n2 ) explicitly, which implies a relaxation
of the condition on f℘ and fpa,pu . This relaxation comes
from the toleration of ϵn incorrect user mappings. As in the
scenario of perfect DA, stronger conditions can be quantiﬁed
to guarantee (1 − ϵ)-perfect DA schemes causing the least
DE as shown in Theorem 6. From Theorem 6, we can see
that even ϵn matching errors are tolerated, the conditions on
℘ and gpa,pu stay the same while the conditions on f℘ and
fpa,pu only have some constant relaxation compared with
the perfect DA scenario.

5. LARGE SCALE EVALUATION ON

RE-AL WORLD STRUCTURAL DATASETS
According to our quantiﬁcation, even without semantic
priori knowledge, anonymized structural data can be de-
anonymized perfectly or (1 − ϵ)-perfectly.
In this section,
we conduct comprehensive evaluations of our DA quantiﬁ-
cation on 26 real world structural datasets4.

4We actually conduct evaluations on 60+ real world
datasets. Due to space limitation, the results on 26 repre-
sentative datasets are shown in the paper. Complete results
and source codes are available up to request.

1044Name

Google+
Twitter

LiveJournal

Facebook
YouTube

Orkut

Slashdot

Pokec

Infocom
Smallblue
Brightkite

Gowalla
HepPh
AstroPh
CondMat

DBLP
Enron
EuAll
Wiki
AS733
Oregon
Caida
Skitter

Gnutella3
Gnutella4
Gnutella5

d

4K

5.3

Table 1: Data statistics.
Type

m

n

ρ

SN
SN
SN
SN
SN
SN
SN
SN

p(1) p(5)
4.7M 90.8M 8.24E-6 38.7 .054 .273
.5M 14.9M 1.20E-4 54.8 .053 .198
4.8M 69M 3.70E-6 17.9 .210 .505
88K 1.08E-2 43.7 .019 .113
1.1M
3M 4.64E-6
.531 .855
3.1M 117.2M 2.48E-5 76.3 .022 .073
82.2K
1M 1.73E-4 14.2 .022 .593
1.6M 30.6M 1.67E-5 27.3 .100 .307
.068 .493
8.07E-2
212
73
.133 .625
375
5.25E-2
120
.354 .718
.2M 1.32E-4
58K
1M 4.92E-5
.252 .645
.2M
.2M 1.87E-3 21.0 .100 .500
12K
18.8K .4M 1.23E-3 22.0 .053 .337
.078 .518
23.1K .2M 4.00E-4
.3M 1.1M 2.09E-5
.136 .670
36.7K .2M 3.19E-4 10.7 .281 .679
.837 .973
.3M
.4M 1.35E-5
WikiTalk 2.4M
.738 .962
5M 1.63E-6
.355 .896
6.5K 13.9K 6.63E-4
.289 .876
11.5K 32.7K 4.98E-4
26.5K 53.4K 1.52E-4
.375 .924
1.7M 11.1M 7.73E-6 13.1 .128 .554
.413 .710
26.5K 65.4K 1.86E-4
.448 .718
36.7K 88.3K 1.32E-4
62.6K .1M 7.56E-5
.458 .725

LMSN
LMSN
LMSN
LMSN
ColN
ColN
ColN
ColN
Email
Email

AS
AS
AS
AS
P2P
P2P
P2P

3.0
3.9
4.3
5.7
4.0

5.8
6.3
7.5
9.7

4.9
4.8
4.7

8.6
6.6

5.1 Evaluation Setup

During the quantiﬁcation, pi,j is an important parameter
although we quantify the conditions in laconic expressions in
terms of its bounds l and h. However, it is diﬃcult to accu-
rately determine pi,j in practice [5][21][27]. Fortunately, it is
not necessary to know the exact pi,j to numerically evaluate
our DA quantiﬁcation. Actually, according to our deriva-
tion, we only have to determine the statistical expectation
value of pi,j, denoted by E(pi,j). For a dataset with degree
sequence D, deﬁne pD = E(pi,j). Then, it is statistically
reasonable (especially for large datasets) to use the graph
n(n−1) to approximate pD, i.e., pD ≃ ρ [5][21].
density ρ = 2m
On the other hand, we focus on demonstrating the statisti-
cal behavior of our perfect/(1−ϵ)-perfect DA quantiﬁcation.
Therefore, we use ρ to approximate pD in our evaluation.
Furthermore, for the convenience of evaluation, we evaluate
the quantiﬁcation in the scenario of pa = pu = ℘. This does
not limit our evaluation since it is straightforward to extend
to the pa ̸= pu scenario (actually, both scenarios exhibit sim-
ilar behaviors, which can also be seen in the quantiﬁcation).
2(2−pD℘−℘) . Then, we have the following con-
clusions, which can be proven by similar techniques as in
Theorems 1, 2, 5, and 6 from the statistical perspective.

Let fD = pD℘(℘−pD℘)2

k

Theorem 7. For perfect DA, (i) Pr(Ψσ ≥ Ψσ0 ) → 1
(1−pD)(2kn−k2)+(2pD−1)k and
2kn−k2−k ); (ii) it is a.a.s. that @σ s.t. σ ̸= σ0 and
(1−pD)(2kn−k2)+(2pD−1)k and fD =

for any σ ̸= σ0 when ℘ >
fD = Ω( 4 ln n+2
Ψσ ≤ Ψσ0 when ℘ >
Ω( 2(k+3) ln n+2

2kn−k2−k ).
Theorem 8. For (1−ϵ)-perfect DA, (i) Pr(Ψσk

≥ Ψσ0 ) →
(1−pD)(2kn−k2)+(2pD−1)k
ϵn2 ); (ii) it is a.a.s. that @σk s.t. k > ϵn and
(1−pD)(2kn−k2)+(2pD−1)k and fD =

1 for any σk with k > ϵn when ℘ >
and fD = Ω( ln n
Ψσ ≤ Ψσ0 when ℘ >
Ω( ln n

k

k

k

n ).

5.2 Datasets

We evaluate our quantiﬁcation on 26 datasets from mul-
tiple domains, including SNs data, Location based Mobility
traces and SN (LMSN) data, Collaboration Network (ColN)
data, communication network (Email, WikiTalk) data, Au-
tonomous Systems (AS) graph data, and Peer-to-Peer (P2P)
network graph data [3][25][26][27]. In Tab. 1, we show some
statistics on the employed datasets, where d represents the
average degree of n nodes and p(i) indicates the percentage
of nodes with degree of i or less.

Due to space limitation, we brieﬂy introduce the datasets

as follows. Detailed descriptions can be found in [3][25][26][27].
SN: Google+, Twitter, LiveJournal, Facebook, YouTube,
Orkut, Slashdot, and Pokec are 8 well known SNs [25][26].
LMSN: Infocom consists of a Bluetooth contact trace and
Smallblue consists of an instant messenger contact trace [3].
Both Brightkite and Gowalla consist of a SN and a check-in
trace of the SN users [26][27]. ColN: HepPh, AstroPh, and
CondMat are three collaboration networks from arXiv in the
areas of High Energy Physics-Phenomenology, Astro Physic-
s, and Condense Matter Physics, respectively [26]. DBLP is
a collaboration network of researchers mainly in Computer
Science [26]. Email and WikiTalk: Enron and EuAll are
two email communication networks [26]. WikiTalk is a net-
work containing the discussion relationships among a group
of users on Wikipedia [26]. AS: AS733, Oregon, Caida, and
Skitter are four AS graphs at diﬀerent locations [26]. P2P:
Gnutella3, Gnutella4, and Gnutella5 are three P2P network
graphs where nodes represent hosts in Gnutella and edges
are connections between hosts [26].

5.3 Evaluation on Perfect DA Quantiﬁcation
The conditions in Theorems 7 and 8 are quantiﬁed in sense
of n being a large number. Therefore, in the evaluation of
perfect/(1− ϵ)-perfect DA quantiﬁcation, we derive an extra
condition on the lower bound on n, denoted by Ω(n). Then,
based on Theorem 7, the conditions on (Ω(fD), Ω(n)) for
perfect DA under diﬀerent ℘ are shown in Tab. 2. From
Tab. 2, we have the following observations.

(i) When ℘ increases, Ω(fD) shows an increasing trend.
For instance, Ω(fD) is increased from 6.5E-8 when ℘ = .3
to 2.7E-6 when ℘ = .9, which implies the condition on fD
becomes stronger. This is consistent with our quantiﬁcation
since fD is an increasing function on ℘ given pD. On the
other hand, we ﬁnd that although Ω(fD) increases for large
℘, it still keeps relatively loose bounds, i.e., fD is easily
satisﬁed. For example, when ℘ = .9, the condition on Ω(fD)
is 2.7E-6 for Google+ (a large scale dataset) and 1.6E-5 for
Gowalla (a medium scale dataset).

(ii) When ℘ increases, Ω(n) decreases. For instance, Ω(n)
is decreased from 1.7E7 when ℘ = .3 to 3.2E5 when ℘ = .9
for Twitter. This is because a large ℘ implies that Ga is
topologically more similar to Gu. Thus, a weaker condition
on Ω(n) is suﬃcient to enable a perfect DA scheme a.a.s.
inducing the least DE.

(iii) For datasets with similar graph densities, e.g., Google+

(ρ = 8.24E-6) and Skitter (ρ = 7.73E-6), the conditions on
(Ω(fD), Ω(n)) are also similar for perfect DA, which is con-
sistent with our theoretical quantiﬁcation. This comes from
the similarity of their statistical pD. For perfect DA on
datasets with diﬀerent graph densities (with similar or d-
iﬀerent sizes), e.g., HepPh (n = 1.2E4, ρ = 1.87E-3) and

1045Dataset
Google+
Twitter

Facebook
YouTube

Orkut

Slashdot

Pokec

n

℘ = .3

℘ = .4

℘ = .5

℘ = .6

℘ = .7

℘ = .8

℘ = .9

Table 2: Evaluation of (Ω(fD), Ω(n)) in perfect DA.

Infocom
Smallblue
Brightkite

4.7E6 (6.5E-8, 3.0E8) (1.6E-7, 1.1E8) (3.4E-7, 5.2E7) (6.4E-7, 2.7E7) (1.1E-6, 1.5E7) (1.8E-6, 9.1E6) (2.7E-6, 5.7E6)
4.6E5 (9.5E-7, 1.7E7) (2.4E-6, 6.5E6) (5.0E-6, 3.0E6) (9.3E-6, 1.5E6) (1.6E-5, 8.6E5) (2.6E-5, 5.1E5) (4.0E-5, 3.2E5)
LiveJournal 4.8E6 (2.9E-8, 6.9E8) (7.4E-8, 2.6E8) (1.5E-7, 1.2E8) (2.9E-7, 6.3E7) (4.9E-7, 3.6E7) (7.9E-7, 2.1E7) (1.2E-6, 1.3E7)
4.0E3 (8.4E-5, 1.4E5) (2.1E-4, 5.1E4) (4.4E-4, 2.3E4) (8.2E-4, 1.1E4) (1.4E-3, 6.2E3) (2.3E-3, 3.6E3) (3.5E-3, 2.2E3)
1.1E6 (3.7E-8, 5.5E8) (9.3E-8, 2.1E8) (1.9E-7, 9.5E7) (3.6E-7, 5.0E7) (6.1E-7, 2.8E7) (9.9E-7, 1.7E7) (1.5E-6, 1.1E7)
3.1E6 (2.0E-7, 9.3E7) (5.0E-7, 3.5E7) (1.0E-6, 1.6E7) (1.9E-6, 8.3E6) (3.3E-6, 4.7E6) (5.3E-6, 2.8E6) (8.2E-6, 1.7E6)
8.2E4 (1.4E-6, 1.2E7) (3.5E-6, 4.4E6) (7.2E-6, 2.0E6) (1.3E-5, 1.0E6) (2.3E-5, 5.8E5) (3.7E-5, 3.5E5) (5.7E-5, 2.1E5)
1.6E6 (1.3E-7, 1.4E8) (3.3E-7, 5.3E7) (7.0E-7, 2.4E7) (1.3E-6, 1.3E7) (2.2E-6, 7.2E6) (3.6E-6, 4.3E6) (5.5E-6, 2.7E6)
7.3E1 (5.5E-4, 1.8E4) (1.4E-3, 6.4E3) (2.9E-3, 2.7E3) (5.4E-3, 1.4E3) (9.4E-3, 7.8E2) (1.5E-2, 3.9E2) (2.4E-2, 2.5E2)
1.2E2 (3.8E-4, 2.7E4) (9.6E-4, 9.7E3) (2.0E-3, 4.2E3) (3.7E-3, 2.1E3) (6.4E-3, 1.2E3) (1.0E-2, 6.8E2) (1.6E-2, 4.4E2)
5.7E4 (1.1E-6, 1.6E7) (2.6E-6, 5.9E6) (5.5E-6, 2.7E6) (1.0E-5, 1.4E6) (1.7E-5, 7.8E5) (2.8E-5, 4.6E5) (4.4E-5, 2.9E5)
2.0E5 (3.9E-7, 4.5E7) (9.8E-7, 1.7E7) (2.0E-6, 7.7E6) (3.8E-6, 4.0E6) (6.5E-6, 2.3E6) (1.0E-5, 1.3E6) (1.6E-5, 8.4E5)
1.2E4 (1.5E-5, 9.3E5) (3.7E-5, 3.4E5) (7.8E-5, 1.5E5) (1.4E-4, 7.8E4) (2.5E-4, 4.3E4) (4.0E-4, 2.6E4) (6.2E-4, 1.6E4)
1.8E4 (9.7E-6, 1.5E6) (2.5E-5, 5.4E5) (5.1E-5, 2.4E5) (9.5E-5, 1.2E5) (1.6E-4, 6.9E4) (2.6E-4, 4.1E4) (4.1E-4, 2.5E4)
2.1E4 (3.2E-6, 4.8E6) (8.0E-6, 1.8E6) (1.7E-5, 8.2E5) (3.1E-5, 4.2E5) (5.3E-5, 2.3E5) (8.5E-5, 1.4E5) (1.3E-4, 8.6E4)
3.2E5 (1.7E-7, 1.1E8) (4.2E-7, 4.2E7) (8.7E-7, 1.9E7) (1.6E-6, 1.0E7) (2.8E-6, 5.6E6) (4.5E-6, 3.4E6) (6.9E-6, 2.1E6)
3.4E4 (2.5E-6, 6.2E6) (6.4E-6, 2.3E6) (1.3E-5, 1.0E6) (2.5E-5, 5.4E5) (4.2E-5, 3.0E5) (6.8E-5, 1.8E5) (1.1E-4, 1.1E5)
2.2E5 (1.1E-7, 1.8E8) (2.7E-7, 6.7E7) (5.6E-7, 3.1E7) (1.0E-6, 1.6E7) (1.8E-6, 9.0E6) (2.9E-6, 5.4E6) (4.5E-6, 3.4E6)
2.4E6 (1.3E-8, 1.6E9) (3.3E-8, 6.2E8) (6.8E-8, 2.9E8) (1.3E-7, 1.5E8) (2.2E-7, 8.5E7) (3.5E-7, 5.1E7) (5.4E-7, 3.2E7)
6.5E3 (5.3E-6, 2.8E6) (1.3E-5, 1.0E6) (2.8E-5, 4.7E5) (5.1E-5, 2.4E5) (8.7E-5, 1.4E5) (1.4E-4, 8.0E4) (2.2E-4, 4.9E4)
1.1E4 (4.0E-6, 3.8E6) (1.0E-5, 1.4E6) (2.1E-5, 6.4E5) (3.8E-5, 3.3E5) (6.6E-5, 1.8E5) (1.1E-4, 1.1E5) (1.7E-4, 6.7E4)
2.6E4 (1.2E-6, 1.4E7) (3.0E-6, 5.1E6) (6.3E-6, 2.3E6) (1.2E-5, 1.2E6) (2.0E-5, 6.7E5) (3.2E-5, 4.0E5) (5.0E-5, 2.5E5)
1.7E6 (6.1E-8, 3.2E8) (1.5E-7, 1.2E8) (3.2E-7, 5.5E7) (6.0E-7, 2.9E7) (1.0E-6, 1.6E7) (1.6E-6, 9.8E6) (2.6E-6, 6.1E6)
2.6E4 (1.5E-6, 1.1E7) (3.7E-6, 4.1E6) (7.8E-6, 1.9E6) (1.4E-5, 9.6E5) (2.5E-5, 5.4E5) (4.0E-5, 3.2E5) (6.2E-5, 2.0E5)
3.7E4 (1.0E-6, 1.6E7) (2.6E-6, 5.9E6) (5.5E-6, 2.7E6) (1.0E-5, 1.4E6) (1.7E-5, 7.8E5) (2.8E-5, 4.7E5) (4.4E-5, 2.9E5)
6.3E4 (6.0E-7, 2.9E7) (1.5E-6, 1.1E7) (3.1E-6, 4.9E6) (5.8E-6, 2.5E6) (1.0E-5, 1.4E6) (1.6E-5, 8.5E5) (2.5E-5, 5.3E5)

DBLP
Enron
EuAll
Wiki
AS733
Oregon
Caida
Skitter

Gowalla
HepPh
AstroPh
CondMat

Gnutella3
Gnutella4
Gnutella5

Oregon (n = 1.15E4, ρ = 4.98E-4), Facebook (n = 4.0E3,
ρ = 1.08E-2) and Twitter (n = 4.6E5, ρ = 1.2E-4), dense
datasets require a stronger condition on fD while a weaker
condition on Ω(n) given ℘, which is also consistent with our
quantiﬁcation. A stronger condition requirement on fD is
because fD is an increasing function on pD ≃ ρ ∈ (0, 0.5]
given ℘ and all the considering datasets have ρ ≤ 0.5. A
looser bound on Ω(n) comes from the fact that more struc-
tural information can be projected to Ga and Gu in dense
datasets.

(iv) From Tab. 2, some datasets can be perfectly de-
anonymized under some conditions. For instance, Orkut and
Facebook are a.a.s. can be perfectly de-anonymized when
℘ ≥ Ω(.8). The perfect DA is due to their good structural
characteristics, e.g., high average degree, small percentage
of nodes with a low degree.

5.4 Evaluation on (1 − ϵ)-Perfect DA Quantiﬁ-

cation

Based on our quantiﬁcation, the percentage of successfully
de-anonymized users by any (1 − ϵ)-perfect DA scheme is
at least 1 − ϵ. Given ℘ varied from .3 to .95, we evaluate
the minimum number of users in the 26 datasets considered
that can be successfully de-anonymized with probability 1
in terms of our quantiﬁcation, i.e., the lower bound of 1 − ϵ,
(Ω(1 − ϵ)), and the results are shown in Tab. 3. From Tab.
3, we make some important observations and comments as
follows.

(i) When ℘ increases, more users can be de-anonymized
for every dataset as expected. For example, when ℘ = .5,
it is a.a.s.
at least 29.7% of the users in Google+ can
be successfully de-aonymized; when ℘ is increased to .8,
at least 72.5% of the users in Google+ can be successful-
ly de-anonymized; when ℘ = .95 all the users in Google+
can a.a.s. be successfully de-anonymized. From Tab. 3,
similar DA phenomena applied to all the datasets, which is

consistent with our quantiﬁcation. The reason is straight-
forward. When ℘ increases, more edges appear in both Ga
and Gu. Thus, the structural similarity between Ga and
Gu is increased followed by more users can statistically be
successfully de-anonymized.

(ii) Most of the existing structural datasets are a.a.s. de-
anonymizable completely or at least partially just based on
the topological information. For instance, Facebook and
Orkut datasets can be completely de-anonymized when ℘ =
.8. Even when a dataset cannot be completely de-anonymized,
it may be de-anonymizable partially in a large-scale. For ex-
ample, when ℘ = .9, at least 60.9%, 48.9%, and 85.7% of
the users in LiveJournal, Gowalla, and AstroPh can be suc-
cessfully de-anonymized, respectively. This fact is consistent
with our quantiﬁcation as well as the intuition that structure
itself can be used to de-anonymize data.

(iii) An interesting observation is that the DA results on
two datasets with similar graph densities may be very d-
iﬀerent in practice. From Tab. 2, for two datasets with
similar graph densities, e.g., Google+ and Skitter, the the-
oretical bounds on (Ω(fD), Ω(n)) for perfect DA are also
similar. However, from Tab. 3, the DA results of Google+
and Skitter are very diﬀerent: when ℘ = .6, the number of
de-anonymizable users in Google+ (41.8%) is about twice
that of in Skitter (23.1%); while when ℘ = .95, all the
users in Google+ are a.a.s. de-anonymizable while the de-
anonymizable users in Skitter is only bounded by Ω(59.1%).
To study the reason for this fact, we need to consider the de-
gree distribution of Google+ and Skitter besides the graph
density (as well as Ω(fD) and Ω(n)). From Tab. 1, the per-
centage of low degree users in Skitter is much higher than
that in Google+. On the other hand, intuitively, low de-
gree users, especially users with degree of 1, do not have too
much distinguishable structural information (this intuition
is conﬁrmed by our theoretical quantiﬁcation on diﬀeren-
t DEs caused by mismatching high degree users and low
degree users), which implies that they are diﬃcult to be de-
anonymized based on structural information. Consequently,

1046Table 3: Evaluation of Ω(1 − ϵ) in (1 − ϵ)-perfect DA.

Dataset
Google+
Twitter

LiveJournal

Facebook
YouTube

Orkut

Slashdot

Pokec

Infocom
Smallblue
Brightkite

Gowalla
HepPh
AstroPh
CondMat

DBLP
Enron
EuAll
Wiki
AS733
Oregon
Caida
Skitter

Gnutella3
Gnutella4
Gnutella5

6.8%

5.3%

℘ = .3 ℘ = .35 ℘ = .4 ℘ = .45 ℘ = .5 ℘ = .55 ℘ = .6 ℘ = .65 ℘ = .7 ℘ = .75 ℘ = .8 ℘ = .85 ℘ = .9 ℘ = .95
11.7% 15.5% 19.7% 24.5% 29.7% 35.5% 41.8% 48.7% 56.1% 64.0% 72.5% 81.6% 91.2% 100.0%
15.1% 20.2% 26.0% 32.4% 39.4% 47.1% 55.4% 64.3% 73.8% 84.0% 94.7% 100.0% 100.0% 100.0%
6.6%
9.1% 11.9% 15.2% 18.8% 22.7% 27.1% 31.8% 36.8% 42.3% 48.1% 54.3% 60.9% 68.1%
3.7% 12.1% 22.4% 31.0% 39.9% 49.5% 59.6% 70.3% 81.5% 93.2% 100.0% 100.0% 100.0% 100.0%
4.0%
8.4% 10.3% 12.3% 14.5% 16.9% 19.5% 22.4% 25.5% 28.9% 32.5% 36.4%
14.2% 19.6% 26.0% 33.3% 41.4% 50.3% 60.0% 70.4% 81.3% 92.7% 100.0% 100.0% 100.0% 100.0%
7.2%
9.8% 12.7% 15.9% 19.5% 23.4% 27.6% 32.2% 37.2% 42.7% 48.6% 54.9% 61.8% 69.3%
7.3% 10.4% 14.1% 18.4% 23.2% 28.5% 34.4% 40.7% 47.5% 54.7% 62.4% 70.5% 79.0% 88.1%
10.4% 11.5% 12.5% 13.0% 13.9% 14.3% 15.1% 15.5% 15.8% 16.6% 16.9% 17.2% 49.9% 62.2%
9.6% 10.3% 10.9% 11.3% 11.8% 12.1% 12.6% 12.9% 13.3% 33.0% 44.6% 54.6% 64.7%
8.9%
8.6% 10.9% 13.5% 16.4% 19.6% 23.1% 26.8% 30.9% 35.3% 40.0% 45.1% 50.6%
6.5%
4.7%
5.3%
7.2%
9.4% 11.9% 14.7% 17.8% 21.2% 25.0% 29.0% 33.4% 38.2% 43.3% 48.9% 54.8%
9.0% 13.2% 17.6% 22.4% 27.6% 33.2% 39.2% 45.7% 52.7% 60.1% 68.1% 76.7% 85.9% 95.7%
7.4% 11.0% 15.3% 20.1% 25.4% 31.2% 37.6% 44.4% 51.7% 59.4% 67.6% 76.4% 85.7% 95.6%
9.6% 12.3% 15.3% 18.7% 22.6% 26.8% 31.4% 36.5% 42.1% 48.2% 54.8%
3.5%
5.2%
4.3%
3.0%
7.6%
9.6% 11.8% 14.3% 17.1% 20.2% 23.6% 27.4% 31.5% 36.0% 40.9%
9.0% 11.7% 14.6% 17.9% 21.4% 25.3% 29.5% 34.1% 39.1% 44.5% 50.3% 56.6% 63.4%
6.6%
3.5%
8.3%
4.5%
9.8% 11.4% 13.3% 15.2% 17.4% 19.6% 22.1% 24.7% 27.6%
6.9%
5.6%
3.7%
4.8%
8.9% 10.5% 12.3% 14.2% 16.3% 18.6% 21.1% 23.8% 26.7% 29.8%
7.4%
6.0%
6.5%
4.8%
1.3%
8.3% 10.3% 12.5% 14.9% 17.6% 20.5% 23.8% 27.4% 31.2% 35.5% 40.0%
8.6% 10.8% 13.1% 15.7% 18.5% 21.6% 24.9% 28.6% 32.5% 36.7% 41.3% 46.3%
6.5%
4.6%
5.1%
3.8%
6.5%
9.9% 11.8% 14.0% 16.3% 18.8% 21.6% 24.6% 27.8% 31.4% 35.3%
8.3% 10.6% 13.3% 16.2% 19.5% 23.1% 27.1% 31.4% 36.1% 41.2% 46.7% 52.6% 59.1%
6.2%
9.5% 12.1% 15.2% 18.8% 23.0% 27.3% 31.5% 36.0% 40.6%
2.6%
1.7%
1.8%
2.8%
9.4% 12.0% 15.0% 18.4% 22.5% 26.7% 30.8% 35.1% 39.6%
1.8%
9.1% 11.5% 14.4% 17.7% 21.6% 25.7% 29.7% 33.8% 38.1%
2.7%

7.2%
7.3%
7.0%

5.4%
5.5%
5.3%

3.8%
4.0%
3.9%

7.2%
5.8%

8.1%

the existence of a large amount of low degree users in Skit-
ter makes it less de-anonymizable than Google+, which is
consistent with our quantiﬁcation. In summary, from Tabs.
1 and 3, if a dataset has a high average degree and a small
percentage of low degree users, it is easier to de-anonymize
and a large amount of its users are a.a.s. de-anonymizable;
otherwise, for datasets with a low average degree and a
large percentage of low degree users, they are diﬃcult to
de-anonymize based solely on structural information.

(iv) Following the above observation, we ﬁnd that there
exists some diﬀerences between theory and practice on the
dominating factor of DA. Theoretically, the graph densi-
ty plays a dominating factor on determining the bound of
(Ω(fD), Ω(n)). In practice, the degree distribution and the
average degree have more impact on the DA results. This is
mainly because we study the quantiﬁcation from an asymp-
totical sense in the theoretical scenario (i.e., n → ∞) and
the key parameter pi,j asymptotically converges to graph
density ρ, i.e., E(pi,j) ≃
n → ∞ ρ. On the other hand, when
quantifying the percentage of de-anonymizable users for each
dataset, the actual degree sequence/distribution D is used
to examine when the DA conditions are satisﬁed.
We also evaluate the impact of ℘ and ϵ on the bound of
Ω(n) in (1 − ϵ)-perfect DA (we do not show Ω(fD) since
it depends on ℘ and exhibits the same behavior as in the
perfect DA) as shown in Tab. 4. From Tab. 4, we have the
following observations.
(i) When ϵ is ﬁxed, the impact of ℘ on Ω(n) in (1 − ϵ)-
perfect DA is similar to that in perfect DA, i.e., when ℘
increases, Ω(n) decreases. The reason is also the same as
before since a large ℘ implies more similarity between Ga
and Gu and thus a loose condition on Ω(n) is suﬃcient to
enable σk (k ≤ ϵn) to induce less DE than σk′ (k

> ϵn).

′

(ii) When ℘ is ﬁxed, Ω(n) is also decreasing with the in-
crease of ϵ. For instance, when ℘ = 0.6, Ω(n) decreases
from 2.2E7 to 9.5E6 for Google+ when ϵ increases from .1
to .4. This is because of that when ϵ increases, more DE is
tolerated, and thus loose condition is required for Ω(n) to

Table 4: Evaluation of Ω(n) in (1 − ϵ)-perfect DA.

Dataset

℘ = .6

℘ = .9

Pokec

Orkut

Slashdot

Facebook
YouTube

ϵ = .1 ϵ = .2 ϵ = .3 ϵ = .4 ϵ = .1 ϵ = .2 ϵ = .3 ϵ = .4
Google+ 2.2E7 1.7E7 1.3E7 9.5E6 4.6E6 3.6E6 2.7E6 2.3E6
1.2E6 9.6E5 7.3E5 5.4E5 2.5E5 2.3E5 2.3E5 2.3E5
Twitter
LiveJournal 5.1E7 4.0E7 3.0E7 2.2E7 1.1E7 8.4E6 6.4E6 4.7E6
9.2E3 7.2E3 5.5E3 4.1E3 2.0E3 2.0E3 2.0E3 2.0E3
4.0E7 3.2E7 2.5E7 1.8E7 8.6E6 6.8E6 5.2E6 3.8E6
6.7E6 5.3E6 4.1E6 3.1E6 1.5E6 1.5E6 1.5E6 1.5E6
8.5E5 6.7E5 5.2E5 3.8E5 1.7E5 1.4E5 1.1E5 7.7E4
1.0E7 8.0E6 6.1E6 4.5E6 2.1E6 1.7E6 1.3E6 9.4E5
Infocom 1.2E3 9.8E2 7.8E2 6.8E2 2.5E2 2.5E2 1.7E2 1.7E2
1.8E3 1.4E3 1.2E3 8.8E2 3.4E2 3.2E2 2.2E2 2.2E2
Smallblue
Brightkite
1.1E6 8.8E5 6.7E5 4.9E5 2.3E5 1.8E5 1.4E5 1.0E5
3.2E6 2.5E6 1.9E6 1.4E6 6.7E5 5.3E5 4.0E5 3.0E5
6.2E4 4.9E4 3.7E4 2.7E4 1.2E4 9.7E3 7.3E3 5.6E3
9.9E4 7.8E4 5.9E4 4.4E4 2.0E4 1.6E4 1.2E4 9.0E3
3.4E5 2.7E5 2.1E5 1.6E5 6.9E4 5.5E4 4.2E4 3.2E4
8.1E6 6.5E6 5.0E6 3.8E6 1.7E6 1.4E6 1.1E6 8.0E5
4.3E5 3.4E5 2.6E5 1.9E5 8.8E4 6.9E4 5.2E4 3.8E4
1.3E7 1.1E7 8.3E6 6.2E6 2.8E6 2.2E6 1.7E6 1.3E6
1.2E8 9.9E7 7.7E7 5.7E7 2.6E7 2.1E7 1.6E7 1.2E7
2.0E5 1.6E5 1.2E5 9.0E4 4.0E4 3.2E4 2.4E4 1.8E4
2.7E5 2.1E5 1.6E5 1.2E5 5.5E4 4.3E4 3.3E4 2.4E4
9.8E5 7.8E5 6.0E5 4.5E5 2.0E5 1.6E5 1.2E5 9.1E4
2.3E7 1.8E7 1.4E7 1.0E7 4.9E6 3.9E6 3.0E6 2.2E6
7.8E5 6.2E5 4.8E5 3.5E5 1.6E5 1.3E5 9.7E4 7.1E4
1.1E6 9.0E5 6.9E5 5.1E5 2.3E5 1.9E5 1.4E5 1.0E5
2.1E6 1.6E6 1.3E6 9.3E5 4.3E5 3.4E5 2.6E5 1.9E5

DBLP
Enron
EuAll
Wiki
AS733
Oregon
Caida
Skitter

Gowalla
HepPh
AstroPh
CondMat

Gnutella3
Gnutella4
Gnutella5

distinguish σk (k ≤ ϵn) and σk′ (k

′

tent with our quantiﬁcation.

> ϵn), which is consis-

(iii) As in the perfect DA scenario, graph density is an
important factor to impact Ω(n). Datasets with similar
graph density, e.g., Google+ and Skitter, exhibits similar
requirement on Ω(n). A dataset with high graph density,
e.g., Facebook and HepPh, corresponds to a loose bound on
Ω(n). The reason is also the same as before.

1047Table 5: Evaluation of (Ω(℘), Ω(fD), Ω(n)) in (1 − ϵ)-perfect DA.

Pokec

ϵ = .1

ϵ = .4

ϵ = .3

ϵ = .5

ϵ = .2

Orkut

Slashdot

Facebook
YouTube

Dataset
Google+ (1.1E-7, 3.5E-6, 2.2E28) (1.2E-7, 3.7E-6, 1.8E28) (1.3E-7, 3.9E-6, 1.6E28) (1.4E-7, 4.2E-6, 1.3E28) (1.4E-7, 4.4E-6, 1.1E28)
(1.2E-6, 3.1E-5, 1.2E24) (1.2E-6, 3.2E-5, 9.7E23) (1.3E-6, 3.4E-5, 8.3E23) (1.4E-6, 3.6E-5, 6.9E23) (1.5E-6, 3.8E-5, 5.8E23)
Twitter
LiveJournal (1.2E-7, 3.6E-6, 4.7E28) (1.2E-7, 3.6E-6, 4.7E28) (1.2E-7, 3.8E-6, 3.8E28) (1.3E-7, 4.1E-6, 3.0E28) (1.4E-7, 4.3E-6, 2.7E28)
(1.3E-4, 2.2E-3, 5.9E15) (1.4E-4, 2.3E-3, 5.0E15) (1.5E-4, 2.5E-3, 4.1E15) (1.6E-4, 2.6E-3, 3.5E15) (1.7E-4, 2.8E-3, 2.9E15)
(6.0E-7, 1.7E-5, 2.4E26) (6.0E-7, 1.7E-5, 2.4E26) (6.0E-7, 1.7E-5, 2.4E26) (6.0E-7, 1.7E-5, 2.4E26) (6.0E-7, 1.7E-5, 2.4E26)
(1.7E-7, 5.1E-6, 2.0E27) (1.8E-7, 5.4E-6, 1.7E27) (1.9E-7, 5.7E-6, 1.4E27) (2.0E-7, 6.1E-6, 1.2E27) (2.2E-7, 6.5E-6, 9.8E26)
(7.4E-6, 1.7E-4, 2.8E21) (7.4E-6, 1.7E-4, 2.8E21) (7.4E-6, 1.7E-4, 2.8E21) (8.2E-6, 1.9E-4, 2.1E21) (8.2E-6, 1.9E-4, 2.1E21)
(3.2E-7, 9.2E-6, 4.4E26) (3.5E-7, 9.9E-6, 3.6E26) (3.6E-7, 1.0E-5, 3.1E26) (3.9E-7, 1.1E-5, 2.5E26) (4.1E-7, 1.2E-5, 2.1E26)
Infocom (8.6E-3, 8.0E-2, 2.0E09) (8.6E-3, 8.0E-2, 2.0E09) (9.1E-3, 8.1E-2, 1.7E09) (1.0E-2, 8.6E-2, 1.2E09) (1.1E-2, 8.9E-2, 1.1E09)
(4.7E-3, 5.2E-2, 1.9E10) (5.1E-3, 5.1E-2, 1.5E10) (5.3E-3, 5.2E-2, 1.3E10) (5.8E-3, 5.6E-2, 1.0E10) (6.4E-3, 6.1E-2, 7.2E09)
Smallblue
(1.1E-5, 2.3E-4, 1.2E21) (1.1E-5, 2.3E-4, 1.2E21) (1.1E-5, 2.3E-4, 1.2E21) (1.2E-5, 2.6E-4, 8.7E20) (1.2E-5, 2.6E-4, 8.7E20)
Brightkite
(2.9E-6, 7.1E-5, 1.8E23) (2.9E-6, 7.1E-5, 1.8E23) (3.2E-6, 7.8E-5, 1.3E23) (3.2E-6, 7.8E-5, 1.3E23) (3.4E-6, 8.3E-5, 1.1E23)
(4.7E-5, 8.8E-4, 8.5E17) (5.1E-5, 9.5E-4, 6.7E17) (5.5E-5, 1.0E-3, 5.3E17) (5.7E-5, 1.1E-3, 4.6E17) (6.2E-5, 1.2E-3, 3.7E17)
(3.0E-5, 5.9E-4, 5.2E18) (3.1E-5, 6.1E-4, 4.6E18) (3.4E-5, 6.6E-4, 3.7E18) (3.5E-5, 6.9E-4, 3.1E18) (3.7E-5, 7.3E-4, 2.6E18)
(2.6E-5, 5.2E-4, 2.5E19) (2.6E-5, 5.2E-4, 2.5E19) (2.8E-5, 5.6E-4, 2.0E19) (3.0E-5, 6.0E-4, 1.7E19) (3.2E-5, 6.3E-4, 1.4E19)
(1.7E-6, 4.3E-5, 2.2E24) (1.9E-6, 4.8E-5, 1.6E24) (1.9E-6, 4.8E-5, 1.6E24) (2.1E-6, 5.3E-5, 1.2E24) (2.2E-6, 5.7E-5, 9.5E23)
(1.7E-5, 3.6E-4, 1.1E20) (1.7E-5, 3.6E-4, 1.1E20) (1.8E-5, 3.8E-4, 9.3E19) (2.0E-5, 4.2E-4, 7.1E19) (2.0E-5, 4.2E-4, 7.1E19)
(3.8E-6, 9.4E-5, 2.9E23) (3.8E-6, 9.4E-5, 2.9E23) (3.8E-6, 9.4E-5, 2.9E23) (3.8E-6, 9.4E-5, 2.9E23) (3.8E-6, 9.4E-5, 2.9E23)
(3.3E-7, 9.7E-6, 4.3E27) (3.3E-7, 9.7E-6, 4.3E27) (3.3E-7, 9.7E-6, 4.3E27) (3.3E-7, 9.7E-6, 4.3E27) (3.3E-7, 9.7E-6, 4.3E27)
(9.4E-5, 1.7E-3, 2.9E17) (9.4E-5, 1.7E-3, 2.9E17) (9.4E-5, 1.7E-3, 2.9E17) (1.1E-4, 2.0E-3, 1.6E17) (1.1E-4, 2.0E-3, 1.6E17)
(5.1E-5, 9.5E-4, 2.6E18) (5.1E-5, 9.5E-4, 2.6E18) (6.7E-5, 1.3E-3, 1.1E18) (6.7E-5, 1.3E-3, 1.1E18) (6.7E-5, 1.3E-3, 1.1E18)
(2.3E-5, 4.7E-4, 9.6E19) (2.3E-5, 4.7E-4, 9.6E19) (2.3E-5, 4.7E-4, 9.6E19) (3.1E-5, 6.3E-4, 4.1E19) (3.1E-5, 6.3E-4, 4.1E19)
(3.2E-7, 9.0E-6, 1.0E27) (3.4E-7, 9.8E-6, 8.0E26) (3.7E-7, 1.1E-5, 6.5E26) (3.9E-7, 1.1E-5, 5.4E26) (4.1E-7, 1.2E-5, 4.7E26)
(2.4E-5, 4.8E-4, 7.3E19) (2.4E-5, 4.8E-4, 7.3E19) (2.4E-5, 4.8E-4, 7.3E19) (2.4E-5, 4.8E-4, 7.3E19) (2.6E-5, 5.3E-4, 5.4E19)
(1.8E-5, 3.7E-4, 2.6E20) (1.8E-5, 3.7E-4, 2.6E20) (1.8E-5, 3.7E-4, 2.6E20) (1.8E-5, 3.7E-4, 2.6E20) (1.9E-5, 4.1E-4, 2.0E20)
(1.0E-5, 2.3E-4, 2.3E21) (1.0E-5, 2.3E-4, 2.3E21) (1.0E-5, 2.3E-4, 2.3E21) (1.0E-5, 2.3E-4, 2.3E21) (1.2E-5, 2.5E-4, 1.7E21)

DBLP
Enron
EuAll
Wiki
AS733
Oregon
Caida
Skitter

Gowalla
HepPh
AstroPh
CondMat

Gnutella3
Gnutella4
Gnutella5

Finally, we also want to evaluate the required bounds on
(Ω(℘), Ω(fD), Ω(n)) in (1 − ϵ)-perfect DA. We demonstrate
the results in Tab. 5 and make the following observations.

(i) Theoretically, the condition on the lower bound of ℘ is
very loose, e.g., when ϵ = .1, Ω(℘) = 1.1E-7 for Google+ and
Ω(℘) = 1.7E-7 for Orkut, which suggests that (1− ϵ)-perfect
DA is implementable in practice. On the other hand, we can
also see that the theoretical loose requirement on Ω(℘) is at
the expense of a strong condition on Ω(n), e.g., when ϵ = .1,
Ω(n) = 2.2E28 for Google+ and Ω(n) = 2.0E27 for Orkut.
Consequently, to de-anonymize most of existing structural
datasets which have sizes of million-level or less, a higher ℘
is desired (as we show in Tab. 2, 3, and 4).

(ii) From Tab. 5, we can see that the conditions on Ω(fD)
and Ω(n) exhibit the same behavior as in perfect DA, i.e.,
Ω(fD) increases and Ω(n) decreases as Ω(℘) increases, which
is consistent with our quantiﬁcation. Again, this is because
fD is an increasing function of ℘ given pD and Ω(n) decreas-
es when more similarity appears between Ga and Gu.

(iii) From Tab. 5, we can also see that the impact of
graph density on Ω(fD) and Ω(n) is also similar to that in
the perfect DA scenario.

6. OPTIMIZATION BASED DA PRACTICE
In Section 4, we comprehensively quantiﬁed conditions for
perfect DA and (1− ϵ)-perfect DA. Based on our large-scale
study on 26 real world datasets in Section 5, we ﬁnd most, if
not all, existing structural datasets are de-anonymizable par-
tially or completely (Tab. 3). Interestingly, our DA quan-
tiﬁcation leads to a DA scheme, denoted by A
, straight-
∗
forwardly. Basically, A
can be implemented as follows: we
can calculate the DE caused by each σk (1 ≤ k ≤ n!) and
let σ0 be the σk that induces the least DE. According to the
∗
quantiﬁcation, the σ0 produced by A
should be the opti-
∗
is computationally infeasible
mum DA scheme. However, A
in practice due to its high computational complexity O(n!).
In this section, we present a novel relaxed and operational

∗

followed by analyzing its performance theoret-

∗
version of A
ically and experimentally on large scale real datasets.
6.1 Optimization based DA

∗

i

i

i

i

1, di

|+1 = di|N u

j ||j ∈ N u

i }), i.e., di

|+1 = di|N a
|+2 = ··· = di

i | < β), we set di|N a
i ||i ∈ V a} (resp., ∆u = max{|N u

version of A
i ∈ V a or V u as follows.
its degree in Ga (resp., Gu), i.e., fd(i) = |N a

Before proposing our relaxed and computationally feasible
, we deﬁne some useful structural features for
Degree: For i ∈ V a (resp., V u), its degree feature fd(i) is
i | (resp., |N u
i |).
Neighborhood: For i ∈ V a (resp., V u), its neighborhood
2,··· , di
feature fn(i) is a β-dimensional vector (di
β), where
j ||j ∈ N a
k (1 ≤ k ≤ β) is the k-th largest degree in {|N a
i }
di
(resp., {|N u
k is the k-th largest degree of
the neighboring users of i. In the case that |N a
i | < β (re-
sp., |N u
|+2 = ··· = di
β = ∆a
β = ∆u), where ∆a =
(resp., di|N u
i ||i ∈ V u}) is the
max{|N a
maximum degree of Ga (resp., Gu).
Top-K reference distance: For i ∈ V a (resp., V u), its Top-
K reference distance feature fK (i) is a K-dimensional vector
k (1 ≤ k ≤ K) is the distance (the
(hi
length of a shortest path) from i to the user with the k-th
largest degree in Ga (resp., Gu). Note that it is possible
k = ∞ if the graph is not connected.
hi
L = {v1, v2,··· , vL
Landmark reference distance: Suppose V a
|vk ∈ V a} is a set of users that has been de-anonymized (ev-
L = {u1, u2,··· , uL|uk ∈ V u}
idently, V a
under some DA scheme σ with σ(vk) = uk (1 ≤ k ≤ L).
Intuitively, V a
L can be used as auxiliary information
for future DA. Therefore, for i ∈ V a \ V a
L (resp., V u \ U u
L),
we deﬁne its landmark reference distance feature fl(i) =
k (1 ≤ k ≤ L) is the distance from
2,··· , hi
1, hi
(hi
L (resp., uk ∈ U u
i to vk ∈ V a
L).
Sampling closeness centrality: For i ∈ V a (resp., V u), we
deﬁne the sampling closeness centrality feature fc(i) to char-
acterize its global topological property without inducing too
much computational overhead. Formally, we ﬁrst randomly

L = ∅ initially) to U u

2,··· , hi

K ), where hi

L), where hi

L and U u

1, hi

1048∑

∑

sample a subset Sa of V a (resp., Su of V u). Then, we deﬁne
fc(i) =
h(i,j) ), where

h(i,j) (resp., fc(i) =

1

1

j∈Su\{i}

j∈Sa\{i}

h(i, j) is the distance from i to j.

According to the aforementioned deﬁnitions, (i) we con-
sider both local and global structural features of a user, e.g.,
the degree and neighborhood features characterize the local
topological properties of a user while the Top-K reference
distance and sampling closeness centrality features demon-
strate the global topological characteristics of a user; (ii) we
also consider the computational eﬃciency of obtaining these
features for a user. For instance, instead of using the accu-
rate closeness centrality, we introduce a sampling closeness
centrality feature, which can characterize the global feature
of a user without causing too much computation overhead.
Now, based on the features deﬁned for each user, we can
quantitatively measure the similarity between an anonymized
user i ∈ V a and a known user j ∈ V u. Let fd,c(i) =
(fd(i), fc(i)). Then, we deﬁne the structural similarity be-
tween i ∈ V a and j ∈ V u as ϕ(i, j) = c1 · s(fd,c(i), fd,c(j)) +
c2 · s(fn(i), fn(j)) + c3 · s(fK (i), fK (j)) + c4 · s(fl(i), fl(j)),
where c1,2,3,4 ∈ [0, 1] are constant values representing the
weights and c1 + c2 + c3 + c4 = 1, and s(·,·) is the Cosine
similarity between two vectors.

According to our theoretical quantiﬁcation in Section 4,
∗
is inherently an optimization based algorithm with the
A
objective of minimizing the DE Ψσk , which is diﬀerent from
most of existing DA algorithms (heuristics based) [1][2][3].
Inspired by our quantiﬁcation, we design a novel and opera-
tional Optimization based De-Anonymization (ODA) scheme,
∗
which is a relaxed version of A
. In ODA, rather than us-
ing the DE function as in the quantiﬁcation, we re-deﬁne
ψi,j and Ψσ as follows. Given a DA scheme σ = {(i, j)|i ∈
V a, j ∈ V u}, we deﬁne the DE on a user mapping (i, j) ∈ σ
as ψi,j = |fd(i) − fd(j)| + (1 − ϕ(i, j)) · |fd(i) − fd(j)|, and
ψi,j. Based on Ψσ, we give the
the DE on σ as Ψσ =

∑

(i,j)∈σ

framework of ODA as shown in Algorithm 1. In Algorithm
1, Λa ⊆ V a is the target DA set and Λu ⊆ V u is the possible
mapping set of Λa. GetTopDegree(X, y) is a function to re-
turn y users with the largest degree values in X, i.e., return
{i|i has the Top-y degree in X}. C(i) ⊆ Λu is the candidate
mapping set for i, which consists of the γ most possible map-
pings of i in Λu. GetTopSimilarity(i, Λu, γ) is a function to
return γ users having the highest similarity scores with i in
Λu, i.e., return {j|j ∈ Λu, and j has the Top-γ ϕ(i, j) in
Λu}.

∗

From Algorithm 1, ODA de-anonymizes Ga iteratively.
During each iteration, ODA is trying to de-anonymize a
subset of V a and seeking the sub-DA scheme σ
(Λa) which
induces the least DE. We explain the idea of ODA in de-
tails as follows. In Line 3, we initialize the target DA set
Λa and the candidate mapping set Λu. From the initial-
ization, α is an important parameter to control how many
anonymized users will be processed in each iteration.
In
Line 4, we compute a candidate mapping set C(i) for each
i ∈ Λa. C(i) consists γ most similar users of i in Λu. Here,
we deﬁne C(·) mainly for reducing the computational com-
plexity. Instead of trying every mapping from i to Λu, we
only consider to map i to some user in C(i). Hence, γ is
another important parameter to control the computation-
al complexity of ODA. We will demonstrate how to set α
and γ to make ODA computationally feasible in Theorem

Algorithm 1: ODA
1 Deﬁne Λa = Λu = ∅;
2 while true do
3

4

5

6
7
8

9

Λa = GetTopDegree(V a, α), Λu =
GetTopDegree(V u, α);
for every i ∈ Λa, compute a candidate mapping set
C(i) = GetTopSimilarity(i, Λu, γ);
apply the consistent rule and pruning rule to ﬁnd

the DA scheme σ(Λa) ∈ ∏

(i × C(i)) which

i∈Λa

induces the least DE Ψσ(Λa), denoted by
(Λa) = {(i1, j1), (i2, j2),··· , (iα, jα)};
∗
σ
for each (i, j) ∈ σ

(Λa), if ϕ(i, j) ≥ θ then

∗

accept the mapping (i, j);
V a = V a \ {i}, V u = V u \ {j};

if no mapping in σ

(Λa) is accepted, break ;

∗

Ψσ∗(Λa) = min{Ψσ(Λa)|σ(Λa) ∈ ∏

9. In Line 5, we ﬁnd a DA scheme σ

i∈Λa

∗
(Λa) on Λa such that
(i × C(i))}, i.e., σ
(Λa)

∗

∗

causes the least DE. Furthermore, the consistent rule and
the pruning rule are applied in this step. The consistent
rule makes any possible DA scheme σ(Λa) consistent, i.e., no
mapping conﬂiction which is deﬁned as the situation that t-
wo or more anonymized users are mapped to the same known
user. This is because it is possible that C(i1) ∩ C(i2) ̸= ∅ for
i1 ̸= i2 ∈ Λa, and the situation σ(i1) = σ(i2) in a DA
scheme should be avoided. Note that, it is possible that
no σ(Λa) is consistent. In this case, we should increase γ
to guarantee at least one σ(Λa) is consistent. The prun-
ing rule is used to remove some DA schemes whose DE is
larger than the current known least DE. For instance, let
(Λa) be the DA scheme having the least DE after testing
σ
k possible DA schemes. Then, when testing the (k + 1)-
th possible DA scheme σk+1(Λa), if partial of mappings in
σk+1(Λa) has already induced a larger DE than σ
(Λa), we
stop test σk+1(Λa) and continue the next one. On the other
hand, if σk+1(Λa) induces a smaller DE than σ
(Λa), we up-
(Λa) to σk+1(Λa). Both the consistent rule and the
date σ
pruning rule can remove some unqualiﬁed DA schemes in ad-
(Λa)
vance, which can speed up ODA. Actually, although σ
(Λa) is a local optimization solution
causes the least DE, σ
∗
(according to our quantiﬁcation, A
induces the optimum
solution). This is because we try to seek a tradeoﬀ between
computational feasibility and DA accuracy. After obtaining
(Λa) with similarity
σ
scores no less than a threshold value θ (Lines 6-8). For the
mappings been rejected, they will be re-considered in the
following iterations for possible better DAs. If no mapping
can be accepted, we stop ODA. Finally, we analyze the time
and space complexities of ODA in Theorem 9. We defer the
proof to Appendix C for readability.

(Λa), we accept the mappings in σ

∗

∗

∗

∗

∗

∗

∗

Theorem 9. (i) The space complexity of ODA is O(min{n2,

m + n}). (ii) Let γ be some constant value, α = Θ(log n),
and Γ be the average number of accepted mappings in each
iteration of ODA. Then, the time complexity of ODA is
O(m + n log n + nΘ(1) log γ+1/Γ) in the worst case.

Finally, we make some remarks on ODA as follows.
(i) ODA is a cold start algorithm, i.e., we do not need
any priori knowledge, e.g., the seed mapping information

1049[1][2][3], to bootstrap the DA process. Furthermore, unlike
existing DA algorithms [1][2][3] which consist of two phases
(landmark identiﬁcation phase and DA propagation phase),
ODA is a single-phase algorithm. Interestingly, ODA itself
can act as a landmark identiﬁcation algorithm. From our
experiment (Section 6.2), ODA can de-anonymize the 60-
180 Top-degree users in Gowalla and Google+ (see Tab. 1)
perfectly, which can serve as landmarks (V a
L) for
future DAs. In addition, ODA as a landmark identiﬁcation
algorithm is much faster than that in [2] (with complexity of
O(ndk−1) = O(nk), where d is maximum degree of Ga/Gu
and k is the number of landmarks) and [3] (with complexity
of k!, could be computationally infeasible for a PC when
k ≥ 20).

L and U u

(ii) Similar to A

, ODA is an optimization based DA
scheme, which is diﬀerent from most of existing heuristics
based solutions [1][2][3]. In ODA, the objective is to mini-
mize a DE function. The reasonableness and soundness of
ODA lie on one direct conclusion of our theoretical quan-
tiﬁcation: minimizing the DE leads to the best possible DA
scheme.

∗

∪

(i × C(i))),

∗

(iii) In ODA, we seek an adjustable tradeoﬀ between DA
accuracy and computational feasibility. Although A
obtain-
s the optimum solution a.a.s. in terms of our quantiﬁcation,
it is computationally infeasible (O(n!)). ODA has a polyno-
mial time complexity of O(m + n log n + nΘ(1) log γ+1/Γ) in
the worst case, which is computationally feasible at the cost
of sacriﬁcing some accuracy. Based on our experiments on
large scale real datasets in the following subsection, ODA is
operable while preserves satisﬁable DA performance.

i∈Λa

(iv) ODA is a general framework. Line 5 can also be im-
plemented by seeking a maximum weighted bipartite graph
matching on a weighted bipartite graph G(Λa∪Λu,
where the weight on each edge is ϕ(i, j) (i ∈ Λa, j ∈ C(i)).
(v) In ODA, one implicit assumption is V a = V u, i.e.,
the Ga and Gu are deﬁned on the same group of users. In
practice, it is possible that V a and V u are not exactly the
same. In this case, if V a and V u are not signiﬁcantly diﬀer-
ent, ODA is also workable at the cost of some performance
degradation ((1 − ϵ)-perfect DA). One better solution could
be estimating the overlap between Ga and Gu ﬁrst, and then
apply ODA to the overlap. We will take the estimation of
the overlap between Ga and Gu as one of the future works.
6.2 Experimental Evaluation and Analysis
6.2.1 Datasets and Setup
We evaluate the performance of ODA on two real world
datasets: Gowalla and Google+ (see the basic information
in Section 5). Gowalla is a location based SN and con-
sists of two diﬀerent datasets [26][27]. The ﬁrst dataset is
a spatiotemporal mobility trace consisting of 6.44M check-
ins generated by .2M users. Each check-in has the format
of <UserID, latitude, longitude, timestamp, location ID>.
The second dataset is a social graph (1M edges) of the same
.2M users. Assume the mobility trace is anonymized. Our
objective is to de-anonymize the mobility trace using the so-
cial graph as auxiliary data. Since the mobility trace does
not have an explicit graph structure, supposing the social
graph is the ground truth, we apply the technique in [27]
on the mobility trace to construct four graphs with diﬀerent
recalls and precisions, denoted by M 1, M 2, M 3, and M 4,

(a) Gowalla

(b) Google+

Figure 2:
[0.1, 0.3], c3 ∈ [0.4, 0.8], c4 = 0, α ∈ [10, 30], γ ∈ [1, 4].

Landmark identi(cid:12)cation.

c1, c2 ∈

tp+fn and precision = tp

respectively (recall = tp
tp+fp , where
tp = true positive, fp = false positive, and fn = false neg-
ative). Particularly, the recall and precision of M 1 are 0.6
and 0.865, of M 2 are 0.72 and 0.83, of M 3 are 0.75 and 0.78,
and of M 4 are 0.8 and 0.72, respectively. The second dataset
considered is the Google+ dataset in Section 5, which has
4.7M users and 90.8M edges. Given some projection prob-
ability ℘ ∈ [0.5, 0.9], We ﬁrst use the projection process in
Section 4 to produce Ga and Gu, and then use ODA to de-
anonymize Ga with Gu as auxiliary data. Note that, the
auxiliary data is from a diﬀerent domain (social data) with
the anonymized data (mobility trace) in Gowalla while the
auxiliary and anonymized data are from the same domain
in Google+.
All the experiments are implemented on a PC with 64 bit
Ubuntu 12.04 LTS OS, Intel Xeon E5620 CPU (2.4GHz ×
8 Threads), 48GB memory, and 2 disks with 8TB storage.
6.2.2 Results
Landmark Identi(cid:12)cation. As we mentioned in the pre-
vious subsection, ODA itself can work as a landmark i-
L = ∅ in ODA, i.e.,
dentiﬁcation algorithm. Let V a
s(fl(·), fl(·)) = 0 in ϕ(·,·). Then, we run ODA on Gowalla
and Google+ to identify some landmarks as shown in Fig.
2 (note that, the DA in ODA is conducted according to the
degree non-increasing order). The results show that we can
de-anonymize the ﬁrst 60-94 users in Gowalla and the ﬁrst
129-179 users in Google+ perfectly (100% correctly). For
instance, when Ga = M 2 in Gowalla, the ﬁrst 75 users are
perfectly de-anonymizable and when ℘ = 0.7, the ﬁrst 137
users in Google+ are perfectly de-anonymizable. According
to ODA, the identiﬁed landmarks can serve as references for
future DA.

L = U u

From Fig. 2 (a), when the recall increases, there are more
common edges between Ga and Gu, which implies it is eas-
ier to identify the high degree users based on the increased
structural information and thus more landmarks can be i-
dentiﬁed. Because of a similar reason, we can see from Fig.
2 (b) that more landmarks can be identiﬁed in Google+ for
large ℘ due to more edge overlap between Ga and Gu.

DA Results. By taking the users identiﬁed in Fig. 2
as landmarks, we employ ODA to de-anonymize Gowalla
(M 1, M 2, M 3, M 4) and Google+ (Ga with diﬀerent ℘)
as shown in Fig. 3, where the x-axis represents the accu-
mulated percentage of users de-anonymized and the y-axis
represents the accumulated percentage of users successfully
de-anonymized. From Fig. 3, we can see that the success-
ful DA rate is higher for large-degree users than that of

60757694M1M2M3M4020406080100Landmarks1291301371521790.50.60.70.80.9020406080100120140160180Landmarks1050perfect DA. Therefore, for secure data publication, besides
the data itself, the information carried by the data’s struc-
ture is also essential and deserves dedicated consideration
before released.

The fact is that we still have a long way to go
to achieve secure data publishing. From our large
scale study on 26 real world datasets, most of the exist-
ing structural datasets are de-anonymizable based only on
their structural information. On the other hand, existing
anonymization techniques are vulnerable to structure based
DA attacks. Therefore, new anonymization techniques should
be developed. Meanwhile, since structural data release, shar-
ing, or transferring has signiﬁcant business and social val-
ue, the data utility should be preserved in the new developed
anonymization schemes. In summary, new secure data pub-
lishing schemes that properly achieve a balance between data
privacy protection and data utility preservation must be de-
veloped.

Suggestions for secure data publishing. Secure da-
ta publishing is important for businesses, research, and the
society. However, with the wide availability of rich auxil-
iary information, especially with the emergence of Collabora-
tive Information Seeking (CIS) systems and data/knowledge
brokers [28][29], the privacy of people, businesses, govern-
ments, etc. will increasingly be compromised. For secure
data publishing, some general suggestions are as follows. (i)
Carefully share data with or transfer data to third parties
and partners. Before sharing the data, the data owner-
s should examine the dedicated applications to see if the
data sharing is necessary. Based on the requirements of
applications, the data could be shared in diﬀerent granular-
ity levels: digest level : share/transfer a digest/summary of
the data to third parties or partners; partial and density-
controlled level : based on our quantiﬁcation, controlling the
graph density could increase the diﬃculty of DA. Therefore,
in this level, only a density-controlled anonymized version
(e.g., by sampling) of a subset of the data (e.g., a communi-
ty) is shared/transferred; density-controlled level : a density-
controlled anonymized version of the data is shared or trans-
ferred; full level : an anonymized version of the full dataset
is shared or transferred. (ii) Evaluate the potential vulnera-
bility of the dataset before actual publishing. Before actually
publishing the data, the data owners can evaluate the vul-
nerability of the data. For instance, the data (structural)
can be evaluated using our quantiﬁcation as in Section 5.
(iii) Develop proper policy on data collection. Many struc-
tural data owners allow public data collection, e.g., Twitter,
Facebook allow crawlers and other automatic programs to
collect users’ information online. This could increase the
data DA risk by providing auxiliary information to adver-
saries. Therefore, it is better for data owners to develop
proper policies to limit such public data collection.

8. CONCLUSION AND FUTURE WORK

In this paper, we study the quantiﬁcation, practice, and
implications of structural data DA. First, for the ﬁrst time,
we address several fundamental open problems in data DA
research by quantifying the conditions for perfect DA and
(1− ϵ)-perfect DA under a general data model. This bridges
the gap between structural data DA practice and theory.
Second, we conduct a large scale study on the de-anonymizability
of 26 diverse real world structural datasets, which turn out
to be de-anonymizable partially or perfectly. Third, follow-

(a) De-anonymize Gowalla

(b) De-anonymize Google+

Figure 3: De-anonymize Gowalla and Google+.
c1, c2 ∈ [0, 0.2], c3 + c4 ∈ [0.4, 1], α ∈ [10, 30], γ ∈ [2, 10].

small-degree users, i.e., when x increases, the percentage of
successfully deanonymized users generally show a decreas-
ing trend. The reason is that large-degree users carry more
structural information, which can thus be more accurate-
ly de-anonymizable. This can also be seen from our quan-
tiﬁcation. For Gowalla, we observe from Fig. 3(a) that
although recall dominates the landmark identiﬁcation pro-
cess, the large-scale DA performance is impacted more by
precision. Generally, a high precision implies this dataset
is more de-anonymizable, e.g. M 4. This is because a high
precision implies a low false positive, which can be viewed as
noise in practice, and thus the DA accuracy is better. For
Google+, we see from Fig. 3 (b) that the Ga projected with
a large ℘, e.g., ℘ = 0.9, is more de-anonymizable. As shown
in our quantiﬁcation, this is because a large ℘ implies more
similarity between Ga and Gu and thus more users can be
successfully de-anonymized.

From Fig. 3, we also see that the DA performance of O-
DA on Gowalla and Google+ is better than the evaluation
results shown in Tab. 3, e.g., when ℘ = 0.9, Tab. 3 indicates
91.2% of the users in Google+ are a.a.s. de-anonymizable
while ODA successfully de-anonymizes 95.5% of the users.
This is because the values shown in Tab. 3 are the low-
er bounds on de-anonymizable users.
In summary, about
77.7%− 83.3% of the users in Gowalla and 86.9%− 95.5% of
the users in Google+ are de-anonymizable. Thus, structure
based DA is implementable and powerful in practice.

Time Consumption. We calculate the time consump-
tion on de-anonymizing Gowalla and Google+. On average,
the initialization time (used for initializations), execution
time (used for executing the iterations in ODA), and total
time are 1.79 mins, 1.6 mins, and 3.39 mins for Gowalla and
0.88 hours, 5.61 hours, and 6.49 hours for Google+, respec-
tively.

7.

IMPLICATIONS AND DISCUSSION

Based on our DA quantiﬁcation, evaluation on real world
datasets, and our implemented DA scheme ODA, we provide
some implications of this paper in this section. We also
discuss the impacts of our ﬁndings to secure data publishing
in practice and provide guidelines for future data publishing.
Structural information may induce privacy leak-
age. Although we have some previous work that show struc-
ture based DA is possible, in this paper, we theoretically
demonstrate the reasons by providing rigorous quantiﬁcation
under a general data model. From the quantiﬁcation, struc-
tural information can enable large-scale perfect or (1 − ϵ)-

0.00.10.20.30.40.50.60.70.80.91.00.780.800.820.840.860.880.900.920.940.960.981.00% of Sucessfully De-anonymized Users% of De-anonymized Users M1 M2 M3 M40.00.10.20.30.40.50.60.70.80.91.00.860.870.880.890.900.910.920.930.940.950.960.970.980.991.00% of Successfully De-anonymized Users% of De-anonymized Users 0.5 0.6 0.7 0.8 0.91051ing our quantiﬁcation, we propose a cold start single-phase
Optimization based DA (ODA) attack. We also analyze O-
DA theoretically and experimentally. The experimental re-
sults show that 77.7% − 83.3% of the users in Gowalla (.2M
users, 1M edges) and 86.9%− 95.5% of the users in Google+
(4.7M users, 90.8M edges) can be de-anonymized, which im-
plies structure based DA is implementable and powerful in
practice. Finally, we conclude with some implications from
our ﬁndings and provide some general suggestions for future
secure data publishing.

Our future work will focus on the following: (i) We will
evaluate our quantiﬁcation on more structural datasets to
further examine its generality. We also plan to improve O-
DA to make it more eﬃcient and robust; (ii) Since existing
anonymization techniques are vulnerable to structure based
DA attacks, we propose to develop application based eﬀec-
tive schemes against such attacks; (iii) Data utility is an-
other important concern. We plan to study how to quantify
the tradeoﬀ between privacy and utility followed by propos-
ing privacy protection schemes with utility preservation; and
(iv) Finally, due to the importance of secure data publish-
ing, we propose to develop a secure data publishing platform
in the future, which is expected to be invulnerable to both
semantics based and structure based DA attacks.
Acknowledgments
The authors are very grateful to Nana Li and Jing S. He
for helpful discussions on graph theory, to Huy Pham who
helped us to process the Gowalla mobility trace (Huy Pham
also shared a social strength graph obtained from the mo-
bility trace of Gowalla users at Texas), and to Neil Z. Gong
who shared the Google+ dataset with us.

This work was partly supported by NSF-CAREER-CNS-
0545667. Mudhakar Srivatsa’s research was sponsored by US
Army Research laboratory and the UK Ministry of Defence
and was accomplished under Agreement Number W911NF-
06-3-0001. The views and conclusions contained in this doc-
ument are those of the authors and should not be inter-
preted as representing the oﬃcial policies, either expressed
or implied, of the US Army Research Laboratory, the U.S.
Government, the UK Ministry of Defense, or the UK Gov-
ernment. The US and UK Governments are authorized to
reproduce and distribute reprints for Government purposes
notwithstanding any copyright notation hereon.

9. REFERENCES
[1] L. Backstrom, C. Dwork, and J. Kleinberg, Wherefore Art

Thou R3579X? Anonymized Social Networks, Hidden
Patterns, and Structural Steganography, WWW 2007.

[2] A. Narayanan and V. Shmatikov, De-anonymizing Social

Networks, S&P 2009.

[3] M. Srivatsa and M. Hicks, Deanonymizing Mobility Traces:

Using Social Networks as a Side-Channel, CCS 2012.
[4] G. Wondracek, T. Holz, E. Kirda, and C. Kruegel, A

Practical Attack to De-Anonymize Social Network Users,
S&P 2010.

[5] P. Pedarsani and M. Grossglauser, On the Privacy of

Anonymized Networks, KDD 2011.

[6] M. Hay, G. Miklau, D. Jensen, D. Towsley, and P. Weis,

Resisting Structural Re-identiﬁcation in Anonymized Social
Networks, VLDB 2008.

[7] K. Liu and E. Terzi, Towards Identity Anonymization on

Graphs, SIGMOD 2008.

[8] N. Li, W. Qardaji, and D. Su, On Sampling,
Anonymization, and Diﬀerential Privacy Or,

K-Anonymization Meets Diﬀerential Privacy, ASIACCS
2012.

[9] C. Dwork, Diﬀerential Privacy, ICALP 2006.
[10] A. Korolova, R. Motwani, S. U. Nabar, and Y. Xu, Link

Privacy in Social Networks, CIKM 2008.

[11] E. Zheleva and L. Getoor, To Join or Not to Join: The
Illusion of Privacy in Social Networks with Mixed Public
and Private User Proﬁles, WWW 2009.

[12] J. Pang, B. Greenstein, R. Gummadi, S. Seshan, and D.

Wetherall, 802.11 User Fingerprinting, Mobicom 2007.
[13] L. Backstrom, E. Sun, and C. Marlow, Find me If You
Can: Improving Geographical Prediction with Social and
Spatial Proximity, WWW 2010.

[14] S. Han, V. Liu, Q. Pu, S. Peter, T. Anderson, A.

Krishnamurthy, and D. Wetherall, Expressive Privacy
Control with Pseudonyms, Sigcomm 2013.

[15] P. Mittal, M. Wright, and N. Borisov, Pisces: Anonymous

Communication Using Social Networks, NDSS 2013.
[16] J. Kannan, G. Altekar, P. Maniatis, and B.-G. Chun

Making programs forget: Enforcing Lifetime for Sensitive
Data, USENIX 2013.

[17] M. Egele, G. Stringhini, C. Krugel, and G. Vigna, COMPA:

Detecting Compromised Accounts on Social Networks,
NDSS 2013.

[18] K. Singh, S. Bhola, and W. Lee, xBook: Redesigning

Privacy Control in Social Networking Pl atforms, USENIX
2009.

[19] R. Shokri, G. Theodorakopoulos, J.-Y. L. Boudec, and J.-P.

Hubaux, Quantifying Location Privacy, S&P 2011.

[20] R. Shokri, G. Theodorakopoulos, C. Troncoso, J.-P.

Hubaux, and J.-Y. L. Boudec, Protecting Location Privacy:
Optimal Strategy against Localization Attacks, CCS 2012.

[21] M. E. J. Newman, Networks: An Introduction, Oxford

University Press, 2010.

[22] M. E. J. Newman, The Structure and Function of Complex

Networks, SIAM Review, No. 45, pp. 167-256, 2003.

[23] B. Bollob(cid:19)as, Random Graphs (Second Edition), Cambridge

University Press, 2001.

[24] J. Riordan, An Introduction to Combinatorial Analysis,

Wiley, 1958.

[25] N. Z. Gong, W. Xu, L. Huang, P. Mittal, E. Stefanov, V.

Sekar and D. Song, Evolution of Social-Attribute Networks:
Measurements, Modeling, and Implications using Google+,
IMC 2012.

[26] http://snap.stanford.edu/data/
[27] H. Pham, C. Shahabi, and Yan Liu, EBM - An

Entropy-Based Model to Infer Social Strength from
Spatiotemporal Data, SIGMOD 2013.

[28] C. Shah, R. Capra, and P. Hansen, Collaborative

Information Seeking, Computer, 2014.

[29] Z. Xu, J. Ramanathan, and R. Ramnath, Identifying

Knowledge Brokers and Their Role in Enterprise Research
through Social Media, Computer, 2014.

APPENDIX
A. PROOF SKETCH OF THEOREM 1
Since k is the number of incorrect mappings in σ ̸= σ0,
2 ≤ k ≤ n is evidently. For convenience of proof, let σk
be a DA scheme that has k incorrect mappings. Under σk,
let Vk ⊆ V be the set of incorrectly de-anonymized users5,
Ek = {ei,j|i ∈ Vk or j ∈ Vk} be the set of all possible
edges adjacent to at least one user in Vk, Eτ = {ei,j|i, j ∈
Vk, (i, j) ∈ σk, and (j, i) ∈ σk} be the set of all possible
edges corresponding to transposition mappings 6 in σk, and
E = {ei,j|1 ≤ i ̸= j ≤ n} be the set of all possible edges on
5Without of causing any confusion, we use V , V a, and

V u interchangeably since V = V a = V u.
6If both mappings (i, j) and (j, i) are in σk, then
{(i, j), (j, i)} is called a transposition mapping, i.e., two users
are incorrectly de-anonymized to each other.

1052(

)

(

)

k
2

n
2

V . Furthermore, deﬁne mk = |Ek| and mτ = |Eτ|. Then,
we have |Vk| = k, mk =
2 since there
are at most k
, and
∀ei,j ∈ E, Pr(ei,j ∈ E) = pi,j = didj
2m−1 .

2 transposition mappings in σk, |E| =

+ k(n − k), mτ ≤ k

Now, we quantify Ψσ0 stochastically. Actually, to quan-
tify Ψσ0 , we considering the DE caused by the projection
of each edge rather than considering the mapping direct-
ly. ∀ei,j ∈ E, if it appears in E and is projected to either
Ga or Gu but not both during the edge projection process,
then according to the deﬁnition of DE, it will cause a DE
of 2. Consequently, the DE caused by ei,j satisﬁes a bino-
mial distribution B(2, 2pi,j · ℘(1 − ℘)). Furthermore, since
the projection process is i.i.d. and considering Lemma 1,
B(2, 2pi,j · ℘(1 − ℘)) =
we have Ψσ0 =

ψt,t′ ∼ ∑

∑

(t,t′)∈σ0

2, 2pi,j · ℘(1 − ℘)).

ei;j∈E

∑

B(

ei;j∈E
When we quantify Ψσk , we consider three cases respec-
for ∀ei,j ∈ E \ Ek, the DE caused by ei,j
tively. Case 1 :
during the projection process also satisﬁes the binomial dis-
tribution B(2, 2pi,j · ℘(1 − ℘)) since i, j ∈ V \ Vk (i.e.,
i, j are successfully de-anonymized under σk). Case 2: for
∀ei,j ∈ Ek\Eτ , it will be mapped to some other possible edge
σk(ei,j) = eσk(i),σk(j) ∈ E since ei,j /∈ Eτ and at least one of

∑

(t,t′)∈σk

ei;j∈E\Ek

ei;j∈Ek\E(cid:28)
pi,j℘)) +

ψt,t′ ∼ ∑

i and j is incorrectly de-anonymized under σk. Therefore, in
this case, the DE caused by ei,j during the projection process
satisﬁes binomial distribution B(2, pi,j·℘(1−pσk(i),σk(j)℘)+
for ∀ei,j ∈ Eτ , since
pσk(i),σk(j) · ℘(1 − pi,j℘)). Case 3:
it corresponds to a transposition mapping, the DE caused
by ei,j during the projection process also satisﬁes the bi-
nomial distribution B(2, 2pi,j · ℘(1 − ℘)). In summary, we
∑
B(2, 2pi,j · ℘(1 − ℘)) +
have Ψσk =
∑
B(2, pi,j · ℘(1 − pσk(i),σk(j)℘) + pσk(i),σk(j) · ℘(1 −
∑
∑
ei;j∈E(cid:28)
B(2, 2pi,j · ℘(1 − ℘)) +
ei;j∈Ek\E(cid:28)
ei;j∈E\Ek
pσk(i),σk(j)℘)+pσk(i),σk(j)·℘(1−pi,j℘)) = B(
℘(1−℘))+B(
℘(1 − pi,j℘)).

≥
∑
B(2, pi,j · ℘(1 −
2, 2pi,j·
2, pi,j·℘(1−pσk(i),σk(j)℘)+pσk(i),σk(j)·
∑
∑

ei;j∈Ek\E(cid:28)
Now, deﬁne X ∼ B(
pσk(i),σk(j)·℘(1−pi,j℘)) and Y ∼ B(

B(2, 2pi,j · ℘(1 − ℘))
∑

2, pi,j · ℘(1− pσk(i),σk(j)℘) +
2, 2pi,j·℘(1−℘)).

Let λx and λy by the mean values of X and Y , respectively.
Thus, we have λx = (

2)·[pi,j ·℘(1−pσk(i),σk(j)℘)+
pσk(i),σk(j) · ℘(1 − pi,j℘)] ≥= 4l℘(1 − h℘)(mk − mτ ) and
λy ≤ 4h℘(1− ℘)mk. Then, ∀σk (k ∈ [2, n]), Pr(Ψσk
≤

ei;j∈Ek\E(cid:28)

ei;j∈Ek\E(cid:28)

− Ψσ0

ei;j∈E\Ek

∑

ei;j∈Ek

stochastically

≤

Pr(X − Y ≤ 0).

0)

stochastically

k
2

≃

(

)
We now derive the upper bound on Pr(X − Y ≤ 0). S-
h−hl , mτ ≤ k
+ k(n − k), ℘ >
ince ℘ > h−l
⇒ λx > λy. Apply-
h−hl = (h−l)mk
h−l
(h−hl)mk
n → ∞
(
)
ing Lemma 1 and considering that f℘ = Ω( 2 ln n+1
), we have
Pr(X − Y ≤ 0) ≤ 2 exp(− (λx−λy )2
∑
= 2 exp(−Ω( 2 ln n+1
)·(
1
n2 .

2 , and mk =
(h−l)mk+lm(cid:28)
(h−hl)mk+lhm(cid:28)
8(λx+λy ) ) ≤ 2 exp(−f℘mk)
+k(n−k))) ≤ 2 exp(−2 ln n−1) ≤

Deﬁne ζ(2) =

1
n2 . Then, ζ(2) is the Euler-Riemann
6 < ∞.
zeta function with parameter 2 and thus ζ(2) = π2
Consequently, according to the Borel-Cantelli Lemma, it is
a.a.s. that X ≥ Y . It follows that it is a.a.s. that Ψσk
≥
Ψσ0 for 2 ≤ k ≤ n, i.e., Pr(Ψσ ≥ Ψσ0 ) → 1 for any σ ̸= σ0.

n>0

kn

kn

k
2

2

σ

kn

σk

σk

n
k

k=2

k=2

Pr(

∪

n∪

Eσ) = Pr(

Eσk ) ≤ n∑
n∑

B. PROOF SKETCH OF THEOREM 2
∪
Let Eσ be the event that Ψσ ≤ Ψσ0 . Then, Pr(E) =
(
Eσk ). Let ϱk be the number of de-

)·!k ≤ nk, where !k is the subfactorial of k [24][5].
anonymizatoin schemes having k incorrect mappings. Then,
ϱk =
≤ Ψσ0 ) ≤ n∑
n∪
∪
Then, considering that f℘ = Ω( (k+3) ln n+1
) and based on
Boole’s inequality and the proof of Theorem 1, we have
nk ·
Pr(E) = Pr(
n∑
2 exp(−f℘mk) ≤
2 exp(k ln n − (k + 3) ln n − 1) ≤
n → ∞
6 < ∞, it is a.a.s.
that Pr(E) → 0 based on the Borel-Cantelli Lemma, i.e., it
is a.a.s. that there exists no DA scheme such that σ ̸= σ0
and Ψσ ≤ Ψσ0 .
C. PROOF OF THEOREM 9
(i) ODA’s space complexity is upper bound by O(min
{n2, m + n}). The proof is straightforward and thus we
omit it.

ϱk · Pr(Ψσk
∑

n2 . Again, since ζ(2) =

≤ 1

= π2

1
n3

n>0

k=2

k=2

k=2

k=2

2

(ii) In ODA, we assume fd(i), fn(i), fK (i), fl(i), and fc(i)
are computed before the iteration starts and the time con-
sumption of computing these features is bounded by O(m +
n log n). Then, from ODA, the worst case time consump-
tion of each iteration is upper bounded by γα = γΘ(log n) =
2log γ(cid:2)(log n)
= 2Θ(log n) log γ = nΘ(1) log γ. Furthermore, the
number of iterations in ODA is Θ(n/Γ).
It follows the
worst case time complexity of ODA is O(m + n log n) +
O(nΘ(1) log γ+1 /Γ) = O(m + n log n + nΘ(1) log γ+1/Γ).
2

1053