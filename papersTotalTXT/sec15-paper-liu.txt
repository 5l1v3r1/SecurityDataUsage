Cloudy with a Chance of Breach:  

Forecasting Cyber Security Incidents

Yang Liu, Armin Sarabi, Jing Zhang, and Parinaz Naghizadeh, University of Michigan; 

Manish Karir, QuadMetrics, Inc.; Michael Bailey, University of Illinois at Urbana-Champaign; 

Mingyan Liu, University of Michigan and QuadMetrics, Inc.

https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/liu

This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-931971-232Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXCloudy with a Chance of Breach: Forecasting Cyber Security Incidents

Yang Liu1, Armin Sarabi1, Jing Zhang1, Parinaz Naghizadeh1

Manish Karir2, Michael Bailey3, Mingyan Liu1,2

1 EECS Department, University of Michigan, Ann Arbor

3 ECE Department, University of Illinois, Urbana-Champaign

2 QuadMetrics, Inc.

Abstract

In this study we characterize the extent to which cyber
security incidents, such as those referenced by Verizon
in its annual Data Breach Investigations Reports (DBIR),
can be predicted based on externally observable prop-
erties of an organization’s network. We seek to proac-
tively forecast an organization’s breaches and to do so
without cooperation of the organization itself. To ac-
complish this goal, we collect 258 externally measur-
able features about an organization’s network from two
main categories: mismanagement symptoms, such as
misconﬁgured DNS or BGP within a network, and mali-
cious activity time series, which include spam, phishing,
and scanning activity sourced from these organizations.
Using these features we train and test a Random For-
est (RF) classiﬁer against more than 1,000 incident re-
ports taken from the VERIS community database, Hack-
mageddon, and the Web Hacking Incidents Database that
cover events from mid-2013 to the end of 2014. The re-
sulting classiﬁer is able to achieve a 90% True Positive
(TP) rate, a 10% False Positive (FP) rate, and an overall
90% accuracy.

1

Introduction

Recent data breaches, such as those at Target [35], JP
Morgan [25], and Home Depot [49] highlight the in-
creasing social and economic impact of such cyber inci-
dents. For example, the JP Morgan Chase attack was be-
lieved to be one of the largest in history, affecting nearly
76 million households [25]. Often, by the time a breach
is detected, it is already too late and the damage has al-
ready occurred. As a result, such events call into the
question whether these breaches could have been pre-
dicted and the damage avoided. In this study we seek
to understand the extent to which one can forecast if an
organization may suffer a cyber security incident in the
near future.

Machine learning has been used extensively in the
cyber security domain, most prominently for detection
of various malicious activities or entities, e.g., spam
[44, 45] and phishing[39]. It has been used far less for the
purpose of prediction, with the notable exception of [51],
where textual data is used to train classiﬁers to predict
whether a currently benign webpage may turn malicious
in the near future. The difference between detection and
prediction is analogous to the difference between diag-
nosing a patient who may already be ill (e.g., by using
biopsy) vs. projecting whether a presently healthy per-
son may become ill based on a variety of relevant fac-
tors. The former typically relies on identifying known
characteristics of the object to be detected, while the lat-
ter on factors believed to correlate with the prediction
objective.

To explore the effectiveness of forecasting security in-
cidences we begin by collecting externally observed data
on Internet organizations; we do not require information
on the internal workings of a network or its hosts. To do
so, we tap into a diverse set of data that captures different
aspects of a network’s security posture, ranging from the
explicit or behavioral, such as externally observed ma-
licious activities originating from a network (e.g., spam
and phishing) to the latent or relational, such as misman-
agement and misconﬁgurations in a network that deviate
from known best practices. From this data we extract 258
features and feed them to a Random Forest (RF) classi-
ﬁer. We train and test the classiﬁer on these features and
more than 1,000 incident reports taken from the VERIS
community database [55], Hackmageddon [42], and the
Web Hacking Incidents Database [31] that cover events
from mid-2013 to 2014. The resulting classiﬁer can be
conﬁgured over a wide range of operating points includ-
ing one with 90% True Positive (TP) rate, 10% False Pos-
itive (FP) rate and an overall accuracy of 90%.

We posit that such cyber incident forecasting offers a
completely different set of characteristics as compared to
detection techniques, which in turn enables entirely new

USENIX Association  

24th USENIX Security Symposium  1009

classes of applications that are not feasible with detection
techniques alone. First and foremost, prediction allows
proactive policies and measures to be adopted rather than
reactive measures following the detection of an incident.
Effective proactive actions can substantially reduce the
potential cost incurred by an incident; in this sense pre-
diction is complementary to detection. Cyber incident
prediction also enables the development of effective risk
management schemes such as cyber insurance, which in-
troduces monetary incentives for the adoption of better
cyber security policies and technologies. In the wake of
recent breaches, the market for such policies has soared,
with current written annual premiums estimated to be be-
tween $500M and $1B [47].

The remainder of the paper is organized as follows.
Section 2 introduces the datasets used in this study and
details the rationale for their use as well as our processing
methodology. We then deﬁne the features we use in con-
structing the classiﬁer and show why they are relevant
in predicting security incidents in Section 3. We present
the main prediction results as well as their implications in
Section 4. In Section 5 we discuss a number of observa-
tions and illustrate several major data breaches in 2014
in the context of this prediction methodology. Related
work is detailed in Section 6, and Section 7 concludes
the paper.

2 Data Collection and Processing

Our study draws from a variety of data sources that col-
lectively characterize the security posture of organiza-
tions, as well as security incident reports used to deter-
mine their security outcomes. These sources are summa-
rized in Table 1 and detailed below; a subset of these has
been made available at [7].

2.1 Security Posture Data

An organization’s network security posture may be mea-
sured in various ways. Here, we utilize two families of
measurement data. The ﬁrst is measurements on a net-
work’s misconﬁgurations or deviations from standards
and other operational recommendations; the second is
measurements on malicious activities seen to originate
from that network. These two types of measurements are
related. In particular, in [58] Zhang et al. quantitatively
established varying degrees of correlation between eight
different mismanagement symptoms and the amount of
malicious activities from an organization. The combina-
tion of both of these datasets represents a fairly compre-
hensive view of an organization’s externally discernible
security posture.

2.1.1 Mismanagement Symptoms

We use the following ﬁve mismanagement symptoms in
our study, a subset of those studied in [58].

Open Recursive Resolvers: Misconﬁgured open DNS
resolvers can be easily used to facilitate massive ampli-
ﬁcation attacks that target others.
In order to help the
network operations community address this wide spread
threat, the Open Resolver Project [14] actively sends a
DNS query to every public IPv4 address in port 53 to
identify misconﬁgured DNS resolvers. In this study, we
use a data snapshot collected on June 2, 2013. In total,
27.1 million open recursive resolvers were identiﬁed.

DNS Source Port Randomization: In order to minimize
the threat of DNS cache poisoning attacks [13], current
best practice (RFC 5452 [34]) recommends that DNS
servers implement both source port randomization and
a randomized query ID. Many servers however have not
been patched to implement source port randomization.
In [58], over 200,000 misconﬁgured DNS resolvers were
detected based on the analysis over a set of DNS queries
seen by VeriSign’s .com and .net TLD name server on
February 26, 2013. This is the data used in this study.

BGP Misconﬁguration: BGP conﬁguration errors or
reconﬁguration events can cause unnecessary routing
protocol updates with short-lived announcements in the
global routing table [40]. Zhang et. al detected 42.4 mil-
lion short-lived routes with BGP updates from 12 BGP
listeners in the Route Views project [32] during the ﬁrst
two weeks of June 2013 [58]; this data is used in our
study.

Untrusted HTTPS Certiﬁcates: Secure websites uti-
lize X.509 certiﬁcates as part of the TLS handshake in
order to prove their identity to clients. Properly conﬁg-
ured certiﬁcates should be signed by a browser-trusted
certiﬁcate authority. It is possible to detect misconﬁg-
ured websites by validating the certiﬁcate presented dur-
ing the TLS handshake [33]. An Internet scan performed
on March 22, 2013 found that only 10.3 million out of a
total of 21.4 million sites presented browser-trusted cer-
tiﬁcates [58]. We use this dataset in our study.

Open SMTP Mail Relays: Email servers should per-
form ﬁltering on the message source or destination to
only allow users in their own domain to send email mes-
sages. This is documented in current best practice (RFC
2505 [38]), and misconﬁgured servers can be used in
large scale spam campaigns. Though small in number,
these represent a severe misconﬁguration in an organiza-
tions’ infrastructure. In this study, we use data collected
on July 23, 2013, which detected 22,284 open mail relays
[58].

None of the datasets mentioned above is necessarily
directly related to a vulnerability. The presence of mis-
conﬁgurations in an organization’s networks and infras-

1010  24th USENIX Security Symposium 

USENIX Association

Category
Mismanagement
symptoms
Malicious activities May 2013 - December 2014

Collection period
February 2013 - July 2013

Incident reports

August 2013 - December 2014

Datasets
Open Recursive Resolvers, DNS Source Port Randomization, BGP misconﬁguration,
Untrusted HTTPS Certiﬁcates, Open SMTP Mail Relays [58]
CBL[4] , SBL[22], SpamCop[19], WPBL[24], UCEPROTECT[23], SURBL[20],
PhishTank[16], hpHosts[11], Darknet scanners list, Dshield[5], OpenBL[15]
VERIS Community Database [55], Hackmageddon [42], Web Hacking Incidents [31]

Table 1: Summary of datasets used in this study. Mismanagement and malicious activity data are used to extract
features, while incident reports are used to generate labels for the training and testing of a classiﬁer.

tructure is, however, an indicator of the lack of appro-
priate policies and technological solutions to detect such
failures. The latter increases the potential for a success-
ful data breach.

Also, note that all of the above datasets were collected
during roughly the ﬁrst half of 2013. As we shall be
using the mismanagement symptoms as features in con-
structing a classiﬁer/predictor, it is important that these
features reﬂect the condition of a network prior to the
incidents. Consequently, our incident datasets (detailed
in Section 2.2) cover incidents that occurred between
August 2013 and December 2014. Note also that we
use only a single snapshot of each of the symptoms;
this is because such symptomatic data is relatively slow-
changing over time, as systems are generally not recon-
ﬁgured on a daily or even weekly basis.

2.1.2 Malicious Activity Data

Another indicator of the lack of policy or technical
measures to improve security at an organization is the
level of malicious activities observed to originate from
its network assets and infrastructure. Such activity is
often observed by well-established monitoring systems
such as spam traps, darknet monitors, or DNS moni-
tors. These observations are then distilled into black-
lists. We use a set of reputation blacklists to measure the
level of malicious activities in a network. This set further
breaks down into three types: (1) those capturing spam
activities, including CBL[4] , SBL[22], SpamCop[19],
WPBL[24], and UCEPROTECT[23], (2) those capturing
phishing and malware activities, including SURBL[20],
PhishTank[16], and hpHosts[11], and (3) those capturing
scanning activities, including the Darknet scanners list,
Dshield[5], and OpenBL[15]. We use reputation black-
lists that have been collected over a period of more than
a year, starting in May 11, 2013 and ending in December
31, 2014. Each blacklist is refreshed on a daily basis and
consists of a set of IP addresses seen to be engaged in
some malicious activity. This longitudinal dataset allows
us to characterize not only the presence of malicious ac-
tivities from an organization, but also its dynamic beha-
vior over time.

2.2 Security Incident Data
In addition to the security posture data described in the
previous section, we require data on reported cyber-
security incidents to serve as ground-truth in our study;
such data is needed for the purpose of training the clas-
siﬁer, as well as for assessing its accuracy in predicting
incidents (testing). In general, we believe such incidents
are vastly under reported. In order to obtain a good cove-
rage, we employ three collections of publicly available
incident datasets. These are described below.

VERIS Community Database (VCDB) [55]: This
dataset represents a broad ranging public effort to gather
cyber security incident reports in a common format [55].
The collection is maintained by the Verizon RISK Team,
and is used by Verizon in its highly publicized annual
Data Breach Investigations Reports (DBIR) [56]. The
current repository contains more than 5,000 incident re-
ports, that cover a variety of different types of events
such as server breach, website defacements, and physi-
cally stolen assets. Table 7 (in the Appendix) provides
some example reports from this repository; a majority
(64.99%) is from the US.

Of the full set, roughly 700 unique incidents were rele-
vant to our study: we include only incidents that occurred
after mid-2013 so that they are aligned with the security
posture data, and those directly reﬂecting cyber-security
issues. We therefore exclude those due to physical at-
tacks, robbery, deliberate mis-operation by internal ac-
tors (e.g. disgruntled employees) and the like, as well as
unnamed or unveriﬁed attack targets. We show several
such examples in Table 2. Also note that even though the
same IPs may appear in both the malicious and incident
data, the independence of the features from ground-truth
data is maintained because malicious activities only re-
veal botnet presence, which is not considered an incident
type by or reported in any of our incident datasets.

Incident report
Student of a college changed score
Road construction sign hacked
Praxair Healthcare Inc. asset stolen
Lucile Packard Child. Hosp.l asset stolen
Medicare Privilege Misuse

Reason to exclude
Unknown target
Physical tampering
Physical theft
Physical theft
Deliberate internal misuse

Table 2: Examples of excluded VCDB incidents.

USENIX Association  

24th USENIX Security Symposium  1011

Hackmageddon [42]: This is an independently main-
tained cyber incident blog that aggregates and documents
various public reports of cyber security incidents on a
monthly basis. From the overall set we extract 300 in-
cidents, in which the reported dates are aligned with our
security posture data, between October 2013 and Febru-
ary 2014, and for which we are able to clearly identify
the affected organizations.

The Web Hacking Incidents Database (WHID) [31]:
This is an actively maintained cyber security incident
repository; its goal is to raise awareness of cyber secu-
rity issues and to provide information for statistical anal-
ysis. From the overall dataset we identify and extract
roughly 150 incidents, for which the reported dates are
aligned with our security posture data, between January
2014 and November 2014.

A breakdown of the incidents by type from each of
these datasets is given in Table 5. Note that Hackmaged-
don and WHID have similar categories while VCDB has
much broader categories.

Incident type
Hackmageddon

WHID

Incident type

VCDB

SQLi
38
12

Crimeware

59

Cyber Esp.

16

Web app.

368

Hijacking

Defacement

DDoS

9
5

97
16

59
45
Else
213

Table 3: Reported cyber incidents by category. Only the
major categories in each set are shown. The “Else” cat-
egory by VCDB represents incidents lacking sufﬁcient
detail for better classiﬁcation.

2.3 Data Pre-processing
Though our diverse datasets give us substantial visibility
into the state of security at an organizational level, the
diversity also presents substantial challenges in aligning
the data in both time and space. All of the security pos-
ture datasets – mismanagement and malicious activities
– record information at the host IP-address level; e.g.,
they reveal whether a particular IP address is blacklisted
on a given day, or whether a host at a speciﬁc IP address
is misconﬁgured. On the other hand, a cyber incident
report is typically associated with a company or organi-
zation, not with a speciﬁc IP address within that domain.
Conceptually, it is more natural to predict incidents for
an organization for the following reasons. Firstly, our
interest is in predicting incidents broadly deﬁned as a
way to assess organizational cyber risk. Secondly, while
some IP addresses are statically associated with a ma-
chine, e.g., a web server, others are dynamically assigned
due to mobility, e.g., through WiFi. In the latter case pre-
dicting for speciﬁc IP addresses no longer makes sense.

This mismatch in resolution means that we will have
to (1) map an organization reported in an incident to a
set of IP addresses and (2) aggregate mismanagement
and maliciousness information over this set of addresses.
To address the ﬁrst step we will ﬁrst retrieve a sam-
ple IP address in the network of the compromised or-
ganization, which is then used to identify an aggrega-
tion unit – a set of IP addresses – that allows us to re-
cover the network asset involved in the incident. Sample
IP addresses are obtained by manually processing each
incident report, and the aggregation units are identiﬁed
by using registration information from Regional Inter-
net Registries (RIR) databases. These databases are col-
lected separately from ARIN [3], LACNIC [12], APNIC
[2], AFRINIC [1] and RIPE [18], who keep records of
IP address blocks/preﬁxes that are allocated to an orga-
nization. ARIN, APNIC, AFRINIC and RIPE databases
keep track of the IP addresses that have been allocated,
along with the organizations they have been allocated to,
labeled with a maintainer ID. LACNIC provides a less
detailed database, only keeping track of allocated blocks
and not the owners. In this case, we take the last alloca-
tion that contains our sample IP address – note a single
IP address might be reallocated several times, as part of
different IP blocks – i.e., the smallest block, as its owner.

2.3.1 Mapping Process

In the following paragraphs we explain in detail the ma-
nual process of (1): (1a) extracting sample IP addresses
through a number of examples, and (1b) identifying the
aggregation unit using the sample IP address. The ge-
neral outline of the process for (1a) is that we ﬁrst read
the report concerning each incident, and extract the web-
site of the company involved. If the website is the in-
trusion point in the breach, or indicative of the compro-
mised network, then we take the address of this website
to be our sample IP address. The website is determined
to be indicative of the compromised network when the
owner ID for the sample IP address matches the reported
name of the victim network. Occasionally the victim net-
work can be identiﬁed separately regardless of the web-
site address, but in most cases this is found to be an ef-
fective way of quickly obtaining the owner ID.

Our ﬁrst example [21] is a website defacement target-
ing the ofﬁcial website of the City of Mansﬁeld, Ohio.
Since the point of intrusion is clearly the website, we
take its address as our sample IP address for this inci-
dent. Note that in this case the website might be ma-
naged by a 3rd party hosting company, a possibility dis-
cussed further when we explain the process to address
(1b). The second example [6] is on Evernote resetting all
user passwords following an attack on its online system.
For this incident we identify said domain (evernote.com),

1012  24th USENIX Security Symposium 

USENIX Association

and trace it to an IP block in ARIN’s database registered
to Evernote Corporation. Since this network is main-
tained by Evernote itself, we take evernote.com to be our
sample IP address. Our ﬁnal example [10] involves the
defacement of Google Kenya and Google Burundi web-
sites. As the report suggests, the hackers altered the DNS
records of the domains by hacking into the Kenya and
Burundi NICs. Since the attack was not through directly
compromising the defaced websites, we excluded this
incident – the victim in this incident is neither Google
Kenya nor Google Burundi, but the networks owned by
the NICs.

The above examples provide insight into the manual
process of mapping incident to a network address. For
a large portion of the reports the incident descriptor is
unique and should therefore be treated as such; this is
the main reason that such a mapping is primarily done
manually. For a signiﬁcant portion (∼ 95%) of the re-
ports we are able to identify the compromised network
with a high level of conﬁdence – in such cases either the
report explicitly cites the website as the intrusion point
(ﬁrst example), or the network identiﬁed by the website
is registered under the victim organization (second exam-
ple). When neither of these conditions is satisﬁed, this
incident is excluded unless we can identify the victim
network through alternative means; such cases are few.
Overall our process is a conservative one: we only in-
clude an incident when there is zero or minimal ambigu-
ity. Finally, we also remove duplicate owner IDs in order
to avoid a bias against commonly used hosting compa-
nies (e.g. Amazon, GoDaddy) in our training and testing
process.

We now explain the process used in (1b) to map an ob-
tained sample IP address (as well as the identiﬁed owner
ID) to network(s) operated by a single entity. The ge-
neral outline of this process is as follows: we take all
the IP blocks that have the same owner ID listed in the
RIR databases, excluding sub-blocks that have been re-
allocated to other organizations, as our aggregation unit.
Continuing with the same set of examples, in the case
of Evernote (second example) we reverse search ARIN’s
database and extract all IP blocks registered to Evernote
Corporation, giving us a total of 520 IP addresses. For
the case of the City of Mansﬁeld website, using records
kept by ARIN we see that its web address belongs to Lin-
ode, a cloud hosting company. Obviously Linode is also
hosting other entities on its network without reported in-
cidents. Nonetheless, in this case we take the network
owned by Linode as our aggregation unit, since we can-
not further differentiate the source IP address(es) more
closely associated with the city. The inclusion of such
cases is a tradeoff as excluding them would have left
us with too few samples to perform a meaningful study.
More on this is discussed in Section 2.4.

2.3.2 A global table of aggregation units
The above explains how we process the incident reports
to identify network units that should be given a label
of “1”, i.e., victim organizations. For training and test-
ing purposes we also need to identify network units that
should be given a label of “0”, i.e., non-victim organi-
zations. To accomplish this, we built a global table us-
ing information gathered from the RIRs that provides us
with a global aggregation rule, containing both victim
and non-victim organizations. Our global table contains
4.4 million preﬁxes listed under 2.6 million owner IDs.
Note that the number of preﬁxes in the RIR databases
is considerably larger than the global BGP routing ta-
ble size, which includes roughly 550,000 unique preﬁxes
[41]. This is partly due to the fact that the preﬁxes in
our table can overlap for those that have been reallocated
multiple times. In other words, the RIR databases can
be viewed as a tree indicating all the ownership alloca-
tions and reallocations over the IP address space. On the
other hand, the BGP table tends to combine preﬁxes that
are located within the same Autonomous System (AS),
in order to reduce routing table sizes. Therefore, the RIR
databases provide us with a ﬁner-grained look into the IP
address space. By taking all the IP addresses that have
been allocated to an organization, and have not been fur-
ther reallocated, we can break the IP address space into
mutually exclusive sets, each owned and/or maintained
by a single organization. Out of the 4.4 million preﬁxes,
300,000 of them are assigned by LACNIC and therefore
have no owner ID. Combined with the 2.6 million owner
IDs from the other registries, the IP address space is bro-
ken, by ownership (or LACNIC preﬁxes), into 2.9 mil-
lion sets. Each set constitutes an aggregation unit that is
given a label of “0”, except for those already identiﬁed
and labeled as “1” by the previous process.

2.3.3 Aggregation Process
Once these aggregation units are identiﬁed, the second
step (2) is relatively straightforward. For each misma-
nagement symptom we simply calculate the fraction of
symptomatic IPs within such a unit. For malicious acti-
vities, we count the number of unique IP addresses listed
on a given day (by a single blacklist, or by blacklists
monitoring the same type of malicious activities) that be-
long to this unit; this results in one or more time series
for each unit. This step is carried out in the same way for
both victim and non-victim organizations.

2.4 A Few Caveats
As already alluded to, our data processing consists of a
series of rules of thumb that we follow to make the data
useable, some perhaps less clear-cut than others. Below

USENIX Association  

24th USENIX Security Symposium  1013

we summarize the typical challenges we encounter in this
process and their possible implications on the prediction
performance.

As described in Section 2.3, the aggregation units
are deﬁned using ownership information from RIR
databases. One issue with the use of ownership infor-
mation is that big corporations tend to register their IP
address blocks under multiple owner IDs, and in our pro-
cessing these IDs are treated as separate organizations.
In principle, as long as each of the aggregation units is
non-trivial in size, each can have its own security pos-
ture assessed. Furthermore, in some cases it is more ac-
curate to treat such IDs separately, since they might rep-
resent different sections of an organization under differ-
ent management. The opposite issue also exists, where
it may be impossible to distinguish between the network
assets of multiple organizations; recall, e.g. our ﬁrst ex-
ample where multiple organizations are hosted on the
same network. As mentioned before, we have chosen
in such cases to use the owner ID as the aggregation unit.
While this mapping process is clearly non-ideal, it is a
best-effort attempt at the problem, and will instead pro-
vide the classiﬁer with the average value of the features
over all organizations hosted on the identiﬁed network.
The labels for our classiﬁer are extracted from real in-
cident reports, and we can safely assume that the amount
of false positives in these reports, if any, is negligible.
However data breach incidents are only reported when
an external source detects the data breach (e.g. website
defacements), or an organization is obligated to report
the incident due to private customer information getting
compromised. In general, organizations tend not to an-
nounce incidents publicly, and security incidents remain
largely under-reported. This will affect our classiﬁer in
two ways: First, by failing to incorporate all incidents in
our training set, we may fail to identify all of the factors
that might affect an organization’s likelihood of suffer-
ing a breach. Second, when choosing non-victim organi-
zations, it is possible that we select some of them from
unreported victims, which could further impact the ac-
curacy of our classiﬁer. We have tried to overcome this
challenge by using three independently maintained in-
cident datasets. Ultimately, however, this can only be
addressed when timely incident reporting becomes the
norm; more on this is discussed in Section 5.

Last but not least, all the raw security posture data
(mismanagement symptoms and blacklists) could con-
tain error, which we have no easy way of calibrating.
However, two aspects of the present study help mitigate
the potential impact of these noises. Firstly, we use many
different datasets from independent sources; the diver-
sity and the total volume generally have a dampening
effect on the impact of the noise contained in any sin-
gle source. Secondly and perhaps more importantly, our

ultimate veriﬁcation and evaluation of the prediction per-
formance are not based on the security posture data, but
on the incident reports (with their own issues as noted
above). In this sense, as long as the prediction perfor-
mance is satisfactory, the noise in the input data becomes
less relevant.

3 Forecasting Methodology

The key to our prediction framework is the construction
of a good classiﬁer. We will primarily focus on the Ran-
dom Forest (RF) method [37], which is an ensemble cla-
ssiﬁer and an enhancement to the classical random de-
cision tree method.
It uses randomly selected subsets
of samples to construct different decision trees to form
a forest, and is generally considered to work well with
large and diverse feature sets. In particularly, it has been
observed to work well in several Internet measurement
studies, see e.g., [57]. As a reference, we will also pro-
vide performance comparison by using the Support Vec-
tor Machine (SVM) [27], one of the earliest and most
common classiﬁers. To train a classiﬁer, we need to iden-
tify a set of features from the measurement data. Below,
we ﬁrst detail the set of features used, and then present
the training and testing procedures.

3.1 Feature Set
We shall use two types of features, a primary set and a
secondary set. The primary set of features consists of the
raw data, while the secondary set is derived or extracted
from the raw data, i.e., in the form of various statistics.
In all, 258 features are used, including 5 mismanagement
features, 180 primary features, 72 secondary features,
and a last feature on the organization size.

3.1.1 Primary Features (186)

Mismanagement symptoms (5). There are ﬁve symp-
toms; each is measured by the ratio between the number
of misconﬁgured systems and the total number of sys-
tems in an organization. For instance, for the untrusted
HTTPS certiﬁcates, this ratio is between the number of
misconﬁgured certiﬁcates over the total number of cer-
tiﬁcates discovered in an organization. Similarly, for
open SMTP mail relay this ratio is between the num-
ber of misconﬁgured mail servers and the total number
of mail servers. The only exception is in the case of open
recursive resolver: since we do not know the total num-
ber of open resolvers, this ratio is between the number
of misconﬁgured open DNS resolvers and the total num-
ber of IPs in an organization. These ratios are denoted as
mi ∈ [0,1]5 for organization i.

1014  24th USENIX Security Symposium 

USENIX Association

Malicious activity time series (60 × 3). For each or-
ganization we collect three separate time series, one for
each malicious activity type, namely spam, phishing, and
scan. Accordingly, for organization i, its time series data
are denoted by rSP
. These time series data are
i
directly fed in their entirety into the classiﬁer. Several
examples of rSP
are given in Fig. 1; these are collected
i
over a two-month (60 days) period and show the total
number of unique IPs blacklisted on each day over all
spam blacklists in our dataset.

,rSC
i

,rPH

i

within the noise inherent in the blacklists), while Exam-
ples 2 and 3 show much higher levels of activity in gen-
eral. These two, however, differ in how persistent they
are at those high levels. Example 2 shows a network
with high levels throughout this period, while Example
3 shows a network that ﬂuctuates much more wildly. In-
tuitively, such dynamic behavior reﬂects to a large degree
how responsive the network operators are to blacklisting,
i.e., time to clean up, time to resurfacing of malicious ac-
tivities, and so on.

4

3

2

1

0

1k

800

600

400

10

20

30
Days

40

50

60

10

20

30
Days

40

50

60

10k

8k

6k

4k

2k

10

20

30
Days

40

50

60

(a) Org. 1

(b) Org. 2

(c) Org. 3

Figure 1: Examples of malicious activity time series of
three organizations; Y-axis is the number of unique IP
addresses listed on all spam blacklists in each day over a
60-day period.

Size (1). This refers to the size of an organization in
terms of the number of IP addresses identiﬁed within that
organization’s aggregation unit as outlined in the previ-
ous section. For organization i, this is denoted by si.

The relevance of these symptoms to an organization’s
security posture is examined more closely by comparing
their distributions among the victim and the non-victim
populations, as shown in Fig. 2. We see a clear difference
between the two populations in their untrusted HTTPS
and Openresolver distributions. This difference suggests
that these symptoms are meaningful distinguishers, and
thus hold predictive power. This is indeed veriﬁed later
when these two symptoms emerge as the most indicative
of the ﬁve. By contrast, the other three mismanagement
symptoms appear much less powerful.

The relevance of the malicious activity time series will
be examined more closely in the next section, within the
context of their secondary features. Lastly, the organi-
zation size can to some extent capture the likelihood of
an organization becoming a target of intentional attacks,
and is therefore included in the feature set.

3.1.2 Secondary Features (72)
In determining what type of statistics to extract to serve
as secondary features, we aim to capture distinct beha-
vioral patterns in an organization’s malicious activities,
particularly concerning their dynamic changes. To illu-
strate, the three examples given in Fig. 1 show drastically
different behavior: Org. 1 shows a network with consis-
tently low level of observed malicious IPs (and possibly

d
e

t
s

9k
8k
7k
6k
5k
4k
3k

i
l
 
s
P

I
 
f

 

o
#

Persistency

10

20

30
Days

40

50

60

Figure 3: Extracting secondary features. The solid red
line indicates time-average of the signal while the two
dotted lines denote the boundary of different regions.
The region above is “bad” with higher-than-average ma-
licious activities, while the region below is “good” with
lower-than-average activities. Persistency refers to the
duration the time series persist in the same region.

These observed differences motivate us to collect
statistics summarizing such behavioral patterns by mea-
suring their persistence and change, e.g., how big is the
change in the magnitude of malicious activities over time
and how frequently does it change. To balance the ex-
pressiveness of the features and their complexity, we
shall do so by ﬁrst value-quantizing a time series into
three regions relative to its time average: “good”, “nor-
mal” and “bad”. An illustration is given in Fig. 3 using
one of the examples shown earlier (Org. 3). The solid
line marks the average magnitude of the time series over
the observation period; the dotted lines then outline the
“normal” region, i.e., a range of magnitude values that
are relatively close (either from above or below) to its
time-average. The region above the top dotted line is a-
ccordingly referred to as the “bad” region, showing large
number of malicious IPs, and the region below the bot-
tom dotted line the “good” region, with a smaller number
of malicious IPs, both relative to its average1.

An additional motivation behind this quantization step
is to capture certain onset and departure of “events”, such
as a wide-area infection, or scheduled patching and soft-
ware update, etc. Viewed this way, the duration an orga-

1The choice on the size of the normal region may lead to differences
in classiﬁer performance, which is discussed in more detail in Section
5.3. In most of our experiments ±20% of the time average is used.

USENIX Association  

24th USENIX Security Symposium  1015

1

F
D
C

0.5

 

1

 

1

 

1

 

1

F
D
C

0.5

Victim org.
Non−victim org.

F
D
C

0.5

Victim org.
Non−victim org.

F
D
C

0.5

Victim org.
Non−victim org.

F
D
C

0.5

Victim org.
Non−victim org.

Victim org.
Non−victim org.

 

0
0

0.5

% Untrusted HTTPS

1

 

0
0

0.2

% openresolver

0.4

 

0
0

0.5

% DNS random port

1

 

0
0

0.1
% Open SMTP Mail Relays

0.05

 

0
0

0.5

% BGP misconfig

 

1

Figure 2: Comparison of mismanagement symptoms between the victim and non-victim populations. There is a clear
separation under the ﬁrst two, while the other three appear to be much weaker predictors.

1

F
D
C

0.5

1

F
D
C

0.5

Victim org.
Non−victim org.

1

F
D
C

0.5

1

F
D
C

0.5

Victim org.
Non−victim org.

Victim org.
Non−victim org.

Victim org.
Non−victim org.

0
0

Un−normalized "bad" magnitude

200

400

0
0
1
Normalized "good" magnitude

0.5

0
0

10
20
"Bad" duration

30

0
0

0.5

"Bad" frequency

1

Figure 4: Proﬁle of selected temporal features extracted from the scanning time series over the period Nov. 13-Dec.13.

nization spends in a “bad” region could be indicative of
the delay in responding to an event, and similarly, how
frequent it re-enters a “bad” region could be indicative
of the effectiveness of the solutions taken in remaining
clean.

Accordingly, for each region we then measure the ave-
rage magnitude (both normalized by the total number of
IPs in an organization and unnormalized), the average
duration that the time series persists in that region upon
each entry (in days), and the frequency at which the time
series enters that region. This results in four summary
statistics for each region, thus 12 values for each time se-
ries. Since each organization has three time series, one
for each malicious activity type, we obtain a total of 36
derived features per organization i. These will be collec-
tively denoted by the feature vector Fi. Note that the set
of 36 values are collected from time series of a certain
duration. Here we further distinguish between statistics
extracted from a longer period of time vs. from a shorter,
most recent period of time. In this study we use two such
feature vectors, one referred to as Recent-60 features that
are collected over a period of 60 days (typically leading
up to the time of incident) and the other Recent-14 fea-
tures collected over a period of 14 days (leading up to the
time of incident).

To give a sense of why these features may be expected
to hold predictive power, we similarly compare the dis-
tribution of these feature values among the victim and
non-victim populations. Fig. 4 shows this comparison
for four examples: un-normalized magnitude in a bad
period, normalized magnitude in a good period, average
duration during bad periods, and the frequency of enter-

ing a bad period. We see that in each case there is a clear
difference between the two populations in how these fea-
ture values are distributed, e.g., victim organizations tend
to have longer bad periods, indicative of slow response
time, and also higher bad/good magnitudes, etc. As we
discuss further in Section 4.4, these features have varying
degrees of inﬂuence over the prediction outcome.

3.2 Training and Testing Procedure
We now describe the construction of the predictor using
the set of features deﬁned above. This consists of a train-
ing step and a testing step. The training step uses the
following two sets of subjects.

A subset of incident or victim organizations. This will
be referred to as Group(1) or the incident group. De-
pending on the experiments, this subset may be selected
from one of the three incident datasets (if we train the
classiﬁer and conduct testing based solely on one inci-
dent dataset), or from the union of all three. This subset
is selected based on the time stamps of the reported in-
cidents, and its size is determined by a training-testing
ratio, e.g., 70-30 split or 50-50 split of the given dataset.
If we use a 50-50 split, it means that we select the ﬁrst
half (in terms of time of occurrence) of the incidents as
Group(1); a 70-30 split means using the ﬁrst 70% of in-
cidents as Group(1). The remaining victim organizations
are used in the testing step.

A randomly selected set of non-victim organizations
(with size comparable to that of Group(1) in any given
experiment). These are taken from the global table des-
cribed in Section 2.3.2. This will be referred to as

1016  24th USENIX Security Symposium 

USENIX Association

Group(0), or the non-incident group. As mentioned ear-
lier, since there are close to three million non-victim
organizations compared to less than a thousand victim
organizations, the random sub-sampling is necessary to
avoid the common problem of imbalance in the machine
learning literature2; this issue has also been discussed in
[51]. This random selection of non-victim organizations
is repeated numerous times, each time training a diffe-
rent classiﬁer. The reported testing results are averages
over all these versions.

i

,rPH

,rSC
i

For a victim organization i in Group(1), its complete
feature set xi includes the mismanagement symptoms mi,
the three time series rSP
over the two months
i
prior to the month in which the incident in i occurred3,
secondary features Fi collected over the same time period
as the time series, namely Recent-60, and that collected
over the two weeks prior to the month of the incident
occurrence, namely Recent-14. Each such feature set is
associated with the label (or ground-truth or group infor-
mation in machine learning) Li = 1 for incident. For a
non-victim organization j in Group(0), its complete fea-
ture set x j consists of exactly the same components listed
above, with the only difference that the time series and
the secondary features are for the two months prior to
the month of the ﬁrst incident in Group(1). It is also as-
sociated with the label L j = 0 for non-incident.

The collections of {[xi,Li]} and {[x j,L j]} constitute
the training data used to train the classiﬁer. The testing
step then uses the following two inputs: (1) The subset
of victim organizations not included in Group(1); denote
this group by Group(1c). (2) A randomly selected set of
non-victim organizations not used in training. Unlike in
training where we try to keep a balance between the vic-
tim and non-victim sets, during testing we use a much
larger set of non-victim organizations to better character-
ize the classiﬁer performance.

For these two subjects their complete feature sets
xi are obtained in exactly the same way as for those
used in training. For the non-victim organizations se-
lected for testing, the features are collected over the two
months prior to the incident month of the ﬁrst incident
in Group(1c). For the victim organization used for test-
ing we further consider two scenarios. In the short-term
forecast scenario, we collect these features over the two
months prior to the incident month for an organization in
Group(1c), while in the long-term forecast scenario, we
collect these features over the two months prior to the
incident month of the ﬁrst incident in Group(1c). In the

2If we use all three million non-victims in training, the resulting
classiﬁer will simply label all of them as non-victims, and achieve per-
formance very close to 100% overall. But clearly this classiﬁer would
be of little use, as it will also have 0 true positive probability.

3Most incident occurrences in our dataset are timestamped with

month and year information.

short-term forecast scenario, since in each incident test
case the incident occurred within a month of collecting
the features, the TP rate is essentially for a forecasting
window of one month.
In the long-term forecast sce-
nario, an incident may occur months after collecting the
features (up to 12 months in the case of VCDB), thus the
TP rate is for a forecasting window of up to a year. Note
that the short-term forecast can be repeatedly done over
time to produce prediction for the immediate future. The
differences between these two forecast schemes are also
illustrated in Fig. 5.

Recent−60 features (60 days) :
Recent−14 features (14 days) :

Training

Timeline

USI Affinity

Nationalist Move. web

Funding site of Kickstarters 
attacked

Short Term Prediction

Long Term Prediction

MOE web hacked

Web of Russian state
TV attacked

MOE web hacked

Web of Russian state
TV attacked

Aug. 13 Sep. 13 Oct. 13 Nov. 13 Dec. 13 Jan. 14 Feb. 14 Mar. 14

Figure 5: Feature extraction, short-term and long-term
forecasting. In training, features are extracted from the
most recent period leading up to an incident. In testing,
the same is done when we perform short-term forecast.
In long-term forecast, features are extracted from periods
leading up to the time of the ﬁrst incident used in testing.

These inputs are then fed into the classiﬁer to produce
a label (or prediction). The output of Random Forest is
actually a risk probability; a threshold is then imposed to
obtain a binary label. For instance, if we set the thresh-
old at 0.5, then all output > 0.5 means a label of 1. By
moving this threshold we obtain different prediction per-
formances, which constitute a ROC curve.

4

Incident Prediction

In this section, we present our main prediction results
and investigate their various implications.

4.1 Main Results
Using the methodology outlined in the previous section,
we performed prediction using the three incident datasets
separately, as well as collectively. When used collec-
tively, we removed duplicate reports of the same inci-
dent whenever applicable. The separation between train-
ing and testing for each dataset is done chronologically,
as shown in Table 4. For each dataset, these separations

USENIX Association  

24th USENIX Security Symposium  1017

result in an approximate 50-50 split of the victim set be-
tween the training and testing sample sizes. In addition,
for each test we randomly sample non-victim test cases
from the non-victim organization set.

Training
Testing

Hackmageddon
Oct 13 – Dec 13
Jan 14 – Feb 14

VCDB

WHID

Aug 13 – Dec 13
Jan 14 – Dec 14

Jan 14 – Mar 14
Apr 14 – Nov 14

Table 4: Chronological separation between training and
testing samples for each incident dataset;
the split is
roughly 50-50 among the victim population.

There is one point worth clarifying. When processing
non-sequential data, the split of samples for the purpose
of training and testing is often done randomly in the ma-
chine learning literature. In our context this would mean
to choose a later incident for training and use an earlier
incident for testing. Due to the sequential nature of our
data, we intentionally and strictly split the data by time:
earlier ones are for training and later ones for testing.
Because of this, our testing results are indeed “predic-
tion” results; for the same reason, we did not set aside a
third, separate dataset for the purpose of “more testing”
as is sometimes done in the literature, as this purpose is
already served by the second, test dataset.

The prediction results are summarized in the set of
ROC (receiver operating characteristic) curves shown in
Fig. 6. Recall that the RF classiﬁer outputs a probabi-
lity of incident for each input sample. To test its accu-
racy, a threshold is adopted that maps this value into a
binary prediction: 1 if it exceeds the threshold and 0 oth-
erwise. This binary prediction is then compared against
the ground-truth: a sample from an incident dataset has
a true label of 1, while a sample from the non-victim or-
ganization set has a true label of 0. Since our non-victim
set for training (to balance) is randomly selected from the
total non-victim population, the above test is repeated 20
times for a given threshold value, each time for a differ-
ent random non-victim set. The average TP and FP over
these repeated tests form one point on the ROC curve.

We see the prediction performance varies slightly be-
tween the datasets, but remain very satisfactory, gene-
rally achieving combined (TP, FP) values of (90%,10%)
or (80%,5%). In particular, when we combine the three
datasets, we can achieve an accuracy level of (88%,4%).
A summary of some of the most desirable operating
points are given in Table 5.

The above prediction results substantially outperform
what has been shown in the literature to date; e.g., the
web maliciousness prediction study in [51] reported a
combination of (66%,17%) for (TP, FP). It is also worth
pointing out that TP and FP values are independent of the
sizes of the respective populations of the victim and non-

e
v
i
t
i
s
o
p

 

1
0.9
0.8
0.7
0.6
0.5
0.4

e
u
r
T

VCDB
Hackmageddon
WHID
ALL

0.1

0.2
0.3
False positive

0.4

0.5

Figure 6: Prediction results. There are variations be-
tween the datasets, but an operating point – combined
(TP, FP) values – of (90%,10%) or (80%,5%) is achiev-
able.
In particular, when we use all three datasets to-
gether, we can achieve an accuracy level of (88%,4%).

Accuracy

Hackmageddon

True Positive (TP)
False Positive (FP)
False Negative (FN)
Overall Accuracy

96%
10%
4%
90%

VCDB WHID
80%
88%
5%
10%
20%
12%
90%
95%

All
90%
10%
10%
90%

Table 5: Best operating points of the classiﬁer for the
best combinations of (TP, FP) values.

victim organizations (these are conditional probability
estimates), whereas the overall accuracy does depend on
the two population sizes as it is the unconditioned prob-
ability of making correct predictions. Since based on our
dataset we have a minuscule victim population (account-
ing for (cid:31) 1% of the overall population), the overall ac-
curacy is simply ∼ (1-FP). Therefore, if the overall ac-
curacy is of interest, the best classiﬁer would be a naive
one that simply labels all inputs as “0”. This would lead
to 0% TP, 0% FP, and an overall accuracy of > 99%.
However, despite achieving maximum overall accuracy,
such a classiﬁer is clearly useless. This point is also em-
phasized in [51] for similar reasons. Additionally, in the
context of forecasting, where the goal is to facilitate pre-
ventative measures at an organizational level, having a
high TP is perhaps more relevant than having a low FP;
this is in contrast to spam detection, where the cost of FP
is much higher than a missed detection. Therefore, the
three measures in Table 5 should be taken as a whole.

Impact of Training:Testing Ratio

4.2
The results in Fig. 6 are obtained under a 50-50 split of
the victim set into training and testing samples, based on
the incident time. Furthermore, they are obtained using
the short-term forecasting method described in Section
3.2. In general, one can improve the prediction perfor-
mance by increasing the training sample size. There is

1018  24th USENIX Security Symposium 

USENIX Association

no exception in our study, as shown in Fig. 7 where we
compare results from a 70-30 training and testing sam-
ple split of the victim set to that from the 50-50 split, for
the VCDB data. A best operating point is now around
(94%,10%), indicating a clear improvement. Note that,
a 70-30 split is not generally regarded high in the ma-
chine learning literatures, see e.g., in [57] a 90-10 split
was used. We however believe a 50-50 split gives a more
objective measure of the prediction performance.

e
v
i
t
i
s
o
p

1
0.9
0.8
0.7
0.6
0.5
0.4

 

e
u
r
T

VCDB: 50−50 & Short
VCDB: 70−30 & Short
VCDB: 50−50 & Long

0.1

0.2
0.3
False positive

0.4

0.5

Figure 7: The impact of larger training set and size
of forecasting window; all results are obtained using
VCDB. The three curves: (1) using a 50-50 split of the
victim set between training and testing under the short-
term forecasting scenario (this curve is identical to the
one in Fig 6); (2) using a 70-30 split of the victim set
between training and testing under the short-term fore-
casting scenario; (3) using a 50-50 split of the victim set
between training and testing under the long-term fore-
casting scenario.

4.3 Short-term vs. Long-term Forecast
Also shown in Fig. 7 are our long-term forecasting re-
sults under a 50-50 training and testing sample split of
the victim set, again for VCDB. As seen, the predic-
tion performance holds even when we move from a one-
month to a 12-month forecasting window. The use of
mismanagement symptoms and long-term malicious be-
haviors in the features contributes to this: they generally
remain stable over time and have relatively high impor-
tance in the prediction, discussed in greater detail in the
next section.

4.4 Relative Importance of the Features
In addition to the prediction output, the RF classiﬁer also
outputs a normalized relevance score for each feature
used in training [17]; the higher the value, the more im-
portant the feature in the prediction. In this section, we
examine these scores more closely. This study will fur-
ther help us understand the extent to which different fea-

tures determine the chance of an organization becoming
breached in the near future. For brevity, the experiments
presented in this section are based on a combination of
all three datasets.

The importance of each category of features is sum-
marized in Table 6. We make a number of interesting ob-

Feature category
Mismanagement
Time series data
Recent-60 secondary features
Organization size
Recent-14 secondary features

Normalized importance

0.3229
0.2994
0.2602
0.0976
0.02

Table 6: Feature importance by category. The misman-
agement features are the most important category in pre-
diction. Secondly, the Recent-60 secondary features are
almost as important as the time series data; the former
capture dynamic behavior over time within an organiza-
tion whereas the latter capture synchronized behavior be-
tween malicious activities of different organizations.

servations. First, note that the mismanagement features
stand out as the most important category in prediction.
Second, the Recent-60 secondary features are almost as
important as the time series data, despite the fact that the
former are derived from the latter. This is because the use
of time series data has the effect of capturing synchro-
nized behavior between malicious activities of different
organizations, while the secondary features are aimed at
capturing the dynamic behavior over time within an orga-
nization itself. That the latter adds value to the predictor
is thus validated by the above importance comparison.
Last but not least, the Recent-60 features appear much
more important than Recent-14 features.

A closer look into each category reveals that among
the mismanagement features, untrusted HTTPS is by
far the most important (0.1531), followed by Openre-
solver (0.0928), DNS random port (0.0469), Mail relay
(0.0169), and BGP misconﬁg. (0.0132). The more sig-
niﬁcant role of untrusted HTTPS in prediction as com-
pared to Openresolver is consistent with the bigger dif-
ference in distributions seen earlier in Fig. 2; that is,
a victim organization tends to have a higher percentage
of mis-conﬁgured HTTPS for their network assets. A
possible explanation is that a majority of the incidents in
our dataset are web-page breaches; these correlate with
the untrusted HTTPS symptom, which reﬂect poorly ma-
naged web systems.

Similarly, a closer look at the secondary features (both
Recent-60 and Recent-14) suggests that the dynamic fea-
tures (duration and frequency together, totaling 0.1769)
are far more important than static features (magnitude,
totaling 0.0834). This suggests that dynamic changes
over time, or in other words, organizations’ response

USENIX Association  

24th USENIX Security Symposium  1019

time in terms of cleaning up the origin of their malicious
activities, is more indicative of security risks.

4.5 The Power of Dataset Diversity
A question that naturally arises is what if only a sin-
gle feature category is used to train the classiﬁer. For
instance, given the prominent score of mismanagement
features in prediction, would it be sufﬁcient to only use
these in prediction? The answer, as shown in Fig. 8,
turns out to be negative. In this ﬁgure, we compare the
prediction performance by using the following four cat-
egories of features separately to build the classiﬁer: mis-
management, time series data, organization size, and the
entire set of secondary features. While it is expected

e
v
i
t
i
s
o
p

1
0.9
0.8
0.7
0.6
0.5
0.4

 

e
u
r
T

Mismanagement
Malicious acitivity time series
Organization size
Secondary features
All

0.1

0.2
0.3
False positive

0.4

0.5

Figure 8:
Independent prediction performance using
only one set of features. The secondary features are
shown to be the most powerful in prediction when used
alone. Mismanagement features perform the worst, even
though they have the highest importance factor. This is
because the factors reﬂect conditional importance, given
the presence of other features. This means that misma-
nagement features alone are poor predictors but they add
valuable information to the other features.

that using only one feature set leads to worse predic-
tion performance, it is somewhat surprising that the sec-
ondary features are more powerful than mismanagement
features or the time series when used separately. Recall
that the secondary features were designed speciﬁcally to
capture the organizational behavior, including their re-
sponsiveness and effectiveness in dealing with malicious
acti-vities. One explanation of this result is that the hu-
man and process element of an organization is the most
slow-changing compared to the change in the threat land-
scape, and thus holds the most predictive power.

Note that this is not inconsistent with the relative im-
portance given in Table 6, as the latter is a measure of
conditional importance of one feature given the presence
of other features. In other words, the relative importance
suggests how much we lose in performance if we leave
out a feature, whereas Fig. 8 shows how well we do when

using only that feature. What’s seen here is that misman-
agement features add very signiﬁcant (orthogonal) infor-
mation to the other features, but they are poor predictors
in and by themselves. Perhaps most importantly, the re-
sults in Fig. 8 validate the idea of using a diverse set of
measurement data that collectively form predictive des-
criptions of an organization’s security risks.

4.6 Comparison with SVM
As a reference, we also trained classiﬁers using SVM;
the prediction results are much poorer compared to using
RF. For instance, using the VCDB data, the best operat-
ing point under SVM (with a 50-50 training-testing split
of the victim population and short-term forecasting) is
around (70%, 25%). This observation is consistent with
existing literature, see e.g., [57].

5 Discussion

5.1 Top Data Breaches of 2014
In Fig. 9, we plot the distribution (CDF) of the predic-
tor output values for the VCDB victim set and a ran-
domly selected non-victim set used in testing. We use
an example threshold of 0.85 for illustration. All points
to the right of a threshold is labeled “1”, indicating
positive prediction, and all to its left “0”. Three inci-
dent examples are also shown, falling into the categories
of true-positive (ACME), false-positive (AXTEL), and
false-negative (BJP Junagadh).

Also highlighted in Fig.

9 are the top ﬁve data
breaches of 2014 [43], namely JP Morgan Chase, Sony
pictures, Ebay, Home Depot, and Target. Using the
suggested threshold value, our prediction method would
have correctly labeled four of these incidents, and only
narrowly missed the Target incident. It is worth noting
that the Target incident was brought on by one of its con-
tractors; however, the fact that Target did not have a more
secure vendor policy in place is indicative of something
else amiss (e.g., lack of consistent procedure between IT
and procurement) that could also have manifested itself
in the data and features we examined.

These examples highlight that, in addition to enabling
proactive measures by an organization, there are poten-
tial business uses of the prediction method presented in
this study. The ﬁrst is in vendor or third party evalua-
tion. Consider Online Tech, the hosting service used by
JP Morgan Chase, as an example. As shown in Fig. 9,
Online Tech posed very high security risks; this infor-
mation could have been used in determining whether to
use this vendor. Furthermore, information provided by
our prediction method can help underwriters better cus-
tomize terms of a cyber-insurance policy. The insurance

1020  24th USENIX Security Symposium 

USENIX Association

F
D
C

1
0.8
0.6
0.4
0.2
0
0.4

Randomly selected non−victim set
VCDB victim set

Threshold 0.85

ACME 0.85

Homedepot 0.85

AXTEL 0.87

Sony picture 0.90
Ebay 0.88

OnlineTech 0.92

Target 0.84

BJP

Junagadh 0.77

0.5

0.6

0.7

Predictor output

0.8

Threshold 0.85

0.9

1

Figure 9: Distribution of predictor outputs with an example threshold value 0.85 (with 91% TP, 10% FP). On the curve
with circles (non-victim) to the right of the threshold are FPs; on the curve with squares (victim) to the right of the
threshold are TPs. Three types of incidents are shown, presenting true-positive (ACME), false-positive (AXTEL), and
false-negative (BJP Junagadh). Also highlighted are top data breach events in 2014.

payout in the case of Target was reported to be around
$60M; with this much at stake, it is highly desirable to
be able to accurately predict the risk of an insured and
adjust terms of the contracts.

5.2 Prediction by Incident Type

For a majority of the incident types, we do not have
enough sample points to serve both training and testing
purposes, except for the 368 reports of the type “web ap-
plications incident” in VCDB. This allows us to train a
classiﬁer to predict the probability of an organization be-
ing hit with a “web app incident”. The corresponding
results are similar in accuracy to those obtained earlier
(e.g., at (92%, 11%)). This suggests that our methodo-
logy has the potential to make more precise predictions
as we accumulate more incident data.

Similarly, the current forecasting methodology is not
aimed at predicting highly targeted attacks motivated by
geo-political reasons (e.g., the Sony Picture breach). Nor
does it use explicit business sector information (e.g., a
bank may be a bigger target than a public library sys-
tem). In this sense, our current results represent more the
likelihood of an organization falling victim provided it is
being targeted. However, an ever increasing swath of the
Internet is rapidly under cyber threats to the point that
all major organizations should simply assume that they
are someone’s target. The use of explicit business sec-
tor information does allow us to make more ﬁne-grained
predictions.
In a more recent study [48], we leverage
a broad array of publicly available business details on
victim organizations reported in VCDB, including busi-
ness sector, employee count, region of operation and web
statistics information from Alexa Web Information Ser-
vice (AWIS), to generate risk proﬁles, the conditional
probabilities of an organization suffering from speciﬁc
types of incident, given that an incident occurs.

5.3 Robustness against adversarial data

manipulation and other design choices
One design choice we made in the feature extraction pro-
cess is the parameter δ which determines how a time se-
ries is quantized to obtain secondary features. Below we
summarize the impact of of having different δ values. In
the results shown so far, a value of δ = 0.2 is used. In
Fig. 10 we test the cases with δ = 0.1 and δ = 0.3. We
see that this parameter choice has relatively minor effect:
with δ = 0.3 a desirable TP/FP combination is around
(91%,9%), and for δ = 0.1, we have (86%,6%). It ap-
pears that having a higher value of δ leads to slightly
better performance; a possible explanation is that quan-
tizing using δ = 0.2 retained more noise and ﬂuctuation
in the time series, while quantizing using δ = 0.3 may be
more consistent with the actual onset of events.

e
v
i
t
i
s
o
p

 

e
u
r
T

1

0.9

0.8

0.7

0.6

0.5

0.4

 

 

VCDB: δ = 0.3
VCDB: δ = 0.2
VCDB: δ = 0.1

0.1

0.2
0.3
False positive

0.4

0.5

Figure 10: Experiment results under different δ.

Throughout the paper we have assumed the data are
truthfully reported (though with noise/error). It is thus
reasonable to question how robust
is the prediction
against possible (malicious) manipulation of the data
used for training, a subject of increasing interest and
commonly referred to as adversarial machine learning.
For instance, an entity may attempt to set up fake net-
works with clean data (no malicious activities) but with
fake reported incidents, and vice versa, to mislead the
classiﬁer. Without presenting a complete solution, which
remains a direction of future research, below we test the

USENIX Association  

24th USENIX Security Symposium  1021

robustness of our current prediction technique using two
scenarios: (1) In the ﬁrst we randomly ﬂip the labels of
victim organizations from 1 to 0; those ﬂipped to 0 are
now part of the non-victim group, thus contaminating the
training data. (2) In the second scenario we do the op-
posite: randomly ﬂip the labels of non-victim organiza-
tions, effectively adding them to the victim group. Inci-
dentally, the former scenario is akin to under-reporting
by randomly selected organizations.

Experimental results suggest no performance differ-
ence for case (1). The reason lies in the imbalance be-
tween the victim and non-victim population sizes. Re-
call that because of this, in our experiment we randomly
select a subset of non-victim organizations with size
comparable to the victim organizations (on the order of
O(1,000)). Then in each training instance, the expected
number of victims selected as part of the non-victim set
is no more than Nv · O(1,000)/N, with Nv denoting the
number of fake non-victims and N the total number of
non-victims. Since N ∼ O(1,000,000), even if one is
able to inject Nv ∼ O(100) victims into the non-victim
population, on average no more than one fake non-victim
will actually be selected for training, resulting in negli-
gible contamination effect unless such alterations can be
done on a scale larger than the actual victim population.
For case (2), we indeed observe performance degrada-
tion, albeit slight, in the true positive, as shown in Fig.
11 at the 20% contamination level (20% of non-victim
organization labels are ﬂipped).

e
v
i
t
i
s
o
p

 

e
u
r
T

1

0.9

0.8

0.7

0.6

0.5

0.4

 

 

VCDB
Adversarial

0.1

0.2
0.3
False positive

0.4

0.5

Figure 11: Adversarial case (2) with 20% contamination.

5.4

Incident Reporting

One of the main obstacles in studies of this nature is the
acquisition of high quality incident data, without which
we can neither train nor verify with conﬁdence. Our re-
sult here demonstrates that machine learning techniques
have the power to make accurate incident forecasts, but
data collection is lagging by comparison. The research
community would beneﬁt enormously from more sys-
tematic and uniform incident reporting.

6 Related Work

As mentioned in the introduction, a large part of the lit-
erature focuses on detection rather than prediction. The
work in [44] is one such example. Among others, Lee et
al. [36] built sophisticated Hidden Markov Model tech-
niques to detect spam deobfuscation, and in [57] Wang
et al. applied (adversarial) machine learning techniques
to the detection of malicious accounts on Weibo.

Relatively fewer studies have focused on prediction;
even fewer are on the type of prediction presented in
this paper where the predicted variable (classiﬁer out-
put) is of a different type from the input variables (fea-
ture input). For instance, the predictive IP-based black-
list works in [50, 30] have the same input and output
variables (content of the blacklist). Similarly, in [54]
the evolution of spam of a certain preﬁx is predicted
using past spam activities as input. Predictive studies
similar to ours include the aforementioned [51] that pre-
dicts whether a website will turn malicious by using tex-
tual and structural analysis of a webpage. The perfor-
mance comparison has been given earlier.
It is worth
pointing out that the intended applications are also differ-
ent: whereas webpage maliciousness prediction can help
point to websites needing improvement or maintenance,
our prediction on the organizational level can help point
to networks facing heightened probability of a broader
class of security problems. Also as mentioned earlier,
our study [48] examines the prediction of incident types,
conditional on an incident occurring, by using an array
of industry, business and web visibility/population infor-
mation. Other predictive studies include [28], where it
is shown that by analyzing user browsing behavior one
can predict whether a user will encounter a malicious
page (attaining a 87% accuracy), [52], where risk fac-
tors are identiﬁed at the organization level (industry sec-
tor and number of employees) and the individual level
(job type, location) that are positively or negatively cor-
related with experiencing spear phishing targeted attacks,
and [53], where risk factors for web server compromise
are identiﬁed through analyzing features from sampled
web servers.

Also related are studies on reputation systems and pro-
ﬁling of networks. These include e.g., [26], a reputation
assigning system trained using DNS features, reputation
systems [8, 9] based on monitoring Internet trafﬁc data,
and those studied in [29, 46].

7 Conclusion

In this study, we characterize the extent to which cyber
security incidences can be predicted based on externally
observable properties of an organization’s network. Our
method is based on 258 externally measurable features

1022  24th USENIX Security Symposium 

USENIX Association

collected from a network’s mismanagement symptoms
and malicious activity time series. Using these to train a
Random Forest classiﬁer, it is shown that we can achieve
fairly high accuracy, such as a combination of 90% true
positive rate and 10% false positive rate. We further an-
alyzed the relative importance of the features sets in the
prediction performance, and showed our prediction out-
come for the top data breaches in 2014.

Acknowledgement

This work is partially supported by the NSF under grant
CNS 1422211, CNS 1409758, CNS 1111699 and by the
DHS under contract number HSHQDC-13-C-B0015.

References

[1] AFRINIC whois database. http://www.afrinic.net/

services/whois-query.
[2] APNIC whois database.

apnic-bin/whois.pl.

http://wq.apnic.net/

[3] ARIN whois database.

https://www.arin.net/

resources/request/bulkwhois.html.

[4] Composite Blocking List. http://cbl.abuseat.org/.
[5] DShield. http://www.dshield.org/.
[6] Evernote resets passwords after major security breach.

http://www.digitalspy.co.uk/tech/news/
a462959/evernote-resets-passwords-after-
major-security-breach.html.

[7] Global Reputation System. http://grs.eecs.umich.

edu//.

[8] Global Security Reports. http://globalsecuritymap.

com/.

[9] Global Spamming Rank. http://www.spamrankings.

net/.

[10] Google Kenya and Google Burundi hacked by 1337.
http://thehackersmedia.blogspot.com/2013/09/
google-kenya-google-burundi-hacked-by.html.

[11] hpHosts for your pretection. http://hosts-file.net/.
[12] LACNIC whois database. http://lacnic.net/cgi-

bin/lacnic/whois.

[13] Multiple DNS implementations vulnerable to cache poi-
soning. http://www.kb.cert.org/vuls/id/800113.
http://

[14] Open

Project.
openresolverproject.org/.

Resolver

[15] OpenBL. http://www.openbl.org/.
[16] PhishTank. http://www.phishtank.com/.
[17] Rf

classiﬁer.

http://scikit-learn.org/

stable/modules/generated/sklearn.ensemble.
RandomForestClassifier.html.

[18] RIPE whois database. https://apps.db.ripe.net/

search/query.html.

[19] SpamCop Blocking List. http://www.spamcop.net/.
[20] SURBL: URL REPUTATION DATA.

http://www.

surbl.org/.

[21] Syrian hacker Dr.SHA6H hacks and defaces City of
http://hackread.com/

Mansﬁeld, OH website.
syrian-hacker-dr-sha6h-hacks-and-defaces-
city-of-mansfield-oh-website-for-free-syria.
[22] The SPAMHAUS project: SBL, XBL, PBL, ZEN Lists.

http://www.spamhaus.org/.
Network.

[23] UCEPROTECTOR
uceprotect.net/.

http://www.

[24] WPBL: Weighted Private Block List.

http://www.

wpbl.info/.

[25] AGRAWAL,

T.,

D.,

AND

HENRY,

FINKLE,
JPMorgan hack exposed data of 83 mil-
http:

J.
lion, among biggest breaches in history.
//www.reuters.com/article/2014/10/03/us-
jpmorgan-cybersecurity-idUSKCN0HR23T20141003,
October 2014.

[26] ANTONAKAKIS, M., PERDISCI, R., DAGON, D., LEE,
W., AND FEAMSTER, N. Building a Dynamic Reputation
System for DNS.
In Proceedings of the 19th USENIX
Security Symposium (Berkeley, CA, USA, August 2010).
[27] BISHOP, C. M., ET AL. Pattern Recognition and Ma-

chine Learning, vol. 1. Springer New York.

[28] CANALI, D., BILGE, L., AND BALZAROTTI, D. On the
Effectiveness of Risk Prediction Based on Users Brows-
ing Behavior.
In ASIA CCS ’14 (New York, NY, USA,
June 2014), ACM, pp. 171–182.

[29] CHANG, J., VENKATASUBRAMANIAN, K. K., WEST,
A. G., KANNAN, S., LEE, I., LOO, B. T., AND SOKOL-
SKY, O. AS-CRED: Reputation and Alert Service for
Interdomain Routing. vol. 7, pp. 396–409.

[30] COLLINS, M. P., SHIMEALL, T. J., FABER, S., JANIES,
J., WEAVER, R., DE SHON, M., AND KADANE, J. Us-
ing Uncleanliness to Predict Future Botnet Addresses. In
Proceedings of ACM IMC (San Diego, California, USA,
October 2007), pp. 93–104.

[31] CONSORTIUM, T. W. A. S. Web-Hacking-Incident-
Database.
http://projects.webappsec.org/w/
page/13246995/Web-Hacking-Incident-Database.
[32] DURUMERIC, Z., KASTEN, J., BAILEY, M., AND HAL-
DERMAN, J. A. Analysis of the HTTPS Certiﬁcate
Ecosystem.
In Proceedings of ACM IMC (Barcelona,
Spain, October 2013), pp. 291–304.

[33] DURUMERIC, Z., WUSTROW, E., AND HALDERMAN,
J. A. ZMap: Fast Internet-Wide Scanning and its Secu-
rity Applications.
In Proceedings of the 22nd USENIX
Security Symposium (Washington, D.C., August 2013),
pp. 605–620.

[34] HUBERT, A., AND MOOK, R. V. Measures for Making
DNS More Resilient against Forged Answers. RFC 5452,
January 2009.

USENIX Association  

24th USENIX Security Symposium  1023

[35] KREBS, B.

The target breach, by the num-
bers. http://krebsonsecurity.com/2014/05/the-
target-breach-by-the-numbers/, May 2014.

[50] SOLDO, F., A., L., AND MARKOPOULOU, A. Predictive
Blacklisting as an Implicit Recommendation System. In
INFOCOM, IEEE (March 2010), pp. 1–9.

[36] LEE, H., AND NG, A. Y. Spam Deobfuscation using a
Hidden Markov Model. In In Conference on Email and
Anti-Spam (July 2005).

[37] LIAW, A., AND WIENER, M. Classiﬁcation and Regres-
sion by randomForest. http://CRAN.R-project.org/
doc/Rnews/, 2002.

[38] LINDBERG, G. Anti-Spam recommendations for SMTP

MTAs. BCP 30/RFC 2505, 1999.

[39] MA, J., SAUL, L. K., SAVAGE, S., AND VOELKER,
G. M. Beyond Blacklists: Learning to Detect Malicious
Web Sites from Suspicious URLs. In Proceedings of the
15th ACM SIGKDD International Conference on Knowl-
edge Discovery and Data Mining (New York, NY, USA,
June 2009), KDD ’09, ACM, pp. 1245–1254.

[40] MAHAJAN, R., WETHERALL, D., AND ANDERSON, T.
Understanding BGP misconﬁguration. In Proceedings of
SIGCOMM ’02 (August 2002), vol. 32, ACM, pp. 3–16.
[41] OF OREGON, U. Route Views Project. http://www.

routeviews.org/.

[42] PASSERI,

P.

Hackmageddon.com.

http:

//hackmageddon.com/.

[43] PRINCE, B. Top data breaches of 2014.

http://
www.securityweek.com/top-data-breaches-2014,
December 2014.

[44] QIAN, Z., MAO, Z. M., XIE, Y., AND YU, F. On
Network-level Clusters for Spam Detection. In Proceed-
ings of the Network and Distributed System Security Sym-
posium (NDSS ’14) (San Diego, CA, March 2010).

[45] RAMACHANDRAN, A., AND FEAMSTER, N. Under-
In
standing the Network-level Behavior of Spammers.
Proceedings of SIGCOMM ’06 (August 2006), vol. 36,
ACM, pp. 291–302.

[46] RESNICK, P., KUWABARA, K., ZECKHAUSER, R., AND
FRIEDMAN, E. Reputation Systems. Commun. ACM 43,
12 (December 2000), 45–48.

[47] ROMANOSKY,

S.

Comments
cybersecurity

on
practices

incentives
noi.

adopt

improved

to
http://www.ntia.doc.gov/federal-register-
notice/2013/comments-incentives-adopt-
improved-cybersecurity-practices-noi,
2013.

April

[48] SARABI, A., NAGHIZADEH, P., LIU, Y., AND LIU, M.
Prioritizing Security Spending: A Quantitative Analysis
of Risk Distributions for Different Business Proﬁles. In
the Annual Workshop on the Economics of Information
Security (WEIS) (June 2015).

[49] SIDEL, R. Home depot’s 56 million card breach bigger
than target’s. http://www.wsj.com/articles/home-
depot-breach-bigger-than-targets-1411073571,
September 2014.

[51] SOSKA, K., AND CHRISTIN, N. Automatically Detect-
ing Vulnerable Websites Before They Turn Malicious.
In Proceedings of the 23rd USENIX Security Symposium
(San Diego, CA, August 2014).

[52] THONNARD, O., BILGE, L., KASHYAP, A., AND LEE,
M. Are You at Risk? Proﬁling Organizations and Indi-
viduals Subject to Targeted Attacks. In Financial Cryp-
tography and Data Security (January 2015).

[53] VASEK, M., AND MOORE, T. Identifying Risk Factors
for Webserver Compromise. In Financial Cryptography
and Data Security. Springer, March 2014, pp. 326–345.
[54] VENKATARAMAN, S., BRUMLEY, D., SEN, S., AND
SPATSCHECK, O. Automatically Inferring the Evolution
of Malicious Activity on the Internet. In Proceedings of
the Network and Distributed System Security Symposium
(NDSS ’14) (San Diego, CA, February 2013).

[55] VERIS. VERIS Community Database (VCDB). http:

//veriscommunity.net/index.html.

[56] VERIZON. Data Breach Investigations Reports (DBIR)

2014. http://www.verizonenterprise.com/DBIR/.

[57] WANG, G., WANG, T., ZHENG, H., AND ZHAO, B. Y.
Man vs. Machine: Practical Adversarial Detection of Ma-
licious Crowdsourcing Workers.
In Proceedings of the
23rd USENIX Security Symposium (San Diego, CA, Au-
gust 2014), pp. 239–254.

[58] ZHANG, J., DURUMERIC, Z., BAILEY, M., KARIR, M.,
AND LIU, M. On the Mismanagement and Malicious-
ness of Networks.
In Proceedings of the Network and
Distributed System Security Symposium (NDSS ’14) (San
Diego, CA, February 2014).

APPENDIX

Incident Dataset
A snapshot of sample incident reports from VCDB
dataset (Table 7).

Incident type
Web site defacement May 2014

Time

Hacking

Apr. 2014

Web site defacement
Server breach

N/A 2013
N/A 2013

Web site hacked
Private key stolen
Phishing

May 2013
Mar. 2014
N/A 2013

Report summary
”ybs-bank.com” a Malaysian
imitation of the real Yorkshire Bank website
4chan hacked by person targeting information
about users posting habits.
AR Argentina Military website hacked.
The systems of AdNet Telecom, a major
Romania-based telecommunications services
provider, have been breached.
Albany International Airport website hacked.
Amazon Web Services, Inc.
Bolivian tourist site was compromised and
a fraudulent secret shopper site was installed.

Table 7: Incidents from the VCDB Community Database

1024  24th USENIX Security Symposium 

USENIX Association

