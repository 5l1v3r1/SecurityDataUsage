A Placement Vulnerability Study in  

Multi-Tenant Public Clouds

Venkatanathan Varadarajan, University of Wisconsin—Madison; Yinqian Zhang,  

The Ohio State University; Thomas Ristenpart, Cornell Tech; Michael Swift,  

University of Wisconsin—Madison

https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/varadarajan

This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-931971-232Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXA Placement Vulnerability Study in Multi-Tenant Public Clouds

Venkatanathan Varadarajan†, Yinqian Zhang‡, Thomas Ristenpart∗, and Michael Swift†

†University of Wisconsin-Madison, ‡The Ohio State University, ∗Cornell Tech,

{venkatv,swift}@cs.wisc.edu, yinqian@cse.ohio-state.edu, ristenpart@cornell.edu

Abstract

Public infrastructure-as-a-service clouds, such as Amazon
EC2, Google Compute Engine (GCE) and Microsoft Azure
allow clients to run virtual machines (VMs) on shared phys-
ical infrastructure. This practice of multi-tenancy brings
economies of scale, but also introduces the risk of sharing a
physical server with an arbitrary and potentially malicious
VM. Past works have demonstrated how to place a VM
alongside a target victim (co-location) in early-generation
clouds and how to extract secret information via side-
channels. Although there have been numerous works on
side-channel attacks, there have been no studies on place-
ment vulnerabilities in public clouds since the adoption
of stronger isolation technologies such as Virtual Private
Clouds (VPCs).

We investigate this problem of placement vulnerabili-
ties and quantitatively evaluate three popular public clouds
for their susceptibility to co-location attacks. We ﬁnd that
adoption of new technologies (e.g., VPC) makes many prior
attacks, such as cloud cartography, ineffective. We ﬁnd new
ways to reliably test for co-location across Amazon EC2,
Google GCE, and Microsoft Azure. We also found ways to
detect co-location with victim web servers in multi-tiered
located behind a load balancer.

We use our new co-residence tests and multiple customer
accounts to launch VM instances under different strategies
that seek to maximize the likelihood of co-residency.
We ﬁnd that it is much easier (10x higher success rate)
and cheaper (up to $114 less) to achieve co-location in
these three clouds when compared to a secure reference
placement policy.

Keywords: co-location detection, multi-tenancy, cloud se-
curity

1

Introduction

Public cloud computing offers easy access to relatively
cheap compute and storage resources. Cloud providers are

∗Work primarily done while at the University of Wisconsin-Madison.

able to sustain this cost-effective solution through multi-
tenancy, where the infrastructure is shared between com-
putations run by arbitrary customers over the Internet. This
increases utilization compared to dedicated infrastructure,
allowing lower prices.

However, this practice of multi-tenancy also enables var-
ious security attacks in the public cloud. Should an ad-
versary be able to launch a virtual machine on the same
physical host as a victim, making the two VMs co-resident
(sometimes the term co-located is used), there exist attacks
that break the logical isolation provided by virtualization to
breach conﬁdentiality [29, 32, 33, 35, 37, 38] or degrade the
performance [30, 39] of the victim. Perhaps most notable
are the side-channel attacks that steal private keys across
the virtual-machine isolation boundary by cleverly moni-
toring shared resource usage [35, 37, 38].

Less understand is the ability of adversaries to arrange
for co-residency in the ﬁrst place.
In general, doing so
consists of using a launch strategy together with a mech-
anism for co-residency detection. The only prior work
on obtaining co-residency [29] showed simple network-
topology-based co-residency checks along with low-cost
launch strategies that obtain a high probability of achieving
co-residency compared to simply launching as many VM
instances as possible. When such advantageous strategies
exist, we say the cloud suffers from a placement vulner-
ability. Since then, Amazon has made several changes to
their architecture, including removing the ability to do the
simplest co-residency check. Whether placement vulnera-
bilities exist in other public clouds has, to the best of our
knowledge, never been explored.

In this work, we provide a framework to systematically
evaluate public clouds for placement vulnerabilities and
show that three popular public cloud providers may be vul-
nerable to co-location attacks. More speciﬁcally, we set out
to answer four questions:
• Can co-residency be effectively detected in modern
• Are known launch strategies [29] still effective in

public clouds?

modern clouds?

USENIX Association  

24th USENIX Security Symposium  913

ties?

• Are there any new exploitable placement vulnerabili-
• Can we quantify the money and time required of an
adversary to achieve a certain probability of success?
We start by exploring the efﬁcacy of prior co-residency
tests (§ 4) and develop more reliable tests for our place-
ment study (§ 4.1). We also ﬁnd a novel test to detect co-
residency with VMs uncontrolled by the attacker by just us-
ing their public interface even when they are behind a load
balancer (§ 4.3).
We use multiple customer accounts across three pop-
ular cloud providers, launch VM instances under differ-
ent scenarios that may affect the placement algorithm,
and test for co-residency between all launched instances.
We analyze three popular cloud providers, Amazon Elas-
tic Cloud (EC2) [2], Google Compute Engine (GCE) [6]
and Microsoft Azure (Azure) [15], for vulnerabilities in
their placement algorithm. After exhaustive experimenta-
tion with each of these cloud providers and at least 190 runs
per cloud provider, we show that an attacker can still suc-
cessfully arrange for co-location (§ 5). We ﬁnd new launch
strategies in these three clouds that obtain co-location faster
(10x higher success rate) and cheaper (up to $114 less)
when compared to a secure reference placement policy.

Next, we start by giving some background on public
clouds (§ 2), then deﬁne our threat model (§ 3). We con-
clude the paper with related and future work (§ 6 and § 7,
respectively).

2 Background
Public clouds.
Infrastructure-as-a-service (IaaS) public
clouds, such as Amazon EC2, Google Compute Engine and
Microsoft Azure, provide a management interface for cus-
tomers to launch and terminate VM instances with a user-
speciﬁed conﬁguration. Typically, users register with the
cloud provider for an account and use the cloud interface
to specify VM conﬁguration, which includes instance type,
disk image, data center or region to host the VMs, and then
launch VM instances. In addition, public clouds also pro-
vide many higher-level services that monitor load and auto-
matically launch or terminate instances based on the work-
load [4,8,13]. These services internally use the same mech-
anisms as users to conﬁgure, launch and terminate VMs.

The provider’s VM launch service receives from a client
a desired set of parameters describing the conﬁguration of
the VM. The service then allocates resources for the new
VM; this process is called VM provisioning. We are most
interested in the portion of VM provisioning that selects
the physical host to run a VM, which we call the VM place-
ment algorithms. The resulting VM-to-host mapping we
call the VM placement. The placement for a speciﬁc virtual
machine may depend on many factors:
the load on each
machine, the number of machines in the data center, the
number of concurrent VM launch requests, etc.

While cloud providers do not generally publish their VM

Type

Placement
Parameters

Environment
Variable

Variable
# of customers
# of instances launched per customer
Instance type
Data Center (DC) or Region
Time launched
Cloud provider
Time of the day
Days of the week
Number of in-use VMs
Number of machines in DC

Figure 1: List of placement variables.

placement algorithms, there are several variables under the
control of the user that could affect the VM placement, such
as time-of-day, requested data center, and number of in-
stances. A list of some notable parameters are given in
Figure 1. By controlling these variables, an adversary can
partially inﬂuence the placement of VMs on physical ma-
chines that may also host a target set of VMs. We call these
variables placement variables and the set of values for these
variables form a launch strategy. An example launch strat-
egy is to launch 20 instances 10 minutes after triggering an
auto-scale event on a victim application. This is, in fact, a
launch strategy suggested by prior work [29].
Placement policies. VM placement algorithms used in
public clouds often aim to increase data center efﬁciency,
quality of service, or both by realizing some placement pol-
icy. For instance, a policy that aims to increase data center
utilization may pack launched VMs on a single machine.
Similarly policies that optimize the time to provision a VM,
which involves fetching an image over the network to the
physical machine and booting, may choose the last machine
that used the same VM image, as it may already have the
VM image cached on local disks. Policies may vary across
cloud providers, and even within a provider.

Public cloud placement policies, although undocu-
mented, often exhibit behavior that is externally observable.
One example is parallel placement locality [29], in which
VMs launched from different accounts within a short time
window are often placed on the same physical machine.
Two instances launched sequentially, where the ﬁrst in-
stance is terminated before the launch of the second one, are
often placed on the same physical machine, a phenomenon
called sequential placement locality [29].

These placement behaviors are artifacts of the two place-
ment policies described earlier, respectively. Other exam-
ples of policies and resulting behaviors exist as well. VMs
launched from the same accounts may either be packed
on the same physical machine to maximize locality (and
hence co-resident with themselves) or striped across differ-
ent physical machines to maximize redundancy (and hence
never co-resident with themselves). In the course of normal
usage, such behaviors are unlikely to be noticed, but they
can be measured with careful experiments.

914  24th USENIX Security Symposium 

USENIX Association

Launch strategies. An adversary can exploit placement
behaviors to increase the likelihood of co-locating with tar-
get victims. As pointed out by Ristenpart et al. [29], parallel
placement locality can be exploited by triggering a scale-up
event on target victim by increasing their load, which will
cause more victim VMs to launch. The adversary can then
simultaneously (or after a time lag) launch multiple VMs
some of which may be co-located with the newly launched
victim VM(s).

In this study, we develop a framework to systematically
evaluate public clouds against launch strategies and un-
cover previously unknown placement behaviors. We ap-
proach this study by (i) identifying a set of placement vari-
ables that characterize a VM, (ii) enumerating the most in-
teresting values for these variables, and (iii) quantifying the
cost of such a strategy, if it in fact exposes a co-residency
vulnerability. We repeat this for three major public cloud
providers: Amazon EC2, Google Compute Engine, and Mi-
crosoft Azure. Note that the goal of this study is not to re-
verse engineer the exact details of the placement policies,
but rather to identify launch strategies that can be exploited
by an adversary.
Co-residency detection. A key technique for understand-
ing placement vulnerabilities is detecting when VMs are
co-resident on the same physical machine (also termed co-
locate). In 2009, Ristenpart et al. [29] proposed several co-
residency detection techniques and used them to identify
several placement vulnerabilities in Amazon EC2. As co-
resident status is not reported directly by the cloud provider,
these detection methods are usually referred to as side-
channel based techniques, which can be further classiﬁed
into two categories: logical side-channels or performance
side-channels.

Logical side-channels: Logical side-channels allow infor-
mation leakage via logical resources that are observable to
a software program, e.g., IP addresses, timestamp counter
values. Particularly in Amazon EC2, each VM is assigned
two IP addresses, a public IP address for communication
over the Internet, and a private or internal IP address for
intra-datacenter communications. The EC2 cloud infras-
tructure allowed translation of public IP addresses to their
internal counterparts. This translation revealed the topol-
ogy of internal data center network, which allowed a remote
adversary to map the entire public cloud infrastructure and
determine, for example the availability zone and instance
type of a victim. Furthermore, co-resident VMs tended to
have adjacent internal IP addresses.

Logical side-channels can also be established via shared
timestamp counters.
In prior work, skew in timestamp
counters were used to ﬁngerprint a physical machine [27],
although this technique has not yet been explored for co-
residency detection. Co-residency detection via shared
state like interrupt counts and process statistics reported in
procfs also come under this category, but are only appli-
cable to container-based platform-as-a-service clouds.

Performance side-channels: Performance side-channels
are created when performance variations due to resource
contention are observable. Such variations can be used as
an indicator of co-residency. For instance, network perfor-
mance has been used for detecting co-residence [29, 30].
This is because hypervisors often short-cut network traf-
ﬁc between VMs on the same host, providing detectably
shorter round-trip times than between VMs on different
hosts.

Moreover, covert channels, as a special case of side-
channels, can be established between two cooperative VMs.
For purposes of co-residency detection, covert channels
based on shared hardware resources, such as last level
caches (LLCs) or local storage disks, can be exploited by
one VM to detect performance degradation caused by a co-
resident VM. Covert channel detection techniques require
control over both VMs, and are usually used in experimen-
tation rather than in practical attacks.

Placement study in PaaS. While we mainly studied
placement vulnerabilities in the context of IaaS, we also
experimented with Platform-as-a-Service (PaaS) clouds.
PaaS clouds offer elastic application hosting services. Un-
like IaaS where users are granted full control of the VM,
PaaS provides managed compute tasks (or instances) for the
execution of hosted web applications, and allow multiple
such instances to share the same operating system. As such,
logical side-channels alone are usually sufﬁcient for co-
residency detection purposes. For instance, in PaaS clouds,
co-resident instances share the same public IP address as
the host machine. This is because the host-to-instance net-
work is often conﬁgured using Network Address Transla-
tion (NAT) and each instance is assigned unique port under
the host IP address for front-facing incoming connections
(e.g., Heroku [10]). We found that many of the above log-
ical side-channel-based co-residency detection approaches
worked on PaaS clouds, even on those that support isolation
better than process isolation [38] (e.g., using Linux con-
tainers). Speciﬁcally, we used both system-level interrupt
statistics via /proc/interrupts and shared public IP ad-
dresses of the instances to detect co-location in Heroku.

Our quick investigation of co-location attacks in Heroku
showed that na¨ıve strategies like scaling two PaaS web ap-
plications to 30 instances with a time interval of 5 minutes
between them, resulted in co-location in 6 out of 10 runs.
Moreover, since the co-location detection was simple and
quick including the time taken for application scaling, we
were able to do these experiments free of cost. This result
reinforces prior ﬁndings on PaaS co-location attacks [38]
and conﬁrms the existence of cheap launch strategies to
achieve co-location and easy detection mechanisms to ver-
ify it. We do not investigate PaaS clouds further in the rest
of this paper.

USENIX Association  

24th USENIX Security Symposium  915

3 Threat Model

Co-residency attacks in public clouds, as mentioned ear-
lier, involve two steps: a launch strategy and co-residency
detection. We assume that the adversary has access to tools
to identify a set of target victims, and either knows vic-
tim VMs’ launch characteristics or can directly trigger their
launches. The latter is possible by increasing load in order
to cause the victim to scale up by launching more instances.
The focus of this study is to identify if there exists any
launch strategy that an adversary can devise to increase the
chance of co-residency with a set of targeted victim VMs.
In our threat model, we assume that the cloud provider
is trusted and the attacker has no afﬁliation of any form
with the cloud provider. This also means that the adversary
has no internal knowledge of the placement policies that
is responsible for the VM placements in the public cloud.
An adversary also has the same interface for launching and
terminated VMs as regular customers, and no other special
interfaces. Even though there may be per-account limits
on the number of VMs that a cloud provider imposes, an
adversary has access to an unlimited number of accounts
and hence has no limit on the number of VMs he could
launch at any given time.

No resource-limited cloud provider is a match to an ad-
versary with limitless resources and hence realistically we
assume that the adversary is resource-limited. For the same
reason, a cloud provider is vulnerable to a launch strategy
only when it is trivial or cost-effective for an adversary. As
such, we aim to (i) quantify the cost of executing a launch
strategy by an adversary, (ii) deﬁne a reference placement
policy with which the placement policies of real clouds can
be compared, and (iii) deﬁne metrics to quantify placement
vulnerability.

Cost of a Launch Strategy. Quantifying the cost of a
launch strategy is straight-forward: it is the cost of launch-
ing a number of VMs and running tests to detect co-
residency with one or more target victim VMs. To be
precise, the cost of a launch strategy S is given by CS =
a ∗ P(atype) ∗ Td(v,a), where a is the number of attacker
VMs of type atype launched to get co-located with one of
the v victim VMs. P(atype) is the price of running one VM
of type atype for a unit time. Td(a,v) is the time to detect
co-residency between all pairs of a attacjers and v victim
VMs, excluding pairs within each group. For simplicity,
we assume that the attacker is running all instances till the
last co-residency check is completed.

Reference Placement Policy.
In order to deﬁne placement
vulnerability, we need a yardstick to compare various place-
ment policies and the launch strategies that they may be vul-
nerable to. To aid this purpose, we deﬁne a simple reference
placement policy that has good security properties against
co-residency attacks and use it to gauge the placement poli-
cies used in public clouds. Let there be N machines in a data
center and let each machine have unlimited capacity. Given

a set of unordered VM launch requests, the mapping of each
VM to a machine follows a uniform random distribution.
Let there be v victim VMs assigned to v unique machines
among N, where v (cid:30) N. The probability of at least one col-
lision (i.e. co-residency) under the random placement pol-
icy and the above attack scenario when attacker launches a
instances is given by 1−(cid:31)1− v/N(cid:30)a. We call this proba-

bility the reference probability1. Recall that for calculating
cost of a launch strategy under this reference policy, we also
need to deﬁne the price function, P(vmtype). For simplicity,
we use the most competitive minimum price offered by any
cloud provider as the price for the compute resource under
the reference policy. For example, at the time of this study,
Amazon EC2 offered t2.small instances at $0.026 per hour
of instance activity, which was the cheapest price across all
three clouds considered in this study.

Note that the placement policy makes several simplify-
ing assumptions but we argue that all these assumptions are
conservative and only beneﬁt the attacker. For instance, the
assumption on unlimited capacity of the servers only bene-
ﬁt the attacker as it never limits the number of victim VMs
an attacker could potentially co-locate with. We use a con-
servative value of 1000 for N, which is at least an order-
of-magnitude less than the number of servers (50,000) in
the smallest reported Amazon EC2 data centers [5]. Sim-
ilarly, the price function of this placement policy also fa-
vors an attacker as it provides the cheapest price possible in
the market even though in reality a secure placement policy
may demand a higher price. Hence, it would be troubling if
the state-of-the-art placement policies used in public clouds
does not measure well even against such a conservative ref-
erence placement policy.
Placement Vulnerability. Putting it all together, we de-
ﬁne two metrics to gauge any launch strategy against a
placement policy: (i) normalized success rate, and (ii) cost-
beneﬁt. The normalized success rate is the success rate of
the launch strategy in the cloud under test normalized to the
success rate of the same strategy under the reference place-
ment policy. The cost-beneﬁt of a strategy is the additional
cost that is incurred by the adversary in the reference place-
ment policy to achieve the same success rate as the strat-
egy in the placement policy under test. We deﬁne that a
placement policy has a placement vulnerability if and only
if there exists a launch strategy with a normalized success
rate that is greater than 1.

Note that the normalized success rate quantiﬁes how easy
it is to get co-location. On the other hand, the cost beneﬁt
metric helps to quantify how cheap it is to get co-location
compared to a more secure placement policy. These metrics
can be used to compare launch strategies under different
placement policies, where a higher value for any of these
metrics indicate that the placement policy is relative more

1This probability event follows hypergeometric distribution. Itis the
same as the probability event of collision when picking two groups of
balls each of size v and a from an urn of total capacity of balls, N.

916  24th USENIX Security Symposium 

USENIX Association

)

%

(
 
y
c
n
e
u
q
e
r
F

 1

 0.9

 0.8

 0.7

 0.6

 0.5

 0.4

 0.3

 0.2

 0.1

 0

Non-coresident
Co-resident

)

%

(
 
y
c
n
e
u
q
e
r
F

 0.4

 0.35

 0.3

 0.25

 0.2

 0.15

 0.1

 0.05

 0

Non-coresident
Co-resident

)

%

(
 
y
c
n
e
u
q
e
r
F

 0.35

 0.3

 0.25

 0.2

 0.15

 0.1

 0.05

 0

Non-coresident
Co-resident

0

0.4

0.8

1.2

1.6

2

2.4

2.8

0

0.4

0.8

1.2

1.6

2

2.4

2.8

0

0.4

0.8

1.2

1.6

2

2.4

2.8

Minimum Network RTT (ms)

Minimum Network RTT (ms)

Minimum Network RTT (ms)

(a) GCE

(b) EC2

(c) Azure

Figure 2: Histogram of minimum network round trip times between pairs of VMs. The frequency is represented as a fraction of total
number of pairs in each category. The ﬁgure does not show the tail of the histogram.

vulnerable to that launch strategy. An ideal placement pol-
icy should aim to reduce both the success rate and the cost
beneﬁt of any strategy.

4 Detecting Co-Residence

An essential prerequisite for the placement vulnerability
study is access to a co-residency detection technique that
identiﬁes whether two VMs are resident on the same phys-
ical machine in a third-party public cloud.

Challenges in modern clouds. Applying the detection
techniques mentioned in Section 2 is no longer feasible in
modern clouds.
In part due to the vulnerability disclo-
sure by Ristenpart et al. [29], modern public clouds have
adopted new technologies that enhance the isolation be-
tween cloud tenants and thwart known co-residence detec-
tion techniques. In the network layer, virtual private clouds
(VPCs) have been broadly employed for data center man-
agement [17,20]. With VPCs, internal IP addresses are pri-
vate to a cloud tenant, and can no longer be used for cloud
cartography. Although EC2 allowed this in older genera-
tion instances (called EC2-classic), this is no longer pos-
sible under Amazon VPC setting.
In addition, VPCs re-
quire communication between tenants to use public IP ad-
dresses for communication. As shown in Figure 2, the net-
work timing test is also defeated, as using public IP ad-
dresses seems to involve routing in the data center network
rather than short-circuiting through the hypervisor. Here,
the ground-truth of co-residency is detected using memory-
based covert-channel (described later in this section). No-
tice that there is no clear distinction between the frequency
distribution of the network round trip times of co-resident
and non-coresident pairs on all three clouds.

In the system layer, persistent storage using local disks
is no longer the default. For instance, many Amazon EC2
instance types do not support local storage [1]; GCE and
Azure provide only local Solid State Drives (SSD) [7, 14],
which are less susceptible to detectable delays from long
seeks.
In addition, covert channels based on last-level
caches [29, 30, 33, 36] are less reliable in modern clouds
that use multiple CPU packages. Two VMs sharing the

same machine may not share LLCs to establish the covert
channel. Hence, these LLC-based covert-channels can only
capture a subset of co-resident instances.

As a result of these technology changes, none of the prior
techniques for detecting co-residency reliably work in mod-
ern clouds, compelling us to develop new approaches for
our study.

4.1 Co-residency Tests

We describe in this subsection a pair of tools for co-
residency tests, with the following design goals:
• Applicable to a variety of heterogeneous software and
• Detect co-residency with high conﬁdence: the false de-
tection rate should be low even in the presence of back-
ground noise from other neighboring VMs.

hardware stacks used in public clouds.

• Detect co-residency fast enough to facilitate experimen-

tation among large sets of VMs.
We chose a performance covert-channel based detection
technique that exploits shared hardware resources, as this
type of covert-channels are often hard to remove and most
clouds are very likely to be vulnerable to it.

A covert-channel consists of a sender and a receiver. The
sender creates contention for a shared resources and uses
it to signal another tenant that potentially share the same
resource. The receiver, on the other hand, senses this con-
tention by periodically measuring the performance of that
shared resource. A signiﬁcant performance degradation
measured at the receiver results in a successful detection of
a sender’s signal. Here the reliability of the covert-channel
is highly dependent on the choice of the shared resource and
the level of contention created by the sender. The sender is
the key component of the co-residency detection techniques
we developed as part of this study.
Memory locking sender. Modern x86 processors sup-
port atomic memory operations, such as XADD for atomic
addition, and maintain their atomicity using cache coher-
ence protocols. However, when a locked operation extends
across a cache-line boundary, the processor may lock the
memory bus temporarily [32]. This locking of the bus can

USENIX Association  

24th USENIX Security Symposium  917

// allocate memory multiples of 64 bits
char_ptr = allocate_memory((N+1)*8)
//move half word up
unaligned_addr = char_ptr + 2

loop forever:

loop i from (1..N):

atomic_op(unaligned_addr + i, some_value)

end loop

end loop

Figure 3: Memory Locking – Sender.

be detected as it slows down other uses of the bus, such
as fetching data from DRAM. Hence, when used properly,
it provides a timing covert channel to send a signal to an-
other VM. Unlike cache-based covert channels, this tech-
nique works regardless of whether VMs share a CPU core
or package.

We developed a sender exploiting this shared memory-
bus covert-channel. The psuedocode for the sender is
shown in Figure 3. The sender creates a memory buffer and
uses pointer arithmetic to force atomic operations on un-
aligned memory addresses. This indirectly locks the mem-
ory bus even on all modern processor architectures [32].

size = LLC_size * (LLC_ways +1)
stride = LLC_sets * cacheline_sz)
buffer = alloc_ptr_chasing_buff(size, stride)

loop sample from (1..10): //number of samples

start_rdtsc = rdtsc()
loop probes from (1..10000):

probe(buffer); //always hits memory

end loop
time_taken[sample] = (rdtsc() - start_rdtsc)

end loop

Figure 4: Memory Probe – Receiver.

Receivers. With the aforementioned memory locking
sender, there are several ways to sense the memory locking
contention induced by the sender in an another co-resident
tenant instance. All the receivers measure the memory
bandwidth of the shared system. We present two types of
receivers that we used in this study that works on heteroge-
neous hardware conﬁgurations.

Memory probing receiver uses carefully crafted memory
requests that always miss in the cache hierarchy and al-
ways hit memory. This is ensured by making data accesses
from receiver fall into the same LLC set. In order to evade
hardware prefetching, we use a pointer-chasing buffer to
randomly access a list of memory addresses (pseudocode
shown in Figure 4). The time needed to complete a ﬁxed
number of probes (e.g., 10,000) provides a signal of co-
residence: when the sender is performing locked opera-
tions, loads from memory proceed slowly.

Memory locking receiver is similar to the sender but mea-
sures the number of unaligned atomic operations that could
be completed per unit time. Although it also measures the
memory bandwidth, unlike memory probing receiver, it
works even when the cache architecture of the machine is
unknown.

The sender along with these two receivers form our
two novel co-residency detection methods that we use in
this study: memory probing test and memory locking test
(named after their respective receivers). These comprise
our co-residency test suite. Each test in the suite starts by
running the receiver on one VM while keeping the other
idle. The performance measured by this run is the baseline
performance without contention. Then the receiver and the
sender are run together. If the receiver detects decreased
performance, the tests concludes that the two VMs are co-
resident. We use a slowdown threshold to detect when the
change in receiver performance indicates co-residence (dis-
cussed later in the section).

Machine

Architecture
Xeon E5645
Xeon X5650
Xeon X5650

Cores Memory Memory
Locking

Probing

6
12
12

3.51
3.61
3.46

1.79
1.77
1.55

Socket

Same
Same
Diff.

Figure 5: Memory probing and locking on testbed machines.
Slowdown relative to the baseline performance observed by the
receiver averaged across 10 samples. Same – sender and receiver
on different cores on the same socket, Diff. – sender and receiver
on different cores on different sockets. Xeon E5645 machine had
a single socket.
Evaluation on local testbed.
In order to measure the efﬁ-
cacy of this covert channel we ran tests in our local testbed.
Results of running memory probing and locking tests under
various conﬁgurations are shown in Figure 5. The hard-
ware architecture of these machines are similar to what is
observed in the cloud [21]. Across these hardware con-
ﬁgurations, we observed a performance degradation of at
least 3.4x compared to not running memory locking sender
on a non-coresident instance (i.e. a baseline run with idle
sender) indicating reliability. Note that this works even
when the co-resident instances are running on cores on dif-
ferent sockets, which does not share the same LLC (works
on heterogeneous hardware). Further, a single run takes one
tenth of a second to complete and hence is also quick.

Note that for this test suite to work in the real world,
an attacker requires control over both the VMs under test,
which includes the victim. We call this scenario as co-
residency detection under cooperative victims (in short,
cooperative co-residency detection). Such a mechanism
is sufﬁcient observe placement behavior in public clouds
(Section 4.2). We further investigated approaches to detect
co-residency under a realistic setting with an uncooperative
victim. In Section 4.3 we show how to adapt the memory
probing test to detect co-location with one of the many web-
servers behind a load balancer.

4.2 Cooperative Co-residency Detection

In this section, we describe the methodology we used to
detect co-residency in public clouds. For the purposes of
studying placement policies, we had the ﬂexibility to con-

918  24th USENIX Security Symposium 

USENIX Association

s
r
i
a
p
 
f
o
 
r
e
b
m
u
N

 16

 14

 12

 10

 8

 6

 4

 2

 0

Non-coresident
Co-resident

s
r
i
a
p
 
f
o
 
r
e
b
m
u
N

 20

 15

 10

 5

 0

Non-coresident
Co-resident

s
r
i
a
p
 
f
o
 
r
e
b
m
u
N

 120

 100

 80

 60

 40

 20

 0

Non-coresident
Co-resident

0.8 1.2 1.6

2

2.4 2.8 3.2 3.6

4

4.4

0.8 1.2 1.6 2 2.4 2.8 3.2 3.6 4 4.4 4.8 5.2 5.6

0.8

1.2

1.5

1.8

2.2

2.6

3

Performance degradation

Performance degradation

Performance degradation

(a) GCE

(b) EC2

(c) Azure – Intel machines

Figure 6: Distribution of performance degradation of memory probing test. For varying number of pairs on each cloud (EC2:300,
GCE:21, Azure:278). Note the x-axis plots performance degradation.

trol both VMs that we test for co-residence. We did this
by launching VMs from two separate accounts and test
them for pairwise co-residence. We encountered several
challenges when running the same on three different pub-
lic clouds - Google Computer Engine, Amazon EC2 and
Microsoft Azure.

First, we had to handle noise from neighboring VMs
sharing the same host. Second, hardware and software het-
erogeneity in the three different public clouds required spe-
cial tuning process for the co-residency detection tests. Fi-
nally, testing co-residency for a large set of VMs demanded
a scalable implementation. We elaborate on our solution to
these challenges below.
Handling noise. Any noise from neighboring VMs could
affect the performance of the receiver with and without the
signal (or baseline) and result in misdetection. To han-
dle such noise, we alternate between measuring the perfor-
mance with and without the sender’s signal, such that any
noise equally affects both the measurements. Secondly, we
take ten samples of each measurement and only detect co-
residence if the ratios of both the mean and median of these
samples exceed the threshold. As each run takes a frac-
tion of a second to complete, repeating 10 times is still fast
enough.

Cloud
Provider

EC2
GCE
Azure
Azure

Machine

Architecture

Intel Xeon E5-2670

Generic Xeon*
Intel E5-2660

AMD Opeteron 4171 HE

Clock
(GHz)
2.50
2.60*
2.20
2.10

LLC

(Ways × Set)
20 × 20480
20 × 16384
20 × 16384
48 × 1706

Figure 7: Machine conﬁguration in public clouds. The machine
conﬁgurations observed over all runs with small instance types.
GCE did not reveal the exact microarchitecture of the physical
host (*). Ways × Sets × Word Size gives the LLC size. The word
size for all these x86-64 machines is 64 bytes.

Tuning thresholds. As expected, we encountered dif-
ferent machine conﬁguration on the three different public
clouds (shown in Figure 7) with heterogeneous cache di-
mensions, organizations and replacement policies [11, 26].

This affects the performance degradation observed by the
receivers with respect to the baseline and the ideal thresh-
old for detecting co-residency. This is important because
the thresholds we use to detect co-residence yield false pos-
itives, if set too low, and false negatives if set too high.
Hence, we tuned the threshold to each hardware we found
on all three clouds.

We started with a conservative threshold of 1.5x and
tuned to a ﬁnal threshold of 2x for GCE and EC2 and
1.5x for Azure for both the memory probing and locking
tests. Figure 6 shows the distribution of performance degra-
dation under the memory probing tests across Intel ma-
chines in EC2, GCE, and Azure. For GCE and EC2, a per-
formance degradation threshold of 2 clearly separates co-
resident from non-co-resident instances. For all Intel ma-
chines we encountered, although we ran both memory lock-
ing and probing tests, memory probing was sufﬁcient to de-
tect co-residency. For Azure, overall we observe lower per-
formance degradation and the initial threshold of 1.5 was
sufﬁcient to detect co-location on Intel machines.

The picture for AMD machines in Azure differs signif-
icantly as shown in Figure 8. The distribution of perfor-
mance degradation for both memory locking and mem-
ory probing shows that, unlike for other architecures, co-
residency detection is highly sensitive to the choice of the
threshold for AMD machines. This may be due to the

Azure - AMD Machines

mprobe-NC
mlock-NC
mprobe-C
mlock-C

s
r
i
a
p

 
f

o

 
r
e
b
m
u
N

 18

 16

 14

 12

 10

 8

 6

 4

 2

 0

0 0.8 1 1.2 1.4 1.5 1.6 1.8 2

2

Performance degradation

Figure 8: Distribution of performance degradation of memory
probing and locking tests. On AMD machines in Azure with
40 pairs of nodes. Here NC stands for non-coresident and C, co-
resident pairs. Note that the x-axis plots performance degradation.

USENIX Association  

24th USENIX Security Symposium  919

more associative cache (48 ways vs. 20 for Intel), or dif-
ferent handling of locked instructions. For these machines,
a threshold of 1.5 was high enough to have no false pos-
itives, which we veriﬁed by hand checking the instances
using the two covert-channels and observed consistent per-
formance degradation of at least 50%.. We determine a
pair of VMs as co-resident if the degradation in either of
the tests is above this threshold. We did not detect any
cross-architecture (false) co-residency detection in any of
the runs.
Scaling co-residency detection tests.
Testing co-
residency at scale is time-consuming and increases quadrat-
ically with the number of instances: checking 40 VM in-
stances, involves 780 pair-wised tests. Even if each run of
the entire co-residency test suite takes only 10 seconds, a
na¨ıve sequential execution of the tests on all the pairs will
take 2 hours. Parallel co-residency checks can speed check-
ing, but concurrent tests may interfere with each other.

To parallelize the test, we partition the set of all VM pairs

((cid:31)v+a
2 (cid:30)) into sets of pairs with no VMs twice; we run one of

these sets at a time and record which pairs detected possible
co-residence. After running all sets, we have a set of can-
didate co-resident pairs, which we test sequentially. Paral-
lelizing co-residency tests signiﬁcantly decreased the time
taken to test all co-residency pairs. For instance, the par-
allel version of the test on one of the cloud providers took
2.4 seconds per pair whereas the serial version took almost
46.3 seconds per pair (a speedup of 20x). While there are
faster ways to parallelize co-residency detection, we chose
this approach for simplicity.
Veracity of our tests. Notice that a performance degra-
dation of 1.5x, 2x and 4x corresponds to 50%, 100% and
300% performance degradation. Such high performance
degradations (even 50%) is clear enough signal to declare
co-residency due to resource sharing. Furthermore, we
hand checked by running the two tests in isolation on the
detected instance-pairs for a signiﬁcant fraction of the runs
for all clouds and observed a consistent covert-channel sig-
nal. Thus our methodology did not detect any false pos-
itives, which are more detrimental to our study than false
negatives. Although “co-residency” here implies sharing
of memory channel, which may not always mean sharing
of cores or other per-core hardware resources.

4.3 Co-residency Detection on Uncoopera-

tive Victims

Until now, we described a method to detect co-residency
with a cooperative victim.
In this section, we look at a
more realistic setting where an adversary wishes to de-
tect co-residency with a victim VM with accesses limited
to only public interfaces like HTTP or a key-value (KV)
store’s put-get interface. We show that the basic coopera-
tive co-residency detection can also be employed to detect
co-residency with an uncooperative victim in the wild.

Attack setting. Unlike previous attack scenarios, we as-
sume the attacker has no access to the victim VMs or its
application other than what is permitted to any user on the
Internet. That is, the victim application exposes a well-
known public interface (e.g., HTTP, FTP, KV-store proto-
col) that allows incoming requests, which is also the only
access point for the attacker to the victim. The front end
of this victim application can range from caching or data
storage services (e.g., memcached, cassandra) to generic
webservers. We also assume that there may be multiple
instances of this front-end service running behind a load
balancer. Under this scenario, the attacker wishes to de-
tect co-location with one or more of the front-facing victim
VMs.

Co-residency test. We adapt the memory tests used in pre-
vious section by running the memory locking sender in the
attacker instance. For a receiver, we use the public inter-
face exposed by the victim by generating a set of requests
that potentially makes the victim VMs hit the memory bus.
This can be achieved looping through large number of re-
quests of sizes approximately equal or greater than the size
of the LLC. This creates a performance side-channel that
leaks co-residency information. This receiver runs in an in-
dependent VM under the adversary’s control, which we call
the co-residency detector.

Experiment setup. To evaluate the efﬁcacy of this
method, we used the Olio multi-tier web application [12]
that is designed to mimic a social-networking application.
We used an instance of this workload from CloudSuite [22].
Although Olio supports several tiers (e.g., memcached to
cache results of database queries), we conﬁgured it with
two tiers, with each webserver and the database server run-
ning in a separate VM of type t2.small on Amazon EC2.
Multiple of these webserver VMs are conﬁgured behind a
HAProxy-based load balancer [9] running in a m3.medium
instance (for better networking performance). The load bal-
ancer follows the standard conﬁguration of using round-
robin load balacing algorithm with sticky client sessions
using cookies. We believe such a victim web application
and its conﬁguration is a reasonable generalization of real
world applications running in the cloud.

For the attacker, we use an off-the-shelf HTTP perfor-
mance measurement utility called HTTPerf [28] as the re-
ceiver in the co-residency detection test. This receiver is
run inside a t2.micro instance (for free of charge). We used
a set of 212 requests that included web pages and web ob-
jects (images, PDF ﬁles). We gathered these requests using
the access log of manual navigation around the web appli-
cation from a web browser.

Evaluation methodology. We start with a known co-
resident VM pair using the cooperative co-residency detec-
tion method. We conﬁgure one of the VMs as a victim web-
server VM and launch four more VMs:
two webservers,
one database server and a load balancer, all of which are

920  24th USENIX Security Symposium 

USENIX Association

not co-resident with the attacker VM.

Co-residency detection starts by measuring the average
request latency at the receiver inside the co-residency de-
tector for the baseline (with idle attacker) and contended
case with the attacker runs the memory locking sender. A
signiﬁcant performance degradation between the baseline
and the contended case across multiple samples reveal co-
residency of one of the victim VMs with the attacker VM.
On Amazon EC2, with the above setup we observed an av-
erage request latency of 4.66ms in the baseline case and
a 10.6ms in the memory locked case, i.e., a performance
degradation of ≈ 2.3x.
Background noise. The above test was performed when
the victim web application was idle. In reality, any victim in
the cloud might experience constant or varying background
load on the system. False positives or negatives may occur
when there is spike in load on the victim servers. In such
case, we use the same solution as in Section 4.2 — alternat-
ing between measuring the idle and the contended case.

In order to gauge the efﬁcacy of the test under constant
background load, we repeated the above experiment with
varying load on the victim. The result of this experiment is
summarized in Figure 9. Counterintuitively, we found that
a constant load on the background server exacerbates the
performance degradation gap, hence resulting in a clearer
signal of co-residency. This is because running memory
locking on the co-resident attacker increases the service
time of all requests as majority of the requests rely on mem-
ory bandwidth. This increases the queuing delay in the sys-
tem and in turn increasing the overall request latency. Inter-
estingly, this aforementioned performance gap stops widen-
ing at higher loads of 750 to 1000 concurrent users as the
system hits a bottleneck (in our case a network bottleneck at
the load balancer) even without running the memory lock-
ing sender. Thus, detecting co-residency with a victim VM
that is part of a highly loaded and bottlenecked application
would be hard using this test.

We also experimented with increasing the number of
victim webservers behind the load balancer beyond 3
(Figure 10). As expected, the co-residency signal grew
weaker with increasing victims, and at 9 webservers, the
performance degradation was too low to be useful for de-
tecting co-residency.

5 Placement Vulnerability Study

In this section, we evaluate three public clouds, Amazon
EC2, Google Compute Engine and Microsoft Azure, for
placement vulnerabilities and answer the following ques-
tions: (i) what are all the strategies that an adversary can
employ to increase the chance of co-location with one or
more victim VMs? (ii) what are the chances of success and
cost of each strategy? and (iii) how does these strategies
compare against the reference placement policy introduced
in Section 3?

)
s
m

(
 
y
c
n
e

t

a
L

 
t
s
e
u
q
e
R
e
g
a
r
e
v
A

 

l

e
a
c
s
 

g
o
L

 

 1024

 512

 256

 128

 64

 32

 16

 8

 4

w/o-mlock
w/-mlock

idle

100

250

500

750

1000

Background Load on Victim 

(# concurrent users)

Figure 9: Co-residency detection on an uncooperative victim.
The graph shows the average request latency at the co-residency
detector without and with memory locking sender running on the
co-resident attacker VM under varying background load on the
victim. Note that the y-axis is in log scale. The load is in the
number of concurrent users, where each user on average generates
20 HTTP requests per second to the webserver.

)
s
m

(
 
y
c
n
e

t

a
L

 
t
s
e
u
q
e
R
e
g
a
r
e
v
A

 

 256

 128

l

e
a
c
s
 

g
o
L

 

 64

 32

 16

 8

 4

w/o-mlock
w/-mlock

w/o-mlock

3

5

7

9

Idle

3

5

7

9

500 users

Number of webservers

Figure 10: Varying number of webservers behind the load bal-
ancer. The graph shows the average request latency at the co-
residency detector without and with memory locking sender run-
ning on the co-residency attacker VM under varying background
load on the victim. Note that the y-axis is in log scale. The error
bars show the standard deviation over 5 samples.
5.1 Experimental Methodology

Before presenting the results, we ﬁrst describe the exper-
iment setting and methodology that we employed for this
placement vulnerability study.
Experiment settings. Recall VM placement depends on
several placement variables (shown in Figure 1). We as-
signed reasonable values to these placement variables and
enumerated through several launch strategies. A run cor-
responds to one launch strategy and it involves launching
multiple VMs from two different accounts (i.e., subscrip-
tions in Azure and projects in GCE) and checking for co-
residency between all pairs of VMs launched. One account
was designated as a proxy for the victim and the other for
the adversary. We denote a run conﬁguration by v × a,
where v is the number of victim instances and a is the num-
ber of attacker instances launched in that run. We varied v
and a for all v, a ∈ {10,20,30} and restricted them to the
inequality, v ≤ a, as it increases the likelihood of getting
co-residency.

USENIX Association  

24th USENIX Security Symposium  921

Other placement variables that are part of the run con-
ﬁguration include: victim launch time (including time of
the day, day of the week), delay between victim and at-
tacker VM launches, victim and attacker instance types and
data center location or region where the VMs are launched.
We repeat each run multiple times across all three cloud
providers. The repetition of experiments is especially re-
quired to control the effect of certain environment variables
like time of day. We repeat experiments for each run con-
ﬁguration over various times of the day and days of the
week. We ﬁx the instance type of VMs to small instances
(t2.small on EC2, g1.small on GCE and small or Standard-
A1 on Azure) and data center regions to us-east for EC2,
us-central1-a for GCE and east-us for Azure, unless other-
wise noted. All these experiments were conducted over 3
months between December 2014 to February 2015.

We used a single, local Intel Core i7-2600 machine with
8 SMT cores to launch VM instances, log instance informa-
tion and run the co-residency detection test suite.
Implementation and the Cloud APIs.
In order to auto-
mate our experiments, we used Python and the libcloud2
library [3] to interface with EC2 and GCE. Unfortunately,
libcloud did not support Azure. The only Azure cloud
API on Linux platform was a node.js library and a cross-
platform command-line interface (CLI). We built a wrap-
per around the CLI. There were no signiﬁcant differences
across different cloud APIs except that Azure did not have
any explicit interface to launch multiple VMs simultane-
ously.

As mentioned in the experiment settings, we experi-
mented with various delays between the victim and attacker
VM launches (0, 1, 2, 4 . . . hours). To save money, we
reused the same set of victim instances for each of the
longer runs. That is, for the run conﬁguration of 10x10 with
0, 1, 2, and 4 hours of delay between victim and attacker
VM launches, we launched the victim VMs only once at the
start of the experiment. After running co-residency tests on
the ﬁrst set of VM pairs, we terminated all the attacker in-
stances and relaunched attacker VM instances after appro-
priate delays (say 1 hour) and rerun the tests with the same
set of victim VMs. We repeat this until we experimented
with all the required delays for this conﬁguration. We call
this methodology the leap-frog method. It is also important
to note that zero delay here means parallel launch of VMs
from our test machine (and not sequential launch of VMs
from one account after another), unless otherwise noted.

In the sections below, we take a closer look at the effect
of varying one placement variable while keeping other vari-
ables ﬁxed across all the cloud providers. In each case, we
use three metrics to measure the degree of co-residency:
chances of getting at least one co-resident instance across
a number of runs (or success rate), average number of co-
resident instances over multiple runs and average coverage

2We used libcloud version 0.15.1 for EC2, and a modiﬁed version of

0.16.0 for GCE to support the use of multiple accounts in GCE.

Delay (hr)

0
0
0
0
0
0
1
1
1
1
1
1

Conﬁg Mean
10x10
0.11
0.2
10x20
0.5
10x30
0.43
20x20
20x30
1.67
1.6
30x30
0.25
10x10
0.33
10x20
10x30
1.6
1.27
20x20
20x30
2.44
30x30

3

S.D. Min Median Max
0.33
0.42
0.71
0.65
1.22
1.65
0.46
0.5
1.07
1.22
1.51
1.12

0
0
0
0
0
0
0
0
0
0
0
1

0
0
0
0
2
1
0
0
2
1
3
3

Figure 11: Distribution of number of co-resident pairs on
GCE. Region: us-central1-a.

S.D. Min Median Max
0

1
1
2
2
4
5
1
1
3
4
4
5

0
2
3
4
9
7
9

Delay (hr)

Conﬁg Mean

0
1
1
1
1
1
1

∗

10x10
10x20
10x30
20x20
20x30
30x30

0

0.44
1.11
1.4
3.57
3.78
3.89

0.73
1.17
1.43
2.59
1.79
2.09

0
0
0
0
0
1
2

0
0
1
1.5
3.5
4
3

Figure 12: Distribution of number of co-resident pairs on EC2.
Region: us-east.

Delay (hr)

0
0
0
0
0
0
1
1
1
1
1
1

Conﬁg Mean
10x10
15.22
3.78
10x20
4.25
10x30
9.67
20x20
20x30
2.38
24.57
30x30
2.78
10x10
0.78
10x20
10x30
0.75
0.67
20x20
0.86
20x30
30x30
4.71

S.D. Min Median Max
19.51
64
14
4.71
19
6.41
27
8.43
1.51
5
99
36.54
12
3.87
3
1.2
1.39
3
5
1.66
2
0.9
9.89
27

14
3
2.5
8
2
6
1
0
0
0
1
1

0
0
0
0
1
1
0
0
0
0
0
0

Figure 13: Distribution of number of co-resident pairs on
Azure. Region: East US 1.

(i.e., fraction of victim VMs with which attacker VMs were
co-resident). Although these experiments were done with
victim VMs under our control, the results can be extrapo-
lated to guide an attacker’s launch strategy for uncoopera-
tive victim. We also discuss a set of such strategic ques-
tions that the results help answer. At the end of this section,
we summarize and calculate the cost of several interesting
launch strategies and evaluate the public clouds against our
reference placement policy.

922  24th USENIX Security Symposium 

USENIX Association

i

y
c
n
e
d
s
e
r
-
o
C

 
f

o

 
s
e
c
n
a
h
C

 1

 0.8

 0.6

 0.4

 0.2

 0

1

1

1

0

0

0

x

x

x

1

2

3

1

1

1

0

0

0

x

x

x

1

2

3

1

1

1

0

0

0

x

x

x

1

2

3

0

0

0

0

0

0

0

0

0

GCE

EC2

Azure

i

y
c
n
e
d
s
e
r
-
o
C

 
f

o

 
s
e
c
n
a
h
C

 1

 0.8

 0.6

 0.4

 0.2

 0

1

2

3

0

0

0

x

x

x

3

3

3

1

2

3

0

0

0

x

x

x

3

3

3

1

2

3

0

0

0

x

x

x

3

3

3

0

0

0

0

0

0

0

0

0

GCE

EC2

Azure

Figure 14: Chances of co-residency of 10 victim instances with
varying number of attacker instances. All these results are
from one data center region (EC2: us-east, GCE: us-central1-a,
Azure: East US) and the delays between victim and attacker in-
stance launch were 1 hour. Results are over at least 9 run per run
conﬁguration with at least 3 run per time of day.

Figure 15: Chances of co-residency of 30 attacker instances
with varying number of victim instances. All these results are
from one data center region (EC2: us-east, GCE: us-central1-a,
Azure: East US) and the delays between victim and attacker in-
stance launch were 1 hour. Results are over at least 9 run per run
conﬁguration with at least 3 run per time of day.

5.2 Effect of Number of Instances

In this section, we observe the placement behavior while
varying the number of victim and attacker instances. Intu-
itively, we expect the chances of co-residency to increase
with the number of attacker and victim instances.

Varying number of attacker instances. Keeping all the
placement variables constant including the number of vic-
tim instances, we measure the chance of co-residency over
multiple runs. The result of this experiment helps to answer
the question: How many VMs should an adversary launch
to increase the chance of co-residency?

As is shown in Figure 14,

the placement behavior
changes across different cloud providers. For GCE and
EC2, we observe that higher the number of attacker in-
stances relative to the victim instances,
the higher the
chance of co-residency is. Figure 11 and 12 show the dis-
tribution of number of co-resident VM pairs on GCE and
EC2, respectively. The number of co-resident VM pairs
also increase with the number of attacker instances, imply-
ing that the coverage of an attack could be increased with
larger fraction of attacker instances than the target VM in-
stances if the launch times are coordinated.

Contrary to our expectations, the placement behavior ob-
served on Azure is the inverse. The chances of co-residency
with 10 attacker instance is almost twice as high as with 30
instances. This is also reﬂected in the distribution of num-
ber of co-residency VM pairs (shown in Figure 13). Further
investigation revealed a correlation between the number of
victim and attacker instances launched and the chance of
co-residency. That is, for the run conﬁguration of 10x10,
20x20 and 30x30, where number of victim and attacker in-
stances are the same, and with 0 delay, the chance of co-
residency were equally high for all these conﬁgurations (be-
tween 0.9 to 1). This suggests a possible placement policy
that collates VM launch requests together based on their re-
quest size and places them on the same group of machines.

Varying number of victim instances. Similarly, we also
varied the number of victim instances by keeping the num-
ber of attacker instances and other placement variables con-
stant (result shown in Figure 15). We expect the chance
of co-residency to increase with the number of victims tar-
geted. Hence, the results presented here help an adversary
answer the question: What are the chances of co-residency
with varying sizes of target victims?

As expected, we see an increase in the chances of co-
residency with increasing number of victim VMs across
all cloud providers. We see that the absolute value of the
chance of co-residency is lower for Azure than other clouds.
This may be the result of signiﬁcant additional delay be-
tween victim and attacker launch times in Azure as a result
of our methodology (more on this later).

5.3 Effect of Instance Launch Time

In this section, we answer two questions that aid an ad-
versary to design better launch strategies: How quickly
should an attacker launch his VMs after the victim VMs
are launched? Is there any increase in chance associated
with the time of day of the launch?

delay

between

attacker

Varying
victim
launches. The result of varying the delay between
0 (i.e., parallel
launch) and 1 hour delay is shown
in Figure 16. We can make two immediate observations
from this result.

and

The ﬁrst observation reveals a signiﬁcant artifact of
EC2’s placement policy: VMs launched within a short time
window are never co-resident on the same machine. This
observation helps an adversary to avoid such a strategy.
We further investigated placement behaviors on EC2 with
shorter non-zero delays in order to ﬁnd the duration of this
time window in which there are zero co-residency (results
shown in Figure 17). We found that this time window is
very short and that even a sequential launch of instances

USENIX Association  

24th USENIX Security Symposium  923

i

y
c
n
e
d
s
e
r
-
o
C

 
f

o

 
s
e
c
n
a
h
C

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

1

2

3

0

0

0

x

x

x

1

2

3

1

2

3

0

0

0

x

x

x

1

2

3

1

2

3

0

0

0

x

x

x

1

2

3

0

0

0

0

0

0

0

0

0

GCE

EC2

Azure

y
c
n
e
d
s
e
r
o
C

i

 
f

o

 
s
e
c
n
a
h
C

 1.2

 1

 0.8

 0.6

 0.4

 0.2

 0

0 1 2 4 8 16 32

0 1 2 4 8 16 32

GCE

EC2

Time lag between victim and attacker instances

(hours)

Figure 16: Chances of co-residency with varying delays be-
tween victim and attacker launches. Solid boxes correspond
to zero delay (simultaneous launches) and gauze-like boxes corre-
spond to 1 hour delay between victim and attacker launches. We
did not observe any co-resident instances for runs with zero delay
on EC2. All these results are from one data center region (EC2:
us-east, GCE: us-central1-a, Azure: East US). Results are over at
least 9 run per run conﬁguration with at least 3 run per time of day.

Delay Mean

S.D. Min Median Max

Success

0+

5 min
1 hr

0.6
1.38
3.57

1.07
0.92
2.59

0
0
0

0
1
3.5

3
3
9

rate
0.30
0.88
0.86

Figure 17: Distribution of number of co-resident pairs and suc-
cess rate or chances of co-residency for shorter delays under
20x20 run conﬁguration in EC2. A delay with 0+ means victim
and attacker instances were launched sequentially, i.e. attacker in-
stances were not launched until all victim instances were running.
The results averaged are over 9 runs with 3 runs per time of day.

(denoted by 0+) could result in co-residency.

The second observation shows that non-zero delay on
GCE and zero delay on Azure increases the chance of co-
residency and hence directly beneﬁts an attacker. It should
be noted that on Azure, the launch delays between victim
and attacker instances were longer than 1 hour due to our
leap-frog experimental methodology; the actual delays be-
tween the VM launches were, on average, 3 hours (with
a maximum delay of 10 hours for few runs). This higher
delay was more common in runs with larger number of
instances as there were signiﬁcantly more false positives,
which required a separate sequential phase to resolve (see
Section 4.2).

We also experimented with longer delays on EC2 and
GCE to understand whether and how quickly the chance
of co-residency drops with increasing delay (results shown
in Figure 18). Contrary to our expectation, we did not ﬁnd
the chance of co-residency to drop to zero even for delays as
high as 16 and 32 hours. We speculate that the reason for
this observation could be that system was under constant
churn where some neighboring VMs on the victim’s ma-
chine were terminated. Note that our leap-frog methodol-
ogy may, in theory, interfere with the VM placement. But it
is noteworthy that we observed increased number of unique

Figure 18: Chances of co-residency over long periods. Across
9 runs over two weeks with 3 runs per time of day. Note that we
only conducted 3 runs for 32 hour delay as opposed to 9 runs for
all other delays.

Chances of Co-residency

Cloud

Morning

02:00 - 10:00

Afternoon
10:00 - 18:00

Night

18:00 - 02:00

GCE
EC2

0.68
0.89

0.61
0.73

0.78
0.6

Figure 19: Effect of time of day. Chances of co-residency when
an attacker changes the launch time of his instances. The results
were aggregated across all run conﬁgurations with 1 hour delay
between victim and attacker launch times. All times are in PT.

co-resident pairs with increasing delays, suggesting fresh
co-residency with victim VMs over longer delays.

Effect of time of day. Prior works have shown that churn
or load is often correlated with the time of day [31]. Our
simple reference placement policy does not have a notion
of load and hence have no effect on time of day. In reality,
with limited number of servers in datacenters and limited
number of capacity per host, load on the system has direct
effect on the placement behavior of any placement policy.
As expected, we observe small effect on VM place-
ment based on the time of day when attacker instances are
launched (results shown in Figure 19). Speciﬁcally, there
is a slightly higher chance of co-residency if the attacker
instances are launched in the early morning for EC2 and at
night for GCE.

5.4 Effect of Data Center Location

All
the above experiments were conducted on rela-
tively popular regions in each cloud (especially true for
EC2 [31]). In this section, we report the results on other
smaller and less popular regions. As the regions are less
popular and have relatively fewer machines, we expect
higher co-residency rates and more co-resident instances.
Figure 20 shows the median number of co-resident VM
pairs placed in these regions alongside the results for pop-
ular regions. The distribution of number of co-resident in-
stances is not shown here in the interest of space.

The main observation from these experiments is that

924  24th USENIX Security Symposium 

USENIX Association

s
r
i
a
P

 
t

i

n
e
d
s
e
r
-
o
C

 
f

o

 
r
e
b
m
u
N
n
a
d
e
M

 

i

s
r
i
a
P

 
t

i

n
e
d
s
e
r
-
o
C

 
f

o
 
r
e
b
m
u
N
n
a
d
e
M

 

i

 10

 8

 6

 4

 2

 0

 8

 7

 6

 5

 4

 3

 2

 1

 0

1

1

1

2

2

3

1

1

1

2

2

3

0

0

0

0

0

0

0

0

0

0

0

0

x

x

x

x

x

x

x

x

x

x

x

x

1

2

3

2

3

3

1

2

3

2

3

3

0

0

0

0

0

0

0

0

0

0

0

0

us-central1-a

europe-west1-b

 

(a) GCE

1

1

1

2

2

3

1

1

1

2

2

3

0

0

0

0

0

0

0

0

0

0

0

0

x

x

x

x

x

x

x

x

x

x

x

x

1

2

3

2

3

3

1

2

3

2

3

3

0

0

0

0

0

0

0

0

0

0

0

0

us-east

us-west-1 (CA)

 

(b) EC2

Figure 20: Median number of co-resident pairs across two re-
gions. The box plot shows the median number of co-resident pairs
excluding co-residency within the same account. Results are over
at least 3 run per run conﬁguration (x-axis).

there is a higher chance of co-residency in these smaller
regions than the larger, more popular regions. Note that we
placed at least one co-resident pair in all the runs in these
regions. Also the higher number of co-resident pairs also
suggests a larger coverage over victim VMs in these smaller
regions.

One anomaly that we found during two 20x20 runs on
EC2 between 30th and 31st of January 2015, when we ob-
served an unusually large number of co-resident instances
(including three VMs from the same account). We believe
this anomaly may be a result of an internal management
incident in the Amazon EC2 us-west-1 region.

5.5 Other Observations

We report several other interesting observations in this sec-
tion. First, we found more than two VMs can be co-resident
on the same host on both Azure and GCE, but not on EC2.
Figure 21 shows the distribution of number of co-resident
instances per host. Particularly, in one of the runs, we
placed 16 VMs on a single host.

Another interesting observation is related to co-resident
instances from the same account. We term them as self-
co-resident instances. We observed many self-co-resident
pairs on GCE and Azure (not shown). On the other hand,
we never noticed any self co-resident pair on EC2 except
for the anomaly in us-west-1. Although we did not notice

y
c
n
e
u
q
e
r
F

 180

 160

 140

 120

 100

 80

 60

 40

 20

 0

169

74

24

6

2

3

4

5

5

6

2

7

1

1

10

16

Number of Coresident Instances per Host

Figure 21: Distribution of number of co-resident instances per
host on Azure. The results shown are across all the runs. We saw
at most 2 instances per host in EC2 and at most 3 instances per
host in GCE.

 250

)
s
e

t

 200

GCE
EC2
Azure

i

u
n
m
n
i
(
 

 

n
o

i
t

a
r
u
D

 
t
s
e
T

 150

 100

 50

 0

1

0

x

1

1

0

x

2

1

0

x

3

2

0

x

2

2

0

x

3

3

0

x

3

0

0

0

0

0

0

Figure 22: Launch strategy and co-residency detection execu-
tion times. The run conﬁgurations v× a indicates the number of
victims vs. number of attackers launched. The error bars show the
standard deviation across at least 7 runs.

Run
conﬁg.
10x10
10x20
10x30
20x20
20x30
30x30

Average Cost ($)

Maximum Cost ($)

GCE
0.137
0.370
1.049
0.770
1.482
1.866

EC2
0.260
0.520
0.780
0.520
1.560
1.560

Azure
0.494
1.171
2.754
2.235
3.792
5.304

GCE
0.140
0.412
1.088
1.595
1.581
2.433

EC2
0.260
0.520
1.560
1.040
1.560
1.560

Azure
0.819
1.358
3.257
3.255
4.420
7.965

Figure 23: Cost of running a launch strategy (in dollars). Max-
imum cost column refers to the maximum cost we incurred out
of all the runs for that particular conﬁguration and cloud provider.
The cost per hour of small instances at the time of this study were:
0.05, 0.026 and 0.06 dollars for GCE, EC2 and Azure, respec-
tively. The minimum and maximum costs are in bold.

any effect on the actual chance of co-residence, we believe
such placement behaviors (or the lack of) may affect VM
placement.

We also experimented with medium instances and suc-
cessfully placed few co-located VMs on both EC2 and
GCE by employing similar successful strategies learned
with small instances.

USENIX Association  

24th USENIX Security Symposium  925

Run Conﬁg.
Pr [ Ev
a > 0 ]

10x10
0.10

10x20
0.18

10x30
0.26

20x20
0.33

20x30
0.45

30x30
0.60

Figure 24: Probability of co-residency under the reference
placement policy.

5.6 Cost of Launch Strategies
Recall that the cost of a launch strategy from Section 3,
CS = a∗ P(atype)∗ Td(v,a). In order to calculate this cost,
we need Td(v,a) which is the time taken to detect co-
location with a attackers and v victims. Figure 22 shows
the average time taken to complete launching attacker in-
stances and complete co-residency detection for each run
conﬁguration. Here the measured co-residency detection is
the parallelized version discussed in Section 4.2 and also
includes time taken to detect co-residency within each ten-
ant account. Hence, for these reasons the time to detect
co-location is an upper bound for a realistic and highly op-
timized co-residency detection mechanism.

We calculate the cost of executing each launch strat-
egy under the three public clouds. The result is summa-
rized in Figure 23. Note that we only consider the cost in-
curred by the compute instances because the cost for other
resources such as network and storage, was insigniﬁcant.
Also note that EC2 bills every hour even if an instance runs
less than an hour [16], whereas GCE and Azure charge per
minute of instance activity. This difference is considered
in our cost calculation. Overall, the maximum cost we in-
curred was about $8 for running 30 VMs for 4 hours 25
minutes on Azure and a minimum of 14 cents on GCE for
running 10 VMs for 17 minutes. We incurred the highest
cost for all the launch strategies in Azure because of overall
higher cost per hour and partly due to longer tests due to
our co-residency detection methodology.

5.7 Summary of Placement Vulnerabilities
In this section, we return to the secure reference placement
policy introduced in Section 3 and use it to identify place-
ment vulnerabilities across all the three clouds. Recall that
the probability of at least one pair of co-residency under
this random placement policy is given by Pr [ Ev
a > 0 ] =
1− (1− v/N)a, where Ev
a is the random variable denoting
the number of co-location observed when placing a attacker
VMs among N = 1000 total machines where v machines
are already picked for the v victim VMs. First, we evaluate
this probability for various run conﬁgurations that we ex-
perimented with in the public clouds. The probabilities are
shown in Figure 24.

Recall that a launch strategy in a cloud implies a place-
ment vulnerability in that cloud’s placement policy if its
normalized success rate is greater than 1. The normalized
success rate of the strategy is the ratio of the chance of co-
location under that launch strategy to the probability of co-
location in the reference policy (Pr [ Ev
a > 0 ]). Below is a
list of selected launch strategies that escalate to placement

Strategy
S1 & S2

S3
S4(i)
S4(ii)

S5

v & a

10
30
20
20
20

a(cid:29)
688
227
105
342
110

Cost beneﬁt ($)

Normalized Success

113.87
32.75
4.36
53.76
4.83

10
1.67
2.67
3.03
1.48

Figure 25: Cost beneﬁt analysis. N = 1000, P(atype) = 0.026,
which is the cost per instance-hour on EC2 (the cheapest). For
simplicity Td(v,a) = (v∗ a)∗ 3.85, where 3.85 is fastest average
time to detect co-residency per instance-pair. Here, v × a is the
run conﬁguration of the strategy under test. Note that the cost
beneﬁt is the additional cost incurred under the reference policy,
hence is equal to cost incurred by a(cid:29) − a additional VMs.

vulnerabilities using our reference policy with their normal-
ized success rate in parenthesis.

(S1) In Azure, launch ten attacker VMs closely after the

victim VMs are launched (1.0/0.10).

(S2) In EC2 and GCE, if there are known victims in any of
the smaller datacenters, launch at least ten attacker VMs
with a non-zero delay (1.0/0.10).

(S3) In all three clouds, launch 30 attacker instances, either
with no delay (Azure) or one hour delay (EC2, GCE)
from victim launch, to get co-located with one of the 30
victim instances (1.00/0.60).

(S4) (i) In Amazon EC2, launch 20 attacker VMs with a
delay of 5 minutes or more after the victims are launched
(0.88/0.33). (ii) The optimal delay between victim and
attacker VM launches is around 4 hours for a 20x20 run
(1.00/0.33).

(S5) In Amazon EC2, launch the attacker VMs with 1 hour
after the victim VMs are launched where the time of day
falls in the early morning, i.e., 02:00 to 10:00hrs PST
(0.89/0.60).

Cost beneﬁt. Next, we quantify the cost beneﬁt of each
of these strategies over the reference policy. As the success
rate of any launch strategy on a vulnerable placement pol-
icy is greater than what is possible in the reference policy,
we need more attacker instances in the reference policy to
achieve the same success rate. We calculate this number
of attacker instances a(cid:29) using: a(cid:29) = ln(1−Sv
a)/ln(1− v/N),
where, Sv
a is the success rate of a strategy with run conﬁg-
uration of v× a. The result of this calculation is presented
in Figure 25. The result shows that the best strategy, S1 and
S2, on all three cloud providers is $114 cheaper than what
is possible in the reference policy.

It is also evident that these metrics enable evaluating and
comparing various launch strategies and their efﬁcacy on
various placement policies both on robust placements and
attack cost. For example, note that although the normal-
ized success rate of S3 is lower than S4, it has a higher cost
beneﬁt for the attacker.

926  24th USENIX Security Symposium 

USENIX Association

5.8 Limitations

Although we exhaustively experimented with a variety of
placement variables, the results have limitations. One ma-
jor limitation of this study is the number of placement vari-
ables and the set of values for the variables that we used to
experiment. For example, we limited our experiments with
only one instance type, one availability zone per region and
used only one account for the victim VMs. Although differ-
ent instance types may exhibit different placement behav-
ior, the presented results still holds strong for the chosen
instance type. The only caveat that may affect the results is
if the placement policy use user account ID for VM place-
ment decisions. Since, we experimented with only one vic-
tim account (separate from the designated attacker account)
across all providers, these results, in the worst case, may
have captured the placement behavior of an unlucky vic-
tim account that was subject to similar placement decisions
(and hence co-resident) as that of the VMs from the desig-
nated attacker account.

Even though we ran at least 190 runs per cloud provider
over a period of 3 months to increase statistical signiﬁcant
of our results, we were still limited to at most 9 runs per run
conﬁguration (with 3 runs per time of day). These limita-
tions have only minor bearing on the results presented, if at
all any, and the reported results are signiﬁcant and impact-
ful for cloud computing security research.

6 Related Work
VM placement vulnerability studies. Ristenpart et
al. [29] ﬁrst studied the placement vulnerability in public
clouds, which showed that a malicious cloud tenant may
place one of his VMs on the same machine as a target
VM with high probability. Placement vulnerabilities ex-
ploited in their study include publicly available mapping of
VM’s public/internal IP addresses, disclosure of Dom0 IP
addresses, and a shortcut communication path between co-
resident VMs. Their study was followed by Xu et al. [33]
and further extended by Herzberg et al. [25]. However, the
results of these studies have been outdated by the recent
development of cloud technologies, which is the main mo-
tivation of our work.

Concurrent with our work, Xu et al. [34] conducted a sys-
tematic measurement study of co-resident threats in Ama-
zon EC2. Their focus, however, is in-depth evaluation
of co-residency detection using network route traces and
quantiﬁcation of co-residence threats on older generation
instances with EC2’s classic networking (prior to Amazon
VPC). In contrast, we study placement vulnerabilities in the
context of VPC on EC2, as well as on Azure and GCE. The
two studies are mostly complementary and strengthen the
arguments made by each other.

New VM placement policies to defend against placement
attacks have been studied by Han et al. [23, 24] and Azar
et al. [18]. It is unclear, however, whether their proposed

policies work against the performance and reliability goals
of public cloud providers.

Co-residency detection techniques. Techniques for co-
residency detection have been studied in various contexts.
We categorize these techniques into one of the two classes:
side-channel approaches to detecting co-residency with un-
cooperative VMs and covert-channel approaches to detect-
ing co-residency with cooperative VMs.

Side-channels allow one party to exﬁltrate secret infor-
mation from another; therefore these approaches may be
adapted in practical placement attack scenarios with targets
not controlled by the attackers. Network round-trip timing
side-channel was used by Ristenpart et al. [29] to detect
co-residency. Zhang et al. [36] developed a system called
HomeAlone to enable VMs to detect third-party VMs us-
ing timing side-channels in the last level caches. Bates et
al. [19] proposed a side-channel for co-residency detection
by causing network trafﬁc congestion in the host NICs from
attacker-controlled VMs; the interference of target VM’s
performance, if the two VMs are co-resident, should be de-
tectable by remote clients. Kohno et al. [27] explored tech-
niques to ﬁngerprint remote machines using timestamps in
TCP or ICMP based network probes, although their ap-
proach was not designed for co-residency detection. How-
ever, none of these approaches works effectively in modern
cloud infrastructures.

Covert-channels on shared hardware components can
be used for co-residency detection when the pair of VMs
are cooperative. Coarse-grained covert-channels in CPU
caches and hard disk drives were used in Ristenpart et
al. [29] for co-residency conﬁrmation. Xu et al. [33] estab-
lished covert-channels in shared last level caches between
two colluding VMs in the public clouds. Wu el al. [32]
exploited memory bus as a covert-channel on modern x86
processors, in which the sender issues atomic operations
on memory blocks spanning multiple cache lines to cause
memory bus locking or similar effects on recent processors.
However, covert-channels proposed in the latter two studies
were not designed for co-residency detection, while those
developed in our work is tuned for such purposes.

7 Conclusion and Future Work

Multi-tenancy in public clouds enable co-residency attacks.
In this paper, we revisited the problem of placement —
can an attacker achieve co-location? — in modern public
clouds. We ﬁnd that while past techniques for verifying co-
location no longer work, insufﬁcient performance isolation
in hardware still allows detection of co-location. Further-
more, we show that in the three popular cloud providers
(EC2, GCE and Azure), achieving co-location is surpris-
ingly simple and cheap. It is even simpler and costs noth-
ing to achieve co-location in some PaaS clouds. Our results
demonstrate that even though cloud providers have massive
datacenters with numerous physical servers, the chances

USENIX Association  

24th USENIX Security Symposium  927

of co-location are far higher than expected. More work is
needed to achieve a better balance of efﬁciency and security
using smarter co-location-aware placement policies.

Acknowledgments

This work was funded by the National Science Founda-
tion under grants CNS-1330308, CNS-1546033 and CNS-
1065134. Swift has a signiﬁcant ﬁnancial interest in Mi-
crosoft Corp.

References
[1] Amazon ec2 instance store.

AWSEC2/latest/UserGuide/InstanceStorage.html.

http://docs.aws.amazon.com/

[2] Amazon elastic compute cloud. http://aws.amazon.com/ec2/.
[3] Apache libcloud. http://libcloud.apache.org/.
[4] Aws

http://aws.amazon.com/

beanstalk.

elastic

elasticbeanstalk/.

[5] Aws innovation at scale,

re:invent 2014, slide 9-10.

http:

//www.slideshare.net/AmazonWebServices/spot301-
aws-innovation-at-scale-aws-reinvent-2014.

[6] Google

compute

compute/.

engine.

https://cloud.google.com/

[7] Google compute engine – disks. https://cloud.google.com/

compute/docs/disks/.

[8] Google compute enginer autoscaler. http://cloud.google.com/

compute/docs/autoscaler/.

[9] Haproxy: The reliable, high performance tcp/http load balancer.

http://www.haproxy.org/.

[10] Heroku devcenter: Dynos and the dyno manager,

ip ad-
dresses. https://devcenter.heroku.com/articles/dynos#
ip-addresses.

[11] Intel

Ivy Bridge cache replacement policy.

http://blog.

stuffedcow.net/2013/01/ivb-cache-replacement/.

[12] Olio workload.

https://cwiki.apache.org/confluence/

display/OLIO/The+Workload.

[13] Rightscale. http://www.rightscale.com.
[14] Virtual machine and cloud service sizes for azure. https://msdn.

microsoft.com/en-us/library/azure/dn197896.aspx.

[15] Windows azure. http://www.windowsazure.com/.
[16] Amazon ec2 pricing, 2015.

http://aws.amazon.com/ec2/

pricing/.

[17] Amazon Web Services. Extend your it infrastructure with amazon

virtual private cloud. Technical report, Amazon, 2013.

[18] Y. Azar, S. Kamara, I. Menache, M. Raykova, and B. Shepard. Co-
location-resistant clouds. In In Proceedings of the ACM Workshop
on Cloud Computing Security, pages 9–20, 2014.

[19] A. Bates, B. Mood, J. Pletcher, H. Pruse, M. Valafar, and K. But-
ler. Detecting co-residency with active trafﬁc analysis techniques.
In Proceedings of the 2012 ACM Workshop on Cloud Computing
Security Workshop, pages 1–12. ACM, 2012.

[20] B. Beach. Virtual private cloud. In Pro Powershell for Amazon Web

Services, pages 67–88. Springer, 2014.

[21] B. Farley, A. Juels, V. Varadarajan, T. Ristenpart, K. D. Bowers,
and M. M. Swift. More for your money: Exploiting performance
heterogeneity in public clouds.
In Proceedings of the Third ACM
Symposium on Cloud Computing. ACM, 2012.

[22] M. Ferdman, A. Adileh, O. Kocberber, S. Volos, M. Alisafaee,
D. Jevdjic, C. Kaynak, A. D. Popescu, A. Ailamaki, and B. Falsaﬁ.
Clearing the clouds: a study of emerging scale-out workloads on
modern hardware. In Proceedings of the seventeenth international
conference on Architectural Support for Programming Languages
and Operating Systems. ACM, 2012.

[23] Y. Han, T. Alpcan, J. Chan, and C. Leckie. Security games for vir-
tual machine allocation in cloud computing. In Decision and Game
Theory for Security. Springer International Publishing, 2013.

[24] Y. Han, J. Chan, T. Alpcan, and C. Leckie. Virtual machine alloca-
tion policies against co-resident attacks in cloud computing. In IEEE
International Conference on Communications,, 2014.

[25] A. Herzberg, H. Shulman, J. Ullrich, and E. Weippl. Cloudoscopy:
Services discovery and topology mapping. In 2013 ACM Workshop
on Cloud Computing Security Workshop, pages 113–122, 2013.

[26] D. Kanter.

L3 cache and ring interconnect.

realworldtech.com/sandy-bridge/8/.

http://www.

[27] T. Kohno, A. Broido, and K. Claffy. Remote physical device ﬁnger-
printing. In Security and Privacy, 2005 IEEE Symposium on, pages
211–225. IEEE, 2005.

[28] D. Mosberger and T. Jin. httperfa tool for measuring web server
performance. ACM SIGMETRICS Performance Evaluation Review,
26(3):31–37, 1998.

[29] T. Ristenpart, E. Tromer, H. Shacham, and S. Savage. Hey, you, get
off of my cloud: Exploring information leakage in third-party com-
pute clouds. In Proceedings of the 16th ACM conference on Com-
puter and communications security, pages 199–212. ACM, 2009.

[30] V. Varadarajan, T. Kooburat, B. Farley, T. Ristenpart, and M. M.
Swift. Resource-freeing attacks: Improve your cloud performance
(at your neighbor’s expense). In Proceedings of the 2012 ACM con-
ference on Computer and communications security, pages 281–292.
ACM, 2012.

[31] L. Wang, A. Nappa, J. Caballero, T. Ristenpart, and A. Akella.
Whowas: A platform for measuring web deployments on IaaS
clouds.
In Proceedings of the 2014 Conference on Internet Mea-
surement Conference, pages 101–114. ACM, 2014.

[32] Z. Wu, Z. Xu, and H. Wang. Whispers in the hyper-space: High-
speed covert channel attacks in the cloud. In USENIX Security sym-
posium, pages 159–173, 2012.

[33] Y. Xu, M. Bailey, F. Jahanian, K. Joshi, M. Hiltunen, and R. Schlicht-
ing. An exploration of L2 cache covert channels in virtualized en-
vironments.
In Proceedings of the 3rd ACM workshop on Cloud
computing security workshop, pages 29–40. ACM, 2011.

[34] Z. Xu, H. Wang, and Z. Wu. A measurement study on co-residence

threat inside the cloud. In USENIX Security Symposium, 2015.

[35] Y. Yarom and K. Falkner. Flush+reload: A high resolution, low
noise, L3 cache side-channel attack. In 23rd USENIX Security Sym-
posium, pages 719–732. USENIX Association, 2014.

[36] Y. Zhang, A. Juels, A. Oprea, and M. K. Reiter. Homealone: Co-
residency detection in the cloud via side-channel analysis. In Pro-
ceedings of the 2011 IEEE Symposium on Security and Privacy,
pages 313–328. IEEE Computer Society, 2011.

[37] Y. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. Cross-VM side
channels and their use to extract private keys. In Proceedings of the
2012 ACM conference on Computer and communications security,
pages 305–316. ACM, 2012.

[38] Y. Zhang, A. Juels, M. K. Reiter, and T. Ristenpart. Cross-tenant
side-channel attacks in PaaS clouds.
In Proceedings of the 2014
ACM SIGSAC Conference on Computer and Communications Secu-
rity, pages 990–1003. ACM, 2014.

[39] F. Zhou, M. Goel, P. Desnoyers, and R. Sundaram. Scheduler vul-
nerabilities and attacks in cloud computing. CoRR, abs/1103.0759,
2011.

928  24th USENIX Security Symposium 

USENIX Association

