Practical Dynamic Proofs of Retrievability

Elaine Shi

University of Maryland
elaine@cs.umd.edu

Emil Stefanov

UC Berkeley

emil@cs.berkeley.edu

Charalampos Papamanthou

University of Maryland
cpap@umd.edu

ABSTRACT
Proofs of Retrievability (PoR), proposed by Juels and Kaliski
in 2007, enable a client to store n ﬁle blocks with a cloud
server so that later the server can prove possession of all the
data in a very eﬃcient manner (i.e., with constant computa-
tion and bandwidth). Although many eﬃcient PoR schemes
for static data have been constructed, only two dynamic PoR
schemes exist. The scheme by Stefanov et al. (ACSAC 2012)
uses a large of amount of client storage and has a large audit
cost. The scheme by Cash et al. (EUROCRYPT 2013) is
mostly of theoretical interest, as it employs Oblivious RAM
(ORAM) as a black box, leading to increased practical over-
head (e.g., it requires about 300 times more bandwidth than
our construction).

We propose a dynamic PoR scheme with constant client
storage whose bandwidth cost is comparable to a Merkle
hash tree, thus being very practical. Our construction out-
performs the constructions of Stefanov et al. and Cash et al.,
both in theory and in practice. Speciﬁcally, for n outsourced
blocks of β bits each, writing a block requires β + O(λ log n)
bandwidth and O(β log n) server computation (λ is the se-
curity parameter). Audits are also very eﬃcient, requiring
β + O(λ2 log n) bandwidth. We also show how to make our
scheme publicly veriﬁable, providing the ﬁrst dynamic PoR
scheme with such a property. We ﬁnally provide a very eﬃ-
cient implementation of our scheme.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection
Keywords
dynamic proofs of retrievability; PoR; erasure code

1.

INTRODUCTION

Storage outsourcing (e.g., Amazon S3, Google Drive) has
become one of the most popular applications of cloud com-
puting, oﬀering various beneﬁts such as economies of scale

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516669.

and ﬂexible accessibility. When the cloud storage provider
(also referred to as server) is untrusted, an important chal-
lenge is how to oﬀer provable outsourced storage guarantees.
In particular, a data owner (also referred to as client) wishes
to obtain the following guarantees:
• Authenticated storage. The client wishes to verify
that data fetched from the server is correct, where cor-
rectness is equivalent to authenticity and freshness;
• Retrievability. The client needs assurance that the
server is indeed storing all of the client’s data, and that
no data loss has occurred.

Proofs of Retrievability (PoR), initially deﬁned and pro-
posed by Juels and Kalisky [14], are designed to oﬀer the
above guarantees for storage outsourcing applications, while
requiring small client-side state.

Most existing PoR schemes [7–9, 20, 26] however, are im-
practical due to the prohibitive costs associated with data
updates or client storage. A recent PoR construction by
Cash et al. [8] shows how to achieve asymptotic eﬃciency
in the presence of updates with constant client storage—
however, their scheme relies on Oblivious RAM (ORAM), a
rather heavy-weight cryptographic primitive. Recent works
on ORAM [21,24,25,30,31] have shown that even the fastest
known constructions incur a large bandwidth overhead in
practice. In particular, under typical parametrizations, and
with O(1) amount of client storage, known ORAM construc-
tions require 400+ blocks be transferred between a client
and a server for a single data access. In contrast, our con-
struction requires transferring only about 1.05 to
1.35 blocks per data access, which makes it both practi-
cal and orders of magnitudes more eﬃcient than the scheme
of Cash et al. [8].

This paper proposes a light-weight dynamic PoR construc-
tion that achieves comparable bandwidth overhead and client-
side computation with a standard Merkle hash tree, reducing
the above cost dramatically. Speciﬁcally, for each read and
write, our construction requires transferring O(λ log n) bits
of cryptographic information (independent of the block size)
in addition to transferring the block itself, where λ is the se-
curity parameter, and n is the total number of outsourced
blocks. To understand the implications of this, note that a
Merkle hash tree oﬀers authenticated storage, but does not
oﬀer retrievability guarantees. This paper essentially shows
that under settings where the client-server bandwidth is the
bottleneck (as is often the case in practice—e.g., Bitcoin
and various ﬁle sharing applications [1]), we can make PoR
almost as eﬃcient as a Merkle hash tree, i.e., ready to be
deployed in practical applications today.

325Scheme

Iris [26]

Cash et al. [8]

This paper
(Section 5)
This paper
(Appendix)

Client storage

√

O(β

n)

Write cost

Server cost

O(β)

BW
O(β)

Audit cost

√

Server cost
O(βλ
n)

√
BW
n)

O(βλ

O(β)

O(β)

O(βλ(log n)2)

O(βλ(log n)2)

O(βλ(log n)2) O(βλ(log n)2)

O(β log n)

β + O(λ log n)

O(βλ log n)

β + O(λ2 log n)

Veriﬁability

secret
secret

secret

O(βλ)

O(β log n)

β(1 + ) + O(λ log n)
for any constant  > 0

O(βλ log n)

O(βλ log n)

public

β: block size (in terms of number of bits).

λ: the security parameter.

n: an upper bound on the number of blocks.

Server cost: includes the server-side disk I/O cost, and server computation.

BW: The client-server bandwidth cost.

Table 1: Comparison with existing dynamic PoR. All schemes have O(βn) server side storage. Reads can be handled
by keeping a separate, Merkle-hashed, up-to-date copy of the dataset; therefore each read costs β + O(λ log n). Numbers in
this table are optimized for the typical case β ≥ λ. We note that for the ORAM-based scheme of Cash et al., we omit log log n
terms from the denominator.

In terms of disk I/O overhead on the server, our scheme
also achieves asymptotic improvements for reads, writes,
as well as audits, in comparison with the state-of-the-art
scheme [8] (see Table 1). In our scheme, reads cost no more
than a Merkle hash tree in terms of server disk I/O. Writes
incur moderate server disk I/O: the server needs to read,
write, and compute on O(log n) blocks for each write. How-
ever, our algorithms for writing access blocks sequentially,
signiﬁcantly reducing the disk seeks required for writes. We
have a full-ﬂedged, working implementation of our scheme
and report detailed experimental results from a deployment.
We also plan to open source our code in the near future.

We also point out that due to the blackbox application
of ORAM, the scheme by Cash et al. [8] additionally oﬀers
access pattern privacy which we do not guarantee. In ap-
plications that demand access privacy, ORAM is necessary.
We observe that the deﬁnition of PoR itself does not require
access privacy, and when one requires only PoR guarantees
(i.e., authenticated storage and retrievability) but not access
privacy, a blackbox application of ORAM is impractical—
in fact, as this paper shows, one can design truly practical
PoR schemes when access privacy is not required. Table 1
summarizes the asymptotic performance of our scheme in
comparison with related work.

1.1 Related Work
Comparison with proofs of data possession. A closely
related line of research is called Proofs of Data Possession
(PDP), initially proposed by Ateniese et al. [5]. We stress
that PDP provides much weaker security guarantees than
PoR. A successful PoR audit ensures that the server main-
tains knowledge of all outsourced data blocks; while a PDP
audit only ensures that the server is storing most of the
data. With PDP, a server that has lost a small number of
data blocks can pass an audit with signiﬁcant probability1.
Erway et al. [10] recently demonstrated a dynamic PDP
scheme with β + O(λ log n) cost for reads and writes, and
β +O(λ2 log n) cost for audits. We stress that we provide the
much stronger PoR guarantees, roughly at the same prac-
tical and asymptotic overhead as dynamic PDP schemes.

1While some PDP schemes [6] achieve full security, they re-
quire that the server read all of the client’s data during an
audit, and thus is impractical.

Proofs of retrievability. Static proofs of retrievability
were initially proposed by Juels and Kaliski [14], and later
improved in a series of subsequent works [7–9, 17, 20, 26, 28,
32]. While some works [17, 28, 32] aim to achieve PoR, they
essentially only achieve the weaker PDP guarantees when
they wish to support dynamic updates eﬃciently.

The ﬁrst dynamic proofs of retrievability construction was
proposed by Stefanov, van Dijk, Juels, and Oprea as part of
a cloud-based ﬁle system called Iris [26]. Although reads
n)
and writes in Iris are quite eﬃcient, it requires O(λβ
bandwidth, server computation, and server I/O to perform
an audit (it also requires β
n local space). Cash et al. [8]
proposed a dynamic POR scheme with constant client stor-
age based on Oblivious RAM, but it requires O(βλ(log n)2)
bandwidth and server I/O to perform writes and audits. In
contrast with these works, our scheme requires β+O(λ log n)
write bandwidth, β + O(λ2 log n) audit bandwidth, and con-
stant storage.

√

√

INFORMAL TECHNICAL OVERVIEW

2.
2.1 Previous Approaches

In this section we describe various approaches that could
be used for the problem of dynamic proofs of retrievability
and we highlight the problems of these approaches.

Strawman. We start from the most straightforward ap-
proach. Imagine that the client attaches a Message Authen-
tication Code (MAC) to every block before uploading it to
the server—to additionally ensure freshness under updates,
one can use a Merkle hash tree instead of MACs. During
the audit protocol, the client randomly samples a small num-
ber of blocks and ensures that the server possesses them by
checking them against the MACs.

In fact, this approach illustrates the underlying idea of
several Proof of Data Possession (PDP) schemes [10, 29].
The drawback of such an approach is that if the server has
lost a small number of blocks (e.g., a o(1) fraction), it can
still pass the audit with signiﬁcant probability.

Prior work: use of redundant encoding to boost de-
tection probability. To address the aforementioned issue,
prior PoR schemes [7, 9, 14, 20, 26] rely on erasure codes to
boost the detection probability, and ensure that the server
must possess all blocks to pass the audit test, which typ-
ically involves checking the authenticity of λ random code

326blocks, where λ is the security parameter. As a concrete
example, suppose that the client outsources a total of n
blocks, which are erasure coded into m = (1 + c)n blocks
for some constant 0 < c ≤ 1, such that knowledge of any n
blocks suﬃces to recover the entire dataset. In this way, the
server has to delete at least cn blocks to actually incur data
loss—however, if the server deletes that many blocks, it will
fail the above audit protocol with overwhelming probability
(speciﬁcally with probability at least 1 − 1/(1 + c)λ).
Diﬃculty with updates. The issue with using erasure
codes is that if the client wishes to update a single block,
it has to additionally update cn parity blocks—this can be
very expensive in practice.

One (failed) attempt to support updates (as described by
Cash et al. [8]) is to use a local erasure coding scheme, i.e.,
each block only aﬀects a small number of codeword blocks.
For this approach to work, all the codeword blocks need
to be randomly permuted, such that the server does not
learn which codeword blocks correspond to one original data
block—since otherwise, the server can selectively delete all
codeword blocks corresponding to one block (say block i).
Since the server has deleted only a small number of blocks,
it can pass the audit with signiﬁcant probability—however,
in this case, block i would become irrecoverable.

The above approach fails to address the eﬃcient update
problem, since whenever the client updates a block, it re-
veals to the server which codeword blocks are related to this
block—and this allows the server to launch the selective
deletion attack as described above. To address this issue,
Cash et al. [8] propose to employ ORAM to hide the access
patterns from the server during updates. However, as men-
tioned in Section 1, known ORAM schemes incurs 400X or
higher bandwidth overhead under typical parameterizations
and constant client-side storage, and thus is impractical.

2.2 Our Idea

As before, suppose the client has n blocks, which are
erasure-coded into n + cn blocks for some small constant
c > 0—we denote the erasure coded copy of data as C. Now
if the client needs to update a block, we encounter the issue
that the client needs to update all of the cn parity blocks.

Our idea is to avoid the need to immediately update the
cn parity blocks upon writes. Instead, the client will place
the newly updated block into an erasure-coded log structure
denoted H, containing recently written blocks. During the
audit, the client will sample blocks to check not only from
the buﬀer C, but also from the log structure H. Note that
since the buﬀer C does not get updated immediately upon
writes, it may contain stale data—however, an up-to-date
snapshot of all blocks is recoverable from the combination of
C and H, both of which are probabilistically checked during
the audit. Two questions however remain:
1. How can reads be supported eﬃciently if the location of
the up-to-date copy of a block is undetermined—since it can
either exist in the buﬀer C, or in the log structure H?

The answer to this ﬁrst question is relatively straight-
forward: one can always keep a separate, up-to-date, and
memory-checked copy of all blocks just to support eﬃcient
reads. The client can verify the integrity (i.e., authentic-
ity and fresheness) of the reads facilitated by the memory
checking scheme (i.e., a Merkle hash tree).
In our basic
construction described in Section 4, this separate copy is

denoted with U. Section 5 describes further optimizations
to reduce the server-side storage by a constant factor.
2. How can the log structure H be eﬃciently updated upon
writes?

The answer to the latter question is much more involved,
and turns out to be the main technical challenge we needed
to overcome. Intuitively, when the log structure H is small,
updating it upon writes should be relatively eﬃcient. How-
ever, as H grows larger, updating it becomes slower. For
example, in the extreme case, when H grows to as large
as C, updating H upon a write would cause an overhead
roughly proportional to cn, as mentioned above.

Perhaps unsurprisingly, in order to achieve eﬃcient amor-
tized cost for updating H upon writes, we use a hierarchical
log structure that is reminiscent of Oblivious RAM construc-
tions [11]. In our construction, the log structure H consists
of exactly (cid:98)log n(cid:99) + 1 levels of exponentially growing capac-
ity, where level i is an erasure coded copy of 2i blocks. At a
very high level, every 2i write operations, level i will be re-
built. Finally, the erasure-coded copy C can be informally
(and a bit imprecisely) thought of as the top level of the
hierarchical log, and is rebuilt every n write operations.

Despite the superﬁcial resemblance to ORAM, our con-
struction is fundamentally diﬀerent from using ORAM as a
blackbox, and thus orders of magnitude more eﬃcient, since
1) we do not aim to achieve access privacy, or rely on access
privacy to prove our PoR guarantees like in the scheme by
Cash et al. [8]; and 2) each level of our hierarchical log struc-
ture H is erasure-coded. For this reason, we need a special
erasure coding scheme that can be incrementally built over
time (see Section 4 for details).

3. PRELIMINARIES

We begin with the deﬁnition of a dynamic PoR scheme,
as given by Cash, K¨up¸c¨u, and Wichs [8]. A dynamic POR
scheme is a collection of the following four protocols between
a stateful client C and a stateful server S.
(st, ¯M) ← Init(1λ, n, β,M): On input the security param-
eter λ and the database M of n β-bit-size entries, it
outputs the client state st and the server state ¯M.

{B, reject} ← Read(i, st, ¯M): On input an index i ∈ [n],
the client state st and the server state ¯M, it outputs
B = M[i] or reject.

{(st(cid:48), ¯M(cid:48)), reject} ← Write(i, B, st, ¯M): On input an index
i ∈ [n], data B, the client state st and the server state
¯M, it sets M[i] = B and outputs a new client state
st(cid:48) and a new server state ¯M(cid:48) or reject.

{accept, reject} ← Audit(st, ¯M): On input the client state
st and the server state ¯M, it outputs accept or reject.

Depending on whether the client state st in the above deﬁ-
nition must be kept secret or not, we say that the dynamic
PoR scheme is secretly or publicly veriﬁable.
3.1 Security Deﬁnitions

We deﬁne security, namely authenticity and retrievability

in the same way as Cash et al. [8].

Authenticity. The authenticity requirement stipulates that
the client can always detect (except with negligible probabil-
ity) if any message sent by the server deviates from honest

327behavior. We use the following game between a challenger C,
a malicious server S∗ and an honest server S for the adaptive
version of authenticity, in the same way as Cash et al. [8].
• S∗ chooses initial memory M. C runs Init(1λ, n, β,M)
and sends the initial memory layout ¯M to S∗. C also
interacts with S and sends ¯M to S.

• For polynomial number of steps t = 1, 2, . . . , poly(λ),
S∗ picks an operation opt where operation opt is either
Read(i, st, ¯M) or Write(i, B, st, ¯M) or Audit(st, ¯M). C
executes the protocol with both S∗ and S.

S∗ is said to win the game, if at any time, the message sent
by S∗ diﬀers from that of S, and C did not output reject.

Definition 1

(Authenticity). A PoR scheme satis-
ﬁes adaptive authenticity, if no polynomial-time adversary
S∗ has more than negligible probability in winning the above
security game.

Retrievability. Intuitively, the retrievability requirement
stipulates that whenever a malicious server can pass the
audit test with non-negligible probability, the server must
know the entire memory contents M; and moreover, one is
able to extract M by repeatedly running the Audit protocol
with the server. Formally, retrievability is deﬁned with a
security game as below, in the same way as Cash et al. [8].
• Initialization phase. The adversary S∗ chooses the
initial memory M. The challenger runs Init(1λ, n, β,M),
and uploads the initial memory layout ¯M to S∗.

• Query phase. For t = 1, 2, . . . , poly(λ), the adversary
S∗ adaptively chooses an operation opt where opt is of the
form Read(i, st, ¯M), Write(i, B, st, ¯M), or Audit(st, ¯M).
The challenger executes the respective protocols with S∗.
At the end of the query phase, suppose the state of the
challenger and the adversary is state C and state S respec-
tively and the ﬁnal state of the memory contents is M.
• Challenge phase. The challenger now gets blackbox
rewinding access in the conﬁguration state S. The chal-
lenger runs the Audit protocol repeatedly for a polynomial
number of times with the server S∗, starting from the con-
ﬁguration (state C , state S). Denote with π1, π2, . . . , πpoly(λ)
the transcripts of all the successful Audit executions.

Definition 2

(Retrievability). A PoR scheme sat-
isﬁes retrievability, if there exists a polynomial-time extrac-
tor algorithm denoted M(cid:48) ← Extract(state C ,{πi}), such that
for any polynomial-time S∗, if S∗ passes the Audit proto-
col with non-negligible probability, then after executing the
Audit protocol with S∗ for a polynomial number of times,
the Extract algorithm can output the correct memory con-
tents M(cid:48) = M except with negligible probability.

We note here that in the above deﬁnition, “correct memory
contents” are the contents of the memory M after the end
of the query phase in the security game.
3.2 Building Blocks
Erasure codes. Our construction makes use of erasure
codes [27] as deﬁned below.

Figure 1: Server-side storage layout.

Definition 3

(Erasure codes). Let Σ denote a ﬁnite
alphabet. An (m, n, d)Σ erasure code is deﬁned to be a pair of
algorithms encode : Σn → Σm, and decode : Σm−d+1 → Σn,
such that as long as the number of erasures is bounded by
d − 1, decode can always recover the original data. A code
is maximum distance separable (MDS), if n + d = m + 1.

Authenticated structures. For describing our basic con-
struction, we can assume that we use a standard Merkle
hash tree [16] to ensure the authenticity and freshness of all
blocks (encoded or unencoded, stored across buﬀers U, C
and H). However, later, we will show that in fact, only the
raw buﬀer U needs to be veriﬁed with a Merkle hash tree;
whereas the erasure-coded copy C and the hierarchical log
structure H only require time/location-dependent MACs to
ensure authenticity and freshness. This will lead to signiﬁ-
cant savings (speciﬁcally, by a multiplicative log n factor).

4. BASIC CONSTRUCTION

To begin with, assume the data to be outsourced contains
n blocks, and each block is from an alphabet Σ. For simplic-
ity, we will ﬁrst assume that n is determined in advance—we
will later explain in Section 6 how to expand or shrink the
storage.

We ﬁrst describe a basic construction that requires the
client to perform O(β log n) computation for each block writ-
ten (recall β is the size of the block). Later in Section 5 we
will describe how to rely on homomorphic checksums to re-
duce this cost to β + O(log n).
4.1 Server-Side Storage Layout

The server-side storage is organized in three diﬀerent buﬀers

denoted with U (stands for unencoded ), C (stands for coded )
and H (stands for hierarchical ). We now explain in detail
the function of these buﬀers (see also Figure 1).

Raw buﬀer. All up-to-date blocks are stored in original,
unencoded format in a buﬀer called U. Reads are performed
by reading the corresponding location in U. Writes update
the corresponding location in the buﬀer U immediately with
the newly written block. However, unlike reads, writes also
cause updates to a hierarchical log as explained later.

Erasure-coded copy.
In addition, we store an (m, n, d)
erasure-coded copy of the data in a buﬀer C, where m =
Θ(n), and d = m− n + 1 = Θ(n), i.e., the code is maximum
distance separable. The buﬀer C does not immediately get
updated upon writes, and therefore may contain stale data.

Hierarchical log of recent writes. A hierarchical log
structure denoted H stores recently overwritten blocks in
erasure-coded format. H contains k + 1 levels, where k =
(cid:98)log n(cid:99). We denote the levels of H as (H0, H1, . . . , Hk).

......Erasure−coded blocksUnencoded blocksC:Erasure−coded, recentlywritten blocksH:U:328Figure 2: Rebuilding of level H3. When a newly written
block is added to the hierarchical log structure H, consec-
utive levels H0, H1, H2 are ﬁlled. A rebuild operation for
H3 occurs as a result, at the end of which H3 is ﬁlled, and
H0, H1, H2 are empty. Unlike ORAM schemes that employ
oblivious sorting for the rebuilidng, our rebuilding algorithm
involves computing linear combinations of blocks.

Each level (cid:96) ∈ {0, 1, . . . , k} is an erasure code of some 2(cid:96)
blocks. For every block in H, we deﬁne its age to be the
time elapsed since when the block was written. Level H0
contains the most recently written block; and the age of the
blocks contained in level H(cid:96) increases with (cid:96). Particularly
Hk (if ﬁlled) contains the oldest blocks.
Note that in practice, it is possible that the client keeps
writing the same block, say Bi where i ∈ [n]. In this case,
the hierarchical log structure H contains multiple copies of
Bi. (as we will see later, duplicates are suppressed when the
erasure-coded copy C is rebuilt.) Particularly, every time a
block is written, the new value of the block along with its
block identiﬁer is added to the hierarchical log structure H.
4.2 Operations

After describing the three types of buﬀers that we use to
organize the original blocks, we are ready to give the high
level intuition of the operations of our construction.

Reading blocks. In order to read a block, we read it di-
rectly from U. Along with the block, the server returns
the respective Merkle hash tree proof, allowing the client to
verify the authenticity and the freshness of the block.

PoR audits. A PoR audit involves the following checks:

1. Checking the authenticity of O(λ) random blocks from

the erasure-coded copy C;

2. Checking the authenticity of O(λ) random blocks from

each ﬁlled level H(cid:96), where (cid:96) ∈ [k].

Writing blocks and periodic rebuilding operations.
Every write not only updates the raw buﬀer U, but also
causes the newly written block to be added to the hierar-
chical log structure H (speciﬁcally, the new block is always
added into H0). Whenever a block is added, assume that
levels H0, H1, . . . , H(cid:96) are consecutively ﬁlled levels where
(cid:96) < k, and level (cid:96) + 1 is the ﬁrst empty level. This leads to
the rebuilding of level H(cid:96)+1. At the end of this rebuilding:
1) level H(cid:96)+1 contains an erasure code of all blocks currently
residing in levels H0, H1, . . . , H(cid:96) and the newly added block;
and 2) level H0, H1, . . . H(cid:96) are emptied (see Figure 2).

This rebuilding technique has been previously used in var-
ious ORAM schemes (e.g., [25]). However, unlike ORAM
schemes that require oblivious sorting for the rebuilding, our
rebuilding process requires just computing an erasure code
of the new level. This is a lot simpler since it just involves
computing linear combinations of blocks (we employ a linear
coding scheme). We will later show that it takes O(β · 2(cid:96))

time to rebuild level (cid:96) ((cid:96) = 0, 1 . . . ,(cid:98)log n(cid:99)), where β is the
block size. Since each level (cid:96) is rebuilt every 2(cid:96) write opera-
tions, the amortized rebuilding cost per write is O(β log n).
In comparison, ORAM schemes [12, 13, 15] require roughly
O(β(log n)2/ log log n) or higher amortized cost for the re-
building due to oblivious sorting.

For now, we assume that the client is performing the re-
building for simplicity. Later in Section 5, we will show how
to have the server perform the rebuilding computations, and
the client instead simply veriﬁes that the server adheres to
the prescribed behavior. This will allow us to further reduce
the bandwidth overhead from O(β log n) to β + O(λ log n).

Periodic rebuilding of the erasure coded copy. Fi-
nally, every n write operations, the erasure coded copy C is
rebuilt, and all levels H0, H1, . . . , Hk are emptied as a re-
sult. Recall that the log structure H may contain multiple
copies of the same block, if that block is written multiple
times. At the time C is rebuilt, all duplicates will be sup-
pressed, and only the most recent copy of each block will be
rebuilt into C. As we later show, the cost of rebuilding C is
O(βn log n) (see Figure 4). Since C is only rebuilt every n
write operations, the amortized cost is O(β log n) per write.
4.3 Security

Before we introduce the rebuilding algorithm, which lies
at the heart of our construction, we give an intuitive ex-
planation of why the main security property of PoR, i.e.,
retrievability (see Deﬁnition 2), is satisﬁed by our construc-
tion. Our algorithm will maintain the following invariant:

Invariant 1. Treat C as the (k + 1)-th level Hk+1. We
will maintain the invariant that each level H(cid:96) where (cid:96) ∈
{0, 1, . . . , k + 1} is a (m(cid:96), 2(cid:96), d(cid:96)) erasure coding of 2(cid:96) blocks,
where m(cid:96) = Θ(2(cid:96)). Furthermore, we will show that encoding
is a maximum distance encoding scheme, such that d(cid:96) =
m(cid:96) − 2(cid:96) + 1 = Θ(2(cid:96)), i.e., the encoding of level H(cid:96) can
tolerate up to d = Θ(2(cid:96)) erasures.

In our speciﬁc construction, each level is encoded into ex-
actly twice as many blocks, i.e., H(cid:96) is a (2(cid:96)+1, 2(cid:96), 2(cid:96)) erasure
coding of 2(cid:96) recently written blocks. Similarly, C is also
encoded into twice as many blocks.

Recall that our audit algorithm checks O(λ) blocks for
each level H(cid:96) and for C. Intuitively, the server has to delete
more than half of the blocks in any level H(cid:96) or C to incur
any data loss. However, if the server deletes so many blocks,
checking O(λ) random blocks in level H(cid:96) or C will almost
surely detect it (with probability at least 1 − 2−λ). Also,
observe that the combination of the hierarchical structure H
and the buﬀer C contains information about the up-to-date
copy of all blocks. Therefore, we can intuitively conclude
that as long as the above invariant is maintained, our audits
can detect any data loss with overwhelming probability.

The formal proof requires showing that there exists an ex-
tractor which can extract the up-to-date copy of all blocks if
the extractor can run the Audit algorithm a polynomial num-
ber of times, with blackbox rewinding access to the server.
We defer the formal proof to the full online version [22].
4.4 Fast Incrementally Constructible Codes

To achieve the promised eﬃciency, our main technical
challenge is to design a fast,
incrementally constructible
code. In our scheme, we employ a fast incrementally con-
structible code based on Fast Fourier Transform (FFT). At

......329a very high level, each level H(cid:96) contains two size-2(cid:96) FFT-
based codes. As is well-known, FFT can be computed using
a standard divide-and-conquer strategy [2,3], forming a but-
terﬂy network (see Figure 3). This means that H(cid:96) can be
computed from two H(cid:96)−1’s in time O(β · 2(cid:96)). Note that in
comparison, the oblivious sorting in ORAM schemes take
time O(β · (cid:96) · 2(cid:96)) time to rebuild level H(cid:96).

Other than our FFT-based encoding scheme, it is possible
to employ known linear-time constructible codes [23], which
take O(c) time to encode c blocks. We choose to use our
FFT-based codes because of the relative conceptual and im-
plementation simplicity. Particularly, rebuilding a level H(cid:96)
requires only taking O(2(cid:96)) linear combinations of blocks—in
fact, in Section 5, we will leverage this linearity property and
a homomorphic checksum scheme to make the client-server
the bandwidth cost per writes independent of the block size.
4.4.1 Detailed Code Construction
In our construction, level H(cid:96) contains 2(cid:96) blocks, which are
encoded into 2(cid:96)+1 codeword blocks, such that knowledge of
any 2(cid:96) codeword blocks can recover the original 2(cid:96) blocks.

Suppose the original blocks in level H(cid:96) are denoted as
a vector x(cid:96), where each block in x(cid:96) arrived at time t, t +
1, t + 2, . . . , t + 2(cid:96) − 1 (mod n) respectively. For level H(cid:96),
t is always a multiple of 2(cid:96). Note that the time t is only
incremented for write requests since reads do not need to
touch the hierarchical log.

Notation. For the description of our code, we deﬁne the
partial bit-reversal function and permutation—this inher-
ently results from the divide and conquer strategy of the
FFT. Let ψc(i) denote a partial bit-reversal function that
reverses the least signiﬁcant c bits of an integer i. For ex-
ample, let n = 8, then ψ3(1) = 4, ψ2(1) = 2. Let πc(x)
denote a partial bit-reversal permutation, where index i is
permuted to index ψc(i).

Let also w denote the 2n-th primitive root of unity in an

appropriate ﬁnite ﬁeld deﬁned explicitly in Figure 4.

Closed-form formula for each level. Level H(cid:96) contains
code blocks output by the following linear encoding scheme:

H(cid:96) := π(cid:96)(x(cid:96)) [F(cid:96), D(cid:96),tF(cid:96)] ,

(1)

where F(cid:96) is the 2(cid:96) by 2(cid:96) Fourier Transform matrix from the
ﬁnite ﬁeld deﬁned in Figure 4 and D(cid:96),t is an appropriate
diagonal matrix deﬁned as below.

D(cid:96),t := diag(wψk−(cid:96)(t), wψk−(cid:96)(t+1), . . . , wψk−(cid:96)(t+2(cid:96)−1)) .

We now have the following lemma (its proof can be found
in the full online version [22]):

Lemma 1. Any 2(cid:96) × 2(cid:96) submatrix of the generator matrix

G(cid:96) := [F(cid:96), D(cid:96),tF(cid:96)] is full rank.

Note that the above lemma means that if we have any 2(cid:96)
(out of 2(cid:96)+1) code blocks for a level H(cid:96), we can recover all
original blocks in that level. As mentioned earlier, using the
divide-and-conquer strategy for computing FFT transforms,
we can eﬃciently build our codes over time. The detailed
algorithm is presented in the next subsection.

For a concrete small example when n = 8, please refer to

the full online version [22].

Figure 3: Butterﬂy network: the 8-th write oper-
ation encounters a full H0, H1, and H2. Blocks in
H0, H1, H2 as well as the newly written block will be
encoded and written to H3.
4.5 Detailed Protocol Description

The detailed protocol description is presented in Figure 4.
The lemma below states that algorithm mix(A0, A1) speciﬁed
in Figure 4 produces codes for the hierarchical log, satisfy-
ing Equation (1). Its proof can be found in the full online
version [22].

Lemma 2. Algorithm mix(A0, A1) in Figure 4 ensures that

each ﬁlled level H(cid:96) is a code of the form of Equation (1).
4.6 Enhancements to Basic Construction
Segmenting blocks to avoid big integer operations.
In the above description, each block is treated as a large
integer during the encoding operations. We can avoid big
integer arithmetic by dividing blocks into smaller segments.
In our implementation, we choose a prime p = αn + 1 for
a small positive integer α, such that p can be represented
with a basic integer type. We divide each block into β0 :=
(cid:100)log p(cid:101) bits. and perform the mix algorithm described above
on each smaller segment of the block. Note that using a
smaller p does not aﬀect the security of our scheme, since
the parameter p is part of the erasure coding, and p is not
related to the size of any cryptographic keys.

Ensuring authenticity and freshness. Since the last-
write time of blocks in the buﬀers H and C are computable
from the current time t, the client can simply use time and
location encoded MACs to ensure authenticity and freshness
of blocks in the buﬀers H and C. Blocks in the buﬀer U
need random access, therefore, we can use a standard Merkle
hash tree (i.e., memory checking) to ensure freshness and
authenticity of blocks in U. We omit the details here since
we will present an improved construction in Section 5 where
we will revisit the authenticity and freshness issue.

Theorem 1. The basic dynamic PoR scheme of Figure 4
satisﬁes both authenticity (Deﬁnition 1) and retrievability
(Deﬁnition 2).

The authenticity and retrievability proofs of the above

theorem can be found in the full online version [22].

5.

IMPROVED CONSTRUCTION

In our basic construction (Section 4), every write incurs
the reading and writing of O(log n) blocks, or O(β log n)
bits on average, where β is the block size. In this section,
we will describe an improved construction that achieves im-
proved bandwidth and client computation overhead (how-
ever the server computation of the new construction remains

67543210Levels before rebuildResult of rebuildPast rebuildsTemporary levels createdduring rebuilds330Main Dynamic PoR Construction

Let (encode, decode) denote a maximum distance separable
(m, n, d) erasure code, where m = O(n).

/* Authenticity: Standard Merkle-hash tree techniques can
be used to verify the authenticity/freshness of blocks re-
trieved from the server. For clarity, we omit such details
from the scheme description. Section 5 presents more opti-
mized techniques for achieving authenticity/freshness. */
Init(1λ, n, β,M):

U ← M; C ← encode(M); H ← ∅.

Read(i, st, ¯M): To read a block i, simply read U[i] and

check its authenticity.

Write(i, B, st, ¯M): To overwrite block i with B,

U[i] ← B. Call HAdd(B).

let

Audit(st, ¯M): Check the authenticity of O(λ) number of
blocks from the erasure coded copy C and O(λ) num-
ber of blocks from each ﬁlled level H(cid:96).

Algorithm HAdd(B)

Suppose each H(cid:96) is of the form H(cid:96) := (X(cid:96), Y(cid:96)), where X(cid:96)
and Y(cid:96) each stores 2(cid:96) blocks.
Let the current write be the t-th write operation ( mod 2k).
Let ψ(·) denote the bit reversal function, such that ψ(t)
outputs the value corresponding to reversing the bits of the
binary representation of t.

• If H0 is empty, let X0 = B, Y0 = B · wψ(t).
• Else, suppose levels 0, . . . , (cid:96) are consecutively full lev-
els for some (cid:96) < k, and level (cid:96) + 1 is the ﬁrst empty
level.

– Call HRebuildX((cid:96) + 1, B).
– Call HRebuildY((cid:96) + 1, B · wψ(t)).
• Every 2k time steps, call CRebuild().

Parameters: Let p = α· (2n) + 1 denote a prime for some
α ∈ N. Suppose p is chosen to be large enough to encode
blocks of β bits.
Let g denote a generator of Z∗
2n-th primitive root of unity mod p.

p. Let w = gα mod p be a

Algorithms HRebuildX((cid:96), B), HRebuildY((cid:96), B)

/* HRebuildY is exactly the same as HRebuildX, except that
all X’s are replaced with Y ’s. Below we formally describe
HRebuildX as an example. */

Inputs: Consecutively ﬁlled levels X0 . . . X(cid:96)−1 (the X

portion), and a new (possibly encoded) block B.

Output: A rebuilt X(cid:96). Levels X0, . . . , X(cid:96)−1 are emptied

at the end of HRebuildX((cid:96), B).

• (cid:101)X1 ← mix(X0, B)
• For i = 1 to (cid:96) − 1: (cid:101)Xi+1 ← mix(Xi, (cid:101)Xi)
// (cid:101)Xi’s are the red arrays in Figure 3.
• Output X(cid:96) := (cid:101)X(cid:96).

Algorithm mix(A0, A1)

Inputs: Two arrays A0, A1 ∈ Z2(cid:96)
Outputs: A new array A of length 2 · 2(cid:96).

p each of length 2(cid:96).

• Let ν = wn/2(cid:96)
• For i = 0 to 2(cid:96) − 1:

be a 2(cid:96)+1-th primitive root of unity.

A[i]
A[i + 2(cid:96)]

:= A0[i] + νiA1[i]
:= A0[i] − νiA1[i]

(mod p)
(mod p)

• Output A.

Algorithm CRebuild()

• C ← encode(U).
• Empty H.

Figure 4: Basic protocol description. We assume that blocks are tagged with their block identiﬁer.

the same), by removing the dependence on the block size.
In our improved construction, writing a block incurs only
β + O(λ log n) cost, where λ is the security parameter; and
the typical value for λ is 128 or 256 bits. This means that
the bandwidth overhead and the client computation for write
operations of our PoR construction is analogous to that of
a Merkle hash tree. Recall that by the deﬁnition of PoR,
PoR is a strictly stronger primitive than a Merkle hash
tree since PoR not only needs to guarantee authenticity and
freshness—which a standard Merkle hash tree ensures—but
it needs to additionally guarantee that the server is storing
all of the client’s data. Our construction basically shows
that we can obtain the additional PoR guarantee (on top of
a Merkle hash tree) almost for free.
5.1 Intuition

Recall that in the basic construction, the O(β log n) cost
arises from the periodical rebuilding of the hierarchical log

structure H and the erasure-coded copy C.
In the basic
construction, the server is conceptually treated as a pas-
sive remote storage device, and the client performs all the
computation. Therefore, the client needs to download the
blocks from the server to perform computation over these
blocks, when rebuilding any level H(cid:96) in the hierarchical log
structure, or when rebuilding C.

Our idea is to have the server perform the computation
on behalf of the client, thus signiﬁcantly reducing the client
computation and bandwidth. Observe that in our basic con-
struction, the algorithms for rebuilding each H(cid:96) or C are
publicly known to the server, and the server can perform
the rebuilding on its own. The client simply needs to check
that the server performed the rebuilding correctly.

To achieve this performance improvement and avoid down-
loading the blocks during a rebuilding, we will attach a
homomorphic checksum along with each (encoded or unen-
coded) block in the hierarchical log structure H or C. Each

331homomorphic checksum is a collision-resistant summary of
a block. Now, instead of performing the rebuilding over real
data blocks, the client simply downloads checksums, checks
their authenticity and performs the rebuilding over these
homomorphic checksums.

The homomorphic checksum for each block is then tagged
with its position and time written, and stored on the server
in encrypted and authenticated format. This ensures that
1) the server does not know the values of the homomorphic
checksum which is necessary for security as explained later;
and 2) the client can always verify the correctness and fresh-
ness of the homomorphic checksum retrieved.

We note here that we choose to use a special type of homo-
morphic checksums that are not publicly veriﬁable (though
we later show that this is not a limitation for making our
scheme publicly veriﬁable). The reason for that is that our
homomorphic checksum construction is designed for per-
formance. In comparison, publicly veriﬁable homomorphic
checksums (e.g., lattice-based ones [19] or RSA-based ones [5])
are not as eﬃcient practice.
5.2 Homomorphic Checksums: Deﬁnitions

We now give the deﬁnition of a homomorphic checksum
scheme, along with its deﬁnition of security (unforgeability).
As mentioned in Section 4.6, we segment a block into seg-
ments of bit-length β0 = (cid:100)log p(cid:101). Therefore, we can write a
block B in the form B ∈ Z(cid:100)β/β0(cid:101)

.

p

Definition 4. We deﬁne a homomorphic checksum scheme

to consist of the following algorithms:
sk ← K(1λ). The key generation algorithm K takes in the
security parameter 1λ, the block size β, outputting a
secret key sk .

σ ← Ssk (B). The authentication algorithm S takes in the
, and outputs a

secret key sk , a block B ∈ Z(cid:100)β/β0(cid:101)
checksum σ. In our scheme σ has bit-length O(λ).

p

Definition 5

(Unforgeability of checksum). We say

(cid:20) sk ← K(1λ)

that a homomorphic checksum scheme is unforgeable, if for
any (polynomial-time) adversary A,
B1 (cid:54)= B2
Ssk (B1) = Ssk (B2)

B1, B2 ← A(1λ)

≤ negl(λ) .

(cid:21)

Pr

:

Namely, an adversary who has not seen the secret key sk or
any checksum cannot produce two blocks B1 and B2 that
result in the same checksum.
In our scheme, the clients
encrypts all checksums before storing them at the server,
thus the server does not see the secret key or any checksums.

Additive homomorphism. We require our homomorphic
checksum scheme to achieve additive homomorphism, i.e.,
for any sk ← K(1λ), for any blocks B1, B2 ∈ Z(cid:100)β/β0(cid:101)
, for any
a1, a2 ∈ Zp, it is a1Ssk (B1) + a2Ssk (B2) = Ssk (a1B1 + a2B2).
5.3 Homomorphic Checksum Construction

p

We now present a simple homomorphic checksum scheme.
K(1λ): The client picks a random matrix M $← Zρ×(cid:100)β/β0(cid:101)
and lets sk := M. The number of rows ρ is chosen
such that ρβ0 = O(λ), i.e., we would like the resulting
checksum to have have about O(λ) number of bits.

p

Ssk (B): On input a block B ∈ Z(cid:100)β/β0(cid:101)

, compute checksum
σ := M · B. Note that the checksum compresses the
block from β bits to O(λ) bits.

p

Additive homomorphism. It is not hard to see that the
above homomorphic checksum scheme satisﬁes additive ho-
momorphism, since a1MB1 + a2MB2 = M(a1B1 + a2B2).

Unforgeability. Inthe full online version [22] we show that
the above construction satisﬁes unforgeability in an infor-
mation theoretic sense (i.e., even for computationally un-
bounded adversaries).

Theorem 2. The above homomorphic checksum construc-

tion satisﬁes the unforgeability notion (Deﬁnition 5).

p

Eﬃciency. As we saw in the deﬁnition of the homomor-
phic checksum, the blocks used are represented as vectors in
Z(cid:100)β/β0(cid:101)
. We use a very small p in our implementation—in
fact we take p = 3·230 +1 and therefore checksum operations
do not need big integer operations and are highly eﬃcient.
5.4 Using Homomorphic Checksums

Relying on homomorphic checksums, we can reduce the
bandwidth cost and the client computation to β +O(λ log n)
for write operations. To do this, we make the following mod-
iﬁcations to the basic construction described in Section 4.

Store encrypted and authenticated checksums on
server. First, for every (encoded or uncoded) block stored
on the server, in the buﬀers U, C and H, the client at-
taches a homomorphic checksum, which is encrypted under
an authenticated encryption scheme AE := (E, D).
Let sk := (M, sk 0) denote the client’s secret key (unknown
to the server). Speciﬁcally M ∈ Zρ×(cid:100)β/β0(cid:101)
is the random
matrix used in the homomorphic checksum scheme, and sk 0
is a master secret key used to generate one-time keys for the
authenticated encryption scheme AE.

p

For the buﬀers C and H, suppose a block B is written to
address addr on the server at time t. The client generates
the following one-time key:

κ = PRFsk 0 (0, addr, t)

and attaches(cid:101)σ(B) with block B, where
(cid:101)σ(B) := AE.Eκ(SM (B)) .

For buﬀer U, the client uses a ﬁxed encryption key κ =
PRFsk 0 (1) as blocks in U have unpredictable last-write times.
Ensuring authenticity and freshness. For each block

allows the client to verify the authenticity and freshness of
the block. The client need not separately MAC the blocks
in H or C.

in buﬀers H and C, its time and location dependent (cid:101)σ(B)
For blocks in U, a Merkle tree is built over these (cid:101)σ(B)’s
corresponding(cid:101)σ(B), veriﬁes(cid:101)σ(B) with the Merkle tree, and
veriﬁes that the block B fetched agrees with(cid:101)σ(B).

for the U buﬀer, and the client keeps track of the root digest.
After fetching a block B from buﬀer U, the client fetches its

Rebuilding H based on homomorphic checksum. In
our improved scheme, the HRebuild algorithm is executed by
the server. In order to enforce honest server behavior, the
client performs the HRebuild algorithm, not directly over the

blocks, but over (cid:101)σ(B)’s. In other words, imagine the client

were to execute the HRebuild algorithm on its own:

332• Whenever the client needed to read a block B from the

server, it now reads(cid:101)σ(B) instead, and decrypts σ(B) ←
AE.Dκ((cid:101)σ(B)).

In the above, κ := PRFsk 0 (0, addr, t),
where addr is the physical address of block B, and t
is the last time B is written. Note that for any block
in the hierarchical log structure H and in the erasure-
coded copy C, the client can compute exactly when the
block was last written from the current time alone. The
client rejects if the decryption fails, which means that
the server is misbehaving.
• Whenever the client needed to perform computation over
two blocks B1 and B2, the client now performs the same
operation over the homomorphic checksums σ(B1) and
σ(B2). Recall that in HRebuild, we only have addition
and multiplications by known constants—therefore, our
additively homomorphic checksum would work.
• Whenever the client needed to write a block to the server,

it now writes the new(cid:101)σ(B) instead.

Rebuilding C based on homomorphic checksum. Sim-
ilar to the above, the server rebuilds C on behalf of the
client, and the client only simulates the rebuilding of C op-

erating on the (cid:101)σ(B)’s instead of the full blocks. However,

slightly diﬀerently from the rebuilding of H, the buﬀer C is
rebuilt by computing an erasure code of the fresh data in U.
One way to do this is to use the same FFT-based code
as described in Section 4. The server can compute the code
using the butterﬂy diagram (Figure 3) in O(βn log n) time.
The client simply has to simulate this encoding process using

the(cid:101)σ(B)’s rather than the data blocks—therefore the client-

server bandwidth is O(λn log n) for each rebuild of C.
5.5 Reducing Client-Server Audit Bandwidth
In our basic construction in Section 4, audits require trans-

ferring O(λ) random from each level H(cid:96) and from C—therefore
the audit cost is O(λβ log n). However this can be reduced
by observing that audited blocks can be aggregated using a
linear aggregation scheme, and the client can check the “ag-
gregated block” using the homomorphic “aggregate check-
sum”. This is similar to the technique used by Shacham and
Waters [20]. The new audit protocol is as below:
• Challenge. Client picks O(λ) random indices for each
level H(cid:96) and for C. Client picks a random challenge
ρi ∈ Zp for each selected index. The random indices
are denoted I := {addr1, addr2, . . . , addrr}, where r =
O(λ log n). The random challenges are denoted R :=
{ρ1, ρ2, . . . , ρr}. Client sends I := {addr1, addr2, . . ., addrr}
and R := {ρ1, ρ2, . . . , ρr} to the server.

• Response. Let Baddr denote the block at addr. Server

sends client the corresponding(cid:101)σ(Baddr) for each requested
index addr ∈ I. Server also computes B∗ =(cid:80)r
• Veriﬁcation. The client decrypts all(cid:101)σ(B)’s received and
obtains σ(B)’s. Client computes v = (cid:80)r

i=1 ρiσ(Baddri ),
and the checksum σ(B∗). The client checks if v ?= σ(B∗).

and sends B∗ to the client.

i=1 ρiBaddri ,

This modiﬁcation reduces the bandwidth for audits to β +
O(λ2 log n) since only checksums and one aggregate block
need to be communicated from the server to the client.

Theorem 3. The improved dynamic PoR scheme that uses

homomorphic checksums satisﬁes both authenticity (Deﬁni-
tion 1) and retrievability (Deﬁnition 2).

The proofs of the above theorem can be found in the full

online version [22].

6.

IMPLEMENTATION AND EVALUATION
We implemented a variant of our secretly-veriﬁable con-
struction described in Section 5. The implementation is in
C# and contains about 6,000 lines of code measured using
SLOCCount [4].
6.1 Practical Considerations

Our implementation features several further optimizations
in comparison with the theoretical construction in Section 5.

Reducing write cost. Our implementation is optimized
for the common scenario where writes are frequent and au-
dits are relatively infrequent. We apply a simple trick to re-
duce the client-server bandwidth for write operations; how-
ever, at slightly higher cost for audits.

To reduce the overhead of writing the log structure H, the
client can group about O(log n) blocks into a bigger block
when writing to H. The simplest instantiation is for the
client to cache O(log n) blocks and write them in a batch as
a single bigger block.

This optimization reduces the amortized cost of writing
to H to β + O(λ). Particularly, the client-server bandwidth
overhead (in addition to transferring the block itself) can be
broken down into two parts: 1) about O(λ log n) due to the
Merkle hash tree for the buﬀer U; and 2) O(λ) for writing
to the log structure H—notice that this part is independent
of n. This trick increases an audit’s disk I/O cost to roughly
O(λβ log2 n) (but only O(λ log n) disk seeks), and increases
an audit’s client-server bandwidth to β log n + O(λ2 log n).

Deamortization. Our theoretic construction has low amor-
tized cost per write, but high worst-case cost. We can per-
form a standard deamortization trick (introduced for sim-
ilar multi-level hierarchical data structures used in several
ORAM constructions [18, 31]) to spread the hierarchical en-
coding work over time, such that we achieve good worst-case
performance. We implemented deamortization, therefore it
is reﬂected in the experimental results.

Reducing disk seeks. Our hierarchical log features se-
quential data accesses when rebuilding level H(cid:96) from two
arrays of size 2(cid:96)−1. (see Figure 3 and Algorithm mix of Fig-
ure 4). Due to such sequential access patterns, we can use a
bigger chunk size to reduce the number of seeks. Every time,
we read a chunk and cache it in memory while performing
the linear combinations in Algorithm mix.
6.2 Experimental Results

We ran experiments on a single server node with an i7-
930 2.8 Ghz CPU and 7 rotational WD1001FALS 1TB 7200
RPM HDDs with 12 ms random I/O latency. Since reads are
not much diﬀerent from a standard Merkle-tree, we focus on
evaluating the performance overhead of writes and audits.
6.2.1 Write Cost

Client-server bandwidth. Figures 5 and 6 depict the
client-server bandwidth cost for our scheme. Figure 5 plots
the total client-server bandwidth consumed for writing a
4KB block, for various storage sizes. We compare the cost
to a standard Merkle hash tree, and show that our POR
scheme achieves comparable client-server bandwidth.

333Figure 5: Client-server bandwidth
for writing a 4KB block.

Figure 6: Percentage of bandwidth
overhead for various block sizes. The
total storage capacity is 1TB.

Figure 7: Throughput for write oper-
ations when client-server bandwidth is
ample. The error bars indicate one stan-
dard deviation over 20 trials. A 4KB block
size is used.

Figure 8: Time spent by server for
performing an audit. Does not include
network transfer time. Error bars denote
1 standard deviation from 20 trials. The
majority of this time is disk I/O cost. A
4KB block size is chosen.

Figure 9: Disk I/O cost for each au-
dit. Block size is chosen to be 4KB.

Figure 10: Client-server bandwidth
for an audit. Block size is chosen to be
4KB.

We also plot the dotted “Hierarchical FFT encoding” curve,
which represents the portion of our bandwidth cost dedi-
cated to the rebuilding of the buﬀers C and H. As men-
tioned earlier in Section 6.1, our implementation features
optimizations such that the client-server bandwidth over-
head contains the sum of two parts 1) a O(λ) part for re-
building H and C. This is represented by the dotted line in
Figure 5—note that it is independent of n, hence a straight-
line; and 2) O(λ log n) bandwidth due to the Merkle tree for
the up-to-date copy U.

Figure 6 shows the percentage of bandwidth overhead (not
including transferring the block itself) for each write.
In
our scheme, the bandwidth overhead is indepedent of the
block size. Therefore, the bigger the block size, the smaller
the percentage of bandwidth overhead. We also compare
against a standard Merkle-tree in the ﬁgure, and show the
portion of the bandwidth overhead for the hierarchical en-
coding procedure.

Server disk I/O performance. Figure 7 shows the write
throughput of our scheme. The server’s disk I/O cost is
about O(log n), i.e., it needs to read roughly O(log n) blocks
for every write operation. Our experiments show that if
server disk I/O were maxed out, the POR write throughput
we can achieve would be roughly 20MB/s under our setup.
We cache the smallest 10 levels of H in memory in this
experiment.
In practice, however, the client-server band-

width is more likely to be the bottleneck, and the actual
POR throughput achievable will be limited by the available
client-server bandwidth.

While the server needs to read O(log n) blocks for each
write, the number of seeks is very small. As mentioned ear-
lier in Section 6.1, the hierarchical log structure H is mostly
written in a sequential fashion, and since we choose a large
chunk size (roughly 50MB), and cache chunks in memory,
every write operation requires only 0.03 to 0.06 seeks on av-
erage. We cache the Merkle hash tree for U in memory in
our experiments.

6.2.2 Audit Cost
We use a λ = 128 for these experiments, i.e., each audit

samples 128 blocks from each level H(cid:96) and buﬀer C.

Figure 8 shows the time spent by the server for each audit
operation—including time for reading disk and performing
computation, but not including network transfer time be-
tween client and server (client-server network overhead is
characterized separately in Figure 10). The majority of this
time is spent on disk I/O, and is dominated by disk seeks.
There are roughly O(λ log n) seeks per audit operation par-
allelized to 7 disks, and each seek takes roughly 12ms.

Figure 9 shows the disk I/O cost for an audit. As men-
tioned in Section 6.1, we optimize for writes at slightly higher
audit cost, and the audit disk I/O cost is O(λβ log2 n)—this
is why the line curves slightly, and is not linear.

334[17] Z. Mo, Y. Zhou, and S. Chen. A dynamic proof of

retrievability (por) scheme with o(log n) complexity. In
ICC’12, pages 912–916, 2012.

[18] R. Ostrovsky and V. Shoup. Private information storage

(extended abstract). In STOC, pages 294–303, 1997.

[19] C. Papamanthou, E. Shi, R. Tamassia, and K. Yi.

Streaming authenticated data structures. In
EUROCRYPT, 2013.

[20] H. Shacham and B. Waters. Compact proofs of

retrievability. In ASIACRYPT, pages 90–107, 2008.

[21] E. Shi, T.-H. H. Chan, E. Stefanov, and M. Li. Oblivious
RAM with O((log N )3) worst-case cost. In ASIACRYPT,
pages 197–214, 2011.

[22] E. Shi, E. Stefanov, and C. Papamanthou. Practical

dynamic proofs of retrievability. Technical report, 2013.

[23] D. A. Spielman. Linear-time encodable and decodable

error-correcting codes. IEEE Transactions on Information
Theory, 42(6):1723–1731, 1996.

[24] E. Stefanov and E. Shi. Oblivistore: High performance

oblivious cloud storage. In IEEE Symposium on Security
and Privacy, 2013.

[25] E. Stefanov, E. Shi, and D. Song. Towards practical

oblivious RAM. In NDSS, 2012.

[26] E. Stefanov, M. van Dijk, A. Juels, and A. Oprea. Iris: a

scalable cloud ﬁle system with eﬃcient integrity checks. In
ACSAC, pages 229–238, 2012.

[27] J. van Lint. Introduction to Coding Theory.

Springer-Verlag, 1992.

[28] Q. Wang, C. Wang, J. Li, K. Ren, and W. Lou. Enabling
public veriﬁability and data dynamics for storage security
in cloud computing. In ESORICS, 2009.

[29] Q. Wang, C. Wang, K. Ren, W. Lou, and J. Li. Enabling
public auditability and data dynamics for storage security
in cloud computing. IEEE Trans. Parallel Distrib. Syst.,
22(5):847–859, 2011.

[30] P. Williams, R. Sion, and B. Carbunar. Building castles out
of mud: practical access pattern privacy and correctness on
untrusted storage. In CCS, 2008.

[31] P. Williams, R. Sion, and A. Tomescu. Privatefs: A parallel

oblivious ﬁle system. In CCS, 2012.

Figure 11: Computational throughput for hierarchical
erasure coding. Error bars denote 1 standard deviation from
20 runs.

Figure 10 shows the client-server bandwidth consumed for
an audit. As mentioned in Section 6.1, unlike our theoretic
construction, our implementation chooses to speed up writes
at slightly higher client-server bandwidth for audits, namely,
β log n + O(λ log n).

Computational overhead for hierarchical erasure cod-
ing. As shown in Figure 11, the hierarchical coding scheme
can be computed at extremely fast speeds, i.e., >1600MB/s
per level on a modern processor. To characterize the com-
putational cost, we cached about 4GB of data in memory,
and avoid performing disk fetches during this experiment.

7. REFERENCES
[1] https://en.wikipedia.org/wiki/Hash_tree.
[2] Fast fourier transform. http://math.berkeley.edu/

~berlek/classes/CLASS.110/LECTURES/FFT.

[3] Fast fourier transform.

http://en.wikipedia.org/wiki/Fast_Fourier_transform.

[4] Sloccount. http://www.dwheeler.com/sloccount/.
[5] G. Ateniese, R. Burns, R. Curtmola, J. Herring, L. Kissner,

Z. Peterson, and D. Song. Provable data possession at
untrusted stores. In CCS, 2007.

[32] Q. Zheng and S. Xu. Fair and dynamic proofs of

retrievability. In CODASPY, 2011.

[6] S. Benabbas, R. Gennaro, and Y. Vahlis. Veriﬁable

delegation of computation over large datasets. In
CRYPTO, pages 111–131, 2011.

[7] K. D. Bowers, A. Juels, and A. Oprea. Proofs of

retrievability: theory and implementation. In CCSW, pages
43–54, 2009.

[8] D. Cash, A. K¨up¸c¨u, and D. Wichs. Dynamic proofs of

retrievability via oblivious ram. In Eurocrypt, 2013.

[9] Y. Dodis, S. P. Vadhan, and D. Wichs. Proofs of

retrievability via hardness ampliﬁcation. In TCC, pages
109–127, 2009.

[10] C. Erway, A. K¨up¸c¨u, C. Papamanthou, and R. Tamassia.

Dynamic provable data possession. In CCS, 2009.

[11] O. Goldreich and R. Ostrovsky. Software protection and

simulation on oblivious RAMs. J. ACM, 1996.

[12] M. T. Goodrich and M. Mitzenmacher. Privacy-preserving

access of outsourced data via oblivious RAM simulation. In
ICALP, 2011.

[13] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and
R. Tamassia. Privacy-preserving group data access via
stateless oblivious RAM simulation. In SODA, 2012.

[14] A. Juels and B. S. K. Jr. Pors: proofs of retrievability for

large ﬁles. In ACM Conference on Computer and
Communications Security, pages 584–597, 2007.
[15] E. Kushilevitz, S. Lu, and R. Ostrovsky. On the

(in)security of hash-based oblivious RAM and a new
balancing scheme. In SODA, 2012.

[16] R. C. Merkle. A certiﬁed digital signature. In Proceedings

on Advances in cryptology, CRYPTO ’89, 1989.

APPENDIX
A. ACHIEVING PUBLIC VERIFIABILITY
Although our basic construction in Section 4 provides pub-
lic veriﬁability, the more eﬃcient scheme described in Sec-
tion 5 does not, since the homomorphic checksum requires
the client keep secret state. In this section we show how to
turn the eﬃcient scheme of Section 5 into publicly veriﬁable.
In a publicly veriﬁable setting, only a trusted data source
can write data; however, anyone can perform veriﬁable reads
and PoR audits.

Ensuring the public veriﬁability of reads comes for free,
since a Merkle hash tree is maintained over the buﬀer U.
Therefore the trusted source can simply sign and publish the
up-to-date root digest of this Merkle-hash tree and make it
available to the public for veriﬁcation of read operations.

We now focus on how to achieve public auditability so
that to enable anyone challenge the server to prove that it
possesses all data (owned by the source) and still maintain
the bandwidth of the write operations to be low.
A.1 Public Auditability with Low Write Cost
To achieve public auditability, we build a separate Merkle
hash tree over the blocks of each level H(cid:96), and one for C.

335The up-to-date root digests of all O(log n) hash trees will
be publicized. During a public audit, a user with the root
digests requests O(λ) blocks at random for each level H(cid:96)
and buﬀer C, and checks them with the root digests.

One question remains: how does the trusted source keep
track of these root digests without having to transfer original
blocks during the rebuilding of the hierarchical levels? To
address this question, we sketch our idea below, and leave
the details and full proofs to the full online version [22].

Our idea is to have the server compute the new Merkle
trees for a level (or the buﬀer C) when it is being rebuilt.
However, we need to protect against a malicious server that
can potentially cheat and output the wrong digests. We
apply a probabilistic checking idea again here.

When rebuilding a level H(cid:96) (or buﬀer C), the following

happens:
• As in the secretly-veriﬁable scheme (Section 5), the trusted
source downloads the encrypted/authenticated homomor-
phic checksums and “simulates” the rebuilding over these

(cid:101)σ(B)’s.

• The server performs the rebuilding, computes the new

digest h of H(cid:96), and commits h to the source.

• The source challenges O(λ) random blocks in H(cid:96). For
each challenged block B: the source downloads the block

itself B, its(cid:101)σ(B), and the path in the Merkle tree neces-
sary to verify block B. The client now checks that(cid:101)σ(B)

veriﬁes for B, and that the already committed root digest
h veriﬁes for B as well.

Proof intuition. At a high level, this idea works because if
the server can pass the probabilistic check (of the commit-
ted root digest h), then at least a constant fraction of the
rebuilt level H(cid:96) (or buﬀer C) is correctly incorporated into
the claimed digest h. Due to PoR’s inherent erasure coding,
it turns out that this suﬃces proving the retrievability of the
publicly veriﬁable PoR scheme. The full proof is deferred to
the full online version [22].

Write cost. Suppose the trusted source caches locally the
smallest log λ+log(2/) levels consisting about 2λ/ number
of blocks, for an arbitrarily small 0 <  < 1. Note that these
are the levels that are accessed more frequently during the
write operations. We can then show that the source-server
bandwidth for each write operation is β(1 + ) + O(λ log n).
The details of this analysis is elementary, and deferred to
the full online version [22]. The above analysis assumes that
exactly λ blocks are probabilistically checked for each Merkle
tree hash tree of the remaining (uncached) levels.
A.2 Reducing Public Audit Cost

The publicly veriﬁable approach described above requires
O(λ log n(β + log n)) overhead for public auditing. Particu-
larly, for each of the O(log n) levels H(cid:96) as well as C, O(λ)
blocks need to be checked; and to check a block involves
downloading the block itself and log n hashes of the Merkle
hash tree. With some additional tricks, in particular, by
carefully aligning the Merkle tree structure with the hierar-
chical log structure H, we can further improve the public
audit overhead to O(λβ log n). We defer these details to the
Appendix. The basic idea is as follows:

• Instead of building a separate Merkle tree per level H(cid:96),
build a single Merkle tree over the entire hirarchical log
structure H, and another one for C. Furthermore, the

Merkle tree will be aligned on top of the hierarchical
structure H. Since H has exponentially growing lev-
els, we can view H as a tree, where internal nodes are
assigned values—in our case, the blocks are the val-
ues of internal nodes of the Merkle tree. The hash
of each internal node in the Merkle tree is computed
as H(hleft, hright, B), where hleft is the hash of the left
child, hright is the hash of the right child, and B is the
block associated with that node. The client publishes
the hash of the Merkle tree for H and the one for C.
• During public audits, random checks for C are still done
as before. To check H, instead of randomly sample
O(λ) blocks from each level H(cid:96), the client randomly
samples O(λ) paths from the root to O(λ) randomly se-
lected leaf nodes, and sample the blocks on these paths.
As a result O(λ) blocks from each level gets sampled,
and it is not hard to see that only O(λ log n) hashes
needs to be transmitted to verify all the paths sampled
– namely, hashes of all sibling nodes to the sampled
paths need to be transmitted. This reduces the public
audit overhead to O(λβ log n), assuming β = O(λ).
• When a non-top level H(cid:96) gets rebuilt, the server re-
builds the Merkle hash tree, and the client performs
the following probabilistic checking protocol.
The client ﬁrst retrieves all hashes at level H(cid:96)+1, and
makes sure that they are consistent with the old di-
gest. The client then randomly samples O(λ) blocks at
level H(cid:96), to make sure that these blocks are correctly
incorporated into the new root digest as claimed by the
server.
When the top level is rebuilt, the client simply checks
O(λ) random blocks in the top level, and ensures that
they are correctly incorporated into the root digest.

Theorem 4. The dynamic PoR scheme with public veri-
ﬁability satisﬁes both authenticity (Deﬁnition 1) and retriev-
ability (Deﬁnition 2). The proof is in the full online ver-
sion [22].
A.3 Resizing the Storage

Our scheme can easily be modiﬁed to support insertions
and deletions of blocks. Insertions can easily be supported
by adding a level to the hierarchical log structure H when-
ever number of blocks doubles. Deletions can be supported
by updating the deleted block with ⊥. Further, whenever
the number of deleted elements exceeds roughly half the size
of the dataset, the client can rebuild and consilidate the hi-
erarchical log by suppressing deleted items. We will provide
more details in the online full version [22].
Acknowledgments
This work is partially supported by NSF grant CNS-1314857,
a Google Research Award, the NSF Graduate Research Fel-
lowship under Grant No. DGE-0946797, by a DoD National
Defense Science and Engineering Graduate Fellowship, by
Intel award through the ISTC for Secure Computing, and a
grant from Amazon Web Services. Any opinions, ﬁndings,
and conclusions or recommendations expressed in this mate-
rial are those of the author(s) and do not necessarily reﬂect
the views of the funding agencies. We would like to thank
Hubert Chan, Jonathan Katz, and Hongsheng Zhou for help-
ful discussions, and the anonymous reviewers for their in-
sightful feedback.

336