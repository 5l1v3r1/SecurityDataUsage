Exploring Identity Impersonation in Online Social Networks

The Doppelgänger Bot Attack:

Oana Goga

MPI-SWS

Giridhari Venkatadri

MPI-SWS

Krishna P. Gummadi

MPI-SWS

ABSTRACT
People have long been aware of malicious users that imper-
sonate celebrities or launch identity theft attacks in social
networks. However, beyond anecdotal evidence, there have
been no in-depth studies of impersonation attacks in today’s
social networks. One reason for the lack of studies in this
space is the absence of datasets about impersonation at-
tacks. To this end, we propose a technique to build extensive
datasets of impersonation attacks in current social networks
and we gather 16,572 cases of impersonation attacks in the
Twitter social network. Our analysis reveals that most iden-
tity impersonation attacks are not targeting celebrities or
identity theft. Instead, we uncover a new class of imperson-
ation attacks that clone the proﬁles of ordinary people on
Twitter to create real-looking fake identities and use them
in malicious activities such as follower fraud. We refer to
these as the doppelg¨anger bot attacks. Our ﬁndings show (i)
that identity impersonation attacks are much broader than
believed and can impact any user, not just celebrities and
(ii) that attackers are evolving and create real-looking ac-
counts that are harder to detect by current systems. We
also propose and evaluate methods to automatically detect
impersonation attacks sooner than they are being detected
in today’s Twitter social network.

INTRODUCTION

1.
Today, users sign on to most online social networks like Face-
book and Twitter via weak identities, i.e., unveriﬁed identi-
ties (accounts) that do not require users to prove that their
online identities match their oﬄine, real world, personali-
ties. Weak identities lower the sign-on barriers for users,
oﬀer users a certain level of anonymity, but they leave the
sites vulnerable to a variety of fake identities or Sybil at-
tacks. Malicious attackers are known to use Sybil identities
to post spam content [31] and to tamper with the popular-
ity of content on these sites [33]. Consequently, a number of
prior works have focussed on understanding and detecting
Sybil attacks in online social networks [20, 25, 21, 14].

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
IMC’15, October 28–30, 2015, Tokyo, Japan.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3848-6/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2815675.2815699.

In this paper, we focus on a special class of Sybil (fake
identity) attacks known as identity impersonation attacks,
where the attacker spoofs or assumes the identity of another
real-world user, the victim. As more and more personal data
about users becomes publicly available on the Web, imper-
sonation attacks become easier to carry out. For instance,
an attacker can easily copy public proﬁle data of a Facebook
user to create an identity on Twitter or Google+.

There are many diﬀerent types of impersonation attacks
based on the attacker’s motivation. (i) In a celebrity imper-
sonation attack, the attacker exploits or maligns the public
reputation of the victim, whose identity she impersonated.
Popular and celebrity users are often targets for such imper-
sonation attacks [19, 26]. (ii) In a social engineering attack,
the attacker abuses victim’s identity to trick the victim’s
friends into revealing sensitive information or providing ser-
vices (e.g., transfer money to the attacker) [30, 22]. (iii) In
a doppelg¨anger bot attack, the attacker is simply interested
in evading detection by the Sybil or Spam defenses deployed
by site operators. As Sybil detectors are often trained to
detect non-real-looking identities (e.g., lacking proﬁle pho-
tos or location information or bio), attackers could create
more real-looking fake identities by copying the attributes
of a real user.

Regardless of the attacker’s motivation, identity imper-
sonation attacks could seriously damage the victim’s online
reputation. As people’s online data is increasingly aggre-
gated by people’s search engines [28] and used for a variety
of purposes including evaluating their suitability for employ-
ment [27], impersonation attacks, particularly those that go
undetected, can have serious adverse consequences for the
victims, even in the oﬄine world.

Despite the serious threat posed by impersonation attacks,
few studies, to date, have systematically studied imperson-
ation attacks in online social networks. Beyond a few anec-
dotal examples that are reported in the popular press, we
lack large datasets about real-world impersonation attacks.
Perhaps, not surprisingly, most social networking sites to-
day lack frameworks to automatically detect impersonators.
Instead, they rely on manual reports from victims about ac-
counts that are impersonating them [32], which can be risky
as victims usually become aware of their attackers after their
online reputation has already been damaged.

Against this background, this paper presents, to the best
of our knowledge, the ﬁrst extensive study of real-world im-
personation attacks in online social networks. Speciﬁcally,
it makes the following contributions:

1411. Methodology for gathering data about imper-
sonation attacks. We identify the fundamental challenges
associated with gathering data about impersonation attacks
and propose practical methods to circumvent the challenges.
Our method, as described in §2, consists of two steps: (1)
identify doppelg¨anger accounts in a social network that por-
tray the same person/entity; and (2) out of the doppel-
g¨anger accounts that portray the same entity, label which
accounts are legitimate and which accounts are imperson-
ators.

The second step is complicated by the fact that people
can maintain multiple legitimate accounts in a social net-
work. We applied our method to gather data on Twitter.
We identiﬁed 54,304 doppelg¨anger pairs of Twitter identi-
ties that portray the same person. We successfully labelled
16,572 of the doppelg¨anger pairs as resulting from imper-
sonation attacks and 3,639 pairs as cases when a person
maintains two legitimate accounts in the Twitter network.
2. Characterizing impersonation attacks. Our anal-
ysis of the data we gathered reveals many interesting facts
about identity impersonation attacks, including some that
contradicted our expectations.

(1) Contrary to our expectation that most impersonation
attacks would largely target celebrity / popular user identi-
ties, we discovered that many impersonation attacks target
ordinary users.
(2) Additionally, many impersonation at-
tacks do not seem to attempt social engineering attacks or
even try to exploit the public reputation of their victims.
They appear to have been created by attackers looking to
create real looking fake accounts that could pass undetected
by current Sybil detection systems. We call these attacks
as the doppelg¨anger bot attacks. Despite their motivation,
doppelg¨anger bot attacks can still harm the online repu-
tation of the victim identity.
(3) We found that it takes
Twitter on average 287 days to suspend the impersonating
accounts from our dataset. The long delay in detecting the
attacks call for developing methods to detect such attacks
sooner.
3. Methods to automatically detect impersonation
attacks. Our analysis characterizing impersonation attacks
yield novel insights for detecting impersonation attacks. First,
given a pair of doppelg¨anger identities, we could determine
whether the identity-pair is managed by the same person or
whether it is the result of an impersonation attack, by com-
paring the social network neighborhood and interests of the
user. In the former scenario, the pair of doppelg¨anger iden-
tities share considerable overlap in network neighborhood
and interests, while in the latter, they are considerably more
dissimilar. Furthermore, we ﬁnd that we can infer which of
the pair of doppelg¨anger identities is legitimate (victim) and
which is the impersonator (attacker), by comparing various
reputation metrics, such as creation date, number of follow-
ers etc. We ﬁnd that victim identities almost always have
higher reputation and older creation dates than imperson-
ating identities.

We leverage these insights to propose automatic methods
(based on machine learning techniques) to detect imperson-
ation attacks in §4. We detect 10,894 more cases of imper-
sonation attacks and 9,354 cases of accounts managed by the
same person when we test the scheme on the 34,091 unla-
beled pairs of accounts that portray the same person. More
than half of the impersonating accounts detected by our
method were subsequently suspended by Twitter (over a 6

month period), which shows the eﬀectiveness of our method
at detecting impersonating accounts sooner than Twitter.

In summary, our paper proposes methods to gather data
about a large number of real-world impersonation attacks in
online social networks. Although our method does not cap-
ture all impersonation attacks, the resulting dataset allows
us to do an insightful exploration of impersonation attacks,
and to build an automatic detector that is not only able
to detect more such attacks, but also detect them sooner.
Our work represents a useful step towards enabling users to
better protect their online reputation from identity imper-
sonation attacks.
2. DATA GATHERING METHODOLOGY
In this section, we ﬁrst propose a methodology for gather-
ing data about impersonation attacks in real-world social
networks. Later, we apply our methodology to gather data
about impersonation attacks on Twitter.

Intuitively, an identity impersonation attack involves an
attacker creating an account (identity) pretending to be some
other real-world user (victim), i.e., the attacker’s identity
mimics or copies the features of the victim’s identity. How-
ever, gathering data about such attacks is surprisingly diﬃ-
cult in practice (which might explain why few prior studies,
if any, have successfully analyzed real-world impersonation
attacks).
2.1 Challenges
To understand why identifying impersonation attacks is hard,
consider the real-world example attack shown in Fig. 1. We
discovered this attack on “Nick Feamster”, a computer sci-
ence researcher, during the course of our study. We alerted
Nick, who conﬁrmed the attack, and the impersonating ac-
count has since been suspended by Twitter. We realized
three challenges in the process:

Figure 1: Example of impersonation attack.

1. How do we determine which identities are attempting
to portray or represent the same user? For instance,
there are many Twitter identities with the same user
name “Nick Feamster”, but we felt that only the ac-
count that shared similar bio and photo as Nick’s orig-
inal account,
is attempting to pretend to be Nick.
Our intuitive decision raises the question: how simi-
lar should the proﬁles of two identities be to qualify as
portraying the same user?

2. After determining that a pair of identities portray the
same user, how do we determine whether the identity-
pair is the result of an impersonation attack or the

142result of a user simply creating multiple (duplicate)
accounts for herself? For instance, users on Twitter
are permitted to create multiple identities, including
pseudonymous identities. To determine the shared
ownership of identities, we would need to contact and
obtain conﬁrmation from the identities’ owners them-
selves (e.g., by sending messages to the identities).
However, such conﬁrmations are not only hard to ob-
tain for a large-scale study, but also when we attempted
it on Twitter, the Twitter identity we created to con-
tact other Twitter users for the study got suspended
for attempting to contact too many unrelated Twitter
identities.

3. After determining that a pair of identities portraying
the same user is the result of an impersonation attack
(i.e., they are not duplicate accounts owned by the
same user), how do we determine which identity is le-
gitimate and which is the impersonating identity? In
our attack example, we were lucky to know the person
portrayed, Nick Feamster, in the oﬄine world. This
knowledge enabled us to diﬀerentiate the legitimate
identity from the attacker’s identity. But, in practice,
the oﬄine user portrayed by the online identities is
often unknown, i.e., it is unclear how to contact the
oﬄine user.
In these scenarios, it is unclear how to
diﬀerentiate the legitimate identity from the imperson-
ating identity, as both may claim to be the legitimate
identity.

Below we propose a practical methodology that tackles
some of these fundamental challenges and circumvents oth-
ers. We applied it on Twitter to gather data about several
hundreds to thousands of real-world impersonation attacks.
Our methodology does not guarantee that we would discover
all or even a representative sample of all impersonation at-
tacks occurring in Twitter. Nevertheless, given the inherent
diﬃculty in collecting any data about impersonation attacks,
we feel that our work represents a ﬁrst step in the direction
of addressing these challenges.
2.2 Terminology
We introduce some terminology to both simplify and clar-
ify our discussion in the rest of the paper.
(i) doppel-
g¨anger identities: We refer to identities as doppelg¨anger iden-
tities when they are determined to be portraying or repre-
senting the same user. (ii) avatar-avatar pair: A pair of
doppelg¨anger identities is referred to as an avatar-avatar pair
when both identities are managed by the same owner. (iii)
victim-impersonator pair: A pair of doppelg¨anger iden-
tities is referred to as a victim-impersonator pair when one
of the identities is legitimate (victim identity) and the other
is created by an attacker (impersonating identity). Using
our terminology, the above challenges can be rephrased as
follows:

1. How do we identify doppelg¨anger identities?

2. How do we determine that a pair of doppelg¨anger iden-
tities is an avatar-avatar pair or a victim-impersonator
pair?

3. How do we determine which of the victim-impersonator
identity-pair is the victim and which is the imperson-
ator?

2.3 Data gathering strategy
In this section, we discuss our strategy to tackle the above
challenges when gathering data. We defer a detailed discus-
sion of the collected data to the next section. At a high-level,
our strategy involves ﬁrst automatically identifying a large
number of pairs of doppelg¨anger identities and then diﬀer-
entiating them into avatar-avatar and victim-impersonator
pairs.

Identifying doppelgänger pairs

2.3.1
The ideal way to determine if a pair of identities is a doppel-
g¨anger pair would be to ask human workers if both identi-
ties portray the same user. Unfortunately, such an exercise
would be very expensive to scale to millions of potential
doppelg¨anger pairs. So we built an automated rule-based
matching scheme that is trained on human-annotated data
to determine when the proﬁle attributes of two identities
match suﬃciently for humans to believe that they portray
the same user.

More concretely, in the Twitter social network, every iden-
tity is associated with a set of proﬁle attributes, such as
user-name, screen-name, location, photo, and bio. We col-
lected pairs of identities with diﬀerent levels of proﬁle at-
tribute matching. Speciﬁcally, we collected three-levels of
matching proﬁle pairs: (i) Loosely matching identities: pairs
of identities that have similar user-name or screen-name;1
Starting from an initial set of Twitter proﬁles (see §2.4), we
discovered these identity pairs via the Twitter search API
that allows searching by names. (ii) Moderately matching
identities: pairs of identities that, in addition to sharing a
similar user-name or screen-name, also share one additional
similar proﬁle attribute be it location or photo or bio.2 In
practice, we found that location information is often very
coarse-grained, at the level of countries, so we deﬁned a
tighter matching scheme ignoring location information. (iii)
Tightly matching identities: pairs of identities that in ad-
dition to sharing a similar user-name or screen-name also
share similar photo or bio.

For each level of matching proﬁle pairs, we estimated the
fraction of proﬁle pairs that humans would believe as por-
traying the same user as follows: We selected between 50
to 250 pairs of matching proﬁles at each level and setup an
Amazon Mechanical Turk experiment, where we gave AMT
workers two links corresponding to the two Twitter accounts
and we asked them to choose between three options:
‘the
accounts portray the same person’, ‘the accounts do not por-
tray the same person’, or ‘cannot say’. For each assignment
we asked the opinion of three diﬀerent AMT workers and
only consider the majority agreement, i.e., when at least two
AMT workers choose the same answer.

We ﬁnd that, by majority agreement, AMT workers be-
lieve that 4% of loosely matching, 43% of moderately match-
ing, and 98% of tightly matching identity-pairs portray the
same user. Thus, by selecting a more conservative matching
scheme, we would increase precision (i.e., be more certain)
of detecting doppelg¨anger pairs. But, in the process, we

1Determining attribute similarity is a challenging task in
and of itself. There is a lot of prior work, including our own,
on this topic [10]. We summarize the relevant details in the
Appendix.
2Twitter accounts that do not have proﬁle information avail-
able such as bio, locations and photos will be automatically
excluded.

143would be sacriﬁcing recall – for instance, we found that the
tightly matching identity scheme captures only 65% of the
doppelg¨anger pairs caught by moderately matching identity
scheme. Since our goal is to correctly identify a large set
of impersonating attacks, even if it comes at the cost of not
identifying all possible impersonation attacks, we propose
to use the conservative tightly matching identity scheme to
detect doppelg¨anger pairs in Twitter.

Potential limitations: While our scheme represents a ﬁrst
step in the direction of scalable automated detection of dop-
pelg¨anger pairs in social networks like Twitter, currently,
we apply it only within a single social network. So we miss
opportunities to detect doppelg¨anger pairs across multiple
social networking sites, e.g., when an attacker copies a Face-
book user’s identity to created a doppelg¨anger Twitter iden-
tity. While our basic scheme could be extended to match
identities across sites, it is beyond the scope of this work.

2.3.2 Identifying victim-impersonator pairs
As discussed earlier, the ideal way to determine whether
a doppelg¨anger pair is a victim-impersonator pair requires
contacting the real oﬄine user represented by the identity
to inquire about the ownership of the identities. However,
this approach is infeasible in practice. When we attempted
this approach, our Twitter identity got quickly suspended
as indulging in potentially spam activity. So we instead rely
on a signal from Twitter, when it oﬃcially suspends one,
but not both, of the doppelg¨anger identities. We crawled
the doppelg¨anger identities periodically (once a week) over
an extended period of time (a three month period) to look
for identity suspensions. We treat the suspended identity of
the doppelg¨anger pair as the impersonating identity and the
other identity as the victim.

Our hypothesis is that at least some fraction of the im-
personating identities would be eventually detected and re-
ported to Twitter (either by the victim herself or some other
users that know the victim), which would result in Twitter
suspending the identity. One concern with our approach is
that we may be detecting impersonating attacks that are
being caught by some automated Twitter spam defense sys-
tem (as opposed to reports ﬁled by victims or other Twit-
ter users).
In this case, we would essentially be reverse-
engineering Twitter’s impersonation detection system rather
than study impersonation attacks in the wild. We address
this concern in §4.2, where we show that the analysis of
the impersonation attacks gathered using our methodology
can help us design new automated detectors that in turn
could be used to detect a large number of yet undiscovered
impersonation attacks in Twitter. Had the identities been
suspended by an automated impersonation detection frame-
work in Twitter, it is unlikely that we would have succeeded
in designing signiﬁcantly better performing detection sys-
tems.

Potential Limitations: Our victim-impersonator pair de-
tection strategy allows us to detect large numbers of such
pairs, but it likely captures only those impersonation attacks
that have been detected by Twitter’s reporting system. We
would be under-sampling clever attacks that have not yet
been detected.

2.3.3 Identifying avatar-avatar pairs
As discussed earlier, the ideal way to determine whether a
doppelg¨anger pair is avatar-avatar by contacting the owners

of the identities is unfeasible in practice. So we instead rely
on observing interactions between the doppelg¨anger iden-
tities that clearly indicate that each identity is aware of
the presence of the other identity. Speciﬁcally, we check
whether one of the doppelg¨anger identity follows or men-
tions or retweets the other doppelg¨anger identity. If it is the
case, it is very likely that the identities are managed by the
same user. Otherwise, the legitimate identity would have
reported the impersonating identity and have it suspended
by Twitter.

Potential Limitations: Our avatar-avatar pair detection
strategy under-samples scenarios where a user maintains
multiple identities but keeps them distinct, i.e., does not
link the identities and use them for very diﬀerent purposes.
However, we suspect that in such scenarios, users would
likely assume diﬀerent pseudonymous identities and would
avoid providing the same proﬁle information. Such identi-
ties would not be matched as doppelg¨anger identities in the
ﬁrst place.

2.4 Data gathered
In this section, we describe how we applied our above data
gathering strategy to collect information about real-world
impersonation attacks in the Twitter social network at scale.
We begin by selecting 1.4 million random Twitter ac-
counts – the initial accounts.3 We call the dataset we gen-
erate starting with these random Twitter accounts the Ran-
dom Dataset. For each account a in the Random Dataset,
we gather a set of up to 40 accounts in Twitter that have
the most similar names as the account (using the Twit-
ter search API). We call the resulting 27 million name-
matching identity-pairs initial accounts. From these pairs,
we identify doppelg¨anger pairs, victim-impersonator pairs,
and avatar-avatar pairs as described in §2.3. Table 1 sum-
marizes the dataset. Note that a signiﬁcant fraction of dop-
pelg¨anger identities are not labeled as either avatar-avatar
or victim-impersonator pairs.

While our strategy for detecting doppelg¨anger and avatar-
avatar pairs yielded sizable numbers, our strategy for detect-
ing victim-impersonator pairs proved quite time consuming.
It took a 3 months waiting time to discover 166 victim-
impersonator pairs amongst the 18,662 doppelg¨anger pairs
and few tens of identities keep getting suspended every pass-
ing week. To quickly identify more victim-impersonator
pairs, we resorted to a focussed crawl in the neighborhood of
the detected impersonating identities. Speciﬁcally, we con-
ducted a breadth ﬁrst search crawl on the followers of four
seed impersonating identities that we detected. Our intu-
ition is that we might ﬁnd other impersonating accounts in
the close network of an impersonating account.

We collected 142,000 accounts with the breadth ﬁrst search
crawl and we call the dataset generated from this biased set
of initial accounts the BFS Dataset. We repeated the same
analysis on the BFS Dataset that we conducted on Ran-
dom Dataset. We report the results in Table 1. In the same
amount of time, we discovered 16,408 victim-impersonator
pairs out of the 35,642 doppelg¨anger pairs, suggesting that
our focussed crawl succeeded in identifying a large number
of real-world impersonation attacks.

3Twitter assigns to every new account a numeric identity
that allows random sampling.

144Table 1: Datasets for studying impersonation attacks.

initial accounts
initial accounts
doppelg¨anger pairs
avatar-avatar pairs
victim-impersonator pairs
unlabeled pairs

Random Dataset

BFS Dataset

1.4 millions
27 millions

142,000

2.9 millions

18,662
2,010
166

16,486

35,642
1,629
16,408
17,605

For each Twitter identity in doppelg¨anger pairs, we use
the Twitter API to collect detailed information about a va-
riety of their features. They include features related to:

1. Proﬁle of the identity: We gather the data about the
identity’s user-name, screen-name, location, photo, and bio.

2. Activity of the identity: We gather data about the
creation date of the account, timestamp of the ﬁrst tweet,
timestamp of the last tweet, number of followings (the num-
ber of accounts a user follows), number of tweets posted,
number of retweets posted, number of tweets favorited and
number of mentions.

3. Reputation of the identity: We collect data about the
number of followers of an account, the number of expert lists
where the user appears and the klout score [16] as metrics
to measure the inﬂuence of an account. The klout score is
a widely used metric to measure the social inﬂuence of an
account.

3. CHARACTERIZING IMPERSONATION

ATTACKS

In this section, we analyze the datasets we collected to char-
acterize identity impersonation attacks in Twitter. We be-
gin by investigating the diﬀerent types of impersonation at-
tacks found in our Random Dataset. Our analysis reveals
the prevalence of a new type of impersonation attack that
we call doppelg¨anger bot attack. We then explore the
features of doppelg¨anger bot attacks and the potential for
detecting such attacks.
3.1 Classifying impersonation attacks
Based on conventional wisdom and anecdotal evidence, we
were expecting to discover two types of impersonation at-
tacks in our Random Dataset: (i) Celebrity impersonation
attacks, where attackers impersonate celebrities and popu-
lar Twitter users to either post untrustworthy information
maligning the celebrity’s reputation or take advantage of the
celebrities’ oﬄine popularity to increase the visibility of their
own posts (e.g., product promotions) or (ii) Social engineer-
ing attacks also known as identity theft attacks, where the
attacker creates a fake account that clones the information of
a victim account and then uses the fake account to connect
and communicate with the victim’s friends [5]. The ultimate
goal here is to launch phishing attacks to harvest sensitive
information about the victim or to trick the victim’s friends
into sending money to the attacker (that claims to be the
victim).

We attempted to map the 166 victim-impersonator pairs
in our dataset to these two types of attacks.
In the pro-
cess, we discovered that many of the victim-impersonators
pairs corresponded to a small number of victims. Speciﬁ-

cally, there were 6 diﬀerent Twitter victims that in total ac-
counted for half (83) of the victim-impersonator pairs. One
hypothesis is that these six victims discovered multiple fake
identities that were impersonating them and reported all
such identities, leading them to be detected in our method-
ology. To avoid over-sampling these identities in our dataset,
we only consider one pair of victim-impersonating identities
for each of the 6 victims, which reduces our dataset to 89
victim-impersonator pairs.
3.1.1 Celebrity impersonation attacks
Twitter allows users to create accounts that are fan pages of
celebrities, however, users have to speciﬁcally declare this in
their bios. Our method to collect data about impersonation
attacks in the previous section is consistent with Twitter’s
terms of service. If the fan account mentions or interacts in
any way with the celebrity, it will be identiﬁed as an avatar,
and if not, it will be identiﬁed as impersonator.

To identify celebrity impersonation attacks we simply check
for victim-impersonator pairs where the victim is either a
veriﬁed Twitter account 4 or has a popular following amongst
Twitter users, i.e., it has more than 1000 or 10,000 follow-
ers.5 Out of the 89 victim-impersonator pairs, we found only
three are celebrity impersonation attacks out of which one
is a impersonator of a football player and one of a journal-
ist. In fact, 70 of the 89 victims have less than 300 followers,
suggesting that most victims of impersonation in our dataset
are not highly popular celebrities.
3.1.2
While it is impossible to exactly know the intentions of the
attacker, we could attempt to infer if an impersonating iden-
tity is attempting to launch a social engineering attack, by
exploiting the observation that attackers try to contact the
friends of the victims. So we select all victim-impersonator
pairs where the impersonating account had any interaction
with users that know the victim account, i.e., the imper-
sonating account is friend of, follows, mentions or retweets
people that are friends of or follow the victim account.

Social engineering attacks

Our hypothesis is that it is unlikely that accounts that
do not fall in the candidate set would try to mount social
engineering attacks since they do not show any intent of
contacting the people that know the victim. Out of the 89
victim-impersonator pairs in our dataset, only two accounts
met our test for potentially being social engineering attacks.
3.1.3 Doppelgänger bot attacks
Our analysis of impersonation attacks above suggests that
most attacks are not directed towards popular celebrities
or attempting to trick a victim’s friends. This observa-
tion raises the question: What then might be motivating an
attacker to launch an impersonation attack targeting non-
celebrity users?

One hypothesis is that these impersonation attacks are
merely an attempt by attackers creating fake identities to
evade detection by Twitter Sybil and spam defense systems.
By simply copying the proﬁle attributes of existing Twit-
ter users, attackers could create new real-looking fake Twit-
ter identities that might escape traditional spam defenses

4Twitter has an account veriﬁcation program for highly pop-
ular users of its service.
5Less than 0.01% (0.007%) of Twitter users have more than
1000 (10,000) followers.

145that analyze features of individual identities. Such identi-
ties could be used proﬁtably to promote other Twitter users
and content in the system (selling fake followers and fake
content promotions is a growing business in Twitter). We
refer to such attacks as doppelg¨anger bot attacks.

To check our hypothesis that many impersonating ac-
counts are involved in follower fraud we analyzed whom
they are following. Since the number of doppelg¨anger bots
in our Random Dataset is limited to less than a hun-
dred, we investigated the doppelg¨anger bots in the BFS
Dataset, where we have identiﬁed tens of thousands of
victim-impersonator pairs using a BFS crawl of the Twitter
network starting with four seed doppelg¨anger bot accounts
(see Table 1). We found that the impersonating accounts in
the BFS Dataset follow a set of 3,030,748 distinct users.
Out of the users followed, 473 are followed by more than
10% of all the impersonating accounts. We checked if the
473 most followed accounts are suspected of having bought
fake followers in the past using a publicly deployed follower
fraud detection service [34]. Among those users for which
the service could do a check, 40% were reported to have
at least 10% fake followers. 6 The fact that a large frac-
tion of impersonating accounts follow the same small set of
users and that the users they follow are suspected of hav-
ing bought fake followers strongly points to the possibility
of impersonating accounts being involved in follower fraud.
3.2 Analyzing doppelgänger bot attacks
In this section, our goal is to better understand doppel-
g¨anger bot attacks with the ultimate goal of detecting dop-
pelg¨anger bots. To this end, we focus on understanding the
characteristics (i.e., various reputation metrics and activi-
ties) of doppelg¨anger bots and their victims. Our investi-
gation reveals important diﬀerences between the character-
istics of victim accounts and doppelg¨anger bots, which we
leverage in a later section to detect doppelg¨anger bots. The
doppelg¨anger bot attacks analyzed in this section are from
the BFS Dataset.

3.2.1 Characterizing victim accounts
To understand who the doppelg¨anger bot attacks are tar-
geting we proceed by asking several questions related to the
reputation and activity of victim accounts. We focus our
analysis on the diﬀerences in the statistical distributions of
properties of victim accounts and randomly chosen Twitter
users.

Reputation of victim accounts.
How popular are the victim accounts? Figure 2a shows
the CDF of the number of followers of victim accounts. The
median number of followers is only 73, which shows that
most doppelg¨anger bots do not target famous people but
ordinary users.
How inﬂuential are the victim accounts? Figures 2b
and 2c show the CDF of klout scores and the number
of expert lists where a victim account appears in, respec-
tively. 40% of victim accounts appear in at least one list

6For a point of comparison, we also checked whom the avatar
accounts from avatar-avatar pairs follow. There are only
four accounts followed by 10% of the avatar accounts and
they correspond to Justin Bieber, Taylor Swift, Katy Perry
and Youtube, all of which are well-known celebrity / corpo-
rate accounts followed widely on Twitter.

(a) Number of followers

(b) Klout score

(c) Number of lists the account
appears in

(d) Account creation date

(e) Number of followings

(f) Number of retweets

(g) Number of tweets favorited
by the user

(h) Number of mentions

(i) Number of tweets

(j) Date of the last tweet

Figure 2: CDFs of diﬀerent features for characterizing the
reputation and activity of impersonating and victim ac-
counts as well as Twitter accounts picked at random.

10010210400.51Number of followersCDF  ImpersonatorVictimRandom05010000.51Klout scoreCDF  ImpersonatorVictimRandom10010210400.51Number of listsCDF  ImpersonatorVictimRandom20042009201500.51Account creation dateCDF  ImpersonatorVictimRandom10010210400.51Number of followingsCDF  ImpersonatorVictimRandom10010200.51Number of retweetsCDF  ImpersonatorVictimRandom10010200.51Number of tweets favorited by the userCDF  ImpersonatorVictimRandom10010210400.51Number of mentionsCDF  ImpersonatorVictimRandom10010210400.51Number of tweetsCDF  ImpersonatorVictimRandom2010201500.51Date of the last tweetCDF  ImpersonatorVictimRandom146and 30% of victim accounts have klout scores higher than
25 (For comparison purposes, researchers like Dina Papa-
giannaki – @dpapagia – and Jon Crowcroft – @tforcworc –
have klout scores of 26 and 45 respectively, while Barack
Obama –@barackobama–has a klout score of 99). The ﬁg-
ures also show that the inﬂuence scores of victims are notice-
ably higher than those of random Twitter users, which indi-
cates that many victim accounts, while not exactly celebri-
ties, correspond to professional users with good reputation
in Twitter.
How old are the victim accounts? Figure 2d shows that
victims have generally older accounts than random Twitter
users. The median creation date for victim accounts is Octo-
ber 2010 while the median creation date for random Twitter
users is May 2012. Attackers target users that have been for
a long time in the system.

Activity of victim accounts.
How active are the victim accounts? Figures 2e through
2j show the CDFs of various activity metrics for victims.
They show that victims are considerably more active than
random Twitter users. For instance, Figure 2i shows the
CDF of the number of tweets per victim account. The me-
dian number of tweets is 181. In contrast, the median num-
ber of tweets for random users is 0, while the median num-
ber of tweets for random users that have at least one post is
20. Similarly, Figure 2j shows that 75% of victim accounts
posted at least one tweet in 2013, while only 20% of random
Twitter users posted at least one tweet in 2013. So the vic-
tims tend to be fairly active Twitter users that have been
active recently too.

In summary, the victims of doppelg¨anger bot attacks are
active users with a level of reputation and inﬂuence that
is signiﬁcantly higher than a random Twitter user (though
they are not necessarily celebrities). Our ﬁnding shows that
attackers target, inadvertently or not, users who put a sig-
niﬁcant amount of eﬀort into building an online image with
a good reputation.
3.2.2 Characterizing doppelgänger bot accounts
To understand how doppelg¨anger bot impersonating accounts
behave we analyze their reputation and activity.

Reputation of doppelgänger bot accounts.
Figures 2a through 2d compares the CDFs of diﬀerent rep-
utation metrics for doppelg¨anger bots, their victims, and
random Twitter accounts. The plots show that: (1) the
number of followers and klout score of impersonating ac-
counts is lower than the number of followers and klout score
of victim accounts but higher than the ones of random ac-
counts (Figures 2a and 2b); (2) impersonating accounts do
not appear in any lists of other accounts (Figure 2c); and (3)
most impersonating accounts were created recently, during
2013 (Figure 2d). At a high level, the reputation of doppel-
g¨anger bots is clearly lower than the reputation of victim ac-
counts, however, it is higher than the reputation of random
Twitter users. In the next section, we will show that these
diﬀerences have important implications for the detectability
of doppelg¨anger bots. Thus, impersonating accounts do not
have suspicious markers of reputation.

Activity of doppelgänger bot accounts.
We refer again to Figures 2e through 2j. The plots show
that: (1) the number of followings, retweets and favorites of
impersonating accounts is higher than victim and random
accounts (Figures 2e, 2f, and 2g). This ﬁnding is consis-
tent with our hypothesis that these accounts are involved
in illegal promotion of content and users; (2) However, the
number of times doppelg¨anger bots mention other users is
unusually low, which is consistent with the intuition that
doppelg¨anger bots would not not wish to draw attention to
themselves and their activities (Figure 2h); and (3) imper-
sonating accounts do not show excessive markers of activity:
the median number of users an impersonating account fol-
lows is 372, which is not very high compared with the median
number a victim account follows which is 111 and imperson-
ating accounts do not tweet excessively (Figure 2i), but are
very active, i.e. their last tweet is in the month we crawled
them (Figure 2j). These markers of activity not only support
our hypothesis that doppelg¨anger bots might be involved in
illegal promotion of content, but they also suggest that at-
tackers may be trying to emulate normal user behavior and
avoid being detected with abnormal (excessive) tweeting or
following behavior.
3.3 Detecting doppelgänger bot attacks
Even if doppelg¨anger bots do not seem to intentionally want
to harm their victims, they can still harm unintentionally the
online image of their victims. For example, in our dataset a
doppelg¨anger bot of a tech company tweeted “I think I was
a stripper in a past life”, which is clearly not the image the
tech company wants to promote. Even worse, Twitter took
in average 287 days to suspend these accounts.7 Thus, the
online image of victims was potentially harmed for several
months.

The natural question that follows is why do these accounts
go undetected for such long periods of time? In this section,
we analyze the challenges in detecting doppelg¨anger bots;
in particular our analysis shows that traditional sybil detec-
tion schemes are not able to accurately detect them. The
low accuracy can be potentially explained by the fact that
doppelg¨anger bots operate under the radar and the accounts
do not have any characteristics that makes them look sus-
picious in absolute. We show that the key to detect doppel-
g¨anger bots is to study their characteristics relative to their
corresponding victim accounts.

Using traditional sybil-detection schemes: reasoning
about the absolute trustworthiness of accounts.
Doppelg¨anger bots are a special type of sybil accounts, thus
we investigate whether we can detect them using existing
methods to detect sybil accounts. At a high level, many tra-
ditional sybil-detection schemes exploit ground truth about
good and bad users to create patterns of good and bad be-
havior. The schemes then decide whether an account is sybil
or not by comparing its behavior to these patterns [3, 40,
29]. We emulate such behavioral methods by training a SVM
classiﬁer with examples of doppelg¨anger bots (bad behav-
ior) and random Twitter accounts (good behavior) using
the methodology in [3].

7We know with an approximation of one week when Twitter
suspended the impersonating accounts and we know from
the Twitter API when the account was created.

147We consider all doppelg¨anger bots from the BFS Dataset
as positive examples (16,408 accounts), and we pick 16,000
random Twitter accounts as negative examples. We use 70%
of examples for training and 30% for testing and we train the
classiﬁer with all features that characterize the reputation
and the activity of a single account we presented in §2.4.

Because doppelg¨anger bots are only a small fraction of all
accounts in Twitter, our classiﬁcation problem has a high
class imbalance (i.e., there are many more negative exam-
ples than positive examples). Such scenarios require clas-
siﬁers that can operate at a very low false positive rate.
The smallest false positive rate our SVM classiﬁer achieves
is 0.1% for a 34% true positive rate (the true positive rate
drops to zero for lower false positive rates). A 0.1% false
positive rate is, however, very high in our scenario. For ex-
ample, if we take the 1.4 million random accounts (in the
Random Dataset), a classiﬁer with 34% true positive rate
for a 0.1% false positive rate will detect 40 actual doppel-
g¨anger bots (34%×122) while mislabeling 1,400 legitimate
accounts as doppelg¨anger bots. This accuracy is clearly un-
satisfying.

A plausible reason why these schemes are not optimal is
precisely because attackers intentionally create real-looking
accounts and emulate the behavior of legitimate users so that
they are harder to detect by current sybil account detection
systems.

Distinguish doppelgänger bots from victim accounts:
reasoning about the relative trustworthiness of accounts.

The previous section showed that, given a set of accounts, it
is hard to detect which ones are doppelg¨anger bots. Here we
try to approach the problem from a diﬀerent perspective and
we ask a diﬀerent question: given a victim-impersonator pair
can we pinpoint the impersonating account? Our intuition
is that, if it is too hard to reason about the trustworthiness
of an account in absolute, it might be easier to reason about
its trustworthiness relative to another account.

To answer the question we refer back to Figure 2 that
presents the CDFs for diﬀerent features of impersonating,
victim and random accounts. We can see that the charac-
teristics of victim accounts in aggregate are very diﬀerent
from the characteristics of impersonating accounts. More
precisely, victim accounts have a much higher reputation
(number of followers, number of lists and klout scores) than
impersonating accounts and they have a much older account
creation date. In fact, in all the victim-impersonator pairs
in the BFS Dataset and Random Dataset, none of the
impersonating accounts have the creation date after the cre-
ation date of their victim accounts and 85% of the victim
accounts have a klout score higher than the one of the im-
personating accounts. Thus, to detect the impersonating ac-
count in a victim-impersonator pair with no miss-detections,
we can simply take the account that has the more recent cre-
ation date. This reasoning opens up solutions to detect dop-
pelg¨anger bots, however, it does not solve the whole problem
because we still have to detect whether a pair of accounts
is a avatar-avatar pair or victim-impersonator pair. This is
the focus of section §4.

How well humans can detect doppelgänger bots.
In this section, we investigate how well humans are able to
detect doppelg¨anger bots. We focus on two questions: (1)

If humans stumble upon a doppelg¨anger bot, are they able
to detect that the account is fake?
(i.e., the question of
assessing the absolute trustworthiness of accounts) – this
scenario is speciﬁc to a recruiter that knows the name of
the person and searches for his accounts in diﬀerent social
networks to learn more about him and stumbles upon the
doppelg¨anger bot; and (2) If humans have access to both
the impersonating and the victim account, are they able to
detect the impersonating account better? (i.e., the question
of assessing the relative trustworthiness of accounts). The
ﬁrst question will show the severity of the doppelg¨anger bot
attacks problem by analyzing whether humans are tricked
into believing the doppelg¨anger bots represent the real per-
son. The second question will show whether humans are also
better at detecting impersonating accounts when they have
a point of reference.

We build two AMT experiments to answer these ques-
tions. For the ﬁrst AMT experiment, we select 50 doppel-
g¨anger bot accounts and 50 avatar accounts from the victim-
impersonator pairs and avatar-avatar pairs. In each assign-
ment, we give AMT workers a link to a Twitter account and
we ask them to choose between three options: ‘the account
looks legitimate’, ‘the account looks fake’ and ‘cannot say’.
We mix doppelg¨anger bot accounts and avatar accounts in
the experiments to force users to examine each case afresh.
In all experiments we ask the opinion of three AMT workers
and we report the results for majority agreement. In this
experiment, AMT workers are able to only detect 18% of
the doppelg¨anger bots as being fake (9 out of 50). Thus,
most AMT workers get tricked by doppelg¨anger bots.

In the second experiment we show AMT workers two ac-
counts that portray the same person. We picked the same 50
impersonating accounts (and their corresponding victims)
and the same 50 avatar accounts (and their correspond-
ing avatar doppelg¨anger ) as in the previous experiment.8
In each assignment, we give AMT workers two links corre-
sponding to the two Twitter accounts and we ask them to
choose between ﬁve options: ‘both accounts are legitimate’,
‘both accounts are fake’, ’account 1 impersonates account 2’,
’account 2 impersonates account 1’, and ’cannot say’. In the
second experiment, AMT workers were able to correctly de-
tect 36% doppelg¨anger bots as fake. The experiment shows
that there is a 100% improvement in their detection rate
when they have a point of reference.

The results in this section have implications on both how
to design automatic techniques to detect doppelg¨anger bots
as well as how to design systems that better protect users
from being deceived online by impersonation attacks.

4. DETECTING IMPERSONATION ATTACKS
The previous section showed that given a victim-impersonator
pair of accounts, we can fairly accurately detect the imper-
sonating account by comparing their account creation times
and reputations. In this section, we investigate the extent
to which we can detect whether a pair of accounts that por-
trays the same person (a doppelg¨anger pair) is a victim-
impersonator pair or an avatar-avatar pair. We start by
analyzing features that can signal the existence of an imper-
sonation attack and we then propose a method to automat-
ically detect such attacks.

8The AMT workers were diﬀerent in the two experiments.

148(a) User-name

(b) Screen-name

(a) Number of common fol-
lowings

(b) Number of common fol-
lowers

(c) Location

(d) Photo

(c) Number of common men-
tioned users

(d) Number
retweeted users

of

common

Figure 4: CDFs of the social neighborhood overlap be-
tween accounts in victim-impersonator pairs and avatar-
avatar pairs.

(e) Bio

(f) Interests similarity

Figure 3: CDFs of the similarity between accounts in victim-
impersonator pairs and avatar-avatar pairs.

Twitter suspension signals and direct account interactions
are very good signals to create a dataset to study imperson-
ation attacks, however, there are still many doppelg¨anger pairs
that are not yet labeled (e.g., there are 16,486 unlabeled
pairs in the Random Dataset). Thus, a secondary goal
of this section is to investigate whether we can detect addi-
tional victim-impersonator pairs and avatar-avatar pairs in
the doppelg¨anger pairs in our dataset.
4.1 Features to detect impersonation attacks
To detect impersonation attacks we consider features that
characterize pairs of accounts and that can potentially diﬀer-
entiate victim-impersonator pairs from avatar-avatar pairs.
We consider all victim-impersonator pairs and avatar-avatar
pairs from the Random Dataset and BFS Dataset com-
bined (we call the combined dataset the Combined Dataset)
to analyze how well the features distinguish between the two
kinds of pairs of accounts.

Proﬁle similarity between accounts.
We ﬁrst analyze the similarity between proﬁle attributes
such as user-names, screen-names, locations, proﬁle photos
and bios (refer to the Appendix for details on how we com-
pute the similarity scores for diﬀerent attributes). Even if
these features were used to collect the dataset of doppel-
g¨anger pairs we can still use them to distinguish between

avatar-avatar pairs and victim-impersonator pairs. In addi-
tion, we measure the similarity between the interests of two
accounts. We use the algorithm proposed by Bhattacharya
et al. [4] to infer the interests of a user.

Figure 3 compares the CDFs of the pairwise proﬁle sim-
ilarity between accounts in avatar-avatar pairs and victim-
impersonator pairs. For user-names, screen-names, photo
and interests similarity, a value of zero means no similar-
ity while one means perfect similarity. For the location,
the similarity is the distance in kilometers between the two
locations, thus a value of zero means the locations are the
same. For bio, the similarity is the number of common words
between two proﬁles, the higher the similarity the more con-
sistent the bios are.

We observe that the similarity between user-names, screen-

names, proﬁle photos and bios is higher for victim-impersonator
pairs than avatar-avatar pairs. Thus, users that maintain
multiple avatar accounts do not spend the eﬀort to make
their accounts look similar, while impersonators do. On the
other hand, the similarity between the interests of avatar-
avatar pairs is higher than the victim-impersonator pairs.
We did not expect such high similarity between avatar-avatar
pairs because we believed that people maintain distinct ac-
counts to promote diﬀerent sides of their persona.

Social neighborhood overlap.
We call the set of users an account interacts with in a social
network the social neighborhood of the account. On Twit-
ter, the social neighborhood of an account a consists of the
followings and followers of a as well as the users mentioned
by a and the users retweeted by a. An overlap in the social
neighborhood suggests that two accounts are positioned in
the same part of the social network graph. This can be in-

00.5100.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar00.20.40.60.800.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar05000100001500000.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar00.5100.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar0102000.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar00.20.40.60.800.51SimilarityCDF  Victim−ImpersonatorAvatar−Avatar10010210400.51Number of common followingsCDF  Victim−ImpersonatorAvatar−Avatar10010210400.51Number of common followersCDF  Victim−ImpersonatorAvatar−Avatar010020000.51Number of common mentioned usersCDF  Victim−ImpersonatorAvatar−Avatar010203000.51Number of common retweeted usersCDF  Victim−ImpersonatorAvatar−Avatar1494.2 Automated detection method
To build an automated method to detect impersonation at-
tacks we build a SVM classiﬁer, with linear kernel, that
distinguishes victim-impersonator pairs from avatar-avatar
pairs. We use, from the Combined Dataset, the victim-
impersonator pairs as positive examples, and avatar-avatar
pairs as negative examples to train the classiﬁer. We use
all the features presented in §4.1 as well as all the features
that characterize individual accounts presented in §2.4 for
the training. Since the features are from diﬀerent categories
and scales (e.g., time in days and distances in kilometers),
we normalize all features values to the interval [-1,1].

We use 10-fold cross validation over the Combined Dataset

to train and test the classiﬁer. The SVM classiﬁer, for each
pair of accounts, outputs a probability of the pair to be
a victim-impersonator pair. To perform the detection of
victim-impersonator pairs and avatar-avatar pairs, we then
proceed as follows. If the probability is higher than a cer-
tain threshold th1 we conclude that the pair is a victim-
impersonator pair and if the probability is lower than a cer-
tain threshold th2 (diﬀerent than th1) the pair is a avatar-
avatar pair. Note that if th1 > th2, some pairs may remain
unlabeled. This is done on purpose here because it is prefer-
able in our problem to leave a pair unlabeled rather than
wrongly label it (i.e., label avatar-avatar pairs as victim-
impersonator pairs or vice versa). We therefore select thresh-
olds th1 and th2 such that there are very few false positives
(i.e., few victim-impersonator pairs mislabeled as avatar-
avatar pairs or vice versa). The resulting classiﬁer is able to
achieve a 90% true positive rate for a 1% false positive rate
to detect victim-impersonator pairs and a 81% true posi-
tive rate for a 1% false positive rate to detect avatar-avatar
pairs.9 Thus, we can detect a signiﬁcant fraction of victim-
impersonator pairs using only features that compare the
reputation and activity of accounts in a doppelg¨anger pair.
Therefore, it is possible to detect impersonation attacks au-
tomatically rather than wait for victims to report them or
wait for the accounts to do something clearly malicious in
order to be suspended by Twitter.

Potential limitations: Our detection method above, while
quite eﬀective today at detecting whether a doppelg¨anger pair
is the result of an impersonation attack, is not necessarily
robust against adaptive attackers that might change their
strategy to avoid detection in the future. Similar to spam
detection, system operators to constantly retrain the de-
tectors (classiﬁers) to account for new attacker strategies.
Also note that the accuracy percentages above only refer to
the accuracy of detecting whether a doppelg¨anger pair is a
victim-impersonator pair and does not include the accuracy
of detecting a doppelg¨anger pair or the accuracy of detecting
the doppelg¨anger bot account within a victim-impersonator
pair of accounts.
4.3 Classifying unlabeled doppelgänger pairs
We apply the classiﬁer over the 17,605 unlabeled pairs from
the BFS Dataset and the 16,486 unlabeled pairs from the
Random Dataset. With a threshold corresponding to 1%
false positive rate (for both detecting victim-impersonator
pairs and avatar-avatar pairs) the classiﬁer is able to iden-
tify 4,390 avatar-avatar pairs and 1,863 victim-impersonator

9Since there is little to no class imbalance in this classiﬁca-
tion problem, contrary to §3.3, a 1% false positive rate is
low enough.

(a) Time diﬀerence between
creation dates

(b) Time diﬀerence between
the last tweet

Figure 5: CDFs of the time diﬀerence in days between ac-
counts in victim-impersonator pairs and avatar-avatar pairs.

dicative of two things: (1) the two accounts correspond to
avatars managed by the same user; or (2) potential evidence
of social engineering attacks. We use four features to mea-
sure the social neighborhood overlap: the number of com-
mon followings, the number of common followers, the num-
ber of overlapping users mentioned and the number of over-
lapping users retweeted by both accounts, which we present
in Figure 4.

There is a striking diﬀerence between avatar-avatar pairs
and victim-impersonator pairs: while victim-impersonator
pairs almost never have a social neighborhood overlap, avatar
accounts are very likely to have an overlap. Social neighbor-
hood overlap is also indicative of social engineering attacks
but there are not many such attacks in our dataset to be
visible in the plots.

Time overlap between accounts.
We add features related to the time overlap between two
accounts: time diﬀerence between the creation dates, time
diﬀerence between the last tweets, time diﬀerence between the
ﬁrst tweets and whether one account stopped being active
after the creation of the second account (we call this feature
outdated account). Figure 5 compares the diﬀerence between
creation dates and the date of the last tweet. Figure 5a
shows that there is a big diﬀerence between account creation
times for victim-impersonator pairs while for avatar-avatar
pairs the diﬀerence is smaller.

Differences between accounts.
Finally, we consider a set of features that represent the dif-
ference between diﬀerent numeric features that characterize
individual accounts: klout score diﬀerence, number of follow-
ers diﬀerence, number of friends diﬀerence, number of tweets
diﬀerence, number of retweets diﬀerence, number of favor-
ited tweets diﬀerence, number of public list diﬀerence. Our
intuition was that the diﬀerence between numeric features
of accounts in avatar-avatar pairs will be smaller than for
victim-impersonator pairs, e.g., a small klout score diﬀerence
between two accounts could be indicative of avatars of the
same person while a large klout score diﬀerence could be in-
dicative of an impersonation attack. To our surprise, the dif-
ference is generally slightly smaller for victim-impersonator
pairs.

Overall, the best features to distinguish between victim-
impersonator pairs and avatar-avatar pairs are the interest
similarity, the social neighborhood overlap as well as the
diﬀerence between the creation dates of the two accounts.

01000200000.51DifferenceCDF  Victim−ImpersonatorAvatar−Avatar01000200000.51DifferenceCDF  Victim−ImpersonatorAvatar−Avatar150Table 2: Unlabeled doppelg¨anger pairs in our dataset that
we can labeled using the classiﬁer.

BFS Dataset

(17,605 unlabeled)

Random Dataset
(16,486 unlabeled)

victim-impersonator pairs
avatar-avatar pairs

9,031
4,964

1,863
4,390

pairs in the Random Dataset (see Table 2). Thus, the clas-
siﬁer can identify a large additional number of avatar-avatar
pairs and victim-impersonator pairs that were not caught in
the initial dataset. For example, on top of the 166 exam-
ples of victim-impersonator pairs we initially labeled, the
classiﬁer labels 1,863 additional victim-impersonator pairs.
We re-crawled all doppelg¨anger pairs (from both datasets)
in May 2015 (the initial crawl ended in Dec 2014), and 5,857
out of the 10,894 victim-impersonator pairs detected by our
classiﬁer were suspended by Twitter. This result shows the
eﬀectiveness of our method at detecting victim-impersonator
pairs sooner than Twitter.

5. RELATED WORK
The closest to our work are a few studies on social engineer-
ing attacks which we will review in more detail. Also related
are studies of matching accounts across social networks and
sybil account detection techniques that we review at a more
higher level.

Social engineering attacks.
We focus in this paper on a broader set of attacks that im-
personate people, of which, social engineering attack are a
subclass. Bilge et al.
[5] demonstrated the feasibility of
automatically creating cloned proﬁles in social networks,
however, they did not propose techniques to detect the at-
tacks. The closest to our work are three studies [17, 13,
15] that made some initial investigations toward detecting
proﬁle cloning. The studies hinted at the fact that cloned
proﬁles can be identiﬁed by searching for proﬁles with sim-
ilar visual features, but they either stopped at returning a
ranked list of accounts that are similar with the victim ac-
count [17, 13], or to just test their technique on simulated
datasets [15]. In contrast, we actually detect accounts that
portray the same person in real-world social networks with
high accuracy and we also detect whether they are involved
in an impersonation attack or they are legitimate. Further-
more, we propose a technique to gather data about imper-
sonation attacks in real-world social networks and we do the
ﬁrst, to our knowledge, characterization of impersonation at-
tacks in Twitter. On the protection part, He et al. [11] pro-
posed ways to protect against friend requests coming from
cloned proﬁles by using adjacent mediums such as instant
chats to verify if the request comes from the real person.
To protect users we showed that humans are much better
at detecting impersonating accounts when they can also see
the victim account. Thus a system that protect users from
friend requests coming from cloned proﬁles could simply just
show the user all the accounts that portray the same person
with the account that is requesting the friendship.

Account matching.
There are a number of works that propose methods to match
the accounts a user has on multiple social networks that
are related to our techniques to detect doppelg¨anger pairs.
Note, however, the subtle diﬀerence, our goal is to ﬁnd ac-
counts that people think they portray the same person which
is slightly diﬀerent than detecting accounts that belong to
the same user. To detect doppelg¨anger pairs we ﬁrstly have
to only rely on visual features of accounts and then under-
stand when humans get confused.

While there are several studies that exploited visual fea-
tures similar to the features we use in this paper to match
accounts (refer to [9] for an overview), none of these stud-
ies applied their methods to detect impersonation attacks.
Most of the studies build classiﬁers that are able to detect
whether two accounts belong or not to the same person. We
drew inspiration from these works, however, we could not di-
rectly apply these techniques to gather doppelg¨anger pairs
because we could not estimate their accuracy as there is
no existing ground truth of accounts that portray the same
person in the same social network.

Sybil account detection.
One of the most widely used approach today to detect fake
accounts is to build behavioral proﬁles for trusted and un-
trusted users [3, 40, 29]. The behavioral proﬁle can include,
for example, the act of sending messages to other identities,
following identities, or rating a particular piece of content.
We showed that behavioral proﬁles are not optimal for de-
tecting impersonating accounts and that we have to exploit
features that characterize pairs of identities to identify im-
personating accounts.

To assess the trustworthiness of identities, another type
of information typically available on social networks is trust
relationship between identities (e.g., friendship relationship
between identities). Researchers have proposed a variety
of schemes such as SybilGuard [39] and SybilRank [6] that
analyze trust networks between identities to assess the trust-
worthiness of identities and thus identify Sybil attackers [39,
36, 35]. The key assumption is that an attacker cannot es-
tablish an arbitrary number of trust edges with honest or
good users in the network. This assumption might break
when we have to deal with impersonating accounts as for
them it is much easier to link to good users, but it would
be interesting to see whether these techniques are able to
detect doppelg¨anger bots.

A third approach to identify suspicious identities is to
crowdsource this task to experts who are familiar with iden-
tifying suspicious proﬁles or actions. Social networking ser-
vices typically have a tiered approach where suspicious pro-
ﬁles reported by end users are further veriﬁed by a group
of experts before taking a decision to suspend the account
or show Captchas to those suspicious users [6]. In fact, re-
searchers recently explored the possibility of using online
crowdsourcing services such as Amazon Mechanical Turk
(AMT) to crowdsource the task of detecting sybil identities
in a social network [37]. Our AMT experiments showed,
however, that such techniques are not optimal for detecting
impersonating accounts because AMT workers get tricked
easily to believe that impersonating accounts are legitimate.

1516. CONCLUSION
We conducted the ﬁrst study to characterize and detect iden-
tity impersonation attacks in Twitter. The key enabler of
this study is our method to gather data of impersonation at-
tacks. Our method is general and can be used to gather data
in other social networks such as Facebook and LinkedIn.

Besides celebrity impersonators and social engineering at-
tacks, we discovered a new type of impersonation attacks
where attackers copy the proﬁles of legitimate users to create
real-looking fake accounts that are used to illegally promote
content on Twitter. Our analysis revealed that attackers
target a wide range of users and anyone that has a Twitter
account can be victim of such attacks.

Finally, we proposed an automated technique to detect
impersonation attacks, that is able to detect 1,863 more im-
personation attacks in our dataset (up from 166).

Our ﬁndings reveal a new type of privacy threat against
the online image of users. Many surveys [12, 38] state that
U.S. ﬁrms do background checks for job applicants that in-
volve mining data from their online proﬁles. In this scenario,
the doppelg¨anger bot attacks can potentially have a signif-
icant negative impact on the online image of users if the
employer stumbles by mistake across the impersonating ac-
count.

7. REFERENCES
[1] Bing Maps API. http:

//www.microsoft.com/maps/developers/web.aspx.
[2] Get better results with less eﬀort with Mechanical

Turk Masters – The Mechanical Turk blog.
http://bit.ly/112GmQI.

[3] F. Benevenuto, G. Magno, T. Rodrigues, and

V. Almeida. Detecting spammers on Twitter. In
CEAS’10.

[4] P. Bhattacharya, M. B. Zafar, N. Ganguly, S. Ghosh,

and K. P. Gummadi. Inferring user interests in the
twitter social network. In RecSys ’14.

[5] L. Bilge, T. Strufe, D. Balzarotti, and E. Kirda. All
your contacts are belong to us: Automated identity
theft attacks on social networks. In WWW’09.

[6] Q. Cao, M. Sirivianos, X. Yang, and T. Pregueiro.
Aiding the detection of fake accounts in large scale
social online services. In NSDI’12.

[7] W. W. Cohen, P. Ravikumar, and S. E. Fienberg. A

comparison of string distance metrics for
name-matching tasks. In IJCAI’03.

[8] S. Corpus, 2015. http://anoncvs.postgresql.org/

cvsweb.cgi/pgsql/src/backend/snowball/stopwords/.

[9] O. Goga. Matching User Accounts Across Online
Social Networks: Methods and Applications. PhD
thesis, Universit´e Pierre et Marie Curie, 2014.

[10] O. Goga, P. Loiseau, R. Sommer, R. Teixeira, and
K. Gummadi. On the reliability of proﬁle matching
across large online social networks. In KDD, 2015.
[11] B.-Z. He, C.-M. Chen, Y.-P. Su, and H.-M. Sun. A

defence scheme against identity theft attack based on
multiple social networks. Expert Syst. Appl., 2014.

[12] Internetnews. Microsoft survey: Online ’reputation’

counts, 2010. http://www.internetnews.com/
webcontent/article.php/3861241/Microsoft+Survey+
Online+Reputation+Counts.htm.

[13] L. Jin, H. Takabi, and J. B. Joshi. Towards active
detection of identity clone attacks on online social
networks. In CODASPY ’11.

[14] A. M. Kakhki, C. Kliman-Silver, and A. Mislove.
Iolaus: Securing online content rating systems. In
WWW’13.

[15] M. Y. Kharaji, F. S. Rizi, and M. Khayyambashi. A

new approach for ﬁnding cloned proﬁles in online
social networks. International Journal of Network
Security, 2014.

[16] Klout. Klout, 2014. http://klout.com/.
[17] G. Kontaxis, I. Polakis, S. Ioannidis, and E. Markatos.

Detecting social network proﬁle cloning. In
PERCOM’11.

[18] D. G. Lowe. Distinctive image features from

scale-invariant keypoints. Int. J. Comput. Vision,
2004.

[19] Mediabistro. Was twitter right to suspend ’christopher

walken’ ?, 2009.
https://www.mediabistro.com/alltwitter/
was-twitter-right-to-suspend-christopher-walken
b5021.

[20] A. Mislove, A. Post, K. P. Gummadi, and P. Druschel.

Ostra: Leveraging trust to thwart unwanted
communication. In NSDI’08.

[21] M. Mondal, B. Viswanath, A. Clement, P. Druschel,
K. P. Gummadi, A. Mislove, and A. Post. Defending
against large-scale crawls in online social networks. In
CoNEXT’12.

[22] Nairobiwire. Sonko’s facebook impersonator arrested,

2014. http://nairobiwire.com/2014/07/
mike-sonko-arrested-swindling-public.html?utm
source=rss&utm medium=rss&utm campaign=
mike-sonko-arrested-swindling-public.

[23] D. Perito, C. Castelluccia, M. Ali Kˆaafar, and

P. Manils. How unique and traceable are usernames?
In Proceedings of the 11th Privacy Enhancing
Technologies Symposium (PETS), 2011.

[24] Phash. http://www.phash.org.
[25] A. Post, V. Shah, and A. Mislove. Bazaar:

Strengthening user reputations in online marketplaces.
In NSDI’11.

[26] Seattlepi. Racism and twitter impersonation prompt

lawsuit for kirkland teen, 2010.
http://www.seattlepi.com/local/sound/article/Racism-
and-Twitter-impersonation-prompt-lawsuit-
893555.php.

[27] Social Intelligence Corp. http://www.socialintel.com/.
[28] Spokeo. http://www.spokeo.com/.
[29] T. Stein, E. Chen, and K. Mangla. Facebook immune

system. In SNS’11.

[30] Turnto23. Impersonator continuously creating fake

facebook proﬁles of a well known bakersﬁeld pastor.
http://www.turnto23.com/news/local-
news/impersonator-continuously-creating-fake-
facebook-proﬁles-of-a-bakersﬁeld-pastor.

[31] Twitter. Explaining twitter’s eﬀorts to shut down

spam. https:
//blog.twitter.com/2012/shutting-down-spammers,
2012.

152[32] Twitter. Twitter reporting impersonation accounts,

2014. https://support.twitter.com/articles/20170142-
reporting-impersonation-accounts.

[33] B. Viswanath, M. A. Bashir, M. Crovella, S. Guha,

K. Gummadi, B. Krishnamurthy, and A. Mislove.
Towards detecting anomalous user behavior in online
social networks. In USENIX Security’14.

[34] B. Viswanath, M. A. Bashir, M. B. Zafar, L. Espin,

K. P. Gummadi, and A. Mislove. Trulyfollowing:
Discover twitter accounts with suspicious followers.
http://trulyfollowing.app-ns.mpi-sws.org/, April 2012.
Last accessed Sept 6, 2015.

[35] B. Viswanath, M. Mondal, A. Clement, P. Druschel,

K. Gummadi, A. Mislove, and A. Post. Exploring the
design space of social network-based sybil defenses. In
COMSNETS’12.

[36] B. Viswanath, A. Post, K. P. Gummadi, and

A. Mislove. An analysis of social network-based sybil
defenses. In SIGCOMM ’10.

[37] G. Wang, M. Mohanlal, C. Wilson, X. Wang, M. J.

Metzger, H. Zheng, and B. Y. Zhao. Social turing
tests: Crowdsourcing sybil detection. In NDSS’13.

[38] Wikibin. Employers using social networks for

screening applicants, 2008.
http://wikibin.org/articles/employers-using-social-
networks-for-screening-applicants.html.

[39] H. Yu, M. Kaminsky, P. B. Gibbons, and A. Flaxman.
Sybilguard: Defending against sybil attacks via social
networks. In SIGCOMM ’06.

[40] C. M. Zhang and V. Paxson. Detecting and analyzing

automated activity on twitter. In PAM’11.

APPENDIX

Here we ﬁrst explain how we computed similarity between
various attribute values (e.g., names and photos) of accounts
and then describe the procedure we used to determine when
two attribute values (e.g., two names or two photos) are
“similar enough” to be deemed to represent the same entity.
A. SIMILARITY METRICS
Name similarity Previous work in the record linkage com-
munity showed that the Jaro string distance is the most
suitable metric to compare similarity between names both
in the oﬄine and online worlds [7, 23]. So we use the Jaro
distance to measure the similarity between user-names and
screen-names.

Photo similarity Estimating photo similarity is tricky as
the same photo can come in diﬀerent formats. To mea-
sure the similarity of two photos while accounting for image
transformations, we use two matching techniques: (i) per-
ceptual hashing, a technique originally invented for identify-
ing illegal copies of copyrighted content that works by reduc-

ing the image to a transformation-resilient “ﬁngerprint” con-
taining its salient characteristics [24] and (ii) SIFT, a size
invariant algorithm that detects local features in an image
and checks if two images are similar by counting the number
of local features that match between two images [18]. We
use two diﬀerent algorithms for robustness. The perceptual
hashing technique does not cope well with some images that
are resized, while the SIFT algorithm does not cope well
with computer generated images.

Location similarity For all proﬁles, we have the textual
representations of the location, like the name of a city. Since
social networks use diﬀerent formats for this information,
a simple textual comparison will be inaccurate.
Instead,
we convert the location to latitude/longitude coordinates
by submitting them to the Bing API [1]. We then compute
the similarity between two locations as the actual geodesic
distance between the corresponding coordinates.

Bio similarity The similarity metric is simply the num-
ber of common words between the bios of two proﬁles after
removing certain frequently used stop words (as is typically
done in text retrieval applications). As the set of stop words,
we use a popular corpus available for several languages [8].

B. SIMILARITY THRESHOLDS
Clearly the more similar two values of an attribute, the
greater the chance that they refer to the same entity, be it a
user-name or photo or location. To determine the threshold
similarity beyond which two attribute values should be con-
sidered as representing the same entity, we rely on human
annotators. Speciﬁcally, we attempt to determine when two
attribute values are similar enough for humans to believe
they represent the same entity.

We gathered human input by asking Amazon Mechanical
Turk (AMT) users to evaluate whether pairs of attribute
values represent the same entity or not. We randomly select
200 pairs of proﬁles and asked AMT users to annotate which
attribute values represent the same entity and which do not.
We followed the standard guidelines for gathering data from
AMT workers [2].

For each attribute, we leverage the AMT experiments to
select the similarity thresholds to declare two values as rep-
resenting the same entity. Speciﬁcally, we select similarity
thresholds, such that more than 90% of values that repre-
sent the same entity (as identiﬁed by AMT workers) and less
than 10% of the values that represent diﬀerent entities (as
identiﬁed by AMT workers) have higher similarities. Conse-
quently, we determine that two user-names or screen-names
represent the same name if they have a similarity higher
than 0.79, and 0.82 respectively. Two locations represent
the same place if they are less than 70km apart. Two photos
represent the same image if their SIFT similarity is higher
than 0.11 and two bios describe the same user if they have
more than 3 words in common.

153