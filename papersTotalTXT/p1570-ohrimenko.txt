Observing and Preventing Leakage in MapReduce

Olga Ohrimenko
Microsoft Research

oohrim@microsoft.com

Christos Gkantsidis

Microsoft Research

christos.gkantsidis@microsoft.com

Manuel Costa
Microsoft Research

manuelc@microsoft.com

Markulf Kohlweiss
Microsoft Research

markulf@microsoft.com

Cédric Fournet
Microsoft Research
˚

fournet@microsoft.com

Divya Sharma

Carnegie Mellon University

divyasharma@cmu.edu

ABSTRACT
The use of public cloud infrastructure for storing and pro-
cessing large datasets raises new security concerns. Cur-
rent solutions propose encrypting all data, and accessing it
in plaintext only within secure hardware. Nonetheless, the
distributed processing of large amounts of data still involves
intensive encrypted communications between diﬀerent pro-
cessing and network storage units, and those communica-
tions patterns may leak sensitive information.

We consider secure implementation of MapReduce jobs,
and analyze their intermediate traﬃc between mappers and
reducers. Using datasets that include personal and geo-
graphical data, we show how an adversary that observes the
runs of typical jobs can infer precise information about their
input. We give a new deﬁnition of data privacy for MapRe-
duce, and describe two provably-secure, practical solutions.
We implement our solutions on top of VC3, a secure imple-
mentation of Hadoop, and evaluate their performance.

Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection.

Keywords
Map-reduce; traﬃc analysis; oblivious shuﬄe; oblivious load
balancing.

1.

INTRODUCTION

The use of shared cloud infrastructure for storing and
processing large structured datasets has gained widespread
prominence.
In particular, the MapReduce framework is
routinely used to outsource such tasks in a simple, scalable,
and cost-eﬀective manner. As can be expected, reliance on
a cloud provider for processing sensitive data entails new
integrity and privacy risks.

˚Work done at Microsoft Research.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813695.

Several recent works explore diﬀerent trade-oﬀs between
performance, security, and (partial) trust in the cloud. Most
proposals involve protecting data at rest—using some form
of authenticated encryption—and protecting data in use with
either advanced cryptography or secure hardware. Although
homomorphic encryption [10] may address our privacy con-
cerns, it remains impractical for general processing of large
data, in particular when they involve complex, dynamic in-
termediate data. Conversely, limited trust assumptions on
the cloud infrastructure may lead to eﬃcient solutions, but
their actual security guarantees are less clear.

As a concrete example, VC3 [24] recently showed that,
by relying on the new Intel SGX infrastructure [17] to pro-
tect local mapper and reducer processing, one can adapt
the popular Hadoop framework [1] and achieve strong in-
tegrity and conﬁdentiality for large MapReduce tasks with
a small performance overhead. All data is systematically
AES-GCM-encrypted, except when processed within hard-
ware-protected, remotely-attested enclaves that include just
the code for mapping and reducing data, whereas the rest of
the Hadoop distributed infrastructure need not be trusted.
They report an average 4% performance overhead for typical
MapReduce jobs. Trusting Intel’s CPUs may be adequate
for many commercial applications, and yields a much smaller
TCB than the whole cloud infrastructure. Similar practical
solutions may rely instead, for instance, on a hypervisor and
simple virtual machines dedicated to the MapReduce job.

Even if we assume perfect encryption for all data and per-
fect isolation for all local processing, mappers and reducers
still need to access shared resources (the memory, the data
store, the network) thereby opening as many side channels.
Their access patterns to memory, storage, and network—
such as for instance the data volume of map and reduce
jobs—are visible to the cloud provider and, to a lesser ex-
tents, to its other tenants. Revealing this information may
be justiﬁed by the practical performance one gains in re-
turn. However, there are circumstances where observing the
encrypted traﬃc of MapReduce jobs on a sensitive dataset
reveals much more information than may be expected.

A ﬁrst, important insight is that observing access to in-
termediate data structures is more informative than just ob-
serving inputs and outputs. In the case of MapReduce, for
instance, observing and correlating intermediate key-value
pairs exchanged between every mapper and every reducer
for a series of typical jobs, each using a diﬀerent ﬁeld of the
input records as key for mapping—say the age, the place of
birth, and the place of work—enables us to label each input
record with precise values for all these ﬁelds. What we learn

1570from such a ‘job composition’ attack is much more detailed
than what we would learn even by accessing the job results
in plaintext. This may come as a surprise for data owners,
who reason about which MapReduce job to authorize, and
which results to declassify, but usually not about leakage in
their distributed execution.

To support our claim, we demonstrate information leaked
from two sample datasets (900MB and 24GB, respectively)
that include personal and geographical attributes. We as-
sume an honest-but-curious adversary that observes the vol-
ume of encrypted communications between long-term stor-
age, mappers, and reducers. (Other lower-level side chan-
nels may be available, such as precise timings, page faults,
cache misses, etc, but they seem harder to exploit in gen-
eral, and may not reveal much more information on the in-
put datasets.) Our attacks suggest that, even with the use
of encryption and secure hardware, stronger methods are
required to avoid leakage through traﬃc analysis.

To remedy this problem, we propose a new deﬁnition of
data privacy for MapReduce—essentially, that observable
I/O should look independent of the input dataset—and we
describe two practical solutions that meet this deﬁnition,
and thus prevent our attacks.

As a trivial solution, we may pad all accesses and com-
munications to their potential maximal lengths. Similarly,
we may apply generic oblivious RAM techniques [11] and
oblivious sorting on top of a MapReduce implementation.
However, such solutions would incur a polylogarithmic over-
head, and they would preclude any speedups enabled by the
parallel nature of MapReduce. We further discuss related
baseline solutions in §9.

Intuitively, many existing mechanisms already in place in
MapReduce frameworks to achieve good performance should
also help us for privacy. Mappers and reducers often use
large I/O buﬀers, making it harder to track individual records
within large, encrypted batches of data. Similarly, for jobs
with adequate load balancing, one would expect mappers
and reducers to exchange roughly the same amount of data
with one another, thereby limiting that side channel. Our
solutions take advantage of these mechanisms to provide
strong, provable security, while respecting the simple, uni-
form, parallel data ﬂow in MapReduce jobs.

In summary, we contribute:
1. an empirical analysis of ‘MapReduce leakage’ on two
sample datasets—a sample of the 1990 US Census and
a log of taxi rides in New York—showing that one
can reliably infer (or even extract) precise information
about these datasets by simply observing the volume of
encrypted communications between the mappers and
reducers that perform a few typical jobs on their data;
2. a model of information leakage in MapReduce jobs,
against an adversary that observes all encrypted inter-
mediate traﬃc between Map and Reduce nodes;

3. two practical solutions that provably limit leakage to
essentially the total volume of their I/O, relying on
shuﬄing, sampling, and a carefully-chosen amount of
padding, with diﬀerent performance trade-oﬀs.

4. an implementation of these solutions, embedded as
auxiliary MapReduce jobs for Hadoop and VC3, and
their performance evaluation on our sample datasets.
Our results suggest that information security can be
achieved for typical MapReduce jobs with a reason-
able overhead (7% on average for our most secure so-

lution, depending on the distribution of intermediate
key-value pairs) and in some cases can outperform the
baseline solution due to internal grouping of key-value
pairs.

Although we focus on MapReduce, and sometimes on the
details of the VC3 secure implementation of Hadoop, our
results may be of interest for a larger class of data-intensive
applications, such as SQL and semi-structured databases.

2. PRELIMINARIES
Notations. We use A “ xa1, a2, . . .y to denote a list with
records a1, a2 etc, and A}B to denote list concatenation.

If record ai

is a key-value pair then ai.key and ai.val
denote the key and value of the pair, respectively. We
let A~k denote the pairs of A with the same key, k, i.e.,
A~k “ xai | ai.key “ ky. For a set K of keys, we write
to
kPK
A~k de-
››
denote concatenation of lists indexed by k, e.g.,
kPK
notes the list of records of A ordered and grouped by k P K.
We also sometimes split A into M batches Am such that
mPr1,MsAm. Mpaiq applies an operation M on ai, while
A “
MpAq applies the operation element-wise on A.

››

››

Cryptographic Primitives. Our solutions rely on seman-
tically secure encryption, pseudo-random permutations, and
pseudo-random functions. We use the usual cryptographic
notions of negligibility and indistinguishability. As we treat
cryptographic schemes as abstract building blocks, we avoid
committing to either an asymptotic or a concrete security
model. Similarly we keep cryptographic keys and their sizes
implicit.

Semantically secure encryption [12] guarantees that every
encryption of the same message is very likely to map to a
diﬀerent ciphertext. That is, given two ciphertexts the ad-
versary cannot distinguish whether they correspond to two
encryptions of the same message or encryptions of two dif-
ferent messages. We use rps to denote a semantically secure
encryption of a plaintext p. We sometimes overload this no-
tation, using rDs to denote an encrypted dataset D, where
each record may be encrypted separately.

The second primitive, a pseudo-random permutation π [14],
is an eﬃciently computable keyed permutation function. Its
security property is expressed as indistinguishability from a
truly random permutation. That is, if an adversary observes
an output of π and a truly random permutation, he is not
able to distinguish the two. We use πpiq to denote the lo-
cation of the ith record according to π and, again overload
notations, use πpDq to denote a dataset that contains the
records of D permuted according to π.

The third primitive, a pseudo-random function f [14], is a
keyed cryptographic primitive that is indistinguishable from
a truly random function with the same domain and range.

Secure Regions/Hardware. Our solutions rely on the abil-
ity to protect local processing inside secure regions. Secure
regions are trusted containers of code and data that are iso-
lated from other code in a system. Secure regions may be
implemented as trusted physical machines, trusted virtual
machines, or other forms of trusted execution environments
such as SGX enclaves [17]. While our solutions apply inde-
pendently of the speciﬁc implementation of secure regions,
we outline an implementation based on SGX processors and

1571used in our experiments. In this case, secure regions are im-
plemented as ranges of virtual memory addresses that are
protected by secure SGX processors using three mechanisms.
First, processors control memory accesses to the secure re-
gions. Code inside the region may be invoked only through a
call-gate mechanism that transfers control to an entry point
inside the region. Code inside the region has full access to
the data inside the region, but external read, write, and
execute accesses to the memory region are blocked by the
processor, even if they originate from code running at a high
level of privilege. Thus, the software TCB in our solutions
is just the code inside secure regions and, in particular, does
not include the operating system or the hypervisor.

Second, processors encrypt and protect the integrity of
cache lines when they are evicted to system memory (RAM).
This guarantees that the data in the regions is never in the
clear outside the physical processor package. This removes
a broad class of hardware attacks, such as cold boot attacks,
and limits our hardware TCB to only the processor.

Finally, processors support remote attestation. When a
region is created, the processor computes a cryptographic
digest of the region and signs it with a secret key available
only to the processor. This allows an external entity to verify
that data originated from a speciﬁc secure region. We use
this mechanism to establish secure channels between regions
and remote systems.

3. MAPREDUCE

MapReduce. Let D be a dataset that contains n records of
equal size. Let pM, Rq be a pair of map and reduce functions
deﬁning a MapReduce job on D. Let X be a list of interme-
diate key-value pairs and O be the output of the MapReduce
job executed on D, as explained below. Lower-case subscript
for each of the datasets denotes a record of the dataset in
the corresponding position (e.g., di is the ith record of D).
››
‚ M takes a record di as input and outputs a list of key-
value pairs Xi. Let X collect the key-value pairs pro-
iP1,...,|D|Mpdiq.
duced by M on every record of D: X “
‚ R takes as input records X~k with key k, and outputs
a list of values. Hence, the output of MapReduce is
O “

RpX~kq where K is the set of keys in X.

››

kPK

In cloud deployments, a user uploads D to a cloud data-
center and later requests that the cloud provider executes
jobs on D by sending pairs pM, Rq. The MapReduce frame-
work (e.g., Hadoop) is responsible for invoking M on every
record of D in parallel and obtain X as a result; grouping
key-value pairs in X by keys; and calling a reduce function
R on each resulting group.

MapReduce on Secure Hardware. We now describe adap-
tations to the MapReduce framework when executed on se-
cure hardware. The high-level idea of such systems (e.g., VC3)
is to store the data, intermediate key-value pairs, and output
in encrypted form, and run the map and reduce functions
within secure regions (see e.g., §2).
The system is set up with the following changes. The user
uploads an encrypted dataset rDs to the cloud. Whenever
she needs to request a run of a MapReduce job, she uses a
secure channel with the secure regions to provide binaries
for functions M and R, numbers M and R, and keys for pro-
tecting the data as well as for evaluating a pseudo-random

Figure 1: Example of MapReduce on secure hard-
ware (see §3) with three Mappers and three Reducers.
Dashed lines indicate intermediate traﬃc observable
by an adversary (see §4).

function f : K Ñ r1, Rs. The key for accessing the dataset is
reused across computations but crucially every MapReduce
job uses a fresh pseudo-random function.

The MapReduce framework then executes the job by in-
voking M Mappers and R Reducers. At a high level, map-
pers and reducers execute M and R within a secure region,
respectively. We describe their functionality in more detail.
The MapReduce framework splits the records of D into M
batches; we write Dm for the mth batch. Each batch is
processed as follows: given the mth encrypted batch rDms,
a mapper decrypts it and executes M on each of its records.
For every key-value pair xj with key xj.key “ k produced
by M, Mapper outputs a tuple pr,rxjsq, where r “ fpkq is
the index of a reducer. Every mapper evaluates the same
pseudo-random function and thus assigns a record with the
same key to the same value of r. The input of the rth
Reducer is pr,rX~rsq for X~r “
kPf´1prqX~k. The index r
hides the exact xj.key and the ordering of the xj, but still
allows the MapReduce framework to group tuples by keys.
The rth Reducer is responsible for decrypting key-value
pairs it receives, grouping them according to keys, executing
R on each group, and outputting the encrypted results rOrs.
Figure 1 shows an example of the system with mappers
processing batches of three records and three reducers, where
two keys happened to be mapped to the last reducer.

››

The batch size an adversary can observe varies from sys-
tem to system and may be deliberately chosen by its design-
ers or may depend on the contingent nature of its perfor-
mance characteristics. For instance, when running Hadoop
unmodiﬁed in a secure region, an adversary can delay in-
puts to mappers to observe batches of a single record. On
the other hand VC3 enforces a pre-conﬁgured batch size.

4. TRAFFIC ANALYSIS
In the previous section, we described MapReduce on secure
hardware using the storage and compute infrastructure of
an untrusted cloud provider. By encrypting data stored or
exchanged between mappers and reducers, and by executing
mappers and reducers within secure regions, it makes an

1572important step towards protecting the privacy of outsourced
data and MapReduce computation.

However, there are many aspects of this system that are
still observable in the environment where MapReduce is run
and, as we show, lead to information leakage.
4.1 What’s the adversary?

We ﬁrst consider a wide range of attacks, and then narrow
it down to the main channels we consider in the rest of the
paper. (These channels suﬃce for our attacks, and they are
simple enough to yield analytical results for our solutions.)
Basically, our adversary observes runs of MapReduce jobs:

At the system level, he may record the exchange of en-
crypted data, either between every node in the system (net-
work traﬃc analysis) or between every node and storage
(storage traﬃc analysis). The volume of data exchanged
may be measured in bytes, pages, packets, or records. Some
batching and padding may blur this side channel.

In our examples, we suppose that the number of records
can be accurately observed (or deduced) from network traf-
ﬁc. Within one job, as further explained in §4.2, the granu-
larity of each observation may range from individual records
to large batches. Conversely, we do not consider cache, tim-
ing and other low-level side channels against local runs of
the map and reduce functions; devising countermeasures for
them is an independent problem.

At the application level, the adversary may have back-
ground knowledge about the job, its input, and its output.
Information about the job itself may be readily available, or
may be inferred from the shape of the traﬃc data. (In VC3,
for instance, the code for M and R is encrypted; still, the
adversary may use, e.g., their data-exchange proﬁle, binary
code size, and runtimes to guess the job being executed.)

Statistical information about the input and output data
may also be common knowledge, e.g., the adversary may
know the distribution of marital status.

In our attacks, unless explicitly mentioned, we assume
that the adversary knows the job and some statistics about
the data, but not the content of its input and output—except
for their total sizes. (Such aggregate information is in gen-
eral hard to hide.)

In our security deﬁnition, we model arbitrary background
knowledge by letting the adversary choose two input datasets
for which the background knowledge is the same while data
contents may diﬀer; the adversary is then challenged to infer
which dataset was computed on. This more demanding def-
inition lets us capture that the adversary cannot learn any-
thing besides his background knowledge by observing traﬃc.

Our adversary may also actively interfere with job runs:

At the system level, an active adversary may control re-
sources and scheduling, e.g., feeding a Mapper one record at
a time. However, as discussed in §2, we assume he cannot
directly alter encrypted traﬃc or break into secure regions.

At the application level, he may partly choose his own
input, or even his own jobs, to mount adaptive attacks. Our
security deﬁnition reﬂects such capabilities by letting the
adversary adaptively choose the jobs as it is trying to use
traﬃc analysis to learn information about the dataset.

Our adversary may observe a sequence of jobs, on the
same datasets, or related datasets. MapReduce implemen-
tations re-use inputs for multiple jobs, inasmuch as they

co-locate the input batches and the mappers on the same
nodes. They also avoid costly re-encryption of data between
jobs. As a qualitative example, assume the adversary ob-
serves runs of a job before and after adding a single, target
record in the dataset. If the job splits on an attribute with
few keys and a known distribution, then the attribute for
the new record can be precisely inferred.

Sample Attacks. Our examples primarily target the passive
adversary that observes traﬃc from mappers to reducers at
the system level, as described in detail in §4.2. Then, in §4.3,
we show that combining these observations with background
knowledge available at the application level leads to privacy
leaks. Considering a stronger active adversary would only
facilitate our attacks and increase their precision.

In practice, the more selective the jobs are, the more pre-
cise information we can extract (assuming we know, or we
can guess, what the job is). In the following, we mostly focus
on observing jobs that split all input records depending on
some of their attributes. More generally, MapReduce jobs
may ﬁrst ﬁlter out parts of the input records before split-
ting. Our attacks would similarly apply to those jobs (in
particular to multiple jobs with the same ﬁlter), except that
we would learn information speciﬁc to the smaller, ﬁltered
collection of inputs, rather than the whole dataset.

Example: Attacking VC3. In VC3, an adversary may gain
full control of the Hadoop scheduler, enabling it, for ex-
ample, to send the same input batch to multiple mappers,
or to send a single input batch to a mapper; it may even
cause mappers to fail or restart, enabling it to improve
the accuracy of his network traﬃc measurements. On the
other hand, the input batches (and their sizes) are GCM-
protected, so the adversary cannot change them. VC3 re-
ducers also incorporate integrity checks against any input-
batch replication: a reducer that receives intermediate key-
value pairs from diﬀerent mappers processing the same input
batch will detect the attack and safely stop processing data.
Hadoop tries to keep nodes stateless, hence they rarely de-
lay sending data between batches. In VC3, mapper-reducer
communications rely on stateful secure channels for the whole
job; however, the adversary may send input batches one at
a time, and measure how many bytes are communicated as
a result of their processing.

Overall, we can thus conservatively model this adversary
as ‘passive’, but able to collect precise network traﬃc at the
granularity of individual input batches.

Example: Attacking Unmodiﬁed Hadoop. An adversary
against unmodiﬁed Hadoop (running in secure regions, and
encrypting all data, but without the VC3 countermeasures)
may have ﬁner control of its scheduling, for example by de-
laying packets in data streams, or exploiting failure-recovery
to observe multiple runs of the same jobs with a diﬀerent
assignment of inputs to mappers, thereby collecting traﬃc
information at a ﬁner granularity, possibly for each record.

4.2 Observing Intermediate Trafﬁc

We model the data collected by an adversary observing
MapReduce traﬃc, then we explain how he can use it to
trace information from the output of a MapReduce job back
to individual input records and how, as he observes runs of

1573multiple jobs on the same input records, he can correlate
this information between jobs.

Data dependent intermediate trafﬁc. We model observa-
tions of intermediate traﬃc using a matrix A with dimen-
sions MˆR where M is the number of mappers and R is the
number of reducers. Arm, rs is the number of intermediate
key-value pairs sent from mapper m to reducer r. Since an
adversary observes input and output of every mapper and
reducer, he can easily construct this matrix.

Before analyzing how he can use A to learn more about
the input dataset, we give an intuition with an aggregate
MapReduce job in Figure 1. The matrix A for this job is
shown below, with aggregate volumes of data sent by each
mapper (right) and received by each reducer (bottom).

1
2
3

3
0
1
2
3

3
3
3

2
0
1
1
2

m/r 1
3
1
0
4

Every Mapper reads three encrypted
records, extracts a zip code and
each Reducer counts the number of
records per zip code. In the matrix,
each entry Arm, rs
indicates how
many intermediate key-value pairs
produced by the mth mapper have
zip code that was returned by the rth
reducer. In particular, the adversary sees that the ﬁrst three
records have the same zip code (43301) and the last three
records do not have this zip code. Given background knowl-
edge of the distribution of zip codes, the adversary can thus,
in this case, label each column of A with a zip code. Abusing
our notation, we refer to the cell Ar1, 1s as Ar1, ‘433011s.

The example illustrates that matrix A lets the adversary
correlate input and output of a MapReduce job as long as
(1) records read by a mapper can be correlated with in-
termediate key-value pairs in A, and (2) there is variation
between values in each row and column. The ﬁrst condition
depends on how mappers read and write their input and
output, while the second condition depends on the data.

Network traﬃc from two or more jobs can easily be com-
bined and lead to a ‘job composition’ attack. The adver-
sary observes a matrix A from each job and, as long as the
same input data is used, he can label each input batch with
the results of such inferences. For example, he can observe
jobs on zip code, gender and data of birth. Sweeney [25]
showed that combinations of such simple demographics of-
ten already identify people uniquely. In the rest of this sec-
tion we show how the adversary can still correlate mapper’s
inputs and outputs for less trivial input datasets.

Granularity: observing trafﬁc on input batches. In gen-
eral a mapper can process a sequence (or batch) of records
(to amortize the cost of encryption, for example).
If the
mapper reads a batch, there are several ways in which it
could control its I/O. For example,
it could sequentially
read a record and immediately return the corresponding key-
value pair; it could buﬀer key-value pairs for several records
and return all of them when the buﬀer is full (as in the
VC3 implementation); or start reading the next sequence of
records while still processing the ﬁrst sequence.

Diﬀerent I/O processing creates noise in the signal of the
adversary when he tries to correlate the input and the out-
put of a mapper. For example, this noise does not allow
the adversary to precisely determine which record resulted
in which key-value pair. However, the adversary can still
correlate a batch of input records with key-value pairs, i.e.,

by using a time window for when records are read and inter-
mediate key-value pairs are returned. Similar I/O buﬀering
can be done on the reducer side. However, due to the func-
tionality of the reducer, in some cases it has to read all its
input records before returning the output.

In our examples of information leakage, we assume that
the mapper would try to protect the correlation between
records it reads and intermediate key-value pairs it returns.
In particular, we assume that the mapper puts a threshold
on how many records it has to process in a batch before
returning the output. He further permutes the intermediate
key-value pairs to break I/O correlation. However, as we
illustrate below, this is only a partial remedy.

4.3 Exploiting Intermediate Trafﬁc

We give concrete evidence of information leakage, both
when records are processed one at a time and when they are
processed in batches. In the latter case, although it is more
diﬃcult to extract information about individual records, we
show that it remains possible when the input records are
somewhat sorted, and that MapReduce traﬃc still leaks in-
formation about many statistics in the input data.

Our goal is not to uncover new facts about these datasets,
readily available from their plaintext, but to show that, more
surprisingly, those facts are also available to an adversary
that merely observes encrypted traﬃc. Our experiments also
suggest that naive techniques based on padding inputs and
outputs would be of limited value for these datasets.

Our experiments are based on two datasets:
‚ U.S. 1990 Census Sample [16] (900 MB). The
dataset contains 2.5 million personal records. Every
record has 120 attributes, including the Age, Gender,
POW (place of work), POB (place of birth), MS (marital
status), etc. Some attributes have been discretized: for
instance, Age ranges over 8 age groups, such as 20–29.
‚ New York 2013 Taxi Rides [26] (24 GB). This
dataset contains records for all the taxi rides (yellow
cabs) in New York city in 2013. It is split in 12 monthly
segments, and each segment contains approximately
14 million records. The records have 14 attributes and
describe trip details including the hashed license num-
ber, pickup date and time, drop oﬀ date and time, and
number of passengers.

The ﬁrst dataset is representative of personal data com-
monly stored in the databases of medical institutions, in-
surance companies, and banks. The second dataset contains
sensitive information and, despite some basic anonymiza-
tion, is susceptible to inference attacks [21, 28]. Some of
these attacks use MapReduce [21] to extract correlation be-
tween the rides (in plaintext). We show that the same kind
of information can also be extracted by traﬃc analysis.
In this section, the adversary is assumed to have the fol-
lowing subset of the capabilities described in §4.1. He ob-
serves only basic aggregate jobs, which all go as follows: M
splits the records, with the attribute used for aggregation
(e.g., the Age) as key; hence R receives all records with the
same attribute value; it may return their count, or any other
function of their contents. He is also assumed to have sta-
tistical information on the attribute values used for splitting
(e.g., distribution of age and marital status in the U.S. and
popular destinations in New York). This allows him to label
columns of A with the corresponding attribute values.

1574Figure 2: Distribution of Census records across age
groups (left), place of birth (center) and marital sta-
tus (right), where U.S. count is trimmed.

4.3.1 Individual records
Our ﬁrst attacks are based on observing mappers, as they
consume one record at a time and immediately produce in-
termediate data. Hence, the intermediate-traﬃc matrixes
have one row for each individual record and, at least for
basic aggregate jobs, each row has exactly one non-zero en-
try. To illustrate the correlation of observations across jobs,
we show that, after observing aggregate jobs on distinct at-
tributes, the adversary is able to answer speciﬁc queries on
any combination of these attributes, such as

1. Given the index of a record in a dataset, return the

values of these attributes;

2. Test if the dataset contains a record that matches par-

ticular values for some of these attributes; and

3. Given the values of some of these attributes, infer the

possible values of the others in the dataset.

Census Data. For these attacks, we observe three aggre-
gate jobs, one for the age group, one for the place-of-birth,
and one for marital status. This yields three intermediate-
traﬃc matrixes: AAge, APOB and AMS with 2.5M rows each.

Figure 2 displays aggregate counts for the three jobs, i.e.,
the number of key-value pairs assigned to each attribute
value. Up to a permutation of the columns, this is the same
information as the sums of the columns in AAge, APOB and AMS.
The adversary can determine the key processed by every re-
ducer in these matrices (i.e., label the columns of A) using
auxiliary public information on the distribution of, for ex-
ample, the age of the U.S. population.
Let us analyze the information in each matrix individually.
AAge gives a precise age group for every record, APOB gives a
geographical region for a place of birth for every record,
and AMS gives the marital status for every record. As long
as all jobs processed the same dataset, the adversary can
combine the information he learns across all jobs. That is
if, AAgeri, ‘1–12’s “ 1 (overloading the reducer key with the
label it processed) and APOBri, ‘Africa’s “ 1, then the ith
record is in the age group “1–12” and was born in Africa.
Thus, the adversary can directly answer queries such as
1. What is the marital status of person #1,326,457 in the

census? Never married, since
AMSr1326457, ‘Never Married’s “ 1.
2. Is there a person with {Age: 13-19, POB: Oceania, Mar-
ital Status: Divorced} in the dataset? Yes, since
there is (exactly, in this case) one index i “ 1, 005, 243
such that AAgeri, ‘13-19’s, APOBri, ‘Oceania’s and AMSri,
‘Divorced’s are all equal to 1.

Taxi Data. Our sample attack is based on observations of
aggregate job on the pickup day, pickup location, and drop-
oﬀ location. Suppose an adversary saw a person getting
in a taxi on a corner of Linden Blvd and 221st Street in
Queens on January 13, 2013. The adversary then looks at
row indices in APickupD and APickupL that have non-zero entries
for ‘Linden Blvd and 221st Street, Queens’ and ‘January 13’,
There is exactly one such index in our dataset, 13, 484, 400.
The drop-oﬀ location is the non-zero entry in ADropoffLris
row, that is, ‘1053 Atlantic Ave, Brooklyn’.
4.3.2 Batch records
Our second series of attacks apply against mappers that
securely process large batches of records at a time. We as-
sume that each mapper reads all records in a batch assigned
to him, applies M on each record, and returns permuted
key-value pairs. Hence, observing an aggregate job yields
an intermediate-traﬃc matrix A with fewer rows (only one
for each batch). Since each job we consider has only a few
keys (at most 50) we still assume that there is a reducer
for every key. Hence, the adversary knows precisely which
intermediate keys in the columns of matrix A produced a
given output value. Thus, each row provides a histogram of
the values of the attribute for all the records in the batch.

Though intuitively, batching makes it harder to extract
precise information from the dataset, we show that it does
not always suﬃce. In particular, if the data is ordered by
some attribute, the information about a batch will provide
information conditional on values of that attribute.

Here, we consider only the New York taxi dataset—see
the full version of the paper for additional examples on
batched Census data. The taxi dataset is split into batches,
as follows: each batch contains „147K records of taxi rides
(24Kb), with 100 batches per monthly segment.

An adversary that observes an aggregate job on some ar-
bitrary attribute (attr) recorded in the dataset can reliably
perform the following tasks:

1. Given an aggregate count of the values of attr over
the whole dataset, he can infer precise counts of attr
values over smaller batches of data (for each day of the
year for the taxi rides).

2. Given prior information about a speciﬁc record, such
as its location in a dataset or the value of its attribute,
he can infer other attributes for that record (i.e., the
pick up day for a taxi ride.)

Consider a curious adversary who observes intermediate
traﬃc for two aggregate jobs, on PickUpD (pick-up day of a
taxi ride) and PassenN (number of passengers in a taxi ride).
Let us ﬁrst describe the information available in APickUpD
and APassenN. Since it is known that the data in every 100
batches corresponds to a month of the year, we look at the
rows that correspond to January. In particular, we consider
the same 31 rows of APickUpD and APassenN (since there are 31
days in January). For each row, we consider its distribu-
tion across attribute values (i.e., the days of the year). As
expected, there are only 31 non-zero counts in each row.
We plot the number of key-value pairs assigned from each
batch to 31 reducers in APickUpD in Figure 3 (top). For exam-
ple, 31% of the records for day ‘Jan 1, 2013’ came from the
dark-blue batch. It is evident that the data is not uniformly
distributed. For comparison, the bottom half of Figure 3
plots the attacker’s guess on batches distribution if he knew
only the aggregate counts for day, i.e., the column sums

1575This information is available in Figure 4 (or the full
version for all days). The adversary simply looks up
the bars that corresponds to Fridays. Note that this in-
formation is more precise than what the adversary can
learn from aggregate information for passenger num-
ber (i.e., the last bars of Figure 4). If the adversary
is a competitive taxi company these counts could be
used for better scheduling of own cars across the week.
‚ Did someone take a taxi ride from Canarsie Park in

Brooklyn in January? When exactly?
The adversary uses combined traﬃc from the two ag-
gregate jobs. Luckily, there is only one record assigned
to pick up location in Canarsie Park in the 100 batches
of January. Moreover, the batch that includes the
record has most of its taxi rides from January 19 (79K),
January 18 (39K), January 20 (24K), and January 6
(6K), with only 11 rides left over 6 other days. Hence,
it is most likely that this ride happened on January 19
(and in fact, it did).

Finally, we point out that the diﬀerence in the distribution
of keys in batches is high (e.g., a batch for January 1 vs. a
batch for January 30 diﬀer by 20K rides for 5 and 6 pas-
sengers). Similarly, the number of key-value pairs processed
by reducers is also diﬀerent (e.g., reducers that count rides
with one passenger vs. ﬁve passengers). Hence, padding the
inputs to some number of keys to protect the identity of
each reducer may become very expensive for long tail distri-
butions. Even for the taxi dataset it would require mappers
to pad their traﬃc to each reducer to 125K key-value pairs.
Moreover, it is not clear how mappers can ﬁx this level of
padding without communicating with each other.

5. SECURITY DEFINITIONS

How can we design systems that are resilient to such traﬃc
analysis attacks? To be able to evaluate protection mecha-
nisms, this section brings forward diﬀerent security notions
for MapReduce on secure hardware. Our deﬁnitions are de-
ﬁned as games between a challenger and an adversary: the
adversary chooses two datasets that share some arbitrary
background information available to the adversary, the chal-
lenger chooses one of them and lets the adversary adaptively
request jobs on it. (For instance, arbitrary background in-
formation may include “knowledge” of the output of a job
computation; this is captured by the adversary choosing two
inputs that yield this output.)

Definition 1

(MapReduce Game). At the start of the
game, the adversary picks two datasets D0 and D1 and ﬁxes
the number M and R of mappers and reducers. He is then
given access to rDs, where D “ D0 or D “ D1, and can
observe the run of multiple MapReduce jobs pM, Rq of his
choice. He observes their full network traﬃc to guess whether
D “ D0 or D “ D1. The adversary’s advantage of winning
the game is his probability of guessing correctly minus 1
2 .
The MapReduce game models that the traﬃc observed for
any two datasets D0 and D1 should appear to be the same
and not reveal anything about the input data. As argued
in §4.1, the integrity checks of secure MapReduce systems
such as VC3 prevent active interference of the adversary on
the network; this enables us to model their adversaries as
passive in that sense.
As stated, the MapReduce game is trivial to win, e.g., if
|rD0s| ‰ |rD1s|, and, as discussed in §4.1, it is reasonable

Figure 3: Distribution of taxi records from 100
batches across reducers for “Aggregate by pickup
day” job (top) vs. uniform,
i.e., attacker’s guess
without observing network traﬃc (bottom).

Figure 4: Passenger counts across six days and the
expected count if the attacker observed only aggre-
gates (bars for single passenger rides are trimmed).

of APickUpD. It appears that the data is sorted according to
individual days and the 100 batch split divides some of the
days in several batches. Furthermore, we note that the orig-
inal dataset was probably slightly shuﬄed, however, the day
order is still preserved.
In particular we see that 98% of
Batch 7 (purple batch) was assigned to Jan 3.
In Figure 4 we capture the distribution of some of the rows
across all values in APassenN. Since we know that each batch
contains rides of only a few days, we label the batches with
the day that is represented the most in that batch. There are
6 passenger counts (there are 0 count that we ignore since
it is also under represented and suggests an error in the
dataset). If the adversary sees the output of the aggregate
job for PassenN, he knows precisely the passenger count. If
not, some of the labels are easily inferable (e.g., 4 passenger
rides are the most rare while 1 person ride is most popular).
Given APickupD and APassenN, the adversary can thus answer
‚ What is the number of passengers on Friday nights?

the following queries:

1576to assume that the adversary may learn the size of D and
a few other aggregate values. We give two variants of our
deﬁnition, of increasing strength, that specify what each job
is allowed to leak about the data as requirements on D0 and
D1, expressed on their matrices A0 and A1. Recall that we
associate the matrix A to a dataset D and a MapReduce job
pM, Rq such that each cell represents the number of interme-
diate key-value pairs from the mth batch to key k, that is,
Arm, ks “ |MpDmq~k| (using notations from §3).

Definition 2. Correlation hiding requires that no ef-
ﬁcient adversary has more than a negligible advantage in
winning the MapReduce game as long as |D0| “ |D1| and,
for every MapReduce job pM, Rq the adversary picked during
the game, the following holds:
for all m P r1, Ms, we have
ř

A1rm, ks.
2. Reduce functions take the same amount of input, i.e.,
there exists a permutation σ on the keys such that, for
all keys k P K, we have
A1rm, σpkqs.

1. Mappers produce the same amount of output, i.e.,

A0rm, ks “
ř

A0rm, ks “

ř

ř

k

k

m

m

3. The output size of the reduce function R is constant.

The permutation in Requirement 2 accounts for the fact
that the adversary only observes encrypted keys. The def-
inition does not leak the details of Arm, ks, hence it hides
which records have common keys, preventing the composi-
tion attack in §4.3. It is applicable to map functions M that
project a key and a value from a record; typically to allow a
reducer function R to then compute some aggregate statistic
about the values grouped by the key: Requirement 1 and 3
are clearly met by such functions, and Requirement 2 is a
statistic, such as those in Figure 2, about the distribution of
keys in D that may often already be publicly known.
MapReduce jobs for which this deﬁnition does not do well
are functions pM, Rq that perform ﬁltering. A map function
M that discards all people living in Oceania from processing
leaks this attribute value trivially if no record was returned.
Our second deﬁnition protects even against such attacks.

Definition 3. Strong hiding requires that no eﬃcient
adversary has more than a negligible advantage in winning
the MapReduce game as long as |D0| “ |D1| and, for every
MapReduce job pM, Rq the adversary picked during the game,
the following holds:
1. The volume of intermediate data is the same: |X 0| “
|X 1|; and the number of keys is the same: |K 0| “ |K 1|;
2. The number of records for the most popular key is the
same: maxkp

ř
A0rm, ksq “ maxkp

A1rm, ksq.

ř

m

m

3. The output size of the reduce function R is constant.

The two security deﬁnitions diﬀer in their requirements on
D0 and D1, which restricts the type of data and sequence of
pM, Rq computations the deﬁnition can protect. The latter
deﬁnition is strictly stronger, since agreement on the cardi-
nality of keys implies agreement for the most popular key,
as well as on the sizes of |O| and |X|. Requirement 2 allows
us to leak the number of records for the most popular key,
which is a lower bound on the traﬃc received by the reducer
that must process all these records.

6. SHUFFLE-IN-THE-MIDDLE SOLUTION
Our ﬁrst solution prevents intermediate traﬃc analysis on
a job by securely shuﬄing all the key-value pairs produced
by the Mappers and consumed by the Reducers. Hence, the
adversary may still observe volume of intermediate traﬃc

for each mapper and for each reducer, but it cannot trace
traﬃc from reducers back to individual mappers.
6.1 Method
We present our solution using a data shuﬄe algorithm
as a black box that, given rXs and a pseudo-random per-
mutation π on 1 . . .|X|, returns rπpXqs. We then describe
our implementation of the Melbourne Shuﬄe algorithm [20]
using MapReduce jobs (§6.2).

Let XM be the output of the mappers, and XR the output
of the shuﬄe passed to the reducers. XM and π are given
as input to a data shuﬄe job to permute the records. The
output XR of the shuﬄe is then grouped and sent to the
Reducers by the MapReduce framework, as before.
In more details, each Mapper proceeds similarly to §3,
except for the content of its output. Recall that a mapper
in §3 returned a tuple pr,rxjsq where r is the index of the
Reducer that processes records with keys k such that fpkq “
r. Instead, we modify Mapper to return prrs,rxjsq, so that
rXMs now consists of pairs of ciphertexts.
Then, the data shuﬄe is invoked on rXMs with a small
adaptation: instead of simply outputting rπpXMqs, the last
step is modiﬁed to return the decrypted value of r, while
re-encrypting rxjs. Hence, the output of the data shuﬄe is
a list of tuples pr,rxjsq that is a random permutation of the
intermediate key-value pairs of the original MapReduce job.
The rest of the protocol is the same as the one for MapRe-
duce on Secure Hardware in §3. The MapReduce framework
(e.g., Hadoop) groups key-value pairs according to their re-
ducer index r and invokes Reducer on each group.
Theorem 1. If r¨s is a semantically secure encryption
scheme, f is a pseudo-random function and π is a pseudo-
random permutation, then the Shuﬄe-in-the-Middle solution
is correlation hiding (Deﬁnition 2).

The proof of the theorem is in the full version of the paper.
6.2 Data Shufﬂe
Given an encrypted dataset rDs as input, the shuﬄe yields
some permutation rπpDqs as output. Since D can be large,
we want an eﬃcient implementation of the shuﬄe within
a secure MapReduce framework. Moreover, we want to en-
sure that the observations about the network traﬃc that the
adversary can make (as described in §4.2) do not leak any
information about the data (except its size) and the shuf-
ﬂe permutation π. Hence, an adversary that observes a job
implementing either π0 and π1, should not be able to say
whether the output is an encryption of π0pDq or π1pDq.

Sorting networks [4] provide the security guarantees above:
their network traﬃc is independent of the data. However,
since these algorithms perform sorting, they incur a loga-
rithmic depth computational overhead (plus additional con-
stants).
Instead, for our solutions, we choose the parallel
version of the Melbourne Shuﬄe [20], which oﬀers the same
security guarantees, and we implement it as two successive
runs of the MapReduce job described below. We refer to
[20] for a detailed analysis of the algorithm.

The input D is viewed as a set of disjoint input batches
where records are split sequentially according to their index
in D. The output πpDq is also viewed as a set of disjoint
output batches but where records are split sequentially us-
ing their permutations tags. The task of the mappers is
to distribute records from input batches into correct output
batches such that an observer of traﬃc cannot tell which

1577output batch an input record was assigned to. To this end,
a mapper sends the same number of records to each output
batch (padding with dummy records if needed). A reducer
then reads a padded output batch, removes the dummies and
places records in their correct locations within the batch.
The number of real and dummy records assigned to each
output batch is the same across batches, hence, all reduc-
ers read and output the same number of records, thereby
revealing nothing about π.

In more details, each mapper takes as input a permuta-
tion π (e.g., it takes a key to a pseudo-random permutation)
and a batch of b records, and outputs a bin of max records
(for some ﬁxed number max ą b{R) for each reducer, that is,
R bins in total. The mapper assigns each record dj (where j
is the index of the record in D) in the batch to one of the R
bins according to its permutation tag: record dj goes to bin
r “ rπpjq{Rs. If a bin is assigned more than max records,
the algorithm aborts. Otherwise, the mapper pads each bin
to max records, by adding dummies with the same size as
genuine records, then it encrypts and outputs each bin as a
single intermediate value with key r.

Each reducer takes a list of bins (one from each mapper),
removes the dummies, sorts the remaining records by their
permutation tags, removes the tags, and outputs the result.
A single run of the MapReduce job above will fail on some
permutations, namely those where a mapper assigns more
than max records to the same reducer. For example, if π is
the identity function, the job will fail unless max ě b. To
remedy this while keeping max small, two successive MapRe-
duce jobs are invoked: the ﬁrst job is for a uniformly random
permutation ρ and the second one is for the target permuta-
tion π. Although these invocations may still fail, this hap-
pens rarely. Moreover, the analysis of [20] shows that success
and failure of the shuﬄes depends only on ρ, hence, it leaks
no information about the actual input and output of the
algorithm. Besides, max can be carefully chosen to control
the probability of failure: on average, each bin should get
b{R records, and balls-and-bins analysis tells us how much
to over-provision as we choose max.

7. SHUFFLE & BALANCE SOLUTION

The Shuﬄe-In-The-Middle described in §6 prevents the
adversary from observing the volume of data exchanged be-
tween individual Mappers and Reducers (the matrix A). How-
ever, the adversary still observes the number of records each
Mapper produces and the distribution of encrypted keys.

Our second solution meets our stronger Deﬁnition 3 by
evenly distributing the intermediate traﬃc sent from each
mapper to each reducer. It preserves the data parallelism of
MapReduce, and may even improve its performance by facil-
itating resource allocation and scheduling. But it requires a
more precise load-balancing than what is typically achieved
by MapReduce implementations.
7.1 Overview

We are seeking solutions that ﬁt into existing MapRe-
duce implementations, which precludes a complete redesign
of mappers with better balancing properties.
Instead, we
use preliminary MapReduce jobs to plan how to balance
(and pad) the intermediate key-value pairs for the ‘main’
job. We split this pre-processing into oﬄine and online jobs.
The oﬄine stage runs on the input data (once for all jobs)
and randomizes the ordering of the input records. This

erases any correlations between the ordering of inputs and
the values of their attributes (as those exploited in §4.3.2),
and ensures that all mappers produce the same distribu-
tion of key-value pairs. This stage may be implemented by
a shuﬄe (§6.2) or, pragmatically, as the user uploads her
input data to the cloud.

The online stage is job speciﬁc; it samples the input data
to collect statistics about the keys produced by mappers, in
order to balance them evenly between reducers and to esti-
mate (with high probability) an upper bound on the number
of key-value pairs sent by each mapper to each reducer. Let
M and R be the map and reduce functions of the job and R
its number of reducers.
1. We ﬁrst run M and R on a ﬁxed sample of the randomized
input. We collect statistics on its intermediate keys: a
list of pairs pk1, f1q,pk2, f2q, . . . ,pkl, flq where ki ranges
over the sampled keys and fi is the fraction of key-value
pairs in the sample with key ki. This list enables us
to estimate the distribution of keys for the whole input
dataset, notably its most popular key.
We also determine the total number of key-value pairs
returned for the sample size and the constant output
size (cid:96) of R.
This task is easily expressed as a small job whose traﬃc
pattern depends only on the size of the sample. The
statistics we collect are reminiscent of those maintained
in databases to optimize, for instance, joins on record
ﬁelds; they may similarly be cached and shared between
jobs that map data on the same attributes.

2. We generate a key assignment for the job: a function
from all (potential) intermediate keys to 1..R, intended
to balance Arm, rs by grouping keys so that every re-
ducer gets roughly the same number of records, as de-
tailed in §7.2.
We also estimate a safe upper bound on the fraction of
traﬃc sent from any mapper to reducer r and an up-
per bound on the number of diﬀerent keys assigned per
reducer. Our algorithms are detailed in §7.2.

The ‘main’ job then runs, essentially unchanged, except that
(1) every mapper uses the resulting assignment to map keys
to reducers, instead of the random, uniform intermediate
partition in the base solution; (2) every mapper ﬁnally sends
dummy key-value pairs to every reducer, up to its upper
bound estimate; and (3) every reducer silently discards in-
termediate dummies, and pads its output with dummy val-
ues up to its upper bound estimate.

ř

m

Deﬁnition 3 leaks the exact values of maxkp

As an optional, post-processing stage, we may use a shuﬄe
on the reducer outputs, to discard their dummy values and
erase any correlation between key frequencies and output
ordering, or pragmatically leave this simple task to the user.
Arm, ksq,
|X|, and the number of keys in the dataset, whereas our solu-
tion leaks these values as observed in a random sample. How
do our estimates relate to the exact values for the dataset?
An estimate, with certain conﬁdence, yields a range in which
the exact value lies. Since our sample is chosen at random,
the estimates depend on these three values and our target
conﬁdence level, but also on the actual records in the sample.
To meet the deﬁnition, we formally require that statistics to
be collected on the whole input dataset. We note, however,
that for large shuﬄed datasets, even relatively small samples
already provide excellent estimates.

15787.2 Mapping vs Bin-Packing

In this section, we explain how we use the statistics col-
lected in the online stage to estimate upper bounds on the
number of key-value pairs with the same key and the total
number of keys, and to produce a secure balanced assignment.
(Intuitively, our sample provides precise frequencies for the
most frequent keys, but may miss many infrequent keys.)

We ﬁrst suppose that the statistics give us exact informa-
tion on the key distribution, notably on the largest fraction
of key-value pairs with the same key (the maximal value of
the fractions fi above, written α in the following) and we
explain how to allocate suﬃcient bandwidth between map-
pers and reducers to ﬁt any distribution with the same α.
(Recall that Deﬁnition 3 enables us to leak only the maxi-
mal value of fi, not the detailed key distribution.) We will
then justify our use of estimates instead of exact values.

Our problem, at heart, is an instance of bin packing, so we
ﬁrst review bin packing basics before giving our algorithm.

Bin packing. The (oﬄine) bin packing instance is expressed
in terms of a ﬁxed bin capacity c and a list of items, each
with a weight at most c. (In our case, a key is viewed as
an item and its frequency as its weight.) The goal of bin
packing algorithms is to minimize the number of bins N
needed to allocate all items without an overﬂow. Since the
oﬄine bin-packing problem is NP-Complete, approximation
algorithms, such as First Fit Decreasing (FFD) [8], return
both a number of bins and a guarantee on how far it can be
from the optimum in the worst case. The FFD algorithm
places items in decreasing weight order, allocating new bins
on demand: it places the heaviest item in the ﬁrst bin, then
proceeds with the next item and tries to place it into one of
the open bins (i.e., the bins that already have items) with-
out exceeding its capacity. If there is no space left in any
open bin, it places the item in a new bin.

Bin Packing, Obliviously. Our problem is more general:
given only some maximal item weight α, we must ﬁnd a bin
capacity c and an upper bound on the number of bins N so
that FFD packing will succeed on any weighted list of items
with maximal weight α.

In the general case, we choose N “ 2

α ´ 1 and c “ α.
(The full paper justiﬁes these choices.) Since the weights of
all items sum up to 1, N c´ 1 is the overhead of the solution
in terms of dummy key-value pairs that have to be added
to ﬁll in the bins. Hence, the values of N and c above yield
an overhead of 1 ´ α. We can reduce the overhead above in
several special cases; for example:
R ` α,

R , then we may pick N “ R and c “ 1

which yields a low overhead of αR.

‚ If α ăă 1
‚ If α ě 1

2 , we have at least one very large reducer of
capacity α and everything else will ﬁt into a second,
smaller reducer of capacity 1 ´ α. In this special case,
there is no actual need for FFD.

The general and special cases of ﬁxing N and c ensure that,
from a traﬃc analysis viewpoint, the number and capacities
of bins (which, as described next, entirely determine the
matrix A for the job we protect) depend only on α.

Bin Packing Keys to Reducers. Once c is ﬁxed, we are
ready to bin-pack the distribution of keys we have sampled,
and to generate our (secret) assignment for the main job.

To this end, we add two dummy keys with weight 0, for
the smallest and largest keys (if not already present in the
sample). We partition the domain of all intermediate keys
into intervals, such that the total weight of all the keys in any
given interval is bounded by α. Hence, there is at least one
interval that contains one single key, with maximal weight α.
The inclusion of dummy keys ensures that assignment is a
total function, even on keys that we have not sampled. We
then sort these intervals by decreasing weight and run the
FFD algorithm on them (assured that at most N bins will
be used) to get a mapping between key intervals and bins.
We independently distribute N bins between our R reduc-
ers, such that each reducer gets at most rN{Rs bins. Hence,
some reducers may get less than rN{Rs bins, or no bins at all
if N ă R. Finally, we use this (public) mapping and the (se-
cret) FFD output to produce an assignment that maps each
key interval to the reducer of the bin it has been placed into.

7.3 Padding Trafﬁc to Fixed Sizes
Intermediate Key-Value Pairs. We select a level of padding
for the traﬃc sent from each mapper to each reducer based
on two considerations: we must hide the actual number of
key-value pairs received by each reducer—that is, the actual
usage of each bin we have allocated—by ﬁlling those bins
with dummies; and we must over-provision to accommodate
(with high probability) for any diﬀerence in key distribu-
tion between the sample and the dataset, and between the
dataset and the output of each mapper. To this end, the
assignment is supplemented with padding target, to be used
by every mapper to compute the apparent number of inter-
mediate key-value pairs it must send to every reducer (as a
function of the size of its input). To set those targets, we use
parameter estimation for α [18, Chapter 4.2]. We treat the
number of key-value pairs to be sent to the rth reducer as
a random variable and apply Chernoﬀ bounds [18, Chapter
5.4] to accommodate variations in mapper inputs.

Interestingly, the resulting matrix of observable interme-
diate traﬃc A for the main job is not necessarily uniform, as
mappers may process batches of diﬀerent sizes and reduc-
ers may process diﬀerent numbers of bins, but this matrix
depends only on α, R, and the sizes of the mapper inputs.

Reducer Output. Preventing traﬃc analysis on the job out-
put is simpler. We set rkey to bound the maximum number
of keys that may be allocated to a single reducer given R,
|K| and α. We count the maximum number of rare keys
that may be assigned to any single bin, assuming that large
keys are distributed elsewhere.
In particular, we set it to
rkey “ |K| ´ r1{αs + 1. Then, for every bin assigned to a
reducer, the reducer output is padded up to rkey ˆ (cid:96).

Theorem 2. If r¨s is a semantically secure encryption
scheme and the permutations π and π1 used in pre- and (op-
tional) post-processing are pseudo-random, then the Shuﬄe
& Balance solution is strongly hiding (Deﬁnition 3).

The proof of the theorem is in the full version of this paper.
Our solution hides any distribution of keys with maximum
frequency α, but it does reveal α. This is justiﬁed, because
at least one Reducer must process all the key-value pairs
for the most frequent key. However, this can be mitigated
(notably when α ăă 1
R ) by increasing α before computing c.

1579Table 1: Run times for Shuﬄe-in-the-Middle (S).

DataSet/Job

Base Run time (Shuﬄe)

Census/Age grouped

Taxi Jan/PassenN
Taxi Jan/PickupD

20
39
40

91 (25)
122 (38)
131 (43)

Table 2: Run times for Shuﬄe & Balance (S) where
PassenN-1 aggregates passenger counts without the
most popular key.

Attrib

PassenN
PickUpD
PassenN-1

α

Taxi Jan (2.5 GB)
|K| Run time
(ˆBase)
45 (1.01)
6
48 (1.12)
31
5
43 (1.09)

.71
.038
.47

Taxi Jan-Apr (10 GB)
|K| Run time
(ˆBase)
61 (0.93)
6
78 (1.21)
55 (1.06)

.71
.01
.45

120

α

5

8. EVALUATION
We have evaluated our framework using a local Hadoop clus-
ter of 8 workstations connected with a Netgear GS108 1Gbps
switch. Our nodes ran under Microsoft Windows Server
2012 R2 64-Bit on workstations with a 2.9 GHz Intel Core
i5-4570 (Haswell) processor, 8 GB of RAM, and a 250 GB
Samsung 840 Evo SSD. We implemented our solutions in
Java for experiments on plain Hadoop and in C++ for VC3
experiments, which use AES-GCM encryption implemented
with AES-NI instructions and a software emulator for SGX.
We perform experiments on the two datasets presented in
§4.3: a census data sample (900 MB) and the New York taxi
rides (2.5 GB per month). We perform two types of jobs:
aggregate and aggregate-ﬁlter, where the latter is an aggre-
gate over records ﬁltered by some parameter. The baseline
run times correspond to the initial job on Hadoop without
protection. The reported numbers are averaged over 5 runs.
The run times for the Shuﬄe-in-the-Middle solution in
Java are summarized in Table 1. This experiment involves
4 MapReduce jobs: for mapping, shuﬄing twice, and reduc-
ing. Hence, no batching and parallelization is enabled and
all jobs are treated sequentially. In contrast, in the base ex-
ecution of this job, grouping of intermediate pairs by keys
starts as soon as mappers produce some output. Starting
the shuﬄe as soon as the map job outputs its ﬁrst key-
value pairs may reduce the I/O overhead (similarly starting
sorting keys before the last shuﬄe ﬁnishes). Shuﬄing costs
highly depend on the size of the data; for passenger count a
value is simply a number vs. a date for pickup date job.

Next we measure the run times of our Java implementa-
tion for the Shuﬄe & Balance solution of the Taxi dataset of
size 2.5 GB (Jan) and 10 GB (Jan-Apr) with R “ 15. The
results for the online phase for a randomized taxi dataset are
presented in Table 2. For each job we show the frequency
of the most popular key and the number of keys. Shuﬄe
& Balance is more eﬃcient than our ﬁrst solution, assuming
one can run the jobs on shuﬄed data: performance overhead
increases on average by 7%. In one example, our solution
even outperforms the baseline. This is due to the smaller
number of key-value pairs returned to the system by the
mappers and, hence, lower overhead for the framework to
group them together. Recall that our solution pre-groups
values with the same key during bin packing, thereby using
a smaller number of keys but larger values.

Finally, we implemented the Melbourne Shuﬄe as two
runs of the MapReduce job presented in §6.2, both in Java
and in C++ for VC3. For the Census and Taxi Jan in-
put datasets, our Java implementation takes 76s and 122s,
respectively, and VC3 takes 160s and 153s, respectively. Re-
call that this phase is run once per dataset, and not once
per every job.

9. RELATED WORK

Several systems protect conﬁdentiality of data in the cloud.
CryptDB [22] and MrCrypt [27] use partial homomorphic
encryption to run some computations on encrypted data;
they neither protect conﬁdentiality of code, nor guarantee
the integrity of results. On the upside, they do not use
trusted hardware. TrustedDB [3], Cipherbase [2], and Mon-
omi [29] use trusted hardware to process database queries
over encrypted data, but do not protect the conﬁdential-
ity and integrity of all code and data. Haven [5] can run
databases on a single machine.

All systems above are vulnerable to side-channel attacks.
For example, Xu et al. [31] show how side-channel attacks
can be exploited in systems such as Haven where an un-
trusted operating system controls page faults. We also refer
the reader to [31] for an overview on side-channel attacks.

Several security-enhanced MapReduce systems have been
proposed. Airavat [23] defends against possibly malicious
map function implementations using diﬀerential privacy. Se-
cureMR [30] is an integrity enhancement for MapReduce
that relies on redundant computations. Ko et al. propose a
hybrid security model for MapReduce where sensitive data
is handled in a private cloud while non-sensitive processing
is outsourced to a public cloud provider [15]. PRISM [6] is a
privacy-preserving word search scheme for MapReduce that
utilizes private information retrieval methods.

Nayak et al. [19] propose a programming model for se-
cure parallel processing of data represented as a graph us-
ing oblivious sorting and garbled circuits. Goodrich and
Mitzenmacher [13] describe a simulation of MapReduce that
resembles a sequential version of our Shuﬄe-in-the-Middle
solution using a sorting network instead of a shuﬄe to pro-
tect against traﬃc analysis. This method can be parallelized
using a step from §6 where oblivious sorting uses a reducer
number (computed as a pseudo-random function of each
key) to sort key-value pairs and returns reducer keys in the
clear. In independent parallel work, Dinh et al. [9] also con-
sider securing MapReduce using a mix network to shuﬄe
traﬃc between mappers and reducers. The three solutions
above rely either on oblivious sort or mix network, and thus
incur a logarithmic depth overhead. In comparison, our use
of the Melbourne Shuﬄe in our ﬁrst solution, Shuﬄe in the
Middle, requires only two additional map-reduce jobs, and
incurs a constant depth overhead. Besides, our second solu-
tion, Shuﬄe & Balance, dominates the ﬁrst, even with the
Melbourne Shuﬄe: the security guarantees are stronger (it
hides key distributions, mapper output sizes, and reducer
input sizes) and incurs a much smaller overhead (§8).

Oblivious RAM (ORAM) [11] is a general, well-studied
technique for protecting computations against memory traf-
ﬁc analysis. Though ORAMs are becoming more eﬃcient,
they incur a logarithmic overhead on every access and do
not hide I/O volume. Moreover most ORAMs, except for
the recent theoretical work by Boyle et al. [7] with polylog-
arithmic access overhead, are intrinsically sequential.

158010. REFERENCES

[1] Apache Software Foundation. Hadoop.

http://wiki.apache.org/hadoop/, 15/05/15.

[2] A. Arasu, S. Blanas, K. Eguro, R. Kaushik,

D. Kossmann, R. Ramamurthy, and R. Venkatesan.
Orthogonal security with Cipherbase. In Conference
on Innovative Data Systems Research (CIDR), 2013.

[3] S. Bajaj and R. Sion. TrustedDB: A trusted

hardware-based database with privacy and data
conﬁdentiality. Knowledge and Data Engineering,
IEEE Transactions on, 26(3):752–765, March 2014.

[4] K. E. Batcher. Sorting networks and their

applications. In Proc. 1968 Spring Joint Computer
Conf., pages 307–314. AFIPS Press, 1968.

[5] A. Baumann, M. Peinado, and G. Hunt. Shielding

applications from an untrusted cloud with haven. In
USENIX Symposium on Operating Systems Design
and Implementation (OSDI), 2014.

[6] E.-O. Blass, R. Di Pietro, R. Molva, and M. ¨Onen.
Prism—privacy-preserving search in MapReduce. In
S. Fischer-H¨ubner and M. Wright, editors, Privacy
Enhancing Technologies, volume 7384 of Lecture Notes
in Computer Science. Springer Berlin Heidelberg,
2012.

[7] E. Boyle, K.-M. Chung, and R. Pass. Oblivious

parallel RAM. Cryptology ePrint Archive, Report
2014/594, 2014. http://eprint.iacr.org/.

[8] E. Coﬀman Jr., J. Csirik, G. Galambosa, S. Martello,
and D. Vigo. Bin packing approximation algorithms:
Survey and classiﬁcation. In P. M. Pardalos, D.-Z. Du,
and R. L. Graham, editors, Handbook of
Combinatorial Optimization, pages 455–531. Springer
New York, 2013.

[9] A. Dinh, P. Saxena, C. Ee-chien, Z. Chunwang, and

O. B. Chin. M2r: Enabling stronger privacy in
mapreduce computation. In 24th USENIX Security
Symposium (USENIX Security 15), Washington, D.C.,
Aug. 2015. USENIX Association.

[10] C. Gentry. Fully homomorphic encryption using ideal
lattices. In Proceedings of the Forty-ﬁrst Annual ACM
Symposium on Theory of Computing, STOC ’09, pages
169–178, New York, NY, USA, 2009. ACM.

[17] F. Mckeen, I. Alexandrovich, A. Berenzon, C. Rozas,

H. Shaﬁ, V. Shanbhogue, and U. Savagaonkar.
Innovative instructions and software model for isolated
execution. In Workshop on Hardware and Architectural
Support for Security and Privacy (HASP), 2013.
[18] M. Mitzenmacher and E. Upfal. Probability and

Computing: Randomized Algorithms and Probabilistic
Analysis. Cambridge University Press, New York, NY,
USA, 2005.

[19] K. Nayak, X. S. Wang, S. Ioannidis, U. Weinsberg,

N. Taft, and E. Shi. GraphSC: Parallel secure
computation made easy. In IEEE Symposium on
Security and Privacy, 2015.

[20] O. Ohrimenko, M. Goodrich, R. Tamassia, and

E. Upfal. The melbourne shuﬄe: Improving oblivious
storage in the cloud. In J. Esparza, P. Fraigniaud,
T. Husfeldt, and E. Koutsoupias, editors,
International Colloquium on Automata, Languages
and Programming (ICALP), volume 8573 of Lecture
Notes in Computer Science, pages 556–567. Springer
Berlin Heidelberg, 2014.

[21] V. Pandurangan. On taxis and rainbows: Lessons

from NYC’s improperly anonymized taxi logs, 2014.

[22] R. A. Popa, C. M. S. Redﬁeld, N. Zeldovich, and

H. Balakrishnan. CryptDB: Protecting conﬁdentiality
with encrypted query processing. In Proceedings of the
Twenty-Third ACM Symposium on Operating Systems
Principles, SOSP ’11, pages 85–100, New York, NY,
USA, 2011. ACM.

[23] I. Roy, S. T. Setty, A. Kilzer, V. Shmatikov, and

E. Witchel. Airavat: Security and privacy for
MapReduce. In USENIX Symposium on Networked
Systems Design and Implementation (NSDI), 2010.

[24] F. Schuster, M. Costa, C. Fournet, C. Gkantsidis,
M. Peinado, G. Mainar-Ruiz, and M. Russinovich.
VC3: Trustworthy data analytics in the cloud using
SGX. In IEEE Symposium on Security and Privacy,
2015.

[25] L. Sweeney. Simple demographics often identify people

uniquely. Carnegie Mellon University, Data Privacy
Working Paper 3, 2000.

[26] NYC taxi trips. www.andresmh.com/nyctaxitrips/,

16/05/15.

[11] O. Goldreich and R. Ostrovsky. Software protection

[27] S. D. Tetali, M. Lesani, R. Majumdar, and

and simulation on oblivious RAMs. J. ACM,
43(3):431–473, 1996.

[12] S. Goldwasser and S. Micali. Probabilistic encryption.

T. Millstein. MrCrypt: Static analysis for secure cloud
computations. SIGPLAN Not., 48(10):271–286, Oct.
2013.

J. Comput. Syst. Sci., 28(2):270–299, 1984.

[28] A. Tockar. Riding with the stars: Passenger privacy in

[13] M. Goodrich and M. Mitzenmacher.

Privacy-preserving access of outsourced data via
oblivious RAM simulation. In L. Aceto, M. Henzinger,
and J. Sgall, editors, International Colloquium on
Automata, Languages and Programming (ICALP),
volume 6756 of Lecture Notes in Computer Science,
pages 576–587. Springer Berlin Heidelberg, 2011.

[14] J. Katz and Y. Lindell. Introduction to Modern

Cryptography. Chapman and Hall/CRC Press, 2007.

[15] S. Y. Ko, K. Jeon, and R. Morales. The Hybrex model
for conﬁdentiality and privacy in cloud computing. In
USENIX Workshop on Hot Topics in Cloud
Computing (HotCloud), 2011.

[16] M. Lichman. UCI machine learning repository, 2013.

the NYC taxicab dataset, 2014.

[29] S. Tu, M. F. Kaashoek, S. Madden, and N. Zeldovich.

Processing analytical queries over encrypted data.
Proc. VLDB Endow., 6(5):289–300, Mar. 2013.

[30] W. Wei, J. Du, T. Yu, and X. Gu. SecureMR: A

service integrity assurance framework for mapreduce.
In Proceedings of the 2009 Annual Computer Security
Applications Conference, ACSAC ’09, pages 73–82,
Washington, DC, USA, 2009. IEEE Computer Society.

[31] Y. Xu, W. Cui, and M. Peinado. Controlled-channel

attacks: Deterministic side channels for untrusted
operating systems. In IEEE Symposium on Security
and Privacy, 2015.

1581