On Measuring the Client-Side DNS Infrastructure

Kyle Schomp†, Tom Callahan†, Michael Rabinovich†, Mark Allman‡

†Case Western Reserve University, Cleveland, OH, USA

{kyle.schomp,tom.callahan,michael.rabinovich}@case.edu

‡International Computer Science Institute, Berkeley, CA, USA

mallman@icir.org

ABSTRACT

The Domain Name System (DNS) is a critical component of the In-
ternet infrastructure. It allows users to interact with Web sites using
human-readable names and provides a foundation for transparent
client request distribution among servers in Web platforms, such as
content delivery networks. In this paper, we present methodologies
for efﬁciently discovering the complex client-side DNS infrastruc-
ture. We further develop measurement techniques for isolating the
behavior of the distinct actors in the infrastructure. Using these
strategies, we study various aspects of the client-side DNS infras-
tructure and its behavior with respect to caching, both in aggregate
and separately for different actors.

Categories and Subject Descriptors

C.2 [Computer-Communication Networks]: Miscellaneous; C.4
[Performance of Systems]: Measurement Techniques

Keywords

Internet Measurement; Domain Name System (DNS)

1.

INTRODUCTION

DNS plays a foundational role in today’s Internet. From its ini-
tial function of providing the mapping between human-readable
names (e.g., “amazon.com”) and obtuse network-level addresses,
it evolved to form a basis for building scalable and agile Web plat-
forms.
In particular, by changing name-to-address bindings dy-
namically and providing different bindings to different clients, Web
sites can transparently distribute client load among replicated Web
servers or redirect client requests from their own servers to content
delivery networks (CDNs), while CDNs and similar platforms can
direct incoming requests to speciﬁc nodes in the platform.

With the crucial role DNS plays, the complexity of the DNS
infrastructure—especially the client-side query-resolving aspect—
has increased dramatically. No longer are address lookups a sim-
ple matter of end devices querying a local DNS resolver, which in
turn queries authoritative nameservers on clients’ behalf. Rather,

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
IMC’13, October 23–25, 2013, Barcelona, Spain.
Copyright 2013 ACM 978-1-4503-1953-9/13/10 ...$15.00.
http://dx.doi.org/10.1145/2504730.2504734 .

facilitated by the simplicity of the stateless and connectionless pro-
tocol, DNS has developed into a complex ecosystem often involv-
ing several layers of shared resolvers which can, in turn, peer with
additional resolvers. The path a DNS query takes through this in-
frastructure is often complex and hidden. This complexity makes it
difﬁcult to understand the behavior of the resolving infrastructure
and to attribute responsibility for distinct behaviors to the individ-
ual actors.

This study targets the above challenge and makes the following
contributions. First, we develop a set of methodologies for dis-
covering the client-side DNS infrastructure efﬁciently. Given the
vastness of this infrastructure and a short lifetime of some of its ac-
tors [8,13], probing strategies that improve the rate of discovery can
facilitate subsequent measurements. Second, we develop measure-
ment techniques for teasing apart the behavior of the actors within
the system, some of whom cannot be accessed directly. Third, we
apply our methodologies and strategies to assess some aspects of
the client-side DNS infrastructure and its behavior with respect to
caching, both in aggregate and separately for different actors. Our
key observations from this assessment include the following:

• We double, from 15 to 32 million, previous estimates of the

number of open resolvers on the Internet.

• We ﬁnd evidence of wide adoption of complex resolution
topologies including large shared pools of resolvers at cer-
tain layers in the infrastructure.

• We observe that DNS queries frequently travel large dis-
tances within the resolving infrastructure—both in terms of
geography and network delay. We ﬁnd 20% of open resolvers
experiencing at least 100 msec of delay before their queries
leave the resolution infrastructure.

• To the best of our knowledge, we contribute the ﬁrst assess-
ment of how various actors treat the time-to-live (TTL) set-
tings given by authoritative nameservers to set the behavior
of DNS caches. Despite being a simple notion, we ﬁnd that
different actors handle the TTL differently. The overall ef-
fect is that in many cases the TTL is distorted before reach-
ing the original requesting client. We ﬁnd that only 19% of
all open resolvers consistently return correct TTL values to
all our probes. A 2004 study [17] reports a wide violation
of TTLs by end-clients while a 2012 study from a client site
with “honest” resolvers shows a much lower violation rate by
clients [6]. We expand upon these studies by demonstrating
not only which actors cause violation but also how they be-
have regarding the TTL setting and, thus, cause other actors
to violate TTL.

• We assess the time an unused record stays in the cache of
various actors within the resolving infrastructure, which in
particular determines whether the TTL or cache capacity is

77the cause of eviction. We ﬁnd scant evidence of a general
capacity limitation problem .

2. RELATED WORK

The over-arching methodology we use to study the DNS
ecosystem—as developed in the next three sections—involves ac-
tively discovering and characterizing DNS resolvers that will an-
swer queries from arbitrary hosts throughout the Internet. In this
manner, we can determine how the client-side DNS infrastructure
behaves with regard to a wide range of test queries. The closest
related work to ours is [8], which scans open resolvers in order to
assess answer rewriting occurring on DNS paths. That work fur-
ther contributes the idea of building a mapping between resolvers
found by probing Internet hosts and the resolvers that ultimately
contact an authoritative DNS server. We build upon that technique
to attribute DNS behavior to speciﬁc actors. Our work extends the
probing methodology presented in [8] to effectively discover re-
solver pools and examines many resolver characteristics not dis-
cussed in that paper.

Efﬁciently scanning the IPv4 address space for service discovery
(including DNS) while avoiding complaints is discussed in [13].
While [13] explores reducing the burden of probing, we focus on
reducing the number of probes required without losing insight. Our
additional methodological contributions are in probing strategies
that (i) increase the discovery rate, (ii) identify pools of recursive
resolvers, and (iii) soundly assess speciﬁc behavior of the various
actors in the system.

We also consider this work related to [2], which performs DNS
lookups from several vantage points in order to compare perfor-
mance among various local resolvers and public DNS services. [2]
ﬁnds that ISP-provided resolvers often outperform public DNS ser-
vices in query latency. Another performance-centric study is [14],
which characterizes the difference in observed DNS performance
for common DNS names from a variety of vantage points. Ad-
ditionally, [14] reports on the behavior of the DNS time-to-live,
which we also explore in more depth in this paper. Finally, [22] re-
ports on several facets of DNS servers, including security conﬁgu-
ration and support for DNS protocol components such as DNSSEC
[5].

Several studies [10, 18, 23] show that information gleaned from
DNS resolvers may be used to measure various aspects of the Inter-
net such as popularity of Web sites and inter-host delays. Our work
supports these efforts by developing effective discovery strategies
and showing the diversity of behavior in differing implementations.
Several prior studies consider the number of open resolvers on
the Internet [13, 22], the distance between clients and their re-
solvers [4, 11, 15, 20] and TTL violations [6, 17, 21]. We contrast
our ﬁndings with these studies throughout this paper.

3. CLIENT-SIDE DNS INFRASTRUCTURE
The architecture of the client-side DNS infrastructure varies
across providers. The actors can be loosely grouped into three
roles: (i) “ingress” servers that receive DNS queries directly from
user devices, (ii) “egress” servers that directly communicate with
authoritative DNS servers (ADNS), which maintain hostname to
IP address mappings for their respective domains, and (iii) hidden
servers that act as intermediaries between the ingress and egress but
are not exposed to either clients requesting mappings or authorita-
tive servers providing mappings. To avoid confusion, in the rest of
the paper we use the following terminology to describe the various
components of the client-side DNS infrastructure that is charged

ODNS

RDNS

Client-side

RDNSd

RDNSi

ADNS

Origin

FDNS

HDNS

Figure 1: Structure of the client-side DNS infrastructure.

with obtaining a hostname to IP address binding from an ADNS.
The actors are also illustrated in Figure 1.

• Origin devices are either user devices or the sources of our
DNS requests sent to probe the client-side DNS infrastruc-
ture.

• ODNSes (“open DNS”) are ingress servers that accept re-

quests from any host.

• RDNSes (“recursive DNS resolvers”) are egress resolvers

that communicate directly with authoritative DNS servers.

• FDNSes (“forwarding ODNS”) refers to an ODNS that does
not itself resolve a query, but rather forwards the request
along to another resolver. The FDNS servers are a subset of
the ODNS servers, or the ingress points that origin devices
query.

• RDNSdes (“direct RDNS”) are RDNSes that are also
ODNSes. In other words, an RDNSd is both an ingress and
egress server.

• RDNSies (“indirect RDNS”) are RDNSes observed at the
authoritative DNS server resolving queries on behalf of an
FDNS. Note: The RDNSi and RDNSd sets overlap. That is,
we ﬁnd some RDNS servers that both (i) accept and resolve
queries from arbitrary origin devices and (ii) act on behalf of
a set of FDNS servers we detect in our survey.

• HDNSes (“hidden DNS”) are servers which operate between
FDNSes and RDNSes. Since these servers are neither ingress
nor egress servers, they are invisible externally. While we
cannot directly detect these servers, their existence is con-
ﬁrmed by DNS operators [9] and therefore we must keep
them in mind as their actions may impact our results.

A typical example path through the maze of DNS-related de-
vices has a client computer (Origin) starting the process by sending
a DNS request to a home routing device (FDNS), which in turn
forwards the request through a chain of zero or more HDNSes and
ultimately to an RDNSi. The RDNSi sends the request to the ap-
propriate ADNS. Note that the RDNSi may cooperate with other
RDNSies and subsequent requests from the same FDNS may be
handled by a different RDNSi. We call such structures “RDNS
pools”. We discuss RDNS pools in more detail in §6. This typical
example accounts for nearly all of our experimental observations.

4. METHODOLOGY OVERVIEW

In this section we sketch our general methodology and datasets.
The speciﬁc methodology for each of our experiments is described
in subsequent sections. Our measurements cover only a fraction of
the Internet and therefore we must consider bias, namely, the de-
gree to which the DNS infrastructure we discover and assess is rep-
resentative of the broader Internet. We defer this question to §8—
after we have further developed experiments that can be brought to
bear on the question.

78Scan

Format

Start

S1
S2
S3
S4
S5
S6

Random IP
Random IP
Random /24
Scan on First Hit
Rescan of S3
Scan on First Hit

2/29/12
7/3/12
8/5/12
10/4/12
11/16/12
2/26/13

Dur.
(days)

17
32
17
25
9
31

ODNSes

RDNSes

1.09M
1.98M
841K
17.6M
892K
11M

69.5K
72.6K
43.9K
72.1K
29.9K
65.8K

Table 1: Dataset characteristics

Non-Interference With Normal Operation: While we are inves-
tigating the various components of the DNS ecosystem, we use our
registered domain. Our probing rates are limited to insure we do
not interfere with normal operation of any of the components of
the system, and, although some of our techniques involve cache in-
jection, all DNS requests are for subdomains of our own domain
and we dot not interfere with any actual name-to-address bindings
in use.
Discovering DNS Infrastructure: To examine the client-side
DNS infrastructure, we must have an efﬁcient method for ﬁnd-
ing both ODNSes and RDNSes. Discovering DNS infrastructure
is a challenge because many of the components have policy re-
strictions preventing the acceptance of DNS requests from arbi-
trary hosts. Our basic discovery technique is an extension of the
process described in [8]. We registered a domain name1 and de-
ployed an ADNS for this domain. We then leverage approximately
100 PlanetLab [7] nodes as our origins and randomly probe the IP
address space with DNS requests for various hostnames within our
domain. By embedding the probed IP address in the hostname re-
quest and observing the queries arrive at our ADNS, we collect the
IP addresses that are willing to handle our probes—thus discover-
ing ODNSes. The IP addresses from which the queries ultimately
arrive at our ADNS illuminate the set of RDNSes. Finally, since
the ADNS has the addresses of both the RDNS and ODNS, we can
associate FDNSes with the RDNSes they use for DNS resolution.
Thus, we can elicit a response from an RDNS that will not respond
to direct probes by indirectly probing via the FDNS.
Attribution of Behavior: When measuring DNS behavior, it is
often necessary to identify the actor responsible for the behavior,
e.g., when a violation of the DNS protocol is detected. A key con-
tribution of this paper is measurement techniques to isolate FDNS
behavior from RDNS and HDNS behavior. Through cache injec-
tion2 on FDNSes, to which we found a sizable fraction of FDNSes
are susceptible, we short-circuit HDNS and RDNS from processing
a measurement probe. Therefore, any artifacts are the sole result of
the FDNS. Similarly, we develop a technique of coordinated prob-
ing through two or more FDNSes to determine the behavior of a
shared RDNS in near isolation from FDNS behavior. We validate
the latter technique using the RDNSdies —which we can probe
both through an FDNS and directly—as ground truth of RDNS be-
havior. Estimating from our experiments, over 77% of RDNSies
will not respond to direct DNS requests from external hosts and are
assumed hidden from an outside observer; despite that, our tech-
nique provides the ability to assess their behavior.
ODNS Lifetimes: We note that during our measurements we ﬁnd
that ODNSes are often short-lived—with around 40% becoming
unreachable within one day (see §5.1.1). Since the duration of our
experiments is typically longer than one day, we rediscover the re-

1dnsresearch.us
2A technique for inserting records into a DNS cache against the
spirit of the protocol.

Criterion
RomPager
Basic auth realm
PBL Listed by SpamHaus
PBL Listed by ISP
Wrong port
Total

No. ODNSes

% ODNSes

258K
265K
566K
180K
529K
849K

24%
24%
51%
17%
48%
78%

Table 2: Home network device criteria

solvers anew for each new experiment we discuss below. Hence,
techniques for quick rediscovery are important. We describe these
techniques below and use them to collect different datasets for dif-
ferent experiments, as summarized in Table 1. We describe the de-
tails of each scan as needed throughout the remainder of the paper.
Our datasets are publicly available [19].

5. METHODOLOGY DETAILS

We ﬁrst turn our attention to discovering various components
of the client-side DNS infrastructure. To facilitate our explo-
ration of discovery methodologies, we use two datasets. The ﬁrst
dataset is from the S2 scan in Table 1 and represents the probing
of 255M unique random IP addresses using 267M DNS requests
from 7/3/2012 to 8/3/2012. Our S2 scan discovered 1.9M ODNSes
and 73K RDNSes. The second dataset is from the S3 scan in Ta-
ble 1 and was collected between 8/5/2012 and 8/21/2012 using a
methodology based on completely scanning random /24 IP address
blocks. This scan represents a probing of 465K random /24 ad-
dress blocks—11.9M IP addresses—via 121M DNS requests. The
S3 dataset includes 841K ODNSes and 44K RDNSes. The num-
ber of probes exceeds the number of IP addresses because some
ODNSes use RDNS pools, which we attempt to discover through
repeated probes to ODNSes (see §5.2 for details).

ODNSes appear to be mostly home network devices. During
the S1 scan of random IP addresses we gather detailed informa-
tion about roughly 1.09M ODNSes. We ﬁnd that 78% are likely
residential networking devices as they meet at least one of the fol-
lowing criteria as shown in Table 2: (i) HTTP probes to the de-
vice show that the Web server reports itself as RomPager, which
is a well-known software package for creating Web interfaces in
embedded devices, (ii) HTTP probes to the device show the use
of basic authentication with a realm indicating a likely home de-
vice (e.g., “3068 DSL-2641R”), (iii) the IP address is listed in the
Spamhaus PBL, or (iv) the device replies to a DNS probe from a
port other than the port to which the probe was directed. We spec-
ulate that the ﬁnal criteria is caused by a low-end NAT device that
is performing translation on its own packets. Together, these indi-
cators provide supporting evidence that the ODNSes we discover
are overwhelmingly low-end network devices residing in residen-
tial settings.

5.1 ODNS Discovery

The fundamental aspect of discovery is ﬁnding ODNSes since,
as we will show, these are the windows into the client-side DNS in-
frastructure. Several projects leverage full scans of the Internet ad-
dress space [1, 13] to understand the prevalence of open resolvers.
However, we are interested not only with discovering the existence
of ODNSes, but also with understanding their characteristics and
behavior, which entails sending far more requests than discovery
would dictate (as detailed in subsequent sections). Additionally,
we ﬁnd—as previously developed in the literature [8,13]—the win-
dow of accessibility for ODNSes to be in general fairly short (see
below). Therefore, we must do in-depth probing in conjunction

79F
D
C

1

0.8

0.6

0.4

0.2

0
1

30M

20M

10M

d
e
r
e
v
o
c
s
i
d
 
s
e
S
N
D
O

 
f
o
 
r
e
b
m
u
N

 

0
0

256

 

Random IP
Scan on First Hit

1B

2B

Number of probes sent

3B

4B

Number of ODNSes per /24 IP address block

10

100

Figure 2: Distribution of ODNSes per /24 IP address block,
excluding empty blocks.

with ODNS discovery as returning to the given address later may
well be fruitless. Finally, our probing rate has to result in a man-
ageable load on the ADNS—both the server itself and the host-
ing network—where ultimately the measurement trafﬁc converges.
For our ADNS, the resource constraints and desire to ﬁnish exper-
iments in a reasonable time frame necessitates a partial scan. The
key questions that arise from this choice involve (i) understanding
the effectiveness of randomly probing arbitrary IP addresses with
DNS requests in the hope of stumbling upon ODNSes, and (ii)
whether there are probing strategies to improve the efﬁciency of
this process.

Our ﬁrst observation is that ODNSes are unevenly distributed
throughout IP space. As sketched above, in S3 we choose and
probe random /24 address blocks. We ﬁnd that only 14% of these
blocks contain ODNSes. Further, as Figure 2 shows, the distri-
bution of ODNSes among the blocks that have some ODNSes is
uneven. We ﬁnd a small number of “dense” blocks with many
ODNSes. For instance, the top 10% of the address blocks each
contain over 30 ODNSes while 40% of the blocks have no more
than two ODNSes. The average across all blocks with at least one
ODNS is approximately 13 ODNSes per /24 block.

Discovery within such a sparse address space requires extensive
scanning. For collecting a sample of ODNSes with a partial scan,
we examine two methods of ODNS discovery. The ﬁrst method
is a random scanning of IP addresses labeled “Random IP”. The
second method, “Scan on First Hit”, acts like “Random IP” but,
once an ODNS is discovered, proceeds to scan the entire /24 block
in which the ODNS resides. This latter method utilizes the above
observation of uneven ODNS distribution among /24 blocks to in-
crease the ODNS discovery rate.

To compare the two methods fairly, we simulate both of them
based on the same dataset from the S3 scan using the following
methodology. We consider the Internet’s 232 IP addresses divided
into 224 /24 blocks. We mark a random 14% of /24 blocks as
“productive” —which as previously discussed is the fraction of /24
blocks found to contain at least one ODNS server—and in each
productive block we mark a number of IP addresses as ODNS ac-
cording to the distribution from Figure 2. ”Random IP” is then
simulated by selecting randomly without replacement from the full
232 address range and counting the rate of discovering ODNS. For
“Scan on First Hit”, we again select an address randomly without
replacement. If the selected address is an ODNS, we count not only
this address, but also all addresses marked as ODNS in the encom-
passing /24 block and remove the block from the address pool for
further selection.

Figure 3: ODNS discovery rate versus DNS requests sent (ex-
trapolation from the S3 scan to the Internet scale).

Figure 3 shows the discovery rate for both methods. We ﬁnd
a drastically higher initial discovery rate using the “Scan on First
Hit” strategy, which maintains its advantage for all scan sizes un-
til the techniques converge to discover the entire set of ODNSes
with a complete scan. The discovery rate of “Scan on First Hit” de-
creases over time. The reason is that the more dense a /24 address
block is the higher the probability of ﬁnding an ODNS; therefore,
dense /24 address blocks have a greater chance of being discov-
ered early. The purely random scan shows steady progress across
the entire scan but is overall less productive for limited scans. As
noted above, only 14% of the /24 blocks contain ODNSes. So,
the random scan misses opportunities to learn about the “neighbor-
hood” when ﬁnding an ODNS and chances are that neighborhood
is populated with additional ODNSes.

While the “Scan on First Hit” strategy discovers more ODNSes
with fewer probes, it has a downside in that it introduces a bias in
the set of discovered ODNSes. Blocks with higher concentrations
of ODNSes have a greater chance of being discovered, thus bias-
ing the resulting dataset towards ODNS in well-populated address
blocks. Thus, when using this efﬁcient discovery method, one must
consider implications of its bias. We consider effects of this bias on
our results in §8.

5.1.1 Rediscovery and Whitelisting

ODNSes have previously been found to be short lived [8,13] and
we conﬁrm these results. In our S6 scan conducted from 2/26/2013
through 3/28/2013 we repeatedly probe discovered ODNSes for a
period of 1M seconds after discovery. Details of the S6 scan and the
intervals at which the ODNSes were probed are discussed in §7. As
shown in Figure 4, 40% of the discovered ODNSes answer queries
for no more than one day and 80% of the ODNSes cease answering
queries within one week. As noted above, there is evidence that the
ODNSes we ﬁnd are predominantly home network devices. There-
fore, we suspect that short ODNS lifetimes are due to DHCP lease
expirations. Thus, we conclude that our lists of ODNSes become
stale and biased quickly and for this reason we discover ODNSes
anew for each phase of our study.

Rescanning can be an expensive and time consuming process.
Fortunately, we ﬁnd that ODNSes demonstrate a tight IP spa-
tial cohesion: while an individual ODNS can be short lived, pro-
ductive /24 blocks tend to remain productive. We rescanned the
same /24 address blocks from the S3 scan between 11/16/2012 and
11/24/2012, nearly three months after the S3 scan ended; this scan
is labeled S5 in Table 1. We also ﬁnd that 76% of the 67K produc-

80F
D
C
C

1

0.8

0.6

0.4

0.2

 

0
1

 

1M

10
100K
Length of ODNS accessibility (seconds)

10K

100

1K

Figure 4: Distribution of the duration of ODNS accessibility.

800K

600K

400K

200K

d
e
r
e
v
o
c
s
i
d
 
s
e
S
N
D
O

 
f
o
 
r
e
b
m
u
N

 

0
0

Random IP on Whitelist
Random IP on Internet

 

4M

12M
Number of probes sent

8M

16M

Figure 5: ODNS discovery rate using whitelisting in the S5 scan
compared to the discovery rate of the S2 scan.

tive /24 address blocks during the former scan remain productive
during the repeat scan. This spatial cohesion over time enables the
use of “whitelisting” to rescan just those /24 address blocks which
were previously productive. Using the same simulation method-
ology we employ to explore Random IP vs. Scan on First Hit
above (Figure 3) we study re-scanning previously productive /24
blocks. Figure 5 shows the discovery rate for rescanning the 67K
productive /24 address blocks from the S5 scan using Random IP—
i.e., scanning random IP addresses from the whitelisted /24 address
blocks—in contrast to random IP selection from the entire Inter-
net address space based on the S2 scan. Clearly, rescanning us-
ing whitelisting is more efﬁcient than random scanning. We also
note that whitelisting may be used in conjunction with the “Scan
on First Hit” strategy to generate a whitelist containing dense /24
address blocks. Rescanning using such a whitelist would likely
have a much higher discovery rate than Figure 5 suggests.

5.2 RDNS Discovery

RDNS discovery provides more of a challenge than ODNS dis-
covery for two reasons. First, unlike ODNS discovery, RDNS dis-
covery is an indirect process whereby the characteristics and behav-
iors of the ODNSes may impact the process. Second, the RDNS re-
solver topologies are complex, unlike the ODNS population, which
is by deﬁnition just a set of simple servers. In particular, an ODNS
may forward DNS queries to a pool of resolvers, which may option-
ally utilize another layer of resolvers before the queries egress the
infrastructure and are visible at our ADNS. For example, Google’s
public DNS utilizes a two-level topology that hashes the requested
hostnames to particular egress resolvers to improve their cache ef-

fectiveness [9]. Unfortunately, we can only discover the egress
RDNSes and do not have a technique for discovering HDNSes in
the middle of the infrastructure.

We use a two-pronged approach for RDNS discovery. First, for a
given ODNS we send multiple DNS requests for hostnames within
our domain in an attempt to spread those requests throughout the
RDNS pool—if such exists. For this we use unique hostnames such
that each request must move through the entire infrastructure and
end up at our ADNS. Second, our ADNS returns a variable-length
chain of CNAME records to queries from RDNSes3. We know
from experience that the RDNS that sends a DNS request is often
not the same RDNS that resolves the CNAME redirections. We use
both these mechanisms until we stop discovering new members of
the observed RDNS pool. Speciﬁcally, our strategy is as follows.

• When a ﬁrst probe to a newly discovered ODNS arrives
at our ADNS through a previously discovered RDNS, the
ADNS responds with a special A record indicating that no
new RDNS discovery has occurred.

• However, when this ﬁrst query arrives from a previously
unknown RDNS, the ADNS responds to a query with a
CNAME record of a new subdomain. After receiving the
subsequent query for this new subdomain we repeat the pro-
cess four additional times. When this batch of ﬁve CNAME
queries leads to the discovery of at least one new RDNS then
the entire process is repeated with ﬁve additional CNAMEs.
This process continues until no new RDNS is found, at which
point a special A record is returned to the client indicating
that new RDNSes were discovered.

• When the A record returned—through the ODNS—indicates
new RDNS were discovered,
the client sends ﬁve more
probes for distinct subdomains to this ODNS. Note that these
subsequent probes may trigger a series of CNAME responses
by our ADNS as described above. As long as the A record
from the ADNS indicates new RDNS discovery, probing ex-
tends with another batch of ﬁve probes.

• When the A record returned to the client indicates that no
new RDNSes were found, the discovery process terminates.
Our S2 scan uses the above procedure. Furthermore, to enable
exploration of alternate scanning strategies—as well as to discover
RDNS pools in §6—our S3 scan uses a modiﬁed version of the
above procedure that triggers a new CNAME batch as long as new
RDNSes are discovered for the current ODNS rather than consult-
ing the full set of RDNSes from all probing.

We test this basic RDNS discovery mechanism with four ODNS
probing strategies: “Random IP”, “Scan On First Hit”, and “Ran-
dom /24 Block” described earlier, plus “Aborted Random Block”,
which is a scan of random /24 address blocks that terminates af-
ter the ﬁrst ODNS in that block is found. The idea behind the last
strategy is that the ODNSes in a /24 block will all share the same
RDNS infrastructure and so the ﬁrst ODNS will trigger the dis-
covery of the lion’s share of the RDNSes. The results for all four
strategies reﬂect simulations driven by the dataset collected by the
S3 scan.

Figure 6 shows the discovery rates of our four methods. The
“Scan on First Hit” method has a higher rate than the alternate
strategies for two reasons. First, we ﬁnd that ODNSes within
the same /24 address block do not all use the same RDNS or
RDNS pool—contrary to our intuition. Therefore, learning about
the “neighborhood” is beneﬁcial not only for ODNS discovery but
also for RDNS discovery. This accounts for “Scan on First Hit”

3A CNAME record indicates a “canonical” name for the hostname
queried. On receiving this record, the resolver will issue a new
query for the name contained in the CNAME record.

8140K

30K

20K

10K

d
e
r
e
v
o
c
s
i
d
 
s
e
S
N
D
R
 
f
o
 
r
e
b
m
u
N

 

0
0

 

Random IP
Scan On First Hit
Aborted Random Block
Random Block

20M

40M
80M
Number of probes sent

60M

100M 120M

Figure 6: RDNS discovery rate versus DNS requests sent, sim-
ulated from the S3 scan.

RDNSd and comparing the returned time-to-live (TTL) value with
the TTL we expect to be set by the Web site’s ADNS—which we
established separately. A TTL value in a DNS response that is less
than the ADNS assigned TTL indicates that the Web site’s record is
in the RDNSd’s cache, suggesting that some real client previously
requested the record. Figure 7 shows the distribution of the num-
ber of popular Web site records that appear to be in the caches of
RDNSdes without FDNSes and in the caches of RDNSdies. Al-
though we later show that RDNSes are prone to inaccurate report-
ing of TTLs, the difference between the two curves indicates a
difference in the behavior of the two sets of RDNSes. We opt to
remove RDNSdes without FDNSes from our analysis since their
purpose is unclear. Instead, we focus the remainder of our study
upon RDNSies which have a clear purpose within the client-side
DNS infrastructure.

5.3 Techniques for Untangling Behavior

 

We now discuss techniques we developed to tease apart the be-

1

0.8

0.6

0.4

0.2

F
D
C

RDNS

RDNS

 

 without FDNSes
d

0
0
10
Number of Alexa Top 100 hostnames in RDNS’s cache

1

2

5

6

8

9

3

4

di

7

havior of FDNS from RDNS.

5.3.1 Measuring FDNS

As we previously note, the vast majority of ODNSes we dis-
cover are in fact FDNSes (over 95% across all datasets). Gaining
an understanding of FDNSes in isolation from the remainder of
the client-side DNS infrastructure is a challenge. Fortunately, we
ﬁnd that a fraction of FDNSes allow a primitive form of cache in-
jection which we leverage to gain insight into the behavior of this
FDNS subset. Speciﬁcally, these FDNSes do not perform any of
the following security checks on DNS responses which could pre-
vent cache injection: (i) change and/or verify the transaction ID,
(ii) verify the source IP address, and (iii) verify the destination
port number. The absence of such checks makes it straightforward
to follow a request to an FDNS with an acceptable response which
only involves the FDNS and not the rest of the infrastructure.

Hence, we can study these FDNSes in isolation from HDNSes
and RDNSes using the following procedure. We begin by sending
a DNS request for a hostname within our domain to the FDNS un-
der study and then immediately issue a DNS response for the same
hostname to the FDNS that binds the requested name to IP address
X. On the other hand, when the request arrives to our ADNS in
a normal way, the latter answers with a response containing IP ad-
dress Y . Then, any subsequent requests made by our probing host
that are responded to with IP address X must have come from the
FDNS’s cache and the FDNS is effectively isolated from the rest of
the client-side infrastructure, which never touched record X. We
stress that the FDNSes we study may be a biased set since the set
only includes FDNSes that exhibit the cache injection vulnerability.

5.3.2 Measuring RDNS

Since typically we cannot query RDNSies directly, we utilize
FDNSes as our window into RDNSi behavior. This, however,
poses a problem as FDNSes may alter DNS requests, responses
and caching phenomena, hence obscuring RDNS behavior. A sec-
ond response for a domain name through a single FDNS may be
returned from either the FDNS’s cache or the RDNSi’s cache, am-
biguously. Fortunately, it is common to ﬁnd multiple FDNSes
which use the same RDNSi. We leverage a general experimen-
tal strategy which requires at least two FDNSes per RDNSi to
succeed—F1 and F2. While the exact details of the technique vary
with different experiments and will be detailed separately, the gen-
eral framework is as follows. We begin by requesting a unique
subdomain of our domain from F1. Our ADNS responds to this
query with a randomly generated record, which should be cached

Figure 7: Number of the Alexa top 100 Web sites in the caches
of RDNSes.

achieving a higher RDNS discovery rate than Random IP. Sec-
ond, because the vast majority of /24 blocks do not contain any
ODNSes, much of the scanning in Aborted Random Block and
Random Block is wasted. We note that Aborted Random Block
was unable to discover 13K of the 43.9K RDNSes within the S3
dataset. The undiscovered RDNSes were not reachable through the
ﬁrst ODNS found within each /24 block. The “Scan on First Hit”
technique provides the best ODNS and RDNS discovery rate in
terms of scanning infrastructure cost and time.

5.2.1 RDNSd Evaluation

The RDNSdes deserve special attention here. These servers have
been counted during both ODNS and RDNS discovery. Yet, it is
unclear if all RDNSdes serve clients. They could, for instance, be
misconﬁgured authoritative DNS servers that happen to be willing
to accept external queries for external domains as discussed in [10].
We ﬁnd that 51% of the RDNSdes in the S2 scan are used by at
least one FDNS, i.e., are in fact RDNSdies. The remaining 49%
could be resolvers which might be accessed by origins directly, or
whose client FDNSes are hidden from our scans’ view or have been
missed by our scans.

To determine if the 49% (17K) of RDNSdes which are not used
by any FDNSes in our dataset from the S2 scan are actually acting
as resolvers for some client population we query them for the top
100 Web sites as listed by Alexa [3]. If the RDNSdes are resolvers,
then they are likely handling DNS requests from their clients for
some of these Web sites. Therefore, some of these popular host-
names should be in the RDNSdes’ caches. We detect if a record
is in the cache by sending a DNS request for the hostname to the

82F
D
C

1

0.8

0.6

0.4

0.2

0
1

10
1K
Number of FDNSes per RDNS

100

10K

F
D
C

1

0.8

0.6

0.4

0.2

0
1

10

RDNS pool size per FDNS

100

1K

Figure 8: Number of FDNSes per RDNSi in the S3 scan.

Figure 9: Distribution of the RDNS pool size for each FDNS.

at the RDNSi on the return path to F1. Then, after a predetermined
amount of time, we query for the same subdomain through F2. If
the RDNSi still has the record in its cache, the response from F2
will match the response from F1. In this case, we further know that
the record is from the RDNSi’s cache and not from F2 because the
request was previously unseen by F2. If the record is no longer in
the RDNSi’s cache, the request will arrive at our ADNS, which will
respond with a different record. In this way, we eliminate FDNS
caching behavior when studying RDNSi caching behavior.

This technique relies upon discovering two FDNSes which use
the same RDNSi at roughly the same time. Figure 8 shows the
number of FDNSes per RDNSi in the S3 dataset. Over 80% of the
RDNSies are used by more than one FDNS and the coordinated
probing technique has a chance of succeeding. Further, 50% of
RDNSies appear with at least 10 FDNSes in the dataset, vastly
increasing the chances of successful measurement.

FDNS behavior may still distort the measurement by altering
records. We discuss ways to mitigate this problem—such as us-
ing all available FDNSes— in §7.

6. TOPOLOGY

In this section, we address the size and structure of the client-side

DNS infrastructure.

6.1 Estimating Global ODNS Population

Extrapolating from our limited scans of IP space, we estimate
that there are approximately 32M ODNSes on the Internet today.
We arrive at this result from two independent scans. First, we ﬁnd
almost 2M ODNS within a set of 254.7M probed IPs in the “Ran-
dom IP” scan S2 where addresses are chosen randomly from the
complete 232 address space. Therefore, we estimate the population
size as 2M/254.7M × 232 = 33M ODNSes across the Internet.
Second, from the “Random /24” scan S3, the fraction of productive
/24 address blocks (those with at least one ODNS) is 0.141 and a
productive block contains on average 13 ODNSes. Therefore, the
ODNS population across the entire Internet is 0.141 × 13 × 224 =
31M (a similar number is also obtained from simulations in Fig-
ure 3).

These estimates signiﬁcantly exceed previous results of com-
plete Internet scans [13] and estimates [22], which show around
15M responding DNS resolvers. One of our partial scans using the
“Scan on First Hit” strategy directly identiﬁes 17.6M ODNSes—
more than found in previous full scans. Additionally, [1], a com-
plete Internet scan, reports 33M open resolvers as of May 2013
which agrees with our estimate. The results show the population of
ODNSes on the Internet has increased since previous studies.

6.2 RDNS Pool Sizes

The ODNSes we ﬁnd in both our S2 scan and S3 scan uti-
lize RDNSes in roughly 99% of the cases—i.e., they are in fact
FDNSes. Moreover, approximately 70% of FDNSes use an RDNS
pool in both scans. Per §5.2, we use repeated DNS requests
and CNAME chaining triggered by per-ODNS discovery of a new
RDNS to identify RDNS pools used by an FDNS. Figure 9 shows
the size distribution of the discovered RDNS pools in the S3 scan.
The plot shows that 10% of FDNSes use RDNS pools consisting
of more than 10 servers. Also, note that these pools can encom-
pass multiple providers, e.g., an ISP’s own DNS infrastructure and
OpenDNS, which could occur when either the FDNS is conﬁgured
to use both, e.g., one as the primary DNS server and the other as the
secondary DNS server, or the ISP is utilizing some alternate DNS
infrastructure for some queries.

6.3 Distance between FDNSes and RDNSes

settings.

As discussed in §5.1.1, ODNSes—and consequently FDNSes—
are predominantly in residential
Many Internet
platforms—notably content delivery networks—rely on the as-
sumption that client machines are close to their RDNS in that the
platforms direct the client to a nearby node within the platform
based on the location of the client’s RDNS. To test this assump-
tion, we use a geolocation database [16] to calculate the distance
between FDNSes and the RDNSes they employ for resolution. We
perform this on the S6 dataset. Figure 10 presents the distribu-
tion of these distances4. Since an FDNS may use several RDNSes,
we plot the minimum, maximum, and mean distance between an
FDNS and all of the RDNSes it utilizes. We ﬁnd that FDNSes are
often quite close to their RDNSes with the median being approxi-
mately 100 miles. However, one in ten FDNSes appear to be over
8K miles from at least one of their RDNSes; these FDNSes expe-
rience a high cache miss cost and potentially incorrect redirections
by content delivery networks to nodes that are not nearby. Prior
studies [4, 11] also consider the geographical distance between the
client hosts and their DNS resolvers and report different results.
In particular, [11] observes shorter distances between FDNSes and
RDNSes, with 60% of clients to be within 17 miles from their re-
solvers, while [4] reports fewer client-resolver pairs at the low dis-
tance range, with only 25% of pairs being within 500 miles from
each other, and also fewer extremely distant pairs, with just 5% of
pairs more than 2000 miles apart. The differences with our experi-
ment could be due to different vantage points. Both [11] and [4]

4The accuracy of our results rests upon the accuracy of the Max-
Mind geolocation database which varies by country. For example,
MaxMind claims 81% accuracy within 25 miles for IP addresses
inside the USA. See [16] for accuracy in other countries.

83F
D
C

1

0.8

0.6

0.4

0.2

 

0
1

Min
Mean
Max

10

100

1K

10K

Distance from FDNS to RDNS pool (miles)

Figure 10: Distribution of the geographic distance from FDNS
to RDNS.

F
D
C

1

0.8

0.6

0.4

0.2

 

0
1

 

Min
Mean
Max

10

100

1K

10K

RTT from FDNS to RDNS pool (milliseconds)

Figure 11: Distribution of round trip time between FDNS and
RDNS.

instrumented a Web page and passively observed FDNS/RDNS
pairs via DNS requests and subsequent HTTP connections, where
as our study is an active scan which exhaustively explores all
FDNS/RDNS pairs. Therefore, we discover FDNS/RDNS pairs
that may not appear in prior studies.

Additionally, we found that some FDNSes respond to ICMP
pings. In the S6 scan, we obtained (1) the round trip time from our
measurement point to the 22% of the FDNSes that were responsive
to a ping and also (2) the round trip time between our measure-
ment point and the respective RDNS through the FDNS for 6.3M
FDNS/RDNS pairs. The later leverages our coordinated probing
technique. We add a record to the RDNS’ cache via a probe from
the ﬁrst FDNS. We then obtain the distance from our measurement
point to the RDNS through the second FDNS by querying for the
same record through the second FDNS. The difference between (2)
and (1) is the round trip time in milliseconds between the second
FDNS and the RDNS. We also repeat the process by swapping the
roles of the two FDNS servers. We perform this measurement for
each FDNS pair we discover using the same RDNS during dis-
covery. Using this technique, we are able to obtain the round trip
time from FDNS to RDNS for 5.6M FDNS/RDNS pairs across
1.3M unique FDNSes. In the case of multiple measurements per
FDNS/RDNS pair, we choose the minimum delay value as it most
accurately reﬂects the actual network delay between the FDNS and
RDNS. We plot the results in Figure 11. The median round trip
time is about 10 ms, however nearly 20% of the FDNSes experi-
ence delays in excess of 200 ms to at least one of their RDNSes.

 

7. CACHING BEHAVIOR

Caching aids the scalability of the DNS system and hides delay
for hostname resolution. Additionally, DNS caching has important
performance and security implications.

In terms of performance, DNS caching complicates Internet
sites’ trafﬁc engineering ability because a single hostname-to-
address binding may pin all clients behind a given resolver to the
selected server for an extended period of time. Not only does this
handicap sites’ control over client request distribution but it also
complicates the removal of unneeded infrastructure without risking
failed interactions from clients using old bindings. DNS nominally
provides sites with the capability to bound these effects by speci-
fying a time-to-live (TTL) value within DNS responses to limit the
amount of time recipients can reuse the response. However, recipi-
ents are known to disobey TTL [6, 17, 21]. With regard to security,
caching determines the lingering effect of a successful record in-
jection (e.g., using the Kaminsky attack [12]).

The extent of these phenomena depends on two inter-related as-
pects: how long a resolver keeps a record in its cache and how the
resolver treats the TTL. We ﬁrst explore the aggregate behavior of
all components of the client-side DNS infrastructure, as this will
be the view of the clients leveraging the infrastructure. We then
use the measurement techniques described above to tease apart the
behavior of the components in isolation to gain insight into which
components are responsible for various behavior.

The results in this section come from the S6 scan (see Table 1).
The scan covers 79M IP addresses with 1.3B DNS requests from
2/26/2013 through 3/28/2013. The S6 scan encompasses 11M
ODNSes and 65.8K RDNSes—46K of which are RDNSies. The
S6 dataset uses the “Scan on First Hit” methodology to increase the
ODNS discovery rate and thus the probability of ﬁnding multiple
FDNSes which use the same RDNSi at roughly the same time. This
assists with the coordinated probing strategy sketched in §5.3.2.

7.1 Aggregate Behavior

To investigate aggregate behavior of the DNS resolution infras-
tructure, we perform in-depth probing of 2.4M FDNSes during the
S6 scan. We did not test all of the FDNSes because of limitations
in the amount of trafﬁc our ADNS can handle. Therefore, we limit
the number of FDNSes which can be concurrently assessed to 25K
per measurement origin (PlanetLab node). When a measurement
origin ﬁnds a new FDNS we skip in-depth measurement if 25K
FDNSes are presently being assessed.

We examine how FDNSes and RDNSies behave when presented
with DNS records with different TTL values: 1, 10, 30, 60, 100,
120, 1,000, 3,600, 10,000, 10,800, 86,400, 100,000, 604,800 and
1,000,000 seconds. For each TTL value, we re-probe at various
intervals to determine whether the records are still available. We
use intervals slightly below the TTL values (by two seconds), to
check if the record is retained for the full TTL. We also use in-
tervals slightly above the TTL values (by two seconds), to check
if the record is retained longer than the TTL. For this experiment,
our ADNS returns a random IP address. Thus, if subsequent DNS
requests for the same hostname return the same IP address we
know—with high likelihood—the request was satisﬁed by a cache
somewhere within the resolving infrastructure, either at the FDNS,
HDNSes, or RDNSi.

Our ﬁrst observation is that some of the responses arrive at the
client with incorrect TTL values, i.e., different from those set by
our ADNS. Furthermore, even when the client receives the correct
TTL for the initial request for a hostname, we ﬁnd subsequent re-
quests result in responses with incorrect TTL values. We interpret
the latter as due to the DNS actor distorting TTL not when con-

84Behavior
Honest

Lie on Initial

Lie on Subsequent

Constant TTL
Increment TTL

Percentage of Measurements

19%
38%
9%
7%
1%

Table 3: Aggregate TTL Behavior

Expected (sec) % <

% >

Mode Lie

1

10-120
1000
3600
10000
10800
86400
100000
604800
1000000

0%

11%

Value
10000
≤ 1% ≤ 8% 10000
10000
10000
3600
3600
21600
21600
21600
604800

1%
2%
5%
8%
16%
22%
22%
64%

3%
2%
0%
0%
0%
0%
0%
0%

% of All Lies

35%

≥ 37%

62%
51%
40%
27%
36%
27%
26%
67%

Table 4: Aggregate TTL Deviations

veying the record back to the requester but when storing the record
in its cache. Table 3 summarizes our results for aggregate behav-
ior. In total, 19% of the FDNSes and their underlying infrastruc-
ture always report the correct TTL value. Meanwhile, 38% of the
FDNSes respond with an incorrect TTL value to an initial request
and 9% of FDNSes respond with an incorrect TTL value to the
subsequent requests. The fact that at least 62% of FDNSes hon-
estly report the TTL on the ﬁrst DNS request will be important in
studying RDNSies below.

Beyond changing the TTL of DNS records, we ﬁnd some
FDNSes fail to correctly decrement the TTL over time5. We ﬁnd
that 7% of the FDNSes return a constant TTL value, never decre-
menting. An additional 1% of the FDNSes actually begin to incre-
ment the TTL after it counts down to zero! Both these behaviors
eventually result in a TTL which disagrees with the initial setting
by the ADNS. Downstream devices—regardless of how they treat
the TTL—may unwittingly use such records in violation of the in-
tent of the ADNS.

Table 4 shows the TTL deviations we observe including those
in response to both the initial and subsequent requests. The table
shows the percentage of cases when TTL is less than (“< %”) and
greater than (“> %”) the ADNS-assigned value. In addition, we
show the most common TTL violation (mode lie) and the percent-
age of the lies the mode represents. For example, we ﬁnd 11% of
FDNSes deviate from a 1 second authoritative TTL. Further, 35%
of the lies are for a TTL of 10,000 seconds. The prevalence of lies
increases for both small and large TTL values. For instance, most
resolvers appear to cap the TTL at one week (604,800 seconds).

We next consider how long a record remains cached and accessi-
ble in the client-side DNS infrastructure. Using repeated probes at
the intervals mentioned above, we record the latest time at which a
record is returned to our probing host. While it is possible that the
record was still in some cache at this point and resolution merely
took a different path through the infrastructure, our experiment re-
ﬂects the behavior a user of the system would experience. Figure 12
shows the length of time records remain in some cache within the
resolving infrastructure. We present results for records with TTLs

5A resolver is supposed to decrement the TTL to account for time
spent in the resolver’s cache.

1

0.8

F
D
C
C

0.6

0.4

0.2

 

0
1

 

All 1M Sec
Accessible 1M Sec
Accessible 30 Sec
All 30 Sec

10

100

Record lifetime per aggregate cache (seconds)

1K

10K

100K

1M

Figure 12: Aggregate cached record lifetimes.

of 30 seconds, a short TTL value similar to those used by con-
tent delivery networks (e.g., Akamai uses 20 seconds and Lime-
light uses 350 seconds). Additionally we use records with TTLs of
1 million seconds, a value chosen to determine how long the infras-
tructure will retain rarely used records. The “All” lines reﬂect the
longest record lifetime we observe from a given FDNS. However,
a number of FDNSes become unreachable in the course of the ex-
periment. When this happens due to IP address reassignments as
discussed in §5.1.1, the records may still be in the cache. Thus,
the “All” line may be an underestimate of the length of time the in-
frastructure retains DNS records. Consequently, we report results
separately for the 189K FDNSes that remain accessible throughout
the experiment (the “Accessible” line). The true cache duration lies
somewhere between these two lines.

Our results show that 90% of FDNSes and their supporting in-
frastructure retain records with a TTL of 30 seconds for no longer
than the TTL—with 60% of the FDNSes retaining the record for
the full 30 seconds. We ﬁnd that 10% of FDNSes retain the record
for longer than the TTL—with 4% retaining the record for over 100
seconds. This indicates that in general short TTLs are relatively ef-
fective in controlling DNS caching behavior. These results deviate
from the ﬁndings from a 2004 passive study that shows TTL viola-
tions on the order of hours were not uncommon [17]. Furthermore,
the records assigned a TTL of 1 million seconds show much longer
retention, with 40% active for more than 1 hour. This indicates that
short cache retention of the records with TTLs of 30 seconds is due
to the TTL setting rather than cache capacity constraints.

7.2 FDNS Behavior

We now study the behavior of FDNSes in isolation by using
the record injection technique described in §5.3.1. We ﬁnd 683K
FDNSes in the S6 dataset (6%) accept our injected response records
and were thus amenable to our measurement.

We utilize the same experimental technique as in §7.1 with the
exception that we follow up the initial DNS request for a hostname
with a DNS response that binds the hostname to IP address X.
The same binding is never returned by our ADNS, so whenever X
appears in a DNS response we know the request was satisﬁed from
the FDNS’ cache. If the DNS response contains an address other
than X we know X is no longer in the FDNS’ cache. We perform
the experiment for the same TTL values and re-probe intervals as
in §7.1.

Table 5 summarizes our general ﬁndings on FDNS’ TTL behav-
ior. First, 60% of the FDNSes never lie with respect to the TTL.
However, we ﬁnd that 12% of FDNSes lie in response to the initial
request and 30% lie in response to the subsequent requests. As we

85Behavior
Honest

Lie on Initial

Lie on Subsequent

Constant TTL

Incrementing TTL

Percentage of Measurements

60%
12%
30%
26%
10%

Table 5: FDNS TTL Behavior

Expected (sec) % < % >

Mode Lie

1

10-3600
10000
10800
86400
100000
604800
1000000

Value % of All Lies

0%

31% 10000
≤ 1% 19% 10000

88%

≥ 95%

1%
19%
19%
19%
19%
25%

60

0%
0% 10000
0% 10000
0% 10000
0% 10000
0% 10000

92%
97%
97%
97%
97%
75%

Table 6: FDNS TTL Deviations

note above, we interpret this latter behavior as due to the FDNS dis-
torting TTL not when conveying the record back to the origin but
when storing the record in its cache. We also determine that 26%
of FDNSes report a constant TTL value without ever decrementing
it. Finally, 10% of the FDNSes began to increment the TTL value
upon decrementing it to zero.

Table 6 shows the TTL deviations we observe for FDNSes, in-
cluding both initial and subsequent deviations. The table shows the
same general trend as aggregate behavior but with more deviations
at the low end of TTL spectrum and less prevalence of capping the
TTL at 1 week. The number of cases where the authoritative short
TTL is replaced by a 10,000 second value jumps by roughly 50%
for the authoritative TTL of 1 second as compared to 10 seconds.
Thus, the ADNS often will retain better control over FDNS caching
by assigning a TTL larger than 1 second.

We now consider how long a record remains accessible in the
cache of FDNSes. Using repeated probes at the intervals mentioned
above, we record the latest time at which the injected record is re-
turned to our probing host. Again, we present results for records
supplied by our ADNS with the TTLs set to 30 seconds and 1 mil-
lion seconds. Figure 13 shows how long the records are retained in
the caches. See §7.1 for an explanation of the “All” and “Accessi-
ble” lines. For this experiment, the “Accessible” line contains the
results for 22.5K (3.3%) of the 683K tested FDNSes.

The results show that roughly 40% of the FDNSes retain the
record with a TTL of 30 seconds for longer than the TTL. Addi-
tionally, 28% of FDNSes hold the record for over 100 seconds,
more than twice the TTL. This deviates from the aggregate results
in Figure 12 indicating that FDNSes which allow cache injection
are more likely to retain records past the TTL value than the general
FDNS population. We ﬁnd records with a TTL of 1 million seconds
are retained for at least 10,000 seconds in 50% of the FDNSes.

7.3 RDNSi Behavior

We now turn to the behavior of RDNSies in near isolation
from FDNSes. RDNSdies are particularly convenient in that we
can examine their behavior in perfect isolation since there is no
other actor to interfere with our results. We have results for 4.9K
RDNSdies. We use the same experimental technique as in §7.1
with the same TTL values and re-probe intervals. Table 7 shows
our general results while Table 8 shows the TTL deviations we ob-

F
D
C
C

1

0.8

0.6

0.4

0.2

 

0
1

 

All 1M Sec
Accessible 1M Sec
All 30 Sec
Accessible 30 Sec

10

100K
Record lifetime per FDNS cache (seconds)

10K

100

1K

1M

Figure 13: Distribution of record availability in FDNS caches
for records with TTLs of 30 and 1 million seconds.

Behavior
Honest

Lie on Initial

Lie on Subsequent

Constant TTL

Incrementing TTL

Percentage of Measurements

2%
80%
18%
0%
0%

Table 7: RDNSdi TTL Behavior

serve. RDNSdies do not exhibit either the constant TTL nor the
pathological TTL incrementing behavior we observe in FDNSes
and the aggregate data. However, there is more TTL distortion with
virtually no consistently honest RDNSdies.

For RDNSies that do not respond to direct probes, we leverage
the coordinated probing strategy to assess their violations. As de-
scribed in §5.3.2, we begin by requesting a unique hostname via
FDNS F1. Our ADNS responds to this query with two IP ad-
dresses, one random and the second uniquely bound to the RDNSi
from which the DNS request arrives. We assign the same set of
TTL values as in §7.1. Both records pass through RDNSi on the
return path to F1 and may be cached for subsequent requests. Next,
our experiment requests the same hostname via F2. If the RDNSi
still has the records in its cache and is leveraged by both F1 and
F2, the random IP address from the response to F1 will be re-used
and hence the response to F2 will show the same address.

However, at this point, any TTL deviations cannot be attributed
to the RDNSi, the FDNSes or even some HDNS in the path. To
gain conﬁdence in our attribution of TTL deviations to RDNSi, we
leverage two pieces of information. First, as noted in the previous
two sections, a signiﬁcant number of FDNSes are honest on the
initial DNS response. We ﬁnd 62% of FDNSes are honest on the

Expected (sec) % < % >

Mode Lie

1-120
1000
3600
10000
10800
86400
100000
604800
1000000

Value
3600
22%
3600
19%
86400
7%
3600
7%
3600
7%
3600
0%
86400
0%
0%
86400
0% 604800

0%
3%
3%
16%
16%
16%
40%
40%
88%

% of All Lies

≥ 52%

53%
69%
53%
52%
72%
59%
59%
54%

Table 8: RDNSdi TTL Deviations

86Percentage of Measurements

Expected (sec) % <

% >

Mode Lie

Behavior
Honest

Lie on Initial

Lie on Subsequent

Constant TTL

Incrementing TTL

36%
55%
5%
5%
0%

Table 9: RDNSi TTL Behavior

initial response in §7.1 and 88% of FDNSes are honest on the initial
response in §7.2. Second, we can utilize more than two FDNSes in
coordinated probing to mitigate the effect of FDNS lies. Instead of
F1 and F2 representing single FDNSes in the above description of
the experiment, we utilize up to 10 FDNSes that we divide into two
sets. We send the same request through each FDNS at roughly the
same time.

If any FDNS responds with the correct TTL value, then we con-
clude the RDNSi must be truthful since every component was truth-
ful in this case. On the other hand, if no FDNS responds with the
correct TTL, then it is probable that the RDNSi is responsible for
the lie. In this situation, there are three scenarios. First, some of
the FDNSes are in the set of FDNSes that were honest in initial re-
sponses and the TTL values from these FDNSes all agree. In this
ideal case, their responses collectively identify the actual TTL the
RDNSi provides. In the second scenario, the TTL values arriving
at honest FDNSes do not agree. One potential cause of this case is
HDNSes interposing between some of the FDNSes and the RDNSi.
In this case, we assume that the RDNSi returns the most common
TTL value among the honest FDNSes. In the ﬁnal scenario, none
of the FDNSes are honest. In this case, we assume that the RDNSi
returns the most common TTL value among all the FDNSes.

If the majority of FDNSes access an RDNSi through the same
HDNS, then our experiment will conﬂate the behavior of the
RDNSi and the HDNS. However, we note that if an RDNSi is only
accessible through a single HDNS, then learning the RDNSi’s be-
havior in isolation is moot because only the aggregate behavior of
both components will ever impact client devices in the real system.
We validate our technique for determining RDNSi TTL behavior
using RDNSdies, which allow us to obtain ground truth by direct
probing. The results using our coordinated probing technique agree
with the ground truth in 98% of the cases, and not only in detecting
whether an RDNSi is honest but also in determining the quantita-
tive TTL violations (as we will show, most lies are from a small
ﬁxed set of TTLs).

In our dataset, there are 46K RDNSies and we conduct in-depth
probing for 22K of them (due to logistical issues, as sketched
above). Table 9 shows our ﬁndings. We determine that 36% of
RDNSies are honest. Further, 55% of RDNSi lie on the initial
response to F1, but only 5% of RDNSies lie in response to sub-
sequent requests from F2 indicating that the behavior of caching a
different TTL than initially returned is less prevalent in RDNSies
than FDNSes. This supports our conjecture that FDNSes, be-
ing mostly home-based devices, represent more primitive imple-
mentations of DNS. In addition to incorrect TTLs, we ﬁnd 8%
of RDNSies return constant TTL values without decrementing.
The TTL deviations from RDNSies, merged with the results for
RDNSdies are shown in Table 10.

We now consider how long a record remains cached and acces-
sible at RDNSies. Again, we use the experimental setup described
earlier in this section with one exception. Instead of querying from
F2 immediately after receiving the response from F1, we wait be-
fore repeating the query (using the same intervals as in §7.1 up to

1-120
1000
3600
10000
10800
86400
100000
604800
1000000

≤ 1% ≤ 1%

1%
1%
2%
2%
5%
11%
11%
49%

0%
0%
0%
0%
0%
0%
0%
0%

Value
300
900
80

3600
7200
21600
86400
86400
604800

% of All Lies

≥ 34%

29%
19%
35%
20%
32%
55%
53%
71%

Table 10: RDNSi TTL Deviations

F
D
C
C

1

0.8

0.6

0.4

0.2

 

0
1

All 1M Sec
Accessible 1M Sec
All 30 Sec
Accessible 30 Sec

10

100K
Record lifetime per RDNS cache (seconds)

10K

100

1K

 

1M

Figure 14: Distribution of record availability in RDNSi caches
for records with TTLs of 30 and 1 million seconds.

the TTL value6). For each RDNSi, we track the latest time at which
the record is available.

Again, when FDNSes become unavailable in the course of the
experiment we cannot detect how much longer a record stays in
the RDNS’s cache. Thus, Figure 14 shows the duration of record
retention separately for all measured RDNSes (the “All” lines, rep-
resenting 22K tested RDNSies and underestimating the result) and
for those RDNSes that remained accessible throughout the experi-
ment (the “Accessible” lines, representing 8.8K and 2.4K RDNSies
for the 30 second TTL and the 1 million second TTL, respectively).
Concentrating on the “Accessible” lines as more reliable, we
show that DNS records with a TTL of 1 million seconds stay in the
cache for a long time, with the records still present 10K seconds
(2.8 hours) after being inserted in 90% of the cases. Furthermore,
step-wise drops indicate record evictions at ﬁxed values of time in
cache, indicating some conﬁguration parameters rather than capac-
ity eviction, and a more gradually descending line in the aggregate
behavior (Figure 12) is likely due to FDNS affects. The record with
a TTL of 30 seconds remains in the cache for the full TTL in 94%
of the tested RDNSies.

8. DATASET REPRESENTATIVENESS

Finally, we return to the issue of bias in our datasets ﬁrst men-
tioned in §4. Since our scans do not encompass the entire Internet,
it is possible that our results are not demonstrative of the entire
population of FDNSes and RDNSies due to biases in our scanning
methodology. In particular, our results on FDNS behavior encom-
pass a subset of FDNSes, i.e., those which allow cache injection.
Similarly, our results for RDNSies include only those RDNSies

6Unfortunately, we neglected to measure intervals beyond TTL and
hence do not check if RDNSes cache records beyond the authorita-
tive TTL as we did for FDNSes.

870.7

0.6

0.5

0.4

0.3

0.2

s
r
o
t
c
A

 

 
f
o
n
o
i
t
c
a
r
F

Aggregate
FDNSes
RDNS

es
i

 

0.1
1

2

3

4

Chronological Snapshots

5

6

7

 

8

9

10

Figure 15: Fraction of honest actors over the discovery process.

for which we discover at least two FDNSes. For these two, we
demonstrate here that our dataset is representative of the respective
subsets of the whole population. On the other hand, we can validate
the aggregate behavior against the whole population.

We assess representativeness of our datasets by calculating the
fraction of actors which honestly report the TTL value for all TTL
values we utilize. We divide our datasets into ten slices ordered by
the time of discovery. For the aggregate behavior and FDNS be-
havior, the 10 slices each include an identical number of measured
FDNSes while for the RDNSi behavior, the 10 slices each include
an identical number of measured RDNSies. We then calculate a
cumulative snapshot of the fraction of honest actors found in the
ﬁrst n slices for 1–10 slices. The cumulative rate should ﬂatten out
if the dataset is typical of the broader population.

Figure 15 shows the results. The fraction of honest actors in the
aggregate data remains constant throughout the 10 snapshots, in-
dicating that we quickly discover a representative sample in this
study. In particular, these results indicate that the “Scan on First
Hit” method of discovery used in this study and which has a bias
potential, does not bias this particular metric. The fraction of hon-
est RDNSies decreases over time but appears to converge to a
constant value by the 7th snapshot. This shows that (1) honest
RDNSies are discovered at a higher rate towards the beginning of
the scan and (2) we discover a sufﬁcient number of RDNSies to
be representative of the general population. Finally, the fraction of
honest FDNSes increases throughout the 10 snapshots though the
growth is ﬂattening. This result indicates that our dataset is not suf-
ﬁcient to capture a representative set of FDNSes that allow cache
injection. The fraction of honest FDNSes in the true population is
likely higher than what we report in this paper.

A larger question of representativeness is whether open DNS
resolvers are representative of the overall population of DNS re-
solvers users employ. In other words, do the behaviors we ﬁnd in
ODNSes more broadly apply to ﬁrst hop resolvers in general? Our
methodology does not afford any way to directly assess this ques-
tion given that we would have to do so from inside many edge net-
works to gain an understanding of resolver behavior for ﬁrst hop re-
solvers that are not arbitrarily accessible. This problem does not ap-
ply to our RDNS results because given the window that the myriad
ODNSes provide we are readily able to get “inside” the RDNSes’
networks and assess their behavior.

9. CONCLUSION

In this paper, we present a set of methodologies for efﬁciently
discovering the client-side DNS infrastructure and, once discov-
ered, teasing apart the behavior of the actors within the system in-

cluding components that cannot be directly probed. Using these
methodologies, we assess various aspects of the client-side DNS in-
frastructure and its behavior with respect to caching, both in aggre-
gate and separately for different actors. In terms of the infrastruc-
ture, we double previous estimates of the number of open resolvers
on the Internet, ﬁnd evidence of wide use of shared resolver pools,
and observe signiﬁcant distances DNS messages travel within the
infrastructure. In terms of caching behavior, we show how long
various actors retain records and how they report TTL values. In
general, we observe that the authoritative TTL value is frequently
modiﬁed by the client-side DNS infrastructure. We show that large
TTLs are reduced in 64% of the cases, and small TTLs are in-
creased in 11% of our measurements. We tease apart these behav-
iors and attribute the former behavior predominantly to RDNSies
and the latter behavior predominantly to FDNSes. Additionally, we
ﬁnd that cache evictions due to capacity limits occur infrequently in
RDNSies even for rarely accessed records. At the same time, while
the TTL is frequently mis-reported to clients, resolvers themselves
do not retain records much past authoritative TTL. We observe that
records are returned past TTL in only 10% of the cases, even for
records with a relatively short TTL of 30 seconds.

Acknowledgments

This work was supported in part by NSF through grants CNS-
0831821, CNS-1213157 and CNS-0831535. The authors would
like to thank the anonymous reviewers – and in particular our shep-
herd, Meeyoung Cha – for their assistance in improving the paper.

10. REFERENCES
[1] Open Resolver Project.

http://openresolverproject.org/.

[2] B. Ager, W. Mühlbauer, G. Smaragdakis, and S. Uhlig.

Comparing DNS Resolvers in the Wild. In 10th ACM
SIGCOMM IMC, pages 15–21, 2010.

[3] Alexa. http://www.alexa.com/topsites.
[4] H. A. Alzoubi, M. Rabinovich, and O. Spatscheck. The

Anatomy of LDNS Clusters: Findings and Implications for
Web Content Delivery. In 22d Int. WWW Conf., 2013.

[5] R. Arends. DNS Security Introduction and Requirements,

2005. RFC 4033.

[6] T. Callahan, M. Allman, and M. Rabinovich. On Modern

DNS Behavior and Properties. ACM SIGCOMM CCR,
43(3):7–15, 2013.

[7] B. Chun, D. Culler, T. Roscoe, A. Bavier, L. Peterson,

M. Wawrzoniak, and M. Bowman. PlanetLab: An Overlay
Testbed for Broad-Coverage Services. ACM SIGCOMM
CCR, 33(3):3–12, 2003.

[8] D. Dagon, N. Provos, C. Lee, and W. Lee. Corrupted DNS

Resolution Paths: The Rise of a Malicious Resolution
Authority. In NDSS, 2008.

[9] I. Google.

https://developers.google.com/speed/
public-dns/docs/performance#loadbalance.

[10] K. Gummadi, S. Saroiu, and S. Gribble. King: Estimating

Latency Between Arbitrary Internet End Hosts. In 2nd ACM
SIGCOMM Workshop on Internet Measurment, pages 5–18.
ACM, 2002.

[11] C. Huang, D. Maltz, J. Li, and A. Greenberg. Public DNS

System and Global Trafﬁc Management. In IEEE
INFOCOM, pages 2615 –2623, 2011.

[12] D. Kaminsky. Black Ops 2008: It’s the End of the Cache As

We Know It. Black Hat USA, 2008.

88[13] D. Leonard and D. Loguinov. Demystifying Service

[19] K. Schomp, T. Callahan, M. Rabinovich, and M. Allman.

Discovery: Implementing an Internet-wide Scanner. In 10th
ACM IMC, pages 109–122, 2010.

Client-Side DNS Infrastructure Dataset, Oct. 2013.
http://dns-scans.eecs.cwru.edu/.

[14] R. Liston, S. Srinivasan, and E. Zegura. Diversity in DNS

[20] A. Shaikh, R. Tewari, and M. Agrawal. On the Effectiveness

Performance Measures. In 2nd ACM SIGCOMM Workshop
on Internet Measurment, pages 19–31. ACM, 2002.

of DNS-based Server Selection. In INFOCOM, pages
1801–1810, 2001.

[15] Z. M. Mao, C. D. Cranor, F. Douglis, M. Rabinovich,

[21] C. Shue, A. Kalafut, M. Allman, and C. Taylor. On Building

O. Spatscheck, and J. Wang. A Precise and Efﬁcient
Evaluation of the Proximity Between Web Clients and Their
Local DNS Servers. In USENIX ATC, pages 229–242, 2002.

[16] Geoip. maxmind llc, 2012.
[17] J. Pang, A. Akella, A. Shaikh, B. Krishnamurthy, and

S. Seshan. On the Responsiveness of DNS-based Network
Control. In 4th ACM SIGCOMM IMC, pages 21–26, 2004.

[18] M. Rajab, F. Monrose, A. Terzis, and N. Provos. Peeking

Through the Cloud: DNS-based Estimation and its
Applications. In Applied Cryptography and Network
Security, pages 21–38. Springer, 2008.

Inexpensive Network Capabilities. ACM SIGCOMM CCR,
42(2), Apr. 2012.

[22] G. Sisson. DNS Survey: October 2010. http://dns.

measurement-factory.com/surveys/201010/,
2010.

[23] C. E. Wills, M. Mikhailov, and H. Shang. Inferring Relative

Popularity of Internet Applications by Actively Querying
DNS Caches. In 3rd ACM SIGCOMM IMC, pages 78–90,
2003.

89