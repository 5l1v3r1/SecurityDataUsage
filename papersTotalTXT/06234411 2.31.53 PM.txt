2012 IEEE Symposium on Security and Privacy

Foundations of Logic-Based Trust Management

Moritz Y. Becker
Microsoft Research
Cambridge, UK

Alessandra Russo

Department of Computing

Imperial College, London, UK

Nik Sultana

Computer Laboratory

University of Cambridge, UK

Abstract—Over the last 15 years, many policy languages have
been developed for specifying policies and credentials under the
trust management paradigm. What has been missing is a formal
semantics – in particular, one that would capture the inherently
dynamic nature of trust management, where access decisions are
based on the local policy in conjunction with varying sets of
dynamically submitted credentials. The goal of this paper is to
rest trust management on a solid formal foundation. To this end,
we present a model theory that is based on Kripke structures for
counterfactual logic. The semantics enjoys compositionality and
full abstraction with respect to a natural notion of observational
equivalence between trust management policies. Furthermore, we
present a corresponding Hilbert-style axiomatization that is ex-
pressive enough for reasoning about a system’s observables on the
object level. We describe an implementation of a mechanization
of the proof theory, which can be used to prove non-trivial meta-
theorems about trust management systems, as well as analyze
probing attacks on such systems. Our benchmark results show
that this logic-based approach performs signiﬁcantly better than
the only previously available, ad-hoc analysis method for probing
attacks.

I. INTRODUCTION

Trust management [12] is an access control paradigm for
decentralized systems that has attracted a lot of attention over
the last 15 years. Research so far has focussed on concrete
architectures and policy languages for trust management, and
on policy analysis. This paper attempts to shed light on some
of the more foundational aspects of trust management.

A. Trust Management

Trust management can be succinctly characterized by two

distinctive features:

1) The access policy of the relying party is speciﬁed in a
[11], [45], [26], [37],

high-level policy language (e.g.
[34], [39], [38], [23], [10], [9], [31], [6]).

2) Access decisions do not depend solely on the local policy,
but also on digitally signed credentials that are submitted
to the relying party together with the access request.
Access is granted only if a proof of compliance can be
constructed, showing that the requested permission Q is
provable from the policy P combined with the set of
credentials C.

The ﬁrst feature effectively decouples the policy from the
implementation of the enforcement mechanism,
improving
maintainability and ﬂexibility in a context of quickly evolving
access control requirements.

The second feature is necessitated by the fact that, in large
decentralized systems, the relying party generally does not
know the identity of the users requesting access in advance.
Therefore, authorization has to be based on attributes rather

© 2012, Moritz Y. Becker. Under license to IEEE.
DOI 10.1109/SP.2012.20

161

than identity. Authority over these attributes may be delegated
to trusted third parties, who may then issue credentials that
assert these attributes or re-delegate authority to yet another
party. The credentials that are used in trust management may
thus be quite expressive, containing attributes, constraints and
conditions, and delegation assertions. For this reason,
the
language for specifying credential assertions is typically the
same as the one for specifying the local policy.

B. Trust Management Semantics

Given a derivability relation (cid:2) between sets of assertions
and permissions, the basic mechanics of a trust management
system can be speciﬁed as follows: a user’s request Q is
granted iff P ∪ C (cid:2) Q, where P is the relying party’s local
policy and C is the set of supporting credentials submitted
by the user. All policy languages mentioned above can be
speciﬁed in terms of such a derivability relation (cid:2); in the
common case of Datalog-based policy languages, the relation
(cid:2) is simply the standard Datalog entailment relation [20].

Hence we arrive at a natural notion of observational equiv-
alence on policies that captures the essential aspects of trust
management: two policies P and P (cid:2) are equivalent iff for all
sets C of credentials and all requests Q,

P ∪ C (cid:2) Q ⇐⇒ P (cid:2) ∪ C (cid:2) Q.

The fundamental question we are concerned with in this
paper is whether an adequate model-theoretic semantics of
trust management exists, i.e., one that matches this notion
of observational equivalence. Neither the standard model-
theoretic Datalog semantics based on minimal Herbrand mod-
els (for Datalog-based languages) nor the Kripke semantics
for authorization logics related to ABLP [2] are adequate in
this sense. While these semantics are sufﬁcient for determining
which permissions are granted by a ﬁxed policy P and a ﬁxed
set C of supporting credentials, they do not provide any insight
into questions that are particular to trust management, such as:
(a) Given the semantics of a policy P , which permissions Q
are granted when P is combined with credential set C?
(b) Given the semantics of two policies P1 and P2, what is
the semantics of their composition P1 ∪ P2?

(c) What can an external user infer about an unknown policy
merely by successively submitting requests together with
varying sets of credentials and observing the relying
party’s responses?

C. Technical Contributions

We present the ﬁrst formal trust management semantics
that accurately captures the action of dynamically submitting

varying sets of credentials. It is compositional with respect to
policy union and provides full abstraction [43] with respect
to observational equivalence. These two properties together
enable it to answer the questions (a) and (b) above.

Furthermore, we develop an axiomatization that is sound and
complete with respect to the model-theoretic semantics, and
provides inferentially complete object-level reasoning about a
trust management system’s observables. For example, judge-
ments such as “if a policy grants access to Q1 when combined
with set C1, and denies access to Q2 when combined with set
C2, then it must grant access to Q3 when combined with C3”
can be expressed as a formula in the logic, and be proved (or
disproved) within it. It is this expressive power that enables
the logic to directly answer questions such as (c) above, and
thus to analyze probing attacks, a recently identiﬁed class of
attacks in which the attacker infers conﬁdential information
by submitting credentials and observing the trust management
system’s reactions [31], [4], [8]. Perhaps even more strikingly,
it is expressive enough to prove general meta-theorems about
trust management systems, e.g. “if a policy satisﬁes some
negation-free property, then this property will still hold when
the policy is combined with an arbitrary credential set”.

A language-independent semantics would be too abstract
to provide any interesting insights. Our trust management
semantics is speciﬁc to Datalog, and thus applicable to the
wide range of Datalog-based policy languages. Datalog has
arguably been the most popular logical basis for languages
in this context; examples include Delegation Logic [37], SD3
[34], RT [39], [38], Binder [23], Cassandra [10], [9], and
SecPAL [6].

The remainder of the paper is structured as follows. We
introduce in Section II a simple language for reasoning about
Datalog-based trust management policies, deﬁned by a re-
lation (cid:2), that captures the intuitive operational meaning of
policies and credential submissions. This relation itself is
straightforward, but, as we argue in Section III, universal
truths (that hold for all policies) are both useful and highly
non-trivial. This justiﬁes the need for a logic with a for-
mal semantics with a notion of validity that coincides with
the intuitive notion of universal truths in trust management
systems (Section IV). The corresponding axiomatization is
presented in Section V. Section VI describes our implemen-
tation of a theorem prover for the logic. Applications and
performance results are discussed in Section VII. We review
related work in Section VIII and conclude with Section IX.
The proofs of our theorems are lengthy; we relegate them
to a technical report [?]. Our implementation is available at
http://research.microsoft.com/counterdog.

II. A SIMPLE TRUST MANAGEMENT LANGUAGE

We ﬁx a countable set At of propositional variables called
atoms.1 A Datalog clause is either an atom p or of the form

1In practice, ﬁrst-order predicates are used as atoms instead of propositional
letters, but if the domain is ﬁnite, as is usually the case, the ﬁrst-order
case reduces to the propositional one. We choose the latter presentation for
simplicity.

162

p : – p1, ..., pn, where p, p1, ..., pn ∈ At. Apolicy γ is a ﬁnite
set of clauses. We write Γ to denote the set of all policies.

Atoms correspond to atomic facts that are relevant to access
control, e.g. “Alice can execute run.exe” or “Bob is a part-
time student” or “the system is in state Red”. From the point
of view of the Datalog engine, the atoms have no inherent
meaning beyond the logical dependencies speciﬁed within the
policy (and the submitted credentials). It is the responsibility
of the reference monitor, which acts as an interface between
requesters and resources, to query the policy in a meaningful
way. For instance, if Alice attempts to execute run.exe, the
reference monitor would check if the corresponding atom
CanExec(Alice, run.exe) is derivable from the policy in
union with Alice’s submitted credentials.
To specify when an atomic query p ∈ At is derivable from
a policy γ, we introduce the relation symbol (cid:2):

γ (cid:2) p iff p ∈ γ or

(1)

(2)

∃(cid:3)p ⊆ﬁn At : (p : – (cid:3)p) ∈ γ ∧ ∀p(cid:2) ∈ (cid:3)p. γ (cid:2) p(cid:2).

We can straightforwardly extend (cid:2) to Boolean compound

formulas ϕ, and the trivially true query:

γ (cid:2) (cid:10).
γ (cid:2) ¬ϕ iff γ (cid:11)(cid:2) ϕ.
γ (cid:2) ϕ ∧ ϕ(cid:2)

iff γ (cid:2) ϕ and γ (cid:2) ϕ(cid:2).

The relation γ (cid:2) ϕ may be read as “ϕ holds in γ”.
It is the negated case where Datalog differs from classical
logic: in the latter, ¬p is entailed by a set of formulas γ only if
p is false in all models of γ. In Datalog, on the other hand, only
the minimal model of γ is considered. This ﬁts in well with the
decentralized security model, where knowledge is generally
incomplete, and thus the absence of information should lead
to fewer permissions.

The purpose of our language is not just to specify concrete
policies, but to speak and reason about policy behaviors in a
trust management context. In particular, recall that the outcome
of queries is not just dependent on the service’s policy alone,
but also on the submitted credentials, which are also Datalog
clauses. To express statements about such interactions, we
introduce the notation (cid:3)γϕ, which informally means “if the
set of credentials γ were submitted to the policy, then ϕ would
be true”. The policy is evaluated in union with the credentials,
so we deﬁne

γ (cid:2) (cid:3)γ(cid:2) ϕ iff γ ∪ γ(cid:2) (cid:2) ϕ.

(3)
The full syntax of formulas in our trust management reason-
ing language is thus summarized by the following grammar:

ϕ ::= (cid:10) | p | ¬ϕ | ϕ ∧ ϕ | (cid:3)γϕ
We write Φ to denote the set of all formulas.
As usual, we deﬁne ϕ ∨ ϕ(cid:2) as ¬(¬ϕ ∧ ¬ϕ(cid:2)), ϕ → ϕ(cid:2) as
¬ϕ ∨ ϕ(cid:2), and ϕ ←→ ϕ(cid:2) as (ϕ → ϕ(cid:2)) ∧ (ϕ(cid:2) → ϕ). The unary
operators (cid:3) and ¬ bind more tightly than the binary ones,
and ∧ and ∨ more tightly than → and ←→. Implication (→)
is right-associative, so we write ϕ1 → ϕ2 → ϕ3 for ϕ1 →
(ϕ2 → ϕ3).

II.1. Let

Example
policy
{p : – q, r; p : – s; q : – p, t; q : – u} (we use the semicolon
as separator in clause sets, to avoid the ambiguity with the
comma).

the Datalog

γ0

be

1) Without supporting credentials, no atom holds in γ0:

γ0 (cid:2) ¬v, for all v ∈ At.

2) If u and r were submitted as supporting credentials, then

p would hold in γ0:

γ0 (cid:2) (cid:3){u; r}p.

3) If credential s were submitted, and then t were submitted,

then q would hold in γ0:

γ0 (cid:2) (cid:3){s}(cid:3){t}q.

This is, of course, equivalent to submitting both at the
same time: γ0 (cid:2) (cid:3){s; t}q.

4) Submitted credentials may include non-atomic clauses:

γ0 (cid:2) (cid:3){s :– q; u}p.

When are two policies (observationally) equivalent? Intu-
itively, they are equivalent if they both make the same set
of statements true, under every set of submitted credentials.
This notion can be formalized using the standard Datalog
containment relation (cid:15), as follows:
Deﬁnition II.2 (Containment, equivalence). Let γ1, γ2 ∈ Γ.
Then γ1 is contained in γ2 (γ1 (cid:15) γ2) iff for all ﬁnite (cid:3)p ⊆ At
and p ∈ At:

γ1 ∪ (cid:3)p (cid:2) p ⇒ γ2 ∪ (cid:3)p (cid:2) p.

Two policies γ1 and γ2 are equivalent (x ≡ y) iff γ1 (cid:15) γ2 and
γ2 (cid:15) γ1.

This deﬁnition may seem a bit narrow at ﬁrst, but

the
following proposition shows that it actually coincides with the
intuitive notion that exactly the same set of formulas (including
(cid:3)-formulas!) holds in two equivalent policies.
Proposition II.3. Let γ1, γ2 ∈ Γ.

γ1 ≡ γ2 iff ∀ϕ ∈ Φ. γ1 (cid:2) ϕ ⇔ γ2 (cid:2) ϕ

Example II.4.
1) ∅ (cid:15) γ, for all γ ∈ Γ.
2) {a} (cid:15) {a; b} (cid:15) {a; b; c}
3) {a : – b, c} (cid:15) {a : – b} (cid:15) {a}
4) {a : – d; d : – b} ≡ {a : – b, c; a : – d; d : – b}.

III. UNIVERSAL TRUTHS

The relation (cid:2) from Section II is a straightforward speciﬁ-
cation of what a policy engine in a trust management system
does. It is merely the standard Datalog evaluation relation
extended with the (cid:3)-operator for expressing the action of sub-
mitting supporting credentials. The relation is easy to evaluate
(for a given γ and ϕ), and it directly reﬂects the intuition of
the operational workings of a trust management system. So
why should we bother developing a formal semantics that, as
we shall see, is much more complex?

163

There are three compelling reasons:
• A model-theoretic semantics lets us interpret and ma-
nipulate policies as mathematical objects in a syntax-
independent way. It also provides additional insights into,
and intuitions about, trust management systems.

• To prove that a formula is not a theorem, it is often easier
to construct a counter-model (or in our case, a counter-
world) than to work directly in the proof theory.
• The relation (cid:2) actually does not even provide a proof
theory for formulas ϕ: it is of no help in answering the
more interesting (but much harder) question if ϕ is valid,
i.e., if holds in all policies γ. A formal semantics is the
ﬁrst step towards a corresponding proof theory.

The ﬁrst two answers also apply to the question on the
beneﬁts of having a model-theoretic semantics for any logic.
The third point is perhaps the most important from a practical
perspective: in policy analysis, we are not mainly interested
in the consequences of concrete policies and concrete sets of
submitted credentials, but in universal truths ϕ that hold in all
policies (or all policies that satisfy some properties).
Deﬁnition III.1. We write (cid:2) ϕ iff ϕ holds in all policies, i.e.,
∀γ ∈ Γ. γ (cid:2) ϕ.

The following examples illustrate that the reasoning tech-
niques required in proving universal truths ϕ are beyond those
directly provided by the deﬁnition of (cid:2).
Example III.2. If p is true in some policy when credential
q : – r is submitted, then p would also be true in the same
policy if credential q were submitted:

(cid:2) (cid:3){q :– r}p → (cid:3){q}p

Intuitively, q is “more informative” than q : – r (more formally,
{q : – r} (cid:15) {q}), and providing more information can only lead
to more (positive) truths, as Datalog is monotonic.
Example III.3. If submitting a and b individually is not
sufﬁcient for making c hold in some policy, but submitting
both of them together is sufﬁcient, then a cannot possibly hold
in the policy:

(cid:2) ¬(cid:3){a}c ∧ ¬(cid:3){b}c ∧ (cid:3){a; b}c → ¬a

For suppose a were true in the policy. Then submitting both
a and b would be equivalent to submitting just b, but this
contradicts the observation that submitting solely b does not
make c true.
Example III.4.
If a does not hold in some policy, and
submitting d is not sufﬁcient for making e hold, but submitting
both credentials b : – a and d : – c is sufﬁcient, then c must hold
in that policy, and furthermore, a would hold if credential d
were submitted:

(cid:2) ¬a ∧ (cid:3){d}¬e ∧ (cid:3){b :– a; d :– c}e → c ∧ (cid:3){d}a.

This small example is already too complex to explain suc-
cinctly by informal arguments, but it illustrates that reasoning
about universal truths is far from trivial. We later present a
formal proof of this statement in Example V.4.

A. Probing Attacks

There is a class of attacks on trust management systems
called probing attacks [31], [4], [8], in which the attacker gains
knowledge of secrets about the policy by submitting a series
of access requests together with sets of supporting credentials,
and by observing the system’s reactions. Checking if a probing
attack allows the attacker to infer a secret can be very complex,
but it turns out that we can express probing attacks succinctly
and directly as universal truths in our language.

Here is a simple (and na¨ıve) example of a probing attack.
A service S has a policy γ that includes the publicly readable
rule

S.canRegister (x) : – x.hasConsented (S).

(4)

Informally, this should mean “S says that x can register with
the service if x says (or has issued a credential saying) that he
or she consents with S’s terms and conditions”. The service
also exposes the query S.canRegister (x) to any user x.

Suppose the user (and attacker) A self-issues a conditional

credential

A.hasConsented (S) : – A.isRegistered (B),

(5)

which informally means “A says that A consents to S’s terms
and conditions, if A says that B is registered”. A then submits
together with the query S.canRegister (A),
this credential
and observes that
is ‘no’. From this single
observation, she learns that neither A.hasConsented nor
A.isRegistered (B) hold in γ – or else the query would have
yielded the answer “yes”. This is not very interesting so far,
as she has only learnt about the falsity of statements made by
herself.

the answer

But suppose she can also issue delegation credentials of the
form A.p : – D.p. Such credentials are usually used to express
delegation of authority; for example, to delegate authority over
who is a student to university U, A would issue the credential
A.isStudent(x) : – U.isStudent(x). But here A abuses this
mechanism by issuing the delegation credential

A.isRegistered (B) : – S.isRegistered (B).

(6)

Now she submits this credential together with the ﬁrst condi-
tional credential, and evaluates the same query. By observing
the service’s reaction to this second probe, and combining this
with her previous observation, she then learns whether B is
registered (according to S!) or not: the service’s answer is
“yes” iff γ (cid:2) S.isRegistered (B). She has thus detected a fact
in γ that had nothing to do with the original query, and may
well be conﬁdential. Moreover, it is generally not possible to
protect against probing attacks without crippling the intended
policy using simple syntactic input sanitization or by enforcing
strict non-interference (see [4] for details).

We now show how this attack can be expressed as a
universal truth. Let c1 and c2 be the credentials 5 and 6,
respectively. A’s knowledge about the public clause (4) in the
policy translates into

ϕ1 = (cid:3){A.hasConsented(S)}S.canRegister (A).

164

Her ﬁrst observation is translated into

ϕ2 = (cid:3){c1}¬S.canRegister (A),

and the second observation into

ϕ3 = (cid:3){c1,c2}S.canRegister (A) or
3 = (cid:3){c1,c2}¬S.canRegister (A),
ϕ(cid:2)

depending on the service’s reaction. Then the following holds:

(cid:2) ϕ1 ∧ ϕ2 ∧ ϕ3 → S.isRegistered (B)
3 → ¬S.isRegistered (B)
(cid:2) ϕ1 ∧ ϕ2 ∧ ϕ(cid:2)

We will later present a logic that can prove such statements,
and thus can also be used to reason about probing attacks (see
Example V.5).

Note that Examples III.3 and III.4 can also be interpreted as
probing attacks. For instance, in Example III.4, let us assume
that e is the only query publicly exposed by the service, and
the attacker initially only knows that a does not hold in the
service’s policy. The attacker possesses three authenticated
credentials: d and b : – a and d : – c. By submitting ﬁrst d
together with the query e, and after that {b : – a; d : – c}
together with the same query, and by observing the service’s
reactions to these two probes, the attacker detects (provided
she is sufﬁciently clever) that c ∧ (cid:3){d}a holds in the policy.
Depending on the circumstances, this may constitute a breach
of secrecy.

We can succinctly deﬁne the notions of probes, probing
attack, detectability and opacity from [4], [8] in our language.
Deﬁnition III.5. A probe π is a formula of the form (cid:3)γψ,
where γ ∈ Γ is called the probe credential set and ψ is a
(cid:3)-free formula from Φ called the probe query.
An observation of a probe π under a policy γ0 is either π
if γ0 (cid:2) π, and otherwise ¬π.
A probing attack on γ0 consisting of probes {π1, ..., πn} is
the conjunction of the observations of πi ∈ {π1, ..., πn} under
γ0.

Clearly, by the above deﬁnition, if ϕ is a probing attack on
γ0, then γ0 (cid:2) ϕ. But there may be other policies γ that also
have the property that ϕ holds in them. In the absence of other
additional knowledge, the attacker cannot distinguish between
γ0 and any such γ. To put it positively, the attacker learns from
the probing attack ϕ precisely that γ0 is in the equivalence
class of policies in which ϕ holds. We denote this equivalence
class induced by probing attack ϕ by |ϕ| = {γ | γ (cid:2) ϕ}.
Now if in all these policies, some property ϕ(cid:2) holds, then
the attacker knows with absolute certainty that ϕ(cid:2) holds in
γ0 in particular, in which case we say that ϕ(cid:2) is detectable.
Conversely, if there exists some policy within |ϕ| in which ϕ(cid:2)
does not hold, the attacker cannot be certain that ϕ(cid:2) holds in
γ0, in which case we say that ϕ(cid:2) is opaque.
Deﬁnition III.6 (Detectability, opacity). A formula ϕ(cid:2) ∈ Φ is
detectable in a probing attack ϕ on a policy γ0 iff

∀γ ∈ |ϕ|. γ (cid:2) ϕ(cid:2).

A formula ϕ(cid:2) is opaque in a probing attack ϕ iff it is not
detectable in ϕ, or equivalently,

∃γ ∈ |ϕ|. γ (cid:11)(cid:2) ϕ(cid:2).

Theorem III.7 (Probing attacks). A formula ϕ(cid:2) is detectable
in a probing attack ϕ iff (cid:2) ϕ → ϕ(cid:2).

This theorem again underlines the importance of being able

to reason about universal truths.

IV. SEMANTICS

The model-theoretic semantics we are looking for has to

satisfy four requirements:

1) Capturing trust management: given ϕ and the semantics

of γ, it is possible to check if γ (cid:2) ϕ.
theory) iff (cid:2) ϕ.

2) Supporting a notion of validity: ϕ is valid (in the model
3) Full abstraction [43]: two policies are equivalent (≡) iff
4) Compositionality: the semantics of γ1 ∪ γ2 can be com-

their respective semantics are equal.

puted from the individual semantics of γ1 and γ2.

A. Na¨ıve Approaches

We ﬁrst consider some simple approaches to developing a
formal semantics that may immediately come to mind, and
show why they fail.

The standard model-theoretic interpretation of a set of
Datalog clauses is its minimal Herbrand model, i.e., the set
of atoms that hold in it. But in this approach, the policy
γ0 from Example II.1 would have the same semantics as
the empty policy ∅, namely the empty model, even though
the two policies are clearly not equivalent (Def. II.2). Hence
such a semantics would not be fully abstract. This semantics
is not compositional either: from the semantics of {p : – q}
(which is again empty) and of {q}, we cannot construct the
semantics of their union. Therefore, this semantics is clearly
unsuitable in a trust management context, where it is common
to temporarily extend the clause set with a set of credentials.
In fact, this semantics fails on all four accounts regarding our
requirements.
to interpret a Datalog clause
p : – p1, ..., pn as an implication p1 ∧ ... ∧ pn → p in classical
(or intuitionistic) logic, and a policy γ as the conjunction
of its clauses: [[γ]] =
c∈γ[[c]]. As shown by Gaifman and
Shapiro ([27]), this semantics would indeed be both compo-
sitional and fully abstract. However, this interpretation does
not correctly capture the trust management relation (cid:2), as
we show now. First of all, we would need to translate (cid:3)-
formulas into logic. The obvious way of doing this would be
to interpret (cid:3)γϕ as the implication [[γ]] → [[ϕ]]. Then, for
instance, we have {p : – q} (cid:2) (cid:3)qp, and correspondingly also
[[{p : – q}]] |= [[(cid:3)qp]], since [[{p : – q}]] = [[(cid:3)qp]] = q → p. Thus
we might be led to conjecture

We could also attempt

(cid:2)

γ (cid:2) ϕ ?⇐⇒ [[γ]] |= [[ϕ]].

Unfortunately, this correspondence does not hold in general.
Consider the formula ϕ = ¬q ∧ (cid:3)qp. From this we can

165

conclude that [[ϕ]] = ¬q ∧ (q → p). But {p : – q} (cid:2) ϕ,
whereas [[{p : – q}]]
(cid:11)|= [[ϕ]]. We could try to ﬁx this by
only considering the minimal model of the semantics, since
minMod([[{p : – q}]]) |= ¬q. But we can break this again:
∅ (cid:11)(cid:2) ϕ, whereas minMod([[∅]]) |= [[ϕ]].
B. A counterfactual Kripke semantics

The crucial observation that leads to an adequate semantics
is that both Datalog clauses and the trust management speciﬁc
(cid:3)-actions are counterfactual, rather than implicational,
in
nature. For instance, p : – (cid:3)p can be interpreted as the coun-
terfactual “if (cid:3)p were added to the policy, then p would hold”.
Similarly, (cid:3)γϕ can be read as “if γ were added to the policy,
then ϕ would hold”. (Note that the counterfactual conditional
“if A were true then B would hold” is strictly stronger than
the material implication “A → B”, which vacuously holds
whenever A is not true.)
Therefore, we can unify the notations and write (cid:3)(cid:3)pp instead
of p : – (cid:3)p. Moreover, instead of writing a policy γ as a set,
we can just as well write it as a conjunction of clauses. We
can thus rewrite the syntax for policies and formulas from
Section II in the following, equivalent, form:

Policies γ ::=(cid:10) | p | (cid:3)(cid:2)
(cid:3)pp | γ ∧ γ
Formulas ϕ ::=γ | ¬ϕ | ϕ ∧ ϕ | (cid:3)γϕ

As before, we write Γ and Φ to denote the set of all policies
and formulas, respectively. The relation (cid:2) is also deﬁned as
before, with the obvious adaptations to the new syntax.
Notation IV.1. Henceforth, we treat (cid:3)ϕ as syntactic sugar for
(cid:2)

(cid:3)ϕ, and p : – (cid:3)p for (cid:3)(cid:3)pp.
Interpreting (cid:3)-formulas as counterfactuals, we can now give
it a multi-modal Kripke semantics in the spirit of Lewis and
Stalnaker [36], [49]: the counterfactual (cid:3)γϕ holds in a possible
world w if in those γ-satisfying worlds w(cid:2) that are closest to
w, ϕ holds. We will express the closeness relation using a
ternary accessibility relation R, and later apply rather strong
conditions on R in order to make it match the intended trust
management context.
Deﬁnition IV.2 (Model, entailment). A model M is a triple
(cid:19)W, R, V (cid:20), where W is a set, R ⊆ ℘(W ) × W × W , and
V : At → ℘(W ).
Given a model M, we inductively deﬁne the model-theoretic
entailment relation (cid:4)M ⊆ W × Φ as follows. For all w ∈ W :

w (cid:4)M (cid:10)
w (cid:4)M p iff w ∈ V (p)
w (cid:4)M ¬ϕ iff w (cid:2)M ϕ
w (cid:4)M ϕ1 ∧ ϕ2 iff w (cid:4)M ϕ1 and w (cid:4)M ϕ2
w (cid:4)M (cid:3)γϕ iff ∀w(cid:2). R|γ|M

(w, w(cid:2)) ⇒ w(cid:2) (cid:4)M ϕ,

where |γ|M = {w ∈ W | w (cid:4)M γ}. Similarly, we write |w|M
to denote the set {γ ∈ Γ | w (cid:4)M γ}.

Intuitively, a world w ∈ W corresponds to a policy; more
precisely, to the (cid:15)-maximal policy in |w|M . Vice versa, a

policy γ corresponds to a world, namely the (cid:15)M -minimal
world in |γ|M , where (cid:15)M is an ordering on worlds that reﬂects
the containment relation (cid:15) on policies (Def. IV.3). (Actually,
in Def. IV.4, we associate γ simply with the entire cone |γ|M .)
Deﬁnition IV.3 (World containment). Given a model M =
(cid:19)W, R, V (cid:20) and x, y ∈ W ,

x (cid:15)M y iff ∀γ ∈ Γ :x (cid:4)M γ implies y (cid:4)M γ.

Deﬁnition IV.4 (Semantics). The semantics of γ (with respect
to M) is |γ|M .

As it is, this deﬁnition keeps the meaning of R completely
the semantics is

abstract, but we can already prove that
compositional, irrespective of R:
Theorem IV.5 (Compositionality). For all models M, and
γ1, γ2 ∈ Γ:

|γ1 ∧ γ2|M = |γ1|M ∩ |γ2|M .

In order to satisfy the remaining three requirements from
Section II, we have to put some restrictions on the models, and
in particular on the accessibility relation R. We call models
that satisfy these constraints TM models (Def. IV.7). Intuitively,
(w, w(cid:2)) should hold if w(cid:2) is a world that is closest to
R|γ|M
w of those worlds in which γ holds. But what do we mean
by ‘closest’? If we interpret worlds as policies, then w(cid:2) is the
policy that results from adding γ, and nothing more but γ, to
w. So we have to consider all worlds that are larger than w
(since we are adding to w) and also satisfy γ, and of these
worlds we take the (cid:15)M -minimal ones (since we are adding
nothing more but γ) (Def. IV.7(1)).

The other two constraints (Def. IV.7(2) and IV.7(3)) ensure
that there is a one-to-one correspondence between policies and
worlds.
Deﬁnition IV.6. If (X,≤) is a pre-ordered set (≤ is a reﬂexive
transitive relation on X) and Y a ﬁnite subset of X, then
min≤(Y ) = {y ∈ Y | ∀y(cid:2) ∈ Y : y(cid:2) (cid:11)< y}, and max≤(Y ) =
{y ∈ Y | ∀y(cid:2) ∈ Y : y(cid:2) (cid:11)> y}.
Deﬁnition IV.7 (Trust management model). A model M =
(cid:19)W, R, V (cid:20) is a TM model iff
1) ∀γ ∈ Γ, x, y ∈ W.
2) ∀γ ∈ Γ, ∃w ∈ W. γ ∈ max(cid:5)|w|M , and
3) ∀w ∈ W, ∃γ ∈ Γ. γ ∈ max(cid:5)|w|M .
To gain a better intuition for TM models, it is useful to
consider the following, particular TM model: imagine a labeled
directed graph with a vertex for each γ ∈ Γ (these are the
worlds W ). There is an edge from γ1 to γ2, labeled with
γ, whenever γ2 = γ1 ∪ γ (corresponding to the accessibility
relation R|γ|).

{w | w (cid:23)M x ∧ w ∈ |γ|M},

(x, y) iff y ∈ min(cid:5)M

R|γ|M

So a TM model models all possible policies and all possible
trust management interactions (submitting a set of credentials
γ for the duration of a query) with these policies. The
following theorem shows that TM models indeed precisely
capture the trust management relation (cid:2), and Theorem IV.9
states that the semantics is fully abstract.

Theorem IV.8 (Capturing trust management). Let M =
(cid:19)W, R, V (cid:20) be a TM model, γ ∈ Γ and ϕ ∈ Φ.

γ (cid:2) ϕ iff ∀w ∈ min(cid:5)M

|γ|M . w (cid:4)M ϕ

Theorem IV.9 (Full abstraction). For all TM models M, and
γ1, γ2 ∈ Γ:

γ1 ≡ γ2 iff |γ1|M = |γ2|M .

The property that is hardest to satisfy (and to prove) is the
requirement that the model theory should support a notion
of validity that coincides with judgements of the form (cid:2) ϕ,
i.e., universal truths about trust management policies. This is
formalized in Theorem IV.11.
Deﬁnition IV.10 (Trust management validity). ϕ is TM-valid
(we write (cid:4)TM ϕ) iff for all TM models M = (cid:19)W, R, V (cid:20) and
w ∈ W : w (cid:4)M ϕ.
Theorem IV.11 (Supporting validity).
(cid:4)TM ϕ iff (cid:2) ϕ.

Example IV.12. Consider the following (false) statement: “in
all policies in which p → q holds, (cid:3)pq also holds.” By the
contrapositive of Theorem IV.11, we can prove that this is not
true, i.e., (cid:11)(cid:2) (p → q) → (cid:3)pq, by identifying a counter-world
w in a TM model M such that w (cid:4)M (p → q) ∧ ¬(cid:3)pq. By
Def. IV.2, this is equivalent to

w (cid:4)M ¬p ∧ ¬(cid:3)pq or w (cid:4)M q ∧ ¬(cid:3)pq.

Let w be a (cid:15)M -minimal world in all of W . By minimality,
w (cid:4)M γ only if γ is universally true. Neither p nor (cid:3)pq
(assuming p (cid:11)= q) are universally true, hence w (cid:4)M ¬p and
w (cid:4)M ¬(cid:3)pq, as required.

In this section, we developed an adequate model-theoretic
semantics for trust management. We started by interpreting
both Datalog clauses and trust management interactions as
counterfactuals, and taking a generic counterfactual model
theory as the basis. We then customized the theory by adding
constraints on the models of interest to arrive at TM models.
The resulting semantics satisﬁes all four requirements from
Section II, and it provides an intuition of a trust management
service as a vertex in a labeled directed graph, where the
reachable vertices represent
the clause sets resulting from
combining the service’s policy with the submitted credential
set (the edge label) to the service.
However, this semantics still does not give us much insight
into proving judgements of the form (cid:2) ϕ (or, equivalently,
(cid:4)TM ϕ). For this purpose, we equip the model theory with a
corresponding proof theory in the following section.

V. AXIOMATIZATION

In standard modal logic, it is usually straightforward to
derive an axiom in the proof theory from each frame condition
in the model theory, i.e., a restriction on the accessibility
relation R. (For example, reﬂexivity of R corresponds to the

166

axiom (cid:3)ϕ → ϕ.) This constructive method can also be applied
to counterfactual multi-modal logic, if the frame conditions are
relatively simple [48]. In our case, however, the restriction on
R (Def. IV.7(1)) is too complex to be simply ‘translated’ into
an axiom. The axiomatization presented below was actually
conceived by guessing the axioms and rules, and adjusting
them until the system was provably sound and complete with
respect to the model theory.
Deﬁnition V.1. In the proof system below, let ϕ, ϕ(cid:2), ϕ(cid:2)(cid:2) ∈ Φ,
γ, γ(cid:2) ∈ Γ, p ∈ At and (cid:3)p ⊆ At. The proof system consists of
the following axiom schemas:

(cid:24) ϕ → ϕ(cid:2) → ϕ
(cid:24) (ϕ → ϕ(cid:2) → ϕ(cid:2)(cid:2)) → (ϕ → ϕ(cid:2)) → ϕ → ϕ(cid:2)(cid:2)
(cid:24) (¬ϕ → ¬ϕ(cid:2)) → ϕ(cid:2) → ϕ
(cid:24) (cid:3)γ(ϕ → ϕ(cid:2)) → (cid:3)γϕ → (cid:3)γϕ(cid:2)
(cid:24) (cid:3)γγ
(cid:24) (cid:3)γϕ → γ → ϕ
(cid:24) (cid:3)(p :– (cid:3)p) ϕ → ((cid:3)p → p) → ϕ

provided ϕ is (cid:3)-free

(cid:24) (cid:3)γ¬ϕ ←→ ¬(cid:3)γϕ
(cid:24) (cid:3)γ∧γ(cid:2) ϕ ←→ (cid:3)γ(cid:3)γ(cid:2) ϕ

Additionally, there are three proof rules:

then (cid:24) ϕ(cid:2).

If (cid:24) ϕ and (cid:24) ϕ → ϕ(cid:2)
If (cid:24) ϕ then (cid:24) (cid:3)γϕ.
If (cid:24) γ → γ(cid:2)

and ϕ is ¬-free
then (cid:24) (cid:3)γ(cid:2) ϕ → (cid:3)γϕ

(Cl1)
(Cl2)
(Cl3)
(K)
(C1)
(C2)
(Dlog)

(Fun)
(Perm)

(MP)
(N)
(Mon)

Axioms (Cl1)–(Cl3) and Modus Ponens (MP) are from the
Hilbert-style axiomatization of classical propositional logic
[47]. It is easy to see that they are sound, irrespective of
R, since the Boolean operators (cid:10), ∧ and ¬ are deﬁned
classically for (cid:4)M . Axiom (K) is the multi-modal version of
the basic Distribution Axiom that is part of every modal logic
((cid:3)(ϕ → ϕ(cid:2)) → (cid:3)ϕ → (cid:3)ϕ(cid:2)). Similarly, Rule (N) is the multi-
modal version of the basic Necessitation Rule (if (cid:24) ϕ then
(cid:24) (cid:3)ϕ).

Axioms (C1) and (C2) are also standard in counterfactual
if γ
logic [48]. The former is the trivial statement
were the case, then γ would hold. The latter axiom states
that the counterfactual conditional is stronger than material
implication.

that

At ﬁrst sight, Axiom (Dlog) may look similar to Ax-
iom (C2), but the two are actually mutually independent. In
fact, while the latter is standard, Axiom (Dlog) is deeply
linked with the intuition that the possible worlds correspond
to Datalog policies. Recall that, intuitively, the left hand side
means “ϕ would hold in the policy if the credential p : – (cid:3)p
were submitted”. Now we expand the right hand side of the
implication to

((cid:3)p ∧ ¬p) ∨ ϕ.

167

So the axiom tells us that the left hand side holds only if it is
the case that

• either ϕ holds in the policy anyway, even without sub-

mitting p : – (cid:3)p,

• or the action of submitting the credential must be crucial
for making ϕ true, but
this is only possible if the
conditions (cid:3)p of the credential are all satisﬁed in the policy,
and furthermore p does not already hold in the policy (or
else the credential could not possibly be crucial).

But the axiom only holds for (cid:3)-free ϕ. To see why, consider
the following instance of Axiom (Dlog), ignoring the side
condition: (cid:3)q :– p(cid:3)pq → (p → q) → (cid:3)pq. The left hand side
is an instance of Axiom (C1), since q : – p is just syntactic
sugar for (cid:3)pq, so the formula simpliﬁes to (p → q) → (cid:3)pq,
which is not TM valid, as shown in Example IV.12.

The following lemma is a useful bidirectional variant of

Ax. (Dlog):
Lemma V.2. Let p, q ∈ At and (cid:3)p ⊆ At.

(cid:24) (cid:3)(p :– (cid:3)p)q ←→ q ∨ (¬p ∧ (cid:3)p ∧ (cid:3)pq),

Axiom (Fun) is also remarkable in that it is rather non-
standard in modal logic. It is also the reason it is not useful to
deﬁne a dual ♦-operator (i.e., ♦γϕ = ¬(cid:3)γ¬ϕ) in our logic,
since (cid:3) and ♦ would be equivalent. The axiom is equivalent
to the property that the accessibility relation R in a TM model
M = (cid:19)W, R, V (cid:20) is essentially functional, i.e., for all w ∈ W ,
and γ ∈ Γ:
• ∃w(cid:2). R|γ|M
• ∀w1, w2. R|γ|M

(w, w(cid:2)), and
w1 (cid:15)M w2 ∧ w2 (cid:15)M w1.

(w, w1) ∧ R|γ|M

(w, w2) ⇒

On the intuitive Datalog level, Axiom (Fun) can easily be
seen to be sound, since the statement “ϕ would not hold if
γ were submitted” is equivalent to “it is not the case that ϕ
would hold if γ were submitted”.

Axiom (Perm) also corresponds to a property of R, namely
that it is transitive (that’s the ‘if’ direction) and dense (the
‘only if’ direction). It captures the intuition that submitting
two credential sets in sequence is equivalent to just submitting
their union.
Rule (Mon) expresses a monotonicity property on the sub-
scripts of (cid:3), and can be reduced to a monotonicity property
of TM models and ¬-free ϕ:

∀w, w(cid:2) ∈ W. w (cid:4)M ϕ ∧ w(cid:2) (cid:23)M w ⇒ w(cid:2) (cid:4)M ϕ.

The intuition here is that submitting more or stronger creden-
tials can only make more (positive) facts true. It is easy to
see that this does not hold in general for negated statements:
suppose p does not hold in a policy (with no submitted
credentials); then the negated fact ¬p holds. But ¬p may cease
to hold when credentials are submitted, in particular, when p
is submitted. In other words, even though p → (cid:10) is valid,
(cid:3)(cid:7)¬p → (cid:3)p¬p is not.

The main result of this section is that the axiomatization
theory

to the model

is sound and complete with respect
(Theorem V.3).

Theorem V.3 (Soundness and Completeness).

(cid:4)TM ϕ iff (cid:24) ϕ

The proof of soundness ((cid:24) ϕ implies (cid:4)TM ϕ) formalizes the
intuitions given above and proceeds, as usual, by structural
induction on ϕ. The proof of completeness ((cid:4)TM ϕ implies
(cid:24) ϕ) is less standard, and can be roughly outlined thus:
1) We will prove the equivalent statement that if ϕ consistent
(with respect to (cid:24)), then there exists a TM model M =
(cid:19)W, R, V (cid:20) and w ∈ W such that w (cid:4)M ϕ.
2) From Lemma V.2, it can be shown that every ϕ is equiva-
lent to a formula ϕ(cid:2) that only consists of conjunctions and
negations of policies in Γ (i.e., one that does not contain
vertically nested boxes).

3) Based on the property of TM models that every world
corresponds to some policy in Γ, it is then possible to
identify w ∈ W such that w (cid:4)M ϕ(cid:2), whenever M is a
TM model.
4) By soundness, this implies that w (cid:4)M ϕ. Furthermore,
we can show that at least one TM model exists, and hence
we arrive at the required existential conclusion.
Together with Theorem IV.11, we have the result

(cid:2) ϕ ⇐⇒ (cid:4)TM ϕ ⇐⇒ (cid:24) ϕ.

We can thus use the axiomatization to prove universal truths
about trust management systems.
Example V.4. We sketch a formal proof of the formula from
Example III.4.

(cid:24) ¬a ∧ (cid:3)d¬e ∧ (cid:3)b :– a∧d :– ce → c ∧ (cid:3)da

Proof: We ﬁrst show that d is equivalent to (cid:3)(cid:7)d. The
direction (cid:24) (cid:3)(cid:7)d → d follows directly from Axiom (C2). The
same axiom also yields (cid:24) (cid:3)(cid:7)¬d → ¬d, the contrapositive of
which is (cid:24) d → (cid:3)(cid:7)d, together with Axiom (Fun). Therefore
(cid:24) d ←→ (cid:3)(cid:7)d.
Since (cid:24) c → (cid:10), we have(cid:24) (cid:3)(cid:7)d → (cid:3)cd, according to
Rule (Mon), and hence equivalently (cid:24) d → d : – c. Taking this
as the premise of Rule (Mon), we get (cid:24) (cid:3)d :– ce → (cid:3)de,
the contrapositive of which is (cid:24) (cid:3)d¬e → (cid:3)d :– c¬e, by
Axiom (Fun).
Therefore, the assumption (cid:3)d¬e from the antecedent of the
formula implies (cid:3)d :– c¬e. Conjoining this with the assump-
tion (cid:3)b :– a∧d :– ce, which is equivalent to (cid:3)d :– c(cid:3)b :– ae, by
Axiom (Perm), we get

(cid:3)d :– c(¬e ∧ (cid:3)b :– ae)

(as it can be easily shown that (cid:3)d :– c distributes over ∧).

By Axiom (Dlog), (cid:24) (cid:3)b :– ae → e ∨ (a ∧ ¬b). Therefore,

formula (7) implies

(cid:3)d :– c(a ∧ ¬b),

(8)

since Axiom (K) allows us to apply Modus Ponens under
(cid:3)d :– c. We have thus shown that the antecedent of the original
formula implies (cid:3)d :– ca. Furthermore, as we have shown,

168

(cid:24) d → d : – c, and hence by Rule (Mon), (cid:24) (cid:3)d :– ca → (cid:3)da.
Modus Ponens yields one of the consequents of the original
formula, (cid:3)da.
For the other consequent, c, we apply Axiom (Dlog) to
formula (8), which yields (a ∧ ¬b) ∨ (c ∧ ¬d). Combining
this with the antecedent ¬a, we can then conclude c.
Example V.5. We sketch a formal proof of the probing attack
result from Section III-A. For brevity, we introduce abbreviated
names for the atoms:

as = A.hasConsented (S)
sa = S.canRegister (A)
ab = A.isRegistered (B)

secret = S.isRegistered (B)

The statement that the attacker can detect secret in the probing
attack can then be expressed as
(cid:24) (cid:3)assa ∧ (cid:3)as :– ab¬sa ∧ (cid:3)as :– ab∧ab :– secretsa → secret.
Proof: Assume the left hand side of the formula that we
want to prove. From the previous proof, we have seen that sa is
equivalent to (cid:3)(cid:7)sa. Since as : – ab → (cid:10), we thus have sa →
(cid:3)as :– absa, by Rule (Mon). Combining the contrapositive of
this with the assumption, we get ¬sa. From the assumption
and Ax. (C2), we get as → sa, which together with ¬sa gives
¬as.
Using Lemma (V.2), we can prove that (cid:3)as :– absa is equiv-
alent to sa ∨ (ab ∧ ¬as ∧ (cid:3)assa).
to
¬(cid:3)as :– absa (by Ax. (Fun)), it is therefore also equivalent to

assumption (cid:3)as :– ab¬sa is

equivalent

Since

the

¬sa ∧ (¬ab ∨ as ∨ ¬(cid:3)assa).

We have already proved ¬as, and (cid:3)assa is in the antecedent.
Therefore, we can conclude ¬ab.
Now consider (cid:3)as :– ab¬sa ∧ (cid:3)as :– ab∧ab :– secretsa in the
assumption. By Ax. (Fun) and (Perm) and distributivity of
(cid:3), this is equivalent to (cid:3)as :– ab(¬sa ∧ (cid:3)ab :– secretsa). By
Ax. (K), we can apply Ax. (Dlog) on the inner box under the
outer box to get

(cid:3)as :– ab(¬sa ∧ (sa ∨ (secret ∧ ¬ab))),

which implies (cid:3)as :– absecret. Again applying Ax. (Dlog)
yields secret ∨ (ab ∧ ¬as). But since we have proved ¬ab
above, we can conclude that secret follows from the assump-
tions.

(7)

VI. MECHANIZING THE LOGIC

Hilbert-style axiomatizations are notoriously difﬁcult to use
directly for building proofs, and they are also difﬁcult
to
mechanize directly, because they are not goal-directed. In this
section, we describe how a goal formula ϕ can be transformed
into an equivalent formula in classical propositional
logic
that can be veriﬁed by a standard SAT solver. We have
implemented a tool based on the contents of this section; some
uses of the tool are described in Section VII.

Our axiomatization has certain characteristics that enables
such a transformation. Firstly, Lemma V.2 shows that ϕ can
be transformed into a formula in which all subscripts of boxes
are (cid:3)-free, and Ax. (Fun) and (Perm) allow us to distribute
boxes through conjunctions, disjunctions and negations. This
forms the basis of a normalization transform.

Secondly, for a given ϕ, it is sufﬁcient to encode just a ﬁnite
number of axiom instantiations in classical propositional logic
in order to characterize the non-classical properties of (cid:3). This
process is called saturation.
In this section, we use literal to mean a (possibly negated)
atom, and (cid:3)-literal to mean a (possibly negated) atom with
some preﬁx of boxes, e.g. (cid:3)(cid:2)rp∧pq and p are both (cid:3)-literals
(p is logically equivalent to (cid:3)(cid:7)p), whereas p is also a literal
but (cid:3)qp is not.

The reasoning process is described in more detail next.

Normalization and expansion. Following parsing, the goal
formula is simpliﬁed through the elimination of subsumed
subformulas; e.g., (cid:3)a :– bc ∧ (cid:3)ac is simpliﬁed to (cid:3)a :– bc. The
formula is then normalized by computing a negation normal
form and distributing all boxes, such that boxes are only
applied to literals, and negation is only applied to (cid:3)-literals.
We also use Ax. (Perm) to collect strings of boxes into a single
box. Normalization takes care of Ax. (K), (Fun), and (Perm).
Next, the goal formula is expanded by applying Lemma V.2
exhaustively until all subscripts of (cid:3)-literals are (cid:3)-free. Ex-
pansion is a very productive process – it can cause the goal
formula’s size to increase exponentially. This step takes care
of Ax. (Dlog) and Rule (N).

The resulting formula is negated and added to the clause
set. The clause set collects formulas which will ultimately be
passed to a SAT solver.
Saturation. Saturation generates propositional formulas that
faithfully characterize the (cid:3)-literals occurring in the clause set.
1) Let β = (cid:3)(cid:2)n
i=1(qi:− (cid:3)qi)p be a (cid:3)-literal occurring in the
clause set. If (cid:24) (cid:2)n
i=1((cid:3)qi → qi) → p holds (which is
checked by the underlying SAT solver), we replace all
occurrences of β by (cid:10). This step is a generalization of
Ax. (C1).
2) For each (cid:3)-literal (cid:3)γp (where γ (cid:11)= (cid:10)) occurring in the
clause set, we add the formulas p → (cid:3)γp and (cid:3)γp →
γ → p.
3) For each pair of (cid:3)-literals (cid:3)γ1 p, (cid:3)γ2p (where γ1 (cid:11)= γ2)

occurring in the clause set, we add the formula

(cid:3)γ1 γ2 ∧ (cid:3)γ2 p −→ (cid:3)γ1p.

Intuitively, this formula encodes the transitivity of coun-
terfactuals. Steps (2) and (3) together cover Ax. (C2) and
Rule (Mon).

Since the second step may create new (cid:3)-literals, the process
is repeated until a ﬁxed point is reached.
Propositionalization and SAT solving. After saturation com-
pletes, all (cid:3)-literals in the clause set are uniformly substituted
by fresh propositional literals. The resulting formulas are then

Figure 1. Comparison of timings for the TC3-based test series on a double
logarithmic scale. BK is the tool from [7], Cd is our tool Counterdog.

checked by a standard SAT solver. Our implementation offers
the choice between using the in-memory API of Z32 [22] and
producing output in the DIMACS [24] format used by many
SAT solvers such as MiniSAT [25].

The classical axioms (Cl1)–(Cl3) and Rule (MP) are covered
by the SAT solver. We have therefore covered all axioms and
rules, and thus the goal formula is valid iff the SAT solver
reports unsatisﬁability (since we negated the goal).

VII. APPLICATIONS AND PERFORMANCE

A. Probing attacks

As an example of how the axiomatization can be used
for security analysis, and to compare the performance of
our implementation, we conducted a small case study on
analyzing probing attacks, based on the benchmark test cases
described by Becker and Koleini [8], [7]. Their benchmark
was set up to test the performance of their tool (henceforth
referred to as BK) for verifying opacity and detectability
in probing attacks. BK’s algorithm attempts to construct a
policy that
to all probes but
makes the fact to be detected false. The fact is opaque if BK
manages to construct such a policy, and detectable otherwise.
In contrast, Counterdog is a general theorem prover for our
logic. By Theorems III.7, IV.11, and V.3, Counterdog can
be used to check opacity and detectability by constructing a
formula corresponding to a probing attack and then proving it
mechanically.

is observationally equivalent

To keep this paper self-contained, we brieﬂy describe the
tested scenarios, and refer the reader to [7] for a more detailed
explanation.

The compute cluster Clstr under attack has the following

policy γClstr:
canExe(Clstr, x, j) : –

mem(Clstr, x), owns(Clstr, x, j), canRd (Data, Clstr, j).

owns(Clstr, x, j) : – owns(y, x, j), isTTP (Clstr, y).
mem(Clstr, x, j) : – mem(y, x, j), isTTP (Clstr, y).
canRd (Data, x, j) : – canRd (y, x, j), owns(Data, y, j).
owns(Data, x, j) : – owns(y, x, j), isTTP (Data, y).
isTTP (Clstr, CA).
isTTP (Data, CA).

Here, x and y range over a set of users, and j ranges
over a set of compute job identiﬁers. The ﬁrst parameter
of each predicate should be interpreted as the principal who

2Z3 is an SMT solver, but we only use its SAT solving capabilities.

169

says, or vouches for, the predicate. The policy stipulates that,
according to Clstr, members who own a job can execute it,
if Clstr can read the data associated with it according to data
center Data. Clstr delegates authority over job ownership
and membership to trusted third parties (TTP). Data delegates
authority over read permissions to job data to data owners.
Data also delegates authority over job data ownership to
TTPs. Furthermore, both Clstr and Data say that certiﬁcate
authority CA is a TTP.
TC1. In the basic test case (TC1), the attacker Eve possesses
four credentials γEve:

owns(CA, Eve, Job).
mem(CA, Eve).
canRd (Eve, Clstr, Job).
canRd (Eve, Clstr, Job) : – mem(Clstr, Bob).

Clstr exports only one query to Eve:

ϕEve = canExe(Clstr, Eve, Job).

With her four credentials and the query, Eve can form
24 = 16 probes (cf. Def. III.5) of the form (cid:3)γϕEve, for each
γ ⊆ γEve. These result in 16 observations under γClstr: the
observation corresponding to probe π is just π if γClstr (cid:2) π,
and otherwise it is ¬π. The resulting probing attack ϕa under
γClstr is then the conjunction of all 16 observations.
In TC1, Eve wishes to ﬁnd out if Bob is not a member of
Clstr – in other words, if ¬mem(Clstr, Bob) is detectable.
By Theorems III.7, IV.11, and V.3,
to
checking

this is equivalent

(cid:24) ϕa → ¬mem(Clstr, Bob).

This is provable, and therefore Eve can detect that Bob is not
a member.
TC2. The atomic clause mem(Clstr, Bob) is added to γClstr,
and the fact to be detected is changed to mem(Clstr, Bob).
The corresponding formula is not provable, and hence
mem(Clstr, Bob) is opaque.
TC3. Based on TC1, three irrelevant atomic clauses p1, p2,
p3 are added to γEve, increasing the number of probes to 27 =
128. The fact to be detected remains the same, and is indeed
detectable.
TC4. This test case was omitted as it only tests a speciﬁc
switch in Becker and Koleini’s tool which is not relevant in
our case.
TC5. Based on TC1, the probe query is changed to ϕEve =
canExe(Clstr, Eve, Job)∧¬isBanned (Clstr, Eve). The fact
remains detectable.
TC6. Based on TC5, the probe set is manually pruned to
a minimal set that is sufﬁcient to prove detectability. This
reduces the number of probes from 16 down to only 3.

To get comparable performance numbers, we ran Becker and
Koleini’s probing attack analyzing tool (henceforth referred
to as BK) and Counterdog on these test cases. For our
experiments we used an Intel Xeon E5630 2.53 GHz with

170

6 GB RAM. The table below summarizes the timings for all
test cases, comparing BK with Counterdog.

Test Case

1
2
3
5
6

# Probes

16
16
128
16
3

BK
28
28
218
3954
16

Timing (ms)

Counterdog

10
10
44
119
13

Counterdog outperforms BK in all test cases. The perfor-
mance gain is most notable in the more expensive test cases.
To test if this is generally the case, we performed a test series,
based on TC3, adding an extra irrelevant clause to the probe
credential set γEve one by one. This doubles the number of
probes (and thus the size of the formula to be proved) at each
step.

Figure 1 compares the performance of both tools for this
test series. Counterdog’s performance gain over BK increases
exponentially with each added credential in γEve. A probe
credential of size 14 (resulting in 16,384 probes) was the
maximum that BK could handle before running out of memory,
taking 408 s (compared to 7 s with Counterdog). We tested
Counterdog with up to 18 credentials (resulting in 262,144
probes), which took 179 s. A simple extrapolation suggests
that Counterdog can check a probing attack based on TC3
extended to 108 probes within less than three hours.

B. Proving Meta-Theorems

As we have seen,

the axiomatization of the semantics
together with our implementation enables us to mechanically
prove universal truths about trust management systems – that
is, statements that are implicitly quantiﬁed over all policies: a
theorem (cid:24) ϕ is equivalent to (cid:2) ϕ, by Theorems IV.11 and V.3,
which can be interpreted as “all policies γ satisfy the property
ϕ”.

But we want to go further than that. In this subsection, we
show that we can use our implementation to automate proofs of
meta-theorems about trust management. These are statements
containing universally quantiﬁed meta-variables ranging over
atoms, conjunctions of atoms, Γ or Φ. Our axiom schemas
and Lemma V.2 are examples of such meta-theorems, with
meta-variables p, (cid:3)p, γ, ϕ etc.

In classical logic as well as all normal modal logics, proving
such meta-theorems is trivial: if a propositional formula f is
a theorem, then substituting any arbitrary formula f(cid:2) for all
occurrences of an atom p in f will also yield a theorem. In
fact, the axiomatization of such logics often explicitly include
a uniform substitution rule, and a ﬁnite number of axioms
(rather than axiom schemas, as in our case).

Our logic breaks the uniform substitution property, as
some of the axioms and rules have syntactic side conditions
(e.g. Ax. (Dlog), Rule (Mon)). It is thus not a normal modal
logic in the strict sense, but this does not pose any problems,
and is perhaps even to be expected, as many belief-revision
and other non-monotonic logics also break uniform substitu-
tion [41].

Φ-hole contexts
Γ-hole contexts
At-hole contexts

C ::= [·] | (cid:10) |p | ¬C |(cid:3) γC | C ∧ C
D ::= [·] | (cid:10) |p | ¬D | (cid:3)γD | (cid:3)[·]D | D ∧ D
E ::= [·] | (cid:10) |p | ¬E |(cid:3) γE | (cid:3)[·]E | (cid:3)p :–[·]E | E ∧ E

Figure 2. Φ-contexts with Φ-holes, Γ-holes and At-holes, respectively. A At-hole context takes as argument an atom or a conjunction of atoms.

The only downside is that proving meta-theorems is non-
trivial, and manual proofs generally require structural induc-
tion over the quantiﬁed meta-variables. It
is therefore not
obvious if proving meta-theorems can be automated easily.
After all, the range of the quantiﬁers is huge, and even inﬁnite
if At is inﬁnite. We answer this question in the afﬁrmative
by presenting a number of proof-theoretical
theorems on
the provability of meta-theorems (meta-meta theorems, so to
speak), that show that it is sufﬁcient to just consider a small
number of base case instantiations of meta-variables.
We will use contexts to formalize the notion of meta-
theorem. A context is a Φ-formula with a ‘hole’ denoted by [·].
We deﬁne three different kinds of contexts in Fig. 2, Φ-hole,
Γ-hole, and At-hole contexts. Intuitively, the holes in a Φ-hole
(Γ-hole, At-hole, respectively) context can be ﬁlled with any
ϕ ∈ Φ (γ ∈ Γ, (cid:3)p ⊆ﬁn At) to form a well-formed Φ-formula.
If A is a Φ-hole (Γ-hole, At-hole, respectively) context and
α ∈ Φ (α ∈ Γ, α ⊆ﬁn At), we write A[α] to denote the
Φ-formula resulting from replacing all holes in A by α.

It is easy to see that every Φ-hole context is also a Γ-hole
context, and every Γ-hole context is also a At-hole context.
Each of the three types of contexts completely cover all of
Φ; in particular, the case (cid:3)γ∧[·]A (for Γ-hole and At-hole
contexts) is covered because (cid:3)γ∧[·]A is equivalent to (cid:3)γ(cid:3)[·]A.
Theorem VII.1. Let E be a At-hole context, and let p be an
atom that does not occur in E.

(cid:24) E[p]

iff ∀(cid:3)p ⊆ﬁn At. (cid:24) E[(cid:3)p].

Theorem VII.2. Let D be a Γ-hole context, and let p and q
be atoms that do not occur in D.

(cid:24) D[p] ∧ D[(cid:3)qp]

iff ∀γ ∈ Γ. (cid:24) D[γ]

Theorem VII.3. Let C be a Φ-hole context, and let p, q and r
be atoms that do not occur in C. Let S = {p; (cid:3)qp; (cid:3)q :– rp}.

(∀ϕs ∈ S. (cid:24) C[ϕs] ∧ C[¬ϕs]) iff ∀ϕ ∈ Φ. (cid:24) C[ϕ]

These theorems enable us to mechanically prove meta-
theorems about trust management. Essentially, they reduce a
meta-level quantiﬁed validity judgement to a small number
of concrete instances. There is only one case to consider
for universally quantiﬁed atoms or conjunctions of atoms
(Theorem VII.1), two cases for meta-variables ranging over
policies (Theorem VII.2), and six cases (three of them negated)
for meta-variables ranging over arbitrary formulas (Theo-
rem VII.3).

Consider, for instance, the meta-statement
∀ϕ ∈ Φ. (cid:24) ϕ ←→ (cid:3)(cid:7)ϕ.

171

More formally and equivalently, we could write

∀ϕ ∈ Φ. (cid:24) C[ϕ], where C = [·] ←→ (cid:3)(cid:7)[·].

This can then be mechanically proved by proving just the six
basic instances from Theorem VII.3.

It is easy to extend this method further. Meta-theorems with
multiple meta-variables can also be mechanically proved with
this approach by combining the theorems. For example,
∀ϕ, ϕ(cid:2) ∈ Φ, γ ∈ Γ. (cid:24) (cid:3)γ(ϕ ∧ ϕ(cid:2)) ←→ ((cid:3)γϕ ∧ (cid:3)γϕ(cid:2))

reduces to 6 × 6 × 2 = 72 propositional cases: six each for ϕ
and ϕ(cid:2), and two for γ.

We can also prove meta-theorems with side-conditions. If
a meta-variable ϕ ranges over negation-free formulas from
Φ, it is sufﬁcient to prove the three positive instances from
Theorem VII.3. Similarly, for meta-variables ranging over (cid:3)-
free ϕ (as in Axiom Dlog), the number of cases reduces to
two (ϕ (cid:25)→ p and ϕ (cid:25)→ ¬p).

In the following, we discuss a number of meta-theorems
that we veriﬁed using the tool, based on Theorems VII.1–VII.3
(in addition to proving them manually). These meta-theorems
provide interesting general insights into Datalog-based trust
management systems. Moreover, they have also been essential
in our (manual) proofs of soundness and completeness (The-
orem V.3).

∀ϕ, ϕ(cid:2) ∈ Φ, γ ∈ Γ. (cid:24) (cid:3)γ(ϕ ∧ ϕ(cid:2)) ←→ ((cid:3)γϕ ∧ (cid:3)γϕ(cid:2))

As in standard modal logic, the (cid:3)-operator distributes over
conjunction. The trust management interpretation is equally
obvious: submitting credential set γ to a policy results in a
new policy that satisﬁes the property ϕ∧ ϕ(cid:2) iff the new policy
satisﬁes both ϕ and ϕ(cid:2). Proving this theorem took 574 ms.
∀ϕ, ϕ(cid:2) ∈ Φ, γ ∈ Γ. (cid:24) (cid:3)γ(ϕ ∨ ϕ(cid:2)) ←→ ((cid:3)γϕ ∨ (cid:3)γϕ(cid:2))

In most modal logics, (cid:3) does not distribute over ∨. This
theorem holds only because the accessibility relation is func-
tional, or equivalently, because the result of combining a
credential set with a policy is always uniquely deﬁned. But
again, the theorem is obviously true in the trust management
interpretation: submitting credential set γ to a policy results
in a new policy that satisﬁes the property ϕ ∨ ϕ(cid:2) iff the new
policy satisﬁes either ϕ or ϕ(cid:2). (577 ms)

∀ϕ ∈ Φ. (cid:24) ϕ ←→ (cid:3)(cid:7)ϕ

Submitting an empty credential set is equivalent to not sub-
mitting anything at all. (3 ms)

∀ϕ ∈ Φ+, γ ∈ Γ. (cid:24) ϕ → (cid:3)γϕ,

where Φ+ denotes the set of ¬-free formulas in Φ. This can
be interpreted as a monotonicity property of the accessibility

relation, and also of credential submissions: positive properties
are retained after credential submissions. (95 ms)

∀ϕ ∈ Φ, γ, γ(cid:2) ∈ Γ. (cid:24) (cid:3)γ(cid:3)γ(cid:2) ϕ ←→ (cid:3)γ(cid:2)(cid:3)γϕ

is also unusual

This property, corresponding to a commutative accessibility
relation,
logics. A simple
corollary is that all permutations of arbitrary strings of boxes
are equivalent, or that
the order in which credentials are
submitted is irrelevant. (3059 ms)

in multi-modal

∀ϕ ∈ Φ, p ∈ At, (cid:3)p ⊆ﬁn At. (cid:24) (cid:3)p → ((cid:3)pϕ ←→ (cid:3)p :– (cid:3)pϕ)

If (cid:3)p holds in a policy, then submitting the atomic p results in a
policy that is indistinguishable from the policy resulting from
submitting the conditional credential p : – (cid:3)p. (30 ms)

∀ϕ ∈ Φ+, γ1, γ2 ∈ Γ. (cid:24) (cid:3)γ1γ2 ∧ (cid:3)γ2 ϕ → (cid:3)γ1ϕ

This theorem asserts that credential-based derivations can
be applied transitively. More precisely: if, after submitting
credential set γ1, the credential set γ2 would be derivable from
the combined policy, and if submitting γ2 directly would be
sufﬁcient for making property ϕ true, then γ1 alone would also
be sufﬁcient. This only holds for negation-free ϕ. A simple
counter-example can be constructed from instantiating γ1 = p,
γ2 = (cid:10), and ϕ = ¬p. (1078 ms)

∀ϕ ∈ Φ, γ ∈ Γ. (cid:24) γ → (ϕ ←→ (cid:3)γϕ)

then submitting γ as
If a policy contains the clauses γ,
credential set is equivalent to not submitting anything at all.
This holds even for properties ϕ containing negation. (157 ms)

C. Proving one’s own completeness

In Section VI, we gave some informal justiﬁcations as to
why the reduction to propositional logic, which our imple-
mentation is based on, is not only sound (which is relatively
easy to prove manually) but also complete with respect to
the axiomatization. We did not prove completeness completely
manually, but instead used the implementation itself to assist in
the proof, thereby letting the implementation effectively prove
its own completeness!

The main reason why this is possible is the ability to
prove meta-theorems mechanically (Theorems VII.1–VII.3).
With this feature in place, we mechanically veriﬁed all axiom
schemas. What
the reduction rules, as
implemented, cover all axioms.

this proves is that

It remained to show that all rules are covered as well. Recall
that there are three rules, Modus Ponens, (Mon), and (N).
Modus Ponens is built into the underlying SAT solver. We
manually proved that Rule (Mon) can be replaced by the axiom
schema (cid:24) (cid:3)γ1 γ2 ∧ (cid:3)γ2 ϕ → (cid:3)γ1 ϕ, which we mechanically
veriﬁed.
To prove coverage of Rule (N), we perform a rule induction
over (cid:24) ϕ in order to conclude (cid:24)∗ (cid:3)γϕ (where (cid:24)∗ denotes
the proofs performed by the implementation). All the base
cases, i.e., the cases where ϕ is an instance of an axiom, were
proven mechanically, again as meta-theorems (for example,
for Ax. (C1), we prove ∀γ, γ(cid:2) ∈ Γ. (cid:24) (cid:3)(cid:2)
γ((cid:3)γγ)). The two

remaining cases, where (cid:24) ϕ is a rule application, were easy
to prove manually.
Together, these results prove that (cid:24) ϕ implies (cid:24)∗ ϕ, in other
words, that the implementation is complete. The correctness
of the proof rests on a couple of assumptions: the soundness
of the implementation itself, the correctness of the underlying
SAT solver, and the correctness of our manual proofs. We are
conﬁdent about the implementation’s soundness, as the reduc-
tion rules it is based on are sound, and it has been extensively
tested. To achieve an even higher level of conﬁdence about
the semi-mechanically proven completeness result, one could
mechanically verify all computer-generated subproofs, since an
automated proof veriﬁer would be much smaller and simpler
than our proof generator.

VIII. RELATED WORK

Trust Management. Blaze et al. coined the term ‘trust
management’ in their seminal paper [12], referring to a set of
principles for managing security policies, security credentials
and trust relationships in a decentralized system. In their
proposed paradigm, decentralization is facilitated by making
policies depend on submitted credentials and by enabling local
control over trust relationships. Policies, credentials and trust
relationships should be expressed in a common language,
thereby separating policy from the application. Early examples
of trust management
languages include PolicyMaker [12],
KeyNote [11], and SPKI/SDSI [45], [26].

Li et al. [40], [39] argue that authorization in decentralized
systems should depend on delegatable attributes rather than
identity, and call systems that support such policies and
credentials attribute-based access control (ABAC) systems. In
essence, their ABAC paradigm is a reﬁnement of trust man-
agement that makes the requirements on the expressiveness of
credentials and policies more explicit: principals may assert
parameterized attributes about other principals; authority over
attributes may be delegated to other principals (that possess
some speciﬁed attribute) via trust relationship credentials; and
attributes may be inferred from other attributes. Their proposed
policy language, RT, satisﬁes all these requirements. Like its
predecessor, Delegation Logic (DL) [37], RT can be translated
into Datalog. (A more expressive variant of RT, RTC [38], can
be translated into Datalog with constraints [33].)

Datalog has also been chosen as the basis of many other
trust management languages. Examples include a language
by Bonatti and Samarati [13], [14], SD3 [34], Binder [23],
Cassandra [10], [9], a language by Wang et al. [53], one by
Giorgini et al. [29], [30] and SecPAL [5], [6].

Apart from their relation to Datalog, what most of these
languages have in common is that attributes are qualiﬁed by
a principal who “says” it, and is vouching for the attribute’s
truth. In a credential, this principal coincides with the creden-
tial’s issuer. For example, in Binder, the fact (or condition)
that principal A is a student, according to authority C, could
be expressed as C.isStudent(A); similarly, in SecPAL, one
would write C says A isStudent. This qualiﬁer does not

172

extend Datalog’s expressiveness, as it is easy to translate a
qualiﬁed atom C.p((cid:3)e) into a normal Datalog atom p(C, (cid:3)e).

The says operator can be traced back to an authorization
logic by Abadi et al. (ABLP) [2], [35]. Even though it predates
the paper by Blaze et al., ABLP could be seen as a trust
management language. It introduced the says operator – but
in contrast to the simpler Datalog-based languages, ABLP and
related languages such as ICL [28], CCD [1] and DKAL [31],
[32] treat the says (or said, in the case of DKAL) construct as
a proper unary operator in the logic, which cannot be simply
translated into an extra predicate parameter. Our semantics
therefore does not cover these languages.
Previous work on trust management semantics.
The
Datalog-based languages inherit their semantics from Datalog.
The most common way to present Datalog’s semantics is as
the minimal ﬁxed point of the immediate consequence operator
Tγ, parameterized on a Datalog program γ [20]. The result
is the set of all atoms p that are true in γ. Our inductive
deﬁnition of γ (cid:2) p coincides with this semantics: γ (cid:2) p
iff p ∈ Tω
γ (∅). A model-theoretic semantics can be given by
taking the minimal Herbrand model (i.e., the intersection of all
Herbrand models) of γ, and a proof-theoretic semantics can be
deﬁned using resolution strategies [3]. All three ﬂavours of the
standard semantics are equivalent, but, as we have shown in
Section IV, they are not adequate for modeling Datalog-based
trust management policies that are combined with varying sets
of credentials.

Abadi et al. [2] deﬁne ABLP axiomatically and then give
it a model-theoretic semantics based on Kripke structures.
However, the axiomatization is not complete with respect to
the semantics. Further work along these lines has been done
by Garg and Abadi [28], who present sound and complete
translations from a minimal logic with a says operator called
ICL, and various extensions of it, into the classical modal
logic S4. Similar, Gurevich and Neeman [32] provide a Kripke
semantics for DKAL2, the successor of the DKAL [31]. These
modal semantics are straightforward compared to the one
presented here, but this is because they have a completely
different focus, namely providing a modal interpretation of the
says (or said) operator. As we have argued above, this operator
is not very interesting in the context of the more practical,
Datalog-based, languages (at least from a foundational point
of view). The focus of our semantics is to give a modal
interpretation of the turnstile operator : – in Datalog policies
and of credential submissions in a trust management context.
Related logics and logic programming.
It has been noted
before that the standard Datalog semantics does not enjoy
compositionality and full abstraction relative to program union.
Gaifman and Shapiro [27] propose a semantics for logic
programs that is compositional, fully abstract and preserves
congruence with respect to program union. These properties
are achieved by interpreting logic program clauses as implica-
tional formulas, as a result of which all dependencies between
atoms are preserved. However, this semantics does not give us
the desired behavior (see Section IV). The problem, in essence,

stems from the fact that material implication is inadequate as
an interpretation of conditional if-then statements [46], and
thus also of Datalog clauses (in our context) and credential
submissions: if ¬p holds in a policy, it follows that p → q
also holds, for every q. However, it should not follow that
the clause q : – p is contained in the policy; and similarly, it
is not justiﬁed to infer that q would hold if credential p were
submitted and combined with the policy.

One of the main claims of this paper is that clauses and
credential submissions ought to be modeled as counterfactual
statements. The complexity of our semantics, then, stems from
the fact that simple, truth-functional Boolean operators cannot
offer an adequate account of counterfactuals. Stalnaker [49]
and Lewis [36] were the ﬁrst to propose a Kripke semantics
for counterfactuals, based on a similarity ordering on worlds:
essentially, “if p were true, then q would be true” holds in
a world w if of those worlds in which p is true, the ones
that are most similar to w also make q true. Our semantics
is based on the same basic framework. Our deﬁnition of
what “most similar” means is novel, as is our counterfactual
interpretation of Datalog. Therefore, our work could also be
seen as a novel semantics for Datalog in general. However, the
action of dynamically injecting varying sets of clauses into a
Datalog program is a characteristic that is rather speciﬁc to
trust management, hence it is more appropriate to frame our
semantics speciﬁcally as a trust management semantics.

Much work has been done on axiomatizations of multi-
modal counterfactual logic. A good overview can be found in a
paper by Ryan and Schobbens [48]. Their paper also contains
a comprehensive listing of axioms proposed in the literature
together with the corresponding frame conditions.

At ﬁrst sight, hypothetical Datalog [15], [16] bears some
resemblance to our logic. Hypothetical Datalog allows clauses
such as p : –(q : add r), meaning “p is derivable provided
that, if r were added to the rule base, q would hold”. In
our logic, this would correspond to (q : – r) → p. But our
logic is signiﬁcantly more expressive: hypothetical Datalog
cannot express statements that are hypothetical at the top-level
and/or hypothetically add non-atomic clauses, for instance “p
would hold if the conditional (credential) ‘if r were true then
q would hold’ were added”. In our logic, this statement can
be expressed as (cid:3)q :– rp. Moreover, the work on hypothetical
Datalog (and similar works on hypothetical reasoning) is only
concerned with query evaluation against concrete rule bases,
and not with the harder problem of universal validity.
Probing attacks. We identiﬁed the security analysis of
probing attacks as one practical area on which the present
work is likely to have an impact. The problem of probing
attacks has gained attention only rather recently. Gurevich and
Neeman were the ﬁrst to identify this general vulnerability of
logic-based trust management systems [31]. In [4], probing
attacks are framed in terms of the information ﬂow properties
opacity [42], [18] and its negation, detectability. Becker and
Koleini developed a tool for checking detectability of conﬁden-
tial facts in Datalog policies, based on constructing a counter-

173

policy, i.e., one that conforms to the given probes but makes the
conﬁdential fact false. The fact is detectable if and only if no
such policy can be found. We use and extend their benchmark
to compare the performance of our logic-based approach in
Section VII-A.

IX. CONCLUDING DISCUSSION

Logics and semantics have long played an important, and
successful, role in security research, especially in the area
of cryptographic protocols [50] . (A prominent example has
been the struggle to ﬁnd an adequate semantics for BAN
logic [19], see e.g. [21], [17], [51], [52]). The area of trust
management, however, has hitherto not been investigated from
a foundational, semantics-based point of view.

Evaluating a Datalog policy is a straightforward task, and
so is taking the union of two sets of Datalog clauses. At
ﬁrst sight, then, it may come as an unwelcome surprise that
our semantics, and the axiomatization, of Datalog-based trust
management is so complex. Indeed, if one were only interested
in the results of evaluating access queries against a concrete
policy under a concrete set of submitted credentials, then a
formal semantics would be unnecessary. But if one is interested
in reasoning about the behavior of trust management systems,
it is necessary to formulate universal truths that are quantiﬁed
over all policies. Proving such statements is remarkably hard,
even though the base language is so simple. As we have
seen, neither the standard Datalog semantics nor the Kripke
semantics for ABLP and related languages properly captures
Datalog-based trust management. The situation has actually
been worse than that of BAN logic,3 since, prior to the present
work, not even a sound and complete proof system existed, let
alone a formal semantics.
Our formal semantics is deﬁned by the notion of TM models
and the TM validity judgement (cid:4)TM, and the axiomatization
of TM validity is given by the proof system (cid:24). Theorem V.3
shows that the proof system is sound and complete with respect
to the semantics. So what role does the relation (cid:2), which
we introduced in Section II, play? We need it, because a
semantics, despite the term’s etymology, does not really convey
the meaning of the logic. As Read [44] puts it,

[f]ormal semantics cannot itself be a theory of mean-
ing. It cannot explain the meaning of the terms in the
logic, for it merely provides a mapping of the syntax
into another formalism, which itself stands in need
of interpretation.

Of course, the relation (cid:2) is also just “another formalism”,
but
it
language
description of what a trust management system does, and
can therefore more easily be accepted as “obviously” correct.
there
Without

it (and Theorem IV.11 providing the glue),

is one that

is much closer to the natural

3 Cohen and Dam succinctly described the BAN situation thus [21]: “While
a number of semantics have been proposed for BAN and BAN-like logics,
none of them capture accurately the intended meaning of the epistemic
modality in BAN [...]. This situation is unsatisfactory. Without a semantics, it
is unclear what is established by a derivation in the proof system of BAN: A
proof system is merely a deﬁnition, and as such it needs further justiﬁcation.”

174

would be a big gap between the intuitive meaning of the
language and its formalization.

What the formal semantics does provide is a number of
alternative, less obvious, interpretations of a trust management
system. TM models are abstract, purely mathematical objects
that are independent of the language’s syntax. They capture
precisely (and only) the essential aspects of a trust manage-
ment system. The easiest interpretation of a TM model is a
graph in which two policies are connected when one is the
result of submitting a set of credentials to the other.

A deeper alternative interpretation is that trust management
logic is a counterfactual
logic – a logic that avoids the
paradoxes of material implication. Both policy clauses as well
as statements about credential submissions are counterfactual,
rather than implicational, statements. They state what would
be the case if something else were the case.

As Ryan and Schobbens have noted, counterfactual state-
ments can also be interpreted as hypothetical minimal updates
to a knowledge base [48]. Under this interpretation, a cre-
dential submission (cid:3)γϕ would be equivalent to saying that
ϕ holds in a policy after it has been minimally updated with
credential set γ. The restrictions on the accessibility relation
(Def. IV.7) can then be seen as a precise speciﬁcation of what
constitutes a minimal update to a policy.

Hence, from a foundational point of view, our semantics
provides new insights into the nature of trust management.
From a more practical point of view, it led us to an ax-
iomatization that can be mechanized. We showed how our
implementation could be put to good use by applying it to
the analysis of probing attacks. It is the ﬁrst automated tool
that can feasibly check real-world probing attacks of realistic
size, comprising millions of probes. But our implementation
is a general automated theorem prover for our language, the
expressiveness of which goes far beyond that needed for
probing attacks. In particular, we used the implementation to
prove general meta-theorems about trust management – some
of which are intuitively obvious (but not necessarily easy to
prove), and some of which are decidedly non-trivial (such as
Lemma V.2 or lemmas that help prove the implementation’s
own completeness).

Our logic is decidable, since every formula is equivalent to a
(potentially much larger) propositional formula. However, the
complexity of the logic remains an open question. We also
leave the development of a ﬁrst-order version of the logic
to future work: in this version, atoms would be predicates
with constant and variable parameters, and clauses would be
implicitly closed under universal quantiﬁcation.

Acknowledgements Alessandra Russo is funded in part
through the US Army Research laboratory and the UK Min-
istry of Defence under Agreement Number W911NF-06-3-
0001. We thank Mark Ryan and Stephen Muggleton for fruitful
discussions, and Christoph Wintersteiger for his support with
Z3. We are also grateful for the valuable comments from the
anonymous reviewers.

REFERENCES

[1] M. Abadi. Access control in a core calculus of dependency. Electronic

Notes in Theoretical Computer Science, 172:5–31, 2007.

[2] M. Abadi, M. Burrows, B. Lampson, and G. Plotkin. A calculus for ac-
cess control in distributed systems. ACM Transactions on Programming
Languages and Systems, 15(4):706–734, 1993.

[3] S. Abiteboul, R. Hull, and V. Vianu. Foundations of Databases. Addison-

Wesley, 1995.

[4] M. Y. Becker. Information ﬂow in credential systems. In IEEE Computer

Security Foundations Symposium, pages 171–185, 2010.

[5] M. Y. Becker, C. Fournet, and A. D. Gordon. Design and semantics
of a decentralized authorization language. In IEEE Computer Security
Foundations Symposium, pages 3–15, 2007.

[6] M. Y. Becker, C. Fournet, and A. D. Gordon.

and semantics of a decentralized authorization language.
Computer Security, 18(4):619–665, 2010.

SecPAL: Design
Journal of

[7] M. Y. Becker and M. Koleini. Information leakage in datalog-based trust
management systems. Technical Report MSR-TR-2011-11, Microsoft
Research, 2011.

[8] M. Y. Becker and M. Koleini. Opacity analysis in trust management
the 14th Information Security Conference

systems. Proceedings of
(ISC2011), pages 229–245, 2011.

[9] M. Y. Becker and P. Sewell. Cassandra: distributed access control
policies with tunable expressiveness. In IEEE International Workshop
on Policies for Distributed Systems and Networks, pages 159–168, 2004.
[10] M. Y. Becker and P. Sewell. Cassandra: Flexible trust management,
In IEEE Computer Security

applied to electronic health records.
Foundations, pages 139–154, 2004.

[11] M. Blaze, J. Feigenbaum, and A. D. Keromytis. The role of trust man-
agement in distributed systems security. In Secure Internet Programming,
pages 185–210, 1999.

[12] M. Blaze, J. Feigenbaum, and J. Lacy. Decentralized trust management.

In IEEE Symposium on Security and Privacy, pages 164–173, 1996.

[13] P. Bonatti and P. Samarati. Regulating service access and information
In Proceedings of the 7th ACM conference on

release on the web.
Computer and communications security, pages 134–143. ACM, 2000.

[14] P. Bonatti and P. Samarati. A uniform framework for regulating service
Journal of Computer

access and information release on the web.
Security, 10(3):241–272, 2002.

[15] A. Bonner. A logic for hypothetical reasoning. In Proceedings of the
Seventh National Conference on Artiﬁcial Intelligence, pages 480–484,
1988.

[16] A. Bonner. Hypothetical Datalog: complexity and expressibility. Theo-

retical Computer Science, 76(1):3–51, 1990.

[17] C. Boyd and W. Mao. On a limitation of BAN logic. In Advances in

Cryptology (EUROCRYPT’93), pages 240–247. Springer, 1994.

[18] J. Bryans, M. Koutny, L. Mazar´e, and P. Ryan. Opacity generalised
International Journal of Information Security,

to transition systems.
7(6):421–435, 2008.

[19] M. Burrows, M. Abadi, and R. Needham. A logic of authentication.

ACM Transactions on Computer Systems, 8:18–36, 1990.

[20] S. Ceri, G. Gottlob, and L. Tanca. What you always wanted to
IEEE Transactions on

know about Datalog (and never dared to ask).
Knowledge and Data Engineering, 1(1):146–166, 1989.

[21] M. Cohen and M. Dam. A completeness result for BAN logic. Methods

for Modalities, 4, 2005.

[22] L. de Moura and N. Bjørner. Z3: An Efﬁcient SMT Solver.

In
C. Ramakrishnan and J. Rehof, editors, Tools and Algorithms for the
Construction and Analysis of Systems, volume 4963 of Lecture Notes in
Computer Science, pages 337–340. Springer Berlin / Heidelberg, 2008.
In IEEE

Binder, a logic-based security language.
Symposium on Security and Privacy, pages 105–113, 2002.
Suggested

[23] J. Detreville.

[24] DIMACS

Satisﬁability:

Challenge

Format.

–

ftp://dimacs.rutgers.edu/pub/challenge/satisﬁability, 1993.

[25] N. E´en and N. S¨orensson. An extensible SAT-solver. In E. Giunchiglia
and A. Tacchella, editors, Theory and Applications of Satisﬁability
Testing, volume 2919 of Lecture Notes in Computer Science, pages 333–
336. Springer Berlin / Heidelberg, 2004.

[26] C. M. Ellison, B. Frantz, B. Lampson, R. Rivest, and Thomasand. SPKI

certiﬁcate theory, RFC 2693, September 1999.

[27] H. Gaifman and E. Shapiro. Fully abstract compositional semantics for
logic programs.
In Proceedings of the 16th ACM SIGPLAN-SIGACT
Symposium on Principles of Programming Languages (POPL), pages
134–142. ACM, 1989.

175

[28] D. Garg and M. Abadi. A modal deconstruction of access control
logics. In Foundations of Software Science and Computation Structures
(FOSSACS’08), pages 216–230, 2008.

[29] P. Giorgini, F. Massacci, J. Mylopoulos, and N. Zannone. Requirements
engineering meets trust management. International Conference on Trust
Management, pages 176–190, 2004.

[30] P. Giorgini, F. Massacci, J. Mylopoulos, and N. Zannone. Requirements
engineering for trust management: model, methodology, and reasoning.
International Journal of Information Security, 5(4):257–274, 2006.

[31] Y. Gurevich and I. Neeman. DKAL: Distributed-knowledge authoriza-
In IEEE Computer Security Foundations Symposium

tion language.
(CSF), pages 149–162, 2008.

[32] Y. Gurevich and I. Neeman. DKAL 2 – a simpliﬁed and improved
authorization language. Technical Report MSR-TR-2009-11, Microsoft
Research, 2009.

[33] J. Jaffar and M. J. Maher. Constraint logic programming: a survey.

Journal of Logic Programming, 19/20:503–581, 1994.

[34] T. Jim. SD3: A trust management system with certiﬁed evaluation. In
Proceedings of the 2001 IEEE Symposium on Security and Privacy,
pages 106–115, 2001.

[35] B. Lampson, M. Abadi, M. Burrows, and E. Wobber. Authentication in
distributed systems: theory and practice. ACM Transactions on Computer
Systems, 10(4):265–310, 1992.

[36] D. Lewis. Counterfactuals. Harvard University Press, 1979.
[37] N. Li, B. Grosof, and J. Feigenbaum. A practically implementable and
tractable delegation logic. In IEEE Symposium on Security and Privacy,
pages 27–42, 2000.

[38] N. Li and J. C. Mitchell. Datalog with constraints: A foundation for trust
management languages. In Practical Aspects of Declarative Languages,
pages 58–73, 2003.

[39] N. Li, J. C. Mitchell, and W. H. Winsborough. Design of a role-based
trust management framework. In Symposium on Security and Privacy,
pages 114–130, 2002.

[40] N. Li, W. Winsborough, and J. Mitchell. Distributed credential chain
discovery in trust management.
the 8th ACM
Conference on Computer and Communications Security, pages 156–165.
ACM, 2001.

In Proceedings of

[41] D. Makinson. Ways of doing logic: What was different about AGM

1985? Journal of Logic and Computation, 13(1):3–14, 2003.

[42] L. Mazar´e. Using uniﬁcation for opacity properties. In In Proceedings
of the Workshop on Issues in the Theory of Security (WITS’04, pages
165–176, 2004.

[43] G. Plotkin. LCF considered as a programming language. Theoretical

Computer Science, 5(3):223–255, 1977.

[44] S. Read. Relevant logic. Blackwell Oxford, 1988.
[45] R. L. Rivest and B. Lampson. SDSI – A simple distributed security

infrastructure, August 1996.

[46] B. Russell. Principles of Mathematics. London: G. Allen & Unwin,

112, 1968.

[50] P. Syverson. The use of logic in the analysis of cryptographic protocols.
In IEEE Symposium on Security and Privacy, pages 156–170. IEEE,
1991.

[51] P. Syverson and P. Van Oorschot. On unifying some cryptographic
In IEEE Symposium on Security and Privacy, pages

protocol logics.
14–28. IEEE, 1994.

[52] W. Teepe. BAN logic is not ‘sound’, constructing epistemic logics for
security is difﬁcult. Workshop on Formal Approaches to Multi-Agent
Systems, 6:79–91, 2006.

[53] L. Wang, D. Wijesekera, and S. Jajodia. A logic-based framework
for attribute based access control.
In Proceedings of the 2004 ACM
workshop on Formal methods in security engineering, pages 45–55.
ACM, 2004.

1903.

[47] M. Ryan and M. Sadler. Valuation systems and consequence relations.
In D. G. S. Abramsky and T. Maibaum, editors, Handbook of Logic in
Computer Science, pages 2–74. 1992.

[48] M. Ryan and P. Schobbens. Counterfactuals and updates as inverse
modalities. Journal of Logic, Language and Information, 6(2):123–146,
1997.

[49] R. Stalnaker. A theory of conditionals. Studies in logical theory, 2:98–

