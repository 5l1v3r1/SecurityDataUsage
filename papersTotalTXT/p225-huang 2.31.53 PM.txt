Confused, Timid, and Unstable:

Picking a Video Streaming Rate is Hard

Te-Yuan Huang Nikhil Handigol Brandon Heller Nick McKeown Ramesh Johari

{huangty,nikhilh,brandonh,nickm,ramesh.johari}@stanford.edu

Stanford University

ABSTRACT
Today’s commercial video streaming services use dynamic
rate selection to provide a high-quality user experience. Most
services host content on standard HTTP servers in CDNs,
so rate selection must occur at the client. We measure three
popular video streaming services – Hulu, Netﬂix, and Vudu
– and ﬁnd that accurate client-side bandwidth estimation
above the HTTP layer is hard. As a result, rate selection
based on inaccurate estimates can trigger a feedback loop,
leading to undesirably variable and low-quality video. We
call this phenomenon the downward spiral eﬀect, and we
measure it on all three services, present insights into its root
causes, and validate initial solutions to prevent it.

Categories and Subject Descriptors
C.2.0 [Computer Systems Organization]: Computer-
Communication Networks—General ; C.4 [Performance of
Systems]: [Measurement techniques]

General Terms
Measurement

Keywords
HTTP-based Video Streaming, Video Rate Adaptation

1.

INTRODUCTION

Video streaming is a huge and growing fraction of Inter-
net traﬃc, with Netﬂix and Youtube alone accounting for
over 50% of the peak download traﬃc in the US [18]. Sev-
eral big video streaming services run over HTTP and TCP
(e.g. Hulu, Netﬂix, Vudu, YouTube) and stream data to the
client from one or more third-party commercial CDNs (e.g.
Akamai, Level3 or Limelight). Streaming over HTTP has
several beneﬁts: It is standardized across CDNs (allowing
a portable video streaming service), it is well-established
(which means the CDNs have already made sure service can

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

5000

4000

3000

s
/

b
k

2000

1000

Video Playback Rate

Competing Flow's
Throughput

1750

1400

1050

750
560
375
235

0
0

100 200 300 400 500 600 700 800

Time (s)

Figure 1:
(Service A) A video starts streaming at
1.75Mb/s over a 5Mb/s network. After 395 seconds,
a second ﬂow starts (from the same server). The
video could stream at 1.75Mb/s (given its fair share
of 2.5Mb/s), but instead drops down to 235kb/s.

reach through NATs to end-hosts), and cheap (the service
is simple, commoditized, and the CDNs compete on price).
These beneﬁts have made possible the huge growth in aﬀord-
able, high-quality movie and TV streaming, for our viewing
delight.

When video is streamed over HTTP, the video service
provider relies on TCP to ﬁnd the available bandwidth and
choose a video rate accordingly. For example, if a client
estimates that there is 1.5Mb/s available in the network,
it might request the server to stream video compressed to
1.3Mb/s (or the highest video rate available at or below
1.5Mb/s). The video streaming service provider must walk
a tightrope: If they pick a video rate that is too high, the
viewer will experience annoying rebuﬀering events; if they
pick a streaming rate that is too low, the viewer will ex-
perience poor video quality. In both cases, the experience
degrades, and user may take their viewing elsewhere [9]. It
is therefore important for a video streaming service to select
the highest safe video rate.

This paper describes a measurement study of three pop-
ular HTTP-based video streaming services (Hulu, Netﬂix,
and Vudu) to see how well they pick the video rate. Accord-
ing to the latest Consumer Reports [21], Netﬂix is the most
popular video streaming provider in the United States, while

2252262275000

4000

3000

2000

1000

s
/

b
k

Video Flow's
Throughput

Competing Flow's
Throughput

Video Playback Rate

5000

4000

3000

2000

1000

s
/

b
k

1750

1400

1050

750
560
375
235

Competing Flow's
 Throughput

3200

2500

2000

1500

1000

650

Video Flow's
Throughput

Zero Window
Advertisement

Video Playback Rate

0
0

100 200 300 400 500 600 700 800

0
0

100 200 300 400 500 600 700 800

Time (s)

Time (s)

(a) Service A. Network bottleneck set to 5Mb/s.

(b) Service B. Network bottleneck set to 5Mb/s.

20000

Video Flow's
Throughput

Competing Flow's
Throughput

15000

s
/
b
k

10000

5000

9000

6750

4500

3000

Video Playback Rate

0
0

100

200

300

400
Time (s)

500

600

700

5000

4000

3000

2000

1000

s
/

b
k

Video Flow's
Throughput

Competing Flow's
 Throughput

2000

1500

1000

Video
 Playback Rate

0
0

100

200

300

Time (s)

400

500

600

(c) Service C HD. Network bottleneck set to 22Mb/s.

(d) Service C SD. Network bottleneck set to 5Mb/s.

Figure 4: The downward spiral eﬀect is visible in all three services.

The video rates available from each of the three services
are summarized in Table 2; some playback rates may not be
available for some videos.

3.3 The Competing Flows

The competing ﬂow is a TCP ﬂow doing a long ﬁle down-
load. To eliminate any unfairness due to variations in net-
work path properties, we ensure that the competing ﬂow is
served by the same CDN, and usually, by the same server.
For Service A and Service C, the competing ﬂow is gener-
ated by an open-ended byte range request to the ﬁle with
the highest rate. Further, we use the DNS cache to make
sure that the competing ﬂow comes from same termination
point (the server or the load-balancer) as the video ﬂow.
For Service B, since the ﬁles are stored as small segments,
an open-ended request only creates short-lived ﬂows.
In-
stead, we generate the competing ﬂow by requesting the
Flash version of the same video stored in the same CDN,
using rtmpdump [20] over TCP.

4. THE DOWNWARD SPIRAL EFFECT

All three services suﬀer from what we call the “downward
spiral eﬀect” – a dramatic anomalous drop in the video play-
back rate in the presence of a competing TCP ﬂow. The
problem is starkly visible in Figure 4.
In all four graphs,
the video stream starts out alone and then competes with
another TCP ﬂow. As soon as the competing ﬂow starts up,
the client mysteriously picks a video playback rate that is far
below the available bandwidth. Our goal is to understand
why this happens.

To gain a ﬁrst inkling into what is going on, we calculate
the upper bound of what the client might believe the instan-
taneous available bandwidth to be, by measuring the arrival
rate of the last video segment. Speciﬁcally, we calculate the
throughput upper bound as the size of a received video seg-
ment divided by the time it took to arrive (the time from
when the ﬁrst byte arrived until the last byte arrived), which
excludes the initial server response time. In all of the graphs,
the video playback rate chosen by the client is quite strongly
correlated with the calculated throughput. As we will see,

228herein lies the problem:
if the client is selecting the video
rate based on some function of the throughput it perceived,
and the throughput is so diﬀerent from the actual available
bandwidth, then it is not surprising the client does such a
poor job. Let’s now see what goes wrong for each service in
turn. For ease of discussion, we will use video throughput to
refer to the throughput a client perceived by downloading a
video segment.

4.1 Service A

Figure 4(a) shows the playback rate of a Service A video
session along with the client’s video throughput over time.
Starting out, the video stream is the only ﬂow and the client
requests the highest video rate (1750kb/s). The competing
ﬂow begins after 400 seconds; the video rate steadily drops
until it reaches the lowest rate (235kb/s), and it stays there
most of the time until the competing ﬂow stops. In theory,
both ﬂows should be able to stream at 2.5Mb/s (their fair
share of the link) and the client should continue to stream
at 1750kb/s.

We repeated the experiment 76 times over four days. In
67 cases (91%) the downward spiral happens, and the client
picks either the lowest rate, or bounces between the two or
three lowest rates. In just seven cases (9%) was the client
able to maintain a playback rate above 1400kb/s. To ensure
accuracy and eliminate problems introduced by competing
ﬂows with diﬀerent characteristics (e.g. TCP ﬂows with
diﬀerent RTTs), we make the competing ﬂow request the
same video ﬁle (encoded at 1750kb/s) from the same CDN.
Unlike the video ﬂow, the competing ﬂow is just a simple
TCP ﬁle download and its download speed is only dictated
by TCP congestion control algorithm and not capped by the
video client.3

Why does throughput of the video ﬂow drop so much be-
low available fair-share bandwidth? Is it an inherent charac-
teristic of streaming video over HTTP, or is the client simply
picking the wrong video rate?

We ﬁrst conﬁrm that the available bandwidth really is
available for streaming video. We do this using a feature
provided by the Service A client that allows users to man-
ually select a video rate and disable the client’s automatic
rate selection algorithm. We repeat the above experiment,
but with a slight modiﬁcation. As soon as the client picks a
lower rate, we manually force the video to play at 1750kb/s.
Figure 5 shows the results. Interestingly, the client main-
tains a playback rate of 1750kb/s without causing rebuﬀer-
ing events, and the throughput also increases. This suggests
that the downward spiral eﬀect is caused by underestima-
tion of the available bandwidth in the client’s rate selection
algorithm. The bandwidth is available, but the client needs
to go grab it.

4.2 Service B

Figure 4(b) shows the same downward spiral eﬀect in Ser-
vice B. As before, the bottleneck bandwidth is 5Mb/s and
the RTT is around 20 ms. We start a video streaming ses-
sion ﬁrst, allow it to settle at its highest rate (3200kb/s) and
then start a competing ﬂow after 337 seconds, by reading the
same video ﬁle from the same server.

3To eliminate variation caused by congestion at the server,
we veriﬁed that the same problem occurs if we download
the competing video ﬁle from a diﬀerent server at the same
CDN.

5000

4000

Video

3000

 Flow's
Throughput

s
/

b
k

2000

1000

Competing Flow's
Throughput

Video Playback Rate

1750

1400

1050

750
560
375
235

0
0

200

400

600
Time (s)

800

1000

1200

Figure 5: (Service A) The client manages to main-
tain the highest playback rate if we disable auto-
matic rate selection.

The client should drop the video rate to 2500kb/s (its fair
share of the available bandwidth). Instead, it steps all the
way to the lowest rate oﬀered by Service B, 650kb/s, and
occasionally to 1000kb/s. The throughput plummets too.

4.3 Service C

We observe the downward spiral eﬀect in Service C as well.
Since Service C does not automatically switch between its
HD and SD bitrates, we do two separate experiments.

In the HD experiment, as shown in Figure 4(c), we set
the bottleneck bandwidth to 22Mb/s. To start with, the
client picks the highest HD video rate (9Mb/s). When the
client’s playback buﬀer is full, the video ﬂow is limited by
the receive window, and the throughput converges to the
same value as the playback rate. We start the competing
ﬂow at 100 seconds, and it downloads the same video ﬁle
(9Mb/s video rate) from the same CDN.

Each ﬂow has 11Mb/s available to it, plenty for the client
to continue playing at 9Mb/s. But instead, the client resets
the connection and switches to 4.5Mb/s and then 3Mb/s,
before bouncing around several rates.

SD is similar. We set the bottleneck bandwidth to 5Mb/s,
and the client correctly picks the highest rate (2000kb/s) to
start with, as shown in Figure 4(d). When we start the
competing ﬂow, the video client drops down to 1000kb/s
even though its share is 2.5Mb/s. Since Service C only oﬀers
three SD rates, we focus on its HD service in the rest of the
paper.

5. WALKING THE DOWNWARD SPIRAL

To understand how the downward spiral happens, we ex-
amine each service in turn. Although each service enters
the downward spiral for a slightly diﬀerent reason, there is
enough commonality for us to focus ﬁrst on Service A (and
Figure 4(a)) and then describe how the other two services
diﬀer.

2295000

4000

3000

2000

1000

s
/
b
k

Playout Buffer is full

6

5

4

3

2

1

)
s
(
 
l
a
v
r
e
t
n
I
 
t
s
e
u
q
e
R

Playout Buffer is full

0
160

170

180

190

Time (s)

200

210

0
0

50

100

150

Time (s)

200

250

300

(a) TCP throughput before and after the buﬀer ﬁlls.

(b) Request interval before and after the buﬀer ﬁlls.

Figure 6: (Service A) Before and after the playback buﬀer ﬁlls at 185 seconds.

 
)

%

(
 
F
D
C

100

80

60

40

20

0
0

Video Rate 235kb/s
Video Rate 375kb/s
Video Rate 560kb/s
Video Rate 750kb/s
Video Rate 1050kb/s
Video Rate 1400kb/s
Video Rate 1750kb/s

Video Rate 235kb/s

Video Rate 375kb/s

Video Rate 560kb/s

Video Rate 750kb/s

Video Rate 1050kb/s

Video Rate 1400kb/s

Video Rate 1750kb/s

100

80

60

40

20

 
)

%

(
 
F
D
C

500

1000

1500

Throughput (kb/s)

2000

2500

0
0

500

1000

1500

Throughput (kb/s)

2000

2500

(a) Service A with no competing ﬂow.

(b) Service A with one competing ﬂow.

Figure 7: (Service A) Throughput at HTTP layer with and without a competing ﬂow.

5.1

Initial Condition: No Competing Flow

In the absence of a competing ﬂow (ﬁrst 400 seconds),
the Service A client correctly chooses the highest playback
rate. Because the available network bandwidth (5Mb/s) is
much higher than the playback rate (1750kb/s), the client
busily ﬁlls up its playback buﬀer and the bottleneck link
is kept fully occupied. Eventually the playback buﬀer ﬁlls
(after 185 seconds) and the client pauses to let it drain a
little before issuing new requests. Figure 6(a) shows how the
TCP throughput varies before and after the playback buﬀer
ﬁlls up. After the buﬀer is full, the client enters a periodic
ON-OFF sequence. As we will see shortly, the ON-OFF
sequence is a part of the problem (but only one part). Before
the buﬀer ﬁlls, the client requests a new 4-second segment
of video every 1.5 seconds on average (because it is ﬁlling
the buﬀer). Figure 6(b) conﬁrms that after the buﬀer is full,
the client requests a new 4-second segment every 4 seconds,
on average. The problem is that during the 4-second OFF
period, the TCP congestion window (cwnd ) times out —
due to inactivity longer than 200ms — and resets cwnd to

its initial value of 10 packets [5, 6]. Even though the client is
using an existing persistent TCP connection, the cwnd needs
to ramp up from slow start for each new segment download.
It is natural to ask if the repeated dropping back to slow-
start reduces the client’s video throughput, causing it to
switch to a lower rate. With no competing ﬂow, it appears
the answer is ‘no’. We verify this by measuring the video
throughput for many requests. We set the bottleneck link
rate to 2.5Mb/s, use traces collected from actual sessions
to replay the requests over a persistent connection to the
same server, and pause the requests at the same interval as
the pauses in the trace. Figure 7(a) shows the CDF of the
client’s video throughput for requests corresponding to vari-
ous playback rates. The video throughput is pretty accurate.
Except for some minor variation, the video throughput ac-
curately reﬂects the available bandwidth, and explains why
the client picks the correct rate.

230Competing Flow

)
t

n
e
m
g
e
s
(
 

w
o
d
n
W
n
o

 

i

Competing Flow

Emulated Video Flow

18

16

14

12

10

18

16

14

12

10

8

6

4

Emulated Video Flow

)
t
n
e
m
g
e
s
(
 

i

w
o
d
n
W
 
n
o
i
t
s
e
g
n
o
C
P
C
T

 

2
0.5

1.0

1.5

2.0

2.5

3.0

Time(s)

(a) A 235kbps Segment.

i
t
s
e
g
n
o
C
P
C
T

 

8

6

4

2
7

8

9

10

11

12

Time(s)

(b) Five contiguous 235kbps segments concatenated into
one.

Figure 8: (Service A) The evolution of cwnd for diﬀerent segment sizes.

 
)
s
/

3000

b
k
(
 
e
a
R

t

 
t
s
e
u
q
e
R
h

/

t

i

d
w
d
n
a
B
e
b
a

 

l

2500

2000

1500

1000

500

Available Bandwidth

Video Playout Rate

l
i

a
v
A

0
0

500 1000 1500 2000 2500 3000 3500 4000

Time(s)

Figure 9: (Service A) The client picks a video rate
depending on the available bandwidth. The hori-
zontal gray lines are the available rates.

5.2 The Trigger: With a Competing Flow

Things go wrong when the competing ﬂows starts (after
400 seconds). Figure 7(b) shows the client’s video through-
put are mostly too low when there is a competing ﬂow.4 If
we look at the progression of cwnd for the video ﬂow after it
resumes from a pause, we can tell how the server opens up
the window diﬀerently when there is a competing ﬂow. Be-
cause we don’t control the server (it belongs to the CDN) we
instead use our local proxy to serve both the video traﬃc and
the competing ﬂow, and use the tcp_probe kernel module

4In Figure 7(b), the bottleneck bandwidth is set to 5Mb/s so
that the available fair-share of bandwidth (2.5Mb/s) is the
same as in Figure 7(a). Note that some segment downloads
are able to get more than its fair share; in these cases, the
competing ﬂow experiences losses and has not ramped up to
its fair share yet. This is the reason why some of the CDF
curves does not end with 100% at 2.5Mb/s in Figure 7(b).

to log the cwnd values. The video traﬃc here is generated
by requesting a 235kbps video segment. Figure 8(a) shows
how cwnd evolves, starting from the initial value of 10 at 1.5
seconds, then repeatedly being beaten down by the competing
wget ﬂow. The competing wget ﬂow has already ﬁlled the
buﬀer during the OFF period, and so the video ﬂow sees
very high packet loss. Worse still, the segment is ﬁnished
before cwnd climbs up again, and we re-enter the OFF pe-
riod. The process will repeat for every ON-OFF period, and
the throughput is held artiﬁcially low.

For comparison, and to understand the problem better,
Figure 8(b) shows the result of the same experiment with a
segment size ﬁve times larger. With a larger segment size,
the cwnd has longer to climb up from the initial value; and
has a much greater likelihood of reaching the correct steady
state value.

Now that we know the video throughput tends to be low
(because of TCP), we would like to better understand how
the client reacts to the low throughputs. We can track the
client’s behavior as we steadily reduce the available band-
width, as shown in Figure 9. We start with a bottleneck link
rate of 5Mb/s (and no competing ﬂow), drop it to 2.5Mb/s
(to mimic a competing ﬂow), and then keep dropping it
by 100kb/s every 3 minutes. The dashed line shows the
available bandwidth, while the solid line shows the video
rate picked by the client. The client chooses the video rate
conservatively; when available bandwidth drops from from
5Mb/s to 2.5Mb/s, the video rate goes down to 1400kb/s,
and so on.

We can now put the two pieces together. Consider a client
streaming at a playback rate of 1750kb/s, the median video
throughput it perceives is 1787kb/s as shown in Figure 7(b).
According to Figure 9, with a video throughput of 1787kb/s,
the client reduces its playback rate to 1050kb/s. Thus, 50%
of the time the playback rate will go down to 1050kb/s once
the competing ﬂow starts.

It is interesting to observe that the Service A client is
behaving quite rationally given the throughput it perceives.
The problem is that because Service A observes the through-
put above TCP, it is not aware that TCP itself is having

231232 
)

%

(
 
F
D
C

100

80

60

40

20

0
0

Video Rate 650kb/s
Video Rate 1000kb/s
Video Rate 1500kb/s
Video Rate 2000kb/s
Video Rate 2500kb/s
Video Rate 3200kb/s

500

1000

1500

2000

2500

Throughput (kb/s)

 
)

%

(
 
F
D
C

100

80

60

40

20

0
0

Video Rate 650kb/s
Video Rate 1000kb/s
Video Rate 1500kb/s
Video Rate 2000kb/s
Video Rate 2500kb/s
Video Rate 3200kb/s

500

1000
1500
Throughput (kbps)

2000

2500

(a) Service B with no competing ﬂow.

(b) Service B with one competing ﬂow.

Figure 16: (Service B) The TCP throughput changes in the presence of a competing ﬂow.

)

%

(
n
o

i
t
c
a
r
F

1.0

0.8

0.6

0.4

0.2

0.0

102

103
104
OFF Duration(ms)

105

1.0

0.8

0.6

0.4

0.2

)

%

(
n
o
i
t
c
a
r
F

0.0

0

2500
Download size between successive zero window ads (KBytes)

2000

1000

1500

500

Figure 14: (Service B) Almost all the OFF periods in
a single video session are greater than RTO (200ms).

Figure 15: (Service B) When the video stream is
receiver-limited, the client does not request many
bytes during an ON period.

video throughput is higher — hence the Service A client
picks a higher rate (1050kb/s).

For comparison, we asked 10 volunteers to rerun this ex-
periment with Service A in their home network connected
to diﬀerent ISPs, such as AT&T DSL, Comcast, Verizon
and university residences. Even though there was suﬃcient
available bandwidth for the highest video rate in the pres-
ence of a competing ﬂow, seven people reported a rate of
only 235kb/s-560kb/s.

5.5 Service B

Service B also exhibits ON-OFF behavior, but at the TCP
level and not the HTTP level, i.e., the pause could happen
while downloading a video segment. When its video play-
back buﬀer is full, the client stops taking data from the
TCP socket buﬀer. Eventually, the TCP socket buﬀer also
ﬁlls and triggers TCP ﬂow control to pause the server by
sending a zero window advertisement. In Figure 4(b), each

zero window advertisement is marked by a hexagon. The
client starts issuing zero window advertisements at around
100s and continues to do so until a few seconds after the
competing ﬂow starts. Figure 14 shows the CDF of the du-
ration of the OFF periods. Almost all the pauses are longer
than 200ms, so cwnd is reset to its initial value. Thus, Ser-
vice B eﬀectively exhibits an ON-OFF behavior similar to
that of Service A.

Worse still, during an ON period, Service B does not re-
quest many bytes; Figure 15 shows that over half of the time,
it reads only 800kbytes, which is not enough for the cwnd
to climb up to its steady state before the next OFF period.
Figure 4(b) and Figure 16(b) show the result, that the TCP
throughput is only around 1Mbps to 1.5Mbps, causing Ser-
vice B to pick a video rate of 1000kb/s, or even 650kb/s.
As we saw earlier, when competing with another ﬂow, the
smaller the request, the higher the likelihood of perceiving

2332345000

4000

3000

)
s
/

b
k
(
 

e

t

Buffer Status

Video Throughput

250

200

150

100

50

)
d
n
o
c
e
S

(
 
r
e

f
f

u
B
n

 

i
 
s
t

n
e
m
g
e
S
o
e
d
V

 

i

5000

4000

3000

)
s
/

b
k
(
 

e

t

 

a
R
o
e
d
V

i

2000

1000

0
0

Buffer Status

Video Throughput

250

200

150

100

50

Video Rate

200

400

600

800

Time (s)

0
1000

)
d
n
o
c
e
S

(
 
r
e

f
f

u
B
n

 

i
 
s
t

n
e
m
g
e
S
o
e
d
V

 

i

 

a
R
o
e
d
V

i

2000

1000

0
0

Video Rate

200

400

600

800

Time (s)

0
1000

Figure 20: Custom client, similar to Service A –
equally conservative, with a 10-sample moving aver-
age ﬁlter – displays the downward spiral.

Figure 21: Custom client – with 10% conservatism,
but with a 10-sample moving average ﬁlter.

a reasonable baseline. For Service A, Figure 9 indicates the
bandwidth below which the client picks a lower video rate.
Assume that Service A estimates bandwidth by simply di-
viding the download size by the download time and passing
it through a ﬁxed-size moving-average ﬁlter. We can esti-
mate the size of the ﬁlter by measuring how long it takes
from when the bandwidth drops until the client picks a new
rate. A number of traces from Service A suggest a ﬁlter with
10 samples, though the true algorithm is probably more nu-
anced.

To closely mimic the Service A client, our custom client re-
quests the video segments with the same sizes from the same
locations in the CDN: we capture the segment map given to
the client after authentication, which locates the video seg-
ments for each supported playback rate. Hence, our custom
client will experience the same segment-size variation over
the course of the movie, and when it shifts playback rate,
the segment size will change as well. Since our custom client
uses tokens from an earlier playback, the CDN cannot tell
the diﬀerence between our custom client and the real Service
A client. To further match Service A, the playback buﬀer
is set to 240 seconds, the client uses a single persistent con-
nection to the server, and it pauses when the buﬀer is full.
We ﬁrst validate the client, then consider three changes: (1)
being less conservative, (2) changing the ﬁltering method,
and (3) aggregating segments.

6.2 Validating our Custom Client

Figure 20 shows the custom client in action. After down-
loading each segment, the custom client selects the playback
rate based on Service A’s conservative rate selection algo-
rithm, observed in Figure 9. Once the playback buﬀer is
full, we introduce a competing ﬂow. Like the real client,
the playback rate drops suddenly when the competing ﬂow
starts, then ﬂuctuates over the course of the movie. The
downward spiral does not bottom out, which we suspect is
due to some subtle diﬀerences between Service A’s algorithm
and ours.

5000

4000

)
s
/
b
k
(
 

3000

t

 

e
a
R
o
e
d
V

i

2000

1000

0
0

Buffer Status

Video Throughput

Video Rate

250

200

150

100

50

200

400

600

800

Time (s)

0
1000

)
d
n
o
c
e
S

(
 
r
e

f
f

u
B
n

 

i
 
s
t

n
e
m
g
e
S
o
e
d
V

 

i

Figure 22: Custom client – with 10% conservatism,
and with an 80th-percentile ﬁlter.

6.3 Less Conservative

Bandwidth estimates based on download sizes and dura-
tions tend to under-report the available bandwidth, espe-
cially in the presence of a competing ﬂow. If the algorithm
is conservative, it exacerbates the problem. We try a less
conservative algorithm, with a conservatism of 10% instead
of 40%. Conservatism of 40% means the client requests for
a video rate of at most 1.2Mb/s when it perceives 2.0Mb/s,
while 10% means it requests at most 1.8Mb/s when per-
ceiving 2.0Mb/s. According to Figure 9, Service A requests
video rate with a conservatism of approximately 40%. Fig-
ure 21 shows that the video rate is higher, even though
the playback buﬀer stays full. The result is higher qual-
ity video, high playback buﬀer occupancy (i.e.
resilience
against rebuﬀering) and four minutes of buﬀering to respond
to changes in bandwidth. Note that even though the algo-
rithm is less conservative, the underlying TCP ensures the
algorithm stays a “good citizen” and only gets its fair share
of available bandwidth.

2355000

4000

3000

)
s
/

b
k
(
 

e

t

 

a
R
o
e
d
V

i

2000

1000

0
0

Buffer Status

Video Throughput

Video Rate

250

200

150

100

50

200

400

600

800

Time (s)

0
1000

)
d
n
o
c
e
S

(
 
r
e

f
f

u
B
n

 

i
 
s
t

n
e
m
g
e
S
o
e
d
V

 

i

Figure 23: Custom client with increased segment
size (5x).

6.4 Better Filtering

Averaging ﬁlters provide a more stable estimate of band-
width, but a single outlier can confuse the algorithm. For
example, a few seconds of low-information movie credits re-
duces the segment size and the algorithm might drop the
rate. In place of averages, we consider medians and quan-
tiles to reduce the vulnerability to outliers. Figure 22 shows
what happens if we use the 80th-percentile of measured rate
of the past ten segment download. Variation is greatly re-
duced, and the majority of the movie plays at the highest-
available rate. The playback buﬀer has small ﬂuctuations,
but it is still far from a rebuﬀer event.

6.5 Bigger Segments

As noted earlier, bigger segments provide better estimates
of the available bandwidth, allowing TCP to escape slow-
start. Figure 23 shows what happens if our client aggregates
ﬁve requests into one. With the larger segment size, the
video throughput is more stable, and both the playback rate
and buﬀer size are more stable.

In summary, larger segments let TCP reach its fair share
and improve the video throughput. Picking higher rates
less conservatively and ﬁltering measurements more care-
fully can improve video quality. But we should note that
these improvements are for one movie on one service. Given
the prevalence of the downward spiral eﬀect, these should
not be interpreted as hard recommendations, merely as added
detail to our understanding of the problem.

7. RELATED WORK

The related work largely considers three overlapping ar-
eas: systems for video streaming; measurements to under-
stand their performance, and the design and analysis of rate
selection algorithms.

Video Streaming Services. The ﬁrst category covers
video streaming approaches using HTTP, such as the com-
mercial ones from Adobe, Apple, and Microsoft described
in [22], which diﬀer in their alignment of video switching
rates, whether A/V streams are combined, and whether re-
quests are issued as byte ranges or for pre-speciﬁed seg-
ments. A more recent technique is MPEG DASH (Dynamic
Adaptive Streaming over HTTP) [7] which standardizes the

formatting of video content and leaves open the speciﬁc
client player algorithm. These techniques underpin the ma-
jor commercial services like YouTube, Netﬂix, and Hulu.

Video Streaming Measurement. The second cate-
gory measures the performance of individual video streaming
clients experiencing local traﬃc conditions (“in the lab”), all
the way to distributed measurement systems that compare
the performance of thousands of clients (“in the wild”).

The work most similar to ours is [3], where the authors
also parse HTTP messages to determine playback rates and
use a bandwidth limiter to test clients under varying network
conditions. However, [3] focuses on the unfairness problem
among two video players, while in this work we focus on
the unfairness problem between a video player and a long-
lived TCP ﬂow. This paper considers a signiﬁcantly diﬀer-
ent scenario: it focuses on a video client competing against
another video client. In this context, they observe similar
pathologies: poor bandwidth estimation, leading to insta-
bility. However, they explain their observations entirely in
terms of the application-layer ON-OFF behavior of video
clients; even if one video client perfectly obtained its fair
share when ON, it can fail to correctly estimate available
bandwidth (depending on the amount of overlap with the
ON periods of the other client). By contrast, our paper
demonstrates that this is only a symptom of a more gen-
eral problem: inaccurate bandwidth estimation occurs even
when the competing ﬂow does not exhibit ON-OFF behav-
ior. As we show in this paper, the problem arises because
it is hard to estimate bandwidth above TCP. Others have
identiﬁed the same problem but not explained its causes or
validated potential ﬁxes [4, 16].

Measuring the CDN servers rather than clients provides
diﬀerent insights. In [1], the authors examine the CDN se-
lection strategy of Hulu, while in [2], the authors look at
Netﬂix. Both papers ﬁnd a predisposition for clients to stay
with the original CDN, despite variation between CDNs and
over time. In [9], the authors describe lessons learned from a
distributed commercial measurement system to understand
the eﬀects of Quality-of-Experience (QoE) metrics on viewer
engagement and retention. Rebuﬀer rates and average video
quality are QoE metrics with measurable impacts on viewer
engagement, which underscores the importance of getting
rate measurement and selection right in the presence of com-
peting ﬂows. With the measurement-driven insights from
the same system, [14] proposes a global video control plane
to dynamically assign clients a choice of video rate and CDN
that optimizes viewers’ experience.

Other work looks at network characteristics of video stream-

ing traﬃc, rather than focusing on the client or viewer expe-
riences [11, 19, 24]. In particular, the authors in [19] show
ON-OFF cycle behavior for YouTube and Netﬂix and use a
model to study aggregates of video client and their eﬀects
on the network. Both CDN and network traﬃc papers do
not consider local eﬀects on measured bandwidth or their
eﬀects on rate stability.

Rate Selection Algorithms. The third category is
work on rate selection algorithms. This work complements
ours, as a control system always beneﬁts from more accu-
rate measurements. In [8], the authors propose an algorithm
to maintain the playout buﬀer at a target level.
In [17],
the authors implement a diﬀerent buﬀer-aware rate selection
algorithm and experimentally measure user preferences for
gradual and infrequent playback rate changes. In [23], the

236authors model the rate selection problem as a Markov Deci-
sion Process and use a dynamic programming technique to
choose a streaming strategy that improves QoE. In [13], the
authors use simulations to show how parallel HTTP sessions
can improve playback quality. Server-side pacing is another
approach to selecting rate used by YouTube, as described
in [10, 12].

8. CONCLUSION

Despite some diﬀerences in speciﬁc service implementa-
tions, all three services we study display degraded perfor-
mance in the presence of competing traﬃc, well below the
video quality possible if the client used its fair share of band-
width. At a high level, our measurement analysis and ex-
periments suggest that the root cause of this failure is a
lack of information. The HTTP layer is simply not privy to
continuous high-ﬁdelity feedback about the fair share at the
bottleneck link.

There are two ways to interpret our observations. On one
hand, we observe that determining the fair share of band-
width available at the bottleneck is precisely the role of TCP.
Thus, one path forward might be to suggest that we should
design the client to improve information ﬂow from TCP to
the HTTP layer. In particular, we should ensure that TCP
has a chance to reach its steady-state fair share; for example,
increasing the segment size enables this eﬀect.

However, we believe there may be a more radical solu-
tion: do not attempt to estimate bandwidth at all! The video
streaming client has two competing goals: attain the highest
bitrate possible while avoiding buﬀer underruns. Thus the
objective is not to ensure the buﬀer stays full; the objective
is to ensure the buﬀer does not go empty. Since the buﬀer
holds several minutes of video, this shift in perspective sug-
gests that if the buﬀer is full then the client has picked a rate
that is too low. Rather, the client should increase the bitrate
when the buﬀer is high and decrease it when the buﬀer falls
low. Though this sounds aggressive, note that it is exactly
the correct layer separation:
it hands oﬀ to TCP the ob-
jective of obtaining the fair share of bandwidth, and tries
to always ensure the client picks the highest rate possible.
This suggests an intriguing path forward for future research:
design video-streaming clients that deliver high performance
by eliminating bandwidth estimation all together.

Acknowledgment
We are grateful to the anonymous reviewers and our shep-
herd Nina Taft for their valuable comments and feedback,
which helped improve the ﬁnal version. The authors would
also like to thank Kok-Kiong Yap, Masayoshi Kobayashi,
Vimalkumar Jeyakumar, Yiannis Yiakoumis and Netﬂix en-
gineers for helpful discussions that shaped the paper. This
work was supported by Mr. and Mrs. Chun Chiu Stanford
Graduate Fellowship, Hewlett-Packard Fellowship, the Stan-
ford Clean Slate Program, and the National Science Foun-
dation under grants CNS-0904609, CNS-0644114, and CNS-
0832820.

9. REFERENCES

[1] V. Adhikari, Y. Guo, F. Hao, V. Hilt, and Z.-L. Zhang.

A Tale of Three CDNs: An Active Measurement
Study of Hulu and its CDNs. In Proceedings of IEEE

Conference on Computer Communications Workshops
(INFOCOM WKSHPS), pages 7–12, March 2012.

[2] V. K. Adhikari, Y. Guo, F. Hao, M. Varvello, V. Hilt,

M. Steiner, and Z.-L. Zhang. Unreeling Netﬂix:
Understanding and Improving Multi-CDN Movie
Delivery. In Proceedings of the IEEE INFOCOM 2012,
Orlando, FL, USA, pages 1620–1628, March 2012.
[3] S. Akhshabi, L. Anantakrishnan, C. Dovrolis, and
A. Begen. What Happens When HTTP Adaptive
Streaming Players Compete for Bandwidth? In
Proceedings of the ACM Workshop on Network and
Operating Systems Support for Digital Audio and
Video (NOSSDAV), June 2012.

[4] S. Akhshabi, C. Dovrolis, and A. Begen. An
Experimental Evaluation of Rate Adaptation
Algorithms in Adaptive Streaming over HTTP. In
Proceedings of the ACM Multimedia Systems
Conference (MMSys), San Jose, CA, USA, Feburary
2011.

[5] M. Allman, V. Paxson, and E. Blanton. TCP

Congestion Control. RFC 5681 (Draft Standard),
Sept. 2009.

[6] M. Allman, V. Paxson, and W. Stevens. TCP

Congestion Control. RFC 2581 (Proposed Standard),
Apr. 1999. Obsoleted by RFC 5681, updated by RFC
3390.

[7] MPEG DASH speciﬁcation (ISO/IEC DIS 23009-1.2),

2011.

[8] L. De Cicco, S. Mascolo, and V. Palmisano. Feedback

Control for Adaptive Live Video Streaming. In
Proceedings of the ACM Multimedia Systems
Conference (MMSys), Febrary 2011.

[9] F. Dobrian, A. Awan, D. Joseph, A. Ganjam, J. Zhan,

V. Sekar, I. Stoica, and H. Zhang. Understanding the
Impact of Video Quality on User Engagement. In
Proceedings of the ACM SIGCOMM, Toronto,
Canada, August 2011.

[10] M. Ghobadi, Y. Cheng, A. Jain, and M. Mathis.

Trickle: Rate Limiting YouTube Video Streaming. In
Proceedings of the USENIX Annual Technical
Conference (ATC), page 6, 2012.

[11] P. Gill, M. Arlitt, Z. Li, and A. Mahanti. Youtube

Traﬃc Characterization: A View From the Edge. In
Proceedings of the ACM SIGCOMM conference on
Internet Measurement (IMC), pages 15–28, 2007.

[12] L. Kontothanassis. Content Delivery Considerations
for Diﬀerent Types of Internet Video. In Proceedings
of the ACM Multimedia Systems Conference (MMSys)
– Keynote, Chapel Hill, NC, USA, Febrary 2012.

[13] C. Liu, I. Bouazizi, and M. Gabbouj. Parallel

Adaptive HTTP Media Streaming. In Proceedings of
the IEEE International Conference on Computer
Communications and Networks (ICCCN), pages 1–6,
2011.

[14] X. Liu, F. Dobrian, H. Milner, J. Jiang, V. Sekar,
I. Stoica, and H. Zhang. A Case for a Coordinated
Internet Video Control Plane. In Proceedings of the
ACM SIGCOMM, Helsinki, Finland, August 2012.

[15] J. W. Lockwood, N. McKeown, G. Watson, G. Gibb,

P. Hartke, J. Naous, R. Raghuraman, and J. Luo.
NetFPGA–An Open Platform for Gigabit-Rate
Network Switching and Routing. In MSE ’07:

237Proceedings of the 2007 IEEE International
Conference on Microelectronic Systems Education,
pages 160–161, 2007.

[16] K. Miller, E. Quacchio, G. Gennari, and A. Wolisz.

Adaptation algorithm for adaptive streaming over
HTTP. In Proceedings of the IEEE International
Packet Video Workshop (PV), pages 173–178, May
2012.

[17] R. Mok, X. Luo, E. Chan, and R. Chang. QDASH: a
QoE-aware DASH system. In Proceedings of the ACM
Multimedia Systems Conference (MMSys), pages
11–22, Febrary 2012.

[18] Sandvine: Global Internet Phenomena Report. http:

//www.sandvine.com/news/pr_detail.asp?ID=312.
[19] A. Rao, A. Legout, Y. Lim, D. Towsley, C. Barakat,

and W. Dabbous. Network characteristics of video
streaming traﬃc. In Proceedings of the ACM
COnference on emerging Networking EXperiments and
Technologies (CONEXT), page 25. ACM, 2011.

[20] RTMPDump. http://rtmpdump.mplayerhq.hu/.
[21] Consumer Report: Streaming Video Services Rating.

http://www.consumerreports.org/cro/magazine/
2012/09/best-streaming-video-services/.

[22] M. Watson. HTTP Adaptive Streaming in Practice. In

Proceedings of the ACM Multimedia Systems
Conference (MMSys) – Keynote, San Jose, CA, USA,
Febrary 2011.

[23] S. Xiang, L. Cai, and J. Pan. Adaptive Scalable Video
Streaming in Wireless Networks. In Proceedings of the
ACM Multimedia Systems Conference (MMSys), pages
167–172, Febrary 2012.

[24] M. Zink, K. Suh, Y. Gu, and J. Kurose.

Characteristics of YouTube network traﬃc at a
campus network - Measurements, models, and
implications. In Computer Networks, Volume 53, Issue
4, pages 501–514. Elsevier, 2009.

238