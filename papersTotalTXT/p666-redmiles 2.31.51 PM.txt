How I Learned to be Secure: a Census-Representative

Survey of Security Advice Sources and Behavior

Elissa M. Redmiles, Sean Kross†, and Michelle L. Mazurek

University of Maryland

†Johns Hopkins University

ABSTRACT
Few users have a single, authoritative, source from whom
they can request digital-security advice. Rather, digital-
security skills are often learned haphazardly, as users ﬁl-
ter through an overwhelming quantity of security advice.
By understanding the factors that contribute to users’ ad-
vice sources, beliefs, and security behaviors, we can help
to pare down the quantity and improve the quality of ad-
vice provided to users, streamlining the process of learn-
ing key behaviors. This paper rigorously investigates how
users’ security beliefs, knowledge, and demographics corre-
late with their sources of security advice, and how all these
factors inﬂuence security behaviors. Using a carefully pre-
tested, U.S.-census-representative survey of 526 users, we
present an overview of the prevalence of respondents’ ad-
vice sources, reasons for accepting and rejecting advice from
those sources, and the impact of these sources and demo-
graphic factors on security behavior. We ﬁnd evidence of a
“digital divide” in security: the advice sources of users with
higher skill levels and socioeconomic status diﬀer from those
with fewer resources. This digital security divide may add to
the vulnerability of already disadvantaged users. Addition-
ally, we conﬁrm and extend results from prior small-sample
studies about why users accept certain digital-security ad-
vice (e.g., because they trust the source rather than the con-
tent) and reject other advice (e.g., because it is inconvenient
and because it contains too much marketing material). We
conclude with recommendations for combating the digital
divide and improving the eﬃcacy of digital-security advice.

1.

INTRODUCTION

It is easy to identify how most people learn the majority
of basic skills:
learning to ride a bicycle from parents, al-
gebra from school, and how to make paper airplanes from
childhood friends. The origins of users’ security behaviors,
however, are less obvious. The majority of today’s users
did not have parents or teachers who could instruct them
about the dangers of computers and the internet. Instead,

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978307

users collect digital-security advice haphazardly from a va-
riety of sources including workplaces, the media, and sto-
ries of negative experiences that have happened to family
and friends [9, 47, 48]. While a plethora of security advice
is available from seemingly authoritative sources such as US
CERT and Microsoft [1,2], and yet more is provided casually
by friends and acquaintances, users adopt only a fraction of
it. Further, it is unclear how useful and eﬀective advice from
these various sources may be, or whether the advice that is
accepted is the most valuable.

Little eﬀort, however, has gone toward understanding how
and why users opt to adopt some recommended behaviors
but reject others. Most prior research has instead focused
on teaching users individual security-promoting behaviors
such as phishing awareness [7, 11, 18, 21, 41, 50, 53, 56, 63]. A
smaller set of prior work has hypothesized general models
for understanding how security behaviors develop, but these
models either have not been empirically validated [26] or
have been based on small samples of 25 users or fewer [9,48].
In this work, we present the ﬁrst large-scale empirical
analysis of how users’ security beliefs, knowledge, and de-
mographics correlate with their sources of security advice,
and how all these factors inﬂuence security behaviors. We
ﬁnd the ﬁrst documented evidence of a digital divide with re-
spect to security. In so doing, we investigate which users take
advice from which sources, why they take that advice, and
what they believe to be the purpose of security-relevant be-
haviors. Our results derive from a census-representative sur-
vey of 526 U.S. residents, which supports statistically gen-
eralizable conclusions regarding user learning, beliefs, and
behavior [8]. Our survey queried respondents’ behaviors,
advice sources, reasoning, and beliefs across four digital-
security domains identiﬁed as highly important by experts [32]:
password strength, antivirus use, software updating, and
two-factor authentication. To enable comparisons between
digital security and the more well-developed domain of phys-
ical security, we asked similar questions about the securing of
exterior doors in respondents’ homes. We also collected par-
ticipants’ demographics, technical and security knowledge,
and internet skill level. Our major ﬁndings include:

• Digital Divide. We are the ﬁrst to document a con-
crete digital divide with respect to security. More
speciﬁcally, we ﬁnd that higher-skilled users, who tend
to be socioeconomically advantaged, are signiﬁcantly
more likely to take advice from their workplace and to
have the skills necessary to learn from a negative ex-
perience. In contrast, those with lower skills tended to
take advice from family, friends and service providers

666(e.g. TimeWarner). This divide may increase the vul-
nerability of already disadvantaged users, who in our
study also self-report fewer security practices.

• Advice Rejection. Unsurprisingly, respondents most
often rejected security behaviors because they were in-
convenient. However, they rejected advice nearly as
often for containing too much marketing material or
because they had not yet had a negative experience.
These ﬁndings support the need not just to make ad-
vice simpler and less intrusive, but also to minimize
marketing messaging. They also conﬁrm and extend
prior work about the importance of negative experi-
ence stories [47], which we ﬁnd to be not only an eﬀec-
tive teaching tool but in some cases almost a prereq-
uisite for certain security behaviors.

• Advice Acceptance. The majority of respondents
accept advice based either on their evaluation of the
advice content or their trust of the advice source. For
passwords and physical security, respondents rely on
their own evaluation of the content, perhaps because
they feel comfortable with these concepts. With regard
to updating, antivirus, and two-factor authentication,
however, users are more likely to rely on their evalua-
tion of the trustworthiness of the advice source. This
may indicate a need to simplify advice on these topics
in order to enable users to evaluate the advice content,
independent of the source; alternatively, this may in-
dicate the need to eﬀectively signal source credibility.

Based on these results, we provide recommendations for
combating the digital security divide and to carefully tar-
get a small set of critical advice, boosting its impact while
reducing the burden of security advice overload.

2. RELATED WORK

In this section, we review prior work discussing factors
that inﬂuence security behavior, the value of diﬀerent secu-
rity recommendations, and how users learn security behav-
iors.
2.1 Factors that Inﬂuence Security Behavior
Previous research has focused on demographic and social
factors as well as beliefs and mental models that inﬂuence
users’ security behaviors. Howe et al. examined how the
belief that one’s information is not “important enough to
hack” can inﬂuence security behavior, especially among low-
socioeconomic status users [30]. Sheng et al. and Halevi et
al. both examined the impact of personality factors on dif-
ferent security behaviors [22,55]. Das et al. found that social
factors, such as peer-pressure, can make users more likely to
adopt the same security behaviors as their friends [12, 13].
Similarly, negative experiences or stories from friends about
negative experiences were found to aﬀect users security deci-
sions and beliefs [47]. Yet other work has shown the impact
of mental models on users’ security choices. Wash investi-
gated how mental models of security inﬂuence users’ per-
ceptions of what is and is not dangerous [65]. Following on
this initial qualitative work, Wash and Rader, in a large,
census-balanced survey of U.S. internet users, found that
those who were more educated tended to take fewer secu-
rity precautions, despite having more advanced beliefs [66].
We extend this prior work to more generally evaluate the

impact of demographic factors, technical background and
exposure to security training, and security beliefs on how
security behavior is learned.

A few generalized models have been proposed to explain
users’ security decisions. Herley suggests that users make a
rational cost-beneﬁt calculation when choosing what secu-
rity advice or behaviors to accept [26]. Relatedly, Beautment
et al. suggests a ‘compliance budget’ model, wherein users’
limited resources that dictate their choice of security behav-
iors [9, 27]. However, the former model has not been empir-
ically validated, and the latter was evaluated with a small,
qualitative sample. We seek to extend this work by de-
veloping a generalizable quantitative model to explain user
behavior as a factor of decision-making, advice sources, de-
mographics, knowledge, and beliefs. We provide actionable
results not only about why users practice diﬀerent security
behaviors, but how they learn these behaviors.
2.2 Evaluation of Security Recommendations
In any study of how users learn security behaviors, it is
important to keep in mind which behaviors are most nec-
essary for users’ safety.
In a recent survey, Ion et al. col-
lected responses from more than 200 security experts re-
garding which security behaviors they most strongly recom-
mend [32]. In our survey, we asked respondents about their
behavior, decision-making, and beliefs with regard to four of
the top suggestions from this paper: software updates, two-
factor authentication, antivirus software, and using strong
passwords. By studying how users learn security behaviors
and ﬁnd sources of advice, we can develop interventions to
better enable the transference of these expert best-practices
to general users.
2.3 Security Learning

In addition to investigating the impact of demographic
factors and mental models on security behavior, and deter-
mining which behaviors are most important to learn, there
has been some study of where users learn behaviors. Prior
qualitative work concludes that users often learn from ex-
perience and the advice of family and friends [19, 20, 47, 48].
This paper builds on such qualitative work, especially our
prior examination of users’ advice sources, reasoning, and
beliefs [48]. We build on those exploratory results, evaluat-
ing them at scale and generating statistically generalizable
models of users’ security-learning mechanisms that can be
used in developing new educational interventions.

Additionally, work by Whitman examined user learning
of security behavior in work environments [67], although lit-
tle investigation has been done to verify whether workplace
behaviors translate to the home environment. We expand
upon this prior work with a larger and more representative
sample and deeper inquiry into how users learn behaviors,
not just from where they learned them. Further, we exam-
ine the transition of security knowledge from the workplace
to the home computing environment.

A large body of work has focused on improving the ef-
ﬁcacy of security behavior teaching tools. This work has
touched upon improving phishing education [7,21,41,50,56],
developing more eﬀective warnings [6,15,54,61,69], reducing
security-warning fatigue [10], and teaching users to create
strong passwords [11, 18, 53, 63]. Speciﬁcally in the realm of
advice, Rader and Wash recently examined security advice
itself, using topic modeling to analyze connections between

667user security decisions and the topics and words in three
types of security advice [46]. However, to enable large-scale
improvement in user security, we must go beyond examin-
ing the content of speciﬁc resources and improving these
resources for individual behaviors. We need to develop a
general model to understand how users learn digital secu-
rity behaviors and consequently model the factors, such as
advice sources and beliefs, that aﬀect users’ security. Such
a model can enable the development and targeting of inter-
ventions to improve a user’s security level in general, rather
than behavior-by-behavior.

3. METHODOLOGY

We conducted a computer-administered, closed-answer sur-
vey of a census-balanced sample of 526 respondents in April
2016 via the Survey Sampling International panel. To ensure
our survey instrument could produce generalizable and rig-
orous results, we pre-tested our questionnaire by conducting
cognitive interviews and expert reviews. These methods are
best practices in survey methodology for minimizing biases
and improving validity in survey data collection [45].

This study was approved by the University of Maryland
Institutional Review Board. Below, we discuss our survey
development process, sampling procedure, details of our sta-
tistical analysis, and limitations of our work.
3.1 Survey Development

Our survey queries respondents’: digital and physical se-
curity advice sources, reasoning for accepting or rejecting ad-
vice from these sources, beliefs about the purpose and value
of diﬀerent digital-security behaviors, and general opinions
regarding the importance and utility of digital- and physical-
security advice. In order to gain a sampling of respondents’
advice sources and reasoning across diﬀerent digital-security
domains, we asked questions about their behaviors, advice
sources, reasoning, and beliefs with regard to four behav-
iors highly recommended by security experts [32]: password
strength, antivirus use, software updating, and two-factor
authentication. To enable comparisons between digital and
physical security, we asked similar questions about the se-
curing of exterior doors in the respondent’s home.

In addition to asking standardized demographic questions
regarding respondents’ age, race, gender, education level,
and income, we also asked whether respondents had ever
held a government security clearance, and if not, whether
they currently work with data governed by HIPAA (U.S.
health-privacy regulation) or FERPA (U.S. student-privacy
regulation). We refer to these participants collectively as
sensitive-data participants. Further, we asked whether re-
spondents held a degree in or worked in the ﬁelds of com-
puter science, computer engineering or IT. We also adminis-
tered the six-item web-use skills index to assess the respon-
dent’s technical skill level [25]. We used this to explore how
exposure to a security-sensitive mindset, educational back-
ground or work experience on computer science or IT, and
technical skill, respectively, inﬂuence users’ learning mecha-
nisms and security behaviors.
3.1.1 Cognitive Interviews
After developing the initial set of questions, we conducted
cognitive interviews with ﬁve demographically-diverse par-
ticipants (see Table 1). Cognitive interviewing is a method
of pre-testing questionnaires that provides insight into how

Gender

Age

Race

Educ.

Income

M
M
M
F
F

Asian

40-49 yrs
18-29 yrs Hispanic
Black
30-39 yrs
50-59 yrs
Black
40-49 yrs White

B.S.
M.S.

Some College
High School

B.S.

$100-$125k

$30-$50k
$30-$50k

<$30k

$50-$75k

Table 1: Cognitive Interview Demographics.

respondents interpret and answer questions, so that errors
can be corrected before deployment [45, 68].

During the interview, participants were instructed to “think
aloud” as they answered each interview question via the
Qualtrics survey interface. After answering each survey item,
they were asked one of the following questions: “Was that
question hard to answer?”; “How did answering that ques-
tion make you feel?”; “Was there an answer choice missing or
one that you would have preferred?” Participants frequently
volunteered information about how they felt or missing an-
swer choices, even when unprompted. The results of these
interviews were used to iteratively revise and re-write certain
questions until they were clearly understood by respondents.
No participants reported ﬁnding the questionnaire stressful
nor the questions uncomfortable [14]. The cognitive inter-
views were 20 to 30 minutes in length.
3.1.2 Expert Reviews
After the third cognitive interview was complete, three ex-
perts reviewed our survey instrument to evaluate question
wording, ordering, and bias: our university’s statistical and
survey methodology consultant and two human-computer-
interaction researchers with survey expertise. Expert re-
viewing is another best practice typically used to identify
sensitive questions, questions that may need additional clar-
iﬁcation, and problems with question ordering or potential
biases [45]. We updated a number of our questions following
the expert review, and then completed cognitive interviews
until no additional questionnaire problems emerged.
3.1.3 Final Survey Instrument
The survey was administered via the Qualtrics web in-
terface. Each question was required, and a “Prefer not to
answer” choice was oﬀered for any questions identiﬁed as
sensitive by the researchers or the expert reviewers. Ad-
ditionally, sensitive questions and questions that may have
had social-desirability bias (in which the respondent may feel
socio-cultural pressure to respond in an “acceptable” man-
ner) were rewritten to reassure respondents that all answers
were acceptable, according to best practices [62]. For ex-
ample, a question asking whether or not respondents used
antivirus software was phrased as follows: “There are diﬀer-
ent reasons that people decide to use or not to use antivirus
software on their personal devices. Which of the following
best describes you:...”. A list of the ﬁnal survey questions
are provided as supplementary material.

The order in which the questions about each of the four
digital security behaviors were presented was randomized
to prevent order bias [40]. The physical-behavior question
was not included in the randomization, as the results of the
cognitive interviews showed that respondents needed to be
prepared for the topic switch to physical security and found
it cognitively challenging to switch between topics. In order
to improve the quality of the data collected, a commonly-
used attention check question was included: “Please select

668Unhappy as your answer choice to the following question.
This question is designed to check that you are paying at-
tention” [58]. Finally, demographic questions were placed
at the end of the questionnaire to minimize sensitivity and
bias, as per expert recommendations and best practices [52].
3.2 Representative Sample

We wanted to examine a representative sample of internet-
using adults in the United States. To this end, we fol-
lowed the American Association for Public Opinion Research
guidelines and used sample quotas to obtain obtain a census-
balanced sample of US adults for our survey [8]. We con-
tracted Survey Sampling International (SSI) to recruit re-
spondents who matched the US-census sample quotas on
the metrics of age, race, income and gender. SSI admin-
istered our Qualtrics survey to these respondents through
their platform, and compensated respondents according to
their agreement with SSI. Respondents were provided with
beneﬁts such as gift cards, airline frequent ﬂyer miles, and
donations to charities of their choice.

By using this large and representative sample, we can
make statistically signiﬁcant and broadly generalizable con-
clusions about user behavior, beliefs, and practices [8]. In
comparison, the majority of work on user security behavior
and learning is drawn from convenience samples on plat-
forms such as Amazon Mechanical Turk and also from small
qualitative lab studies [16, 32]. Prior work comparing Me-
chanical Turk samples with the general population has shown
these samples to have important limitations when general-
izing to the internet-using population both with regard to
demographics and privacy attitudes [33,36,51]. As such, our
work provides a more robust picture of user behavior at a
national scale than has prior work.
3.3 Statistical Analysis

In addition to presenting descriptive statistics regarding
the prevalence of respondents’ behaviors, advice sources,
reasoning, and beliefs, we compare Likert-scale factors be-
tween participant sub-groups (e.g. beliefs between sensitive-
data and general participants) using the Mann-Whitney U
test [42]. We also construct several logistic-regression mod-
els in an eﬀort to describe the relationship between respon-
dent security behavior and informal learning. Logistic re-
gression is a well-known statistical method for modeling
binary outcomes [29].
In order to avoid over-ﬁtting these
models, we used the standard backward-elimination tech-
nique, removing one factor from the model at a time, until
we minimize the Akaike Information Criterion (AIC) [5, 70].
We present the results of our models in Section 4.8. For
each model, we present the outcome variable, included and
eliminated factors, log-adjusted regression coeﬃcients (odds
ratios), 95% conﬁdence intervals, and p-values.

We use Pearson’s X 2 test to assess independence among
categorical variables, such as between respondents’ security
behavior and their sources of computer security informa-
tion [17]. For comparisons across many categories, we use
omnibus tests; if the omnibus tests are signiﬁcant, we then
apply pairwise tests selected a priori to compare individual
categories. One limitation of this method is that our data
contains repeated measurements of the same respondent, as
every respondent answered questions about multiple advice
sources and security behaviors. Pearson’s X 2 test does not
take into account this repeated measurement, meaning it is

possible reported test statistics are overstated; nonetheless
we believe X 2 is the most appropriate test for these analyses.
We interpret the results with this limitation in mind.
3.4 Limitations

As discussed in Section 3.2, our representative sample pro-
vides for robust, broadly generalizable results. It is currently
not possible to obtain a purely probabilistic sample via an
internet survey [8], as such we cannot precisely state the
prevalence of user behaviors and advice sources in the entire
U.S. population. Nonetheless, our work provides a strong
foundation for understanding national behaviors and trends.
As with any survey, some respondents may have selected
the ﬁrst answer that seemed to satisfactorily answer the
question, without thinking deeply about their own beliefs [39].
To mitigate this, we included an attention check question to
screen out inattentive participants and kept our question-
naire to 10 minutes in length, following expert recommen-
dations for minimizing respondent fatigue.

It is also possible that respondents mis-reported answers
in an eﬀort to answer in a socially desirable manner. How-
ever, we focus primarily on asking respondents to recall their
advice sources, about which signiﬁcant social desirability
bias seems unlikely; additionally, our questionnaire-testing
procedure revealed no evidence of social desirability bias.
Further, while we asked participants to search their mem-
ory for answers to our questions, they may not have fully
done so, or they may have forgotten some information. We
also assume that participants are largely able to correctly
identify which of their behaviors are security behaviors and
why they practiced those behaviors. Finally, it is possible
that our survey questions do not accurately assess the con-
structs we sought to measure. To mitigate these errors, we
extensively pre-tested the questionnaire. While we made ev-
ery eﬀort to eliminate errors and biases, as with any survey,
there may have still been lingering measurement errors.

4. RESULTS

In this section we describe our sample, the prevalence of
behaviors in our sample, our ﬁndings regarding respondents’
advice sources and beliefs, which users take advice from
which sources, their reasons for accepting and rejecting ad-
vice, and how their sources of advice aﬀect their security
behaviors.
4.1 Sample

Our sample is nearly representative of the demographics of
the United States with regard to age, education, gender, and
race. Our sample is very slightly wealthier than the general
population, potentially due to lack of internet or device ac-
cess among those earning less than $30,000. Additionally, we
had a 5% higher incidence of Caucasian respondents and a
5% lower incidence of Hispanic participants than in the gen-
eral population. This may be due to the fact that we used
a single “select all that apply” race and ethnicity question
which oﬀered both Hispanic (ethnicity) and White (race)
as answer choices to the same question. Table 2 compares
the demographics of our sample to the 2014 United States
Census [3].
4.1.1 Knowledge and Skills
To assess the knowledge and skills of our respondents,
we administered the extensively validated six-item web-use

669Metric

Male
Female

Caucasian
Hispanic

African American

Other

Some HS

Completed HS

Completed Some College

Associates Degree

College Degree

Master’s
Doctoral

18-29 years
30-39 years
40-49 years
50-59 years
60-69 years
70+ years

<$30k

$30k-$50k
$50k-$75k
$75k-$100k
$100k-$150k

$150k+

Sample Census

49%
50%

69%
11%
12%
8%

3%
23%
25%
10%
26%
10%
4%

22%
20%
19%
16%
15%
8%

26%
19%
17%
13%
14%
9%

49%
51%

64%
16%
12%
8%

8%
28%
18%
9%
26%
7%
4%

23%
17%
17%
18%
14%
11%

32%
19%
18%
11%
12%
8%

Table 2: Demographics of participants in our sam-
ple. Some percentages may not add to 100% due
to item non-response. Census statistics from the
American Community Survey [3].

skills index, which measures internet skills on a scale from
1 (low) to 5 (high) [25]. The mean for our participants is
3.75 (SD=0.99). This is slightly higher than the mean of
3.37 that would be anticipated from Hargittai and Hsieh’s
work developing this scale. However, our result still seems
reasonable, as Hargittai and Hsieh collected their data in
2010 and the internet skill level of the population has almost
certainly increased in the past six years. Additionally, thirty
percent of our respondents were ‘sensitive-data’ respondents
and 19% of our respondents held a degree in or worked in
the ﬁelds of computer science (CS), computer engineering,
or IT.
4.2 Security Behavior

Overall, we ﬁnd that 53% of respondents report making
stronger passwords for some accounts than for others and
84% of respondents report using antivirus software. Al-
though there has been no prior work requesting users to
self-report their antivirus use, our ﬁndings agree with prior
industry work by McAfee, which analyzed log data to de-
termine that 88% of computers had antivirus software in-
stalled [57].

The majority of our respondents updated
Updating.
their software: 37% reported updating all of their software
immediately and 41% reported updating their software a lit-
tle while after learning of updates. Only 5% of respondents
reported rarely or never updating their software, while 17%
of respondents reported updating some but not all software.
This self-report data contradicts the ﬁndings of Nappa et al.,
who used Symantec logs to ﬁnd that at most 14% of vulnera-
bilities were repaired when an exploit was released [43]. Fur-

thermore, our ﬁndings are also higher than those reported
by Ion et al. who reported that 25% of experts and 9%
of non-experts in their survey reported installing updates
“immediately” [32]. These discrepancies could have several
possible explanations: diﬀerent sample (Nappa et al. mea-
sured machines rather than people; Ion et al. used Amazon
Mechanical Turk while we used a census-representative sam-
ple), social desirability (although we believe our wording was
more neutral), an increase in user updating behavior since
the Ion et al. survey in 2014, diﬀerences in how diﬀerent re-
spondents interpret “immediately” updating their software,
and/or the explicit “Updates are installed automatically” op-
tion in used in Ion et al’s survey but not ours.

Two-Factor Authentication.
Finally, of our respon-
dents, 25% used 2FA on all of the devices or services that
oﬀered it; 45% used 2FA on some, but not all services; and
28% never used 2FA (2% NR). The proportion of our re-
spondents that use 2FA is higher than the rates cited in
prior work by Ion et al. [32]. This may reﬂect an increase
in 2FA adoption since the Ion et al. survey was conducted
in 2014; we also deﬁned 2FA for all respondents, which may
have prevented some under-reporting from participants who
did not recognize the term. We asked the 236 (45% of total)
respondents in our sample who used 2FA on some but not all
services why they used 2FA for those services. The major-
ity (62%) said they used 2FA on only some services because
they were required to do so by those platforms. Twenty-
eight percent of these 236 participants said that they used
2FA for the services that were more important to them. Very
few participants (8%) said that they activated 2FA on only
some services because it was easier to do so on those partic-
ular services. Of those who did not use 2FA for any services
(149, 28% of total), 64% had never seen information about
nor had been prompted to use this security strategy.
4.3 Beliefs about Behavior Purpose

To assess respondents’ beliefs about the purpose of secu-
rity behaviors, we asked respondents what they thought the
“primary reason” was for updating software and for using
2FA. We did not ask these questions for passwords and an-
tivirus use, as cognitive interview results showed that these
behaviors had a single, intuitive answer, and asking a ques-
tion deemed “obvious” by all respondents caused negative
survey sentiment.

Security was not believed to be the primary driver for
completing updates. The majority of respondents (40%) be-
lieved that the primary purpose was to “ensure the software
is free of bugs and crashes less often.” Twenty-nine percent
of respondents selected “to increase the security of the soft-
ware software” as the primary purpose, and 30% believed
the purpose was to get the “latest and greatest software fea-
tures” (1% NR). However, for 2FA, security was cited as the
primary purpose by the majority, 67% of respondents. The
remaining 21% of respondents believed that 2FA was used
to ensure that they could regain access to their account, and
10% believed 2FA was to enable the website to contact them
(2% NR).
4.4 Beliefs about Security Importance

In addition to asking respondents where they learned dig-
ital (and physical) security behaviors, we asked them to
compare the usefulness and trustworthiness of digital and
physical security advice using two 5-point Likert scales an-

670chored on “Digital is a lot more useful (trustworthy) than
physical” and “Physical is a lot more useful (trustworthy)
than digital”. Our prior qualitative work suggests that re-
spondents who handle sensitive data value digital-security
advice more highly [48].
In our sample, we found that
sensitive-data respondents were signiﬁcantly more likely to
ﬁnd digital-security advice more trustworthy than their non-
sensitive peers (MWU p<0.01 ), while they were not quite
signiﬁcantly more likely to ﬁnd digital-security advice more
useful (MWU p=0.08 ) . We hypothesize receiving digital-
security information and emphasis in the workplace leads to
higher levels of trust of security information in general, and
that implementing security practices in their routine work-
place activities leads to sensitive-data respondents regarding
digital-security advice as more useful.
4.5 How Users Learn Security Behaviors

For each behavior that a respondent reported completing,
we asked how they learned the behavior or what instigated
them to do the behavior. See Figure 1 for a summary of
respondents’ advice sources. Note that participants were
allowed to select multiple options; as a result, percentages
may add to more than 100%.

The majority of respondents (80%) cited device- or software-

based prompts or requirements as a reason for doing at least
one digital-security behavior. These prompts included pass-
word meters, update reminders, or invitations to use 2FA.
Further, 53% of respondents cited being required to use a
behavior or automatic behaviors, such as automatic software
updates, as a reason for using a behavior.

Media and family/friends were the most prevalent sources
of digital-security advice. This ﬁnding conﬁrms the results
of prior, less representative studies [20, 48]. For the major-
ity of respondents, online, print, or TV news articles were at
least one of the types of media advice they saw (67.5%). On-
line forums served as a digital-security advice source for 40%
of respondents, and ﬁctional narratives and advertisements
accounted for 25% of media advice, each. Additionally, the
majority (60%) of advice from a family members and friend
was given by a person with a background in CS or IT. This
conﬁrms prior qualitative work, indicating that respondents
may feel that these individuals are experts or individuals
with these backgrounds may volunteer more unsolicited ad-
vice [44].

As also noted in prior qualitative work, negative expe-
riences appear to be a key source of digital-security ad-
vice [47, 48]. Our work conﬁrms this ﬁnding, although at
a lower level of prevalence than may have been expected:
28% of our respondents learned to practice a behavior due
to a negative experience or a story told about a negative
experience.

We were surprised to ﬁnd that digital-service providers,
such as TimeWarner or Bank of America, were a source of
advice for 33% of respondents, as this source of advice is
little discussed in prior research. Respondents also reported
receiving a signiﬁcant amount of advice from their workplace
(29.5%). Of those who received advice from work, over 50%
received advice from an IT newsletter or a friend or colleague
who worked in the IT department. The remainder received
advice from formal security training or from colleagues who
did not have an IT background.

Figure 1: Prevalence of advice sources.

4.6 Who Takes Which Advice?

How do these beliefs about the usefulness and trustworthi-
ness of digital-security advice, as well as demographic and
knowledge factors, impact from where users take advice? In
this section we present the results of binary logistic regres-
sion models for each source from which respondents reported
learning behaviors. These models provide insight into the
audience of each advice source. Recommendations for more
appropriate targeting of advice and the improvement of on-
line equity between users of diﬀerent skill levels are discussed
in Section 5.

Whether or not a respondent reported a given advice
source at least once was used as the outcome variable in
our models. The input factors considered in each model are
listed in Table 3. Included in these factors are two interac-
tion factors—between sensitive-data and beliefs about the
usefulness and trustworthiness, respectively, of digital- vs.
physical-security advice—which were informed by our prior
qualitative work [48]. Below, we describe and interpret the
signiﬁcant factors included in each model. Because we did
not conduct a controlled experiment, these results do not
imply causality. The ﬁnal regression results for each model
after backward elimination, including nonsigniﬁcant factors,
are shown in Table 4.

4.6.1 Media
As described in Section 3.3, we used backward elimina-
tion, minimizing AIC, to reach our ﬁnal model. The ﬁnal
model for media advice included internet skill, exposure to
sensitive data, age, income, and belief about the usefulness
of digital security as factors. We ﬁnd that respondents with
higher internet skill are 32% more likely to use media as
an advice source, potentially because media is increasingly
being distributed online rather than through print, TV or
radio.

4.6.2 Work
The ﬁnal model for work advice included internet skill, ex-
posure to sensitive data, age, income, education, and belief
factors. Those who work with sensitive data are 4.5× more
likely to cite their workplace as an advice source than those

Num. Respondents090180270360450Family/FriendsService ProviderWorkNegativeSchoolPromptAuto/Forced27942696147155173219671Factor

Description

Baseline

Gender
Age
Income
Race
Education
CS Background
Sensitive Data

Internet Skill
Belief: Useful

Belief: Trust

Sens. Data & Useful
Sens. Data & Trust

Male, female or other.
18-39 years, 40-59 years, and over 60 years.
<$50,000, $50,000-$100,000, and >$100,000)
Black, Hispanic, White, and Other.
Less than Bachelor’s degree, Bachelor’s degree, Graduate degree.
Whether or not the respondent reported working in or holding a degree in CS or IT. N/A
Whether or not the respondent reported working with HIPAA, FERPA, social secu-
rity/credit card data, or holding an active or prior clearance.
Level of internet skill as measured by the six-item general web-use skills index [25].
Response to whether digital-security advice was a lot more useful than physical-
security advice, somewhat more useful, equal, or that physical-security advice was
somewhat or a lot more useful than digital. On a ﬁve-point Likert scale.
Response to whether digital-security advice was a lot more trustworthy than physical-
security advice, somewhat more trustworthy, equal, or that physical-security advice
was somewhat or a lot more trustworthy than digital. On a ﬁve-point Likert scale.
Interaction between the Sensitive Data and Belief: Useful factors described above.
Interaction between the Sensitive Data and Belief: Trust factors described above.

N/A
N/A

N/A

Female
18-39 yrs
<$50,000
White
<B.S.

sensi-

No
tive data
N/A
N/A

Table 3: Factors used in regression models. Categorical factors are represented by binary variable sets and
individually compared to the baseline; a numerical value, centered on the middle value, was used for Likert
factors.

who are not exposed to sensitive data. This conﬁrms results
of prior work: those who have clearances and/or handle sen-
sitive data may receive beneﬁts that improve their security
through workplace training [48]. Those who had higher in-
ternet skill were 41% more likely to cite the workplace as a
source of advice. Further, those who cited their workplace
as a source of advice were more likely to believe that digital
security was more useful than physical security. Thus, there
is a need for increased digital equity and improved security
interventions for users who do not have the opportunity to
receive workplace training.
4.6.3 Negative Experiences
The ﬁnal model to describe those who cited negative ex-
periences, or stories of these experiences included internet
skill and sensitive data. Those respondents with higher in-
ternet skill levels were 31% more likely to cite a negative
experience as an advice source and those who handled sen-
sitive data were 50% more likely. Those with higher internet
skill or those who handle sensitive data may be more likely
spend more time online, and thus may be more likely to
have and learn from a personal negative experience. Fur-
thermore, more skilled users, and those who are exposed to
sensitive data, may be more likely to recognize a negative
experience when it occurs and identify the underlying cause
of that experience than less skilled or experienced users.
4.6.4 School
The ﬁnal model for advice from school included age, CS
background, income, sensitive data, and beliefs. Those with
a CS background were approximately 6× more likely than
those without this educational background to cite school as
an advice source, and those who were over 60 were only 11%
as likely as younger respondents to cite school as an advice
source. This is likely due to the fact that computers were
not used in schools until relatively recently, and those who
work in the ﬁeld of CS or IT and/or hold a degree in this
ﬁeld most likely received digital-security advice as they were
obtaining their degrees or training.
4.6.5 Device Prompts
Perhaps because they have already learned digital-security
behaviors from school, before they ever see a prompt, those

with a background in CS were only 20% as likely as those
without this background to cite a device prompt as a way
that they learned about a particular security behavior. That
said, those who used prompts as a source of security advice
were more likely to believe that security advice was impor-
tant—perhaps because this belief encouraged them to heed
the security prompts. The ﬁnal model for device prompts
also controlled for internet skill and income.
4.6.6 Device automation/requirements
The model for device automation—that is, whether re-
spondents reported learning about a security behavior be-
cause it was automated or required—included CS back-
ground, belief about the usefulness of digital-security ad-
vice, age, and education. Those with a background in CS
or IT were only 59% as likely as those without this back-
ground to learn from security requirements or automations.
Similarly to device prompts, this may reﬂect that respon-
dents with technical education already know about security
behaviors before encountering them as requirements. Ad-
ditionally, those who used a behavior because it was auto-
mated or required were more likely to believe that digital
security advice was important—a belief in the importance
of security may inspire users to utilize prompts and automa-
tion.
4.6.7 Family and Friends
The ﬁnal model for advice from family and friends in-
cluded age, ethnicity, CS background, and the respondent’s
internet skill. There appears to be a relationship between
age and taking advice from family and friends. Although
there is no discernible pattern between the coeﬃcients for
ages 40-59 and over 60, there appears to be a signiﬁcant
diﬀerence between respondents who are 18-39 years old and
those 40 and over. Additionally, respondents who are His-
panic were only 47% as likely to report taking advice from
family and friends as White respondents. These ﬁndings in-
dicate potential diﬀerences in how respondents in diﬀerent
age and ethnic groups chose to solicit advice.
4.6.8
Finally, the model for service provider advice included age,
internet skill, exposure to sensitive data, CS background,

Service Provider

672Source

Factor

Media

Work

Negative
Experience

School

Prompt

Automatic

Family &
Friends

Service
Provider

Internet Skill
40-59yrs
Over 60yrs
$50k - $100k
> $100k
Sensitive Data
Belief: Useful
Sens. Data & Useful

$50k - $100k
> $100k
Sensitive Data
Internet Skill
Sens. Data & Useful
Belief: Useful
40-59yrs
Over 60yrs
Bachelors degree
Graduate degree

Internet Skill
Sensitive Data
40-59yrs
Over 60yrs

CS Background
40-59yrs
Over 60yrs
$50k - $100k
> $100k
Belief: Useful
Sensitive Data
Belief: Trust
Sens. Data & Useful

CS Background
Belief: Useful
$50k - $100k
> $100k
Internet Skill

Belief: Useful
CS Background
Bachelors degree
Graduate degree
40-59yrs
Over 60yrs

40-59yrs
Over 60yrs
Black
Hispanic
Other
CS Background
Internet Skill

Sensitive Data
Male
Internet Skill
40-59yrs
Over 60yrs
Belief: Useful
CS Background

OR

1.32
0.59
0.87
0.85
1.82
1.50
1.09
1.31

1.93
2.91
4.53
1.41
1.58
0.69
0.86
0.53
1.53
1.83

1.31
1.50
0.55
0.61

6.22
0.26
0.11
0.57
1.99
1.14
0.99
1.23
0.71

0.20
0.80
0.48
0.64
1.26

0.79
0.59
0.60
0.68
1.00
1.74

0.40
0.51
0.66
0.47
1.56
1.37
0.87

1.81
1.57
1.24
1.40
2.10
1.14
0.65

CI

p-value

[1.09, 1.6] < 0.01*

[0.39, 0.91]
[0.54, 1.41]
[0.55, 1.31]
[1.12, 2.97]
[0.98, 2.29]
[0.86, 1.38]
[0.94, 1.83]

0.02*
0.58
0.45
0.02*
0.06
0.47
0.11

0.01*

[1.16, 3.21]
[1.63, 5.18] < 0.01*
[2.65, 7.74] < 0.01*
[1.11, 1.8] < 0.01*

[1.04, 2.41]
[0.48, 0.99]
[0.53, 1.39]
[0.29, 0.97]
[0.92, 2.53]
[0.97, 3.47]

[1.06, 1.63]
[1, 2.23]
[0.35, 0.87]
[0.36, 1.04]

0.03*
0.04*
0.54
0.04*

0.1
0.06

0.01*
0.05*
0.01*
0.07

[3.46, 11.17] < 0.01*
[0.14, 0.47] < 0.01*
[0.04, 0.27] < 0.01*

[0.3, 1.09]
[1.06, 3.74]
[0.8, 1.64]
[0.55, 1.78]
[0.95, 1.6]
[0.46, 1.11]

0.09
0.03*
0.46
0.97
0.12
0.13

[0.12, 0.35] < 0.01*
[0.65, 0.97]
[0.28, 0.83] < 0.01*
[0.35, 1.19]
[0.99, 1.6]

0.16
0.07

0.02*

[0.67, 0.92] < 0.01*
[0.37, 0.94]
[0.39, 0.92]
[0.39, 1.18]
[0.66, 1.5]
[1.08, 2.79]

0.03*
0.02*
0.17

0.02*

1

[0.26, 0.61] < 0.01*
[0.32, 0.83] < 0.01*
[0.37, 1.21]
[0.25, 0.89]
[0.81, 2.99]
[0.85, 2.2]
[0.72, 1.05]

0.18
0.02*
0.18
0.2
0.15

[1.21, 2.7] < 0.01*

[1.07, 2.31]
[1.01, 1.53]
[0.9, 2.19]

0.02*
0.04*
0.14

[1.27, 3.48] < 0.01*
[0.96, 1.35]
[0.39, 1.09]

0.13
0.11

Table 4: Regression results for advice source models. OR is the odds ratio between the given factor and the
baseline; CI is the 95% conﬁdence interval; statistically signiﬁcant factors (p<0.05) are denoted with *.

gender, and beliefs about the usefulness of digital-security
advice. Respondents who are male were 57% more likely to
report receiving advice from service providers, than Female
respondents. Additionally, respondents with higher internet
skill levels were 24% more likely to report taking advice from
a service provider, and those who are exposed to sensitive

data were 81% more likely to have taken advice from this
source.
4.7 Why Users Accept and Reject Advice

In addition to examining the factors that aﬀect which
users take advice from which sources, we wanted to under-
stand why the users of each advice source choose to accept

673and reject advice and behaviors. Prior work has identiﬁed
a number of diﬀerent reasons that users accept and reject
security advice [48]. In order to evaluate these ﬁndings and
determine their prevalence, we asked respondents why they
chose to practice (or not practice) a behavior based on the
advice that they received. The answer choices that we pro-
vided were drawn from our prior qualitative work and feed-
back gained during our cognitive interview sessions [48]. We
detail the results below.

4.7.1 Advice Acceptance
To assess respondents’ reasons for accepting advice, we
asked, “Which of the following best describes why the infor-
mation you received made you decide to do this behavior?”
We then presented four answer choices. The ﬁrst two “I
trusted the person or source of the information.” and “The
information made sense to me.” were drawn from our prior
work [48]. The other two choices, “The information in-
creased my fear of a negative event.” and “Other.” (with
a write in option), were added based on the cognitive in-
terviews and expert reviews to ensure that we captured all
possible respondent answers.

From our prior work, we hypothesized that respondents
would be more likely to accept physical-security advice based
on their evaluation of the advice content, while they would
accept digital-security advice based on their trust of the
source. As shown in Figure 2, we found that trusting the
source was most popular for antivirus (53%) and updating
(51%), while trusting the content was most popular for 2FA
(52%), passwords (57%), and door locking (58%). This ap-
pears to conﬁrm our hypothesis for some digital-security be-
haviors, but not others.
To investigate further, we ran an omnibus X 2 across all
advice sources and behaviors (X 2 = 58.96, p = 4.79e−12),
followed by planned pairwise comparisons of each digital be-
havior to door locking. 1 Our results strongly support that
most digital behaviors are diﬀerent from the physical be-
havior, especially antivirus (X 2 = 41.15, p = 1.41e−10) and
updating (X 2 = 25.49, p = 4.45e−7). Two-factor was also
signiﬁcantly diﬀerent from physical, but to a lesser degree
(X 2 = 6.78, p = 0.0083). We hypothesize that passwords
are not signiﬁcantly diﬀerent from physical (p > 0.05) be-
cause participants have been exposed to enough passwords
advice to feel comfortable evaluating its content directly.

Finally, we validate that the two choices presented in our
prior qualitative work [48] are near-exhaustive of the reasons
that users accept advice: for each of the behavior questions,
only a small portion of respondents (µ= 5%) reported that
increased fear of a negative event caused them to take advice;
an average of 2% of our respondents selected “Other.”

4.7.2 Advice Rejection
Similar to advice acceptance, we drew the answer choices
for our question regarding why respondents chose not to
practice a behavior, even after seeing information recom-
mending that behavior, from multiple prior studies [47, 48,
65, 66] and from the results of our survey pre-testing. We
only asked these questions regarding antivirus, updating,
and 2FA; as it is rarely, if ever, an option to not use pass-

1 As described in Section 3.3, the signiﬁcance of the results
in this section are somewhat overstated due to repeated mea-
sures; however, the results are so strong that we believe they
hold.

Figure 2: Reasons for accepting digital-security ad-
vice. Percentage per behavior.

words. Nearly half of our respondents (43%, 225) rejected
at least one of these three behaviors. Among these respon-
dents, we found that inconvenience (28%) and advice that
contained too much marketing material (17%) were the two
most common reasons for advice rejection, across all behav-
iors. We also found that a lack of negative experience was
the third most common reason (13%) for rejecting a behav-
ior. Although believing that one’s data has no value [30],
diﬃculty understanding advice [4], and being ‘careful’ on the
internet [65, 66] have been oﬀered as reasons for rejection in
prior work, these reasons were all cited by less than 10% of
our respondents.

The reasons respondents selected for rejecting advice var-

ied by behavior.

For antivirus software, “They were trying to
Antivirus.
sell me something” was most often cited as a reason for re-
jecting advice related to antivirus software (33%). Other
reasons for rejection included having had no prior negative
experience (13%), feeling that they were “careful” when us-
ing their computer and the internet (15%), and ﬁnding an-
tivirus software too diﬃcult to use (11%).

Updating.
Inconvenience, which included both “it was
inconvenient” and “I did not have time”, was the most com-
mon reason (50%) selected by respondents for why they did
not complete software updates. All other reasons provided
were cited by fewer than 10% of respondents and are thus
not reported here.

Two-Factor Authentication.
Inconvenience was also
the most common reason given by respondents for not using
2FA (41%). Our prior work also suggested that privacy con-
cerns may inhibit advice taking, especially for 2FA, where
users may be reluctant to share a phone number. We found
that while privacy concerns were not very prevalent in gen-
eral, they were somewhat more prevalent for 2FA (15%) [48].
See Figure 3 for more detail on respondents’ reasoning for
rejecting security advice.
4.8 Advice Sources and Behavior

We next examine which advice sources were most com-
monly associated with which security behaviors (Figure 4).
We ﬁnd that media was the primary source of advice for
both passwords (28%) and 2FA (26%), while family and
friends accounted for a larger portion of the advice about
antivirus behavior (28%) and software updates (24%). Ser-
vice providers (21%) were also a primary advice source for
2FA. Finally, learning a behavior via a negative experience

PhysicalPasswordsTwo-Factor  AuthenticationAntivirusUpdatesPercent of Responses0%15%30%45%60%4%8%5%8%12%45%39%52%57%58%51%53%42%35%30%Trust SourceEvaluate ContentFear of Neg. Event674for sensitive accounts (X 2=9.60, p = 0.048). Although prior
work has suggested that diﬀerences in security behaviors
may be caused by lower-SES users not highly valuing their
data [30], in our sample this was not the case.

Our respondents were 41% more likely to cite their work-
place as an advice source if they had higher internet skill,
and 4.5× more likely if they held a job that we categorized
as security-sensitive. While the workplace may be a valu-
able source of advice for those who have access to it and
the skills to understand this training, such resources may
not be available for low-SES users; furthermore, security is
often forgotten in digital literacy interventions that do exist
in the workplace. For example, the Kesla+ project aimed
at increasing the digital skills of low-skill oﬃce workers in
the workplace included no training on digital security [38].
Thus, we advocate piloting and evaluating digital literacy
programs which include or focus entirely on digital security.
Additionally, future work should include analyzing and im-
proving the grade-level readability and clarity of security
advice to avoid widening the digital security gap.

We also found that participants with higher levels of in-
ternet skill were more likely to have learned from a negative
experience, either their own or someone else’s. We hypoth-
esize that lower-skill users are less likely to recognize the
causes of a negative experience and therefore learn from it.
Because our results indicate that users who have not learned
from negative experiences are more likely to reject advice,
this inequity may put lower-skill users at additional risk. Of
course, we want to minimize all users’ direct exposure to neg-
ative experiences; instead we recommend amplifying stories
of others’ negative experiences. Future work could examine
how to eﬀectively simulate negative experiences, for exam-
ple by using short, relatable stories that clearly demonstrate
how to prevent the problem.

Advertisements & TV Shows.
Our ﬁndings suggest
the need for additional research into the content and teach-
ing power of advertisements and TV shows. Fifty-percent of
respondents who reported receiving digital-security advice
from media received that advice from ads and TV shows.
Thus far, the research community has had little input into
nor focus on these forms of media. We recommend leverag-
ing work in the communications and health ﬁelds [28, 31, 35,
59] to rigorously evaluate this media.

Finally, we make two

Improving Security Advice.
speciﬁc suggestions for improving digital-security advice.
First, we ﬁnd that much advice from the workplace and from
family and friends comes from those with backgrounds in
IT and CS. Anecdotally, many people with technical back-
grounds are overloaded with “help-needed” requests from
friends, family, and colleagues. Thus, we suggest that schools
and the companies that employ these individuals support
this important but potentially under-rewarded role, by pro-
viding more resources to help them meet these requests ef-
fectively. Disseminating a small set of essential security ad-
vice via these channels could have a large positive impact on
user behavior. Second, we recommend explicitly stating the
source and purpose of security advice. Nearly 50% of users
accept advice because they trust the advice source; thus, it is
crucial that the source be clearly identiﬁed and demonstra-
bly authoritative, for example via professional credentials.

Figure 3: Reasons for rejecting digital-security ad-
vice. Total per behavior, multiple responses possi-
ble. This question was not asked for passwords, as
not using them is rarely, if ever, an option.

Figure 4: Advice source prevalence by behavior.

was most common for antivirus use (19%). An omnibus
X 2 test showed that these diﬀerences among behaviors are
signiﬁcant (X 2 = 59.05, p = 3.67e−7).

5. DISCUSSION

Below we draw conclusions from our ﬁndings and suggest

directions for future research and design.

Digital Inequity.
Users with lower socioeconomic sta-
tus tend to be part of a knowledge gap: they have dimin-
ished access to digital media and more diﬃculty ﬁnding rep-
utable and useful information on the web [23, 24, 49, 60, 64].
Our work expands these ﬁndings to digital security: we ﬁnd
evidence that users with higher levels of internet skills—
demonstrated by prior work to be wealthier and somewhat
more secure [25, 37]—use diﬀerent advice sources. In par-
ticular, lower-skill users rely more on prompts, the advice
of family and friends, and service providers than higher-skill
users do. These diﬀerences, combined with discrepancies in
skills and resources, may lead already disadvantaged users
to be disproportionately victimized [34]. Indeed, while we
could not control for all confounding factors, we found that
users with lower incomes were less likely to update their
software (X 2=28.03, p ≤ 0.001), to use 2FA (X 2=15.60,
p = 0.004), and slightly less likely to use stronger passwords

Rejection RationaleInevitableDidn’t understandPrivacy threatData has no value“I’m careful”No negative experienceToo much marketingInconvenientNum. Respondents01326395265AntivirusUpdatesTwo-FactorAuthenticationTwo-Factor AuthenticationUpdatesAntivirusPasswordsNum. Respondents0150300450600MediaFamily/FriendsWorkSchoolNegative Exp.Service Provider6756. SUMMARY

In this paper we presented the results of a survey of 526

census-representative U.S. users’ beliefs, behaviors, and sources
of advice for digital and physical security. We ﬁnd the ﬁrst
explicit evidence of a digital security divide: users with
higher skill levels and socioeconomic status, as well as those
who handle sensitive data at work, are signiﬁcantly more
likely to get advice from work and to learn from negative
experiences. This divide may increase the vulnerability of
already disadvantaged users. In addition, we conﬁrm that
users evaluate advice based on the credibility of the source
(rather than the content of the advice) signiﬁcantly more
often for digital-security topics than for a physical-security
topic. Our results indicate that users reject advice not only
because it is inconvenient and they have maxed-out their
compliance budget, but because it contains too much mar-
keting material; recommendations to use 2FA are also no-
tably rejected due to privacy concern. Based on these re-
sults, we provide recommendations for combating the digital
security divide, as well as recommendations for increasing
the impact of security advice more generally. We hope that
these results can improve the eﬃcacy of truly critical recom-
mendations, by helping them to stand out within the noisy
space of other advice.

7. ACKNOWLEDGMENTS

Our thanks to Guy Aurelien, Lujo Bauer, Brenna Mc-
Nally, and Uran Oh. This work is supported by Maryland
Procurement Oﬃce contract no. H98230-14-C-0137.

8. REFERENCES
[1] Microsoft safety and security center.
[2] US-CERT:Tips.
[3] American community survey 5-year estimates, 2014.
[4] Adams, A., and Sasse, M. A. Users are not the

enemy.

[5] Akaike, H. A new look at the statistical model

identiﬁcation.

[6] Akhawe, D., and Felt, A. P. Alice in warningland:

A large-scale ﬁeld study of browser security warning
eﬀectiveness. In USENIX Sec. (2013).

[7] Arachchilage, N. A. G., and Love, S. A game

design framework for avoiding phishing attacks.
Comput. Hum. Behav. (2013).

[8] Baker, R., Blumberg, S., and et al. AAPOR

report on online panels. The Public Opinion Quarterly
(2010).

[9] Beautement, A., Sasse, M. A., and Wonham, M.
The compliance budget: Managing security behaviour
in organisations. In workshop on new security
paradigms (2008).

[10] Bravo-Lillo, C., Komanduri, S., Cranor, L. F.,

Reeder, R. W., Sleeper, M., Downs, J., and
Schechter, S. Your attention please: Designing
security-decision UIs to make genuine risks harder to
ignore. In SOUPS (2013).

[11] Ciampa, M. A comparison of password feedback

mechanisms and their impact on password entropy.
Information Management & Computer Security
(2013).

[12] Das, S., Kim, T. H., Dabbish, L., and Hong, J.

The eﬀect of social inﬂuence on security sensitivity. In
SOUPS (2014).

[13] Das, S., Kramer, A. D., Dabbish, L. A., and

Hong, J. I. Increasing security sensitivity with social
proof: A large-scale experimental conﬁrmation. In
CCS (2014).

[14] DeMaio, T. J., Rothgeb, J., and Hess, J.

Improving survey quality through pretesting. U.S.
Bureau of the Census (2003).

[15] Egelman, S., Cranor, L. F., and Hong, J. You’ve
been warned: An empirical study of the eﬀectiveness
of web browser phishing warnings. In CHI (2008).
[16] Egelman, S., and Peer, E. Scaling the security

wall: Developing a security behavior intentions scale
(sebis). In CHI (2015).

[17] F.R.S., K. P. X. on the criterion that a given system

of deviations from the probable in the case of a
correlated system of variables is such that it can be
reasonably supposed to have arisen from random
sampling. Philosophical Magazine Series 5 (1900).

[18] Fujita, M., Yamada, M., Arimura, S., Ikeya, Y.,
and Nishigaki, M. An attempt to memorize strong
passwords while playing games. In NBIS (2015).
[19] Furman, S. M., Theofanos, M. F., Choong,
Y.-Y., and Stanton, B. Basing cybersecurity
training on user perceptions. IEEE S&P (2012).

[20] Furnell, S., Bryant, P., and Phippen, A.

Assessing the security perceptions of personal internet
users. Computers & Security (2007).

[21] Garg, V., Camp, L. J., Connelly, K., and

Lorenzen-Huber, L. Risk communication design:
Video vs. text. In PETS (2012).

[22] Halevi, T., Lewis, J., and Memon, N. A pilot

study of cyber security and privacy related behavior
and personality traits. In WWW (2013).

[23] Hargittai, E. Second-level digital divide: Mapping

diﬀerences in people’s online skills. First Monday
(2002).

[24] Hargittai, E. The Digital Divide and What to Do

About It. 2003, pp. 822–841.

[25] Hargittai, E., and Hsieh, Y. P. Succinct survey
measures of web-use skills. Soc. Sci. Comput. Rev.
(2012).

[26] Herley, C. So long, and no thanks for the

externalities: The rational rejection of security advice
by users. In NPSW (2009).

[27] Herley, C. More is not the answer. IEEE Security &

Privacy magazine (2014).

[28] Hinyard, L. J., and Kreuter, M. W. Using

narrative communication as a tool for health behavior
change: a conceptual, theoretical, and empirical
overview. Health Educ Behav (2007).

[29] Hosmer, D. W., and Lemeshow, S. Applied logistic

regression. 2000.

[30] Howe, A. E., Ray, I., Roberts, M., Urbanska,
M., and Byrne, Z. The psychology of security for
the home computer user. In IEEE S&P (2012).

[31] Hu, X. Assessing source credibility on social media—

An electronic word-of-mouth communication

676perspective. PhD thesis, Bowling Green State
University, 2015.

[32] Ion, I., Reeder, R., and Consolvo, S. “...no one

can hack my mind”: Comparing expert and
non-expert security practices. In SOUPS (2015).

[33] Ipeirotis, P. Demographics of mechanical turk. NYU

Center for Digital Economy (2010).

[34] Jerome, J. Buying and Selling Privacy: Big Data’s

Diﬀerent Burdens and Beneﬁts. Stanford Law Review
(2013).

[35] Kang, M. Measuring social media credibility: A

study on a measure of blog credibility. Institute for
Public Relations (2009).

[36] Kang, R., Brown, S., Dabbish, L., and Kiesler,
S. Privacy attitudes of mechanical turk workers and
the u.s. public. In SOUPS (2014).

[37] Kelley, T., and Bertenthal, B. I. Attention and

past behavior, not security knowledge, modulate
users? decisions to login to insecure websites.
Information and Computer Security (2016).

[38] Kentaro Toyama, A. R. Kelsa+: Digital literacy for
low-income oﬃce workers. In International Conference
on Information and Communication Technologies and
Development (2009).

[39] Krosnick, J. A. The threat of satisﬁcing in surveys:
the shortcuts respondents take in answering questions.
Survey Methods Centre Newsletter, 2000.

[40] Krosnick, J. A. Handbook of Survey Research. 2010.
[41] Lin, E., Greenberg, S., Trotter, E., Ma, D., and

Aycock, J. Does domain highlighting help people
identify phishing sites? In CHI (2011).

[42] Mann, H. B., and Whitney, D. R. On a test of

whether one of two random variables is stochastically
larger than the other. Ann. Math. Statist. (1947).

[43] Nappa, A., Johnson, R., Bilge, L., Caballero,
J., and Dumitras, T. The attack of the clones: A
study of the impact of shared code on vulnerability
patching. In IEEE S&P (2015).

[44] Poole, E. S., Chetty, M., Morgan, T., Grinter,

R. E., and Edwards, W. K. Computer help at
home: Methods and motivations for informal technical
support. CHI.

[45] Presser, S., Couper, M. P., Lessler, J. T.,

Martin, E., Martin, J., Rothgeb, J. M., and
Singer, E. Methods for testing and evaluating survey
questions. Public Opinion Quarterly (2004).

[46] Rader, E., and Wash, R. Identifying patterns in

informal sources of security information. J.
Cybersecurity (2015).

[47] Rader, E., Wash, R., and Brooks, B. Stories as
informal lessons about security. In SOUPS (2012).

[48] Redmiles, E., Malone, A. R., and Mazurek,

M. L. How i learned to be secure: Advice sources and
selection in digital security. In IEEE S&P (2016).

[49] Rice, R. E. Inﬂuences, usage, and outcomes of

internet health information searching: Multivariate
results from the pew surveys. International J. Medical
Informatics (2006). Health and the Internet for All.

[51] Ross, J., Irani, L., Silberman, M. S., Zaldivar,

A., and Tomlinson, B. Who are the crowdworkers?:
Shifting demographics in mechanical turk. In CHI
(2010).

[52] Schaeffer, N. C., and Presser, S. The science of
asking questions. Annual Review of Sociology (2003).
[53] Schechter, S., and Bonneau, J. Learning assigned

secrets for unlocking mobile devices. In SOUPS
(2015).

[54] Schechter, S. E., Dhamija, R., Ozment, A., and
Fischer, I. The Emperor’s New Security Indicators.
IEEE S&P (2007).

[55] Sheng, S., Holbrook, M., Kumaraguru, P.,

Cranor, L. F., and Downs, J. Who falls for phish?:
A demographic analysis of phishing susceptibility and
eﬀectiveness of interventions. In CHI (2010).
[56] Sheng, S., Magnien, B., Kumaraguru, P.,

Acquisti, A., Cranor, L. F., Hong, J., and
Nunge, E. Anti-phishing phil: The design and
evaluation of a game that teaches people not to fall for
phish. In SOUPS (2007).

[57] Siciliano, R. 17 percent of pcs are exposed.
[58] Smith, S. 4 ways to ensure valid responses for your

online survey. Qualtrics.

[59] Sole, D., and Wilson, D. G. Storytelling in

Organizations : The power and traps of using stories
to share knowledge in organizations. Training and
Development (1999).

[60] Stanley, L. D. Beyond access: Psychosocial barriers
to computer literacy special issue: Icts and community
networking. The Information Society (2003).

[61] Sunshine, J., Egelman, S., Almuhimedi, H., Atri,

N., and Cranor, L. F. Crying wolf: An empirical
study of ssl warning eﬀectiveness. In USENIX Sec.
(2009).

[62] Tourangeau, R., and Yan, T. Sensitive Questions

in Surveys. Psychological Bulletin (2007).

[63] Ur, B., Kelley, P. G., Komanduri, S., Lee, J.,
Maass, M., Mazurek, M. L., Passaro, T., Shay,
R., Vidas, T., Bauer, L., Christin, N., and
Cranor, L. F. How does your password measure up?
the eﬀect of strength meters on password creation. In
USENIX Sec. (2012).

[64] van Dijk, J., and Hacker, K. The digital divide as

a complex and dynamic phenomenon. The
Information Society (2003).

[65] Wash, R. Folk models of home computer security. In

SOUPS (2010).

[66] Wash, R., and Rader, E. Too much knowledge?

security beliefs and protective behaviors among united
states internet users. In SOUPS (2015).

[67] Whitman, M. E. Enemy at the gate: Threats to

information security. Commun. ACM (2003).

[68] Willis, G. B. Cognitive Interviewing: A Tool for

Improving Questionnaire Design. 2005.

[69] Wu, M., Miller, R. C., and Garfinkel, S. L. Do
security toolbars actually prevent phishing attacks? In
CHI (2006).

[50] Robila, S. A., and Ragucci, J. W. Don’t be a

[70] Yuan, M., and Lin, Y. Model selection and

phish: Steps in user education. In SIGCSE (2006).

estimation in regression with grouped variables. J.
Royal Statistical Society (2006).

677