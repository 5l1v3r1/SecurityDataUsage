PCF: A Portable Circuit Format for Scalable  

Two-Party Secure Computation

Ben Kreuter, University of Virginia; Benjamin Mood, University of Oregon;  

abhi shelat, University of Virginia; Kevin Butler, University of Oregon

Open access to the Proceedings of the 22nd USENIX Security Symposium is sponsored by USENIXThis paper is included in the Proceedings of the 22nd USENIX Security Symposium.August 14–16, 2013 • Washington, D.C., USAISBN 978-1-931971-03-4PCF: A Portable Circuit Format For Scalable Two-Party Secure

Computation

Computer Science Dept.

Computer and Info. Science Dept.

Computer Science Dept.

Benjamin Mood

Ben Kreuter

U. Virginia

abhi shelat

U. Virginia

U. Oregon
Kevin Butler

U. Oregon

Computer and Info. Science Dept.

Abstract

A secure computation protocol for a function f (x,y)
must leak no information about inputs x,y during its ex-
ecution; thus it is imperative to compute the function f
in a data-oblivious manner. Traditionally, this has been
accomplished by compiling f into a boolean circuit. Pre-
vious approaches, however, have scaled poorly as the cir-
cuit size increases. We present a new approach to com-
piling such circuits that is substantially more efﬁcient
than prior work. Our approach is based on online cir-
cuit compression and lazy gate generation. We imple-
mented an optimizing compiler for this new representa-
tion of circuits, and evaluated the use of this representa-
tion in two secure computation environments. Our eval-
uation demonstrates the utility of this approach, allow-
ing us to scale secure computation beyond any previous
system while requiring substantially less CPU time and
disk space. In our largest test, we evaluate an RSA-1024
signature function with more than 42 billion gates, that
was generated and optimized using our compiler. With
our techniques, the bottleneck in secure computation lies
with the cryptographic primitives, not the compilation or
storage of circuits.

1

Introduction

Secure function evaluation (SFE) refers to several related
cryptographic constructions for evaluating functions on
unknown inputs. Typically, these constructions require
an oblivious representation of the function being eval-
uated, which ensures that the control ﬂow of the algo-
rithm will not depend on its input; in the two party case,
boolean circuits are most frequently seen. These oblivi-
ous representations are often large, with millions and in
some cases billions of gates even for relatively simple
functions, which has motivated the creation of software
tools for producing such circuits. While there has been
substantial work on the practicality of secure function

evaluation, it was only recently that researchers began
investigating the practicality of compiling such oblivious
representations from high-level descriptions.

The work on generating boolean circuits for SFE has
largely focused on two approaches.
In one approach,
a library for a general purpose programming language
such as Java is created, with functions for emitting cir-
cuits [13, 20]. For convenience, these libraries typically
include pre-built gadgets such as adders or multiplex-
ers, which can be used to create more complete func-
tions. The other approach is to write a compiler for a
high level language, which computes and optimizes cir-
cuits based on a high level description of the functional-
ity that may not explicitly state how the circuit should
be organized [18, 21].
It has been shown in previous
work that both of these approaches can scale up to cir-
cuits with at least hundreds of millions of gates on mod-
ern computer hardware, and in some cases even billions
of gates [13, 18].

The approaches described above were limited in terms
of their practical utility. Library-based approaches like
HEKM [13] or VMCrypt [20] require users to understand
the organization of the circuit description of their func-
tion, and were unable to apply any optimizations across
modules. The Fairplay compiler [21] was unable to scale
to circuits with only millions of gates, which excludes
many interesting functions that have been investigated.
The poor scalability of Fairplay is a result of the com-
piler ﬁrst unrolling all loops and inlining all subroutines,
storing the results in memory for later compiler stages.
The PALC system [23] was more resource efﬁcient than
Fairplay, but did not attempt to optimize functions, re-
lying instead on precomputed optimizations of speciﬁc
subcircuits. The KSS12 [18] system was able to apply
some global optimizations and used less memory than
Fairplay, but also had to unroll all loops and store the
complete circuit description, which caused some func-
tions to require days to compile. Additionally, the lan-
guage used to describe circuits in the KSS12 system was

USENIX Association  

22nd USENIX Security Symposium  321

1

brittle and difﬁcult to use; for example, array index val-
ues could not be arbitrary functions of loop indices.

1.1 Our Approach
In this work, we demonstrate a new approach to compil-
ing, optimizing, and storing circuits for SFE systems. At
a high level, our approach is based on representing the
function to be evaluated as a program that computes the
circuit representation of the function, similar to the cir-
cuit library approaches described in previous work. Our
compiler then optimizes this program with the goal of
producing a smaller circuit. We refer to our circuit rep-
resentation as the Portable Circuit Format (PCF).

When the SFE system is run, it uses our interpreter
to load the PCF program and execute it. As the PCF
program runs, it interacts with the SFE system, managing
information about gates internally based on the responses
from the SFE system itself. In our system, the circuit is
ephemeral; it is not necessary to store the entire circuit,
and wires will be deleted from memory once they are no
longer required.

The key insight of our approach is that it is not neces-
sary to unroll loops until the SFE protocol runs. While
previous compilers discard the loop structure of the func-
tion, ours emits it as part of the control structure of the
PCF program. Rather than dealing directly with wires,
our system treats wire IDs as memory addresses; a wire
is “deleted” by overwriting its location in memory. Loop
termination conditions have only one constraint:
they
must not depend on any secret wire values. There is no
upper bound on the number of loop iterations, and the
programmer is responsible for ensuring that there are no
inﬁnite loops.

To summarize, we present the following contributions:
• A new compiler that has the same advantages as the

circuit library approach

• A novel, more general algorithm for translating con-

ditional statements into circuits

• A new representation of circuits that is more com-
pact than previous representations which scales to
arbitrary circuit sizes.

• A portable interpreter that can be used with differ-
ent SFE execution systems regardless of the security
model.

Our compiler is a back end that can read the byte-
code emitted by a front end; thus our compiler allows
any language to be used for SFE. Instead of focusing on
global optimizations of boolean functions, our optimiza-
tion strategy is based on using higher-level information

from the bytecode itself, which we show to be more ef-
fective and less resource-intensive. We present compar-
isons of our compiler with previous work and show ex-
perimental results using our compiler in two complete
SFE systems, one based on an updated version of the
KSS12 system and one based on HEKM. In some of our
test cases, our compiler produced circuits only 30% as
large as previous compilers starting from the same source
code. With the techniques presented in this work, we
demonstrate that the RSA algorithm with a real-world
key size and real-world security level can be compiled
and run in a garbled circuit protocol using a typical desk-
top computer. To the best of our knowledge, the RSA-
1024 circuit we tested is larger than any previous garbled
circuit experiment, with more than 42 billion gates. We
also present preliminary results of our system running
on smartphones, using a modiﬁed version of the HEKM
system.

For testing purposes, we used the LCC compiler [8]
as a front-end to our system. A high-level view of our
system, with the LCC front-end, is given in Figure 1.

The rest of this paper is organized as follows: Sec-
tion 2 is a review of SFE and garbled circuits; Section 3
presents an overview of bytecode languages; Section 4
explains our compiler design and describes our represen-
tation; Section 5 discusses the possibility of using dif-
ferent bytecode and SFE systems; Section 6 details the
experiments we performed to evaluate our system and re-
sults of those experiments; Section 7 details other work
which is related to our own; and Section 8 presents future
lines of research.

2 Secure Function Evaluation

The problem of secure two-party computation is to allow
two mutually distrustful parties to compute a function
of their two inputs without revealing their inputs to the
opposing party (privacy) and with a guarantee that the
output could not have been manipulated (correctness).
Yao was the ﬁrst to show that such a protocol can be
constructed for any computable function, by using the
garbled circuits technique [30]. In his original formula-
tion, Yao proposed a system that would allow users to de-
scribe the function in a high level language, which would
then be compiled into a circuit to be used in the garbled
circuits protocol. The ﬁrst complete implementation of
this design was the Fairplay system given by Malkihi et
al. [21].
Oblivious Transfer One of the key building blocks
in Yao’s protocol is oblivious transfer, a cryptographic
primitive ﬁrst proposed by Rabin [25]. In this primitive,
the “sender” party holds a database of n strings, and the
“receiver” party learns exactly k strings with the guar-
antee that the sender will not learn which k strings were

322  22nd USENIX Security Symposium 

USENIX Association

2

C Compiler

LCC to PCF 
Compiler

LCC 

Bytecode

PCF File

Gen. PCF 
Interpreter

Evl. PCF 
Interpreter

Figure 1: High-level design of our system. We take a C
program and compile it down to the LCC bytecode. Our
compiler then transforms the LCC bytecode to our new
language PCF. Both parties then execute the protocol in
their respective role in the SFE protocol. The interpreter
could be any execution system.

sent and the receiver will not learn more than k strings;
this is known as a k-out-of-n oblivious transfer. Given a
public key encryption system it is possible to construct
a 1-out-of-2 oblivious transfer protocol [7], which is the
building block used in Yao’s protocol.
Garbled Circuits The core of Yao’s protocol is the con-
struction of garbled circuits, which involves encrypting
the truth table of each gate in a circuit description of the
function. When the protocol is run, the truth values in the
circuit will be represented as decryption keys for some
cipher, with each gate receiving a unique pair of keys for
its output wire. The keys for a gate’s input wires are then
used to encrypt the keys for its output wires. Given a sin-
gle key for each input wire of the circuit, the party that
evaluates the circuit can decrypt a single key that rep-
resents a hidden truth value for each gate’s output wire,
until the output gates are reached. Since this encryption
process can be applied to any circuit, and since any com-
putable function has a corresponding circuit family, this
allows the construction of a secure protocol for any com-
putable function.

The typical garbled circuit protocol has two parties
though it can be expanded to more. Those two parties
are Bob, the generator of the garbled circuit, and Alice,
the evaluator of the garbled circuit. Bob creates the gar-
bled circuit and therefore knows the decryption keys, but
does not know which speciﬁc keys Alice uses. Alice will
receive the input keys from Bob using an oblivious trans-
fer protocol, and thus learns only one key for each input
wire; if the keys are generated independent of Bob’s in-
put, Alice will learn only enough to compute the output
of the circuit.

Several variations on the Yao protocol have been pub-
lished; a simple description of the garbling and eval-
f : {0,1}A × {0,1}B →
uation process follows. Let
{0,1} j × {0,1}k be a computable function, which will
receive input bits from two parties and produce output
bits for each party (not necessarily the same outputs). To
garble the circuit, a block cipher (cid:27)E,D,G(cid:26) will be used.
For each wire in the circuit, Bob computes a pair of
random keys (k0,k1) ← (G(1n),G(1n)), which represent

logical 0 and 1 values. For each of Alice’s outputs, Bob
uses these keys to encrypt a 0 and a 1 and sends the pair
of ciphertexts to Alice. Bob records the keys correspond-
ing to his own outputs. The rest of the wires in the cir-
cuit are inputs to gates. For each gate, if the truth table is
[v0,0,v0,1,v1,0,v1,1], Bob computes the following cipher-
text:

(cid:31)Ekl,0(Ekr,0(kv0,0)),Ekl,0(Ekr,1(kv0,1))
Ekl,1(Ekr,0(kv1,0)),Ekl,1(Ekr,1(kv1,1))(cid:30)

where kl,∗ and kr,∗ are the keys for the left and right input
wires (this can be generalized for gates with more than
two inputs). The order of the four ciphertexts is then
randomly permuted and sent to Alice.

Now that Alice has the garbled gates, she can begin
evaluating the circuit. Bob will send Alice his input wire
keys. Alice and Bob then use an oblivious transfer to give
Alice the keys for her input wires. For each gate, Alice
will only be able to decrypt one entry, and will receive
one key for the gate’s output, and will continue to de-
crypt truth table entries until the output wires have been
computed. Alice will then send Bob his output keys, and
decrypt her own outputs.
Optimizations Numerous optimizations to the basic Yao
protocol have been published [10, 13, 17, 24, 27]. Of
these, the most relevant to compiling circuits is the “free
XOR trick” given by Kolesnikov and Schneider [17].
This technique allows XOR gates to be evaluated with-
out the need to garble them, which greatly reduces the
amount of data that must be transferred and the CPU time
required for both the generator and the evaluator. One ba-
sic way to take advantage of this technique is to choose
subcircuits with fewer non-XOR gates; Schneider pub-
lished a list of XOR-optimal circuits for even three-input
functions [27].

Huang et al. noted that there is no need for the eval-
uator to wait for the generator to garble all gates in the
circuit [13]. Once a gate is garbled, it can be sent to
the evaluator, allowing generation and evaluation to oc-
cur in parallel. This technique is very important for large
circuits, which can quickly become too large to store in
RAM [18]. Our approach uniﬁes this technique with the
use of an optimizing compiler.

3 Bytecode

A common approach to compiler design is to translate a
high level language into a sequence of instructions for a
simple, abstract machine architecture; this is known as
the intermediate representation or bytecode. Bytecode
representations have the advantage of being machine-
independent, thus allowing a compiler front-end to be
used for multiple target architectures. Optimizations per-

USENIX Association  

22nd USENIX Security Symposium  323

3

C Codeformed on bytecode are machine independent as well; for
example, dead code elimination is typically performed
on bytecode, as removing dead code causes programs to
run faster on all realistic machines.

For the purposes of this work, we focus on a com-
monly used bytecode abstraction, the stack machine. In
this model, operands must be pushed onto an abstract
stack, and operations involve popping operands off of the
stack and pushing the result. In addition to the stack, a
stack machine has RAM, which is accessed by instruc-
tions that pop an address off the stack. Instructions in
a stack machine are partially ordered, and are divided
into subroutines in which there is a total ordering.
In
addition to simple operations and operations that interact
with RAM, a stack machine has operations that can mod-
ify the program counter, a pointer to the next instruction
to be executed, either conditionally or unconditionally.

At a high level, our system translates bytecode pro-
grams for a stack machine into boolean circuits for SFE.
At ﬁrst glance, this would appear to be at least highly
inefﬁcient, if not impossible, because of the many ways
such an input program could loop. We show, however,
that imposing only a small set of restrictions on permis-
sible sequences of instructions enables an efﬁcient and
practical translator, without signiﬁcantly reducing the us-
ability or expressive power of the high level language.

4 System Design

Our system divides the compiler into several stages, fol-
lowing a common compiler design. For testing, we used
the LCC compiler front end to parse C source code and
produce a bytecode intermediate representation (IR). Our
back end performs optimizations and translates the byte-
code into a description of a secure computation proto-
col using our new format. This representation greatly re-
duces the disk space requirements for large circuits com-
pared to previous work, while still allowing optimiza-
tions to be done at the bit level. We wrote our compiler
in Common Lisp, using the Steel Bank Common Lisp
system.

4.1 Compact Representations of Boolean

Circuits

In Fairplay and the systems that followed its design, the
common pattern has been to represent Boolean circuits as
adjacency lists, with each node in the graph being a gate.
The introduces a scalability problem, as it requires stor-
age proportional to the size of the circuit. Generating,
optimizing, and storing circuits has been a bottleneck
for previous compilers, even for relatively simple func-
tions like RSA. Loading such large circuits into RAM

…   ...

NO

Loop?

YES

Memory

…   ...

LOC: 33+i

LOC: 1+i

LOC: 65+i

OR

Memory

LOC: 65+i

LOC: 97+i

LOC: 65+i

OR

…   ...

Figure 2: The high-level concept of the PCF design. It
is not necessary to unroll loops at compile time, even to
perform optimizations on the circuit. Instead, loops can
be evaluated at runtime, with gates being computed on-
the-ﬂy, and loop indices being updated locally by each
party. Wire values are stored in a table, with each gate
specifying which two table entries should be used as in-
puts and where the output should be written; previous
wire values in the table can be overwritten during this
process, if they are no longer needed.

is a challenge, as even very high-end machines may not
have enough RAM for relatively simple functions.

There have been some approaches to addressing this
scalability problem presented in previous work. The
KSS12 system reduced the RAM required for protocol
executions by assigning each gate’s output wire a refer-
ence count, allowing the memory used for a wire value to
be deallocated once the gate is no longer needed. How-
ever, the compiler bottleneck was not solved in KSS12,
as even computing the reference count required memory
proportional to the size of the circuit. Even with the engi-
neering improvements presented by Kreuter, shelat, and
Shen, the KSS12 compiler was unable to compile circuits
with more than a few billion gates, and required several
days to compile their largest test cases [18].

The PAL system [23] also addresses memory require-
ments, by adding control structures to the circuit descrip-
tion, allowing parts of the description to be re-used. In
the original presentation of PAL, however, a large circuit
ﬁle would still be emitted in the Fairplay format when
the secure protocol was run. An extension of this work
presented by Mood [22] allowed the PAL description to
be used directly at runtime, but this work sacriﬁced the
ability to optimize circuits automatically.

Our system builds upon the PAL and KSS12 systems
to solve the memory scalability problem without sacri-

324  22nd USENIX Security Symposium 

USENIX Association

4

ﬁcing the ability to optimize circuits automatically. Two
observations are key to our approach.

Our ﬁrst observation is that it is possible to free the
memory required for storing wire values without com-
puting a reference count for the wire. In previous work,
each wire in a circuit is assigned a unique global identi-
ﬁer, and gate input wires are speciﬁed in terms of these
identiﬁers (output wires can be identiﬁed by the position
of the gate in the gate list). Rather than using global
identiﬁers, we observe that wire values are ephemeral,
and only require a unique identity until their last use as
the input to a gate.

We therefore maintain a table of “active” wire values,
similar to KSS12, but change the gate description.
In
this format, wire values are identiﬁed by their index in
the table, and gates specify the index of each input wire
and an index for the output wire; in other words, a gate
is a tuple (cid:31)t,i1,i2,o(cid:30), where t is a truth table, i1,i2 are the
input wire indexes, and o is the output wire index. When
a wire value is no longer needed, its index in the table
can be safely used as an output wire for a gate.

Now, consider the following example of a circuit
described in the above format, which accumulates the
Boolean AND of seven wire values:

(cid:31)AND1,1,2,0(cid:30)
(cid:31)AND2,0,3,0(cid:30)
(cid:31)AND3,0,4,0(cid:30)
(cid:31)AND4,0,5,0(cid:30)
(cid:31)AND5,0,6,0(cid:30)
(cid:31)AND6,0,7,0(cid:30)

Our second observation is that circuits such as this can
be described more compactly using a loop. This builds
on our ﬁrst observation, which allows wire values to be
overwritten once they are no longer needed. A simple ap-
proach to allowing this would add a conditional branch
operation to the description format. This is more general
than the approach of PAL, which includes loops but al-
lows only simple iteration. Additionally, it is necessary
to allow the loop index to be used to specify the input or
output wire index of the gates; as a general solution, we
add support for indirection, allowing wire values to be
copied.

This representation of Boolean circuits is a bytecode
for a one-bit CPU, where the operations are the 16 pos-
sible two-arity Boolean gates, a conditional branch, and
indirect copy.
In our system, we also add instructions
for function calls (which need not be inlined at compile
time) and handling the parties’ inputs/outputs. When the
secure protocol is run, a three-level logic is used for wire
values: 0, 1, or ⊥, where ⊥ represents an “unknown”
value that depends on one of the party’s inputs. In the
case of a Yao protocol, the ⊥ value is represented by a

5

garbled wire value. Conditional branches are not allowed
to depend on ⊥ values, and indirection operations use
a separate table of pointers that cannot computed from
⊥ values (if such an indirection operation is required, it
must be translated into a large multiplexer, as in previous
work).

We refer to our circuit representation as the Portable
Circuit Format or PCF. In addition to gates and branches,
PCF includes support for copying wires indirectly, a
function call stack, data stacks, and setting function pa-
rameters. These additional operations do not emit any
gates and can therefore be viewed as “free” operations.
PCF is modeled after the concept of PAL, but instead
of using predeﬁned sub-circuits for complex operations,
a PCF ﬁle deﬁnes the sub-circuits for a given function
to allow for circuit structure optimization. PCF includes
lower level control structures compared to PAL, which
allows for more general loop structures.

In Appendix A, we describe in detail the semantics of
the PCF instructions. Example PCF ﬁles are available at
the authors’ website.

4.2 Describing Functions for SFE
Most commonly used programming languages can de-
scribe processes that cannot be translated to SFE; for ex-
ample, a program that does not terminate, or one which
terminates after reading a speciﬁc input pattern.
It is
therefore necessary to impose some limitation on the de-
scriptions of functions for SFE. In systems with domain
speciﬁc languages, these limitations can be imposed by
the grammar of the language, or can be enforced by
taking advantage of particular features of the grammar.
However, one goal of our system is to allow any pro-
gramming language to be used to describe functionality
for SFE, and so we cannot rely on the grammar of the
language being used.

We make a compromise when it comes to restricting
the inputs to our system. Unlike model checking sys-
tems [2], we impose no upper bound on loop iterations or
on recursive function calls (other than the memory avail-
able for the call stack), and leave the responsibility of en-
suring that programs terminate to the user. On the other
hand, our system does forbid certain easily-detectable
conditions that could result in inﬁnite loops, such as
unconditional backwards jumps, conditional backwards
jumps that depend on input, and indirect function calls.
These restrictions are similar to those imposed by the
Fairplay and KSS12 systems [18,21], but allow for more
general iteration than incrementing the loop index by a
constant. Although false positives, i.e., programs that
terminate but which contain such constructs are possible,
our hypothesis is that useful functions and typical com-
pilers would not result in such instruction sequences, and

USENIX Association  

22nd USENIX Security Symposium  325

we observed no such functions in our experiments with
LCC.

True

[code]

If

True

If

[code]

[code]

[code]

4.3 Algorithms for Translating Bytecode
Our compiler reads a bytecode representation of the
function, which lacks the structure of higher-level de-
scriptions and poses a unique challenge in circuit gener-
ation. As mentioned above, we do not impose any upper
limit on loop iterations or the depth of the function call
stack. Our approach to translation does not use any sym-
bolic analysis of the function. Instead, we translate the
bytecode into PCF, using conditional branches and func-
tion calls as needed and translating other instructions into
lists of gates. For testing, we use the IR from the LCC
compiler, which is based on the common stack machine
model; we will use examples of this IR to illustrate our
design, but note that none of our techniques strictly re-
quire a stack machine model or any particular features of
the LCC bytecode.

In our compiler, we divide bytecode instructions into

three classes:

Normal Instructions which have exactly one successor
and which can be represented by a simple circuit.
Examples of such instructions are arithmetic and
bitwise logic operations, operations that push data
onto the stack or move data to memory, etc.

Jump Instructions that result in an unconditional con-
trol ﬂow switch to a speciﬁc label. This does not
include function calls, which we represent directly
in PCF. Such instructions are usually used for if/else
constructs or preceding the entry to a loop.

Conditional Instructions that result

in control ﬂow
switching to either a label or the subsequent instruc-
tion, depending on the result of some conditional
statement. Examples include arithmetic compar-
isons.

In the stack machine model, all operands and the
results of operations are pushed onto a global stack.
For “normal” instructions, the translation procedure is
straightforward: the operands are popped off the stack
and assigned temporary wires, the subcircuit for the op-
eration is connected to these wires, and the output of the
operation is pushed onto the stack. “Jump” instructions
appear, at ﬁrst, to be equally straightforward, but actually
require special care as we describe below.

“Conditional” instructions present a challenge. Condi-
tional jumps whose targets precede the jump are assumed
to be loop constructs, and are translated directly into PCF
branch instructions. All other conditional jumps require
the creation of multiplexers in the circuit to deal with

False

False

Figure 3: Nested if statements, which can be handled
using the stack-based algorithm.

conditional assignments. Therefore, the branch targets
must be tracked to ensure that the appropriate condition
wires are used to control those multiplexers.

In the Fairplay and KSS12 compilers, the condition
wire for an “if” statement is pushed onto a stack along
with a “scope” that is used to track the values (wire as-
signments) of variables. When a conditional block is
closed, the condition wire at the top of the stack is used
to multiplex the value of all the variables in the scope at
the top with the values from the scope second to the top,
and then the stack is popped. This procedure relies on
the grammar of “if/else” constructs, which ensures that
conditional blocks can be arranged as a tree. An exam-
ple of this type of “if/else” construct is in Figure 3. In a
bytecode representation, however, it is possible for con-
ditional blocks to “overlap” with each other without be-
ing nested.

In the sequence shown in Figure 4, the ﬁrst branch’s
target precedes the second branch’s target, and indirect
loads and assignments exist in the overlapping region of
these two branches. The control ﬂow of such an overlap
is given in Figure 5. A stack is no longer sufﬁcient in this
case, as the top of the stack will not correspond to the ap-
propriate branch when the next branch target is encoun-
tered. Such instruction sequences are not uncommon in
the code generated by production compilers, as they are
a convenient way to generate code for “else” blocks and
ternary operators.

To handle such sequences, we use a novel algorithm
based on a priority queue rather than a stack, and we
maintain a global condition wire that is modiﬁed as
branches and branch targets are reached. When a branch
instruction is reached, the global condition wire is up-
dated by logically ANDing the branch condition with
the global condition wire. The priority queue is updated
with the branch condition and a scope, as in the stack-
based algorithm; the priority is the target, with lower
targets having higher priority. When an assignment is
performed, the scope at the top of the priority queue is
updated with the value being assigned, the location be-
ing assigned to, the old value, and a copy of the global
condition wire. When a branch target is reached, multi-
plexers are emitted for each assignment recorded in the
scope at the top of the priority queue, using the copy of
the global condition wire that was recorded. After the

326  22nd USENIX Security Symposium 

USENIX Association

6

EQU4 A
INDIRI4 16
EQU4 B
INDIRI4 24
LABELV A
ASGNI4
LABELV B
ASGNI4

Figure 4: A bytecode sequence where overlapping con-
ditional blocks are not nested; note that the target of
the ﬁrst branch, “A,” precedes the target of the second
branch, “B.”

False

EQU4: A

[code]

EQU4: B

False

[code]

True

B: 
[code]

A: 
[code]

True

Figure 5: A control ﬂow with overlapping conditional
blocks.

multiplexers are emitted, the global condition wire is up-
dated by ORing the inverse of the condition wire at the
top of the priority queue, and then the top is removed.

Unconditional jumps are only allowed in the forward
direction, i.e., only if the jump precedes its target. When
such instructions are encountered, they are translated into
conditional branches whose condition wire is the inverse
of the conjunction of the condition wires of all enclos-
ing branches.
In the case of a jump that is not in any
conditional block, the condition wire is set to false; this
does not necessarily mean that subsequent assignments
will not occur, as the multiplexers for these assignments
will be emitted and will depend on a global control line
that may be updated as part of a loop construct. The op-
timizer is responsible for determining whether such as-
signments can occur, and will rewrite the multiplexers as
direct assignments when possible.

Finally, it is possible that the operand stack will have
changed in the fall-through path of a conditional jump.
In that case, the stack itself must be multiplexed. For
simplicity, we require that the depth of the stack not
change in a fall-through path. We did not observe any
such changes to the stack in our experiments with LCC.

4.4 Optimization
One of the shortcomings of the KSS12 system was the
amount of time and memory required to perform opti-
mizations on the computed circuit. In our system, opti-
mization is performed before loops are unrolled but after
the functionality is translated into a PCF representation.
This allows optimizations to be performed on a smaller

representation, but increases the complexity of the opti-
mization process somewhat.

The KSS12 compiler bases its optimization on a rudi-
mentary dataﬂow analysis, but without any conditional
branches or loops, and with single assignments to each
wire. In our system, loops are not eliminated and wires
may be overwritten, but conditional branches are elim-
inated. As in KSS12, we use an approach based on
dataﬂow analysis, but we must make multiple passes to
ﬁnd a ﬁxed point solution to the dataﬂow equations. Our
dataﬂow equations take advantage of the logical rules of
each gate, allowing more gates to be identiﬁed for elimi-
nation than the textbook equations identify.

We perform our dataﬂow analysis on individual PCF
instructions, which allows us to remove single gates even
where entire bytecode instructions could not be removed,
but which carries the cost of somewhat longer compila-
tion time, on the order of minutes for the experiments we
ran. Currently, our framework only performs optimiza-
tion within individual functions, without any interproce-
dural analysis. Compile times in our system can be re-
duced by splitting a large procedure into several smaller
procedures.

Optimization
None
Const. Prop.
Dead Elim.
Both

128 mult.
707,244
296,960
700,096
260,073

5x5 matrix
260,000
198,000
255,875
131,875

256 RSA

904,171,008
651,504,495
883,307,712
573,156,735

Table 1: Effects of constant propagation and dead code
elimination on circuit size, measured with simulator that
performs no simpliﬁcation rules. For each function, the
number of non-XOR gates are given for all combinations
of optimizations enabled.

4.4.1 Constant Propagation
The constant propagation framework we use is straight-
forward, similar to the methods used in typical compil-
ers. However, for some gates, simpliﬁcation rules can re-
sult in constants being computed even when the inputs to
a gate are not constant; for example, XORing a variable
with itself. The transfer function we use is augmented
with a check against logic simpliﬁcation rules to account
for this situation, but remains monotonic and so conver-
gence is still guaranteed.

4.4.2 Dead Gate Removal
The last step of our optimizer is to remove gates whose
output wires are never used. This is a standard bit vector
dataﬂow problem that requires little tailoring for our sys-
tem. As is common in compilers, performing this step

USENIX Association  

22nd USENIX Security Symposium  327

7

Function
16384-bit Comp.
128-bit Sum
256-sit Sum
1024-bit Sum
128-bit Mult.
256-bit Mult.
1024-bit Mult.

With
32,228
345
721
2,977
76,574
300,634
8,301,962

Without

49,314
508
1,016
4,064
260,073
1,032,416
19,209,120

Ratio
65%
67%
70%
73%
20%
20%
21%

Function
16384-bit Comp.

With (s)
Without (s)
4.41± 0.3%
4.44± 0.3%
128-bit Sum 0.0581± 0.3% 0.060± 2%
256-bit Sum
0.103± 0.3% 0.105± 0.3%
1024-bit Sum
0.365± 0.3% 0.367± 0.2%
128-bit Mult.
0.892± 0.1% 0.894± 0.1%
256-bit Mult.
3.02± 0.1%
3.04± 0.1%
1024-bit Mult.
39.7± 0.2%
39.9± 0.06%

Table 2: Non-XOR gates in circuits computed by the in-
terpreter with and without the application of simpliﬁca-
tion rules by the runtime system.

Table 3: Simulator time with simpliﬁcation rules versus
without, using the C interpreter. Times are averaged over
50 samples, with 95% conﬁdence intervals, measured us-
ing the time function implemented by SBCL.

last yields the best results, as large numbers of gates be-
come dead following earlier optimizations.

4.5 Externally-Deﬁned Functions
Some functionality is difﬁcult to describe well in byte-
code formats. For example, the graph isomorphism ex-
periment presented in Section 6 uses AES as a PRNG
building block, but the best known description of the
AES S-box is given at the bit-level [4], whereas the
smallest width operation supported by LCC is a single
byte. To compensate for this difﬁculty, we allow users to
specify functions with the same language used internally
to translate bytecode operations into circuits; an example
of this language is shown in Section 5.1. This allows for
possible combinations of our compiler with other circuit
generation and optimization tools.

4.6 PCF Interpreter
To use a PCF description of a circuit in a secure protocol,
an interpreter is needed. The interpreter simulates the ex-
ecution of the PCF ﬁle for a single-bit machine, emitting
gates as needed for the protocol. Loops are not explicitly
unrolled; instead, PCF branch instructions are condition-
ally followed, based on the logic value of some wire, and
each wire identiﬁer is treated as an address in memory.
This is where the requirement that loop bounds be in-
dependent of both parties’ inputs is ultimately enforced:
the interpreter cannot determine whether or not to take a
branch if it cannot determine the condition wire’s value.
For testing purposes, we wrote two PCF interpreters:
one in C, which is packaged as a reusable library, and
one in Java that was used for tests on smartphones. The
C library can be used as a simulator or for full protocol
execution. As a simulator it simply evaluates the PCF ﬁle
without any garbling to measure the size of the circuit
that would have been garbled in a real protocol. This
interpreter was used for the LAN tests, using an updated
version of the KSS12 protocol. The Java interpreter was

incorporated into the HEKM system for the smartphone
experiments, and can also be used in a simulator mode.

4.7 Threat Model
The PCF system treats the underlying secure computa-
tion protocol as a black box, without making any as-
sumptions about the threat model.
In Section 6, we
present running times for smaller circuits in the mali-
cious model version of the KSS12 protocol. This ma-
licious model implementation simply invokes multiple
copies of the same PCF interpreter used for the semi-
honest version, one for each copy of the circuit needed
in the protocol.

4.8 Runtime Optimization
Some optimizations cannot be performed without un-
rolling loops, and so we defer these optimizations until
the PCF program is interpreted. As an example, logic
simpliﬁcation rules that eliminate gates whose output
values depend on no more than one of their input wires
can only be partially applied at compile time, as some
potential applications of these rules might only be possi-
ble for some iterations of a loop. While it is possible to
compute this information at compile time, in the general
case this would involve storing information about each
gate for every iteration of every loop, which would be as
expensive as unrolling all loops at compile time.

A side effect of applying such logic simpliﬁcation
rules is copy propagation. A gate that always takes on
the same value as one of its inputs is equivalent to a copy
operation. The application of logic simpliﬁcation rules to
such a gate results in the interpreter simply copying the
value of the input wire to the output wire, without emit-
ting any gate. As there is little overhead resulting from
the application of simpliﬁcation rules at runtime, we are
able to reduce compile times further by not performing
this optimization at compile time.

328  22nd USENIX Security Symposium 

USENIX Association

8

Function
16384 Comp.
RSA 256
Hamming 160
Hamming 1600
3x3 Matrix
5x5 Matrix
8x8 Matrix
16x16 Matrix

This Work
32,229
235,925,023
880
9,625
27,369
127,225
522,304
4,186,368

KSS12

HFKV

49,149
332,085,981

-
-
160,949
746,177
3,058,754
24,502,530

-
-
3,003
30,318
47,871
221,625
907,776
7,262,208

Table 4: Comparisons between our compiler’s output and
the output of the KSS12 and Holzer et al. (HFKV) com-
pilers, in terms of non-XOR gates.

For each gate, the interpreter checks if the gate’s value
can be statically determined, i.e., if its output value does
not rely on either party’s input bits. This is critical, as
some of the gates in a PCF ﬁle are used for control ﬂow,
e.g., to increment a loop index. Additionally, logic sim-
pliﬁcation rules are applied where possible in the inter-
preter. This allows the interpreter to not emit gates that
follow an input or which have static outputs even when
their inputs cannot be statically determined. As shown
in Table 2, we observed cases where up to 80% of the
gates could be removed in this manner. Even in a sim-
ulator that performs no garbling, applying this runtime
optimization not only shows no performance overhead,
but actually a very slight performance gain, as shown in
Table 3. The slight performance gain is a result of the
transfer of control that occurs when a gate is emitted,
which has a small but non-trivial cost in the simulator. In
a garbled circuit protocol, this cost would be even higher,
because of the time spent garbling gates.

5 Portability

5.1 Portability Between Bytecodes
Our compiler can be given a description of how to trans-
late bytecode instructions into boolean circuits using a
special internal language. An example, for the LCC in-
struction “ADDU,” is shown in Figure 6. The ﬁrst line is
speciﬁc to LCC, and would need to be modiﬁed for use
with other front-ends. The second line assumes a stack
machine model:
this instruction reads two instructions
from the stack. Following that is the body of the transla-
tion rule, which can be used in general to describe circuit
components and how the input variables should be con-
nected to those components.

The description follows an abstraction similar to VM-
Crypt, in which a unit gadget is “chained” to create a
larger gadget. It is possible to create chains of chains,
e.g., for a shift-and-add multiplier as well. For more
complex operations, Lisp source code can be embedded,

(‘‘ADDU’’ nil second normal nil nil

(two-stack-arg (x y) (var var)
(chain [o1 = i1 + i2 + i3,

o2 = i1 + (i1 + i2) * (i1 + i3)]

(o2 -> i3

x -> i1
y -> i2
o1 -> stack)

(0 -> i3))))

Figure 6: Code used in our compiler to map the bytecode
instruction for unsigned integer addition to the subcircuit
for that operation.

which can interact directly with the compiler’s internal
data structures.

5.2 Portability Between SFE Systems
Both the PCF compiler and the interpreter can treat the
underlying secure computation system as a black box.
Switching between secure computation systems, there-
fore, requires work only at the “back end” of the inter-
preter, where gates are emitted. We envision two pos-
sible approaches to this, both of which we implemented
for our tests:

1. A single function should be called when a gate
should be used in the secure computation proto-
col. The Java implementation of PCF uses this ap-
proach, with the HEKM system.

2. Gates should be generated as if they are being read
from a ﬁle, with the secure computation system call-
ing a function. The secure computation system may
need to provide “callback” functions to the PCF in-
terpreter for copying protocol-speciﬁc data between
wires. The C implementation we tested uses this
abstraction for the KSS12 system.

6 Evaluation

We compiled a variety of functions to test our com-
piler, optimizer, and PCF interpreter. For each circuit,
we tested the performance of the KSS12 system on a
LAN, described below. For the KSS12 timings, we av-
eraged the runtime for 50 runs, alternating which com-
puter acted as the generator and which as the evaluator to
account for slight conﬁguration differences between the
systems. Compiler timings are based on 50 runs of the
compiler on a desktop PC with an Intel Xeon 5560 pro-
cessor, 8GB of RAM, a 7200 RPM hard disk, Scientiﬁc
Linux 6.3 (kernel version 2.6.32, SBCL version 1.0.38).

USENIX Association  

22nd USENIX Security Symposium  329

9

Total Gates

Function
97,733
16384-bit Comp.
4,368
Hamming 160
Hamming 1600
32,912
Hamming 16000
389,312
1,443
128-bit Sum
256-bit Sum
2,951
1024-bit Sum
11,999
105,880
64-bit Mult.
128-bit Mult.
423,064
256-bit Mult.
1,659,808
1024-bit Mult.
25,592,368
673,105,990
256-bit RSA
512-bit RSA
5,397,821,470
1024-bit RSA 42,151,698,718
92,961
433,475
1,782,656
14,308,864
482,391
10,908,749

3x3 Matrix Mult.
5x5 Matrix Mult.
8x8 Matrix Mult.
16x16 Matrix Mult.
4-Node Graph Iso.
16-Node Graph Iso.

non-XOR Gates
32,229
880
6,375
97,175
345
721
2,977
24,766
100,250
400,210
6,371,746
235,925,023
1,916,813,808
15,149,856,895
27,369
127,225
522,304
4,186,368
97,819
4,112,135

Compile Time (s)
3.40± 4%
9.81± 1%
11.0± 0.4%
10.8± 0.2%
4.70± 3%
4.60± 3%
4.60± 3%
71.7± 0.2%
74.9± 0.1%
79.5± 0.9%
74.0± 0.2%
381.± 0.2%
350.± 0.2%
564.± 0.2%
306.± 1%
343.± 0.7%
109.± 0.1%
109.± 0.1%
684.± 0.2%
1040± 0.1%

Simulator Time (s)
4.40± 0.2%
0.0810± 0.3%
0.52± 8%
4.83± 0.5%
0.0433± 0.4%
0.0732± 0.4%
0.250± 0.5%
0.332± 0.4%
0.903± 0.3%
3.07± 0.2%
40.9± 0.4%
980.± 0.3%
7,330± 0.2%
56,000± 0.3%
0.256± 0.5%
0.94± 2%
3.14± 0.3%
23.7± 0.3%
3.63± 0.5%
47.0± 0.1%

Table 5: Summary of circuit sizes for various functions and the time required to compile and interpret the PCF ﬁles
in a protocol simulator. Times are averaged over 50 samples, with 95% conﬁdence intervals, except for RSA-1024
simulator time, which is averaged over 8 samples. Run times were measured using the time function implemented in
SBCL.

Source code for our compiler, our test systems, and our
test functions is available at the authors’ website.

6.1 Effect of Array Sizes on Timing
Some changes in compile time can be observed as some
of the functions grow larger. The dataﬂow analysis deals
with certain pointer operations by traversing the entire
local variable space of the function and all global mem-
ory, which in functions with large local arrays or pro-
grams with large global arrays is costly as it increases the
number of wires that optimizer must analyze. Reducing
this cost is an ongoing engineering effort.

6.2 Experiments
We compiled and executed the circuits described below
to evaluate our compiler and representation. Several of
these circuits were tested in other systems; we present
the non-XOR gate counts of the circuits generated by our
compiler and other work in Table 4. The sizes, compile
times, and interpreter times required for these circuits are
listed in Table 5. By comparison, we show compile times
and circuit sizes using the KSS12 and HFKV compilers
in Table 6. As expected, the PCF compiler outperforms

these previous compilers as the size of the circuits grow,
due to the improved scalability of the system.
Arbitrary-Width Millionaire’s Problem As a simple
sanity check for our system, we tested an arbitrary-width
function for the millionaire’s problem; this can be viewed
as a string comparison function on 32 bit characters. It
outputs a 1 to the party which has the larger input. We
found that for this simple function, our performance was
only slightly better than the performance of the KSS12
compiler on the same circuit.
Matrix Multiplication To compare our system with the
work of Holzer et al. [12], we duplicated some of their
experiments, beginning with matrix multiplication on
32-bit integers. We found that our system performed fa-
vorably, particularly due to the optimizations our com-
piler and PCF interpreter perform. On average, our sys-
tem generated circuits that are 60% smaller. We tested
matrices of 3x3, 5x5, 8x8, and 16x16, with 32 bit integer
elements.
Hamming Distance Here, we duplicate the Hamming
distance experiment from Holzer et al. [12]. Again, we
found that our system generated substantially smaller cir-
cuits. We tested input sizes of 160, 1600, and 16000 bits.
Integer Sum We implemented a basic arbitrary-width in-
teger addition function, using ripple-carry addition. No

330  22nd USENIX Security Symposium 

USENIX Association

10

Function
16384-bit Comp.
3x3 Matrix Mult.
5x5 Matrix Mult.
8x8 Matrix Mult.
16x16 Matrix Mult.
32-bit Mult.
64-bit Mult.
128-bit Mult.
256-bit Mult.
512-bit Mult.
1024-bit Mult.

Total Gates
330,784
172,315
797,751
3,267,585
26,140,673
65,121
321,665
1,409,025
5,880,833

-
-

HFKV

non-XOR gates

131,103
47,871
221,625
907,776
7,262,208
26,624
126,529
546,182
2,264,860

-
-

Time (s)
Total Gates
105. ± 0.1%
98,303
2.2 ± 4%
424,748
8.40 ± 0.3%
1,968,452
59.4 ± 0.3%
8,067,458
2,600 ± 7% 64,570,969
15,935
6.43 ± 0.3%
64,639
71.4 ± 0.3%
999. ± 0.1%
260,351
1,044,991
16,000 ± 2%
4,187,135
16,763,518

-
-

KSS12

non-XOR gates

49,154
160,949
746,177
3,058,754
24,502,530
5,983
24,384
97,663
391,935
1,570,303
6,286,335

Time (s)
4.66 ± 0.5%
10.5 ± 0.5%
48.2 ± 0.2%
210 ± 2%
2,200 ± 1%
0.55 ± 5%
1.6 ± 2%
6.10 ± 0.6%
24.5 ± 0.2%
105. ± 0.2%
430. ± 0.3%

Table 6: Times of HFKV and KSS12 compilers with circuit sizes. The Mult. program uses a Shift-Add implementa-
tion. All times are averaged over 50 samples with the exception of the HFKV 256-bit multiplication, which was run
for 10 samples; times are given with 95% conﬁdence intervals.

array references are needed, and so our compiler easily
handles this function even for very large input sizes. We
tested input sizes of 128, 256, and 1024 bits.
Integer Multiplication Building on the integer addition
function, we tested an integer multiplication function that
uses the textbook shift-and-add algorithm. Unlike the in-
teger sum and hamming distance functions, the multipli-
cation function requires arrays for both input and out-
put, which slows the compiler down as the problem size
grows. We tested bit sizes of 64, 128, 256, and 1024.
RSA (Modular Exponentiation) In the KSS12 sys-
tem [18], it was possible to compile an RSA circuit for
toy problem sizes, and it took over 24 hours to compile
a circuit for 256-bit RSA. This lengthy compile time and
large memory requirement stems from the fact that all
loops are unrolled before any optimization is performed,
resulting in a very large intermediate representation to
be analyzed. As a demonstration of the improvement
our approach represents, we compiled not only toy RSA
sizes, but also an RSA-1024 circuit, using only modest
computational resources. We tested bit sizes of 256, 512,
and 1024.
Graph Isomorpism We created a program that allows
two parties to jointly prove the zero knowledge proof
of knowledge for graph isomorphism, ﬁrst presented by
Goldreich et al. [9]. In Goldreich et al.’s proof system,
the prover has secret knowledge of an isomorphism be-
tween two graphs, g1 and g2. To prove this, the prover
sends the veriﬁer a random graph g3 that is isomorphic
to g1 and g2, and the veriﬁer will then choose to learn
either the g1 → g3 isomorphism or the g2 → g3 isomor-
phism. We modify this protocol so that Alice and Bob
must jointly act as the prover; each is given shares of
an isomorphism between graphs g1 and g2, and will use
the online protocol to compute g3 and shares of the two
isomorphisms.

Our implementation works as follows:

the program
takes in XOR shares of the isomophism between g1 and
g2 and a random seed from both participants.
It also
takes the adjacency matrix representation of g1 as input
by a single party. The program XORs the shares together
to create the g1 → g2 isomorphism. The program then
creates a random isomorphism from g1 → g3 using AES
as the PRNG (to reduce the input sizes and thus the OT
costs), which effectively also creates g3.

Once the random isomorphism g1 → g3 is created, the
original isomorphism, g1 → g2, is inverted to get an iso-
morphism from g2 → g1. Then the two isomorphisms
are “followed” in a chain to get the g2 to g3 isomor-
phism, i.e., for the ith instance in the isomorphic ma-
trix, iso2→3[i] = iso1→3[iso2→1[i]]. The program outputs
shares of both the isomorphism from g1 to g3 and the
isomorphism from g2 to g3 to both parties.

An adjacency matrix of g3 is also an output for the
party which input the adjacency matrix g1. This is calcu-
lated by using g1 and the g1 → g3 isomorphism.

6.3 Online Running Times
To test the online performance of our new format, we
modiﬁed the KSS12 protocol to use the PCF interpreter.
Two sets of tests were run: one between two computers
with similar speciﬁcations on the University of Virginia
LAN, a busy 100 megabit Ethernet network, and one be-
tween two smartphones communicating over a wiﬁ net-
work.

For the LAN experiments, we used two comput-
ers running ScientiﬁcLinux 6.3, a four core Intel Xeon
E5506 2.13GHz CPU, and 8GB of RAM. No time limit
on computation was imposed on these machines, so we
were able to run the RSA-1024 circuit, which requires a
little less than two days. To compensate for slight con-

USENIX Association  

22nd USENIX Security Symposium  331

11

Function

CPU (s)

Network (s)

CPU (s)

Network (s)

Generator

Evaluator

99.8± 0.2%
9.13± 0.4%
91.2± 0.2%
0.749± 0.3%
2.04± 0.3%
5.74± 0.5%
72.7± 0.2%
1940± 0.2%

16384-bit Comp.
79.4± 0.2%
Hamming 1600
6.87± 2%
Hamming 16000
69.± 2%
64-bit Mult.
0.494± 0.6%
128-bit Mult.
1.31± 0.6%
256-bit Mult.
2.7± 3%
1024-bit Mult.
40.± 3%
256-bit RSA
1080± 3%
1024-bit RSA 1.15× 105 ± 0.5% 4.4× 104 ± 4% 9.5× 104 ± 5% 6.5× 104 ± 7%
4.28± 0.6%
19.5± 0.4%
81.1± 0.3%
6.15± 0.4%
60.3± 0.6%

26.0± 0.6%
2.9± 4%
28.± 3%
0.409± 0.3%
1.25± 0.2%
4.2± 2%
60.± 2%
1620± 2%

5.63± 0.6%
0.64± 4%
5.67± 0.7%
0.158± 0.7%
0.52± 1%
1.2± 2%
28.± 4%
767.± 0.7%

0.403± 0.6%
1.81± 0.4%
7.39± 0.4%
1.05± 0.7%
15.7± 0.6%

3x3 Matrix Mult.
5x5 Matrix Mult.
8x8 Matrix Mult.
4-node ISO
16-node ISO

5.33± 0.4%
24.4± 0.2%
100.± 0.2%
10.1± 0.1%
116.± 0.2%

1.45± 0.8%
6.75± 0.9%
26.8± 0.7%
4.96± 0.3%
71.6± 0.3%

Table 7: Total running time, including PCF operations and protocol operations such as oblivious transfer, for online
protocols using the PCF interpreter and the KSS12 two party computation system, on two computers communicating
over the University of Virginia LAN. With the exception of RSA-1024, all times are averaged over 50 samples; RSA-
1024 is averaged over 8 samples. Running time is divided into time spent on computation and time spent on network
operations (including blocking).

ﬁguration differences between the two systems, we alter-
nated between each machine acting as the generator and
acting as the evaluator.

We give the results of this experiment in Table 7. We
note that while the simulator times given in Table 5 are
more than half the CPU time measured, they are also on
par with the time spent waiting on the network. Non-
blocking I/O or a background thread for the PCF inter-
preter may improve performance somewhat, which is an
ongoing engineering task in our implementation.

6.4 Malicious Model Tests
The PCF system is not limited to the semi-honest model.
We give preliminary results in the malicious model ver-
sion of KSS12. These experiments were run on the same
test systems as above, using two cores for each party.
We present our results in Table 9. The increased running
times are expected, as we used only two cores per party.
In the case of 16384-bit comparison, the increase is very
dramatic, due to the large amount of time spent on obliv-
ious transfer (as both parties have long inputs).

6.5 Phone Execution
We created a PCF interpreter for use with the HEKM ex-
ecution system and ported it to the Android environment.
We then ran it on two Galaxy Nexus phones where one

phone was the generator and another phone was the eval-
uator. These phones have dual core 1.2Ghz processors
and were linked over Wi-Fi using an Apple Airport.

6.6 Phone Trials
As seen in Table 8, we were able to run the smaller pro-
grams directly on two phones. Since the interpreter ex-
ecutes slower on a phone and what would have taken
a week of LAN trials would have taken years of phone
time, we did not complete trials of the larger programs.
Not all of the programs had output for the generator, al-
lowing the generator to ﬁnish before the evaluator. This
leads to a noticeable difference in total running time be-
tween the two parties.

Mood’s work on designing SFE applications for mo-
bile devices [22] found that allocation and deallocation
was a bottleneck to circuit execution. This issue was
addressed by substituting the standard BigInteger type
for a custom class that reduced the amount of alloca-
tion required for numeric operations, resulting in a four-
fold improvement in execution time. The lack of this
optimization in our mobile phone experiments may con-
tribute to the reduced performance that we observed.

In future work, we will port the C interpreter and
KSS12 system to Android and run the experiment with
that execution system. Since overhead appears to be tied
to Android’s Dalvik Virtual Machine (DVM), running
programs natively should reduce overhead and hence re-

332  22nd USENIX Security Symposium 

USENIX Association

12

Function

CPU (s)

Network (s)

CPU (s)

Network (s)

Generator

Evaluator

16384-bit Comp.
128-bit Sum
256-bit Sum
1024-bit Sum
64-bit Mult.
128-bit Mult.
3x3 Matrix Mult.
5x5 Matrix Mult.
8x8 Matrix Mult.

163.± 0.5%
5.8± 8.2%
7.3± 5.0%
16.± 3.1%
63.3± 0.5%
257.± 0.2%
76.9± 0.4%
352.± 0.3%
1,588.± 0.1%

68.± 1%
12.± 3%
3.± 20%
1.± 30%
4.± 20%
1.± 30%
6.4± 7%
2.± 20%
5.± 10%
1.± 10%
12.± 6%
3.8± 5%
12.± 2%
8.5± 4%
32.± 4%
49.± 2%
82.± 3% 1,550.± 0.1% 120.± 1%

142.± 0.5%
5.6± 8%
6.± 5%
16.± 3%
66.3± 0.6%
280.± 0.3%
82.0± 0.5%
371.± 0.3%

Table 8: Execution results from the phone interpreter using the HEKM execution system on two Galaxy Nexus phones.
Times are averages of 50 samples, with 95% conﬁdence intervals.

Function

CPU (s)

Network (s)

CPU (s)

Network (s)

Generator

Evaluator

16384-bit comp.
128-bit sum
256-bit sum
1024-bit sum
128-bit mult.
256-bit mult.

76± 4% 2820± 2% 1200± 10%
3900± 3%
21± 2%
11.2± 0.2%
23.± 2%
27.± 4%
10± 20%
63.0± 0.4%
100± 40%
260± 10%
16± 6%
70.1± 1%
192.± 0.3% 47.2± 0.6%
637.± 0.5% 160± 1%
210± 2%

33.3± 0.5%
49.± 6%
187.± 2%
168.± 0.4%
577.± 0.3%

Table 9: Online running time in the malicious model for several circuits. Times are averaged over 50 samples, with
95% conﬁdence intervals.

duce the performance differential between the phone and
PC environments. Additionally, the KSS12 system uses
more efﬁcient cryptographic primitives, potentially fur-
ther improving performance.

7 Related Work

Compiler approaches to secure two-party computation
have attracted signiﬁcant attention in recent years. The
TASTY system presented by Henecka et al. [11] com-
bines garbled circuit approaches with homomorphic en-
cryption, and includes a compiler that emits circuits that
can be used in both models. As with Fairplay and
KSS12, TASTY requires functions to be described in a
domain-speciﬁc language. The TASTY compiler per-
forms optimizations on the abstract syntax tree for the
function being compiled. Kruger et al. developed an or-
dered BDD compiler to test the performance of their sys-
tem relative to Fairplay [19]. Mood et al. focused on
compiling secure functions on mobile devices with the
PALC system, which involved a modiﬁcation to the Fair-
play compiler [23].

Recently, a compiler approach based on bounded
model checking was present by Holzer et al. [12]. In that

work, the CBMC system [5] was used to construct cir-
cuits, which were then rewritten to have fewer non-XOR
gates. This approach had several advantages over pre-
vious approaches, most prominent being that functions
could be described in the widely used C programming
language, and that the use of CBMC allows for more
advanced software engineering techniques to be applied
to secure computation protocols. Like KSS12, however,
this approach unrolls all loops (up to some ﬁxed number
of iterations), and converts a high level description di-
rectly to a boolean circuit which must then be optimized.

In addition to SFE, work on efﬁcient compilers for
proof systems has also been presented. Almeida et al.
developed a zero-knowledge proof of knowledge com-
piler for Σ-protocols, which converts a protocol speciﬁ-
cation given in a domain-speciﬁc language into a pro-
gram for the prover and the veriﬁer to run [1]. Setty
et al. presented a system for veriﬁable computation that
uses a modiﬁcation of the Fairplay compiler, which com-
putes a system of quadratic constraints instead of boolean
circuits, and emits executables for the prover and veri-
ﬁer [28, 29]. Our system is somewhat similar to these
approaches, in that the circuit representation we present
can be viewed as a program that is executed by the par-

USENIX Association  

22nd USENIX Security Symposium  333

13

ties in the SFE system; however, our approach is unique
in its handling of control ﬂow and iterative constructs.

Closely related to our work is the Sharemind sys-
tem [3, 14], which uses secure computation as a building
block for privacy-preserving distributed applications. As
in our approach, the circuits used in the secure compu-
tation portions of Sharemind are not fully unrolled until
the protocol is actually run. Functions in Sharemind are
described using a domain-speciﬁc language called Se-
creC. Although there has been work on static analysis
for SecreC [26], the SecreC compiler does not perform
automatic optimizations. By contrast, our approach is fo-
cused on allowing circuit optimizations at the bit-level to
occur without having to unroll an entire circuit.

Kerschbaum has presented work on automatically op-
timizing secure computation at the protocol level, with
an approach based on term and expression rewriting [15,
16]. This approach is based on maximizing the use of of-
ﬂine computation by inferring what each party can com-
pute without knowledge of the other party’s input, and
does not treat the underlying secure computation primi-
tives as a black box. It therefore requires additional work
to remain secure in the malicious model. Our techniques
could conceivably be combined with Kerschbaum’s to re-
duce the overhead of online components.

8 Future Work

Our compiler can conceivably read any bytecode repre-
sentation as input; one immediate future direction is to
write translations for the instructions of another byte-
code format, such as LLVM or the JVM, which would
allow functions to be expressed in a broader range of
languages. Additionally, we believe that our techniques
could be combined with Sharemind, by having our com-
piler read the bytecode for the Sharemind VM and com-
pute optimized PCF ﬁles for cases where garbled circuit
computations are used in a Sharemind protocol.

The PCF format does not convey high-level informa-
tion about data operations or types. Such information
may further reduce the size of the circuits that are com-
puted. Static analysis of such information by compilers
has been widely studied, and it is possible that our com-
piler could be extended to support further reductions in
the sizes of circuits emitted by the PCF interpreter. High-
level information about data structures could also be used
to improve the generation of circuits prior to optimiza-
tion, using techniques recently presented by Evans and
Zahur [6].

Our system and techniques can likely be generalized to
the multiparty case, and to other representations of func-
tions, such as arithmetic circuits. This would require sig-
niﬁcant changes to the optimization strategies and goals
in our compiler, but fewer changes would be necessary

for the PCF interpreter. Similar modiﬁcations to support
homomorphic encryption systems are also possible.

9 Conclusion

We have presented an approach to compiling and stor-
ing circuits for secure computation systems that requires
substantially lower computational resources than previ-
ous approaches. Empirical evidence of the improve-
ment and utility of our approach is given, using a vari-
ety of functions with different circuit sizes and control
ﬂow structures. Additionally, we have presented a com-
piler for secure computation that reads bytecode as an in-
put, rather than a domain-speciﬁc language, and have ex-
plored the challenges associated with such an approach.
We also presented interpreters, which evaluate our new
language on both PCs and phones.

The code for the compiler, PCF interpreters, and test

cases will be available on the authors’ website.

Acknowledgments We would like to thank Elaine Shi
for her helpful advice. We also thank Chih-hao Shen for
his help with porting KSS12 to use PCF. This material is
based on research sponsored by the Defense Advanced
Research Projects Agency (DARPA) and the Air Force
Research Laboratory (AFRL) under contract FA8750-
11-2-0211. The U.S. Government is authorized to re-
produce and distribute reprints for Governmental pur-
poses notwithstanding any copyright notation thereon.
The views and conclusions contained herein are those
of the authors and should not be interpreted as neces-
sarily representing the ofﬁcial policies or endorsements,
either expressed or implied, of DARPA or the U.S. Gov-
ernment.

References
[1] J. B. Almeida, E. Bangerter, M. Barbosa, S. Krenn, A.-R.
Sadeghi, and T. Schneider. A Certifying Compiler For Zero-
Knowledge Proofs of Knowledge Based on Σ-Protocols. In Pro-
ceedings of the 15th European conference on Research in com-
puter security, ESORICS’10, pages 151–167, Berlin, Heidelberg,
2010. Springer-Verlag.

[2] A. Biere, A. Cimatti, E. M. Clarke, and Y. Zhu. Symbolic Model
Checking without BDDs. In Proceedings of the 5th International
Conference on Tools and Algorithms for Construction and Anal-
ysis of Systems, TACAS ’99, pages 193–207, London, UK, UK,
1999. Springer-Verlag.

[3] D. Bogdanov, S. Laur, and J. Willemson. Sharemind: A Frame-
work for Fast Privacy-Preserving Computations. In Proceedings
of the 13th European Symposium on Research in Computer Secu-
rity - ESORICS’08, 2008.

[4] J. Boyar and R. Peralta. A New Combinational Logic Minimiza-
tion Technique with Applications to Cryptology. In P. Festa, ed-
itor, Experimental Algorithms, volume 6049 of Lecture Notes in
Computer Science, pages 178–189. Springer Berlin / Heidelberg,
2010.

334  22nd USENIX Security Symposium 

USENIX Association

14

[5] E. Clarke, D. Kroening, and F. Lerda. A Tool for Checking ANSI-
C Programs.
In K. Jensen and A. Podelski, editors, Tools and
Algorithms for the Construction and Analysis of Systems (TACAS
2004), volume 2988 of Lecture Notes in Computer Science, pages
168–176. Springer, 2004.

[6] D. Evans and S. Zahur. Circuit structures for improving efﬁciency
of security and privacy tools. In IEEE Symposium on Security and
Privacy (to appear), 2013.

[7] S. Even, O. Goldreich, and A. Lempel. A randomized protocol
for signing contracts. Commun. ACM, 28(6):637–647, June 1985.
[8] C. W. Fraser and D. R. Hanson. A Retargetable C Compiler: De-
sign and Implementation. Addison-Wesley Longman Publishing
Co., Inc., Boston, MA, USA, 1995.

[9] O. Goldreich, S. Micali, and A. Wigderson. Proofs that yield
nothing but their validity or all languages in np have zero-
knowledge proof systems. J. ACM, 38(3):690–728, July 1991.

[10] V. Goyal, P. Mohassel, and A. Smith. Efﬁcient Two Party and
Multi Party Computation Against Covert Adversaries.
In Pro-
ceedings of 27th annual international conference on Advances
in cryptology, EUROCRYPT’08, pages 289–306, Berlin, Heidel-
berg, 2008. Springer-Verlag.

[11] W. Henecka, S. K¨ogl, A.-R. Sadeghi, T. Schneider, and
I. Wehrenberg. TASTY: Tool for Automating Secure Two-partY
computations. In ACM Conference on Computer and Communi-
cations Security, 2010.

[12] A. Holzer, M. Franz, S. Katzenbeisser, and H. Veith. Secure Two-
Party computations in ANSI C. In Proceedings of the 2012 ACM
conference on Computer and communications security, CCS ’12,
pages 772–783, New York, NY, USA, 2012. ACM.

[13] Y. Huang, D. Evans, J. Katz, and L. Malka. Faster Secure Two-
Party Computation Using Garbled Circuits. In USENIX Security
Symposium, 2011.

[14] R. Jagom¨agis. SecreC: a Privacy-Aware Programming Language
with Apllications in Data Mining. Master’s thesis, University of
Tartu, 2010.

[15] F. Kerschbaum. Automatically optimizing secure computation.
In Proceedings of the 18th ACM conference on Computer and
communications security, CCS ’11, pages 703–714, New York,
NY, USA, 2011. ACM.

[16] F. Kerschbaum. Expression rewriting for optimizing secure com-
putation. In Conference on Data and Application Security and
Privacy, 2013.

[17] V. Kolesnikov and T. Schneider. Improved Garbled Circuit: Free
XOR Gates and Applications. In L. Aceto, I. Damg˚ard, L. Gold-
berg, M. Halld´orsson, A. Ing´olfsd´ottir, and I. Walukiewicz, edi-
tors, ALP 2008, volume 5126 of LNCS, pages 486–498. Springer,
2008.

[18] B. Kreuter, A. Shelat, and C.-H. Shen. Billion-gate secure com-
putation with malicious adversaries. In Proceedings of the 21st
USENIX conference on Security symposium, Security’12, pages
14–14, Berkeley, CA, USA, 2012. USENIX Association.

[19] L. Kruger, S. Jha, E.-J. Goh, and D. Boneh. Secure function
evaluation with ordered binary decision diagrams. In Proceedings
of the 13th ACM conference on Computer and communications
security (CCS’06), Alexandria, VA, Oct. 2006.

[20] L. Malka. VMCrypt: modular software architecture for scalable
secure computation. In ACM Conference on Computer and Com-
munications Security, pages 715–724, 2011.

[21] D. Malkhi, N. Nisan, B. Pinkas, and Y. Sella. Fairplay: A Secure
Two-Party Computation System. In 13th Conference on USENIX
Security Symposium, volume 13, pages 287–302. USENIX Asso-
ciation, 2004.

[22] B. Mood. Optimizing Secure Function Evaluation on Mobile De-

vices. Master’s thesis, 2012, University of Oregon.

[23] B. Mood, L. Letaw, and K. Butler. Memory-Efﬁcient Garbled
Circuit Generation for Mobile Devices. In Financial Cryptogra-
phy and Data Security, volume 7397. Springer Berlin Heidelberg,
2012.

[24] B. Pinkas, T. Schneider, N. Smart, and S. Williams. Secure Two-
Party Computation Is Practical. In M. Matsui, editor, Asiacrypt,
volume 5912 of LNCS, pages 250–267. Springer, 2009.

[25] M. Rabin. How to Exchange Secrets by Oblivious Transfer.
Technical Report TR-81, Harvard Aiken Computation Labora-
tory, 1981.

[26] J. Ristioja. An analysis framework for an imperative privacy-
preserving programming language. Master’s thesis, Institute of
Computer Science, University of Tartu, 2010.

[27] T. Schneider. Engineering Secure Two-Party Computation Proto-
cols - Design, Optimization, and Applications of Efﬁcient Secure
Function Evaluation. Springer, 2012.

[28] S. Setty, R. McPherson, A. J. Blumberg, and M. Walﬁsh. Mak-
ing Argument Systems for Outsourced Computation Practical
(Sometimes). In NDSS, 2012.

[29] S. Setty, V. Vu, N. Panpalia, B. Braun, A. J. Blumberg, and
M. Walﬁsh. Taking proof-based veriﬁed computation a few steps
closer to practicality. In Proceedings of the 21st USENIX confer-
ence on Security symposium, Berkeley, CA, USA, 2012.

[30] A. Yao. Protocols for Secure Computations.

In 23rd Sympo-
sium on Foundations of Computer Science, pages 160–164. IEEE
Computer Society, 1982.

A PCF Semantics

The PCF ﬁle format consists of a header section that de-
clares the input size, followed by a list of operations that
are divided into subroutines. At runtime, these opera-
tions manipulate the internal state of the PCF interpreter,
causing gates to be emitted when necessary. The inter-
nal state of the PCF interpreter consists of an instruction
pointer, a call stack, an array of wire values, and an ar-
ray of pointers. The pointers are positive integers. Wire
values are 0, 1, or ⊥, where ⊥ represents a value that de-
pends on input data, which is supplied by the code that
invokes the interpreter. Each position in the wire table
can be treated as a stack.

Each PCF instruction can take up to 3 arguments. The

instructions and their semantics are as follows:

CLABEL/SETLABELC Appears only in the header,
used for setting the input size for each party. CLA-
BEL declares the bit width of a value, SETLA-
BELC sets the value.

FUNCTION Denotes the beginning of a subroutine.
When the subroutine is called,
the instruction
pointer is set to the position following this instruc-
tion.

GADGET Denotes a branch target

USENIX Association  

22nd USENIX Security Symposium  335

15

A.1 Example PCF Description
Below is an example of a PCF ﬁle. It iterates over a loop
several times times, XORing the two parties’ inputs with
a bit from the internal state.

GADGET: main
CLABEL ALICEINLENGTH 32
CLABEL BOBINLEGNTH 32
CLABEL xxx 32
SETLABELC ALICEINLENGTH 128
SETLABELC ALICEINLENGTH 128
FUNCTION: main
1111 32 0 0
0000 33 0 0
0000 34 0 0
0000 35 0 0
GADGET: L
0110 36 35 34
0001 35 36 36
0110 36 34 33
0001 34 36 36
0110 36 33 32
0001 33 36 36
ALICEINPUT32 0 0
0001 36 0 0
BOBINPUT32 0 0
0001 37 0 0
0110 38 37 36
0110 39 33 38
SHIFT OUT ALICE 39
BRANCH L 35
RETURN xxx

BRANCH Takes two arguments: a target, declared with
GADGET, and a location in the wire table. In the
wire value is 0, the instruction pointer is set to the
instruction following the target. If the wire value is
1, the instruction pointer is incremented. If the wire
value is ⊥, evaluation halts with an error.

FUNC Calls a subroutine, pushing the current instruc-

tion pointer onto the call stack.

PUSH Pushes a copy of the wire value at a speciﬁed

position onto the stack at that position.

POP Pops a stack at a speciﬁed position. If there is only
one value on that stack, evaluation halts with an er-
ror.

ALICEIN32/BOBIN32 Fetches 32 input bits from one
party, beginning at a speciﬁed bit position in that
party’s input. The bit position is speciﬁed by an
array of 32 values in the wire table. If any of the
values is ⊥, evaluation halts with an error. The input
values will all have the value ⊥, and will be stored
in the wire table at positions 0 through 31.

SHIFT OUT Outputs a single bit for a given party
RETURN Return from a subroutine. The instruction
pointer is repositioned to the value popped from the
top of the call stack.

STORECONSTPTR Sets a value in the pointer table
OFFSETPTR Adds a value to a pointer, speciﬁed by an
array of 32 wire values starting at a position in the
wire table. If any value in the array is ⊥, evaluation
halts with an error.

PTRTOWIRE Saves a pointer value as a 32 bit un-
signed integer. Each of the bits is pushed onto the
stack at a location in the wire table.

PTRTOPTR Copies a value from one position in the

pointer table to another.

CPY121 Copy a wire value from a position speciﬁed by

a pointer to a statically speciﬁed position.

CPY32 Copy a wire value from a statically speciﬁc po-

sition to a position speciﬁed by a pointer.

g0,0g0,1g1,0g1,1 Compute a gate with the speciﬁed truth
table on two input values from the wire table, with
output stored at a speciﬁed position. Logic simpli-
ﬁcation rules are applied when one or both of the
input values is ⊥. If no simpliﬁcation is possible,
then the output will be ⊥ and the interpreter will
emit a gate. This is used for both local computa-
tions such as updating a loop index, and for com-
puting the gates used by the protocol.

336  22nd USENIX Security Symposium 

USENIX Association

16

