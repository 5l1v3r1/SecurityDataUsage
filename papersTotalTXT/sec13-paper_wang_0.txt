You Are How You Click: Clickstream Analysis 

for Sybil Detection

Gang Wang and Tristan Konolige, University of California, Santa Barbara;  

Christo Wilson, Northeastern University; Xiao Wang, Renren Inc.;  

Haitao Zheng and Ben Y. Zhao, University of California, Santa Barbara

Open access to the Proceedings of the 22nd USENIX Security Symposium is sponsored by USENIXThis paper is included in the Proceedings of the 22nd USENIX Security Symposium.August 14–16, 2013 • Washington, D.C., USAISBN 978-1-931971-03-4You are How You Click: Clickstream Analysis for Sybil Detection

Gang Wang, Tristan Konolige, Christo Wilson†, Xiao Wang‡,

‡Renren Inc.
UC Santa Barbara
{gangw, tkonolige, htzheng, ravenben}@cs.ucsb.edu, cbw@ccs.neu.edu, xiao.wang@renren-inc.com

Haitao Zheng and Ben Y. Zhao
†Northeastern University

Abstract

Fake identities and Sybil accounts are pervasive in to-
day’s online communities. They are responsible for a
growing number of threats, including fake product re-
views, malware and spam on social networks, and as-
troturf political campaigns. Unfortunately, studies show
that existing tools such as CAPTCHAs and graph-based
Sybil detectors have not proven to be effective defenses.
In this paper, we describe our work on building a prac-
tical system for detecting fake identities using server-side
clickstream models. We develop a detection approach
that groups “similar” user clickstreams into behavioral
clusters, by partitioning a similarity graph that cap-
tures distances between clickstream sequences. We vali-
date our clickstream models using ground-truth traces of
16,000 real and Sybil users from Renren, a large Chinese
social network with 220M users. We propose a practical
detection system based on these models, and show that it
provides very high detection accuracy on our clickstream
traces. Finally, we worked with collaborators at Renren
and LinkedIn to test our prototype on their server-side
data. Following positive results, both companies have
expressed strong interest in further experimentation and
possible internal deployment.

1 Introduction
It is easier than ever to create fake identities and user ac-
counts in today’s online communities. Despite increas-
ing efforts from providers, existing services cannot pre-
vent malicious entities from creating large numbers of
fake accounts or Sybils [9]. Current defense mecha-
nisms are largely ineffective. Online Turing tests such as
CAPTCHAs are routinely solved by dedicated workers
for pennies per request [22], and even complex human-
based tasks can be overcome by a growing community
of malicious crowdsourcing services [23, 39]. The result
of this trend is a dramatic rise in forged and malicious

online content such as fake reviews on Yelp [35], mal-
ware and spam on social networks [10, 11, 32], and large,
Sybil-based political lobbying efforts [27].
Recent work has explored a number of potential so-
lutions to this problem. Most proposals focus on de-
tecting Sybils in social networks by leveraging the as-
sumption that Sybils will ﬁnd it difﬁcult to befriend real
users. This forces Sybils to connect to each other and
form strongly connected subgraphs [36] that can be de-
tected using graph theoretic approaches [8, 34, 45, 46].
However, the efﬁcacy of these approaches in practice is
unclear. While some Sybil communities have been lo-
cated in the Spanish Tuenti network [7], another study on
the Chinese Renren network shows the large majority of
Sybils actively and successfully integrating themselves
into real user communities [43].
In this paper, we describe a new approach to Sybil
detection rooted in the fundamental behavioral patterns
that separate real and Sybil users. Speciﬁcally, we pro-
pose the use of clickstream models as a tool to detect
fake identities in online services such as social networks.
Clickstreams are traces of click-throughevents generated
by online users during each web browsing “session,” and
have been used in the past to model web trafﬁc and user
browsing patterns [12, 20, 24, 28]. Intuitively, Sybils and
real users have very different goals in their usage of on-
line services: where real users likely partake of numerous
features in the system, Sybils focus on speciﬁc actions
(i.e. acquiring friends and disseminating spam) while try-
ing to maximize utility per time spent. We hypothesize
that these differences will manifest as signiﬁcantly dif-
ferent (and distinctive) patterns in clickstreams, making
them effective tools for “proﬁling” user behavior. In our
context, we use these proﬁles to distinguish between real
and Sybil users.
Our work focuses on building a practical model for ac-
curate detection of Sybils in social networks. We develop
several models that encode distinct event sequences and
inter-event gaps in clickstreams. We build weighted

USENIX Association  

22nd USENIX Security Symposium  241

graphs of these sequences that capture pairwise “similar-
ity distance” between clickstreams, and apply clustering
to identify groups of user behavior patterns. We validate
our models using ground-truth clickstream traces from
16,000 real and Sybil users from Renren, a large Chinese
social network with 220M users. Using our methodol-
ogy, we build a detection system that requires little or
no knowledge of ground-truth. Finally, we validate the
usability of our system by running initial prototypes on
internal datasets at Renren and LinkedIn.

The key contributions of this paper are as follows:
• To the best of our knowledge, we are the ﬁrst to ana-
lyze click patterns of Sybils and real users on social
networks. By analyzing detailed clickstream logs
from a large social network provider, we gain new in-
sights on activity patterns of Sybils and normal users.
• We propose and evaluate several clickstream mod-
els to characterize user clicks patterns. Specially,
we map clickstreams to a similarity graph, where
clickstreams (vertices) are connected using weighted
edges that capture pairwise similarity. We apply
graph partitioning to identify clusters that repre-
sent speciﬁc click patterns. Experiments show that
our model can efﬁciently distinguish between click-
streams of Sybil and normal users.
• We develop a practical Sybil detection system based
on our clickstream model, requiring minimal in-
put from the service provider. Experiments using
ground-truth data show that our system generates
<1% false positives and <4% false negatives.
• Working closely with industrial collaborators, we
have deployed prototypes of our system at Renren
and LinkedIn. Security teams at both companies
have run our system on real user data and received
very positive results. While corporate privacy poli-
cies limit the feedback visible to us, both companies
have expressed strong interest in further experimen-
tation and possible deployment of our system.
To the best of our knowledge, we are the ﬁrst to study
clickstream models as a way to detect fake accounts in
online social networks. Moving forward, we believe
clickstream models are a valuable tool that can com-
plement existing techniques, by not only detecting well-
disguised Sybil accounts, but also reducing the activity
level of any remaining Sybils to that of normal users.
Roadmap.
We begin in Section 2 by describing the
problem context and our ground-truth dataset, followed
by preliminaryanalysis results in Section 3. Next, in Sec-
tion 4 we propose our clickstream models to effectively
distinguish Sybil with normal users. Then in Section 5,
we develop an incremental Sybil detector that can scale
with today’s large social networks. We then extend this
detector in Section 6 by proposing an unsupervised Sybil

Dataset
Sybil
Normal

Date (2011)
Clicks
1,008,031
Feb.28-Apr.30
5,856,941 Mar.31-Apr.30

Users
9,994
5,998
Table 1: Clickstream dataset.

Sessions
113,595
467,179

detector, where only a minimal (and ﬁxed) amount of
ground-truthis needed. Finally, in Section 7, we describe
experimental experience of testing our prototype code in
real-world social networks (Renren and LinkedIn). We
then discuss related work in Section 8 and conclude in
Section 9.

2 Background
In this section, we provide background for our study.
First, we brieﬂy introduce the Renren social network
and the malicious Sybils that attack it. Second, we de-
scribe the key concepts of user clickstreams, as well as
the ground-truth dataset we use in our study.
Renren.
Renren is the oldest and largest Online So-
cial Network (OSN) in China, with more than 220 mil-
lion users [17]. Renren offers similar features and func-
tionalities as Facebook: users maintain personal proﬁles
and establish social connections with their friends. Ren-
ren users can update their status, write blogs, upload pho-
tos and video, and share URLs to content on and off Ren-
ren. When a user logs-in to Renren, the ﬁrst page they
see is a “news-feed” of their friends’ recent activities.
Sybils.
Like other popular OSNs, Renren is targeted
by malicious parties looking to distribute spam and steal
personal information. As in prior work, we refer to the
fake accounts involved in these attacks as Sybils [43].
Our goal is to detect and deter these malicious Sybils; our
goal is not to identify benign fakes, e.g. pseudonymous
accounts used by people to preserve their privacy.
Prior studies show that attackers try to friend normal
users using Sybil accounts [43]. On Renren, Sybils usu-
ally have complete, realistic proﬁles and use attractive
proﬁle pictures to entice normal users.
It is challeng-
ing to identify these Sybils using existing techniques be-
cause their proﬁles are well maintained, and they inte-
grate seamlessly into the social graph structure.
Clickstream Data.
In this paper, we investigate the
feasibility of using clickstreams to detect Sybils. A click-
stream is the sequence of HTTP requests made by a user
to a website. Most requests correspond to a user explic-
itly fetching a page by clicking a link, although some
requests may be programmatically generated (e.g. Xml-
HttpRequest). In our work, we assume that a clickstream
can be unambiguously attributed to a speciﬁc user ac-
count, e.g. by examining the HTTP request cookies.
Our study is based on detailed clickstreams for 9994

242  22nd USENIX Security Symposium 

USENIX Association

)

%

(
 
s
r
e
s
U

 
f

 

o
F
D
C

 100
 80
 60
 40
 20
 0

 1

i

s
n
o
s
s
e
S

 
f

 

o
%

 10
 9
 8
 7
 6
 5
 4
 3
 2
 1
 0

 1000

Sybil
Normal

 0  2  4  6  8  10  12  14  16  18  20  22

Hour In Day

)

%

(
 
s
r
e
s
U

 
f

 

o
F
D
C

 100
 80
 60
 40
 20
 0

 1

 2

Sybil
Normal
 3
 8
Sessions Per Day Per User

 4

 7

 5

 6

 9  10

Sybil
Normal
 100

 10

# of Sessions Per User

Figure 2: Sessions through the day.

Figure 3: Sessions per day per user.

)

%

(
 
s
r
e
s
U

 
f

 

o
F
D
C

 100
 80
 60
 40
 20
 0
 0.1

 100

Sybil
Normal

 1
 100
Average Inter-arrival Time 

 10

 Per Session Per User (Seconds)

 1000

Sybil
Normal

 10

Average Clicks Per Session Per User

Sybil
Normal

Figure 1: # of sessions per user.
 100
 80
 60
 40
 20
 0

)

%

(
 
s
r
e
s
U

 
f

 

o
F
D
C

)

%

(
 
s
r
e
s
U

 
f

 

o
F
D
C

 100
 80
 60
 40
 20
 0

 1

 10

 100

 1
 1000
Average Session Length Per User (Seconds)
Figure 4: Average session length per
user.
Sybils and 5998 normal users on Renren. Sybil click-
streams were selected at random from the population of
malicious accounts that were banned by Renren in March
and April 2011. Accounts could be banned for abu-
sive behaviors such as spamming, harvesting user data
or sending massive numbers of friend requests. Nor-
mal user clickstreams were selected uniformly at random
from Renren user population in April 2011, and were
manually veriﬁed by Renren’s security team.
In total,
The dataset summary is shown in Table 1.
our dataset includes 1,008,031 and 5,856,941 clicks for
Sybils and normal users, respectively. Each click is char-
acterized by a timestamp, an anonymized userID, and an
activity. The activity is derived from the request URL,
and describes the action the user is undertaking. For ex-
ample, the “friend request” activity corresponds to a user
sending a friend request to another user. We discuss the
different categories of activities in detail in Section 3.2.
Each user’s clickstream can be divided into sessions,
where a session represents the sequence of a user’s clicks
during a single visit to Renren. Unfortunately, users do
not always explicitly end their session by logging out of
Renren. As in prior work, we assume that a user’s ses-
sion is over if they do not make any requests for 20 min-
utes [6]. Session duration is calculated as the time in-
terval between the ﬁrst and last click within a session.
Overall, our traces contain 113,595 sessions for Sybils
and 467,179 sessions for normal users.

3 Preliminary Clickstream Analysis
We begin the analysis of our data by looking at the high-
level characteristics of Sybil and normal users on Ren-

Figure 5: Average # of clicks per ses-
sion per user.

Figure 6: Average time interval be-
tween clicks per session per user.

ren. Our goals are to provide an overview of the dataset,
and to motivate the use of clickstreams as a rich data
source for uncoveringmalicious behavior. Towards these
ends, we analyze our data in four ways: ﬁrst, we exam-
ine session-level characteristics. Second, we analyze the
activities users engage in during each session. Third, we
construct a state-based Markov Chain model to charac-
terize the transitions between clicks during sessions. Fi-
nally, we use a Support Vector Machine (SVM) approach
to learn the important features that distinguish Sybil and
normal user clickstreams.

3.1 Session-level Characteristics
In this section, we seek to determine the session-level
differences between normal and Sybil accounts in our
dataset. First, we examine the total number of sessions
from each user. As shown in Figure 1, >50% of Sybils
have only a single session; far fewer than normal users.
It is likely that these Sybils sent spam during this sin-
gle session and were banned shortly thereafter. A small
portion of Sybils are very active and have >100 sessions.
Next, we examine when Sybils and normal users are
active each day. Figure 2 shows that all users exhibit a
clear diurnal pattern, with most sessions beginning dur-
ing daytime. This indicates that at least a signiﬁcant por-
tion of Sybils in our dataset could be controlled by real
people exhibiting normal behavioral patterns.
Next, we investigate the number of sessions per user
per day. Figure 3 shows that 80% of Sybils only login to
Renren once per day or less, versus 20% of normal users.
The duration of Sybil sessions is also much shorter, as
shown in Figure 4: 70% of Sybil session are <100 sec-
onds long, versus 10% of normal sessions. The vast ma-

USENIX Association  

22nd USENIX Security Symposium  243

jority of normal sessions last several minutes.
Figure 5 shows the number of clicks per session per
user. Almost 60% of Sybil sessions only contain one
click, whereas 60% of normal user sessions have ≥10
clicks. Not only do Sybil sessions tend to be shorter,
but Sybils also click much faster than normal users. As
shown in Figure 6, the average inter-arrival time between
Sybil clicks is an order of magnitude shorter than for nor-
mal clicks. This indicates that Sybils do not linger on
pages, and some of their activities may be automated.
The observed session-level Sybil characteristics are
driven by attacker’s attempts to circumvent Renren’s se-
curity features. Renren limits the number of actions each
account can take, e.g. 50 friend requests per day, and 100
proﬁles browsed per hour. Thus, in order to maximize
efﬁciency, attackers create many Sybils, quickly login to
each one and perform malicious activities (e.g. sending
unsolicited friend requests and spam), then logout and
move to the next Sybil. As shown in Table 2, Sybils
spend a great deal of clicks sending friend requests and
browsing proﬁles, despite Renren’s security restrictions.

3.2 Clicks and Activities
Having characterized the session-level characteristics of
our data, we now analyze the type and frequency clicks
within each session. As shown in Table 2, we organize
clicks into categories that correspond to high-level OSN
features. Within each category there are activities that
map to particular Renren features. In total, we observe 55
activities that can be grouped into 8 primary categories.
These categories are:

• Friending: Includes sending friend requests, accept-
ing or denying those requests, and un-friending.
• Photo:
Includes uploading photos, organizing al-
bums, tagging friends, browsing friend’s photos, and
writing comments on photos.
• Proﬁle: This category encompasses browsing user
proﬁles. Like Facebook, proﬁles on Renren can be
browsed by anyone, but the information that is dis-
played is restricted by the owner’s privacy settings.
• Share: Refers to users posting hyperlinks on their
wall. Common examples include links to videos and
news stories on external websites, or links to blog
posts and photo albums on Renren.
• Message: Includes status updates, wall posts, and
real-time instant-messages (IM).
• Blog: Encompasses writing blogs, browsing blog ar-
ticles, and leaving comments on blogs.
• Notiﬁcation: Refers to clicks on Renren’s notiﬁca-
tion mechanism that alerts users to comments or likes
on their content.

Category

Friending

Description
Send request
Accept invitation
Invite from guide
Visit photo
Visit album
Visit proﬁles
Share content
Send IM
Visit/reply blog

Sybil Clks Nrml Clks
# (K) % # (K) %
0
417
20
0
16
0
76
242
Photo
6
25
4
160
Proﬁle
4
27
Share
20
2
Message
2
12
Blog
Notiﬁcation Check notiﬁcation
2
8
Table 2: Clicks from normal users and Sybils on different
Renren activities. # of clicks are presented in thousands.
Activities with <1% of clicks are omitted for brevity.

16
13
0
4,432
330
214
258
99
103
136

41
2
2
24
2
16
3
2
1
1

• Like: Corresponds to the user liking (or unliking)

content on Renren.

Table 2 displays the most popular activities on Ren-
ren. The number of clicks on each activity is shown (in
thousands), as well as the percent of clicks. Percentages
are calculated for Sybils and normal users separately, i.e.
each “%” column sums to 100%. For the sake of brevity,
only activities with ≥1% of clicks for either Sybils or
normal users are shown. The “Like” category has no ac-
tivity with ≥1% of clicks, and is omitted from the table.
Table 2 reveals contrasting behavior between Sybils
and normal users. Unsurprisingly, normal users’ clicks
are heavily skewed toward viewing photos (76%), al-
bums (6%), and sharing (4%).
In contrast, Sybils ex-
pend most of their clicks sending friend requests (41%),
viewing photos (24%), and browsing proﬁles (16%). The
photo browsing and proﬁle viewing behavior are related:
these Sybils crawl Renren and download users’ personal
information, including proﬁle photos.

Sybils’ clicks are heavily skewed toward friending
(41% for Sybils, 0.3% for normal users). This behavior
supports one particular attack strategy on Renren: friend-
ing normal users and then spamming them. However,
given that other attacks are possible (e.g. manipulating
trending topics [16], passively collecting friends [32]),
we cannot rely on this feature alone to identify Sybils.

Normal users and Sybils share content (4% and 3%,
respectively) as well as send messages (2% and 2%)
at similar rates. This is an important observation, be-
cause sharing and messaging are the primary channels
for spam dissemination on Renren. The similar rates
of legitimate and illegitimate sharing/messaging indicate
that spam detection systems cannot simply leverage nu-
meric thresholds to detect spam content.

244  22nd USENIX Security Symposium 

USENIX Association

0.89

Friend Invitation

0.38

0.91

0.07

INITIAL

0.06

Photo

0.04

FINAL

0.44

0.05

0.57

Profile

0.34

0.07

0.21

Notification

0.33

0.42

Blog

0.19

0.11

0.25

0.93

0.14

0.31

INITIAL

0.39

Photo

0.04

FINAL

0.13

0.17

0.16

0.31

Share

Profile

0.46

0.16

0.47

0.31

(a) State transitions for a Sybil account.

(b) State transitions for a real user.

Figure 7: Categories and transition probabilities in the clickstream models of Sybils and normal users.

3.3 Click Transitions
Sections 3.1 and 3.2 highlight some of the differences
between Sybils and normal users. Next, we examine dif-
ferences in click ordering, i.e. how likely is it for a user
to transition from activity A to activity B during a single
session?
We use a Markov Chain model to analyze click tran-
sitions. In this model, each state is a click category, and
edges represent transitions between categories. We add
two abstract states, initial and ﬁnal, that mark the begin-
ning and end of each click session. Figure 7 shows the
category transition probabilities for both Sybils and nor-
mal users. The sum of all outgoing transitions from each
category is 1.0. To reduce the complexity of the Figure,
edges with probability <5% have been pruned (except
for transitions to the ﬁnal state). Categories with no in-
coming edges after this pruning process are also omitted.
Figure 7(a) demonstrates that Sybils follow a very reg-
imented set of behaviors. After logging-in Sybils imme-
diately begin one of three malicious activities: friend in-
vitation spamming, spamming photos, or proﬁle brows-
ing. The proﬁle browsing path represents crawling be-
havior: the Sybil repeatedly views user proﬁles until their
daily allotment of views is exhausted.
Compared to Sybils, normal users (Figure 7(b)) en-
gage in a wider range of activities, and the transitions
between states are more diverse. The highest centrality
category is photos, and it is also the most probable state
after login. Intuitively, users start from their newsfeed,
where they are likely to see and click on friends’ recent
photos. The second most probable state after login is
checking recent notiﬁcations. Sharing and messaging are
both low probability states. This makes sense, given that
studies of interactions on OSNs have shown that users
generate new content less than once per day [41, 17].
It is clear from Figure 7 that currently, Sybils on Ren-
ren are not trying to precisely mimic the behavior of nor-
mal users. However, we do not feel that this type of
modeling represents a viable Sybil detection approach.

Simply put, it would be trivial for Sybils to modify
their behavior in order to appear more like normal users.
If Sybils obfuscated their behavior by decreasing their
transition probability to friending and proﬁle browsing
while increasing their transition probability to photosand
blogs, then distinguishing between the two modelswould
be extremely difﬁcult.
3.4 SVM Classiﬁcation
The above analysis shows that Sybil sessions have very
different characteristics compared to normal user ses-
sions. Based on these results, we explore the possibil-
ity of distinguishing normal and Sybil sessions using a
Support Vector Machine (SVM) [26]. For our SVM ex-
periments, we extract 4 features from session-level infor-
mation and 8 features from click activities:
• Session Features: We leverage 4 features extracted
from user sessions: average clicks per session, aver-
age session length, average inter-arrivaltime between
clicks, and average sessions per day.
• Click Features: As mentioned in Section 3.2, there
are 8 categories of clicks activities on Renren. For
each user, we use the percentage of clicks in each
category as a feature.
We computed values for all 12 features for all users in
our dataset, input the data to an SVM, and ran 10 fold
cross-validation. The resulting classiﬁcation accuracy
was 98.9%, with 0.8% false positives (i.e. classify nor-
mal users as Sybils) and 0.13% false negatives (i.e. clas-
sify Sybils as normal users). Table 3 shows the weights
assigned to the top 5 features. Features with positive
weight values are more indicative of Sybils, while fea-
tures with negative weights indicate they are more likely
in normal users. Overall, higher absolute value of the
weights corresponds to features that more strongly indi-
cate either Sybils or normal users. These features agree
with our ad-hoc observations in previous sections.

USENIX Association  

22nd USENIX Security Symposium  245

Feature
% of clicks under Friending
% of clicks under Notiﬁcation
Time interval of clicks (TBC)
Session length (SL)
% of clicks under Photo

Weight
+5.65
-3.68
-3.73
+1.34
+0.93

Table 3: Weight of features generated by SVM.
While our SVM results are quite good, an SVM-based
approach is still a supervised learning tool. In practice,
we would like to avoid using any ground truth datasets
to train detection models, since they can introduce un-
known biases. Later, we will describe our unsupervised
detection techniques in detail.

3.5 Discussion
In summary, we analyze the Renren clickstream data to
characterize user behavior from three angles: sessions,
click activities, and click transitions. SVM analysis of
these basic features demonstrates that clickstreams are
useful for identifying Sybils on social networks.

However,

these basic tools (session distributions,
Markov Chain models, SVM) are of limited use in prac-
tice: they require training on large samples of ground-
truth data. For a practical Sybil detection system, we
must develop clickstream analysis techniques that lever-
age unsupervised learning on real-time data samples, i.e.
require zero or little ground-truth. In the next section, we
will focus on developing clickstreams models for real-
time, unsupervised Sybil detection.

4 Clickstream Modeling and Clustering
In Section 3, we showed that clickstream data for Sybils
and normal users captured the differences in their behav-
ior. In this section, we build models of user activity pat-
terns that can effectively distinguish Sybils from normal
users. Our goal is to cluster similar clickstreams together
to form general user “proﬁles” that capture speciﬁc activ-
ity patterns. We then leverage these clusters (or proﬁles)
to build a Sybil detection system.

We begin by deﬁning three models to represent a
user’s clickstream. For each model, we describe similar-
ity metrics that allow us to cluster similar clickstreams
together. Finally, we use our ground-truth data to eval-
uate the efﬁcacy of each model in distinguishing Sybils
from normal users. We build upon these results later to
develop practical Sybil detection systems based on click-
stream analysis.

4.1 Clickstream Models
We deﬁne three models to capture a user’s clickstream.
Click Sequence Model.
We start with the
most straightforward model, which only considers click
events. As shown in Section 3, Sybils and normal users
exhibit different click transition patterns and focus their
energy on different activities. The Click Sequence (CS)
Model treats each user’s clickstream as a sequence of
click events, sorted by order of arrival.
Time-based Model.
As shown in Figure 6,
Sybils and normal users generate click events at different
speeds. The Time-based Model focuses on the distribu-
tion of gaps between events: each user’s clickstream is
represented by a list of inter-arrival times [t1,t2,t3, ...,tn]
where n is the number of clicks in a user’s clickstream.
Hybrid Model.
The Hybrid Model combines click
types and click inter-arrival times. Each user’s click-
stream is represented as an in-order sequence of clicks
along with inter-event gaps between clicks. For exam-
ple: a(t1)c(t2)a(t3)d(t4)b where a,b,c,d are click types,
and ti is the time interval between the ith and (i + 1)th
event.
Click Types. Both the Click Sequence Model and the
Hybrid Model represent each event in the sequence by
its click event type. We note that we can control how
granular the event types are in our sequence representa-
tion. One approach is to encode clicks based on their
speciﬁc activity. Renren’s logs deﬁne 55 unique activi-
ties. Another option is to encode click events using their
broader category. In our dataset, our 55 activities fall un-
der 8 click categories(see Section 3.2). Our experimental
analysis evaluates both representations to understand the
impact of granularity on model accuracy.

4.2 Computing Sequence Similarity
Having deﬁned three models of clickstream sequences,
we now move on to investigating methods to quantify the
similarity between clickstreams. In other words, we want
to compute the distance between pairs of clickstreams.
First, we discuss general approaches to computing the
distance between sequences. Then we discuss how to
apply each approach to our three clickstream models.

4.2.1 Deﬁning Distance Functions
Common Subsequences.
The ﬁrst distance met-
ric involves locating the common subsequences of vary-
ing lengths between two clickstreams. We formalize
a clickstream as a sequence S = (s1s2...si...sn), where
is the ith element in the sequence. We then de-
si
ﬁne Tk as the set of all possible k-grams (k consecu-

246  22nd USENIX Security Symposium 

USENIX Association

tive elements) in sequence S: Tk (S) = {k-gram|k-gram =
(sisi+1...si+k−1),i ∈ [1,n + 1 − k]}. Simply put, each k-
gram in Tk (S) is a subsequence of S. Finally, the distance
between two sequences can then be computed based on
the number of common subsequences shared by the two
sequences. Inspired by the Jaccard Coefﬁcient [19], we
deﬁne the distance between sequences S1 and S2 as:

Dk(S1,S2) = 1 −

|Tk (S1) ∩Tk(S2)|
|Tk (S1) ∪Tk(S2)|

(1)

We will discuss the choice of k in Section 4.2.2.
Common Subsequences With Counts.
The com-
mon subsequence metric deﬁned above only measures
distinct common subsequences, i.e. it does not consider
the frequency of common subsequences. We propose a
second distance metric that rectiﬁes this by taking the
count of common subsequences into consideration. For
sequences S1, S2 and a chosen k, we ﬁrst compute the
set of all possible subsequences from both sequences as
T = Tk(S1) ∪ Tk(S2). Next, we count the frequency of
each subsequence within each sequence i (i = 1,2) as ar-
ray [ci1,ci2, ...,cin] where n = |T |. Finally, the distance
between S1 and S2 can be computed as the normalized
Euclidean Distance between the two arrays:

(2)

(c1j −c2j)2

D(S1,S2) = 1√2(cid:31) n
∑
j=1
Distribution-based Method.
Unfortunately, the
prior metrics cannot be applied to sequences of contin-
uous values (i.e. the Time-based Model).
Instead, for
continuous value sequences S1 and S2, we compute the
distance by comparing their value distribution using a
two-sample KolmogorovSmirnov test (K-S test). A two-
sample K-S test is a general nonparametric method for
comparing two empirical samples. It is sensitive to dif-
ferences in location and shape of the empirical Cumu-
lative Distribution Functions (CDF) of the two samples.
We deﬁne the distance function using K-S statistics:

D(S1,S2) = supt |Fn,1(t ) −Fn′,2(t )|

(3)

where Fn,i(t ) is the CDF of values in sequence Si.

4.2.2 Applying Distances Functions to Clickstreams
Having deﬁned three distance functions for computing
sequence similarity, we now apply these metrics to our
three clickstream models. Table 4 summarizes the dis-
tance metrics we apply to each of our models. The Time-
based Model is the simplest case, because it only has one
corresponding distance metric (K-S Test). For the Click
Sequence and Hybrid Models, we use several different
parameterizations of our distance metrics.

Model
Click Sequence Model
Time-based Model
Hybrid Model

Distance Metrics
unigram, unigram+count,
10gram, 10gram+count
K-S test
5gram, 5gram+count
Table 4: Summary of distance functions.

Click Sequence Model. We use the common subse-
quence and common subsequence with counts metrics to
compute distances in the CS model. However, these two
metrics require that we choose k, the length of k-gram
subsequences to consider. We choose two values for k: 1
and 10, which we refer to as unigram and 10gram. Un-
igram represents the trivial case of comparing common
click events in two clickstreams, while ignoring the or-
dering of clicks. 10gramincludes all unigrams, as well as
bigrams, trigrams, etc. As shown in Table 4, we also in-
stantiate unigram+count and 10gram+count, which in-
clude the frequency counts of each unique subsequence.
Although values of k > 10 are possible, we limit our
experiments to k = 10 for two reasons. First, when k = n
(where n is the length of a clickstream), the computa-
tional complexity becomes O(n2). This overhead is sig-
niﬁcant when you consider that O(n2) subsequences will
be computed for every user in a clickstream dataset. Sec-
ond, long subsequenceshave diminishing utility, because
they are likely to be unique for a particular user. In our
tests, we found k = 10 to be a good limit on computa-
tional overhead and subsequence over-speciﬁcity.
Hybrid Model.
Like the Click Sequence Model, dis-
tances between sequences in the Hybrid Model can also
be computed using the common subsequence and com-
mon subsequence plus count metrics. The only change
between the Click Sequence and Hybrid Models is that
we must discretize the inter-arrival times between clicks
so they can be encoded into the sequence. We do this
by placing inter-arrival times into log-scale buckets (in
seconds):
[0,1], [1,10], [10,100], [100,1000], [1000,∞].
Based on Figure 6, the inter-arrival time distribution is
highly skewed, so log-scale buckets are better suited than
linear buckets to evenly encode the times.

After we discretize the inter-arrival times and insert
them into the clickstream, we use k = 5 as the parameter
for conﬁguringthe two distance metrics. Further increas-
ing k offers little improvement in the model but intro-
duces extra computation overhead. As shown in Table 4,
we refer to these metrics as 5gram and 5gram+count.
Thus, each 5gram contains three consecutive click events
along with two tokens representing inter-arrival time
gaps between them.

USENIX Association  

22nd USENIX Security Symposium  247

)

%

(
 

e

t

a
R

 
r
o
r
r

E

 10
 8
 6
 4
 2
 0

False Positive
False Negative

(Activities)

(Categories)

CS Hybrid CS Hybrid

Models

Time

)

%

(
 

e

t

a
R

 
r
o
r
r

E

 7
 6
 5
 4
 3
 2
 1
 0

False Positive
False Negative
(CS Model)

(Hybrid Model)

unigram

10gram

unigram-c
10gram-c
Distance Functions

5gram

5gram-c

)

%

(
 

e

t

a
R

 
r
o
r
r

E

 6
 5
 4
 3
 2
 1
 0

False Positive
False Negative

10 20 30 40 50 60 70 80 90 100

# of Clusters (Hybrid Model)

Figure 8: Error rate of three models.

Figure 9: Error rate using different
distance functions.

Figure 10: Impact of number of clus-
ters (K).

4.3 Sequence Clustering
At this point we have deﬁned models of clickstreams
as well as metrics for computing the distance between
them. Our next step is to cluster users with similar click-
streams together. As shown in Section 3, Sybil and nor-
mal users exhibit very different behaviors, and should
naturally form distinctive clusters.

To achieve our goal, we build and then partition a
sequence similarity graph. Each user’s clickstream is
represented by a single node. The sequence similarity
graph is complete, i.e. every pair of nodes is connected
by a weighted edge, where the weight is the similarity
distance between the sequences. Partitioning this graph
means producing the desired clusters while minimizing
the total weight of cut edges: users with similar activi-
ties (high weights between them) will be placed in the
same cluster, while users with dissimilar activities will
be placed in different clusters. Thus the clustering pro-
cess separates Sybil and normal users. Note that not all
Sybils and normal users exhibit homogeneous behavior;
thus, we expect there to be multiple, distinct clusters of
Sybils and normal users.
Graph Clustering.
To cluster sequence similarity
graphs, we use METIS [18], a widely used multilevel k-
way partitioning algorithm. The objective of METIS is
to minimize the weight of edges that cross partitions. In
the sequence similarity graph, longer distances (i.e. dis-
similar sequences) have lower weights. Thus, METIS
is likely to place dissimilar sequences in different parti-
tions. METIS requires a parameter K that speciﬁes the
number of partitions desired. We will assess the impact
of K on our system performance in Section 4.4.
Cluster Quality.
A key question when evaluat-
ing our methodology is assessing the quality of clus-
ters produced by METIS. In Section 4.4, we leverage
our ground-truth data to evaluate false positives and
negatives after clustering the sequence similarity graph.
We label each cluster as “Sybil” or “normal” based on
whether the majority of nodes in the cluster are Sybils
or normal users. Normal users that get placed into Sybil
clusters are false positives, while Sybils placed in normal

clusters are false negatives. We use these criteria to eval-
uate different clickstream models and distance functions.

4.4 Model Evaluation
We now evaluate our clickstream models and distance
functions to determine which can best distinguish Sybil
activity patterns from those of normal users. We examine
four different variables: 1) choice of clickstream model,
2) choice of distance function for each model, 3) what
representation of clicks to use (speciﬁc activities or cat-
egories), and 4) K, the number of desired partitions for
METIS.
Experiment Setup.
The experimental dataset con-
sists of 4000 normal users and 4000 Sybils randomly se-
lected from our dataset. In each scenario, we build click
sequences for each user (based on a given clickstream
model and click representation), compute all distances
between each pair of sequences, and then cluster the re-
sulting sequence similarity graph for a given value of K.
Finally, each experimental run is evaluated based on the
false positive and negative error rates.
Model Analysis.
First, we examine the error rates
of different clickstream models and click representations
in Figure 8. For the CS and Hybrid models, we en-
code clicks based on activities as well as categories.
In the Time model, all clicks are encoded as inter-
arrival times. In this experiment, we use 10gram+count,
5gram+count, and K-S as the distance function for CS,
Hybrid, and Time, respectively. We ﬁx K = 100. We in-
vestigate the impact of distance functions and K in sub-
sequent experiments.
Two conclusions can be drawn from Figure 8. First,
the CS and Hybrid models signiﬁcantly outperform the
Time-based model, especially in false negatives. This
demonstrates that click inter-arrival times alone are in-
sufﬁcient to disambiguate Sybils from normal users.
Manual inspection of false negative Sybils from the Time
experimentreveals that these Sybils click at the same rate
as normal users. Thus these Sybils are either operated by
real people, or the software that controls them has been
intentionally rate limited.

248  22nd USENIX Security Symposium 

USENIX Association

The second conclusion from Figure 8 is that encod-
ing clicks based on category outperforms encoding by
activity. This result conﬁrms ﬁndings from the existing
literatures on web usage mining [3]: representing clicks
using high-level categories (or concepts) instead of raw
click types better exposes the browsing patterns of users.
A possible explanation is that high-level categories have
better tolerance for noise in the clickstream log. In the
rest of our paper, we use categories to encode clicks.
Next, we examine the error rate of different distance
functions for the CS and Hybrid models. As shown in
Figure 9, we evaluate the CS model using the unigram
and 10gram functions, as well as counting versions of
those functions. We evaluate the Hybrid model using the
5gram and 5gram+count functions.
Several conclusions can be drawn from Figure 9. First,
the unigram functions have the highest false negative
rates. This indicates that looking at clicks in isolation
(i.e. without click transitions) is insufﬁcient to discover
many Sybils. Second, the counting versions of all three
distance functions produce low false positive rates. This
demonstrates that the repeat frequency of sequences is
important for identifying normal users. Finally, we ob-
serve that CS 10gram+countand Hybrid have similar ac-
curacy. This shows that click inter-arrival times are not
necessary to achieve low error rates.
Finally, we examine the impact of the number of clus-
ters K on detection accuracy. Figure 10 shows the error
rate of Hybrid 5gram+count as we vary K. The overall
trend is that larger K produces lower error rates. This
is because larger K grants METIS more opportunities to
partition weakly connected sequences. This observation
is somewhat trivial: if K = N, where N is the number
of sequences in the graph, then the error rate would be
zero given our evaluation methodology. In Section 6, we
discuss practical reasons why K must be kept ≈100.
Summary.
Our evaluation shows that the Click
Sequence and Hybrid models perform best at disam-
biguating Sybils and normal users. 10gram+count and
5gram+count are the best distance functions for each
model, respectively. We ﬁnd that accuracy is highest
when clicks are encoded based on categories, and when
the number of partitions K is large. In the following sec-
tions, we will use these parameters when building our
Sybil detection system.
5 Incremental Sybil Detection
Our results in Section 4 showed that our models can ef-
fectively distinguish between Sybil clickstreams and nor-
mal user clickstreams. In this section, we leverage this
methodology to build a real-time, incremental Sybil de-
tector. This system works in two phases: ﬁrst, we cre-
ate clusters of Sybil and normal users based on ground-

truth data, as we did in Section 4. Second, we compute
the position of unclassiﬁed clickstreams in our sequence
similarity graph. If an unclassiﬁed clickstream falls into
a cluster representing clickstreams from ground-truth
Sybils, we conclude the new clickstream is a Sybil. Oth-
erwise, it is benign.
5.1 Incremental Detection
To classify a new clickstream given an existing clustered
sequence similarity graph, we must determine how to
“re-cluster” new sequences into the existing graph. We
investigate three algorithms.
The ﬁrst is K Nearest Neighbor (KNN). For a given
unclassiﬁed sequence, we ﬁnd the top-K nearest se-
quences in the ground-truth data. If the majority of these
sequences are located in Sybil clusters, then the new se-
quence is classiﬁed as a Sybil sequence.
The second algorithm is Nearest Cluster (NC). We
compute the average distance from an unclassiﬁed se-
quence to all sequences in each cluster. The unclassiﬁed
sequence is then added to the cluster with the closest av-
erage distance. The new sequence is classiﬁed as Sybil
or normal based on the cluster it is placed in.
The third algorithm is a less computationally-intensive
version of Nearest Cluster that we refer to as Nearest
Cluster-Center (NCC). NC and KNN require comput-
ing the distance from an unclassiﬁed sequence to all se-
quences in the ground-truth clusters. We can streamline
NC’s classiﬁcation process by precomputing centers for
each cluster. In NCC, we only need to compute the dis-
tance from an unclassiﬁed sequence to the center of each
existing cluster.
For each existing cluster, the center is chosen by close-
ness centrality. Intuitively, the center sequence is the one
that has the shortest distance to all the other sequences
in the same cluster. To be more robust, we precompute
three centers for each cluster, that is, the three sequences
with highest closeness centrality.
5.2 System Evaluation
In this section, we evaluate our incremental Sybil detec-
tion system using our ground-truth clickstream dataset.
We start by evaluating the basic accuracy of the system at
classifying unknown sequences. Next, we evaluate how
quickly the system can identify Sybils, in terms of num-
ber of clicks in their clickstream. Finally, we explore
how long the system can remain effective before it needs
to be retrained using updated ground-truth data.
Detection Accuracy. We start with a basic evaluation
of system accuracy using our ground-truth dataset. We
split the dataset into training data and testing data. Both
datasets include 3000 Sybils and 3000 normal users. We
build sequence similarity graphs from the training data

USENIX Association  

22nd USENIX Security Symposium  249

)

%

(
 

e

t

a
R

 
r
o
r
r

E

 5
 4
 3
 2
 1
 0

False Positive
False Negative

KNN

NC

NCC

Detection Algorithm

)

%

(
 

e

t

a
R

 
r
o
r
r

E

 6
 5
 4
 3
 2
 1
 0

False Positive
False Negative

(KNN)

(NC)

(NCC)

50 100 All

50 100 All
# of Clicks

50 100 All

)

%

(
 

e

t

a
R

 
r
o
r
r

E

 5
 4
 3
 2
 1
 0

False Positive
False Negative

KNN

NC

NCC

Detection Algorithm

Figure 11: Error rate of three reclus-
tering algorithms.

Figure 12: Error rate vs. maximum #
of clicks in each sequence.

Figure 13: Detection accuracy when
training data is two weeks old.

using Hybrid Model with 5gram+countas distance func-
tion. The number of clusters K = 100. In each sequence
similarity graph, we label the Sybil and normal clusters.
Next, we examine the error rates of the incremental
detector when unclassiﬁed users (3000 Sybils and 3000
normal users) are added to the sequence similarity graph.
We perform this experiment three times, once for each
of the proposed reclustering algorithms (KNN, NC and
NCC). As shown in Figure 11, the error rates for all three
reclustering algorithms are very similar, and all three
have <1% false positives. NC has slightly fewer false
positives, while NCC has the fewest false negatives.
Detection Speed.
The next question we want to ad-
dress is: what is the minimum number of clicks neces-
sary to accurately classify clickstreams? Another way to
frame this question is in terms of detection speed: how
quickly (in terms of clicks) can our system accurately
classify clickstreams? To identify and respond to Sybils
quickly, we must detect Sybils using the minimal number
of click events.
Figure 12 shows the results of our evaluation when the
maximum number of clicks in all sequences are capped.
The “All” results refer to a cap of inﬁnity, i.e. all clicks
in our dataset are considered. Note that not all sequences
in our dataset have 50 or 100 clicks: some Sybils were
banned before they produced this may clicks. Hence, the
caps are upper bounds on sequence length.
Surprisingly, the “All” results are not the most accurate
overall. As shown in Figure 12, using all clicks results
in more false negatives. This occurs due to overﬁtting:
given a large number of very long clickstreams from nor-
mal users, it is likely that they will occasionally exhibit
unusual, Sybil-like behavior. However, this problem is
mitigated if the sequence length is capped, since this nat-
urally excludes these infrequent, aberrant clickstreams.
In contrast to the “All” results, the results from the
≤ 50 click experiments produce the most false posi-
tives. This demonstrates that there is a minimum se-
quence length necessary to perform accurate classiﬁca-
tion of clickstreams. We repeated these experiments us-
ing CS/10gram+count and received similar result, which
we omit for brevity.

There are two additional, practical take-aways from
Figure 12. First, the NCC algorithm performs equally
well versus NC and KNN. This is a positive result,
since the computational complexity of NCC is dramat-
ically lower than NC and KNN. Second, we observe that
our system can produce accurate results (false positives
<1%, false negatives <3%) when only considering short
sequences. This means that the system can make classiﬁ-
cations quickly, without needing to store very long click-
streams in memory.
Accuracy Over Time.
In order for our incremen-
tal detection system to be practically useful, its accuracy
should remain high for long periods of time. Put an-
other way, sequence similarity graphs trained with old
data should be able to detect fresh Sybil clickstreams. To
evaluate the accuracy of our system over time, we split
our dataset based on date. We train our detector using
the early data, and then apply the detector to the later
data. We restrict our analysis to data from April 2011;
although we have Sybil data from March 2011, we do not
have corresponding data on normal users for this month.
Figure 13 shows the accuracy of the detector when it is
trained on data from March 31-April 15, then applied to
data from April 16-30. As the results show, the detector
remains highly accurate for at least two weeks after it has
been trained using the NCC reclustering algorithm. Un-
fortunately, the limited duration of our dataset prevents
us from examining accuracy at longer time intervals.
We repeated this experiment using only one week of
training data, but the false negative rate of the detector
increased to ≈10%. This shows that the detector needs
to be trained on sufﬁcientdata to provideaccurate results.
6 Unsupervised Sybil Detection
Our incremental Sybil detection system from Section 5
has a serious shortcoming: it must be trained using large
samples of ground-truth data.
In this section, we de-
velop an unsupervised Sybil detection system that re-
quires only a small, constant amount of ground-truth.
The key idea is to build a clustered sequence similarity
graph as before. But instead of using full ground-truth

250  22nd USENIX Security Symposium 

USENIX Association

Known Good

Users

METIS

Partitions

Colored Clusters

Uncolored Cluster

)

%

(
 

e
g
a
r
e
v
o
C

 
r
e

t
s
u
C

l

 
l

a
m
r
o
N

 100
 80
 60
 40
 20
 0

 50

)

%

(
 

e
g
a
r
e
v
o
C

 
r
e

t
s
u
C

l

 
l

a
m
r
o
N

 100
 80
 60
 40
 20
 0
March(1-15) March(16-31) April(1-15)
Time of Datasets

600 Seeds
450 Seeds
300 Seeds

April(16-30)

20 Clusters
50 Clusters
100 Clusters

 100

Number of Seeds

 150

 200

 250

Figure 14: Unsupervised clustering
with coloring.

Figure 15: # of seeds vs. % of cor-
rectly colored normal user clusters.

Figure 16: Consistency over time of
normal seeds for coloring.

of all clickstreams to mark a cluster as Sybil or normal,
we only need a small number of clickstreams of known
real users as “seeds” that color the clusters they reside
in. These seeds can be manually veriﬁed as needed. We
color all clusters that include a seed sequence as “nor-
mal,” while uncolored clusters are assumed to be “Sybil.”
Since normalusers are likely to fall under a small number
of behavioral proﬁles (clusters in the graph), we expect a
small ﬁxed number of seeds will be sufﬁcient to color all
clusters of normal user clickstreams.
Figure 14 depicts our unsupervised approach, showing
how METIS partitions nodes into clusters which are then
colored if they contain seed users. Once the system is
trained in this manner, it can be used incrementally to
detect more Sybils over time, as described in Section 5.
In this section, we discuss the design of our unsuper-
vised system and evaluate its performance. We begin by
analyzing the number and composition of seeds that are
necessary to ensure high accuracy of the system. Next,
we evaluate the performance of the system by compar-
ing its accuracy to our ground-truth data. Finally, we
examine how the ratio of Sybils to normal users in the
unclassiﬁed data impacts system accuracy.
6.1 Seed Selection and Composition
Number of Seeds.
The most important parameter in
our unsupervised Sybil detection system is the number
of seeds. On one hand, the number of seeds needs to be
large and diverse enough to color all “normal” clusters.
Normal clusters that remain uncolored are potential false
positives. On the other hand, the seed set needs to be
small enough to be practical. If the size of the seed set
is large, it is equivalent to having ground-truth about the
dataset, which is the situation we are trying to avoid.
We now conduct experiments to determine how many
seeds are necessary to color the clusters. We choose
3000 Sybils and 3000 normal users at random from our
dataset to be the unclassiﬁed dataset. We also randomly
choose some number of additional normal users to be the
seeds. As in Section 5, we use the Hybrid Model with
the 5gram+count distance function. We also conducted

experiments with CS/10gram+count, but the results are
very similar and we omit them for brevity.
Figure 15 depicts the percentage of normal of clus-
ters that are correctly colored for different values of K
(number of METIS partitions) as the number of seeds is
varied. As expected, fewer seeds are necessary when K
is small because there are fewer clusters (and thus each
cluster includes more sequences). When K = 100, 250
seeds (or 4% of all normal users in the experiment) are
able to color 99% of normal clusters.
Seed Consistency Over Time.
Next, we examine
whether a set of seeds chosen at an early date are equally
effective at coloring clusters based on later data. In other
words, we want to know if the seeds are consistent over
time. If this is not the case, it would represent additional
overhead on the deployment of our system.
To test seed consistency over time, we divide our two
months of Sybil clickstream data into four, two-week
long datasets. We add an equal number of randomly
selected normal users to each of the four datasets. Fi-
nally, we select an additional x random normal users to
act as seeds. We verify (for each value of x) that these
seeds color 100% of the normal clusters in the ﬁrst tem-
poral dataset. We then evaluate what percentage of nor-
mal clusters are colored in the subsequent three tempo-
ral datasets. In all experiments, we set K = 100, i.e. the
worst case scenario for our graph coloring approach.
The results of the temporal consistency experiments
are shown in Figure 16. In general, even though the Sybil
and normal clickstreams change over time, the vast ma-
jority of normal clusters are successfully colored. Given
600 seeds, 99% of normal clusters are colored after 4
weeks, although the percentage drops to 83% with 300
seeds. These results demonstrate that the seed set does
not need to be drastically altered over time.
6.2 Coloring Evaluation
We now evaluate the overall effectiveness of our Sybil
detection system when it leverages unsupervised train-
ing. In these experiments, we use our entire clickstream
dataset. We choose x random normal users as seeds,

USENIX Association  

22nd USENIX Security Symposium  251

build and cluster the sequence similarity graph using Hy-
brid/5gram+count, and then color the clusters that con-
tain the seeds. Finally, we calculate the false positive
and negative rates using the same methodologyas in Sec-
tion 5, i.e. by comparing the composition of the colored
clusters to ground-truth.

The results are shown in Figure 17. As the num-
ber of seeds increases, the false positive rate decreases.
This is because more seeds mean more normal clusters
are correctly colored. With just 400 seeds, the false
positive rate drops to <1%. Unfortunately, relying on
unsupervised training does increase the false negative
rate of our system by 2% versus training with ground-
truth data. However, in cases where ground-truth data
is unavailable, we believe that this is a reasonable trade-
off. Note that we also repeated these experiment with
CMS/10gram+count, and it produced slightly higher
false positive rates, although they were still <1%.
Unbalanced Training Dataset.
Next, we evaluate
the impact of having an unbalanced training dataset (e.g.
more normal users than Sybils) on the accuracy of our
system. Thus far, all of our experiments have assumed
a roughly equal percentage of Sybils and normal users
in the data. However, in practice it is likely that normal
users will outnumber Sybils when unsupervised learning
is used. For example, Facebook suspects that 8.7% of its
user base is illegitimate, out of >1 billion total users [1].
We now evaluate how detection accuracy changes
when we decrease the percentage of Sybils in the train-
ing data. In these experiments, we construct training sets
of 6000 total users with different normal-to-Sybil ratios.
We then run unsupervised training with 500 seeds. Fi-
nally, we incrementally add an additional 3000 Sybils
and 3000 normal users to the colored similarity graph
using the NCC algorithm (see Section 5.1). We ran ad-
ditional tests using the NC and KNN algorithms, but the
results were very similar and we omit them for brevity.
Figure18 shows the ﬁnal error rate of the system (i.e.
after 6000 users have been incrementally added) for
varying normal-to-Sybil ratios. The false positive rate
remains ≤1.2% regardless of the normal-to-Sybil ratio.
This is a very good result: even with highly skewed
training data, the system is unlikely to penalize normal
users. Unfortunately, the false negative rate does rise as
the number of Sybils in the training data falls. This result
is to be expected: the system cannot adequately classify
Sybil clickstreams if it is trained on insufﬁcient data.
Handling False Positives.
The above analy-
sis demonstrates that our system achieves high accuracy
with a false positive rate of 1% or less. Through man-
ual inspection, we ﬁnd that “false positives” generated
by our detector exhibit behaviors generally attributed to
Sybils, including aggressively sending friend requests or

browsing proﬁles. In real-world OSNs, suspicious users
identiﬁed by our system could be further veriﬁed via ex-
isting complementary systems that examines other as-
pects of users. For example, this might include systems
that classify user proﬁles [32, 43], systems that verify
user real-world identity [2], or even Sybil detection sys-
tems using crowdsourced human inspection [38]. These
efforts could further protect benign users from misclassi-
ﬁcation.
7 Practical Sybil Detection
In this section, we examine the practical performance of
our proposed Sybil detection system. First, we shipped
our code to the security teams at Renren and LinkedIn,
where it was evaluated on fresh data in a production en-
vironment. Both test results are very positive, and we
report them here. Second, we discuss the fundamental
limits of our approach, by looking at our impact on Sybil
accounts that can perfectly mimic the clickstream pat-
terns of normal users.
7.1 Real-world Sybil Detection
With the help of supportive collaborators at both Ren-
ren and LinkedIn, we were able to ship prototype code
to the security teams at both companies for internal test-
ing on fresh data. We conﬁgured our system to use un-
supervised learning to color clusters. Sequence similar-
ity graphs are constructed using the Hybrid Model and
the 5gram+count distance function, and the number of
METIS partitions K is 100.
Renren.
Renren’s security team trained our system
using clickstreams from 10K users, of which 8K were
randomly selected, and 2K were previously identiﬁed
as suspicious by the security team. These clickstreams
were collected between January 17–27, 2013. 500 hon-
est users that have been manually veriﬁed by Renren’s
security team were used as seeds. Once trained, our sys-
tem was fed clickstreams from 1 million random users
(collected in early February, 2013) for classiﬁcation as
normal or suspicious. In total, our system identiﬁed 22K
potential Sybil accounts. These accounts are now being
investigated by the security team.
While corporate privacy policies prevented Renren
from sharing detailed results with us, their feedback was
very positive. They also indicated that our system identi-
ﬁed a new attack performed by a large cluster of users
whose clickstream behavior focused heavily on photo
sharing. Manual inspection revealed that these photos
used embedded text to spread spam for brands of clothes
and shoes. Traditional text analysis-based spam detec-
tors and URL blacklists were unable to catch this new
attack, but our system identiﬁed it immediately.

252  22nd USENIX Security Symposium 

USENIX Association

)

%

(
 

e

t

a
R

 
r
o
r
r

E

 10
 8
 6
 4
 2
 0

False Positive
False Negative

300

400

500

Number of Seeds

)

%

(
 

e

t

a
R

 
r
o
r
r

E

 14
 12
 10
 8
 6
 4
 2
 0

600

False Positive
False Negative

1

2

5

Normal-Sybil Ratio

)

%

(
 
F
D
C

 100
 80
 60
 40
 20
 0

 0

 2

10

Friending
Messages
Profiles
 6

 4

Clicks Per Day

 8

 10

Figure 17: Detection accuracy versus
number of seeds.

Figure 18: Detection accuracy versus
Normal-Sybil ratio.

Figure 19: Clicks per day by outlier
normal users.

LinkedIn.
LinkedIn’s security team used our soft-
ware to analyze the clickstreams of 40K users, of which
36K were randomly sampled, and 4K were previously
identiﬁed as suspicious by the security team. These
clickstreams were gathered in February, 2013. Again,
our feedback was very positive, but did not include pre-
cise statistics. However, we were told that our system
conﬁrmed that ≈1700 of the 4000 suspicious accounts
are likely to be Sybils. Our system also detected an ad-
ditional 200 previously unknown Sybils.
A closer look at the data shows that many of the ac-
counts not detected by our system were borderline ac-
counts with speciﬁc ﬂags popping up in their proﬁles.
For example, some accounts had unusual names or oc-
cupational specialties, while others had suspicious URLs
in their proﬁles. These results remind us that a behavior
model is clearly only a part of the equation, and should
be used in conjunction with existing proﬁle analysis tools
and spam detectors [5, 10, 37, 38, 44].
Ongoing Collaboration.
In summary, the security
teams at both Renren and LinkedIn were very pleased
with the initial results of our system. We plan to con-
tinue collaborating with both groups to improve our sys-
tem and implement it in production.
7.2 Limits of Sybil Detection
Finally, we wish to discuss the worst case scenario for
our system, i.e. a scenario where attackers have full
knowledge of the clickstream patterns for real users,
and are able to instrument the behavior of their Sybils
to mimic them precisely.
In this attack model, the at-
tacker’s goal is to have Sybils carry out malicious actions
(e.g. sending spam) without being detected. However, to
evade detection, these Sybils must limit themselves to
behavior consistent with that of normal users.
We can thus bound the capabilities of Sybils that avoid
detection in this attack model. First, the Sybil’s click-
stream must remain inside the “normal” clusters pro-
duced by our detector. Second, the most aberrant behav-
ior within a given “normal” cluster is exhibited by real
users within the cluster who are farthest from the center.

The activities performed by these outliers serve as effec-
tive bounds on Sybil behavior. Sybil clickstreams cannot
deviate from the center of the cluster more than these
outliers, otherwise they will be excluded from the clus-
ter and risk detection. Thus, we can estimate the maxi-
mum amount of malicious activity a Sybil could perform
(without getting caught) by studying these outliers.
We now examine the behavior of outliers. We cali-
brate our system to produce clusters with false positive
rate <1% using Hybrid/5gram+count, and K = 100. In
this conﬁguration, the detector outputs 40 Sybil and 60
normal clusters when run on our full dataset. Next, we
identify the two farthest outliers in each normal cluster.
Finally, we plot the clicks per day in three activities from
the 120 outliers in Figure 19. We focus on clicks for
sending friend requests, posting status updates/wall mes-
sages, and viewing user proﬁles. These activities corre-
spond to the three most common attacks we observe in
our ground-truth data, i.e. sending friend request spam,
status/wall spam, and proﬁle crawling.
As shown in Figure 19, 99% of outliers generate ≤10
clicks per day in the target activities.
In the vast ma-
jority of cases, even the outliers generate <2 clicks per
day. These results show that the effective bound on Sybil
behavior is very tight, i.e. to avoid detection, Sybils can
barely generate any clicks each day. These bounds sig-
niﬁcantly increase the cost for attackers, since they will
need many more Sybils to maintain the same level of
spam generation capacity.
8 Related Work
Sybil Detection on OSNs.
Studies have shown
that Sybils are responsible for large amounts of spam
on Facebook [10], Twitter [11, 32], and Renren [43].
Various systems have been proposed by the research
community to detect and mitigate these Sybils. One
body of work leverages social graphs to detect Sybils.
These systems detect tight-knit Sybil communities that
have a small quotient-cut from the honest region of the
graph [46, 45, 34, 36, 8, 7]. However, recent studies have
demonstrated the limitations of this approach. Yang et al.

USENIX Association  

22nd USENIX Security Symposium  253

show that Sybils on Renren blend into the social graph
rather than forming tight communities [43]. Mohaisen
et al. show that many social graphs are not fast-mixing,
which is a necessary precondition for community-based
Sybil detectors to be effective [21].
A second body of work has used machine learning to
detect Sybil behavior on Twitter [44, 5, 37] and Face-
book [31]. However, relying on speciﬁc features makes
these systems vulnerable to Sybils with different attack
strategies. Finally, one study proposes using crowd-
sourcing to identify Sybils [38].
Web Usage Mining.
Researchers have studied the
usage patterns of web services for the last decade [30].
Several studies focus on session-level analysis to learn
user’s browsing habits [14, 13, 24]. Others develop ses-
sion clustering techniques [4, 42, 40, 33, 25], Markov
Chain models [20, 28], and tree-based models [12] to
characterize user browsing patterns. We also leverage
a Markov Chain model and clustering in our work. Two
studies have focused speciﬁcally on characterizing click-
streams from OSNs [6, 29].
The vast majority of the web usage mining litera-
ture focuses on characterizing the behavior of normal
users. To the best of our knowledge, there are only
two studies that leverage clickstreams for anomaly de-
tection [15, 28]. Both of these studies use session-
level features to identify crawlers, one focusing on e-
commerce and the other on search engines. Their tech-
niques (e.g. session distributions, Markov Chain models)
require training on large samples of ground-truth data,
and cannot scale to today’s large social networks.
9 Conclusion
To the best of our knowledge, this is the ﬁrst work
to leverage clickstream models for detecting malicious
users in OSNs. Our results show that we can build an
accurate Sybil detector by identifying and coloring clus-
ters of “similar” clickstreams. Our system has been val-
idated on ground-truth data, and a prototype has already
detected new types of image-spam attacks on Renren.
We believe clickstream models can be a powerfultech-
nique for user proﬁling in contexts outside of OSNs. In
our ongoing work, we are studying ways to extend click-
stream models to detect malicious crowdsourcing work-
ers and forged online product and travel reviews.
IRB Protocol
This work was carried out under an approved IRB pro-
tocol. All data was anonymized by Renren prior to our
use. The clickstreams are old enough that the events they
describe are no longer accessible via the current website.
All experiments run on recent user data were conducted

on-site at Renren and LinkedIn respectively, and all re-
sults remain on-site.

Acknowledgments
We would like to thank the anonymous reviewers for
their feedback, and Yanjie Liang (Renren) and David
Freeman (LinkedIn) for their assistant in experiments.
This work is supported in part by NSF grants CNS-
1224100and IIS-0916307and DARPA GRAPHS (BAA-
12-01). Any opinions, ﬁndings, and conclusions or rec-
ommendations expressed in this material are those of the
authors and do not necessarily reﬂect the views of the
funding agencies.

References
[1] Facebook has more than 83 million illegitimate accounts.

BBC News, August 2012.
[2] Verify facebook account.

com/help/398085743567023/, 2013.

https://www.facebook.

[3] BANERJEE, A., AND GHOSH, J. Concept-based cluster-

ing of clickstream data. In Proc. of ICIT (2000).

[4] BANERJEE, A., AND GHOSH, J. Clickstream clustering
using weighted longest common subsequences. In Proc.
of the Web Mining Workshop, SIAM Conference on Data
Mining (2001).

[5] BENEVENUTO, F., MAGNO, G., RODRIGUES, T., AND
ALMEIDA, V. Detecting spammers on twitter. In Proc. of
CEAS (2010).

[6] BENEVENUTO, F., RODRIGUES, T., CHA, M., AND
ALMEIDA, V. Characterizing user behavior in online so-
cial networks. In Proc. of IMC (2009).

[7] CAO, Q., SIRIVIANOS, M., YANG, X., AND
PREGUEIRO, T. Aiding the detection of fake accounts
In Proc. of NSDI
in large scale social online services.
(2012).

[8] DANEZIS, G., AND MITTAL, P. Sybilinfer: Detect-
ing sybil nodes using social networks. In Proc of NDSS
(2009).

[9] DOUCEUR, J. R. The Sybil attack.

(2002).

In Proc. of IPTPS

[10] GAO, H., HU, J., WILSON, C., LI, Z., CHEN, Y., AND
ZHAO, B. Y. Detecting and characterizing social spam
campaigns. In Proc. of IMC (2010).

[11] GRIER, C., THOMAS, K., PAXSON, V., AND ZHANG,
M. @spam: the underground on 140 characters or less.
In Proc. of CCS (2010).

[12] G ¨UND ¨UZ, C., AND ¨OZSU, M. T. A web page prediction
model based on click-stream tree representation of user
behavior. In Proc. of SIGKDD (2003).

254  22nd USENIX Security Symposium 

USENIX Association

[13] HEER, J., AND CHI, E. H. Mining the structure of
user activity using cluster stability. In Proc. of the Work-
shop on Web Analytics, SIAM Conference on Data Mining
(2002).

[14] HEER, J., AND CHI, E. H. Separating the swarm: cate-
gorization methods for user sessions on the web. In Proc.
of CHI (2002).

[15] HOFGESANG, P. I., AND KOWALCZYK, W. Analysing
clickstream data: From anomaly detection to visitor pro-
In Proc. of ECML/PKDD Discovery Challenge
ﬁling.
(2005).

[16] IRANI, D., BALDUZZI, M., BALZAROTTI, D., KIRDA,
E., AND PU, C. Reverse social engineering attacks in
online social networks. In Proc of DIMVA (2011).

[17] JIANG, J., WILSON, C., WANG, X., HUANG, P., SHA,
W., DAI, Y., AND ZHAO, B. Y. Understanding latent
In Proc. of IMC
interactions in online social networks.
(2010).

[18] KARYPIS, G., KUMAR, V., AND KUMAR, V. Multilevel
k-way partitioning scheme for irregular graphs. Journal
of Parallel and Distributed Computing 48 (1998), 96–
129.

[19] LEVANDOWSKY, M., AND WINTER, D. Distance be-

tween sets. Nature 234 (1971), 34–35.

[20] LU, L., DUNHAM, M., AND MENG, Y. Mining signif-
In Proc. of

icant usage patterns from clickstream data.
WebKDD (2005).

[21] MOHAISEN, A., YUN, A., AND KIM, Y. Measuring the
Mixing Time of Social Graphs. In Proc. of IMC (2010).
[22] MOTOYAMA, M., LEVCHENKO, K., KANICH, C., MC-
COY, D., VOELKER, G. M., AND SAVAGE, S. Re:
Captchas – understanding captcha-solving from an eco-
nomic context. In Proc. of USENIX Security (2010).

[23] MOTOYAMA, M., MCCOY, D., LEVCHENKO, K., SAV-
AGE, S., AND VOELKER, G. M. Dirty jobs: The role of
freelance labor in web service abuse. In Proc. of Usenix
Security (2011).

[24] OBENDORF, H., WEINREICH, H., HERDER, E., AND
MAYER, M. Web page revisitation revisited: implications
In
of a long-term click-stream study of browser usage.
Proc. of CHI (2007).

[25] PETRIDOU, S. G., KOUTSONIKOLA, V. A., VAKALI,
A. I., AND PAPADIMITRIOU, G. I. Time-aware web
users’ clustering. IEEE Trans. on Knowl. and Data Eng.
(2008), 653–667.

[26] PLATT, J. C. Advances in kernel methods. MIT Press,
1999, ch. Fast training of support vector machines using
sequential minimal optimization, pp. 185–208.

[27] Russian twitter political protests ’swamped by spam’.

BBC News, December 2011.

[28] SADAGOPAN, N., AND LI, J. Characterizing typical and
atypical user sessions in clickstreams. In Proc. of WWW
(2008).
[29] SCHNEIDER, F., FELDMANN, A., KRISHNAMURTHY,
B., AND WILLINGER, W. Understanding online social
In Proc. of
network usage from a network perspective.
IMC (2009).

[30] SRIVASTAVA, J., COOLEY, R., DESHPANDE, M., AND
TAN, P. N. Web usage mining: discovery and applica-
tions of usage patterns from Web data. SIGKDD Explor.
Newsl. 1, 2 (2000), 12–23.

[31] STRINGHINI, G., KRUEGEL, C., AND VIGNA, G. De-
tecting spammers on social networks. In Proc. of ACSAC
(2010).

[32] THOMAS, K., ET AL. Suspended accounts in retrospect:

An analysis of twitter spam. In Proc. of IMC (2011).

[33] TING, I.-H., KIMBLE, C., AND KUDENKO, D. Ubb
mining: Finding unexpected browsing behaviour in click-
stream data to improve a web site’s design. In Proc. of
International Conference on Web Intelligence (2005).

[34] TRAN, N., MIN, B., LI, J., AND SUBRAMANIAN, L.
In Proc. of NSDI

Sybil-resilient online content voting.
(2009).

[35] VEGA, C. Yelp outs companies that pay for positive re-
views. ABC News, November 2012. http://abcnews.
go.com/blogs/business/2012/11/yelp-outs-
companies-that-pay-for-positive-reviews.

[36] VISWANATH, B., POST, A., GUMMADI, K. P., AND
MISLOVE, A. An analysis of social network-based sybil
defenses. In Proc. of SIGCOMM (2010).

[37] WANG, A. H. Don’t follow me: Spam detection on twit-

ter. In Proc. of SECRYPT (2010).

[38] WANG, G., MOHANLAL, M., WILSON, C., WANG, X.,
METZGER, M., ZHENG, H., AND ZHAO, B. Y. Social
turing tests: Crowdsourcing sybil detection. In Proc. of
NDSS (2013).

[39] WANG, G., WILSON, C., ZHAO, X., ZHU, Y., MOHAN-
LAL, M., ZHENG, H., AND ZHAO, B. Y. Serf and turf:
crowdturﬁng for fun and proﬁt. In Proc. of WWW (2012).
[40] WANG, W., AND ZA¨IANE, O. R. Clustering web ses-
sions by sequence alignment. In Proc. of DEXA (2002).
[41] WILSON, C., BOE, B., SALA, A., PUTTASWAMY, K.
P. N., AND ZHAO, B. Y. User interactions in social net-
works and their implications. In Proc. of EuroSys (2009).
[42] XIAO, J., AND ZHANG, Y. Clustering of web users using
session-based similarity measures. In Proc. of ICCNMC
(2001).

[43] YANG, Z., WILSON, C., WANG, X., GAO, T., ZHAO,
B. Y., AND DAI, Y. Uncovering social network sybils in
the wild. In Proc. of IMC (2011).

[44] YARDI, S., ROMERO, D., SCHOENEBECK, G., AND
BOYD, D. Detecting spam in a twitter network. First
Monday 15, 1 (2010).

[45] YU, H., GIBBONS, P. B., KAMINSKY, M., AND XIAO,
F. Sybillimit: A near-optimal social network defense
against sybil attacks. In Proc. of IEEE S&P (2008).

[46] YU, H., KAMINSKY, M., GIBBONS, P. B., AND FLAX-
MAN, A. Sybilguard: defending against sybil attacks via
social networks. In Proc. of SIGCOMM (2006).

USENIX Association  

22nd USENIX Security Symposium  255

