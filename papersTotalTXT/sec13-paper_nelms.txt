ExecScent: Mining for New C&C Domains in Live 

Networks with Adaptive Control Protocol Templates

Terry Nelms, Damballa, Inc. and Georgia Institute of Technology;  

Roberto Perdisci, University of Georgia and Georgia Institute of Technology;  

Mustaque Ahamad, Georgia Institute of Technology and New York University Abu Dhabi

Open access to the Proceedings of the 22nd USENIX Security Symposium is sponsored by USENIXThis paper is included in the Proceedings of the 22nd USENIX Security Symposium.August 14–16, 2013 • Washington, D.C., USAISBN 978-1-931971-03-4ExecScent: Mining for New C&C Domains in Live Networks

with Adaptive Control Protocol Templates

Terry Nelms1,2, Roberto Perdisci3,2, and Mustaque Ahamad2,4

2Georgia Institute of Technology – College of Computing

3University of Georgia – Dept. of Computer Science

1Damballa, Inc.

4New York University Abu Dhabi

Abstract

In this paper, we present ExecScent, a novel system that
aims to mine new, previously unknown C&C domain
names from live enterprise network trafﬁc. ExecScent
automatically learns control protocol templates (CPTs)
from examples of known C&C communications. These
CPTs are then adapted to the “background trafﬁc” of the
network where the templates are to be deployed. The
goal is to generate hybrid templates that can self-tune to
each speciﬁc deployment scenario, thus yielding a bet-
ter trade-off between true and false positives for a given
network environment. To the best of our knowledge, Ex-
ecScent is the ﬁrst system to use this type of adaptive
C&C trafﬁc models.

We implemented a prototype version of ExecScent,
and deployed it in three different large networks for a
period of two weeks. During the deployment, we discov-
ered many new, previously unknown C&C domains and
hundreds of new infected machines, compared to using a
large up-to-date commercial C&C domain blacklist. Fur-
thermore, we deployed the new C&C domains mined by
ExecScent to six large ISP networks, discovering more
than 25,000 new infected machines.

1

Introduction

Code reuse is common practice in malware [18, 20]. Of-
ten, new (polymorphic) malware releases are created by
simply re-packaging previous samples, or by augmenting
previous versions with a few new functionalities. More-
over, it is not uncommon for the source code of success-
ful malware to be sold or leaked on underground forums,
and to be reused by other malware operators [14].

Most modern malware, especially botnets, consist of
(at least) two fundamental components: a client agent,
which runs on victim machines, and a control server ap-
plication, which is administered by the malware owner.

Because code reuse applies to both components1, this
naturally results in many different malware samples shar-
ing a common command-and-control (C&C) protocol,
even when control server instances owned by different
malware operators use different C&C domains and IPs.
In this paper, we present ExecScent, a novel system
that aims to mine new, previously unknown C&C domain
names from live enterprise network trafﬁc (see Figure 1).
Starting from a seed list of known C&C communications
and related domain names found in malware-generated
network traces, ExecScent aims to discover new C&C
domains by taking advantage of the commonalities in
the C&C protocol shared by different malware samples.
More precisely, we refer to the C&C protocol as the set of
speciﬁcations implemented to enable the malware con-
trol application logic, which is deﬁned at a higher level
of abstraction compared to the underlying transport (e.g.,
TCP or UDP) or application (e.g., HTTP) protocols that
facilitate the C&C communications. ExecScent aims to
automatically learn the unique traits of a given C&C pro-
tocol from the seed of known C&C communications to
derive a control protocol template (CPT), which can in
turn be deployed at the edge of a network to detect trafﬁc
destined to new C&C domains.

ExecScent builds adaptive templates that also learn
from the trafﬁc proﬁle of the network where the tem-
plates are to be deployed. The goal is to generate hybrid
templates that can self-tune to each speciﬁc deployment
scenario, thus yielding a better trade-off between true
and false positives for a given network environment. The
intuition is that different networks have different trafﬁc
proﬁles (e.g., the network of a ﬁnancial institution may
generate very different trafﬁc compared to a technology
company).
It may therefore happen that a CPT could
(by chance) raise a non-negligible number of false posi-

1For example, web-based malware control panels can be acquired
in the Internet underground markets and re-deployed essentially as is,
while the client agents can be obtained using do-it-yourself malware
creation kits [10].

USENIX Association  

22nd USENIX Security Symposium  589

tives in a given network, say NetA, while generating true
C&C domain detections and no false positives in other
networks. We take a pragmatic approach, aiming to au-
tomatically identify these cases and lowering the “con-
ﬁdence” on that CPT only when it is deployed to NetA.
This allows us to lower the overall risk of false positives,
while maintaining a high probability of detection in other
networks. We further motivate the use of adaptive tem-
plates in Section 3.

ExecScent focuses on HTTP-based C&C protocols,
because studies have shown that HTTP-based C&C com-
munications are used by a large majority of malware
families [26] and almost all known mobile bots [31].
Moreover, many enterprise networks employ strict egress
ﬁltering ﬁrewall rules that block all non-web trafﬁc. This
forces malware that target enterprise networks to use
HTTP (or HTTPS) as the communication protocol of
choice. It is also important to notice that many modern
enterprise networks deploy web proxies that enforce SSL
man-in-the-middle2 (SSL-MITM). Therefore, enterprise
networks can apply ExecScent’s templates at the web
proxy level to discover new C&C domains even in cases
of HTTPS-based C&C trafﬁc.

Our system is different from previously proposed
URL-based C&C signature generation systems [23, 30].
Unlike previous work, we build templates that can adapt
to the deployment network and that model the entire con-
tent of HTTP requests, rather than being limited to the
URL string. We show in Section 5 that this allows us to
obtain a much better trade-off between true and false pos-
itives, compared to “statically” modeling only the URL
of C&C requests.

Anomaly-based botnet detection systems such as [15,
16] typically require that more than one host in the mon-
itored network be compromised with the same malware
type, do not scale well to large networks, and are not
able to directly attribute the suspected C&C communica-
tions to a speciﬁc malware family. Unlike these systems,
ExecScent can detect C&C communications initiated by
a single malware-infected machine with low false pos-
itives, and can attribute the newly discovered C&C do-
mains to a known malware family name or malware op-
erator (i.e., the name of the cyber-criminal group behind
the malware operation). In turn, the labeled C&C do-
main names discovered by ExecScent may also be de-
ployed in existing lightweight malware detection sys-
tems based on DNS trafﬁc inspection, thus contributing
to the detection and attribution of malware infections in
very large networks (e.g., ISP networks) where monitor-
ing all trafﬁc may not be practically feasible.

Currently, we identify the seed of known C&C trafﬁc
required by ExecScent to learn the control protocol tem-

2See http://crypto.stanford.edu/ssl-mitm/, for example.

Enterprise Network

Proxy

HTTP(S) trafﬁc

Malware

C&C Traces

Template Learning 

ExecScent
and Matching

C&C Server

Infected 
Hosts
Report

New C&C
Domains

Figure 1: ExecScent deployment overview. Adap-
tive control protocol templates are learned from both
malware-generated network traces and the live network
trafﬁc observation. The obtained adaptive templates are
matched against new network trafﬁc to discover new
C&C domains.

plates by leveraging blacklists of known C&C domain
names. C&C discovery systems based on dynamic anal-
ysis such as Jackstraws [17] may also be used for this
purpose. However, unlike ExecScent, while Jackstraws
may be useful to ﬁnd “seed” C&C trafﬁc, its system-call-
based detection models [17] cannot be deployed to detect
new C&C domains in live network trafﬁc.

In summary, we make the following contributions:
• We present ExecScent, a novel system for mining
new malware C&C domains from live networks.
ExecScent automatically learns C&C trafﬁc models
that can adapt to the deployment network’s trafﬁc.
This adaptive approach allows us to greatly reduce
the false positives while maintaining a high number
of true positives. To the best of our knowledge, Ex-
ecScent is the ﬁrst system to use this type of adap-
tive C&C trafﬁc models.

• We implemented a prototype version of ExecScent,
and deployed it in three different large networks for
a period of two weeks. During the deployment, we
discovered many new, previously unknown C&C
domains and hundreds of new infected machines,
compared to using a large up-to-date commercial
C&C domain blacklist.

• We deployed the new C&C domains mined by Ex-
ecScent to six large ISP networks, discovering more
than 25,000 new infected machines.

2 System Overview

The primary goal of ExecScent is to generate control pro-
tocol templates (CPTs) from a seed of known malware-
generated HTTP-based C&C communications. We then
use these CPTs to identify new, previously unknown
C&C domains.

590  22nd USENIX Security Symposium 

USENIX Association

Malware
C&C
Traces

Request

Generalization

Request
Clustering

Labeled 
C&C 
Domains

Generate
Control 
Protocol
Templates

Background

Network
Trafﬁc

Labeled
Control 
Protocol 
Templates

Figure 2: ExecScent system overview.

ExecScent automatically ﬁnds common traits among
the C&C protocol used by different malware samples,
and encodes these common traits into a set of CPTs.
Each template is labeled with the name of the malware
family or (if known) criminal operator associated with
the C&C trafﬁc from which the CPT is derived. Once
a CPT is deployed at the edge of a network (see Fig-
ure 1), any new HTTP(S) trafﬁc that matches the tem-
plate is classiﬁed as C&C trafﬁc. The domain names
associated with the matched trafﬁc are then ﬂagged as
C&C domains, and attributed to the malware family or
operator with which the CPT was labeled.

Figure 2 presents an overview of the process used by
ExecScent to generate and label the CPTs. We brieﬂy
describe the role of the different system components in
this section, deferring the details to Section 4.

Given a large repository of malware-generated net-
work traces, we ﬁrst reconstruct all HTTP requests per-
formed by each malware sample. Then, we apply a re-
quest generalization process, in which (wherever pos-
sible) we replace some of the request parameters (e.g.,
URL parameter values) with their data type and length,
as shown in the example in Figure 3. Notice that Exec-
Scent considers the entire content of the HTTP requests,
not only the URLs (see Section 3.2), and the generaliza-
tion process is applied to different parts of the request
header. The main motivation for applying the general-
ization step is to improve the accuracy of the request
clustering process, in which we aim to group together
malware-generated requests that follow a similar C&C
protocol.

Once the malware requests have been clustered, we
apply a template learning process in which we derive the
CPTs. Essentially, a CPT summarizes the (generalized)
HTTP requests grouped in a cluster, and records a num-
ber of key properties such as the structure of the URLs,
the set of request headers, the IP addresses contacted by
the malware, etc. Furthermore, the templates associate a
malware-family label to each template (see Section 4.4
for details).

Before the templates are deployed in a network, we
adapt the CPTs to the “background trafﬁc” observed in

that network. In particular, for each template component
(e.g., the generalized URL path, the user-agent string, the
request header set, etc.), we compute how frequently the
component appeared in the deployment network. CPT
components that are “popular” in the background trafﬁc
will be assigned a lower “match conﬁdence” for that net-
work. On the other hand, components that appear very
infrequently (or not at all) in the trafﬁc are assigned a
higher conﬁdence. We refer to these “rare” components
as having high speciﬁcity, with respect to the deployment
network’s trafﬁc. The intuitions and motivations for this
approach are discussed in more detail in the next section.
After deployment, an HTTP request is labeled as C&C
if it matches a CPT with high similarity and speciﬁcity.
That is, if the request closely matches a CPT and the
matching CPT components have high speciﬁcity (i.e.,
rarely appeared) in that particular deployment network.

3 Approach Motivations and Intuitions

In this section, we discuss the intuitions that motivated
us to build adaptive control protocol templates. Further-
more, we discuss the advantages of considering the en-
tire content of C&C HTTP requests, rather than limit-
ing ourselves to the URL strings, as done in previous
work [23, 30].

3.1 Why Adaptive Templates?
As most other trafﬁc models, ExecScent’s CPTs, which
are derived from and therefore can match C&C trafﬁc,
may be imperfect and could generate some false posi-
tives. To minimize this risk, ExecScent builds adaptive
control protocol templates that, besides learning from
known malware-generated C&C trafﬁc, also learn from
the trafﬁc observed in the network where the templates
are being deployed. Our key observation is that different
enterprise networks have different trafﬁc proﬁles. The
trafﬁc generated by the computer network of a ﬁnancial
institute (e.g., a large bank) may look quite different from
trafﬁc at a manufacturing company (e.g., a car producer)
or a technology company (e.g., a software-development
company). It may therefore happen that a CPT could (by
chance) raise a non-negligible number of false positives
in a given network, say NetX, and several true detections
with no or very few false positives in other networks. In-
tuitively, our objective is to automatically identify these
cases, and lower the “conﬁdence” on that template when
it matches trafﬁc from NetX, while keeping its conﬁdence
high when it is deployed elsewhere.

For example, assume NetB is a US bank whose hosts
have rarely or never contacted IPs located, say, in China.
If an HTTP request towards an IP address in China is
found, this is by itself an anomalous event. Intuitively, if

USENIX Association  

22nd USENIX Security Symposium  591

the request also matches a CPT, our conﬁdence on a cor-
rect match (true C&C communication) can be fairly high.
On the other hand, assume NetA is a car manufacturer
with partners in China, with which NetA’s hosts commu-
nicate frequently. If an HTTP request in NetA matches a
CPT but is directed towards an address within one of the
IP ranges of the manufacturer’s partners, our conﬁdence
on a correct match should be lowered.

More speciﬁcally, consider the following hypothetical
scenario. Assume we have a template τ that matches
an HTTP request in both NetA and NetB with a similar-
ity score s. For simplicity, let us assume the score s is
the same for both NetA’s trafﬁc and NetB’s trafﬁc. Sup-
pose also that the server’s IP (or it’s /24 preﬁx) asso-
ciated with the matching trafﬁc is ipa for NetA and ipb
for NetB. Also, suppose that ipa is “popular” in network
NetA, whereas ipb has very low popularity in NetB be-
cause it has never been contacted by hosts in that net-
work. Because ipa is very popular in NetA, meaning
that a large fraction (e.g., more than 50%) of the hosts
in NetA has contacted the domain in the past, it is likely
that the template τ is fortuitously matching benign traf-
ﬁc, thus potentially causing a large number of false pos-
itives in NetA. On the other hand, because ipb has very
low popularity in NetB, it is more likely that the match is
a true detection, or that in any case τ will generate very
few (potentially only one) false positives in NetB. Con-
sequently, based on a model of recent trafﬁc observed
in NetA and NetB, we should lower our conﬁdence in
τ for the matches observed in NetA, but not for NetB.
In other words, τ should automatically adapt to NetA to
“tune down” the false positives. At the same time, keep-
ing the conﬁdence in τ high for NetB means that we will
still be able to detect C&C communications that match τ,
while keeping the risk of false positives low. We general-
ize this approach to all other components of ExecScent’s
templates (e.g, the structure of the URLs, the user-agent
strings, the other request headers, etc.), in addition to the
destination IPs.

Overall, our conﬁdence on a match of template τ in a

given network NetX will depend on two factors:

• Similarity: a measure of how closely an HTTP re-

quest matches τ.

• Speciﬁcity: a measure of how speciﬁc (or rare) are
the components of τ with respect to NetX’s trafﬁc.

An HTTP request is labeled as C&C if it matches a CPT
with both high similarity and high speciﬁcity. We show
in Section 5 that this approach outperforms C&C models
that do not take such speciﬁcity into account.

3.2 Why Consider All Request Content?
Malware C&C requests typically need to carry enough
information for a malware agent running on a victim to
(loosely) authenticate itself with the C&C server.
In-
tuitively, the C&C server wants to make sure that it is
talking to one of its bots, thus avoiding exposure of its
true nature or functionalities to crawlers or security re-
searchers who may be probing the server as part of an
investigation. This is often achieved by using a speciﬁc
set of parameter names and values that must be embed-
ded in the URL for the C&C requests to be successful.
Previous work on automatic URL signature generation
has shown promising results in such cases [23,30]. How-
ever, some malware (e.g., TDL4 [1]) exchanges informa-
tion with the C&C by ﬁrst encrypting it, encoding it (e.g.,
using base-64 encoding), and embedding it in the URL
path. Alternatively, identiﬁer strings can also be embed-
ded in ﬁelds such as user-agent (e.g., some malware
samples use their MD5 hash as user-agent name), en-
coded in other request headers (e.g., in the referrer), or
in the body of POST requests. Therefore, only consider-
ing URLs may not be enough to accurately model C&C
requests and detect new C&C domains, as supported by
our experimental results (Section 5).

4 System Details

We now detail the internals of ExecScent. Please refer
to Section 2 for a higher-level overview of the entire sys-
tem.

Input Network Trafﬁc

4.1
As we mentioned in Section 1, ExecScent focuses on
HTTP-based malware, namely malware that leverage
HTTP (or HTTPS) as a base network protocol on top of
which the malware control protocol is “transported”. To
this end, ExecScent takes in as input a feed of malware-
generated HTTP trafﬁc traces (in our evaluation, we use
a large set of malware traces provided to us by a well-
known company that specializes in malware defense).

It is worth remembering that while some malware may
use HTTPS trafﬁc as a way to evade detection, this does
not represent an insurmountable obstacle in our deploy-
ment scenarios (see Figure 1). In fact, many enterprise
networks, which represent our target deployment envi-
ronment, already deploy web proxy servers that can per-
form SSL-MITM and can therefore forward the clear-
text HTTP requests to ExecScent’s template matching
module, e.g., using the ICAP protocol (RFC 3507). Also,
malware samples that appear to be using HTTPS trafﬁc
may be re-analyzed in a controlled environment that in-
cludes an SSL-MITM proxy interposed between the (vir-

592  22nd USENIX Security Symposium 

USENIX Association

tual) machine running the sample and the egress router.
After all, HTTPS-based malware that do not support or
choose not to run when an SSL-MITM proxy is present
will also fail to run in enterprise networks that have a
similar setting, and are therefore of less interest.

4.2 Request Generalization

As we discuss in the following sections, to obtain quality
control protocol templates we ﬁrst need to group sim-
ilar C&C requests. To this end, an appropriate simi-
larity metric needs to be deﬁned before clustering al-
gorithms can be applied. Previous works that propose
URL-centric clustering systems [23,30] are mainly based
on string similarity measures. Essentially, two URLs are
considered similar if they have a small edit distance, or
share a number of substrings (or tokens). However, these
systems do not take into account the fact that URLs of-
ten contain variables whose similarity is better measured
according to their data type rather than considering spe-
ciﬁc sequences of characters. Consider the two hypo-
thetical C&C requests in Figure 3. Taken as they are
(Figure 3a), their distance is relatively large, due to the
presence of several different characters in the strings. To
avoid this, ExecScent uses a set of heuristics to detect
strings that represent data of a certain type, and replaces
them accordingly using a placeholder tag containing the
data type and string length (Figure 3b).

For example, we would identify “fa45e” as lowercase
hexadecimal because it contains numeric characters and
the alphabetic characters are all valid lowercase hexadec-
imal digits. The data types we currently identify are inte-
ger, hexadecimal (upper, lower and mixed case), base64
(standard and “URL safe”) and string (upper, lower and
mixed case). In addition, for integer, hexadecimal and
string we can identify the data type plus additional punc-
tuation such as “:” or “.” (e.g., 192.168.1.1 would be
identiﬁed as a data type of integer+period of length 11).
Furthermore, our heuristics can easily be extended to
support data types such as IP address, MAC address,
MD5 hash and version number.

This generalization process allows us to deﬁne a bet-
ter similarity metric (Section 4.7), which is instrumental
to obtaining higher quality C&C request clusters. No-
tice also that while previous works such as [23,30] focus
only on URL strings, ExecScent takes the entire request
into account. For example, in Figure 3 the user-agent
strings are MD5s, and can be generalized by replacing
the speciﬁc MD5 strings with the appropriate data type
and length information.

(a)

(b)

Request 1:
GET /Ym90bmV0DQo=/cnc.php?v=121&cc=IT
Host: www.bot.net
User-Agent: 680e4a9a7eb391bc48118baba2dc8e16
...
Request 2:
GET /bWFsd2FyZQ0KDQo=/cnc.php?v=425&cc=US
Host: www.malwa.re
User-Agent: dae4a66124940351a65639019b50bf5a
...

Request 1:
GET /<Base64;12>/cnc.php?v=<Int;3>&cc=<Str;2>
Host: www.bot.net
User-Agent: <Hex;32>
...
Request 2:
GET /<Base64;16>/cnc.php?v=<Int;3>&cc=<Str;2>
Host: www.malwa.re
User-Agent: <Hex;32>
...

Figure 3: Example C&C requests: (a) original; (b) gen-
eralized.

4.3 Request Clustering
Before extracting the templates, we group together sim-
ilar C&C requests. This clustering step simply aims to
assist the automatic CPT generation algorithm, improv-
ing efﬁciency and yielding templates that are at the same
time generic enough to match similar (but not identical)
C&C communications in new trafﬁc, and precise enough
to generate very few or no false positives.

We perform C&C request clustering in two phases.
During the ﬁrst phase, we coarsely group C&C requests
based on their destination IPs. Speciﬁcally, given two
C&C requests, we group them together if their destina-
tion IPs reside in /24 (or class C) networks that share a
DNS-based relationship. Namely, we consider two /24
networks as related if there exists at least one domain
name that within the last 30 days resolved to different
IP addresses residing in the two different networks. To
ﬁnd such relationships, we rely on a large passive DNS
database [12].

In the second phase, we consider one coarse-grained
cluster at a time, and we further group a cluster’s C&C
requests according to a content similarity function. We
use an agglomerative hierarchical clustering algorithm
to group together C&C requests within a coarse-grained
cluster that carry similar generalized URLs, similar
user-agent strings, similar numbers of HTTP header
ﬁelds and respective values, etc. When measuring the
similarity between two requests, we take into account
both the similarity and speciﬁcity of the requests’ con-
tent, where the speciﬁcity (or low “popularity”) can be
measured with respect to a dataset of trafﬁc recently
collected from different networks (dashed arrow in Fig-
ure 2). For a more detailed deﬁnition of the similarity

USENIX Association  

22nd USENIX Security Symposium  593

1) Median URL path: /<Base64;14>/cnc.php
2) URL query component: {v=<Int,3>, cc=<String;2>}
3) User Agent: {<Hex;32>}
4) Other headers: {(Host;13), (Accept-Encoding;8)}
5) Dst nets: {172.16.8.0/24, 10.10.4.0/24, 192.168.1.0/24}  
Malware family: {Trojan-A, BotFamily-1} 
URL regex: GET /.*\?(cc|v)=
Background trafﬁc proﬁle:
speciﬁcity scores used to adapt the CPT 
to the deployment environment

Figure 4: Example CPT.

τ5) Dst. networks: the set of all destination /24 net-
works associated with the C&C requests in the clus-
ter. Intuition: in some cases, the C&C server may
be relocated to a new IP address within the same
(possibly “bullet-proof”) network.

• Malware family:

the (set of) malware family
name(s) associated to the known C&C requests in
the cluster.

In addition,

deployment-related information:

each CPT includes

the following

function used in the clustering step, we refer the reader
to Section 4.7.

4.4 Generating CPTs
Once C&C requests have been clustered, a control pro-
tocol template (CPT) is generated from each cluster. At
this stage, we consider only clusters that contain at least
one HTTP request to a known C&C domain. Each tem-
plate represents a summary of all C&C requests in a
cluster, and contains the following components, as also
shown in Figure 4:

τ1) Median URL path: median path string that mini-
mizes the sum of edit distances from all URL paths
in the requests (see [11] for a deﬁnition of median
string). Intuition: although the URL path may vary
signiﬁcantly from one malware installation to an-
other, we observed many cases in which there exist
“stable” path components that are unique to a spe-
ciﬁc malware family or operation.

τ2) URL query component: stores the set of parame-
ter names, value types and lengths observed in the
query component [5] of each of the URLs.
Intu-
ition: URL parameters are often used by malware
to convey information about the infected host, such
as its OS version, a unique identiﬁer for the infected
machine, etc.

τ3) User-agent:

the set of all different (generalized)
user-agent strings found in the requests. Intuition:
the user-agent is one of the most abused HTTP
headers by malware, and is sometimes used as a
loose form of authentication.

τ4) Other headers: the set of other HTTP headers ob-
served in the requests. For each header, we also
store the length of its value string. Intuition: the set
of header names, their order and values are some-
times unique to a malware family.

• URL regex:

to increase the efﬁciency of the tem-
plate matching phase (Section 4.6), each template
includes a regular expression automatically gener-
ated from the set of URL strings in the requests. The
URL regex is intentionally built to be very generic
and is used during deployment for the sole purpose
of ﬁltering out trafﬁc that is extremely unlikely to
closely match the entire template, thus reducing the
cost of computing the similarity between HTTP re-
quests in live trafﬁc and the template.

• Background trafﬁc proﬁle:

information derived
from the trafﬁc observed in the deployment environ-
ment within the past W days (where W is a system
parameter). This is used for computing the speci-
ﬁcity of the CPT components, thus allowing us to
adapt the CPT to the the deployment network, as
explained in detail in Section 4.5.

Notice that a CPT acts as the centroid for the cluster
from which it was derived. To determine if a new request
is similar enough to a given cluster, we only need to com-
pare it with the CPT, rather than all of the clustered C&C
requests. Therefore, CPTs provide an efﬁcient means of
measuring the similarity of a new request to the C&C
protocol used by the clustered malware samples.

4.5 Adapting to a Deployment Network
As explained in Section 3.1, once the CPTs are deployed,
an HTTP request is labeled as C&C if it matches a CPT τ
with both high similarity and speciﬁcity. To this end, we
ﬁrst need to compute a speciﬁcity score for each element
of the k-th component τk of τ, which indicates how “un-
popular” that element is with respect to the trafﬁc proﬁle
in the deployment network (notice that k = 1, . . . ,5, as
shown in Figure 4 and Section 4.4).

For example, to compute the speciﬁcity scores for τ3,
we ﬁrst compute a host-based popularity score hpuai for
each user-agent string uai in the set τ3. We consider
the number of hosts hnuai in the deployment network that
generated an HTTP request containing uai during the last

594  22nd USENIX Security Symposium 

USENIX Association

hnuai

max j{hnua j}

dnuai

max j{dnua j}

W days, where W is a conﬁgurable time-window pa-
, where the max
rameter. We deﬁne hpuai =
is taken over all user-agent strings ua j observed in
the deployment network’s trafﬁc. Similarly, we com-
pute a domain-based popularity score d puai, based on the
number of distinct destination domain names dnuai with
one or more HTTP requests that contain uai. We deﬁne
. The intuition is that a user-agent
d puai =
string can only be considered truly popular if it spans
many hosts and domains. On the other hand, we do
not want to consider a uai as very popular if it has high
host-based popularity (e.g., “Windows-Update-Agent”)
but low domain-based popularity (e.g., because the only
domain on which it is used is microsoft.com). Fi-
nally, we deﬁne the speciﬁcity score for uai as σ3,uai =
1 − min(hpuai,d puai). In a similar way, we compute a
speciﬁcity score σ4,hdl for each header element hdl in τ4.
To compute the speciﬁcity scores for τ5, we simply
compute the host-based popularity hpneti for each /24
network preﬁx neti ∈ τ5, and we deﬁne a separate score
σ5,neti = (1− hpneti) for each preﬁx.

4.5.1 URL Speciﬁcity
Computing the speciﬁcity of the components of a URL is
more complex, due to the large variety of unique URLs
observed every day on a given network. To address this
problem, we rely on a supervised classiﬁcation approach.
First, given a dataset of trafﬁc collected from a large net-
work, we extract all URLs, and learn a map of URL word
frequencies, where the “words” are extracted by tokeniz-
ing the URLs (e.g., extracting elements of the URL path,
ﬁlename, query string, etc.). Then, given a new URL, we
translate it into a feature vector in which the statistical
features measure things such as the average frequency of
single “words” in the tokenized URL, the average fre-
quency of word bigrams in the query parameters, the fre-
quency of the ﬁle name, etc. (to extract the frequency
values for each word found in the URL we lookup the
previously learned map of word frequencies).

After we translate a large set of “background trafﬁc
URLs” into feature vectors, we train an SVM classi-
ﬁer [8] that can label new URLs as either popular or
unpopular. To prepare the training dataset we proceed as
follows. We ﬁrst rank the “background URLs” according
to their domain-based popularity (i.e., URLs that appear
on requests to multiple sites on different domain names
are considered as more popular). Then, we take a sam-
ple of URLs from the top and from the bottom of this
ranking, which we label as popular and unpopular, re-
spectively. We use this labeled dataset to train the SVM
classiﬁer, and we rely on the max-margin approach used
by the SVM [9] to produce a model that can generalize

to URLs not seen during training.

During the operational phase (once the SVM classiﬁer
is trained and deployed), given a URL ui, we can ﬁrst
translate ui into its corresponding feature vector vi, as
described above, and feed vi to the SVM classiﬁer. The
classiﬁer can then label ui as either popular or unpop-
ular. In practice, though, rather than considering these
class labels, we only take into account the classiﬁcation
score (or conﬁdence) associated with the popular class3.
Therefore, the SVM’s output can be interpreted as fol-
the higher the score, the more ui “looks like” a
lows:
popular URL, when compared to the large set of URLs
observed in the background trafﬁc. Finally, the speci-
ﬁcity score for the URL is computed as σui = 1 − pui,
where pui is the SVM output for URL ui.
Now, let us go back to consider the template τ and its
URL-related components τ1 and τ2 (see Figure 4). We
ﬁrst build a “median URL” um by concatenating the me-
dian URL path (τ1) to the (sorted) set of generalized pa-
rameter names and values (τ2). We then set the similarity
scores σ1 = σ2 = σum, where σum is the speciﬁcity of um.

4.6 Template Matching
Template matching happens in two phases. As men-
tioned above, each template contains an URL regular ex-
pression automatically derived from the C&C requests in
a cluster. Given a new HTTP request r, to test whether
this request matches a template τ, we ﬁrst match r’s URL
to τ’s URL regex. It is worth noting that, as mentioned
in Section 4.4, the URL regex is intentionally built to be
very generic, and is merely used to efﬁciently ﬁlter out
trafﬁc that is extremely unlikely to match the entire tem-
plate. Furthermore, we check if the destination IP of r
resides within any of the /24 preﬁxes in τ (speciﬁcally
in component τ5). If neither the URL regex nor the des-
tination IP have a match, we assume r does not match τ.
Otherwise, we proceed by considering the entire content
of request r, transforming r according to the request gen-
eralization process (see Section 4.2), and measuring the
overall matching score S(r,τ) between the (generalized)
request r and the template τ.

In summary, the score S is obtained by measuring the
similarity between all the components of the request r
and the respective components of the template τ. These
similarity measures are then weighted according to their
speciﬁcity, and the matching score S(r,τ) is computed as
the average of all weighted component similarities. A
detailed deﬁnition of the similarity functions and how
speciﬁcity plays an explicit role in computing S(r,τ) is
given in Section 4.7.

3We calibrate the classiﬁcation scores output by the SVM classiﬁer

using the method proposed by Platt [24].

USENIX Association  

22nd USENIX Security Symposium  595

If S(r,τ) exceeds a tunable detection threshold θ, then
the request r will be deemed a C&C request and the do-
main name associated with r (assuming r is not using a
hardcoded IP address) is classiﬁed as C&C domain and
labeled with the malware family associated to τ. Fur-
thermore, the host from which the request r originated is
labeled as compromised with τ’s malware family.

4.7 Similarity Functions
4.7.1 CPT matching score
To determine if a new HTTP request r matches a CPT τ,
we compute a matching score S(r,τ) as follows:

S(r,τ) =

∑k wk(sk,σk)· sk(rk,τk)

∑k wk(sk,σk)

· σd

(1)

md

where sk is a similarity function that compares each ele-
ment τk of τ (Section 4.4) with its respective counterpart
rk of r, and where wk is a dynamic weight (whose deﬁni-
tion is given below) that is a function of both the similar-
ity sk and the speciﬁcity σk of the k-th component of τ.
The denominator scales S(r,τ) between zero and one.

The factor σd is the speciﬁcity of the destination do-
main d of request r, which is computed as σd = 1 −
, where md is the number of hosts in the de-
maxi{mdi}
ployment network’s trafﬁc that queried domain d, and
maxi{mdi} is the number of hosts that queried the most
“popular” domain in the trafﬁc. Accordingly, we use σd
to decrease the matching score S(r,τ) for low-speciﬁcity
domains (i.e., domains queried by a large number of
hosts). The intuition is that infections of a speciﬁc mal-
ware family often affect a relatively limited fraction of all
hosts in an enterprise network, as most modern malware
propagate relatively “slowly” via drive-by downloads or
social engineering attacks. In turn, it is unlikely that a
new C&C domain will be queried by a very large frac-
tion (e.g., > 50% ) of all hosts in the monitored network,
within a limited amount of time (e.g., one day).

In the following, we describe the details of the sim-
ilarity functions sk(·) used in Equation 1.
In addition,
we further detail how the speciﬁcity value of each com-
ponent is selected, once the value of sk(·) has been com-
puted (for the deﬁnition of speciﬁcity, we refer the reader
to Section 4.5).
s1 - Given the path of the URL associated with r, we
measure the normalized edit distance between the
path and the CPT’s median URL path τ1. The URL
path speciﬁcity σ1 is computed as outlined in Sec-
tion 4.5.

s2a - We measure the Jaccard similarity 4 between the set
of parameter names in the URL query-string of r

4J = |A∩B|
|A∪B|

and the set of names in τ2. The speciﬁcity of the pa-
rameter names σ2a is equal to σ2 (see Section 4.5).

s2b - We compare the data types and lengths of the val-
ues in the generalized URL query-string parameters
(see Section 4.2). For each element of the query
string, we assign a score of one if its data type in r
matches the data type recorded in τ2. Furthermore,
we compute the ratio between the value length in
r and in τ2. Finally, s2b is computed by averaging
all these scores, whereby the more data types and
lengths that match, the higher the similarity score.
As in s2a, we set σ2b = σ2.

s3 - We compute the normalized edit distance between
the (generalized) user-agent string in r, and each
of the strings in the set τ3. Let dm be the smallest
of such distances, where m is the closest of the tem-
plate’s user-agent strings. We deﬁne s3 = 1−dm,
and set the speciﬁcity σ3 = σ3,m.

s4 - Given the remaining request header ﬁelds in r, we
measure the similarity from different perspectives.
First, we compute the Jaccard similarity j between
the set of headers in r and the set τ4. Furthermore,
we consider the order of the headers as they appear
in r and in the requests from which τ was derived.
If the order matches, we set a variable o = 1, oth-
erwise we set o = 0. Finally, for each header, we
compare the ratio between the length of its value as
it appears in r and in τ5, respectively. The similarity
s4 is deﬁned as the average of all these partial simi-
larity scores (i.e., of j, o, and the length ratios). We
set the speciﬁcity score σ5 = minl{σ5,hdl}, where
the hdl are the request headers.

s5 - Let ρ be the destination IP of request r. If ρ resides
within any of the /24 network preﬁxes in τ5, we set
s5 = 1, otherwise we assign s5 = 0. Assume ρ is
within preﬁx n ∈ τ5 (in which case s5 = 1). In this
case, we set the speciﬁcity σ5 = σ5,n.

The dynamic weights wk(·) are computed as follows:

wk(sk,σk) = ˆwk ·(cid:31)1 +

1

(2− sk · σk)n(cid:30)

(2)

where ˆwk is a static weight (i.e., it takes a ﬁxed value),
and n is a conﬁguration parameter. Notice that wk ∈
[ ˆwk(1 + 1
2n ),2 ˆwk], and that these weights are effectively
normalized by the denominator of Equation 1, thus re-
sulting in S(r,τ) ∈ [0,1] (since sk ∈ [0,1],∀k, and σd ∈
[0,1], by deﬁnition).
The intuition for the dynamic weights wk(·) is that we
want to give higher weight to components of a request r
that match their respective counterpart in a CPT τ with

596  22nd USENIX Security Symposium 

USENIX Association

In fact, the
both high similarity and high speciﬁcity.
weight will be maximum when both the similarity and
speciﬁcity are equal to one, and will tend to the mini-
mum when either the similarity or speciﬁcity (or both)
tend to zero.

In summary, similarity measures the likeness of two
values, whereas speciﬁcity measures their uniqueness in
the underlying network trafﬁc. The dynamic weights al-
low us to highlight the rare structural elements that are
common between a CPT and a request, so that we can
leverage them as the dominant features for detection. Be-
cause rare structural elements differ in their importance
across malware families, by emphasizing these “unique
features” we are able to detect and distinguish between
different malware families.

4.7.2 Similarity function for clustering phase

In Section 4.3, we have described the C&C request clus-
tering process.
In this section we deﬁne the function
used to compute the similarity between pairs of HTTP
requests, which is needed to perform the clustering.

Given two HTTP requests r1 and r2, we compute their
similarity using Equation 1. At this point, the reader may
notice that Equation 1 is deﬁned to compare an HTTP
request to a CPT, rather than two requests. The reason
why we can use Equation 1, is that we can think of a
request as a CPT derived from only one HTTP request.
Furthermore, if we want to include the speciﬁcity scores,
which are used to make the weights wk dynamic, we can
use a dataset of trafﬁc previously collected from one or
more networks (see dashed arrow in Figure 2).

5 Evaluation

In this section, we describe the data used to evalu-
ate ExecScent (Section 5.1), how the system was setup
to conduct the experiments (Section 5.2), and present
the experimental results in different live networks (Sec-
tion 5.3). Furthermore, we quantify the advantage of
modeling entire HTTP requests, rather than only con-
sidering URLs, and of using adaptive templates over
“static” C&C models (Section 5.4). In addition, we show
the beneﬁts obtained by deploying new C&C domains
discovered by ExecScent into large ISP networks (Sec-
tion 5.5).

5.1 Evaluation Data
5.1.1 Malware Network Traces

We obtained access to a commercial feed of malware in-
telligence data (provided to us by a well known security

company), which we used to generate the control proto-
col templates (CPTs). Through this feed, we collected
about 8,000 malware-generated network traces per day
that contained HTTP trafﬁc. Each network trace was
marked with a hash of the malware executable that gen-
erated the network activity, and (if known) by the related
malware family name.

5.1.2 Live Network Trafﬁc
To evaluate ExecScent, we had access to the live traf-
ﬁc of three large production networks, which we refer
to as UNETA, UNETB, and FNET. Networks UNETA
and UNETB are two different academic networks based
in the US, while FNET is the computer network of a
large North-American ﬁnancial institution. Table 1 re-
ports statistics with respect to the network trafﬁc ob-
served in these three networks. For example, in UNetA
we observed an average of 7,893 distinct active source IP
addresses per day. In average, these network hosts gener-
ated more than 34.8M HTTP requests per day, destined
to 149,481 different domain names (in average, per day).

Table 1: Live Network Trafﬁc Statistics (Avg. per day)

Distinct Src IPs
HTTP Requests
Distinct Domains

UNETA
7,893
34,871,003
149,481

UNETB
27,340
66,298,395
238,014

FNET
7,091
58,019,718
113,778

5.1.3 Ground Truth
To estimate true and false positives, we rely on the fol-
lowing data:

• CCBL: we obtained a large black-list containing
hundreds of thousands of C&C domains provided
by a well known security company, which we refer
to as CCBL. It is worth noting that CCBL is dif-
ferent from most publicly available domain black-
lists for two reasons: 1) the C&C domains are care-
fully vetted by professional threat analysts; 2) the
domains are labeled with their respective malware
families and, when available, a malware operator
name (i.e., an identiﬁer for the cyber-criminal group
that operates the C&C).

• ATWL: we derived a large white-list of benign do-
main names from Alexa’s top 1 million global do-
mains list (alexa.com). From these 1M domains,
we ﬁltered out domains that can be considered as
effective top level domains5 (TLDs), such as do-
mains related to dynamic DNS services (e.g., dyn-
dns.org, no-ip.com, etc.). Next, we discarded do-
mains that have not been in the top 1M list for at

5http://publicsuffix.org

USENIX Association  

22nd USENIX Security Symposium  597

least 90% of the time during the entire past year.
To this end, we collected an updated top domains
list every day for the past year, and only considered
as benign those domains that have consistently ap-
peared in the top 1M domains list. The purpose of
this ﬁltering process is to remove possible noise due
to malicious domains that may became popular for
a limited amount of time. After this pruning op-
erations, we were left with about 450,000 popular
domain names6.

• PKIP: we also maintain a list of parking IPs, PKIP.
Namely, IP addresses related to domain parking ser-
vices (e.g., IPs pointed to by expired or unused do-
mains which have been temporarily taken over by
a registrar). We use this list to prune ExecScent’s
templates. In fact, CPTs are automatically derived
from HTTP requests in malware-generated network
traces that are labeled as C&C communications due
to their associated domain name being in the CCBL
list (Section 4). However, some of the domains
in CCBL may be expired, and could be currently
pointing to a parking site. This may cause some of
the HTTP requests in the malware traces to be erro-
neously labeled as C&C requests, thus introducing
noise in ExecScent’s CPTs. We use the PKIP to
ﬁlter out this noise.

• Threat Analysis: clearly, it is not feasible to obtain
complete ground truth about all trafﬁc crossing the
perimeter of the live networks where we evaluated
ExecScent. To compensate for this and obtain a bet-
ter estimate of the false and true positives (com-
pared to only using CCBL and ATWL), we per-
formed an extensive manual analysis of our exper-
imental results with the help of professional threat
analysts.

5.2 System Setup
To conduct our evaluation, we have implemented and de-
ployed a Python-based proof-of-concept version of Ex-
ecScent. In this section we discuss how we prepared the
system for live network deployment.

5.2.1 Clustering Parameters

Figure 5: Effect of the dendrogram cut height (FPs).

by the clustering algorithm) needs to be cut to partition
the HTTP requests into request clusters.

To select the dendrogram cut height, we proceeded as
follows. We considered one day of malware traces col-
lected from our malware intelligence feed (about 8,000
different malware traces). We then applied the clustering
process to these traces, and produced different clustering
results by cutting the dendrogram at different hights. For
each of these different clustering results, we extracted the
related set of CPTs, and we tested these CPTs over the
next day of malware traces from our feed with varying
matching thresholds. The obtained number of false posi-
tives, i.e., misclassiﬁed benign domains (measured using
ATWL), and true positives, i.e., new correctly classiﬁed
C&C domains (measured using CCBL), are summarized
in Figure 5 and Figure 6, respectively. Notice that al-
though in this phase we tested the CPTs over malware-
generated network traces, we can still have false posi-
tives due to the fact that some malware query numerous
benign domain names, along with C&C domains.

As Figures 5 and 6 show, per each ﬁxed CPT match-
ing threshold, varying the dendrogram cut height does
not signiﬁcantly change the false positives and true pos-
itives. In other words, the CPT matching results are not
very sensitive to the speciﬁc value of the clustering pa-
rameter. We decided to ﬁnally set the value of the cut
hight to 0.38, which we use during all remaining ex-
periments, because this provided good efﬁciency during
the CPT generation process, while maintaining high CPT
quality.

As discussed in Section 4.3, to generate the CPTs, we
ﬁrst apply a request clustering step. The main purpose
of this step is to improve the efﬁciency of the CPT learn-
ing process. The clustering phase relies on a hierarchical
clustering algorithm that takes in as input the height at
which the dendrogram (i.e., the “distance tree” generated

6More precisely, second level domains (2LDs).

5.2.2 CPT Generation
To generate the CPTs used for the evaluation of Exec-
Scent on live network trafﬁc (Section 5.3), we initially
used two weeks of malware traces collected from our
malware intelligence feed. To label the seed of C&C
HTTP requests in the malware traces, we used the CCBL
black-list. We also use the list of parking IPs PKIP to

598  22nd USENIX Security Symposium 

USENIX Association

Figure 6: Effect of the dendrogram cut height (TPs).

Figure 7: CPT detection results for varying detection
thresholds.

prune CPTs related to parked C&C domains, as men-
tioned in Section 5.1.3. Once this initial set of CPTs was
deployed, we continued to collect new malware traces
from the feed, and updated the CPT set daily by adding
new CPTs derived from the additional malware traces.
More precisely, let D1 be the day when the initial set of
CPTs was ﬁrst deployed in a live network, and let C1 be
this initial CPT set. C1 is generated from the malware
traces collected during a two-week period immediately
before day D1. The CPTs set C1 was then used to detect
new C&C domains during the entire day D1. At the same
time, during D1 we generated additional CPTs from the
malware traces collected on that day, and added them to
set C1. Therefore, at the end of day D1 we had an ex-
panded set C2 of CPTs, which we deployed on day D2,
and so on. At the end of the deployment period we had
just over 4,000 distinct CPTs.

To adapt the CPTs to the trafﬁc of each deployment
network (see Section 4.5), we proceeded in a similar way.
We built a background trafﬁc proﬁle based on all HTTP
trafﬁc observed at each deployment network during the
two days immediately before day D1, and used this pro-
ﬁle to adapt the initial set of CPTs C1. Then, every day
we updated the trafﬁc proﬁle statistics based on the new
live trafﬁc observed on that day, and used this informa-
tion to further adapt all the CPTs. Notice that the set of
CPTs deployed to different networks are different, in that
they adapt differently to each deployment network (using
that network’s background trafﬁc proﬁle).

5.3 Live Network Deployment Results
To evaluate ExecScent, we deployed it in three differ-
ent large networks, UNETA, UNETB, and FNET, for a
period of two weeks. We generated the set of adaptive
CPTs as explained above (Section 5.2.2), using a total
of four weeks of malware-generated network traces (two
weeks before deployment, plus daily updates during the

two-week deployment period). The CPT matching en-
gine was deployed at the edge of each network.

The detection phase proceeded as follows. For each
network, we logged all HTTP requests that matched any
of the adapted CPTs with a matching score S ≥ 0.5, along
with information such as the destination IP address of the
request, the related domain name, the source IP address
of the host that generated the request, and the actual value
of the score S. This allowed us to compute the trade-off
between the number of true and false positives for vary-
ing values of the detection threshold θ. Speciﬁcally, let
h be a request whose matching score Sh is above the de-
tection threshold θ, and let d be the domain name related
to h. Consequently, we label h as a C&C request, and
classify d as a C&C domain. We then rely on the CCBL
and ATWL lists and on manual analysis (with the help of
professional threat analysis) to conﬁrm whether the de-
tection of d represents a true positive, i.e., if d is in fact a
C&C domain, or a false positive, in which case d is not a
C&C domain.

Figure 7 summarizes the overall number of true pos-
itives and false positives obtained during the two-week
deployment period over the three different live networks,
while Table 2 shows a breakdown of the results on the
different networks for a set of representative detection
thresholds. For example, in Table 2, consider UNETA
with a detection threshold of 0.65. During the two-week
deployment period, we detected a total of 66 C&C do-
mains, of which 34 are new, previously unknown C&C
domains that were not present in our commercial black-
list, CCBL. The 66 C&C domains were related to 17
distinct malware families. Overall, we detected 105 in-
fected hosts, 90 of which were new infections related to
the 34 previously unknown C&C domains. This means
that 90 ((cid:30) 86%) of the infected hosts could not be de-
tected by simply relying on the CCBL black-list.
The CPTs generated 118 false positives, namely do-
main names that we misclassiﬁed as C&C domains. We

USENIX Association  

22nd USENIX Security Symposium  599

Detection Threshold
All C&C Domains
New C&C Domains
Distinct Malware Families
Number of Infected Hosts
Number of New Infected Hosts
FP Domains
FP Domains (reduced CPT set)

.62
68
35
17
114
91
133
25

UNETA
.65
66
34
17
105
90
118
13

.73
46
26
14
98
86
114
10

.84
25
13
8
37
25
0
0

.62
36
21
14
185
145
152
40

UNETB
.65
32
18
12
150
135
117
26

.73
24
15
10
147
133
105
22

Table 2: Live network results over a two-week deployment period

FNET

.84
10
4
4
21
11
0
0

.62
2
2
1
7
7
109
30

.65
2
2
1
7
7
63
23

.73
2
2
1
7
7
49
16

.84
1
1
1
7
7
0
0

noticed that most of these false positives were generated
by only two CPTs (the same two CPTs generated most
false positives in all networks). By subtracting the false
positives due to these two “noisy” CPTs, we were left
with only 13 false positives, as shown in the last row of
Table 2. The false positives marked with “reduced CPT
set” in Figure 7 are also related to results without these
two CPTs. Overall, within the entire two-week test pe-
riod ExecScent generated a quite manageable number of
false positives, in that a professional threat analyst could
analyze and ﬁlter out the false C&C domains in a matter
of hours.

Notice that the low number (only two) of new C&C
domains found in the FNET network was expected. In
fact, FNET is a very sensitive ﬁnancial institution, where
many layers of network security mechanisms are already
in use to prevent malware infections. However, our ﬁnd-
ings conﬁrm that even well guarded networks remain
vulnerable.

5.3.1 Pushdo Downloader

It is worth clarifying that all results reported in Figure 7
and Table 2 have been obtained after discounting the do-
mains detected through a single CPT that was causing
hundreds of misclassiﬁcations. Through a manual inves-
tigation, we easily found that ExecScent had correctly
learned this CPT, which actually models the HTTP-based
C&C communications of a PUSHDO downloader vari-
ant [28]. This particular variant purposely replicates its
C&C requests, and sends them to a large number of de-
coy benign domain names. The malware does this to try
to hide the true C&C domain in plain sight, among a
large set of benign domains. However, while this makes
it somewhat harder to ﬁnd the true C&C among hundreds
or even thousands of benign domains (which requires
some manual analysis effort), it makes it very easy to
identify the fact that the source hosts of these requests,
which matched our PUSHDO CPT, are infected with that
speciﬁc malware variant.

We further discuss the implications of similar types of

noisy or misleading malware behaviors in Section 6.

5.3.2 UNETB Deployment Results

The results we obtained for the UNETB deployment
have been obtained in a slightly different way, compared
to UNETA and FNET. Because of the higher volume of
trafﬁc in UNETB our proof-of-concept implementation
of the CPT match engine could not easily keep pace with
the trafﬁc. This was due especially to the fact that our
match engine software was sharing hardware resources
with other production software that have to be given a
much higher priority. A few weeks after conducting
the experiments reported here, we implemented an op-
timized version (written in C, rather than Python) that
is almost 8x faster; thus, it can easily keep up with the
trafﬁc on UNETB.

To compensate for the performance problems of our
prototype implementation, during the two-week deploy-
ment period we only considered the trafﬁc for every other
day. That is, we only matched the CPTs over about seven
days of trafﬁc in UNETB, effectively cutting in half the
trafﬁc volume processed by ExecScent.

“Static” and URL-Only Models

5.4
In this section we compare the results of ExecScent’s
adaptive templates, to “static” (i.e., non-adaptive) tem-
plates, which only learn from malware-generated traces
and do not take into account the trafﬁc proﬁle of the de-
ployment network, and to URL-based C&C request mod-
els, which only use information extracted from URLs.

To obtain the “static” models, we simply took ExecS-
cent’s CPTs and “turned off” the speciﬁcity parameters.
In other words, we set the speciﬁcity scores in Equa-
tion 1 to zero (with the exception of σd, which is set to
one), essentially turning the dynamic waits wk into their
static counterparts ˆwk (see Section 4.7). In the follow-
ing, we refer to these static (non-adaptive) templates as
“Speciﬁcitiy-Off” models.

To obtain the URL-based models, again we “turn-off”
the speciﬁcity information, and also ignore all compo-
nents of ExecScent’s CPT apart from URL-related com-
ponents. Effectively, in Equation 1 we only use the sim-
ilarity functions s1, s2a, and s2b deﬁned in Section 4.7.
We refer to these templates as “URL-Only” models.

600  22nd USENIX Security Symposium 

USENIX Association

of distinct source IP addresses that queried any of the 65
C&C domains. We found a maximum of 25,584 of dis-
tinct source IPs that in any given day queried these C&C
domains.
In other words, the new C&C domains dis-
covered by ExecScent allowed us to identify 25,584 new
potential malware infections across the six ISP networks.

6 Limitations

An attacker who gains knowledge of how ExecScent
works may try to avoid detection by mutating her bot-
net’s C&C protocol every time the C&C server is relo-
cated to a new domain. One possible approach would be
to implement a new protocol that can be deployed on all
the clients (i.e., malware agents) and servers (i.e., mal-
ware controllers) before switching to the new domain.
However, this would substantially increase the complex-
ity of managing the botnet and hurt its agility. Further-
more, for moderate to large botnets the updates would
take time to deploy and a mistake in the update proce-
dure could result in losing parts of or the entire botnet.

Another evasion approach may consist in injecting
noise into the C&C protocol to make it appear “differ-
ent”. For example, an attacker may randomly generate
the C&C URL path or name-value pairs in the query-
string, when making a request. However, if a malware
agent needs to convey enough information to (loosely)
authenticate itself to the C&C server, then at least one
request component must have some form of “structured”
data. Since ExecScent measures similarity by protocol
structure and gives more weight to the shared unique
components, it is non-trivial for an attacker to avoid de-
tection on all deployment networks. In fact, several mal-
ware families we detect during our evaluation of ExecS-
cent use such types of techniques to try to avoid detection
via regular expressions.

An attacker may also try to “mislead” the detector by
injecting noise into the domain name matches. For in-
stance, an attacker may send requests to many decoy be-
nign domains using the same malware C&C requests sent
to the true C&C server. This is the approach used by
the PUSHDO malware variant we discovered during our
evaluation. This type of noisy malware is actually easy
to identify, because of the number of unique destination
domains contacted by a single host that match one partic-
ular CPT within a short period of time. Thus, detecting
the infected hosts is easy. However, this makes it some-
what more difﬁcult to determine the true C&C domains
among all other domains. In this case, a threat analyst
must review the domains, before they can be added to a
blacklist; but at the same time, a security administrator
can be immediately alerted regarding the infected hosts,
thus enabling a prompt remediation.

Figure 8: Comparing C&C Models - True Positives

Figure 9: Comparing C&C Models - False Positives

To perform a comparison, we deployed the ExecS-
cent CPTs and their related “Speciﬁcity-Off” and “URL-
Only” models to UNETA, UNETB, and FNET for a pe-
riod of 4 days. Figure 8 and 9 summarize the overall
true and false positives, respectively, obtained by vary-
ing the detection threshold θ ∈ [0.6,1]. As can be seen
from the ﬁgures, ExecScent’s adaptive templates outper-
form the two alternative models, for detection thresholds
θ < 0.85. Unless we are willing to sacriﬁce a large frac-
tion of all true positives, compared to the numbers ob-
tained at θ = 0.6, the “Speciﬁcity-Off” and “URL-Only”
models will generate a very large, likely unsustainable,
number of false positives (notice the log scale on the y
axes of Figure 9).

5.5 Deployment in ISP Networks
We were also able to evaluate the results of ExecScent
over six large ISP networks serving several million hosts.
We proceeded as follows: given 65 new C&C domains
discovered by ExecScent during the live network deploy-
ment described in Section 5.3, we deployed the domains
to the six ISPs for an entire week, during which we mon-
itored all DNS trafﬁc. Each day, we counted the number

USENIX Association  

22nd USENIX Security Symposium  601

Blending into the background trafﬁc is another tech-
nique that may be used to avoid detection. For example,
an attacker may choose “common” data types and values
for their C&C protocol components. For some compo-
nents such as the URL path it may be easy to select a
popular value (e.g., “index.html”). However for many
of the components, the “commonality” is relative to the
deployment network’s trafﬁc proﬁle. Therefore, an at-
tacker would need to customize the protocol based on
the infected machine’s network. This may be difﬁcult
to do, because most network hosts have limited or no
visibility into the trafﬁc produced by other hosts in the
same network. Therefore, although a C&C protocol may
carry some “common” components, ExecScent’s adap-
tive CPTs may still be able to use those components that
are speciﬁc (i.e., non-popular) in the deployment network
to detect the C&C requests.

Finally, ExecScent’s CPTs depends on the malware
traces and labeled C&C requests from which they are
derived. Thus, ExecScent requires at least one or a few
malware samples from a malware family, before its C&C
protocol can be modeled and detected.
In this case,
though, malware code reuse plays to our advantage. A
few samples of a malware family whose code has been
reused elsewhere (because it was sold or leaked) will
in fact facilitate the detection of future malware strains.
Note that ExecScent in principle requires only a single
sample to generate a CPT, thanks in particular to the re-
quest generalization process (Section 4.2). That being
said, the quality of a CPT can be signiﬁcantly improved
when more than one sample sharing the same C&C pro-
tocol are available.

7 Related Work

Malware Clustering and Signature Generation: Group-
ing malware based on features extracted from HTTP re-
quests has beed studied for example in [7, 22, 23, 25].
Speciﬁcally, Perdisci et al. [22, 23] proposed a system
for clustering malware samples that request similar sets
of URLs. In addition, token-subsequences are extracted
from the URLs, and used to detect infected hosts on live
networks. In [7], information about HTTP request meth-
ods and URL parameters are used to cluster similar mal-
ware samples. The authors describe their clustering tech-
nique as a manual process and mention replacing it with
an automated system in the future.

A recently proposed system FRIMA [25] clusters mal-
ware samples into families based on protocol features
(e.g., same URL path) and for each family creates a
set of network signatures. The network signatures are
token-sets created from byte strings that are common to
a large percentage of the network trafﬁc within a clus-

ter. To reduce false positives, network signatures are
pruned by removing the ones that match any commu-
nication in the authors’ benign trafﬁc pool. Automated
network signature generation has also been studied for
detecting worms [19, 21, 27]. The generated signatures
typically consist of ﬁxed strings or token subsequences
that can be deployed in an intrusion detection system.
AutoRE [30] extends the automated signature generation
process to produce regular expressions that can be used
to match URLs in emails for the purpose of detecting
spam emails and group them into spam campaigns.

Our work focuses on automatic template generation
for detecting C&C communications and attributing them
to a known malware family. In particular, our main fo-
cus is not on clustering malware samples per se. Rather,
we apply clustering techniques mainly as an optimization
step to generate high quality control protocol templates.
Furthermore, we do not limit ourselves to only consider-
ing URLs or to extracting sets of common tokens. More
importantly, our C&C templates are adaptive, in that they
learn from the trafﬁc of the network where they are to be
deployed, thus self-tuning and automatically yielding a
better trade-off between true and false positives.

Botnet Detection and C&C Identiﬁcation: A number
of studies have addressed the problem of detecting bot-
net trafﬁc, for example [15, 16, 29]. BotSniffer [16] and
BotMiner [15] are anomaly-based botnet detection sys-
tems that look for similar network behavior across hosts.
The idea is that hosts infected with the same bot malware
have common C&C communication patterns. Further-
more, BotMiner [15] leverages the fact that bots respond
to commands in a coordinated way, producing similar
malicious network activities. This type of systems re-
quire multiple infected hosts on the same monitored net-
work for detection.
In addition, being anomaly-based,
they are not capable of attributing the infections to a spe-
ciﬁc malware family, and tend to suffer from relatively
high false positive rates.

Our work is different, because ExecScent can detect
botnets’ C&C even when only one bot is present in the
monitored network. Furthermore, unlike previous work,
ExecScent uses a hybrid detection approach, learning
from both known C&C communications and the deploy-
ment network’s trafﬁc to generated adaptive templates
that can detect new C&C domains with high true posi-
tives and low false positives.

Wurzinger et al. [29] propose to isolate C&C trafﬁc
from mixed malicious and legitimated trafﬁc generated
by executing malware samples in a controlled environ-
ment. They propose to ﬁrst identify malicious network
activities (e.g., scanning, spamming, etc.), and then an-
alyze the network trafﬁc going back in time until a net-
work ﬂow is found that is likely to represent the com-
mand sent to the malware that caused the previously

602  22nd USENIX Security Symposium 

USENIX Association

identiﬁed malicious activities to be initiated. However,
ﬁnding commands in malware network traces is not al-
ways possible.
In fact, most datasets of malware net-
work traces are obtained by running thousands of mal-
ware samples, with only a few minutes of execution time
allocated to each sample. Therefore, the chances of wit-
nessing a valid command being sent to a sample within
such a small amount of time is intuitively small. On the
other hand, malware samples typically attempt to con-
tact the C&C server as soon as they run, even though no
command to perform malicious activities may be issued
at ﬁrst contact. For this reason, ExecScent does not focus
on identifying malicious network activities performed by
the malware, and the related commands. Rather, ExecS-
cent leverages any type of (HTTP-based) communication
with a C&C server to learn control protocol templates
that can be later used to identify new C&C communi-
cations and related C&C domains, even when malicious
activities are not directly observable.

Jackstraws [17], executes malware in an instrumented
sandbox [13] to generate behavior graphs of the system
calls related to network communications. These system-
level behavior graphs are then compared to C&C graph
templates to ﬁnd new C&C communications. ExecScent
is different because it relies only on network information,
and does not require malware to be executed in an instru-
mented sandbox (e.g., it can use traces collected from
“bare metal” execution or live networks) to learn the tem-
plates. Furthermore, unlike Jackstraws [17], ExecScent
learns adaptive templates, which allow us to identify new
C&C domains in live networks.

Malicious Domains: Recently, a number of ap-
proaches for identifying malicious domains by monitor-
ing DNS trafﬁc have been proposed [2–4, 6]. These sys-
tems classify domains as malicious or benign, but do not
attribute them to a speciﬁc malware family. Also, [2, 6]
are mainly domain reputation system, and may assign a
low reputation score to generic malicious domains, not
only C&C domains, without providing any explicit dis-
tinction. On the other hand, [4] focuses only on malware
that use pseudo-random domain generation algorithms.
Kopis [3] is the only system that focuses explicitly on
generic malware domains, but it requires the ability to
monitor DNS trafﬁc at the upper DNS hierarchy, which
is difﬁcult to obtain.

Unlike the DNS-based systems mentioned above, Ex-
ecScent focuses on detecting new C&C domains in live
enterprise networks by inspecting HTTP(S) trafﬁc, and
using adaptive C&C protocol templates.

8 Conclusion

We presented ExecScent, a novel system that can dis-
cover new C&C domain names in live enterprise network
trafﬁc. ExecScent learns adaptive control protocol tem-
plates (CPTs) from both examples of known C&C com-
munications and the “background trafﬁc” of the network
where the templates are to be deployed, yielding a bet-
ter trade-off between true and false positives for a given
network environment.

We deployed a prototype version of ExecScent in three
large networks for a period of two weeks, discovering
many new C&C domains and hundreds of new infected
machines, compared to using a large up-to-date commer-
cial C&C domain blacklist. We also compared ExecS-
cent’s adaptive templates to “static” (non-adaptive) C&C
trafﬁc models. Our results show that ExecScent outper-
forms models that do not take the deployment network’s
trafﬁc into account. Furthermore, we deployed the new
C&C domains we discovered using ExecScent to six
large ISP networks, ﬁnding over 25,000 new malware-
infected machines.

Acknowledgments

We thank the anonymous reviewers for their helpful
comments. This material is based in part upon work sup-
ported by the National Science Foundation under Grant
No. CNS-1149051. Any opinions, ﬁndings, and con-
clusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect the
views of the National Science Foundation.

References
[1] ANTONAKAKIS, M., DEMAR,

J., STEVENS, K., AND
infrastructure
https://www.damballa.com/downloads/r_

DAGON, D. Unveiling the network criminal
of tdss/tdl4.
pubs/Damballa_tdss_tdl4_case_study_public.pdf.

[2] ANTONAKAKIS, M., PERDISCI, R., DAGON, D., LEE, W., AND
FEAMSTER, N. Building a dynamic reputation system for dns. In
Proceedings of the 19th USENIX conference on Security (Berke-
ley, CA, USA, 2010), USENIX Security’10, USENIX Associa-
tion, pp. 18–18.

[3] ANTONAKAKIS, M., PERDISCI, R., LEE, W., VASILOGLOU,
II, N., AND DAGON, D. Detecting malware domains at the up-
per dns hierarchy. In Proceedings of the 20th USENIX conference
on Security (Berkeley, CA, USA, 2011), SEC’11, USENIX As-
sociation, pp. 27–27.

[4] ANTONAKAKIS, M., PERDISCI, R., NADJI, Y., VASILOGLOU,
N., ABU-NIMEH, S., LEE, W., AND DAGON, D. From throw-
away trafﬁc to bots: detecting the rise of dga-based malware. In
Proceedings of the 21st USENIX conference on Security sympo-
sium (Berkeley, CA, USA, 2012), Security’12, USENIX Associ-
ation, pp. 24–24.

[5] BERNERS-LEE, T., FIELDING, R., AND MASINTER, L.
RFC3986 - Uniform Resource Identiﬁer (URI): Generic Syntax,
2005.

USENIX Association  

22nd USENIX Security Symposium  603

[6] BILGE, L., KIRDA, E., KRUEGEL, C., AND BALDUZZI, M.
Exposure: Finding malicious domains using passive dns analysis.
In NDSS (2011), The Internet Society.

[20] KOLTER, J. Z., AND MALOOF, M. A. Learning to detect and
classify malicious executables in the wild. J. Mach. Learn. Res.
7 (Dec. 2006), 2721–2744.

[7] CABALLERO, J., GRIER, C., KREIBICH, C., AND PAXSON, V.
Measuring pay-per-install: the commoditization of malware dis-
tribution. In Proceedings of the 20th USENIX conference on Se-
curity (Berkeley, CA, USA, 2011), SEC’11, USENIX Associa-
tion, pp. 13–13.

[8] CHANG, C.-C., AND LIN, C.-J. LIBSVM: A library for support
vector machines. ACM Transactions on Intelligent Systems and
Technology 2 (2011), 27:1–27:27. Software available at http:
//www.csie.ntu.edu.tw/~cjlin/libsvm.

[9] CRISTIANINI, N., AND SHAWE-TAYLOR, J. An introduction to
support Vector Machines: and other kernel-based learning meth-
ods. Cambridge University Press, New York, NY, USA, 2000.

[10] DANCHEV, D. Leaked DIY malware generating tool spotted in
the wild, 2013. http://blog.webroot.com/2013/01/18/
leaked-diy-malware-generating-tool-spotted-in-the-wild/.

[11] DE LA HIGUERA, C., AND CASACUBERTA, F. Topology of
strings: Median string is np-complete. Theoretical computer sci-
ence 230, 1 (2000), 39–48.

[12] EDMONDS, R.

ISC Passive DNS Architecture, 2012.

https://kb.isc.org/getAttach/30/AA-00654/
passive-dns-architecture.pdf.

[13] EGELE, M., SCHOLTE, T., KIRDA, E., AND KRUEGEL, C. A
survey on automated dynamic malware-analysis techniques and
tools. ACM Comput. Surv. 44, 2 (Mar. 2008), 6:1–6:42.

[14] FISHER, D. Zeus source code leaked. http://threatpost.

com/en_us/blogs/zeus-source-code-leaked-051011.

[15] GU, G., PERDISCI, R., ZHANG, J., AND LEE, W. Botminer:
clustering analysis of network trafﬁc for protocol- and structure-
independent botnet detection. In Proceedings of the 17th confer-
ence on Security symposium (Berkeley, CA, USA, 2008), SS’08,
USENIX Association, pp. 139–154.

[16] GU, G., ZHANG, J., AND LEE, W. BotSniffer: Detecting botnet
command and control channels in network trafﬁc.
In Proceed-
ings of the 15th Annual Network and Distributed System Security
Symposium (NDSS’08) (February 2008).

[17] JACOB, G., HUND, R., KRUEGEL, C., AND HOLZ, T. Jack-
straws: picking command and control connections from bot traf-
ﬁc.
In Proceedings of the 20th USENIX conference on Secu-
rity (Berkeley, CA, USA, 2011), SEC’11, USENIX Association,
pp. 29–29.

[18] JANG, J., BRUMLEY, D., AND VENKATARAMAN, S. Bitshred:
feature hashing malware for scalable triage and semantic analy-
sis. In Proceedings of the 18th ACM conference on Computer and
communications security (New York, NY, USA, 2011), CCS ’11,
ACM, pp. 309–320.

[19] KIM, H.-A., AND KARP, B. Autograph: toward automated, dis-
tributed worm signature detection.
In Proceedings of the 13th
conference on USENIX Security Symposium - Volume 13 (Berke-
ley, CA, USA, 2004), SSYM’04, USENIX Association, pp. 19–
19.

[21] NEWSOME, J., KARP, B., AND SONG, D. Polygraph: Auto-
matically generating signatures for polymorphic worms. In Pro-
ceedings of the 2005 IEEE Symposium on Security and Privacy
(Washington, DC, USA, 2005), SP ’05, IEEE Computer Society,
pp. 226–241.

[22] PERDISCI, R., ARIU, D., AND GIACINTO, G. Scalable ﬁne-
grained behavioral clustering of http-based malware. Computer
Networks 57, 2 (2013), 487 – 500. Botnet Activity: Analysis,
Detection and Shutdown.

[23] PERDISCI, R., LEE, W., AND FEAMSTER, N. Behavioral clus-
tering of http-based malware and signature generation using mali-
cious network traces. In Proceedings of the 7th USENIX confer-
ence on Networked systems design and implementation (Berke-
ley, CA, USA, 2010), NSDI’10, USENIX Association, pp. 26–
26.

[24] PLATT, J. Probabilistic outputs for support vector machines
and comparisons to regularized likelihood methods. Advances
in large margin classiﬁers 10, 3 (1999), 61–74.

[25] RAFIQUE, M. Z., AND CABALLERO, J. Firma: Malware clus-
tering and network signature generation with mixed network be-
haviors. In Proceedings of the 16th international conference on
Research in Attacks, Intrusions, and Defenses (2013), RAID’13,
Springer-Verlag. To be published. Research conducted concur-
rently and independently of ExecScent.

[26] SANTORELLI, S. Developing botnets - an analysis of recent ac-
tivity, 2010. http://www.team-cymru.com/ReadingRoom/
Whitepapers/2010/developing-botnets.pdf.

[27] SINGH, S., ESTAN, C., VARGHESE, G., AND SAVAGE, S. Auto-
mated worm ﬁngerprinting. In Proceedings of the 6th conference
on Symposium on Opearting Systems Design & Implementation -
Volume 6 (Berkeley, CA, USA, 2004), OSDI’04, USENIX Asso-
ciation, pp. 4–4.

[28] STONE-GROSS, B.

Pushdo downloader variant generating
http://www.secureworks.

fake HTTP requests, 2012.
com/cyber-threat-intelligence/threats/Pushdo_
Downloader_Variant_Generating_Fake_HTTP_
Requests/.

[29] WURZINGER, P., BILGE, L., HOLZ, T., GOEBEL,

J.,
KRUEGEL, C., AND KIRDA, E. Automatically generating mod-
els for botnet detection.
In Proceedings of the 14th European
conference on Research in computer security (Berlin, Heidelberg,
2009), ESORICS’09, Springer-Verlag, pp. 232–249.

[30] XIE, Y., YU, F., ACHAN, K., PANIGRAHY, R., HULTEN, G.,
AND OSIPKOV, I. Spamming botnets: signatures and character-
istics. In Proceedings of the ACM SIGCOMM 2008 conference on
Data communication (New York, NY, USA, 2008), SIGCOMM
’08, ACM, pp. 171–182.

[31] ZHOU, Y., AND JIANG, X. Dissecting android malware: Char-
In Security and Privacy (SP), 2012

acterization and evolution.
IEEE Symposium on (2012), IEEE, pp. 95–109.

604  22nd USENIX Security Symposium 

USENIX Association

