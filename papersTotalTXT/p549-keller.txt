An Architecture for Practical Actively Secure MPC with

Dishonest Majority

Marcel Keller

Department of Computer

Science, University of Bristol
m.keller@bristol.ac.uk

Peter Scholl

Department of Computer

Science, University of Bristol
Peter.Scholl@bristol.ac.uk

Nigel P. Smart

Department of Computer

Science, University of Bristol

nigel@cs.bris.ac.uk

ABSTRACT
We present a runtime environment for executing secure programs
via a multi-party computation protocol in the preprocessing model.
The runtime environment is general and allows arbitrary reactive
computations to be performed. A particularly novel aspect is that
it automatically determines the minimum number of rounds needed
for a computation, given a speciﬁc instruction sequence, and it then
uses this to minimize the overall cost of the computation. Various
experiments are reported on, on various non-trivial functionalities.
We show how, by utilizing the ability of modern processors to ex-
ecute multiple threads at a time, one can obtain various tradeoffs
between latency and throughput.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—security
and protection; D.4.6 [Operating Systems]: Security and Protec-
tion—cryptographic controls

Keywords
Multi-Party Computation

1.

INTRODUCTION

Recent years have seen great advances in practical variants of
protocols for secure Multi Party Computation (MPC). Initially these
practical instantiations came in the restricted security model of deal-
ing with passive (semi-honest) adversaries [3, 6, 25, 32, 9]. How-
ever, more recent work has focused on the case of full active secu-
rity [5, 11, 15, 26, 27], or sometimes covert security [13].

MPC protocols come in two ﬂavours: one ﬂavour based on Yao
circuits and one ﬂavour based on secret sharing.
In those based
on Yao circuits (which are predominantly focused on the two party
case) a function to be evaluated is expressed ﬁrst as a boolean cir-
cuit. The boolean circuit is then “encrypted” by one party in a pro-
cess known as garbling. The encryptor then passes the encrypted
circuit over to the other player, who then evaluates the circuit on
their own input. The input of the evaluating player is obtained from
the encryptor via an oblivious transfer protocol.

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
CCS’13, November 4–8, 2013, Berlin, Germany.
ACM 978-1-4503-2477-9/13/11.
http://dx.doi.org/10.1145/2508859.2516744.

The ﬁrst implementation of the Yao approach in the semi-honest
setting was the FairPlay system [25], which was followed by an im-
plementation in the active security case [24]. Following the work
in [28] it has been common to benchmark such Yao based imple-
mentations against evaluating the AES circuit, see for example [16,
17, 23, 31]. The state-of-the-art in terms of Yao based computation
would currently appear to be the system in [20].

In the secret sharing based approach initial practical work fo-
cused mainly on the case of three parties with only one allowed
semi-honest adversary [6, 32]. Systems such as [11, 32] could cope
with larger numbers of players, and more powerful adversaries, but
only with a signiﬁcant degradation in performance. Recent practi-
cal MPC work in the secret sharing tradition has focused on full
active security against a dishonest majority, utilizing a form of
“shared MAC” to provide security [5, 13, 15, 26]. It is in this area
that our work is focused. Whilst secret sharing based approaches
are not as well tailored to evaluate functions such as AES, the AES
functionality is still used as an important benchmark in this area
[12, 13, 21, 22].

Another aspect of modern practical work on secret sharing based
MPC protocols is the fact that they are often presented in a “pre-
processing model”. In this model one is allowed to perform in an
“ofﬂine phase” the generation of random data, which will be con-
sumed in the “online phase”. The key aspect is that the data pro-
duced in the ofﬂine phase must be generic and not depend on the
inputs to the function (which are unknown until the online phase),
nor too tied to the speciﬁc function being evaluated (which again
may not be known until the online phase commences).

The ﬁnal consideration is whether the protocol supports fully re-
active computation. By this we mean that, after a function result
has been determined, one can execute another function; with the
precise choice of the new function depending on the output of the
ﬁrst computation. A fully reactive computation allows this process
to be continued ad inﬁnitum, including possibly executing the of-
ﬂine phase again if the preprocessed data runs out.

Previous implementation work, in the secret sharing setting, has
mainly focused on the underlying MPC protocol and how to per-
form speciﬁc pre-deﬁned functionalities; for example, the AES cir-
cuit above. The traditional approach is to express the function to be
evaluated is described as an arithmetic circuit and then this circuit
is evaluated by executing a sequence of addition and multiplica-
tion “gates”. This is a throw back to the origins of MPC in the
theoretical community; where functions are always represented as
arithmetic circuits.

In this work we focus on modern secret sharing based MPC pro-
tocols in the pre-processing model whose online phase is actively
secure, for example the SPDZ protocol [13, 14, 15] or the Tiny-OT
protocol [26], or even the earlier information theoretic based pro-

549Shifting ones viewpoint to treating the three operations of local
additions, “open” and consuming pre-processed data as the basic
operations enables far more expressive operations. This viewpoint
is indeed implicit in the work of [10] which examines protocols for
comparison, bit-extraction and exponentiation amongst others. The
key ability utilized, in its most basic form, is to enable a secret value
to be masked, and then the masked value to be opened. Whilst this
might seem a small additional functionality, it turns out in prac-
tice that such an operation allows one to perform highly complex
tasks without needing to resort to their description as an arithmetic
circuit.

Shifting ones view point in this way is also important from a
practical perspective as the main bottleneck in implementations is
the scheduling of the open instructions. By focusing on “open”, as
opposed to multiplication, as the key operation to optimize one can
achieve signiﬁcant performance improvements.

In this work we present the design of an efﬁcient runtime exe-
cution environment for the protocols in the SPDZ family [13, 14,
15], much of our work will extend to other protocols such as Tiny-
OT [26]. This protocol family supports fully malicious (active) or
covert security, is in the preprocessing model and can (using the
techniques of [14]) support fully reactive computation. Unlike prior
work we concentrate only on the online phase and show how to de-
sign efﬁcient compilers and a runtime environment to enable high
level operations. Prior implementation work on the SPDZ family
has either concentrated on the speciﬁc functionalities over small ﬁ-
nite ﬁelds, such as evaluating AES [13], or has focused on the more
mathematically complex ofﬂine phase [14, 15].

tocols such as VIFF [11]. In practice such protocols do not execute
arithmetic circuits; they can evaluate far more elaborate basic oper-
ations since they have the ability to selectively “open” secret data.
In fact if the pre-processed data is so called “Beaver Multiplication
Triples”, then a multiplication gate is actually implemented as a
sequence of local additions and selective openings; and of course
consuming some pre-processed data.

Our focus is on more general purpose computation, and in par-
ticular how to enable a general purpose high level function descrip-
tion, which is then translated into low level operations, which are
then executed by an MPC runtime engine. In particular we present
experimental results for an actively secure online phase for var-
ious high level functionalities such as integer multiplication and
comparison, ﬁxed point and ﬂoating point operations as well as
runtimes for AES. Our AES runtimes are signiﬁcantly better than
prior runtimes for AES using other actively secure MPC implemen-
tations.

Our prime motivation is to understand how to engineer the over-
all runtime environment, rather than the design of speciﬁc proto-
cols. We therefore utilize prior work of others for our higher level
functions, i.e. for integer comparison we use [7], for ﬁxed point
numbers we use [8], for ﬂoating point numbers we use [1], for AES
we use the techniques of [13]. Whilst much of our design is biased
towards the SPDZ protocol, many aspects can be directly extended
to other MPC protocols that only involve broadcasting shares to all
parties for communication. This is true for all protocols based on
Beaver’s technique [2]. However, the basic idea of automatically
optimizing the communication pattern is also applicable to proto-
cols based on Shamir secret sharing such as [4]. Other work, e.g.
[19, 29], on optimization for MPC compilers has focused on issues
related to what information is leaked when values are revealed to
enable branching; this is an orthogonal question to those that we
consider.

We present our experimental results in the following way. We
show how one can trade, for the different high level functionalities,

latency for throughput. In many previous works on MPC the fo-
cus has been on throughput or latency alone. This however is not
realistic; in a given application scenario one is likely to have a spe-
ciﬁc latency and a speciﬁc throughput one has to meet for the MPC
solution to be applicable. We show how, by batching different num-
bers of operations together, and by running in different numbers of
threads, one is able to trade throughput for latency. This allows one
to reach a speciﬁc sweet spot for a given application.

In summary, existing MPC implementations do not provide ac-
tive security against a dishonest majority, do not scale (efﬁciently)
to any number of parties, require knowledge of (and manual opti-
mizations for) the underlying MPC protocol, or are generally not
very efﬁcient. To our knowledge, we present the ﬁrst MPC imple-
mentation that achieves the highest level of security in the online
phase, efﬁciently scales to at least ﬁve parties, provides a high-
level language that does not require a detailed understanding of
MPC, and is competitive with previous implementations achieving
the same level of security. To accomplish this, we use the SPDZ
protocol for the desired security and scalability, and we have im-
plemented a compiler for a Python-like language that optimizes the
communication pattern of the program. The compiler outputs a
bytecode that can be interpreted by a virtual machine. For efﬁ-
ciency, this virtual machine is implemented in C++ and kept rel-
atively simple. We believe that our approach optimally combines
usability with the efﬁciency gains of using machine code and avoid-
ing idle time caused by the multi-round nature of MPC based on
arithmetic circuits.

2. OVERVIEW OF THE SPDZ PROTOCOL
At the core of our work is a multiparty computation protocol in
the preprocessing model, with speciﬁc properties. In particular we
assume the preprocessing produces so-called Beaver Multiplication
Triples. These are triples of secret shared random values, such that
the third random value is the product of the ﬁrst two. In addition
we assume the ability to produce other forms of pre-processed data.
For more details see below.

There are a number of modern protocols in this family, and we
will focus on those in the SPDZ (”Speedz”) family, [14, 15]. This
family is an extension of a protocol ﬁrst introduced by by Bendlin
et al. [5]. The advantage of this approach is that expensive, public-
key machinery can be ofﬂoaded into the preprocessing phase. The
online phase uses (essentially) only cheap, information-theoretically
secure primitives and can therefore be extremely efﬁcient. More-
over, the technique comes with extremely strong security guaran-
tees: the online phase is actively secure against a dishonest major-
ity of corrupted players, irrespective of the security of the ofﬂine
phase. Thus we obtain a protocol which is actively (resp. covertly)
secure if we combine our online phase with an ofﬂine phase which
is actively (resp. covertly) secure as in [15] (resp. [14]). In addition
our online phase can evaluate reactive functionalities (i.e. multiple
sequential programs) and not just execute secure function evalua-
tion.
Throughout this exposition we assume a number of players n and
a ﬁnite ﬁeld Fq over which computations will be performed have
been ﬁxed. The basic data type used is a secret sharing scheme
over the ﬁeld Fq. Each party in the protocol Pi has a uniform share
αi ∈ Fq of a secret value α = α1 + ··· + αn, thought of as a
ﬁxed MAC key. We say that a data item a ∈ Fp is (cid:104)·(cid:105)-shared if Pi
holds a tuple (ai, γ(a)i), where ai is an additive secret sharing of
a, i.e. a = a1 + ··· + an, and γ(a)i is an additive secret sharing
of γ(a) := α · a, i.e.

γ(a) = γ(a)1 + ··· + γ(a)n.

5502.1 Ofﬂine Phase

In this section we summarize the data produced during the of-
ﬂine phase of the SPDZ protocol that is required by our runtime.
The main purpose of the preprocessing phase is to generate multi-
plication triples, namely shares of random values (cid:104)a(cid:105) ,(cid:104)b(cid:105) ,(cid:104)c(cid:105) sat-
isfying a · b = c, which are used to multiply secret shared values
during the online phase using a technique of Beaver [2].

Whilst multiplication triples are the only data theoretically needed
for the online phase, we can also use the ofﬂine phase from [14, 15]
to create additional raw data that improves the efﬁciency of certain
operations.
In Table 1 we detail all data that is used during the
online phase in our various higher level functions. The input data
is used to enable parties to input values into the computation, see
[15], we shall not discuss this further in this paper. The square data
is used to perform squaring of shared values more efﬁciently than
general multiplications; this is useful in general computation but
does not feature in our speciﬁc examples later in the paper. The inv
tuples are used for constant round protocols for integer comparison;
we shall see later that the logarithmic round protocols are actually
more efﬁcient in practice for the data types we are interested in.

Command

triple

square

bit

input

inv

Outputs

(cid:104)a(cid:105) ,(cid:104)b(cid:105) ,(cid:104)c(cid:105)

(cid:104)a(cid:105) ,(cid:104)b(cid:105)

Properties

a, b $← Fp, c = a · b
a $← Fp, b = a2

(cid:104)a(cid:105) ,(cid:104)a(cid:48)(cid:105)

a $← Fp, a(cid:48) = a−1

(cid:104)b(cid:105)
(cid:104)r(cid:105)

b $← {0, 1}
r $← Fp

Table 1: Data Prepared During the Ofﬂine Phase

The ofﬂine phase is assumed to produce ﬁve ﬁles of pre-processed
data for each player. Each ﬁle containing the relevant shares of the
above data. Exactly how much of each type of data one needs to
compute in the ofﬂine phase is of course unknown until the exact
function one is computing is known. However, one can make a
heuristic over estimate of this number. In addition as soon as data
produced in the ofﬂine phase has been exhausted one can return to
the ofﬂine phase to compute some more (note this option is only
available if one is using the MAC checking strategy described in
[14]).
2.2 Online phase

During the online phase the parties compute on open values (i.e.
values which are not secret shared) and secret values (i.e. values
which are secret shared). All standard arithmetic (+, ×, /, %) and
bitwise boolean (∨, ∧, ⊕) operations can be performed on open
values; where for the latter boolean operations we assume a ﬁxed
binary representation of values within the ﬁnite ﬁeld Fq. The arith-
metic operations can also be applied to one value which is shared
and one value which is opened, resulting in a new value which is
shared. Finally, in terms of arithmetic operations, two shared val-
ues can be added together to result in a shared value. Another set
of operations deal with loading data from the ﬁles precomputed in
the ofﬂine phase. These are all shared random values, subject to the
constraints given in Table 1.

All of the prior operations are “local”, i.e. they do not require any
interaction between the parties. As these local operations which
produce shared values are performed, the parties can locally update
the value of their shares of the associated MAC value. The power
of the online phase comes from the ability to interactively “open”
shared values so as to produce opened values. In such an opera-

tion a shared value (cid:104)a(cid:105) is opened to reveal a to all parties, but the
associated shared MAC values, a · α, are kept secret.

To protect the parties against malicious parties the shared MACs
of all such opened values need to be checked for correctness, and
this must be done in a way so that neither the MAC key nor the
MAC value itself is revealed. This is done using the protocol de-
scribed in [14]. We execute this protocol so that batches of MAC
values are checked all at once; in particular we batch up to 100,000
such MAC values to be checked into one operation. Thus the MAC
checking protocol is executed after every 100,000 such openings,
and once at the end of the computation to check any remaining val-
ues.
the opening operation on the shared value (cid:104)a(cid:105).

In [15] the following O(n) protocol is suggested for performing

• The players pick an arbitrary “nominated player”, say P1.
• Players Pi for i = 2,··· , n send P1 the value ai.
• P1 computes a = a1 + ··· + an and sends a to all other

players.

It is readily seen that the communication complexity of the above
protocol is that each player (on average) sends a total of three ﬁeld
elements per opening. However, in practice we have found the fol-
lowing naive protocol to be more efﬁcient.

• Pi sends ai to all other players.
• Each player locally computes a = a1 + ··· + an.

Here the parties need to each send n−1 ﬁeld elements per opening,
which will be worse than the prior method as soon as n > 3. How-
ever, in practice we found that the naive method performs better
than the O(n) method for n ≤ 5. This is because it is a one round
protocol as opposed to a two round protocol. This dependence of
efﬁciency on rounds will be a feature of our work and we will take a
great deal of trouble in later sections in minimizing the round com-
plexity of the entire computation in an automatic manner. Clearly
to minimize rounds across a computation many different values to
be opened can be opened at once in either of the methods above.

From the above description of the basic functionalities of the
online phase it is not clear whether such a runtime is universal, i.e.
can compute any arbitrary function. Since every function can be
expressed as an arithmetic circuit over Fq and addition of shared
values is a local operation, all that remains to show universality is
to demonstrate that multiplication can be performed. Given two
shared values (cid:104)x(cid:105) ,(cid:104)y(cid:105), to multiply them we take a precomputed
multiplication triple {(cid:104)a(cid:105) ,(cid:104)b(cid:105) ,(cid:104)c(cid:105)} and then the parties open the
values α = x + a and β = y + b. The shared value (cid:104)x · y(cid:105) can then
be computed from the local operation
(cid:104)x · y(cid:105) = (α − (cid:104)a(cid:105)) · (β − (cid:104)b(cid:105)) = α · β − β · (cid:104)a(cid:105) − α · (cid:104)b(cid:105) + (cid:104)c(cid:105) .
Thus multiplication is itself composed of our three primitive oper-
ations of local additions, open and consuming pre-processed data.

3. VIRTUAL MACHINE

To implement the online protocol we took a ‘ground up’ ap-
proach to architecture, starting with a very simple register-based
virtual machine that can be run on an ordinary desktop computer
by each party. By focussing initially on the low level design details
we have obtained a high performance implementation with mini-
mal overhead, leaving decisions for optimization and compilation
to a higher level. Our virtual machine could be used to run code
from a variety of language descriptions, given a cross-compiler to

551output the appropriate bytecode. To demonstrate this we use a sim-
ple language based on Python for all our programs. Details of the
compilation process and special-purpose optimizations that we de-
veloped to reduce communication costs are given in Section 4.

Programs to be run by the virtual machine must be encoded into
bytecode as assembly-like instructions (generally produced by a
higher level compiler), which are then parsed and executed. To
make the most of modern multi-core processors, we allow a number
of such precomputed bytecode ﬁles to run simultaneously, each in
a separate thread of the runtime virtual machine. This then leads to
an additional level of complication in relation to how the bytecode
ﬁles consume data from the ofﬂine phase, how the bytecode ﬁles
are scheduled across a large computation and communicate with
each other, and how branching and looping are to be performed. In
this section we describe the low level design decisions which we
have made to solve these problems. To ease exposition we shall
refer to a single sequence of bytecode instructions as a tape in the
following presentation.

The bytecode instructions within a tape are inﬂuenced by the
RISC design strategy, coming in only three basic types; correspond-

The virtual machine consists of a memory holding a ﬁxed num-
ber of clear and secret shared values. This memory is long term,
and as the virtual machine starts up/shuts down the memory is read
from/written to the player’s local disk. In addition the virtual ma-
chine runs a series of t + 1 threads; one thread acts as a control
thread while the other t threads have the task of executing (or not)
a tape of bytecode. The t tape-running threads open pairwise point-
to-point communication channels with the associated threads in the
other players’ runtime environments. There is no limit to the num-
ber of concurrent tapes, t, but in practice one will be restricted by
the number of cores. For our test machines with eight cores, we
found best performance when t is limited to seven or fewer.

The control thread executes the main program, or schedule. This
is a program which speciﬁes exactly which tape should be run at
which point. The schedule also speciﬁes which, and how many,
tapes are executed in parallel; with up to t such tapes may be exe-
cuted in parallel. The control thread takes the tapes to be executed
at a given step in the program, passes them to the execution threads,
and then waits for the threads to ﬁnish their execution, at which
point the control thread processes the next stage of the program.
Communication between tapes is done via reading and writing to
the virtual machine’s long term memory. To avoid unnecessary
stalls there is no locking mechanism provided to this memory. So
if two simultaneously running threads execute a read and a write,
or two writes, to the same memory location then the result is un-
deﬁned since it is unspeciﬁed as to which order the associated in-
structions will be performed in. Thus in this respect we have traded
speed for safety at the lowest level; it is the task of the compiler or
programmer to ensure the safe behaviour of concurrently running
bytecode.

We now turn to discussing the execution threads and the associ-
ated bytecode instructions within each tape. Each execution thread
has its own local shared and clear memory. To avoid confusion
with the long term memory we shall call this local memory a reg-
ister ﬁle, referring to the values as shared or clear registers. The
values of a register are not assumed to be maintained between an
execution thread running one tape and the next tape, so all passing
of values between two sequential tape executions must be done by
reading and writing to the main virtual machine memory. Again,
whilst this may seem like a very primitive approach to take, we
are ensuring that there is no unnecessary overhead and bloat in the
virtual machine, leaving more complex elements to the next level
up.

ing to the operations described in Section 2.2. The ﬁrst of these
most closely resembles traditional assembly code instructions, con-
sisting of load and store operations for moving registers to and from
the long term memory, and instructions for executing local arith-
metic and bitwise logical operations described in Section 2.2, that
can be performed without interaction. All of this class of bytecode
instructions are of the simple one output, one or two input variety
found in modern processors, where the inputs may be registers or
immediate values and the output register type depends on the input
types as speciﬁed earlier. The other two types of instructions are
data access instructions, which serve to load data from the prepro-
cessing phase, and a special instruction to open shared register val-
ues into clear registers. Note that this last instruction is the only one
requiring interaction between players. We now turn to discussing
each of these special MPC instruction types in turn.

The instructions for loading data from the preprocessing phase
are denoted triple, square, bit, input and inv, and they take as argu-
ment three, two, one, one and two shared registers respectively. The
associated data is loaded from the precomputed data and loaded
into the registers given as arguments. This leads to the problem that
multiple concurrent tapes could read the same data from the pre-
computed global store. Thus the control thread on passing a tape to
an execution thread for execution ﬁrst calculates a (reasonable) up-
per bound on the amount of pre-processing data it will consume be-
fore it is executed. This can be easily done in the case of programs
with no loops by simply counting the number of such instructions
in the tape. Then the control thread allocates a speciﬁc portion
of the unused precomputed data to the thread for use within this
tape. This solution avoids complexity in the ofﬂine phase (by not
requiring it to produce separate data for each thread) or added com-
plexity to the online phase (by not requiring a locking mechanism
to access the precomputed data), both of which would negatively
affect program complexity and/or performance. The cost for this
simple solution comes when executing tapes containing branching
instructions, an issue we return to in Appendix A.

The process of opening secret values is covered by two instruc-
tions. The startopen instruction takes as input a set of m shared
registers, and stopopen an associated set of m clear registers,
where m can be an arbitrary integer. They then execute the pro-
tocol from Section 2.2 to open the shared registers and place the
results into the designated clear registers. The fact that we can
perform m of these operations in parallel is vital in improving the
throughput and latency of applications run on the virtual machine.
Furthermore, splitting the process in two steps allows one to exe-
cute local instructions while the runtime is waiting for data from
other parties to arrive.

4. THE COMPILATION PROCESS

To produce the bytecode tapes required by our virtual machine,
we developed a compiler for a simple high-level language derived
from Python. The compiler takes a Python-like program descrip-
tion, extracts individual threads from this and breaks each thread
down into basic blocks. The individual basic blocks are then op-
timized and each thread used to produce a bytecode tape for the
virtual machine. Our optimization guarantees that, within a basic
block, the program will be compiled with the minimum possible
number of rounds of communication. This, coupled with the fa-
miliarity to many of the Python programming language, ensures
that the workload of implementing complex MPC functionalities is
eased considerably.

Instead of the traditional approach of using parsing and lexing
tools for compilation, we chose to implement the core language
features as a Python library, so that programs can be executed by

552Python itself, which then produces the relevant bytecode. This al-
lows for (compile-time) access to many of Python’s powerful fea-
tures such as list comprehensions, classes and functions with rela-
tively little effort. The library consists of a set of functions for cre-
ating, storing and loading clear and secret shared data types; load-
ing preprocessed data and more complex features such as threading
and branching. We then use operator overloading to provide trans-
parent support for arithmetic functions on our data types.

Upon execution, the Python code creates bytecode instructions
from our library functions and the overloaded arithmetic operators,
whilst keeping track of the appropriate threading and basic block
information. Complex functions such as secret multiplication and
comparison are expanded into sequences of the basic RISC-like in-
structions described in Section 3. Note that at this stage the open
instructions will only have one shared and one clear register as ar-
guments.
So despite our RISC instructions not supporting complex opera-
tions such as the multiplication of two shared values in Fq, we can
easily implement this using the compiler. This abstraction is key
to our approach; we do not at the low level execute a program cor-
responding to an arithmetic circuit over Fq as in other approaches.
We instead execute only a sequence of operations which are linear
operations on the shared values, open instructions, or load precom-
puted data. All high level operations can be built up from these
instructions; and indeed more efﬁcient high level operations can be
built which do not involve reduction to arithmetic circuits.

Using the above ideas we arrive at a point where we have pro-
duced a set of bytecode tapes in static single assignment (SSA)
form, with an unbounded number of clear and shared registers, and
standard compiler optimizations such as common subexpression
elimination can be performed if required. We then optimize the
communication complexity of each tape by reordering and merg-
ing the open instructions, as follows:

• Calculate the program dependency graph G = (V, E), whose
vertices correspond to instructions in the bytecode, and di-
rected edges represent dependencies between instructions.
• Let T ⊆ V denote the set of startopen vertices and let S

be the subset of T with no preceding nodes in T .

• Since all nodes in S are independent of each other, we now
merge them together into a single source node (and instruc-
tion) s, and do the same with associated stopopens.

• To compute the remaining merges that can be done, assign

weights to edges as follows:

(cid:40)−1 if vj ∈ T \ S

0

otherwise

wi,j =

This ensures that the length of a path between any two nodes
is exactly −1 times the number of startopen nodes lying on
the path, excluding the ﬁrst node and any source startopen
nodes.

• For each vertex v, compute distv, the shortest distance from
the merged source s to v. Since G is a DAG, this can be done
in linear time with a standard shortest paths algorithm using
dynamic programming.

• Merge all startopens (and their associated stopopens) that
are the same distance from s (by the shortest path property,
these must all be independent).

After this step, all independent startopen and stopopen instruc-
tions in the original program have been merged together. We can
now compute a topological ordering of G to produce a program
with the minimal number of rounds of communication for the given
instruction sequence.

To illustrate the beneﬁt of this, we implemented a standard poly-
nomial approximation for the ﬂoating point sin(x) function. If this
is compiled without merging open instructions (bar those merges
which are automatically encoded in the lower level constituent op-
erations) one would obtain a round complexity of 5874. On appli-
cation of our optimizer the number of rounds reduces to 759.

When computing the topological ordering, we calculate which
round of openings each instruction depends on and which round of
openings depends on the instruction. This can be done similarly
to the shortest paths method above, traversing G in reverse order.
Instructions that do not have a dependency in both directions, like
memory operations, are placed close to the remaining dependency
or close to the beginning if there is no dependency at all. This
reduces the memory usage of the virtual machine because it reduces
the lifetime of registers.

Finally, there are instructions that have dependencies in both
ways, i.e. those that depend on some opening round i, while at the
same time opening round i + 1 depends on them, clearly have to
be placed between the two rounds. However, instructions that only
depend on rounds up to i and are independent of round i + 1 can
be placed between the startopen and stopopen of round i + 1.
This utilizes the time used for waiting for data from other parties
for performing local computations.

To complete compilation, the compiler allocates registers. Since
all programs are in SSA form at this point, the allocation is straight-
forward. The compiler processes the program backwards, allocates
a register whenever a register is read for the last time, and deallo-
cates it whenever a register is written to. This ensures that a mini-
mal numbers of registers are used, which minimizes memory usage
of the virtual machine.

x = a * b + c * d
y = a * x
z = reveal(e)

Figure 1: A sample program.

For way of illustration, Figure 1 shows a possible high-level pro-
gram, and Figure 2 shows the corresponding output of the com-
piler. The semantics are as follows: Registers starting with s hold
secret values, registers starting with c clear values. adds, addm,
subs, mulc, and mulm denote addition, subtraction, and mul-
tiplication, respectively. The result is assigned to the ﬁrst regis-
ter, with the last letter of the instruction denoting the type of the
inputs: s for secret, c for clear, and m for mixed. The instruc-
tion triple loads a multiplication triple into the given registers.
Finally, startopen opens any number of secret registers, and
stopopen assigns the results of the last startopen to the clear
registers. The compiler mapped a-e to s0-s4, x to s5, y to s6,
and z to c0. Note that the openings required for the multiplications
a*b and c*d and the opening of e are all merged together in the
ﬁrst startopen/stopopen pair. Furthermore, the triple needed
for the multiplication a*x is loaded only after that. This allows for
the registers s6-s8 to be used for something else prior to that, thus
reducing the register usage.

Vector operations.

The virtual machine also supports vector instructions like adding
two vectors of registers and storing the result in a vector of regis-

553triple s13, s10, s14
triple s11, s12, s5
subs s9, s3, s10
subs s7, s2, s13
subs s6, s1, s12
subs s8, s0, s11
startopen s8, s6, s7, s9, s4
stopopen c1, c3, c4, c5, c0
triple s7, s8, s6
subs s9, s0, s7
mulc c2, c4, c5
mulm s13, s13, c5
mulm s10, s10, c4
adds s10, s14, s10
adds s10, s10, s13
addm s10, s10, c2
mulc c2, c1, c3
mulm s11, s11, c3
mulm s12, s12, c1
adds s5, s5, s12
adds s5, s5, s11
addm s5, s5, c2
adds s5, s5, s10
subs s10, s5, s8
startopen s9, s10
stopopen c1, c2
mulm s8, s8, c1
adds s6, s6, s8
mulm s7, s7, c2
adds s6, s6, s7
mulc c1, c1, c2
addm s6, s6, c1

Figure 2: Compiled sample program.

ters. This does not signiﬁcantly improve the performance of the
runtime (in that one could obtain the same effect using multiple
copies of the non-vectorized instructions), but vectorized instruc-
tions reduces memory and time needed for the compilation. Vector
instructions are based on the basic instructions and take the size of
the vector as an additional parameter. The further parameters have
the same format as for basic instruction. Register and memory ad-
dresses are interpreted as the base address of the vector, and con-
stants as constants. Furthermore, the high-level language allows to
declare vectors of secret and clear values. Operations on those are
compiled into vector instructions.

The key reason for enabling such vector instructions is to en-
able SIMD processing of multiple programs in the same thread. By
introducing vectorized processing we can obtain high SIMD paral-
lelism without compromising the speed of the compiler. We shall
see later that the SIMD execution of higher level operations, such as
ﬂoating point operations and AES evaluations, allow one to obtain
a higher throughput without sacriﬁcing latency too much. Thus the
vectorized instructions give us a another way of obtaining higher
throughput via parallelism, in addition to the execution of multiple
tapes in multiple threads.

5. RUN TIME ARITHMETIC
The runtime comes in two variants. One for working with ﬁnite
ﬁelds Fq where q is a “large” prime, say q ≈ 2128. This runtime
variant is more suited to general purpose computations; it supports
integer operations and working on ﬁxed and ﬂoating point numbers
as we shall describe later. Here we assume a statistical security pa-
rameter of 40, i.e. we allow deviations in the statistical distributions
of the various shared value output of 2−40. The second runtime is
for working with functionalities more related to binary circuit de-
scriptions, and it works in the ﬁnite ﬁeld F
240. The choice of this

speciﬁc ﬁnite ﬁeld is to obtain a probability of an active adversary
cheating of 2−40, as well as to enable the efﬁcient evaluation of the
AES functionality using the ﬁeld embedding technique described
in [13]. We now discuss each variant in turn:
5.1 The Large Prime Variant

Here we represent a signed t-bit integer value, where t < 128,
by its value modulo the 128-bit prime q. Using the basic embed-
ding of the range [−2t−1, 2t−1 − 1] into [0, q − 1], we can then
provide standard integer operations such as addition, multiplica-
tion and comparison. Note however that “wrap around” will only
occur in the range [0, q − 1] and not in [−2t−1, 2t−1 − 1] unless
special code is produced to do so.

For some computations, we may wish to decompose a secret
shared integer into bits and then perform bitwise operations on the
resulting shared bits. This can be done using fairly standard bit de-
composition techniques [7], but we achieve further efﬁciency gains
through use of shared bits from the preprocessing. In particular the
following bit decomposition technique is used in the ﬂoating point
addition protocol of [1] which we use. We pause to overview how a
bit decomposition is performed using the precomputed shared ran-
dom bits: Given a shared value (cid:104)x(cid:105) which is known to represent
an integer which is m < 128 − 40 = 88 bits long, we use a set
of m + 40 shared bits {(cid:104)bi(cid:105)}m+39
(loaded via the bit operation)
to obtain t sharings of the t least signiﬁcant bits of x. The shared
2i (cid:104)bi(cid:105) is ﬁrst computed, and applied
to (cid:104)x(cid:105) to obtain (cid:104)c(cid:105) = 2m+40 + 2m + (cid:104)x(cid:105) − (cid:104)y(cid:105). The value (cid:104)c(cid:105) is
then opened and the result taken modulo 2m to obtain r = x − y
mod 2m. We then take the m least signiﬁcant bits of r, and the
shared bits {(cid:104)bi(cid:105)}m−1
i=0 , and execute the circuit for binary addition
on these to compute the result. Since (cid:104)x(cid:105) is known to be m bits
long, the length of (cid:104)y(cid:105) provides 40 bits of statistical security when
revealing x − y.

masking value (cid:104)y(cid:105) =(cid:80)m+39

This last step requires us to efﬁciently evaluate binary circuits for
shared bit values over a large ﬁnite ﬁeld. The usual approach is to
execute a squaring for every XOR operation, since a⊕ b = (a− b)2
over the integers for binary a and b. This is relatively expensive; so
we instead adopt one of two different approaches.

i=0

i=0

• For the bitwise addition circuit used in bit decomposition
In this circuit for
above, we use the technique from [30].
every XOR of two bit values we also need to compute their
AND. If a, b ∈ {0, 1} then computing a ∧ b is the same as
computing their integer product; and compute a ⊕ b is the
the integer calculation a + b − 2 · a · b, and so comes at no
additional cost (since a ∧ b is also needed).

• For more general binary circuits, the overhead of multiplying
at every XOR gate can be costly, so we propose an alternative
method for avoiding this. We simply execute an addition op-
eration instead of XOR, and keep track of the maximum size
of the resulting shared integer. When the result may exceed
2128−40, or we need to return the ﬁnal bit value, we can re-
duce the integer modulo two by applying the bit decomposi-
tion procedure (with m = 1) to output the least signiﬁcant
bit. This allows us to emulate binary circuits of reasonable
depth at very little extra cost, bar the unavoidable overhead
of arithmetic modulo q.

Fixed and ﬂoating point arithmetic: In addition to integer types,
our compiler supports ﬁxed and ﬂoating point arithmetic. Our ﬁxed
point arithmetic type consists of a secret shared integer of size up to
64 bits, with the point ﬁxed at 32 binary places. Our secure ﬂoating

554point representation is based on the IEEE single precision format,
where the signiﬁcand has 24 bits of precision and the exponent 8
bits. This allows both signiﬁcands and exponents to be stored as
128-bit ﬁeld elements, leaving plenty of room for expansion after
addition and multiplication with a 40-bit statistical security param-
eter.

The results of ﬂoating point operations are compliant to the IEEE
standard, except we do not currently handle errors for overﬂow or
invalid operations. Clearly any error handling performed must be
secret to preserve privacy; we could use secret shared bits to handle
error ﬂags (as described in [1]) but chose to omit this for simplicity.
Note that we always use deterministic truncation in our proto-
cols, and not the probabilistic rounding method of [8]. Whilst more
efﬁcient, the probabilistic method leads to lower accuracy and more
complex analysis of errors, and is impractical for operations like
comparison, where a deterministic result is required.
5.2 The F

240 Variant

We also present results for a runtime based on secret sharing
over the ﬁeld F
240. This ﬁeld size is chosen to enable us to upper
bound the probability of an active adversary going undetected by of
2−40, which is a common value in previous work on actively secure
MPC protocols. In addition the ﬁeld F
240 enables us to efﬁciently
embed a copy of F
28 within it; this was shown to be useful in [13]
in evaluating an AES functionality.
In a similar way we embed
the binary ﬁeld F2 into F
240 thus enabling the evaluation of binary
circuits.

Unlike the case of the large prime characteristic ﬁeld, for the
characteristic two ﬁelds, our ofﬂine phase only produces triples,
bits and input sharings. We shall see in Section 6.2 that our run-
time is able to evaluate the AES functionality rather efﬁciently. In
particular one can obtain a factor of 100 improvement in either la-
tency of throughput on the previous best runtimes for AES evalua-
tion with active security [13]. In addition in trading throughput for
latency one can always obtain a factor of 10 improvement in either
of these values (and a factor of 100 improvement in the other) over
the results in [13]. However, we shall see that the runtime is less
efﬁcient at evaluating complex general binary circuits.

6. EXPERIMENTAL RESULTS

Our basic experimental setup was as follows. We used two ma-
chines with Intel i7-2600S CPU’s running at 2.8 GHz with 4GB of
RAM, connected by a local area network. Each machine represents
a party to the computation. Our experimental setup could have
been performed with more than two parties with only a marginal
decrease in performance.

Each program was written using our system and then compiled
to bytecode tapes. Each tape was run (cid:96) times (with (cid:96) at least 50,
the precise value depending on the experiment being carried out)
sequentially in a single thread, and then the experiment repeated
using tapes which had been vectorized with our SIMD instructions.
For the SIMD experiments, a tape executed n versions of the algo-
rithm in parallel with all of the communication batched together,
for values of n in {1, 2, 3, 4, 5, 10, 16, 32, 64, 128}. Finally the
same set of experiments were carried out running in t threads in-
stead of one, for t ∈ {1, 4, 7}. The upper bound of seven was
chosen since our processors had eight cores and one thread was
acting as the controller thread.

For all experiments we measured performance using through-
put and latency. Latency is simply the average execution time for
n· t operations (obtained by dividing the total runtime by (cid:96)), whilst
throughput is the number of operations executed per second. We
plot these measures against other in the graphs that follow, showing

tradeoffs that are possible depending on the application. Reading
the graphs from left-to-right, the number n of operations in paral-
lel is increased; note that the ﬁrst data point always corresponds to
n = 1, for purely sequential operations. Higher values of n result
is higher latency as more data needs to be passed across the net-
work in each communication. However, we can also see for most
functionalities that at a certain point increasing n no longer buys us
any extra throughput, and indeed it can decrease the throughput in
some cases.

The reason for focusing on the trade-off between throughput and
latency is as follows. Whilst many previous works on MPC fo-
cus on throughput only (i.e. number of AES operations per sec-
ond), they often hide the high latency needed to achieve such high
throughputs. In a real application one will have a speciﬁc latency
one is trying to achieve, with throughput often being a secondary
consideration. By presenting graphs which show the trade-off be-
tween throughput and latency, one can more accurately see the kind
of performance characteristics a real world application would en-
joy.
6.1 Arithmetic Circuits in Large Characteris-

tic

Integer Multiplication

In this section we detail the performance of our runtime when
working in Fq, for q a 128-bit prime. The most basic operation
interactive operation is multiplication of secret shared values, for
which we give timings below. We then discuss integer compari-
son, where we assume the input values are guaranteed to be 32 bit
or 64 bit signed integers. Integer comparison is, unlike for stan-
dard processors, more expensive than multiplication. Having inte-
ger comparison allows us to implement sorting of lists with sorting
networks. From comparison we move onto computing on approxi-
mations to real number computations using ﬁxed point and ﬂoating
point arithmetic. Table 6.1 summarizes the various costs and com-
plexities of the algorithms discussed in this section.
6.1.1
The basic non-local operation performed by the run time is ex-
ecuting an integer multiplication in the ﬁeld Fq. This requires a
single round of communication, with each player needing to send
two elements in Fq to each other player. In Figure 3 we present
the how the latency and throughput varies depending on how many
threads are used, and how many multiplications per thread are exe-
cuted in a SIMD like manner per thread.
6.1.2
Comparison of secret shared integers is a fundamental operation
in secure computation, used extensively throughout our ﬁxed and
ﬂoating point arithmetic below. The protocols we use all subtract
one input from the other and compare the result to zero by extract-
ing the most signiﬁcant bit without doing bitwise decomposition.

Integer Comparison

plication protocol to compute the preﬁx products pi =(cid:81)k

For the comparison operation we implemented two protocols de-
scribed by Catrina and de Hoogh [7], one with logarithmic round
complexity and one in constant rounds but with slightly higher
communication costs. The logarithmic rounds solution evaluates
a binary circuit for computing the carry bit from binary addition,
using this to obtain the comparison result. The constant rounds
variant takes a different approach, using an efﬁcient preﬁx multi-
(cid:80)k
j=i(dj +
j=i dj , where di = ai ⊕ bi, from which the comparison
1) = 2
bit can be derived. The preﬁx multiplication by Damgård et al.
[10] works as follows: Given non-zero inputs x0, x1, x2, . . . open-
−1
ing the masked values x0a
2 , . . . for random
non-zero a0, a1, . . . allows to compute a0a1, a0a1a2, . . . in a con-

−1
0 , a0x1a

−1
1 , a1x1a

555Protocol
Multiplication
32-Bit Comparison (Constant Rounds)
32-Bit Comparison (Logarithmic Rounds)
64-Bit Comparison (Constant Rounds)
64-Bit Comparison (Logarithmic Rounds)
Fixed Point Mult.
Floating Point Mult.
Floating Point Add
Fixed Point Comp.
Floating Point Comp.

Rounds Triples

1
4
5
4
6
7
15
47
10
7

1
61
52
125
114
63
95
483
73
124

Bits
0
143
72
207
104
168
218
2185
159
104

Inverses

0
31
0
63
0
0
0
32
0
0

Table 2: Round and communication complexities of the large prime ﬁeld protocols.

1 thread

4 threads

7 threads

1 thread (32-Bit)
4 threads (32-Bit)
7 threads (32-bit)

1 thread (64-Bit)
4 threads (64-bit)
7 threads (64-bit)

6,000

4,000

2,000

)
s
/
(

t
u
p
h
g
u
o
r
h
T

·103

600

400

200

)
s
/
0
0
0
1
(

t
u
p
h
g
u
o
r
h
T

0

0

2

4

6

8

10
Latency (ms)

12

14

16

0

0

10

20

30

40

Latency (ms)

Figure 3: Integer multiplication

Figure 4: Integer comparison (logarithmic rounds protocol).

stant number of rounds. Catrina and de Hoogh also gave a second
constant rounds protocol with better communication complexity,
however this exploits properties of Shamir secret sharing which we
cannot use.

For 32-bit comparison (resp. 64-bit comparison) operations with
a 40-bit statistical security parameter, the logarithmic rounds solu-
tion requires six (resp. seven) rounds of communication, compared
with four for the constant rounds protocol. However, the amount of
data transmitted is 20% smaller; 121 (resp. 249) required openings
for the logarithmic round version as opposed to 155 (resp. 315)
for the constant round version (32-bit). Our experiments indicate
that the saving of one round in the constant variant is dominated
by the communication cost of sending more data in each round.
The resulting performance is around 20% slower for the constant
round method compared to the logarithmic round method. There-
fore we used the logarithmic rounds protocol for all comparison
and higher-level operations. Note that our implementations have
slightly differing round and communication complexities to those
stated in [7], ﬁrstly because we can generate shared bits without in-
teraction using our preprocessing, and also as their constant rounds
solution uses a special public output multiplication protocol, which
cannot be applied to our setting.

Sorting

6.1.3
Based on integer comparison, we implemented odd-even merge
sorting networks similarly to Jónsson et al. [18]. Our results are
shown in Figure 5. For the implemented sizes they are similar to the
results by Jónsson et al. Note that the ﬁgures are based on only one
sequential sorting operation, using one thread for shorter lists and
seven threads for longer lists. The latter improves the performance
because sorting networks are highly parallelizable by design.

Sorting networks are the most efﬁcient known method for fully
private sorting because the most sorting algorithms rely on branch-
ing, which is not possible in multi-party computation if the condi-
tion is supposed to stay secret. Sorting networks only use compare-
and-swap operations, which guarantee that the ﬁrst output is the
minimum of the two inputs.
It is straightforward to implement
this operation using comparison. Odd-even merge sorting networks
have complexity O(n · log2 n). While this is not optimal, there is
no known sorting network with complexity O(n · log n) that is ef-
ﬁcient for practical parameters.

6.1.4 Fixed and Floating Point Arithmetic
As explained earlier ﬁxed point arithmetic is implemented using
a secret shared integer of size up to 64 bits, with the point ﬁxed
at 32 binary places. Here addition is simply a local operation (as
with integer arithmetic) and multiplication consists of multiplying

5561 thread (32-Bit)
7 threads (32-bit)

1 thread (64-Bit)
7 threads (64-bit)

1 thread (ﬁxed)
4 threads (ﬁxed)
7 threads (ﬁxed)

(ﬂoating)
(ﬂoating)
(ﬂoating)

)
s
m

(

y
c
n
e
t
a
L

107

106

105

104

103

6,000

)
s
/
(

t
u
p
h
g
u
o
r
h
T

4,000

2,000

102

103

104
List size

105

106

0

0

10

20
30
Latency (ms)

40

50

Figure 5: Sorting.

the two integers and truncating the result by 32 bits. This is essen-
tially the same cost as a 32-bit comparison operation. The relevant
performance is presented in Figures 7 and 8.

1 thread

4 threads

7 threads

)
s
/
(

t
u
p
h
g
u
o
r
h
T

600

400

200

0

0

20

40

80

60
Latency (ms)

100

120

140

160

Figure 6: Floating point addition

We use the ﬂoating point protocols from [1] for addition, mul-
tiplication and comparison (see Figures 6, 7, 8). From these oper-
ations other more complex ﬂoating computations can be built up.
As was also the case with integer comparison earlier, the complex-
ities of our protocols improve upon those stated in [1] due to the
preprocessing we use. Floating point protocols make extensive use
of comparison and bitwise addition protocols, for which using pre-
computed shared bits improves performance considerably. Recall
that our ﬂoating point operations are almost compliant to the IEEE
standard for single precision ﬂoating point numbers; the only devi-
ation being how we handle errors for overﬂow and invalid opera-
tions.

As can be seen in Figure 7, the difference in performance be-
tween ﬁxed and ﬂoating point multiplication is not huge; ﬂoating
point is only a factor of two slower.
It is in addition, however,
where ﬂoating point operations really suffer. This is due to the need
to obliviously align the exponents of both inputs, whereas ﬁxed

Figure 7: Fixed and ﬂoating point multiplication

point addition is a simple local operation (as with integers). With
the comparison operation, ﬂoating point is slightly faster due to
the smaller lengths of integers involved, despite the protocol being
more complex.

Our throughput results compare favourably with those for a pas-
sively secure, three-party protocol based on Shamir secret sharing
from [1]. They had a similar setup to ours, also running on a lo-
cal network, and our operations are roughly an order of magnitude
faster when maximum parallelism is used, probably due to our so-
phisticated implementation techniques described earlier.

1 thread (ﬁxed)
4 threads (ﬁxed)
7 threads (ﬁxed)

(ﬂoating)
(ﬂoating)
(ﬂoating)

4,000

3,000

2,000

1,000

)
s
/
(

t
u
p
h
g
u
o
r
h
T

0

0

5

10

15
20
Latency (ms)

25

30

35

Figure 8: Fixed and ﬂoating point comparison operation (less
than)

6.2 Circuits in Characteristic Two

For our experiments in characteristic two we settled on the fol-
lowing six functionalities as exemplars: AES (with and without
the key schedule included), DES (again with and without the key
schedule included), MD5, SHA-1 and SHA-256. The reason for

557selecting these was ﬁrstly, as has already been mentioned, AES is
the standard example in the ﬁeld and secondly, the circuits for DES,
MD5, SHA-1 and SHA-256 are readily available. In addition if one
is using AES as a prototypical PRF in an application it is interest-
ing to compare just how much more efﬁcient it is compared to other
PRF’s used in various cryptographic operations.

In Table 6.2 we present various statistics associated with these
functionalities; how many multiplication triples they consume, how
many bits, and how many rounds of communication are required,
all for a single invocation of the functionality. Note, this latter
ﬁgure is determined automatically by our compilation strategy to
be the minimum necessary given the algorithm used to compute
the function. The DES, MD5, SHA-1 and SHA-256 functional-
ities were computed via their standard binary circuit description
whereas the AES functionality made use of arithmetic description
over F
28 and the efﬁcient bit-decomposition method to compute
the S-Box from [12, 13].
6.2.1 AES

1 thread excl. KS
4 threads excl. KS
7 threads excl. KS

1 thread incl. KS
4 threads incl. KS
7 threads incl. KS

s
/
#

:

t
u
p
h
g
u
o
r
h
T

103

102

0

50

100

150

200

250

Latency : ms

Figure 9: AES runtimes

In this functionality we assume a single AES secret key k, which
has been shared between the parties. The goal of the functionality
is to evaluate the AES function on a single block of public data,
producing a shared output ciphertext block. From Figure 9 we can
see that having a pre-expanded secret key shared between the par-
ties makes very little difference in the execution times. We also see
that with either four or seven threads a throughput of roughly 1000
blocks per second is easily obtainable; with a latency of around
100ms. The smallest latency comes when using a single thread and
only batching up a single AES execution at a time; here we obtain
a latency of around 12ms, but only a throughput of 83 blocks per
second. Note, that the performance we obtain with our runtime is at
least an order of magnitude better than that obtained in [13], with-
out introducing any algorithmic improvements. Our improvement
is purely down to a deeper understanding of how to schedule and
execute MPC instructions.
6.2.2 DES
In Figure 10 we present a similar experiment for the DES cipher.
Notice, just as for AES there is little difference between evaluating
the cipher with a pre-expanded key and having to evaluate the key

s
/
#

:

t
u
p
h
g
u
o
r
h
T

300

200

100

0

0

1 thread excl. KS
4 threads excl. KS
7 threads excl. KS

1 thread incl. KS
4 threads incl. KS
7 threads incl. KS

200

400
600
Latency : ms

800

1,000

Figure 10: DES runtimes

schedule using MPC. Except in the case of running seven threads,
in which case having to execute the key schedule signiﬁcantly re-
duces the throughput. The latency and throughput are about ten
times worse than what was obtained from AES. This however is
not (directly) because we are evaluating a binary circuit over F2
as opposed to working with bytes in F
28, it is more a function of
the number of rounds of communication needed and the amount of
data communicated in those rounds.
6.2.3 MD5, SHA-1 and SHA-256

1 thread

4 threads

7 threads

s
/
#

:

t
u
p
h
g
u
o
r
h
T

60

40

20

0
350

400

450

500
550
Latency : ms

600

650

700

Figure 11: MD5 runtimes

When applying the runtime to the MD5, SHA-1 and SHA-256
circuits one sees that the number of multiplications/rounds has an
even more dramatic effect. The latency increases and the through-
put decreases as the number of rounds increases, and the number of
triples consumed increases. See Figures 11, 12 and 13 for details.

7. ACKNOWLEDGEMENTS

The third author was supported in part by a Royal Society Wolf-
son Merit Award. This work has been supported in part by ERC

558Functionality
AES (incl. Key Schedule)
AES (excl. Key Schedule)
DES (incl. Key Schedule)
DES (excl. Key Schedule)
MD5
SHA-1
SHA-256

Rounds Triples
1200
960
18124
18175
29084
37300
90825

50
50
263
261
2972
5503
3976

Bits
3200
2560

0
0
0
0
0

Table 3: Round and required pre-processed data for the various functionalities in characteristic two.

1 thread

4 threads

7 threads

576 of Lecture Notes in Computer Science, pages 420–432.
Springer, 1991.

s
/
#

:

t
u
p
h
g
u
o
r
h
T

s
/
#

:

t
u
p
h
g
u
o
r
h
T

30

20

10

0
650

20

15

10

5

0
700

700

750

800
850
Latency : ms

900

950

1,000

Figure 12: SHA-1 runtimes

1 thread

4 threads

7 threads

800

900

1,000 1,100 1,200 1,300 1,400
Latency : ms

Figure 13: SHA-256 runtimes

Advanced Grant ERC-2010-AdG-267188-CRIPTO and by EPSRC
via grant EP/I03126X.

8. REFERENCES
[1] M. Aliasgari, M. Blanton, Y. Zhang, and A. Steele. Secure

computation on ﬂoating point numbers. In Network and
Distributed System Security Symposium – NDSS 2013, 2013.

[2] D. Beaver. Efﬁcient multiparty protocols using circuit

randomization. In J. Feigenbaum, editor, CRYPTO, volume

[3] A. Ben-David, N. Nisan, and B. Pinkas. FairplayMP: a

system for secure multi-party computation. In P. Ning, P. F.
Syverson, and S. Jha, editors, ACM Conference on Computer
and Communications Security, pages 257–266. ACM, 2008.
[4] M. Ben-Or, S. Goldwasser, and A. Wigderson. Completeness

theorems for non-cryptographic fault-tolerant distributed
computation (extended abstract). In J. Simon, editor, STOC,
pages 1–10. ACM, 1988.

[5] R. Bendlin, I. Damgård, C. Orlandi, and S. Zakarias.

Semi-homomorphic encryption and multiparty computation.
In EUROCRYPT, volume 6632 of Lecture Notes in Computer
Science, pages 169–188, 2011.

[6] D. Bogdanov, S. Laur, and J. Willemson. Sharemind: A
framework for fast privacy-preserving computations. In
ESORICS, volume 5283 of Lecture Notes in Computer
Science, pages 192–206. Springer, 2008.

[7] O. Catrina and S. de Hoogh. Improved primitives for secure

multiparty integer computation. In J. A. Garay and R. D.
Prisco, editors, SCN, volume 6280 of Lecture Notes in
Computer Science, pages 182–199. Springer, 2010.
[8] O. Catrina and A. Saxena. Secure computation with

ﬁxed-point numbers. In Financial Cryptography, volume
6052 of Lecture Notes in Computer Science, pages 35–50.
Springer, 2010.

[9] S. G. Choi, K.-W. Hwang, J. Katz, T. Malkin, and

D. Rubenstein. Secure multi-party computation of boolean
circuits with applications to privacy in on-line marketplaces.
In O. Dunkelman, editor, CT-RSA, volume 7178 of Lecture
Notes in Computer Science, pages 416–432. Springer, 2012.

[10] I. Damgård, M. Fitzi, E. Kiltz, J. B. Nielsen, and T. Toft.

Unconditionally secure constant-rounds multi-party
computation for equality, comparison, bits and
exponentiation. In S. Halevi and T. Rabin, editors, Theory of
Cryptography – TCC 2006, volume 3876 of Lecture Notes in
Computer Science, pages 285–304. Springer, 2006.

[11] I. Damgård, M. Geisler, M. Krøigaard, and J. B. Nielsen.

Asynchronous multiparty computation: Theory and
implementation. In S. Jarecki and G. Tsudik, editors, Public
Key Cryptography – PKC 2009, volume 5443 of Lecture
Notes in Computer Science, pages 160–179. Springer, 2009.

[12] I. Damgård and M. Keller. Secure multiparty AES. In

R. Sion, editor, Financial Cryptography, volume 6052 of
Lecture Notes in Computer Science, pages 367–374.
Springer, 2010.

[13] I. Damgård, M. Keller, E. Larraia, C. Miles, and N. P. Smart.

Implementing AES via an actively/covertly secure
dishonest-majority MPC protocol. In SCN, volume 7485 of

559Lecture Notes in Computer Science, pages 241–263.
Springer, 2012.

[14] I. Damgard, M. Keller, E. Larraia, V. Pastro, P. Scholl, and
N. P. Smart. Practical Covertly Secure MPC for Dishonest
Majority – or: Breaking the SPDZ Limits. In To appear -
ESORICS. Springer, 2013.

[15] I. Damgård, V. Pastro, N. P. Smart, and S. Zakarias.

Multiparty computation from somewhat homomorphic
encryption. In CRYPTO, volume 7417 of Lecture Notes in
Computer Science, pages 643–662. Springer, 2012.

[16] W. Henecka, S. Kögl, A.-R. Sadeghi, T. Schneider, and

I. Wehrenberg. Tasty: tool for automating secure two-party
computations. In E. Al-Shaer, A. D. Keromytis, and
V. Shmatikov, editors, ACM Conference on Computer and
Communications Security, pages 451–462. ACM, 2010.
[17] Y. Huang, D. Evans, J. Katz, and L. Malka. Faster secure
two-party computation using garbled circuits. In USENIX
Security Symposium. USENIX Association, 2011.

[18] K. V. Jónsson, G. Kreitz, and M. Uddin. Secure multi-party
sorting and applications. IACR Cryptology ePrint Archive,
2011:122, 2011.

[19] F. Kerschbaum. Automatically optimizing secure

computation. In Y. Chen, G. Danezis, and V. Shmatikov,
editors, ACM Conference on Computer and Communications
Security, pages 703–714. ACM, 2011.

[20] B. Kreuter, A. Shelat, and C.-H. Shen. Towards billion-gate
secure computation with malicious adversaries. In USENIX
Security Symposium – 2012, pages 285–300, 2012.

[21] J. Launchbury, I. S. Diatchki, T. DuBuisson, and

A. Adams-Moran. Efﬁcient lookup-table protocol in secure
multiparty computation. In P. Thiemann and R. B. Findler,
editors, ICFP, pages 189–200. ACM, 2012.

[22] S. Laur, R. Talviste, and J. Willemson. Aes block cipher

implementation and secure database join on the sharemind
secure multi-party computation framework, 2012.

[23] Y. Lindell, E. Oxman, and B. Pinkas. The IPS compiler:

Optimizations, variants and concrete efﬁciency. In
P. Rogaway, editor, CRYPTO, volume 6841 of Lecture Notes
in Computer Science, pages 259–276. Springer, 2011.
[24] Y. Lindell, B. Pinkas, and N. P. Smart. Implementing

two-party computation efﬁciently with security against
malicious adversaries. In R. Ostrovsky, R. D. Prisco, and
I. Visconti, editors, Security and Cryptography for Networks
– SCN 2008, volume 5229 of Lecture Notes in Computer
Science, pages 2–20. Springer, 2008.

[25] D. Malkhi, N. Nisan, B. Pinkas, and Y. Sella. Fairplay -

Secure two-party computation system. In USENIX Security
Symposium – 2004, pages 287–302, 2004.

[26] J. B. Nielsen, P. S. Nordholt, C. Orlandi, and S. S. Burra. A

new approach to practical active-secure two-party
computation. In CRYPTO, volume 7417 of Lecture Notes in
Computer Science, pages 681–700. Springer, 2012.

[27] J. B. Nielsen and C. Orlandi. LEGO for two-party secure

computation. In TCC, volume 5444 of Lecture Notes in
Computer Science, pages 368–386. Springer, 2009.

[28] B. Pinkas, T. Schneider, N. P. Smart, and S. C. Williams.
Secure two-party computation is practical. In M. Matsui,
editor, Advances in Cryptology – ASIACRYPT 2009, volume
5912 of Lecture Notes in Computer Science, pages 250–267.
Springer, 2009.

[29] A. Rastogi, P. Mardziel, M. Hicks, and M. A. Hammer.
Knowledge inference for optimizing secure multi-party
computation. Manuscript, 2013.

[30] SecureSCM Project:. Cryptographic aspects - security

analysis, 2010. secureSCM deliverable D9.2.

[31] A. Shelat and C.-H. Shen. Two-output secure computation

with malicious adversaries. In K. G. Paterson, editor,
EUROCRYPT, volume 6632 of Lecture Notes in Computer
Science, pages 386–405. Springer, 2011.

[32] SIMAP Project. SIMAP: Secure information management

and processing. http://alexandra.dk/uk/
Projects/Pages/SIMAP.aspx.

APPENDIX
A. BRANCHING/LOOPING

Due to the way the precomputed data is processed the runtime
needs to know before executing a speciﬁc bytecode tape within a
thread how much precomputed data will be consumed. This pro-
duces a particular issue in relation to branching and looping. We
distinguish three cases:

• Loops where the number of iterations are known at compile

time.

structions).

• Conditional branches forwards (as in if-then-else con-

• Conditional branches backwards (as in do-while loops).

We note that conditional branching can only be executed on opened
values; we do not allow branching on shared values. We now dis-
cuss each of the above three cases in turn.

For loops where the number of iterations is known at compile
time, we can determine the amount of precomputed data used within
the loop by simply counting each instruction which loads data by
the requisite number of times the loop is executed; with a suitable
modiﬁcation for nested loops. Forward conditional branches essen-
tially allow one to conditionally skip code portions; thus by simply
counting the total number of instructions which load precomputed
data we will obtain an upper bound on the amount of precomputed
data; this will be enough for the runtime to allocate different por-
tions of the precomputed data to each bytecode tape within each
thread.

The main issue occurs with conditional branches backwards; these
arise for loop constructions where the number of iterations cannot
be determined at compile time. Here a design decision has to be
made, either we install logic to cope with this case (which would
incur a large performance penalty) or we restrict the use of such
constructs in some way. We decided to come up with the following
constraint, after considering various use-cases. The main problem
arises from not being able to allocate portions of the existing pool of
precomputed data to each thread. Thus we impose a constraint on
the main control thread in that it cannot execute any bytecode tape
containing a conditional backward branch with any other thread at
the same time. This means that tapes which could in theory be
executed concurrently will be executed serially. Then the actual
amount of precomputed data consumed during the execution can
be determined; rather than estimated ahead of time. We found this
compromise provided a suitable balance between efﬁciency and ap-
plicability; we can process the single tape very fast, at the expense
of not being able to execute multiple tapes in parallel for such situ-
ations.

560