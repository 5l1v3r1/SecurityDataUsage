Multi-Cloud Oblivious Storage

Emil Stefanov

UC Berkeley

emil@cs.berkeley.edu

Elaine Shi

University of Maryland
elaine@cs.umd.edu

ABSTRACT
We present a 2-cloud oblivious storage (ORAM) system that
achieves 2.6X bandwidth cost between the client and the
cloud. Splitting an ORAM across 2 or more non-colluding
clouds allows us to reduce the client-cloud bandwidth cost
by at least one order of magnitude, shifting the higher-
bandwidth communication to in-between the clouds where
bandwidth provisioning is abundant. Our approach makes
ORAM practical for bandwidth-constrained clients such as
home or mobile Internet connections. We provide a full-
ﬂedged implementation of our 2-cloud ORAM system, and
report results from a real-world deployment over Amazon
EC2 and Microsoft Azure.
Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection
General Terms
Algorithms; Security
Keywords
Oblivious RAM; outsourced storage; multi-cloud; privacy

1.

INTRODUCTION

Storage outsourcing is a growing industry that shields
storage users from the burden of in-house infrastructure
maintenance, and oﬀers economies of scale. However, due to
concerns over data privacy, “many potential cloud users have
yet to join the cloud, and many are for the most part putting
only their less sensitive data in the cloud” [8]. Encryption
alone is insuﬃcient for ensuring data privacy, since access
patterns can also leak a considerable amount of sensitive
information. For example, Islam et al. demonstrate statisti-
cal attacks that leverage access patterns to successfully infer
about 80% of the search queries made to an encrypted email
repository [18].

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516673.

hiding

completely

Oblivious RAM (ORAM) [7, 9–11, 13, 14, 16, 19, 22, 23, 25,
30–32,34,35], ﬁrst proposed by Goldreich and Ostrovsky [11],
is a cryptographic construct that allows a client to access
encrypted data residing on an untrusted storage provider,
while
patterns.
ORAM guarantees that the sequence of physical addresses
accessed is independent of the actual data requested – in
other words, the physical access patterns do not reveal in-
formation about the logical access patterns. To achieve this,
the client continuously re-encrypts and reshuﬄes data blocks
on the storage server, to cryptographically conceal the logi-
cal access pattern.

data

access

Due to the attractive security guarantees ORAM provides,
several recent works have investigated how to make ORAM
practical in an outsourced storage setting [20, 30, 31, 34, 35].
These works suggest that a primary hurdle for real-world
ORAM deployment stems from its high bandwidth cost. Un-
der realistic parameterizations (e.g., gigabytes to terabytes
of outsourced storage), the best known ORAM schemes in-
cur about 20X – 35X bandwidth cost, i.e., for every data
block accessed, on average 20 to 35 blocks need to be trans-
ferred. This makes ORAM unsuitable especially for band-
width constrained clients, e.g., home Internet connections or
mobile devices.

In some cases, shipping computation to the cloud can al-
leviate the client-cloud bandwidth consumption. When the
client does not trust the cloud provider, however, one im-
mediate challenge is how to securely compute in the cloud
without leaking data to the cloud provider. To this end, re-
searchers have suggested combining trusted hardware and
ORAM to securely delegate both computation and stor-
age to the cloud [2, 3, 6, 17, 20, 26]. While this approach
is promising, trusted hardware is not widely deployed on to-
day’s cloud platforms. Instead, we seek a solution that is
readily deployable on today’s cloud service providers.
1.1 Two-Cloud ORAM

We observe the following asymmetry in bandwidth re-
source provisioning: while client-cloud bandwidth is scarce
in many settings such as cellular networks or slower home
Internet connections; network bandwidth in between major
cloud providers (e.g., Amazon Web Services and Microsoft
Azure) is ample.

We show that by spreading data across multiple non-
colluding cloud providers, we can shift the 20X – 35X band-
width cost to inter-cloud communication, while keeping the
client-cloud bandwidth cost minimal — about 2.6X under
typical parametrization, and out of which 2X is necessary
to hide whether each access is a read or write. While a

247general k-cloud scheme exists as described in the full on-
line version [29], we focus on designing a highly optimized
two-cloud solution which we believe to be the more likely
deployment scenario. We provide a full-ﬂedged two-cloud
ORAM implementation, and have successfully deployed it
on two major cloud providers, Amazon Web Services (AWS)
and Microsoft Azure.

While the idea of using multiple non-colluding clouds is
natural, how to design a provably secure and practically ef-
ﬁcient scheme is challenging. The main challenge is how to
avoid passing data through the client during data shuﬄing,
and still ensuring security against a potentially malicious
cloud that can arbitrarily deviate from the prescribed behav-
ior. Multi-server ORAM has been considered in a theoretic
setting by Lu and Ostrovsky [21]. Their construction, how-
ever, focuses on asymptotic performance, and still requires
passing data through the client during shuﬄing. Conse-
quently, their construction would result in at least 20X-35X
client-cloud bandwidth cost under typical parametrization.
Technical highlights. Our main technique is to let the two
clouds shuﬄe the data amongst themselves. Particularly,
one cloud shuﬄes the data blocks, adds an “onion” layer of
encryption, and send them to the other cloud. In this way,
one cloud observes the permutation while the other observes
the accesses to these blocks — and neither cloud observes
both. In the next round, the two clouds switch roles.

To enforce honest behavior when one of the clouds may be

malicious, we design a novel commutative checksum-encryption
construction. At a very high level, the client shares a secret
checksum function with each cloud, such that the clouds can
monitor each other’s actions. After every shuﬄe, about 4λ
checksum bits per block are transferred between the client
and the clouds such that the client can verify the correctness
of the shuﬄe. In the above, λ = 128 is the security parame-
ter. The typical block size is much larger than λ, e.g., 4KB
(size of a memory page) or higher. While transferring entire
blocks to the client during shuﬄing would have incurred
20X-35X cost, by transferring 4λ bits per block we bring
this part of the cost down to 0.1X to 0.2X (out of a total of
2.6X) as shown in Section 5. We also show that a similar
technique may be employed to avoid reading a logarithmic
number of blocks for each data access, which is necessary in
most existing ORAM schemes.

√

Similar to the SSS construction [31], our 2-cloud ORAM
scheme requires the client to store less than 1.5GB of data
for an ORAM of 1TB in capacity (i.e., less than 0.15% of
the ORAM capacity). Particularly, in addition to caching
N ) data blocks where N is the total number of
about O(
blocks, the client also stores about 4 bytes of metadata per
block. While the metadata size is linear in theory; its size
√
in practice is typically comparable to or smaller than the
N ) data blocks cached. Out of theoretic interest, with
O(
suitable modiﬁcations to our scheme, it is possible to achieve
sublinear client storage by applying the recursion technique
described in earlier works [27,31], such that we could (recur-
sively) outsource the linear metadata to the cloud as well. In
practice, however, this is rarely necessary – and even if one
does apply the recursion, the recursion depth rarely exceeds
2 to 3 in practical settings [20, 27].

In addition to minimizing the client-cloud bandwidth con-
sumption, our 2-cloud ORAM scheme also roughly halves
the total bandwidth consumption (including inter-cloud and

client-cloud communication) in comparison with single-cloud
counterparts [31].
Threat model. We assume that at least 1 cloud provider is
honest, i.e., faithfully executes the prescribed protocol. The
other cloud may be malicious and arbitrarily deviate from
the protocol. We do not know ahead of time which cloud is
malicious. It is not within our goal to prevent the malicious
cloud from launching Denial-of-Service (DoS) style attacks
— DoS defense is orthogonal to and outside the scope of
this work. We also cannot prevent a malicious cloud from
voluntarily divulging its local states and views to the hon-
est cloud, which can potentially allow the honest cloud to
learn additional information. For example, a malicious cloud
can simply publish its states and protocol views on a public
website.

However, we do guarantee that as long as there exists at
least one honest cloud, a malicious cloud cannot learn any
information about the client’s logical access patterns. In fact,
any deviation from the prescribed protocol is immediately de-
tectable by the client. In other words, by deviating from the
prescribed protocol, a malicious cloud can only help an hon-
est cloud gain information, but cannot learn anything itself.
Moreover, it will be caught immediately if it ever deviates.
The above intuitive security model is generalized to the
k-cloud setting (when only 1 out of k clouds needs to be
honest), and deﬁned formally using a simulation-based def-
inition in the full online version [29]. We prove that our
constructions are secure under this simulation-based deﬁni-
tion in the full online version [29].

2. PRELIMINARIES

Our algorithm depends on the partitioning framework and
the partition ORAM construction proposed by Stefanov,
Shi, and Song [31], referred to as the SSS construction. We
give a brief background of the SSS construction.

It is important to note that partitions do not represent
clouds. In fact, as will be later explained in our construction,
each partition is split across multiple clouds.
2.1 Partitioning Framework

The SSS partitioning framework [31] allows us to securely
break-up ORAM read/write operations into read/write op-
erations over smaller partitions, where each partition is an
ORAM itself.

The framework consists of two main techniques, partition-
√
ing and eviction. Through partitioning, a bigger ORAM in-
√
N ) smaller ORAM
stance of capacity N is divided into O(
instances (called partitions), each with capacity O(
N ).
While naive partitioning can break security, Stefanov et al. [31]
propose a novel approach to allow partitioning without com-
promising security, as outlined below.

Figure 1 illustrates the partitioning framework. At any
point of time, a block resides in a random partition. The
client stores a position map to keep track of which parti-
tion each block resides in. To access a block whose identiﬁer
is u, the client ﬁrst looks up the position map and deter-
mine block u’s current partition p. Then the client issues an
ORAM read operation to partition p and looks up block u.
On fetching the block from the server, the client logically as-
signs it to a freshly chosen random partition – without writ-
ing the block to the server immediately. Instead, this block
is temporarily cached in the client’s local eviction cache.

248The partitioning framework [31]

√

// Divide the ORAM into

N partitions of size O(

√
N ).

them, re-encrypt them, and then write them back to the
server.

Read(u):
• Look up position map and determine that u is assigned to
• If u is not found in eviction caches:

partition p.

– ReadPartition(p, u)

Else if u is found in local eviction caches:

– ReadPartition(p, ⊥)

//read dummy

• Pick a random partition p(cid:48), add the block identiﬁed by u to
• Call Evict ν times where ν > 1 is the eviction rate.

the eviction caches, and logically assign u to partition p(cid:48).

Write(u, B):
Same as Read(u), except that the block written to the eviction
cache is replaced with the new block.

Evict:
• Pick a random partition p.
• If a block B exists in the eviction cache assigned to partition
• Else, call WritePartition(p, ⊥), where ⊥ represents a

p, call WritePartition(p, B).

dummy block.

Figure 1: The SSS partitioning framework [31]. Our
construction uses this framework to express ORAM Read
and Write operations in terms of the ReadPartition and
WritePartition operations of our multi-clouds construc-
tion.

A background eviction process evicts blocks from the evic-
tion cache back to the server in an oblivious manner. With
every data access, randomly select 2 partitions for eviction.
If a block exists in the eviction cache assigned to the chosen
partition, evict a real block; otherwise, evict a dummy block
to prevent leakage.
2.2 Partition ORAM

Each partition is a fully functional ORAM in itself. Our

partition ORAM construction is based on the partition ORAM
of the SSS construction [31], which is a variant of the origi-
nal hierarchical construction [11], but with various optimiza-
tions geared towards maximal practical performance.
2 log N + 1 levels, where
level i ∈ {0, 1, . . . , L − 1} can store at more 2i real blocks,
and 2i or more dummy blocks. We refer to the largest level,
i.e., level L − 1, as the top level.

Each partition consists of L := 1

We extend the client’s position map to store the position
tuple (p, (cid:96), oﬀset) for each block, where p is the partition, (cid:96)
is the level, and oﬀset denotes the oﬀset within the level.

Read. To read a block, the client ﬁrst reads its position
map and ﬁnd out the position (p∗, (cid:96)∗, oﬀset∗) of the block.
Then, the client reads one block from each level of partition
p. For level (cid:96) = (cid:96)∗, the client reads the block at oﬀset∗.
For other levels, the client reads a random unread dummy
block.
If the block is found in the client’s eviction cache,
one dummy block is read from each level.

Write. Writing back a block to a partition causes a reshuf-
ﬂing operation. Speciﬁcally, let 0, 1, . . . , (cid:96) denote consecu-
tively ﬁlled levels such that level (cid:96)+1 is empty. Writing back
a block B causes levels 0, 1, . . . , (cid:96) to be reshuﬄed into level
(cid:96) + 1. In the single-cloud SSS construction, the reshuﬄing
is done by having the client download all blocks, permute

3. BASIC TWO-CLOUD CONSTRUCTION

IN THE SEMI-HONEST MODEL

We ﬁrst explain a scheme that is secure when both clouds
are semi-honest. Then, in Section 4, we explain how to
use our commutative checksum-encryption construction to
achieve security against one malicious cloud (without know-
ing which one might be malicious).
3.1 Intuition
Reducing ORAM shuﬄing overhead. In existing single-
cloud ORAM schemes [30–32, 35], a major source of the
client bandwidth overhead stems from shuﬄing. Periodi-
cally, the client has to download a subset of data blocks
from the cloud, permute them, re-encrypt them locally, and
write them back.

In our multi-cloud ORAM construction, we delegate the
shuﬄing work to the two clouds to minimize client-cloud
bandwidth usage. For example, cloud S1 shuﬄes a subset
of blocks, adds a layer of encryption (i.e., an “onion layer”),
and sends them to S2. Later when the client needs to read
from that subset of blocks, it fetches them from S2. In this
way, while cloud S1 knows the permutation used in the shuf-
ﬂing, it does not see which blocks the client requests from
S2 later. In contrast, while S2 sees which shuﬄed blocks the
client requests later, it has no knowledge of the permutation
applied by S1. As a result, neither cloud learns any infor-
mation about the client’s logical (i.e., “unshuﬄed”) access
pattern. The details of inter-cloud shuﬄing are described in
Section 3.5.2.

Note that it is necessary for security to add onion encryp-
tion layers after every shuﬄe: if a block B gets shuﬄed from
S1 to S2 and then back to S1, we do not want S1 to be able
to link the two occurrences of the same block B based on
the block’s data.
In the full online version [29], we intro-
duce a background onion removal process to avoid adding
an unbounded number of onion layers to a block.

Reducing ORAM read overhead. Existing single-cloud
ORAM schemes can also incur signiﬁcant bandwidth over-
head for reading a data block in-between shuﬄings [30–32,
35]. For example, the client may need to read one or more
blocks from O(log N ) levels in a hierarchy.

In our multi-cloud construction, we are able to signiﬁ-
cantly reduce the bandwidth overhead for reading a block
by doing the following. The client requests a set of (already
shuﬄed) blocks from S1. S1 then randomly permutes them
with a permutation known only by S1 and the client, and
then S1 sends them to S2. Finally, the client requests and
downloads only a single block from S2 by providing S2 with
the permuted index of the requested block. The details of
reading a block from multiple clouds are described in Sec-
tion 3.5.1.
3.2 Data Layout on Clouds

√
Our scheme leverages the partitioning framework as de-
√
N )
scribed in Section 2.1. We divide our ORAM into O(
partitions each of capacity O(
N ). Each partition is di-
vided into L = O(log N ) levels, and each level resides on
one of the two clouds. Which level resides on which cloud

249changes as partitions are shuﬄed. Each level can be ﬁlled
(i.e., it contains blocks) or empty.

In practice, each cloud can distribute the data across mul-
tiple servers. For simplicity, we will ﬁrst regard each cloud
as a single logical entity; then in the full online version [29],
we explain how to distribute the data and workload across
multiple servers within each cloud.
New use of partitioning framework. We use partition-
ing for a somewhat diﬀerent motivation than described in
the SSS paper [31]. Particularly, the SSS construction relies
on partitioning mainly to get rid of oblivious sorting [11,12],
thus achieving a constant factor saving in bandwidth.

Like in ObliviStore [30], we use partitioning to achieve
parallelism, and to greatly improve response time. We shuf-
ﬂe and read from multiple partitions in parallel as described
in the full online version [29], and such parallelism is espe-
cially useful if each cloud has multiple servers each serving
a subset of the partitions.
If a read request happens on
a partition not currently being shuﬄed, the read does not
block on shuﬄing.
In our system, the contention (proba-
bility of a read occurring on a partition involved in a large
shuﬄe) is very small — since (1) each read happens to a uni-
formly random partition (regardless of the access pattern),
(2) there are thousands of partitions, and (3) large shuﬄes
happen very infrequently, i.e., with exponentially decreasing
frequency. It’s also important to note that as the amount
of parallelism increases, the contention increases. However,
in practice, for large enough ORAMs with many partitions,
the amount of parallelism necessary to saturate the server
bandwidth does not cause signiﬁcant contention.

In our multi-cloud ORAM, we can still achieve about 2X-
3X client-cloud bandwidth cost even with a single partition.
However, with a single partition, reads would have to block
waiting for reshuﬄing operations of up to N blocks to ﬁnish
(e.g., gigabytes or terabytes of data transfered between the
two clouds), resulting in prolonged response times. Williams
and Sion [35] describe an algorithm for simultaneously read-
ing and shuﬄing a single-partition ORAM, but we observe
that this technique about doubles the cloud-to-cloud band-
√
width used because the single partition has twice as many
levels than our O(

N ) sized partitions.

As studied in [31], a larger number of partitions requires a
larger amount of client-side cache. Therefore, the number of
partitions can be used as a knob to tune the tradeoﬀ between
the client-side storage and the response time.

3.3 Client Metadata

The client stores the following metadata. Note that even
though the client storage is asymptotically linear, it has a
very small constant and is quite small in practice. For ex-
ample, all of the client storage combined is less than 1.5
GB for a 1 TB ORAM with 4KB blocks (i.e., less than
0.15% of the ORAM capacity). The amount of client stor-
age used is similar to several existing single-cloud ORAM
systems [30, 31, 35].
• The position tuple (p, (cid:96), oﬀset) denoting the block’s cur-

rent partition, level, and oﬀset within the level.

• A bit vector for every partition and every ﬁlled level, indi-
cating which blocks in the level have been read and (logi-
cally) removed.

• A time value for every partition, i.e., the total number of

operations that have occurred so far for each partition.

• A next dummy block counter for each partition and each
level. The counter is set to 0 when a level is being rebuilt;
and incremented when a next dummy element is read.
This counter is suﬃcient to implement the GetNextDummy
function mentioned in Figure 4.
√
It is possible to even further reduce the client storage to
N ) by recursing on the position map as was proposed

O(
in [31].
3.4 Initializing

Each partition is initially placed in a random cloud (S1
or S2). Each block is assigned to a random partition, and a
random oﬀset in the top level (the largest level) of the par-
tition. The client’s position map is initialized accordingly.

We assume initially, all blocks have the value zero. This
can be done by initializing all blocks on the server to be
encryptions of the zero block with appropriate metadata at-
tached.
Initialization can be optimized to consume much
less bandwidth using a similar technique described in [31].
3.5 ORAM Read and Write Operations

A standard ORAM includes two operations: Read and
Write. As previously mentioned in Section 2 and Sec-
tion 3.2, our construction uses the SSS partitioning frame-
work [31]. The SSS framework speciﬁes how Read and
Write operations can be securely expressed in terms of par-
tition ORAM read/write operations called ReadPartition
and WritePartition. Section 2 explains how this can be
achieved. Since we rely on this framework we only need to
describe
and
WritePartition.

implement ReadPartition

how to

ReadPartition and WritePartition operations are per-
formed in parallel across all partitions but in serial for each
individual partition to ensure consistency.
3.5.1 Reading from a Partition
In a typical single-cloud ORAM scheme [31], the client
needs to download one block for each of the O(log N ) levels
in a partition to hide which level it wants to read from. In
our construction, we rely on the inter-cloud shuﬄing tech-
nique such that the client only needs to download a single
block.

Our ReadPartition protocol is illustrated in Figure 2(a)
and described in detail in Figure 4. To read a block u,
the client ﬁrst looks up its position map to get the tuple
(p∗, (cid:96)∗, oﬀset∗) indicating the position (partition p∗, level
(cid:96)∗, oﬀset oﬀset∗) for block u. The client then generates
one oﬀset for each ﬁlled level (cid:96) in partition p. For level
(cid:96) = (cid:96)∗, the oﬀset oﬀset (cid:96) = oﬀset∗. For all other levels, its
oﬀset (cid:96) corresponds to a random unread dummy (obtained
via GetNextDummy) determined by computing the permuted
oﬀset of the nextDummy[p∗, (cid:96)] counter of each level. The
client now divides these oﬀsets based on which cloud con-
tains each level, and sends the corresponding oﬀsets to each
cloud. It is important for security that the client sends to
each cloud only the oﬀsets of the levels contained within the
cloud.

Now, each cloud reads one block from each of its ﬁlled
level, at the oﬀsets indicated by the client. The cloud with
fewer ﬁlled levels onion-encrypts the fetched blocks, and

250(a) ReadPartition

(b) WritePartition (when (cid:96) < L − 1)

Figure 2: Two-cloud ORAM protocol. Actions are performed in nondecreasing order speciﬁed by the circled
numbers. If two actions have the same number within a diagram, it means that they can happen in parallel.

Figure 3: Layout of an ORAM partition across two clouds over time. This ﬁgure shows for a speciﬁc partition
which levels are ﬁlled in each cloud (S1 and S2) over time (ti ∈ {t1, . . . , t16}). When the bottom-most level is
empty, our algorithm automatically decides to which cloud to write the next block to preserve the invariant
that all consequentially ﬁlled levels (from the bottom upwards) are always located on the same cloud.

sends them to the other cloud. Without loss of general-
ity, assume S2 sends its onion-encrypted blocks to S1. S1
now merges the blocks from S2 with its own fetched set,
onion-encrypts them, and reshuﬄes them using a PRP key
shared with the client. The reshuﬄed set is sent to S2.

The client now reveals the desired index within the reshuf-
ﬂed array to S2, and S2 returns the block at that index.
In the full online version [29], we explain how the client
decrypts the fetched block. Decrypting involves removing
multiple onion layers of encryption as described in the full
online version [29].

3.5.2 Writing to a Partition
Our WritePartition protocol is illustrated in Figure 2(b)
and described in detail in Figure 4. Whenever a block is
written to a partition p, it is shuﬄed with all consecutively
ﬁlled levels 0, 1, 2, . . . , (cid:96) of the partition into level (cid:96) + 1. If
(cid:96) = L − 1 is the top level, all levels will be shuﬄed into the
top level.

Note that there are 2L possible ways to divide L levels be-
tween two clouds. Our assignment algorithm is designed to
best facilitate inter-cloud shuﬄing by enforcing the following
invariant.
Invariant. Any time when consecutive levels 0..i are ﬁlled
and need to be shuﬄed into level i + 1, levels 0..i all reside
within the same cloud.

This minimizes the cloud-cloud bandwidth because shuf-
ﬂes always involve consequentially ﬁlled levels (i.e., 0..i).
Suppose that, without loss of generality, cloud S1 holds lev-
els 0..i to be shuﬄed into level i+1 of cloud S2. By ensuring
that the above invariant holds, at the start of shuﬄing, S1

already has all of levels being shuﬄed (0..i) and does not
have to fetch additional levels from S2 before starting the
shuﬄe. Figure 3 demonstrates how the levels of a partition
are divided amongst the two clouds over time.

These shuﬄed blocks now form level (cid:96) + 1 of the same
partition, and reside on S2. Levels 0, . . . , (cid:96) on S1 of partition
p are deleted and future requests for level (cid:96) of partition p
are directed to S2.
Shuﬄing the top level. The only exception is when all
levels 0..L−1 are shuﬄed, i.e., when the top level is involved
in the shuﬄe. In this case, some dummy or obsolete blocks
need to be discarded during the shuﬄe. Therefore, a top-
level shuﬄe needs to be treated slightly diﬀerently, where the
client tells S1 (i.e., source of the shuﬄe) a subset of blocks
to keep for each level — without leaking any information.
The detailed protocol is described in Figure 4.

Note that almost all of the bandwidth consumed happens
when the two clouds exchange blocks, and the client’s band-
width is conserved.
3.6 Onion Encryption Layers
Key generation. The client shares a master secret key
msk 1 with cloud S1; and msk 2 with cloud S2.
Onion encryption. Whenever a cloud, say S1, needs to
onion encrypt a block, it generates a pseudo-random one-
time encryption key based on msk 1, the time value for the
partition under consideration, and the block’s position tuple
pos(cid:48) after the shuﬄe1.

1For read-phase shuﬄes, we use the block’s position prior to
shuﬄe.

251ReadPartition(p∗, u):

(p∗, (cid:96)∗, oﬀset∗) ← pos(u)

For j ∈ {1, 2} : Sj : Bi ←(cid:8)BlockAt [p∗, (cid:96), oﬀset (cid:96)] : ∀ ﬁlled level (cid:96) in partition p∗ on Sj

C :
C : Let oﬀset (cid:96)∗ := oﬀset∗. For (cid:96) (cid:54)= (cid:96)∗, let oﬀset (cid:96) = GetNextDummy(p∗, (cid:96))
(cid:9)

// The GetNextDummy function returns the position of a random unread dummy in the level
{oﬀset (cid:96) : for each ﬁlled level (cid:96) in partition p∗ ∈ Sj}

// ﬁnd out where block u resides.

For j ∈ {1, 2} : C → Sj :

2[i] := Eek [i](B2[i]) where ek [i] := PRFmsk 2 (i, p∗, t,“read-enc-A”)

// Without loss of generality, assume S2 has fewer ﬁlled levels.

S2 → S1 : B(cid:48)

S2 : ∀i ∈ |B2| : B(cid:48)
S1 : B := B1 ∪ B(cid:48)
S1 : ∀i ∈ |B| : B(cid:48)[PRPsk (i)] := Eek [i](B[i])

2

2

where sk := PRFmsk 1 (p∗, t,“read-shuﬄe”), ek [i] := PRFmsk 1 (i, p∗, t,“read-enc-B”)
i = (oﬀset of block u in B(cid:48))

// C can compute this with msk 1 and msk 2

S1 → S2 : B(cid:48)
C → S2 :
S2 → C : B(cid:48)[i]

For j ∈ {1, 2} : Sj → C : “Done”

WritePartition(p∗, B):
/* Without loss of generality, assume that cloud 1 needs to shuﬄe (cid:96) consecutive levels and send to cloud 2. Assume that the block to
be written B has been encrypted by the client using a secret key known only to itself.*/

Case 1: (cid:96) < L − 1

C → S1:

“Write partition”, p∗, B

S1: B ← BlocksAt[p∗, 0..(cid:96)] ∪ {B}
S1:

∀i ∈ |B| : do

Let i(cid:48) = PRPsk (i), B(cid:48)[i(cid:48)] := Eek [i](B[i])
where sk := PRFmsk 1 (p∗, t,“write-shuﬄe”),
ek [i] := PRFmsk 1 (pos(cid:48), t,“write-enc”)
// pos(cid:48) is B[i]’s position after the shuﬄe
// pos(cid:48) can be computed given i(cid:48)

done

S1 → S2: B(cid:48)
S1 → C:
S2 → C:

“Done.”
“Done.”

S2: BlocksAt[p∗, (cid:96) + 1] ← B(cid:48)

Case 2: (cid:96) = L − 1

C → S1: “Write partition”, p∗, B

C: I ← ∅

For each level i = 0 to L − 1:

let Ir := {positions of all unread real blocks},
let Id := {positions of 2i − |Ir| randomly
chosen unread dummy blocks},
I ← I ∪ (Ir ∪ Id)

C → S1: sorted(I)

S1 : B ← BlocksAt[I] ∪ {B}
S1: ∀i ∈ |B| : do

Let i(cid:48) = PRPsk (i), B(cid:48)[i(cid:48)] := Eek [i](B[i])
where sk := PRFmsk 1 (p∗, t,“write-shuﬄe”),
ek [i] := PRFmsk 1 (pos(cid:48), t,“write-enc”)
// pos(cid:48) is B[i]’s position after the shuﬄe
// pos(cid:48) can be computed given i(cid:48)

done

S1 → S2: B(cid:48)
S1 → C: “Done.”
S2 → C: “Done.”

S2: BlocksAt[p∗, L − 1] ← B(cid:48)

C: Update local metadata appropriately.

C: Update local metadata appropriately.

Figure 4: Partition read and write algorithms for the honest-but-curious model.

Whenever a cloud, say S1, needs to shuﬄe a set of blocks,
it generates a pseudo-random one-time shuﬄing key based
on msk 1, the current partition, and its the time value.
Onion decryption and background onion removal.
The client can recover all encryption keys and decrypt any
onion-encrypted block. The details of this is deferred to the
full online version [29] for our full construction — which is
based on the basic construction described in this section, but
augmented to provide security when one of the two clouds
is malicious.

4. SECURITY AGAINST ONE MALICIOUS

CLOUD

Informally, a malicious server can 1) corrupt data in stor-
age; and 2) deviate from the prescribed protocol, particu-
larly, not performing shuﬄing correctly.

Corrupted data blocks can be detected through standard
techniques such as message authentication codes (MAC), as
described in several earlier works [11, 31]. Therefore, we
focus our attention on how to detect deviations from the
prescribed protocol behavior.

High-level idea. The high-level idea is as follows. The
client has two secret checksum functions σ1 and σ2. σ1 is
shared with cloud S1, and σ2 is shared with cloud S2. The
client attaches encrypted and authenticated version of the
two checksums, denoted ˜σ1(B) and ˜σ2(B) to each block B
and stored on the servers along with the block. The check-
sums are encrypted and authenticated with a private key
that only the client knows.

The client treats the checksums the analogously to the
way the clouds treats the blocks. For example, whenever a
cloud permutes a set of blocks, the client will permute the
corresponding checksums. Whenever a cloud adds an onion
layer of encryption to a block, the client will compute the

252S1: B(cid:48) ← onion-encrypt and shuﬄe B.

// Let π denote the shuﬄing permuta-
tion.
S1 → S2: B(cid:48)
{˜σ1(B) and ˜σ2(B) : ∀B ∈ B}
S1 → C:
{σ2(B(cid:48)): ∀B(cid:48) ∈ B(cid:48)}
S2 → C:
∀B ∈ B: σ1(B) := Decrypt ˜σ1(B)
C:
σ2(B) := Decrypt ˜σ2(B)
C: Verify that gek i (σ2(Bi)) = σ2(B(cid:48)

C: Compute σ1(B(cid:48))

π(i)) for
0 ≤ i < |B| = |B(cid:48)| where ek i is the time
and position dependent one-time encryp-
tion key (for block Bi known only to C
and S1).

from σ1(B)

for all
blocks.
∀B(cid:48) ∈ B(cid:48) : ˜σ1(B(cid:48)) := Encrypt & authen-
ticate σ1(B(cid:48))
{˜σ1(B(cid:48)) : ∀B(cid:48) ∈ B(cid:48)}

C:
C → S2:

Figure 5: Verifying shuﬄing and onion-encryption
performed by cloud S1. The protocol is symmetric and
the same veriﬁcation can be done with for S2 by swapping
the roles of S1 and S2. The veriﬁcation is bandwidth eﬃcient
in that the client only needs to transfer checksums and not
the blocks themselves. Encryption and authentication of the
checksums is performed using a secret key known only to the
client.

checksum for the onion-encrypted block from its old check-
sum. After updating a checksum and before uploading it
back to the clouds, the client always re-encrypts it and re-
authenticates it using authenticated encryption.

Suppose Cloud S1 onion encrypts and shuﬄes a set of
blocks and sends them to S2. The client can verify that
S1 did this correctly by asking S2 to compute and send the
checksums of the shuﬄed and onion-encrypted blocks it re-
ceived from S1. The client can then verify the checksums it
received from S2 against the ones it computed directly from
the checksums of the old blocks (before shuﬄing and onion
encryption).

Detailed algorithm. Figure 5 describes in more detail
how the client can verify that a set of blocks have been
correctly shuﬄed and onion-encrypted by cloud S1. The
same protocol can be run with S1 and S2 interchanged to
verify shuﬄing and onion encryption performed by S2. We
prove the security of this veriﬁcation algorithm in the full
online version [29].

In order make the veriﬁcation in Figure 5 possible, our
checksum construction is designed to have the following prop-
erties.

Commutative checksum/encryption construction. The
client can compute the new checksum of a block (after onion
encryption by S1) from an old checksum (before onion en-
cryption by S1), without having to download the block itself.
In other words, for j ∈ {1, 2}, there is an eﬃciently com-
putable function gek taking the encryption key ek as the
key, such that

gek (σj(B)) = σj(Eek (B))

(1)

Unforgeability of checksum function. For the above
construction to be secure, the checksum function σ needs

to satisfy the following unforgeability property: when the
checksum function σ is kept secret from an adversary (i.e.,
one of the clouds), and the adversary has not observed any
input-output pairs, then, the adversary cannot ﬁnd a pair
B (cid:54)= B(cid:48) such that σ(B) = σ(B(cid:48)) except with negligible
probability. Note that our checksum function is not a hash
function in the traditional sense, since for a traditional hash
function, the description of the function is public.

Intuitively, this property ensures that S1 cannot deviate
from the shuﬄing protocol: suppose that after a shuﬄe from
S1 to S2, the correct block at permuted index i(cid:48) should be
B(cid:48). If S1 sent to S2 some other B∗ (cid:54)= B(cid:48) for index i(cid:48), then
σ2(B(cid:48)) (cid:54)= σ2(B∗) except with negligible probability. Hence,
the client can immediately detect such deviation.

This property can be formalized as below, and is used as a
building block in our full proof in the full online version [29].

Definition 1

(Unforgeability). We say that a check-
sum function family Σ has our special collision resistance
property if for all adversaries A (even when A is computa-
tionally unbounded),

(cid:20) Pick random σ from Σ;

(B0, B1) ← A(1λ)

Pr

(cid:21)

:

σ(B0) = σ(B1)
∧ B0 (cid:54)= B1

= negl(λ)

4.1 Commutative Checksum-Encryption Con-

struction

Counter mode of AES encryption. We use counter
mode of AES encryption. To encrypt a block B ∈ {0, 1}β,
with a one-time key ek ∈ {0, 1}κ where κ is 128 bits or 256
bits, compute the ciphertext R ⊕ B where R is deﬁned as

R := preﬁx (AESek (0)||AESek (1)||AESek (2) . . .)

(2)

In the above preﬁx denotes the function that takes the
ﬁrst β bits of the string input. This is a secure one-time
encryption scheme, since each ek is used to encrypt only
one message.
Checksum construction. Let σ : {0, 1}β → {0, 1}λ de-
note our specially constructed checksum function, where β
is the block size, and λ is the security parameter with a typ-
ical choice of 128. Our checksum function σ is deﬁned by a
λ × β matrix M ∈ {0, 1}λβ, where λ is the number of rows,
and β is the number of columns. Each entry in the matrix
is a bit picked at random from {0, 1}. Checksuming a block
is simply a matrix multiplication operation:
σ(B) := M · B (mod 2)

Commutative property of checksum-encryption com-
bination. Given an encryption key ek , and a previous
checksum σ, we deﬁne a veriﬁcation function g taking ek
and σ as inputs:

gek (σ) := σ ⊕ (M · R)

(3)

where the vector R is deﬁned as in Equation (2) — taking
ek as input.

Lemma 1. The above encryption and checksum combina-
tion satisfy Equation (1) for any ek ∈ {0, 1}κ, and any block
B ∈ {0, 1}β,

Proof. Note that XOR denoted ⊕ is simply addition
mod 2. Let R ∈ {0, 1}β be a function of ek as deﬁned

253by Equation (2). Therefore,

gek (σ(B)) = σ(B) ⊕ (M · R)
=(M · B) ⊕ (M · R) = M · (B ⊕ R) = σ(Eek (B))

Lemma 2

(Security of checksum function). Our

checksum function is secure by Deﬁnition 1.

The proof of this lemma is deferred to the full online ver-
sion [29].

Authenticated encryption of checksums. As mentioned
earlier, our checksum function is secure only when an adver-
sary has not seen any checksum value. Furthermore, we
also need to ensure that a malicious cloud cannot modify
the checksum and claim a diﬀerent checksum. We therefore
need to ensure the conﬁdentiality and authenticity (implies
freshness) of the checksums. To do this, the client will rely
on authenticated encryption to encrypt these checksums be-
fore attaching them to the blocks and uploading them to
a cloud. To ensure freshness, the authenticated encryption
incorporate the time of write and the position as well. One
way to do this is the following:
for each block shuﬄed at
time t and shuﬄed to position pos on cloud Sj, generate a
one-time authenticated encryption key ak based oﬀ a master
secret key ck known only to the client:

ak := PRFck (pos, t, j,“AE-Write-Shuﬄe”)
˜σ := AEak (σ)

where AE is an authenticated encryption scheme. For shuf-
ﬂes that occur in the ReadPartition algorithm (see Fig-
ure 4), a diﬀerent tag “AE-Read-Shuﬄe” may be used.
Optimization. In description so far, S2 sends the client a
new checksum σ2(B(cid:48)) for each block B(cid:48) after shuﬄing, and
the client veriﬁes them one-by-one. A simple optimization is
for S2 to use a collision-resistant hash function (e.g., SHA-
256), and hash all of the σ2(B)’s, and send a single hash to
the client C — who can then verify the hash by reconstruct-
ing all the σ2(B)’s.
4.2 Verifying Fetched Blocks

Whenever the client reads any data block using the
ReadPartition algorithm, it needs to verify the authen-
ticity and freshness of the fetched block. Our encrypted
and authenticated checksums ˜σ1(B) and ˜σ2(B) allow the
client to verify the authenticity and freshness for the block
B. Particularly, the freshness property is due to the fact
the authenticated encryption uses a time and position de-
pendent key.

In the ReadPartition algorithm, we simply use the afore-
mentioned algorithm to verify the shuﬄes associated with
read operations. Finally, after fetching a block B, the client
veriﬁes its authenticity and freshness using ˜σ1(B) or ˜σ2(B).

5. EXPERIMENTAL RESULTS
5.1 Overview and Experimental Setup

We have implemented a full-ﬂedged multi-cloud ORAM
implementation, consisting of 12,000 lines of code. Our im-
plementation uses hardware-accelerated AES-NI whenever
available (it is available on most modern Intel processors).

Figure 6: Our deployment. The workload is distributed
across multiple servers per cloud communicating in pairs.
We use this deployment for the experiments.

Our ORAM implementation uses asynchronous I/O oper-
ations to increase throughput. As Stefanov and Shi point
out in their recent work [30], in an asynchronous ORAM, to
avoid information leakage through timing of I/O events, we
need to make sure that the scheduling module does not use
any information related to the logical access pattern. We
therefore follow this guideline in our multi-cloud ORAM de-
sign, to ensure security in the asynchronous ORAM model.

Deployment. We deployed our multi-cloud ORAM im-
plementation running on top of two real-world major cloud
providers: Amazon Web Services (AWS) [1] and Microsoft
Azure [4]. We rented up to 5 servers per cloud. The AWS
servers were High I/O Quadruple Extra Large Instances,
each with 2 SSD-based volumes (1 TB each). The Azure
servers were Extra Large instances, with the Azure blob
storage as the storage backend (SSDs were not available on
Azure). Figure 6 illustrates our deployment.

Although these VM instances have large memory and fast
CPUs, we chose them only because they are provisioned
by the cloud providers with higher disk I/O and network
I/O than other instances. As explained in Section 5.4 and
Section 5.5, CPU and memory were never the bottlenecks
in our experiments.

Latency. Unless otherwise speciﬁed, we simulate a 50ms
(round-trip) latency between the client and the closest cloud
by asynchronously delaying the client’s requests and responses.
Since our two clouds are in fact two diﬀerent clouds (Amazon
and Azure), we do not simulate additional latency between
the clouds.

Scaling. We scale our experiments up to 5 servers per
cloud, because our experiments suggest that by this time,
the client-cloud bandwidth would already have been satu-
rated in most typical settings – hence, further scaling up in-
side the clouds would not have increased the overall ORAM
throughput. We consider a single client as in most existing
ORAM work.
Warming up. All of our experimental results represent the
performance of a warmed-up ORAM with N blocks. We
use the same technique for warming-up the ORAM as in
previous work [30, 31]. As explained in the full online ver-
sion [29], the client initializes the internal data structures

254of our construction so that the ORAM immediately starts-
oﬀ in a warmed-up state (i.e., with multiple ﬁlled levels in
each partition) without having to pre-initialize or zero-out
the server-side storage. This ensures that our performance
measurements are steady-state measurements, not burst or
best-case performance.
Access pattern. Because our ORAM construction protects
the privacy of the access pattern, the access pattern will have
no distinguishable eﬀect on the performance of the clouds.
However, the access pattern can aﬀect the size of the client’s
internal data structures (up to a certain point).
In order
to fairly evaluate our construction, we used a round-robin
access pattern as in previous work [30, 31], which turns out
to maximize the client storage by reducing the possibility of
reusing data already stored by the client.

For ORAM, reads and writes are also indistinguishable. In
our construction, they each involve invoking ReadPartition
and then WritePartition, except that during write opera-
tion, the client locally updates the block’s value in between
the ReadPartition and WritePartition operations. Our
access pattern hence consisted of writes.

System load. All of our experiments represent the perfor-
mance of our system under full load. In other words, data
is continuously being written to the ORAM as fast as the
ORAM can handle it. For each trial of each data point, we
started measuring the performance after 200 MB of data had
been written to the ORAM and we stopped measuring after
400 MB of data had been written (committed to disk by the
cloud). Although the client memory is up to 1.5 GB for a
1 TB ORAM, most of it is used for the position map, and
semaphores in our implementation ensure that only at most
70 MB of it is used to temporarily store data blocks before
they are evicted to the clouds. Therefore by starting our
measurements after 200 MB of data is written, we eliminate
the eﬀect of data buﬀering by the client.

Number of trails. Except for Figure 15, which provides
exact values calculated for our construction, each data point
in each ﬁgure represents the average over 20 trials. The error
bars represent one standard deviation.

5.2 Results: Single Server Per Cloud

We ﬁrst experimented with a single server per cloud, and

report our experimental results below.

Throughput to a large extent depends on the bandwidth
of the bottleneck resource.
If the client-cloud link is the
bottleneck, then our throughput would be roughly BW/2.6
where BW denotes the available bandwidth on the client-
cloud link. We refer to the number 2.6 as the client-cloud
bandwidth cost, i.e., to access a block, roughly 2.6 blocks
need to be transferred across the client-cloud link. A break-
down of the client-cloud bandwidth cost is shown in Fig-
ure 12.

In Figures 7, we measure maximum ORAM throughput
that our multi-cloud system can handle with with 1 server
per cloud when the client-to-cloud bandwidth is ample. A
300 GB ORAM of 4 KB blocks with a client over a 50ms
latency connection to the clouds can handle up to about 0.8
to 1.0 MB/s throughput.
If the block size is increased to
8 KB or 16 KB, the ORAM can sustain 1.2 to 1.6 MB/s
of throughput. The graph shows that for ORAMs of sizes
between 50 and 300 GB, the throughput is about the same.
Theoretically it should decrease by about 15% because an

ORAM of size 300 GB has about 14 levels per partition
whereas an ORAM with size 50 GB has about 12 levels
per partition and the cost I/O cost of the ORAM is pro-
portional to the number of levels in the partitions. This
slight decrease in performance is somewhat noticeable but
it’s mostly masked by variance due to external factors such
as ﬂuctuation in cloud network performance.

Figures 8,9, and 10 show the response time of our sys-
tem under diﬀerent parameterizations and the eﬀect that
the client-to-cloud latency has on the system throughput
and response time. In Figure 10 the dotted line is the ideal
y = x curve, and represents the case when the request can be
handled in exactly the client-to-cloud latency. The response-
time of our system is about 200ms to 300ms higher than the
client-to-cloud latency due to the cloud-to-cloud latency and
the network congestion created by the fact that our system
is under full load.

In these ﬁgures, the actual client-cloud bandwidth con-

sumption is about 2.6 times the ORAM throughput.
5.3 Scaling to Multiple Servers per Cloud

Figure 11 shows the scaling eﬀect when we use multiple
servers per cloud as described in the full online version [29].
Each server handles roughly 300GB of ORAM capacity, and
the load distribution scheme is described in the full online
version [29]. The total ORAM capacity is therefore 300GB
times the number of servers per cloud. We can see that the
throughput roughly scales linearly as the number of servers
grow (when the client-cloud link is not the bottleneck). The
variations are due mostly to varying network performance
between speciﬁc servers in the clouds. Our experiments also
suggest that the response time grows very slightly as the
number of servers per cloud grows (Figure 15).

We did not further scale up our experiments beyond 5
servers per cloud, since in the real-world settings we con-
sider, the client-cloud bandwidth would already have been
saturated at this scale.
5.4 Bottleneck Analysis
Client-cloud link is the bottleneck. As mentioned ear-
lier, if the client-cloud link is the bottleneck, the ORAM
throughput would just be BW/2.6, where BW is the client-
cloud bandwidth.
Storage performance is the bottleneck. When the
client-cloud link has ample bandwidth, the next immedi-
ate bottleneck is storage performance. In our deployment,
since Azure did not oﬀer SSD-capable instances, Azure’s
blob storage becomes the bottleneck. Figures 7, 8, 9, 10,
and 11, reported above focus on this case.
Inter-cloud networking is the bottleneck. Figures 14
and 15 report the throughput and response time we could
potentially obtain had the storage performance not been the
bottleneck – e.g., if Azure could provide SSD as their stor-
age; or if in the future, RAM-based persistent storage be-
comes available (e.g., RAMClouds [24]). In our deployment
scenario, the cloud-cloud network bandwidth varied from
30–60MB/s between a pair of servers, which is slower than
typical SSD performance (about 150MB/s). Therefore, to
extrapolate the performance we could obtain had Azure pro-
vided SSD-based storage, in Figures 14 and 15 we emulate
the storage back-end using /dev/zero (or its equivalent on
Windows) – such that the cloud-cloud network is the bot-
tleneck.

255Figure 7: ORAM throughput vs. ca-
pacity. With 1 server per cloud, and a
simulated 50ms latency client-cloud network
link.

Figure 8: ORAM response time vs.
capacity. With 1 server per cloud, and a
simulated 50ms latency client-cloud network
link.

Figure 9: Eﬀect of client-cloud net-
work latency on throughput. With 1
server per cloud, and an ORAM of 300GB
capacity.

Figure 10: Eﬀect of client-cloud net-
work latency on response time. With
1 server per cloud, and an ORAM of 300GB
capacity. The dotted line represents the net-
work roundtrip latency – this is the lower
bound on response time.

Figure 11: Scaling to multiple servers
per cloud: Throughput. With a sim-
ulated 50ms latency client-cloud network
link. The total ORAM capacity is 300GB
times the number of servers. The work-
load distribution scheme across servers is ex-
plained in the full online version [29].

Figure 12: Breakdown of client-cloud
bandwidth cost. Measured between the
client and all clouds combined for a single
ORAM read/write. The number of servers
per cloud does not aﬀect the client-cloud
bandwidth cost.

Figure 13: Microbenchmarks for
checksum computation. Rate at which
each VM in our experiments is able to per-
form the commutative checksum computa-
tion (parallelizing across all cores). Check-
sum performance slightly decreases with the
block size due to a decrease in memory lo-
cality as the checksum matrix increases pro-
portionally to the block size.

Figure 14: Potential
improvement
in throughput if storage were fast
enough (for 4 KB blocks). The “Stor-
age” curve represents the real-world case
where the storage performance is the bot-
tleneck. The “Cloud-cloud network” curve
emulates the storage with /dev/zero such
that the cloud-cloud network link is the bot-
tleneck.

Figure 15: Potential
improvement
in response time if storage were fast
enough (for 4 KB blocks). The “Stor-
age” curve represents the real-world case
where the storage performance is the bot-
tleneck. The “Cloud-cloud network” curve
emulates the storage with /dev/zero such
that the cloud-cloud network link is the bot-
tleneck.

5.5 Breakdown of Costs

Bandwidth baseline. Our baseline for bandwidth is a
simple cloud storage system that just reads and writes un-

encrypted blocks to storage. For example, reading or writing
a 4 KB block from the baseline system will incur 4 KB of

256bandwidth. When we say that there is kX ORAM band-
width cost, we mean that it takes k times as much band-
width to perform an ORAM read or a write operation.

Client-to-cloud bandwidth. Our construction achieves
about 2.6X bandwidth cost which means it takes about 10,647
bytes to read or write 4096 bytes from our multi-cloud ORAM.
Figure 12 shows the breakdown of the client-cloud band-
width cost. Particularly, about 2X (out of a total of ∼ 2.6X)
of the bandwidth cost is due to hiding whether the block is
being read or written (i.e., fetching one block of data with
1X cost and writing one block of data with 1X cost).

For the SSS partitioning framework (Section 2), we use
a background eviction rate at 0.3 times rate of data access
(in addition to the piggybacked write-back with each oper-
ation). Therefore, background evictions account for 0.3X
client-cloud bandwidth cost. Due to space limitations, the
reader is encouraged to refer to [31] for details about the
background eviction process.

The remaining 0.3X or so bandwidth cost is due to the
background onion removal process and the transfer of (en-
crypted and authenticated) checksums between the client
and cloud – as mentioned in Section 4, these checksums
are necessary to ensure security against one malicious cloud.
The checksum size is independent of the block size therefore
when the block size increases, the checksum fraction of the
bandwidth cost decreases.

Cloud-to-cloud bandwidth. The bandwidth cost of the
entire system, which mostly consists of the cloud-to-cloud
bandwidth is a constant factor smaller than that for the
best known single-cloud construction [30] using the same
amount of client memory. The intuition behind why that
is the case is as follows.
In the single-cloud ORAM by
Stefanov et al. [30], when blocks need to be shuﬄed, the
client downloads the blocks, shuﬄes them locally, and up-
loads them back to the cloud in order to avoid increasing the
the client’s storage. However, in our multi-cloud construc-
tion, one of the clouds just shuﬄes the blocks and sends
them to the other cloud and the other cloud simply stores
them and does not need to send the blocks back to the ﬁrst
cloud.

Cryptographic microbenchmarks. We report micro-
benchmarks for our new checksum function described in Sec-
tion 4.1. Figure 13 shows that our checksum function can be
computed at roughly 250-350 MB/s on our Amazon VM, and
at 150 MB/s on our Azure VM. Using hardware-accelerated
AES-NI in modern processors, decryption can be performed
at 2.5 GB/s per onion layer.

Client computation. The client computation for a 1 TB
ORAM mainly consists of: (1) encrypting and computing
the initial checksum of a block when it is written to the
ORAM, (2) encrypting about 15 checksums (16 bytes each)
per block written using the commutative checksum-encryption
technique described in Section 4.1, and (3) decrypting using
AES about 5-15 onion layers of a fetched block that were
added by the clouds during shuﬄing.

Even without hardware AES available on the client, a
modern laptop or desktop computer can easily perform these
cryptographic operations to sustain several megabytes per
second of ORAM throughput, and most likely saturate its
Internet connection (depending on the connection available
to the client).

Server computation.
In order for a server to sustain
about 1 MB/s per sever of bandwidth for a 1 TB ORAM,
the the server needs to be able to sustain about 30 MB/s
of AES computation and 30 MB/s checksum computation
over 4 KB blocks. As we just mentioned, our cryptographic
microbenchmarks show that the Amazon and Azure servers
we rented can sustain at least 5 to 10 times that rate for
checksums and over 80 times that rate for AES.
Client memory and storage. Our client-side storage
(memory) is less than 1.5 GB for an ORAM of up to 1 TB
capacity (i.e., less than 0.15% of the entire ORAM capacity).

Monetary cost. The monetary cost of our system depends
on several factors such as (1) the desired throughput and re-
sponse time, (2) the size of the ORAM, (3) idle time, and
(4) other factors such as geographic location. At the time
the experiment was run, under full load, it’s cost was about
$3.10/hour + $2.50/GB data transfer for 1-server per cloud.
By using Amazon S3 and cheaper VMs instead of high I/O
VM instances, the hourly cost can likely be reduced signiﬁ-
cantly.

6. RELATED WORK

Oblivious RAM was ﬁrst proposed by Goldreich and Os-
trovsky [11] in the context of protecting software from piracy.
They propose a seminal hierarchical construction with
O((log N )3) amortized bandwidth cost, where N denotes
the storage capacity of the ORAM. Since then, a line of
research in the theory community has been dedicated to
ORAM [7, 9–11, 13–16, 19, 22, 23, 25, 32–34].

Several works have suggested the use of ORAM in cloud
computing applications [13, 15, 16, 30, 32, 34, 35]. Williams,
Sion et al. have signiﬁcantly bridged the theory and prac-
tice of ORAM [32, 34, 35]. Goodrich, Mitzenmacher, Ohri-
menko, Tamassia et al. [15, 16] have also made signiﬁcant
contributions in this space. Backes et al. [6] use a combina-
tion of the binary-tree ORAM [27] and trusted hardware to
build privacy-preserving behavioral advertising applications.
Several recent eﬀorts have made further progress towards
making ORAM practical,
including PrivateFS [35],
Shroud [20], and ObliviStore [30]. PrivateFS and Oblivi-
Store show how to build parallel or asynchronous ORAM
schemes that achieve throughput in the range of hundreds of
kilobytes per second on a single disk-bound server — assum-
ing the client-cloud bandwidth is not the bottleneck. Unless
trusted hardware is deployed in the cloud, these schemes
result in 20X-35X client-cloud bandwidth cost, and is un-
suitable for bandwidth constrained clients. Our multi-cloud
ORAM is able to achieve a lower overall bandwidth cost
than both ObliviStore [30] and PrivateFS [35], and an even
lower client-cloud bandwidth cost. Shroud shows how to
implement ORAM with trusted co-processors such as IBM
4764 [2], and scale it up in a distributed data center setting,
but its performance is severely limited by the trusted co-
processors. In comparison, our goal is to provide an imple-
mentation that is readily deployable today (i.e., not relying
on trusted hardware), and that addresses the client-cloud
bandwidth bottleneck.

As mentioned earlier, Lu and Ostrovsky study multi-server
oblivious RAM from a theoretic perspective [21]. They show
that if there exists two non-colluding servers, the total band-
width cost of oblivious RAM (including messages sent be-
tween the client and the servers, as well as amongst the

257servers themselves) may be reduced to O(log N ) under the
constant client local storage setting. Their work is mostly
theory-minded, and results in a high client-server bandwidth
in practice. Lu and Ostrovsky’s non-collusion model is stronger
than ours, since their two non-colluding servers do not com-
municate with each other.

7. CONCLUSION

In this paper, we described a practical two-cloud Oblivi-
ous RAM protocol that reduces the client-server bandwidth
cost to about 2.6 times that of simply reading or writing the
block from non-oblivious cloud storage. In comparison, for
the same amount of client memory available to the client,
the best-known existing (single-cloud) ORAM constructions
have a bandwidth cost of about 20X–35X.

We proposed a novel commutative checksum-encryption
construction that allows our multi-cloud ORAM protocol
to to eﬃciently protect the privacy of the access pattern
against one malicious cloud (without necessarily knowing
which one).

We implemented a complete end-to-end systemdeployed
on multiple servers across two clouds (Amazon EC2 and
Windows Azure) and demonstrated that it can scale and
achieve several megabytes of throughput, saturating the avail-
able bandwidth of most typical client Internet connections.

Acknowledgments
This work is partially supported2 by an NSF Graduate Re-
search Fellowship under Grant No. DGE-0946797, by a DoD
National Defense Science and Engineering Graduate Fellow-
ship, NSF grant CNS-1314857, a Google Research Award,
and a grant from Amazon Web Services. We would like to
thank Matthew Green for helpful discussions and the anony-
mous reviewers for their insightful feedback.

8. REFERENCES
[1] Amazon web services. http://aws.amazon.com/.
[2] IBM 4764 PCI-X cryptographic coprocessor (PCIXCC).

http://www-03.ibm.com/security/cryptocards/pcixcc/
overview.shtml.

[3] Trusted computing group.

http://www.trustedcomputinggroup.org/.

[4] Windows azure. http://www.windowsazure.com/.
[5] D. Asonov and J.-C. Freytag. Almost optimal private

information retrieval. In PET, 2003.

[6] M. Backes, A. Kate, M. Maﬀe, and K. Pecina. Obliviad:

Provably secure and practical online behavioral advertising.
In S & P, 2012.

[7] D. Boneh, D. Mazieres, and R. A. Popa. Remote oblivious

storage: Making oblivious RAM practical. Manuscript,
2011.

[8] R. Chow, P. Golle, M. Jakobsson, E. Shi, J. Staddon,

R. Masuoka, and J. Molina. Controlling data in the cloud:
outsourcing computation without outsourcing control. In
CCSW, 2009.

[9] I. Damg˚ard, S. Meldgaard, and J. B. Nielsen. Perfectly

secure oblivious RAM without random oracles. In TCC,
2011.

[10] O. Goldreich. Towards a theory of software protection and

simulation by oblivious RAMs. In STOC, 1987.

2Any opinions, ﬁndings, and conclusions or recommenda-
tions expressed in this material are those of the author(s)
and do not necessarily reﬂect the views of the funding agen-
cies.

[11] O. Goldreich and R. Ostrovsky. Software protection and

simulation on oblivious RAMs. J. ACM, 1996.

[12] M. T. Goodrich. Randomized shellsort: A simple

data-oblivious sorting algorithm. J. ACM, 58(6):27:1–27:26,
Dec. 2011.

[13] M. T. Goodrich and M. Mitzenmacher. Privacy-preserving

access of outsourced data via oblivious RAM simulation. In
ICALP, 2011.

[14] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and

R. Tamassia. Oblivious RAM simulation with eﬃcient
worst-case access overhead. In ACM Cloud Computing
Security Workshop (CCSW), 2011.

[15] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and
R. Tamassia. Practical oblivious storage. In CODASPY,
2012.

[16] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko, and
R. Tamassia. Privacy-preserving group data access via
stateless oblivious RAM simulation. In SODA, 2012.

[17] A. Iliev and S. W. Smith. Protecting client privacy with

trusted computing at the server. IEEE Security and
Privacy, 3(2):20–28, Mar. 2005.

[18] M. Islam, M. Kuzu, and M. Kantarcioglu. Access pattern
disclosure on searchable encryption: Ramiﬁcation, attack
and mitigation. In Network and Distributed System
Security Symposium (NDSS), 2012.

[19] E. Kushilevitz, S. Lu, and R. Ostrovsky. On the

(in)security of hash-based oblivious RAM and a new
balancing scheme. In SODA, 2012.

[20] J. R. Lorch, J. W. Mickens, B. Parno, M. Raykova, and

J. Schiﬀman. Shroud: Enabling private access to large-scale
data in the data center. In FAST, 2013.

[21] S. Lu and R. Ostrovsky. Distributed oblivious ram for

secure two-party computation. Cryptology ePrint Archive,
Report 2011/384, 2011. http://eprint.iacr.org/.

[22] R. Ostrovsky. Eﬃcient computation on oblivious RAMs. In
ACM Symposium on Theory of Computing (STOC), 1990.

[23] R. Ostrovsky and V. Shoup. Private information storage

(extended abstract). In STOC, pages 294–303, 1997.

[24] J. Ousterhout, P. Agrawal, D. Erickson, C. Kozyrakis,

J. Leverich, D. Mazi`eres, S. Mitra, A. Narayanan,
G. Parulkar, M. Rosenblum, S. M. Rumble, E. Stratmann,
and R. Stutsman. The case for ramclouds: scalable
high-performance storage entirely in dram. SIGOPS Oper.
Syst. Rev., 43(4):92–105, Jan. 2010.

[25] B. Pinkas and T. Reinman. Oblivious RAM revisited. In

CRYPTO, 2010.

[26] J. Schiﬀman, T. Moyer, H. Vijayakumar, T. Jaeger, and

P. McDaniel. Seeding clouds with trust anchors. In CCSW,
pages 43–46, 2010.

[27] E. Shi, T.-H. H. Chan, E. Stefanov, and M. Li. Oblivious
RAM with O((log N )3) worst-case cost. In ASIACRYPT,
pages 197–214, 2011.

[28] S. W. Smith and D. Saﬀord. Practical server privacy with

secure coprocessors. IBM Syst. J., 40(3):683–695, Mar.
2001.

[29] E. Stefanov and E. Shi. Multi-cloud oblivious storage.

Technical report.

[30] E. Stefanov and E. Shi. ObliviStore: High performance

oblivious cloud storage. In IEEE Symposium on Security
and Privacy, 2013.

[31] E. Stefanov, E. Shi, and D. Song. Towards practical

oblivious RAM. In NDSS, 2012.

[32] P. Williams and R. Sion. Usable PIR. In NDSS, 2008.
[33] P. Williams and R. Sion. Round-optimal access privacy on

outsourced storage. In CCS, 2012.

[34] P. Williams, R. Sion, and B. Carbunar. Building castles out
of mud: practical access pattern privacy and correctness on
untrusted storage. In CCS, 2008.

[35] P. Williams, R. Sion, and A. Tomescu. Privatefs: A parallel

oblivious ﬁle system. In CCS, 2012.

258