2015 IEEE Symposium on Security and Privacy
2015 IEEE Symposium on Security and Privacy

SoK: A comprehensive analysis of game-based

ballot privacy deﬁnitions

David Bernhard∗, V´eronique Cortier†, David Galindo†‡, Olivier Pereira§, Bogdan Warinschi∗

∗University of Bristol, United Kingdom
‡SCYTL Secure Electronic Voting, Spain
§Universit´e Catholique de Louvain, Belgium

†LORIA/CNRS, France

Abstract—We critically survey game-based security deﬁnitions
for the privacy of voting schemes. In addition to known lim-
itations, we unveil several previously unnoticed shortcomings.
Surprisingly, the conclusion of our study is that none of the
existing deﬁnitions is satisfactory: they either provide only weak
guarantees, or can be applied only to a limited class of schemes,
or both.

Based on our ﬁndings, we propose a new game-based deﬁnition
of privacy which we call BPRIV. We also identify a new property
which we call strong consistency, needed to express that tallying
does not leak sensitive information. We validate our security
notions by showing that BPRIV, strong consistency (and an
additional simple property called strong correctness) for a voting
scheme imply its security in a simulation-based sense. This result
also yields a proof technique for proving entropy-based notions
of privacy which offer the strongest security guarantees but are
hard to prove directly: ﬁrst prove your scheme BPRIV, strongly
consistent (and correct), then study the entropy-based privacy of
the result function of the election, which is a much easier task.

I. INTRODUCTION

Privacy of votes was the subject of major debates during
the 19th century, at the time of the progressive introduction
of universal suffrage. Since then, it has become a standard in
all major democracies.

The introduction of electronic technologies as part of the
voting process however raises new challenges and privacy
concerns. Cryptographic voting protocols aim to guarantee
ballot privacy in e-voting by deﬁning security models and
then constructing schemes1to meet these models. Generally,
cryptographic voting schemes may be categorised into the
purely electronic where voters may vote from the privacy
of their own computers (e.g., Helios [1], [2] or Civitas [3])
and hybrid systems which use paper ballots and computers to
facilitate the tally (e.g., ThreeBallot [4], Prˆet-`a-Voter [5] and
Scantegrity [6]).

Modelling privacy: The development of security models for
ballot privacy started with the work of Benaloh [7], [8] and
has recently started to receive more attention with new models
being developed in both symbolic models
[9] and compu-
tational ones [10], [11], [12]. Unlike related privacy notions
such as conﬁdential message transmission, ballot privacy is

not absolute but relative to speciﬁc election bylaws and voter
choices. Consider a voting system that discloses the number
of votes received by each candidate: such a system essentially
reveals how each voter voted in the case where all voters vote
for the same candidate. Classifying such a system as insecure
is clearly undesirable; ballot privacy notions require a more
nuanced classiﬁcation.

One generic approach is to deﬁne vote privacy through
the design of an ideal functionality [13], [14], [15]: a voting
scheme is declared to satisfy privacy if it securely realizes (in
some formal sense) the ideal functionality. While being very
powerful, these deﬁnitions are also quite difﬁcult to prove on
real cryptographic voting protocols. For example, Helios is a
purely cryptographic voting protocol (that does not rely on
paper ballots) which has been used for real elections, and we
are not aware of any security proofs for Helios in a simulation-
based model2.

Another, more general, approach to ballot privacy deﬁni-
tions focuses on entropy [16], [17], used as a measure of
the amount of information that a voting system leaks about
votes. Early works were based on Shannon entropy, but other
entropy notions, based on min-entropy and Hartley entropy
showed to be particularly informative in natural contexts.
The interest of these entropy-based deﬁnitions is that they
capture many possible sources of privacy leakage: the privacy
leakage can be caused by the choice of the cryptographic
primitives but it may also be due to the election result itself
(which is out of the scope of simulation-based models) or
from the distribution of the votes. A voting scheme should
be considered as private under such a notion, if its privacy
leakage is (computationally) close to the privacy leakage of
an associated “ideal protocol” where voters send their vote on
a secure channel to a trusted party that simply computes and
announces the result. Furthermore, the amount of information
that is leaked by this result is precisely measured, making it
possible to compare different election tallying rules in various
contexts.

A third approach, initiated in the early works of Benaloh [8],
considers game-based deﬁnitions of vote privacy. This ap-

1The terms “scheme” and “protocol” can be read interchangeably without
much loss of precision. We use the former to refer to a collection of algorithms
and the latter to include the speciﬁcation of who should execute these
algorithms and when.

2Groth [13] proved a class of protocols to be UC-secure that at a ﬁrst glance
might seem to include Helios. However, Groth requires that the protocols use
voter’s identiﬁers as part of the correctness proofs in ballots — Helios does
not do this.

© 2015, David Bernhard. Under license to IEEE.
© 2015, David Bernhard. Under license to IEEE.
DOI 10.1109/SP.2015.37
DOI 10.1109/SP.2015.37

499
499

The boundaries between these three deﬁnition approaches
can of course sometimes become fuzzy, with some works
mixing them into a single deﬁnition. For instance, K¨usters,
Truderung, and Vogt [11] provide a privacy metric that focuses
on the probability that an attacker notices when an (honest)
voter changes her vote, while all of the other (honest) votes
follow a given distribution. These authors show how to analyze
the privacy offered by several paper-based voting protocols
such as ThreeBallot and VAV.

Comparing existing notions of privacy: Our ﬁrst contribu-
tion is to systematically review, compare, and discuss existing
game-based computational notions for vote privacy. In partic-
ular, we present them in a uniﬁed framework which facilitates
their comparison. Our review of the literature shows that none
of the existing deﬁnitions is satisfactory. Some limitations
were already known but we discovered further unnoticed
shortcomings in several of them. In short, based on our
ﬁndings, we classify existing deﬁnitions in three categories:
• too weak [18], [19], [20], [21]: these declare protocols to
be secure which intuitively do not preserve vote privacy.
We give examples which we think should be considered
privacy breaches, despite the examples meeting the given
privacy deﬁnitions.

proach has been used in the literature to model e.g. Helios.
The literature is actually quite abundant in terms of game-
based deﬁnitions of vote privacy, the differences between them
are poorly understood, and there is little guidance on how
to select one to analyze protocols. This work started as an
attempt to understand the strength and weaknesses of the many
game-based notions (as well as those of several non game-
based ones). In these game-based notions, privacy is described
as the negligible probability of an adversary to distinguish
between two situations, precisely described as games in the
deﬁnition. By only focusing on the information leakage due to
the cryptography, leaving the measurement of the leakages due
the tally to be measured with information theoretic techniques,
this approach typically leads to simpler, more generic and
more modular security proofs.

• too strong [22]: by contrast, this deﬁnition is so strong
that no veriﬁable protocol can meet the privacy constraint.
More precisely, we show that any protocol meeting this
deﬁnition must allow the authorities to announce any
result that is consistent with the number of votes on the
board. Therefore this deﬁnition cannot be used for any
protocol that aims at some veriﬁability, which is the case
of most protocols of the literature.

• too limited [23], [24], [25], [12], [11], [17], [26]: while
we did not identify any ﬂaw in these deﬁnitions, they
restrict the class of protocols or privacy breaches that
can be considered.
For example, they may lead to inconsistent results when
applied to protocols that use some natural result functions
(e.g., the majority function) or may not be applicable to
protocols that output not only a result but also a proof
of correct tallying (such as most cryptographic protocols
do), or they may only detect speciﬁc privacy breaches

(e.g. the case whether two voters vote the same is not
covered).

We summarise the limitations of each deﬁnition in Table I
later in the paper. For our classiﬁcation, we designed several
test-case (dummy) protocols that may be used to evaluate a
privacy deﬁnition; these are available in Appendix VIII.

A new notion for privacy: Our second contribution is to
propose a new game-based deﬁnition of privacy, called BPRIV,
that incorporates the lessons learned from our study. Our new
deﬁnition accounts for auxiliary data in the tally (such as
proofs of correct decryption), is compatible with veriﬁability,
and does not suffer from any of the ﬂaws we uncovered. As a
test of our new deﬁnition, we prove the Helios voting protocol
to be BPRIV secure. Speciﬁcally, we analyze what could be
considered the standard version of Helios nowadays (at least,
academically speaking), that uses strong Fiat-Shamir proofs
[19], implements duplicate weeding [27] and homomorphic
tallying. With respect to the threat model, we consider an
honest single trustee, an honest ballot box, and an adversary
that can adaptively corrupt a subset of voters. Apart from the
single trustee (that can be dealt of by adapting BPRIV to a
multi-authority setting), the other adversarial assumptions are
similar to those used in previous ballot privacy analyses of
Helios.

The ﬁrst of two novelties of our deﬁnition is that it accounts
for tallying operations that possibly include revote policies (for
example, only your last vote counts). Understanding tallying
is crucial for the privacy of ballots: not only does the tallying
operation usually disclose auxiliary data in addition to the
result (such as proofs of correct tallying) but it also performs
some cleaning operations such as removing invalid or dupli-
cate ballots or ballots that should be erased due to re-voting,
etc. For example, in Civitas [3], coercion-resistance crucially
relies on the fact that ballots submitted under coercion can be
(anonymously) identiﬁed and removed, without endangering
the voter’s identity. In Helios 2.0, removing duplicates is
necessary for privacy, otherwise the protocol is subject to
replay attack [28]. These operations are often considered
harmless but, perhaps surprisingly, they may be crucial for the
security of a voting system. Since these cleaning operations
may be used to guarantee privacy, they may also damage it
if performed incorrectly. Therefore, a good privacy deﬁnition
should account for these operations too. In this direction,
we identify a second security property, which we call strong
consistency, that ensures that the tally phase counts the votes
properly, even in the presence of an adversary. While this
property is clearly desirable for veriﬁability, it is also crucial
for privacy: otherwise the tally phase could leak information
on honest votes in the result itself or, more subtly, in the choice
of ballots that are removed during the cleaning operations.

A simulation-based notion of security: A natural question
is how to convince ourselves that the new deﬁnition BPRIV is
not ﬂawed as well. The second novelty of our deﬁnition is that
we establish a tight relation between BPRIV and security in a

500500

simulation-based3 model. Speciﬁcally, any voting scheme that
is BPRIV, strongly consistent (and strongly correct) securely
implements an ideal functionality. The privacy guarantees of
this ideal functionality are simpler to understand compared to
game-based deﬁnitions, but simulation-based security proofs
are harder to carry out. We view the relation with the
simulation-based notion as a validation of our new BPRIV
notion. In addition, we obtain the ﬁrst security proof for Helios
in a simulation-based model.

A potential advantage of simulation-based models is that
their security guarantees are easier to understand intuitively
than game-based ones. Our ideal functionality can be described
in one sentence: it simply collects all votes from the voters,
then computes and announces the result. Any scheme that
securely implements this functionality cannot leak any more
information (in particular about people’s votes) than this
functionality. This result gives us a way to explain the privacy
guarantees of Helios even to non-cryptographers. In addition,
the result provides a bridge towards entropy-based security
notions (similar to [17]), which incorporates, simultaneously,
privacy loss due to cryptography in use and to leaking result
functions.

A potential disadvantage however is that direct simulation-
based proofs are harder to get right; an incorrect proof does not
prove anything, however intuitive the functionality involved
might be. Furthermore, simulation based-deﬁnitions are quite
constraining, as far as security against adaptive corruption is
concerned.

Summary: We review existing game-based privacy notions
and ﬁnd them insufﬁcient. We develop a new privacy notion
BPRIV based on our insights from the reviewed notions and
subject it to three tests to assure its validity: (1) we prove that
Helios meets the new BPRIV notion (2) we check that the new
notion does not fail under any of the counter-examples that we
identiﬁed for existing notions and (3) we prove that BPRIV
(together with strong consistency) implies a simulation-based
notion of security. To our knowledge, BPRIV is the ﬁrst game-
based notion that comes with a justiﬁcation in a simulation-
based model too. As a corollary we obtain the ﬁrst simulation-
based security proof for Helios. In addition, BPRIV is the ﬁrst
notion to capture security requirements for two features found
in real voting systems: (1) revote policies and (2) auxiliary
data output by the tallying algorithm.

II. TERMINOLOGY AND BASIC PROPERTIES

We ﬁrst

introduce some terminology. In particular, we
consider here single-pass voting systems where voters only
have to post a single message to the ballot box to cast their
vote.

3For readers familiar with the Universal Composability (UC) notion of
security: our simulation-based model can be used to show UC security for a
particular functionality. We sketch this in our paper but we do not formally
introduce the UC framework and do not rely on any result holding in this
framework (e.g., composition.)

A. Single-Pass Voting

A voting scheme is relative to a result function ρ : (I ×
∗ → R where I is the set of voters identiﬁers, V is the
V)
set of possible votes, and R is the result space. Depending on
the voting scheme, voters identiﬁers range from voters public
identity (appended to ballots in Helios [29] for example) to
pseudonyms as well as sets of private credentials. For example
in Civitas [30], voters are supposed to use invalid credentials
when under coercion and valid ones when they wish to cast
their true vote.

The result function explains how the election system should
behave. When voters cast their votes, possibly several times,
ρ is in charge of specifying which votes shall be counted and
how. This involves typically two main tasks. First, usually
a revote policy speciﬁes which votes shall be retained. A
typical revote policy consists in keeping the last cast vote
from each voter. However, more complex revote policies can
be considered such as computing the average of the votes
or counting a vote at a polling station in preference to all
online votes4 (for the same voter). Then, once the revote policy
has been applied, votes are counted with some appropriate
counting function. Typical counting functions are:

• the multiset function multiset that discloses the sequence

of all the casted votes, in a random order;

• the counting function counting that

tells how many

votes each candidate received;

winner.

• the majority function majority that only discloses the
We consider a set I of voters, a subset H ⊆ I of honest
voters, a ballot box BB and a public bulletin board PBB. A
single-pass protocol πρ executes in three phases.

1) In the setup phase, the ballot box expects one message
from an administrator after which it may either transition
to the voting phase or abort.

2) In the voting phase, all parties may post messages to
the ballot box at any time; the ballot box decides to
accept or reject a message based on its current state and
a public algorithm. The ballot box stores all accepted
messages. Any party may ask to read the bulletin board
at any time; the ballot box replies by running a public
ﬁltering5algorithm (that we will call Publish) on its
current state and returns the result. Once the voting phase
is closed, the ballot box transitions to the result phase.

3) In the result phase, the ballot box is closed: it accepts no
more messages but can still be read. The administrator
computes the ﬁnal outcome r and a proof of valid
tabulation Π via a tallying procedure that may operate
on the ballot box.

4This is the policy in Estonian elections, for example.
5Filtering serves several purposes. In an election where only the last vote
from each voter counts, the ﬁlter may return only last ballot from each voter.
In Helios, there is a “short” board consisting only of hashes of the ballots;
this is sufﬁcient to check whether one’s ballot has been included and could
again be modelled as a ﬁlter. Such a ﬁlter could even be used as a defense
against ballot-copying, by not revealing the full ballots until the voting phase
is closed.

501501

Formally, a voting scheme V = (Setup, Vote, Publish,
Valid, Tally, Verify), for a list of voters I and a result function
ρ, consists of six algorithms with the syntax given below.
The new features of our formalisation are (1) the Publish
algorithm, which allows ballot boxes to store more information
than they display to the public (i.e. they may keep invalid
ballots internally but not display them) and (2) we let each
ballot have an explicit identity, which seems to be required
to model revoting policies (i.e. “the last ballot from each
voter counts”). Our model matches how e.g. Helios handles
identities in practice.

• Setup(λ) on input a security parameter λ outputs an
election public key pk and a secret tallying key sk.
We assume pk to be an implicit input of the remaining
algorithms.
• Vote(id, v) is used by voter id to cast his vote v ∈ V for
the election, as a ballot b ← Vote(id, v)6.
• Valid(BB, b) takes as input the ballot box BB and a ballot
b and checks it validity. It returns (cid:6) for valid ballots
and ⊥ for invalid ones (ill-formed, contains duplicated
ciphertext from the ballot box, etc.).
• Publish(BB) takes as input the ballot box BB and outputs
the public view PBB of BB, called public bulletin board.
• Tally(BB, sk) takes as input the ballot box BB and the
secret key sk. It outputs the tally r, together with a proof
of correct tabulation Π. Possibly, r = ⊥. This algorithm
might be (partially) in charge of implementing the revote
policy.

• Verify(PBB, r, Π) takes as input a public bulletin board
PBB, and a result/proof pair (r, Π) and checks whether
Π is a valid proof of correct tallying for r, it returns (cid:6)
if so, otherwise it returns ⊥.

Any voting protocol should ensure that if everyone acts cor-
rectly, the protocol indeed computes ρ on the votes submitted.
Formally, a voting scheme is correct if the following properties
hold with overwhelming probability. Let v1, . . . , vn ∈ V be
valid votes and id1, . . . , idn ∈ I be voter identities. We
consider an honest execution. Let (pk, sk) ← Setup(λ). Let
bi ← Vote(idi, vi) and BBi := [b1, . . . , bi] for all i. Then
honest ballots are valid, that is, Valid(BBi−1, bi) = (cid:6) for all i;
and the protocol computes the correct election result, that is, let
(r, Π) ← Tally(BBn, sk) then r = ρ((id1, v1), . . . , (idn, vn))
and Verify(Publish(BBn), r, Π) = (cid:6).
An important contribution of our work is an explicit for-
malisation of revote policies. Intuitively, there are two main
and distinct reasons for removing a ballot:

• Cryptographic cleaning: This deletion might be due to
the cryptographic implementation. In Helios for exam-
ple, a ballot may be removed because it is ill-formed
(e.g. invalid zero-knowledge proofs) or because it con-
tains a duplicated ciphertext, which may yield a privacy

6We do not include in our syntax, as most of the related work in the area,
the necessary algorithms that allow an election administrator to distribute
credentials among users, that will be in turn used to authenticate the voter to
the ballot box and cast a ballot.

breach [27]. In our formalism, this cleaning operation can
be taken care of either by the Valid predicate or by the
Tally function itself.

• Mandatory revote policy: This deletion might correspond
to the implementation of the “ideal” revote policy spec-
iﬁed by the result function ρ. A typical revote policy is
that, for each voter, only the last ballot shall be retained.

B. Tally uniqueness

We deﬁne tally uniqueness, a “minimal” property that any
veriﬁable system should satisfy. This property is of course not
mandatory for ballot privacy but we will use it in the next
sections to illustrate that some ballot privacy deﬁnitions are
incompatible with any veriﬁable system.
Intuitively, tally uniqueness of a voting scheme ensures
that two different tallies r (cid:8)= r
(cid:3) for the same board cannot
be accepted by the veriﬁcation algorithm, even if all
the
players in the system are malicious. The goal of the adversary
against tally uniqueness is to output a public key pk, a list of
legitimate public identities, a ballot box BB, and two tallies
r (cid:8)= r
(cid:3), and corresponding proofs of valid tabulation Π and
(cid:3), such that both pass veriﬁcation.
Π

(cid:3)

) ← A(1λ)

Experiment ExpuniqA (λ)
(cid:3)
(1) (pk, BB, r, Π, r
, Π
(2) PBB ← Publish(BB)
(3) if r
(4) Verify(PBB, r, Π) = Verify(PBB, r

(cid:3) (cid:8)= r and

return 1 else return 0

Fig. 1. Tally Uniqueness

(cid:3)

(cid:3)
, Π

) = (cid:6)

We deﬁne tally uniqueness by the experiment ExpuniqA in Fig-
ure 1. A voting scheme V has tally uniqueness if Succuniq(A)
is a negligible function for any PPT adversary A, where

Succuniq(A) = Pr

(cid:2)
ExpuniqA (λ) = 1

(cid:3)

Tally uniqueness is often considered as a requirement for

election veriﬁability [31], [32].

III. SURVEY AND ANALYSIS OF PREVIOUS GAME-BASED

COMPUTATIONAL VOTE PRIVACY DEFINITIONS

Game-based privacy [23], [24], [25], [12], [18], [19], [26],
[22], [20], [21] (our terminology) deﬁnitions require that it
should be hard for an adversary to win a game with a
challenger behaving in a fully speciﬁed manner, in the spirit
of traditional indistinguishability deﬁnitions for encryption. In
this section we review the most relevant ballot privacy deﬁni-
tions in the literature. We unveil that some previous deﬁnitions
present shortcomings that have gone unnoticed: either ballot
secrecy and election veriﬁability are incompatible [22], or
ballot secrecy does not guarantee that the choices of the voters
remain private once the election result is published [19], [21].
We also discuss known limitations of the approaches in [23],
[25], [24], [31], [33]. A summary of our ﬁndings is shown in
Table I, at the end of this section.

502502

is actually published,

Most existing deﬁnitions do not distinguish between the
ballot box and what
the bul-
letin board. Therefore, in this section, we implicitly assume
Publish(BB) = BB. Moreover, some deﬁnitions do not model
voters identiﬁers. By a slight abuse of notation, we may write
Vote(v) instead of Vote(id, v) when id is ignored.

i.e.

We formalize ballot privacy deﬁnitions by using two ballot
boxes BB0 and BB1, from which only one box will be visible
to the adversary. Of course, the adversary’s goal is to learn
which one of them is visible. For the uniformity of the
presentation, we write explicitly both ballot boxes in each of
the deﬁnitions that follow, even if some of them can be deﬁned
using only one ballot box (such as Deﬁnitions III-A and III-B).
We use the notation and some terminology that we ﬁx above
to discuss the existing game-based deﬁnitions. We proceed
following, roughly, the chronological order.

A. Ballot privacy for permutations of honest votes - PODC
1986 [23], STOC 1994 [25]

which states that “no party receives information which would
allow them to distinguish one situation from another one
in which two voters swap their votes”. Dreier, Lafoucarde
and Laknech have generalised the swap-equivalent symbolic
privacy deﬁnition to weighted votes [33].

However IND-BB privacy does not guarantee indistin-
guishability between different assignments that lead to the
same result but which are not equivalent permutation-wise.
For instance, consider the case where voters can give a
score of 0, 1, or 2 to a (single) candidate. It may be the
case that an attacker cannot distinguish the sequences of
votes [(ida, 0); (idb, 2)] from [(ida, 2); (idb, 0)] but could well
distinguish [(ida, 0); (idb, 2)] from [(ida, 1); (idb, 1)]. In fact,
this example serves as a simpliﬁed abstraction of actual voting
rules, such as those in the European Parliament elections in
Luxembourg [34] or the Swiss Government Federal Elections.
Deﬁnitions [23], [25], [31] do not capture these real cases.
B. Benaloh’s ballot privacy [24]

One ﬁrst deﬁnition of ballot privacy follows a simple idea:
an attacker should not notice if the votes of two voters
are swapped. More precisely, a coalition of voters should
not be able to distinguish when two honest voters id0, id1
vote respectively v0 and v1, from the case where they vote
respectively v1 and v0.

as follows:

Deﬁnition 1 (IND-BB): I is a list of voters. BB0, BB1 are
lists initialized at empty. The challenger starts by picking a
random bit β, and the adversary B = (B1,B2) is given access
to lists I and BBβ. The challenger runs the setup algorithm and
the keys (pk, sk) are created. The adversary B1 can repeatedly
query the oracle Ocast as follows:
• Ocast(id, b): runs bb (cid:9)→ bb(cid:10)b on ballot boxes BB0 and
BB1. (The expression bb(cid:10)b appends b to bb.)
The adversary can also query once an oracle OVoteIND(·,·)
• OVoteIND(id0, id1, v0, v1) : if vδ /∈ V for δ = 0, 1, halt.
Else, runs BB0 ← BB0(cid:10){Vote(id0, v0), Vote(id1, v1)} as
well as BB1 ← BB1(cid:10){Vote(id0, v1), Vote(id1, v0)}.
At some point, the adversary B1 asks to see the result.
The challenger computes (r, Π) ← Tally(BBβ, sk). Finally
the IND-BB adversary B2 outputs β
Formally, we say that a voting scheme V is IND-BB secure
if no PPT algorithm B can distinguish between the outputs in
the experiment just described for β = 0 and β = 1, i.e. for
any PPT adversary B,

(cid:3) as the guess for β.

(cid:4)(cid:4)(cid:4) Pr

(cid:2)
Expindbb,0

B

(cid:3)

(cid:2)

− Pr

(λ) = 1

Expindbb,1

B

(λ) = 1

(cid:3) (cid:4)(cid:4)(cid:4)

B

is negligible, where Expindbb,β

is the experiment deﬁned above.
The deﬁnition IND-BB can be seen as a generalization of
the private elections deﬁnition by Benaloh and Yung [23],
which was deﬁned only with respect to referendum elections,
so that only v0 = 0 and v1 = 1 were considered, and by
Benaloh and Tuinstra [25]. It also resembles the symbolic
vote privacy deﬁnition by Delaune, Kremer and Ryan [31],

503503

To cope with the aforementioned limitation, [24] has gener-
alized the previous deﬁnition to an arbitrary set of voters that
may vote arbitrarily provided that the tally corresponding to
honest voters remains unchanged.

The following deﬁnition is a restatement, using contem-
porary notation, of Benaloh’s privacy deﬁnition for voting
schemes [24].

Deﬁnition 2 (PRIV): BB0, BB1, V0, V1 are lists initialized
at empty. The challenger starts by picking a random bit β,
and adversary B = (B1,B2) is given access to list BBβ. The
challenger runs the setup algorithm and the keys (pk, sk)
are created. B1 on input pk is given access to oracles
Ovote(·),Oballot(·) as follows:
• Ovote(id, v0, v1) : runs BBδ ← BBδ(cid:10)Vote(id, vδ)),
Vδ ← Vδ(cid:10)(id, vδ)) for δ = 0, 1.
• Oballot(b): runs bb (cid:9)→ bb(cid:10)b on ballot boxes BB0, BB1
(that is, appends b to both boards).
At some point, B2 asks to see the tallying output. The chal-
lenger proceeds as follows: if ρ(V0) (cid:8)= ρ(V1), halts. Otherwise,
the challenger outputs (rβ, Πβ) ← Tally(BBβ, sk). Finally, B2
(cid:3) a the guess for β. Formally, we declare a voting
outputs β
scheme V Benaloh’s private if for any PRIV adversary B,
(cid:3) (cid:4)(cid:4)(cid:4)

− Pr
is the experiment deﬁned above.
The main drawback of Benaloh’s deﬁnition is that it restricts
the set of functions for which eventually a scheme could
be declared ballot private. To see this, let us assume that
the possible votes are {a, b} and consider the result function
majority. We assume that a wins in case of a tie. Let B
be an adversary that chooses to play the game such that
V0 = [(id1, a); (id2, a)] and V1 = [(id1, a); (id2, b)]. Clearly
majority(V0) = majority(V1) = a (V1 is a tie) so a wins).
Let now B cast a ballot query for vote b. Then r0 = a and
r1 = b and thus Succpriv(B) = 1, independently of the scheme
V.

is negligible, where Exppriv,βB

(cid:4)(cid:4)(cid:4) Pr

Exppriv,0B

Exppriv,1B

(λ) = 1

(λ) = 1

(cid:2)

(cid:3)

(cid:2)

C. Ballot privacy ESORICS 2011 - CCS 2012 [12], [18], [17]
More recently, a deﬁnition has been proposed, that aims
to deal with arbitrary result functions. It is inspired by cryp-
tographic security for encryption and roughly says that an
attacker should not be able to tell whether he is seeing the
real board or a fake board where all (honest) voters vote for
the same arbitrary dummy vote. Of course, the tally itself is
always performed on the real ballot box.
Deﬁnition 3 (BPRIV1): BB0, BB1, BB(cid:3)
are lists initialized
at empty. A distinguished vote value  ∈ V is chosen by
the challenger. The challenger starts by picking a random bit
β, and the adversary B = (B1,B2) is given access to list
BBβ. The challenger runs the setup algorithm to create keys
(pk, sk). Next, the adversary B1 can query oracles Ovote(·)
and Oballot(·) as follows:
• Ovote(v) : computes b0 := Vote(v) and b1 := Vote();
next runs BBβ ← BBβ(cid:10)bβ and BB(cid:3) ← BB(cid:3)(cid:10)b0.
• Oballot(b): runs bb ← bb(cid:10)b on inputs BBβ, BB(cid:3)

At some point, the adversary B2 asks to see the tallying output.
The challenger obtains (r, Π) ← Tally(BB(cid:3)
, sk) and returns r
to the adversary. Finally the BPRIV1 adversary B2 outputs β
(cid:3)
as its guess for β.
Formally, we say that a voting scheme V is BPRIV1 secure
if no PPT algorithm B can distinguish between the outputs in
the previous experiment for β = 0 and β = 1, i.e. for any
PPT adversary B,

.

(cid:2)

(cid:4)(cid:4)(cid:4) Pr

Expbpriv1,0

B

(λ) = 1

Expbpriv1,1

B

(λ) = 1

(cid:3)

(cid:2)

− Pr

(cid:3) (cid:4)(cid:4)(cid:4)

B

is the experiment deﬁned

is negligible, where Expbpriv1,β
above.
In the BPRIV1 deﬁnition, tally is executed either over the
faithful ballot box (namely, BB), or a fake box (namely, BB(cid:3)
),
containing fake votes  for honest voters. A limitation of
this deﬁnition is that the adversary is not allowed to see the
auxiliary data Π. Indeed in most cases, if the adversary can
see Π (e.g. a proof of correct tally of the ballot box), he would
be immediately able to tell whether he has seen the real or the
fake board. Therefore BPRIV1 does not fully model veriﬁable
voting protocols such as Helios or Civitas.

Recently, Cuvelier, Pereira and Peters [26] proposed a
variant of the BPRIV1 setting/deﬁnition to capture ballot
privacy even in the presence of computationally unbounded
adversaries, and which is named perfectly private audit trail
(PPAT). Similarly to BPRIV1, the deﬁnition PPAT is limited in
the sense that an adversary is not allowed to see the auxiliary
data Π.

D. Ballot privacy - ASIACRYPT 2012 [19], [20]

A variation of the BPRIV1 deﬁnition has been proposed,
that we name as BPRIV2 privacy. Its goal is to be able to
fully model veriﬁable voting protocols where the tally does not
produce just a result but also proofs of correct tally. Intuitively,
to avoid that the adversary immediately wins the game because
of the auxiliary data, this data needs to be simulated on the

504504

fake board. This deﬁnition makes therefore use of a simulator
SimProof that simulates the auxiliary data of the tally when
the adversary is not given the real board.

Deﬁnition 4 (BPRIV2): BB0, BB1, L are lists initialized
at empty. The challenger starts by picking a random bit
β, and the adversary B = (B1,B2) is given access to list
BBβ. The challenger runs the setup algorithm to create keys
(pk, sk). B1 is given access to oracles Ocorrupt,OvoteLR
and Oballot(·) as follows:

• OvoteLR(id, v0, v1) : if vγ /∈ V for γ = 0, 1, halts. Else,
runs BBγ ← BBγ(cid:10)Vote(id, vγ)) for γ = 0, 1 and sets
L ← L ∪ {id}, meaning that id has already voted.
• Ocast(id, b): if id ∈ L (a honest user already cast a
ballot), halts. Else runs BBβ ← BBβ(cid:10)b.
At some point, B2 asks to see the tallying output. The
the challenger
challenger proceeds as follows:
outputs (r, Π) ← Tally(BB0, sk). But if β = 1, the challenger
) ← Tally(BB0, sk) and Π ← SimProof(BB0, BB1,
†
sets (r, Π
pk, info), where info contains any information known to the
challenger. The challenger outputs (r, Π).
Finally the BPRIV2 adversary B2 outputs β
(cid:3) a the guess for
β.
We say that a voting scheme V is BPRIV2 secure if no
PPT algorithm B can distinguish between the outputs in the
experiment just described for β = 0 and β = 1, i.e. for any
PPT adversary B, there exists a simulator SimProof such that

if β = 0,

Expbpriv2,0

B

(λ) = 1

Expbpriv2,1

B

(λ) = 1

(cid:3)

(cid:2)

− Pr

(cid:3) (cid:4)(cid:4)(cid:4)

is negligible, where Expbpriv2,β

B

is the experiment deﬁned

(cid:2)

(cid:4)(cid:4)(cid:4) Pr

above.

Unfortunately, allowing the auxiliary data to be simulated
actually weakens too much the deﬁnition of ballot privacy. We
show next that BPRIV2 privacy declares as private, protocols
that reveal exactly how voters voted in the tabulation proof Π.
Such protocols should clearly not be declared private.

BPRIV2 fails to ensure ballot privacy: Let V(cid:3) be any
BPRIV2 secure scheme. We assume that ballots of V(cid:3) can
be extracted, that is, we assume a function Extract(sk, b) that
returns the vote v corresponding to ballot b. For example, if
b contains the encryption of v then Extract is simply the
decryption function. Let Leak(V(cid:3)
) be the scheme obtained
from V(cid:3) such that the tally now outputs the correspondence
between ballots and votes. Formally, Leak(V(cid:3)
) is obtained
from V(cid:3) by changing Tally(cid:3)
to Tally, Verify as follows:
• (r, Π) ← Tally(sk, BB), where (r, Π
(cid:3)
(sk, BB)
vb ←
and Π ← Π
Extract(sk, b);
• Verify(PBB, r, Π) parses Π as Π
outputs Verify(cid:3)

) ← Tally(cid:3)
b∈BB, where
(cid:3) ||{( b, vb )}

, Verify(cid:3)
(cid:3) ||{( b, vb )}

We write V := Leak(V(cid:3)
). Intuitively, it is easy to see that
V satisﬁes BPRIV2 although it is not private. Indeed, the
simulator SimProof knows all the OvoteLR(id, v0, v1) queries

(cid:3)
(PBB, r, Π

b∈BB and

).

made by the adversary. It may therefore pretend that any ballot
b corresponding to a query OvoteLR(id, v0, v1) corresponds
to v0 even when it corresponds to v1 (when β = 1). This
argument is worked out in detail in Appendix IX.

E. Ballot secrecy - ESORICS 2013 [22]

Another deﬁnition, called IND-SEC, has been recently been
proposed [22] to ﬁx the privacy breach of the deﬁnition
BPRIV2. This deﬁnition can be seen as a combination of
the BPRIV2 [19], [20] and the Benaloh’s [24] deﬁnitions.
Indeed, the IND-SEC game is intuitively deﬁned as follows.
The honest voters vote for an arbitrary sequence of votes V0
in the ﬁrst board and an other arbitrary sequence of votes
V1 in the second board. If the two sequences coincide when
viewed as multisets, then the real tally is disclosed (as in
Benaloh’s [24] deﬁnition). If the two multisets differ, the tally
is always performed on the ﬁrst ballot box (even when the
adversary has seen the second ballot box).

Deﬁnition 5 (IND-SEC): BB0, BB1, V0, V1 are lists initial-
ized at empty. The challenger starts by picking a random bit
β, and the adversary B = (B1,B2) is given access to list BBβ.
The challenger runs the setup algorithm and the keys (pk, sk)
are created. On input pk the adversary B1 can query oracles
OvoteLR(·,·) and Oballot(·) as follows:
• Ovote(v0, v1) : runs BBγ ← BBγ(cid:10){Vote(vγ)}, and
updates Vγ ← Vγ ∪ {vγ} for γ = 0, 1.
• Oballot(b): runs bb (cid:9)→ bb(cid:10)b on ballot boxes BB0, BB1.
At some point, B2 asks to see the tallying output. The

(cid:8)= V1
Tally(BB0, sk).

challenger proceeds as follows:
• If V0 = V1 (as multisets) the output is set to be that of
(r, Π) ← Tally(BBβ, sk).
(r, Π) ←
• If V0
the challenger outputs
Finally the IND-SEC adversary B2 outputs β
(cid:3) a the guess
for β.
Formally, we say that a voting scheme V has IND-SEC
secrecy if no PPT algorithm B can distinguish between the
outputs in the experiment just described for β = 0 and β = 1,
i.e. for any PPT adversary B,
(cid:3)

(cid:2)

(cid:2)

(cid:3) (cid:4)(cid:4)(cid:4)

(cid:4)(cid:4)(cid:4) Pr

Expindsec,0

B

(λ) = 1

Expindsec,1

B

(λ) = 1

− Pr

is negligible, where Expindsec,β

B

is the experiment deﬁned

above.
IND-SEC and Tally Uniqueness are Incompatible.: While
IND-SEC declares the Leak(V) voting scheme insecure, it
turns out that IND-SEC secrecy and veriﬁability are incom-
patible properties. In other words, any veriﬁable protocol will
be declared not private by IND-SEC.
Let V be a (correct) voting scheme with tally uniqueness
for a non-trivial result function ρ. We describe next a IND-
SEC adversary B that has advantage negligibly close to 1/2
against V in the IND-SEC game. More precisely, for any IND-
SEC adversary B there exists a tally uniqueness adversary B(cid:3)
such that Succbpriv1(B) ≥ 1 − Succuniq(B(cid:3)
). Therefore both

(cid:3)

(cid:3)

advantages can not be negligible at the same time, and thus
IND-SECl and tally uniqueness are incompatible properties.
The adversary B proceeds as follows. It chooses votes v, 
such that ρ(v) (cid:8)= ρ() and it makes a single query Ovote(v, ),
which causes b0 := Vote(v) and b1 := Vote() to be created.
:= 0 if Verify(BBβ, ρ(v), Π) = (cid:6),
Then B sets its guess β
where Π is computed by the IND-SEC challenger as (r, Π) ←
Tally({b0}, sk); otherwise sets β
:= 1. The claim follows
from the following facts:

• Verify({b0}, ρ(v), Π) = (cid:6), since V is correct;
• Verify({b1}, ρ(), Π1) = (cid:6), where Π1 is not explicitly
known but it is deﬁned by (r1, Π1) ← Tally({b1}, sk).
This holds since V is correct;
• Verify({b1}, ρ(v), Π) = ⊥ with overwhelming proba-
bility. Indeed, since V has tally uniqueness, and given
that Verify(Publish({b1}), ρ(), Π1) = (cid:6), the equation
Verify(Publish({b1}), ρ(v), Π) = (cid:6) for ρ(v) (cid:8)= ρ() is
satisﬁed only with negligible probability.

The latter implies in particular that Helios,

in any of
its known ﬂavours, cannot be both IND-SEC private and
veriﬁable.

F. Ballot privacy for restricted adversaries - PKC 2013 [21]
Another deﬁnition has been recently proposed by Chase et
al. [21]. As for the previous IND-SEC deﬁnition, the adversary
triggers honest voters, providing a left and right votes for each
voter. The tally is performed on the visible ballot box, that is,
the one the adversary sees (no simulation). However, if the
result announced differs depending on whether β = 0 and
β = 1, then the adversary loses the game.

Deﬁnition 6 (RPRIV): BB0, BB1, V0, V1 are lists initialized
at empty. The challenger starts by picking a random bit
β, and the adversary B = (B1,B2) is given access to list
BBβ. The challenger runs the setup algorithm and the keys
(pk, sk) are created. B1 on input pk is given access to oracles
Ovote(·),Oballot(·) as follows:
• Ovote(id, v0, v1) : runs BBδ ← BBδ(cid:10)Vote(id, vδ)),
Vδ ← Vδ ∪ {vδ} for δ = 0, 1.
• Oballot(id, b): runs bb (cid:9)→ bb(cid:10)b on ballot boxes BB0, BB1.
At some point, B2 asks to see the tallying output. The chal-
lenger computes (r0, Π0) ← Tally(BB0, sk) and (r1, Π1) ←
Tally(BB1, sk). If r0 (cid:8)= r1, the adversary loses. Otherwise, the
challenger replies (rβ, Πβ) to B2. Finally, B2 outputs β
(cid:3) a the
guess for β. Formally, we declare a voting scheme V RPRIV
private if for any adversary B the advantage
(cid:4)(cid:4)(cid:4) Pr
(cid:3) (cid:4)(cid:4)(cid:4) is neg-
Exprpriv,1

− Pr

Exprpriv,0

(λ) = 1

(λ) = 1

(cid:3)

(cid:2)

(cid:2)

B

B

ligible, where Exprpriv,β

B

is the experiment above.

This deﬁnition fails to capture replay attacks, whereby a
malicious voter replays a previously ballot cast by a honest
voter. It is known that replay attacks violate ballot secrecy.
Indeed, consider a referendum election with three voters,
namely, Alice, Bob, and Mallory: if Mallory replays Alice’s
ballot without being detected or rejected, then Mallory can
reveal Alice’s vote by observing the election outcome and

505505

checking which candidate obtained at least two votes. This
attack was successfully implemented against Helios 2.0 by
Cortier and Smyth [27], [28], who also showed this constitutes
a privacy threat in real scenarios by studying the potential
impact on French legislative elections.

Let us argue that RPRIV does not capture replay attacks.
Consider a protocol where voters submit their encrypted votes
on a secure channel,
to an election server that publishes
the encrypted votes on the board (thus allowing ciphertext
copying). Assume moreover that only the result of the election
is published, once the voting phase is closed. Clearly, this
protocol is subject to replay attacks and therefore does not
ensure privacy. However, it is declared private by RPRIV.

Indeed, the only information that the adversary obtains is
the encrypted ballots and the result of the election. Since the
encryption scheme is IND-CPA, the adversary does not get any
information from the encrypted votes, so it must use the result
itself to try to win the RPRIV distinguishing game. Moreover,
by deﬁnition of this game, the adversary can see the result
only in cases where both ballots boxes yield identical results.
Therefore the adversary cannot win the game, since encrypted
votes without the result give no information, and the result
does not help distinguishing the boxes, since it is identical in
both boxes. For similar reasons, RPRIV would declare Helios
2.0 private.

IV. BALLOT PRIVACY: A COMPREHENSIVE

CRYPTOGRAPHIC GAME-BASED DEFINITION FOR VOTE

PRIVACY

Based on our ﬁndings on existing deﬁnitions of privacy, we
propose a new deﬁnition that avoids the aforementioned issues.
As in [12], [18], [19], [17], [22], we deﬁne ballot secrecy for
a voting scheme V in terms of a game between a challenger
and an adversary.

We give our new security notion in two steps. We start with
a vanilla variant that reﬂects the core deﬁnitional ideas behind
our notion and allows a focused discussion on its features.
Then, we explain how to incorporate in the deﬁnition the
existence of global setup assumption (e.g. random oracles or
common reference strings) as used by many existing voting
protocols.
Recall

that ballot privacy attempts to capture the idea
that during its execution a secure protocol does not reveal
information about the votes cast, beyond what is unavoidably
leaked (e.g. what
the result of the election leaks). As in
previous deﬁnitions, we formalize this idea via an adversary
that attempts to distinguish between two worlds. In the ﬁrst
world the adversary has (indirect) access to a ballot box that
contains ballots created by honest users as well as adversarial
ballots and gets to see the result corresponding to the ballot
box. In the second world the adversary sees a fake board
instead of the real one, yet gets to see the result of the election
as tallied on the real ballot box. Since we model explicitly
the additional information that the tally may include besides
the result (e.g. a proof of correct tally), we require that this
information does not reveal any information either: we require

via

the

We

formalize

discussion

the existence of a simulator that can “fake” the additional
information corresponding to the real result, but with respect
to the fake ballot box.
this

experiments
Expbpriv,βA,V (λ) deﬁned in Figure 2. In these games BB0, BB1
are ballot boxes that start out empty. Ballot box BB0
corresponds to the real election (that will be tallied). The
adversary gets access to BB0 in the ﬁrst game and access to
BB1 (a fake ballot box) in the second game. The experiment
starts with generating long term keys sk and pk;
the
adversary A is given the public key and access to the oracles
that we describe below and formalize in Figure 2.
Oboard: This models the ability of the adversary to see the
publishable part of the ballot box, i.e. the bulletin board.
The oracle returns Publish(BBβ).
OvoteLR: The left-right oracle OvoteLR takes two potential
votes (v0, v1) for user id, produces ballots b0 and b1 for
these votes and places them on the ballot box (one on
BB0 and one one on BB1), provided that bβ is valid with
respect to BBβ.
Ocast: This oracle allows the adversary to cast a ballot b on
behalf on any party. If the ballot is valid with respect to
BBβ, it is placed on both ballot boxes.
Otally: This oracle allows the adversary to see the result of the
election. In both worlds the result is obtained by tallying
BB0. In the ﬁrst experiment the additional information
calculated by the tally is given to the adversary, whereas
in the second experiment the additional information is
simulated.
The adversary can call oracles OvoteLR,Ocast,Oboard in
any order, and any number of times. Finally, A can call Otally
once; after it receives the answer to his query A returns a
guess bit on the value of β. This bit is the result returned by
the game.
Deﬁnition 7 (BPRIV): Consider a voting scheme V =
(Setup, Vote, Valid, Publish, Tally, Verify) for a set I of voter
identities and a result function ρ. We say the scheme has
ballot privacy if there exists an algorithm SimProof such
that no efﬁcient adversary can distinguish between games
Expbpriv,0
B,V (λ) deﬁned by the oracles in Figure
2, that is for any efﬁcient algorithm A
(cid:2)

B,V (λ) and Expbpriv,1

Expbpriv,0A,V (λ) = 1

Expbpriv,1A,V (λ) = 1

(cid:2)

(cid:4)(cid:4)(cid:4) Pr

(cid:3)

− Pr

(cid:3) (cid:4)(cid:4)(cid:4)

is negligible in λ.

A. Extension of the deﬁnition to setup assumptions

Most voting protocols rely on non-interactive zero knowl-
edge (NIZK) proofs to enforce honest behaviors for the parties
involved, yet such proofs require some setup assumptions (like
the CRS or the RO model). To be able to analyze these
protocols we need to extend our deﬁnition to account for such
setups. While it would be simple enough to provide a deﬁnition
of ROM-BPRIV or CRS-BPRIV, we would like our deﬁnition
to abstract away details of any particular model of zero-
knowledge as far as possible. Finding a truly model-agnostic

506506

ESOR.11

[12]

ESOR.09
[31], [33]

game-based
notion
detects leaky
revote policies
Helios 2.0 is
not private
protects ag. vote
comparisons
compatible with
tally uniqueness
admits duplicate
weed before tally
admits duplicate
weed inside tally
detects leaky
tally proofs
revoting allowed

models partially
hidden board
admits result
functions w/o
partial tallying

(cid:2)



(cid:2)

(cid:2)

?

(cid:2)



?

(cid:2)



(cid:2)





(cid:2)



(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)





PODC86

STOC94

[23]

[25]
(cid:2)



(cid:2)



(cid:2)





(cid:2)

(cid:2)





Benaloh

[24]

ESOR.13

[22]

ASIA. 12

[19]

PKC13

[21]

S&P 10

[10]

CCS12

[17]

ACNS04

[13]

Ours
Sec. IV

(cid:2)



(cid:2)

(cid:2)

(cid:2)





(cid:2)

(cid:2)





(cid:2)



(cid:2)

(cid:2)







(cid:2)

(cid:2)
/(cid:2)
(cid:2)

(cid:2)



(cid:2)

(cid:2)

(cid:2)

(cid:2)





(cid:2)



(cid:2)

(cid:2)







(cid:2)

(cid:2)



(cid:2)

(cid:2)



?



(cid:2)

(cid:2)



(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)







?

(cid:2)

(cid:2)

(cid:2)

(cid:2)



(cid:2)

?



(cid:2)



?

(cid:2)

(cid:2)

(cid:2)

?

?

(cid:2)







(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

(cid:2)

SUMMARY OF OUR SURVEY. (cid:2)= DEFINITION SATISFIES A DESIRABLE PROPERTY (THE MORE (cid:2)S A DEFINITION HAS, THE BETTER); = DEFINITION DOES

NOT SATISFY THE PROPERTY; ? = RESULT IS NOT ADDRESSED IN THE DEFINITION OR WE COULD NOT ESTABLISH IT FROM THE REFERENCE.

TABLE I

notion of zero-knowledge is an open research problem so we
choose to sketch an extension of ballot privacy that keeps the
setup assumption abstract thus sacriﬁcing some precision for
greater generality.

We also note that this extension is needed, not only to allow
for the analysis of more schemes, but also because in its vanilla
format BPRIV security would be too strong: the existence of
a simulator that can fake the proof without using a global
setup, would mean that secure schemes would not satisfy tally
uniqueness.

A global setup consists of a set public parameters and
algorithms that can be accessed by all parties, and in particular
by the adversary. These parameters are initialized at
the
beginning of the execution. We write GlobalSetup.init for
the algorithm that initializes the parameters of the global setup,
and write AGlobalSetup to indicate that algorithm A can access
GlobalSetup. For example, in the CRS case GlobalSetup.init
would select a random string (of some ﬁxed length) and
GlobalSetup simply makes this string available to all parties.
In the random oracle model GlobalSetup consist of a truly
random function to which parties only have oracle access.

The power of setup assumptions comes from the ability to
generate simulated setups, indistinguishable to an adversary
from a normal one. The simulated setup however, grant to
a simulator additional powers that are useful
in crafting
reduction proofs. For example in the CRS model the fake
setup would consist of a CRS indistinguishable from an

honestly generated one, but which comes with a trapdoor that
allows, for example, to produce valid looking proofs for false
statements. In the random oracle model the oracle is under the
control of a simulator who can “program” (i.e. ﬁx) its output
on some values of interest. Naturally, this programming should
be such that the adversary cannot distinguish the simulated or-
acle from a truly random one. We write SimGlobalSetup for a
simulated setup and SimGlobalSetup.init for its initialization
algorithm. When SimGlobalSetup is used, some of the parties
in the system may have access to its associated trapdoor (like
in the CRS setting), or control it in some other way (like in
the RO model).

The extension of BPRIV to global setups is as follows. In
both of the games we initialize a real and a fake setup via
GlobalSetup.init and SimGlobalSetup.init. When β = 0,
that is in the game that corresponds to the real execution
the adversary has access to GlobalSetup; in this experiment
SimGlobalSetup does not play any role – we simply initialize
it for simplicity and for ease of comparing with the other
experiment. When β = 1,
the adversary has access to
SimGlobalSetup. However, the fake global setup is under the
control of SimProof: all calls to SimGlobalSetup are sent to
SimProof which is in charge of answering them. Importantly,
in this experiment the result is produced by tallying the “real”
ballot box (BB0) and using the real GlobalSetup.

We provide a fully worked out instantiation of our deﬁnition
in the ROM in the full version of this paper [35], since this is

507507

Expbpriv,βA,V (λ)
(pk, sk) ← Setup(1k)
d ← AO
Output d

(pk)

OvoteLR(id, v0, v1)

Let b0 = Vote(id, v0) and b1 = Vote(id, v1).
If Valid(BBβ, bβ) = ⊥ return ⊥.
Else BB0 ← BB0(cid:10)b0 and BB1 ← BB1(cid:10)b1

If Valid(BBβ, b) = ⊥ return ⊥.
Else BB0 ← BB0(cid:10)b and BB1 ← BB1(cid:10)b.

Ocast(b)

Oboard()

return Publish(BBβ)

Otally() for β = 0

(r, Π) ← Tally(BB0, sk)

return (r, Π)

Otally() for β = 1

(r, Π) ← Tally(BB0, sk)
(cid:3) ← SimProof(BB1, r)
Π
return (r, Π

)

(cid:3)

Fig. 2. In the experiments Expbpriv,βA,V (λ) deﬁned above for β = 0, 1 adversary
A has access to the set of oracles O = {Ocast, Oboard, OvoteLR, Otally}.
The adversary is allowed to querry Otally only once. For β = 1 the
experiment also depends on SimProof – we do not show the dependence
explicitly to simplify notation.

the model used for Helios.

B. Strong Consistency

The ballot privacy deﬁnition BPRIV strongly relies on
a split between the result r and the auxiliary data Π re-
turned by the tally. This split should be meaningful, that is
r should correspond to the expected result and should not
contain hidden auxiliary data. To enforce this, we introduce
a companion deﬁnition of BPRIV, called strong consistency,
that has two main goals. Firstly, it ensures that the result
always corresponds to the result function applied to the votes,
and nothing more. Secondly, it controls the damages that an
intentionally leaky revote policy could implicitly cause while
tallying. In fact, since tallying takes as inputs the ballot box
and the secret key, this allows a malicious designer/election
administrator to implement a leaky revote policy. We limit the
damages of such a behaviour by asking the voting scheme to
satisfy a stronger correctness property, that we name strong
consistency.

Intuitively, strong consistency guarantees that

the Tally
algorithm behaves like ρ except possibly on invalid ballots.
(Note that BPRIV is independent of ρ.) We formalize the
requirement by requiring the existence of an “extraction”
algorithm which, with the help of the secret key, can determine
for each ballot the underlying vote and the identity/identiﬁer
with which it was created or ⊥ if the ballot is somehow invalid.
We require that from an honestly created ballot the extraction
algorithm works as desired. Using the extraction algorithm

508508

we can capture the intuition that the Tally algorithm works as
expected: we demand that the result reported for some valid
ballot box BB by Tally is the same as the result function
applied to the votes that underlie the ballots on BB, as deﬁned
using the extraction algorithm.
Deﬁnition 8 (Strong consistency): A scheme V =
(Setup, Vote, Valid, Publish, Tally, Verify) relative to a result
∗ → R has strong consistency if there
function ρ : (I × V)
exist

• an extraction algorithm Extract that takes as input a secret
key sk and a ballot b and outputs (id, v) ∈ I × V or ⊥;
• a ballot validation algorithm ValidInd that takes as input
the public key of the election pk and a ballot b and outputs
(cid:6) or ⊥;

which satisfy the following conditions:

1) For any (pk, sk) that are in the image of Setup and
for any (id, v) ∈ I × V if b ← Vote(pk, id, v) then
Extract(sk, b) = (id, v) with overwhelming probability.
2) For any (BB, b) ← A, Valid(BB, b) = (cid:6) implies
ValidInd(b) = (cid:6).

3) Consider an adversary A which is given pk and consider

the experiment:
Exps-cons

A,V (λ)
(pk, sk) ← Setup(λ);
BB ← A
(r, Π) ← Tally(sk, BB)
If r (cid:8)= ρ(Extract(sk, b1), . . . , Extract(sk, bn))
Then return 1 Else return 0

Above we consider only adversaries A that return BB of the
form [b1, . . . , bn] such that ValidInd(bi) = (cid:6) for i = 1..n. We
require that the probability Pr[Exps-cons
A,V (λ) = 1] is negligible
in the security parameter.

In case the result function ρ does not use voter identiﬁers,

that is

(cid:3)
1, v1), . . . , (id

(cid:3)
n, vn))

ρ((id1, v1), . . . , (idn, vn)) = ρ((id

(cid:3)
j, then the function Extract is not required
for any vj, idj, id
to output the voter identiﬁer.

Informally, strong consistency prevents an adversary from
encoding instructions in her own ballots, causing the tallying to
leak information on the honest votes or prevent the validation
of honestly generated ballot boxes. Indeed,
the extraction
algorithm works “locally” on each ballot so it cannot respond
to instructions encoded in one ballot on how to treat another
ballot or to return no result. This is not
the
tallying algorithm cannot be aware of adversarial instructions
— indeed, it is free to write what it likes to the auxiliary data.
It is only the result that is protected by strong consistency.
The rest is handled by the BPRIV deﬁnition.

to say that

Just like in the case of the ballot privacy deﬁnition, we
started with a vanilla variant of strong consistency that does
not account for global setups. The deﬁnition above can be
extended to this more realistic setup by providing to adversary

A and to the Tally algorithm access to GlobalSetup initialized,
as usual, via GlobalSetup ← GlobalSetup.init.
C. Strong Correctness

This notion requires a strong independence relation between
honestly created ballots and the ballot box (and the global
setup) in the voting scheme, which we capture by requiring
that an honestly created ballot is valid, even with respect to
an adversarially created ballot box.

Deﬁnition 9 (Strong correctness): Consider an adversary A
against π that takes as input pk and has access to a global
setup GlobalSetup generated as expected. Then,
Pr[(id, v, BB) ← A(pk); b ← Vote(id, v) : Valid(b, BB) (cid:8)= (cid:6)]
is negligible. The probability is over the coins used by the
adversary and Vote, but also over the coins used in the
generation of pk.

D. Necessity of Strong Consistency and Strong Correctness

that

Let us see an example of a voting scheme that satisﬁes
implements a leaky revote policy while
BPRIV but
tallying. Let V be a BPRIV voting scheme for the multiset
result function (i.e. ρ(v1, . . . , vn) outputs {v1, . . . , vn} viewed
as a multiset), such that Publish(BB) = BB, Π = ∅ (i.e.
there are no proofs of correct tallying), Verify(PBB, r,∅) = (cid:6)
(i.e. since there are no tallying proofs, we accept any result
published by the election administrator) and V = {0, 1}. To
simplify further, let us assume that V only allows single voting
(i.e. voters cannot revote).
We build a voting scheme V(cid:3) that inherits BPRIV privacy
from V, but which possibly reveals how the ﬁrst voter voted,
and thus it is, intuitively, not ballot private. V(cid:3) is obtained by
replacing algorithm Tally by Tally(cid:3)
(BB, sk)
ﬁrst checks whether the ﬁrst ballot in BB contains a 1-vote. If
so, removes this ballot from BB, and let BB(cid:3)
be the resulting
ballot box. Finally, outputs Tally(BB(cid:3)
, sk). Let us sketch a
proof that V(cid:3) is BPRIV.

as follows: Tally(cid:3)

In the ﬁrst place, let BB0 contain in its ﬁrst entry a 1-vote,
while BB1 contains in its ﬁrst entry a 0-vote. If β = 0, the
adversary sees ballot box BB0. The output of tallying in this
(BB0, sk) = Tally(BB0 \{b1}, sk). If β = 1, the
case is Tally(cid:3)
adversary sees ballot box BB1, but still sees the same tallying
output Tally(cid:3)

(BB0, sk).

In the second place, let us consider the complementary
case where BB0 contain in its ﬁrst entry a 0-vote, while
BB1 contains in its ﬁrst entry a 1-vote. If β = 0,
the
adversary sees board BB0. The output of tallying in this
case is Tally(cid:3)
(BB0, sk) = Tally(BB0, sk). If β = 1, the
adversary sees board BB1, but still sees the same tallying
output Tally(cid:3)
Finally, if BB0 and BB1 both contain ballots at their ﬁrst
entry for the same vote v ∈ {0, 1}, then Tally(cid:3)
does not
help distinguishing when compared to the original Tally.
Thus, for every adversary A(cid:3) against BPRIV of V(cid:3)
there
exists an adversary A against BPRIV of V with roughly
the same distinguishing advantage. If V is BPRIV, so is V(cid:3).

(BB0, sk).

Unfortunately, V(cid:3) reveals how a honest voter voted, since the
output of Tally tells whether the ﬁrst ballot on BB contains
the vote 1.

Our solution to defeat these leaky revote policies embedded
in Tally is to ask for strong consistency. Indeed, it is easy to see
that V(cid:3) does not satisfy strong consistency. Note that whenever
the ﬁrst ballot of BB is a 1-vote, then running Tally and
running the alternative tally procedure using Extract results
in different outputs.
Regarding the need for strong correctness, let us assume a
BPRIV voting scheme V(cid:3). We build a new voting scheme V
such that ballots b in V are obtained by appending a t-bit to
(cid:3)). The vote algorithm
ballots b
in V just appends a 0-bit to the result of the vote algorithm in
V(cid:3). Tally in V just drops the appended bit from any ballot b
and next applies the tally from V(cid:3). Valid(BB, b) is deﬁned as
follows:

(cid:3) in V(cid:3) (say at the beginning of b

• if there is any ballot in BB starting with 1-bit, rejects b
• otherwise, drops the the appended bit from any ballot
in BB and from b, let BB(cid:3)
(cid:3) be the corresponding
outputs, and computes Valid(cid:3)
(cid:3)
, b
V is BPRIV but not strongly correct. In particular, a
malicious voter can cause votes from honest voters to be
rejected. Actually, this remark also applies to other privacy
deﬁnitions, such as IND-BB.

and b
(BB(cid:3)

)

E. Revote policies

Formalizing revote policies has been rarely done in the
literature on foundations of electronic voting. Previously,
revote policies were considered as an external component of
the voting scheme. Roughly speaking, one used to proceed
as follows. Firstly the cryptographic workﬂow of the voting
protocol is described, next its ballot privacy is proven, and then
the revote policy is decided at the implementation level by
the election administrator. The bottom line is that reasonable
revote policies will not
the ballot privacy of the
underlying voting scheme, so they can chosen independently
of the scheme (we just need to make sure that the protocol
implements the given revote policy).

impact

One of our ﬁndings is that a good ballot privacy deﬁnition
must restrict
the class of revote policies allowed. This is
because the algorithms implementing a revote policy might
need the secret key as an input — some voting schemes,
such as JCJ/Civitas, need the election secret key to implement
revote. This opens the door to intentionally leaky revote
policies, which could use this secret key to gain information
on voters’ votes.

In Civitas [30], ballots with invalid credentials are removed
based on a plaintext-equivalence test that uses some trapdoor
unavailable to the ballot privacy attacker. This can be captured
in a strongly consistent way by including this operation in
ρ. Given a ballot b that contains a vote v and a credential
cred, the Extract function should return both cred and v.
Then ρ should remove any vote that corresponds to an invalid
credential.

509509

Other schemes also require the election secret key to be
able to clean the ballots. Sometimes, (part of) the private key
is needed to determine the validity of ciphertexts, e.g., when
Cramer-Shoup encryption is used [36]. This can be captured by
having Extract performing the appropriate tests. In some other
cases, e.g., in mixnet-based schemes, the validity of ballots
can only be determined even later, after full decryption. We
also include this cleaning operation in ρ: whenever ρ sees an
invalid vote v, then this vote shall be removed. In this way,
Tally remains strongly consistent.

V. A SIMULATION-BASED MODEL OF BALLOT PRIVACY
In this section we deﬁne an ideal functionality for voting
protocols, in the spirit of Groth [13] and de Marneffe et
al. [14], but handling voters that submit multiple votes. The
inspection of this functionality should make it obvious to the
reader that, if a trusted party were available to provide its
service, then we would have a voting system offering all
expected privacy guarantees and that all adversaries against
that functionality are harmless.

We show that any voting scheme satisfying BPRIV and
strong consistency offers at least as much7 privacy guarantees
as this ideal functionality. This will be demonstrated using
the traditional real-world/ideal world paradigm: we show that
anything that can be done by an adversary against the voting
protocol can also be done by an adversary interacting with the
ideal functionality. Since we know that any adversary against
the ideal functionality is harmless, this shows that the real
protocol adversary must be harmless too. Our treatment leaves
out many technical details of the execution model and focuses
on the crucial part. We expect that it could be cast formally in
traditional security frameworks in which universal composition
theorems hold, even though we are not concerned with any
such composition result here.

We start by describing our ideal world setting, then the real
world one, and eventually show the implications of our game-
based security deﬁnitions.

A. An ideal functionality for voting
identities I and a result function ρ : (I × V)
functionality has a simple behavior:

We describe an ideal voting functionality for a set of
∗ → R. This

1) It ﬁrst expects to receive one or more votes, both from
honest and from corrupted voters. Every time a vote is
received, the identity of the voter and the vote content
are stored, and the functionality lets the adversary know
who submitted a vote (but not the content of the vote,
of course). This captures the idea that submitting a vote
is a public action but could be relaxed in settings where
voting would be private (though this usually has a cost
in terms of eligibility veriﬁability.)

7Actually, BPRIV, strong correctness, and strong consistency are a strictly
stronger condition than securely realising our ideal functionality. Essentially,
these conditions deﬁne one particular simulator that work for the simulation-
based deﬁnition. The converse implication is not necessarily true.

510510

2) When it receives the order to tally,

the functionality
evaluates the ρ function on the sequence of pairs of
identities and votes that it has received, and sends the
result to the adversary.

Following the traditional ideal world/real world terminol-
ogy, we give the control of the honest voters to an entity called
the environment, which submits honest votes of its choice
directly to the functionality. This single entity coordinating the
honest voters ensures that security will be satisﬁed whoever
the honest votes are and whatever distribution they follow.
The ideal-world adversary, which we also call the simulator
(following the tradition), can also submit arbitrary votes to
the functionality, on behalf of (maybe temporarily) corrupted
voters. Furthermore, this simulator can receive arbitrary infor-
mation from the environment. This captures information that
the adversary could obtain about the honest votes externally
from the voting protocol (e.g., through polls,
. . . ). It also
interacts with the environment as part of its adversarial be-
havior, reporting its achievements. Eventually, the environment
outputs a single bit that expresses whether or not it feels that
it is interacting in the ideal-wold we just described. If the
environment can notice that it is not running in this ideal world
when it is actually running with a real protocol, then the real
world protocol will be claimed to be insecure.

Deﬁnition 10: The functionality Fvoting(ρ), interacting with
an environment E and an adversary S, proceeds as follows:

1) On input vote(id, v) from E or S, store (id, v) and send

ack(id) to S.

2) On input tally from S, return to both E and S the result
of the function ρ applied on the sequence of (id, v) pairs
received, then halt.

This functionality has clear privacy properties: it reveals
who votes, and the result of the election, but nothing more.
Furthermore, the votes from the simulator are sent to the
functionality in complete independence of the honest votes
from the environment unless the environment itself tipped the
simulator in the ﬁrst place (which cannot be prevented by
any voting system): the functionality does not give S any
information related to the honest votes before it provides the
election result. This seems to be the best we can hope for in
terms of privacy of votes.

The full process in which an environment E, an adversary
S and an ideal functionality Fvoting(ρ) interact, returning the
single output bit β produced by the environment, is written
β ← idealexec(E(cid:10)S(cid:10)Fvoting(ρ)).
B. The real protocol execution
real world, we
scheme
V = (GlobalSetup, Setup, Vote, Valid, Publish, Tally, Verify)
played by a set of honest voters, an adversary A who can
take control of voters, a honest administrator, and an honest
ballot box. A global setup might be available to all these
parties, e.g., under the form of a random oracle or a common
random string, as provided by GlobalSetup. These three

voting

have

In

the

a

honest elements, i.e. the administrator, the ballot box and the
global setup, can be seen as incorruptible ideal functionalities,
and we say that a protocol running in the presence of these
three functionalities runs in the ABG-hybrid model. If they
are not readily available, these functionalities can be securely
implemented using lower-level protocols, which are kept out
of our model here. For instance, one would typically have
multiple administrators running a threshold scheme in a real
election.

As in the ideal world,

there is an environment E who
commands the election: it asks the administrator to setup the
election, provides voting instructions to the honest voters, asks
to read the ballot box, and can also have arbitrary interactions
with A.
An execution of the scheme V in interaction with environ-
ment E and adversary A works as follows.
1) At ﬁrst, and if there is a global setup, E sends a
globalsetup.init command in order to initialize the
setup that is available to everyone.

2) E sends a setup command to the administrator, who

sends the election public key pk to everyone.

3) The ballot box creates an empty BB and can have two

types of interaction:
a) It can receive a ballot(b) command. In this case, it
runs Valid(BB, b) and, if successful, appends b to BB.
b) It can receive a publish command, on which returns

Publish(BB) to the party that issued that command.

4) The following commands can be issued in arbitrary

sequence.
a) E can send vote instructions vote(id, v) to honest
voters, who compute a ballot b = Vote(id, v) and send
ballot(b) to the ballot box.

b) Possibly in coordination with E, the adversary A can
submit arbitrary ballot(b) commands to the ballot
box on behalf of dishonest voters.

c) Anyone can send publish queries to BB.

5) At some point, the adversary sends tally to the ballot
box. The content of BB is then sent to the administrator
who, using the secret key sk, runs Tally on the ballots
and sends the result r and proof Π to the adversary.

The scheduling mechanism we use can be seen as token
passing with the environment as master scheduler: E is active
ﬁrst and, every time a command is issued to a party, this party
gets the token. When a party halts, E gets the token back, until
it sends its output bit which halts the entire system.
The full process in which an environment E and an adver-
sary A run in an execution of a voting scheme V as described
above, returning the single output bit β produced by E, is
written β ← realexec(E(cid:10)A(cid:10)V).

C. Secure implementation

The following deﬁnition captures the idea that a secure
protocol should not leak more information than whatever is
leaked in the ideal world.

511511

Deﬁnition 11: We say that scheme V securely implements
functionality Fvoting(ρ) in the ABG-hybrid model if for any
real adversary A there exists an ideal adversary S such that for
any environment E the distribution idealexec(E||S||Fvoting(ρ))
is computationally indistinguishable from realexec(E||A||V).
The following theorem establishes that correctness, ballot
privacy, and strong consistency for the voting scheme are suf-
ﬁcient conditions to ensure that its associated voting protocol
is secure in the sense deﬁned above.
Theorem 12: If a voting scheme V for result function ρ is
strongly correct, strongly consistent, and ballot private, then
the protocol for V securely implements Fvoting(ρ) in the ABG-
hybrid model.

Proof: We prove the stronger statement that there is a
simulator, with black-box access to the adversary, which works
for any (adversary, environment) pair. We proceed by “game
hopping”.

1) Game 0: is described by realexec(E||A||V).
2) Game 1: is a modiﬁed version of Game 0:
• The board that the adversary sees contains ballots to some

∗ ∈ V instead of the honest voters’ votes.

ﬁxed vote v

• The election result is obtained from the tally of a ballot
box containing ballots for the real votes (which is hidden
from the adversary).

• The auxiliary data are faked, using SimProof as deﬁned

from the BPRIV property.

The reader can immediately observe the close correspon-
dence between the changes made to Game 0 and the guarantees
offered by the BPRIV deﬁnition. Indeed, we show that Game
0 and Game 1 cannot be distinguished, unless the underlying
voting scheme is not BPRIV secure.

We now give a more detailed description of Game 1 and
sketch the proof for the above statement. The initialization step
in Game 1 includes, besides the steps in the intialization of
Game 0, the initialization of the fake board BB1. In the case
of a global setup, a simulated global setup SimGlobalSetup is
produced and add added on BB1: we formally add:

BB1 ← []; SimGlobalSetup ← SimGlobalSetup.init

to the initialization step in Game 0, and the adversary gets
access to SimGlobalSetup.

Next, a setup command is issued to the administrator, and

the adversary gets access to pk.

queries as in Game 0. These are answered as follows:

The adversary and the environment E are allowed the same
• on vote(id, v) ∈ (I × V) from E:
b0 ← Vote(id, v); b1 ← Vote(id, v
∗
then BB0 ← BB0(cid:10)b0, BB1 ← BB1(cid:10)b1.
• on ballot(b) ∈ ({0, 1}∗
if Valid(BB1, b) then BB0 ← BB0(cid:10)b and BB1 ← BB1(cid:10)b.
• on read from A:

); if Valid(BB1, b1)

) from A:

return Publish(BB1) to the adversary.
• on tally from A:
run (r, Π) ← Tally(sk, BB0), Π1 ← SimProof(r, BB1);
return (r, Π1) to the adversary and halt.

We write realexec(cid:3)
execution (which is also the output of Game 1).

(E||A||V) for the output of A in the above

We claim that for any adversary A and environment E,
the distance between the outputs of the games Game 0 and
Game 1, is at most the advantage of some adversary B against
BPRIV security of V.
Let D be a distinguisher (for the outputs of realexec and
realexec(cid:3)
). Adversary B against BPRIV uses D and operates as
follows. B runs E and A internally and answers their queries
as follows.
• on vote(id, v) ∈ I × V from E: adversary B sets v0 ← v
and v1 ← v
∗ and submits (id, v0, v1) to its own OvoteLR
oracle.
• on ballot(b) from A: adversary B issues b to its own
Ocast oracle.
• on read from A: adversary B queries Oboard and
forwards the result to A.
• on tally from A: adversary B queries Otally and
forwards the result to A.
• queries issued by A to his global setup are forwarded by

B to his own global setup; the answers are sent to A.

When E stops, B runs D on the local output of E and
outputs whatever D outputs. When B is in the BPRIV game
with β = 0 the view that B provides to A and the E is as in
realexec(E||A||V). At the same time, if β = 1, then the view
(E||A||V). It fol-
of A and E is the one they have in realexec(cid:3)
lows that whenever D successfully distinguishes between the
outputs of these two games, then B successfully distinguishes
the corresponding experiments Expbpriv,0A,V

and Expbpriv,1A,V .

3) Game 2: We introduce the next game in order to
establish a set of invariants that hold for the execution above;
these will serve as stepping stone to identify and argue about
the properties of the simulator S.

Recall that, in Game 1, ballot box BB0 contains the real
ballots submitted (by the adversary and the honest parties)
and ballot box BB1 contains only fake ballots. In this game
we introduce a third box BB2 which contains a list (id, v) of
the actual votes cast by the users. The ballot box is initially
empty and is updated when ballots are submitted. Speciﬁcally,
we modify the way queries vote(id, v) and ballot(b) are
processed in Game 1, as follows (to ease readability we
underline the parts added in Game 2):
• on vote(id, v) ∈ (I × V) from E:
b0 ← Vote(id, v); b1 ← Vote(id, v
if Valid(BB1, b1) then BB0 ← BB0(cid:10)b0, BB1 ← BB1(cid:10)b1,
BB2 ← BB2(cid:10)(id, v) and return (id, ack) to the simulator.
• on ballot(b) ∈ (I × {0, 1}∗
if Valid(BB1, b) then BB0 ← BB0(cid:10)b, BB1 ← BB1(cid:10)b;
(id, v) ← Extract(sk, b); BB2 ← BB2(cid:10)(id, v).

);

∗

):

As a result from these changes, every time a ballot is added
to BB0 and BB1, a corresponding vote is added on BB2: this
is just the vote submitted in clear in the case of a vote query,
or the extracted vote in the case of a ballot query, using
the Extract algorithm resulting from the strong consistency
guarantee. The view of the adversary is not modiﬁed in any
way for the moment.

512512

When A issues tally, Game 2 checks that the content of
BB2 coincides with the votes extracted from BB0.8 If this
is not the case the game aborts the execution, otherwise the
result provided to the adversary is computed as in Game 1.
Formally, the tally step in Game 2 is as follows:
• on tally from A:
if BB2 (cid:8)= Extract(sk, BB0) output ⊥ and halt;
run (r, Π) ← TallyGlobalSetup(sk, BB0);
run Π1 ← SimProof(r, BB1); return (r, Π1) to the
adversary.

Here, the view of the adversary can differ from Game 1
only if the test BB2 (cid:8)= Extract(sk, BB0) succeeds. We claim
that this can only happen with negligible probability, thanks
to the properties of Extract guaranteed by strong consistency
(Item 1. of Def. 8).

Indeed, we show how to build an adversary B that breaks
strong consistency with a probability identical to the proba-
bility that BB2 (cid:8)= Extract(sk, BB0) in Game 2.

B starts by emulating Game 2 internally, running by itself
the roles of the administrator, adversary, environment, . . . and
inspects whether, in the end, BB2 (cid:8)= Extract(sk, BB0). If it is
the case, B extracts the query that makes those boards differ.
This query can certainly not be a setup, read or tally
query, since these queries do not modify any board. It can also
not be a ballot query, since the vote that is added on BB2
in that case is, by deﬁnition, the extraction of the ballot added
on BB0. Eventually, if it is a vote(id, v) query, then B found
a situation where the extraction of Vote(id, v) (which appears
on BB0) differs from (id, v) (which appears on BB2). But,
from the ﬁrst item of the deﬁnition of strong consistency, this
can only happen with negligible probability, and so does the
BB2 (cid:8)= Extract(sk, BB0) property.

4) Game 3: This game is identical to the one above, with
the difference that, during vote queries, we check whether
the b0 ballot is valid with respect to BB0 and trigger an
error othewise. Formally, we make the following changes
(underlining the changes compared to Game 2, as before).

• on vote(id, v) ∈ (I × V) from E:
b0 ← Vote(id, v); b1 ← Vote(id, v
if not Valid(BB0, b0) then output ⊥ and halt.
if Valid(BB1, b1) then BB0 ← BB0(cid:10)b0, BB1 ← BB1(cid:10)b1,
BB2 ← BB2(cid:10)(id, v) and return (id, ack) to the simulator.
A difference between Game 2 and Game 3 only appears
when Valid(BB0, b0) is false. However, this would immedi-
ately violate the strong correctness property (Def. 9.)

);

∗

Indeed, an adversary against strong correctness can start
emulating Game 3 internally, using a honestly generated public
key provided as in the strong correctness experiment, up to a
guess on which vote(id, v) query would result in falsifying
Valid(BB0, b0). Then, it can submit (id, v, BB0) as output to
the strong correctness experiment and, if the guess is correct
(which happens with non negligible probability since there
are at most a polynomial number of vote queries), it wins the

8Applying Extract to a list of ballots means apply it component-wise.

experiment. But, since our voting scheme is strongly correct,
this can only happen with negligible probability.

5) Game 4: This game is identical to the one above, with
the difference that, during vote and ballot queries, we check
whether ballots satisfy the ValidInd predicate (deﬁned as part
of strong consistency) after having tested ballots with Valid.
Formally, we make the following changes to these queries:

∗

);

• on vote(id, v) ∈ (I × V) from E:
b0 ← Vote(id, v); b1 ← Vote(id, v
if not Valid(BB0, b0) then output ⊥ and halt.
if not ValidInd(b0) then output ⊥ and halt.
if Valid(BB1, b1) then BB0 ← BB0(cid:10)b0, BB1 ← BB1(cid:10)b1,
BB2 ← BB2(cid:10)(id, v) and return (id, ack) to the simulator.
• on ballot(b) ∈ (I × {0, 1}∗
if not ValidInd(b) then output ⊥ and halt, else
BB0 ← BB0(cid:10)b, BB1 ← BB1(cid:10)b;
Extract(sk, b); BB2 ← BB2(cid:10)(id, v).

): if Valid(BB1, b) then

(id, v) ←

A difference between Game 3 and Game 4 can only appear
if one of the ValidInd tests fails while the Valid test on the
same ballot succeeds. However, such an event would lead to a
contradiction of the second requirement of strong consistency,
which requires that a ballot that is Valid for any board should
also be ValidInd. An adversary could indeed emulate Game 4
internally and, as soon as a ValidInd test fails while the Valid
test on the same ballot succeeds for a speciﬁc board, it would
output that board and ballot, which would be in contradiction
with Item 2. of the deﬁnition of strong consistency.

6) Game 5: This game is identical to the one above with
the difference that, instead of running Tally on BB0 in order
to obtain the result r, the ρ function is evaluated on BB2.

Formally, we modify the tally query as follows:
• on tally from A:
if BB2 (cid:8)= Extract(sk, BB0) output ⊥ and halt;
run (r, Π) ← TallyGlobalSetup(sk, BB0);
r = ρ(BB2).
run Π1 ← SimProof(r, BB1); return (r, Π1) to the
adversary.
We claim that if V is strongly consistent (Deﬁnition 8) then

the outputs of Game 4 and Game 5 are indistinguishable.

Consider a reduction B against Property 3. of strong con-
sistency. Reduction B obtains pk, then simulates the execution
in Game 5 up to the point where A issues tally. At this point
B outputs BB0 and halts. Since BB2 = Extract(sk, BB0),
the output distributions of Games 4 and 5 only differ if
R(Tally(sk, BB0)) (cid:8)= ρ(Extract(sk, BB0)), where R(r, Π) :=
r extracts the result from a tally. Furthermore, the tests added
in Game 4 guarantee that BB0 only contains ballots that satisfy
ValidInd. So, if a difference happens, the reduction B wins
against the strong consistency property of V.

7) Game 6: This game is identical to the one above with

the difference that we remove:

• The test on BB2 added in Game 2;
• The Valid test added in Game 3;
• The ValidInd tests added in Game 4.

these tests only make a
We proved in those games that
difference with negligible probability, and we can use those
same arguments to justify that a difference between Game 5
and Game 6 will only appear with negligible probability.

)

∗

To sum up, the queries in Game 6 are treated as follows:
• on vote(id, v) ∈ (I × V) from E:
b0 ← Vote(id, v); b1 ← Vote(id, v
if Valid(BB1, b1) then BB0 ← BB0(cid:10)b0, BB1 ← BB1(cid:10)b1,
BB2 ← BB2(cid:10)(id, v) and return (id, ack) to the simulator.
• on ballot(b) ∈ ({0, 1}∗
if Valid(BB1, b) then BB0 ← BB0(cid:10)b; BB1 ← BB1(cid:10)b;
(id, v) ← Extract(sk, b) and BB2 ← BB2(cid:10)(id, v).
• on read from A:

) from A:

return Publish(BB1) to the adversary.
• on tally from A:
run r ← ρ(BB2); run Π1 ← SimProof(r, BB1); return
(r, Π1) to the adversary and halt.

8) Ideal adversary: Finally, we construct a simulator S for
which the output in idealexec(E||S||Fvoting(ρ)) is distributed
identically to the output of Game 6, thus completing the proof.
The simulator runs SimGlobalSetup.init to obtain an en-
vironment SimGlobalSetup (if needed), runs Setup to obtain
(pk, sk) and initializes a board BB1. It then starts running A
internally and answers queries as follows:

∗

• When S receives (id, ack) from Fvoting(ρ), it produces
b ← Vote(id, v
). If Valid(BB1, b) returns true, then S
executes BB1 ← BB1(cid:10)b.
• When A issues ballot(b), S runs Valid(BB1, b). If this
returns false, then S does nothing. Otherwise, S runs
BB1 ← BB1(cid:10)b and (id, v) ← Extract(sk, b) and sends
(id, v) to Fvoting.
• When A issues query read, S returns Publish(BB1).
• When A issues query tally, S issues tally to Fvoting
and obtains r, calculates Π1 ← SimProof(r, BB1) and
returns (r, Π1) to the adversary.

The claim is that the view of A and E in the above game

is identical to their view in Game 6. Indeed:

• The read queries from the adversary are answered from

the board BB1 in both cases;

• The tally query applies ρ to the sequence of votes
received by the functionality, votes that precisely match
those posted on BB2 in Game 6.

(cid:3)

VI. APPLICATION TO HELIOS

Helios [37] is a remote voting protocol aimed at providing
both privacy and veriﬁability. Helios builds on Cramer et
al [38] and Benaloh [29]. The attractiveness of Helios resides
on its open-source nature, simplicity and that it consists of
well-known cryptographic building blocks. Furthermore, it has
been used several times to run binding elections, including
the election of the president of the University of Louvain-
La-Neuve and the election of the board directors of the
International Association for Cryptographic Research (on a
regular basis since 2010) [39] and of the ACM. As such, Helios

513513

is the ideal candidate to test any new cryptographic model
for electronic voting. It is not surprising then that Helios has
been shown to ensure ballot privacy in the past [13], [27],
[12], [19]. However, as we have seen in the previous sections,
those analysis use unsatisfactory vote privacy deﬁnitions or do
not apply directly to Helios.

We show that our BPRIV and strong consistency deﬁ-
nitions can be realized by Helios, which implies the ﬁrst
simulation-based vote privacy proof for Helios. Speciﬁcally,
we analyze what could be considered the standard version
of Helios nowadays, that uses strong Fiat-Shamir proofs [19],
implements duplicate weeding [27] and homomorphic tallying.
With respect to the threat model, we consider an honest single
trustee and an honest bulletin board. Except for the single
trustee, the other adversarial assumptions are similar to those
used in previous ballot privacy analyses of Helios. Adapting
our proof to a multi-authority scenario is expected to carry
over in a similar manner as in [20], by extending BPRIV to a
multi-trustee scenario.

More explicitly,
the ElGamal

the Helios version that we analyze
IND-CPA cryptosystem D =
uses
[40]
(KeyGen, Enc, Dec) in a given group G where the Decisional
Difﬁe-Hellman assumption holds; the NIZK proof system [41],
H (g, pk, R, S) to prove in zero-knowledge that
[42] DisjProof
(R, S) encrypts g0 or g1 (with proof builder DisjProve and
proof veriﬁer DisjVerify); and the NIZK proof system [41]
vk =
EqDl
logR c for g, R, vk, c ∈ G (with proof builder ProveEq and
proof veriﬁer VerifyEq). H and G are hash functions mapping
to Zq.

G(g, R, vk, c) to prove in zero-knowledge that logg

Helios allows both “referendum” style votes (a single yes/no
question) and more complex ballots; in addition it allows for
several revote policies. The result function in Helios is ﬁxed to
return the number of votes that each option received (since it
tallies homomorphically). We formalise Helios for referendum
style votes and the “only your last vote counts” revote policy.

Theorem 13: Helios is BPRIV, strongly consistent, and
strongly correct under the DDH assumption in the Random
Oracle Model.

Our BPRIV proof is largely inspired by [19]: we make use
of the fact that Vote(id, v) produces non-malleable ciphertexts
(when interpreted as a public key encryption algorithm) and
that
tallying proofs are in fact zero-knowledge proofs of
decryption correctness. Details can be found in the full version
of this paper [35].

VII. CONCLUSION

We have reviewed the literature in order to ﬁnd a game-
based cryptographic deﬁnition of vote privacy suitable for
remote voting protocols, in particular compatible with veriﬁ-
ability. We have identiﬁed shortcomings in several previous
such deﬁnitions, and concluded that none of the existing
deﬁnitions was satisfactory. Based on our ﬁndings, we have
proposed a new deﬁnition, BPRIV, that avoids the existing
limitations. In particular, BPRIV provides a more precise

514514

model of the tally function, with a revote policy and explicit
auxiliary data.

We have additionally introduced the notions of strong
consistency and strong correctness, and we have been able to
show that together with BPRIV, they imply simulation-based
privacy. More precisely, we showed that a single-pass protocol
for computing some result function ρ, that is secure in the
BPRIV game-based sense and strongly correct and consistent,
achieves at least the same level of privacy as the ideal protocol
for ρ. One immediate interpretation of this result is that from
a scheme secure in the BPRIV sense the adversary can extract
as much information as it can extract from only seeing the
result and nothing more. This is a very desirable and intuitively
appealing property as it reduces understanding the level of
privacy of a protocol to that of understanding the level of
privacy of a corresponding ideal protocol. Furthermore proofs
using game-based deﬁnitions are more standard and easier
to construct than those using the simulation-based notions.
Whenever possible, proving game-based privacy is desirable.
We have shown that BPRIV can indeed be realized by a
real-scale protocol, namely Helios [2]. Since Helios satisﬁes
BPRIV and is strongly consistent, it immediately follows that
Helios is secure for a simulation-based notion of privacy.

We can see several directions for future work. Like previous
papers in the same area we model a single, trusted talling
functionality (and assume that this could be implemented via a
threshold scheme). It would be useful to spell out the required
arguments and verify this assumption. A detailed treatment of
mixnets (with more than one mixer) would also be interesting.
Another extension could be to consider dishonest ballot boxes
where, among other things, honest ballots could get “lost”.
On a similar note, the use of voter credentials to prevent
ballot stufﬁng by the ballot box has gained some interest
recently (there is also a Helios variant called Belenios that
implements this [43]) and we think it would be interesting to
model credentials in a privacy notion too.

ACKNOWLEDGMENT

The research leading to these results has received funding
from the European Research Council under the European
Union’s Seventh Framework Programme (FP7/2007-2013) /
ERC grant agreement no 258865 (ProSecure) and grant agree-
ment 609611 (PRACTICE), and under the 1317971 Wal-
loon Region Greentic–Truedev project. This work has been
supported in part by ERC Advanced Grant ERC-2010-AdG-
267188-CRIPTO.

REFERENCES

[1] B. Adida, “Helios: Web-based Open-Audit Voting,” in 17th USENIX
Security Symposium, 2008, pp. 335–348, helios website: http://
heliosvoting.org.

[2] B. Adida, O. de Marneffe, O. Pereira, and J.-J. Quisquater, “Electing a
University President Using Open-Audit Voting: Analysis of Real-World
Use of Helios,” in Electronic Voting Technology Workshop/Workshop on
Trustworthy Elections. Usenix, Aug. 2009.

[3] M. R. Clarkson, S. Chong, and A. C. Myers, “Civitas: Toward a Secure
Voting System,” in 29th Security and Privacy Symposium (S&P’08).
IEEE, 2008.

[4] R. L. Rivest and W. D. Smith., “ThreeVotingProtocols: ThreeBallot,
VAV, and Twin,” in Electronic Voting Technology Workshop (EVT 2007),
2007.

[5] P. Ryan, D. Bismark, J. Heather, S. Schneider, and Z. Xia, “The prˆet
`a voter veriﬁable election system,” IEEE Transactions on Information
Forensics and Security, vol. 4, pp. 662–673, 2009.

[6] D. Chaum, A. Essex, R. Carback, J. Clark, S. Popoveniuc, A. Sherman,
and P. Vora, “Scantegrity: End-to-End Voter-Veriﬁable Optical-Scan
Voting,” IEEE Security and Privacy, vol. 6, no. 3, pp. 40–46, 2008.

[7] J. Cohen (Benaloh) and M. Fischer, “A robust and veriﬁable crypto-
graphically secure election scheme,” in 26th Symposium on Foundations
of Computer Science. Portland, OR: IEEE, 1985, pp. 372–382.

[8] J. Benaloh, “Veriﬁable secret-ballot elections,” Yale University Depart-

ment of Computer Science, Tech. Rep. 561, September 1987.

[9] S. Delaune, S. Kremer, and M. D. Ryan, “Verifying privacy-type
properties of electronic voting protocols,” Journal of Computer Security,
vol. 17, no. 4, pp. 435–487, 2009.

[10] R. K¨usters, T. Truderung, and A. Vogt, “A Game-Based Deﬁnition
of Coercion-Resistance and its Applications,” in 23rd IEEE Computer
IEEE, 2010, pp. 122–136.
Security Foundations Symposium (CSF’10).
[11] ——, “Veriﬁability, Privacy, and Coercion-Resistance: New Insights
from a Case Study,” in IEEE Symposium on Security and Privacy (S&P
2011).

IEEE Computer Society, 2011, pp. 538–553.

[12] D. Bernhard, V. Cortier, O. Pereira, B. Smyth, and B. Warinschi, “Adapt-
ing Helios for provable ballot secrecy,” in 16th European Symposium
on Research in Computer Security (ESORICS’11), ser. LNCS, Springer,
Ed., vol. 6879, 2011.

[13] J. Groth, “Evaluating security of voting schemes in the universal
composability framework,” in ACNS, ser. Lecture Notes in Computer
Science, M. Jakobsson, M. Yung, and J. Zhou, Eds., vol. 3089. Springer,
2004, pp. 46–60.

[14] O. de Marneffe, O. Pereira, and J.-J. Quisquater, “Simulation-based
analysis of e2e voting systems,” in Proceedings of the First Conference
on E-Voting and Identity (VOTE-ID 2007), ser. LNCS, A. Alkasar and
M. Volkamer, Eds., no. 4896. Springer, Oct. 2007, pp. 137–149.

[15] T. Moran and M. Naor, “Split-ballot voting: everlasting privacy with
distributed trust,” in Proceedings of the 2007 ACM Conference on Com-
puter and Communications Security, (CCS 2007), Alexandria, Virginia,
USA, 2007, pp. 246–255.

[16] L. Coney, J. L. Hall, P. L. Vora, and D. Wagner, “Towards a privacy
measurement criterion for voting systems,” in In National Conference
on Digital Government Research, 2005.

[17] D. Bernhard, V. Cortier, O. Pereira, and B. Warinschi, “Measuring
vote privacy, revisited,” in 19th ACM Conference on Computer and
Communications Security (CCS’12).
Raleigh, USA: ACM, October
2012.

[18] D. Bernhard, O. Pereira, and B. Warinschi, “On necessary and sufﬁcient
conditions for private ballot submission,” Cryptology ePrint Archive,
Report 2012/236, 2012, http://eprint.iacr.org/.

[19] ——, “How not to prove yourself: Pitfalls of the Fiat-Shamir heuristic
and applications to Helios,” in ASIACRYPT, ser. Lecture Notes in
Computer Science, X. Wang and K. Sako, Eds., vol. 7658. Springer,
2012, pp. 626–643.

[20] V. Cortier, D. Galindo, S. Glondu, and M. Izabach`ene, “Distributed
ElGamal `a la Pedersen: Application to helios,” in WPES, A.-R. Sadeghi
and S. Foresti, Eds. ACM, 2013, pp. 131–142.

[21] M. Chase, M. Kohlweiss, A. Lysyanskaya, and S. Meiklejohn, “Ver-
iﬁable elections that scale for free,” in Public Key Cryptography, ser.
Lecture Notes in Computer Science, K. Kurosawa and G. Hanaoka, Eds.,
vol. 7778. Springer, 2013, pp. 479–496.

[22] D. Bernhard and B. Smyth, “Ballot privacy and ballot independence
coincide,” in Proceedings of the 18th European Symposium on Research
in Computer Security (ESORICS’13), ser. Lecture Notes in Computer
Science, Springer, Ed., 2013.

[23] J. C. Benaloh and M. Yung, “Distributing the power of a government
to enhance the privacy of voters (extended abstract),” in PODC, J. Y.
Halpern, Ed. ACM, 1986, pp. 52–62.

[24] J. Benaloh, “Veriﬁable secret-ballot elections,” PhD thesis, Yale Univer-

sity, 1987.

[25] J. C. Benaloh and D. Tuinstra, “Receipt-free secret-ballot elections
(extended abstract),” in STOC, F. T. Leighton and M. T. Goodrich, Eds.
ACM, 1994, pp. 544–553.

[26] E. Cuvelier, O. Pereira, and T. Peters, “Election veriﬁability or ballot
privacy: Do we need to choose?” in Proceedings of the 18th European

Symposium on Research in Computer Security (ESORICS’13), ser.
Lecture Notes in Computer Science, Springer, Ed., 2013.

[27] V. Cortier and B. Smyth, “Attacking and ﬁxing Helios: An analysis of
ballot secrecy,” in 24th IEEE Computer Security Foundations Symposium
(CSF’11).

IEEE, 2011, pp. 297–311.

[28] ——, “Attacking and ﬁxing Helios: An analysis of ballot secrecy,”

Journal of Computer Security, vol. 21, no. 1, pp. 89–148, 2013.

[29] J. Benaloh, “Ballot casting assurance via voter-initiated poll station
auditing,” in Proceedings of the Second Usenix/ACCURATE Electronic
Voting Technology Workshop, 2007.

[30] M. R. Clarkson, S. Chong, and A. C. Myers, “Civitas: Toward a secure
voting system,” in Proc. IEEE Symposium on Security and Privacy,
2008, pp. 354–368.

[31] S. Delaune, S. Kremer, and M. Ryan, “Verifying privacy-type properties
of electronic voting protocols,” Journal of Computer Security, vol. 17,
no. 4, pp. 435–487, 2009.

[32] A. Juels, D. Catalano, and M. Jakobsson, “Coercion-Resistant Electronic
Elections,” in 4th Workshop on Privacy in the Electronic Society (WPES
2005). ACM, 2005, pp. 61–70.

[33] J. Dreier, P. Lafourcade, and Y. Lakhnech, “Deﬁning privacy for
weighted votes, single and multi-voter coercion,” in ESORICS, ser. Lec-
ture Notes in Computer Science, S. Foresti, M. Yung, and F. Martinelli,
Eds., vol. 7459. Springer, 2012, pp. 451–468.

[34] “Ballot paper for European Parliament election in Luxembourg, 2014,”

http://upload.wikimedia.org/wikipedia/commons/b/bf/Ballot paper
European Parliament elections 2014 in Luxembourg.JPG.

[35] D. Bernhard, V. Cortier, D. Galindo, O. Pereira, and B. Warinschi, “A
comprehensive analysis of game-based ballot privacy deﬁnitions,” Cryp-
tology ePrint Archive, Report 2015/255, 2015, http://eprint.iacr.org/.

[36] D. Wikstr¨om, “Simpliﬁed submission of inputs to protocols,” in Security
and Cryptography for Networks, 6th International Conference, SCN
2008, ser. Lecture Notes in Computer Science, R. Ostrovsky, R. D.
Prisco, and I. Visconti, Eds., vol. 5229. Springer, 2008, pp. 293–308.
[37] B. Adida, O. de Marneffe, O. Pereira, and J.-J. Quisquater, “Electing a
university president using open-audit voting: Analysis of real-world use
of Helios,” in Proceedings of the 2009 conference on Electronic voting
technology/workshop on trustworthy elections, 2009.

[38] R. Cramer, R. Gennaro, and B. Schoenmakers, “A secure and optimally
efﬁcient multi-authority election scheme,” in Advances in Cryptology
(EUROCRYPT’97), 1997, p. 103118.

[39] International Association for Cryptologic Research. Elections page at

http://www.iacr.org/elections/.

[40] T. E. Gamal, “A public key cryptosystem and a signature scheme based
on discrete logarithms,” IEEE Transactions on Information Theory,
vol. 31, no. 4, pp. 469–472, 1985.

[41] D. Chaum and T. P. Pedersen, “Wallet databases with observers,” in
CRYPTO, ser. Lecture Notes in Computer Science, E. F. Brickell, Ed.,
vol. 740. Springer, 1992, pp. 89–105.

[42] R. Cramer, I. Damg˚ard, and B. Schoenmakers, “Proofs of partial knowl-
edge and simpliﬁed design of witness hiding protocols,” in CRYPTO,
ser. Lecture Notes in Computer Science, Y. Desmedt, Ed., vol. 839.
Springer, 1994, pp. 174–187.

[43] V. Cortier, D. Galindo, S. Glondu, and M. Izabach`ene, “Election veriﬁa-
bility for Helios under weaker trust assumptions,” in Computer Security
- ESORICS 2014 Proceedings, Part II, ser. Lecture Notes in Computer
Science, M. Kutylowski and J. Vaidya, Eds., vol. 8713. Springer, 2014,
pp. 327–344.

APPENDIX

VIII. TEST CASES FOR BPRIV DEFINITIONS

During our work on this paper, we listed some insecure
variations on voting protocols. Future designers of ballot
privacy-related notions can use these as “safety checks”: if
any of these schemes is not insecure under some notion of
privacy then the notion may be too weak. We argue that none
of these insecure schemes can satisfy BPRIV.

A. Helios v3 — Replay attacks

Any voting scheme in which an adversary can read a honest
voter’s ballot from the ballot box and resubmit a ballot for the

515515

If β = 0
If β = 1

If β = 0
If β = 0

B sees
=⇒
B sees
=⇒
B(cid:2) sees
=⇒
B(cid:2) sees
=⇒

{b0},{bc}, {( b0, vb0 )} , {vb1} , {( bc, vbc )} , Π
(cid:3)
} , {( bc, vbc )} , Π
{b1},{bc}, {( b1, vb0 )} , {vb1
†
{b0},{bc}, {vb0} , {vb1} , {vbc} , Π
(cid:3)
{b1},{bc}, {vb0
} , Π
†

} , {vb1

} , {vbc

Fig. 3. Comparison between BPRIV adversaries.

contain any information about the order of the votes in BB0.
(cid:3) containing the list (v0, v1) is at most
So the probability of Π
the probability of guessing the adversary’s “left” votes without
access to any ballots. Therefore, the adversary can distinguish
β = 0 from β = 1 with high probability.

IX. BPRIV2 DOES NOT GUARANTEE BALLOT PRIVACY
Let us prove formally that V deﬁned in Section III-D is
BPRIV2 secure, while it should intuitively not be declared
private.

SimProof(BB0, BB1, pk, info) := Π
{( b, vb )}

||{( b1, vb0 )}

b∈BB0∩BB1

b1∈BB1,b0∈BB0

† ||

that

turns out

:= SimProof(cid:3)

is BPRIV2 private then so is V. Indeed,

where BBγ := BBγ \ (BB0 ∩ BB1), vb := Extract(sk, b) and
†
(BB0, BB1, pk, info). Intuitively, SimProof
Π
decrypts correctly the adversarial ballots but mimics the case
if
β = 0 for all honest ballots. Then it
V(cid:3)
let us write
BBβ = Hβ ∪ C, where Hβ is the sublist of honest ballots
output by the OvoteLR oracle for β = 0, 1, and C :=
BBβ \ Hβ. By construction C ⊆ BB0 ∩ BB1. We want
to see that for every BPRIV2 adversary B against V there
exists a BPRIV2 adversary B(cid:3) against scheme V(cid:3) such that
Succbpriv2(B) ≈negl Succbpriv2(B(cid:3)
), their advantages only differ
in a negligible quantity. We compare the data seen by BPRIV2
adversaries B against scheme V and adversaries B(cid:3) against
scheme V(cid:3) in Figure 3, wherein b0 ∈ H0, b1 ∈ H1, bc ∈ C.
We observe that the only extra information that adversary
B against V sees compared to adversary B(cid:3) against V(cid:3), are
the relations {( b0, vb0 )} and {( bc, vbc )} when β = 0,
and the relations {( b1, vb0 )} and {( bc, vbc )} when β = 1.
However this does not allow B to have a greater distinguishing
advantage than B(cid:3). Indeed, since the new matching between
ballots and votes is not veriﬁable (the original tallying proofs
† did not change and V(cid:3) is BPRIV2 private), adversary
(cid:3)
Π
B(cid:3) can simulate B’s view using its own view. It sufﬁces for
B(cid:3) to tell apart the multisets of votes V0 and VC (this can
be done easily, since V0 are the left vote queries of B and
VC := r \ V0), and later link H0 or H1 to V0 at its liking.
Finally, Succbpriv2(B) ≈negl Succbpriv2(B(cid:3)

, Π

).

same vote as her own fails to ensure vote privacy, as argued
in Section III-F and illustrated on Helios (version 3) [28].

More precisely, consider an adversary who, for any two
distinct honest votes v0 and v1, can perform the following
sequence of queries:

start-voting(); vote(v0, v1); vote(v1, v0);
bb ←board(); b ← first(bb); ballot(b);
(r, a) ← tally()
Here first returns the ﬁrst ballot of bb. We may also imagine
more sophisticated attacks where the adversary has to change
e.g. identifying information on the ballot. We deliberately
choose two vote queries that would produce the same “left”
and “right” tallies, if b was independent of the honest ballots,
In the BPRIV game, if the ballot b is not rejected (by
Publish) then the tally for β = 0 is ρ(v0, v1, v0) and for
β = 1 it is ρ(v1, v0, v1). If these are not the same value,
which happens for example if ρ counts the number of v0 and
v1 votes submitted, then our adversary can win the BPRIV
game (with probability 1), hence the broken scheme is not
BPRIV secure.

B. Leaky auxiliary data

Consider any voting scheme where the result function ρ is
supposed to output how many times each choice was voted
for, i.e. one could write ρ(v0, v1, v0, v0) = {(v0, 3), (v1, 1)}.
Imagine that, unfortunately, a scheme’s tally also contains
all votes in clear, in the order that they were cast. This is
clearly not desirable. Intuitively, there are two distinct ways of
discarding such a scheme. Either the (leaky) output of the tally
is classiﬁed as being the result itself. Then the scheme does
not implement the desired functionality and this is captured
with our “strong consistency” notion. Or this leaky output is
contained in the auxiliary data. In that case, we can construct
a BPRIV adversary against such a scheme:

start-voting(); vote(v0, v1); vote(v1, v0);
(r, a) ← tally()
To show that this adversary can win, we recall that the
output of tally is split into a result r and auxiliary data Π in
such a way that r is guaranteed not to leak any information.
In other words, r is the correct/intended result so the list of
all votes must be part of Π. In the case β = 0, the adversary
sees auxiliary data that reveals all the votes on BB0, i.e. the
sequence (v0, v1). In contrast, if β = 1 then Π is computed as
a (probabilistic) function of BB1 and r, neither of which can

516516

