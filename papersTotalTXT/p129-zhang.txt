A Dual Perturbation Approach for Differential Private
ADMM-Based Distributed Empirical Risk Minimization

Department of Electrical and Computer

Department of Electrical and Computer

Tao Zhang

Engineering

Tandon School of Engineering

New York University
Brooklyn, NY, 11201
tz636@nyu.edu

Quanyan Zhu

Engineering

Tandon School of Engineering

New York University
Brooklyn, NY, 11201
qz494@nyu.edu

ABSTRACT
The rapid growth of data has raised the importance of
privacy-preserving techniques in distributed machine learn-
ing. In this paper, we develop a privacy-preserving method
to a class of regularized empirical risk minimization (ERM)
machine learning problems. We ﬁrst decentralize the learn-
ing algorithm using the alternating direction method of mul-
tipliers (ADMM), and propose the method of dual variable
perturbation to provide dynamic diﬀerential privacy. The
mechanism leads to a privacy-preserving algorithm under
mild conditions of the convexity and diﬀerentiability of the
loss function and the regularizer. We study the performance
of the algorithm measured by the number of data points re-
quired to achieve a bounded error. To design an optimal
privacy mechanism, we analyze the fundamental tradeoﬀ be-
tween privacy and accuracy, and provide guidelines to choose
privacy parameters. Numerical experiments using the real-
world database are performed to corroborate the results on
the privacy and utility tradeoﬀs and design.
Keywords
Machine Learning; Distributed Optimization; Diﬀerential

Privacy; ADMM; Privacy Tradeoﬀs
1

Introduction

Recent years have witnessed that Distributed machine learn-

ing is a promising way to manage the deluge of data. The
size of the training data ranges from 1T B to 1P B [13]; as a
result, a centralized machine learning that collects and pro-
cesses the data can lead to signiﬁcant computational com-
plexity and communications overhead. Therefore, a decen-
tralized approach to machine learning is essential to reduce
the computational cost, provide the scalability of the data
processing and improve the quality of decision-making.

Alternating direction method of multiplier (ADMM) is one
suitable approach to decentralizing a centralized machine
learning problem. ADMM enables distributed training over

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
AISec’16, October 28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4573-6/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2996758.2996762

a network of collaborative nodes which exchange their pa-
rameters and outcomes with the neighbors. However, se-
rious privacy concerns arise from the communications be-
tween two neighboring nodes, which process sensitive data
including social network data, the web search histories, ﬁ-
nancial information, and medical records. It is possible for
an adversary to acquire conﬁdential information about the
training data of individual nodes by observing the outcome
of the learning. The adversary can be either a member of
the learning network or an outsider. Diﬀerential privacy is
a well-suitable concept that provides a strong privacy guar-
antee that the absence of a single database item does not
allow an adversary to distinguish (substantially) an individ-
ual data point [8].

This paper focuses on a class of distributed ADMM-based
empirical risk minimization (ERM) problems, and develops
a randomized algorithm that can provide diﬀerential privacy
[8, 17] while keeping the learning procedure accurate. The
privacy concepts of [8, 17] is extended to distributed ma-
chine learning over networks based on ADMM, and we pro-
pose a privacy-preserving scheme of the regularized ERM-
based optimization. The method is dual variable perturba-
tion (DVP), in which we perturb the dual variable of each
node at every ADMM iteration.

We investigate the performance of the algorithms and
show that the DVP is useful for non-separable learning prob-
lems. We characterize the fundamental tradeoﬀs between
privacy and accuracy by formulating an optimization prob-
lem and use numerical experiments to demonstrate the opti-
mal design of privacy mechanisms. We use ADMM to decen-
tralize regularized ERM algorithms to achieve distributed
training of large datasets. Dynamic diﬀerential privacy is
guaranteed for the distributed algorithm using the DVP,
which adds noise to the update of the dual variable. We pro-
vide the theoretical performance guarantees of the DVP ver-
sion of the distributed ERM with l2 regularization. The per-
formance is measured by the number of sample data points
required to achieve certain criteria. We also propose a de-
sign principle to select the optimal privacy parameters by
solving an optimization problem.
1.1 Related Work
A signiﬁcant amount of research has investigated the dis-
tributed classiﬁcation learning algorithm. These works have
focused on either enhancing the eﬃciency of the learning
model or on producing a global classiﬁer from multiple dis-
tributed local classiﬁer trained at individual nodes. Eﬀorts

129Suppose that ˆD is available to the fusion center node, then
we can choose the global classiﬁer f : X → Y that minimizes
the CR-ERM.
The loss function ˆL(yip, f T xip) : R × Rd × Rd → R, is
used to measure the quality of the classiﬁer trained. In this
paper, we focus on the speciﬁc loss function ˆL(yip, f T xip) =
L(yipf T xip). The function R(f ) in (1) is a regularizer that
prevents overﬁtting.
In this paper, we have the following
assumptions on the loss, regularization functions, and the
data.
Assumption 1. The loss function L is strictly convex and
doubly diﬀerentiable of f with |L(cid:48)| (cid:54) 1 and |L(cid:48)(cid:48)| (cid:54) c1, where
c1 is a constant. Both L and L(cid:48) are continuous.
Assumption 2. The regularizer function R(·) is continu-
ous, diﬀerentiable and 1-strongly convex. Both R(·) and
∇R(·) are continuous.
Assumption 3. We assume that (cid:107)xip(cid:107)(cid:54) 1. Since yip ∈
{−1, 1}, |yip| = 1.
2.1 Distributed ERM
To decentralize CR-ERM, we introduce decision variables
{fp}P
p=1, where node p determines its own classiﬁer fp, and
impose consensus constraints f1 = f2 = ... = fP that guar-
antee global consistency of the classiﬁers. Let {wjp} be the
auxiliary variables to decouple fp of node p from its neigh-
bors j ∈ Np. Then, the consensus-based reformulation of
(1) becomes

P(cid:88)

Bp(cid:88)

P(cid:88)

min
{fp}P

p=1

ZC2 :=

L(yipf T

C R
ρR(fp).
Bp
fp = wpj, wpj = fj, p = 1, ..., P, j ∈ Np

p xip) +

p=1

p=1

i=1

(2)

s.t.

where ZC2 ({fp}p∈P| ˆD) is the reformulated objective as a
function of {fp}P
p=1. According to Lemma 1 in [10], if {fp}P
p=1
presents a feasible solution of (2) and the network is con-
nected, then problems (1) and (2) are equivalent, i.e., f =
fp, p = 1, ..., P , where f is a feasible solution of CR-ERM.
Problem (2) can be solved in a distributed fashion using
the alternative direction method of multiplier (ADMM) with
each node p ∈ P optimizing the following distributed regu-
larized empirical risk minimization problem (DR-ERM):

Zp(fp|Dp) :=

C R
Bp

L(yipf T

p xip) + ρR(fp).

(3)

Bp(cid:88)

i=1

have been on making the distributed algorithm suitable for
large-scale datasets, e.g., MapReduce has been used to ex-
plore the performance improvements [7]. In addition, meth-
ods such as voting classiﬁcation [5], mixing parameters [14],
and ADMM methods [10], have been used to achieve dis-
tributed computation. ADMM is used in our approach to
distributed machine learning, in which the centralized prob-
lem acts as a group of distributed convex optimization sub-
problems connected by the consensus constraints on the de-
cision parameters over a network.

In privacy-preserving data mining research, a large amount
of literature on data perturbation for privacy (e.g., [9],[12])
has focused on additive or multiplicative perturbations of
individual samples, which might aﬀect certain relationships
among diﬀerent samples in the database. Many works also
have studied the diﬀerential-private machine learning. For
example, Kasiviswanathan et al. have derived a general
method for probably approximately correct (PAC, [19]) in
[11]. A body of existing literature has investigated the trade-
oﬀ privacy and accuracy while researching on the theory of
diﬀerential privacy (examples include [8, 15, 2]). This work
extends the notion of diﬀerential privacy to a dynamic set-
ting, and deﬁnes dynamic diﬀerential privacy to capture the
distributed and iterative nature of the ADMM-based dis-
tributed ERM.
1.2 Organization of the Paper
The rest of the paper is organized as follows. In Section
2, we present the ADMM approach to decentralizing a cen-
tralized ERM problem, and describe the privacy concerns
associated with the distributed machine learning. Section 3
presents the DVP algorithm to provide dynamic diﬀerential
privacy. Section 4 studies the performance of the DVP al-
gorithm and Section 5 shows numerical experiments to cor-
roborate the results and optimal design principles to the
tradeoﬀ between privacy and accuracy. Finally, Section 6
presents concluding remarks and future research directions.
2 Problem Statement
Consider a connected network, which contains P nodes
described by an undirected graph G(P,E) with the set of
nodes P = {1, 2, 3, ..., P}, and a set of edges E denoting the
links between connected nodes. A particular node p ∈ P
only exchanges information between its neighboring node
j ∈ Np, where j ∈ Np is the set of all neighboring nodes
of node p, and Np = |Np| is the number of neighboring
nodes of node p. Each node p contains a dataset Dp =
{(xip, yip) ⊂ X × Y : i = 0, 1, ..., Bp}, which is of size Bp
with data vector xip ∈ X ⊆ Rd, and the corresponding label
yip ∈ Y := {−1, 1}. The entire network therefore has a set

of data ˆD =(cid:83)

p∈P Dp.

The target of the centralized classiﬁcation algorithm is to
ﬁnd a classiﬁer f : X → Y using all available data ˆD that
enables the entire network to classify any data x(cid:48) input to a
label y(cid:48) ∈ {−1, 1}. Let ZC1 (f| ˆD) be the objective function
of a regularized empirical risk minimization problem (CR-
ERM), deﬁned as follows:

ZC1 (f| ˆD) :=

C R
Bp

ˆL(yip, f T xip) + ρR(f ),

(1)

P(cid:88)

Bp(cid:88)

p=1

i=1

The augmented Lagrange function associated with the DR-
ERM is:

LD

p (fp, wpj, λk

(cid:88)

pj)

(cid:0)λa

pi

(cid:1)T (fp − wpi) +

(cid:88)

(cid:0)λb

pi

(cid:1)T (wpi − fi)

= Zp +

(cid:88)

i∈Np

+

η
2

i∈Np
((cid:107) fp − wpi (cid:107)2 + (cid:107) wpi − fi (cid:107)2).

i∈Np

The distributed iterations solving (3) are:

fp(t + 1) = arg min
fp

LD
p

(cid:0)fp, wpj(t), λk
pj(t)(cid:1),
pj(t)(cid:1),
(cid:0)fp(t + 1), wpj, λk

(4)

(5)

(6)

where C R (cid:54) Bp is a regularization parameter, and ρ > 0
is the parameter that controls the impact of the regularizer.

wpj(t + 1) = arg min
wpj

LD
p

130Algorithm 1 Distributed ERM

Required: Randomly initialize fp, λp = 0d×1 for every p ∈ P
Input: ˆD
for t = 0, 1, 2, 3, ... do
for p = 0 to P do

Compute fp(t + 1) via (10).

end for
for p = 0 to P do

Broadcast fp(t + 1) to all neighbors j ∈ Np.

end for
for p = 0 to P do

Compute λp(t + 1) via (11).

end for

end for
Output: f∗.

λa
pj(t + 1) = λa

pj(t) + η(fp(t + 1) − wpj(t + 1)),

p ∈ P, j ∈ Np,

λb
pj(t + 1) = λb

pj(t) + η(wpj(t + 1) − fp(t + 1)),

p ∈ P, j ∈ Np.

(7)

(8)

be further simpliﬁed by initializing the dual variables λk

0d×d, and letting λp(t) =(cid:80)

According to Lemma 2 in [10], iterations (5) to (8) can
pj =
pj, p ∈ P, j ∈ Np, k = a,
λk
b, we can combine (7) and (8) into one update. Thus, we
simplify (5)-(8) by introducing the following: Let LN
p (t) be
p ({fp},{fp(t)},{λp(t)}) as :
the short-hand notation of LN

j∈Np

LN

p (t) :=

C R
Bp

+ η

L(yipf T

p xip) + ρR(fp) + 2λp(t)T fp

(cid:107) fp − 1
2

(fp(t) + fi(t)) (cid:107)2 .

(9)

Bp(cid:88)
(cid:88)

i=1

i∈Np

The ADMM iterations (5)-(8) can be reduced to

fp(t + 1) = arg min
fp

LN

p (fp, fp(t), λp(t)),

(10)

λp(t + 1) = λp(t) +

η
2

[fp(t + 1) − fj(t + 1)].

(11)

(cid:88)

j∈Np

step and extract the sensitive information based on his ob-
servation of the learning output of his neighboring nodes.
Simple anonymization is not suﬃcient to address this issue
as discussed in Section 1.
In the following subsection, we
will discuss the adversary models, and present diﬀerential
privacy solutions.
2.2 Privacy Concerns
Although the data stored at each node is not exchanged
during the entire ADMM algorithm, the potential privacy
risk still exists. Suppose that the dataset Dp stored at node
p contains sensitive information in data point (xi, yi) that
is not allowed to be released to other nodes in the network
or anyone else outside. Let K : Rd → R be the randomized
version of Algorithm 1, and let {f∗
p}p∈P be the output of
K at all the nodes. Then, the output {f∗
p}p∈P is random.
In the distributed version of the algorithm, each node opti-
mizes its local empirical risk based on its own dataset Dp.
Let K t
p be the node-p-dependent stochastic sub-algorithm of
K at iteration t, and let fp(t) be the output of K t
p(Dp) at
iteration t inputting Dp. Hence, the output fp(t) is stochas-
tic at each t. In this work, we consider the following attack
model. The adversary can access the learning outputs of
intermediate ADMM iterations as well as the ﬁnal output.
This type of adversary aims to obtain sensitive information
about the private data point of the training dataset by ob-
p of K for all p ∈ P at
serving the output fp(t) of K t
every stage t of the training. We protect the privacy of dis-
tributed network using the deﬁnition of diﬀerential privacy
in [8]. Speciﬁcally, we require that a change of any single
data point in the dataset might only change the distribution
of the output of the algorithm slightly, which is visible to the
adversary; this is done by adding randomness to the output
of the algorithm. Let Dp and D(cid:48)
p be two datasets diﬀering in
ip) ⊂ D(cid:48)
one data point; i.e., let (xip, yip) ⊂ Dp, and (x(cid:48)
ip, y(cid:48)
p,
then (xip, yip) (cid:54)= (x(cid:48)
ip). In other words, their Hamming
i=0 1{i : xi (cid:54)=
Distance, which is deﬁned as Hd(Dp, D(cid:48)
i}, equals 1; i.e., Hd(Dp, D(cid:48)
x(cid:48)
To protect the privacy against the adversary, we propose
the concept of dynamic diﬀerential privacy, which enables
the dynamic algorithm to be privacy-preserving at every
stage of the learning.

p) =(cid:80)Bp

p or f∗

ip, y(cid:48)

p) = 1.

The ADMM-based distributed ERM iterations (10)-(11)
are summarized in Algorithm 1. Every node p ∈ P updates
its local d × 1 estimates fp(t) and λp(t). At iteration t + 1,
node p updates the local fp(t + 1) through (10). Next, node
p broadcasts the latest fp(t + 1) to all its neighboring nodes
j ∈ Np. Iteration t + 1 ﬁnishes as each node updates the
λp(t + 1) via (11).

one in the centralized problem, which is (cid:80)P

Every iteration of our algorithm is still a minimization
problem similar to the centralized problem (1). However,
the number of variables participating in solving (10) per
node per iteration is Np, which is much smaller than the
p=1 Np. There
are several methods to solve (10). For instance, projected
gradient method, Newton method, and Broyden-Fletcher-
Goldfarb-Shanno (BFGS) method [6] that approximates the
Newton method, to name a few.

ADMM-based distributed machine learning has beneﬁts
due to its high scalability. It also provides a certain level
of privacy since nodes do not communicate data directly
but their decision variable fp. However, the privacy arises
when an adversary can make intelligent inferences at each

node p has a training dataset Dp, and ˆD = (cid:83)

Deﬁnition 1. (Dynamic α(t)-Diﬀerential Privacy (DDP))
Consider a network of P nodes P = {1, 2, ..., P}, and each
p∈P Dp. Let
K : Rd → R be a randomized version of Algorithm 1. Let
α(t) = (α1(t), α2(t), ..., αP (t)) ∈ RP
+, where αp(t) ∈ R+ is
the privacy parameter of node p at iteration t. Let K t
p be the
node-p-dependent sub-algorithm of K, which corresponds to
an ADMM iteration at t that outputs fp(t). Let D(cid:48)
p be any
dataset with Hd(D(cid:48)
p). We say
that the algorithm K is dynamic αp(t)-diﬀerential private
p, and for all p ∈ P that can
(DDP) if for any dataset D(cid:48)
be observed by an adversary, and for all possible sets of the
outcomes S ⊆ R, the following inequality holds:
Pr[fp(t) ∈ S] (cid:54) eαp(t) · Pr[gp(t) ∈ S],

(12)
for all t ∈ Z during a learning process. The probability is
taken with respect to fp(t), the output of K t
p at every stage t.
The algorithm K is called dynamic α(t)-diﬀerential private
if the above conditions are satisﬁed.

p, Dp) = 1, and gp(t) = K t

p(D(cid:48)

1313 Dynamic Private Preserving
In this subsection, we describe the algorithm that pro-
vides dynamic α-diﬀerential privacy deﬁned in Section 2.2.
We extend the deﬁnition of diﬀerential privacy in [8], in
which the output, i.e., the variable in our case, is perturbed
by random noise before releasing it. The fact that the opti-
mization at each iteration in ADMM is independent of each
other at diﬀerent iterations has made it possible to deal with
the privacy issue at a node p ∈ P in each iteration indi-
vidually. However, the directly perturbed variables will be
transmitted to neighboring nodes and thus the noise terms
from all the neighboring nodes as well as the node p itself
will be directly involved in the optimization at the next iter-
ation. Thus, we perturb the Lagrange multiplier, i.e., dual
variable, instead. The perturbed dual variable randomizes
optimization at each iteration, and thus the output of each
iteration. We name this method as the dual variable per-
turbation (DVP).
3.1 Dual Variable Perturbation
In the DVP, the dual variables {λp(t)}P
p=1 are perturbed
with a random noise vector p(t) ∈ Rd with the probability
density function Kp() ∼ e−ζp(t)(cid:107)(cid:107), where ζp(t) is a param-
eter related to the value of αp(t), and (cid:107) · (cid:107) denotes the l2
norm. At each iteration, we ﬁrst perturb the dual variable
λp(t), obtained from the last iteration, and store it in a new
variable µp(t) = λp(t) + p(t). Now the corresponding node-
p-based augmented Lagrange function LN

p (t) becomes

(cid:0)fp, fp(t), µp(t + 1),{fi(t)}i∈Np

(cid:1),

deﬁned as follows, and Ldual
tation:

p

(t) is used as a short-hand no-

Ldual

p

(t) =

(cid:80)Bp
i=1 L(yipf T

CR
Bp
+2µp(t + 1)T fp + Φ

+η(cid:80)

p xip) + ρR(fp)

2 (cid:107) fp (cid:107)2

i∈Np

(cid:107) fp − 1

2 (fp(t) + fi(t)) (cid:107)2,

(13)

2 (cid:107) fp (cid:107)2 is an additional penalty. As a result,

where Φ
the minimizer of Ldual
(t) is random. At each iteration, we
ﬁrst perturb the dual variable λp(t), obtained from the last
iteration, and store it in a new variable µp(t + 1).

p

Now, the iterations (10)-(11) become follows:

µp(t + 1) = λp(t) +

C R
2Bp

p(t + 1),

(14)

(15)

fp(t + 1) = arg min
fp

Ldual

p

(t),

(cid:88)

j∈Np

λp(t + 1) = λp(t) +

η
2

[fp(t + 1) − fj(t + 1)].

(16)

The iterations (14)-(16) are summarized in Algorithm 2,
and are illustrated in Figure 1. All nodes have its corre-
sponding value of ρ. Every node p ∈ P updates its local
estimates µp(t), fp(t) and λp(t) at time t; at time t+1, node
p ﬁrst perturbs the dual variable λp(t) obtained at time t to
obtain µp(t + 1) via (14), and then uses training dataset Dp
to compute fp(t + 1) via (15). Next, node p sends fp(t + 1)
to all its neighboring nodes. The (t + 1)-th update is done
when each node updates its local λp(t+1) via (16). We then
have the following theorem.

(a) DVP during intermediate iterations

(b) The ﬁnal iteration of DVP

Ldual

p

Figure 1.
Illustration of DVP: (a) DVP during intermedi-
ate iterations. The perturbed µp participates in the (15). As
a result, the output fp at each iteration is a random vari-
able, and the transmission of fp is diﬀerential private. (b)
The ﬁnal iteration of DVP. Note that the evil face symbol
represents the adversary.

Deﬁnition 1 provides a suitable diﬀerential privacy con-
cept for the adversary. For dynamic αp(t)-diﬀerential pri-
vate algorithms, the adversaries cannot extract additional
information by observing the intermediate updates of fp(t)
at each step. Clearly, the algorithm with ADMM iterations
shown in (10) to (11) is not dynamic αp(t)-diﬀerential pri-
vate. This is because the intermediate and ﬁnal optimal
output fp’s are deterministic given dataset Dp. For D(cid:48)
p
with Hd(Dp, D(cid:48)
p) = 1, the classiﬁer will change completely,
and the probability density Pr([fp|D(cid:48)
p]) = 0, which leads to
the ratio of probabilities Pr[fp|Dp]
p] → ∞. Please note that
Pr[fp|D(cid:48)
the optimization at each iteration in ADMM is independent
of each other diﬀerent iteration. This property of ADMM
makes it possible that the privacy at each node each iter-
ation is independent; the level of privacy at node p ∈ P
iteration t totally depends on the value of αp(t) chosen at
(cid:54)= t and all
time t and is independent of αj(t(cid:48)), for all t(cid:48)
j (cid:54)= p. Thus, our dynamic privacy is also independent of the
number of iterations. As a result, the adversaries cannot
obtain additional information from each iteration and there
is nothing in previous iterations they can take advantage of
to extract more information in later iterations.

132Algorithm 2 Dual Variable Perturbation

Required: Randomly initialize fp, λp = 0d×1 for every p ∈ P
Input: ˆD, {[αp(1), αp(2), ...]}P
for t = 0, 1, 2, 3, ... do
for p = 0 to P do

p=1

(cid:16)

(cid:1)(cid:17)2

.

(cid:0)ρ+2ηNp

c1

Bp
CR

Let ˆαp = αp(t) − ln

1 +

if ˆαp > 0 then

Φ = 0.

else

Φ =

c1

Bp

CR (eαp (t)/4−1)

− ρ − 2ηNp and ˆαp = αp(t)/2.

end if
Draw noise p(t) according to Kp() ∼ e−ζp(t)(cid:107)(cid:107) with
ζp(t) = ˆαp.
Compute µp(t + 1) via (14) and fp(t + 1) via (15) with
augmented Lagrange function as (13).

end for
for p = 0 to P do

Broadcast fp(t + 1) to all neighbors j ∈ Np.

end for
for p = 0 to P do

Compute λp(t + 1) via (16).

end for

end for
Output: {f∗

p}P

p=1.

Theorem 1. Under Assumption 1, 2 and 3, if the DR-ERM
problem can be solved by Algorithm 2, then Algorithm 2 solv-
ing this distributed problem is dynamic α-diﬀerential private
with αp(t) for each node p ∈ P at time t. Let Q(fp(t)|D)
and Q(fp(t)|D(cid:48)
p) be the probability density functions of fp(t)
given dataset D and D(cid:48)
p) = 1.
The ratio of conditional probabilities of fp(t) is bounded as
follows:

p, respectively, with Hd(D, D(cid:48)

Q(fp(t)|D)
Q(fp(t)|D(cid:48)
p)

(cid:54) eαp(t).

(17)

Proof: See Appendix.
4 Performance Analysis
In this section, we discuss the performance of Algorithm
2. We establish performance bounds for regularization func-
tions with l2 norm. Our analysis is based on the following
assumptions:
Assumption 4. The data points {(xpi, ypi)}Bp
i.i.d.
Pxy(xpi, ypi) at each node p ∈ P.
Assumption 5. p(t) is drawn from (15) with the same
αp(t) = α(t) for all p ∈ P at time t ∈ Z.

i=1 are drawn
from a ﬁxed but unknown probability distribution

We then deﬁne the expected loss of node p using classiﬁer

fp as follows, under Assumption 4:

ˆC(fp) := C RE(x,y)∼Pxy (L(yf T x)),

and the corresponding expected objective function ˆZ is:

ˆZp(fp) := ˆC(fp) + ρR(fp).

The performance of non-private non-distributed ERM clas-
siﬁcation learning has been already studied by, for example,
Shalev et al.
in [18] (also see the work of Chaudhuri et al.
in [3]), which introduces a reference classiﬁer f 0 with ex-
pected loss ˆC(f 0), and shows that if the number of data

points is suﬃciently large, then the actual expected loss of
the trained l2 regularized support vector machine (SVM)
classiﬁer fSV M satisﬁes

ˆC(fSV M ) (cid:54) ˆC 0 + αacc,

where αacc is the generalization error. We use a similar
argument to study the accuracy of Algorithm 1. Let f 0
be the reference classiﬁer of Algorithm 1. We quantify the
performance of our algorithms with f∗ as the ﬁnal output
by the number of data points required to obtain

ˆC(f

∗

) (cid:54) ˆC 0 + αacc.

(t + 1) = arg minfp LN

However, instead of focusing on only the ﬁnal output, we
care about the learning performance at all iterations. Let
f non
p (t) be the intermediate updated
classiﬁer at t, and let f∗ = arg minfp Zp(fp|Dp) be the ﬁnal
p
output of Algorithm 1. From Theorem 9 (see Appendix A),
(t)} is bounded and converges to the opti-
the sequence {f non
mal value f∗ as time t → ∞. Note that {f non
(t)} is a non-
private classiﬁer without added perturbations. Since the
optimization is minimization, then there exists a constant
(t))− ˆC(f∗) (cid:54) ∆non(t),
∆non(t) at time t such that: ˆC(f non
and substituting it to ˆC(f∗) (cid:54) ˆC 0 + αacc, yields:
(t)) (cid:54) ˆC 0 + ∆non(t) + αacc.

ˆC(f non

(18)

p

p

p

p

Clearly, the above condition depends on the reference clas-
siﬁer f 0; actually, as shown later in this section, the number
of data points depends on the l2-norm (cid:107) f 0 (cid:107) of the reference
classiﬁer. Usually, the reference classiﬁer is chosen with an
upper bound on (cid:107) f 0 (cid:107), say b0. Based on (18), we provide
the following theorem on the performance of Algorithm 1.

p

(t + 1) = arg minfp LN

2 (cid:107) fp(t) (cid:107)2, and let f 0 such
Theorem 2. Let R(fp(t)) = 1
that ˆC(f 0) = ˆC 0 for all p ∈ P at time t, and δ > 0 is a pos-
p (fp, t|Dp)
itive real number. Let f non
(xip, yip) ⊂ Rd × {−1, 1}(cid:111)
(cid:110)
be the output of Algorithm 1. If Assumption 1 and 4 are sat-
(cid:32)
(cid:33)
isﬁed, then there exists a constant βnon such that if the num-
ber of data points, Bp in Dp =
(t + 1)) (cid:54) ˆC 0 + αacc + ∆non(t)(cid:1) (cid:62) 1 − δ. for

ﬁes: P(cid:0) ˆC(f non

satisfy: Bp > βnon

CR(cid:107)f 0(cid:107)2ln( 1
δ )

, then f non

(t + 1) satis-

α2

acc

p

all t ∈ Z+.

p

Note that αacc (cid:54) 1 is required for most machine learn-
ing algorithms. In the case of SVM, if the constraints are
yif T xi (cid:54) cSV M , for i = 1, , ..., n, where n is the number
of data points, then, classiﬁcation margin is csvm/ (cid:107) f (cid:107).
Thus, if we want to maximization the margin cSV M / (cid:107) f 0 (cid:107)
we need to choose a large value of (cid:107) f 0 (cid:107). Larger values
of (cid:107) f 0 (cid:107) are usually chosen for non-separable or with small
margin. In the following section, we provide the performance
guarantees of Algorithm 2.
4.1 Performance of Private Algorithms
Similar to Algorithm 1, we solve an optimization prob-
(fp, t|Dp) at each iteration. Let fp(t)
lem minimizing Ldual
and λp(t) be the primal and dual variables used in mini-
(fp, t|Dp) at iteration t, respectively. Suppose
mizing Ldual
that starting from iteration t, the noise vector is static with
p(t) generated at iteration t. To compare our private clas-
siﬁer at iteration t with a private reference classiﬁer f 0(t),
we construct a corresponding algorithm, Alg-2, associated

p

p

133with Algorithm 2. However, starting from iteration t + 1,
the noise vector in Alg-2 p(t(cid:48)) = p(t) for all t(cid:48) > t. In other
words, solving Alg-2 is equivalent to solving the optimization
(fp, t|Dp, pi(t)),
problem with the objective function Z dual
t (cid:62) 0 deﬁned as follows:

p

p

Z dual
p(t) and λ(cid:48)

(fp, t|Dp, p(t)) := Zp(fp|Dp) +

C R
Bp

p(t)fp.

Let f(cid:48)
based algorithm minimizing Z dual
Then, Alg-2 can be interpreted as minimizing

p(t) be the updated variables of the ADMM-
(fp, t|Dp) at iteration t.

p

Z dual

p

(fp, t|Dp, pi(t))

p

Since Z dual

p(0) = fp(t) and λ(cid:48)

with initial condition as f(cid:48)
p(0) = λp(t)
(fp, t|Dp, pi(t)) be regarded as the
for all p ∈ P. Let Z dual
associated objective function of Alg-2.
(fp, t|Dp, pi(t)) is real and convex, then, sim-
ilar to Algorithm 1, the sequence {fp(t)} is bounded and
fp(t) converges to f∗
p (t), which is a limit point of fp(t). Thus,
there exists a constant ∆dual
(t) given noise vector p(t) such
that

p

p

ˆC(fp(t)) − ˆC(f

∗
p (t)) (cid:54) ∆dual

p

(t).

The performance analysis in Theorem 2 can also used in
DVP. Speciﬁcally, the performance is measured by the num-
ber of data points, Bp, for all p ∈ P required to obtain

ˆC(fp(t)) (cid:54) ˆC 0(t) + αacc + ∆dual

p

(t).

We say that every learned fp(t) is αacc-optimal if it satisﬁes
the above inequality.

We now establish the performance bounds for Algorithm

2, DVP, which is summarized in the following theorem.

2 (cid:107) fp(t) (cid:107)2, and f 0

Theorem 3. Let R(fp(t)) = 1
p (t) such
p (t)) = ˆC 0(t) for all p ∈ P, and a real number
that ˆC(f 0
δ > 0. If Assumption 1, 4 and 5 are satisﬁed, then there
exists a constant βdual such that if the number of data points,
Bp in Dp =

(xip, yip) ⊂ Rd × {−1, 1}(cid:111)
(cid:32)

satisfy:

(cid:110)

(cid:16)(cid:107) f 0
(cid:17)
p (t + 1) (cid:107)2

max

t

p (t + 1) (cid:107) d ln( d
δ )
(cid:16) CR (cid:107) f 0

αaccαp(t)

(cid:17)
p (t + 1) (cid:107)2 ln( 1
δ )

,

Bp > βdual max

(cid:17)(cid:33)

,

(cid:16) CRc1 (cid:107) f 0
P(cid:0) ˆC(f

max

t

αaccαp(t)

, max

t

then f∗

p (t + 1) satisﬁes:

α2

acc

(cid:1) (cid:62) 1 − 2δ.

∗
p (t + 1)) (cid:54) ˆC 0(t + 1) + αacc

(fp, t|Dp)
Corollary 3.1. Let fp(t + 1) = arg minfp Ldual
be the updated classiﬁer of Algorithm 2 and let f 0
p (t) be a
reference classiﬁer such that ˆC(f 0
If all the
conditions of Theorem 3 are satisﬁed, then fp(t + 1) satisﬁes

P(cid:0) ˆC(fp(t + 1)) (cid:54) ˆC 0(t) + αacc + ∆dual

(t)(cid:1) (cid:62) 1 − 2δ. (19)

p (t) = ˆC 0(t).

p

p

Proof. The following inequality holds for fp(t) and f∗

p (t)

ˆC(fp(t)) − ˆC(f

∗
p (t)) (cid:54) ∆dual

p

and from Theorem 3,

P(cid:0) ˆC(f

∗
p (t + 1)) (cid:54) ˆC 0(t + 1) + αacc

(t),

(cid:1) (cid:62) 1 − 2δ.

Therefore, we can arrive at (19).

1

αacc

p (t) (cid:107) is used, the terms

Theorem 3 and Corollary 3.1 can guarantee the privacy
deﬁned in both Deﬁnition 1. From Theorem 3, we can see
that, for non-separable problems or ones with a small mar-
gin, in which a larger (cid:107) f 0
and
p (t) (cid:107) have a signiﬁcant inﬂuence on the requirement of
(cid:107) f 0
datasets size for DVP. Moreover, the privacy increases by
trading the accuracy. Therefore, It is essential to manage
the tradeoﬀ between the privacy and the accuracy, and this
will be discussed in Section 5.
5 Numerical Experiment
In this section, we test Algorithm 2 with real world train-
ing dataset. The dataset used is the Adult dataset from
UCI Machine Learning Repository [1], which contains de-
mographic information such as age, sex, education, occu-
pation, marital status, and native country. In the experi-
ments, we use our algorithm to develop a dynamic diﬀeren-
tial private logistic regression. The logistic regression, i.e.,
LLR takes the following form: LLR(yipf T xip) = log(1 +
exp(−yipf T
p xip)), whose ﬁrst-order derivative and the second-
order derivative can be bounded as |L(cid:48)
LR| (cid:54)
1
4 , respectively, satisfying Assumption 3. Therefore, the loss
function of logistic regression satisﬁes the conditions shown
in Assumption 2 and 3. In this experiment, we set R(fp) =
2 (cid:107) fp (cid:107)2, and c1 = 1
1
4 . We can directly apply the loss func-
tion LLR to Theorem 1 and 2 with R(f ) = 1
2 (cid:107) fp (cid:107)2, and
c1 = 1
4 , and then it can provide αp(t)-diﬀerential privacy for
all t ∈ Z.

LR| (cid:54) 1 and |L(cid:48)(cid:48)

We also study the privacy-accuracy tradeoﬀ of Algorithm
2. The privacy is quantiﬁed by the value of αp(t). A larger
αp(t) implies that the ratio of the densities of the classi-
ﬁer fp(t) on two diﬀerent data sets is larger, which implies a
higher belief of the adversary when one data point in dataset
D is changed; thus, it provides lower privacy. However, the
accuracy of the algorithm increases as αp(t) becomes larger.
As shown in Figure 2, a larger αp(t) leads to faster conver-
gence of the algorithm. When αp(t) is small, the model is
more private but less accurate. Therefore, the utilities of
privacy and accuracy need to satisfy the following assump-
tions:

Assumption 6. The utilities of privacy is monotonically
increasing with respect to αp(t) for every p ∈ P but accuracy
is monotonically decreasing with respect to αp(t) for every
p ∈ P.

(cid:80)Bp
The quality of classiﬁer is measured by the total empirical
i=1 L(yipfp(t)T xip). Let Lacc(·) : R+ → R
loss C(t) = CR
Bp
represent the relationship between αp(t) and C(t). The func-
tion Lacc is obtained by curve ﬁtting given the experimental
data points (αp(t), C(t)). Let Upriv(αp(t)) : R+ → R be the
utility of privacy, same for every node p ∈ P. Besides the de-
creasing monotonicity, Upriv(αp(t)) is assumed to be convex
and doubly diﬀerentiable function of αp(t).
In our exper-
iment, we model the utility of privacy as: Upriv(αp(t)) =
p(t) , where, ωpj ∈ R for j = 1, 2, 3, 4.
ωp1 · ln
(cid:80)Bp
For training the classiﬁer, we use a few ﬁxed values of ρ and
i=1 LLR(t) of the classi-
test the empirical loss C(t) = CR
Bp
ﬁer. Then, we select the value of ρ that minimizes the empir-
ical loss for a ﬁxed αp (0.3 in this experiment). We also test
the non-private version of algorithm, and the corresponding
minimum ρ is obtained as the control. We choose the cor-
responding optimal values of the regularization parameter ρ

ωp3αp(t)+ωp4α2

ωp2

134(a) αp(t) = 0.01

(b) αp(t) = 0.1

(c) αp(t) = 0.5

(d) αp(t) = 1

Figure 2. Convergence of DVP algorithm, at iteration t = 100 (before the stop time) with diﬀerent values of αp(t) with
ρ = 10−2.5 and C R = 1750

(a) DVP: t = 2

(b) DVP: t = 100

(c) Empirical risk vs. αp.

(d) Misclassiﬁcation error rate
vs. αp.

Figure 3. Privacy-accuracy tradeoﬀ.(a)-(b): DVP, with ωp1 = 0.02, ωp2 = 6, ωp3 = 9, ωp4 = 1 (before the stop time); (c):
Privacy-accuracy tradeoﬀ; empirical risk vs. αp of ﬁnal optimum output. (d): Privacy-accuracy tradeoﬀ; misclassiﬁcations
error rate vs. αp of iteration 100.

as 10−10, 10−2.5 for Algorithm 1 and 2, respectively. The
values of C R are chosen as 1750 and 1750 for Algorithm 1
and 2, respectively. Figure 2 shows the convergence of DVP
at diﬀerent values of αp(t) at a given iteration t. Larger
values of αp yield better convergence. However, a larger αp
leads to poorer privacy. Figure 3 (a)-(b) shows the privacy-
accuracy tradeoﬀ of DVP at diﬀerent iterations. By curve
ﬁtting, we model the function

Lacc(αp(t)) = c4 · e

−c5αp(t) + c6,

(cid:80)100

where c4, c5, c6 ∈ R+. From the experimental results, we
determine c4 = 0.2, c5 = 25, c6 = mint{C(t)}; these values
are applicable at all iterations. For iteration t > 1, c4 = 20,
c5 = 20, c6 = 1
t=20 C(t). Figure 3 (c)-(d) shows the
81
privacy-accuracy tradeoﬀ of the ﬁnal optimum classiﬁer in
terms of the empirical loss and misclassiﬁcation error rate
(MER). The MER is determined by the fraction of times the
trained classiﬁer predicts a wrong label.
6 Conclusion
In this work, we have developed an ADMM-based algo-
rithm, using dual variable perturbation (DVP) to solve a
centralized regularized ERM in a distributed fashion while
providing dynamic α(t)-diﬀerential privacy for the ADMM
iterations as well as the ﬁnal trained output. Thus, the
sensitive information stored in the training dataset at each
node is protected against both the internal and the external
adversaries.
Based on distributed training datasets, Algorithm 2 per-
turbs the dual variable λp(t) for every node p ∈ P at it-
eration t before minimizing the augmented Lagrange func-
tion to calculate the primal variable fp(t). The performance
analysis of DVP indicates that it is suitable for diﬃcult

problems that are non-separable or with small margin. In
general, the accuracy decreases as privacy requirements are
more stringent. The tradeoﬀ between the privacy and ac-
curacy is studied. Our experiments on real data from UCI
Machine Learning Repository show that dual variable per-
turbation is able to manage the privacy-accuracy tradeoﬀ
while keeping a good convergence performance.
APPENDIX
Proof. (Theorem 1) Let fp(t + 1) be the optimal primal
variable with zero duality gap. From the Assumption 1 and
2, we know that both the loss function L and the regularizer
R(·) are diﬀerentiable and convex, and by using the Karush-
Kuhn-Tucker (KKT) optimality condition, we have

0 =

CR
Bp

Bp(cid:88)
yipL(cid:48)(yipfp(t + 1)T xip)xip + ρ∇R(fp)
+ 2(cid:0) CR
(cid:88)

p(t) + λp(t)(cid:1) + (Φ + 2ηNp)fp(t + 1)

2Bp

i=1

(fp(t) + fi(t)),

− η

i∈Np

from which we can establish the relationship between the
noise p(t) and the optimal primal variable fp(t + 1) as:

p(t) = −

(yipfp(t + 1)T xip)xip − Bp

C R ρ∇R(fp)

yipL(cid:48)

Bp(cid:88)
C R λp(t) − Bp

i=1

− 2Bp
Bpη
C R

+

(cid:88)

i∈Np

C R (Φ + 2ηNp)fp(t + 1)

(fp(t) + fi(t)).

(20)

135Bp(cid:88)

i=1

− Bp

θ(cid:89)

From the convexity (Assumption 1) of Ldual

(t) is strictly
convex, there is a unique value of fp(t + 1) for ﬁxed p(t)
and dataset Dp. The equation (20) shows that for any value
of fp(t + 1), we can ﬁnd a unique value of p(t) such that
fp(t+1) is the minimizer of Ldual
. Therefore, given a dataset
Dp, the relation between p(t) and fp(t + 1) is bijective.

p

p

Let Dp and D(cid:48)

p be two datasets with

Hd(Dp, D

p) = 1, (xi, yi) ∈ Dp
(cid:48)

i, y(cid:48)

i) ∈ D(cid:48)

and (x(cid:48)
p are the corresponding two diﬀerent data
points. Let two matrices Jf (p(t)|Dp) and Jf ((cid:48)
p) de-
note the Jacobian matrices of mapping from fp(t+1) to p(t)
and (cid:48)
p(t), respectively. Then, transformation from noise
fp(t + 1) to p(t) by Jacobian yields:

p(t)|D(cid:48)

=

Q(fp(t + 1)|Dp)
Q(fp(t + 1)|D(cid:48)
p)

q(p(t)|Dp)
p(t)|D(cid:48)
q((cid:48)
p)
where q(p(t)|Dp) and q((cid:48)
p(t)|D(cid:48)
and (cid:48)
Dp and D(cid:48)

p, respectively.

p) are the densities of p(t)
p(t), respectively, given fp(t + 1) when the datasets are

| det(Jf (p(t)|Dp))|−1
| det(Jf ((cid:48)
p))|−1 ,

p(t)|D(cid:48)

From equation (20), the Jacobian matrix is found as:

Jf (p(t)|Dp) = −

f (xi, yi) − Bp
J0

C R ρ∇2R(fp(t + 1))

Let M = J0

C R (Φ + 2ηNp)Id.
f (xi, yi), and H = −Jf (p(t)|Dp),
i) − J0
i, y(cid:48)
and thus Jf (p(t)|D(cid:48)
p) = −(M + H). Let hj(W) be the j-th
largest eigenvalue of a symmetric matrix W ∈ Rd×d with
rank θ. Then, we have the fact:

f (x(cid:48)

det(I + W) =

(1 + hj(W)).

j

Since the matrix xixT
i has rank 1, matrix M has rank at
most 2; thus matrix H−1M has rank at most 2; therefore,
we have:
det(H + M) = det(H) · det(I + H
= det(H) · (1 + h1(H

−1M)
−1M))(1 + h2(H

−1M).

Thus, the ratio of determinants of the Jacobian matrices can
be expressed as:
| det(Jf (p(t)|Dp))|−1
| det(Jf ((cid:48)

| det(H + M)|

p))|−1 =

p(t)|D(cid:48)

| det(H)|
=| det(I + H
=(1 + h1(H
=|1 + h1(H
+ h1(H

−1M)|
−1M))(1 + h2(H
−1M) + h2(H
−1M)h2(H

−1M)|.

−1M)

−1M)

Based on Assumption 2, all the eigenvalues of ∇2R(fp(t +
1)) is greater than 1 [16]. Thus, from Assumption 1, matrix
H has all eigenvalues at least Bp
CR
|h1(H−1M)| (cid:54)

(cid:0)ρ + Φ + 2ηNp

(cid:1). Therefore,

(cid:1) .

(cid:0)

(cid:80)

i |hi(M)| (cid:54)(cid:80)

Let σi(M) be the non-negative singular value of the sym-
metric matrix M. According to [4], we have the inequality
i σi(M). Thus, we have |h1(M)|+|h2(M)| (cid:54)

|hi(M)|
ρ+Φ+2ηNp

Bp
CR

σ1(M) + σ2(M). Let (cid:107) X (cid:107)Σ= (cid:80)

i σi be the trace norm of
X. Then, according to the trace norm inequality, we have:

(cid:107) M (cid:107)Σ(cid:54)(cid:107) J0(x
(cid:48)
i, y

i) (cid:107)Σ + (cid:107) −J0(xi, yi) (cid:107)Σ .
(cid:48)

As a result, based on the upper bounds from Assumption 1
and 3, we have:

i) (cid:107)Σ + (cid:107) −J0(xi, yi) (cid:107)Σ
(cid:48)

|h1(M)| + |h2(M)| (cid:54)(cid:107) J0(x
(cid:48)
i, y
(yifp(t + 1)T xi)|· (cid:107) xi (cid:107)
i L(cid:48)(cid:48)
(cid:54) |(y2
(cid:48)2
+ |(y
i)|· (cid:107) x
i (cid:107)
i L(cid:48)(cid:48)
(cid:48)
(cid:48)
(y
(cid:54) 2c1,

(cid:48)
ifp(t + 1)T x

which follows h1(M)h2(M) (cid:54) c2
minants of Jacobian matrices is bounded as:

1. Finally, the ratio of deter-

| det(Jf (p(t)|Dp))|−1
p))|−1
| det(Jf ((cid:48)

p(t)|D(cid:48)

(cid:1))

)2

(21)

Bp
CR

c1

(cid:0)ρ + Φ + 2ηNp
(cid:1)(cid:17)2

.

(cid:54) (1 +

= eα,

(cid:0)

(cid:16)

where α = ln

1 +

c1

ρ+Φ+2ηNp

Bp
CR

Now, we bound the ratio of densities of p(t). Let sur(E)
be the surface area of the sphere in d dimension with radius
E, and sur(E) = sur(1) · Ed−1. We can write
q(p(t)|Dp)
p|D(cid:48)
q((cid:48)
p)

(cid:54) eζp(t)((cid:107)(cid:48)

K(p(t))
K((cid:48)

(cid:107)p(t)(cid:107)d−1
sur((cid:107)1(t)(cid:107))
p(t)(cid:107)d−1
(cid:107)(cid:48)
sur((cid:107)(cid:48)
p(t)(cid:107))

p(t))

=

(22)
where ˆαp is a constant satisfying the above inequality. For
non-negative Φ, let ˆαp = αp(t) − ln
.
If ˆαp > 0, then we ﬁx Φ = 0, and thus ˆαp = αp(t) − α.
− ρ − 2ηNp, and ˆαp =
Otherwise, let Φ =

c1
ρ+2ηNp

(cid:0)

Bp
CR

1 +

c1

(cid:1)(cid:17)2

(cid:16)

, then ˆαp = αp(t) − α. Therefore, we obtain

αp(t)

2

Bp

CR (eαp(t)/4−1)

p(t)(cid:107)−(cid:107)p(t)(cid:107)) (cid:54) e ˆαp ,

| det(Jf (p(t)|Dp))|−1
| det(Jf ((cid:48)
p))|−1

p(t)|D(cid:48)

(cid:54) eαp(t)− ˆαp .

From the upper bounds stated in Assumption 1 and 3, the
l2 norm of the diﬀerence of 1 and 2 can be bounded as:

(cid:107) 
p(t) − p(t) (cid:107)=
(cid:48)

(cid:107) yipL(cid:48)

(cid:48)
(cid:48)
ipfp(t + 1)T x
ip)x

(y

(cid:48)
ip

i=1

− (yipL(cid:48)

(yipfp(t + 1)T xip)xip (cid:107)(cid:54) 2.

p(t) (cid:107) − (cid:107) p(t) (cid:107)(cid:54)(cid:107) (cid:48)

Thus, (cid:107) (cid:48)
fore, by selecting ζp(t) = ˆαp
conditional densities of fp(t + 1) as

p(t) − p(t) (cid:107)(cid:54) 2. There-
2 , we can bound the ratio of

Q(fp(t + 1)|Dp)
Q(fp(t + 1)|D(cid:48)
p)

(cid:54) eαp(t),

and prove that the DVP can provide αp(t)-diﬀerential pri-
vacy.

Acknowledgement
We would like to thank National Science Foundation (CNS-

1544782) and NYU University Research Challenge Fund (URCF)
that partially support this research.

Bp(cid:88)

136A References
[1] A. Asuncion and D. Newman. Uci machine learning

repository, 2007.

[2] A. Blum, C. Dwork, F. McSherry, and K. Nissim.

Practical privacy: the sulq framework. In Proceedings
of the twenty-fourth ACM
SIGMOD-SIGACT-SIGART symposium on Principles
of database systems, pages 128–138. ACM, 2005.

[3] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate.

Diﬀerentially private empirical risk minimization. The
Journal of Machine Learning Research, 12:1069–1109,
2011.

[4] P. Chilstrom. Singular value inequalities: New

approaches to conjectures. 2013.

[5] M. Collins. Discriminative training methods for

hidden markov models: Theory and experiments with
perceptron algorithms. In Proceedings of the ACL-02
conference on Empirical methods in natural language
processing-Volume 10, pages 1–8. Association for
Computational Linguistics, 2002.

[6] Y.-H. Dai. A perfect example for the bfgs method.

Mathematical Programming, 138(1-2):501–530, 2013.

[7] J. Dean and S. Ghemawat. Mapreduce: simpliﬁed

data processing on large clusters. Communications of
the ACM, 51(1):107–113, 2008.

[8] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data
analysis. In Theory of cryptography, pages 265–284.
Springer, 2006.

[9] A. Evﬁmievski, R. Srikant, R. Agrawal, and

J. Gehrke. Privacy preserving mining of association
rules. Information Systems, 29(4):343–364, 2004.

[10] P. A. Forero, A. Cano, and G. B. Giannakis.

Consensus-based distributed support vector machines.
The Journal of Machine Learning Research,
11:1663–1707, 2010.

[11] S. P. Kasiviswanathan, H. K. Lee, K. Nissim,

S. Raskhodnikova, and A. Smith. What can we learn
privately? SIAM Journal on Computing,
40(3):793–826, 2011.

[12] J. Kim and W. Winkler. Multiplicative noise for

masking continuous data. Statistics, page 01, 2003.

[13] M. Li, D. G. Andersen, J. W. Park, A. J. Smola,

A. Ahmed, V. Josifovski, J. Long, E. J. Shekita, and
B.-Y. Su. Scaling distributed machine learning with
the parameter server. In Proc. OSDI, pages 583–598,
2014.

[14] R. McDonald, K. Hall, and G. Mann. Distributed

training strategies for the structured perceptron. In
Human Language Technologies: The 2010 Annual
Conference of the North American Chapter of the
Association for Computational Linguistics, pages
456–464. Association for Computational Linguistics,
2010.

[15] F. McSherry and K. Talwar. Mechanism design via

diﬀerential privacy. In Foundations of Computer
Science, 2007. FOCS’07. 48th Annual IEEE
Symposium on, pages 94–103. IEEE, 2007.

[16] A. Narayanan and V. Shmatikov. Robust

de-anonymization of large sparse datasets. In Security
and Privacy, 2008. SP 2008. IEEE Symposium on,
pages 111–125. IEEE, 2008.

[17] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth
sensitivity and sampling in private data analysis. In
Proceedings of the thirty-ninth annual ACM
symposium on Theory of computing, pages 75–84.
ACM, 2007.

[18] S. Shalev-Shwartz and N. Srebro. Svm optimization:

inverse dependence on training set size. In Proceedings
of the 25th international conference on Machine
learning, pages 928–935. ACM, 2008.

[19] L. G. Valiant. A theory of the learnable.

Communications of the ACM, 27(11):1134–1142, 1984.

137