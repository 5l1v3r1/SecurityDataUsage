Automated Attacks on Compression-Based Classiﬁers

Igor Burago

Department of Computer Science

University of California, Irvine

iburago@uci.edu

Daniel Lowd

Department of Computer
and Information Science

University of Oregon

lowd@cs.uoregon.edu

ABSTRACT
Methods of compression-based text classiﬁcation have proven
their usefulness for various applications. However, in some
classiﬁcation problems, such as spam ﬁltering, a classiﬁer
confronts one or many adversaries willing to induce errors in
the classiﬁer’s judgment on certain kinds of input. In this
paper, we consider the problem of ﬁnding thrifty strategies for
character-based text modiﬁcation that allow an adversary to
revert classiﬁer’s verdict on a given family of input texts. We
propose three statistical statements of the problem that can
be used by an attacker to obtain transformation models which
are optimal in some sense. Evaluating these three techniques
on a realistic spam corpus, we ﬁnd that an adversary can
transform a spam message (detectable as such by an entropy-
based text classiﬁer) into a legitimate one by generating and
appending, in some cases, as few additional characters as
11% of the original length of the message.

Categories and Subject Descriptors
I.2.6 [Artiﬁcial Intelligence]: Learning

Keywords
Adversarial machine learning, Evasion attacks, Compression-
based classiﬁers, Prediction by partial mapping (PPM), Text
classiﬁcation, Spam ﬁltering

1.

INTRODUCTION

Machine learning is now being used to detect many kinds
of malicious activity, including online auction fraud [11], fake
reviews [1, 21], social network spam [4, 33], email spam [7, 16,
31], comment spam [23], and malware [3, 24, 29]. In response,
adversaries continually modify their behavior to avoid being
detected. As a result, machine learning models that work
well on historical data may work very poorly in practice as
adversaries ﬁnd and exploit their weaknesses [17, 38]. For
example, spammers regularly modify their spam messages to

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
AISec’15, October 16 2015, Denver, Colorado, USA
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3826-4/15/10 ...$15.00
DOI: http://dx.doi.org/10.1145/2808769.2808778

make them appear less spammy to a classiﬁer while remaining
as persuasive as possible.

In adversarial domains such as these, we would prefer to
use classiﬁers that are harder to evade. Previous analysis has
shown that linear classiﬁers with independent features are
relatively easy to defeat [20, 25, 26]. When the parameters
of the classiﬁer are known, the adversary can identify the
most inﬂuential features. For classifying spam messages,
many of the features are individual words. By removing
the “spammiest” words and adding the “hammiest” words, a
spammer can disguise an email with a relatively small number
of modiﬁcations. When the parameters of the classiﬁer are
unknown, good features to modify can be guessed using
background knowledge [26], the classiﬁer’s training data or
similar data from the same distribution [5, 36], or through
direct interaction with the classiﬁer [25]. These attacks are
surprisingly eﬀective against realistic spam ﬁlters. Nelson
et al. [28] demonstrate that convex-inducing classiﬁers with
continuous features are also vulnerable, at least in theory.

Given the vulnerability of linear classiﬁers to these attacks,
researchers have proposed using other classiﬁers instead. Jor-
gensen et al. [22] introduce a multiple instance learning
strategy designed to be more robust to “good word” attacks,
in which only “hammy” words are added. However, it is not
clear how well this idea generalizes to other problems, other
sets of features, or other evasion techniques, including remov-
ing “spammy” words. A more general alternative is to use a
classiﬁer that exploits sequence information to produce pre-
dictions that are more accurate and harder to defeat. Bratko
et al. [7] propose compression-based classiﬁers in general and
prediction by partial mapping (PPM) in particular as a more
eﬀective and robust approach to spam classiﬁcation.

Compression-based classiﬁers represent the data as a se-
quence of overlapping features rather than as a set of inde-
pendent features. This means that the eﬀect of adding a
word to a message could depend on where in the message it is
added — the same word could be more or less eﬀective at the
beginning, end, or somewhere in the middle of the message.
Thus, evading a compression-based classiﬁer is more compli-
cated than evading a linear classiﬁer. However, there has
not yet been a systematic study of how compression-based
classiﬁers could be evaded.

In this paper, we formally deﬁne the problem of evading
a compression-based classiﬁer, present three methods for
solving the problem, and evaluate them on a standard email
spam dataset. All three methods are based on a single key
assumption that an adversary has an access to a sample of
legitimate and spam messages that has the same statistical

69properties as the sample that was used to train the classiﬁer.
Our methods ﬁnd a distribution of sequences that can be
appended to help disguise any instance. This is more eﬃcient
than trying to disguise each instance separately. It is also
much less brittle than appending a single, ﬁxed sequence,
which could be quickly learned by the classiﬁer.

Empirically, we ﬁnd that the median spam can be disguised
by increasing its length by just 11%. This suggests that, like
linear classiﬁers, compression-based classiﬁers are indeed
vulnerable to evasion techniques.

2. RELATED WORK

Research in adversarial machine learning has shown that
linear spam ﬁlters are succeptible to “good word attacks” in
which an attacker evades a spam ﬁlter by appending non-
spammy words to a spam email [26]. This attack can be
generalized into the problem of ﬁnding a minimal or near-
minimal set of changes to transform a malicious instance into
one that is labeled as innocent [25]. In follow-up work, other
researchers have developed eﬃcient evasion attacks against
convex-inducing classiﬁers [28] and certain combinations of
linear classiﬁers [32].

Robust learning algorithms can reduce the vulnerability
of linear classiﬁers, so that attackers need to make larger
modiﬁcations in order to evade detection [9, 10, 15, 35]. Other
researchers have proposed non-linear spam ﬁlters that are
speciﬁcally robust to good word attacks, such as multiple
instance learning [22] and compression-based classiﬁers [8].
The authors demonstrate that these methods are robust
to good word attacks, but do not fully explore what other
attacks might be possible against them.

Compression-based text classiﬁcation rests on the assump-
tion that two documents are likely to be of the same kind
if they compress well together. This approach has been
widely studied and applied for categorizing diﬀerent types of
text [6–8, 14, 18, 19, 23, 39]. We focus on entropy-based classi-
ﬁers that use cross entropy as a similarity measure. However,
our work is mostly agnostic to how the classiﬁer’s parame-
ters are estimated. For evaluation, we use the prediction by
partial matching (PPM) algorithm [12, 13, 27, 34], which has
proven eﬀective for a special case of text classiﬁcation— spam
ﬁltering [7, 8]. While a signiﬁcant amount of eﬀort has been
applied to studying the eﬀectiveness of compression models
on selected applications of text classiﬁcation, there is still no
complete understanding of how robust such algorithms are
to diﬀerent kinds of adversarial noise.

3. COMPRESSION-BASED CLASSIFIERS
3.1 Preliminaries

Let X ⊆ A
∗

be a space of arbitrary text strings over some
ﬁnite alphabet A. On this space, we consider sources or
classes of strings that are deﬁned by probability distributions
over the set X. In particular, from now on, whenever we
discuss a classiﬁcation problem, we assume that there exists
a single input source of strings from X that come on the
input of the classiﬁer.
The input source is described by the probability g(x) ≡
P (ξ = x) assigned to values x ∈ X of the discrete random
variable ξ standing for the input strings. The classiﬁer recon-
structs the probability distributions f (κ)(x) corresponding
to one or more classes κ. Formally, we deﬁne the proba-

bility f (κ)(x) ≡ P (ξ = x | C (κ)) for C (κ) being the event
{ξ belongs to class κ}. In this work we concentrate on the
case of two classes of strings: legitimate Ham messages and
unsolicited Spam messages that are designated with κ = H
and κ = S, respectively.
3.2 Finite-Memory Markov Model

The probability of a string x ∈ X originating from a class

κ is equal to

f (κ)(x) =

|x|(cid:89)

l=1

P (xl | xl−1

1

, κ),

(1)

1

1 = xl−1
|x|(cid:89)

l=1

where xl is the l-th character of the string x, and xl
k is the
substring of x from the k-th up to the l-th character (if k > l,
k is empty). For the sake of brevity, P (xl | xl−1
xl
, κ) stands
for the probability P (ξl = xl | ξl−1
, C (κ)) of character
xl following the context xl−1
. Naturally, we can parametrize
distributions f (κ)(x) using these probabilities:

1

1

f (κ)(x) = f (x, θ(κ)) =

θ(κ)
i(xl−1

1

,

), j(xl)

(2)

1

1 = ci ∈ A
∗

where i(xl−1
context xl−1
some orderings on the sets A and A
are the probabilities P (ξl = aj | ξl−1

) and j(xl) denote the ordinal numbers of the
and the character xl = aj ∈ A for
, and parameters θ(κ)
ij

1 = ci, C (κ)).

∗

From this point on, we will also assume that each class κ
can be modeled as a stationary and ergodic Markov chain
which memory is bounded by certain order K ≥ 1. Under the
assumption that limited memory K is suﬃcient for evaluating
probability (2), we can rewrite it for our convenience as

f (x, θ) =

θi(xl−1

l−K ), j(xl) =

nij (x)
ij

,

(3)

|x|(cid:89)

l=1

(cid:89)

θ
ci∈AK
aj∈A

for nij(x) being the number of times character aj follows
context ci in string x (i.e., substring ciaj occurs in x), where

nij(x) = |x|, and

θij = 1 for all ci ∈ AK .

(4)

(cid:88)

aj∈A

(cid:88)

ci∈AK
aj∈A

This way, any string x and any class κ are viewed as sets
of overlapping (K + 1)-grams with frequencies nij(x) and
conditional probabilities θij, respectively.

Reasoning completely analogously for the probability g(x),

we obtain the same parametrized form:

g(x, τ ) =

.

(5)

(cid:89)

nij (x)
τ
ij
ci∈AK
aj∈A

To avoid confusion, we use the letter τ to denote the vector of
parameters of the input source as distinguished from vectors
of class parameters θ(κ).
3.3 Classiﬁcation Problem

The above parametrization, following from the ﬁnite-me-
mory Markov model, allows us to view the mathematical
problem of inferring a class model as the optimization prob-
lem in the space of parameters:

R(θ) = Eξ

,

(6)

(cid:2)r(ξ, θ)(cid:3) → min

θ

70for some measure function r(ξ, θ) evaluating the “loss” or
“penalty” of classifying message ξ as belonging to the class
described by the probability distribution with parameters
θ. In other words, the objective of the problem (6) for each
class κ is to ﬁnd parameters θ(κ) giving the least losses on
average according to r(ξ, θ(κ)). The expectation is taken
over the probability distribution g(x) of strings ξ from the
input source. Generally, probability g(x) is supposed to be
unknown for all classes. For this reason, a version of the
problem (6) for empirical averaging is considered:

r(xk, θ) → min

θ

,

(7)

(cid:98)R(θ) =

(cid:88)

xk∈T

where T stands for a training sample of messages correspond-
ing to the class in question. Hereinafter, for consistency,
training samples of Ham and Spam classes are labeled as
T (H) and T (S) accordingly.

When the inference problem is solved and the vectors of
parameters θ(H) and θ(S) are estimated for each class, they
can be used to make classifying decision based on the same
principle of least loss:

(cid:40)

q(x, θ) = r(x, θ(H)) − r(x, θ(S)),
if q(x, θ) < α(H);
if q(x, θ) ≥ α(S).

κ(x) =

H,
S,

(8)

(9)

In case of α(H) ≤ q(x, θ) < α(S), additional measures are
needed to decide the class (e.g., increasing the length of the
message in question). Most commonly, both parameters are
set the same value, α(H) = α(S) = α. The choice of parameter
α is guided by the number of type I and type II errors.
3.4 Entropy Classiﬁcation

Let us consider the measure
r(ξ, θ) = − 1

|ξ| log f (ξ, θ).

(10)

Then, as it is obvious from the above deﬁnitions, the general
criterial function (6) specializes to the cross entropy

(cid:21)

(cid:20) 1
= −(cid:88)
|ξ| log f (ξ, θ)
|x| g(x) log f (x, θ) → min
1

θ

x∈X

.

(11)

R(θ) = H(θ) ≡ −Eξ

We will refer to this specialization of the problem (6) as the
classiﬁer problem. Similarly, empiric version (7) becomes

(cid:98)R(θ) = (cid:98)H(θ) ≡ −(cid:88)

xk∈T

1

|xk| log f (xk, θ) → min

θ

,

(12)

where T , of course, is assumed to be a sample of strings
distributed according to g(x).

Decision rule (9) can be rewritten as

q(x, θ) =

κ(x) =

(cid:40)

1
|x| log
H,
S,

f (S)(x)
f (H)(x)

,

if q(x, θ) < α;
if q(x, θ) ≥ α.

(13)

(14)

In practice, parameter α is often set to zero.

It is well known that if the function g(x) is given and

f (x, θ) > 0 for all x such that g(x) > 0, then

f (x, θ) ∝ g(x)
|x|

(15)

is an exact solution of the problem (11). Because both f (x)
and g(x) can be parametrized identically, at least in the case
when all texts x have the same length (or the variation in
lengths can be neglected), f (x, θ) can be constructed from
g(x, τ ) by letting θ = τ . The parameters τ , in turn, can
be directly found by estimating conditional probabilities
P (xl | xl−1

l−K ) on some training sample T .

This observation forms the basis of prediction by partial
matching (PPM). Aside from diﬀerences in strategies of
approximating probabilities for character-context pairs that
do not occur in a given sample, PPM works as a simple
frequency estimator, setting

θij ≈ Nij
Ni

,

for Nij =

nij(x), Ni =

Nij.

(16)

(cid:88)

x∈T

(cid:88)

aj∈A

For versions of PPM estimators and the details of their
implementation, see [12, 13, 27, 34].

4. ADVERSARIAL ATTACKS
4.1 Problem Deﬁnition

As we have seen above, in the classiﬁer problem (11), the
goal was to ﬁnd an optimal statistical model f (x, θ) for
messages of some class, given a ﬁxed input source induced
by probabilities g(x) observed through a sample T . More
strictly, the function g(x) was ﬁxed (although unknown),
while the probability distribution f (x, θ) was known up to
the vector of parameters θ.

It is of interest to consider the inverse problem statement
where, given ﬁxed statistical model f (x, θ) of some class, it
is required to ﬁnd the source distribution g(x) which is the
most favorable for certain classiﬁcation outcome. In this
setting, g(x) becomes the function in question, while f (x, θ)
is ﬁxed through known parameters θ.

One example of such inverse objective is the problem of
determining g(x) generating messages that are as close to
Ham messages as possible in terms of probability of passing
the spam ﬁlter. Another version of the problem that also
falls into this category is the following adversary problem
(or, in case of spam ﬁltering, the spammer problem). For
a given string z from some set of base messages Z, ﬁnd
probability distribution for generating strings xt, such that
after a certain combining transformation ψ(z, xt) they satisfy
some statistical requirement, e.g., being classiﬁed as Ham on
average. This setting is especially practical for a spammer
when z by itself has low chances of passing the ﬁlter.

To state the spammer problem more formally, we will
assume that there is a generator algorithm which plays the
role of a source of strings xt(τ ) for a speciﬁed vector of
parameters τ . Strings xt(τ ) are considered to be generated
randomly and independently, and have the same distribution
in the space of strings X. These strings are then used to
obtain new messages ut = ψ(z, xt(τ )) from a given base
message z according to the predetermined transformation
ψ.
In general, the function ψ(z, x) can associate a pair
of strings with any string. One such transformation that
is simple but still keeps the problem non-trivial is string

71where nij(x) is, as usual, the number of occurrences of a
substring ciaj in x, and

(cid:88)
(cid:88)

aj∈A

ni(x) =

Zi(w) =

nij(x),

exp(wij).

aj∈A

(cid:32)
wijnij(x) −(cid:88)

nij(x) log

exp(wij)

Zi(w)

ci∈AK
aj∈A

(cid:88)
(cid:88)

ci∈AK
aj∈A

=
ci∈AK
aj∈A

(cid:18)

(24)

(25)

(cid:33)

nij(x) log Zi(w), (26)

Since

log g(x, τ (w)) =

0 < τij < 1, and

τij = 1

(22)

Eξ

(33)

concatenation, ψ(z, x) = zx. Even though our method does
not suﬃciently depend on any particular transformation, for
illustration purposes, we will be using concatenation as the
transformation ψ.

The objective of the inverse problem has the same form:

1

|x| g(x, τ ) log f (x) → min

τ

,

(17)

G(τ ) ≡ −(cid:88)

x∈X

(cid:88)

but with the optimization being done for the parameters τ
of the source distribution g(x, τ ), not the class distribution
f (x, θ) = f (x). The decision to search in the parametrized
space of distributions g(x, τ ) is justiﬁed by the necessity to
obtain a generative (rather than discriminative) model of the
desired message source.

As in the case of the classiﬁer problem (11), it is well known
that, in the non-parametrical form, the inverse problem (17)
also has an analytical solution. Any function g(x) such that

x∈Xfmax

g(x) = 1, where Xfmax = arg max
g(x) = 0 for all x ∈ X \ Xfmax ,

x

f (x)
|x| ,

(18)

(19)

minimizes the cross entropy for a given f (x). These solutions
for the non-parametric problem, however, does not solve the
spammer problem. None of the functions g(x) satisfying the
above properties is guaranteed to be represented in the space
of parametrized functions g(x, τ ) which makes them useless
for generating xt(τ ). Moreover, even if this diﬃculty did
not exist, the diversity of the generated messages would be
extremely low, because any of such g(x) leads to generating
the same few messages from Xfmax over and over again which
makes spammer easily detectable.

Empirical analog of the criterion (17) is

(cid:98)G(τ ) ≡ (cid:88)

xk∈T

1

|x| g(xk, τ ) → min

τ

,

(20)

where a sample T is obtained from the distribution P (ξ =
x) ∝ log 1
f (x) . Therefore, in order to approach the inference
problem in the form (20), it is necessary to have an auxiliary
instrumental sample which, unlike training samples for the
classes or the combined sample for the input source, cannot
be observed in practice.
4.2 Instrumental Sampling Approach
Let us introduce new parameters wij such that

(cid:80)

τij =

exp(wij)
aj∈A exp(wij)

,

(21)

where, as before, subscripts i and j correspond to some con-
text ci ∈ AK and character aj ∈ A, respectively. For any
values of wij, the required conditions on τij hold automati-
cally:

(cid:88)

aj∈A
(0 ≤ τij ≤ 1, if wij = ±∞ are allowed).

For the new parameters, probability (5) changes to

(cid:33)(cid:33)ni(x)

(cid:32)

(cid:89)

ci∈AK

(cid:32) (cid:88)

aj∈A

g(x, τ (w)) =

1
Zi

exp

wij

nij(x)
ni(x)

,

(23)

it is clear that

∂g(x, τ (w))

∂wlk

for

= g(x, τ (w))

= g(x, τ (w)) nl(x)(cid:0)(cid:98)τlk(x) − τlk(w)(cid:1),

nlk(x) − nl(x)
Zl

exp(wlk)

nlk(x)
nl(x)

.

(cid:98)τlk(x) =
(cid:2)F (ξ, w)(cid:3) ≈(cid:88)

xk∈T

Eξ

Now, consider the problem

F (xk, w) → min

w

,

(29)

(cid:19)

(27)

(28)

where both the random variable ξ(w) and strings xk from the
instrumental sample T are distributed according to the prob-
abilities g(x, τ (w)). The problem (20) that has motivated us
to consider this approach is a special case for

F (x, w) = F (x) =

1
|x| log

1

f (x)

.

(30)

∂wlk

(cid:20) ∂F (ξ, w)

Taking derivatives of (29) and keeping in mind (27), we
obtain the following necessary condition of extremum:

Eξ
= 0, (31)
for all cl ∈ AK , ak ∈ A. In the case when function F (x, w) =
F (x) is independent of parameters, it simpliﬁes to

+ F (ξ, w) nl(ξ)(cid:0)(cid:98)τlk(ξ) − τlk(w)(cid:1)(cid:21)
(cid:104)
F (ξ) nl(ξ)(cid:0)(cid:98)τlk(ξ) − τlk(w)(cid:1)(cid:105)

(32)
Since ξ ∼ g(x, τ (w)), as the size of instrumental sample T

grows, frequencies(cid:98)τlk(x) converge to the current estimations

τlk(w) that were used to generate the sample in the ﬁrst
place. Therefore, attempts to iteratively optimize (29) will
turn into random walks around initial values of wij.

= 0.

Eξ

Moreover, for many practical generation procedures,

(cid:2)(cid:98)τlk(ξ)(cid:3) = τlk(w).

In a simpliﬁed case of both F (x) and nl(x) being indepen-
dent of parameters w, which takes place when, for example,
generation procedure stops after reaching some ﬁxed length
of x, the equation (32) simply degenerates, and the problem
becomes meaningless.

If the function F preserves some dependence on param-
eters — either in the general form F (x, w), or in a weaker

72variant F (x(w)) — the problem (32) is not strictly meaning-
less. However, for suﬃciently long samples, as the diﬀer-

ence |(cid:98)τlk(ξ) − τlk(w)| approaches zero, the inﬂuence of the

F (x, w)-multiplier becomes eﬀectively eliminated making the
expectation (32) almost independent of F .

For these reasons, we do not regard approaching prob-

lem (17) in form (29) as promising.
4.3 Importance Sampling Approach

Formally, we consider a vector of parameters τ to be a

solution to the inverse problem, if
FD[q(u, θ)] ≡ F [q(u, θ) | u = ψ(z, x), x ∈ D] → max
, (34)
where D is a set of text strings, and F(·) is an ensemble
operation deﬁned on the domain D. For example, the domain
D might be the set of all strings of some bounded length,
or some subset of that set. An empirical sample of strings
produced by the adversarial generator can also be taken as
a domain Dτ = {xt(τ )}t.

τ

The choice of ensemble operation depends on what criterion
of success aligns best with the goals of the spammer in a
particular problem setting. In this work, we consider the
following two cases.

(a) The average logarithmic ratio of probabilities q(u, θ)

estimated over a sample Dτ is minimal:

FDτ [q(u, θ)] = −(cid:88)

q(u, θ) → max

τ

.

(35)

(b) Empirical frequency of passing the spam ﬁlter success-

1
|Dτ|

fully estimated over a sample Dτ is maximal:
1(H)(u) → max

FDτ [q(u, θ)] =

,

x∈Dτ
where 1(H)(u) ≡ 1[q(u, θ) < α].
4.3.1 Entropy-Based Criterion
Empirical criterion (35) is equivalent to the problem

τ

(36)

(cid:2)q(u, θ)(cid:3) → min

, (37)

τ

R(τ | z) ≡ (cid:88)

x∈X

q(u, θ) g(x| z, τ ) = Eξ

where the expected value is taken over the probability dis-
tribution g(x | z, τ ) of text x being generated for the base
string z and parameters τ .

Let us now rearrange the sum in (37) using the well-known

technique of importance sampling:

(cid:88)

(cid:34)

R(τ | z) =

p(κ) E(κ)

γ(κ) q(ξ | z, θ)

(cid:2)W (κ)(ξ | z, θ) g(ξ | z, τ )(cid:3) → min

g(ξ | z, τ )
p(κ) f (κ)(ξ)

ξ

κ∈{H,S}

= Eξ,κ

x∈Dτ

(cid:88)

(cid:35)

τ

,

(38)

(39)

for

W (κ)(x | z, θ) =

γ(κ) q(x | z, θ)
p(κ) f (κ)(x)

,

where for each class κ ∈ {H, S}, expected value E(κ)
[·] denotes
conditional expectation Eξ[·| ξ ∼ f (κ)(x)], p(κ) stands for
the a priori probability of the class κ, and γ(H), γ(S) are
arbitrary splitting weights such that γ(H) + γ(S) = 1 (for
example, γ(H) = γ(S) = 1

2 , or γ(H) = p(H), γ(S) = p(S)).

ξ

In this problem setting, all statistical information that can
be available to the adversary — that is, both samples T (H)
and T (S) of Ham and Spam messages — is used:

R(τ | z) ≈ (cid:98)R(τ | z) =

(cid:88)

W (κk)(xk | z, θ) g(xk | z, τ ).

(40)

(xk,κk)∈T

(cid:34)

Here κk are true labelings of messages xk from the sample
T , which is the union of samples T (H) and T (S) drawn from
the distributions f (H)(x) and f (S)(x), respectively.

From the necessary condition of extremum,
∂g(ξ | z, τ )

∂

R(τ | z) = Eξ,κ

W (κ)(ξ | z, θ)

= 0. (41)

∂τij

∂τij
Since it has the form E[· ] = 0, we may apply the method
of stochastic optimization [30]. Switching to the parameters
wij that were introduced in (21), we obtain the algorithm
ij − γtW (κk(t))(z, xk(t)) g(xk(t) | z, τ (w(t)))

= w(t)

w(t+1)

ij

(cid:35)

· ni(xk(t) | z)(cid:0)(cid:98)τij(xk(t) | z) − τij(w(t))(cid:1),
∞(cid:88)

∞(cid:88)

γt = ∞,

t < ∞,
γ2

t=0

t=0

(42)

(43)

where γt ≥ 0 is a sequence satisfying the properties

and xk(t), κk(t) run through the sample T in some order
deﬁned by k(t) (potentially repeatedly).
4.3.2 Probability-Based Criterion
Objective function (36) is nothing else but an empirical

1(H)(u) g(x | z, τ ) = Eξ

(44)

(cid:2)1(H)(u)(cid:3),

version of the criterion

R(H)(τ ) ≡ (cid:88)

x∈X

where ξ ∼ g(x | z, τ ) and, as previously, g(x | z, τ ) is gen-
erational probability distribution for a base string z and
parameters τ . This criterion, in turn, makes the problem be
equivalent to maximizing the probability of the transformed
message ψ(z, ξ) passing the spam ﬁlter:

R(τ ) = Pr(cid:2)1(H)(ψ(z, ξ)) | z, τ(cid:3) → max
R(S)(τ ) ≡ (cid:88)

1(S)(ψ(z, x)) g(x | z, τ ),

τ

x∈X

.

(45)

(46)

As we have only two classes, maximization of the crite-
rion (44), is equivalent to minimization of the dual criterion

for 1(S)(u) = 1 − 1(H)(u). Combining (44) and (46), we have

R(τ ) ≡ γ(H)R(H)(τ ) − γ(S)R(S)(τ ) → max

τ

,

(47)

where γ(H) + γ(S) = 1 are some splitting weights. Let us now
consider this problem in the context of both supervised and
unsupervised learning.

Supervised Learning.

Formally rearranging the criterion function (47) into two
sums and applying the importance sampling for the distribu-
tion of the pair (ξ, κ), we see that

R(τ ) =

γ(H)1(H)(ψ(z, x)) − γ(S)1(S)(ψ(z, x))

g(x | z, τ )

(cid:17)

(cid:16)
(cid:88)

x∈X

73p(κ)(cid:88)
(cid:88)
(cid:2)W (κ)(z, ξ) g(ξ | z, τ )(cid:3) → max

x∈X

=
κ∈{H,S}

= Eξ

W (κ)(z, x)f (κ)(x) g(x | z, τ )

,

(48)

τ

where the variable ξ is distributed according to f (κ)(x), and

W (κ)(z, x) =

γ(H)1(H)(ψ(z, x)) − γ(S)1(S)(ψ(z, x))

f (κ)(x)

1(H)(ψ(z, x)) − γ(S)

=

(49)
Assuming that a sample of messages xk ∈ T is available
together with the true labeling of classes κk = κ(xk), the
criterion R(τ ) can be estimated as

f (κ)(x)

.

W (κk)(z, xk) g(xk | z, τ ).

(50)

R(τ ) ≈ (cid:98)R(τ ) ≡(cid:88)

(xk,κk)∈T

(cid:104)

For the parameters wij from (21), we obtain the following

necessary condition of extremum:

∂R(w)
∂wij

= Eξ

W (κ)(z, ξ) g(ξ | z, τ (w)) ni(ξ | z)

·(cid:0)(cid:98)τij(ξ | z) − τij(w)(cid:1)(cid:105)

where (cid:98)τlk(x) is deﬁned as in (28), and ni(x | z), nij(x | z)

stand for the number of occurrences of the context ci ∈ AK
alone and followed by the character aj ∈ A, respectively,
in the text x. The corresponding stochastic approximation
algorithm takes the form

= 0,

(51)

w(t+1)

ij

= w(t)

ij + γtW (κk(t))(z, xk(t)) g(xk(t) | z, τ (w(t)))

· ni(xk(t) | z)(cid:0)(cid:98)τij(xk(t) | z) − τij(w(t))(cid:1).

(52)

Unsupervised Learning.
In the case when true labelings κk of sampled messages
xk ∈ T are unknown, we can alternatively apply importance
sampling for the distribution

Then,

R(τ ) = Eξ

f (x) = p(H)f (H)(x) + p(S)f (S)(x).

(cid:2)W (z, ξ) g(ξ | z, τ )(cid:3) → max

(53)

,

(54)

τ

where ξ is distributed in accordance with f (x), and
γ(H)1(H)(ψ(z, x)) − γ(S)1(S)(ψ(z, x))

W (z, x) =

f (x)
1(H)(ψ(z, x)) − γ(S)

p(H)f (H)(x) + p(S)f (S)(x)

.

(55)

=

(cid:88)

nij(u) log τij.

(56)

= − 1
|u|

ci∈AK
aj∈A

(cid:2)H(u | τ )(cid:3) = −(cid:88)

ci∈AK

(cid:88)

aj∈A

Averaged over random transformed messages u, it is equal to

H(τ ) = Eu

pi Hi(τ ),

(57)

where

Hi(τ ) =

pj|i log τij,

(58)

pi is the probability of the context ci occurring in a random
transformed message u, and pj|i is the conditional probability
of the character aj occurring in u after the context ci. On a
single message u, Hi(τ ) can be estimated as

Hi(τ ) ≈ (cid:98)Hi(u | τ ) =

(cid:88)

aj∈A

nij(u)
ni(u)

log τij.

(59)

Assuming a sample T of messages x ∈ X is available, we
can split T into auxiliary samples depending on the class
κ ∈ {H, S} to which u = ψ(z, x) is assigned by the classiﬁer:

T (κ) = {xk ∈ T | 1(κ)(ψ(z, xk) | θ) = 1},

(60)
for 1(H)(x | θ) = 1[q(x, θ) < α], and 1(S)(x | θ) = 1[q(x, θ)≥ α].
That is, T (H) and T (S) consist of messages xk ∈ T that make
the base message z being recognized as Ham and Spam,
respectively.

(cid:98)H(u | τ ) to the estimates over samples T (H) and T (S):

Considering these samples, we can generalize the estimate

(cid:88)

(cid:88)

xk∈T (κ)

ci∈AK

pi (cid:98)Hi(ψ(z, xk)| τ ). (61)

R(κ)(τ | z) = − 1

|T (κ)|

Then, we can state our goal in a new way: Find parameters
τ such that the entropy estimate R(H)(τ | z) becomes low,
while the estimate R(S)(τ | z) remains high. One way to
achieve both of these goals is to formalize them as a problem
of minimizing the diﬀerence of the above objective functions:

R(τ ) = R(H)(τ | z) − R(S)(τ | z) → min

τ

,

(62)

subject to usual normalization requirements on τ .

Substituting the entropy estimation (59) deﬁnition into

the criterion (62), we have:

R(τ | z) = −(cid:88)

pi
ci∈AK

(cid:18) 1

|T (H)|

− 1

(cid:88)
(cid:98)Hi(u | τ )
(cid:88)
(cid:98)Hi(u | τ )

uk∈U (H)

= −(cid:88)

(cid:88)

|T (S)|
ij − ν(S)
(ν(H)

uk∈U (S)

ij ) log τij,

pi
ci∈AK

aj∈A

(cid:19)

(63)

(64)

(65)

Since the criteria (54) and (48) diﬀer only in deﬁnitions
of the weights W (z, x) which are independent of wij, the
resulting stochastic optimization algorithm is exactly the
same as in (52) (again, up to diﬀerences between W (z, x)
and W (κ)(z, x)).
4.4 Likelihood-Based Criterion

for

Let us again consider a transformation u = ψ(z, x) of a base
message z with an arbitrary string x. Entropy per character
of the resulting string u can be estimated empirically as

(cid:33)

(cid:32) |u|(cid:89)

l=1

U (κ) = {ψ(z, xk) | xk ∈ T (κ)},
nij(uk)
ν(κ)
ij =
ni(uk)

|T (κ)|

(cid:88)

1

.

uk∈U (κ)

H(u | τ ) = − 1

|u| log

g(ul | ul−1

l−K , τ )

Since parameters τij occur only in summands for the context
ci, optimization of (63) naturally falls into |A|K smaller

74problems:

Ri(τ | z) = R(H)

i

(τ | z) − R(S)

i

(τ | z) → min{τi },

where {τi } stands for τi1, τi2, . . . , τi|A|, and

(τ | z) = −(cid:88)

aj∈A

R(κ)

i

ν(κ)
ij

log τij.

Unlike optimization problems (37) and (44) induced by
the two approaches presented in Section 4.3, it is possible to
solve problem (66) analytically:

ij ) log τij,

Ri(τ | z) = −(cid:88)

Theorem 1. The criterion function
ij − ν(S)
(ν(H)
(cid:88)

aj∈A

τij ≥ 0 and

subject to constraints

τij = 1,

aj∈A

reaches its minimum value at

where

∗
ij =

τ

µij
µi

,

µij = max(cid:8)0, ν(H)

µi =

µij.

(cid:88)

aj∈A

(cid:9),

ij − ν(S)

ij

(66)

(67)

(68)

(69)

(70)

(71)

(72)

the models for each of the two classes, and then makes
classifying decisions depending on for which class entropy
per character is the minimal. We also implemented all three
of the algorithms proposed in Sections 4.3.1, 4.3.2, and 4.4.
Throughout all evaluations presented in this section, in
the spirit of the works [7, 8, 18], we worked with character-
based PPM models. Although it is possible to use a word-
based alphabet instead, switching to it vastly increases the
alphabet size (and, consequently, the number of parameters),
ignores diﬀerences in punctuation and spacing, and makes the
algorithm sensitive to tokenization. Furthermore, it does not
make the classiﬁer stronger: In our experiments, word-based
PPM spam ﬁlters have comparable or worse accuracy than
character-based ones (for the latter having greater context
sizes than the former, of course).

Our numerical experiments were organized as follows. For
each evaluation run, ﬁrst, a combined sample T of both
legitimate (T (H)) and spam (T (S)) messages was drawn out of
the SpamAssassin public corpus [2]. Each message in xk ∈ T
was accompanied with the true class labeling κk ∈ {H, S}.
The sample T was additionally temporarily split at random
in proportion seven to three into the training and testing
samples, correspondingly. The former was used to train the
classiﬁer, the latter was used to ensure that performance of
the classiﬁer is within the expected boundaries (as compared,
for example, to [7]). All of the spam messages in T that were
recognized as such according to the obtained class parameters
θ(H) and θ(S), were remembered and declared to be the set
of base messages Z.

Then, our algorithms (42), (52), and (70) were run on
the combined sample T in order to obtain transformation
parameters τ (E), τ (P), and τ (L). The ﬁrst two algorithms
based on the stochastic optimization were repeatedly run over
all pairs (zl, xk) ∈ Z × T , where the index k was incremented
ﬁrst. To control the convergence, after each pass over T
(i.e., every |T| iterations), the value of the criterion function
corresponding to the current algorithm was estimated using
a ten percent subsample of Z × T . This estimation together
with the total number of iterations performed by the moment
were used to make a stopping decision.
Once in a several passes over T (between |T| and 10|T|
iterations, depending on the size of the problem), the current
parameters τ (t) = τ (w(t)) were supplied to the Markov chain
generator. For each base message z ∈ Z, the generator
produced a continuation stream of characters distributed
according to the distributions g(x, τ (t)). The generation was

stopped when the string (cid:101)x of characters produced so far was
enough to get the transformed message u = ψ(z,(cid:101)x) = z(cid:101)x past
the classiﬁer’s spam ﬁlter. If the length of (cid:101)x exceeded 20|z|,
a thousand of continuations (cid:101)x were generated to estimate
a secondary evaluation measure — the average length of (cid:101)x

the generator was forcefully stopped. This way, for each z,

required to make z legitimate to the classiﬁer.

The third algorithm (70) required less work since it pro-
vides the analytical solution as long as the values µij are
calculated. To do so, a single pass of averaging nij(ψ(zl, xk))
over the samples Z × T (H) and Z × T (S) was done. After
that, the same generation procedure described above was
run, so there was an auxiliary measure for comparing this
algorithm with the other two and the baseline strategy.

The role of the baseline strategy in our experiments was
played by the same generation procedure, but executed for
the vector of parameters τ (H) = θ(H) that were estimated

Proof of Theorem 1 is provided in Appendix.
4.5 Multiple Base Messages

Throughout this section, we have considered the method
for a single arbitrary base message z that is chosen before-
hand. However, with minor modiﬁcations, the presented
reasoning holds for the same criteria, but averaged over mul-
tiple base messages zl ∈ Z. Indeed, for both approaches
from Section 4.3 resulting in stochastic optimization, the
only change caused by averaging over Z is that the variable
z, like x, also runs over a sample on iterations:

= w(t)

· ni(xk(t) | zl(t))(cid:0)(cid:98)τij(xk(t) | zl(t)) − τij(w(t))(cid:1).

ij + γtW (κk(t))(zl(t), xk(t)) g(xk(t) | zl(t), τ (w(t)))
(73)

w(t+1)

ij

The same is true for the likelihood-based approach dis-
cussed in Section 4.4. If criterion (62) is averaged over a set
of base messages Z, the order of summation can be seamlessly
changed so that the new outer sum over zl ∈ Z, together
with the sum over uk ∈ U , is taken before the sum over
ci ∈ AK and aj ∈ A. Consequently, the resulting objective
function takes the same form (63) but for

ν(κ)
ij =

1

|Z||T (κ)|

nij(ψ(zl, xk))
ni(ψ(zl, xk))

.

(74)

(cid:88)

(cid:88)

zl∈Z

uk∈T (κ)

5. EVALUATION
5.1 Methodology

In order to validate the method proposed in this work,
ﬁrst we implemented the entropy classiﬁer for the problem
of spam ﬁltering. Following the deﬁnition of the problem
given in Section 3.4, our implementation uses PPM to learn

75Figure 1: Histograms of the length ratio |ψ(zl, xk)|/|zl| averaged over appendices xk generated for the order
K = 3 on 1% datasets using (a) the parameters τ (E) optimized for the entropy-based criterion (38), (b) the
parameters τ (P) optimized for the probability-based criterion (47), (c) the optimal parameters τ (L) (70) for the
likelihood-based criterion (63), and (d) the baseline parameters τ (H) estimated from θ(H).

Table 1: Accuracy of the classiﬁer on the full dataset

True class

Classiﬁed as

Ham

Spam

Ham
Spam

68.0% (1645)
1.5% (36)

0.5% (13)
30.0% (725)

during the training of the classiﬁer on the sample of legitimate
messages T (H). That same vector θ(H) also served as an initial
estimate for the stochastic optimization.
5.2 Results

Due to limited computational resources, during all evalua-
tion runs, Markov models’ memory was set to three characters
for both the classiﬁer and the adversary. In practice, entropy-
based spam ﬁlters demonstrate the best performance for the
Markov models of orders between six and eight characters [7].
However, even for K = 3 our implementation of the classiﬁer
based on the algorithm of prediction by partial matching has
error rate of approximately 2% on the SpamAssassin dataset.
Table 1 shows statistics for one run of the classiﬁer, when all
6046 bodies of email messages were randomly split into 3627
training and 2419 testing messages.

For K = 3, the space of parameters τ , representing con-
ditional probabilities of one-byte characters given a context
of at most K another one-byte characters, is bounded by

Table 2: Summary statistics for the performance of
diﬀerent generation parameters on 1% datasets

Index

Optimization based on

Entropy Prob. Likelih.

Ham
baseline

Averaged length ratio

Minimum
5% quantile
Median
Mean
95% quantile
Maximum

Failure rate (%)

1.21
1.29
1.58
1.65
2.26
2.89
0

1.15
1.26
1.49
1.59
2.35
3.14
0

1.16
1.26
1.52
1.57
2.13
2.61
0

1.32
1.52
2.06
2.13
3.27
4.27
0

2561 + 2562 + 2563 + 2564 ≈ 232. The number of charac-
ter-context combinations that actually occur in the whole
SpamAssassin dataset for K = 3 is approximately 524 000.

To avoid memory pressure and achieve faster convergence,
algorithms (42) and (52) requiring stochastic optimization,
were run on a series of small subsets of the original dataset.
Each time, approximately one percent of messages were
sampled at random from the full dataset. Let us present
evaluations for a typical run on a 1% dataset done for all
three algorithms, as described in the previous section.

The failure rate of the chosen concatenation-based trans-
formation was zero for all spam messages and parameters τ

11.522.533.5400.030.060.090.120.15AveragedlengthratioFrequency(a)Parametersτ(E)11.522.533.5400.050.10.150.2AveragedlengthratioFrequency(b)Parametersτ(P)11.522.533.5400.050.10.150.2AveragedlengthratioFrequency(c)Parametersτ(L)11.522.533.5400.030.060.090.12AveragedlengthratioFrequency(d)Parametersτ(H)76Table 3: Summary statistics for the performance of generation with parameters optimal for the likelihood-based
criterion on the full dataset for orders K from 3 to 10

Index

3

4

Averaged length ratio

Likelihood-based optimization
9

5

6

7

8

10

3

4

5

6

7

8

9

10

Ham baseline

Minimum
5% quantile
Median
Mean
95% quantile
Maximum

Failure rate (%)

1.00 1.01 1.01 1.02 1.02 1.02 1.01 1.01
1.04 1.04 1.04 1.05 1.05 1.04 1.04 1.04
1.08 1.08 1.08 1.08 1.08 1.07 1.07 1.07
1.11 1.10 1.10 1.09 1.09 1.08 1.08 1.07
1.33 1.27 1.23 1.19 1.17 1.15 1.14 1.13
2.63 1.56 1.51 1.39 1.27 1.46 1.66 1.84
0

0

0

0

0

0

0

0

1.00 1.01 1.04 1.04 1.02 1.03 1.02 1.05
1.25 1.34 1.40 1.45 1.48 1.51 1.52 1.54
1.70 1.75 1.79 1.81 1.82 1.85 1.87 1.89
1.99 1.99 1.98 1.96 1.94 1.95 1.95 1.96
4.14 3.73 3.38 3.12 2.93 2.82 2.72 2.68
8.50 7.53 7.81 7.94 5.44 6.19 7.72 6.19
0
0.16

0 0.01 0.01 0.01 0.01

0

ground) is followed by a few continuations generated ac-
cording to optimal parameters (highlighted with gray back-
ground). Any of the presented appendices (by itself) is
suﬃcient to make the corresponding spam message legiti-
mate for the classiﬁer trained on a 1% dataset, and cannot
be shortened without changing the class of the transformed
message back to spam.

Since the likelihood-based algorithm (70) does not have as
high computational requirements as the other two algorithms
resorting to stochastic optimization, it was possible for us to
run it on the full dataset, and also with higher values of the
Markov-chain order K. Resulting distributions of the length
ratio for the optimal parameters under the likelihood-based
criterion, and for the baseline Ham parameters are presented
in Figure 2. Table 3 lists the same ﬁve quantiles of the length
ratio as well as its mean values. Comparing these statistics
with the ones in Table 2, we conclude that the gap between
the Ham-like generation and the likelihood-based algorithm
is even greater on the full dataset.

6. CONCLUSIONS AND FUTURE WORK
We introduced three formalizations of possible adversarial
objectives for classiﬁers using cross-entropy as the decid-
ing criterion. Each of the three approaches has proved its
eﬃciency as compared to the baseline approach of using
the probability distributions estimated only on legitimate
messages to deﬁne the transformation source. The third
technique showed itself as the most eﬃcient of three, both
in terms of transformation and computational requirements.
Although the ﬁrst two techniques have some implementation
diﬃculties, after appropriate calibration, they showed compa-
rable performance. Together, all three methods have shown
the feasibility of attacking compression-based classiﬁers sta-
tistically using relatively limited extent of transformation.

Future work includes three directions of possible extension
of this research. To begin with, it is of interest to explore how
methods of parametrized optimization, examples of which
are considered in this work, compare with other algorithms
for optimizing the contents of transformation texts directly
on a per-character basis (e.g., genetic algorithms or Markov
chain Monte Carlo). This problem setting can potentially
increase the number of diﬀerent criterion functions that can
be considered to formalize the adversary problem.

Another promising direction consists in analyzing the dy-
namics of classiﬁer-adversary system for the particular case
of entropy-based classiﬁcation considered in this paper. Al-
though it is hard to attack this problem in general, it might

2:

of

the

Histograms

Figure
length ratio
|ψ(zl, xk)|/|zl| averaged over appendices xk generated
for the order K = 3 on the full dataset using (a) the
optimal parameters τ (L) (70) for the likelihood-based
criterion (63), and (b) the baseline parameters τ (H)
estimated from θ(H).

obtained from all three algorithms as well as the Ham base-
line τ (H). That is, it was possible to generate an appendix xk
for each base spam message zl such that their concatenation
uk = ψ(zl, xk) = zlxk was classiﬁed as legitimate. For this
reason, to compare performance for diﬀerent parameters, we
used a supplementary index of the ratio |uk|/|zl| for each
transformed message. Note that none of the methods pro-
posed in this work was constructed to directly optimize this
ratio of lengths.

Figures 1(a), 1(b), and 1(c) depict distributions of length
ratios averaged over transformation appendices xk gener-
ated according to the parameters τ (E), τ (P), and τ (L) that
were optimized for the entropy-based, probability-based,
and likelihood-based criteria, respectively. As it is evi-
dent from the shapes of the histograms, the algorithms us-
ing probability-based and likelihood-based criteria are more
preferable to the one using entropy-based criterion in terms
of the length ratio. However, comparing these graphs with
Figure 1(d), showing the histogram for the parameters τ (H),
we see that all three techniques provide a noticeably better
performance than the baseline of generating Ham-like appen-
dices. A short summary of the statistics from these ﬁgures
is given in Table 2.

Figure 3 shows several examples of generated transforma-
tion texts for a few short spam messages from the dataset.
Each of the original spam messages (typeset on white back-

12345600.20.40.6AveragedlengthratioFrequency(a)Parametersτ(L)(b)Parametersτ(H)77Hi we are luke ’ s secret f o l l o w i n g we love luke

(cid:44)→ f i c t i t i o u s !

We are also your long lost friend ! Hi

This email has nothing to do with l u k e f i c t i t i o u s . com

We wil be putting up our very own fan site soon
and wanted to let you know in advance !

Have a b e a u t i f u l l day !

Joseph

Regard E

_ _ _ _ _ _ _ _ _ _
Exm

Hey ,

I just wanted to tell you about a GREAT

http :// www . m e t r o j o k e s . com

F e a t u r e s lots

(cid:44)→ website .
(cid:44)→ of jokes !
(cid:44)→ in c a t e g o r i e s .

E x t r e m l y unique f e a t u r e s and c l a s s i f i e d

I a p p r i c i a t e your time .

Thank you

your loved one out of your typical diam

- No , the out their until your typi

from you ’ re decent ? I

ass , but the
sun doesn ’ t

fat able to be and w o n d e r f u l
>>

( s u d d e n l y s u s a n @ S t o o l m a i l . zzn . com ) on Tuesday , July

(cid:44)→ 30 , 2002 at 1 7 : 0 7 : 5 6

: Why Spend upwards of $4000 on a DVD Burner when we
(cid:44)→ will show you an a l t e r n a t i v e that will do the exact
(cid:44)→ same thing for just a f r a c t i o n of the cost ? Copy
(cid:44)→ your DVD ’ s NOW .

It spamassin - dev

--
" If you ."

This a multi - part , s u r r o u n d you ’ re du

This a must IM . Build
s e a r c h a r s e l f with think ?

This a deady to be looking of some merge . net

DON ’ T MISS OUT ON AN AMAZING B U S I N E S S O P P O R T U N I T Y

(cid:44)→ AND WEIGHT LOSS PRODUCT !

PLEASE VISIT www . good4u . a u t o d r e a m t e a m . com
THERE IS NO O B L I G A T I O N
AND IT ’ S WORTH A LOOK !

Remore
> OK guys -- I r

md : rules .
>
> with s m a r t _ 0 0 8 8

[ evel

Yet e m a i l e n a m e =" smime

> OK guys -- I reck_f

>
> BSMTP - support people
> OK guy

Figure 3: Examples of original spam messages zl
(white background) and several appendices xk cor-
responding to each zl that are generated for the
order K = 3 using parameters τ (E) optimized on a
1% dataset (gray background).

be feasible to derive some useful properties for the speciﬁc
algorithms that we have discussed in our work.

Finally, it is important to investigate ways of improving
robustness of compression-based classiﬁers, given the knowl-
edge of potential adversary attacks. For linear ﬁlters, it has
been shown that additional regularization [37] or frequent
retraining [26] can mitigate the severity of most attacks. The
research of whether such methods may also be adaptable to
compression-based classiﬁers is left for future work.
Acknowledgments
We thank the anonymous reviewers for helpful comments.
This research was funded by NSF grant IIS-1451453.

7. REFERENCES
[1] L. Akoglu, R. Chandy, and C. Faloutsos. Opinion fraud

detection in online reviews by network eﬀects. In
Proceedings of the 7th International AAAI Conference
on Weblogs and Social Media. AAAI Press, 2013.

[2] Apache SpamAssassin Project. The SpamAssassin

public mail corpus. Available at
https://spamassassin.apache.org/publiccorpus/, 2005.

[3] M. Bailey, J. Oberheide, J. Andersen, Z. M. Mao,

F. Jahanian, and J. Nazario. Automated classiﬁcation
and analysis of internet malware. In Recent Advances
in Intrusion Detection, pages 178–197. Springer, 2007.

[4] F. Benevenuto, G. Magno, T. Rodrigues, and

V. Almeida. Detecting spammers on Twitter. In
Proceedings of the 7th Annual Collaboration, Electronic
messaging, Anti-Abuse and Spam Conference, 2010.

[5] B. Biggio, I. Corona, D. Maiorca, B. Nelson, N. ˇSrndi´c,

P. Laskov, G. Giacinto, and F. Roli. Evasion attacks
against machine learning at test time. In Machine
Learning and Knowledge Discovery in Databases, pages
387–402. Springer, 2013.

[6] V. Bobicev and M. Sokolova. An eﬀective and robust
method for short text classiﬁcation. In Proceedings of
the 23rd AAAI Conference on Artiﬁcial Intelligence,
pages 1444–1445, 2008.

[7] A. Bratko, G. V. Cormack, B. Filipiˇc, T. R. Lynam,

and B. Zupan. Spam ﬁltering using statistical data
compression models. The Journal of Machine Learning
Research, 7:2673–2698, 2006.

[8] A. Bratko, B. Filipiˇc, and B. Zupan. Towards practical
PPM spam ﬁltering: Experiments for the TREC 2006
Spam Track. In Proceedings of the 15th Text REtrieval
Conference (TREC 2006), 2006.

[9] M. Br¨uckner and T. Scheﬀer. Nash equilibria of static
prediction games. In Advances in Neural Information
Processing Systems 22, 2009.

[10] M. Br¨uckner and T. Scheﬀer. Stackelberg games for

adversarial prediction problems. In Proceedings of the
Seventeenth ACM SIGKDD International Conference
on Knowledge Discovery and Data Mining. ACM Press,
2011.

[11] D. Chau, S. Pandit, and C. Faloutsos. Detecting

fraudulent personalities in networks of online
auctioneers. Knowledge Discovery in Databases: PKDD
2006, pages 103–114, 2006.

[12] J. G. Cleary and W. J. Teahan. Unbounded length

contexts for PPM. The Computer Journal,
40(2/3):67–75, 1997.

78[13] J. G. Cleary and I. H. Witten. Data compression using

[31] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz.

adaptive coding and partial string matching. IEEE
Transactions on Communications, 32(4):396–402, 1984.

[14] G. V. Cormack and R. N. S. Horspool. Data

compression using dynamic Markov modelling. The
Computer Journal, 30(6):541–550, 1987.

[15] O. Dekel, O. Shamir, and L. Xiao. Learning to classify
with missing and corrupted features. Machine Learning,
81(2):149–178, 2010.

[16] H. Drucker, D. Wu, and V. N. Vapnik. Support vector
machines for spam categorization. IEEE Transactions
on Neural Networks, 10(5):1048–1054, 1999.

[17] T. Fawcett. “In vivo” spam ﬁltering: A challenge

problem for KDD. SIGKDD Explorations, 5(2):140–148,
2003.

[18] E. Frank, C. Chui, and I. H. Witten. Text

categorization using compression models. In
Proceedings of DCC-00, IEEE Data Compression
Conference, pages 200–209. IEEE Computer Society
Press, Los Alamitos, US, 2000.

[19] J. Goodman, D. Heckerman, and R. Rounthwaite.
Stopping spam. Scientiﬁc American, 292(4):42–49,
2005.

[20] J. Graham-Cumming. How to beat an adaptive spam

ﬁlter. In MIT Spam Conference, 2004.

[21] N. Jindal and B. Liu. Opinion spam and analysis. In
Proceedings of the International Conference on Web
Search and Data Mining, pages 219–230. ACM, 2008.

[22] Z. Jorgensen, Y. Zhou, and M. Inge. A multiple

instance learning strategy for combating good word
attacks on spam ﬁlters. Journal of Machine Learning
Research, 9:1115–1146, 2008.

[23] A. Kantchelian, J. Ma, L. Huang, S. Afroz, A. Joseph,

and J. D. Tygar. Robust detection of comment spam
using entropy rate. In Proceedings of 5th ACM
Workshop on Artiﬁcial Intelligence and Security, pages
59–70. ACM Press, 2012.

[24] J. Z. Kolter and M. A. Maloof. Learning to detect and

classify malicious executables in the wild. Journal of
Machine Learning Research, 7:2721–2744, 2006.
[25] D. Lowd and C. Meek. Adversarial learning. In

Proceedings of the Eleventh ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, pages 641–647. ACM, 2005.

[26] D. Lowd and C. Meek. Good word attacks on

statistical spam ﬁlters. In Proceedings of the Second
Conference on Email and Anti-Spam, 2005.

[27] A. Moﬀat. Implementing the PPM data compression

scheme. IEEE Transactions on Communications,
38(11):1917–1921, 1990.

[28] B. Nelson, B. I. P. Rubinstein, L. Huang, A. D. Joseph,
S. J. Lee, S. Rao, and J. D. Tygar. Query strategies for
evading convex-inducing classiﬁers. Journal of Machine
Learning Research, 13:1293–1332, 2012.

[29] K. Rieck, P. Trinius, C. Willems, and T. Holz.

Automatic analysis of malware behavior using machine
learning. Journal of Computer Security, 19(4):639–668,
2011.

[30] H. Robbins and S. Monro. A stochastic approximation

method. The Annals of Mathematical Statistics,
22:400–407, 1951.

A Bayesian approach to ﬁltering junk e-mail. In
Proceedings of the AAAI-98 Workshop on Learning for
Text Categorization, Madison, WI, 1998. AAAI Press.
[32] D. Stevens and D. Lowd. On the hardness of evading

combinations of linear classiﬁers. In Proceedings on the
2013 ACM Workshop on Artiﬁcial Intelligence and
Security (AISec), Berlin, Germany, 2013. ACM Press.

[33] G. Stringhini, C. Kruegel, and G. Vigna. Detecting
spammers on social networks. In Proceedings of the
26th Annual Computer Security Applications
Conference, pages 1–9. ACM, 2010.

[34] W. J. Teahan. Probability estimation for PPM. In New

Zealand Computer Science Research Students’
Conference, 1995.

[35] C. H. Teo, A. Globerson, S. Roweis, and A. Smola.

Convex learning with invariances. In Advances in
Neural Information Processing Systems 20, 2007.
[36] Y. Vorobeychik and B. Li. Optimal randomized

classiﬁcation in adversarial settings. In Proceedings of
the 2014 International Conference on Autonomous
Agents and Multi-Agent Systems, pages 485–492.
International Foundation for Autonomous Agents and
Multiagent Systems, 2014.

[37] H. Xu, C. Caramanis, and S. Mannor. Robustness and
regularization of support vector machines. The Journal
of Machine Learning Research, 10:1485–1510, 2009.

[38] C. Yang, R. C. Harkreader, and G. Gu. Die free or live
hard? Empirical evaluation and new design for ﬁghting
evolving Twitter spammers. In Recent Advances in
Intrusion Detection, pages 318–337. Springer, 2011.
[39] Y. Zhou and W. M. Inge. Malware detection using

adaptive data compression. In Proceedings of 1st ACM
Workshop on Artiﬁcial Intelligence and Security, pages
53–60. ACM, 2008.

APPENDIX

Lemma 1. The objective function

Ri(τ ) = −(cid:88)
(cid:88)

j∈J

τij ≥ 0 and

j∈J

νij log τij,

(75)

where weights νij ≥ 0 for any j ∈ J, and parameters τij are
subject to constraints

τij = s > 0,

(76)

reaches its minimum value at

where

νi =

∗
τ
ij = s

νij
νi

,

(cid:88)

j∈J

νij.

(77)

(78)

Proof. Considering that log  ≤ ( − 1) for any  > 0, we

see that for an arbitrary vector of parameters τ ,

∗

Ri(τ

) − Ri(τ ) = −(cid:88)
(cid:88)

j∈J

=

νij log

τijνi
sνij

j∈J

νij log

≤(cid:88)

j∈J

(cid:88)
(cid:18) τijνi

j∈J

+

sνij
νi

νij

sνij

(cid:19)

− 1

νij log τij

79νij = 0,

(79)

Clearly, the problem of ﬁnding optimal τij can be solved

νij log

1
τij

,

(80)

conditions of Lemma 1 hold for J = J +
s = s+

i . Hence, the function R+

i (τ ) is minimized for

i , νij = δij, and

(cid:88)

j∈J

τij −(cid:88)

j∈J

=

νi
s

Ri(τ ) = −(cid:88)
(cid:88)

τij ≥ 0 and

j∈J

j∈J

 s

|J min
0,

i

∗
τ
ij =

by deﬁnition of νi and requirements (76). From the obtained
relation it follows that Ri(τ

) ≤ Ri(τ ) for any τ .

∗

Lemma 2. The objective function

where weights νij ≥ 0 for any j ∈ J, and parameters τij are
subject to constraints

τij = s > 0,

(81)

reaches its minimum value at

| ,

if j ∈ J min
if j ∈ J \ J min

;

i

i

(82)

;

s

∗
ij(ε) =

τ

i

sε,

i = J.

unless J min

We can assume that(cid:80)
(ε)) = −(cid:88)
−(cid:88)

Ri(τ

Ri(τ

∗

∗

i

where J min

i = {j ∈ J | νij = minj∈J νij}.

Proof. Let us consider the following values of parameters
under the temporary assumption that τij ≥ ε for some
arbitrarily small ε > 0 and all j ∈ J:

1 − (|J| − |J min

i

|J min

|

j∈J\J min

i

|)ε

,

if j ∈ J min
if j ∈ J \ J min

;

i

i

(83)

.

νij > 0, which is always true

It is clear that for smaller values of ε, criterion function

(ε)) also gets smaller values:

s(cid:0)1 − (|J| − |J min

|J min

|

i

i

|)ε(cid:1)

(84)

νij log

j∈J min

νij log

1
sε

.

j(cid:54)∈J min

i

Therefore, passing to the limit, we can make the criterion
∗
:
arbitrarily small while approaching the desired solution τ

∗

(ε)) = −∞,

lim
ε→0

Ri(τ

∗

lim
ε→0

τ

(ε) = τ

∗

.

(85)

Solution (82) is not unique: any distribution of the proba-
, minimizes the criterion.

bility mass across τ
However, one solution is suﬃcient for our purposes.

ij for j ∈ J min
∗

i

Now we can proceed to the proof of Theorem 1.
Proof. Let us divide the sum in the objective function
Ri(τ | z) into the following three sums over disjoint subsets of
indices according to the sign of the diﬀerence δij ≡ ν(H)
ij −ν(S)
ij :

δij log τij −(cid:88)

−
i
(−δij) log
−
i

j∈J 0

i

1
τij

,

j∈J +

δij log τij −(cid:88)
Ri(τ | z) = −(cid:88)
δij log τij −(cid:88)
= −(cid:88)
(cid:88)

j∈J +

j∈J

j∈J

i

i

where J σ

sσ
i =

τij,

j∈J σ

i

i = {j | aj ∈ A ∧ sgn(δij) = σ}, and, similarly,

s+
i + s

−
i + s0

i = 1.

(87)

separately for each of the sums in (86).

• For the ﬁrst sum
R+

i (τ ) = −(cid:88)

δij log τij,

(88)

j∈J +

i

∗
ij =

τ

i δij(cid:80)

s+
l∈J +

i

δil

,

j ∈ J +
i .

(89)

Notice that the greater the sum s+
is the minimal value R+

).

∗

i becomes, the lesser

• For the second sum

i (τ

i (τ ) = −(cid:88)

−

,

R

j∈J

(−δij) log
−
i

1
τij
i , νij = −δij,
−
conditions of Lemma 2 hold for J = J
−
i . As we have shown in Lemma 2, when
and s = s
parameters are bounded below by some arbitrarily small
ε > 0, the function R

−
i (τ ) is minimized for

(90)

s

τ

∗
ij(ε) =

−
i
−
i ε,
i = {j ∈ J
J min

s

1 − (|A| − |J min

|)ε

i

|J min

|

i

−
i

| −δij = min
−
l∈J
i

(−δil)}.

,

if j ∈ J min
i
i \ J min
if j ∈ J
−

;

i

;

(91)

(92)

−
i (τ ) inverted, unlike
−
i becomes, the lesser is the

Notice that, since τij occurs in R
R+
minimal value R

i (τ ), the lesser the sum s

−
∗
i (τ
• For the indices j ∈ J 0

).

i , the choice of τij is irrelevant and
does not change the value of Ri(τ | z) regardless of s0
i .
In order to combine independent solutions (89) and (91)
optimizing separate sums, it is necessary to determine in
which proportion should the probability mass be distributed
between parameters belonging to J +
i . As we
have seen above, for the minimal value (as a function of
the bound ε) to be the least, s+
i has to be as large as pos-
sible, while both s
i , to the contrary, have to be as
small as possible. Therefore, the optimal proportion for the
parameters bounded below by ε is
i = 1 − s
s+

−
i , and J 0

−
i and s0

i = |J 0
s0

i − s0
−
i .

i | ε,

i , J

(93)

s

The corresponding parameters are then

(cid:0)1 − (|A| − |J +



ε,

i

,

δil

l∈J +

i | ε,
−

i |)ε(cid:1)δij

i = |J
−
(cid:80)
 δij(cid:80)
(cid:80)
max{0, δij}
aj∈A max{0, δij} =

l∈J +

δil

0,

,

i

=

if j ∈ J +
i ;
if j ∈ J
i ∪ J 0
−
i .

(94)

if j ∈ J +
i ;
if j ∈ J
i ∪ J 0
−
i ;
µij
µi

.

(95)

∗
ij(ε) =

τ

δij log τij

Passing to the limit for ε → 0, we ﬁnally obtain the parame-
ters that deliver minimum to the function Ri(τ | z):

(86)

∗
ij = lim
τ
ε→0

∗
τ
ij(ε) =

80