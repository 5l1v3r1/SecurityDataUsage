Wire-Speed Statistical Classiﬁcation of
Network Trafﬁc on Commodity Hardware

Pedro M. Santiago del Rio
pedro.santiago@uam.es

UAM, Madrid, Spain

Dario Rossi

Telecom ParisTech, France
dario.rossi@enst.fr

Francesco Gringoli
UNIBS, Brescia, Italy

francesco.gringoli@ing.unibs.it

Lorenzo Nava
UNIBS, Brescia, Italy

lorenzo.nava@ing.unibs.it

Luca Salgarelli
UNIBS, Brescia, Italy

luca.salgarelli@ing.unibs.it

Javier Aracil

UAM, Madrid, Spain

javier.aracil@uam.es

ABSTRACT

In this paper we present a software-based traﬃc classiﬁca-
tion engine running on commodity multi-core hardware, able
to process in real-time aggregates of up to 14.2 Mpps over a
single 10 Gbps interface – i.e., the maximum possible packet
rate over a 10 Gbps Ethernet links given the minimum frame
size of 64 Bytes.

This signiﬁcant advance with respect to the current state
of the art in terms of achieved classiﬁcation rates are made
possible by:
(i) the use of an improved network driver,
PacketShader, to eﬃciently move batches of packets from
the NIC to the main CPU; (ii) the use of lightweight statis-
tical classiﬁcation techniques exploiting the size of the ﬁrst
few packets of every observed ﬂow; (iii) a careful tuning
of critical parameters of the hardware environment and the
software application itself.

Categories and Subject Descriptors

C.2.3 [Network Operations]: Network Monitoring

Keywords

Statistical Identiﬁcation, Commodity Hardware, Traﬃc Mon-
itoring

1.

INTRODUCTION AND MOTIVATION

The ability to identify which application is generating ev-
ery single traﬃc session is recognized as a crucial building
block of today IP networks and unavoidable requisite for
their evolution [6]. Eﬀective techniques could open new pos-
sibilities for actual deployment of QoS, for enforcing user
traﬃc to comply with policies, for legal interception and in-
trusion detection [14].

Classic techniques based on Deep Packet Inspection (DPI)
have been thoroughly analyzed during the years: though

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

specialized hardware based on Network Processor [17] and
FPGAs [18] have been considered, the emergence of multi-
core commodity hardware has gained increasing attention
and exhaustive performance analyses have been reported
also for advanced systems using oﬀ-the-shelf Graphics Pro-
cessing Units (GPUs) [28, 27]. Unfortunately, despite the
powerfulness of the underlying hardware none of aforemen-
tioned approaches is able to actually sustain a 10 Gbps
throughput.

The latest years have also seen a ﬂurry of proposals ex-
ploiting diﬀerent “features” (in machine learning terms) to
perform the classiﬁcation [20]. Statistical techniques based
on the size and directions of the ﬁrst few packets of a ﬂow [1,
5] emerged as especially appealing due to their low complex-
ity if compared to current state of the art DPI approaches.
Furthermore, such techniques can be used when traﬃc is
encrypted, while DPI approaches simply cannot. However,
most of the previous work on statistical classiﬁcation focused
on assessing the accuracy of the diﬀerent techniques (that
we take for granted given results in [1, 5, 14, 16]) without
measuring their achievable classiﬁcation rates.

In this work, we argue that commodity multi-core hard-
ware oﬀers intrinsic scalability at low cost, while providing
the unbeatable ﬂexibility of software-only solutions. Hence,
we take a diﬀerent twist with respect to works employ-
ing specialized hardware based on Network Processor or
FPGAs:
furthermore, our software based solution is the
ﬁrst to achieve two important milestones. First, using both
real Tier-1 traces and synthetic traﬃc, we demonstrate that
multi-Gbps statistical traﬃc classiﬁcation is feasible with
open-source software on oﬀ-the-shelf hardware. Second, our
solution is able to sustain higher classiﬁcation rates than
previous work [27, 17, 28, 16] with a sizeable gain in terms
of the maximum amount of classiﬁcation actions per second
and manageable packet rates.

In more detail our software can easily handle a real Tier-1
traﬃc aggregate (i.e., a CAIDA OC192 trace [29]) replayed
at 10 Gbps, corresponding to 1.6 million packets per sec-
ond (Mpps) and 58 thousand ﬂow classiﬁcations per second
(Kfps). Using two interfaces, our system sustains classi-
ﬁcation rates of 20 Gbps, 3.2 Mpps, 116 Kfps. Yet, the
upper bound of the system performance is much higher, as
we manage to handle classiﬁcation rates up to 14.2 Mpps
and 2.8 Mfps without any losses (benchmark with synthetic

65worst-case traﬃc scenario with trains of 64B packets, 5 pack-
ets per ﬂow, over a 10 Gbps link).

Such astonishing performance follows the use of a light-
weight classiﬁcation algorithm (which bases its decisions upon
the size of the ﬁrst 4 packets of every unidirectional ﬂow [1,
5]), as well as of the most recent advances in terms of packet
processing techniques [11]. Yet, as we will see in the follow-
ing, engineering the system so that it sustains wire-speed
classiﬁcation in the worst-case traﬃc scenario required in-
vestigation of several delicate architectural trade-oﬀs of the
software, as well as the careful tuning of hardware parame-
ters. We believe the removal of all software bottlenecks in
the workﬂow to be another major contribution of this paper.
The paper sheds light on traﬃc monitoring and measure-
ment, giving advices and guidelines for designing high per-
formance traﬃc analysis tools which may be helpful for re-
searchers and network program designers. In this light, we
have released the code of our implementation to the com-
munity as open-software1 which may enlighten researchers
and designers about how to implement their own network
monitoring tools (or modify the existing ones) to cope with
current high speeds, i.e. 10Gbps and beyond.

Finally, we point out that to stress-test our system, we also
had to develop highly eﬃcient traﬃc injection engines able
to either generate inﬁnite synthetic traﬃc or replay huge
traces thus saturating 10 Gbps links during very long exper-
iments. As ﬁnal contribution of this paper, we make these
latter tools, whose scope of applicability goes beyond that
of the Internet traﬃc classiﬁcation, available as open-source
software2 as a research byproduct.
2. RELATED WORK

We present recent advances in (i) packet capturing en-
gines, (ii) ﬂow management and (iii) traﬃc classiﬁcation
techniques. We summarize performance for each category in
Tab. 1, further providing a comparison of our results with
respect to the current state of the art.
Packet capturing engines. Although modern Network
Interface Cards (NICs) hardware can handle high packet
rates, standard operating system network stacks are still af-
fected by a few software bottlenecks (e.g., per-packet op-
erations like buﬀer allocation and transfer to user-space).
To overcome such issues, several approaches bypass stan-
dard stacks by (i) processing multiple packets in batch to
limit IRQs and DMA transactions; (ii) exposing memory of
packet buﬀers to the user-space for zero-copy access; (iii)
tying every capture thread with its own ring buﬀer to a
ﬁxed CPU to increase cache memory hits (Non-Uniform
Memory Access, NUMA) and (iv) using Receive Side Scal-
ing (RSS) to split incoming ﬂows among diﬀerent input
queues/capture threads. We report in Tab. 1 packet capture
performance for a number of systems including: PF RING
with Threaded NAPI [10] and variants [4]; Netmap [24,
23]; PacketShader [11]; and PFQ [2]. As can be seen from
the table, performance at 10 Gbps are impressive and this
demonstrates that oﬀ-the-shelf hardware can now be used
in place of costly Network Processors. Finally, since these
systems achieve comparable performance, the choice of a
speciﬁc technique is not critical for our purposes, and falls
on PacketShader [11] (see Sec. 3).

1http://www.eps.uam.es/~psantiago/hptrac.html
2http://www.eps.uam.es/~psantiago/hpcap.html

Flow matching. Monitoring at the ﬂow level requires
to match each packet to the correct ﬂow bin. In software
based solutions such as Tstat [25] or YAF [12] this is usu-
ally accomplished using hash-based structures over the ﬂow
5-tuple. To the best of our knowledge, however, perfor-
mance of ﬂow matching code in complex monitoring sys-
tems is rarely evaluated alone and extrapolating such data
from overall measurements can be tough or even mislead-
ing. For instance, [12] describes a ﬂow management module
in detail, explaining how to optimize ﬂow management using
slab allocator [3] for fast recycling of expired ﬂow records,
but benchmarks of the system performance are not pub-
licly available. Otherwise, the performance analysis for ﬂow
matching modules has been done either monitoring real ISP
deployments [9] or over oﬄine traces [25, 30]. However, as
real 10 Gbps traﬃc is not by itself a stress-test scenario, this
calls for synthetic benchmarks.

Explicit performance are reported instead in [8] where a
dual Xeon box hosts a dedicate Endace DAG card which
achieves matching of up to 6 Mpps. In [22] an Intel IXP2850
Network Processor is shown matching 10 million concurrent
ﬂows at 10 Gbps at full packet rate. Switching to oﬀ-the-
shelf setup, an application note from Intel [15] reports ﬂow
matching of trains of 64 bytes packets at 17 Mpps out of
24 Mpps received over 16× 1 Gbps interfaces, where each
NIC is tied to a diﬀerent core of an Intel multi-core CPU
system (unfortunately the study does not report the number
of concurrent ﬂows). A similar architecture [7] matches up
to 11 Mpps for 1 million concurrent ﬂows at 10 Gbps using
“FastFlow” algorithms spawned over 6 cores. For compari-
son, our system is able to handle aggregate ﬂow rates up tp
2.8 Mfps using just two cores.
Statistical traﬃc classiﬁcation. Several techniques have
been proposed for the classiﬁcation of Internet traﬃc. Tra-
ditional ones are based either on the analysis of the trans-
port layer port numbers as in CoralReef [19], or on Deep
Packet Inspection of the packet payload as most commercial
tools do. Statistical techniques, instead, observe basic prop-
erties like packet lengths and interarrival times to classify
traﬃc and are celebrated for their accuracy and speed [20].
However, while the former has been experimentally demon-
strated [14, 16] on real traces, the latter is far from being
assessed. As a result, the classiﬁcation rates and scalability
of these new algorithms are either unknown or really far from
those needed for real world deployment [6]: e.g., [1, 5] merely
discuss the complexity of the classiﬁcation technique, while
the most recent performance analysis in [16] reports as few
as 30 · 103 classiﬁcation per seconds with Na¨ıve Bayes. On
a oﬀ-the-shelf setup similar to that used in [16], we achieve
2.8·106 classiﬁcation per seconds running custom implemen-
tation of Na¨ıve Bayes, thus boosting performance of a factor
of 100.

Furthermore we can compare our results to that achieved

by non-commercial Deep Packet Inspection (DPI) systems [17,
28, 27], that run pattern matching algorithms on multicore
GPUs. It is worth noting that while the amount of GPUs
power is already enough to process up to 40 Gbps traﬃc,
bottlenecks in the communication subsystem crushes the ac-
tual performance down to a mere 5.2 Gbps corresponding to
1 Mpps and 5 Kfps [28]. Similarly, [17] and [27] achieve
3.5 Gbps and 6 Gbps of aggregated traﬃc rate, correspond-
ing to less than 2 Mpps. Our technique not only sustains

66Table 1: Maximum processing rate in the state of the art, and advances oﬀered by this work.

Category

Ref.

Packet capture

Flow handling

Traﬃc classiﬁcation

[10, 4]
[24, 23]
[11]
[2]
this work[11]
[8]
[15]
[7]
this work
[27]
[17]
[28]
[16]

this work

Rates

MFlow/s MPkts/s Gb/s
-
-
-
-
-
-
-
1
2.8
-
-
0.005
0.03
0.116
2.8

14.8
14.2
14.2
12
14.2
6
17
10
14.2
1.8
-
1
-
3.2
14.2

10
10
10
10
10
10
-
10
10
6.7
3.5
5.2
-
20
10

Comments

60B frames
64B frames
64B frames
Sender limitation
64B frames
Using Endace DAG cards
Using 16 cores (16x1Gbps interfaces)
Using 6 cores
Using 2 cores
Using a GPU, synthetic traﬃc
Using real traﬃc
Using a GPU, real traﬃc
Oﬄine experiments on real traﬃc (Weka)
Real Tier-1 OC192 traﬃc over 2×10Gbps NIC
Synthetic traﬃc (64B frames, 5pkts/ﬂow)

SNIFFING
MODULE

FLOW 

HANDLING
MODULE

CLASS
MODULE

CHUNK RING

JOB RING

Figure 1: System modules.

10 Gbps of aggregated traﬃc, but potentially much more as
we handle 2.8 · 106 classiﬁcations per second.

3. SYSTEM MODULES

We report in Fig. 1 the three main blocks that compose
our classiﬁcation system, the two data ring structures for
packet/ﬂow queuing and the logical connections that push
information from left to right.
Sniﬃng module. We capture incoming packets making
use of PacketShader [11], a customized version of the In-
tel ixgbe driver that can fetch chunks of multiple frames
from the NIC using one single DMA data transfer, greatly
reducing the I/O overhead and the per-packet buﬀer alloca-
tion cost. Thanks to a native feature of the Intel 82599EB
10 Gbps Ethernet controller[13], incoming frames are par-
titioned in RSS queues according to a hash function: one
sniﬃng module (a thread running in user space that has di-
rect access to the kernel-space buﬀers) can then be set up
to fetch frames only from a given RSS queue. Following In-
tel paradigm, we also tie every capture thread to a speciﬁc
CPU core (thread aﬃnity) so as to keep the data locally
in that CPU cache (hence limiting cache thrashing between
processor sockets). This same feature extends parallelism
from the NIC to the user layer, as RSS queues feed diﬀer-
ent cores with multiple chunks at the same time, pushing
them through multiple lanes of the PCIe bus and hence in-
creasing the overall throughput with respect to a single core
solution. Packets are then organized in a circular ring and
made available to the user space with zero-copy technology.
Here a thread running on the same CPU copies the chunks
from the kernel ring and enqueues resulting data to a Chunk

Ring of Fig. 1: if the ring is full, a chunk might be lost. We
set the chunk size to 128 packets.
Flow Handling module. A thread then dequeues packets
from the Chunk Ring and perform lookup into a Flow Table.
A hash over the packet 5-tuple is used as a primary key to
access a hash table, while collisions are handled by chaining
(a data structure based on linked list of ﬂow buckets). Once
the bucket is found (or a new one is appended if the ﬂow
was not already known), a new feature is added to the ﬂow
structure, namely the length of the corresponding packet
(read from the IP header). Each ﬂow is considered active
within a timeout (default 15 sec) after the reception of the
last packet. When the timeout expires, its position in the
linked list can be reused by a new ﬂow (no deallocation
overhead). Once a conﬁgurable number of packets for a
given ﬂow has been seen (4 in this work), a new classiﬁcation
Job is ﬁred to the Job Ring of Fig. 1 if a position is available
(otherwise, the Job will likely be inserted the next time a
packet from that ﬂow will be analyzed). We set the ﬂow
hash table size3 to 50 millions.
Classiﬁcation Module. Classiﬁcation threads run a cus-
tom implementation of Na¨ıve Bayes with Gaussian density
estimation. Given the ﬁrst four packets of a ﬂow have been
received, the algorithm associates the ﬂow to the protocol
whose model scores the maximum likelihood for the genera-
tion of the ﬂow. For each protocol model the algorithm uses
the size of the packets as indexes into the four lookup tables
that have been associated to that protocol during the train-
ing phase: the values extracted are then summed together,
we optimize, in fact, the algorithm by storing the logarithms
of the table values to avoid products as reported in [26]. By
comparing the values obtained for each of the protocols for
which a model is available, the algorithm chooses the ap-
plication and the classiﬁcation of the ﬂow terminates:
for
more details please refer to [5]. Although classiﬁcation ac-
curacy, in terms of packets correctly classiﬁed, is a key issue,
it is not the goal of this paper because it has been already

3For reason of space, we are unable to report a detailed
sensitivity analysis of the hash table size; here, we merely
stress that we set a hash size large enough to signiﬁcantly
mitigate the occurrence of chaining.

67Tag

Interfaces (I) Queues (Q) Processes (P)

1I-1Q-1P

xge0

Rx

Flow

Class

1I-2Q-1P

xge0

1I-2Q-2P

xge0

2I-1Q-2P

xge0

xge1

Rx
Rx

Rx

Rx

Flow
Flow

Class

Flow

Class

Flow

Class

Rx

Rx

Flow

Class

Flow

Class

CPU afﬁnity
Flow Class
Rx

Flow Class
Rx
Flow
Rx

Flow Class
Rx
Flow Class
Rx

Flow Class
Rx
Flow Class
Rx

CPU1

CPU2

CPU1

CPU2

CPU1

CPU2

CPU1

CPU2

Figure 2: Diﬀerent architectural conﬁgurations of
our system.

analyzed. Thus, we have chosen a representative classiﬁ-
cation technique, such as Na¨ıve-Bayes, whose accuracy has
been previously showed as enough for traﬃc classiﬁcation
purposes [20]. Once chosen the classiﬁcation technique, we
evaluated its performance in terms of computational cost
and the feasibility of its implementation on a real system
(based on commodity hardware and open software). Note
that the computational complexity, given a model, is not a
function of accuracy.

To increase the speed at which Jobs are extracted from
the Job Ring, multiple threads can be spawned according
also to the complexity of the algorithm. Each thread ex-
tracts one Job from the ring, processes the data, and writes
the classiﬁcation verdict in the ﬂow bucket inside the Flow
Table. New packet for that ﬂow will be marked with this
verdict in their Type Of Service (TOS) ﬁeld of the IPv4
header4. By default, we set only one thread to classify: as
we will see in the next section, this is suﬃcient to classify
all Jobs generated by ﬂow handling module.

4. SYSTEM CONFIGURATION

Contrary to traditional network applications, new capture
engines extend the capture data rates by several orders of
magnitude. This implies that probably current networking
software is not able to work adequately at this speed.
In
addition to this, multi-core hardware opens a new oppor-
tunity to develop applications that take advantage of the
parallelism that such engines allow, whereas current applica-
tions were developed in the pre-multicore area. Thus, care-
ful engineering of the system (e.g., CPU and memory aﬃn-
ity, threads/process choice) is needed to achieve the highest
classiﬁcation rates of Tab. 1 under worst-case traﬃc. Con-
ﬁgurations explored in this paper are sketched in Fig. 2.
traﬃc re-
1I-1Q-1P. This is the simplest conﬁguration:
ceived at the same RSS queue of a single interface is captured

4For reason of space, we do not assess packet forwarding
after classiﬁcation in this paper.

by a thread which in turn pushes packets to one chunk ring
in the user space. All packets are matched in the ﬂow table
sequentially: one classiﬁcation thread works on the single
Job ring. In this case, though, some CPU cores are not uti-
lized. Note that using a single RSS queue may be preferable
to multi-queue in some cases —whenever the performance is
enough to cope with line-rate. For instance: to get smaller
CPU usage (and therefore less power consumption), not to
re-implement monitoring tools which have not been designed
for parallel processing, or to avoid packet reordering issues
due to multi-queue[31].
1I-2Q-1P. This conﬁguration holds the single process model
of the previous one but two RSS queues are used: this means
two threads for fetching packets chunks and two separate
ﬂow matching modules. Each capturing ﬂow is executed on
a diﬀerent CPU, but only one hash table is used. Since a
single Job ring is used, data coming from two diﬀerent CPUs
is merged again in a single data ﬂow. In this case, locking
is used to enable concurrent accesses to the ﬂow table, and
this might decrease the overall performance.
1I-2Q-2P. Though similar to the previous conﬁguration
(two RSS queues, two threads for capturing, bound to diﬀer-
ent CPUs), the two threads for ﬂow-handling reside in dif-
ferent processes, each on the same CPU of the corresponding
sniﬀer. This means that two separated ﬂow tables are main-
tained and no locking is required thanks to the hash function
used at the NIC for dividing packets into the two queues:
each single ﬂow lives on a single queue only (no mixing).
Moreover, the classiﬁcation code (threads) and data struc-
tures (Job rings) are duplicated and fairly spread among the
two CPUs with no data ﬂow merge. In this case, locking is
solved at the price of doubling the amount of memory.
2I-1Q-2P. The last conﬁguration is exactly as the ﬁrst one,
but two NICs are used:
for each NIC a complete capture
and classiﬁcation chain is instantiated, each complete chain
lives on a separated CPU. Given the number of cores in
our system, we cannot explore other conﬁgurations when 2
interfaces are in use.

5. PERFORMANCE EVALUATION

First, we describe our experimental testbed, covering hard-
ware, software and traﬃc details. Then, we benchmark sys-
tem performance in two scenarios: (i) we locate and solve
system bottlenecks using synthetic traﬃc in a worst-case
scenario (64B packets at maximum rate); (ii) we assess our
system in a real scenario, replaying traces from a Tier-1 link
over one or two 10 Gbps interfaces.

5.1 Experimental testbed

Hardware setup. Our setup consists of two general-purpose
servers: one acts as traﬃc generator, the other receives and
classiﬁes the traﬃc. Both are equipped with 24 GB of DDR3
memory and feature two Intel Xeon E5620 processors, count-
ing four cores each (with hyper-threading capabilities dis-
abled to obtain actual parallelism among cores) working at
2.40 GHz. Concerning connectivity, each server is equipped
with one dual port 10Gbps Intel X520-SR2 NIC, and servers
are directly connected with a ﬁber link. This NIC model is
based on 82599 chipset, that enables multi-queue techniques,
up to 16 RSS queues per interface and direction.

683 x 107
2.5

d
n
o
c
e
s
 
r
e
p

 
s
t

e
k
c
a
P

2

1.5

1

0.5

0

 

Sending

100%+100%

77%+78%
100%+100%

 

X1%+X2% CPU Usage Thr1+Thr2

100%+100%
87%+86%

42%+41%
46%+45%

1I−1Q−1P
1I−2Q−1P
1I−2Q−2P
2I−1Q−2P

100%

100%

100%+100%

39%

43%

Sniffing

Flow handling

Classification

Figure 3: System Performance. Worst-case sce-
nario: synthetic traﬃc 64B packets, 5 packet/ﬂow.

Software. Ubuntu 10.04 server 64-bit is installed on both
servers with a 2.6.35 Linux kernel. In order to inject traﬃc,
we have developed a tool on top of PacketShader API to send
traﬃc at maximum rate: generic tools such as tcpreplay5
can not, in fact, saturate 10Gbps links. Our tool6 instead
is able either to inject inﬁnite synthetic traﬃc or to replay
very long packet-level traces at maximum speed.
Traﬃc. We utilized both synthetic traﬃc and real traces.
Synthetic traﬃc consists of TCP segments encapsulated into
64B-size Ethernet frames, forged with incremental IP ad-
dresses and TCP ports. We stress once more that, though
all packets have 64B-size, the packet length features are ex-
tracted from the IPv4 header: hence, our benchmark method-
ology does not aﬀect the relevance of the classiﬁcation re-
sults. For each ﬂow (5-tuple combination), we send 5 pack-
ets, since the 5-th packet will be the ﬁrst to have the chance
to be classiﬁed (on the basis of the packet-length features of
the previous 4 packets). At a maximum rate of 14.2 Mpps
for 64B frames, this translate into 2.8 Mfps. Real traﬃc
consists of a packet-level trace sniﬀed in 2009 at an OC192
(9953 Mbps) backbone link of a Tier-1 ISP located between
San Jose and Los Angeles, available from CAIDA [29]. All
the packets in the trace are anonymized and captured with-
out payload, the average of the original packet size in the
trace is 744 bytes and the average number of packets per
ﬂow is 49.

5.2 Stress test: ﬁnding the bottlenecks

We ﬁrst stress test the system in a worst-case scenario, i.e.,
using synthetically generated 5-packets long ﬂows, sending
64B frames at maximum rate, i.e., 14.2 Mpps or 2.8 Mfps
per interface. We measure the amount of packets processed
by each module during 60-second experiments (but we also
tested the system on 24-hr long experiments on the best
conﬁguration without observing losses). Note that using two
interfaces we were only able to send at ≈25 Mpps (instead
of the theoretic maximum 28.4 Mpps) due to limitations in
the sender.

Fig. 3 shows the performance of each module (sniﬃng,
ﬂow handling and classiﬁcation) for the diﬀerent conﬁgura-
tions. In the parallel coordinates plot, a negative slope in
the curves reﬂects a bottleneck: i.e., a module is not able to
process all packets generated by the previous one. Curves

in Fig. 3 are annotated with the CPU usage of each module:
if a module runs two threads, CPU usage is expressed sum-
ming the two corresponding values.CPU usage is computed
using sysstat utilities7. We obtained the CPU load per
thread every 5 seconds and then we averaged. Background
CPU usage (when there is no classiﬁcation systems running)
and experimental variance are negligible.
The simplest conﬁguration. The simplest conﬁguration
1I-1Q-1P sniﬀs traﬃc from one interface, uses one RSS queue
and one process. In this case, it can be observed that not
all packets can be sniﬀed using a single RSS queue (a single
core). Particularly, only 12.1 Mpps are sniﬀed out of the
14.2 Mpps sent: notice that the CPU usage of the sniﬃng
module is 100% which pinpoints a processing bottleneck.
Similarly, ﬂow module is only able to process 7.9 Mpps out
of 12.1 Mpps sniﬀed packets. As CPU utilization of the ﬂow
handling module is also 100%, we have strong indication of a
second processing bottleneck. Finally, a slight negative slope
can be observed in the classiﬁcation module. This is not due
to a CPU bottleneck (39%), but rather to the fact that only
ﬂows with 5 packets can be classiﬁed and there have been
packet losses in previous modules. The classiﬁcation rates
sustained by 1I-1Q-1P conﬁguration are thus 7.9 Mpps and
1.5 Mfps.
Using 2 RSS queues. In order to remove bottlenecks ob-
served in the ﬁrst conﬁguration, we increment the number
of RSS queues and the number of cores dedicated to packet
sniﬃng and ﬂow management. Thus, we test two conﬁgu-
rations, namely 1I-2Q-1P and 1I-2Q-2P. As shown in Fig. 2
and explained in Sec. 4, the former conﬁguration uses one
process with two threads for sniﬃng, one per queue, and
two threads for ﬂow handling, having a unique hash table
and a unique classiﬁcation thread. Conversely, 1I-2Q-2P
conﬁguration uses two processes, one per queue, which do
not share neither data structures nor processing cores. We
can observe that the bottleneck in the sniﬃng module is re-
moved in both cases: i.e., at 14.2 Mpps, two RSS queues are
enough to receive and process the traﬃc. Besides, the 1I-
2Q-2P conﬁguration with two processes consumes less CPU
power.
Locking issue. The behavior of the ﬂow handling module
is diﬀerent for 1I-2Q-1P and 1I-2Q-2P. Indeed, 1I-2Q-1P is
not able to process all packets received by the sniﬃng mod-
ule, and performance are even worse than in the case with
only one RSS queue. The bottleneck in this conﬁguration
is tied to the contention in the access to shared data struc-
tures, such as the hash table and the Job ring. To arbitrate
concurrent access to shared memory, synchronization and
locking mechanisms are necessary but they are detrimen-
tal to overall performance. As in the ﬁrst conﬁguration, all
ﬂows generated by the ﬂow handling module can be classi-
ﬁed. With 1I-2Q-1P conﬁguration, the performance of the
whole system fall to 2.1 Mpps and 0.4 Mfps.
Wire-speed classiﬁcation. Using two RSS queues and
two independent processes, we remove both sniﬃng bottle-
neck and ﬂow management locking issues. With 1I-2Q-2P,
the system sniﬀs, processes and classiﬁes all packets sent at
wire-speed without losses. Notice further that not even a
single CPU core is saturated (sniﬃng 77%+78%, ﬂow man-
agement 87%+86%, classiﬁcation 45%+46%), so that the
remaining processing power could be useful to perform other

5http://tcpreplay.synfin.net/
6http://www.eps.uam.es/~psantiago/hpcap.html

7http://sebastien.godard.pagesperso-orange.fr/
documentation.html

69 

%
e
g
a
s
U
U
P
C

 

100

80

60

40

20

0

 

10Gbps
6GB
11.9%

Sniffing
Flow handling
Classification

Class rate
Memory usage
Total CPU usage

10Gbps
6GB
16.1%

10Gbps
12GB
14.6%

 

20Gbps
12GB
29.5%

1I−1Q−1P

1I−2Q−1P

1I−2Q−2P

2I−1Q−2P

14.2
284

7.4
154

20

15

10

5

]
s
p
b
G

[
 

e

t

a
r
 

n
o

i
t

a
c
i
f
i
s
s
a
C

l

Packet rate (per interface) [Mpps]
Flow rate (per interface) [Kfps]

3.8
103

2.8
85

1.6
58
 

1I−1Q−1P
1I−2Q−1P
1I−2Q−2P
2I−1Q−2P

Figure 4: Real scenario: CAIDA trace with original
packet length.

0
 
0 64

200

500
Max. Packet size [Bytes]

750

1.500

tasks (such as packet forwarding or statistics collection).
This also means that the processing capabilities of 1I-2Q-2P
conﬁguration exceed the maximum data rate at 10 Gbps.,
i.e., 14.2 Mpps and 2.8 Mfps.
Using 2 interfaces. The latest conﬁguration, 2I-1Q-2P
uses two interfaces to receive traﬃc, but a single RSS queue
per interface due to the limit in the number of cores. As
expected, behavior is similar to 1I-1Q-1P, with bottlenecks
in both sniﬃng (24.2 Mpps received out of 25.2 Mpps sent)
and ﬂow handling (15.2 Mpps processed out of 24.2 Mpps
received).

5.3 Real scenario: classifying at 20Gbps

Original packet size. We now test performance on real
Tier-1 traﬃc. We replay traces by sending packets back-to-
back at 10 Gbps by ﬁlling payload with zeros. Using full
packet size in Fig. 4, all system conﬁgurations sustain max-
imum rate: i.e., 1.6 Mpps and 58 Kfps on a single 10Gbps
interface, or 3.2 Mpps and 116 Kfps on two interfaces. CPU
usage and memory occupancy report that cores are far from
being saturated: this proves that our system could classify
more than 20 Gbps traﬃc in a realistic scenario. Notice that
the simplest conﬁguration (only 3 threads) is enough to clas-
sify all traﬃc with the smallest CPU load (hence the lowest
carbon footprint).
Capped packet size. Finally, in Fig. 5 for each frame
of size Si we control the maximum amount of bytes sent
on the wire as max(Si, L) with L the maximum frame size,
that we vary in the range [64, 1500]B to tune the ﬂow and
packets arrival rates for a ﬁxed datarate of 10 Gbps – hence
ﬁnding the packet and ﬂow processing rate bottleneck of
each conﬁguration. From the ﬁgure, we gather that the sim-
plest conﬁguration 1I-1Q-1P can sustain up to 3.8 Mpps and
103 Kfps using only 3 cores.

6. CONCLUSIONS

We propose an all software solution for statistical traﬃc
classiﬁcation on commodity hardware. Our system achieves
a signiﬁcant advance with respect to the state of the art in
several ways. First, it demonstrates the feasibility of on-
line statistical traﬃc classiﬁcation, that was so far conﬁned
on oﬄine analysis published in the literature. Second, it
signiﬁcantly outperforms state of the art classiﬁcation tech-
niques. Indeed, while the raw classiﬁcation throughput on
real traﬃc aggregates is about 3× higher than [27] and 4×
higher than [28], however our system is able to sustain ﬂow
classiﬁcation rates 93× higher than [16] and 560× higher
than [28]. Additionally, this paper sheds light on traﬃc mon-

Figure 5: CAIDA trace with capped packet length.

itoring and measurement, giving advices and guidelines for
designing high performance traﬃc analysis tools which may
be helpful for researchers and designers.
In this light, we
released the code of our implementation to the community
as open-software which may be useful for researchers and
designers in order to implement their own network monitor-
ing tools (or modify the existing ones) to cope with current
high speeds (10Gbps and beyond).

Notice that the above performance gap between our work
and the previous ones, is hard to remove. Indeed, on the one
hand, while GPUs in [28] could in principle process 40 Gbps
equivalent of traﬃc, this is forbidden by a bottleneck in the
path from the NIC to the GPU. That is, current approaches
force to pass through main memory, and waste processing
time, to transfer data between the NIC and the GPU cre-
ating a bottleneck — although there are preliminary results
which may avoid such limitation, they can be only used with
Inﬁniband technology yet[21]. On the other hand, the statis-
tical technique is anyway much more lightweight than DPI
(only process packet headers), so that it would beneﬁt more
from a GPU. This gap is intrinsic to the nature of the sta-
tistical classiﬁcation process, that avoid to transfer packet
payload from the NIC to GPUs unlike DPI. Thus, statisti-
cal approaches are unachievable for DPI, even making use
of diﬀerent hardware, such as GPUs.

While this work constitutes a signiﬁcant advance, we be-
lieve that further optimization are possible, which are part
of our current ongoing work. First, we aim at wire-speed
performance on two interfaces, which should be achieved
with a 2I-2Q-4P conﬁguration on a 12 core server. Sec-
ond, we plan to implement C4.5 trees, due (i) their known
discriminative power, and (ii) the fact that they can be ef-
ﬁciently implemented as if-then-else branches. Finally, we
want to optimize ﬂow management, avoiding to compute
hash in software, by exporting the hash computed by the
NIC to map packets to RSS queues, which should requires
only simple modiﬁcation to the NIC driver.

Acknowledgements
This work has been carried out when Pedro was at LINCS8,
with the support of the Spanish FPU scholarship, and has
been partly supported by the FP7 IP mPlane project. This
work was supported in part by a grant from the Italian
MIUR, under the PRIN project IMPRESA.

8http://www.lincs.fr

707. REFERENCES

[1] L. Bernaille, R. Teixeira, and K. Salamatian. Early
application identiﬁcation. In ACM CoNEXT 2006.

[2] N. Bonelli, A. Di Pietro, S. Giordano, and G. Procissi.

On multi-gigabit packet capturing with multi-core
commodity hardware. In Passive and Active
Measurement (PAM) 2012.

[3] J. Bonwick. The slab allocator: An object-caching

kernel memory allocator. In USENIX Summer
Technical Conference 1994.

[4] A. Cardigliano, J. Gasparakis, and F. Fusco.

vPF RING: Towards wire-speed network monitoring
using virtual machines. In ACM IMC 2011.

[5] M. Crotti, M. Dusi, F. Gringoli, and L. Salgarelli.

Traﬃc classiﬁcation through simple statistical
ﬁngerprinting. ACM SIGCOMM Comput. Commun.
Rev., 37(1):5–16, 2007.

[6] A. Dainotti, A. Pescape, and K. Claﬀy. Issues and
future directions in traﬃc classiﬁcation. Network,
IEEE, 26(1):35 –40, 2012.

[7] M. Danelutto, L. Deri, and D. De Sensi. Network

monitoring on multicores with algorithmic skeletons.
In International Conference on Parallel Computing
(PARCO) 2011.

[8] L. Deri. IP traﬃc monitoring at 10 Gbit and above.
http://www.terena.org/activities/ngn-ws/ws2/
deri-10g.pdf.

[9] A. Finamore, M. Mellia, M. Meo, M. Munafo, and
D. Rossi. Experiences of Internet traﬃc monitoring
with Tstat. Network, IEEE, 25(3):8–14, 2011.

[10] F. Fusco and L. Deri. High speed network traﬃc

analysis with commodity multi-core systems. In ACM
IMC 2010.

[11] S. Han, K. Jang, K. Park, and S. Moon.

PacketShader: a GPU-accelerated software router. In
ACM SIGCOMM Comput. Commun. Rev., volume 40,
pages 195–206, 2010.

[12] C. Inacio and B. Trammell. YAF: yet another

ﬂowmeter. In International conference on Large
installation system administration (LISA) 2010.
[13] Intel. Intel ´l 82599 10 GbE Controller Datasheet.

October, (December), 2010.

[14] H. Kim, K. Claﬀy, M. Fomenkov, D. Barman,

M. Faloutsos, and K. Lee. Internet traﬃc classiﬁcation
demystiﬁed: myths, caveats, and the best practices. In
ACM CoNEXT 2008.

[15] A. Lim and R. Kinsella. Data plane packet processing

on embedded intel architecture platforms.
http://download.intel.com/design/intarch/
papers/322516.pdf.

[16] Y. Lim, H. Kim, J. Jeong, C. Kim, T. Kwon, and

Y. Choi. Internet traﬃc classiﬁcation demystiﬁed: on
the sources of the discriminative power. In ACM
CoNEXT 2010.

[17] Y. Liu, D. Xu, L. Sun, and D. Liu. Accurate traﬃc

classiﬁcation with multi-threaded processors. In IEEE
International Symposium on Knowledge Acquisition
and Modeling Workshop (KAM) 2008.

[18] A. Mitra, W. Najjar, and L. Bhuyan. Compiling
PCRE to FPGA for accelerating SNORT IDS. In
ACM/IEEE Symposium on Architecture for
networking and communications systems (ANCS)
2007.

[19] D. Moore, K. Keys, R. Koga, E. Lagache, and K. C.

Claﬀy. The CoralReef software suite as a tool for
system and network administrators. In USENIX
conference on System administration 2001.

[20] T. Nguyen and G. Armitage. A survey of techniques

for Internet traﬃc classiﬁcation using machine
learning. Communications Surveys & Tutorials, IEEE,
10(4):56–76, 2008.

[21] NVIDIA Corporation. NVIDIA GPUDirect

Technology. http://developer.download.nvidia.
com/devzone//devcenter/cuda/docs/GPUDirect_
Technology_Overview.pdf.

[22] Y. Qi, B. Xu, F. He, B. Yang, J. Yu, and J. Li.

Towards high-performance ﬂow-level packet processing
on multi-core network processors. In ACM/IEEE
Symposium on Architecture for networking and
communications systems (ANCS) 2007.

[23] L. Rizzo. netmap: a novel framework for fast packet
I/O. In USENIX Annual Technical Conference 2012.

[24] L. Rizzo, M. Carbone, and G. Catalli. Transparent

acceleration of software packet forwarding using
netmap. In IEEE INFOCOM 2012.

[25] D. Rossi and M. Mellia. Real-time TCP/IP analysis

with common hardware. In IEEE ICC 2006.

[26] D. Rossi, S. Valenti, P. Veglia, D. Bonﬁglio, M. Mellia,

and M. Meo. Pictures from the Skype. ACM
Performance Evaluation Review (PER), 36(2):83–86,
2008.

[27] G. Szab´o, I. G´odor, A. Veres, S. Malomsoky, and

S. Moln´ar. Traﬃc classiﬁcation over Gbit speed with
commodity hardware. IEEE J. Communications
Software and Systems, 5, 2010.

[28] G. Vasiliadis, M. Polychronakis, and S. Ioannidis.

MIDeA: a multi-parallel intrusion detection
architecture. In ACM conference on Computer and
communications security (CSS) 2011.

[29] C. Walsworth, E. Aben, k. claﬀy, and D. Andersen.

The CAIDA anonymized 2009 Internet traces.
http://www.caida.org/data/passive/passive_
2009_dataset.xml.

[30] D. Wang, Y. Xue, and Y. D. Memory-eﬃcient
hypercube ﬂow table for packet processing on
multi-cores. In IEEE GLOBECOM 2011.

[31] W. Wu, P. DeMar, and M. Crawford. Why can some

advanced Ethernet NICs cause packet reordering?
IEEE Communications Letters, 15(2):253–255, 2011.

71