A Secure Sharding Protocol For Open Blockchains

Loi Luu

National University of Singapore

loiluu@comp.nus.edu.sg

Viswesh Narayanan

National University of Singapore

visweshn@comp.nus.edu.sg

Chaodong Zheng

National University of Singapore

chaodong.zheng@comp.nus.edu.sg

Kunal Baweja

National University of Singapore

bawejaku@comp.nus.edu.sg

Seth Gilbert

National University of Singapore
seth.gilbert@comp.nus.edu.sg

Prateek Saxena

National University of Singapore

prateeks@comp.nus.edu.sg

ABSTRACT
Cryptocurrencies, such as Bitcoin and 250 similar alt-coins, em-
body at their core a blockchain protocol — a mechanism for a dis-
tributed network of computational nodes to periodically agree on
a set of new transactions. Designing a secure blockchain protocol
relies on an open challenge in security, that of designing a highly-
scalable agreement protocol open to manipulation by byzantine or
arbitrarily malicious nodes. Bitcoin’s blockchain agreement proto-
col exhibits security, but does not scale: it processes 3–7 transac-
tions per second at present, irrespective of the available computa-
tion capacity at hand.

In this paper, we propose a new distributed agreement proto-
col for permission-less blockchains called ELASTICO. ELASTICO
scales transaction rates almost linearly with available computation
for mining: the more the computation power in the network, the
higher the number of transaction blocks selected per unit time.
ELASTICO is efﬁcient in its network messages and tolerates byzan-
tine adversaries of up to one-fourth of the total computational power.
Technically, ELASTICO uniformly partitions or parallelizes the min-
ing network (securely) into smaller committees, each of which pro-
cesses a disjoint set of transactions (or “shards”). While sharding
is common in non-byzantine settings, ELASTICO is the ﬁrst candi-
date for a secure sharding protocol with presence of byzantine ad-
versaries. Our scalability experiments on Amazon EC2 with up to
1, 600 nodes conﬁrm ELASTICO’s theoretical scaling properties.

1.

INTRODUCTION

A blockchain is an append-only distributed database that stores
a time-ordered set of facts, also known as transactions. Trans-
actions are grouped into batches or “blocks” and form a crypto-
graphic hash-chain, hence the name blockchain. In 2009, Bitcoin
introduced the ﬁrst blockchain protocol called Nakamoto consen-
sus which underlies over 250 cryptocurrencies [1]. The blockchain
protocol maintains the distributed database in a decentralized net-
work, thus aiming to solve what we call as the blockchain agree-
ment problem. Conceptually, the problem is to allow an arbitrary
large network of several processors to agree on the blockchain state
(identiﬁed by its cryptographic digest), under the assumption that

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
© 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978389

the fraction of malicious processors is bounded by f (0 ≤ f < 1).
Processors have no inherent identities, nor is there any trusted PKI
infrastructure to establish identities for processors. Each processor
can choose a set (e.g., block) of transactions it wishes to commit to
the blockchain; the goal of the protocol is to ensure that all honest
processors agree on one set of transactions at the end of the proto-
col. The commit set is appended as a new block to the blockchain.
At a high level, the blockchain protocol in Bitcoin randomly
selects one processor per epoch (say 10 minutes) which issues a
proposal that everyone adopts, thus requiring only a single broad-
cast to reach agreement [1]. There may be temporary disagree-
ment if two proposals occur at the same time; eventually, with very
high probability, one proposal will be established by picking the
longest blockchain. Nakamoto consensus uses a proof-of-work
(PoW) mechanism to probabilistically elect the leader, ensuring
a fair choice of leaders.
In terms of scale, Bitcoin employs bil-
lions of CPUs worth of computational power today (by observable
hashrates [2]), and is one of the largest completely decentralized
systems of such scale.

Unfortunately, Bitcoin’s transaction throughput does not scale
well. The Bitcoin network consumes massive computational power
and presently processes up to 7 transactions per second [3]. Other
centralized ﬁat payment processing systems, like MasterCard or
Visa are reported to processing 1, 200 to 56, 000 transactions per
second [4, 5]. The demand from practical applications is 3 to 4
orders of magnitude higher. Modiﬁcation to scale up existing pro-
tocol is a raging debate in the Bitcoin community [6–9]. Recent
work shows that these proposals have fundamental scalability lim-
its [10].

On the other hand, solutions which use classical Byzantine con-
sensus protocols [11–14] do not work in an open environment like
cryptocurrencies because of two fundamental challenges. First,
many of these papers assume that the network nodes have pre-
established identities or public-key infrastructure in place, which
does not exist in open environments like Bitcoin. Second, prac-
tical byzantine consensus protocols such as PBFT [13] require at
least a quadratic number of messages in the number of participants,
thus they are bandwidth-limited — more network identities leads
to worse performance. Network bandwidth limits the transaction
throughputs for a network of even a few hundred nodes severely.
This raises a fundamental question — are there any blockchain pro-
tocols that scale throughput linearly with the increase in the size of
the network?

Problem & Approach. Our goal is to seek a protocol for the
open, permissionless network wherein participating processors have
no pre-established identities, and where the transaction throughput
scales. We provide a new blockchain protocol called ELASTICO,
which achieves a sweet spot between classical byzantine consensus

17and Nakamoto consensus protocols. The key idea in our approach
is to partition the network into smaller committees, each of which
processes a disjoint set of transactions (or a “shard"). Speciﬁcally,
the number of committees grows near linearly in the total com-
putational power of the network. Each committee has a reasonably
small number of members so they can run a classical byzantine con-
sensus protocol to decide their agreed set of transactions in paral-
lel. Sharding protocols are commonly used in distributed databases
and in cloud infrastructure, wherein certain network infrastructure
can be trusted (e.g., see the commonly used two-phase commit
protocol) or where the goal is to tolerate crash (non-byzantine)
failures [15–17]. Note that several commercial and open-source
blockchains do not target the permissionless (open) setting, and as a
result, promise to scale by relying on trusted infrastructure [18–20]
or by using federated identities [21, 22] (see Section 6). To our
knowledge, we provide the ﬁrst sharding protocol for permission-
less blockchains tolerating a constant fraction of byzantine network
nodes. This is a well-recognized open problem [10]. Our protocol
makes the same assumptions as those implied in Bitcoin and other
cryptocurrencies, and we provide security proofs for key invariants
in our protocol.

Results. Without loss of generality, we assume that the net-
work contains n processors which have equivalent computational
power. ELASTICO exhibits almost linear scalability with computa-
tion capacity and does not require quadratic number of messages
as the network grows. ELASTICO tolerates up to f < n/4 adaptive
byzantine adversaries, where f and n are bounds on the adversar-
ial and total computational power respectively. 1 The protocol can
support the same blockchain data structure format (a hash-chain) as
Bitcoin; but, for further scalability, we propose a modiﬁcation that
permits better efﬁciency parameters.

From an efﬁciency perspective, our protocol shards the network
into an almost linear number of committees that scales with com-
putation capacity. Within each committee of size c (a few hundred)
identities, we run a secure consensus protocol which has message
complexity of O(c2) (best case) to O(c3) (worst case). Overall,
this yields a message complexity of at most O(nc3), where mes-
sages are of constant size.

We implement ELASTICO based on the most popular client for
Bitcoin [23]. Our implementation adds roughly 5, 000 C++ LoCs
on top of Bitcoin. The throughput of our prototype scales near lin-
early with respect to available computation i.e., O(n/ log log(n)),
when runs on our network simulation. With the same network im-
plementation as in Bitcoin, the scale up (blocks per epoch) for 100,
200, 400, 800 and 1, 600 nodes with equal computational power 2
are as theoretical expectation, namely 1, 1.89, 3.61, 6.98 and 13.5
times respectively. Finally, ELASTICO’s clean-slate design decou-
ples the consensus from block-data broadcasts, hence the band-
width spent by each node remains almost constant, regardless of the
size of the network. Our simulations are necessarily on a smaller
scale than Bitcoin; however, if we project our results to a full de-
ployment to a network of Bitcoin’s scale, we can expect a scale up
of 10, 000 in the number of agreed values per epoch. This agree-
ment throughput is 4 orders of magnitude larger than Bitcoin’s.

Contributions. We claim the following contributions.
• To our knowledge, ELASTICO is the ﬁrst secure candidate
for a sharding protocol for open blockchains that tolerates
byzantine adversaries. ELASTICO increases the blockchain’s

1Here, 1/4 is an arbitrary constant bounded away from 1/3, se-
lected as such to yield reasonable constant parameters.
2each node is one Amazon EC2 vCPU

transaction throughput almost linearly with the computational
power of the network.

• Our experiments on an idealized network simulation on Ama-
zon EC2, ranging up to 1, 600 network nodes, conﬁrm a near
linear scalability for ELASTICO.

2. PROBLEM & CHALLENGES
2.1 Problem Deﬁnition

We formalize the problem of designing a secure sharding proto-
col for blockchains as follows. Let there be n identity-less proces-
sors having the same computational power, a fraction f of which
are controlled by a byzantine adversary. The network accepts trans-
actions per block, a transaction i in block j is represented by an
i ∈ ZN . All processors have access to an externally-
integer xj
speciﬁed constraint function C : ZN (cid:55)→ {0, 1} to determine the
validity of each transaction. We seek a protocol Π run between
the processors which outputs a set X which contains k separate
“shards” or subsets Xi = {xj
i} (1 ≤ j ≤ |Xi|) such that the
following conditions hold:

• Agreement. Honest processors agree on X with a probability

of at least 1 − 2−λ, for a given security parameter λ.

• Validity. The agreed shard X satisﬁes the speciﬁed constraint

function C, i.e., ∀i ∈ {1..k},∀xj

i ∈ Xi,C(xj

i ) = 1.

• Scalability. The value of k grows near linearly with the size

of the network (or n).

• Efﬁciency. The computation and bandwidth used per proces-

sor stays constant regardless of n and k.

Our goal is to split the network into multiple committees, each
processes a separate set of transactions (e.g., Xi) called a shard.
The number of shards (k) grows near linearly on the size of the
network. The efﬁciency property represents the sharding advantage,
where the cost is localized within a committee. Once the network
agrees on the set X, it can create a cryptographic digest of X and
form a hash-chain with previous agreed sets in the previous runs of
Π. This serves as a distributed ledger of facts or transactions.

We point out that the agreement property in our problem is a re-
laxation of the original byzantine consensus problem [11, 12]. The
ﬁrst signiﬁcant distinction is the deﬁnition of “agreement.” Here,
we allow the honest processors to be in “probabilistic agreement”
such that processors agree on a value with some high probability,
rather than be in exact agreement. The second distinction is that the
agreed value can be the input of any processor, honest or byzantine.
The classical deﬁnition requires that the agreed value also be the
inputs of honest processors. In the blockchain problem, validity
can be checked externally — each honest processor can check if
the agreed value satisﬁes an externally-speciﬁed constraint C, and
accept a solution only if so.

Remark. Notice that this problem does not directly guarantee a
double spending check (a problem in cryptocurrency [1]), but im-
plementing such a check is possible given the agreement on the
transaction set which satisﬁes the constraints speciﬁed in C. In Ap-
pendix 10.2, we describe how one might build a new cryptocur-
rency like Bitcoin based on ELASTICO with all validity checks.

Threat Model. We consider the threat model of a static, round-
adaptive adversary. Processors controlled by the byzantine adver-
sary can be arbitrarily malicious, e.g., deviate from the protocol,

18and/or drop messages from other processors. All malicious proces-
sors can collude together. Further, we consider a round-adaptive
adversary, which can select which processors to corrupt at the start
of each run Π. The adversary has complete access to the outputs of
all previous i−1 runs to make its choices. However, once a protocol
run begins, the choices of compromised processors are ﬁxed. The
processors can setup point-to-point communication links between
them, and the adversary has full information about the messages
transmitted on all links.

Security Assumptions. We make two assumptions about the
underlying network overlay layer as in Bitcoin. Explicitly, (a) the
network graph between honest processors is connected and (b) the
communication channel between honest processors is synchronous,
i.e., once an honest user broadcasts any message, other honest pro-
cessors will receive it within a known bounded delay of δt seconds.
Note that such timing and connectivity assumptions are implicit
and necessary even in Bitcoin; otherwise, byzantine nodes can de-
lay blocks signiﬁcantly (simulating a gain in computation power)
or worse — a fraction of the network can be “eclipsed” by the ad-
versary. Attacks targeting our assumptions will apply to Bitcoin
too. However, such assumptions can be achieved with the right de-
sign of the underlying network topology [24, 25] — an orthogonal
problem of active research. On the other hand, we do not make any
assumption about a secure broadcast channel or a PKI system or
access to external source of randomness. That means, in our threat
model, the malicious processors can drop or modify messages, send
different messages to honest processors. We show in Section 4 that
the most that an adversary can do is to delay our consensus process.
Further, we assume that we know the upper bounds on the true
computation power n (say in Gigahash/sec), and that f is less than
1/4. Estimating such a bound is feasible from observing network
hashrates, as in Bitcoin, with the caveat that adversaries can pre-
tend to control f much lower than they actually do (just as in Bit-
coin today). For this work, we assume such information is exter-
nally available. We further assume that nodes are reliable during
protocol runs, and failed nodes are counted in the f malicious frac-
tion. Second, we assume that the total computation power of the
byzantine adversaries is still conﬁned to standard cryptographic as-
sumptions of probabilistic polynomial-time adversaries. Lastly, we
assume there exists a random oracle H : {0, 1}∗ (cid:55)→ {0, 1}γ which
outputs γ random bits given any input string.
2.2 Challenges

Sharding in a permission-less blockchain with the presence of
byzantine adversary is a well-recognized open problem [10] due
to many challenges. First, processors have no inherent identities
or external PKI to trust. A malicious processor can thus simulate
many virtual processors, thereby creating a large set of sybils [26,
27]. Thus, the protocol must prescribe some mechanism to allow
processors to establish their identities, yet limiting the number of
sybil identities created by malicious processors.

Once identities are established, the next challenge is to run a
sharding protocol among the identities with a fraction f of them
are byzantine. Our goal is to uniformly split all identities into sev-
eral committees with a condition that each committee has the ma-
jority as honest with high probability. Such a protocol becomes
straight-forward if one assumes a shared random coin to do the
sharding properly [15–17]. However, achieving a good random-
ness in a distributed network is a known hard problem. The best
solution to date can only tolerate up to 1/6 fraction of malicious,
with excessive message complexity [28]. Our protocol makes no
such assumption.

Third, we must ensure that an adaptive adversary observing all

the protocol runs, does not gain signiﬁcant advantage in biasing its
operations or creating sybil identities. Our protocol must tolerate
variable rate of identity creation and inconsistency in views of com-
mittee members (i.e., they may not agree on who are in the same
committee) because of both byzantine failures and network delays
in real networks (as in our threat model).

3. ELASTICO DESIGN

In this section, we present ELASTICO. For the rest of the pa-
per, unless otherwise stated, if some probability p is negligible, it
means it happens with probability at most O(1/2λ) for some secu-
rity parameter λ. Similarly, if some event happens with high prob-
ability (w.h.p), it happens with probability of at least 1−O(1/2λ).
If some event happens with non-negligible probability, it happens
with probability greater than O(1/2λ).
3.1 Solution Overview

set of values X =(cid:83)2s

The algorithm proceeds in epochs, each of which decides on a
i=1 Xi where 2s is the number of subsets Xi.
In this description, we describe the steps taken during one epoch.

The key idea is to automatically parallelize the available com-
putation power, dividing it into several smaller committees, each
processes a disjoint set of transactions (or shards). The number of
committees grows proportionally to the total computation power in
the network. All committees, each of which has a small constant
number c of members, run a classical byzantine consensus protocol
internally to agree on one value. A ﬁnal committee called the con-
sensus committee is responsible for combining the shards selected
by other committees, computing a cryptographic digest and broad-
casting it to the whole network. As the last step in the epoch, the
ﬁnal committee generates a set of shared public random bit strings,
which have a bounded bias. These random strings are used in the
subsequent epoch as a source of randomness—ensuring that the
adversary cannot use observations from previous epoch to simulate
any non-negligible gain in computational power.

In each epoch, processors execute the following 5 steps:

1. Identity Establishment and Committee Formation. Each pro-
cessor locally generates an identity consisting of a public key,
an IP address and a proof-of-work (PoW) solution [1]. The
processor must solve a PoW puzzle which has publicly veriﬁ-
able solutions to generate the ﬁnal component of the identity.
A PoW solution also allows others to verify and accept the
identity of a processor. Because solving PoW requires com-
putation, the number of identities that the malicious proces-
sors can create is limited by the fraction of malicious com-
putational power f. Each processor will then be assigned to
a committee corresponding to its established identity.

2. Overlay Setup for Committees. In this step, processors com-
municate to discover identities of other processors in their
committee. The overlay of a committee is a fully-connected
subgraph containing all the committee members. A naïve
solution is for every processor to broadcast its identity and
committee membership to everyone; however, this solution
will result in O(n2) messages, which is not scalable. We
provide a simple solution that requires a small number of
broadcasts, i.e., O(nc), after which identities in same com-
mittees can quickly identify each other.

3. Intra-committee Consensus. Processors run a standard byzan-
tine agreement protocol (e.g., PBFT [13]) within their com-
mittee to agree on a single set of transactions (or a shard).

19There exist simple solutions to guarantee that all commit-
tees propose disjoint shards, e.g., each committee works on a
separate shard of transactions based on their committee ID.
Each committee sends the selected shard, signed by enough
members (i.e., c/2 + 1), to a designated ﬁnal committee.

4. Final Consensus Broadcast. The ﬁnal committee computes
a ﬁnal value from all the values received from other commit-
tees. To do so, members in ﬁnal committee run a byzantine
consensus protocol to agree on the ﬁnal result and broadcast
it to the network.

5. Epoch Randomness Generation. The ﬁnal committee runs a
distributed commit-and-xor scheme to generate an exponen-
tial biased, but bounded, set of random values. The random
values are broadcast to the network and used in the PoW in
the next epoch.

Parameters. Throughout this paper, we use the following nota-
tion: n is the total number of identities that we expect to be gener-
ated in an epoch, f = 1/4 is the fraction of computational power
controlled by malicious users, the size of each committee is c, the
number of committees is 2s, for some constant s. Without loss of
generality, s can be picked such that n = c · 2s. Thus, ELASTICO
scales up almost linearly because the expected number of PoW so-
lutions to have each committee has at least c members is O(n log s)
(or O(n log log (n/c))). For a concrete analysis, we refer readers
to Appendix 10.1. Note that picking smaller s leads to lower la-
tency and bandwidth consumption, which allows one to tune the
network consumption. In addition, the size of committee c is deter-
mined by the security parameter λ and the expected network delay
δt.
Efﬁciency. Our protocol requires O(c) broadcasts to the whole
network (steps 2, 4 and 5). Each such broadcast can be imple-
mented in O(n) message transmissions. Steps 3, 4 and 5 require at
most c round of c2 multicasts for each committee of size c. There-
fore, the total number of messages is O(nc + nc3) or roughly O(n)
if we consider c to be a small constant compared to n.

Security. In each epoch, for f = 1/4, our protocol guarantees
the following security properties S1–S5, the proofs of which are
presented in Section 4.

• S1. Given a security parameter λ, there exists n0 such that
∀n(cid:48) ≥ n0, among the ﬁrst n(cid:48) identities created, at most 1/3
are malicious w.h.p. The gap between f and 1/3 accounts
for the variance in the number of PoW solutions found by
the adversary. Our committee size c is then chosen as based
on the value of n0 (e.g., c ≥ n0) such that every committee
has at most a fraction 1/3 of malicious identities.

• S2. After Step 2, all committee members have their own
view of at least c members in the committee. There may be
discrepancies between two views due to network latency and
byzantine behaviors. This discrepancy, however, is bounded
by c/3 w.h.p and all honest members have identities of other
honest members in their views. Further, the number of unique
identities in all views is bounded by 3c/2, of which at most
1/3 fraction are malicious w.h.p.

• S3. For each committee, Step 3 yields a consensus on the set
of transactions Xi proposed by members in the committee.
The chosen Xi is signed by at least c/2 + 1 of the identities
on the committee. This ensures at least one honest member
has veriﬁed and agreed on the value.

• S4. Step 4 yields a valid set X =(cid:83)2s

i=1 Xi which combines
all proposed sets Xi from other committees. X is also signed
by at least c/2 + 1 of the members in the ﬁnal committee.

• S5. Step 5 will yield a set of random r-bit values with sufﬁ-
cient randomness. Explicitly, the attacker can predict the ran-
dom value, given r is large enough, with a negligible proba-
bility in λ.

Note that we select f = 1/4 in order to achieve a practical value
of committee size. Theoretically, ELASTICO can work with any
f less than 1/3 by increasing the committee size c accordingly to
f. The 1/3 bound is because we need to run a consensus protocol
(e.g., PBFT [13]) at every committee in Step 3, which can tolerate
at most 1/3 fraction of malicious committee members.
3.2 Identity Setup and Committee Formation
First, each processor locally chooses its own identity of the form
(IP, PK), which are public key and IP address respectively for the
authenticated communication later.
In order for the network to
accept an identity, the processor must ﬁnd a PoW solution corre-
sponding to its chosen identity. As a “seed” for the PoW, we need
a public random string epochRandomness generated at the end of
the previous epoch to ensure that the PoW was not precomputed.
We discuss how this is generated and veriﬁed in Section 3.6. As-
sume, for now, that epochRandomness is a public random string
generated in the previous epoch. Speciﬁcally, each processor lo-
cally searches for a valid nonce that satisﬁes the following con-
straint:

O = H(epochRandomness||IP||PK||nonce) ≤ 2γ−D.

D is a predeﬁned parameter in the network which determines how
much work a processor has to do to solves a PoW 3. Note that
one can use other mechanisms like Proof of Stake [29], Proof of
Space [30,31] instead of PoW to establish identities for processors.
Next, our protocol assigns each identity to a random commit-
tee in 2s, identiﬁed by an s-bit committee identity. The committee
assignment must be random, even for the malicious users: a prob-
abilistic polynomial-time adversary should not be able to bias its
committee assignment with non- negligible probability. We use a
PoW to achieve these goals. Speciﬁcally, the last s bits of O speci-
ﬁes which (s-bit) committee id that the processor belongs to. Each
committee will process a separate set of values (e.g., a shard) based
on this s-bit ID.

All processors know epochRandomness and choose their iden-
tity IP and P K privately. For any choice of nonce, H produces
a γ-bit random output. The probability that a single invocation
of H satisﬁes the constraint for a randomly chosen nonce is thus
p = 2−D. No efﬁcient adversary can ﬁnd a nonce that satis-
ﬁes the constraint with non-negligible probability better than p by
the cryptographic pre-image resistance assumption. We later prove
in Lemma 2 (Section 4) that among the ﬁrst n(cid:48) identities, at most
1/3 · n of identities are created by byzantine processors w.h.p.

For establishing S1, we need to examine the number of honest
and byzantine identities that map to any given committee. Since H
is a random oracle, we can treat the bits in its output as unbiased
and random. Therefore, the s bit strings generated in the solution
are random, and an identity is mapped to a given committee with
probability 2−s. Further, if n = 2sc, then on average it requires
O(n log s) PoW solutions to have each of 2s committees has at
least c members (see Appendix 10.1). This is why the scale up
factor is almost linear.
3For example, D = 20 means O has at least 20 leading zeros.

20Byzantine adversaries can choose not to broadcast valid solu-
tions, thereby denying membership in a committee. However, this
does not advantage their membership in any other committee. It
remains to choose the parameters s, which determines the number
of committees, and D, which determines the difﬁculty of the PoW.
These are discussed in Section 5 with our experiments.

3.3 Overlay Setup for Committees

Once identities and their committees are established, committee
members need a way to establish point-to-point connections with
their committee peers. A naıve solution is to ask every proces-
sor to broadcast its identity and committee membership to every-
one; however, this solution will result in O(n2) messages, which
is not scalable. Another challenge is that identities are established
through a PoW, which is a probabilistic process that occurs over
time: new identities are continuously being created at some rate.
Ideally, we need a mechanism to establish the ﬁrst c members of
the committee so that all honest members have the same view of
the member set. One could run any byzantine agreement proto-
col here which tolerates up to 1/3 fraction of malicious identities.
However, this would yield BFT protocol running over the entire
network without any parallelization (e.g., O(n3) cost in the worst
case). Here we show something more efﬁcient which has O(nc)
message complexity.

To reduce the number of broadcast messages, we have a special
committee of size c to serve as a set of “directories." All identities
can ﬁnd their committee peers by contacting the directory commit-
tee and then set up point-to-point links. Further, we allow commit-
tee members (including directory members) to have different views
of the member set, a challenge that most previous BFT protocols do
not face. Our protocol can tolerate this discrepancy and show that
i) all honest members have others’ identities in their view; ii) the
difference is bounded by c/2 as in S2. Our algorithm to setup the
overlay for committee is depicted in Algorithm 1.

More speciﬁcally, the directory committee is simply a commit-
tee of the ﬁrst c identities. During step one, if a processor ﬁnds
a valid solution for PoW, and it has not seen c identities, then the
processor will broadcast this identity to the whole network. On
the other hand, during step one, whenever a processor ﬁnds a valid
solution for PoW for an identity, the processor will inform all di-
rectory members (Line 17). Each processor informs only the ﬁrst c
directories that it sees in the network. Note that each processor can
have its own view of who are the ﬁrst c directories.

In this way, directory members keep track of the committee mem-
bership announcements. Once each committee contains at least c
identities each, directory members multicast the list of committee
members to each committee member (Line 33). To reduce the num-
ber of messages, a non-directory committee member only receives
the list of members in its own committee. Notice that directory
members do not have to agree on the same set of members for
a committee. Each directory member can decide independently
which members are in a given committee.

For committee members, each will receive at least 2c/3 lists of c
committee members (from at least 2c/3 honest directories — due
to S2). A malicious directory may not send any list to committee
members. Worse, malicious directories may send a bad member
list to committee members (i.e., a list which favors malicious iden-
tities). To prevent that, a committee member takes the union of all
the identities that it receives to create a view of at least c committee
members (including itself) (Line 19). It is still possible that com-
mittee members have different member sets. We analyze both the
sources and show that the discrepancy is bounded by at most c/2
members in Lemma 3, which proves S2.

Algorithm 1 Algorithm to form the directory committee and setup
overlay for other committees.
Input: c: committee size; k = 2s: number of committees
Output: Every committee member receives at least c members of

(cid:46) Done by everyone

(cid:46) Receive a PoW solution

(cid:46) Find a PoW solution

end if
if lV iews ← GetViewsFromDirectories() then

(cid:46) Move to next step in the protocol

its committee

w ← ReceivePoW()
if len(curDirectories) < c then

myPoW.append(w)
Send(curDirectories, w)

curDirectories.append(w)

end if
w ← SolvePoW()
if len(curDirectories) < c then
curDirectories.append(w)
Execute RunAsDirectory()
BroadcastToNetwork(w)

else

curDirectories ← empty
myPoW ← empty
while True do

1: procedure FORMCOMMITTEE
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
end if
21:
end while
22:
23: end procedure
24: procedure RUNASDIRECTORY
25:
26:
27:
28:
29:
30:
31:
32:
33:

v ← union(lV iews)
return v

while True do

to members of committee i

end for
return True

34:
35:
end if
36:
end while
37:
38: end procedure

(cid:46) Done by directories
w ← ReceivePoW() (cid:46) Accept all PoWs in last round
i ← GetCommitteeNo(w)
if len(commList[i]) < c then
commList[i].append(w)

end if
if ∀i ≤ k, len(commList[i])≥ c then

for i = 0, i < k, i ← i + 1 do

MulticastCommittee(i) (cid:46) Send commList[i]

Overall, security follows from two arguments. First, the most
that a malicious directory can do is to not send honest identities
and favor malicious identities in a committee. It is because mali-
cious directories cannot create new identities or change committee
assignment of new identities due to the PoW. Further, committee
members decide who are in their committees based on all views re-
ceived from directories. As per S1, among the c directories, at least
2c/3 are honest. Thus honest members in a committee will know
all other honest members.
3.4 Intra-committee Consensus

Once a committee is established, the protocol to agree on a set of
transactions can reuse any existing authenticated byzantine agree-
ment protocol [32, 33]. As shown in S3, the union of all views will
have at most 3c/2 members, and at most 1/3 of them are mali-
cious. In the worst case, we can assume that each committee has
3c/2 members, of which at most 1/3 of them are malicious. Since
each honest member knows all other honest members (due to S2),
a member can treat all identities which are not in its view as ma-
licious. Hence, existing byzantine agreement protocols will work
securely in our setting. For example, we can use the POLYBYZ
algorithm [32], or PBFT [13], or the more recent SYBILSENSUS
algorithm [33].

21Once an agreement is reached, the selected transaction set is
signed by at least c/2+1 signatures to guarantee some honest mem-
ber has veriﬁed and accepted the value. Each committee member
then sends the signed value along with the signatures to the ﬁnal
committee (using the directory, again, to acquire the list of ﬁnal
committee members). The ﬁnal committee can verify that a certain
value is the selected one by checking that it has sufﬁcient signa-
tures. Note that this ﬁnal value can be a small cryptographic digest
(e.g., a Merkle hash root) representing the set Xi of values xj
i (or
transactions) that the committee agrees on.
3.5 Final Consensus Broadcast

The next step of the protocol is to merge the agreed values of
committees and to create a cryptographic digest (a digital signa-
ture) of the ﬁnal agreed result. A ﬁnal committee (which includes
all members with a ﬁxed s-bit committee id) is designated to per-
form this step. The merge function is simple: each ﬁnal committee
member validates that the values received from the committees are
signed by at least c/2 + 1 members of the proper committee, and
takes the ordered set union of all inputs. To ensure that the re-
sult is indeed correctly composed from the correct inputs, the ﬁnal
committee runs the same intra-committee algorithm described pre-
viously. This step obtains a veriﬁable signature by at least c/2 + 1
members of the ﬁnal committee, which the entire network can ver-
ify upon broadcast.
3.6 Generating Epoch Randomness

In the ﬁnal step of the protocol, the ﬁnal committee (or consensus
committee) also generates a set of random strings for use in the
next epoch. For the ﬁrst epoch, a previous approach can be used
to generate the epochRandomness [28]. However, this solution
tolerates at most 1/6 fraction of malicious members and only works
for a small network since it requires excessive message complexity.
Our solution for this step consists of two main phases. In the
ﬁrst phase, each member of the ﬁnal committee chooses a r-bit
random string Ri and sends a hash H(Ri) to everyone in the com-
mittee. The ﬁnal committee then runs an interactive consistency
protocol to agree on a single set of hash values S [11]. The ﬁnal
committee members will broadcast S, along with the ﬁnal set of
X in Section 3.5 to everyone in the network. This set S contains
at least 2c/3 hash values and serves as a commitment to the ran-
dom strings. This ﬁrst phase can be done with the previous step in
Section 3.5, but for simplicity, we describe it separately here.

In the second phase, each member of the ﬁnal committee broad-
casts a message containing the random string Ri itself to everyone
(i.e., not just to the ﬁnal committee). This phase starts only after the
agreement of S is done, i.e., having 2c/3 signatures on S. This is
to guarantee that honest members release their commitments only
after they are sure that the committee has agreed on S and the ad-
versary cannot change its commitment.

At this point, each user in the system has received at least 2c/3
and at most 3c/2 pairs of Ri and H(Ri) from members of the ﬁnal
committee: the honest members follow the protocol, while the ma-
licious users may choose to not release their commitments. Users
discard any random strings Ri that do not match the commitments
H(Ri).

For the purpose of the next epoch, each user takes an XOR of
any c/2 + 1 random strings Ri that it receives. Note that users
may select different Ri. We consider the XOR of any subset of
c/2 + 1 valid random strings sent by the ﬁnal committee to be
a valid epochRandomness. Recall that these random strings are
used as the seed for the PoW in the next epoch. In order for others
to verify that a PoW is valid, the user should attach to their PoW

Step Potential problems

1

2

P1. PoW variance
- Too many malicious identities
- Bias the committee distribution
P2. Inconsistency between views
- Adversaries withhold identities
- Network latency

3, 4 P3. Cannot reach consensus
5
P4. Bias the random strings

State-
ment Lemma

S1

S2

Lem. 2

Lem. 3

S3, S4 Lem. 4
Lem. 6
S5

Table 1: Sources of byzantine advantage in each step of ELASTICO and the
corresponding supporting security statements and lemmas which resolve
them.

solution the set of c/2+1 strings Ri used to generate the seed. Any
other user can then verify if these c/2 + 1 random strings match the
commitments in S.

A formal proof which shows the strings generated from this pro-
cess are sufﬁciently random is presented in Section 4. Here we
provide an intuition why this works. Any combination of c/2 + 1
strings Ri must share at least one Ri from an honest member,
and that value is not known to the adversaries before everyone has
agreed on the set of commitments S. Thus, the value R produced
by XOR-ing c/2 + 1 strings Ri is perfectly random. The adver-
sary, however, can choose which c/2 + 1 values he uses to bias
R to be in a set Sa of his choice. We show that this probability is
bounded by 1/2r−λ−c+log(c)/2. Thus, if we pick r large enough
(e.g., r > 2λ + c − log(c)/2), the advantage of the adversary is
negligible in λ.

4. SECURITY ANALYSIS

In this section, we provide security analysis for how ELASTICO
prevents potential threats and works securely. We also discuss how
byzantine adversary gain no signiﬁcant advantage.

We begin by clarifying several assumptions. First, we assume
that the network is partially synchronous: any message can reach
the destination with a maximum delay of δt, thus network protocol
can be assumed to happen in rounds, each lasts for δt time. All
our claims for an epoch depend on there being sufﬁciently random
strings generated in the previous epoch.

Deﬁnition 1 (Good randomness). We say that an epoch has a
good randomness if:

(i) every user has a publicly random string of r bits, veriﬁably

generated in the previous epoch;

(ii) no user has access to such a veriﬁable random string more

than δt prior to the beginning of the epoch;

(iii) malicious users can bias the randomness with negligible prob-

ability.

We now prove the security properties of ELASTICO. In particu-
lar, we start with S1, which states honest identities take a dominate
portion in all the generated identities.

Lemma 2 (S1: Good Majority). In every epoch with good ran-
domness, for every sufﬁciently large integer n(cid:48) ≥ n0: among the
ﬁrst n(cid:48) identities created, at most n(cid:48)/3 − 1 are controlled by the
adversary w.h.p.

Proof. If all the users start at the same time, each solution gener-
ated has a probability 1 − f = 3/4 of being taken by the honest
processors. Now, let Xi be an indicator random variable which
takes value one if the ith identity is generated by an honest proces-
i=1 Xi. Then, X follows a binomial distribution.

sor. Let X =(cid:80)n(cid:48)

22Thus we have:

Pr(cid:2)X ≤ 2n

/3(cid:3) =

(cid:48)

=

(cid:100)2n(cid:48)/3(cid:101)(cid:88)
(cid:100)2n(cid:48)/3(cid:101)(cid:88)

k=0

k=0

Pr[X = k]

(cid:32)

(cid:33)

n(cid:48)
k

f n(cid:48)−k(1 − f )k

This probability decreases exponentially in n(cid:48). Given a security pa-
rameter λ, we can ﬁnd n0 such that Pr [X ≤ 2n(cid:48)/3] ≤ 2−λ,∀n(cid:48) ≥
n0.

The committee size c is at least n0 to guarantee that the fraction
of malicious members in a committee is bounded by 1/3, with re-
gard to the security parameter λ. The value of n0 depends on the
security parameter λ. For example, if λ = 20, or the probabil-
ity that something bad happens is once every 1 million epochs, we
have n0 ≈ 600.
Lemma 3 (S2: Good views with bounded inconsistency). In every
epoch with good randomness, if the directory size is c, then for
each committee, with high probability, we guarantee the following
properties.

(i) Each member has their own view of who are in the commit-
tee. Two views of two honest members differ by at most 1/3
of the committee size;

(ii) All honest members have identities of other honest members

in their views;

(iii) The total number of unique identities in all views is at most

3c/2 of which less than 1/3 fraction are malicious.

Proof sketch. The inconsistency between views of member set is
because of two reasons: network latency and byzantine behaviors.
We calculate the difference in views caused by each source.

Since the network delay and the probability in ﬁnding a PoW so-
lution, e.g., it is hard to tell who are the last committee members.
Since honest directories accept all valid PoWs in the last round
(before the committee gets ﬁlled by at least c members), all hon-
est committee members will have other honest identities in their
view. This leaves the malicious behaviors the main source of the
discrepancy.

Malicious processors can decide to withhold their identities and
only publish them in some particular time to cause the maximum
discrepancy between views of member set of honest members. How-
ever, honest members always share the same view of honest identi-
ties, thus the maximum discrepancy that an adversary can cause is
the number of identities that he/she can create. As per Lemma 2,
this amount is always less than 1/3 of the total number of commit-
tee members w.h.p.

We next prove that the maximum number of identities in all
views is 3c/2 and less than c/2 identities are malicious. An ad-
versary can wait until right before the last identity in a committee
is found to publish all his/her identities. According to Lemma 2,
the maximum number of malicious identities can be created before
honest processors ﬁnd all c identities is c/2. Thus, the total num-
ber of identities is bounded by 3c/2 and at most c/2 identities are
(cid:3)
malicious w.h.p.
We now show S3, S4 which argue that a committee (including
the ﬁnal committee and other commitees) correctly decides a single
value (or a set of transactions Xi).

Lemma 4 (S3, S4: Consensus). In every epoch with good random-
ness, the honest members agree on a unique set Xi with at least
c/2 + 1 signatures, with high probability.

Proof. Although committee members have different views of mem-
ber set, we can tolerate the inconsistency by considering a broader
committee of size up to 3c/2. As we have established above, the
fraction of malicious identities is bounded by 1/3 in any view, and
honest members have other honest members in their view. Thus,
any Byzantine consensus protocol (e.g., POLYBYZ, PBFT) which
can tolerate up to 1/3 malicious fraction can guarantee agreement
in the committee, i.e., only one value selected.

Since our network is synchronous, the liveness property is al-
ways achieved in these consensus protocols, i.e., the committee al-
ways reaches consensus in polynomial time. The committee then
collects enough number of signatures (e.g., c/2 + 1) to guarantee
that at least one honest member has signed the chosen value.

Lastly, we show Lemma 5 and S5, i.e., the shared randomness
generated is sufﬁciently random. Speciﬁcally, we derive the fol-
lowing deﬁnition from the standard deﬁnitions of bias [28].
Deﬁnition 1. Let F : {R0, R1, .., Rc} (cid:55)→ {0, 1}r, computed over
the set of r-bit input strings. For all choices of set S ⊆ {0, 1}r, let
EF (S) be the expected number of R values that the adversary can
generate such that R ∈ S. Assuming the adversary controls some
of the inputs, the bias β(F ) is deﬁned as:

(cid:26) EF (S)

E(S)

,

E(S)
EF (S)

(cid:27)(cid:33)

,

(cid:32)

β(F ) = log

S ⊆ {0, 1}r

max

max

in which E(S) = |S|/2r — the value of EF (S) if the adversary

does not control any inputs.

Lemma 5 (Bounded Bias to Randomness). Given a set of c > 3
random r-bit strings of which at most (cid:98)c/2(cid:99) strings are known and
generated by the adversary, the computation of R = ⊕Ri has a
bias bounded by c − log(c)/2.
Proof. We deﬁne F : {R0, R1, .., Rc} (cid:55)→ {0, 1}r as a function
which selects c/2 + 1 strings Ri from c strings and compute R =
⊕Ri. The computation of R can be decomposed into F = Fh ⊕
Fd, in which Fd is computed over the set of inputs controlled by
the adversary, and Fh is computed over the set of inputs Rh from
honest users which the adversary does not know.
The bias of Fd, by deﬁnition of bias is c−log(c)/2. This follows

from the fact that the adversary can pick (cid:0) c

(cid:1) combinations of

c/2 strings Ri to decide Rd. This number has an upper bound of
2c−log(c)/2 computed based on Stirling’s approximation. Thus, we
have

β(Fd) < c − log(c)/2.

On the other hand, Rh is not generated by the adversary, so bias of
Fh is 0 because EFh (S) = E(S). By composition, the total bias
is c − log(c)/2.

c/2

Lemma 6 (S5: Good Randomness). In every epoch with good ran-
domness, with high probability, at the end of the epoch every user
computes a random string of r bits that:

(i) can be predicted with a negligible probability;
(ii) can be veriﬁed as validly generated in that epoch.

Moreover, no user knows any of the random strings until at least
one round prior to the end of the epoch. That is, if epoch e has
good randomness, then epoch e + 1 also has good randomness.

Proof. By Lemma 3, there are at least 2c/3 honest members in the
ﬁnal committee w.h.p. Thus, each user in the network receives a set

23R having from 2c/3 to 3c/2 random strings from members in the
ﬁnal committee in the last round of the epoch. The user’s random
string R is generated by XOR-ing any c/2 + 1 strings in R.

Since there are at most c/2 malicious members in the ﬁnal com-
mittee, at least one random string Ri used by malicious users orig-
inated at an honest user. Further, this Ri is not known by the ma-
licious users before the ﬁnal committee has agreed on the set S.
Thus, the r-bit string R generated by XOR-ing any c/2 + 1 is per-
fectly random. However, the attacker can still pick any of his c/2
values Ri to bias his random string R. As per Lemma 5, the at-
tacker’s bias to R is bounded by c − log(c)/2. Thus if we pick r
large enough, e.g., r > O(λ) + c − log(c)/2, the probability that
the attacker can guess R is negligible.

Finally, by construction any user can verify that the set of c/2+1
bit strings is valid by checking the commitments sent out previ-
ously. Similarly, it is immediate that no user knows the random
string prior to two rounds before the end of the epoch.

With the above lemmas in hand, the correctness of the ELAS-
TICO protocol follows from the following facts. We omit a theo-
rem and its formal proof, which is by induction and invokes Lemma 2–
Lemma 6 on different cases.
Claim 7. For every epoch i ≥ 1, with high probability, the follow-
ing properties hold:

(i) the ﬁnal committee will broadcast only one combined value
to the network with at least c/2 + 1 signatures and no other
combined value will have c/2 + 1 signatures;

(ii) this combined value contains 2s sub-values each of which
comes from a committee and is veriﬁed by at least one honest
processor; and

(iii) at the end of the epoch, each user has a publicly veriﬁable
random bit string of length r which has sufﬁcient randomness
(i.e., the following epoch has good randomness).

Notice that we have shown, at this point, that each epoch ends
with a correct (combined) value selected. Any user that is in the
system can, by listening, verify that this combined value is the cor-
rect and honest one. Often, it is desirable that the correctness by
externally veriﬁable, e.g., by a user that was not in the system at
the time.
In Bitcoin, this is achieved by showing that the chain
constructed is the longest chain, with very high probability (i.e.,
exponentially small probability). In fact, the same property holds
here: on average, it will take the malicious users twice as long to
generate a signed ﬁnal value as the honest users. Hence, an honest
“chain” will grow twice as fast as a malicious “chain” and hence
with very high probability it will be externally veriﬁable. (For a
more detailed discussion of this issue in Bitcoin, see [34]; the argu-
ment here is similar.)

5.

IMPLEMENTATION & EVALUATION

We implement ELASTICO and empirically evaluate the scalabil-
ity of ELASTICO and previous solutions. The goals of our evalua-
tion are twofold. We ﬁrst measure the scalability and efﬁciency of
ELASTICO when the network size increases. We aim to establish
that the performance of ELASTICO matches its theoretical analysis.
The second goal is to compare ELASTICO to other related consen-
sus protocols including Bitcoin [1], Bitcoin-NG [9] and PBFT [13].
5.1 Implementation

We implemented all components of ELASTICO based on the most
popular Bitcoin client version v0.12.1 [23]. Our implementation
is in C++ and has roughly 4, 000 LoC above that of Bitcoin’s code

base. We choose PBFT [13] as the consensus protocol for com-
mittees in ELASTICO. Since there was no well-maintained open
source implementation of PBFT when we started, we built a full-
ﬂedged implementation of our own, and the implementation may
be of independent interest to the community. We plan to release
ELASTICO for public use.

Recall that once the committee formation is complete, a normal
committee runs one instance of PBFT protocol to agree on a data
block of size 1 MB. The ﬁnal committee (or consensus committee)
runs two consecutive instances of PBFT protocol to agree on one
data block and the ﬁnal block which aggregates all data blocks from
other committees. Thus, nodes in the consensus committee bear
more cost than in normal committees.

Network implementation. We reuse the network implementa-
tion of Bitcoin for our peer-to-peer layer in the overall network.
The communications within committee are handled separately, in
which nodes in a committee form their own peer-to-peer overlay
network. Messages passed within a committee are not sent to nodes
in other committees. The choice of using peer-to-peer instead of
point-to-point communications within a committee is because the
latter requires much more resource for a node to open a socket con-
nection to each of 100 committee nodes.

Rate-limited mining. We artiﬁcially limit the mining rate to
allow testing ELASTICO with large scale experiments on a the pub-
lic EC2 infrastructure (which is expensive for our scale of testing).
In order to control the exact fraction of malicious computational
power, we limit the number of hash operations (i.e.,SHA2) that a
node (i.e., a processor) can perform to 1 operation per second. Each
hash output has a probability 1/600 of ﬁnding a valid PoW solu-
tion. Thus, on average it takes a processor 600 seconds to establish
its identity. Note that we still faithfully simulate the mining process
in Bitcoin, since we use the same implementation for mining.
5.2

ELASTICO’s Scalability

Experimental Setup. We run several experiments with different
settings on Amazon EC2 to measure the scalability of ELASTICO.
We vary the number of nodes in the network from 100 to 1, 600,
using up to 800 c4.large EC2 instances in two different regions
including Oregon and California. Each EC2 instance is shared by
two nodes, has 2 Amazon vCPUs and 3.75 GB of memory. Thus,
increasing the number of nodes in the network will increase the
computation power accordingly. We ﬁx our committee size at c =
100, to limit the performance overheads due to PBFT as we discuss
in Section 5.3.

Scalability of ELASTICO. We start with a network of 100 nodes
then double the network size 4 times, raising to 1, 600 nodes in
the last setting. We quantify the number of blocks created in each
epoch, time to reach consensus, number of PoWs required to ﬁll all
committees. The results are plotted in Figure 1. All numbers are
averaged after 10 epochs.

Figure 1 shows that ELASTICO scales up the block throughput
almost linear to the size of the network. The number of blocks per
epoch increases linearly (from 1 to 16) as we increase more nodes
to the network (100 to 1, 600 accordingly). However, the epoch
time is longer (e.g., 600 seconds in 100 nodes to 711 seconds in
1, 600 nodes) since it requires relatively more time to ﬁnd enough
PoWs to ﬁll up all committees when the number of committees in-
creases. In addition, we observe that the time to reach consensus on
blocks once the committee formation is complete remains almost
constant, regardless of the network size. For example, the laten-
cies to reach consensus are 103 and 110 seconds when the network
sizes are 400 and 800 nodes respectively. In summary, our experi-

24500

400

300

200

100

0

)
s
(
y
c
n
e
t
a
L
n
o
i
t
a
g
a
p
o
r
P

1 Block
2 Blocks
4 Blocks

96

48
24

50
26
13

456

227

114

322

161

80

187

92

46

100

200

400

800

1600

Network size (number of nodes)

Figure 3: Latency of Bitcoin-NG with different number of micro blocks

8.1

8.09

8.11

8.12

8.13

1 Block
2 Blocks
4 Blocks

4.05

4.05

4.06

4.06

4.07

2.04

2.02

2.03

2.03

2.03

)

B
M

(

e
d
o
n
r
e
p

h
t
d
i
w
d
n
a
B

8

6

4

2

100

200

400

800

1600

Network size (number of nodes)

Figure 4: Bandwidth consumption of Bitcoin-NG with different number of
micro blocks and network sizes

therefore the number of blocks per epoch that the network can agree
on is 10, 000. State differently, ELASTICO scale up the agreement
by 4 orders of magnitude when deployed in a network of current
Bitcoin’s scale.
5.3 Comparison to Related Systems

We show how ELASTICO outperforms existing protocols in many
applications where the veriﬁcation check of a block does not re-
quire to scan through the entire history of the blockchain, but only
the block itself. A prominent example of such applications is an
append-only database for certiﬁcate directory, information regis-
tration platform and so on. In these applications, verifying a data
block can be done independently disregard to the current state of
the blockchain, thus ELASTICO nodes do not have to download
data blocks from other committees. We show that only ELASTICO
achieves the efﬁciency property deﬁned in Section 2. On the con-
trary, other protocols like Bitcoin, Bitcoin-NG [9] and PBFT re-
quire much more network bandwidth and/ or local computation at
each node when processing more transactions.

Bitcoin. Recent work has shown the scalability limit of Bit-
coin [10]. We also run our own set of experiments and get con-
sistent results with the results in [10]. Due to the space constraint,
we do not include this set of experiments and the results here.

Figure 1: ELASTICO scales up the throughput nearly linearly in the
computation capacity of the network.

Figure 2: Cost per node in ELASTICO stays almost constant regard-
less of network size.

ments conﬁrm the expected scalability of ELASTICO’s transaction
throughput.

Efﬁciency of ELASTICO. We next evaluate the efﬁciency prop-
erty (deﬁned in Section 2) of our protocol. Figure 2 depicts the
number of messages sent/received and the bandwidth consumed at
each node for different network sizes for reaching consensus once
the committees are formed. The number of messages per node re-
duces as more nodes join the network, while the bandwidth con-
sumed at each node ﬂuctuates around 5 MB per node.

The decrease in the number of messages exchanged when there
are more nodes in the network is because the extra messages re-
quired for running the second instance of PBFT in the ﬁnal commit-
tee are amortized. Thus, we see the reduction in the number of mes-
sages exchanged per node when the network size increases from
100 nodes (2, 416 messages) to 200 nodes (1, 821 messages). The
bandwidth used at each node, on the other hand, remains roughly
unchanged, e.g., 4.93 MB (400 nodes) and 5.01 MB (800 nodes)
per node. This is because the communication costs within commit-
tees are localized. Thus even when more nodes join the network,
existing nodes does not have to bear more costs.

Extrapolation to Bitcoin’s scale. We extrapolate the scale up
that ELASTICO can achieve in a network of current Bitcoin’s scale,
if we assume our scalability holds in real network. As of February
2016, the hash rate of Bitcoin network is 1.2× 109 GHash/s. Thus,
we assume that the network consists of n = 1, 000, 000 equivalent
processors, each of which can perform 1.2 × 103 GHash/s. We
consider a setting where the committee size is 100, thus the number
of committees is 10, 000. Each committee agrees on a single block,

0 2 4 6 8 10 12 14 16 18 0 100 200 300 400 500 600 700 800 900 100 200 400 800 1600 No. of Block  Time (s) Network size (number of nodes) Committee Formation time Consensus Time Blocks Final Scalability 4.00 4.40 4.80 5.20 5.60 6.00 500 1000 1500 2000 2500 3000 3500 100 200 400 800 1600 Bandwidth used per node (MB) Number of messages per node Network size (number of nodes) Number of messages Bandwidth 25Bitcoin-NG. Our next set of experiments compare the scalabil-
ity of ELASTICO and a recent scalability proposal namely Bitcoin-
NG [9]. Bitcoin-NG slightly modiﬁes Nakamoto consensus proto-
col to propose more blocks per epoch. At a high level idea, Bitcoin-
NG also probabilistically selects a leader, and the leader can pro-
pose several micro blocks (i.e., data blocks) per epoch. The leader
of the next epoch can decide which data block of previous epoch
he/ she wants to build on top.

We measure the bandwidth consumption per node of Bitcoin-NG
in different settings where the numbers of nodes are 100, 200, 400,
800, 1, 600. We also quantify the latency to broadcast different
number of micro blocks.
In terms of experimental setup, nodes
are distributed equally in two Amazon regions namely Oregon and
California. Each node randomly connects with 4 other nodes to
form a peer-to-peer network. We collect the source code of Bitcoin-
NG from the authors through our private communication. Figure 3
and Figure 4 report our results.

Figure 3 shows the latency of Bitcoin-NG for different number of
micro blocks. Bitcoin-NG performs well (i.e., low latency) when
the network size is small (100 nodes, 4 blocks, 50 seconds). Its
performance worsens when the network size grows larger (1, 600
nodes, 4 blocks, 456 seconds). Since Bitcoin-NG is a variant of
Nakamoto consensus, nodes have to broadcast all blocks to the
whole network since they are needed for the consensus protocol
of the next epoch. Thus, increasing the block throughput entails a
longer overall latency, especially when the network grows. ELAS-
TICO, on the other hand, scales up the throughput as more nodes
join the network without expanding the system latency, as we estab-
lish in Section 5.2. ELASTICO does that by decoupling messages
needed for consensus from the ﬁnal data broadcast.

In terms of bandwidth consumption, the bandwidth used at each
node in Bitcoin-NG increases linearly with the throughput, e.g.,
2.04 and 4.05 MB per node when there are 1 and 2 data blocks
respectively. This is because Bitcoin-NG needs to broadcast all
blocks to the network for the next consensus step, thus more blocks,
more bandwidth used. On the other hand, in ELASTICO, only the
block headers are broadcast to the whole network since ELAS-
TICO decouples data broadcast from consensus metadata. There-
fore the bandwidth used at each node for the consensus step re-
mains roughly the same, regardless of the throughput, e.g., 4.93
and 5.01 MB when there are 4 and 8 blocks per epoch respectively.
Network constraint in PBFT. We next measure the scalability
of traditional byzantine consensus protocols. The selected candi-
date is PBFT [13] with our own implementation. Similar to the
experiments with Bitcoin-NG, we report the number of messages
exchanged per node, the bandwidth consumed at each node and the
latency to reach consensus for various network sizes. The results
of our experiments are in Figure 5 and Figure 6. Note that all com-
munications are done via a peer-to-peer network, with each node
connects to 4 other peers randomly.

We observe that the bandwidth cost and the total number of mes-
sages exchanged at each node increase linearly with the size of the
network. For example, a node sends/ receives 970 and 1, 930 mes-
sages when the network is of sizes 80 and 160 respectively. Simi-
larly, the latency grows quadratically as we introduce more nodes.
For instance, when network size increases from 40 to 80 nodes (2
times), the latency is 6 times longer (e.g., from 3 seconds to 18
seconds). This linear increase in cost and quadratic increase in la-
tency render PBFT inefﬁcient even if the network has only a few
hundreds nodes. In fact, our experiment when the network has 320
nodes did not terminate after running for 1 hour. We remark that
in our experiments, there is no faulty (malicious) nodes. Thus, the
cost will increase if we introduce faulty nodes in our experiments.

Figure 5: Cost per node in PBFT increases linearly with the size of
the network.

)
s
(

y
c
n
e
t
a
L

150

100

50

0

56

18

1 2

3

10 20

40
Network size (number of nodes)

100

80

165

160

Figure 6: Latency to reach consensus in PBFT with different network sizes

6. RELATED WORK

We compare our solution to existing solutions for blockchain

scalability in Table 2. The detailed discussions are below.
6.1 Centralized Sharding Protocols

ELASTICO is related to other sharding protocols in distributed
databases, e.g., Google’s Spanner [15], Scatter [16], RSCoin [39].
However, these sharding protocols consider a different model which
does not handle byzantine failures, make assumptions of PKI sys-
tems and a trusted infrastructure and access to external random
seed. For example, RSCoin only works for centralized cryptocur-
rencies where there is a central point of trust (central bank). Such
protocols are inapplicable to deploy in a byzantine environment like
blockchains. In fact, sharding is a well-recognized open problem
in byzantine environment [10].
In this work, we explain all the
challenges and propose the ﬁrst such sharding solution in the par-
tially synchronous setting. We have established that ELASTICO is
secure and cost-efﬁcient even with byzantine adversaries, allowing
the transaction throughput to scale up almost linearly with the net-
work computation capacity.
6.2 Blockchain Scalability Solutions

Building a scalable blockchain is an active problem in the Bit-
coin and cryptocurrency community. There have been several pro-
posals from both academia and industry.

The ﬁrst approach is to push more blocks to the blockchain, e.g.,
GHOST [40], Bitcoin-NG [9]. GHOST modiﬁes the rule to accept
the main valid blockchain to accept not only the earliest block at
each epoch, but also other blocks which are found later, e.g., “or-

0 500 1000 1500 2000 2500 4.50 4.60 4.70 4.80 4.90 5.00 5.10 5.20 5.30 10 20 40 80 100 160 No. of messages Bandwidth (MB) Network size (number of nodes) Data bandwidth Main messages's bandwidth No. of messages 26BFT
Tendermint [35]
IBM Blockchain [36]
Chain OS [19]
DigitalAsset [37]

Candidate

Decentralized Yes (cid:88)
Identity-less
Bandwidth
(per node)
Scalability

O(n2)

No

No

Nakamoto
Bitcoin [1]
Ethereum [38]
BitcoinNG [9]
IntelLedger [18]
Yes (cid:88)
Yes (cid:88)
Constant (cid:88)
No

Quorum-BFT Two-phase commit

ELASTICO

Ripple [22]
Stellar [21]

No
No

O(n)

No

Spanner [15]
RSCoin [39]
Databases
No
No
Constant (cid:88)
Yes (cid:88)

This work

Yes (cid:88)
Yes (cid:88)
Constant (cid:88)
Yes (cid:88)

Table 2: Comparison between ELASTICO and existing blockchain protocols in academia and industry. ELASTICO is the ﬁrst solution which can scale up the
throughput when the network size increases in a byzantine and decentralized environment.

phaned blocks". On the other hand, Bitcoin-NG allows each leader
in an epoch to propose more blocks to the network. Both GHOST
and Bitcoin-NG succeed at allowing block parallelism in the net-
work, but they do not localize the veriﬁcation of transactions as
in ELASTICO. Thus, more transactions in the network, more lo-
cal computation is required at each node and delay the consensus
process as pointed out in previous work [41].

A different approach for increasing Bitcoin’s transaction through-
put is to allow transactions to take place outside of the main Bitcoin
blockchain. Two prominent examples are lightning-network [42]
and Sidechains [43]. Lightning-network creates offchain micro-
payment channels between users. Thus users can send multiple and
instant transactions more efﬁciently, with only a few transactions
included in the blockchain. Sidechains takes a different approach
to allow users to move coins to different blockchain, thus allowing
Bitcoin transactions to happen elsewhere. It is widely understood
that Sidechains do not solve the scalability problem in Bitcoin [44].
Both the techniques, although improve the transaction throughput
signiﬁcantly, are applications running on top of Bitcoin thus still
rely on the scalability of the underlying protocol. It is worth not-
ing that applications enabled by Sidechains do not enjoy the same
security guarantee provided by Bitcoin, and micro-payment chan-
nels only work for a few applications. ELASTICO, however, allows
scaling up the underlying blockchain protocol without degrading
any security property.

Buterin et al. also address the scalability problem in blockchain
with sampling and challenging techniques [45]. Similar to ELAS-
TICO, the paper’s approach is to use sharding. However, the pro-
tocol “randomly" samples veriﬁers to verify others’ updates, and
allows users to challenge others’ veriﬁcation results if they ever
detect an invalid update. The solution relies on a random seed, for
which the paper does not provide any security analysis. Further, the
paper does not consider byzantine adversaries but rational ones in a
“cryptoeconomic" threat model, which is different from the threat
model that we consider in this paper.

Recent non-peer-reviewed proposals including Stellar [21], Rip-
ple [22], and Tendermint [35] claim to support high transaction
rate, but either have weaker threat models or are not as scalable
as ELASTICO. Speciﬁcally, Tendermint assumes all identities are
known before the protocol starts, thus is not applicable in decen-
tralized environments like cryptocurrencies. Besides, Tendermint
is essentially a variant of PBFT 4, which has its own scalability lim-
itation if the network size grows as we discussed in Section 2. Plus,
the network nodes in Ripple and Stellar are permissioned, hence it
faces no challenges of establishing identities. For instance, identi-
ties in Stellar need ﬁnancial agreements or reputations of others to
form their “slices" (or committees). In Elastico, these have to be
chosen randomly based on computational assumptions.

4http://tendermint.com/posts/tendermint-vs-pbft/

6.3 Prior Byzantine Consensus Protocols

There have been signiﬁcant efforts devoted to developing scal-
able communication-efﬁcient consensus protocols. The idea of di-
viding the users into committees (as we do in this paper) is preva-
lent in the existing literature; ﬁrst introduced by Bracha [46].

If the users are honest, but crash prone, there exists an optimal
algorithm with Θ(n) communication complexity based on the idea
of universe reduction, i.e., choosing a small committee to manage
the process [47].

If the users are malicious, it is much more difﬁcult to achieve
good communication complexity. For many years, the best known
protocols had exponential communication complexity [11, 12]. A
key improvement was made by Srikanth et al. [14], who developed
an efﬁcient algorithm with polynomial communication complexity.
While the preceding algorithms generally assumed a synchronous
network, there was also signiﬁcant work on consensus in asyn-
chronous and partially synchronous networks. In a seminal paper,
Castro et al. [13] implemented a replicated state machine based on
Byzantine agreement, often described as the ﬁrst practical BFT sys-
tem. It led to a ﬂoor of work on Byzantine agreement, with many
attempts to improve the efﬁciency and trade-off different aspects of
the performance (e.g., [48–51]).

Despite these signiﬁcant efforts, these protocols remained band-
width limited, typically requiring Θ(n2) messages (or more). Over
the last several years, there has been an exciting breakthrough [52–
55], reducing the communication complexity of agreement to O(n·
polylog(n)) for a system with n players. The basic idea is to ﬁrst
solve almost everywhere agreement, convincing most of the users
to agree on a single value. Then, a secondary almost-everywhere-
to-everywhere protocol is used to spread the information to the re-
maining laggards. To achieve almost everywhere agreement, they
assign the users to committees, and organize the committees into a
tree with a guarantee that almost all the committees have a majority
of honest users (using an adapted version of Feige’s leader election
algorithm [56]). A leader is elected at the root of the tree, and then
information is propagated down the tree to everyone. Later, to cope
with an adaptive adversary, secret sharing and additional informa-
tion hiding techniques are needed [57]. However, the protocols are
complex and practical implementations are yet to be demonstrated.
ELASTICO is not directly comparable to these newer communi-
cation efﬁcient protocols: ELASTICO is simpler and works in open
networks like cryptocurrencies where identities are unknown. Its
key advantage is in using computational power to tune the paral-
lelization of network, yet detaining security on bounded computa-
tional assumption.

ELASTICO is related to other protocols which use proof of work
for processors to establish their identities [58–61]. The main differ-
ence here is that ELASTICO is a sharding protocol, and establishing
identities is just the ﬁrst step of the 5 major steps in the protocol.

277. CONCLUSION

We present ELASTICO, the ﬁrst candidate for a secure shard-
ing protocol for permissionless blockchains. At its core, ELAS-
TICO scales up the agreement throughput near linearly with the
computational power of the network and tolerates byzantine adver-
saries which controls up to one-forth computation capacity, in the
partially synchronous network.
It offers promising scalability in
experiments and suggest strong usability in next-generation cryp-
tocurrencies.
8. ACKNOWLEDGMENT

We thank Dawn Song, Elaine Shi, Christian Cachin, Andrew
Miller, Jason Teutsch, Shweta Shinde, Shruti Tople, Alex Zikai
Wen, Hung Dang, Xiao Liu and Vincent Gramoli for useful dis-
cussions and feedback on the early version of the paper.

This research is supported by the National Research Foundation,
Prime Minister’s Ofﬁce, Singapore under its National Cybersecu-
rity R&D Program (Award No. NRF2014NCR-NCR001-21) and
administered by the National Cybersecurity R&D Directorate. All
opinions expressed in this work are solely those of the authors.
9. REFERENCES
[1] Satoshi Nakamoto. Bitcoin: A peer-to-peer electronic cash

system. bitcoin.org, 2009.

[2] Blockchain stats. Bitcoin statistics.
https://blockchain.info/stats, 2012.

[3] Bitcoin wiki. Scalability.

https://en.bitcoin.it/wiki/Scalability, 2015.

[4] Mastercard. Mastercard’s transaction per second.

http://newsroom.mastercard.com/2012/11/26/mastercard-
sees-black-friday-performance-up-26-percent/, 2016.

[5] Visa. Visa’s transactions per second. https://usa.visa.com/
content_library/modal/beneﬁts-accepting-visa.html, 2016.
[6] Jeff Garzik. Making decentralized economic policy. http://

gtf.org/garzik/bitcoin/BIP100-blocksizechangeproposal.pdf,
2015.

[7] Gavin Andresen. Bitcoin improvement proposal 101. https:
//github.com/bitcoin/bips/blob/master/bip-0101.mediawiki,
2015.

[8] Jeff Garzik. Bitcoin improvement proposal 102. https:

//github.com/bitcoin/bips/blob/master/bip-0102.mediawiki,
2015.

[9] Ittay Eyal, Adem Efe Gencer, Emin Gun Sirer, and Robbert

van Renesse. Bitcoin-ng: A scalable blockchain protocol.
http://arxiv.org/abs/1510.02037, 2015.

[10] Kyle Croman, Christian Decker, Ittay Eyal, Adem Efe

Gencer, Ari Juels, Ahmed Kosba, Andrew Miller, Prateek
Saxena, Elaine Shi, Emin Gun Sirer, Dawn Song, and Roger
Wattenhofer. On scaling decentralized blockchains (a
position paper). Workshop on Bitcoin and Blockchain
Research, 2016.

[11] M. Pease, R. Shostak, and L. Lamport. Reaching agreement
in the presence of faults. J. ACM, 27(2):228–234, April 1980.

[12] Leslie Lamport, Robert Shostak, and Marshall Pease. The
byzantine generals problem. ACM Trans. Program. Lang.
Syst., 4(3):382–401, July 1982.

[13] Miguel Castro and Barbara Liskov. Practical byzantine fault

tolerance. In Proceedings of the Third Symposium on
Operating Systems Design and Implementation, pages
173–186. USENIX Association, 1999.

[14] Sam Toueg, Kenneth J. Perry, and T. K. Srikanth. Fast

distributed agreement (preliminary version). In Proceedings

of the Fourth Annual ACM Symposium on Principles of
Distributed Computing, pages 87–101. ACM, 1985.

[15] James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew
Fikes, Christopher Frost, J. J. Furman, Sanjay Ghemawat,
Andrey Gubarev, Christopher Heiser, Peter Hochschild,
Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi
Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David
Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi
Saito, Michal Szymaniak, Christopher Taylor, Ruth Wang,
and Dale Woodford. Spanner: Google’s globally distributed
database. ACM Trans. Comput. Syst., aug 2013.

[16] Lisa Glendenning, Ivan Beschastnikh, Arvind

Krishnamurthy, and Thomas Anderson. Scalable consistency
in scatter. In Proceedings of the Twenty-Third ACM
Symposium on Operating Systems Principles, SOSP ’11,
pages 15–28, New York, NY, USA, 2011. ACM.

[17] Jason Baker, Chris Bond, James C. Corbett, JJ Furman,

Andrey Khorlin, James Larson, Jean-Michel Leon, Yawei Li,
Alexander Lloyd, and Vadim Yushprakh. Megastore:
Providing scalable, highly available storage for interactive
services. In Proceedings of the Conference on Innovative
Data system Research (CIDR), pages 223–234, 2011.

[18] Intel. Intel distributed ledger. http://intelledger.github.io/,

2016.

[19] Chain Inc. Chain open standard: A secure blockchain

protocol for high-scale ﬁnancial networks.
http://chain.com/os/, 2016.

[20] Rhett Creighton. Domus tower blockchain.

http://domustower.com/domus-tower-blockchain-latest.pdf,
2016.

[21] David Mazieres. The stellar consensus protocol: A federated

model for internet-level consensus. April 2015.

[22] Arthur Britto David Schwartz, Noah Youngs. The ripple

protocol consensus algorithm. Ripple Labs Inc., 2014.

[23] Bitcoin client. https://github.com/bitcoin/bitcoin.
[24] Rachid Guerraoui, Florian Huc, and Anne-Marie Kermarrec.

Highly dynamic distributed computing with byzantine
failures. In Proceedings of the 2013 ACM Symposium on
Principles of Distributed Computing, PODC ’13, 2013.

[25] Gopal Pandurangan, Peter Robinson, and Amitabh Trehan.

Self-healing deterministic expanders. CoRR, abs/1206.1522,
2012.

[26] John R. Douceur. The sybil attack. In Proceedings of 1st

International Workshop on Peer-to-Peer Systems (IPTPS),
2002.

[27] James Newsome, Elaine Shi, Dawn Song, and Adrian Perrig.
The sybil attack in sensor networks: Analysis & defenses. In
Proceedings of the 3rd International Symposium on
Information Processing in Sensor Networks, IPSN ’04, pages
259–268, New York, NY, USA, 2004. ACM.

[28] Baruch Awerbuch and Christian Scheideler. Robust random
number generation for peer-to-peer systems. Theor. Comput.
Sci., 410(6-7):453–466, feb 2009.

[29] Bitcoin wiki. Proof of stake.

https://en.bitcoin.it/wiki/Proof_of_Stake, 2015.
[30] Stefan Dziembowski, Sebastian Faust, Vladimir

Kolmogorov, and Krzysztof Pietrzak. Proofs of space.
Cryptology ePrint Archive, Report 2013/796, 2013.
http://eprint.iacr.org/.

[31] Giuseppe Ateniese, Ilario Bonacina, Antonio Faonio, and

Nicola Galesi. Proofs of space: When space is of the

28essence. Cryptology ePrint Archive, Report 2013/805, 2013.
http://eprint.iacr.org/.

[32] Nancy Lynch. Distributed algorithms. Morgan Kaufmann,

1996.

[33] Seth Gilbert, Calvin Newport, and Chaodong Zheng. Who

are you? secure identities in ad hoc networks. In Distributed
Computing, pages 227–242. Springer, 2014.

[34] Juan Garay, Aggelos Kiayias, and Nikos Leonardos. The

bitcoin backbone protocol: Analysis and applications.
Cryptology ePrint Archive, Report 2014/765, 2014.
http://eprint.iacr.org/.

[35] Jae Kwon. Tendermint: Consensus without mining.
[36] IBM. Ibm blockchain. http://www.ibm.com/blockchain/,

2016.

[37] Digital Asset Holdings. Digital asset.

https://digitalasset.com/, 2016.

[38] Ethereum Foundation. Ethereum’s white paper.

https://github.com/ethereum/wiki/wiki/White-Paper, 2014.

[39] George Danezis and Sarah Meiklejohn. Centrally banked

cryptocurrencies. Cryptology ePrint Archive, Report
2015/502, 2015. http://eprint.iacr.org/.

[40] Yonatan Sompolinsky and Aviv Zohar. Accelerating bitcoin’s

transaction processing. fast money grows on trees, not
chains. Cryptology ePrint Archive, Report 2013/881, 2013.
http://eprint.iacr.org/.

[41] Loi Luu, Jason Teutsch, Raghav Kulkarni, and Prateek

Saxena. Demystifying incentives in the consensus computer.
Cryptology ePrint Archive, Report 2015/702, 2015.
http://eprint.iacr.org/.

[42] Thaddeus Dryja Joseph Poon. The bitcoin lightning network:

Scalable off-chain instant payments.
http://lightning.network/lightning-network-paper.pdf, 2015.
[43] Adam Back, Matt Corallo, Luke Dashjr, Mark Friedenbach,
Gregory Maxwell, Andrew Miller, Andrew Poelstra, Jorge
Timon, , and Pieter Wuille. Enabling blockchain innovations
with pegged sidechains.
https://blockstream.com/sidechains.pdf, 2014.

[44] Pieter Wuille. Would sidechains help bitcoin scale? 2015.
[45] Buteri Vitalik, Wood Gavin, Zamﬁr Vlad, Coleman Jeff,

Wampler-Doty Matthew, and Cohn John. Notes on scalable
blockchain protocols (version 0.3.2). https://github.com/
vbuterin/scalability_paper/raw/master/scalability.pdf, 2015.
[46] Gabriel Bracha. An O(log n) expected rounds randomized
byzantine generals protocol. J. ACM, 34:910–920, October
1987.

[47] Seth Gilbert and Dariusz R. Kowalski. Distributed agreement

with optimal communication complexity. In Proceedings of
the Twenty-ﬁrst Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 965–977. Society for Industrial and
Applied Mathematics, 2010.

[48] Leslie Lamport. Fast paxos. Distributed Computing,

19(2):79–103, October 2006.

[49] Allen Clement, Edmund Wong, Lorenzo Alvisi, Mike
Dahlin, and Mirco Marchetti. Making byzantine fault
tolerant systems tolerate byzantine faults. In Proceedings of
the 6th USENIX Symposium on Networked Systems Design
and Implementation, pages 153–168. USENIX Association,
2009.

[50] Ramakrishna Kotla, Lorenzo Alvisi, Mike Dahlin, Allen

Clement, and Edmund Wong. Zyzzyva: Speculative
byzantine fault tolerance. ACM Trans. Comput. Syst.,

27(4):7:1–7:39, January 2010.

[51] Pierre-Louis Aublin, Rachid Guerraoui, Nikola Kneževi´c,

Vivien Quéma, and Marko Vukoli´c. The next 700 bft
protocols. ACM Trans. Comput. Syst., 32(4):12:1–12:45,
January 2015.

[52] V. King, J. Saia, V. Sanwalani, and E. Vee. Towards secure

and scalable computation in peer-to-peer networks. In
Foundations of Computer Science, 2006. FOCS ’06. 47th
Annual IEEE Symposium on, pages 87–98, 2006.

[53] Valerie King and Jared Saia. From almost everywhere to
everywhere: Byzantine agreement with ˜O(n3/2) bits. In
Proceedings of the 23rd International Conference on
Distributed Computing, pages 464–478. Springer-Verlag,
2009.

[54] Valerie King, Steven Lonargan, Jared Saia, and Amitabh

Trehan. Load balanced scalable byzantine agreement through
quorum building, with full information. In Distributed
Computing and Networking, volume 6522 of Lecture Notes
in Computer Science, pages 203–214. Springer Berlin
Heidelberg, 2011.

[55] Nicolas Braud-Santoni, Rachid Guerraoui, and Florian Huc.
Fast byzantine agreement. In Proceedings of the 2013 ACM
Symposium on Principles of Distributed Computing, pages
57–64. ACM, 2013.

[56] U. Feige. Noncryptographic selection protocols. In

Foundations of Computer Science, 1999. 40th Annual
Symposium on, pages 142–152, 1999.

[57] Valerie King and Jared Saia. Breaking the O(n2) bit barrier:
Scalable byzantine agreement with an adaptive adversary. J.
ACM, 58:18:1–18:24, July 2011.

[58] Jonathan Katz, Andrew Miller, and Elaine Shi.

Pseudonymous broadcast and secure computation from
cryptographic puzzles. Cryptology ePrint Archive, Report
2014/857, 2014. http://eprint.iacr.org/2014/857.

[59] Christian Decker, Jochen Seidel, and Roger Wattenhofer.

Bitcoin meets strong consistency. In Proceedings of the 17th
International Conference on Distributed Computing and
Networking, ICDCN ’16, pages 13:1–13:10, New York, NY,
USA, 2016. ACM.

[60] James Aspnes, Collin Jackson, and Arvind Krishnamurthy.
Exposing computationally-challenged byzantine impostors.
Department of Computer Science, Yale University, New
Haven, CT, Tech. Rep, 2005.

[61] Marcin Andrychowicz and Stefan Dziembowski. Distributed
cryptography based on the proofs of work. Cryptology ePrint
Archive, Report 2014/796, 2014.
http://eprint.iacr.org/2014/796.

[62] Wikipedia. Coupon collector’s problem. https:

//en.wikipedia.org/wiki/Coupon_collector%27s_problem.

[63] Donald J. Newman. The double dixie cup problem. The

American Mathematical Monthly, 67(1):58–61, 1960.

[64] Christina Garman, Matthew Green, and Ian Miers.

Decentralized anonymous credentials. Cryptology ePrint
Archive, Report 2013/622, 2013. http://eprint.iacr.org/.

[65] Seth Gilbert and Nancy Lynch. Brewer’s conjecture and the

feasibility of consistent, available, partition-tolerant web
services. SIGACT News, 33(2):51–59, June 2002.

[66] Seth Gilbert and Nancy Lynch. Perspectives on the cap

theorem. Computer, 45(2):30–36, February 2012.

2910. APPENDIX
10.1 The Scalability of ELASTICO
We now explain why ELASTICO achieves the desired asymptotic
O(n/ log log n) scalability by calculating the expected number of
PoW solutions that n processors have to generate such that each of
2s committees has c members. Our problems is equivalent to the
extended coupon collector problem [62] where there are 2s types
of coupon in some urn and we want to calculate the number of
draws to collect c copies of each coupon. It is shown in [63] that
the expected number of PoW solutions E is

E = 2s log 2s + (c − 1)2s log log 2s + O(2s)

= s · n
c

+ n log s − n
c

log s + O(n/c)

If c < s, ELASTICO can achieve O(n/ log n) scalability. On the
other hand, if c > s, we have

s · n
c

< n,

or E = O(n log s) asymptotically given that s < c. Since n =
c · 2s, we have E = O(n log log n), hence the O(n/ log log n)
scalability. The second scenario is more likely to happen in practice
because c must be of a few hundred to provide a reasonable security
guarantee.
In addition, [63] also shows that if c is large enough, E is asymp-
totically O(c · 2s). That means, ELASTICO can achieve linear scal-
ability if the committee size is chosen correctly. By using Chernoff
bound, we prove in Lemma 8 that c > 24 · ln(2s) gives us such
scalability.
Lemma 8 (Balls to Bins). We throw balls uniformly at random into
a collection of X bins. When c > 24 · ln X, the expected number
of balls to ensure each bin to have at least c balls is O(cX)
Proof. Let us consider the scenario of throwing cX balls uniformly
at random into those X bins. The expected number of balls in
each bin is c. Let’s ﬁx a particular bin of interest and we consider
xi be a random variable that equals 1 if ball i lands in that bin,

and 0 otherwise. Let C = (cid:80) xi. Notice that the expected value

E(C) = c, and all xi are independent. Using Chernoff bound, we
have:

Pr [C < (1 − δ)E(C)] ≤ e

−E(C)δ2/2

Fix δ = 1/2, and we get:

Pr [C < c/2] ≤ e

−c/8

Given c > 24 · ln X, we get:

Pr [C < c/2] ≤ 1/X 3.

Recall we are looking at all one particular bin. Now we consider
all X bins. Take a union bound, and we get the probability that any
bin has less than c/2 balls is at most 1/X 2. This also means that
with probability 1/X 2, we throw cX balls more into X bin. Doing
the same calculation for the new cX balls, we need to throw cX
more balls with probability 1/X 2.

Thus the expected number of balls we have to throw to ensure

each bin to have at least c/2 balls is:

cX + cX/X 2 + cX/X 4 + cX/X 6 + ... ≤ 2cX.

Therefore, the number of balls to throw guarantee each bin to

have at least c balls is O(cX).

10.2 Veriﬁcation Checks in ELASTICO

Different blockchain applications may require different veriﬁ-
cation checks on the transactions (e.g., [64]). In the presence of
sharding, supporting arbitrary checks within local committees is
not possible, as is well-known in sharded databases [65, 66].
In
a cryptocurrency, the most important checks are ensuring that the
payer signs the coin transfer and that the transaction is not double-
spending. We next discuss how one can efﬁciently implement such
veriﬁcation check for transactions in a hypothetical cryptocurrency
built on top of ELASTICO.

In Bitcoin and popular cryptocurrencies, a regular transaction
sends some amount of coins from a sender to a recipient. Each
transaction has two parts, namely an Input and an Output. The
Input speciﬁes the source of the coin and a proof (by digital
signatures) which shows the sender is a valid coin owner. The
Output names the recipient; later on the recipient can use this
Output as the source of the coin in his/her new transaction). Check-
ing if a transaction is double-spending entails checking if it uses
some coin which has been spent in a previous transaction. A naive
implementation of such check would require one to scan through
the entire history of the blockchain. In Bitcoin, to avoid such costs,
each node maintains a local database containing all “unspent trans-
action outputs" (or UTXO for short) to check if an Output has
been spent or not. Bitcoin nodes frequently update the UTXO
database after each new block based on the set of transactions in-
cluded in the block. Thus, if a transaction is ﬁnalized in the Bitcoin
blockchain, it is guaranteed to be valid.

In ELASTICO, our transaction veriﬁcation is similar, i.e., the
node also maintain a local database of UTXOs and update such
database after each epoch when they download data blocks from
committees. To avoid the scenario that two committees process the
same transaction, each committee in ELASTICO processes a sepa-
rate list of transactions which have some speciﬁc range of Input.
For example, if there are 4 committees, they will handle transac-
tions having Inputs’ IDs (i.e., the hashes of the Inputs) with
different preﬁxes of 00, 01, 10, 11 respectively. As a result, within
an epoch, all shards will include disjoint transaction inputs (thus
disjoint transaction sets).

The veriﬁcation in ELASTICO is superior to other solutions like
GHOST [40], Bitcoin- NG [9] in many aspects. First, ELASTICO
nodes do not need to verify data blocks from other committees.
Instead, they only need to check if a block is committed in the
consensus block (from the ﬁnal committee) to decide if the block
is valid. It is because the committee corresponding to that shard
already veriﬁes its block, and the ﬁnal committee has veriﬁed if
a block is agreed by a committee before committing the block to
the ﬁnal (consensus) block. On the other hand, in other solutions,
nodes have to verify all transactions included in the blockchain,
thus higher throughput will require more local computation from
nodes. Further, increasing the amount of veriﬁcation check will
slow down the block broadcast, and further pose a security threat
in the consensus protocol as pointed out by previous work [41].

Thus, ELASTICO scales up the throughput as the network al-
lows, without requiring more local computation for veriﬁcation at
each node or affecting the consensus security. Note that ELAS-
TICO still permits bandwidth efﬁciency in its consensus step for a
cryptocurrency application, but has to broadcast all the data blocks
to all nodes (as done in all solutions today). However, in appli-
cations where the consistency checks can themselves be sharded
and checked locally, there would be no need to broadcast the data
blocks to the network.

30