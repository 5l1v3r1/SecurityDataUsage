A Bayesian Approach to Privacy Enforcement  

in Smartphones

Omer Tripp, IBM Research, USA; Julia Rubin, IBM Research, Israel

https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/tripp

This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXA Bayesian Approach to Privacy Enforcement in Smartphones

Omer Tripp

IBM Research, USA

Julia Rubin

IBM Research, Israel

Abstract

Mobile apps often require access to private data, such
as the device ID or location. At the same time, popular
platforms like Android and iOS have limited support for
user privacy. This frequently leads to unauthorized dis-
closure of private information by mobile apps, e.g. for
advertising and analytics purposes. This paper addresses
the problem of privacy enforcement in mobile systems,
which we formulate as a classiﬁcation problem: When
arriving at a privacy sink (e.g., database update or outgo-
ing web message), the runtime system must classify the
sink’s behavior as either legitimate or illegitimate. The
traditional approach of information-ﬂow (or taint) track-
ing applies “binary” classiﬁcation, whereby information
release is legitimate iff there is no data ﬂow from a pri-
vacy source to sink arguments. While this is a useful
heuristic, it also leads to false alarms.

We propose to address privacy enforcement as a learn-
ing problem, relaxing binary judgments into a quanti-
tative/probabilistic mode of reasoning. Speciﬁcally, we
propose a Bayesian notion of statistical classiﬁcation,
which conditions the judgment whether a release point is
legitimate on the evidence arising at that point. In our
concrete approach, implemented as the BAYESDROID
system that is soon to be featured in a commercial prod-
uct, the evidence refers to the similarity between the data
values about to be released and the private data stored on
the device. Compared to TaintDroid, a state-of-the-art
taint-based tool for privacy enforcement, BAYESDROID
is substantially more accurate. Applied to 54 top-popular
Google Play apps, BAYESDROID is able to detect 27 pri-
vacy violations with only 1 false alarm.

1

Introduction

Mobile apps frequently demand access to private infor-
mation. This includes unique device and user identiﬁers,
such as the phone number or IMEI number (identify-
ing the physical device); social and contacts data; the

1

user’s location; audio (microphone) and video (camera)
data; etc. While private information often serves the core
functionality of an app, it may also serve other purposes,
such as advertising, analytics or cross-application proﬁl-
ing [9]. From the outside, the user is typically unable
to distinguish legitimate usage of their private informa-
tion from illegitimate scenarios, such as sending of the
IMEI number to a remote advertising website to create a
persistent proﬁle of the user.

Existing platforms provide limited protection against
privacy threats. Both the Android and the iOS plat-
forms mediate access to private information via a per-
mission model. Each permission is mapped to a desig-
nated resource, and holds per all application behaviors
and resource accesses. In Android, permissions are given
or denied at installation time.
In iOS, permissions are
granted or revoked upon ﬁrst access to the respective re-
source. Hence, both platforms cannot disambiguate le-
gitimate from illegitimate usage of a resource once an
app is granted the corresponding permission [8].

Threat Model
In this paper, we address privacy threats
due to authentic (as opposed to malicious) mobile ap-
plications [4, 18]. Contrary to malware, such applica-
tions execute their declared functionality, though they
may still expose the user to unnecessary threats by in-
corporating extraneous behaviors — neither required by
their core business logic nor approved by the user [11]
— such as analytics, advertising, cross-application pro-
ﬁling, social computing, etc. We consider unauthorized
release of private information that (almost) unambigu-
ously identiﬁes the user as a privacy threat. Henceforth,
we dub such threats illegitimate.

While in general there is no bullet-proof solution
for privacy enforcement that can deal with any type of
covert channel, implicit ﬂow or application-speciﬁc data
transformation, and even conservative enforcement ap-
proaches can easily be bypassed [19], there is strong evi-
dence that authentic apps rarely exhibit these challenges.

USENIX Association  

23rd USENIX Security Symposium  175

According to a recent study [9], and also our empiri-
cal data (presented in Section 5), private information is
normally sent to independent third-party servers. Conse-
quently, data items are released in clear form, or at most
following well-known encoding/encryption transforma-
tions (like Base64 or MD5), to meet the requirement of a
standard and general client/server interface.

The challenge, in this setting, is to determine whether
the app has taken sufﬁcient means to protect user pri-
vacy. Release of private information, even without user
authorization, is still legitimate if only a small amount of
information has been released. As an example, if an ap-
plication obtains the full location of the user, but then re-
leases to an analytics server only coarse information like
the country or continent, then in most cases this would
be perceived as legitimate.

Privacy Enforcement via Taint Analysis The short-
comings of mobile platforms in ensuring user privacy
have led to a surge of research on realtime privacy mon-
itoring. The foundational technique grounding this re-
search is information-ﬂow tracking, often in the form of
taint analysis [23, 15]: Private data, obtained via privacy
sources (e.g.
TelephonyManager.getSubscriberId(),
which reads the device’s IMSI), is labeled with a taint
tag denoting its source. The tag is then propagated
along data-ﬂow paths within the code. Any such path
that ends up in a release point, or privacy sink (e.g.
WebView.loadUrl(...), which sends out an HTTP re-
quest), triggers a leakage alarm.

The tainting approach effectively reduces leakage
judgments to boolean reachability queries. This can po-
tentially lead to false reports, as the real-world example
shown in Figure 1 illustrates. This code fragment, ex-
tracted from a core library in the Android platform, reads
the device’s IMSI number, and then either (ii) persists
the full number to an error log if the number is invalid
(the loge(...) call), or (ii) writes a preﬁx of the IMSI
(of length 6) to the standard log while carefully masking
away the sufﬁx (of length 9) as ’x’ characters. Impor-
tantly, data ﬂow into the log(...) sink is not a privacy
problem, because the ﬁrst 6 digits merely carry model
and origin information. Distinctions of this sort are be-
yond the discriminative power of taint analysis [26].

Quantitative extensions of the core tainting approach
have been proposed to address this limitation. A notable
example is McCamant and Ernst’s [13] information-ﬂow
tracking system, which quantities ﬂow of secret informa-
tion by dynamically tracking taint labels at the bit level.
Other approaches — based e.g. on distinguishability be-
tween secrets [1], the rate of data transmission [12] or the
inﬂuence inputs have on output values [14] — have also
been proposed. While these systems are useful as ofﬂine
analyses, it is highly unlikely that any of them can be en-

1 String mImsi = ...; // source
2 // 6 digits <= IMSI (MCC+MNC+MSIN) <= 15 (usually 15)
3 if (mImsi != null &&
4

(mImsi.length() < 6 || mImsi.length() > 15)) {

5

loge(” invalid IMSI ” + mImsi); // sink

6 mImsi = null; }
7 log(”IMSI: ” + mImsi.substring (0, 6) + ”xxxxxxxxx”); // sink

Figure 1: Fragment from an internal Android library,
com.android.internal.telephony.cdma.RuimRecords,
where a preﬁx of the mobile device’s IMSI number
ﬂows into the standard log ﬁle

gineered to meet the performance requirements of a re-
altime monitoring solution due to the high complexity of
their underlying algorithms. As an example, McCamant
and Ernst report on a workload on which their analysis
spent over an hour.

Our Approach We formulate data leakage as a clas-
siﬁcation problem, which generalizes the source/sink
reachability judgment enforced by standard information-
ﬂow analysis, permitting richer and more relaxed judg-
ments in the form of statistical classiﬁcation. The mo-
tivating observation is that reasoning about information
release is fuzzy in nature. While there are clear exam-
ples of legitimate versus illegitimate information release,
there are also less obvious cases (e.g., a variant of the
example in Figure 1 with a 10- rather than 6-character
preﬁx). A statistical approach, accounting for multiple
factors and based on rich data sets, is better able to ad-
dress these subtleties.

Concretely, we propose Bayesian classiﬁcation. To la-
bel a release point as either legitimate or illegitimate, the
Bayesian classiﬁer refers to the “evidence” at that point,
and computes the likelihood of each label given the ev-
idence. The evidence consists of feature/value pairs.
There are many ways of deﬁning the evidence. In this
study, we concentrate on the data arguments ﬂowing into
release operations, though we intend to consider other
classes of features in the future. (See Section 7.)

Speciﬁcally, we induce features over the private values
stored on the device, and evaluate these features accord-
ing to the level of similarity between the private values
and those arising at release points. This distinguishes in-
stances where data that is dependent on private values
ﬂows into a release point, but its structural and/or quan-
titative characteristics make it eligible for release, from
illegitimate behaviors. Failure to make such distinctions
is a common source of false alarms suffered by the taint-
ing approach [4].

To illustrate this notion of features, we return to the ex-
ample in Figure 1. Because the IMSI number is consid-

176  23rd USENIX Security Symposium 

USENIX Association

2

mImsi = ...;

"4(cid:22)46855(cid:22)56(cid:22)1234"

similarity: 0.4=6/15

loge(...);

similarity: 1.0=15/15
"invalid IMSI 4(cid:22)46855(cid:22)56(cid:22)1234"

log(...);

"IMSI: 4(cid:22)4685xxxxxxxxx"

Figure 2: Similarity analysis applied to the code in Fig-
ure 1

ered private, we deﬁne a respective feature IMSI. Assume
that the concrete IMSI value is “404685505601234”.
Then the value arising at the log(...) release point is
“IMSI: 404685xxxxxxxxx”. The quantitative similarity
between these two values serves as evidence for the de-
cision whether or not log(...) is behaving legitimately.
This style of reasoning is depicted in Figure 2.

Evaluation To evaluate our approach, we have imple-
mented the BAYESDROID system for privacy enforce-
ment.
We report on two sets of experiments over
BAYESDROID.

First, to measure the accuracy gain thanks to Bayesian
analysis, we compared BAYESDROID with the Taint-
Droid system [4], a highly popular and mature imple-
mentation of the tainting approach that is considered both
efﬁcient (with average overhead of approximately 10%)
and accurate. We applied both BAYESDROID and Taint-
Droid to the DroidBench suite,1 which comprises the
most mature and comprehensive set of privacy bench-
marks currently available. The results suggest dramatic
improvement in accuracy thanks to Bayesian elimina-
tion of false reports, yielding accuracy scores of 0.96 for
BAYESDROID versus 0.66 for TaintDroid.

The second experiment examines the practical value
of BAYESDROID by applying it to 54 top-popular mo-
bile apps from Google Play. We evaluate two variants of
BAYESDROID, one of which is able to detect a total of
27 distinct instances of illegitimate information release
across 15 of the applications with only 1 false alarm.

Contributions This paper makes the following princi-
pal contributions:

1. Novel approach to leakage detection (Section 2):
We present a Bayesian classiﬁcation alternative to
the classic tainting approach. Our approach is more
ﬂexible than taint tracking by permitting statistical
weighting of different features as the basis for pri-
vacy judgments.

2. Similarity-based reasoning (Section 3): We instanti-
ate the Bayesian approach by applying quantitative

similarity judgments over private values and values
about to be released. This enables consideration of
actual data, rather than only data ﬂow, as evidence
for privacy judgments.

3. Implementation and evaluation (Sections 4–5): We
have instantiated our approach as the BAYESDROID
system, which is about to be featured in an IBM
cloud-based security service. We report on two sets
of experiments, whose results (i) demonstrate sub-
stantial accuracy gain thanks to Bayesian reason-
ing, and (ii) substantiate the overall effectiveness of
BAYESDROID when applied to real-world apps. All
the leakage reports by BAYESDROID are publicly
available for scrutiny.2

2 The Bayesian Setting

Our starting point is to treat privacy enforcement as a
classiﬁcation problem, being the decision whether or not
a given release point is legitimate. The events, or in-
stances, to be classiﬁed are (runtime) release points. The
labels are legitimate and illegitimate. Misclassiﬁcation
either yields a false alarm (mistaking benign information
release as a privacy threat) or a missed data leak (failing
to intercept illegitimate information release).

2.1 Bayes and Naive Bayes
Our general approach is to base the classiﬁcation on the
evidence arising at the release point. Items of evidence
may refer to qualitative facts, such as source/sink data-
ﬂow reachability, as well as quantitative measures, such
as the degree of similarity between private values and
values about to be released. These latter criteria are es-
sential in going beyond the question of whether private
information is released to also reason about the amount
and form of private information about to be released.

A popular classiﬁcation method, representing this
mode of reasoning, is based on Bayes’ theorem (or rule).
Given events X and Y , Bayes’ theorem states the follow-
ing equality:

Pr(Y|X) =

Pr(X|Y )· Pr(Y )

Pr(X)

(1)

where Pr(Y|X) is the conditional probability of Y given
X (i.e., the probability for Y to occur given that X has
occurred). X is referred to as the evidence. Given ev-
idence X, Bayesian classiﬁers compute the conditional
likelihood of each label (in our case, legitimate and ille-
gitimate).

We begin with the formal background by stating Equa-
tion 1 more rigorously. Assume that Y is a discrete-
valued random variable, and let X = [X1, . . . ,Xn] be a

1http://sseblog.ec-spride.de/tools/droidbench/

2 researcher.ibm.com/researcher/ﬁles/us-otripp/Artifacts.zip

USENIX Association  

23rd USENIX Security Symposium  177

3

%
%


vector of n discrete or real-valued attributes Xi. Then

Pr(Y = yk|X1 . . .Xn) =

Pr(Y = yk)· Pr(X1 . . .Xn|Y = yk)
Σ j Pr(Y = y j)· Pr(X1 . . .Xn|Y = y j)
(2)
As Equation 2 hints, training a Bayesian classiﬁer is, in
general, impractical. Even in the simple case where the
evidence X is a vector of n boolean attributes and Y is
boolean, we are still required to estimate a set

θi j = Pr(X = xi|Y = y j)

of parameters, where i assumes 2n values and j assumes
2 values for a total of 2·(2n−1) independent parameters.
Naive Bayes deals with the intractable sample com-
plexity by introducing the assumption of conditional in-
dependence, as stated in Deﬁnition 2.1 below, which re-
duces the number of independent parameters sharply to
2n. Intuitively, conditional independence prescribes that
events X and Y are independent given knowledge that
event Z has occurred.

Deﬁnition 2.1 (Conditional Independence). Given ran-
dom variables X, Y and Z, we say that X is conditionally
independent of Y given Z iff the probability distribution
governing X is independent of the value of Y given Z.
That is,
∀i, j,k. Pr(X = xi|Y = y j,Z = zk) =Pr(X = xi|Z = zk)
Under the assumption of conditional independence,

we obtain the following equality:
Pr(X1 . . .Xn|Y ) =Π n

i=1 Pr(Xi|Y )

(3)

Therefore,

Pr(Y = yk|X1 . . .Xn) =

Pr(Y = yk)· Πi Pr(Xi|Y = yk)
Σ j Pr(Y = y j)· Πi Pr(Xi|Y = y j)
(4)

2.2 Bayesian Reasoning about Leakage
For leakage detection, conditional independence trans-
lates into the requirement that at a release point st, the
“weight” of evidence e1 is not affected by the “weight”
of evidence e2 knowing that st is legitimate/illegitimate.
As an example, assuming the evidence is computed as
the similarity between private and released values, if st
is known to be a statement sending private data to the
network, then the similarity between the IMSI number
and respective values about to be released is assumed to
be independent of the similarity between location coor-
dinates and respective values about to be released.

The assumption of conditional independence induces
a “modular” mode of reasoning, whereby the privacy

features comprising the evidence are evaluated indepen-
dently. This simpliﬁes the problem of classifying a re-
lease point according to the Bayesian method into two
quantities that we need to clarify and estimate: (i) the
likelihood of legitimate/illegitimate release (Pr(Y = yk))
and (ii) the conditional probabilities Pr(Xi|Y = yk).

3 Privacy Features

In this section we develop, based on the mathematical
background in Section 2, an algorithm to compute the
conditional likelihood of legitimate versus illegitimate
data release given privacy features Fi. With such an al-
gorithm in place, given values vi for the features Fi, we
obtain

vleg = Pr(legitimate| [F1 = v1, . . . ,Fn = vn])
villeg = Pr(illegitimate| [F1 = v1, . . . ,Fn = vn])

Bayesian classiﬁcation then reduces to comparing be-
tween vleg and villeg, where the label corresponding to
the greater of these values is the classiﬁcation result.

3.1 Feature Extraction
The ﬁrst challenge that arises is how to deﬁne the fea-
tures (denoted with italicized font: F) corresponding to
the private values (denoted with regular font: F). This re-
quires simultaneous consideration of both the actual pri-
vate value and the “relevant” values arising at the sink
statement (or release point). We apply the following
computation:

1. Reference value: We refer to the actual private value
as the reference value, denoting the value of pri-
vate item F as [[F]]. For the example in Figures 1–
2, the reference value, [[IMSI]], of the IMSI fea-
ture would be the device’s IMSI number: [[IMSI]]
= “404685505601234”.

2. Relevant value: We refer to value v about to be re-
leased by the sink statement as relevant with respect
to feature F if there is data-ﬂow connectivity be-
tween a source statement reading the value [[F]] of
F and v. Relevant values can thus be computed via
information-ﬂow tracking by propagating a unique
tag (or label) per each private value, as tools like
TaintDroid already do. Note that for a given feature
F, multiple different relevant values may arise at a
given sink statement (if the private item F ﬂows into
more than one sink argument).

3. Feature value: Finally, given the reference value [[F]]
and a set {v1, . . . ,v k} of relevant values for feature
F, the value we assign to F (roughly) reﬂects the
highest degree of pairwise similarity (i.e., minimal

178  23rd USENIX Security Symposium 

USENIX Association

4

distance) between [[F]] and the values vi. Formally,
we assume a distance metric d. Given d, we deﬁne:

[[F]] ≡ min

1≤i≤k{d([[F]],vi)}

We leave the distance metric d(. . .) unspeciﬁed for
now, and return to its instantiation in Section 3.2.

According to our description above, feature values are
unbounded in principle, as they represent the distance be-
tween the reference value and any data-dependent sink
values. In practice, however, assuming (i) the distance
metric d(. . .) satisﬁes d(x,y) ≤ max{|x|,|y|}, (ii) ∃c ∈
N. |[[F]]| ≤c (as with the IMEI, IMSI, location, etc.), and
(iii) [[F]] is not compared with values larger than it, we
can bound [[F]] by c. In general, any feature can be made
ﬁnite, with (at most) n+1 possible values, by introducing
a privileged “≥ n” value, which denotes that the distance
between the reference and relevant values is at least n.

3.2 Measuring Distance between Values
To compute a quantitative measure of similarity between
data values, we exploit the fact that private data often
manifests as strings of ASCII characters [4, 9, 27]. These
include e.g. device identiﬁers (like the IMEI and IMSI
numbers), GPS coordinates, inter-application communi-
cation (IPC) parameters, etc. This lets us quantify dis-
tance between values in terms of string metrics.

Many string metrics have been proposed to date [17].
Two simple and popular metrics, which we have exper-
imented with and satisfy the requirement that d(x,y) ≤
max{|x|,|y|}, are the following:
Hamming Distance This metric assumes that
the
strings are of equal length. The Hamming distance be-
tween two strings is equal to the number of positions at
which the corresponding symbols are different (as indi-
cated by the indicator function δc1(cid:23)=c2(. . .)):
ham(a,b) =Σ 0≤i<|a|δc1(cid:23)=c2 (a(i),b(i))

In another view, Hamming distance measures the num-
ber of substitutions required to change one string into the
other.

Levenshtein Distance The Levenshtein string met-
ric computes the distance between strings a and b as
leva,b(|a|,|b|) (abbreviated as lev(|a|,|b|)), where

Informally,
lev(|a|,|b|) is the minimum number of
single-character edits — either insertion or deletion or

lev(i, j) =

max(i, j)

min

lev(i− 1, j) +1
lev(i, j− 1) +1
lev(i− 1, j− 1) +δ ai(cid:23)=b j

if min(i, j) = 0

 otherwise

Data: Strings u and v
Data: Distance metric d
begin

x ←− |u| < |v| ? u : v // min
y ←− |u| ≥ |v| ? u : v // max
r ←− y
for i = 0 to |y|−|x| do
y(cid:20) ←− y[i,i +|x|−1]
if d(x,y(cid:20)) < r then
r ←− d(x,y(cid:20))

end

end
return r

end
Algorithm 1: The BAYESDROID distance measure-
ment algorithm

substitution — needed to transform one string into the
other. An efﬁcient algorithm for computing the Leven-
shtein distance is bottom-up dynamic programming [24].
The asymptotic complexity is O(|a|·|b|).
Given string metric d(x,y) and pair (u,v) of reference
value u and relevant value v, BAYESDROID computes
their distance according to the following steps:

1. BAYESDROID ensures that both u and v are String
objects by either (i) invoking toString() on refer-
ence types or (ii) converting primitive types into
Strings (via String.valueOf(. . .)), if the argument
is not already of type String.

2. To meet the conservative requirement that |x| = |y|
(i.e., x and y are of equal length), BAYESDROID ap-
plies Algorithm 1. This algorithm induces a sliding
window over the longer of the two strings, whose
width is equal to the length of the shorter string. The
shorter string is then compared to contiguous seg-
ments of the longer string that have the same length.
The output is the minimum across all comparisons.
To ensure that comparisons are still meaningful un-
der length adjustment, we decompose private values into
indivisible information units. These are components of
the private value that cannot be broken further, and so
comparing them with a shorter value mandates that the
shorter value be padded. In our speciﬁcation, the phone,
IMEI and IMSI numbers consist of only one unit of
information. The Location object is an example of a
data structure that consists of several distinct informa-
tion units. These include the integral and fractional parts
of the longitude and latitude values, etc. BAYESDROID
handles objects that decompose into multiple informa-
tion units by treating each unit as a separate object and
applying the steps above to each unit in turn. The no-
tion of information units guards BAYESDROID against
ill-founded judgments, such as treating release of a sin-
gle IMEI digit as strong evidence for leakage.

USENIX Association  

23rd USENIX Security Symposium  179

5

3.3 Estimating Probabilities
The remaining challenge, having clariﬁed what the fea-
tures are and how their values are computed, is to esti-
mate the probabilities appearing in Equation 4:

(cid:127) We need to estimate the probability of the legiti-
mate event, Pr(legitimate), where illegitimate is the
complementary event and thus Pr(illegitimate) =
1− Pr(legitimate).
(cid:127) We need to estimate the conditional probabilities
Pr(F = u|legitimate) and Pr(F = u|illegitimate) for
all features F and respective values u.
Pr(legitimate) can be approximated straightforwardly
based on available statistics on the frequency of data
leaks in the wild. For the conditional probabilities, as-
suming feature Xi is discrete valued with j distinct val-
ues (per the discussion in Section 3.1 above), we would
naively compute the estimated conditional probability
θi jk according to the following equation:

#D{Xi=xi j∧Y =yk}

#D{Y =yk}

(5)

θi jk = (cid:31)Pr(Xi = xi j|Y = yk) =

The danger, however, is that this equation would produce
estimates of zero if the data happens not to contain any
training examples satisfying the condition in the numer-
ator. To ﬁx this, we modify Equation 5 as follows:

#D{Xi=xi j∧Y =yk}+l

#D{Y =yk}+l·J

θi jk = (cid:31)Pr(Xi = xi j|Y = yk) =

(6)
where l is a factor that “smoothens” the estimate by
adding in a number of “hallucinated” examples that are
assumed to be spread evenly across the J possible values
of Xi. In Section 5.1, we provide concrete detail on the
data sets and parameter values we used for our estimates.

4 The BAYESDROID Algorithm

In this section, we describe the complete BAYESDROID
algorithm. We then discuss enhancements of the core
algorithm.

4.1 Pseudocode Description
Algorithm 2 summarizes the main steps of BAYES-
DROID. For simplicity, the description in Algorithm 2
assumes that source statements serve private data as
their return value, though the BAYESDROID implemen-
tation also supports other sources (e.g. callbacks like
onLocationChanged(. . .), where the private Location ob-
ject is passed as a parameter). We also assume that each
source maps to a unique privacy feature. Hence, when
a source is invoked (i.e., the OnSourceStatement event
ﬁres), we obtain the unique tag corresponding to its re-
spective feature via the GetFeature(. . .) function. We

Input: S // privacy specification

begin

while true do

OnSourceStatement r := src p :

// map source to feature

f ←− GetFeature src
attach tag f to r

OnNormalStatement r := nrm p :

propagate feature tags according to data ﬂow

OnSinkStatement r := snk p :

// map feat.s to param.s with resp.

tag

{ f (cid:24)→ p f} ←− ExtractTags p
foreach f → p f ∈ { f → p f} do
u ←− ref f
δ ←− min{d(u, [[p]])}p∈p f
f ←− δ ≥ c f ? “ ≥c f ”: δ

end
if IsLeakageClassification { f} then
end

Alarm snk p

end

end
Algorithm 2: Outline of the core BAYESDROID algo-
rithm

then attach the tag to the return value r. Normal data
ﬂow obeys the standard rules of tag propagation, which
are provided e.g. by Enck et al. [4]. (See Table 1 there.)
When an OnSinkStatement event is triggered, the ar-
guments ﬂowing into the sink snk are searched for pri-
vacy tags, and a mapping from features f to parameters
p f carrying the respective tag is built. The value of f is
then computed as the minimal pairwise distance between
the parameters p ∈ p f and ref f . If this value is greater
than some constant c f deﬁned for f , then the privileged
value “≥ c f ” is assigned to f . (See Section 3.1.) Finally,
the judgment IsLeakageClassification is applied over
the features whose tags have reached the sink snk. This
judgment is executed according to Equation 4.

We illustrate the BAYESDROID algorithm with ref-
erence to Figure 3, which demonstrates a real leak-
age instance in com.g6677.android.princesshs, a pop-
ular gaming application.
In this example, two differ-
ent private items ﬂow into the sink statement: both the
IMEI, read via getDeviceId(), and the Android ID, read
via getString(...).

At sink statement URL.openConnection(...), the re-
spective tags IMEI and AndroidID are extracted. Values
are assigned to these features according to the description
in Section 3, where we utilize training data, as discussed
later in Section 5.1, for Equation 6:

Pr(IMEI ≥ 5|leg) = 0.071 Pr(AndID ≥ 5|leg) = 0.047
Pr(IMEI ≥ 5|ilg) = 0.809
Pr(AndID ≥ 5|ilg) = 0.833

6

180  23rd USENIX Security Symposium 

USENIX Association

1 source : private value

TelephonyManager.getDeviceId() : 000000000000000
Settings $Secure.getString (...)
: cdf15124ea4c7ad5

2

3

4

5 sink : arguments

getSystemService(TELEPHONY SERVICE);

1 TelephonyManager tm =
2
3 String imei = tm.getDeviceId(); // source
4 String encodedIMEI = Base64Encoder.encode(imei);
5 Log.i (encodedIMEI); // sink

6 URL.openConnection(...) : app id=2aec0559c930 ... &
7

android id =cdf15124ea4c7ad5 \& udid= ... &
serial id = ... & ... &
publisher user id =000000000000000

8

9

Figure 3: True leakage detected by BAYESDROID in
com.g6677.android.princesshs

We then compute Equation 4, where the denominator
is the same for both leg and illeg, and so it sufﬁces to
evaluate the nominator (denoted with ˜Pr(...) rather than
Pr(...)):

˜Pr(leg|IMEI ≥ 5,AndID ≥ 5) =
Pr(leg)× Pr(IMEI ≥ 5|leg)× Pr(AndID ≥ 5|leg) =
˜Pr(ilg|IMEI ≥ 5,AndID ≥ 5) =
Pr(ilg)× Pr(IMEI ≥ 5|ilg)× Pr(AndID ≥ 5|ilg) =

0.66× 0.071× 0.047 = 0.002

0.33× 0.809× 0.833 = 0.222
Our estimates of 0.66 for Pr(leg) and 0.33 for Pr(ilg) are
again based on training data as explained in Section 5.1.
The obtained conditional measure of 0.222 for ilg is (far)
greater than 0.002 for leg, and so BAYESDROID resolves
the release instance in Figure 3 as a privacy threat, which
is indeed the correct judgment.

4.2 Enhancements
We conclude our description of BAYESDROID by high-
lighting two extensions of the core algorithm.

Beyond Plain Text While many instances of illegiti-
mate information release involve plain text, and can be
handled by the machinery in Section 3.1, there are also
more challenging scenarios. Two notable challenges are
(i) data transformations, whereby data is released follow-
ing an encoding, encryption or hashing transformation;
and (ii) high-volume binary data, such as camera or mi-
crophone output. We have extended BAYESDROID to
address both of these cases.

We begin with data transformations. As noted ear-
lier, in Section 1, private information is sometimes re-
leased following standard hashing/encoding transforma-
tions, such as the Base64 scheme. This situation, il-
lustrated in Figure 4, can distort feature values, thereby

Figure 4: Adaptation of the DroidBench Loop1 bench-
mark, which releases the device ID following Base64 en-
coding

leading BAYESDROID to erroneous judgments. Fortu-
nately, the transformations that commonly manifest in
leakage scenarios are all standard, and there is a small
number of such transformations [9].

To account for these transformations, BAYESDROID
applies each of them to the value obtained at a source
statement, thereby exploding the private value into mul-
tiple representations. This is done lazily, once a sink is
reached, for performance. This enhancement is speciﬁed
in pseudocode form in Algorithm 3. The main change is
the introduction of a loop that traverses the transforma-
tions τ ∈ T , where the identity transformation, λ x. x, is
included to preserve the (non-transformed) value read at
the source. The value assigned to feature f is then the
minimum with respect to all transformed values.

Binary data — originating from the microphone, cam-
era or bluetooth adapter — also requires special han-
dling because of the binary versus ASCII representation
and, more signiﬁcantly, its high volume. Our solution is
guided by the assumption that such data is largely treated
as “uninterpreted” and immutable by application code
due to its form and format. This leads to a simple yet
effective strategy for similarity measurement, whereby a
ﬁxed-length preﬁx is truncated out of the binary content.
Truncation is also applied to sink arguments consisting
of binary data.

Heuristic Detection of Relevant Values So far, our
description of the BAYESDROID algorithm has relied on
tag propagation to identify relevant values at the sink
statement. While this is a robust mechanism to drive fea-
ture computation, ﬂowing tags throughout the code also
has its costs, incurring runtime overheads of ≥ 10% and
affecting the stability of the application due to intrusive
instrumentation [4].

These weaknesses of the tainting approach have led us
to investigate an alternative method of detecting relevant
values. A straightforward relaxation of data-ﬂow track-
ing is bounded (“brute-force”) traversal of the reachable
values from the arguments to a sink statement up to some
depth bound k: All values pointed-to by a sink argument
or reachable from a sink argument via a sequence of ≤ k
ﬁeld dereferences are deemed relevant. Though in theory

USENIX Association  

23rd USENIX Security Symposium  181

7

Input: T ≡ {λ x. x,τ1, . . . ,τ n} // std.
begin
. . .
OnSinkStatement r := snk p :

transformations

{ f (cid:28)→ p f} ←− ExtractTags p
foreach f → p f ∈ { f → p f} do

foreach τ ∈ T do
u ←− τ (ref f )
δ ←− min{d(u, [[p]])}p∈p f
f ←− min{[[ f ]],δ ≥ c f ? “ ≥c f ”: δ}

end

end

. . .

end
Algorithm 3: BAYESDROID support for standard data
transformations

this might introduce both false positives (due to irrele-
vant values that are incidentally similar to the reference
value) and false negatives (if k is too small, blocking rel-
evant values from view), in practice both are unlikely, as
we conﬁrmed experimentally. (See Section 5.)

For false positives, private values are often unique, and
so incidental similarity to irrelevant values is improbable.
For false negatives, the arguments ﬂowing into privacy
sinks are typically either String objects or simple data
structures. Also, because the number of privacy sinks is
relatively small, and the number of complex data struc-
tures accepted by such sinks is even smaller, it is pos-
sible to specify relevant values manually for such data
structures. We have encountered only a handful of data
structures (e.g.
the android.content.Intent class) that
motivate a speciﬁcation.

5 Experimental Evaluation

In this section, we describe the BAYESDROID implemen-
tation, and present two sets of experiments that we have
conducted to evaluate our approach.

5.1 The BAYESDROID System
Implementation Similarly to existing tools like Taint-
Droid, BAYESDROID is implemented as an instrumented
version of the Android SDK. Speciﬁcally, we have in-
strumented version 4.1.1 r6 of the SDK, which was cho-
sen intentionally to match the most recent version of
TaintDroid.3 The experimental data we present indeed
utilizes TaintDroid for tag propagation (as required for
accurate resolution for relevant values).

3 http://appanalysis.org/download.html

Beyond the TaintDroid instrumentation scheme, the
BAYESDROID scheme speciﬁes additional behaviors for
sources and sinks within the SDK. At source points, a
hook is added to record the private value read by the
source statement (which acts as a reference value). At
sink points, a hook is installed to apply Bayesian reason-
ing regarding the legitimacy of the sink.

Analogously to TaintDroid, BAYESDROID performs
privacy monitoring over APIs for ﬁle-system access and
manipulation, inter-application and socket communica-
tion, reading the phone’s state and location, and sending
of text messages. BAYESDROID also monitors the HTTP
interface, camera, microphone, bluetooth and contacts.
As explained in Section 4.1, each of the privacy sources
monitored by BAYESDROID is mirrored by a tag/fea-
ture. The full list of features is as follows: IMEI, IMSI,
AndroidID, Location, Microphone, Bluetooth, Camera,
Contacts and FileSystem.

The BAYESDROID implementation is conﬁgurable,
enabling the user to switch between distance metrics as
well as enable/disable information-ﬂow tracking for pre-
cise/heuristic determination of relevant values. (See Sec-
tion 4.2.) In our experiments, we tried both the Leven-
shtein and the Hamming metrics, but found no observ-
able differences, and so we report the results only once.
Our reasoning for why the metrics are indistinguishable
is because we apply both to equal-length strings (see Sec-
tion 3.2), and have made sure to apply the same metric
both ofﬂine and online, and so both metrics achieve a
very similar effect in the Bayesian setting.

Training To instantiate BAYESDROID with the re-
quired estimates, as explained in Section 3.3, we ap-
plied the following methodology: First,
to estimate
Pr(legitimate), we relied on (i) an extensive study by
Hornyack et al. spanning 1,100 top-popular free Android
apps [9], as well as (ii) a similarly comprehensive study
by Enck et al. [5], which also spans a set of 1,100 free
apps. According to the data presented in these studies,
approximately one out of three release points is illegiti-

mate, and thus (cid:31)Pr(legitimate) =0.66 and complementar-
ily (cid:31)Pr(illegitimate) =1 − 0.66 ≈ 0.33.
For the conditional probabilities (cid:31)Pr(Xi = xi j|Y = yk),

we queried Google Play for the 100 most popular apps
(across all domains) in the geography of one of the au-
thors. We then selected at random 35 of these apps, and
analyzed their information-release behavior using debug
breakpoints (which we inserted via the adb tool that is
distributed as part of the Android SDK).

Illegitimate leaks that we detected ofﬂine mainly in-
volved (i) location information and (ii) device and user
identiﬁers, which is consistent with the ﬁndings reported
by past studies [9, 5]. We conﬁrmed that illegitimate
leaks are largely correlated with high similarity between

182  23rd USENIX Security Symposium 

USENIX Association

8

private data and sink arguments, and so we ﬁxed six dis-
tance levels for each private item: [0,4] and “≥ 5”. (See
Section 3.1.) Finally, to avoid zero estimates for con-
ditional probabilities while also minimizing data pertur-
bation, we set the “smoothing” factor l in Equation 6 at
1, where the illegitimate ﬂows we detected were in the
order of several dozens per private item.

5.2 Experimental Hypotheses
In our experimental evaluation of BAYESDROID, we
tested two hypotheses:

1. H1: Accuracy. Bayesian reasoning, as implemented
in BAYESDROID, yields a signiﬁcant improvement
in leakage-detection accuracy compared to the base-
line of information-ﬂow tracking.

For

2. H2: Applicability.

applications,
BAYESDROID remains effective under relaxation
of the tag-based method for detection of relevant
values and its stability improves.

real-life

5.3 H1: Accuracy
To assess the accuracy of BAYESDROID, we compared it
to that of TaintDroid, a state-of-the-art information-ﬂow
tracking tool for privacy enforcement. Our experimental
settings and results are described below.

Subjects We applied both TaintDroid and BAYES-
DROID to DroidBench, an independent and publicly
available collection of benchmarks serving as testing
ground for both static and dynamic privacy enforcement
algorithms. DroidBench models a large set of realistic
challenges in leakage detection, including precise track-
ing of sensitive data through containers, handling of call-
backs, ﬁeld and object sensitivity, lifecycle modeling,
inter-app communication, reﬂection and implicit ﬂows.
The DroidBench suite consists of 50 cases. We ex-
cluded from our experiment (i) 8 benchmarks that crash
at startup, as well as (ii) 5 benchmarks that leak data
via callbacks that we did not manage to trigger (e.g.,
onLowMemory()), as both TaintDroid and BAYESDROID
were naturally unable to detect leakages in these two
cases. The complete list of benchmarks that we used can
be found in Table 4 of Appendix B.

Methodology For each benchmark, we measured the
number of true positive (TP), false positive (FP) and false
negative (FN) results. We then summarized the results
and calculated the overall precision and recall of each
tool using the formulas below:

Precision = T P

T P+FP

Recall = T P

T P+FN

TaintDroid
BAYESDROID

TPs FPs FNs Precision Recall F-measure
31
29

0.64
0.96

1.00
0.93

0.78
0.94

17
1

0
2

Table 1: Accuracy of BAYESDROID and TaintDroid on
DroidBench

High precision implies that a technique returns few irrel-
evant results, whereas high recall implies that it misses
only few relevant ones.

Since ideal techniques have both high recall and high
precision, the F-measure is commonly used to combine
both precision and recall into a single measure. The F-
measure is deﬁned as the harmonic mean of precision
and recall, and is calculated as follows:

F-Measure = 2× Precision×Recall

Precision+Recall

The value of F-measure is high only when both preci-
sion and recall are high. We thus use the F-measure for
accuracy evaluation.

Results The results obtained for both TaintDroid and
BAYESDROID on version 1.1 of DroidBench are sum-
marized in Table 1 and presented in detail in Table 4.
The ﬁndings reported by BAYESDROID are also publicly
available.4

Overall, TaintDroid detects 31 true leakages while also
reporting 17 false positives, whereas BAYESDROID suf-
fers from 2 false negatives, discovering 29 of the true
leakages while ﬂagging only 1 false alarm. The recall of
both TaintDroid and BAYESDROID is high (1 and 0.93,
respectively) due to a low number of false-negative re-
sults. Yet the precision of TaintDroid is much lower than
that of BAYESDROID (0.64 vs. 0.96), due to a high num-
ber of false positives. The overall F-measure is thus
lower for TaintDroid than for BAYESDROID (0.78 vs.
0.94).

The results mark BAYESDROID as visibly more ac-
curate than TaintDroid. To further conﬁrm this result,
we performed a two-tail McNemar test, considering 48
observations for each tool. These observations corre-
spond to ﬁndings reported in Table 4: 31 true positives
and 17 classiﬁed as false alarms. Each observation is
a boolean value that represents the accuracy of the tool
and is assumed to be from a Bernoulli distribution. We
then checked whether the difference in accuracy is statis-
tically signiﬁcant by testing the null hypothesis that the
set of 48 observations from TaintDroid are sampled from
the same Bernoulli distribution as the set of 48 observa-
tions from BAYESDROID.

4

See

archive
otripp/droidbench.zip.

ﬁle

researcher.ibm.com/researcher/ﬁles/us-

USENIX Association  

23rd USENIX Security Symposium  183

9

getSystemService(TELEPHONY SERVICE);

1 TelephonyManager tm =
2
3 String imei = tm.getDeviceId(); //source
4 String obfuscatedIMEI = obfuscateIMEI(imei);
5 Log.i (imei ); // sink
6

...;

7 private String obfuscateIMEI(String imei) {

8

9

10

11

12

13

String result = ””;
for (char c :
switch(c) {
case ’0’ :
case ’1’ :
case ’2’ :

imei .toCharArray()) {
result += ’a’; break;
result += ’b’; break;
result += ’c’; break;

...; } }

Figure 5:
from the DroidBench
ImplicitFlow1 benchmark, which applies a custom
transformation to private data

Fragment

We found that TaintDroid was accurate in 31 out of
48 cases, and BAYESDROID was accurate in 45 out of
48 cases. We built the 2×2 contingency table showing
when each tool was correct and applied a two-tail Mc-
Nemar test. We found a p-value of 0.001, which rejects
the null hypothesis that the observations come from the
same underlying distribution and provides evidence that
BAYESDROID is more accurate than TaintDroid, thereby
conﬁrming H1.

Discussion Analysis of the per-benchmark ﬁndings re-
veals the following: First,
the 2 false negatives of
BAYESDROID on ImplicitFlow1 are both due to cus-
tom (i.e., non-standard) data transformations, which are
outside the current scope of BAYESDROID. An illustra-
tive fragment from the ImplicitFlow1 code is shown in
Figure 5. The obfuscateIMEI(. . .) transformation maps
IMEI digits to English letters, which is a non-standard
behavior that is unlikely to arise in an authentic app.

The false positive reported by BAYESDROID, in com-
mon with TaintDroid, is on release of sensitive data to
the ﬁle system, albeit using the MODE PRIVATE ﬂag, which
does not constitute a leakage problem in itself. This can
be resolved by performing Bayesian reasoning not only
over argument values, but also over properties of the sink
API (in this case, the storage location mapped to a ﬁle
handle). We intend to implement this enhancement.

Beyond the false alarm in common with BAYES-
DROID, TaintDroid has multiple other sources of impre-
cision. The main reasons for its false positives are

(cid:127) coarse modeling of containers, mapping their en-
tire contents to a single taint bit, which accounts
e.g. for the false alarms on ArrayAccess{1,2} and
HashMapAccess1;
in
(cid:127) ﬁeld
and
false

insensitivity,
resulting
FieldSensitivity{2,4}

and
alarms

object
on

)

%

(
 
d
a
e
h
r
e
v
O

 
l
l

a
r
e
v
O

140

105

70

35

0

Tag Propagation Overhead

Bayesian Analysis Overhead

1

2

3

4

5

6

7

9 10 11 12 13 14 15 16 17 18 19

8
Propagation Steps

Figure 6: Overhead breakdown into tag propagation and
Bayesian analysis at sink

ObjectSensitivity{1,2}; and more fundamentally,
(cid:127) ignoring of data values, which causes TaintDroid
to issue false warnings on LocationLeak{1,2} even
when location reads fail, yielding a Location object
without any meaningful information.

The fundamental reason for these imprecisions is to con-
strain the overhead of TaintDroid, such that it can meet
the performance demands of online privacy enforcement.
BAYESDROID is able to accommodate such optimiza-
tions while still ensuring high accuracy.

5.4 H2: Applicability
The second aspect of the evaluation compared between
two versions of BAYESDROID, whose sole difference
lies in the method used for detecting relevant values: In
one conﬁguration (T-BD), relevant values are detected
via tag propagation. The other conﬁguration (H-BD)
uses the heuristic detailed in Section 4.2 of treating all
values reachable from sink arguments (either directly or
via the heap graph) up to a depth bound of k as relevant,
which places more responsibility on Bayesian reasoning.
We set k at 3 based on manual review of the data struc-
tures ﬂowing into privacy sinks.

We designed a parametric benchmark application to
quantify the overhead reduction imposed by the H-BD
variant of BAYESDROID. The application consists of a
simple loop that ﬂows the device IMEI into a log ﬁle.
Loop iterations perform intermediate data propagation
steps. We then performed a series of experiments —
over the range of 1 to 19 propagation steps — to quantify
the relative overhead of tag propagation versus Bayesian
analysis.

The results, presented in Figure 6, suggest that the
overhead of tag propagation is more dominant than that
of Bayesian analysis (with a ratio of roughly 2:1), even
when the set of relevant values is naively over approx-
imated. Discussion of the methodology underlying this

184  23rd USENIX Security Symposium 

USENIX Association

10

experiment is provided in Appendix A.

In general, H-BD trades overhead reduction for accu-
racy. H2 then asserts that, in practice, the tradeoff posed
by H-BD is effective. Below, we discuss our empirical
evaluation of this hypothesis over real-life subjects.

TPs FPs FNs Precision Recall F-measure Crashes

H-BD 27
T-BD
14

1
0

0
10

0.96
1.00

1.00
0.58

0.98
0.73

12
22

Table 2: Accuracy of H-BD and T-BD BAYESDROID
conﬁgurations

Subjects To avoid evaluators’ bias, we applied the fol-
lowing selection process: We started from the 65 Google
Play apps not chosen for the training phase. We then ex-
cluded 8 apps that do not have permission to access sen-
sitive data and/or perform release operations (i.e., their
manifest does not declare sufﬁcient permissions out of
INTERNET, READ PHONE STATE, SEND SMS, etc), as well as 3
apps that we did not manage to install properly, resulting
in 54 apps that installed successfully and exercise privacy
sources and sinks.

The complete list of the application we used is given
in Table 5 of Appendix B. A subset of the applications,
for which at least one leakage was detected, is also listed
in Table 3.

Methodology We deployed the apps under the two
BAYESDROID conﬁgurations. Each execution was done
from a clean starting state. The third column of both
Tables 3 and 5 denotes whether our exploration of the
app was exhaustive. By that we mean exercising all the
UI points exposed by the app in a sensible order. Ide-
ally we would do so for all apps. However, (i) some
of the apps, and in particular gaming apps, had stability
issues, and (ii) certain apps require SMS-validated sign
in, which we did not perform. We did, however, cre-
ate Facebook, Gmail and Dropbox accounts to log into
apps that demand such information yet do not ask for
SMS validation. We were also careful to execute the ex-
act same crawling scenario under both the T-BD and H-
BD conﬁgurations. We comment, from our experience,
that most data leaks happen when an app launches, and
initializes advertising/analytics functionality, and so for
apps for which deep crawling was not possible the results
are still largely meaningful.

For comparability between the H-BD and T-BD con-
ﬁgurations, we counted different dynamic reports involv-
ing the same pair of source/sink APIs as a single leakage
instance. We manually classiﬁed the ﬁndings into true
positives and false positives. For this classiﬁcation, we
scrutinized the reports by the two conﬁgurations, and
also — in cases of uncertainty — decompiled and/or
reran the app to examine its behavior more closely. As in
the experiment described in Section 5.3, we then calcu-
lated the precision, recall and F-measure for each of the
tools.

Results The results obtained for H-BD and T-BD are
summarized in Table 2. Table 3 summarizes the ﬁnd-
ings reported by both H-BD and T-BD at the granularity
of privacy items: the device number, identiﬁer and loca-
tion, while Table 5 provides a detailed description of the
results across all benchmarks including those on which
no leakages were detected. The warnings reported by
the H-BD conﬁguration are also publicly available for
review.5

As Table 2 indicates, the H-BD variant is more accu-
rate than the T-BD variant overall (F-measure of 0.98 vs.
0.73). As in the experiment described in Section 5.3, we
further performed a two-tail McNemar test, considering
67 observations for each tool: 27 that correspond to true
positives, 1 to the false positive due to H-BD and 39 to
cases where no leakages were found.

We found that H-BD was accurate in 66 out of 67
cases, and T-DB was accurate in 54 out of 67 cases.
Building the 2×2 contingency table and applying the
two-tail McNemar test showed that the difference be-
tween the tools in accuracy is signiﬁcant (with a p-
value of 0.001 to reject the null hypothesis that the ac-
curacy observations for both tools come from the same
Bernoulli distribution). Moreover, H-BD has a lower
number of crashes and lower runtime overhead, which
conﬁrms H2.

Discussion To give the reader a taste of the ﬁndings,
we present in Figures 7–8 two examples of potential
leakages that BAYESDROID (both the H-BD and the
T-BD conﬁgurations) deemed legitimate. The instance
in Figure 7 reﬂects the common scenario of obtain-
ing the current (or last known) location, converting it
into one or more addresses, and then releasing only the
country or zip code.
In the second instance, in Fig-
ure 8, the 64-bit Android ID — generated when the
user ﬁrst sets up the device — is read via a call to
Settings$Secure.getString(ANDROID ID). At the release
point, into the ﬁle system, only a preﬁx of the Android
ID consisting of the ﬁrst 12 digits is published.

As Table 3 makes apparent, the ﬁndings by H-BD are
more complete: It detects 18 leakages (versus 8 reports
by T-BD), with no false negative results and only one
false positive. We attribute that to (i) the intrusive instru-

See

archive

ﬁle

researcher.ibm.com/researcher/ﬁles/us-

5

otripp/realworldapps.zip.

USENIX Association  

23rd USENIX Security Symposium  185

11

App

Domain

Deep crawl?

atsoft.games.smgame
com.antivirus
com.appershopper.ios7lockscreen
com.bestcoolfungames.antsmasher
com.bitﬁtlabs.ﬁngerprint.lockscreen
com.cleanmaster.mguard
com.coolﬁsh.cathairsalon
com.coolﬁsh.snipershooting
com.digisoft.TransparentScreen
com.g6677.android.cbaby
com.g6677.android.chospital
com.g6677.android.design
com.g6677.android.pnailspa
com.g6677.android.princesshs
com.goldtouch.mako
15

games/arcade
communication
personalization
games/arcade
games/casual

tools

games/casual
games/action
entertainment
games/casual
games/casual
games/casual
games/casual
games/casual

news

(cid:31)
(cid:31)

(cid:31)

(cid:31)
(cid:31)
(cid:31)
(cid:31)

(cid:31)
8

(cid:31)

1

(cid:31)
(cid:31)
(cid:31)

(cid:31)
(cid:31)
(cid:31)
(cid:31)

(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
13

(cid:31)

(cid:31)
(cid:31)

(cid:31)

4

0

H-BD

T-BD

number dev. ID location number dev. ID location

(cid:31)
(cid:31)

(cid:31)

(cid:31)
4

(cid:31)

(cid:31)
(cid:31)

(cid:31)

4

Table 3: Warnings by the H-BD and T-BD BAYESDROID conﬁgurations on 15/54 top-popular mobile apps

source : private value

source : private value

GeoCoder.getFromLocation(...) :
...,

..., Bearing:

Alt :

..., Long:

...,

Settings $Secure.getString (...)

: cdf15124ea4c7ad5

[ Lat:
...,

IL ]

sink : arguments

WebView.loadUrl(...) : http ://linux .appwiz.com/

proﬁle /72/72 exitad.html?
p1=RnVsbCtBbmRyb2lkK29uK0VtdWxhdG9y&
p2=Y2RmMTUxMjRlYTRjN2FkNQ%3d%3d&
... LOCATION=IL& ...
MOBILE COUNTRY CODE=&
NETWORK=WIFI

Figure 7: Suppressed warning on ios7lockscreen

mentation required for tag propagation, which can cause
instabilities, and (ii) inability to track tags through native
code, as discussed below.

The T-BD variant introduces signiﬁcantly more insta-
bility than the H-BD variant, causing illegal application
behaviors in 21 cases compared to only 12 under H-BD.
We have investigated this large gap between the H-BD
and T-BD conﬁgurations, including by decompiling the
subject apps. Our analysis links the vast majority of ille-
gal behaviors to limitations that TaintDroid casts on load-
ing of third-party libraries. For this reason, certain func-
tionality is not executed, also leading to exceptional app
states, which both inhibit certain data leaks.6

A secondary reason why H-BD is able to detect more
leakages, e.g. in the lockscreen app, is that this bench-

a

technical

For
by William Enck,

6
ment
https://groups.google.com/forum/#!topic/android-security-
discuss/U1fteEX26bk.

explanation,

see

the

TaintDroid moderator,

forum com-
at

sink : arguments

FileOutputStream.write (...)

:

<?xml version=’1.0’ encoding=’utf−8’
standalone=’yes’
?><map><string
name=”openudid”>cdf15124ea4c

Figure 8: Suppressed warning on fruitninjafree

mark makes use of the mobileCore module,7 which is a
highly optimized and obfuscated library. We suspect that
data-ﬂow tracking breaks within this library, though we
could not fully conﬁrm this.

At the same time, the loss in accuracy due to heuris-
tic identiﬁcation of relevant values is negligible, as
suggested by the discussion in Section 4.2. H-BD
triggers only one false alarm, on ios7lockscreen, which
is due to overlap between irrelevant values:
extra
information on the Location object returned by a call
to
LocationManager.getLastKnownLocation(...)
and
a
ContextWrapper.startService(...) request. Finally, as
expected, H-BD does not incur false negatives.

unrelated

metadata

passed

into

6 Related Work

As most of the research on privacy monitoring builds on
the tainting approach, we survey related research mainly
in this space. We also mention several speciﬁc studies in
other areas.

7 https://www.mobilecore.com/sdk/

186  23rd USENIX Security Symposium 

USENIX Association

12

Realtime Techniques The state-of-the-art system for
realtime privacy monitoring is TaintDroid [4]. Taint-
Droid features tolerable runtime overhead of about 10%,
and can track taint ﬂow not only through variables and
methods but also through ﬁles and messages passed be-
tween apps. TaintDroid has been used, extended and cus-
tomized by several follow-up research projects. Jung et
al. [10] enhance TaintDroid to track additional sources
(including contacts, camera, microphone, etc). They
used the enhanced version in a ﬁeld study, which re-
vealed 129 of the 223 apps they studied as vulnerable.
30 out of 257 alarms were judged as false positives.
The Kynoid system [20] extends TaintDroid with user-
deﬁned security policies, which include e.g.
temporal
constraints on data processing as well as restrictions on
destinations to which data is released.

The main difference between BAYESDROID and the
approaches above, which all apply information-ﬂow
tracking, is that BAYESDROID exercises “fuzzy” reason-
ing, in the form of statistical classiﬁcation, rather than
enforcing a clear-cut criterion. As part of this, BAYES-
DROID factors into the privacy judgment the data val-
ues ﬂowing into the sink statement, which provides ad-
ditional evidence beyond data ﬂow.

Quantitative Approaches Different approaches have
been proposed for quantitative information-ﬂow analy-
sis, all uniﬁed by the observation that data leakage is a
quantitative rather than boolean judgment. McCamant
and Ernst [13] present an ofﬂine dynamic analysis that
measures the amount of secret information that can be
inferred from a program’s outputs, where the text of the
program is considered public. Their approach relies on
taint analysis at the bit level. Newsome et al. [14] de-
velop complementary techniques to bound a program’s
channel capacity using decision procedures (SAT and
#SAT solvers). They apply these techniques to the prob-
lem of false positives in dynamic taint analysis. Backes
et al. [1] measure leakage in terms of indistinguishabil-
ity, or equivalence, between outputs due to different se-
cret artifacts. Their characterization of equivalence re-
lations builds on the information-theoretic notion of en-
tropy. Budi et al. [2] propose kb-anonymity, a model in-
spired by k-anonymity that replaces certain information
in the original data for privacy preservation, but beyond
that also ensures that the replaced data does not lead to
divergent program behaviors.

While these proposals have all been shown useful,
none of these approaches has been shown to be efﬁcient
enough to meet realtime constraints. The algorithmic
complexity of computing the information-theoretic mea-
sures introduced by these works seriously limits their ap-
plicability in a realtime setting. Our approach, instead,
enables a quantitative/probabilistic mode of reasoning

that is simultaneously lightweight, and therefore accept-
able for online monitoring, by focusing on relevant fea-
tures that are efﬁciently computable.

Techniques for Protecting Web Applications There
exist numerous static and dynamic approaches for pre-
venting attacks on web applications, e.g., [23, 22, 7].
Most relevant to our work are Sekar’s taint-inference
technique for deducing taint propagation by comparing
inputs and outputs of a protected server-side applica-
tion [21] and a similar browser-resident technique devel-
oped in a subsequent study [16]. While BAYESDROID
shares ideas with these approaches, it is explicitly de-
signed for mobile devices and applications. Curtsinger et
al. [3] apply a Bayesian classiﬁer to identify JavaScript
syntax elements that are highly predictive of malware.
The proposed system, ZOZZLE, analyzes the applica-
tion’s code statically, while BAYESDROID operates dy-
namically and focuses on data values.

7 Conclusion and Future Work

In this paper, we articulated the problem of privacy en-
forcement in mobile systems as a classiﬁcation problem.
We explored an alternative to the traditional approach of
information-ﬂow tracking, based on statistical reasoning,
which addresses more effectively the inherent fuzziness
in leakage judgements. We have instantiated our ap-
proach as the BAYESDROID system. Our experimental
data establishes the high accuracy of BAYESDROID as
well as its applicability to real-world mobile apps.

Moving forward, we have two main objectives. The
ﬁrst is to extend BAYESDROID with additional feature
types. Speciﬁcally, we would like to account for (i) sink
properties, such as ﬁle access modes (private vs pub-
lic), the target URL of HTTP communication (same do-
main or third party), etc; as well as (ii) the history of
privacy-relevant API invocations up to the release point
(checking e.g. if/which declassiﬁcation operations were
invoked). Our second objective is to optimize our ﬂow-
based method for detecting relevant values (see Sec-
tion 3.1) by applying (ofﬂine) static taint analysis to the
subject program, e.g. using the FlowDroid tool [6].

References

[1] M. Backes, B. Kopf, and A. Rybalchenko. Auto-
matic discovery and quantiﬁcation of information
leaks. In S&P, pages 141–153, 2009.

[2] A. Budi, D. Lo, L. Jiang, and Lucia. kb-anonymity:
a model for anonymized behaviour-preserving test
and debugging data.
In PLDI, pages 447–457,
2011.

USENIX Association  

23rd USENIX Security Symposium  187

13

[3] Charlie Curtsinger, Benjamin Livshits, Ben-
jamin G. Zorn, and Christian Seifert. Zozzle: Fast
and precise in-browser javascript malware detec-
tion. In USENIX Security, pages 33–48, 2011.

[15] J. Newsome and D. X. Song. Dynamic taint analy-
sis for automatic detection, analysis, and signature
generation of exploits on commodity software. In
NDSS, 2005.

[4] W. Enck, P. Gilbert, B. Chun, L. P. Cox, J. Jung,
P. McDaniel, and A. N. Sheth. Taintdroid: an
information-ﬂow tracking system for realtime pri-
vacy monitoring on smartphones. In OSDI, pages
1–6, 2010.

[5] W. Enck, D. Octeau, P. McDaniel, and S. Chaud-
In

huri. A study of android application security.
USENIX Security, pages 21–21, 2011.

[6] C. Fritz, S. Arzt, S. Rasthofer, E. Bodden, A. Bar-
tel, J. Klein, Y. Traon, D. Octeau, and P. Mc-
Daniel. Flowdroid: Precise context, ﬂow, ﬁeld,
object-sensitive and lifecycle-aware taint analysis
for android apps, 2014.

[7] S. Guarnieri, M. Pistoia, O. Tripp, J. Dolby, S. Teil-
het, and R. Berg. Saving the world wide web from
vulnerable javascript.
In ISSTA, pages 177–187,
2011.

[8] S. Holavanalli, D. Manuel, V. Nanjundaswamy,
B. Rosenberg, F. Shen, S. Y. Ko, and L. Ziarek.
Flow permissions for android. In ASE, pages 652–
657, 2013.

[9] P. Hornyack, S. Han, J. Jung, S. E. Schechter, and
D. Wetherall. These aren’t the droids you’re look-
ing for: retroﬁtting android to protect data from im-
perious applications. In CCS, pages 639–652, 2011.

[10] J. Jung, S. Han, and D. Wetherall. Short paper: en-
hancing mobile application permissions with run-
time feedback and constraints. In SPSM, pages 45–
50, 2012.

[11] B. Livshits and J. Jung. Automatic mediation of
privacy-sensitive resource access in smartphone ap-
plications.
In USENIX Security, pages 113–130,
2013.

[12] G. Lowe. Quantifying information ﬂow. In CSFW,

pages 18–31, 2002.

[13] S. McCamant and M. D. Ernst. Quantitative infor-
In PLDI,

mation ﬂow as network ﬂow capacity.
pages 193–205, 2008.

[14] J. Newsome, S. McCamant, and D. Song. Measur-
ing channel capacity to distinguish undue inﬂuence.
In PLAS, pages 73–85, 2009.

[16] Riccardo Pelizzi and R. Sekar. Protection, usability
and improvements in reﬂected xss ﬁlters. In ASI-
ACCS, pages 5–5, 2012.

[17] J. Piskorski and M. Sydow. String distance metrics
for reference matching and search query correction.
In BIS, pages 353–365, 2007.

[18] V. Rastogi, Y. Chen, and W. Enck. Appsplay-
ground: automatic security analysis of smartphone
applications. In CODAPSY, pages 209–220, 2013.

[19] G. Sarwar, O. Mehani, R. Boreli, and M. A. Ka-
far. On the effectiveness of dynamic taint analysis
for protecting against private information leaks on
android-based devices.
In SECRYPT, pages 461–
468, 2013.

[20] D. Schreckling,

J. K¨ostler,

J. Posegga,

and
M. Schaff. Kynoid:
real-time enforcement of
ﬁne-grained, user-deﬁned, and data-centric security
policies for android.
In WISTP, pages 208–223,
2012.

[21] R. Sekar. An efﬁcient black-box technique for de-

feating web application attacks. In NDSS, 2009.

[22] O. Tripp, M. Pistoia, P. Cousot, R. Cousot, and
S. Guarnieri. Andromeda: Accurate and scalable
security analysis of web applications.
In FASE,
pages 210–225, 2013.

[23] O. Tripp, M. Pistoia, S. J. Fink, M. Sridharan, and
O. Weisman. Taj: effective taint analysis of web
applications. In PLDI, pages 87–97, 2009.

[24] R. A. Wagner and M. J. Fischer. The string-to-
string correction problem. J. ACM, 21(1):168–173,
1974.

[25] Bernard L Welch. The generalization of student’s
problem when several different population vari-
ances are involved. Biometrika, 34(1–2):28–35,
1947.

[26] D. Wetherall, D. Choffnes, B. Greenstein, S. Han,
P. Hornyack, J. Jung, S. Schechter, and X. Wang.
Privacy revelations for web and mobile apps.
In
HotOS, pages 21–21, 2011.

[27] Z. Yang, M. Yang, Y. Zhang, G. Gu, P. Ning, and
X. S. Wang. Appintent: analyzing sensitive data
transmission in android for privacy leakage detec-
tion. In CCS, pages 1043–1054, 2013.

188  23rd USENIX Security Symposium 

USENIX Association

14

Benchmark

Algorithm

TPs

FPs

FNs

ActivityCommunication1

ActivityLifecycle1

ActivityLifecycle2

ActivityLifecycle4

Library2

Obfuscation1

PrivateDataLeak3

AnonymousClass1

ArrayAccess1

ArrayAccess2

HashMapAccess1

Button1

Button3

Ordering1

RegisterGlobal1

DirectLeak1

FieldSensitivity2

FieldSensitivity3

FieldSensitivity4

ImplicitFlow1

InheritedObjects1

ListAccess1

LocationLeak1

LocationLeak2

Loop1

Loop2

ApplicationLifecycle1

ApplicationLifecycle3

MethodOverride1

ObjectSensitivity1

ObjectSensitivity2

Reﬂection1

Reﬂection2

Reﬂection3

Reﬂection4

SourceCodeSpeciﬁc1

StaticInitialization1

Total

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

BAYESDROID

TaintDroid

1
1
1
1
1
1
1
1
1
1
1
1
1
1
0
0
0
0
0
0
0
0
1
1
2
2
0
0
1
1
1
1
0
0
1
1
0
0
0
2
1
1
0
0
0
0
0
0
1
1
1
1
1
1
1
1
1
1
0
0
0
0
1
1
1
1
1
1
1
1
5
5
1
1
29
31

0
0
0
0
0
0
0
0
0
0
0
0
1
1
0
1
0
1
0
1
0
1
0
0
0
0
0
2
0
0
0
0
0
1
0
0
0
1
0
0
0
0
0
1
0
2
0
2
0
0
0
0
0
0
0
0
0
0
0
1
0
2
0
0
0
0
0
0
0
0
0
0
0
0
1
17

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
2
0

A Overhead Measurement: Methodology

To complete the description in Section 5.4, we now detail
the methodology governing our overhead measurements.
The behavior of the benchmark app is governed by two
user-controlled values: (i) the length (cid:31) of the source/sink
data-ﬂow path (which is proportional to the number of
loop iterations) and (ii) the number m of values reachable
from sink arguments.

Based on our actual benchmarks, as well as data re-
ported in past studies [23], we deﬁned the ranges 1 ≤ (cid:31) ≤
19 and 1 ≤ m ≤ 13 = Σ2
n=03n. We then ran the parametric
app atop a “hybrid” conﬁguration of BAYESDROID that
simultaneously propagates tags and treats all the values
ﬂowing into a sink as relevant. For each value of (cid:31), we
executed the app 51 times, picking a value from the range
[0,2] for n uniformly at random in each of the 51 runs.
We then computed the average overhead over the runs,
excluding the ﬁrst (cold) run to remove unrelated initial-
ization costs. The stacked columns in Figure 6 each cor-
respond to a unique value of (cid:31).

B Detailed Results

Table 4 summarizes the results of the H1 experiment de-
scribed in Section 5.3. For each of the benchmarks, it
speciﬁes the number of true-positive, false-positive and
false-negative ﬁndings for the compared tools, BAYES-
DROID and TaintDroid. The benchmarks on which the
tools differ are highlighted for convenience.

Similarly, Table 5 summarizes the results of the H2 ex-
periment described in Section 5.4. The ﬁrst two columns
of Table 5 list the applications and their respective do-
main, and the third column denotes whether crawling
was exhaustive. Then, the number of crashes, true-
positive, false-positive and false-negative ﬁndings are
reported for both the H-BD and the T-BD variants of
BAYESDROID.

In Section 5.4, we describe an experiment designed to
evaluate our Bayesian analysis in “pure” form, i.e. with-
out the support of information-ﬂow tracking to detect
relevant values. To make our description of this experi-
ment complete, we include Table 5, which provides a de-
tailed summary of the results of this experiment across all
benchmarks (including ones on which no leakages were
detected). For comparability between the H-BD and T-
BD conﬁgurations, we count different dynamic reports
involving the same pair of source/sink APIs as a single
leakage instance.

Table 4: Detailed summary of the results of the H1 ex-
periment described in Section 5.3

USENIX Association  

15

23rd USENIX Security Symposium  189

App

Domain

Deep crawl?

air.au.com.metro.DumbWaysToDie
at.nerbrothers.SuperJump
atsoft.games.smgame
com.antivirus
com.appershopper.ios7lockscreen
com.applicaster.il.hotvod
com.appstar.callrecorder
com.awesomecargames.mountainclimbrace 1
com.bestcoolfungames.antsmasher
com.bigduckgames.ﬂow
com.bitﬁtlabs.ﬁngerprint.lockscreen
com.channel2.mobile.ui
com.chillingo.parkingmaniafree.android.rowgplay
com.cleanmaster.mguard
com.coolﬁsh.cathairsalon
com.coolﬁsh.snipershooting
com.cube.gdpc.isr
com.cyworld.camera
com.devuni.ﬂashlight
com.digisoft.TransparentScreen
com.domobile.applock
com.dropbox.android
com.ea.game.ﬁfa14 row
com.ebay.mobile
com.facebook.katana
com.facebook.orca
com.g6677.android.cbaby
com.g6677.android.chospital
com.g6677.android.design
com.g6677.android.pnailspa
com.g6677.android.princesshs
com.gameclassic.towerblock
com.gameloft.android.ANMP.GloftDMHM
com.game.fruitlegendsaga
com.gau.go.launcherex
com.glu.deerhunt2
com.goldtouch.mako
com.goldtouch.ynet
com.google.android.apps.docs
com.google.android.apps.translate
com.google.android.youtube
com.google.earth
com.halfbrick.fruitninjafree
com.halfbrick.jetpackjoyride
com.icloudzone.AsphaltMoto2
com.ideomobile.hapoalim
com.imangi.templerun2
com.kiloo.subwaysurf
com.king.candycrushsaga
com.sgiggle.production
com.skype.raider
com.UBI.A90.WW
com.viber.voip
com.whatsapp

games/casual
games/arcade
games/arcade
communication
personalization
entertainment

tools

games/racing
games/arcade
games/puzzles
games/casual

news

games/racing

tools

games/casual
games/action
health & ﬁtness

photography

entertainment

tools

tools

productivity
games/sports

shopping

social

communication
games/casual
games/casual
games/casual
games/casual
games/casual
games/puzzles
games/casual
games/puzzles
personalization
games/arcade

news
news

productivity

tools

media & video
travel & local
games/arcade
games/arcade
games/racing

ﬁnance

games/arcade
games/arcade
games/arcade

social

communication
games/arcade
communication
communication

(cid:31)
(cid:31)

(cid:31)

(cid:31)

(cid:31)

(cid:31)
(cid:31)
(cid:31)

(cid:31)
(cid:31)
(cid:31)
(cid:31)

(cid:31)

(cid:31)

(cid:31)
(cid:31)

(cid:31)

H-BD

T-BD

(cid:31)

(cid:31)
(cid:31)

(cid:31)

(cid:31)

(cid:31)

(cid:31)
(cid:31)

(cid:31)

(cid:31)

(cid:31)

(cid:31)
(cid:31)
(cid:31)

Crashes TPs FPs FNs Crashes TPs FPs FNs
0
0
0
0
3
0
0
0
0
0
1
0
0
0
1
1
0
0
0
0
0
0
0
0
0
0
1
1
1
1
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
4
1
5
0
0
0
2
0
2
0
0
1
2
2
0
0
0
2
0
0
0
0
0
0
1
1
1
1
1
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
4
1
3
0
0
0
2
0
0
0
0
1
0
0
0
0
0
2
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0

(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)
(cid:31)

(cid:31)

(cid:31)
(cid:31)
(cid:31)

(cid:31)
(cid:31)
(cid:31)

(cid:31)
(cid:31)
(cid:31)

(cid:31)

(cid:31)

(cid:31)

(cid:31)

Total

17

12

27

1

0

22

14

0

10

Table 5: Detailed summary of the results of the H2 experiment described in Section 5.4

190  23rd USENIX Security Symposium 

16

USENIX Association

