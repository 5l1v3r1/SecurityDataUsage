De-anonymizing Programmers  

via Code Stylometry

Aylin Caliskan-Islam, Drexel University; Richard Harang, U.S. Army Research Laboratory; 

Andrew Liu, University of Maryland; Arvind Narayanan, Princeton University;  

Clare Voss, U.S. Army Research Laboratory; Fabian Yamaguchi, University of Goettingen; 

Rachel Greenstadt, Drexel University

https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/caliskan-islam

This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-931971-232Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXDe-anonymizing Programmers via Code Stylometry

Aylin Caliskan-Islam

Drexel University
Arvind Narayanan
Princeton University

Richard Harang

U.S. Army Research Laboratory

Clare Voss

U.S. Army Research Laboratory

Andrew Liu

University of Maryland

Fabian Yamaguchi

University of Goettingen

Rachel Greenstadt
Drexel University

Abstract
Source code authorship attribution is a signiﬁcant pri-
vacy threat to anonymous code contributors. However,
it may also enable attribution of successful attacks from
code left behind on an infected system, or aid in resolv-
ing copyright, copyleft, and plagiarism issues in the pro-
gramming ﬁelds. In this work, we investigate machine
learning methods to de-anonymize source code authors
of C/C++ using coding style. Our Code Stylometry Fea-
ture Set is a novel representation of coding style found
in source code that reﬂects coding style from properties
derived from abstract syntax trees.

Our random forest and abstract syntax tree-based ap-
proach attributes more authors (1,600 and 250) with sig-
niﬁcantly higher accuracy (94% and 98%) on a larger
data set (Google Code Jam) than has been previously
achieved. Furthermore, these novel features are robust,
difﬁcult to obfuscate, and can be used in other program-
ming languages, such as Python. We also ﬁnd that (i) the
code resulting from difﬁcult programming tasks is easier
to attribute than easier tasks and (ii) skilled programmers
(who can complete the more difﬁcult tasks) are easier to
attribute than less skilled programmers.

1

Introduction

Do programmers leave ﬁngerprints in their source code?
That is, does each programmer have a distinctive “cod-
ing style”? Perhaps a programmer has a preference for
spaces over tabs, or while loops over for loops, or,
more subtly, modular rather than monolithic code.

These questions have strong privacy and security im-
plications. Contributors to open-source projects may
hide their identity whether they are Bitcoin’s creator or
just a programmer who does not want her employer to
know about her side activities. They may live in a regime
that prohibits certain types of software, such as censor-
ship circumvention tools. For example, an Iranian pro-

grammer was sentenced to death in 2012 for developing
photo sharing software that was used on pornographic
websites [31].

The ﬂip side of this scenario is that code attribution
may be helpful in a forensic context, such as detection of
ghostwriting, a form of plagiarism, and investigation of
copyright disputes. It might also give us clues about the
identity of malware authors. A careful adversary may
only leave binaries, but others may leave behind code
written in a scripting language or source code down-
loaded into the breached system for compilation.

While this problem has been studied previously, our
work represents a qualitative advance over the state of the
art by showing that Abstract Syntax Trees (ASTs) carry
authorial ‘ﬁngerprints.’ The highest accuracy achieved
in the literature is 97%, but this is achieved on a set of
only 30 programmers and furthermore relies on using
programmer comments and larger amounts of training
data [12, 14]. We match this accuracy on small program-
mer sets without this limitation. The largest scale exper-
iments in the literature use 46 programmers and achieve
67.2% accuracy [10]. We are able to handle orders of
magnitude more programmers (1,600) while using less
training data with 92.83% accuracy. Furthermore, the
features we are using are not trivial to obfuscate. We are
able to maintain high accuracy while using commercial
obfuscators. While abstract syntax trees can be obfus-
cated to an extent, doing so incurs signiﬁcant overhead
and maintenance costs.

Contributions. First, we use syntactic features for
code stylometry. Extracting such features requires pars-
ing of incomplete source code using a fuzzy parser to
generate an abstract syntax tree. These features add a
component to code stylometry that has so far remained
almost completely unexplored. We provide evidence that
these features are more fundamental and harder to ob-
fuscate. Our complete feature set consists of a compre-
hensive set of around 120,000 layout-based, lexical, and
syntactic features. With this complete feature set we are

USENIX Association  

24th USENIX Security Symposium  255

able to achieve a signiﬁcant increase in accuracy com-
pared to previous work. Second, we show that we can
scale our method to 1,600 programmers without losing
much accuracy. Third, this method is not speciﬁc to C or
C++, and can be applied to any programming language.
We collected C++ source of thousands of contestants
from the annual international competition “Google Code
Jam”. A bagging (portmanteau of “bootstrap aggregat-
ing”) classiﬁer - random forest was used to attribute pro-
grammers to source code. Our classiﬁers reach 98% ac-
curacy in a 250-class closed world task, 93% accuracy in
a 1,600-class closed world task, 100% accuracy on av-
erage in a two-class task. Finally, we analyze various
attributes of programmers, types of programming tasks,
and types of features that appear to inﬂuence the success
of attribution. We identiﬁed the most important 928 fea-
tures out of 120,000; 44% of them are syntactic, 1% are
layout-based and the rest of the features are lexical. 8
training ﬁles with an average of 70 lines of code is sufﬁ-
cient for training when using the lexical, layout and syn-
tactic features. We also observe that programmers with
a greater skill set are more easily identiﬁable compared
to less advanced programmers and that a programmer’s
coding style is more distinctive in implementations of
difﬁcult tasks as opposed to easier tasks.

The remainder of this paper is structured as follows.
We begin by introducing applications of source code au-
thorship attribution considered throughout this paper in
Section 2, and present our AST-based approach in Sec-
tion 3. We proceed to give a detailed overview of the ex-
periments conducted to evaluate our method in Section 4
and discuss the insights they provide in Section 5. Sec-
tion 6 presents related work, and Section 7 concludes.

2 Motivation

Throughout this work, we consider an analyst interested
in determining the programmer of an anonymous frag-
ment of source code purely based on its style. To do so,
the analyst only has access to labeled samples from a set
of candidate programmers, as well as from zero or more
unrelated programmers.

The analyst addresses this problem by converting each
labeled sample into a numerical feature vector, in order to
train a machine learning classiﬁer, that can subsequently
be used to determine the code’s programmer. In partic-
ular, this abstract problem formulation captures the fol-
lowing ﬁve settings and corresponding applications (see
Table 1). The experimental formulations are presented in
Section 4.2.

We emphasize that while these applications motivate
our work, we have not directly studied them. Rather, we
formulate them as variants of a machine-learning (classi-
ﬁcation) problem. Our data comes from the Google Code

Jam competition, as we discuss in Section 4.1. Doubt-
less there will be additional challenges in using our tech-
niques for digital forensics or any of the other real-world
applications. We describe some known limitations in
Section 5.

Programmer De-anonymization.

In this scenario,
the analyst is interested in determining the identity of an
anonymous programmer. For example, if she has a set of
programmers who she suspects might be Bitcoin’s cre-
ator, Satoshi, and samples of source code from each of
these programmers, she could use the initial versions of
Bitcoin’s source code to try to determine Satoshi’s iden-
tity. Of course, this assumes that Satoshi did not make
any attempts to obfuscate his or her coding style. Given a
set of probable programmers, this is considered a closed-
world machine learning task with multiple classes where
anonymous source code is attributed to a programmer.
This is a threat to privacy for open source contributors
who wish to remain anonymous.

Ghostwriting Detection. Ghostwriting detection is
related to but different from traditional plagiarism detec-
tion. We are given a suspicious piece of code and one or
more candidate pieces of code that the suspicious code
may have been plagiarized from. This is a well-studied
problem, typically solved using code similarity metrics,
as implemented by widely used tools such as MOSS [6],
JPlag [25], and Sherlock [24].

For example, a professor may want

to determine
whether a student’s programming assignment has been
written by a student who has previously taken the class.
Unfortunately, even though submissions of the previous
year are available, the assignments may have changed
considerably, rendering code-similarity based methods
ineffective. Luckily, stylometry can be applied in this
setting—we ﬁnd the most stylistically similar piece of
code from the previous year’s corpus and bring both stu-
dents in for gentle questioning. Given the limited set of
students, this can be considered a closed-world machine
learning problem.

Software Forensics. In software forensics, the analyst
assembles a set of candidate programmers based on pre-
viously collected malware samples or online code repos-
itories. Unfortunately, she cannot be sure that the anony-
mous programmer is one of the candidates, making this
an open world classiﬁcation problem as the test sample
might not belong to any known category.

Copyright Investigation. Theft of code often leads to
copyright disputes. Informal arrangements of hired pro-
gramming labor are very common, and in the absence of
a written contract, someone might claim a piece of code
was her own after it was developed for hire and delivered.
A dispute between two parties is thus a two-class classi-
ﬁcation problem; we assume that labeled code from both
programmers is available to the forensic expert.

256  24th USENIX Security Symposium 

USENIX Association

2

Authorship Veriﬁcation. Finally, we may suspect
that a piece of code was not written by the claimed pro-
grammer, but have no leads on who the actual program-
mer might be. This is the authorship veriﬁcation prob-
lem. In this work, we take the textbook approach and
model it as a two-class problem where positive examples
come from previous works of the claimed programmer
and negative examples come from randomly selected un-
related programmers. Alternatively, anomaly detection
could be employed in this setting, e.g., using a one-class
support vector machine [see 30].

As an example, a recent investigation conducted by
Verizon [17] on a US company’s anomalous virtual pri-
vate network trafﬁc, revealed an employee who was out-
sourcing her work to programmers in China.
In such
cases, training a classiﬁer on employee’s original code
and that of random programmers, and subsequently test-
ing pieces of recent code, could demonstrate if the em-
ployee was the actual programmer.

In each of these applications, the adversary may try to
actively modify the program’s coding style. In the soft-
ware forensics application, the adversary tries to modify
code written by her to hide her style. In the copyright and
authorship veriﬁcation applications, the adversary mod-
iﬁes code written by another programmer to match his
own style. Finally, in the ghostwriting application, two
of the parties may collaborate to modify the style of code
written by one to match the other’s style.

Application
De-anonymization
Ghostwriting detection
Software forensics
Copyright investigation
Authorship veriﬁcation

Learner
Multiclass
Multiclass
Multiclass
Two-class
Two/One-class

Comments
Closed world
Closed world
Open world
Closed world
Open world

Evaluation
Section 4.2.1
Section 4.2.1
Section 4.2.2
Section 4.2.3
Section 4.2.4

Table 1: Overview of Applications for Code Stylometry

We emphasize that code stylometry that is robust to
adversarial manipulation is largely left to future work.
However, we hope that our demonstration of the power
of features based on the abstract syntax tree will serve as
the starting point for such research.

3 De-anonymizing Programmers

One of the goals of our research is to create a classiﬁer
that automatically determines the most likely author of
a source ﬁle. Machine learning methods are an obvious
choice to tackle this problem, however, their success cru-
cially depends on the choice of a feature set that clearly
represents programming style. To this end, we begin by
parsing source code, thereby obtaining access to a wide
range of possible features that reﬂect programming lan-
guage use (Section 3.1). We then deﬁne a number of

different features to represent both syntax and structure
of program code (Section 3.2) and ﬁnally, we train a ran-
dom forest classiﬁer for classiﬁcation of previously un-
seen source ﬁles (Section 3.3). In the following sections,
we will discuss each of these steps in detail and outline
design decisions along the way. The code for our ap-
proach is made available as open-source to allow other
researchers to reproduce our results1.

3.1 Fuzzy Abstract Syntax Trees
To date, methods for source code authorship attribu-
tion focus mostly on sequential feature representations of
code such as byte-level and feature level n-grams [8, 13].
While these models are well suited to capture naming
conventions and preference of keywords, they are en-
tirely language agnostic and thus cannot model author
characteristics that become apparent only in the compo-
sition of language constructs. For example, an author’s
tendency to create deeply nested code, unusually long
functions or long chains of assignments cannot be mod-
eled using n-grams alone.

Addressing these limitations requires source code to
be parsed. Unfortunately, parsing C/C++ code using tra-
ditional compiler front-ends is only possible for complete
code, i.e., source code where all identiﬁers can be re-
solved. This severely limits their applicability in the set-
ting of authorship attribution as it prohibits analysis of
lone functions or code fragments, as is possible with sim-
ple n-gram models.

As a compromise, we employ the fuzzy parser Jo-
ern that has been designed speciﬁcally with incomplete
code in mind [32]. Where possible, the parser produces
abstract syntax trees for code fragments while ignoring
fragments that cannot be parsed without further infor-
mation. The produced syntax trees form the basis for
our feature extraction procedure. While they largely pre-
serve the information required to create n-grams or bag-
of-words representations, in addition, they allow a wealth
of features to be extracted that encode programmer habits
visible in the code’s structure.

As an example, consider the function foo as shown
in Figure 1, and a simpliﬁed version of its correspond-
ing abstract syntax tree in Figure 2. The function con-
tains a number of common language constructs found
in many programming languages, such as if-statements
(line 3 and 7), return-statements (line 4, 8 and 10), and
function call expressions (line 6). For each of these con-
structs, the abstract syntax tree contains a corresponding
node. While the leaves of the tree make classical syn-
tactic features such as keywords, identiﬁers and opera-
tors accessible, inner nodes represent operations showing

1https://github.com/calaylin/CodeStylometry

USENIX Association  

24th USENIX Security Symposium  257

3

Figure 1: Sample Code Listing

Figure 2: Corresponding Abstract Syntax Tree

how these basic elements are combined to form expres-
sions and statements. In effect, the nesting of language
constructs can also be analyzed to obtain a feature set
representing the code’s structure.

3.2 Feature Extraction
Analyzing coding style using machine learning ap-
proaches is not possible without a suitable representa-
tion of source code that clearly expresses program style.
To address this problem, we present the Code Stylome-
try Feature Set (CSFS), a novel representation of source
code developed speciﬁcally for code stylometry. Our fea-
ture set combines three types of features, namely lexical
features, layout features and syntactic features. Lexical
and layout features are obtained from source code while
the syntactic features can only be obtained from ASTs.
We now describe each of these feature types in detail.

3.2.1 Lexical and Layout Features

We begin by extracting numerical features from the
source code that express preferences for certain identi-
ﬁers and keywords, as well as some statistics on the use
of functions or the nesting depth. Lexical and layout fea-
tures can be calculated from the source code, without
having access to a parser, with basic knowledge of the
programming language in use. For example, we mea-
sure the number of functions per source line to determine
the programmer’s preference of longer over shorter func-
tions. Furthermore, we tokenize the source ﬁle to obtain
the number of occurrences of each token, so called word
unigrams. Table 2 gives an overview of lexical features.
In addition, we consider layout features that represent
code-indentation. For example, we determine whether
the majority of indented lines begin with whitespace
or tabulator characters, and we determine the ratio of
whitespace to the ﬁle size. Table 3 gives a detailed de-
scription of these features.

Feature
WordUnigramTF

ln(numkeyword/
length)

ln(numTernary/
length)
ln(numTokens/
length)
ln(numComments/
length)
ln(numLiterals/
length)

ln(numKeywords/
length)
ln(numFunctions/
length)
ln(numMacros/
length)
nestingDepth

branchingFactor

avgParams

Deﬁnition
Term frequency of word unigrams in
source code
Log of the number of occurrences of key-
word divided by ﬁle length in characters,
where keyword is one of do, else-if, if, else,
switch, for or while
Log of the number of ternary operators di-
vided by ﬁle length in characters
Log of the number of word tokens divided
by ﬁle length in characters
Log of the number of comments divided by
ﬁle length in characters
Log of the number of string, character, and
numeric literals divided by ﬁle length in
characters
Log of the number of unique keywords
used divided by ﬁle length in characters
Log of the number of functions divided by
ﬁle length in characters
Log of the number of preprocessor direc-
tives divided by ﬁle length in characters
Highest degree to which control statements
and loops are nested within each other
Branching factor of the tree formed by con-
verting code blocks of ﬁles into nodes
The average number of parameters among
all functions

stdDevNumParams The standard deviation of the number of

avgLineLength
stdDevLineLength The standard deviation of the character

parameters among all functions
The average length of each line

lengths of each line

Count
dynamic*

7

1

1

1

1

1

1

1

1

1

1

1

1
1

*About 55,000 for 250 authors with 9 ﬁles.

Table 2: Lexical Features

3.2.2 Syntactic Features

The syntactic feature set describes the properties of the
language dependent abstract syntax tree, and keywords.
Calculating these features requires access to an abstract
syntax tree. All of these features are invariant to changes
in source-code layout, as well as comments.

Table 4 gives an overview of our syntactic features.
We obtain these features by preprocessing all C++ source
ﬁles in the dataset to produce their abstract syntax trees.

258  24th USENIX Security Symposium 

USENIX Association

4

FunctionintfooCompoundStmtIfIfDeclConditionReturnConditionReturnElseOrUnaryOp (-)intretAssign(=)EqExpr (!=)UnaryOp (-)ReturnRelExpr (<)RelExpr (>)1retCallret011x0xMAXbarArgsxFeature
ln(numTabs/length)

ln(numSpaces/length)

ln(numEmptyLines/
length)

whiteSpaceRatio

newLineBefore
OpenBrace

tabsLeadLines

Deﬁnition
Log of the number of tab characters di-
vided by ﬁle length in characters
Log of the number of space characters di-
vided by ﬁle length in characters
Log of the number of empty lines divided
by ﬁle length in characters, excluding
leading and trailing lines between lines of
text
The ratio between the number of whites-
pace characters (spaces, tabs, and new-
lines) and non-whitespace characters
A boolean representing whether the ma-
jority of code-block braces are preceded
by a newline character
A boolean representing whether the ma-
jority of indented lines begin with spaces
or tabs

Count
1

1

1

1

1

1

Table 3: Layout Features

An abstract syntax tree is created for each function in the
code. There are 58 node types in the abstract syntax tree
(see Appendix A) produced by Joern [33].

Feature
MaxDepthASTNode
ASTNodeBigramsTF
ASTNodeTypesTF

Deﬁnition
Maximum depth of an AST node
Term frequency AST node bigrams
Term frequency of 58 possible AST
node type excluding leaves

Count
1
dynamic*
58

ASTNodeTypesTFIDF Term frequency inverse document fre-
quency of 58 possible AST node type
excluding leaves

ASTNodeTypeAvgDep Average depth of 58 possible AST

58

58

84
dynamic**

dynamic**

dynamic**

cppKeywords
CodeInASTLeavesTF

CodeInASTLeaves
TFIDF

node types excluding leaves
Term frequency of 84 C++ keywords
Term frequency of code unigrams in
AST leaves
Term frequency inverse document fre-
quency of code unigrams in AST
leaves
Average depth of code unigrams in
AST leaves

CodeInASTLeaves
AvgDep
*About 45,000 for 250 authors with 9 ﬁles.
**About 7,000 for 250 authors with 9 ﬁles.
**About 4,000 for 150 authors with 6 ﬁles.
**About 2,000 for 25 authors with 9 ﬁles.

Table 4: Syntactic Features

The AST node bigrams are the most discriminating
features of all. AST node bigrams are two AST nodes
that are connected to each other. In most cases, when
used alone, they provide similar classiﬁcation results to
using the entire feature set.

The term frequency (TF) is the raw frequency of a
node found in the abstract syntax trees for each ﬁle. The
term frequency inverse document frequency (TFIDF) of
nodes is calculated by multiplying the term frequency of
a node by inverse document frequency. The goal in using
the inverse document frequency is normalizing the term
frequency by the number of authors actually using that

particular type of node. The inverse document frequency
is calculated by dividing the number of authors in the
dataset by the number of authors that use that particular
node. Consequently, we are able to capture how rare of a
node it is and weight it more according to its rarity.

The maximum depth of an abstract syntax tree re-
ﬂects the deepest level a programmer nests a node in the
solution. The average depth of the AST nodes shows
how nested or deep a programmer tends to use particular
structural pieces. And lastly, term frequency of each C++
keyword is calculated. Each of these features is written
to a feature vector to represent the solution ﬁle of a spe-
ciﬁc author and these vectors are later used in training
and testing by machine learning classiﬁers.

3.3 Classiﬁcation
Using the feature set presented in the previous section,
we can now express fragments of source code as numeri-
cal vectors, making them accessible to machine learning
algorithms. We proceed to perform feature selection and
train a random forest classiﬁer capable of identifying the
most likely author of a code fragment.

3.3.1 Feature Selection
Due to our heavy use of unigram term frequency and
TF/IDF measures, and the diversity of individual terms
in the code, our resulting feature vectors are extremely
large and sparse, consisting of tens of thousands of fea-
tures for hundreds of classes. The dynamic Code stylom-
etry feature set, for example, produced close to 120,000
features for 250 authors with 9 solution ﬁles each.

In many cases, such feature vectors can lead to over-
ﬁtting (where a rare term, by chance, uniquely identiﬁes
a particular author). Extremely sparse feature vectors
can also damage the accuracy of random forest classi-
ﬁers, as the sparsity may result in large numbers of zero-
valued features being selected during the random sub-
sampling of the features to select a best split. This re-
duces the number of ‘useful’ splits that can be obtained
at any given node, leading to poorer ﬁts and larger trees.
Large, sparse feature vectors can also lead to slowdowns
in model ﬁtting and evaluation, and are often more difﬁ-
cult to interpret. By selecting a smaller number of more
informative features, the sparsity in the feature vector can
be greatly reduced, thus allowing the classiﬁer to both
produce more accurate results and ﬁt the data faster.

We therefore employed a feature selection step using
WEKA’s information gain [26] criterion, which evaluates
the difference between the entropy of the distribution of
classes and the entropy of the conditional distribution of
classes given a particular feature:

IG(A,Mi) =H (A)− H(A|Mi)

(1)

5

USENIX Association  

24th USENIX Security Symposium  259

where A is the class corresponding to an author, H is
Shannon entropy, and Mi is the ith feature of the dataset.
Intuitively, the information gain can be thought of as
measuring the amount of information that the observa-
tion of the value of feature i gives about the class label
associated with the example.

To reduce the total size and sparsity of the feature vec-
tor, we retained only those features that individually had
non-zero information gain.
(These features can be re-
ferred to as IG-CSFS throughout the rest of the paper.)
Note that, as H(A|Mi) ≤ H(A), information gain is al-
ways non-negative. While the use of information gain
on a variable-per-variable basis implicitly assumes inde-
pendence between the features with respect to their im-
pact on the class label, this conservative approach to fea-
ture selection means that we only use features that have
demonstrable value in classiﬁcation.

To validate this approach to feature selection, we ap-
plied this method to two distinct sets of source code ﬁles,
and observed that sets of features with non-zero informa-
tion gain were nearly identical between the two sets, and
the ranking of features was substantially similar between
the two. This suggests that the application of information
gain to feature selection is producing a robust and con-
sistent set of features (see Section 4 for further discus-
sion). All the results are calculated by using CSFS and
IG-CSFS. Using IG-CSFS on all experiments demon-
strates how these features generalize to different datasets
that are larger in magnitude. One other advantage of IG-
CSFS is that it consists of a few hundred features that
result in non-sparse feature vectors. Such a compact rep-
resentation of coding style makes de-anonymizing thou-
sands of programmers possible in minutes.

3.3.2 Random Forest Classiﬁcation

We used the random forest ensemble classiﬁer [7] as
our classiﬁer for authorship attribution. Random forests
are ensemble learners built from collections of decision
trees, each of which is grown by randomly sampling
N training samples with replacement, where N is the
number of instances in the dataset. To reduce correla-
tion between trees, features are also subsampled; com-
monly (logM) + 1 features are selected at random (with-
out replacement) out of M, and the best split on these
(logM) +1 features is used to split the tree nodes. The
number of selected features represents one of the few
tuning parameters in random forests: increasing the num-
ber of features increases the correlation between trees in
the forest which can harm the accuracy of the overall en-
semble, however increasing the number of features that
can be chosen at each split increases the classiﬁcation ac-
curacy of each individual tree making them stronger clas-
siﬁers with low error rates. The optimal range of number

of features can be found using the out of bag (oob) error
estimate, or the error estimate derived from those sam-
ples not selected for training on a given tree.

During classiﬁcation, each test example is classiﬁed
via each of the trained decision trees by following the bi-
nary decisions made at each node until a leaf is reached,
and the results are then aggregated. The most populous
class can be selected as the output of the forest for simple
classiﬁcation, or classiﬁcations can be ranked according
to the number of trees that ‘voted’ for a label when per-
forming relaxed attribution (see Section 4.3.4).

We employed random forests with 300 trees, which
empirically provided the best trade-off between accuracy
and processing time. Examination of numerous oob val-
ues across multiple ﬁts suggested that (logM) + 1 ran-
dom features (where M denotes the total number of fea-
tures) at each split of the decision trees was in fact op-
timal in all of the experiments (listed in Section 4), and
was used throughout. Node splits were selected based on
the information gain criteria, and all trees were grown to
the largest extent possible, without pruning.

The data was analyzed via k-fold cross-validation,
where the data was split into training and test sets strat-
iﬁed by author (ensuring that the number of code sam-
ples per author in the training and test sets was identi-
cal across authors). k varies according to datasets and
is equal to the number of instances present from each
author. The cross-validation procedure was repeated 10
times, each with a different random seed. We report the
average results across all iterations in the results, ensur-
ing that they are not biased by improbably easy or difﬁ-
cult to classify subsets.

4 Evaluation

In the evaluation section, we present the results to the
possible scenarios formulated in the problem statement
and evaluate our method. The corpus section gives an
overview of the data we collected. Then, we present the
main results to programmer de-anonymization and how
it scales to 1,600 programmers, which is an immediate
privacy concern for open source contributors that prefer
to remain anonymous. We then present the training data
requirements and efﬁcacy of types of features. The ob-
fuscation section discusses a possible countermeasure to
programmer de-anonymization. We then present possi-
ble machine learning formulations along with the veriﬁ-
cation section that extends the approach to an open world
problem. We conclude the evaluation with generalizing
the method to other programming languages and provid-
ing software engineering insights.

260  24th USENIX Security Symposium 

USENIX Association

6

4.1 Corpus
One concern in source code authorship attribution is that
we are actually identifying differences in coding style,
rather than merely differences in functionality. Consider
the case where Alice and Bob collaborate on an open
source project. Bob writes user interface code whereas
Alice works on the network interface and backend ana-
lytics. If we used a dataset derived from their project,
we might differentiate differences between frontend and
backend code rather than differences in style.

In order to minimize these effects, we evaluate our
method on the source code of solutions to programming
tasks from the international programming competition
Google Code Jam (GCJ), made public in 2008 [2]. The
competition consists of algorithmic problems that need
to be solved in a programming language of choice. In
particular, this means that all programmers solve the
same problems, and hence implement similar functional-
ity, a property of the dataset crucial for code stylometry
analysis.

The dataset contains solutions by professional pro-
grammers, students, academics, and hobbyists from 166
countries. Participation statistics are similar over the
years. Moreover, it contains problems of different dif-
ﬁculty, as the contest takes place in several rounds. This
allows us to assess whether coding style is related to pro-
grammer experience and problem difﬁculty.

The most commonly used programming language was
C++, followed by Java, and Python. We chose to inves-
tigate source code stylometry on C++ and C because of
their popularity in the competition and having a parser
for C/C++ readily available [32]. We also conducted
some preliminary experimentation on Python.

A validation dataset was created from 2012’s GCJ
competition. Some problems had two stages, where the
second stage involved answering the same problem in a
limited amount of time and for a larger input. The so-
lution to the large input is essentially a solution for the
small input but not vice versa. Therefore, collecting both
of these solutions could result in duplicate and identical
source code. In order to avoid multiple entries, we only
collected the small input versions’ solutions to be used in
our dataset.

The programmers had up to 19 solution ﬁles in these
datasets. Solution ﬁles have an average of 70 lines of
code per programmer.

To create our experimental datasets that are discussed

in further detail in the results section;
(i) We ﬁrst partitioned the corpus of ﬁles by year of com-
petition. The “main” dataset includes ﬁles drawn from
2014 (250 programmers). The “validation” dataset ﬁles
come from 2012, and the “multi-year” dataset ﬁles come
from years 2008 through 2014 (1,600 programmers).

(ii) Within each year, we ordered the corpus ﬁles by the
round in which they were written, and by the problem
within a round, as all competitors proceed through the
same sequence of rounds in that year. As a result, we
performed stratiﬁed cross validation on each program ﬁle
by the year it was written, by the round in which the pro-
gram was written, by the problems solved in the round,
and by the author’s highest round completed in that year.
Some limitations of this dataset are that it does not al-
low us to assess the effect of style guidelines that may
be imposed on a project or attributing code with mul-
tiple/mixed programmers. We leave these interesting
questions for future work, but posit that our improved re-
sults with basic stylometry make them worthy of study.

4.2 Applications
In this section, we will go over machine learning task
formulations representing ﬁve possible real-world appli-
cations presented in Section 2.

4.2.1 Multiclass Closed World Task

This
section presents our main experiment—de-
anonymizing 250 programmers in the difﬁcult scenario
where all programmers solved the same set of prob-
lems.
The machine learning task formulation for
de-anonymizing programmers also applies to ghostwrit-
ing detection. The biggest dataset formed from 2014’s
Google Code Jam Competition with 9 solution ﬁles to
the same problem had 250 programmers. These were the
easiest set of 9 problems, making the classiﬁcation more
challenging (see Section 4.3.6). We reached 91.78%
accuracy in classifying 250 programmers with the Code
Stylometry Feature Set. After applying information gain
and using the features that had information gain, the
accuracy was 95.08%.

We also took 250 programmers from different years
and randomly selected 9 solution ﬁles for each one of
them. We used the information gain features obtained
from 2014’s dataset to see how well they generalize.
We reached 98.04% accuracy in classifying 250 pro-
grammers. This is 3% higher than the controlled large
dataset’s results. The accuracy might be increasing be-
cause of using a mixed set of Google Code Jam prob-
lems, which potentially contains the possible solutions’
properties along with programmers’ coding style and
makes the code more distinct.

We wanted to evaluate our approach and validate our
method and important features. We created a dataset
from 2012’s Google Code Jam Competition with 250
programmers who had the solutions to the same set of
9 problems. We extracted only the features that had pos-
itive information gain in 2014’s dataset that was used as

USENIX Association  

24th USENIX Security Symposium  261

7

the main dataset to implement the approach. The classi-
ﬁcation accuracy was 96.83%, which is higher than the
95.07% accuracy obtained in 2014’s dataset.

The high accuracy of validation results in Table 5 show
that we identiﬁed the important features of code stylom-
etry and found a stable feature set. This feature set does
not necessarily represent the exact features for all pos-
sible datasets. For a given dataset that has ground truth
information on authorship, following the same approach
should generate the most important features that repre-
sent coding style in that particular dataset.

A = #programmers, F = max #problems completed

N = #problems included in dataset (N ≤ F)

A = 250 all years
F ≥ 9 all years

N = 9

A = 250 from 2014
F = 9 from 2014

A = 250 from 2012
F = 9 from 2014

N = 9

N = 9

Average accuracy after 10 iterations with IG-CSFS features

95.07%

96.83%

98.04%

Table 5: Validation Experiments

4.2.2 Mutliclass Open World Task

The experiments in this section can be used in software
forensics to ﬁnd out the programmer of a piece of mal-
ware. In software forensics, the analyst does not know if
source code belongs to one of the programmers in the
candidate set of programmers.
In such cases, we can
classify the anonymous source code, and if the majority
number of votes of trees in the random forest is below a
certain threshold, we can reject the classiﬁcation consid-
ering the possibility that it might not belong to any of the
classes in the training data. By doing so, we can scale
our approach to an open world scenario, where we might
not have encountered the suspect before. As long as we
determine a conﬁdence threshold based on training data
[30], we can calculate the probability that an instance
belongs to one of the programmers in the set and accord-
ingly accept or reject the classiﬁcation.

We performed 270 classiﬁcations in a 30-class prob-
lem using all the features to determine the conﬁdence
threshold based on the training data. The accuracy was
96.67%. There were 9 misclassiﬁcations and all of them
were classiﬁed with less than 15% conﬁdence by the
classiﬁer. The class probability or classiﬁcation conﬁ-
dence that source code fragment C is of class i is cal-
culated by taking the percentage of trees in the random
forest that voted for that particular class, as follows2:

P(Ci) =

∑ j Vj(i)
|T| f

(2)

8

Where Vj(i) =1 if the jth tree voted for class i and
0 otherwise, and |T| f denotes the total number of trees
in forest f . Note that by construction, ∑i P(Ci) =1 and
P(Ci) ≥ 0 ∀ i, allowing us to treat P(Ci) as a probability
measure.
There was one correct classiﬁcation made with 13.7%
conﬁdence. This suggests that we can use a threshold be-
tween 13.7% and 15% conﬁdence level for veriﬁcation,
and manually analyze the classiﬁcations that did not pass
the conﬁdence threshold or exclude them from results.

We picked an aggressive threshold of 15% and to vali-
date it, we trained a random forest classiﬁer on the same
set of 30 programmers 270 code samples. We tested on
150 different ﬁles from the programmers in the training
set. There were 6 classiﬁcations below the 15% threshold
and two of them were misclassiﬁed. We took another set
of 420 test ﬁles from 30 programmers that were not in the
training set. All the ﬁles from the 30 programmers were
attributed to one of the 30 programmers in the training
set since this is a closed world classiﬁcation task, how-
ever, the highest conﬁdence level in these classiﬁcations
was 14.7%. The 15% threshold catches all the instances
that do not belong to the programmers in the suspect set,
gets rid of 2 misclassiﬁcations and 4 correct classiﬁca-
tions. Consequently, when we see a classiﬁcation with
less than a threshold value, we can reject the classiﬁca-
tion and attribute the test instance to an unknown suspect.

4.2.3 Two-class Closed World Task

Source code author identiﬁcation could automatically
deal with source code copyright disputes without requir-
ing manual analysis by an objective code investigator.
A copyright dispute on code ownership can be resolved
by comparing the styles of both parties claiming to have
generated the code. The style of the disputed code can
be compared to both parties’ other source code to aid in
the investigation. To imitate such a scenario, we took
60 different pairs of programmers, each with 9 solution
ﬁles. We used a random forest and 9-fold cross validation
to classify two programmers’ source code. The average
classiﬁcation accuracy using CSFS set is 100.00% and
100.00% with the information gain features.

4.2.4 Two-class/One-class Open World Task

Another two-class machine learning task can be formu-
lated for authorship veriﬁcation. We suspect Mallory of
plagiarizing, so we mix in some code of hers with a large
sample of other people, test, and see if the disputed code
gets classiﬁed as hers or someone else’s. If it gets clas-
siﬁed as hers, then it was with high probability really
written by her.
If it is classiﬁed as someone else’s, it
really was someone else’s code. This could be an open

262  24th USENIX Security Symposium 

USENIX Association

USENIX Association  

24th USENIX Security Symposium  263

worldproblemandthepersonthatoriginallywrotethecodecouldbeapreviouslyunknownprogrammer.Thisisatwo-classproblemwithclassesMalloryandothers.WetrainonMallory’ssolutionstoproblemsa,b,c,d,e,f,g,h.WealsotrainonprogrammerA’ssolu-tiontoproblema,programmerB’ssolutiontoproblemb,programmerC’ssolutiontoproblemc,programmerD’ssolutiontoproblemd,programmerE’ssolutiontoprob-leme,programmerF’ssolutiontoproblemf,program-merG’ssolutiontoproblemg,programmerH’ssolutiontoproblemhandputtheminoneclasscalledABCDE-FGH.Wetrainarandomforestclassiﬁerwith300treesonclassesMalloryandABCDEFGH.Wehave6testin-stancesfromMalloryand6testinstancesfromanotherprogrammerZZZZZZ,whoisnotinthetrainingset.Theseexperimentshavebeenrepeatedintheex-actsamesettingwith80differentsetsofprogrammersABCDEFGH,ZZZZZZandMallorys.Theaverageclas-siﬁcationaccuracyforMalloryusingtheCSFSsetis100.00%.ZZZZZZ’stestinstancesareclassiﬁedaspro-grammerABCDEFGH82.04%ofthetime,andclassi-ﬁedasMalloryfortherestofthetimewhileusingtheCSFS.Dependingontheamountoffalsepositiveswearewillingtoaccept,wecanchangetheoperatingpointontheROCcurve.Theseresultsarealsopromisingforuseincaseswhereapieceofcodeissuspectedtobeplagiarized.Followingthesameapproach,iftheclassiﬁcationresultofthepieceofcodeissomeoneotherthanMallory,thatpieceofcodewaswithveryhighprobabilitynotwrittenbyMallory.4.3AdditionalInsights4.3.1ScalingWecollectedalargerdatasetof1,600programmersfromvariousyears.Eachoftheprogrammershad9sourcecodesamples.Wecreated7subsetsofthislargedatasetindifferingsizes,with250,500,750,1,000,1,250,1,500,and1,600programmers.Thesesubsetsareuse-fultounderstandhowwellourapproachscales.Weex-tractedthespeciﬁcfeaturesthathadinformationgaininthemain250programmerdatasetfromthislargedataset.Intheory,weneedtousemoretreesintherandomfor-estasthenumberofclassesincreasetodecreasevari-ance,butweusedfewertreescomparedtosmallerex-periments.Weused300treesintherandomforesttoruntheexperimentsinareasonableamountoftimewithareasonableamountofmemory.Theaccuracydidnotdecreasetoomuchwhenincreasingthenumberofpro-grammers.Thisresultshowsthatinformationgainfea-turesarerobustagainstchangesinclassandareim-portantpropertiesofprogrammers’codingstyles.ThefollowingFigure3demonstrateshowwellourmethodscales.Weareabletode-anonymize1,600programmersusing32GBmemorywithinonehour.Alternately,wecanuse40treesandgetnearlythesameaccuracy(within0.5%)inafewminutes.Figure3:LargeScaleDe-anonymization4.3.2TrainingDataandFeaturesWeselecteddifferentsetsof62programmersthathadFsolutionﬁles,from2upto14.Eachdatasethastheso-lutionstothesamesetofFproblemsbydifferentsetsofprogrammers.EachdatasetconsistedofprogrammersthatwereabletosolveexactlyFproblems.Suchanex-perimentalsetupmakesitpossibletoinvestigatetheef-fectofprogrammerskillsetoncodingstyle.Thesizeofthedatasetswerelimitedto62,becausetherewereonly62contestantswith14ﬁles.Therewereafewcontes-tantswithupto19ﬁlesbutwehadtoexcludethemsincetherewerenotenoughprogrammerstocomparethem.ThesamesetofFproblemswereusedtoensurethatthecodingstyleoftheprogrammerisbeingclassiﬁedandnotthepropertiesofpossiblesolutionsoftheprob-lemitself.Wewereabletocapturepersonalprogram-mingstylesincealltheprogrammersarecodingthesamefunctionalityintheirownways.StratiﬁedF-foldcrossvalidationwasusedbytrainingoneveryone’s(F−1)solutionsandtestingontheFthproblemthatdidnotappearinthetrainingset.Asare-sult,theproblemsinthetestﬁleswereencounteredfortheﬁrsttimebytheclassiﬁer.Weusedarandomforestwith300treesand(logM)+1featureswithF-foldstratiﬁedcrossvalidation,ﬁrstwiththeCodeStylometryFeatureSet(CSFS)andthenwiththeCSFS’sfeaturesthathadinformationgain.Figure4showstheaccuracyfrom13differentsetsof62programmerswith2to14solutionﬁles,andconse-quently1to13trainingﬁles.TheCSFSreachesanopti-maltrainingsetsizeat9solutionﬁles,wheretheclassi-ﬁertrainson8(F−1)solutions.Inthedatasetsweconstructed,asthenumberofﬁlesincreaseandproblemsfrommoreadvancedroundsareincluded,theaveragelineofcode(LOC)perﬁlealsoincreases.Theaveragelinesofcodepersourcecodeinthedatasetis70.Increasednumberoflinesofcodemighthaveapositiveeffectontheaccuracybutatthesametimeitrevealsprogrammer’schoiceofprogram9264  24th USENIX Security Symposium 

USENIX Association

Figure4:TrainingDatalengthinimplementingthesamefunctionality.Ontheotherhand,theaveragelineofcodeofthe7easier(76LOC)ordifﬁcultproblems(83LOC)takenfromcontes-tantsthatwereabletocomplete14problems,ishigherthantheaveragelineofcode(68)ofcontestantsthatwereabletosolveonly7problems.ThisshowsthatprogrammerswithbetterskillstendtowritelongercodetosolveGoogleCodeJamproblems.Themainstreamideaisthatbetterprogrammerswriteshorterandcleanercodewhichcontradictswithlineofcodestatisticsinourdatasets.GoogleCodeJamcontestantsaresupposedtooptimizetheircodetoprocesslargeinputswithfasterperformance.Thisimplementationstrategymightbeleadingtoadvancedprogrammersimplementinglongersolutionsforthesakeofoptimization.Wetookthedatasetwith62programmerseachwith9solutions.Weget97.67%accuracywithallthefea-turesand99.28%accuracywiththeinformationgainfea-tures.Weexcludedallthesyntacticfeaturesandtheac-curacydroppedto88.89%withallthenon-syntacticfea-turesand88.35%withtheinformationgainfeaturesofthenon-syntacticfeatureset.Werananotherexperimentusingonlythesyntacticfeaturesandobtained96.06%withallthesyntacticfeaturesand96.96%withtheinfor-mationgainfeaturesofthesyntacticfeatureset.Mostoftheclassiﬁcationpowerispreservedwiththesyntac-ticfeatures,andusingnon-syntacticfeaturesleadstoasigniﬁcantdeclineinaccuracy.4.3.3ObfuscationWetookadatasetwith9solutionﬁlesand20program-mersandobfuscatedthecodeusinganoff-the-shelfC++obfuscatorcalledstunnix[3].Theaccuracywiththein-formationgaincodestylometryfeaturesetontheob-fuscateddatasetis98.89%.Theaccuracyonthesamedatasetwhenthecodeisnotobfuscatedis100.00%.Theobfuscatorrefactoredfunctionandvariablenames,aswellascomments,andstrippedallthespaces,preserv-ingthefunctionalityofcodewithoutchangingthestruc-tureoftheprogram.Obfuscatingthedataproducedlittledetectablechangeintheperformanceoftheclassiﬁerforthissample.TheresultsaresummarizedinTable6.Wetookthemaximumnumberofprogrammers,20,thathadsolutionsto9problemsinCandobfuscatedthecode(seeexampleinAppendixB)usingamuchmoresophisticatedopensourceobfuscatorcalledTigress[1].Inparticular,Tigressimplementsfunctionvirtualiza-tion,anobfuscationtechniquethatturnsfunctionsintointerpretersandconvertstheoriginalprogramintocor-respondingbytecode.Afterapplyingfunctionvirtual-ization,wewerelessabletoeffectivelyde-anonymizeprogrammers,soithaspotentialasacountermeasuretoprogrammerde-anonymization.However,thisobfusca-tioncomesatacost.Firstofall,theobfuscatedcodeisneitherreadablenormaintainable,andisthusunsuitableforanopensourceproject.Second,theobfuscationaddssigniﬁcantoverhead(9timesslower)totheruntimeoftheprogram,whichisanotherdisadvantage.Theaccuracywiththeinformationgainfeaturesetontheobfuscateddatasetisreducedto67.22%.WhenwelimitthefeaturesettoASTnodebigrams,weget18.89%accuracy,whichdemonstratestheneedforallfeaturetypesincertainscenarios.Theaccuracyonthesamedatasetwhenthecodeisnotobfuscatedis95.91%.ObfuscatorProgrammersLangResultsw/oObfuscationResultsw/ObfuscationStunnix20C++98.89%100.00%Stunnix20C++98.89*%98.89*%Tigress20C93.65%58.33%Tigress20C95.91*%67.22*%*InformationgainfeaturesTable6:EffectofObfuscationonDe-anonymization4.3.4RelaxedClassiﬁcationThegoalhereistodeterminewhetheritispossibletore-ducethenumberofsuspectsusingcodestylometry.Re-ducingthesetofsuspectsinchallengingcases,suchashavingtoomanysuspects,wouldreducetheeffortre-quiredtomanuallyﬁndtheactualprogrammerofthecode.Inthissection,weperformedclassiﬁcationonthemain250programmerdatasetfrom2014usingthein-formationgainfeatures.TheclassiﬁcationwasrelaxedtoasetoftopRsuspectsinsteadofexactclassiﬁcationoftheprogrammer.TherelaxedfactorRvariedfrom1to10.Insteadoftakingthehighestmajorityvoteofthedecisionstreesintherandomforest,thehighestRmajor-ityvotedecisionsweretakenandtheclassiﬁcationresultwasconsideredcorrectiftheprogrammerwasinthesetoftopRhighestvotedclasses.Theaccuracydoesnotimprovemuchaftertherelaxedfactorislargerthan5.10USENIX Association  

24th USENIX Security Symposium  265

Figure5:RelaxedClassiﬁcationwith250Programmers4.3.5GeneralizingtheMethodFeaturesderivedfromASTscanrepresentcodingstylesinvariouslanguages.Thesefeaturesareapplicableincaseswhenlexicalandlayoutfeaturesmaybelessdis-criminatingduetoformattingstandardsandrelianceonwhitespaceandother‘lexical’featuresassyntax,suchasPython’sPEP8formatting.Toshowthatourmethodgeneralizes,wecollectedsourcecodeof229Pythonpro-grammersfromGCJ’s2014competition.229program-mershadexactly9solutions.UsingonlythePythonequivalentsofsyntacticfeatureslistedinTable4and9-foldcross-validation,theaverageaccuracyis53.91%fortop-1classiﬁcation,75.69%fortop-5relaxedattri-bution.Thelargestsetofprogrammerstoallworkonthesamesetof9problemswas23programmers.Theaverageaccuracyinidentifyingthese23programmersis87.93%fortop-1and99.52%fortop-5relaxedattribu-tion.ThesameclassiﬁcationtasksusingtheinformationgainfeaturesarealsolistedinTable7.Theoverallac-curacyindatasetscomposedofPythoncodearelowerthanC++datasets.InPythondatasets,weonlyusedsyntacticfeaturesfromASTsthatweregeneratedbyaparserthatwasnotfuzzy.Thelackofquantityandspeci-ﬁcityoffeaturesaccountsforthedecreasedaccuracy.ThePythondataset’sinformationgainfeaturesaresig-niﬁcantlyfewerinquantity,comparedtoC++dataset’sinformationgainfeatures.Informationgainonlykeepsfeaturesthathavediscriminativevalueallontheirown.Iftwofeaturesonlyprovidediscriminativevaluewhenusedtogether,theninformationgainwilldiscardthem.SoifalotofthefeaturesforthePythonsetareonlyjointlydiscriminative(andnotindividuallydiscrimina-tive),thentheinformationgaincriteriamayberemovingfeaturesthatincombinationcouldeffectivelydiscrimi-natebetweenauthors.Thismightaccountforthede-creasewhenusinginformationgainfeatures.WhileinthecontextofotherresultsinthispapertheresultsinTa-ble7appearlackluster,itisworthnotingthateventhispreliminarytestusingonlysyntacticfeatureshascompa-rableperformancetootherpriorworkatasimilarscale(seeSection6andTable9),demonstratingtheutilityofsyntacticfeaturesandtherelativeeaseofgeneratingthemfornovelprogramminglanguages.Nevertheless,aCSFSequivalentfeaturesetcanbegeneratedforotherprogramminglanguagesbyimplementingthelayoutandlexicalfeaturesaswellasusingafuzzyparser.Lang.ProgrammersClassiﬁcationIGTop-5Top-5IGPython2387.93%79.71%99.52%96.62Python22953.91%39.16%75.69%55.46Table7:GeneralizingtoOtherProgrammingLanguages4.3.6SoftwareEngineeringInsightsWewantedtoinvestigateifprogrammingstyleisconsis-tentthroughoutyears.Wefoundthecontestantsthathadthesameusernameandcountryinformationbothin2012and2014.Weassumedthatthesearethesamepeoplebutthereisachancethattheymightbedifferentpeople.In2014,someoneelsemighthavepickedupthesameuser-namefromthesamecountryandstartedusingit.Wearegoingtoignoresuchagroundtruthproblemfornowandassumethattheyarethesamepeople.Wetookasetof25programmersfrom2012thatwerealsocontestantsin2014’scompetition.Wetook8ﬁlesfromtheirsubmissionsin2012andtrainedarandomfor-estclassiﬁerwith300treesusingCSFS.Wehadonein-stancefromeachoneofthecontestantsfrom2014.Thecorrectclassiﬁcationofthesetestinstancesfrom2014is96.00%.Theaccuracydroppedto92.00%whenusingonlyinformationgainfeatures,whichmightbeduetotheaggressiveeliminationofpairsoffeaturesthatarejointlydiscriminative.These25programmers’9ﬁlesfrom2014hadacorrectclassiﬁcationaccuracyof98.04%.Theseresultsindicatethatcodingstyleispreserveduptosomedegreethroughoutyears.Toinvestigateproblemdifﬁculty’seffectoncodingstyle,wecreatedtwodatasetsfrom62programmersthathadexactly14solutionﬁles.Table8summarizesthefollowingresults.Adatasetwith7oftheeasierprob-lemsoutof14resultedin95.62%accuracy.Adatasetwith7ofthemoredifﬁcultproblemsoutof14resultedin99.31%accuracy.Thismightimplythatmoredifﬁcultcodingtaskshaveamoreprevalentreﬂectionofcodingstyle.Ontheotherhand,thedatasetthathad62pro-grammerswithexactly7oftheeasierproblemsresultedin91.24%accuracy,whichisalotlowerthantheaccu-racyobtainedfromthedatasetwhoseprogrammerswereabletoadvancetosolve14problems.Thismightindi-catethat,programmerswhoareadvancedenoughtoan-swer14problemslikelyhavemoreuniquecodingstylescomparedtocontestantsthatwereonlyabletosolvetheﬁrst7problems.Toinvestigatethepossibilitythatcontestantswhoareabletoadvancefurtherintheroundshavemoreuniquecodingstyles,weperformedasecondroundofexperi-mentsoncomparabledatasets.Wetookthedatasetwith1112 solution ﬁles and 62 programmers. A dataset with 6
of the easier problems out of 12 resulted in 91.39% ac-
curacy. A dataset with 6 of the more difﬁcult problems
out of 12 resulted in 94.35% accuracy. These results are
higher than the dataset whose programmers were only
able to solve the easier 6 problems. The dataset that had
62 programmers with exactly 6 of the easier problems
resulted in 90.05% accuracy.

A = #programmers, F = max #problems completed

N = #problems included in dataset (N ≤ F)

A = 62

F = 14

N = 7

N = 7

F = 7
N = 7

F = 12

N = 6

N = 6

F = 6
N = 6

Average accuracy after 10 iterations while using CSFS

99.31%

95.62%2

91.24%1

94.35%

91.39%2

90.05%1

Average accuracy after 10 iterations while using IG CSFS

98.62%2

96.77%1

99.38%
1 Drop in accuracy due to programmer skill set.
2 Coding style is more distinct in more difﬁcult tasks.

96.69%

95.43%2

94.89%1

Table 8: Effect of Problem Difﬁculty on Coding Style

5 Discussion

In this section, we discuss the conclusions we draw from
the experiments outlined in the previous section, limita-
tions, as well as questions raised by our results. In par-
ticular, we discuss the difﬁculty of the different settings
considered, the effects of obfuscation, and limitations of
our current approach.

Problem Difﬁculty. The experiment with random
problems from random authors among seven years most
closely resembles a real world scenario. In such an ex-
perimental setting, there is a chance that instead of only
identifying authors we are also identifying the properties
of a speciﬁc problem’s solution, which results in a boost
in accuracy.

In contrast, our main experimental setting where all
authors have only answered the nine easiest problems is
possibly the hardest scenario, since we are training on the
same set of eight problems that all the authors have algo-
rithmically solved and try to identify the authors from
the test instances that are all solutions of the 9th prob-
lem. On the upside, these test instances help us precisely
capture the differences between individual coding style
that represent the same functionality. We also see that
such a scenario is harder since the randomized dataset
has higher accuracy.

Classifying authors that have implemented the solu-
tion to a set of difﬁcult problems is easier than identi-
fying authors with a set of easier problems. This shows

that coding style is reﬂected more through difﬁcult pro-
gramming tasks. This might indicate that programmers
come up with unique solutions and preserve their cod-
ing style more when problems get harder. On the other
hand, programmers with a better skill set have a prevalent
coding style which can be identiﬁed more easily com-
pared to contestants who were not able to advance as
far in the competition. This might indicate that as pro-
grammers become more advanced, they build a stronger
coding style compared to novices. There is another pos-
sibility that maybe better programmers start out with a
more unique coding style.

Effects of Obfuscation. A malware author or pla-
giarizing programmer might deliberately try to hide his
source code by obfuscation. Our experiments indicate
that our method is resistant to simple off-the-shelf obfus-
cators such as stunnix, that make code look cryptic while
preserving functionality. The reason for this success is
that the changes stunnix makes to the code have no effect
on syntactic features, e.g., removal of comments, chang-
ing of names, and stripping of whitespace.

In contrast, sophisticated obfuscation techniques such
as function virtualization hinder de-anonymization to
some degree, however, at
the cost of making code
unreadable and introducing a signiﬁcant performance
penalty. Unfortunately, unreadability of code is not ac-
ceptable for open-source projects, while it is no problem
for attackers interested in covering their tracks. Develop-
ing methods to automatically remove stylometric infor-
mation from source code without sacriﬁcing readability
is therefore a promising direction for future research.

Limitations. We have not considered the case where
a source ﬁle might be written by a different author than
the stated contestant, which is a ground truth problem
that we cannot control. Moreover, it is often the case that
code fragments are the work of multiple authors. We
plan to extend this work to study such datasets. To shed
light on the feasibility of classifying such code, we are
currently working with a dataset of git commits to open
source projects. Our parser works on code fragments
rather than complete code, consequently we believe this
analysis will be possible.

Another fundamental problem for machine learning
classiﬁers are mimicry attacks. For example, our clas-
siﬁer may be evaded by an adversary by adding extra
dummy code to a ﬁle that closely resembles that of an-
other programmer, albeit without affecting the program’s
behavior. This evasion is possible, but trivial to resolve
when an analysts veriﬁes the decision.

Finally, we cannot be sure whether the original au-
thor is actually a Google Code Jam contestant. In this
case, we can detect those by a classify and then verify
approach as explained in Stolerman et al.’s work [30].
Each classiﬁcation could go through a veriﬁcation step

266  24th USENIX Security Symposium 

USENIX Association

12

to eliminate instances where the classiﬁer’s conﬁdence is
below a threshold. After the veriﬁcation step, instances
that do not belong to the set of known authors can be
separated from the dataset to be excluded or for further
manual analysis.

6 Related Work

Our work is inspired by the research done on authorship
attribution of unstructured or semi-structured text [5, 22].
In this section, we discuss prior work on source code
authorship attribution. In general, such work (Table 9)
looks at smaller scale problems, does not use structural
features, and achieves lower accuracies than our work.

The highest accuracies in the related work are
achieved by Frantzeskou et al. [12, 14]. They used 1,500
7-grams to reach 97% accuracy with 30 programmers.
They investigated the high-level features that contribute
to source code authorship attribution in Java and Com-
mon Lisp. They determined the importance of each fea-
ture by iteratively excluding one of the features from the
feature set. They showed that comments, layout features
and naming patterns have a strong inﬂuence on the au-
thor classiﬁcation accuracy. They used more training
data (172 line of code on average) than us (70 lines of
code). We replicated their experiments on a 30 program-
mer subset of our C++ data set, with eleven ﬁles contain-
ing 70 lines of code on average and no comments. We
reach 76.67% accuracy with 6-grams, and 76.06% accu-
racy with 7-grams. When we used a 6 and 7-gram fea-
ture set on 250 programmers with 9 ﬁles, we got 63.42%
accuracy. With our original feature set, we get 98% ac-
curacy on 250 programmers.

The largest number of programmers studied in the re-
lated work was 46 programmers with 67.2% accuracy.
Ding and Samadzadeh [10] use statistical methods for
authorship attribution in Java. They show that among
lexical, keyword and layout properties, layout metrics
have a more important role than others which is not the
case in our analysis.

There are also a number of smaller scale, lower ac-
curacy approaches in the literature [9, 11, 18–21, 28],
shown in Table 9, all of which we signiﬁcantly outper-
form. These approaches use a combination of layout and
lexical features.

The only other work to explore structural features is
by Pellin [23], who used manually parsed abstract syntax
trees with an SVM that has a tree based kernel to classify
functions of two programmers. He obtains an average of
73% accuracy in a two class classiﬁcation task. His ap-
proach explained in the white paper can be extended to
our approach, so it is the closest to our work in the lit-
erature. This work demonstrates that it is non-trivial to
use ASTs effectively. Our work is the ﬁrst to use struc-

tural features to achieve higher accuracies at larger scales
and the ﬁrst to study how code obfuscation affects code
stylometry.

There has also been some code stylometry work that
focused on manual analysis and case studies. Spafford
and Weeber [29] suggest that use of lexical features such
as variable names, formatting and comments, as well as
some syntactic features such as usage of keywords, scop-
ing and presence of bugs could aid in source code at-
tribution but they do not present results or a case study
experiment with a formal approach. Gray et al.
[15]
identify three categories in code stylometry: the layout
of the code, variable and function naming conventions,
types of data structures being used and also the cyclo-
matic complexity of the code obtained from the control
ﬂow graph. They do not mention anything about the syn-
tactic characteristics of code, which could potentially be
a great marker of coding style that reveals the usage of
programming language’s grammar. Their case study is
based on a manual analysis of three worms, rather than
a statistical learning approach. Hayes and Offutt [16]
examine coding style in source code by their consistent
programmer hypothesis. They focused on lexical and
layout features, such as the occurrence of semicolons,
operators and constants. Their dataset consisted of 20
programmers and the analysis was not automated. They
concluded that coding style exists through some of their
features and professional programmers have a stronger
programming style compared to students. In our results
in Section 4.3.6, we also show that more advanced pro-
grammers have a more identifying coding style.

There is also a great deal of research on plagiarism
detection which is carried out by identifying the similar-
ities between different programs. For example, there is a
widely used tool called Moss that originated from Stan-
ford University for detecting software plagiarism. Moss
[6] is able to analyze the similarities of code written by
different programmers. Rosenblum et al. [27] present a
novel program representation and techniques that auto-
matically detect the stylistic features of binary code.

Related Work
Pellin [23]
MacDonell et al.[21]
Frantzeskou et al.[14]
Burrows et al. [9]
Elenbogen and Seliya [11]
Kothari et al. [18]
Lange and Mancoridis [20]
Krsul and Spafford [19]
Frantzeskou et al. [14]
Ding and Samadzadeh [10]
This work
This work
This work
This work

# of Programmers
2
7
8
10
12
12
20
29
30
46
8
35
250
1,600

Results
73%
88.00%
100.0%
76.78%
74.70%
76%
75%
73%
96.9%
67.2%
100.00%
100.00%
98.04%
92.83%

Table 9: Comparison to Previous Results

USENIX Association  

24th USENIX Security Symposium  267

13

7 Conclusion and Future Work

Source code stylometry has direct applications for pri-
vacy, security, software forensics, plagiarism, copy-
right infringement disputes, and authorship veriﬁcation.
Source code stylometry is an immediate concern for pro-
grammers who want to contribute code anonymously be-
cause de-anonymization is quite possible. We introduce
the ﬁrst principled use of syntactic features along with
lexical and layout features to investigate style in source
code. We can reach 94% accuracy in classifying 1,600
authors and 98% accuracy in classifying 250 authors
with eight training ﬁles per class. This is a signiﬁcant
increase in accuracy and scale in source code authorship
attribution. In particular, it shows that source code au-
thorship attribution with the Code Stylometry Feature Set
scales even better than regular stylometric authorship at-
tribution, as these methods can only identify individuals
in sets of 50 authors with slightly over 90% accuracy [see
4]. Furthermore, this performance is achieved by training
on only 550 lines of code or eight solution ﬁles, whereas
classical stylometric analysis requires 5,000 words.

Additionally, our results raise a number of questions
that motivate future research. First, as malicious code
is often only available in binary format, it would be in-
teresting to investigate whether syntactic features can be
partially preserved in binaries. This may require our fea-
ture set to be improved in order to incorporate informa-
tion obtained from control ﬂow graphs.

Second, we would also like to see if classiﬁcation ac-
curacy can be further increased. For example, we would
like to explore whether using features that have joint in-
formation gain alongside features that have information
gain by themselves improve performance. Moreover, de-
signing features that capture larger fragments of the ab-
stract syntax tree could provide improvements. These
changes (along with adding lexical and layout features)
may provide signiﬁcant improvements to the Python re-
sults and help generalize the approach further.

Finally, we would like to investigate whether code can
be automatically normalized to remove stylistic informa-
tion while preserving functionality and readability.

8 Acknowledgments

This material is based on work supported by the ARO
(U.S. Army Research Ofﬁce) Grant W911NF-14-1-
0444, the DFG (German Research Foundation) under the
project DEVIL (RI 2469/1-1), and AWS in Education
Research Grant award. Any opinions, ﬁndings, and con-
clusions or recommendations expressed in this material
are those of the authors and do not necessarily reﬂect
those of the ARO, DFG, and AWS.

References
[1] The tigress diversifying c virtualizer, http://tigress.cs.arizona.edu.

[2] Google code jam, https://code.google.com/codejam, 2014.

[3] Stunnix, http://www.stunnix.com/prod/cxxo/, November 2014.

[4] ABBASI, A., AND CHEN, H. Writeprints: A stylometric ap-
proach to identity-level identiﬁcation and similarity detection in
cyberspace. ACM Trans. Inf. Syst. 26, 2 (2008), 1–29.

[5] AFROZ, S., BRENNAN, M., AND GREENSTADT, R. Detecting
hoaxes, frauds, and deception in writing style online.
In Secu-
rity and Privacy (SP), 2012 IEEE Symposium on (2012), IEEE,
pp. 461–475.

[6] AIKEN, A., ET AL. Moss: A system for detecting software pla-
giarism. University of California–Berkeley. See www. cs. berke-
ley. edu/aiken/moss. html 9 (2005).

[7] BREIMAN, L. Random forests. Machine Learning 45, 1 (2001),

5–32.

[8] BURROWS, S., AND TAHAGHOGHI, S. M. Source code author-
ship attribution using n-grams. In Proc. of the Australasian Doc-
ument Computing Symposium (2007).

[9] BURROWS, S., UITDENBOGERD, A. L., AND TURPIN, A. Ap-
plication of information retrieval techniques for source code au-
thorship attribution. In Database Systems for Advanced Applica-
tions (2009), Springer, pp. 699–713.

[10] DING, H., AND SAMADZADEH, M. H. Extraction of java pro-
gram ﬁngerprints for software authorship identiﬁcation. Journal
of Systems and Software 72, 1 (2004), 49–57.

[11] ELENBOGEN, B. S., AND SELIYA, N. Detecting outsourced stu-
dent programming assignments. Journal of Computing Sciences
in Colleges 23, 3 (2008), 50–57.

[12] FRANTZESKOU, G., MACDONELL, S., STAMATATOS, E., AND
GRITZALIS, S. Examining the signiﬁcance of high-level pro-
gramming features in source code author classiﬁcation. Journal
of Systems and Software 81, 3 (2008), 447–460.

[13] FRANTZESKOU, G., STAMATATOS, E., GRITZALIS, S.,
CHASKI, C. E., AND HOWALD, B. S.
Identifying authorship
by byte-level n-grams: The source code author proﬁle (scap)
method. International Journal of Digital Evidence 6, 1 (2007),
1–18.

[14] FRANTZESKOU, G., STAMATATOS, E., GRITZALIS, S., AND
KATSIKAS, S. Effective identiﬁcation of source code authors
using byte-level information.
In Proceedings of the 28th In-
ternational Conference on Software Engineering (2006), ACM,
pp. 893–896.

[15] GRAY, A., SALLIS, P., AND MACDONELL, S. Software foren-
sics: Extending authorship analysis techniques to computer pro-
grams.

[16] HAYES, J. H., AND OFFUTT, J. Recognizing authors: an exam-
ination of the consistent programmer hypothesis. Software Test-
ing, Veriﬁcation and Reliability 20, 4 (2010), 329–356.

[17] INOCENCIO, R. U.s. programmer outsources own job to china,

surfs cat videos, January 2013.

268  24th USENIX Security Symposium 

USENIX Association

14

[18] KOTHARI, J., SHEVERTALOV, M., STEHLE, E., AND MAN-
CORIDIS, S. A probabilistic approach to source code authorship
identiﬁcation. In Information Technology, 2007. ITNG’07. Fourth
International Conference on (2007), IEEE, pp. 243–248.

[19] KRSUL, I., AND SPAFFORD, E. H. Authorship analysis: Iden-
tifying the author of a program. Computers & Security 16, 3
(1997), 233–257.

[20] LANGE, R. C., AND MANCORIDIS, S. Using code metric his-
tograms and genetic algorithms to perform author identiﬁcation
for software forensics.
In Proceedings of the 9th Annual Con-
ference on Genetic and Evolutionary Computation (2007), ACM,
pp. 2082–2089.

[21] MACDONELL, S. G., GRAY, A. R., MACLENNAN, G., AND
SALLIS, P. J. Software forensics for discriminating between
program authors using case-based reasoning, feedforward neural
networks and multiple discriminant analysis.
In Neural Infor-
mation Processing, 1999. Proceedings. ICONIP’99. 6th Interna-
tional Conference on (1999), vol. 1, IEEE, pp. 66–71.

[22] NARAYANAN, A., PASKOV, H., GONG, N. Z., BETHENCOURT,
J., STEFANOV, E., SHIN, E. C. R., AND SONG, D. On the
feasibility of internet-scale author identiﬁcation. In Security and
Privacy (SP), 2012 IEEE Symposium on (2012), IEEE, pp. 300–
314.

[23] PELLIN, B. N. Using classiﬁcation techniques to determine
source code authorship. White Paper: Department of Computer
Science, University of Wisconsin (2000).

[24] PIKE, R. The sherlock plagiarism detector, 2011.

[25] PRECHELT, L., MALPOHL, G., AND PHILIPPSEN, M. Finding
plagiarisms among a set of programs with jplag. J. UCS 8, 11
(2002), 1016.

[26] QUINLAN, J. Induction of decision trees. Machine learning 1, 1

(1986), 81–106.

[27] ROSENBLUM, N., ZHU, X., AND MILLER, B. Who wrote this
code? identifying the authors of program binaries. Computer
Security–ESORICS 2011 (2011), 172–189.

[28] SHEVERTALOV, M., KOTHARI, J., STEHLE, E., AND MAN-
CORIDIS, S. On the use of discretized source code metrics for au-
thor identiﬁcation. In Search Based Software Engineering, 2009
1st International Symposium on (2009), IEEE, pp. 69–78.

[29] SPAFFORD, E. H., AND WEEBER, S. A. Software forensics:
Can we track code to its authors? Computers & Security 12, 6
(1993), 585–595.

[30] STOLERMAN, A., OVERDORF, R., AFROZ, S., AND GREEN-
STADT, R. Classify, but verify: Breaking the closed-world as-
sumption in stylometric authorship attribution. In IFIP Working
Group 11.9 on Digital Forensics (2014), IFIP.

[31] WIKIPEDIA. Saeed Malekpour, 2014.

November-2014].

[Online; accessed 04-

[32] YAMAGUCHI, F., GOLDE, N., ARP, D., AND RIECK, K. Model-
ing and discovering vulnerabilities with code property graphs. In
Proc of IEEE Symposium on Security and Privacy (S&P) (2014).

[33] YAMAGUCHI, F., WRESSNEGGER, C., GASCON, H., AND
RIECK, K. Chucky: Exposing missing checks in source code
for vulnerability discovery.
In Proceedings of the 2013 ACM
SIGSAC Conference on Computer & Communications Security
(2013), ACM, pp. 499–510.

A Appendix: Keywords and Node Types

AdditiveExpression
ArgumentList
BitAndExpression
Callee
CastTarget
ConditionalExpression
ElseStatement
Expression
ForStatement
Identiﬁer
IdentiﬁerDeclType
IncDecOp
Label
OrExpression
ParameterType
RelationalExpression
ShiftExpression
SizeofOperand
UnaryExpression
WhileStatement

AndExpression
ArrayIndexing
BlockStarter
CallExpression
CompoundStatement
ContinueStatement
EqualityExpression
ExpressionStatement
FunctionDef
IdentiﬁerDecl
IfStatement
InclusiveOrExpression
MemberAccess
Parameter
PrimaryExpression
ReturnStatement
Sizeof
Statement
UnaryOp

Argument
AssignmentExpr
BreakStatement
CastExpression
Condition
DoStatement
ExclusiveOrExpression
ForInit
GotoStatement
IdentiﬁerDeclStatement
IncDec
InitializerList
MultiplicativeExpression
ParameterList
PtrMemberAccess
ReturnType
SizeofExpr
SwitchStatement
UnaryOperator

Table 10: Abstract syntax tree node types

Table 10 lists the AST node types generated by Joern
that were incorporated to the feature set. Table 11 shows
the C++ keywords used in the feature set.

alignas
auto
case
class
continue
double
export
friend
long
not
or_eq
reinterpret_cast
static
template
try
unsigned
wchar_t

alignof
bitand
catch
compl
decltype
dynamic_cast
extern
goto
mutable
not_eq
private
return
static_assert
this
typedef
using
while

and
bitor
char
const
default
else
false
if
namespace
nullptr
protected
short
static_cast
thread_local
typeid
virtual
xor

and_eq
bool
char16_t
constexpr
delete
enum
ﬂoat
inline
new
operator
public
signed
struct
throw
typename
void
xor_eq

asm
break
char32_t
const_cast
do
explicit
for
int
noexcept
or
register
sizeof
switch
true
union
volatile

Table 11: C++ keywords

USENIX Association  

24th USENIX Security Symposium  269

15

B Appendix: Original vs Obfuscated Code

Figure 6: A code sample X

Figure 6 shows a source code sample X from our
dataset that is 21 lines long. After obfuscation with Ti-
gress, sample X became 537 lines long. Figure 7 shows
the ﬁrst 13 lines of the obfuscated sample X.

Figure 7: Code sample X after obfuscation

270  24th USENIX Security Symposium 

USENIX Association

16

