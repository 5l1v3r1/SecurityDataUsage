Mirror: Enabling Proofs of Data Replication  

and Retrievability in the Cloud

Frederik Armknecht, University of Mannheim; Ludovic Barman, Jens-Matthias Bohli,  

and Ghassan O. Karame, NEC Laboratories Europe

 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/armknecht

This paper is included in the Proceedings of the 25th USENIX Security SymposiumAugust 10–12, 2016 • Austin, TXISBN 978-1-931971-32-4Open access to the Proceedings of the 25th USENIX Security Symposium is sponsored by USENIX Mirror: Enabling Proofs of Data Replication and Retrievability

in the Cloud

Frederik Armknecht

Ludovic Barman

University of Mannheim, Germany

NEC Laboratories Europe, Germany

armknecht@uni-mannheim.de

ludovic.barman@neclab.eu

Jens-Matthias Bohli

Ghassan O. Karame

NEC Laboratories Europe, Germany

NEC Laboratories Europe, Germany

Hochschule Mannheim, Germany

ghassan.karame@neclab.eu

jens.bohli@neclab.eu

Abstract

Proofs of Retrievability (POR) and Data Possession
(PDP) are cryptographic protocols that enable a
cloud provider to prove that data is correctly stored
in the cloud. PDP have been recently extended
to enable users to check in a single protocol that
additional ﬁle replicas are stored as well. To conduct
multi-replica PDP, users are however required to
process, construct, and upload their data replicas
by themselves. This incurs additional bandwidth
overhead on both the service provider and the user
and also poses new security risks for the provider.
Namely, since uploaded ﬁles are typically encrypted,
the provider cannot recognize if the uploaded content
are indeed replicas. This limits the business models
available to the provider, since e.g., reduced costs for
storing replicas can be abused by users who upload
diﬀerent ﬁles—while claiming that they are replicas.
In this paper, we address this problem and pro-
pose a novel solution for proving data replication
and retrievability in the cloud, Mirror, which allows
to shift the burden of constructing replicas to the
cloud provider itself—thus conforming with the cur-
rent cloud model. We show that Mirror is secure
against malicious users and a rational cloud provider.
Finally, we implement a prototype based on Mirror,
and evaluate its performance in a realistic cloud set-
ting. Our evaluation results show that our proposal
incurs tolerable overhead on the users and the cloud
provider.

1

Introduction

The cloud promises a cost-eﬀective alternative for
small and medium enterprises to downscale/upscale
their services without the need for huge upfront in-
vestments, e.g., to ensure high service availability.

Currently, most cloud storage services guarantee

service and data availability [4, 6] in their Service
Level Agreements (SLAs). Availability is typically
ensured by means of full replication [4, 23]. Repli-
cas are stored onto diﬀerent servers—thus ensuring
data availability in spite of server failure. Storage
services such as Amazon S3 and Google FS provide
such resiliency against a maximum of two concur-
rent failures [30]; here, users are typically charged
according to the required redundancy level [4].

Nevertheless, none of today’s cloud providers ac-
cept any liability for data loss in their SLAs. This
makes users reluctant—and rightly so—when using
cloud services due to concerns with respect to the
integrity of their outsourced data [2, 7, 10]. These
concerns have been recently fueled by a number
of data loss incidents within large cloud service
providers [5, 10]. For instance, Google recently ad-
mitted that a small fraction of their customers’ data
was permanently lost due to lightning strikes which
caused temporary electricity outages [10].

To remedy this, the literature features a number
of solutions that enable users to remotely verify the
availability and integrity of stored data [11, 15, 16,
25, 34]. Examples include Proofs of Retrievability
(POR) [25, 34] which provide clients with the assur-
ance that their data is available in its entirety, and
Proofs of Data Possession (PDP) [12] which enable a
client to verify that its stored data has not undergone
any modiﬁcations. PDP schemes have been recently
extended to verify the replication of ﬁles [18, 22, 30].
These extensions can provide guarantees for the users
that the storage provider is replicating their data as
agreed in the SLA, and that they are indeed getting
the value for their money.

Notice, however, that existing solutions require
the users themselves to create replicas of their ﬁles,
appropriately pre-process the replicas (i.e., to cre-
ate authentication tags for PDP), and ﬁnally upload
all processed replicas in the cloud. Clearly, this in-

USENIX Association  

25th USENIX Security Symposium  1051

Table 1: Bandwidth cost in diﬀerent regions as pro-
vided by CloudFlare [3]. “% Peered” refers to the
percentage of traﬃc exchanged for free with other
providers.

Region
Europe

North America

Asia

Latin America

Australia

% Peered Eﬀective price/Mbps/Month

50%
20%
55%
60%
50%

$5
$8
$32
$32
$100

curs signiﬁcant burden on the users. Moreover, this
consumes considerable bandwidth from the provider,
that might have to scale up its bandwidth to ac-
commodate for such large upload requests. For ex-
ample, in order to store a 10 GB ﬁle together with
three replicas, users have to process and upload at
least 40 GB of content. Recall that the provider’s
bandwidth is a scarce resource; most providers, such
as AWS and CloudFlare, currently buy bandwidth
from a number of so-called Tier 1 providers to en-
sure global connectivity to their datacenters [3]. For
example, CloudFlare pays for maximum utilization
(i.e., maximum number of Mbps) used per month.
This process is costly (cf. Table 3) and is consid-
erably more expensive than acquiring storage and
computing resources [24].

Besides consuming the provider’s bandwidth re-
sources, this also limits the business models available
to the provider, since e.g., reduced costs for stor-
ing replicas can be oﬀered in the case where the
replication process does not consume considerable
bandwidth resources from the provider (e.g., when
the replication is locally performed by the provider).
Alternatively, providers can oﬀer reduced costs by
oﬀering infrequent/limited access to stored replicas,
etc. Amazon S3, for example, charges its users almost
25% of the underlying storage costs for additional
replication [1,9]. Users therefore have considerable in-
centives to abuse this service, and to store their data
at reduced costs as if they were replicas. Since the
outsourced data is usually encrypted, the provider
cannot recognize if the uploaded contents are indeed
replicas.

In this paper, we address this problem, and pro-
pose a novel solution for proving data replication
and retrievability in the cloud, Mirror, which goes
beyond existing multi-replica PDP solutions and en-
ables users to eﬃciently verify the retrievability of
all their replicas without requiring them to replicate
data by themselves. Notably, in Mirror, users need
to process/upload their original ﬁles only once irre-
spective of the replication undergone by their data;
here, conforming with the current cloud model [4],

the cloud provider appropriately constructs the repli-
cas given the original user ﬁles. Nevertheless, Mirror
allows users to eﬃciently verify the retrievability of
all data replicas—including those constructed by the
service provider.

To this end, Mirror leverages cryptographic puzzles
to impose signiﬁcant resource constraints—and thus
an economic disincentive—on a cloud provider which
creates the replicas on demand, i.e., whenever the
client initiates the veriﬁcation protocol. By doing so,
Mirror incentivizes a rational cloud provider to cor-
rectly store and replicate the clients’ data—otherwise
the provider risks detection with signiﬁcant probabil-
ity.

In summary, we make the following contributions

in this work:

• We propose a novel formal model and a security
model for proofs of replication and retrievability.
Our proposed model, PoR2, extends the POR
model outlined in [34] and addresses security
risks that have not been covered so far in existing
multi-replica PDP models.

• We describe a concrete PoR2 scheme, dubbed
Mirror that is secure in our enhanced security
model. Mirror leverages a tunable replication
scheme based on the combination of Linear Feed-
back Shift Registers (LFSRs) with the RSA-
based puzzle by Rivest [33]. By doing so, Mirror
shifts the burden of constructing replicas to the
cloud provider itself and is therefore likely to
be appreciated by cloud providers since it al-
lows them to trade their expensive bandwidth
resources with relatively cheaper computing re-
sources.

• We implement and evaluate a prototype based
on Mirror in a realistic cloud setting, and we
show that our proposal incurs tolerable over-
head on both the users and the cloud provider
when compared to existing multi-replica PDP
schemes.

The remainder of this paper is organized as fol-
lows. In Section 2, we introduce a novel model for
proofs of replication and retrievability. In Section 3,
we propose Mirror, an eﬃcient instantiation of our
proposed model, and analyze its security in Section 4.
In Section 5, we evaluate a prototype implementation
of Mirror in realistic cloud settings and compare its
performance to the multi-replica PDP scheme of [18].
In Section 6, we overview related work in the area,
and we conclude the paper in Section 7.

1052  25th USENIX Security Symposium 

USENIX Association

2

2 PoR2: Proofs of Replication and Re-

trievability

In this section, we introduce a formal model for proofs
of replication and retrievability, PoR2.

2.1 System Model

We consider a setting where a user U outsources
a ﬁle D to a service provider S who agrees to the
following two conditions:

1. Store the ﬁle D in its entirety.
2. Additionally store r replicas of D in their en-

tirety.

A PoR2 protocol aims to ensure to the user that
both conditions are kept without the need for users
to download the ﬁles and the replicas. Hence, our
model comprises a further party: the veriﬁer V who
runs the PoR2 scheme to validate that indeed the data
and suﬃcient copies are stored by S . In a privately-
veriﬁable scheme, the user and the veriﬁer consist
of the same entity; these roles might be however
diﬀerent in publicly-veriﬁable schemes.

As one can see, Condition 1 indirectly implies
that a PoR2 scheme needs to comprise a PDP or
POR scheme. Consequently, similar to the POR
model, a PoR2 involves a process for outsourcing the
original data, referred to as Store, and an interactive
veriﬁcation protocol Verify.

However, Condition 2 goes beyond common
POR/PDP requirements. Hence, one needs an ad-
ditional (possibly interactive) process denoted by
Replicate for generating the replicas and a second
process, dubbed CheckReplica, which checks the cor-
rectness of the replicas (in case the replicas were
created by the user). Moreover, the interactive ver-
iﬁcation protocol Verify needs to be extended such
that it veriﬁes the storage of the original ﬁle and the
copies computed by the service provider. In what
follows, we give a formal speciﬁcation of the proce-
dures and protocols involved in a PoR2 scheme. Our
model adapts and extends the original POR model
introduced in [25, 34]. In Section 2.2, we summarize
the relation between PoR2 and previous POR models.

The Store Procedure: This randomized procedure
is executed by the user once at the start. Store takes

as input the security parameter κ and the ﬁle (cid:31)D to be

outsourced, and produces a ﬁle tag τ that is required
to run the veriﬁcation procedure. Depending on
whether the scheme is private or public veriﬁable,
the veriﬁcation tag needs to be kept secret or can
be made public. The output of Store comprises the
ﬁle D that the service provider should store. D may

be diﬀerent from (cid:31)D, e.g., contain some additional
information, but (cid:31)D should be eﬃciently recoverable

from D. Finally, Store outputs public parameters
Π which allow the generation of r replicas D(i) of
the outsourced ﬁle. We assume that the number of
copies is (implicitly) given in the copy parameters Π,
possibly being speciﬁed in the SLA before. Summing
up, the formal speciﬁcation of Store is:

(D,τ,Π) ← Store(κ,(cid:31)D)

The Replicate Procedure: The Replicate procedure
is a protocol executed between the veriﬁer (who holds
the veriﬁcation tag τ) and the service provider to
generate replicas of the original ﬁle. To this end,
Replicate takes as inputs the copy parameters Π
and the outsourced ﬁle D, and outputs the r copies
D(1), . . . ,D (r). In addition, the provider gets a copy
tag τ∗ which allows him to validate the correctness
of the copies. Formally, we have:

Replicate :

[V : τ,Π; S : D,Π]

→ [V : τ; S : D(1), . . . ,D (r),τ∗]

Recall that the veriﬁer V refers to the party that
holds the veriﬁcation tag and may not necessarily be
a third party. This captures (i) the case where the
user creates the copies on his own at the beginning
(as discussed in [18]), and (ii) the case where this
replication process is completely outsourced to the
service provider (or even to a third party).

Observe that the output for the veriﬁer includes
again the veriﬁcation tag. This is the case since we
want to capture situations where the veriﬁcation tag
can be changed, depending on the protocol ﬂow of
Replicate. To keep the description simple, we denote
both values (the veriﬁcation tag as output of the Store
procedure and the potentially updated veriﬁcation
tag after running Replicate) by τ.

The CheckReplica Procedure:
The purpose of
the CheckReplica procedure, which is used by the
service provider, is to validate that the replicas have
been correctly generated, i.e., are indeed copies of the
original ﬁle. Notice that CheckReplica is mandatory
for a comprehensive model but is not necessary in the
case where the service provider replicates the data
itself (in this case the service provider can ensure
that the replication process is done correctly).

CheckReplica is executed between the veriﬁer and
the service provider. The veriﬁer V takes as input the
copy parameters Π and veriﬁcation tag τ, being his
output of the Replicate procedure (see above), while
the service provider S takes as input the uploaded
ﬁle D, a possible replica D∗ (together with a replica

USENIX Association  

25th USENIX Security Symposium  1053

3

index i ∈ {1, . . . ,r}), the copy parameters Π, and the
copy tag τ∗. CheckReplica then outputs a binary
decision expressing whether the service provider S
believes that D∗ is a correct i-th replica of D according
to the Replicate procedure and the copy parameters
Π.
CheckReplica: [V : τ,Π; S : τ∗,Π,D,D∗,i] → [S : dec]
The Verify Protocol: A veriﬁer V , i.e., the user
if the scheme is privately veriﬁable and possibly a
third party if the scheme is publicly veriﬁable, and
the provider S execute an interactive protocol to
convince the veriﬁer that both the outsourced D
and the r replicas D(1), . . . ,D (r) are correctly stored.
The input of V is the tag τ given by Store and the
copy parameters Π, while the input of the provider
S is the ﬁle D outsourced by the user and the r
replicas generated by the Replicate procedure. The
output dec ∈ {accept,reject} of the veriﬁer expresses
his decision, i.e., whether he accepts or rejects. It
holds that:
Verify :

[V : τ,Π; S : D,D(1), . . . ,D (r)] −→ [V : dec]

Note that Verify and CheckReplica aim for com-
pletely diﬀerent goals. The CheckReplica procedure
allows the service provider S to check if the repli-
cas have been correctly generated and hence protects
against a malicious customer who misuses the repli-
cas for storing additional data at lower costs. On
the other side, the Verify procedure enables a client
or veriﬁer V to validate that the ﬁle and all copies
are indeed stored in their entirety to provide secu-
rity against a rational service provider. For instance,
CheckReplica can be omitted if the replicas have been
generated by the service provider directly while Verify
would still be required.

2.2 Relation to Previous Models
Notice that the introduced PoR2 model covers and
extends both proofs of retrievability and proofs of
multiple replicas. For example in case that no replicas
are created at all, i.e., neither the Replicate nor the
CheckReplica procedures are used, the scheme reduces
to a standard POR according to the model given
in [34]. Observe that in such cases storage allocation
is a direct consequence of the incompressibility of the
ﬁle. Moreover, the multi-replica schemes presented so
far (see Section 6 for an overview) can be seen as PoR2
schemes where the correct replication requirement
is simply ignored. In fact, we argue that if existing
multi-replica schemes are coupled with a proof that
the replicas are honestly generated by the user, then
the resulting scheme would be a secure PoR2 scheme.

2.3 Attacker Model

Similar to existing work in the area [35,36], we adapt
the concept of the rational attacker model. Here,
rational means that if the provider cannot save any
costs by misbehaving, then he is likely to simply
behave honestly. In our particular case, the advan-
tage of the adversary clearly depends on the rela-
tion between storage costs and other resources (such
as computation), and on the availability of these
resources to the adversary. In the sequel, we cap-
ture such a rational adversary by restricting him to
a bounded number of concurrent threads of execu-
tion. Given that the provisioning of computational
resources would incur additional costs, our restric-
tion is justiﬁed by the fact that a rational adversary
would only invest in additional computing resources
if such a strategy would result in lower total costs
(including the underlying costs of storage).

Likewise, we assume that users are interested to
misuse the replicas for storing more data than has
been agreed upon. Recall that since replicas are typ-
ically charged less than original ﬁles [1, 9], a rational
user may try to encode additional information or
other ﬁles into the replicas.

2.4 Security Goals and Correctness

In this section, we formalize the security goals of a
PoR2 scheme and deﬁne the correctness requirements.
Note that we do not consider conﬁdentiality of the

ﬁle (cid:31)D, since we assume that the user encrypts the

ﬁle prior to the start the PoR2 protocol. We start by
deﬁning three security notions that a PoR2 scheme
must guarantee:

Extractability: The user can recover the uploaded

ﬁle D.

Storage Allocation: Provider uses at

least as
much storage as required to store the ﬁle and
all replicas.

Correct Replication: The ﬁles D(i) are correct

replicas of D.

The extractability notion protects the user against
a malicious service provider who does not store the
whole ﬁle. Similarly, the storage allocation notion
aims to protect a user against a service provider who
does not commit enough storage to store all replicas.
Clearly, the ﬁrst two conditions together imply that a
rational provider S indeed stores D and the replicas
D(1), . . . ,D (r) and therefore fulﬁlls his part of provid-
ing redundancy to protect the data. In contrast to
the two previous notions, correct replication aims to

1054  25th USENIX Security Symposium 

USENIX Association

4

protect the service provider against a malicious user
who tries to encode additional data in the replicas.
This is an important property, which is not satisﬁed
by existing multi-replica PDP models, but which
should cater to any practical deployment of PoR2. In
Section 3, we propose an instantiation of PoR2 which
allows the provider to run Replicate by itself—thus
inherently satisfying this property. In the following
paragraphs, we provide a formal description of the
above deﬁned notions.

Extractability. Extractability guarantees that an hon-

est user is able to recover the data (cid:31)D. Adopting

[25, 34], this is formalized as follows.
If a service
provider is able to convince a honest user with signif-
icant probability during the Verify procedure, then
there exists an extractor algorithm that can interact
with the service provider and extract the ﬁle. This is
captured by a hypothetical game between an adver-
sary and an environment where the latter simulates
all honest users and an honest veriﬁer. The adver-
sary is allowed to request the environment to create
new honest users (including respective public and
private keys), to let them store chosen ﬁles, and to
run the Verify and Replicate procedures. At the end,
the adversary chooses a user U with the correspond-
ing outsourced ﬁle D and outputs a service provider
S who can execute the Verify protocol with U with
respect to the chosen ﬁle D. We say that a service
provider is ε-admissible if the probability that the
veriﬁer does not abort is at least ε.

Deﬁnition 1 (Extractability) We say that a
PoR2 scheme is ε-extractable if there exists an ex-
traction algorithm such that for any PPT algorithm
who plays the aforementioned game and outputs an
ε-admissible service provider S , the extraction algo-
rithm recovers D with overwhelming probability.

In addition, we say that correctness is provided
with respect to the extractability if the following holds.
If all parties are honest, i.e., the user, the veriﬁer,
and the provider, then the veriﬁer accepts the output
of the Verify protocol with probability 1. This should

hold for any ﬁle (cid:31)D ∈ {0,1}∗.

Storage Allocation. Let ST denote the storage of the
service provider that has been allocated for storing
the ﬁle D and the replicas D(1), . . . ,D (r). We compute
the storage allocation by the provider, ρ, as follows:

ρ :=

|ST|

|D| +|D(1)| + . . . +|D(r)|

(1)

Here, we consider the generic case where the sizes
of the replicas can be diﬀerent (e.g., due to diﬀerent

metadata). Moreover, we assume that neither the
ﬁle nor the replicas can be (further) compressed, e.g.,
because these have been encrypted ﬁrst. Since the
service provider aims to save storage, it holds in
general that 0 ≤ ρ ≤ 1. Storage allocation ensures
that ρ ≥ δ for a threshold 0 ≤ δ ≤ 1 chosen by the
user.

Deﬁnition 2 (Binding) We say that a PoR2
scheme is (δ ,ε)-binding if for any rational attacker
who plays the aforementioned game, and outputs an
ε-admissible service provider S who invests only a
fraction ρ < δ of memory, it holds that the veriﬁer
accepts only with negligible probability (in the security
parameter).

We say that the scheme is even strongly (δ ,ε)-
binding if it holds for any PPT attacker, i.e., also
for non-rational attackers.

We stress that the distinction between binding
and strongly binding is necessary in a comprehensive
model. For instance, for schemes where the repli-
cas are generated locally by the service provider S
himself, i.e., to save bandwidth, the strongly binding
property is impossible to achieve for δ > |ST|/|D|.
The reason is that a non-rational service provider
could always store D only and run the Replicate pro-
cedure over and over again when needed. On the
other hand, if the user is generating and uploading
the replicas, strong binding could be achieved when
replicas are diﬀerent encryptions of the original ﬁle,
e.g., as done in [18].
In Fortress, we aim to out-
source the replica generation to the service provider
to save bandwidth and hence only aim for the binding
property.

Correct Replication. Correct replication means es-
sentially that both, Replicate and CheckReplica, are
sound and correct. We detail this below.

We say that Replicate is sound if in the case where
the user is involved in the replica generation, the
service provider can get assurance that the addition-
ally uploaded data represents indeed correctly built
replicas that do not encode, for example, some addi-
tional data. That is, Replicate must not be able to
encode a signiﬁcant amount of additional data in the
replicas. This is formally covered by the requirement
that inputs of the veriﬁer to the replicate procedure
Replicate, namely the veriﬁcation tag τ and the copy
parameters Π, have a size that is independent of the
ﬁle size.

On the other hand, we say that Replicate is correct
if replicas represent indeed copies of the uploaded
ﬁle D. This is formally captured by requiring that

USENIX Association  

25th USENIX Security Symposium  1055

5

D can be eﬃciently recovered from any replica D(k).
More precisely, we say that Replicate is correct if
there exists an eﬃcient algorithm which given τ, Π,
and any replica D(k) outputs D.

With respect to CheckReplica, we require that
S only accepts replicas which are valid output of
Replicate. Let D and Π be the output of the Store pro-
cedure. Let E be the event that τ∗ and D(1), . . . ,D (r)
are the output of a Replicate run. Let dec be the
decision of the service provider at the end of the
CheckReplica protocol. We say that the scheme is
ε∗-correctly building replicas if:

∀i ∈ {1, . . . ,r} : Pr[dec = Accept|E ] =1,
i∈{1,...,r}{Pr[dec = Accept|¬E ]} ≤ε

max

∗.

Observe that the ﬁrst and second condition express
the correctness and soundness of CheckReplica, re-
spectively.

3 Mirror: An Eﬃcient PoR2 Instantia-

tion

3.1 Overview

The goal of Mirror is to provide a veriﬁable replication
mechanism for storage providers. Note that straight-
forward approaches to construct PoR2 would either
be communication-expensive or would be insecure in
the presence of a rational cloud provider.

For instance, the user could create and upload
the required t replicas of his ﬁles, similar to [18].
Obviously, this alternative incurs considerable band-
width overhead on the providers and users can abuse
the replicas to outsource several, diﬀerent ﬁles in
encrypted form. An alternative solution would be
to enable the cloud provider to create the repli-
cas (and their tags) on his own given the original
ﬁles. This would signiﬁcantly reduce the provider’s
bandwidth consumption incurred in existing multi-
replica schemes at the expense of investing additional
(cheaper) computing resources [24]. This alternative
might be, however, insecure since it gives considerable
advantage for the provider to misbehave, e.g., store
only one single replica and construct the replicas on
the ﬂy when needed.

To thwart the generic attacks described above,
Mirror ensures that a malicious cloud provider can
only reply correctly within the veriﬁcation protocol
by investing a minimum amount of resources, i.e.,
memory and/or time. However, to ensure the binding
property (Deﬁnition 2), i.e., that the provider invests
memory and not time, Mirror allows to scale the
computational eﬀort that a dishonest provider would

have to invest without increasing the memory eﬀort
of an honest provider. This allows to adjust the
computational eﬀort of a dishonest provider such
that the costs of storing the replicas is cheaper than
the costs of computing the response to the challenges
on the ﬂy—giving an economic incentive to a rational
provider to behave honestly.

This is achieved in Mirror through the use of a
tunable puzzle-based replication scheme. Namely, in
Mirror, the user has to outsource only his original
ﬁles and compact puzzles to the cloud provider; the
solution of these puzzles will be then combined with
the original ﬁle in order to construct the r required
replicas. Puzzles are constructed such that (i) they
require noticeable time to be solved by the cloud
provider while the user is signiﬁcantly more eﬃcient
by exploiting a trapdoor, (ii) storing their solution
incurs storage costs that are at least as large as
the required storage for replicas, (iii) their diﬃculty
can be easily adjusted by the creator to cater for
variable strengths (and diﬀerent cost metrics), and
(iv) they can be eﬃciently combined with the original
ﬁle blocks in order to create r correct replicas of the
ﬁle preserving the homomorphic properties needed
for compact proofs1.

To this end, Mirror combines the use of the RSA
puzzle of Rivest [33] and Linear Feedback Shift Reg-
isters (LFSR) (cf. Section 3.3). A crucial aspect here
is that the user creates two LFSRs: a short one which
is kept secret, and a longer public LFSR. The service
provider is only given the public LFSR to generate
the exponent values. As we show later, this allows
for high degrees of freedom with respect to security
and performance of Mirror. In the following, we ﬁrst
explain the deployed main building blocks and give
afterwards the full protocol speciﬁcation.

3.2 Building Blocks

RSA-based Puzzles: Mirror ties each sector with a
cryptographic puzzle that is inspired by the RSA
puzzle of Rivest [33]. In a nutshell, the puzzle re-
quires the repeated exponentiation of given values

X a mod N where N = p· q is publicly known RSA
modulus and a, p,q remain secret. Without know-
ing these secrets, this requires to perform modular
exponentiation. Modular exponentiation is an inher-
ently sequential process [33]. The running time of the
fastest known algorithm for modular exponentiation
is linear in the size of the exponent. Although the
provider might try to parallelize the computation of

1This condition restricts our choice of puzzles since e.g.,
hash-based puzzles cannot be eﬃciently combined with the
authentication tag of each data block/sector.

1056  25th USENIX Security Symposium 

USENIX Association

6

the puzzle, the parallelization advantage is expected
to be negligible [17, 26, 28, 33]. On the other hand,
the computation can be eﬃciently veriﬁed by the
puzzle generator through the trapdoor oﬀered by Eu-
ler’s function in O(log(N)) modular multiplications
by computing X a(cid:31) mod N ≡ X a(cid:31) mod φ (N) mod N.
Observe that this puzzle is likewise multiplicative
homomorphic: given a and a(cid:31), the product of the
solutions X a(cid:31) and X a(cid:31)(cid:31) represents a solution for a(cid:31) +a(cid:31)(cid:31).
This preserves the homomorphic properties of the
underlying POR and allows for batch veriﬁcation for
all the replicas and hence enables compact proofs.

To further reduce the veriﬁcation burden on users,
Mirror generates the exponents using a Linear Feed-
back Shift Registers (LFSR) as follows.

Linear Feedback Shift Registers: A Linear Feedback
Shift Register (LFSR) is a popular building block for
designing stream ciphers as it enables the generation
of long output streams based on a initial state. In
Mirror, LFSRs will be used to generate the exponents
for the RSA-based puzzle described above. In what
follows, we brieﬂy describe the concept of an LFSR
sequence and refer the readers to [29] for further
details.

Deﬁnition 3 (Linear Feedback Shift Register)
Let F be some ﬁnite ﬁeld, e.g., Zp for some prime
p. A Linear Feedback Shift Register (LFSR) of
length λ consists of an internal state of length λ
and a linear feedback function F : Fλ → F with
F(x1, . . . ,x λ ) = ∑λ
i=1 ci · xi. Given an initial state
(s1, . . . ,s λ ) ∈ Fλ ,
it deﬁnes inductively an LFSR
sequence (st )t≥1 by st+λ = F(st , . . . ,s t+λ−1) for t ≥ 1.
An important and related notion is that of a feedback
polynomial. Given an LFSR with feedback function
F(x1, . . . ,x λ ) = ∑λ
i=1 ci · xi, the feedback polynomial
f (x) ∈ F[x] is deﬁned as:
f (x) =x λ −

ci · xi−1.

λ
∑

(2)

i=1

It holds that any multiple of a feedback polyno-
mial is again a feedback polynomial. That is, if

f ∗(x) =x λ∗ − ∑λ∗
holds that st+λ∗ − ∑λ∗

i=1 c∗i · xi−1 is a multiple of f , then it
i=1 c∗i · st+i−1 = 0 for each t ≥ 1.
Mirror exploits this feature in order to realize a gap
between the puzzle solution created by provider and
the veriﬁcation done by the user.

3.3 Protocol Speciﬁcation

We now start by detailing the procedures in Mirror.

Speciﬁcation of the Store Procedure: In the store
phase, the user is interested in uploading a ﬁle
D ∈ {0,1}∗. We assume that the ﬁle D is encrypted
to protect its conﬁdentiality and encoded with an era-
sure code (as required by the utilized POR in order
to provide extractability guarantees) prior to being
input to the Store protocol [25, 34]. First, the user
generates an RSA modulus N := p· q where p and q
are two safe primes2 whose size is chosen according
to the security parameter κ.

Similar to [34], the ﬁle is interpreted as n blocks,
each is s sectors long. A sector is an element of ZN
and is denoted by di, j with 1 ≤ i ≤ n, 1 ≤ j ≤ s. That
is, the overall number of sectors in the ﬁle is n· s.
To ensure unique extractability (see Section 4.1), we
require that the bit representation of each sector di, j
contains a characteristic pattern, e.g., a sequence of
zero bits. The length of this pattern depends on the
ﬁle size and should be larger than log2(n· s).
Furthermore, the user samples a key kprf per ﬁle,
where the key length is determined by the security
parameter, e.g., kprf ∈ {0,1}κ . By invoking kprf as a
seed to a pseudo-random function (PRF), the user
R
samples s non-zero elements of Zφ (N), i.e., ε1, . . . ,ε s
←
Zφ (N) \{0}. Finally, the user computes σi for each i,
1 ≤ i ≤ n, as follows:
σi ←

∈ ZN.

∏

di, j

(3)

s

j=1

ε j

These values are appended to the original ﬁle so
that the user uploads (D,{σi}1≤i≤n). Unless speciﬁed
otherwise, we note that all operations are performed
in the multiplicative group Z∗N of invertible integers
modulo N.3

Assuming that the user is interested in maintaining
r replicas in addition to the original ﬁle D at the cloud,
the user additionally constructs copy parameters Π
which will also be sent to the server. To this end, the
user ﬁrst generates two elements g,h ∈ Z∗N of order
p(cid:31) and q(cid:31), respectively. Recall that the order of Z∗N is
ϕ(N) = (p− 1)(q− 1) =4 · p(cid:31) · q(cid:31). The elements g and
h will be made public to the server while their orders
are kept secret.

Then, the user proceeds to specify feedback poly-
nomials for two LFSRs, one being deﬁned over Zp(cid:31)
and the other over Zq(cid:31). Both LFSRs need to have a
length λ such that |F|λ > n· s. Here, for each of the

two LFSRs, two feedback polynomials are speciﬁed:
a shorter one which will be kept secret by the user

distinct primes p(cid:31) and q(cid:31).

2That is, p − 1 = 2 · p(cid:31) and likewise q − 1 = 2 · q(cid:31) for two
3Observe that hitting by coincidence a value outside of Z∗N
allows to factor N which is considered to be a hard problem.

USENIX Association  

25th USENIX Security Symposium  1057

7

and a larger one that will be made public to the
provider. More precisely, for the LFSR deﬁned over
Zp(cid:31) the user chooses two polynomials

fa(x) := xλ −

λ
∑

i=1

αi · xi−1,

f ∗a (x) := xλ∗ −

λ∗
∑

i=1

α∗i · xi−1

t

such that f ∗a (x) is a multiple of fa(x) (and hence
λ < λ∗). For security reasons, it is necessary to
ensure that α∗1 ≥ 2.
The feedback polynomial fa(x) with the lower de-
gree will be kept secret while the polynomial f ∗a (x)
of the higher degree and the larger coeﬃcients will
fa(x) will serve as a
be given to the provider.
feedback polynomial to generate for each replica
k ∈ {1, . . . ,r} an LFSR sequence (a(k)
). To this
end, the user chooses for each k an initial state
(a(k)
1 , . . . ,a (k)
p(cid:31) which deﬁnes the full sequence by
a(k)
t+λ +1 = ∑λ
t+i for any t ≥ 0. Observe that due
to the fact that f ∗a (x) is a multiple of fa(x), it likewise
i=1 α∗i · a(k)
holds a(k)
t+i for any t ≥ 0. Finally,
the user publishes as part of the copy parameters
the values ga(k)
λ∗ ∈ ZN for each replica and the
coeﬃcients α∗1 , . . . ,α ∗λ∗ ∈ Z. Afterwards, he proceeds
analogously over Zq(cid:31) , i.e., sample coeﬃcients βi ∈ Zq(cid:31),
compute feedback functions fb(x), f ∗b (x), choose an
initial state (b(k)
λ ) for each replica, and so on.
Summing up, and assuming that the server should
construct r replicas, the user sets the ﬁle speciﬁc
veriﬁcation tag (which are kept secret by the user)
to:

λ ) ∈ Zλ
i=1 αi·a(k)
t+λ∗+1 = ∑λ∗

1 , . . . ,b (k)

1 , . . . ,g a(k)

τ

:= (cid:31)kprf, p,q,g,h, (a(k)

(b(k)

1 , . . . ,b (k)

1 , . . . ,a (k)

λ )1≤k≤r, (α1, . . . ,α λ ),

λ )1≤k≤r, (β1, . . . ,β λ )(cid:30) .

To enable the server to construct the r replicas, the
following copy parameters are given to the server:

Π := (cid:29)(ga(k)

1 , . . . ,g a(k)

(hb(k)

1 , . . . ,h b(k)

λ∗ )1≤k≤r, (α∗1 , . . . ,α ∗λ∗),
λ∗ )1≤k≤r, (β∗1 , . . . ,β ∗λ∗)(cid:28) .

That is, the user sends D, the values {σi}1≤i≤n, and
Π to the service provider and keeps the veriﬁcation
tag τ secret. Observe that the size of τ and Π are
independent of the ﬁle size.

Speciﬁcation of the CheckReplica Procedure: As the
replicas are completely generated by the service
provider, a CheckReplica procedure is not required in
Mirror. However, one could check the validity of the

data replicas by running the Replicate procedure and
simply comparing the outputs.

Speciﬁcation of the Replicate Procedure: Upon re-
ception of D, the values {σi}1≤i≤n, and Π, the service
provider S stores D and starts the construction of
the r additional replicas D(k) for 1 ≤ k ≤ r, of D. Here,
each sector d(k)
i, j of replica k has the following form:

i, j = di, j · g(k)
d(k)

i, j · h(k)
i, j ,
i, j and h(k)

(4)

We call these values g(k)
i, j blinding factors.
Both sets of blinding factors are computed by raising
g and h by elements of the LFSR sequences a(k)
and
t
b(k)
, respectively, but one in the forward and the
t
other in the backward order, namely:

i, j := ga(k)
g(k)

(i−1)·s+ j ,

i, j := hb(k)
h(k)

(n·s+1)−(i−1)·s− j .

(5)

To enable the provider to compute the blinding fac-
tors ga(k)
, we make use of the fact that for any t ≥ 0
it holds a(k)
t+λ∗+1 = ∑λ∗

t+i and hence

i

i=1 α∗i · a(k)
i=1(cid:29)ga(k)
t+i(cid:28)α∗i
λ∗
∏

ga(k)

t+λ∗+1 =

.

(6)

The computation of the blinding factors hb(k)
analogously.

i works

In summary, the server constructs replicas D(k) for

k = 1, . . . ,r as follows:



1

(n−1)·s+s

d1,1 · ga(k)
d2,1 · ga(k)
dn,1 · ga(k)

· hb(k)
s+1 · hb(k)
...
(s−1)·s+1 · hb(k)

(n−2)·s+s

s

. . .

. . .
. . .

. . .

(n−1)·s+1

(n−2)·s+1

s

d1,s · ga(k)
d2,s · ga(k)

· hb(k)
2·s · hb(k)
...
n·s · hb(k)
dn,s · ga(k)

1



R

Speciﬁcation of the Verify Procedure: The Verify pro-
tocol generates at ﬁrst a random challenge C. It con-
tains a random (cid:28)-element set of tuples (i,νi) where
i ∈ {1, . . . ,n} denotes a block index, and νi
← ZN is
a randomly generated integer. In addition, a non-
zero subset R ⊂ {1, . . . ,r} is sampled. The set R will
indicate which replicas will be involved in the chal-
lenge. Observe that R = /0 would mean that simply a
proof of retrievability is executed without checking
the replicas. The challenge is then the combination
of both:

c=1,R).

C = ((ic,νc)(cid:28)

(7)
Given a challenge C, the server computes the re-
sponse µ = (µ1, . . . , µs) ∈ Zs
dνc
ic, j,

N as follows:

j = 1, . . . ,s.

µ j :=

(8)

∏

(cid:28)

c=1

1058  25th USENIX Security Symposium 

USENIX Association

8

Observe that µ j
original data, that is dic, j
are processed in the same manner to obtain:

is the product of powers of the
νc . In addition, the ﬁle tags

σ =

(cid:30)

c=1(cid:31)σic ·
∏

s

∏

j=1

(k)(cid:30)νc

.

dic, j

∏
k∈R

(9)

The tuple (µ,σ ) marks the response and is sent back
to the user who veriﬁes (µ,σ ) similar to the private-
veriﬁable POR of [34]. First, he computes:

˜σ := σ ·

(cid:30)

c=1(cid:31) s
∏
∏

j=1

g(k)
ic, jh(k)

ic, j(cid:30)−νc

∏
k∈R

(10)

Observe that this will require the reconstruction of
the blinding factors. In Appendix C, we show how to
eﬃciently perform this veriﬁcation by the user, using
the knowledge of the veriﬁcation tags—in particular
the knowledge of the secret shorter LFSR, its initial
state, and the order of g and h, respectively.

Next, the user recovers the secret parameters εik ,
k = 1, . . . , (cid:30), using the key kprf as a seed for the PRF.
Finally, the user veriﬁes that the following holds:

s

∏

j=1

µ j

ε j+|R| = ˜σ .

(11)

it

.

First,

j=1 ∏r∈R dic, j(cid:28)νc

We now explain why the veriﬁcation step has
to hold if the response has been computed cor-
rectly.
from Equation (4)
that ∏(cid:30)

rewritten

can be

follows

ic, j(cid:28)νc
j=1 ∏r∈R d(r)
c=1(cid:29)∏s
as the product of ∏(cid:30)
ic, j(cid:28)νc
ic, jh(r)

c=1(cid:29)∏s
j=1 ∏r∈R g(r)
ic, j(cid:28)νc

j=1 d|R|

c=1(cid:29)∏s
c=1(cid:29)∏s

∏(cid:30)
. The second factor is
exactly the part that is canceled out in Equa-
tion (10) while the ﬁrst factor can be simpliﬁed to
∏(cid:30)

Given a series of straightforward calculations, one
j=1 (µ j)ε j+|R|.
can show that ˜σ can be rewritten to ∏s
This proves the correctness of Equation (11)—hence
the correctness with respect to extractability.

and

Moreover, it is easy to see that, since each sector
of a replica corresponds to the multiplication of the
corresponding sector of the uploaded ﬁle D with a
blinding factor (that can be reconstructed from Π),
the replicas are indeed copies of the original ﬁle. This
means that Replicate is correct. Summing up, all
three correctness requirements explained in Section 2
are fulﬁlled in Mirror.

4 Security Analysis

We now proceed to prove the security of our scheme
according to the deﬁnitions in Section 2.4. Recall that

the user is not involved in the replica generation and
that the size of the parameters involved in creating a
replica is independent of the ﬁle size. This ensures the
correct replication property described in Section 2.4.
It remains to prove that (i) if the service provider
S stores at least a fraction δ of all sectors in one
replica, then the ﬁle can be reconstructed (extractabil-
ity) and (ii) if the service provider stores less than
a fraction of δ of any replica, this misbehavior will
be detected with overwhelming probability (storage
allocation).

4.1 Extractability

In principle, the computations done in the Store and
Verify procedures of Mirror can be seen as multiplica-
tive variants of the corresponding mechanisms of the
privately-veriﬁable POR of [34] (see Appendix B for
details on the scheme of [34]).
In particular, the
extractability arguments given in [34] transport di-
rectly to Mirror. We assume that an erasure coding
is applied to the ﬁle to ensure the recovery of ﬁle
contents from any fraction δ of the ﬁle. In particular,
we refer to [34] for additional details on the choice
of parameters (e.g., for erasure coding) such that
retrievability is ensured if a fraction δ of the ﬁle is
stored.

= du

c=1 dic, j

To show that Mirror enables the reconstruction
of the ﬁle from suﬃciently many correct responses,
we point out that given a correct response, the user
νc for
learns expressions of the form µ j = ∏(cid:30)
known exponents νc ∈ Z and known indices. Let us
assume some arbitrary ordering µ (1), µ (2), . . . on these
expressions. If suﬃciently many responses µ (i) are
known, the user can choose for any (i, j) coeﬃcients
i, j =: ˜d for a known

c(k) ∈ Z such that ∏k(cid:29)µ (k)(cid:28)c(k)
value u ∈ Z.
Recall that the order of any di, j ∈ Z∗N is a divi-
sor of 2p(cid:26)q(cid:26). If u is odd, u is co-prime to p(cid:26)q(cid:26) with
overwhelming probability and the user can simply
compute u−1 mod p(cid:26)q(cid:26) and determine di, j = ˜du−1
. On
the other hand, if u is even (i.e., u = 2 · u(cid:26)), two
cases emerge. If the order of di, j is a divisor of p(cid:26)q(cid:26),
the exponent is again co-prime to the order with
In this case, the user computes
high probability.
u−1 mod p(cid:26)q(cid:26) and checks if
contains the char-
acteristic bit pattern (see description of the Store
procedure). If this fails, this means that the order of
˜d is even and the user proceeds as follows. Observe
that the order of d2
i, j is a divisor of p(cid:26)q(cid:26). Thus, the
user ﬁrst computes (u(cid:26))−1 mod p(cid:26)q(cid:26) and then ˜d(u(cid:26))−1
.
This yields d2
i, j. As the user knows the factorization
of N, he can compute all four possible roots of d2
i, j

˜du−1

USENIX Association  

25th USENIX Security Symposium  1059

9

(e.g., using the Chinese Remainder Theorem). Due
to the characteristic pattern embedded in di, j (see
the speciﬁcation of the Store procedure), the user is
able to identify the correct di, j.4

4.2 Storage Allocation

Observe that Mirror represents in fact a proof of
retrievability over the uploaded ﬁle and all repli-
cas. This means that if a challenge involves sectors
that are not stored, Mirror ensures that the service
provider fails with high probability unless he is able
to correctly reconstruct the missing replicas. In the
following, we therefore investigate the eﬀort of a
malicious service provider in reconstructing missing
sectors. That is, we consider the scenario where the
service provider has stored the complete ﬁle5 but
only parts of some replicas.

In Mirror, the service provider needs to compute the
corresponding blinding factors in order to recompute
any missing sectors. As these are products of the form
g(k)
· h(k)
j and since these sequences are independent
i
from each other, the service provider is forced to store
values of the sequences (g(k)
j ) j separately.
Moreover, since these values are diﬀerent for the
individual replicas, knowing (or reconstructing) a
value from one sequence and one replica does not
help the service provider in deriving values of other
sequences and/or replicas.

i )i and (h(k)

tion of larger coeﬃcients, this would incur additional
(signiﬁcant) computational overhead on the provider
compared to the user. For deducing the shorter feed-
back functions fa(x), fb(x), the provider would have
to determine Zp(cid:28) respectively Zq(cid:28)—which equally re-
quire the knowledge of the factors of N.

It remains to investigate the eﬀort for reconstruct-
ing values of the LFSR sequences. Note that the
sequences used in the replicas are deﬁned by diﬀerent
independent internal states and that, for each replica,
the sequences (a(k)i) and (b(k)
j ) are independent. We
can therefore without loss of generality restrict our
analysis to one sequence (gt )t≥1. For the ease of rep-
resentation, we omit in the sequel the index (k) and
write gi = gai for the ease of representation. We say
that (cid:30)v = (v1, . . . ,v n·s) ∈ Zn·s represents a valid relation
with respect to (gt )t≥1 if:
n·s
∏
gvi
i = 1.
i=1

(12)

It follows from known facts about LFSRs that valid
relations are the only means for the service provider
to compute missing values g j from known values gi
(see Appendix D for more details).

As the provider is forced to use the feedback func-
tion deﬁned by the coeﬃcients (α∗1 , . . . ,α ∗λ∗) (see
above), the only valid relations the provider can
derive are linear combinations of:

i

,hb(k)

A crucial aspect of Mirror is that a cheating service
provider should require a signiﬁcantly higher eﬀort
compared to an honest service provider in recom-
puting missing replicas. Recall that both the user
and the provider determine the blinding factors by
computing LFSR sequences. One diﬀerence though
is that the provider has to do his computations on
values ga(k)
j ∈ Z∗N while the user can eﬃciently
compute on the exponents in a(k)
j ∈ Zq(cid:28)
directly. Observe that the provider is not able to
transfer his computations into Zp(cid:28) and Zq(cid:28) without
eventually factoring N = p · q, which is commonly
assumed to be a hard problem. A further gap is
that the user deploys LFSRs with feedback functions
fa(x) ∈ Zp(cid:28)[x] and fb(x) ∈ Zq(cid:28) where the provider only
knows the feedback functions f ∗a (x), f ∗b (x) ∈ Z[x] that
are multiples of fa(x), fb(x). Given that these func-
tions involve more inputs, and require the construc-

i ∈ Zp(cid:28) and b(k)

4Observe that in principle it may happen with some prob-
ability that more than one root exhibits this pattern. This is
the reason why a padding length ≥ log2(n· s) is proposed such
that the expected number of incorrectly reconstructed sectors
in the ﬁle is less than 1.

5Recall that this property is validated by the proof of

retrievability already.

−→b j := (0 . . .0
j−1

(cid:31)(cid:30)(cid:29)(cid:28)

n·s−( j−1)−λ∗

0 . . .0

(cid:31)(cid:30)(cid:29)(cid:28)

,α∗1 , . . . ,α ∗λ∗,

),

(13)

where −→b1 corresponds to the given feedback polyno-
mial f ∗a (x) and the others are derived by simple shift
of indexes.

Hence, for any valid relation (cid:30)v = (vi)i ∈ V, there
exist unique coeﬃcients c1, . . . ,c n·s−λ∗+1 ∈ Z such that
(cid:30)v = ∑i ci · −→bi . Let imin be the smallest index with
cimin (cid:24)= 0. Then, it holds that the ﬁrst v j = 0 for
j < imin − 1 and that vimin = cimin · α∗1 (as the other
vectors with index i > imin) are zero at index imin.
Hence, it holds that maxi{(cid:22)log2(vi)(cid:21)} ≥ (cid:22)log2(α∗1 )(cid:21).
This shows that the eﬀort of executing a valid rela-
tion (cf. Equation (12)) involves at least one exponen-
tiation with an exponent of size (cid:22)log2(α∗1 )(cid:21).6 The ef-
fort to compute one exponentiation for an exponent of
bitsize k is 3/2·k·Tmult, where Tmult stands for the time
it takes the resource-constrained rational attacker
(see Section 2.3) to multiply two values modulo N [27].

6One may combine several exponentiations to reduce the
overall number of exponentiations but one cannot reduce the
eﬀort to compute at least once the exponentiation with the
highest exponent.

1060  25th USENIX Security Symposium 

USENIX Association

10

Thus, a pessimistic lower bound for reconstructing
a missing value gi is 3/2·(cid:30)log2(α∗1 )(cid:28)· Tmult. Observe
that we ignore here additional eﬀorts such as ﬁnding
appropriate valid relations (cf. Equation (12)), etc.
Assume now that the service provider stores less
than a fraction δ of all sectors of a given replica
where δ refers to the threshold chosen by the user
(see also Deﬁnition 2). Thus, for any value gi con-
tained in the challenge, the probability that gi has
to be re-computed is at least 1− δ . Due to the fact
that this holds for the values h j as well and that a
challenge requests (cid:30)· s sectors, the expected number
of values that need to be recomputed is 2(cid:30)· s· (1−δ ).
To achieve the binding property with respect to a
rational attacker, one has to ensure that the time
eﬀort for recomputing these values incurs costs that
exceed the costs for storing these values. This im-
plies a time threshold Tthr which marks the minimum
computational eﬀort this should take. Given such a
threshold Tthr, we get the following inequality:

(cid:30)log2(α∗1 )(cid:28) ≥

Tthr

3(cid:30)· s· (1− δ )· Tmult

.

(14)

That is, if the parameters are chosen as displayed
in (14), a dishonest provider would bear on average
higher costs than an honest provider. Here, one
can use the common cut-and-choose approach, by
posing a number of challenges where the number is
linear in the security parameter κ, to ensure that the
overall probability to circumvent these computations
is negligible in κ. This proves the binding property
(cf. Deﬁnition 2) with respect to the class of PPT
service providers that can execute a bounded number
of threads in parallel only.

Notice that Mirror can easily cope with (i) diﬀerent
attacker strengths, and (ii) variable cost metrics, as
follows:

j

i and hb(k)

Length of LFSR: One option would be to increase
λ∗, i.e., the length of the LFSR communicated
to the provider, and hence the number of values
ga(k)
the provider has to use for gener-
ating the replicas. In the extreme case, λ∗ could
be made equal to half of the total number of
sectors n· s—which would result into a scheme
whose bandwidth consumption is comparable
to [18].

Bitlength of coeﬃcients: Another

alternative
would be to keep λ∗ short, but to increase
the bitlengths of the coeﬃcients α∗i and β∗j .
This would preserve the small bandwidth
consumption of Mirror but would increase the
time eﬀort to run Replicate. This option will

also not aﬀect the latency borne by users in
verifying the provider’s response.

Hybrid approach: Clearly, one can also aim for a
hybrid scheme, by increasing both the public
LFSR length λ∗ and the coeﬃcients α∗i and β∗j .
In Section 5, we investigate reasonable choices of α∗1 ,
ρ, and Tthr to satisfy Equation 14. Of course, the
same considerations can also be applied with respect
to β∗1 which we omit for space reasons.

5

Implementation & Evaluation

In this section, we evaluate an implementation of
Mirror within a realistic cloud setting and we compare
the performance of Mirror to the MR-PDP solution
of [18].

5.1

Implementation Setup

We implemented a prototype of Mirror in Scala. For a
baseline comparison, we also implemented the multi-
replica PDP protocol7 of [18], which we denote by
MR-PDP in the sequel (see Appendix A for a descrip-
tion of the MR-PDP of [18]). In our implementation,
we relied on SHA-256, and the Scala built-in random
number generator.

We deployed our implementation on a private net-
work consisting of two 24-core Intel Xeon E5-2640
with 32GB of RAM. The storage server was running
on one 24-core Xeon E5-2640 machine, whereas the
clients and auditors were co-located on the second
24-core Xeon E5-2640 machine.

To emulate a realistic Wide Area Network (WAN),
the communication between various machines was
bridged using a 100 Mbps switch. All traﬃc ex-
changed on the networking interfaces of our machines
was shaped with NetEm [31] to ﬁt a Pareto distri-
bution with a mean of 20 ms and a variance of 4
ms—thus emulating the packet delay variance spe-
ciﬁc to WANs [19].

In our setup, each client invokes an operation in
a closed loop, i.e., a client may have at most one
pending operation.

When implementing Mirror, we spawned multiple
threads on the client machine, each thread corre-
sponding to a unique worker handling a request of a
client. Each data point in our plots is averaged over
10 independent measurements; where appropriate, we
include the corresponding 95% conﬁdence intervals.

7We acknowledge that PDP oﬀers weaker guarantees than
POR. However, to the best of our knowledge, no prior propos-
als for multi-replica-POR exist. MR-PDP thus oﬀers one-of-
the-few reasonable benchmarks for Mirror.

USENIX Association  

25th USENIX Security Symposium  1061

11

]
s
[
 

B
M
4
6
e

 

t

a
c

i
l

p
e
r
 
o

t
 

e
m
T

i

 1200
 1000
 800
 600
 400
 200
 0

λ*=5
λ*=15
λ*=40
λ*=60

10 20 30 40 50 60 70 80 90 100

|α1*|

]
s
[
 
y
c
n
e
a
L

t

 3
 2.5
 2
 1.5
 1
 0.5
 0

0

40

80

120

|α1*|

160

200

]
s
[
 
y
c
n
e
a
L

t

 7000
 6000
 5000
 4000
 3000
 2000
 1000
 0

MR-PDP (Store)
Mirror (Store)

1

2

8

16

32

File size [MB]

64

128 1024

(a) Impact of |α∗1| on replication time.

(b) Impact of |α∗1| on the time required
by a rational provider to issue correct
responses.

(c) Latency incurred in Store w.r.t. the
ﬁle size.

]
s
[
 
y
c
n
e
a
L

t

 400
 350
 300
 250
 200
 150
 100
 50
 0

MR-PDP (Replicate)

1

2

8
r

32

64

]
s
[
 
y
c
n
e
a
L

t

 4000
 3500
 3000
 2500
 2000
 1500
 1000
 500

Mirror (Replicate)

1

2

8
r

32

64

]
s
[
 
y
c
n
e
a
L

t

 6
 5
 4
 3
 2
 1
 0

MR-PDP (Verify-Client)
MR-PDP (Verify-Server)
Mirror (Verify-Client)
Mirror (Verify-Server)

1

2

8
r

32

64

(d) Latency incurred in Replicate as wit-
nessed by the clients of MR-PDP w.r.t.
r.

(e) Latency incurred in Replicate as wit-
nessed by the service provider in Mirror
w.r.t. r.

(f) Latency incurred in Verify as seen by
clients.

Figure 1: Performance evaluation of Mirror in comparison to the MR-PDP scheme of [18]. Each data point
in our plots is averaged over 10 independent runs; where appropriate, we also show the corresponding 95%
conﬁdence intervals.

Parameter

File size

|p|
|q|

RSA modulus size

Number of challenges (cid:31)
Length of secret LFSR λ
Length of public LFSR λ∗
Fraction of stored sectors δ

Number of replicas r

Default Value

64 MB
1024 bits
1024 bits
2048 bit

40 challenges

2
15
0.9
2

Table 2: Default parameters used in the evaluation.

Table 2 summarizes the default parameters assumed
in our setup.

5.2 Evaluation Results

Before evaluating the performance of Mirror, we start
by analyzing the impact of the block size on the
latency incurred in the veriﬁcation of Mirror and
in MR-PDP. Our results (Figure 4 in Appendix E)
show that modest block sizes of 8 KB yield the most
balanced performance, on average, across the inves-
tigated schemes. In the rest of our evaluation, we
therefore set the block size to 8 KB.

Impact of the bitsize of α∗1 : In our implementa-
tion, our choice of parameters was mainly governed
by the need to establish a tradeoﬀ between the repli-
cation performance and the resource penalty incurred
on a dishonest provider. To this end, we choose a
small value for the public LFSR length λ∗, i.e., the
LFSR length communicated to S , and small coeﬃ-
cients α∗i and β∗j (these coeﬃcients were set to 1 bit
for i, j > 1). Recall that using smaller coeﬃcients al-
lows for faster exponentiations and hence a decreased
replication eﬀort.

However, as shown in Equation 14, the bitsize of
α∗1 (which we shortly denote by |α∗1| in the follow-
ing) plays a paramount role in the security of Mirror.
Note that the same analysis applies to β∗1 —which we
do not further consider to keep the discussion short.
Clearly, |α∗1| (and λ∗) also impacts the ﬁle replica-
tion time at the service provider. In Figure 1(a), we
evaluate the impact of |α∗1| on the replication time,
and on the time invested by a rational provider (who
does not replicate) to answer every client challenge in
Mirror. Our results indicate that the ﬁle replication
time grows linearly with |α∗1|. Moreover, the higher
λ∗ is, the longer it takes to replicate a given ﬁle. On
the other hand, as shown in Figure 1(b), |α∗1| consid-

1062  25th USENIX Security Symposium 

USENIX Association

12

|α∗1| Estimated EC2 costs per challenge (USD)
40
70
80
120

0.000058
0.00011
0.00013
0.00019

Table 3: Costs borne by a rational provider who
computes the responses to a challenge of size l = 40
on the ﬂy. We assume two replicas of size 64 MB,
and estimate costs for a compute-optimized (extra
large) instance from Amazon EC2 (at 0.42 USD per
hour).

erably aﬀects the time incurred on a rational provider
which did not correctly replicate (some) user ﬁles.
The larger |α∗1| is, the longer it takes a misbehaving
provider to reply to the user challenges, and thus the
bigger are the amount of computational resources
that the provider needs to invest in. Here, we as-
sume the lower bound on the eﬀort of a misbehaving
provider (i.e., which only stores a fraction δ of the
sectors per replica) given by Equation 14.

Setting |α∗1|: Following this analysis, suitable
choices for α∗1 need to be large enough such that
the costs borne by a rational provider who computes
the responses on the ﬂy are higher than those borne
by an honest provider who correctly stores the repli-
cas. In Table 3, we display the corresponding costs
borne by a rational provider who computes the re-
sponses on the ﬂy to a single challenge, assuming
l = 40, and r = 2 replicas of size 64 MB. Here, we
estimate the computation costs as follows: we in-
terpolate the time required by a rational provider
in answering challenges from Figure 1(c). We then
estimate the corresponding computation costs assum-
ing a compute-optimized (extra large) instance from
Amazon EC2 (at 0.42 USD per hour), which oﬀers
comparable computing power than that used in our
implementation setup.

For comparison purposes, notice that the cost of
storing two 64 MB replicas per day (based on Amazon
S3 pricing [9]) is approximately 0.00011 USD. This
shows that when instantiating Mirror with parameters
|α∗1| = 70, the provider should not gain any (rational)
advantage in misbehaving, if the user issues at least
one PoR2 challenge of l = 40 randomly selected blocks
per day. Clearly, users can increase the number of
challenges that they issue accordingly to ensure that
the costs borne by a rational provider are even more
pronounced, e.g., to account for possible ﬂuctuations
in costs.

Following this analysis, we assume that |α∗1| = 70
throughout the rest of the evaluation. As shown in

Figure 1(a), this parameter choice results in reason-
able replication times. e.g., when λ∗ = 5 or λ∗ = 15.
Observe that, in this case, users can detect/suspect
misbehavior by observing the cloud’s response time.
As shown in Figure 1(f), the typical response time
of an honest service provider is less than 2 seconds
when r = 2. An additional 0.9 seconds of delay (i.e.,
totalling 2.9 seconds) in responding to a challenge
can be then detected by users.

Store performance: In Figure 1(c), we evaluate the
latency incurred in Store with respect to the ﬁle size.
Our ﬁndings suggest that the Store procedure of Mir-
ror is considerably faster than that of MR-PDP. This
is the case since the tag creation in Mirror requires
fewer exponentiations per block (cf. Appendix A).
For instance, the Store procedures is almost 20%
faster than that of MR-PDP for ﬁles of 64MB in size.

Replicate performance: Figure 1(d) depicts the
latency incurred on the clients of MR-PDP in the
replicate procedure Replicate with respect to the num-
ber of replicas. Recall that, in MR-PDP, clients have
to process and upload all replicas by themselves to
the cloud. Clearly, the latency of Replicate increases
with the number of replicas stored. Given our multi-
threaded implementation, notice that the replication
process can be performed in parallel. Here, as the
number of concurrent replication requests increases,
the threads in our thread pool are exhausted and the
system saturates—which explains the sharp increase
in the latency witnessed by clients who issue more
than 8 concurrent replication requests. Notice that
users of Mirror do not bear any overhead due to repli-
cation since this process is performed by the service
provider.

In Figure 1(e), we show the latency incurred on the
service provider in Mirror with respect to the number
of replicas r. Since Mirror relies on puzzles, the repli-
cation process consumes considerable resources from
the service provider. However, we point out that
is a one-time eﬀort per ﬁle, and can be performed
oﬄine (i.e., the provider can replicate ﬁles using “of-
ﬂine” resources in the back-end). For example, the
creation of 8 additional 64 MB ﬁle replicas incurs a
latency of almost 765 seconds. As mentioned earlier,
Mirror trades this additional computational burden
with bandwidth. Namely, users of Mirror only have
to upload the ﬁle once, irrespective of the number of
replicas desired. This, in turn, reduces the download
bandwidth of providers and, as a consequence, the
costs of oﬀering the service.

In Figure 2, we estimate the costs of the additional
computations incurred in Mirror for a 64 MB ﬁle,

USENIX Association  

25th USENIX Security Symposium  1063

13

Existing Multi Replica Schemes
Mirror

10

1

0.1

)
D
S
U
 
n
i
(
 
t
s
o
C

0.01

 2

 10

 18

 26

 42

 50

 58

 34
r

Figure 2: Replication costs for a 64 MB ﬁle (in USD)
incurred Mirror vs. existing multi-replica schemes.
We assume that the provider provisions a large gen-
eral instance from Amazon EC2 at 0.441 USD per
hour). We assume the replication time given by our
experiments in Figure 1(e) and we estimate band-
width costs by adapting the ﬁndings of [3] (cf. Ta-
ble 3).

compared to those incurred by existing multi-replica
schemes which require users to upload all the repli-
cas. To estimate computing costs, we rely on the
AWS pricing model [8]; we assume that the provider
provisions a multi-core (compute-optimized) extra
large instance from Amazon EC2 (at 0.441 USD per
hour). We rely on our ﬁndings in Figure 1(e) to
estimate the computing time for replication. We
estimate bandwidth costs by adapting the ﬁndings
of [3] (i.e., by assuming $5 per Mbps per month cf.
Table 3). Our estimates suggest that Mirror consid-
erably reduces the costs borne on the provider by
trading bandwidth costs with the relatively cheaper
computing costs. We expect that the cost savings of
Mirror will be more signiﬁcant for larger ﬁles, and/or
additional replicas.

Verify performance: In Figure 1(f), we evaluate the
latency witnessed by the users and service provider
in the Verify procedure of Mirror and MR-PDP, re-
spectively. Our ﬁndings show that the veriﬁcation
overhead witnessed by the service provider in Mirror
is almost twice that of MR-PDP. Moreover, users of
Mirror require almost 1 second to verify the response
issued by the provider. Notice that the majority of
this overhead is spent while computing/verifying the
response to the issued challenge. This discrepancy
mainly originates from the fact that the challenge-
response in Mirror involves all the 32 sectors of
each block in order to ensure the extractability of
all replicas8. We contrast this to MR-PDP where
each block comprises a single sector—which only en-
sures data/replica possession but does not provide

8We point out that this is not particular to Mirror and
applies to all schemes which ensure retrievability (e.g., [34]).

]
s
[
 
y
c
n
e
a
L

t

 12
 10
 8
 6
 4
 2
 0

MR-PDP
Mirror

 0

 2

 4

Number of operations per second [op/s]

 6

 8

 10

 12

 14

Figure 3: Latency vs. throughput comparison be-
tween MR-PDP and Mirror.

extractability guarantees.

In Figure 3, we evaluate the peak throughput ex-
hibited by the service provider in the Verify procedure.
Here, we require that the service provider handles
veriﬁcation requests back to back; we then gradu-
ally increase the number of requests in the system
(until the throughput is saturated) and measure the
associated latency. Our results conﬁrm our previous
analysis and show that Mirror attains a maximum
throughput of 6 veriﬁcation operations per second;
on the other hand, the service provider in MR-PDP
can handle almost 12 operations per second. As
mentioned earlier, this discrepancy is mainly due to
the fact that the MR-PDP blocks only comprise a
single sector, whereas each block in Mirror comprises
32 sectors. However, we argue that the overhead
introduced by our scheme compared to MR-PDP can
be easily tolerated by clients; for instance, for 64 MB
ﬁles, our proposal only incurs an additional latency
overhead of 800 ms on the clients when compared to
MR-PDP.

6 Related Work

Curtmola et al. propose in [18] a multi-replica PDP,
which extends the basic PDP scheme in [12] and en-
ables a user to verify that a ﬁle is replicated at least
across t replicas by the cloud. In [16], Bowers et al.
propose a scheme that enables a user to verify if his
data is stored (redundantly) at multiple servers by
measuring the time taken for a server to respond to
a read request for a set of data blocks. In [13, 14],
Barsoum and Hasan propose a multi-replica dynamic
data possession scheme which allows users to verify
multiple replicas, and to selectively update/insert
their data blocks. This scheme builds upon the BLS-
based SW scheme of [34]. In [22], the authors extend
the dynamic PDP scheme of [21] to transparently sup-
port replication in distributed cloud storage systems.
All existing schemes however share a common system
model, where the user constructs and uploads the

1064  25th USENIX Security Symposium 

USENIX Association

14

replicas onto the cloud. On the other hand, Mirror
conforms with the existing cloud model by allowing
users need to process/upload their original ﬁles only
once irrespective of the replication performed by the
cloud provider.

Proofs of location (PoL) [32, 36] aim at proving
the geographic position of data, e.g., if it is stored
on servers within a certain country. In [36], Watson
et al. provide a formal deﬁnition for PoL schemes by
combining the use of geolocation techniques together
with the SW POR schemes [34]. In [36], the authors
assume a similar system model to Mirror, where the
user uploads his ﬁles to the service provider only
once. The latter then re-codes the tags of the ﬁle,
and replicates content across diﬀerent geo-located
servers. Users can then execute individual PORs
with each server to ensure that their data is stored
in its entirety at the desired geographical location.
We contrast this to our solution, where the user has
to invoke a single Mirror instance to eﬃciently verify
the integrity of all stored replicas.

Proofs of space [20] ensure that a prover can only re-
spond correctly if he invests at least a certain amount
of space or time per execution. However, this notion
is not applicable to our scenario where we need to
ensure that a minimum amount of space has been
invested by the prover. Moreover, the instantiation
in [20] does not support batch-veriﬁcation which is
essential in Mirror to conduct POR on several replicas
in a single protocol run.

7 Conclusion

In this paper, we proposed a novel solution, Mirror,
which enables users to eﬃciently verify the retriev-
ability of their data replicas in the cloud. Unlike
existing schemes, the cloud provider replicates the
data by itself in Mirror; by doing so, Mirror trades
expensive bandwidth resources with cheaper comput-
ing resources—a move which is likely to be welcomed
by providers and customers since it promises better
service while lowering costs.

Consequently, we see Mirror as one of the few
economically-viable and workable solutions that en-
able the realization of veriﬁable replicated cloud stor-
age.

8 Acknowledgements

The authors would like to thank the anonymous re-
viewers for their valuable feedback and comments.
This work was partly supported by the TREDISEC
project (G.A. no 644412), funded by the European

Union (EU) under the Information and Communica-
tion Technologies (ICT) theme of the Horizon 2020
(H2020) research and innovation programme.

References

[1] Amazon S3 Introduces Cross-Region Replication.

[2] Cloud Computing:

cerns.
magazine/hh536219.aspx.

Security Con-
http://technet.microsoft.com/en-us/

Cloud

[3] The Relative Cost of Bandwidth Around the World.

[4] Amazon S3 Service Level Agreement, 2009. http:

//aws.amazon.com/s3-sla/.

[5] Are We

Safeguarding

Social

MIT Technology Review,

2009.
//www.technologyreview.com/view/412041/
are-we-safeguarding-social-data/.

Data?,
http:

[6] Microsoft Corporation. Windows Azure Pricing and

Service Agreement, 2009.

[7] Protect data stored and shared in public cloud
http://i.dell.com/sites/doccontent/

storage.
shared-content/data-sheets/en/Documents/
Dell_Data_Protection_Cloud_Edition_Data_
Sheet.pdf, 2013.

[8] Amazon EC2 Pricing, 2015. https://aws.amazon.

com/ec2/pricing/.

[9] Amazon S3 Pricing, 2015. http://aws.amazon.com/

s3/pricing/?nc2=h_ls.

[10] Google

loses

data

after

lightning

strikes.

http://money.cnn.com/2015/08/19/technology/
google-data-loss-lightning/, 2015.

[11] Frederik Armknecht, Jens-Matthias Bohli, Ghas-
san O. Karame, Zongren Liu, and Christian A.
Reuter. Outsourced proofs of retrievability. In Pro-
ceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security, CCS ’14,
pages 831–843, New York, NY, USA, 2014. ACM.

[12] Giuseppe Ateniese, Randal C. Burns, Reza Curt-
mola, Joseph Herring, Lea Kissner, Zachary N. J.
Peterson, and Dawn Xiaodong Song. Provable data
possession at untrusted stores. In ACM Conference
on Computer and Communications Security, pages
598–609, 2007.

[13] Ayad F. Barsoum and M. Anwar Hasan. Integrity
veriﬁcation of multiple data copies over untrusted
cloud servers.
In 12th IEEE/ACM International
Symposium on Cluster, Cloud and Grid Computing,
CCGrid 2012, Ottawa, Canada, May 13-16, 2012,
pages 829–834, 2012.

[14] Ayad F. Barsoum and M. Anwar Hasan. Provable
multicopy dynamic data possession in cloud com-
puting systems. IEEE Transactions on Information
Forensics and Security, 10(3):485–497, 2015.

USENIX Association  

25th USENIX Security Symposium  1065

15

[15] Kevin D. Bowers, Ari Juels, and Alina Oprea. HAIL:
a high-availability and integrity layer for cloud stor-
age. In ACM Conference on Computer and Commu-
nications Security, pages 187–198, 2009.

[16] Kevin D. Bowers, Marten van Dijk, Ari Juels, Alina
Oprea, and Ronald L. Rivest. How to tell if your
cloud ﬁles are vulnerable to drive crashes. In ACM
Conference on Computer and Communications Se-
curity, pages 501–514, 2011.

[17] Jin-yi Cai, Richard J. Lipton, Robert Sedgewick,
and Andrew Chi-Chih Yao. Towards uncheatable
benchmarks.
In Proceedings of the Eigth Annual
Structure in Complexity Theory Conference, San
Diego, CA, USA, May 18-21, 1993, pages 2–11, 1993.

[18] Reza Curtmola, Osama Khan, Randal C. Burns,
and Giuseppe Ateniese. MR-PDP: Multiple-Replica
Provable Data Possession. In ICDCS, pages 411–420,
2008.

[19] Dan Dobre, Ghassan Karame, Wenting Li, Matthias
Majuntke, Neeraj Suri, and Marko Vukoli´c. Pow-
erstore: Proofs of writing for eﬃcient and robust
storage. In Proceedings of the 2013 ACM SIGSAC
Conference on Computer &#38; Communications
Security, CCS ’13, pages 285–298, New York, NY,
USA, 2013. ACM.

[20] Stefan Dziembowski, Sebastian Faust, Vladimir Kol-
mogorov, and Krzysztof Pietrzak. Proofs of space.
In Rosario Gennaro and Matthew Robshaw, editors,
Advances in Cryptology - CRYPTO 2015 - 35th An-
nual Cryptology Conference, Santa Barbara, CA,
USA, August 16-20, 2015, Proceedings, Part II, vol-
ume 9216 of Lecture Notes in Computer Science,
pages 585–605. Springer, 2015.

[21] C. Christopher Erway, Alptekin K¨up¸c¨u, Charalam-
pos Papamanthou, and Roberto Tamassia. Dynamic
provable data possession. In ACM Conference on
Computer and Communications Security, pages 213–
222, 2009.

[22] Mohammad Etemad and Alptekin K¨up¸c¨u. Transpar-
ent, distributed, and replicated dynamic provable
data possession. In Proceedings of the 11th Inter-
national Conference on Applied Cryptography and
Network Security, ACNS’13, pages 1–18, Berlin, Hei-
delberg, 2013. Springer-Verlag.

[26] Ghassan Karame and Srdjan Capkun. Low-cost
client puzzles based on modular exponentiation. In
Computer Security - ESORICS 2010, 15th Euro-
pean Symposium on Research in Computer Security,
Athens, Greece, September 20-22, 2010. Proceedings,
pages 679–697, 2010.

[27] Neal Koblitz. A Course in Number Theory and
Cryptography. Springer-Verlag New York, Inc., New
York, NY, USA, 1987.

[28] ¸Cetin Kaya Ko¸c, Tolga Acar, and Burton S. Kaliski,
Jr. Analyzing and comparing montgomery multipli-
cation algorithms. IEEE Micro, 16(3):26–33, June
1996.

[29] Rudolf Lidl and Harald Niederreiter. Introduction
to Finite Fields and Their Applications. Cambridge
University Press, New York, NY, USA, 1986.

[30] Yadi Ma, Thyaga Nandagopal, Krishna P. N. Put-
taswamy, and Suman Banerjee. An ensemble of
replication and erasure codes for cloud ﬁle systems.
In Proceedings of the IEEE INFOCOM 2013, Turin,
Italy, April 14-19, 2013, pages 1276–1284, 2013.

[31] NetEm.

NetEm,

Founda-
tion. Website, 2009.
Available online at
http://www.linuxfoundation.org/collaborate/
workgroups/networking/netem.

the

Linux

[32] Zachary N. J. Peterson, Mark Gondree, and Robert
Beverly. A position paper on data sovereignty: The
importance of geolocating data in the cloud.
In
Proceedings of the 3rd USENIX Conference on Hot
Topics in Cloud Computing, HotCloud’11, pages 9–9,
Berkeley, CA, USA, 2011. USENIX Association.

[33] R. L. Rivest, A. Shamir, and D. A. Wagner. Time-
lock puzzles and timed-release crypto. Technical
report, Cambridge, MA, USA, 1996.

[34] Hovav Shacham and Brent Waters. Compact Proofs
of Retrievability. In ASIACRYPT, pages 90–107,
2008.

[35] Marten van Dijk, Ari Juels, Alina Oprea, Ronald L.
Rivest, Emil Stefanov, and Nikos Triandopoulos.
Hourglass schemes: how to prove that cloud ﬁles
are encrypted. In Ting Yu, George Danezis, and
Virgil D. Gligor, editors, ACM Conference on Com-
puter and Communications Security, pages 265–280.
ACM, 2012.

[23] Sanjay Ghemawat, Howard Gobioﬀ, and Shun-Tak
Leung. The google ﬁle system. In Proceedings of the
Nineteenth ACM Symposium on Operating Systems
Principles, SOSP ’03, pages 29–43, New York, NY,
USA, 2003. ACM.

[36] Gaven J. Watson, Reihaneh Safavi-Naini, Mohsen
Alimomeni, Michael E. Locasto, and Shivaramakr-
ishnan Narayan. LoSt: location based storage. In
Ting Yu, Srdjan Capkun, and Seny Kamara, editors,
CCSW, pages 59–70. ACM, 2012.

[24] Jim Gray. Distributed computing economics. Queue,

6(3):63–68, May 2008.

[25] Ari Juels and Burton S. Kaliski Jr. PORs: Proofs Of
Retrievability for Large Files. In ACM Conference
on Computer and Communications Security, pages
584–597, 2007.

A MR-PDP

In what follows, we brieﬂy describe the multi-replica
provable data possession scheme by Curtmola et
al. [18]. Here, the user ﬁrst splits the ﬁle D into

1066  25th USENIX Security Symposium 

USENIX Association

16

n blocks d1, . . . ,d n ∈ ZN. Let p = 2p(cid:30) + 1,q = 2q(cid:30) + 1
be safe primes, and N = pq an RSA modulus; more-
over, let g be a generator of the quadratic residues
of Z∗N, and e,d a pair of integers such that e· d = 1
mod p(cid:30)q(cid:30). The user creates authentication tags for
each block i ∈ [1,n] by computing Ti ← (h(v||i)gdi)d
mod N, where h : {0,1}∗ → ZN is a hash function and
v ∈ ZN.
Subsequently, each replica is created by the user
as follows: d(k)
i ← di + PRF(k||i) where PRF denotes
a pseudorandom function. The user sends to the
service provider the tags {Ti}, the original ﬁle blocks
{di}, and the replica blocks d(k)
At the veriﬁcation stage, the user selects a replica
k and creates a (pseudo-)random challenge set I =
{(k1,i1), . . . ,(k (cid:29),i(cid:29))} where k j denotes the replica num-
ber and i j the block index.
In addition, the user
picks s ∈ Z∗N, and computes gs = gs mod N. The
challenge query then comprises the set I and the
value gs which are both sent to the service provider
who stores replica k. The service provider then com-
putes the response (T,σ ) as follows and sends it back
to the veriﬁer:

.

i

T ← ∏
i∈I

Ti, σ ← g

(k j )
i j

∑1≤ j≤(cid:29) d
s

Finally, the user checks whether:

σ ?

=(cid:31) T e
∏h(v||i)

g∑1≤ j≤(cid:29) PRF(k j||i j)(cid:30)s

B POR Schemes of Shacham and

Waters

In what follows, we brieﬂy describe the private POR
scheme by Shacham and Waters [34]. This scheme
leverages a pseudo-random function PRF. Here, the
user ﬁrst applies an erasure code to the ﬁle and
then splits it into n blocks d1, . . . ,d n ∈ Zp, where p
is a large prime. The user then chooses a random
α ∈R Zp and creates for each block an authentication
value as follows:

σi = PRF ˚key(i) +α · di ∈ Zp.

(15)

The blocks {di} and their authentication values {σi}
are all stored at the service provider in D∗.
At the POR veriﬁcation stage, the veriﬁer (here,
the user) chooses a random challenge set I ⊂{1, . . . ,n}
of size (cid:29), and (cid:29) random coeﬃcients νi ∈R Zp. The
challenge query then is the set Q := {(i,νi)}i∈I which
is sent to the prover (here, service provider). The

prover computes the response (σ , µ) as follows and
sends it back to the veriﬁer:

σ ← ∑
(i,νi)∈Q

νiσi, µ ← ∑
(i,νi)∈Q

νidi.

Finally, the veriﬁer checks the correctness of the
response:

σ ?
= αµ + ∑
(i,νi)∈Q

νi · PRF(i).

C Improving User Veriﬁcation in Mir-

ror

In what follows, we describe a number of optimiza-
tions that we adopted in our implementation in order
to reduce the eﬀort in verifying the service provider’s
response.

Using either g or h: Recall that the service
provider’s response involves powers of g and of h
which have order p(cid:30) and q(cid:30), respectively. One tech-
nique that allows to reduce the eﬀort on the user’s
side is to rely on either g or h. That is, at the be-
ginning of the veriﬁcation step, the user randomly
decides whether only g or only h shall be taken into
account. For example, let us assume that the choice
falls on g. Then, the user proceeds as follows:

1. The user computes:

˜σ := σ q(cid:30) ·

(cid:29)

c=1(cid:29) s
∏
∏

j=1

ic, j(cid:28)−(q(cid:30)·νc)

gk

.

∏
k∈R

(16)

Here, we exploit the fact that (he)q(cid:30) = 1 for any
e.

2. The user checks if:

(cid:29) s
∏

j=1

ε j+|R|(cid:28)q(cid:30)

µ j

= ˜σ .

(17)

This approach incurs two additional exponentiations
but completely eliminates the need to compute the
expressions for the values h.

Representing LFSRs by Pre-computed Matri-
ces: According to our experiments, suitable parame-
ter choices are to choose the length λ of the secret
LFSR quite small, e.g., equal to 2, while the block
size s is comparatively large. This motivates the
following optimization.

:= (a(k)
t

, . . . ,a (k)
Let A(k)
t
k ∈ {1, . . . ,r}. That is, A(k)
the k-th LFSR while A(k)

t+λ−1) for any t ≥ 1 and any
1 denotes the initial state of
t denotes the state after t − 1

USENIX Association  

25th USENIX Security Symposium  1067

17

clocks. Recall that we consider r LFSR sequences
which are all generated by the same feedback function.
Namely, it holds for any t ≥ 1 and any k ∈ {1, . . . ,r}
that a(k)
t+i−1. Due to the linearity of
the feedback function, there exists a λ ×λ matrix M,
called the companion matrix, for which it holds that:

i=1 αi · a(k)

t+λ = ∑λ

Mt · A(k)

1 = A(k)
t+1,

π(ic,1) + a(k)

π(ic,1)+1 + . . . + a(k)

∀t ≥ 0.
π(ic,1)+s−1(cid:30)

(18)
Recall that we aim to compute for each i ∈ I the value
k∈R(cid:31)a(k)
∑
(19)
where π : N× N → N is a mapping such that g(k)
i, j =
π(i, j) = ga(k)
g(k)
and to raise g by the resulting value.
The idea is now to combine as many computations
as possible to reduce the overall eﬀort. To this end,
the goal is to sum-up for each k ∈ R and each i ∈ I
the following internal states:

t

A(k)
π(i,1) = (a(k)

π(i,1), . . . ,a (k)

π(i,1)+λ−1)

...

A(k)
π(i,1)+(cid:22)s/λ(cid:21)·λ = (a(k)

π(i,1)+(cid:22)s/λ(cid:21)·λ , . . . ,a (k)

π(i,1)+(cid:22)s/λ(cid:21)·λ−1)

This can be accomplished by computing:

(cid:29)∑

i∈I

Mπ(i,1)(cid:28)·(cid:29)(cid:22)s/λ(cid:21)∑

j=0

M j·λ(cid:28)·(cid:29)∑

k∈R

1 (cid:28) .

A(k)

(20)

Observe that ∑(cid:22)s/λ(cid:21)
j=0 M j·λ is independent of the current
challenge and can be precomputed. Moreover, due
to the fact that we aim for a small LFSR length, e.g.,
λ = 2, the user may consider to precompute the value
in the last bracket for any choice of R, yielding an
additional storage eﬀort of λ · (2r−1) sectors. In such

case, the computation would boil down to the eﬀort
of computing the ﬁrst bracket only. We note that if
λ does not divide s, then the user has to compute in
addition:

(cid:29)∑

i∈I

Mπ(i,1)(cid:28)· M(cid:20)s/λ(cid:19)·λ ·(cid:29)∑

1 (cid:28) ,

A(k)

k∈R

(21)

and to add the sum of the ﬁrst s mod λ entries to
the value computed above. Also here, similar pre-
computations can be done to accelerate this step.

D Valid Relations
We now explain why (cid:28)v = (v1, . . . ,v n·s) ∈ Zn·s such that

n·s
∏
i=1

gvi
i = 1,

(22)

18

represents the only type of equations that allows the
provider to compute missing values g j from known
values gi.

To see why, recall that g j = ga j where (a j) j rep-
resents an LFSR-sequence. More precisely, this se-
quence is deﬁned by the feedback polynomial and the
initial state, i.e., the ﬁrst λ entries. Without knowing
these entries, it is (information-theoretically) impos-
sible to determine a j for larger indices. Also, any
element in (a j) j is a linear combination of the initial
state values. Let us denote this combination by L1.
This can be relaxed as follows: the knowledge of any
λ elements a j1, . . . ,a jλ of the sequence (a j) j allows
almost always to compute another element by an ap-
propriate linear combination (say L2) of a j1, . . . ,a jλ .
Notice that the coeﬃcients of L2 have to be a linear
combination of the coeﬃcients (or shifted versions)
of L1 (this is in inherent property of LFSRs). This
is exactly the deﬁnition of “valid relations” given in
Equation (12).

E Impact of the Block Size on the
Performance of MR-PDP and Mir-
ror

]
s
[
 
y
c
n
e
t
a
L

 1e+006
 100000
 10000
 1000
 100
 10
 1
 0.1

Store
Replicate
Verify (Server)
Verify (Client)

1

8

32

512

1024 2048

Block size [KB]

(a) Impact of block size on the performance
of MR-PDP [18].

]
s
[
 
y
c
n
e
a
L

t

 100000
 10000
 1000
 100
 10
 1
 0.1

Store
Replicate
Verify (Server)
Verify (Client)

1

8

32

512

1024 2048

Block size [KB]

(b) Impact of block size on the performance
of Mirror.

Figure 4: Impact of the block size on the performance
of MR-PDP [18] and Mirror.

1068  25th USENIX Security Symposium 

USENIX Association

