Machine Learning Classiﬁcation over Encrypted Data

Raphael Bost

DGA MI

MIT

Raluca Ada Popa
ETH Zürich and MIT

rpopa@inf.ethz.ch

Stephen Tu

MIT

Shaﬁ Goldwasser

MIT

stephentu@csail.mit.edu

shaﬁ@theory.csail.mit.edu

raphael_bost@alumni.brown.edu

Abstract—Machine learning classiﬁcation is used for numer-
ous tasks nowadays, such as medical or genomics predictions,
spam detection, face recognition, and ﬁnancial predictions. Due
to privacy concerns, in some of these applications, it is important
that the data and the classiﬁer remain conﬁdential.

In this work, we construct three major classiﬁcation protocols
that satisfy this privacy constraint: hyperplane decision, Naïve
Bayes, and decision trees. We also enable these protocols to be
combined with AdaBoost. At the basis of these constructions is
a new library of building blocks, which enables constructing a
wide range of privacy-preserving classiﬁers; we demonstrate how
this library can be used to construct other classiﬁers than the
three mentioned above, such as a multiplexer and a face detection
classiﬁer.

We implemented and evaluated our library and our classiﬁers.
Our protocols are efﬁcient, taking milliseconds to a few seconds
to perform a classiﬁcation when running on real medical datasets.

I.

INTRODUCTION

Classiﬁers are an invaluable tool for many tasks today,
such as medical or genomics predictions, spam detection, face
recognition, and ﬁnance. Many of these applications handle
sensitive data [1], [2], [3], so it is important that the data and
the classiﬁer remain private.

Consider the typical setup of supervised learning, depicted in
Figure 1. Supervised learning algorithms consist of two phases:
(i) the training phase during which the algorithm learns a model
w from a data set of labeled examples, and (ii) the classiﬁcation
phase that runs a classiﬁer C over a previously unseen feature
vector x, using the model w to output a prediction C(x, w).
In applications that handle sensitive data, it is important
that the feature vector x and the model w remain secret to
one or some of the parties involved. Consider the example
of a medical study or a hospital having a model built out of
the private medical proﬁles of some patients; the model is
sensitive because it can leak information about the patients,
and its usage has to be HIPAA1 compliant. A client wants
to use the model to make a prediction about her health (e.g.,

1Health Insurance Portability and Accountability Act of 1996

Permission to freely reproduce all or part of this paper for noncommercial
purposes is granted provided that copies bear this notice and the full citation
on the ﬁrst page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the ﬁrst-named author
(for reproduction of an entire paper only), and the author’s employer if the
paper was prepared within the scope of employment.
NDSS ’15, 8-11 February 2015, San Diego, CA, USA
Copyright 2015 Internet Society, ISBN 1-891562-38-X
http://dx.doi.org/10.14722/ndss.2015.23241

Fig. 1: Model overview. Each shaded box indicates private data
that should be accessible to only one party: the dataset and
the model to the server, and the input and prediction result
to the client. Each straight non-dashed rectangle indicates an
algorithm, single arrows indicate inputs to these algorithms,
and double arrows indicate outputs.

if she is likely to contract a certain disease, or if she would
be treated successfully at the hospital), but does not want to
reveal her sensitive medical proﬁle. Ideally, the hospital and
the client run a protocol at the end of which the client learns
one bit (“yes/no”), and neither party learns anything else about
the other party’s input. A similar setting arises for a ﬁnancial
institution (e.g., an insurance company) holding a sensitive
model, and a customer wanting to estimate rates or quality of
service based on her personal information.

Throughout this paper, we refer to this goal shortly as
privacy-preserving classiﬁcation. Concretely, a client has a
private input represented as a feature vector x, and the server
has a private input consisting of a private model w. The way
the model w is obtained is independent of our protocols here.
For example, the server could have computed the model w after
running the training phase on plaintext data as usual. Only the
classiﬁcation needs to be privacy-preserving: the client should
learn C(x, w) but nothing else about the model w, while the
server should not learn anything about the client’s input or the
classiﬁcation result.

In this work, we construct efﬁcient privacy-preserving
protocols for three of the most common classiﬁers: hyperplane
decision, Naïve Bayes, and decision trees, as well as a more
general classiﬁer combining these using AdaBoost. These
classiﬁers are widely used – even though there are many
machine learning algorithms, most of them end up using one
of these three classiﬁers, as described in Table I.

While generic secure multi-party computation [4], [5], [6],
[7], [8] can implement any classiﬁer in principle, due to
their generality, such schemes are not efﬁcient for common
classiﬁers. As described in Section X-E, on a small classiﬁcation
instance, such tools ([6], [8]) ran out of memory on a powerful
machine with 256GB of RAM; also, on an artiﬁcially simpliﬁed
classiﬁcation instance, these protocols ran ≈ 500 times slower

server data set training phase model w classification C client feature vector x prediction C(w,x) Machine learning algorithm Classiﬁer
Perceptron
Least squares
Fischer linear discriminant
Support vector machine
Naive Bayes
Decision trees (ID3/C4.5)

Hyperplane decision
Hyperplane decision
Hyperplane decision
Hyperplane decision
Naïve Bayes
Decision trees

TABLE I: Machine learning algorithms and their classiﬁers,
deﬁned in Section III-A.

than our protocols ran on the non-simpliﬁed instance.

Hence, protocols specialized to the classiﬁcation problem
promise better performance. However, most existing work
in machine learning and privacy [9], [10], [11], [12], [13],
[14], [15] focuses on preserving privacy during the training
phase, and does not address classiﬁcation. The few works
on privacy-preserving classiﬁcation either consider a weaker
security setting in which the client learns the model [16] or
focus on speciﬁc classiﬁers (e.g., face detectors [17], [18], [19],
[20]) that are useful in limited situations.

Designing efﬁcient privacy-preserving classiﬁcation faces
two main challenges. The ﬁrst is that the computation performed
over sensitive data by some classiﬁers is quite complex (e.g.,
decision trees), making it hard to support efﬁciently. The
second is providing a solution that is more generic than the
three classiﬁers: constructing a separate solution for each
classiﬁer does not provide insight into how to combine these
classiﬁers or how to construct other classiﬁers. Even though we
contribute privacy-preserving protocols for three of the most
common classiﬁers, various settings use other classiﬁers or use
a combination of these three classiﬁers (e.g., AdaBoost). We
address these challenges using two key techniques.

Our main technique is to identify a set of core operations
over encrypted data that underlie many classiﬁcation protocols.
We found these operations to be comparison, argmax, and dot
product. We use efﬁcient protocols for each one of these, either
by improving existing schemes (e.g., for comparison) or by
constructing new schemes (e.g., for argmax).

Our second technique is to design these building blocks
in a composable way, with regard to both functionality and
security. To achieve this goal, we use a set of sub-techniques:
The input and output of all our building blocks are data
encrypted with additively homomorphic encryption. In
addition, we provide a mechanism to switch from one
encryption scheme to another. Intuitively, this enables a
building block’s output to become the input of another
building block;
The API of these building blocks is ﬂexible: even
though each building block computes a ﬁxed function,
it allows a choice of which party provides the inputs
to the protocol, which party obtains the output of the
computation, and whether the output is encrypted or
decrypted;
The security of these protocols composes using modu-
lar sequential composition [21].

•

•

•

We emphasize that the contribution of our building blocks
library goes beyond the classiﬁers we build in this paper: a user

2

of the library can construct other privacy-preserving classiﬁers
in a modular fashion. To demonstrate this point, we use our
building blocks to construct a multiplexer and a classiﬁer for
face detection, as well as to combine our classiﬁers using
AdaBoost.

We then use these building blocks to construct novel
privacy-preserving protocols for three common classiﬁers.
Some of these classiﬁers incorporate additional techniques,
such as an efﬁcient evaluation of a decision tree with
fully homomorphic encryption (FHE) based on a polynomial
representation requiring only a small number of multiplications
and based on SIMD FHE slots (see Section VII-B). All
of our protocols are secure against passive adversaries (see
Section III-B3).

We also provide an implementation and an evaluation of
our building blocks and classiﬁers. We evaluate our classiﬁers
on real datasets with private data about breast cancer, credit
card approval, audiology, and nursery data; our algorithms are
efﬁcient, running in milliseconds up to a few seconds, and
consume a modest amount of bandwidth.

The rest of the paper is organized as follows. Section II
describes related work, Section III provide the necessary
machine learning and cryptographic background, Section IV
presents our building blocks, Sections V–VIII describe our
classiﬁers, and Sections IX–X present our implementation and
evaluation results.

II. RELATED WORK

Our work is the ﬁrst to provide efﬁcient privacy-preserving

protocols for a broad class of classiﬁers.

Secure two-party computation protocols for generic func-
tions exist in theory [4], [5], [22], [23], [24] and in practice [6],
[7], [8]. However, these rely on heavy cryptographic machinery,
and applying them directly to our problem setting would be
too inefﬁcient as exempliﬁed in Section X-E.

Previous work focusing on privacy-preserving machine
learning can be broadly divided into two categories: (i)
techniques for privacy-preserving training, and (ii) techniques
for privacy-preserving classiﬁcation (recall the distinction from
Figure 1). Most existing work falls in the ﬁrst category, which
we discuss in Section II-A. Our work falls in the second
category, where little work has been done, as we discuss in
Section II-B. We also mention work related to the building
blocks we use in our protocols in Section II-C.

It is worth mentioning that our work on privacy-preserving
classiﬁcation is complementary to work on differential privacy
in the machine learning community (see e.g. [25]). Our work
aims to hide each user’s input data to the classiﬁcation phase,
whereas differential privacy seeks to construct classiﬁers/models
from sensitive user training data that leak a bounded amount
of information about each individual in the training data set.

A. Privacy-preserving training

A set of techniques have been developed for privacy-
preserving training algorithms such as Naïve Bayes [14], [11],
[12], decision trees [13], [9], linear discriminant classiﬁers [10],
and more general kernel methods [26].

Grapel et al. [15] show how to train several machine
learning classiﬁers using a somewhat homomorphic encryption
scheme. They focus on a few simple classiﬁers (e.g. the linear
means classiﬁer), and do not elaborate on more complex
algorithms such as support vector machines. They also support
private classiﬁcation, but in a weaker security model where
the client learns more about the model than just the ﬁnal sign
of the classiﬁcation. Indeed, performing the ﬁnal comparison
with fully homomorphic encryption (FHE) alone is inefﬁcient,
a difﬁculty we overcome with an interactive setting.

B. Privacy-preserving classiﬁcation

Little work has been done to address the general problem
of privacy-preserving classiﬁcation in practice; previous work
focuses on a weaker security setting (in which the client learns
the model) and/or only supports speciﬁc classiﬁers.

In Bos et al. [16], a third party can compute medical
prediction functions over the encrypted data of a patient
using fully homomorphic encryption. In their setting, everyone
(including the patient) knows the predictive model, and their
algorithm hides only the input of the patient from the cloud.
Our protocols, on the other hand, also hide the model from
the patient. Their algorithms cannot be applied to our setting
because they leak more information than just the bit of the
prediction to the patient. Furthermore, our techniques are
notably different; using FHE directly for our classiﬁers would
result in signiﬁcant overheads.

Barni et al. [27], [28] construct secure evaluation of linear
branching programs, which they use to implement a secure
classiﬁer of ECG signals. Their technique is based on ﬁnely-
tuned garbled circuits. By comparison, our construction is not
limited to branching programs (or decision trees), and our
evaluation shows that our construction is twice as fast on
branching programs. In a subsequent work [29], Barni et al.
study secure classiﬁers based on neural networks, which is
a generalization of the perceptron classiﬁers, and hence also
covered by our work.

Other works [17], [18], [19], [20] construct speciﬁc face
recognition or detection classiﬁers. We focus on providing a
set of generic classiﬁers and building blocks to construct more
complex classiﬁers. In Section X-A2, we show how to construct
a private face detection classiﬁer using the modularity of our
techniques.

C. Work related to our building blocks

Two of the basic components we use are private comparison
and private computation of dot products. These items have
been well-studied previously; see [4], [30], [31], [32], [33],
[19], [34] for comparison techniques and [35], [36], [37], [19]
for techniques to compute dot products. Section IV-A discusses
how we build on these tools.

III. BACKGROUND AND PRELIMINARIES

A. Classiﬁcation in machine learning algorithms

The user’s input x is a vector of d elements x =
(x1, . . . , xd) ∈ Rd, called a feature vector. To classify the
input x means to evaluate a classiﬁcation function Cw :
Rd (cid:55)→ {c1, ..., ck} on x. The output is ck∗ = Cw(x), where

k∗
∈ {1 . . . k}; ck∗ is the class to which x corresponds, based
on the model w. For ease of notation, we often write k∗ instead
of ck∗, namely k∗ = Cw(x).

We now describe how three popular classiﬁers work on
regular, unencrypted data. These classiﬁers differ in the model
w and the function Cw. For more details, we refer the reader
to [38].
Hyperplane decision-based classiﬁers. For this classiﬁer,
the model w consists of k vectors in Rd (w = {wi}k
i=1). The
classiﬁer is (cf. [38]):

∗

k

= argmax

i∈[k] (cid:104)wi, x(cid:105),

(1)

where (cid:104)wi, x(cid:105) denotes inner product between wi and x.
We now explain how Eq. (1) captures many common
machine learning algorithms. A hyperplane based classiﬁer
typically works with a hypothesis space H equipped with
an inner product (cid:104)·,·(cid:105). This classiﬁer usually solves a binary
classiﬁcation problem (k = 2): given a user input x, x is
classiﬁed in class c2 if (cid:104)w, φ(x)(cid:105) ≥ 0, otherwise it is labeled as
part of class c1. Here, φ : Rd (cid:55)→ H denotes the feature mapping
from Rd to H [38]. In this work, we focus on the case when
H = Rd and note that a large class of inﬁnite dimensional
spaces can be approximated with a ﬁnite dimensional space
(as in [39]), including the popular gaussian kernel (RBF). In
this case, φ(x) = x or φ(x) = P x for a randomized projection
matrix P chosen during training. Notice that P x consists
solely of inner products; we will show how to support private
evaluation of inner products later, so for simplicity we drop P
from the discussion. To extend such a classiﬁer from 2 classes
to k classes, we use one of the most common approaches,
one-versus-all, where k different models {wi}k
i=1 are trained
to discriminate each class from all the others. The decision
rule is then given by (cf. [38]) to be Eq. (1). This framework
is general enough to cover many common algorithms, such as
support vector machines (SVMs), logistic regression, and least
squares.
Naïve Bayes classiﬁers. For this classiﬁer, the model w
consists of various probabilities: the probability that each class
i=1, and the probabilities that an
ci occurs, namely {p(C = ci)}k
element xj of x occurs in a certain class ci. More concretely,
the latter is the probability of the j-th component xj of x
to be v when x belongs to category ci; this is denoted by
i=1, where Dj is Xj’s
{{{p(Xj = v|C = ci)}v∈Dj}d
domain2. The classiﬁcation function, using a maximum a
posteriori decision rule, works by choosing the class with
the highest posterior probability:

j=1}k

∗

k

= argmax

i∈[k]

= argmax

i∈[k]

= argmax

i∈[k]

p(C = ci|X = x)
p(C = ci, X = x)

p(C = ci, X1 = x1, . . . , Xd = xd)

where the second equality follows from applying Bayes’ rule
(we omitted the normalizing factor p(X = x) because it is the
same for a ﬁxed x).

2Be careful to distinguish between Xj, the probabilistic random variable
representing the values taken by the j-th feature of user’s input, and xj, the
actual value taken by the speciﬁc vector x.

3

Fig. 2: Decision tree

The Naïve Bayes model assumes that p(C = ci, X = x)

has the following factorization:

p(C = ci, X1 = x1, . . . , Xd = xd)

d(cid:89)

j=1

2) Cryptographic assumptions: We prove that our protocols
are secure based on the semantic security [44] of the above
cryptosystems. These cryptosytems rely on standard and well-
studied computational assumptions: the Quadratic Residuosity
assumption, the Decisional Composite Residuosity assumption,
and the Ring Learning With Error (RLWE) assumption.

3) Adversarial model: We prove security of our protocols
using the secure two-party computation framework for passive
adversaries (or honest-but-curious [44]) deﬁned in our extended
paper [45]. To explain what a passive adversary is, at a high
level, consider that a party called party A is compromised by
such an adversary. This adversary tries to learn as much private
information about the input of the other party by watching all
the information party A receives; nevertheless, this adversary
cannot prevent party A from following the prescribed protocol
faithfully (hence, it is not an active adversary).

To enable us to compose various protocols into a bigger
protocol securely, we invoke modular sequential composition
(see our extended paper [45]).

C. Notation

= p(C = ci)

p(Xj = xj|C = ci),

All our protocols are between two parties: parties A and B
for our building blocks and parties C (client) and S (server)
for our classiﬁers.

namely, each of the d features are conditionally independent
given the class. For simplicity, we assume that the domain
of the features values (the xi’s) is discrete and ﬁnite, so the
p(Xj = xj|C = ci)’s are probability masses.
Decision trees. A decision tree is a non-parametric classiﬁer
which works by partitioning the feature vector space one
attribute at a time; interior nodes in the tree correspond to
partitioning rules, and leaf nodes correspond to class labels.
A feature vector x is classiﬁed by walking the tree starting
from the root, using the partitioning rule at each node to decide
which branch to take until a leaf node is encountered. The
class at the leaf node is the result of the classiﬁcation.

Figure 2 gives an example of a decision tree. The model
consists of the structure of the tree and the decision criteria at
each node (in this case the thresholds w1, . . . , w4).

B. Cryptographic preliminaries

1) Cryptosystems: In this work, we use three additively ho-
momorphic cryptosystems. A public-key encryption scheme HE
is additively homomorphic if, given two encrypted messages
HE.Enc(a) and HE.Enc(b), there exists a public-key operation
⊕ such that HE.Enc(a)⊕ HE.Enc(b) is an encryption of a + b.
We emphasize that these are homomorphic only for addition,
which makes them efﬁcient, unlike fully homomorphic encryp-
tion [40], which supports any function. The cryptosystems we
use are:

Inputs and outputs of our building blocks are either
unencrypted or encrypted with an additively homomorphic
encryption scheme. We use the following notation. The plaintext
space of QR is F2 (bits), and we denote by [b] a bit b encrypted
under QR; the plaintext space of Paillier is ZN where N is the

public modulus of Paillier, and we denote by(cid:74)m(cid:75) an integer

m encrypted under Paillier. The plaintext space of the FHE
scheme is F2. We denote by SKP and PKP , a secret and a
public key for Paillier, respectively. Also, we denote by SKQR
and PKQR, a secret and a public key for QR.

For a constant b, a ← b means that a is assigned the value
of b. For a distribution D, a ← D means that a gets a sample
from D.

IV. BUILDING BLOCKS

In this section, we develop a library of building blocks,
which we later use to build our classiﬁers. We designed this
library to also enable constructing other classiﬁers than the
ones described in our paper. The building blocks in this section
combine existing techniques with either new techniques or new
optimizations.

Proofs. We provide formal cryptographic proofs of the security
and correctness of the protocols. This task naturally requires
space beyond the page limit. Hence, in this paper, we provide
only the intuition behind the proofs, and delegate all formal
proofs to the extended paper [45].

A. Comparison

1)

2)
3)

the QR (Quadratic Residuosity) cryptosystem of
Goldwasser-Micali [41],
the Paillier cryptosystem [42], and
a leveled fully homomorphic encryption (FHE)
scheme, HELib [43]

We now describe our comparison protocol. In order for
this protocol to be used in a wide range of classiﬁers, its
setup needs to be ﬂexible: namely, it has to support a range of
choices regarding which party gets the input, which party gets
the output, and whether the input or output are encrypted or

4

c1c2c3c4c5x1>w1x1w1x2w2x2>w2x3w3x4w4x4>w4Type

1
2
3
4
5

Input A

PKP , PKQR, a

PKP , SKQR,(cid:74)a(cid:75),(cid:74)b(cid:75)
PKP , SKQR,(cid:74)a(cid:75),(cid:74)b(cid:75)
PKP , PKQR,(cid:74)a(cid:75),(cid:74)b(cid:75)
PKP , PKQR,(cid:74)a(cid:75),(cid:74)b(cid:75)

Input B

SKP ,SKQR, b
SKP ,PKQR
SKP ,PKQR
SKP ,SKQR
SKP ,SKQR

Output A Output B Implementation
[a < b]

–

–

a ≤ b
[a ≤ b]
[a ≤ b]

[a ≤ b]
[a ≤ b]
a ≤ b

–

Sec. IV-A1
Sec. IV-A2
Sec. IV-A2
Sec. IV-A3
Sec. IV-A3

TABLE II: The API of our comparison protocol and its implementation. There are ﬁve types of comparisons each having a different setup.

not. Table II shows the various ways our comparison protocol
can be used. In each case, each party learns nothing else about
the other party’s input other than what Table II indicates as
the output.

We implemented each row of Table II by modifying existing
protocols. We explain only the modiﬁcations here, and defer
full protocol descriptions to our extended paper [45].

There are at least two approaches to performing comparison
efﬁciently: using specialized homomorphic encryption [30],
[31], [17], [32], or using garbled circuits [46]. We compared
empirically the performance of these approaches and concluded
that the former is more efﬁcient for comparison of encrypted
values, and the second is more efﬁcient for comparison of
unencrypted values.

1) Comparison with unencrypted inputs (Row 1): To com-
pare unencrypted inputs, we use garbled circuits implemented
with the state-of-the-art garbling scheme of Bellare et al. [46],
the short circuit for comparison of Kolesnikov et al. [34] and
a well-known oblivious transfer (OT) scheme due to Naor and
Pinkas [47]. Since most of our other building blocks expect
inputs encrypted with homomorphic encryption, one also needs
to convert from a garbled output to homomorphic encryption
to enable composition. We can implement this easily using the
random shares technique in [48].

The above techniques combined give us the desired
comparison protocol. Actually, we can directly combine them
to build an even more efﬁcient protocol: we use an enhanced
comparison circuit that also takes as input a masking bit.
Using a garbled circuit and oblivious transfer, A will compute
(a < b)⊕ c where c is a bit randomly chosen by B. B will also
provide an encryption [c] of c, enabling A to compute [a < b]
using the homomorphic properties of QR.

2) Comparison with encrypted inputs (Rows 2, 3): Our
classiﬁers also require the ability to compare two encrypted
inputs. More speciﬁcally, suppose that party A wants to compare
two encrypted integers a and b, but party B holds the decryption
key. To implement this task, we slightly modify Veugen’s [32]
protocol: it uses a comparison with unencrypted inputs protocol
as a sub-procedure, and we replaced it with the comparison
protocol we just described above. This yields a protocol for the
setup in Row 2. To ensure that A receives the plaintext output
as in Row 3, B sends the encrypted result to A who decrypts
it. Our extended paper [45] provides the detailed protocol.

3) Reversed comparison over encrypted data (Row 4, 5):
In some cases, we want the result of the comparison to be
held by the party that does not hold the encrypted data. For
this, we modify Veugen’s protocol to reverse the outputs of
party A and party B: we achieve this by exchanging the role

of party A and party B in the last few steps of the protocol,
after invoking the comparison protocol with unencrypted inputs.
We do not present the details in the paper body because they
are not insightful, and instead include them in our extended
paper [45].

This results in a protocol whose speciﬁcation is in Row 4.
To obtain Row 5, A sends the encrypted result to B who can
decrypt it.

4) Negative integers comparison and sign determination:

Negative numbers are handled by the protocols above un-
changed. Even though the Paillier plaintext size is “positive”,
a negative number simply becomes a large number in the
plaintext space due to cyclicity of the space. As long as the
values encrypted are within a preset interval (−2(cid:96), 2(cid:96)) for
some ﬁxed (cid:96), Veugen’s protocol and the above protocols work
correctly.
integer(cid:74)b(cid:75). In this case, we simply compare to the encryption

In some cases, we need to compute the sign of an encrypted

of 0.

B. argmax over encrypted data

In this scenario, party A has k values a1, . . . , ak encrypted
under party B’s secret key and wants party B to know the
argmax over these values (the index of the largest value), but
neither party should learn anything else. For example, if A has

values(cid:74)1(cid:75),(cid:74)100(cid:75) and(cid:74)2(cid:75), B should learn that the second is

the largest value, but learn nothing else. In particular, B should
not learn the order relations between the ai’s.

Our protocol for argmax is shown in Protocol 1. We now

provide intuition into the protocol and its security.

Intuition. Let’s start with a strawman. To prevent B from
i=1, A applies a random

permutation π. The i-th element becomes(cid:74)a(cid:48)
i(cid:75) =(cid:74)aπ(i)(cid:75) instead
learning the order of the k values {ai}k
of(cid:74)ai(cid:75).
1(cid:75) and(cid:74)a(cid:48)
2(cid:75)
Now, A and B compare the ﬁrst two values(cid:74)a(cid:48)
the index, m, of the larger value, and tells A to compare(cid:74)a(cid:48)
m(cid:75)
3(cid:75) next. After iterating in this manner through all the
to (cid:74)a(cid:48)

using the comparison protocol from row 4 of Table II. B learns

k values, B determines the index m of the largest value. A
can then compute π−1(m) which represents the argmax in the
original, unpermuted order.

Since A applied a random permutation π, B does not learn
the ordering of the values. The problem, though, is that A
learns this ordering because, at every iteration, A knows the
value of m up to that step and π. One way to ﬁx this problem
is for B to compare every pair of inputs from A, but this would
result in a quadratic number of comparisons, which is too slow.

5

Instead, our protocol preserves the linear number of
comparisons from above. The idea is that, at each iteration,
once B determines which is the maximum of the two values
compared, B should randomize the encryption of this maximum
in such a way that A cannot link this value to one of the values
compared. B uses the Refresh procedure for the randomization
of Paillier ciphertexts. In the case where the “refresher” knows
the secret key, this can be seen as a decryption followed by a
re-encryption. If not, it can be seen as a multiplication by an
encryption of 0.

A difﬁculty is that, to randomize the encryption of the

must not receive this encryption because B has the key SKP
to decrypt it, which violates privacy. Instead, the idea is for A

random values, then B refreshes the ciphertext, and then A
removes the randomness ri and si it added.

maximum(cid:74)a(cid:48)
m(cid:75), B needs to get this encryption – however, B
itself to add noise ri and si to(cid:74)a(cid:48)
m(cid:75), so decryption at B yields
In the end, our protocol performs k − 1 encrypted compar-
isons of l bits integers and 7(k − 1) homomorphic operations
(refreshes, multiplications and subtractions). In terms of round
trips, we add k − 1 roundtrips to the comparison protocol, one
roundtrip per loop iteration.
Input A: k encrypted integers ((cid:74)a1(cid:75), . . . ,(cid:74)ak(cid:75)), the bit length

Protocol 1 argmax over encrypted data

l of the ai, and public keys PKQR and PKP
Input B: Secret keys SKP and SKQR, the bit length l
Output A: argmaxi ai

(cid:46) m(cid:48)
(cid:46) a(cid:48)

if bi is true then

2: A:(cid:74)max(cid:75) ←(cid:74)aπ(1)(cid:75)
1: A: chooses a random permutation π over {1, . . . , k}
3: B: m ← 1
4: for i = 2 to k do
5:

Using the comparison protocol (Sec. IV-A3), B gets
the bit bi = (max ≤ aπ(i))
A:(cid:74)m(cid:48)
i(cid:75) ←(cid:74)max(cid:75) ·(cid:74)ri(cid:75)
A picks two random integers ri, si ← (0, 2λ+l) ∩ Z
i(cid:75) ←(cid:74)aπ(i)(cid:75) ·(cid:74)si(cid:75)
A:(cid:74)a(cid:48)
i = max + ri
i(cid:75) to B
i(cid:75) and(cid:74)a(cid:48)
A sends(cid:74)m(cid:48)
i = aπ(i) + si
i(cid:75)
B:(cid:74)vi(cid:75) ← Refresh(cid:74)a(cid:48)
B: m ← i
B:(cid:74)vi(cid:75) ← Refresh(cid:74)m(cid:48)
i(cid:75)
B sends to A(cid:74)vi(cid:75)
B sends to A(cid:74)bi(cid:75)
A:(cid:74)max(cid:75) ←(cid:74)vi(cid:75) · (g−1 ·(cid:74)bi(cid:75))ri ·(cid:74)bi(cid:75)−si

6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20: end for
21: B sends m to A
22: A outputs π−1(m)

(cid:46) max = vi + (bi − 1) · ri − bi · ti

(cid:46) vi = a(cid:48)
(cid:46) vi = m(cid:48)

end if

else

i

i

cannot couple to a previously seen ciphertext. B does learn
the result of each comparison – however, since A applied a
random permutation before the comparison, B learns no useful
information. See our extended paper [45] for a complete proof.

C. Changing the encryption scheme

To enable us to compose various building blocks, we
developed a protocol for converting ciphertexts from one
encryption scheme to another while maintaining the underlying
plaintexts. We ﬁrst present a protocol that switches between
two encryption schemes with the same plaintext size (such as
QR and FHE over bits), and then present a different protocol
for switching from QR to Paillier.

Concretely, consider two additively homomorphic encryp-
tion schemes E1 and E2, both semantically secure with the

same plaintext space M. Let(cid:74).(cid:75)1 be an encryption using E1
and (cid:74).(cid:75)2 an encryption using E2. Consider that party B has
a value encrypted with PK1, (cid:74)c(cid:75)1. Our protocol, protocol 2,
enables A to obtain an encryption of c under E2,(cid:74)c(cid:75)2 without

the secret keys SK1 and SK2 for both schemes and A has the
corresponding public keys PK1 and PK2. Party A also has

revealing anything to B about c.

Protocol 2 Changing the encryption scheme

Protocol intuition. The idea is for A to add a random
noise r to the ciphertext using the homomorphic property of
E1. Then B decrypts the resulting value with E1 (obtaining
x+r ∈ M) and encrypts it with E2, sends the result to A which
of E2. Even though B was able to decrypt (cid:74)c(cid:48)(cid:75)1, B obtains
removes the randomness r using the homomorphic property
x + r ∈ M which hides x in an information-theoretic way (it
is a one-time pad).
Input A:(cid:74)c(cid:75)1 and public keys PK1 and PK2
Output A:(cid:74)c(cid:75)2
2: A sends(cid:74)c(cid:48)(cid:75)1 ←(cid:74)c(cid:75)1 ·(cid:74)r(cid:75)1 to B
1: A uniformly picks r ← M
4: B sends(cid:74)c(cid:48)(cid:75)2 to A
3: B decrypts c(cid:48) and re-encrypts with E2
5: A:(cid:74)c(cid:75)2 =(cid:74)c(cid:48)(cid:75)2 ·(cid:74)r(cid:75)−1
6: A outputs(cid:74)c(cid:75)2

Input B: Secret keys SK1 and SK2

2

Note that, for some schemes, the plaintext space M depends
on the secret keys. In this case, we must be sure that party A
can still choose uniformly elements of M without knowing it.
For example, for Paillier, M = Z∗
N (cid:39) Z∗
q where p and
q are the private primes. However, in this case, A can sample
noise in ZN that will not be in Z∗
N with negligible probability
(remember N is large – 1024 bits
(1 − 1
in our instantiation).

q ) ≈ 1 − 2√

p × Z∗

p )(1 − 1

N

Proposition 4.1: Protocol 1 is correct and secure in the

honest-but-curious model.

Proof intuition. The correctness property is straightforward.
Let’s argue security. A does not learn intermediary results in the
computation because of the security of the comparison protocol
and because she gets a refreshed ciphertext from B which A

Proposition 4.2: Protocol 2 is secure in the honest-but-

curious model.

In our classiﬁers, we use this protocol for M = {0, 1}
and the encryption schemes are QR (for E1) and an FHE
scheme over bits (for E2). In some cases, we might also want
to switch from QR to Paillier (e.g. reuse the encrypted result
of a comparison in a homomorphic computation), which has

6

a different message space. Note that we can simulate the
homomorphic XOR operation and a message space M = {0, 1}
with Paillier: we can easily compute the encryption of b1 ⊕ b2
under Paillier when at most one of the bi is encrypted (which
we explain in the next subsection). This is the case in our
setting because party A has the randomness r in the clear.

1) XOR with Paillier.: Suppose a party gets the bit b1
encrypted under Paillier’s encryption scheme, and that this
party only has the public key. This party knows the bit b2 in

the clear and wants to compute the encryption of(cid:74)b1 ⊕ b2(cid:75).

To do so, we just have to notice that

(cid:26) b1

b1 ⊕ b2 =

1 − b1

if b2 = 0
if b2 = 1

Hence, it is very easy to compute an encryption of b1 ⊕ b2
if we know the modulus N and the generator g (cf. Paillier’s
scheme construction):

(cid:74)b1 ⊕ b2(cid:75) =

(cid:26)(cid:74)b1(cid:75)
g(cid:74)b1(cid:75)−1 mod N 2

if b2 = 0
if b2 = 1

If we want to unveil the result to an adversary who knows
the original encryption of b1 (but not the secret key), we have
to refresh the result of the previous function to ensure semantic
security.

D. Computing dot products

For completeness, we include a straightforward algorithm
for computing dot products of two vectors, which relies on
Paillier’s homomorphic property.

Protocol 3 Private dot product
Input A: x = (x1, . . . , xd) ∈ Zd, public key PKP
Output A:(cid:74)(cid:104)x, y(cid:105)(cid:75)
Input B: y = (y1, . . . , yd) ∈ Zd, secret key SKP
2: A computes(cid:74)v(cid:75) =(cid:81)
(cid:46) v =(cid:80) yixi
1: B encrypts y1, . . . , yd and sends the encryptions(cid:74)yi(cid:75) to A
3: A re-randomizes and outputs(cid:74)v(cid:75)

i(cid:74)yi(cid:75)xi mod N 2

Proposition 4.3: Protocol 3 is secure in the honest-but-

curious model.

E. Dealing with ﬂoating point numbers

Although all our protocols manipulate integers, classiﬁers
usually use ﬂoating point numbers. Hence, when developing
classiﬁers with our protocol library, we must adapt our protocols
accordingly.

V. PRIVATE HYPERPLANE DECISION

Recall from Section III-A that this classiﬁer computes

∗

k

= argmax

i∈[k] (cid:104)wi, x(cid:105).

Now that we constructed our library of building blocks, it is
straightforward to implement this classiﬁer securely: the client

computes the encryption of(cid:74)(cid:104)wi, x(cid:105)(cid:75) for all i ∈ [k] using the

dot product protocol and then applies the argmax protocol
(Protocol 1) to the encrypted dot products.

Protocol 4 Private hyperplane decision
Client’s (C) Input: x = (x1, . . . , xd) ∈ Zd, public keys PKP
and PKQR
i=1 where ∀i ∈ [k], wi ∈ Zn, secret
Server’s (S) Input: {wi}k
keys SKP and SKQR
Client’s Output: argmax

i∈[k] (cid:104)wi, x(cid:105)

1: for i = 1 to k do
2:

3:

C and S run Protocol 3 for private dot product where
C is party A with input x and S is party B with input wi.

C gets(cid:74)vi(cid:75) the result of the protocol.
S the B, and(cid:74)v1(cid:75), . . . ,(cid:74)vk(cid:75) the input ciphertexts. C gets

(cid:46) vi ← (cid:104)x, wi(cid:105)
4: end for
5: C and S run Protocol 1 for argmax where C is the A, and

the result i0 of the protocol.

6: C outputs i0

(cid:46) i0 ← argmax
i∈[k]

vi

Proposition 5.1: Protocol 4 is secure in the honest-but-

curious model.

VI. SECURE NAÏVE BAYES CLASSIFIER

Section III-A describes the Naïve Bayes classiﬁer. The goal
is for the client to learn k∗ without learning anything about the
probabilities that constitute the model, and the server should
learn nothing about x. Recall that the features values domain
is discrete and ﬁnite.

As is typically done for numerical stability reasons, we

work with the logarithm of the probability distributions:
∗

k

= argmax

i∈[k]

= argmax

i∈[k]

log p(C = ci|X = x)

log p(C = ci) +

d(cid:88)

j=1



log p(Xj = xj|C = ci)
(2)

A. Preparing the model

Fortunately, most of the operations involved are either
additions or multiplications. As a consequence, a simple
solution is to multiply each ﬂoating point value by a constant
K (e.g. K = 252 for IEEE 754 doubles) and thus support
ﬁnite precision. We must also consider the bit length for
the comparisons. We show an example of a full analysis in
Section VI for the Naïve Bayes classiﬁer.

Since the Paillier encryption scheme works with integers,
we convert each log of a probability from above to an integer by
multiplying it with a large number K (recall that the plaintext
space of Paillier is large ≈ 21024 thus allowing for a large K),
thus still maintaining high accuracy. The issues due to using
integers for bayesian classiﬁcation have been previously studied
in [49], even though their setting was even more restricting

7

than ours. However, they use a similar idea to ours: shifting
the probabilities logarithms and use ﬁxed point representation.
As the only operations used in the classiﬁcation step are
additions and comparisons (cf. Equation (2)), we can just
multiply the conditional probabilities p(xj|ci) by a constant
K so to get integers everywhere, while keeping the same
classiﬁcation result.

For example, if we are able to compute the conditional
probabilities using IEEE 754 double precision ﬂoating point
numbers, with 52 bits of precision, then we can represent every
probability p as

p = m · 2e

where m binary representation is (m)2 = 1.d and d is a 52
bits integer. Hence we have 1 ≤ m < 2 and we can rewrite m
as

m =

m(cid:48)
252

(cid:48)
with m

∈ N ∩ [252, 253)

(cid:48)

vi = m

(cid:48)
vi = m

≥ 0. Then,

i · 2ei−52
i · 2δi · 2e∗−52
. We have K·vi = m(cid:48)

We are using this representation to ﬁnd a constant K such
that K · vi ∈ N for all i. As seen before, we can write the vi’s
as
Let e∗ = mini ei, and δi = ei − e∗
So let K = 252−e∗
i·2δi ∈ N. An important
thing to notice is that the vi’s can be very large integers (due to
δi), and this might cause overﬂows errors. However, remember
that we are doing all this to store logarithms of probabilities in
Paillier cyphertexts, and as Paillier plaintext space is very large
(more than 1024 bits in our setting) and δi’s remain small3.
Also notice that this shifting procedure can be done without
any loss of precision as we can directly work with the bit
representation of the ﬂoating points numbers.

Finally, we must also ensure that we do not overﬂow
Paillier’s message space when doing all the operations (homo-
morphic additions, comparisons, . . . ). If – as before – d is
the number of features, the maximum number of bits when
doing the computations will be lmax = d + 1 + (52 + δ∗)
where δ∗ = max δi: we have to add the probabilities for the d
features and the probability of the class label (the d + 1 term),
and each probability is encoded using (52 + δ∗) bits. Hence,
the value l used for the comparison protocols must be chosen
larger than lmax.

Hence, we must ensure that log2 N > lmax + 1 + λ where
λ is the security parameter and N is the modulus for Paillier’s
cryptosystem plaintext space (cf. Section IV-A2). This condition
is easily fulﬁlled as, for a good level of security, we have to
take log2 N ≥ 1024 and we usually take λ ≈ 100.
Let Dj be the domain of possible values of xj (the j-th
attribute of the feature vector x). The server prepares kd + 1
tables as part of the model, where K is computed as described
just before:
• One table for the priors on the classes P : P (i) =

(cid:100)K log p(C = ci)(cid:101).
3If the biggest δi is 10, the ratio between the smallest and the biggest

probability is of order 2210

= 21024 ...

• One table per feature j per class i, Ti,j: Ti,j(v) ≈

(cid:100)K log p(Xj = v|C = ci)(cid:101), for all v ∈ Dj.

i.e. k · D entries where D = (cid:80)

The tables remain small: P has one entry by category i.e. k
entries total, and T has one entry by category and feature value
|Dj|. In our examples, this
represents less than 3600 entries. Moreover, this preparation
step can be done once and for all at server startup, and is hence
amortized.

B. Protocol

Let us begin with some intuition. The server encrypts each
entry in these tables with Paillier and gives the resulting
encryption (the encrypted model) to the client. For every
class ci, the client uses Paillier’s additive homomorphism

to compute(cid:74)pi(cid:75) =(cid:74)P (i)(cid:75)(cid:81)d

j=1(cid:74)Ti,j(xj)(cid:75). Finally, the client

runs the argmax protocol, Protocol 1, to get argmax pi. For
completeness, the protocol is shown in Protocol 5.

Protocol 5 Naïve Bayes Classiﬁer
Client’s (C) Input: x = (x1, . . . , xd) ∈ Zd, public key PKP ,
secret key SKQR
(cid:110)
Input: The secret key SKP , public key
Server’s
PKQR and probability tables {log p(C = ci)}1≤i≤k and
{log p(Xj = v|C = ci)}v∈Dj
Client’s Output: i0 such that p(x, ci0) is maximum

1≤j≤d,1≤i≤k

(cid:111)

(S)

and encrypts their entries using Paillier.

1: The server prepares the tables P and {Ti,j}1≤i≤k,1≤j≤d
2: The server sends(cid:74)P(cid:75) and {(cid:74)Ti,j(cid:75)}i,j to the client.
(cid:74)P (i)(cid:75)(cid:81)d
the client computes (cid:74)pi(cid:75) =
j=1(cid:74)Ti,j(xj)(cid:75).
3: For all 1 ≤ i ≤ k,
4: The client runs the argmax protocol (Protocol 1) with the

server and gets i0 = argmaxi pi

5: C outputs i0

Proposition 6.1: Protocol 5 is secure in the honest-but-

curious model.

Proof intuition. Given the security property of the argmax
protocol, Protocol 1, and the semantic security of the Paillier
cryptosystem, the security of this classiﬁer follows trivially, by
invoking a modular composition theorem.

Efﬁciency. Note that the tables P and {Ti,j}1≤i≤k,1≤j≤d
can be prepared in advance. Hence the cost of constructing
the tables can be amortized over many uses. To compute the
encrypted probabilities pi’s, the client runs d homomorphic
operations (here multiplications) for each i, hence doing kd
modular multiplications. Then the parties run a single argmax
i.e. k − 1 comparisons and O(k) homomorphic
protocol
operations. Thus, compared to non-encrypted computation, the
overhead comes only from the use of homomorphic encryption
operations instead of plaintext operations. Regarding the number
of round trips, these are due to the argmax protocol: k−1 runs
of the comparison protocol and k − 1 additional roundtrips.

VII. PRIVATE DECISION TREES

A private decision tree classiﬁer allows the server to traverse
a binary decision tree using the client’s input x such that the

8

server does not learn the input x, and the client does not learn
the structure of the tree and the thresholds at each node. A
challenge is that, in particular, the client should not learn the
path in the tree that corresponds to x – the position of the
path in the tree and the length of the path leaks information
about the model. The outcome of the classiﬁcation does not
necessarily leak the path in the tree

The idea is to express the decision tree as a polynomial
P whose output is the result of the classiﬁcation, the class
predicted for x. Then, the server and the client privately
compute inputs to this polynomial based on x and the thresholds
wi. Finally, the server evaluates the polynomial P privately.

A. Polynomial form of a decision tree

Consider that each node of the tree has a boolean variable
associated to it. The value of the boolean at a node is 1 if, on
input x, one should follow the right branch, and 0 otherwise.
For example, denote the boolean variable at the root of the tree
by b1. The value of b1 is 1 if x1 ≤ w1 (recall Figure 2), and
0 otherwise.

We construct a polynomial P that, on input all these boolean
variables and the value of each class at a leaf node, outputs
the class predicted for x. The idea is that P is a sum of terms,
where each term (say t) corresponds to a path in the tree from
root to a leaf node (say c). A term t evaluates to c iff x is
classiﬁed along that path in T , else it evaluates to zero. Hence,
the term corresponding to a path in the tree is naturally the
multiplication of the boolean variables on that path and the
class at the leaf node. For example, for the tree in Figure 3,
P is P (b1, b2, b3, b4, c1, . . . , c5) = b1(b3 · (b4 · c5 + (1 − b4) ·
c4) + (1 − b3) · c3) +(1 − b1)(b2 · c2 + (1 − b2) · c1).

Fig. 3: Decision tree with booleans

We now present F, a recursive procedure for constructing

P given a binary decision tree T :

If T consists only of a leaf node with
category index ci, F(T ) = ci.
If T is empty, return F(T ) = 0.
Otherwise, T has an internal node using
boolean b and T0 and T1 are its left and
right subtrees. Then F(T ) = b·F(T1)+
(1 − b) · F(T0).

9

B. Private evaluation of a polynomial

Let us ﬁrst explain how to compute the values of the boolean
variables securely. Let n be the number of nodes in the tree and
nleaves be the number of leaves in the tree. These values must
remain unknown to the server because they leak information
about x: they are the result of the intermediate computations
of the classiﬁcation criterion. For each boolean variable bi,
the server and the client engage in the comparison protocol to
compare wi and the corresponding attribute of x. As a result,
the server obtains [bi] for i ∈ 1 . . . n; the server then changes
obtaining [(cid:74)bi(cid:75)].
the encryption of these values to FHE using Protocol 2, thus
The server evaluates P on ([(cid:74)b1(cid:75)], . . . , [(cid:74)bn(cid:75)]) using the

homomorphic properties of FHE. In most cases, FHE evaluation
is very slow, but we succeed to make it efﬁcient through a
combination of techniques we now discuss. To understand these
techniques, recall that a typical FHE evaluation happens over
a circuit whose gates are modular addition and multiplication.
The performance of FHE depends a lot on the depth of
multiplications in this circuit.

First, we use a leveled FHE scheme: a scheme that supports
only an a priori ﬁxed multiplicative depth instead of an arbitrary
such depth. As long as this depth is small, such a scheme is
much faster than a full FHE scheme.

Second, we ensure that the multiplicative depth is very
small using a tree-based evaluation. If hmax is the maximum
height of the decision tree, then P has a term a1 · . . . · ahmax.
If we evaluate this term naïvely with FHE, we multiply these
values sequentially. This yields a multiplicative depth of hmax,
which makes FHE slow for common hmax values. Instead, we
construct a binary tree over these values and multiply them
in pairs based on the structure of this tree. This results in a
multiplicative depth of log2 hmax (e.g., 4), which makes FHE
evaluation signiﬁcantly more efﬁcient.

Finally, we use F2 as the plaintext space and SIMD slots
for parallelism. FHE schemes are signiﬁcantly faster when the
values encrypted are bits (namely, in F2); however, P contains
classes (e.g., c1) which are usually more than a bit in length. To
enable computing P over F2, we represent each class in binary.
Let l = (cid:100)log2 k(cid:101) (k is the number of classes) be the number of
bits needed to represent a class. We evaluate P l times, once
for each of the l bits of a class. Concretely, the j-th evaluation
of P takes as input b1, . . . , bn and for each leaf node ci, its
j-th bit cij. The result is P (b1, . . . , bn, c1j, c2j, . . . , cnleavesj),
which represents the j-th bit of the outcome class. Hence, we
need to run the FHE evaluation l times.

To avoid this factor of l, the idea is to use a nice feature
of FHE called SIMD slots (as described in [50]): these allow
encrypting multiple bits in a single ciphertext such that any
operation applied to the ciphertext gets applied in parallel to
each of the bits. Hence, for each class cj, the server creates an

FHE ciphertext [(cid:74)cj0, . . . , cjl−1(cid:75)]. For each node bi, it creates
an FHE ciphertext [(cid:74)bi, . . . , bi(cid:75)] by simply repeating the bi value
over all these ciphertexts and obtains [(cid:74)co0, . . . , col−1(cid:75)] where

in each slot. Then, the server runs one FHE evaluation of P

co is the outcome class. Hence, instead of l FHE evaluations,
the server runs the evaluation only once. This results in a
performance improvement of log k, a factor of 2 and more in
our experiments. We were able to apply SIMD slots parallelism

b1b2c1c2b3c3b4c4c501011100cT1T0b01due to the fortunate fact that the same polynomial P had to
be computed for each slot.

Finally, evaluating the decision tree is done using 2n FHE
multiplications and 2n FHE additions where n is the number
of criteria. The evaluation circuit has multiplication depth
(cid:100)log2(n) + 1(cid:101).
C. Formal description

Protocol 6 describes the resulting protocol.

Protocol 6 Decision Tree Classiﬁer
Client’s (C) Input: x = (x1, . . . , xn) ∈ Zn, secret keys
SKQR, SKF HE
Server’s (S) Input: The public keys PKQR, PKF HE, the
model as a decision tree, including the n thresholds {wi}n
i=1.
Client’s Output: The value of the leaf of the decision tree
associated with the inputs b1, . . . , bn.

1: S produces an n-variate polynomial P as described in

section VII-A.

4: To

evaluate P , S encrypts

3: Using Protocol 2, S changes the encryption from QR to

2: S and C interact in the comparison protocol, so that S
to the

each
category ci using FHE and SIMD slots, obtaining
compute

obtains [bi] for i ∈ [1 . . . n] by comparing wi
corresponding attribute of x.
FHE and obtains [(cid:74)b1(cid:75)], . . . , [(cid:74)bn(cid:75)].
[(cid:74)ci1, . . . , cil(cid:75)].
[(cid:74)P (b1, . . . , bn, c10, . . . , cnleaves0),
. . . , P (b1, . . . , bn, c1l−1, . . . , cnleavesl−1)(cid:75)]. It rerandomizes
outputs(cid:80)l−1

the resulting ciphertext using FHE’s rerandomization
function, and sends the result to the client.

S
homomorphically

5: C decrypts the result as the bit vector (v0, . . . , vl−1) and

uses

SIMD slots

to

the

bits

of

i=0 vi · 2i.

Proposition 7.1: Protocol 6 is secure in the honest-but-

curious model.

Proof intuition. The proof is in the extended paper [45], but
we give some intuition here. During the comparison protocol,
the server only learns encrypted bits, so it learns nothing about
x. During FHE evaluation, it similarly learns nothing about the
input due to the security of FHE. The client does not learn the
structure of the tree because the server performs the evaluation
of the polynomial. Similarly, the client does not learn the bits at
the nodes in the tree because of the security of the comparison
protocol.

The interactions between the client and the server are due to
the comparisons almost exclusively: the decision tree evaluation
does not need any interaction but sending the encrypted result
of the evaluation.

VIII. COMBINING CLASSIFIERS WITH ADABOOST
AdaBoost is a technique introduced in [51]. The idea is to
combine a set of weak classiﬁers hi(x) : Rd (cid:55)→ {−1, +1} to
obtain a better classiﬁer. The AdaBoost algorithm chooses t
scalars {αi}t

i=1 and constructs a strong classiﬁer as:

(cid:33)

If each of the hi(·)’s is an instance of a classiﬁer supported
by our protocols, then given the scalars αi, we can easily
and securely evaluate H(x) by simply composing our building
blocks. First, we run the secure protocols for each of hi, except
that the server keeps the intermediate result, the outcome
of hi(x), encrypted using one of our comparison protocols
(Rows 2 or 4 of Table II). Second, if necessary, we convert them
to Paillier’s encryption scheme with Protocol 2, and combine
these intermediate results using Paillier’s additive homomorphic
property as in the dot product protocol Protocol 3. Finally, we
run the comparison over encrypted data algorithm to compare
the result so far with zero, so that the client gets the ﬁnal result.

IX.

IMPLEMENTATION

We have implemented the protocols and the classiﬁers in
C++ using GMP4, Boost, Google’s Protocol Buffers5, and
HELib [43] for the FHE implementation.

The code is written in a modular way: all the elementary
protocols deﬁned in Section IV can be used as black boxes
with minimal developer effort. Thus, writing secure classiﬁers
comes down to invoking the right API calls to the protocols.
For example, for the linear classiﬁer, the client simply calls a
key exchange protocol to setup the various keys, followed by
the dot product protocol, and then the comparison of encrypted
data protocol to output the result, as shown in Figure 4.

X. EVALUATION

To evaluate our work, we answer the following questions:
(i) can our building blocks be used to construct other classiﬁers
in a modular way (Section X-A), (ii) what is the performance
overhead of our building blocks (Section X-C), and (iii) what
is the performance overhead of our classiﬁers (Section X-D)?

A. Using our building blocks library

Here we demonstrate that our building blocks library can be
used to build other classiﬁers modularly and that it is a useful
contribution by itself. We will construct a multiplexer and a
face detector. A face detection algorithm over encrypted data
already exists [19], [20], so our construction here is not the
ﬁrst such construction, but it serves as a proof of functionality
for our library.

1) Building a multiplexer classiﬁer: A multiplexer is the

following generalized comparison function:

(cid:26) α if a > b

β otherwise

fα,β(a, b) =

We can express fα,β as a linear combination of the bit d =
(a ≤ b):
To implement this classiﬁer privately, we compute(cid:74)d(cid:75) by

fα,β(d) = d · β + (1 − d) · α = α + d · (β − α).

comparing a and b, keeping the result encrypted with QR, and
then changing the encryption scheme (cf. Section IV-C) to
Paillier.

(cid:32) t(cid:88)

i=1

H(x) = sign

αihi(x)

4http://gmplib.org/
5https://code.google.com/p/protobuf/

10

bool Linear_Classifier_Client::run()
{

exchange_keys();

// values_ is a vector of integers
// compute the dot product
mpz_class v = compute_dot_product(values_);
mpz_class w = 1; // encryption of 0

// compare the dot product with 0
return enc_comparison(v, w, bit_size_, false);

}

void Linear_Classifier_Server_session::

run_session()

{

}

exchange_keys();

// enc_model_ is the encrypted model vector
// compute the dot product
help_compute_dot_product(enc_model_, true);

// help the client to get
// the sign of the dot product
help_enc_comparison(bit_size_, false);

Fig. 4: Implementation example: a linear classiﬁer

Bit size

10
20
32
64

A Computation

14.11 ms
18.29 ms
22.9 ms
34.7 ms

B Computation

8.39 ms
14.1 ms
18.8 ms
32.6 ms

Total Time
105.5 ms
117.5 ms
122.6 ms
134.5 ms

Communication

4.60 kB
8.82 kB
13.89 kB
27.38 kB

Interactions

3
3
3
3

TABLE III: Comparison with unencrypted input protocols evaluation.

Protocol

Bit size

Comparison

Reversed Comp.

64
64

Computation

Party A
45.34 ms
48.78 ms

Party B
43.78 ms
42.49 ms

Total Time

Communication

Interactions

190.9 ms
195.7 ms

27.91 kB
27.91 kB

6
6

TABLE IV: Comparison with encrypted input protocols evaluation.

Party A Computation

30.80 ms

Party B Computation

255.3 ms

Total Time
360.7 ms

Communication

420.1 kB

Interactions

2

TABLE V: Change encryption scheme protocol evaluation.

Then, using Paillier’s homomorphism and knowledge of α

and β, we can compute an encryption of fα,β(d):

(cid:74)fα,β(d)(cid:75) =(cid:74)α(cid:75) ·(cid:74)d(cid:75)β−α.

2) Viola and Jones face detection: The Viola and Jones face
detection algorithm [52] is a particular case of an AdaBoost
classiﬁer. Denote by X an image represented as an integer
vector and x a particular detection window (a subset of X’s
coefﬁcients). The strong classiﬁer H for this particular detection
window is

(cid:32) t(cid:88)

(cid:33)

H(x) = sign

αihi(x)

i=1

where the ht are weak classiﬁers of the form hi(x) = sign
((cid:104)x, yi(cid:105) − θi) .
In our setting, Alice owns the image and Bob the classiﬁer
(e.g. the vectors {yi} and the scalars {θi} and {αi}). Neither
of them wants to disclose their input to the other party. Thanks
to our building blocks, Alice can run Bob’s classiﬁer on her
image without her learning anything about the parameters and
Bob learning any information about her image.

The weak classiﬁers can be seen as multiplexers; with the

above notation, we have ht(x) = f1,−1((cid:104)x, yt(cid:105) − θt).

11

Using the elements of Section X-A1, we can easily compute
the encrypted evaluation of every one of these weak classiﬁers
under Paillier, and then, as described in Section VIII, compute
the encryption of H(x).

B. Performance evaluation setup

Our performance evaluations were run using two desktop
computers each with identical conﬁguration: two Intel Core i7
(64 bit) processors for a total 4 cores running at 2.66 GHz and
8 GB RAM. Since the machines were on the same network, we
inﬂated the roundtrip time for a packet to be 40 ms to mimic
real network latency. We used 1024-bit cryptographic keys,
and chose the statistical security parameter λ to be 100. When
using HELib, we use 80 bits of security, which corresponds to
a 1024-bit asymmetric key.

C. Building blocks performance

We examine performance in terms of computation time
at the client and server, communication bandwidth, and also
number of interactions (round trips). We can see that all
these protocols are efﬁcient, with a runtime on the order of
milliseconds.

Data set

Model size

Breast cancer (2)

Credit (3)

30
47

Computation

Client
46.4 ms
55.5 ms

Server
43.8 ms
43.8 ms

Time per protocol

Compare Dot product
194 ms
194 ms

9.67 ms
23.6 ms

Total

running time

204 ms
217 ms

(a) Linear Classiﬁer. Time per protocol includes communication.

Data set

Breast Cancer (1)

Nursery (5)
Audiology (4)

Specs.
F
C
9
2
9
5
24
70

Computation

Client
150 ms
537 ms
1652 ms

Server
104 ms
368 ms
1664 ms

Time per protocol

Prob. Comp.

82.9 ms
82.8 ms
431 ms

Argmax
396 ms
1332 ms
3379 ms

Total

running time

479 ms
1415 ms
3810 ms

Comm.
35.84 kB
40.19 kB

Interactions

7
7

Comm.
72.47 kB
150.7 kB
1911 kB

Interactions

14
42
166

(b) Naïve Bayes Classiﬁer. C is the number of classes and F is the number of features. The Prob. Comp. column corresponds to the computation
of the probabilities p(ci|x) (cf. Section VI). Time per protocol includes communication.

Data set

Nursery (5)

ECG (6)

Tree Specs.
N
4
6

D
4
4

Computation

Client
1579 ms
2297 ms

Server
798 ms
1723 ms

Time per protocol

Compare
446 ms
1410 ms

ES Change
1639 ms
7406 ms

FHE

Decrypt
33.51 ms
35.1 ms

Eval.
239 ms
899 ms

Comm.
2639 kB
3555 kB

Interactions

30
44

(c) Decision Tree Classiﬁer. ES change indicates the time to run the protocol for changing encryption schemes. N is the number of nodes of
the tree and D is its depth. Time per protocol includes communication.

TABLE VI: Classiﬁers evaluation.

1) Comparison protocols:

Comparison with unencrypted input. Table III gives the
running time of the comparison protocol with unencrypted
input for various input size.
Comparison with encrypted input. Table IV presents the
performance of the comparison with encrypted inputs protocols.
2) argmax: Figure 5 presents the running times and the
communication overhead of the argmax of encrypted data
protocol (cf. Section IV-B). The input integers were 64 bit
integers.

Fig. 5: Argmax of encrypted data protocol evaluation. The bars
represent the execution of the protocol when the comparisons are
executed one after each other, linearly. The line represents the execution
when comparisons are executed in parallel, tree-wise.

3) Consequences of the latency on performances: It is worth
noticing that for most blocks, most of the running time is spend
communicating: the network’s latency has a huge inﬂuence on
the performances of the protocols (running time almost linear in
the latency for some protocols). To improve the performances
of a classiﬁer implemented with our blocks, we might want to
run several instances of some building blocks in parallel. This
is actually what we did with the tree-based implementation of
the argmax protocol, greatly improving the performances of
the protocol (cf. Figure 5).

D. Classiﬁer performance

Here we evaluate each of the classiﬁers described in
Sections V–VII. The models are trained non-privately using
scikit-learn6. We used the following datasets from the
UCI machine learning repository [53]:

1)
2)

3)
4)
5)
6)

the Wisconsin Diagnostic Breast Cancer data set,
the Wisconsin Breast Cancer (Original) data set, a
simpliﬁed version of the previous dataset,
Credit Approval data set,
Audiology (Standardized) data set,
Nursery data set, and
ECG (electrocardiogram) classiﬁcation data from
Barni et al. [27]

These data sets are scenarios when we want to ensure

privacy of the server’s model and client’s input.

Based on the suitability of each classiﬁer, we used data
sets 2 and 3 to test the hyperplane decision classiﬁer, sets 1, 4
and 5 for the Naïve Bayes classiﬁer, and sets 5 and 6 for the
decision tree classiﬁer.

6http://scikit-learn.org

12

 0 1000 2000 3000 4000 5000 6000 7000456789101112131415161718192025303550Time (ms)ElementsParty AParty BCommunicationTreeTable VI shows the performance results. Our classiﬁers run
in at most a few seconds, which we believe to be practical for
sensitive applications. Note that even if the datasets become
very large, the size of the model stays the same – the dataset size
only affects the training phase which happens on unencrypted
data before one uses our classiﬁers. Hence, the cost of our
classiﬁcation will be the same even for very large data sets.

For the decision tree classiﬁer, we compared our construc-
tion to Barni et al. [27] on the ECG dataset (by turning their
branching program into a decision tree). Their performance
is 2609 ms7 for the client and 6260 ms for the server with
communication cost of 112.2KB. Even though their evaluation
does not consider the communication delays, we are still more
than three times as fast for the server and faster for the client.

E. Comparison to generic two-party tools

A set of generic secure two- or multi-party computation
tools have been developed, such as TASTY [6] and Fairplay [7],
[8]. These support general functions, which include our
classiﬁers.

However,

they are prohibitively slow for our speciﬁc
setting. Our efﬁciency comes from specializing to classiﬁcation
functionality. To demonstrate their performance, we attempted
to evaluate the Naïve Bayes classiﬁer with these. We used
FairplayMP to generate the circuit for this classiﬁer and then
TASTY to run the private computation on the circuit thus
obtained. We tried to run the smallest Naïve Bayes instance,
the Nursery dataset from our evaluation, which has only 3
possible values for each feature, but we ran out of memory
during the circuit generation phase on a powerful machine with
256GB of RAM.

Hence, we had to reduce the classiﬁcation problem to
only 3 classes (versus 5). Then, the circuit generation took
more than 2 hours with FairplayMP, and the time to run the
classiﬁcation with TASTY was 413196 msec (with no network
delay), which is ≈ 500 times slower than our performance (on
the non-reduced classiﬁcation problem with 5 classes). Thus,
our specialized protocols improve performance by orders of
magnitude.

XI. CONCLUSION

In this paper, we constructed three major privacy-preserving
classiﬁers as well as provided a library of building blocks that
enables constructing other classiﬁers. We demonstrated the
efﬁciency of our classiﬁers and library on real datasets.

ACKNOWLEDGMENT

We thank Thijs Veugen, Thomas Schneider, and the

anonymous reviewers for their helpful comments.

REFERENCES

[1]

J. Wiens, J. Guttag, and E. Horvitz, “Learning evolving patient risk
processes for c. diff colonization,” in ICML, 2012.

7In Barni et al. [27], the evaluation was run over two 3GHz computers
2.3 to

directly connected via Gigabit Ethernet. We scaled the given results by 3
get a better comparison basis.

[2] A. Singh and J. Guttag, “Cardiovascular risk stratiﬁcation using non-
symmetric entropy-based classiﬁcation trees,” in NIPS workshop on
personalized medicine, 2011.

[3] A. Singh and J. Guttag, “Leveraging hierarchical structure in diagnostic
codes for predicting incident heart failure,” in ICML workshop on role
of machine learning in transforming healthcare, 2013.

[4] A. C. Yao, “Protocols for secure computations,” in FOCS, 1982, pp.

160–164.

[5] O. Goldreich, S. Micali, and A. Wigderson, “How to play any mental

game,” in STOC, 1987, pp. 218–229.

[6] W. Henecka, S. Kögl, A.-R. Sadeghi, T. Schneider, and I. Wehrenberg,
“Tasty: Tool for automating secure two-party computations,” in CCS,
2010, pp. 451–462.

[7] D. Malkhi, N. Nisan, B. Pinkas, and Y. Sella, “Fairplay-secure two-
party computation system.” in USENIX Security Symposium, 2004, pp.
287–302.

[8] A. Ben-David, N. Nisan, and B. Pinkas, “Fairplaymp: A system for

secure multi-party computation,” in CCS, 2008, pp. 17–21.

[9] Y. Lindell and B. Pinkas, “Privacy preserving data mining,” in Advances

in Cryptology (CRYPTO), 2000, pp. 36–54.

[10] W. Du, Y. S. Han, and S. Chen, “Privacy-preserving multivariate
statistical analysis: Linear regression and classiﬁcation,” in Proceedings
of the 4th SIAM International Conference on Data Mining, vol. 233,
2004.

[11] R. Wright and Z. Yang, “Privacy-preserving bayesian network structure
computation on distributed heterogeneous data,” in Proceedings of the
tenth ACM SIGKDD international conference on Knowledge discovery
and data mining, 2004, pp. 713–718.

[12] Z. Y. S. Zhong and R. N. Wright, “Privacy-preserving classiﬁcation
of customer data without loss of accuracy,” in SIAM International
Conference on Data Mining (SDM), 2005.

[13] A. Blum, C. Dwork, F. McSherry, and K. Nissim, “Practical privacy: the
sulq framework,” in Proceedings of the twenty-fourth ACM SIGMOD-
SIGACT-SIGART symposium on Principles of database systems, 2005,
pp. 128–138.
J. Vaidya, M. Kantarcıo˘glu, and C. Clifton, “Privacy-preserving naive
bayes classiﬁcation,” The International Journal on Very Large Data
Bases, vol. 17, no. 4, pp. 879–898, 2008.

[14]

[15] T. Graepel, K. Lauter, and M. Naehrig, “ML conﬁdential: Machine
learning on encrypted data,” in Information Security and Cryptology
(ICISC), 2012, pp. 1–21.
J. W. Bos, K. Lauter, and M. Naehrig, “Private predictive analysis on
encrypted medical data,” in Microsoft Tech Report 200652, 2013.

[16]

[17] Z. Erkin, M. Franz, J. Guajardo, S. Katzenbeisser, I. Lagendijk, and
T. Toft, “Privacy-preserving face recognition,” in Privacy Enhancing
Technologies, 2009, pp. 235–253.

[18] A.-R. Sadeghi, T. Schneider, and I. Wehrenberg, “Efﬁcient privacy-
preserving face recognition,” in Information, Security and Cryptology
(ICISC), 2009, pp. 229–244.

[19] S. Avidan and M. Butman, “Blind vision,” in Computer Vision–ECCV

2006, 2006, pp. 1–13.

[20] S. Avidan and M. Butman, “Efﬁcient methods for privacy preserving
face detection,” in Advances in Neural Information Processing Systems,
2007, p. 57.

[21] R. Canetti, “Security and composition of multi-party cryptographic

protocols,” JOURNAL OF CRYPTOLOGY, vol. 13, p. 2000, 1998.

[22] Y. Lindell and B. Pinkas, “An efﬁcient protocol for secure two-party
computation in the presence of malicious adversaries,” in Advances in
Cryptology (EUROCRYPT), 2007, vol. 4515, pp. 52–78.

[23] Y. Ishai, M. Prabhakaran, and A. Sahai, “Founding cryptography on
oblivious transfer – efﬁciently,” in Advances in Cryptology – CRYPTO
2008, 2008, vol. 5157, pp. 572–591.

[24] Y. Lindell and B. Pinkas, “A proof of security of Yao’s protocol for
two-party computation,” J. Cryptol., vol. 22, pp. 161–188, April 2009.
[25] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate, “Differentially private

empirical risk minimization,” J. Mach. Learn. Res., vol. 12, 2011.

[26] S. Laur, H. Lipmaa, and T. Mielikäinen, “Cryptographically private
support vector machines,” in Proceedings of the 12th ACM SIGKDD

13

[39] A. Rahimi and B. Recht, “Random features for large-scale kernel

machines,” in NIPS, 2007.

[40] C. Gentry, “Fully homomorphic encryption using ideal lattices,” in STOC,

2009, pp. 169–178.

[41] S. Goldwasser and S. Micali, “Probabilistic encryption and how to play
mental poker keeping secret all partial information,” in STOC. ACM,
1982, pp. 365–377.

[42] P. Paillier, “Public-key cryptosystems based on composite degree

residuosity classes,” in EUROCRYPT, 1999, pp. 223–238.

[43] S. Halevi, “Helib - an implementation of homomorphic encryption,”
https://github.com/shaih/HElib, 2013. [Online]. Available: https://github.
com/shaih/HElib

[44] O. Goldreich, Foundations of Cryptography - Basic Applications.

Cambridge University Press, 2004.

[45] R. Bost, R. A. Popa, S. Tu, and S. Goldwasser, “Machine learning
classiﬁcation over encrypted data,” in Crypto ePrint Archive, 2014,
https://eprint.iacr.org/2014/331.pdf.

[46] M. Bellare, V. T. Hoang, S. Keelveedhi, and P. Rogaway, “Efﬁcient
garbling from a ﬁxed-key blockcipher,” in IEEE SP, 2013, pp. 478–492.
[47] M. Naor and B. Pinkas, “Efﬁcient oblivious transfer protocols,” in
Proceedings of the twelfth annual ACM-SIAM symposium on Discrete
algorithms, 2001, pp. 448–457.

[48] V. Kolesnikov, A.-R. Sadeghi, and T. Schneider, “A systematic approach
to practically efﬁcient general two-party secure function evaluation
protocols and their modular design,” in Journal of Computer Security,
2013.

[49] S. Tschiatschek, P. Reinprecht, M. Mücke, and F. Pernkopf, “Bayesian
network classiﬁers with reduced precision parameters,” in Machine
Learning and Knowledge Discovery in Databases, 2012, pp. 74–89.

[50] N. P. Smart and F. Vercauteren, “Fully homomorphic SIMD operations,”

Cryptology ePrint Archive, Report 2011/133, 2011.

[51] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of
on-line learning and an application to boosting,” Journal of computer
and system sciences, no. 1, pp. 119–139, 1997.

[52] P. Viola and M. Jones, “Rapid object detection using a boosted cascade
of simple features,” in IEEE Computer Vision and Pattern Recognition
(CVPR), vol. 1, 2001, pp. I–511.

[53] K. Bache and M. Lichman, “UCI machine learning repository,” 2013.

[Online]. Available: http://archive.ics.uci.edu/ml

See our full paper [45] for details and proofs.

international conference on Knowledge discovery and data mining.
ACM, 2006, pp. 618–624.

[27] M. Barni, P. Failla, V. Kolesnikov, R. Lazzeretti, A.-R. Sadeghi, and
T. Schneider, “Secure evaluation of private linear branching programs
with medical applications,” in Computer Security (ESORICS), 2009, pp.
424–439.

[28] M. Barni, P. Failla, R. Lazzeretti, A. Paus, A.-R. Sadeghi, T. Schneider,
and V. Kolesnikov, “Efﬁcient privacy-preserving classiﬁcation of ecg
signals,” in Information Forensics and Security, 2009. WIFS 2009. First
IEEE International Workshop on, 2009, pp. 91–95.

[30]

[29] M. Barni, P. Failla, R. Lazzeretti, A.-R. Sadeghi, and T. Schneider,
“Privacy-preserving ECG classiﬁcation with branching programs and
neural networks,” IEEE Transactions on Information Forensics and
Security (TIFS), vol. 6, no. 2, pp. 452–468, June 2011.
I. Damgård, M. Geisler, and M. Krøigaard, “Efﬁcient and secure
comparison for on-line auctions,” in Information Security and Privacy,
2007, pp. 416–430.
I. Damgard, M. Geisler, and M. Kroigard, “A correction to efﬁcient
and secure comparison for on-line auctions,” vol. 1, no. 4, pp. 323–324,
2009.

[31]

[32] T. Veugen, “Comparing encrypted data,” http://msp.ewi.tudelft.nl/sites/

default/ﬁles/Comparingencrypteddata.pdf, 2011.

[33] H.-Y. Lin and W.-G. Tzeng, “An efﬁcient solution to the millionaires’
problem based on homomorphic encryption,” in Applied Cryptography
and Network Security, 2005, pp. 456–466.

[34] V. Kolesnikov, A.-R. Sadeghi, and T. Schneider, “How to combine
homomorphic encryption and garbled circuits - improved circuits and
computing the minimum distance efﬁciently,” in 1st International
Workshop on Signal Processing in the EncryptEd Domain (SPEED’09),
2009.

[35] M. J. Atallah and W. Du, “Secure multi-party computational geometry,”

in Algorithms and Data Structures, 2001, pp. 165–179.

[36] B. Goethals, S. Laur, H. Lipmaa, and T. Mielikäinen, “On private scalar
product computation for privacy-preserving data mining,” in Information
Security and Cryptology (ICISC), 2004, pp. 104–120.

[37] E. Kiltz, “Unconditionally secure constant round multi-party computation
for equality, comparison, bits and exponentiation.” IACR Cryptology
ePrint Archive, p. 66, 2005.

[38] C. M. Bishop and N. M. Nasrabadi, “Pattern recognition and machine

learning,” in Journal of Electronic Imaging, vol. 1, 2006.

14

