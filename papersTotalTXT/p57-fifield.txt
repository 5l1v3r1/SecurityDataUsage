Remote Operating System Classiﬁcation over IPv6

∗

David Fiﬁeld

University of California,

Berkeley

ﬁﬁeld@eecs.berkeley.edu

Alexandru Geana
Technische Universiteit

Eindhoven
Fox-IT, Delft

alex@alegen.net

Luis MartinGarcia

ETSIT, Polytechnic University

of Madrid

luis@luismg.com

Mathias Morbitzer

Fox-IT, Delft

m.morbitzer@runbox.com

J. D. Tygar

University of California,

Berkeley

doug.tygar@gmail.com

ABSTRACT
Diﬀerences in the implementation of common networking
protocols make it possible to identify the operating system
of a remote host by the characteristics of its TCP and IP
packets, even in the absence of application-layer informa-
tion. This technique, “OS ﬁngerprinting,” is relevant to net-
work security because of its relationship to network inven-
tory, vulnerability scanning, and tailoring of exploits. Vari-
ous techniques of ﬁngerprinting over IPv4 have been in use
for over a decade; however IPv6 has had comparatively scant
attention in both research and in practical tools. In this pa-
per we describe an IPv6-based OS ﬁngerprinting engine that
is based on a linear classiﬁer. It introduces innovative classi-
ﬁcation features and network probes that take advantage of
the speciﬁcs of IPv6, while also making use of existing proven
techniques. The engine is deployed in Nmap, a widely used
network security scanner. This engine provides good per-
formance at a fraction of the maintenance costs of classical
signature-based systems. We describe our work in progress
to enhance the deployed system: new network probes that
help to further distinguish operating systems, and imputa-
tion of incomplete feature vectors.

Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Network
Protocols

Keywords
OS ﬁngerprinting

∗Authors are listed alphabetically.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
AISec’15, October 16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3826-4/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2808769.2808777 .

1.

INTRODUCTION

The ability to remotely identify the operating system of a
network host has several implications for network security:
• Network inventory: identifying hosts that should and

should not be on a network.

• Vulnerability assessment: though OS ﬁngerprinting is
fairly coarse-grained with respect to version informa-
tion, it can often identify out-of-date hosts that are
more promising targets of attack.

• Exploit tailoring: a penetration tester or network at-
tacker can use knowledge of the operating system to
select proper shellcode, for example.

The possibility of OS ﬁngerprinting—the fact that idiosyn-
crasies in the implementation of network protocols such as
TCP and IP are remotely detectable and hard to disguise—
has long been known, and there exist many software tools
that take advantage of it. However, both research and prac-
tice have mainly focused on IPv4. In this paper we apply
machine learning techniques to the problem of OS ﬁnger-
printing over IPv6, with a thorough examination of the ﬁn-
gerprinting engine we built and which is deployed in the
Nmap security scanner. Figure 1 is sample output, which in
this case shows that the remote host is running Linux, and
one of a small range of kernel versions.

The engine’s design is guided by many years’ experience
in dealing with IPv4-based detection.
It is fundamentally
based on a logistic regression model trained on a few hun-
dred known OS “ﬁngerprints,” which are the packets received
in response to up to 18 specially crafted network probes. OS
ﬁngerprinting has traditionally relied on a nearest-neighbor
match against a database of known ﬁngerprints. The use of
machine learning is aimed at enabling the engine to better
identify ﬁngerprints it has not seen exactly before; and also
to cope with the many ways a packet may be corrupted in
transit (unfortunately many of the protocol ﬁelds useful for
ﬁngerprinting are often modiﬁed by intermediate devices).
The great diversity of operating systems and types of net-
work interference have in the past required a proportionally
large database of known ﬁngerprints. The cost of maintain-
ing such a database is non-trivial and we hope to make it
more manageable.

The training set of known operating system ﬁngerprints
was initially seeded by us through manual scans of common

57# nmap -6 -O scanme.nmap.org

STATE SERVICE

Starting Nmap 6.49SVN ( http://nmap.org )
Nmap scan report for scanme.nmap.org (2600:3c01::f03c:91ff:fe18:bb2f)
Host is up (0.069s latency).
Other addresses for scanme.nmap.org (not scanned): 45.33.32.156
Not shown: 998 closed ports
PORT
22/tcp open ssh
80/tcp open http
Device type: general purpose
Running: Linux 3.X
OS CPE: cpe:/o:linux:linux_kernel:3
OS details: Linux 3.13 - 3.19
Network Distance: 15 hops

OS detection performed. Please report any incorrect results\

at https://nmap.org/submit/ .

Nmap done: 1 IP address (1 host up) scanned in 8.10 seconds

Figure 1: Nmap OS detection run against a sample
host. In about 8 seconds, the program has scanned
1,000 ports, selected an open and a closed port,
and then sent several probes designed to uncover id-
iosyncrasies of the host’s OS. The lines starting with
‘Device type:’, ‘Running:’, ‘OS CPE:’, and ‘OS details:’
are the output of the OS classiﬁcation engine. The
-6 option switches to IPv6 mode (the default is
IPv4) and -O activates OS detection. The “submit”
web form is the primary vehicle for the growth of
the OS database.

systems, but it is mainly driven by community submissions.
When Nmap scans a host and fails to ﬁnd an OS match of
suﬃcient quality, it displays a textual ﬁngerprint and en-
courages the user to submit it to a web form if they happen
to know the OS. The submissions are individually vetted
by a human before being added to the training set. Sec-
tion 5 describes how the program makes the call of whether
to print a potentially useful but low-conﬁdence OS match,
or to display a ﬁngerprint and ask for its submission.

Although application protocols may provide a wealth of
information, and in many cases an application can be mapped
back to the OS hosting it, such techniques are beyond the
scope of this paper. We are concerned only with the network
and transport layers.

1.1 What makes OS ﬁngerprinting possible

While the interoperability of hosts on the Internet relies on
network stacks supporting the same standard protocols, OS
vendors have considerable leeway in the implementation of
those standards. Some protocol ﬁelds are left reserved or un-
speciﬁed; some have optional features that may or may not
be present in any particular implementation; and sometimes
speciﬁcations are incomplete and implementers must invent
their own behavior for some corner cases. Even where spec-
iﬁcations are completely unambiguous, an implementation
can simply contain bugs that make it deviate from standard
behavior. The network-visible features of network traﬃc are
typically deeply tied with the implementation of the oper-
ating system and are not easy to disguise. Some diﬀerences
are not merely a matter of taste or conﬁguration but can
have performance and security implications, so they cannot
simply be disabled without consequences. A good example
of this are TCP timestamps [17], which serve both to en-

hance round-trip time estimation, and to prevent sequence
numbers from wrapping.

The freedom of protocol implementers to make choices
that do not aﬀect compliance to the standards often makes
the network traﬃc generated by their implementations to be
distinguishable from others. There are many good examples
of this.

Versions of Windows up to Windows 98 used a simple
incrementing counter for the IP ID ﬁeld—but sent the ﬁeld
with little-endian byte order. Versions of Solaris use diﬀerent
initial TTLs for TCP and ICMP packets. Other examples
of features that vary across operating systems and are useful
for ﬁngerprinting are: TCP options (their values and order);
TCP initial window sizes; the initial IPv4 TTL or IPv6 hop
limit; handling of IPv6 extension headers; and the frequency
of the TCP timestamp counter. In addition, OSes vary in
their responses to unusual or malformed packets; even the
information of whether a response was sent can be useful for
ﬁngerprinting.

The basics of OS ﬁngerprinting are fairly robust. Almost
always, given an open and a closed TCP port, it is possi-
ble to remotely identify at least the operating system family
(e.g. “Linux” or “Windows”) using nothing more than sim-
ple procedural rules and high-level features. (The earliest
ﬁngerprinting tools worked that way, building classiﬁcation
logic directly in code [18]. Contemporary systems use ﬁxed
classiﬁcation code and a variable external database of ﬁn-
gerprints.) We know through experience that it is possible,
though it requires more care, to make ﬁner distinctions in
versioning, in many cases separating diﬀerent revisions of
the same OS. For example, Linux 2.4 is readily distinguish-
able from Linux 2.6—but a good classiﬁer should be able to
do even better than that.

Some examples from the history of the 2.6 series of Linux
kernels will illustrate how OS ﬁngerprints can change over
time and make diﬀerent revisions distinguishable. Linux
2.6.8 in 2004 began sending a nonzero TCP window scale
option by default [9];
it is thus easy to distinguish from
earlier versions. Linux 2.6.22 in 2007 increased by a fac-
tor of 15 the frequency of the counter used to set TCP ini-
tial sequence numbers. Linux 2.6.38 in 2010 increased the
size of the initial congestion window [10]. In 2008, a bug-
ﬁx and code refactoring in Linux 2.6.27 had the side eﬀect
of changing the order of TCP options. The change had to
be hurriedly reverted after it was found incompatible with
some consumer networking equipment that wrongly required
a certain option ordering [13]. Linux TCP segments bear-
ing this peculiar option ordering may as well be stamped
“2.6.27”: they are permanently and narrowly isolated to a
very small number of released kernels and a narrow period of
time. Changes like these create clear “break points” between
which ranges of OS versions should be distinguishable.

A primary challenge of OS ﬁngerprinting is “middlebox in-
terference,” changes made to packets en route by various net-
work devices such as routers, ﬁrewalls, and load balancers.
Interference means that the packets you receive often do not
perfectly reﬂect the packets a remote host sent. Sometimes
the overwritten ﬁelds are those that are good for ﬁngerprint-
ing. In addition, some protocol ﬁelds are not a function only
of the remote operating system but also of the network link.
For example, the TCP maximum segment size (MSS) is of-
ten characteristic of an OS but is also constrained by the
network. Some ﬁelds vary naturally, like Linux’s window

58scale option that partly depends on how much RAM is in-
stalled [29]. The trick is to account for variations such as
these, without completely ignoring the underlying OS fea-
tures it may mask. Enumerating all possible forms of inter-
ference is diﬃcult and has, for us in the past, led to a pro-
liferation of speciﬁc ﬁngerprints or overbroad ﬁngerprints.
1.2 Differences in IPv6

Many IPv4 ﬁngerprinting techniques carry over straight-
forwardly to IPv6, though IPv6 brings its own challenges
and opportunities. Much of the power of OS ﬁngerprint-
ing comes from the TCP layer, the implementation of which
is usually the same between IPv4 and IPv6. (Windows XP
used diﬀerent TCP implementations for IPv4 and IPv6, with
diﬀerent window sizes and options. However, this appears to
be an exception to the general rule: almost all other OSes,
including later versions of Windows, use the same TCP im-
plementation in IPv4 and IPv6, as far as we have seen.)

The IPv6 header lacks some ﬁelds of the IPv4 header.
There are no type-of-service, identiﬁcation, or fragment oﬀ-
set ﬁelds. In exchange, IPv6 gains traﬃc class and ﬂow la-
bel ﬁelds. The hop limit ﬁeld takes the place of IPv4’s time
to live (TTL). The most important change in IPv6 is the
introduction of extension headers [12] at the IP layer. In-
stead of the length-limited options block, IPv6 uses a chain
of typed headers that terminates in the upper-layer proto-
col payload (e.g. TCP or UDP). Much like TCP options,
extension headers and how they are processed tend to be
speciﬁc to network stack implementation. Some of our OS
probes send speciﬁc extension headers in order to uncover
diﬀerences in header processing.

2. RELATED WORK

OS ﬁngerprinting techniques can generally be divided into
two categories: passive and active. Passive techniques rely
only on naturally occurring traﬃc, such as the SYN and
SYN/ACK in a TCP handshake. They do not directly stim-
ulate responses from remote hosts and therefore do not make
strong assumptions about what kind of packets may have
provoked observed traﬃc. Passive detection has the advan-
tage that it can use both live and recorded traﬃc. It gives
up some discriminating power because it tends to see net-
work stacks only in under normal conditions, and not in
the exceptional conditions that may reveal more diﬀerences.
Active detection, on the other hand, sends probes that are
specially designed to elicit diﬀerent responses from diﬀer-
ent operating systems. Active techniques have the potential
to be more sensitive than passive techniques, but they are
noisier, easier to detect and block.

The focus of this paper is the active IPv6 OS detection
engine in Nmap. OS detection in Nmap has a long history,
going back to 1998 and the debut of its “ﬁrst-generation”
engine for IPv4 [18]. This engine sent a battery of probes:
6 TCP sequencing probes to assess sequence and timestamp
generation, 4 TCP probes to an open port, 3 TCP probes to
a closed port, and 1 UDP probe to a closed port. Responses
to the probes were examined for various features such as the
TCP timestamp clock frequency, TCP ﬂags and options, IP
ﬁelds, and others, and matched against a database of known
ﬁngerprints. The database had a simple pattern language
that allowed many values to match a particular feature ﬁeld.
The ﬁrst-generation database contained 1,684 ﬁngerprints
when it was retired in 2007.

Nmap’s OS detection engine was overhauled in 2006. The
“second-generation” system uses roughly the same probes as
the ﬁrst generation [19]. The main diﬀerence is the intro-
duction of manually tuned weights (called MatchPoints) on
features that allow inexact matches and multiple matches.
The MatchPoints assign a penalty for a mismatch in each
feature. The more important a feature, the higher its Match-
Points. Certain features have lower MatchPoints in recog-
nition of their susceptibility to middlebox interference. The
best match is the one with the fewest penalties. The system
has been enhanced in many ways since 2006, but its funda-
mentals remain the same today. Its principal drawback is
the high cost of maintaining the ﬁngerprint database. Be-
cause the database pattern language is fairly simple, it is
necessary to pad the database with partly redundant ﬁnger-
prints that represent the most common operating systems
under a variety of network conditions: with diﬀerent ﬁrewall
conﬁgurations, over diﬀerent physical links, and with vary-
ing interference by intermediate routers. Maintaining the
database requires substantial human curation, which was
one of the motivations to employ machine learning in the
IPv6 engine. To date, in Nmap 6.49, the second-generation
OS database contains 4,766 ﬁngerprints. To give an idea
of the amount of redundancy in the database, 789 (17%) of
those ﬁngerprints are varieties of Linux (not including Linux
embedded in hardware devices).

The premier passive ﬁngerprinting tool is p0f. It has been
rewritten several times: v1 was released in 2000 [36]; v2 in
2003 [37]; and v3 in 2012 [38]. p0f uses the same ﬁngerprints
for IPv4 and IPv6; it ignores certain IPv4-speciﬁc ﬁelds (like
the “don’t fragment” ﬁeld) when matching against IPv6 traf-
ﬁc. OpenBSD’s packet ﬁlter PF incorporates passive OS
ﬁngerprinting based on p0f.

Beverly [6] built a classiﬁer that used a subset of p0f’s
features: the TTL, TCP window size, SYN packet size, and
the value of the “don’t fragment” bit. The classiﬁer makes
the naive Bayes assumption that each feature is indepen-
dent and outputs the OS match with maximum likelihood.
He tried training the classiﬁer using both the p0f database
and by setting up a web server and correlating TCP/IP sig-
natures with HTTP-layer OS identiﬁcation from the server
logs.

Xprobe2 [1, 35] is an active tool whose design prioritizes
sending a small number of probes. Its default probes em-
phasize ICMP, though it has a module programming inter-
face that allows for the addition of other tests with weights
and reliability ratings. Xprobe2 employs non-strict match-
ing against a database, assigning a score from 0 to 3 to each
feature, and returning the match with the highest sum of
feature scores. Some of its tests target the application layer.
Some systems skirt the boundary between passive and ac-
tive by sending only a single SYN probe. Examples of these
are RING [33], Snacktime [5], and Hershel [31]. These sys-
tems augment the usual response feature set with temporal
features, exploiting diﬀerences in SYN/ACK retransmission.
They wait for a relatively long time (up to 120 seconds) af-
ter sending their probe in order to collect a vector of time
diﬀerences between SYN/ACK retransmissions.

SinFP [3], released in 2006, was the ﬁrst public tool to
It does both passive and
do OS classiﬁcation over IPv6.
active classiﬁcation. It has a database of IPv6 ﬁngerprints,
and the ability to fall back to an IPv4 ﬁngerprint if none
of the IPv6 ﬁngerprints match. To automatically convert

59an IPv4 ﬁngerprint into one that is compatible with IPv6,
SinFP applies the correspondences:
IPv4 identiﬁcation → IPv6 ﬂow label
IPv4 time to live → IPv6 hop limit
IPv4 don’t fragment → IPv6 traﬃc class
The conversion is reasonable, because it is common for an
OS to share a TCP implementation across IPv4 and IPv6,
and SinFP applies heuristics to resolve inexact matches.

There have been experiments with bootstrapping a ma-
chine learning–based classiﬁer using Nmap’s large databases
of IPv4 OS ﬁngerprints. Sarraute and Burroni [30] built a
hierarchy of neural networks trained on the ﬁrst-generation
database (at the time, it contained 1,684 samples). The ﬁrst
neural network separated “relevant” OS families (Windows,
Linux, Solaris, OpenBSD, FreeBSD, and NetBSD) from all
others. The next separated the “relevant” OSes into fami-
lies, and further layers separated OS versions within a fam-
ily. Their raw feature vectors had dimension 568. Medeiros
et al. [23] converted the second-generation database into fea-
ture vectors (at the time, the database contained 430 sam-
ples). They used the vectors to build a contextual map of
similar operating systems, with the goal of classifying pre-
viously unseen operating systems.

Greenwald and Thomas [16] measured the information
gain of tests in the second-generation database, with the
goal of achieving high accuracy while sending fewer probes.
They achieved accuracy almost as high as Nmap’s full set
of 13 TCP probes, while sending only 3 or 4. They found
that the TCP window size and options were overall the most
information-carrying features. They ran their tests when the
database contained 417 samples.

3. PROBE SELECTION

As a starting point for our research, we studied a wide

variety of RFCs related to IPv6. Examples include:

RFC 2460 (the speciﬁcation of the IPv6 protocol itself)
RFC 2463 (ICMP for IPv6)
RFC 2473 (Generic Packet Tunneling)
RFC 2675 (Jumbograms)
RFC 3122 (Inverse Discovery)
RFC 3775 (Mobility)
RFC 3971 (Secure Neighbor Discovery)
RFC 4620 (Node Information Queries)
RFC 4782 (Quick-Start)
RFC 4861 (Neighbor Discovery)
RFC 5570 (CALIPSO)

In the RFCs we looked for features, options or ﬁelds la-
beled as “optional” or “recommended,” deﬁnitions we con-
sidered ambiguous, and behaviors described using the verb
“should,” which in the RFC terminology means that imple-
menters may ﬁnd valid reasons or particular circumstances
to ignore the speciﬁed behavior.

From the standards documented in the RFCs, we created
a set of 154 network probes. The criteria for the deﬁnition of
those probes was to meet as many as possible of the following
goals:

• Use packets that are likely to elicit responses from
the target device (e.g. use ICMP request messages
as opposed to replies, which will likely be silently dis-
carded).

Apple iOS 4.2.1 (iPhone)
Apple OS X 10.6.8
Apple OS X 10.7.0
Apple OS X 10.8.0
Apple TV 3.0.2
Cisco IOS 12.3(26)
Cisco IOS 15.1(3)T1
Cisco IOS (unknown version)
FreeBSD 6.1
FreeBSD 8.1
HP LaserJet M1212nf printer
HP ProCurve 2520G switch
Linux 2.6.11 (Fedora Core 4)
Linux 2.6.15 (Ubuntu 6.06)
Linux 2.6.33 (embedded in Thecus N4100 storage device)
Linux 2.6.35 (Fedora 14)
Linux 2.6.35 (Ubuntu 10.10)
Microsoft Windows 7 Professional SP1
Microsoft Windows 7 Ultimate
Microsoft Windows XP SP3
NetBSD 5.0
OpenBSD 3.8
OpenBSD 4.4
OpenBSD 4.8
OpenIndiana oi 148
OpenSolaris 2009.06

Table 1: List of operating systems against which we
ran the ipv6fp.py program. These are the systems
whose behavior guided our selection of probes.

• Use packets that test for features or behaviors that
are described ambiguously or left as optional or rec-
ommended in the RFC.

• Use packets that test for corner cases or non-general
purpose protocols that vendors may have decided not
to implement.

• Use packets that are not likely to be dropped in transit
(e.g. well-formed packets, packets expected to be ob-
served in the network under normal conditions, etc.).

We wrote a custom test program, ipv6fp.py [22], that had
the ability to create all 154 network probes (crafting network
packets with all required network layers), inject them into
the network and capture any responses sent by the target
host. We also implemented all IPv4 probes currently used
by Nmap for OS detection over IPv4, so we could double-
check consistency and also have the ability to tell diﬀerences
in upper layers when the underlying layer-3 changes. The
application was written in Python and used the Scapy li-
brary.

With the help from Nmap’s users and developers, we were
able to run the application against 26 diﬀerent operating
systems and versions, which are listed in Table 1.

We then created another application, fpanaly.py [21], to
analyze the results. In particular, we wanted to determine
which probes produced the highest amount of variability in
the responses. In order to facilitate the analysis, our tool
displayed the data from diﬀerent angles:

• For each probe, the diﬀerent values observed on each

protocol ﬁeld.

60• For each probe, percentage of hosts that replied to it.
• For each protocol ﬁeld, the diﬀerent values observed

on each probe.

• For each protocol ﬁeld, the diﬀerent values observed

on each OS.

• For all IPv4 tests and their IPv6 equivalent, all diﬀer-

ences in upper layers.

After analysis, we saw that 18.06% of the probes gener-
ated responses from all hosts, 72.9% were only responded by
some hosts, and 9.03% were not responded at all. Discard-
ing the probes that didn’t produce any replies, we analyzed
all responses to determine which ones showed the most non-
trivial diﬀerences for the same probe. We narrowed down
the list to a set of 18 probes, that we labeled S1 through
S6, IE1, IE2, NI, NS, U1, TECN, and T2 through T7. The
following sections describe the selected probes.

S1–S6: TCP sequence probes.

These are six TCP-based probes sent from diﬀerent source
ports but to a common open destination port (note that in
our implementation, Nmap performs a port scan beforehand,
so we already have a list of open ports on the target system).
The probes have the SYN ﬂag set so we force the other end
to reply back with a SYN/ACK packet, following the stan-
dard TCP three-way handshake.
In addition, each probe
contains a diﬀerent set of window sizes and TCP options,
which forces the other end to negotiate the connection in a
diﬀerent manner for each probe and shows its level of sup-
port for optional features like TCP timestamps and window
scaling. The table below shows the values that we use. Note
that the Timestamp option (indicated by “TS”) always has
the ﬁxed values TSval=0xFFFFFFFF, TSecr=0.

The probes in this set are sent sequentially, with a con-
stant interval of 100 ms. This allows us to analyze the initial
sequence numbers generated by the target host at speciﬁc
points in time and estimate the increment rate of its inter-
nal sequence number counter, which varies across operating
systems.

TCP options

S1 WScale=10, NOP, MSS=1460, TS, SAckOK
S2 MSS=1400, WScale=0, SAckOK, TS, EOL
S3 TS, NOP, NOP, WScale=5, NOP, MSS=640
S4
S5 MSS=536, SAckOK, TS, WScale=10, EOL
S6 MSS=265, SAckOK, TS

SAckOK, TS, WScale=10, EOL

window
1
63
4
4
16
512

TECN, T2-T7: additional TCP probes.

These probes are also used to extract information about
the implementation of TCP. In particular, they are designed
to test the response to various combinations of TCP ﬂags.
TECN, T2, T3, and T4 are sent to an open port while T5,
T6 and T7 are sent to a port that Nmap reported as closed
(a closed port is one that responds to a SYN probe with a
RST, not one that simply drops SYNs).

TECN sets the SYN ﬂag, plus the two high-order bits of
the ﬂags ﬁeld, ECE and CWR, which are used for explicit
congestion notiﬁcation [28]. It additionally sets the urgent
pointer, despite not setting the URG ﬂag. T2 sets no ﬂags
at all; T3 sets FIN, SYN, PSH, and URG; T4 and T6 set

ACK alone; T5 sets SYN and T7 sets FIN, PSH, and URG.
The options and window sizes vary as shown:

TCP options

window
3
TECN WScale=10, NOP, MSS=1460, SAckOK, NOP, NOP
128
T2 WScale=10, NOP, MSS=265, TS, SAckOK
T3 WScale=10, NOP, MSS=265, TS, SAckOK
256
T4 WScale=10, NOP, MSS=265, TS, SAckOK 1024
T5 WScale=10, NOP, MSS=265, TS, SAckOK 31337
T6 WScale=10, NOP, MSS=265, TS, SAckOK 32768
T7 WScale=15, NOP, MSS=265, TS, SAckOK 65535

Note that there is no separate T1 probe, since the S1 probe
already ﬁlls that purpose. The probe names were chosen to
keep harmony with the names of the existing IPv4 probes.

U1: UDP probe.

This probe is a UDP packet with a ﬁxed payload (300
octets with the value 0x43), sent to a closed port. Since
the UDP protocol does not have any built-in mechanism to
indicate a closed state, the expected response is an ICMPv6
Destination Unreachable message. We added this probe be-
cause of its historical use in Nmap’s IPv4 OS ﬁngerprinting
engine, but with no real evidence that the probe provides
meaningful information to the classiﬁer.

IE1, IE2, NI: ICMPv6 probes.

This is a set of three probes whose purpose is to test how
the target host responds to diﬀerent types of ICMPv6 pack-
ets.

IE1 is a regular ICMPv6 Echo Request message (ICMPv6
type=128), similar to those that sent by the ping utility but
with its ICMPv6 ﬁeld set to 0x09, even though RFC 4443
does not specify a code other than 0x00 [8]. The reason
why we set an arbitrary code is because we found that some
systems use the same value in their reply while others use
zero.

IE2 is also an ICMPv6 Echo Request but in this case, it is
designed to test how the remote host handles erroneous ex-
tension headers. The probe includes four extension headers:
Hop-by-Hop, Destination Options, Routing and again, Hop-
by-Hop. Such headers are not compliant with the standard
since no header other than Destination Options is supposed
to appear more than once, and the Hop-by-Hop header is
supposed to appear in the ﬁrst position only.

Since the packet is incorrect, it is rightly rejected by all the
IPv6 implementations that we tested. However, operating
systems disagree about what exactly is the nature of the
problem and they reject it in diﬀerent ways.

A small number of implementations (7.69%) discard the
packet. All the rest respond with an ICMPv6 Parameter
Problem error (type 4). Our tests show that some imple-
mentations set the code ﬁeld to 0 (“erroneous header ﬁeld”),
some others to 1 (“unrecognized Next Header type”), and the
rest to 2 (“unrecognized IPv6 option”). In addition, the Pa-
rameter Problem message has a ﬁeld that allows the system
originating the packet to point out which part of the original
packet is the cause of the problem. That ﬁeld, the Parame-
ter Problem Pointer, also showed diﬀerent values across the
set of analyzed implementations.

The NI probe is a Node Information query [11] that asks
the target to send back its IPv4 addresses. Its Qtype is 4
(“IPv6 addresses”) and it has the “A” ﬂag (“all unicast ad-
dresses”) set. Interestingly, all the operating systems that
we have seen responding to this probe (including OpenBSD

61and OS X) seem to misinterpret it and send back a host
name, not a list of addresses, as if the Qtype had been 2
(“node name”).

NS: Neighbor Solicitation probe.

The ﬁnal probe, NS, uses the Neighbor Discovery proto-
col [25], which is the equivalent of IPv4’s Address Resolution
Protocol (its main purpose is to allow the discovery of the
link-layer address that belongs to a particular IP address).
The expected response to the probe is a Neighbor Adver-
tisement message containing the target’s link-layer address.
Our tests show that diﬀerent implementations set diﬀerent
ﬂags in the reply, or even add extra options to it.

Note that in our implementation, the NS probe is only
sent to hosts on the same local network segment, since the
standard forbids hosts from replying to Neighbor Solicita-
tions if the hop limit is not 255.

4. FEATURE VECTORS

Nmap receives the responses to its probes as ﬂat byte
buﬀers. These it parses and converts to a single feature
vector. As of Nmap version 6.49, an IPv6 feature vector
consists of 676 features. Most of the features are associated
with a single response packet, and therefore are named after
the corresponding probe. For example, the hop limit is ex-
tracted from every IPv6 packet in features named S1.HLIM,
S2.HLIM, S3.HLIM, etc. The only exception to this rule is
the TCP ISR (initial sequence number rate) feature, which
is a derived from all the responses to the S1–S6 probes.

Features are scaled and oﬀset to lie within the range [0, 1].
The necessary scaling oﬀsets and coeﬃcients are stored with
the computed model and used to scale test samples at run-
time.

In practice, many ﬁngerprints do not have complete in-
formation. For example, if a host lacks a closed TCP port,
then there is no way to measure the responses to the T5,
T6, and T7 probes. Presumably the host would send some
kind of response if ﬁrewall rules allowed it—we just do not
know what it would be. The fact that a host did not send
a response to certain probes should not necessarily prevent
it from being classiﬁed the same way as another that did
send a response. On the other hand, the pattern of probe
responses and non-responses, in the absence of any inter-
ference by ﬁrewalls, can be a useful OS-speciﬁc signature.
The absence of certain features is always meaningful; for
example, a lack of TCP options.

We handle incomplete samples by reserving two special
feature values: MISSING and UNKNOWN. Postprocessing
on the feature vectors converts them to concrete numerical
values. MISSING features are those whose absence is pre-
sumed to be meaningful; for example missing TCP options.
UNKNOWN features are those that are presumed to exist,
but that could not be probed for some reason, for example
those belonging to a closed port on a host that does not have
a closed port. Although the distinction between MISSING
and UNKNOWN is maintained in vectorization, the current
engine discards the information and maps both values to −1.
Section 6.2 describes ongoing work to impute the values of
UNKNOWN features.

The remainder of this section describes each of the fea-

tures.

IPv6 packet features: PLEN, TC, HLIM.

These generic features are extracted from every IPv6 re-
sponse: PLEN, the total packet length; TC, the value of the
traﬃc class ﬁeld; and HLIM, the guessed initial hop limit.
All of them have numerical values that map directly to fea-
tures.

TCP window size: TCP_WINDOW.

The TCP window size, ignoring window scaling.

TCP ﬂags: TCP_FLAG_{F,S,R,P,A,U,E,C,RES8–10}.
These 12 zero–one features correspond to the 8 TCP ﬂag

bits, plus the four adjacent reserved bits.

TCP options: TCP_OPT_0–15, TCP_OPTLEN_0–15.

These features record the ﬁrst 16 TCP option types and
their lengths. (Any options after the 16th are ignored, but
our OS database does not contain any response with more
than 9 options.) As an example, consider these TCP options
(which happen to come from a SonicWALL ﬁrewall device):

NOP, NOP, MSS=1440, SAckOK, NOP, WScale=0

These options lead to the TCP OPT features:

[1, 1, 2, 4, 1, 3, MISSING, . . . , MISSING]

and the TCP OPTLEN features:

[1, 1, 4, 2, 1, 3, MISSING, . . . , MISSING].

(NOP has type 1 and length 1, MSS has type 2 and length 4,
and so on.) These features ignore the options’ values. Sep-
arate features, described in the next paragraph, extract the
values of some common options.

TCP options: TCP_MSS, TCP_SACKOK, TCP_WSCALE.

These features represent the values of some commonly
used TCP options. TCP SACK is a Boolean feature that
indicates whether the SAckOK (selective acknowledgment)
option is present. With the options string in the previous
paragraph, these features would have the values:

TCP MSS=1440
TCP SACKOK=1
TCP WSCALE=0

TCP initial sequence rate: TCP_ISR.

This feature records the average rate of increase of the se-
quence numbers in the responses to the S1–S6 probes. Many
versions of Linux, for example, use a 1 GHz counter.

5. CLASSIFICATION

The training database is composed mainly of user submis-
sions, with some manual additions by us. The database con-
tains known ﬁngerprints, grouped into classes that have tex-
tual labels like “Linux 2.6.38 – 3.2” and “Microsoft Windows
7 Professional SP1”.
In addition to these freeform labels,
each class has one or more machine-readable classiﬁcation
strings, which take the form of “vendor | family | version |
type” quadruples such as “Linux | Linux | 2.6.X | general
purpose” and Common Platform Enumeration [26] strings
like “cpe:/o:linux:linux kernel:2.6”.

62Using the database of training samples, we train a lin-
ear model using LIBLINEAR [14] in its L2-regularized lo-
gistic regression mode. The resulting model is embedded
in the Nmap binary. LIBLINEAR produces a number of
one-versus-rest binary classiﬁers, one for each OS class. At
runtime, an observed feature vector is tested against each
of these classiﬁers. For compatibility with the score format
of the older IPv4 classiﬁer, scores are mapped to the range
0–100%. A match is reported when a class scores 90% or
greater and it has suﬃciently low novelty; that is, if it does
not appear to be too dissimilar from the other feature vec-
tors in the class.

Naively applied, the linear model would not perform well
when faced with an feature vector unlike any it has seen
before. When faced with a totally new feature vector, one
not related to any in the training set, the classiﬁer’s out-
put can be nonsense. In the early days of deployment, we
observed the naive classiﬁer often to settle on an obscure
class with a score near 100%. To deal with this issue, the
classiﬁer employs a “novelty threshold” that avoids return-
ing a match when an observed feature vector is suﬃciently
unlike the members of the winning class. This is not only
to avoid presenting users with meaningless output, but also
because when there is no match, the system displays a ﬁn-
gerprint and encourages the user to submit it so that the
new operating system may be matched in a future version.
Some means of novelty detection is essential in OS de-
tection, because it is not possible to anticipate the uni-
verse of possible operating systems.
It is easy enough to
get good coverage of common desktop and server systems,
but the problem is fraught with the huge number of non-
mainstream, one-oﬀ and custom OSes such as those running
in embedded devices. Such embedded devices—for example
routers, printers, and ﬁrewalls—compose 30% of the IPv6
OS database. Even well-known, mainstream operating sys-
tems change over time. Especially given that the primary
source of updates to the database is user contributions, the
engine must know when to hazard an OS guess, and when
to admit ignorance and display a ﬁngerprint for submission.
Our deﬁnition of novelty is, brieﬂy, the Euclidean distance
from the observed feature vector to the mean of a class, in
a space where every dimension is scaled by the inverse of
the variance of the corresponding feature. The novelty com-
putation can be viewed as an approximation of the Maha-
lanobis distance [20]—it would be the same if features were
independent. In practice the approximation performs well
enough for its function of pruning outlandish matches, and
it has the advantage of not needing the entire feature covari-
ance matrix to be present at classiﬁcation time. The novelty
threshold is a manually tuned parameter. In Nmap version
6.49, it is set to 15.0. (Recall that there are ≈ 600 features
and that features are scaled to the range [0, 1], or [−1, 1]
when considering missing and unknown values. A non-novel
vector is one whose length is less than 15.0 in this space.)

When the variance of a feature is zero, as happens when
there is only one member of a training class or when a fea-
ture takes on only one value within a class, the inverse of the
variance is not deﬁned. In this case, we artiﬁcially set the
variance to a small constant. This has the eﬀect of making
any diﬀerence from the mean seem highly novel, and makes
the engine likely to print a ﬁngerprint for submission if there
are even a few such diﬀerences. The constant is a param-
eter that compromises between the desire to acquire a new

training ﬁngerprint with a never-before-seen feature value,
and the desire to give the user an OS match when the rest
of the features match up. It is unfortunately common for
classes to contain just one member—another eﬀect of the
diversity of OSes in the wild—making the need to acquire
new ﬁngerprints for them more acute.

Evaluation of the classiﬁer is hampered by the diﬃculty of
obtaining ground-truth testing samples. To obtain ground
truth, one needs to scan a host remotely and to have some
level of access to independently verify its operating system.
Even cross-validation is diﬃcult, because there are so many
training samples that are the sole member of their class—
they have no chance of being classiﬁed correctly if held out
during cross validation.
In our current database, 15% of
training samples are members of a one-element class, and
another 9% are members of a two-element class. The only
classes that have more than 10 elements are common desk-
top OSes: Linux, Windows, and OS X. With these caveats
in mind, the accuracy of leave-one-out cross validation on
our training set of 285 samples is 70.2%. (Near misses, for
example Linux being classiﬁed into the wrong range of ker-
nel revisions, are counted as inaccurate classiﬁcations for the
purpose of this calculation.)

6. WORK IN PROGRESS

In this discussion we discuss ﬁngerprinting possibilities we
have discovered and enhancements to the ﬁngerprinting en-
gine we are in the progress of testing and implementing.
6.1 New probes

We have described how we selected our current set of
probe packets. Since then, we have learned of other IPv6-
speciﬁc ﬁngerprinting features that could be tested with ad-
ditional probes. In choosing whether to implement a new
probe, we weigh its potential beneﬁt against the cost of ad-
ditional network traﬃc and implementation complexity.

IPv6 lacks the fragment identiﬁer ﬁeld that is a part of
every IPv4 packet, depriving us of tests that measure how
sequences of identiﬁers are generated (for example, whether
incremental or random, and whether global or host-speciﬁc).
Instead, fragmentation in IPv6 uses an extension header
that is not sent by default. However, by sending additional
probes, it is possible to force a remote host to append the
fragmentation extension header to its replies [24, 15], en-
abling the analysis of identiﬁer sequences.

There are more possibilities for experimentation with ex-
tension headers. Probes can be sent using legal and illegal
combinations of headers, in diﬀerent orders of those head-
ers. One example for this is the Routing header, which is
intended to provide a list of intermediate node addresses
as well as a count of how many of them are still left to
be visited. This header could for example contain a very
big list of nodes, or a node count that does not ﬁt within
the header bounds. In experiments with variations on the
Routing header, we have observed hosts to reply normally,
to report an erroneous header or destination unreachable
error, or to simply drop the probe.

The Multicast Listener Discovery (MLD) protocol [34] al-
lows discovery of multicast listeners on the local link. By
sending MLD queries onto the local network, it is possible
to identify diﬀerent OSes by the diﬀerent multicast groups
they listen to [2]. Like the existing NS probe, an MLD probe
would work only on the local network.

63We have found that even hosts with strict ﬁrewalls will
respond with an ICMPv6 Time Exceeded message when
they time out reassembling a fragmented packet. The Time
Exceeded message will contain a hop limit, which allows a
coarse determination of the OS. To induce such a reply, it
is necessary only to send the ﬁrst fragment of a fragmented
message and wait for a timeout.

There are multiple bugs in IPv6 stacks, such as operating
systems dropping ICMPv6 Packet Too Big messages that
declare a path MTU smaller than the IPv6 minimum of
1280 bytes, and hosts that stop responding at all or that
respond with incorrect TCP checksums after receiving such
a message [24]. These diﬀerences, despite being potentially
disruptive, allow for a better OS guess.

6.2 Feature imputation

Because network connections are not always reliable and
packets may be dropped by intermediate nodes on a path be-
tween two devices, some of the ﬁngerprints in the training
set are not complete. This leads to incomplete feature vec-
tors and a decrease in the accuracy of classiﬁcation. Scien-
tiﬁc literature proposes diﬀerent solutions to the problem of
missing data, although for practical reasons, not all of them
are applicable to our scenario. For example, one method of
dealing with missing data is to drop all incomplete ﬁnger-
prints, but the current average number of ﬁngerprints per
OS class is not high enough to permit this. Dropping exist-
ing ﬁngerprints would negatively inﬂuence the classiﬁcation
accuracy. Other methods which do not rely on removing in-
complete ﬁngerprints, such as replacing missing values with
the average of existing values from the same feature, also
have a negative impact since they may introduce bias. We
are now experimenting with imputing unknown feature val-
ues from related feature vectors.

We chose to apply multiple imputation by chained equa-
tions, following a custom-tailored approach adapted to the
existing workﬂow. For practical reasons, we made use of an
existing implementation in the R programming language [7].
Multiple imputation is not an algorithm in itself, but rather
a framework that speciﬁes how diﬀerent algorithms can be
combined in order to ﬁnd meaningful values for missing data.
The imputation process produces multiple matrices which
are copies of the original feature matrix, but without miss-
ing values. Each matrix is calculated by repeatedly applying
an imputation algorithm that infers missing values of one de-
pendent feature from existing values of all other features [4].
During one iteration, the algorithm is applied once to all
features that are missing values. After a certain number of
iterations, the imputed matrix should stabilize and converge.
The complete multiple imputation method pools all im-
puted matrices which then result in a set of parameters that
allow features to ﬁt a model of choice. During our testing,
we decided that feeding the complete feature matrix to the
imputation algorithm was not a feasible approach and de-
cided to group features together to form subsets of the main
feature matrix. These sub-matrices are then imputed in-
dividually. Because Nmap uses a logistic regression model
trained on the complete feature matrix, it was necessary to
combine the results of imputing all the sub-matrices into an
imputed version of the full original matrix. For this reason,
we developed a pooling method that can combine multiple
imputed sub-matrices, depending on the type of the vari-
ables they contain (e.g. binary, categorical, or continuous).

While developing this imputation method for Nmap’s train-
ing feature matrix, a number of additional research questions
arose. While some have been answered, there is ongoing re-
search that aims to perfect the scheme before being put into
practice. The questions fall into four topics that inﬂuence
the ﬁnal imputed matrix:

• The exact set of features to impute.
• The exact ways in which to group features into sub-

matrices.

• The number of imputed sets for each sub-matrix.
• The number of iterations per imputed set.

These questions inﬂuence two main factors, namely the time
required to run imputation and the quality of the data in the
ﬁnal imputed matrix. The problem of which set of features
to impute was solved by applying a recursive feature elimina-
tion algorithm that ﬁnds the features that have the greatest
eﬀect on accuracy during classiﬁcation. We used the imple-
mentation in the scikit-learn library [27]. Having a minimal
set of features, we did further testing to discover how they
should be grouped. The results show that features ought
to be grouped such that (1) relevant information links them
together (i.e., they are correlated); and (2) they are of the
same type. The choices for the number of imputed sets and
number of iterations per set aﬀect not only the amount of
time for execution, but also the quality of the ﬁnal imputed
matrix. Too few iterations mean that imputed sets do not
converge, while too many iterations require more time to
ﬁnish.

Our experiments indicate that imputation improves the
logistic regression model. Without imputation, the missing
values are mapped to −1 for the purpose of training, and as a
result the optimization process (e.g., gradient descent) that
computes the weights adds penalization. When the features
are imputed with meaningful values, the weights are penal-
ized less and the resulting model becomes more accurate.
Imputation realizes increases of 1–2% in cross-validation ac-
curacy and of the logit model even more when the model is
used directly with Nmap in our test environment.

7. DEPLOYMENT

The machine learning–based OS classiﬁcation system we
have described was ﬁrst released in September 2011 in Nmap
version 5.61TEST2. It originally had 62 OS classes, collected
from our and volunteers’ scans during development. We had
put out a call for ﬁngerprint submissions using the prototype
prober script of Section 3 in July 2011. The ﬁrst version of
the engine did not include the novelty threshold described
in Section 5, which probably hindered the rate of submis-
sion of new ﬁngerprints until it was integrated in version
5.61TEST5 in March 2012.

When a user runs Nmap OS detection against a target
and the classiﬁcation engine fails to ﬁnd a suﬃciently good
match, Nmap prints a ﬁngerprint and asks the user to sub-
mit it to a web form, along with any information the user
has about the remote operating system.
(The user can
override this behavior through the --osscan-guess option,
which forces the program to print the best OS match even
if too novel.) Despite this channel of user contributions, the
collection of adequate ground-truth training data remains

64that should be separately classed. It would be diﬃcult to
modify one operating system to completely imitate another
at the network level. A better way to frustrate ﬁngerprinting
is to pass traﬃc through a gateway that scrubs diﬀerences
in network stacks, though even this approach presents diﬃ-
culties [32].

There can arise interesting cases of “hybrid” ﬁngerprints,
in which some probes receive responses from one host and
other probes from another. In this case, the ﬁngerprint will
have a mixture of features from diﬀerent systems and what
label it should receive is not well deﬁned. This can happen,
for example, with virtualized systems and transparent prox-
ies. (We once saw a high-latency satellite link that would
spoof SYN/ACKs in response to SYN on any port, evidently
to decrease perceived latency—a port scan through such a
link would show every port on every host as open.) The sys-
tem we have described in this paper ﬁngerprints whatever
device ultimately generates the network- and transport-layer
packets, which may not match the user’s idea of what the
target host really is. Apart from the system we have de-
scribed, Nmap also has an independent application-layer OS
classiﬁer that scrapes version information from SSH banners
and the like. It can happen that the low-level OS classiﬁ-
cation is diﬀerent than the application-layer classiﬁcation—
and that nevertheless both are correct. An example of this
is an web server running behind a load balancer: the low-
level ﬁngerprint will be that of the load balancer and the
application-layer ﬁngerprint will be that of the web server.

8. SUMMARY

We have presented a machine learning–based classiﬁer for
operating systems based on remotely measurable network
characteristics over IPv6. The classiﬁer is deployed in the
popular Nmap security scanner. We described the challenges
and opportunities of operating system ﬁngerprinting over
IPv6, explained the reasoning behind our design decisions,
and introduced work in progress to improve the classiﬁer.

9. ACKNOWLEDGMENTS

This work was supported in part by funding from the
Open Technology Fund through the Freedom2Connect Foun-
dation and from the US Department of State, Bureau of
Democracy, Human Rights and Labor. The opinions in this
paper are those of the authors and do not necessarily reﬂect
those any funding agency or governmental organization.

10. REFERENCES
[1] O. Arkin and F. Yarochkin. Xprobe v2.0: A “fuzzy”

approach to remote active operating system
ﬁngerprinting. Technical report, Aug. 2002.
http://www.ouah.org/Xprobe2.pdf.

[2] A. Atlas. MLD considered harmful?, Nov. 2014.

http://www.insinuator.net/2014/11/mld-considered-
harmful/.

[3] P. Auﬀret. SinFP, uniﬁcation of active and passive

operating system ﬁngerprinting. Journal in Computer
Virology, 6(3):197–205, Aug. 2010.
http://patriceauﬀret.com/ﬁles/publications/jcv.pdf.

[4] M. J. Azur, E. A. Stuart, C. Frangakis, and P. J. Leaf.

Multiple imputation by chained equations: what is it
and how does it work? International journal of
methods in psychiatric research, 20(1):40–49, 2011.

Figure 2: Growth of the IPv6 database over time.
Decreases in the number of classes reﬂect consolida-
tion after manual re-evaluation of training samples.

challenging. Figure 2 shows how the IPv6 OS database has
grown over time. The adoption of IPv6 is still far below
that of IPv4, and the diﬀerence in submission rates between
IPv4 and IPv6 is stark. Between June 2013 and Febru-
ary 2015, we received over 4,700 IPv4 submissions. In the
same period, we received only 97 for IPv6. We would prefer
to have more training samples, in order to reﬂect real-world
diversity in end hosts and network routes, while still keeping
distinguishable OS versions in separate, speciﬁc classes.

Assigning newly submitted ﬁngerprints to classes is a su-
pervised, mostly manual aﬀair. There are few enough sub-
missions that each one is reviewed manually. The submis-
sion form asks the user for information such as how they
know what OS is running, and the OS version down to the
patch level (using the output of uname -a or an equivalent).
We have not detected any malicious submissions that ap-
pear intended to poison the classiﬁcation database, though
the process of vetting submissions is admittedly subjective
and involves human judgment.
In reviewing submissions,
we look for evidence that the submitter really knew what
OS was running, and cross-check the submission against the
existing database to see that it is not obviously some other
OS. A large fraction of submissions are ignored for a lack of
information. Occasionally we follow up with the submitter
by email in order to get more details.

A database ﬁngerprint consists of the raw byte contents
of the responses to each probe, with certain sensitive ﬁelds
like the source and destination addresses redacted. There
are extra metadata ﬁelds for information that cannot be
recovered from packet contents alone, such as the time oﬀset
at which the response was received. We store these redacted
packet dumps, not the processed feature vectors, so that we
may add or remove features, or change how features are
computed, without having to discard past training samples.
Our system is mostly designed to work against operating
systems in their usual conﬁgurations, not against adversarial
conﬁgurations that deliberately obscure their network ﬁn-
gerprint. Most of our probes are distinctive and would be
easy to detect and block. Some of the features we use—the
initial hop limit for example—are easily modiﬁable from user
space. Our classiﬁer takes advantage of the fact that settings
like these are rarely changed away from the defaults—and
when they are, they may represent a custom distribution

number of classesnumber of samples100200201220132014201565[5] T. Beardsley. Snacktime: A Perl solution for remote

[22] L. MartinGarcia. [tool] ipv6fp.py: IPv6 OS detection

OS ﬁngerprinting, June 2003.
http://www.planb-security.net/wp/snacktime.html.

[6] R. Beverly. A robust classiﬁer for passive TCP/IP
ﬁngerprinting. In C. Barakat and I. Pratt, editors,
Passive and Active Network Measurement, volume
3015 of Lecture Notes in Computer Science, pages
158–167. Springer Berlin Heidelberg, 2004.
https://www.cl.cam.ac.uk/research/srg/netos/
pam2004/papers/260.pdf.

[7] S. Buuren and K. Groothuis-Oudshoorn. mice:

Multivariate imputation by chained equations in R.
Journal of statistical software, 45(3), 2011.
http://www.jstatsoft.org/v45/i03/paper.

[8] A. Conta, S. Deering, and M. Gupta. Internet Control
Message Protocol (ICMPv6) for the Internet Protocol
Version 6 (IPv6) speciﬁcation. RFC 4443, RFC
Editor, Mar. 2006. https://tools.ietf.org/html/rfc4443.

[9] J. Corbet. TCP window scaling and broken routers,

July 2004. https://lwn.net/Articles/92727/.

[10] J. Corbet. Increasing the TCP initial congestion

window, Feb. 2011. https://lwn.net/Articles/427104/.

[11] M. Crawford and B. Haberman. IPv6 Node

Information queries. RFC 4620, RFC Editor, Aug.
2006. https://tools.ietf.org/html/rfc4620.

[12] S. E. Deering and R. M. Hinden. Internet Protocol,

version 6 (IPv6) speciﬁcation. RFC 2460, RFC Editor,
Dec. 1998. https://tools.ietf.org/html/rfc2460.
[13] J. Edge. Networking change causes distribution

headaches, Oct. 2008.
https://lwn.net/Articles/304791/.

[14] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang,

and C.-J. Lin. LIBLINEAR: A library for large linear
classiﬁcation. Journal of Machine Learning Research,
9:1871–1874, 2008.
http://www.csie.ntu.edu.tw/˜cjlin/liblinear/.

[15] F. Gont. Processing of IPv6 “atomic” fragments. RFC

6946, RFC Editor, May 2013.
https://tools.ietf.org/html/rfc6946.

[16] L. G. Greenwald and T. J. Thomas. Toward

undetected operating system ﬁngerprinting. In 1st
USENIX Workshop on Oﬀensive Technologies, 2007.
https://www.usenix.org/event/woot07/tech/full
papers/greenwald/greenwald.pdf.

[17] V. Jacobson, B. Braden, and D. Borman. TCP

extensions for high performance. RFC 1323, RFC
Editor, May 1992. https://tools.ietf.org/html/rfc1323.

[18] G. Lyon. Remote OS detection via TCP/IP stack

ﬁngerprinting. Phrack, 8(54), Oct. 1998.
https://nmap.org/nmap-ﬁngerprinting-old.html.

[19] G. Lyon. Nmap Network Scanning, chapter Remote

OS Detection. 2009.
https://nmap.org/book/osdetect.html.

[20] P. C. Mahalanobis. On the generalised distance in
statistics. Proceedings of the National Institute of
Sciences of India, 2(1):49–55, 1936.

[21] L. MartinGarcia. [tool] fpanaly.py: IPv6 OS detection

test result analysis tool, June 2011.
https://svn.nmap.org/nmap-
exp/luis/ipv6tests/fpanaly.py?p=34606.

test suite, June 2011. https://svn.nmap.org/nmap-
exp/luis/ipv6tests/ipv6fp.py?p=34606.

[23] J. P. S. Medeiros, A. C. da Cunha, A. M. Brito, and

P. S. Motta Pires. Automating security tests for
industrial automation devices using neural networks.
In IEEE Conference on Emerging Technologies and
Factory Automation, pages 772–775, Sept. 2007.

[24] M. Morbitzer. TCP idle scans in IPv6. Master’s thesis,

Radboud University Nijmegen, Aug. 2013.
https://www.ru.nl/publish/pages/578936/m
morbitzer masterthesis.pdf.

[25] T. Narten, E. Nordmark, W. Simpson, and

H. Soliman. Neighbor discovery for IP version 6
(IPv6). RFC 4861, RFC Editor, Sept. 2007.
https://tools.ietf.org/html/rfc4861.

[26] National Institute of Standards and Technology.

Common Platform Enumeration (CPE).
https://nvd.nist.gov/cpe.cfm.

[27] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,

B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,
R. Weiss, V. Dubourg, J. Vanderplas, A. Passos,
D. Cournapeau, M. Brucher, M. Perrot, and
E. Duchesnay. Scikit-learn: Machine learning in
Python. Journal of Machine Learning Research,
12:2825–2830, 2011.

[28] K. Ramakrishnan, S. Floyd, and D. Black. The

addition of explicit congestion notiﬁcation (ECN) to
IP. RFC 3168, RFC Editor, Sept. 2001.
https://tools.ietf.org/html/rfc3168.

[29] D. W. Richardson, S. D. Gribble, and T. Kohno. The

limits of automatic os ﬁngerprint generation. In
Proceedings of the 3rd ACM Workshop on Artiﬁcial
Intelligence and Security, AISec ’10, pages 24–34, New
York, NY, USA, 2010. ACM. https://homes.cs.
washington.edu/˜yoshi/papers/fuzzing aisec2010.pdf.
[30] C. Sarraute and J. Burroni. Using neural networks to

improve classical operating system ﬁngerprinting
techniques. Electronic Journal of SADIO, 8(1):35–47,
2008. http://www.coresecurity.com/ﬁles/attachments/
Sarraute EJS.pdf.

[31] Z. Shamsi, A. Nandwani, D. Leonard, and

D. Loguinov. Hershel: Single-packet OS ﬁngerprinting.
In The 2014 ACM International Conference on
Measurement and Modeling of Computer Systems,
SIGMETRICS ’14, pages 195–206, New York, NY,
USA, 2014. ACM. http://irl.cse.tamu.edu/people/
zain/papers/sigmetrics2014.pdf.

[32] M. Smart, G. R. Malan, and F. Jahanian. Defeating

TCP/IP stack ﬁngerprinting. In 9th USENIX Security
Symposium, Aug. 2000.
https://www.usenix.org/legacy/publications/library/
proceedings/sec2000/full papers/smart/smart.pdf.

[33] F. Veysset, O. Courtay, and O. Heen. New tool and

technique for remote operating system ﬁngerprinting.
Apr. 2002.
http://www.gomor.org/ﬁles/ring-full-paper.pdf.

[34] R. Vida and L. Costa. Multicast listener discovery
version 2 (mldv2) for ipv6. RFC 3810, RFC Editor,
June 2004. https://tools.ietf.org/html/rfc3810.

66[35] F. V. Yarochkin, O. Arkin, M. Kydyraliev, S.-Y. Dai,
Y. Huang, and S.-Y. Kuo. Xprobe2++: Low volume
remote network information gathering tool. In
IEEE/IFIP International Conference on Dependable
Systems & Networks, pages 205–210. IEEE, 2009.
http://xprobe.sourceforge.net/xprobe-ng.pdf.

[36] M. Zalewski. p0f - passive os ﬁngerprinting tool, June

2000. http://seclists.org/bugtraq/2000/Jun/141.

[37] M. Zalewski. [tool] the new p0f 2.0.1 is now out, Sept.

2003. http://seclists.org/bugtraq/2003/Sep/36.
[38] M. Zalewski. p0f3 release candidate, Jan. 2012.
http://seclists.org/fulldisclosure/2012/Jan/123.

67