Membership Privacy: A Unifying Framework For Privacy

Deﬁnitions

Ninghui Li, Wahbeh Qardaji, Dong Su, Yi Wu, Weining Yang

Department of Computer Science and CERIAS, Purdue University
{ninghui,wqardaji,su17,wu510,yang469}@cs.purdue.edu

ABSTRACT

We introduce a novel privacy framework that we call Membership
Privacy. The framework includes positive membership privacy,
which prevents the adversary from signiﬁcantly increasing its a-
bility to conclude that an entity is in the input dataset, and negative
membership privacy, which prevents leaking of non-membership.
These notions are parameterized by a family of distributions that
captures the adversary’s prior knowledge. The power and ﬂexibili-
ty of the proposed framework lies in the ability to choose different
distribution families to instantiate membership privacy. Many pri-
vacy notions in the literature are equivalent to membership privacy
with interesting distribution families, including differential privacy,
differential identiﬁability, and differential privacy under sampling.
Casting these notions into the framework leads to deeper under-
standing of the strengthes and weaknesses of these notions, as well
as their relationships to each other. The framework also provides a
principled approach to developing new privacy notions under which
better utility can be achieved than what is possible under differen-
tial privacy.

Categories and Subject Descriptors

K.4.1 [COMPUTERS AND SOCIETY]: Privacy

Keywords

Differential Privacy; Privacy Notions; Membership Privacy

1.

INTRODUCTION

The spate of privacy related incidents [30, 3, 27, 17] has spurred
a long line of research in privacy notions for data publishing and
analysis [30, 29, 24, 21]. A privacy notion that is increasingly
gaining acceptance is differential privacy [7, 10].
Informally, d-
ifferential privacy requires any individual entity in a dataset to have
only a limited impact on the output. More speciﬁcally, differential
privacy requires that any two neighboring input datasets will induce
output distributions that are close in the sense that the probabilities
of each possible output differ by a bounded multiplicative factor.
There are two major ﬂavors of differential privacy, depending on

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516686 .

the condition under which two datasets are considered to be neigh-
bors. In [19], these were referred to as unbounded and bounded
differential privacy. In Unbounded Differential Privacy (UDP), T
and T ′ are neighbors if T can be obtained from T ′ by adding or
removing an entity. In Bounded Differential Privacy (BDP), T and
T ′ are neighbors if T can be obtained from T ′ by replacing one
entity in T ′ with another entity.

Because privacy is a social notion with many facets, there is
a long tradition in the research community to examine the vari-
ous technical formulations of privacy in order to understand their
strengthes and weaknesses. Several researchers have questioned
whether differential privacy provides sufﬁcient protection, and how
to choose the ǫ parameter. In [19], Kifer and Machanavajjhala ar-
gued that it is incorrect to claim that differential privacy is robust
to arbitrary background knowledge. In [5], Cormode argued that
differential privacy does not prevent inferential disclosure. That is,
from differentially private output, it is possible to infer potentially
sensitive information about an individual with non-trivial accura-
cy. In [20], Lee and Clifton argued that while the parameter ǫ in
ǫ-DP limits how much one individual can affect the output, it does
not limit how much information is revealed about an individual;
and this does not match legal deﬁnition of privacy, which requires
protection of individually identiﬁable data. Lee and Clifton pro-
posed the notion of differential identiﬁability. At the same time, it
has been recognized that differential privacy may be too restrictive
in some settings, and there are several efforts that aim at relaxing
it, including differential privacy under sampling [22] and crowd
blending privacy [15].

This paper is motivated by these lines of work. Our aim is to
gain a deeper understanding of privacy both as a social concep-
t and in terms of technical formulations. We begin by analyzing
the recent privacy incidents. Our analysis concludes that what so-
ciety often views as a privacy breach is the ability of an adversary
to either re-identify or assert the membership of an individual in a
supposedly “anonymized” dataset. Hence, a privacy measure needs
to protect everyone in the anonymized dataset against membership
disclosure. Such a privacy deﬁnition, however, is incomplete with-
out specifying the adversary’s prior knowledge about what might
be in the dataset. The need to consider this background knowledge
is indeed apparent from recent privacy breaches [3, 27].

We combine these two requirements and introduce a novel pri-
vacy framework that we call Membership Privacy. This framework
comprises of two notions: Positive Membership Privacy (PMP),
which prevents an adversary from signiﬁcantly improving its con-
ﬁdence that an entity is in the input dataset; and Negative Mem-
bership Privacy (NMP), which prevents an adversary from signiﬁ-
cantly improving its conﬁdence that an entity is not in the dataset.
These notions are parameterized by two parameters D and γ. The

889Distribution
Family

Description of Distributions in the Family

Equivalent Privacy Notion

DU
DI
D2
I

Dβ
F

DB

D2
B

Dm
C

DN

Includes all distributions over 2U ; all other families are sub-families of this.
Includes all mutually independent (MI) distributions
Sub-family of DI . Includes MI distributions that have only two datasets with non-
zero probability, i.e., distributions in which Pr[T ∪ {t}] = p and Pr[T ] = 1 − p.
Sub-family of DI . Includes MI distributions such that all entities have probability
in {0, β}, i.e., all entities that may appear have a ﬁxed probability.
Includes distributions that are the conditional distributions of some MI distribution
conditioned upon that all datasets with non-zero probability have the same size.
Sub-family of DB. Includes distributions where Pr[T ∪ {t1}] = p and Pr[T ∪
{t2}] = 1 − p
Sub-family of DB. Includes all distributions in which Pr[{T ∪ {t1}] = Pr[T ∪
{t2}] = Pr[T ∪ {tm}] = 1
Sub-family of D0.5
i.e., all entities have probability 0.5.

F . Include the single uniform distribution over all subsets of U,

m and T ∪ {t1, · · · , tm} = U

Privacy with no Utility
Unbounded Differential Privacy [8]
Unbounded Differential Privacy

Differential Privacy Under Sam-
pling [22]
Bounded Differential Privacy [10]

Bounded Differential Privacy

Differential Identiﬁability [20]

New Privacy Notion

Table 1: Distributions for which membership privacy is considered in this paper, and their equivalent privacy notions in the literature.

ﬁrst parameter captures an adversary’s prior knowledge. In partic-
ular, D is a set of probability distributions. Each element D ∈ D
is a probability distribution over all possible datasets, and encodes
one possible state of prior knowledge of the adversary. D as a set
captures all states of prior knowledge against which membership
privacy is guaranteed. The second parameter γ is a number that is
greater than or equal to 1; it limits the increase in conﬁdence of
accurate membership assertion.

The power of the membership privacy framework is demonstrat-
ed by the fact that many privacy notions in the literature are equiv-
alent to membership privacy with interesting distribution families.
These notions and their corresponding distribution families are giv-
en in Table 1. For example, Unbounded Differential Privacy (UDP)
is equivalent to membership privacy in DI , the family of all mutual-
ly independent (MI) distributions. Similarly, Bounded Differential
Privacy (BDP) is equivalent to membership privacy under DB, the
family that includes those obtained by conditioning a MI distribu-
tion such that all datasets with non-zero probability have the same
size. Differential identiﬁability [20] and Differential Privacy under
Sampling [22] are also instantiations of membership privacy.

Identifying the family under which a privacy notion guarantees
membership privacy provides deeper understanding of the power
and limitation of the privacy notion. For example, this framework
enables us to show that under what condition differential identiﬁa-
bility is equivalent to BDP and under what condition that it is strict-
ly weaker. We stress that almost all privacy notions make some
assumptions about the adversary’s background knowledge. For ex-
ample, differential privacy’s main assumption is independence, as
also pointed out in [19]. The only membership privacy notion with-
out any assumption is the one under DU , the family that includes
all distributions. We show that this essentially requires giving up
all utility. This is another formulation of the “no free lunch” result
in [19, 8, 12].

As all practical privacy notion requires some assumptions on the
allowed distributions, it makes sense to analyze whether the as-
sumption made in a notion is appropriate for a given setting, and
choose a privacy that is neither too strong nor too weak, in order
to maximize utility. Our membership privacy framework enables
such analysis. One could develop privacy notions that are stronger
than differential privacy (satisfying membership privacy for beyond
DI ), as well as ones that are weaker (satisfying membership privacy
for a sub-family of DI ).

It has often been recognized that differential privacy can be too
strong to satisfy in some settings, and there are some efforts aim-
ing at relaxing it. Our membership privacy framework provides a
principled way to conduct this. For example, one weaker priva-
cy notion that may be useful for some applications is membership
privacy under DN , which includes the single uniform distribution
over all possible datasets. As a demonstration that this notion en-
ables higher utility, we show that under this notion it is possible
to compute the max value of a set with high accuracy, which is
difﬁcult to do under differential privacy.

The rest of the paper is organized as follows. In Section 2, we
analyze privacy incidents and motivate membership privacy. We
then introduce the membership privacy framework in Section 3,
show how differential privacy ﬁts in the framework in Section 4,
and consider several other instantiations of the framework in Sec-
tion 5. Finally, we discuss related work in Section 6 and conclude
in Section 7.

2. WHAT IS PRIVACY?

Similar to other contexts in security and privacy, the concept of
privacy is easier to deﬁne by identifying what are privacy breach-
es. Privacy can then be simply deﬁned by requiring that no privacy
breach occurs. As privacy is a social concept, any formalization
of privacy violation must be based on what the society perceives
as privacy breaches.
In this section, we examine several well-
publicized privacy incidents in data publishing in recent years, and
identify the common features of what the society considered to be
privacy breaches. We show that the disclosures in these incidents
all fall into a general class that we call positive membership disclo-
sures. In such an disclosure, when given the published dataset, an
adversary can ﬁnd an entity t and assert with high conﬁdence that
t’s data is in the original dataset.

2.1 Privacy Incidents

An early and well publicized privacy incident is from the suppos-
edly anonymized medical visit data made available by the Group
Insurance Commission (GIC) [30]. While the obvious personal i-
dentiﬁers are removed, the published data included zip code, date
of birth, and gender, which are sufﬁcient to uniquely identify a sig-
niﬁcant fraction of the population. Sweeney [30] showed that by
correlating this data with the publicly available Voter Registration

890List for Cambridge Massachusetts, medical visits about many indi-
viduals can be easily identiﬁed, including those of William Weld, a
former governor of Massachusetts. We note that even without ac-
cess to the public voter registration list, the same privacy breaches
can occur. Many individuals’ birthdate, gender and zip code are
public information. This is especially the case with the advent of
social media, including Facebook, where users share seemingly in-
nocuous personal information to the public.

Another well-known privacy incident came from publishing we-
b search logs. In 2006, AOL released three months of search logs
involving 650,000 users. The only privacy protection technique
used is replacing user ids with random numbers. This proved to
be a failure. Two New York Time journalists [3] were able to re-
identify Thelma Arnold, a 62 year old women living in Lilburn,
Ga. from the published search logs. Ms. Arnold’s search log in-
cludes her last name and location names near where she lived. The
reporters were able to cross-reference this information with phone-
book entries. After the New York Time article is published, the
data was immediately retracted by AOL. Later a class action law-
suit was ﬁled against AOL. This scandal led to the resignation of
AOL’s CTO and the dismissal of two employees.

In 2009, Netﬂix released a dataset containing the movie rating
data from 500,000 users as part of a one-million dollar challenge to
the data mining research community for developing effective algo-
rithms for predicting users’ movie preferences based on their view-
ing history and ratings. While the data was anonymized in order to
protect users’ privacy, Narayanan and Schmatikov [27] showed that
an adversary who has some knowledge about a subscriber’s movie
viewing experience can easily identify the subscriber’s record if it
is present in the dataset. For example, [27] shows that, from the
proﬁles of 50 IMDB users, at least 2 of them also appear in the
Netﬂix dataset.

Another privacy incident targeted the Genome-Wide Associa-
tion Studies (GWAS). These studies normally compare the DNA
of two groups of participants: people with the disease (cases) and
similar people without (controls). Each person’s DNA mutations
(single-nucleotide polymorphisms, or SNPs) at indicative locations
are read, and the data is then analyzed. Traditionally, researchers
publish aggregate frequencies of SNPs for participants in the two
groups. In 2008, Homer et al. [17] proposed attacks that could tell
with high conﬁdence whether an individual is in the case group, as-
suming that the individual’s DNA is known. The attack works even
if the group includes hundreds of individuals. Because of the priva-
cy concerns from such attacks, a number of institutions, including
the US National Institute of Health (NIH) and the Wellcome Trust
in London all decided to restrict access to data from GWAS. Such
attacks need access to the victim’s DNA data and publicly avail-
able genomic database to establish the likely SNP frequencies in
the general population.

2.2 Lessons from Privacy Incidents

From these incidents, we learn the following lessons.
Re-identiﬁcation matters.

In the GIC, AOL, and Netﬂix inci-
dents, one is able to correctly identify one individual’s record from
supposedly anonymous data. This fact alone is sufﬁcient for the so-
ciety to agree that privacy is breached. It does not matter whether
an adversary has learnt additional sensitive information about the
individual.

Positive assertion of membership matters. In the GWAS exam-
ple, only aggregate information is published and there is no indi-
vidual record for re-identiﬁcation to occur. However, so long as
the adversary can positively assert that one individual’s data is in

the input dataset based on the output, this is considered a privacy
breach.

Must protect everyone. In several attacks, only a single individ-
ual is re-identiﬁed; however, that is sufﬁcient to cause widespread
privacy concerns and serious consequences (e.g., the AOL case).
This suggests that privacy protection must apply to every individ-
ual. A method that on average offers good protection, but may
compromise some individual’s privacy is not acceptable.

No separation of Quasi-Identiﬁer and Sensitive Attributes. Much
of database privacy research assumes the division of all attributes
into quasi-identiﬁers (QIDs) and sensitive attributes (SA), where
the adversary is assumed to know the QIDs, but not SAs. This
separation, however, is very hard to obtain in practice. No such
separation exists in the cases of AOL, Netﬂix, or GWAS. Even
though only some attributes are used in the GIC incident, it is d-
ifﬁcult to assume that they are the only QIDs. Other attributes in
the GIC data includes visit date, diagnosis, etc. There may well
exist an adversary who knows this information about some individ-
uals, and if with this knowledge these individuals’ record can be
re-identiﬁed, it is still a serious privacy breach. The same difﬁculty
is true for publishing any kind of census, medical, or transactional
data. When publishing anonymized microdata, one has to defend a-
gainst all kinds of adversaries, some know one set of attributes, and
others know a different set. An attribute about one individual may
be known by some adversaries, and unknown (and thus should be
considered sensitive) for other adversaries. In summary, one should
assume that for every individual, there may exist an adversary who
knows all attributes of that individual.

2.3 Positive Membership Disclosure

The discussions above suggest that a privacy breach is a positive
assertion of membership for some entity t all attributes of which
may be known by the adversary. We note that re-identiﬁcation, i.e.,
pointing to some feature of the output dataset and concluding that
this feature reﬂects that t is in the input dataset, implies an assertion
of positive membership. Therefore, any reidentiﬁcation attack will
also result in a positive membership assertion. After all, if one
cannot be sure that an entity t is in the input dataset, one cannot be
sure that a particular feature in the dataset is attributable to t.

Privacy breach = Positive Membership Disclosure:
Given an input dataset T , we say that the output O =
A(T ) is subject to positive membership disclosure,
when there exists a entity t such that an adversary can
assert: the fact that O is output indicates that it is high-
ly likely that t is in the input dataset T .

Note that whether positive membership disclosure occurs or not
is not just a property of the output O = A(T ). In order to say that
O indicates t is in the input dataset, one must consider the behavior
of the algorithm A on datasets that do not include t.

Some authors have considered privacy breaches as attribute dis-
closures, i.e., the ability to infer one’s sensitive attributes from the
data publishing. In [5], it has been shown that while satisfying dif-
ferential privacy, one could still build reasonably accurate classiﬁer
to learn sensitive attribute values of some entity. We argue that
attribute disclosure is problematic as a privacy notion. As shown
by Dwork and others [8], attribute disclosure may occur even if an
entity’s data is not included in the input dataset. For example, sup-
pose that one conducts a study and collects data from individuals
in a particular population group, e.g., of a certain age and ethnicity
group, and these individuals are willing to share their data. Fur-
ther suppose publishing this dataset enables one to learn new at-
tribute information about this group, e.g., they have a much higher

891probability of have some disease than the general population. This
new attribute information, however, applies to any individual in this
population group, even the ones that do not contribute data, and en-
ables the learning of new sensitive attribute information about the
individual. We believe that calling such an attribute disclosure a
privacy violation is incorrect. Under this interpretation of privacy,
an individual could claim privacy violation if there is any data about
anyone with some common feature (e.g., is of the same gender) as
the individual.

3. THE MEMBERSHIP PRIVACY FRAME-

WORK

In this section, we introduce our framework for formalizing
membership privacy. More speciﬁcally, we introduce the notion
of Positive Membership Privacy (PMP), which prevents Positive
Membership Disclosure. To enable establishing a clear connection
with differential privacy, we also introduce Negative Membership
Privacy (NMP), which prevents an adversary from increasing sig-
niﬁcantly the conﬁdence that a particular entity’s data does not oc-
cur in the dataset.

3.1 Notations

We assume that there is a universe U of entities, and each dataset
is generated by choosing a set T ⊆ U of entities to be included in
the dataset. That is, there is a deterministic procedure G such that
every dataset is given by G(T ) for some T ⊆ U. Below, we often
abuse the terminology and call T the dataset.

Each entity t ∈ U corresponds to a physical entity that exists in
the physical world and needs privacy protection. For example, in
many scenarios, a physical entity corresponds to an individual, i.e.,
a human being.

When we are working with relational datasets, typically an en-
tity corresponds to one tuple. However, it is possible that in some
scenarios one would need to have an entity corresponding to a set
of tuples. For example, in a database of medical insurance infor-
mation for employees and their family members, it may be neces-
sary to treat an employee’s family as an entity. When we deal with
non-relational datasets such as social network data, one may want
to choose a social network account as an entity so that when the
account is not included, the dataset should not include any infor-
mation related to the account.

The adversary may have prior beliefs about what the dataset is;
this is captured by a distribution D over 2U , the powerset of U.
A distribution D assigns a probability to each possible subset of
U. From the adversary’s point of view, the dataset is a random
variable drawn according to the distribution D. We use T to denote
this random variable.

When publishing a dataset, often one is unable to ﬁx one par-
ticular distribution D. One needs to defend against multiple adver-
saries that may have very different beliefs. Even if one is concerned
with a single adversary, it is often impossible to know exactly what
the adversary believes. Therefore, we need to consider a family
of possible distributions. This is modeled by D, which is a set of
distributions. That is, each D ∈ D is a distribution over 2U . The
family D speciﬁes what kind of background knowledge regarding
the underlying dataset we allow the adversary to have. For perfec-
t privacy, one would desire D to include all possible distributions
over 2U . However, as we show later, achieving such perfect priva-
cy would require not publishing any meaningful information of the
underlying dataset.

3.2 Positive Membership Privacy

While the notion of membership privacy has been alluded to in
several papers, e.g., [19, 20], it has not been formalized to deal with
the possibility of different kinds of background knowledge for the
adversary. We now provide such a formalization.

DEFINITION 3.1. [Positive Membership Privacy

((D, γ)-
PMP)]: We say that a mechanism A provides γ-positive mem-
bership privacy under a family D of distributions over 2U , i.e.,
((D, γ)-PMP), where γ ≥ 1, if and only if for any S ⊆ range(A),
any distribution D ∈ D, and any entity t ∈ U , we have

Pr
D,A

[t ∈ T | A(T) ∈ S] ≤ γ Pr
D

[t ∈ T]

and Pr
D,A

[t 6∈ T | A(T) ∈ S] ≥

PrD[t 6∈ T]

γ

(1)

(2)

where T is a random variable drawn according to the distribution
D.

With a slight abuse of notation, we use S to denote the event
A(T) ∈ S, t to denote the event that t ∈ T and ¬ t to denote the
PrD [t] ≤
event that t 6∈ T. Equation (1) can then be written as
γ, where PrD[t] denotes the prior belief that entity t is in an input
dataset sampled from the distribution D, and PrD,A[t|S] denotes
the posterior belief that t is in the input dataset after observing that
the event S has happened for the output of A. When D and A are
obvious from the context, we drop them from the subscript and
write Pr[t] and Pr[t|S]. Equation (1) requires that Pr[t|S] can
increase at most by a factor of γ over the prior belief Pr[t].

PrD,A[t|S]

Equation (1) by itself, however, may not offer sufﬁcient protec-
tion when the prior belief Pr[t] is already quite large. For example,
setting γ = 1.2 might seem a reasonable strong privacy protection.
However, if Pr[t] = 0.85, then Equation (1) will bound the poste-
rior belief Pr[t|S] to be less than 0.85 ∗ 1.2 = 1.02. This allows
an adversary to be 100% certain that t is in the input dataset after
observing the output, which is arguably undesirable.

Equation (2), which can be written as Pr[¬ t|S]

γ , provides
a more effective upper-bound for the posterior belief Pr[t|S] when
the prior is large. In the above example, Pr[¬ t|S] is lower-bounded
by (1 − 0.85)/1.2 = 0.125, i.e., Pr[t|S] can increase from 0.85 to
at most 0.875. Equations (1) and (2) together are equivalent to:

Pr[¬ t] ≥ 1

Pr[t|S] ≤ min(cid:18)γ Pr[t], 1 −

Pr[¬ t]

γ (cid:19)

= min(cid:18)γ Pr[t],

γ − 1 + Pr[t]

γ

(cid:19) .

(3)

For example, suppose γ = 2, then PMP requires that Pr[t|S] ≤

min(cid:16)2 Pr[t], 1+Pr[t]

2

quation (3) as the formulation of the PMP condition.

(cid:17). In the rest of the paper, we often use E-

3.3 Satisfying PMP

In the rest of this section, we establish a relationship between
PMP’s bounding of Pr[t|S] in Equations (1) and (2) to a condition
bounding Pr[S| t]
Pr[S|¬ t] , which is more directly controlled by choosing
an appropriate mechanism. This is also key to establish the rela-
tionship of PMP with differential privacy. To start, note that by the
Bayes’ theorem, we have

Pr[t|S] =

Pr[S| t] Pr[t]

Pr[S| t] Pr[t] + Pr[S|¬t] Pr[¬ t]

.

(4)

892LEMMA 3.2. For any mechanism A, distribution D, and γ ≥
1, if for any entity t s.t. 0 < Pr[t] < 1 and any output event S, A
satisﬁes Pr[S|t] ≤ γ · Pr[S|¬ t], then A provides ({D}, γ)-PMP.

PROOF. For any t, if Pr[t] = 0 or Pr[t] = 1, then for any S,
Pr[t|S] = Pr[t], trivially satisfying Equation (3). If 0 < Pr[t] <
1, for any S, from Equation (4), we have

Pr[t|S] = Pr[t] ·

Pr[S|t]

Pr[t] Pr[S|t]+(1−Pr[t]) Pr[S|¬ t]

Note that the denumerator above is a weighted average of Pr[S|t]
and Pr[S|¬ t]. Since Pr[S|t] ≤ γ Pr[S|t] because γ ≥ 1, and
Pr[S|t] ≤ γ Pr[S|¬ t] from the given condition, it must be that

Pr[S|t] ≤ γ(Pr[t] Pr[S|t] + (1 − Pr[t]) Pr[S|¬ t]).

And thus Pr[t|S] ≤ γ Pr[t]. Similarly, we have

Pr[¬ t|S] = Pr[¬ t]

Pr[S|t] Pr[t]+Pr[S|¬ t](1−Pr[t]) ≥ Pr[¬ t] 1

Pr[S|¬ t]

γ

The last step above is because Pr[S|¬ t] ≥ Pr[S|t]
condition) and Pr[S|¬ t] ≥ Pr[S|¬ t]

γ
(from γ ≥ 1).

γ

(from given

Note that the above relationship is one-directional. Bounding
Pr[S|t]
Pr[S|¬ t] ≤ γ under a distribution D is sufﬁcient but not neces-
sary for satisfying PMP. Below we show that an equivalence exists
between the two when one aims at satisfying PMP for all distri-
butions in a family that has the property that we call downward
scalable, which essentially means that within the family, one could
scale down the probability Pr[t] to an arbitrarily small value. We
ﬁrst need to deﬁne what we call a t-scaled distribution.

DEFINITION 3.3

(t-SCALED DISTRIBUTION). Given a dis-
tribution D, we say that D′ is t-scaled from D if there exist two
constants c1 and c2 such that

∀T ⊆ U Pr
D′

[T ] = (cid:26) c1 PrD[T ] when t ∈ T ,

c2 PrD[T ] when t 6∈ T

The following property is the reason why we introduce the notion

of t-scaled distribution.

LEMMA 3.4. If D′ is t-scaled from D, then for any mechanism
A and any output event S, we have PrD′,A[S| t] = PrD,A[S| t]
and PrD′,A[S|¬ t] = PrD,A[S|¬ t].

PROOF. Let c1 and c2 be the scaling parameter. We have

PrD′,A[S| t]

= PT :t∈T PrD′ [T ] PrA[S| T ]
= PT :t∈T c1 PrD [T ] PrA[S| T ]
PrD′,A[S|¬ t] = PT :t6∈T PrD′ [T ] PrA[S| T ]
= PT :t6∈T c2 PrD [T ] PrA[S| T ]

PT :t∈T PrD′ [T ]
PT :t∈T c1 PrD [T ]
PT :t∈T PrD′ [T ]
PT :t6∈T c2 PrD [T ]

= PrD,A[S| t]

= PrD,A[S| t]

DEFINITION 3.5

(DOWNWARD SCALABLE FAMILIES). We
say that that a family D is downward scalable if and only if for any
distribution D ∈ D, any entity t such that 0 < PrD[t] < 1, and
any p > 0, D also contains a distribution D′ that is t-scaled from
D such that PrD′ [t] < p.

The following theorem is helpful for establishing equivalence of

other privacy notions with PMP.

THEOREM 3.6. Given a mechanism A, γ, and a downward s-
calable family D, A satisﬁes (D, γ)-PMP if and only if for any
D ∈ D, any entity t s.t. 0 < Pr[t] < 1, and any S, we have
Pr[S|t] ≤ γ · Pr[S|¬ t].

PROOF. The “if” direction is implied by Lemma 3.2. For the
“only if” direction, assume, for the sake of contradiction, that A
provides (D, γ)-PMP yet does not satisfy the condition, then there
exists a distribution D ∈ D and entity t such that 0 < PrD[t] < 1
and PrD,A[S|t] > γ · PrD,A[S|¬ t]. There are two cases.

Case one: PrD,A[S|t] > 0 and PrD,A[S|¬ t] = 0. Let p = 1
γ .
Because D is downward scalable, it must contain a D′ which is t-
γ . Because D′ is
scaled from D such that PrD′ [t] = p′ < p = 1
t-scaled from D, we have PrD′,A[S|¬ t] = PrD,A[S|¬ t] = 0,
and thus PrD′,A[t|S] = 1 (if ¬ t cannot result in S, observing S
means t must be true). Therefore, PrD′,A[t|S] = 1 = γp > γp′ =
γ PrD′ [t], which contradicts the fact that A satisﬁes (D, γ)-PMP.
Case two: PrD,A[S|t] = α PrD,A[S|¬t], where α > γ ≥ 1.
Because D is downward scalable, it must contains a D′ which is t-
scaled from D such that PrD′ [t] = p′ < p for an arbitrarily small
p, and that PrD′,A[S| t] = α PrD′,A[S|¬ t], and thus under D′ We
have:

Pr[t|S]
Pr[t] =
=
=

Pr[S| t]

p′ Pr[S| t]+(1−p′) Pr[S|¬t]

α Pr[S|¬ t]

p′α Pr[S|¬ t]+(1−p′) Pr[S|¬ t]

α

αp′+1−p′

Note that the above can be made arbitrarily close to α by forcing
p′ to be arbitrarily small, as α > γ, one could choose a small p′
(forcing p′ < α−γ
γ(α−1) sufﬁces) to make the above > γ, contradicts
the fact that A satisﬁes (D, γ)-PMP.

3.4 Infeasibility of PMP under Arbitrary Dis-

tributions

PMP is parameterized by D. Given two families of distributions
D1 and D2 such that D1 ⊆ D2, if A provides (D2, γ)-PMP, then
A also provides (D1, γ)-PMP. Therefore, to provide the maximum
level of privacy, it is desirable to provide (D, γ)-PMP for as large
a family D as possible. We use DU to denote the largest family of
distributions, namely the set of all distributions over 2U . Unfortu-
nately, achieving PMP under DU requires A to provide almost no
utility, as A must provide very similar output distributions on any
pair of dataset T1 and T2, even if when T1 = ∅ and T2 = U.

PROPOSITION 3.7. If an algorithm A provides (DU , γ)-PMP,
then for any datasets (T1, T2) such that T1 \ T2 6= ∅, and any
S ⊆ range(A), it must be

Pr[A(T1) ∈ S] ≤ γ Pr[A(T2) ∈ S]

(5)

PROOF. If A provides (DU , γ)-PMP, then it must provide γ-
PMP for the family that consists of all distributions of the form
{hT1 : pi, hT2 : 1 − pi}. Clearly, this family is downward scalable.
Let t be any tuple in T1 \ T2, from Theorem 3.6, A must satisfy
∀S, Pr[S| t] ≤ γ Pr[S|¬t]. Note that Pr[S| t] = Pr[S| T1] and
Pr[S|¬ t] = Pr[S| T2]; thus (5) holds.

Proposition 3.7 requires A’s output on any pair of datasets to be
close. For example, when T1 = U and T2 = ∅, and A is supposed
to answer a simple counting query. For any result that A(U) may
return with probability p, A(∅) must also return it with probability
at least p
γ . This destroys the utility of publishing A(T ), since one
could compute A(∅), and the distribution of A(T ) for any T would
be similar. Therefore, we must be content with choosing some sub-
families of DU for PMP.

3.5 Negative Membership Privacy

Note that satisfying PMP will upper-bound Pr[t|S]; however,
PMP does not lower-bound Pr[t|S]. That is, it is permitted that

893after observing an output, one concludes with high conﬁdence that
an entity is not in the input dataset. While we believe that for the
vast majority of cases, satisfying PMP is sufﬁcient for privacy pro-
tection, it is possible that in some unusual situations one desires
protection against inference of non-membership as well as infer-
ence of membership.

DEFINITION 3.8. [Negative Membership Privacy ((D, γ)-
NMP)]: We say that a mechanism A provides γ-negative member-
ship privacy under a distribution family D ((D, γ)-NMP), if and
only if for any S ⊆ range(A), any D ∈ D, and any tuple t ∈ U ,
we have both Pr[¬t|S] ≤ γ · Pr[¬t] and Pr[t|S] ≥ Pr[t]
γ , which
are equivalent to

Pr[¬ t|S] ≤ min(cid:18)γ Pr[¬t],

γ − 1 + Pr[¬t]

γ

(cid:19) ,

and to

Pr[t|S] ≥ max(cid:18)γ Pr[t] − γ + 1,

Pr[t]

γ (cid:19)

The properties of NMP are analogous to PMP.

(6)

(7)

DEFINITION 3.9

(UPWARD SCALABLE FAMILIES). We say
that that a family D is upward scalable if and only if for any dis-
tribution D ∈ D, any entity t such that 0 < PrD[t] < 1, and any
probability PrD[t] < p < 1, D also contains a distribution D′ that
is t-scaled from D such that PrD′ [t] > p.

THEOREM 3.10. For any mechanism A, γ, and any upward s-
calable family D, A satisﬁes (D, γ)-NMP if and only if for any
D ∈ D, any entity t, any entity t s.t. 0 < Pr[t] < 1, and any S, we
have Pr[S|¬t] ≤ γ · Pr[S|t].

The proof for Theorem 3.10 is analogous of the proof for Theo-

rem 3.6, and is omitted here.

3.6 Privacy Axioms

In [18], it is suggested that all privacy notions should satisfy
the two axioms: the Privacy Axiom of Choice and the Axiom of
Transformation Invariance. The following two theorems show that
(D, γ)-PMP satisfy both axioms.

THEOREM 3.11. Given two mechanisms A1 and A2 that both
satisfy (D, γ)-PMP, for any p ∈ [0, 1], let Ap be the mechanism
that outputs A1 with probability p and A2 with probability 1 − p,
then Ap satisﬁes (D, γ)-PMP.

Proof is Appendix A.1

There are two major ﬂavors of differential privacy, depending on
the condition under which two datasets are considered to be neigh-
bors. In [19], these were referred to as unbounded and bounded
differential privacy. In Unbounded Differential Privacy (UDP), T
and T ′ are neighbors if T can be obtained from T ′ by adding or
removing an entity. In Bounded Differential Privacy (BDP), T and
T ′ are neighbors if T can be obtained from T ′ by replacing one
entity in T ′ with another entity.

In UDP, closeness in output distributions is required only be-
tween two datasets that contain the same number of entities. There-
fore, one could output the accurate total number of entities in a
dataset without affecting BDP at all. Thus, there exist algorithm-
s that satisfy ǫ-BDP for ǫ = 0 and yet do not satisfy ǫ′-UDP for
any value of ǫ′. On the other hand, any algorithm that satisﬁes ǫ-
UDP must also satisfy 2ǫ-BDP, since replacing one entity can be
achieved by removing an entity and then adding another.

In the rest of this section, we show that these two differential pri-
vacy notions are instantiations of the membership privacy frame-
work, by choosing particular families of distributions. This rela-
tionship enables a clear understanding of the power and limitations
of differential privacy.

4.1 Unbounded Differential Privacy

We ﬁrst establish the relation between UDP and membership pri-
vacy. We decompose the UDP condition into the conjunction of the
following two conditions.

DEFINITION 4.2

(POSITIVE AND NEGATIVE UDP). A

mechanism A gives ǫ-positive unbounded differential privacy if
and only if for any dataset T , any entity t, and any S ⊆ Range(A),

Pr[A(T ∪ {t}) ∈ S] ≤ eǫ · Pr[A(T ) ∈ S].

(8)

A mechanism A gives ǫ-negative unbounded differential privacy if
and only if for any dataset T , any entity t, and any S ⊆ Range(A),

Pr[A(T ) ∈ S] ≤ eǫ · Pr[A(T ∪ {t}) ∈ S].

(9)

As it turns out, UDP corresponds to membership privacy under

the family of mutually independent distributions.

DISTRIBUTION FAMILY 4.3. DI : Mutually Independent (MI)
Distributions. We say that a distribution is mutually independent if
and only if it can be characterized by assigning a probability pt to
each tuple t such that the probability of any dataset T is given by

Pr[T ] = Yt∈T

pt Yt6∈T

(1 − pt)

THEOREM 3.12. Given A1 that satisﬁes (D, γ)-PMP, and any

DI includes all such mutually independent distributions over U .

algorithm A2, A(·) = A2(A1(·)) satisﬁes (D, γ)-PMP.

Proof is Appendix A.2. The same results hold for NMP; we omit
explicitly stating them here.

4. DIFFERENTIAL PRIVACY AS MEM-

BERSHIP PRIVACY

Informally, differential privacy requires that the output of a data
analysis mechanism is not overly affected by any single tuple in the
input dataset.

DEFINITION 4.1

(ǫ-DIFFERENTIAL PRIVACY [8, 10]).

A mechanism A gives ǫ-differential privacy if for any pair of
neighboring datasets T and T ′, and any S ⊆ Range(A),

Pr[A(T ) ∈ S] ≤ eǫ · Pr[A(T ′) ∈ S].

We note that DI includes distributions in which the probabilities
for different entities differ. In particular, DI includes distribution-
s where some entities have probability 1 and some other entities
have probability 0. We also observe that in a mutually independent
distribution, we have

Pr[T ∪ {t}] = Pr[T ]qt, where qt =

pt

1 − pt

(10)

We note that the deﬁnition for positive UDP essentially means
that Pr[S|t]
Pr[S|¬t] ≤ eǫ for all prior distributions such that each distri-
bution has just two possible datasets: D ∪ {t} and D. We use D2
I
to denote the family that includes all such distributions.

DISTRIBUTION FAMILY 4.4. D2
I : 1-out-2 MI Distributions.
I includes every MI distribution for which there are only two

D2

894In other words, there exists t
datasets with nonzero probability.
such that 0 < Pr[t] < 1, and furthermore, for any t′ 6= t, either
Pr[t′] = 1 or Pr[t′] = 0. In such a distribution, only two datasets
are possible: T ∪ {t} and T , where T = {t ∈ U | Pr[t] = 1}.

Clearly, D2

I is a subset of DI . Also, D2

I is both upward and down-
ward scalable. Combining the observation that positive UDP is e-
quivalent to Pr[S|t]
I with Theorem 3.6, it follows that
ǫ-positive UDP is equivalent to (D2

Pr[S|¬t] ≤ eǫ for D2

I , eǫ)-PMP.

The elegance and power of differential privacy lies in the fact
that while positive UDP directly achieves PMP only for D2
I , it turns
that this is sufﬁcient for achieving PMP for the larger family DI , as
shown by the following theorem.

THEOREM 4.5. A mechanism A satisﬁes ǫ-positive UDP if and

only if it provides (DI , eǫ)-PMP.

PROOF. The “if” direction is trivial since DI is a superset of D2
I ,

and ǫ-positive UDP is equivalent to (D2

I , eǫ)-PMP.

Pr[S|¬t] ≤ eǫ. Satisfying positive UDP gives us Pr[S|T ∪{t}]

For the “only if” direction. If A satisﬁes ǫ-positive UDP, then
given any distribution D ∈ DI , for any entity t, we want to show
that Pr[S|t]
Pr[S|T ] ≤
eǫ for any individual dataset T . Let Ti range over 2U −{t}, i.e., Ti
ranges over all datasets without t, then Ti ∪ {t} ranges over all
datasets that contain the tuple t. We have

Pr[S|t]
Pr[S|¬t] =

(cid:16)PTi

(cid:16)PTi

=
= PTi
≤ PTi
PTi

PTi

Pr[Ti∪{t}]

Pr[Ti∪{t}] Pr[S|Ti∪{t}](cid:17)/ PTi
(cid:16)PTi
Pr[Ti] Pr[S|Ti](cid:17)/ PTi
Pr[Ti]·qt Pr[S|Ti∪{t}](cid:17)/ PTi
(cid:16)PTi
Pr[Ti] Pr[S|Ti∪{t}]

Pr[Ti] Pr[S|Ti](cid:17)/ PTi

Pr[Ti]

Pr[Ti]

Pr[Ti]·qt

Pr[Ti] Pr[S|Ti]
Pr[Ti] Pr[S|Ti]eǫ
Pr[Ti] Pr[S|Ti] = eǫ

The second equality above uses the fact that D is mutually inde-
pendent and therefore Pr[Ti ∪ {t}] = Pr[Ti]qt (Equation 10). The
≤ above uses the positive UDP condition.

A natural question is whether ǫ-positive UDP would satisfy PM-
P for distributions that are not mutually independent. The follow-
ing observations hints that in a sense the family of all mutually-
independent distributions is the limit for which UDP guarantees
PMP. This corroborates the analysis that differential privacy’s pro-
tection is limited to the case where all entities are mutually inde-
pendent [19].

Consider the following distribution Dk

NI , in which all entities
except for t1, t2, · · · , tk are independent from any other sets of
entities but t1, t2, · · · , tk are totally correlated, i.e., either all of
them or none of them are included. It is easy to see that satisfy-
ing ǫ-positive UDP satisﬁes (Dk
NI , ekǫ)-PMP; however, there exist
mechanisms that satisfy ǫ-positive UDP, yet does not satisfy γ-PMP
for any γ < ekǫ.

Consider

the mechanism that outputs

the number of
random noise

from the Laplace

t1, t2, · · · , tk in the dataset by adding to it
drawn
Pr[Lap(β) = x] = 1
UDP. Now consider the event S = k, we have Pr[S|t1]
Therefore, Pr[S|t1]

Lap(cid:0) 1
ǫ(cid:1), where
This satisﬁes ǫ-positive
Pr[S|¬t1] = 1
ekǫ .
, which can get arbitrarily close

2β e−|x|/β.

distribution

1

Pr[t1] =

p+(1−p) 1
ekǫ

to ekǫ by choosing very small p. Thus, for any γ < ekǫ, we can
choose p so that Pr[S|t1]

Pr[t1] > γ, violating γ-PMP.

Similar to Theorem 4.5, we can show that ǫ-negative UDP is

equivalent to (DI , eǫ)-NMP. We omit the proof here.

4.2 Bounded Differential Privacy

We now consider how BDP relates to membership privacy. We
ﬁrst note that unlike the unbounded version of differential privacy,
BDP has a symmetry to it and cannot be decomposed into a positive
condition and a negative condition.

Because BDP only requires closeness in output distributions a-
mong datasets of the same size, we must limit to distributions where
all datasets that have non-zero probabilities have the same number
of entities. We introduce the following two families.

DISTRIBUTION FAMILY 4.6. DB : Bounded Mutually Inde-
pendent (BMI) Distributions. We say a distribution is a BMI distri-
bution if it is the conditional distribution of a mutually independent
distribution given that all datasets with non-zero probability has
the same size. DB includes all such BMI distributions.

DISTRIBUTION FAMILY 4.7. D2

B : 1-out-of-2 BMI Distribu-
tions. We say a BMI distribution is a 1-out-of-2 BMI distribution
if each distribution has exactly two datasets with nonzero proba-
bility. That is, all except for two entities have probability of either
1 or 0, and it is required that the size of the sampled dataset is
1 + |{t| Pr[t] = 1}|.

By Deﬁnition, ǫ-BDP is equivalent to satisfying Pr[S|t]
B. Thus, ǫ-BDP is equivalent to (D2

Pr[S|¬t] ≤ eǫ
for distributions in D2
B, eǫ)-
PMP (from Theorem 3.6). The following theorem shows that ǫ-
BDP achieves PMP for all distributions in DB.

THEOREM 4.8. A mechanism A satisﬁes ǫ-BDP if and only if it

provides (DB, eǫ)-PMP.

PROOF. The “if” direction follows straightforwardly from the

fact that ǫ-BDP is equivalent to D2

B and that D2

B ⊂ DB.

For the “only if” direction, similar to the proof of the unbounded
Pr[S|¬t] ≤ eǫ assum-

case, we would show that for any t, we have Pr[S|t]
ing that we have (DB, eǫ)-PMP.

Let T2 range over all datasets such that |T2| = k − 1 ∧ t 6∈ T2,
and T1 ranges over all datasets such that |T1| = k ∧ t 6∈ T1. We
have

Pr[S|t]
Pr[S|¬t] =

=

(cid:16)PT2

(cid:16)PT2

Pr[T2∪{t}] Pr[S|T2∪{t}](cid:17)/ Pr[t]

(cid:16)PT1

Pr[T1] Pr[S|T1](cid:17)/ Pr[¬ t]
Pr[T2∪{t}| t] Pr[S|T2∪{t}](cid:17)

(cid:16)PT1

Pr[T1|¬ t] Pr[S|T1](cid:17)

The main idea of the remaining proof is to construct a joint distribu-
tion Pk,k−1 deﬁned on all pairs of (T1, T2) such that T2 ⊆ T1 (so
that T2 ∪ {t} is a neighbor of T1) while the margin distribution of
Pk,k−1 on T1, T2 is Pr(T1|¬t) and Pr(T2 ∪ {t}|t). The existence
of such a joint distribution when we do not have size constraints
on T1, T2 is trivial as we can simply take T1 = T2 and this is al-
so essentially how we prove the unbounded case. The proof for
the existence of such a distribution for the bounded privacy case is
technically more challenging as we need to show the existence of
a such distribution when T1 is sampled from all sets of size k − 1
that does not contain t and T2 is sampled from all sets of size k that
does not contains t. Formally, we prove the following lemma:

LEMMA 4.9. For any k-bouneded mutually independent distri-
bution Pk deﬁned on n entities, there exists a distribution Pk,k−1
deﬁned on pair (T1, T2) satisfying that |T1| = k, |T2| = k −
1, T2 ⊂ T1 ⊆ [n], t /∈ T1. The margin probability of Pk,k−1
on T1 is Pr(T1|¬t) and on T2 is Pr(T2 ∪ {t}|t).

895Assuming the correctness of Lemma 4.9 and by the margin prop-

erty of Pk,k−1 , we know that

Pr[S|t]
Pr[S|¬t]

= P(T1,T2) PrPk,k−1 [T1, T2] · Pr[S|T2 ∪ {t}]

P(T1,T2) PrPk,k−1 [T1, T2] · Pr[S|T1]

(11)

Notice that T2 ∪ {t} and T1 are adjacent to each other accord-
ing to the deﬁnition of bounded differential privacy, and therefore
Pr[S|T2∪{t}]

Pr[S|T1] ≤ eǫ. This directly implies that (11) ≤ eǫ.

The complete proof of Lemma 4.9 is quite involved. We ﬁrst
show that the existence of the distribution Pk,k−1 is equivalent to
the existence of a perfect solution in certain network ﬂow problem;
then we use a generalization of Hall’s marriage theorem (see [14])
to show the existence of such a perfect solution in the network. The
full proof appears in Appendix A.3.

5. OTHER INSTANTIATIONS

The membership privacy framework enables one to design and
choose privacy notions suitable for particular situations by choos-
ing appropriate families of distributions. We have shown that
choosing DU , the family that includes all possible distributions,
results in a privacy notion that is likely too strong. We have al-
so shown that UDP corresponds to choosing DI , the family that
includes all mutually independent distributions, and that BDP cor-
responds to choosing DB
I , that family that includes all bounded mu-
tually independent distributions.

In this section, we explore membership privacy under three oth-
er families of distributions. The ﬁrst one is DN , the family that
includes the single uniform distribution over all possible subsets of
U. The second one corresponds to differential identiﬁability [20],
and the third one corresponds to differential privacy under sam-
pling [22].

5.1 PMP Under the Uniform Distribution

The ﬂexibility offered by PMP enables data publishers to have
more choices in trading off privacy versus utility. Here we show
that it is possible to satisfy PMP under a more restrictive family of
distributions while providing a much better utility than it is known
to be possible under differential privacy. We consider PMP under
the following family.

DISTRIBUTION FAMILY 5.1. DN : Non-informative Distribu-
tion DN includes a single distribution DN , the uniform distribution
such that each subset of U has the same probability

1
2|U| .

Note that DN , which contains a single distribution, should not
be confused with DU , which contains an inﬁnite number of distri-
butions. Under DN , we have Pr[t] = 1
2 for every entity t. PMP
under DN means that one assumes an uninformed adversary that
has no prior knowledge about the possible dataset, which may be
reasonable for some data publishing scenarios. We note that mech-
anisms that satisfy syntactic privacy notions such as k-anonymity
generally does not satisfy (DN , γ)-PMP, because from anonymized
dataset one can tell membership with high conﬁdence even assum-
ing the uninformed prior. We now show that it is possible to satisfy
DN while providing signiﬁcantly more utility than satisfying dif-
ferential privacy. As an example, we consider the universe U to
be one where each entity has a single numerical attribute, e.g., in-
come. And our goal is to publish the maximum income value in
T ⊂ U. Naturally the global range of the income attribute is very
large, while the max value of most datasets is likely to be much
smaller than the global max.

It is very difﬁcult to compute max while satisfying differential
privacy. The global sensitivity of the max function is very high. By
changing one entity, the max may change all the way from the low-
est possible value to the highest possible value. Therefore, whether
one applies the Laplacian mechanism or the exponential mechanis-
m, the amount of noise one has to add would dominate the result.

When one aims at satisfying (DN , γ)-PMP, however, it is pos-
sible to output the max of a set with very high accuracy. For sim-
plicity of explnanation, let us sort all entities in U based on their
income value, and use the index to refer to these entities. That is, 1
denote the entity with smallest income value, and n = |U| denotes
the entity with the largest income value, and we use ci to denote
the income of element i.

DEFINITION 5.2

(k-MAX MECHANISM). The k-Max mech-
anism, parameterized by an integer k ≥ 2, works as follows. Given
a dataset T , let j be the largest element in T , the mechanism gives
a value picked at uniform random from the following set:

C = (cid:26) {cj , cj+1, · · · , cj+k−1}

{cn−k+1, cn−k+2, · · · , cn} otherwise

when j + k − 1 ≤ n,

We note that this mechanism is highly accurate when k is not
large. It outputs the value of an element whose rank in the universe
is within distance k of the true max of the dataset. For example,
suppose that the elements in the universe have values that are the
ﬁrst 10000 prime numbers (up to 104729), on a dataset that has
four entities with values {2, 5, 113, 9851}, the mechanism 3-Max
outputs one of 9851, 9857, 9859 each with probability 1
3 , which is
remarkably close to the true max. The intuition that this satisﬁes
PMP is that no matter which of the three value is outputted, the
posterior probability that the entity with value 9851 (or any other
entity) is in the input is not signiﬁcantly higher than the prior prob-
ability of 1
12 according to the following proposition.

2 ; it is at most 7

PROPOSITION 5.3. The k-Max mechanism satisﬁes (DN , γ)-

PMP for γ = 2k −1
2k −2

.

PROOF. Because for any i, we have Pr[i] = 1

2 , we need to show

that for each i, j, Pr[i|cj ] ≤ min( γ

2 , γ−0.5

γ

).

Consider Pr[i|cj ], when cj is the output, we know that the
true max element in the input must be with distance k of j. Let
Mj denote the set of entities that are possible true max when
cj is output, and sj = |Mj| denote the number of elements in
Mj , and mj denote the max in Mj. When j ≤ n − k, then
Mj = {j − k + 1, · · · , j} includes exactly k elements. When
j > n − k, then all elements in {j − k + 1, · · · , j} are still in Mj ,
but Mj may include elements with rank > j. (For example, when
k = 2 and cn−1 is output, both n and n − 2 may be the true max.)
In any case, we always have sj ≥ k.

When cj is output, we know that at least one element in Mj
must appear, and no element with index > mj can appear. The
event under which that cj is output can be divided into 2sj − 1
equal-probability events, each corresponding to the selection of one
subset of Mj to be included in T (∅ is not allowed). For each
event, the probability that cj is output is exactly 1
k . Each i in Mj is
included in 2sj −1 of the events, and not included in 2sj −1 − 1 of
them.

We have Pr[i|cj ] = 0 when i > mj, and Pr[i|cj ] = 1
2 when i ≤
j − k. When j − k < i ≤ mj, we have Pr[i|cj ] = 2sj −1
because
2sj −1
when cj is output, there are 2sj − 1 equally likely possibilities, and
i appears in 2sj −1 of these possibilities.

896Therefore, we want 2

k, the above holds when 2(k−1)
gets γ ≥ max(cid:16) 2k

2k −1 , 2k −1

(sj −1)

2sj −1 ≤ min( γ
2k −1 ≤ min( γ
2k −2(cid:17) = 2k −1

2k −2

.

γ

2 , γ−0.5
2 , γ−0.5

γ

). Because sj ≥

), solving which

This means that one obtains (DN , 3
PMP for k = 3, and (DN , 15

14 )-PMP for k = 4.

2 )-PMP for k = 2, (DN , 7

6 )-

This example demonstrates that when one is willing to accept a
privacy notion weaker than differential privacy, then one can obtain
dramatic improvement in utility for some queries. k-Max is just a
preliminary example. Many other data analysis tasks may become
signiﬁcantly more accurate under this weaker privacy notion. We
also note that the accuracy of k-Max is also due in part to satisfying
only PMP and not NMP. k-Max does not satisfy (DN , γ)-NMP for
any γ, as observing output cj can reduce the posterior probability
of some large entities to 0.

We emphasize that we are not arguing that (DN , γ)-PMP is a
suitable privacy notion for all situations. Indeed even differential
privacy is not; as it is unsuitable when one cannot make the inde-
pendence assumption. We believe that a range of privacy notions
is needed in practice. We also stress than (DN , γ)-PMP is by no
means the only meaningful relaxation of differential privacy in the
membership privacy framework; and it is interesting future work
to explore other family of distributions which would induce useful
privacy notions.

5.2 Differential Identiﬁability

In [20], Lee and Clifton argued that there are no clear guide-
lines on how to set ǫ for ǫ-differential privacy, because ǫ limits how
much one individual can affect the resulting output, not how much
information is revealed about an individual; and this does not match
legal deﬁnitions of privacy, which require protection of individual-
ly identiﬁable data [1, 2]. From analysis of US HIPAA safe harbor
rule [2], it is concluded that the goal of the privacy policy is met
if one limits the estimate of the probability that an individual is in
the data to approximately 1.5%. Lee and Clifton thus propose the
following notion.

DEFINITION 5.4. (ρ-differential

identiﬁability (DI) [20]) A
mechanism A is said to satisfy ρ-DI if for any dataset T , any entity
t ∈ T , let T ′ = T \ {t}, for any output event S
Pr[t ∈ T|A(T) ∈ S, T ′] ≤ ρ,

where T is a random variable drawn from the following distribu-
tion: each dataset T ′ ∪ {t′}, where t′ ∈ U \ T ′ is equally likely.

The notion of ρ-DI aims at limiting the posterior probability that
the tuple t is in the input dataset after observing the output even-
t S. To be able to compute the posterior probability, one has to
assume some prior beliefs the adversary may have. The notion of
ρ-DI speciﬁes the allowed distributions to be from the following
families.

DISTRIBUTION FAMILY 5.5. Dm
C :

1-out-of-m equal-
probability choice BMI Distributions. Dm
C , where m ≥ 2 is
a positive integer, includes every distributions that is fully speciﬁed
by ﬁxing a set T ⊂ U such that |U \ T | = m. To sample a dataset
from the distribution, one includes T plus one additional entity
sampled uniformly at random from U \ T . In other words, in each
distribution only one entity is uncertain, and this entity is assumed
to be drawn uniform from m possible entities.

Dm

C is a sub-family of DB, the family of BMI distributions, as it
allows only BMI distributions that satisfy three further conditions.

First, the sampled dataset must have size 1 + |{t | pt = 1}|. Sec-
ond, there must exist m entities with probability between 0 and 1.
Third, all these m entities must have the same probability.

To determine whether a non-trivial mechanism A satisﬁes ρ-DI
or not, one has to also specify the value m; thus it is more appropri-
ate to say (ρ, m)-DI, which requires that for any distribution in Dm
C
and any output event S, we have Pr[t|S] ≤ ρ. Note that (ρ, m)-DI
makes sense only when ρ > 1
m .

THEOREM 5.6. (ρ, m)-DI is equivalent to (Dm

C , γ)-PMP for

γ = ρm for γ = max(cid:16)ρm, m−1

m(1−ρ)(cid:17).

PROOF. (ρ, m)-DI means that for any distribution in Dm

C and
m , the posterior is at most ρ, i.e.

any entity with prior probability 1
Pr[t|S] ≤ ρ.

(Dm

C , γ)-PMP means that for any distribution in Dm

C , Pr[t|S] ≤

min(cid:16)γ Pr[t], γ−1+Pr[t]
We only need to prove that ρ = min(cid:16) γ

(cid:17) = min(cid:16) γ

mγ (cid:17).
m , 1 − m−1
m , 1 − m−1

γ

two cases.

Case one: we have ρm ≤ m−1

and min(cid:16) γ

ρ ≤ m−1

m , 1 − m−1

mγ (cid:17) = min(cid:16) m−1

m2(1−ρ) from the case condition.
Case two: we have ρm > m−1

m , 1 − m−1

mγ (cid:17) = min(cid:16)ρ, 1 − (m−1)/m
min(cid:16) γ
from the case condition, we have 1 − (m−1)/m

mγ (cid:17). There are
m(1−ρ) , and thus γ = m−1
m(1−ρ) ,
m2(1−ρ) , ρ(cid:17) = ρ because
m(1−ρ) , and thus γ = ρm, and
ρm (cid:17) = ρ because

> 1 − (m−1)/m

m−1

=

ρm

m(1−ρ)

ρ.

The following theorem show an interesting relationship between

DI and BDP.

THEOREM 5.7. When ρ > 1

for ǫ = ln(cid:16) ρ

1−ρ(cid:17). However, when m > 2 and ρ > 1

2 , (ρ, 2)-DI is equivalent to ǫ-BDP
m−1 , it is

possible to satisfy (ρ, m)-DI while violating ǫ-BDP for any ǫ.

Pr[S|T1]

PROOF. When m = 2, (ρ, 2)-DI requires that on all dataset
pairs T1 and T2 that have the same number of entities and differ
in only one entity, when T1 and T2 are equally likely, we have
Pr[S|T1]+Pr[S|T2] ≤ ρ; this holds
Pr[T1|S] ≤ ρ, i.e., Pr[T1|S] =
if and only if Pr[S|T1] ≤ ρ
1−ρ Pr[S|T2]; hence the equivalence
with ln(cid:16) ρ

1−ρ(cid:17)-BDP.

m−1 , consider the following case, the
universe consists of m entities t1 · · · tm, the mechanism A, on
input dataset T outputs a value chosen at uniform random from
{1, · · · , m}, except for the case T = {t1}, when it outputs a value
chosen at uniform random from {2, · · · , m}.

When m > 2 and ρ > 1

This trivially satisﬁes the DI condition for any input dataset con-
taining more than one entities, since the output is always uniform.
When one consider datasets containing exactly one entity, it sat-
isﬁes the DI condition too. First consider the case 1 is the out-
put. We have Pr[t1|1] = 0 < ρ, and when i 6= 1, we have
Pr[ti|1] = 1
m−1 < ρ, because each of {t2}, · · · , {tm} is equally
likely. Then consider the case when j 6= 1 as the output, we have
Pr[t1|j] =
1/(m−1)+(m−1)/m =
m−1 <
ρ and when i 6= 1, Pr[ti|j] < 1

m2−m+1 < m

m2−m = 1

1/(m−1)

m

m−1 < ρ.

However, this mechanism violates ǫ-BDP for any ǫ because

Pr[1|{t2}] = 1

m > eǫ Pr[1|{t1}] = 0 for any ǫ.

8975.3 Differential Privacy Under Sampling

In [22], Li et al. proposed a relaxation to differential privacy that
exploits the adversary’s uncertainty about the dataset. While the o-
riginal deﬁnition of differential privacy assumes that the adversary
has precise knowledge of all the tuples in the dataset, Li et al. argue
that this, in fact, might be too strong for some data publishing sce-
narios. Instead, it is reasonable to relax the assumption to that the
adversary knows all attributes of a tuple t (but not whether t is in
the dataset), and in addition statistical information about the rest of
the dataset D. The privacy notion should prevent such an adversary
from substantially distinguishing between D and D ∪ {t} based on
the output. This intuition is formalized using the following deﬁni-
tion.

DEFINITION 5.8

((β, ǫ)-DPS [22]). An algorithm A gives
(β, ǫ)-DPS if and only if the algorithm Aβ gives ǫ-UDP, where Aβ
denotes the algorithm to ﬁrst sample each tuple with probability β,
and then apply A to the sampled dataset.

Similar to our analysis of differential privacy, we focus on posi-
tive DPS. It turns out that (β, ǫ)-DPS is equivalent to PMP for the
following family.

DISTRIBUTION FAMILY 5.9. Dβ

F : Fixed-probability Mutual-
ly Independent Distributions. This is a sub-family of DI . It in-
cludes all mutually independent distributions such that ∀t Pr[t] ∈
{0, β}. In other words, all entities that may appear have the same
probability.

THEOREM 5.10. A randomized mechanism satisﬁes (β, ǫ)-
F , γ)-PMP for γ =

Positive DPS if and only if it satisﬁes (Dβ
βeǫ (cid:17).
max(cid:16)eǫ, eǫ−1+β

Proof is given in Appendix A.4.

6. RELATED WORK

A number of syntactic privacy deﬁnitions have proposed over
the years; the most prominent ones include k-anonymity [30, 29],
l-diversity [24] and t-closeness [21]. Their weaknesses have been
often identiﬁed. See, e.g., Dwork [9] for a survey.
Instead, D-
work argues that we should consider privacy protection problems
in a more rigorous and formal way. This is the motivation of the
research of differential privacy. The notion of differential privacy
was developed in a series of works [6, 13, 4, 11, 8], and several
methods of satisfying have been developed [8, 11, 26, 28].

In [5], Cormode argued that differential privacy does not prevent
inferential disclosure. It is shown that, from differentially private
output, it is possible to infer potentially sensitive information about
an individual with non-trivial accuracy. In Section 2.3, we argued
that it may be inappropriate to use prevention of attribute disclosure
as the privacy objective. In this paper, we formalize membership
disclosure, and show that it closely matches the social and legal
deﬁnitions of privacy. Lee and Clifton [20] proposed the notion
of ρ-differential identiﬁability, which captures membership disclo-
sure under speciﬁc adversarial background knowledge. This notion
is a special case of membership privacy, and we analyze it in Sec-
tion 5.2. Li et al. [22] relax an adversary’s background knowledge
by sampling the dataset prior to applying the privacy mechanism.
We analyze differential privacy under sampling and its relation to
our privacy notion in Section 5.3.

In [19], Kifer and Machanavajjhala argued that it is not possible
to provide privacy and utility without making assumptions about

how the data are generated. Our modeling of the attacker’s back-
ground knowledge in terms of a family of distributions complies
with their ﬁnding. Kifer et al. also questioned whether the dif-
ferential privacy guarantees when data points are correlated. Our
analysis suggests that differential privacy only guarantees member-
ship protection under distributions where data points are mutually
independent.

Kifer and Lin [18] proposed the privacy axioms of choice and
the axiom of transformation, which we show that membership pri-
vacy satisfy. They also introduce a generalization of differential
privacy, called generic differential privacy, which follows the syn-
tactic structure of differential privacy, but allows more ﬂexible
deﬁnition of neighboring datasets and the condition between that
Pr[S|D] and Pr[S|D′] should satisfy. A similar approach is tak-
en by Machanavajjhala et al. [23], which introduced a framework
called ǫ-privacy, which limits the impact the inclusion of one entity
can have on the adversary’s belief about the individual’s attribute
value. Gehrke et al. [16] introduced zero-knowledge based deﬁni-
tion of privacy, which deﬁnes a mechanism to be private if its out-
put can be simulated by a simulator with access to some aggregate
function of the data, but without direct access to the data. One need-
s to choose appropriate aggregate function to instantiate the privacy
notion. Gehrke et al. [15] then introduced crowd-blending privacy,
which combines safe k-anonymization and differential privacy. We
also generalize differential privacy; however, our approach differs
in that we formalize membership privacy, which is justiﬁed from
analysis of privacy incidents, and in that our notion is parameter-
ized by families of dataset distributions.

In an attempt to make differential privacy more amenable to
more sensitive queries, several relaxations have been developed, in-
cluding (ǫ, δ)-differential privacy [6, 13, 4, 11]. Machanavajjhala
et al. [25] introduced a variant of (ǫ, δ)-differential privacy called
(ǫ, δ)-probabilistic differential privacy. Roughly, all these relax-
ations use δ to bound the probability that ǫ-DP is violated. Our pri-
vacy framework does not yet deal with this relaxation of allowing
a δ of error probability. It is interesting future work to investigate
how they can be accommodated by extending membership privacy
to allow such an error probability.

7. CONCLUSIONS

Through analysis of the recent privacy incidents, we have con-
cluded that what society often views as a privacy breach is the abil-
ity of an adversary to either re-identify or assert the membership of
any individual in a supposedly “anonymized” dataset. Thus we in-
troduce the membership privacy framework. We have demonstrat-
ed that differential privacy and several other related privacy notions
are instantiations of the framework.

Identifying the family under which a privacy notion guarantees
membership privacy provides deeper understanding of the power
and limitation of the privacy notion. In particular, they identify the
assumptions that are made by the privacy notion. As all practical
privacy notion requires some assumptions on the allowed distribu-
tions, it makes sense to analyze whether the assumption made in a
notion is appropriate for a given setting, and choose a privacy that
is neither too strong nor too weak, in order to maximize utility. Our
framework enables the development of new privacy notions. We
believe that the membership privacy framework opens doors for fu-
ture research at developing new privacy notions, understanding and
comparing privacy notions, and designing mechanisms for satisfy-
ing different privacy notions.

This paper is based upon work support-
Acknowledgement.
ed by the United States National Science Foundation under Grant
No. 1116991, and by the United States AFOSR.

8988. REFERENCES

[1] Directive 95/46/ec of the European Parliament and of the

council of 24 october 1995 on the protection of individuals
with regard to the processing of personal data and on the free
movement of such data. Ofﬁcial Journal L,
281(23/11):0031–0050, 1995.

[2] Standard for privacy of individually identiﬁable health

information. Federal Register, 67(157):53 181–53 273, Aug
2002.
http://www.hhs.gov/ocr/privacy/hipaa/
administrative/privacyrule/index.html.
[3] M. Barbaro and J. Tom Zeller. A face is exposed for aol

searcher no. 4417749. New York Times, Aug 2006.

[4] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical

privacy: the SuLQ framework. In PODS, pages 128–138,
2005.

[5] G. Cormode. Personal privacy vs population privacy:

learning to attack anonymization. In KDD, pages
1253–1261, 2011.

[6] I. Dinur and K. Nissim. Revealing information while

preserving privacy. In PODS.

[7] C. Dwork. Differential privacy. In in ICALP, pages 1–12.

Springer, 2006.

[8] C. Dwork. Differential privacy. In ICALP, pages 1–12, 2006.
[9] C. Dwork. An ad omnia approach to deﬁning and achieving

private data analysis. In PinKDD ’07, pages 1–13, Berlin,
Heidelberg, 2008. Springer-Verlag.

[10] C. Dwork, F. Mcsherry, K. Nissim, and A. Smith. Calibrating
noise to sensitivity in private data analysis. In In Proceedings
of the 3rd Theory of Cryptography Conference, pages
265–284. Springer, 2006.

[11] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data analysis. In
TCC, pages 265–284, 2006.

[12] C. Dwork and M. Naor. On the difﬁculties of disclosure

prevention in statistical databases or the case for differential
privacy. Journal of Privacy and Conﬁdentiality, 2(1):8, 2010.

[13] C. Dwork and K. Nissim. Privacy-preserving datamining on
vertically partitioned databases. In CRYPTO, pages 528–544.
Springer, 2004.

[14] D. Gale. A theorem on ﬂows in networks. Paciﬁc Journal of

Mathematics, 7(2):1073–1082, 1957.

[15] J. Gehrke, M. Hay, E. Lui, and R. Pass. Crowd-blending

privacy. In CRYPTO, pages 479–496, 2012.

[16] J. Gehrke, E. Lui, and R. Pass. Towards privacy for social

networks: a zero-knowledge based deﬁnition of privacy. In
TCC, pages 432–449, Berlin, Heidelberg, 2011.
Springer-Verlag.

[17] N. Homer, S. Szelinger, M. Redman, D. Duggan, W. Tembe,
J. Muehling, J. V. Pearson, D. A. Stephan, S. F. Nelson, and
D. W. Craig. Resolving individuals contributing trace
amounts of DNA to highly complex mixtures using
high-density SNP genotyping microarrays. PLoS Genet,
4(8):e1000167+, 08 2008.

[18] D. Kifer and B.-R. Lin. Towards an axiomatization of

statistical privacy and utility. In PODS, PODS ’10, pages
147–158, New York, NY, USA, 2010. ACM.

[19] D. Kifer and A. Machanavajjhala. No free lunch in data

privacy. In SIGMOD, pages 193–204, 2011.

[20] J. Lee and C. Clifton. Differential identiﬁability. In KDD,

pages 1041–1049, 2012.

[21] N. Li, T. Li, and S. Venkatasubramanian. t-closeness:

Privacy beyond k-anonymity and l-diversity. In ICDE, pages
106–115, 2007.

[22] N. Li, W. Qardaji, and D. Su. On sampling, anonymization,

and differential privacy or, k-anonymization meets
differential privacy. In ASIACCS, pages 32–33, 2012.

[23] A. Machanavajjhala, J. Gehrke, and M. Götz. Data

publishing against realistic adversaries. Proc. VLDB Endow.,
2(1):790–801, Aug. 2009.

[24] A. Machanavajjhala, J. Gehrke, D. Kifer, and

M. Venkitasubramaniam. ℓ-diversity: Privacy beyond
k-anonymity. In ICDE, page 24, 2006.

[25] A. Machanavajjhala, D. Kifer, J. M. Abowd, J. Gehrke, and
L. Vilhuber. Privacy: Theory meets practice on the map. In
ICDE, pages 277–286, 2008.

[26] F. McSherry and K. Talwar. Mechanism design via
differential privacy. In FOCS, pages 94–103, 2007.

[27] A. Narayanan and V. Shmatikov. Robust de-anonymization

of large sparse datasets. In S&P, pages 111–125, 2008.

[28] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth

sensitivity and sampling in private data analysis. In STOC,
pages 75–84, 2007.

[29] P. Samarati. Protecting respondents’ identities in microdata

release. IEEE Trans. on Knowl. and Data Eng.,
13:1010–1027, November 2001.

[30] L. Sweeney. k-anonymity: A model for protecting privacy.

Int. J. Uncertain. Fuzziness Knowl.-Based Syst.,
10(5):557–570, 2002.

APPENDIX
A. PROOFS

A.1 Proof of Theorem 3.11

PROOF. For any S ⊆ Range(A),

Pr[t|A ∈ S] = p Pr[t | A1(T ) ∈ S]+(1−p) Pr[t|A2(T ) ∈ S]

≤ p · γ Pr[t] + (1 − p)γ Pr[t] = γ Pr[t]

By a similar argument, we know that

Pr[¬t|A ∈ S] ≥

Pr[¬t]

γ

and therefore A also satisﬁes (D, γ)-PMP.
It is easy to see that above proof can be generalized to the case
that the mechanism A is the combination of more than two mech-
anisms; i.e., we have a set of k mechanisms and A is set to be
Ai with some probability pi for any p1, p2, . . . , pk ≥ 0 such that
i=1 pi = 1. In fact, this can be generalized to the case where the

Pk

number of methods is inﬁnite.

A.2 Proof of Theorem 3.12

PROOF. Let us ﬁrst prove for the case that A2 is a determin-
istic mechanism. For any S ⊆ Range(A2), we deﬁne S′ =
{s|A2(s) ∈ S, s ∈ Range(A1)}.

We know then

Pr[t|A2(A1(T )) ∈ S|] = Pr[t|A1(T )) ∈ S′] ≤ γ Pr[t]

and
Pr[¬t|A2(A1(T )) ∈ S|] = Pr[¬t|A1(T )) ∈ S′] ≥

Pr[¬t]

γ

As for a randomized mechanism A2, it can be viewed as com-
binations of a set of deterministic mechanisms. Suppose it is a
2 with probability pi to choose Ai
combination of A1
2.
We know then A2(A1(·)) is set to be Ai
2(A1(·)) with probability

2 . . . , Ak

2, A2

899pi. Clearly, each Ai
2 is a deter-
ministic mechanism. Also by the proof of Theorem 3.11, we know
that Ai

2(A1(·)) satisﬁes (D, γ)-PMP as Ai

2(A1(·)) also satisﬁes (D, γ)-PMP.

A.3 Proof of Lemma 4.9

PROOF OF LEMMA 4.9. Suppose we have a mutually indepen-
dent distribution with probability p1, p2, . . . , pn on each entity
t ∈ [n]. Let Pk be its k-bounded distribution. For any particu-
lar T of size k, it is easy to see that

Pr
Pk

[T ] ∝ Yt∈T

pt Yt6∈T

(1 − pt).

If we divide Qt∈T ptQt6∈T (1 − pt) by Qt∈[n](1 − pt) (which is
a constant independent of T ), we get that PrPk [T ] ∝ Qt∈T
Let us denote Qt∈T

as p(T ). We know then

1−pt

1−pt

pt

pt

.

[T ] =

Pr
Pk

p(T )

PT ⊆[n],|T |=k p(T )

.

In addition, under distribution Pk, for any t ∈ [n] and T1 that

contains t,

Pr[T1|t] =

and for any t /∈ T2,

p(T1)

Pt∈T,|T |=k p(T )

=

p(T1\{t})

Pt /∈T,|T |=k−1 p(T )

(12)

Pr[T2|¬t] =

.

(13)

p(T2)

Pt /∈T,|T |=k p(T )

The existence of Pk,k−1 is equivalent to whether there exists a
good solution for the following network ﬂow problem on a bipar-
tite graph G(U, V, E). Here each vertex in U is corresponding to
some set T2 such that |T2| = k and t /∈ T2 and each vertex in V
is corresponding to some set |T1| = k − 1 and t /∈ T1. There is
an edge between (T1, T2) if T2 ⊂ T1. There is Pr[T2|¬t] amount
of goods on each node T2 and the capacity for each node T1 in V
is Pr[T1 ∪ {t}|t]. We can only ship goods along the edges and the
question is whether there exists a “perfect ﬂow” so that we can ship
all the goods from the U side to the V side without violating the
capacity constraints on the V side. This problem is equivalent to
the existence of Pk,k−1 as essentially Pr(T1, T2) is corresponding
to the amount of goods we want to ship from T2 to T1.

A necessary condition for the existence of the perfect ﬂow is that:
for any W ⊆ U and let the neighbor of W in G to be N (W ), we
have that

XT2∈W

Pr[T2|¬t] ≥ XT1∈N(W )

Pr[T1 ∪ {t}|t];

(14)

By [14] (which is a generalization of Hall’s marriage theorem),
(14) is also a sufﬁcient condition for the existence of the perfect
ﬂow. To prove (14), if we denote qi = pi
and use (12),(13), it
1−pi
is equivalent to show that

qi

PT2∈W Qi∈T2
P|T |=k,t /∈T Qi∈T qi

or equivalently

≤ PT1∈N(W )Qi∈T1

P|T |=k−1,t /∈T Qi∈T qi

qi

(cid:16)PT2∈W Qi∈T2

qi(cid:17) ·(cid:16)P|T |=k−1,t /∈T Qi∈T qi(cid:17)

≤ (cid:16)PT1∈N(W ) Qi∈T1

qi(cid:17) ·(cid:16)P|T |=k,t /∈T Qi∈T qi(cid:17)

If we expand the left hand side and also the right hand side of
the above expression, we would get the sum of many terms of the

form q(S) = QS qi for S being a multiset such that t /∈ S and

|S| = 2k − 1. We know that the number of times q(S) appear
on the left hand side is equal to the number of different T2 ∈ W
so that T2 ⊆ S and S\T2 has distinct elements. Let us call such
T2 “good” for S. The number of times q(S) appear in the right
is equal to the number of different T1 in N (W ) so that S\T1 has
distinct elements. Let us also call such T1 “good” for S. Suppose
U2 contains all the T2 good for S in W and U1 contains all the T1
good for S in N (W ). It remains to prove the following claim.

LEMMA A.1. For any W and S (which deﬁnes U1, U2), we

have that |U1| ≥ |U2|.

We would prove this by induction on k (or equivalently the size
of |S| = 2k − 1). It is easy to check when k=1, above claim holds.
Suppose that |U2| ≤ |U1| holds for any k < k0. When k = k0, ﬁrst
let us consider the case that the elements in S are all distinct. We
construct a graph G(U1, U2, E) where the edge is added between
any T1 ∈ U1 and T2 ∈ U2 such that T1 ⊂ T2.

For any T2, we know that T2\{e} for any e ∈ T2 must be in U1.
Therefore, any T2 in U2 has exactly k neighbours. On the other
hand, for any T1 ∈ U1, any its neighbor in U2 must be of the form
T1 ∪ {e} for some e in S\T1 and there are at most |S| − |T1| = k
of such e. If we count the number of edges from U2 side, there are
k · U2 such edges. And if we count the number of edges from U1
side, there are at most k · |U1| edges. Therefore, |U1| ≥ |U2|.

When some element e in S are not distinct, it must be the case
that it can appear in both T1 and S\T1 (and also T2 and S\T2).
This also means that e appear exactly twice in S. Then essentially
we reduce it to the case when W ′ = {T \{e}|e ∈ T ∈ W }and
S′ = S\{e} and k′ = k0 −1 and therefore holds by induction.

A.4 Proof of Theorem 5.10

PROOF. Let Λβ(T ) denote the process of sampling tuples
Then, by deﬁnition, A satis-
from T with probability β.
ﬁes (β, ǫ)-Positive DPS means for each T , t, and S, we have
Pr[A(Λβ (T ∪{t}))∈S]

Pr[A(Λβ (T ))∈S] ≤ eǫ, which is equivalent to

Pr[A(Λβ (T ∪{t}))∈S]

Pr[A(Λβ (T ))∈S]

= (1−β) Pr[A(Λβ (T ))∈S]+β Pr[A(Λβ (T )∪{t})∈S]
Pr[A(Λβ (T ))∈S]
= 1 − β + β Pr[A(Λβ (T )∪{t})∈S]
Pr[A(Λβ (T ))∈S] ≤ eǫ

Hence, (β, ǫ)-Positive DPS is equivalent to ∀T ∀t∀S

PrD [S|t]

PrD [S|¬t] = Pr[A(Λβ (T )∪{t})∈S]

Pr[A(Λβ (T ))∈S] ≤ eǫ−1+β

β

where D such that all entities in T ∪ {t} have probability β, and
all other entities have probability 0; clearly D ∈ Dβ
F . In fact, there
is a one-to-one correspondence between a pair of T and t in DPS
deﬁnition, and such a distribution in Dβ
F and a tuple t such that
Pr[t] = β.

By deﬁnition, that A satisﬁes (Dβ

F , and S, we have Pr[t|S] ≤ min(γβ, 1 − 1−β

F , γ)-PMP means that for each
γ ). As

Pr[S|t]β

Pr[S|t]β+Pr[S|¬t](1−β) =

1+ 1−β

β

1
Pr[S|¬t]
Pr[S|t]

.

We only need to show that by selecting γ = max(cid:16)eǫ, eǫ−1+β
βeǫ (cid:17),
the two conditions PrD [S|t]

PrD [S|¬t] ≤ eǫ−1+β

and

≤

β

1+ 1−β

β

1
Pr[S|¬t]
Pr[S|t]

γ ) become equivalent. Note that PrD [S|t]

PrD [S|¬t] ≤

min(γβ, 1 − 1−β
eǫ−1+β

if and only if

β

1+ 1−β

β

1
Pr[S|¬t]
Pr[S|t]

≤ eǫ−1+β

.

eǫ

Case one, when eǫ ≥ eǫ−1+β

βeǫ
γ = eǫ and min(γβ, 1 − 1−β
Case two, when eǫ < eǫ−1+β

βeǫ

, is similar.

, we have βeǫeǫ ≥ eǫ − 1 + β,
) = eǫ−1+β
.

γ ) = min(βeǫ, eǫ−1+β

eǫ

eǫ

D ∈ Dβ
Pr[t|S] =

900