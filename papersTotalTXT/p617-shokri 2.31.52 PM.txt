Protecting Location Privacy:

Optimal Strategy against Localization Attacks

Reza Shokri†, George Theodorakopoulos‡, Carmela Troncoso∗,

Jean-Pierre Hubaux†, and Jean-Yves Le Boudec†

†LCA, EPFL, Lausanne, Switzerland,

∗ESAT/COSIC, K.U.Leuven, Leuven-Heverlee, Belgium,

‡School of Computer Science and Informatics, Cardiff University, Cardiff, UK

†ﬁrstname.lastname@epﬂ.ch, ‡g.theodorakopoulos@cs.cardiff.ac.uk,

∗carmela.troncoso@esat.kuleuven.be

ABSTRACT
The mainstream approach to protecting the location-privacy
of mobile users in location-based services (LBSs) is to alter
the users’ actual locations in order to reduce the location
information exposed to the service provider. The location
obfuscation algorithm behind an eﬀective location-privacy
preserving mechanism (LPPM) must consider three funda-
mental elements: the privacy requirements of the users, the
adversary’s knowledge and capabilities, and the maximal
tolerated service quality degradation stemming from the ob-
fuscation of true locations. We propose the ﬁrst methodol-
ogy, to the best of our knowledge, that enables a designer to
ﬁnd the optimal LPPM for a LBS given each user’s service
quality constraints against an adversary implementing the
optimal inference algorithm. Such LPPM is the one that
maximizes the expected distortion (error) that the optimal
adversary incurs in reconstructing the actual location of a
user, while fulﬁlling the user’s service-quality requirement.
We formalize the mutual optimization of user-adversary ob-
jectives (location privacy vs. correctness of localization) by
using the framework of Stackelberg Bayesian games. We de-
velop two linear programs that output the best LPPM strat-
egy and its corresponding optimal inference attack. We show
that the optimal LPPM is superior to a straightforward ob-
fuscation method, and that the optimal localization attack
performs better compared to a Bayesian inference attack.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General—
Security and protection; K.4.1 [Computers and Society]:
Public Policy Issues—Privacy

Keywords
Location Privacy, Location Inference Attacks, Service Qual-
ity, Location-based Services, Stackelberg Bayesian Games

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$10.00.

1.

INTRODUCTION

The widespread use of smart mobile devices with continu-
ous connection to the Internet has fostered the development
of a variety of successful location-based services (LBSs).
Even though LBSs can be very useful, these beneﬁts come
at a cost of users’ privacy. The whereabouts users’ disclose
to the service provider expose aspects of their private life
that is not apparent at ﬁrst, but can be inferred from the
revealed location data [8, 11, 18].

A large body of research has focused on developing location-

privacy protection mechanisms (LPPMs) that allow users to
make use of LBSs while limiting the amount of disclosed sen-
sitive information [1, 3, 9, 10, 14, 16, 22]. These protection
mechanisms are based on hiding or perturbing the real lo-
cations of a user, or even sending fake locations to the LBS,
in order to increase the uncertainty of the adversary about
a user’s true whereabouts. However, the evaluation of these
designs usually disregards that the adversary might have
some knowledge about the users’ access pattern to the LBS
and also about the algorithm implemented by the LPPM.
Such information allows the attacker to reduce his uncer-
tainty on the user’s true location [25]. Hence, prior evalu-
ations overestimate the location privacy oﬀered by a given
protection system.

In this paper, we focus on a broad range of LBSs and lo-
cation sharing services in which users reveal their location in
a sporadic manner, e.g., location check-in, location-tagging,
or applications for ﬁnding nearby points-of-interests, local
events, or nearby friends. We consider an adversary inter-
ested in uncovering the location of a user at the time when
she sends the LBS query (i.e., an adversary performing local-
ization attacks [25, 26]). We focus on user-centric LPPMs
in which decision taken to protect privacy (e.g., hiding, per-
turbing, or faking the location) is made locally by the user.
Hence, these LPPMs can be easily integrated in the mobile
device that she uses to access LBS. We note, however, that
the principles behind our protection mechanism design are
applicable to LBSs where users reveal their location continu-
ously (rather than sporadically), and where the adversary’s
aim is to track users continuously over space and time [26].
We propose an analytical framework that allows system
designers to ﬁnd the optimal LPPM against a strategic ad-
versary who, knowing each user’s LBS access pattern and
the underlying obfuscation algorithm, employs the optimal
attack to localize them. The challenge is to design such opti-

617mal protection mechanism when the inference attack, depen-
dent on the mechanism being designed, is unknown to the
designer. As opposed to making any assumption about the
adversary’s inference algorithm (i.e., limiting his power), we
co-infer the optimal attack while ﬁnding the defense mech-
anism. Additionally, our methodology constrains the search
space to LPPMs that obfuscate locations in such a way that
the quality of the LBS response is not degraded below a
threshold imposed by the user, hence guaranteeing required
service quality for the user. We assume that the adversary
is also aware of this user-speciﬁed service quality constraint.
We formalize the problem of ﬁnding the optimal LPPM
anticipating the optimal inference attack as an instance of a
zero-sum Bayesian Stackelberg game. In this game, a leader
and a follower interact strategically with each one’s gain be-
ing the loss of the other. The leader decides on her strategy
knowing that it will be observed by the follower, who will
optimize his choice based on this observation. In our sce-
nario the user is the leader and the adversary is the follower.
Then, this game precisely models that the adversary knows
the user’s choice of protection mechanism and will use that
knowledge to improve his attack’s eﬀectiveness. We extend
the classic formulation of a Stackelberg game with an extra
constraint to ensure that the service quality is satisfactory
for the user. This enables us to ﬁnd the optimal point in
the tradeoﬀ curve between privacy and service quality that
satisﬁes both user privacy and service quality requirements.
We build on the probabilistic model proposed by Shokri
et al. [24, 25, 26] to ﬁnd the best localization attack against
a given LPPM and to measure the users’ location privacy.
This privacy measure is in turn used in the Stackelberg game
to ﬁnd the optimal LPPM for each user, i.e., the one that
oﬀers the best location privacy subject to the user’s service
quality requirements. Ours is, to the best of our knowl-
edge, the ﬁrst analytical framework that allows engineers to
methodologically integrate adversarial knowledge in the de-
sign of optimal user-centric privacy protection mechanisms.
We evaluate the LPPMs generated by our method us-
ing real location traces. We show how, for a given user’s
LBS access pattern and service-quality threshold, our game-
theoretic approach enables us to simultaneously ﬁnd the op-
timal LPPM and the optimal attack against it. We conﬁrm
that there is a trade-oﬀ between the maximum achievable
privacy and the service quality but once a certain privacy
level is reached, loosening the quality requirements does not
necessarily result in a privacy gain. We also ﬁnd that the
location-privacy gain of using the optimal LPPM, with re-
spect to a suboptimal one, is larger when the quality con-
straint is tighter (compared to the case where users’ quality
requirements allow the LPPM to signiﬁcantly perturb loca-
tions before sending them to the LBS).

The rest of the paper is organized as follows. We present
the elements of our framework and describe the objectives
of the user and adversary in the next section. We formalize
the problem of ﬁnding an LPPM that oﬀers optimal location
privacy in terms of a Bayesian Stackelberg game in Section 3,
and develop the best solution for both user and adversary in
Section 4. We evaluate our method in Section 5 against real
location traces. Section 6 revisits previous work on location
privacy protection mechanisms, as well as on game theory
applied to security-related scenarios. Finally, we conclude
the paper in Section 7.

2. THE PROBLEM STATEMENT

In this section, we explain our system model based on the
probabilistic framework proposed in [25, 26], as well as our
assumptions and adversarial model. We conclude by sketch-
ing the problem this work aims at solving. In Table 1, we
summarize the notations introduced throughout the section.

2.1 User and Adversary

We consider a scenario in which users move in an area
partitioned into M discrete regions R = {r1, r2, · · · , rM }.
We also assume that time is discrete and it is partitioned
into diﬀerent time periods (e.g., morning, afternoon). We
denote the spatiotemporal position of a user u at time t as
the actual event au(t) = hu, t, ri. We do not make any spe-
ciﬁc assumption about the users’ mobility patterns. Users
connect sporadically to an LBS provider to which they need
to share their current location in order to obtain a service,
i.e., there is a non-negligible time gap between two succes-
sive accesses a user to the LBS. The access proﬁle ψτ
u(r) of
user u is the probability distribution of the location r from
which user u accesses the LBS in time period τ . For a given
u(r) = 1.
We note that this proﬁle is time-dependent (i.e., users may
have diﬀerent access patterns in the morning than in the
afternoon). This dependency also aﬀects users’ location pri-
vacy requirements, and service quality requirements. For the
sake of simplicity, in this paper, we omit the time-period τ
and provide a solution for each user in a given time period.
But, we note that the method is easily adaptable to more
complex access patterns and privacy/quality requirements
elicitation that account for such changes in time (e.g., by
applying the method to each time period separately).

user u in a given time period τ , we have Pr∈R ψτ

We assume that the LBS to which the user connects, or
any entity that can eavesdrop on the user-LBS communi-
cations, is a passive and curious adversary whose aim is to
discover the location of the user at the query time. As the
LBS use is sporadic, the knowledge that the adversary can
accumulate with repeated observations/eavesdropping is the
frequency with which the user issues queries from regions in
R, i.e., ψu(r). We assume that the adversary learns the
user’s proﬁle ψu(.) for example by using the algorithm ex-
plained in [26]. As we focus on user-centric mechanisms,
which give protection to each user separately, in the remain-
der of the paper we omit the user identity u and present the
model for this user with proﬁle ψ(.).

2.2 Location-Privacy Protection Mechanism

We consider that users want to preserve their location pri-
vacy when they use the LBS. Users implement a local and
user-centric LPPM that transforms each true location r into
a pseudolocation r′ ∈ R′, which is then sent to the LBS in-
stead of the actual location. We set R′ = R (however, in the
most general case R′ is the powerset of R). The spatiotem-
poral position of a user as perceived by the LBS, denoted
o(t) = ht, r′i, is called an observed event. For each actual
event a(t) = ht, ri the LPPM chooses a pseudolocation r′ by
sampling from the following probability distribution:

f (r′|r) = Pr(cid:8)o(t) = ht, r′i|a(t) = ht, ri(cid:9)

The adversary’s knowledge is modeled as the user’s ac-
cess proﬁle ψ(.). As accesses to the LBS are sporadic, two
successive query locations of the user are conditionally inde-
pendent given ψ(.). The larger the inter-query time is, the

(1)

618Symbol

u

r, R
ψ(r)

a(t) = ht, ri

r′, R′

o(t) = ht, r′i

f (r′|r)
dq(r′, r)

Qloss(ψ, f, dq)

Qmax
loss
ˆr

h(ˆr|r′)
dp(ˆr, r)

Table 1: Summary of notations

Meaning
Identity of the user
Actual location of the user, set of possible locations for the user
Location access proﬁle of the user (probability of being at location r when accessing the LBS)
Actual location r of the user at time t
Pseudolocation output by the LPPM, set of possible pseudolocations output by the LPPM
Observed pseudolocation r′ of the user at time t
Location obfuscation function implemented by the LPPM: Probability of replacing r with r′.
Incurred service-quality loss by the user if LPPM replaces location r with pseudolocation r′
Expected quality loss of an LPPM with location obfuscation function f
Maximum tolerable service quality loss
Adversary’s estimate of the user’s actual location
Adversary’s attack function: Probability of estimating ˆr as user’s actual location, if r′ is observed
Distance between locations ˆr and r: Privacy of the user at location r if adversary’s estimate is ˆr

P rivacy(ψ, f, h, dp) Expected location privacy of the user with proﬁle ψ(.) using protection f against attack h

more independent the two locations of the user in her succes-
sive LBS accesses are. This is also reﬂected in the LPPM’s
obfuscation algorithm that outputs pseudolocations that de-
pend only on the user’s current location.

2.3 Service Quality Metric

In the aforementioned setting, the LBS response quality
depends on the pseudolocation output by the LPPM and
not on the user’s actual location. The distortion introduced
in the observed pseudolocations determines the quality of
service that the user experiences. The more similar the ac-
tual and the observed location are, the higher the service
quality is. The expected quality loss due to an LPPM f (.)
is computed as an average of dq(r′, r) over all r and r′:

Qloss(ψ, f, dq) =Xr,r′

ψ(r)f (r′|r)dq(r′, r).

(2)

Function dq(.) determines the dissimilarity between loca-
tion r and pseudolocation r′. The semantics of this dis-
similarity depend on the LBS under consideration, and also
on the user’s speciﬁc service-quality expectations. In many
applications, the service quality can be considered inversely
proportional to the physical distance between r and r′. For
example, applications that ﬁnd nearby points of interest
could give very diﬀerent responses to r and to r′ even if they
are only a couple of kilometers apart. In contrast, there exist
LBSs in which the service quality depends on other criteria,
such as on whether r′ is within a region of interest. For a
weather forecast application, for instance, any pseudoloca-
tion r′ in the same city as the actual location r would result
in a high quality LBS response.

We assume that users impose a maximum tolerable ser-
loss , caused by sharing pseudolocations

vice quality loss, Qmax
instead of their actual locations. Formally,

Qloss(ψ, f, dq) ≤ Qmax
loss .

(3)

This constraints the LPPM obfuscation function f (r′|r),
that must not output pseudolocations that, on average, re-
sult in lower quality. We note that the inﬂuence of threshold
Qmax
loss on the LPPM depends on the function dq(.), hence it
is also dependent on the type of the LBS the user is query-
ing. In the case of an LBS that ﬁnds nearby points of in-
terest, where dq(.) is proportional to the physical distance
between r and r′, enforcing the quality threshold could re-

sult in ensuring a maximum allowed distance between these
two locations. For the weather application, enforcing the
quality threshold could result in setting region boundaries
within which locations lead to the same forecast. For other
location-based applications, the function dq(.) and the thresh-
old Qmax

loss can be deﬁned in the same vein.

2.4 Location Privacy Metric

The adversary’s goal is to infer the user’s actual events
a(t) = ht, ri given the observed events o(t) = ht, r′i. Recall
that the adversary knows the user’s proﬁle, ψ(.). He uses
this background knowledge to run an inference attack on
the observed events in order to output estimations ˆr of the
user’s actual locations. Formally, the attack result can be
described as a probability density function h(.) such that

(4)

h(ˆr|r′) = Pr(cid:8)a(t) = ht, ˆri|o(t) = ht, r′i(cid:9) .

As the adversary’s prior information is the probability
that the user is at a given location when she accesses the
LBS, the current (query) location of the user is condition-
ally independent of her observed past and future locations.
This is reﬂected in that the computation of the estimated
location ˆr at time t only depends on the pseudolocation r′
observed at the same time t.

We note that the attack formulation is independent of
whether the considered LPPM anonymizes the events or not.
In this work, we assume that the adversary knows the iden-
tity of the users behind the events, but the framework can
be adapted to anonymous LPPMs as well. Note that even
when users are anonymous, our optimal solution provides a
guarantee for their location privacy (even after a potential
re-identiﬁcation attack).

We follow the deﬁnition in [26] and quantify the user’s lo-
cation privacy as the adversary’s expected error in his infer-
ence attack, i.e., the expected distortion in the reconstructed
event. We compute the expectation over all r, r′, and ˆr:

P rivacy(ψ, f, h, dp) = Xˆr,r′,r

ψ(r)f (r′|r)h(ˆr|r′)dp(ˆr, r)

(5)

The distortion function quantiﬁes the loss of privacy stem-
ming from the inference attack. The privacy loss depends
on the locations’ semantics and also on the privacy require-
ments of the user (i.e., users might consider locations inside
a hospital more sensitive than other places), and dp(.) must

619be deﬁned accordingly. For instance, if the user wants to
hide just her exact current location (as opposed to hiding
her location area), the appropriate distortion function could
be the Hamming distance (probability of error) between the
estimated location ˆr and the actual location r:

dp(ˆr, r) =(0,

1,

if ˆr = r
otherwise

(6)

In this case, any location diﬀerent from the user’s actual
location results in a high level of location privacy. Alterna-
tively, the user’s privacy might depend on the physical dis-
tance between the estimated and actual locations, hence the
distortion function can be modeled as the Euclidean distance
between these locations, i.e., the squared-error distortion:

dp(ˆr, r) = (ˆr − r)2

(7)

2.5 Problem Statement

Given

1. a maximum tolerable service-quality loss Qmax

loss imposed
by the user as a bound for Qloss(.), computed using the
quality function dq(.), and

2. a prior adversarial knowledge of the user’s proﬁle ψ(.),
the problem is ﬁnding the LPPM obfuscation function f (.)
that maximizes the user’s location privacy as deﬁned in (5).
The solution must consider that the adversary

1. observes the LPPM’s output r′, and
2. is aware of the LPPM’s internal algorithm f (.).

Hence, the adversary implements the optimal attack h(.)
that estimates the true location of the user with the least
distortion as measured by dp(.).

3. GAME FORMULATION

The problem of ﬁnding an LPPM that oﬀers optimal lo-
cation privacy given the knowledge of the adversary is an
instance of a zero-sum Bayesian Stackelberg game.
In a
Stackelberg game the leader, in our case the user, plays ﬁrst
by choosing an LPPM and committing to it by running it on
her actual location. The follower, in our case the adversary,
plays next by estimating the user’s location, knowing the
LPPM that the user has committed to. It is a Bayesian game
because the adversary has incomplete information about the
user’s true location, and plays according to his hypothesis
about this location.
It is also an instance of a zero-sum
game, as the adversary’s gain (or loss) of utility is exactly
balanced by the losses (or gains) of the utility of the user:
the information gained (lost) by the adversary is the loca-
tion privacy lost (gained) by the user. We now proceed to
deﬁne the game adapted to our problem:

Step 0 Nature selects a location r ∈ R for the user to access
the LBS, according to a probability distribution ψ(.).
That is, location r is selected with probability ψ(r).

Step 1 Given r, the user runs the LPPM f (r′|r) to select
a pseudolocation r′ ∈ R′, subject to f (.) complying
with the service quality constraint (3).

Step 2 Having observed r′, the adversary selects an esti-
mated location ˆr ∼ h(ˆr|r′), ˆr ∈ R. The adversary
knows the probability distribution f (r′|r) used by the
LPPM; he also knows the user’s proﬁle ψ(.), but not
the true location r.

Final Step The adversary pays an amount dp(ˆr, r) to the
user. This amount represents the adversary’s error
(equivalently, the location privacy gained by the user).

The above description is common knowledge to both the
adversary and the user. They both aim to maximize their
payoﬀ, i.e.
the adversary tries to minimize the expected
amount that he will pay, while the user tries to maximize it.

4. SOLUTION

In this section, we describe a precise optimization problem
that formalizes the objectives of the user and of the adver-
sary. We construct two linear programs that, given ψ(.),
dp(.) and dq(.), we can compute the user’s optimal choice
of protection mechanism f (.), and the adversary’s optimal
choice of inference attack h(.).

4.1 Optimal Strategy for the User

The adversary observes the pseudolocation r′ output by
the LPPM, he knows the function f (r′|r) implemented by
the LPPM, and he also knows the user’s proﬁle ψ(.). Thus,
he can form the posterior distribution

Pr(r|r′) =

Pr(r, r′)
Pr(r′)

=

f (r′|r)ψ(r)

Pr f (r′|r)ψ(r)

(8)

on the true location r of the user, conditional on the obser-
vation r′. The adversary’s objective is then to choose ˆr to
minimize the user’s conditional expected privacy, where the
expectation is taken under Pr(r|r′). The user’s conditional
expected privacy for an arbitrary ˆr is

Pr(r|r′)dp(ˆr, r),

Xr

and for the minimizing ˆr it is

Pr(r|r′)dp(ˆr, r).

min

ˆr Xr

(9)

(10)

If there are multiple values of ˆr that satisfy (10), then the
adversary randomizes arbitrarily among them. The proba-
bility with which ˆr is chosen in this randomization is h(ˆr|r′).
Of course, h(ˆr|r′) will be positive only for minimizing values
of ˆr; for all other values h(ˆr|r′) will be zero. When random-
izing, (10) is rewritten as

Pr(r|r′)h(ˆr|r′)dp(ˆr, r).

(11)

Xr,ˆr

Note that if there is only one value of ˆr satisfying (10),
then this value is selected with probability 1 in the random-
ization, whereas all other values are selected with probability
0, so (11) reduces to (10). In this sense, (11) is a general-
ization of (10), but it should be noted that both expressions
compute the same conditional expected privacy.

We see that for a given r′, the user’s conditional privacy
is given by (10). The probability that r′ is output by the

LPPM is Pr(r′) =Pr f (r′|r)ψ(r). Hence, the user’s uncon-

ditional expected privacy (averaged over all r′) is

Pr(r′) min

Pr(r|r′)dp(ˆr, r)

ˆr Xr

Xr′
=Xr′

min

ˆr Xr

ψ(r)f (r′|r)dp(ˆr, r).

(12)

620To facilitate the computations, we deﬁne

xr′ , min

ˆr Xr

ψ(r)f (r′|r)dp(ˆr, r).

(13)

Incorporating xr′ into (12), we rewrite the unconditional

expected privacy of the user as

xr′ ,

Xr′

(14)

which the user aims to maximize by choosing the optimal
f (r′|r). The minimum operator makes the problem non-
linear, which is undesirable, but (13) can be transformed to
a series of linear constraints:

ψ(r)f (r′|r)dp(ˆr, r), ∀ˆr.

(15)

xr′ ≤Xr

It turns out that maximizing (14) under (13) is equivalent

to maximizing (14) under (15) [4, Ch. 7, p. 224].

We construct the linear program for the user from (14)
and (15). Note that variable xr′ is a decision variable in the
linear program, i.e. it is among the quantities chosen by the
solver. This might appear counterintuitive, as xr′ is deﬁned
in (13) as a function of f (.), rather than as an independent
variable that can be freely selected. But, because of the
transformation, it is always guaranteed that (13) will hold.
The linear program for the user is the following: Choose

f (r′|r), xr′ , ∀r, r′ in order to

Maximize Xr′

subject to

xr′

xr′ ≤Xr
Xr
ψ(r)Xr′
Xr′

f (r′|r) = 1, ∀r

f (r′|r) ≥ 0, ∀r, r′

ψ(r)f (r′|r)dp(ˆr, r), ∀ˆr, r′

f (r′|r)dq(r′, r) ≤ Qmax

loss

(18)

(16)

(17)

(19)

(20)

Inequalities (17) are the series of linear constraints (15),
one series for each value of r′; inequality (18) reﬂects the
service quality constraint; constraints (19) and (20) reﬂect
that f (r′|r) is a probability distribution function.

4.2 Optimal Strategy for the Adversary

The reasoning is similar for the formalization of the ad-
versary’s optimization problem. When the LPPM’s output
is pseudolocation r′, the adversary will solve (10) to ﬁnd an
estimate ˆr. More generally, the adversary will ﬁnd many
minimizing values of ˆr, and each of them will be selected
with some probability h(ˆr|r′). Given that the true location
is r and that the observed pseudolocation is r′, the condi-
tional expected user privacy is

h(ˆr|r′)dp(ˆr, r).

(21)

Xˆr

The user chooses r′ to maximize (21). So, given that the
true location is r, the conditional expected user privacy for
the maximizing r′ is

yr , max

r′ Xˆr

h(ˆr|r′)dp(ˆr, r).

(22)

Similarly as before, the maximization can be generalized
to a randomization among maximizing values of r′. The
probability with which r′ is chosen is f (r′|r).

The prior distribution ψ(r) contains the adversary’s knowl-
edge of r. Thus, the unconditional expected user privacy is

ψ(r) yr,

Xr

(23)

that the adversary aims to minimize by choosing h(ˆr|r′).
Similarly as before, (22) can be transformed to an equivalent
series of linear constraints:

h(ˆr|r′)dp(ˆr, r), ∀r′.

(24)

yr ≥Xˆr

We construct the linear program for the adversary (which
is the dual of the user’s linear program) from (23) and (24):
Choose h(ˆr|r′), yr, ∀r, r′, ˆr, and z ∈ [0, ∞) in order to

ψ(r) yr + zQmax
loss

(25)

Minimize Xr

subject to

h(ˆr|r′)dp(ˆr, r) + zdq(r′, r), ∀r, r′ (26)

yr ≥Xˆr
Xˆr

h(ˆr|r′) = 1, ∀r′

h(ˆr|r′) ≥ 0, ∀r′, ˆr
z ≥ 0

(27)

(28)
(29)

Note the role of variable z: In linear programming par-
lance, it is the shadow price of the service quality constraint.
Intuitively, z is the “exchange rate” between service quality
and privacy. Its value in the optimal solution indicates the
amount of privacy (in privacy units) that is lost (gained) if
the service quality threshold Qmax
loss increases (decreases) by
one unit of quality.

loss in Qmax

For example, if z > 0 in the optimal solution, then any
change ∆Qmax
loss will aﬀect the privacy achieved by
z∆Qmax
loss . In this case, constraint (18) is satisﬁed as a strict
equality. In contrast, if constraint (18) is satisﬁed as a strict
inequality, then, intuitively, the selection of f (r′|r) has not
been constrained by Qmax
loss . In this case, any (small) changes
loss will have no eﬀect on f (r′|r), nor on the privacy
in Qmax
achieved. So, z would be zero.

Note that both linear programs compute the uncondi-
tional expected privacy of the user (5), which we repeat here
for convenience.

P rivacy(ψ, f, h, dp) = Xˆr,r′,r

ψ(r)f (r′|r)h(ˆr|r′)dp(ˆr, r). (30)

Previous expressions can be derived from this one. For
instance, if there is a single best choice of a pseudolocation
r′ for each given location r, then f (r′|r) is always either 0
or 1, so (10) is obtained.

The optimal solution of each linear program results in the
same value for the privacy of the user. Hence, in princi-
ple, we only need to compute one of the two to quantify
maximum level of privacy of the user. We choose to present
both, because the user’s linear program incorporates the ser-
vice quality constraint in a more straightforward manner,
whereas the adversary’s linear program explicitly computes
the “exchange rate” between service quality and privacy.

621 

 

Figure 1: Spatial histogram showing the density of
users per region (in log scale) in Lausanne. The area
size is 15.32km × 7.58km, divided into 20 × 15 regions.

5. EVALUATION

The proposed optimization framework enables us to deter-
mine the most eﬀective location-privacy protection mecha-
nism (LPPM) against optimal inference attacks. The opti-
mal LPPM is designed under the constraint of guaranteeing
a minimum service quality such that the location-based ser-
vice remains useful for the user. In this section, we evalu-
ate the relation between location privacy and service qual-
ity for a few example location-based services (Recall that the
service-quality sensitivity of a LBS to location obfuscation is
encoded through the dissimilarity function dq(.)). Moreover,
we evaluate the performance of non-optimal LPPMs and
non-optimal inference attacks against the optimal strategies.
We use real location traces of people (in Lausanne, Switzer-
land) who use various means of transportation.1 We select
11 users at random, and we focus on their location traces
during the day (8am to 8pm), when it is more probable that
user use location-based services. The length of the consid-
ered traces is one month. The location area, within which
they move, is divided into 300 regions. Figure 1 shows the
density of users across all the regions. The grayness of the
cells shows the density of its corresponding region in log
scale. As many of the regions are abandoned (or very rarely
visited) by many individual users, we compute each user’s
proﬁle ψ(.) by considering only the 30 most popular regions
across the whole population. This prevents sparse user pro-
ﬁles. A user’s proﬁle is the normalized number of her visits
to each region.

Given distance functions dp(.) and dq(.) and service-quality
loss threshold Qmax
loss , we compute the optimal LPPM and its
corresponding optimal attack by solving (16) and (25) us-
ing Matlab’s linear programming solver. We then compare
the obtained optimal protection mechanism and the optimal
inference attack against obfuscation LPPMs and Bayesian
inference attacks, respectively.

Basic Obfuscation LPPM.

The basic obfuscation LPPM, with an obfuscation level
k = 1, 2, 3, . . ., is constructed in the following way: For each
location r, we ﬁnd its k − 1 closest locations (using the Eu-
clidean distance between the centers of the regions). The

1The traces are obtained from the Lausanne Data Collection
Campaign dataset, http://research.nokia.com/page/11367

probability distribution function f (.|r) will be the uniform
probability distribution on the set of the k − 1 selected lo-
cations together with the location r. That is, location r
is replaced by each of the k locations, as a pseudolocation,
with the same probability 1
k , and all the rest of locations
have probability 0. Thus, in practice, an actual location r
is hidden among its k − 1 nearest locations. We choose this
mechanism, as it has been very popular in the literature.

Given the user proﬁle ψ(.) and quality distance function
dq(.), we use (2) to compute the expected service-quality
loss Qloss(ψ, f, dq) for any LPPM obfuscation f (.), whether
it be optimal or not.

Bayesian Inference Attack on an LPPM.

We compare the eﬀectiveness of our optimal attack with
the Bayesian inference attack, which has been shown eﬀec-
tive before in [26]. In the Bayesian approach, for each pseu-
dolocation r′, the posterior probability distribution over the
locations is used to invert the noise added by the LPPM
and, thus, to estimate the actual location:

h(ˆr|r′) =

Pr(ˆr, r′)
Pr(r′)

=

f (r′|ˆr)ψ(ˆr)

Pr f (r′|r)ψ(r)

(31)

We use (5) to compute the expected location privacy of
a user who adopts a given (obfuscation or optimal) LPPM
f (.) against a (Bayesian or optimal) inference attack h(.).
The expected location privacy also depends on the distortion
function dp(.) that we choose to use.

Brieﬂy, if dp(.) is the Hamming distance, then the Bayesian
attack chooses the location with the highest posterior proba-
bility Pr(ˆr|r′). If dp(.) is the Euclidean distance, the Bayesian
attack chooses the conditional expected value E[ˆr|r′].

Optimal Inference Attack on an Arbitrary LPPM.

In order to make a fair comparison between the eﬀec-
tiveness of the optimal and obfuscation LPPM, we need to
run the same attack on both of them. The Bayesian in-
ference attack described by (31) can be performed against
both. However, we still need to design an optimal attack
against arbitrary LPPMs that have not been constructed in
our game-theoretic framework.

The optimal inference attack is the one that minimizes

the expected user privacy:

h(ˆr|r′) = arg min

h

P rivacy(ψ, f, h, dp).

(32)

Given the user proﬁle ψ(.), an LPPM f (.) and distortion
function dp(.), the following linear program ﬁnds the optimal
attack h(.). Note that, compared to (25), there is no service
quality constraint here, as the LPPM has been assumed to
be arbitrary.

ψ(r)f (r′|r)h(ˆr|r′)dp(ˆr, r)

(33)

h(ˆr|r′) = 1, ∀r′, and h(ˆr|r′) ≥ 0, ∀ˆr, r′ (34)

Minimize Xˆr,r′,r
subject toXˆr

Location-Privacy Protection Mechanism Output.

Consider a LBS user making use of our optimal LPPM on
her mobile device. The way her location appears in the eyes
of the adversary is shown in Figure 2. For the sake of com-
parison, Figure 2 also shows how a basic obfuscation LPPM
distributes the pseudolocations over space. In order to make

622User’s Profile

Optimal LPPM, over all locations

Obfuscation LPPM, over all locations

Optimal LPPM. Loc(13,7)

Obfuscation LPPM. Loc(13,7)

(1)

(2)

(3)

(4)

(5)

Figure 2: Input/Output of LPPM. Proﬁle of a user for whom the subsequent calculations are made (sub-ﬁgure
1). Distribution Pr(r′) of observed pseudolocations when using the optimal LPPM with Qmax
loss = 0.8690 (sub-
ﬁgure 2). Distribution Pr(r′) of observed pseudolocations when using obfuscation LPPM with Qloss(ψ, f, dq) =
0.8690 (sub-ﬁgure 3). Conditional distribution Pr(r′|r) when using the optimal LPPM on location r = (13, 7)
(sub-ﬁgure 4). Conditional distribution Pr(r′|r) when using obfuscation LPPM on location r = (13, 7) (sub-
ﬁgure 5). Column 1 is the leftmost column, and row 1 is the bottom row. (Euclidean dp, Hamming dq)

a fair comparison, we need to make sure that the cost of the
two LPPMs, in terms of service quality, is the same. To
do so, we compute the quality loss Qloss of the obfuscation
LPPM and assign this loss as the quality threshold Qmax
loss of
the optimal LPPM. Hence, the optimal LPPM cannot sac-
riﬁce the service quality more than the obfuscation LPPM
to gain higher location privacy.

Figures 2(2) and 2(3) show Pr(r′), the distribution of
pseudolocations averaged over all locations for optimal and
obfuscation LPPMs, respectively. Given arbitrary LPPM
location obfuscation function f (.) and user proﬁle ψ(.), the
probability distribution of pseudolocations is

Pr(r′) =Xr

ψ(r)f (r′|r).

(35)

As it is shown, the distribution corresponding to the opti-
mal LPPM is more uniform, making it more diﬃcult for the
adversary to invert it eﬀectively.

In Figures 2(4) and 2(5), we show the distribution of pseu-
dolocations for speciﬁc location r = loc(13, 7). By observing
how uniform their outputs are, we can easily make the com-
parison between the two LPPMs. The obfuscation LPPM
is obviously more concentrated around the actual location,
whereas the optimal LPPM (with the same service-quality
loss as the obfuscation method) broadens the set of pseudolo-
cations to most of possible regions including highly probable
regions (i.e. regions r with a large ψ(r)). This higher diver-
sity brings higher privacy as we will see later in this section.

Tradeoﬀ between Privacy and Service Quality.

We now study the tradeoﬀ between the level of privacy
that the optimal LPPM provides, against the optimal at-
tack, and the service-quality loss that it causes. We plot
in Figure 3(a) the evolution of the service quality loss, as
the optimal LPPM is conﬁgured to guarantee diﬀerent lev-
els of service quality (for users with diverse proﬁles and for
various service quality thresholds). Each line in the ﬁgure
represents one user and each ◦ represents one Qmax
loss . We plot
P rivacy(ψ, f, h, dp) versus Qloss(ψ, f, dq).

Unsurprisingly, increasing the level of location-privacy pro-
tection signiﬁcantly degrades the service quality. Also, as
expected, we can observe that the maximum achievable loca-
tion privacy is strongly dependent on the user proﬁle. This is
reﬂected by the separation between the diﬀerent lines. Each
user can have up to a certain level of privacy regardless of

the quality threshold (represented by ◦ in the ﬁgure). Hence,
the service-quality loss remains constant once this level has
been reached. This is due to the presence of the optimal
attack that squeezes the location-privacy gain.

This eﬀect is further illustrated in Figure 3(b), where the
service-quality loss of optimal LPPM is plotted against the
service-quality threshold. Once the optimal LPPM oﬀers the
maximal location privacy for a given user proﬁle, loosening
the service-quality constraint does not signiﬁcantly change
the LPPM’s underlying function f , and thus there is no
reduction in service quality.
In other words, there is no
need to sacriﬁce the service quality, because doing so does
not increase the user’s location privacy.

Eﬀectiveness of the Optimal Strategies.

Given Euclidean distance functions dp(.) and dq(.), we
compute the optimal LPPM and attack methods for a set of
service quality thresholds Qmax
loss . For each user, we run the
Bayesian inference attack on her optimal LPPM. We also
evaluate the location privacy oﬀered by the basic obfuscation
LPPM with respect to the optimal attack. We vary the
obfuscation level from 1 (minimum) to 30 (maximum), and
for each case we compute the corresponding quality loss.
Then, this value is set as the threshold Qmax
loss for ﬁnding the
optimal attack mechanism.

Figure 4(a) shows the superiority of the optimal attack
to the Bayesian attack, when location privacy of users is
protected using the optimal LPPM: For any given user and
service-quality threshold, the location privacy that the user
obtains is smaller when the adversary implements the opti-
mal strategy rather than the Bayesian inference attack.

Figure 4(b) shows the superiority of the optimal LPPM
to the obfuscation LPPM, against the optimal attack: For
any given user and service-quality threshold, a user has a
higher privacy level when the LPPM implements the opti-
mal strategy. As expected, obtained privacy by both mech-
anisms become equal when no service quality is guaranteed
for the user (i.e., Qmax

loss is set to its maximum value).

Consider a single user. To further investigate the eﬀec-
tiveness of optimal strategies, we evaluate her privacy under
four diﬀerent combinations of optimal and non-optimal pro-
tection/attack methods, that have been explained before.

Similar to Figure 2, we consider the basic obfuscation
LPPM as the basis for generating the service-quality thresh-

6235

4.5

4

3.5

3

5

4.5

4

3.5

3

q

)

d

 
,
f
 
,

ψ

(

s
s
o

q

)

d

 
,
f
 
,

ψ

(

s
s
o

l

Q

2.5

2

1.5

1

1

l

Q

2.5

2

1.5

1

0

1.5

2

2.5

3

3.5

4

Privacy(ψ, f, h, d
)
p

2

4

6
max
loss

Q

8

10

12

(a) Location privacy P rivacy(ψ, f, h, dp) vs. Service-quality
loss Qloss(ψ, f, dq) for a given service-quality threshold Qmax
loss .
The circles ◦ represent diﬀerent values of Qmax
loss .

(b) Service-quality threshold Qmax
Service-quality
loss Qloss(ψ, f, dq),
location privacy
P rivacy(ψ, f, h, dp). The circles ◦ represent diﬀerent values
of P rivacy(ψ, f, h, dp).

loss vs.
for a given level of

Figure 3: Tradeoﬀ between Privacy and Service Quality: Optimal LPPM against the optimal attack. The
diﬀerent lines represent users with diverse proﬁles ψ(.). (Euclidean dq(.) and Euclidean dp(.).)

h
 
k
c
a
t
t

i

 

A
n
a
s
e
y
a
B

 
,
)

d

 
,
h
 
,
f
 
,

ψ
(
y
c
a
v
i
r

P

p

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

f
 

M
P
P
L
 
n
o

i
t

a
c
s
u
f
b
O

 
,
)

d

p

 
,
h
 
,
f
 
,

ψ
(
y
c
a
v
i
r

P

1

2

3

4

5

Privacy(ψ, f, h, d
), Optimal Attack h
p

5

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

0

1

2

3

4

5

Privacy(ψ, f, h, d
), Optimal LPPM f
p

(a) Location privacy P rivacy(ψ, f, h, dp) oﬀered by the opti-
mal LPPM against the optimal attack derived using the game
theoretic approach vs. against the Bayesian-inference attack.

(b) Location privacy P rivacy(ψ, f, h, dp) oﬀered by the opti-
mal LPPM vs. location privacy oﬀered by the basic obfusca-
tion LPPM, both evaluated against the optimal attack.

Figure 4: Eﬀectiveness of the optimal attack and optimal LPPM strategies. Diﬀerent lines represent users
with diverse proﬁles ψ(.), and the circles ◦ represent diﬀerent values of Qmax

loss . (Euclidean dq(.) and dp(.).)

old Qmax
loss . In all graphs of Figure 5 each dot represents one
obfuscation level used in the basic obfuscation LPPM. The
corresponding service-quality loss for each obfuscation level
is shown on the x-axis of all four plots. As it can be eas-
ily observed from the ﬁgures, the optimal attack, compared
with the Bayesian attack, always results in a higher degra-
dation of the user’s location privacy. Moreover, the optimal
LPPM always provides a higher level of privacy for the user
(regardless of the service-quality threshold) compared with
the basic obfuscation LPPM.

The ﬁgures well illustrate how both user and adversary
converge to use optimal strategies against each other. The
user’s favorite setting, i.e. the one that brings her a high
level of privacy, is (Optimal, Bayesian). Inversely, the (Ob-
fuscation, Optimal) combination is the favorite setting for
the adversary, in which he pays the minimum cost of estimation-
error. However, neither of these two settings is a stable
state. In the (Optimal, Bayesian) combination, the adver-
sary would gain more by choosing the Optimal attack. In the
(Obfuscation, Optimal) combination, the user would gain

624 

Obfuscation,Bayesian
Optimal,Bayesian
Obfuscation,Optimal
Optimal,Optimal

0.221774

0.418235

0.638889

0.721595

0.773723

0.811246

Q

max
loss

(b) Hamming dq(.) and Euclidean dp(.)

 

Obfuscation,Bayesian
Optimal,Bayesian
Obfuscation,Optimal
Optimal,Optimal

0.666667

0.75

0.8

0.5
max
loss

Q

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

 
0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

p

)

d

 
,

h

 
,
f
 
,

ψ
(
y
c
a
v
i
r

P

p

)

d
 
,
h
 
,
f
 
,

ψ
(
y
c
a
v
i
r

P

Obfuscation,Bayesian
Optimal,Bayesian
Obfuscation,Optimal
Optimal,Optimal

 

)

d

p

 
,

h

 
,
f
 
,

ψ
(
y
c
a
v
i
r

P

0.160329

0.399895

0.930509

1.16883

1.35572

1.57467

Q

max
loss

(a) Euclidean dq(.) and Euclidean dp(.)

 

p

)

d
 
,
h
 
,
f
 
,

ψ
(
y
c
a
v
i
r

P

Obfuscation,Bayesian
Optimal,Bayesian
Obfuscation,Optimal
Optimal,Optimal

4.5

4

3.5

3

2.5

2

1.5

1

0.5

0

 
0

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0

 
0

1.66545

1.66545

1.67123
Q

max
loss

2.39066

3.51999

4.01646

0

 
0

0.488575

0.488575

(c) Euclidean dq(.) and Hamming dp(.)

(d) Hamming dq(.) and Hamming dp(.)

Figure 5: Service-quality threshold Qmax
loss vs. Location privacy P rivacy(ψ, f, h, dp), for one single user. The
diﬀerent lines represent combinations of optimal (◦) and basic obfuscation (•) LPPMs tested against optimal
(· · · ) and Bayesian-inference (–) attacks. The service-quality threshold Qmax
loss is equal to the service quality
obtained by the basic obfuscation LPPM when the number of obfuscation levels used to perturb the location
varies from 1 to 30 (its maximum value).

more by choosing the Optimal LPPM. Hence, the (Optimal,
Optimal) combination is a stable equilibrium for both.

The fourth combination (Obfuscation, Bayesian) illustrates
an interesting behavior. For small quality thresholds Qmax
loss
(or, equivalently, smaller obfuscation levels) the user’s pri-
vacy is lower compared with the (Optimal, Optimal) case.
However, at some middle point its provided privacy increases
and surpasses the privacy obtained from the optimal meth-
loss , the optimal LPPM uses all
ods.
its available capacity to increase privacy by distributing the
user’s pseudolocations over a higher number of locations.
So, it performs better than the basic obfuscation LPPM,
which is limited to distributing pseudolocations only in a
small set of regions. But when the obfuscation level (or,
similarly, the service-quality threshold) increases, the basic

Indeed, for small Qmax

obfuscation LPPM does better. First, because it is no longer
severely limited, and, second, because it is paired against the
Bayesian inference attack, which is weaker than the optimal
inference attack.

6. RELATED WORK

The ﬁeld of location privacy has been a very active area of
research in recent years. Work on this topic can be roughly
classiﬁed in three categories: mainly focused on the design of
LPPMs; mainly focused on recovering actual user trajecto-
ries from anonymized or perturbed traces; or mainly focused
on the formal analysis and the search for an appropriate lo-
cation privacy metric that allows for the fair comparison
between LPPMs.

625Existing LPPMs are built according to diﬀerent design
principles. The most popular approach to obtaining loca-
tion privacy is to send a space- or time-obfuscated version
of the users’ actual locations to the service provider [10, 12,
14, 16]. A diﬀerent approach consists in hiding some of the
users’ locations by using mix zones [1, 9], or silent periods
[15]. These are regions where users do not communicate with
the provider while changing their pseudonym. Provided that
several users traverse the zone simultaneously, this mecha-
nism prevents an adversary from tracking them, as he cannot
link those who enter with those who exit the region. A third
line of work protects location privacy by adding dummy re-
quests, indistinguishable from real requests, issued from fake
locations to the service provider [3]. The purpose of these
fake locations is to increase the uncertainty of the adversary
about the users’ real movements.

A number of papers show that the predictability of users’
location traces, and the particular constraints of users’ move-
ments, are suﬃcient to reconstruct and/or identify anony-
mous or perturbed locations. For instance, an adversary
can, to name but a few possibilities, infer users’ activities
from the frequency of their visits to certain locations [19]; re-
identify anonymous low-granularity location traces given the
users’ mobility proﬁles [5]; or derive [13], and re-identify [11,
18] the home address of individuals from location traces.

Several authors have made eﬀorts towards formalizing the
desirable location privacy requirements that LPPMs should
fulﬁll, as well as towards ﬁnding suitable metrics to evaluate
the degree to which these requirements are fulﬁlled. Exam-
ples of these lines of work are Krumm [18], Decker [6], and
Duckham [7]. Shokri et al. [24] revisit existing LPPMs and
the location-privacy metrics used in their evaluation. They
classify these metrics in three categories: uncertainty-based
(entropy), error-based and k-anonymity. The authors con-
clude, by means of a qualitative evaluation, that metrics
such as entropy and k-anonymity are not suitable for mea-
suring location privacy. In a follow-up of this work, Shokri
et al. provide a framework [25, 26] to quantify location pri-
vacy. The framework allows us to specify an LPPM and
then to evaluate various questions about the location infor-
mation leaked. Our design methodology uses this analytical
framework as an evaluation tool to quantifying the LPPMs’
oﬀered privacy against the localization attack.

Despite the extent to which location privacy has been
studied, there is a patent disconnection between these dif-
ferent lines of work. Most of the aforementioned papers use
diﬀerent models to state the problem and evaluate location
privacy. This hinders the comparison of systems and slows
down the design of robust LPPMs. Further, in some of these
papers there is a detachment between the proposed design
and the adversarial model against which it is evaluated. Of-
ten the considered adversary is static in its knowledge and
disregards the information leaked by the LPPM algorithm;
or adversarial knowledge is not even considered in the evalu-
ation. The works by Freudiger et al. [9] and Shokri et al. [24,
25, 26] do consider an strategic adversary that exploits the
information leaked by the LPPM in order to compute loca-
tion privacy. Nevertheless, their work, which we build on in
this paper, does not address how this privacy computation
can be integrated in the design of location-privacy preserv-
ing mechanisms.

In this work, we bridge the gap between design and eval-
uation of LPPMs. We provide a systematic method for de-

veloping LPPMs; it maximize users’ location privacy while
guaranteeing a minimum service quality. We formalize the
optimal design problem as a Bayesian Stackelberg game sim-
ilar to previous work on security in which, as in the location-
privacy scenario, the defender can be modeled as a Stack-
elberg game leader, and the adversary as the follower. The
common theme is that the defender must commit to a de-
fense strategy/protocol, which is then disclosed to the adver-
sary, who can then choose an optimal course of action after
observing the defender’s strategy. Paruchuri et al. [23] pro-
pose an eﬃcient algorithm for ﬁnding the leader’s optimal
strategy considering as a main case study a patrolling agent
who searches for a robber in a limited area. In their case,
the defender is unsure about the type of the adversary (i.e.
where the adversary will attack). In contrast, in our work it
is the adversary who is unsure about the type (i.e. the true
location) of the user/defender. A similar approach is used
by Liu and Chawla [20] in the design of an optimal e-mail
spam ﬁlter, taking into account that spammers adapt their
e-mails to get past the spam detectors. The same problem
is tackled by Br¨uckner and Scheﬀer [2], who further com-
pare the Stackelberg-based approach with previous spam ﬁl-
ters based on support vector machines, logistic regression,
and Nash-logistic regression. Korzhyk et al. [17] contrast
the Stackelberg framework with the more traditional Nash
framework, within a class of security games. A recent survey
[21] explores the connections between security and game the-
ory more generally. To the best of our knowledge, our work
is the ﬁrst that uses Bayesian Stackelberg games to design
optimal privacy-protection mechanisms.

7. CONCLUSION

Accessing location-based services from mobile devices en-
tails a privacy risk for users whose sensitive information can
be inferred from the locations they visit. This information
leakage raises the need for robust location-privacy protect-
ing mechanisms (LPPMs). In this paper, we have proposed
a game-theoretic framework that enables a designer to ﬁnd
the optimal LPPM for a given location-based service, ensur-
ing a satisfactory service quality for the user. This LPPM
is designed to provide user-centric location privacy, hence it
is ideal to be implemented in the users’ mobile devices.

Our method accounts for the fact that the strongest ad-
versary not only observes the perturbed location sent by the
user but also knows the algorithm implemented by the pro-
tection mechanism. Hence, he can exploit the information
leaked by the LPPM’s algorithm to reduce his uncertainty
about the user’s true location. However, the user is only
aware of the adversary’s knowledge and does not make any
assumption about his inference attack. Hence, she prepares
the protection mechanism against the most optimal attack.
By modeling the problem as a Bayesian Stackelberg compe-
tition, we ensure that the optimal LPPM is designed antic-
ipating such strong inference attack.

We have validated our method using real location traces.
We have demonstrated that our approach ﬁnds the optimal
attack for a given LPPM and service-quality constraint, and
we have shown that it is superior to other LPPMs such as
basic location obfuscation. We have also shown that the
superiority of the optimal LPPM over alternatives is more
signiﬁcant when the service-quality constraint imposed by
the user is tightened. Hence, our solution is eﬀective exactly
where it will be used. Finally, our results conﬁrm that loos-

626ening the service-quality constraint allows for increased pri-
vacy protection, but the magnitude of this increase strongly
depends on the user proﬁle, i.e., on the degree to which a
user’s location is predictable from her LBS access proﬁle.

To the best of our knowledge, this is the ﬁrst frame-
work that explicitly includes the adversarial knowledge into
a privacy-preserving design process, and considers the com-
mon knowledge between the privacy protector and the at-
tacker. Our obtained result is a promising step forward in
the quest for robust and eﬀective privacy preserving systems.

8. REFERENCES

[1] A. R. Beresford and F. Stajano. Location privacy in

pervasive computing. IEEE Pervasive Computing,
2(1):46–55, 2003.

[2] M. Br¨uckner and T. Scheﬀer. Stackelberg games for

adversarial prediction problems. In C. Apt´e, J. Ghosh,
and P. Smyth, editors, 17th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining (KDD), 2011.

[3] R. Chow and P. Golle. Faking contextual data for fun,

proﬁt, and privacy. In WPES ’09: Proceedings of the
8th ACM workshop on Privacy in the electronic
society, New York, NY, USA, 2009.

[4] S. Dasgupta, C. Papadimitriou, and U. Vazirani.
Algorithms. McGraw-Hill, New York, NY, 2008.

[5] Y. De Mulder, G. Danezis, L. Batina, and B. Preneel.

Identiﬁcation via location-proﬁling in gsm networks.
In WPES ’08: Proceedings of the 7th ACM workshop
on Privacy in the electronic society, New York, NY,
USA, 2008.

[6] M. Decker. Location privacy - an overview. In

International Conference on Mobile Business, 2009.
[7] M. Duckham. Moving forward: location privacy and

location awareness. In Proceedings of the 3rd ACM
SIGSPATIAL International Workshop on Security and
Privacy in GIS and LBS, New York, NY, USA, 2010.

[8] J. Freudiger, R. Shokri, , and J.-P. Hubaux.

Evaluating the privacy risk of location-based services.
In Financial Cryptography and Data Security (FC),
2011.

[9] J. Freudiger, R. Shokri, and J.-P. Hubaux. On the

optimal placement of mix zones. In PETS ’09:
Proceedings of the 9th International Symposium on
Privacy Enhancing Technologies, Berlin, Heidelberg,
2009.

[10] B. Gedik and L. Liu. Location privacy in mobile

systems: A personalized anonymization model. In
ICDCS ’05: Proceedings of the 25th IEEE
International Conference on Distributed Computing
Systems, Washington, DC, USA, 2005.

[11] P. Golle and K. Partridge. On the anonymity of

home/work location pairs. In Pervasive ’09:
Proceedings of the 7th International Conference on
Pervasive Computing, Berlin, Heidelberg, 2009.

[12] M. Gruteser and D. Grunwald. Anonymous usage of
location-based services through spatial and temporal
cloaking. In MobiSys ’03: Proceedings of the 1st
international conference on Mobile systems,
applications and services, New York, NY, USA, 2003.

[13] B. Hoh, M. Gruteser, H. Xiong, and A. Alrabady.

Enhancing security and privacy in traﬃc-monitoring
systems. IEEE Pervasive Computing, 5(4):38–46, 2006.

[14] B. Hoh, M. Gruteser, H. Xiong, and A. Alrabady.

Preserving privacy in gps traces via uncertainty-aware
path cloaking. In CCS ’07: Proceedings of the 14th
ACM conference on Computer and communications
security, New York, NY, USA, 2007.

[15] T. Jiang, H. J. Wang, and Y.-C. Hu. Preserving
location privacy in wireless lans. In MobiSys ’07:
Proceedings of the 5th international conference on
Mobile systems, applications and services, New York,
NY, USA, 2007.

[16] P. Kalnis, G. Ghinita, K. Mouratidis, and

D. Papadias. Preventing location-based identity
inference in anonymous spatial queries. Knowledge
and Data Engineering, IEEE Transactions on,
19(12):1719–1733, Dec. 2007.

[17] D. Korzhyk, Z. Yin, C. Kiekintveld, V. Conitzer, and

M. Tambe. Stackelberg vs. Nash in security games:
An extended investigation of interchangeability,
equivalence, and uniqueness. Journal of Artiﬁcial
Intelligence Research, 41:297–327, May–August 2011.
[18] J. Krumm. Inference attacks on location tracks. In In

Proceedings of the Fifth International Conference on
Pervasive Computing (Pervasive), 2007.

[19] L. Liao, D. J. Patterson, D. Fox, and H. A. Kautz.

Learning and inferring transportation routines. Artif.
Intell., 171(5-6):311–331, 2007.

[20] W. Liu and S. Chawla. A game theoretical model for

adversarial learning. In Y. Saygin, J. X. Yu,
H. Kargupta, W. Wang, S. Ranka, P. S. Yu, and
X. Wu, editors, IEEE International Conference on
Data Mining Workshops (ICDM 2009), 2009.

[21] M. Manshaei, Q. Zhu, T. Alpcan, T. Basar, and J.-P.

Hubaux. Game theory meets network security and
privacy. ACM Computing Surveys, 2011.

[22] J. Meyerowitz and R. Roy Choudhury. Hiding stars

with ﬁreworks: location privacy through camouﬂage.
In MobiCom ’09: Proceedings of the 15th annual
international conference on Mobile computing and
networking, New York, NY, USA, 2009.

[23] P. Paruchuri, J. P. Pearce, J. Marecki, M. Tambe,
F. Ord´o˜nez, and S. Kraus. Eﬃcient algorithms to
solve Bayesian Stackelberg games for security
applications. In D. Fox and C. P. Gomes, editors, 23rd
AAAI Conference on Artiﬁcial Intelligence (AAAI
2008), 2008.

[24] R. Shokri, J. Freudiger, M. Jadliwala, and J.-P.
Hubaux. A distortion-based metric for location
privacy. In WPES ’09: Proceedings of the 8th ACM
workshop on Privacy in the electronic society, New
York, NY, USA, 2009.

[25] R. Shokri, G. Theodorakopoulos, G. Danezis, J.-P.

Hubaux, and J.-Y. Le Boudec. Quantifying location
privacy: the case of sporadic location exposure. In
Proceedings of the 11th international conference on
Privacy enhancing technologies (PETS), Berlin,
Heidelberg, 2011.

[26] R. Shokri, G. Theodorakopoulos, J.-Y. Le Boudec,
and J.-P. Hubaux. Quantifying location privacy. In
IEEE Symposium on Security and Privacy, Oakland,
CA, USA, 2011.

627