Network Performance of Smart Mobile Handhelds in a

University Campus WiFi Network

Xian Chen, Ruofan Jin
University of Connecticut

Kyoungwon Suh

Illinois State University

Bing Wang, Wei Wei
University of Connecticut

ABSTRACT
Smart mobile handheld devices (MHDs) such as smartphones
have been used for a wide range of applications. Despite the
recent ﬂurry of research on various aspects of smart MHDs,
little is known about their network performance in WiFi net-
works. In this paper, we measure the network performance
of smart MHDs inside a university campus WiFi network,
and identify the dominant factors that aﬀect the network
performance. Speciﬁcally, we analyze 2.9TB of data col-
lected over three days by a monitor that is located at a gate-
way router of the network, and make the following ﬁndings:
(1) Compared to non-handheld devices (NHDs), MHDs use
well provisioned Akamai and Google servers more heavily,
which boosts the overall network performance of MHDs.
Furthermore, MHD ﬂows, particularly short ﬂows, bene-
ﬁt from the large initial congestion window that has been
adopted by Akamai and Google servers.
(2) MHDs tend
to have larger local delays inside the WiFi network and are
more adversely aﬀected by the number of concurrent ﬂows.
(3) Earlier versions of Android OS (before 4.X) cannot take
advantage of the large initial congestion window adopted by
many servers. On the other hand, the large receive window
adopted by iOS is not fully utilized by most ﬂows, poten-
tially leading to waste of resources. (4) Some application-
level protocols cause ineﬃcient use of network and operat-
ing system resources of MHDs in WiFi networks. Our ob-
servations provide valuable insights on content distribution,
server provisioning, MHD system design, and application-
level protocol design.

Categories and Subject Descriptors
D.4.8 [Performance]: Measurements

General Terms
Performance, Measurement

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

Keywords
Smart Mobile Handhelds, WiFi Networks, Performance Mea-
surement

1.

INTRODUCTION

Smart mobile handheld devices (MHDs) such as smart-
phones have been used for a wide range of applications in-
cluding surﬁng web, checking email, watching video, ac-
cessing social networking services, and online games. Most
MHDs are equipped with both cellular and WiFi interface
cards. Whenever available, WiFi is still a preferred way for
Internet connection due to its higher bandwidth, lower delay
and lower energy consumption [9]. Although recently there
is a ﬂurry of studies on various aspects of smart MHDs (see
Section 2), little is known about the network performance
of smart MHDs in WiFi networks. Speciﬁcally, how is their
network performance? What are the major factors that af-
fect the network performance?

In this paper, we answer the above questions by measuring
the performance of smart MHDs inside a university campus
WiFi network. Speciﬁcally, our study is over a data set
that is passively captured by a monitor placed at a gateway
router in the University of Connecticut (UConn). The data
set is collected over three days (2.9TB of data), containing
traﬃc from various wireless devices, including MHDs such
as iPhones, iPod touches, iPads, Android phones, Windows
phones, and Blackberry phones, and wireless non-handheld
devices (NHDs) such as Windows laptops and MacBooks.
To understand the performance of MHDs, we ﬁrst separate
the traﬃc of MHDs from that of NHDs. Our focus is on
MHDs; we describe the results on NHDs when necessary for
comparison.

Analyzing the data set, we ﬁnd HTTP is the dominant
traﬃc type, accounting for over 92% of the TCP ﬂows. We
therefore focus on the performance of HTTP ﬂows in this pa-
per. The behavior of HTTP is complicated: we ﬁnd many
HTTP ﬂows contain multiple HTTP requests, and a sig-
niﬁcant portion of an HTTP ﬂow is idling (with no data
packets). Hence, the traditional throughput metric (i.e., the
amount of data downloaded in a ﬂow divided by the total
duration of the ﬂow) may introduce bias in measuring the
performance of HTTP ﬂows. We therefore deﬁne a metric,
per-ﬂow servicing rate, i.e., the amount of data downloaded
corresponding to HTTP requests in a ﬂow divided by the
downloading duration of the ﬂow, to quantify the perfor-
mance of an HTTP ﬂow (see Section 4). This metric is
interesting in its own right:
it represents the network per-
formance of an HTTP ﬂow, while excluding the eﬀect of

315various delays (e.g., client processing delays and user pause
times) that are irrelevant to network performance. Our main
ﬁndings are as follows.

• We observe that the best performance is achieved for
the MHD ﬂows served by an Akamai server cluster
that is located close to UConn. Furthermore, 16% of
the MHD ﬂows are served by this server cluster, ac-
counting for 35% of the bytes. Overall, a large per-
centage (38.4%) of MHD ﬂows are served by well pro-
visioned Akamai and Google servers, account for 62%
of the bytes. Interestingly, these fractions are signiﬁ-
cantly larger than the corresponding values for NHDs,
indicating that MHDs use Akamai and Google servers
more heavily, which boosts their overall network per-
formance. We also observe that Akamai and Google
servers have adopted large initial congestion window,
which contributes to the network performance of MHDs
(particularly for short ﬂows). On the other hand, we
observe that most ﬂows have low loss rates, indicating
that loss rate is not a limiting factor in the campus
WiFi network that we study.

• We ﬁnd that MHDs tend to have longer local RTTs
(i.e., delays within the university network) than NHDs.
In addition, the number of concurrent ﬂows has nega-
tive eﬀect on performance, and the eﬀect is more sig-
niﬁcant for MHDs than NHDs. These two diﬀerences
between MHDs and NHDs might be caused by the infe-
rior computation and I/O capabilities on MHDs. The
eﬀect of local RTT on network performance seems neg-
ligible due the fact that local RTT only takes a small
portion of the RTT.

• We ﬁnd that earlier versions of Android OS (before
4.X) cannot take advantage of the large initial conges-
tion window adopted by many servers. While Android
OS increases the receive window adaptively in every
round trip time, it uses a very small initial receive win-
dow (much smaller than the initial congestion window
adopted by Google and Akamai servers), which limits
the performance of Andriod devices. In contrast, we
observe that iOS uses a large static receive window,
which fully exploits the beneﬁt of large initial conges-
tion window. On the other hand, most ﬂows do not
fully utilize the large receive window, potentially lead-
ing to unnecessary waste of resources on iOS devices.

• We ﬁnd that some application-level protocols cause in-
eﬃcient use of network and operating system resources
of MHDs in WiFi networks. One example is the native
YouTube application on iOS devices, which can use a
large number of TCP ﬂows to serve a single video. We
suspect this is a design optimization for cellular net-
works, which is not suitable for WiFi networks.

Our ﬁndings highlight the impact of TCP parameters and
application-level design choices on MHD network perfor-
mance, providing valuable insights on content distribution,
server provisioning, mobile system design, and application-
level protocol design.

The rest of the paper is organized as follows. Section 2 de-
scribes related work. Section 3 describes data collection and
classiﬁcation. Section 4 introduces the performance met-
ric. Section 5 describes our methodology. Section 6 presents

network performance of MHDs, and explores the impact of
network and application layer factors on the performance.
Section 7 discusses what ﬁndings are speciﬁc to UConn WiFi
network and what can be applied to other networks. Last,
Section 8 concludes the paper and presents future research
directions.

2. RELATED WORK

Several recent studies characterize the usage and traﬃcs
of MHDs in 3G cellular networks, public WiFi or residen-
tial WiFi networks. Trestian et al. analyze a trace collected
from the content billing system of a large 3G mobile service
provider to understand the relationship among people, loca-
tions and interests in 3G mobile networks [31]. Maier et al.
study packet traces collected from more than 20,000 residen-
tial DSL customers to characterize the traﬃc from MHDs in
home WiFi networks [21]. Falaki et al. employ passive snif-
fers on individual devices (Android and Windows Mobile
smartphones) to record sent and received traﬃc, and pro-
vide a detailed look at smartphone traﬃc [11]. This study is
limited to a small user population (43 users), and the traces
were not separately analyzed for the diﬀerent network inter-
faces (i.e., 3G and WiFi) being used, which have diﬀerent
properties. The study in [12] uses a larger population of
255 users, and characterizes intentional user activities and
the impact of these activities on network and energy usage.
Gember et al. compare the content and ﬂow characteristics
of MHDs and NHDs in campus WiFi networks [14]. We
focus on network performance of MHDs in a campus WiFi
network and the impact of various factors on the network
performance of MHDs, which are not investigated in the
above studies.

Huang et al. anatomize the performance of smartphone
applications in 3G networks using their widely-deployed ac-
tive measurement tool, 3GTest [16]. Tso et al. study the
performance of mobile HSPA (a 3.5G cellular standard) net-
works in Hong Kong using extensive ﬁeld tests [32]. Our
study diﬀers from them in that we study the network per-
formance of MHDs in WiFi networks (instead of cellular
networks). Furthermore, our study is based on large-scale
traces collected passively from a university campus network,
while the study in [16] adopts an active measurement tool
and the study in [32] uses customized applications. The
studies of [13, 26] report that iOS native YouTube player
generates multiple TCP ﬂows to stream a single video. We
expand these studies by presenting the degree of ineﬃcien-
cies caused by the large number of TCP ﬂows and the main
reason for the design choice in the player.

Several measurement studies are on university WiFi net-
works. However, MHDs have only been widely adopted re-
cently, and none of those studies explores the network per-
formance of MHDs as in our study. Our study builds upon
the rich literature on understanding the behavior of TCPs
and content distribution in operational environments (e.g.,
[19, 23, 20, 15, 30, 10, 4, 22]), and our observations conﬁrm
some of the ﬁndings in those studies.

Last, several other aspects of MHDs have been studied re-
cently, including battery use and charging behaviors [6, 25],
energy consumption [29, 5], and performance enhancement
techniques (e.g., [24, 34]). Our study diﬀers in scope from
them.

3163. DATA COLLECTION & CLASSIFICATION

We collect measurements at a monitoring point inside the
University of Connecticut (UConn). The monitor is a com-
modity PC, equipped with a DAG card [2]. It is placed at a
gateway router of UConn, capturing all incoming and outgo-
ing packets through the router with accurate timestamps1.
In particular, it captures up to 900 bytes of each packet, in-
cluding application-level, TCP and IP headers. The campus
network uses separate IP pools for Ethernet and Wireless
LAN (WLAN). Since we are interested in the network per-
formance of MHDs, which use the WLAN IP address pool,
we use ﬁlters to capture only WLAN traﬃc.

We have collected two data sets. One data set is for three
days, from 9am March 2 to 9am March 5, 2011. The other
data set is for one day, from 9am April 23 to 9am April
24, 2012. Unless otherwise stated, we use the ﬁrst data set
(reasons for focusing on this data set and ﬁndings from the
second data set are deferred to Section 6.9). In the follow-
ing, we ﬁrst provide a high-level view of the data, and then
present methodology to separate MHD traﬃc from NHD
traﬃc (the captured data set contains a mixture of traﬃc
from both types of devices).

3.1 Data

Table 1 lists the number of packets captured during the
three days. Overall, we collected over 5.8G packets (2.9 TB
of data). Among them, 91.9% of the packets are carried
by TCP. We only report the results obtained from the data
collected on the ﬁrst day; the statistical characteristics of
the data on the other two days are similar2.

Table 1: The number of packets captured during the
three days.

incoming pkts
outgoing pkts

total pkts

% TCP pkts

Day 1 Day 2 Day 3 Overall
1.2G
3.5G
0.8G
2.3G
2.0G
5.8G
91.8% 92.7% 90.8% 91.9%

0.8G
0.5G
1.4G

1.4G
1.0G
2.4G

We say a TCP ﬂow is valid if it ﬁnishes three-way hand-
shaking and does not contain a RESET packet3. Among the
valid TCP ﬂows, we identify the applications using destina-
tion port numbers (a recent study shows that this simple
approach provides accurate results [20]). Table 2 lists the
most commonly used applications, and the percentages of
their traﬃc over all TCP traﬃc in terms of ﬂows, packets
and bytes, where the number of bytes in a packet is obtained
from the IP header. We observe a predominant percentage
(92.3%) of the TCP ﬂows are HTTP ﬂows. This is consistent
with measurement results in other campus networks [14] and
home environments [20]. In the rest of the paper, we focus on
the network performance of HTTP ﬂows; the performance
of other protocols is left as future work.

1The campus network balances loads among two gateway
routers. The load balancing strategy is set up so that data
packets and ACKs in a TCP ﬂow are through the same
router.
2The amount of traﬃc on Day 3 is less than that of the other
two days since Day 3 is a Friday.
311.9% of the ﬂows cotain a RESET packet.

Table 2: Percentage of the traﬃc from commonly
used applications (in terms of ﬂows, packets and
bytes) over all the TCP traﬃc (Day 1).

Application
HTTP (80)

HTTPS (443)
IMAPS (993)

ﬂows
92.3%
4.3%
0.2%

packets

bytes
86.7%
5.4%
0.28% 0.13%

82%
8%

3.2 Data classiﬁcation

We refer to an HTTP ﬂow that contains packets coming
from and/or destinating to an MHD as an MHD ﬂow (sim-
ilar deﬁnition for an NHD ﬂow). Since our captured data
contains a mixture of MHD and NHD ﬂows, we need to sep-
arate these two types of ﬂows. The ﬁrst approach we use to
diﬀerentiate MHD and NHD ﬂows is based on the keywords
in the user-agent ﬁeld in HTTP headers (MHDs and NHDs
use diﬀerent keywords) [21, 14]. Once the type of a ﬂow
is identiﬁed using the user-agent ﬁeld, it provides us infor-
mation on the type of the associated device (i.e., whether
it is an MHD or NHD), which can be used to further iden-
tify the types of other ﬂows. Speciﬁcally, if we know that
a ﬂow, f , from a device (the device is represented using an
IP address) is an MHD ﬂow, then we know all the ﬂows
that are concurrent with f and have the same source IP ad-
dress as f are MHD ﬂows4. If the type of an HTTP ﬂow
is not identiﬁed using the above two approaches, we use the
knowledge that an IP address pool is dedicated to Apple
mobile devices at UConn5, and hence a ﬂow using an IP
address from this pool is an MHD ﬂow. Using the above
three approaches, we categorize 94.1% of the HTTP ﬂows
to be either MHD or NHD ﬂows (91.9% of them are directly
identiﬁed through the user-agent ﬁeld, 6.7% are identiﬁed
through the concurrent-ﬂow approach, and the rest 1.4% are
identiﬁed using the knowledge of the dedicated IP address
pool). Speciﬁcally, we identify 0.7M MHD ﬂows and 10.4M
NHD ﬂows, containing 38.8M and 821.1M packets, respec-
tively. Further separation of the ﬂows using other heuristics
(e.g., TTL [21]) is left as future work.

We further identify the operating systems used by the
MHDs and NHDs using the user-agent ﬁeld. For MHDs,
the dominant operating system is iOS, used in 96.2% of the
MHD ﬂows (iOS is used by iPhone, iPod touch, and iPad,
with the percentages of ﬂows as 65%, 26%, and 9%, respec-
tively); and Android is the second most popular operating
system, used in 3.2% of the MHD ﬂows. For NHDs, the two
dominant operating systems are OS X and Windows, taking
56.7% and 42.2% of the NHD ﬂows, respectively; and Linux
is at the third place, used by 1.1% of the NHD ﬂows.

4. PERFORMANCE METRIC

As mentioned earlier, we mainly characterize the perfor-

4We can only classify the ﬂows that are concurrent with f
due to IP reassignment which can assign the same IP address
to another device at another point of time.
5UConn network administrators set aside this dedicated
pool to ease the management of IP addresses. Speciﬁcally,
a device is assigned an IP address from this dedicated pool
if its host name indicates that it is an Apple mobile device
during the DHCP request phase.

317mance of HTTP ﬂows since HTTP is the predominant traﬃc
type. The behavior of HTTP is complicated. For instance,
modern browsers often open multiple TCP connections when
browsing a web page [10]. Furthermore, an HTTP ﬂow can
send and receive multiple HTTP requests/responses.
In
our passive measurements, without any knowledge of the
accessed content, it is diﬃcult to correlate multiple TCP
connections. We therefore focus on per-ﬂow network perfor-
mance, speciﬁcally, the performance that is the result of the
complex interactions of myriad network and application re-
lated factors. In the following, we ﬁrst describe our measure-
ment results on HTTP ﬂows, and then deﬁne a performance
metric that we will use in the rest of the paper.

F
D
C

1.00

0.95

0.90

0.85

0.80

0.75

1

 MHD
 NHD

10

100

1000

Number of HTTP Requests

Figure 1: Distribution of the number of HTTP GET
requests in an HTTP ﬂow.







t

(cid:2)

1

t1

tA



tS


	

  

	

  


   
 
  


tn

tF



(cid:2)

t

n

te

Figure 2: Illustration of a persistent HTTP ﬂow.

We ﬁnd that over 99% of the HTTP ﬂows use HTTP 1.1,
where TCP connections are persistent, i.e., each TCP con-
nection allows multiple GET and POST commands. On the
other hand, we observe only 6.5% of the HTTP ﬂows contain
POST commands. This is not surprising since most of the
users in the campus network are content consumers. In the
following, we ignore the HTTP ﬂows with POST commands.
Fig. 1 plots the distribution of the number of HTTP GET
requests within an HTTP ﬂow. Observe that over 20% of
the HTTP ﬂows contain at least two requests. The number

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

1.0

0.8

0.6

0.4

0.2

F
D
C

 MHD,Down.
 NHD,Down.
 MHD,Total.
 NHD,Total.

 MHD

 NHD

0.01

0.1

1

10

100

1000

0.0

1E-3

0.01

0.1

1

Duration (sec)

Down. Duration / Total Duration

(a)

(b)

Figure 3: (a) Total and downloading durations of
MHD and NHD ﬂows. (b) The ratio of downloading
duration over total duration.

of requests in a ﬂow can be as large as 100. In addition, NHD
ﬂows tend to contain a larger number of requests than MHD
ﬂows. This might be because regular web pages accessed by
NHDs contain more objects than their mobile versions that
are often accessed by MHDs [33].

Fig. 2 illustrates a persistent HTTP ﬂow with n requests
(the requests are sequential since we do not observe any
pipelined requests in our trace where a request is sent out
without waiting for the response of the previous request6).
The measurement point sits between the client and server,
capturing the packets with accurate timestamps. In partic-
ular, let tS denote the timestamp of the SYN packet, tA
denote the timestamp of the ACK packet in the three-way
handshaking, ti denote the timestamp of the ith HTTP re-
quest, t(cid:2)
i denote the timestamp of the last data packet corre-
sponding to the ith HTTP request, tF denote the timestamp
of the ﬁrst FIN packet, and te denote the timestamp indi-
cating the end of a TCP ﬂow. Then the measured duration
of the HTTP ﬂow at the measurement point is te − tS, while
i − ti). We refer
the duration for data downloading is
to the former as total duration and the latter as download-
ing duration. Fig. 3(a) plots the distributions of the total
and downloading durations for MHD and NHD ﬂows. We
observe that total durations can be signiﬁcantly longer than
downloading durations: the median downloading durations
are 0.63s and 0.93s for MHD and NHD ﬂows, respectively,
while the median total durations are 5.06s and 15.45s for
MHD and NHD ﬂows, respectively. As shown in Fig. 3(b),
the downloading duration is less than 10% of the total du-
ration for respectively 60% and 42% of the MHD and NHD
ﬂows.

i=1(t(cid:2)

(cid:2)n

The diﬀerence between the downloading and total dura-
tion contains various delays inside an HTTP ﬂow. Parts
of the delays are for the three-way handshaking and TCP
connection termination, and the rest of the delays are ap-
plication idle/processing time. We divide the application
idle/processing time into three parts. The ﬁrst is the delay
from ﬁnishing the three-way handshaking to the ﬁrst GET
request, deﬁned as TS = t1 − tA. The second is the sum
of the delays from ﬁnishing one HTTP request to starting
the next HTTP request, i.e., TG =
i), and the
last is the delay from ﬁnishing downloading the last object to
start to close the TCP connection, i.e., TF = tF − t(cid:2)
n. Fig. 4

i=1 (ti+1 − t(cid:2)

(cid:2)n−1

6HTTP pipeline is not recommended and disabled in most
browsers [3].

3181.0

0.8

0.6

0.4

0.2

0.0

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

F
D
C

 TS
 TF
 TG

 TS
 TF
 TG

0.01

0.1

1

10

100

0.01

0.1

1

10

100

Delay (sec)

(a) MHD

Delay (sec)

(b) NHD

Figure 4: Distribution of the various delays in an
HTTP ﬂow.

1.0

0.8

0.6

0.4

0.2

F
D
C

1.0

0.8

0.6

0.4

0.2

F
D
C

 TS
 TF
 TG

0.0

1E-3

0.01

0.1

1

0.0

0.01

0.1

 TS
 TF
 TG

1

Delay / Total Duration

Delay / Total Duration

(a) MHD

(b) NHD

Figure 5: Distribution of the ratios of the various
delay over the total duration in an HTTP ﬂow.

plots the distributions of the various delays. We observe
that TS is small for both MHD and NHD ﬂows. Although a
large fraction of the TF values is small, it can be as large as
several seconds for MHD ﬂows and tens of seconds for NHD
ﬂows. The values of TG are in a wide range from millisec-
onds to tens of seconds. Fig. 5 plots the CDF of the ratios
of the various delay over the total duration.

In summary, the drastic diﬀerence between total dura-
tion and downloading duration highlights the importance of
deﬁning performance metric carefully: a traditional through-
put metric, deﬁned as the total amount of data downloaded
divided by the total duration, can lead to biased results on
the performance of HTTP ﬂows. To exclude the eﬀects of
application-level delays on network performance, we deﬁne
a performance metric, per-ﬂow servicing rate, i.e., the total
number of data bytes that are downloaded corresponding
to HTTP requests divided by the downloading duration, to
represent network performance. This metric represents the
rate at which data are being fetched in an HTTP ﬂow from a
server to a client, while excluding the eﬀect of various delays
(e.g., client processing delays and user pause times) that are
irrelevant to network performance.

5. METHODOLOGY

In the traces that we collected, MHDs are typically clients,
requesting contents from servers outside UConn campus net-
work. To understand the performance of MHD ﬂows, we
ﬁrst group them according to their destinations. The ratio-
nale is that since the clients (i.e., MHDs) are at the same
geographic location, the destinations (corresponding to the
content servers) directly determine the network path. For

the ﬂows that are served by the same group of servers, we
further divide the ﬂows according to their lengths (in terms
of number of packets inside the ﬂow), since ﬂow length af-
fects the impact of the various TCP parameters and network
path conditions on servicing rate. Speciﬁcally, short ﬂows
may terminate before ﬁnishing the slow-start phase in TCP,
while long ﬂows are more likely to reach congestion avoid-
ance phase, and hence achieve more steady throughput.

In the following, we ﬁrst describe server and ﬂow classiﬁ-
cation, and then describe how we measure the various TCP
and application-level characteristics.

5.1 Server Classiﬁcation

We use two steps to study each ﬂow destination IP ad-
dress. First, we use reverse DNS lookup to ﬁnd its domain
name, and then query the IP registration information to de-
termine the organization that registered the IP address. For
the IP addresses that do not have a published domain name,
we use the registration information only.

Table 3: Top ﬁve domain names for MHD and NHD
ﬂows (percentage is in terms of ﬂows).

MHD
Akamai.com (26.1%)
Google.com (12.3%)
Facebook.com (10.0%) Facebook.com (8.3%)
Amazon.com (6.8%)
Apple.com (5.0%)

NHD
Akamai.com (20.0%)
Google.com (8.9%)

Yahoo.com (5.9%)
Amazon.com (5.3%)

Table 3 presents the top ﬁve domain names of MHD ﬂows
that are obtained using reverse DNS lookup. We see that the
largest percentage of MHD ﬂows are served by Akamai CDN.
Indeed, many widely used applications on MHDs are served
by Akamai. For instance, when an iOS device downloads
an application from Apple App Store, the data are actually
downloaded from Akamai. In addition, some popular desti-
nations, e.g., “fbcdn.net”, are served by Akamai (we ﬁnd 12%
of MHD ﬂows are destined to “fbcdn.net”). Following Aka-
mai.com, Google.com is the second most frequently accessed
domain name. This is not surprising: Google.com is one of
the most frequently accessed web sites [1], and many MHDs
have embedded Google applications (e.g., Google Map, Google
Voice, Gmail). The third one is Facebook.com, which uses
Akamai to serve many static contents, and use its own servers
to serve many dynamic contents directly. The fourth one is
Amazon.com, whose cloud services serve many popular ap-
plications running on iOS (e.g., Foursquare).

Since Akamai.com and Google.com are the top two do-
main names, we detail the network performance of the MHD
ﬂows that are served by Akamai and Google servers, as well
as those served by the rest of the servers in Section 6. We
also observe from Table 3 that 38.4% of MHD ﬂows are
served by Akamai CDN and Google servers. This percentage
is signiﬁcantly larger than the corresponding value for NHD
ﬂows (i.e., 28.9%). We believe the reason why MHD ﬂows
use Akamai and Google servers more heavily is that the des-
tination hosts accessed by MHDs are less diverse than those
accessed by NHDs [14]. Based on the host ﬁeld in the HTTP
request headers, we ﬁnd around 6k distinct destination host
domains from MHDs and 51k distinct destination host do-
In addition, Table 4 lists the top 10
mains from NHDs.

319destination host domains accessed by MHD and NHD ﬂows.
We see for MHDs, accesses to the top ten host domains ac-
count for 39.2% of the MHD ﬂows, while the percentage
for NHDs (32%) is much lower. Last, we also observe from
Table 4 that for MHDs, except for facebook.com and pan-
dora.com, the other top eight host domains are all served by
Akamai and Google, conﬁrming the heavy usage of Akamai
and Google servers by MHDs.

Table 4: Top ten destination host domains for MHD
and NHD ﬂows (percentage is in terms of ﬂows).

MHD
fbcdn.net (12.0%)
facebook.com (10%)
apple.com (3.9%)
googlevideo.com (3.8%)
google.com (2.5%)
admob.com (1.6%)
doubleclick.net (1.5%)
youtube.com (1.4%)
google-analytics.com (1.3%)
pandora.com (1.2%)

NHD
fbcdn.net (12.9%)
facebook.com (6.5%)
google.com (2.4%)
doubleclick.net (2.0%)
ytimg.com (1.7%)
quantservec.om (1.6%)
yieldmanager.com (1.5%)
tumblr.com (1.2%)
twitter.com (1.1%)
yahoo.com (1.1%)

5.2 Flow Length Classiﬁcation

For the ﬂows that are served by the same server category,
we divide the ﬂows according to their lengths into three
groups, denoted as G1, G2, and G3. Speciﬁcally, G1 con-
tains short ﬂows, with one to ten data packets, G2 and G3
contain longer ﬂows, with at least 10 and 50 data packets,
respectively. We make the above grouping since many web
pages belong to G1 [10], while long video streaming sessions
7. Table 5 lists the number of ﬂows, pack-
may belong to G3
ets and bytes (calculated from packet size in IP header) of
the various groups. Observe that around 75% of the ﬂows
are short G1 ﬂows.

Table 5: Information of the various groups of MHD
ﬂows (for the data collected on Day 1).

ﬂows

packets
bytes

G3

G2
0.2M

G1
0.5M
0.03M
2.9M 25.1M 19.0M
2.3GB 22.8GB 19.4GB

5.3 TCP ﬂow characteristics

We now describe how we estimate the various TCP ﬂow
characteristics, including RTT, loss rate, local RTT and loss
rate, server’s initial congestion window, client’s advertised
receive window, and the maximum window size.

We use the techniques in [19] to obtain a sequence of RTT
samples for a ﬂow, and use the median to represent the ﬂow’s
RTT. Packet losses are detected when observing triple dupli-

cate ACKs, or a retransmission that is caused by timeout8.
Local RTT represents the delay that a TCP ﬂow experiences
locally, inside a local-area network (i.e., the UConn campus
network in our context). Similarly, local loss rate represents
the loss rate that a TCP ﬂow experiences inside a local-area
network. In our study, both local RTT and local loss rate
can be directly obtained from the measurements at the mon-
itoring point since it is at the edge of the local-area network
(see details in [7]).

The server’s initial congestion window size (icwnd) has a
signiﬁcant impact on the ﬂow downloading rate. It deter-
mines the amount of data that the server can send in the
ﬁrst window. If the icwnd is too small, the sender needs to
pause and waits for ACKs before transmitting again; on the
other hand, if it is too large, the sender pushes too much
data into the network, and congestion could happen [10].
We infer the icwnd based on the packet arrival times at
the monitoring point. Speciﬁcally, we obtain an estimate of
the icwnd as n when observing the ﬁrst window of n pack-
ets arriving close in time at the the monitoring point. The
ﬁrst window is dynamically estimated based on RTTs using
the technique in [19]. This approach will underestimate the
icwnd if the application does not have suﬃcient amount of
data to send in the ﬁrst window. To avoid potential under-
estimation, we only apply the approach to HTTP ﬂows that
contain at least one GET request and the data correspond-
ing to the ﬁrst GET request contain at least 20 packets. The
constraint of at least 20 data packets (corresponding to at
least 20KB when each packet is at least 1000 bytes) is based
on the literature that servers can choose a large icwnd, e.g.,
between 10KB to 20KB [10, 23]. To obtain a server’s icwnd,
we infer a series of icwnd values from all of the ﬂows that
are served by this server and satisfy the requirements stated
earlier, and then obtain the average, median, and maximum
from these inferred values (our measurements indicate that
a server’s icwnd can vary among the ﬂows). In Section 6.4,
we only report the results from servers that have at least 10
estimates.

Client’s advertised receive window, constantly reported
from the client to the server, states the size of the available
receive buﬀer, i.e., the maximum size of data that a sender
transmits to the receiver before the data is acknowledged by
the receiver. The receive window ﬁeld in the TCP header
is 16 bits, limiting the maximum size of receive window to
64KB. However, if both TCP server and client support win-
dow scaling option, they can agree upon a window scaling
factor, which is the multiplier to the 16-bit receive window
ﬁeld. For example, if window scaling factor is 4, the maxi-
mum receive window that a client can specify is 256KB.

At each time point, the server’s available window is the
minimum of the client’s advertised receive window and server
congestion window. We also measure the maximum window
of each ﬂow during its lifetime. The maximum window has
impact on the network performance as well:
it determines
the maximum amount of data that the sender can transmit
in one RTT, which aﬀects the maximum downloading rate.

5.4 Application layer characteristics

We consider the following three application-level charac-
teristics. The ﬁrst is the design of application-level proto-

7We also consider another group, G4, which contains ﬂows
with at least 100 packets. The results of G4 are similar to
those of G3, and hence are not reported in this paper.

8These are inferred losses at the TCP level. The loss rate
thus estimated is an overestimate since the inferred losses
may be due to long delays, not actual losses.

320cols.
In particular, we consider the protocol for YouTube
videos. This is motivated by the popularity of this applica-
tion and the large amount of video data in our trace (38%
of the data are video contents). Secondly, we examine how
servers respond to requests for diﬀerent types of contents
(e.g., video, image and texts). For this purpose, we deﬁne
application response time as the delay from the ﬁrst GET
message to the ﬁrst responding data packet for an HTTP
request (the delay is measured at the monitoring point).
Last, we quantify the relationship between the number of
concurrent TCP ﬂows and per-ﬂow servicing rate. The ra-
tionale behind this is that the number of concurrent TCP
ﬂows might be an indicator of the amount of CPU and I/O
activities on an MHD device. We calculate the amount of
concurrent TCP ﬂows for a ﬂow as follows. Consider a ﬂow
f . Since the number of its concurrent ﬂows can vary over
time, we divide time into 0.3s bins, and count the number
of concurrent ﬂows in each bin, and then obtain the average
number of concurrent ﬂows during f ’s life-time as its num-
ber of concurrent ﬂows. We only consider concurrent ﬂows
for long ﬂows, speciﬁcally, the ﬂows in G3.

6. NETWORK PERFORMANCE OF MHDS

AND RELATED FACTORS

In this section, we ﬁrst present network performance of
MHDs and then investigate how network and application
layer factors aﬀect the performance. Unless otherwise stated,
we mainly report the results of iOS devices (i.e., iPhone,
iPod touch, iPad) that account for 96.2% of the MHD ﬂows.
As shown in Table 4, a large percentage (38.4%) of MHD
ﬂows are served by Akamai and Google servers. Using a
commercial database [17], we identify that the Akamai servers
are located in four main clusters, respectively 40km, 113km,
517km, and 966km away from UConn. Furthermore, consis-
tent with the Akamai DNS design policy that gets servers
close to end users [15], we ﬁnd 90% of the MHD Akamai
ﬂows are served by the ﬁrst two close-by server clusters. IP
registration information reveals that all Akamai servers in
the 40km range belong to the University of Hartford (only
one hop away from UConn network), while it does not reveal
any detailed information for the servers in the other distance
ranges. We therefore refer to the ﬁrst two server clusters
as Akamai-Hartford and Akamai-113km, respectively.
In
the rest of the paper, we classify the servers into four clus-
ters: Akamai-Hartford, Akamai-113km, Google9, and other
servers.

Figures 6(a), (b), and (c) plot per-ﬂow servicing rate dis-
tributions of G1, G2 and G3 ﬂows, respectively. The re-
sults for all four server clusters are plotted in the ﬁgure.
For the same server cluster, we observe larger per-ﬂow ser-
vicing rate for longer ﬂows because longer ﬂows allow the
congestion window to ramp up. Overall, Akamai-Hartford
servers provide the best performance, with median servic-
ing rates of 4.7Mbps, 5.9Mbps, and 6.5Mbps for G1, G2,
and G3 ﬂows, respectively. For G2 and G3 ﬂows, we ob-

9Google does not publish the location information of its data
centers. Therefore we cannot simply use the database [17] to
determine the geographic locations of Google servers. Hence
we present the aggregate results of all Google servers. It is
possible to determine the geographic location of the servers
using RTT [30]. Dividing the servers into clusters according
to their geographic locations and investigating their respec-
tive performance are left as future work.

serve a clear stochastic order: the performance achieved by
Akamai-Hartford servers is the best, followed by Akamai-
113km, Google, and the other servers. For G1 ﬂows, the
performance achieved by Google servers is superior to that
of Akamai-113km servers, and is superior to that of Akamai-
Hartford servers at low percentiles, a point that we will re-
turn later.

Existing studies (e.g.,

[28, 26]) have shown that video
servers may control the sending rate of video ﬂows, which
can aﬀect per-ﬂow servicing rate of those ﬂows. In the traces
that we collected, we conﬁrm that YouTube ﬂash videos to
NHDs are indeed rate controlled. On the other hand, videos
to NHDs are predominantly mp4 videos, which are not rate
controlled. Therefore, per-ﬂow servicing rates of MHD ﬂows
presented in Fig. 6 are not aﬀected by rate control mecha-
nisms. We next investigate the impact of various network
and application-layer factors on MHD network performance.

6.1 RTT

We observe that RTTs to Akamai-Hartford servers tend
to be the lowest, followed by Akamai-113km, Google, and
the other servers. The main reason for the lowest RTTs
to Akamai-Hartford servers is the close geographic distance
and good network provisioning (both the University of Hart-
ford where the servers are deployed and UConn belong to
Connecticut education network). Fig. 7 plots the RTT dis-
tributions of G2 MHD ﬂows served by the four server clusters
(results for G1 and G3 ﬂows are similar). The median RTT
of the ﬂows served by Akamai-Hartford servers is 8.5ms,
while for Akamai-113km, Google, and other servers, the me-
dian RTTs are respectively 2.3, 4.3, and 10.3 times larger.
For the four server clusters, their relative order in terms of
RTT is consistent with their relative performance as shown
in Fig. 6. That is, a server cluster with lower RTTs tends
to provide better performance. This is not surprising since
RTT is a major factor that aﬀects network performance.

What is more interesting is perhaps the large percent-
age of data that is served by the ﬁrst three server clus-
ters. Speciﬁcally, 58% of the MHD data bytes are served
by the ﬁrst three server clusters, much larger than the cor-
responding value (41%) for NHDs. Furthermore, 16% of
the MHDs ﬂows (accounting for 35% of the data) are served
by Akamai-Hartford servers, the server cluster that provides
the best performance. As described earlier, we believe that
these large percentages originate from the fact that the most
popular services accessed by MHDs are provided by major
companies such as Facebook, Google, and Apple that use
CDN services heavily. The large percentage of data served
by Akamai and Google servers boost the overall network
performance of MHDs.

6.2 Local RTT

We observe that long MHD ﬂows tend to experience larger
local RTTs than short MHD ﬂows. As an example, Fig. 8(a)
plots local RTT distributions of G1, G2, and G3 MHD ﬂows
served by Akamai-Hartford server cluster (results for the
other three server clusters are similar). We see that local
RTT of G1 ﬂows is smaller than that of G2 ﬂows, which is
in turn smaller than that of G3 ﬂows. In addition, we ob-
serve that local RTT of MHDs tends to be larger than that
of NHDs. One example is shown in Fig. 8(b), which plots lo-
cal RTT distributions of G3 MHD and NHD ﬂows served by
Akamai-Hartford servers. Using t-test [18], we conﬁrm that

321 Aka-hfd
 Aka-113
 Google
 Other

1.0

0.8

0.6

0.4

F
D
C

0.2

0.0

 Aka-hfd
 Aka-113
 Google
 Other

1.0

0.8

0.6

0.4

F
D
C

0.2

0.0

 Aka-hfd
 Aka-113
 Google
 Other

1.0

0.8

F
D
C

0.6

0.4

0.2

0.0

10

100

1000 10000 100000

10

100

1000 10000 100000

10

100

1000 10000 100000

Per-flow Servicing Rate (Kbps)

Per-flow Servicing Rate (Kbps)

Per-flow Servicing Rate (Kbps)

(a) G1

(b) G2

(c) G3

Figure 6: Per-ﬂow servicing rate of MHD ﬂows served by the four server clusters.

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

 Aka-hfd
 Aka-113
 Google
 Other

1E-3

0.01

0.1

1

10

RTT (sec)

Figure 7: RTT distributions of G2 ﬂows served by
the four server clusters.

1.0

0.8

0.6

0.4

0.2

0.0

F
D
C

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

 G1
 G2
 G3

the above two observations indeed hold statistically. Specif-
ically, for the MHD ﬂows served by the same server cluster,
the average local RTT of Gi ﬂows is statistically smaller
than that of Gj ﬂows, i < j; and for Gi ﬂows, the average
local RTT of MHD ﬂows is statistically larger than that of
NHD ﬂows, i = 1, 2, 3. The reason might be limited compu-
tation and I/O capabilities on MHDs, which lead to longer
processing time for longer ﬂows, as well as longer processing
time than that on NHDs (a more in-depth study using active
measurements is left as future work). On the other hand, ex-
cept for Akamai-Hartford servers, local RTT is only a small
portion of RTT (the ratio of local RTT over RTT is below
0.2 for 10% to 56% of the ﬂows). Therefore, the impact of
local RTT on per-ﬂow servicing rate is negligible. We also
observe a small fraction of local RTTs that are longer than
100ms, a point we will return to in Section 6.3.

6.3 Loss Rate

F
D
C

1.0

0.9

0.8

0.7

0.6

0.5

 G1

 G2

 G3

 MHD
 NHD

1E-4

1E-3

0.01

0.1

1

Loss Rate

1E-3

0.01

0.1

1

10

1E-3

0.01

0.1

1

10

Local RTT (sec)

Local RTT (sec)

(a)

(b)

Figure 8: (a) Local RTT for the MHD ﬂows served
by Akamai-Hartford servers. (b) Local RTT of G3
MHD and NHD ﬂows served by Akamai-Hartford
servers.

Figure 9: Loss rate distribution for the MHD ﬂows
served by Akamai-Hartford servers.

We ﬁnd that most ﬂows have very low loss rates, indi-
cating that loss rate is not a limiting factor in the campus
WiFi network that we study. An example is shown in Fig. 9,
which plots loss rate distribution of the MHD ﬂows served by
Akamai-Hartford servers (results for the other three server
clusters have similar trend). We observe that, for G1, G2
and G3 ﬂows, respectively 86%, 74% and 54% of the ﬂows

322have zero loss. Furthermore, 90% of G3 ﬂows have loss rates
below 0.02. The loss rate of a short ﬂow can be large, which
is however due to the artifact of small ﬂow size (i.e., even a
small number of losses can lead to high loss rate). We also
ﬁnd that all of the losses occur outside UConn (i.e., local
loss rate is zero). This, however, does not mean that there
is no loss at the MAC layer. We observe that a small per-
centage (around 3%) of local RTTs are longer than 100ms
(see Fig. 8(a)). These long local RTTs might be caused by
MAC-layer retransmissions and back-oﬀ times. Our mea-
surements captured at the monitoring point does not contain
MAC layer information; validating this conjecture through
measurements captured at MAC layer is left as future work.

6.4

Initial Congestion Window

We ﬁnd that Akamai and Google servers have adopted
large initial congestion window. Speciﬁcally, the median
initial congestion window of Akamai-Hartford servers is be-
tween 5 and 6KB, and the maximum initial congestion win-
dow is between 10 and 12KB. For Akamai-113km servers,
79% and 19% of the servers have median initial congestion
window of 8KB and 5.5KB, respectively; the maximum ini-
tial congestion window is as large as 15KB. Google’s adop-
tion of large initial congestion window is even more aggres-
sive: 98% of the Google servers use a median initial conges-
tion window of 14KB (consistent with the advocated value
of 15KB [10]), and the largest initial congestion window can
be as large as 25KB. The rest of the servers (i.e., those other
than Akamai and Google servers) have not adopted large ini-
tial congestion window as aggressively. Speciﬁcally, 61% of
them use initial congestion window smaller than 4KB.

Large initial congestion window is particularly beneﬁcial
to short ﬂows that can be completed in one RTT when ini-
tial congestion window is large. The more aggressive ini-
tial congestion window adopted by Google servers leads to
its superior performance in serving G1 ﬂows: we observe
from Fig. 6(a) better performance from Google servers than
Akamai-113km servers at low percentiles despite that RTT
to Google servers tends to be larger than that to Akamai-
113km (RTT distributions for G1 ﬂows are similar to those
for G2 ﬂows, which is plotted in Fig. 7). Furthermore, a
smaller fraction of G1 ﬂows served by Google servers has
very low servicing rate, as shown in Fig. 6(a). Even for G2
ﬂows, we observe similar servicing rates between ﬂows served
by Akamai-113km and Google servers (Fig. 6(b)) despite of
the more dramatic diﬀerences in RTT (Fig. 7).

On the other hand, adopting larger initial congestion win-
dow can lead to congestion in the network. The study in [10]
shows that this is only true for slow network connections.
From our results, we do not observe signiﬁcantly diﬀerent
loss rates from the server clusters that adopt diﬀerent initial
congestion windows.

6.5 Advertised Receive Window

We ﬁnd that both iOS and Andriod operating systems
support window scaling. When connecting to servers that
support window scaling (e.g., we ﬁnd all Akamai and Google
servers support window scaling), the receive window adver-
tised by iOS devices is 128KB10, implying that the maxi-
mum window of a ﬂow is 128KB. In contrast to the static

10The windows scaling factor is 4, leading to receive window
of 256KB. However, we observe that the receive window is
reduced to 128KB at the end of three-way handshake.

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

1

 G1
 G2
 G3

10

100

1000

Maximum Window (KB)

(a) iOS

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

1

 G1
 G2
 G3

10

100

1000

Maximum Window (KB)
(b) Android

Figure 10: Distribution of maximum window size for
iOS and Android ﬂows served by Google servers.

large receive window advertised by iOS devices, Andriod
devices dynamically adjust receive window every round trip
time, starting from a fairly small size of 5KB. Furthermore,
since the receive window can grow dynamically, the maxi-
mum window size can be larger than 128KB.

The diﬀerent design choices adopted by iOS and Andriod
devices have the following implications. First, we ﬁnd the
large receive window of iOS devices cannot be fully utilized
by most ﬂows. To illustrate this, we plot the distribution of
maximum window size of iOS ﬂows that are served by Google
servers, as shown in Fig. 10(a). The reason for choosing
Google server cluster is that it serves the highest percentage
of video ﬂows among all the four server clusters (the percent-
age is 30.5%), and video ﬂows tend to be longer than other
types of ﬂows, and hence are more likely to reach large win-
dow size. From Fig. 10(a), we see that only 8% of G3 ﬂows
reach the maximum window limit of 128KB, and the per-
centages for G1 and G2 ﬂows are even lower, indicating that
the large receive window of 128KB could potentially cause
unnecessary waste of resources on iOS devices (whether it
wastes resources or not depends on the kernel memory allo-
cation mechanism: If the iOS kernel preallocates the mem-
ory for the socket, this causes waste of resources; otherwise,
it does not waste resources).

Secondly, the very small receive window adopted by An-
driod devices, while conserving resources, making Andriod
devices unable to take advantage of large initial congestion
window adopted by many servers (recall TCP congestion
window is the minimum of the sender and receiver window),
and hence can lead to inferior performance, particularly for
short ﬂows. As we shall see in Section 6.9, this is indeed the
case (Section 6.9 reports ﬁndings from the data set collected
in 2012, which contains more Andriod traﬃc and hence al-
lows us to draw more convincing statistical conclusions re-
garding Andriod traﬃc). The adverse eﬀect of small receive
window may be even more dramatic in cellular networks
where the round trip time can be signiﬁcantly larger than
that in WiFi networks.

Thirdly, since the receive window of Andriod devices is
adjusted dynamically, it can grow to large values. Fig. 10(b)
plots the distribution of maximum window size for Android
ﬂows that are served by Google servers. Indeed, we observe
9% of G3 ﬂows reach maximum window sizes larger than
128KB, while the maximum window size is bounded below
128KB for iOS ﬂows. We suspect that since the receive
window of Andriod devices can grow to large values, Android
devices can achieve better performance for very long ﬂows.

323Our dataset, however, does not contain suﬃcient number of
very long ﬂows from Android devices (even in the data set
that was collected in April 2012, which contains much more
traﬃc from Android devices) to statistically verify the above
conjecture.

Summarizing the above, we believe dynamic receive win-
dow adopted by Andriod devices is a suitable choice for re-
source limited MHDs. On the other hand, using very small
initial receive window (e.g., 5KB) is too conservative, which
can lead to inferior performance, particularly for short ﬂows.
How to choose receive window for MHDs to be both resource
eﬃcient and performance enhancing is beyond the scope of
this paper, but is an interesting problem that we leave as
future work.

6.6 Application-level Protocol

)

B
K

(
 
a

t

 

a
D
d
e
d
a
o
n
w
o
D

l

 20000
 18000
 16000
 14000
 12000
 10000
 8000
 6000
 4000
 2000

 0

 0  20  40  60  80 100 120 140 160 180

Time (s)

Figure 11: A series of 75 TCP ﬂows is used to
download one YouTube video when using the native
YouTube application on an iOS device. The ﬁgure
shows the starting and ending times of each TCP
ﬂow with the requested range. For better clarity,
we use diﬀerent colors (red and blue) to represent
two adjacent TCP ﬂows.

We ﬁnd that some application-level protocols for MHDs
are highly optimized for cellular networks, which may cause
ineﬃcient use of network and operating system resources of
MHDs in WiFi networks. The native YouTube player in iOS
is an example. When using this player, video contents are
downloaded in segments; each segment is requested in a sep-
arate TCP connection, using the HTTP Range header ﬁeld
to specify the requested portion of the video [13]. While it
has been reported in literature that multiple TCP connec-
tions are used to serve a single YouTube video [13, 26], we
ﬁnd, surprisingly, the number of TCP connections can be
extremely large. Fig. 11 shows an example where a series of
75 TCP ﬂows (most of them very short-lived) are used to
download a single YouTube video. Considering the overhead
of creating new TCP connections and the slow-start phase
at the beginning of a TCP connection, using so many short-
lived TCP ﬂows to serve a single video does not seem to be
sensible. We suspect this is a design choice that tries to cope
with TCP timeouts (e.g., caused by long delays originated
from handoﬀs, disconnections, and MAC-level retransmis-
sion) in cellular networks [8]. For example, the handoﬀ in
cellular network can take more than one second, which might

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

F
D
C

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

F
D
C

 img
 video
 txt

 img
 video
 txt

0.010 0.015 0.020 0.025 0.030 0.035 0.040

0.010 0.015 0.020 0.025 0.030 0.035 0.040

Application response time (s)

Application response time (s)

(a) MHD

(b) NHD

Figure 12: Application response time for MHD and
NHD ﬂows that are served by Google servers.

cause TCP timeout and reduced congestion window (in the
worst case, the congestion window might be reduced to one).
Therefore, it might make sense to open a new TCP connec-
tion to take advantage of large initial congestion window to
overcome the impact of the signiﬁcantly degraded through-
put.

We again ﬁnd the design choices taken by iOS and An-
droid devices are diﬀerent: Android devices use only one
TCP ﬂow to download a single YouTube video. How to
optimize application-level protocols for MHDs, considering
the characteristics of both cellular and WiFi networks, is an
interesting problem, but is beyond the scope of this paper.

6.7 Application Response Time

We investigate application response time for diﬀerent types
of content to understand whether servers use diﬀerent mech-
anisms based on content type (recall application response
time represents the delay from the ﬁrst GET message to the
ﬁrst responding data packet for an HTTP request). Fig. 12(a)
plots application response time distribution for three types
of contents, video, image and text, that are served by Google
servers. Most of the requested videos are YouTube videos,
which are destined to Google servers because Google has
migrated YouTube servers to its own network infrastructure
after acquiring YouTube [30]. We observe from Fig. 12(a)
that, perhaps surprisingly, videos have the lowest response
time, followed by images and text. The reason why videos
have the lowest response time might be that users tend to
watch popular videos (note that YouTube iOS application
has “Featured” and “Most Viewed” categories, convenient for
users to select and watch those popular videos) and servers
cache popular videos, leading to low response time. The
slowest response time for texts might be because most of
the requests for texts query search engines, which can take
a longer time to respond.

Interestingly, the results for NHDs (see Fig. 12(b)) dif-
fer from those for MHDs: for NHDs, the response times for
videos and texts are similar. We do not know the exact rea-
sons that cause the diﬀerence between MHDs and NHDs.
One observation is that the user interface for NHDs diﬀer
from that in MHDs: for NHDs, YouTube suggests related
videos based on a user’s preference (such as browsing his-
tory, local cache, etc.) while for MHDs, YouTube suggests
featured and most viewed videos. Further investigation is
left as future work.

In Fig. 12, we observe that the distribution of the response
times for image ﬂows that are served by Google contains

324F
D
C

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

F
D
C

0.9

0.8

0.7

0.6

0.5

0.4

0.3

0.2

0.1

0.0

 img
 video
 txt

 img
 video
 txt

0.015

0.020

0.025

0.030

0.035

0.040

0.015

0.020

0.025

0.030

0.035

0.040

RTT (s)

(a) MHD

RTT (s)

(b) NHD

Figure 13: RTT for MHD and NHD ﬂows that are
served by Google servers.

two modes. We conjecture that this is because the images
are served by two clusters of servers, one closer to UConn
than the other. Since Google data center information is
not disclosed to the public, we try to infer the Google server
locations through the RTT distribution [30]. Fig. 13 presents
the RTT distributions for the ﬂows with diﬀerent content
types. We indeed observe that the RTT distribution for
image ﬂows contains two modes, implying that they may
be served by two clusters of servers at diﬀerent geographic
locations.

6.8 Number of Concurrent TCP Flows

As stated in Section 5.4, the number of concurrent TCP
ﬂows might be an indicator of the amount of CPU and I/O
activities on an MHD device. We next present the rela-
tionship between the number of concurrent TCP ﬂows and
per-ﬂow servicing rate.

F
D
C

1.0

0.8

0.6

0.4

0.2

0.0

1

 MHD

 NHD

10

100

Number of Concurrent TCP Flows

Figure 14: Average number of concurrent TCP ﬂows
for G3 ﬂows served by Akamai-113km servers.

We ﬁnd that many G3 ﬂows have more than one concur-
rent TCP ﬂow. Fig. 14 plots the distribution of the aver-
age number of concurrent TCP ﬂows for G3 ﬂows served by
Akamai-113km servers (for comparison, the results for both
MHDs and NHDs are plotted in the ﬁgure). For MHDs,
over 90% of the G3 ﬂows have more than one concurrent
TCP ﬂow, and there can be tens of concurrent ﬂows. The
results for the other three server clusters are similar (ﬁgure
omitted). Intuitively, a larger number of concurrent ﬂows on
an MHD may lead to less resources to each ﬂow, and hence

lower per-ﬂow servicing rate. To verify this conjecture, we
obtain the correlation coeﬃcient between the average num-
ber of TCP ﬂows that are concurrent with a G3 video ﬂow
and the per-ﬂow servicing rate of the G3 video ﬂow. We ﬁnd
that the correlation coeﬃcients are -0.03, -0.4 and -0.29 for
G3 video ﬂows served by Akamai-Hartford, Akamai-113km
and Google servers, respectively. The negative values indi-
cate that indeed more concurrent ﬂows can lead to lower
per-ﬂow servicing rates. The correlation is more signiﬁcant
for video ﬂows served by Akamai-113km and Google. For
the ﬂows served by Akamai-Harford, the less signiﬁcant cor-
relation might be due to the superior performance achieved
by this server cluster, which makes the eﬀect of other factors
less visible.

For comparison, let us look at the distribution of the av-
erage number of concurrent TCP ﬂows for G3 NHD ﬂows
in Fig. 14. Not surprisingly, the number of concurrent TCP
ﬂows on NHDs can be much larger than that on MHDs.
On the other hand, we observe less signiﬁcant correlation
between the number of concurrent TCP ﬂows and per-ﬂow
servicing rates on NHDs:
in general, the negative correla-
tion coeﬃcient varies between -0.12 and -0.10 for the NHD
ﬂows served by the four server clusters. This less signiﬁcant
correlation may be due to superior computation and I/O
capabilities on NHDs.

6.9 Findings from the 2012 Data Set

In the previous sections, we have presented the results
from the data set collected in March 2011. We next report
the results from the data set collected in April 2012.

In this data set, we ﬁnd that only 64.7% of the TCP ﬂows
use HTTP, compared to 92.3% in the 2011 data set. On the
other hand, the percentage of HTTPS ﬂows has increased
from 4.3% to 25.3%. This sharp increase is caused by the
fact that many popular web sites such as Google, Facebook,
and Hotmail, add the functionality to access their services
using HTTPS, which protects users’ privacy especially when
they use public WiFi access points. For the rest of the TCP
ﬂows (11%), we do not observe any dominant application
protocol.

Since the percentage of HTTP ﬂows in the 2012 data set is
much lower than that in the 2011 data set, and our approach
to classifying MHD and NHD ﬂows cannot be applied to the
non-HTTP ﬂows (since it uses User-Agent ﬁeld in HTTP
headers), leaving a large percentage (36.3%) of the TCP
ﬂows in the 2012 data set not analyzed, we have focused on
the 2011 data set in this paper. We next describe the main
results from analyzing the HTTP ﬂows in the 2012 data set
(developing new approaches to identify the device types for
the non-HTTP ﬂows in the 2012 data set, and analyzing
their characteristics are left as future work). In the interests
of space, we only present the main ﬁndings that diﬀer from
those from the 2011 data set.

• We observe that more MHD and NHD ﬂows are served
by Akamai and Google servers. Speciﬁcally, 43.5% of
MHD ﬂows and 38.4% of NHD ﬂows are served by
these two sets of servers, respectively. The number of
distinct destination host domains accessed by MHDs
has increased signiﬁcantly to 9.3k (compared to 6k in
the 2011 data set), while for NHDs, the increase is
less dramatic (from 51k to 53k). The destination host
domains accessed by MHDs are still less diverse than
those accessed by NHDs. This is also evidenced by

325the observation that for MHDs, 38% of the ﬂows are
destined to the top ten host domains, while for NHDs,
the percentage is 29%.

• We ﬁnd that Google has deployed a set of servers (con-
taining 12 IP addresses) very close to UConn, in the
Connecticut Education Network at Hartford. Simi-
lar to the Akamai-Hartford server, this set of close-by
Google servers provide high per-ﬂow servicing rate due
to small RTTs. We ﬁnd 2.8% of MHD ﬂows are served
by these servers.
In addition, analyzing the content
type, we ﬁnd that they mainly serve Youtube traﬃc
(account for 45% of the MHD ﬂows from these servers).

1.0

0.8

F
D
C

0.6

0.4

0.2

0.0

10

 iOS-G1
 Android-S-G1
 Android-L-G1

100

1000

10000

Per-flow Servicing Rate (Kbps)

Figure 15: Distribution of per-ﬂow servicing rate of
G1 iOS and Android ﬂows that are served by Google
servers from the 2012 data set, where Android-S-G1
corresponds to Andriod ﬂows using small initial re-
ceive window (5KB) and Android-L-G1 corresponds
to Andriod ﬂows using large initial receive window
(14KB).

• The amount of Android MHD ﬂows has increased to
10% (compared to 3.2% in the 2011 data set), indi-
cating the increasing popularity of Android devices.
In addition, we observe 12% of the Android ﬂows start
with a signiﬁcantly larger advertised receive window of
14KB, compared to the small initial receive window of
5KB that is observed predominantly in the 2011 data
set. This larger initial receive window was adopted by
newer versions of Android OS (starting from Android
OS 4.X), and allows Android devices to better utilize
the larger initial congestion window adopted by many
servers. Fig. 15 plots the distribution of per-ﬂow ser-
vicing rate of iOS and Andriod ﬂows that are served
by Google servers. We separate the Android ﬂows into
two groups, one using initial receive window of 5KB,
and the other using initial receive window of 14KB. We
only plot the results for G1 ﬂows; the results for G2
and G3 ﬂows are consistent. Fig. 15 shows that iOS de-
vices outperform Android devices that use small initial
receive window, a point that we made in Section 6.5.
On the other hand, the performance of Android de-
vices that have adopted large initial receive window is
comparable to that of iOS devices.

7. DISCUSSION

Our study has been conducted in a speciﬁc WiFi network,
UConn campus WiFi network. A natural question is what
ﬁndings from our study are speciﬁc to UConn network and
what are applicable to other WiFi networks.

• Our ﬁndings related to hardware and software of MHDs

and application-level protocols are not speciﬁc to UConn
network, and hence we believe that they are equally
applicable to other WiFi networks.

• The content delivery infrastructures in other networks
may diﬀer signiﬁcantly from that perceived by UConn
network. On the other hand, we believe MHDs in other
networks also use Akamai and Google servers heavily
because the most popular services accessed by MHDs
are provided by major companies such as Facebook,
Google, and Apple that use Akamai and Google servers
heavily. The extent of usage and whether the usage by
MHDs is heavier than that by NHDs in other networks,
however, depend on the popularity of the applications
in other networks.

• The use of Akamai and Google servers can also boost
the network performance of MHDs in other networks
in the US due to small RTTs provided by these servers.
Speciﬁcally, the study of [15] reports that Akamai tends
to deploy CDN sites close to the end users, and among
all of the Akamai services, the 10th percentile and me-
dian of delays to Akamai are around 10ms and 20ms,
respectively (these delays are comparable to the me-
dian delays of 8.5ms and 20ms from UConn campus
to the two closest Akamai sites); Google is reported to
provide similar RTT performance to most of the users
in the US [27].

• The loss rates and RTTs in other networks may be sig-
niﬁcantly larger than those in UConn network. This
implies that local losses and RTTs may play a signif-
icant role in network performance of MHDs in other
networks. In addition, large initial congestion window
adopted by many servers may not be strictly beneﬁcial
in other networks.

8. CONCLUSION AND FUTURE WORK

In this paper, we have studied the network performance of
MHDs inside UConn campus network. We ﬁnd that, com-
pared to NHDs, MHDs use well provisioned Akamai and
Google servers more heavily, which boosts the overall net-
work performance of MHDs. Furthermore, MHD ﬂows, par-
ticularly short ﬂows, beneﬁt from the large initial conges-
tion window that has been adopted by Akamai and Google
servers. Secondly, MHDs tend to have longer local delays
inside the WiFi network and are more adversely aﬀected by
the number of concurrent ﬂows. Thirdly, Android OS can-
not take advantage of the large initial congestion window
adopted by many servers, while the large receive window
adopted by iOS is not fully utilized by most ﬂows, leading
to waste of resources. Last, some application-level proto-
cols cause ineﬃcient use of network and operating system
resources of MHDs in WiFi networks. Our observations pro-
vide valuable insights on content distribution, server provi-
sioning, MHD system design, and application-level protocol
design.

326As future work, we plan to use active controlled experi-
ments to understand why local RTTs of MHDs tend to be
larger than those on NHDs, and to understand the bot-
tleneck(s) of MHD ﬂows. We also plan to study network
performance of MHDs in other public WiFi networks. For
instance, we believe that WiFi hotspots (e.g., hotspots in
Starbucks) and other campus WiFi networks might have
diﬀerent network characteristics (e.g., higher loss rates and
higher RTTs) as well as diﬀerent content popularity among
users. Furthermore, the content delivery infrastructure may
diﬀer signiﬁcantly from that perceived by UConn network.
Quantifying the network performance of MHDs and iden-
tifying the performance limiting factors in a wide range of
settings will provide us better insights on designing network
services for MHDs. Last, we plan to study how the perfor-
mance of 3G cellular network diﬀers from that of WiFi in
our campus network and when/why users on campus switch
from one network interface to another network interface.

Acknowledgement
The work was supported in part by NSF CAREER Award
0746841. We thank J. Farese, R. Kocsondy, J. Pufahl, and S.
Maresca (UConn) for their help with the monitoring equip-
ment and their assistance in gathering the data. We thank
Ilhwan Kim and Jaewan Jang (NHN) and D. Xuan (Ohio
State University) for helpful discussions. We also thank the
anonymous reviewers for their insightful comments, and our
shepherd A. Balasubramanian for many helpful suggestions.

9. REFERENCES
[1] Alexa. http://www.alexa.com.
[2] DAG card. http://www.endace.com.
[3] HTTP Pipelining.

http://en.wikipedia.org/wiki/HTTP pipelining.

[4] M. Allman, S. Floyd, and C. Partridge. Increasing

TCP’s initial window, 2002. RFC 3390.

[5] N. Balasubramanian, A. Balasubramanian, and

A. Venkataramani. Energy consumption in mobile
phones: a measurement study and implications for
network applications. In Proc. of ACM IMC, 2009.
[6] N. Banerjee, A. Rahmati, M. D. Corner, S. Rollins,
and L. Zhong. Users and batteries: Interactions and
adaptive energy management in mobile systems. In
Proc. of ACM Ubicomp, 2007.

[7] X. Chen, B. Wang, K. Suh, and W. Wei. Passive

online wireless LAN health monitoring from a single
measurement point. ACM Mobile Computer Comm.
Review, 14, November 2010.

[8] C. M. Choon and R. Ram. Improving TCP/IP

performance over third-generation wireless networks.
IEEE Transactions on Mobile Computing, 2008.

[9] P. Deshpande, X. Hou, and S. R. Das. Performance

comparison of 3G and metro-scale WiFi for vehicular
network access. In Proc. of ACM IMC, 2010.

[10] N. Dukkipati, T. Reﬁce, Y. Cheng, J. Chu,

T. Herbert, A. Agarwal, A. Jain, and N. Sutin. An
argument for increasing TCP’s initial congestion
window. ACM SIGCOMM CCR, 40:26–33, June 2010.

[11] H. Falaki, D. Lymberopoulos, R. Mahajan,

S. Kandula, and D. Estrin. A ﬁrst look at traﬃc on
smartphones. In Proc. of ACM IMC, 2010.

[12] H. Falaki, R. Mahajan, S. Kandula,

D. Lymberopoulos, R. Govindan, and D. Estrin.
Diversity in smartphone usage. In Proc. of ACM
MobiSys, 2010.

[13] A. Finamore, M. Mellia, M. Munafo, and S. G. Rao.

YouTube everywhere: Impact of device and
infrastructure synergies on user experience. In Proc. of
ACM IMC, 2011.

[14] A. Gember, A. Anand, and A. Akella. A comparative
study of handheld and non-handheld traﬃc in campus
WiFi networks. In Proc. of PAM, 2011.

[15] C. Huang, A. Wang, J. Li, and K. W. Ross. Measuring

and evaluating large-scale CDNs. Technical Report
MSR-TR-2008-106, Microsoft Research, 2008.

[16] J. Huang, Q. Xu, B. Tiwana, Z. M. Mao, M. Zhang,

and P. Bahl. Anatomizing application performance
diﬀerences on smartphones. In Proc. of ACM Mobisys,
2010.

[17] IP2Location. http://www.ip2location.com.
[18] R. Jain. The art of computer systems performance

analysis - techniques for experimental design,
measurement, simulation, and modeling. Wiley
professional computing. Wiley, 1991.

[19] S. Jaiswal, G. Iannaccone, C. Diot, J. Kurose, and

D. Towsley. Inferring TCP connection characteristics
through passive measurements. In Proc. of IEEE
INFOCOM, 2004.

[20] G. Maier, A. Feldmann, V. Paxson, and M. Allman.
On dominant characteristics of residential broadband
Internet traﬃc. In Proc. of ACM IMC, 2009.

[21] G. Maier, F. Schneider, and A. Feldmann. A ﬁrst look

at mobile hand-held device traﬃc. In Proc. of PAM,
2010.

[22] A.-F. Mohammad, E. Khaled, R. Benjamin, and

G. Igor. Overclocking the Yahoo!: CDN for faster web
page loads. In Proc. of ACM IMC, 2011.

[23] F. Qian, A. Gerber, Z. M. Mao, S. Sen, O. Spatscheck,
and W. Willinger. TCP revisited: a fresh look at TCP
in the wild. In Proc. of ACM IMC, 2009.

[24] M.-R. Ra, J. Paek, A. B. Sharma, R. Govindan, M. H.

Krieger, and M. J. Neely. Energy-delay tradeoﬀs in
smartphone applications. In Proc. of ACM MobiSys,
2010.

[25] A. Rahmati and L. Zhong. Human battery interaction

on mobile phones. Elsevier Pervasive and Mobile
Computing Journal, 5(5), 2009.

[26] A. Rao, Y.-S. Lim, C. Barakat, A. Legout, D. Towsley,

and W. Dabbous. Network characteristics of video
streaming traﬃc. In Proc. of ACM CoNext, 2011.

[27] K. Rupa, M. H. V., S. Sridhar, J. Sushant, K. Arvind,

A. Thomas, and G. Jie. Moving beyond end-to-end
path information to optimize CDN performance. In
Proc. of ACM IMC, 2009.

[28] A. Shane and N. Richard. Application ﬂow control in

YouTube video streams. SIGCOMM Computer
Comm. Review, 41, 2011.

[29] A. Shye, B. Sholbrock, and G. Memik. Into the wild:

Studying real user activity patterns to guide power
optimization for mobile architectures. In Proc. of
IEEE/ACM MICRO, 2009.

[30] R. Torres, A. Finamore, J. Kim, M. Mellia,

327M. Munafo, and S. Rao. Dissecting video server
selection strategies in the YouTube CDN. In Proc.
IEEE ICDCS, 2011.

[33] D. Zhang. Web content adaptation for mobile

handheld devices. Communications of the ACM, 50(2),
February 2007.

[31] I. Trestian, S. Ranjan, A. Kuzmanovic, and A. Nucci.

[34] Z. Zhuang, K.-H. Kim, and J. P. Singh. Improving

Measuring serendipity: connecting people, locations
and interests in a mobile 3G network. In Proc. of
ACM IMC, 2009.

[32] F. P. Tso, J. Teng, W. Jia, and D. Xuan. Mobility: a
double-edged sword for HSPA networks: a large-scale
test on Hong Kong mobile HSPA networks. In Proc. of
ACM MobiHoc, 2010.

energy eﬃciency of location sensing on smartphones.
In Proc. of ACM MobiSys, 2010.

328