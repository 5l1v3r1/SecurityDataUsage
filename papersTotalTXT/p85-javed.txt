Detecting Stealthy, Distributed SSH Brute-Forcing

Mobin Javed† and Vern Paxson†(cid:2)

†University of California, Berkeley

(cid:2)International Computer Science Institute

Abstract
In this work we propose a general approach for detecting dis-
tributed malicious activity in which individual attack sources each
operate in a stealthy, low-proﬁle manner. We base our approach on
observing statistically signiﬁcant changes in a parameter that sum-
marizes aggregate activity, bracketing a distributed attack in time,
and then determining which sources present during that interval
appear to have coordinated their activity. We apply this approach
to the problem of detecting stealthy distributed SSH bruteforcing
activity, showing that we can model the process of legitimate users
failing to authenticate using a beta-binomial distribution, which en-
ables us to tune a detector that trades off an expected level of false
positives versus time-to-detection. Using the detector we study the
prevalence of distributed bruteforcing, ﬁnding dozens of instances
in an extensive 8-year dataset collected from a site with several
thousand SSH users. Many of the attacks—some of which last
months—would be quite difﬁcult to detect individually. While a
number of the attacks reﬂect indiscriminant global probing, we also
ﬁnd attacks that targeted only the local site, as well as occasional
attacks that succeeded.

Categories and Subject Descriptors
K.6.5 [Computing Milieux]: MANAGEMENT OF COMPUT-
ING AND INFORMATION SYSTEMS—Security and Protection

Keywords
Scanning; SSH; Brute-forcing; Distributed

1.

INTRODUCTION

A longstanding challenge for detecting malicious activity has
been the problem of how to identify attacks spread across numer-
ous sources, such that the individual activity of any given source
remains modest, and thus potentially not particularly out of the or-
dinary. These scenarios can arise whenever a detector employs a
threshold used to ﬂag that a given candidate attack source has ex-
hibited a suspiciously high level of activity (e.g., when conducting
scanning or DoS ﬂooding). Attackers can respond to such detection

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright 2013 ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516719.

procedures by employing multiple sources in order to thin out their
activity to prevent any single source from exceeding the threshold;
their attack becomes distributed and therefore potentially stealthy,
i.e., hard to detect based on any individualized analysis.

In this work we present a general strategy for potentially de-
tecting such stealthy activity, which consists of two basic steps.
First, we employ the statistical technique of change-point detec-
tion to identify times during which a global property has shifted—
indicating that, in aggregate, a site’s activity reﬂects the pres-
ence of problematic activity. We then determine the range of time
over which this activity occurred and, within that interval, identify
which sources appear to have contributed to the activity.

In particular, we apply this approach to the problem of detect-
ing distributed SSH brute-forcing: attackers employing a number
of systems that each try different username/password combinations
against a site’s SSH login servers, hoping that one of them will
stumble across a working combination made possible by a careless
user. The threat of SSH brute-forcing is well-known: indeed, any
SSH server open to general Internet access receives incessant prob-
ing by hostile remote systems that energetically attempt to locate
instances of weak authentication [5]. The degree to which such at-
tempts also occur in a stealthy slow-but-steady fashion, however,
has attracted little study. The difference between single energetic
probes and stealthy distributed ones is signiﬁcant: defenders can
easily detect the former, and therefore either block the activity or
investigate it (to ensure none of the attempts succeeded). The lat-
ter, however, poses a much more difﬁcult detection problem.
If
each host in a distributed brute-forcing attack itself only attempts
username/password logins at a low rate, then distinguishing hos-
tile activity from the inevitable login failures made by legitimate
user errors becomes much more difﬁcult. Yet the distinction is vi-
tal: a pattern of attempt/attempt/attempt/success made by a legit-
imate user simply reﬂects a set of typos, or a password that took
a few stabs to remember; but by a distributed SSH brute-forcer, it
provides the only slender indication of success amongst a mass of
probing that in aggregate predominantly failed.

We aim to both provide an exemplar of our general strategy in
terms of detecting distributed (but coordinated) SSH brute-forcing
attacks, as well as developing an assessment of the prevalence of
such attacks as seen over years of data. In terms of our two-step
approach, we ﬁrst identify attack epochs during which in aggre-
gate we can with statistical conﬁdence determine that some sort
of SSH brute-forcing event occurred. Here, we employ change-
point detection framed in terms of a parameter that summarizes
the network/server activity of groups of remote hosts—in particu-
lar, the aggregate login failure rate. Our second step classiﬁes the
hosts appearing during the detected epochs as either participants or
non-participants in the activity, based on both individual past his-

85tory and “coordination glue”, i.e., the degree to which a given host
manifests patterns of probing similar to that of other hosts during
the epoch.

We develop and evaluate our detector on 8 years of SSH login
records collected via central syslogging at the Lawrence Berkeley
National Laboratory, a large (≈ 4,000 employees and visitors) re-
search facility. We measure and quantify the duration, intensity
and behavior of the detected attacks. We ﬁnd multiple large-scale
coordinated attacks from botnets, the longest one spanning about
1.5 months. All the attacks we detect would have been completely
missed by a point-wise host-based detector. We correlate these at-
tacks with data from several other sources, ﬁnding that half of the
large-scale incidents at the site are part of global attacks, with a sig-
niﬁcant average overlap of ≈ 70% attack hosts appearing at multi-
ple sites in the same time span.

We organize the rest of the paper as follows. We begin with
related work in § 2. § 3 details the characteristics of the dataset
we use in developing and evaluating our detector. § 4 frames our
detection approach. In § 5 we develop a model of the process by
which legitimate users make authentication errors when attempting
to log in, which serves as the basis for parameterizing our SSH
password brute-force detector. We discuss our evaluation results
and ﬁndings in § 6, and summarize in § 7.

2. RELATED WORK

The literature relevant to our work lies in three domains: (i) coor-
dinated attack detection, (ii) SSH brute-force attack detection, and
(iii) studies of the prevalence of SSH brute-forcing activity.

The detection of coordinated attacks has received little treatment
in the literature. The earliest work of which we are aware is that
of Staniford et al., who correlate anomalous events using simulated
annealing for clustering [17]. Gate’s work on coordinated scan de-
tection is the most prominent subsequent effort in this domain [8].
Given an input set of scan sources, Gate’s algorithm extracts the
subset of hosts that appear coordinated by using a set-covering ap-
proach; the premise is that the attacker divides the work among
the coordinating scanning hosts in a manner that maximizes in-
formation gain while minimizing work overlap. For our purposes
this work has two limitations: (i) the individual attack hosts re-
quire pointwise identiﬁcation, and thus the approach will not ﬁnd
stealthy attacks, and (ii) the algorithm lacks a procedure for deter-
mining when a site is under attack. Other work has addressed the
somewhat similar problem of detecting DDoS attacks, but these de-
tection approaches face a difﬁcult problem of how to differentiate
attack participants from legitimate users [19, 16].

With regard to SSH brute-forcing, host-based detection tech-
niques such as DenyHosts [2], BlockHosts [1], BruteForce-
Blocker [9], fail2ban [12], and sshguard [3] block hosts that cross
a threshold for failed attempts in a speciﬁed amount of time. Other
work has developed network-based approaches. Kumagai et al.
propose an increase in the number of DNS PTR record queries
to detect SSH dictionary attacks [13]. This increase results from
the SSH server logging the fully qualiﬁed domain names of the
SSH clients attempting access. This work does not discuss how
to establish detection thresholds, nor does it present an evaluation
of the system’s accuracy. Vykopal et al. develop ﬂow signatures
for SSH dictionary attacks [18]. They show that a large number
of short ﬂows having a few bytes transferred in both directions
and appearing together in a short duration of time are indicative
of failed login attempts, providing the means to then detect brute-
force attacks from ﬂow data. Hellemons also studied the possibility
of using only ﬂow data to detect SSH brute-force attacks, model-
ing the brute-force attacks as consisting of three phases: scanning,

brute-force and die-off (in case of successful compromise) [11].
They monitor the ranges of three parameters—ﬂows-per-second,
packets-per-ﬂow and bytes-per-packet—to identify these phases.
Both of these works test their detectors only on simulated dictio-
nary attacks, and do not address how to distinguish instances of
forgotten usernames/passwords from brute-forcers. More gener-
ally, none of these brute-force detection approaches have the ability
to detect stealthy coordinated attacks.

Malecot et al. use information visualization techniques to de-
tect distributed SSH brute-force attacks [14]. For each local host,
the remote IP addresses that attempt to log in are displayed using
a quadtree—a tree data structure formed by recursively subdivid-
ing two dimensional space into four quadrants. The procedure per-
forms 16 iterations to map 32-bit IP addresses onto a quadtree, each
time deciding the next sub-quadrant by looking at the next two bits
of the IP address. The analyst then visually compares quadtrees for
different local hosts to identify as coordinated attackers remote IP
address(es) that appear in quadtrees of multiple local hosts.

Finally, regarding the prevalence of SSH brute-force attacks,
Bezut et al. studied four months of SSH brute-force data collected
using three honeypot machines [6]. They ﬁnd recurring brute-
forcing activity, sometimes with several weeks in between, indicat-
ing that the attacks target a wide range of IP address space. Owens
et al. performed a measurement study of SSH brute-force attacks
by analyzing data from honeypots on three networks—a small busi-
ness network, a residential network, and a university network—for
eleven weeks during 2007–2008 [15]. They ﬁnd that the number of
login attempts during different attacks varied from 1 or 2 to thou-
sands. More than a third of the attacks consisted of ten or fewer
login attempts. They ﬁnd instances of both slow and distributed at-
tacks designed to evade detection. They also ﬁnd that precompiled
lists of usernames and passwords are shared across different attack-
ers, identifying ﬁve such dictionaries. Their study reveals that only
11% of the attempted passwords are dictionary words.

3. DATASETS AND DATA FILTERING

We evaluate our detector on eight years of SSH login data col-
lected at the Lawrence Berkeley National Laboratory (LBNL), a
US national research laboratory. The temporal breadth of this
dataset allows us to study attack patterns at the site across the years.
We also draw upon SSH datasets from four other sites spread across
the IP address space (and several geographic locations) to assess
whether attacks we detect at LBNL reﬂect targeted behavior or in-
discriminant probing. We refer to these sites as HONEY, RSRCH-
LAB, HOMEOFF, and CAMPOFF, and describe them below. In this
section we present these datasets and discuss ways in which we
ﬁltered the data for our subsequent analysis.

3.1 Main dataset

Table 1 provides summary statistics for our main dataset, LBNL.
This site’s systems primarily reside in two /16 address blocks (from
two separate /8’s). Only a small fraction of the address space runs
externally accessible SSH servers, providing access to both individ-
ual user machines and compute clusters. The benign SSH activity
in this data consists of interactive as well as scripted logins.

For this site we have datasets collected at two vantage points:
(i) logs collected by a central syslog server that records informa-
tion about login attempts reported by (most of) the SSH servers,
and (ii) ﬂow data for SSH port 22 collected by border monitoring.
For each login attempt, the syslog data provides the time, client

86Time span
SSH servers
Valid users
Distinct valid user/server pairs
Login attempts
Login successes
Remote clients
Attempts using passwords

successes
remote clients
SSH border ﬂows

remote clients seen in ﬂows

High-rate brute-forcers
Mean attempts per high-rate brute-forcer
Mean daily password login attempts
Mean daily users

Jan 2005–Dec 2012
2,243
4,364
10,809
12,917,223
8,935,298
154,318
5,354,833
1,416,590
119,826
215,244,481
140,164
7,476
382.84
486.13 (σ = 182.95)
116.44 (σ = 32.41)

0

.

1

F
D
C
E

8

.

0

6

.

0

1

2

5

10

20

50

100

Number of failed login attempts

Figure 1: Empirical CDF of the number of failed login attempts per hour
until a success for legitimate user login efforts with forgotten or mistyped
usernames/passwords.

Table 1: Summary of LBNL syslog and ﬂow data.

and server1 IP addresses, username on the server, whether the login
succeeded, and the authentication type used. The ﬂow data sup-
plements this perspective by providing contact information (but no
details) for attempts to access IP addresses that do not run an SSH
server, or that run an SSH server that does not log via the central
syslog server. This data thus enables us to establish the complete2
set of machines targeted by an attack.

Filtering. For the central syslog data, we work with the subset
of SSH authentication types vulnerable to brute-forcing (i.e., we
omit those using public key authentication), about half of the at-
tempts. We perform all of the characterizations and analyses in the
remainder of the paper in terms of this subset.

In addition, we ﬁlter this dataset to remove individual brute-
forcers that we can readily detect using a per-host threshold for the
number of failed login attempts per remote host within a window
of time. Given the ease of detecting these brute-forcers, they do not
reﬂect an interesting problem for our detector, though they would
heavily dominate the data by sheer volume if we kept them as part
of our analysis.

To identify and remove such brute-forcers, we need to empiri-
cally establish reasonable thresholds for such a per-host detector.
We do so by analyzing the process by which legitimate users make
password authentication failures, as follows. We assume that any
user who makes repeated failed login attempts followed by a suc-
cessful attempt reﬂects a legitimate user.
(This assumption may
allow consideration of a few successful SSH brute-forcers as “le-
gitimate”, but these are very rare and thus will not skew the results.)
Figure 1 plots the number of failed attempts such users make
prior to ﬁnally succeeding. We see that instances exceeding
10 failed attempts are quite rare, but do happen occasionally. Ac-
cordingly, we consider 20 failed attempts as constituting a conser-
vative threshold. We manually analyzed the instances of legitimate
users falling after this cutoff (the upper right tail in the ﬁgure) and
found they all reﬂect apparent misconﬁgurations where the user
evidently set up automation but misconﬁgured the associated pass-
word. Thus, we deem any client exhibiting 20 or more failures
logging into a single server (with no success) over a one-hour pe-
riod with as a high-rate brute-forcer, and remove the client’s entire

1 Some of the syslog records log the server’s hostname rather than
its IP address. For these we correlated the records against the site’s
DNS and DHCP logs to resolve to IP addresses.
2 The ﬂow data has some gaps, though we have it in full for each
attack we identiﬁed. These gaps are the source of observing fewer
“remote clients seen in ﬂows” than “Remote clients” in Table 1.

F
D
C
E

0

.

1

8

.

0

6

.

0

4

.

0

2

.

0

0

.

0

1

5

10

50

500

Number of attempts/users

Figure 2: Empirical CDFs for benign password-based SSH usage in LBNL
data. Left to right: (i) valid users per hour, (ii) successful logins per hour,
(iii) valid users per day, (iv) successful attempts per day.

activity from our dataset. Table 1 summarizes the brute-forcers re-
moved using this deﬁnition.

Finally, to give a sense of the volume of activity that remains af-
ter these ﬁltering steps, Figure 2 shows the empirical CDF of the
hourly and daily numbers of successful logins. A typical day sees
500 successful logins (maximum seen: 1,200) involving 117 dis-
tinct users (maximum: 197).
3.2 Correlation datasets

The HONEY dataset reﬂects ﬁve manually identiﬁed SSH brute-
forcing “campaigns” (our term for ongoing instances, as discussed
later) as captured by 2 SSH honeypot servers in Norway [4]. Ta-
ble 2 summarizes these campaigns, which for the most part we
characterize as large-scale, persistent, and stealthy. For all but the
last campaign, many of the remote clients would evade detection
by our simple per-host detector.

The RSRCHLAB dataset reﬂects ﬂow data from the International
Computer Science Institute, a research organization in Berkeley,
CA, with a /23 address block. The dataset spans the same time as
that of LBNL, though due to the limitations of ﬂow data, we cannot

Attack Episode

Oct 2009–Jan 2010
Jun 2010–Aug 2010

Oct 2011
Nov 2011
Apr 2012

Days
78
56
6
13
6

Remote
clients
4,158
5,568
338
252
23

Login
attempts
44,513
23,009
4,773
4,903
4,757

Avg. attempts
per remote client
10 (σ=24)
4 (σ=7)
14 (σ=16)
20 (σ=24)
206 (σ=760)

Table 2: Summary of attacks in the HONEY data.

8788than real-valued. We do so for our subsequent development of the
detector.

We then accumulate Zn over time using the following (again
discrete) test statistic: Sn = max(0, Sn−1 + Zn), where S0 = 0.
In the case of no change, the value of Sn hovers around zero, but
in the face of a change (increase), Sn starts to accumulate in the
positive direction.

By convention, one terms the situation of the mean correspond-
ing to normality as in-control. When the mean shifts by an amount
Δμ, one terms the situation out-of-control, which corresponds to
an attack in our problem domain. Note that the choice of Δμ is
speciﬁc to the problem we design the detector to detect. In some
situations, we might desire a small Δμ, while in others we might
only have interest in detecting changes corresponding to a large
value of Δμ. In practical terms, we achieve a given target Δμ by
setting two detector parameters, k and H, as discussed below.

The algorithm ﬂags an out-of-control situation when Sn crosses
an operator-set threshold, H. The subsequent appearance of an
event with normal mean marks the return of the situation to in-
control, and we reset the test statistic Sn to zero at this point. Thus,
the CUSUM detector decides whether the mean has shifted or not
according to the decision function Dn:

(cid:2)

Dn =

1,
0,

if Sn > Sn−1 and Sn > H
otherwise.

Determining CUSUM parameters and span of change. One
tunes the parameters k and H of CUSUM based on: the amount of
change Δμ to detect, the desired false alarm rate, and the desired
time to detection. First, a general rule of thumb when designing
a CUSUM detector to detect a mean shift of Δμ is to set k equal
to half the change in shift [10]. The other parameter, H, controls
both the false alarm rate and the detection speed. A lower H means
faster detection but a higher false alarm rate.

To assess the balance between these two, we consider the effects
of H on the average number of steps the CUSUM detector takes
to raise an alarm under in-control and out-of-control distributions.
(Note that the ﬁrst of these corresponds to alarms reﬂecting false
positives, while the latter corresponds to true positives.) We refer to
these as in-control ARL (Average Run Length) and out-of-control
ARL, respectively, and choose the value of H that results in the
closest match with the desired ARLs.

To determine these ARLs, we can model the CUSUM process as
a Markov chain with ﬁnite states X0, X1, . . . , XH, corresponding
to the test statistic values Sn ≤ 0, Sn = 1, Sn = 2, . . . , Sn ≥ H
(Recall that we constrain Z and thus S to discrete
respectively.
integer values.) Note that XH is the absorbing state. The transition
probabilities of this Markov chain depend only on the underlying
distribution of the random variable Z:

P [Xi → X0] = P [Z ≤ −i]
P [Xi → Xj] = P [Z = j − i]
P [Xi → XH ] = P [Z ≥ H − i]

For the intuition behind this formulation, consider the ﬁrst equa-
If the cumulative sum has reached i (i.e., Sn = i, corre-
tion.
sponding to the state Xi) then the possible ways for progressing
from it to the state X0 (i.e., Sn ≤ 0) are to add a value of Z less
than or equal to −i. A similar perspective holds for the other two
equations. Given the complete transition probability matrix R of
the Markov chain, we can compute the above probabilities and the
in-control ARL as:

in-control ARL = (I − R)−11

where R is the transition probability matrix, I is the (H + 1) ×
(H + 1) identity matrix, and 1 the (H + 1) × 1 matrix of ones [7].
We can likewise compute the out-of-control ARL of the detector
using the same formulation but substituting k(cid:2) = k − Δμ [10].
We can then estimate the point of the true start of a change by
subtracting the value of out-of-control ARL (detection delay) from
the time of detection.

Finally, the Aggregate Site Analyzer reports the information from
CUSUM in the form of attack epochs. An attack epoch constitutes
of: (i) the set of consecutive out-of-control events (i.e., i = 1 . . . n
where Di = 1), and (ii) the set of previous events also incorporated
into the epoch based on stepping back through the number of events
given by the out-of-control ARL.

Each attack epoch can reﬂect instances of either singleton or co-
ordinated attacks. The ﬁrst of these corresponds to a global pertur-
bation of the site-wide variable Y induced by a single source. The
second refers to the perturbation arising due to the combined ac-
tion of multiple sources. Since in this work we focus on distributed
attack epochs, we need at this point to exclude singleton attacks.3
We do so by checking whether CUSUM still ﬂags any events in
the epoch as reﬂecting an attack even if we remove the remote host
with the highest number of failed login attempts. If so, we mark the
attack epoch as a coordinated attack epoch, and proceed to the sec-
ond component of our analysis. Otherwise, we discard the epoch
as uninteresting (which occurred about 3/4s of the time).
4.2 Attack Participants Classiﬁer

The second component of our general detection approach ad-
dresses how to go from the global site-wide view to that of indi-
vidual entities. Here we employ a set of heuristics to analyze ac-
tivity in the attack epochs ﬂagged by the Aggregate Site Analyzer
to identify who participated in the attack. (The need for heuristics
rather than more principled identiﬁcation arises almost fundamen-
tally from the problem domain: if we could directly identify partic-
ipants with conﬁdence, we could very likely use the same approach
to develop an effective pointwise detector and not have to employ
a separate approach for detecting stealthy distributed activity in the
ﬁrst place.)

For our particular problem of detecting distributed SSH brute-
force attacks, the individual entities we wish to identify are re-
mote hosts (clients). In addition to the problem of including re-
mote hosts corresponding to legitimate users within it, a distributed
attack epoch—particularly if spanning a long period of time—can
capture multiple brute-forcers, some of whom might operate in a
coordinated fashion, while others might function independently.
For example, an attack epoch we detect that includes activity from
ﬁve remote hosts might in fact be composed of four coordinating
remote hosts and one singleton brute-forcer that happens to probe
the site at the same time.

For each remote host that appears during the attack epoch, we
make a decision about whether to classify it as a legitimate re-
mote host, a singleton brute-forcer (operating alone), or a brute-
forcer working with other hosts as part of a coordinated attack.
This decision might require manual analysis, as sometimes the cat-
egories have signiﬁcant overlap. To illustrate, Figure 4 diagrams
the characteristics that remote hosts in each of these categories can
manifest. Legitimate users that fail due to forgotten or mistyped
usernames/passwords generally exhibit only a modest number of
attempts, similar to low-rate distributed brute-forcers. A remote

3 Note that such single sources can arise even though we previously
ﬁltered out high-rate brute-forcers (per § 3.1) because these single-
tons might spread their activity across multiple servers, or probe at
a rate lower than the 20 failures/hour threshold.

899091instead used a beta-binomial distribution. We see from the inset
that the actual data exhibits more variance than we can capture us-
ing a binomial model. The beta-binomial model provides a sig-
niﬁcantly better ﬁt as it allows for an extra variance factor termed
over-dispersion. Beta-binomial is the predictive distribution of a
binomial random variable with a beta distribution prior on the suc-
cess probability, i.e., k ∼ Binomial(p, n) where p ∼ Beta(α, β).
Then for a given n, α and β, we have:

n
k

Beta(k + α, n − k + β)

Beta(α, β)

(cid:3)

(cid:4)

k ∼

We can interpret the success of this ﬁtting in terms of lack of in-
dependence. If all login attempts were IID, then we would expect
to capture GFI effectively using a binomial distribution. The need to
resort to a beta-binomial distribution indicates that the random vari-
ables lack independence or come from different distributions, such
that the probability of success has a beta-prior instead of being con-
stant. This makes sense intuitively because (i) different users will
have different probabilities of success, and (ii) the login attempts
from a single user are not independent: one failed login attempt
affects the probability of success of the next login attempt (neg-
atively if the user has forgotten their password, positively if they
simply mistyped it).

6. EVALUATION

In this section we apply our detection procedure to the extensive
LBNL dataset. We discuss parameterizing the detector, assess its
accuracy, and characterize the attacks it ﬁnds, including whether
the attacks appear targeted or indiscriminant.
6.1 Parameterization

Our procedure ﬁrst requires selecting a mean shift Δμ that we
wish to detect. We set this to 10 failed logins per event of 100 lo-
gins, basing our choice on the stealthiest attack we wish to de-
tect without overburdening the analyst. On average this site sees
500 logins per day, so a threshold of Δμ = 10 bounds the number
of attempts a brute-forcer can on average make without detection
to 45 (9 attempts × 5 events) spread over a day. Fitting our beta-
binomial distribution (§ 5) to the 2005–2008 data yields the param-
eters μ = 7 and σ = 4.24, and so our chosen value corresponds
to a shift in mean of approximately 2σ. (Note that this is different
from stating that we detect a “two sigma” event, because due to
the cumulative nature of the detection process, it takes signiﬁcantly
more perturbation to the site’s activity than simple stochastic ﬂuc-
tuations to reach this level.)

We choose the other parameter, the decision threshold H, based
on computing ARLs using the Markov chain analysis sketched
in § 4.1. Table 3 shows the in-control and out-of-control ARLs for
k = 5 and varying values of H. (We use k = 5 based on the rule-
of-thumb of setting k = Δµ
2 [10].) Given these results, we choose
H = 20, as this gives a quite manageable expected false alarm rate
of one-per-3,720 events, which, given that the site produces about
5 events per day, translates to an average of two alarms per year,
and thus an expected 16 false alarms for the complete dataset. This
level detects actual stealthy attacks after 5 events (50 brute-forcer
login attempts, since the computation is for a shift in the mean of
Δμ = 10). In a practical setting, H = 10 (one false alarm per
month) could work effectively.

To validate the assumptions underlying our detection model, we
ran the CUSUM detector on the “cleaned” data (per § 5) to compare
the expected false alarms with empirical false alarms. The detector

H In-control ARL Out-of-control ARL
1
1
3
10
20
5
7
30
40
9

9
144
3,720
99,548
2,643,440

Table 3: In-control and out-of-control ARLs for k = 5 and varying values
of H.

ﬂagged a total of 12 false alarms, reﬂecting cases where the failure
of benign users lead to the alarm.
6.2 Assessment of Detection

The two components of our detector can each exhibit false
alarms: false coordinated attack epochs and false attack partici-
pants. We can readily identify the former by inspection, as incor-
rect attack epochs can manifest in one of three ways: (i) the epoch
consists of a singleton brute-forcer and a collection of legitimate
users who had failures, (ii) the epoch consists of non-coordinating
brute-forcers having no apparent coordination glue, and (iii) bad
luck: the epoch consists of just legitimate users who failed. The
latter kind of false alarms (false attack participants) pose a harder
challenge to classify, given we lack ground truth. Since LBNL
didn’t itself detect and assess the majority of the attacks we de-
tect, we use the following heuristic to gauge whether our procedure
correctly classiﬁed a remote host as a brute-forcer. We inspect the
the host’s login activity in the attack epoch along with its future
activity. If none of this succeeded, we can with high conﬁdence
deem that host as a brute-forcer. For hosts that ultimately succeed,
we conﬁrm whether the success reﬂected a break-in by checking
whether LBNL’s incident database eventually noted the activity.

Running the procedure over 8 years of data, the Aggregate Site
Analyzer detected a total of 99 attack epochs. After then processing
these with the Attack Participants Classiﬁer, we ﬁnd nine5 repre-
sent false alarms. We detect a total of 9,306 unique attack hosts
participating in the distributed attack epochs, two of which suc-
ceed in breaking-in. The procedure classiﬁed only 37 benign hosts
as attack hosts, reﬂecting a very low false alarm rate. On days
that include an attack epoch, we ﬁnd on average about 100 benign
(cid:7)R, U(cid:8) pairs ﬁltered out using our past-history assessment, but on
average only about 1.7 “forgotten username” instances detected and
removed.

F
D
C
E

8

.

0

4

.

0

0

.

0

1

2

5

10

20

Duration of attack (days)

Figure 7: Empirical CDF of the duration of attacks (number of days)

5Note that this number differs from that found earlier on “cleaned”
data because some of those false alarms coincided with actual at-
tack epochs, and the Attack Participants Classiﬁer then removed
them due to a mismatch of coordination glue.

92s
s
e
r
d
d
a
 
k
r
o
w
t
e
n
 
e
t
o
m
e
r
 
d
e
z
m
y
n
o
n
A

i

0
0
0
8

0
0
0
6

0
0
0
4

0
0
0
2

0

6
0
0
2
 
y
a
M

6
0
0
2
 
p
e
S

7
0
0
2
 
n
a
J

7
0
0
2
 
y
a
M

7
0
0
2
 
p
e
S

8
0
0
2
 
n
a
J

8
0
0
2
 
y
a
M

8
0
0
2
 
p
e
S

9
0
0
2
 
n
a
J

9
0
0
2
 
y
a
M

9
0
0
2
 
p
e
S

0
1
0
2
 
n
a
J

0
1
0
2
 
y
a
M

0
1
0
2
 
p
e
S

1
1
0
2
 
n
a
J

1
1
0
2
 
y
a
M

1
1
0
2
 
p
e
S

2
1
0
2
 
n
a
J

2
1
0
2
 
y
a
M

2
1
0
2
 
p
e
S

Figure 8: Participating attack hosts in the distributed attacks detected from
2005 to 2012 at LBNL.

HONEY

CAMPOFF

RSRCHLAB

p
a
l
r
e
v
o
 
e
g
a
t
n
e
c
r
e
P

0
0
1

0
8

0
6

0
4

0
2

0

1

2

3

4

6

7

8

9

10 11 12 24 25 26 29 30

Attack Number

Figure 9: Percentage overlap of attack hosts seen at LBNL with that at sites
HONEY, CAMPOFF and RSRCHLAB. The ﬁgure displays only the subset
of attacks that appear in at least one of the three sites. (Note that none of
the attacks appear in HOMEOFF).

Figure 7 shows the empirical CDF of the span of detected at-
tack epochs. These coordinated attacks often span multiple days,
and sometimes multiple weeks. The majority of the attacks ex-
hibit strong coordination glue in terms of either the set of local ma-
chines probed or the usernames targeted for brute-forcing. Of the
90 true attack epochs, 62 have common-set-of-local-machines glue
and 25 have username-“root” glue. Only 3 epochs did not manifest
any glue we could identify; these epochs probed machines across
a wide range of addresses and using a dictionary of generic user-
names, such as mysql and admin.

Figure 8 shows the attack hosts participating in the various dis-
tributed attack epochs over time, where we number distinct hosts
consecutively starting at 1 for the ﬁrst one observed. The sig-
niﬁcant overlap of attack hosts across attack episodes shows that
many of these attacks employ the same botnet. We then analyzed
the coordination glue in these attack epochs to consolidate the set
of epochs into attack campaigns. We use the following rules to
group epochs into campaigns based on observing evidence that
the same attacker conducting different attack epochs that work to-
wards the same goal: (i) epochs with the same common-set-of-
local-machines coordination glue, and (ii) epochs appearing on the
same day with username-root coordination glue. Our detector con-
siders these as multiple attack epochs rather than a single attack
because this is indeed how the campaign proceeds, stopping for a
few hours/days and then reappearing. Using these rules, we group

the 62 attacks with common-set-of-local-machines glue into 12 dis-
tinct attack campaigns. Only a few of the 25 epochs group using
heuristic (ii), condensing the root-set to 20 campaigns. This leaves
us with a total of 35 attack campaigns.

Table 4 summarizes the magnitude, scope and stealthiness of the
attacks we detect. All of these attacks were stealthy when observed
from the viewpoint of individual hosts; on average the attack hosts
made ≈ 2 attempts per local machine per hour. We can however
detect a large fraction of these attack campaigns using a point-wise
network-based detector that looks for high-rate hourly activity in
terms of either the total number of failed attempts or the number of
local hosts contacted. Note that we also detect attacks that a site
cannot detect using either host-based or network-based point-wise
detection (campaigns 5, 7 and 8 in Table 4). Finally, two of the
campaigns succeeded, the ﬁrst of which (campaign 1) as best as we
can tell went undetected by the site.

We also ﬁnd a difference in the characteristics between attacks
that have set-of-local-machines coordination glue versus the ones
that only have username-root glue. The latter tend to target a wide
range of the site’s address space and often involve just a few at-
tack hosts brute-forcing at a high rate. Attacks having set-of-local-
machines coordination glue often exhibit the pattern of the attack-
ers stopping and coming back. We did not ﬁnd any sequential pat-
tern in any of these campaigns; rather, the attackers targeted servers
spread across the address space, often including addresses in both
of LBNL’s distinct address blocks. We also did not ﬁnd any pattern
among the local servers in terms of belonging to the same research
group or compute cluster.
6.3 Establishing the scope of attacks

Next, we attempt to establish which attacks speciﬁcally targeted
the LBNL site versus global attacks that indiscriminantly probed
the site. To do so we look for whether the attack hosts of a given
campaign appeared in any of our four correlation datasets, HONEY,
RSRCHLAB, HOMEOFF, and CAMPOFF.

We ﬁnd that 16 campaigns appear in at least one of these four
datasets. These include ﬁve username-root coordination glue at-
tacks and all but one of the attacks with set-of-local-machines co-
ordination. Figure 9 plots the percentage overlap of the attack hosts
detected in the global attacks at LBNL with that at other sites,
showing a high overlap in most cases. We investigated campaign 5,
which does not appear at any of the other sites, and found that it
indeed targeted LBNL, as the attack hosts all probed a set of six
usernames each valid at the site. As shown by the hourly rates in
Table 4, this targeted attack also proceeded in a stealthy fashion,
with each remote host on average making only 9 attempts and con-
tacting 3 local servers per hour. It’s possible that some of the other
campaigns also speciﬁcally targeted LBNL, though for them we
lack a “smoking gun” that betrays clear knowledge of the site.

Finally, to give a sense of the nature of global attacks, Fig-
ure 10 shows the timing patterns of login attempts at the LBNL
and HONEY sites during part of campaign 8. From the clear corre-
lation (though with a lag in time), we see that the activity at both
reﬂects the same rate (which varies) and, for the most part, the same
active and inactive periods.

7. CONCLUSION

In this work we propose a general approach for detecting dis-
tributed, potentially stealthy activity at a site. The foundation of the
method lies in detecting change in a site-wide parameter that sum-
marizes aggregate activity at the site. We explored this approach in
concrete terms in the context of detecting stealthy distributed SSH
brute-forcing activity, showing that the process of legitimate users

93ID Appearances

Attrs.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35

2007: [Jul 7-9], [Oct 20-23], [Nov 5-9](2), [Nov 13-18](2)
2008: [Apr 30 - May 7],[May 8-14](3)
2008: [Jun 28-29], [Jun 30 - Jul 1]

[Jul 7-9], [Aug 17-21], [Sep 1-8] (5)

2008: [Sep 8-13](3)
2008: [Sep 16-18]
2008: [Sep 23-26](2), [Sep 29 - Oct 2](2)
2008: [Nov 18-19], [Nov 20 - Dec 29](5) 2009: [Apr 7-9]
2009: [Oct 22-23], [Oct 27 - Nov 24](5)
2010: [Dec 6 - Jan 10](6), [Jan 11-18], [Jan 20-22], [Mar 4-8]
2010: [Jun 16 - Jul 27](2), [Jul 29 - Aug 11]
2010: [Nov 1-6] (2), [Nov 7-8], [Nov 27 - Dec 1], [Dec 15-17]
2011: [Oct 11-19], [Oct 25-29](2), [Nov 4-7], [Nov 17-20]
2010: [Mar 30 - Apr 1]
2010: [Apr 23-26]
2010: [May 7-10]
2010: [Sep 20-22]
2010: [Dec 27-30]
2011: [Feb 10-14](2)
2011: [May 16-18]
2011: [Jul 21-22]
2011: [Aug 2-6]
2011: [Aug 7-9]
2011: [Aug 17-21](2)
2011: [Nov 2-4]
2011: [Nov 30 - Dec 5]
2011: [Dec 18-20]
2012: [Jul 20-21]
2012: [Aug 27 - Sep 2]
2012: [Sep 26-29]
2012: [Oct 8 - Nov 1](4)
2012: [Nov 16-18]
2012: [Nov 30 - Dec 2]
2008: [Jan 9-12]
2011: [Apr 8-26]
2012: [Dec 14-17]

L,S,T

L,!!
L
L
L

L
L,S
L,S
L
L
L,!
L
R,t
R,t
R,t
R,t
R,t
R,t
R,t
R,t
R,t
R,t
R,t
R
R
R
R,t
R,t
R
R,S
R,t
R,t
X,t
X,t
X,t

Aggregate statistics
Attack

Local Attempts
machines machines
74.68
133
98.50
140
293.30
113
52.50
257
12
9.00
48.50
109
16.01
22
5.60
5
38.80
44
1,494
90.80
140.60
98
33.93
158
999.70
18,815
2325.57
29,924
9,300
713.05
69.05
5,380
260.59
3,881
40.45
7,520
153.23
1,621
388.25
2,556
9,465
315.12
444.16
6,516
33.07
3,279
273.80
3,446
829.68
10,467
961
1099.85
20,844
53,219
20.84
1,912
72.30
1,971
5.27
19,639
493
38.36
133.00
344
2,846.44
63,015
591.34
19,158
45,738
1,490.26

431
286
969
378
88
185
1,097
1,734
3,496
7,445
581
377
78
130
72
33
32
108
30
20
45
48
22
31
181
258
2
10
6
190
3
3
17
67
13

Per remote avg. hourly characteristics
Per-Local
attempts
1.33
1.79
7.00
1.28
3.57
1.26
1.99
1.50
1.80
2.70
3.09
1.34
1.33
1.22
1.36
1.14
1.34
1.48
2.02
1.18
2.41
2.18
2.02
1.02
1.03
1.02
1.06
1.23
1.59
1.06
2.99
1.93
1.61
6.76
1.04

Locals
contacted
56.10
54.80
41.70
40.70
2.53
38.38
8.04
3.70
21.50
34.50
45.47
25.25
118.91
117.97
67.47
60.72
43.11
27.21
19.70
38.13
21.66
17.60
16.40
20.08
18.31
14.00
11,749
14.38
13.05
4.97
12.22
68.80
1,761.69
87.41
1,430.67

Table 4: Characteristics of the detected coordinated attack campaigns. In Appearances, numbers in parentheses reﬂect how many attack epochs occurred
during the given interval. Attrs. summarizes different attributes of the activity: L = coordination glue was set of local machines, R = coordination glue was
username “root”, X = no discernible coordination glue, S = stealthy, T = targeted, t = possibly targeted but no corroborating evidence, ! = successful, !! =
successful and apparently undetected by the site.

failing to authenticate is well-described using a beta-binomial dis-
tribution. This model enables us tune the detector to trade off an
expected level of false positives versus time-to-detection.

Using the detector we studied the prevalence of distributed brute-
forcing, which we ﬁnd occurs fairly often: for eight years of data

NATLAB
HONEY

.

o
N

 
t

p
m
e

t
t

A

0
0
0
0
1

0
0
0
5

0

0e+00

1e+05

2e+05

3e+05

4e+05

5e+05

6e+05

7e+05

Number of seconds elapsed since Oct 28 16:16:37 PDT 2009

Figure 10: Timing of login attempts at HONEY machine and LBNL sites
during part of attack number 8 (Oct 2009 - Nov 2009). The plot is based on
data for only one of the machines targeted during the attack at LBNL.

collected at a US National Lab, we identify 35 attack campaigns in
which the participating attack hosts would have evaded detection
by a pointwise host detector. Many of these campaigns targeted
a wide range of machines and could possibly have been detected
using a detector with a site-wide view, but we also ﬁnd instances
of stealthy attacks that would have proven very difﬁcult to detect
other than in aggregate. We correlated attacks found at the site
with data from other sites and found many of them appear at multi-
ple sites simultaneously, indicating indiscriminant global probing.
However, we also ﬁnd a number of attacks that lack such global
corroboration, at least one of which clearly targeted only the local
site. Some campaigns in addition have extensive persistence, last-
ing multiple months. Finally, we also ﬁnd that such detection can
have signiﬁcant positive beneﬁts: users indeed sometimes choose
weak passwords, enabling brute-forcers to occasionally succeed.

Acknowledgments
Our thanks to Mark Allman, Peter Hansteen, and Robin Sommer
for facilitating access to the different datasets required for this
work. Our special thanks to Aashish Sharma for running down

94various puzzles and to Partha Bannerjee and James Welcher for
providing crucial support for the processing of the LBNL dataset.
This work was supported by the U.S. Army Research Ofﬁce un-
der MURI grant W911NF-09-1-0553, and by the National Science
Foundation under grants 0831535, 1161799, and 1237265. Any
opinions, ﬁndings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily re-
ﬂect the views of the sponsors.
8. REFERENCES
[1] BlockHosts.

http://www.aczoom.com/blockhosts/.

[2] DenyHosts.

http://denyhosts.sourceforge.net/.

[3] sshguard. http://www.sshguard.net/.
[4] The Hail Mary Cloud Data - Data collected by Peter N. M.

Hansteen (peter@bsdly.net).
http://www.bsdly.net/~peter/hailmary/.

[5] ICS-ALERT-12-034-01 — SSH Scanning Activity Targets

Control Systems.
http://www.us-cert.gov/control_systems/
pdf/ICS-ALERT-12-034-01.pdf, Feburary, 2012.

[6] R. Bezut and V. Bernet-Rollande. Study of Dictionary

Attacks on SSH. Technical report, University of Technology
of Compiegne, http://files.xdec.net/TX_EN_
Bezut_Bernet-Rollande_BruteForce_SSH.pdf,
2010.

[7] D. Brook and D. A. Evans. An approach to the probability

distribution of CUSUM run length. In Biometrika,
volume 59, pages 539–549, 1972.

[8] C. Gates. Coordinated scan detection. In 16th Annual

Network and Distributed System Security Symposium, 2009.
[9] D. Gerzo. BruteForceBlocker. http://danger.rulez.

sk/projects/bruteforceblocker.

[10] D. M. Hawkins and D. H. Olwell. Cumulative sum charts

and charting for quality improvement. Springer, 1998.

[11] L. Hellemons. Flow-based Detection of SSH Intrusion

Attempts. In 16th Twente Student Conference on IT.
University of Twente, January 2012.

[12] C. Jacquier. Fail2Ban. http://www.fail2ban.org.
[13] M. Kumagai, Y. Musashi, D. Arturo, L. Romana,

K. Takemori, S. Kubota, and K. Sugitani. SSH Dictionary
Attack and DNS Reverse Resolution Trafﬁc in Campus
Network. In 3rd International Conference on Intelligent
Networks and Intelligent Systems, pages 645–648, 2010.
[14] E. L. Malecot, Y. Hori, K. Sakurai, J. Ryou, and H. Lee.

(Visually) Tracking Distributed SSH BruteForce Attacks? In
3rd International Joint Workshop on Information Security
and Its Applications, pages 1–8, Feburary, 2008.

[15] J. Owens and J. Matthews. A Study of Passwords and

Methods Used in Brute-Force SSH Attacks. In USENIX
Workshop on Large-Scale Exploits and Emergent Threats
(LEET), 2008.

[16] A. V. Siris and F. Papagalou. Application of anomaly

detection algorithms for detecting SYN ﬂooding attacks. In
IEEE GLOBECOM, pages 2050–2054. IEEE, 2004.

[17] S. Staniford, J. A. Hoagland, and J. M. McAlerney. Practical

automated detection of stealthy portscans. In 7th ACM
Conference on Computer and Communications Security,
Athens, Greece, 2000.

[18] J. Vykopal, T. Plesnik, and P. Minarik. Network-based

Dictionary Attack Detection. In International Conference on
Future Networks, 2009.

[19] H. Wang, D. Zhang, and S. K. Detecting SYN ﬂooding
attacks. In 21st Joint Conference IEEE Computer and
Communication Societies (IEEE INFOCOM), pages
1530–1539, 2002.

[20] C. M. Zhang and V. Paxson. Detecting and Analyzing
Automated Activity on Twitter. In Passive and Active
Measurement. Springer, 2011.

95