Seeing Double: Reconstructing Obscured Typed Input

from Repeated Compromising Reﬂections

Yi Xu, Jared Heinly, Andrew M. White, Fabian Monrose and Jan-Michael Frahm

Department of Computer Science, University of North Carolina at Chapel Hill

Chapel Hill, North Carolina, USA

{yix,jheinly,amw,fabian,jmf}@cs.unc.edu

ABSTRACT
Of late, threats enabled by the ubiquitous use of mobile devices
have drawn much interest from the research community. However,
prior threats all suffer from a similar, and profound, weakness —
namely the requirement that the adversary is either within visual
range of the victim (e.g., to ensure that the pop-out events in reﬂec-
tions in the victim’s sunglasses can be discerned) or is close enough
to the target to avoid the use of expensive telescopes. In this pa-
per, we broaden the scope of the attacks by relaxing these require-
ments and show that breaches of privacy are possible even when
the adversary is around a corner. The approach we take overcomes
challenges posed by low image resolution by extending computer
vision methods to operate on small, high-noise, images. Moreover,
our work is applicable to all types of keyboards because of a novel
application of ﬁngertip motion analysis for key-press detection. In
doing so, we are also able to exploit reﬂections in the eyeball of the
user or even repeated reﬂections (i.e., a reﬂection of a reﬂection of
the mobile device in the eyeball of the user). Our empirical results
show that we can perform these attacks with high accuracy, and can
do so in scenarios that aptly demonstrate the realism of this threat.
Categories and Subject Descriptors: K.4.1 [Computers and So-
ciety]: Privacy
General Terms: Human Factors, Security
Keywords: Compromising Emanations; Mobile Devices

1.

INTRODUCTION

Gone are the days when mobile phones were used exclusively
for voice communication. Today, as these handheld devices have
become more sophisticated, they are routinely used for a myriad
of everyday activities that include checking email, text messaging,
performing ﬁnancial transactions, and ﬁnding directions to a lo-
cation of interest. Inevitably, as our day-to-day reliance on these
devices increases, the sensitive information (e.g., passwords) input
on these devices becomes increasingly valuable to prying eyes.

While the academic community has long acknowledged that the
ubiquity of these devices provides new opportunities for privacy
abuse (as users communicate private data in ways more vulnerable

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
CCS’13, November 4-8, 2013, Berlin, Germany.
ACM 978-1-4503-2477-9/13/11.
http://dx.doi.org/10.1145/2508859.2516709 .

Figure 1: Some example threat scenarios that we investigated.
Top left: Through reﬂection from sunglasses and toaster; Top
right: Through reﬂection from eyeball and mirror; Bottom
left: Through reﬂection from sunglasses; Bottom Right: Di-
rect long-distance view. Note: As with the remaining ﬁgures in
this work, this image is best viewed in color.

to eavesdropping than ever before), the severity of the threat posed
by advancements in computer vision techniques is only now being
well understood [3, 34]. As a case in point, both Raguram et al. [34]
and Maggi et al. [29] recently showed that modern touch-screen
smartphones may offer a greater privacy threat than their traditional
counterparts. The increased risk comes from the fact that many
touch-screen smartphones utilize virtual keyboards that overcome
the perceived lack of tactile feedback by providing users with visual
conﬁrmation (a key “pop-out” effect) as a key is pressed. These
effects, however, provide strong visual cues that can be exploited by
an attacker to help identify the keys tapped on the victim’s device.
The techniques used to leverage the so-called compromising re-
ﬂections in these prior works have raised our collective awareness
of the realism of these threats. However, they all suffer from a sim-
ilar, and profound, weakness — namely the requirement that the
adversary is either within visual range of the victim (e.g., to ensure
that the pop-out events in reﬂections in the victim’s sunglasses can
be discerned [34]) or is close enough to the target to avoid the use
of expensive telescopes [3].

In this paper, we push the limits of these attacks by exploiting
even more fundamental, and harder to conceal, observable events.
That is, unlike prior work, we do not rely on the attacker’s ability to
capture detail (e.g., a key pop-out event) on the screen, but instead
target a common factor in user interaction with mobile devices: the

1063relationship between a user’s ﬁngers and the keyboard. By track-
ing the positions of a user’s ﬁngers as they move across the vir-
tual keyboard, we can successfully reconstruct the typed input. In
particular, we show that even using inexpensive consumer devices
(e.g., small hand-held camcorders), we can successfully perform a
reconstruction attack as long as both the device’s orientation and
the user’s ﬁngers are detectable.

Tracking ﬁngers, rather than recognizing displayed images, broad-
ens the class of vulnerable devices to include those without any
pop-out effect, such as Apple’s iPad. Moreover, our approach is
capable of operating at signiﬁcant distances from the victim (e.g.,
up to 50 meters away with a camcorder). Perhaps most importantly,
our approach operates effectively even for repeated reﬂections, i.e.,
reﬂections of reﬂections in nearby objects. This feat allows us to
reconstruct typed input from the image of a mobile phone’s screen
on a user’s eyeball as reﬂected through a nearby mirror (see Fig-
ure 1), extending the privacy threat to include situations where the
adversary is located around a corner from the user.

Our approach operates despite low resolution images (a natural
consequence of increasing the distance between target and cam-
era) and high noise levels (due to reduced light levels in reﬂections
of objects as compared to the originals). To overcome these chal-
lenges and achieve our goals, we extend a large body of work on ob-
ject detection and tracking in the area of computer vision. Specif-
ically, we extend existing ﬁnger tracking mechanisms to consider
spatial context in order to reliably locate ﬁngertips in the images.
In addition, we propose a novel method for identifying ﬁngertip
trajectories based on robust estimation.

2. RELATED WORK

Techniques for undermining a user’s privacy via several forms of
compromising emanations have a rich and storied history (e.g., [1–
3, 6, 7, 12, 16, 21, 29, 33, 41, 43, 49]). Probably the earliest of these
ideas is embodied in the work of van Eck [41] and Highland [16] on
compromising signals from electromagnetic radiation. That work
was later extended by Kuhn and Kuhn [21], wherein it was argued
that a telescope could be used to spy on reﬂections of a computer
screen from afar. Intuitively, the light emitted from the display was
modeled as a function of time and space, and then analyzed using
signal processing methods. Naturally, the captured light is a dis-
turbed signal of the light emitted by the screen, where disturbances
include atmospheric effects, dispersion, attenuation, or lens distor-
tion of the capture device. Regardless of these disturbances, Kuhn
and Kuhn [21] show that by utilizing an expensive telescope with
a wide aperture, they were able to reconstruct text on a 32×24 cm
display from 60 m away.

More recently, Backes et al. [2, 3] overcame the requirement of
direct line-of-sight. The key innovation was to exploit reﬂections
to vary the path of light between the target screen and the observer,
and showed it was possible to leverage the reﬂection off a human
eyeball (reading a very large 36 pt text font from a distance of
10 m). However, the approach still used a high-powered telescope
inheriting the drawbacks of high cost along with limited versatility
and ability to go undetected. In addition, the setting of Backes et
al. did not have to consider motion (of either the victim’s device or
the adversary), and also was not concerned with the daunting task
of automatically reconstructing text from typed input.

The use of less expensive and more practical equipment was in-
troduced by Raguram et al. [34]. Unlike the approach we present,
that method relies on detecting the presence of key pop-outs in vir-
tual keyboards. While that approach worked well for direct line-of-
sight attacks, reconstructions involving a single reﬂection (in this
case, off the victim’s sunglasses) did not perform as well [34]. A re-

lated approach that also relied on the ability to detect pop-outs was
proposed by Maggi et al. [29]. However, the approach of Maggi
et al. is sensitive to movement of the device and camera and suffers
in the presence of occlusions (including the ﬁngers of the device’s
user). Neither approach could handle reconstructions of low reso-
lution images of reﬂections of reﬂections in nearby objects.

Within the computer vision and human computer interaction com-
munities, work on ﬁnger tracking for gesture recognition [8, 32],
virtual input systems [40, 48], virtual object manipulation [25, 26],
and hand writing recognition [19, 46] all share similarities to our
application of ﬁnger motion analysis. Probably the most germane
of these is the work of Iturbe et al. [18] which uses ﬁnger movement
to control a virtual input system in which a user’s ﬁnger is modeled
as a straight line and its movement is used to determine the button
being pointed to by the user. Unfortunately, their approach quickly
fails for small mobile devices where the ﬁngers need to bend in
order to reach the keys. Similarly, Jin et al. [19] utilized ﬁnger in-
put for character writing recognition. In their approach, the user’s
ﬁnger is isolated using a background modeling technique. Next,
the path taken by the ﬁnger is tracked as the user writes individual
letters, effectively recognizing the letter through the trajectory of
the ﬁnger. Sadly, the approach of Jin et al. [19] can not be directly
applied to mobile devices as users do not spell words by forming
one character at a time, but instead interact with a keyboard via a
series of touch events.

Lastly, Kerdvibulvech and Saito [20] apply a novel technique for
tracking the ﬁngers while a user plays a guitar. Instead of trying
to uniquely identify each individual ﬁnger, the authors use a neu-
ral network classiﬁer to recognize known patterns corresponding
to different chord formations. While promising, their approach is
also not applicable in our setting, as the way users type on mobile
devices can differ signiﬁcantly for the same user (e.g., switching
between typing with one, two, or several ﬁngers), and even more
among different users, making the learning strategy less practical.
Nevertheless, as we show later, we found that by combining many
of the strengths of prior works alongside our own enhancements,
we are able to achieve a solution that surpasses previous attempts
in terms of its practically, range, and robustness.

3. BACKGROUND

Before introducing our approach, we provide pertinent back-
ground information that is helpful in understanding the set of chal-
lenges that impact how well an adversary can observe reﬂected ob-
jects in a scene.

Obviously, the size of the object in the captured images is of
critical importance and naturally depends on the size of the object
itself. Another factor is the focal length of the camera. Loosely
speaking, the size of the object in the image for direct line-of-sight
can be computed as:

SizeDirect =

Sensor Resolution

(cid:125)

Target Distance

· Ob ject Size
(cid:124)
(cid:123)(cid:122)

(cid:125)
Focal Length − 1
size on sensor

(1)

(cid:124)

(cid:123)(cid:122)

Sensor Size
pixel scale

Intuitively, the observed size is dependent on the physical size of
the projection of the object on the sensor and the characteristics of
the camera sensor, namely the size and number of pixels (picture
elements). The size of the object on the sensor (its projection) is
controlled by the focal length, the distance to the object, and the
object size. Focal length can be viewed as the amount of mag-
niﬁcation of the image, where longer focal lengths (zoom lenses)
provide higher magniﬁcations. Thus, by using a lens with a longer
focal length, an adversary can gain a better view of the target. The

1064ﬁnal size of the image of the device in pixels (given the image’s size
on the sensor) then depends on the ratio between how many pixels
are on the image sensor (SensorResolution) and the physical size
of that sensor (SensorSize). At the same focal length, for exam-
ple, the size of the object in pixels tends to decrease with full frame
sensors found in high-end digital SLRs compared to cheaper digital
camcorders with smaller sensors but the same video resolution.

In addition to the physical object size, the size of the object on
the image sensor also depends on the presence and shape of any re-
ﬂecting surfaces between the observer and the object. For instance,
if the reﬂecting object is convex (e.g., a pair of sunglasses or the hu-
man eyeball), the size of the observed object will be much smaller
than if observed with direct line-of-sight. When an object is viewed
via a reﬂection, the ﬁnal observed size can be computed as:

SizeRe f lection = SizeDirect ∗

1

2 Distance f romSur f ace

CurvatureRadius + 1

(2)

Thus, the curvature of the reﬂecting surface is an important factor
in the observed size. The more curved the reﬂecting surface is, the
more the light will be bent. For convex surfaces, the bending of
the light will result in a smaller observed object size. Lastly, the
distance between the reﬂecting surface and the target object itself
(Distance f romSur f ace) naturally affects the observed object size.

Takeaway. One way to acquire a larger observed size is to sim-
ply reduce the distance to the target object. However, from an ad-
versarial point of view, it is desirable to be as far away as possible
from the victim. Hence, a better solution would be to use a lens
with a long focal length. For similar reasons, cameras with higher
pixel density in their sensors are preferred. Finally, the curvature of
any reﬂecting surface must be taken into account. For example, the
human eyeball has a typical curvature of about 8 mm [2]. Hence,
when people look at an object 25 cm away, the reﬂection in their
eyeball will be about 60 times smaller than with direct line-of-sight.

Impact of Diffraction. The quality of the acquired image is
signiﬁcantly inﬂuenced by the wave properties of the light. When
light comes near the edge of the lens, not all light rays traveling
from the object pass directly through the lens to the image sensor.
Instead, a fraction of the light diffuses and travels in every direc-
tion, leading to a blurring of the image. This phenomenon is called
diffraction and cannot be eliminated. It presents a physical bound-
ary for the effective resolution of the object in the captured image
(commonly referred to as the Rayleigh Criterion [2]).

The maximum effective size of the observed object (MaxSize)

can be approximated as:

MaxSize =

Aperture/Wavelength
1.22 Target Distance

∗

Ob ject Size

2Distance f romSur f ace

CurvatureRadius + 1

(3)

Notice that the actual amount of diffraction is impacted by the
wavelength of the light (Wavelength). While the adversary has no
control over this factor, we include it here for completeness (along
with the well-known constant scale factor of 1/1.22 for circular
apertures [37]). However, the adversary can select a lens with an
appropriate aperture (i.e., opening of the lens), which lets the de-
sired amount of unobstructed light pass through the lens.

Takeaway. The larger the aperture of the lens, the smaller the
amount of diffraction. It is for precisely this reason that prior work
(e.g., [2, 3, 21]) resorted to the use of telescopes. However, lenses
with large apertures are typically very expensive, costing well over
$1,000 per square cm [2], and are difﬁcult to conceal.

Impact of Noise. A very signiﬁcant factor that affects the qual-
ity of the acquired image of the target object is imaging noise.
Noise is a random variation in a pixel’s intensity, causing the im-
age to appear speckled. As noted by Nakamura [30], there can be
several types of background noise in the captured image, each with
a constant noise level. To avoid visual impact on the image qual-
ity by the noise, the exposure time is typically chosen so that the
overall amount of light overwhelms the background noise making it
hardly noticeable. However, for video capture, the exposure is nat-
urally limited to the time of a frame, which for darker scenes makes
the background noise become more noticeable. The resulting noise
causes signiﬁcant challenges in identifying ﬁne detail.

Typically, cameras with large sensors are more resistant to noise,
as their pixels are usually larger and can capture more photons of
light. For that reason, the larger sensors provided in digital SLR
cameras (as opposed to cellphones or point-and-shoot cameras) are
desirable for photography even though they provide a smaller num-
ber of pixels on the object.

Taken as a whole, the aforementioned factors present challenges
that severely limit the use of existing techniques (e.g., [2, 3, 34, 35])
when considering reconstruction of typed input from repeated re-
ﬂections. For instance, the recently used technique of identifying
salient feature points [34, 35] within the image in order to facilitate
alignment will fail because the poor image resolution does not pro-
vide the required details for the salient feature points. The approach
suggested by [29] faces similar challenges. Additionally, the low
image quality (e.g., as acquired from a reﬂection in the eyeball) pre-
vents the detection of ﬁne detail. Therefore, the key pop-out events
exploited by previous work will no longer be distinguishable.
4. AUTOMATED TRANSCRIPTION

Our proposed method successfully transcribes the text typed on
a keyboard by exploiting video of the user typing (either directly
or through up to two reﬂections from nearby objects). In practice,
we must overcome all of the aforementioned challenges of image
resolution, diffraction, and noise. To do so, we devise techniques
that are resistant to low resolution, blurring, and noise. Figure 2
shows a high-level depiction of our approach.

We take as input a recording of the target device while the user
types on its keyboard. First, we roughly estimate the location of
the device in the image (Stage ). Next, the image of the device
is aligned to a known reference template of the device’s keyboard
layout (Stage ). Then, the ﬁngertips are identiﬁed in the video and
the locations of the ﬁngertips over the video frames are combined
into trajectories (Stage ). These trajectories are then analyzed to
identify the pressed keys (Stage ). From these pressed keys we
then reconstruct the typed text (Stage ). Finally, as an optional
post-processing step, we apply a language model in order to im-
prove the readability of the ﬁnal text (Stage ). Aside from some
initial input in Stage  to identify the ﬁrst frame containing the tar-
get device, the entire process is fully automated. In what follows,
we discuss each step in turn.
Stage : Tracking
With a recording in hand, we ﬁrst identify the device (phone, iPad,
etc.) in the video so that we can focus our remaining analyses on
only the relevant parts of the image. Depending on the number of
reﬂections and the distance between the victim and the observer,
the device is often only seen in a small region of each video frame
(refer to Figure 3 for an example of reﬂection in an eyeball).

In order to identify the device’s location in every frame, we uti-
lize a tracking framework based on AdaBoost [15]. The basic idea
is as follows. First, the user selects the region of the ﬁrst video

1065Figure 2: Overall design depicting the stages of our approach. First we track the motion of the mobile device (Stage ). Then the
mobile device is aligned to a template (Stage ). In the stabilized image the ﬁnger is extracted (Stage ) and its ﬁngertip trajectory is
computed (Stage ). From the trajectories the likely pressed keys are identiﬁed (Stage ). As an optional step, we apply a language
model to improve the quality of the reconstructed text (Stage ).

cause of the high frame rate (30 frames per second) and the small
motion of both the device and the user between frames.
Stage : Alignment
Stage  acquired the rough position of the device in each frame.
The task in Stage  is to determine the exact location of each key
in the video frame (so that later stages can identify the keys which
were most likely pressed). In general, the device will have varying
orientation, scale, and image position in the video. In order to de-
termine a mapping between the image in the video and a reference
keyboard layout of the device (lifted, for example, from a manual),
we must estimate the transformation that will align the device in
the video to a reference image of the device’s layout.

Prior work [29, 34] faced a similar challenge, and used salient
points in the image as visual features to determine the transforma-
tion. In our setting, however, those visually distinct salient feature
points are no longer visible. Thus we must overcome this chal-
lenge in a different manner. The approach we take is to utilize a
line detection technique that provides a more robust set of features
to achieve the same goal.

Preliminary Alignment: As an initial step, we ﬁrst reduce the
noise present in the images by employing anisotropic ﬁltering [45].
This technique (detailed in Appendix A) can be used to intelligently
preserve line structure in the image (otherwise often lost in nor-
mal noise reduction techniques), while simultaneously suppressing
noise in the remaining parts of the image.

To determine the correct orientation of the device, we transform
the video image so that all of the device’s lines become either hor-
izontal or vertical. This is accomplished by detecting edges within
the image [11], and then using the lines to vote for the device’s
orientation using a Hough transform. The Hough transform is a
voting scheme, where each detected line in the image votes for all
possible orientations of the device that conform with the line’s own
orientation. For robustness, our voting scheme exploits the fact that
lines of the same orientation will have a common vanishing point;
hence each line is voting for its corresponding vanishing point (see
Appendix B). Then, the vertical and horizontal vanishing points

Figure 3: The captured image showing a reﬂection (of the vic-
tim’s screen) as observed in the eyeball.

frame (corresponding to the device) that should be localized. Given
this initial position, AdaBoost learns the appearance of the target
object, and then automatically determines its location in all suc-
ceeding video frames. Since AdaBoost tracking works relatively
well even in the presence of noise, we can successfully track small
or even partially occluded objects in the image. The latter is im-
portant because the device is usually occluded by the ﬁngers as the
victim types on the keyboard.

That said, despite its robustness to noise, a straightforward ap-
plication of AdaBoost tracking frequently fails in our setting. This
is because tracking relies on the target device maintaining a similar
appearance between frames, which is not necessarily the case with
the noisy images we must process. In order to mitigate this prob-
lem, we average each video frame with the temporally preceding
and trailing frame in the sequence. In this manner, each image is
the combination of three consecutive frames, which reduces noise
as random variation will be smoothed.1 Although averaging could
cause motion blur if there is signiﬁcant scene motion between the
frames, we found that in practice this is not a signiﬁcant issue be-

1Gaussian noise is reduced by a factor of

√
3.

Track➊➋➌➍The quick brown fox…AlignExtract FingertipAnalyze MotionGenerate Key Weights➎➏Language ModelingKey 0: T (1.00)Key 1: H (0.97)Key 2: E (0.79)       R (0.21)Key 3: _ (0.91)Key 4: Q (0.77)1066with the most votes represent the dominant vertical and horizontal
line directions in the image. With this information at hand, we can
successfully transform the device to its proper orientation.

Note, however, that even with the correct orientation, the image
is still not aligned with the reference keyboard layout (i.e., the tem-
plate) as the size and position may still differ. To handle this, we
once again turn to a Hough transformation to vote on the trans-
lation and scale for aligning the image to the reference image of
the device. The voting process relies on correspondences between
the detected lines in the current frame and the lines in the tem-
plate. A descriptor is built for each line in the current frame and
the reference image, representing the appearance of each line’s sur-
roundings. The lines then vote for possible translation and scaling
settings, and their votes are weighted by the similarity of the lines’
descriptors. This voting results in a translation and scale in each of
the x and y directions. Although the scale should be equal in both
directions, we allow different scales (allowing afﬁne transforma-
tion) to overcome small artifacts of slightly misaligned rotations.
Appendix C details the computed transformation.

Reﬁnement: A result of the Hough transform above is that the
voting space will be quantized, leading to small residual misalign-
ments. To provide an alignment that is stable across multiple frames
and is as precise as possible we include a ﬁnal reﬁnement step
(termed dense alignment [4, 38]). As opposed to relying on only
lines, this step considers all pixels in the image to compute the ﬁnal
alignment. We employ a non-linear optimization in the parame-
ters of the alignment (rotation, scale, translation) using the sum of
squared differences between the overlapping pixels of the current
and the ﬁrst frame as an error metric for the residual misalignment.
The ﬁnal alignments are more stable and accurate (to sub-pixel pre-
cision) and are thus better suited for further analysis. An example
alignment is shown in Figure 4.

main colors: a lighter color corresponding to the bright screen, and
a darker color corresponding to the user’s ﬁngers. Gaussian model-
ing allows us to represent their respective appearance by two differ-
ent Gaussian distributions. The idea is to assign each pixel in the
keyboard area of the image to either one of the two distributions
based on whichever has the highest likelihood for the pixel under
consideration. To learn the different distributions, we analyze the
pixel colors of the ﬁrst n = 50 frames.

In essence, by assigning each pixel to one of the two Gaussian
distributions we convert the input image into a binary image (e.g.,
the right image in Figure 5). This strategy effectively segments the
ﬁngers and the device’s screen and allows us to isolate the position
and orientation of the ﬁngers. To isolate the position, we simply
model the user’s ﬁnger as an ellipse, and then attempt to ﬁt an el-
lipse to the boundary of the segmentation. In practice, this means
that we must identify points of high curvature along the boundary
of the ﬁnger’s segmentation and then apply an ellipse-ﬁtting regres-
sion function to determine the best ﬁt (see Figure 5).

To identify the location of the tip of the ﬁnger, we assume that
the ﬁngertip corresponds to one of the four apexes of the ellipse.
In practice, only three of the four apexes are reasonable ﬁngertip
locations because the ﬁnger only covers the keyboard from the left,
right, or bottom. To determine which of the three cases is present,
we consider the intersection of the ellipse with the device’s bound-
ary and select the appropriate apex. Speciﬁcally, if the ellipse inter-
sects with the left and bottom edges of the device, then we assume
that the ﬁnger is moving into view from the left; if the ellipse in-
tersects with the right and bottom, it moves in from the right, and
if the ellipse only intersects with the bottom, then it is moves into
view from the bottom. Notice that by repeating the above process
several times (and ignoring already identiﬁed ﬁngers) we can detect
multiple ﬁngers on the screen and locate the ﬁngertip for each.

Figure 4: Cropped image (left) and alignment result (right)
from Stage  (overlaid with a reference device layout).

Stage : Fingertip Extraction
At this point, the images are now aligned to the reference layout,
and we are now primarily concerned with identifying the ﬁngers as
they interact with the device.

Similar to Jin et al. [19] and Nguyen et al. [31], we leverage
Gaussian modeling of the appearance of the ﬁngers as well as the
appearance of the device. Intuitively, at this point, we consider the
keyboard area in our input image as consisting primarily of two

Figure 5: Input aligned image (left) and ﬁnger region extrac-
tion (right). Shown in the right image is the ellipse ﬁtting (blue
ellipse), segmentation (cyan), points with high curvature (yel-
low), detected ﬁngertip (red circle), and an overlaid reference
device layout.

Stage : Fingertip Motion Analysis
We now turn our attention to how we identify the relative position
of the ﬁngers when keys are pressed. We take advantage of the in-
sight that when one presses a key, the ﬁnger typically hovers at a
certain position or suddenly changes direction in order to move to

1067the next key. That observation allows us to use the location of the
ﬁngertip to identify when a key is pressed. To infer the series of
likely keys pressed as the victim types, we combine the locations
of the ﬁngertips extracted in Stage  into trajectories for each ﬁn-
gertip. The trajectory of a ﬁngertip represents its path through time,
and by analyzing this path, we can deduce when a key was pressed.
To accomplish this, we propose a robust method that represents
the ﬁngertip trajectories as a curve in 3D space (where the dimen-
sions are the u,v location in the image, corresponding to the x-y
plane, and the video frame number). When a ﬁngertip stops mov-
ing, the location in the x-y plane will cease to change, but the frame
number will still increase. Hence a stopped ﬁngertip describes a
line segment in the frame number direction that is perpendicular to
the x-y plane and originates at the stopping position u,v.

To ﬁnd these stopping positions or the direction changes, we ap-
proximate the entire 3D curve as a set of line segments. In order to
identify the individual line segments, we use a robust model ﬁtting
technique called RANSAC [13]. Loosely speaking, the RANSAC
algorithm will ﬁnd a consistent model (a 3D line in this case) in
the presence of noisy data even with a small number of correct data
points complying with the model. Every time we run RANSAC,
we model a part of the 3D curve as a line segment. Next, we re-
move the previously modeled part of the curve, and focus on the
remaining portions. By running RANSAC repeatedly, and remov-
ing the modeled parts, we obtain a set of 3D line segments that
approximate the original ﬁngertip trajectory.

Given the set of line segments, stopping positions (u,v) are de-
termined by line segments that are nearly perpendicular to the x-y
plane. Likewise, sudden changes in direction are detected by ﬁnd-
ing two adjacent lines with a large change of direction between
them. Figure 6 shows an example trajectory and the inferred line
segment approximation.

Figure 6: Fingertip trajectory (left) and line modeling (right).
The red circles indicate stopping positions for the word “hello”,
where the size of the circle corresponds to uncertainty in the
stopping position.

Stage : Inferring the Keys Pressed
Given the stopping positions inferred in Stage , we now move on
to determining what keys were likely pressed. This task is some-
what complicated by the fact that users rarely make contact with the

surface of the device with the precise tip of their ﬁnger(s). More-
over, when a user types, her ﬁngers will not necessarily be at the
same place each time they hit a particular key. Therefore, to deter-
mine which key was most likely pressed, we apply image recog-
nition techniques to identify different positions of the user’s ﬁnger
with respect to the keyboard when pressing a key.

To learn the appearance of different key presses, we manually
labeled 87 videos, recording the time and position of each key that
was pressed. For each key on the keyboard, we collect a sample
of its surrounding area as a pixel patch that is 60 × 100 pixels
(matching the aspect ratio of a typical keyboard key). When the key
under consideration is pressed, we identify the patch as a positive
training sample for that key, and also use it as a negative training
sample for all of the other keys on the keyboard. For each of these
patches, we extract dense SIFT descriptors [42] and use these to
train an AdaBoost classiﬁer. In the end, we have a classiﬁer for
each key, where the sole purpose of the classiﬁer is to determine
whether or not its associated key has been pressed given an example
pixel patch as input.

With this set of classiﬁers, we then analyze the key-press events
detected in Stage . Our analyses show that each event typically
lasts anywhere from 10 to 20 frames, and during this time the ﬁn-
ger stays at a relatively ﬁxed location. We collect three adjacent
frames from the middle of the duration, and extract from each a
60 by 100 pixel patch (corresponding in size to the training patch
above) centered at the detected ﬁngertip location. These patches
are then fed into each of the nearby classiﬁers, with each classi-
ﬁer outputting its conﬁdence of a key-press as a value in a range of
[0,1] (with 1 being high conﬁdence). By averaging the conﬁdence
scores of each classiﬁer (one for each key) from the three chosen
frames, we create a set of possible key-presses and their associated
conﬁdence levels. If the maximum conﬁdence in the set falls be-
low a predeﬁned threshold (δ = 0.5), we assume the key-press was
invalid and discard the event. For a valid event with key-press con-
ﬁdence above δ , we record all the possible keys and their scores.
By repeating this process for each key-press event, the ﬁnal output
of this stage is a sequence of key-press sets, where each set has a
list of keys and their associated conﬁdence values.

Note also that we do not explicitly detect presses of the “space”
button, but instead treat longer pauses between key-press events as
the space character. Likewise, shorter pauses are identiﬁed as the
gaps between different letters in a word. Similar ideas for detecting
spaces were utilized by Raguram et al. [34] and Balzarotti et al. [5].
Stage : Language Model
As an optional post-processing step, we can feed the candidate keys
and associated conﬁdence values from the previous stage into a lan-
guage model in order to achieve more accurate and readable results.
Similar to Raguram et al. [34], we view the language model-
ing task as a noisy channel decoding problem and adapt techniques
from the speech recognition community to perform this inference (a
process known as maximum likelihood decoding). In particular, we
employ a cascade of models, each of which represents a stage in the
process of transforming a sequence of candidate character sets with
associated conﬁdence values into words and phrases. Each model
in the cascade is represented as a weighted ﬁnite state transducer
(WFST), i.e., a ﬁnite state machine with an output tape in addi-
tion to the input tape, which maps sequences in one representation
to sequences in another. For instance, speech recognition systems
often include a component (known as the lexicon) which maps se-
quences of phones, the distinct sounds which comprise speech, to
(valid) words. A sequence of such component models can then be
used to model more complex mappings, such as that from an acous-

1068tic signal to a sequence of words and phrases, by composition of the
WFST representations of the component models. With a lexicon as
the ﬁrst component, taking as input a sequence of phones, a second
component (often an n-gram language model) maps the resulting
sequence of words to likely phrases. The composition of these two
models, itself a WFST, maps sequences of phones to likely phrases.
One advantage of the uniform representation of these compo-
nents as WFSTs is that the decoding or inference step (i.e., ﬁnd-
ing the most likely phrase corresponding to the input sequence
of sounds) can be performed on the composition of the compo-
nent models rather than proceeding sequentially through compo-
nent models. In other words, traditional speech recognition cas-
cades were effectively Markov chains:
the output of each com-
ponent depended only on the output of the previous component.
Often, the resource-intensive nature of the speech recognition task
forced the pruning of unlikely sequences, e.g., phones after the ap-
plication of a component model. Thus a sequence of phones which
might be considered unlikely (but not impossible) in the absence
of a language model might be pruned in the early stages of such a
speech recognition system. A WFST cascade, on the other hand,
can be represented as a single WFST, which allows for inference
from all components simultaneously and mitigates many problems
resulting from pruning.

In our case, the input for the language modeling stage is a se-
quence of key-press events with character labels and associated
conﬁdence values, including marked spaces. We then employ a
composite WFST model to smooth over any potential errors in the
earlier stages by modifying the output words and phrases to more
closely match natural English language. We ﬁrst apply an edit dis-
tance model E for error correction. Conceptually, the edit distance
model produces, for each input string, a set of similar strings each
weighted by (keyboard) edit distance from the input string. We then
employ a dictionary model D, which prunes those strings which do
not result in dictionary words, and an n-gram language model L ,
which promotes sequences of words that appear more frequently in
English and demotes those which appear rarely. Then the cascade
model can be represented as C = E ◦D ◦L . To ﬁnd the most likely
sequence of words corresponding to an input sequence of character
sets (as output by the previous stage), the input is ﬁrst represented
as a ﬁnite state acceptor I . Then the shortest path through the
WFST I ◦ C gives the most likely series of English words given
the input sequence of predicted character sets.

Our n-gram language model L is a unigram model trained on
the Brown corpus [14]. The dictionary model D is based on the
‘medium’ word list from the Spell Checker Oriented Word Lists2
with roman numerals, unusual abbreviations, and proper nouns re-
moved. For the keyboard edit distance, we deﬁne the distance be-
tween two keys as the normalized product of the difference in rows
(on a ‘vertical’ axis through Q and Z) plus one and the difference
in keys (on a ‘horizontal’ axis through A and L) plus one. For in-
stance, the (unnormalized) distance between A and B is (1 + 1)∗
(4 + 1) = 10, since A and B are a single row apart vertically and
four keys apart horizontally. We offset by one in each case to avoid
zero values if the keys are in the same row or column.

5. EVALUATION

Recall that one of the key motivating scenarios driving our de-
sign is the ability to reconstruct the typed input from repeated re-
ﬂections. From an adversarial point of view, we are also interested
in understanding the realism of the threat posed by an adversary
performing such an attack from as far as possible, yet relying on

2wordlist.sourceforge.net

Device Name

Focal
Length Aperture

Canon 60D

DSLR,

400mm Lens
Canon VIXIA

HG21

Camcorder

400mm

F/5.6

57mm

F/3.0

Sensor
Size
22.3×
14.9mm
4.84×
3.42mm

Resolution
5184×
3456
1920×
1080

Table 1: Speciﬁcations of the cameras used in our evaluation.

inexpensive equipment. In what follows, we provide an analysis of
our results under these conditions.

For the case of multiple reﬂections, we make use of a Canon 60D
digital SLR ($700) with 400mm Lens ($1340). For experiments
with direct line-of-sight and a single reﬂection, we use a Canon
VIXIA HG21 Camcorder ($1000). These devices are considerably
smaller than the telescopes used previously [2, 3, 21] and are also
more affordable. The speciﬁcations are listed in Table 1.

Figure 7: METEOR scores for our approach, broken down by
scenario.

For our lab-based evaluation, ﬁve different subjects took part in
our experiments. To explore different typing conditions, we asked
the subjects to perform 3 different styles of typing: typing a speciﬁc
sentence, providing a quick response to a question (e.g., as one
might do when text messaging), and typing a password of their
choosing. The chosen sentences are from “The Art of the War” and
the questions from SMS messages collected by NUS [9]. In all,
we collected 73 responses containing 584 total words. In addition,
we collected 15 passwords (each consisting of 5 to 8 lower-case
characters).

To gauge our performance, we evaluate the reconstruction results
for sentences and passwords using separate metrics. The reason is
obvious: the use of a natural language model can easily improve
the result of the former, while for passwords a better measure is to
examine how many guesses are required to recover the password
given our initial hypothesis.

For evaluating the quality of our reconstructed text, we use the
METEOR metric proposed by Lavie and Denkowski [24]. Essen-
tially, METEOR scores machine translation hypotheses (i.e., our
guesses) by aligning them to one or more reference translations.

1069Scenario

Distance

Reference Sentence

Reconstructed Results

Toaster+Sunglasses[C1]

Mirror+Eyeball[C1]

Sunglasses[C2]

Direct view[C2]

3m
3m
3m
3m

4m

10m

30m
50m

when your round is a short one you

when your round is a short one he

take a walk

take a walk

when it is a long one you take a cab

when a is a long is you the a can

when it is a long one

i am busy tonight

when it is a long one

i at be tonight

if you know your enemy and you
know yourself you need not fear the

if you know your enemy and you
know yourself you need not fear the

results of a hundred battles

if you know neither the enemy nor
yourself you will succumb in every

results of a hundred battles

if you his neither the enemy for
yourself to will succumb in every

battle

battle

when your round is a short one you

when your round is a short one you

take a walk

i plan to stay at home

take a walk

i men to stay at home

METEOR

Score

0.88
0.43
1.00
0.33

1.00

0.71

1.00
0.74

Table 2: Example reconstructions. [C1]: Using Canon 60D DSLR($700) with 400mm Lens($1340). [C2]: Using Canon VIXIA HG21
Camcorder($1000).

It estimates human perception of the readability of the sentence in
terms of the adequacy and ﬂuency of the translation. METEOR
assigns scores on a scale from 0.0 to 1.0, with 1.0 indicating a per-
fect translation. As a guide for interpreting METEOR scores, Lavie
[23] suggests that scores of 0.5 or higher indicate understandable
translations, while scores of 0.7 or higher indicate good or ﬂuent
translations. Examples from our experiments are shown in Table 2.
5.1 Results

The aggregate results of our approach, including conﬁdence in-
tervals, are illustrated in Figure 7. Notice that in every circum-
stance, our reconstruction results in an average METEOR score
above 0.5, indicating an understandable level. Additionally, we re-
constructed 23% (17/73) of the sentences perfectly, and 92% (67/73)
of all the sentences have a METEOR score above 0.5. All of the
captured videos were within the automatic focus and exposure lim-
its of the cameras, leading to no unusable videos which might oc-
cur in more challenging environments. We remind the reader that
these results are already obtained in situations where previous ef-
forts [29, 34, 35] fail, either due to an increased distance from the
target or an additional reﬂection. These results, as well as the indi-
vidual examples in Table 2, indicate that our attack is quite stable
to changes in the distance to the target and the number and type of
reﬂecting surfaces.

Closer look: Double Reﬂections
In order to evaluate the performance of double-reﬂection attacks we
conducted the following experiments. First, we position a user with
sunglasses at a desk with a nearby toaster. The user then types on an
iPhone 4, and an attacker observes the typed input from a concealed
location around a corner of 60 to 90 degrees by leveraging the two
reﬂections. For the second setup, the user is once again at a desk,
but instead of relying on the reﬂection from sunglasses, we use
the reﬂection from the user’s own eye. To enable this attack, we
utilized a small mirror instead of the toaster. The use of the mirror
does not inﬂuence the image quality but provides the additional
ability to attack around the corner. The results without the mirror
are similar (Figure 7). These scenarios are illustrated in Figure 8.

The use of multiple reﬂections naturally limits the information
we can acquire from the target. With the double reﬂection, the main
limitation of the system is the contrast between the user’s ﬁngers
and the device’s screen. After two reﬂections, the device’s screen

Figure 8: Left: the cellphone image reﬂects from the user’s
sunglasses, off the toaster’s surface, and is then captured by
the camera. Right: the cellphone image reﬂects from user’s
eyeball, off the mirror, and then is captured by the camera.

can barely be recognized by a human (see Figure 9). The contrast
between the device screen and the background is so low that it is
much harder to extract the device screen and ﬁnger regions. Yet we
are still able to achieve results with an understandable level (0.65).
In Figure 10, we illustrate the effect that contrast has on the ﬁnal
result. Reﬂections signiﬁcantly reduce the contrast of the image.
The RMS contrast (standard deviation of the pixel intensities) takes
values in the range [0.0,0.5], where 0.0 corresponds to an image
with uniform color and 0.5 corresponds to a checkerboard pattern
of black and white squares. The RMS contrast in our experiments
drops from around 0.35 in direct view to 0.03 with two reﬂections.
This leads to difﬁculties in ﬁnger region extraction and therefore
lower METEOR scores.

In the reﬂection off the eyeball (Figure 11), the ﬁnger and key-
board are even more difﬁcult to discern. The image is both small
and signiﬁcantly blurred. Backes et al. [2] discuss the theoretical
boundary that a telescope with a 62 cm aperture from a distance of
2 m would be required in order to obtain a full resolution image
from the reﬂection of an eyeball. In our setting (i.e., a lens with a
7.1 cm aperture from a distance of 3 m), we achieve understandable
reconstruction results with less than 1/10th of the full resolution.
The main problem with the reﬂection from the eyeball is the blur-
ring caused by diffraction and noise, as discussed in Section 3, as
well as partial occlusion by the eyelashes.

1070Figure 9: Example double-reﬂection from sunglasses and a
toaster. Left: Captured image; Right: (correctly) identiﬁed
keypress (T, with conﬁdence 0.55). The brightness of the de-
tected image (right) is adjusted for viewing convenience and
has been overlaid with a reference device layout.

Figure 11: Example double-reﬂection from an eyeball and mir-
ror. Left: Captured image; Right: candidate keys (I with con-
ﬁdence 0.10, L with conﬁdence 0.07, and the correct key O with
conﬁdence 1.00). The right image has been overlaid with a ref-
erence device layout.

Figure 10: Plot of captured RMS contrast versus METEOR
score for the three different scenarios (direct, single-reﬂection,
and repeated reﬂections).

Closer look: Single Reﬂection and Direct Sight
Compared with the prior state of the art, our attack can be executed
from a much farther distance, as illustrated in Figure 12. For exam-
ple, using the same equipment (Canon VIXIA HG21 Camcorder),
the approach of Raguram et al. [35] can only run a direct attack at
24 m and a single-reﬂection attack at 4 m, while our work increased
the distance to be 50 m and 10 m respectively (see Figure 7) while
maintaining comparable results.
Devices without Pop-ups
As a ﬁnal experiment to demonstrate that our design is applicable
to a broad range of devices, we include a preliminary study on a
device (a ﬁrst generation iPad) with a virtual keyboard but no pop-
out events. In this case, the iPad was placed at a distance of 4 meters
from a pair of sunglasses which were in turn situated 4 meters from
our camera. As the iPad’s screen is considerably larger than the
iPhone 4, it is perhaps not surprising that we are able to perfectly
reconstruct all 8 sentences tested.
5.1.1 Password Guessing
We also apply our approach to the reconstruction of passwords.
While passwords typically chosen by users may follow certain dis-
tributions that make them easier to guess, we assume that pass-
words are chosen with sufﬁcient randomness to mitigate any lan-

Figure 12: Example single reﬂection from 10m. Left: Single re-
ﬂection from 10m (correctly predicting I with conﬁdence 1.0).
Right: Direct view from 50m (correctly predicting T with con-
ﬁdence 1.0). Both images have been overlaid with a reference
device layout.

guage modeling, and therefore exclude the optional Stage  from
this portion of the analysis. We instead examine the classiﬁcation
results from Stage  directly. One advantage of our approach is
that the output of Stage  is a sequence of sets of candidate keys
with associated conﬁdence values. We therefore have a natural or-
dering with which we can prioritize our password guessing strategy.
We implement this ordering with a best-ﬁrst search, where the
‘best’ candidate password is the string which maximizes the prod-
uct of the conﬁdence values for the individual keypresses. We can
calculate this product for each child of a given candidate password,
where a child is identical to the candidate except that a single char-
acter has been replaced with the next most likely character for that
position. Beginning with the most likely candidate, i.e., that for
which the individual conﬁdence values are maximized, we check if
the candidate matches the password for which we are looking. If
not, we add each child of the candidate to a priority queue, ordered
by the product of the conﬁdence values. Once the children of one

1071Scenario

Distance

Direct
Sight

Eyeball

and Mirror

Sunglasses
and Toaster

30m

50m

3m

3m

5m

Reference
(Typed)
bjtam
mstys
dlxmzd
zywmxq
wabjta
tatsta
wdlxmzd
jrairbf
hvgcjyx
bgditv
sjyfh
aluhe

kydhwria
jyerpsk
hdyeri

Top Guess

bjtam
matya
elxmad
atahxq
ewbjta
tatata
wdiebae
jfairbf
hvccnyx
cgcitv
sjyfn
aauce

kydhwria
jynraak
hdierx

Number of
Guesses

1
47
64
5967

3
10
277
3
620
3
2
14
1
21
6

Table 3: Expected number of guesses required to identify user
passwords given a ranking, by decreasing conﬁdence, of candi-
dates.

Stage Name

Stage 1: Tracking
Stage 2: Alignment
Stage 3: Finger Extraction
Stage 4: Fingertip Analysis
Stage 5: Key Weight Generation
Stage 6: Language Model
Total

Time Spent
(sec/frame)

Percentage

0.228
1.054
1.042
0.123
0.256
0.007
2.708

8.4%
38.9%
38.4%
4.5%
9.4%
0.2%
100.0%

Table 4: Runtime performance of our approach.

candidate are added, the candidate is discarded, the highest priority
candidate is drawn from the queue, and the process begins anew.
Letters outside of the candidate set are assigned a weight of
10−6, which is signiﬁcantly smaller than the weights of any ob-
served keypresses. This accounts for cases where the actual key
is not contained within the predicted set. We break ties randomly,
i.e., we report the average-case estimates when multiple candidates
have the same value for the product of the conﬁdence values.

Table 3 shows our password guessing results. Of the 15 pass-
words used, 6 were reconstructed in 3 or fewer guesses, 12 in less
than 100 guesses, and all 15 in less than 6000 guesses. Random
guessing of passwords would result in almost 6 million guesses
for 5-letter passwords and almost 155 million guesses for 6-letter
passwords, in expectation. Accordingly, our approach results in a
speedup of four orders of magnitude. That said, we remind the
reader that the passwords used were those chosen by our test sub-
jects. Unfortunately, our test subjects chose passwords which were
between 5 and 8 characters long and consisted of only lower-case
letters. However, since the mobile devices in which we are inter-
ested simply overlay the lower-case keyboard with an upper-case,
numeric, or symbolic keyboard as necessary, extending our analy-
sis to cover these cases would not be difﬁcult.

5.1.2 Runtime Performance
The overall performance is 0.37 frames per second, the compo-
sition of which is shown in Table 4. The current version of our
approach is mainly implemented in MATLAB; only the tracking
scheme in Stage  is optimized with C++ and the use of a GPU. As
such, re-implementation of the remainder of our approach in C++
and/or on the GPU would likely result in a signiﬁcant speedup.

6. MITIGATIONS

Our attack invalidates a signiﬁcant portion of prior defences (e.g.,
like eliminating key pop-up events (Raguram et al.)). Perhaps the
most natural mitigation is to apply a privacy screen/ﬁlm to the de-
vice. This will limit the amount of light emitted, making recon-
struction of reﬂected images (with inherently reduced brightness)
extremely difﬁcult. Moreover, it will signiﬁcantly limit the possi-
bility of direct sight attacks by narrowing the angle of screen obser-
vation. However, many materials are diffuse reﬂectors (reﬂection
in all directions), which may lift the restriction on viewing angle
imposed by privacy ﬁlms by allowing an adversary to exploit the
reﬂection rather than a direct view of the screen.

More esoteric mitigations that are focused on hindering the re-
covery of sensitive input (e.g., passwords) call for the application of
gaze-based passwords [22, 44], shoulder-surﬁng resistant graphical
password schemes (e.g., [17, 36]), and randomized keyboards [39,
47]. Gaze-based password systems, for example, take into consid-
eration the focus of the user’s eyes on the screen as the source of
input, greatly reducing the ability of an adversary without an un-
obstructed view of the user’s eyes and orientation with respect to
the screen [22, 44]. Likewise, randomized keyboards permute on-
screen keyboard layouts as the user enters her password [39, 47],
effectively thwarting shoulder-surﬁng attacks during such times.

Clearly, these proposals would hamper the ability to reconstruct
entered passwords using approaches similar to ours; in particular,
attacking either the graphical password schemes or the randomized
keyboard schemes would require the ability to discern detail on
the screen of the device. That said, all of these proposals are lim-
ited to password entry, and as such, only offer a partial solution to
this challenge of limiting recovery of typed input exposed via com-
promising reﬂections. Moreover, the real-world usability of these
proposals remains unclear.

7. CONCLUSIONS

In this paper, we provide a technique for accurately reconstruct-
ing the text typed on a mobile device, with a broader range of cred-
ible threats that underscore the realism of the attack. Speciﬁcally,
we show that it is possible to perform such attacks in a number of
challenging scenarios: on devices with any type of virtual or phys-
ical keyboard, without direct line-of-sight, and at distances farther
away from the victim than previously thought possible. Our attack
can even directly use reﬂections on the eye-ball, which was not
possible before due to the noise and physical boundaries of the op-
tics in prior work. Our empirical analysis, which covers a number
of scenarios (including direct line-of-sight, single reﬂections, and
repeated reﬂections) as well as a range of distances (3 m - 50 m),
demonstrates the success of our approach and underscores the sig-
niﬁcance of this privacy threat.

8. ACKNOWLEDGEMENTS

We are thankful to the anonymous reviewers for their insightful
comments. This work is supported by a grant from the National
Science Foundation, under award number 1148895.

References
[1] D. Asonov and R. Agrawal. Keyboard acoustic emanations.

In Proceedings of the IEEE Symposium on Security and
Privacy, 2004.

[2] M. Backes, M. Durmuth, and D. Unruh. Compromising

reﬂections-or-how to read LCD monitors around the corner.
In Proceedings of the IEEE Symposium on Security and
Privacy, 2008.

1072[3] M. Backes, T. Chen, M. Dürmuth, H. Lensch, and M. Welk.
Tempest in a teapot: Compromising reﬂections revisited. In
Proceedings of the IEEE Symposium on Security and
Privacy, 2009.

[4] S. Baker and I. Matthews. Lucas-Kanade 20 years on.

International Journal of Computer Vision, 56(3):221–255,
2004.

[5] D. Balzarotti, M. Cova, and G. Vigna. ClearShot:
Eavesdropping on keyboard input from video. In
Proceedings of the IEEE Symposium on Security and
Privacy, 2008.

[6] L. Cai and H. Chen. Touchlogger: inferring keystrokes on

touch screen from smartphone motion. In USENIX Workshop
on Hot Topics in Security (HotSec), 2011.

[7] L. Cai and H. Chen. On the practicality of motion based

keystroke inference attack. Trust and Trustworthy
Computing, pages 273–290, 2012.

[8] A. Chaudhary, J. Raheja, and S. Raheja. A vision based
geometrical method to ﬁnd ﬁngers positions in real time
hand gesture recognition. Journal of Software, 7(4):
861–869, 2012.

[9] T. Chen and M.-Y. Kan. Creating a live, public short

message service corpus: The NUS SMS corpus. Language
Resources and Evaluation, 2011.

[10] R. Collins and R. Weiss. Vanishing point calculation as a

statistical inference on the unit sphere. In Proceedings of the
Third International Conference on Computer Vision, 1990.

[11] R. O. Duda and P. E. Hart. Use of the Hough transformation
to detect lines and curves in pictures. Communications of the
ACM, 15(1):11–15, 1972.

[12] F. Elibol, U. Sarac, and I. Erer. Realistic eavesdropping
attacks on computer displays with low-cost and mobile
receiver system. In Proceedings of the 20th European Signal
Processing Conference, 2012.

[13] M. Fischler and R. Bolles. Random sample consensus: a

paradigm for model ﬁtting with applications to image
analysis and automated cartography. Communications of the
ACM, 24(6):381–395, 1981.

[14] W. N. Francis and H. Kucera. Brown corpus manual.

Technical report, Dept. of Linguistics, Brown University,
1979.

[15] H. Grabner, M. Grabner, and H. Bischof. Real-time tracking
via on-line boosting. In Proceedings of the British Machine
Vision Conference, volume 1, pages 47–56, 2006.

[16] H. J. Highland. Electromagnetic radiation revisited.

Computer Security, 5:85–93, June 1986.

[17] B. Hoanca and K. J. Mock. Password entry scheme resistant

to eavesdropping. In Security and Management, 2008.

[18] X. Iturbe, A. Altuna, A. Ruiz de Olano, and I. Martinez.

VHDL described ﬁnger tracking system for real-time
human-machine interaction. In International Conference on
Signals and Electronic Systems, 2008.

[19] L. Jin, D. Yang, L. Zhen, and J. Huang. A novel vision-based

ﬁnger-writing character recognition system. Journal of
Circuits, Systems, and Computers, 16(03):421–436, 2007.
[20] C. Kerdvibulvech and H. Saito. Vision-based detection of

guitar players’ ﬁngertips without markers. In Computer
Graphics, Imaging and Visualisation, 2007.

[21] M. Kuhn and C. Kuhn. Compromising emanations:

eavesdropping risks of computer displays. Technical report,
University of Cambridge, 2003.

[22] M. Kumar, T. Garﬁnkel, D. Boneh, and T. Winograd.

Reducing shoulder-surﬁng by using gaze-based password
entry. In Symposium on Usable Privacy and Security, 2007.

[23] A. Lavie. Evaluating the output of machine translation

systems. AMTA Tutorial, 2010.

[24] A. Lavie and M. J. Denkowski. The METEOR metric for

automatic evaluation of machine translation. Machine
Translation, 23(2-3):105–115, 2009.

[25] B. Lee and J. Chun. Manipulation of virtual objects in
marker-less AR system by ﬁngertip tracking and hand
gesture recognition. In Proceedings of the 2nd International
Conference on Interaction Sciences, 2009.

[26] T. Lee and T. Hollerer. Handy AR: Markerless inspection of
augmented reality objects using ﬁngertip tracking. In IEEE
International Symposium on Wearable Computers, 2007.
[27] E. Lutton, H. Maitre, and J. Lopez-Krahe. Contribution to

the determination of vanishing points using Hough
transform. Transactions on Pattern Analysis and Machine
Intelligence, 16(4):430–438, 1994.

[28] M. Magee and J. Aggarwal. Determining vanishing points
from perspective images. Computer Vision, Graphics, and
Image Processing, 26(2):256–267, 1984.

[29] F. Maggi, A. Volpatto, S. Gasparini, G. Boracchi, and

S. Zanero. A fast eavesdropping attack against touchscreens.
In Information Assurance and Security (IAS). IEEE, 2011.

[30] J. Nakamura. Image sensors and signal processing for

digital still cameras. CRC, 2005.

[31] D. Nguyen, T. Pham, and J. Jeon. Fingertip detection with

morphology and geometric calculation. In IEEE/RSJ
International Conference on Intelligent Robots and Systems,
2009.

[32] K. Oka, Y. Sato, and H. Koike. Real-time ﬁngertip tracking

and gesture recognition. Computer Graphics and
Applications, 22(6):64–71, 2002.

[33] E. Owusu, J. Han, S. Das, A. Perrig, and J. Zhang.

Accessory: password inference using accelerometers on
smartphones. In Proceedings of the Twelfth Workshop on
Mobile Computing Systems & Applications. ACM, 2012.

[34] R. Raguram, A. White, D. Goswami, F. Monrose, and

J. Frahm. iSpy: automatic reconstruction of typed input from
compromising reﬂections. In Proceedings of the ACM
Conference on Computer and Communications Security,
2011.

[35] R. Raguram, A. M. White, Y. Xu, J.-M. Frahm, P. Georgel,
and F. Monrose. On the privacy risks of virtual keyboards:
automatic reconstruction of typed input from compromising
reﬂections. IEEE Transactions on Dependable and Secure
Computing, 2013.

[36] L. Sobrado and J.-C. Birget. Graphical passwords. The

Rutgers Scholar, 4, 2002.

[37] E. Stelzer. Contrast, resolution, pixelation, dynamic range

and signal-to-noise ratio: fundamental limits to resolution in
ﬂuorescence light microscopy. Journal of Microscopy, 189
(1):15–24, 1998.

[38] R. Szeliski. Image alignment and stitching: A tutorial.

Foundations and Trends in Computer Graphics and Vision,
2006.

[39] D. S. Tan, P. Keyani, and M. Czerwinski. Spy-resistant
keyboard: More secure password entry on public touch
screen displays. In Proceedings of the 17th Australia
Conference on Computer-Human Interaction, 2005.

[40] N. Ukita and M. Kidode. Wearable virtual tablet: ﬁngertip
drawing on a portable plane-object using an active-infrared
camera. In Proceedings of the International Conference on
Intelligent User Interfaces. ACM, 2004.

[41] W. van Eck. Electromagnetic radiation from video display

units: an eavesdropping risk? Computer Security, 4:
269–286, December 1985.

[42] A. Vedaldi and B. Fulkerson. VLFeat: An open and portable
library of computer vision algorithms. In Proceedings of the
International Conference on Multimedia, 2010.

[43] M. Vuagnoux and S. Pasini. Compromising electromagnetic
emanations of wired and wireless keyboards. In Proceedings
of the 18th USENIX Security Symposium, 2009.

[44] J. Weaver, K. J. Mock, and B. Hoanca. Gaze-based password

authentication through automatic clustering of gaze points.
In IEEE International Conference on Systems, Man and
Cybernetics, 2011.

[45] J. Weickert. Anisotropic diffusion in image processing,

volume 1. Teubner Stuttgart, 1998.

[46] D. Yang, L. Jin, and J. Yin. An effective robust ﬁngertip
detection method for ﬁnger writing character recognition
system. In Proceedings of the International Conference on
Machine Learning and Cybernetics, 2005.

1073[47] Y. Zhang, P. Xia, J. Luo, Z. Ling, B. Liu, and X. Fu.

Fingerprint attack against touch-enabled devices. In Security
and Privacy in Smartphones and Mobile Devices, SPSM ’12,
2012.

[48] Z. Zhang. Vision-based interaction with ﬁngers and papers.

In Proceedings International Symposium on the CREST
Digital Archiving Project, 2003.

[49] L. Zhuang, F. Zhou, and J. Tygar. Keyboard acoustic
emanations revisited. In Proceedings of the 12th ACM
Conference on Computer and Communications Security,
2005.

APPENDIX
A. ANISOTROPIC DIFFUSION
Our alignment to the reference layout (Section 4) is based on
image edges. Hence we opt for anisotropic diffusion, which is a
noise suppression algorithm that maintains edges in the image. Our
anisotropic diffusion is in the spirit of the method proposed by We-
ickert [45], which was designed to preserve all detected edges in the
image. In our work however, we only preserve images edges that
are along the dominant directions of the boundaries of the mobile
device. This not only reduces noise but also effectively suppresses
spurious lines, as for example, caused by reﬂections on the screen.
The diffusion process is described by:

∂tu = div((1− c(φ )(1− D(Jρ ((cid:79)uσ ))))(cid:79)u)
(cid:19)
c(θ ) = KΣ(φ − Dir1) + KΣ(φ − Dir2)
D(Jρ ((cid:79)uσ )) = U T

(cid:18) 1/(1 + λ1) 0

U

0

1

(4)



u(cid:48) =

where u is the pixel value of the original image, u(cid:48) is the new pixel
value, and φ is the angle of the eigenvector of the biggest eigen-
value λ1 of Jρ (the Jacobian matrix of the image at pixel u). The
matrix U is the unitary matrix consisting of the eigenvectors of Jρ.
KΣ is a Gaussian kernel with parameter Σ, and Dir1,Dir2 are the
pre-calculated dominant directions. The resulting ﬁlter output has
the following properties:

1) If the edge is strongly deviating from the main direction (i.e.
has a large angle to the dominant direction), it will be blurred since
Equation (4) converges to ∂tu = div((cid:79)u).
2) Edges along the dominant directions will be enhanced along
the edge since D(Jρ ((cid:79)uσ )) becomes very small. This means the
edge will not be blurred and will maintain its high contrast.
3) In addition, the areas alongside the edges in the dominant
directions will be smoothed in the direction parallel to the edge
to suppress the noise since in this direction we also have ∂tu =
div((cid:79)u).
In summary this will ensure an effective noise suppression while
maintaining edges in the dominant directions, which are then used
to align the device image to the reference layout.

B. VANISHING POINTS
Vanishing points are points in the 2D projected image where
lines that are parallel converge. There is a large body of work on
estimating vanishing points. Most of the methods are based on pre-
marked points and mainly aim to solve problems when there is sig-
niﬁcant skew in the directions of the lines, such as [28]. However,
these methods fail in our application because the vanishing points
are so far away that the data matrix becomes singular. Alternative
methods are based on transformations such as the Hough transform
[27] and Sphere representation [10].

Due to its ability to detect multiple vanishing points at once we
chose to use a Hough transform to calculate the vanishing points.
The Hough transform is a voting scheme in the parameter space
where every pixel along a line segment votes for all compatible pa-
rameterizations of its line. The detected lines are then chosen as
the local maxima in the voting space. We use the angular repre-
sentation xcosθ + ysinθ = ρ of line segments for the voting, which
avoids the singularity of vertical lines in the slope and bias param-
eterization of lines. The Hough transformation then provides the

θi,ρi of all lines li in the image. These lines are then used to com-
pute the vanishing point v = (x,y) through solving:

(cid:32) ρ dθ

(cid:19)

(cid:18) x

y

=

dρ cosθ − sinθ
ρ dρ
dθ sinθ + cosθ

(cid:33)

∗

1

dθ /dρ

(5)

for each of the lines. To combine the information from all lines
along each of the major directions we use regression to compute
dθ /dρ and θ. This is then used in solving for the vanishing points
(x,y) with Equation (5), which results in the horizontal vanishing
point vx and the vertical vanishing point vy. Assuming vx,vy are
normalized to satisfy (cid:107)vx(cid:107) = (cid:107)vy(cid:107) = 1 the camera rotation matrix
R is given by

 vx

vy

vx × vy



R =

Applying the inverse rotation ensures the device screen plane and
the coordinate systems x-y-plane are parallel, effectively removing
any rotation-related distortion from the image.

C. TRANSLATION ALIGNMENT
The transformation of a given image to the template can be viewed
as rotation of the camera followed by translation and zoom. Our
method uses the Hough transformation to extract the camera’s ro-
tation, translation, and scale as explained in Section 4. Here, we
introduce how to compute the resulting transformation given esti-
mated parameters.

The vanishing points reveal the rotation information. For in-
stance, all the horizontal lines on the device correspond to a van-
ishing point (x,y) on the image plane, which is calculated as de-
scribed above. The direction of these lines can be calculated as the
direction from the center of the camera to the pixel (x,y). For a
pinhole camera, this 3D direction Direction can be represented as:
(x − cx, y− cy, f )T with (cx, cy) being the coordinate of the cen-
ter of the image and f representing the focal length of the camera
normalized by the physical size of the pixels. These are intrinsic
parameters modeling the imaging parameters of the camera sensor
and lens.

With the vanishing points, we acquire the two dominant direc-
tions of the edges of the device as Directionh = (xh, yh, zh) and
Directionv = (xv,yv,zv). The edges of the device can be trans-
formed to become horizontal and vertical with following equation:

(cid:19)

(cid:18) x(cid:48)

y(cid:48)

=

(cid:32) x−cx
 xh

f xh +
x−cx
f xv +

det

xv
x−cx
f

y−cy
f yh + zh
y−cy
f yv + zv
zh
zv
1

yh
yv
y−cy
f

∗ f

(cid:33)
 +

(cid:19)

(cid:18) cx

cy

Here, point (x,y) in the image is transformed to (x(cid:48),y(cid:48)). This
equation virtually rotates the camera in the 3D space so that the co-
ordinate systems x-y-plane is parallel with the device’s boundary.
As discussed in Section 4, the translation parameters are calculated
using a Hough transform using the weighted line voting. Consid-
ering the parameters acquired by the line matching (translations tx,
ty and scaling sx, sy in x-direction and y-direction respectively), the
transformation performing translation and scale alignment is given
as: (x(cid:48)(cid:48), y(cid:48)(cid:48))T = (sxx(cid:48) +tx, syy(cid:48) +ty)T . This maps the previous trans-
formation result (x(cid:48),y(cid:48)) to the point (x(cid:48)(cid:48),y(cid:48)(cid:48)). In this way, we suc-
cessfully transform the device screen’s image into the template’s
coordinate system.

1074