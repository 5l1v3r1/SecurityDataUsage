Build It, Break It, Fix It: Contesting Secure Development

Andrew Ruef

Dave Levin

Michael Hicks

Michelle L. Mazurek

James Parker

Piotr Mardziel†

University of Maryland

†Carnegie Mellon University

ABSTRACT
Typical security contests focus on breaking or mitigating the
impact of buggy systems. We present the Build-it, Break-it,
Fix-it (BIBIFI) contest, which aims to assess the ability to
securely build software, not just break it. In BIBIFI, teams
build speciﬁed software with the goal of maximizing correct-
ness, performance, and security. The latter is tested when
teams attempt to break other teams’ submissions. Win-
ners are chosen from among the best builders and the best
breakers. BIBIFI was designed to be open-ended—teams
can use any language, tool, process, etc. that they like. As
such, contest outcomes shed light on factors that correlate
with successfully building secure software and breaking inse-
cure software. During 2015, we ran three contests involving
a total of 116 teams and two diﬀerent programming prob-
lems. Quantitative analysis from these contests found that
the most eﬃcient build-it submissions used C/C++, but
submissions coded in other statically-typed languages were
less likely to have a security ﬂaw; build-it teams with di-
verse programming-language knowledge also produced more
secure code. Shorter programs correlated with better scores.
Break-it teams that were also successful build-it teams were
signiﬁcantly better at ﬁnding security bugs.

1.

INTRODUCTION

Cybersecurity contests [24, 25, 11, 27, 13] are popular
proving grounds for cybersecurity talent. Existing contests
largely focus on breaking (e.g., exploiting vulnerabilities or
misconﬁgurations) and mitigation (e.g., rapid patching or
reconﬁguration). They do not, however, test contestants’
ability to build (i.e., design and implement) systems that are
secure in the ﬁrst place. Typical programming contests [35,
2, 21] do focus on design and implementation, but generally
ignore security. This state of aﬀairs is unfortunate because

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 – 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978382

experts have long advocated that achieving security in a
computer system requires treating security as a ﬁrst-order
design goal [32], and is not something that can be added
after the fact. As such, we should not assume that good
breakers will necessarily be good builders [23], nor that top
coders necessarily produce secure systems.

This paper presents Build-it, Break-it, Fix-it (BIBIFI),
a new security contest with a focus on building secure sys-
tems. A BIBIFI contest has three phases. The ﬁrst phase,
Build-it, asks small development teams to build software ac-
cording to a provided speciﬁcation that includes security
goals. The software is scored for being correct, eﬃcient, and
feature-ful. The second phase, Break-it, asks teams to ﬁnd
defects in other teams’ build-it submissions. Reported de-
fects, proved via test cases vetted by an oracle implementa-
tion, beneﬁt a break-it team’s score and penalize the build-it
team’s score; more points are assigned to security-relevant
problems. (A team’s break-it and build-it scores are inde-
pendent, with prizes for top scorers in each category.) The
ﬁnal phase, Fix-it, asks builders to ﬁx bugs and thereby get
points back if the process discovers that distinct break-it
test cases identify the same defect.

BIBIFI’s design aims to minimize the manual eﬀort of
running a contest, helping it scale. BIBIFI’s structure and
scoring system also aim to encourage meaningful outcomes,
e.g., to ensure that the top-scoring build-it teams really pro-
duce secure and eﬃcient software. Behaviors that would
thwart such outcomes are discouraged. For example, break-
it teams may submit a limited number of bug reports per
build-it submission, and will lose points during ﬁx-it for test
cases that expose the same underlying defect or a defect also
identiﬁed by other teams. As such, they are encouraged to
look for bugs broadly (in many submissions) and deeply (to
uncover hard-to-ﬁnd bugs).

In addition to providing a novel educational experience,
BIBIFI presents an opportunity to study the building and
breaking process scientiﬁcally.
In particular, BIBIFI con-
tests may serve as a quasi-controlled experiment that cor-
relates participation data with ﬁnal outcome. By exam-
ining artifacts and participant surveys, we can study how
the choice of build-it programming language, team size and
experience, code size, testing technique, etc. can inﬂuence a
team’s (non)success in the build-it or break-it phases. To the
extent that contest problems are realistic and contest partic-
ipants represent the professional developer community, the
results of this study may provide useful empirical evidence
for practices that help or harm real-world security. Indeed,
the contest environment could be used to incubate ideas to

690improve development security, with the best ideas making
their way to practice.

This paper studies the outcomes of three BIBIFI contests
that we held during 2015, involving two diﬀerent program-
ming problems. The ﬁrst contest asked participants to build
a secure, append-only log for adding and querying events
generated by a hypothetical art gallery security system. At-
tackers with direct access to the log, but lacking an “authen-
tication token,” should not be able to steal or corrupt the
data it contains. The second and third contests were run
simultaneously. They asked participants to build a pair of
secure, communicating programs, one representing an ATM
and the other representing a bank. Attackers acting as a
man in the middle (MITM) should neither be able to steal
information (e.g., bank account names or balances) nor cor-
rupt it (e.g., stealing from or adding money to accounts).
Two of the three contests drew participants from a MOOC
(Massive Online Open Courseware) course on cybersecurity.
These participants (278 total, comprising 109 teams) had an
average of 10 years of programming experience and had just
completed a four-course sequence including courses on se-
cure software and cryptography. The third contest involved
U.S.-based graduate and undergraduate students (23 total,
comprising 6 teams) with less experience and training.

BIBIFI’s design permitted it to scale reasonably well. For
example, one full-time person and two part-time judges ran
the ﬁrst 2015 contest in its entirety. This contest involved
156 participants comprising 68 teams, which submitted more
than 20,000 test cases. And yet, organizer eﬀort was lim-
ited to judging whether the few hundred submitted ﬁxes
addressed only a single conceptual defect; other work was
handled automatically or by the participants themselves.

Rigorous quantitative analysis of the contests’ outcomes
revealed several interesting, statistically signiﬁcant eﬀects.
Considering build-it scores: Writing code in C/C++ in-
creased build-it scores initially, but also increased chances
of a security bug found later.
Interestingly, the increased
insecurity for C/C++ programs appears to be almost en-
tirely attributable to memory-safety bugs. Teams that had
broader programming language knowledge or that wrote less
code also produced more secure implementations. Consid-
ering break-it scores: Larger teams found more bugs during
the break-it phase. Greater programming experience and
knowledge of C were also helpful. Break-it teams that also
qualiﬁed during the build-it phase were signiﬁcantly more
likely to ﬁnd a security bug than those that did not. Use
of advanced tools such as fuzzing or static analysis did not
provide a signiﬁcant advantage among our contest partici-
pants.

We manually examined both build-it and break-it arti-
facts. Successful build-it teams typically employed third-
party libraries—e.g., SSL, NaCL, and BouncyCastle—to im-
plement cryptographic operations and/or communications,
which freed up worry of proper use of randomness, nonces,
etc. Unsuccessful teams typically failed to employ cryptog-
raphy, implemented it incorrectly, used insuﬃcient random-
ness, or failed to use authentication. Break-it teams found
clever ways to exploit security problems; some MITM im-
plementations were quite sophisticated.

In summary, this paper makes two main contributions.
First, it presents BIBIFI, a new security contest that en-
courages building, not just breaking. Second, it presents a
detailed description of three BIBIFI contests along with both

a quantitative and qualitative analysis of the results. We
will be making the BIBIFI code and infrastructure publicly
available so that others may run their own competitions; we
hope that this opens up a line of research built on empir-
ical experiments with secure programming methodologies.1
More information, data, and opportunities to participate are
available at https://builditbreakit.org.
The rest of this paper is organized as follows. We present
the design of BIBIFI in §2 and describe speciﬁcs of the con-
tests we ran in §3. We present the quantitative analysis of
the data we collected from these contests in §4, and qual-
itative analysis in §5. We review related work in §6 and
conclude in §7.

2. BUILD-IT, BREAK-IT, FIX-IT

This section describes the goals, design, and implementa-
tion of the BIBIFI competition. At the highest level, our
aim is to create an environment that closely reﬂects real-
world development goals and constraints, and to encourage
build-it teams to write the most secure code they can, and
break-it teams to perform the most thorough, creative anal-
ysis of others’ code they can. We achieve this through a
careful design of how the competition is run and how vari-
ous acts are scored (or penalized). We also aim to minimize
the manual work required of the organizers—to allow the
contest to scale—by employing automation and proper par-
ticipant incentives.
2.1 Competition phases

We begin by describing the high-level mechanics of what
occurs during a BIBIFI competition. BIBIFI may be ad-
ministered on-line, rather than on-site, so teams may be geo-
graphically distributed. The contest comprises three phases,
each of which last about two weeks for the contests we de-
scribe in this paper.

BIBIFI begins with the build-it phase. Registered build-
it teams aim to implement the target software system ac-
cording to a published speciﬁcation created by the contest
organizers. A suitable target is one that can be completed
by good programmers in a short time (just about two weeks,
for the contests we ran), is easily benchmarked for perfor-
mance, and has an interesting attack surface. The software
should have speciﬁc security goals—e.g., protecting private
information or communications—which could be compro-
mised by poor design and/or implementation. The software
should also not be too similar to existing software to ensure
that contestants do the coding themselves (while still tak-
ing advantage of high-quality libraries and frameworks to
the extent possible). The software must build and run on a
standard Linux VM made available prior to the start of the
contest. Teams must develop using Git [17]; with each push,
the contest infrastructure downloads the submission, builds
it, tests it (for correctness and performance), and updates
the scoreboard. §3 describes the two target problems we
developed: (1) an append-only log; and (2) a pair of com-
municating programs that simulate a bank and an ATM.

1This paper subsumes a previously published short workshop pa-
per [31] and a short invited article [30]. The initial BIBIFI design
and implementation also appeared in those papers, as did a brief
description of a pilot run of the contest. This paper presents
many more details about the contest setup along with a quanti-
tative and qualitative analysis of the outcomes of several larger
contests.

691The next phase is the break-it phase. Break-it teams
can download, build, and inspect all qualifying build-it sub-
missions, including source code; to qualify, the submission
must build properly, pass all correctness tests, and not be
purposely obfuscated (accusations of obfuscation are manu-
ally judged by the contest organizers). We randomize each
break-it team’s view of the build-it teams’ submissions,2 but
organize them by meta-data, such as programming language.
When they think they have found a defect, breakers submit
a test case that exposes the defect and an explanation of
the issue. To encourage coverage, a break-it team may only
submit up a ﬁxed number of test cases per build-it submis-
sion. BIBIFI’s infrastructure automatically judges whether
a submitted test case truly reveals a defect. For example,
for a correctness bug, it will run the test against a reference
implementation (“the oracle”) and the targeted submission,
and only if the test passes on the former but fails on the
latter will it be accepted.3 More points are awarded to bugs
that clearly reveal security problems, which may be demon-
strated using alternative test formats. The auto-judgment
approaches we developed for the two diﬀerent contest prob-
lems are described in §3.

The ﬁnal phase is the ﬁx-it phase. Build-it teams are
provided with the bug reports and test cases implicating
their submission. They may ﬁx ﬂaws these test cases iden-
tify; if a single ﬁx corrects more than one failing test case,
the test cases are “morally the same,” and thus points are
only deducted for one of them. The organizers determine,
based on information provided by the build-it teams and
other assessment, whether a submitted ﬁx is “atomic” in the
sense that it corrects only one conceptual ﬂaw; if not, the
ﬁx is rejected.

Once the ﬁnal phase concludes, prizes are awarded to the
best builders and best breakers as determined by the scoring
system described next.
2.2 Competition scoring

BIBIFI’s scoring system aims to encourage the contest’s
basic goals, which are that the winners of the build-it phase
truly produced the highest quality software, and that the
winners of the break-it phase performed the most thorough,
creative analysis of others’ code. The scoring rules create
incentives for good behavior (and disincentives for bad be-
havior).

2.2.1 Build-it scores
To reﬂect real-world development concerns, the winning
build-it team would ideally develop software that is correct,
secure, and eﬃcient. While security is of primary interest to
our contest, developers in practice must balance these other
aspects of quality against security [1, 36], leading to a set of
trade-oﬀs that cannot be ignored if we wish to understand
real developer decision-making.

To encourage these, each build-it team’s score is the sum
of the ship score4 and the resilience score. The ship score is
composed of points gained for correctness tests and perfor-

2This avoids spurious unfair eﬀects, such as if break-it teams
investigating code in the order in which we give it to them.
3Teams can also earn points by reporting bugs in the oracle, i.e.,
where its behavior contradicts the written speciﬁcation; these re-
ports are judged by the organizers.
4The name is meant to evoke a quality measure at the time soft-
ware is shipped.

mance tests. Each mandatory correctness test is worth M
points, for some constant M , while each optional correct-
ness test is worth M/2 points. Each performance test has
a numeric measure depending on the speciﬁc nature of the
programming project—e.g., latency, space consumed, ﬁles
left unprocessed—where lower measures are better. A test’s
worth is M · (worst − v)/(worst − best), where v is the mea-
sured result, best is the measure for the best-performing sub-
mission, and worst is the worst performing. As such, each
performance test’s value ranges from 0 to M .

The resilience score is determined after the break-it and
ﬁx-it phases, at which point the set of unique defects against
a submission is known. For each unique bug found against a
team’s submission, we subtract P points from its resilience
score; as such, it is non-positive, and the best possible re-
silience score is 0. For correctness bugs, we set P to M/2;
for crashes that violate memory safety we set P to M , and
for exploits and other security property failures we set P to
2M .

2.2.2 Break-it scores
Our primary goal with break-it teams is to encourage
them to ﬁnd as many defects as possible in the submitted
software, as this would give greater conﬁdence in our assess-
ment that one build-it team’s software is of higher quality
than another’s. While we are particularly interested in obvi-
ous security defects, correctness defects are also important,
as they can have non-obvious security implications.

After the break-it phase, a break-it team’s score is the
summed value of all defects they have found, using the above
P valuations. After the ﬁx-it phase, this score is reduced. In
particular, each of the N break-it teams’ scores that identi-
ﬁed the same defect are adjusted to receive P/N points for
that defect, splitting the P points among them.

Through a combination of requiring concrete test cases
and scoring, BIBIFI encourages break-it teams to follow the
spirit of the competition. First, by requiring them to provide
test cases as evidence of a defect or vulnerability, we ensure
they are providing useful bug reports. By providing 4× more
points for security-relevant bugs than for correctness bugs,
we nudge break-it teams to look for these sorts of ﬂaws, and
to not just focus on correctness issues. (But a diﬀerent ratio
might work better; see §2.3.2.) Because break-it teams are
limited to a ﬁxed number of test cases per submission, and
because they could lose points during the ﬁx-it phase for
submitting test cases that could be considered “morally the
same,” break-it teams are encouraged to submit tests that
demonstrate diﬀerent bugs. Limiting per-submission test
cases also encourages examining many submissions. Finally,
because points for defects found by other teams are shared,
break-it teams are encouraged to look for hard-to-ﬁnd bugs,
rather than just low-hanging fruit.
2.3 Discussion

The contest’s design also aims to enable scalability by
reducing work on contest organizers.
In our experience,
BIBIFI’s design succeeds at what it sets out to achieve, but
is not perfect. We close by discussing some limitations.

2.3.1 Minimizing manual effort
Once the contest begins, manual eﬀort by the organizers
is, by design, limited. All bug reports submitted during the
break-it phase are automatically judged by the oracle; orga-

692nizers only need to vet any bug reports against the oracle
itself. Organizers may also need to judge accusations by
breakers of code obfuscation by builders. Finally, organizers
must judge whether submitted ﬁxes address a single defect;
this is the most time-consuming task.
It is necessary be-
cause we cannot automatically determine whether multiple
bug reports against one team map to the same software de-
fect. Instead, we incentivize build-it teams to demonstrate
overlap through ﬁxes; organizers manually conﬁrm that each
ﬁx addresses only a single defect, not several.

Previewing some of the results presented later, we can con-
ﬁrm that the design works reasonably well. For example, as
detailed in Table 4, 68 teams submitted 24,796 test cases in
our Spring 2015 contest. The oracle auto-rejected 15,314 of
these, and build-it teams addressed 2,252 of those remaining
with 375 ﬁxes, a 6× reduction. Most conﬁrmations that a
ﬁx truly addressed a single bug took 1–2 minutes each. Only
30 of these ﬁxes were rejected. No accusations of code ob-
fuscation were made by break-it teams, and few bug reports
were submitted against the oracle. All told, the Spring 2015
contest was successfully managed by one full-time person,
with two others helping with judging.

2.3.2 Limitations
While we believe BIBIFI’s structural and scoring incen-
tives are properly designed, we should emphasize several lim-
itations.

First, there is no guarantee that all implementation de-
fects will be found. Break-it teams may lack the time or
skill to ﬁnd problems in all submissions, and not all submis-
sions may receive equal scrutiny. Break-it teams may also
act contrary to incentives and focus on easy-to-ﬁnd and/or
duplicated bugs, rather than the harder and/or unique ones.
Finally, break-it teams may ﬁnd defects that the BIBIFI in-
frastructure cannot automatically validate, meaning those
defects will go unreported. However, with a large enough
pool of break-it teams, and suﬃciently general defect vali-
dations automation, we still anticipate good coverage both
in breadth and depth.

Second, builders may fail to ﬁx bugs in a manner that is
in their best interests. For example, in not wanting to have
a ﬁx rejected as addressing more than one conceptual defect,
teams may use several speciﬁc ﬁxes when a more general ﬁx
would have been allowed. Additionally, teams that are out
of contention for prizes may simply not participate in the
ﬁx-it phase.5 We observed this behavior for our contests, as
described in §4.5. Both actions decrease a team’s resilience
score (and correspondingly increase breakers’ scores). We
can mitigate these issues with suﬃciently strong incentives,
e.g., by oﬀering prizes to all participants commensurate with
their ﬁnal score, rather than oﬀering prizes only to the top
scorers.

Finally, there are several design points in the problem def-
inition that may skew results. For example, too few correct-
ness tests may leave too many correctness bugs to be found
during break-it (distracting break-it teams’ attention from
security issues); too many correctness tests may leave too
few (meaning teams are diﬀerentiated insuﬃciently by gen-
eral bug-ﬁnding ability). Scoring prioritizes security prob-
lems 4 to 1 over correctness problems, but it is hard to say

5Hiding scores during the contest might help mitigate this, but
would harm incentives during break-it to go after submissions
with no bugs reported against them.

Figure 1: Overview of BIBIFI’s implementation.

what ratio makes the most sense when trying to maximize
real-world outcomes; both higher and lower ratios could be
argued. Finally, performance tests may fail to expose im-
portant design trade-oﬀs (e.g., space vs. time), aﬀecting the
ways that teams approach maximizing their ship scores. For
the contests we report in this paper, we are fairly comfort-
able with these design points. In particular, our earlier con-
test [31] prioritized security bugs 2-to-1 and had fewer in-
teresting performance tests, and outcomes were better when
we increased the ratio.

2.3.3 Discouraging collusion
BIBIFI contestants may form teams however they wish,
and may participate remotely. This encourages wider partic-
ipation, but it also opens the possibility of collusion between
teams, as there cannot be a judge overseeing their communi-
cation and coordination. There are three broad possibilities
for collusion, each of which BIBIFI’s scoring discourages.

First, two break-it teams could consider sharing bugs they
ﬁnd with one another. By scaling the points each ﬁnder of
a particular bug obtains, we remove incentive for them to
both submit the same bugs, as they would risk diluting how
many points they both obtain.

The second class of collusion is between a build-it team
and a break-it team, but neither have incentive to assist
one another. The zero-sum nature of the scoring between
breakers and builders places them at odds with one another;
revealing a bug to a break-it team hurts the builder, and not
reporting a bug hurts the breaker.

Finally, two build-it teams could collude, for instance by
sharing code with one another. It might be in their inter-
ests to do this in the event that the competition oﬀers prizes
to two or more build-it teams, since collusion could obtain
more than one prize-position. We use judging and auto-
mated tools (and feedback from break-it teams) to detect if
two teams share the same code (and disqualify them), but it
is not clear how to detect whether two teams provided out-
of-band feedback to one another prior to submitting code
(e.g., by holding their own informal “break-it” and “ﬁx-it”
stages). We view this as a minor threat to validity; at the
surface, such assistance appears unfair, but it is not clear
that it is contrary to the goals of the contest, that is, to
develop secure code.
2.4 Implementation

Figure 1 provides an overview of the BIBIFI implementa-
tion. It consists of a web frontend, providing the interface to
both participants and organizers, and a backend for testing
builds and breaks. Two key goals of the infrastructure are

BIBIFI InfrastructureContestWebsiteSurveysScoreboardBreak-it subsContest DatabaseBuild-it subsMetadataGit ListenerTesterAmazon EC2Contest VMBuildTestBenchmarkOracleBuild-it &Break-itTeamsOrganizersGit ReposParticipants693security—we do not want participants to succeed by hacking
BIBIFI itself—and scalability.

Web frontend. Contestants sign up for the contest through
our web application frontend, and ﬁll out a survey when do-
ing so, to gather demographic and other data potentially rel-
evant to the contest outcome (e.g., programming experience
and security training). During the contest, the web appli-
cation tests build-it submissions and break-it bug reports,
keeps the current scores updated, and provides a workbench
for the judges for considering whether or not a submitted ﬁx
covers one bug or not.
To secure the web application against unscrupulous par-
ticipants, we implemented it in ∼11,000 lines of Haskell us-
ing the Yesod [39] web framework backed by a PostgreSQL
[29] database. Haskell’s strong type system defends against
use-after-free, buﬀer overrun, and other memory safety-based
attacks. The use of Yesod adds further automatic protec-
tion against various attacks like CSRF, XSS, and SQL in-
jection. As one further layer of defense, the web applica-
tion incorporates the information ﬂow control framework
LMonad [26], which is derived from LIO [34], in order to
protect against inadvertent information leaks and privilege
escalations. LMonad dynamically guarantees that users can
only access their own information.

Testing backend. The backend infrastructure is used dur-
ing the build-it phase to test for correctness and perfor-
mance, and during the break-it phase to assess potential
It consists of ∼5,100 lines of Haskell code
vulnerabilities.
(and a little Python).

To automate testing, we require contestants to specify a
URL to a Git [17] repository hosted on either Github or
Bitbucket, and shared with a designated bibifi username,
read-only. The backend “listens” to each contestant reposi-
tory for pushes, upon which it downloads and archives each
commit. Testing is then handled by a scheduler that spins
up an Amazon EC2 virtual machine which builds and tests
each submission. We require that teams’ code builds and
runs, without any network access, in an Ubuntu Linux VM
that we share in advance. Teams can request that we in-
stall additional packages not present on the VM. The use of
VMs supports both scalability (Amazon EC2 instances are
dynamically provisioned) and security (using fresh VM in-
stances prevents a team from aﬀecting the results of future
tests, or of tests on other teams’ submissions).

All qualifying build-it submissions may be downloaded
by break-it teams at the start of the break-it phase. As
break-it teams identify bugs, they prepare a (JSON-based)
ﬁle specifying the buggy submission along with a sequence
of commands with expected outputs that demonstrate the
bug. Break-it teams commit and push this ﬁle (to their Git
repository). The backend uses the ﬁle to set up a test of the
implicated submission to see if it indeed is a bug.

3. CONTEST PROBLEMS

This section presents the two programming problems we
developed for the contests held during 2015, including prob-
lem-speciﬁc notions of security defect and how breaks ex-
ploiting such defects are automatically judged.

3.1 Secure log (Spring 2015)

The secure log problem was motivated as support for an
art gallery security system. Contestants write two programs.
The ﬁrst, logappend, appends events to the log; these events
indicate when employees and visitors enter and exit gallery
rooms. The second, logread, queries the log about past
events. To qualify, submissions must implement two basic
queries (involving the current state of the gallery and the
movements of particular individuals), but they could im-
plement two more for extra points (involving time spent in
the museum, and intersections among diﬀerent individuals’
histories). An empty log is created by logappend with a
given authentication token, and later calls to logappend and
logread on the same log must use that token or the requests
will be denied.

A canonical way of implementing the secure log is to treat
the authentication token as a symmetric key for authen-
ticated encryption, e.g., using a combination of AES and
HMAC. There are several tempting shortcuts that we antic-
ipated build-it teams would take (and that break-it teams
would exploit). For instance, one may be tempted to en-
crypt and sign individual log records as opposed to the en-
tire log, thereby making logappend faster. But this could
permit integrity breaks that duplicate or reorder log records.
Teams may also be tempted to implement their own encryp-
tion rather than use existing libraries, or to simply sidestep
encryption altogether. §5 reports several cases we observed.
A submission’s performance is measured in terms of time
to perform a particular sequence of operations, and space
consumed by the resulting log. Correctness (and crash) bug
reports comprise sequences of logread and/or logappend
operations with expected outputs (vetted by the oracle).
Security is deﬁned by privacy and integrity: any attempt
to learn something about the log’s contents, or to change
them, without the use of the logread and logappend and
the proper token should be disallowed. How violations of
these properties are speciﬁed and tested is described next.

Privacy breaks. When providing a build-it submission to
the break-it teams, we also included a set of log ﬁles that
were generated using a sequence of invocations of that sub-
mission’s logappend program. We generated diﬀerent logs
for diﬀerent build-it submissions, using a distinct command
sequence and authentication token for each. All logs were
distributed to break-it teams without the authentication to-
ken; some were distributed without revealing the sequence
of commands (the “transcript”) that generated them. For
these, a break-it team could submit a test case involving
a call to logread (with the authentication token omitted)
that queries the ﬁle. The BIBIFI infrastructure would run
the query on the speciﬁed ﬁle with the authentication token,
and if the output matched that speciﬁed by the breaker, then
a privacy violation is conﬁrmed.

Integrity breaks. For about half of the generated log ﬁles
we also provided the transcript of the logappend operations
(sans auth token) used to generate the ﬁle. A team could
submit a test case specifying the name of the log ﬁle, the
contents of a corrupted version of that ﬁle, and a logread
query over it (without the authentication token). For both
the speciﬁed log ﬁle and the corrupted one, the BIBIFI in-
frastructure would run the query using the correct authenti-

694cation token. An integrity violation is detected if the query
command produces a non-error answer for the corrupted log
that diﬀers from the correct answer (which can be conﬁrmed
against the transcript using the oracle).

This approach to determining privacy and integrity breaks
has the drawback that it does not reveal the source of the
issue, only that there is (at least) one. As such, we cannot
automatically tell two privacy or two integrity breaks apart.
We sidestep this issue by counting only up to one integrity
break and one privacy break against the score of each build-
it submission, even if there are multiple defects that could
be exploited to produce privacy/integrity violations.

3.2 Securing ATM interactions (Fall 2015)

The ATM problem asks builders to construct two com-
municating programs: atm acts as an ATM client, allowing
customers to set up an account, and deposit and withdraw
money, while bank is a server that processes client requests,
tracking bank balances. atm and bank should only permit
a customer with a correct card ﬁle to learn or modify the
balance of their account, and only in an appropriate way
(e.g., they may not withdraw more money than they have).
In addition, atm and bank should only communicate if they
can authenticate each other. They can use an auth ﬁle for
this purpose; it will be shared between the two via a trusted
channel unavailable to the attacker.6 Since the atm is com-
municating with bank over the network, a “man in the mid-
dle” (MITM) could observe and modify exchanged messages,
or insert new messages. The MITM could try to compromise
security despite not having access to auth or card ﬁles.

A canonical way of implementing the atm and bank pro-
grams would be to use public key-based authenticated and
encrypted communications. The auth ﬁle could be used as
the bank’s public key to ensure that key negotiation initi-
ated by the atm is with the bank and not the MITM. When
creating an account, the card ﬁle should be a suitably large
random number, so that the MITM is unable to feasibly pre-
dict it. It is also necessary to protect against replay attacks
by using nonces or similar mechanisms. As with the secure
log, a wise approach would be use a library like OpenSSL to
implement these features. Both good and bad implementa-
tions we observed in the competition are discussed further
in §5.

Build-it submissions’ performance is measured as the time

to complete a series of benchmarks involving various atm/bank
interactions.7 Correctness (and crash) bug reports comprise
sequences of atm commands where the targeted submission
produces diﬀerent outputs than the oracle (or crashes). Se-
curity defects are speciﬁed as follows.

Integrity breaks. Integrity violations are demonstrated us-
ing a custom MITM program that acts as a proxy: It listens
on a speciﬁed IP address and TCP port,8 and accepts a
connection from the atm while connecting to the bank. The
MITM program can thus observe and/or modify commu-
nications between atm and bank, as well as drop messages
or initiate its own. We provided a Python-based proxy as

6In a real deployment, this might be done by “burning” the auth
ﬁle into the ATM’s ROM prior to installing it.
7This transcript was always serial, so there was no direct moti-
vation to support parallelism for higher throughput.
8All submissions were required to communicate via TCP.

a starter MITM: It sets up the connections and forwards
communications between the two endpoints.

To demonstrate an integrity violation, the MITM sends
requests to a command server. It can tell the server to run
inputs on the atm and it can ask for the card ﬁle for any
account whose creation it initiated. Eventually the MITM
will declare the test complete. At this point, the same set of
atm commands is run using the oracle’s atm and bank without
the MITM. This means that any messages that the MITM
sends directly to the target submission’s atm or bank will not
be replayed/sent to the oracle. If the oracle and target both
complete the command list without error, but they diﬀer on
the outputs of one or more commands, or on the balances
of accounts at the bank whose card ﬁles were not revealed
to the MITM during the test, then there is evidence of an
integrity violation.

As an example (based on a real attack we observed), con-
sider a submission that uses deterministic encryption with-
out nonces in messages. The MITM could direct the com-
mand server to withdraw money from an account, and then
replay the message it observes. When run on the vulner-
able submission, this would debit the account twice. But
when run on the oracle without the MITM, no message is
replayed, leading to diﬀering ﬁnal account balances. A cor-
rect submission would reject the replayed message, which
would invalidate the break.

Privacy breaks. Privacy violations are also demonstrated
using a MITM. In this case, at the start of a test the com-
mand server will generate two random values, “amount” and
“account name.” If by the end of the test no errors have oc-
curred and the attacker can prove it knows the actual value
of either secret (by sending a command that speciﬁes it), the
break is considered successful. Before demonstrating knowl-
edge of the secret, the MITM can send commands to the
server with a symbolic “amount” and “account name”; the
server ﬁlls in the actual secrets before forwarding these mes-
sages. The command server does not automatically create a
secret account or an account with a secret balance; it is up to
the breaker to do that (referencing the secrets symbolically
when doing so).

As an example, suppose the target does not encrypt ex-
changed messages. Then a privacy attack might be for the
MITM to direct the command server to create an account
whose balance contains the secret amount. Then the MITM
can observe an unencrypted message sent from atm to bank;
this message will contain the actual amount, ﬁlled in by the
command server. The MITM can then send its guess to the
command server showing that it knows the amount.

As with the log problem, we cannot tell whether an in-
tegrity or privacy test is exploiting the same underlying
weakness in a submission, so we only accept one violation of
each category against each submission.

Timeouts and denial of service. One diﬃculty with our
use of a MITM is that we cannot reliably detect bugs in
submissions that would result in inﬁnite loops, missed mes-
sages, or corrupted messages. This is because such bugs can
be simulated by the MITM by dropping or corrupting mes-
sages it receives. Since the builders are free to implement
any protocol they like, our auto-testing infrastructure can-
not tell if a protocol error or timeout is due to a bug in the
target or due to misbehavior of the MITM. As such, we con-

695servatively disallow reporting any such errors. Such ﬂaws in
builder implementations might exist but evidence of those
bugs might not be realizable in our testing system.

Contest

USA Brazil Russia

India Other

Spring 2015
Fall 2015

30
64

12
20

12
12

7
14

120
110

4. QUANTITATIVE ANALYSIS

This section analyzes data we have gathered from three
contests we ran during 2015.9 We consider participants’
performance in each phase of the contest, including which
factors contribute to high scores after the build-it round, re-
sistance to breaking by other teams, and strong performance
as breakers.

We ﬁnd that on average, teams that program in languages
other than C and C++, and those whose members know
more programming languages (perhaps a proxy for overall
programming skill), are less likely to have security bugs iden-
tiﬁed in their code. However, when memory management
bugs are not included, programming language is no longer
a signiﬁcant factor, suggesting that memory safety is the
main discriminator between C/C++ and other languages.
Success in breaking, and particularly in identifying security
bugs in other teams’ code, is correlated with having more
team members, as well as with participating successfully in
the build-it phase (and therefore having given thought to
how to secure an implementation). Somewhat surprisingly,
use of advanced techniques like fuzzing and static analysis
did not appear to aﬀect breaking success. Overall, integrity
bugs were far more common than privacy bugs or crashes.
The Fall 2015 contest, which used the ATM problem, was
associated with more security bugs than the Spring 2015
secure log contest.
4.1 Data collection

For each team, we collected a variety of observed and self-
reported data. When signing up for the contest, teams re-
ported standard demographics and features such as coding
experience and programming language familiarity. After the
contest, each team member optionally completed a survey
about their performance.
In addition, we extracted infor-
mation about lines of code written, number of commits,
etc. from teams’ Git repositories.

Participant data was anonymized and stored in a manner
approved by our institution’s human-subjects review board.
Participants consented to have data related to their activi-
ties collected, anonymized, stored, and analyzed. A few par-
ticipants did not consent to research involvement, so their
personal data was not used in the data analysis.
4.2 Analysis approach

To examine factors that correlated with success in build-
ing and breaking, we apply regression analysis. Each regres-
sion model attempts to explain some outcome variable using
one or more measured factors. For most outcomes, such as
participants’ scores, we can use ordinary linear regression,
which estimates how many points a given factor contributes
to (or takes away from) a team’s score. To analyze binary
outcomes, such as whether or not a security bug was found,
we apply logistic regression. This allows us to estimate how
each factor impacts the likelihood of an outcome.

We consider many variables that could potentially impact
teams’ results. To avoid over-ﬁtting, we initially select as

9We also ran a contest during Fall 2014 [31] but exclude it from
consideration due to diﬀerences in how it was administered.

Table 1: Contestants, by self-reported country.

potential factors those variables that we believe are of most
interest, within acceptable limits for power and eﬀect size.
(Our choices are detailed below.) In addition, we test mod-
els with all possible combinations of these initial factors and
select the model with the minimum Akaike Information Cri-
terion (AIC) [4]. Only the ﬁnal models are presented.

This was not a completely controlled experiment (e.g., we
do not use random assignment), so our models demonstrate
correlation rather than causation. Our observed eﬀects may
involve confounding variables, and many factors used as in-
dependent variables in our data are correlated with each
other. This analysis also assumes that the factors we exam-
ine have linear eﬀect on participants’ scores (or on likelihood
of binary outcomes); while this may not be the case in real-
ity, it is a common simpliﬁcation for considering the eﬀects
of many factors. We also note that some of the data we an-
alyze is self-reported, and thus may not be entirely precise
(e.g., some participants may have exaggerated about which
programming languages they know); however, minor devia-
tions, distributed across the population, act like noise and
have little impact on the regression outcomes.
4.3 Contestants

We consider three contests oﬀered at two times:
Spring 2015: We held one contest during May–June 2015
as the capstone to a Cybersecurity MOOC sequence.10 Be-
fore competing in the capstone, participants passed courses
on software security, cryptography, usable security, and hard-
ware security. The contest problem was the secure log prob-
lem (§3.1).

Fall 2015: During Oct.–Nov. 2015, we oﬀered two con-
tests simultaneously, one as a MOOC capstone, and the
other open to U.S.-based graduate and undergraduate stu-
dents. We merged the contests after the build-it phase, due
to low participation in the open contest; from here on we
refer to these two as a single contest. The contest problem
was the ATM problem (§3.2).

The U.S. had more contestants than any other country.
There was representation from developed countries with a
reputation both for high technology and hacking acumen.
Details of the most popular countries of origin can be found
in Table 1, and additional information about contestant de-
mographics is presented in Table 2.
4.4 Ship scores

We ﬁrst consider factors correlating with a team’s ship
score, which assesses their submission’s quality before it is
attacked by the other teams (§2.1). This data set contains all
101 teams from the Spring 2015 and Fall 2015 contests that
qualiﬁed after the build-it phase. Both contests have nearly
the same number of correctness and performance tests, but
diﬀerent numbers of participants. We set the constant mul-

10https://www.coursera.org/specializations/cyber-security

696Contest

# Contestants
% Male
% Female
Age
% with CS degrees
Years programming
# Build-it teams
Build-it team size
# Break-it teams
(that also built)
Break-it team size
# PLs known per team

Spring ’15†
156
91%
5%
34.8/20/61
35%
9.6/0/30
61
2.2/1/5
65
(58)
2.4/1/5
6.8/1/22

Fall ’15†
122
89%
9%
33.5/19/69
38%
9.9/0/37
34
3.1/1/5
39
(32)
3.0/1/5
10.0/2/20

Fall ’15

23
100%
0%
25.1/17/31
23%
6.6/2/13
6
3.1/1/6
4
(3)
3.5/1/6
4.2/1/8

Table 2: Demographics of contestants from qualifying teams.
† indicates MOOC participants. Some participants declined
to specify gender. Slashed values represent mean/min/max.

tiplier M to be 50 for both contests, which eﬀectively nor-
malizes the scores.

Model setup. To ensure enough power to ﬁnd meaning-
ful relationships, we decided to aim for a prospective eﬀect
size roughly equivalent to Cohen’s medium eﬀect heuristic,
f 2 = 0.15 [7]. An eﬀect this size suggests the model can
explain up to 13% of the variance in the outcome variable.
With an assumed power of 0.75 and population N = 101,
we limited ourselves to nine degrees of freedom, which yields
a prospective f 2 = 0.154. (Observed eﬀect size for the ﬁnal
model is reported with the regression results below.) Within
this limit, we selected the following potential factors:

Contest: Whether the team’s submission was for the

Spring 2015 contest or the Fall 2015 contest.

# Team members: A team’s size.
Knowledge of C: The fraction of team members who
listed C or C++ as a programming language they know. We
included this variable as a proxy for comfort with low-level
implementation details, a skill often viewed as a prerequisite
for successful secure building or breaking.

# Languages known: How many unique programming
languages team members collectively claim to know (see the
last row of Table 2). For example, on a two-member team
where member A claims to know C++, Java, and Perl and
member B claims to know Java, Perl, Python, and Ruby,
the language count would be 5.

Coding experience: The average years of programming

experience reported by a team’s members.

Language category: We manually identiﬁed each team’s
submission as having one “primary” language. These lan-
guages were then assigned to one of three categories: C/C++,
statically-typed (e.g., Java, Go, but not C/C++) and dy-
namically-typed (e.g., Perl, Python). C/C++ is the baseline
category for the regression. Precise category allocations, and
total submissions for each language, segregated by contest,
are given in Figure 2.

Lines of code: The SLOC11 count of lines of code for

the team’s ﬁnal submission at qualiﬁcation time.

MOOC: Whether the team was participating in the MOOC

capstone project.

11http://www.dwheeler.com/sloccount

Figure 2: The number of build-it submissions in each con-
test, organized by primary programming language used.
The brackets group the languages into categories.

Results. Our regression results (Table 3) indicate that ship
score is strongly correlated with language choice. Teams
that programmed in C or C++ performed on average 121
and 92 points better than those who programmed in dy-
namically typed or statically typed languages, respectively.
Figure 3 illustrates that while teams in many language cate-
gories performed well in this phase, only teams that did not
use C or C++ scored poorly.

The high scores for C/C++ teams could be due to bet-
ter scores on performance tests and/or due to implementing
optional features. We conﬁrmed the main cause is the for-
mer. Every C/C++ team for Spring 2015 implemented all
optional features, while six teams in the other categories
implemented only 6 of 10, and one team implemented none;
the Fall 2015 contest oﬀered no optional features. We artiﬁ-
cially increased the scores of those seven teams as if they had
implemented all optional features and reran the regression
model. The resulting model had very similar coeﬃcients.

Our results also suggest that teams that were associated
with the MOOC capstone performed 119 points better than
non-MOOC teams. MOOC participants typically had more
programming experience and CS training.

Finally, we found that each additional line of code in a
team’s submission was associated with a drop of 0.03 points
in ship score. Based on our qualitative observations (see §5),
we hypothesize this may relate to more reuse of code from
libraries, which frequently are not counted in a team’s LOC
(most libraries were installed directly on the VM, not in the
submission itself). We also found that, as further noted be-
low, submissions that used libraries with more sophisticated,
lower-level interfaces tended to have more code and more
mistakes; their use required more code in the application,
lending themselves to missing steps or incorrect use, and
thus security and correctness bugs. As shown in Figure 3,
LOC is also (as expected) associated with the category of
language being used. While LOC varied widely within each
language type, dynamic submissions were generally short-

697Figure 3: Each team’s ship score, compared to the lines
of code in its implementation and organized by language
category. Fewer LOC and using C/C++ correlate with a
higher ship score.

Factor

Coef.

SE p-value

Fall 2015
Lines of code
Dynamically typed
Statically typed
MOOC

-21.462
-0.031
-120.577
-91.782
119.359

28.359
0.014
40.953
39.388
58.375

0.451
0.036*
0.004*
0.022*
0.044*

Table 3: Final linear regression model of teams’ ship scores,
indicating how many points each selected factor adds to the
total score. Overall eﬀect size f 2 = 0.163.

est, followed by static submissions, and then those written
in C/C++ (which has the largest minimum size).
4.5 Code quality measures

Now we turn to measures of a build-it submission’s quality—

in terms of its correctness and security—based on how it held
up under scrutiny by break-it teams.

Resilience. The total build-it score is the sum of ship score,
just discussed, and resilience. Resilience is a non-positive
score that derives from break-it teams’ test cases that prove
the presence of defects. Builders may increase this score
during the ﬁx-it phase, as ﬁxes prevent double-counting test
cases that identify the same defect (see §2.1).

Unfortunately, upon studying the data we found that a
large percentage of build-it teams opted not to ﬁx any bugs
reported against their code, forgoing the scoring advantage
of doing so. We can see this in Figure 4, which graphs the
resilience scores (Y-axis) of all teams, ordered by score, for
the two contests. The circles in the plot indicate teams that
ﬁxed at least one bug, whereas the triangles indicate teams
that ﬁxed no bugs. We can see that, overwhelmingly, the
teams with the lower resilience scores did not ﬁx any bugs.
We further conﬁrmed that ﬁxing, or not, was a dominant
factor by running a regression on resilience score that in-
cluded ﬁx-it phase participation as a factor (not shown).
Overall, teams ﬁxed an average of 34.5% of bugs in Spring

Figure 4: Final resilience scores, ordered by team, and plot-
ted for each contest problem. Build-it teams who did not
bother to ﬁx bugs generally had lower scores.

Spring 2015 Fall 2015

Bug reports submitted
Bug reports accepted
Fixes submitted
Bugs addressed by ﬁxes

24,796
9,482
375
2,252

3,701
2,482
166
966

Table 4: Break-it teams in each contest submitted bug re-
ports, which were judged by the automated oracle. Build-it
teams then submitted ﬁxes, each of which could potentially
address multiple bug reports.

2015 and 45.3% of bugs in Fall 2015. Counting only “active”
ﬁxers who ﬁxed at least one bug, these averages were 56.9%
and 72.5% respectively.

Table 4 digs a little further into the situation. It shows
that of the bug reports deemed acceptable by the oracle (the
second row), submitted ﬁxes (row 3) addressed only 23% of
those from Spring 2015 and 38% of those from Fall 2015
(row 4 divided by row 2).

This situation is disappointing, as we cannot treat re-
silience score as a good measure of code quality (when added
to ship score). Our hypothesis is that participants were not
suﬃciently incentivized to ﬁx bugs, for two reasons. First,
teams that are suﬃciently far from the lead may have cho-
sen to ﬁx no bugs because winning was unlikely. Second,
for MOOC students, once a minimum score is achieved they
were assured to pass; it may be that ﬁxing (many) bugs
was unnecessary for attaining this minimum score. We are
exploring alternative reward structures that more strongly
incentivize all teams to ﬁx all (duplicated) bugs.

Presence of security bugs. While resilience score is not
suﬃciently meaningful, a useful alternative is the likelihood
that a build-it submission contains a security-relevant bug;
by this we mean any submission against which at least one
crash, privacy, or integrity defect is demonstrated. In this
model we used logistic regression over the same set of factors
as the ship model.

Table 5 lists the results of this logistic regression; the co-
eﬃcients represent the change in log likelihood associated

698Factor

Coef. Exp(coef )

SE p-value

Fall 2015
# Languages known
Lines of code
Dynamically typed
Statically typed
MOOC

5.692
-0.184
0.001
-0.751
-2.138
2.872

296.395
0.832
1.001
0.472
0.118
17.674

1.374 <0.001*
0.086
0.033*
0.030*
0.0003
0.393
0.879
0.016*
0.889
1.672
0.086

Table 5: Final logistic regression model, measuring log like-
lihood of a security bug being found in a team’s submission.

with each factor. Negative coeﬃcients indicate lower likeli-
hood of ﬁnding a security bug. For categorical factors, the
exponential of the coeﬃcient (Exp(coef)) indicates roughly
how strongly that factor being true aﬀects the likelihood
relative to the baseline category.12 For numeric factors, the
exponential indicates how the likelihood changes with each
unit change in that factor.
Fall 2015 implementations were 296× as likely as Spring
2015 implementations to have a discovered security bug.13
We hypothesize this is due to the increased security design
space in the ATM problem as compared to the gallery prob-
lem. Although it is easier to demonstrate a security error
in the gallery problem, the ATM problem allows for a much
more powerful adversary (the MITM) that can interact with
the implementation; breakers often took advantage of this
capability, as discussed in §5.

The model also shows that C/C++ implementations were
more likely to contain an identiﬁed security bug than either
static or dynamic implementations. For static languages,
this eﬀect is signiﬁcant and indicates that a C/C++ program
was about 8.5× (that is, 1/0.118) as likely to contain an
identiﬁed bug. This eﬀect is clear in Figure 5, which plots
the fraction of implementations that contain a security bug,
broken down by language type and contest problem. Of the
16 C/C++ submissions (see Figure 2), 12 of them had a
security bug: 5/9 for Spring 2015 and 7/7 for Fall 2015.
All 5 of the buggy implementations from Spring 2015 had a
crash defect, and this was the only security-related problem
for three of them; none of the Fall 2015 implementations had
crash defects.

If we reclassify crash defects as not security relevant and
rerun the model we ﬁnd that the impact due to language cat-
egory is no longer statistically signiﬁcant. This may indicate
that lack of memory safety is the main disadvantage to using
C/C++ from a security perspective, and thus a memory-safe
C/C++ could be of signiﬁcant value. Figure 6 shows how
many security bugs of each type (memory safety, integrity,
privacy) were found in each language category, across both
contests. This ﬁgure reports bugs before uniﬁcation dur-
ing the ﬁx-it phase, and is of course aﬀected by diﬀerences
among teams’ skills and language choices in the two contests,
but it provides a high-level perspective.

Our model shows that teams that knew more unique lan-
guages (even if they did not use those languages in their sub-
mission) performed slightly better, about 1.2× for each lan-

12In cases (such as the Fall 2015 contest) where the rate of security
bug discovery is close to 100%, the change in log likelihood starts
to approach inﬁnity, somewhat distorting this coeﬃcient upwards.
13This coeﬃcient is somewhat exaggerated (see prior footnote),
but the diﬀerence between contests is large and signiﬁcant.

Figure 5: The fraction of teams in whose submission a secu-
rity bug was found, for each contest and language category.

Figure 6: How many of each type of security bug were found,
across both contests, for each language category. Counts are
normalized by the number of qualiﬁed Build-it submissions
in each language category.

guage known. Additional LOC in an implementation were
also associated with a very small increase in the presence of
an identiﬁed security bug.

Finally, the model shows two factors that played a role
in the outcome, but not in a statistically signiﬁcant way:
using a dynamically typed language, and participating in
the MOOC. We see the eﬀect of the former in Figure 5. For
the latter, the eﬀect size is quite large; it is possible that the
MOOC security training played a role.

4.6 Breaking success

Now we turn our attention to break-it team performance,
i.e., how eﬀective teams were at ﬁnding defects in others’
submissions. First, we consider how and why teams per-
formed as indicated by their (normalized) break-it score
prior to the ﬁx-it phase. We do this to measure a team’s
raw output, ignoring whether other teams found the same
bug (which we cannot assess with conﬁdence due to the lack
of ﬁx-it phase participation per §4.5). This data set includes
108 teams that participated in the break-it phase in Spring
and Fall 2015. We also model which factors contributed
to security bug count, or how many total security bugs a
break-it team found. Doing this disregards a break-it team’s
eﬀort at ﬁnding correctness bugs.

699Factor

Coef.

SE p-value

Fall 2015
# Team members
Knowledge of C
Coding experience
Build participant

-2406.89
430.01
-1591.02
99.24
1534.13

685.73 <0.001*
193.22
0.028*
0.117
1006.13
0.056
51.29
995.87
0.127

Table 6: Final linear regression model of teams’ break-it
scores, indicating how many points each selected factor adds
to the total score. Overall eﬀect size f 2 = 0.039.

We model both break-it score and security bug count us-
ing several of the same potential factors as discussed pre-
viously, but applied to the breaking team rather than the
building team. In particular, we include which contest they
participated in, whether they were MOOC participants, the
number of break-it Team members, average team-member
Coding experience, average team-member Knowledge
of C, and unique Languages known by the break-it team
members. We also add two new potential factors:

Build participant: Whether the breaking team also

qualiﬁed during the build-it phase.

Advanced techniques: Whether the breaking team re-
ported using software analysis or fuzzing to aid in bug ﬁnd-
ing. Teams that only used manual inspection and testing
are categorized as false. 26 break-it teams (24%) reported
using advanced techniques.

For these two initial models, our potential factors provide
eight degrees of freedom; again assuming power of 0.75, this
yields a prospective eﬀect size f 2 = 0.136, indicating we
could again expect to ﬁnd eﬀects of roughly medium size by
Cohen’s heuristic [7].

Break score. The model considering break-it score is given
in Table 6. It shows that teams with more members per-
formed better, with an average of 430 additional points per
team member. Auditing code for errors is an easily paral-
lelized task, so teams with more members could divide their
eﬀort and achieve better coverage. Recall that having more
team members did not help build-it teams (see Tables 3
and 5); this makes sense as development requires more co-
ordination, especially during the early stages.

The model also indicates that Spring 2015 teams per-
formed signiﬁcantly better than Fall 2015 teams. Figure 7
illustrates that correctness bugs, despite being worth fewer
points than security bugs, dominate overall break-it scores
for Spring 2015. In Fall 2015 the scores are more evenly dis-
tributed between correctness and security bugs. This out-
come is not surprising to us, as it was somewhat by design.
The Spring 2015 problem deﬁnes a rich command-line inter-
face with many opportunities for subtle errors that break-it
teams can target.
It also allowed a break-it team to sub-
mit up to 10 correctness bugs per build-it submission. To
nudge teams toward ﬁnding more security-relevant bugs, we
reduced the submission limit from 10 to 5, and designed the
Fall 2015 interface to be far simpler.

Interestingly, making use of advanced analysis techniques
did not factor into the ﬁnal model; i.e., such techniques did
not provide a meaningful advantage. This makes sense when
we consider that such techniques tend to ﬁnd generic errors
such as crashes, bounds violations, or null pointer derefer-

(a) Spring 2015

(b) Fall 2015

Figure 7: Scores of break-it teams prior to the ﬁx-it phase,
broken down by points from security and correctness bugs.
The ﬁnal score of the break-it team (after ﬁx-it phase) is
noted as a dot. Note the diﬀerent ranges in the y-axes; in
general, the Spring 2015 contest (secure log problem) had
higher scores for breaking.

Factor

Coef.

SE p-value

Fall 2015
# Team members
Build participant

3.847
1.218
5.430

1.486
0.417
2.116

0.011*
0.004*
0.012*

Table 7: Final linear regression modeling the count of se-
curity bugs found by each team. Coeﬃcients indicate how
many security bugs each factor adds to the count. Overall
eﬀect size f 2 = 0.035.

ences. Security violations for our problems are more seman-
tic, e.g., involving incorrect design or use of cryptography.
Many correctness bugs were non-generic too, e.g., involving
incorrect argument processing or mishandling of inconsistent
or incorrect inputs.

Being a build participant and having more coding expe-
rience is identiﬁed as a postive factor in the break-it score,
according to the model, but neither is statistically signiﬁ-
cant (though they are close to the threshold). Interestingly,
knowledge of C is identiﬁed as a strongly negative factor in
break-it score (though again, not statistically signiﬁcant).
Looking closely at the results, we ﬁnd that lack of C knowl-
edge is extremely uncommon, but that the handful of teams
in this category did unusually well. However, there are to
few of them for the result to be signiﬁcant.

Security bugs found. We next consider breaking success
as measured by the count of security bugs a breaking team

700level cryptographic libraries with few “knobs” that allow for
incorrect usage[3].

One implementation of the ATM problem, written in Python,

made use of the SSL PKI infrastructure. The implementa-
tion used generated SSL private keys to establish a root of
trust that authenticated the atm program to the bank pro-
gram. Both the atm and bank required that the connection
be signed with the certiﬁcate generated at runtime. Both
the bank and the atm implemented their communication pro-
tocol as plain text then wrapped in HTTPS. This put the
contestant on good footing; to ﬁnd bugs in this system, other
contestants would need to break the security of OpenSSL.

Another implementation, also for the ATM problem, writ-
ten in Java, used the NaCl library. This library intentionally
provides a very high level API to “box” and “unbox” secret
values, freeing the user from dangerous choices. As above,
to break this system, other contestants would need to ﬁrst
break the security of NaCl.

An implementation of the log reader problem, also written
in Java, achieved success using a high level API. They used
the BouncyCastle library to construct a valid encrypt-then-
MAC scheme over the entire log ﬁle.

5.2 Failure Stories

The failure modes for build-it submissions are distributed
along a spectrum ranging from “failed to provide any se-
curity at all” to “vulnerable to extremely subtle timing at-
tacks.” This is interesting because it is a similar dynamic
observed in the software marketplace today.

Many implementations of the log problem lacked encryp-
tion or authentication. Exploiting these design ﬂaws was
trivial for break-it teams. Sometimes log data was written
as plain text, other times log data was serialized using the
Java object serialization protocol.

One break-it team discovered a privacy ﬂaw which they
could exploit with at most ﬁfty probes. The target sub-
mission truncated the “authentication token,” so that it was
vulnerable to a brute force attack.

The ATM problem allows for interactive attacks (not pos-
sible for the log), and the attacks became cleverer as im-
plementations used cryptographic constructions incorrectly.
One implementation used cryptography, but implemented
RC4 from scratch and did not add any randomness to the
key or the cipher stream. An attacker observed that the
ciphertext of messages was distinguishable and largely un-
changed from transaction to transaction, and was able to
ﬂip bits in a message to change the withdrawn amount.

Another implementation used encryption with authenti-
cation, but did not use randomness; as such error messages
were always distinguishable success messages. An attack was
constructed against this implementation where the attack
leaked the bank balance by observing diﬀerent withdrawal
attempts, distinguishing the successful from failed transac-
tions, and performing a binary search to identify the bank
balance given a series of withdraw attempts.

Some failures were common across ATM problem imple-
mentations. Many implementations kept the key ﬁxed across
the lifetime of the bank and atm programs and did not use
a nonce in the messages. This allowed attackers to replay
messages freely between the bank and the atm, violating in-
tegrity via unauthorized withdrawals. Several implementa-
tions used encryption, but without authentication. These
implementations used a library such as OpenSSL, the Java

Figure 8: Count of security bugs found by each break-it
team, organized by contest and whether the team also par-
ticipated in build-it. The heavy vertical line in the box is
the median, the boxes show the ﬁrst and third quartiles, and
the whiskers extend to the most outlying data within ±1.5×
the interquartile range. Dots indicate further outliers.

found. This model (Table 7) again shows that team size
is important, with an average of one extra security bug
found for each additional team member. Being a qualiﬁed
builder also signiﬁcantly helps one’s score; this makes in-
tuitive sense, as one would expect to gain a great deal of
insight into how a system could fail after successfully build-
ing a similar system. Figure 8 shows the distribution of
the number of security bugs found, per contest, for break-it
teams that were and were not qualiﬁed build-it teams. Note
that all but three of the 108 break-it teams made some at-
tempt, as deﬁned by having made a commit, to participate
during the build-it phase—most of these (93) qualiﬁed, but
12 did not.
If the reason was that these teams were less
capable programmers, that may imply that programming
ability generally has some correlation with break-it success.
On average, four more security bugs were found by a Fall
2015 team than a Spring 2015 team. This contrasts with the
ﬁnding that Spring 2015 teams had higher overall break-it
scores, but corresponds to the ﬁnding that more Fall 2015
submissions had security bugs found against them. As dis-
cussed above, this is because correctness bugs dominated in
Spring 2015 but were not as dominant in Fall 2015. Once
again, the reasons may have been the smaller budget on per-
submission correctness bugs in Fall 2015, and the greater
potential attack surface in the ATM problem.

5. QUALITATIVE ANALYSIS

As part of the data gathered, we also have the entire pro-
gram produced during the build-it phase as well as the pro-
grams patched during the ﬁx-it phase. We can then perform
a qualitative analysis of the programs which is guided by
knowing the security outcome of a given program. Did lots
of break-it teams ﬁnd bugs in the program, or did they not?
What are traits or characteristics of well-designed programs?
5.1 Success Stories

The success stories bear out some old chestnuts of wis-
dom in the security community: submissions that fared well
through the break-it phase made heavy use of existing high-

701cryptographic framework, or the Python pycrypto library to
have access to a symmetric cipher such as AES, but either
did not use these libraries at a level where authentication
was provided in addition to encryption, or they did not en-
able authentication.

Some failures were common across log implementations as
well: if an implementation used encryption, it might not use
authentication. If it used authentication, it would authenti-
cate records stored in the ﬁle individually and not globally.
The implementations would also relate the ordering of en-
tries in the ﬁle to the ordering of events in time, allowing
for an integrity attack that changes history by re-ordering
entries in the ﬁle.

As a corpus for research, this data set is of interest for
future mining. What common design patterns were used and
how did they impact the outcome? Are there any metrics
we can extract from the code itself that can predict break-it
scores? We defer this analysis to future work.

6. RELATED WORK

BIBIFI bears similarity to existing programming and se-
curity contests but is unique in its focus on building secure
systems. BIBIFI also is related to studies of code and secure
development, but diﬀers in its open-ended contest format.

Contests. Cybersecurity contests typically focus on vulner-
ability discovery and exploitation, and sometimes involve a
system administration component for defense. One popular
style of contest is dubbed capture the ﬂag (CTF) and is ex-
empliﬁed by a contest held at DEFCON [22]. Here, teams
run an identical system that has buggy components. The
goal is to ﬁnd and exploit the bugs in other competitors’
systems while mitigating the bugs in your own. Compro-
mising a system enables a team to acquire the system’s key
and thus “capture the ﬂag.” In addition to DEFCON CTF,
there are other CTFs such as iCTF [6, 12] and PicoCTF [5].
The use of this style of contest in an educational setting
has been explored in prior work [10, 14, 20]. The Collegiate
Cyber Defense Challenge [25, 9, 8] and the Maryland Cy-
ber Challenge & Competition [24] have contestants defend
a system, so their responsibilities end at the identiﬁcation
and mitigation of vulnerabilities. These contests focus on
bugs in systems as a key factor of play, but neglect software
development.

Programming contests challenge students to build clever,
eﬃcient software, usually with constraints and while under
(extreme) time pressure. The ACM programming contest [2]
asks teams to write several programs in C/C++ or Java dur-
ing a 5-hour time period. Google Code Jam [18] sets tasks
that must be solved in minutes, which are then graded ac-
cording to development speed (and implicitly, correctness).
Topcoder [35] runs several contests; the Algorithm compe-
titions are small projects that take a few hours to a week,
whereas Design and Development competitions are for larger
projects that must meet a broader speciﬁcation. Code is
judged for correctness (by passing tests), performance, and
sometimes subjectively in terms of code quality or practi-
cality of design. All of these resemble the build-it phase
of BIBIFI but typically consider smaller tasks; they do not
consider the security of the produced code.

Studies of secure software development. There have been
a few studies of diﬀerent methods and techniques for ensur-
ing security. Work by Finifter and Wagner [16] and Prechelt [28]
relates to both our build-it and break-it phases: they asked
diﬀerent teams to develop the same web application using
diﬀerent frameworks, and then subjected each implementa-
tion to automated (black box) testing and manual review.
They found that both forms of review were eﬀective in dif-
ferent ways, and that framework support for mitigating cer-
tain vulnerabilities improved overall security. Other studies
focused on the eﬀectiveness of vulnerability discovery tech-
niques, e.g., as might be used during our break-it phase.
Edmundson et al. [15] considered manual code review; Scan-
dariato et al. [33] compared diﬀerent vulnerability detection
tools; other studies looked at software properties that might
co-occur with security problems [37, 38, 19]. BIBIFI diﬀers
from all of these in its open-ended, contest format: Partici-
pants can employ any technique they like, and with a large
enough population and/or measurable impact, the eﬀective-
ness of a given technique will be evident in ﬁnal outcomes.

7. CONCLUSIONS

This paper has presented Build-it, Break-it, Fix-it (BIBIFI),

a new security contest that brings together features from
typical security contests, which focus on vulnerability de-
tection and mitigation but not secure development, and pro-
gramming contests, which focus on development but not se-
curity. During the ﬁrst phase of the contest, teams construct
software they intend to be correct, eﬃcient, and secure. Dur-
ing the second phase, break-it teams report security vulner-
abilities and other defects in submitted software.
In the
ﬁnal, ﬁx-it, phase, builders ﬁx reported bugs and thereby
identify redundant defect reports. Final scores, following an
incentives-conscious scoring system, reward the best builders
and breakers.

During 2015, we ran three contests involving a total of 116
teams and two diﬀerent programming problems. Quantita-
tive analysis from these contests found that the best per-
forming build-it submissions used C/C++, but submissions
coded in a statically-typed language were less likely to have
a security ﬂaw; build-it teams with diverse programming-
language knowledge also produced more secure code. Shorter
programs correlated with better scores. Break-it teams that
were also successful build-it teams were signiﬁcantly better
at ﬁnding security bugs.

There are many interesting areas of future work that BIBIFI

opens up. The BIBIFI design lends itself well to conduct-
ing more focused studies; our competitions allow partici-
pants to use any languages and tools they desire, but one
could narrow their options for closer evaluation. Although
we have reduced manual judging considerably, an interesting
technical problem that often arises is determining whether
two bugs are morally equivalent; an automated method for
determining this could be broadly applicable. Finally, one
limitation of our study is that we do not evaluate whether
break-it teams ﬁnd all of the bugs there are to ﬁnd; one
improvement would be to apply a set of fuzzers and static
analyzers, or to recruit professional teams to eﬀectively par-
ticipate in the break-it phase as a sort of baseline against
which to compare the break-it teams’ performance.

We plan to freely release BIBIFI to support future re-
search. We believe it can act as an incubator for ideas
to improve secure development. More information, data,

702and opportunities to participate are available at https://
builditbreakit.org.

Acknowledgments
We thank Jandelyn Plane and Atif Memon who contributed
to the initial development of BIBIFI and its preliminary data
analysis. Many people in the security community, too nu-
merous to list, contributed ideas and thoughts about BIBIFI
during its development—thank you! Bobby Bhattacharjee
and the anonymous reviewers provided helpful comments
on drafts of this paper. This project was supported gifts
from Accenture, AT&T, Galois, Leidos, Patriot Technolo-
gies, NCC Group, Trail of Bits, Synposis, ASTech Consult-
ing, Cigital, SuprTek, Cyberpoint, and Lockheed Martin;
by a grant from the NSF under award EDU-1319147; by
DARPA under contract FA8750-15-2-0104; and by the U.S.
Department of Commerce, National Institute for Standards
and Technology, under Cooperative Agreement 70NANB15H330.

8. REFERENCES
[1] Abadi, M., Budiu, M., Erlingsson, U., and Ligatti, J.

Control-ﬂow integrity principles, implementations, and
applications. ACM Transactions on Information and
System Security (TISSEC) 13, 1 (2009), 4:1–4:40.

[2] The ACM-ICPC International Collegiate Programming

Contest. http://icpc.baylor.edu.

[3] Bernstein, D. J., Lange, T., and Schwabe, P. The

security impact of a new cryptographic library. In
International Conference on Cryptology and Information
Security in Latin America (2012).

[4] Burnham, K. P., Anderson, D. R., and Huyvaert, K. P.
AIC model selection and multimodel inference in behavioral
ecology: some background, observations, and comparisons.
Behavioral Ecology and Sociobiology 65, 1 (2011), 23–35.

[5] Chapman, P., Burket, J., and Brumley, D. PicoCTF: A
game-based computer security competition for high school
students. In USENIX Summit on Gaming, Games, and
Gamiﬁcation in Security Education (3GSE) (2014).

[6] Childers, N., Boe, B., Cavallaro, L., Cavedon, L.,
Cova, M., Egele, M., and Vigna, G. Organizing large
scale hacking competitions. In DIMVA (2010).

[7] Cohen, J. Statistical Power Analysis for the Behavioral

Sciences. Lawrence Erlbaum Associates, 1988.

[8] Conklin, A. The use of a collegiate cyber defense
competition in information security education. In
Information Security Curriculum Development Conference
(InfoSecCD) (2005).

[9] Conklin, A. Cyber defense competitions and information

security education: An active learning solution for a
capstone course. In Hawaii International Conference on
System Sciences (HICSS) (2006).

[10] Conti, G., Babbitt, T., and Nelson, J. Hacking

competitions and their untapped potential for security
education. Security & Privacy 9, 3 (2011), 56–59.

[11] DEF CON Communications, Inc. Capture the ﬂag

archive. https://www.defcon.org/html/links/dc-ctf.html.

[12] Doup´e, A., Egele, M., Caillat, B., Stringhini, G.,

Yakin, G., Zand, A., Cavedon, L., and Vigna, G. Hit
’em where it hurts: A live security exercise on cyber
situational awareness. In Annual Computer Security
Applications Conference (ACSAC) (2011).

[13] dragostech.com inc. Cansecwest applied security

conference. http://cansecwest.com.

[14] Eagle, C. Computer security competitions: Expanding

educational outcomes. IEEE Security & Privacy 11, 4
(2013), 69–71.

[15] Edmundson, A., Holtkamp, B., Rivera, E., Finifter,

M., Mettler, A., and Wagner, D. An empirical study on

the eﬀectiveness of security code review. In International
Symposium on Engineering Secure Software and Systems
(ESSoS) (2013).

[16] Finifter, M., and Wagner, D. Exploring the relationship
between web application development tools and security. In
USENIX Conference on Web Application Development
(WebApps) (2011).

[17] Git – distributed version control management system.

http://git-scm.com.

[18] Google code jam. http://code.google.com/codejam.
[19] Harrison, K., and White, G. An empirical study on the

eﬀectiveness of common security measures. In Hawaii
International Conference on System Sciences (HICSS)
(2010).

[20] Hoffman, L. J., Rosenberg, T., and Dodge, R.

Exploring a national cybersecurity exercise for universities.
IEEE Security & Privacy 3, 5 (2005), 27–33.

[21] ICFP programming contest. http://icfpcontest.org.
[22] Inc, D. C. C. Def con hacking conference.

http://www.defcon.org.

[23] Kim, Q. Want to learn cybersecurity? Head to Def Con.

http://www.marketplace.org/2014/08/25/tech/
want-learn-cybersecurity-head-def-con, 2014.

[24] Maryland cyber challenge & competition. http://www.
fbcinc.com/e/cybermdconference/competitorinfo.aspx.
[25] National Collegiate Cyber Defense Competition.

http://www.nationalccdc.org.

[26] Parker, J. LMonad: Information ﬂow control for Haskell

web applications. Master’s thesis, Dept of Computer
Science, the University of Maryland, 2014.

[27] Polytechnic Institute of New York University. Csaw

- cybersecurity competition 2012.
http://www.poly.edu/csaw2012/csaw-CTF.

[28] Prechelt, L. Plat forms: A web development platform
comparison by an exploratory experiment searching for
emergent platform properties. IEEE Transactions on
Software Engineering 37, 1 (2011), 95–108.

[29] PostgreSQL. http://www.postgresql.org.
[30] Ruef, A., and Hicks, M. Build it, break it, ﬁx it:

Competing to build secure systems. The Next Wave 21, 1
(2015), 19–23.

[31] Ruef, A., Hicks, M., Parker, J., Levin, D., Memon, A.,
Plane, J., and Mardziel, P. Build it break it: Measuring
and comparing development security. In Workshop on
Cyber Security Experimentation and Test (CSET) (2015).
[32] Saltzer, J. H., and Schroeder, M. D. The protection of
information in computer systems. Proceedings of the IEEE
63, 9 (1975), 1278–1308.

[33] Scandariato, R., Walden, J., and Joosen, W. Static

analysis versus penetration testing: A controlled
experiment. In IEEE International Symposium on
Reliability Engineering (ISSRE) (2013).

[34] Stefan, D., Russo, A., Mitchell, J., and Mazieres, D.

Flexible dynamic information ﬂow control in Haskell. In
ACM SIGPLAN Haskell Symposium (2011).

[35] Top coder competitions. http://apps.topcoder.com/wiki/

display/tc/Algorithm+Overview.

[36] ´Ulfar Erlingsson. personal communication stating that

CFI was not deployed at Microsoft due to its overhead
exceeding 10%, 2012.

[37] Walden, J., Stuckman, J., and Scandariato, R.

Predicting vulnerable components: Software metrics vs text
mining. In IEEE International Symposium on Software
Reliability Engineering (2014).

[38] Yang, J., Ryu, D., and Baik, J. Improving vulnerability
prediction accuracy with secure coding standard violation
measures. In International Conference on Big Data and
Smart Computing (BigComp) (2016).

[39] Yesod web framework for Haskell.

http://www.yesodweb.com
.

703