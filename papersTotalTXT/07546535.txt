2016 IEEE Symposium on Security and Privacy
2016 IEEE Symposium on Security and Privacy

SoK: Veriﬁability Notions for E-Voting Protocols

V´eronique Cortier∗, David Galindo†, Ralf K¨usters‡, Johannes M¨uller‡, Tomasz Truderung§

∗LORIA/CNRS, France

†University of Birmingham, UK
‡University of Trier, Germany

§Polyas GmbH, Germany

e-mail: veronique.cortier@loria.fr, D.Galindo@cs.bham.ac.uk, {kuesters, muellerjoh}@uni-trier.de, ttruderung@gmail.com
Abstract—There have been intensive research efforts in the last
numerous elections it has been demonstrated that the employed
two decades or so to design and deploy electronic voting (e-voting)
systems can easily be manipulated (e.g., by replacing hardware
protocols/systems which allow voters and/or external auditors
components in voting machines) or that they contained ﬂaws
to check that the votes were counted correctly. This security
that made it possible for more or less sophisticated attackers
property, which not least was motivated by numerous problems
to change the result of the elections (see, e.g., [29], [14],
is called veriﬁability. It is meant to
in even national elections,
[2], [3], [52], [53], [48], [25]). In some occasions, announced
defend against voting devices and servers that have programming
results were incorrect and/or elections had to be rerun (see,
errors or are outright malicious. In order to properly evaluate
e.g., [1], [4]). Given that e-voting systems are complex software
and analyze e-voting protocols w.r.t. veriﬁability, one fundamental
and hardware systems, programming errors are unavoidable
challenge has been to formally capture the meaning of this
and deliberate manipulation of such systems is often hard or
security property. While the ﬁrst formal deﬁnitions of veriﬁability
virtually impossible to detect.
were devised in the late 1980s already, new veriﬁability deﬁnitions
are still being proposed. The deﬁnitions differ in various aspects,
including the classes of protocols they capture and even their
formulations of the very core of the meaning of veriﬁability. This
is an unsatisfying state of affairs, leaving the research on the
veriﬁability of e-voting protocols in a fuzzy state.

In this paper, we review all formal deﬁnitions of veriﬁability
proposed in the literature and cast them in a framework proposed
by K¨usters, Truderung, and Vogt (the KTV framework), yielding
a uniform treatment of veriﬁability. This enables us to provide a
detailed comparison of the various deﬁnitions of veriﬁability from
the literature. We thoroughly discuss advantages and disadvan-
tages, and point to limitations and problems. Finally, from these
discussions and based on the KTV framework, we distill a general
deﬁnition of veriﬁability, which can be instantiated in various
ways, and provide precise guidelines for its instantiation. The
concepts for veriﬁability we develop should be widely applicable
also beyond the framework used here. Altogether, our work
offers a well-founded reference point for future research on the
veriﬁability of e-voting systems.

Keywords-e-voting; veriﬁability; protocol analysis

I.

INTRODUCTION

Systems for electronic voting (e-voting systems) have been
and are being employed in many countries for national, state-
wide and municipal elections, for example in the US, Estonia,
India, Switzerland, France, and Australia. They are also used
for elections within companies, organizations, and associations.
There are roughly two types of e-voting systems: those where
the voter has to go to a polling station in order to cast her
vote using a voting machine and those that allow the voter to
cast her vote remotely over the Internet, using her own device.
When voting at a polling station, the voter either has to ﬁll in
a paper ballot, which then is scanned by an optical scan voting
system, or the voter enters her vote into a machine directly, a
so-called Direct Recording Electronic (DRE) voting system.
For most voting systems used in practice today, voters
have no guarantees that their votes have actually been counted:
the voters’ devices, voting machines, and/or voting servers
might have (unintentional or deliberate) programming errors
or might have been tampered with in some other way. In

2375-1207/16 $31.00 © 2016 IEEE
© 2016, Véronique Cortier. Under license to IEEE.
DOI 10.1109/SP.2016.52
DOI 10.1109/SP.2016.52

779
779

Therefore, there has been intensive and ongoing research
in the last two decades or so to design e-voting protocols
and systems1 which provide what is called veriﬁability (see,
e.g., [21], [31], [17], [6], [15], [10], [9], [19], [34], [27], [33]).
Roughly speaking, veriﬁability means that voters and possibly
external auditors should be able to check whether the votes
were actually counted and whether the published election result
is correct, even if voting devices and servers have programming
errors or are outright malicious. Several of such systems have
already been deployed in real binding elections (see, e.g., [6],
[15], [7], [44], [13], [50], [22], [26]).

For the systematic security analysis of such systems and
protocols, one challenge has been to formally and precisely
capture the meaning of veriﬁability. While the ﬁrst attempts at a
formal deﬁnition stem from the late 1980s [12], new deﬁnitions
are still being put forward, with many deﬁnitions having been
proposed in the last few years [16], [35], [32], [37], [19],
[34], [33], [47], [49]. The deﬁnitions differ in many aspects,
including the classes of protocols they capture, the underlying
models and assumptions, the notation, and importantly, the
formulations of the very core of the meaning of veriﬁability.
This is an unsatisfying state of affairs, which leaves the
research on the veriﬁability of e-voting protocols and systems
in a fuzzy state and raises many questions, such as: What are
the advantages, disadvantages, problems, and limitations of the
various deﬁnitions? How do the security guarantees provided
by the deﬁnitions compare? Are they similar or fundamentally
different? Answering such questions is non-trivial. It requires
some common basis on which the deﬁnitions can be discussed
and compared.
Contribution of this paper. First, we show that the essence of
all formal deﬁnitions of veriﬁability proposed in the literature
so far can be cast in one framework. We choose the framework
proposed by K¨usters, Truderung, and Vogt [37] for this purpose.

1In what follows, we use the terms protocols and systems interchangeably.
We point out, however, that this work is mostly concerned with the protocol
aspects of e-voting rather than speciﬁc system aspects.

The generic deﬁnition of veriﬁability in this framework is
applicable to essentially any kind of protocol, with a ﬂexible
way of dealing with various trust assumptions and types of
corruption. Most importantly, it allows us to capture many
kinds and ﬂavors of veriﬁability.

The casting of the different deﬁnitions in one framework
is an important contribution by itself as it yields a uniform
treatment of veriﬁability. This uniform treatment enables us to
provide a detailed and systematic comparison of the different
formal deﬁnitions of veriﬁability proposed in the literature
until now. We present thorough discussions of all relevant
deﬁnitions and models concerning their advantages, disad-
vantages, problems, and limitations, resulting in various new
insights concerning the deﬁnitions itself and their relationships.
Among others, it turns out that while the deﬁnitions share
a common intuition about the meaning of veriﬁability, the
security guarantees that are actually captured and formalized
often vary, with many technical subtleties involved. Cast
in tailored models, different, sometimes implicit, and often
unnecessary assumptions about the protocol structure or the
trust assumptions are made. For some deﬁnitions, we point out
severe limitations and weaknesses.

Finally, we distill

these discussions and insights into
detailed guidelines that highlight several aspects any veriﬁability
deﬁnition should cover. Based on the KTV framework, we
provide a solid, general, and ﬂexible veriﬁability deﬁnition
that covers a wide range of protocols, trust assumptions, and
voting infrastructures. Even if alternative frameworks are used,
for example in order to leverage speciﬁc proof techniques
or analysis tools, our guidelines provide insights on which
parameters may be changed and what the implications of such
modiﬁcations are. This lays down a common, uniform, and
yet general basis for all design and analysis efforts of existing
and future e-voting protocols. As such, our work offers a well-
founded reference point for future research on the veriﬁability
of e-voting systems and protocols.
Structure of this paper.
In Section II, we introduce some
notation which we use throughout this paper. We brieﬂy recall
the KTV framework in Section III. In Sections IV to VIII
we then cast various deﬁnitions in this framework and based
on this we carry out detailed discussions on these deﬁnitions.
Further deﬁnitions are brieﬂy discussed in Section IX, with
some of them treated in detail in the appendix. The mentioned
deﬁnitions and guidelines we distill from our discussions,
together with various insights, are presented in Section X.
The appendix contains further details, with full details provided
in our technical report [20].

II. NOTATION AND PRELIMINARIES

Next, we provide some background on e-voting and intro-

duce notation that we use throughout the paper.

In an e-voting protocol/system, a voter, possibly using some
voter supporting device (VSD) (e.g., a desktop computer or
smartphone), computes a ballot, typically containing the voter’s
choice in an encrypted or encoded form, and casts it. Often
this means that the ballot is put on a bulletin board (see also
below). The ballots are collected (e.g., from the bulletin board)
and tallied by tellers/voting authorities. In modern e-voting
protocols, the tallying is, for example, done by combining
all ballots into one, using homomorphic encryption, and then
decrypting the resulting ballot, or by using mix-nets, where the
ballots before being decrypted are shufﬂed. At the beginning

of an election, the voting authorities produce the election
parameters prm, typically containing keys and a set of valid
choices C, the choice space. In general, C can be an arbitrary
set, containing just the set of candidates, if voters can choose
one candidate among a set of candidates, or even tuples of
candidates, if voters can choose several candidates or rank
them. We emphasize that we consider abstention to be one of
the choices in C.

In this paper, we denote the voters by V1, . . . , Vn and their
VSDs (if any) by VSD1, . . . , VSDn. In order to cast a vote, a
voter Vi ﬁrst picks her choice ci ∈ C. She then runs her voting
procedure Vote(ci), which in turn might involve providing her
VSD with her choice. The VSD runs some procedure VoteVSD,
given certain parameters, e.g., the voters choice. The result
of running the voting procedure is a ballot bi, which, for
example, might contain ci in encrypted form. Some models do
not distinguish between the voter and her VSD, and in such a
case, we simply denote the voter’s voting procedure by Vote.
Often voters have to perform some veriﬁcation procedure
during or at the end of the election in order to prevent/detect
malicious behavior by their VSDs or the voting authorities. We
denote such a procedure by Verify. This procedure might for
example involve checking that the voter’s ballot appears on
the bulletin board or performing certain cryptographic tasks.
Carrying out Verify will often require some trusted device.

We denote the tellers by T1, . . . , Tm. As mentioned, they
collect the ballots, tally them, and output the election result
Tally, which belongs to what we call the result space R (ﬁxed
for a given election). The result is computed according to
a result function ρ : Cn → R which takes the voters’ choices
(cid:3)c = (c1, . . . , cn) as input and outputs the result. (Of course
dishonest tellers might try to manipulate the election outcome,
which by the veriﬁability property, as discussed in the next
section, should be detected.) The result function should be
speciﬁed by the election authorities before an election starts.
At the end or throughout the election, auditors/judges
might check certain information in order to detect malicious
behavior. Typically, these checks are based solely on publicly
available information, and hence, in most cases their task can
be carried out by any party. They might for example check
certain zero-knowledge proofs. In what follows, we consider
the auditors/judges to be one party J, who is assumed to be
honest.

As already noted above, most election protocols assume an
append-only bulletin board B. An honest bulletin board stores
all the input it receives from arbitrary participants in a list, and
it outputs the list on request. Typically, public parameters, such
as public keys, the election result, voters’ ballots, and other
public information, such as zero-knowledge proofs generated by
voting authorities, are published on the bulletin board. As we
will see, in most models (and many protocols) a single honest
bulletin board is assumed. However, trust can be distributed
[23]. Providing robust and trustworthy bulletin boards, while
very important, is mainly considered to be a task orthogonal
to the rest of the election protocol. For this reason, we will
mostly refer to the (honest) bulletin board B, which in practice
might involve a distributed solution rather than a single trusted
server.

III. THE KTV FRAMEWORK

In this section, we brieﬂy recall the KTV framework [37],
which is based on a general computational model and provides

780780

a general deﬁnition of veriﬁability. As already mentioned in the
introduction, in the subsequent sections we use this framework
to cast all other formal deﬁnitions of veriﬁability. Here, we
slightly simplify this framework without losing generality.
These simpliﬁcations help, in particular, to smoothly deal with
modeling dynamic corruption of parties (see below).

A. Computational Model

Processes are the core of the computational model. Based

on them, protocols are deﬁned.
Process. A process is a set of probabilistic polynomial-time
interactive Turing machines (ITMs, also called programs) which
are connected via named tapes (also called channels). Two
programs with a channel of the same name but opposite
directions (input/output) are connected by this channel. A
process may have external input/output channels, those that
are not connected internally. At any time of a process run,
one program is active only. The active program may send a
message to another program via a channel. This program then
becomes active and after some computation can send a message
to another program, and so on. Each process contains a master
program, which is the ﬁrst program to be activated and which
is activated if the active program did not produce output (and
hence, did not activate another program). If the master program
is active but does not produce output, a run stops.
We write a process π as π = p1 (cid:5)···(cid:5) pl, where p1 . . . , pl are
programs. If π1 and π2 are processes, then π1 (cid:5) π2 is a process,
provided that the processes are connectible: two processes
are connectible if common external channels, i.e., channels
with the same name, have opposite directions (input/output);
internal channels are renamed, if necessary. A process π where
all programs are given the security parameter 1(cid:5) is denoted
by π((cid:5)). In the processes we consider the length of a run is
always polynomially bounded in (cid:5). Clearly, a run is uniquely
determined by the random coins used by the programs in π.
Protocol. A protocol P is deﬁned by a set of agents Σ (also
called parties or protocol participants), and a program πa which
is supposed to be run by the agent. This program is the honest
program of a. Agents are pairwise connected by channels and
every agent has a channel to the adversary (see below).2

Typically, a protocol P contains a scheduler S as one of its
participants which acts as the master program of the protocol
process (see below). The task of the scheduler is to trigger
the protocol participants and the adversary in the appropriate
order. For example, in the context of e-voting, the scheduler
would trigger protocol participants according to the phases of
an election, e.g., i) register, ii) vote, iii) tally, iv) verify.

If πa1

, . . . , πan are the honest programs of the agents of P,

The process πP is always run with an adversary A. The
adversary may run an arbitrary probabilistic polynomial-time
program and has channels to all protocol participants in πP.
Hence, a run r of P with adversary (adversary program) πA is
a run of the process πP (cid:5) πA. We consider πP (cid:5) πA to be part of
the description of r, so that it is always clear to which process,
including the adversary, the run r belongs.

The honest programs of the agents of P are typically

then we denote the process πa1

(cid:5) . . . (cid:5) πan by πP.

speciﬁed in such a way that the adversary A can corrupt the
programs by sending the message corrupt. Upon receiving such
a message, the agent reveals all or some of its internal state to
the adversary and from then on is controlled by the adversary.
Some agents, such as the scheduler or a judge, will typically not
be corruptible, i.e., they would ignore corrupt messages. Also,
agents might only accept corrupt message upon initialization,
modeling static corruption. Altogether, this allows for great
ﬂexibility in deﬁning different kinds of corruption, including
various forms of static and dynamic corruption.

We say that an agent a is honest in a protocol run r if the
agent has not been corrupted in this run, i.e., has not accepted a
corrupt message throughout the run. We say that an agent a is
honest if for all adversarial programs πA the agent is honest in
all runs of πP (cid:5) πA, i.e., a always ignores all corrupt messages.
Property. A property γ of P is a subset of the set of all runs
of P.3 By ¬γ we denote the complement of γ.
Negligible, overwhelming, δ-bounded. As usual, a function
f from the natural numbers to the interval [0,1] is negligible
if, for every c > 0, there exists (cid:5)0 such that f ((cid:5)) ≤ 1
(cid:5)c for all
(cid:5) > (cid:5)0. The function f is overwhelming if the function 1− f is
negligible. A function f is δ-bounded if, for every c > 0 there
exists (cid:5)0 such that f ((cid:5)) ≤ δ + 1
B. Veriﬁability

(cid:5)c for all (cid:5) > (cid:5)0.

The KTV framework comes with a general deﬁnition of
veriﬁability. The deﬁnition assumes a judge J whose role is
to accept or reject a protocol run by writing accept or reject
on a dedicated channel decisionJ. To make a decision, the
judge runs a so-called judging procedure, which performs
certain checks (depending on the protocol speciﬁcation), such
as veriﬁcation of all zero-knowledge proofs (if any). Intuitively,
J accepts a run if the protocol run looks as expected. The
judging procedure should be part of the protocol speciﬁcation.
So, formally the judge should be one of the protocol participants
in the considered protocol P, and hence, precisely speciﬁed.
The input to the judge typically is solely public information,
including all information and complaints (e.g., by voters) posted
on the bulletin board. Therefore the judge can be thought of
as a “virtual” entity: the judging procedure can be carried out
by any party, including external observers and even voters
themselves.

The deﬁnition of veriﬁability is centered around the notion
of a goal of the protocol. Formally, a goal is simply a property
γ of the system, i.e. a set of runs (see Section III-A). Intuitively,
such a goal speciﬁes those runs which are “correct” in some
protocol-speciﬁc sense. For e-voting, intuitively, the goal would
contain those runs where the announced result of the election
corresponds to the actual choices of the voters.

Now, the idea behind the deﬁnition is very simple. The
judge J should accept a run only if the goal γ is met, and hence,
the published election result corresponds to the actual choices
of the voters. More precisely, the deﬁnition requires that the
probability (over the set of all runs of the protocol) that the
goal γ is not satisﬁed but the judge nevertheless accepts the run
is δ-bounded. Although δ = 0 is desirable, this would be too
strong for almost all e-voting protocols. For example, typically
not all voters check whether their ballot appears on the bulletin

2We note that in [37] agents were assigned sets of potential programs they
could run plus an honest program. Here, w.l.o.g., they are assigned only one
honest program (which, however, might be corrupted later on).

3Recall that the description of a run r of P contains the description of
the process πP (cid:5) πA (and hence, in particular the adversary) from which r
originates. Hence, γ can be formulated independently of a speciﬁc adversary.

781781

board, giving an adversary A the opportunity to manipulate
or drop some ballots without being detected. Therefore, δ = 0
cannot be achieved in general.
By Pr[π((cid:5)) (cid:7)→ (J: accept)] we denote the probability that π,
with security parameter 1(cid:5), produces a run which is accepted
by J. Analogously, by Pr[π((cid:5)) (cid:7)→ ¬γ, (J: accept)] we denote
the probability that π, with security parameter 1(cid:5), produces a
run which is not in γ but nevertheless accepted by J.
Deﬁnition 1 (Veriﬁability). Let P be a protocol with the set of
agents Σ. Let δ ∈ [0,1] be the tolerance, J ∈ Σ be the judge and
γ be a goal. Then, we say that the protocol P is (γ, δ)-veriﬁable
by the judge J if for all adversaries πA and π = (πP (cid:5) πA), the
probability

Pr[π((cid:5)) (cid:7)→ ¬γ, (J: accept)]

is δ-bounded as a function of (cid:5).

A protocol P could trivially satisfy veriﬁability with a judge
who never accepts a run. Therefore, one of course would also
require a soundness or fairness condition. That is, one would
except at the very least that if the protocol runs with a benign
adversary, which, in particular, would not corrupt parties, then
the judge accepts a run. Formally, for a benign adversary πA
we require that Pr[π((cid:5)) (cid:7)→ (J: accept)] is overwhelming. One
could even require that the judge accepts a run as soon as a
certain subset of protocol participants are honest, e.g., the voting
authorities (see, e.g., [37] for a more detailed discussion). These
kinds of fairness/soundness properties can be considered to be
sanity checks of the judging procedure and are typically easy
to check. Most deﬁnitions of veriﬁability in the literature do
not explicitly mention this property. For brevity of presentation,
we therefore mostly ignore this issue here as well. In the
subsequent sections, we, however, mention and brieﬂy discuss
fairness conditions unless addressed by a deﬁnition.

Deﬁnition 1 captures the essence of the notion of veriﬁability
in a very simple way, as explained above. In addition, it provides
great ﬂexibility and it is applicable to arbitrary classes of e-
voting protocols. This is in contrast to most other deﬁnitions of
veriﬁability, as we will see in the subsequent sections, which are
mostly tailored to speciﬁc classes of protocols. This ﬂexibility in
fact lets us express the other deﬁnitions in terms of Deﬁnition 1.
There are two reasons for the ﬂexibility. First, the notion of
a protocol P used in Deﬁnition 1 is very general: a protocol
is simply an arbitrary set of interacting Turing machines, with
one of them playing the role of the judge. Second, the goal
γ provides great ﬂexibility in expressing what an e-voting
protocol is supposed to achieve in terms of veriﬁability.

As mentioned in the introduction, in the following sections,
we present all relevant deﬁnitions of veriﬁability from the
literature, discuss them in detail, and then express their essence
in terms of Deﬁnition 1. The latter, in particular, allows for a
uniform treatment of the various deﬁnitions from the literature,
and by this a better understanding of the individual deﬁnitions
and their relationships to other deﬁnitions. Advantages and
disadvantages of the deﬁnitions can be clearly seen in terms
of the classes of protocols that are captured by the deﬁnitions
and the security guarantees that they give. It seems particularly
interesting to see which goals γ (in the sense deﬁned above)
these deﬁnitions consider. In Section X, among others, we use
these insights to distill precise guidelines for important aspects
of deﬁnitions of veriﬁability and propose goals γ applicable to

a broad class of e-voting protocols, and hence, we provide a
particularly useful instantiation of Deﬁnition 1 given what we
have learned from all deﬁnitions from the literature.

The following sections, in which we present and discuss
the various deﬁnitions of veriﬁability from the literature, are
ordered in such a way that deﬁnitions that are close in spirit are
discussed consecutively. All sections follow the same structure.
In every section, we ﬁrst brieﬂy sketch the underlying model,
then present the actual deﬁnition of veriﬁability, followed by
a discussion of the deﬁnition, and ﬁnally the casting of the
deﬁnition in Deﬁnition 1. We emphasize that the discussions
about the deﬁnitions provided in these sections reﬂect the
insights we obtained by casting the deﬁnitions in the KTV
framework. For simplicity and clarity of the presentation, we,
however, present the (informal) discussions before casting the
deﬁnitions.

IV. A SPECIFIC VERIFIABILITY GOAL BY K ¨USTERS ET AL.
In [37], K¨usters et al. also propose a speciﬁc family of
goals for e-voting protocols that they used in [37] as well
as subsequent works [40], [39], [38]. We present this family
of goals below as well as the way they have instantiated the
model when applied to concrete protocols. Since this is a
speciﬁc instantiation of the KTV framework, we can omit the
casting of their deﬁnition in this framework.

A. Model

When applying the KTV framework in order to model
speciﬁc e-voting protocols, K¨usters et al. model static corruption
of parties. That is, it is clear from the outset whether or not a
protocol participant (and in particular a voter) is corrupted. An
honest voter V runs her honest program πV with her choice
c ∈ C provided by the adversary. This choice is called the actual
choice of the voter, and says how the voter intends to vote.

B. Veriﬁability

In [37], K¨usters et al. propose a general deﬁnition of
accountability, with veriﬁability being a special case. Their
veriﬁability deﬁnition, as mentioned, corresponds to Deﬁni-
tion 1. Their deﬁnition, however, also captures the fairness
condition which we brieﬂy mentioned in Section III-B. To this
end, K¨usters et al. consider Boolean formulas with propositional
variables of the form hon(a) to express constraints on the
honesty of protocol participants. Roughly speaking, given a
Boolean formula ϕ, their fairness condition says that if in a
run parties are honest according to ϕ, then the judge should
accept the run.

While just as in Deﬁnition 1, the veriﬁability deﬁnition
proposed by K¨usters et al. does not require to ﬁx a speciﬁc
goal, for e-voting they propose a family {γk}k≥0 of goals,
which has been applied to analyze various e-voting protocols
and mix nets [37], [40], [39], [38].
Roughly speaking, for k ≥ 0, the goal γk contains exactly
those runs of the voting protocol in which all but up to k votes
of the honest voters are counted correctly and every dishonest
voter votes at most once.

Before recalling the formal deﬁnition of γk from [37], we
ﬁrst illustrate γk by a simple example. For this purpose, consider
an election with ﬁve eligible voters, two candidates, with the
result of the election simply being the number of votes for
each candidate. Let the result function ρ (see Section II) be
deﬁned accordingly. Now, let r be a run with three honest and

782782

two dishonest voters such that A, A, B are the actual choices
of the honest voters in r and the published election result in
r is the following: one vote for A and four votes for B. Then,
the goal γ1 is satisﬁed because the actual choice of one of
the honest voters choosing A can be changed to B and at the
same time the choice of each dishonest voter can be B. Hence,
the result is equal to ρ(A,B,B,B,B), which is the published
result. However, the goal γ0 is not satisﬁed in r because in this
case, all honest voters’ choices (A,A,B) have to be counted
correctly, which, in particular, means that the ﬁnal result has
to contain at least two votes for A and at least one vote for
B. In particular, a ﬁnal result with only two votes for A but
none for B would also not satisfy γ0, but it would satisfy γ1.
(Recall from Section II that abstention is a possible choice.)
Deﬁnition 2 (Goal γk). Let r be a run of an e-voting protocol.
Let nh be the number of honest voters in r and nd = n− nh
be the number of dishonest voters in r. Let c1, . . . , cnh be the
actual choices of the honest voters in this run, as deﬁned above.
Then γk is satisﬁed in r if there exist valid choices ˜c1, . . . , ˜cn
such that the following conditions hold true:
(i) The multiset {˜c1, . . . , ˜cn} contains at least nh−k elements

(ii) The result of the election as published in r (if any) is

of the multiset {c1, . . . , cnh
equal to ρ({˜c1, . . . , ˜cn}).

}.

If no election result is published in r, then γk is not satisﬁed
in r.

With this goal, Deﬁnition 1 requires that if more than
k votes of honest voters were dropped/manipulated or the
number of votes cast by dishonest voters (which are subsumed
by the adversary) is higher than the number dishonest voters
(ballot stufﬁng), then the judge should not accept the run. More
precisely, the probability that the judge nevertheless accepts
the run should be bounded by δ.

We note that the deﬁnition of γk does not require that
choices made by dishonest voters in r need to be extracted
from r in some way and that these extracted choices need to be
reﬂected in {˜c1, . . . ˜cn}: the multiset {˜c1, . . . , ˜cn} of choices
is simply quantiﬁed existentially. It has to contain nh − k
honest votes but no speciﬁc requirements are made for votes
of dishonest voters in this multiset. They can be chosen fairly
independently of the speciﬁc run r (except for reﬂecting the
published result and the requirement that there is at most one
vote for every dishonest voter). This is motivated by the fact
that, in general, one cannot provide any guarantees for dishonest
voters, since, for example, their ballots might be altered or
ignored by dishonest authorities without the dishonest voters
complaining (see also the discussion in [37]).

C. Discussion

The goal γk makes only very minimal assumptions about
the structure of a voting system. Namely, it requires only that,
given a run r, it is possible to determine the actual choice
(intention) of an honest voter and the actual election result.
Therefore, the goal γk can be used in the analysis of a wide
range of e-voting protocols.

One drawback of the goal γk is that it assumes static
corruption. Another disadvantage of γk (for k > 0) is the fact that
it does not distinguish between honest votes that are dropped
and those that are turned into different valid votes, although the
impact on the ﬁnal result by the second kind of manipulation

783783

is stronger than the one by the ﬁrst kind. To illustrate this
issue, consider two voting protocols P1 and P2 (with the result
function ρ being the counting function). In P1 the adversary
might not be able to turn votes by honest voters into different
valid votes, e.g., turn a vote for candidate A into a vote for
B. This can be achieved if voters sign their ballots. In this
case, the adversary can only drop ballots of honest voters. In
P2 voters might not sign their ballots, and hence, the adversary
can potentially manipulate honest votes. Now, P1 obviously
offers stronger veriﬁability because in P1 votes of honest voters
can only be dropped, but not changed: while in P2 the adversary
could potentially turn ﬁve honest votes, say for candidate A,
into ﬁve votes for B, in P1 one could at most drop the ﬁve
honest votes, which is less harm. Still both protocols might
achieve the same level of veriﬁability in terms of the parameters
γk and δ. If γk distinguished between dropping of votes and
manipulation, one could distinguish the security levels of P1
and P2.

In Section X we propose a new goal which solves the

mentioned problems.

V. VERIFIABILITY BY BENALOH

In this section, we study the veriﬁability deﬁnition by
Benaloh [12]. This deﬁnition constitutes the ﬁrst formal
veriﬁability deﬁnition proposed in the literature, and hence,
the starting point for the formal treatment of veriﬁability.
This deﬁnition is close in its essence to the one discussed
in Section IV.

A. Model

Following [12], an l-threshold m-teller n-voter election
system (or simply (l,m,n)-election system) E is a synchronous
system of communicating processes (probabilistic Turing ma-
chines) consisting of m tellers T1, . . . , Tm, n voters V1, . . . , Vn
and further participants. Each process of an election system
controls one bulletin board. Each bulletin board can be read
by every other process, but only be written by the owner.

The intended (honest) behavior of the system participants is
speciﬁed by an election schema. An (l,m,n)-election schema
S consists of a collection of programs to be used by the
participants of an (l,m,n)-election system and an efﬁciently
computable function check, which, given the security parameter
(cid:5) and the messages posted to the public bulletin boards, returns
either ”good” or ”bad”. The election schema S describes a
program πT for each teller process and two possible programs
for each voter: πyes to be used to cast a ”yes” vote and program
πno to be used to cast a ”no” vote. At the end of the election,
each teller Tk releases a value τk.

Any process which follows (one of) its program(s) pre-
scribed by S is said to be proper. We say that a voter casts a
valid “yes” vote, if the messages it posts are consistent with the
program πyes, and similarly for a “no” vote. Note that a proper
voter, by deﬁnition, always casts a valid vote; an improper
voter may or may not cast a valid vote, and if it does not cast
a valid vote, that fact may or may not be detectable by others.
The tally of an election is the pair (tyes,tno) where tyes and
tno are the numbers of voters who cast valid ”yes” and ”no”
votes, respectively. Note that this pair expresses the expected
result corresponding to the cast valid votes. The tally of the
election is said to be correct if ρ(τ1, . . . , τm) = (tyes,tno), where
ρ is a pre-determined function. The expression ρ(τ1, . . . , τm)
describes the actual tally, that is the result of the election

as jointly computed by the tellers (and combined using the
function ρ).

B. Veriﬁability

Now, in [12], veriﬁability is deﬁned as follows.

Deﬁnition 3 (Veriﬁability). Let δ be a function of (cid:5). The
(l,m,n)-election schema S is said to be veriﬁable with con-
ﬁdence 1 − δ if, for any election system E, check satisﬁes
the following properties for random runs of E using security
parameter (cid:5):
(1) If at least l tellers are proper in E, then, with probability
at least 1− δ((cid:5)), check returns good and the tally of the
election is correct.

(2) The joint probability that check returns good and the

election tally is not correct is at most δ((cid:5)).

The election schema S is said to be veriﬁable if δ is negligible.

Condition (1) of Deﬁnition 3 expresses a fairness condition
(see Section III-B), where to guarantee the successful (and
correct) run of a protocol, it is enough to only assume that l
tellers are honest.

Condition (2) of Deﬁnition 3 is the core of Deﬁnition 3.
Roughly speaking, it corresponds to Deﬁnition 1 with the goal
γ0 deﬁned by K¨usters et al. (see Section IV-B). As discussed
below, there are, however, subtle differences, resulting in a too
strong deﬁnition.

C. Discussion

As mentioned before, Benaloh’s deﬁnition constitutes the
ﬁrst formal veriﬁability deﬁnition, mainly envisaging an entirely
computer-operated process based on trusted machines and
where, for example, voters were not asked to perform any
kind of veriﬁcation. Given this setting, the deﬁnition has some
limitations from a more modern point of view.

Similarly to the deﬁnition in Section IV, this deﬁnition
is fairly simple and general, except that only yes/no-votes
are allowed, tellers are explicitly required in this deﬁnition,
and every participant has his/her own bulletin board. These
restrictions, however, are not necessary in order to deﬁne
veriﬁability as illustrated in Section IV. This deﬁnition also
focuses on static corruption. The main problem with this
deﬁnition is that it is too strong in settings typically considered
nowadays, and hence, it would exclude most e-voting protocols,
even those that intuitively should be considered veriﬁable.

As already mentioned, Condition (2) of Deﬁnition 3 is
related to the goal γ0. The goal γ0 is, however, typically too
strong because, for example, not all honest voters perform the
veriﬁcation process, e.g., check whether their ballots actual
appear on the bulletin board. Hence, there is a non-negligible
chance that the adversary is not caught when dropping or
manipulating ballots. This is why K¨usters et al. (Section IV)
considered goals γk for k ≥ 0.

Moreover, the goal considered here is even stronger (see also
Section V-D). Condition (2) in Deﬁnition 3 is concerned not
only with honest voters, but also with dishonest ones who post
messages consistent with honest programs. Now, the problem is
that a dishonest voter could simply cast a vote just like an honest
one. The dishonest voter may, however, never complain even if
dishonest tellers (who might even team up with the dishonest
voter) drop or manipulate the ballot of the dishonest voter.

Hence, it cannot be guaranteed that votes of such dishonest
voters are counted, unlike what Condition (2) in Deﬁnition 3
requires. So, Deﬁnition 3 would deem almost all e-voting
protocols in settings typically considered nowadays insecure,
even completely reasonable ones.

Also, Condition (1) of Deﬁnition 3 may be too strong in
many cases. It says that the threshold of l tellers is enough to
guarantee that a protocol run is correct, i.e., in terms of the
KTV framework, the judge would accept the run. It might not
always be possible to resolve disputes, for example, when voters
complain (possibly for no reason). For the sake of generality of
the deﬁnition, it would therefore be better to allow for a more
ﬂexible fairness condition, as the one sketched in Section IV.

D. Casting in the KTV Framework

We now cast Deﬁnition 3 in the KTV Framework. To this
end, we have to deﬁne the class of protocols considered in [12]
in terms of the KTV Framework and the goal γ.
Protocol PB. The set of agents Σ consists of the voters,
the judge J, one bulletin board for each of
the tellers,
these participants, and the remaining participants. Since static
corruption is considered, the agents accept a corrupt message
only at the beginning of an election run. The bulletin boards and
the judge do not accept corrupt message at all. As usual, we
consider an additional honest party, the scheduler. The honest
programs are deﬁned as follows:
– The scheduler behaves in the expected way: it triggers all
the parties in every protocol step. The judge is triggered in
the ﬁnal phase, after the tellers are supposed to output their
(partial) tallying.

– The honest behavior of the bulletin boards is as described
in Section II, with the only difference that a bulletin board
owned by some party accepts messages posted only by this
party; it serves its content to all parties, though.

– When a voter V runs her honest program πV, she ﬁrst expects
”yes” or ”no” as input (if the input is empty, she stops). If
the input is ”yes”, she runs πyes, and otherwise πno. She
sends the result to her bulletin board B(V); πV might later
be triggered again to perform veriﬁcation steps.

– When the judge J runs πJ and is triggered in the ﬁnal
phase, it reads the content of all the bulletin boards and
computes the result of the function check on this content. If
check evaluates to ”good”, it outputs ”accept”, and otherwise
”reject”.

– The honest program πT of T depends on the concrete election

system that is used.

The goal. We deﬁne the goal γ∗
0 to be γ0 (see Deﬁnition 2),
with the difference that, instead of considering the multiset
c1, . . . , cnh of choices of honest voters only, we now consider
the multiset of choices of all voters who cast a valid vote. This,
as explained, includes not only honest voters, but might also
include some dishonest voters.
Veriﬁability. Now, it should be clear that the notion of
veriﬁability deﬁned by Benaloh can be characterized in terms
of Deﬁnition 1 as (γ∗
, δ)-veriﬁability.4 As discussed before,
the goal γ∗

0 is too strong for several reasons.

0

4Recall that here we do not consider the fairness conditions.

784784

VI. E2E VERIFIABILITY BY KIAYIAS ET AL.

In this section, we study the end-to-end veriﬁability deﬁni-

tion by Kiayias et al. [34], [33].

A. Model

According to Kiayias et al., an e-voting scheme Π is a tuple
(Setup, Cast, Tally, Result, Verify) of probabilistic polynomial-
time (ppt) algorithms where Cast and Tally are interactive. The
entities are the election authority EA, the bulletin board B, the
tellers T1, . . . , Tm and the voters. The algorithm Cast is run
interactively between B and a voter Vi where the voter operates
a voter supporting device VSD on the following inputs: public
parameters prmpub, a choice ci, and her credentials credi. Upon
successful termination, Vi obtains a receipt αi. The algorithm
Tally is run between EA, the tellers and B. This computation
updates the public transcript τ. The algorithm Verify(τ , αi)
denotes the individual veriﬁcation of the public transcript τ by
voter Vi, while Verify(τ , sti) denotes the veriﬁcation of τ by
teller Ti on her private state sti; the output of Verify is a bit.
The algorithm Setup is run for setting up an election, and the
algorithm Result, given τ, outputs the result of the election, if
any.

B. E2E Veriﬁability

The E2E-veriﬁability deﬁnition by Kiayias et al. [34], [33]
is given in Figure 1. The adversary can corrupt voters and
tellers, and he controls the EA and the VSDs of voters. The
bulletin board is assumed to be honest, but the adversary can
determine the content τ of it. The set Vcast contains all voters
who successfully terminated their protocol, and hence, obtained
a receipt. However, they might not have veriﬁed their receipts.
The adversary wins the game if (i) |Vcast| ≥ θ, i.e., not too few
voters successfully terminated, and (ii) would all of these voters
verify their receipt, then they would verify successfully, and
(iii) the published result of the election Result(τ ) deviates by
at least k from the actual result ρ(c1, . . . , cn) obtained according
to the actual votes of voters. More speciﬁcally, for the last
condition, i.e., Condition (iii), Kiayias et al. postulates the
existence of a vote extractor algorithm Extr (not necessarily
running in polynomial-time) which is supposed to determine
the votes of all voters not in Vcast, where Extr is given the
transcript and the receipt of voters in Vcast as input. Note that
the adversary wins the game if Extr fails to return these votes
(Condition (iii-b)).
Deﬁnition 4 (E2E-veriﬁability). Let 0 < δ < 1 and n,w,k,t, θ ∈
N with k > 0 and 0 < θ ≤ n. The election protocol Π w.r.t.
election function achieves E2E veriﬁability with error δ, for
a number of at least θ honest successful voters and tally
deviation k, if there exists a vote-extractor Extr such that for
any adversary A controlling less than n− θ voters and t tellers,
(cid:3) ≤ δ.
the EA and all VSD’s holds: Pr

GA,Extr,k,θ(1(cid:5),w,n,t) = 1

(cid:2)

We note that [34] considers a fairness condition (named

perfect correctness) similarly to the one in Section III-B.

C. Discussion

We ﬁrst note that the deﬁnition is too speciﬁc in some
situations due to the use of the extractor in the deﬁnition.
Indeed, it does not seem to apply to voting protocols where
ballots published on the bulletin board hide the choices of
voters information-theoretically, such as [24]. In this case, the
adversary could, for example, corrupt some voters but just

E2E Veriﬁability Game GA,Extr,k,θ(1(cid:5),w,n,t)
1) A chooses a list of choices C = {c1, . . . , cw}, a
set of voters {V1, . . . , Vn}, and a set of tellers
{T1, . . . , Tt}. It provides the challenger Ch with
these sets along with information prmpub and
voter credentials {credi}1≤i≤ n. Throughout the
game, Ch plays the role of B.
2) A and Ch engage in an interaction where A
schedules the Cast protocols of all voters. For
each voter Vi, A can either completely control
the voter or allow Ch operate on Vi’s behalf,
in which case A provides a choice ci to Ch.
Then, Ch engages in the Cast protocol with
the adversary A, so that A plays the roles of
EA and VSD. Provided the protocol terminates
successfully, Ch obtains a receipt αi on behalf
of Vi.

3) Finally, A posts the election transcript τ to B.
The game returns a bit which is 1 if the following
conditions hold true:
i) |Vcast| ≥ θ, (i.e., at least θ honest voters termi-
ii) ∀Vi ∈ Vcast : Verify(τ , αi) = 1 (i.e. the honest
voters that terminated veriﬁed successfully)
and either one of the following two conditions:
← Extr(τ ,{αi}Vi∈Vcast
(iii-a). If ⊥ (cid:11)= (ci)
),
then d1(Result(τ ), ρ(c1, . . . , cn)) ≥ k (d1 is a
metric).

Vi /∈Vcast

nated)

(iii-b). ⊥ ← Extr(τ ,{αi}Vi∈Vcast

)

Fig. 1: E2E-veriﬁability by Kiayias et al.

follow the protocol honestly. For these voters and those in
Vcast the extractor could not determine their votes, and hence,
it would be very likely that the adversary wins the game in
Figure 1: if the extractor outputs votes, then it would be very
likely that Condition (iii-a) is satisﬁed, and otherwise Condition
(iii-b) would be satisﬁed.

This problem can be ﬁxed by providing the extractor with
the votes of the voters in Vcast, not only with their receipts.
In this case, the extractor could simply compute Result(τ )
Vi /∈Vcast such that d1(Result(τ ), ρ(c1, . . . , cn)) is
and choose (ci)
minimal. This would be the best extractor, i.e., the one that
makes it the hardest for the adversary to win the game. Note
that this extractor does not have to actually extract votes from
τ, or even look closely at τ, except for computing Result(τ ).
Conditions (iii-a) and (iii-b) could therefore be replaced by

the following one:
(iii)* For any combination of choices (ci)

Vi /∈Vcast :
d1(Result(τ ), ρ(c1, . . . , cn)) ≥ k.

This is then similar to Deﬁnition 2 where votes of dishonest
voters are quantiﬁed existentially. (Note that (iii)* talks about
when veriﬁability is broken, while Deﬁnition 2 talks about the
goal, i.e., what veriﬁability should achieve, hence the switch
from existential quantiﬁcation in Deﬁnition 2 to universal
quantiﬁcation in (iii)*). As explained in Section IV,
the
existential quantiﬁcation is very reasonable because, for several
reasons, it is often not possible to extract votes of dishonest
voters.

785785

Our second observation is that the deﬁnition (even the
version with the ﬁx above) is too weak in the following sense.
Consider runs where honest voters cast their votes successfully,
and hence, obtain a receipt, but do not verify their receipt, and
where the veriﬁcation would even fail. Because of Condition
(ii), the adversary would right away loose the game in these
runs. However, these runs are realistic threats (since often voters
do not verify), and hence, guarantees should be given even for
such runs. The game in Figure 1 simply discards such runs.
Therefore, instead of Condition (ii) one should simply require
that the judge (looking at τ and waiting for complaints from
voters, if any) accepts the run. Note that if the judge does not
accept the run, then the election is invalid.

D. Casting in the KTV Framework
Protocol PKZZ. The set of agents Σ consists of the voters,
the bulletin board B, the voting authority EA, the judge J, the
tellers T1, . . . , Tm and the remaining participants.

When a voter V runs her honest program πV in the casting
phase, she expects a choice c, a credential and the public
parameters of the election (if her input is empty, she stops).
Then, she runs Cast in interaction with B, and expects a receipt
α (if she does not receive a receipt, she stops). When the voter
is triggered by the judge in the veriﬁcation phase, the voter
reads the election transcript τ from the bulletin board B (if she
does not receive τ, she outputs ”reject”) and runs Verify(τ , α).
If Verify(τ , α) evaluates to ”false” or ”true”, respectively, she
sends ”reject” or ”accept” to the judge J. The deﬁnition of
Kiayias et al. is not explicit about whether voters always verify
when triggered or not. So here one could also model that
they decide whether they verify according to some probability
distribution.

When a teller T runs its honest program πT in the setup
phase, it interacts with the remaining tellers, the EA and B.
It expects as output its secret state st (otherwise, it stops). In
the tally phase, on input st and the contents of B (if any input
is empty, it stops), it runs Tally in interaction with B and EA,
and outputs a partial tally ta that is sent to EA.

When the election authority EA runs its honest program
πEA, it expects a security parameter 1(cid:5) in the setup phase (if the
input is empty, it stops). Then, it runs Setup in interaction with
B and the tellers, and outputs the election parameters, which are
published in B, and the voters’ credentials (cred1, . . . , credn),
which are sent to the corresponding voters (V1, . . . , Vn). In the
tally phase, EA runs Tally in interaction with B and the tellers,
and publishes the partial tally data ta1, . . . ,tam produced by
each teller at the end of the interaction.

When the judge J runs its honest program πJ and is triggered
in the veriﬁcation phase, it reads the election transcript τ. It
performs whatever check prescribed by the protocol. If one of
these checks fails, J outputs “reject”. Otherwise, J iteratively
triggers all voters and asks about their veriﬁcation results
(if any). If one of the voters rejects, J outputs “reject”, and
otherwise, “accept”.
E2E veriﬁability. We deﬁne the goal γθ,k,Extr, which is
parameterized by θ, k, and Extr as in Figure 1, to be the
set of runs of PKZZ (with some adversary A) such that at least
one of the Conditions (i), (ii), (iii-a) or (iii-b) in Figure 1 is
not satisﬁed. With this, Deﬁnition 4, corresponds to the notion
of (γθ,k,Extr, δ)-veriﬁability according to Deﬁnition 1 when the
same extractors are used and one quantiﬁes over the same set

of adversaries.

As already discussed above, this deﬁnition on the one hand
is too speciﬁc (due to the use of the extractor) and on the other
hand too weak (due to Condition (ii)). Therefore, as mentioned,
the deﬁnition would be improved if Conditions (iii-a) and (iii-b)
were replaced by (iii)* and Condition (ii) was replaced by the
condition that the judge accepts the run. If one set θ = 0 in
addition, then Deﬁnition 4 would closely resemble γk from
Deﬁnition 2.

VII. COMPUTATIONAL ELECTION VERIFIABILITY BY

CORTIER ET AL.

In this section, we study the deﬁnition of veriﬁability by
Cortier et al. [19], which can be seen as an extension of a
previous veriﬁability deﬁnition by Catalano et al. [32], whereby
the bulletin board may act maliciously, and thus it could
potentially perform ballot stufﬁng (i.e. stuff itself with self-
made ballots on behalf of voters who did not vote) or erase
ballots previously cast by voters.

A. Model

Cortier et al. [19] model an e-voting scheme Π as a
tuple (Setup, Credential, Vote, VerifyVote, Valid, Board, Tally,
Verify) of ppt algorithms where VerifyVote and Verify are
non-interactive. The entities are the registrar Reg, the bulletin
board B, the teller T and the voters. The algorithm Setup((cid:5))
is run by the teller T, and outputs the public parameters of the
election prmpub and the secret tallying key sk. The procedure
Credential is run by Reg with the identity idi of voter Vi,
and outputs a public/secret credential pair (upki, uski). The
algorithms discussed next implicitly take prmpub as input. The
algorithm Vote is run interactively between B and a voter Vi, on
inputs prmpub, a choice ci and her credentials (upki, uski). Upon
successful termination, a ballot bi is appended to the public
transcript τ of the election. The procedure Valid(b) outputs 1
or 0 depending on whether b is well-formed. Board denotes the
algorithm that B must run to update τ. The algorithm Tally is
run at the end of the election by T, given the content of B and
the secret key sk as input, and outputs tallying proofs P and
the ﬁnal election result Result. VerifyVote(τ , upki, uski, b) is an
algorithm run by voter Vi that checks whether ballot b appears
in τ. The algorithm Verify(τ , Result, P) denotes the veriﬁcation
of the result of the election, while VerifyVote(τ , upki, bi)
denotes the veriﬁcation that ballot bi from voter Vi was included
in the ﬁnal transcript of the election as published by B.

B. Veriﬁability Against Malicious Bulletin Board

In the e-voting system Helios [6], a dishonest bulletin board
B may add ballots, since it is the sole entity checking the
eligibility of voters. If B is corrupted, then it might stuff the
ballot box with ballots on behalf of voters that in fact did
not vote. This problem, as already mentioned in Section IV-B,
is called ballot stufﬁng. The work in [19] gives a deﬁnition
of veriﬁability in the computational model to account for a
malicious bulletin board. To defend voters against a dishonest
B, a registration authority Reg is required. Depending on
whether both B and Reg are required to be honest, [19] deﬁnes
weak veriﬁability (both are honest) or strong veriﬁability (not
simultaneously dishonest).

In Figure 2 we give a snapshot of the cryptographic game
used in [19] to deﬁne veriﬁability in case B is dishonest. The
adversary has oracles to register voters, corrupt voters, and

786786

Experiment Expverb
A,Π

creates voters’

Adversary A has access to the following oracles:

• Oreg(id):
credentials via
(upkid, uskid)←Credential(id), stores them as
U = U ∪{(id, upkid, uskid)}, and returns upkid
to the attacker.
• Ocorrupt(id): ﬁrstly, checks if an entry (i,∗,∗)
appears in U ;
if not, stops. Else, gives
(upkid, uskid) to A, updates a list of corrupted
voters C U = C U ∪{(i, upkid)} and updates the
list of honest cast ballots HVote by removing
any occurrence (id,∗,∗).
• Ovote(id, c): if (i,∗,∗) /∈ U , or (i,∗) ∈ C U ,
aborts; else returns b = Vote(i, upkid, uskid, c)
and replaces any previous entry (id,∗,∗) in
HVote with (i, c,b). The latter list is used to
record the voter’s intention.
Let Checked ⊆ HVote contain those id’s who checked
that their ballot appears in τ at the end of the election.
The experiment outputs a bit as follows, with 1
meaning that the attacker was successful:
(1) (τ , Result, P)←AOreg,Ocorrupt,Ovote
(2) if Verify(τ , Result, P) = 0 return 0
(3) if Result =⊥ return 0
(4) if ∃ (iA
,∗), . . . , (iA
, cA
nA
1
1
∈ C s.t. 0 ≤ nB ≤ |C U | s.t.
∃ cB
, . . . , cB
(cid:5)
(cid:4){cE
nB
(cid:12)R ρ
Result = ρ

,∗) ∈ HVote\Checked
(cid:4){cB
(cid:4){cA

, cA
nA

(cid:12)R ρ

(cid:5)

1

}nA
i=1

i

i

i

}nE
i=1
return 0 else return 1
, cE
1

1

(cid:5)

}nB
i=1

where Checked = {(iE
Fig. 2: Veriﬁability against bulletin board by Cortier et al. [19]

), . . . , (iE
nE

,bE
nE

, cE
nE

,bE
1

)}

A,Π) = Pr[Expverb

let honest voters vote. The condition for winning the game
is explained below. Note that Cortier et al. assume that the
result function admits partial counting, namely ρ(S1 ∪ S2) =
ρ(S1) (cid:12)R ρ(S2) for any two lists S1,S2 containing sequences of
elements c ∈ C and where (cid:12)R : R× R → R is a commutative
operation. For example, the standard result function that counts
the number of votes per candidate admits partial counting.
Deﬁnition 5 (Veriﬁability against malicious bulletin board).
An election scheme achieves veriﬁability against the bulletin
board if the success rate Succ(Expverb
A,Π((cid:5)) = 1] of
any ppt adversary A is negligible as a function of (cid:5), where
Expverb

A,Π is deﬁned as in Figure 2.
Roughly speaking, this deﬁnition declares a protocol veriﬁ-
able if, in the presence of a malicious bulletin board (which
can erase previous cast ballots and/or cast ballots on behalf
of absentee voters), voters who check that their ballot has not
been removed are guaranteed that their choice has been counted
in the ﬁnal result. Also some of the votes of honest voters
who did not check might also be contained in the ﬁnal result.
However, their votes may as well have been dropped (but not
altered to other votes). Voters under adversarial control can
only vote once, with choices belonging to the choice space.
The bulletin board cannot stuff itself with additional ballots
without getting caught.

Experiment Expverg
A,Π

Adversary A has access to the oracle Ovote, Ocorrupt
as before, and additionally Ocast, that allows A to
cast ballots to B on behalf of corrupted voters, as
follows:

• Ocast(id, b): run Board(τ , b).

Let HVote and Checked be the lists deﬁned before.
that HVote contains entries of the form
Recall
(id, c, b), that stand for honest voters’ choices. The
experiment outputs a bit as follows:
(1) (Result, P)←AOcast,Ocorrupt,Ovote
(2) if Verify(τ , Result, P) = 0 return 0
(3) if Result =⊥ return 0
,∗), . . . , (iA
(4) if ∃ (iA
, cA
nA
1
1
∈ C s.t. 0 ≤ nB ≤ |C U | s.t.
∃ cB
, . . . , cB
(cid:5)
(cid:4){cE
nB
}nE
(cid:12)R ρ
Result = ρ
i=1
return 0 else return 1
where Checked = {(iE
Fig. 3: Veriﬁability against registrar by Cortier et al. [19]

,∗) ∈ HVote\Checked
(cid:4){cA
(cid:4){cB

), . . . , (iE
nE

}nB
i=1

}nA
i=1

,bE
nE

, cA
nA

, cE
nE

(cid:12)R ρ

,bE
1

, cE
1

)}

(cid:5)

(cid:5)

1

1

i

i

i

C. Veriﬁability Against Malicious Registrar

In Helios, the bulletin board B accepts only ballots cast
by eligible voters. The bulletin board B can tell apart eligible
from ineligible voters generally by using some kind of authen-
tication mechanism. In this situation, one might hope to enjoy
veriﬁability against a dishonest registrar Reg, which is deﬁned
in Figure 3.
Deﬁnition 6 (Veriﬁability against malicious registrar). An
election scheme achieves veriﬁability against the registrar if
the success rate Succ(Expverg
A,Π((cid:5)) = 1] of any ppt
adversary A is negligible as a function of (cid:5), where Expverg
A,Π is
deﬁned as in Figure 3.

A,Π) = Pr[Expverg

The intuition behind and the guarantees provided by
Deﬁnition 6 are similar to those of Deﬁnition 5 except that
instead of a malicious bulletin board a malicious registrar is
considered, which thus can handle credentials for voters in a
malicious way, i.e. provide invalid credentials or make several
users share the same credentials.

D. Strong Veriﬁability

A protocol is said to have strong veriﬁability if it enjoys
veriﬁability against a dishonest registrar and veriﬁability against
a dishonest bulletin board. Intuitively, this allows one to give
veriﬁability guarantees under a weaker trust assumption than
that used in Section VI, since for strong veriﬁability we do
not need the bulletin board and the registrar to be honest
simultaneously; in Section V, it was assumed that every party
has its own bulletin board, and in Section IV, no speciﬁc trust
assumptions were ﬁxed or assumed.

We note Cortier et al. also consider a fairness (correctness)
condition similar to the ones mentioned above: the result
corresponds to the votes of honest voters whenever all the
parties (Reg, T, B), including the voters, are honest.

787787

Experiment Expverw
A,Π

to

the

access

Adversary A has
oracles
Ovote, Ocorrupt, Oreg and Ocast deﬁned before
in this section. Let HVote the list containing the
intended choices of the honest voters. The experiment
outputs a bit as follows:
(1) (Result, P)←AOcast,Ocorrupt,Ovote,Ocast
(2)
(3)
(4)

if Verify(τ , Result, P) = 0 return 0
if Result =⊥ return 0
,∗) ∈ HVote
,∗), . . . , (iA
if ∃ (iA
, cA
, cA
nA
nA
1
1
∈ C s.t. 0 ≤ nB ≤ |C U |
∃ cB
, . . . , cB
(cid:5)
(cid:4){cA
(cid:4){cB
nB
}nB
}nA
s.t. Result = ρ
i=1
i=1

(cid:12)R ρ

(cid:5)

1

i

i
return 0 else return 1

Fig. 4: Weak veriﬁability by Cortier et al. [19]

E. Weak Veriﬁability

For weak veriﬁability, the trust assumptions are stronger:
both the registrar Reg and the board B are assumed to be honest.
This means, in particular, that B does not remove ballots, nor
stuffs itself; and that Reg faithfully distributes credentials to
the eligible voters. The formal deﬁnition is given in Figure 4.
Intuitively, weak veriﬁability guarantees that all votes that
have been successfully cast are counted and that dishonest
voters can only vote once; additionally only choices belonging
to the choice space can be cast and counted.

F. Tally Uniqueness

As part of their deﬁnitional framework for veriﬁability,
Cortier et al. [19] and Juels et al. [32], require the notion
of tally uniqueness. Roughly speaking, tally uniqueness of a
voting protocol ensures that the tally of an election is unique,
even if all the players in the system are malicious.

More formally, the goal of the adversary against tally unique-
ness is to output public election parameters prmpub, a public
transcript τ, two results Result (cid:11)= Result
(cid:16)
, and corresponding
(cid:16) such that both pass veriﬁcation,
proofs of valid tallying P and P
(cid:16), P
(cid:16)) = 1. A voting
i.e. Verify(τ , Result, P) = Verify(τ , Result
protocol Π has tally uniqueness if every ppt adversary A has a
negligible advantage in this game.

Following [19], tally uniqueness ensures that, given a
tally, there is at most one plausible instantiation (one-to-one
property).

G. Discussion

Strong veriﬁability explicitly captures the situation where
key players in an electronic election, such as the bulletin board
or the registrar, might be corrupted and willing to alter the
legitimate operation of the election. This is notably the case
for Helios without identiﬁers (i.e. the transcript τ does not
contain voters’ identiﬁers), where a malicious B can stuff itself
with ballots on behalf of absentee voters. Additionally, strong
veriﬁability provides stronger guarantees, compared to previous
deﬁnitions, to honest voters: ballots from honest voters that
do not verify successfully at the end of the election can at
worst be removed from the election’s announced result, but
never changed. In [19], sufﬁcient properties for proving strong
veriﬁability have been established.

A downside of the above deﬁnitions is that the voter’s intent
is not captured by the oracle Ovote(id, c), as this oracle simply
performs the honest voting algorithm. In reality, voters typically
use some VSD, which might be corrupted. Additionally, since
Cortier et al. require that the adversary wins the game (i.e.,
successfully cheats) with at most negligible probability, ballot
audit checks, such as Benaloh’s audits5 [11], are deemed
non-veriﬁable as these checks may fail with non-negligible
probability. Another weak point, although less important than
the previous ones, is that this framework assumes that the result
function ρ admits partial tallying, which is commonly the case,
but it is, for example, not applicable to voting protocols which
use the majority function as the result function.

H. Casting in the KTV Framework
Protocol PCGGI. The set of agents Σ consists of the voters,
the bulletin board B, the registrar Reg, the teller T, judge J,
the scheduler, and the remaining participants. As usually, we
assume that the judge and the scheduler cannot be corrupted
(they ignore the corrupt message). As in the deﬁnition of Cortier
et al., Reg and B can be corrupted statically, i.e., they accept
the corrupt message at the beginning of a run only. Voters can
be corrupted dynamically.

When the voter V runs her honest program πV, she expects
a candidate c, a credential pair upk, usk as input (if the input is
empty, she stops). After that, she reads the election parameters
prmpub and C from the bulletin board B (if she cannot ﬁnd
any election paramaters on B, she stops). Then, she runs
Vote(prmpub
, c, upk, usk) and sends the result b to the bulletin
board B. Once the election is closed, she reads the content of the
bulletin board and checks whether her ballot has been properly
handled by the ballot box by running VerifyVote(τ , upk, usk, b).
If not, the voters send her complaint to the judge. The program
of the judge accepts a run, if it does not receive any complaint
from a voter and the procedure Verify(τ , Result, P) returns 1.
When the registrar Reg runs the honest program πR, it
generates and distributes secret credentials to voters and
registers the corresponding public credentials in the bulletin
board.
When the teller T runs its honest program πT, it reads
the public transcript τ and runs (Result, P)←Tally(τ , sk), with
the election private key sk. The transcript is updated to τ(cid:16) =
τ||Result||P.
Strong veriﬁability. We deﬁne the goal γSV to be the set
of all runs of PCGGI in which either (a) both Reg and B are
corrupted, (b) the result is not output, or (c) the result r of the
election is deﬁned and satisﬁes:

(cid:4){cE

(cid:4){cA
i }nA
i=1
, cB
, cA
for some nE ,nA,nB and some cE
i
i

i }nE

r = ρ

(cid:12)R ρ

(cid:4){cB
i }nB
(cid:12)R ρ
i such that

i=1

(cid:5)

(cid:5)

i=1

(cid:5)

, . . . , cE

• cE
nE are the choices read by honest voters that
1
successfully checked their ballots at the end of the election
(and report it to the judge).
• w1, . . . ,wmA are the candidates read by honest voters that
did not check their ballots and {cA
∈ C and nb is smaller then the number of voters
• cB
, . . . , cB
nb

⊆ {w j}mA
j=1;

}nA
i=1

1
running a dishonest program.

i

5In these audits the voter can decide to cast or to audit a ballot created by
her VSD. If she decides to audit the ballot, she can check whether it actually
encodes her choice.

788788

Note that, according to the above deﬁnition, if both the registrar
and the bulletin board are corrupted, then the goal is trivially
achieved, as we do not expect to provide any guarantees in
this case.

For the protocol PCGGI, strong veriﬁability by Cortier et
al. can essentially be characterized by the fact that it is (γSV , δ)-
veriﬁable by the judge J in the sense of Deﬁnition 1, for δ = 0.
Let us emphasize that this goal ensures that votes of honest
voters who do not verify at the end of the election are at most
dropped, but not changed. This is in contrast to the goals we
have seen so far. In these goals, votes of honest voters who do
not verify might have been tampered with.
Weak veriﬁability. We deﬁne the goal γWV to be the set of all
runs of PCGGI in which either (a) either Reg or B is corrupted,
(b) the result is not output, or (c) the result r of the election is
deﬁned and satisﬁes:

(cid:4){cA

i }nA
for some nA,nB and some cA
i

r = ρ

i=1

, cB

(cid:5)

(cid:5)

(cid:12)R ρ

(cid:4){cB
i }nB
i such that

i=1

• cA
• cB

, . . . , cA

, . . . , cB
nb

nA are the candidates read by honest voters that
∈ C and nb is smaller then the number of voters

1
cast their votes;
1
running a dishonest program.
For the protocol PCGGI, weak veriﬁability by Cortier et
al. can essentially be characterized by the fact that it is (γWV , δ)-
veriﬁable in the sense of Deﬁnition 1.

Note that Item (c) of the goal γWV is stronger than the
corresponding item of γSV (since all honest cast votes shall be
counted). However, the latter is called weak veriﬁability in [19]
because the trust assumptions (Item (a)) are stronger (both the
ballot box and the registrar shall be honest).

VIII. COMPUTATIONAL ELECTION VERIFIABILITY BY

SMYTH ET AL.

This section focuses on the deﬁnitions of individual,
universal and election veriﬁability by Smyth et al. [47]. Smyth
et al. consider two different veriﬁability settings, one for
election schemes with external and the other one for election
schemes with internal authentication (such as Helios and
Civitas, respectively). For the sake of brevity, we focus on
election schemes with external authentication because the issues
discussed in Section VIII-E apply to both of them. We present
the casting of the deﬁnitions of Smyth et al. in the KTV
framework in Appendix C.

A. Model

According to Smyth et al., an election scheme Π is a tuple
(Setup, Vote, Tally, Verify) of probabilistic polynomial-time
algorithms. The algorithms Setup and Vote are deﬁned as
usual. The algorithm Tally is run by the tellers and receives
the content of the bulletin board B and the parameters prm
as input, and outputs the tally along with a non-interactive
proof P for the correctness of the tally. The algorithm Verify
describes the veriﬁcation of the election result and receives the
content of the bulletin board B, the public parameters prmpub,
the tally, denoted by tally, and a proof P, and outputs a bit.
The algorithm Verify is deterministic.

789789

Experiment ExpIV(Π, A)
(cid:16)) ←− A
(1) (prmpub
(2) b ←− Vote(c, prmpub
(cid:16), prmpub
(3) b
(4) if b = b

, c, c
(cid:16) ←− Vote(c

(cid:16) and b (cid:11)= ⊥ and b

)

)

(cid:16) (cid:11)= ⊥ then

return 1 else return 0

Fig. 5: Individual veriﬁability experiment by Smyth et al. [47]
Experiment ExpUV(Π, A)

(cid:16), P

(cid:16)) ←− A

(1) (B, prmpub
, tally
(2) tally ←− correct tally(B, prmpub
(3) if tally (cid:11)= tally

and Verify(B, prmpub

)

(cid:16)

, tally

(cid:16), P

(cid:16)) then

return 1 else return 0

Fig. 6: Universal veriﬁability experiment by Smyth et al. [47]

B. Individual Veriﬁability

According to Smyth et al., an election scheme achieves
individual veriﬁability if, for any two honest voters,
the
probability that their ballots are equal is negligible, which
formally is expressed as follows.
Deﬁnition 7 (Individual veriﬁability). An election scheme Π =
(Setup, Vote, Tally, Verify) achieves individual veriﬁability if
the success rate Succ(ExpIV(Π, A)) of any ppt adversary A in
Experiment ExpIV(Π, A) (Fig. 5) is negligible as a function of
(cid:5).

C. Universal Veriﬁability

According to Smyth et al., an election scheme achieves
universal veriﬁability if no ppt adversary A can simulate a
tallying phase such that, on the one hand, the veriﬁcation
algorithm Verify accepts the output (e.g., all zero-knowledge
proofs are successful), and, on the other hand, the given output
of the tallying phase does not agree with what Smyth et al. call
the correct tally.

The function correct tally, deﬁned as follows, extracts the

actual votes from the ballots on the bulletin board.
Deﬁnition 8 (Correct Tally). The function correct tally maps
) to a vector in {0, . . . , nballots}ncand such
each tuple (B, prmpub
that for every choice c ∈ {1, . . . , ncand} and every number l ∈
{0, . . . , nballots} we have that correct tally(B, prmpub
)[c] = l if
and only if there are exactly l different ballots b ((cid:11)= ⊥) on the
bulletin board B and for each of them there exists a random
bit string r such that b = Vote(c, prmpub;r).

Now, universal veriﬁability is deﬁned as follows according

to Smyth et al.
Deﬁnition 9 (Universal veriﬁability). An election scheme
(Setup, Vote, Tally, Verify) achieves universal veriﬁability if
the success rate Succ(ExpUV(Π, A)) of every ppt adversary A
in Experiment ExpUV(Π, A) (Fig. 6) is negligible as a function
of (cid:5).

D. Election Veriﬁability

The notion of veriﬁability proposed by Smyth et al. now
combines the notions of individual and universal veriﬁability.

Deﬁnition 10 (Election Veriﬁability). An election scheme
(Setup, Vote, Tally, Verify) satisﬁes election veriﬁability if
for every ppt adversaries A, there exists a negligible function
μ such that for all security parameters (cid:5), we have that
Succ(ExpIV(Π, A)) + Succ(ExpUV(Π, A)) ≤ μ((cid:5)).

Smyth et al. also consider some soundness properties, including
fairness and correctness, similar to the ones mentioned in
previous sections.

E. Discussion

This deﬁnition has two main shortcomings. First, as stated
by the authors, their “deﬁnitions of veriﬁability have not
addressed the issue of voter intent, that is, whether the ballot
constructed by the Vote algorithm corresponds to the candidate
choice that a voter intended to make.” (Page 12, [47]). In
general, it is not clear that the combination of the proposed
deﬁnitions of veriﬁability along with additional soundness
properties implies any form of end-to-end veriﬁability. More
precisely, if all the veriﬁcation procedures succeed, it is unclear
whether the ﬁnal outcome of an election corresponds to the
voters’ choices at least with reasonable probability.6 We think,
however, that capturing such overall correctness and the voter’s
intent is at the very core of a meaningful notion of veriﬁability.
Second, the deﬁnition considers a restricted class of proto-
cols (the authors themselves provide a list of protocols excluded
by their deﬁnition), some of these restrictions, as pointed out
before, also apply to some of the other deﬁnitions discussed
in this paper: (1) The model captures “single-pass” protocols
only: voters send a single ballot to the election server, without
any further interaction. (2) The authors assume that the whole
ballot is published. (3) The authors assume that the vote can
be recovered directly from the ballot, which excludes protocols
using information-theoretically hiding commitments. (4) There
is no revote. (5) The bulletin board publishes the list of ballots,
as received. And hence, voting schemes such as ThreeBallot
[45] cannot be modeled.

As mentioned before, the casting of the Smyth et al. deﬁ-

nitions in the KTV framework is presented in Appendix C.

IX. FURTHER RELATED WORK

Since the focus of this paper is on veriﬁability notions that
have been formally deﬁned, we excluded those veriﬁability
notions from our analysis which do not fulﬁll this requirement
([46], [30], [51], [43], [41], [42]). An important paper is the
one by Sako and Kilian [46] who were the ﬁrst to propose
the concept of individual and universal veriﬁability. This then
motivated other researchers to regard end-to-end veriﬁability
as the sum of certain veriﬁability subproperties; we discuss
this issue in Section X.

Due to space constraints, a few formal deﬁnitions of
veriﬁability are discussed and cast in the KTV framework

6It indeed seems that this is not the case due to some technical issues: Their
correctness property requires only that Vote correctly encodes the given choice
in the case of the honest setup; it does not guarantee anything for the dishonest
setup which is considered in the veriﬁability games. Therefore, if, for instance,
Vote always produces ⊥ (an invalid ballot) for some dishonestly generated
public key, the system can still be proved veriﬁable according to the deﬁnition
of Smyth et al., although it clearly produces a wrong election outcome. This
particular technicality seems to be easy to ﬁx, but it nevertheless demonstrates
that there is some gap between the given combination of disconnected properties
and an overarching and meaningful veriﬁability notion.

in the appendix or in the full version [20] of this paper only.
We brieﬂy discuss them here.

Kremer et al. [35] (Appendix A) and Cortier et al. [18]
(Appendix B) deﬁne veriﬁability in symbolic models, where
messages are modeled by terms. Kremer et al. propose a
deﬁnition that corresponds to γ0 but under the trust assumption
that every voter is honest and veriﬁes the ﬁnal result, which is
clearly too strong. Cortier et al. [18] devise formulations for
individual veriﬁability, universal veriﬁability, and no clash (two
honest ballots should never collude), and they show that these
three properties imply what they call end-2-end veriﬁability, the
latter being close to the goal γSV (introduced in Section VII),
except that ballot stufﬁng is not prohibited.

In the full version of this paper [20], we also analyze the
deﬁnition by Baum et al. [8] (Szepieniec et al. [49] proposed
a closely related deﬁnition), the one by Chevallier-Mames et
al. [16], and by Hosp et al. [28]. The deﬁnition by Baum et
al. (auditable correctness) can be applied to arbitrary multi-
party computation (MPC) protocols and is based on an ideal
functionality in the Universal Composability (UC) framework.
In the context of e-voting protocols, the goal of this deﬁnition
is γ0. Baum et al. also consider a very (in fact too) strong
fairness condition: auditors have to always accept a protocol run
if the goal γ0 is achieved, regardless of whether, for example,
zero-knowledge proofs are valid or not. As for the deﬁnition by
Chevallier-Mames et al., it captures universal veriﬁability, and
hence, a subproperty of end-to-end veriﬁability only. Hosp et al.
propose information-theoretic measures for the veriﬁability of
voting systems, by comparing these systems to perfect voting
systems which always output the correct result, independently
of voters being honest or dishonest. This deﬁnition is even
much stronger than what is required by γ0, and therefore, does
not seem to be applicable to any practical voting protocol.

X. SUMMARY AND CONCLUSION

In the previous sections, we have studied the formal
deﬁnitions of veriﬁability for e-voting system proposed in the
literature. We have presented the original deﬁnitions and cast
them in the KTV framework. This casting has demonstrated
that the essence of these notions can be captured within a
uniform framework and enabled us to identify their relative
and recurrent merits and weaknesses as well as their speciﬁc
(partly severe) limitations and problems.

In Section X-A, we distill these discussions and insights
into detailed requirements and guidelines that highlight several
aspects any veriﬁability deﬁnition should cover. We also
summarize from the previous sections how the different
existing deﬁnitions of veriﬁability from the literature handle
these aspects, with a brief overview for some of the aspects
provided in Table I. Finally, in Section X-B, as a viable and
concrete embodiment of our guidelines, we instantiate the KTV
framework accordingly, obtaining a solid and ready to use
deﬁnition of veriﬁability.

A. Guidelines

We now present our requirements and guidelines for the
following central aspects, along with a summary of the previous
sections concerning these aspects.
Generality. Many veriﬁability deﬁnitions are designed for
protocols with speciﬁc protocol structures and are tailored to
them (see Sections VI, VII, VIII and Appendix A, B). As a

790790

TABLE I: Overview of Veriﬁability Notions

Notion (Section & Paper)

Veriﬁability (IV, [37])

Veriﬁability (V, [12])

E2E veriﬁability (VI, [34])

Strong veriﬁability (VII, [19])

Weak veriﬁability (VII, [19])

Individual veriﬁability (VIII,
[47])
Universal veriﬁability (VIII,
[47])
Election
[47])
Individual and universal veri-
ﬁability (A, [35])

veriﬁability

(VIII,

Individual
[18])
Universal
[18])

veriﬁability

(B,

veriﬁability

(B,

E2E veriﬁability (B, [18])

Veriﬁability goal (Intuition)
Flexible γ, with γk (k ≥ 0) being one example
γ∗
0 (The votes of all eligible (honest and dishonest)
voters who submit valid ballots are counted.)
γθ,k,Extr (θ,k ≥ 0) (Either (i) the published result
differs on less than k positions from the correct re-
sult (as extracted by Extr), or (ii) less than θ many
honest voters successfully cast their ballots, or (iii)
at least one of these honest voters complains if she
veriﬁes the ﬁnal result.)
γSV (If Reg or B are honest, then (i) the votes
of all honest voters who check are counted, and
(ii) further honest votes can only be dropped (not
manipulated), and (iii) only votes of eligible voters
are counted.)
γWV (If Reg and B are honest,
achieved.)
γIV (All honest voters’ valid ballots are pairwise
different.)
γUV ((Tally, P) published on B and Tally =
correct tally(B))
γIV ∧ γUV

then γ0

is

γIUV (γ0 and all honest voters’ ballots are pairwise
different.)
γIV (The ballots of all honest voter who check are
on B.)
γUV (The votes of all honest voters whose ballots
are on B are counted. Non-eligible voters are
allowed.)
γE2E (The votes of all honest voter who check are
counted. Non-eligible voters are allowed.)

Veriﬁability
tolerance

General trust
assumptions

Protocol classes

Yes

Yes

Yes

No

No

No

No

No

No

No

No

No

Flexible

hon(B)

hon(B)

Flexible

No speciﬁc structure required.
Assumes personal bulletin board for each protocol
participant. Only ”yes” or ”no” choices possible.
Otherwise no speciﬁc protocol structure.

Assumes speciﬁc (Setup, Cast, Tally, Result,
Verify) protocol structure. Requires extraction
property.

Assumes speciﬁc (Setup, Credential, Vote,
VerifyVote, Valid, Board, Tally, Verify)
protocol structure. Tally function with partial
tallying property.

hon(B)

Assumes speciﬁc (Setup, Vote, Tally, Verify)
protocol structure. Requires extraction property.

∧

hon(B)
(cid:6)
n
i=1 hon(Vi)

hon(B)

Requires bulletin board B. Assumes that all voters
verify. Otherwise no speciﬁc protocol structure.

Assumes speciﬁc structure and extraction
property for the deﬁnition of individual and
universal veriﬁability. Requires bulletin board B
for the E2E veriﬁability deﬁnition, otherwise no
speciﬁc protocol structure.

Table description: We group associated deﬁnitions. For each goal we have extracted from the deﬁnition, we include a short informal description.
The third column (”Veriﬁability tolerance”) states whether or not the associated veriﬁability deﬁnition allows for some tolerance: ”Yes” if δ ≥ 0
is allowed, ”No” if δ = 0 is required, with δ as in Deﬁnition 1. The fourth column (”General trust assumptions”) describes which protocol
participants are assumed to be always honest (besides the judge and the scheduler) and in the ﬁfth column (”Protocol classes”) requirements on
the protocol structure are listed, where extraction property is the requirement that single ballots and their content (i.e. the plain vote) can be
extracted from the bulletin board B.

result, for new classes of protocols often new deﬁnitions are
necessary.

Clearly, it is desirable for a veriﬁability deﬁnition to be
applicable to as many protocols as possible. It provides not
only reusability, but also comparability: by applying the same
deﬁnition to different protocols and protocol classes we can
clearly see the differences in the level and nature of veriﬁability
they provide. A very minimal set of assumptions on the
protocol structure is sufﬁcient to express a meaningful notion
of veriﬁability, as illustrated by the deﬁnition in Section IV and
also by the instantiation of the KTV framework given below.
Note, however, that some additional assumptions on the
protocol structure allow one to express some speciﬁc properties,
such as universal veriﬁability, which, as discussed in the
previous sections, on their own do not capture end-to-end
veriﬁability, but may be seen as valuable additions.
Static versus dynamic corruption. We observe that most of
the studied veriﬁability deﬁnitions focus on static corruption,
except the deﬁnitions in Sections VI and VII, which capture the
dynamic corruption of voters. In general, modeling dynamic
corruption can yield stronger security guarantees. In the context
of veriﬁability, one could, for example, provide guarantees not

only to honest voters but also to certain corrupted voters. If
a voter is corrupted only late in the election, e.g., when the
voting phase, one might still want to guarantee that her vote is
counted. None of the existing deﬁnitions provide this kind of
guarantee so far. We brieﬂy discuss how this can be captured
in the KTV framework in Section X-B.

Binary versus quantitative veriﬁability. As discussed in
Section III-B, the probability δ (see Deﬁnition 1) that under
realistic assumptions some cheating by an adversary remains
undetected may be bigger than 0 even for reasonable protocols:
often some kind of partial and/or probabilistic checking is
carried out, with Benaloh audits (see Section VII-G) being an
example. These checks might fail to detect manipulations with
some non-negligible probability. Still, as we have seen when
casting the different veriﬁability notions in the KTV framework,
most of the studied deﬁnitions assume the veriﬁability tolerance
to be δ = 0. This yields a binary notion of veriﬁability which,
as explained, outright rejects reasonable protocols.

In contrast, the deﬁnitions studied in the KTV framework
(including Section IV) as well as the ones in Sections V and
VI , allow for measuring the level of veriﬁability. This gives
more expressiveness and allows one to establish meaningful

791791

veriﬁability results for (reasonable) protocols which do not
provide perfect veriﬁability.
Goals. As pointed out in Section IV, the goal γ0, which,
among others, requires that all the ballots cast by honest voters
are correctly tallied and make it to the ﬁnal result is very strong
and typically too strong. In order to satisfy this goal very strong
trust assumptions are necessary, for instance, the assumptions
taken in the deﬁnition of weak veriﬁability in Section VII.

From the previous sections,

two main and reasonable
approaches for deﬁning a goal emerged, which one could
characterize as quantitative and qualitative, respectively:
Quantitative. In Section IV, a family of goals γk, k ≥ 0,
together with a non-zero tolerance level δ is considered;
a similar approach is taken in Section VI, but see the
discussion in this section. This approach, among others,
captures that the probability that more than k votes of
honest voters can be changed without anybody noticing
should be small, i.e., bounded by δ. To be more precise
and allow for stronger guarantees, this approach could be
combined with an aspect of the goal deﬁned for strong
veriﬁability, namely the distinction between votes that
are manipulated and those that are “just” dropped (see
Section VII).

Qualitative. In Section VII (“strong veriﬁability”), the protocol
goal (as cast in the KTV framework), among others,
stipulates that votes of voters who verify their receipt are
contained in the ﬁnal result. To be general, this approach
should also be combined with a non-zero tolerance level
δ (which, however, was not captured in the original
deﬁnition). The reason is that checks (such as Benaloh
challenges) might not be perfect, i.e., manipulation might
go undetected with some probability.

In both cases, votes of dishonest voters were restricted to be
counted at most once (no ballot stufﬁng).

The quantitative approach, on the one hand, provides overall
guarantees about the deviation of the published result from the
correct one and measures the probability δ that the deviation
is too big (bigger than k) but nobody notices this. On the
other hand, it does not explicitly require that voters who check
their receipts can be sure (up to some probability) that their
votes were counted. But, of course, to prove veriﬁability of a
system w.r.t. this goal, one has to take into account whether
or not voters checked, and more precisely, the probabilities
thereof. These probabilities also capture the uncertainty of the
adversary about whether or not speciﬁc voters check, and by
this, provides protection even for voters who do not check.

The qualitative approach explicitly provides guarantees for
those honest voters who verify their receipts. On the one hand,
this has the advantage that one does not need to consider
probabilities of voters checking or not, which simpliﬁes the
analysis of systems. On the other hand, such probabilities of
course play an important role for measuring the overall security
of a system, an aspect this simpler approach abstracts away.
Nevertheless, it provides a good qualitative assessment of a
system.

Interestingly, one could in principle combine both ap-
proaches, i.e., consider the intersection of both goals. While
this would give voters also in the quantitative approach direct
guarantees (in addition to the aspect of making a distinction
between manipulating and dropping votes, mentioned above
already), it would typically not really change the analysis and

792792

its result: as mentioned, in the quantitative analysis one would
anyway have to analyze and take into account the guarantees
offered when checking receipts.

Below, we provide concrete instantiations for both ap-

proaches in the KTV framework.
Ballot stufﬁng. Not all deﬁnitions of veriﬁabiltiy rule out
ballot stufﬁng, even though ballot stufﬁng, if unnoticed, can
dramatically change the election result. Some deﬁnitions go
even further and abstract away from this problem by assuming
that there are only honest voters (see trust assumptions below).
Clearly, allowing undetected ballot stufﬁng makes a veriﬁa-
bility deﬁnition too weak. We recommend that a veriﬁability
deﬁnition should exclude undetected ballot stufﬁng. It might
also be useful to capture different levels of ballot stufﬁng in
order to distinguish systems where it is very risky to add even
a small number of ballots from those where adding such a
small number is relatively safe. The goals discussed above, as
mentioned, both require that no ballot stufﬁng is possible at
all.
Trust assumptions. Some veriﬁability deﬁnitions assume
some protocol participants to be always honest, for example
the bulletin board (Sections V, VI, VIII, Appendix A, B),
or all voters (Appendix A) or all voter supporting devices
(Sections VIII, VII), or some disjunctions of participants
(Section VII); the deﬁnition discussed in Section IV does not
make such assumptions. We think that veriﬁability deﬁnitions
which rely on the unrealistic assumption that all voters are
honest are too weak. The other trust assumptions might
be reasonable depending on the threat scenario. A general
veriﬁability deﬁnition should be capable of expressing different
trust assumptions and make them explicit; embedding trust
assumptions into a deﬁnition not only makes the deﬁnition less
general, but also makes the assumptions more implicit, and
hence, easy to overlook.
Individual and universal veriﬁability.
In Section VIII and
Appendix B, deﬁnitions of individual and universal veriﬁability
were presented. We already pointed out that the split-up of
end-to-end veriﬁability into sub-properties is problematic. In
fact, K¨usters et al. [39] have proven that, in general, individual
and universal veriﬁability (even assuming that only eligible
voters vote) do not imply end-to-end veriﬁability, e.g. for
ThreeBallot [45]. For the deﬁnitions of individual and universal
veriﬁability presented in Section VII, it was shown in [18]
that they imply end-to-end veriﬁability under the assumption
that there are no clashes [39]. However, the notion of end-to-
end veriﬁability considered there is too weak since it allows
ballot stufﬁng. For the deﬁnitions of individual and universal
veriﬁability in Section VIII no such proof was provided, and
therefore, it remains unclear whether it implies end-to-end
veriﬁability. (In fact, technically these deﬁnitions, without some
ﬁxes applied, do not provide end-to-end veriﬁability as pointed
out in Section VIII.)

The (combination of) notions of individual and universal
veriﬁability (and other properties and subproperties, such as
eligibility veriﬁability, cast-as-intended, recorded-as-cast, and
counted-as-recorded) should not be used as a replacement for
end-to-end veriﬁability per se since they capture only speciﬁc
aspects rather than the full picture. Unless formally proven that
their combination in fact implies end-to-end veriﬁability they
might miss important aspects, as discussed above. Therefore,
the security analysis of e-voting systems should be based on the

notion of end-to-end veriﬁability (as, for example, concretely
deﬁned below). Subproperties could then possibly be used as
useful proof techniques.

B. Exempliﬁed Instantiation of the Guideline

We now demonstrate how the guidelines given above can be
put into practice, using, as an example, the KTV framework. By
this, we obtain a solid, ready-to-use deﬁnition of veriﬁability.
More speciﬁcally, we propose two variants, one for qualitative
and one for quantitative reasoning, as explained next.

The distillation of our observations given in Section X-A
reviews different aspects of veriﬁability and, in most cases,
it clearly identiﬁes the best and favorable ways they should
be handled by veriﬁability deﬁnitions. When it comes to the
distinction between qualitative and quantitative approaches to
deﬁne veriﬁability goals, we have, however, found out that both
approaches have merits and both can yield viable deﬁnitions
of veriﬁability. This is why we propose two instantiations of
the KTV framework, one following the qualitative approach
and one for the quantitative approach.

To instantiate the KTV framework, one only has to provide
a deﬁnition of a goal (a family of goals) that a protocol is
supposed to guarantee. Note that, as for the second parameter
of Deﬁnition 1, δ, one should always try, for a given goal, to
establish an as small δ as possible. In other words, the value
of δ is the result of the analysis of a concrete system, rather
than something ﬁxed up front.

In the following, we deﬁne two goals corresponding to the
two variants of veriﬁability discussed above: goal γql(ϕ) for
the qualitative variant and goal γqn(k, ϕ) for the quantitative
one. We explain the meaning of the parameters below. Here we
only remark that the common parameter ϕ describes the trust
assumptions (i.e., it determines which parties are assumed to be
honest and which can be corrupted and when) under which the
protocol is supposed to provide speciﬁc guarantees. Recall that,
in the KTV framework, the adversary sends a special message
corrupt to a participant in order to corrupt it (a participant can
then accept or reject such a message). This allows for modeling
various forms of static and dynamic corruption. Note also that
it is easily visible, given a run, if and when a party is corrupted.
In the following, for a given run r of an e-voting protocol
with n eligible voters, we denote by nh the number of honest
and by nd the number of dishonest voters in r. Recall that
we say a party is honest in a run r if it has not received a
corrupt message or at least has not accepted such a message
throughout the whole run. We denote by c1, . . . , cnh the actual
choices of the honest voters in this run (which might include
abstention), as deﬁned in Section IV-A.
Qualitative goal. The goal γql(ϕ) we deﬁne here corresponds
to the strong veriﬁability goal γSV from Section VII. In contrast
to γSV , γql(ϕ) has the parameter ϕ for the trust assumptions,
which were ﬁxed in γSV . Informally, this goal requires that, if
the trust assumption ϕ holds true in a protocol run, then (i)
the choices of all honest voters who successfully performed
their checks are included in the ﬁnal result, (ii) votes of those
honest voters who did not performed their check may be
dropped, but not altered, and (iii) there is only at most one
ballot cast for every dishonest voter (no ballot stufﬁng). If the
trust assumptions ϕ are not met in a protocol run, we do not
expect the protocol to provide any guarantees in this run. For
example, if in a setting with two bulletin boards, ϕ says that at

least one of the bulletin boards should be honest in a run, but in
the run considered both have been corrupted by the adversary,
then no guarantees need to be provided in this run.
Formally, the goal γql(ϕ) is satisﬁed in r (i.e., r ∈ γql(ϕ))
if either (a) the trust assumption ϕ does not hold true in r, or
if (b) ϕ holds true in r and there exist valid choices ˜c1, . . . , ˜cn
for which the following conditions are satisﬁed:

ρ(˜c1, . . . , ˜cn).

(i) An election result is published in r and it is equal to
(ii) The multiset {˜c1, . . . , ˜cn} consists of all the actual choices
of honest voters who successfully performed their check,
plus a subset of actual choices of honest voters who did
not perform their check (successfully), and plus at most
nd additional choices.

If the checks performed by voters do not fully guarantee
that their votes are actually counted, because, for example,
Benaloh checks were performed (and hence, some probabilistic
checking), then along with this goal one will obtain a δ > 0,
as there is some probability for cheating going undetected.
Also, the requirement that votes of honest voters who do not
checked can at most be dropped, but not altered, might only
be achievable under certain trust assumptions. If one wants to
make weaker trust assumptions, one would have to weaken
γql(ϕ) accordingly.
Quantitative goal. The goal γqn(k, ϕ) of the quantitative
veriﬁability deﬁnition is a reﬁnement of the goal γk from
Section IV (note that now, ϕ can specify trust assumption with
dynamic corruption). Similarly to Section VI, we use a distance
function on election results. Roughly, the goal γqn(k, ϕ) requires
that the distance between the produced result and the “ideal” one
(obtained when the actual choices of honest voters are counted
and one choice for every dishonest voter) is bounded by k,
where, for γqn(k, ϕ), we consider a speciﬁc distance function d.
In order to deﬁne d, we ﬁrst deﬁne a function fcount : Cl → NC
which, for a vector (c1, . . . , cl) ∈ Cl (representing a multiset of
voters’ choices), counts how many times each choice occurs
in this vector. For example, fcount(B,C,C) asigns 1 to B, 2 to
C, and 0 to all the remaining choices. Now, for two vectors of
choices (cid:3)c,(cid:3)c

(cid:16) the distance function d is deﬁned by
(cid:16))[c]|.
d((cid:3)c,(cid:3)c

| fcount((cid:3)c)[c]− fcount((cid:3)c

(cid:16)) = ∑
c∈C

For instance, d((B,C,C), (A,C,C,C)) = 3.

Now, the goal γqn(k, ϕ) is satisﬁed in r if either (a) the
trust assumption ϕ does not hold true in r, or if (b) ϕ holds
(cid:16)
true in r and there exist valid choices c
nd (representing
possible choices of dishonest voters) and ˜c1, . . . , ˜cn, such that:
to

(i) an election result

is equal

, . . . , c

(cid:16)
1

is published and it
), (˜c1, . . . , ˜cn)) ≤ k.
(cid:16)
nd

ρ(˜c1, . . . , ˜cn), and
(cid:16)
1

, c

, . . . , c

(ii) d((c1, . . . , cnh
Note that when an adversary drops one honest vote, this
increases the distance in Condition (ii) by one, but when he
replaces an honest voter’s choice by another one, this increases
the distance by two. This corresponds to the real effect of a
manipulation on the ﬁnal result (goal γk does not distinguish
between these two types of manipulations).

As already explained, since not all voters will check their
receipts, some manipulation will go undetected. And hence, for
this goal δ = 0 is typically not achievable. The security analysis

793793

carried out on a concrete protocol will have to determine the
optimal (i.e., minimal) δ, given the parameter k.

We ﬁnally note that both of the above goals could be
reﬁned by providing guarantees for those voters who have been
corrupted sufﬁciently late in the protocol. For this, one merely
has to change what it means for a voter to be honest: voters
corrupted late enough would still be considered honest for the
purpose of the above goal deﬁnitions. For such voters, one
would then also provide guarantees. However, such reﬁnements
are protocol dependent, whereas the above goals are applicable
to a wide range of protocols.
Acknowledgements. This work was partially supported by
Deutsche Forschungsgemeinschaft (DFG) under Grant KU
1434/6-3 within the priority programme 1496 “Reliably Secure
Software Systems – RS3”. The research leading to these
results has also received funding from the European Research
Council under the European Union’s Seventh Framework
Programme (FP7/2007-2013) / ERC grant agreement no 258865
(ProSecure).

REFERENCES

http://www.economist.com/node/8382578, December 7th 2006.

[1]
[2] http://www.computerworld.com/s/article/9118204/Princeton report

[3]

rips N.J. e voting machines as easily hackable , October 27th 2008.
http://ucsdnews.ucsd.edu/newsrel/science/08-09ElectronicVoting.asp, Au-
gust 10th 2009.

[4] https://freedom-to-tinker.com/blog/appel/nj-election-cover/, September

13th 2011.

[5] M. Abadi and C. Fournet. Mobile Values, New Names, and Secure
In Proceedings of the 28th ACM Symposium on
Communication.
Principles of Programming Languages (POPL 2001), pages 104–115.
ACM Press, 2001.

[6] Ben Adida. Helios: Web-based Open-Audit Voting. In Paul C. van
Oorschot, editor, Proceedings of the 17th USENIX Security Symposium,
pages 335–348. USENIX Association, 2008.

[7] Ben Adida, Olivier de Marneffe, Olivier Pereira, and Jean-Jaques
Quisquater. Electing a University President Using Open-Audit Voting:
Analysis of Real-World Use of Helios.
In USENIX/ACCURATE
Electronic Voting Technology (EVT 2009), 2009.

[8] Carsten Baum, Ivan Damg˚ard, and Claudio Orlandi. Publicly auditable
secure multi-party computation. Cryptology ePrint Archive, Report
2014/075, 2014. http://eprint.iacr.org/.

[10]

[9] Susan Bell, Josh Benaloh, Mike Byrne, Dana DeBeauvoir, Bryce Eakin,
Gail Fischer, Philip Kortum, Neal McBurnett, Julian Montoya, Michelle
Parker, Olivier Pereira, Philip Stark, Dan Wallach, , and Michael Winn.
STAR-Vote: A Secure, Transparent, Auditable, and Reliable Voting
System. USENIX Journal of Election Technology and Systems (JETS),
1:18–37, August 2013.
Jonathan Ben-Nun, Niko Fahri, Morgan Llewellyn, Ben Riva, Alon
Rosen, Amnon Ta-Shma, and Douglas Wikstr¨om. A New Implementation
of a Dual (Paper and Cryptographic) Voting System. In Kripp et al.
[36], pages 315–329.
Josh Benaloh. Simple veriﬁable elections.
In Dan S. Wallach and
Ronald L. Rivest, editors, 2006 USENIX/ACCURATE Electronic Voting
Technology Workshop, EVT’06, Vancouver, BC, Canada, August 1, 2006.
USENIX Association, 2006.
Josh Daniel Cohen Benaloh. Veriﬁable Secret-Ballot Elections. PhD
thesis, 1987.

[11]

[12]

[13] Craig Burton, Chris Culnane, James Heather, Thea Peacock, Peter Y. A.
Ryan, Steve Schneider, Vanessa Teague, Roland Wen, Zhe Xia, and
Sriramkrishnan Srinivasan. Using Prˆet `a Voter in Victoria State Elections.
In J. Alex Halderman and Olivier Pereira, editors, 2012 Electronic Voting
Technology Workshop / Workshop on Trustworthy Elections, EVT/WOTE
’12, Bellevue, WA, USA, August 6-7, 2012. USENIX Association, 2012.
J. A. Calandrino, A. J. Feldman, J. A. Halderman, D. Wagner, H. Yu,
and W. P. Zeller. Source Code Review of the Diebold Voting System,
2007. Report commissioned as part of the California Secretary of State’s

[14]

794794

Top-To-Bottom Review of California voting systems. http://www.eecs.
berkeley.edu/∼daw/papers/dieboldsrc-ttbr.pdf.

[15] D. Chaum, R. Carback, J. Clark, A. Essex, S. Popoveniuc, R. L. Rivest,
P. Y. A. Ryan, E. Shen, and A. T. Sherman. Scantegrity II: End-to-
End Veriﬁability for Optical Scan Election Systems using Invisible
Ink Conﬁrmation Codes.
In USENIX/ACCURATE Electronic Voting
Technology (EVT 2008). USENIX Association, 2008. See also http:
//www.scantegrity.org/elections.php.

[16] Benoˆıt Chevallier-Mames, Pierre-Alain Fouque, David Pointcheval,
Julien Stern, and Jacques Traor´e. On Some Incompatible Properties
of Voting Schemes. In David Chaum, Markus Jakobsson, Ronald L.
Rivest, Peter Y. A. Ryan, Josh Benaloh, Miroslaw Kutylowski, and
Ben Adida, editors, Towards Trustworthy Elections, New Directions in
Electronic Voting, volume 6000 of Lecture Notes in Computer Science,
pages 191–199. Springer, 2010.

[17] M. R. Clarkson, S. Chong, and A. C. Myers. Civitas: Toward a Secure
Voting System. In 2008 IEEE Symposium on Security and Privacy (S&P
2008), pages 354–368. IEEE Computer Society, 2008.

[18] V´eronique Cortier, Fabienne Eigner, Steve Kremer, Matteo Maffei, and
Cyrille Wiedling. Type-Based Veriﬁcation of Electronic Voting Protocols.
In Proceedings of the 4th Conference on Principles of Security and
Trust (POST’15), Lecture Notes in Computer Science, London, UK,
April 2015. Springer.

[19] V´eronique Cortier, David Galindo, St´ephane Glondu, and Malika
Izabachene. Election Veriﬁability for Helios under Weaker Trust
Assumptions.
In Proceedings of the 19th European Symposium on
Research in Computer Security (ESORICS’14), LNCS, Wroclaw, Poland,
September 2014. Springer.

[20] V´eronique Cortier, David Galindo, Ralf K¨usters, Johannes M¨uller,
and Tomasz Truderung. Veriﬁability Notions for E-Voting Protocols.
Technical Report 2016/287, Cryptology ePrint Archive, 2016. Available
at http://eprint.iacr.org/2016/287.

[21] R. Cramer, R. Gennaro, and B. Schoenmakers. A Secure and Optimally
Efﬁcient Multi-Authority Election Scheme. In Advances in Cryptology
— EUROCRYPT ’97, International Conference on the Theory and
Application of Cryptographic Techniques, volume 1233 of Lecture Notes
in Computer Science. Springer-Verlag, 1997.

[22] Chris Culnane, Peter Y. A. Ryan, Steve A. Schneider, and Vanessa
Teague. vVote: A Veriﬁable Voting System. ACM Trans. Inf. Syst.
Secur., 18(1):3, 2015.

[23] Chris Culnane and Steve A. Schneider. A Peered Bulletin Board for
Robust Use in Veriﬁable Voting Systems.
In IEEE 27th Computer
Security Foundations Symposium, CSF 2014, Vienna, Austria, 19-22
July, 2014, pages 169–183. IEEE, 2014.

[24] Edouard Cuvelier, Olivier Pereira, and Thomas Peters.

Election
Veriﬁability or Ballot Privacy: Do We Need to Choose? In Jason
Crampton, Sushil Jajodia, and Keith Mayes, editors, Computer Security
- ESORICS 2013 - 18th European Symposium on Research in Computer
Security, Egham, UK, September 9-13, 2013. Proceedings, volume 8134
of Lecture Notes in Computer Science, pages 481–498. Springer, 2013.
Jeremy Epstein. Weakness in Depth: A Voting Machine’s Demise. IEEE
Security & Privacy, 13(3):55–58, 2015.

[25]

[26] David Galindo, Sandra Guasch, and Jordi Puiggali. 2015 Neuchˆatel’s
Cast-as-Intended Veriﬁcation Mechanism.
In Rolf Haenni, Reto E.
Koenig, and Douglas Wikstr¨om, editors, E-Voting and Identity - 5th
International Conference, VoteID 2015, Bern, Switzerland, September
2-4, 2015, Proceedings, volume 9269 of Lecture Notes in Computer
Science, pages 3–18. Springer, 2015.

[27] Gurchetan S. Grewal, Mark Dermot Ryan, Liqun Chen, and Michael R.
Clarkson. Du-Vote: Remote Electronic Voting with Untrusted Computers.
In C´edric Fournet, Michael W. Hicks, and Luca Vigan`o, editors, IEEE
28th Computer Security Foundations Symposium, CSF 2015, Verona,
Italy, 13-17 July, 2015, pages 155–169. IEEE, 2015.

[28] Benjamin Hosp and Poorvi L. Vora. An information-theoretic model
of voting systems. In David Chaum, Miroslaw Kutylowski, Ronald L.
Rivest, and Peter Y. A. Ryan, editors, Frontiers of Electronic Voting,
29.07. - 03.08.2007, volume 07311 of Dagstuhl Seminar Proceedings.
Internationales Begegnungs- und Forschungszentrum fuer Informatik
(IBFI), Schloss Dagstuhl, Germany, 2007.

[29] David Jefferson, Aviel D. Rubin, Barbara Simons, and David Wagner.
Analyzing internet voting security. Communications of the ACM, Special

issue: The problems and potentials of voting systems, 47(10):59–64,
2004.

[30] Hugo Jonker, Sjouke Mauw, and Jun Pang. Privacy and veriﬁability in
voting systems: Methods, developments and trends. Computer Science
Review, 10:1–30, 2013.

[31] A. Juels, D. Catalano, and M. Jakobsson. Coercion-Resistant Electronic
In Proceedings of Workshop on Privacy in the Eletronic

Elections.
Society (WPES 2005), pages 61–70. ACM Press, 2005.

[32] Ari Juels, Dario Catalano, and Markus Jakobsson. Coercion-resistant
electronic elections. In David Chaum, Markus Jakobsson, Ronald L.
Rivest, Peter Y. A. Ryan, Josh Benaloh, Miroslaw Kutylowski, and
Ben Adida, editors, Towards Trustworthy Elections, New Directions in
Electronic Voting, volume 6000 of Lecture Notes in Computer Science,
pages 37–63. Springer, 2010.

[33] Aggelos Kiayias, Thomas Zacharias, and Bingsheng Zhang. DEMOS-2:
Scalable E2E Veriﬁable Elections without Random Oracles. In Indrajit
Ray, Ninghui Li, and Christopher Kruegel, editors, Proceedings of the
22nd ACM SIGSAC Conference on Computer and Communications
Security, Denver, CO, USA, October 12-6, 2015, pages 352–363. ACM,
2015.

[34] Aggelos Kiayias, Thomas Zacharias, and Bingsheng Zhang. End-to-End
Veriﬁable Elections in the Standard Model. In Advances in Cryptology -
EUROCRYPT 2015, volume 9057 of Lecture Notes in Computer Science,
pages 468–498. Springer, 2015.

[35] Steve Kremer, Mark Ryan, and Ben Smyth. Election Veriﬁability in
Electronic Voting Protocols. In Dimitris Gritzalis, Bart Preneel, and
Marianthi Theoharidou, editors, 15th European Symposium on Research
in Computer Security (ESORICS2010), volume 6345 of Lecture Notes
in Computer Science, pages 389–404. Springer, 2010.

[36] Manuel J. Kripp, Melanie Volkamer, and R¨udiger Grimm, editors. 5th
International Conference on Electronic Voting 2012, (EVOTE 2012),
Co-organized by the Council of Europe, Gesellschaft f¨ur Informatik and
E-Voting.CC, July 11-14, 2012, Castle Hofen, Bregenz, Austria, volume
205 of LNI. GI, 2012.

[37] Ralf K¨usters, Tomasz Truderung, and Andreas Vogt. Accountability:
Deﬁnition and Relationship to Veriﬁability. In Proceedings of the 17th
ACM Conference on Computer and Communications Security (CCS
2010), pages 526–535. ACM, 2010. Full version available at http:
//eprint.iacr.org/2010/236/.

[38] Ralf K¨usters, Tomasz Truderung, and Andreas Vogt. Veriﬁability, Privacy,
and Coercion-Resistance: New Insights from a Case Study. In 32nd
IEEE Symposium on Security and Privacy (S&P 2011), pages 538–553.
IEEE Computer Society, 2011.

[39] Ralf K¨usters, Tomasz Truderung, and Andreas Vogt. Clash Attacks
on the Veriﬁability of E-Voting Systems.
In 33rd IEEE Symposium
on Security and Privacy (S&P 2012), pages 395–409. IEEE Computer
Society, 2012.

[40] Ralf K¨usters, Tomasz Truderung, and Andreas Vogt. Formal Analysis of
Chaumian Mix Nets with Randomized Partial Checking. In 35th IEEE
Symposium on Security and Privacy (S&P 2014), pages 343–358. IEEE
Computer Society, 2014.

[41] Lucie Langer. Privacy and veriﬁability in electronic voting. PhD thesis,

Darmstadt University of Technology, 2010.

[42] Thea Peacock, Peter YA Ryan, Steve Schneider, and Zhe Xia. Veriﬁable
voting systems. Computer and Information Security Handbook, pages
1103–1125, 2013.

[43] Stefan Popoveniuc, John Kelsey, Andrew Regenscheid, and Poorvi L.
Vora. Performance Requirements for End-to-End Veriﬁable Elections. In
Douglas W. Jones, Jean-Jacques Quisquater, and Eric Rescorla, editors,
2010 Electronic Voting Technology Workshop / Workshop on Trustworthy
Elections, EVT/WOTE ’10, Washington, D.C., USA, August 9-10, 2010.
USENIX Association, 2010.
Jordi Puigalli and Sandra Guasch. Cast-as-Intended Veriﬁcation in
Norway. In Kripp et al. [36], pages 49–63.

[44]

[45] R. L. Rivest and W. D. Smith. Three Voting Protocols: ThreeBallot,
VAV and Twin. In USENIX/ACCURATE Electronic Voting Technology
(EVT 2007), 2007.

[46] Kazue Sako and Joe Kilian. Receipt-free mix-type voting scheme
- A practical solution to the implementation of a voting booth.
In
Louis C. Guillou and Jean-Jacques Quisquater, editors, Advances in
Cryptology - EUROCRYPT ’95, International Conference on the Theory

and Application of Cryptographic Techniques, Proceeding, volume 921
of Lecture Notes in Computer Science, pages 393–403. Springer, 1995.
[47] Ben Smyth, Steven Frink, and Michael R. Clarkson. Computational
Election Veriﬁability: Deﬁnitions and an Analysis of Helios and JCJ.
Technical Report 2015/233, Cryptology ePrint Archive, 2015. http:
//eprint.iacr.org/2015/233.

[48] Drew Springall, Travis Finkenauer, Zakir Durumeric, Jason Kitcat,
Harri Hursti, Margaret MacAlpine, and J. Alex Halderman. Security
Analysis of the Estonian Internet Voting System. In Gail-Joon Ahn,
Moti Yung, and Ninghui Li, editors, Proceedings of the 2014 ACM
SIGSAC Conference on Computer and Communications Security, pages
703–715. ACM, 2014.

[49] Alan Szepieniec and Bart Preneel. New Techniques for Electronic Voting.
USENIX Journal of Election Technology and Systems (JETS), 3(2):46 –
69, 2015. Cryptology ePrint Archive, Report 2015/809.

[50] Georgios Tsoukalas, Kostas Papadimitriou, Panos Louridas, and Panayi-
otis Tsanakas. From Helios to Zeus.
In 2013 Electronic Voting
Technology Workshop / Workshop on Trustworthy Elections, EVT/WOTE
’13, Washington, D.C., USA, August 12-13, 2013. USENIX Association,
2013.

[51] Douglas Wikstr¨om. A Sender Veriﬁable Mix-Net and a New Proof of a
Shufﬂe. In Bimal K. Roy, editor, Advances in Cryptology - ASIACRYPT
2005, 11th International Conference on the Theory and Application
of Cryptology and Information Security, Proceedings, volume 3788 of
Lecture Notes in Computer Science, pages 273–292. Springer, 2005.

[52] Scott Wolchok, Eric Wustrow, J. Alex Halderman, Hari K. Prasad,
Arun Kankipati, Sai Krishna Sakhamuri, Vasavya Yagati, and Rop
Gonggrijp. Security analysis of India’s electronic voting machines.
In ACM Conference on Computer and Communications Security (CCS
2010), pages 1–14, 2010.

[53] Scott Wolchok, Eric Wustrow, Dawn Isabel, and J. Alex Halderman.
Attacking the Washington, D.C. Internet Voting System. In 16th Intl.
Conference on Financial Cryptography and Data Security (FC’12),
2012.

SYMBOLIC VERIFIABILITY BY KREMER ET AL.

APPENDIX A

In this section, we focus on the veriﬁability deﬁnition by
Kremer et al. [35] who divide veriﬁability into three sub-
properties.
• Individual veriﬁability: a voter should be able to check
• Universal veriﬁability: anyone should be able to check
that the result corresponds to the content of the ballot box.
• Eligibility veriﬁability: only eligible voter may vote.

that his vote belongs to the ballot box.

Since the proposed formal deﬁnition for eligibility veriﬁability
is rather long and technical, we focus here on individual and
universal veriﬁability.

A. Model

In symbolic models, messages are represented by terms.
Kremer et al. model protocols as processes in the applied-pi
calculus [5]. A voting speciﬁcation is a pair (V,A) where V
is a process that represents the program of a voter while A is
an evaluation context that represents the (honest) authorities
and the infrastructure. All voters are (implicitly) assumed to
be honest.

B. Individual and Universal Veriﬁability

We can express the deﬁnitions of Kremer et al. indepen-
dently of the execution model, which slightly extends their
deﬁnitions.

The symbolic veriﬁability deﬁnition by Kremer et al. [35]
assumes that each voter Vi performs an individual test ϕIV
, and
i
that observers perform a universal test ϕUV. The individual test
ϕIV
takes as input the voter’s vote and all his local knowledge
i

795795

(e.g. randomness, credentials, and public election data) as well
as a partial view of the ballot box (which should correspond to
his ballot). The universal test ϕUV takes as input the outcome
of the election, the public election data, the ballot box, and
possibly some extra data generated during the protocol used
for the purposes of veriﬁcation. These tests should satisfy the
following conditions for any execution.
Deﬁnition 11 (Individual and Universal Veriﬁability). A voting
speciﬁcation (V,A) satisﬁes individual and universal veriﬁability
if for all n ∈ N,

i (b)∧ ϕIV
∀i, j : ϕIV
ϕUV (B,r)∧ ϕUV (B,r
(cid:7)

j (b) ⇒ i = j
(cid:16)) ⇒ r ≈ r
(cid:16)
i (bi)∧ ϕUV (B,r) ⇒ c ≈ r
ϕIV

1≤i≤n

(1)
(2)
(3)

evaluates to ”false”, the judge outputs ”reject”. Otherwise,
the judge iteratively triggers each voter Vi in order to verify
her ballot. If every voter outputs ”accept”, the judge outputs
”accept”, and otherwise ”false”. (This models the requirement
in the deﬁnition of Kremer et al. that all voters have to verify
successfully in order for the run to be accepted. It also means
that if not all voters verify, no guarantees are given.)

End-to-end honest veriﬁability. Let the goal γIUV be the
sub-goal of γ0 in which all voters produce pairwise different
ballots. Then, individual and universal veriﬁability by Kremer
et al. (Deﬁnition (11)) can essentially be characterized by the
fact that the protocol PKRS is (γIUV ,0)-veriﬁable by the judge
J.

(cid:6)

1≤i≤n ϕIV
i

To see this, ﬁrst observe that the judge J as deﬁned above
(bi)∧
outputs ”accept” if and only if the Condition
ϕUV (B,r) in Condition (3) evaluates to true. As we already
pointed out, the implication c ≈ r in Condition (3) describes
the goal γ0. Condition (1) stating that there are no clashes
between the ballots of honest voters is also satisﬁed in γIUV by
deﬁnition. Thus, for a protocol which achieves individual and
universal veriﬁability according to Deﬁnition 11, the probability
that the judge J in PKRS accepts a protocol run in which γIUV
is not fulﬁlled, is negligible (δ = 0), i.e., we have Pr[π((cid:5)) (cid:7)→
¬γIUV , (J: accept)] ≤ δ = 0 with overwhelming probability as
in Deﬁnition 1.

SYMBOLIC VERIFIABILITY BY CORTIER ET AL.

APPENDIX B

In this section, we study the symbolic veriﬁability deﬁnition
by Cortier et al. [18]. Cortier et al. also deﬁne different notions
of veriﬁability: individual, universal, and end-to-end veriﬁability.
They prove that under the assumption of an additional property,
called ”no clash”, individual and universal veriﬁability imply
end-to-end veriﬁability in their symbolic model.

A. Model

As in [35], the deﬁnitions of Cortier et al. are cast in a
symbolic model. That is, messages are represented by terms
and protocols are deﬁned as symbolic processes. Additionally,
Cortier et al. assume that voters reach several successive states
denoted as follows:
• Vote(i, c, cred):
• MyBallot(i, c, b):
the voter i has prepared a ballot b
• VHappy(i, c, cred, B): the voter i with credential cred has
cast a choice c and is happy with the content of the ballot
box B.

credential cred and is willing to cast a choice c.

the voter with identity i owns some

corresponding to the choice c.

Cortier et al. [18] also assume a judge that checks whether
a result r corresponds to a ballot box B and reaches a state
JHappy(B,r) whenever this is the case.

After the casting and before the tallying, some ballots
may be removed because they are invalid (e.g., due to ﬂawed
signatures or zero-knowledge proofs) or simply because some
voters have voted several times and only the last vote counts.
This yields a “sanitized” list of ballots Bsan.
B. Individual Veriﬁability

Intuitively, individual veriﬁability by Cortier et al. holds
true if whenever honest voters perform the checks prescribed
by the protocol, then their ballots belong to the ballot box.

796796

where c = (c1, . . . , cn) are the choices of the voters, b is an
arbitrary ballot, B is the (content of the) bulletin board, r
and r(cid:16) are possible outcomes, and ≈ denotes equality up to
permutation.

Intuitively, Condition (1) ensures that two distinct voters
may not agree on the same ballot,
i.e., no clash occurs.
Condition (2) guarantees the unicity of the outcome: if the
observers successfully check the execution, there is at most
one outcome they may accept (up to permutation). Finally,
Condition (3) is the key property: if all tests succeed, the
outcome should correspond to the voters’ intent. Observe that,
since all voters are assumed to be honest, the implication c ≈ r
in Condition (3) can be described by the goal γ0 (see below).

C. Discussion

Deﬁnition (11) is tailored to a speciﬁc tally: the outcome of
the election has to be the sequence of the votes. Moreover, the
deﬁnition assumes that the ballot of the voter can be retrieved
from the ballot box, which does not apply to ThreeBallot for
example. The main restriction is that all voters are assumed to
be honest.

Observe that by Condition (3) the goal γ0 is guaranteed
only for protocol runs in which all voters successfully verify
their ballots (and the universal test is positive). For the other
runs, the outcome can be arbitrary. However, the assumption
that all honest voters verify their ballot is unrealistically strong.
Therefore, even though this deﬁnition uses the strong goal γ0,
this assumption makes the deﬁnition weak.

D. Casting in the KTV Framework
Protocol PKRS. The set of agents Σ consists of the voters, the
bulletin board B, the judge J, and the remaining participants.
Only static corruption is considered. The voters, the bulletin
board and the judge do not accept to be corrupted. The honest
programs are deﬁned as follows:
– When a voter Vi runs her honest program πVi and is triggered
in order to cast a ballot, she runs the usual program. When
Vi is triggered in order to verify her vote, she performs the
individual test ϕIV
(b) with her ballot b, and if this evaluates
i
to ”true”, she outputs ”accept”, otherwise ”reject”.

– When the judge J runs its honest program πJ, it reads the
content from the bulletin board B including the result r (if
it does not receive any content, it outputs ”reject”). Then
the judge performs the universal test ϕUV (B,r), and if this

Deﬁnition 12 (Individual Veriﬁability). A protocol guarantees
individual veriﬁability if for every execution, and for every
voter Vi, choice c, credentials cred and ballot box B, whenever
the state VHappy(i, c, cred, B) is reached, it follows that

Vote(i, c, cred)∧∃b ∈ B: MyBallot(i, c, b).

C. Universal Veriﬁability

The universal veriﬁability deﬁnition by Cortier et al. de-
pends on certain predicates whose purpose is to formally deﬁne
what it means that a ballot ”contains” a vote and that the tallying
proceeds correctly.
Wrap. To deﬁne that a vote is “contained” in a ballot, Cortier
et al. introduce a predicate Wrap(c, b) that is left undeﬁned,
but has to satisfy the following properties:

(i) Any well-formed ballot b corresponding to some choice

c satisﬁes the Wrap predicate:

MyBallot(i, c, b) ⇒ Wrap(c, b)

(ii) A ballot b cannot wrap two distinct choices c1 and c2:

Wrap(c1, b)∧ Wrap(c2, b) ⇒ c1 = c2

For a given protocol, the deﬁnition of Wrap typically follows
from the protocol speciﬁcation.
Good sanitization. When the ballot box B is sanitized,
it is acceptable to remove some ballots but of course true
honest ballots should not be removed. Therefore, Cortier
et al. deﬁne the predicate GoodSan(B, Bsan) to hold true
(implicitly relatively to a run) if the honest ballots of B are
not removed from Bsan. This means that (i) Bsan ⊆ B, and (ii)
for any b ∈ B such that MyBallot(i, c, b) holds true for some
voter Vi and some choice c, it is guaranteed that b ∈ Bsan.
Good counting.
a predicate
GoodCount(Bsan,r) in order to describe that the ﬁnal result r
corresponds to counting the votes of Bsan. This is technically
deﬁned in [18] by introducing an auxiliary bulletin board
(cid:16)
san which is a permutation of Bsan and from which the list
B
rlist of votes (such that r = ρ(rlist) where ρ is the counting
(cid:16)
san. More formally,
function) can be extracted line by line from B
(cid:16)
,rlist such that
GoodCount(Bsan,r) holds true if there exist B
(cid:16)
san
(i) Bsan and rlist have the same size, and (ii) Bsan and B
san are
equal as multisets, and (iii) r = ρ(rlist), and (iv) for all ballots
[ j] = b for some index j, there exists a choice c
b with B
such that Wrap(c,b) as well as rlist[ j] = c hold true. Note that
the deﬁnition of GoodCount is parameterized by the counting
function ρ of the protocol under consideration.

al. deﬁne

(cid:16)
san

Cortier

et

Then, universal veriﬁability is deﬁned as follows.

Deﬁnition 13 (Universal Veriﬁability). A protocol guarantees
universal veriﬁability if for every execution, and every ballot
box B and result r, whenever the state JHappy(B,r) is reached,
it holds that

∃Bsan : GoodSan(B, Bsan)∧ GoodCount(Bsan,r).

Intuitively, whenever the judge (some election authority)
states that some result r corresponds to a ballot box B, then r
corresponds to the votes contained in a subset Bsan of B (some
ballots may have been discarded because they were ill-formed
for example) and this subset Bsan contains at least all ballots
formed by honest voters that played the entire protocol (that
is, including the ﬁnal checks).

797797

D. E2E Veriﬁability

Intuitively, end-2-end veriﬁability according to Cortier et
al. holds if, whenever no one complains (including the judge),
then the election result includes all the votes corresponding
to honest voters that performed the checks prescribed by the
protocol.
Deﬁnition 14 (E2E Veriﬁability). A protocol guarantees end-
2-end veriﬁability if for every execution, and every ballot box
B and result r, whenever a state is reached such that for some
subset of the honest voters (indexed by some set I) with choices
ci and credentials credi (i ∈ I) we have

JHappy(B,r)∧ (cid:7)
i∈I

VHappy(i, ci, credi, B),

then there exist rlist such that we have r = ρ(rlist) and {ci}i∈I ⊆
rlist (as multisets).

E. No Clash

Finally, Cortier et al. deﬁne the notion of “no clash” as
follows. Intuitively, ”no clash” describes the property that two
distinct honest voters may not build the same ballot.
Deﬁnition 15 (No Clash). A protocol guarantees no clash if
for every execution, whenever a state is reached such that
MyBallot(i, ci, b)∧ MyBallot( j, c j, b), then it must be the case
that i = j and ci = c j.

F. Discussion

Cortier et al. [18] showed that

individual veriﬁability,
universal veriﬁability, and the ”no clash” property together
imply End-to-End veriﬁability (all as deﬁned above).

In order to be able to deﬁne their notions of individual
and universal veriﬁability, Cortier et al. proposed a model
in which it is possible to (i) extract single ballots from the
bulletin board (implicit in the predicate VHappy), and to (ii)
uniquely determine the content, i.e. the plain vote, of each
single ballot (Wrap predicate). Therefore, these deﬁnitions can
only be applied to a class of protocols which fulﬁll these
requirements, and by this, for example, ThreeBallot [45] as
well as protocols in which ballots are information theoretically
secure commitments (e.g. [24]) can not be analyzed.

The notion of end-2-end veriﬁability (Deﬁnition 14) is rather
weak since it only requires that honest votes are counted (for
voters that checked). It does not control dishonest votes. In
particular, this notion does not prevent ballot stufﬁng. The
authors of [18] introduced this notion because the Helios
protocol does not satisfy strong veriﬁability as deﬁned in [19]
for example. Moreover, the veriﬁcation technique based on
typing developed in [18] would probably require some adaption
to cover strong veriﬁability as it would need to count the number
of votes, which is a difﬁcult task for type-checkers.

G. Casting in the KTV Framework
Protocol PCEKMW . The set of agents Σ consists of the honest
voters, the bulletin board B, the judge J, and the remaining
participants. Only static corruption is considered. The bulletin
board and the judge do not accept to be corrupted. The honest
programs are deﬁned as follows:
– When a voter V runs her honest program πV, and is triggered
to cast her ballot, she expects an identity i and a choice c (if
not, she stops). Then, she runs Vote(c) to build her ballot

, Tally, P) evaluates to ”true” or

respectively, if Verify(B, prmpub
”false”.
Individual veriﬁability. We deﬁne the goal γIV to be the set of
all runs of PSFC in which all honest voters’ ballots are pairwise
different (if (cid:11)= ⊥), i.e., no clashes occur. For the protocol PSFC,
individual veriﬁability according to Smyth et al. can essentially
be characterized by the fact that the protocol PSFC is (γIV ,0)-
veriﬁable by the judge J in the sense of Deﬁnition 1.

To see this, observe that, if a protocol achieves individual
veriﬁability according to Deﬁnition 7, then for all ppt ad-
versaries πA the probability Pr[π(1(cid:5)) (cid:7)→ ¬γIV , (J: accept)] ≤
Pr[π(1(cid:5)) (cid:7)→ ¬γIV ] is negligible for π = πP (cid:5) πA, where the latter
probability is negligible, if the protocol satisﬁes Deﬁnition 7.
For the implication in the opposite direction, let us as-
sume that Pr[π(1(cid:5)) (cid:7)→ ¬γIV , (J: accept)] is negligible for
all adversaries. Now, for each adversary A from the game
7, there is a corresponding adversary
used in Deﬁnition
πA which always produces correct
tally (note that A is
not concerned with tallying). For this adversary we have
Pr[π(1(cid:5)) (cid:7)→ ¬γIV , (J: accept)] = Pr[π(1(cid:5)) (cid:7)→ ¬γIV ] which, by
the above assumption, is negligible. This implies individual
veriﬁability (in the sense of Deﬁnition 7).
Universal veriﬁability. We deﬁne the goal γUV to be the set
of all runs of PSFC in which ﬁrst prmpub and then a ﬁnal result
(Tally, P) are published and Tally = correct tally(B, prmpub
)
(recall that B is the content of the bulletin board that contains
voters’ ballots).

For the protocol PSFC, universal veriﬁability according to
Smyth et al. can essentially be characterized by the fact that the
protocol PSFC is (γUV ,0)-veriﬁable in the sense of Deﬁnition 1.
To see this, ﬁrst observe that, for each adversary A, the con-
(cid:16)) in Experiment ExpUV(Π, A)
dition Verify(B, prmpub
(Fig. 6) is true if an honest judge J outputs “accept” (in
the system π with the corresponding adversary), and false
otherwise. Second, the adversary A in Experiment ExpUV(Π, A)
(cid:16) (cid:11)=
produces a tuple (B, prmpub
) holds true if and only if we have ¬γUV
correct tally(B, prmpub
(in the corresponding run of π).

(cid:16)) for which Tally

, Tally

, Tally

(cid:16), P

(cid:16), P

Thus, essentially, for a voting protocol P achieving universal
veriﬁability according to Deﬁnition 9 (which means that
the success rate in Experiment ExpUV(Π, A) (Fig. 6) is
negligible for every ppt adversary A) is equivalent to the
statement that the goal γUV is 0-veriﬁable by the judge J
according to Deﬁnition 1 (which means that the probability
Pr[π(1(cid:5)) (cid:7)→ ¬γUV , (J: accept)] is negligible in every instance
πP (cid:5) πA).
Election veriﬁability. According to Smyth et al. the protocol
PSFC achieves election veriﬁability if it achieves individual and
universal veriﬁability. Therefore this notion can be expressed
in the language of Deﬁnition 1 using the goal γIV ∧ γUV .

b and to submit it to the bulletin board. Afterwards, she
reaches a state MyBallot(i, c, b). When the voter is triggered
to verify her vote, she reads the content of the bulletin board
B and reaches a state VHappy(i, c, B) if her checks evaluate
to true.

– When the judge J runs its honest program πJ and is triggered
to verify the election run, it reads the content of the bulletin
board B including the ﬁnal result r (if not possible, J outputs
”reject”). If the judge successfully performs some checks
(which depend on the concrete voting protocol), then he
outputs ”accept” and reaches a state JHappy(B,r).

Individual veriﬁability. We deﬁne the goal γIV to be the
set of all runs of PCEKMW in which whenever an honest voter
Vi reaches the state VHappy(i, c, B) for some choice c and
ballot b, then there exists a ballot b ∈ B such that this voter
started with (i, c) as her input and reached MyBallot(i, c, b) as
intermediary state. Then, individual veriﬁability by Cortier et
al. (Deﬁnition 12) can essentially be characterized by the fact
that the protocol PCEKMW is (γIV ,0)-veriﬁable by the judge J.
Universal veriﬁability. We deﬁne the goal γUV to be the set
of all runs of PCEKMW in which whenever a result r is obtained
and the ﬁnal content of the ballot box is B then there exists
Bsan such that GoodSan(B, Bsan) and GoodCount(Bsan,r) hold
true (as deﬁned above). Then, universal veriﬁability by Cortier
et al. (Deﬁnition 13) can essentially be characterized by the fact
that the protocol PCEKMW is (γUV ,0)-veriﬁable by the judge J.
End-to-end veriﬁability. We deﬁne the goal γE2E to be the
set of all runs of PCEKMW in which the result r of the election
satisﬁes r = ρ(rlist) for some rlist that contains (as multiset)
all the choices ci for which some honest voter Vi reached a
state VHappy(i, ci, crediB). Then, end-to-end veriﬁability by
Cortier et al. (Deﬁnition 14) can essentially be characterized
by the fact that the protocol PCEKMW is (γE2E ,0)-veriﬁable by
the judge J.

DEFINITION OF SMYTH ET AL.: CASTING IN THE KTV

APPENDIX C

FRAMEWORK

We cast the deﬁnitions of individual veriﬁability, universal
veriﬁability and election veriﬁability by Smyth et al. [47] in
the framework of Deﬁnition 1.
Protocol PSFC. The set of agents Σ consists of the voters, the
bulletin board B, the judge J, the scheduler, and the remaining
participants. Since static corruption is considered, the agents
only accept the corrupt message at the beginning of an election
run. The bulletin board and the judge do not accept to be
corrupted.

When a voter V runs her honest program πV, she expects
a candidate c as input (if the input is empty, she stops). After
that, she reads the public election parameters prmpub from the
bulletin board B (if she does not receive any election paramaters
on B, she stops). Then, she runs Vote(c, prmpub
) and sends the
resulting ballot b to the bulletin board B. Although this is kept
implicit in the discussed paper, we will assume here that V
subsequently checks that its ballot is published on B.

When the judge J runs its honest program πJ, it reads
the content from the bulletin board B, including the public
paramaters prmpub, the tally Tally, and the proof P (if the
judge does not receive one of these inputs, it outputs ”reject”).
Then, the judge runs Verify and outputs ”accept” or ”reject”,

798798

