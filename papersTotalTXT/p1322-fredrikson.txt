Model Inversion Attacks that Exploit Conﬁdence Information

and Basic Countermeasures

Matt Fredrikson

Carnegie Mellon University

Somesh Jha

University of Wisconsin–Madison

Thomas Ristenpart

Cornell Tech

ABSTRACT
Machine-learning (ML) algorithms are increasingly utilized
in privacy-sensitive applications such as predicting lifestyle
choices, making medical diagnoses, and facial recognition. In
a model inversion attack, recently introduced in a case study
of linear classiﬁers in personalized medicine by Fredrikson
et al. [13], adversarial access to an ML model is abused
to learn sensitive genomic information about individuals.
Whether model inversion attacks apply to settings outside
theirs, however, is unknown.

We develop a new class of model inversion attack that
exploits conﬁdence values revealed along with predictions.
Our new attacks are applicable in a variety of settings, and
we explore two in depth: decision trees for lifestyle surveys
as used on machine-learning-as-a-service systems and neural
networks for facial recognition. In both cases conﬁdence val-
ues are revealed to those with the ability to make prediction
queries to models. We experimentally show attacks that are
able to estimate whether a respondent in a lifestyle survey
admitted to cheating on their signiﬁcant other and, in the
other context, show how to recover recognizable images of
people’s faces given only their name and access to the ML
model. We also initiate experimental exploration of natural
countermeasures, investigating a privacy-aware decision tree
training algorithm that is a simple variant of CART learn-
ing, as well as revealing only rounded conﬁdence values. The
lesson that emerges is that one can avoid these kinds of MI
attacks with negligible degradation to utility.

1.

INTRODUCTION

Computing systems increasingly incorporate machine learn-
ing (ML) algorithms in order to provide predictions of lifestyle
choices [6], medical diagnoses [20], facial recognition [1],
and more. The need for easy “push-button” ML has even
prompted a number of companies to build ML-as-a-service
cloud systems, wherein customers can upload data sets, train
classiﬁers or regression models, and then obtain access to
perform prediction queries using the trained model — all

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813677 .

over easy-to-use public HTTP interfaces. The features used
by these models, and queried via APIs to make predictions,
often represent sensitive information. In facial recognition,
the features are the individual pixels of a picture of a per-
son’s face. In lifestyle surveys, features may contain sensitive
information, such as the sexual habits of respondents.

In the context of these services, a clear threat is that
providers might be poor stewards of sensitive data, allow-
ing training data or query logs to fall prey to insider at-
tacks or exposure via system compromises. A number of
works have focused on attacks that result from access to
(even anonymized) data [18,29,32,38]. A perhaps more sub-
tle concern is that the ability to make prediction queries
might enable adversarial clients to back out sensitive data.
Recent work by Fredrikson et al. [13] in the context of ge-
nomic privacy shows a model inversion attack that is able
to use black-box access to prediction models in order to es-
timate aspects of someone’s genotype. Their attack works
for any setting in which the sensitive feature being inferred
is drawn from a small set. They only evaluated it in a single
setting, and it remains unclear if inversion attacks pose a
broader risk.

In this paper we investigate commercial ML-as-a-service
APIs. We start by showing that the Fredrikson et al. at-
tack, even when it is computationally tractable to mount, is
not particularly eﬀective in our new settings. We therefore
introduce new attacks that infer sensitive features used as
inputs to decision tree models, as well as attacks that re-
cover images from API access to facial recognition services.
The key enabling insight across both situations is that we
can build attack algorithms that exploit conﬁdence values
exposed by the APIs. One example from our facial recogni-
tion attacks is depicted in Figure 1: an attacker can produce
a recognizable image of a person, given only API access to a
facial recognition system and the name of the person whose
face is recognized by it.

ML APIs and model inversion. We provide an overview
of contemporary ML services in Section 2, but for the pur-
poses of discussion here we roughly classify client-side access
as being either black-box or white-box. In a black-box setting,
an adversarial client can make prediction queries against a
model, but not actually download the model description.
In a white-box setting, clients are allowed to download a
description of the model. The new generation of ML-as-
a-service systems—including general-purpose ones such as
BigML [4] and Microsoft Azure Learning [31]—allow data
owners to specify whether APIs should allow white-box or
black-box access to their models.

1322Our new estimator, as well as the Fredrikson et al. one,
query or run predictions a number of times that is linear
in the number of possible values of the target sensitive fea-
ture(s). Thus they do not extend to settings where features
have exponentially large domains, or when we want to invert
a large number of features from small domains.

Extracting faces from neural networks. An example
of a tricky setting with large-dimension, large-domain data
is facial recognition:
features are vectors of ﬂoating-point
pixel data.
In theory, a solution to this large-domain in-
version problem might enable, for example, an attacker to
use a facial recognition API to recover an image of a person
given just their name (the class label). Of course this would
seem impossible in the black-box setting if the API returns
answers to queries that are just a class label. Inspecting fa-
cial recognition APIs, it turns out that it is common to give
ﬂoating-point conﬁdence measures along with the class label
(person’s name). This enables us to craft attacks that cast
the inversion task as an optimization problem: ﬁnd the input
that maximizes the returned conﬁdence, subject to the clas-
siﬁcation also matching the target. We give an algorithm for
solving this problem that uses gradient descent along with
modiﬁcations speciﬁc to this domain. It is eﬃcient, despite
the exponentially large search space: reconstruction com-
pletes in as few as 1.4 seconds in many cases, and in 10–20
minutes for more complex models in the white-box setting.

We apply this attack to a number of typical neural network-
style facial recognition algorithms, including a softmax clas-
siﬁer, a multilayer perceptron, and a stacked denoising auto-
encoder. As can be seen in Figure 1, the recovered image
is not perfect. To quantify eﬃcacy, we perform experiments
using Amazon’s Mechanical Turk to see if humans can use
the recovered image to correctly pick the target person out of
a line up. Skilled humans (deﬁned in Section 5) can correctly
do so for the softmax classiﬁer with close to 95% accuracy
(average performance across all workers is above 80%). The
results are worse for the other two algorithms, but still beat
random guessing by a large amount. We also investigate re-
lated attacks in the facial recognition setting, such as using
model inversion to help identify a person given a blurred-out
picture of their face.

Countermeasures. We provide a preliminary exploration
of countermeasures. We show empirically that simple mech-
anisms including taking sensitive features into account while
using training decision trees and rounding reported conﬁ-
dence values can drastically reduce the eﬀectiveness of our
attacks. We have not yet evaluated whether MI attacks
might be adapted to these countermeasures, and this sug-
gests the need for future research on MI-resistant ML.

Summary. We explore privacy issues in ML APIs, showing
that conﬁdence information can be exploited by adversar-
ial clients in order to mount model inversion attacks. We
provide new model inversion algorithms that can be used
to infer sensitive features from decision trees hosted on ML
services, or to extract images of training subjects from facial
recognition models. We evaluate these attacks on real data,
and show that models trained over datasets involving survey
respondents pose signiﬁcant risks to feature conﬁdentiality,
and that recognizable images of people’s faces can be ex-
tracted from facial recognition models. We evaluate prelim-
inary countermeasures that mitigate the attacks we develop,
and might help prevent future attacks.

Figure 1: An image recovered using a new model in-
version attack (left) and a training set image of the
victim (right). The attacker is given only the per-
son’s name and access to a facial recognition system
that returns a class conﬁdence score.

Consider a model deﬁning a function f that takes input a
feature vector x1, . . . , xd for some feature dimension d and
outputs a prediction y = f (x1, . . . , xd).
In the model in-
version attack of Fredrikson et al. [13], an adversarial client
uses black-box access to f to infer a sensitive feature, say
x1, given some knowledge about the other features and the
dependent value y, error statistics regarding the model, and
marginal priors for individual variables. Their algorithm is
a maximum a posteriori (MAP) estimator that picks the
value for x1 which maximizes the probability of having ob-
served the known values (under some seemingly reasonable
independence assumptions). To do so, however, requires
computing f (x1, . . . , xd) for every possible value of x1 (and
any other unknown features). This limits its applicability
to settings where x1 takes on only a limited set of possible
values.

Our ﬁrst contribution is evaluating their MAP estima-
tor in a new context. We perform a case study showing
that it provides only limited eﬀectiveness in estimating sen-
sitive features (marital inﬁdelity and pornographic viewing
habits) in decision-tree models currently hosted on BigML’s
model gallery [4]. In particular the false positive rate is too
high: our experiments show that the Fredrikson et al. algo-
rithm would incorrectly conclude, for example, that a per-
son (known to be in the training set) watched pornographic
videos in the past year almost 60% of the time. This might
suggest that inversion is not a signiﬁcant risk, but in fact we
show new attacks that can signiﬁcantly improve inversion
eﬃcacy.

White-box decision tree attacks. Investigating the ac-
tual data available via the BigML service APIs, one sees that
model descriptions include more information than leveraged
in the black-box attack.
In particular, they provide the
count of instances from the training set that match each
path in the decision tree. Dividing by the total number of
instances gives a conﬁdence in the classiﬁcation. While a
priori this additional information may seem innocuous, we
show that it can in fact be exploited.

We give a new MAP estimator that uses the conﬁdence
information in the white-box setting to infer sensitive in-
formation with no false positives when tested against two
diﬀerent BigML decision tree models. This high precision
holds for target subjects who are known to be in the training
data, while the estimator’s precision is signiﬁcantly worse
for those not in the training data set. This demonstrates
that publishing these models poses a privacy risk for those
contributing to the training data.

13232. BACKGROUND

Our focus is on systems that incorporate machine learning
models and the potential conﬁdentiality threats that arise
when adversaries can obtain access to these models.

ML basics. For our purposes, an ML model is simply a
deterministic function f : Rd (cid:55)→ Y from d features to a set
of responses Y . When Y is a ﬁnite set, such as the names
of people in the case of facial recognition, we refer to f as a
classiﬁer and call the elements in Y the classes. If instead
Y = R, then f is a regression model or simply a regression.
Many classiﬁers, and particularly the facial recognition
ones we target in Section 5, ﬁrst compute one or more re-
gressions on the feature vector for each class, with the cor-
responding outputs representing estimates of the likelihood
that the feature vector should be associated with a class.
These outputs are often called conﬁdences, and the classi-
ﬁcation is obtained by choosing the class label for the re-
gression with highest conﬁdence. More formally, we deﬁne
f in these cases as the composition of two functions. The
ﬁrst is a function ˜f : Rd (cid:55)→ [0, 1]m where m is a parame-
ter specifying the number of conﬁdences. For our purposes
m = |Y | − 1, i.e., one less than the number of class labels.
The second function is a selection function t : [0, 1]m → Y
that, for example when m = 1, might output one label if
the input is above 0.5 and another label otherwise. When
m > 1, t may output the label whose associated conﬁdence
is greatest. Ultimately f (x) = t( ˜f (x)). It is common among
APIs for such models that classiﬁcation queries return both
f (x) as well as ˜f (x). This provides feedback on the model’s
conﬁdence in its classiﬁcation. Unless otherwise noted we
will assume both are returned from a query to f , with the
implied abuse of notation.

One generates a model f via some (randomized) training
algorithm train. It takes as input labeled training data db, a
sequence of (d+1)-dimensional vectors (x, y) ∈ Rd×Y where
x = x1, . . . , xd is the set of features and y is the label. We
refer to such a pair as an instance, and typically we assume
that instances are drawn independently from some prior dis-
tribution that is joint over the features and responses. The
output of train is the model1 f and some auxiliary informa-
tion aux. Examples of auxiliary information might include
error statistics and/or marginal priors for the training data.

ML APIs. Systems that incorporate models f will do so
via well-deﬁned application-programming interfaces (APIs).
The recent trend towards ML-as-a-service systems exem-
pliﬁes this model, where users upload their training data
db and subsequently use such an API to query a model
trained by the service. The API is typically provided over
HTTP(S). There are currently a number of such services,
including Microsoft Machine Learning [31], Google’s Predic-
tion API [16], BigML [4], Wise.io [40], that focus on analyt-
ics. Others [21,26,36] focus on ML for facial detection and
recognition.

Some of these services have marketplaces within which
users can make models or data sets available to other users.
A model can be white-box, meaning anyone can download
a description of f suitable to run it locally, or black-box,
meaning one cannot download the model but can only make

1We abuse notation and write f to denote both the function
and an eﬃcient representation of it.

prediction queries against it. Most services charge by the
number of prediction queries made [4,16,31,36].

Threat model. We focus on settings in which an adver-
sarial client seeks to abuse access to an ML model API.
The adversary is assumed to have whatever information the
In a white-box setting this means access to
API exposes.
download a model f .
In a black-box setting, the attacker
has only the ability to make prediction queries on feature
vectors of the adversary’s choosing. These queries can be
adaptive, however, meaning queried features can be a func-
tion of previously retrieved predictions. In both settings, the
adversary obtains the auxiliary information aux output by
training, which reﬂects the fact that in ML APIs this data
is currently most often made public.2

We will focus on contexts in which the adversary does
not have access to the training data db, nor the ability to
sample from the joint prior. While in some cases training
data is public, our focus will be on considering settings with
conﬁdentiality risks, where db may be medical data, lifestyle
surveys, or images for facial recognition. Thus, the adver-
sary’s only insight on this data is indirect, through the ML
API.

We do not consider malicious service providers, and we
do not consider adversarial clients that can compromise the
service, e.g., bypassing authentication somehow or otherwise
exploiting a bug in the server’s software. Such threats are
important for the security of ML services, but already known
as important issues. On the other hand, we believe that the
kinds of attacks we will show against ML APIs are more
subtle and unrecognized as threats.

3. THE FREDRIKSON ET AL. ATTACK

We start by recalling the generic model inversion attack
for target features with small domains from Fredrikson et
al. [13].

Fredrikson et al. considered a linear regression model f
that predicted a real-valued suggested initial dose of the
drug Warfarin using a feature vector consisting of patient de-
mographic information, medical history, and genetic mark-
ers.3 The sensitive attribute was considered to be the ge-
netic marker, which we assume for simplicity to be the ﬁrst
feature x1. They explored model inversion where an at-
tacker, given white-box access to f and auxiliary informa-
tion side(x, y) def= (x2, . . . , xt, y) for a patient instance (x, y),
attempts to infer the patient’s genetic marker x1.

Figure 2 gives their inversion algorithm. Here we assume
aux gives the empirically computed standard deviation σ
for a Gaussian error model err and marginal priors p =
(p1, . . . , pt). The marginal prior pi is computed by ﬁrst
partitioning the real line into disjoint buckets (ranges of val-
ues), and then letting pi(v) for each bucket v be the the
number of times xi falls in v over all x in db divided by the
number of training vectors |db|.

In words, the algorithm simply completes the target fea-
ture vector with each of the possible values for x1, and then
computes a weighted probability estimate that this is the
correct value. The Gaussian error model will penalize val-

2For example, BigML.io includes this data on web pages
describing black-box models in their model gallery [4].
3The inputs were processed in a standard way to make nom-
inal valued data real-valued. See [13] for details.

1324adversary Af (err, pi, x2, . . . , xt, y):
1: for each possible value v of x1 do
2:
3:
4: Return arg maxv rv

rv ← err(y, f (x(cid:48))) ·(cid:81)

x(cid:48) = (v, x2, . . . , xt)

i pi(xi)

x1

1

0

0

x2

φ1(x) = x1
w1 = 0
φ2(x) = (1 − x1)(x2)
w2 = 1
φ3(x) = (1 − x1)(1 − x2) w3 = 0

1

1

0

0

Figure 2: Generic inversion attack for nominal tar-
get features.

Figure 3: Decision tree for the formula y = ¬x1 ∧ x2.

ues of x1 that force the prediction to be far from the given
label y.

As argued in their work, this algorithm produces the least-
biased maximum a posteriori (MAP) estimate for x1 given
the available information. Thus, it minimizes the adver-
sary’s misprediction rate. They only analyzed its eﬃcacy in
the case of Warfarin dosing, showing that the MI algorithm
above achieves accuracy in predicting a genetic marker close
to that of a linear model trained directly from the original
data set.

It is easy to see that their algorithm is, in fact, black-
box, and in fact agnostic to the way f works. That means
it is potentially applicable in other settings, where f is not
a linear regression model but some other algorithm. An
obvious extension handles a larger set of unknown features,
simply by changing the main loop to iterate over all possible
combinations of values for the unknown features. Of course
this only makes sense for combinations of nominal-valued
features, and not truly real-valued features.

However, the algorithm proposed by Fredrikson et al. has
various limitations. Most obviously, it cannot be used when
the unknown features cover an intractably large set. One
example of such a setting is facial recognition, where the
feature space is huge: in the facial recognition examples con-
sidered later d ≈ 10, 304 with each feature a real number in
[0, 1]. Even if one only wanted to infer a portion of the fea-
tures this is computationally infeasible. A more subtle issue,
explored in the next section, is that even when eﬃcient it
may not be eﬀective.

It turns out that we can give attacks that overcome both
issues above by giving inversion attacks that utilize the con-
ﬁdence information revealed by ML APIs. To do so will re-
quire new techniques, which we explore using as case stud-
ies decision trees (the next section) and facial recognition
(Section 5).

4. MAP INVERTERS FOR TREES

We turn now to inversion attacks on decision trees. This
type of model is used on a wide range of data, and is often
favored because tree-structured rules are easy for users to
understand. There are two types of decision trees common
in the literature: classiﬁcation (where the class variable is
discrete), and regression (where the class variable is contin-
uous). In the following, we focus on classiﬁcation trees.

Decision tree background. A decision tree model re-
cursively partitions the feature space into disjoint regions
R1, . . . , Rm. Predictions are made for an instance (x, y) by
ﬁnding the region containing x, and returning the most likely
value for y observed in the training data within that region.

We characterize trees mathematically as follows:
wiφi(x), where φi(x) ∈ {0, 1}

f (x) =

m(cid:88)

i=1

where each basis function φi is an indicator for Ri, and wi
corresponds to the most common response observed in the
training set within Ri. A simple example is shown in Fig-
ure 3. Notice that there is exactly one basis function for
each path through the tree, and that a given x will “acti-
vate” only one φi (and thus traverse only one path through
the tree) because the basis functions partition the feature
space.

Decision trees can be extended to return conﬁdence mea-
sures for classiﬁcations by changing the form the wi coeﬃ-
cients take. This is usually accomplished by setting wi to
a vector corresponding to the distribution of class labels in
a given region, as observed in the training set. Looking at
the example in Figure 3, we would set w1 = [89, 11] if there
were 89 training examples with x1 = 1 and x2 = 0, and 11
with x1 = 1, x2 = 1. The classiﬁcation and corresponding
conﬁdences are given by:

i=1 wi[j]φi(x)(cid:1) , and
(cid:0)(cid:80)m
(cid:80)

wi∗ [|Y |]
i wm[i]

, . . . ,

(cid:21)

(cid:20) wi∗ [1](cid:80)

i w1[i]

f (x) = arg maxj

˜f (x) =

where i∗ in the second formula takes the value in {1, . . . , m}
such that φi∗ (x) = 1.
Decision tree APIs. Several web APIs expose training
and querying routines for decision trees to end-users, in-
cluding Microsoft Machine Learning [31], Wise.io [40], and
BigML [4]. Users can upload their datasets to these services,
train a decision tree to predict selected variables, and then
make the resulting tree available for others to use for free or
for charge per-query. We use as running example the API
exposed by BigML, as it currently has the largest market-
place of trained models and focuses on decision trees. The
results carry over to services with a similar API as well.

BigML allows users to publish trees in either black-box or
white-box mode.
In black-box mode, other users are only
allowed to query the decision tree for predictions using a
REST API. In white-box mode, users are allowed to see the
internal structure of the tree, as well as download a JSON
representation of it for local use. The amount of available
information about the training set therefore varies between
the two modes. In both settings, the adversary has access to
marginal priors for each feature of the training set (see Sec-
tion 3 for a description of this), in addition to a confusion
matrix C for which Ci,j gives the number of training in-
stances with y = i for which the model predicted label j. In
the white-box setting, the attacker also has access to a count
ni of the number of training set instances that match path

1325φi in the tree. This allows one to compute the conﬁdence of
a particular prediction.

The inversion problem. Fix a tree f (x) =(cid:80)m

i=1 wiφi(x)
and let (x, y) be a target instance that will either be from
the training data db or not (we will be clear in the following
about which setting we are in). We assume for simplicity
that there is one sensitive feature, the ﬁrst, making the tar-
get feature set in this case T = {1}; extending our attacks
to more than one feature is straightforward. The side infor-
mation side(cid:96)(x, y) = (x(cid:96), . . . , xd, y) for some to-be-speciﬁed
(cid:96) ≥ 2. We let K = {(cid:96), . . . , d} be the set of known feature
indices, and we abuse notation slightly to let xK represent
the d − 1 dimensional vector equal to x(cid:96), . . . , xd.
Black-box MI. For black-box MI we turn to the generic
algorithm from Section 3 and adapt it to this setting. The
main diﬀerence from the prior work’s setting is that the de-
cision tree models we consider produce discrete outputs, and
the error model information is diﬀerent, being a confusion
matrix as opposed to the standard deviation of a Gaussian.
For our purposes here, then, we use the confusion matrix C
and deﬁne err(y, y(cid:48)) ∝ Pr [ f (x) = y(cid:48) | y is the true label ].
In Section 4.1 we evaluate the algorithm of Figure 2, with
this error model, showing that it has unsatisfying perfor-
mance in terms of a prohibitively high false positive rate.

to φi. From this, he also knows N = (cid:80)m

White-box MI. Recall from before that in the white-box
setting, we not only assume that the attacker knows each φi,
but also the number of training samples ni that correspond
i=1 ni, the total
number of samples in the training set.
The known values xK induce a set of paths S = {si}1≤i≤m:
K = xK ∧ φi(x(cid:48))}. Each path
S = {(φi, ni) | ∃x(cid:48) ∈ Rd . x(cid:48)
corresponds to a basis function φi and a sample count ni.
We let pi denote ni/N , and note that each pi gives us some
information about the joint distribution on features used to
build the training set. In the following, we write si as short-
hand for the event that a row drawn from the joint prior
traverses the path si, i.e., Pr [ si ] corresponds to the proba-
bility of drawing a row from the joint prior that traverses si.
Observe that pi is an empirical estimate for this quantity,
derived from the draws used to produce the training set.

Recall that the basis functions partition the feature space,
so we know that x traverses exactly one of the paths in S.
Below we denote a speciﬁc value for the ﬁrst feature by v
and a speciﬁc value for the other d − 1 features as vK . We
will abuse notation slightly and write φi(v) as shorthand
1 = v ∧ φi(x(cid:48))).
for the indicator function I(∃x(cid:48) ∈ Rd . x(cid:48)
The following estimator characterizes the probability that
x1 = v given that x traverses one of the paths s1, . . . , sm
and xK = vK :

Pr [ x1 = v | (s1 ∨ ··· ∨ sm) ∧ xK = vK ]

piφi(v) · Pr [ xK = vK ] · Pr [ x1 = v ]

∝ m(cid:88)
1(cid:80)m

∝

i=1

(cid:80)m
(cid:88)

j=1 pjφj(v)

piφi(v) · Pr [ x1 = v ]

(1)

j=1 pjφj(v)

1≤i≤m

We refer to this as the white-box with counts (WBWC) esti-
mator. The adversary then outputs a value for v that max-
imizes (1) as a guess for x1. Like the Fredrikson et al. esti-
mator, it returns the MAP prediction given the additional
count information.

In the preceding analysis, we assumed that the attacker
knew all of x except x1. The WBWC estimator can be
extended to the general case where the attacker does not
know x2, . . . , xl (so xK = {l + 1, . . . , d− 1}) by summing (1)
over the unknown variables.

4.1 Experiments

We applied the black-box and white-box WBWC attacks
to decision trees trained on two datasets: FiveThirtyEight’s
“How Americans Like Their Steak” survey [17], and a subset
of the General Social Survey (GSS) focusing on responses re-
lated to marital happiness [33]. Each dataset contains rows
that correspond to individuals, with attributes correspond-
ing to survey responses. These datasets contain at least
one sensitive feature, and have been used to derive models
that are available on BigML’s marketplace. Additionally,
the source data is public, which makes them appropriate
surrogates for our study—without source data, we cannot
evaluate the eﬀectiveness of our attacks.

FiveThirtyEight survey. In May 2014, Walt Hickey wrote
an article for FiveThirtyEight’s DataLab section that at-
tempted a statistical analysis of the connection between peo-
ples’ steak preparation preferences and their propensity for
risk-taking behaviors [17]. To support the analysis, FiveThir-
tyEight commissioned a survey of 553 individuals from Sur-
veyMonkey, which collected responses to questions such as:
“Do you ever smoke cigarettes?”, “Have you ever cheated
on your signiﬁcant other?”, and of course, “How do you like
your steak prepared?”. Demographic characteristics such as
age, gender, household income, education, and census region
were also collected. We discarded rows that did not contain
responses for the inﬁdelity question or the steak preparation
question, resulting in a total of 332 rows for the inversion
experiments. We used model inversion on the decision tree
learned from this dataset to infer whether each participant
responded “Yes” to the question about inﬁdelity.

GSS marital happiness survey. The General Social Sur-
vey (GSS) collects detailed information on the demograph-
ics, interests, and attitudes of United States residents [37].
We use a subset of the GSS data [33] created by Joseph
Price for the purposes of studying various societal eﬀects of
pornography. This subset corresponds to 51,020 individuals
and 11 variables, including basic demographic information
and responses to questions such as, “How happy are you in
your marriage?” and “Have you watched X-rated movies in
the last year?” We discarded rows that did not contain re-
sponses to either of these questions, resulting in 16,127 total
rows for the inversion experiments. We use model inversion
to infer each participant’s response to the latter question.

Summary of results. For both datasets, we were able to
identify positive instances of the sensitive variable (i.e., a
“Yes” answer to “Have you ever cheated on your signiﬁcant
other?” or “Have you watched X-rated movies in the last
year?”) with high precision. Key ﬁndings are:
• Given white-box access to the BigML trees published by
others for these datasets, we are able to predict positive
instances with perfect precision, i.e., no false positives.
• Individuals whose responses are used in the training data
are at signiﬁcantly higher risk than individuals not in-
cluded in the training data. The results are stark: on
the FiveThirtyEight survey, white-box inversion yields

1326on average 593× improved precision and 371× improved
recall. Similar results hold for the GSS survey.

• White-box access to decision trees enhances the adver-
sary’s advantage. On the BigML tree trained using GSS
data, the white-box adversary has a signiﬁcant boost in
precision over the black-box adversary (158% greater)
for a modest decrease in recall (32% less).
In other
words, the white-box adversary is able to identify slightly
fewer “Yes” responses than the black-box adversary, but
with much better (i.e., perfect) precision.

Our results indicate that releasing a decision tree trained
over either of these datasets substantially increases the risk
of conﬁdential data leakage for the individuals who provided
responses. Details are given below.

Methodology. We ran the white-box and black-box inver-
sion attacks for each row of the datasets described above. To
evaluate the risk posed by existing, publicly-available mod-
els, we used trees obtained from the BigML service [4]. Be-
cause the sensitive feature in each dataset is binary-valued,
we use precision and recall to measure the eﬀectiveness of
the attack. In our setting, precision measures the fraction of
predicted “Yes” responses that are correct, and recall mea-
sures the fraction of “Yes” responses in the dataset that
are predicted by the adversary. We also measure the ac-
curacy which is deﬁned as the fraction of correct responses.
The trees published on BigML used the entire dataset for
training. To evaluate the eﬀect of training set inclusion,
we trained trees locally by constructing 100 trees using de-
fault parameters on randomly-sampled stratiﬁed training
sets comprised of 50% of the available data. We downloaded
the models and ran the experiments locally to avoid placing
a burden on BigML’s servers. We used a machine with 8
Xeon cores running at 2.5 Ghz, with 16G of memory.

To establish a baseline for comparison, we use three pre-
diction strategies corresponding to the capabilities of a ran-
dom adversary, a baseline adversary, and an ideal adversary.
• The random adversary has access to no information aside
from the domain of the sensitive attribute. This corre-
sponds to an attacker who cannot access a model, and
knows nothing about the prior. For both of our datasets,
the sensitive feature is binary, so the adversary’s best
strategy is to ﬂip a fair coin.

• The baseline adversary has access only to the marginal
probability distribution for the sensitive attribute, and
not the tree or any other information about the training
set. The baseline adversary’s best strategy is to always
guess the most likely value according to the marginal
distribution (i.e., its mode, “No” on our data).

• The ideal adversary has access to a decision tree trained
from the original dataset to predict the sensitive at-
tribute, and given the known features for an individual,
uses the tree to make a prediction.

This strategy for the ideal adversary is the appropriate one
in our setting as it inherits the limitations of the model
class: because the attack inverts a decision tree to make
predictions, we do not expect those predictions to be more
accurate than ones made by a decision tree trained to pre-
dict the sensitive feature in the forward direction, from the

algorithm acc. prec.
whitebox
100.0
blackbox
85.7
random
50.0
baseline
0.0
ideal

FiveThirtyEight
rec.
21.1
21.1
50.0
0.0
98.6

86.4
85.8
50.0
82.9
99.8

100.0

GSS
acc. prec.
100.0
80.3
80.0
38.8
50.0
50.0
0.0
82.0
80.3
61.5

rec.
0.7
1.0
50.0
0.0
2.3

Figure 4: MI results for for BigML models. All
numbers shown are percentages.

same data. This kind of same-model-class comparison was
also used in [13].

Performance. The amount of time needed to run both
the black-box and white-box attacks is negligible, with the
white-box attack being the most expensive. This attack took
about 6 milliseconds on average for both datasets, with the
most expensive component being path enumeration through
a fairly large (about 200 paths) tree. The number of calls
needed to run the black-box attack is small: 4 for the FiveThir-
tyEight dataset (there are 2 unknown binary features), and
2 for the GSS dataset. We conclude that for datasets similar
to these, having few unknown features from small domains,
the black-box attack is feasible to execute remotely.

Discussion. Figure 4 shows the full results for our ex-
periments. The largest diﬀerence between black-box and
white-box accuracy is precision: the white-box attack gave
no false positives, whereas black-box yielded about 15% in
FiveThirtyEight and about 60% on GSS. This shows that
the instance counts on tree paths gives useful information
about the joint prior, allowing the attacker to better iden-
tify negative instances of the sensitive feature. Measured
by both accuracy and precision, both attacks signiﬁcantly
outperform the random-guessing strategy, by at least 30%.
However, neither attack achieves higher recall. This is due
to the skewed prior distribution on the sensitive attribute
(about 80% “No” in both cases), which leads the attack to
favor answers which do not contribute to higher recall. The
upshot of this condition is much higher precision, which sup-
ports greater conﬁdence in positive predictions.

Figure 5a shows the advantage of the MI algorithms over
the baseline strategy, with advantage deﬁned as the increase
in accuracy, precision, or recall. Both algorithms compare
favorably in terms of precision on FiveThirtyEight (at least
80% improvement) and recall (at least 20% improvement),
but are only slightly better in terms of accuracy (3-5% im-
provement). However, on GSS accuracy is slightly lower,
whereas precision and recall are greater. Figure 5b shows
the results as a percentage of the ideal strategy’s perfor-
mance. Reaching 100% of the ideal strategy’s performance
means that the attack performs as well as one could rea-
sonably expect it to. The whitebox attack achieves this for
precision, and comes close (within 15%) for accuracy.

Figure 5c compares the performance of the attack on in-
stances from the training set versus instances outside it.
Both attacks dramatically increase the attacker’s chances
of predicting the sensitive attribute, by as much as 70% pre-
cision and 20% recall. This demonstrates that inclusion in
the training set poses a signiﬁcant risk to the conﬁdentiality
of these individuals’ responses.

1327e
n

i
l
e
s
a
b

r
e
v
o

.
v
d
A

100

80

60

40

20

0

l
a
e
d
I

f
o
%

100

80

60

40

20

0

acc.

prec.

rec.

acc.

FiveThirtyEight

rec.

prec.
GSS

Black-box

White-box

Ideal

acc.

prec.

rec.

acc.

FiveThirtyEight

Baseline

Black-box

rec.

prec.
GSS
White-box

l
a
e
d
I

f
o
%

100

80

60

40

20

0

acc.

prec.

rec.

acc.

FiveThirtyEight

prec.
GSS

rec.

BB Test

BB Train

WB Test

WB Train

(a) Results as advantage over baseline.

(b) Results as a percentage of ideal.

(c) Training vs. test attack performance.

Figure 5: BigML model inversion comparison to the baseline and ideal prediction strategies. Although the
white-box attack achieved greater than 100% improvement over the ideal strategy on the FiveThirtyEight
model, all percentages shown here are cropped at 100% to make the other results legible.

5. FACIAL RECOGNITION MODELS

Facial recognition models are functions that label an im-
age containing a face with an identiﬁer corresponding to the
individual depicted in the image. These models are used in-
creasingly often for a wide range of tasks such as authenti-
cation [8], subject identiﬁcation in security and law enforce-
ment settings [35], augmented reality [9], and photo organi-
zation (e.g., Facebook’s “DeepFace” [1]). A growing number
of web APIs support facial recognition, such as those oﬀered
by Lambda Labs [26], Kairos [21], and SkyBiometry [36]. A
number of local libraries also expose APIs for facial recog-
nition, such as OpenCV [5] and OpenBR [24]. Common to
all these APIs is the ability to train a model using a set of
images labeled with the names of individuals that appear in
them, and the ability to perform classiﬁcation given some
previously trained model. Notice that classiﬁcation may be
performed by a larger set of individuals, including ones who
do not have access to the labeled training set; use of the APIs
in this way has been suggested, for example, in augmented
reality applications [9].

In this section we discuss two MI attacks on the models
used by these APIs to violate the privacy of subjects in the
training set. Both attacks assume that the adversary has
access only to a trained model, but not to any of the original
training data.

1) In the ﬁrst attack we assume an adversary who knows
a label produced by the model, i.e. a person’s name or
unique identiﬁer, and wishes to produce an image of the
person associated with that label (i.e., the victim). The
adversary uses MI to “reconstruct” an image of the victim’s
face from the label. This attack violates the privacy of an
individual who is willing to provide images of themselves as
training data, as the adversary can potentially reconstruct
images of every individual in the training set. The adversary
“wins” an instance of this attack if, when shown a set of face
images including the victim, he can identify the victim. In
subsequent text, we refer to this as the reconstruction attack.
2) In the second attack we assume an adversary who has
an image containing a blurred-out face, and wishes to learn
the identity of the corresponding individual. The adversary
uses the blurred image as side information in a series of
MI attacks, the output of which is a deblurred image of the
subject. Assuming the original image was blurred to protect
anonymity, this attack violates the privacy of the person in
the image. Note that the image has been blurred to an
extent that the model no longer classiﬁes it correctly. The
adversary can only hope to succeed if the individual was in

the training set for the model. Here the adversary wins if he
identiﬁes the victim from a set of face images taken from the
training set, or if the subject of the blurred image was not
in the training set, and the adversary determines that the
image produced by the attack does not correspond to any
of the faces. We refer to this as the deblurring attack. This
attack builds directly on the reconstruction attack. Due
to space constraints we defer discussing it in detail, with
experimental results, to the companion technical report [14].

5.1 Background

There are many proposed algorithms and models for per-
forming facial recognition. Traditional approaches often re-
lied on complex, hand-coded feature extraction, alignment,
and adjustment algorithms, but more recently a number of
promising systems have achieved better performance using
neural networks [1,19,27]. These models are quickly becom-
ing the standard by which facial recognition systems are
evaluated, so we consider three types of neural network mod-
els: softmax regression, a multilayer perceptron network,
and a stacked denoising autoencoder network. These mod-
els vary in complexity, with softmax regression being the
simplest and the stacked denoising autoencoder network be-
ing the most complex.

Softmax regression. This classiﬁer is a generalization of
logistic regression that allows the class variable to take more
than two values—in our case, there are 40 individuals in the
dataset, so the classiﬁer needs to distinguish between 40 la-
bels. Softmax regression is often used as the ﬁnal layer in
deep neural network architectures, so on its own this classi-
ﬁer can be seen as a neural network with no hidden layers.

Multilayer perceptron network. We use a multilayer
perceptron network with one hidden layer of 3000 sigmoid
neurons (or units) and a softmax output layer. This clas-
siﬁer can be understood as performing softmax regression
after ﬁrst applying a non-linear transformation to the fea-
ture vector. The point of this transformation, which corre-
sponds to the hidden layer, is to map the feature vector into
a lower-dimensional space in which the classes are separable
by the softmax output layer.

Stacked denoising autoencoder network. This classi-
ﬁer is an example of a deep architecture, and consists of two
hidden layers and one softmax output layer. The two hid-
den layers, which have 1000 and 300 sigmoid units, are each
instances of a denoising autoencoder. An autoencoder is a
neural network that maps its feature vector to a latent rep-

1328c(x) def= 1 − ˜flabel (x) + AuxTerm(x)
x0 ← 0
for i ← 1 . . . α do

Algorithm 1 Inversion attack for facial recognition models.
1: function MI-Face(label , α, β, γ, λ)
2:
3:
4:
5:
6:
7:
8:
9:
10:

xi ← Process(xi−1 − λ · ∇c(xi−1))
if c(xi) ≥ max(c(xi−1), . . . , c(xi−β)) then
if c(xi) ≤ γ then

(c(xi)), minxi (c(xi))]

return [arg minxi

break

break

resentation (typically in a smaller space), and then maps it
back (i.e., reconstructs) into the original feature space. Au-
toencoders are trained to minimize reconstruction error, so
vectors in the latent space can be thought of as compressed
encodings of the feature space that should decode well for in-
stances in the training data. The hope with this architecture
is that these encodings capture the main factors of variation
in feature space (much like Principal Component Analysis),
leading to greater separability for the softmax layer.

Error
7.5%
4.2%
3.3%

Model
Softmax
MLP
DAE

Dataset. We trained each type
of model over the AT&T Lab-
oratories Cambridge database of
faces [2]. This set contains ten
black-and-white images of 40 indi-
viduals in various lighting condi-
tions, facial expressions, and de-
tails (e.g., glasses/no glasses), for
a total of 400 images. We divided
the images of each person into a training set (7 images) and
a validation set (3 images), and trained each model using
pylearn2’s stochastic gradient descent algorithm [15] until
the model’s performance on the training set failed to im-
prove after 100 iterations. The error rate for each model is
given in Figure 6.

Figure 6: Model
accuracy.

Basic MI attack. We now turn to inversion attacks against
the models described above. The features that we will at-
tempt to invert in this case are the full vector of pixel intensi-
ties that comprise an image, and each intensity corresponds
to a ﬂoating-point value in the range [0, 1]. In all of the at-
tacks we consider, we do not assume that the attacker knows
exact values for any of the pixels in the vector he is trying to
infer. These factors combine to make this type of inversion
substantially diﬀerent from the previous cases we consider,
so these attacks require new techniques.

Assuming feature vectors with n components and m face
classes, we model each facial recognition classiﬁer as a func-
tion, ˜f : [0, 1]n (cid:55)→ [0, 1]m. Recall that the output of the
model is a vector of probability values, with the ith compo-
nent corresponding to the probability that the feature vector
belongs to the ith class. We write ˜fi(x) as shorthand for the
ith component of the output.

We use gradient descent (GD) to minimize a cost func-
tion involving ˜f to perform model inversion in this setting.
Gradient descent ﬁnds the local minima of a diﬀerentiable
function by iteratively transforming a candidate solution to-
wards the negative of the gradient at the candidate solution.
Our algorithm is given by the function MI-Face in Algo-
rithm 1. The algorithm ﬁrst deﬁnes a cost function c in

Algorithm 2 Processing function for stacked DAE.

function Process-DAE(x)

encoder .Decode(x)
x ← NLMeansDenoise(x)
x ← Sharpen(x)
return encoder .Encode(vecx)

Figure 7: Reconstruction without using Process-
DAE (Algorithm 2) (left), with it (center), and the
training set image (right).

terms of the facial recognition model ˜f and a case-speciﬁc
function AuxTerm, which uses any available auxiliary in-
formation to inform the cost function. We will describe an
instantiation of AuxTerm when we discuss facial deblur-
ring. MI-Face then applies gradient descent for up to α
iterations, using gradient steps of size λ. After each step of
gradient descent, the resulting feature vector is given to a
post-processing function Process, which can perform vari-
ous image manipulations such as denoising and sharpening,
as necessary for a given attack. If the cost of the candidate
fails to improve in β iterations, or if the cost is at least as
great as γ, then descent terminates and the best candidate
is returned.

MI-Face needs to compute the gradient of the cost func-
tion c, which in turn requires computing the gradient of the
facial recognition model ˜f . This means that ˜f must be dif-
ferentiable for the attack to succeed. ∇ ˜f can be computed
manually or automatically using symbolic techniques. Our
experiments use symbolic facilities to implement the latter
approach.
5.2 Reconstruction Attack

The ﬁrst speciﬁc attack that we consider, which we will re-
fer to as Face-Rec, supposes that the adversary knows one of
the labels output by the model and wishes to reconstruct an
image depicting a recognizable face for the individual corre-
sponding to the label. This attack is a fairly straightforward
instantiation of MI-Face (Algorithm 1). The attacker has
no auxiliary information aside from the target label, so we
deﬁne AuxTerm(x) def= 0 for all x. Our experiments set the
parameters for MI-Face to: α = 5000, β = 100, γ = 0.99,
and λ = 0.1; we arrived at these values based on our ex-
perience running the algorithm on test data, and out of
consideration for the resources needed to run a full set of
experiments with these parameters.

In all cases except for the stacked DAE network, we set
Process to be the identity function. For stacked DAE net-
work, we use the function Process-DAE in Algorithm 2.
Since the adversary can inspect each of the model’s layers,
he can isolate the two autoencoder layers. We conﬁgure the
attack to generate candidate solutions in the latent space of

1329the ﬁrst autoencoder layer, and at each iteration, Process-
DAE decodes the candidate into the original pixel space,
applies a denoising ﬁlter followed by a sharpening ﬁlter, and
re-encodes the resulting pixels into the latent space. We
ﬁnd that this processing removes a substantial amount of
noise from the ﬁnal reconstruction, while enhancing recog-
nizable features and bringing relative pixel intensities closer
to the images from the training set. An example is shown
in Figure 7. The image on the left was reconstructed using
the label of the individual from the right image without the
use of Process-DAE, while the center picture was recon-
structed using this processing step.

Experiments. To evaluate the eﬀectiveness of the attack,
we ran it on each of the 40 labels in the AT&T Face Database,
and asked Mechanical Turk workers to match the recon-
structed image to one of ﬁve face images from the AT&T
set, or to respond that the displayed image does not corre-
spond to one of the ﬁve images. Each batch of experiments
was run three times, with the same test images shown to
workers in each run. This allowed us to study the consis-
tency of responses across workers. The images shown to
Turk workers were taken from the validation set, and thus
were not part of the training data.

In 80% of the experiments, one of the ﬁve images con-
tained the individual corresponding to the label used in the
attack. As a control, 10% of the instances used a plain im-
age from the data set rather than one produced by MI-Face.
This allowed us to gauge the baseline ability of the workers
at matching faces from the training set. In all cases, the im-
ages not corresponding to the attack label were selected at
random from the training set. Workers were paid $0.08 for
each task that they completed, and given a $0.05 bonus if
they answered the question correctly. We found that work-
ers were usually able to provide a response in less than 40
seconds. They were allowed to complete at most three tasks
for a given experiment. As a safeguard against random or
careless responses, we only hired workers who have com-
pleted at least 1,000 jobs on Mechanical Turk and achieved
at least a 95% approval rating.
5.2.1 Performance
We ran the attack for
each model on an 8-
core Xeon machine with
16G memory. The re-
sults are shown in Fig-
ure 8. Reconstructing
faces out of the softmax model is very eﬃcient, taking only
1.4 seconds on average and requiring 5.6 epochs (i.e., itera-
tions) of gradient descent. MLP takes substantially longer,
requiring about 21 minutes to complete and on the order
of 3000 epochs of gradient descent. DAE requires less time
(about 11 minutes) but a greater number of epochs. This is
due to the fact that the search takes place in the latent fea-
ture space of the ﬁrst autoencoder layer. Because this has
fewer units than the visible layer of our MLP architecture,
each epoch takes less time to complete.
5.2.2 Accuracy results
The main accuracy results are shown in Figure 9.

algorithm time (s)
Softmax
MLP
DAE

In
this ﬁgure, overall refers to all correct responses, i.e., the
worker selected the image corresponding to the individual
targeted in the attack when present, and otherwise selected

Figure 8: Attack runtime.

1.4

1298.7
692.5

epochs

5.6

3096.3
4728.5

“Not Present”. Identiﬁed refers to instances where the tar-
geted individual was displayed among the test images, and
the worker identiﬁed the correct image. Excluded refers to
instances where the targeted individual was not displayed,
and the worker correctly responded “Not Present”.

Figure 9a gives results averaged over all responses, whereas
9b only counts an instance as correct when a majority (at
least two out of three) users responded correctly. In both
cases, Softmax produced the best reconstructions, yielding
75% overall accuracy and up to an 87% identiﬁcation rate.
This is not surprising when one examines the reconstruction
produced by the three algorithms, as shown in Figure 10.
The reconstruction given by Softmax generally has sharp,
recognizable features, whereas MLP produces a faint out-
line of these features, and DAE produces an image that is
considerably more blurry.
In all cases, the attack signiﬁ-
cantly outperforms, by at least a factor of two, randomly
guessing from the six choices as this would give accuracy
just under 20%.
5.3 Black-Box Attacks

It may be possible in some cases to use numeric approxi-
mations for the gradient function in place of the explicit gra-
dient computation used above. This would allow a black-box
adversary for both types of attack on facial recognition mod-
els. We implemented this approach using scipy’s numeric
gradient approximation, and found that it worked well for
Softmax models—the reconstructed images look identical to
those produced using explicit gradients. Predictably, how-
ever, performance suﬀers. While Softmax only takes about a
minute to complete on average, MLP and DAE models take
signiﬁcantly longer. Each numeric gradient approximation
requires on the order of 2d black-box calls to the cost func-
tion, each of which takes approximately 70 milliseconds to
complete. At this rate, a single MLP or DAE experiment
would take 50–80 days to complete. Finding ways to opti-
mize the attack using approximate gradients is interesting
future work.

6. COUNTERMEASURES

In this section, we explore some potential avenues for
developing eﬀective countermeasures against these attacks.
Ideally one would develop full-ﬂedged countermeasures for
which it can be argued that no future MI attack will suc-
ceed. Here we do a more limited preliminary investigation,
showing discussing simple heuristics that prevent our cur-
rent attacks and which might guide future countermeasure
design.

Decision Trees. Just as the ordering of features in a de-
cision tree is crucial for accuracy, it may also be relevant to
the tree’s susceptibility to inversion attacks. In particular,
the level at which the sensitive feature occurs may aﬀect the
accuracy of the attack. To test this hypothesis, we imple-
mented a variant of CART learning that takes a parameter
(cid:96) which speciﬁes the priority at which the sensitive feature is
considered: the feature is only considered for splitting after
(cid:96)− 1 other features have already been selected, and removed
from consideration afterwards. We then sampled 90% of the
FiveThirtyEight dataset 100 times, and used this algorithm
to train a decision tree on each sample for every value of (cid:96).
We evaluated the classiﬁcation accuracy of the tree alongside
white-box inversion performance.

1330100
80
60
40
20

t
c
e
r
r
o
c
%

Softmax

MLP

DAE

100
80
60
40
20

100
80
60
40
20

overall

identiﬁed

excluded

overall

identiﬁed

excluded

overall

identiﬁed

excluded

(a) Average over all responses.

(b) Correct by majority vote.

(c) Accuracy with skilled workers.

Figure 9: Reconstruction attack results from Mechanical Turk surveys. “Skilled workers” are those who
completed at least ﬁve MTurk tasks, achieving at least 75% accuracy.

Target

Softmax

MLP

DAE

Figure 10: Reconstruction of the individual on the
left by Softmax, MLP, and DAE.

Figure 11: White-box MI vs. classiﬁcation accuracy
on decision trees trained on FiveThirtyEight data
with the sensitive feature at each priority level (cid:96).
For this data, the optimal placement of the sensi-
tive feature is at the ﬁrst level, achieving the best
classiﬁcation accuracy while admitting MI accuracy
only 1% greater than baseline.

The results are displayed in Figure 11 (we observed similar
trends for black-box performance). The eﬀectiveness of the
attack in this case is clearly aﬀected by the depth at which
the sensitive feature appears in the tree. When the feature
appears near the top or bottom of the tree, the attack fails
with greater probability than otherwise. Furthermore, al-
though prioritizing placement of the sensitive feature at a
particular level does impact model accuracy, there is an op-
timal placement in this case: when the feature is placed at
the top of the tree, classiﬁcation accuracy is maximized while
inversion accuracy is only 1% greater than baseline guess-
ing. This suggests that it may be possible to design more
sophisticated training algorithms that incorporate model in-
version metrics into the splitting criteria in order to achieve
resistance to attacks without unduly sacriﬁcing accuracy.

Figure 12: Black-box face reconstruction attack
with rounding level r. The attack fails to produce a
non-empy image at r = 0.1, thus showing that round-
ing yields a simple-but-eﬀective countermeasure.

To understand why attack performance is not monotone
in (cid:96), we counted the number of times each tree used the
sensitive feature as a split. This measure increases until it
reaches its maximum at (cid:96) = 8, and steadily decreases until
(cid:96) = 12. The diﬀerence in split frequency between (cid:96) = 8 and
(cid:96) = 12 is approximately 6×. This is most likely because once
most of the features have been used, the training algorithm
deems further splitting unnecessary, thus omitting the sen-
sitive feature from many subtrees. The inversion algorithm
is unable to do better than baseline guessing for individu-
als matching paths through these subtrees, thus making the
attack less eﬀective.

Facial Recognition. Our attacks on facial recognition
models are all based on gradient descent. One possible de-
fense is to degrade the quality or precision of the gradient
information retreivable from the model. There is no obvious
way to achieve this in the white-box setting while preserv-
ing model utility, but in the black-box setting this might be
achieved by reducing the precision at which conﬁdence scores
are reported. We tested this approach by rounding the score
produced by the softmax model, and running the black-box
reconstruction attack. The results are presented in Figure 12
for rounding levels r = {0.001, 0.005, 0.01, 0.05}; the attack
failed to produce an image for r = 0.1. “No rounding” corre-
sponds to using raw 64-bit ﬂoating-point scores to compute
numeric gradients. Notice that even at r = 0.05, the attack
fails to produce a recognizable image. This suggests that
black-box facial recognition models can produce conﬁdence
scores that are useful for many purposes while remaining
resistant to reconstruction attacks.

1234567891011120.80.820.840.860.880.9‘MIaccuracy1234567891011120.30.320.340.360.380.4Class.accuracyMIaccuracyClass.accuracynoroundingr=0.001r=0.005r=0.01r=0.0513317. RELATED WORK

Machine-learning techniques are used in a variety of ap-
plications, such as intrusion detection, spam ﬁltering, and
virus detection. The basic idea behind these systems is to
train a classiﬁer that recognizes “normal behavior”, so that
malicious behaviors as can be labeled as abnormal. An ac-
tive adversary can try to subvert these systems in a variety
of ways, such as crafting malicious inputs that the classiﬁer
mistakenly labels as normal. This is called an evasion or
mimicry attack. An adversary can also try to degrade the
performance of such systems by devising inputs that cre-
ate a large number of false alarms, thus overwhelming the
system administrator. This is a variant of classic denial-of-
service attacks. Barreno et al. [3] consider such attacks on
machine learning algorithms. Lowd and Meek [30] extend
these attacks to scenarios where the attacker does not have
a complete description of the classiﬁer. In contrast to this
line of work, we target the privacy implications of exposing
machine learning model functionality. However, a connec-
tion between these two lines of research will be interesting
to explore in the future, as the mechanisms used for our
attacks might prove useful in their settings.

Several authors have explored linear reconstruction at-
tacks [10,12,23], which work as follows: given some released
information y and assuming a hidden vector s of sensi-
tive features (e.g., Boolean indicators for disease status of a
group of patients), the attacker constructs a system of linear
inequalities (a matrix A and a vector z) such that As ≈ z,
and attempts to solve for s using techniques that minimize
some norm, such as LP decoding. Kasiviswanathan, Rudel-
son, and Smith [22] extended these attacks to releases that
are non-linear, such as M -estimators. We also explore at-
tacks on non-linear models, and we further investigate these
attacks in realistic settings. It will be interesting future work
to investigate whether strategies such as LP decoding work
in settings similar to those considered in this paper.

Many disclosure attacks that have been explored in the lit-
erature. The classic attack by Sweeney [38] showed that it
was possible to correlate publicly-available anonymzied hos-
pital visit data with voter registration data, leading to re-
identiﬁcation of some individuals. Narayananan [32] demon-
strated that an adversary with some prior knowledge can
identify a subscriber’s record in the anonymized Netﬂix prize
dataset. Wang et al. [39], Sankararaman et al. [34], and
Homer et al. [18] consider disclosure attacks on datasets gen-
erated from Genome-Wide Association Studies (GWAS) and
other partial genetic information for an individual. Cormode
showed that if an adversary is allowed to make queries that
relate sensitive attributes to quasi-identiﬁers, then it is pos-
sible to build a diﬀerentially-private Naive Bayes classiﬁer
that accurately predicts a sensitive attribute [7]. Loukides
et al. [29] show that one can infer sensitive patient infor-
mation from de-identiﬁed clinical data. The main diﬀerence
between this line of work and that presented in this pa-
per is the type of information used by the adversary to infer
sensitive information. Whereas previous work relied primar-
ily on de-identiﬁed datasets, our attacks operate entirely on
trained machine learning models and the metadata that is
commonly released with them.

Komarova et al. [25] studied the problem of partial disclo-
sure, where an adversary is given ﬁxed statistical estimates
from combined public and private sources, and attempts to
infer the sensitive feature of an individual referenced in those

sources. The key diﬀerence between theirs and our setting
is that we assume the adversary is given a statistical estima-
tor as a function, and can thus use it directly to make and
evaluate predictions about individuals. It is worth studying
whether the statistics used in their study can be used as ad-
ditional side information to boost the performance of model
inversion.

Li et al. [28] explore a privacy framework called member-
ship privacy. This framework consists of two notions: pos-
itive membership privacy (PMP) and negative membership
privacy (NMP). The framework gives rise to a number of se-
curity notions, including variants of diﬀerential privacy [11].
These security notions were not designed to resist MI at-
tacks, and whether they might help guide countermeasure
design is an open question. (See also the discussion of dif-
ferential privacy in [13].) We also note that MI may serve as
a primitive useful to adversaries seeking to violate these pri-
vacy notions (e.g., a high MI accuracy might reveal whether
a person was in the training set).

8. CONCLUSION

We demonstrated how the conﬁdence information returned
by many machine learning ML classiﬁers enables new model
inversion attacks that could lead to unexpected privacy is-
sues. By evaluating our model inversion algorithms over
decision trees published on a ML-as-a-service marketplace,
we showed that they can be used to infer sensitive responses
given by survey respondents with no false positives. Using a
large-scale study on Mechanical Turk, we showed they they
can also be used to extract images from facial recognition
models that a large majority of skilled humans are able to
consistently re-identify.

We explored some simple approaches that can be used
to build eﬀective countermeasures to our attacks, initiating
an experimental evaluation of defensive strategies. Although
these approaches do not constitute full-ﬂedged private learn-
ing algorithms, they illustrate trends that can be used to
guide future work towards more complete algorithms. Our
future eﬀorts will follow this path, as we continue to work
towards new systems that are able to beneﬁt from advances
in machine learning without introducing vulnerabilities that
lead to model inversion attacks.

9. REFERENCES
[1] DeepFace: Closing the Gap to Human-Level

Performance in Face Veriﬁcation. In Conference on
Computer Vision and Pattern Recognition (CVPR).

[2] AT&T Laboratories Cambridge. The ORL database of

faces. http://www.cl.cam.ac.uk/research/dtg/
attarchive/facedatabase.html.

[3] M. Barreno, B. Nelson, R. Sears, A. D. Joseph, and

J. D. Tygar. Can machine learning be secure? In
Proceedings of the 2006 ACM Symposium on
Information, computer and communications security,
pages 16–25. ACM, 2006.

[4] BigML. https://www.bigml.com/.
[5] G. Bradski. The OpenCV library. Dr. Dobb’s Journal

of Software Tools, Jan. 2000.

[6] C.-L. Chi, W. Nick Street, J. G. Robinson, and M. A.

Crawford. Individualized patient-centered lifestyle
recommendations: An expert system for
communicating patient speciﬁc cardiovascular risk

1332information and prioritizing lifestyle options. J. of
Biomedical Informatics, 45(6):1164–1174, Dec. 2012.
[7] G. Cormode. Personal privacy vs population privacy:

learning to attack anonymization. In KDD, 2011.

[8] M. Dabbah, W. Woo, and S. Dlay. Secure

authentication for face recognition. In IEEE
Symposium on Computational Intelligence in Image
and Signal Processing, pages 121–126, April 2007.

[9] C. Dillow. Augmented identity app helps you identify
strangers on the street. Popular Science, Feb. 23 2010.

[10] I. Dinur and K. Nissim. Revealing information while

preserving privacy. In PODS, 2003.

[11] C. Dwork. Diﬀerential privacy. In ICALP. Springer,

2006.

[12] C. Dwork, F. McSherry, and K. Talwar. The price of

privacy and the limits of lp decoding. In STOC, 2007.
[13] M. Fredrikson, E. Lantz, S. Jha, S. Lin, D. Page, and

T. Ristenpart. Privacy in pharmacogenetics: An
end-to-end case study of personalized warfarin dosing.
In USENIX Security Symposium, pages 17–32, 2014.
[14] Fredrikson, M. and Jha, S. and Ristenpart, T. Model

inversion attacks and basic countermeasures
( Technical Report). Technical report, 2015.

[15] I. J. Goodfellow, D. Warde-Farley, P. Lamblin,

V. Dumoulin, M. Mirza, R. Pascanu, J. Bergstra,
F. Bastien, and Y. Bengio. Pylearn2: a machine
learning research library. arXiv preprint
arXiv:1308.4214, 2013.

[16] Google. Prediction API.

https://cloud.google.com/prediction/.

[17] W. Hickey. FiveThirtyEight.com DataLab: How

americans like their steak.
http://fivethirtyeight.com/datalab/how-
americans-like-their-steak/, May 2014.

[18] N. Homer, S. Szelinger, M. Redman, D. Duggan,

W. Tembe, J. Muehling, J. V. Pearson, D. A. Stephan,
S. F. Nelson, and D. W. Craig. Resolving individuals
contributing trace amounts of dna to highly complex
mixtures using high-density snp genotyping
microarrays. PLOS Genetics, 2008.

[19] G. Huang, H. Lee, and E. Learned-Miller. Learning
hierarchical representations for face veriﬁcation with
convolutional deep belief networks. In Computer
Vision and Pattern Recognition (CVPR), June 2012.
[20] International Warfarin Pharmacogenetic Consortium.

Estimation of the warfarin dose with clinical and
pharmacogenetic data. New England Journal of
Medicine, 360(8):753–764, 2009.

[21] Kairos AR, Inc. Facial recognition API.
https://developer.kairos.com/docs.

[25] T. Komarova, D. Nekipelov, and E. Yakovlev.

Estimation of Treatment Eﬀects from Combined Data:
Identiﬁcation versus Data Security. NBER volume
Economics of Digitization: An Agenda, To appear.

[26] Lambda Labs. Facial recognition API.

https://lambdal.com/face-recognition-api.
[27] H. Lee, R. Grosse, R. Ranganath, and A. Y. Ng.

Convolutional deep belief networks for scalable
unsupervised learning of hierarchical representations.
In Proceedings of the 26th Annual International
Conference on Machine Learning, ICML ’09, pages
609–616, New York, NY, USA, 2009. ACM.

[28] N. Li, W. Qardaji, D. Su, Y. Wu, and W. Yang.
Membership privacy: A unifying framework for
privacy deﬁnitions. In Proceedings of ACM CCS, 2013.

[29] G. Loukides, J. C. Denny, and B. Malin. The

disclosure of diagnosis codes can breach research
participants’ privacy. Journal of the American Medical
Informatics Association, 17(3):322–327, 2010.

[30] D. Lowd and C. Meek. Adversarial learning. In

Proceedings of the eleventh ACM SIGKDD
international conference on Knowledge discovery in
data mining, pages 641–647. ACM, 2005.

[31] Microsoft. Microsoft Azure Machine Learning.
[32] A. Narayanan and V. Shmatikov. Robust

de-anonymization of large sparse datasets. In IEEE
Symposium on Security and Privacy, pages 111–125,
2008.

[33] J. Prince. Social science research on pornography.

http://byuresearch.org/ssrp/downloads/
GSShappiness.pdf.

[34] S. Sankararaman, G. Obozinski, M. I. Jordan, and

E. Halperin. Genomic privacy and limits of individual
detection in a pool. Nature Genetics, 41(9):965–967,
2009.

[35] C. Savage. Facial scanning is making gains in

surveillance. The New York Times, Aug. 21 2013.

[36] SkyBiometry. Facial recognition API. https://www.
skybiometry.com/Documentation#faces/recognize.

[37] T. W. Smith, P. Marsden, M. Hout, and J. Kim.

General social surveys, 1972-2012. National Opinion
Research Center [producer]; The Roper Center for
Public Opinion Research, University of Connecticut
[distributor], 2103.

[38] L. Sweeney. Simple demographics often identify people

uniquely. 2000.

[39] R. Wang, Y. F. Li, X. Wang, H. Tang, and X. Zhou.

Learning your identity and disease from research
papers: information leaks in genome wide association
studies. In CCS, 2009.

[22] S. P. Kasiviswanathan, M. Rudelson, and A. Smith.

[40] Wise.io. http://www.wise.io/.

The power of linear reconstruction attacks. In SODA,
2013.

[23] S. P. Kasiviswanathan, M. Rudelson, A. Smith, and

J. Ullman. The price of privately releasing contingency
tables and the spectra of random matrices with
correlated rows. In STOC, 2010.

[24] J. Klontz, B. Klare, S. Klum, A. Jain, and M. Burge.

Open source biometric recognition. In IEEE
International Conference on Biometrics: Theory,
Applications and Systems, pages 1–8, Sept 2013.

1333