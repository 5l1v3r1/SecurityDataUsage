TACHYON: Tandem Execution for Efﬁcient Live Patch Testing

Matthew Maurer

maurer@cmu.edu

David Brumley

dbrumley@cmu.edu

Carnegie Mellon University

Carnegie Mellon University

Abstract

The vast number of security incidents are caused by
exploits against vulnerabilities for which a patch is al-
ready available, but that users simply did not install.
Patch installation is often delayed because patches must
be tested manually to make sure they do not introduce
problems, especially at the enterprise level.

In this paper we propose a new tandem execution ap-
proach for automated patch testing. Our approach is
based on a patch execution consistency model which
maintains that a patch is safe to apply if the executions
of the pre and post-patch program only differ on at-
tack inputs. Tandem execution runs both pre and post-
patch programs simultaneously in order to check for ex-
ecution consistency. We have implemented our tech-
niques in TACHYON, a system for online patch testing
in Linux. TACHYON is able to automatically check and
verify patches without source access.

1

Introduction

Most attacks target known vulnerabilities for which there
are already patches. For example, Microsoft reported
that only 0.12activity in the ﬁrst half of 2011 involved a
zero-day attack for which a patch has been available for
a month or less [19]. For the other 99.88%, exploits were
successful simply because available patches were not in-
stalled. These statistics indicate that one of the best ways
to reduce security incidents due to exploits is to simply
patch vulnerable systems.

The need to rapidly deploy security patches in enter-
prise environments is hampered by the need to also test
patches for problems. Bad patches often have more busi-
ness risk than a security breach, suggesting that the abil-
ity to test patches and guard against such problems might
result in faster deployment of security patches. Measures
currently deployed in cloud environments deal with ran-
dom failures, rather than systematic ones caused by bad

patches. As a result, current best practices amount to
manual testing, which is slow, error-prone, and expen-
sive. For example, NIST best practices recommend man-
ual patch testing (which is slow) on pre-production envi-
ronments (which are expensive to acquire and maintain)
when available, or simply waiting to see if others report a
problem or not [18]. While such approaches prevent bad
patches from being applied, they increase the vulnerabil-
ity window. Even when a pre-production environment
is provided through virtualization, reducing the cost sub-
stantially, auxiliary services, such as databases, must be
simulated. This leads to excessive administration over-
head and compute overhead. Additionally, effects are
captured in an often ad-hoc manner (e.g. by recording
network traces), which can miss changes the administra-
tor did not think to look for. For example, the US Air
Force implements a centralized patch testing procedure
for their half million managed machines, but as a result,
delay patch rollout by up to a quarter year [14].

If we could automatically test patches, then we could
shorten the vulnerable time window between when a
patch is released until it is installed. However, automated
patch testing faces several challenges.

First, in order to faithfully check that functionality is
preserved in a patch, we should be able to test a patch on
the system it will ultimately protect. Second, in order to
be widely applicable, we should be able to test patches
in the common scenario where the patch is a new binary
program, as source is often not available. Third, we want
to minimize manual effort. As patches can change the
semantics of a program, a human will likely always need
to be in the loop to determine if the semantic changes
are meaningful. However, we still wish to automate the
system as much as possible. Unfortunately, there is very
little work on automated patch testing, and no previous
work addresses all these requirements.

In this paper we propose the ﬁrst techniques for live
patch testing via tandem execution. Our insight is that
current manual testing checks whether the executions of

a pre-patch and post-patch binary produce different out-
puts on known inputs. We call this observational equiv-
alence between pre and post-patch.

Tandem execution uses this insight to automate patch
testing by simultaneously running both programs on the
same input. More speciﬁcally, in tandem execution one
program runs live on the system (e.g., the patched pro-
gram), with all system calls (syscalls) being serviced by
the kernel. The second version of the program (e.g.,
the unpatched program) runs in tandem, but with each
syscall to the kernel simulated by replaying the side ef-
fects from the corresponding calls of the live version.
The replay prevents duplicating side-effects, such as
writing to the same ﬁle twice.

If the two programs deviate on the syscalls issued, or
the arguments to syscalls, then they are not observation-
ally equivalent. We record the deviation and inform the
user of the potential problem. At this point, the actions
that can be taken are to halt the program, or to specify
that the deviation is permitted. In order to continue test-
ing, the user provides a rewrite rule that speciﬁes how to
handle the deviation, and automated patch testing con-
tinues. In our experiments we show that rewrite rules are
small when needed, and often completely unnecessary
for security-related ﬁxes.

We implement our approach in a system called
TACHYON. TACHYON is based upon syscall replay tech-
niques for binary programs, but with a new twist. Ex-
isting system call replay schemes are designed to record
system calls from one run of a binary for replay against
exactly the same binary, e.g., [1, 11, 22]. Since both
record and replay are against the same binary, the record
step only needs to conceptually keep a snapshot of the
memory cells affected by the syscall. The affected mem-
ory cells are typically determined by differencing the pre
and post-syscall memory state. During replay, the mem-
ory cells at exactly the same addresses are replayed with
the recorded data.

The twist in our setting is we want to replay syscalls
to a different binary. Typically the two binaries will
have a different memory layout, and may make differ-
ent syscalls. For example, the system may have ASLR
enabled, or a patch may change a buffer from the too-
small size of 1024 bytes to the just-right size of 4096.
In either case, all pointers are likely to have different ad-
dresses between the patched and unpatched versions. As
a result, previous raw memory snapshot and replay ap-
proaches do not work.

To address our twist, TACHYON takes a semantic-
based approach to replay only the semantically mean-
ingful information from a syscall in a recording during
replay, rather than capturing details like pointer values
which may change between patches. Our approach is
enabled by three techniques. First, we extend the C

type system to include a full description of the syscall
side-effects. The description enables TACHYON to iden-
tify semantically meaningful arguments and results in
syscalls instead of relying on blind memory differenc-
ing. The work to annotate the system calls needs to be
performed once, and can be reused for all programs. Sec-
ond, TACHYON utilizes a rewrite rule system to compare
syscall sequences for equivalence and rewrite if neces-
sary. The rewrite system gives the end-user the abil-
ity to say when deviations are permitted in a systematic
manner. Third, TACHYON uses syscall interposition tech-
niques to record the effects of syscalls on a live program,
and replay those effects to simulate syscalls on the tan-
dem program.

Tandem execution makes patch testing in new scenar-
ios possible. For example, a test administrator can run
the pre-patch binary live and the post-patch in-tandem
on the same system. Any deviation reported is either (a)
a bug in the patch, or (b) an exploit against the buggy
program that is averted in the patched program, or (c)
a permitted change that changes IO behavior.
In all
cases, the administrator should be informed of the devia-
tion. Alternately, a security-conscious administrator may
run the patched version live, noting deviations with the
pre-patched version. In either case, the tandem execu-
tion achieves live patch testing without duplicating side-
effects. The closest current best practices come to similar
results requires mirroring a production environment with
a pre-production environment, which is expensive and
requires signiﬁcant effort to maintain. We discuss other
possible applications such as creating honeypots in § 7.
We have implemented and evaluated TACHYON on a
number of security patches, and demonstrated that our
techniques can successfully detect deviations. We have
also performed micro-benchmarks that show our imple-
mentation is efﬁcient with respect to the amount of I/O
performed. We show our implementation records full
syscall information faster than strace, a tracing tool
targeted at binary programs, and is efﬁcient in compari-
son to an untraced run.

Contributions. The main contribution of this paper is
techniques for live tandem execution for patch testing.
These techniques automate a large part of patch testing,
thus reducing the vulnerability window for unpatched
systems. In particular:
• We are the ﬁrst to propose techniques for auto-
mated patch testing that address all the above chal-
lenges; we have implemented them in TACHYON.
We demonstrate where we run both the unpatched
and patched binary and use tandem execution to de-
tect deviations. An additional beneﬁt of tandem ex-
ecution is that it can utilize extra cores for the secu-
rity purpose of patch testing.

2

Listing 1: Example patch

f d = open ( ” / tmp / f i l e A ” , O RDONLY) ;
f d = open ( ” / tmp / f i l e B ” , O RDWR) ;

1 −i n t
2 + i n t
3 −i n t ∗ s t o r a g e = m a l l o c ( . . . ) ;
4 −/∗ . . . Do some p r o c e s s i n g w i t h s t o r a g e . .
5 + f s t a t ( fd ,
6
7
8
9

char∗ incoming = m a l l o c ( c h u n k s i z e ) ;
s s i z e t
i f

( s i z e != −1)
w r i t e ( fdOut ,

s t a t B u f ) ;

∗ /

s i z e = r e a d ( fd ,

incoming ,

c h u n k s i z e ) ;

incoming ,

s i z e ) ;

Consider the example shown in Listing 1, with the
patch difference being displayed in diff style with full
context. The ﬁrst edit changes the ﬁle opened from
/tmp/fileA to /tmp/fileB. The next few edits re-
move an unneeded call to malloc, and add fstat.
The rest of the program is the same. Note that since a
malloc call was removed, the returned memory chunk
for incoming will be at a different address, even on
systems with a deterministic memory layout. Overall,
this patch example illustrates three challenges: patches
may change arguments to system calls, may change sys-
tem calls issued, may change memory allocation pat-
terns, and any of these changes may have effects on sub-
sequent execution.

The above challenges motivate three main require-
ments of live patch testing as distinguished from a nor-
mal replay system. First, instead of ofﬂine replay, a live
patch testing solution should be online where P produces
the syscall stream that P(cid:48) should consume. Second, a live
patch tester should not depend upon pointers because ab-
solute memory addresses may change between runs. For
example, P(cid:48) and P may issue calls to malloc for dif-
ferent amounts or ASLR may be enabled. Either case
prevents patch testing. As a result, we cannot determine
(cid:126)O by simply difﬁng the memory state before and after a
syscall, as in previous syscall replay schemes [11, 22].
Additionally, memory difﬁng does not allow us to deter-
mine the inputs (cid:126)I to system calls. As a good live patch
tester should verify the inputs as well, we need some way
to extract all inputs of a system call. Without a semantic
model, we will be unable to both locate all the relevant
components of the input, and to avoid capturing irrele-
vant components. Thus, we need a semantic model of the
inputs. Third, since a patch may remove or add system
calls, the live patch testing scheme should allow for the
syscall stream to be rewritten during replay. This can be
accounted for by allowing rewriting of the tuple stream
(cid:104)C,(cid:126)I, (cid:126)O(cid:105).

Figure 1: Tachyon System Overview

• We develop a type system that fully encapsulates
the side effects of syscalls necessary for replay. Our
type system is similar to [11], but does not require
source code access or explicit developer coopera-
tion.
• We propose a light-weight rule-based system for
checking syscall stream equivalence (and rewriting
if necessary) when the sequence of syscalls between
the patched and unpatched binary are not exactly the
same.
• We have implemented our techniques in TACHYON
using Haskell (a type-safe language) and validated
the techniques experimentally. Our system is ro-
bust enough to handle single-threaded and multi-
threaded programs. We evaluate our approach on
several real-world patches, as well as synthetic
benchmarks, to show the effectiveness and perfor-
mance of TACHYON.

2 Design of TACHYON

2.1 Overview of TACHYON and Challenges
The overall architecture of TACHYON is shown in Fig-
ure 1. In this paper we call the program running live P
(e.g., the patched program) and the program running in-
tandem with simulated syscalls P(cid:48) (e.g., the unpatched
program). TACHYON is a user-land program that uti-
lizes the Linux ptrace facility to interpose on syscalls is-
sued by P and P(cid:48). Like replay schemes, TACHYON has a
recorder and a replay module. The recorder records the
stream of system calls issued by P and outputs a stream
of tuples (cid:104)C,(cid:126)I, (cid:126)O(cid:105) where C is the system call number, (cid:126)I is
a list of inputs to the system call, and (cid:126)O is a list of out-
puts. The replay module interposes on P(cid:48), and for each
syscall C with arguments (cid:126)I made by P(cid:48), simulates the OS
by returning (cid:126)O.

3

Replayed Program (P')Traced Program (P)Type SystemRewrite RulesTachyon ReplayerTachyon RecorderMatchingSyscall Stream2.2 System Calls and Side-Effects
TACHYON needs to determine what the semantic inputs
and outputs to a syscall are in order to record and replay
them. Speciﬁcally, it needs to (1) determine the types of
arguments to a syscall, (2) differentiate input from out-
put, and (3) pointers from the pointed-to data. While ex-
isting C syscall prototypes are sufﬁcient for (1), they do
not provide enough information for (2) and (3). Consider
the read syscall declaration:

1 s s i z e t

r e a d ( i n t

fd , void ∗ buf ,

s i z e t

c o u n t ) ;

This C declaration misses crucial information. First,
it gives no clue how the void pointer buf works. How
big is it? Is it null-terminated? Are the contents rele-
vant before the call, after, or both? We need to answer all
these questions in order to copy the appropriate semantic
data. We can see that even the assertion that a pointer
points at some data before or after the system call is not
the case, as with sbrk (pointer points at the end of your
address space) and mmap (one pointer is only a sugges-
tion). read is one of the simple cases; several syscalls
have complicated dependencies between input and out-
put parameters, as will be discussed later in §3.

TACHYON addresses the challenges associated with
understanding the semantics of syscalls by adding type
annotations, as described in §3. The TACHYON anno-
tation language is a light-weight dependent type system
that says how to parse the inputs and arguments into se-
mantic data at runtime. These type annotations only need
to be written once per system call, and are portable across
systems with the same syscall signatures.

2.3 Syscall Stream Rewriting
Many patches also change the sequence of syscalls made
in addition to the actual parameters. Consider Listing 1.
The system call stream when executing the patched pro-
gram is (cid:104)..., open, fstat, read, write, ...(cid:105). However,
the call after open in the unpatched program is read,
not fstat.

New patched versions often have new system call pat-
terns that cause the program to behave differently at an
IO level.
It is not possible to tell whether a particular
change in system call patterns is valid without a human
to validate it. For example, if it turns out the above code
is just shifting a few things around and adding a new
inconsequential call to fstat, then the user may want
to ignore the deviation. However, the fstat may have
been inserted for security, and a deviation may indicate
an attack. When opening ﬁles in /tmp/ a common se-
curity practice is to then call fstat to obtain the user ID
and group ID of the ﬁle to make sure they are correct in

4

order to detect race conditions. TACHYON should report
the deviation and halt execution in such instances.

Although we cannot automatically decide which de-
viations matter, TACHYON does automate ﬁnding devi-
ations, as well as provide a mechanism to ignore such
deviations when found to continue testing. The rewriting
engine relies upon rules that are created for each patched
program that detail how to handle semantic differences.
For example, if a system administrator decides the above
deviation is inconsequential a rule can be written to ig-
nore the fstat call. Alternatively, patch creators could
write such rules and distribute them with their patches to
aid testing.

2.4 Road Map
In the rest of this paper, we ﬁrst describe the TACHYON
type system in detail. We then discuss how TACHYON
rewrites system calls, as well as some common rules
we have found in patches we have tested. We next de-
scribe our implementation and evaluation. We ﬁnally
discuss several applications of TACHYON outside auto-
mated patch testing.

3 System Call Types

The C function declarations for syscalls do not describe
all side-effects. TACHYON proposes an extension to the
C type declarations to encode all semantic information
necessary to record which parameters are inputs, which
are outputs, and how to identify all bytes for each param-
eter. While this problem has been attacked before [11],
our particular needs are different due to the binary only-
nature of our approach, as we discuss in § 9.

The TACHYON type system takes advantage of the
user-space/kernel-space barrier for interposition. The
barrier provides a clean separation that can be monitored
without requiring source to the program. In addition, the
barrier allows TACHYON to not monitor internal kernel
state. The reason is that the only way P(cid:48) and P can in-
teract with the underlying system is via TACHYON, and
TACHYON’s mechanism ensures that both programs see
an identical state. This is a vital complexity reduction.

TACHYON uses a limited form of lightweight de-
pendent types (types which depend on values). Our
lightweight use avoids pitfalls such as undecidability
normally associated with dependent types.
In the rest
of this section, we ﬁrst provide an intuition on the issues
and how dependent types are used, and then describe the
full system.

Intuition

3.1
A basic approach for recording syscalls is to decorate C
types with information about which parameters should
be treated as inputs, outputs, or both. We call such an-
notations the IO class for each parameter.
In order to
specify how to copy output parameters, we also need to
know the size of their values. The size information is
needed because we need to copy all output bytes from
buf in the monitored program P to the address space of
P(cid:48). For example, we could imagine annotating the read
call as:

1 s s i z e t

r e a d ( i n t

s i z e t

c o u n t ) ;

fd , void o u t p u t ∗ buf{ r e t } ,

The parameter buf has been annotated as an output
parameter, thus should be copied and replayed to P(cid:48).
The annotation also speciﬁes that ret bytes should be
copied, where ret is the return value.

Unfortunately, such simple annotations are insufﬁcient
with many data structures, such as vectors. A prime ex-
ample of a difﬁcult system call is readv, which pro-
vides vectored reads of a ﬁle descriptor. Its C type dec-
laration is:

1 s s i z e t
i n t

2 s t r u c t
void
3
4
s i z e t
5 } ;

r e a d v ( i n t
i o v c n t ) ;
i o v e c {
∗ i o v b a s e ;
i o v l e n ;

fd ,

const

s t r u c t

i o v e c ∗ iov ,

/∗ S t a r t i n g a d d r e s s ∗ /
/∗ Num b y t e s

i n i o v b a s e ∗ /

The main issue demonstrated is that a complete descrip-
tion of the IO behavior of parameters may refer to other
parameters. The iov base length is determined by
iov len, and the total number of iov items is given
by iovcnt. readv is not alone: it has many friends
such as writev, preadv, and pwritev. In order to
handle such cases, we need a type system that allows us
to express relationships between parameter values.

TACHYON uses lightweight dependent types that can
express relationships between the value of one parameter
and the value of another. We view types as a tree, and use
dependent types to walk the tree to determine a value.

The types allow us to walk from the top of the tree,
or from the current parameter. In TACHYON, we specify
readv as:

fd ,

const

s t r u c t

i n p u t o u t p u t

i n t

i o v c n t ) ;

1 s s i z e t

i o v e c i n ∗ i o v{ i o v c n t } ,

r e a d v ( i n t
i o v e c i n {

void i n p u t ∗ i o v b a s e {undo ( s e l f ) . i o v l e n } ;
s i z e t

i o v l e n ;

2 s t r u c t
3
4
5 }

Figure 2: A lookup in action

We now call the struct iovecin, because while both
readv and writev take an iovec, they are used dif-
ferently, and so are assigned differing types (speciﬁcally,
in one case the buffers inside are output, while in the
other they are input). The only new annotations com-
pared to before are undo and self, which are used to
walk the type tree to reference other ﬁelds. The seman-
tic meaning is that iov base is iov len bytes. self
refers to the location at which the current value is being
read from. undo simply says to step back along what-
ever indexing step was done to get there. In this case, this
means that self represents the tree traversal up through
that instance of iov base. The “undo” brings us up
a level, to be looking at the struct. Then, we index the
struct to iov len and are done. Figure 2 graphically
shows the type tree for readv and how the syntax ex-
presses ﬁelds in the tree.

3.2 The Tachyon Type System
The full TACHYON dependent type system is shown in
Figure 3, and is taken directly from the TACHYON source
code in Haskell. The language is similar to BNF, where
non-terminals are to the left of the equal sign, and brack-
ets denote a list (e.g., [A] is a list with elements of type
A).

In TACHYON’s language, IOC represents an IO class,
that is, whether the pointer is input, output, or both. T
represents some form of termination, to allow us to in-
clude null-terminated data. NT is for null-terminated
data; UT is for unterminated data. If a pointer is null-
terminated, reading will cease when a 0 is hit, if this hap-
pens prior to the end of the buffer. The index operation
is used on both arrays and structs, where the i’th index
refers to the i’th ﬁeld (counting from 0).

The types available are
• Small - These correspond to basic integer C types,

5

readvint fdconst struct iovec *iov{iovcnt}int iovcnt0nvoid *iov_base{undo(self).iov_len}size_t iov_lenselfundo(self)undo(self).iov_lenI nt
[ Type ]

1 data SysSig = SysSig Type [ Type ]
2 data Type = Small
| S t r u c t
3
| P t r
4
5 data T = NT | UT
6 data IOC = I n | Out
7 data Bound = Const
8 data Lookup = Ret

|
InOut
| Lookup Lookup
I nt
| Arg In t

IOC Type Bound T

|

| S e l f

| Undo Lookup

In d ex I nt Lookup

Figure 3: The TACHYON annotation language

like char or long, and indicate values that should
not be treated as pointers. The type parameter is
the number of bytes of the type, e.g., Small(1) is a
1-byte value corresponding to a char.
• Struct - an aggregate of other types. Note that pre-
vious replay work treated such types as raw buffers
because they could determine size by simply difﬁng
memory before and after a syscall. In live replay,
we explicitly lay out all ﬁelds because the underly-
ing types may yield further information.
• Ptr - a pointer annotated with an IO class, the type
of element it is pointing to, a way to tell how many
elements it points to, and whether or not it respects
a null termination convention.

We introduce the concept of a “lookup”. This is just
a series of steps that can be performed from either the
arguments of a function in the case of an input or in/out
class pointer, or the arguments and return value in the
case of an output pointer, to arrive at a memory location
or register. This is demonstrated in Figure 2. The Ret
and Arg constructs for a lookup are used to allow us to
reference the return value or various arguments in a sys-
tem call, respectively. This is just the generalization of
the tree walking described earlier.

Given this, the encoding of a bound as either a con-
stant or a lookup is rather natural. It is the use of this
bound that makes us lightly dependently typed—the type
depends on the data in question.

Finally, we can build the fundamental structure all this
is for—the system signature. A system signature, indi-
cated by SysSig, is what is assigned to each system call
in order to allow the tracer to record and play back its ef-
fects. The ﬁrst parameter is the type of the return value,
and the second is a list of the types of its arguments.

Type Checking
TACHYON Declarations. The
TACHYON types need only be written once for each
system call, and can be reused for any program. How-
ever, since they are written manually, we would like to
prevent mistakes.
In order to achieve this, TACHYON

6

also provides type-checking to make sure the annotations
make sense. In particular, TACHYON checks:

1. Bounds are numbers, not pointers or something

else.

raw data.

type.

2. Bounds use only information which is available for
the IO class of the pointer (e.g., input class may not
use the return value as a size).

3. Output pointers do not contain structure; they are

4. Types are potentially compatible with the original C

These checks ensure annotations which are usable, self-
consistent, and match the C type.

4 System Call Stream Deviation Detection

and Rewriting

Patches often add, delete, or modify new system calls
in the original buggy program. Our example in List-
ing 1 shows all three cases. When the streams of syscalls
differ, then the two programs are semantically different.
While this means we cannot automatically tell if the dif-
ferences are meaningful, we can (a) automatically detect
deviations and (b) rewrite deviations when informed by
the user that the semantic differences are permitted. The
heart of detection and rewriting is TACHYON’s syscall
stream matching and rewriting engine.

4.1 Stream Matching
TACHYON uses a rule-based system for rewriting system
call streams during execution, designed to be employed
by a user of the tracing software to explain to the system
what behaviors it should consider equivalent. The rules
must consume a sequence of system calls by P, and pro-
duce a corresponding set of system calls for P(cid:48) to make in
order to allow for writing call results into P(cid:48) and checking
that P(cid:48) indeed matches the particular equivalence rule.
As we execute, we have two streams of tuples.
TACHYON represents the stream from P as (cid:104)Ci,(cid:126)Ii, (cid:126)Oi(cid:105),
and the stream from P(cid:48) as (cid:104)C(cid:48)
i(cid:105). The easy case is
when the two programs are semantically equivalent by
issuing the same system calls, i.e., ∀i : Ci = C(cid:48)
i ∧(cid:126)Ii = (cid:126)I(cid:48)
i .
In this case no rule is needed, and TACHYON will send
the corresponding (cid:126)Oi for each (cid:126)Ii to P(cid:48).

i , (cid:126)O(cid:48)

i,(cid:126)I(cid:48)

Any time the syscall input arguments do not line up,
TACHYON reports a semantic deviation. In order to per-
mit some deviations, TACHYON provides the ability to
rewrite the system call stream. The rewrite engine takes
in a set of rewriting rules f . Each rewrite rule fk is a
i(cid:105). The rule
function which takes in (cid:104)Ci,(cid:126)Ii, (cid:126)Oi(cid:105) and (cid:104)C(cid:48)
uses pattern matching to decide if it applies, and if so,
returns a pair of equivalent syscall streams to perform a

i,(cid:126)I(cid:48)

substitution with. After a match, the stream continues to
be consumed by the simulated program P(cid:48).
The overall mechanism can be used for:
• Determining roughly equivalent syscalls, e.g., many
• Ignoring syscalls, e.g., the P program issues a call
• Limited reordering, e.g., allowing for syscalls to be

small writes being patched to be one big rewrite.
that is not needed by P(cid:48).

switched.

4.2 Rewriting Rules
Each rewrite rule f takes a system call (the one made by
P) and the input to a potential system call made by P(cid:48),
and returns a substitution in the stream. The substitution
is implemented as a pair of lists, where the left list indi-
cates the syscalls consumed by the rule, and the right list
indicates the corresponding substitution produced by the
rule. The type signature for f in TACHYON is:

Syscall → SysReq → Maybe ([Syscall], [Syscall])

where the “Maybe” indicates that the rule may also re-
turn that no substitution was performed.

The rewriting rules are pure functions, which means
they have no access to outside resources like the current
syscall stream or application state. By being pure we
ensure that rewrite rules can be applied in any order. In
addition, it ensures that the rule engine itself will not con-
tinually accumulate state, i.e., while individual rewrites
may take substantial space, the space used will remain
constant in the number of system calls which have gone
through, which is vital to an online system.

During execution, the matching engine maintains a
queue of syscalls executed by the live program P. Sup-
pose the queue contains any syscall x that is not write,
but the simulated program P(cid:48) issues a write syscall.
The simplest rule is to ignore the write. This is accom-
plished by adding a write to the queue before x. When
the matching engine re-examines the queue, it will match
the still-pending write to the one in the queue, and not
report a deviation.

In TACHYON, the rule is written as:

is on the stream at the moment, and replace it on line 6
with the stream of Write followed by x. This can be
thought of as “faking” the call for Write to the stream
matcher so that it does not report a deviation. On line 7,
we catch the case where our conditions are not met, and
indicate we did not modify the stream.

The simple, no-look-behind method of replaying with
this equivalence is to replay the stream normally until a
match fails. At this point, the two syscalls that failed
to match are fed into all rewrite rules, and their replace-
ment list for the original stream is checked. If there is
still more than one rewrite rule remaining, one is chosen
arbitrarily. In future work, checkpointing could be used
here to allow for the ability to rewind if the wrong re-
placement was chosen. In practice, the rules we tested
have only yielded one matching rewrite.

A more complex example is what we call write split-
ting, which occurs when a larger write in the original
program is translated into two smaller writes in the re-
played program. This is useful if the buffer size used in a
transmission was decreased during the patch, as it allows
for a roughly equivalent operation—writing one part of
the message, then the other—to be treated the same as the
original system call writing the entire message. A con-
crete example would the difference between the program
fragments:

1 −# d e f i n e CHUNK 4096
2 +# d e f i n e CHUNK 1024
3 w h i l e ( buf < end ) {
4

CHUNK) ) ;

5

}

buf += w r i t e ( fd , buf , min ( end − buf ,

In the patched case here, we will see on average 4
times as many system calls, but fundamentally, the same
thing is happening. A rewriting rule for write splitting
says that a sequence of previous writes can be used to ﬁll
on big write request:

1 w r i t e S p l i t
2
3
4 w r i t e S p l i t

: : S y s c a l l
−> SysReq
−> Maybe ( [ S y s c a l l ] ,
( S y s c a l l

( Wr it e f d buf

−−s z

i s

r e t u r n s i z e

[ S y s c a l l ] )
s z )

sz0 )

: : S y s c a l l
−> SysReq
−> Maybe ( [ S y s c a l l ] ,
s z ) =

1 i g n o r e W r i t e
2
3
4 i g n o r e W r i t e x ( Wr it e 2 buf
5
6
7 i g n o r e W r i t e

( [ x ] ,
[ S y s c a l l

( Wr it e 2 buf

= Nothing

Just

[ S y s c a l l ] )

s z )

sz , x ] )

This rule ﬁres when line 4 is matched. This occurs
when P issues a syscall x that doesn’t match P(cid:48)’s syscall
write. On line 5 the rule directs it to consume whatever

|

5
6
7
8
9
10
11
12
13
14 w r i t e S p l i t

= Just

( Wr it e fd ’ buf ’

sz ’ )

( f d == fd ’ )

&& ( sz ’ < s z )
&& ( ( take sz ’ buf ) == buf ’ )

( [ S y s c a l l
[ S y s c a l l
S y s c a l l

( Wr it e f d buf
( Wr it e fd ’ buf ’
( Wr it e fd ’

sz0 )

s z ] ,

sz ’ )

sz ’ ,
( drop sz ’ buf )
( sz0 − sz ’ ) )

( s z − sz ’ ) )

= Nothing

7

This rule states that if we have a write call in the orig-
inal stream, and the replayed program is trying to make
a non-matching write call, but it matches on the ﬁle de-
scriptor, and has a smaller size, and the write it is trying
to make is a preﬁx of the original write, then we can re-
place the original write with two smaller writes, the ﬁrst
of which is the target write, and the second of which is
set up to represent the rest of the write.

In line 6, we do a sanity check that the ﬁle descrip-
tors we are writing to are the same, followed by a similar
check in line 7 that the request from P(cid:48) has a smaller size
than the original form P. Finally, in line 8 we make sure
that what is trying to be written is a preﬁx of the appropri-
ate length of the original write. Given these conditions,
we know that we can provide a replacement rule which
will allow the trace to proceed. In line 9, it tells its caller
to consume the most recent call, asserting that it matches
the call passed into us. In line 10, we see the ﬁrst call that
is going to end up on the new stream, which matches the
input vector we’ve received from P(cid:48), and so will allow the
trace to continue. In the 11,12, and 13, the function re-
turns an additional item for the system call stream, which
represents the rest of the write that has been split. In 14,
we catch the case where we don’t apply, and simply re-
turn no pattern.

5 Architecture

TACHYON is built to target Linux for the x86-64 architec-
ture via the ptrace call, written in Haskell. An abstrac-
tion barrier is in place around ptrace, to ensure the
technique’s generality and portability to other systems
with similarly powered tracing libraries. Haskell was
chosen for abstract data type support, multi-OS portabil-
ity, relative speed, and a monadic abstraction layer that
proved useful for our tracing environment.

ptrace TACHYON uses the ptrace system call to
accomplish system interposition. Use of ptrace starts
with initialization, in which options are set and the re-
mote process either volunteers itself for tracing, or is
traced via a command to attach to its pid. Then, wait
and wait4 are used in an event loop to get status infor-
mation about processes (which are paused when gener-
ating one of these messages) and are then resumed at a
later time.

The wakeup and sleep powers are implemented by se-
lectively choosing to not resume or resume threads at
system call boundaries. While this only enables us to
support putting the currently running thread to sleep, we
never needed to stop any thread for which we were not
currently processing an event.

Trace Abstraction. The trace abstraction layer is de-
signed to expose only primitives we believe to be con-
structible on all platforms for portability purposes. Ad-
ditionally, the interface was higher level than the tracing
interface directly available on most platforms, enabling
easier authoring of the code. Fundamentally, a handler
is provided for events which the tracing interface detects
and sends back. The potential events currently supported
are pre/post syscall, and process split. Available to the
callback is the ability to put threads to sleep, wake them
up, read and write registers, and read or write memory in
the target process.

Multithreading tolerance. Up until now, we have
considered only programs using one thread. However,
many modern programs use multiple threads in their nor-
mal operation. For example, curl in § 6 uses them dur-
ing DNS lookup.

To deal with threads, TACHYON employs techniques
inspired by the ﬁeld of deterministic multithreading
(DMT)[2, 3, 15]. A DMT mechanism is one that makes a
program insensitive to the scheduler as an input. That is,
given the same inputs other than kernel scheduler action,
it will yield the same outputs.

In TACHYON, we enforce an ordering over system call
events. We differentiate from a mismatched system call
in need of rewriting and a thread being early or late by
choosing to block the thread if the thread IDs on the
syscalls don’t match, and invoke the rewrite engine if
they do match, but the system calls don’t. This forms a
looser notion of consistency than is used in regular DMT,
but is sufﬁcient for our purposes. However, applying a
real DMT system in addition to our techniques would
likely yield an even more robust treatment of threading
able to deal with shared memory data transfers and other
such intricacies, as we discuss in § 8.

Special Syscalls. While in general the simulated ap-
plication P(cid:48) uses the effects of syscalls from the live P,
TACHYON does have a few exceptions. The exceptions
occur when a syscall result from P cannot be emulated in
P(cid:48). This usually occurs when there is something which
is part of the thread life cycle or virtual memory system,
which are not facilities that can be directly accessed by
TACHYON. Luckily, there are only a limited number of
cases.

The ﬁrst is sbrk(), which is the syscall responsible
for dynamic memory allocation. Luckily, this particu-
lar call can be passed through, as it does not modify OS
state, it only serves to modify the process’s VM system.
When a clone() occurs, we match input arguments
like any other system call, and then allow it through.
ptrace feeds us an event notifying us as soon as the
new thread exists, and pauses that thread before it can do

8

anything so that we can attach to it. This event is also part
of the synchronized system call stream. TACHYON then
registers that new thread and its pair between emulated
tid and real tid, and proceeds with normal operation.

The exit group() system call works like any
other, except that it acts as an end-of trace marker. We
currently only accept full application exit, in which all
threads are simultaneously terminated, but there is no
reason our techniques could not be extended to individ-
ual thread destruction.

The most complicated is mmap(), which maps a ﬁle
or device into memory. Our implementation depends
upon the operations performed on the region. The read-
/write case would be extremely expensive to monitor, as
all writes are effects, so we would have to interpose on
every memory write to that page, and so this case is disal-
lowed entirely. It might be possible to deal with writable
mapped ﬁles or shared regions shared outside the pro-
gram, using page faults and slow execution, or some
form of snapshot trick. We leave this as future work.

In the read only case however,

the translation is
straightforward. Rather than issuing the mapping as re-
quested, we instead simply ask for a private mapping
of the same size this thread received during recording,
and ﬁll that buffer with the contents that memory con-
tained after the initial map. Private mappings of anony-
mous memory are also easy to support, and are be simply
passed through. Finally, in the case of shared memory,
we can allow it if it is both anonymous and our multi-
threading system is in place. This could be extended to
allow for non-anonymous shared memory by spawning
fake ﬁle descriptors to the region, but is left as future
work.

Other system calls for which we would need to add
special implementation to allow them to be serviced by
the kernel, but still safely matched include munmap,
mprotect, fork, sigaction, sigreturn, and
exit. These were unneeded for our test cases and were
not implemented.

6 Evaluation

We have evaluated TACHYON with respect to three main
questions. First, can TACHYON detect deviations where
the patched program is semantically different than the
unpatched, and how hard is it to write rules to ignore de-
viations that do not matter? Second, what is the perfor-
mance factors for TACHYON, including best and worst
case settings? Third, what is the performance on real
programs? In this section, we describe our results.

9

Program

Issue ID

Description

cURL

CVE-2011-2192

Improper key delegation

mplayer

EDB-ID 11792

Table index out of bounds

Bad Argument handling

Buffer Overﬂow

Buffer overﬂow

Buffer overﬂow

Buffer Overﬂow

Buffer Overﬂow

php5

php5

CVE-2012-0832

CVE-2011-1938

ncompress

CVE-2001-1413

CVE-2004-0852

CVE-2010-1869

EDB-ID-476

htget

gs

glftpd

socat

CVE-2004-1484

Format String

corehttp

CVE-2009-3586

Off-by-one Buffer

Figure 4: Successfully Detected Deviations for Security
Patches

6.1 Detecting Deviations
To test the effectiveness of TACHYON, we used it on
real patches to detect known deviations. The patches
we tested are shown in Figure 4. In this experiment, we
tested the program on normal inputs, and veriﬁed that
TACHYON did not report a deviation. We then tested on
inputs that triggered known deviations, e.g., exploits in
the original program or bugs in the patch.

For cURL, the patched vulnerability was an informa-
tion disclosure bug. In the unpatched version, Kerberos
credentials were (accidentally) forwarded instead of just
a proof the user was authorized. We veriﬁed that the un-
patched program would send credentials, and the patched
program did not. In order to test the patch and allow nor-
mal operation of safe inputs, we had to write two rules
for cURL that totaled 11 lines. The rules were necessary
because cURL added a non-security feature that affected
ﬁle descriptors in their patch.

CVE-2011-4885 addresses a problem in PHP where
hash collisions are easy to ﬁnd, which can be used to
launch a remote denial of service attack. TACHYON re-
quired no rewrite rules to run the patch on safe inputs.
The patch, however, broke the argument handling for ar-
rays after loading many arguments. CVE-2012-083 ad-
dressed this problem. For CVE-2012-083, we again re-
quired no rewrite rules for safe inputs.

CVE-2011-4885 and CVE-2012-0832 demonstrate
a patch that
is broken, and provide motivation for
TACHYON. Since CVE-2011-4885 ﬁxed a purported vul-
nerability, it should be applied immediately. However,
after applying CVE-2011-4885, a new vulnerability is in-
troduced. TACHYON detects those new exploits as devi-
ations immediately.
In particular, we checked exploits
(addressed in CVE-2012-0832) that were unknown in

2011, and veriﬁed that they caused detected deviations.
Thus, if an administrator had been running TACHYON,
and immediately applied the patch, they would detect ex-
ploits immediately against the vulnerability introduced.
The EDB-ID 11792, CVE-2001-1412, and CVE-
2004-0852 all patch typical security bugs by adding in-
line checks. These checks did not change the system call
pattern or arguments, thus no rules were needed for patch
testing.

For CVE-2010-1869, gs’s memory problems required
a rewrite rule to admit additional or skipped calls to brk.
8 lines were required for these rewrite rules. Three lines
were required for EDB-ID-476 to allow for rewriting of
the format of a usage message. Four were required to
deal with the new lstats in the patch for CVE-2009-
3586.

6.2 False Positive Testing
To show that TACHYON is fairly precise, we tested it on
the most recent 207 patches to coreutils. (The number
207 was chosen because that was how far backwards
we could go easily with an automated building system.)
From this, we found that in 18 cases out of 1656 exe-
cutions, a deviation was reported, or TACHYON crashed.
Looking at the output, 16 of these were TACHYON bugs,
but are not systematic, so that re-running the test pro-
duced correct results. 2 of these were actual deviations.
In the ﬁrst, a call to fadvise() was introduced into
cp. An equivalence can be reached with a one-line
rewrite rule. In the second, a buffer size was changed.
The read/write splitting/coalescing rules described ear-
lier in this paper allow an equivalence to be reached.
Overall, this indicates that while TACHYON is not per-
fectly bug free, it never reported a deviation when one
had not happened, and deviations that should be accept-
able could be easily expressed in the rule system.

We also ran TACHYON on patches for two common
utilities with no known vulnerabilities: /bin/ls and
/bin/cat, and used them interactively.
In the one
month testing period, TACHYON was able to use tan-
dem execution on these utilities for normal day-to-day
use with no perceived slowdown. Further, TACHYON re-
ported no deviations (i.e., had no false positives).

6.3 Micro-benchmarks
TACHYON has three main sources of overhead: our ap-
proach to syscall interposition, transferring bytes from
the source to the sync application, and running both P(cid:48)
and P in tandem. Overall we measured a linear overhead
for both data transfer and system calls, and 0% of CPU
time, as detailed below.

10

Syscall interposition overhead.
TACHYON is a user-
space system call interposition scheme, which imposes
additional context switches but provides for a clean sep-
aration of interposition, kernel, and user-space applica-
tion. Our user-space interposition has 4 context switches
per call issued by the live application P. For each syscall
in P, TACHYON context switches from P to the kernel,
from the kernel to TACHYON, from TACHYON back to
the kernel, and ﬁnally back to P. Normal operation only
has two context switches: from user to kernel space, and
back again.

In order to test the effect of these two extra switches,
we wrote a simple program that executed getpid() in
a tight loop. We were sure to call the system call di-
rectly, as the standard version of getpid() in C actu-
ally caches its result.

Data copy overhead.
TACHYON needs to copy the out-
put data from P to P(cid:48). A ﬁrst naive implementation actu-
ally incurred 6 copies. TACHYON originally copied out-
put data from P into TACHYON for syscall rewriting, and
then copied it to P(cid:48). Each copy between systems was ac-
tually two copies: one from the user-space into kernel
space, and one from kernel-space into user-space. This
lead to a huge slowdown in early benchmarks (over 6x).
To reduce this, we added the ability to the Linux kernel to
map a remote process’s memory via /proc/pid/mem
under most situations, making the normal case of reading
from the remote process only have one copy.

In Figure 5, we see the overhead is in a linear relation-
ship. The overhead here is the total overhead time for
the system. While they make a difference for small data
transfers, they are rapidly dominated. This can be seen
by the rapid transition to a tight grouping around a linear
relationship.

In Figure 6, we again observe a nice linear relation-
ship, showing no residual effects on performance from
processing a system call. It shows that it takes less than
a second of overhead to process 60,000 syscalls.

When varying the CPU load of the traced program,
no noticeable difference in execution time was noticed,
as we do not intercept regular computation, only system
calls.

Given this, if it is known how many system calls are
used, how much data is being transferred, and how much
time is being spent on the CPU, we can model how long
a given workload would take under our tracer. TACHYON
previously incurred a large number of copies and control
transfers to move buffers around in comparison with the
register fetching it does for simple system calls, and the
remote memory fetch path is not optimized in the OS.
However, a kernel patch allowing for mmap to be used
on the special ﬁle /proc/pid/mem considerably ame-
liorate this, resulting in the new statistics above.

Figure 5: Data Transfer Overhead

Figure 6: Syscall Overhead

Tandem execution CPU time. The ﬁnal source of
overhead is in CPU operations. Since TACHYON sleeps
when system calls are not being issued, it does not slow
down applications that are CPU-intensive. This is a
signiﬁcant advantage over instruction-level interposition
tools such as Pin [16] and Valgrind [21], which typically
suffer at least several times overhead.
However, we are running both P(cid:48) and P. We veriﬁed
that TACHYON can utilize independent cores to run both
programs with no additional overhead (other than the
memory transfer and syscall overhead measured above).
Thus, we conclude that TACHYON can utilize multi-core
to test patches.

Program

Load

TACHYON

strace

compress

32M Random

primegaps

mencoder

First 35
h264

1.41
1.00
1.07

19.78
1.00
1.12

Figure 7: Tracing Performance (relative to native execu-
tion)

improved facility for retrieving remote memory via our
patch to mmap /proc/pid/mem.

Efﬁciency on real programs. To test the efﬁciency
of TACHYON interposition, we measured its tandem ex-
ecution against strace and gdb’s reverse execution.
strace is a tool built on top of ptrace used to mon-
itor system calls. gdb allows programs to back-step
through operations.1 Note these tools have different
goals than TACHYON; we only use them to evaluate per-
formance.

gdb’s replay mechanism derived from its reversible
debugging support. However, it proved wholly unsuit-
able for regions of more than a few instructions. Due to
a lack of SSE support, memcpy would be improperly re-
wound and replayed. Additionally, even then the record-
ing overhead was more than 100x native execution, and
built up a huge memory data structure, making it imprac-
tical to benchmark.

The results compared to strace are shown in Fig-
ure 7. Overall, TACHYON was faster, sometimes by
a large margin, than a comparable syscall interposition
scheme. This is partially because TACHYON can and
does process some of its overhead while the traced pro-
gram is doing work, but mostly due to the massively

1We were unable to test against what is likely the most similar sys-
tem, R2 [11], as it is both Windows only and requires build-time sup-
port (as well as not being public).

11

Web Server Tests. We also tested the throughput
of lighttpd and thttpd when monitored under
TACHYON. In this test, we use ApacheBench (ab) conﬁg-
ured to make 1000 requests in two threads, downloading
4096 byte web page. We ran the experiment in two sce-
narios: on localhost and across the Internet.

In the network experiment, the web servers ran at one
university, and requests were made from another univer-
sity on the opposite US coast. There was no detectable
degradation for thttpd, and only about a 30% slow-
down for lighttpd. Essentially, what this shows is
that while the system does not deal well with applica-
tions whose progress is primarily based on the rate at
which they can issue system calls, when we move closer
to a real deployment, applications do not tend to have
that as their primary limiting factor.

In the second experiment, we ran ab on localhost. This
is a worst-case test because both web servers spin in a
tight loop on a syscall (lighttpd spins on epoll,
and thttpd on poll). This creates a pathological case
for TACHYON, because the application spends most of
its time neither doing IO, nor doing computation, but in-
stead spends most of its time moving across the system
call barrier.

TACHYON took 8.9 times longer on lighttpd

 0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0 2000 4000 6000 8000 10000 12000Overhead (seconds)Data Transferred (4 KB Chunks)Data Transfer OverheadTransfer OverheadLeast Squares Fit 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 10 20 30 40 50 60Overhead (seconds)Thousands of System CallsSyscall OverheadSystem Call OverheadLeast Squares Fit(throughput decreased to 10% of original values) and
14.2 times longer on thttpd (throughput decreased to
7% of original values).

To round things out, we also ran a test over a few hops
on the local network. As expected, intermediate results
were measured to be between the two, with 1.17 times
untraced completion to complete the test with thttpd,
and 3.72 times untraced completion to complete the test
with lighttpd.

With a more real network (or a more complicated we-
bapp), we can see the slowdown is lessened. With a real
network, epoll will spend more time waiting, dimin-
ishing the perceived effects.

7 Discussion

Other Patch Testing Scenarios. While throughout
this paper we have focused on online patch testing where
the patched version is run live, we could also run the
unpatched version live. We note that the live program
can continue executing after a deviation, but currently
the syscall sync application cannot. Thus, by running the
patched live, we are assuming that after a deviation the
right thing is to continue executing the patched version.
However, by running the unpatched version live, we can
check for incompatibilities while allowing for the origi-
nal program to continue executing after a deviation.

Honeypots.
TACHYON can also be used as a type of
lightweight honeypot. Let P be a patch for a security
vulnerability, and P(cid:48) be the vulnerable program. Observe
that P and P(cid:48) differ on exploits by deﬁnition. By running
P and P(cid:48) in-tandem, TACHYON will report a deviation on
attacks.
A clever approach to running a honeypot is to run P as
the live program, with P(cid:48) as the sync. In this setting an
attacker only seeing the buggy program. TACHYON will
report attacks, e.g., by logging a deviation when shell-
code tries to execute /bin/sh. However, the system is
safe from a real compromise because TACHYON can be
conﬁgured to abort execution after the deviation.

Debugging. One of the most difﬁcult to debug classes
of bugs is commonly known as heisenbugs. These are
bugs which will seemingly randomly occur or not occur
with all of the inputs the programmer knows about held
constant. These traces, and the associated replay mech-
anism, provide a way to step through the program in a
completely deterministic way, so that once a heisenbug
has been caught with tracing on, it has been captured and
the sequence leading to it can be carefully explored and
debugged. As we capture all inputs, this also makes it
possible for the programmer to debug a crash that took

place on another machine, without having to try to repli-
cate the OS state to reproduce the crash.

Efﬁciency. Recall TACHYON uses user-land syscall in-
terposition, and has our approach to syscall interposition
as its primary source of overhead Currently, interposing
on each system call on the live program requires 4 con-
text switches. TACHYON context switches from P to the
kernel, from the kernel to TACHYON, from TACHYON
back to the kernel, and ﬁnally back to P. Normal op-
eration only has two context switches: from user to ker-
nel space, and back again. A kernel-space interposition
scheme would also have only two switches.
Recall from § 6 the overhead from copying data is al-
most linear in the amount data transferred between P and
P(cid:48). A basic in-kernel approach would still have a linear
overhead (since data has to be copied into both virtual
memory spaces), but likely with a smaller constant fac-
tor.

Our user-land approach was chosen because it offers a
clean separation of functionality, isn’t kernel dependent,
and offers an easier development environment. Moving
the system call interposition into the kernel would not
have these advantages, but would likely improve perfor-
mance. We leave further study of in-kernel tandem exe-
cution schemes as future work.

8 Limitations and Future Work

TACHYON could be extended to provide better deter-
minism for shared memory. At the moment, because
TACHYON does not schedule individual memory opera-
tions, multithreaded programs which have concurrency
bugs could run differently under TACHYON.
(Non-
concurrency bugs are not a problem.) One approach
would be to incorporate recent advances in DMT, e.g.,
[2, 3, 8, 15] into TACHYON. This would also allow for
effectful shared memory.
TACHYON currently

support
virtual
not
dynamically-linked shared objects
(vDSO), a sec-
tion of kernel memory mapped into the user-space
process to allow for more efﬁcient calls. Unfortunately,
some system calls made through a vDSO do not trigger
the ptrace trap. However, vDSOs are known to
provide increased efﬁciency, so being able to trap that
interface could be an improvement, and limit host
system modiﬁcation.

does

9 Related Work

Our approach is motivated by existing replay systems. At
a high level, previous work in this area has focused on
system call replay (e.g., [11, 22]), virtual-machine level

12

[12, 17, 24]), and instruction-level replay
replay (e.g.,
schemes (e.g., [1, 10, 22]). These systems address the
related but different problem of replay against the same
binary, e.g., for debugging, while we want to replay to a
different binary for patch testing.

Delta Execution [23] uses a similar insight to us for
testing, namely that patches tend to change very little,
and the majority of the program should remain the same.
To accomplish this, they structure execution so that it
splits every place in which the patch modiﬁed the code,
and attempts to merge the execution afterwards, check-
ing that the overall state change during the split matched
appropriately. Their approach has the additional advan-
tage of avoiding duplicate computation. TACHYON dif-
ferentiates itself from this work primarily in its general-
ity; speciﬁcally, it works at a binary-only level, it allows
matching global effects (e.g. heap changes and IO), can
be conﬁgured to allow speciﬁc non-matching global ef-
fects, and allows for structure size changing. Fundamen-
tally, Delta Execution attacks the problem at the level of
matching internal state, while TACHYON attacks it from
the point of view of observational equivalence from the
outside world.

Where Delta Execution places their consistency level
inside the application state, which is more speciﬁc than
us, Capo [20] attacks it from the point of what signals are
coming into and going out of the computer. This wins
them several beneﬁts, namely the ability to deal with
fewer effects and a lesser need for a rewrite or match-
ing system (for example, coalescing or splitting reads or
writes is free). Unfortunately, this system also needs spe-
cialized hardware and in-kernel code to operate. While
we have used kernel code to accelerate TACHYON under
Linux, it is not required or fundamental to the technique.
Like TACHYON, R2 [11] designed a type system to
express all side-effects, but for the purpose of describ-
ing intercepted APIs at the source level. R2 [11] dif-
fers from TACHYON in that it targets developers, inter-
poses at the function level (thus requires source), and
replays recorded syscalls against the same binary. Al-
though the problem setting is different, there are numer-
ous good ideas that could be borrowed for live replay if
source was available. For example, R2 proposed analyz-
ing the call graph to ﬁnd efﬁcient cut points at which to
perform interposition, while we always interpose at ev-
ery syscall. Unfortunately, we at the binary only level
cannot easily prove that a set of interposition points form
a complete cut. Additionally, R2’s annotation language
is actually too powerful, and allows for the expression
of types that we cannot appropriately interact with at
a binary-only level without compile-time assistance, as
it allowed for the computation of arbitrary expressions.
We instead limited ourself to navigating trees of point-
ers, which turns out to be sufﬁcient for the vast majority

of system calls.

We record system calls and discover divergences when
system call requests do not line up. Another approach
would be to perform replay at the instruction level, which
would be useful for pinpointing the ﬁrst point of diver-
gence. This could be done by augmenting instruction-
level replay systems like PinPlay [22], and gdb[10] 7.0
and above to take into account differences in memory
layout. In undodb [1], memory snapshots and individ-
ually optimized system calls are used to accomplish re-
verse execution.

Alternatively, one could utilize VM playback mech-
anisms [9, 12, 24] to simulate patches at the whole-
machine level. However, testing then requires accurately
replaying very low level events. We chose system calls
over instruction level or whole-machine level because
system call interposition is signiﬁcantly cheaper, thus
more amenable to end-user deployment scenarios. Addi-
tionally, rewriting system calls is much more reasonable
than rewriting low level events.

Another recent idea in interacting with multiple pro-
grams which should meet the same speciﬁcation, simi-
lar to our patched and unpatched pair, is the idea of N-
Variant systems [7]. It intends to increase reliability by
forcing any exploit or otherwise bad input sent as input
to cause the same bad effect in other versions of the soft-
ware in order to actually occur. There are some similari-
ties here, but our techniques are aimed at fundamentally
differing process images, while theirs are aimed at the
same underlying code put together in different ways.

Our system does not ﬁnd new inputs for patch testing.
While we assume live or recorded inputs, one could re-
play both systems on automatically generated inputs as
well. For example, we could use test cases produced by
automated systems such as KLEE [6], BitBlaze [4], and
BAP [13].

Brumley et al. have previously proposed deviation de-
tection at the binary level [5]. The main goal in this work
is to automatically generate likely deviations, which is a
different problem than tandem execution. Once a candi-
date deviation is generated, the deviation was manually
validated (Section 3.3 [5]). Our approach could be used
to validate deviations automatically at the syscall level.

10 Conclusion

In this paper, we presented TACHYON, a system for test-
ing binary-only patches on real inputs in a live sys-
tem. We have demonstrated an efﬁcient way to de-
scribe interface boundaries on C-style declarations using
a lightweight dependent type system. Our experiments
show TACHYON is able to automatically detect deviations
on real programs. We also suggest our techniques may
apply to other problem domains, such as honeypots.

References
[1] undodb. http://undo-software.com, Nov. 2011.

[2] AVIRAM, A., WENG, S.-C., HU, S., AND FORD, B. Efﬁcient
system-enforced deterministic parallelism. In Proceedings of the
9th USENIX conference on Operating systems design and imple-
mentation (Berkeley, CA, USA, 2010), OSDI’10, USENIX As-
sociation, pp. 1–16.

[3] BERGAN, T., ANDERSON, O., DEVIETTI, J., CEZE, L., AND
GROSSMAN, D. Coredet: a compiler and runtime system for de-
terministic multithreaded execution. SIGARCH Comput. Archit.
News 38 (March 2010), 53–64.

[4] BitBlaze binary analysis project. http://bitblaze.cs.

berkeley.edu, 2007.

[5] BRUMLEY, D., CABALLERO, J., LIANG, Z., NEWSOME, J.,
AND SONG, D. Towards automatic discovery of deviations in
binary implementations with applications to error detection and
ﬁngerprint generation. In Proceedings of the USENIX Security
Symposium (Boston, MA, Aug. 2007).

[6] CADAR, C., DUNBAR, D., AND ENGLER, D. Klee: Unassisted
and automatic generation of high-coverage tests for complex sys-
In Proceedings of the USENIX Symposium on
tems programs.
Operating System Design and Implementation (2008).

[7] COX, B., EVANS, D., FILIPI, A., ROWANHILL, J., AND HU, W.
N-variant systems: A secretless framework for security through
In USENIX Security Symposium (2006), no. August,
diversity.
pp. 1–16.

[8] CUI, H., WU, J., GALLAGHER, J., GUO, H., AND YANG, J. Ef-
ﬁcient deterministic multithreading through schedule relaxation.
In Proceedings of the Twenty-Third ACM Symposium on Operat-
ing Systems Principles (New York, NY, USA, 2011), SOSP ’11,
ACM, pp. 337–351.

[9] DUNLAP, G. W., KING, S. T., CINAR, S., BASRAI, M. A., AND
CHEN, P. M. Revirt: Enabling intrusion analysis through virtual-
machine logging and replay. In In Proceedings of the 2002 Sym-
posium on Operating Systems Design and Implementation (OSDI
(2002), pp. 211–224.

[10] FSF. Documentation for GDB, 7.3.1 ed., Sept. 2011.

[11] GUO, Z., WANG, X., TANG, J., LIU, X., XU, Z., WU, M.,
KAASHOEK, M. F., AND ZHANG, Z. R2: An application-level
kernel for record and replay. In OSDI (2008), pp. 193–208.

[12] HERROD, S. The amazing vm record/replay feature in vmware
http://communities.vmware.com/

workstation 6.
community/vmtn/cto/steve/blog/2007/04/18/
the-amazing-vm-recordreplay-feature-in-
vmware-workstation-6, Apr. 2007.

[13] JAGER, I., AVGERINOS, T., SCHWARTZ, E., AND BRUMLEY,
D. BAP: A binary analysis platform. In Proceedings of the Con-
ference on Computer Aided Veriﬁcation (2011).

[14] LANE, B. A. The USAF standard desktop conﬁguration (SDC).

download.microsoft.com, June 2007.

[15] LIU, T., CURTSINGER, C., AND BERGER, E. D. Dthreads: efﬁ-
cient deterministic multithreading. In Proceedings of the Twenty-
Third ACM Symposium on Operating Systems Principles (New
York, NY, USA, 2011), SOSP ’11, ACM, pp. 327–336.

[16] LUK, C.-K., COHN, R., MUTH, R., PATIL, H., KLAUSER, A.,
LOWNEY, G., WALLACE, S., REDDI, V. J., AND HAZELWOOD,
K. Pin: Building customized program analysis tools with dy-
namic instrumentation. In Proceedings of the ACM Conference
on Programming Language Design and Implementation (June
2005).

[17] MAGNUSSON, P. S., CHRISTENSSON, M., ESKILSON, J.,
FORSGREN, D., H ˚ALLBERG, G., H ¨OGBERG, J., LARSSON, F.,
MOESTEDT, A., AND WERNER, B. Simics: A full system sim-
ulation platform. Computer 35 (February 2002), 50–58.

[18] MELL, P., BERGERON, T., AND HENNING, D. Creating a Patch
and Vulnerability Management Program. National Institution of
Standards and Technology, Nov. 2005.

[19] MICROSOFT. Microsoft security intelligence report vol. 11. Tech.

rep., Microsoft, 2011.

[20] MONTESINOS, P., HICKS, M., KING, S. T., AND TORRELLAS,

J. Capo. ACM SIGPLAN Notices 44, 3 (Feb. 2009), 73.

[21] NETHERCOTE, N., AND SEWARD, J. Valgrind: A program su-
pervision framework. In Proceedings of the Third Workshop on
Runtime Veriﬁcation (Boulder, Colorado, USA, July 2003).

[22] PATIL, H., PEREIRA, C., STALLCUP, M., LUECK, G., AND
COWNIE, J. Pinplay: a framework for deterministic replay and
reproducible analysis of parallel programs. In Proceedings of the
8th annual IEEE/ACM international symposium on Code gener-
ation and optimization (New York, NY, USA, 2010), CGO ’10,
ACM, pp. 2–11.

[23] TUCEK, J., XIONG, W., AND ZHOU, Y. Efﬁcient online vali-
dation with delta execution. ACM SIGPLAN Notices 44, 3 (Feb.
2009), 193.

[24] XU, M., MALYUGIN, V., SHELDON, J., VENKITACHALAM,
G., WEISSMAN, B., AND INC, V. Retrace: Collecting execution
trace with virtual machine deterministic replay. In In Proceed-
ings of the 3rd Annual Workshop on Modeling, Benchmarking
and Simulation, MoBS (2007).

14

