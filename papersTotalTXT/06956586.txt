2014 IEEE Symposium on Security and Privacy

Quantifying Information Flow

for Dynamic Secrets

Piotr Mardziel,‚Ä† M√°rio S. Alvim,‚Ä° Michael Hicks,‚Ä† and Michael R. Clarkson‚àó

‚Ä†University of Maryland, College Park
‚Ä°Universidade Federal de Minas Gerais

‚àóGeorge Washington University

Abstract‚ÄîA metric is proposed for quantifying leakage of
information about secrets and about how secrets change over
time. The metric is used with a model of information Ô¨Çow for
probabilistic, interactive systems with adaptive adversaries. The
model and metric are implemented in a probabilistic program-
ming language and used to analyze several examples. The analysis
demonstrates that adaptivity increases information Ô¨Çow.

Keywords‚Äîdynamic secret, quantitative information Ô¨Çow, prob-

abilistic programming, gain function, vulnerability

I.

INTRODUCTION

Quantitative information-Ô¨Çow models [1]‚Äì[5] and analy-
ses [6]‚Äì[9] typically assume that secret information is static.
But real-world secrets evolve over time. Passwords, for exam-
ple, should be changed periodically. Cryptographic keys have
periods after which they must be retired. Memory offsets in
address space randomization techniques are periodically regen-
erated. Medical diagnoses evolve, military convoys move, and
mobile phones travel with their owners. Leaking the current
value of these secrets is undesirable. But if information leaks
about how these secrets change, adversaries might also be able
to predict future secrets or infer past secrets. For example, an
adversary who learns how people choose their passwords might
have an advantage in guessing future passwords. Similarly, an
adversary who learns a trajectory can infer future locations.
So it is not just the current value of a secret that matters,
but also how the secret changes. Methods for quantifying
leakage and protecting secrets should, therefore, account for
these dynamics.

This work initiates the study of quantitative information
Ô¨Çow (henceforth, QIF) for dynamic secrets. First, we present
a core model of programs that compute with dynamic se-
crets. We use probabilistic automata [10] to model program
execution. These automata are interactive: they accept inputs
and produce outputs throughout execution. The output they
produce is a random function of the inputs. To capture the
dynamics of secrets, our model uses strategy functions [11]
to generate new inputs based on the history of inputs and
outputs. For example, a strategy function might yield the GPS
coordinates of a high-security user as a function of time, and
of the path the user has taken so far.1

Our model includes wait-adaptive adversaries, which are
adversaries that can observe execution of a system, waiting

until a point in time at which it appears proÔ¨Åtable to attack.
For example, an attacker might delay attacking until collecting
enough observations of a GPS location to reach a high conÔ¨Å-
dence level about that location. Or an attacker might passively
observe application outputs to determine memory layout, and
once determined, inject shell code that accesses some secret.

Second, we propose an information-theoretic metric for
quantifying Ô¨Çow of dynamic secrets. Our metric can be used to
quantify leakage of the current value of the secret, of a secret
at a particular point in time, of the history of secrets, or even of
the strategy function that produces the secrets. We show how
to construct an optimal wait-adaptive adversary with respect to
the metric, and how to quantify that adversary‚Äôs expected gain,
as determined by a scenario-speciÔ¨Åc gain function [14]. These
functions consider when, as a result of an attack, the adversary
might learn all, some, or no information about dynamic secrets.
We show that our metric generalizes previous metrics for quan-
tifying leakage of static secrets, including vulnerability [4],
guessing entropy [15], and g-vulnerability [14]. We also show
how to limit the power of the adversary, such that it cannot
inÔ¨Çuence inputs or delay attacks.

Finally, we put our model and metric to use by imple-
menting them in a probabilistic programming language [16]
and conducting a series of experiments. Several conclusions
can be drawn from these experiments:

‚Ä¢ Frequent change of a secret can increase leakage, even
though intuition might
that frequent
changes should decrease it. The increase occurs when
there is an underlying order that can be inferred and used
to guess future (or past) secrets.

initially suggest

‚Ä¢ Wait-adaptive adversaries can derive signiÔ¨Åcantly more
gain than adversaries who cannot adaptively choose when
to attack. So ignoring the adversary‚Äôs adaptivity (as in
prior work on static secrets) might lead one to conclude
secrets are safe when they really are not.

‚Ä¢ A wait-adaptive adversary‚Äôs expected gain increases
monotonically with time, whereas a non-adaptive adver-
sary‚Äôs gain might not.

‚Ä¢ Adversaries that are low adaptive, meaning they are ca-
pable of inÔ¨Çuencing their observations by providing low-
security inputs, can learn exponentially more information
than adversaries who cannot provide inputs.

1Our probabilistic model of interaction is a reÔ¨Ånement of the nondetermin-
istic model of Clark and Hunt [12], and it is a generalization of the interaction
model of O‚ÄôNeill et al. [13]. See Section VII for details.

We proceed as follows. Section II reviews QIF for static se-
crets and motivates the improvements we propose. Section III

¬© 2014, Piotr Mardziel. Under license to IEEE.
DOI 10.1109/SP.2014.41

540

presents our model of dynamic secrets, and Section IV presents
our metric for leakage. Section V describes our implementation
and Section VI presents our experimental results. Section VII
discusses related work, and Section VIII concludes.

II. QUANTITATIVE INFORMATION FLOW

Consider a password checker, which grants or forbids
access to a user based on whether the password supplied
by the user matches the password stored by the system. The
password checker must leak secret information, because it must
reveal whether the supplied password is correct. Quantifying
the amount of information leaked is useful for understanding
the security of the password checker‚Äîand of other systems
that, by design or by technological constraints, must
leak
information.

A. QIF for static secrets

The classic model for QIF, pioneered by Denning [17],

represents a system as an information-theoretic channel:



	

	

	



		

A channel is a probabilistic function. The system is a channel,
because it probabilistically maps a high security (i.e., secret)
input and a low security (i.e., public) input to an observable
(i.e., public) output. (High security outputs can also be mod-
eled, but we have no need for them in this work.) The adversary
is assumed to know this probabilistic function.

The adversary is assumed to have some initial uncertainty
about the high input. By providing low input and observing
output, the adversary derives some revised uncertainty about
the high input. The change in the adversary‚Äôs uncertainty is
the amount of leakage:

leakage = initial uncertainty ‚àí revised uncertainty.

Uncertainty is typically represented with probability distribu-
tions [18]. SpeciÔ¨Åc metrics for QIF use these distributions to
calculate a numeric quantity of leakage [1]‚Äì[5].

More formally, let XH, XL and XO be random variables
representing the distribution of high inputs, low inputs, and
observables, respectively. Given a function F (X) of the un-
certainty of X, leakage is calculated as follows:
F (XH) ‚àí F (XH | XL = (cid:2), XO),

(1)

is

where (cid:2)
the low input chosen by the adversary,
F (XH) is the adversary‚Äôs initial uncertainty about the high
input, and F (XH | XL = (cid:2), XO) is the revised uncertainty.
As is standard, F (XH | XL = (cid:2), XO) is deÔ¨Åned to be
(cid:2)
[F (XH | XO = o, XL = (cid:2))], where Ex‚ÜêX [f (x)] de-
Eo‚ÜêX
x Pr (X = x) ¬∑ f (x).
notes

O

B. Toward QIF for dynamic secrets

There are several ways in which the classic model for QIF

is insufÔ¨Åcient for reasoning about dynamic secrets:

‚Ä¢ Interactivity: Since secrets can change, the adversary
should be able to choose inputs based on past observa-
tions. That is, we want to allow feedback from outputs to
inputs. The classic model doesn‚Äôt permit feedback. There
are some QIF models for interactivity; we discuss them
in Section VII.

‚Ä¢ Input vs. attack: Classic QIF metrics quantify leakage
with respect to a single low input from the adversary.
Each input is an attack made by the adversary to learn
information. But with an interactive system, some low
inputs might not be attacks. For example, an adversary
might navigate through a website before uploading a
maliciously crafted string to launch a SQL injection
attack; the navigation inputs themselves are not attacks.
Our model naturally supports quantiÔ¨Åcation of leakage at
the times when attacks occur.

‚Ä¢ Delayed attack: Combining the above two features, ad-
versaries should be permitted to adaptively choose when
to attack based on their interaction with the system,
and this decision process should be considered when
quantifying leakage.

‚Ä¢ Moving target: New secrets potentially replace old se-
crets. The classic model cannot handle these moving
targets. To quantify leakage about moving-target secrets,
we need a model of how secrets evolve over time. Prior
work [27], [28] has considered leakage only about the
entire stream of secrets, rather than a particular value of
a secret at a particular time.

To address these insufÔ¨Åciencies in the classic model, we

introduce a new model for QIF of dynamic secrets.

III. MODEL OF DYNAMIC SECRETS

Our model of dynamic secrets involves a system, which
executes within a context. The system represents a program,
such as a password checker. The context represents the pro-
gram‚Äôs environment, which includes agents such as users
and adversaries. During an execution, the system and context
interact to produce traces of inputs and outputs. The system
receives inputs from the context, and it yields outputs back
to that context. With the password checker, for example, a
password Ô¨Åle might be a source of high inputs, and the
adversary might be a source of low inputs‚ÄîspeciÔ¨Åcally, of
guesses in an online guessing attack.

A. Systems as fully probabilistic automata

As in the classic model of QIF, a system accepts a high
input and a low input, and probabilistically produces an
observable output. Let H, L, and O be Ô¨Ånite sets of high
inputs, low inputs, and observable outputs.

Various instantiations of F have been proposed, including
Shannon entropy [1]‚Äì[3], [19]‚Äì[22], guessing entropy [23],
[24], marginal guesswork [25], vulnerability [4], [26], and g-
leakage [14].

Our model adds a notion of logical time to the classic
model. At each time step t of an execution, three events occur
sequentially: (i) the system accepts a high input ht; (ii) the
system accepts a low input (cid:2)t; and (iii) the system produces

541


	





	












 

   



  








 




 

  


	










 




 


 

Fig. 1.
A tree depicting all possible executions of a fully probabilistic
automaton. Each edge would also be labeled with a probability, which is
omitted from the Ô¨Ågure for simplicity.

Fig. 2. Model of dynamic secrets. The arrows feeding high inputs, low inputs,
and observables to the gain function are omitted for simplicity.

an observable output ot. An execution lasting T time steps
thus produces an execution history (synonomously, a trace) of
the following form:
(cid:3) (cid:4)(cid:5) (cid:6)

(cid:3) (cid:4)(cid:5) (cid:6)

h1 (cid:2)1 o1

(cid:4)(cid:5)

h2 (cid:2)2 o2

. . . hT (cid:2)T oT

.

(cid:3)

(cid:6)

t=1

t=2

t=T

Our assumption of cyclic alternation between high inputs, low
inputs and observable outputs does not preclude executions
where these events happen in other orders. To model such
executions, it sufÔ¨Åces to deÔ¨Åne dummy inputs or outputs that
are used whenever an event is skipped.

The execution of a system within a context can be rep-
resented using a fully probabilistic automaton [10].2 The
execution of such an automaton can be represented as a tree
where any path from the root to a leaf corresponds to an
execution history, as depicted in Figure 1. Each edge in the
tree corresponds to an event in the history. Let xt denote
the sequence of events of type x up to time t‚Äîfor example,
ht = h1, . . . , ht. Denote the probability of an event as follows:

‚Ä¢ High inputs: Pr(ht | ht‚àí1, (cid:2)t‚àí1, ot‚àí1) is the probability
of high input ht occurring, conditioned on the execution
history (ht‚àí1, (cid:2)t‚àí1, ot‚àí1) through time t ‚àí 1.

‚Ä¢ Low inputs: Pr((cid:2)t | (cid:2)t‚àí1, ot‚àí1) is the probability of low
input (cid:2)t occurring, conditioned on the public execution
history ((cid:2)t‚àí1, ot‚àí1) through time t‚àí1. Since this probabil-
ity is not conditioned on ht‚àí1, low inputs cannot depend
on past high inputs.

‚Ä¢ Observable outputs: Pr(ot | ht, (cid:2)t, ot‚àí1) is the prob-
ability of the system producing observable output ot,
conditioned on the execution history (ht‚àí1, (cid:2)t‚àí1, ot‚àí1) up
to time t ‚àí 1, as well as the high input ht and low input
(cid:2)t occurring at time t.

Given the probabilities of events, the probability of an execu-

2The execution of a system within an unknown context can be modeled by
an automaton in which the events corresponding to the production of inputs are
nondeterministic hence are not labeled with probabilities. Such automata can
be used to reason about the maximum leakage of a system over all possible
contexts [28].

542

tion (hT , (cid:2)T , oT ) is deÔ¨Åned as follows:

T(cid:7)

Pr(hT , (cid:2)T , oT ) =

[ Pr(ht | ht‚àí1, (cid:2)t‚àí1, ot‚àí1)

t=1

¬∑ Pr((cid:2)t | (cid:2)t‚àí1, ot‚àí1)
¬∑ Pr((cid:2)t | ht, (cid:2)t, ot‚àí1) ].

(2)

B. Interaction of a system with the environment

An execution context comprises a pair of functions‚Äîthe
high-input strategy [11] and the action strategy‚Äîthat generate
inputs for the system. These functions represent the high and
low agents in the environment. Our interaction model makes
these functions explicit, whereas they are left implicit in a fully
probabilistic automaton.3 Figure 2 depicts the interaction of a
system with its context. The high-input strategy Œ∑t produces a
(cid:8)
high input ht at each time step t as a function of the history
ht‚àí1, (cid:2)t‚àí1, ot‚àí1
of execution. Notice that the strategy can
change at each time step; we return to this point, below.

(cid:9)

The action strategy models the distinction between attacks
and inputs. At each time step, the action strategy produces
a new input on behalf of the adversary, resulting in a new
observable from the system. That observable is fed back
into the action strategy at the next time step. Eventually, the
adversary has gathered enough observations and commits to an
attack, represented by the strategy producing an exploit. (We
leave modeling interleaved attacks and low inputs as future
work.) Let E be set of exploits. DeÔ¨Åne an action to be either
a low input or an exploit, and let A be the set of actions,
where A = L ‚à™ E. At each time step, the action strategy
(cid:8)
Œ±t produces an action a as a function of the public history
(cid:2)t‚àí1, ot‚àí1
of execution. As with high-input strategies, there
can be a different action strategy at each time step.

(cid:9)

The adversary in our model is wait adaptive: it can pick
the best time to attack. An adversary that is not wait adaptive,
as in the classic model of QIF, can attack only at a Ô¨Åxed time.
Similarly, the adversary in our model is low adaptive: it can
generate low inputs, based on past observations, that inÔ¨Çuence
the system. An adversary that is not low adaptive could only
passively observe the system prior to attack.

3In the accompanying technical report [29] we show how to extract the

context from a fully probabilistic automaton.

Our model employs a time-indexed sequence of determinis-
tic strategy functions. Such a sequence can be used to emulate
a single probabilistic strategy by assuming the sequence is
generated by making a probabilistic choice, at each time step,
from among a set of deterministic strategies. Indeed, the equiv-
alence between the two approaches is stated by Theorem 2
in Section III-E. Deterministic strategies are more convenient
for proving mathematical properties of the model, so we use
them in the remainder of this section. Probabilistic strategies,
on the other hand, are more convenient for expressing our
metrics, and also make it more straightforward to reason
about increasing knowledge (i.e., decreased uncertainty) of
the strategy and the secrets it generates. Section IV and the
following sections use probabilistic strategies.

C. Success of the adversary

Once the adversary commits to an exploit e, no more
observations take place. The success of the adversary is now
determined. DeÔ¨Åne a scenario to be the history (ht, (cid:2)t‚àí1, ot‚àí1)
of execution up to the exploit. The number of high inputs in
a scenario is one more than the number of low inputs and
number of observations, because at the time the exploit has
been produced, high input ht has already been generated. The
gain function, shown in Figure 2, is a function of exploit e and
scenario (ht, (cid:2)t‚àí1, ot‚àí1). It yields a real number g representing
the success of the exploit.4 Some prior metrics for QIF
consider an exploit to be successful iff the adversary perfectly
guesses the secret. But more sophisticated gain functions can
quantify the success of the adversary in guessing part of a
secret, guessing a secret approximately, or guessing a past
secret [14].

D. Example: Password checker

We formalize the password checker from Section II as a
system that receives the real password as high input, and a
guessed password provided by the adversary as low input. The
system produces either accept or reject as the observable
output, depending on whether the guess equals the password.
Let H be the set of real passwords, L be the set of possible
guesses, and O be {accept, reject}. At each time step t,
it holds that Pr(ot = accept) = 1 iff ht = (cid:2)t, and Pr(ot =
reject) = 1 iff ht (cid:3)= (cid:2)t. Each time step models an invocation
of the password checker. We do not model the passage of
time when there is no guess. If a password change occurs
between two guesses, the new password becomes high input
to the system when the later guess is provided as low input.

For the high-input strategy, assume that a new password
is produced at each time step according to the following
password-changing policy:

A log is maintained of all failed login attempts. From
that log, the 10 most frequently guessed passwords
are extracted. A log is also maintained of the 5
most recently chosen passwords for each user. When
users change their passwords, they must do so in

accordance with two rules: (i) a new password cannot
coincide with any of the 5 previous passwords; and
(ii) a new password cannot coincide with any of the
10 most common guesses.

The high-input strategy, therefore, depends on the history of
low inputs and high inputs.

For the action strategy, the adversary produces guesses
based on the observation of whether past guesses were suc-
cessful. Further, there is no need to retry a failed guess until it
is likely the password has been changed. So the action strategy
depends on the past history of low inputs, and on the past
history of observables.

When the adversary has gathered enough information, he
can attack by choosing an exploit. The set E of exploits could
simply be the set L of low inputs, since a natural attack is to
try logging in with the password.

E. Probabilities in the presence of feedback

Having added high-input and action strategies to our model,
we now need to adapt equation (2) to use them in calculating
the probability of executions. Doing so is not straightforward:
as shown by Alvim et al. [28], equation (2) does not deÔ¨Åne
a channel that is invariant with respect to the distribution on
low and high inputs. Such a channel is required by classic
information-theoretic metrics of maximum leakage.

Our solution is to employ a technique proposed by
Tatikonda and Mitter [30] and applied by Alvim et al. [28]
to interactive systems. The details are given in our technical
report [29]. Here, we summarize the two main results.

First, we give a well-deÔ¨Åned probability distribution of

executions:

Theorem 1. Given a system and a context, there is a unique
joint probability distribution Q(Œ∑T , Œ±T , hT , (cid:2)T , oT ) capturing
the model of Figure 2.

The existence of Q allows us to condition the occurrence
of system-related events on the occurrence of context-related
events, and vice-versa. For example, we can use Q to reason
about how much information observables reveal about high-
input strategies.

Second, we show that our model‚Äôs use of deterministic

strategies is not a fundamental restriction:

Theorem 2. Probabilistic strategies can be modeled by prob-
ability distributions on deterministic strategies.

So we can model users and adversaries who use probabilistic
strategies. Nonetheless, as we prove below in Theorem 4, there
will always be a deterministic action strategy that is optimal
against a given high strategy. Clark and Hunt [12] show
that, similarly, deterministic strategies sufÔ¨Åce to determine
whether input-output labeled transition systems satisfy non-
interference.

4The gain function does not have access to future secrets values hu, where
u > t. So even if the adversary determines at time t what the high input will
be at time u > t, the adversary must wait until time u to realize the gain.
To give a real-world example, a thief who learns today that a house will be
unoccupied next Tuesday still has to wait until next Tuesday to rob the house.

IV. QUANTIFYING SECURITY

Having deÔ¨Åned the full model we can now derive from
it means of quantifying security. To facilitate this, we re-
present our model as a probabilistic program, which precisely

543

describes the joint distribution it induces. Using this notation
makes it easier to deÔ¨Åne particular scenarios precisely, as
is done in Section VI, and maps closely what we do in
our implementation. In particular, as Section V describes, we
literally implement this model in a probabilistic programming
language and use it to compute metrics of interest.

Given this new presentation of the model, we show how to
quantify security in terms of the gain adversaries are expected
to achieve while interacting with a system. We will describe
this expectation in terms of the optimal adversary that strives
to achieve the most gain (Section IV-C).5 In Sections IV-D and
IV-E we show this general deÔ¨Ånition of security expresses and
extends the existing metrics of vulnerability, g-vulnerability,
and guessing entropy.

A. Probabilistic programming

Probabilistic programs permit the expression of probability
distributions using familiar programming language notation. In
this paper, we will express probabilistic programs in slightly
sugared OCaml, an ML-style functional programming lan-
guage [31]. In essence, probabilistic programs are just normal
programs that employ randomness. For example, the following
program employs the random_int function to draw a random
integer from the uniform distribution of non-negative integers
(representing the possible real locations of some hidden stash).
Each run of the gen_stash program can thus be viewed as
sampling a number from a uniform distribution of integers
between 0 and 7:

let gen_stash () =

let real_loc = (random_int () mod 8) in real_loc

Though gen_stash is traditionally viewed as sampling values,
it can also be seen as a deÔ¨Ånition of a random variable
Xgen_stash (), whose values are uniformly distributed be-
tween 0 and 7. We write Xexp to denote a random variable
whose distribution is deÔ¨Åned by the probabilistic program exp.

While gen_stash takes no arguments, in general functions
may take arguments and these arguments might themselves be
probabilistic. Consider the following example:

let guess (realval: int) (guessval: int) =

let correct = (realval == guessval) in

correct

The guess program takes two arguments and returns whether
they are equal. Thus we can deÔ¨Åne random variable
Xguess (gen_stash()) 5 over booleans, where true will
have probability 1/8, and false have the remaining probability.

The distribution of

this random variable depends on
the distribution of
the random variable corresponding to
gen_stash (), so we can condition the probability of an
outcome on the latter given an outcome of the former; e.g.,

Pr (Xgen_stash () | Xguess (gen_stash ()) 5 = false)

would be the uniform distribution of integers between 0 and
7, but not including 5.

544

type time = int
type history =

{t: time; tmax: time;

highs: H list;
lows: L list;
obss: O list
atk: E option}

type A = Wait of L | Attack of E

type sysf = history ‚Üí O
type highf = history ‚Üí H
type actf = int ‚Üí int ‚Üí L list ‚Üí O list ‚Üí A
type gainf = history ‚Üí float

Fig. 3. Types used by the probabilistic program implementing the model.

B. The model as a probabilistic program

Now we consider how to express the model of Figure 2 as

a probabilistic program.

Elements of the model: The types of values in the
program are H, L, O, and E, as in the information-theoretic
presentation. Figure 3 gives the types of other elements used in
the model.6 DeÔ¨Åne a record type history to be the history of
execution: the Ô¨Årst Ô¨Åeld of the record contains the current time;
the next is the maximum time for the length of the execution of
a scenario; the next three contain the high inputs, low inputs,
and observations produced thus far; and the last contains the
exploit. At each time step, the adversary will produce an action
A, which is either a low input or an exploit.

Operation of the model: Figure 4 presents the model
as a probabilistic program, using two functions. The Ô¨Årst,
scenario, corresponds to the identically named element in
Figure 2 while the second, evaluate, uses scenario and then
evaluates the resulting gain from the history produced.

The scenario function takes four arguments. The Ô¨Årst, T,
is maximum number of time steps to consider. The second is
the system being modeled, which is a function of type sysf (all
types are deÔ¨Åned in Figure 3). The last two arguments comprise
the context, i.e., the high-input strategy (of type highf) and the
action strategy (of type actf). The scenario starts with the initial
history at time 0, with empty lists, and no attack. The loop at
line 8 captures the iterations of Figure 2, updating the history
for up to T iterations, or until the adversary attacks using an
exploit. In each iteration, a new secret is produced (Line 11),
an adversary action is computed (Line 15), and if the action is
to wait, a new observation is made (Line 23).7 This function
returns the full history when it completes.

The evaluate function computes how successful the ad-
versary was by applying their exploit to the gain function
(Line 36), which has type gainf. Evaluation returns minimal
gain if the history does not contain an exploit.

Comparing to the information theoretic model: Our
probabilistic program corresponds quite closely to the informa-

5Our technical report [29] also considers a memory-limited adversary.
6The type Œ± option is an OCaml type whose values are either None, or

Some x where x is of type Œ±.

7The @ operator appends two lists. We will use OCaml‚Äôs array indexing
notation a.(i) for lists as well. That is, l.(i) = nth l i. We also
deÔ¨Åne last l = l.(length l - 1) to get the last element of a list.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38

let scenario (T: time) (system: sysf)

(high_func: highf) (strat_func: actf) =

let hist = {t = 0; tmax = T; atk = None;

highs = []; lows = [];
obss = []} in

while hist.t <= T && hist.atk = None do

hist.t <- hist.t + 1;

let new_high = high_func hist in

hist.highs <- hist.highs @ [new_high];

let new_action =

strat_func hist.t T hist.lows hist.obss in

match new_action with
| Attack exp ->

hist.atk <- Some exp

| Wait new_low ->

hist.lows <- hist.lows @ [new_low];
let new_obs = system hist in

hist.obss <- hist.obss @ [new_obs]

done;
hist

let evaluate (T: time)

(system: sysf)
(high_func: highf)
(gain_func: gainf)
(strat_func: actf) =
let hist = scenario T system

high_func strat_func in

let gain = match hist.atk with

| Some exp -> gain_func hist exp
| None -> ‚àí‚àû in

gain

Fig. 4. The model as a probabilistic program.

tion theoretic model of the previous section. The one difference
is that the probabilistic program directly employs a single high-
input strategy high_func that can itself be randomized, as
opposed to using a randomized stream of deterministic high-
input strategies (and likewise for the action strategy). As per
Theorem 2, doing this is still faithful to the model, and it turns
out to be more tractable (and convenient) to implement.

C. The general metric

Now we turn our attention to deÔ¨Åning a general quantitative
metric of information leaked by the model. In what follows
we will Ô¨Åx all the evaluate parameters except the last two
(the gain function and the strategy function). We will use the
expression

model = evaluate T system high_func

as an instantiation of the Ô¨Åxed parameters. The expected gain
is thus E [Xmodel gain_func strat_func].

An information Ô¨Çow metric is most useful when considered
from the point of view of a powerful adversary. In fact, we
are most interested in what the system can be expected to
leak when interacting with an adversary employing the optimal
strategy.

DeÔ¨Ånition 3. The dynamic gain Dgain_func (model) is the
gain an optimal adversary is expected to achieve under the
parameters of a model from a gain function gain_func.

Dgain_func (model) def= max
s‚ààactf

E [Xmodel gain_func s]

One way to compute the dynamic gain is to consider essen-
tially all adversary choices in the joint distribution describing
the model, picking out the best ones. The best choices made
thus will both deÔ¨Åne the optimal adversary, which we call
opt_strat, and the gain they can expect to achieve. The rest
of this subsection considers an algorithm for doing this.

To start, note the adversary‚Äôs strategy can control three
things: the low inputs to the system, when to attack, and
the exploit. We introduce all allowed choices via a random
strategy that tries everything;8 its parameters permit modeling
adversaries lacking some capabilities.

let rand_strat (adaptwait: bool)

(loworder: L list option): actf =

fun (t: time) (tmax: time)

(lows: L list) (obss: O list): A ->

if t == tmax || (adaptwait && flip 0.5) then

Attack (uniform_select [E])

else match loworder with

| None -> Wait (uniform_select [L])
| Some order -> Wait order.(t)

This function is parameterized by two things: adaptwait that
determines whether we are modeling a wait-adaptive adversary
and loworder, which is None for a low-adaptive adversary
and otherwise provides a sequence of low inputs. Whenever
adaptwait is set to false this strategy only attacks at time
tmax = T and when it is true it attempts to attack at any point,
randomly, choosing an exploit, randomly again, from [E], the
list of all attacks. If loworder is Some order, the strategy picks
low inputs according to order but otherwise picks a low input
randomly from [L], the list of all possible low inputs.

The evaluation of model gain_func strat (for some
gain_func and strat = rand_strat adaptwait loworder)
produces the random variable Xmodel gain_func strat.
Along with the Ô¨Ånal return value of this expression we will also
make use of the random variables for some other expressions
that are involved in this computation. Namely, we will make
use of the joint distribution over the Ô¨Ånal gain variable, and
the atk, highs, lows, and obss Ô¨Åelds of hist:

Pr (Xgain, Xhist.atk,

Xhist.highs, Xhist.lows, Xhist.obss)

To compute the dynamic gain for the optimal adversary, we
deÔ¨Åne two maps Act (for action) and G (for gain) from lists of
low inputs and observations to the optimal adversary‚Äôs action
and expected gain, respectively. Starting from full histories
(lists of length T) and working backwards to the initial history
(empty lists), these maps‚Äô construction will deÔ¨Åne the behavior
of strategy opt_strat and determine its expected gain. Though
the joint distribution is constructed using a random strategy, in

8The function flip p is a random coin Ô¨Çip returning true or false with
probabilities p and 1 ‚àí p respectively. uniform_select uniformly picks
an element from a given list.

545

what follows, the probabilities introduced by the strategy will
be factored out by conditioning on its output.

When length lows = length obss = T we Ô¨Åll in the map

G as follows:

G [lows, obss] def= max
e‚ààE

Gattack(lows,obss,e)

Act [lows, obss] def= Attack argmax

e‚ààE

Gattack(lows,obss,e)

(3)

Gattack(lows,obss,e) def= E[Xgain | Xhist.lows = lows,
Xhist.obss = obss,
Xhist.atk = Some e]

The expression determines the best exploit for an adversary
that has chosen the list of lows lows and observed obss as a
result. There is no need to consider a choice of waiting at time
T as that would result in the minimum gain. As a technicality,
we assume that E [Xgain | X] = ‚àí‚àû whenever Pr (X) = 0.
This is necessary when modeling non-adaptive adversaries that
do not necessarily try attacking at every point (i.e., when the
adaptwait parameter to rand_strat is false).

For actions before time T, the adversary can either attack
now, optimizing their exploit e, or wait, optimizing their
choice of low input l for the next observation. Formally, if
length lows = length obss = n < T we deÔ¨Åne:

G [lows, obss] def= max{

max
e‚ààE
max
l‚ààL

{Gattack(lows,obss,e)} ,
{Gwait(lows,obss,l)}}

def= Ba

def= Bw

(4)

Act [lows, obss] def=

‚éß‚é®
‚é©

e‚ààE
Wait argmax

l‚ààL

Attack argmax

Gattack(lows,obss,e)

if Ba ‚â• Bw

Gwait(lows,obss,l)

otherwise

Gwait(lows,obss,l) def=

E

o‚ÜêOn+

(l)

lows,obss
G [lows @ [l], obss @ [o]]

The outer maximization of Equation 4 describes the adaptive
choice of either attacking the system at time n or waiting. The
optimization of attack is identical to that of Equation 3. Choos-
ing to wait presents the further adaptive choice of the next
low input. The notation On+
lows,obss(l) in this optimization is
a random variable representing the distribution of observables
at the next time step given a particular choice l of low input.

(cid:13)

(cid:14)
lows,obss(l) = o

def=
Pr
Pr (Xhist.obss.(n+1) = o |

On+

Xhist.lows =(n+1) (lows @ [l]),
Xhist.obss =n obss)

has control over the potentially non-deterministic aspects of the
system function. When modeling non-low-adaptive adversaries
(i.e., when the loworder parameter to rand_strat is the list
of inputs), the strategy does not necessarily try all low inputs,
so we need to carefully deÔ¨Åne Gwait(lows, obss, l) = ‚àí‚àû
whenever Pr (Xhist.lows = lows @ [l]) = 0.

Constructing the map G backwards in the length of histo-
ries (due to the recursion in the deÔ¨Ånition of G) eventually
produces G [[], []] and this is the expected gain of the
optimal attacker in a model, or Dgain_func (model).
Theorem 4. The maps Act and G deÔ¨Åne the behavior
and expected gain of an optimal adaptive adversary (using
rand_strat with waitadapt = true and loworder = None).

SpeciÔ¨Åcally, let opt_strat be deÔ¨Åned as follows:

let opt_strat: actf =

fun (t: time) (lows: L list) (obss: O list) ->

Act[lows, obss]

Using opt_strat achieves the maximal expected gain:

Dgain_func (model)
def= max
s‚ààactf

E [Xmodel gain_func s]

= E [Xmodel gain_func opt_strat]
= G[[], []]

Proof: (sketch) There are two parts to this claim: (1)
that opt_strat achieves maximal gain over all strategies, and
(2) that this gain is equal to G[[],[]]. To show both let us
consider the optimal behavior (not necessarily of opt_strat)
at time T, having observed some list of observations obss
and having chosen low inputs lows. The optimal action has
to be an exploit as otherwise the gain becomes ‚àí‚àû. The
deÔ¨Ånitions of Equation 3 pick out the exploit e which max-
imizes Gattack(lows,obss,e), which by deÔ¨Ånition is exactly
the expected gain achieved by exploiting using e. There is no
other behavior that does better here, as we have speciÔ¨Åcally
maximized over all options.

We can therefore conclude that G[lows,obss] is indeed
the optimal expected gain at time T given that lows and obss
have occurred. We then carry this argument backwards to time
T ‚àí 1 (and then T ‚àí 2 and so on until time 0). Once again, note
the deÔ¨Ånitions of Equation 4 maximize over all choices the
adversary might make. It remains to show that the quantities
maximized over are accurate representations of expected gain.
The option to attack is based on Gattack(lows,obss) which is
the same as in the argument for time T. For Gwait(lows,obss),
note that its deÔ¨Ånition uses the quantities from G for a later
time T which we presumed are correct. The deÔ¨Ånition merely
performs an expectation over possible observations o that
might occur at that point.

The =n notation above corresponds to equality of the Ô¨Årst n
def= Xtake n list1 =
elements of lists: Xlist1 =n list2
take n list2. The expectation in Gwait is due to the fact that
the adversary does not know the high part of the history nor

546

(5)

D. Expressing existing metrics

Here we show how our metric for optimal adversary
gain subsumes existing metrics, in particular, vulnerability, g-
vulnerability, and guessing entropy. We do this in two steps.
First, we must use the following non-wait-adaptive version of
DeÔ¨Ånition 3 for deÔ¨Åning dynamic gain (since classic metrics
use non-wait-adaptive adversaries).

5. The

dynamic,

gain
DeÔ¨Ånition
Dnowait
gain_func (model) is the gain an optimal, not wait-adaptive,
adversary is expected to achieve under the parameters of a
model from a gain function gain_func.

not wait-adaptive,

D

nowait

gain_func (model) def= max

s‚ààactfnowait

E [Xmodel gain_func s]

The set actfnowait is the set of all action strategies that only
attack at time T, hence strategies that are not wait adaptive.

Proposition 1. Constructing opt_strat as deÔ¨Åned in Sec-
tion IV-C but using rand_strat with waitadapt = false
instead yields an action strategy that maximizes the expected
gain for a non-wait-adaptive adversary.

The proof for this proposition essentially follows that
of Theorem 4, merely noting that using rand_strat with
waitadapt = false to enumerate possible adversary choices
removes exactly those, and no more, that are not available to
an adversary who cannot wait.

Now we deÔ¨Åne a restricted model and a scenario therein
that corresponds to the classic scenario, a gain function speciÔ¨Åc
to each metric, and prove that the dynamic not-wait-adaptive
gain matches the standard metric. Further details (and proofs)
are given in our technical report [29].

The restricted model is deÔ¨Åned as follows. First, the high-
input strategy is an identity function, since the secret never
changes. Second, the gain is deÔ¨Åned only in terms of the
high value and the exploit (not past observations or low
inputs, which are ignored). Additionally, there are no low input
choices to make (type L = unit) and we use rand_strat with
adaptwait = false, thus eliminating any beneÔ¨Åt of low or
wait adaptivity. Under these restrictions, Equations 4 and 5 can
be rewritten resulting in a simpler deÔ¨Ånition of gain. In what
follows we omit the lows parts, and write secret to express
last hist.highs, the sole unchanging high value. We will
refer to the expressions model and gain functions gain_func
that instantiate parameters in the restricted manner enumerated
here as static.

Lemma 1. If model and gain_func are static then the dynamic
gain (which would be more appropriately named the static
gain) simpliÔ¨Åes to the following.

‚é°
nowait
gain_func (model) =
‚é¢‚é£ max

e‚ààE

E
hist.obss

D

obss‚ÜêX

‚é§
‚é•‚é¶

E(Xgain |

Xhist.obss = obss,
Xhist.atk = Some e)

Theorem 6. In a static model, the vulnerability (written V)
of the secret conditioned on the observations is equivalent to
dynamic gain using the gain_vul gain function.

D

nowait

gain_vul (model) = V (Xsecret | Xhist.obss)

g-vulnerabiity [14]: Generalized gain functions can be
used to evaluate metrics in a more Ô¨Åne-grained manner, leading
to a metric called g-vulnerability. This metric can also be
expressed in terms of the static model. Let g be a generalized
gain function, returning a float between 0.0 and 1.0, then we
have:
let gain_gen_gain (g: H ‚Üí E ‚Üí float): gainf =

fun (hist: history) (exp: E ) : float ->
g secret exp

The difference between expected gain and g-vulnerability
are non-existent in the static model. The gain of a system
corresponds exactly to g-vulnerability of g, written Vg (¬∑).
Theorem 7. In a static model the g-vulnerability of g is equiv-
alent to dynamic gain using gain_gen_gain g gain function.
gain_gen_gain (model) = Vg (Xsecret | Xhist.obss)

nowait

D

Guessing-entropy [23]: Guessing entropy, characteriz-
ing the expected number of guesses an optimal adversary will
need in order to guess the secret, can also be expressed in
terms of the static model. We let attacks be lists of highs (really
permutations of all highs). The attack permutation corresponds
to an order in which secrets are to be guessed. We then deÔ¨Åne
expected gain to be proportional to how early in that list of
guesses the secret appears.
type E = H list

let pos_of (secret: H) (exp: H list) =

(* compute the position of secret in exp *)

let gain_guess_ent: gainf =

fun (hist: history) (exp: E ) =

-1.0 * (1.0 + (pos_of secret exp))

Note that we negate the gain as an adversary would opti-
mize for the minimum number of guesses, not the maximum.
Guessing entropy, written G, is related to dynamic gain as
follows.

Theorem 8. In a static model, guessing entropy is equivalent
to (the negation of) dynamic gain using the gain_guess_ent
gain function.

‚àí D

nowait

gain_guess_ent (model) = G (Xsecret | Xhist.obss)

Now we turn to the particular metrics.

E. Extending existing metrics

Vulnerability [4]: The notion of vulnerability corre-

sponds to an equality gain function (i.e., a guess).

let gain_vul: gainf =

fun (hist: history) (exp: E ) ->

if secret == exp then 1.0 else 0.0

The goal of the attacker assumed in vulnerability is evident
from gain_vul; they are directly guessing the secret, and they
only have one chance to do it.

Our model lets us take existing information Ô¨Çow metrics
and extend their purview in two directions: temporal variations
in the target of the attack and the attacker‚Äôs capabilities. Each
of these extensions can be speciÔ¨Åed in terms of the more
general scenario and gain functions permitted in our model.
As such, we preserve the goal of an existing metric but apply
it to situations in which the existing deÔ¨Ånitions are insufÔ¨Åcient.

The Ô¨Årst direction gives us several choices as to what about
the present, past, or future is the intended attack target. We

547

will brieÔ¨Çy cover four categories: moving target, speciÔ¨Åc past,
historical, and change inference.

1) Moving target: The target of the attack is the current
high value. DeÔ¨Åning the gain in terms of the most
recent high value, rather than the original high value,
produces the moving-target equivalents of vulnerability,
g-vulnerability, and guessing entropy; i.e., we use the
same gain functions as Section IV-D but with non-identity
high-input strategies.

2) SpeciÔ¨Åc past gain: The target of attack is the high value
at some Ô¨Åxed point in time (rather than the most recent
one). The gain function would thus evaluate success
against this secret, independent of the current secret.

3) Historical gain: The target of attack is the entire history
of high values up to the present time. An attack on the
whole history can be formulated by extending the set of
attacks to include lists of all lengths up to T and specifying
dynamic gain in terms of equality of this attack and the
history.

4) Change inference: The target of the attack is not one
or more high values, but rather the high-input strategy
that produces them. An adversary‚Äôs interactions will in-
crease his knowledge of the high-input strategy, but such
knowledge is only indirectly quantiÔ¨Åed by knowledge of
the high values themselves. To quantify knowledge of the
high-input strategy directly, we could slightly extend the
deÔ¨Ånition of gain functions:
type gainf = history ‚Üí highf ‚Üí float

Due to the difÔ¨Åculty inherent in the problem of deter-
mining functional equivalence, such discrimination would
need to be syntactic (two semantically identical high-input
strategies could be seen as distinct).

On the second axis of extension we have capabilities by
which the adversary can interact, or inÔ¨Çuence, the system prior
to attacking. Our model permits consideration of low-adaptive
and wait-adaptive adversaries, who can carefully choose how
to interact with the system prior to attacking it, and/or wait
for the best moment to attack.

Section VI conducts experiments that explore some of these

metrics.

V.

IMPLEMENTATION

We have implemented our model using a simple monadic
embedding of probabilistic computing [32] in OCaml, as per
Kiselyov and Shan [33]. The basic approach is as follows.
The model function, translated into monadic style, is proba-
bilistically evaluated to produce the full joint distribution over
the history up to some time T. From this joint distribution
the optimal adversary‚Äôs expected gain is constructed according
to the algorithm in Section IV-C. The implementation (and
experiments from the next section) are available online.9

In more detail, our implementation works as follows. We
represent a distribution as either a map from values to Ô¨Çoats
(a slight extension of PMap of the extlib library10) or an

9https://github.com/plum-umd/qif/tree/master/oakland14
10ocaml-extlib: https://code.google.com/p/ocaml-extlib

enumerator of value and Ô¨Çoat tuples (Enum of the extlib library),
converting between the two at various points when evaluating
the program.

module M = struct

type ‚Äôa dist = (‚Äôa, float) PMap.t
...

module E = struct

type ‚Äôa dist = (‚Äôa * float) Enum.t
...

The former is used to represent a mapping of values to
their probabilities but requires all these values to be stored
in memory. The latter has low memory requirements and is
used before the probabilities of values need to be retrieved.
These distributions are manipulated in a monadic style using
functions such as:

val bind: ‚Äôa dist -> (‚Äôa -> ‚Äôb dist) -> ‚Äôb dist
val return: ‚Äôa -> ‚Äôa dist
val bind_flip: float -> (bool -> ‚Äôb dist) -> ‚Äôb dist
val bind_uniform:

int -> int -> (int -> ‚Äôb dist) -> ‚Äôb dist

val bind_uniform_select:

‚Äôa list -> (‚Äôa -> ‚Äôb dist) -> ‚Äôb dist

The Ô¨Årst two are standard. The next creates a random coin
Ô¨Çip and continues the computation over this Ô¨Çip to produce a
distribution. The next does the same with a random integer in
a given range and the last uniformly picks a value from a list
to continue the computation with. The program below shows
how these functions are used to compute the probability that
the sum of two dice is 10.

let d_dice1 = M.bind_uniform 1 6 M.return in
let d_dice2 = M.bind_uniform 1 6 M.return in
let d_sum = M.bind d_dice1 (fun dice1 ->

M.bind d_dice2 (fun dice2 ->

M.return (dice1 + dice2))) in

let prob_10 = PMap.find d_sum 10

In the Ô¨Årst two lines, two distributions are created that map
integers 1 through 6 to the probability 1/6. In the third line
bind is used to take an initial distribution d_dice1 and a
function that will continue the computation of a distribution
starting from a value of the Ô¨Årst. This continuation gets
called once for each possible value in d_dice1 and each time
produces a distribution of 6 values. All 6 of these distributions
are merged into one when this bind Ô¨Ånishes. The nested
bind performs a similar task starting with values of d_dice2,
eventually producing a distribution, d_sum over the sum of two
dice rolls. The probability of this sum being 10 is looked up
at the last line.

To use this approach, functions must be rewritten in
monadic style. For example, the rewritten rand_strat function
(given just below DeÔ¨Ånition 3, page 6) is the following:

let rand_strat (adaptwait: bool)

(loworder: L list option) =

fun (t: time) (tmax: time)

(lows: L list) (obss: O list): A dist ->

bind_flip 0.5 (fun flip ->

if t == tmax || (adaptwait && flip)

then bind_uniform_select [E]

(fun exp -> return (Attack exp))

else match loworder with

| None -> bind_uniform_select [L]

(fun low -> return (Wait low)))

| Some order -> return order.(t))

548

Notice that the output type is now a distribution on actions,
not just a single action. Monadic style becomes cumbersome
with more complex functions, and though a more direct-style
implementation is possible (e.g., as in the second half of
Kiselyov and Shan‚Äôs paper [33]), it did not perform as well.
Our current implementation makes little attempt to opti-
mize the construction of a distribution, and thus is susceptible
to a state-space explosion when a scenario has many time
steps. Fortunately, probabilistic programming is a growing
research area (cf. the survey of Gordon et al. [16]) so we are
optimistic that the feasible scale of experiments will increase
in the future. Approximate probabilistic inference systems
such as those based on graphical models or sampling can
be used to estimate gain. Exact implementations based on
smarter representations of distributions such as those using
algebraic decision diagrams [16] could potentially be used to
simulate our model. Additionally, Mardziel et al. [9], [34]
show how to soundly approximate a simple metric using
probabilistic abstract interpretation; this technique could poten-
tially be extended to also soundly approximate dynamic gain.
Presently, our simple implementation proved to be sufÔ¨Åcient
to demonstrate various aspects of the model on a variety of
scenarios.

VI. EXPERIMENTS

This section describes several experiments we conducted,
illustrating the interesting outcomes mentioned in the introduc-
tion by measuring the effect of varying different parameters
of the model. Our experiments develop several examples on
the theme of stakeouts and raids, where each example varies
different parameters,
including whether and how a secret
changes, whether and in what manner the adversary is low- and
wait-adaptive, and the impact of adding costs to observations.
We describe the common elements of the scenario next,
and describe each variation we considered in the following
subsections.

Suppose an illicit-substance dealer is locked in an ever-
persistent game of hiding his stash from the police. The
simplest form of this example resembles password guessing,
replacing the password with the location of the stash and au-
thentication attempts with ‚Äústakeouts‚Äù in which police observe
a potential stash location for the presence of the stash. After
making observations the police will have a chance to ‚Äúraid‚Äù
the stash, potentially succeeding. In the meantime the stash
location might change.

The stash location will be represented as a integer from 0

to 7, as will be the attacks. Observations will be booleans:
type H = L = E = int
type O = bool

(* 0,...,7 *)

Stakeouts are carried out by comparing the stakeout location
to the real stash location.

let stakeout: sysf =

fun (hist: history) ->

last hist.highs = last hist.lows

i

 

n
a
g
d
a
r
 

i

d
e

t
c
e
p
x
e

2-0.0
2-0.5
2-1.0
2-1.5
2-2.0
2-2.5
2-3.0

0

1

2

perfect raid (1)
imperfect raid (2)
dynamic, imperfect raid (3)

3

4

5

T = max number of stakeouts before raid

6

7

8

9

10

11

12

Fig. 5. Expected raid gain over time given stakeouts stakeout with (1)
static stash stay_stash and perfect raids raid, (2) static stash and imper-
fect raids raid_imperfect, and (3) moving stash move_stash 4 and
imperfect raids, all with non-adaptive adversaries (waitadapt = false,
loworder = Some rotate).

Raids fail unless the stash and raid locations match:

let raid: gainf =

fun (hist: history) (exp: E ) ->

if last hist.highs = exp then 1.0 else 0.0

Finally, for most experiments, the stash moves randomly every
4 time steps (rate is 4):

let move_stash (rate: int): highf =

fun (hist: history) ->

if hist.t mod rate = 0

then gen_stash ()
else last hist.highs

Above, we reference the gen_stash function introduced earlier,
which randomly selects a stash location.

let gen_stash () =

let real_loc = (random_int () mod 8) in real_loc

The high-input strategy for our Ô¨Ånal example (Section VI-E)
is more complex and will be described later.

A. How does gain differ for dynamic secrets, rather than static
secrets?

Our Ô¨Årst experiment considers the impact on information
leakage due to a dynamic secret (using high-input strategy
move_stash). The adversary here will be non-adaptive and for
comparison we also consider a high-input strategy that does
not change the secret:

let stay_stash: highf =

fun (hist: history) ->

if hist.t = 0 then gen_stash ()

else last hist.highs

We also consider a variation of a raid that has a chance to
fail even if the raid location is correct, and has a chance to
accidentally discover a new stash even if the raid takes place
at the wrong location.

For modeling non-low-adaptive adversaries
in some of
our experiments, we will use a Ô¨Åxed stakeout order
by using rand_strat with loworder = Some rotate where
rotate = [0;...;7;0;...].

let raid_imperfect: gainf =

fun (hist: history) (exp: E ) ->

if last hist.highs = exp

then flip 0.8
else flip 0.2

549

i

 

n
a
g
d
a
r
 

i

d
e

t
c
e
p
x
e

2-0.0
2-0.5
2-1.0
2-1.5
2-2.0
2-2.5
2-3.0

0

1

i

 

n
a
g
d
a
r
 

i

all fixed strategies (1)
a fixed strategy (2)
adaptive (3)

d
e
t
c
e
p
x
e

non-adaptive
adaptive wait

2-0.0
2-0.5
2-1.0
2-1.5
2-2.0
2-2.5
2-3.0

2

3

T = max number of stakeouts before raid

4

5

6

7

8

0

1

2

3

4

5

6

7

8

9

10

11

12

T = max number of stakeouts before raid

Fig. 6. Expected raid raid gain over time with static stash stay_stash
given stakeouts stakeout_east_west with (1) all possible non-adaptive
orderings order (loworder = Some order),
(2) a possible non-
adaptive ordering (loworder = Some [0;1;2;7;3;4;5;6]), and (3)
adaptive (loworder = None) stakeout
locations, all not wait-adaptive
(waitadapt = false).

Figure 5 plots how the gain differs when we have a static
secret with perfect raids, a static secret with imperfect raids,
and a dynamic secret with imperfect raids. The static portion
(1) with perfect raid is an example of an analysis achievable
by a parallel composition of channels and the vulnerability
metric [35]. Adding the imperfect gain function (2) alters the
shape of vulnerability over time though in a manner that is
not a mere scaling of the perfect raid case. The small chance
of a successful raid at the wrong location results in higher
gains (compared to perfect raid) when knowledge is low. With
more knowledge, the perfect raid results in more gain than the
imperfect. Adding a dynamically changing stash (3) results
in a periodic, non-monotonic, gain; though gain increases in
the period of unchanging secret, it falls right after the secret
changes. This, in effect, is a recovery of uncertainty, which
is thus not a non-renewable resource [35]. In the following
sections we will refer to the period of time in which the secret
does not change as an epoch.

B. How does low adaptivity impact gain?

To demonstrate the power of low adaptivity we will use
a system function that outputs whether the stash is east or
west of the stakeout location. Assuming the stash locations
are ordered longitudinally, this function is just a comparison
between the stash and stakeout location. The non-adaptive
adversary will pick the stakeouts in a Ô¨Åxed order speciÔ¨Åed
by letting loworder = Some order for some ordering of low
inputs order, whereas the low-adaptive adversary will use
loworder = None:

let stakeout_east_west: sysf

fun (hist: history) ->

last hist.highs <= last hist.lows

Figure 6 demonstrates the expected gain of both types of
attackers. For Ô¨Åxed-order (non-adaptive) adversaries, we used
all 8! permutations of the 8 stash locations as possible orders
and plotted them all as the wide light gray lines in the Ô¨Ågure.
Though there are many possible orders, the only thing that
makes any difference in the gain over time is the position of
7 (the highest stash location) in the ordering as the system
function for this input reveals no information whatsoever. All
other stakeout locations reveal an equal amount of information
in terms of the expected gain. To demonstrate this behavior,

stakeouts

Fig. 7.
given
adversary
adversary
(loworder = Some rotate).

Expected raid gain with moving stash move_stash 4
non-wait-adaptive
wait-adaptive
low-adaptive

(waitadapt = false)
(waitadapt = true),

stakeout

and
all

(2)
not

with

(1)

i

s
d
a
r
 

d
e

t
c
e
p
x
e

 
f

o

 
r
e
b
m
u
n

 4.5
 4
 3.5
 3
 2.5
 2
 1.5
 1

non-adaptive
adaptive wait

0

1

3

2
T = max number of stakeouts before raids

4

5

6

7

8

9

10

11

12

Expected number of

raids
stash move_stash 4 given

Fig. 8.
moving
(1)
wait-adaptive adversary (waitadapt = true), all not
(loworder = Some rotate).

(gain of raid_guess) with
stakeouts stakeout with
(2)
low-adaptive

(waitadapt = false)

non-wait-adaptive

adversary

and

we have speciÔ¨Åcally plotted in the Ô¨Ågure the gain for an
ordering in which location 7 is staked-out at time 3 (labeled ‚Äúa
Ô¨Åxed strategy‚Äù). Gain increases linearly with every non-useless
observation. On the other hand, the low-adaptive adversary
performs binary search, increasing his gain exponentially.

C. How does wait adaptivity impact gain?

An adversary that can wait is allowed to attack at any time.
Adaptive wait has a signiÔ¨Åcant impact on the gain an adversary
might expect. In the simple stakeout/raid example of Figure 7,
it transforms an ever-bounded vulnerability to one that steadily
increases in time. Using the raid_guess function given below
we can compute the guessing entropy, which the experiment in
Figure 8 shows will steadily decrease (recall from Section IV-D
that guessing entropy is inversely related to dynamic gain).

type E = H list

let raid_guess: gainf =

fun (hist: history) (exp: E ) ->

-1.0*(1.0+(pos_of (last hist.highs) exp))

Roughly, the optimal behavior for a wait-adaptive adver-
sary is to wait until a successful stakeout before attacking.
The more observations there are, the higher the chance this
will occur. This results in the monotonic trend in gain over
time.

550

There are subtle decisions the adversary makes in order to
determine whether to wait and allow the secret to change. For
example, in Figure 7, if the adversary has to attack at time
5 or earlier and has not yet observed a successful stakeout
by time 3, they will wait until time 5 to attack, letting all
their accumulated knowledge be invalidated by the change that
occurs at time 4. This seems counter-intuitive as their odds of
a successful raid at time 3 is 1/5 (they eliminated 3 stash
locations from consideration), whereas they will accumulate
only 1 relevant stakeout observation by time 5. Having 3
observations seems preferable to 1. The optimal adversary will
wait because the expected gain at time 5 is actually better
than 1/5; there is 1/8 chance that the stakeout at time 5 will
pinpoint the stash, resulting in gain of 1, and 7/8 chance it
will not, resulting in expected gain of 1/7. The expectation of
gain is thus 1/8 ¬∑ 1 + 7/8 ¬∑ 1/7 = 1/4 > 1/5. The adversary thus
has better expected gain if they have to attack by time 5 as
compared to having to attack by time 3 or 4, despite having to
raid with only one observation‚Äôs worth of knowledge. One can
modify the parameters of this experiment so that this does not
occur, forcing the adversary to attack before the secret changes.
This results in a longer period of constant vulnerability after
each stash movement.

that

This ability to wait

is the antithesis of moving-target
vulnerability. Though a secret
is changing with time
serves to keep the vulnerability at any Ô¨Åxed point low, the
vulnerability for some point can only increase with time. The
drug dealer of our running example would be foolhardy to
believe that he is safe from a police raid just because it is
unlikely to happen on Wednesday, or any other Ô¨Åxed day;
if stakeouts continue and the police are smart enough to not
schedule drug busts before having performed the surveillance,
the dealer will be caught. On the other hand, if there is a
high enough cost associated with making an observation, the
vulnerability against raids can be very effectively bounded (as
we show in the next experiment).

In fact,

the monotonically increasing vulnerability is a
property of any scenario where the adversary can decide when
to attack.

Theorem 9. Given any gain function g that is invariant in T
(the value hist.tmax), we have Dg (evaluate (t+1) ...) ‚â•
Dg (evaluate t ...).

Proof: (Sketch) The theorem holds as an adversary attack-
ing a system with T = t+1 will have a chance to attack at time
t, using the same exact gain function that the adversary would
attack were T = t, and using the same inputs. We assumed
the gain functions are invariant in T hence their gains must be
identical. Naturally in the Ô¨Årst situation, the attacker can also
wait if the expectation of gain due to waiting one more time
step is higher.

cost 0.12
cost 0.10
cost 0.08

cost 0.06
cost 0.04
cost 0.02

cost 0.00

0

1

2

3

4

5

6

7

8

9

10

11

12

T = max number of stakeouts before (maybe) raid

i

 

n
a
g
d
a
r
 

i

d
e
t
c
e
p
x
e

i

 

n
a
g
d
a
r
 

i

d
e

t
c
e
p
x
e

 0.6

 0.4

 0.2

 0

 0.8

 0.6

 0.4

 0.2

 0

9.

Expected raid_option gain with

Fig.
stakeouts
stakeout_option and moving stash move_stash 4 with (top)
(bottom)
non-wait-adaptive
wait-adaptive adversaries (waitadapt = true), all not
low-adaptive
(loworder = Some rotate).

(waitadapt = false)

adversaries

costly

and

cost 0.1430
cost 0.1425
cost 0.1420
cost 0.1415

cost 0.1410
cost 0.1405
cost 0.1400

i

n
a
g

 

i

d
a
r
 
d
e

t
c
e
p
x
e

 0.02

 0.01

 0

0

1

2

3

4

5

6

7

8

9

10

11

12

T = max number of stakeouts before (maybe) raid

Fig. 10.
(Zoomed in) expected raid_option gain with costly
stakeouts stakeout_option and moving stash move_stash 4 with
(top) non-wait-adaptive adversaries (waitadapt = false) and (bot-
tom) wait-adaptive adversaries (waitadapt = true), all not low-adaptive
(loworder = Some rotate).

type L = E = int option
type O = bool option

(* int in 0,...,7 *)

let stakeout_option: sysf =

fun (hist: history) ->

match last hist.lows with

| None -> None
| Some stakeout ->

Some (last hist.highs = stakeout)

However, each stakeout performed will have a cost that is
applied to the Ô¨Ånal gain (c per stakeout). Additionally, the
police are penalized ‚àí1.0 units for a raid on the wrong place,
and have the option of not raiding at all (for 0.0 gain).

let raid_option (cost: float): gainf =

fun (hist: history) (exp: E ) ->

let raid_gain =

match exp with

| None -> 0.0
| Some raid_loc ->

D. Can gain be bounded by costly observations?

Our model is general enough to express costs associated
with observations. For example, we can modify our scenario so
that the police either observe nothing (indicated by low input
and output None), or perform a stakeout.

if last hist.highs = raid_loc

then 1.0
else -1.0 in

let stakeouts = (count_some hist.lows) in

(* count how many low inputs in

hist.lows were not None *)

raid_gain - cost * stakeouts

551

The results of this scenario are summarized in Figure 9 for
stakeout costs ranging from 0.00 to 0.12 and in Figure 10
for higher costs in the range 0.1400 to 0.1430. In the top
half of the Ô¨Årst Ô¨Ågure, the police do not have a choice of
when to attack and the optimal behavior is to only perform
stakeouts in the epoch that the raid will happen, resulting in
the periodic behavior seen in the Ô¨Ågure. Higher costs scale
down the expected gain.

For wait-adaptive adversaries the result is more interesting.
For sufÔ¨Åciently small stakeout costs, the adversary will keep
performing stakeouts at all times except for the time period
right before the change in the stash location and attack only
when the stash is pinpointed. This results in the temporary
plateau every 4 steps seen in the bottom half of Figure 9.
This behavior seems counter-intuitive as one would think the
stakeout costs will eventually make observations prohibitively
expensive. Every epoch, however, is identical in this scenario,
the stash location is uniform in 0, ..., 7 at the start, and the
adversary has 4 observations before it gets reset. If the optimal
behavior of the adversary in the Ô¨Årst epoch is to stakeout (at
most 3 times), then it is also optimal for them to do it during
the second (if they have not yet pinpointed the stash). It is still
optimal on the (n + 1)th epoch after failures in the Ô¨Årst n. As
it is, the expectation of gain due to 3 stakeouts is higher than
no guesses in any epoch, despite the costs.

The optimal behavior is slightly different for high enough
stakeout costs but the overall pattern remains the same. Fig-
ure 10 shows the result of an adversary staking out in an epoch
only if he is allowed enough time to attack at the end of the
epoch. That is, for T equal to 1 or 2, the police would not
stakeout at all, but if T is 3, they will stakeout at times 1,2,
and 3. This is because the expected gain given 1 or 2 stakeouts
is lower than 0, but for 3 stakeouts, it is greater than 0. As
was the case with the lower cost, since the expected gain of
observing in an epoch (3 times) is greater than not, the optimal
adversary will continue to observe indeÔ¨Ånitely if he is given
the time.

Note however, that observing indeÔ¨Ånitely in these examples
does not mean that the expected gain approaches 1. Analytical
(cid:2)n‚àí1
analysis of this scenario tells us that after n full epochs, the
i=0 (5/8)i and in the limit,
expected gain is 1/8(3 ‚àí 21c)
this quantity approaches 1 ‚àí 7c. The adversary‚Äôs gain is thus
bounded by 1 ‚àí 7c for varying stakeout costs c (the point at
which it is optimal to not observe at all is c = 1/7 ‚âà 0.1429).

The fact that the adversary‚Äôs gain can be bounded arbitrar-
ily close to 0, despite their strategy of performing stakeouts
indeÔ¨Ånitely, highlights a shortcoming of our present model:
the assumption of zero-sum relationship between adversary
and system. By quantifying optimal adversary‚Äôs gain, we are
implicitly assuming that their gain is our loss. This, however,
is hard to justify in some situations like the example of this
section. Under the optimal adversary, the drug dealer‚Äôs stash
will be discovered despite the bounded expected police gain
from said discovery. Ideally the drug dealer‚Äôs gain should be a
different function than the negation of the police‚Äôs gain. This
idea, and the more game theoretic scenario it suggests, forms
part of our ongoing work.

2-0.0
2-1.0
2-2.0
2-3.0
2-4.0
2-5.0
2-6.0
2-7.0

2-0.0

2-1.0

2-2.0

2-3.0

2-4.0

2-5.0

i

 

n
a
g
d
a
r
 

i

d
e
t
c
e
p
x
e

f

c
n
u
h
g
h

i

 
f

o
 
y
t
i
l
i

b
a
r
e
n
u
v
 
]

l

b
o
r
p

[

0

1

2

3

4

5

T = max number of stakeouts before raid

6

7

8

9

10

11

12

r=5

r=4

r=3

r=2

r=1

0

1

3

10
2
T = max number of stakeouts before guess

4

5

6

7

8

9

11

12

Fig. 11.
Changing stash according to gangs_move with stakeouts
stakeout_building quantifying (top) expected raid raid_apartment
gain and (bottom) expected high_func vulnerability, all with wait-adaptive
adversaries (waitadapt = true) and no low choices (type L = unit).

E. Does more frequent change necessarily imply less gain?

In our last example, we demonstrate a counter-intuitive fact
that more change is not always better. So far we have used a
high-input strategy that is known to the attacker but here he
will only know the distribution over a set of possible strategies.
Furthermore, guessing the full secret will require knowledge
of the high-input strategy and therefore will require change to
occur sufÔ¨Åciently many times.

be

the

Let

nbuilds

number

example

be
Ô¨Ågure,

buildings
5),
be

of
nbuilds will

(in
the
and
of
nfloors = factorial (nbuilds-1)
Ô¨Çoors in each of
these buildings. The (nbuilds) shady
buildings are in a city where illicit stashes tend to be found.
Each building has nfloors Ô¨Çoors and each Ô¨Çoor of each
building is claimed by a drug-dealing gang. A single gang has
the same-numbered Ô¨Çoor in all the buildings. That is, gang 0
will have Ô¨Çoor 0 claimed in every building, gang 1 will have
Ô¨Çoor 1 in every building, and so on.

number

the

The police know there is a stash hidden on some Ô¨Çoor in
some building and that every gang moves their stash once in
a while from one building to another in a predictable pattern
(the Ô¨Çoor does not change). They know all nbuilds gangs
each have a unique permutation œÄ of 0, ..., nbuilds-1 as their
stash movement pattern. The police also know which Ô¨Çoors
belong to which gangs (and their permutation). Given r as the
stash movement rate parameter, the movement of the stash is
governed by the following function:

type H = {building: int; floor: int}

(* building int in 0,...,4 *)
(* floor int in 0,...,23 *)

let gang_move: highf =

let gang = (random_int ()) mod nfloors in
let œÄ = (gen_all_permutations nbuilds).(gang) in

fun (hist: history) ->

552

let c_floor = (last hist.highs).floor in
let c_build = (last hist.highs).building in
if hist.t > 0
then begin

if t mod r = 0

then {building = œÄ (c_build);

floor

= c_floor}

else stash

end else

{ building = (random_int ()) mod nbuilds;

floor

= c_floor }

in gang_move

The function Ô¨Årst picks a random gang and generates the per-
mutation that represents that gang‚Äôs stash movement pattern.
It then creates a high-input strategy that will perform that
movement, keeping the Ô¨Çoor the same, while moving from
building to building (the function picks a random building at
time 0).

The police set up stakeouts to observe all the buildings but
are only successful at detecting activity half the time, and they
cannot tell on which Ô¨Çoor the stash activity takes place, just
which building:
type L = unit
type O = int option

(* int in 0,...,4 *)

let stakeout_building: sysf =

fun (hist: history) ->

if flip 0.5

then Some (last hist.highs).building
else None

The police want to raid the stash but cannot get a warrant for
the whole building, they need to know the Ô¨Çoor too:
type E = H

let raid_apartment: gainf =

fun (hist: history) (exp: E ) ->

let build = (last hist.highs).building in
let floor = (last hist.highs).floor in
if build = exp.building &&

floor = exp.floor

then 1.0 else 0.0

Now, the chances of a successful police raid after a varying
number of stakeouts depends on the stash change rate r.
Unintuitively, frequent stash changes lead the police to the
stash more quickly. Figure 11(top) shows the gain in the raid
after various number of stakeouts, for four different stash
change rates (nbuilds = 5). The chances of a failed observation
in this example are not important and are used to demonstrate
the trend in gain over a longer period of time. Without this
randomness, the gains quickly reach 1.0 after r‚àó(nbuilds‚àí1)
observations (exactly enough to learn the initially unknown
permutation).

The example has a property that the change function (the
permutation œÄ of the gang) needs to be learned in order to
determine the Ô¨Çoor of the stash accurately. Observing inÔ¨Ånitely
many stakeouts of the same building would not
improve
police‚Äôs chance beyond 1 in nfloors; learning the high-input
strategy here absolutely requires learning how it changes the
secret. One can see the expected progress in learning the high-
input strategy in Figure 11(bottom), note the clear association
between knowing the strategy and knowing the secret. We

theorize that this correlation is a necessary part of examples
that have the undesirable property that more change leads to
more vulnerability. Characterizing this for scenarios in general
is another part of our ongoing work.

VII. RELATED WORK

Other works in the literature have considered systems with
some notion of time-passing. Massey [36] considers systems
that can be re-executed several times, whereby new secret and
observable values are produced constantly. He conjectured that
the Ô¨Çow of information in these systems is more precisely
quantiÔ¨Åed by directed information, a form of Shannon entropy
that takes causality into consideration, which was later proved
correct by Tatikonda and Mitter [30]. Alvim et al. use these
works to build a model for interactive systems [28], in which
secrets and observables may interleave and inÔ¨Çuence each
other during an execution. The main differences between their
model and ours are: (i) they see the secret growing with time,
rather than evolving; (ii) they consider Shannon-entropy as a
metric of information, rather than vulnerability metrics; and
(iii) they only consider passive adversaries.

K√∂pf and Basin [15] propose a model for adaptive attacks
on deterministic systems, and show how to calculate bounds
on the information leakage of such systems. Our model
generalizes theirs in that we consider probabilistic systems.
Moreover, we distinguish between the adversarial production
of low inputs and of exploits, and allow adversaries to wait
until the best time to attack, based on observations of the
system, which itself could be inÔ¨Çuenced by the choice of low
inputs.

In the context of Location Based Services, the privacy of a
moving individual is closely related to the amount of time he
spends in a certain area, and Marconi et al. [37] demonstrate
how an adversary with structured knowledge about a user‚Äôs
behavior across time can pose a direct threat to his privacy.

The work of Shokri et al. [38] strives to quantify the privacy
of users of location-based services using Markov models
and various machine learning techniques for constructing and
applying them. Location privacy is a useful application of
our framework, as a principal‚Äôs location may be private, and
evolves over time in potentially predictable ways. Shokri et
al.‚Äôs work employs two phases, one for learning a model of
how a principal‚Äôs location could change over time, and one
for de-anonymizing subsequently observed, but obfuscated,
location information using this model. Our work focuses on
information theoretic characterizations of security in such
applications, and permits learning the change function and the
secrets in a continual, interleaved (and optimized) fashion. That
said, Shokri et al‚Äôs simpler model and approximate techniques
allows them to consider more realistic examples than those
described in our work. Subsequent work [39] considers even
simpler models (e.g., that a user‚Äôs locations are independent).

Classic models of QIF in general assume that the secret
is Ô¨Åxed across multiple observations, whereas we consider
dynamic secrets that evolve over time, and that may vary as
the system interacts with its environment. Some approaches
capture interactivity by encoding it as a single ‚Äúbatch job‚Äù exe-
cution of the system. Desharnais et al. [27], for instance, model
the system as a channel matrix of conditional probabilities of

553

whole output traces given whole input traces. Besides creating
technical difÔ¨Åculties for computing maximum leakage [28],
this approach does not permit low-adaptive or wait-adaptive
adversaries, because it lacks the feedback loop present in our
model.

O‚ÄôNeill et al. [13], based on Wittbold and Johnson [11],
improve on batch-job models by introducing strategies. The
strategy functions of O‚ÄôNeill et al. are deterministic, whereas
ours are probabilistic. And their model does not support wait-
adaptive adversaries. So our model of interactivity subsumes
theirs.

Clark and Hunt [12], following O‚ÄôNeill et al., investigate
a hierarchy of strategies. Stream strategies, at the bottom of
the hierarchy, are equivalent to having agents provide all their
inputs before system execution as a stream of values. So with
stream strategies, adversaries must be non-adaptive. Clark and
Hunt show that, for deterministic systems, noninterference
against a low-adaptive adversary is the same as noninterfer-
ence against a non-adaptive adversary. This result does not
carry over to quantiÔ¨Åcation of information Ô¨Çow; low-adaptive
adversaries derive much more gain than non-adaptive ones as
seen in Section VI-B. At the top of Clark and Hunt‚Äôs hierar-
chy, strategies may be nondeterministic, whereas our model‚Äôs
are probabilistic. Probabilistic choice reÔ¨Ånes nondeterministic
choice [40], so in that sense our model is a reÔ¨Ånement of Clark
and Hunt‚Äôs. But probabilities are essential for information-
theoretic quantiÔ¨Åcation of information Ô¨Çow. Clark and Hunt
do not address quantiÔ¨Åcation, instead focusing on the more
limited problem of noninterference. Nor does their model
support wait-adaptive adversaries. There are other, nonessential
differences between our model and Clark and Hunt‚Äôs models:
Clark and Hunt allow a lattice of security levels, whereas we
allow just high and low; our model only produces low outputs,
whereas theirs permits outputs at any security level; and our
computation model is probabilistic automata, whereas theirs is
labeled transition systems.

VIII. CONCLUSIONS AND FUTURE WORK

In this paper we have presented a new model for quantify-
ing the information Ô¨Çow of a system whose secrets evolve
over time. Our model involves an adaptive adversary, and
characterizes the costs and beneÔ¨Åts of attacks on the system.
We showed that an adaptive view of an adversary is crucial
in calculating a system‚Äôs true vulnerability which could be
greatly underestimated otherwise. We also showed that though
adversary uncertainty can effectively be recovered if the secret
changes, if the adversary can adaptively wait to attack, vulner-
ability can only increase in time. Also, contrary to intuition,
we showed that more frequent changes to secrets can actually
make them more vulnerable.

Our future work has three main thrusts. Firstly we are
generalizing our model further to give the user non-Ô¨Åxed
decisions and goals distinct from those of the adversary. Doing
so will let us better model examples like that of Section VI-D
where adversary gain does not quite mirror the loss of the
user. Furthermore, to handle scenarios in which the user and
adversary do not fully observe each other‚Äôs decisions, or when
they make decisions simultaneously, we are looking into a
game-theoretic analysis of the problem using Bayesian games
and their equilibria.

The second avenue of our future work is information
theoretical characterizations of various phenomena present in
our model which we have hinted at in this paper:

‚Ä¢ In Section VI-B we saw that the impact of low-adaptive
adversaries range from none to an exponential difference
in gain. It is easier, however, to analyze non-adaptive
adversaries. It would be valuable to be able to tell when
ignoring low-adaptivity will not have a signiÔ¨Åcant impact
on the resulting analysis. This would in effect be the
quantiÔ¨Åed version of the theorem of Clark and Hunt [12]
which states that non-adaptive strategies are sufÔ¨Åcient to
ascertain non-interference in deterministic systems.

‚Ä¢ In Section VI-E we saw how changing the secret more
often is not always preferable to changing it less. We con-
jectured that such situations require a strong correlation
between the secret and the high-input strategy used to
evolve the secret. Precisely characterizing this correlation
and the contexts in which it is relevant would be useful
for building more robust systems.

‚Ä¢ The analyses in the paper are framed in the context of
a system. Unfortunately, this context directly inÔ¨Çuences
how much information is leaked, so that the less that is
known about the context, the less we can say about the
security of the system. Prior works attempt to speak of
the worst-case leakage for all possible contexts, but for
us contexts are richer in structure, making such analysis
more difÔ¨Åcult. We consider such worst-case reasoning
challenging future work.

Finally we are bringing in the works on approximate
but sound probabilistic programming [9], [34] to enable the
simulation of larger and more complex scenarios. Though
these works are only concerned with a metric similar to
vulnerability, it may be possible to extend the approach to
soundly approximate dynamic gain as well. Additionally exact
probabilistic programming systems with smarter representa-
tions of distributions such as algebraic decision diagrams in
[16] could potentially be applied to our model. We are also
investigating this possibility.

Acknowledgements: We thank Mudhakar Srivatsa and An-
dre Scedrov for useful discussions about this work. We also
gratefully thank the anonymous reviewers and our shepherd
for their helpful feedback and comments.

For Mardziel and Hicks, this research was sponsored by
US Army Research laboratory and the UK Ministry of Defence
and was accomplished under Agreement Number W911NF-06-
3-0001. The views and conclusions contained in this document
are those of the authors and should not be interpreted as
representing the ofÔ¨Åcial policies, either expressed or implied,
of the US Army Research Laboratory, the U.S. Government,
the UK Ministry of Defense, or the UK Government. The US
and UK Governments are authorized to reproduce and dis-
tribute reprints for Government purposes notwithstanding any
copyright notation hereon. Alvim was supported in part from
the AFOSR MURI ‚ÄúScience of Cyber Security: Modeling,
Composition, and Measurement‚Äù as AFOSR grant FA9550-11-
1-0137. Much of this research was conducted while Alvim was
a postdoctoral research associate in the Mathematics Depart-
ment at the University of Pennsylvania under the supervision

554

of Prof. Andre Scedrov, whom we gratefully acknowledge.
Clarkson was supported in part by AFOSR grant FA9550-12-
1-0334.

REFERENCES

[1]

I. S. Moskowitz, R. E. Newman, and P. F. Syverson, ‚ÄúQuasi-anonymous
channels,‚Äù in Proc. of CNIS.

IASTED, 2003, pp. 126‚Äì131.

[2] D. Clark, S. Hunt, and P. Malacaria, ‚ÄúQuantitative information Ô¨Çow,
relations and polymorphic types,‚Äù J. of Logic and Computation, vol. 18,
no. 2, pp. 181‚Äì199, 2005.

[3] K. Chatzikokolakis, C. Palamidessi, and P. Panangaden, ‚ÄúAnonymity
protocols as noisy channels,‚Äù Inf. and Comp., vol. 206, no.
2‚Äì4, pp. 378‚Äì401, 2008. [Online]. Available: http://hal.inria.fr/inria-
00349225/en/

[4] G. Smith, ‚ÄúOn the foundations of quantitative information Ô¨Çow,‚Äù in
Proceedings of the Conference on Foundations of Software Science and
Computation Structures (FoSSaCS), 2009.

[5] M. R. Clarkson, A. C. Myers, and F. B. Schneider, ‚ÄúQuantifying
information Ô¨Çow with beliefs,‚Äù Journal of Computer Security, vol. 17,
no. 5, pp. 655‚Äì701, 2009.

[6] M. Backes, B. K√∂pf, and A. Rybalchenko, ‚ÄúAutomatic discovery and
quantiÔ¨Åcation of information leaks,‚Äù in IEEE Security and Privacy,
2009.

[7] C. Mu and D. Clark, ‚ÄúAn interval-based abstraction for quantifying
information Ô¨Çow,‚Äù Electron. Notes Theor. Comput. Sci., vol. 253, pp.
119‚Äì141, November 2009.

[8] B. K√∂pf and A. Rybalchenko, ‚ÄúApproximation and randomization for

quantitative information-Ô¨Çow analysis,‚Äù in CSF, 2010.

[9] P. Mardziel, S. Magill, M. Hicks, and M. Srivatsa, ‚ÄúDynamic enforce-
ment of knowledge-based security policies,‚Äù in Proceedings of the IEEE
Computer Security Foundations Symposium (CSF), 2011.

[10] R. Segala, ‚ÄúModeling and veriÔ¨Åcation of randomized distributed real-
time systems,‚Äù Ph.D. dissertation, Massachusetts Institute of Technol-
ogy, Jun. 1995, tech. Rep. MIT/LCS/TR-676.
J. T. Wittbold and D. Johnson, ‚ÄúInformation Ô¨Çow in nondeterministic
systems,‚Äù in Proceedings of the IEEE Symposium on Security and
Privacy (S&P), 1990, pp. 144‚Äì161.

[11]

[12] D. Clark and S. Hunt, ‚ÄúNon-interference for deterministic interactive
the Workshop on Formal Aspects in

programs,‚Äù in Proceedings of
Security and Trust, 2008, pp. 50‚Äì66.

[13] K. R. O‚ÄôNeill, M. R. Clarkson, and S. Chong, ‚ÄúInformation-Ô¨Çow
security for interactive programs,‚Äù in Proceedings of the IEEE Computer
Security Foundations Symposium (CSF), 2006, pp. 190‚Äì201.

[14] M. S. Alvim, K. Chatzikokolakis, C. Palamidessi, and G. Smith,
‚ÄúMeasuring information leakage using generalized gain functions,‚Äù in
Proceedings of the IEEE Computer Security Foundations Symposium
(CSF), 2012.

[15] B. K√∂pf and D. Basin, ‚ÄúAn information-theoretic model for adaptive
the ACM Conference on

side-channel attacks,‚Äù in Proceedings of
Computer and Communications Security (CCS), 2007.

[16] A. D. Gordon, T. A. Henzinger, A. V. Nori,

and S. K.
Rajamani, ‚ÄúProbabilistic programming,‚Äù in International Conference on
Software Engineering (ICSE, FOSE track), 2014. [Online]. Available:
http://research.microsoft.com/pubs/208585/fose-icse2014.pdf

[17] D. Denning, Cryptography and Data Security. Reading, Massachusetts:

[18]

Addison-Wesley, 1982.
J. Y. Halpern, Reasoning about Uncertainty.
sachusetts: MIT Press, 2003.

Cambridge, Mas-

[19] P. Malacaria, ‚ÄúAssessing security threats of

looping constructs,‚Äù
in Proceedings of
the 34th ACM SIGPLAN-SIGACT Symposium
2007,
on
and
Nice,
M. Felleisen, Eds. ACM, 2007, pp. 225‚Äì235. [Online]. Available:
http://doi.acm.org/10.1145/1190216.1190251

Languages,
2007, M. Hofmann

Principles
France,

of
January

Programming

17-19,

POPL

[20] P. Malacaria and H. Chen, ‚ÄúLagrange multipliers and maximum infor-
mation leakage in different observational models,‚Äù in Proceedings of the
2008 Workshop on Programming Languages and Analysis for Security
(PLAS 2008), √ölfar Erlingsson and Marco Pistoia, Ed. Tucson, AZ,
USA: ACM, June 2008, pp. 135‚Äì146.

555

[21]

I. S. Moskowitz, R. E. Newman, D. P. Crepeau, and A. R. Miller,
‚ÄúCovert channels and anonymizing networks.‚Äù in Workshop on Privacy
in the Electronic Society 2003, 2003, pp. 79‚Äì88.

[22] M. S. Alvim, M. E. Andr√©s, and C. Palamidessi, ‚ÄúInformation Flow
in Interactive Systems,‚Äù in Proceedings of
the 21th International
Conference on Concurrency Theory (CONCUR 2010), Paris, France,
August 31-September 3, ser. Lecture Notes in Computer Science,
P. Gastin and F. Laroussinie, Eds., vol. 6269.
Springer, 2010,
pp. 102‚Äì116. [Online]. Available: http://hal.archives-ouvertes.fr/inria-
00479672/en/

[23] Massey, ‚ÄúGuessing and entropy,‚Äù in Proceedings of the IEEE Interna-

tional Symposium on Information Theory.

IEEE, 1994, p. 204.

[24] P. Malacaria, ‚ÄúAlgebraic foundations for information theoretical, prob-
abilistic and guessability measures of information Ô¨Çow,‚Äù CoRR, vol.
abs/1101.3453, 2011.

[25] Pliam, ‚ÄúOn the incomparability of entropy and marginal guesswork
in brute-force attacks,‚Äù in Proceedings of INDOCRYPT: International
Conference in Cryptology in India, ser. Lecture Notes in Computer
Science, no. 1977. Springer-Verlag, 2000, pp. 67‚Äì79.

leakage for one-try attacks,‚Äù in Proceedings of

[26] C. Braun, K. Chatzikokolakis, and C. Palamidessi, ‚ÄúQuantitative
notions of
the
25th Conf. on Mathematical Foundations of Programming Semantics,
ser. Electronic Notes in Theoretical Computer Science, vol. 249.
Elsevier B.V., 2009, pp. 75‚Äì91. [Online]. Available: http://hal.archives-
ouvertes.fr/inria-00424852/en/
J. Desharnais, R. Jagadeesan, V. Gupta, and P. Panangaden, ‚ÄúThe metric
analogue of weak bisimulation for probabilistic processes,‚Äù in LICS,
2002, pp. 413‚Äì422.

[27]

[28] M. S. Alvim, M. E. Andr√©s, and C. Palamidessi, ‚ÄúQuantitative informa-
tion Ô¨Çow in interactive systems,‚Äù Journal of Computer Security, vol. 20,
no. 1, pp. 3‚Äì50, 2012.

[29] P. Mardziel, M. S. Alvim, M. Hicks, and M. R. Clarkson, ‚ÄúQuantifying
information Ô¨Çow for dynamic secrets,‚Äù University of Maryland Depart-
ment of Computer Science, Tech. Rep. CS-TR-5035, 2014, (extended
technical report).

[30] S. Tatikonda and S. K. Mitter, ‚ÄúThe capacity of channels with feedback,‚Äù
IEEE Transactions on Information Theory, vol. 55, no. 1, pp. 323‚Äì349,
2009.
‚ÄúThe Caml language,‚Äù http://caml.inria.fr.

[31]
[32] N. Ramsey and A. Pfeffer, ‚ÄúStochastic lambda calculus and monads
of probability distributions,‚Äù in Proceedings of the ACM SIGPLAN
Conference on Principles of Programming Languages (POPL), 2002.
[33] O. Kiselyov and C. chieh Shan, ‚ÄúEmbedded probabilistic program-
ming,‚Äù in Proceedings of the Working Conference on Domain SpeciÔ¨Åc
Languages (DSL), 2009.

[34] P. Mardziel, S. Magill, M. Hicks, and M. Srivatsa, ‚ÄúDynamic enforce-
ment of knowledge-based security policies using abstract interpreta-
tion,‚Äù Journal of Computer Security, vol. 21, no. 4, pp. 463‚Äì532, 2013.
[35] B. Espinoza and G. Smith, ‚ÄúMin-entropy as a resource,‚Äù in Information

[36]

and Computation, 2013.
J. L. Massey, ‚ÄúCausality, feedback and directed information,‚Äù in Proc. of
the 1990 Intl. Symposium on Information Theory and its Applications,
November 1990.

[37] L. Marconi, R. D. Pietro, B. Crispo, and M. Conti, ‚ÄúTime warp: How

time affects privacy in lbss,‚Äù in ICICS, 2010, pp. 325‚Äì339.

[38] R. Shokri, G. Theodorakopoulos, J.-Y. L. Boudec, and J.-P. Hubaux,
‚ÄúQuantifying location privacy,‚Äù in Proceedings of the IEEE Symposium
on Security and Privacy (S&P), 2011.

[39] R. Shokri, G. Theodorakopoulos, C. Troncoso, J.-P. Hubaux, and J.-Y. L.
Boudec, ‚ÄúProtecting location privacy: optimal strategy against localiza-
tion attacks,‚Äù in Proceedings of the ACM Conference on Computer and
Communications Security (CCS), 2012, pp. 617‚Äì627.

[40] A. McIver and C. Morgan, Abstraction, ReÔ¨Ånement and Proof

for

Probabilistic Systems. New York: Springer, 2005.

