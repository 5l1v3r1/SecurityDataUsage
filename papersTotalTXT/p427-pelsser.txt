From Paris to Tokyo:

On the Suitability of ping to Measure Latency

Cristel Pelsser

Internet Initiative Japan

Tokyo, Japan

cristel@iij.ad.jp

Luca Cittadini

Roma Tre University

Rome, Italy

ratm@dia.uniroma3.it

Stefano Vissicchio
Universite catholique de

Louvain

Louvain-la-Neuve, Belgium

stefano.vissicchio@uclouvain.be

Randy Bush

Internet Initiative Japan

Tokyo, Japan

randy@psg.com

ABSTRACT
Monitoring Internet performance and measuring user qual-
ity of experience are drawing increased attention from both
research and industry. To match this interest, large-scale
measurement infrastructures have been constructed. We be-
lieve that this eﬀort must be combined with a critical review
and calibrarion of the tools being used to measure perfor-
mance.

In this paper, we analyze the suitability of ping for delay
measurement. By performing several experiments on diﬀer-
ent source and destination pairs, we found cases in which
ping gave very poor estimates of delay and jitter as they
might be experienced by an application.
In those cases,
delay was heavily dependent on the ﬂow identiﬁer, even if
only one IP path was used. For accurate delay measure-
ment we propose to replace the ping tool with an adapta-
tion of paris-traceroute which supports delay and jitter
estimation, without being biased by per-ﬂow network load
balancing.

Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations—Network monitoring; C.4 [Performance of
Systems]: Measurement techniques

General Terms
Measurement, Performance

Keywords
Ping; delay; jitter; load-balancing

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
IMC’13, October 23–25, 2013, Barcelona, Spain.
Copyright 2013 ACM 978-1-4503-1953-9/13/10 ...$15.00.
http://dx.doi.org/10.1145/2504730.2504765.

1.

INTRODUCTION

With the Internet carrying more and more critical traﬃc,
network performance becomes ever more important. Hence,
network operators need to constantly measure their net-
works in order to detect and troubleshoot performance degra-
dation which can be experienced by users and applications.
Growing interest in network performance measurement
has translated into the deployment of a number of large-
scale end-to-end measurement infrastructures such as the
RIPE Atlas [13], RIPE TTM [14], BISmark [19] and M-Lab
projects [7], and infrastructures such as the one of Sam-
Knows probes [17]. Those infrastructures make the issues
of how we measure Internet performance even more signiﬁ-
cant. For example, few basic measurement tools, like ping,
traceroute, and paris-traceroute, can be used in perfor-
mance measurements from Atlas probes.

We believe that it is imperative to calibrate these basic
tools to ensure accurate characterization of Internet perfor-
mance. We focus on ping, one of the most commonly used
tools to measure delay and jitter.

By running a few manual experiments with ping, we dis-
covered an unexpectedly high variance in the measurements
even when they were conducted within a single ISP. Hence,
we decided to take a more rigorous approach to understand
whether such a surprising delay variability depended on some
speciﬁc features of the network, on some bias of ping itself,
or both.

In this paper, we assume that a set of ﬁelds, identifying the
ﬂow to which a packet belongs, is typically used by network
devices to perform load-balancing. We rely on Augustin et
al. [1] deﬁnition’s of ﬂow. We discovered that most of the
delay variability that ping reported was due to ping sending
probes belonging to diﬀerent ﬂows. This variance is likely
due to diversity and redundancy of paths at diﬀerent layers.
In contrast, the delay variability was much less for probes
belonging to the same ﬂow. From an application perspec-
tive, this means that delay and jitter can vary from ﬂow to
ﬂow, that is, the network may not perform as expected from
ping results on speciﬁc ﬂows. More importantly, applica-
tions that use several transport channels (e.g., to transport
diﬀerent audio, video, and data streams as in videoconfer-
encing) should not assume that delay is consistent across
channels.

427The remainder of the paper is structured as follows. We
cover background information and illustrate our measure-
ment methodology in Section 2. We describe the results of
our experiments in Section 3. We compare to related work
in Section 4. Finally, we discuss the implications of our
ﬁndings in Section 5.
2. MEASURING PER-FLOW DELAY

In this section, we describe some background, introduce
the tokyo-ping tool that we developed and reﬁned based on
the work of paris-traceroute [1], and we detail our measure-
ment methods.
2.1 Background

Tuples of source IP address, source port, destination IP
address, and destination port are used to identify TCP ﬂows.
Load-balancing and redundancy mechanisms, such as Equal
Cost Multi-Path (ECMP) and Link Aggregation Group (LAG),
commonly rely on hashes over these tuples to map an in-
coming packet to an outgoing physical interface. Routers
performing layer-4 hashing often use bytes 12-19 of the IP
header and bytes 1-4 of the IP payload [1]. In the follow-
ing, we refer to a single combination of those twelve bytes
as a ﬂow-id. Note that the ﬂow-id of an ICMP packet is
composed of the type, code, and checksum.

Source port

Length

Destination port

Checksum

Figure 1: UDP header [15]. Fields in bold are part
of the ﬂow-id.

Type

Code

Identiﬁer

Checksum

Sequence Number

Figure 2: ICMP echo message [16]. Echo request
messages have type=8 and code=0. Echo reply mes-
sages have type=0 and code=0.

Type

Code
unused (zero)

Checksum

IP Header + 64 bits of payload

Figure 3:
Type and code ﬁelds are both set to 3.

ICMP port unreachable message [16].

In our experiments, we used both UDP and ICMP probes.
Figs. 1 and 2 show the structure of a UDP and an ICMP
probe, respectively. When replying to an ICMP probe, the
target host simply echoes the ICMP payload back in an echo
reply message, which looks exactly the same as the probe
except the type ﬁeld is set to 8 instead of 0 (Fig. 2). When
replying to a UDP probe, a target host generates an ICMP
port unreachable message (Fig. 3) including the oﬀending IP
header and the ﬁrst eight bytes of the oﬀending IP payload,
which map to the UDP header. Classic ping and traceroute
emit probes that do not keep the ﬂow-id constant, so their
output is aﬀected by the presence of load balancing. Paris-
traceroute [1] is a traceroute-like tool which overcomes this
limitation by keeping the ﬁelds that contribute to the ﬂow-id
(i.e., the ﬁelds in bold in Figs. 1 and 2) set to user-speciﬁed
constants.

2.2 Adapting Paris-Traceroute

To isolate delay behavior of diﬀerent ﬂows, we used a
modiﬁed version of paris-traceroute, which we called tokyo-
ping [5]. The tokyo-ping tool reports delay as the Round-
Trip Time (RTT) between a given source-destination pair
using a user-speciﬁed ﬂow-id. The main diﬀerence from
paris-traceroute is that our tool keeps the ﬂow-id of the re-
turn path constant when probing servers. Tokyo-ping sup-
ports both UDP and ICMP probes, and can be conﬁgured
to emit probes with the same length as ping probes.

To measure RTT, tokyo-ping considers the ﬂow-id of both
the probes and the responses. For ICMP probes, responses
are automatically guaranteed to keep a constant ﬂow-id. In
fact, a response to an ICMP probe contains the same pay-
load as the probe, but has a diﬀerent type value, hence a
diﬀerent checksum (Fig. 2). The return ﬂow-id cannot be
controlled by the probe source, making it impossible to ex-
plore the return paths. However, there is a one-to-one map-
ping between the ﬂow-id of the probe and the ﬂow-id of the
response.

In general, the same does not hold for UDP probes. For
UDP probes, the ﬂow-id of the response depends on the pay-
load of the ICMP error message (see Fig. 3), which is the
IP header followed by the ﬁrst eight bytes of the probe, i.e.,
the UDP header (Fig.1). Note that the UDP payload inﬂu-
ences the UDP checksum, which in turn inﬂuences the ICMP
checksum in the response. The original paris-traceroute only
supports control of the return ﬂow-id when targeting routers.
We extend the technique in [1] to predict the ICMP message
generated at a destination host (rather than an intermedi-
ate router) and then craft the UDP payload of the probe to
keep the UDP checksum constant, yielding a constant return
ﬂow-id.

Further, using UDP probes with tokyo-ping, we were able
to isolate the separate contributions of the forward and re-
turn paths to the RTT variability. This has been done by
comparing the RTT on paths with the same forward path
and on paths with the same return path (see Section 3.4).

Unfortunately, tokyo-ping is unable to control the return
ﬂow-id when targeting some operating systems (e.g., Linux)
that include the full IP payload in ICMP error messages [2].
In this case, crafting the UDP payload makes little sense
because the sum of the UDP data and UDP checksum is
constant, so we cannot control the ﬂow-id of the response.
In such cases, we resorted to ICMP probes.
2.3 Measurement Methodology

We used tokyo-ping, experimenting with both ICMP and
UDP probes, to measure diﬀerent ﬂows between source-
destination pairs. Probes were sent with a TTL value large
enough not to expire before reaching the target host.

During each run, we sent probes with 100ms spacing in
order to reduce the likelihood of ICMP rate limiting at the
destination. For each run, we sent 100 sequential classic
pings followed by 100 probes for each diﬀerent ﬂow-id to be
tested. We repeated this procedure 100 times. The inter-
leaving of probes reduced the risk of having speciﬁc ﬂow-ids
biased by temporary network events during the experiment
(e.g., routing changes). In such cases multiple or all ﬂow-ids
were likely to be aﬀected by the event. Additionally, running
the experiment 100 times improved statistical signiﬁcance.
Finally, we compared the distribution of RTTs returned by
normal ping with the distribution of RTTs obtained for each

428Note that the RTT range between diﬀerent ﬂow-ids is small
(4ms). The same does not always hold in the experiments
we performed.

We stress that the RTT variability measured within each
individual ﬂow-id is quite low, especially compared to the
variability measured by ping.
In particular, ping reports
jitter approximately ﬁve times greater than the largest per
ﬂow-id jitter. The apparent discrepancy of ping measuring
lower RTTs than the minimum observed by setting the ﬂow-
ids is due to the fact that the six ﬂow-id values did not cover
the full range of treatment that the ﬂows might experience.
Repeating the experiment with 32 ﬂow-ids resulted in the
ping RTTs being in the same range as the RTTs measured
setting the ﬂow-ids.

Seeing this diﬀerence in performance among diﬀerent ﬂow-
ids, we decided to use paris-traceroute to enumerate the IP-
level paths between the two hosts. We found a surprisingly
high number of IP paths, mainly due to Equal Cost Multi-
Path (ECMP) conﬁgured over transoceanic links by an in-
termediate Tier-1 ISP.

3.2 Crossing the US using a single ISP

The above experiment showed that, in the presence of
diﬀerent IP paths, there may be diﬀerences in the RTT ob-
served by packets of diﬀerent ﬂows. To ensure we completely
understood what was happening, we decided to perform an
even more controlled experiment. We sent ICMP probes
from a server in Dallas to a server in Ashburn. The probes
crossed a single IP path in a single ISP.

Fig. 5 shows the results of this experiment. In this case,
the CDF for ping is not quantized because the source is a
FreeBSD host running a version of ping that does not round
RTT to three digits. In addition to the black staircase show-
ing the CDF of RTT as measured by ping, we plot another
staircase (shown in red) which is obtained by merging all of
the RTT measures with diﬀerent ﬂow-ids and applying the
same rounding. The red CDF is a very good approxima-
tion of the black one. This shows that the RTT distribu-
tion of ping is actually a sample from the collection of all
distributions for diﬀerent RTTs. The slight diﬀerences be-
tween the red and the black staircases are likely because our
merged distribution assumes that each ﬂow-id contributes
equally to the sample, which might not be the case with
ping. Moreover, 32 ﬂow-ids may not be enough to capture
the full variability observed by ping.

Looking at the per ﬂow-id curves, we see that the diﬀer-
ence between the RTTs of the ﬂows with highest and lowest
RTTs is even larger than in the ﬁrst experiment, while we
again observe low variability within each ﬂow-id. Traditional
ping can reliably estimate the upper and lower bounds of the
distribution, but substantially overestimates jitter.

This result puzzled us. Thanks to the operator, we looked
at the router conﬁgurations. Our probes crossed three lagged
hops, one within the ingress Point-of-Presence (PoP), one
across the long-haul core, and the last in the egress PoP.
Each bundle was composed of slightly less than ten links.
Could LAG cause ﬂows to experience such diﬀerent RTTs?
Experiments between Dallas and Seattle with both ECMP
and LAG showed similar behavior. We then ran two ex-
periments with sources and destinations within the same
PoP. In the ﬁrst, the source and destination hosts were both
connected to the same router. For the second, one LAG
was used between routers in the PoP. Neither of these tests

Figure 4: Per-ﬂow and ping measured RTT from a
source in Italy to a destination in the US.

distinct value of ﬂow-id. Each experiment was repeated sev-
eral times, on diﬀerent days, and at diﬀerent times of day,
to avoid bias due to timing and traﬃc patterns.

Our study diﬀers from [1] in the following aspects. First,
we focused on RTT rather than on IP-level topology discov-
ery. We made no attempt to discover paths to intermediate
routers. Second, we targeted destination hosts instead of
tuning the TTL to expire at the last router. This allowed
us to be much more conﬁdent of the distribution of RTTs,
as we were immune to the very common ICMP throttling
by routers. Further, as ICMP TTL expired message gener-
ation is done in software, the load of the destination router
strongly inﬂuences the RTT, adding to the variability of the
distribution. This was not the case with our method. Third,
we did not assume a one-to-one mapping of ﬂow-id to IP
path, in fact, the opposite. We probed diﬀerent ﬂow-ids
even when the IP-level path was exactly the same. Our ex-
periments showed that, in some cases, a single IP-level path
could exhibit signiﬁcantly diﬀerent per-ﬂow RTT distribu-
tions. Finally, we avoided the use of virtual machines as
sources and destinations, to avoid virtualization overhead’s
eﬀect on ﬁne grained timing.

3. PING, RTTS, AND JITTER

In this section we report results of our methods when ap-
plied to measure diﬀerent source-destination pairs. First, we
performed measurement in controlled environments, where
we had ground truth knowledge of the traversed paths. Then,
we conducted larger-scale experiments. We found that dif-
ferences in performance between diﬀerent ﬂow-ids can be
signiﬁcant. As a consequence, ping is in general a mediocre
estimator for RTTs and heavily overestimates jitter.
3.1 From Italy to the US

Our ﬁrst experiment ran tokyo-ping from a server in Italy
to a destination in the US, using 6 diﬀerent ﬂow-id values.
Results are depicted in Fig. 4 where each curve represents a
Cumulative Distribution Function (CDF). The black stair-
case is the CDF of RTTs as measured by ping. Each data
point (x, y) indicates that a y-fraction of ping probes ob-
served an RTT of at most x milliseconds. The ping CDF is
quantized because the Linux version of ping imposes a three
digit precision on the reported RTTs. Each colored curve
represents the CDF of RTTs measured for diﬀerent ﬂow-ids.

1041051061071081091100.00.20.40.60.81.0RTT (ms)CDFflow−id=1flow−id=2flow−id=3flow−id=4flow−id=5flow−id=6ping429Figure 5: Experiment 2 - Performance of probes
with and without ﬁxed ﬂow-id on an single IP path
from Dallas to Ashburn.

Figure 6: Contribution of return paths. Each color
represents a diﬀerent return ﬂow-id. Data are nor-
malized based on the minimum RTT experienced for
a single forward ﬂow-id.

showed any RTT diﬀerence for diﬀerent ﬂow IDs. This made
us suspect that something special happened on the long-haul
inter-PoP LAG.
3.3 Pruning out some potential causes

We could see a few potential culprits for the per-ﬂow per-
formance behavior, namely (1) the use of ICMP probes, (2)
the traﬃc load carried by the network, (3) synchronization
eﬀects due to the interleaving of probes, (4) MPLS settings,
(5) the conﬁguration of hashing functions and hash keys, (6)
the diversity of links being bundled, or (7) the LAG vendor
implementation. To test whether ICMP probes were the
culprit, we sent UDP probes from Ashburn to Dallas. We
set the header ﬁelds in such a way as to control both the
forward and the return path ﬂow-ids. We used 6 diﬀerent
values for the forward ﬂow-id and 6 diﬀerent values for the
return ﬂow-id, resulting in 36 diﬀerent RTT distributions. In
order to quantify the contribution of the forward and return
ﬂow-id separately, we normalized the data across forward
ﬂow-ids by taking the diﬀerence with respect to the mini-
mum RTT value measured with that forward ﬂow-id (and
across return ﬂow-id). Fig. 6 shows the CDFs of the RTT
diﬀerences, where each color represents a diﬀerent value of
the return ﬂow-id. Observe that the distribution of the RTT
diﬀerence is consistent across diﬀerent forward ﬂow-ids, in-
dicated by the fact that the distributions for the same return
ﬂow-id (i.e., same color) are not scattered around the plot.
We perform a similar analysis to isolate the contribution of
the forward ﬂow-id, shown in Fig. 7. The ability to clearly
isolate the contributions of forward and return ﬂow-ids also
indicates that the RTT diﬀerences are not measurement ar-
tifacts.

Cross-traﬃc cannot be the culprit for the RTT diﬀerences
either. Running the Dallas - Ashburn experiment at dif-
ferent times of the day, diﬀerent days of the week, and for
diﬀerent durations led to the same graphs, with the speciﬁc
ﬂow-ids always mapping to the corresponding RTT distri-
bution. Discussion with the operator also conﬁrmed that
the cross-traﬃc was very low, as the traﬃc load never ex-
ceeded 50% of link capacity during our experiments. With
respect to synchronization eﬀects, we repeated the experi-
ment using a diﬀerent inter-probe spacing of 157 ms, i.e., a
prime number reasonably far from multiples of 100 ms (used
in all our experiments), and obtained identical results. We

Figure 7: Contribution of forward paths. Each color
represents a diﬀerent forward ﬂow-id. Data are nor-
malized based on the minimum RTT experienced for
a single return ﬂow-id.

excluded MPLS settings and hashing speciﬁcs by checking
router conﬁgurations in collaboration with network opera-
tors. This left us with LAG bundle physical link path diver-
sity, LAG vendor implementation, or more obscure reasons
as potential causes.

Despite our precautions to avoid VMs and routers as end-
points, and to cross networks for which we could get ground
truth, we have not been able to pinpoint the exact causes
of per-ﬂow behavior. We know two major causes, LAG and
ECMP. The ECMP issue is obvious, diﬀerent path lengths,
equipment, . . . LAG is more subtle, and can be any combina-
tion of varied hashing algorithms on line cards (for resilience,
operators usually use multiple line cards on a single LAG),
varied circuit paths, etc. And serious diagnosis of LAG vari-
ance is severely hampered by lack of vendor instrumentation,
e.g. one can not look into the indvidual circuits’ queues.
3.4 Collecting More Evidence

To ensure that our experiments did not just discover a few
corner cases, we ran ping and tokyo-ping on a larger num-
ber of Internet-wide source-destination pairs. Our sources
were FreeBSD servers in Dallas, Ashburn and Seattle, plus
a Linux server in Rome. As destinations, we used a subset

30354045500.00.20.40.60.81.0RTT (ms)CDF010203040500.00.20.40.60.81.0(rtt[x,y] − min_z(rtt[x,z])) / min_z(rtt[x,z]) * 100 (%)CDFreturn flow−id=1return flow−id=2return flow−id=3return flow−id=4return flow−id=5return flow−id=6010203040500.00.20.40.60.81.0(rtt[x,y] − min_z(rtt[z,y])) / min_z(rtt[z,y]) * 100 (%)CDFfwd flow−id=1fwd flow−id=2fwd flow−id=3fwd flow−id=4fwd flow−id=5fwd flow−id=6430RTT across 100 measurements per destination and iterating
over multiple ﬂow identiﬁers, the authors conclude that, for
most paths, there is no signiﬁcant RTT diﬀerence among
ﬂow identiﬁers. This paper is close in spirit and comple-
mentary to [1], as we show that ping results are also bi-
ased by per-ﬂow load-balancing techniques. We extend [1]
in two ways. First, we show that IP is not the only layer
to be considered in load-balanced networks (see Section 3).
Path diversity may be present at lower layers given LAG or
MPLS. Second, by relying on the RTT distribution instead
of the minimum observation, we are able to show that ping
is a bad estimator for any metric related to the RTT. Es-
sentially, ping systematically overestimates RTT variability,
which is signiﬁcantly lower when measured for individual
ﬂow identiﬁers.

Previous eﬀorts were devoted to better implementation of
per-ﬂow load balancing. For example, the MPLS entropy
label [10] is intended to avoid routers going through deep
packet inspection to perform per-ﬂow balancing, which en-
ables support of high traﬃc rates in core routers. [20] high-
lights the diﬃculties in hashing encapsulated traﬃc.

The eﬀect of using diﬀerent hash functions to balance traf-
ﬁc on multiple links is studied in [4], and best practices for
optimal LAG/ECMP component link utilization while us-
ing hash-based techniques are described in [11]. In [21], the
use of a given path in data centers, e.g., to avoid conges-
tion, is achieved by modifying packet headers on the basis
of traceroute-like results.

We show that predictability is a key feature for new load
balancing methods, especially to ease diagnosis. From this
perspective, we ﬁnd interesting the load balancing technique
proposed in [6]. Work on balancing traﬃc is of major im-
portance as it is commonly used in ISP networks [1, 8] as
well as in data centers [9, 12].

5. LESSONS LEARNED

We have shown that using ping for delay measurement
may be biased if one ignores ﬂow-based load balancing. This
bias is intrinsic to ping’s design, hence predictable a priori.
In carefully crafted measurements, the dispersion reported
by ping can be up to 40% of the RTT experienced by the
ﬂow with lowest latency. In other words, when we observe
high variability in the delay measurements of ping, it is likely
a measurement artifact of the tool itself.

This observation has several consequences.
1. Ping is unﬁt to measure RTT distributions. The
distribution measured by ping is often a sample from a wider
set of per-ﬂow distributions. While this can identify upper
and lower bounds for the delay with good approximation,
it provides a mediocre estimate for delay. It overestimates
jitter (or any other metric measuring the variability of the
distribution). For this reason, it cannot reliably represent
the performance experienced by applications. This should
sound a warning both for researchers studying end-to-end
Internet performance, and operators using ping for measure-
ment or debugging purposes. We suggest that tokyo-ping,
an adaptation of paris-traceroute, be deployed on large-scale
measurement infrastructures.

2. The importance of the ﬂow identiﬁer. That a
signiﬁcant diﬀerence in latency may exist between ﬂows for
the same source and destination pair represents both a dan-
ger and an opportunity for applications. On the one hand,
applications using multiple transport channels, e.g., the con-

Figure 8: RTT variance with ping and tokyo-ping
from a single source to 850 destinations

of the Google servers discovered by [3]. As our goal was
to check the generality of the per-ﬂow RTT behavior, we
targeted distributed destinations selecting one IP per Au-
tonomous System (AS) in this dataset. This resulted in 850
destinations. We used tokyo-ping to send ICMP probes with
16 diﬀerent ﬂow-ids. We sent ten probes for each ﬂow-id and
ten ping probes. We repeated this experiment 20 times.

Fig. 8 depicts the CDF of the inter-quantile ranges of
RTT measurements that we observed using Ashburn as the
source. The green curve (on the left) and the blue curve (in
the middle) show the lowest and highest per-ﬂow-id inter-
quantile ranges, while the red curve shows the inter-quantile
range for ping. Note that the variability of the distribution
of RTTs reported by ping is systematically higher (with
very few exceptions) than the variation of the most vari-
able ﬂow-id. In particular, for 40% of the destinations, the
worst per-ﬂow inter-quantile range is above 0.3 ms while for
ping is above 5.2 ms. For 15 destinations, ping experienced
lower RTT variability than the per-ﬂow measurements (in
the bottom-left). The lack of ground knowledge of router
conﬁgurations and circuit paths in these larger-scale exper-
iments prevented us from ﬁnding clear explanations.

We observed a similar behavior for all sources. We also
performed experiments toward Alexa’s top 100 sites and ob-
tained very similar results.

Which portion of the RTT distribution exhibits per-ﬂow
behavior? When extending the range to incorporate 90%
of the RTT distribution (the 95th-5th percentile range), in-
stead of the 50% for the inter-quantile, the ﬁgure changes.
The per-ﬂow variance increases. For most destinations (90%)
the worse per-ﬂow variance is higher than the ping variance.
This is likely a sign that while there are speciﬁc per-ﬂow
behaviors, other factors are also at play. Slightly reducing
the portion of the RTT distribution we consider is enough to
reﬂect per-ﬂow behavior. From our source in Ashburn, 90%
of the destinations have an RTT distribution with 90th-10th
range lower with the ﬂow id ﬁxed than for ping. This shows
that in general 80% of the ﬂow observations are consistent.

4. RELATED WORK

Traceroute is renown to be error prone in load-balanced
networks [1]. In [1], the authors propose a replacement for
traceroute, called paris-traceroute, which takes into account
path diversity at the IP layer. By measuring the minimum

0.050.200.502.005.0020.000.00.20.40.60.81.0Interquartile rangeCDFPingWorst flow−idBest flow−id431Stateless path selection in data center networks.
Computer Networks, 57(5):1204–1216, April 2013.

[7] C. Dovrolis, K. Gummadi, A. Kuzmanovic, and S. D.

Meinrath. Measurement Lab: Overview and an
Invitation to the Research Community. SIGCOMM
CCR 2010, 2010.

[8] T. Flach, E. Katz-Bassett, and R. Govindan.

Quantifying violations of destination-based forwarding
on the Internet. In ACM Internet Measurement
Conference (IMC 2012), 2012.

[9] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula,

C. Kim, P. Lahiri, D. A. Maltz, P. Patel, and
S. Sengupta. VL2: a scalable and ﬂexible data center
network. In ACM SIGCOMM, 2009.

[10] K. Kompella, J. Drake, S. Amante, W. Henderickx,
and L. Yong. The Use of Entropy Labels in MPLS
Forwarding, November 2012. Internet Engineering
Task Force (IETF), RFC 6790.

[11] R. Krishnan, S. Khanna, L. Yong, A. Ghanwani, Ning

So, and B. Khasnabish. Mechanisms for optimal
LAG/ECMP component link utilization in networks,
April 2013. draft-krishnan-opsawg-large-ﬂow-load-
balancing-08.txt, work in
progress.

[12] J. Mudigonda, P. Yalagandula, M. Al-Fares, and J. C.

Mogul. Spain: Cots data-center ethernet for
multipathing over arbitrary topologies. In NSDI, 2010.

[13] RIPE NCC. RIPE Atlas. https://atlas.ripe.net.
[14] RIPE NCC. RIPE Test Traﬃc Measurement Service.

https://atlas.ripe.net.

[15] J. Postel. User Datagram Protocol, 1980. Internet

Engineering Task Force (IETF) , RFC 768.

[16] J. Postel. Internet Control Message Protocol, 1981.
Internet Engineering Task Force (IETF) , RFC 792.
[17] SamKnows. Samknows. http://www.samknows.com.
[18] R. Stewart. Stream Control Transmission Protocol,

2007. Internet Engineering Task Force (IETF) , RFC
4960.

[19] Georgia Tech et al. Project BISmark.

http://projectbismark.net.

[20] Jeﬀ Wheeler and Job Snijders. Understanding MPLS

hashing. NANOG 57, February 2013.

[21] K. Xi, Y. Liu, and J. Chao. Enabling ﬂow-based

routing control in data center networks using probe
and ECMP. In INFOCOM Workshop on cloud
computing (2011), page 614ˆa ˘A¸S619, 2001.

trol, video, audio and data channels found in videoconfer-
encing and streaming, cannot assume that network perfor-
mance is consistent across channels.
This implies that
multi-channel applications are advised not to rely on a sin-
gle control channel to accurately estimate delay and jitter of
all opened TCP connections. If the application needs con-
sistency across channels (e.g. to keep the lip sync in video
streaming), SCTP [18] is a good alternative as it is able to
multiplex streams while keeping a constant ﬂow identiﬁer.
On the other hand, applications might experience better per-
formance by carefully selecting the source and destination
ports, e.g., during an initial negotiation phase. Moreover,
our ﬁndings suggest that accurately monitoring per-channel
performance from outside the application is harder than is
commonly believed. Indeed, the performance that a moni-
toring tool (e.g., ping, IP-SLA, etc.) measures is not nec-
essarily representative of the performance experienced by
speciﬁc applications.

3.

Impact on common wisdom and prior work.
Our ﬁndings show how reality is often more complex than
expected. Technologies and conﬁgurations at diﬀerent lay-
ers of the protocol stack often interact in unexpected ways.
Understanding these interactions and the behavior of diﬀer-
ent vendor implementations is diﬃcult. This can frustrate
or make inaccurate the modeling eﬀort of research. Our ex-
periments show that, at least in some cases, latency over a
network path is not a well-deﬁned concept, and we should
more precisely deﬁne latency over a transport session. Gen-
erally speaking, we recommend researchers be cautious when
drawing conclusions from experiments based solely on ping
results.

Acknowledgements
We thank Olivier Bonaventure and Jean Lorchat for their in-
sightful comments. We are very grateful to several network
operators without whom this work would not have been pos-
sible. We are indebted to our shepherd Ethan Katz-Bassett
for his guidance in producing the camera-ready. This work is
partially supported by the European Commission’s Seventh
Framework Programme (FP7/2007-2013) Grant No. 317647
(Leone).

6. REFERENCES
[1] B. Augustin, T. Friedman, and R. Teixeira. Measuring

load-balanced paths in the internet. In ACM Internet
Measurement Conference (IMC 2007), 2007.

[2] F. Baker, 1995. Internet Engineering Task Force

(IETF) , RFC 1812.

[3] Matt Calder, Zi Hu Xun Fan, Ethan Katz-Bassett,
John Heidemann, and Ramesh Govindan. Mapping
the Expansion of Google’s Serving Infrastructure (To
Appear). In Proceedings of the ACM Internet
Measurement Conference (IMC ’13), October 2013.

[4] Z. Cao, Z. Wang, and E. Zegura. Performance of

hashing-based schemes for Internet load balancing. In
INFOCOM, pages 332–341, 2000.

[5] Luca Cittadini. Tokyo-ping.

http://psg.com/tokyo-ping-v2.tar.gz.

[6] Gregory Detal, Christoph Paasch, Simon van der

Linden, Pascal M ˜Al’rindol, Gildas Avoine, and Olivier
Bonaventure. Revisiting ﬂow-based load balancing:

432