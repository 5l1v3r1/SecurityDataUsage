2016 IEEE Symposium on Security and Privacy
2016 IEEE Symposium on Security and Privacy

High-Speed Inter-domain Fault Localization

∗
Cristina Basescu

∗
, Yue-Hsun Lin†, Haoming Zhang‡, Adrian Perrig

Department of Computer Science, ETH Zurich, {cba,adrian.perrig}@inf.ethz.ch

∗

†Samsung Research America, yuehhsun.lin@samsung.com

‡Carnegie Mellon University, haoming@cmu.edu

Abstract—Data-plane fault

localization enhances network
availability and reliability by enabling localization and circum-
vention of malicious entities on a network path. Algorithms
for data-plane fault localization exist for intra-domain settings,
however, the per-ﬂow or per-source state required at intermediate
routers makes them prohibitively expensive in inter-domain
settings. We present Faultprints, the ﬁrst secure data-plane fault
localization protocol that is practical for inter-domain settings.
Faultprints enables a source to precisely localize malicious net-
work links that drop, delay, or modify packets. We implemented
an efﬁcient version of Faultprints on a software router by taking
advantage of the parallelism in the AES-NI module of Intel CPUs.
Our evaluation on real-world trafﬁc shows fast forwarding on a
commodity server at 116.95 Gbps out of 120 Gbps capacity, and
a goodput of 94 Gbps. Additionally, Faultprints achieves a high
failure localization rate, while incurring a low communication
overhead.

Index Terms—data plane security; secure fault localization;

inter-domain communication; network reliability

I. INTRODUCTION

Availability is an important property to achieve in the
Internet: critical infrastructure and services require reliable
data delivery. Yet malicious and misconﬁgured entities in the
network compromise availability through data-plane attacks:
dropping, delaying, or modifying trafﬁc. Localization and
circumvention of such malicious nodes on a network path
enhances availability, and deters adversarial nodes through the
threat of identifying them.

There are numerous attack examples that compromise net-
work availability: i) First, a malicious Autonomous System
(AS) may drop, delay, or modify packets, e.g., for censorship
purposes. Even when the AS is not malicious, the AS may have
compromised routers, in an attempt to disrupt communication;
or the AS may experience conﬁguration errors. Indeed, accord-
ing to a 2014 worldwide survey [2], network infrastructure
attacks continue to account for 17% of the attack target mix,
for the second year in a row. Infrastructure outages caused by
failures or misconﬁguration rank third amongst most signif-
icant operational threats, experienced by 53% of the service
providers. ii) ASes may engage in anti-competitive behavior.
In May 2014, Level 3 Communications and Netﬂix accused
ﬁve unnamed US ISPs of congesting their interconnection
points to force content providers to reserve more ports, thus
increasing the ISPs’ revenue [1]. Congestion leads to packet
drop, and to a decrease in service availability. iii) Another
attack is trafﬁc modiﬁcation by government operations, as The

Haoming Zhang’s work was completed at ETH Zurich, and Yue-Hsun Lin’s
work was completed at Carnegie Mellon University.

Guardian reports about the NSA [5]. Such trafﬁc modiﬁcation
can manifest through trafﬁc delay along the intended path,
through packet modiﬁcation, or through trafﬁc rerouting, thus
absence of the packets along the intended path. For all these
cases, secure fault
localization enables localization of the
problem, even when malicious entities attempt to hide and
interfere with localization.

Related Work. The state of the art for localizing faulty
network links is to store at each network node a summary of
the observed packets, for a determined time period, and then
have the nodes securely send their summaries for analysis,
either on request or periodically. In one line of research, the
summaries are sent to the packets’ source [10, 11, 19, 38].
This empowers the source, but has the drawback of raising
the storage requirements at nodes with an increasing number
of sources [10, 19, 38]: namely, existing approaches requires
shared keys between each source and each network node,
which are stored at the network nodes. To relieve this storage
strain, in a second line of research, summaries are sent to fewer
entities: either to trusted direct neighbors [37], or to a trusted
centralized location [36]. In this setting, fault localization runs
collectively on a network of nodes.

Other fault

localization approaches have different goals
than Faultprints: WATCHERS [14, 20] explicitly targets single
domain fault localization, while AudIt [7] and Network Con-
fessional [8] localize packet drop and delay, but do not handle
packet modiﬁcation. Yet, packet modiﬁcation can be equiva-
lent to dropping, because the destination discards a packet with
an invalid checksum. Secure Network Provenance [39] creates
a provenance graph, which if performed on a per-packet basis
would not scale to a core router.

Unfortunately, all secure fault localization approaches that
can handle packet drop, delay, and modiﬁcation are designed
for intra-domain environments and do not scale to the size
of the Internet for the following reasons. Keeping per-packet
state on the router’s fast path,1 as well as per-ﬂow, per-source,
or per-path state, would render core routers prohibitively
expensive, which can additionally be attacked through state
exhaustion (AudIt, Network Confessional, and Secure Net-
work Provenance also share this fate); relying on a centralized
trusted entity would hinder deployment, as it is challenging
for different countries to decide who should own the root of

1A router’s fast path refers to packet forwarding in hardware, at line rate.
In contrast, the slow path refers to software packet processing, for instance
network management.

2375-1207/16 $31.00 © 2016 IEEE
© 2016, Cristina Basescu. Under license to IEEE.
DOI 10.1109/SP.2016.56
DOI 10.1109/SP.2016.56

859
859

trust; and requiring all routers to be equipped with trusted
hardware would delay deployment and complicate the router
architecture. In addition, the communication overhead should
be low, especially in benign cases, and the localization delay
should be minimized.

Table I compares several fault localization systems with
Faultprints, considering an attacker who drops, delays, and
modiﬁes packets. The ﬁrst two rows present path-based ap-
proaches: Secure sketch FL [19], and ShortMAC [38]. They
generally have a low communication overhead. ShortMAC
achieves a good balance between the communication overhead
and a small localization delay, even for 99% localization pre-
cision. But these solutions require per-source storage (routers
store symmetric keys shared with each source), and also per-
ﬂow storage for trafﬁc summaries, requiring up to 149GB
of storage for a 10Gbps link in the case of Secure sketch
FL. The storage and symmetric paths issues are addressed in
TrueNet [37] and DynaFL [36], which only require small per-
neighbor storage. But TrueNet requires trusted hardware, and
DynaFL relies on a trusted central entity, both of which are
unrealistic in an Internet-scale deployment.

Challenges and Contributions. The design decisions for
practical fault localization protocols become apparent when
we target a large-scale deployment in inter-domain settings.
As Schuchard et al. [32] show, attacks on the control plane
of the Internet can exhaust the memory of routers. In “N-
square” Distributed Denial of Service (DDoS) attacks, such
as Crossﬁre [22], N attack nodes establish O(N2) trafﬁc ﬂows
between each other. Maintaining per-ﬂow state during such
attacks puts a signiﬁcant strain on core Internet routers.

The main challenge in Faultprints is: How can we achieve
good precision to detect and localize misbehavior, while re-
quiring only per-bandwidth storage on the routers’ fast path?
Our insight is that each node on a network path can perform
sampling of the packets it observes in a local Bloom ﬁlter,
according to a function known to the source, but unknown to
other nodes. Faultprints nodes require only constant storage
because they consistently derive on the ﬂy the key for the
sampling function, and store sampled packets only for a
limited amount of time. The storage requirement is only 46.8
MB for a router with 120 Gbps throughput. Faultprints can
also support asymmetric paths, but achieving weaker security
properties. We provide an efﬁcient implementation based on
parallelism offered by AES-NI support on commodity Intel
CPUs. Our implementation forwards IPv4 Internet trafﬁc (i.e.,
in the proportion determined by a CAIDA study [16]) on an
off-the-shelf PC platform at a throughput of 116.20 Gbps out
of 120 Gbps, providing a goodput of 94 Gbps. To the best
of our knowledge, Faultprints is the ﬁrst fault localization
protocol that can operate in inter-domain settings.

A. Adversary model

We assume that an adversary can compromise any number
of ASes on a path from source to destination. The ASes under
the control of an adversary can drop, delay, and modify any
packet they forward, as well as inject packets. However, the
adversary cannot eavesdrop or inﬂuence trafﬁc on links that
are not adjacent to any of its routers, including their drop rate.
Malicious ASes can adapt their strategy in any way, in-
cluding misbehaving only when they know they cannot be
accurately localized – known as a coward attack [27]. Attack-
ers can also launch framing attacks, where malicious ASes
behave such that they incriminate other ASes of adversarial
activity. Moreover, adversaries can collude by exchanging their
cryptographic secrets, and by sharing information about the
packets traversing them.

Malicious ASes and malicious end-hosts can launch denial-
of-service (DoS) attacks by misusing Faultprints, e.g., through
unnecessary control packets and an excessive number of ﬂows
to cause router state inﬂation. However, defending against
traditional network congestion DoS attacks is out of scope
for this paper.

B. Protocol properties

We consider an Internet-scale network where a routing
protocol directs trafﬁc from a source to a destination through
intermediate ASes. Faultprints enables sources to localize the
links adjacent to the adversarial nodes on the path to the
destination, as localizing the adversarial nodes themselves was
shown to be impossible by Barak et al. [11]. Faultprints is
particularly suited for long-lived ﬂows, or several short-lived
ﬂows sent by a source along the same path.

Under our attacker model, we seek the following goals for

an Internet-scale deployment of fault localization:

• Low, ﬁxed storage at each node. The storage require-
ments at each node must allow for Internet-scale deploy-
ment, where nodes may switch several Tbps of trafﬁc.
As we seek an upper bound on the storage required, each
node’s state must not depend on the number of transiting
ﬂows, nor the number of sources that originate the ﬂows,
nor the number of destinations, nor the path length.

• Near line-rate forwarding speed. The processing la-
tency, both in the presence and absence of faults, must
allow for line-rate forwarding of packets in the fast path.
• Low communication overhead. The communication
overhead must be low, especially when there are no faults
in the network.

• High fault localization rate. Using the deﬁnition of
Zhang et al. [38], the fault localization rate is the number
of data packets needed by the protocol to localize a
malicious link, for ﬁxed false positive and false negative
localization rates.

II. PROBLEM SETTING

C. Assumptions

In this section we discuss the adversary model, describe the

properties of our protocol and our assumptions.

We assume that the source knows an AS ingress/egress
router-level symmetric path to the destination.
(An AS
ingress/egress router-level symmetric path indicates that the

860860

TABLE I: Comparison of the practicality of existing Fault Localization (FL) protocols.

FL scheme

No trusted

central entity

No trusted
hardware

Router storage per 10 Gbps link

(FP: fast path, SP: slow path)

Comm.

(extra %)

Max. eval.
throughput

Localiz. delay for

99% accuracy (pkts)

Assumptions

Overhead

Practicality

Secure sketch FL

ShortMAC

TrueNet

DynaFL

Faultprints

(cid:2)

(cid:2)

(cid:2)

X

(cid:2)

(cid:2)

(cid:2)

X

(cid:2)

(cid:2)

FP: 149.87GB1+key ∗ #src
SP: timer ∗ #slowpath pkts2
FP: 21B ∗ # f lows+key ∗ #src
SP: timer ∗ #slowpath pkts

0.0002%

No eval

0.01%

0.9Gbps

FP: 512KB3+ 40B ∗ #neighbors

0.0001%4

∼1Gbps

FP: 1.95MB5∗ #neighbors + 1key

FP: ∼46.8MB

SP: (timer+ctrl pkts) ∗ #slowpath pkts

0.002% -
0.012%

No eval

3.3%

119.7Gbps

106

3.8 ∗ 104

104

5 ∗ 104

4 ∗ 103

1 Node i stores a sketch of i2(10 − log2 i) ∗ 100B for each source sending 100pps, e.g., node 3 stores 6.14MB. For a 10Gbps core link, and 512B packets,
there are 24.41 ∗ 105 pps, and node 3 has to store 6.14 ∗ 24.41 ∗ 103MB = 149.87GB.

2 slowpath pkts denotes the protocol’s control packets, processed on the router’s slow path.
3 For the less-expensive average fault localization, with the monitoring interval of 104 pkts (paper example), 512B packets require 512KB of storage.
4 One counter report per monitoring interval (e.g., 104 packets).
5 The storage per neighbor is (cid:4) D
L

+ 1(cid:5) ∗ 20 ∗ η ∗ L, where D is the upper-bound on one-way latency (20ms in the paper), L is the epoch length (20ms), and

η is the pps rate: for a 10Gbps link and 512B packets, η = 24.41 ∗ 105.

sequence of AS ingress and egress routers on the path from the
source to the destination is the exact inverse of the path from
the destination to the source.) For simplicity of presentation,
in the remainder of the paper we abstract the ingress/egress
router operations and simply assume that an AS performs
an operation. Many paths in today’s Internet are asymmet-
ric [21],
thus, we assume proposed Internet architectures
such as SCION [35] or Pathlet routing [18] which provide
packet-carried forwarding state that can be reversed and offer
router-level symmetric paths. For ease of presentation, we
describe Faultprints in the context of SCION paths. Faultprints
could also work in the current Internet, which we discuss in
Sections X and XI.

Similar to previous protocols [7, 30, 36], Faultprints re-
quires time synchronization to localize packet delay. We
rely on nodes being loosely time-synchronized, within 100
milliseconds. In fact, today NTP allows for time synchro-
nization along intercontinental paths within several tens of
milliseconds, however, paths with large asymmetric delays
can lower accuracy on the order of 100 milliseconds or more
[29]. Since Faultprints sources have knowledge of network
paths, they can detect such cases and lower their expectations
of ﬁne-grained localization of packet delay. We recommend
clock synchronization using authenticated NTP version 4, as
unauthenticated NTP is vulnerable to attacks [28].

The source and the destination share a symmetric key KSD,
e.g., set up using SSL / TLS, IPsec, or SSH. We also require
that each AS has a public-private key pair. The ASes’ public
keys need to be known only to the source and the destination.
The source and the destination can verify these public keys
using a regular PKI, such as RPKI.

III. FAULTPRINTS OVERVIEW

We now present a high-level overview of Faultprints, to
provide intuition for how we achieve a highly scalable, secure,
accurate, and efﬁcient inter-domain fault localization protocol.
To catch malicious ASes, Faultprints relies on deterministic
packet sampling: each AS samples observed packets in a way
that is predictable by the source, but unpredictable by all




























Fig. 1: An example of Faultprints operation with 4 ASes on the
forward path. AS3 is malicious and drops the depicted DATA
packet, while the malicious AS5 may drop AS4’s PREPLY.

other network entities. Packet sampling yields to the source
a complete picture of the transit ASes that drop, delay, and
modify packets.

In essence, at

the beginning of the protocol, each ASi
establishes with the source S a secret packet sampling key
,S, enabling the prediction by the source of which packets
KASi
will be sampled by each AS. An important aspect is that ASes
,S, as this would require per-source state;
do not store KASi
instead, Faultprints uses the DRKey protocol [24] to let ASi
rapidly derive on-the-ﬂy the sampling key KASi
,S when a packet
arrives. This approach offers several important properties: (1)
is stored for key derivation by ASi,
only a single secret
regardless of the number of sources, (2) since the ASes do
not know each others’ sampling keys, a malicious AS that
wants to drop, delay, or modify a packet takes the risk of being
detected in case its neighbor AS samples that packet, and (3)
the source knows the secret packet sampling keys established
with the ASes on the path, enabling the source to predict the
sampling behavior of each AS for each packet. Although the
actual protocol needs to handle numerous corner cases and
attacks, we next present the main challenges and intuitions
for Faultprints based on a concrete example.

Consider the setting of Figure 1. The source S commu-
nicates with the destination D over 4 intermediate ASes, of
which AS3 is malicious. Faultprints adds to the default data
trafﬁc (denoted by DATA) control packets of three types:
(i) DACK: destination acknowledgement, (ii) PREQ: probe

861861

requests to investigate what happened to a lost DATA packet,
and (iii) PREPLY: probe replies carrying reports back to S.
Each ASi decides whether to sample the data packet shown in
Figure 1 by computing a thresholding packet sampling function
,S. In this example, for the illustrated DATA
that uses KASi
packet, the sampling triggers at AS1 and AS3. The sampling
would have triggered at AS4 as well, had the data packet not
been dropped by AS3. Other DATA packets could be sampled
by other ASes, possibly fewer, but overall each AS samples a
ﬁxed percentage of the packets it forwards. To achieve highly-
efﬁcient sampling while being robust against cheating ASes,
Faultprints uses only symmetric cryptographic operations.

When an AS decides to sample a packet, it stores a packet
ﬁngerprint into a Bloom ﬁlter [13], a space-efﬁcient proba-
bilistic data structure. The ﬁngerprint captures the packet’s
presence, as well as the packet’s content, i.e., whether the
packet was modiﬁed en route. To prevent Bloom ﬁlters from
ﬁlling up, time is discretized into equal-sized units, called
epochs, and the lifetime of each Bloom ﬁlter is constrained
to an epoch.

Since the packet in Figure 1 is dropped en route, D does
not send a DACK packet to S. Not having received a DACK
packet, S decides to send a probe for the data packet. All the
ASes reply to the probe, but their reply packets could also
be dropped, delayed, as well as modiﬁed by malicious ASes.
For instance, AS2 could drop AS4’s probe reply, launching
a framing attack against AS4. To prevent framing attacks,
Faultprints’ probe replies are indistinguishable from each other
based on their origin AS (Section 4.3).

The source receives information about sampled packets
in probe replies, sent by each AS. The source checks the
sampling information in the replies using the secret packet
sampling keys KASi
,S. According to the replies the source
receives, AS4 behaved differently than dictated by its packet
sampling function: AS4 did not sample the packet, although
it should have sampled it. Consequently, the source becomes
suspicious of the link between AS3, and AS4. The source’s sus-
picions sum into corruption scores for the ASes neighboring
suspicious links, according to the probing results of different
packets. It is also possible that malicious ASes try to hide
misbehavior by refusing to send probe replies, or by dropping
other ASes’ probe replies, as it is the case for AS2. Or they
could drop acknowledgements to cause unnecessary probing.
To localize such misbehavior, Faultprints uses a probabilistic
model to let the source assign misbehavior probabilities to
ASes on the paths taken by acknowledgements and replies.

Because the sampling is independent and unpredictable to
other ASes, and the probe replies are indistinguishable from
each other, the misbehaving ASes eventually either have a
higher corruption score than their neighbors, or a higher
misbehavior probability. The source identiﬁes the link between
such neighboring ASes as malicious.

IV. FAULTPRINTS DETAILED DESCRIPTION

We explain the three main phases of Faultprints, namely
key setup, data sending, and probing, emphasizing the design

TABLE II: Notation.

AuthEncK (·)
PRFK (·)
(·)
H(·)

SigPrivKN

Authenticated encryption using symmetric key K
Pseudo-random function keyed with K, output in [0, 1]
Signature using node’s N private key
Cryptographic hash function

MACK (·) Message authentication code using key K

Cst(P)
SESSIONID
SV ASi
KASi
,S
KSD
cTimeX
Coni
Authdelay/modify

PSample
PProbe
DATA
DACK
PREQ
PREPLY

Constant part of packet P
DRKey session identiﬁer
A local secret value of ASi
Secret key between ASi and source S
Symmetric key between S and D
Current time at entity X
Precomputed authenticator for packet contents at ASi
Nested authenticated value indicating packet de-
lay/modiﬁcation
Probability to store any packet in a node’s Bloom ﬁlter
Probability to send a probe for any packet
Data packet
Acknowledgement packet (control packet)
Probe request packet (control packet)
Probe reply packet (control packet)

decisions that make Faultprints operations fast. The scoring
for fault localization is explained in Section V. Throughout the
description of the protocol, we consider a source S that sends
DATA packets towards a destination D through n intermediate
ASes ASi, and we use the notation listed in Table II.

A. Key Setup

To start using Faultprints, the source initiates the key setup
phase, using the DRKey protocol [24, 34] to share a key with
each AS on the path to the destination. These keys are valid
for a predetermined period of time, and expired keys need to
be updated if the communication session lasts longer. At the
beginning of a session σ , the source picks a fresh random
−1),
public-private key pair as the session key pair (PKσ , PKσ
and then computes a unique session identiﬁer SESSIONID
based on the current time at the source, and the session public
key (Equation 1). cTimeS prevents replay attacks: ASes drop
old packets, based on loose time synchronization. Rather than
including a path in the SESSIONID computation, as in the
original DRKey protocol, Faultprints extends the concept of a
session to cover multiple paths, which may not all be known
to the source when the session is created. Nevertheless, the
source records which paths are used within the session. This
design decision allows a session to persist when paths change.
We discuss this aspect in detail in Section XI.

SESSIONID ← H(cTimeS, PKσ )

(1)
The source inserts SESSIONID in a key setup packet, and
locally stores the mapping between the forward paths and
SESSIONID. In addition, cTimeS and PKσ are inserted by the
source in the key setup packet. Each ASi that receives the key
setup packet derives the key shared with the source, using
SESSIONID in the packet as the input to an efﬁcient pseudo-
random function (PRF), keyed with a secret SV ASi known
only to the AS (Equation 2). Since the AS can efﬁciently
re-derive the key on-the-ﬂy based on the session identiﬁer, the
AS does not store any per-host keys, regardless of the number
of sending sources.

KASi

,S ← PRFSV ASi

(SESSIONID)

(2)

862862

To allow the source to learn the key, without disclosing it
to other entities, ASi encrypts the key with the session public
key in the key setup packet (Equation 3). Since any node
could encrypt an arbitrary key with the session public key,
ASi also signs the encrypted key, together with SESSIONID
(Equation 4).

,S = EncPKσ (KASi

,S)

(3)

EncKEYASi
,S = SigASi

(EncKEYASi

,S, SESSIONID)

SignKEYASi

,S using the session private key PKσ

(4)
The encrypted keys and signatures generated by each
AS are accumulated in the key setup packet
towards the
destination, which sends the key setup packet back to the
,S by decrypting
source. The source learns the shared keys KASi
−1. However,
EncKEYASi
if the SESSIONID was maliciously modiﬁed on the forward
path, some ASes derived incorrect keys. The source detects
both SESSIONID and KASi
,S modiﬁcation by verifying To check
the signatures, the source uses the public key certiﬁcates of
the ASes that were either added to the key setup message, or
retrieved via RPKI.
Secret value rollover. We support nodes to change their
secret value once a day, at midnight UTC. A challenge is
to ensure a working protocol for ﬂows that operate across
a secret value rollover. We use the following approach to
support between 24 and 48 hours of key validity without any
additional protocol messages or data ﬁelds. Speciﬁcally, we
use the Least Signiﬁcant Bit (LSB) of the SESSIONID as a
selector for which key a node should use to derive the shared
secret key. By separating days into “odd” and “even” days,
the LSB of the SESSIONID would indicate whether the “odd”
day’s key (LSB=1) or the “even” day’s key (LSB=0) should
be used for key derivation. Depending on whether the current
day is “odd” or “even”, today’s or tomorrow’s key would be
selected. Upon generation of the SESSIONID, the sender can
compute the validity period, and if it is shorter than 24 hours
it can generate another SESSIONID until the LSB is different
from the parity of the current day and thus extend the key
lifetime beyond 24 hours. For longer session lifetimes, the
SESSIONID could be changed every day and the key setup
operation could be repeated. To determine the parity of a day,
we can simply ﬁx the parity of a certain day, e.g., 1 January
2016 is an “even” day, which determines the parity of all other
days.
Key setup overhead. A source can send multiple ﬂows within
the same session, and has the incentive to do so to improve the
fault localization speed and accuracy. Therefore, the latency
introduced by the key setup on routers (381μs per key setup
in the DRKey evaluation) is amortized by the trafﬁc forwarded
during the lifetime of multiple ﬂows.

B. Data Sending

We now describe the steps that source, intermediate nodes,
and destination take for processing DATA packets. We assume
the source already performed the DRKey setup, and has the
keys KASi

,S.

863863

0

16

32

48

64

Layer 3 hdr

SESSIONID (128)

CurTimeS (32)

Idx (8)

IDDATA (24)

Authmodif (128)

Authdelay (128)

ConAS1 (32)

ConASn (32)

CurTimeAS1 (32)

CurTimeASn (32)

Layer 4 hdr

Fig. 2: The Faultprints header for DATA packets, between
the network and transport layer headers dark gray). The light
colors show the values updated by the ASes. All sized are
indicated in bits.

it

When S sends a DATA packet,

inserts in the header
the DRKey session identiﬁer SESSIONID, used by each AS
for key derivation, and the source’s local timestamp cTimeS,
used to detect packet delay. In addition, the source computes
for each ASi on the path a ﬁeld Coni
to authenticate the
packet, and binds the packet to the forward path through nested
authenticators (Equation 5). Cst(r)epresents the constant part
of the packet, i.e., excluding the variable IP header ﬁelds, such
as TTL and checksum. These Coni ﬁelds are used by ASes
to efﬁciently expose packet modiﬁcation and trafﬁc rerouting
attacks. To enable localization of packet modiﬁcation and
delay, the source reserves space in the Faultprints header for
the packet modiﬁcation authenticator, for the packet delay
authenticator, and for a timestamp per AS on the path to
indicate the receiving time of the packet. Reserving space
prevents the packet from increasing in size while traversing
the network, thus avoiding packet fragmentation or drop if
the packet becomes larger than the Maximum Transfer Unit
(MTU) size. The source also inserts IndexAS, an AS index
initialized to 0 and incremented by each AS, used to access
the correct per-AS ﬁelds in the packet header. Figure 2 depicts
the Faultprints header for DATA packets.

Con1 ← MACKAS1
Coni ← MACKASi

,S

(Cst(DATA)),
,S
(Cst(DATA)||Coni−1)

(5)

Before sending the packet, S computes a DATA packet
identiﬁer IDDATA, used to match acknowledgments generated
by D with the DATA packets they acknowledge. Since the
acknowledgments are end-to-end, S computes IDDATA as a
packet authenticator between S and D (using the key KSD
shared with D) (Equation 6). IDDATA can be as small as 24 bits,
as we explain in the security argument (Section VI). IDDATA
is stored at S, as well as inserted in the packet header. S also
stores the Coni ﬁelds, which it uses for DATA packet probing.

(6)
In Faultprints, each AS samples incoming DATA packets

IDDATA

= MACKSD

(Cst(DATA))

according to a deterministic sampling function. Sampling at
each AS must be unpredictable to other ASes, otherwise
an AS could launch coward attacks [27] and drop only the
packets that would not increase the localization accuracy, i.e.,
drop packets that are not sampled by earlier and following
ASes. At the same time, sampling performed at ASes must be
veriﬁable by the source against a deterministic ground truth
sampling behavior: when the source sends a probe request
for an unacknowledged packet, the source must be able to
identify sampling misbehavior. For these reasons, the sampling
function behavior at ASi is determined by the key KASi
,S shared
with the source.

Speciﬁcally, when receiving the DATA packet, ASi ﬁrst
derives the key KASi
,S shared with the source (Equation 2).
Then, ASi checks whether to sample the DATA packet by
using a PRF computed over the constant part of the packet
(i.e., excluding the variable IP header ﬁelds, such as TTL and
checksum). Packets are sampled with a predeﬁned sampling
probability, PSample. If the packet is sampled, ASi stores the
packet ﬁngerprint in a local Bloom ﬁlter.

Packets are sampled such that the source can later probe for
missing packets. However, if ASes use the raw DATA packet
contents for sampling, probe packets must include the entire
contents of the probed DATA packet – an unnecessary over-
head. Instead, nodes base their packet sampling and storage
on a much smaller packet ﬁngerprint Authmodif ,i, which reﬂects
the packet contents observed by ASi (Equation 8). Authmodif ,i
is a nested authenticator that ASi computes over the previous
packet ﬁngerprint, and the corresponding Coni ﬁeld, namely
the ﬁeld precomputed by the source to describe the packet
contents for ASi (Equation 7). ASi
then updates the result
in the Authmodif ﬁeld in the packet header. Through nested
authentication, once a packet is modiﬁed, the modiﬁcation
is reﬂected through the absence of expected ﬁngerprints in
the Bloom ﬁlters of subsequent sampling ASes. In this sense,
packet modiﬁcation has a similar effect as packet drop.

(Coni||Authmodif ,i−1

)

,S

(7)

Authmodif ,0

Authmodif ,i

← IDDATA
← MACKASi

||cTimeS,

(Authmodif ,i

) ≥ PSample

,S

PRFKASi

(8)
Basing the sampling decision and storage on the pre-
computed Coni values is an important optimization, because
computation of the MAC on the packet contents is necessary
only when the packet is sampled. Since a malicious AS can
tamper with the Coni values in the packet header, ASi checks
indeed equals the
only for sampled packets whether Coni
nested MAC of the constant part of the packet (Equation 9). In
case of a mismatch, ASi does not store the wrong ﬁngerprint. It
may seem the AS could simply drop the packet, but this action
would let colluders located before and after ASi know that the
AS was supposed to sample the packet. Such colluders could
simply replay packets that are sampled to avoid localization.
to
Instead,
colluders looks just like an update of Authmodif ), such that
packet modiﬁcation is visible through the chained Authmodif to
all subsequent ASes, and forwards the packet. A bogus Coni

the AS garbles Authmodif

in the packet (that

hdr

contents

fingerprint

sampling PRF

KASi,S

store/

not store

check ConASi
correct

Bloom filter

Fig. 3: Faultprints sampling of DATA packets.

can still cause an AS not to sample a packet that, in fact,
the AS should have sampled. This case is revealed through
Faultprints scoring, the mechanism to localize the malicious
entity (Section V). Figure 3 illustrates packet sampling and
ﬁngerprint storage.

,S

(Cst(DATA)||Coni−1)

Coni == MACKASi

(9)
Alternatively, the source could summarize the packet con-
tents in a hash value, instead of a MAC value for each AS on
the forward path. However, this design decision deteriorates
fast-path packet forwarding: in Equation 9, computing a hash
for each sampled DATA packet is up to 6x more computation-
ally expensive than computing a MAC (Section IX). Since fast-
path forwarding speed is of primary importance on Internet
core routers, we opted for MACs. It may seem a drawback that
the source computes several MACs instead of a single hash
value, but for the average path lengths in today’s Internet (4.2
ASes [15]), both alternatives have a comparable computational
overhead on the source (Section IX). Regarding the DATA
packet overhead, we show in the security analysis (Section VI)
that each Coni value can be as small as 32 bits.

For delay localization, each AS also updates its timestamp
cTimeASi in the Faultprints packet header. The timestamp is a
nested authenticator computed by the AS, to enable the source
to detect timestamp alterations (Equation 10). Each AS updates
the nested authenticator Authdelay in the packet header.

Authdelay,0

Authdelay,i

← IDDATA
← MACKASi

||cTimeS,

(cTimeASi

,S

||Authdelay,i−1

)

(10)

DATA packet acknowledgments. When D receives a DATA
packet from S, it ﬁrst recomputes IDDATA, and checks whether
the value matches the one in the packet header. In case of
a successful match, D creates a DACK packet to indicate
successful data packet reception. The acknowledgment data
Ackinfo contains the identiﬁer IDDATA, recomputed by D, which
allows the source to easily identify the acknowledged DATA
packet (Equation 6)). D also adds to Ackinfo the delay informa-
tion Authdelay about the DATA packet accumulated along the
forward path (Equation 11). Ackinfo is authenticated with the
key KSD shared with S (Equation 12). Each AS on the return
path forwards the acknowledgment.

Ackinfo

= IDDATA

||Authdelay

||cTimeAS1

||..||cTimeASn

DACK[DATA] = Ackinfo

||MACKSD

(Ackinfo

)

(11)

(12)

864864

Yet attackers could drop, delay, or modify DACK packets, to
prevent the information from reaching the source. Faultprints
empowers the source to localize such attacks by requiring
ASes on the reverse paths to sample DACK[DATA] packets
in the Bloom ﬁlters, similarly to DATA sampling.

Bloom ﬁlter size. One of the goals of Faultprints is to
bound the storage requirements at ASes. For this reason,
the Bloom ﬁlter at each AS has a ﬁxed size, depending on
the link bandwidth. As the Bloom ﬁlter stores more packet
ﬁngerprints, the false positive probability for testing whether
a packet is in the Bloom ﬁlter increases. To minimize the
false positive probability, ASes use the Bloom ﬁlter to store
packet ﬁngerprints sampled during an epoch. Faultprints uses
two alternating Bloom ﬁlters to allow sufﬁcient time for packet
probing, and to enable Bloom ﬁlter erasure. We show later in
this section that two Bloom ﬁlters are sufﬁcient to guarantee
up to one epoch storage.

Even though epochs start at precise moments in time, ASes
are not perfectly time synchronized. Therefore, different ASes
may ﬁnd themselves in different epochs, at the same wall
clock time. To prevent this problem, ASes establish which
Bloom ﬁlter to use based on each source’s time, using the
timestamp cTimeS inserted by the source in the DATA packet. If
cTimeS is absent, then either a previous AS deleted the value –
which is detected through Coni and handled as a typical packet
modiﬁcation attack, or the source did not write the value in
the packet. In the latter case, ASes simply use the current
Bloom ﬁlter, which shrinks the probing time for a source with
non-standard behavior.

C. Probing

If the source does not receive a valid acknowledgment
within a certain timeout, e.g., a round-trip time (RTT), it means
an adversary may have dropped, delayed, or modiﬁed either
the DATA packet or the DACK packet, or in the common
case, the packet was simply dropped. An acknowledgment is
valid if it is authentic, and its identiﬁer IDDATA computed by
the destination equals the identiﬁer stored at the source. The
source decides with probability PProbe whether to probe an
unacknowledged DATA packet, and the corresponding missing
DACK packet. Since the source alone decides whether a
certain packet would be probed, all DATA and DACK packet
drop, delay, and modiﬁcation attacks become daring attacks
(i.e., an attacker has a non-zero risk of being localized).

DATA packet probing. The source probes a DATA packet to
learn which ASes on the forward path observed the DATA
packet with the correct content. To probe, the source retrieves
IDDATA and Coni ﬁelds for the probed DATA packets, and uses
them to assemble a PREQ packet, together with the Authmodif
ﬁeld initialized with Authmodif ,0, and SESSIONID, needed for
key derivation. The source also inserts a counter Ctr unique per
packet, which is later used for duplicate detection. A counter
of 24 bits is sufﬁcient for PREQ packets arriving at line rate
(7.81 ∗ 106 PREQ packets arrive at line rate, as explained in
Section VII). The PREQ packet also contains the timestamp of

the DATA packet, cTimeS, to identify the epoch corresponding
to the DATA packet, as well as timing information for the
replies, ReplyTiming. Later in this section we explain in detail
why ReplyTiming is required. Equation 13 depicts the PREQ
packet. The source sends the PREQ packet along the forward
path.

PREQ[DATA] =SESSIONID||cTimeS||IndexAS||Ctr||

||Con1||..||Conn||Authmodif

||ReplyTiming

(13)
To look up the probed packet in the Bloom ﬁlter, each
ASi ﬁrst derives the key shared with the source (Equation 2),
updates the value Authmodif (Equation 7), and checks whether
the queried packet was sampled (Equation 8). If sampled, ASi
then queries the Bloom ﬁlter corresponding to the epoch of
cTimeS for the packet ﬁngerprint Authmodif ,i. ASi replies to
the source with a bit indicating whether the queried packet
ﬁngerprint was stored (bitAuthmodif = 1 if the packet ﬁngerprint
was stored).

If the bit is sent in clear, colluding attackers could gain
insight into the location of nodes originating the reply. The
attack exploits a corner case, based on the position of colluders
on the forward and reverse paths; we explain the attack in
detail below, in the paragraph on the indistinguishability of
probe replies. Thus, the bit needs to be encrypted. In addition,
to expose modiﬁcations of the request packet, which may
cause an incorrect Bloom ﬁlter lookup, as well as reply
modiﬁcations, ASi inserts in the reply an authenticator over
the request packet and over the encrypted bit (Equation 14).

PREPLYASi
||MACKASi

,S

[DATA] = EncKASi
(EncKASi

,S
(bitAuthmodif

,S

)||

(bitAuthmodif
)||PREQ[DATA])

(14)

In Faultprints, each AS observing a PREQ packet sends a
separate PREPLY packet to accommodate potentially asym-
metric return paths. We discuss the additional cost of asym-
metric paths in Section XI.

DACK packet probing. Similarly to DATA packet probing,
the source sends probe requests for a portion of the DACK
packets to every AS on the path. To probe acknowledgments,
S stores the DACK packets that either contain an IDDATA
value the source does not recognize, or whose contents
were not authentic according to the MAC computed by the
destination (Equation 12). The request packet contains the
acknowledgment DACK[DATA]. The replies are similar to
[DATA], containing the bit of whether the packet
PREPLYASi
ﬁngerprint was found in the Bloom ﬁlter bit, and IDDATA
(instead of cTimeS) for fast matching between probes and
replies.

PREPLY packet indistinguishability. PREPLY packets need
to be indistinguishable amongst each other by their origin
AS,
to prevent malicious ASes to launch framing attacks
by dropping the replies originating at speciﬁc ASes. The
contents of probe replies is already indistinguishable regarding
its origin AS to any entity that does not know the shared key,

865865

AS1

AS2

AS3

AS4

AS5

AS6

S

PReq

PReply4

PReply5

PReply6

D

Fig. 4: Colluder nodes can track PREPLY packets, but targeted
PREPLY damage is localized.

because the Bloom ﬁlter bits are encrypted. 2

Still, the contents of each PREPLY packet does not change
on its way to the source, letting colluders to track a packet
along the path. Speciﬁcally, in Figure 4, AS4 observes PREPLY
packets from AS5 and AS6, and can inform AS2 about them.
But if AS2 damages only those packets, its colluder AS4 is
localized by Faultprints. Thus, PREPLY indistinguishability
leverages its problem setup: fault localization.

Attackers could also use the timing between PREQ and
PREPLY packets to infer the number of hops from the AS that
sent the PREPLY packet, through a timing attack. Faultprints
prevents timing attacks on PREPLY packets by requiring each
ASi to return the PREPLY packet after a predetermined amount
of time ti, speciﬁed by the source in the PREQ packet: the
source encrypts and authenticates in the PREQ packet a time
delay for each AS to send the PREPLY packet (ReplyTiming in
Equation 15). When an AS receives a PREQ packet, it decrypts
and checks the authenticity of the timer at index IndexAS,
then it waits for the indicated time delay before sending the
PREPLY packet to the source. Since the source knows the
values ti and the RTT, it can estimate the amount of time to
wait for the replies.

To determine the timers, the source must ensure that the
delays each AS on the forward path adds to replies cause the
RTT to tend towards the average RTT in today’s Internet, lower
bounded by a low RTT in today’s Internet. Such a delay does
not allow two colluding ASes to differentiate replies based on
their origin: in Figure 4, malicious AS2 knows the timing of the
PREQ packet, but observes replies arriving after an RTT value
averaging the RTT in the Internet. Thus, the AS originating the
reply could be almost any AS in the Internet, without allowing
AS2 to guess reply origin AS6, for instance.

For realistic timers, we use the RTT measurements per-
formed by CAIDA for trafﬁc sent from the West US coast
to the rest of the world. The measurements exhibit two RTT
peaks at 100 ms and 200 ms, with a long tail distribution
towards 350 ms [17]. Based on these values, in Faultprints
sources randomly choose timer values uniformly distributed
from 100 ms to 350 ms, with an average delay of 225 ms.

The complete PREQ packet is depicted in Figure 5. Timers
are authenticated and encrypted using AES in GCM mode.
Each operation takes the respective Coni value as addi-
tional authenticated data input, to detect attacks that copy
ReplyTiming values and overwrite them in a different PREQ

2To make PREPLY packets indistinguishable by the origin AS’s IP address,

PREPLY packets in a session use the IP of the source S.

0

16

32

48

64

Layer 3 hdr

SESSIONID (128)

CurTimeS (32)

Idx (8)

Ctr(24)

Authmodif (128)

ConAS1 (32)

AuthEncAS1 (160)

ConASn (32)

AuthEncASn (160)

Layer 4 hdr

Fig. 5: The Faultprints header for PREQ packets, between layer
3 and layer 4 headers (dark gray). The values computed by
the source are depicted in mid gray, and the light colors show
the values updated by the ASes. The numbers are in bits.

packet. The output size is 160 bits, corresponding to a 32 bit
ciphertext, and a 128 bit authentication tag.

ReplyTiming = AuthEncKAS1

,S

(t1)||..||AuthEncKASn,S

(tn)

(15)

D. Bloom ﬁlter handling, storage, and packet processing

Processing control packets should not hinder the fast path
forwarding of data trafﬁc, thus PREQ and PREPLY packets are
processed on the router slow path (Figure 6). On the slow path,
ASes store timers, as well as their own PREQ packets, before
sending them. Other ASes simply forward the PREPLY packets
as soon as they receive them. Moreover, to prevent replay
of PREQ packets, which would inﬂate the number of stored
PREPLY packets at ASes, each AS maintains a Bloom ﬁlter
of PREQ packet counters, hashing the tuple SESSIONID||Ctr.
When an AS receives a previously observed packet, it simply
discards it, without storing any additional information. This
Bloom ﬁlter is emptied after each epoch.

The PREPLY packets and timers stored at each AS for
triggering replies imply per-PREQ packet state (there are as
many replies as requests). However, since Faultprints routers
keep this state for 225 ms on average, as explained above, and
because the number of PREQ packets is naturally bounded by

AS

AS

Data

PReq

fast path

Router

slow path

DAck

AS

PReply

Fig. 6: While transiting an AS, DATA and DACK packets are
processed on the fast path, while PREQ and PREPLY packets
are processed on the slow path.

866866

Epoch e-1

Epoch e

Epoch e+1

sample

probe

late probe
epoch e-1
sample

probe

BF1

BF1

BF2

delay bound for
probes epoch e-1

BF2

clear BF
BF1

BF2

Fig. 7: Packet sampling and probing at an AS. Each AS
requires two Bloom ﬁlters, to accommodate late probes, and
to allow Bloom ﬁlter deletion.

the link bandwidth (thus the number of replies is bounded),
the AS needs to keep per-bandwidth state on the slow path,
at most 93.74 MB per 10 Gbps link. We give more details in
the upper-bound analysis in Section VII.

Faultprints processes DATA packets on the fast path, re-
quiring a ﬁxed amount of state. However, actually setting the
Bloom ﬁlter bits is not performance critical, because Bloom
ﬁlters are only read on the slow path by control packet
processing. Thus, Bloom ﬁlter operations are performed on
the slow path. Each Bloom ﬁlter stores packet ﬁngerprints
until probing ﬁnishes for that epoch. DATA packets stored
during an epoch can arrive later than the end of the epoch, due
to propagation delay and clock drift between the source and
the AS (recall that cTimeS decides the epoch a DATA packet
belongs to). Figure 7 shows that probe requests can arrive
later than the end of epoch e − 1, due to clock drift between
the source and the ASes, and also because probes for the last
packet in epoch e − 1 incur a delay of at most RTT. To look
up such late-probed packets, Bloom ﬁlters must be stored for
at least an epoch plus an RTT plus the maximum clock drift.
This period of time overlaps with the subsequent epoch, thus
another Bloom ﬁlter is required at the AS, to store ﬁngerprints
of the packets arriving during epoch e. With epochs longer
than an RTT plus the maximum clock drift (in total about
500 ms), two Bloom ﬁlters are sufﬁcient at each AS. In fact,
even for epochs as long as 1s, that accommodate a large
time synchronization error, Bloom ﬁlters incur a relatively low
storage overhead (Section VIII).

V. FAULT LOCALIZATION MECHANISM

The source proceeds with localizing an adversarial AS only
after it detects packet loss, unusual delay, or modiﬁcation. An
adversary exists on the forward or reverse paths when the
end-to-end corruption rate of the packets exceeds a threshold
given by the natural drop rate of the links. We give details on
how the source computes the end-to-end corruption rate in the
theoretical analysis (Section VII). In this section we describe
the fault localization mechanism.

Faultprints empowers the source to localize attackers based
both on correct probe replies, and on the absence of authentic
probe replies. We deﬁne a correct probe reply as a reply that
is authentic, which means the reply corresponds to a PREQ
packet previously sent by the source, and the response bit is
authentic, and was conﬁdentially sent (see Equation 14). The
source maintains two values for each AS, to indicate a degree
of misbehavior associated with the AS. One value, called
corruption score, is obtained from correct probe replies: the
source compares the real AS sampling behavior against their
expected sampling, to infer malicious activity. A corruption
score associated with an AS indicates the number of times
the source suspects the AS of having misbehaved. A second
value is required, because attackers and colluders may attempt
to hide misbehavior by dropping, delaying, or modifying other
entities’ probe replies, and could even avoid sending a probe
reply themselves, or send a garbled or delayed probe reply. The
second value, called misbehavior probability, is a probability
computed based on the number of incorrect probe replies
(including absent replies) from each AS on the path.

The two score values essentially create a no-escape situation
for attackers and colluders: once an attacker misbehaves with
respect to DATA and DACK trafﬁc, the attacker (or a colluder)
either tries to cover misbehavior, and is detected through
misbehavior probabilities, or the attacker sends back correct
replies, and is detected through corruption scores. We present
the two scoring methods below.

A. Corruption score computation

A high corruption score does not, by itself, necessarily
imply misbehavior. Only by comparing corruption scores of
AS neighbors can the source ﬂag a link as malicious. Intu-
itively, a corruption score very different from the neighbors’
score signals a different, possibly malicious behavior during
Faultprints execution.

The source updates the corruption scores of ASes after it
compares the Bloom ﬁlter bits in correct probe replies with the
expected values. We consider ASi to be the reply originator.
The comparison yields the following possibilities:

RECEIVEDBIT and EXPECTEDBIT have the same value. In
this case, ASi appears honest, and its score remains unchanged.
RECEIVEDBIT = FALSE and EXPECTEDBIT = TRUE. In
this case, ASi did not sample the DATA packet, even though it
should have sampled it. Therefore, either ASi lies, or the packet
has been dropped, delayed, or modiﬁed on the path before ASi.
The source aims to localize where the packet went missing,
as depicted in Figure 8. For this, the source gathers all correct
replies, identiﬁes the last ASx that observed the packet, i.e.,
replied (TRUE, TRUE), and the ﬁrst node ASy that reported
the packet as missing, i.e., replied (FALSE, TRUE). ASy may
be the same as ASi, or before ASi on the path. Because the
packet appears to have disappeared between ASx and ASy, the
source increments the corruptions scores of these two ASes,
and of all ASes in between.

867867

T,T

F,F
?

T,T
T,T

ASx
ASx

F,F
?

F,F
?

F,T

F,F
?

F,T
F,T

ASy
ASy

Fig. 8: Faultprints scoring. The letter pairs A, B denote: A if
the packet was sampled, B if the packet should have been
sampled.

RECEIVEDBIT = TRUE and EXPECTEDBIT = FALSE. In
this case, ASi should not have sampled the packet, but reported
to have the packet in its Bloom ﬁlter. However, according to
Faultprints, ASi queries its Bloom ﬁlter only when the packet
is sampled (section IV). Thus, if ASi is honest, it queries its
Bloom ﬁlter because the PREQ packet was modiﬁed such that
ASi believes the probed packet is sampled. But a modiﬁed
PREQ packet causes an incorrect reply packet; this case is
handled by misbehavior probabilities, explained later in this
section. The only possibility left is that ASi is malicious, thus
the source increments ASi’s corruption score.

B. Misbehavior probability computation

We deﬁne as damaged packets the PREQ and PREPLY
packets that an adversary drops, delays, and modiﬁes. To
localize such adversaries, the source keeps per epoch counters
dmgi of the damaged PREPLY packets from each ASi on the
forward path. At the end of the epoch, the source localizes
as malicious the ASi which maximizes the probability Pi
(Equation 16).

P(ASi malicious|dmg1, dmg2, .., dmgn) notation= Pi

(16)
In the following, we ﬁrst explain why PREQ and PREPLY
drops, delays, and modiﬁcations are equivalent for the source
w.r.t. the amount of information they offer to localize the
misbehavior. This equivalence allows the source to maintain
a single counter dmgi per ASi for all these attacks. Then, we
explain step by step how we compute Pi.

PREQ and PREPLY drop, delay, and modiﬁcation provide
equivalent information to the source. The source observes
the effects of PREQ packet drop, delay, and modiﬁcation only
indirectly, through PREPLY packets. In what concerns PREPLY
packets, drop and delay events are indistinguishable by the
source, because a delay longer than a large RTT (e.g., 400
ms) becomes a drop event for the source, and a delay shorter
than RTT is regular packet arrival. PREPLY modiﬁcation can
be detected by the source by checking PREPLY authenticators,
however, there is no additional information about the particular
AS that may have modiﬁed the PREPLY. Thus, we consider
these attacks equivalent, and refer to them as packet damaging.

Computation of Pi. We ﬁrst consider the return path. We
characterize the state of a PREPLY packet as correct (CORR)
or damaged (DMG), and model the state of packets traversing
ASes using Markov chains. The resulting Markov transition
rate matrix denotes the packet’s probability to switch from the

current state (given by the row name) to the next state (given
by the column name). For the transition rate computation,
we assume the Markov property that the next state of the
packet depends only on its current state, not on the sequence
of states that preceded it. The Markov property holds when
PREPLY packets are indistinguishable, and furthermore when
each AS acts independently. In the following computation, we
assume attackers do not collude. We relax this assumption
in Section VI, where we explain that Faultprints scoring
localizes colluding attackers one at a time: the ﬁrst colluder to
damage packets, which implicitly acts before other colluders
could damage packets (thus independently w.r.t. the observable
packet state), is localized ﬁrst.

Equation 17 gives the transition rate matrices for a PREPLY
packet transiting a benign AS (matrix B) and a malicious
AS (matrix D). PD is the probability of a malicious AS on
the return path to damage a PREPLY packet. For equation
simplicity, we assume all malicious ASes have the same
PD value. Nevertheless, the computation is very similar for
different PD values PDi. A benign AS damages a PREPLY
packet with a probability equal to the natural drop rate ρ.

(cid:3)

(cid:2)

DMG CORR

1
PD

0

1 − PD

(cid:2)

DMG CORR

(cid:3)

B =

DMG
CORR

1
ρ

0

1 − ρ

, D =

DMG
CORR

(17)
From these matrices, the probability of a correct PREPLY
packet to remain correct after a benign AS is 1 − ρ, and after
a malicious AS it is 1 − PD. Thus, the probability P(t, r) of a
correct PREPLY packet to be damaged after traversing t ASes
on the return path, out of which r are malicious, is given
by Equation 18. Note that the location of the malicious ASes
does not inﬂuence the result, because we simply multiply these
probabilities, and multiplication is commutative.
P(t, r) = 1 − (1 − ρ)t−r(1 − PD)r

(18)
We now also consider the forward path, in addition to the
reverse path. We denote by PQ the probability of a malicious
AS on the forward path to damage a PREQ packet. The
probability P(t, r, f ) of a PREPLY packet being damaged after
the corresponding PREQ packet traversed f malicious ASes on
the forward path, and the PREPLY packet traversed r malicious
ASes on the return path (both paths have length t, as they are
symmetric), is given by Equation 19.

P(t, r, f ) = 1 − (1 − ρ)t−r(1 − PD)r(1 − PQ) f

(19)

We now compute Pi as in Equation 20, where k denotes
the number of ASes between the source and the destination.
is the number of ASes on ASi’s return path, and r j,i
ti
(respectively f j,i) is 1 if and only if ASj
is on the return
path (respectively the forward path) of ASi. The fraction in
Equation 20 is constant for each AS, therefore we only com-
pute P(dmg1, .., dmgk
|ASi mal), where mal denotes malicious.
n represents the number of reply packets the source expects
to receive. To localize the malicious link, the source ﬁnally

868868

computes all Pi+1/Pi and identiﬁes as malicious the link
ASi → ASi+1 with the highest Pi (Equation 21). We omit the
intermediate steps of the computation.

PREPLY packets are indistinguishable by their originating AS,
the attacker is unable to target-drop them with a probability
better than random.

Pi =

P(ASi mal)

P(dmg1, ..., dmgk

)

(cid:3)
∗ P(dmg1, ..., dmgk

(cid:2)

|ASi mal)

(20)

C. Framing attacks by independent attackers

P(dmg1, ..., dmgk

|ASi mal) =

k
∏
j=1

[

n

dmg j

P(t j, r j,i, f j,i)dmg j ∗

∗ (1 − P(t j, r j,i, f j,i)n−dmg j )]

(21)

VI. SECURITY ARGUMENT

We argue that Faultprints provides localization of links
neighboring adversarial ASes that drop, delay, or modify
trafﬁc. We discuss why Faultprints is resilient
to framing
attacks, and also argue two or more colluding adversaries
cannot gain an additional advantage compared to attackers
operating independently. Finally, we explain why Faultprints
does not create the opportunity for new DoS attacks.

A. DATA and DACK packet damage

Sampling of DATA and DACK packets is deterministic at
each AS based on the shared key between the AS and the
source, yet unpredictable to other ASes. Thus, an attacker
that drops and modiﬁes DATA and DACK packets, and delays
DACK packets cannot choose to damage only packets that are
not sampled by the next AS. In fact, the next AS samples
packets uniformly at random in the space of all possible
packets, including the damaged ones. If the attacker inﬂicts
more damage than the natural packet damage,
the source
randomly picks which unacknowledged packets to probe, and
collects Bloom ﬁlter bits revealed in PREPLY packets. If PREQ
and PREPLY packets are not damaged, the source computes
corruption scores and localizes a link adjacent to attacker.
We analyze in the next subsection the case when PREQ and
PREPLY packets are damaged.

To detect DATA delay,

the source inspects the onion-
authenticated timestamps added to the DATA packets by each
AS, and included by the destination in the DACK packet.
Onion authentication ensures these timestamps cannot be se-
lectively modiﬁed by a malicious party. In case the timestamps
in the DACK packet are damaged to hide DATA delay, the
source ﬁrst localizes a link adjacent to the attacker on the
return path that damaged the DACK packet.

B. PREQ and PREPLY packet damage

PREQ packet damage is observed by the source only
through PREPLY packet damage, as explained in section V.
PREQ and PREPLY modiﬁcation are detected using the au-
thenticator in the PREPLY packet. If the PREPLY packet does
not arrive at the source because of PREPLY drop and delay
attacks, PREPLY packet absence increases the misbehavior
probabilities of nodes. To escape localization, the attacker thus
needs to thwart misbehavior probabilities by target-damaging
PREPLY packets originating at chosen ASes. However, since

869869

To frame another AS of misbehavior, attackers could collude
or act independently. We ﬁrst analyze an independent attacker,
and examine colluding attackers in the next subsection.

Assume the attacker attempts to cause wrong corruption
scores, which are based on Bloom ﬁlter contents. An at-
tacker acting independently can control trafﬁc only on its
outgoing links, with the contents of DATA and DACK trafﬁc
reﬂected in the Bloom ﬁlters of random subsequent ASes.
However, the attacker cannot control trafﬁc on non-adjacent
links to selectively pollute Bloom ﬁlters of non-neighboring
ASes, in an attempt to alter corruption scores. Nor can the
attacker forward only packets that are sampled by non-adjacent
ASes, because the attacker does not know the sampling keys.
Moreover, these Bloom ﬁlters have the capacity to store line-
rate trafﬁc, therefore injection attacks cannot pollute them
(Section VII-C). As a result,
leverage
corruption scores to frame another AS of DATA and DACK
packet drop and modiﬁcation, and of DACK packet delay. In
addition, since authenticators in DATA packets are onioned, the
attacker cannot selectively change the information inserted by
another AS, for instance to frame the AS of DATA packet
delay.

the attacker cannot

Besides altering corruption score computation, the attacker
can attempt to cause wrong misbehavior probabilities, which
are based on PREQ and PREPLY packets. Similarly to the
previous case, an attacker can drop, delay, and modify PREQ
packets only on its outgoing links. Since each AS sends
a PREPLY packet after observing a PREQ packet, and the
PREPLY packet contents is linked to the PREQ packet, the
attack on PREQ packets is visible through PREPLY packets.
In addition, as explained previously,
targeted drop, delay,
and modiﬁcation of PREPLY packets is not possible, because
PREPLY packets are indistinguishable based on their origin
AS.

D. Colluding attackers

Colluding attackers try to escape localization by framing an-
other AS. The more packets of the same session the colluders
observe that traverse the framed AS, the more sophisticated
attacks they can launch. From this point of view, symmetric
paths are the best case for colluders, because they can easily
observe all the trafﬁc in a session. The framed AS could be

Data

PReq

S

ASM1

ASI

ASM2

ASJ

DAck

D

PReply

Fig. 9: Colluding attack: ASM1 and ASM2 try to frame ASI and
ASJ.

in Figure 9
situated between colluding ASes, such as ASI
situated on the forwarding path between malicious ASes ASM1
and ASM2, or either before or after the colluders, such as ASJ.
In the case of ASJ, since attackers cannot inﬂuence trafﬁc
on links other than their outgoing links, their effect on the
trafﬁc ASJ and subsequent ASes observe is the same as the
effect of an independent attacker. Thus the case reduces to the
analysis in the previous subsection.

We now consider the case of ASI. In a framing attack using
packet modiﬁcation, one attacker modiﬁes a packet, and its
colluder changes the packet back to its original contents. In
case of DATA modiﬁcation, ASes between ASM1 and ASM2 do
not store in their Bloom ﬁlters a ﬁngerprint of the modiﬁed
packet. Moreover, even though ASM2 changes the packet back,
it cannot guess the correct value of the nested Authmodif , which
is a 128-bit MAC. Since this value is used for ﬁngerprinting,
ASes following ASM2 do not store the packet in their Bloom
ﬁlters either. Through corruption scores, the source localizes
as malicious the link of ASM1 towards D. Similarly, PREQ
modiﬁcation would cause incorrect PREQ packets originating
at ASes following ASM1 (or would cause PREPLY packets not
to be sent at all, when the timers in the packet are changed).
The attack is detectable through misbehavior probabilities,
allowing the source to localize the same link neighboring
ASM1. If ASM2 modiﬁes DACK packets and ASM1 changes
them back, the source still receives valid DACK packets, thus
the colluders do no harm. Similarly, PREPLY modiﬁcation by
ASM2, followed by a change back by ASM1 does not do any
harm.

Packet drop has a similar effect: the colluder can re-inject
the dropped packets, but she cannot guess the correct value
of the nested Authmodif . Thus, the ASes between ASM1 and
ASM2 do not store anything in their Bloom ﬁlter because they
do not observe the packet, nor do the ASes following ASM2,
because they do not store the ﬁngerprint of a wrong Authmodif
value. PREQ drop and re-inject causes the ASes between ASM1
and ASM2 not to send PREPLY packets, detectable through
misbehavior probabilities, as explained above. PREPLY and
DACK drop and re-inject (without delay) do not have any
effect.

To collaborate through packet delay, ASM1 delays DATA
packets, and its colluder ASM2 changes the delay authenticators
so as to frame ASI. However, since the authenticators are
onioned, and the colluders do not have the cryptographic
keys of the other nodes,
the colluder cannot craft delay
authenticators. Also, an attacker could delay DACK, PREQ
and PREPLY packets. Its colluder, however, cannot forward
faster than line rate the delayed packet. Instead, the colluder
can re-inject the delayed packets. For DACK packets, the
attack either is harmless, or becomes a packet drop attack.
For PREQ delay, the ASes between ASM1 and ASM2 may
send delayed PREPLY packets, which either become incorrect
packets when they are delayed for too long (explained above),
or do not have any effect. Finally, for PREPLY packet delay,
it is again either harmless, or causes incorrect packets from
ASes between ASM2 and D, detected through misbehavior

probabilities.

E. 32 bit Coni values sufﬁce

The MAC values Coni computed by the source have only
32 bits. A malicious ASM could try to change modify the
packet contents and craft these authenticators accordingly, for
instance only craft the Coni ﬁeld on the next AS, but not the
other ones, so as to incriminate the outgoing link of the next
AS. ASM has a chance of only 1
232 to guess the correct Coni.
For 4000 DATA packets sent by S, rate used in our simulation
(Section VIII), even successfully modifying 5% of them (200
packets), which would minimally impact our high localization
accuracy, has a very low probability (

1
26400 ).

However, ASM could inject several packets with different
guesses for Coni, to increase its chances of a correct guess.
For instance, to increase the probability to 0.05 (as the above
example), ASM needs to inject about 215∗ 106 packets for each
DATA packet. We assume the attacker has the computational
power to compute these MACs without incurring a high delay.3
However, even for a single smallest-size packet (64 bytes), this
results in 13.74 Gbps additional trafﬁc, and for 200 smallest-
size packets – 2748 Gbps additional trafﬁc. Clearly, such an
overhead exceeds the capacity of most links.

A similar argument holds for the security of 24-bit IDDATA

values. We omit the details due to lack of space.

F. Faultprints does not introduce new DoS attacks

Malicious ASes, as well as malicious sources and des-
tinations could aggressively send control packets (DACK,
PREQ, PREPLY) to put a computational strain on routers
to increase latency. Regardless of the number of DATA and
control packets, the storage on Faultprints routers forwarding
120 Gbps is at most 561.66 MB on the fast path, and at
most 1.23 GB on the slow path – both values representing
worst case attack scenarios (Section VII). Also, Faultprints
operations are very fast, achieving a router throughput 116.2
Gbps for Internet MIX trafﬁc (Section IX).)

Defending against other DoS attacks, possible without a

Faultprints deployment, e.g., link ﬂooding, is out of scope.

VII. UPPER BOUND ANALYSIS

We analyze the maximum trafﬁc corruption an adversary
can inﬂict without risking detection and localization. We show
in the simulation the chosen false positive rates are practical
for an accurate fault localization. Then we compute the upper
bound on storage overhead and number of control packets. In
our analysis, we consider a forward path with f ASes, and a
reverse path of r ASes. We use the parameters in Table III.

3Using our evaluation in Figure 15, the attacker spends 5.28 cycles per byte
of input, considering the keys are already expanded. Thus the attacker spends
about 338 cycles per MAC computation, and 726.7 ∗ 108 for all 215 ∗ 106
MAC values considered. For a delay of at most a second, the attacker requires
computation capabilities at a total clock rate of at least 73 GHz.

870870

TABLE III: Faultprints parameters.

ρi
ρ
ρ ∗
i
ψ
FPBf

Natural packet corruption rate of link Li (unidirectional)
The maximum value among all ρi
Average corruption rate (natural and malicious) of link Li
End-to-end corruption rate of a path
False positive rate of Bloom ﬁlter

A. End-to-end maximum corruption rate
We measure the end-to-end fraction ψ

threshold of packets that
an adversary can corrupt (drop, delay, and modify) on a path
without the source being able to detect the malicious activity.
For the forward and reverse paths, the threshold on natural
end-to-end trafﬁc corruption is:

ψ

threshold

= 1 − (1 − ρ) f ∗ (1 − ρ)r

We denote by ψ

(22)
observed the actual observed end-to-end
corruption rate. While ψ
threshold, the source decides
there is no signiﬁcant malicious activity on the path. Consider-
ing z ASes on the forward path and reverse paths are malicious,
the source computes ψ

observed as follows:

observed

≤ ψ

ψ

observed

f +r−z
∏
i=1
), the source records the observed
fraction of corrupted DATA, DACK, PREQ, and PREPLY
packets.

= 1 −
(1 − ρ ∗

To compute ∏z

(1 − ρi) ∗

(1 − ρ ∗

z
∏
k=1

(23)

k=1

k

)

k

B. Control packets upper bound

Faultprints introduces control packets when sources decide
to probe unacknowledged data packets. To probe one DATA
packet, the source sends one PREQ packet along the forward
path, and receives back at most f PREPLY packets. To probe
a DACK packet on the reverse path, the source sends r PREQ
packets, and receives at most r PREPLY packets. Assuming the
source sends k DATA packets in total, the maximum number of
probes is registered when attackers corrupt all k DATA packets
or the corresponding DACK packets. In this case, the number
of PREPLY packets is:

k · PProbe

· ( f + 1 + 2 ∗ r)

(24)
As an example, consider forward and reverse paths of 5
ASes (the average AS hop count is 4.2 in the Internet [9]),
and PProbe = 10%. The source sends 4000 DATA packets.
Then, the upper bound on the number of probes and probe
replies is 32000 packets. However, this bound is reached when
an adversary corrupts either the DATA or the DACK packet,
across all packets sent by the source. Faultprints localizes
even weaker attackers (0.05% corruption rate) in the same
settings with over 95% accuracy, as we show in our simulation
(Section VIII). Thus such an attack ceases after 4000 DATA
packets with high probability.

DACK packet

information can be piggybacked on TCP
acknowledgments, thus incurring no increase in the number of
packets. For Layer 4 protocols other than TCP, multiple DACK
packets can be bundled together in a Maximum Transmission
unit (MTU)-sized packet, e.g., a DACK packet totals 100B

871871

for a path with 5 ASes, therefore 15 DACK packets can be
bundled.

C. Storage upper bound

Fast path storage. We compute the storage upper bound
on the AS fast path. On the fast path, Faultprints stores
two Bloom ﬁlters, as explained in Section IV. The storage
requirement depends on the total AS forwarding bandwidth
(in Gbps, denoted by Bw), the epoch duration (in seconds,
denoted by T ), and the false positive rate of the Bloom ﬁlter
(FPBf ). Although in benign cases also the packet sampling rate
(Psample) inﬂuences the storage, in the worse case a malicious
source may deliberately send packets that are all sampled by
a target node. We consider smallest-size packets, 64 bytes in
Ethernet, arriving at line rate. This scenario stores the largest
number of packets – the worst-case scenario for Faultprints
Bloom ﬁlters. The total number of packets Nrpkt stored in an
epoch is:

= (Bw · 109/8)bytes/64bytes · T,
and the number of bits of the two Bloom ﬁlters is:

Nrpkt

(25)

/(ln 2)2

− 2 · Nrpkt

· ln FPBf

(26)
As an example, we choose a false positive rate of the Bloom
ﬁlter of 0.01 (we show in the simulation (Section VIII) the
chosen false positive rate is practical for an accurate fault
localization). Epochs last 1 second, sufﬁcient to allow sources
to query Bloom ﬁlters within an RTT, as shown in a recent
study on large-scale RTT measurements [26]. For an AS
forwarding 120 Gbps of minimum-sized packets, the worst
case Bloom ﬁlter storage, under attack, is 561.66 MB, practical
for today’s core routers. In case of a 10 Gbps link, the storage
is 46.8 MB, as depicted in the introduction in Table I.

Slow path storage. On the slow path, Faultprints stores
PREPLY packets, a Bloom ﬁlter to detect duplicate PREQ
packets, and keeps timers per-PREQ packet, as explained in
section IV. We consider a scenario where different sources’
PREQ packets traverse the same ASI. In the worst case, all the
trafﬁc ASI receives consists of PREQ packets, and all these
PREQ packets originate at different sources. As before, we
consider forward paths of 5 ASes, which require PREQ packets
of 192 bytes (160 bytes PREQ header, 28 bytes UDP/IP, 4
bytes padding). On an input link of 10 Gbps, ASI receives per
second at most 6.51 ∗ 106 PREQ packets. Since the AS stores
PREPLY packets of 64 bytes each (32 bytes PREPLY header,
28 bytes UDP/IP, 4 bytes padding), for 225 ms on average, the
AS stores 93.74 MB of PREPLY packets. To store 6.51 ∗ 106
packets in a Bloom ﬁlter, with a small false positive rate of
0.01, each AS requires 7.8 MB. To store a timer between 100
ms and 350 ms, each AS stores a value between 0 and 250, and
adds the value 100. Thus, each timer is 1 byte. During 225 ms,
ASI stores 1.46 MB of timers. Thus, in total, ASI stores 103
MB for a 10 Gbps input link, and 1.23 GB when receiving
PREQ packets at a rate of 120 Gbps. Such a storage upper
bound is sustainable on the router slow path, for instance in
the router’s DRAM.

)
d
e
v
r
e
s
b
o
ψ
(

e
t
a
r

n
o
i
t
p
u
r
r
o
c

d
e
v
r
e
s
b
O

100

10-1

10-2

10-3

10-4

10-5

= 0. 00

ψThreshold
ρ ∗
i
ρ ∗
i
ρ ∗
i

= 0. 05

= 0. 10

4

5

6

7

8

9

10

Length of AS path

Fig. 10: Theoretical bound rate ψ
observed for varying malicious link corruption rates ρ ∗
ψ
path lengths.

threshold and observed rate
i and

VIII. SIMULATION

We evaluate Faultprints through a simulation of the fault
localization accuracy against various types of attacks. An
adversary tries to maximize her inﬂicted damage by corrupting
(drop, modify, and delay) any type of packet in the pro-
tocol. We ﬁrst validate the maximum end-to-end corruption
rate calculated in Section VII-A. We then evaluate the fault
localization accuracy using corruption scores and misbehavior
probabilities, as described in Section V.

Simulation Setup. Each simulation scenario uses a forward
network path consisting of up to 10 ASes,
including the
source and the destination domains. According to a CAIDA
study [15], the vast majority of Internet paths have at most 10
AS hops.

Our simulation scenarios consider one malicious node, at
random locations on the forward or reverse path. We set the
natural packet loss of each link Li to ρi = 0.001. Each result
represents an average over 1000 runs.

A. End-to-end maximum corruption rate

In this simulation, a source sends 4000 DATA packets to
a destination along AS-level paths of various lengths. The
adversary corrupts packets with an adjustable rate ρ ∗
i , ranging
from 0 (no malicious activity in the network, only natural
packet drop) to 0.1. Figure 10 depicts the measured end-to-
end corruption rate ψ
observed. Based on this value, the source
assesses whether adversaries on the path corrupt packets, as
follows: The measured end-to-end corruption rate is always
lower than ψ
threshold when no adversary is present on the
path (case ρ ∗
i = 0), yet there is natural packet drop. Paths
with adversaries where ρ ∗
(cid:10) ρi may evade detection, when
the exhibited natural packet loss is very small. But these
adversaries cause very little damage. By contrast, paths with
adversaries with higher ρ ∗
i always result in an end-to-end
corruption rate ψ

observed higher than ψ

threshold.

i

B. Localization accuracy

We now evaluate the effect on localization accuracy of the
following parameters: the Bloom ﬁlter false positive rate, the

872872

number of data packets sent by the source, and the probing
rate. Localization accuracy is deﬁned as the probability for a
source to successfully localize a malicious AS on its routing
path, within an epoch, given an upper bound on the number
of data packets sent by the source. The simulation shows
how the adversary cannot escape detection once it corrupts
DATA and DACK packets:
if the adversary also corrupts
PREQ and PREPLY packets, it is localized using misbehavior
probabilities, and if the adversary behaves correctly w.r.t.
PREQ and PREPLY packets, it is localized using corruption
scores. The source cannot know the adversary strategy, but it
knows there is at least an adversary on the path, because of a
higher end-to-end corruption rate than the threshold. Therefore
the source computes both scores, and localizes according to
the scoring method that yields the highest difference between
an AS (the malicious one) and the others.

Our methodology is to ﬁnd the minimum number of data
packets for Faultprints to achieve localization with high ac-
curacy. Therefore,
the source sends a
ﬁxed, sufﬁciently-large amount of data packets. The malicious
corruption rate of links is ρ ∗

in each experiment

i = 0.05.

Corruption scores. We ﬁrst compute the corruption scores
of each node, which considers only correct probe replies.
Correct probe replies carry wrong information only when there
are Bloom ﬁlter false positives. Therefore, we vary the false
positive rate between 0.01 and 0.04. The probing rate is PProbe
= 0.1. Figure 12 shows the localization accuracy, denoted by δ ,
based on the corruption score calculation from collected probe
replies. We observe that the Faultprints protocol reaches a high
localization accuracy (e.g., δ ≥ 0.95) when the total number
of data packet sent by source exceeds 3000. On the other
hand, δ only slightly decreases when the false positive rate of
the Bloom ﬁlters increases, which implies the AS storage can
be signiﬁcantly reduced, e.g., corresponding to a Bloom ﬁlter
false positive rate of 0.04.

However, probe reply packets could be corrupted by nat-
ural packet
loss. Even with such incomplete information,
the corruption scores computed by the source should still
allow it to distinguish benign cases from cases with malicious
activity, given a known packet loss rate ρi. We simulate this
scenario with a source sending 4000 packets along paths of
varying length. We also vary the malicious link corruption
rates. Figure 11 depicts the average of highest corruption
score gap computed by the source in relation to the threshold
upper bound.4 Score gaps higher than the threshold indicate
malicious activity. The threshold upper bound is given by
the malicious activity rate threshold ψ
threshold multiplied by
the total number of data packets and by PProbe, for various
AS path length. The ﬁgure shows that corruption score gaps
for benign cases are always lower than the threshold upper
bound, and the opposite for cases with adversarial activity.
Even more, although the deviation of corruption score gaps

4The corruption score gap is the difference between the corruption scores

of neighboring ASes

p
a
g

e
r
o
c
s

n
o
i
t
p
u
r
r
o
c

t
s
e
h
g
h

i

f
o
g
v
A

20

18

16

14

12

10

8

6

4

2

0

Threshold Upper Bound

ρ ∗
i
ρ ∗
i
ρ ∗
i

=0.0

=0.05

=0.10

4

5

6

7

8

9

10

Length of AS path

)

%

(

n
o
i
t
a
c
i
n
u
m
m
o
C

l

a
n
o
i
t
i
d
d
A

14

13

12

11

10

9

8

7

6

5

4

3

2

1

0

Average
Theoretical Upper Bound

10

9

8

7

6

5

4

1.67

1.81

2.50

2.60

2.18

3.04

3.29

4

5

6

7

8

9

10

Length of AS path

Fig. 11: Average and deviation of highest corruption score gaps
computed by source, for varying malicious link corruption
rates ρ ∗
i and varying path lengths. AS parameters are PProbe
= 0.1 and FPBf = 0.02.

Fig. 14: Communication overhead along various path lengths:
theoretical upper bound in plain colors, and average case in
pattern colors.

y
c
a
r
u
c
c
A
n
o
i
t
a
z
i
l

a
c
o
L

1.00

0.95

0.90

0.85

0.80

# DATA = 1000

# DATA = 2000

# DATA = 3000

# DATA = 4000

0.010

0.015

0.020

0.025

0.030

0.035

0.040

False positive rate of Bloom filter

Fig. 12: Localization accuracy of corruption scores, with
varying sending rates of DATA packets and false positive rate
of Bloom ﬁlter.

1.00

0.95

0.90

0.85

0.80

0.75

0.70

0.65

y
c
a
r
u
c
c
A
n
o
i
t
a
z
i
l

a
c
o
L

# DATA = 1000

# DATA = 2000

# DATA = 3000

# DATA = 4000

0.05

0.10

0.15

0.20

Probe Rate

Fig. 13: Localization accuracy of misbehavior probabilities,
with varying sending rates of DATA packets and probe rate
PProbe.

increases with path length, the source still correctly identiﬁes
adversarial activity.

based on absent or corrupt probe replies. Thus, in this scenario,
we adjust the probing rate PProbe to collect either more or
fewer PREPLY packets. The simulation result
is given in
Figure 13, and shows the probabilistic model can achieve a
high localization, i.e., δ ≥ 0.95 with a PProbe of 0.15 and
4000 DATA packets.

We summarize that Faultprints can perform localization
with high accuracy when the source either sends enough
data packets, or performs more aggressive probing. In fact,
even with only 1000 packets, and a PProbe of 0.1, Faultprints
achieves over 75% accuracy. This is especially useful for an
enterprise sending many of its ﬂows along the same path, for
instance between two remote ofﬁces. An intermediate AS that
tries to sabotage the communication is detected quickly by
Faultprints.

C. Probing overhead

We evaluate the bandwidth required by PREQ and PREPLY
packets (Figure 14). We set PProbe to 0.1, to obtain a good
localization accuracy. The result conﬁrms that the total com-
munication overhead increases with the number of transit ASes
on the routing path. The average overhead incurred by probing
is between 1.6% – 3.2%, with non-signiﬁcant deviation.

IX. IMPLEMENTATION & EVALUATION

The software router is a commodity server in our testbed,
equipped with two 8-core Intel Xeon E5-2680 2.7 GHz CPUs,
four banks of 16 GB DDR3 RAM, and six dual-port Intel
82599EB X520-DA2 10GbE NICs. The software router is con-
nected to a high-throughput trafﬁc generator – a Spirent SPT-
N4U-220 machine [6]. The trafﬁc generator is both a source
and a destination, sending 120 Gbps to the Faultprints software
router, which the router forwards back to the generator.

A. Software Router Implementation

Misbehavior probabilities. An adversary that corrupts PREQ
and PREPLY packets tries to prevent the source from calcu-
lating corruption scores based on correct PREPLY packets.
However, the probabilistic model localizes a malicious AS

The software router performs all Faultprints operations
when forwarding DATA packets: DRKey key computation,
session authenticator computation, packet sampling, and ﬁn-
gerprint storing in the Bloom ﬁlter.

873873

To evaluate the performance of Faultprints, we choose Intel
DPDK [4] as a packet I/O engine to implement Faultprints
data forwarding on the software router. DPDK is a highly-
efﬁcient packet I/O engine. Directly sending packets to the
user application through a continuous polling mechanism,
DPDK is suitable for high-bandwidth trafﬁc settings.

To implement the Bloom ﬁlters, we use the open source
library libbloom5. This library reduces the number of required
hashes to a constant: only two hashes are required without loss
in any asymptotic false positive rate [25]. The router proba-
bilistically samples packets in its Bloom ﬁlter by evaluating
the PRF on the H(Cst(DATA)), included in the Faultprints
header.

To achieve high-speed cryptographic processing, we use the

AES-NI instruction set [3] supported by Intel processors.

CBC-MAC vs. PMAC. Faultprints requires a MAC compu-
tation over the entire packet payload (Equation 7), resulting
in poor performance for a conventional algorithm such as
CBC-MAC, because the operation cannot be parallelized: each
block encryption depends on the output of the previous block
encryption. To increase the throughput, we implement the
MAC functionality using PMAC [12], a parallelizable algo-
rithm. We implement PMAC making full use of the parallelism
in the hardware-accelerated AES-NI architecture: each core
has its own 128-bit xmm registers and AES engine. Our
implementation issues simultaneously four block encryptions
without any chaining, leading to a speed up factor up to 4x.
We compare PMAC against the CBC-MAC implementation
in the Intel AES-NI sample library. We measure the exact
clock cycles using the rdtsc instruction for both algorithms.
The result is summarized in Figure 15. For very small inputs,
e.g., 16 or 32 bytes, PMAC performs worse than CBC-MAC.
However, for larger inputs, PMAC is up to ﬁve times faster
than CBC-MAC.

As an example, consider each core processes trafﬁc on
one 10Gbps port saturated with 512-byte packets. Using our
PMAC implementation, a 2.7GHz Intel Xeon can process up
to 1.698 GBytes per second (≈13.56 Gbps), reaching line-
rate processing on 10Gbps links. By contrast, CBC-MAC
processing supports only up to 4.04 Gbps.

In our evaluation, the minimum packet size is 145 B, as
we explain below. For such packet sizes, PMAC always out-
performs CBC-MAC, thus we evaluate Faultprints throughput
and goodput using only our PMAC implementation.

B. Router throughput and goodput

We generate 120Gbps of trafﬁc of various packet sizes, send
the packets to the Faultprints router. For each port connected
to the software router, Spirent measures the sending bit rate
(TX rate) and the receiving bit rate (RX rate). We deﬁne the
forwarding efﬁciency by dividing the RX rate and the TX rate.
We set the sample rate PSample = 0.1, the false positive rate
of the Bloom ﬁlter = 0.02, and the path length to 5 ASes. We
compute the goodput as the amount of useful forwarded data,

5https://github.com/jvirkki/libbloom

20

18

16

14

12

10

8

6

4

2

0

)
s
d
n
u
o
r
k
0
1
r
e
v
o

e
g
a
r
e
v
a
(

B
P
C

CBC-MAC
PMAC

15.42

10.28

10.28

7.73

6.47

5.28

5.83

3.31

5.51

5.35

5.30

5.26

5.23

2.17

1.59

1.31

1.17

1.10

16

32

64

128

256

512

1024

2014

4096

Input Size (bytes)

Fig. 15: CPB (Cycle Per Byte) for 1 key expansion and 1
MAC operation using CBC-MAC and PMAC, over various
input sizes, averaged over 105 computations.

namely the forwarded data without Faultprints’s overhead.
Faultprints’s overhead is composed of the Faultprints header
of each DATA packet (85B + 4 ∗ nrAS = 105B), the Faultprints
header of each DACK packet (80B + 4 ∗ nrAS = 100B), and
the probing overhead (1.81% for 5 hops in Section VIII-C).
Since links are full-duplex, and the DATA and DACK packets
travel in opposite directions, measuring the goodput along the
forward path accounts only for DATA packet overhead, but not
for DACK packet overhead. To capture both overheads, we
consider a scenario with two identical DATA ﬂows traversing
the link in opposite directions. In this scenario, the overhead
of one ﬂow’s DACK packets reﬂects on the goodput of the
other ﬂow.

The smallest DATA packet in our evaluation consists of a
TCP/IP header of 40 B, and the Faultprints header, in total
145 B. We vary the DATA packet size up to 1500 B. DACK
packets always consist of the smallest TCP/IP header and the
Faultprints header, in total 140 B.

First we measure the baseline case, when the router per-
forms only DPDK forwarding. The sum of the RX rates on
all ports is shown in Figure 16. In the analyzed cases, DPDK
degrades for packets of up to 256 bytes: for 145-byte packets,
DPDK achieves 97.87 Gbps, and for 256-byte packets, 115.42
Gbps. Packets of at least 512 bytes are forwarded at the line
rate, 120 Gbps.

Figure 16 shows that a Faultprints router can process nearly
120Gbps of trafﬁc for DATA packets of 1024 B or larger,
and their corresponding DACK packets of 140 B. For smaller
DATA packets, the throughput degrades to 94.49 Gbps, 79.44
Gbps, and 52.2 Gbps for packet sizes of 512 B, 256 B, and
145 B, respectively. Compared to throughput, the goodput
results peak at 102.86 Gbps, when forwarding 1500 B DATA
packets. This is a degradation of 14% of goodput compared to
throughput. The goodput degrades more for smaller packets,
because the Faultprints header constitutes a higher proportion
of the DATA and DACK packets. For DATA packets of 145 B,
256 B, 512 B, and 1024 B, the goodput degradation compared
to throughput is 72.45%, 52.65%, 33.01%, and 19.1%.

To understand Faultprints’s performance in real-world con-
ditions, we also evaluate the throughput for typical Internet

874874

)
s
p
b
G

(
 
y
c
n
e
c
i
f
f

i

e

 

i

g
n
d
r
a
w
r
o
F

 160

 140

 120

 100

 80

 60

 40

 20

 0

Packet I/O (baseline)
Throughput (PMAC)
Goodput (PMAC)

S

PReq

M1

M2
PReply

Internet

I

D

145 B

256 B

512 B

1024 B

1500 B      Internet MIX

DATA packet size

Fig. 17: PREPLY packets must be delayed by at least an RTT,
to defeat a timing attack launched by colluders ASM1 and
ASM2.

Fig. 16: Data plane throughput and goodput for IPv4 packets
of 145 to 1500 bytes, and typical IPv4 Internet trafﬁc [16].

trafﬁc (Internet MIX), using CAIDA’s packet size distribution
for IPv4 trafﬁc [16]. In this case, Faultprints achieves an
overall throughput of 116.2 Gbps, and a goodput of 94 Gbps.
We thus conclude that Faultprints is suitable for high-speed

routers.

X. FAULTPRINTS OVER ASYMMETRIC PATHS

Faultprints is an efﬁcient and scalable approach for fault
localization along Internet symmetric paths. Internet paths,
however, are frequently asymmetric [21]. We explain the
hurdles Faultprints overcomes towards fault localization along
asymmetric paths, and the remaining challenges.

First, the source requires, besides the forward path, knowl-
edge of all AS-level reverse paths to itself originating from
each AS on the forward path. Reverse traceroute [23] aims to
provide such paths, however, it is not widely deployed. Instead,
as we only require AS-level paths, one can use the techniques
of Nithyanand et al. [31]. They infer such paths by combining
maps of the Internet topology with algorithmic simulations.

The Faultprints protocol described in Section IV requires
changes to handle packets traversing return paths: DACK
packets, and PREPLY packets. DACK packets are sampled by
ASes on the return path, requiring a key shared between these
ASes and the source. This requires minimal changes to the
protocol: during key setup, the packet sent by the destination
to the source must include SESSIONID, allowing ASes on
the return path to derive a key, and append it to the packet
encrypted and signed.

Faultprints already eases support of asymmetric paths, by
requiring each AS to send a separate PREPLY packet. The
alternative, as done by Zhang et al. [38], is to bundle replies
into a single PREPLY packet: each AS on the path starts a
timer when it observes the PREQ packet, and either sends
its own PREPLY packet when the timer expires, or appends
its reply bits to a PREPLY packet received from upstream,
onion-authenticates it, and deletes the timer. The advantage of
this approach is a smaller number of PREPLY packets, namely
one instead of N, where N is the path length, and a smaller
total size of PREPLY packet contents, namely smaller by N-1

UDP/IP headers ((N − 1) · 28 B). Also, the approach avoids
by construction timing attacks on PREPLY packets. But their
approach works only for symmetric paths. Besides, it still
requires the same storage as Faultprints on the router slow
path, namely for the timer and the data required to generate
the PREQ packet.

The most difﬁcult challenge is to ensure PREPLY packet
indistinguishability regarding their origin AS. Without this
property, ASes could frame other ASes of misbehavior. PRE-
PLY packets remain indistinguishable w.r.t.
timing attacks,
where an attacker on the forward path observes the PREQ
packet, and informs its colluder on the return path about the
timing. Since PREPLY packets are delayed, the colluder cannot
leverage time correlation (Figure 17). Moreover, it is essential
for the reply bits to be encrypted. Otherwise, if bits were not
encrypted, colluding nodes could launch an attack as follows:
the ﬁrst colluder drops DATA packets, causing all subsequent
nodes to reply with bitAuthmodif = 0. When this colluder is not on
the return paths taken by probe replies, but the second colluder
is located on some or all return paths, the second colluder can
simply drop replies containing bitAuthmodif = 0. For instance,
it drops all such replies originating only at non-neighboring
nodes, otherwise it could be localized. In this case, a benign
node is incriminated.

Yet, PREPLY indistinguishability along asymmetric return
paths depends on the network topology. Indistinguishability
requires cover trafﬁc to prevent identiﬁcation of the correct
packet. However, in the extreme case of completely disjoint
return paths, each such path carries a single PREPLY packet
towards the source. Unless an AS observes sufﬁcient additional
Faultprints trafﬁc towards the same source,
the AS could
identify PREPLY packets. Ensuring a sufﬁcient number of
packets (that are indistinguishable to the PREPLY packet) sent
to the same source is challenging to accomplish.

Recent work of Internet path topology suggest that paths are
rarely completely disjoint. For instance, de Vries et al. [33]
found that ASes on a forward path have a probability of at
least 0.6 to be situated also on the reverse path. These results
are encouraging, but quantifying the effect of path asymmetry
on PREPLY indistinguishability in Faultprints remains subject
of future work.

875875

Internet

S

D

Fig. 18: Early-adopter ASes (dark colors) identify a faulty
segment along the thicker line. The source chooses another
path, giving incentive to the circled ASes to deploy Faultprints.

XI. DISCUSSION

Incremental deployment. ASes have the incentive to deploy
Faultprints to attract more customers, especially those that
want high availability. In turn,
these ASes would prefer
paths through ASes that also deploy Faultprints. Deployment
incentives are especially strong in source-controlled path ar-
chitectures, because end hosts can select paths through ASes
that deploy Faultprints.

Even if not all ASes on the path adopt Faultprints, early-
adopter ASes allow sources to identify path segments (which
may contain a single link or multiple ASes) where malicious
activity occurs. Sources with such knowledge can then choose
a path that avoids the troublesome segment. Of course, such
a decision affects potential benign ASes on the suspicious
segment. To maintain their trafﬁc, such benign ASes can
deploy Faultprints to keep themselves on the paths chosen
by sources. The scenario is depicted in Figure 18, where the
thick path segment is localized as malicious.

Path stability. Although paths may change, Faultprints is still
effective as long as sufﬁciently many packets are sent along
a path. However, Faultprints is used on paths that inherently
exhibit errors, which tend to be unstable. For instance, a highly
unreliable link may cause paths to fail often. In this case,
DRKey can be performed several times, until every AS on the
various paths shares a key with the source. As long as DRKey
uses the same session identiﬁer, up to a maximum time period
determined by the session timestamp, ASes derive the same
keys. Then, regardless of the path used, the source accumulates
information about faults, and when enough information is
collected, the source localizes the faulty link.

Faultprints’s deployment over SCION allows Faultprints to
control the path used. In fact, when BGP would issue a route
change because of a link failure, leaving the source with no
choice to choose a path, SCION allows the source to purposely
forward trafﬁc along the faulty path, so that the source itself
localizes the faulty link. Path choice empowers a source to
keep a session alive until a fault is localized, if the source
desires so. Afterwards, the source can choose another path,
and learns to avoid paths containing the unreliable link. In
the case of BGP, the source still avoids for the moment the
malicious AS due to the route change, but never learns whom
to avoid in the future (e.g., if the source is an AS, it can use

the knowledge to avoid peering with the malicious AS).

Faultprints works in the current Internet on stable paths.
Although the paths used by Faultprints tend to exhibit errors,
some types of errors do not cause a BGP route change. For
instance, if a core AS selectively drops only the trafﬁc of
sources from a small country or organization, effectively an
example of censorship, the drop rate may be small enough
w.r.t. the total amount of trafﬁc forwarded by the AS that
its neighbor does not issue a route change. However, the
discriminated sources continue to use the path and localize
the error.

XII. CONCLUSION

Despite the importance of malicious AS localization, cur-
rently there is no fault localization protocol that is viable in
inter-domain settings. Through probabilistic packet sampling
at ASes, which is deterministic for the source but unpredictable
for other ASes, Faultprints considerably reduces the storage
requirements compared to other protocols, while retaining their
accuracy. Faultprints is secure against malicious ASes that
alone or together with colluders try to cover their misbehavior,
or try to frame other ASes. Also, Faultprints can execute
efﬁciently on a commodity PC: the evaluated throughput for
Internet MIX trafﬁc is 116.2 Gbps, and the goodput is 94 Gbps.
We anticipate that Faultprints brings us closer to localizing and
deterring malicious ASes in the Internet.

ACKNOWLEDGMENT

We would like to thank Jun Han for his help during the
early stage of this project. We also thank Cristina Nita-Rotaru,
Virgil Gligor, Samuel Hitz, Joel Reardon, and the anonymous
reviewers for their valuable comments that helped improve the
paper.

The research leading to these results has received funding
from the European Research Council under the European
Union’s Seventh Framework Programme (FP7/2007-2013) /
ERC grant agreement 617605.

We also gratefully acknowledge support by ETH Zurich,
by NSF under award number CNS-1040801, and by Intel for
their equipment donation that enabled the high-capacity router
experiments.

REFERENCES

[1] Accusation of ISPs abusing their market power in peering.

http:

//gigaom.com/2014/05/05/level-3-accuses-ﬁve-unnamed-us-isps-of-
abusing-their-market-power-in-peering.

[2] Arbor

security

report.

https://www.arbornetworks.com/resources/

infrastructure-security-report.

[3] Intel AES-NI white paper. http://www.intel.com/content/www/us/en/

enterprise-security/enterprise-security-aes-ni-white-paper.html.

[4] Intel Data Plane Development Kit (DPDK). http://dpdk.org/.
[5] NSA tampers with US-made routers. http://www.theguardian.com/

books/2014/may/12/glenn-greenwald-nsa-tampers-us-internet-routers-
snowden.

[6] Spirent

SPT-N4U-220

chassis.

http://www.spirent.com/Ethernet

Testing/Platforms/N4U Chassis.

[7] K. Argyraki, P. Maniatis, O. Irzak, S. Ashish, and S. Shenker. Loss and
delay accountability for the Internet. In IEEE International Conference
on Network Protocols (ICNP), 2007.

876876

[8] K. Argyraki, P. Maniatis, and A. Singla. Veriﬁable Network-performance

Measurements. In Proceedings of ACM CoNEXT, 2010.

[25] A. Kirsch and M. Mitzenmacher. Less hashing, same performance:
Building a better Bloom ﬁlter. Random Structures & Algorithms, 2008.

[9] V. Asturiano. Update on AS path lengths over time. https://labs.ripe.net/

Members/mirjam/update-on-as-path-lengths-over-time.

[10] B. Awerbuch, R. Curtmola, D. Holmer, C. Nita-Rotaru, and H. Rubens.
ODSBR: An on-demand secure Byzantine resilient routing protocol for
wireless ad hoc networks. ACM Transactions on Information and System
Security, 2008.

[11] B. Barak, S. Goldberg, and D. Xiao. Protocols and lower bounds
In Advances in Cryptology

for failure localization in the Internet.
(EUROCRYPT), 2008.

[12] J. Black and P. Rogaway. A block-cipher mode of operation for
In Advances in Cryptology

parallelizable message authentication.
(EUROCRYPT), 2002.

[13] B. H. Bloom. Space/time trade-offs in hash coding with allowable errors.

Communications of the ACM, 1970.

[14] K. A. Bradley, S. Cheung, N. Puketza, B. Mukherjee, and R. A. Olsson.
Detecting disruptive routers: A distributed network monitoring approach.
EEE Network, 1998.

[15] CAIDA. AS path lengths. https://www.caida.org/research/trafﬁc-

analysis/ﬁx-west-1998/aspathlengths/.

[16] CAIDA. Packet size distribution comparison between Internet links in

1998 and 2008. http://www.caida.org/research/trafﬁc-analysis/pkt size
distribution/graphs.xml.

[26] R. Landa, J. Araujo, R. Clegg, E. Mykoniati, D. Grifﬁn, and M. Rio. The
large-scale geography of internet round trip times. In IFIP Networking
Conference, 2013.

[27] B. Liu, J. T. Chiang, J. J. Haas, and Y.-C. Hu. Coward attacks
in vehicular networks. ACM SIGMOBILE Mobile Computing and
Communications Review, 2010.

[28] A. Malhotra, I. E. Cohen, E. Brakke, and S. Goldberg. Attacking the
network time protocol. In Proceedings of IS Network and Distributed
System Security Symposium (NDSS), 2016.

[29] D. L. Mills. Executive summary: Computer network time synchroniza-

tion. https://www.eecis.udel.edu/∼mills/exec.html.

[30] A. T. Mizrak, Y. chung Cheng, K. Marzullo, and S. Savage. Fatih:
In IEEE Transactions on

Detecting and isolating malicious routers.
Dependable and Secure Computing, 2005.

[31] R. Nithyanand, O. Starov, A. Zair, P. Gill, and M. Schapira. Measuring
In Proceedings of IS

and mitigating as-level adversaries against tor.
Network and Distributed System Security Symposium (NDSS), 2016.

[32] M. Schuchard, A. Mohaisen, D. Foo Kune, N. Hopper, Y. Kim, and
E. Y. Vasserman. Losing control of the internet: using the data plane to
attack the control plane. In Proceedings of the 17th ACM conference
on Computer and communications security (CCS), 2010.

[17] CAIDA. Round-Trip Time Internet Measurements. https://www.caida.

[33] W. Vries, J. J. Santanna, A. Sperotto, and A. Pras. ”how asymmetric is

org/research/performance/rtt/walrus0202/.

[18] P. B. Godfrey, I. Ganichev, S. Shenker, and I. Stoica. Pathlet rout-
ing. Proceedings of the ACM SIGCOMM 2009 conference on Data
communication, 2009.

[19] S. Goldberg, D. Xiao, E. Tromer, B. Barak, and J. Rexford. Path-quality
monitoring in the presence of adversaries: The secure sketch protocols.
IEEE/ACM Transactions on Networking, 2014.

[20] J. R. Hughes, T. Aura, and M. Bishop. Using conservation of ﬂow as
In IEEE Symposium on

a security mechanism in network protocols.
Security and Privacy (S&P), 2000.

[21] W. John, M. Dusi, and K. Claffy. Estimating routing symmetry on single

links by passive ﬂow measurements. 2010.

[22] M. S. Kang, S. B. Lee, and V. D. Gligor. The Crossﬁre Attack.

In
Proceedings of the 2013 IEEE Symposium on Security and Privacy
(S&P), 2013.

[23] E. Katz-Bassett, H. V. Madhyastha, V. K. Adhikari, C. Scott, J. Sherry,
P. Van Wesep, T. Anderson, and A. Krishnamurthy. Reverse traceroute.
In Proceedings of the 7th USENIX Conference on Networked Systems
Design and Implementation (NSDI), 2010.

[24] T. H.-J. Kim, C. Basescu, L. Jia, S. B. Lee, Y.-C. Hu, and A. Perrig.
Lightweight source authentication and path validation. In Proceedings
of the 2014 ACM Conference on SIGCOMM, 2014.

the internet?”. In Proceesings of AIMS, 2015.

[34] F. Zhang, L. Jia, C. Basescu, T. H.-J. Kim, Y.-C. Hu, and A. Per-
rig. Mechanized network origin and path authenticity proofs.
In
Proceedings of the 2014 ACM SIGSAC Conference on Computer and
Communications Security (CCS), 2014.

[35] X. Zhang, H.-C. Hsiao, G. Hasker, H. Chan, A. Perrig, and D. G.
Andersen. Scion: Scalability, control, and isolation on next-generation
networks. In IEEE Symposium on Security and Privacy, 2011.

[36] X. Zhang, C. Lan, and A. Perrig. Secure and scalable fault localization
In IEEE Symposium on Security and

under dynamic trafﬁc patterns.
Privacy (S&P), 2012.

[37] X. Zhang, Z. Zhou, G. Hasker, A. Perrig, and V. Gligor. Network fault
localization with small TCB. In Proceedings of the IEEE International
Conference on Network Protocols (ICNP), 2011.

[38] X. Zhang, Z. Zhou, H.-C. Hsiao, T. H.-J. Kim, A. Perrig, and P. Tague.
In Proceedings
ShortMAC: Efﬁcient Data-plane Fault Localization.
of the Network and Distributed System Security Symposium (NDSS),
2012.

[39] W. Zhou, Q. Fei, A. Narayan, A. Haeberlen, B. T. Loo, and M. Sherr.
Secure Network Provenance. In Proceedings of ACM Symposium on

Operating Systems Principles (SOSP), 2011.

877877

