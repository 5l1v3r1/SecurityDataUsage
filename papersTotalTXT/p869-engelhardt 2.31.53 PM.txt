Intransitive Noninterference in Nondeterministic Systems∗

Kai Engelhardt

Computer Science and

Engineering, The University of

New South Wales, Sydney,
kaie@cse.unsw.edu.au

NSW 2052, Australia

Ron van der Meyden
Computer Science and

Engineering, The University of

New South Wales, Sydney,

NSW 2052, Australia

meyden@cse.unsw.edu.au

†

Chenyi Zhang

Information Technology and
Electrical Engineering, The
University of Queensland,
chenyi@uq.edu.au

Brisbane, QLD 4072, Australia

ABSTRACT
This paper addresses the question of how TA-security, a semantics
for intransitive information-ﬂow policies in deterministic systems,
can be generalized to nondeterministic systems. Various deﬁni-
tions are proposed, including deﬁnitions that state that the system
enforces as much of the policy as possible in the context of attacks
in which groups of agents collude by sharing information through
channels that lie outside the system. Relationships between the
various deﬁnitions proposed are characterized, and an unwinding-
based proof technique is developed. Finally, it is shown that on a
speciﬁc class of systems, access control systems with local non-
determinism, the strongest deﬁnition can be veriﬁed by checking a
simple static property.

Categories and Subject Descriptors
D.4.6 [Security and Protection]: Information ﬂow controls

Keywords
access control, information-ﬂow, nondeterminism, noninterference,
security

1.

INTRODUCTION

The theory of information ﬂow security has been studied most
extensively with respect to transitive security policies, motivated by
the lattices associated with military multi-level security policies. It
has long been recognised, however, that even in this setting, richer
types of policies are required in order to deal with trusted compo-
nents such as downgraders, which may violate a transitive policy.
An example of this is given in Figure 1 which presents an abstract
architecture for a system in which two multi-level secure machines
M1 and M2 communicate across the internet.
∗Research supported by Australian Research Council Discovery
Grant DP1097203.
†Work of the third author conducted while employed at The Uni-
versity of New South Wales.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.

M1

M2

H1

E1

E2

H2

L1

NI1

N

NI2

L2

Figure 1: Architecture for a MILS system.

Each machine Mi contains a high-level domain Hi and a low-
level domain Li, with the usual policy Hi (cid:54)
(cid:55)→ Li, intuitively requir-
ing that no high-level information ﬂow to the low level domain,
enforced between the two. The internet is represented by the do-
main N. Additionally, there are two domains E1, E2 that represent
downgraders that are responsible for encrypting and decrypting all
communications between the domains H1, H2, as well as domains
NI1, NI2 representing the network interface in each machine.

Globally, the security policy requires that there is no direct ﬂow
of information from the domains Hi to the domains N, NIj or Lj.
All such ﬂow of information must be mediated by the domains Ei.
This does not provide a complete guarantee of all security proper-
ties that one might wish the system to satisfy, but it helps to focus
proofs that high-level information remains secure onto the speciﬁc
trusted components Ei. The intention of the architecture is to de-
compose the proof of the desired security property that high-level
information does not ﬂow to low domains (even if only the compo-
nents Ei are trusted), to the proof that the components Ei properly
encrypt all output (and possibly also, that they maintain a trafﬁc
stream to circumvent trafﬁc analysis), and the fact that the archi-
tecture is enforced. We refer the reader to recent work on MILS [4]
for a more detailed explanation of this idea.

In order to provide an account of formal security veriﬁcation that
captures the intuitions underlying such architectures, we ﬁrst need
a mathematically precise semantics for policies in the form of in-
transitive relations (note, e.g., that in Figure 1 we have H1 (cid:55)→ E1
and E1 (cid:55)→ NI1 but not H1 (cid:55)→ NI1). Compared to classical in-
formation ﬂow theory for transitive policies, this area is much less
studied.

One of the landmarks in the area remains the work of Rushby [21],
which clariﬁed earlier work of Haigh and Young [10]. In particular,
Rushby provides an proof method using unwinding relations that
may be used to show security of a system, and moreover proves
that secure systems can be concretely constructed by using an ac-
cess control discipline satisfying a simple syntactic condition. This
latter result is signiﬁcant in that it can be understood as providing
a more satisfactory basis for ideas from Bell and La Padula [2],
addressing complaints about the well-foundedness of Bell and La

869Padula’s methods [14], by giving semantic meaning to the notions
of read and write.

Recently, van der Meyden [24] pointed to weaknesses in Rushby’s
deﬁnitions and provided improvements, including a new deﬁnition
of security called TA-security, that yield results similar to Rushby’s
but which make the unwinding proof method and access control
discipline not just sound but also complete for security (showing
that any secure system can be proved to be secure using the method,
or constructed so that security is easily checkable.) This yields a
pleasant theory in which the deﬁnition of security, proof methods
and engineering discipline are tightly integrated.

This theory is limited to deterministic systems, however. There
have been proposals for semantics of intransitive policies in non-
deterministic systems [3, 19, 13, 1, 26], but none of these works
deals with access control systems with nondeterminism. Our con-
tribution in this paper is to develop the ﬁrst such generalization,
taking as our starting point van der Meyden’s formulation of the
deterministic case.

We ﬁrst investigate how to generalize the notion of TA-security
to the nondeterministic setting. After setting up the semantic frame-
work in Section 2, in Section 3 we carefully tease out a number
of dimensions that are relevant to the formulation of the seman-
tics for intransitive policies in nondeterministic systems. Some
deﬁnitions in the literature, we believe, have made inappropriate
choices with respect to these ingredients, and in some, critical is-
sues have been ignored. In particular, we argue that it is impor-
tant in non-deterministic systems to base the deﬁnition of security
on the effect of actions on agents’ history of actions and observa-
tions, whereas some deﬁnitions in the literature have considered
only their effect on single observations. We also recall from the
literature on noninterference with respect to transitive policies the
notion of persistence, which helps with an important distinction be-
tween deducibility and causality that emerges in nondeterministic
systems. Finally, we present an example showing that while col-
lusion attacks can be ignored in deterministic systems, they need
to be taken into account in nondeterministic systems. Many of the
deﬁnitions in the literature do not take such attacks into account.

We give our generalizations of TA-security to the nondetermin-
istic setting in Section 4. We show that there are subtleties in the
formulation of deﬁnitions that cover collusion attacks: it makes a
difference, to what the attackers can deduce, whether they share
information during a run of the system, or only at completion of
the run. This leads us to a spectrum of deﬁnitions, depending on
whether and, if so, how, collusion is treated, and via application
of persistence or not, whether the deﬁnition is causal or deductive.
We discuss some special cases of policies and systems in Section 5,
where we show that our spectrum of deﬁnitions collapses to two
well-known deﬁnitions of security in the case of deterministic sys-
tems or a two agent policy.

We then proceed, in Section 6, to develop an unwinding proof
technique that is sound for all our deﬁnitions.
In Section 7, we
present a generalization of access control systems that covers non-
determinism, and identify conditions on such systems that imply
that all our deﬁnitions of security hold. We discuss related work
in Section 8 and conclude with a discussion of future directions for
research in Section 9.

2. SYSTEMS MODEL

Goguen and Meseguer [7] developed a theory of information-
ﬂow that has formed a starting point for later research. They intro-
duced the following policy model.

Deﬁnition 1. A noninterference policy for a set U of security
domains is a reﬂexive binary relation (cid:55)→ on U, representing the
permitted interferences between domains.
Intuitively, u (cid:55)→ v represents that the policy permits information
to ﬂow from domain u to domain v. Another intuition for this is
that actions of domain u are permitted to have causal effects on, or
interfere with, domain v. Conversely, when u (cid:54)
(cid:55)→ v, domain u is
not permitted to interfere with v. Reﬂexivity is assumed since, in
general, nothing can be done to prevent ﬂows of information from a
domain to itself, so it is not sensible for the policy to prohibit this. A
machine with domains U is a tuple M = (S, s0, A,−→, obs, dom)
with a set S of states, including a designated initial state s0 ∈ S,
a set A of actions, a (nondeterministic) transition relation −→ ⊆
S × A× S an observation function obs : U → (S → O), for some
set O, and a domain function dom : A → U. We write obsu for
the function obs(u) : S → O, which represents the observation
that domain u makes at each state of the machine. Following much
of the security literature, we assume that the transition relation is
input-enabled: for all states s and actions a, there exists a state t
a−→ t. This helps to prevent enabledness of actions
such that s
being a source of information. A machine is deterministic if for all
states s, t, t(cid:48) and actions a, if s a−→ t and s a−→ t(cid:48) then t = t(cid:48).

Notational and diagrammatic conventions.

Sequences play an important role in this paper. Most of the time,
we denote a sequence such as [a, b, c] by just abc, however, if the
sequence elements have structure themselves, or if confusion is
likely to arise, we tend to separate sequence elements with a dot:
a · b · c.
A concise way to deﬁne A and dom is to list, for each u ∈ U,
the set Au = { a ∈ A | dom(a) = u } of actions of that domain.
Given a domain u, we write (cid:55)→u for { v ∈ U | v (cid:55)→ u } and
(cid:55)→u for U \ (cid:55)→u = { v ∈ U | v (cid:54)
(cid:55)→ u }. We also write u(cid:55)→ for
{ v ∈ U | u (cid:55)→ v } and u(cid:54)

(cid:55)→ for U \ (cid:55)→u.

We use the following convention in diagrams representing ma-
chines. States are represented by circles labelled internally by the
state name. The initial state is always named s0. A transition
s a−→ t is represented by an edge from s to t labelled by a. To re-
move clutter from diagrams, we elide edges corresponding to self-
a−→ s unless we wish to draw attention to them; thus, if
loops s
there is no edge from a state labelled by an action a, then using the
input-enabled assumption, we infer that the missing edge is a self-
loop. (Note that if there does exist an edge labelled a from state s,
we do not make this inference.) States are labelled externally by
observations of some of the domains: the details of this depend on
the example and are given with each diagram.
a1−→ s1 . . . an−→ sn in which
n ≥ 0, the si ∈ S are states (the ﬁrst being the initial state) and
the ai ∈ A are actions, such that (si−1, ai, si) ∈ −→ for all i =
1 . . . n. We write R(M ) for the set of runs of machine M. The
function last maps a nonempty sequence to its ﬁnal element. A
state is reachable if it is the ﬁnal state of some run. The sequence
of all actions in a run r is denoted Act(r). With M implicit, we
also write R(α) for the set of r ∈ R(M ) such that Act(r) = α.
For a domain u, we write Act u(r) for the subsequence of actions
a in Act(r) with dom(a) = u.

A run is a sequence of the form s0

The view of a run obtained by a domain, or group of domains,
records all the actions and observations of the domain or group dur-
ing the run, except that stuttering observations are collapsed to a
single copy to model that the agent/group operates asynchronously,
so does not perceive the passing of time unless some event hap-
pens to it. Let X ⊆ U be a nonempty set of domains. We de-

870(cid:54)
ﬁne the joint observation function of X as obsX : S → OX by
obsX (s)(u) = obsu(s) for u ∈ X. That is, obsX (s) is the tu-
ple of observations made by the domains u ∈ X on state s. The
view function viewX : R(M ) → (OX )+(A(OX )+)∗ is deﬁned
inductively by viewX (s0) = obsX (s0) and

viewX (r a−→ q) =(cid:40)

viewX (r) · a · obsX (q)
viewX (r) ˆ◦ obsX (q)

if dom(a) ∈ X
otherwise

where “ˆ◦” denotes absorptive concatenation, that is,

(cid:40)

α ˆ◦ a =

if last(α) = a

α
α · a otherwise.

We write the special case where X = {u} is a singleton as viewu.
We note that viewX (r) may contain information about the order
of actions from domains in X that cannot be deduced from the
collection of views (viewu(r))u∈X. Intuitively, viewX (r) is the
information that the group would obtain in r when the members of
the group share their local information with the group at each step
of the run, whereas the collection (viewu(r))u∈X corresponds to
the information that the group would have if the members shared
their views only after r has completed.

3. BACKGROUND

In this section we review some of the literature on information-
ﬂow security and formulate versions of existing deﬁnitions within
our system model. This review will motivate several of the ingre-
dients we use in the new deﬁnitions we propose later. This section
is largely a review of the existing literature, however, we do give
new characterizations of some existing deﬁnitions, using a notion
of relative information, that helps to clarify the relationship to the
new deﬁnitions we introduce later on.
3.1 Noninterference, policy H (cid:54)

(cid:55)→ L

Goguen and Meseguer’s formal semantics for noninterference
policies is restricted to transitive policies in deterministic machines.
They deﬁne for each domain u ∈ U the purge function purgeu :
A∗ → A∗ that maps a sequence of actions to the subsequence of
actions a with dom(a) (cid:55)→ u.

Deﬁnition 2. A machine M satisﬁes noninterference (NI) w.r.t.

(cid:55)→ if for all domains u and all runs r, r(cid:48) of M with purgeu(Act(r)) =
purgeu(Act(r(cid:48))), we have obsu(last(r)) = obsu(last(r(cid:48))).

It can be shown that, in deterministic machines, this is equivalent
to the following: for all runs r, r(cid:48) of M with purgeL(Act(r)) =
purgeL(Act(r(cid:48))), we have viewL(r) = viewL(r(cid:48)). This presen-
tation makes it clear that the deﬁnition says that the information in
L’s view in a run depends only on the L actions in the run, and are
independent of the H actions.

One of the main questions of study since the seminal work of
Goguen and Meseguer on deterministic systems is how their deﬁ-
nitions should be generalized to nondeterministic systems. A great
deal of the literature on this topic has been concerned with the sim-
ple two domain policy (which we write as H (cid:54)
(cid:55)→ L) given by the
relation {(L, L), (H, H), (L, H)} on the set U = {L, H}, comprised
of the low-level (public) domain L and the high-level (classiﬁed)
domain H. This is in part because even this simple policy presents
many subtleties in the setting of nondeterministic systems, but also
because there has been a view that it is possible to reduce arbitrary
policies to this special case.

Sutherland [22] proposed to interpret the policy H (cid:54)

Numerous deﬁnitions have been proposed that generalize NI for
the policy H (cid:54)
(cid:55)→ L to nondeterministic systems. We now discuss a
number of points from this literature that highlight issues that are
relevant to the novel deﬁnitions for the more general intransitive
policies that we introduce later.
3.2 Nondeducibility and Relative Information
(cid:55)→ L, as stat-
ing, informally, that L cannot deduce information about H, and gave
a general formal account of deducibility using a relation on func-
tions with domain the state space of the system. A generalization
of (non)deducibility will be useful for what follows, to make ex-
plicit a common logical structure that underlies the deﬁnitions we
propose. Deﬁnitions similar to the following have been considered
by Halpern and O’Neill [11] and More et al [17].

Deﬁnition 3. Let f, g and h be functions, each with domain the
set W of ‘worlds’. We say that in W , the function f contains
no more information than g about h, if for all w, w(cid:48) ∈ W such
that g(w) = g(w(cid:48)) there exists w(cid:48)(cid:48) such that h(w(cid:48)(cid:48)) = h(w(cid:48)) and
f (w(cid:48)(cid:48)) = f (w).

As an application of Deﬁnition 3, consider the following deﬁnition
of security:

Deﬁnition 4. A machine M for the policy H (cid:54)

(cid:55)→ L satisﬁes cor-
rectability (COR) if for all runs r and sequences α ∈ A∗ with
Act L(r) = Act L(α), there exists a run r(cid:48) with Act(r(cid:48)) = α and
viewL(r(cid:48)) = viewL(r).

We call this notion correctability in view of its similarity to a no-
tion of that name from [12]. It is easily seen that correctability may
be given a clean characterization using the above notion of rela-
tive information as follows: M satisﬁes correctability iff in R(M ),
the function viewL contains no more information about Act than
purgeL ◦ Act.
In a special case, we can furthermore formulate relative infor-
mation in a more symmetric way. Using a notation reminiscent
of probability theory, for functions f and g with domain W , and
value v in the range of f, deﬁne poss(g | f = v) to be the set
{ g(w) | w ∈ W ∧ f (w) = v }. Then we have the following re-
sult.

Proposition 1. Let f be a function with domain W , let g : W →
V be surjective and let h be a function with domain V (so that
h ◦ g also has domain W ). Then in W , the function f contains
no more information than h ◦ g about g iff for all v, v(cid:48) ∈ V with
h(v) = h(v(cid:48)) we have poss(f | g = v) = poss(f | g = v(cid:48)).
Since we work with input-enabled machines, the function Act :
R(M ) → A∗ is surjective. By Proposition 1 and the relative infor-
mation characterization of COR above, an equivalent formulation
of COR is that for all α, α(cid:48) ∈ A∗ with purgeL(α) = purgeL(α(cid:48))
we have poss(viewL | Act = α) = poss(viewL | Act = α(cid:48)). This
states COR in a form that clariﬁes its relationship to the view-based
formulation of NI, by showing that that COR is obtained by gener-
alizing the single-valued view function of a deterministic machine
used in NI by a set-valued view function in the nondeterministic
setting.
3.3 Observation-based deﬁnitions are too weak
(cid:55)→ L, the deﬁnition of noninterference states
that the observation of L in the ﬁnal state of a run r should depend

in non-deterministic systems

For the policy H (cid:54)

871only on purgeL(Act(r)). As noted above, in deterministic sys-
tems, this is equivalent to the statement that the history viewL(r)
of everything that is observable to L in the run should depend only
on purgeL(Act(r)). However, as the following example shows, a
similar equivalence between deﬁnitions stated in terms of observa-
tions in the ﬁnal state of a run and deﬁnitions stated in terms of the
view on the run does not hold in nondeterministic systems.

Example 1. Consider the security policy H (cid:54)

(cid:55)→ L and the non-
deterministic machine M depicted in Fig. 2, where states are la-
beled externally with L’s observation, A = {(cid:96), h}, dom((cid:96)) =
L, and dom(h) = H. Suppose we deﬁne an observation-based
version of correctability: M is obs-COR if in R(M ), the func-
tion obsL ◦ last contains no more information than purgeL ◦ Act
about Act. Equivalently, by Proposition 1, for all α, α(cid:48) ∈ A∗, if
purgeL(α) = purgeL(α(cid:48)) then poss(obsL ◦ last | Act = α) =
poss(obsL ◦ last | Act = α(cid:48)). It can be seen that M satisﬁes obs-
COR: the only H transition that can affect L observations is that
from s0, but if this is added or deleted from a run, there exists an-
other run with the same subsequent sequence of actions ending in
the same ﬁnal observation for L. However, the possible views may
differ, depending on whether h occurred: note that purgeL(h(cid:96)(cid:96)) =
(cid:96)(cid:96) = purgeL((cid:96)(cid:96)), but poss(viewL | Act = (cid:96)(cid:96)) = {0(cid:96)1(cid:96)2, 0(cid:96)2(cid:96)1}
whereas poss(viewL | Act = h(cid:96)(cid:96)) = {0(cid:96)1(cid:96)1, 0(cid:96)2(cid:96)2}. Thus, this
machine does not satisfy COR. Intuitively, it is insecure, since L
can determine from its view whether the initial h action occurred.

s0

0
(cid:96)

(cid:96)

(cid:96)

s01

1

(cid:96)

s02

2

h

(cid:96)

(cid:96)

s11

1

s1

0

(cid:96)

(cid:96)

s12

2

Figure 2: Deductions ought to be based on views rather than
observations

This example points to the fact that in deﬁning security in non-
deterministic systems, we need to take the evidence from which an
agent makes deductions to be its view, i.e., all that it could have
observed to the present moment, rather than just its current obser-
vation, as in the deﬁnition of noninterference and some of its later
generalizations (e.g., the deﬁnition of intransitive noninterference
in deterministic systems [10, 21]). This point has sometimes been
missed in the literature on nondeterministic systems, e.g., see the
discussion of the work of von Oheimb [26] in Section 8 below.
3.4 Persistence

Two different intuitive interpretations of the notion of noninter-
ference can be given: an epistemic interpretation which says that
L is not able to know, or deduce anything about H activity, and a
causal interpretation, which says that H actions may not have any
causal effect on L observations.
In deterministic systems, these
interpretations may coincide, but this is no longer the case in non-
deterministic systems, as the following example shows.

Example 2. Consider the machine depicted in Fig. 3 for the pol-
icy H (cid:54)
(cid:55)→ L. States are labeled externally with L’s observation,
except for states on which L observes ⊥, in which case we elide the
observation. As in the previous example, A = {(cid:96), h}, dom((cid:96)) = L,
and dom(h) = H. It can be seen that this machine satisﬁes the epis-
temic notion COR: domain L cannot make any deductions from its

s0

(cid:96)

(cid:96)

s2

s1

(cid:96)

(cid:96)

(cid:96)

h

s6

s5

s4

s3

1

0

1

s7

0

(cid:96)

Figure 3: A machine that is COR but not P-COR.

view about H actions, or how these are interleaved with its own.
(Recall that we elide self-loops.) However, it can reasonably be
argued that this machine is not secure on a causal interpretation of
security: note that by performing or not the action h from the state
s1, domain H is able to inﬂuence whether L subsequently observes
0 or 1.

Examples such as this can be addressed using the notion of per-
sistence, which has been factored into a number of deﬁnitions in
the literature [5, 6, 18].

Deﬁnition 5. For a security deﬁnition X, we say that machine
M = (S, s0, A,−→, obs, dom) persistently satisﬁes X (P-X) w.r.t.
a policy (cid:55)→ if the machine (S, s, A,−→, obs, dom) satisﬁes X w.r.t.
(cid:55)→, for all reachable states s of M.

Note that the machine in Example 2 is not P-COR.1
3.5 Collusion
For policies that generalize from the two-domain setting of the
policy H (cid:54)
(cid:55)→ L in nondeterministic systems, the issue of collusion
becomes of concern. As we illustrate in the present section, this is
so even for transitive security policies.

The simplest type of policy for which this point can be made is
the separability policy [20, 15] which says that no domain may in-
terfere with any other. That is, for set of domains U, separability is
the policy ∆U = { (u, u) | u ∈ U }. In the case that U consists of
two domains A and B, this seems to say that A may not interfere B,
and vice versa, so one apparently reasonable interpretation of the
policy is to apply a semantics for H (cid:54)
(cid:55)→ L for all domains. This idea
suggests the following deﬁnition, when we apply the correctability
semantics for H (cid:54)
(cid:55)→ L to each domain, and use our relative informa-
tion formulation.

Deﬁnition 6. A machine M for a set U of domains satisﬁes mu-
tual correctability (MCOR) w.r.t. (cid:55)→ if for all domains u ∈ U, in
R(M ), viewu contains no more information than purgeu ◦ Act
about Act.

In the case of the policy (cid:55)→ = ∆U, this says that no domain
is able to deduce from its view anything about what actions other
1We remark that in the case of COR, the notion P-COR is in
the spirit of the notion 0-forward correctability of Johnson and
Thayer [12], which says that an addition or deletion from a trace
of an H action requires only changes to subsequent H observations
to obtain another trace that looks the same to L. However, P-COR
is stronger, since it requires a correction from a given state, while
0-forward-correctability is a trace-based notion that allows the cor-
rection to pass through different states on the common preﬁx of
events.

872domains have performed, or how those actions were interleaved
with its own.

It turns out that, in some circumstances, this is an insufﬁcient
guarantee. Suppose that we have a system with three separated
domains, i.e., U = {H, L1, L2} and the policy is ∆U. However, L1
and L2 are corrupt, and collude by communicating via a channel
that lies outside the system. Under these circumstances, there is
nothing that can be done in practice to prevent information-ﬂow
between L1 and L2, even if the system is secure, so that it is not the
cause of the information-ﬂow. However, we expect that, since the
system enforces the policy, H’s information is still protected from
leakage to L1 and L2. In fact, mutual correctability is too weak to
provide such a guarantee, as is shown by the following example.

Example 3. Consider the machine depicted in Fig. 4 under the
policy ∆{H,L1,L2}. The domain H observes ⊥ at all states. The
two low domains L1 and L2 have observations in the set {⊥, 0, 1}.
These are indicated in Fig. 6 by labelling states to the right above
and below by the observations made by L1 and L2, respectively.
We omit the observation ⊥ to reduce clutter.
It can be veriﬁed
that machine M satisﬁes MCOR. However, H’s information is not
secure against collusion by the coalition L = {L1, L2}. We may
represent the information held by the coalition L in a run r by
viewL(r), using the set-based view deﬁnition introduced above.
Similarly, we may generalize the purge function to the coalition by
writing purgeL(α) for the subsequence of actions a in α ∈ A∗
with dom(a) ∈ L. Let α = (cid:96)1(cid:96)2 and β = h(cid:96)1(cid:96)2. Note that
purgeL(α) = α = purgeL(β). But
1}
poss(viewL | Act = α) = { ⊥
(cid:54)= { ⊥
0}
= poss(viewL | Act = β) .

0 , ⊥
1 , ⊥

0⊥ (cid:96)2
0⊥ (cid:96)2

1⊥ (cid:96)2
1⊥ (cid:96)2

⊥ (cid:96)1
⊥ (cid:96)1

⊥ (cid:96)1
⊥ (cid:96)1

0

0

1

1

Intuitively, although the policy suggests that the coalition should
not be able to distinguish between the sequences α and β, in fact
they can, using the parity of their ﬁnal observations.

There has been some informal recognition in the literature of the
relevance of coalitions when dealing with policies beyond H (cid:54)
(cid:55)→ L,
but there appear to have been very few formal studies of the issue.
(We defer discussion of the related literature to Section 8.) One of
our contributions in this paper is to pursue such a formal study in
the general setting of intransitive policies.
3.6 Intransitive Noninterference

L

H

D

is

The ﬁnal background we require concerns semantics for intran-
sitive policies: this issue has been studied primarily in the setting
of deterministic systems.

One of the main motivations for
intransitive policies
to repre-
sent the role of trusted components
within an architectural design of a
Figure 5: Policy HDL.
system. A canonical example of this
is a downgrader, a trusted component that manages declassiﬁca-
tion of high-level secrets to low-level domains. A policy for such
a system is the policy HDL = {H, D, L}2 \ {(H, L)} depicted in
Figure 5. Here D represents a trusted downgrader component.

Even in deterministic systems, NI is not an adequate deﬁnition
of security for such intransitive policies, since it implies that L can
learn nothing about H activity (not even when the downgrader per-
mits it). To give a more adequate semantics, Haigh and Young [10]
generalized the deﬁnition of the purge function to intransitive poli-
cies; we follow the formulation of Rushby [21]. Intuitively, the in-
transitive purge of a sequence of actions with respect to a domain u

s0

h

s

(cid:96)1

(cid:96)2

(cid:96)1

(cid:96)2

s1

(cid:96)2

s2

(cid:96)1

s3

s4

s5

(cid:96)2

s6

(cid:96)1

s7

s8

0

0

1

1

0

0

1

1

(cid:96)2

(cid:96)1

(cid:96)2

(cid:96)1

(cid:96)2

(cid:96)1

(cid:96)2

(cid:96)1

s9

s10

s11

s12

s13

s14

s15

s16

0

0

0

0

1

1

1

1

0

1

1

0

1

0

0

1

Figure 4: An insecure system that satisﬁes MCOR.

is the largest subsequence of actions that could form part of a causal
chain of effects (permitted by the policy) ending with an effect on
domain u. More formally, the deﬁnition makes use of a function
src : U× A∗ → P(U) deﬁned inductively by srcu() = {u} and
srcu(aα) =

srcu(α) ∪ { dom(a) | ∃v ∈ srcu(α) (dom(a) (cid:55)→ v) }

for a ∈ A and α ∈ A∗. Intuitively, srcu(α) is the set of domains
v such that there exists a sequence of permitted interferences from
v to u within α. The intransitive purge function ipu : A∗ → A∗
for each domain u ∈ U is then deﬁned inductively by ipu() = 
and, for a ∈ A and α ∈ A∗,

(cid:40)

ipu(aα) =

a · ipu(α)
ipu(α)

if dom(a) ∈ srcu(aα)
otherwise.

Haigh and Young’s deﬁnition of security uses the intransitive purge
function in place of the purge function in Goguen and Meseguer’s
deﬁnition. Using our relative information formulation, the follow-
ing is equivalent:

Deﬁnition 7. A deterministic machine M is IP-secure w.r.t. a
(possibly intransitive) policy (cid:55)→ if for all u ∈ U, in R(M ), the
function obsu ◦ last contains no more information than ipu ◦ Act
about Act.
As with NI, an equivalent statement is that for all u ∈ U, in R(M ),
the function viewu contains no more information than ipu ◦ Act

873about Act. It can be seen that ipu = purgeu when (cid:55)→ is transitive,
so IP-security is in fact a generalization of the deﬁnition of security
for transitive policies.

It has been argued by van der Meyden [24] that IP-security misses
some subtle ﬂows of information relating to the ordering of events.
A very simple example (from [25]) illustrating the problem is that
there exists an IP-secure system for the policy H (cid:55)→ D (cid:55)→ L
with actions h, d, (cid:96) in domains H, D, L respectively, in which L
makes different observation after the sequence of actions h(cid:96)d than
after the sequence of actions (cid:96)hd. Since ipL(h(cid:96)d) = h(cid:96)d and
ipL((cid:96)hd) = (cid:96)hd are also distinct, this is not a violation of IP-
security. However, it can be argued that such a system is not secure.
Intuitively, L learns whether or not the h action came before the (cid:96)
action. But, the system model is asynchronous, and according to
the policy, the only way L is permitted to learn about the h action
is via D. Since D is not permitted to know about the (cid:96) action, D
cannot securely inform L how action h was ordered with respect to
action (cid:96).

In response to this sort of example, van der Meyden proposes
an alternate semantics that is based on a function tau for each do-
main u, that is intended to capture the maximal information about
actions that domain u may have, according to the policy.
(The
name of the function and the associated notion of security abbrevi-
ate “transmission of information about actions”.) This function is
inductively deﬁned by

tau() =  and

(cid:40)

tau(αa) =

(tau(α), tadom(a)(α), a)
tau(α)

if dom(a) (cid:55)→ u
otherwise.

This deﬁnition resembles a full-information protocol, in which, when
performing an action a after sequence α, domain dom(a) sends ev-
erything that it knows (as represented by tadom(a)(α)), as well as
the fact that it has performed action a, to every domain u to which it
is permitted to transmit information. The recipient domain u adds
this new information to its existing information tau(α). Domains
to which dom(a) is not permitted to send information learn noth-
ing when a happens. This function forms the basis for a deﬁnition
of security which may be formulated using relative information as
follows:

Deﬁnition 8. A deterministic machine M is TA-secure w.r.t. a
policy (cid:55)→ if obsu◦last contains no more information than tau◦Act
about Act.
Again, it proves to be equivalent to say that viewu contains no more
information than tau◦Act about Act. This deﬁnition is equivalent
to NI in the case of transitive policies.

4. MAIN DEFINITIONS

We are now positioned to state our new deﬁnitions, which give
meaning to intransitive noninterference policies in nondeterminis-
tic machines. As noted above, our focus in this paper is with deﬁ-
nitions that place constraints on the ﬂow of information concerning
the actions that have been performed. We furthermore focus on
generalizing the notion of TA-security from van der Meyden [24].
We characterized TA-security in deterministic machines as stat-
ing that for each domain u, the function obsu ◦ last contains no
more information than tau ◦ Act about Act, and noted that it is
equivalent to take viewu in place of obsu ◦ last in this deﬁnition.
However, in nondeterministic systems, an observation-based deﬁ-
nition of security may be too weak, as shown in Section 3.3. This
suggests the following as an appropriate generalization of the deﬁ-
nition to nondeterministic systems.

Deﬁnition 9. A nondeterministic machine M is nTA-secure w.r.t.
a policy (cid:55)→ if, for all u ∈ U, the function viewu contains no more
information than tau ◦ Act about Act.

Intuitively, as in the deterministic case, this deﬁnition places, in
each run r, an upper bound on the information about the action
sequence Act(r) that is permitted to be contained in each possible
view viewu(r): it may be no more than the information contained
in tau(Act(r)). By Proposition 1, an equivalent statement is that
for all α, β in A∗, if tau(α) = tau(β) then poss(viewu | Act =
α) = poss(viewu | Act = β).

As we noted above in Section 3.5, a nondeterministic system
may be secure with respect to a deﬁnition of security that constrains
ﬂow of information to individual domains, while allowing ﬂows
of information to groups of domains.
If the system needs to be
protected against collusion, this means that we require a stronger
deﬁnition that takes into account the deductive capability of groups.
One way to approach such a deﬁnition is to focus on what the group
would know if agents in the group were, after the completion of a
run of the system, to share what they have observed. We call this
a post-hoc coalition. The state of information of such a coalition
X ⊆ U may be represented by the function (cid:104)viewu(cid:105)u∈X. We
would like them to be able to deduce no more than they would be
able to deduce if, after the run, they were to share their maximal
permitted information, represented by the terms tau(Act(r)) for
u in the group. This leads to the following deﬁnition.

Deﬁnition 10. A nondeterministic machine M is nTA-secure w.r.t.
post-hoc coalitions (PCnTA) for a policy (cid:55)→ if, for all nonempty
sets X ⊆ U, (cid:104)viewu(cid:105)u∈X contains no more information than
(cid:104)tau(cid:105)u∈X ◦ Act about Act.
By Proposition 1, an equivalent statement is that for all X ⊆ U and
α, β in A∗, if ∀u ∈ X (tau(α) = tau(β)) then

poss((cid:104)viewu(cid:105)u∈X | Act = α)

= poss((cid:104)viewu(cid:105)u∈X | Act = β) .

One situation in which this attack model is appropriate is a sys-
tem in which multiple agents submit computations to a cloud server,
and receive output only after the computation is complete.
In a
more interactive setting, the colluding coalition X may be able to
do more than share information at the end of the run: they may
be able to share information at each step of the run. We may call
this a runtime coalition. Since our systems model is asynchronous,
agents in X are typically not able to deduce a linear ordering on
the actions performed by members of X from the set of views
viewu(r) for u ∈ X. However, if they are able to communicate
each time that an action is performed by one of them, then at the
end of the run their information is the joint view viewX (r), which
does contain the order of actions in domains X. This means that
reasoning based on viewX represents a stronger attack model. The
following deﬁnition attempts to state that the system is resilient to
this stronger type of attack.

In order to formulate it, we need to characterize the maximal
permitted information for a group X, if they are colluding by shar-
ing information at each step. In the previous deﬁnition, the group’s
maximal permitted information in a run r is represented by the col-
lection of terms tau(Act(r)) for u ∈ X. In general, this will be
too weak to allow deduction of the order of the actions performed
in domains X, whereas as we just noted, the group can deduce
this from its joint view viewX (r). This suggests that we need a
stronger deﬁnition of the maximal permitted information on this
attack model. We take the approach here that if the group cannot

874be prevented from sharing information through channels lying out-
side the system, its maximal permitted information in the event of
such collusion is represented by sharing of the maximal permitted
information at each step of the computation. This leads us to gener-
alize the deﬁnition of ta to take as a parameter, in place of a single
agent u, a nonempty set X ⊆ U. Inductively, we deﬁne taX by
taX () =  and, for α ∈ A∗ and a ∈ A,

taX (αa) =

(taX (α), tadom(a)(α), a)
taX (α)

if X ∩ dom(a)
otherwise

(cid:55)→ (cid:54)= ∅

(cid:40)

It is easily seen that ta{u}(α) = tau(α), so this is in fact a gen-
eralization of the previous notion. Using this notion, we obtain the
following deﬁnition.

Deﬁnition 11. A nondeterministic machine M is nTA-secure
w.r.t. runtime coalitions (RCnTA) for a policy (cid:55)→ if for all nonempty
sets X ⊆ U, viewX contains no more information than taX ◦ Act
about Act.
By Proposition 1, an equivalent statement is that for all X ⊆ U and
α, β in A∗, if taX (α) = taX (β) then poss(viewX | Act = α) =
poss(viewX | Act = β).

We have the following straightforward implications between these

notions:

Proposition 2. RCnTA implies nTA; PCnTA implies nTA.

These containment relationships are strict, as is shown by the fol-
lowing two examples.

(s)

Example 4. For a machine that is RCnTA but not PCnTA, con-
sider M as given in Fig. 6 under the security policy ∆{H1,H2}. The
two domains have observations in the set {⊥, 0, 1}. Each domain
Hi has one action, hi. Below each state s of M there is a label
of the form obsH1
(s) to indicate the observations made by the two
obsH2
domains in s.

Machine M is not only nTA but also RCnTA for ∆{H1,H2}.
For instance, for α = h1h2, β = h2h1, and the coalition H =
{H1, H2} we have that taH(α) = (taH(h1), taH2 (h1), h2) =
((, , h1), , h2) (cid:54)= ((, , h2), , h1) = taH(β) and thus it is con-
sidered secure that

poss(viewH | Act = α)
0⊥ h2
⊥
0 h1

= { ⊥
(cid:54)= { ⊥

⊥ h1
⊥ h2

0

0 , ⊥
0 , ⊥

⊥ h1
⊥ h2

0

1

1

0 , ⊥
1 , ⊥

⊥ h1
⊥ h2

1⊥ h2
⊥
1 h1
= poss(viewH | Act = β) .

1⊥ h2
⊥
1 h1

1}
1}

1

0

On the other hand, we have that M is not PCnTA. The individual
h1−→
taHi (α) = taHi (hi) = taHi (β), for i = 1, 2. But r = s0
h2−→ s6 ∈ R(α) gives the pair (cid:104)viewH1 (r), viewH2 (r)(cid:105) =
s3
(cid:104)⊥h11,⊥h20(cid:105) of views. There is no run in R(β) with this pair of
views.

We next show that PCnTA does not imply RCnTA.

Example 5. Consider the security policy HNLL consisting of a
High domain H and two Low domains L1 and L2 who are all al-
lowed to interfere with each other, except that H is not permitted
to interfere with any Low domain. (See Fig. 7b.) Let M be the
machine depicted in Fig. 7a where the only actions are (cid:96)2 in do-
main L2 and h in domain H. Only L1 has non-trivial observations,
in {0, 1}, as indicated by the external labels below states. Machine
M is not only nTA but also PCnTA for HNLL. That it is not RC-
nTA for HNLL can be seen by comparing α = (cid:96)2h and β = (cid:96)2

s1

0⊥
h2

h2

s2

⊥
0
h1

h1

s5

0
0

s0

⊥
⊥

h2

s6

1
0

h1

s3

1⊥
h2

h2

s7

1
1

s4

⊥
1
h1

h1

s8

0
1

Figure 6: A system that is RCnTA but not PCnTA.

h

h

s3

0

s4

1

(cid:96)2

(cid:96)2

s0

0

s1

0

s2

1

L1

H

L2

(a) A system that is PCnTA
but not RCnTA for HNLL.

(b) Policy HNLL

Figure 7: Example 5.

for the coalition X = {L1, L2}. We have taX (α) = taX (β), by
deﬁnition, but

poss(viewX | Act = α)
= { 0⊥ (cid:96)2
(cid:54)= { 0⊥ (cid:96)2

0⊥ 1⊥}

0⊥ , 0⊥ (cid:96)2
0⊥ , 0⊥ (cid:96)2

1⊥ , 0⊥ (cid:96)2
1⊥}
= poss(viewX | Act = β) .

Note that this problem does not show up in the views considered
separately:

poss((cid:104)viewu(cid:105)u∈X | Act = α)

= {(cid:104)0,⊥(cid:96)2⊥(cid:105),(cid:104)01,⊥(cid:96)2⊥(cid:105)}
= poss((cid:104)viewu(cid:105)u∈X | Act = β)

Checking the other cases, it can be veriﬁed that machine M is PC-
nTA for HNLL.
Evidently, like the machine of Example 2, the machine of Exam-
ple 5 should not be considered to be secure on a causal interpreta-
tion of security, since at state s1, the occurrence of the H action h
causes a change to the L1 observation, whereas H is not supposed
to interfere with L1.

To obtain notions of security that avoid this difﬁculty, we can
apply the persistence construct of Deﬁnition 5. This yields sev-
eral further notions of security: P-nTA, P-PCnTA and P-RCnTA.
Plainly the latter two imply the former, and each P-X implies the
source notion X from which it is derived. We now present some
examples that show that these notions are all distinct.

875The ﬁrst such example demonstrates that PCnTA and RCnTA but

neither of their persistent variants.

Example 6. Reconsider the machine of Example 5, but with an
additional edge H (cid:55)→ L2 in the security policy. That is, we have
an instance of the well-known downgrader policy where D hap-
pens to be called L2. Making the policy more liberal plainly cannot
turn a secure system to an insecure system (for any of our deﬁni-
tions) so M is still PCnTA. Moreover, now the counter-example to
this from Example 5 no longer works because now we have that
taX (α) = (taX ((cid:96)2), taH((cid:96)2), h) = ((, , (cid:96)2), (, , (cid:96)2), h) dif-
fers from taX (β) = (, , (cid:96)2).
It can be veriﬁed that M now
satisﬁes RCnTA. However, it is immediate that this example does
not satisfy any of the persistent notions of security (in particular, it
does not satisfy the weakest of these, P-nTA) because the H action
h changes the L1 observation from state s1.

The next examples shows that P-nTA does not imply P-RCnTA,
and at the same time that P-PCnTA also does not imply P-RCnTA.

Example 7. Recall the policy HNLL (see Fig. 7b) and let M
be as depicted in Fig. 8. Only L1 has non-trivial observations in
{0, 1}, indicated by labels below states. The other domains observe
⊥ at all states. Actions (cid:96)2 and h belong to domains L2 and H,
respectively. Machine M is P-nTA for HNLL, and also P-PCnTA.

(cid:96)2

(cid:96)2

s3

0

s4

1

(cid:96)2

s0

0

h

(cid:96)2

s1

0

s2

1

Figure 8: A system that is P-nTA and P-PCnTA but not P-
RCnTA.

That it is not P-RCnTA can be seen by comparing α = h(cid:96)2(cid:96)2
and β = (cid:96)2(cid:96)2 for the coalition X = {L1, L2}. We have taX (α) =
((, , (cid:96)2), (, , (cid:96)2), (cid:96)2) = taX (β) but

poss(viewX | Act = α)

= { 0⊥ (cid:96)2
(cid:54)= { 0⊥ (cid:96)2

0⊥ (cid:96)2
0⊥ (cid:96)2

0⊥ , 0⊥ (cid:96)2
0⊥ , 0⊥ (cid:96)2

1⊥}
1⊥ , 0⊥ (cid:96)2

1⊥ (cid:96)2
0⊥ (cid:96)2
= poss(viewX | Act = β) .

1⊥}

1⊥ (cid:96)2

This problem vanishes when considering the views separately: both
poss((cid:104)viewu(cid:105)u∈X | Act = α) and poss((cid:104)viewu(cid:105)u∈X | Act = β)
are equal to {(cid:104)0,⊥(cid:96)2⊥(cid:96)2⊥(cid:105),(cid:104)01,⊥(cid:96)2⊥(cid:96)2⊥}.

Finally, P-RCnTA implies neither PC-PCnTA nor PCnTA. For
this, note that the machine in Example 4 (Fig. 6) is P-RCnTA. We
have already argued that it is RCnTA, so the veriﬁcation of this
claim amounts to checking RCnTA on each of the very simple ma-
chines that result from choosing any state other than s0 as the ini-
tial state. This is easily done by inspection. We have already noted
above that this machine is not PCnTA, so, a fortiori, it is also not
PC-PCnTA.

5. SPECIAL CASES

In this section we reconsider some simple special cases of poli-
cies and systems and show that our new deﬁnitions of security re-
duce to some of the existing notions of security discussed above.
This lends credibility to the deﬁnitions and helps clarify their posi-
tioning with respect to the existing literature.

We have started with the aim of generalizing the notion of TA-
security on deterministic systems, and introduced a variety of dis-
tinct deﬁnitions in the more general setting of nondeterministic sys-
tems. The following result shows that all of these notions collapse
to TA-security in the deterministic setting.

Proposition 3. Given a security policy, the classes of determin-
istic machines that are nTA-, PCnTA-, or RCnTA-secure are all
equal to the class of TA-secure machines. Moreover, the persis-
tent versions of these notions in deterministic machines are also all
identical to TA-security.

We have built our deﬁnitions of security for intransitive poli-
cies using ingredients from prior work on nondeterministic systems
with respect to the simple policy H (cid:54)
(cid:55)→ L. The following result
characterizes our deﬁnitions in the special case of this policy (in
nondeterministic systems) as collapsing to variants of the notion of
Correctability.

Proposition 4. For the policy H (cid:54)

(cid:55)→ L, we have that nTA = PC-
nTA = RCnTA = COR and P-nTA = P-PCnTA = P-RCnTA = P-
COR.

We showed in Example 2 that COR and its persistent version P-
COR are distinct. Thus, we have a collapse to two distinct notions
in the case of this policy.

6. UNWINDING

Unwinding is a useful technique for security proofs, which re-
duces the veriﬁcation of security to checking the existence of bi-
nary relations on states. In deterministic systems, unwinding is a
sound and complete technique for the proof of NI, and the exis-
tence of a weak form of unwinding relations is a sound technique
for IP-security and TA-security [8, 21, 24]. We deﬁne a generaliza-
tion of this method that is sound for our deﬁnitions of security in
nondeterministic systems.

equivalence relations on S. For X ⊆ U deﬁne ∼X =(cid:84)

Deﬁnition 12. Let M = (S, s0, A,−→, obs, dom) be a machine
for a security policy (U,(cid:55)→); let S = (∼u)u∈U be a family of
u∈X ∼u.
We say that S is a generalized weak unwinding on M with re-
spect to (cid:55)→ (or GWU) if the following conditions are met for all
s, s(cid:48), t ∈ S, u ∈ U, nonempty X ⊆ U, and a ∈ A:

s ∼u t ⇒ obsu(s) = obsu(t)

s a−→ t ⇒ s ∼dom(a)(cid:54)
(cid:48) ⇒ ∃t

(cid:55)→ t
t a−→ t

(cid:48)(cid:16)

OC
LR

(cid:48)(cid:17)

(cid:48) ∧ s

(cid:48) ∼X t

s ∼X t ∧ s ∼dom(a) t ∧ s a−→ s

GWSC

Condition OC (for “output consistency”) implies that coalition X
can distinguish states that some member can tell apart based on
its observation. Condition LR (for “local respect”) implies that
actions that are not allowed to interfere with anyone in the coali-
tion cannot change a state to another state that is distinguishable
to the coalition. According to GWSC (“generalized weak step
consistency”), if two states s and t can be distinguished neither by

876P-PCnTA

PCnTA

GWU

P-nTA

nTA

P-RCnTA

RCnTA

where N is a set, n : A × S → P(N ) and r : A × S × N → S,
such that the following properties hold:
∀s ∈ S, a ∈ A, c ∈ n(a, s)

s a−→ r(a, s, c)

(cid:16)

(cid:17)

CR1

(cid:16)

(cid:17)

Figure 9: Implications between notions deﬁned in this paper.

∀s, t ∈ S, a ∈ A

s a−→ t ⇒ ∃c ∈ n(a, s) (t = r(a, s, c))

coalition X nor by the domain of action a, then each state reach-
able from s by performing a is indistinguishable to X from some
state reachable via a from t. This condition is the main point of dif-
ference with the notion of weak unwinding from [21]: it adds to the
condition WSC the universal quantiﬁcation over X (which is just
a single domain u in WSC) and the existential quantiﬁcation over
t(cid:48) (this corresponds to the nondeterminism of the system, whereas
WSC is designed for deterministic systems.)

THEOREM 1. If there is a GWU on M w.r.t. (cid:55)→, then M is PC-

nTA and RCnTA w.r.t. (cid:55)→.
We may note that the deﬁnition of GWU is insensitive to the initial
state of the machine. Thus, if the existence of a GWU is sound
evidence for X-security, then it is also sound evidence for P-X-
security. Thus, we also have the following.

Corollary 1. If there is a GWU on M w.r.t. (cid:55)→, then M is both

P-PCnTA-secure and P-RCnTA-secure w.r.t. (cid:55)→.
Thus, by the corollary above and our previous results, the existence
of a GWU implies every single one of the security notions deﬁned
in this paper.

Fig. 9 summarizes the relationships we have found to hold be-

tween the various security deﬁnitions discussed above.

7. ACCESS CONTROL SYSTEMS

In the preceding sections, we developed deﬁnitions of security,
and showed that the notion of generalized weak unwinding pro-
vides a sound proof technique for showing that these deﬁnitions
hold in a system. Neither the deﬁnitions nor this proof technique
provide much concrete guidance for the engineer seeking to con-
struct a secure system, however. A result by Rushby [21] (and
reﬁned in [24]) shows that, in deterministic systems, intransitive
noninterference is guaranteed to hold in a system built from a col-
lection of objects subject to an access control discipline that satis-
ﬁes a simple static check that is essentially Bell and La Padula’s [2]
information ﬂow condition. In the present section, we show that it
is possible to generalize this result to nondeterministic systems.

We ﬁrst recall the notion of a machine with structured state [21].
This is a machine M (with states S) together with (we rename
some of the components) a set Obj of objects, a set V of values,
and functions contents : S × Obj → V , with contents(s, n)
interpreted as the value of object n in state s, observe, alter :
U → P(Obj), with observe(u) and alter(u) interpreted as the
set of objects that domain u can observe and alter, respectively.
For brevity, we write s(x) for contents(s, x). We call the pair
(observe, alter) the access control table of the machine.

Rushby introduced reference monitor conditions on such ma-
chines in order to capture formally the intuitions associated with
the pair (observe, alter) being an access control table that re-
stricts the ability of the actions to “read” and “write” the objects
Obj. In order to formulate a generalization of these conditions in
nondeterministic systems, we ﬁrst need to introduce some further
structure that constrains the nondeterminism in the system.

Deﬁne a choose-resolve characterization of nondeterminism in
a machine M with states S and actions A to be a tuple (N, n, r),

CR2
Intuitively, N represents the set of all possible nondeterministic
choices that can be made in the process of executing an action, n
restricts those choices as a function of the particular state and ac-
tion being executed, and r deterministically resolves the transition,
as a function of the state, action and nondeterministic choice made
in that context. Condition CR1 says that the resolution of every
possible nondeterministic choice, allowed in the context of a given
state and action, is in fact a state to which a transition is possible.
Conversely, CR2 says that every transition can be obtained by re-
solving some allowed nondeterministic choice.

Trivially, every nondeterministic machine has a choose-resolve
characterization of nondeterminism. (For, we can simply take N =
S, and deﬁne n(a, s) as { t| s a−→ t}, and r(a, s, t) = t, and CR1
and CR2 are then satisﬁed.) The following restrictions on the struc-
ture of the characterization make this notion more interesting.

Suppose the machine M has structured state, with objects Obj
and access control table (observe, alter). Write Val(x) for the
set of all possible values of x ∈ Obj. Say that the choose-resolve
characterization of nondeterminism (N, n, r) has local choices if
for each x ∈ Obj there exists a set Nx and a function nx : A ×
Val(x) → P(Nx) such that N = Πx∈ObjNx, and n(a, s) is the
set of c ∈ N such that for all x ∈ Obj, we have cx ∈ nx(a, s(x)).
That is, a choice of nondeterminism on performing action a in state
s is obtained by independently making a choice of nondetermin-
ism at each of the objects x ∈ Obj. Say that a machine has local
nondeterminism if it is a machine with structured state that has a
choose-resolve characterization with local choices.
u t to hold if s(x) =
t(x) for all x ∈ observe(u).
Intuitively, s ∼oc
u t says that u
could not distinguish s from t if all it knew were the content of
objects that it is permitted to observe. Similarly, for choices c, c(cid:48) ∈
Πx∈ObjNx, and u ∈ U we write c ∼oc
x for all x ∈
observe(u). The following conditions generalize the reference
monitor conditions to machines with local nondeterminism.

For states s, t and u ∈ U deﬁne s ∼oc

u c(cid:48) if cx = c(cid:48)

u t ⇒ obsu(s) = obsu(t)

s ∼oc

 c ∈ n(a, s) ∧ c(cid:48) ∈ n(a, t) ∧

s ∼oc
s(x) = t(x) ∧ cx = c(cid:48)

dom(a) t ∧ c ∼oc

dom(a) c(cid:48) ∧

x

 ⇒ r(a, s, c)(x) = r(a, t, c

LC-RM1

(cid:48)

)(x)

s a−→ t ∧ x (cid:54)∈ alter(dom(a)) ⇒ t(x) = s(x)

LC-RM2
LC-RM3
Intuitively, LC-RM1 says that the observations of domain u de-
pend only on the values of objects u is permitted to observe. Con-
dition LC-RM2 says that the value of object x after performing
action a depends only on the previous values of objects observable
to dom(a), nondeterministic choices made locally at those objects,
the previous value of x, and the nondeterministic choice made at
x. Condition LC-RM3 says that, for an action a, if dom(a) is not
permitted to alter the value of object x, then the value of x is in fact
unaffected when a is performed.

We also have a condition that relates the policy to the access
control structure, essentially that identiﬁed by Bell and La Padula
[2].

alter(u) ∩ observe(v) (cid:54)= ∅ ⇒ u (cid:55)→ v

AOI

877Intuitively, if there is an object x that can be altered by u and ob-
served by v, then x provides a channel through which information
can ﬂow from u to v. Thus, for the system to be secure, this ﬂow
must be permitted by the policy. The following result shows that
access control provides a design discipline that sufﬁces to enforce
our deﬁnitions of security.

THEOREM 2. If M is a machine with local nondeterminism
satisfying LC-RM1-LC-RM3 and AOI w.r.t. (cid:55)→ then there exists
a GWU on M w.r.t. (cid:55)→.

It follows that such a machine M satisﬁes all the security prop-
erties identiﬁed in this paper. The following example illustrates the
two theorems.

Example 8. Consider a system comprised of three domains, for
a sender S, a receiver R, communicating through an unreliable chan-
nel B. We have four objects Obj = {xS, xI, xO, xR}, representing
the state of the sender memory, buffer input queue, buffer output
queue, and receiver memory, respectively. That the channel is mod-
eled by two queues is motivated by the desire to separate the access
rights to the head of the queue from those for the tail. The function-
ality of the channel is thus limited to the transportation within the
buffer, shipping messages from the input queue to the output queue.
Letting Msg be a set of messages, we may suppose that Val(xS) =
Val(xR) = Msg, so that the memory of the sender or receiver
∗,
consists of a single message, and Val(xI) = Val(xO) = Msg
so that the states of the buffer queues are sequences of messages.
A state of the system as a whole may be represented as a tuple
(mS, w, v, mR) ∈ Msg × Msg
∗ × Msg. We suppose that the
sender has actions set(m) (setting its memory to message m) and
put (enqueue the current memory value to the buffer input queue),
the buffer has an action trans (dequeue a message from the input
queue and enqueue that message on the output queue), and the re-
ceiver has an action get (dequeue a message from the buffer output
queue and store it in the receiver’s memory).

∗ × Msg

We suppose that only the channel is unreliable. It may drop mes-
sages in transit from the head (modelled as xI) to the tail (xO). To
that end, we take NxO = {ok, drop} consisting of two choices
representing normal performance of the transmission action and
dropping off the associated message. This is the only nondeter-
mism we need, so we set NxI = NxS = NxR = {0}. Further-
more, we make all the nx(a, s) = {0} except for nxO(trans, s) =
{ok, drop}. The only interesting case of the resolve function is
r(trans, (mS, w · m, v, mR), (0, 0, f, 0)), which is deﬁned as the
value (mS, w, m · v, mR) if f = ok, and as (mS, w, v, mR) if
f = drop.

We deﬁne observations of the domains by obsS((mS, w, v, mR)) =

mS, obsB((mS, w, v, mR)) = (w, v), and obsR((mS, w, v, mR)) =
(v, mR). (We omit the discussion of minor issues such as no effects
will take place if R performs get in case the output queue is empty.)
Let the access control structure for the system be given by the

following table

xS

observe alter
xS, xI
xI, xO
xO, xR

xI, xO
xO, xR

S
B
R

R), from which it follows that obsR((mS, w, v, mR)) = (v, mR)

which is derived from the descriptions of the actions. Then we
have (mS, w, v, mR) ∼oc
R) just when (v, mR) =
(v(cid:48), m(cid:48)
= (v(cid:48), m(cid:48)
R) = obsR((m(cid:48)
R)), conﬁrming part of LC-
RM1. The reader may verify that the remainder of LC-RM1 holds
in this system, as do conditions LC-RM2 and LC-RM3. It is clear

(m(cid:48)
S, w(cid:48), v(cid:48), m(cid:48)

S, w(cid:48), v(cid:48), m(cid:48)

R

from the access control structure that the system also satisﬁes con-
dition AOI for the policy (cid:55)→ deﬁned by S (cid:55)→ B ↔ R.
P-PCnTA-secure w.r.t. (cid:55)→.

Thus, by Theorems 2 and 1 the system is P-RCnTA-secure and

We now give a sketch of how our results might apply to the ex-

ample of the introduction (Figure 1).

Example 9. Within each machine Mi, access control (monitored,
e.g., by a security kernel and by hardware access control features)
can be used to enforce the local portion of the policy. At this lower,
access control level, the system might contain objects chanu,v rep-
resenting communication channels for each of the pairs (u, v) in
{(Li, Hi), (Hi, Ei), (Ei, NIi), (Li, NIi), (NIi, N)}.

The details of the channels might be reliable or resemble the
lossy buffer of Example 8. (For example, one expects that E1 and
L1 will place their output in a buffer read by the network interface
NI1, and that domain N consists of a network of lossy buffers.)
Some of the operations by a domain u on its data components Du
may also involve nondeterminism, e.g., E1 may draw on a local
source of nondeterminism to generate random nonces to be used as
padding in the encrypted messages it constructs.

Suitable access control settings on such nondeterministic com-
ponents that reside within each machine Mi sufﬁce to enforce the
policy.
In particular, consider the access control setting for M1
depicted in Figure 10.

DH1

DE1

H1

chanH1,E1

E1

1
H
,
1
L
n
a
h
c

1
I
N
,
1
E
n
a
h
c

L1

chanL1,NI1

NI1

chanNI1,N

N

DL1

DNI1

Figure 10: Access Control within machine M1. A bidirectional
arrow u ↔ d between a domain u and a data object d denotes
d ∈ observe(u) ∩ alter(u) whereas a unidirectional arrow
u → d denotes d ∈ alter(u).

Note that all edges indicated are bidirectional, except the edge
from domain L1 to object chanL1,H1. This ensures that L1 may
pass information to H1, but not vice versa. It is straighforward to
check that this access control policy ensures satisfaction of AOI
with respect to the policy in Figure 1.

8. RELATED WORK

The original work on semantics of intransitive noninterference [10,

21] was conﬁned to transitive policies and deterministic systems.
We have already noted, in Section 3, some of the main points of
connection between our contributions in this paper and the volumi-
nous literature on the transitive policy H (cid:54)
(cid:55)→ L in nondeterministic

878systems. We focus here on comparing our work to that of some oth-
ers who have considered semantics for intransitive policies speciﬁ-
cally in the setting of nondeterministic systems.

The earliest work in this vein appears to be that of Bevier and
Young [3]. Unlike most of the subsequent work, Bevier and Young
are alert to the issue of collusion: they present an example to jus-
tify a component of their deﬁnition as necessary in order to deal
with inferences by groups of agents. Moreover, their deﬁnition im-
plies a type of persistence, in that it places requirements on future
views from all pairs of states related by an equivalence relation
(hence also from all individual states compared to themselves.)
The equivalence relations used are the concrete relations deﬁned
by s ∼X t iff obsu(s) = obsu(t) for all u ∈ X, where X is a set
of domains. Additionally, their deﬁnition refers to versions of the
intransitive purge functions ipu and the equivalence relation and
viewu(r) = viewu(r(cid:48)) that generalize the single domain u to a
list of domains. Unfortunately, a deﬁnition of neither generaliza-
tion is provided. Our distinction between post-hoc and run-time
coalitions shows that there are in fact several plausible candidates
for such a generalization — it is unclear which approach was in-
tended by Bevier and Young. They also give a corresponding def-
inition of unwinding, which is also based on the speciﬁc concrete
equivalence relation of observation identity (although they discuss
the fact that the analyst might tailor the deﬁnition of observation).
Thus, although this work is close ours in the mix of concerns fac-
tored into the deﬁnition of security, it differs in the use of a speciﬁc
concrete equivalence relation and its basis in ip rather than ta, and
failed to notice subtleties concerning the treatment of coalitions.

Roscoe and Goldsmith [19] considered the application of the no-
tion of local determinism within the setting of the process algebra
CSP to give semantics to intransitive policies: they give two ver-
sions, one using a lazy abstraction operator, the other using a mixed
operator that gives special treatment to signal events. It is shown
in [23] that, in the special case of deterministic systems, the lazy
abstraction approach corresponds to the application of the purge
function of NI to nondeterministic systems, and the mixed abstrac-
tion approach corresponds to a notion of security van der Meyden
calls ITO-security: this differs from TA security in that it uses func-
tions that are like tau, but which track information about observa-
tions as well as information about actions. Thus, both deﬁnitions
differ from the TA-based deﬁnitions we have presented. The issue
of collusion is not considered in [19], however, semantics of a pol-
icy is given by checking for ﬂows of information to each individual
domain u from the noninterfering domains (cid:54)
(cid:55)→u, considered jointly.
Mantel [13] enriches the type of security policies we have con-
sidered (by distinguishing between observable, deducible and non-
interfering events) and studies the extension of his (trace-based)
Basic Security Property compositional approach to deﬁnitions of
security for H (cid:54)
(cid:55)→ L to this richer policy setting. His deﬁnitions
make use of a restricted quantiﬁcation over sets of domains. How-
ever, the intended effect of this quantiﬁcation is not to capture col-
lusion attacks, but to spread the local purge-like effect of the basic
security properties to simulate ip-like noninterference conditions.
He also proves soundness of an unwinding proof technique.

Another deﬁnition of intransitive noninterference in nondeter-
ministic systems is given by von Oheimb [26]. His approach is
also based on the ip function, but relates this to ﬁnal observations
in a run. As we noted in Section 3.3, this may be inappropriate in
nondeterministic systems. A notion of unwinding is also presented
that is similar to ours, in that it quantiﬁes over sets of domains.
However, the motivation for this appears to have been to obtain a
soundness result for unwinding, rather than an overt recognition

of the possibility of collusion attacks, since these are not acknowl-
edged in the deﬁnition of security.

Bismulation-based deﬁnitions of security (an approach that is
closely related to unwinding) are considered in the context of the
very speciﬁc intransitive policy HDL in [5]. This work deals with
the persistence dimension, but it is not clear how to generalize the
deﬁnitions to policies richer than HDL. Gorrieri and Vernali [9]
deﬁne similar notions of noninterference for the H (cid:54)
(cid:55)→ L and HDL
policies on labelled transition systems and elementary Petri nets:
the emphasis here is on the particularities of these semantic mod-
els rather than essential novelty in the semantics of security, where
they essentially follow [5].

Finally, Backes and Pﬁtzman [1] go beyond the nondeterministic
setting to consider intransitive policies in systems with probabilistic
transitions and cryptographic notions of information-ﬂow. They
give a number of deﬁnitions that take a rather different approach
from the rest of the literature, focussing on whether information
about an initial bit can be transmitted from one party to another,
in some sense "only via particular intermediaries". It would take
some research to clarify relationships to any of the deﬁnitions in
the literature, but, prima facie, it would seem that these deﬁnitions
do not constrain information ﬂow after the ﬁrst transmission by the
intermediaries, as do all the other deﬁnitions.

Of the works discussed above, only von Oheimb applies his re-
sults to a class of Access Control systems, but these are just Rushby’s
deterministic Access Control systems, whereas we have introduced
a more general non-deterministic class of such systems.
9. CONCLUSION

Our focus in this paper has been to present a set of general-
izations of the TA-security semantics for intransitive noninterfer-
ence in nondeterministic systems. We have teased out a number
of parameters of such generalizations, leading to a range of deﬁni-
tions. We characterize the relationships between these deﬁnitions
and provide an unwinding proof technique that is sound for all.

Several issues are left open by this work. The TA style of seman-
tics concentrates of the possibility for attackers to make deductions
about the actions that have been performed.
In a nondetermin-
istic setting, the observations nondeterministically resulting from
these actions may also need to be protected against inference at-
tacks (e.g., when these observations are randomly generated keys).
Some TA-like deﬁnitions (TO-security and ITO-security) that take
observations into account have been presented by van der Mey-
den [24]. It remains to work out how these deﬁnitions should be
generalized to the nondeterministic setting.

Another concern is that whereas unwinding proof techniques
should preferably be complete as well as sound (and some in the
literature achieve both) our technique is just sound. It would be of
interest to have sound and complete proof techniques for each of
our deﬁnitions, in order to cover examples that lie outside of their
intersection. Given the connection we establish between P-nTA
and 0-forward correctability, ideas from Millen’s sound and com-
plete unwinding for forward correctability [16] may be appropriate
in this case.
10. REFERENCES
[1] M. Backes and B. Pﬁtzmann. Intransitive non-interference

for cryptographic purposes. In Proc. IEEE Symp. on Security
and Privacy, pages 140–152, 2003.

[2] D. Bell and L. L. Padula. Secure computer system: uniﬁed

exposition and multics interpretation. Technical Report
ESD-TR-75-306, Mitre Corporation, Bedford, M.A., Mar.
1976.

879[3] W. Bevier and W. Young. A state-based approach to

noninterference. Journal of Computer Security, 3(1):55–70,
1995. (an earlier version appears in CSFW’94).

[4] C. Boettcher, R. DeLong, J. Rushby, and W. Sifre. The MILS

component integration approach to secure information
sharing. In Proc. 27th IEEE/AIAA Digital Avionics Systems
Conference, pages 1.C.2–1–1.C.2–14, Oct. 2008.

[5] A. Bossi, C. Piazza, and S. Rossi. Modelling downgrading in
information ﬂow security. In Proc. IEEE Computer Security
Foundations Workshop, pages 187–201, 2004.

[6] R. Focardi and S. Rossi. Information ﬂow security in
dynamic contexts. In Proc. IEEE Computer Security
Foundations Workshop, pages 307–319, 2002.

[7] J. Goguen and J. Meseguer. Security policies and security

models. In Proc. IEEE Symp. on Security and Privacy, pages
11–20, 1982.

[8] J. Goguen and J. Meseguer. Unwinding and inference
control. In IEEE Symp. on Security and Privacy, pages
75–87, 1984.

[9] R. Gorrieri and M. Vernali. On intransitive non-interference
in some models of concurrency. In Foundations of Security
Analysis and Design VI - FOSAD Tutorial Lectures, volume
6858 of LNCS, pages 125–151. Springer-Verlag, 2011.
[10] J. Haigh and W. Young. Extending the noninterference

version of MLS for SAT. IEEE Trans. Softw. Eng.,
SE-13(2):141–150, Feb. 1987.

[11] J. Halpern and K. O’Neill. Secrecy in multiagent systems. In

Proc. IEEE Computer Security Foundations Workshop,
page 32, Los Alamitos, CA, USA, 2002. IEEE Computer
Society.

[12] D. M. Johnson and F. J. Thayer. Security and the

composition of machines. In Proc. IEEE Computer Security
Foundations Workshop, pages 72–89, 1988.

[13] H. Mantel. Information ﬂow control and applications -

bridging a gap. In J. N. Oliveira and P. Zave, editors, FME,
volume 2021 of LNCS, pages 153–172. Springer-Verlag,
2001.

[14] J. McLean. Reasoning about security models. In Proc. IEEE

Conf. on Security and Privacy, pages 123–131, 1987.

[15] J. McLean. A general theory of composition for trace sets

closed under selective interleaving functions. In Proc. IEEE
Symp. on Security and Privacy, pages 79–93, May 1994.

[16] J. K. Millen. Unwinding forward correctability. In

Proc. IEEE Computer Security Foundations Workshop, pages
2–10, 1994.

[17] S. M. More, P. Naumov, B. Nicholls, and A. Yang. A ternary
knowledge relation on secrets. In Proc. Conf. on Theoretical
Aspects of Rationality and Knowledge (TARK-2011),
Groningen, The Netherlands, July 12-14, 2011, pages 46–54,
2011.

[18] J. Mullins. Nondeterministic admissible interference. Journal

of Universal Computer Science, 6(11):1054–1070, 2000.

[19] A. Roscoe and M. Goldsmith. What is intransitive
noninterference? In Proc. IEEE Computer Security
Foundations Workshop, pages 228–238, 1999.

[20] J. Rushby. Design and veriﬁcation of secure systems. In
Proc. 8th Symposium on Operating Systems Principles,
pages 12–21, Asilomar CA, Dec. 1981. (ACM Operating
Systems Review, Vol 15, No. 1).

[21] J. Rushby. Noninterference, transitivity, and channel-control

security policies. Technical Report CSL-92-02, SRI
International, Dec. 1992.

[22] D. Sutherland. A model of information. In Proc. 9th National

Computer Security Conf., pages 175–183, 1986.

[23] R. van der Meyden. A comparison of semantic models for

intransitive noninterference. unpublished manuscript,
available at
http://www.cse.unsw.edu.au/~meyden, Dec.
2007.

[24] R. van der Meyden. What, indeed, is intransitive

noninterference? In J. Biskup and J. Lopez, editors, Proc.
European Symposium On Research In Computer Security
(ESORICS), volume 4734 of LNCS, pages 235–250.
Springer-Verlag, 2007.

[25] R. van der Meyden. On notions of causality and distributed

knowledge. In Proc. 11th Int. Conf. on Principles of
Knowledge Representation and Reasoning, pages 209–219,
2008.

[26] D. von Oheimb. Information ﬂow control revisited:

Noninﬂuence = Noninterference + Nonleakage. In Proc.
European Symposium On Research In Computer Security
(ESORICS), volume 3193 of LNCS, pages 225–243.
Springer-Verlag, 2004.

880