Transparent Data Deduplication in the Cloud

Frederik Armknecht
University of Mannheim

68131 Mannheim, Germany
armknecht@uni-mannheim.de
Ghassan O. Karame
NEC Laboratories Europe
69115 Heidelberg, Germany
ghassan.karame@neclab.eu

ABSTRACT
Cloud storage providers such as Dropbox and Google drive heav-
ily rely on data deduplication to save storage costs by only stor-
ing one copy of each uploaded ﬁle. Although recent studies report
that whole ﬁle deduplication can achieve up to 50% storage reduc-
tion, users do not directly beneﬁt from these savings—as there is no
transparent relation between effective storage costs and the prices
offered to the users.

In this paper, we propose a novel storage solution, ClearBox,
which allows a storage service provider to transparently attest to
its customers the deduplication patterns of the (encrypted) data that
it is storing. By doing so, ClearBox enables cloud users to ver-
ify the effective storage space that their data is occupying in the
cloud, and consequently to check whether they qualify for bene-
ﬁts such as price reductions, etc. ClearBox is secure against ma-
licious users and a rational storage provider, and ensures that ﬁles
can only be accessed by their legitimate owners. We evaluate a
prototype implementation of ClearBox using both Amazon S3 and
Dropbox as back-end cloud storage. Our ﬁndings show that our so-
lution works with the APIs provided by existing service providers
without any modiﬁcations and achieves comparable performance
to existing solutions.

Categories and Subject Descriptors
C.2.0 [Computer-Communication Networks]: General – Secu-
rity and protection.

General Terms
Security, Measurement, Experimentation.

Keywords
Cloud security; Secure data deduplication; Transparent attestation
of deduplication.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.

c(cid:2) 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.

DOI: http://dx.doi.org/10.1145/2810103.2813630.

Jens-Matthias Bohli
NEC Laboratories Europe
69115 Heidelberg, Germany

Jens-Matthias.Bohli@neclab.eu

Franck Youssef

NEC Laboratories Europe
69115 Heidelberg, Germany
franck.youssef@neclab.eu

1.

INTRODUCTION

Cloud storage services have become an integral part of our daily
lives. With more and more people operating multiple devices, cloud
storage promises a convenient means for users to store, access, and
seamlessly synchronize their data from multiple devices.

The increasing adoption of the cloud is also fueled by the mul-
titude of competing cloud storage services which offer relatively
cheap services. For example, Dropbox offers its customers free
2 GB accounts, Google drive offers 100 GB for only 1.99 USD,
while Box.com offers its business clients unlimited storage for only
12 EUR per month. These competitive offers are mainly due to
the sharp plummet exhibited by modern hard drives, going from
20 USD per GB to just about few cents per GB in 2014 [9].

To further increase their proﬁts,1 existing cloud storage providers

adopt aggressive storage efﬁciency solutions when storing their
clients’ data. Namely, existing clouds store duplicate data (either
at the block level or the ﬁle level) uploaded by different users only
once—thus tremendously saving storage costs. Recent studies show
that cross-user data deduplication can save storage costs by more
than 50% in standard ﬁle systems [35,36], and by up to 90-95% for
back-up applications [35].

The literature features a large number of proposals for securing
data deduplication (e.g., [14, 25, 42]) in the cloud. All these pro-
posals share the goal of enabling cloud providers to deduplicate
encrypted data stored by their users. Such solutions allow the cloud
provider to reduce its total storage, while ensuring the conﬁdential-
ity of stored data.

By doing so, existing solutions increase the proﬁtability of the
cloud, but do not allow users to directly beneﬁt from the savings of
deduplication over their data. Notice that cloud service providers
charge their customers based on the amount of data that they store—
irrespective of the level of data deduplication exhibited by their
data. However, a user who is using the cloud as back-up storage
should beneﬁt—and rightly so—from reductions (by up to 90%),
when compared to a client who is storing personal ﬁles in the cloud
which are less likely to be deduplicated. In Figure 1, we estimate
the cost reductions per user due to data deduplication in comparison
to the price per user in existing commodity cloud providers such as
Dropbox, Google drive, and Microsoft Onedrive. Our estimates
clearly suggest that there is considerable room for price reductions
for those users whose data undergoes considerable deduplication.
In this paper, we address this problem and we propose a novel
secure storage solution, dubbed ClearBox, which enables a cloud
provider to transparently and veriﬁably attest the deduplication pat-
terns of every ﬁle stored at the cloud. Our solution relies on gate-

1Cloud services are contributing to a 150 billion USD market [6].

886]

D
S
U

[
 
t
s
o
C
/
e
c
i
r

P

 10

 8

 6

 4

 2

 0

a r g i n

n   m

c ti o

u

d

e

R

Dropbox (ppu)
Google Drive (ppu)
MS OneDrive (ppu)
Estimated cost per user

 0

 100

 200

 300

 400

 500

Data Volume [GB]

Figure 1: Cost reductions due to data deduplication vs. prices
of commodity storage providers. The blue, green, and red
curves show the price currently charged by Dropbox, Google
Drive, and MS OneDrive, respectively. The dotted black line
depicts the estimated cost of storage per user in Amazon S3 [1]
after data undergoes deduplication. We assume that 50% of the
data stored by clients is deduplicated [36] with the data pertain-
ing to 2 other cloud users and that clients download 0.96% of
the data stored in their accounts per day [34]. The “reduction
margin” refers to the difference between the price borne by the
users and the effective cost of users’ storage after deduplica-
tion. “ppu” refers to the price per user.

way to orchestrate cross-user ﬁle-based deduplication prior to stor-
ing ﬁles on (public) cloud servers. ClearBox ensures that ﬁles can
only be accessed by legitimate owners, resists against a curious
cloud provider, and enables cloud users to verify the effective stor-
age space occupied by their encrypted ﬁles in the cloud (after dedu-
plication). By doing so, ClearBox provides its users with full trans-
parency on the storage savings exhibited by their data; this allows
users to assess whether they are acquiring appropriate service and
price reductions for their money—in spite of a rational gateway
that aims at maximizing its proﬁt.

ClearBox can be integrated with existing cloud storage providers
such as Amazon S3 and Dropbox without any modiﬁcations, and
motivates a new cloud pricing model which takes into account the
level of deduplication undergone by data. We believe that such
a model does not threaten the proﬁtability of the cloud business
and—on the contrary—gives considerable incentives for users to
store large and popular data such as music and video ﬁles in the
cloud (since the storage costs of popular data might be cheaper). In
summary, we make the following contributions in this work:

Concrete Instantiation: We describe a cloud storage scheme, dubb-
ed ClearBox, which employs a novel cryptographic tree-based
accumulator, CARDIAC, to attest in logarithmic time (with
respect to the number of clients that uploaded the ﬁle) the
deduplication patterns of every ﬁle stored in the cloud.
ClearBox additionally leverages Proofs of Ownership [23,
27], and self-expiring URL commands in order to effectively
manage access control on the ﬁles stored in the cloud.

Security Analysis: We provide a model for ClearBox and analyze
the security of our proposal according to our model. Namely,
we show that ClearBox enables users to verify the dedupli-
cation undergone by their ﬁles in spite of a rational provider
that aims at maximizing its proﬁt in the system. In addition,
we show that ClearBox resists against malicious clients and
a curious storage provider.

Prototype Implementation: We implement and evaluate a proto-
type based on ClearBox using both Amazon S3 and Dropbox
as back-end cloud storage, and we show that our proposal
does not impair the experience witnessed by cloud users and
incurs tolerable overhead on the gateway when orchestrating
ﬁle operations amongst its clients.

The remainder of this paper is organized as follows.

In Sec-
tion 2, we introduce our model and goals. In Section 3, we present
ClearBox and analyze its security. In Section 4, we evaluate a pro-
totype implementation based on the integration of ClearBox with
Amazon S3 and Dropbox. In Section 5, we review related work in
the area, and we conclude the paper in Section 6. In Appendices A
and B, we provide additional insights with respect to our scheme.

2. MODEL

Before we give a full description of ClearBox in Section 3, we

present in this section our system and security models.
2.1 System Model

ClearBox comprises a number of clients C1, C2, . . . that are in-
terested in storing their ﬁles at a storage provider S such that each
client learns the level of deduplication undergone by his ﬁles, that
is the number of clients that outsourced the same ﬁle. Since exist-
ing storage providers do not report the deduplication patterns of the
stored data, a straightforward approach would be to rely on a decen-
tralized scheme whereby users coordinate their ﬁle uploads prior
to storing their data onto the cloud; such a decentralized scheme,
however, requires interaction among users, and is unlikely to scale
as the number of users storing the same data increases [41]. This
is exactly why ClearBox makes use of an additional gateway G,
which interfaces between the users and the cloud, and orchestrates
data deduplication prior to storing the data on the cloud. Note that
G is a logically centralized entity, and can be easily instantiated
using distributed servers. We argue that our model is generic and
captures a number of practical deployment scenarios. For instance,
G could be an independent service which offers cheaper cloud stor-
age, by performing data deduplication over existing public clouds;
alternatively, G could be a service offered by S itself in order to
offer differentiation to existing cloud storage services, etc.

In our scheme, we assume that G owns an account hosted by
S, and orchestrates cross-user ﬁle-based deduplication2. By doing
so, G can provide its users with full transparency on the storage
savings due to deduplication exhibited by their data. For instance,
G can offer price reductions for customers whose data is highly
deduplicated (e.g., 50% discount if the ﬁle is deduplicated among
at least n entities). Alternatively, G could fairly distribute storage
costs amongst users who are storing deduplicated ﬁles; for exam-
ple, assume that there are n clients all storing the same ﬁle f, then
n of the cost
each client can effectively be charged a fraction of 1
of storing f. Notice that these reductions do not threaten the prof-
itability of G (nor S) which can still comfortably proﬁt from the
various offered services (e.g., resilience to failures, mobile access,
data synch across devices). On the contrary, we argue that offer-
ing the option of sharing storage costs with other clients storing
the same ﬁles may provide a clear differentiator compared to other
competitors. Nevertheless, providers are clearly not keen on shar-
ing parts of their proﬁts with the users and may not correctly report
the cost reductions due to data deduplication. The challenge here
2Although block-based deduplication can lower storage consump-
tion to as little as 32% of its original requirements, we point out
that nearly three quarters of the improvement observed could be
captured through whole-ﬁle deduplication [36].

887therefore lies in transparently and efﬁciently attesting data stor-
age costs (i.e., including deduplication savings) across users in the
presence of a storage provider which might not correctly report the
deduplication (and access) patterns of the data that it is storing.

Conforming with the operation of existing storage providers, we
assume that time is divided into epochs Ei of equal length, e.g., 12
hours [1]. Clients receive from G a list of their ﬁles and deduplica-
tion patterns at the end of every epoch. The deduplication pattern
of a given ﬁle refers to the number of users that store the same
deduplicated ﬁle in the cloud.

Similar to existing storage providers, ClearBox supports the fol-
lowing operations: Put, Get, Delete. In addition, ClearBox sup-
ports two additional protocols, Attest and Verify, which are used
to prove/verify the deduplication patterns undergone by ﬁles. We
start by describing these protocols. Here, the expression
Π : [P1 : in1; . . . , Pn : inn] → [P1 : out1; . . . , Pn : outn]
denotes the event that a set of parties P1, . . . , Pn run an interactive
protocol Π where ini and outi denote the input and output of Pi,
respectively.

The Put Protocol.

∗

The Put protocol is executed between the gateway and a client
C who aims to upload a ﬁle f. Initially, the client derives a key
kFID from the ﬁle which he uses to encrypt f to f
which is even-
tually uploaded to S via G. Next, both parties derive a ﬁle ID FID
that will serve as a practically unique handle to f. The gateway
G maintains internally a set F which contains pairs (FID,CFID )
where CFID is the set of clients that are registered to the ﬁle refer-
enced by FID. If no client is registered to a ﬁle with ﬁle ID FID,
it holds that F does not contain an element of the form (FID,∗).
Given this, the gateway checks if any users are registered to this
ﬁle already, i.e., if (FID,CFID ) ∈ F. If so, it inserts C into CFID.
Otherwise, a new set CFID = {C} is created and (FID,CFID ) in-
serted into F. Moreover, the client gets a veriﬁcation tag τ which
later allows to validate the proofs generated by G.

Put

:

[C : f ; G : F; S : ⊥] −→
[C : FID, kFID , τ ; G : FID,F ; S : f

∗

]

When we need to specify the veriﬁcation tag of a speciﬁc client C,
we write τC.
The Get Protocol.

When a client C wants to download a ﬁle f, it initiates this proto-
col with G and sends the corresponding ﬁle ID FID. The gateway
ﬁrst checks if F contains an entry (FID,CFID ). In the positive
case, it further veriﬁes if C ∈ CFID. If this veriﬁcation passes,
actions are taken such that eventually the client can download the
from S and decrypt it to f with kFID. Observe
encrypted ﬁle f
that we do not specify additional tokens to authenticate the client
as we assume the existence of authenticated channels between the
clients and G.

∗

Get

:

[C : FID, kFID ; G : F ; S : f
[C : f ; G : ⊥; S : ⊥]

∗

] −→

The Delete Protocol.

This protocol allows a client to delete a ﬁle or, more precisely,
to cancel his registration. To this end, the client sends the ﬁle ID
FID to the gateway who checks if (FID,CFID ) ∈ F for some set
CFID and if C ∈ CFID. If this is not the case, the request is sim-
ply ignored and C receives f = ⊥. Otherwise CFID is updated to
CFID \ {C} at the beginning of the next epoch. If CFID becomes

empty by this action, this means that no user is any longer regis-
tered to this ﬁle. Hence, G can request to S to delete the ﬁle.

Delete

:

[C : FID; G : F; S : f
[C : ⊥; G : F; S : ⊥]

∗

] −→

The Attest Protocol.

The purpose of the Attest procedure, which is executed by the
gateway only, is twofold. On the one hand, it generates a proof of
cardinality for a given ﬁle ID FID and an epoch E that attests an
upper bound for |CFID|—the number of clients registered to this
ﬁle within this epoch. On the other hand, it also includes a proof of
membership for a given client C with respect to CFID. Formally,
we have:

A ←Attest (FID, E,C FID , C).

The proof A, i.e., the output of Attest, contains a claim on an upper
bound of |CFID|, a compact digest for CFID, and possibly additional
information, so that a client C ∈ CFID can use the subsequent
Verify to get convinced of the upper bound of the set CFID and
its own membership. As described in Section 2.2, an upper bound
for |CFID| is sufﬁcient and necessary in our model. Namely, G
does not have incentives to report a larger value of |CFID| since this
results in G under-charging clients and hence a reduction of the
proﬁt. Therefore, to increase its proﬁts, G’s interest is to claim the
smallest possible upper bound at the end of each epoch.

The Verify Protocol.

In ClearBox, customers can verify (using the Verify protocol) the
proof generated by the Attest protocol to conﬁrm that they are part
of the set of ﬁle users, and to verify the upper bound on the total
number of ﬁle users. The Verify algorithm is executed by a client,
and uses the veriﬁcation tag which has been generated during the
Put procedure.

It outputs either accept or reject to indicate whether the proof

is accepted or not.

accept|reject ← Verify(FID, E,A, τ ).

2.2 Security Model

(cid:3)

(cid:3)

means that E either took place before epoch E

In the sequel, we assume that the communication between a
client and the gateway is authenticated to provide non-repudiation
and, in the case of need, encrypted. As already stated, we assume
that time is divided into a sequence of epochs E1, E2, . . ., where
E ≤ E
or that
both refer to the same epoch. If not mentioned otherwise, we will
always refer to epochs that happened in the past. Moreover, we
assume that all parties are synchronized. That is, at each point in
time, all parties share the same view on the current epoch (e.g., its
index number if epochs are represented by an increasing counter).
In each epoch, several protocol runs may be executed between
the gateway G and a client. A protocol run is represented by a
quadruple pi = (C, prot, (in), (out)) where C is a client, prot
denotes one of the protocols Put, Get, Delete, in denotes the inputs
of C, and out its outputs. For any past epoch E, we denote by PE
all protocol runs that occurred within this epoch. Here, we restrict
ourselves to full protocol runs, i.e., which have not been aborted.
We assume that these are uniquely ordered within the epoch. That
is for PE = {p1, . . . , p(cid:2)} where (cid:3) denotes the total number of
(cid:3) ∈ PE with
protocol runs within this epoch, it holds for any p, p
p (cid:9)= p
(cid:3)
) or p
(when p took place before p
< p
happened ﬁrst). We extend this order to the set of all
(when p
protocol runs in a straightforward way. More precisely, for any two
for all p ∈ PE
epochs E (cid:9)= E

that either p < p
(cid:3)

it holds that p < p

with E < E

(cid:3)

(cid:3)

(cid:3)

(cid:3)

(cid:3)

(cid:3)

888(cid:3) ∈ PE(cid:2). Finally, we denote by P≤E =

E(cid:2)≤E PE(cid:2) all
and all p
protocol runs that took place up to epoch E inclusive and deﬁne
P<E analogously.

(cid:2)

DEFINITION 1

(FILE REGISTRATION).

We say that a client C is registered to a ﬁle ID FID at some epoch
E if there exists a p = (C, Put, (f ), (FID, kFID , τ )) ∈ P≤E for
which one of the following two conditions hold:

1. p ∈ PE
2. p ∈ P<E and for all p

(cid:3) ∈ P<E with p < p

(cid:3)

p

(cid:3) (cid:9)= (C, Delete, (FID), (⊥)).

it holds that
We denote by CFID (E) the set of all clients that are registered to
FID at epoch E. If the considered epoch is clear from the context,
we simply write CFID. Finally, we denote by Ereg(E, C, f, FID,
τ ) the event that C is registered to FID within epoch E where f is
the ﬁle speciﬁed in the protocol run p mentioned above (that is the
ﬁle used in the last upload). Obviously, it holds that C ∈ CFID (E)
if and only if Ereg(E, C, f, FID, τ ) holds for some ﬁle f.

The ﬁrst condition in Def.1 means that if C uploaded the ﬁle within
epoch E, he is registered to the ﬁle, no matter if he requests to
delete it later in the same epoch. The second condition means that
if the upload took place in a previous epoch, C must not have sub-
sequently asked to delete it in an older epoch. Observe that also
here, if C asks to delete it within epoch E, he is still registered to
FID within this epoch. Likewise, we allow the client to download
the ﬁle any time within an epoch if he is registered to the ﬁle within
this epoch.

We are now ready to formally deﬁne correctness and soundness.

Correctness
For correctness, two requirements arise. First, a user who uploaded
a ﬁle with Put, must be able to obtain it with Get for those epochs
during which he is registered to this ﬁle. As ﬁles are identiﬁed
by their ﬁle ID, we address the ﬁle ID generation process ﬁrst.
Namely, we say that the ﬁle ID generation process creates ε-unique
ﬁle IDs if it holds for any two protocol runs (C, Put, (f ), (FID, kFID ))
and (C

, Put, (f

), (FID

, k

(cid:3)

(cid:3)

(cid:3)

(cid:3)
(cid:3)

(cid:3)
FID )) that
(cid:3)|f = f
(cid:3)|f (cid:9)= f

(cid:3)(cid:4)
(cid:3)(cid:4) ≤ ε.

= 1,

P r

P r

FID = FID

FID = FID

The ﬁrst condition means that the same ﬁle leads always to the same
ﬁle ID while the probability that different ﬁles result into the same
ﬁle ID is at most ε. This allows us to deﬁne correct access for our
scheme:

DEFINITION 2

(CORRECT ACCESS). Assume that the ﬁle ID
generation process is ε-unique. We say that the scheme provides
correct access if it holds for any client C, for any epoch E, and any
ﬁle f that if the event Ereg(E, C, f, FID) holds and if there exists
a protocol run (C, Get, (FID, kFID ), (f

)) ∈ PE, then:

(cid:3)

(cid:3)

(cid:3)

f

= f

P r

(cid:4) ≥ 1 − ε.

The reason that one cannot require the probability to be equal to 1
is that with some probability ε, two different ﬁles may get the same
ﬁle ID. In these cases, correctness cannot be guaranteed anymore.

DEFINITION 3

(CORRECT ATTESTATION). Let FID be an ar-
bitrary ﬁle ID and E be an arbitrary epoch. Moreover, let A be a
proof generated by the gateway for a ﬁle FID and epoch E. That
is, A ←Attest(FID , E,C FID ). Moreover, let bd denote the upper
bound for |CFID (E)| claimed in A and let C be an arbitrary client.

We say that the scheme provides correct attestation if under the
conditions of Ereg(E, C, f, FID, τ ) and |CFID (E)| ≤ bd it holds
that:

P r [accept ← Verify(FID, E,A, τ )|] = 1.

(4)

Soundness
We assume that each party (client, gateway, service provider) can
potentially misbehave but aim for different attack goals. Conse-
quently, soundness requires that security is achieved against all
these attackers. In what follows, we motivate each attacker type
and deﬁne the corresponding security goals.

Malicious Clients: In principle, we assume that a client may be
arbitrarily malicious with various aims: disrupting the service, re-
pudiate actions, and gain illegitimate access to ﬁles. The ﬁrst two
can be thwarted with standard mechanisms like authenticated chan-
nels. Thus, we focus on the third only: a user must only be able to
access a ﬁle f via Get if he had uploaded it before to G and if he is
still registered to it.

DEFINITION 4

(SECURE FILE ACCESS). We say that the sche-
me provides ε-secure ﬁle access if it holds for any client C, for any
)) ∈
epoch E, and any ﬁle f that for any (C, Get, (FID, kFID ), (f
PE while Ereg(E, C, f, FID, τ ) does not hold, then

(cid:3)

(cid:3)

= ⊥(cid:4) ≥ 1 − ε.

(cid:3)

P r

f

(5)

Rational Gateway: We assume that both the gateway and the ser-
vice provider are rational entities, e.g., see [43] for a similar as-
sumption. By rational, we mean that the gateway and the storage
provider will only deviate from the protocol if such a strategy in-
creases their proﬁt in the system. Notice that gateway has access to
all the information witnessed by the service provider (e.g., access
to the stored ﬁles). This implies that any attack by a rational service
provider may equally be executed by the gateway. Likewise, a ra-
tional gateway could not mount stronger attacks by colluding with
the service provider. Hence, we restrict our analysis in the sequel
to the security of our scheme given a rational gateway.

Recall that the gateway performs two types of interactions: at-
testing the number of registered clients and handling clients’ ﬁles.
Consequently, we see two different types of strategies that may al-
low the gateway to increase its advantage in the system: (i) over-
charging clients and (ii) illegitimately extracting ﬁle content.

With respect to the ﬁrst strategy, if a rational gateway manages
to convince clients of a smaller level of deduplication, the gateway
can charge higher prices. On the other hand, with respect to the sec-
ond strategy, the gateway may try to derive information about the
contents of the uploaded ﬁles3; notice that acquiring information
about users’ data can be economically beneﬁcial for the gateway
since the stored data can be misused, e.g. sold. Consequently, two
security goals need to be ensured: secure attestation and data con-
ﬁdentiality, that we formalize next.

DEFINITION 5

(SECURE ATTESTATION). Let FID be an ar-
bitrary ﬁle ID and E be an arbitrary epoch. Moreover, let A be a
proof generated by the gateway for a ﬁle FID, an epoch E,and a
client C, that is A ←Attest(FID , E,C FID , C). Moreover, let bd
denote the upper bound for |CFID (E)| claimed in A. We say that
the scheme provides ε-secure attestation if ¬Ereg(E, C, f, FID,
τ ) or |CFID (E)| > bd implies that

P r [accept ← Verify(FID, E,A, τ )|] ≤ ε.
3We assume the gateway to be curious in this respect.

(6)

(1)
(2)

(3)

889Observe that it is not in the interest of a rational gateway in our
model to report a larger value for |CFID (E)| as this would allow
the clients to demand further cost reductions.

With respect to the conﬁdentiality of the uploaded data, notice
that standard semantically secure encryption schemes cannot be
used in our context since they effectively prevent the deduplica-
tion of data [14, 25, 42]. Schemes that are suitable for deduplica-
tion produce the same ciphertext for multiple encryptions of the
same message and are referred to as message-locked encryption
(MLE) [13, 15]. The achievable security in our case is therefore
that of MLE schemes.

To describe the security of MLE schemes, we adapt in what fol-
lows the security notion introduced in [15] which guarantees pri-
vacy under chosen distribution attacks.

An MLE scheme consists of a tuple of algorithms (setup, keygen,
enc,dec, tag), where setup generates (public) parameters P , the
key generation keygen generates a key k given P and a message
f; ﬁnally, enc and dec are the algorithms to encrypt and decrypt
f with key k. The tag generation algorithm tag creates a tag for a
ciphertext. This is covered in our model by the unique ﬁle identiﬁer
FID and does not play a role in the privacy notion.
A message space is given by an algorithm M which samples
messages M ∈ {0, 1}∗
according to a distribution and may provide
additional context information. The message sampling must be un-
predictable, i.e., the probability to predict the output of M is neg-
ligible given context information. The security of an MLE scheme
is deﬁned by the following experiment PRV$-CDAA
M LE,M, an un-
predictable sampling algorithm M and an adversary A [15]:

1. The environment randomly generates a parameter set P and

a bit b ← {0, 1}.
2. The environment samples a set of messages M and context
information Z: (M, Z) ← M. As the same message would
result in the same encryption, the restriction here is that all
messages are distinct.

3. For all messages M [i] in M, the environment encrypts mes-

sage M [i] with the MLE scheme:

C0[i] ← enckeygen(M [i])(M [i])

and generates a random string of the same length:

C1[i] ← {0, 1}|C0[i]|

.

4. The adversary obtains the set Cb and outputs a guess for b:

(cid:5)b ← A(P, Cb, Z)
5. The adversary wins if b equals(cid:5)b in this case the experiment

returns 1, otherwise 0.

DEFINITION 6

(DATA CONFIDENTIALITY). We say that the
M LE scheme for message sampling algorithm M is secure, if the
adversary’s advantage in winning the PRV$-CDAA
M LE,M experi-
ment is negligible, i.e.

2 · P r

1 ← PRV$-CDAA

M LE,M

− 1 ≤ negl(κ),

(cid:6)

(cid:7)

for a security parameter κ.

Notice that it is integral for both G and S to know the ﬁle sizes
and the download patterns of ﬁles in order to perform correct ac-
counting4. Therefore, hiding this information cannot be part of our
goals.

4Current cloud services maintain a detailed log containing all the
activities per account.

Command
createBucket(B)
PUT(B, FID)
GET(B, FID)
DELETE(B, FID)
generateURL(COMMAND, t) Generate a URL expiring at time

Description
Creates a bucket B
Upload a ﬁle FID to B
Download a ﬁle FID from B
Delete a ﬁle FID from B

t supporting PUT, GET, DELETE.

Table 1: Sample API exposed by Amazon S3 and Google Cloud
Storage. COMMAND refers to an HTTP command such as
PUT(B, FID).
2.3 Design Goals

In addition to the security goals stated above, our proposed solu-
tion should satisfy the following functional requirements. ClearBox
should work within the APIs provided by current service providers,
without deteriorating the performance witnessed by users when
compared to standard solutions where users directly interface with
S. Similar to existing cloud storage providers such as Amazon S3
and Google Cloud Storage, we assume that S exposes to its clients a
standard interface consisting of a handful of basic operations, such
as storing a ﬁle, retrieving a ﬁle, deleting a ﬁle, generating a signed
URL for sending HTTP commands for storage/retrieval, etc. (cf.
Table 1). Where appropriate, we also discuss the case where S is a
commodity cloud service provider and exposes a simpler interface,
e.g., that does not allow storing a ﬁle using a URL.

Moreover, our solution should scale with the number of users,
the ﬁle size, and the number of uploaded ﬁles, and should incur
tolerable overhead on the users when verifying the deduplication
patterns of their ﬁles at the end of every epoch.

3. ClearBox

In this section, we present our solution, and analyze its security

according to the model outlined in Section 2.
3.1 Overview

ClearBox ensures a transparent attestation of the storage con-
sumption of users whose data is effectively being deduplicated—
without compromising the conﬁdentiality of the stored data.

To attest the deduplication patterns to its customers, one naive
solution would be for the gateway to publish the list of all clients
associated to each deduplicated ﬁle (e.g., on a public bulletin board)
such that each client could ﬁrst check if (i) he is a member of the
list and (ii) if the size of the list corresponds to the price reduction
offered by the gateway for storing this ﬁle. Besides the fact that
this solution does not scale, it is likewise within the interest of G
not to publish the entire list of its clients and their ﬁles; for exam-
ple, competitors could otherwise learn information on the service
offered by G (e.g., total turnover).

To remedy this, ClearBox employs a novel Merkle-tree based
cryptographic accumulator which is maintained by the gateway to
efﬁciently accumulate the IDs of the users registered to the same
ﬁle within the same time epoch. Our construct ensures that each
user can check that his ID is correctly accumulated at the end of ev-
ery epoch. Additionally, our accumulator encodes an upper bound
on the number of accumulated values, thus enabling any legitimate
client associated to the accumulator to verify (in logarithmic time
with respect to the number of clients that uploaded the same ﬁle)
this bound.

Clearly, a solution that requires the gateway to publish details
about all the accumulators for all the stored ﬁles does not scale with
the number of ﬁles stored in the cloud. This is why ClearBox relies
on a probabilistic algorithm which selectively reveals details about
a number of ﬁle accumulators in each epoch. However, if the gate-

890way could select which ﬁle accumulators to publish, then G could
easily cheat by only creating correct accumulators for the selected
ﬁles, while misreporting the deduplication patterns of the remain-
ing ﬁles. In ClearBox, the choice of which accumulators are pub-
lished in each epoch is seeded by an external source of randomness
which cannot be predicted but can be veriﬁed by any entity. We
show how such a secure source of randomness can be efﬁciently
instantiated using Bitcoin. This enables any client to validate that
the sampling has been done correctly, and as such that he is acquir-
ing the promised price reductions—without giving any advantage
for G to misbehave.

ClearBox enforces ﬁne-grained access control on shared ﬁles by
leveraging self-expiring URLs when accessing content. Namely,
whenever a user wishes to access a given resource, the gateway
generates a URL for that resource on the ﬂy, which expires after
a short period of time. As shown in Table 1, existing cloud APIs
support the dynamic generation of expiring URLs. By doing so,
ClearBox does not only ensure that G can restrict access to the data
stored on the cloud, but also enables G to keep track of the access
patterns of its users (e.g., to be used in billing). ClearBox also relies
on an oblivious server-aided key generation protocol to ensure that
the stored ﬁles are encrypted with keys that are dependent on both
the hash of the ﬁle and the gateway’s secret. This protects against
brute force search attacks when the message content is predictable,
but also ensures that a curious gateway/storage provider which does
not know the ﬁle hash cannot acquire the necessary keys to decrypt
the ﬁle (since the key generation protocol is oblivious). To protect
against malicious users who otherwise have obtained the ﬁle hash
(e.g., by theft/malware) but do not possess the full ﬁle, ClearBox
employs proofs of ownership over the encrypted ﬁle to verify that
a given user is indeed in possession of the full ﬁle.

In the following subsections, we go into greater detail on the
various parts of ClearBox, starting with the building blocks that we
will use in our solution, then moving on to the protocol speciﬁca-
tion, and ﬁnally to its security analysis.
3.2 Building Blocks

Before describing ClearBox in detail, we start by outlining the

building blocks that will be used in ClearBox.

3.2.1 CARDIAC
Cryptographic accumulators (e.g. [12, 20–22, 30, 33, 39]) basi-
cally constitute one-way membership functions; these functions
can be used to answer a query whether a given candidate belongs
to a set.

In what follows, we show how to construct a cardinality-proving
accumulator (CARDIAC) which leverages Merkle trees in order to
efﬁciently provide proofs of membership and (non-public) proofs
of maximum set cardinality. As we show in Section 3.3, proofs
of cardinality are needed to attest the ﬁle deduplication patterns to
users.

A Merkle tree is a binary tree, in which the data is stored in the
leaves. Let ai,j denote a node in the tree located at the i-th level
and j-th position. Here, the level refers to the distance (in hops) to
the leaf nodes; clearly, leaf nodes are located at distance 0. On the
other hand, the position within a level is computed incrementally
from left to right starting from position 0; for example, the leftmost
node of level 1 is denoted by a1,0. In a Merkle tree, the intermediate
nodes are computed as the hash of their respective child nodes;
namely ai+1,j = H(ai,2j, ai,2j+1).

Given a tree of height (cid:3), CARDIAC accumulates elements of a
set X by assigning these to the leaf nodes (starting from position
0) while the remaining leaf nodes a0,|X|, . . . , a0,2(cid:2)−1 are ﬁlled

with a distinct symbol 0. We call these the zero leaves. Nodes
that can be computed from the zero leaves play a special role.
We denote these as open nodes in the sense that their values are
openly known. More formally, the zero leaves a0,|X|, . . . , a0,2(cid:2) are
open. Moreover, if ai,2j and ai,2j+1 are both open, so is ai+1,j =
H(i + 1, ai,2j, ai,2j+1).

We now outline the main algorithms (Acc, ProveM , VerifyM,
ProveC , VerifyC) provided by CARDIAC. Observe that ProveM
and ProveC will be used to implement Attest and likewise VerifyM
and VerifyM to instantiate Verify.
δ ← Acc(X). This algorithm accumulates the elements of a set
X into a digest δ. In CARDIAC, δ corresponds to the hash
of the root node of the modiﬁed Merkle tree, a(cid:2),0, and the
height (cid:3) of the tree, i.e., δ = H(a(cid:2),0, (cid:3)).
πM ← ProveM (X, x). Given a set X and element x ∈ X, this
algorithm outputs a proof of membership πM asserting that
x ∈ X. πM consists of the sibling path of x in the modiﬁed
Merkle tree and the root a(cid:2),0.

VerifyM(δ, x, πM ). Given δ, an element x, its sibling path and
the root a(cid:2),0, this algorithm outputs true if and only if δ =
H(a(cid:2),0, (cid:3)) where (cid:3) is the length of the sibling path and the
sibling path of x matches the root a(cid:2),0.
πC ← ProveC (X). Given a set X, this algorithm outputs a proof
πC attesting an upper bound on the size |X| of the set. Here,
πC consists of the size of X, the right-most non-zero element
a0,|X|−1, its sibling path, and the root of the tree a(cid:2),0.
VerifyC(δ, c, πC ). Given the digest δ, the cardinality |X| of the
set X, and a proof of cardinality consisting of a0,|X|−1, its
sibling path, and the root of the tree a(cid:2),0, this algorithm out-
puts true if the following conditions are met:
• it holds that δ = H(a(cid:2),0, (cid:3)) where (cid:3) is the length (cid:3) of
• it holds that 2(cid:2)−1 < |X| ≤ 2(cid:2),5
• the sibling path of a0,|X|−1 matches the root a(cid:2),0, and
• all nodes on the sibling path that are open do contain

the sibling path,

the right value.

Here, VerifyC assumes that all the leafs with position larger
than |X| −1 in level 0 are ﬁlled with the 0 element which is
not contained in X.

In Appendix A, we show that, in addition to proofs of member-
ship, our CARDIAC instantiation provides a proof that the number
of non-zero leaves in the Merkle tree with root a(cid:2),0 is at most |X|.
3.2.2 Time-Dependent Randomness
ClearBox leverages a time-dependent randomness generator
GetRandomness : T → {0, 1}(cid:2)seed where T denotes a set of dis-
crete points in time. In a nutshell, GetRandomness produces val-
ues that are unpredictable but publicly reconstructible.
More formally, let cur denote the current time. On input t ∈ T ,
GetRandomness outputs a uniformly random string in {0, 1}(cid:2)seed
if t ≤ cur, otherwise GetRandomness outputs ⊥. We say that
GetRandomness is secure if the output of GetRandomness(t) can-
−(cid:2)seed
not be predicted with probability signiﬁcantly better than 2
as long as t < cur.

Similar to [10], we instantiate GetRandomness by leveraging
functionality from Bitcoin, since the latter offers a convenient means
(e.g., by means of API) to acquire time-dependent randomness.
5This condition minimizes the number of open nodes in the tree,
and hence the effort in proving/verifying membership and cardi-
nality.

891h ← H∗

Client
(f )
R← Z
∗
p
ˆh ← h · gr

r

1

s ← ˆs · y

−r
v1 ← e (s, g2) , v2 ← e (h, y2)
1
If v1 (cid:9)= v2 then ret ⊥
Else ret k ← H(s)

Gateway

(cid:9)x

(cid:8)
ˆh

ˆs ←

ˆh

ˆs

Figure 2: Server-aided key generation module based on blind
BLS signatures. Here, Γ1, Γ2 are two groups with order p,
g1, g2 generators of Γ1 and Γ2 respectively, the pairing func-
tion e : Γ1 × Γ2 → ΓT , the hash function H∗
: {0, 1}∗ → Γ1,
the secret key x ∈ Z
∗
p, with corresponding public keys y1 =
1 , y2 = gx
gx
2 .

Namely, Bitcoin relies on blocks, a hash-based Proof of Work con-
cept, to ensure the security of transactions. In particular, Bitcoin
peers need to ﬁnd a nonce, which when hashed with the last block
hash and the root of the Merkle tree accumulating recent transac-
tions, the overall hash is smaller than a 256-bit threshold.

The difﬁculty of block generation in Bitcoin is adjusted so that
blocks are generated once every 10 minutes on average; it was
shown in [29] that the block generation in Bitcoin follows a shifted
geometric distribution with parameter p = 0.19. Recent studies
show that a public randomness beacon—outputting 64 bits of min-
entropy every 10 minutes—can be built atop Bitcoin [8].

Given this, GetRandomness then unfolds as follows. On input
time t, GetRandomness outputs the hash of the latest block that
has appeared since time t in the Bitcoin block chain. Clearly, if
t >cur corresponds to a time in the future, then GetRandomness
will output ⊥, since the hash of a Bitcoin block that would appear
in the future cannot be predicted. On the other hand, it is straight-
forward to compute GetRandomness(t), for a value t ≤ cur (i.e.,
t is in the past) by fetching the hash of previous Bitcoin blocks.
In this way, GetRandomness enables an untrusted party to sample
randomness—without being able to predict the outcome ahead of
time.

The purpose of GetRandomness is to ensure that the selection
of ﬁles to which the gateway attests the deduplication pattern is
randomly chosen and not precomputed.

While using a time-dependent source of randomness is a viable
option, other alternatives may be imaginable. For example, one
may couple the selection of ﬁles with Fiat-Shamir heuristics [26]
to ensure that the selection is randomly chosen from the point of
view of the gateway and couple it with proofs of work. If the time
effort to generate a valid proof of work is about the duration of an
epoch, a malicious gateway may not be able to precompute the se-
lections. We leave the question of investigating viable alternatives
to external sources of randomness as an interesting direction for
future research.
3.2.3
ClearBox employs an oblivious protocol adapted from [14] which
is executed between clients and the gateway to generate the keys
required to encrypt the stored ﬁles. Unlike [14], our protocol does
not rely on RSA, and is based on blind BLS signatures [17, 18].
Although the veriﬁcation of BLS signatures is more expensive than
its RSA counterpart, BLS signatures are considerably shorter than
RSA signatures, and are faster to compute by the gateway.

Server-Aided Key Generation

As shown in Figure 2, we assume that at setup, the gateway
chooses two groups Γ1 and Γ2 with order p, and a computable bi-

(cid:10)

, g2

= e (h, y2) .

−rx
hxgrx
1 g
1

2 ∈ Γ2. Let H∗

(f ), blinds it by multiplying it with gr

linear map e : Γ1 × Γ2 → ΓT . Additionally, the gateway chooses
a private key x ∈ Zp, and the corresponding public keys y1 =
1 ∈ Γ1 and y2 = gx
: {0, 1}∗ → Γ1 be a crypto-
gx
graphic hash function which maps bitstrings of arbitrary length to
group elements in Γ1. Prior to storing a ﬁle f, the client computes
h ← H∗
1, given a randomly
chosen r ∈ Zp, and sends the blinded hash ˆh to the gateway. The
latter derives the signature on the received message and sends the
(cid:11)
result back to the client, who computes the unblinded signature s
and veriﬁes that: e (s, g2) = e
The encryption key is then computed as the hash of the unblinded
signature: k ← H(s). The beneﬁts of such a key generation module
are twofold:
• Since the protocol is oblivious, it ensures that the gateway
does not learn any information about the ﬁles (e.g., about the
ﬁle hash) during the process. On the other hand, this proto-
col enables the client to check the correctness of the compu-
tation performed by the gateway (i.e., verify the gateway’s
signature). As we show later, this veriﬁcation is needed to
prevent a rational G from registering users of the same ﬁle to
different ﬁle versions with reduced level of deduplication.
• By involving the gateway in the key generation module, brute-
force attacks on predictable messages can be slowed down
by rate-limiting key-generation requests to G. Notice that,
similar to [14], this scheme does not prevent a curious G
from performing brute-force searches on predictable mes-
sages, acquiring the hash, and the corresponding key k. In
this sense, the security offered by our scheme reduces to that
of existing MLE schemes (cf. Section 3.4).

3.2.4 Proof of Ownership
The aforementioned server-aided key generation ensures that an
adversary which is not equipped with the correct ﬁle hash cannot
acquire the ﬁle encryption key k. However, a user who has wrong-
fully obtained the ﬁle hash would be able to claim ﬁle ownership.
In this case, Proofs of Ownership (PoW) can be used by the gate-
way to ensure that the client is in possession of the ﬁle in its entirety
(and not only of its hash) [16, 23, 27].

In this paper, we rely on the PoW due to Halevi et al. [27] which,
as far as we are aware, results in the lowest computational and stor-
age overhead on the gateway [16]. This PoW computes a Merkle
tree over the ﬁle f, such that the root of the Merkle tree constitutes
a commitment to the ﬁle. In the sequel, we denote by M TBuf(f ) the
root of the Merkle tree as output by the PoW of [27] given an input
Buf(f ), being an encoding of a ﬁle f. The veriﬁer can challenge
the prover for any block of the ﬁle, and the prover is able to prove
knowledge of the challenged block by submitting the authentica-
tion path of this block. In order to reduce the number of challenges
for verifying the PoW, the ﬁle is encoded into Buf(f ) before com-
puting the Merkle tree. An additional trade-off can be applied by
limiting Buf(f ) to a maximum size of 64 MB in the case of larger
ﬁles.

The security of this scheme is based on the minimum distance of
random linear codes. We refer the readers to Appendix B for more
details on the PoW of [27].

3.3 ClearBox: Protocol Speciﬁcation

We now detail the speciﬁcations for the procedures of ClearBox.
As mentioned earlier, we assume in the sequel that G owns an ac-
count hosted by S, and that the communication between C, S, and
G occurs over authenticated and encrypted channels.

892/
/
o
o
Speciﬁcation of the Put Procedure.

In ClearBox, when a client C wishes to upload a new ﬁle f
onto S, C issues an upload request to G. Subsequently, C and G
start executing the server-aided key generation protocol described
in Section 3.2.3. The outcome of that protocol is the key k ← H(s),
where s ← H∗

(f )x given a cryptographic hash function H∗

C then encrypts f using encryption algorithm enc under key k,
computes and sends to G the root of the Merkle tree output by the
PoW of [27], that is FID ← M TBuf(enc(k,f )) where Buf is an
encoding function (cf. Section 3.2.4).

.

Subsequently, G checks if any other client has previously stored

a ﬁle indexed by FID. Here, two cases emerge:

f has not been stored before:

In this case, G issues a timed gen-
erateURL command allowing the client to upload the data
onto G’s account within a time interval. Recall that a timed
generateURL command results in a URL which expires af-
ter the speciﬁed period of time6. After the upload of the
∗ ← enc(k, f ) terminates, G accesses S,
encrypted ﬁle f
computes M TBuf(f∗) using the PoW of [27], and veriﬁes
that it matches FID. If the veriﬁcation matches, G stores
in a newly generated struc-
the metadata associated with f
ture indexed by FID (such as the client ID C and the size of
, see Section 4 for more details). Otherwise, if M TBuf(f∗)
f
does not match FID, G deletes the ﬁle and appends C to a
blacklist.

∗

∗

f has been stored before:

In this case, G requests that C proves
that it owns the ﬁle f. For that purpose, G and C execute
the PoW protocol of [27] (we refer the reader to Appendix B
for more details). In essence, G chooses a random number
),
u of leaf indexes of the Merkle tree computed over Buf(f
and asks C for for the sibling-paths of all the u leaves. In
response, C returns the sibling paths corresponding to the
).
chosen u leafs associated with the Merkle tree of Buf(f
G accepts if all the sibling paths are valid with respect to
the stored FID. If this veriﬁcation passes, G appends C to
the ﬁle structure FID, and sends an ACK to C. In turn, C
deletes the local copy of the ﬁle, and only needs to store FID
and the key k.

∗

∗

Speciﬁcation of the Get Procedure.

To download a ﬁle with index FID, C submits FID to G; the
latter checks that C is a member of the user list added to the meta-
data structure of FID. If so, G generates a timed URL allowing C
to download the requested ﬁle from S.7
Notice that if C did not locally cache the decryption key associ-
ated with FID, then C can leverage its knowledge of H∗
(f ) in or-
der to acquire the corresponding key by executing the server-aided
generation protocol with G.

Speciﬁcation of the Delete Procedure.

When C wants to delete ﬁle FID, it informs G. G marks C for
removal from the metadata structure associated with FID in the
subsequent epoch (see Section 3.3 for more details). If no further
clients are registered for this ﬁle, G sends a request to S to delete it.

6In our implementation, we set the expiry timeout of the URL to 30
seconds; this means that clients have 30 seconds to start the upload
process.
7Here, G additionally notes the number of download requests per-
formed by C for ﬁle FID.

Speciﬁcation of the Attest Procedure.

At the end of each epoch, G attests the deduplication patterns
of its clients’ ﬁles, e.g., in their bills. Notice that if a client re-
quested the deletion of a ﬁle f during the epoch, G only removes
the marked clients from the clients list subscribed to FID after the
end of the epoch.

At the end of epoch Ej, the bill of every client C of G includes
for each stored ﬁle f the number of accesses by C and the cardinal-
ity of CFID, which denotes the set of clients registered to f. Here,
we assume a static setting where clients are charged for ﬁles that
they are storing within each epoch; this conforms with the func-
tionality of existing providers, such as Amazon S3, which rely on
ﬁxed epochs for measuring storage consumption (e.g., the epoch
interval is 12 hours in Amazon S3). Since our Attest procedure
is efﬁcient (cf. Section 4), our solution can be made practically
dynamic by relying on extremely small epochs—in which case the
bill can accumulate the ﬁne-grained storage consumption over a
number of epochs.

The bill issued by the provider acts as a binding commitment by
G to the deduplication (and access) patterns of its clients’ ﬁles. Af-
ter the bills are issued, G needs to convince his clients that they are
correctly formed. Notice that G keeps a record of the authenticated
download requests by clients, which can be used to prove the num-
ber of their accesses per ﬁle. Moreover, G needs to prove to all
clients of a ﬁle f that:
• Each of these clients is included in the set of clients CFID
• The size of CFID is as claimed in the bill.
• Clients storing f are referenced to the same set CFID.
As described in Section 3.4, this last condition prevents G from
including users of same ﬁle into different accumulators, resulting
in the under-reporting of the ﬁle deduplication patterns.

storing FID.

The ﬁrst two proofs can be realized by G using CARDIAC.
More speciﬁcally, G accumulates the IDs of the clients storing
FID; here, for each user Ci ∈ CFID, a leaf node with value Ui ←
H(FID||Ci||Ej||seedi,j) is created, where seedi,j denotes a nonce
which is sampled for this client and this epoch and which is com-
municated within the bill. As we show in Section 3.4, this protects
user privacy and ensures that the ID of any user is not given in
the clear to other users when G issues proofs of size and/or mem-
bership. The accumulation of all Ui ∈ CFID results into a digest
denoted by δ. As described earlier, CARDIAC enables G to prove
to each client in the set that (i) the client is part of the accumu-
lated set CFID and (ii) an upper bound of the size |CFID| of the
accumulated set. Observe that an upper bound on the cardinality
is sufﬁcient to protect against a rational gateway. This is the case
since the gateway does not beneﬁt from reporting a larger |CFID|,
since this might entail price reductions to the clients storing f.

Figure 3 shows an example for a set comprising 5 clients U0, . . . ,
U4. The elements circled in dotted red depict the membership proof
for U2, while the elements circled in solid blue depict the proof of
set cardinality which consists of the sibling path of the last non-
zero element. The grey elements consist of the open nodes, i.e., the
zero leaves which are not associated to any client and hence ﬁlled
with symbol 0, and all nodes that can be derived from these. Note
that the proofs of membership and set cardinality can be further
compressed in case the respective sibling paths contain the same
elements.

G still needs to show that all the clients storing FID are accumu-
lated in the same accumulator. This can be achieved by publishing
the association between FID, and its corresponding accumulator
digest on a public bulletin board (e.g., on the public website of
G). However, this approach does not scale with the number of ﬁles

893a3,0

a2,0

a2,1

a1,0

a1,1

a1,2

a1,3

a0,0

a0,1

a0,2

a0,3

a0,4

a0,5

a0,6

a0,7

U0

U1

U2

U3

U4

0

0

0

Figure 3: Sketch of a proof of set membership and set cardinal-
ity given a set of 5 elements in CARDIAC.

processed by G. For that reason, we elect to only publish those
associations for a randomly chosen subset of all ﬁles. Here, G ﬁrst
obtains a random seed by invoking GetRandomness(tj), where tj
denotes a point in time after the issuance of the bills for epoch Ej
which is determined by a deterministic and publicly known process.
This seed is used to sample the ﬁles whose accumulators need to be
published during this epoch; for instance, G can extract ν-bits of
the seed and use them as a mask to decide which ﬁle accumulators
to publish. The probability that any ﬁle FID is selected at the end
of a given epoch is subsequently given by 2

−ν.

Notice that, in this case, G needs to compute proofs of member-
ship and set cardinality only for the selected ﬁles; this information
is sent to each client storing the selected ﬁles. The pairs (FID, δ)
for the selected ﬁles are subsequently published by G (e.g., on its
own website).

Speciﬁcation of the Verify Procedure.

The Verify procedure is only conducted by clients storing ﬁles
for which the corresponding pairs (FID, δ) have been published
by G at the end of the epoch. Notice that clients can easily check
which FID are sampled by invoking GetRandomness(tj).

Given the proofs of membership and set cardinality issued by G
for their ﬁles, clients invoke the VerifyC and VerifyM algorithms
of CARDIAC in order to verify that their ID is included in the set
of clients CFID storing FID, and that |CFID| is as claimed in the
bill. Moreover, clients check the pairs (FID, δ) published by G to
verify that there is only one set CFID which they are referenced to.
In this case, the proof of membership consists of the sibling-
paths of leaves of the tree of height (cid:11)log2(|CFID|)(cid:12). Hence, the
size of the membership proof is at most 2 · (cid:11)log2(|CFID|)(cid:12) hash
values and veriﬁcation takes at most 2 · (cid:11)log2(|CFID|)(cid:12) executions
of the hash function. On the other hand, the proof of cardinal-
ity requires at most 2(cid:2)−1 hash operations to check the open leaves
(since we assume that at least half of the leaves are occupied by
the clients); that is, the size of the proof of cardinality is at most
2 · (cid:11)log2(|CFID|)(cid:12) hash values, but veriﬁcation requires O(|CFID|)
hash operations. Notice that this is at most half the effort required
to construct δ in CARDIAC. As shown in Section 4, this process
is extremely efﬁcient; notice that the veriﬁcation of VerifyC and
VerifyM can be made even more efﬁcient if the client pre-computes
and stores the open (zero) leaves in the tree. As the values of these
nodes are independent of the ﬁle ID and the IDs of the clients, this
precomputation could be performed once for a selection of trees of
different heights.

Additional Operations.
Directories and other functionality: ClearBox hides the clients’
directory structures from G by working on a single directory struc-
ture hosted within S’s account on the cloud. This has the beneﬁt of

reducing the overhead borne by G (i.e., no path related overhead)
and minimizes information leakage towards G.

Directory operations such as directory creation, directory renam-
ing, etc. are locally handled by the software client of the users.
Here, local directories contain pointers to the ﬁles stored therein
and outsourced to the cloud; this enables the local client to perform
operations such as directory listing and ﬁle renaming without the
need to contact G—thereby minimizing the overhead incurred on
G. Only operations that affect the client ﬁles stored on the cloud
(e.g., ﬁle deletion/creation) are transmitted to G.
Other APIs: Recall that ClearBox leverages expiring URL-based
PUT commands (exposed by Amazon S3 and Google Cloud Stor-
age [3]) to enable clients to upload new objects directly to S; ex-
piring URLs are also important in ClearBox to revoke data access
to clients.

A number of commodity cloud service providers such as Drop-
box, and Google drive, however, do not support URL commands
for ﬁle creation, and only provide (non-expiring) URL-based ﬁle
download. To integrate ClearBox with such commodity storage
providers, we note the following differences to the protocol speci-
ﬁcation of ClearBox (cf. Section 3.3):
• At ﬁle upload time, the URL-based PUT is replaced by the
clients uploading the ﬁle to G, which in turn uploads the ﬁle
to S. Recall that G has to compute the Merkle tree over
the uploaded ﬁle; this can be done asynchronously before G
uploads the ﬁle to S—therefore reducing the performance
penalty incurred on G.
• Files are stored under random identiﬁers, and can be ac-
cessed by means of permanent URLs which map to the ﬁle
ID. When the user requests to delete a ﬁle, G renames the ﬁle
to a new randomly selected ID. Other legitimate clients who
require access to the ﬁle have to contact G who informs them
of the new URL corresponding to the renamed ﬁle object.

In Section 4, we present a prototype implementation of ClearBox
which interfaces with Dropbox, and we use it to evaluate the per-
formance of this alternative technique.
Rate-Limiting: Similar to [14], we rate-limit requests to G to pre-
vent possible brute-force search attacks on predictable ﬁle contents
(in the assisted key generation phase), and to prevent resource ex-
haustion attacks by malicious clients. For this purpose, we limit
the number of requests Ri that a client can perform in a given time
frame Ti to a threshold θmax.

3.4 Security Analysis

In this section, we address the security of ClearBox with respect

to the model outlined in Section 2.

Secure Access: We start by showing that ClearBox ensures that
a client who uploaded a ﬁle with Put will be able to recover the
ﬁle with Get as long as he does not delete it. Since we assume
that G and S will not tamper with the storage protocol, the threat
to the soundness argument can only originate from other malicious
clients. Moreover, since the procedures Get and Verify do not mod-
ify the stored data, we will focus the analysis on the operations
Delete and Put.

Clearly, the Delete procedure only removes the access of the user
requesting the deletion. Impersonation is not possible here since we
assume a proper identity management by G. Namely, the gateway
will only delete a ﬁle with ID FID if |CFID| = 0.

On the other hand, the Put procedure can only incur in the mod-
iﬁcation of data when a ﬁle f is uploaded for the ﬁrst time. A
subsequent upload of f is deduplicated and therefore does not en-

894∗

∗

∗

∗

tail any data modiﬁcation at S. Notice that during initial upload, a
malicious client can try to defect from the protocol, and construct
an FID that does not ﬁt to f, upload another ﬁle, or encrypt the
ﬁle using the wrong key, etc. Recall that G veriﬁes FID by down-
to check whether FID ?= M TBuf(f∗).
loading the uploaded ﬁle f
, that is
Notice that if a malicious user creates a malformed f
∗ (cid:9)= enc(H(H∗
(f )x), f ) for any ﬁle f, then this will result into
f
a random FID value which, with overwhelming probability, will
would be stored by
not collide with any other ﬁle ID. That is, f
S without being deduplicated—which incurs storage costs on the
) without af-
malicious client (as he is fully charged for storing f
fecting the remaining honest clients.

With respect to illegitimate client access, we distinguish between
(i) the case that a client obtains ownership of a ﬁle when uploading
the ﬁle for the ﬁrst time with Put and (ii) the case where the client
accesses the ﬁle with Get at some later point in time without having
the ownership in the ﬁle. For case (i), uploading of a ﬁle that is not
yet stored will not help the client. Namely, the client cannot pretend
to upload a ﬁle with a different FID as G will check the integrity
of the ﬁle. When a user requests to upload a ﬁle which has been
stored already, the adopted proof of ownership scheme (PoW) (cf.
Section 3.2.4 and Appendix B) ensures that the client must know
the entire ﬁle, or the corresponding encoded data buffer (in the case
of larger ﬁles) which is larger than an assumed leakage threshold.
More details of the security of the PoW scheme can be found in
Appendix B. In case (ii), when a client requests access to f, G
ﬁrst checks if this client is still subscribed to f. Observe that this
veriﬁcation is handled internally by the gateway—without giving
clients the ability to interfere with this process. If access is granted,
the client gets a temporary URL which expires after a short time.
Thus, this information cannot be re-used for further accesses; in
particular, clients that are no longer subscribed to this ﬁle are not
able to access the ﬁle any further.

Notice that also external adversaries cannot acquire access to f
due to reliance on authenticated channels. Namely, we assume that
all exchanged messages are appropriated signed, thus no repudia-
tion of requests will be possible.
Secure Attestation: Let CFID denote the set of clients that are
storing ﬁle f.
Recall that f is referenced with FID ← M TBuf(enc(k,f )) where
k ← H (H∗
(f )x) and Buf denotes the encoding used in PoW. The
key k is deterministically computed from the hash H∗
(f ) of the
ﬁle according to our server-assisted encryption scheme. Due to the
collision resistant hash function used throughout the process, FID
is uniquely bound to f. Therefore, all clients that are registered to
f are likewise expecting the same ID FID.
According to the work ﬂow of ClearBox, each client Ci ∈ CFID
is informed about the size of CFID referencing to the according ﬁle
ID FID. Therefore, G ﬁrst commits to the pair FID,|CFID|. Sub-
sequently, G samples a subset of ﬁles for which it proves the cor-
rectness of the deduplication patterns using the accumulator. For
these ﬁles the gateway G publishes the association between FID
and the digest δ of the corresponding accumulator. This sampling
is enforced by the GetRandomness function which acts as an un-
predictably time-dependent source of randomness. This gives neg-
ligible advantage for G in learning the ﬁles that will be sampled
when committing to |CFID| at the end of the epoch. As the out-
put of GetRandomness can be veriﬁed by the clients, each client
Ci ∈ CFID can check if G has reported δ for the corresponding
FID.

Following from the security of CARDIAC (cf. Appendix A),
G can only prove set membership and cardinality to its clients, if

the underlying set and δ were computed correctly. Recall that δ
constitutes a commitment to the set CFID. By publishing a list of
associations (FID, δ) for the sampled ﬁle IDs, clients can ensure
that G did not split CFID into separate subgroups. We refer the
readers to Appendix A for a security treatment of CARDIAC.

Data Conﬁdentiality: As explained in Section 2, the knowledge of
ﬁles size and user access patterns is essential for G and S to operate
their business. Hence, hiding this information cannot be achieved
in our solution. In the sequel, we analyze the conﬁdentiality of the
stored ﬁles in the presence of curious G and S.

Observe that both, the gateway G and the service provider S,
∗ ← enc(k, f ). Therefore, if the
only see the encrypted ﬁles, i.e., f
underlying encryption scheme is secure, the contents of the ﬁles
are protected against any eavesdropper unless the key k is leaked.
Our server-assisted encryption scheme is an instance of a message-
locked encryption scheme [14]. As the key generation is oblivi-
ous, G does not have any additional advantage when compared to a
standard message-locked encryption adversary. As such, similar to
MLE schemes, ClearBox achieves indistinguishability for unpre-
dictable ﬁles.

Notice that a curious gateway G may guess H∗

(fi) for popular
(predictable) content fi and compute the corresponding key (as he
knows the secret x) in order to identify if this fi is stored by some
clients. A client who stores a low-entropy conﬁdential ﬁle, can pro-
tect against this attack, by appending a high-entropy string so that
the ﬁle cannot be guessed anymore. However, the deduplication of
the ﬁle is no longer possible. ClearBox offers a stronger protec-
tion towards any other entity who does not know the secret value
x, e.g., the service provider S. Recall that G rate-limits client re-
quests for encryption keys, to slow down brute-force search attacks
on predictable ﬁle contents (via the interface of gateway).

4. DEPLOYMENT IN AMAZON S3

AND DROPBOX

In what follows, we evaluate a prototype implementation of

ClearBox using Amazon S3 and Dropbox as a back-end storage.
4.1

Implementation Setup

We implemented a prototype of ClearBox in Java. In our imple-
mentation, we relied on SHA-256, the Java built-in random num-
ber generator, and the JPBC library [7] (based on the PBC crypto-
graphic library [5]) to implement BLS signatures. For a baseline
comparison, we also implemented the server-based deduplication
DupLESS of Bellare et al. [14] and integrated it with Amazon S3.
Recall that, in DupLESS, clients directly interact with the cloud
providers when storing/fetching their ﬁles. To generate keys, Dup-
LESS uses a server-assisted oblivious protocol based on RSA blind
signatures [14]. Notice that we did not implement a plain (unen-
crypted) cloud storage system since the performance of plain stor-
age can be directly interpolated from the line rate (i.e., network
capacity); where appropriate, we will discuss the performance of
ClearBox relative to a plain cloud storage system.

We deployed our implementations on two dual 6-core Intel Xeon
E5-2640 clocked at 2.50GHz with 32GB of RAM, and 100Mbps
network interface card. The ClearBox gateway, and the assisting
server of DupLESS were running on one dual 6-core Xeon E5-
2640 machine, whereas the clients were co-located on the second
dual 6-core Xeon E5-2640 machine; this ensures a fair compari-
son between ClearBox and DupLESS. To emulate a realistic Wide
Area Network (WAN), we relied on NetEm [38] to shape all trafﬁc
exchanged on the networking interfaces following a Pareto distribu-

895 1400

 1200

 1000

 800

 600

 400

 200

]
s
m

[
 
y
c
n
e
a
L

t

 0

 0

ClearBox
DupLESS

 10

]
s
[
 
y
c
n
e
a
L

t

 1

 200  400  600  800  1000  1200

Throughput [op/s]

4

16

ClearBox
DupLESS

64

32
Filesize [MB]

128 256 1024

]
s
[
 
y
c
n
e
a
L

t

 35

 30

 25

 20

 15

 10

 5

 0

4

16

32

64 128 256 512 1024

Filesize [MB]

(a) Key generation performance at G

(b) Overhead of key generation on clients

(c) Construction of PoW

 10

 1

 0.1

 0.01

 0.001

]
s
m

[
 
y
c
n
e
a
L

t

 0.0001

 1

Construction
Proof generation

 4
Number of clients sharing a file

 256  1024  4096

 64

 16

]
s
m

[
 
y
c
n
e
a
L

t

 2000
 1800
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0

 1

 4

 16

 64

 256  1024  4096

Number of files

]
s
u

[
 
y
c
n
e
a
L

t

 35

 30

 25

 20

 15

 10

 5

 0

 1

 4
Number of clients sharing a file

 256  1024  4096

 16

 64

(d) CARDIAC construction and proof gener-
ation

(e) Latency in CARDIAC w.r.t. number of
published ﬁles.

(f) CARDIAC veriﬁcation by clients

Figure 4: Performance evaluation of the building blocks used in ClearBox with respect to a number of parameters.

tion with a mean of 20 ms and a variance of 4 ms (which emulates
the packet delay variance of WANs [24]).

Our implementation interfaces with both Amazon S3 and Drop-
box (respectively), which are used to store the user ﬁles. To acquire
Bitcoin block hashes, our clients invoke an HTTP request to a get-
blockhash tool offered by the Bitcoin block explorer8 [2]. In our
setup, each client invokes an operation in a closed loop, i.e., a client
may have at most one pending operation. We spawned multiple
threads on G’s machine—each thread corresponding to a unique
worker handling requests/bills of a given client. We bounded the
maximum number of threads that can be spawned in parallel to 100.
In our implementation, the gateway and clients store the metadata
information associated to each ﬁle in a local MySQL database.

The gateway leverages the caching feature of MySQL in or-
der to reduce I/O costs. Recall that the Query Caching engine of
MySQL [4] maps the text of the SELECT queries to their results.
For each ﬁle, the gateway stores FID, the size of each ﬁle, and the
IDs of the clients sharing the ﬁle.

Each data point in our plots is averaged over 10 independent
measurements; where appropriate, we include the corresponding
95% conﬁdence intervals. To accurately measure (micro-)latencies,
we made use of the benchmarking tool due to Boyer [19].

4.2 Performance Evaluation

Before evaluating the overall performance of ClearBox, we start
by analyzing the performance of the building blocks with respect
to a number of parameters. Unless otherwise speciﬁed, we rely in
our evaluation on the default parameters listed in Table 2.

8For example,
the hash of Bitcoin block ‘X’ can be ac-
quired by invoking https://blockexplorer.com/q/
getblockhash/X.

Gateway-assisted key generation: In Figure 4(a), we evaluate
the overhead incurred by the oblivious key generation module (cf.
Section 3.2.3) on the gateway. Here, we require that the gateway
handles key generation requests back to back; we then gradually
increase the number of requests in the system (until the through-
put is saturated) and measure the associated latency. Our results
show that our scheme incurs almost 125 ms latency per client key
generation request on the gateway and attains a maximum through-
put of 1185 operations per second; this considerably improves the
maximum throughput of key generation of DupLESS (449 oper-
ations per second). This is mainly due to the fact that BLS sig-
natures are considerably faster to compute by the gateway when
compared to RSA signatures. In contrast, as shown in Figure 4(b),
BLS signatures are more expensive to verify by the clients than
the RSA-variant employed in DupLESS. However, we argue that
the overhead introduced by our scheme compared to DupLESS can
be easily tolerated by clients, as the client effort is dominated by
hashing the ﬁle; for instance, for 16 MB ﬁles, our proposal only in-
curs an additional latency overhead of 213 ms on the clients when
compared to DupLESS.
Proofs of ownership: Figure 4(c) depicts the overhead incurred by
FID required to instantiate the PoW scheme of [27] with respect to
the ﬁle size. As explained in Appendix B, the PoW scheme of [27]

Parameter

Default Value

Default ﬁle size
RSA modulus size

|CFID|

Num. of PoW challenges

Elliptic Curve (BLS)

16 MB
2048 bit

100
50

PBC Library Curve F

Table 2: Default parameters used in evaluation.

896reduces the costs of verifying PoW by encoding the original ﬁle
in a bounded size buffer (64 MB in our case). Our results show
that the computation of FID results in a tolerable overhead on both
the gateway and the clients. For example, the computation of FID
requires 1.07 seconds for a ﬁle with size 16 MB.
CARDIAC: In Figure 4(d), we evaluate the overhead incurred by
the construction of the accumulators and the proof generation in
CARDIAC with respect to |CFID|, i.e., the number of clients sub-
scribed to the same ﬁle f. Our results show that the latency in-
curred by the construction of CARDIAC can be easily tolerated by
G; for instance, the construction of an accumulator for 1000 users
requires 2.34 ms. Clearly, this overhead increases as the number
of users sharing the same ﬁle increases. Notice that once the entire
Merkle tree is constructed, proof generation can be performed con-
siderably faster than the construction of the accumulator. In this
case, G only needs to traverse the tree, and record the sibling paths
for all members of the accumulator.

In Figure 4(e), we evaluate the overhead incurred on G by the
proof generation in CARDIAC with respect to the ﬁles selected for
attestation at the end of each epoch. Our results show that this la-
tency is around 150 ms when less than 100 ﬁles are selected, since
the accumulators of these ﬁles can be handled in parallel by sep-
arate threads in our pool. When the number of selected ﬁles in-
creases beyond our pool threshold (i.e., 100), the latency of proof
generation increases e.g., to reach almost 1 second for 4000 ﬁles.
In Figure 4(f), we evaluate the overhead of the proof veriﬁcation
by the clients. Given that the veriﬁcation only requires (cid:11)log |CFID|(cid:12)
hashes, this operation only incurs marginal overhead on the clients.
For example, the veriﬁcation of membership and cardinality in
CARDIAC when |CFID| = 1000 only requires 27 μs.
ClearBox: In Figure 5(a), we evaluate the latency witnessed by
users in the PUT operation with respect to the ﬁle size. Our results
show that ClearBox achieves comparable performance than Dup-
LESS over S3. For example, when uploading 16 MB ﬁles on Ama-
zon S3, DupLESS incurs a latency of 4.71 seconds, while ClearBox
requires 6.33 seconds. The additional overhead in ClearBox mainly
originates from the computation of FID by the users (cf. Fig-
ure 4(c)); the latency is mainly dominated by the upload of the
ﬁle to Amazon.

In contrast, when users want to upload a ﬁle which is already
stored on the cloud, ClearBox results in faster upload performance
than DupLESS, since users are no longer required to upload the
ﬁle, but have to execute the PoW protocol with G—which incurs
negligible latency when compared to the upload of modest-sized
ﬁles. Recall that this comes at the expense of additional load on
G; in contrast, the server in DupLESS bears no load when users
upload/download ﬁles.

When using Dropbox, recall that clients upload the ﬁle directly to
G, which in turn uploads it to its Dropbox account (since Dropbox
does not provide URL commands for ﬁle creation). Although this
process requires less communication rounds between the clients
and G (to acquire the signed URL), our results show that the la-
tency incurred when uploading un-deduplicated ﬁles in ClearBox
using Dropbox is marginally larger than its Amazon S3 counter-
part; we believe that this discrepancy is due to the lower upload
bandwidth between our clients and G when compared to Amazon
S3. Notice that the performance of uploading deduplicated ﬁles in
ClearBox does not depend on the back-end cloud provider and is
therefore identical for both our Amazon S3 and Dropbox imple-
mentations.

In Figure 5(b), we evaluate the latency witnessed by users in the
GET operation in ClearBox. Our results show that GET operation

incurs comparable latencies in both ClearBox and DupLESS. Re-
call that in ClearBox, clients have to ﬁrst contact G and acquire
the timed GET URL to download resources. Given that the la-
tency of the GET operation is dominated by the download speed,
the overhead incurred by this additional communication round is
marginal. Notice that the latency exhibited by ClearBox users is
tolerable compared to that witnessed in a plain cloud storage sys-
tem. For instance, assuming a 100 Mbps line rate, the download
of a 32MB ﬁle in plain clouds requests almost 3 seconds; on the
other hand, downloading this ﬁle in ClearBox incurs a latency of
10 seconds.

In Figure 5(c), we evaluate the latency incurred on the gateway
in ClearBox with respect to the achieved throughput. Here, we as-
sume that 50% of the requests handled by G correspond to PUT
requests, while the remaining 50% are GET requests. We further
assume that 50% of the upload requests correspond to ﬁles which
are already stored at S. When uploading (un-deduplicated) ﬁles to
S, we do not measure the overhead incurred on G when verifying
FID since this veriﬁcation is asynchronous and can be done by
G at a later point in time. We further simulate a large download
bandwidth at G by emulating client requests from a local socket.
Our ﬁndings show that ClearBox achieves a maximum throughput
of approximately 2138 operations per second when integrated with
Amazon S3. This shows that our solution scales to a large number
of users in the system. When interfacing with Dropbox, the perfor-
mance of ClearBox deteriorates since the load on G considerably
increases in this case;9the maximum throughput exhibited by our
scheme decreases to almost 289 operations per second.
5. RELATED WORK

Data deduplication in cloud storage systems has acquired con-

siderable attention in the literature.

In [28], Harnik et al. describe a number of threats posed by
client-side data deduplication, in which an adversary can learn if a
ﬁle is already stored in a particular cloud by guessing the hashes of
predictable messages. This leakage can be countered using Proofs
of Ownership schemes (PoW) [23, 27], which enable a client to
prove it possesses the ﬁle in its entirety. PoW are inspired by Proofs
of Retrievability and Data Possession (POR/PDP) schemes [11,40],
with the difference that PoW do not have a pre-processing step at
setup time. Halevi et al. [27] propose a PoW construct based on
Merkle trees which incurs low overhead on the server in construct-
ing and verifying PoW. Xu et al. [44] build upon the PoW of [27]
to construct a PoW scheme that supports client-side deduplication
in a bounded leakage setting. Di Pietro and Sorniotti [23] propose a
PoW scheme which reduces the communication complexity of [27]
at the expense of additional server computational overhead. Blasco
et al. [16] propose a PoW based on Bloom ﬁlters which further
reduces the server-side overhead of [23].

Douceur et al. [25] introduced the notion of convergent encryp-
tion, a type of deterministic encryption in which a message is en-
crypted using a key derived from the plaintext itself. Convergent
encryption is not semantically secure [15] and only offers conﬁ-
dentiality for messages whose content is unpredictable. Bellare et
al. [31] proposed DupLESS, a server-aided encryption to perform
data deduplication scheme; here, the encryption key is obliviously
computed based on the hash of the ﬁle and the private key of the as-
sisting server. In [42], Stanek et al. propose an encryption scheme
which guarantees semantic security for unpopular data and weaker
security (using convergent encryption) for popular ﬁles. In [41],
9For comparison purposes, we did not include in our measurements
the overhead introduced by the uploading of ﬁles by G onto S; this
process can be executed at a later point in time by G.

897]
s
[
 
y
c
n
e
a
L

t

 100

 10

 1

4

16

ClearBox (Dropbox)
ClearBox (S3)
DupLESS
ClearBox dedup

64

32
Filesize [MB]

128 256 1024

 1000

]
s
[
 
y
c
n
e
a
L

t

 100

 10

 1

4

16

ClearBox (Dropbox)
ClearBox (S3)
DupLESS

64

32
Filesize [MB]

128 256 1024

]
s
m

[
 
y
c
n
e
a
L

t

 900
 800
 700
 600
 500
 400
 300
 200
 100
 0

 0

 500

S3
Dropbox

 1000

Throughput [op/s]

 1500

 2000

 2500

(a) PUT Performance

(b) GET Performance

(c) Latency vs. throughput

Figure 5: Performance evaluation of ClearBox using Amazon S3 and Dropbox as back-end cloud storage.

Soriente et al. proposed a solution which distributively enforces
shared ownership in agnostic clouds. This solution can be used in
conjunction with ClearBox to enable users to distributively manage
the access control of their deduplicated ﬁles.

Several proposals for accumulating hidden sets exist such as the
original RSA-based construction [12], which has been extended
to dynamic accumulators [21], and non-membership proofs [32].
There exist as well constructions based on bilinear groups [22,
39] and on hash-functions [20, 33]. A related concept are zero-
knowledge sets [30, 37]. These structures provide proofs of set
membership. Notice, however, that cryptographic accumulators do
not typically provide information about the accumulated set, such
as content or cardinality.

Our attacker model shares similarities with the one considered
in [43], where the cloud provider is assumed to be economically ra-
tional. This scheme relies on an hourglass function—to verify that
the cloud provider stores the ﬁles in encrypted form—which im-
poses signiﬁcant constraints on a rational cloud provider that tries
to apply the hourglass functions on demand.
6. CONCLUSION

In this paper, we proposed ClearBox, which enables a cloud
provider to transparently attest to its clients the deduplication pat-
terns of their stored data. ClearBox additionally enforces ﬁne-
grained access control over deduplicated ﬁles, supports data con-
ﬁdentiality, and resists against malicious users. Evaluation results
derived from a prototype implementation of ClearBox show that
our proposal scales well with the number of users and ﬁles in the
system.

As far as we are aware, ClearBox is the ﬁrst complete system
which enables users to verify the storage savings exhibited by their
data. We argue that ClearBox motivates a novel cloud pricing
model which promises a fairer allocation of storage costs amongst
users—without compromising data conﬁdentiality nor system per-
formance. We believe that such a model provides strong incentives
for users to store popular data in the cloud (since popular data will
be cheaper to store) and discourages the upload of personal and
unique content. As a by-product, the popularity of ﬁles addition-
ally gives an indication to cloud users on their level of privacy in
the cloud; for example, the user can verify that his private ﬁles are
not deduplicated—and thus have not been leaked.
Acknowledgements
The authors would like to thank Nikos Triandopoulos and the anony-
mous reviewers for their valuable feedback and comments. This
work was partly supported by the TREDISEC project (G.A. no
644412), funded by the European Union (EU) under the Informa-

tion and Communication Technologies (ICT) theme of the Horizon
2020 (H2020) research and innovation programme.

7. REFERENCES

[1] Amazon S3 Pricing.

http://aws.amazon.com/s3/pricing/.

[2] Bitcoin real-time stats and tools.

http://blockexplorer.com/q.

[3] Google Cloud Storage.

https://cloud.google.com/storage/.

[4] The MySQL Query Cache. http://dev.mysql.com/

doc/refman/5.1/en/query-cache.html.

[5] PBC Library. http://crypto.stanford.edu/pbc/,

2007.

[6] Cloud Market Will More Than Triple by 2014, Reaching

$150 Billion.
http://www.msptoday.com/topics/msp-
today/articles/364312-cloud-market-will-
more-than-triple-2014-reaching.htm, 2013.

[7] JPBC:Java Pairing-Based Cryptography Library.

http://gas.dia.unisa.it/projects/jpbc/#.
U3HBFfna5cY, 2013.

[8] Bitcoin as a public source of randomness.

https://docs.google.com/presentation/d/
1VWHm4Moza2znhXSOJ8FacfNK2B_
vxnfbdZgC5EpeXFE/view?pli=1#slide=id.
g3934beb89_034, 2014.

[9] These are the cheapest cloud storage providers right now.

http://qz.com/256824/these-are-the-
cheapest-cloud-storage-providers-right-
now/, 2014.

[10] ARMKNECHT, F., BOHLI, J., KARAME, G. O., LIU, Z.,

AND REUTER, C. A. Outsourced proofs of retrievability. In
Proceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security, Scottsdale, AZ,
USA, November 3-7, 2014 (2014), pp. 831–843.

[11] ATENIESE, G., BURNS, R. C., CURTMOLA, R., HERRING,

J., KISSNER, L., PETERSON, Z. N. J., AND SONG, D. X.
Provable data possession at untrusted stores. In ACM
Conference on Computer and Communications Security
(2007), pp. 598–609.

[12] BARIC, N., AND PFITZMANN, B. Collision-free

accumulators and fail-stop signature schemes without trees.
In EUROCRYPT (1997), W. Fumy, Ed., vol. 1233 of Lecture
Notes in Computer Science, Springer, pp. 480–494.

898[13] BELLARE, M., AND KEELVEEDHI, S. Interactive

message-locked encryption and secure deduplication. In
Public-Key Cryptography - PKC 2015 - 18th IACR
International Conference on Practice and Theory in
Public-Key Cryptography, Gaithersburg, MD, USA, March
30 - April 1, 2015, Proceedings (2015), J. Katz, Ed.,
vol. 9020 of Lecture Notes in Computer Science, Springer,
pp. 516–538.

[14] BELLARE, M., KEELVEEDHI, S., AND RISTENPART, T.

DupLESS: Server-aided encryption for deduplicated storage.
In Proceedings of the 22Nd USENIX Conference on Security
(Berkeley, CA, USA, 2013), SEC’13, USENIX Association,
pp. 179–194.

[15] BELLARE, M., KEELVEEDHI, S., AND RISTENPART, T.
Message-locked encryption and secure deduplication. In
Advances in Cryptology - EUROCRYPT 2013, 32nd Annual
International Conference on the Theory and Applications of
Cryptographic Techniques, Athens, Greece, May 26-30,
2013. Proceedings (2013), T. Johansson and P. Q. Nguyen,
Eds., vol. 7881 of Lecture Notes in Computer Science,
Springer, pp. 296–312.

[16] BLASCO, J., DI PIETRO, R., ORFILA, A., AND

SORNIOTTI, A. A tunable proof of ownership scheme for
deduplication using bloom ﬁlters. In Communications and
Network Security (CNS), 2014 IEEE Conference on (Oct
2014), pp. 481–489.

[17] BOLDYREVA, A. Efﬁcient threshold signature,

multisignature and blind signature schemes based on the
gap-difﬁe-hellman-group signature scheme.

[18] BONEH, D., LYNN, B., AND SHACHAM, H. Short

signatures from the weil pairing. J. Cryptology 17, 4 (2004),
297–319.

[19] BRENT BOYER. Robust Java benchmarking. http:

//www.ibm.com/developerworks/library/j-
benchmark2/j\-benchmark2\-pdf.pdf.

[20] BULDAS, A., LAUD, P., AND LIPMAA, H. Eliminating

counterevidence with applications to accountable certiﬁcate
management. Journal of Computer Security 10, 3 (2002),
273–296.

[21] CAMENISCH, J., AND LYSYANSKAYA, A. Dynamic

accumulators and application to efﬁcient revocation of
anonymous credentials. In Advances in Cryptology -
CRYPTO 2002 (2002), Springer, pp. 61–76.

[22] DAMGÅRD, I., AND TRIANDOPOULOS, N. Supporting
non-membership proofs with bilinear-map accumulators.
IACR Cryptology ePrint Archive 2008 (2008), 538.

[23] DI PIETRO, R., AND SORNIOTTI, A. Boosting efﬁciency

and security in proof of ownership for deduplication. In
Proceedings of the 7th ACM Symposium on Information,
Computer and Communications Security (New York, NY,
USA, 2012), ASIACCS ’12, ACM, pp. 81–82.

[24] DOBRE, D., KARAME, G., LI, W., MAJUNTKE, M., SURI,

N., AND VUKOLI ´C, M. Powerstore: Proofs of writing for
efﬁcient and robust storage. In Proceedings of the 2013 ACM
SIGSAC Conference on Computer &#38; Communications
Security (New York, NY, USA, 2013), CCS ’13, ACM,
pp. 285–298.

[25] DOUCEUR, J. R., ADYA, A., BOLOSKY, W. J., SIMON, D.,
AND THEIMER, M. Reclaiming space from duplicate ﬁles in
a serverless distributed ﬁle system. In ICDCS (2002),
pp. 617–624.

[26] FIAT, A., AND SHAMIR, A. How to prove yourself:

Practical solutions to identiﬁcation and signature problems.
In Proceedings on Advances in cryptology—CRYPTO ’86
(London, UK, UK, 1987), Springer-Verlag, pp. 186–194.

[27] HALEVI, S., HARNIK, D., PINKAS, B., AND

SHULMAN-PELEG, A. Proofs of ownership in remote
storage systems. In Proceedings of the 18th ACM Conference
on Computer and Communications Security (New York, NY,
USA, 2011), CCS ’11, ACM, pp. 491–500.

[28] HARNIK, D., PINKAS, B., AND SHULMAN-PELEG, A. Side

channels in cloud services: Deduplication in cloud storage.
IEEE Security & Privacy 8, 6 (2010), 40–47.

[29] KARAME, G. O., ANDROULAKI, E., AND CAPKUN, S.

Double-spending fast payments in bitcoin. In Proceedings of
the 2012 ACM conference on Computer and communications
security (New York, NY, USA, 2012), CCS ’12, ACM,
pp. 906–917.

[30] KATE, A., ZAVERUCHA, G. M., AND GOLDBERG, I.

Constant-size commitments to polynomials and their
applications. In Advances in Cryptology-ASIACRYPT 2010.
Springer, 2010, pp. 177–194.

[31] KEELVEEDHI, S., BELLARE, M., AND RISTENPART, T.

DupLESS: Server-aided encryption for deduplicated storage.
In Presented as part of the 22nd USENIX Security
Symposium (USENIX Security 13) (Washington, D.C., 2013),
USENIX, pp. 179–194.

[32] LI, J., LI, N., AND XUE, R. Universal accumulators with
efﬁcient nonmembership proofs. In Applied Cryptography
and Network Security, 5th International Conference, ACNS
2007, Zhuhai, China, June 5-8, 2007, Proceedings (2007),
pp. 253–269.

[33] LIPMAA, H. Secure accumulators from euclidean rings

without trusted setup. In Applied Cryptography and Network
Security - 10th International Conference, ACNS 2012,
Singapore, June 26-29, 2012. Proceedings (2012),
pp. 224–240.

[34] LIU, S., HUANG, X., FU, H., AND YANG, G.

Understanding data characteristics and access patterns in a
cloud storage system. In 13th IEEE/ACM International
Symposium on Cluster, Cloud, and Grid Computing, CCGrid
2013, Delft, Netherlands, May 13-16, 2013 (2013),
pp. 327–334.

[35] MEYER, D. T., AND BOLOSKY, W. J. A study of practical

deduplication. In Proceedings of the 9th USENIX Conference
on File and Stroage Technologies (Berkeley, CA, USA,
2011), FAST’11, USENIX Association, pp. 1–1.

[36] MEYER, D. T., AND BOLOSKY, W. J. A study of practical
deduplication. Trans. Storage 7, 4 (Feb. 2012), 14:1–14:20.
[37] MICALI, S., RABIN, M., AND KILIAN, J. Zero-knowledge

sets. In Foundations of Computer Science, 2003.
Proceedings. 44th Annual IEEE Symposium on (2003),
IEEE, pp. 80–91.

[38] NETEM. NetEm, the Linux Foundation. Website, 2009.

Available online at
http://www.linuxfoundation.org/
collaborate/workgroups/networking/netem.

[39] NGUYEN, L. Accumulators from bilinear pairings and

applications. In Topics in Cryptology - CT-RSA 2005, The
Cryptographers’ Track at the RSA Conference 2005, San
Francisco, CA, USA, February 14-18, 2005, Proceedings
(2005), pp. 275–292.

899[40] SHACHAM, H., AND WATERS, B. Compact Proofs of

Retrievability. In ASIACRYPT (2008), pp. 90–107.
[41] SORIENTE, C., KARAME, G. O., RITZDORF, H.,

MARINOVIC, S., AND CAPKUN, S. Commune: Shared
ownership in an agnostic cloud. In Proceedings of the 20th
ACM Symposium on Access Control Models and
Technologies, Vienna, Austria, June 1-3, 2015 (2015),
pp. 39–50.

[42] STANEK, J., SORNIOTTI, A., ANDROULAKI, E., AND

KENCL, L. A secure data deduplication scheme for cloud
storage. In Financial Cryptography and Data Security - 18th
International Conference, FC 2014, Christ Church,
Barbados, March 3-7, 2014, Revised Selected Papers (2014),
pp. 99–118.

[43] VAN DIJK, M., JUELS, A., OPREA, A., RIVEST, R. L.,

STEFANOV, E., AND TRIANDOPOULOS, N. Hourglass
schemes: How to prove that cloud ﬁles are encrypted. In
Proceedings of the 2012 ACM Conference on Computer and
Communications Security (New York, NY, USA, 2012), CCS
’12, ACM, pp. 265–280.

[44] XU, J., CHANG, E.-C., AND ZHOU, J. Weak

leakage-resilient client-side deduplication of encrypted data
in cloud storage. In Proceedings of the 8th ACM SIGSAC
Symposium on Information, Computer and Communications
Security (New York, NY, USA, 2013), ASIA CCS ’13,
ACM, pp. 195–206.

APPENDIX
A. SECURITY ANALYSIS OF CARDIAC
In what follows, we analyze the security of CARDIAC. Let a set
X be given and the accumulated digest for the set is δ ← Acc(X).
Recall that δ = H(a(cid:2),0, (cid:3)) (with H being a cryptographically se-
cure hash function) acts as a commitment to the root a(cid:2),0 and the
height (cid:3) of a Merkle tree. For the purpose of our analysis, we dis-
tinguish between two security goals: (i) proving membership using
ProveM and VerifyM and (ii) proving an upper bound on |X| us-
ing ProveC and VerifyC. As the security of Merkle trees is well
understood with respect to the ﬁrst goal [20], we focus in the sequel
on analyzing the second goal.

Since (cid:3) is veriﬁed (indirectly) using the length of the sibling path
and assuming that the hash function is secure, CARDIAC ensures
that the Merkle tree can encode at most 2(cid:2) elements. ProveC out-
puts the sibling path of the last set element a0,|X|−1. The veriﬁ-
cation algorithm VerifyC requires to check the values of all open
nodes in the sibling path. Recall that the open nodes are those nodes
that depend only on the zero leaves and hence represent publicly
known values. In fact, this veriﬁcation step constitutes member-
ship proofs for every single zero leaf a0,j for j = |X|, . . . ,2 (cid:2) − 1.
In turn, this ensures that at least 2(cid:2)−|X| leaves of the tree are zero.
A direct consequence is that at most |X| leaves can be non-zero,
giving an upper bound on X. Observe that the proof assumes that
further leaves located on the left of the last non-zero element are
not set to zero. This assumption can be safely made since G has
no incentives to place more zero-leaves (since this means that less
users are registered than claimed and would pay too little to G).

Input: A ﬁle f of length M bit, broken into m = (cid:11) M
length 512 bit.

512

(cid:12) blocks of

Initialize positions:

• Compute bit-length (cid:3) = min{20,(cid:11)log2 m(cid:12)}, being close to
M, but at most 64 MB, i.e. 220 blocks with 512 bits.
• Initialize the buffer Buf with 2(cid:2) blocks of 512 bits.
• Initialize a table ptr of m rows and 4 columns of (cid:3) bits
which holds pointers into the buffer Buf. A temporary value
IV will be initialized with SHA-256’ IV .

Reduction phase:
For each i ∈ [m]:

1. Update IV := SHA-256 (IV ; File[i]), where File[i] de-

notes the i-th block of the ﬁle.
Initialize ptr [i] = trunc4(cid:2)(IV ) with IV truncated to 4(cid:3) bits..

2. For j = 0 : 3, do
3.
4.

Block = Cyclic Shift of F ile[i] by j ∗ 128 bits
XOR Block into location ptr [i][j] in Buf

Mixing phase:
Repeat 5 times:

1. For each block i ∈ [(cid:3)], Forj = 0 : 3, do
2.
3.

Block = Cyclic Shift of Buf[i] by j ∗ 128 bits
If i (cid:9)= ptr [i mod 2(cid:2)][j] then XOR Block into location

ptr [i mod 2(cid:2)][j] in Buf

Tree construction: The ﬁnal buffer content is denoted by Buf(f ).
Construct a Merkle tree using the blocks of Buf(f ) as the leaves.

Figure 6: Detailed PoW Protocol of [27].

B. POW BASED ON [24]

In Figure 6, we detail the Proof of Ownership (PoW) due to
Halevi et al. [27]. The protocol is divided into three phases: in
the ﬁrst phase, the ﬁle f is reduced into a buffer with maximum 64
MB in size. Then, the contents of the buffer are pseudo-randomly
mixed and ﬁnally, the Merkle tree of the buffer is computed. The
ﬁrst two phases can be seen as applying a linear code to the ﬁle, and
outputting a buffer Buf(f ). Subsequently, to verify a PoW, the ver-
iﬁer asks for the sibling paths of a random number of leaves from
Buf(f ) and checks that the authentication path matches the root of
the Merkle tree.

The buffer is an encoding of the ﬁle with a random linear code.
The code is generated using SHA-256 to generate an array of pointer
ptr. Due to the collision resistance of the hash function, any modi-
ﬁcation of the ﬁle will therefore lead to a different code. Under the
assumption that the code has a minimum distance of L/3, where L
is the number of blocks in the buffer, any two valid buffers will be
different in at least L/3 positions. Thus, the prover will only suc-
ceed with a probability of (2/3)t in answering t challenges without
knowing the correct buffer. For example, when t = 20 challenges
−12. Note that with-
are made, the probability of success is at most 2
out knowing the ﬁle in its entirety, two different random codes are
derived in the ﬁrst two steps due to the collision-resistance of the
deployed hash function. In particular, it is highly unlikely that any
block in the buffer can be predicted. We refer the readers to [27]
for more details on the security of this construct.

900