Emil Stefanov
UC Berkeley

emil@cs.berkeley.edu

Abstract

We take an important step forward in making Obliv-
ious RAM (O-RAM) practical. We propose an O-RAM
construction achieving an amortized overhead of 20 ∼
35X (for an O-RAM roughly 1 terabyte in size), about
63 times faster than the best existing scheme. On the
theoretic front, we propose a fundamentally novel tech-
nique for constructing Oblivious RAMs: speciﬁcally, we
partition a bigger O-RAM into smaller O-RAMs, and
employ a background eviction technique to obliviously
evict blocks from the client-side cache into a randomly
assigned server-side partition. This novel technique is
the key to achieving the gains in practical performance.

1

Introduction

As cloud computing gains momentum, an increasing
amount of data is outsourced to cloud storage, and data
privacy has become an important concern for many busi-
nesses and individuals alike. Encryption alone may not
sufﬁce for ensuring data privacy, as data access patterns
can leak a considerable amount of information about the
data as well. Pinkas et al. gave an example [13]: if a
sequence of data access requests q1, q2, q3 is always fol-
lowed by a stock exchange operation, the server can gain
sensitive information even when the data is encrypted.

Oblivious RAM (or O-RAM) [3, 4, 11], ﬁrst inves-
tigated by Goldreich and Ostrovsky, is a primitive in-
tended for hiding storage access patterns. The problem
was initially studied in the context of software protec-
tion, i.e., hiding a program’s memory access patterns to
prevent reverse engineering.

With the trend of cloud computing, O-RAM also
has important applications in privacy-preserving storage
outsourcing applications. In this paper, we consider the
setting where a client wishes to store N blocks each of
size B bytes at an untrusted server.

The community’s interest in O-RAM has recently
rekindled, partly due to its potential high impact
in privacy-preserving storage outsourcing applications.

Towards Practical Oblivious RAM

Elaine Shi

UC Berkeley/PARC

elaines@cs.berkeley.edu

Dawn Song
UC Berkeley

dawnsong@cs.berkeley.edu

One of the best schemes known to date is a novel con-
struction recently proposed by Goodrich and Mitzen-
macher [5]. Speciﬁcally, let N denotes the total stor-
age capacity of the O-RAM in terms of the num-
ber of blocks.
The Goodrich-Mitzenmacher con-
struction achieves O((log N )2) amortized cost when
parametrized with O(1) client-side storage; or
it
achieves O(log N ) amortized cost when parametrized
with O(N a) client-side storage where 0 < a < 1. In
this context, an amortized cost of f (N ) means that each
data request will generate f (N ) read or write operations
on the server.

Despite elegant asymptotic guarantees, the practi-
cal performance of existing O-RAM constructions are
still unsatisfactory. As shown in Table 1, one of the
most practical schemes known to date is the construction
by Goodrich and Mitzenmacher [5] when parametrized
with N a (a < 1) client-side storage. This scheme has
more than 1, 400X overhead compared to non-oblivious
storage under reasonable parametrization, which is pro-
hibitive in practice. In summary, although it has been
nearly two decades since O-RAM was ﬁrst invented, so
far, it has mostly remained a theoretical concept.

1.1 Results and Contributions

Our main goal is to make Oblivious RAM practical

for cloud outsourcing applications.
Practical construction. We propose an O-RAM con-
struction geared towards optimal practical performance.
The practical construction achieves an amortized over-
head of 20 ∼ 35X (Tables 1 and 2), about 63 times
faster than the best known construction.
In addi-
tion, this practical construction also achieves sub-linear
O(log N ) worst-case cost, and constant round-trip la-
tency per operation. Although our practical construc-
tion requires asymptotically linear amount of client-side
storage, the constant is so small (0.01% to 0.3% of the
O-RAM capacity) that in realistic settings, the amount
of client-side storage is comparable to
Theoretical construction. By applying recursion to
the practical construction, we can reduce the client-side

√

N.

Scheme

Amortized Cost

Worst-case Cost

Client Storage

Server Storage

Goldreich-Ostrovsky [4]

Pinkas-Reinman [13]

O((log N )3)
O((log N )2)

Ω(N )

O(N log N )

Goodrich-Mitzenmacher [5]

O(log N )

Practical, Non-Concurrent

Practical, Concurrent

Theoretic, Non-Concurrent

Theoretic, Concurrent

O(log N )
O(log N )
O((log N )2)
O((log N )2)

O(N log N )
√
√

This paper:

< 3

N )

N + o(
O(log N )
N )
O(

√

O((log N )2)

O(1)
O(1)
O(N a)

(0 < a < 1)

O(N log N )

8N

8N

cN (c very small) < 4N + o(N )
cN (c very small) < 4N + o(N )

O(
O(

N )
N )

O(N )
O(N )

√
√

Practical

Performance
> 120, 000X

60, 000 ∼ 80, 000X

> 1, 400X

20 ∼ 35X
20 ∼ 35X

—
—

Table 1: Our contributions.
The practical performance is the number of client-server operations per O-RAM operation for
typical realistic parameters, e.g., when the server stores terabytes of data, and the client has several hundred megabytes to gigabytes
of local storage, and N ≥ 220. For our theoretic constructions, the same asymptotic bounds also work for the more general case
where client-side storage is N a for some constant 0 < a < 1.

O-RAM Capacity

# Blocks

Block Size

Client Storage

Server Storage

Client Storage

O-RAM Capacity

Practical

Performance

64 GB
256 GB
1 TB
16 TB
256 TB
1024 TB

220
222
224
228
232
234

64 KB
64 KB
64 KB
64 KB
64 KB
64 KB

204 MB
415 MB
858 MB
4.2 GB
31 GB
101 GB

205 GB
819 GB
3.2 TB
51 TB
819 TB
3072 TB

0.297%
0.151%
0.078%
0.024%
0.011%
0.009%

22.5X
24.1X
25.9X
29.5X
32.7X
34.4X

Table 2: Suggested parametrizations of our practical construction. The practical performance is the number of client-server opera-
tions per O-RAM operation as measured by our simulation experiments.
√
O(

√

storage to a sublinear amount, and obtain a novel con-
struction of theoretic interest, achieving O((log N )2)
amortized and worst-case cost, and requires O(
N )
client-side storage, and O(N ) server-side storage. Note
that in the O((log N )2) asymptotic notation, one of the
log N factors stems from the the depth of the recursion;
and in realistic settings (see Table 2), the depth of the
recursion is typically 2 or 3.

√

Table 1 summarizes our contributions in the con-
Table 2 provides suggested

text of related work.
parametrizations for our practical construction.

1.2 Main Technique: Partitioning

We propose a novel partitioning technique, which is
the key to achieving the claimed theoretical bounds as
well as major practical savings. The basic idea is to par-
tition a single O-RAM of size N blocks into P different
O-RAMs of size roughly N
P blocks each. This allows
us to break down a bigger O-RAM into multiple smaller
O-RAMs.

The idea of partitioning is motivated by the fact
that the major source of overhead in existing O-RAM
constructions arises from an expensive remote oblivi-
ous sorting protocol performed between the client and
the server. Because the oblivious sorting protocol can
take up to O(N ) time, existing O-RAM schemes re-
quire Ω(N ) time in the worst-case or have unreasonable

N ) amortized cost.
√

√

the client can use

We partition the O-RAM into roughly P =

N par-
titions, each having
N blocks approximately. This
way,
N blocks of storage to
sort/reshufﬂe the data blocks locally, and then simply
transfer the reshufﬂed data blocks to the server. This not
only circumvents the need for the expensive oblivious
sorting protocol, but also allows us to achieve O(
N )
worst-case cost. Furthermore, by allowing reshufﬂing
to happen concurrently with reads, we can further re-
duce the worst-case cost of the practical construction to
O(log N ).

√

While the idea of partitioning is attractive, it also
brings along an important challenge in terms of security.
Partitioning creates an extra channel through which the
data access pattern can potentially be inferred by observ-
ing the sequence of partitions accessed. Therefore, we
must take care to ensure that the sequence of partitions
accessed does not leak information about the identities
of blocks being accessed. Speciﬁcally, our construction
ensures that the sequence of partitions accessed appears
pseudo-random to an untrusted server.

It is worth noting that Ostrovsky and Shoup [12]
also came up with a technique to spread the reshufﬂing
work of the hierarchical solution [4] over time, thereby
achieving poly-logarithmic worst-case cost. However,
our technique of achieving poly-logarithmic worst-case
cost is fundamentally from Ostrovsky and Shoup [12].

Moreover, our partitioning and background eviction
techniques are also key to the practical performance gain
that we can achieve.

1.3 Related Work

Oblivious RAM was ﬁrst investigated by Goldreich
and Ostrovsky [3, 4, 11]. Since their original work, sev-
eral seminal improvements have been proposed [5, 13,
15, 16]. These approaches mainly fall into two broad
categories: constructions that use O(1) client-side stor-
age, and constructions that use O(N a) client-side stor-
age where 0 < a < 1.

√

Williams and Sion [15] propose an O-RAM con-
N ) client-side storage, and
struction that requires O(
achieves an amortized cost of O((log N )2). Williams
et al. propose another construction that uses O(
N )
client-side storage, and achieves O(log N log log N )
amortized cost [16]; however, researchers have ex-
pressed concerns over the assumptions used in their
original analysis [5, 13]. A corrected analysis of this
construction can be found in an appendix in a recent
work by Pinkas and Reinman [13].

√

Pinkas and Reinman [13] discovered an O-RAM con-
struction that achieves O((log N )2) overhead with O(1)
client-side storage. However, some researchers have ob-
served a security ﬂaw of the Pinkas-Reinman construc-
tion, due to the fact that the lookups can reveal, with
considerable probability, whether the client is searching
for blocks that exist in the hash table [5]. The authors
of that paper will ﬁx this issue in a future journal ver-
sion. While Table 1 shows the overhead of the Pinkas-
Reinman scheme as is, the overhead of the scheme is
likely to increase after ﬁxing this security ﬂaw.

In an elegant work by Goodrich and Mitzen-
macher [5], they proposed a novel O-RAM construc-
tion which achieves O((log N )2) amortized cost with
O(1) client-side storage; or O(log N ) amortized cost
with O(N a) client-side storage where 0 < a < 1.
The Goodrich-Mitzenmacher construction achieves the
best asymptotic performance among all known construc-
√
tions. However, their practical performance is still pro-
hibitive. For example, with O(
N ) client-side storage,
their amortized cost is > 1, 400X from a very conserva-
tive estimate. In reality, their overhead could be higher.
In an independent and concurrent work by Boneh,
Mazieres, and Popa [2], they propose a construction that
√
N ) reads while shufﬂing (us-
can support up to O(
√
N ) client-side storage). The scheme achieves
ing O(
O(log N ) online cost, and O(

N ) amortized cost.

√

Almost all prior constructions have Ω(N ) worst-
case cost, except the seminal work by Ostrovsky and
Shoup [12], in which they demonstrate how to spread
the reshufﬂing operations of the hierarchical construc-

√

tion [4] across time to achieve poly-logarithmic worst-
case cost. While the aforementioned concurrent work
by Boneh et al. [2] alleviates this problem by separating
the cost into an online part for reading and writing data,
√
and an ofﬂine part for reshufﬂing, they do so at an in-
√
N ) (when their scheme
creased amortized cost of O(
N ) client-side storage). In ad-
is conﬁgured with O(
dition, if Ω(
N ) consecutive requests take place within
a small time window (e.g., during peak usage times),
their scheme can still block on a reshufﬂing operation of
Ω(N ) cost.
Concurrent and subsequent work.
In concur-
rent/subsequent work, Goodrich et al. [6] invented an
O-RAM scheme achieving O((log N )2) worst-case cost
with O(1) memory; and and Kushilevitz et al. [9]
invented a scheme with O( (log N )2
log log N ) worst-case cost.
Goodrich et al. also came up with a stateless Obliv-
ious RAM [7] scheme, with O(log N ) amortized cost
and O(N a) (0 < a < 1) client-side transient (as op-
posed to permanent) buffers. Due to larger constants in
their constructions, our construction is two to three or-
ders of magnitude more efﬁcient in realistic settings.

2 Problem Deﬁnition

As shown in Figure 1, we consider a client that
wishes to store data at a remote untrusted server while
preserving its privacy. While traditional encryption
schemes can provide conﬁdentiality, they do not hide
the data access pattern which can reveal very sensitive
information to the untrusted server. We assume that the
server is untrusted, and the client is trusted, including
the client’s CPU and memory hierarchy (including RAM
and disk).

The goal of O-RAM is to completely hide the data ac-
cess pattern (which blocks were read/written) from the
server. In other words, each data read or write request
will generate a completely random sequence of data ac-
cesses from the server’s perspective.
Notations. We assume that data is fetched and stored in
atomic units, referred to as blocks, of size B bytes each.
For example, a typical value for B for cloud storage is
64 KB to 256 KB. Throughout the paper, we use the
notation N to denote total number of data blocks that
the O-RAM can support, also referred to as the capacity
of the O-RAM.
Practical considerations. One of our goals is to design
a practical O-RAM scheme in realistic settings. We ob-
serve that bandwidth is much more costly than computa-
tion and storage in real-world scenarios. For example,
typical off-the-shelf PCs and laptops today have giga-
bytes of RAM, and several hundred gigabytes of disk

storage. When deploying O-RAM in a realistic setting,
it is very likely that the bottleneck is network bandwidth
and latency.

As a result, our practical O-RAM construction lever-
ages available client-side storage as a working buffer,
and this allows us to drastically optimize the bandwidth
consumption between the server and the client. As a typ-
ical scenario, we assume that the client wishes to store
terabytes of data on the remote server, and the client has
megabytes to gigabytes of storage (in the form of RAM
or disk). We wish to design a scheme in which the client
can maximally leverage its local storage to reduce the
overhead of O-RAM.
Security deﬁnitions. We adopt the standard security
deﬁnition for O-RAMs.
Intuitively, the security deﬁ-
nition requires that the server learns nothing about the
access pattern. In other words, no information should be
leaked about: 1) which data is being accessed; 2) how
old it is (when it was last accessed); 3) whether the same
data is being accessed (linkability); 4) access pattern
(sequential, random, etc); or 5) whether the access is
a read or a write. Like previous work, our O-RAM con-
structions do not consider information leakage through
the timing channel, such as when or how frequently the
client makes data requests.
Deﬁnition 1
((op1, u1, data1), (op2, u2, data2), ..., (opM , uM , dataM ))
denote a data request sequence of length M, where each
opi denotes a read(ui) or a write(ui, data) operation.
Speciﬁcally, ui denotes the identiﬁer of the block being
read or written, and datai denotes the data being
written. Let A((cid:126)y) denote the (possibly randomized)
sequence of accesses to the remote storage given the
sequence of data requests (cid:126)y. An O-RAM construction is
said to be secure if for any two data request sequences (cid:126)y
and (cid:126)z of the same length, their access patterns A((cid:126)y) and
A((cid:126)z) are computationally indistinguishable by anyone
but the client.

deﬁnition). Let (cid:126)y

(Security

:=

3 The Partitioning Framework

In this section, we describe our main technique, parti-
tioning, as a framework. At a high level, the goal of par-
titioning is to subdivide the O-RAM into much smaller
partitions, so that the operations performed on the par-
titions can be handled much more efﬁciently than if the
O-RAM was not partitioned.

The main challenge of partitioning the O-RAM is
to ensure that the sequence of partitions accessed dur-
ing the lifetime of the O-RAM appears random to the
untrusted server while keeping the client-side storage
small. In this way, no information about data access pat-
tern is revealed.

3.1 Server Storage

We divide the server’s storage into P fully functional
partition O-RAM’s, each containing N/P blocks on av-
erage. For now, we can think of each partition O-RAM
as a black box, exporting a read and a write operation,
while hiding the access patterns within that partition.

At any point of time, each block is randomly assigned
to any of the P partitions. Whenever a block is accessed,
the block is logically removed from its current partition
(although a stale copy of the block may remain), and
logically assigned to a fresh random partition selected
from all P partitions. Thus, the client needs to keep
track of which partition each block is associated with at
any point of time, as speciﬁed in Section 3.2.

The maximum amount of blocks that an O-RAM can
contain is referred to as the capacity of the O-RAM.
In our partitioning framework, blocks are randomly as-
signed to partitions, so the capacity of an O-RAM parti-
tion has to be slightly more than N/P blocks to accom-
√
modate the variance of assignments. Due to the standard
√
N, each partition
balls and bins analysis, for P =
needs to have capacity
N ) to have a sufﬁ-
N + o(
poly(N ).
ciently small failure probability
1

√

3.2 Client Storage

The client storage is divided into the following com-

ponents.
Data cache with P slots. The data cache is a cache for
temporarily storing data blocks fetched from the server.
There are exactly P cache slots, equal to the number of
partitions on the server. Logically, the P cache slots can
be thought of an extension to the server-side partitions.
Each slot can store 0, 1, or multiple blocks. In the full
version of this paper [1], we prove that each cache slot
will have a constant number of data blocks in expecta-
tion, and that the total number of data blocks in all cache
slots will be bounded by O(P ) with high probability . In
√
both our theoretic and practical constructions, we will let
√
N. In this case, the client’s data cache capacity
P =
N ).
is O(
Position map. As mentioned earlier, the client needs to
keep track of which partition (or cache slot) each block
resides in. The position map serves exactly this purpose.
We use the notation position[u] for the partition number
where block u currently resides. In our practical con-
struction described in Section 4, the position map is ex-
tended to also contain the exact location (level number
and index within the level) of block u within its current
partition.

Intuitively, each block’s position (i.e., partition num-
ber) requires about c log N bits to describe. In our prac-

Figure 1: Oblivious RAM system architecture.

Figure 2: The partitioning framework.

8·B

tical construction, c ≤ 1.1, since the practical construc-
tion also stores the block’s exact location inside a parti-
tion. Hence, the position map requires at most cN log N
8 N log N bytes, which is cN log N
bits of storage, or c
blocks. Since in practice the block size B > c
8 log N,
the size of the position map is a constant fraction of the
original capacity of the O-RAM (with a very small con-
stant).
Shufﬂing buffer. The shufﬂing buffer is used for the
shufﬂing operation when two or more levels inside a par-
tition O-RAM need to be merged into the next level. For
√
this paper, we assume that the shufﬂing buffer has size
O(
Miscellaneous. Finally, we need some client-side stor-
age to store miscellaneous states and information, such
as cryptographic keys for authentication, encryption,
and pseudo-random permutations.

N ).

3.3

Intuition

In our construction, regardless of whether a block is
found in the client’s data cache, the client always per-
forms a read and a write operation to the server upon
every data request – with a dummy read operation in
case of a cache hit. Otherwise, the server might be able
to infer the age of the blocks being accessed. Therefore,
the client data cache is required for security rather than
for efﬁciency.

In some sense, the data cache acts like a holding
buffer. When the client fetches a block from the server, it
cannot immediately write the block back to a some par-
tition, since this would result in linkability attacks the
next the this block is read. Instead, the fetched block is
associated with a fresh randomly chosen cache slot, but
the block resides in the client data cache until a back-
ground eviction process writes it back to the server par-
tition corresponding to its cache slot.

Another crucial observation is that the eviction pro-
cess should not reveal which client cache slots are ﬁlled

and which are not, as this can lead to linkability attacks
as well. To achieve this, we use an eviction process that
is independent of the load of each cache slot. Therefore
we sometimes have to write back a dummy block to the
server. For example, one possible eviction algorithm is
to sequentially scan the cache slots at a ﬁxed rate and
evict a block from it or evict a dummy block if the cache
slot is empty.

To aid the understanding of the partitioning frame-
work, it helps to think of each client cache slot i as an
extension of the i-th server partition. At any point of
time, a data block is associated with a random partition
(or slot), and the client has a position map to keep track
of the location of each block. If a block is associated
with partition (or slot) i ∈ [P ], it means that an up-to-
date version of a block currently resides in partition i (or
cache slot i). However, it is possible that other partitions
(or even the same partition) may still carry a stale ver-
sion of block i, which will be removed during a future
reshufﬂing operation.

Every time a read or write operation is performed on
a block, the block is re-assigned to a partition (or slot)
selected independently at random from all P partitions
(or slots). This ensures that two operations on the same
block cannot be linked to each other.

3.4 Setup

When the construction is initialized, we ﬁrst assign
each block to an independently random partition. Since
initially all blocks are zeroed, their values are implicit
and we don’t write them to the server. The position map
stores an additional bit per block to indicate if it has
never been accessed and is hence zeroed. In the prac-
tical construction, this bit can be implicitly calculated
from other metadata that the client stores. Additionally,
the data cache is initially empty.

3.5 Partition O-RAM Semantics and Notations

3.6 Reading a Block

Before we present

the main operations of our
partitioning-based O-RAM, we ﬁrst need to deﬁne the
operations supported by each partition O-RAM.

Recall that each partition is a fully functional O-
RAM by itself. To understand our partitioning frame-
work, it helps to think of each partition as a blackbox
O-RAM. For example, for each partition, we can plug
in the Goodrich-Mitzenmacher O-RAM [5] (with either
N ) client-side storage) or our own parti-
O(1) or O(
tion O-RAM construction described in Section 4.

√

We make a few small assumptions about the partition
O-RAM, and use slightly different semantics to refer to
the partition O-RAM operations than existing work. Ex-
isting O-RAM constructions [3, 5, 13] always perform
both a read and a write operation upon any data access
request. For the purpose of the partitioning framework,
it helps to separate the reads from the writes. In particu-
lar, we require that a ReadPartition operation “logically
remove” the fetched block from the corresponding par-
tition. Many existing constructions [3, 5, 13] can be eas-
ily modiﬁed to support this operation, simply by writing
back a dummy block to the ﬁrst level of the hierarchy
after reading.

Formally, we think of each partition O-RAM
as a blackbox O-RAM exporting two operations,
ReadPartition and WritePartition, as explained below.
• ReadPartition(p, u) reads a block identiﬁed by its
unique identiﬁer u ∈ {⊥, 1, 2, . . . , N − 1} from
partition p.
the read opera-
tion is called a dummy read. We assume that the
ReadPartition operation will logically remove the
fetched block from the corresponding partition.

In case u = ⊥,

• WritePartition(p, u, data)

by

its

writes

identiﬁed

back
a
block
identiﬁer
u ∈ {⊥, 1, 2, . . . , N − 1} to partition p.
In
case u = ⊥, the write operation is called a dummy
write. The parameter data denotes the block’s
data.

unique

Remark 1 (About the dummy block identiﬁer ⊥). The
dummy block identiﬁer ⊥ represents a meaningless data
block. It is used as a substitute for a real block when the
client does not want the server to know that there is no
real block for some operation.

Remark 2 (Block identiﬁer space). Another weak as-
sumption we make is that each partition O-RAM needs
to support non-contiguous block identiﬁers.
In other
words, the block identiﬁers need not be a number within
[1, N ], where N is the O-RAM capacity. Most existing
schemes [3, 5, 13] satisfy this property.

Let read(u) denote a read operation for a block iden-
tiﬁed by u. The client looks it up in the position map,
and ﬁnds out which partition block u is associated with.
Suppose that block u is associated with partition p. The
client then performs the following steps:
Step 1: Read a block from partition p.

• If block u is found in cache slot p, the client per-
forms a dummy read from partition p of the server,
i.e., call ReadPartition(p,⊥) where ⊥ denotes a
reading a dummy block.
• Otherwise, the client reads block u from partition p

of the server by calling ReadPartition(p, u).

Step 2: Place block u that was fetched in Step 1 into the
client’s cache, and update the position map.
• Pick a fresh random slot number s, and place block
u into cache slot s. This means that block u is
scheduled to be evicted to partition s in the future,
unless another read(u) preempts the eviction of this
block.
• Update the position map, and associate block u
with partition s. In this way, the next read(u) will
cause partition s to be read and written.

Afterwards, a background eviction takes place as de-

scribed in Section 3.8.

3.7 Writing a Block

) denote writing data

∗
Let write(u, data

∗ to the block
identiﬁed by u. This operation is implemented as a
read(u) operation with the following exception: when
block u is placed in the cache during the read(u) opera-
∗.
tion, its data is set to data
Observation 1. Each read or write operation will cause
an independent, random partition to be accessed.

Proof. (sketch.) Consider each client cache slot as an
extension of the corresponding server partition. Every
time a block u is read or written, it is placed into a
fresh random cache slot s, i.e., associated with partition
s. Note that every time s is chosen at random, and in-
dependent of operations to the O-RAM. The next time
block u is accessed, regardless of whether block u has
been evicted from the cache slot before this access, the
corresponding partition s is read and written. As the
value of the random variable s has not been revealed to
the server before this, from the server’s perspective s is
independently and uniformly at random.

3.8 Background Eviction

To prevent the client data cache from building up,
blocks need to be evicted to the server at some point.

):

data ← slot[p].read and del(u)
ReadPartition(p,⊥)
data ← ReadPartition(p, u)

∗
Access(op, u, data
1: r ← UniformRandom(1 . . . P )
2: p ← position[u], position[u] ← r
3: if block u is in slot[p] then
4:
5:
6: else
7:
8: end if
9: if op = write then
data ← data
∗
10:
11: end if
12: slot[r] ← slot[r] ∪ {(u, data)}
13: Call Evict(p)

/*Optional eviction piggy-backed on normal data access requests. Can improve perfor-

mance by a constant factor.*/

14: Call SequentialEvict(ν) or RandomEvict(ν)
15: return data

Figure 3: Algorithm for data access. Read or write a data block identiﬁed by u. If op = read, the input parameter data∗ = None,
and the Access operation returns the newly fetched block. If op = write, the Access operation writes the speciﬁed data∗ to the
block identiﬁed by u, and returns the old value of the block u.

There are two eviction processes:

1. Piggy-backed evictions are those that take place
on regular O-RAM read or write operations (see
Line 13 of Figure 3). Basically, if the data ac-
cess request operates on a block currently associ-
ated with partition p, we can piggy-back a write-
back to partition p at that time. The piggy-backed
evictions are optional, but their existence can im-
prove performance by a constant factor.

2. Background evictions take place at a rate propor-
tional to the data access rate (see Line 14 of Fig-
ure 3). The background evictions are completely
independent of the data access requests, and there-
fore can be equivalently thought of as taking place
in a separate background thread. Our construction
uses an eviction rate of ν > 0, meaning that in ex-
pectation, ν number of background evictions are at-
tempted with every data access request. Below are
two potential algorithms for background eviction:

(a) Sequentially scan the cache slots at a ﬁxed
rate ν (see the SequentialEvict algorithm in
Figure 4);

(b) At a ﬁxed rate ν, randomly select a slot from
all P slots to evict from. (a modiﬁed version
of the random eviction algorithm is presented
as RandomEvict in Figure 4);

Our eviction algorithm is designed to deal with two

main challenges:

• Bounding the cache size. To avoid the client’s data
cache from building up indeﬁnitely, the above two
eviction processes combined evict blocks at least
as fast as blocks are placed into the cache. The
actual size of the client’s data cache depends on
the choice of the background eviction rate ν. We
choose ν > 0 to be a constant factor of the actual
data request rate. For our practical construction,
in Section 5.1 we empirically demonstrate the re-
lationship of ν and the cache size. In the full ver-
sion of this paper [1], we prove that our background
eviction algorithm results in a cache size of O(P ).

• Privacy.

It is important to ensure that the back-
ground eviction process does not reveal whether a
cache slot is ﬁlled or the number of blocks in a slot.
For this reason, if an empty slot is selected for evic-
tion, a dummy block is evicted to hide the fact that
the cache slot does not contain any real blocks.

Observation 2. By design, the background eviction pro-
cess generates a partition access sequence independent
of the data access pattern.
Lemma 1 (Partition access sequence reveals nothing
about the data request sequence.). Let (cid:126)y denote a data
request sequence. Let f ((cid:126)y) denote the sequence of par-
tition numbers accessed given data request sequence
(cid:126)y. Then, for any two data request sequences of the
same length (cid:126)y and (cid:126)z, f ((cid:126)y) and f ((cid:126)z) are identically dis-
tributed. In other words, the sequence of partition num-
bers accessed during the life-time of the O-RAM does
not leak any information about the data access pattern.

Evict(p):
1: if len(slot[p]) = 0 then
2:
3: else
4:
5:
6: end if

WritePartition(p,⊥, None)
(u, data) ← slot[cnt].pop()
WritePartition(cnt, u, data)

RandomEvict(ν):
1: for i = 1 to ν do
2:
3:
4: end for

//Assume integer ν

r ← UniformRandom(1 . . . P)
Evict(r)

SequentialEvict(ν):
1: num ← D(ν)
2: for i = 1 to num do
cnt ← cnt + 1
3:
Evict(cnt)
4:
5: end for

//Pick the number of blocks to evict according to distribution D

//cnt is a global counter for the sequential scan

Figure 4: Background evicition algorithms with eviction rate ν. Here we provide two candidate eviction algorithms
SequentialEvict and RandomEvict. SequentialEvict determines the number of blocks to evict num based on a prescribed dis-
tribution D(ν) and sequentially scans num slots to evict from. RandomEvict samples ν ∈ N random slots (with replacement) to
evict from. In both SequentialEvict and RandomEvict, if a slot selected for eviction is empty, evict a dummy block for the
sake of security.

Proof. The sequence of partition numbers are generated
in two ways 1) the regular read or write operations, and
2) the background eviction processes. Due to Observa-
tions 1 and 2, both of the above processes generate a
sequence of partition numbers completely independent
of the data access pattern.

Theorem 1. Suppose that each partition uses a secure
O-RAM construction, then the new O-RAM construction
obtained by applying the partitioning framework over P
partition O-RAMs is also secure.

later show how to allow concurrent shufﬂing and reads
to reduce the worst-case cost to O(log N ) (Section 6).

√

While the practical construction require cN client-
side storage, the constant c is so small that our cN is
smaller than or comparable to
N for typical storage
sizes ranging from gigabytes to terabytes. For the sake
of theoretic interest, in Appendix A, we show how to
recursively apply our O-RAM construction to part of the
√
client-side storage, and reduce the client-side storage to
N ), while incurring only a logarithmic factor in the
O(
amortized cost.

Proof. Straightforward conclusion from Lemma 1 and
the security of the partition O-RAM.

4.1 Overview

3.9 Algorithm Pseudo-code

Figures 3 and 4 describe in formal pseudo-code our
O-RAM operations based on the partitioning frame-
work. For ease of presentation, in Figure 3, we unify
∗
read and write operations into an Access(op, u, data
)
operation.

4 Practical Construction

In this section, we apply the partitioning techniques
mentioned in the previous section to obtain a practical
construction with an amortized cost of 20 ∼ 35X over-
head under typical settings, about 63 times faster than
the best known construction. The client storage is typi-
√
cally 0.01% to 0.3% of the O-RAM capacity. The worst-
N ), and we will
case cost of this construction is O(

N partitions, each with

Our practical construction uses the partitioning
framework (Section 3). For the partitions, we use our
own highly optimized O-RAM construction resembling
the Pinkas-Reinman O-RAM at a very high level [13].
√
√
Choice of parameters.
In this practical construc-
tion, we choose P =
N
blocks on average. We use the SequentialEvict algo-
rithm as the background eviction algorithm. Every time
SequentialEvict is invoked with an eviction rate of ν,
it decides the number of blocks to evict numbased on
a bounded geometric distribution with mean ν, i.e., let c
be a constant representing the maximum number of evic-
tions per data access operation, then Pr[num = k] ∝ pk
for 0 ≤ k ≤ c, and Pr[num = k] = 0 for k > c. Here is
0 < p < 1 is a probability dependent on ν and c.

As mentioned earlier, the piggybacked evictions en-
able practical savings up to a constant factor, so we use

continue

// skip empty levels

end if
if block u is in partition p, level (cid:96) then

ReadPartition(p, u):
1: L ← number of levels
2: for (cid:96) = 0, 1, . . . , L − 1 (in parallel) do
if level (cid:96) of partition p is not ﬁlled then
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:

i = position[u].index

else

set i(cid:48).

i = nextDummy[p, (cid:96)]
nextDummy[p, (cid:96)] ← nextDummy[p, (cid:96)] + 1

end if
i(cid:48) = PRP (K[p, (cid:96)], i)
Fetch from the server the block in partition p, level (cid:96), and off-

Decrypt the block with the key K[p, (cid:96)].

14:
15: end for

Figure 5: The ReadPartition operation of our practical construction
that reads the block with id u from partition p.

Figure 6: WritePartition leads to the shufﬂing of consec-
utively ﬁlled levels into the ﬁrst empty level.

piggybacked evictions in this construction.
Optimized partition O-RAM construction. While any
existing O-RAM construction satisfying the modiﬁed
ReadPartition and WritePartition semantics can be used
as a partition O-RAM, we propose our own highly op-
timized partition O-RAM. Our partition O-RAM con-
struction resembles the Pinkas-Reinman O-RAM at a
very high level [13], but with several optimizations to
gain practical savings. The practical savings come from
at least three sources, in comparison with the Pinkas-
Reinman construction:

√

√

• Local sorting. Due our partitioning framework,
each partition is now of size O(
N ) blocks. This
allows us to use a client shufﬂing buffer of size
N ) blocks to reshufﬂe the partition locally,
O(
thereby eliminating the need for extremely expen-
sive oblivious sorting procedures during a reshuf-
ﬂing operation. This is our most signiﬁcant source
of saving in comparison with all other existing
schemes.
• No Cuckoo hashing. Second, since we use a posi-
tion map to save the locations of all blocks, we no
longer need Cuckoo hashing, thereby saving a 2X
factor for lookups.
• Compressed data transfer during reshufﬂing.
Third, during the reshufﬂing operation, the client
only reads blocks from each level that have not
been previously read. Also, when the client writes
back a set of shufﬂed blocks to the server (at least
half of which are dummy blocks), it uses a com-
pression algorithm to compress the shufﬂing buffer
down to half its size. These two optimizations save

about another 2X factor.

• Latency reduction.

In the practical construction,
the client saves a position map which records the
locations of each block on the server. This allows
the client to query the O(log N ) levels in each par-
tition in a single round-trip, thereby reducing the
latency to O(1).

4.2 Partition Layout

√

N ) + 1 = 1

As mentioned earlier, we choose P =

N partitions
√
for the practical construction. Each partition consists of
2 log2 N + 1 levels, indexed by
L = log2(
2 log2 N respectively. Except for the top level,
0, 1, . . . , 1
each level (cid:96) has 2 · 2(cid:96) blocks, among which at most half
are real blocks, and the rest (at least half) are dummy
blocks.
2 log2 N has 2 · 2(cid:96) +  =
√
The top level where (cid:96) = 1
N +  blocks, where the surplus  is due to the
2
fact that some partition may have more blocks than oth-
ers when the blocks are assigned in a random fashion
to the partitions. Due to a standard balls and bins ar-
√
gument [14], each partition’s maximum size (including
N ) such
real and dummy blocks) should be 4
that the failure probability
poly(N ). In Appendix 5.3, we
empirically demonstrate that in practice, the maximum
√
number of real blocks in each partition is not more than
N for N ≥ 20, hence the partition capacity is
1.15
no more than 4 · 1.15
N blocks, and the
total server storage is no more than 4.6N blocks. In Ap-
pendix B.2, we propose an optimization to reduce the
server storage to less than 3.2N blocks.

√
N = 4.6

N + o(

√

√

1

At any given time, a partition on the server might
have some of its levels ﬁlled with blocks and others un-
ﬁlled. The top partition is always ﬁlled. Also, a data
block can be located in any partition, any ﬁlled level, and
any offset within the level. In the practical construction,
we extend the position map of the partition framework
to also keep track of the level number and offset of each
block.

From the perspective of the server, all blocks within a
level are pseudo-randomly arranged. Because the blocks
are encrypted, the server cannot even tell which blocks
are real and which ones are dummy. We use keyed
pseudo-random permutation (PRP) function for permut-
ing blocks within a level in our construction. When the
context is clear, we omit the range or the PRP function
in the pseudo-code.

4.3 Setup

The initial set of ﬁlled levels that contain the blocks
depends on the partition number p.
In order to better
amortize the reshufﬂing costs of our scheme, the client
randomly chooses which levels of each partition will be
initially ﬁlled (with the restriction that the top level is
always ﬁlled). Note that there are 2L−1 such possible
ﬁllings of a partition where L is the number of levels in
the partition. The client notiﬁes the server which lev-
els are ﬁlled but does not write the actual blocks to the
server because the blocks are initially zeroed and their
values can be calculated implicitly by storing one bit for
each level of each partition. This bit indicates if the en-
tire level has never been reshufﬂed and is hence zeroed.

4.4 Reading from a Partition

The ReadPartition operation reads the block with id
u from partition p as described in Figure 5. If u = ⊥,
then the ReadPartition operation is a dummy read and a
dummy block is read from each ﬁlled level. If u (cid:54)= ⊥,
block u is read from the level that contains it, and a
dummy block is read from from each of the other ﬁlled
levels. Note that all of the fetches from the server are
performed in parallel and hence this operation has sin-
gle round trip latency unlike existing schemes [3, 4, 11]
which take Ω(log N ) round trips.

4.5 Writing to a Partition

Each write to a partition is essentially a reshufﬂing
operation performed on consecutively ﬁlled levels in
a partition. Therefore, we sometimes use the terms
“write” and “shufﬂing” interchangeably. First, unread
blocks from consecutively ﬁlled levels of the partition
are read from the server into the client’s shufﬂing buffer.

Then, the client permutes the shufﬂing buffer accord-
ing to a pseudo-random permutation (PRP) function. Fi-
nally, the client uploads its shufﬂing buffer into the ﬁrst
unﬁlled level and marks all of the levels below it as un-
ﬁlled. The detailed pseudo-code for the WritePartition
operation is given in Figure 7.

There is an exception when all levels of a partition
are ﬁlled. In that case, the reshufﬂing operation is per-
formed on all levels, but at the end, the top level (which
was already ﬁlled) is overwritten with the contents of the
shufﬂing buffer and the remaining levels are marked as
unﬁlled. Note that the shufﬂing buffer is never bigger
than the top level because only unread real (not dummy)
blocks are placed into the shufﬂing buffer before it is
padded with dummy blocks. Since the top level is big
enough to contain all of the real items inside a partition,
it can hold the entire shufﬂing buffer.

During a reshufﬂing operation, the client uses the
pseudo-random permutation PRP to determine the off-
set of all blocks (real and dummy) within a level on the
server. Every time blocks are shufﬂed and written into
the next level, the client generates a fresh random PRP
key K[p, (cid:96)] so that blocks end up at random offsets every
time that level is constructed. The client remembers the
keys for all levels of all partitions in its local cache.

Reading levels during shufﬂing. When the client reads
a partition’s levels into the shufﬂing buffer (Line 5 of
Figure 7), it reads exactly 2(cid:96) previously unread blocks.
Unread blocks are those that were written during a
WritePartition operation when the level was last con-
structed, but have not been read by a ReadPartition op-
eration since then. The client only needs to read the un-
read blocks because the read blocks were already logi-
cally removed from the partition when they were read.
There is a further restriction that among those 2(cid:96) blocks
must be all of the unread real (non-dummy) blocks.
Since a level contains up to 2(cid:96) real blocks, and there are
always at least 2(cid:96) unread blocks in a level, this is always
possible.

The client can compute which blocks have been
read/unread for each level. It does this by ﬁrst fetching
from the server a small amount of metadata for the level
that contains the list of all blocks (read and unread) that
were in the level when it was last ﬁlled. Then the client
looks up each of those blocks in the position map to de-
termine if the most recent version of that block is still in
this level. Hence, the client can obtain the list of unread
real blocks. The offsets of the of unread dummy blocks
can be easily obtained by repeatedly applying the PRP
function to nextDummy and incrementing nextDummy.
Note that for security, the offsets of the 2(cid:96) unread blocks
must be ﬁrst computed and then the blocks must be read
in order of their offset (or some other order independent

WritePartition(p, u∗, data∗):
1: // Read consecutively ﬁlled levels into the client’s shufﬂing buffer denoted sbuffer.
2: (cid:96)0 ← last consecutively ﬁlled level
3: for (cid:96) = 0 to (cid:96)0 do
4:
5:

Fetch the metadata (list of block ID’s) for level (cid:96) in partition p. Decrypt with key K[p, (cid:96)].
Fetch exactly 2(cid:96) previously unread blocks from level (cid:96) into sbuffer such that all unread real blocks are among them.

Decrypt everything with the key K[p, (cid:96)]. Ignore dummy blocks when they arrive.

Mark level (cid:96) in partition p as unﬁlled.

6:
7: end for
8: (cid:96) = min((cid:96)0, L − 1) // Don’t spill above the top level.
9: Add the (u∗, data∗) to sbuffer.
10: k ← number of real blocks in sbuffer.
11: for i = 1 to k do
12:
13:
14: end for
15: K[p, (cid:96)] ←R K
16: Pad the shufﬂing buffer with dummy blocks up to length 2 · 2(cid:96). // The ﬁrst k blocks are real and the rest are dummy.
17: Permute sbuffer with PRP(cid:96)(K[p, (cid:96)],·). A block originally at index i in the shufﬂing buffer is now located at offset i(cid:48) in the

Let (u, data) = sbuﬀer[i]
position[u] ← {p, (cid:96), i}

// generate fresh key for level (cid:96) in partition p

// update position map

shufﬂing buffer, where i(cid:48) = PRP(cid:96)(K[p, (cid:96)], i)

18: Write the shufﬂing buffer into level (cid:96) in partition p on the server, encrypted with key K[p, (cid:96)].
19: Write the metadata (list of block ID’s) of level (cid:96) in partition p to the server, encrypted with key K[p, (cid:96)].
20: Mark level (cid:96) in partition p as ﬁlled.
21: nextDummy[p, (cid:96)] ← k + 1
Notations:

// initialize counter to ﬁrst dummy block.

K[p, (cid:96)]

nextDummy[p, (cid:96)]

{p, (cid:96), i} ← position[u]

Secret key for partition p, level (cid:96). (PRP or symmetric encryption key depending on context)
Index of next unread dummy block for partition p, level (cid:96).
Position information for block u (partition p, level (cid:96), index i within the level).

Figure 7: The WritePartition operation of our practical construction that writes block u∗ with data∗ to partition p.

of which blocks are real/dummy).

4.6 Security

Our practical construction has the following secu-
rity guarantees: obliviousness (privacy), conﬁdential-
ity, and authentication (with freshness).

Theorem 2. The practical construction is oblivious ac-
cording to Deﬁnition 1.

Proof. The proof is presented in the full version of the
paper [1].

Theorem 3. The practical construction provides conﬁ-
dentiality and authentication (with freshness) of all data
stored on the server.

Proof. The proof is presented in the full version of the
paper [1].

It should be noted that although our partition O-RAM
construction resembles the Pinkas-Reinman construc-
tion, it does not have the security ﬂaw discovered by

Goodrich and Mitzenmacher [5] because it does not use
Cuckoo hash tables.

5 Experimental Results

For our experiments, we implemented a simulator of
our construction. Each read/write operation is simu-
lated and the simulator keeps track of exactly where each
block is located, the amount of client side storage used,
and the total bytes transferred for all communication be-
tween the client and server. We also implemented a sim-
ulator for the best previously known O-RAM scheme for
comparison.

For each parametrization of our O-RAM construc-
tion, we simulated exactly 3N read/write operations.
For example, for each O-RAM instances with N = 228
blocks, we simulated about 800 million operations. We
used a round-robin access pattern which maximizes the
size of the client’s data cache of our construction by
maximizing the probability of a cache miss. Therefore
our results always show the worst case cache size. Also,
because our construction is oblivious, our amortized cost

measurements are independent of the simulated access
pattern. We used the level compression, server storage
reduction, and piggy-backed eviction optimizations de-
scribed in Appendix B.

5.1 Client Storage and Bandwidth

In this experiment, we measure the performance
overhead (or bandwidth overhead) of our O-RAM. An
O-RAM scheme with a bandwidth overhead of w per-
forms w times the data transfer as an unsecured remote
storage protocol. In the experiments we ignore the meta-
data needed to store and fetch blocks because in practice
it is much smaller than the block size. For example, we
may have 256 KB blocks, but the metadata will be only
a few bytes.

In our scheme, the bandwidth overhead depends on
the background eviction rate, and the background evic-
tion rate determines the client’s cache size. The client
is free to choose its cache size by using the appropriate
eviction rate. Figure 8 shows the correlation between the
background eviction and cache size as measured in our
simulation.

Once the client chooses its cache size it has deter-
√
mined the total amount of client storage. As previ-
N B) bytes
ously mentioned, our scheme requires O(
of client storage plus an extra cN B bytes of client stor-
age for the position map with a very small constant.
√
For most practical values of N and B, the position map
is much smaller than the remaining O(
N B) bytes of
√
client storage, so the client storage approximately scales
N B) bytes. We therefore express the total
like O(
client storage as k
N B bytes. Then, we ask the ques-
tion: How does the client’s choice of k affect the band-
width overhead of our entire O-RAM construction? Fig-
ure 9 shows this trade-off between the total client storage
and the bandwidth overhead.

√

5.2 Comparison with Previous Work

To the best of our knowledge,

the most practi-
cal existing O-RAM construction was developed by
Goodrich et al. [5]. It works by constructing a hierar-
chy of Cuckoo hash tables via Map-Reduce jobs and an
efﬁcient sorting algorithm which utilizes N a (a < 1)
blocks of client-side storage. We implemented a simula-
tor that estimates a lower bound on the performance of
their construction. Then we compared it to the simula-
tion of our construction.

To be fair, we parametrized both our and their con-
√
struction to use the exact same amount of client stor-
N B bytes. The client storage includes all of our
age: 4
client data structures, including our position map (stored
uncompressed). We parametrized both constructions for

exactly 1 TB O-RAM capacity (meaning that each con-
struction could store a total of 1 TB of blocks). We var-
ied the number of blocks from N = 216 to N = 224.
Since the O-RAM size was ﬁxed to 1 TB, the blocks
size varied between B = 224 bytes and B = 216 bytes.
Table 11 shows the results. As it can be clearly seen,
our construction uses 63 to 66 times less bandwidth than
the best previously known scheme for the exact same
parameters.

5.3 Partition Capacity

√

√

N partitions each containing about

Finally, we examine the effects of splitting up the O-
RAM into partitions. Recall that in our practical con-
struction with N blocks, we have split up the server
storage into
N
blocks. Since the blocks are placed into partitions uni-
formly randomly rather than uniformly, a partition might
N blocks. For
end up with slightly more or less than
security reasons, we want to hide from the server how
many blocks are in each partition at any given time, so
a partition must be large enough to contain (with high
probability) the maximum number of blocks that could
end up in a single partition.

√

√

Figure 10 shows how many times more blocks a parti-
tion contains than the expected number:
N. Note that
as the size of the O-RAM grows, the maximum size of
a partition approaches its expected size. In fact, one can
formally show that the maximum number of real data
blocks in each partition over time is
N ) [14].
Hence, for large enough N, the partition capacity is less
than 5% larger than

N blocks.

N + o(

√

√

√

6 Reducing the Worst-Case Cost With

Concurrency

√
The constructions described thus far have a worst-
√
case cost O(
N ) because a WritePartition operation
N ) blocks. We
sometimes causes a reshufﬂing of O(
√
reduce the worst-case cost by spreading out expen-
sive WritePartition operations of O(
N ) cost over a
long period of time, and at each time step performing
O(log N ) work.

To achieve this, we allow reads and writes (i.e.,
reshufﬂing) to a partition to happen concurrently. This
way, an operation does not have to wait for previous
long-running operations to complete before executing.
We introduce an amortizer which keeps track of which
partitions need to be reshufﬂed, and schedules O(log N )
work (or O((log N )2) for the theoretic recursive con-
struction) per time step. There is a slight storage cost of
allowing these operations to be done in parallel, but we
will later show that concurrency does not increase the

Figure 8: Background Eviction Rate vs. Cache Capac-
ity. The x-axis is the eviction rate, deﬁned as the ratio
of background evictions over real data requests. For exam-
ple, an eviction rate of 1 suggests an equal rate of data re-
√
quests and background evictions. The y-axis is the quantity
(cache capacity)/
N, where cache capacity is the maxi-
mum number of data blocks in the cache over the course of
time.

Figure 9: Trade-off between Client Storage and Band-
√
width. The plot shows what bandwidth overhead a client can
N B bytes of client storage for dif-
achieve by using exactly k
ferent values of k (horizontal axis). The client storage includes
the cache, sorting buffer, and an uncompressed position map.
A block size of 256 KB was assumed.

# Blocks Block Size

216
218
220
222
224

16 MB
4 MB
1 MB
256 KB
64 KB

Best Known [5]

Practical Performance
Ours
18.4X
19.9X
21.5X
23.2X
25.0X

> 1165X
> 1288X
> 1408X
> 1529X
> 1651X

√
Figure 10: Partition capacity. The y-axis is the quantity
(partition capacity)/
N, where partition capacity is the
maximum number of real data blocks that the partition must
be able to hold, i.e., the maximum number of real data blocks
inside a partition over time.

Figure 11: Comparison between our construction and the
√
best known previous O-RAM construction. A 1 TB O-RAM
is considered with both constructions using exactly 4
N B
client storage. The practical performance is the number of
client-server operations per O-RAM operation. Our construc-
tion has a 63 to 66 times better performance than the best pre-
viously known scheme for the exact same parameters.

asymptotic storage and amortized costs of our construc-
tions.

By performing operations concurrently, we decrease
√
the worst-case cost of the practical construction from
√
N ) to O(log N ) and we reduce the worst-case
O(
cost of the recursive construction from O(
N ) to
O((log N )2). Our concurrent constructions preserve
the same amortized cost as their non-concurrent coun-
terparts; however, in the concurrent constructions, the
worst-case cost is the same as the amortized cost. Fur-
thermore, in the concurrent practical construction, the
latency is O(1) just like the non-concurrent practical

construction, as each data request requires only a single
round-trip to complete.

6.1 Overview

We reduce the worst case cost of our construc-
tions by inserting an Amortizer component into our
system which explicitly amortizes ReadPartition and
WritePartition operations as described in Figure 12.
Speciﬁcally, the Amortizer schedules a ReadPartition
operation as a ConcurrentReadPartition operation, so
the read can occur while shufﬂing. A ReadPartition al-

TimeStep(op):
1: if op is ReadPartition(p, u) then
Call ConcurrentReadPartition(p, u)
2:
3: else if op is WritePartition(p, u, data) then
λ ← max i for which Cp = 0 mod 2i.
4:
β ← {(u, data)}
5:
if (p, λ(cid:48), β(cid:48)) ∈ Q for some λ(cid:48) and β(cid:48) then
6:
7:
8:
9:
10:
11:
12: end if
13: Perform O(log N ) work from job queue Q in the form of

Q ← Q − {(p, λ(cid:48), β(cid:48))}
λ ← max(λ, λ(cid:48)), β ← β(cid:48) ∪ β

end if
Q ← Q ∪ {(p, λ, β)}
Cp ← Cp + 1

ConcurrentWritePartition operations.

Figure 12: The Amortizer component helps reduce the worst case costs of our constructions.
It is inserted between the
background eviction process and the server-side partitions as shown on the left. The component executes one operation per time
step as deﬁned on the right.

ways ﬁnishes in O(log N ) time. Upon a WritePartition
operation (which invokes the shufﬂing of a partition),
the Amortizer creates a new shufﬂing “job”, and ap-
pends it to a queue Q of jobs. The Amortizer schedules
O(log N ) amount of work to be done per time step for
jobs in the shufﬂing queue.

If reads are taking place concurrently with shufﬂing,
special care needs to be taken to avoid leakages through
the access pattern. This will be explained in the detailed
scheme description below.
Terminology. To aid understanding, it helps to deﬁne
the following terminology.
• Job. A job (p, λ, β) denotes a reshufﬂing of levels
0, . . . , λ of partition p and then writing the blocks
in β to partition p on the server.
• Job Queue Q. The job queue Q is a FIFO list of
jobs. It is also possible to remove jobs that are not
necessarily at the head of the queue for the purpose
of merging them with other jobs, however jobs are
always added at the tail.
• Partition counter. Let Cp ∈ Zs denote a counter
for partition p, where s is the maximum capacity of
partition p. All operations on Cp are modulus s.
• Work. The work of an operation is measured in
terms of the number of blocks that it reads and
writes to partitions on the server.

Handling ReadPartition operations.
The amor-
tizer performs a ReadPartition(p, u) operation as a
ConcurrentReadPartition(p, u) operation as deﬁned in
Sections 6.1.1 for the practical and recursive construc-
tions respectively.
If block u is cached by a previous

ConcurrentReadPartition operation, then it is instead
read from β where (p, λ, β) ∈ Q for some λ and β.
Handling WritePartition operations.
The amor-
tizer component handles a WritePartition operation by
adding it to the job queue Q. The job is later dequeued
in some time step and processed (possibly across mul-
tiple time steps). If the queue already has a job involv-
ing the same partition, the existing job is merged with
the new job for the current WritePartition operation.
Speciﬁcally, if one job requires shufﬂing levels 0, . . . , λ
and the other job requires shufﬂing levels 0, . . . , λ(cid:48), we
merge the two jobs into a job that requires shufﬂing lev-
els 0, . . . , max(λ, λ(cid:48)). We also merge the blocks to be
written by both jobs.
Processing jobs from the job queue. For each time
step, the reshufﬂing component perform w log N work
for a predetermined constant w such that w log N is
greater than or equal to the amortized cost of the con-
struction. Part of that work may be consumed by a
ConcurrentReadPartition operation executing at the be-
ginning of the time step as described in Figure 12. The
remaining work is performed in the form of jobs ob-
tained from Q.

Deﬁnition 2 (Processing a job). A job (p, λ, β) is
performed as a ConcurrentWritePartition(p, λ, β) op-
eration that reshufﬂes levels 0, . . . , λ of partition p
and writes the blocks in β to partition p.
The
ConcurrentWritePartition operation is described in
Sections 6.1.2 for the practical and recursive construc-
tions respectively. Additionally, every block read and

written to the server is counted to calculate the amount
of work performed as the job is running. A job may be
paused after having completed part of its work.

Jobs are always dequeued from the head of Q. At any
point only a single job called the current job is being
processed unless the queue is empty (then there are no
jobs to process). Each job starts after the previous job
has completed, hence multiple jobs are never processed
at the same time.

this fact to the server does not affect the secu-
rity of the construction because the server al-
ready knows that the client has the entire level
stored in its reshufﬂing buffer.

When u = ⊥, block u is treated as not being con-
tained in any level. Due to concurrency, it is possible
that a level of a partition needs to be read during reshuf-
ﬂing. In that case, blocks may be read directly from the
client’s shufﬂing buffer containing the level.

If the current job does not consume all of the remain-
ing work of the time step, the the next job in Q becomes
the current job is paused
the current job, and so on.
when the total amount of work performed in the time
step is exactly w log N. In the next time step, the cur-
rent job is resumed from where it was paused.
to

We
per-
form
and
ConcurrentWritePartition operations in the practi-
cal construction to achieve an O(log N ) worst-case cost
with high probability.

ConcurrentReadPartition

explain

now

how

6.1.1 Concurrent Reads

The client performs the ConcurrentReadPartition(p, u)
operation by reading 0 or 1 blocks from each ﬁlled level
(cid:96) of partition p on the server as follows:

• If level (cid:96) in partition p contains block u, then

– Read block u from level (cid:96) in partition p on the
• If level (cid:96) in partition p does not contain block u and

server like in the ReadPartition operation.

– Recall that when level (cid:96) is being reshufﬂed,
2(cid:96) previously unread blocks are chosen to be
read. Let S be the identiﬁers of that set of
blocks for level (cid:96) in partition p.
– Let S(cid:48) ⊆ S be the IDs of blocks
in S that were not
read by a previous
ConcurrentReadPartition operation after the
level started being reshufﬂed. The client
keeps track of S(cid:48) for each level by ﬁrst set-
ting it to S when a ConcurrentWritePartition
operation begins and then removing u from
S(cid:48) after every ConcurrentReadPartition(p, u)
operation.
– If S(cid:48) is not empty, the client reads a random
block in S(cid:48) from the server.
– If S(cid:48) is empty, then the client doesn’t read
anything from level (cid:96) in partition p. Revealing

this level is not being reshufﬂed, then

– Read the next dummy block from level (cid:96)
like in the
• If level (cid:96) in partition p does not contain block u and

in partition p on the server
ReadPartition operation.

this level is being reshufﬂed, then

6.1.2 Concurrent Writes

A ConcurrentWritePartition(p, λ, β) operation is per-
formed like the non-concurrent WritePartition opera-
tion described in Figure 7, except for three differences.
The ﬁrst difference is that the client does not shufﬂe
based on the last consecutively ﬁlled level.
Instead it
shufﬂes the levels 0, ..., λ which may include a few more
levels than the WritePartition operation would shufﬂe.
The second difference is that at Line 9 of Figure 7,

the client adds all of the blocks in β to the buffer.

The third difference is at Line 4 of Figure 7. In the
non-concurrent construction, client fetches the list of 2(cid:96)
blocks ID’s in a level that is about to be reshufﬂed. It
then uses this list to determine which blocks have al-
√
ready been read as described in Section 4.5). Because 2(cid:96)
N ) fetching this metadata in the non-concurrent
is O(
construction takes O(

N ) work in the worst case.

√

To ensure the worst case cost of the concurrent con-
struction is O(log N ), the metadata is stored as a bit
array by the client. This bit array indicates which real
blocks in that level have already been read. The client
also knows which dummy blocks have been read be-
cause it already stores the nextDummy counter and it
can apply the PRP function for all dummy blocks be-
tween k + 1 and nextDummy where k is the number
of real blocks in a level. Observe that the client only
needs to store a single bit for each real block on the
√
server. Hence this only increases the client storage by
N bits, which is signiﬁcantly smaller than the
2N + 
size of index structure that the client already stores.

Theorem 4 (Practical concurrent construction). With
1 − 1
poly(N ) probability, the concurrent practical con-
struction described above has O(log N ) worst-case and
amortized cost, and requires cN client-side storage with
a very small c, and O(N ) server-side storage.

The formal proof of the above theorem is in the full

version of the paper [1].

Figure 13: The recursive construction.

Appendices

A Recursive Construction

The practical (non-current and concurrent) construc-
tions described so far are geared towards optimal prac-
tical performance. However, they are arguably not ideal
in terms of asymptotic performance, since they require
a linear fraction of client-side storage for storing a posi-
tion map of linear size.

For theoretic interest, we describe in this section how
to recursively apply our O-RAM constructions to store
√
the position map on the server, thereby obtaining O-
RAM constructions with O(
N ) client-side storage,
while incurring only a logarithmic factor in terms of
amortized and worst-case cost.

We ﬁrst describe the recursive, non-concurrent con-
√
struction which achieves O((log N )2) amortized cost,
and O(
N ) worst-case cost. We then describe how to
apply the concurrency techniques to further reduce the
worst-case cost to O((log N )2), such that the worst-case
cost and amortized cost will be the same.

A.1 Recursive Non-Concurrent Construction

Intuition. Instead of storing the linearly sized position
map locally, the client stores it in a separate O-RAM on
the server. Furthermore, the O-RAM for the position
map is guaranteed to be a constant factor smaller than
the original O-RAM. In other words, each level of re-
cursion reduces the O-RAM capacity by a constant fac-
tor. After a logarithmic number of recursions, the size of
the position map stored on the client is reduced to O(1).

The total size of all data caches is O(
construction uses O(

√
N ) client storage.

√

N ), hence the

For

the recursive construction, we employ the
Goodrich-Mitzenmacher O-RAM scheme as the parti-
tion O-RAM. Speciﬁcally, we employ their O-RAM
scheme which for an O-RAM of capacity N, achieves
√
O(log N ) amortized cost and O(N ) worst-case cost,
while using O(
N ) client-side storage, and O(N )
server-side storage.

Deﬁnition 3 (O-RAMGM ). Let O-RAMGM denote the
Goodrich-Mitzenmacher O-RAM scheme [5]: for an O-
RAM of capacity N, the O-RAMGM scheme achieves
√
O(log N ) amortized cost, and O(N ) worst-case cost,
while using O(
N ) client-side storage, and O(N )
server-side storage.

√

Deﬁnition 4 (O-RAMbase). Let O-RAMbase denote the
O-RAM scheme derived through the partitioning frame-
work with the following parameterizations: (1) we set
N denote the number of partitions, where each
P =
partition has approximately
N blocks, and (2) we use
the O-RAMGM construction as the partition O-RAM.

√

B

B

+

N log N = N

√
Notice that in O-RAMbase, the client requires a data
N ) and a position map of size less
cache of size O(
than 2N log N
blocks. If we assume that the data block
√
size B > 2 log N, then the client needs to store at most
2N log N
α + o(N ) blocks of data,
2 log N > 1. To re-
where the compression rate α = B
duce the client-side storage, we can recursively apply
the O-RAM construction to store the position map on
the server side.
Deﬁnition 5 (Recursive construction: O-RAM∗). Let
O-RAM∗ denote a recursive O-RAM scheme constructed

as below. In O-RAMbase, the client needs to store a po-
sition map of size cN (c < 1). Now, instead of storing
the position map locally on the client, store it in a recur-
sive O-RAM on the server side. The pseudocode of the
O-RAM∗ scheme would be otherwise be the same as in
Figure 3 except that Line 2 is modiﬁed to the the follow-
ing recursive O-RAM lookup and update operation:

The position position[u] is stored in block u/α of the
smaller O-RAM. The client looks up this block, updates
the corresponding entry position[u] with the new value
r, and writes the new block back. Note that the read and
update can be achieved in a single O-RAM operation
to the smaller O-RAM.
Theorem 5 (Recursive O-RAM construction). Suppose
that the block size B > 2 log N , and that the number
√
log N ). Our
of data accesses M < N k for some k = O(
√
recursive O-RAM construction achieves O((log N )2)
amortized cost, O(
N ) worst-case cost, and requires
O(N ) server-side storage, and O(
N ) client-side stor-
age.

√

N

reading two random blocks within a level instead of
applying a Cuckoo hash on a element from small
domain. Observe that the Cuckoo hashes for real read
operations output uniformly random block positions.
Because the blocks read by dummy read operation
are also chosen from a uniformly random distribution,
dummy reads are indistinguishable from real reads.

This observation allows the client to securely perform
a ConcurrentReadPartition(p, u) operation as follows.
For each level (from smaller to larger) of partition p, as
usual the client performs a Cuckoo hash of the block
identiﬁer u to determine which two blocks to read within
the level. Once the block is found in some level, the
client performs dummy reads on subsequent levels. The
client always ﬁrst checks the local storage to see if the
block is in the job queue. If so, then the client performs
dummy reads of all levels.

In summary,

instead of reading speciﬁc dummy
blocks (which can be exhausted since there are only 2(cid:96)
dummy blocks in level (cid:96)), the client performs dummy
reads by choosing two random positions in the level.

The proof of the above theorem is presented in in the

full version of the paper [1].

A.2.2 Concurrent Writes

A.2 Recursive Concurrent Construction

Using similar concurrency techniques as in Section 6,
we can further reduce the worst-case cost of the recur-
sive construction to O((log N )2). Recall that the re-
cursive construction differs from the practical construc-
tion in two ways: (1) it uses the O-RAMGM (Goodrich-
Mitzenmacher [5]) scheme as the partition O-RAM and
(2) it recurses on its position map. We explain how
to to perform concurrent operations in the O-RAMGM
scheme to reduce the worst case cost of the base con-
struction to O(log N ) with high probability. Then
when the recursion is applied, the recursive construc-
tion achieves a worst case cost of O((log N )2) with high
probability.

A.2.1 Concurrent Reads

As concurrency allows reshufﬂes to be queued for later,
it is possible that a level (cid:96) is read more than 2(cid:96) times in
between reshufﬂings. The O-RAMGM scheme imposes
a restriction that at most 2(cid:96) blocks can be read from a
level before it must be reshufﬂed by using a set of 2(cid:96)
dummy blocks. We observe that it is possible to perform
a dummy read operation instead of using a dummy block
and performing a normal read on it. This essentially
eliminates the use of dummy blocks. Note that the same
idea was suggested in the work by Goodrich et al. [8].
ConcurrentReadPartition(p,⊥)

operation
by

performed

read
is

dummy

A

the

recursive

concurrent

construction,

In
the
ConcurrentWritePartition(p, λ, β) operation is per-
formed by ﬁrst reshufﬂing levels 0, ..., λ along with
the blocks in β using the oblivious shufﬂing protocol
of the O-RAMGM scheme. After the reshufﬂing has
completed, the updated Cuckoo hash tables have been
formed.

Theorem 6 (Recursive concurrent O-RAM). Assume
that the block size B > 2 log N. With 1 − 1
poly(N ) prob-
ability, the concurrent recursive construction described
√
above has O((log N )2) worst-case and amortized cost,
and requires O(
N ) client-side storage, and O(N )
server-side storage.

The formal proof of the above theorem will be in-

cluded in the full version of the paper [1].

B Optimizations and Extensions

B.1 Compressing the Position Map

The position map is highly compressible under real-
istic workloads due to the natural sequentiality of data
accesses. Overall we can compress the position map to
about 0.255 bytes per block. Hence its compressed size
is 0.225N bytes. Even for an extremely large, 1024 TB
O-RAM with N = 232 blocks, the position map will be
under 1 GB in size. We now explain how to compress
the position map.

Compressing partition numbers. In [10], Oprea et al.
showed that real-world ﬁle systems induce almost en-
tirely sequential access patterns. They used this obser-
vation to compress a data structure that stores a counter
of how many times each block has been accessed. Their
experimental results on real-world traces show that the
compressed data structure stores about 0.13 bytes per
block. Every time a block is accessed, their data struc-
ture stores a unique value (speciﬁcally, a counter) for
that block. In our construction, instead of placing newly
read blocks in a random cache slot, we can place them in
a pseudo-random cache slot determined by the block id
and counter. Speciﬁcally, when block i is accessed for
the j’th time (i.e., it’s counter is j), it is placed in cache
slot PRF(i.j). PRF(·) is a pseudo-random function that
outputs a cache slot (or partition) number.
Compressing level numbers. Recall that each partition
contains L levels such that level (cid:96) contains at most 2(cid:96)
real blocks. We can represent the level number of each
block by using only 1 bit per block on average, regard-
less of the number of levels. This can be easily shown by
computing the entropy of the level number as follows. If
all levels are ﬁlled, each block has probability 2(cid:96)−L of
being in level (cid:96). Then the entropy of the level number is

− L−1(cid:88)

L(cid:88)

2(cid:96)−L log2(2(cid:96)−L) =

i · 2−i < 1

(cid:96)=0

i=1

If not all levels in a partition are ﬁlled, then the entropy
is even less, but for the sake of simplicity let’s still use
1 bit to represent a level number within that partition.
Note that since the top level is slightly larger (it contains
 extra blocks), the entropy might be slightly larger than
1 bit, but only by a negligible amount.
Using the compressed position map. These two com-
pression tricks allow us to compress our position map
immensely. On average, we can use 0.13 bytes for the
partition number and 0.125 bytes (1 bit) for the level
number, for a total of 0.255 bytes per block.

is easy. When each level

Once we have located a block’s partition p and
level (cid:96), retrieving it
is
constructed, each real block can be assigned a fresh
alias PRF(K[p, (cid:96)], “real-alias”, u) where u is the ID
of
the block and PRF is a pseudo-random func-
tion. Each dummy block can be assigned the alias
PRF(K[p, (cid:96)], “dummy-alias”, i) where i is the index of
the dummy block in partition p, level (cid:96). Then during re-
trieval, the client fetches blocks from the server by their
alias.

B.2 Reducing Server Storage

√

Each partition’s capacity is

N + blocks, where the
surplus  is due to the fact that some partitions may have

√

more blocks than others when the blocks are assigned
in a random fashion to the partitions. A partition has
N. Each level contains 2 · 2(cid:96)
levels (cid:96) = 0, . . . , log2
blocks (real and dummy blocks), except for the top level
√
that contains 2 additional blocks. Then, the maximum
N + 2 blocks.
size of a partition on the server is 4
N.
Therefore, the maximum server storage is 4N + 2
However, the maximum amount of server storage re-

√

quired is less than that, due to several reasons:

1. Levels of partitions are sometimes not ﬁlled. It is
extremely unlikely that at some point in time, all
levels of all partitions are simultaneously ﬁlled.

2. As soon as a block is read from a level, it can be
deleted by the server because its value is no longer
needed.

In our simulation experiments, we calculated that the
server never needs to store more than 3.2N blocks at any
point in time. Hence, in practice, the server storage can
be regarded as being less than 3.2N blocks.

B.3 Compressing Levels During Uploading

√

N +o(

√
In Line 18 of the WritePartition algorithm in Fig-
ure 7, the client needs to write back up to 2
N )
blocks to the server, at least half of which are dummy
blocks. Since the values of the dummy blocks are ir-
relevant (since the server cannot differentiate between
real and dummy blocks), it is possible to use a matrix
compression algorithm to save a 2X factor in terms of
bandwidth.
Suppose that the client wishes to transfer 2k blocks
b := (b1, b2, . . . b2k). Let S ⊆ {1, 2, . . . , 2k} denote
the offsets of the real blocks, let bS denote the vector
of real blocks. We consider the case when exactly k of
the blocks are real, i.e., bS is of length k (if less than
k blocks are real, simply select some dummy blocks to
ﬁll in). To achieve the 2X compression, the server and
client share a Vandermonde matrix M2k×k during an ini-
tial setup phase. Now to transfer the blocks b, the client
solves the linear equation:

MS · x = bS

where MS denotes the matrix formed by rows of M in-
dexed by the set S, and bS denote the vector B indexed
by the set S, i.e., the list of real blocks.

The client can simply transfer x (length k) to the
server in place of b (length 2k). The server decom-
presses it by computing y ← M x, and it is not hard
to see that yS = bS. The server is unable to distinguish
which blocks are real and which ones are dummy, since
the Vandermonde matrix ensures that any subset of k
values of y are a predetermined linear combination of
the remaining k values of y.

[9] E. Kushilevitz, S. Lu, and R. Ostrovsky. On the
(in)security of hash-based oblivious ram and a new
balancing scheme. In SODA, 2012.

[10] A. Oprea and M. K. Reiter.

Integrity checking
in cryptographic ﬁle systems with constant trusted
storage. In USENIX Security, 2007.

[11] R. Ostrovsky. Efﬁcient computation on oblivious

rams. In STOC, 1990.

[12] R. Ostrovsky and V. Shoup. Private information
storage (extended abstract). In STOC, pages 294–
303, 1997.

[13] B. Pinkas and T. Reinman. Oblivious ram revisited.

[14] M. Raab and A. Steger. Balls into bins – a simple

In CRYPTO, 2010.

and tight analysis, 1998.

[15] P. Williams and R. Sion. Usable PIR. In NDSS,

2008.

[16] P. Williams, R. Sion, and B. Carbunar. Building
castles out of mud: practical access pattern pri-
vacy and correctness on untrusted storage. In CCS,
2008.

Acknowledgments

We would like to thank Hubert Chan, Yinian Qi, and
Alina Oprea for insightful feedback, helpful discussions,
and proofreading.

This material is partially supported by the National
Science Foundation Graduate Research Fellowship un-
der Grant No. DGE-0946797, the National Science
Foundation under Grants No. CCF-0424422, 0311808,
0832943, 0448452, 0842694, 0627511, 0842695,
0808617, 0831501 CT-L, by the Air Force Ofﬁce of Sci-
entiﬁc Research under MURI Award No. FA9550-09-1-
0539, by the Air Force Research Laboratory under grant
No. P010071555, by the Ofﬁce of Naval Research un-
der MURI Grant No. N000140911081, by the MURI
program under AFOSR Grant No. FA9550-08-1-0352,
and by a grant from the Amazon Web Services in Educa-
tion program. Any opinions, ﬁndings, and conclusions
or recommendations expressed in this material are those
of the author(s) and do not necessarily reﬂect the views
of the funding agencies.

References

[1] Towards practical oblivious

Techini-
cal Report, http://arxiv.org/abs/1106.
3652, 2011.

ram.

[2] D. Boneh, D. Mazieres, and R. A. Popa. Remote
oblivious storage: Making oblivious ram practical.
Manuscript,
http://dspace.mit.edu/
bitstream/handle/1721.1/62006/
MIT-CSAIL-TR-2011-018.pdf, 2011.

[3] O. Goldreich. Towards a theory of software protec-
tion and simulation by oblivious rams. In STOC,
1987.

[4] O. Goldreich and R. Ostrovsky. Software protec-
tion and simulation on oblivious rams. J. ACM,
1996.

[5] M. T. Goodrich and M. Mitzenmacher. Privacy-
preserving access of outsourced data via oblivious
ram simulation. CoRR, abs/1007.1259, 2010.

[6] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko,
and R. Tamassia. Oblivious ram simulation with
In CCSW,
efﬁcient worst-case access overhead.
2011.

[7] M. T. Goodrich, M. Mitzenmacher, O. Ohrimenko,
and R. Tamassia. Privacy-preserving group data
access via stateless oblivious ram simulation.
In
SODA, 2012.

[8] M. T. Goodrich, O. Ohrimenko, M. Mitzen-
macher, and R. Tamassia.
Privacy-preserving
group data access via stateless oblivious ram sim-
ulation.
http://arxiv.org/abs/1105.
4125, 2011.

