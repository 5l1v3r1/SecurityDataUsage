Sharing the Cost of Backbone Networks

Cui Bono?

László Gyarmati!, Rade Stanojevic!, Michael Sirivianos†, Nikolaos Laoutaris!

!Telefonica Research, †Cyprus University of Technology

{laszlo,rade,nikos}@tid.es, michael.sirivianos@cut.ac.cy

ABSTRACT

We study the problem of how to share the cost of a backbone
network among its customers. A variety of empirical cost-
sharing policies are used in practice by backbone network
operators but very little ever reaches the research literature
about their properties. Motivated by this, we present a sys-
tematic study of such policies focusing on the discrepancies
between their cost allocations. We aim at quantifying how
the selection of a particular policy biases an operator’s un-
derstanding of cost generation.

We identify F-discrepancies due to the speciﬁc function
used to map traﬃc into cost (e.g., volume vs. peak rate vs.
95-percentile) and M-discrepancies, which have to do with
where traﬃc ismetered (per device vs.
ingress metering).
We also identify L-discrepancies relating to the liability of
individual customers for triggered upgrades and consequent
costs (full vs. proportional), and ﬁnally, TCO-discrepancies
emanating from the fact that the cost of carrying a bit is not
uniform across the network (old vs. new equipment, high vs.
low energy or real estate costs, etc.).

Using extensive traﬃc, routing, and cost data from a tier-1
network we show that F-discrepancies are large when looking
at individual links but cancel out when considering network-
wide cost-sharing. Metering at ingress points is convenient
but leads to large M-discrepancies, while TCO-discrepancies
are huge. Finally, L-discrepancies are intriguing and esoteric
but understanding them is central to determining the cost a
customer inﬂicts on the network.

Categories and Subject Descriptors

C.2.3 [Computer-Communication Networks]: Network
Operations; J.4 [Social and Behavioral Sciences]: Eco-
nomics

Keywords

cost sharing, backbone network, network economics, fairness

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.

1.

INTRODUCTION

After several years of growth during which Internet Ser-
vice Providers (ISPs) have enjoyed healthy proﬁt margins,
we are entering a new era in which margins are progressively
declining. This is due to intense competition that pushes
prices down, while at the same time traﬃc increases faster
than the ability of technology to reduce capital (CAPEX)
and operational (OPEX) expenses. The viability of an ISP
in such a competitive ecosystem will largely depend on its
ability to understand and manage its costs.

Understanding and optimizing the total cost of ownership
(TCO) of a network, i.e. both CAPEX and OPEX, is an
important aspect of network operations and therefore it has
received substantial attention from procurement, network
development, and network planning departments of large
telcos [23]. However, the impact of individual customers
on the cost of the network, which in the case of backbone
networks are other (smaller) networks and big enterprises, is
much less understood. There are multiple reasons for this,
including the diﬃculties of monitoring usage statistics for
each customer at each device, variable per device TCO, and
non-linearities of cost-capacity functions. There is also the
issue of which customer’s additional traﬃc demand forces
the operator to upgrade his network in order to maintain
the Service Level Agreement (SLA) for all customers, that
is, which customer is liable for a triggered upgrade and to
what extent.

Overcoming such challenges and quantifying in detail how
individual customers aﬀect the TCO of a network is crucial
and can be used for provisioning and operations purposes,
including routing and peering. It can also be used for creat-
ing better tariﬀ schemes, e.g., to give discounts to customers
inﬂicting low costs or justify premiums in the opposite case.
Tariﬀs are also inﬂuenced by additional factors including
regulation, competition, and demand, and thus their rela-
tionship to cost is not necessarily directly proportional. In
this paper we analyze a wide range of cost-sharing policies
used in practice by backbone operators and contrast their
outcome using extensive input data from a tier-1 backbone
network, including 590 ingress interfaces to large customers,
691 main backbone links, and 71 core routers spread over 3
continents. Our main objective is to see how the selection
of a certain cost-sharing policy impacts the picture that an
operator has about which customers generate high costs.

Our main contributions are:

• The development of a methodology for studying mul-

509tiple facets of cost-sharing in multi-resource environ-
ments such as a backbone network. 1

enough to capture both CAPEX and OPEX with appro-
priate parameterization.

• We analyze multiple cost-sharing policies of varying
complexity and accuracy under various settings. Our
analysis reveals large discrepancies among diﬀerent
policies when splitting the cost of an individual
network element among the customers that use it:
in 20% of cases the ratio of costs associated by two
diﬀerent policies is under 0.5 or over 2. This in turn
implies that operators must be careful
in selecting
among diﬀerent policies since their cost-allocation
outcomes may vary widely for individual devices or
small networks.

• The above is a negative result but luckily it becomes
less severe when one considers cost-sharing among all
In this
the devices that constitute a large network.
case, individual discrepancies at the device level cancel
out as one sums them up across a network. This in
turn points to an opportunity for selecting cost-sharing
policies from the least complicated side of the spectrum
without risking much sacriﬁce in terms of accuracy.

• Metering only at ingress links (which is the de-facto
norm in monitoring the customer’s usage) may greatly
under- or over-estimate the actual costs, depending on
the locality of the traﬃc and temporal characteristics
of the customer.

• We study additional aspects of cost-sharing related to
the issues of liability for triggered upgrades, and dis-
crepancies due to non-uniform TCO costs of diﬀerent
network elements.

The structure of the paper is as follows. In Section 2 we
give an overview on what determines the cost of a back-
bone network and present why the sharing of these costs
is a challenging task. We present our methodology in Sec-
tion 3, where we introduce four diﬀerent aspects that af-
fect the quantiﬁed costs of the customers. We describe our
datasets in Section 4. We investigate the costs of customers
and the severity of the diﬀerent types of discrepancies in Sec-
tion 5. Afterwards, we review the related work in Section 6
while we draw the conclusions of our work in Section 7.

2. BACKGROUND

What contributes to the cost of a backbone? The
cost of a network consists of CAPEX and OPEX for all de-
vices and Points-of-Presence (PoPs). The CAPEX is the
one-time cost paid whenever equipment is bought and in-
stalled [23]. It depends on the amount of traﬃc the device
must carry at a speciﬁc level of Quality-of-Service (QoS). A
key observation is that the capacity needed to guarantee a
certain QoS depends on the peak traﬃc that needs to be car-
ried. This is because for a given capacity, QoS is minimized
when the traﬃc peaks.

The OPEX corresponds to operational costs such as real
estate, energy, and personnel. It also depends on the amount
of traﬃc and the QoS; however, that dependence is more
elastic. The cost sharing policies we discuss are generic
1Our methodology is suﬃciently general to be applied in
settings beyond backbone networks, such as 3G networks,
datacenters, etc., but we do not examine such cases here.

Why is it diﬃcult to split cost among customers?
From the above discussion, one may conclude that splitting
the cost among customers is straightforward: for each device
of the network each customer should pay in proportion to
his contribution to the peak traﬃc carried by the device and
then sum up over all devices. Things, however, are not that
simple:

• Accounting complications. It is diﬃcult to know
for each network device the contribution of each cus-
tomer to its peak. This is because backbone opera-
tors need to measure and keep state at many points in
the network, which requires costly monitoring equip-
ments. In addition, computing traﬃc rates introduces
the problem of identifying the appropriate time-scale
for the computation owing to the limited resources of
the monitoring tools.

• Liability complications. If we were to build from
scratch a new network for a ﬁxed set of customers of
known demand, then the cost attributed to each cus-
tomer should be proportional to the sum of its con-
tributions to the peaks of individual devices. Split-
ting costs based on the contribution to the peak is
indeed exact, but only for this “oﬄine problem”. How-
ever, in reality, networks are not deployed as a single
event but grow organically with the addition of new
customers and the ramping up of their traﬃc. Un-
der this more realistic case, peak-based cost-sharing is
not guaranteed to be fair. Consider for example the
case in which a network is already operating at the
maximum utilization allowed by QoS constraints and
a small new customer triggers an expensive upgrade
that leads to a new network with plentiful unallocated
capacity (upgrades typically involve large jumps, e.g.,
1Gbps to 2.5Gbps, to 10Gbps, etc.). Peak-based cost
sharing would attribute to the new customer only a
small fraction of the overall cost. Is that fair? The an-
swer depends on what happens with the unallocated
capacity.
If the network can easily sell it to new or
existing customers then indeed it is fair. If, however,
selling this leftover capacity is not guaranteed, then
the new customer may have a larger liability for the
upgrade costs.

For the above reasons, we will present a methodology in
which we ﬁrst compare among policies of varying complexity,
and then switch to comparing among policies that assign
diﬀerent liability levels.

3. METHODOLOGY

In this section, we present a thorough classiﬁcation of the
discrepancies between methods when a backbone operator
quantiﬁes the costs that its customers inﬂict on the network.
By discrepancy we mean the diﬀerence in a customer’s costs
according to two cost sharing policies. Each type of discrep-
ancy reveals a separate facet of the challenge of customer
cost quantiﬁcation. These facets relate to the following ques-
tions:

• How does the backbone operator compute the cost of

the customers?

510• What kind of traﬃc metering does the backbone op-

erator apply in the network?

• Which customers are liable for the incurred costs?
• How diverse are the costs of the components of the

network?

Before presenting the taxonomy of discrepancies, we ﬁrst
introduce the metric we use to quantify the discrepancies of
a pair of cost sharing procedures. Let N denote the set of
customers who utilize resources in the network. Let A and B
denote the sets of costs allocated to each customer using two
diﬀerent cost sharing policies. It holds that |A| = |B| = |N|.
Accordingly, ai ∈ A denotes the cost of customer i ∈ N
quantiﬁed based on the ﬁrst cost sharing policy while bi ∈ B
represents customer i’s cost based on the second policy. We
deﬁne the discrepancy of the costs of customer i as

d(ai, bi) = max! ai

bi

,

bi

ai"

• Volume-Customer. We measure the amount of
data that a single customer sends on a speciﬁc net-
work device (e.g., on a single link) for the whole an-
alyzed time period. Afterwards, we share the cost of
the device proportionally to the traﬃc volumes of the
customers using it. Hence, the cost of customer i for
device l is:

cl

i = cl · &t∈T xl
&j∈N&t∈T xl

i(t)

j(t)

• 95Percentile-Customer. We distribute the cost
of the device proportional to the 95th percentile [12]
of the customers’ traﬃc that traverses the particular
device:

cl
i = cl ·

P95’. . . , xl
&j∈N P95’. . . , xl

i(t), . . .(
j(t), . . .(

(1)

where P95() denotes the 95th percentile of the argu-
ments.

(3)

(4)

(5)

(6)

We use this measure of discrepancy because it describes the
relation of the costs with a simple, comprehensible value.
We use several statistics of the customers’ individual dis-
crepancies to quantify the discrepancy of two cost sharing
policies including the 95th percentile and the median.

• Peak-Customer. Under this policy, we share the
expenditure of the network device proportional to the
customers’ maximum usage volumes for the given time
interval:

We now describe how we determine the aggregate cost
that the sharing policies distribute. A network consists of
various network devices, such as routers and links. Let L
denote the set of devices of the network. Let xl
i(t) denote
the traﬃc volume of customer i ∈ N on network device
l ∈ L during the time interval t ∈ [1, T ]. Furthermore, let cl
denote the cost of network device l ∈ L.
The cost of a speciﬁc device depends on the maximum
amount of traﬃc that it has to carry during a certain time
interval. Thus, we obtain cl by examining the available ca-
pacity rates of the device (e.g., 1 Gbps, 10 Gbps, etc.) and
then using the cost of the smallest device whose capacity
satisﬁes the requested Service Level Agreement (SLA) for
the given traﬃc demand. We assume that the backbone
operator fulﬁlls its SLA by upgrading its devices when uti-
lization hits the 50% threshold. To this end, we assume that
the costs follow a step function C : R → R. Thus, the cost
of device l is

cl = C#max
t∈T $i∈N

i(t)%

xl

(2)

3.1 F-discrepancies

The ﬁrst source of discrepancies between cost allocation
methods is the function that the backbone operator uses
to compute the contribution of the customers to the aggre-
gate cost. We next present four policies that strike diﬀerent
balances between precision and resource needs, which we
discuss in more details at the end of this section. We con-
sider these methods because backbone operators apply some
of these policies (e.g., the 95Percentile-Customer and the
Aggregate-Peak-Device) in practice to determine the costs a
customer inﬂicts and consequently the price the customers
pays. For example, one can easily map some of the tar-
iﬀs (e.g., based on the purchased raw capacity or on the
95th percentile of the traﬃc) used in practice to the in-
troduced policies (e.g., Volume-Customer and 95Percentile-
Customer).

i = cl ·
cl

i(t)

maxt∈T xl

&j∈N maxt∈T xl

j(t)

• Aggregate-Peak-Device.

Backbone operators
plan the capacity of the network based on the max-
imum utilization, e.g., the 50% of the capacity of a
device is larger than the expected maximum of the
traﬃc that traverses it. Accordingly, we distribute
the cost of the devices based on the contribution of
individual customers to the peak utilization. Assum-
ing that the peak utilization of device l happens at
j(t), we allocate the

following cost to customer i as:

time step tm = arg maxt&j∈N xl
&j∈N xl

cl
i = cl ·

xl
i(tm)

j(tm)

We evaluate F-discrepancies by comparing a policy with
Aggregate-Peak-Device, which shares the costs in a fair way
when it is guaranteed that new unallocated capacity from
an upgrade will soon ﬁnd a customer to amortize it. The
F-discrepancies of the policies arise from the misalignment
of traﬃc peaks: the peak of a customer’s traﬃc may not
coincide with the peak of the aggregate traﬃc the device
carries.

Illustrative example:
Let us assume that two cus-
tomers utilize device l1 with the time-series depicted in
Fig. 1. Based on the time-series we compute the costs
of the customers. The percentages of the cost that cus-
tomer 1 covers are 69.9%, 56.2%, 60%, and 53.3% of
the total cost of the device for the Volume-Customer,
Peak-Customer, 95Percentile-Customer, and Aggregate-
Peak-Device policies, respectively. For example, in case of
the 95Percentile-Customer policy, customer 1 has a traﬃc of
0.9 Gbps while the cumulative traﬃc is 1.5 Gbps resulting
in 60% cost share. The main cause behind the discrepancies
of the costs are the misalignment of the customers’ peak,

511and that the diﬀerent policies consider diverse parts of the
time-series to compute a value that describes the traﬃc of
the customer.

Figure 1: Illustrative example for F-discrepancies.
The misalignment of the customers’ peak causes cost
diﬀerences across policies.

The introduced policies quantify the cost of the customers
using diﬀerent functions on a per device basis. The F-
discrepancies of the customers emerge at two diﬀerent levels:

• Device-level discrepancies. We compute sepa-
rately for each network device the discrepancy among
diﬀerent policies based on the customers’ costs. In this
case, the set of costs is

)cl
i | ∀i ∈ N* , l ∈ L

and has cardinality |N||L|.
For example, the F-
discrepancy of customer i in case of policies a and b is
d(al

i, bl

i).

• Network-level discrepancies. We ﬁrst summarize
the costs of a customer over all the devices of the net-
work, i.e., we compute the total cost of each customer.
Afterwards, we compute the discrepancies of the poli-
cies. In this case, the set of the costs over which we
compute the discrepancies is

(7)

(8)

i | ∀i ∈ N,

cl

+$l∈L
i,&l∈L bl
i).
3.2 M-discrepancies

d(&l∈L al

and has cardinality |N|.
cies a and b the F-discrepancy of customer i

For example,

for poli-
is

The traﬃc metering method is the second source of dis-
crepancies. The resource requirements of the traﬃc moni-
toring tools depend on the resolution of metering. The main
cause behind the M-discrepancies is the trade-oﬀ that back-
bone operators face:
increasing the precision of the meter-
ing improves the validity of the quantiﬁed cost, however,
this comes with an elevated cost for traﬃc monitoring. We
study the two corner cases of traﬃc metering:

• Customer-Ingress.

Each customer has several
ingress devices through which it injects its traﬃc to
the network. The backbone operator keeps track of
the customers’ usage solely on the ingress devices.
This is the least expensive metering method. The
operator uses the ingress traﬃc time-series to share
the network-wide expenditures among the customers.

• Customer-per-Device.

If the backbone operator
deploys more advanced network monitoring tools, it
can capture the time-series of the customers not only
on the ingress devices but on all the devices located
in the network. This is the most expensive metering
method and is typically done using NetFlow technol-
ogy, which comes at a high procurement and adminis-
tration cost. Metering the actual traﬃc on each net-
work device allows the backbone operator to compute
the costs of the customers based on the device speciﬁc
time-series. Therefore, the backbone operator faces a
trade-oﬀ: more accurate expenditure sharing vs. more
cost eﬃcient operation.

We deﬁne the M-discrepancies as follows. First, we com-
pute the cost of customer i on each device l using a given cost
allocation function (e.g., based on the Volume-Customer
policy of Section 3.1), and we compute the network-level
i. Second, we compute using
the given cost allocation function the customer’s share (c∗i )

traﬃc time-series of the customers. The total ingress traﬃc
xl
i(t) where Ii denotes the
set of ingress devices that customer i has. Accordingly, the
M-discrepancy of customer i is

cost of customer i as &l∈L cl
of the network’s total cost (c = &l∈L cl) using the ingress
of customer i is x∗i (t) = &l∈Ii
di#$l∈L

i, c∗i%

(9)

cl

where di is our metric of discrepancy (Eq. 1).

Illustrative example: Let us now assume that the ingress
traﬃc of the customers is as we show in Fig. 2. The backbone
network consists of two devices: L1 on which the traﬃc of
the customers is as depicted in Fig. 1 and L2 which is solely
utilized by customer 1 with a constant traﬃc of 1 Gbps. For
illustration purposes, we separate the two ﬂows of customer
1, one on L1 and the other on L2, with a dashed line in the
Fig. 2. Because the traﬃc on device L2 is modest it can
be transmitted on a 2.5 Gbps device while the capacity of
L1 should be 10 Gbps. The diverse device capacities imply
diverse costs as well. In the case of ingress metering, i.e.,
sharing the cost of the network just based on the aggregate
traﬃc shown in Fig. 2, the cost of customer 1 is 83.9%,
73.1%, 76%, and 72% of the cost of the whole network for the
Volume-Customer, Peak-Customer, 95Percentile-Customer,
and Aggregate-Peak-Device policies, respectively. However,
if we measure the traﬃc of the customers on all the devices
then customer 1’s shares of costs are 75.9%, 65%, 68%, and
62.7%. If we compare these cost fractions we encounter large
discrepancies caused by the level of the metering.

3.3 L-discrepancies

The third type of discrepancies is caused by the diﬀerent
types of customer liability as discussed in Section 2. We will
examine the following policies:

• Aggregate-Peak-Device. This is the already in-
troduced policy that is the measure of fairness when
the customer liability is proportional to the aggregate
peak of devices.

• Trigger. With this policy, the backbone operator allo-
cates the cost of the device exclusively to the customer

5122 customer leaves from P at t1 then the peak will
move to t2 and X will go from paying 90% to paying
only 1% after a tiny 2 perturbation of the aggregate
traﬃc. On the contrary, the Shapley policy is aware
of such situations as it takes into account all the local
maxima of the aggregate traﬃc in quantifying the
costs of the customers.
Under the Shapley policy, the cost of each customer
is proportional to its average marginal contribution to
the device’s total cost. Particularly, let us consider
all the possible S ⊂ N subsets (coalitions) of the cus-
tomers who utilize resources of the network device l.
The cost of coalition S depends on the aggregate traf-
ﬁc volume of the participants, i.e., it is equal to the
cost of a network device that has suﬃcient capacity:

vl(S) = C#max
t∈T $j∈S

j (t)%

xl

(11)

Based on the v cost function of the coalitions, we com-
pute the Shapley value of customer i as

φi(vl) =

1

N! $Π∈SN-vl (S (Π, i)) − vl (S (Π, i) \ i).

(12)
where Π is a permutation of arrival order of the set
N and S(Π, i) denotes the set of players who arrived
no later than i. The (φ1(v), . . . ,φ N (v)) Shapley values
describe the fair distribution of costs in the case of the
S = N grand coalition. Fair in a way that it satisﬁes
four intuitive fairness criteria [1, 13, 22]. We quantify
the cost of customer i based on its Shapley value for
the device l as

cl
i = cl ·

φi(vl)

&j∈N φj(vl)

(13)

While computing the aggregate traﬃc volumes of the
coalitions, we assume that the routing inside the net-
work is static, i.e., removing some traﬃc from the net-
work device does not aﬀect the traﬃc volumes of other
customers (e.g., the backbone operator does not apply
load balancing mechanisms).

Illustrative example: We present the traﬃc patterns
of two customers and the thresholds where the capacity of
the device needs to be upgraded in Fig. 3. Customer 1 is
liable for 53.3% and 87.5% of the cost of the device in case of
the Aggregate-Peak-Device and Shapley policies. The peak
of the aggregate traﬃc happens in a time step where the
customers’ traﬃc volumes are balanced. Although there are
local maxima where the traﬃc of customer 2 is small, it is
not considered by the Aggregate-Peak-Device policy. From
a Shapley policy viewpoint, the traﬃc peak of customer 1
is too large to be transmitted with a lower-capacity device,
i.e., its traﬃc is mainly responsible for the total cost of the
device. If we assume that customer 1 arrived ﬁrst it causes
100% of the costs according to the Trigger policy because its
peak needs a larger-capacity device whose leftover capacity
can be used by customer 2 afterwards.

Customers can have both device- or network-level L-
discrepancies, depending on whether we consider the costs

Figure 2: Illustrative example for M-discrepancies:
metering traﬃc only at the ingress link causes cus-
tomer 1 to have a larger share of the cost than if we
meter at all devices.

that triggered the capacity upgrade. This policy is ap-
plied when the backbone operator is not conﬁdent that
it can sell the newly obtained but unallocated capac-
ity.2

To this end, the backbone operator utilizes the histor-
ical traﬃc patterns of the customers and their arriving
order. For example, the cost of the ﬁrst customer is
equal to the cost of the device that is capable to trans-
mit his traﬃc demand. We assume that the customers
are numbered based on their arriving order while ti
denotes the time when the customer started to use the
network. Accordingly, the cost of customer i in case of
the Trigger policy is

cl
i = C(max

t≤ti $j∈{N|j≤i}

xl
j (t))− C(max

t≤ti $j∈{N|j<i}

xl
j (ti))

(10)

The main drawbacks of Trigger are: a) it assigns cost
only to the customer whose traﬃc trigger upgrades and
0 to everyone else, therefore order of arrival can have a
huge impact on the costs attributed to a customer; and
b) it is diﬃcult to compute Trigger since it requires ex-
tensive historical data on the order of customer arrival
and traﬃc build up.

• Shapley.

The Shapley cost sharing policy lies
the
between the two above presented extremes;
It
Aggregate-Peak-Device and Trigger policies.
assigns to customers partial
liability for upgrades,
thereby avoiding the all-or-nothing assignments of
Triggers. Therefore it is less strict than Trigger
but more strict than Aggregate-Peak-Device since it
assigns “averaged” liabilities rather than proportional
liabilities based on a single time interval when a device
peaks.

the Shapley over

The main advantage of
the
Aggregate-Peak-Device policy is that its allocations
are more stable than that of the Aggregate-Peak-
Device policy in view of customer churn. For example,
let us imagine that the aggregate peak traﬃc of a
device is P and appears at t1. We also assume that
the device at time t2 has load P − . Now let us
suppose that customer X is responsible for 90% of
P at t1 and 1% of P −  at t2. Then if a small
2Recall that upgrades generally involve large jumps that can
leave substantial unallocated capacity.

5133.5 Discussion

The cost sharing methods introduced strike various trade-
oﬀs in terms of computational complexity, amount of re-
quired information, and accuracy. The Volume-Customer,
Peak-Customer, Aggregate-Peak-Device, and 95Percentile-
Customer policies require the least computational resources
due to their sum, maximum, and percentile computations.
The Trigger method determines when the device upgrade
thresholds are surpassed. Finally, the Shapley policy has
the largest complexity as it computes the costs based on the
sub-coalitions of the customers. For computational reasons,
we consider the 15 largest customers per network device in
our evaluations to quantify the Shapley costs. On average,
these customers cover 96% of the traﬃc of the devices.

In terms of the amount of information, all the policies ex-
cept the Shapley and Trigger are similarly modest. They
utilize single values for the historical usage (sum and max-
imum, respectively) and the current traﬃc volumes. How-
ever, the Shapley policy uses the whole time-series of traﬃc
volumes to compute the costs. The information need of the
Trigger policy is even larger as it needs both the historical
traﬃc volumes of the customers and their arriving orders.

From a fairness point of view, the Aggregate-Peak-device
policy is the absolute measure of fairness when the customer
liability is proportional to the aggregate peak of devices
(Section 3.3). Otherwise, the liability of the triggered net-
work upgrade should be considered. The Shapley policy lies
in between the two extreme policies because: a) unlike Trig-
ger, it does not allocate the cost only to the late-comers but
to the previous customers as well by considering all possible
orders of arrivals; and b) unlike Aggregate-Peak-Device, it
does not consider only the time instance of the peak utiliza-
tion.

We postpone the analysis of the cost sharing policies’ ac-
curacy to Section 5 where we present a thorough investi-
gation of the policies based on dataset-driven evaluations.
Our comparative study has the following structure. We ﬁrst
compare the Aggregate-Peak-Device policy with other more
practical, albeit less accurate policies. Then, we compare
the Aggregate-Peak-Device policy with the Shapley policy
and comment on how much error is introduced ignoring the
local peaks.

4. DATASETS

We use several datasets from a tier-1 backbone network,
which interconnects with other ISPs that it serves. In our
dataset, the network consists of 26 Points of Presence (PoPs)
and each customer connects to the network at one or more
PoPs through one or more interfaces. Overall there are 590
ingress links, and each ingress link is used by exactly one
customer. Internally, the backbone has 71 routers and 691
links, which are typically used by more than one customer.
We collected detailed NetFlow-based statistics for each of
the internal and ingress links including the traﬃc volumes
to and from each customer on every link in the network.
Such information allows us to assess which customer uses
which component in the shared infrastructure and how it
aﬀects the load on each component.

The per-link and per-customer traﬃc statistics cover the
period from 18 March 2012 to 10 April 2012, with a 2-
hour granularity (i.e., reporting volumes sent and received
within 2 hours). We have two additional datasets contain-

Figure 3: Illustrative example for L-discrepancies

of the customers on particular devices (e.g., cl

i) or on the

aggregate (e.g.,&l∈L cl
3.4 TCO-discrepancies

i).

The ﬁnal class of discrepancies is related to the Total Cost
of Ownership (TCO) of diﬀerent devices of the network.
Due to the heterogeneous nature of the network—caused by
the geographic and technological diﬀerences of its parts—
the same traﬃc patterns imply diverse expenditures for the
backbone operator on diﬀerent devices. Therefore, addi-
tional discrepancies occur when we consider the TCO of the
network in more detail. The following levels of TCO impact
the costs and the discrepancies of the customers:

• Pieces of costs. Even if the capacity of two partic-
ular equipment are equal, their costs can vary signiﬁ-
cantly due to technology diﬀerences (newer vs. older
generation), location (cost of shipping), diﬀerences in
purchase price, etc..

• Point-of-Presence (PoP) costs.

The backbone
network operator deals with diverse costs at each geo-
graphic location where it has a presence. The causes
behind the varying costs include but are not limited to
the following factors: energy (e.g., the energy price in
Germany can be twice as much as in the UK), facil-
ity costs (e.g., the rental cost of oﬃce space in Hong
Kong can be four times higher than in Germany [9]),
taxation, and personnel costs.

Contrary to the former types of discrepancies, in the case
of the TCO only network-level discrepancies exist. At the
network level, where we summarize the costs of the cus-
tomers across all the devices, additional discrepancies ap-
pear due to the diverse costs of the equipments.

Formally, we deﬁne the network-level TCO-discrepancy of

customer i as

(14)

d#$l∈L

cl

i,$l∈L

i%
cl · &l∈L el
&l∈L&i∈N el

i

where the ﬁrst term considers the diverse costs of the devices
contrary to the second. el
i denotes the cost of customer i in
case of device l assuming uniform cost across all the devices
(el = e∗,∀l ∈ L).
Illustrative example:
If we consider the traﬃc pattern
of the customers in Fig. 1 but we assume that the cost of the
investigated device is ﬁve times more than before, e.g. it is lo-
cated in a developing country, the costs of the customers also
increase by a factor of 5. Accordingly, the TCO-discrepancy
is 5 in the case of all the customers.

514ing time-series with a 5-min and 30-min granularity for one
day and one week, respectively. The traﬃc aggregated over
all ingress links peaks at around 1.35 T bps in both inbound
and outbound directions. Thus, without loss of generality,
we utilize the time-series of the customers’ incoming traﬃc.
The cost of a network link depends on the one hand on the
capacity of the interface, i.e., how much traﬃc it is capable
of forwarding. On the other hand, the geographic location
and the applied technology have an impact as well. Hard-
ware costs, energy prices, deployment costs, and taxation,
among others, contribute to the cost of a network device.
Thereby, it is challenging to accurately quantify the cost of
every single device.

To estimate the cost of the network links, we use the
wholesale point-to-point transport price database of Tele-
Geography [29]. We stress that these are the prices of whole-
sale physical layer circuits, however, do not diﬀer substan-
tially from the actual cost of ownership. In our empirical
analysis, we apply the prices of network links with diﬀerent
bandwidth, ranging from E-1 (2 Mbps) throughout STM-4
(622 Mbps) and 2.5G waves to 40G waves (40000 Mbps).
The costs of these links deﬁne a step function for the net-
work expenditures. Exact values can be provided to inter-
ested parties if conﬁdentiality requirements are met. As
future work, we intend to extend our analysis using more
detailed expenditure datasets that contains information on
the OPEX costs related to for example power supply, host-
ing centers, etc..

5. DATA-DRIVEN EVALUATION

In this section we use the datasets introduced in Section 4
to evaluate the various discrepancies discussed in Section 3.
In case of the F-, M-, and L-discrepancies, we use a uniform
cost function for the network devices to focus on the speciﬁc
properties of cost-sharing.

5.1 F-discrepancies

We start by looking at the eﬀect of the function applied

to the traﬃc of a customer.
5.1.1 Device-level F-discrepancies
To showcase the intricacies of F-discrepancies we start
with an example based on a backbone link between two
major PoPs in Europe. The monthly cost of this link is
$2163. In Fig. 4 we plot the amount of this cost attributed
to each one of the 10 largest customers according to the four
diﬀerent policies detailed in Section 3.1. The F-discrepancy,
i.e., the ratio of the cost computed by the Aggregate-
Peak-Device policy and the cost computed by the simpler
policy X∈ [Volume-Customer,
95Percentile-Customer,
Peak-Customer] is as high as 2.36 for customer 4 in this
example. This particular customer impacts the aggregate
peak of the device disproportionally more than the other
customers when we focus on the traﬃc volumes of the
customers. For several other customers the F-discrepancies
are much milder, i.e., the diﬀerent cost-sharing policies are
more or less in agreement.

We now look at F-discrepancies across all customers
and all links in our dataset.
In Fig. 5, we plot the the
F-discrepancies for the three simpler policies and we
summarize the main statistics in Table 1. The results show
generally high F-discrepancies. For example, 60% of the
customers are assigned 25% higher or lower cost than the

Figure 4: Device-level cost of customers for varying
cost-sharing policies. Link between two European
PoPs.

Method

Volume-Customer

Peak-Customer

95Percentile-Customer

>25% 95th percentile median
1.372
0.592
1.263
0.520
0.508
1.259

63.07
89.55
80.79

Table 1: Device-level F-discrepancies compared to
the Aggregate-Peak-Device policy.

real one they inﬂict according to Aggregate-Peak-Device.
F-discrepancies are particularly high for Volume-Customer
and smaller for Peak-Customer and 95Percentile-Customer.
The last two policies are sensitive to peaks, albeit those
of particular customers instead of peaks of the aggregate
traﬃc on the device. Volume-Customer is even less accurate
since it is not looking at any peaks, but only at aggregate
volume over a longer time scale.

One may expect that there is a strong correlation between
the traﬃc volumes and the discrepancies of the policies. For
example, the other policies may always overestimate the cost
of customers, compared to the Aggregate-Peak-Device, if
the customers inject a large amount of traﬃc. However, our
results refute this as we illustrate in Fig. 6, where we present
the F-discrepancies as a function of the traﬃc volumes.

To illustrate the diﬀerences between the policies, Fig. 7
depicts a portion of the time-series of a link where a large F-
discrepancy (3.07) exists between the Volume-Customer and
the Aggregate-Peak-Device policies. The ﬁgure shows the
traﬃc pattern of the customer with the large F-discrepancy
and the aggregate traﬃc pattern of the other customers. The
traﬃc of the customer is marginal compared to the traﬃc
of the others, yielding a very low Volume-Customer cost.
However during the peak, the customer with the large dis-
crepancy contributes a signiﬁcant portion to the aggregate
traﬃc, thereby inducing a 3.07 times higher Aggregate-Peak-
Device than Volume-Customer cost.

Summary and implications: In the case of device-level
discrepancies, numerous and substantial F-discrepancies ex-
ist. This implies that backbone operators should apply the
Aggregate-Peak-Device policy for computing the costs in
case of a single device instead of the simpler policies.

515(a) Volume-Customer

(b) Peak-Customer

(c) 95Percentile-Customer

Figure 5: Distribution of the device-level F-discrepancies between the simpler cost-sharing policies and the
Aggregate-Peak-Device policy. Distributions are based on the percentage of the customers and the traﬃc
(insets). All the policies have high F-discrepancies, especially the F-discrepancies of the Volume-Customer
policy are large.

(a) Volume-Customer

(b) Peak-Customer

(c) 95Percentile-Customer

Figure 6: There does not exist a strong correlation between the traﬃc volumes and the discrepancies of the
policies. Moreover, there is no signiﬁcant diﬀerence between the over- and under-estimation of the costs.
The axes are logarithmically scaled.

5.1.2 Network-level F-discrepancies
We now examine F-discrepancies in the context of the
entire network. We do this by summing the costs of a cus-
tomer over all the network’s devices. We present the rela-
tive aggregate costs of the 10 largest customers in Fig. 8;
we consider the largest cost as the baseline. We present the
F-discrepancies of the policies in Table 2. The results reveal
that F-discrepancies at the network-level are much smaller
than at the device-level. For example, the network-level
median F-discrepancies are ∼40% less than the device-level
ones. This is because in large networks positive and neg-
ative cost diﬀerences at each device cancel each other out,
thus the cost predictions of the simpler policies become more
aligned.

Summary and implications: F-discrepancies although
important for individual links or small networks tend to
become less signiﬁcant for larger networks. Thus, backbone
operators can use simpler policies than the Aggregate-
Peak-Device without running a high risk of miscalculating
the costs of the customers if they are just interested in the
aggregate costs of the customers.

Method

Volume-Customer

Peak-Customer

95Percentile-Customer

>25% 95th percentile median
1.251
1.151
1.181

3.141
12.71
5.046

0.5
0.35
0.37

Table 2: Network-level F-discrepancies compared to
the Aggregate-Peak-Device policy.

5.2 M-discrepancies

Next we compute the discrepancy between the customer’s
network-level cost derived by (1) metering its traﬃc at its
ingress links (Customer-Ingress or CI) and (2) metering its
traﬃc on each device that the customer uses (Customer-
per-Device or CD). All of the policies result in high M-
discrepancies (ratios as high as 34) as summarized in Ta-
ble 3.

Up to this point, we analyzed the impact of diﬀerent
discrepancies separately. Next, we quantify the joint ef-
fect of F-discrepancies and M-discrepancies, i.e., how large
can the diﬀerence be between the most and the least accu-
rate combination of function and metering schemes. We do
this by comparing the network-level costs of customers un-
der the Volume-Customer+CI, Volume-Customer+CD, and

516Method

Volume-Customer

Peak-Customer

95Percentile-Customer
Aggregate-Peak-Device

>25% 95th pct median
1.543
0.695
0.752
1.738
1.630
0.750
0.763
1.801

34.53
32.34
19.10
28.52

Table 3: Network-level M-discrepancies of the cost-
sharing policies. Comparison of the Customer-
Ingress and the Customer-per-Device costs of the
customers.

Method

Volume-Customer+CI

Aggregate-Peak-Device+CI

Volume-Customer+CD

Aggregate-Peak-Device+CD

>25% 95th pct median
1.816
0.760
1.801
0.763
1.251
0.500
0.0
1.0

32.69
28.52
3.141
1.0

Figure 7: Time-series of the traﬃc of a customer
with a large (3.07) F-discrepancy on a single link.
Volume-Customer vs. Aggregate-Peak-Device pol-
icy.

Table 4: Discrepancies with the Aggregate-Peak-
Device policy using Customer-per-Device (real
traﬃc) metering (CI – Customer-Ingress, CD –
Customer-per-Device).

Summary and implications:
The level at which the
backbone operator meters the traﬃc of the customers has
a large impact on the quantiﬁed costs. Based on the me-
dians, the most and the least accurate policies diverge by
80%. Therefore, the backbone operators should apply so-
phisticated metering strategies (e.g., network-wide deploy-
ment of NetFlow-capable traﬃc monitoring devices) in order
to accurately quantify the costs of the customers. Moreover,
the simple methods are no longer aligned with the real cost
of the customers (i.e., with the Aggregate-Peak-Customer
policy) if the traﬃc is metered on the ingress links. From
an accuracy point of view, this implies that backbone op-
erators should reconsider the pricing of IP transit services,
which they currently price based on simpler policies such as
the 95Percentile one.

5.3 L-discrepancies

Next, we focus on the L-discrepancies. Out of the three
policies described in Section 3.3, one, the Trigger policy,
requires historic information on customer arrival events as
well as customer traﬃc information on long time scales that
relate to network upgrade events. Since we do not have full
historic information on all the links, we approximate the
Trigger policy as follows. We assume for each customer he
was the last one arriving to the network. Then we compute
the marginal cost contribution of the customer as the actual
cost of the device minus the cost of the device without the
traﬃc of the customer. Formally, we quantify the marginal
contribution of customer i as:

ml

i = C#max
t∈T $j∈N

xl

j (t)% − Cmax

t∈T $j∈N\{i}

xl

j (t) (15)

Finally, we allocate the cost of the device to the customers
in proportion to their marginal contributions:

i = cl ·
cl

ml

i&j∈N ml

j

In the following, we refer to this method as Trigger*.

(16)

Figure 8: Aggregate relative costs of the 10 largest
customers; comparison normalized by the largest
cost.

Aggregate-Peak-Device+CI policies with the nominally ac-
curate one, namely, the Aggregate-Peak-Device+CD pol-
icy. The results are summarized in Table 4. The Volume-
Customer policy has the smallest M-discrepancy, that is, the
median ratio of the Customer-Ingress and the Customer-per-
Device costs is 1.5. On the contrary, the Aggregate-Peak-
Device policy yields the largest M-discrepancies. The rea-
son behind this is twofold. First, when metering traﬃc at
the ingress links, traﬃc that results in peaks at individual
links does not result in peaks of the aggregate ingress traf-
ﬁc. Second, under Customer-Ingress, the Aggregate-Peak-
Device policy takes into account only the time interval with
the largest aggregate traﬃc while the peaks of the internal
devices may happen in other time intervals neglected by the
Aggregate-Peak-Device+CI policy. We observe that under
the Aggregate-Peak-Device+CI combination, the costs di-
verge by at least 25% for 76% of the customers. In addition
we note that under the Volume-Customer+CI policy and
metering, the discrepancy can be as high as 32.

517Method >25% 95th percentile median
1.497
Shapley
Trigger*
2.475

0.674
0.861

472.4
1188

Table 5: Device-level L-discrepancies compared to
the Aggregate-Peak-Device policy.

5.3.1 Device-level L-discrepancies
We present the L-discrepancies in Table 5 by computing
the ratio between X∈ [Trigger*,Shapley] and the Aggregate-
Peak-Device policy. L-discrepancies are quite high (ratios up
to 1180) pointing to the fact that liability can bias signiﬁ-
cantly the cost-sharing picture that a telco has. For example,
if we compute costs based on the Trigger* and based on the
Aggregate-Peak-Device policy, the gap between the two is
very large: in more than 85% of the cases the L-discrepancy
is larger than 25%. The Trigger* policy allocates the cost
in a full-liability fashion, while the Aggregate-Peak-Device
policy applies a proportional liability scheme. Our empiri-
cal results conﬁrm that the Shapley policy, which estimates
the customers’ average contribution to the capacity upgrade
considering every possible arriving order,
lies in between
these two extremes. Again, the diﬀerence between the costs
of the Shapley and the Aggregate-Peak-Device policy is sub-
stantial: the median ratio of the costs is 1.5; however, in
some cases the ratio can be larger than 400.

We further examine the L-discrepancy between the
Aggregate-Peak-Device and the Shapley policies. In Fig. 9
we plot the L-discrepancies for all customers and all links as
a function of the traﬃc volumes, which the customers had
on the particular device in the analyzed time period. The
magnitude of the L-discrepancy is inversely proportional
to the traﬃc volumes. Thus, smaller customers tend
to have larger L-discrepancies. Small customers usually
do not signiﬁcantly inﬂuence the peak utilization of the
devices, i.e., they have some marginal share of the costs.
However, they may trigger a capacity upgrade of the link
and therefore have a larger share of the costs because of
the link’s step-based cost function. The ﬁgure also reveals
that whether the Shapley policy over- or under-estimates
the Aggregate-Peak-Device policy’ costs is not signiﬁcantly
inﬂuenced by the customers’ traﬃc volume or by the
magnitude of their L-discrepancies.

We present in Fig. 10 a part of the time-series of a cus-
tomer with a large L-discrepancy (3.25) along with the ag-
gregate time-series of the other customers who utilize the
same link. The dashed horizontal lines denote the traﬃc vol-
umes where the capacity of the link needs to be upgraded.
The traﬃc of the customer is small enough to be transmit-
ted over a link with lower capacity. However, the traﬃc of
the other customers pushes the link to have larger capac-
ity and thus larger cost. The Shapley policy considers this
fact when it computes the average marginal contribution of
the customer. As a result, the cost of the customer is less
than if we compute it based solely on time of the largest
utilization of the device. On the contrary, the Aggregate-
Peak-Customer focuses only on the time-interval when the
link has its aggregate peak. The particular customer has
signiﬁcant share of the aggregate peak and thus of the cost
of the link according to the Aggregate-Peak-Customer. This
however masks who is responsible for the link’s larger capac-
ity. However, this single time-interval masks the fact that

Figure 9: Device-level L-discrepancies as a function
of the traﬃc volumes. The Shapley policy both over-
or underestimates the Aggregate-Peak-Device pol-
icy.

Method >25% 95th percentile median
1.316
Shapley
Trigger*
1.767

179.3
231.2

0.54
0.771

Table 6: Network-level L-discrepancies compared to
the Aggregate-Peak-Device policy.

the speciﬁc customer is not responsible for the link’s needing
a larger capacity.
5.3.2 Network-level L-discrepancies
We show the network-level L-discrepancies in Table 6. At
the network level, the number and the magnitude of the
L-discrepancies is smaller than at the device level. Never-
theless, for more than 50% of the customers the costs are oﬀ
by at least 25%. The median L-discrepancies of the policies
are notable too, e.g., 1.3 under the Shapley policy.

Summary and implications: The liability of network up-
grades plays an important role in the quantiﬁcation of the
costs of customers in backbone networks. The median value
of L-discrepancies is at least 1.3 while the L-discrepancies
impact more than half of the customers with at least 25%.
The implication of the results is that if the backbone network
is not built in one-shot but is rather organically grown and
upgraded then the Aggregate-Peak-Customer policy may in-
duce cross-subsidization problems: customers may be ac-
counted for costs of upgrades for which they are not li-
able (or not in that degree). From a customer point of
view, this cross-subsidization may not be tolerated in a long-
run given the competitive environment of the backbone net-
works. That is the customers may select other backbone
network operator where they are not liable for the costs
of others. From the operator point of view, the large L-
discrepancies dictate that he needs to take them under se-
rious consideration. If it is anticipated that the market for
backbone services will be healthy, the operator should choose
the Aggregate-Peak-Device policy.
If however, he expects
diﬃculties in selling its capacity, our results indicate that
Shapley should be the policy of choice.

518Figure 10: Time-series of a customer with large
(3.25) L-discrepancy (Shapley vs. Aggregate-Peak-
Device policy). The dashed lines represent the traf-
ﬁc volumes where the capacity of the link needs to
be upgraded.

Figure 11: The customers’ total costs for the uni-
form and diverse link costs using the Aggregate-
Peak-Device cost-sharing policy; the size of the cir-
cles is proportional to the aggregate traﬃc volume
of the customers.

Method

Volume-Customer

Peak-Customer

95Percentile-Customer
Aggregate-Peak-Device

Shapley

>25% 95th percentile median
0.830
4.305
4.187
0.802
4.079
0.817
4.019
0.840
0.830
3.460

961.1
933.1
922.4
862.1
761.1

Table 7: Network-level TCO-discrepancies, i.e., the
costs of the customers based on uniform vs. diverse
link costs

5.4 TCO-discrepancies

In this section we take into account that our dataset con-
tains a geographically distributed set of links with diverse
costs as we introduced in Section 4. We compute the TCO-
discrepancies by computing the ratio between the customers’
costs given links with uniform and diverse costs. In Fig. 11
we illustrate the TCO-discrepancies under the Aggregate-
Peak-Device policy. Each customer is aﬀected by the TCO-
discrepancies. The diﬀerence between the two costs can be
as high as 5% of the cost of the entire network.

We report the quantiﬁed TCO-discrepancies of ﬁve poli-
cies in Table 7. The results show generally extreme TCO-
discrepancies; some customers have TCO-discrepancies as
high as 900. In addition, 80% of the customers are assigned
25% higher or lower cost when the diverse costs of the links
is considered. The Shapley value is aﬀected the least based
on the medians of the TCO-discrepancies.

Summary and implications:
TCO-discrepancies have
a very large impact on the costs of the customers. The
median ratio of the customers’ costs is as high as a factor
of four. Similar to the L-discrepancies, cross-subsidization
problems arise if the impact of TCO diﬀerences is neglected.
Backbone operators are aware of the fact that diﬀerent parts
of their network have diﬀerent TCOs. The implication of our
results is that this diversity should also be reﬂected in the

Method

Peak-Customer (5-min)
Peak-Customer (30-min)
Peak-Customer (2-hour)
Volume-Customer (5-min)
Volume-Customer (30-min)
Volume-Customer (2-hour)

95Percentile-Customer (5-min)
95Percentile-Customer (30-min)
95Percentile-Customer (2-hour)

Shapley (5-min)
Shapley (30-min)
Shapley (2-hour)

>25% Median
1.157
1.157
1.151
1.261
1.261
1.251
1.186
1.186
1.181
1.359
1.359
1.316

0.3
0.3
0.35
0.49
0.51
0.50
0.27
0.4
0.37
0.56
0.54
0.54

Table 8: The impact of time intervals on the
network-level discrepancies. The policies are com-
pared to the Aggregate-Peak-Device policy for the
same time interval.

quantiﬁcation of the customers’ costs— and eventually in
the tariﬀs too, based on which customers are charged.

5.5 Sensitivity analysis

We investigate the robustness of the cost-sharing policies
from two additional angles. First, we quantify the costs
of the customers for time-series with 5-minute, 30-minute,
and 2-hour intervals. We compare the network-level costs
of the methods to the Aggregate-Peak-Device policy in Ta-
ble 8, where the two policies are compared based on the same
time interval. The results reveal that all the policies are af-
fected equally, i.e., the duration of the time-interval does
not introduce additional discrepancies among the schemes.
For example, the median ratios of the costs based on the
Volume-Customer and the Aggregate-Peak-Device policies
are 1.261, 1.261, and 1.251 in the case of the datasets with
5-minute, 30-minute, and 2-hour intervals, respectively.

Second, backbone operators apply diﬀerent utilization
thresholds at which they upgrade the capacity of their
devices. We compute the costs of the customers in the case

519Upgrade policy >25% 95th percentile Median
1.453
1.497
1.542

0.642
0.673
0.689

40%
50%
60%

512.5
472.4
451.3

Table 9: Network-level discrepancy for varying net-
work upgrade thresholds (Aggregate-Peak-Device
vs. Shapley policy).

 

content
latam
eu
us
asia
tier−1/2

)
$
(
 
s
p
b
M

 
r
e
p

 
t
s
o
c

103

102

101

100

10−1

 

traffic

Figure 12: Cost per Mbps for diﬀerent types of cus-
tomers.

of 40%, 50%, and 60% threshold values and quantify the
ratio between the customers’ costs based on the Aggregate-
Peak-Device and the Shapley policies (Table 9). Similar
to the time intervals, the impact of the network upgrade
policy is modest. The median ratio of the costs increases
only by 0.1 if the upgrade threshold is increased from 40%
to 60%.

Summary and implications: Neither the time-intervals,
over which we aggregate the traﬃc of the customers, nor
the upgrade policy, which is determined by the utilization
level when the capacity of a device is upgraded, have a sig-
niﬁcant impact on the discrepancies between the diﬀerent
cost-sharing policies.

5.6 Costs in different type of customers

We next dive deeper into the costs and focus on the diﬀer-
ent types of the customers present in our datasets. In Fig. 12
we depict the cost per Mbps and the peak data rate for all
customers classiﬁed into six diﬀerent types: content ISPs,
tier 1/2 ISPs, and EU, Latin American, US and Asian access
providers. We consider the network-level costs quantiﬁed
based on the Aggregate-Peak-Device policy. The ﬁgure re-
veals the following three main properties of the costs. First,
the cost per Mbps of individual customers covers a wide
range of several orders of magnitude. Second, even within a
single type of customer, one can expect large variability of
cost per Mbps though there is a tendency for lower costs for
the EU/US ISPs, compared to the LatAM and Asian ones.
This is a consequence of the fact that the EU/US ISPs send
most of their traﬃc to EU/US and thus utilize lower-cost
infrastructure. Finally, content providers fall consistently
in the range of ‘expensive’ customers (over 10$ per Mbps)
in terms of the cost per Mbps served. This indicates that
a large fraction of their traﬃc crosses expensive interconti-

nental links. We were able to justify this assumption based
raw data in our datasets.

6. RELATED WORK

We refer to the textbook of Courcoubetis and Weber [8]
for a thorough treatment of pricing in communication net-
works. A workshop version preceded this work [14] where
we focused on the diﬀerent cost-sharing policies that can be
applied in backbone networks. Our current work extends
that preliminary work with the following: a) we propose a
methodology for cost-sharing in backbone networks and in-
troduce the diﬀerent types of discrepancies; b) we evaluate
the liability facet of the cost-sharing domain; and c) we uti-
lize a much richer dataset that describes the traﬃc patterns
of a backbone network with three-times more links.

Several studies investigated how to reduce the transit costs
including ISP peering [2, 11, 10], CDNs [24], P2P localiza-
tion [6], and traﬃc smoothing [20]. Dimitropoulos et al. [12]
presented a comprehensive analysis of the 95th percentile
pricing. A proposal by Laoutaris et al. [16, 15] showed how
traﬃc can be transferred in the network without increasing
the 95th percentile of the customers. A recent proposal by
Stanojevic et al. [27] proposes to the customers of transit
providers to form a coalition to reduce their transit costs.
Valancius et al. [30] show that a small number of pricing
tiers are enough to extract close-to-optimal eﬃciency in the
transit provider. In our work we augment the price eﬃciency
analysis of [30] with a methodology that quantiﬁes how indi-
vidual customers aﬀect the cost of running the network that
demonstrate the complex nature of the customers’ costs in
the real-world networks.

Motiwala [21] et al. developed a cost model that operators
can use to evaluate the costs of their routing and peering
decisions. Their study is complementary to ours, as we can
use their model to assess the CAPEX and OPEX cost of the
network instead of using our step function (Section 3). A
diﬀerence is that we primarily focus on how the cost can be
more fairly and accurately distributed among the customers
of an operator.

The net neutrality debate is in many ways related to the
question of who is responsible for the costs in the network [7],
and our work contributes towards better understanding of
such costs.

Due to the desirable fairness properties [1, 13, 22] of the
Shapley value [25], recent studies proposed pricing and cost
sharing mechanisms using Shapley values. Briscoe [3, 4]
motivates the usage of mechanisms that share the costs of
the users fairly as a way to reduce widely known cross-
subsidization3 of the common infrastructure that often hap-
pens in the communication networks [5]. Cooperative ap-
proaches for cost sharing are investigated in case of inter-
domain routing [19, 26] and IP multicast [1, 13]. Ma et
al. [17, 18] presented a fair revenue sharing method for ISPs
that quantiﬁes the importance of each ISP in the Internet
ecosystem. The work of Stanojevic et al. [28] is the closest to
ours. The authors empirically investigated the temporal us-
age eﬀects using the Shapley and the 95Percentile-Customer
method. This work is diﬀerent in several ways: a) we focus
on the costs of the large customers of a backbone network
with geographically diverse links; b) we study additional cost

3The phenomenon in which a small set of customers is sub-
sidized by a large fraction of other customers of the service.

520sharing policies and aspects such as liability and TCO; and
c) we study a more detailed cost and traﬃc dataset.

7. CONCLUSIONS

With the increasing traﬃc volumes, intense market com-
petition, and technological barriers, most commercial back-
bone operators are faced with the challenge of maintain-
ing healthy proﬁt margins. Providing services to their cus-
tomers, which are often ISPs themselves, carries signiﬁcant
maintenance and upgrade costs. Attributing these costs to
individual customers is critical for ensuring smooth opera-
tions of the backbone network as well as oﬀering fair tar-
iﬀs to customers. However, the process of quantifying the
cost contribution of customers in a distributed backbone net-
work is far from simple involving the complex interaction of
many factors ranging from temporal/spatial characteristics
of the customers and non-linear cost-capacity relationships
to measurement infrastructure issues and high variability of
the component costs.

In this paper, we make a step towards understanding the
relationship between several cost-sharing policies and how
they aﬀect the individual customers. While our analysis re-
veals several important properties of the backbone network
cost-sharing, there are many questions that remain open.
For example, how can one utilize the global view of cost al-
location per customer to create simple yet proﬁtable tariﬀs?
Based on our ﬁndings, such tariﬀs should include device-
level expenditures and measurements to assure its accuracy.
Another open research question is how tariﬀs inspired by
the presented cost sharing policies would alter the behav-
ior of the customers, i.e., their traﬃc patterns to minimize
their expenditures. Additionally, it would be interesting to
study if our observed properties of the cost-sharing could be
mapped to other types of networks that are spatially less
diverse but serve a more populous customer base, such as
the residential broadband or 3G access networks.

Acknowledgement

We would like to thank Juan Manuel Ortigosa, Emilio Sepul-
veda, and Gabriel Bonilha from Telefonica for their valuable
insights. This work and its dissemination eﬀorts have been
supported in part by the ENVISION FP7 project of the Eu-
ropean Union.

8. REFERENCES

[1] Aaron Archer, Joan Feigenbaum, Arvind

Krishnamurthy, Rahul Sami, and Scott Shenker.
Approximation and collusion in multicast cost sharing.
Games and Economic Behavior, 47(1):36 – 71, 2004.

[2] Brice Augustin, Balachander Krishnamurthy, and

Walter Willinger. Ixps: mapped? In Proceedings of the
9th ACM SIGCOMM conference on Internet
measurement conference, IMC ’09, pages 336–349,
New York, NY, USA, 2009. ACM.

[3] Bob Briscoe. Flow rate fairness: dismantling a
religion. SIGCOMM Comput. Commun. Rev.,
37(2):63–74, 2007.

[4] Bob Briscoe. A Fairer, Faster Internet. IEEE

Spectrum, 45(12):42–47, 2008.

[5] Kenjiro Cho, Kensuke Fukuda, Hiroshi Esaki, and

Akira Kato. The impact and implications of the

growth in residential user-to-user traﬃc. SIGCOMM
’06, pages 207–218, New York, NY, USA, 2006. ACM.
[6] David R. Choﬀnes and Fabi´an E. Bustamante. Taming
the torrent: a practical approach to reducing cross-isp
traﬃc in peer-to-peer systems. In Proceedings of the
ACM SIGCOMM 2008 conference on Data
communication, SIGCOMM ’08, pages 363–374, New
York, NY, USA, 2008. ACM.

[7] kc claﬀy. ”network neutrality”: the meme, its cost, its

future. SIGCOMM Comput. Commun. Rev.,
41(5):44–45.

[8] C. Courcoubetis and R. Weber. Pricing and

Communications Networks. John Wiley & Sons, Ltd,
2003.

[9] Cushman & Wakeﬁeld. Oﬃce Space Across the World,

2012.

[10] A. Dhamdhere, C. Dovrolis, and P. Francois. A

Value-based Framework for Internet Peering
Agreements. In Teletraﬃc Congress (ITC), 2010 22nd
International, 2010.

[11] Amogh Dhamdhere and Constantine Dovrolis. The

internet is ﬂat: modeling the transition from a transit
hierarchy to a peering mesh. In Proceedings of the 6th
International COnference, Co-NEXT ’10, pages
21:1–21:12, New York, NY, USA, 2010. ACM.

[12] Xenofontas Dimitropoulos, Paul Hurley, Andreas

Kind, and Marc Stoecklin. On the 95-percentile billing
method. In Sue Moon, Renata Teixeira, and Steve
Uhlig, editors, Passive and Active Network
Measurement, volume 5448 of Lecture Notes in
Computer Science, pages 207–216. Springer Berlin,
Heidelberg, 2009.

[13] Joan Feigenbaum, Christos H. Papadimitriou, and

Scott Shenker. Sharing the cost of multicast
transmissions. Journal of Computer and System
Sciences, 63(1):21 – 41, 2001.

[14] L. Gyarmati, M. Sirivianos, and N. Laoutaris.

Simplicity vs Precision: Sharing the Cost of Backbone
Networks. In NetEcon 2012 - Seventh Workshop on
the Economics of Networks, Systems, and
Computation, 2012.

[15] Nikolaos Laoutaris, Michael Sirivianos, Xiaoyuan
Yang, and Pablo Rodriguez. Inter-datacenter bulk
transfers with netstitcher. In Proceedings of the ACM
SIGCOMM 2011 conference, SIGCOMM ’11, pages
74–85, New York, NY, USA, 2011. ACM.

[16] Nikolaos Laoutaris, Georgios Smaragdakis, Pablo

Rodriguez, and Ravi Sundaram. Delay tolerant bulk
data transfers on the internet. In Proceedings of the
eleventh international joint conference on
Measurement and modeling of computer systems,
SIGMETRICS ’09, pages 229–238, New York, NY,
USA, 2009. ACM.

[17] Richard T. B. Ma, Dah ming Chiu, John C. S. Lui,

Vishal Misra, and Dan Rubenstein. Internet
economics: the use of shapley value for isp settlement.
In Proceedings of the 2007 ACM CoNEXT conference,
CoNEXT ’07, pages 6:1–6:12, New York, NY, USA,
2007. ACM.

[18] Richard T. B. Ma, Dah-ming Chiu, John C. S. Lui,
Vishal Misra, and Dan Rubenstein. On cooperative
settlement between content, transit and eyeball

521internet service providers. In Proceedings of the 2008
ACM CoNEXT Conference, CoNEXT ’08, pages
7:1–7:12, New York, NY, USA, 2008. ACM.

[19] Ratul Mahajan, David Wetherall, and Thomas

Anderson. Negotiation-based routing between
neighboring isps. In Proceedings of the 2nd conference
on Symposium on Networked Systems Design &
Implementation - Volume 2, NSDI’05, pages 29–42,
Berkeley, CA, USA, 2005. USENIX Association.
[20] M. Marcon, M. Dischinger, K.P. Gummadi, and

A. Vahdat. The Local and Global eﬀects of Traﬃc
Shaping in the Internet. In Third International
Conference on Communication Systems and Networks
(COMSNETS), 2011.

[21] Murtaza Motiwala, Amogh Dhamdhere, Nick

Feamster, and Anukool Lakhina. Towards a cost
model for network traﬃc. SIGCOMM Comput.
Commun. Rev., 42(1):54–60.

[22] Herv ˘AˇS Moulin and Scott Shenker. Strategyproof
sharing of submodular costs:budget balance versus
eﬃciency. Economic Theory, 18:511–533, 2001.
10.1007/PL00004200.

[23] W. B. Norton. The Internet Peering Playbook:

Connecting to the Core of the Internet. DrPeering
Press, 2012.

[24] L. Qiu, V.N. Padmanabhan, and G.M. Voelker. On

the Placement of Web Server Replicas. In IEEE
INFOCOM, pages 1587–1596, 2001.

[25] L. S. Shapley. A value for n-person games. Annals of

Mathematical Studies, 1953.

[26] Gireesh Shrimali, Aditya Akella, and Almir Mutapcic.
Cooperative interdomain traﬃc engineering using nash
bargaining and decomposition. IEEE/ACM Trans.
Netw., 18(2):341–352, April 2010.

[27] Rade Stanojevic, Ignacio Castro, and Sergey Gorinsky.

Cipt: using tuangou to reduce ip transit costs. In
Proceedings of the Seventh COnference on emerging
Networking EXperiments and Technologies, CoNEXT
’11, pages 17:1–17:12, New York, NY, USA, 2011.
ACM.

[28] Rade Stanojevic, Nikolaos Laoutaris, and Pablo

Rodriguez. On economic heavy hitters: shapley value
analysis of 95th-percentile pricing. In Proceedings of
the 10th ACM SIGCOMM conference on Internet
measurement, IMC ’10, pages 75–80, New York, NY,
USA, 2010. ACM.

[29] TeleGeography. Wholesale IP transit price database,

http://www.telegeography.com/.

[30] Vytautas Valancius, Cristian Lumezanu, Nick

Feamster, Ramesh Johari, and Vijay V. Vazirani. How
many tiers?: pricing in the internet transit market. In
Proceedings of the ACM SIGCOMM 2011 conference,
SIGCOMM ’11, pages 194–205, New York, NY, USA,
2011. ACM.

522