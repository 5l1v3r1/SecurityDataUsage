A Security Framework for the Analysis and Design

of Software Attestation

Frederik Armknecht
Universität Mannheim,

Germany

armknecht@uni-mannheim.de

Ahmad-Reza Sadeghi

Technische Universität
Darmstadt (CASED),

Germany

ahmad.sadeghi@cased.de
Christian Wachsmann

Intel CRI-SC at TU Darmstadt,

Germany

christian.wachsmann@cased.de

∗
Steffen Schulz
Intel Corporation

steffen.schulz@intel.com

ABSTRACT
Software attestation has become a popular and challenging
research topic at many established security conferences with
an expected strong impact in practice. It aims at verifying
the software integrity of (typically) resource-constrained em-
bedded devices. However, for practical reasons, software at-
testation cannot rely on stored cryptographic secrets or ded-
icated trusted hardware. Instead, it exploits side-channel in-
formation, such as the time that the underlying device needs
for a speciﬁc computation. As traditional cryptographic so-
lutions and arguments are not applicable, novel approaches
for the design and analysis are necessary. This is certainly
one of the main reasons why the security goals, properties
and underlying assumptions of existing software attestation
schemes have been only vaguely discussed so far, limiting
the conﬁdence in their security claims. Thus, putting soft-
ware attestation on a solid ground and having a founded
approach for designing secure software attestation schemes
is still an important open problem.

We provide the ﬁrst steps towards closing this gap. Our
ﬁrst contribution is a security framework that formally cap-
tures security goals, attacker models and various system and
design parameters. Moreover, we present a generic software
attestation scheme that covers most existing schemes in the
literature. Finally, we analyze its security within our frame-
work, yielding suﬃcient conditions for provably secure soft-
ware attestation schemes. We expect that such a consolidat-
ing work allows for a meaningful security analysis of existing
schemes, supports the design of secure software attestation
schemes and will inspire new research in this area.

∗At the time of writing this author was aﬃliated with Ruhr-

University Bochum, Germany, Macquarie University, Aus-
tralia as well as the Intel Collaborative Research Institute
for Secure Computing at TU Darmstadt, Germany.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’13, November 4–8, 2013, Berlin, Germany.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2477-9/13/11 ...$15.00.
http://dx.doi.org/10.1145/2508859.2516650.

Categories and Subject Descriptors
D.4.6 [Operating Systems]: Security and Protection
Keywords
software attestation; security framework; keyless crypto
1.

INTRODUCTION

Embedded systems are increasingly permeating our infor-
mation society, being more and more used also in security-
and safety-critical applications. This generates an increasing
need for enabling technologies that can validate and verify
the integrity of a system’s software state against malicious
code. In this context, software attestation has become a pop-
ular research topic at many established security conferences
with a large body of literature [15, 24, 26, 11, 23, 25, 22, 9,
21, 20, 3, 10, 14, 17, 19, 16, 29].

Software attestation is a trust establishment mechanism
that allows a system, the veriﬁer, to check the integrity of
the program memory content of another system, the prover,
against modiﬁcation, e.g., by malicious code. As it mainly
targets resource-constrained embedded systems (such as At-
mel tinyAVR [2] microcontrollers), software attestation aims
to work without any security hardware at the prover. Soft-
ware attestation deploys the common approach of challenge-
response protocols, where the veriﬁer challenges the prover
with respect to the expected memory content. However,
cryptographic challenge-response protocols typically rely on
secret values that should be unknown to malicious provers.
This cannot be assumed for software attestation, where the
provers are resource-constrained embedded systems that typ-
ically cannot aﬀord secure hardware (such as a TPM) [28,
18, 27, 5, 16]. Hence, the adversary may get full control of
the prover and its cryptographic secrets, rendering classical
cryptographic primitives and protocols useless, a fact that
demands for keyless security solutions.

Therefore software attestation follows a radically diﬀerent
approach than most conventional security mechanisms: It
exploits the intrinsic physical constraints of the underlying
hardware and side-channel information, typically the compu-
tation time required by the prover to complete the attesta-
tion protocol. More detailed, software attestation schemes
are typically designed to temporarily utilize all the comput-
ing and memory resources of the prover, aiming at ensuring

1that the prover can only give the correct responses in time
if the memory state is genuine. Of course this requires that
the veriﬁer can authenticate the device characteristics of the
prover using an out-of-band channel, such as visual authenti-
cation, which clearly distinguishes software attestation from
other approaches like remote attestation.

Without question, this requires completely diﬀerent forms
of security reasoning and likewise demands for other security
assumptions on the underlying core functionalities and sys-
tem properties, representing a highly challenging task. This
may be the main reason that, despite its popularity and
practical relevance, software attestation has not received any
formal treatment yet but is still subject to ambiguities. To
start with, there exist no common adversary model and no
precise formalization of the security goals so far, hindering
a founded security analysis and making it diﬃcult or even
impossible to compare existing schemes.

Likewise the underlying security properties and assump-
tions have been only vaguely discussed, limiting the conﬁ-
dence in the security claims. In fact, current proposals of-
ten combine weak PRNGs and ad-hoc checksum function
designs with unclear and possibly insuﬃcient security prop-
erties. As a result, checksum collisions have been exploited
directly to hide code modiﬁcations [24] and indirectly to
manipulate the location of the measured code in the mem-
ory (memory copy attack [3]). Some works even propose to
simply XOR consecutive memory blocks [32], leading to ob-
vious collision attacks that were only indirectly considered
in subsequent work [1]. Likewise, although several works
consider the problem of free memory, i.e., unused sections of
the memory, code compression attacks [3] have been ignored
in recent works [17, 31] and considered as impractical [16]
without giving any arguments.

Contribution.

In this paper, we make a ﬁrst step towards putting soft-

ware attestation on a solid ground. Our contributions are:

Security framework: We describe the ﬁrst formal secu-
rity framework for software attestation. This includes an
adversary model that, interestingly, fundamentally deviates
from classical cryptographic adversary models. Typically,
the adversary is modelled by a polynomially bounded algo-
rithm that aims to achieve a certain goal without having
certain knowledge (e.g., cryptographic keys).
In contrast,
an adversary against a software attestation scheme can be
unbounded in principle and has complete knowledge of the
prover device conﬁguration and state. However, during the
attack it has to specify (or program) a malicious prover de-
vice with tight resource constraints. The goal is that this
malicious prover can cheat in the attestation protocol within
the strict time bound with reasonable success probability but
without any interaction with the adversary. In other words,
the adversary has unbound resources for preparing the at-
tack but only a tight time-bound and limited computational
and memory resources for executing the attack. Moreover
we provide precise deﬁnitions for the security and the correct-
ness of software attestation schemes and propose a formal
system model that aims to balance between expressiveness
and applicability. This allows a founded and comparable
treatment of current and coming schemes and should help
to avoid ambiguities in the future.

Generic software attestation scheme: We present a
generic software attestation scheme that covers most exist-
ing software attestation protocols in the literature. More-
over, we identify and formalize several system parameters
of software attestation and provide an upper bound of the
success probability of a malicious prover against the generic
scheme as a function of these parameters. The derived upper
bound of the success probability implies suﬃcient conditions
on the system parameters. Although some of these aspects
have been implicitly assumed and informally discussed in the
literature, we present their ﬁrst formal treatment. Moreover,
our approach provides new insights on how these parame-
ters impact the security of the underlying software attesta-
tion scheme, which has never been analyzed before. This
result allows to argue on the security of software attestation
schemes by mapping the generic scheme and properties to
the concrete scheme and by examining whether the proper-
ties are fulﬁlled. Moreover the generic scheme may serve as
a blueprint for future schemes.

Argumentation techniques: The security treatment of
the generic scheme required to use novel types of arguments.
Since the common cryptographic proof technique of reducing
the security of the scheme to a hard problem is not possible
anymore, we had to argue directly that any attack strategy
that is possible within the given time-bound fails with a
certain probability. We conjecture that our approach may
be of independent interest. For example, we expect that the
security of concrete schemes that are not directly covered by
the generic scheme may be argued using similar techniques.

New insights: Furthermore, our investigations yield new
insights with respect to the cryptographic properties of the
underlying primitives. Our work shows that cryptographic
primitives can be used that are similar to established prim-
itives, such as pseudo-random number generators or hash
functions, but that diﬀer in subtleties: Some cryptographic
assumptions can be relaxed while others need to be strength-
ened. Such observations are relevant with respect to con-
crete realizations of secure software attestation schemes.

We see our work as a ﬁrst step paving the way for a
founded treatment of secure software attestation schemes.
We expect that such a consolidating work allows for a mean-
ingful security analysis of existing schemes and supports the
design of arguably secure software attestation schemes and
will inspire new research in this area.

Outline.

We give an overview of the related work in Section 2 and
introduce our system model in Section 3. We present the for-
mal framework for software attestation in Section 4, describe
the generic software attestation scheme and its requirements
in Section 5 and formally analyze its security in Section 6.
Finally, we discuss our results and conclude in Section 7.

2. RELATED WORK

The existing literature on software attestation focuses on
the design of checksum functions for diﬀerent platform ar-
chitectures and countering platform-speciﬁc attacks [24, 23,
9, 22, 17]. Several works consider the strengthening of check-
sum algorithms and their implementations against unintended
modiﬁcations by either limiting the memory available to the
prover during attestation [10, 29] or by using self-modifying

2and/or obfuscated attestation algorithms [25, 11]. Many
works investigate the suitability and extension of software
attestation to a variety of computing platforms, including
sensors, peripherals and voting machines [21, 17, 9, 23, 16].
Furthermore, software attestation has been proposed as a
key establishment mechanism [21].

Software attestation is diﬀerent from remote attestation
which has the goal to verify the integrity of remote provers,
e.g., over a network. Speciﬁcally, remote attestation usually
relies on secrets shared between the veriﬁer and the hon-
est prover, which is fundamentally diﬀerent from software
attestation that cannot rely on cryptographic secrets and in-
stead typically assumes that the veriﬁer can authenticate the
prover using an out-of-band channel, such as visual authen-
tication. Several works consider how to combine software
attestation with hardware trust anchors such as TPMs and
SIM-cards [20, 16, 14] or intrinsic hardware characteristics
such as code execution side-eﬀects [15, 24, 26] and Physically
Unclonable Functions [19]. Interestingly, most proposed im-
plementations employ hash functions and PRNGs that are
not cryptographically secure. Further, works that use cryp-
tographically secure algorithms do not consider whether these
algorithms maintain their security properties in the “keyless”
software attestation scenario where the underlying secrets,
such as the PRNG states, are known to the adversary. This
is the reason why existing analysis papers on remote attes-
tation, such as [4, 8], cannot be applied to software-based
attestation, as they assume trusted hardware or software
components. In this respect, our formal analysis provides a
fundamental ﬁrst step towards a deeper and more compre-
hensive understanding of software attestation.

An approach [13, 12] related to software attestation uses
Quines. The basic idea is that the device outputs the whole
content of its memory such that the veriﬁer can compare it
to the expected memory content. In contrast, software attes-
tation aims to use short outputs only for practical reasons.
In that sense, both approaches can be seen as special in-
stantiations of proof-of-knowledge schemes where the proof
either includes the knowledge itself (Quines) or responses de-
pending on the knowledge (software attestation). A further
diﬀerence is that, to reduce the impact of network jitter, soft-
ware attestation typically minimizes the interaction between
the prover and the veriﬁer. In contrast the Quine-schemes
in [13, 12] require signiﬁcant interaction between the veriﬁer
and the device.

Similar to software attestation protocols, proofs of work
schemes challenge the prover with computationally expen-
sive or memory-bound tasks [6, 7]. However, while the goal
of these schemes is to mitigate denial-of-service attacks and
Spam by imposing artiﬁcial load on the service requester,
the goal of software attestation schemes is using all of the
prover’s resources to prevent it from executing malicious
code within a certain time frame. Hence, proofs of work are
in general not suitable for software attestation since they
are usually less eﬃcient and not designed to achieve the op-
timality requirements of software attestation algorithms.

3. PRELIMINARIES
Notation. Let X, Y be two sets and x ∈ X and y ∈ Y be an
element of each set. We denote with f : X → Y a function
f that maps the set X to set Y . Further f : x (cid:55)→ y means
that function f maps an element x to an element y. Let A

and B be arbitrary algorithms. Then y ← A(x) means that
means that A has black-box access to B. We denote with A(cid:98)B
on input x, A assigns its output to y. The expression AB
an algorithm A that does not access an algorithm B. Let D
be a probability distribution over the set X, then the term
x D← X means the event of assigning an element of X to
variable x that has been chosen according to D. Further, we

deﬁne D(x) := Pr(cid:2)x|x D← X(cid:3) for each x ∈ X and denote

with U the uniform distribution.

System Model. Software attestation is a protocol between
a veriﬁer V and a (potentially malicious) prover P where the
latter belongs to a class of devices with clearly speciﬁed char-
acteristics. That is, whenever we speak about a prover P,
we refer to a device that belongs to this class. Typically
a prover P is a low-end embedded system that consists of
memory and a computing engine (CE). The memory is com-
posed of primary memory (PM), such as CPU registers and
cache, and secondary memory (SM), such as RAM and Flash
memory. We assume that the memory is divided into mem-
ory words and denote by Σ := {0, 1}ls the set of all possible
memory words (e.g., ls = 8 if words are bytes). Let s and p
be the number of memory words that can be stored in SM
and PM, respectively. An important notion is the state of a
prover:
Deﬁnition 1 (State). Let P be a prover, i.e., a device that
belongs to the speciﬁed class of devices. The state State(P) =
S of P are the memory words stored in the secondary mem-
ory (SM) of P.
Note that S includes the program code of P and hence spec-
iﬁes the algorithms executed by P.

The computing engine (CE) comprises an arithmetics and
logic unit that can perform computations on the data in
primary memory (PM) and alter the program ﬂow. For per-
formance reasons, PM is typically fast but also expensive.
Hence, the size of PM is usually much smaller than the size
of SM. To make use of SM, CE includes the Read instruction
to transfer data from SM to PM and the Write instruction
to write data from PM to SM. More precisely, Read(S, a, b)
takes as input a memory address a of SM and a memory ad-
dress b of PM and copies the data word x stored at address
a in SM to the data word at address b in PM. For conve-
nience, we write Read(S, a) instead of Read(S, a, b) whenever
the address b of PM is not relevant. Note that Read(S, a, b)
overwrites the content y of PM at address b. Hence, in
case y should not be lost, it must be ﬁrst copied to SM us-
ing Write or copied to another location in PM before Read
is performed. It is important to stress that, whenever CE
should perform some computation on some value x stored
in SM, it is mandatory that x is copied to PM before CE
can perform the computation. Further, since SM is typically
much slower than PM, Read and Write incur a certain time
overhead and delay computations on x. We denote the time
required by CE to perform some instruction or algorithm
Ins with Time(Ins). Note that we only consider provers as
described above while the veriﬁer V can be an arbitrary com-
puting platform that may interact with P.

Remark 1: Platform Architecture. Note that we focus
on embedded microcontrollers since these are commonly tar-
geted by software attestation. We explicitly exclude provers
that are high-end computing platforms with multiple CPUs

3and/or Direct Memory Access (DMA) since these are typ-
ically equipped with secure hardware (such as TPMs) and
hence could support common cryptographic solutions based
on secrets. Further, their memory architectures are usually
more complex than in our system model. In particular, such
platforms usually feature additional hardware to predict and
fetch memory blocks in advance, making the time-bounded
approach much more diﬃcult and its realization highly de-
pendent on the concrete system.

4. SECURE SOFTWARE ATTESTATION
Secure software attestation enables the veriﬁer V to gain
assurance that the state of a prover P is equal to a particular
If this is the case, we say that P is in state S,
state S.
i.e., formally State(P) = S. Consequently, a prover P is
called honest (with respect to some state S) if State(P) = S,
otherwise it is considered to be malicious.

Prover State. Observe that a prover (cid:101)P is already con-

Remark 2: Distance between Honest and Malicious

sidered to be malicious even if its state diﬀers by only one
state entry (memory word) from S. This is a necessary con-
sequence of the goal of having a deﬁnition of honest and
malicious provers that is as generic as possible.

The common approach of software attestation is to vali-
date the prover’s state by requesting N random samples of
the prover memory content. Hence, the Hamming distance

λ between the state S of an honest prover and the state (cid:101)S
of a malicious prover (cid:101)P directly aﬀects a malicious prover’s

success probability. As far as we know, we are the ﬁrst to
formally take into account the impact of λ on the security
of software attestation schemes (cf. λ in Theorem 1).
Ideal Approach. Ideally, V could disable the computing
engine (CE) of P and directly read and verify the state S
stored in the secondary memory (SM) of P. However, expos-
ing CE and SM of P to V in such a way requires hardware
extensions1 on P, which contradicts the goal of software at-
testation to work with no hardware modiﬁcations.

Practical Approach. As the ideal approach is not feasi-
ble in practice, the common approach in the literature is
that V and P engage in a challenge-response protocol Attest
where P must answer to a challenge of V with a response
that depends on S. In the following, whenever we refer to
a software attestation scheme we actually mean the corre-
sponding challenge-response protocol Attest. Observe that
Attest needs to include a description of the algorithm that
processes the veriﬁer’s challenge and computes the prover’s
response.

In general, software attestation aims to ﬁgure out whether
the original state S of a device has been replaced by the

adversary with a malicious state (cid:101)S (cid:54)= S. Observe that al-
though (cid:101)S is diﬀerent from S, we cannot exclude that (cid:101)S may

depend on S. This implies an important diﬀerence to com-
mon cryptographic scenarios: Software attestation cannot
rely on any secrets since the adversary has access to the
same information as the honest prover P. Therefore soft-
ware attestation follows a fundamentally diﬀerent approach

1Note that existing testing interfaces such as JTAG cannot
be used since they are typically disabled on consumer devices
to prevent damage to the device and unintended reverse-
engineering.

and leverages side-channel information, typically the time
δ the prover takes to compute the response. A basic re-
quirement of this approach is that S speciﬁes a practically
optimal implementation of the algorithm that processes the
challenge according to Attest. This means that it should
be hard to ﬁnd any other implementation of this algorithm
that can be executed by a prover P in signiﬁcantly less time
than δ. Otherwise, a malicious prover could use a faster
implementation and exploit the time diﬀerence to perform
additional computations, e.g., to lie about its state.
Furthermore, the communication time jitter between V
and P is typically much higher than the time needed by
the computing engine of P to perform a few instructions.
Hence, to ensure that V can measure also slight changes to
the prover’s code (that could be exploited by a malicious
prover to lie about its state), V needs to amplify the eﬀect
of such changes. The most promising approach to realize
this in practice is designing the attestation protocol as an
iterative algorithm with a large number of rounds.
Further, since showing the optimality of complex imple-
mentations is a hard problem and since P must compute
the response in a reasonable amount of time, it is paramount
that the individual rounds are simple and eﬃcient. As a re-
sult, cryptographically secure hash functions and complex
Pseudo-Random Number Generators (PRNGs) are not a vi-
able option. Hence, several previous works deployed light-
weight ad-hoc designs of compression functions and PRNGs,
however, without analyzing the underlying requirements on
these components and their interaction. In contrast, we iden-
tify concrete suﬃcient requirements.

Adversary Model and Security Deﬁnition.

receives as input a state S and a time-bound δ.

In the following, we provide the ﬁrst formal speciﬁcation
of the adversary model and the security of a software attesta-
tion scheme Attest based on a security experiment ExpAAttest
that involves an adversary A. The experiment is divided
into two phases and works as follows:
Preparation Phase: At the beginning, the adversary A
It

outputs a (possibly) malicious prover (cid:101)P by specifying
its state (cid:101)S, i.e., State((cid:101)P) = (cid:101)S.
Execution Phase: The prover (cid:101)P speciﬁed in the previous
phase receives a challenge c and returns a “guess”(cid:101)r for
The result of the experiment is accept if (cid:101)P responded within
time δ and(cid:101)r = r , and reject otherwise.
case State((cid:101)P) = S, that is (cid:101)P is in the state expected by V,
the prover (cid:101)P should always succeed, i.e., the result of the ex-
in case State((cid:101)P) (cid:54)= S, the probability that the result of the

Based on this experiment we deﬁne correctness and sound-
ness. Correctness is deﬁned analogously to the common
meaning of correctness of challenge-response protocols: In

the correct response r .

periment should always be accept. Soundness means that

experiment is accept should be below a certain threshold.

Deﬁnition 2 (Correctness and Soundness). Consider a soft-
ware attestation scheme Attest and a state S. For a given ad-
versary A we denote by EqualState the event that the output

of A during the experiment is a prover (cid:101)P with State((cid:101)P) = S.

4The software attestation scheme Attest is correct if for all

adversaries A it holds that

Pr

ExpAAttest(S) = accept|EqualState

= 1.
Attest is ε-secure if for all adversaries A it holds that

Pr

ExpAAttest(S) = accept|¬EqualState

(cid:104)
(cid:104)

(cid:105)
(cid:105) ≤ ε.

Remark 3: Power of A. The security of software atten-
tion signiﬁcantly diﬀers from common cryptographic models,
where the time eﬀort of the adversary is typically bounded
(often polynomially bounded in some security parameter).
More detailed, in the preparation phase, A can be any un-
restricted probabilistic algorithm. However, A has no in-

ﬂuence anymore once Attest is executed between (cid:101)P and V
in the execution phase. As (cid:101)P is a device with the same
characteristics as an honest prover, (cid:101)P has to comply to the

same restrictions as P. In other words, the adversary has un-
bounded resources for preparing the attack but only a tight
time-bound and limited resources for executing the attack.

Observe that this reﬂects the strongest possible adver-
sary model, which in principle could be relaxed by imposing
bounds during the preparation phase.

Remark 4: Diﬀerence to Remote Attestation. The
goal of remote attestation is to verify the integrity of re-
mote provers, e.g., over a network. In particular, in practice
a veriﬁer V usually cannot exclude that a malicous prover
may have more computational power than the honest prover.
Therefore, remote attestation schemes usually rely on secrets
shared between the veriﬁer and the honest prover.

This is fundamentally diﬀerent from software attestation
which cannot rely on cryptographic secrets to authenticate
the prover device to V. Hence, as already elaborated, exist-
ing works on software attestation typically assume that V
can authenticate the prover hardware using an out-of-band
channel, such as visual authentication.

5. GENERIC SOFTWARE ATTESTATION
In this section, we formalize a generic software attestation
scheme that captures most existing schemes in the litera-
ture. In particular, we formally deﬁne several aspects and
assumptions, most of them being only informally discussed
or implicitly deﬁned so far.
5.1 Protocol Speciﬁcation

The main components of our generic attestation scheme

(cf. Figure 1) are two deterministic algorithms:

• Memory address generator:

Gen : {0, 1}lg → {0, 1}lg × {0, 1}la , g (cid:55)→ (g(cid:48), a(cid:48))

• Compression function:

Chk : {0, 1}lr × Σ → {0, 1}lr , (r , s) (cid:55)→ r(cid:48)

Here lg, la and lr are the bit length of the state g of Gen,
the memory addresses a and the attestation response r(cid:48), re-
spectively, and Σ is the set of possible state entries (memory
words). Both algorithms are iteratively applied within the
scheme over N ∈ N rounds. For the sake of readability,
we provide an iterative deﬁnition of ChkN : For some r0 ∈
{0, 1}lr and (cid:126)s := (s1, . . . , sN ), we deﬁne rN ← ChkN (c, (cid:126)s) as
ri := Chk(ri−1, si) for i = 1, . . . , N .

Figure 1: The Generic Attestation Scheme Attest

The protocol works as follows: The veriﬁer V sends a ran-
dom attestation challenge (g0, r0) to the prover P, which is
used as seed for Gen and Chk. Speciﬁcally, P iteratively gen-
erates a sequence of memory addresses (a1, . . . , aN ) based
on g0 using Gen. For each i ∈ {1, . . . , N}, P reads the state
entry si = Read(S, ai) at address ai and iteratively com-
putes r(cid:48)i = Chk(r(cid:48)i−1, si) using r(cid:48)0 = r0. Finally, P sends r(cid:48)N
to V, which executes exactly the same computations2 as P
using the state S and compares the ﬁnal result with the re-
sponse r(cid:48)N from P. Eventually, V accepts iﬀ r(cid:48)N = rN and P
responded in time ≤ δ := N (δGen + δRead + δChk), where δGen,
δRead and δChk are upper time-bounds for running Gen, Read
and Chk, respectively, on a genuine and honest prover.

In practice the delay for submitting and receiving mes-
sages needs to be considered. The common approach is to
choose N , the number of rounds, big enough such that this
delay is small compared to the runtime of the protocol. For
simplicity, we assume that this is the case in the following
and hence ignore the time for sending messages.

Remark 5: Correctness. Observe that an honest prover
P always makes an honest veriﬁer V accept since both per-
form exactly the same computations on the same inputs and
the honest prover by assumption requires at most time δ.

Remark 6: Generality of the Protocol. Note that the
basic concept of our generic scheme and several instantia-
tions of this concept for speciﬁc platforms can be found in
the literature on software attestation (cf. Section 2). How-
ever, we aim at abstracting from the particularities of indi-
vidual platforms and instead design and analyze a construc-
tion that is as generic as possible. Further, some existing
software attestation schemes also use the memory addresses
ai and/or the index i as input to the checksum function
Chk. However, since there is a clear dependence between
the index i, the memory address ai and the memory block
si = Read(S, ai) and since the use of simple components
is a primary goal of software attestation, we restrict to the
case where only the memory blocks are used as input to the
checksum function.

5.2 Design Criteria and Properties

Next, we discuss the design criteria of the underlying al-
gorithms and formally deﬁne their properties required later
in the security analysis. Note that, although some of these

2Note that the veriﬁer could also compute the expected re-
sult before, after or in parallel to the protocol.

ProverPVeriﬁerVSSg0,r0Acceptiﬀr0N=rN∧t0−t≤δr0Nfori=1,...,Ndoendforsi←Read(S,ai)fori=1,...,NdoendforStorecurrenttimetStorecurrenttimet0(gi,ai)←Gen(gi−1)r0i←CHK(ri−1,si)si←Read(S,ai)ri←CHK(ri−1,si)(g0,r0)U←{0,1}lg+lr(gi,ai)←Gen(gi−1)r00←r05properties have been informally discussed or implicitly made
in prior work, they have never been formally speciﬁed and
analyzed before.

5.2.1 Implementation of the Core Functionalities
The generic protocol deploys three core functionalities:
Read, Gen and Chk, which of the execution time is of para-
mount importance for the security of software attestation.
Hence, we make the following assumptions that are strongly
dependent on the concrete implementation and prover hard-
ware and are hard to cover in a generic formal framework:

1. Optimality: There is no implementation of Read, Gen
and Chk (or their combination) that is more eﬃcient
(with respect to time and/or memory) than the imple-
mentation used by the honest prover in state S.

2. Atomicity: It is not possible to execute Read, Gen
and Chk only partially, e.g., by omitting some of the
underlying instructions.

We formally cover these assumptions by modelling Read, Gen
and Chk as oracles. That is, whenever P wants, e.g., to ex-
ecute Read(State(P), a), P sends a to the Read-oracle and
receives the corresponding result s. While sending and re-
ceiving messages between P and the oracles are modelled to
take no time, the determination of the response does. More
precisely when P invokes one of these oracles, it takes a cer-
tain amount of time before P gets the result. Within this
time period P is inactive and cannot perform any computa-
tions. We denote the response time of the Read, Gen and
Chk-oracle by δRead, δGen and δChk, respectively. Moreover the
inputs to and the outputs of the oracles need to be stored
in the primary memory of P.

Remark 7: Order of Computations. A consequence of

this modelling approach is that a malicious prover (cid:101)P can
order. For instance, before (cid:101)P can determine si it must ﬁrst

compute the outputs of Gen and Chk only in the correct

determine si−1. Given that concrete instantiations of the
generic scheme are iteratively executed, the limited size of
the primary memory (PM) (see below) and the fact that
accessing the secondary memory requires signiﬁcantly more
time than accessing PM, we consider this assumption to be
reasonable for most practical instantiations.

5.2.2 System-Level Properties
The size and utilization of the primary memory (PM)
plays a fundamental role for assessing the optimality of a
software attestation scheme with regard to the resources

used by a prover (cid:101)P. Therefore, a common assumption is

that PM is just big enough to honestly execute Attest, i.e.,
there is no free PM that could be used otherwise.3

Another crucial assumption of any software attestation
scheme not explicitly made in most previous works is that
the state S should not be compressible into PM. For in-
stance, consider the extreme case where all entries of S con-

tain the same value s. In this case a malicious prover (cid:101)P could

easily determine the correct attestation response by simply

3Possible measures to achieve this are either to choose Gen
and/or Chk accordingly or to execute Gen and/or Chk several
times per round on diﬀerent inputs to occupy more primary
memory.

storing s in PM while having a diﬀerent state State((cid:101)P) (cid:54)= S.
Hence, we require that (cid:101)P should not be able to determine a

randomly selected state entry si of S without accessing the
secondary memory with better probability than guessing:

Deﬁnition 3 (State Incompressibility). For a state S, let
DS denote the probability distribution of S in the following
sense: For any state entry x ∈ Σ it holds that

DS(x) := Pr

x = s|a U← {0, 1}la ∧ s := Read(S, a)

.

(cid:104)

(cid:105)

(cid:105) ≤ γ = max

S is called incompressible if for any algorithm Alg(cid:91)Read that
can be executed by the prover P and that does not invoke
Read, it holds that

(cid:104)(cid:101)s = s|a U← {0, 1}la ∧ s = Read(S, a) ∧(cid:101)s ← Alg(cid:91)Read(a)

Pr

x∈Σ

DS(x).

∧ TimeP (Alg(cid:91)Read) ≤ δRead
5.2.3 Cryptographic Properties
Although it is quite obvious that the security of the soft-
ware attestation scheme depends on the cryptographic prop-
erties of Gen and Chk, these requirements have not been sys-
tematically analyzed and formally speciﬁed before. While it
would be straightforward to model these functions as pseudo-
random number generators (PRNGs) and hash functions (or
even random oracles), respectively, there are some subtle dif-
ferences to the common cryptographic scenario which must
be carefully considered. As we elaborate below, Gen needs
to meet a property which is stronger than the common secu-
rity deﬁnition of cryptographic PRNGs while for Chk a sig-
niﬁcantly weaker condition than the classical security prop-
erties of hash functions is suﬃcient.

Pseudo-Randomness of the Outputs of Gen. To pre-

vent a malicious prover (cid:101)P from using pre-computed attesta-

tion responses, the memory addresses ai generated by Gen
should be “suﬃciently random”.
Ideally, all combinations
should be possible for (a1, . . . , aN ). While this is impossible
from an information-theoretic point of view, the best one
may ask for is that the memory addresses ai generated by
Gen should be computationally indistinguishable from uni-
formly random values within a certain time-bound t:

Deﬁnition 4 (Time-Bounded Pseudo-Randomness of Gen).
Gen : {0, 1}lg → {0, 1}lg +la is called (t, )-pseudo-random if
for any algorithm Alg that can be executed by P in Time(Alg) ≤
t it holds that

(cid:104)

(cid:12)(cid:12)(cid:12) Pr

(cid:104)

− Pr

b = 1|g0

U← {0, 1}lg
∧ (gi, ai) ← Gen(gi−1) : i ∈ {1, . . . , N}
∧ b ← Alg(a1, . . . , aN )

b = 1|ai

U← {0, 1}la : i ∈ {1, . . . , N}

(cid:105)
(cid:105)(cid:12)(cid:12)(cid:12) ≤ .

∧ b ← Alg(a1, . . . , aN )

Observe that this deﬁnition requires that Alg does not know
the seed g0 of Gen, which is not given in the generic soft-
ware attestation scheme.
In principle nothing prevents a

malicious prover (cid:101)P from using g0 to compute the addresses
from random values. The best we can do is to require that (cid:101)P

(a1, . . . , aN ) on its own, making them easily distinguishable

6cannot derive any meaningful information about ai+1 from
gi without investing a certain minimum amount of time.
Speciﬁcally, we assume that an algorithm with input g that
does not execute Gen cannot distinguish (g(cid:48), a(cid:48)) = Gen(g)
from uniformly random values. Formally:

Deﬁnition 5 (Time-Bounded Unpredictability of Gen). Gen :
{0, 1}lg → {0, 1}lg ×{0, 1}la is νGen-unpredictable if for any
algorithm Alg(cid:100)Gen that can be executed by P and that does not
(cid:12)(cid:12)(cid:12) Pr

b = 1|g U← {0, 1}lg ∧ (g(cid:48), a(cid:48)) ← Gen(g)

execute Gen, it holds that

∧ b ← Alg(cid:100)Gen(g, g(cid:48), a(cid:48))

∧ b ← Alg(cid:100)Gen(g, g(cid:48), a(cid:48))

(cid:105)
(cid:105)(cid:12)(cid:12)(cid:12) ≤ νGen.

− Pr

b = 1|g U← {0, 1}lg ∧ (g(cid:48), a(cid:48)) U← {0, 1}lg × {0, 1}la

(cid:104)
(cid:104)

hard for a malicious prover (cid:101)P to replace the correct input

Weakened Pre-image Resistance of Chk. The purpose
of the compression function ChkN is to map the state S of
the prover P to a smaller attestation response rN , which
reduces the amount of data to be sent from P to the veri-
ﬁer V. Observe that the output of ChkN depends also on
the challenge sent by the veriﬁer to avoid simple replay at-
tacks and the pre-computation of attestation responses. A
necessary security requirement on Chk is that it should be
(cid:126)s = (s1, . . . , sN ) to Chk with some other value (cid:126)˜s (cid:54)= (cid:126)s that
yields the same attestation response rN as (cid:126)s. This is similar
to the common notion of second pre-image resistance of cryp-
tographic hash functions. However, due to the time-bound
of the software attestation scheme it is suﬃcient that ChkN
fulﬁlls only a much weaker form of second pre-image resis-
tance since we need to consider only “blind” adversaries who
(in contrast to the classical deﬁnition of second pre-image
resistance) do not know the correct response rN to the ver-
iﬁer’s challenge (g0, r0). The reason is that, as soon as P
knows the correct response rN , he could send it to V and
would not bother to determine a second pre-image. Hence,
we introduce the deﬁnition of blind second pre-image resis-
tance which concerns algorithms that are given only part of
the input (cid:126)s of ChkN and that have to determine the correct
output of ChkN (r0, (cid:126)s):

Pr

Deﬁnition 6 (Blind Second Pre-image Resistance). Chk :
{0, 1}lr × Σ → {0, 1}lr is ω-blind second pre-image resistant
with respect to the distribution DS (cf. Deﬁnition 3) if for
any N ∈ N, any subset of indices J (cid:40) {1, . . . , N} and for
any algorithm Alg that can be executed by P, it holds that

(cid:104)(cid:101)r = r|r0
(cid:105)
DS← Σ : i ∈ {1, . . . , N}
∧(cid:101)r ← Alg(r0, (sj )j∈J ) ∧ r ← ChkN (r0, s1, . . . , sN )

U← {0, 1}lr ∧ si

In addition we require (similar to Deﬁnition 5) that (cid:101)P can-

≤ ω.

not determine any information on rN = ChkN (r0, s1, . . . , sN )
without executing ChkN :
Deﬁnition 7 (Unpredictability of ChkN ). Chk : {0, 1}lr ×
Σ → {0, 1}lr
is νChk-unpredictable with respect to the dis-
(cid:92)ChkN
tribution DS (cf. Deﬁnition 3) if for any algorithm Alg
that can be executed by P and that does not execute ChkN ,

it holds that

(cid:12)(cid:12)(cid:12) Pr

(cid:104)

(cid:104)

b = 1|r0

U← {0, 1}lr ∧ si

DS← Σ : i ∈ {1, . . . , N}

∧ r = ChkN (r0, s1, . . . , sN )
∧ b ← Alg

(cid:92)ChkN

(r0, s1, . . . , sN , r )

(cid:105)

− Pr

b = 1|r0

U← {0, 1}lr ∧ si

DS← Σ : i ∈ {1, . . . , N}

∧ r U← {0, 1}lr
∧ b ← Alg
(cid:92)ChkN

(cid:105)(cid:12)(cid:12)(cid:12) ≤ νChk.
probability of a malicious prover (cid:101)P to make the veriﬁer V

6. SECURITY OF THE SCHEME

In this section we derive an upper bound for the success

(r0, s1, . . . , sN , r )

accept. This bound depends on the parameters deﬁned in
Section 5.2 which provide a suﬃcient condition to prove the
generic attestation scheme secure. The bound is as follows:

Theorem 1 (Generic Upper Bound). Let S be an incom-
pressible state (Deﬁnition 3). Consider the generic attesta-
tion scheme in Figure 1 with the components Read, Gen and
Chk such that

1. Gen is (N (δGen + δRead), )-pseudo-random (Deﬁnition 4)

and νGen-unpredictable (Deﬁnition 5),

λ :=

2. Chk is ω-blind second pre-image resistant (Deﬁnition 6)

and νChk-unpredictable (Deﬁnition 7).

denote the fraction of state entries that are diﬀerent in S and

mary memory and s memory words in its secondary memory
(cf. Section 3). Let

(cid:12)(cid:12)(cid:12)(cid:110)
a ∈ {0, 1}la|Read((cid:101)S, a) = Read(S, a)

Consider an arbitrary prover (cid:101)P as in Section 3 with state
State((cid:101)P) = (cid:101)S that can store p memory words in its pri-
(cid:111)(cid:12)(cid:12)(cid:12) · 2−la ,
(cid:101)S. Then the probability of (cid:101)P to win the security experiment
(cid:8)(π (M, ops) + ) · γN−M + νGen · (N − M )(cid:9) (1)
(cid:18) n
(cid:19)
(cid:19)j

(cid:0)max(cid:8)λx+1, γ(cid:9)(cid:1) n
(cid:33)
(cid:32)n−j(cid:89)
(cid:18) n − j

ExpAAttest (Deﬁnition 2) is upper bounded by

· 2−(lg +lr ) + max{ω, νChk} +

j=max{0,n−2la}
·

and ops denotes the number of instructions (cid:101)P can execute

2la − i
2la

n−1(cid:88)

x+1 −j ·

π(n, x) :=

0≤M≤N

p+s
ls/lr

where

(2)

2la

max

i=0

·

j

in time δRead + δGen.

This result implies that a software attestation scheme is
ε-secure if the expression in Equation 1 is ≤ ε, yielding a
suﬃcient condition for security. For example if a user aims
for ε-security for a prover device with ﬁxed system parame-
ters, he may choose the number of rounds N in dependence
of an expected value of λ accordingly (cf. Appendix B).

Note that the bound given in Equation 1 gives new in-
sights on the impact of the distribution of the state entries

7in S (expressed by γ) and the similarity between the ex-

pected prover state S and the actual state (cid:101)S of the prover

(expressed by λ) on the security of the scheme. Both aspects
have been either neglected or have been considered only in-
formally in previous work (cf. Section 7). To provide a better
intuition and to show the general applicability of Theorem 1,
we compute and discuss the bound for several concrete pa-
rameters that are typical for the systems considered in the
literature on software attestation in Appendix B.

Proof of Theorem 1. Let Win denote the event that a mali-

cious prover (cid:101)P wins the security experiment ExpAAttest, i.e.,
veriﬁer V sends a challenge (g0, r0) to (cid:101)P for which (cid:101)P has

Win means that ExpAAttest(S, l) = accept. We are interested
in an upper bound for Pr [Win]. To this end we consider
several sub-cases. Let Precomp denote the event that the

precomputed and stored the correct response rN in its mem-
ory (primary and/or secondary).4 Then we have
Pr [Win] =Pr [Win|Precomp] · Pr [Precomp]

+Pr [Win|¬Precomp] · Pr [¬Precomp]
≤Pr [Precomp] + Pr [Win|¬Precomp].

Since the challenge (g0, r0) ∈ {0, 1}lg +lr
pled, it follows that Pr [Precomp] = p+s
ls/lr

The maximum number of responses (cid:101)P can store is
abbreviate to Pr [Win]. Let Correct denote the event that (cid:101)P
and (gi, ai) = Gen(gi−1) for i ∈ {1, . . . , N} and that (cid:101)P has

We now estimate the term Pr [Win|¬Precomp], which we

determined all state entries (s1, . . . , sN ), i.e., si = Read(S, ai)

is uniformly sam-
· 2−(lg +lr ).

p+s
ls/lr

.

executed ChkN . Then we have

Pr [Win] ≤ Pr [Correct] + Pr [Win|¬Correct].

It follows from the fact that ChkN is ω-blind second pre-
image resistant (Deﬁnition 6) and νChk-unpredictable (Deﬁ-
nition 7) that Pr [Win|¬Correct] ≤ max{ω, νChk}.

For the ﬁnal term Pr [Correct], we use the following claim,

which we prove afterwards.

Claim 1. The probability Pr [Correct] that (cid:101)P determines all

(s1, . . . , sN ) and rN = ChkN (r0, s1, . . . , sN ) in the security
experiment ExpAAttest under the assumption that the response
to the requested challenge has not been precomputed is upper
bounded by

(cid:8) (π(M, ops) + ) · γN−M + νGen · (N − M )(cid:9)

max

0≤M≤N

where π(N, x) and ops are as in Theorem 1.

Taking these bounds together concludes the proof.
Proof of Claim 1
We now prove Claim 1 used in the proof of Theorem 1.
That is we show the claimed upper bound of Pr [Correct],

which is the probability that a malicious prover (cid:101)P with
state (cid:101)S := State((cid:101)P) (cid:54)= S correctly determines all state en-

tries (s1, . . . , sN ) in the security experiment ExpAAttest (Def-
inition 2) under the assumption that the response for the
requested challenge has not been precomputed.
4More precisely, A has precomputed this value during the

preparation phase and stored the response as part of (cid:101)S.

Observe that (cid:101)P may decide to deviate from the protocol

speciﬁcation, e.g., skipping some instructions with respect to
one round i (probably accepting a lower success probability
for determining si) to save some time that could be spent
on the determination of another state entry sj with i (cid:54)=
j (probably aiming for a higher probability to determine
sj). Hence the challenge is to show that for any of these
approaches the success probability does not exceed a certain
(non-trivial) bound, which cannot be done by a reduction to
a single assumption.
an oracle O that has access to S. All these games are divided
into two phases: A setup phase and a challenge phase. In
the setup phase O generates all addresses (a1, . . . , aN ) and
determines the corresponding state entries si = Read(S, ai).

We base our proof on a sequence of games played by (cid:101)P and

Afterwards, in the challenge phase, (cid:101)P and O exchange sev-
eral messages. In particular (cid:101)P must submit its guesses (cid:101)xi
for the state entries si to O. (cid:101)P wins the game only if all
guesses are correct, i.e., (cid:101)xi = si for all i = 1, . . . , N .
of (cid:101)P to deviate from the protocol speciﬁcation. While these
possibilities are quite limited in the ﬁrst game (Game 0), (cid:101)P
success probability of (cid:101)P changes. In most cases it turns out

gets more and more control with each subsequent game and
thus can to perform more powerful attacks. For each trans-
formation between two consecutive games, we show how the

The diﬀerences between the games lie in the possibilities

that the previous game represents a subset of the possible
attack strategies of the current game. Note that O formally
represents the honest execution of certain parts of the pro-
tocol and should not be confused with a real party. Con-
and O takes no time.

sequently, we assume that transferring messages between (cid:101)P
method for ignoring all computations of (cid:101)P which are hon-
attacks where (cid:101)P uses the time and/or memory gained by out-
and the size of the primary memory of (cid:101)P to what is nec-

estly executed by assumption. Hence to exclude artiﬁcial
sourcing the computation to O, we restrict the time-bound

Observe that the intention of O is to have an elegant

essary for honestly executing those computations that are
outsourced to O.

Game 0: Randomly Sampling Addresses in Regular
Time Intervals.

Game Description. The purpose of this game is to investi-

gate provers (cid:101)P which (1) do not exploit any aspects related

to the execution of Gen and (2) that are forced to use ex-
actly time δRead for the determination of each state entry si.
This is captured by modelling the game as follows: Within
the setup phase, O samples pairwise independent and uni-
form addresses (a1, . . . , aN ) and sets si := Read(S, ai) for all
i ∈ {1, . . . , N}. In the challenge phase, O iteratively queries

(cid:101)P with ai and (cid:101)P returns some response (cid:101)xi.
Hereby, (cid:101)P can access the Read oracle, which on input a
returns s = Read((cid:101)S, a) after time δRead. Since this is the
N · δRead, meaning that (cid:101)P automatically fails if it needs more
Observe that O ensures that (cid:101)P cannot change the order of
the memory addresses, i.e., O only sends ai to (cid:101)P after ai−1

only operation expected from an honest prover, the size of
the primary memory only allows to store an address a and
a state entry s. Moreover the total time-bound is limited to

time in total than this bound.

8round N we denote the time-frame between the point in time

has been sent.5 We denote with round i the time-frame

between the point in time where (cid:101)P receives ai and the point
in time where (cid:101)P receives ai+1 for i ∈ {1, . . . , N − 1}. With
where (cid:101)P receives aN and the point in time where (cid:101)P sends
the last response (cid:101)xN to O. (cid:101)P wins the game if (1) (cid:101)xi = si
δRead. Otherwise (cid:101)P looses the game.
for the probability Pr [Win0] that (cid:101)P wins Game 0. Since (cid:101)P

for all i ∈ {1, . . . , N} and (2) each round took at most time

Success Probability. We are interested in an upper bound

looses for sure when he uses more time than δRead to respond
to ai in at least one round i, it is suﬃcient to restrict to
provers that take at most time δRead in each round. To this
end, we derive an upper bound which allows to treat the
individual rounds separately. We start with the ﬁnal round
N and distinguish between two cases.

Read(S, aN ) is λ (cf. Theorem 1) since aN is sampled uni-
formly and independently from the previous addresses. Now

of at most γ (cf. Deﬁnition 3).
It follows that in Case 1
the probability Pr [Win0] is upper bounded by max{λ, γ} ·

instructions than Read during round N .
In particular a
could not be chosen in dependence of aN , hence being in-

In Case 1 the response (cid:101)xN is the direct result of a query
to the Read oracle, i.e., (cid:101)xN = Read((cid:101)S, a) for some address
a. If a = aN the probability of (cid:101)xN := Read((cid:101)S, aN ) = sN :=
consider that a (cid:54)= aN . Since (cid:101)xN = Read((cid:101)S, a) and due
to the fact that (cid:101)P must respond with (cid:101)xN in time δRead af-
ter receiving aN , (cid:101)P has no time left to perform any other
dependent of aN . Then (cid:101)xN = sN happens with probability
Pr [(cid:101)x1 = Read(S, a1) ∧ . . . ∧(cid:101)xN−1 = Read(S, aN−1)].
Next we consider Case 2, where (cid:101)xN is not the result of a
i < N , that the probability of (cid:101)xN = Read(S, aN ) is upper
γ · Pr [(cid:101)x1 = Read(S, a1) ∧ . . . ∧(cid:101)xN−1 = Read(S, aN−1)]
Pr [Win0] ≤ max{λ, γ} · Pr(cid:2)(cid:101)x1 = Read(S, a1)
∧ . . . ∧(cid:101)xN−1 = Read(S, aN−1)(cid:3)

query to the Read oracle. It follows from the incompressibil-
ity of S (Deﬁnition 3) and the fact that aN has been sampled
uniformly and independent of all previous addresses ai with

is an upper bound of Pr [Win0] in Case 2. It follows from
Cases 1 and 2 that

bounded by γ. Hence,

and by induction Pr [Win0] ≤ π0 = π0(N ) := (max{λ, γ})N .

Game 1: Prover Controls the Address Generation Time.

Game Description. In this game we increase the power

how much time he devotes to determine each value si, as
long as the total time for determining (s1, . . . , sN ) does not
exceed N · δRead. This reﬂects the fact that in the attestation

of the malicious prover (cid:101)P and allow him to freely choose
protocol, a malicious prover (cid:101)P may generate the memory
message which (cid:101)P needs to send to O for receiving the next
sends ai to (cid:101)P only when (cid:101)P sent the i-th request req to O.

Formally, this is captured by introducing a req protocol
address ai during the challenge phase. More precisely, O

addresses (a1, . . . , aN ) on its own whenever it wants to.

5This is a consequence of Remark 7.

Since each round may take a diﬀerent time period, the
winning conditions are relaxed by replacing the time restric-
tion on the individual rounds by an overall time-bound for

the entire challenge phase. This means that (cid:101)P wins Game 1
if (1) (cid:101)xi = si for all i ∈ {1, . . . , N} and (2) the duration of
Pr [Win1] that (cid:101)P wins Game 1. To this end, we divide the

the challenge phase does not exceed the time N · δRead. The
size of the primary memory remains as in Game 0.

Success Probability. We now upper bound the probability

number of rounds into four distinct sets. Let Ncoll denote the
number of rounds where the address sampled by O is equal
to an address of some previous round by coincidence, i.e.,
Ncoll := |{i ∈ {2, . . . , N}|∃j ∈ {1, . . . , i − 1} : ai = aj}| .
With respect to the remaining N −Ncoll rounds, let Nequal
(resp. Nmore, resp. Nless) be the number of rounds where

(cid:101)P responds in time equal (resp. more, resp. less) than δRead.

Thus we have N = Ncoll + Nequal + Nless + Nmore.

Let Coll(Ncoll) denote the event that exactly Ncoll of the
N addresses are equal to some previous addresses. This
implies that in N − Ncoll rounds pairwise diﬀerent addresses
are sampled. Moreover, since there are only 2la diﬀerent
addresses, N − Ncoll is upper bound by 2la . It follows that
N − Ncoll ≤ 2la ⇔ Ncoll ≥ N − 2la . Thus it must hold that
Ncoll ≥ max{0, N − 2la}. Hence, Pr [Win1] is equal to

Pr [Win1|Coll(Ncoll)] · Pr [Coll(Ncoll)].

Ncoll=max{0,N−2la}

We now derive upper bounds for Pr [Win1|Coll(Ncoll)] and
Pr [Coll(Ncoll)].
In general, Pr [Coll(Ncoll)] can be expressed by (number
combinations of rounds with equal addresses) × (probabil-
ity that addresses in N − Ncoll rounds are pairwise diﬀer-
ent) × (probability that addresses in the remaining rounds
are equal to some previous address). The ﬁrst term is at

while an upper bound for the last term is

N−1(cid:88)

most

(cid:16) N
(cid:17)
(cid:16) N−Ncoll
(cid:17)Ncoll
(cid:18) N
(cid:19)

Ncoll

2la

Ncoll

·

. This gives an upper bound

N−Ncoll(cid:89)

i=0

2la − i
2la

 ·

(cid:18) N − Ncoll

(cid:19)Ncoll

.

(3)

2la

for Pr [Coll(Ncoll)] if max{0, N − 2la} ≤ Ncoll ≤ N − 1. We
now ﬁx a value for Ncoll and aim for an upper bound for the
probability Pr [Win1|Coll(Ncoll)]. We do so by giving sepa-
rate upper bounds on the success probability for the four
diﬀerent types of rounds. Let ops = ops(δRead) be the num-
ber of instructions that can be executed by the computing

engine of (cid:101)P in time δRead. Since we are interested in an upper
bound of (cid:101)P’s success probability, we make several assump-
tions in favor of (cid:101)P: (1) For rounds where (cid:101)P invested more

time than δRead, we use the trivial upper bound of 1 even if
the time period exceeded δRead only by the time required to
execute one single instruction. (2) For rounds where the re-
quested address coincides with an address previously asked,
we likewise use the bound of 1. Moreover we assume that
these rounds take no time at all and the ops instructions
saved can be used in ops other rounds. (3) In rounds that
take less time than δRead, it follows from the incompressibil-
ity of S (Deﬁnition 3) and the fact that all addresses are

pairwise distinct that (cid:101)xi = si with probability ≤ γ. Again,

we assume that these rounds take no time at all and that

9the ops instructions saved can be used in ops other rounds.
most with probability max{λ, γ} (cf. Game 0).

(4) In a round that takes exactly time δRead (cid:101)P succeeds at
ities of (cid:101)P, they allow to identify optimum strategies. More
precisely for each round where (cid:101)P uses less time than δRead

While these assumptions strongly exaggerate the possibil-

or where a previously asked address is requested again, the
best approach is to spend the ops saved instructions in ops
other rounds such that for each of these rounds the probabil-
ity of correctly determining si is equal to 1. It follows that
Nmore = ops· (Ncoll + Nless) and hence N = Ncoll + Nequal +
Nless + Nmore = Nequal + (ops + 1) · (Ncoll + Nless). Hence,
we have
(cid:111)

(cid:110)
Pr [Win1|Coll(Ncoll)] ≤ π0(Nequal) · γNless · 1Ncoll+Nmore =
λN−(ops+1)·(Ncoll+Nless)·γNless , γN−(ops+1)·Ncoll−ops·Nless

max
Nless

(cid:16)

(cid:110)

(cid:111)(cid:17)

cf. Apx. A

=

max

λops(δRead)+1, γ

N

ops(δRead )+1 −Ncoll

,

(4)

where the last equation is shown in Appendix A. This shows
that π (N, ops(δRead)) is an upper bound for the probability
Pr [Win1], where π(n, x) is deﬁned as in Equation 2. Observe
that for any ﬁxed value of Ncoll, the probability of having
Ncoll collisions (Equation 3) increases with N (as long as
Ncoll ≥ max{0, N − 2la}) while the probability to determine
the values (s1, . . . , sN ) (Equation 4) decreases for N .

Game 2: Skipping Address Generation.

that honestly generate all addresses (a1, . . . , aN ). Now we

more time for determining the values si but at the “cost”
of not knowing ai. Formally this is captured by deﬁning a
second message skip besides req. Speciﬁcally, in each round
In
case of req, O behaves as in Game 1 and sends the next ai

Game Description. So far we covered only provers (cid:101)P
change the game such that (cid:101)P may decide in each round i
to skip the generation of address ai. This allows (cid:101)P to “buy”
i of the challenge phase, (cid:101)P either sends req or skip.
to (cid:101)P. However, when (cid:101)P sends skip then O does not send
ai to (cid:101)P and extends the time-bound by δGen. That is, at
are that (1) all responses ((cid:101)x1, . . . ,(cid:101)xN ) of (cid:101)P are correct, i.e.,
(cid:101)xi = si ∀i ∈ {1, . . . , N} and (2) the challenge phase does not
take more time than N · δRead. However each time (cid:101)P sends
Pr [Win2] that (cid:101)P wins Game 2. To this end we follow the

a skip message to O, the time-bound is extended by δGen.

Success Probability. We now determine the probability

the beginning of the challenge phase, the winning conditions

same line of arguments as in Game 1. The only diﬀerence is
that rounds where collisions in the addresses took place or
where either Read or Gen have been skipped take no time at
all and free ops(δRead + δGen) instructions for other rounds.
That is we get a bound with the same structure as in Game 1
but where ops(δRead) is replaced by ops(δRead + δGen), i.e.,
Pr [Win2] ≤ π (N, ops(δRead + δGen)).

Game 3: Replacing the Random Sampling with Gen .

Game Description. Now we consider a variant of Game 2
with the only diﬀerence being that the addresses (a1, . . . , aN )
are generated by Gen instead of being randomly sampled by
O. That is, during the setup phase O randomly samples g0
and generates (a1, . . . , aN ) using Gen.

Success Probability. Let Pr [Win3] be the probability that

(cid:101)P wins Game 3. Using a standard argument, it follows

from the pseudo-randomness of the outputs of Gen (Deﬁni-
tion 4) that |Pr [Win3]− Pr [Win2]| ≤  and hence Pr [Win3] ≤
Pr [Win2] +  ≤ π (N, ops(δRead + δGen)) + .

Game 4: Giving Access to Gen.

Game Description.

In the ﬁnal game O no longer gen-

erates (a1, . . . , aN ) for (cid:101)P. Instead (cid:101)P now queries the Gen
gives this value to (cid:101)P.
Observe that the size of the primary memory of (cid:101)P is in-

oracle, which on input gi returns (gi, ai) = Gen(gi−1) after
time δGen. To this end, O samples g0 in the setup phase and

creased to additionally store a value g. Further, the time-
bound of the challenge phase is increased to N · (δGen + δRead).

Success Probability. The only diﬀerence between Game 4

Gen oracle. Recall that g0 is used by Gen for computing

and Game 3 is that (cid:101)P now knows g0 and can query the
(a1, . . . , aN ). Hence (cid:101)P may decide to skip the generation of
νGen-unpredictable (Deﬁnition 5), (cid:101)P cannot derive any infor-
if (cid:101)P never queries Gen with some value gi it cannot distin-

one or more addresses and save the time and memory for
other computations. However, since Gen is assumed to be

mation on ai+1 or gi+1 from gi without querying Gen. Thus

guish the subsequent values (gi+1, . . . , gN ) with a probabil-
ity better than (N − i) · νGen. Therefore we can restrict to
provers that compute (a1, g1), . . . , (aM , gM ) and that skip
(aM +1, gM +1), . . . , (aN , gN ).

Let Pr [Win4] be the probability to win Game 4 and let
Pr [Win4(M )] be the probability to win Game 4 for a ﬁxed
M . That is we have Pr [Win4] ≤ maxM {Pr [Win4(M )]}.
Now consider a variation of Game 4 where O replaces the
values (aM +1, gM +1), . . . , (aN , gN ) by independent and uni-
formly sampled values and we denote with Pr [Win(cid:48)4(M )]

the probability that (cid:101)P wins this game. Since Gen is as-
sumed to be νGen-unpredictable (cf. Deﬁnition 5), it holds
that Pr [Win4(M )] ≤ Pr [Win(cid:48)4(M )] + νGen · (N − M ).

With respect to Pr [Win4(M )], observe that for the ﬁrst
M rounds the situation is as in Game 3. Hence the suc-
cess probability for the ﬁrst M rounds is upper bounded
by π (M, ops(δRead + δGen)) + . For the remaining N − M
rounds, O uses uniformly sampled values (aM +1, . . . , aN )

that are unknown to (cid:101)P. Hence the probability of (cid:101)P deriving
(sM +1, . . . , sN ) correctly is upper bounded by γN−M . This
yields Pr [Win(cid:48)4(M )] ≤ (π (M, ops(δRead + δGen)) + ) · γN−M
and hence Pr [Correct] is at most

(cid:8)(π(M, ops(δRead + δGen)) + )· γN−M + νGen · (N − M )(cid:9).

0≤M≤N
7. DISCUSSION AND CONCLUSION

max

We presented the ﬁrst formal security framework for soft-
ware attestation and precisely deﬁned various of the underly-
ing system and design parameters. Moreover we presented a
generic software attestation protocol that encompasses most
existing schemes in the literature. For this generic scheme
we derived an upper bound on the success probability of a
malicious prover that depends on the formalized parameters.
We see the generic scheme and its proof as blueprints that
can be adapted to concrete instantiations.

10One lesson learned is the impact of these parameters on
the security of the generic scheme. For example,
it has
been observed before that free memory should be ﬁlled with
pseudo-random data [24] and a later code-compression at-
tack [3] indicated that code redundancy also impacts secu-
rity. However, the attack was dismissed as impractical [16]
and ignored in later works [17, 29]. In contrast, we consider
the general probability distribution of the state (code and
data) in Deﬁnition 3 and directly connect it to the adversary
advantage. As a result, one can directly evaluate the honest
prover state and determine whether additional measures to
achieve state incompressibility, such as ﬁlling free memory
with pseudo-random data, are required to prevent memory
copy and code compression attacks.

Our results also show that traditional cryptographic as-
sumptions are partially too strong (second pre-image resis-
tance) and partially too weak (pseudo-randomness). Fur-
ther, we identiﬁed new (suﬃcient) conditions on the core
functionalities of software attestation. Most previous works
require the software attestation algorithm to iterate over
all memory words of the secondary memory without giving
any formal justiﬁcation. Our bound allows to identify lower
values for N (if the other parameters are known), enabling
more eﬃcient solutions that provide a tradeoﬀ between the
number of iterations N and the success probability of a mali-
cious prover. Thus our work represents the ﬁrst step towards
eﬃcient and provably secure software attestation schemes.

Still, several open questions remain for future work. One
being to relax the presented conditions or to derive necessary
conditions. A further task is to determine concrete instan-
tiations. While Gen and Chk could be easily realized on
devices that provide hardware-assisted cryptographic func-
tions, such as block cipher implementations in hardware
similar to the AES instructions in modern CPUs [30], this
becomes more challenging on other platforms.

We are currently working on the following aspects: (1) a
practical instantiation of the generic software attestation
scheme and its evaluation and (2) the evaluation of exist-
ing software attestation schemes in our framework.
8. REFERENCES
[1] T. AbuHmed, N. Nyamaa, and D. Nyang.

Software-based remote code attestation in wireless
sensor network. In Global Telecommunications
(GLOBECOM). IEEE, 2009.

[2] Atmel. tinyAVR homepage.

http://www.atmel.com/tinyavr/, 2013.

[3] C. Castelluccia, A. Francillon, D. Perito, and

C. Soriente. On the diﬃculty of software-based
attestation of embedded devices. In Computer and
Communications Security (CCS). ACM, 2009.

[4] A. Datta, J. Franklin, D. Garg, and D. Kaynar. A

logic of secure systems and its application to trusted
computing. In Security and Privacy, 2009 30th IEEE
Symposium on, 2009.

[5] K. E. Defrawy, A. Francillon, D. Perito, and

G. Tsudik. SMART: Secure and minimal architecture
for (establishing a dynamic) root of trust. In Network
and Distributed System Security Symposium (NDSS).
Internet Society, 2012.

[6] C. Dwork and M. Naor. Pricing via processing or

combatting junk mail. In Advances in Cryptology –
CRYPTO. Springer, 1993.

[7] C. Dwork, M. Naor, and H. Wee. Pebbling and proofs

of work. In Advances in Cryptology – CRYPTO.
Springer, 2005.

[8] A. Francillon, Q. Nguyen, K. B. Rasmussen, and

G. Tsudik. Systematic treatment of remote
attestation. Cryptology ePrint Archive, Report
2012/713, 2012. http://eprint.iacr.org/.

[9] J. Franklin, M. Luk, A. Seshadri, and A. Perrig.

PRISM: Human-veriﬁeable code execution. Technical
report, Carnegie Mellon University, 2007.

[10] R. W. Gardner, S. Garera, and A. D. Rubin. Detecting

code alteration by creating a temporary memory
bottleneck. Trans. Info. For. Sec., 4(4):638–650, 2009.

[11] J. T. Giﬃn, M. Christodorescu, and L. Kruger.

Strengthening software self-checksumming via
self-modifying code. In Annual Computer Security
Applications Conference (ACSAC). IEEE, 2005.

[12] V. Graizer and D. Naccache. Alien vs. Quine. Security

Privacy, IEEE, 5(2):26 –31, 2007.

[13] V. Gratzer and D. Naccache. Alien vs. Quine, the

vanishing circuit and other tales from the industry’s
crypt. In Advances in Cryptology – EUROCRYPT.
Springer, 2007. Invited Talk.

[14] M. Jakobsson and K.-A. Johansson. Retroactive

Detection of Malware With Applications to Mobile
Platforms. In Workshop on Hot Topics in
Security (HotSec). USENIX, 2010.

[15] R. Kennell and L. H. Jamieson. Establishing the

genuinity of remote computer systems. In USENIX
Security Symposium. USENIX, 2003.

[16] X. Kovah, C. Kallenberg, C. Weathers, A. Herzog,

M. Albin, and J. Butterworth. New results for
Timing-Based attestation. In Security and Privacy
(S&P). IEEE, 2012.

[17] Y. Li, J. M. McCune, and A. Perrig. VIPER: verifying

the integrity of PERipherals’ ﬁrmware. In Computer
and Communications Security (CCS). ACM, 2011.

[18] B. Parno, J. M. McCune, and A. Perrig.

Bootstrapping Trust in Commodity Computers. In
Security and Privacy (S&P). IEEE, 2010.

[19] A.-R. Sadeghi, S. Schulz, and C. Wachsmann.
Lightweight remote attestation using physical
functions. In Wireless Network Security (WiSec).
ACM, 2011.

[20] D. Schellekens, B. Wyseur, and B. Preneel. Remote

attestation on legacy operating systems with Trusted
Platform Modules. Sci. Comput. Program.,
74(1-2):13–22, 2008.

[21] A. Seshadri, M. Luk, and A. Perrig. SAKE: Software
attestation for key establishment in sensor networks.
Distributed Computing in Sensor Systems, pages
372–385, 2008.

[22] A. Seshadri, M. Luk, A. Perrig, L. van Doorn, and

P. Khosla. SCUBA: Secure code update by attestation
in sensor networks. In Workshop on Wireless
security (WiSe). ACM, 2006.

[23] A. Seshadri, M. Luk, E. Shi, A. Perrig, L. van Doorn,

and P. Khosla. Pioneer: Verifying integrity and
guaranteeing execution of code on legacy platforms. In
Symposium on Operating Systems Principles (SOSP).
ACM, 2005.

11[24] A. Seshadri, A. Perrig, L. van Doorn, and P. K.

Khosla. SWATT: SoftWare-based ATTestation for
embedded devices. In Security and Privacy (S&P).
IEEE, 2004.

[25] M. Shaneck, K. Mahadevan, V. Kher, and Y. Kim.

Remote software-based attestation for wireless sensors.
In Security and Privacy in Ad-hoc and Sensor
Networks. Springer, 2005.

[26] U. Shankar, M. Chew, and J. D. Tygar. Side eﬀects

are not suﬃcient to authenticate software. In USENIX
Security Symposium. USENIX, 2004.

[27] R. Strackx, F. Piessens, and B. Preneel. Eﬃcient

isolation of trusted subsystems in embedded systems.
In Security and Privacy in Communication Networks
(SecureComm). Springer, 2010.

[28] Trusted Computing Group (TCG). TPM Main

Speciﬁcation, Version 1.2, 2011.

[29] A. Vasudevan, J. Mccune, J. Newsome, A. Perrig, and

L. V. Doorn. CARMA: A hardware tamper-resistant
isolated execution environment on commodity x86
platforms. In ACM Symposium on Information,
Computer and Communications Security (AsiaCCS).
ACM, 2012.

[30] Wikipedia. AES instruction set. http:

//en.wikipedia.org/wiki/AES_instruction_set,
2013.

[31] Q. Yan, J. Han, Y. Li, R. H. Deng, and T. Li. A

software-based root-of-trust primitive on multicore
platforms. In ACM Symposium on Information,
Computer and Communications Security (AsiaCCS).
ACM, 2011.

[32] Y. Yang, X. Wang, S. Zhu, and G. Cao. Distributed

software-based attestation for node compromise
detection in sensor networks. In Symposium on
Reliable Distributed Systems (SRDS). IEEE, 2007.

N

APPENDIX
A. DETAILS OF THE PROOF
We prove the upper bound of Pr [Win1|Coll(Ncoll)] used in
Game 1 of the proof of Theorem 1. Observe that 0 ≤ Nless
and 0 ≤ Nequal = N − (ops + 1) · (Ncoll + Nless) ⇔ Nless ≤
ops+1 − Ncoll, i.e., 0 ≤ Nless ≤ N
ops+1 − Ncoll. To simplify
the ﬁrst term λN−(ops+1)·(Ncoll+Nless) · γNless , we deﬁne e :=
logλ(γ) and rephrase it as λN−(ops+1)·Ncoll−(ops+1−e)·Nless .
When ops+1−e < 0, the maximum is achieved for Nless =
0, hence in this case the upper bound is λN−(ops+1)·Ncoll . In
ops+1 −
the other case we get an upper bound for Nless = N
Ncoll, yielding

(cid:16) N

(cid:17)

λN−(ops+1)·Ncoll−(ops+1−e)·

ops+1 −Ncoll

= γ

N

ops+1 −Ncoll .

With respect to the second term, i.e., γN−(ops+1)·Ncoll−ops·Nless ,
the maximum value is achieved if Nless is as big as possible,
i.e., Nless = N

ops+1 − Ncoll. This gives an upper bound of

γN−(ops+1)·Ncoll−ops·Nless = γ

N

ops+1 −Ncoll .

Altogether, it follows that

Pr [Win1|Coll(Ncoll)]

≤(cid:16)

(cid:110)

max

λops(δRead)+1, γ

(cid:111)(cid:17)

N

ops(δRead )+1 −Ncoll

.

Figure 2: Upper Bound (Equation 1) for diﬀerent values of
N and λ.

B. EXAMPLE FOR UPPER BOUND

To get a better intuition of the upper bound (Equation 1)
given in Theorem 1, especially on the impact of the similarity
of the malicious prover’s state to the honest prover’s state
expressed by λ and the number of rounds N , we provide
some concrete examples in this section.

At ﬁrst we have to ﬁx various parameters. To this end, we
consider typical parameters for lg and lr that can be found in
the literature on software attestation. Moreover, we assume
that all cryptographic primitives are perfectly secure and
that the values in S are uniformly distributed:

ω := 2−lr , νChk = 0,  := 0, γ := 2−ls , νGen := 0.

The bound in Equation 1 then simpliﬁes to

(cid:110)

(π (M, ops)) · 2−ls·(N−M )(cid:111)

.

p + s

+ 2−lr + max

0≤M≤N

ls/lr · 2(lg +lr )
Recall that the value ops has been deﬁned as the number
of operations a prover can perform in time δRead + δGen. It
was used in the proof to tackle the following question: If
an attacker decides to skip one round, for how many other
rounds can he increase his probability of success? While
ops certainly expresses an upper bound on this number (the
adversary has to spend at least one instruction per round), it
certainly is an overestimation of the adversary’s capabilities.
Hence, we set ops = 2 to get more meaningful results. This
represents an adversary who can win two rounds if he skips
another round.

Recall that λ expresses the fraction of state entries where
the state of the malicious prover matches with the state S
of the honest prover. We exemplarily use λ ∈ {0.2, . . . , 0.8}.
As shown in Figure 2, for small values of λ (i.e., for malicious
provers with a state that is quite diﬀerent from the honest
prover’s state), a relatively low number of rounds is suﬃcient
to achieve a reasonably low adversary advantage. However,
for large values of λ more rounds are required. Further,
for the chosen system parameters, the advantage seems to
converge to a minimal adversary advantage of 10−48.
Observe that in the literature, it is often suggested to use
N = log(s)·s rounds. Interestingly, our experiments indicate
that signiﬁcantly less rounds can be suﬃcient.

 1e-50 1e-45 1e-40 1e-35 1e-30 1e-25 1e-20 1e-15 1e-10 1e-05 1 0 20 40 60 80 100 120 140 160 180 200Adversary Success Probability (log)Number of checksum iterations Nlambda=0.2lambda=0.4lambda=0.6lambda=0.812