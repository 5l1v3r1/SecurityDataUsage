An Historical Examination of Open Source Releases and

Their Vulnerabilities

Nigel Edwards, Liqun Chen

Hewlett-Packard Laboratories

Long Down Avenue

Bristol, BS34 8QZ, UK

<ﬁrstname.lastname>@hp.com

ABSTRACT
This paper examines historical releases of Sendmail, Post-
ﬁx, Apache httpd and OpenSSL by using static source code
analysis and the entry-rate in the Common Vulnerabilities
and Exposures dictionary (CVE) for a release, which we
take as a measure of the rate of discovery of exploitable
bugs. We show that the change in number and density of
issues reported by the source code analyzer is indicative of
the change in rate of discovery of exploitable bugs for new
releases — formally we demonstrate a statistically signiﬁ-
cant correlation of moderate strength. The strength of the
correlation is an artifact of other factors such as the degree
of scrutiny: the number of security analysts investigating
the software. This also demonstrates that static source code
analysis can be used to make some assessment of risk even
when constraints do not permit human review of the issues
identiﬁed by the analysis.

We ﬁnd only a weak correlation between absolute values
measured by the source code analyzer and rate of discovery
of exploitable bugs, so in general it is unsafe to use abso-
lute values of number of issues or issue densities to compare
diﬀerent applications or software. Our results demonstrate
that software quality, as measured by the number of issues,
issue density or number of exploitable bugs, does not always
improve with each new release. However, generally the rate
of discovery of exploitable bugs begins to drop three to ﬁve
years after the initial release.

Categories and Subject Descriptors
K.6.5 [Management of Computing and Information
Systems]: Security and Protection

General Terms
Security, Measurement

Keywords
Static Analysis, Risk Analysis, Open Source Software

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.

1.

INTRODUCTION

In this paper we present an investigation of the vulnerabil-
ity history of various open source projects. We use a static
source code analysis tool to investigate sample releases over
a number of years for potential security issues and compare
the results to the rate at which entries appear for the soft-
ware in the Common Vulnerabilities and Exposures dictio-
nary (CVE1) [12].

The purpose of the investigation is to understand what
static code analysis of software can tell us about the poten-
tial future intrinsic risks of using that software. The size and
complexity of much commonly used software renders man-
ual analysis impractical: the Linux kernel contains over 13
million line of code and OpenOﬃce contains over 9 million
lines of code at the time of writing. There is also no guar-
antee that any particular piece of software has been subject
to rigorous analysis by skilled security analysts. Therefore
we believe automatic analysis is needed to make a sound
assessment of the risk of using any software.

In this study we are interested in estimating the number
of bugs that might be in the software that could allow the
construction of a successful exploit leading to system com-
promise, for example: buﬀer overﬂow, cross-site-scripting
and SQL injection. We call these exploitable bugs. As well
as comparing diﬀerent releases of the same software, we also
compare diﬀerent software. Our method of comparison is to
use a static source code analysis tool and measure the num-
ber of security issues it identiﬁes. We are trying to answer
the following questions.

1. Does the change in number of issues or issue densities
between a previous release and a new release of the
same software indicate anything?

2. What is the range of issue densities for popular open

source software?

3. Do very large diﬀerences in issue densities or number

of issue between diﬀerent software tell us anything?

CVE is a dictionary of common names, “CVE Identiﬁers”,
for publicly known security vulnerabilities. Its purpose is to
provide one name for each vulnerability to enable the use of
multiple tools and databases. CVE identiﬁers are assigned
so that each separate vulnerability is assigned a unique CVE
Identiﬁer – see [12] for further details. We use the rate at
which entries appear in CVE for the software after the re-
lease date as an estimate of the number of exploitable bugs

1CVE is a trademark of The MITRE Corporation.

183it contained on its release date. So if the rate of appear-
ance of CVE entries for the software after its release date
is very low, then one might say that particular release had
relatively low number of exploitable bugs. Conversely if the
rate is higher, we assume that there were a larger number
of exploitable bugs. The validity of these assumptions is
discussed next.

The approach of using the CVE entries to estimate the
number of exploitable bugs that might have been in a re-
lease has obvious limitations: some software is subject to
greater investigation by security researchers than others —
the degree of scrutiny also matters. We also look at this
eﬀect.
In addition not all exploitable bugs may result in
CVE entries. Not all exploitable bugs may be discovered or
reported. So CVE cannot be used as an absolute measure
of the number of exploitable bugs. Assuming the degree of
scrutiny is constant, we believe it can be used to indicate
whether or not there were more or less security bugs in a
given release.

We chose the following software for our study.

Sendmail Email server software: 1996 to 2011

Postﬁx Email server software: 1999 to 2010

Apache httpd Web server software: the 1.3 release series
from 1998 to 2010; the 2.0 release series from 2002 to
2010; the 2.2 release series from 2005 to 2011

OpenSSL A toolkit for implementing SSL and TLS: the
0.9.6 release series from 2000 to 2004; the 0.9.7 release
series from 2002 to 2007; the 0.9.8 release series from
2005 to 2011; the 1.0.0 release series from 2009 to 2011

All of the above are very widely used to provide Internet
accessible services. Therefore there are many millions of
instances — over 320 million Apache web servers in Octo-
ber 2011 [15]. The wide spread availability makes them a
tempting target for attackers and security researchers, so
the degree of scrutiny is high. The choice of software was
further driven by the availability of public release archives
enabling us to obtain older releases. Both Apache httpd
and OpenSSL have a number of separate major release se-
ries in which substantial new amounts of code were intro-
duced with a new series (e.g. changing from 1.3 to 2.0 for
Apache). These major releases series evolved separately and
are maintained in parallel. Therefore we have analyzed each
of these series separately. We did not have the resources to
analyze all instances of a release series, so we took samples
approximately one year apart.

The remainder of this paper is structured as follows. Sec-
tion 2 gives an overview of static source code analysis. Sec-
tion 3 presents the results of our analysis of Sendmail, Post-
ﬁx, Apache and OpenSSL. Section 4 discusses to what ex-
tent it is possible to compare analysis results from diﬀerent
software. In section 5 we discuss degree of scrutiny, using
additional CVE histories of two open source databases. Sec-
tion 6 describes some related work. Section 7 is our conclu-
sion. The full analysis results are given in appendix A.

2. OVERVIEW OF STATIC SOURCE CODE

ANALYSIS

Static analysis of source code is the automatic examina-
tion of source code to determine particular non-functional

properties of interest. The term “static” is used to denote
that no execution is involved in contrast to “dynamic” anal-
ysis in which some form of execution and test data set is
usually involved. Static analysis is used for a variety of pur-
poses including type-checking, style-checking, performance-
optimization and program veriﬁcation.
In this paper we
are concerned with security analysis which is to detect the
presence of bugs which may lead to security problems —
exploitable bugs.

Static source code analysis attempts to detect exploitable
bugs automatically.
It uses data ﬂow analysis techniques
[16], [6] to trace the potential paths of ingested data through
the program. Each time a call of a dangerous function
such as strcpy() or mysql query() occurs the analysis tool
checks function-speciﬁc rules about the parameters being
passed: whether the length of the target is greater than the
length of the source for strcpy(); whether the query param-
eter of mysql query() has been cleansed – characters that
might modify the query (e.g. SQL injection) have been re-
moved.

Unfortunately many static analysis problems are unde-
cidable (as a consequence of Rice’s theorem [22], see also
discussion in [11]) which means that static analysis tools
must use approximation techniques. For example, they may
analyze paths that can never be executed2. Typically the
consequence of this is that the tool will identify many more
issues for a program than there are security bugs. Therefore
the results it produces must be vetted by human auditors
to determine their legitimacy and impact.
In addition, it
is often possible to add program-speciﬁc rules. For exam-
ple, the software may contain a data cleansing routine. So
we might choose to add a rule that declares any ingested
data that ﬂows through the cleansing routine to be safe so
it won’t trigger SQL injection or buﬀer overﬂow. Building
a set of program-speciﬁc rules requires inspection of source
code to determine how and when data is cleansed. Typi-
cally this is done by auditors inspecting the results from the
ﬁrst and subsequent analyses. Each issue identiﬁed by the
analyzer is considered and the code that triggered the issue
inspected to determine if a custom rule to suppress the is-
sue is warranted. One rule may suppress many issues. This
paper explores the value we can derive when constraints do
not permit human review of all the issues. What can the
raw results do for us?

It follows from the above that the number of issues or
density of issues for two diﬀerent programs cannot be taken
as an absolute measure of the number of security defects
[5]. The diﬀerence might be explainable because, in the ab-
sence of program-speciﬁc rules, the tool is doing a better
job of understanding one program compared to the other.
However, whilst we accept some range of diﬀerence is to be
expected and is unlikely to be signiﬁcant, we seek to under-
stand the range of issue densities and determine if extremely
large diﬀerences are signiﬁcant.

3. THE ANALYSIS

In this section we present and discuss the results of our

analysis. Full results are given in appendix A.

2Some tools are capable of detecting some cases of “dead
code”: code which is never executed. Dead code can arise
because of interfering logical constraints; this can be arbi-
trarily diﬃcult to detect in practice.

184For our study we used the HP Fortify Source Code Ana-
lyzer (SCA) version 5.10.0.0102 without any program-speciﬁc
or custom rules. Other static source code analyzers include
IBM Rational AppScan and Klocwork Insight. For all soft-
ware we conﬁgured the analyzer to trust the local system:
the ﬁle system, environment variables and the like. So the
primary source of untrusted data is the network.

The analyzer classiﬁes issues as “Critical”, “High” or “Low”.
For each software release that we analyzed we consider the
following metrics which we generate from SCA.

• The total number of issues (TI): critical + high + low

• The total issue density (T-density): the number of
critical, high and low issues per 100 lines of executable
code

• The number of critical issues (CI)

• The critical issue density (C-density): the number of

critical issues per 10,000 lines of executable code

• The number of critical and high issues(CHI): critical+

high

• The critical and high issue density (CH-density): the
number of critical and high issues per 1,000 lines of
executable code

We compared these measurements to the number of en-
tries appearing in CVE per year for that software release
(CVE/yr). CVE only contains entries from 1999 on. CVE
entries up to the end of calendar year 2011 are included.
Note that the units for the density metrics, T-density, C-
density and CH-Density are diﬀerent: respectively issues per
100 lines, issues per 10,000 lines and issues per 1,000 lines.
This is because we believe it makes comparison of the full
results from diﬀerent software given in appendix A simpler.
It is easier to compare C-densities of 7.64 and 25.26 (number
of critical issues per 10,000 lines of code) rather than 0.0764
and .2526 (number of critical issues per 100 lines of code).
We did apply some ﬁltering to the CVE entries. Only
entries which detailed problems with the software being an-
alyzed were used. We excluded entries in which references
were made to the use of the software, but the bug lay else-
where. We included entries which were speciﬁc to a single
operating system: due to the limited information available
it is usually not possible to tell if these are simple packag-
ing errors or coding errors. Examples of CVE entries we
excluded include the following.

• Incorrect use of APIs by third party software. In 2009
there were at least 41 CVE entries that referenced
OpenSSL, 29 of these were reports of incorrect use of
OpenSSL by third parties and are therefore excluded
from our analysis. Examples include CVE-2009-5057
(incorrect conﬁguration of OpenSSL) and CVE-2009-
3766 (failure to verify the domain name in the Com-
mon Name ﬁeld of a certiﬁcate when using OpenSSL).
The remaining 12 CVE entries in 2009 were bugs in
various releases of OpenSSL software and are there-
fore included in our analysis.

• Bugs in third party plug-in software or plug-ins. For
example CVE-2009-1012 concerns a bug for the Ora-
cle BEA Weblogic plug-in for Apache httpd. This is

not a bug in any software that was included in any of
the httpd releases. It is therefore excluded from our
analysis.

We make the simplifying assumption that the rate of CVE
occurrence is constant over any given year. We assume each
CVE entry corresponds to one distinct issue. Although this
is the intent of the CVE editorial policy and seems to be the
case for the large majority of entries, we cannot guarantee
that a few CVE entries do not refer to multiple bugs or
that some entries may be duplicates. Classiﬁcation is an
imprecise activity depending on human skill and judgment.
To calculate the number of CVE entries per year, CVE/yr,
for a release rn, we use the CVE entries that occurred from
the release of rn until the next release analyzed rn+1 and
divide by the time interval between release dates of rn and
rn+1 release. The time interval spans fractions of years,
as software is not generally released on January 1st. We
therefore apportion CVE entries based on the fraction of
the year covered by the time interval between release dates.
Let dn denote the day of the year on which rn was released,
and let dn+1 denote the day of the year on which rn+1 was
released. Then if rn was released in y1 and replaced by rn+1
in y2, the CVE entries per year, or CVE/yr for rn is given
by:

cvey1 × (365 − dn) + cvey2 × dn+1

365 − dn + dn+1

More generally, for a release interval spanning m years, m >
2, the CVE/yr is given by:

cvey1 × (365 − dn) + cveym × dn+1 + 365 × Pi=m−1

i=2

cveyi

365 − dn + dn+1 + (m − 2) × 365

If rn and rn+1 were released in the same year, then the
CVE/yr is given by:

cvey1 × (dn+1 − dn)

dn+1 − dn

= cvey1

Particularly for the older CVE entries, it is not always possi-
ble to determine to which versions of the software the entry
applies. Also in many cases there were beta releases preced-
ing the ﬁrst release which we analyzed. These beta releases
do not appear in the software archives and so we could not
analyze them. However, they still have CVE entries. By
weighting the CVE count with the number of days for which
the software was available in any given year we compensate
for these eﬀects.

For example, OpenSSL 0.9.7 was released December 31st,
2002. As shown in table 4 in appendix A, there are 4
CVE entries for OpenSSL 0.9.7. All these entries applied to
beta releases of 0.9.7 which are not in the OpenSSL source
archives. The next release we analyzed was 0.9.7c which was
released on September 30th, 2003 (day 272). The CVE/yr
for 0.9.7 is given by:

4 × (365 − 364) + 7 × 272

365 − 364 + 272

= 6.99

Thus the 4 CVE entries for 2002 are given negligible weight
compared to the 7 for 2003.

We did not have the resources to analyze all consecu-
tive releases of all the software, so we took samples ap-
proximately 12 months apart.
In some cases the sample
time is longer because the software was stable and exhibited

1858.7.6, which was itself released in 1996. We excluded these 7
entries from our count of CVE entries, but believes it justiﬁes
including release 8.7.6 in our analysis, since it was clearly the
subject of security analysis work in 1999.

For easy visual comparison the metrics are scaled as shown
in the ﬁgure so that all seven data sets can be represented
by a common vertical axis. Full analysis details including
the unscaled values are given in appendix A table 5. The
earliest releases we analyzed had a fairly large number of
issues reported by SCA: 2548 (total) and 136 (critical) for
version 8.9.3. This is reﬂected by the large number of CVE
entries per year for that release. The 8.10.0 release had dra-
matically fewer issues (662(total) and 120(critical)) and this
is reﬂected in the drop in CVE entries per year. Note that
although for recent releases of Sendmail SCA is reporting
841 issues and 87 critical issues, this does not mean there
are 841 exploitable bugs. Rather it is an artifact that we did
not write any custom rules for Sendmail to denote defensive
code responsible for cleansing data to make it safe. There-
fore SCA must assume all data being processed by Sendmail
to be unsafe throughout its processing. Of these 841 issues,
572 are unique: multiple paths to a dangerous function call
are each ﬂagged separately. Of these 572 unique issues many
are issues that might lead to denial of service rather than
system compromise, for example 174 potential memory leaks
are identiﬁed.

Over the releases we analyzed substantial amount of ad-
ditional functionality was added to Sendmail. The 8.7.6 re-
lease (September 17 1996) had 11,861 executable lines of
code3. This increased to 15,099 for the 8.9.3 (February 5,
1999). In the next three releases (8.10.0, (March 6, 2000),
8.11.0 (July 19, 2000), 8.11.6 (August 20, 2001) ) the lines of
code count dropped to under 11,000 with signiﬁcant drops
in total number of issues, issue densities and CVE entries
per year. This is possibly indicative of a “clean-up” by the
developers. Over the remaining releases the number of lines
of code increased to just over 32,000 in 8.14.5 (September 15,
2011). With the largest increase coming between 8.12.6 and
8.13.0: 16195 to 31668. This was marked by an increase in
the total number of issues, critical issues and critical+high
issues, but a drop in densities. This may indicate signiﬁcant
eﬀort in improving code quality.

Release 8.13.0 has 0 CVE entries per year. This is because
there were no CVE entries for 2004 and 2005 and then ﬁve
in 2006 (8.13.5) (see appendix A table 4). This pattern
of CVE entries is hard to explain. Possibly it is a normal
statistical variation and an artifact of there being relatively
few undiscovered bugs in the software, or possibly it is due
to delayed reporting. The most recent release of Sendmail
that we analyzed, 8.14.5, has 32,270 lines of executable code.
The drops in issue density and low number of CVE entries
since 2004 (8.13.0) (see table 4), suggests Sendmail has ma-
tured with signiﬁcant attention being paid to code quality.

3.2 Postﬁx

Figure 1: Sendmail issues & CVE entries

Figure 2: Postﬁx issues & CVE entries

very little change. For example for Sendmail we analyzed
8.14.0 which was released on 1st February 2007, the next
release analyzed was 8.14.5 which was released September
15th 2011, this has just 124 more lines of code and 3 fewer
issues detected by SCA. If the CVE entry did not men-
tioned a release which we analyzed, then to determine if we
should count it against one of our analyzed releases we had
to use our judgment and other sources of publicly available
information such as the SecurityFocus database [24] and the
National Vulnerability Database [14]. Thus CVE-2009-4565
begins with “Sendmail before 8.14.4 does not properly han-
dle a ’\0’ character in a Common Name (CN) ﬁeld of an
X.509 certiﬁcate...”. Although release 8.14.4 is not included
in our analysis, we interpret the above to mean the issue
also applies to release 8.14.0 which is included, so CVE-
2009-4565 is included in the calculation of CVE per year for
release 8.14.0.

It will be apparent from the above that our process for
classiﬁcation of CVE entries was manual, relying on our
judgment of the contents of the entry supplemented by fur-
ther manual searches of publicly available sources, so some
errors are possible.

3.1 Sendmail

Sendmail was originally developed by Eric Allman in the
late 1970s and early 1980s. Being one of the earliest Internet
capable programs it was exploited in a number of incidents
including the Morris Internet Worm of 1988 [25]. Figure 1
shows our metrics for various releases of Sendmail from 1996-
2011. CVE information is available from 1999. Even in 1999
there were 7 CVE entries for releases of Sendmail prior to

Postﬁx was originally developed by Wietse Venema in the
late 1990s. Figure 2 shows our metrics for various releases of
Postﬁx from 1999 to 2010. For easy visual comparison the
values are scaled as shown in the ﬁgure so that all seven data
sets can be represented by a common vertical axis. Note that
the range of the vertical axis and scaling of the CVE/yr met-

3We use the lines of code reported by SCA; other tools may
generate slightly diﬀerent counts.

186Figure 3: Sendmail & Postﬁx dangerous function
density

Figure 4: Apache httpd 1.3 issues & CVE entries

ric is very diﬀerent to that shown in ﬁgure 1. Full analysis
details are given in appendix A table 6. Over the course of
the analyzed releases Postﬁx evolved from 10,472 executable
lines of code to 32,470. By most standards Postﬁx has had
an excellent record with relatively few CVE entries over the
years: only 12 (see appendix A table 4). Such a small num-
ber makes it diﬃcult to draw conclusions about the increase
in number of issues reported by SCA compared to the num-
ber of CVE entries. Although the rise from 127 to 394 total
issues does seem to have some relation to 6 of the 12 entries
having occurred in the last 4 years to end of 2011.

The ﬁgures and tables show a striking diﬀerence in the
metrics for Sendmail compared to Postﬁx. For example, for
Postﬁx releases the “Total issues” metric ranges from 127 to
394, whereas for Sendmail releases the metric ranges from
2548 to 548 (841 for the most recent release analyzed). For
“T-density”, the number of issues per 100 lines of code, the
range for all Postﬁx releases varied from 0.96 to 1.33 and
for Sendmail it varied from 2.55 to 21.48. We believe this
demonstrates a fundamentally diﬀerent approach adopted
by Venema in the initial development of Postﬁx compared
to that used by Sendmail, perhaps learning from nearly two
decades of Sendmail experience. One simple measure is the
diﬀerence in use of known “dangerous” C functions by the
two programs. These functions are generally accepted to
be “dangerous” because they don’t perform bounds checks
or the bounds checking can be subverted so that can be
used in exploits to overwrite arbitrary areas of memory. Ex-
amples include strcpy, strcat, memset, memcpy and printf.
Figure 3 shows the density of dangerous functions used in
the various releases of Sendmail and Postﬁx which we an-
alyzed (the units are number of calls per 100 Lines). This
appears to conﬁrm that a fundamentally diﬀerent approach
was taken for the implementation of the initial versions of
Postﬁx. Historically, it made much less use of these danger-
ous functions. For the most recent releases of Sendmail and
Postﬁx the usage is very similar.

The large diﬀerences we see in the SCA analyses for earlier
releases of Sendmail are matched by the much greater num-
ber of CVE entries for Sendmail. However, more recently
Sendmail has had very few issues indeed, indicating much
improved quality.

3.3 Apache httpd

Full details of our analyses of the 1.3, 2.0 and 2.2 Apache
httpd series are given in appendix A table 7. Figure 4 sum-

Figure 5: Apache httpd 2.0 issues & CVE entries

marizes the results for the 1.3 series from 1998 to 2010. Dur-
ing this period the number of lines of executable code in-
creased from 11,079 to 14,201 (with 1.3.19 having the most
at 17099), and the T-density varied from 2.18 to 3.63. The
pattern shown by the ﬁgure is that an initial rise in SCA
measure metrics being matched by a rise in CVE entries per
year. From release 1.3.32, although there is no drop in SCA
measured metrics, there is a drop in the CVE/yr which ap-
pears to be consistent with relatively few changes to mature
code in which most of the serious bugs have been discovered.
This is also reﬂected in the number of lines of executable
code which increased by only 83 lines from 1.3.32 to 1.3.42.
CVE information is shown for release 1.3.2 and later. 1.3.0
was released June 1, 1998 but no CVE information is avail-
able until 1999. The release of 1.3.2 was on September 21,
1998 and 1.3.11 was January 1, 2000. So release 1.3.2 is the
ﬁrst release for which we show CVE information.

The results of the analysis for the 2.0 series from 2002 to
2010 is shown in ﬁgure 5. 2.0.35 (April 6, 2002) is the earli-
est release of the 2.0 series available in the Apache archives,
but we had system compatibility problems and so we were
unable to compile it - a prerequisite to using SCA. Our anal-
ysis therefore begins with 2.0.43 (October 3, 2002). During
this series the number of lines of executable code increased
from 23,982 lines of executable code to 25,720. There was
a modest reduction in some of the SCA measured metrics
such as Total issues and T-density. Other metrics remained
largely unchanged: Critical issues and C-density. From re-
lease 2.0.52 there is a reduction in CVE/yr consistent with
maturing code.

The results of the analysis for the 2.2 series from 2005 to

187Figure 6: Apache httpd 2.2 issues & CVE entries

Figure 8: OpenSSL 0.9.7 issues & CVE entries

Figure 7: OpenSSL 0.9.6 issues & CVE entries

Figure 9: OpenSSL 0.9.8 issues & CVE entries

2011 are shown in ﬁgure 6. During this series the lines of
executable code increased from 28,057 to 30,655. The SCA
measured metrics vary little through this series. There is
also no signiﬁcant reduction in CVE/yr with later releases
in the series.

3.4 OpenSSL

Full details are of our analyses of the OpenSSL 0.9.6, 0.9.7,
0.9.8 and 1.0.0 series are given in appendix A table 8. Fig-
ure 7 summarizes the results for the 0.9.6 series from 2000 to
2004. During this series of releases the number of lines of ex-
ecutable code increased from 44,396 to 45,173. Most of the
SCA measure metrics show a slight increase. The exception
is Critical issues and C-density which show a slight drop.
The comparatively low CVE/yr for the ﬁrst release, 0.9.6,
may be an artifact of the software being new and receiving
little attention [7].

Figure 8 shows the results of the analysis of the OpenSSL
0.9.7 series from 2002 to 2007. During this series of releases
the number of lines of executable code increased from 56,216
to 59,064. It is noticeable that for the 0.9.7.e release SCA
detected zero critical issues (hence C-density is also 0) and
there does seem to be a corresponding drop in CVE/yr for
that release. More generally there is little change in many
of the SCA measured metrics for this release series. The
downward trend exhibited by some such as C-density does
seem to be matched by the reduction in CVE/yr.

Figure 9 shows the results of the analysis of the OpenSSL
0.9.8 series from 2005 to 2011. During this series of re-
leases the number of lines of executable code increased from
71,529 to 75,324. Many of the SCA measured metrics show

little change across releases. The most noticeable change is
in the rise in number of critical issues and c-density which
again seems to show some correspondence to the slight rise
in CVE/yr.

Figure 10 shows the results of the analysis of the OpenSSL
1.0.0 release series from 2009 to 2011. During this release
series the number of lines of executable code increased from
87,987 to 88,603; there is very little change in any of the
SCA measured metrics or CVE/yr.

3.5 Statistical Analysis

Can we show a statistically signiﬁcant correlation between
metrics generated using SCA and CVE/yr? The sample size
for each of the separate nine datasets for Sendmail, Postﬁx,
the three releases of Apache httpd and the four releases of
OpenSSL is too small, in most cases, to show a statistically
signiﬁcant correlation. The largest number of samples we
have is 13 which is for the Apache httpd 1.3 dataset.4 Typ-
ically 50 to 100 samples is required to show a statistically
signiﬁcant correlation. If we combine all our analysis data
we have 75 samples. We performed two separate correla-
tion analyses of the SCA metrics and CVE/yr, one on the
combined unnormalized dataset the second on the combine
normalized dataset, as explained below.

The correlation analysis on the unnormalized datasets uses
the absolute values of the metrics for each dataset. For each
metric the mean used in the calculation is the mean across
all datasets. This tells use the strength of the relationship
between absolute values of a metric and CVE/yr. For exam-

41.3.0 is excluded from the correlation calculation as it was
replaced by 1.3.2 before CVE was established in 1999.

188Figure 10: OpenSSL 1.0.0 issues & CVE entries

Figure 11: Sendmail, Postﬁx & Apache httpd 2.0
Total issues

ple, does a high value of “Total issues” or “T-density” suggest
a high value for CVE/yr?

We normalized each dataset by dividing the values for
CVE/yr and each SCA measured metric by the mean values
for that dataset. Each release series of Apache httpd (1.3,
2.0, 2.2) and OpenSSL (0.9.6, 0.9.7, 0.9.8, 1.0.0) was treated
separately and normalized with means for that release series.
We performed a correlation analysis on the combined nor-
malized dataset of 75 samples. This form of normalization
is not concerned with absolute values. It is concerned with
determining whether or not a change in the value of a metric
with respect to the mean for that release series can explain
a corresponding change in CVE/yr.

Table 1: SCA metrics - CVE/yr correlation
CD
SL
0.044
90
0.120
99
0.093
99
95
0.054
NA 0.015
99
0.105

Metric
Total issues
T-density
Critical issues
C-density
Critcal + high issues
CH-Density

CC
0.211
0.346
0.305
0.232
0.124
0.324

t-value

1.84
3.15
2.73
2.03
1.07
2.92

Table 2: Normalized metrics - CVE/yr correlation

Metric
Total issues
T-density
Critical issues
C-density
Critcal + high issues
CH-Density

CC
0.565
0.559
0.326
0.313
0.495
0.559

t-value

5.85
5.76
2.95
2.82
4.86
5.76

SL
99
99
99
99
99
99

CD
0.319
0.313
0.107
0.098
0.245
0.312

Tables 1 and 2 summarize the results of the correlation
calculations. The meaning of the columns in the tables is
as follows. CC and t-value are the Pearson’s Correlation
Coeﬃcient and t-value for the metric with CVE/yr. SL is
the signiﬁcance level calculated from the t-value: 99 means
that the correlation is signiﬁcant at the 99% level; “NA”
means not acceptable — there is no signiﬁcant correlation.
CD is the Coeﬃcient of Determination which is give by the
square of the correlation coeﬃcient. This is the percentage
of variation in CVE/yr that is “explainable” by the metric:
0.044 is thus 4.4% and 0.319 is 31.9%.

The tables show a moderate correlation for the normalized
SCA metrics of “Total issues“, “T-density” and “CH-density”
which are signiﬁcant at the 99% level and explain over 30%
of the variance in CVE/yr for the software analyzed. Thus
a large increase in the T-density for a new release compared
to a previous release is indicative of an increase in CVE/yr.
A moderate, rather than strong correlation is artifact of the
presence of other factors also discussed in this paper.

The correlation for the unnormalized metrics is weak: the
best being T-density with a correlation coeﬃcient of 0.346
and a coeﬃcient of determination of 12%. This is consistent
with absolute values being less important than a change in
measured metrics with respect to the mean for the series.
The weak correlation for the absolute values of metrics to
CVE/yr, show that drawing conclusions about CVE/yr from
the absolute values of metrics measured from diﬀerent soft-
ware or release series is dangerous.

3.6 The effect of time

We did not explicitly set out to explore the eﬀect of time
on CVE/yr. However, with a few exceptions, the sam-
ples of releases are spaced by approximately a year (see
appendix A). So the pattern of CVE/yr shown in ﬁgures 1-
10 is similar if time is used as the x-axis instead of release
version. Setting aside Sendmail which has releases going
back well beyond our analysis and CVE records, the graphs
suggest a non-linear relationship: an initial rise in CVE/yr
the ﬁrst three to ﬁve years of a release series before a re-
duction. Note that ﬁgure 5 for Apache 2.0 is consistent
with this trend. The ﬁrst release of 2.0 occurred in March
2000 [2], but the ﬁrst release that we could analyze success-
fully, 2.0.43, was not until October 2002. Release 2.0.55 was
a few months after the ﬁfth anniversary of the initial release.

4. COMPARING DIFFERENT SOFTWARE
In section 3.5 we established that there was only a weak
correlation between the absolute values of SCA metrics and
CVE/yr. So using metrics to compare diﬀerent software
is dangerous.
In
particular can anything be said about orders of magnitude
diﬀerences?

In this section we explore this further.

Consider ﬁgures 11 and 12 which shows the “Total issues”
and “T-density” for Sendmail, Postﬁx and the Apache httpd-
2.0 series. If one looks at these graphs for the period 2002 to
2011 one might reasonably expect there to be more security
bugs discovered and hence more CVE entries for Sendmail

189Table 3: Security bug reporters

Apache httpd
OpenSSL
Database-1
Database-2

Tot NA Devs Ext Unique Ext
24
26
26
32

191
173
64
105

1
5
4
1

3
32
15
19

20
21
7
12

Figure 12: Sendmail, Postﬁx & Apache httpd 2.0
T-density

Figure 13: Sendmail, Postﬁx & Apache httpd 2.0
CVE entries

than Apache httpd or Postﬁx — there is generally a factor
2 or greater diﬀerence in the metrics. However, as is shown
by ﬁgure 13, there were signiﬁcantly more CVE entries for
Apache httpd 2.0 than either Sendmail or Postﬁx.

It is clear from the correlation coeﬃcients for absolute val-
ues of SCA metrics with CVE/yr and the above qualitative
example that even a factor 2 diﬀerence in absolute values
does not necessarily indicate a corresponding diﬀerence in
CVE/yr. However, there is an order of magnitude (10x or
20x) between the metrics measured for the earliest releases of
Sendmail and that of Postﬁx and httpd 2.0 which does seem
to show some relationship to the larger number of CVE/yr
for Sendmail during this period. Unfortunately, the dataset
is very small, so this is only weak evidence.

5. DEGREE OF SCRUTINY

For the above analysis we chose the software which we
expected to have a high degree of scrutiny. Sendmail, Post-
ﬁx, Apache httpd and OpenSSL present readily accessible
targets to attackers by providing accessible services to the
Internet. We expected this to motivate signiﬁcant numbers
of security researchers external to the development teams to
scrutinize the code for security ﬂaws. Not all open source
software is subject to the same level of scrutiny. To illustrate
this we looked at two open source databases whose names we
have chosen to redact and for the most recent CVE entries
used the SecurityFocus database [24] to identify the reporter
of the bug. The results are summarized in table 3. “Tot”
(Total) is the total number of CVE entries in the sample.

1One individual reported two bugs
2Developer team credited along with named individuals
3Two individuals reported three bugs each
4One individual reported two bugs
5Two individuals reported two bugs each

“NA” (No Attribution) is the number for which we could
not ﬁnd any attribution to who ﬁrst reported the vulnera-
bility. “Devs” (Developers) is the number of vulnerabilities
reported by the development team. “Ext” (External) is the
number of vulnerabilities reported by security researchers
who, so far as we could tell, are independent of the devel-
opment team. “Unique Ext” (Unique External) is the num-
ber of unique external security researchers (or teams) who
reported vulnerabilities — some reported more than one.
Note that the sum of the 3rd, 4th and 5th columns do not
equal the 2nd column in the case of OpenSSL as a named
individual as well as the developers are jointly credited for
3 vulnerabilities.

This clearly demonstrates that it is the developers of the
databases that are reporting the majority of the security
bugs (15 out of 26 for database-1 and 19 out of 32 for
database-2) in contrast to Apache httpd and OpenSSL where
individuals and groups outside the development team are re-
porting the majority of security bugs. This is consistent with
the hypothesis that Apache httpd and OpenSSL are subject
to a much greater degree of scrutiny than the databases. It is
possible that the relative lack of scrutiny of the databases by
the security community means that only a relatively small
percentage of the vulnerabilities in the code is being reported
and ﬁxed. However, since we cannot safely infer CVE/yr
from absolute values of SCA metrics we can neither conﬁrm
or refute this hypothesis.

6. RELATED WORK

There have been several attempts to build “Vulnerability
Discovery Models” by analyzing vulnerability reporting data
and using it to predict likely future vulnerabilities: [1], [10],
[18] and [21]. A survey of the approaches and shortcom-
ings is given in by Ozment in [19]. Major issues identiﬁed
by Ozment include accounting for the skill and numbers of
security searchers; the discovery of new classes of vulnerabil-
ities that can lead to temporary spikes in discovery; whether
or not discoveries are dependent or independent. By using
an automatic tool we are not dependent on the skill or oth-
erwise of security researchers. However, if a new class of
vulnerability is discovered, we do need to update the tool’s
knowledge-base to account for it.

In [21] Rescorla reports that software quality does not im-
prove overtime, as measured by the rate of defect discovery.
However, Ozment and Schechter studied releases of BSD
to show that software quality does improve over time [20].
Similarly Doyle and Walden demonstrated a drop in the pe-
riod 2006-2010 in the number of issues detected by static

190source code analysis in a set of PHP applications [8]. Our
analysis shows that software quality as measured by metrics
generated from SCA or CVE/yr does not always improve
with each new release: sometimes it is better; sometimes
it is worse; sometimes it is unchanged. Generally CVE/yr
begins to drop three to ﬁve years after the initial release.

Factors extrinsic to the software, such as the degree of
scrutiny, can have an aﬀect on the rate and number of vul-
nerabilities discovered. We did not set out to explore all
possible factors and there are others which we have not con-
sidered. For example, in [7] Clark and colleagues show that
the degree of familiarity can aﬀect the rate of discovery. If
the community is already familiar with the code, then a
vulnerability is likely to be discovered sooner. This has im-
plications for software reuse.

Okun and colleagues [17] have investigated whether or not
using static analysis tools on a project improves security.
However, they do not look at the predictive capabilities of
these tools which is the objective of our work. Nagappan
and Ball [13] studied the use of two static analysis tool to
predict defect density of Windows Server 2003 components.
For the more eﬀective of the two tools they report a statisti-
cally signiﬁcant correlation coeﬃcient of 0.577 which aligns
well with our own results. We have focused exclusively on
security vulnerabilities for open source software. Ayewah
and colleagues [3] investigate the classiﬁcation of issues by a
single static analysis tool. They conclude that it is diﬃcult
for the tool to distinguish trivial bugs and false positives
from serious bugs. We have discussed the reasons for this in
section 2: undecidability and the need for approximations.
This may also be a factor in why there can be a relatively
weak correlation in issues reported by diﬀerent tools:
[13],
[23].

There is a signiﬁcant body of work, including the work of
Brooks [4], on software engineering that explores the general
pathology of bugs: their causes, their lifecycle and what
can be down to prevent them. The focus of our work has
not been to explore the pathology. Rather it has been to
investigate what static analysis of source code can tell us
about the number of security bugs in software.

7. CONCLUSION

In this paper we presented our analysis of the vulnerability
history of various popular open source software using a static
source code analyzer and the rate at which CVE entries oc-
cur. We have demonstrated that the changes in the number
of analyzer identiﬁed security issues between releases of the
same software are statistically signiﬁcantly correlated to the
rate of occurrences of vulnerabilities in CVE, which we take
to be a measure of the number of exploitable bugs in the
code, subject to the limitations described in section 1 and
section 5. The correlation is moderate, as many other fac-
tors are at work, some of which we have discussed. The
correlation is suﬃcient to suggest that that an increase in
the number of issues (or issue density) found by SCA in a
new release is an indication of increase in in CVE/yr and
therefore the number of exploitable bugs. Correspondingly
a decrease in issues or issue-density is an indication of reduc-
tion in CVE/yr and therefore in the number of exploitable
bugs.

This also demonstrates that static source code analysis
can be used to make some assessment of risk even when
constraints do not permit human review of the issues identi-

ﬁed by the analysis. This is necessarily an imprecise assess-
ment of risk: the correlation between exploitable bugs and
changes in issues and issue density is moderate, not strong.
In addition, some defects are likely to pose greater risks than
others.

Both the number and density of analyzer identiﬁed is-
sues need to be considered when asking the question, has
the code quality improved? If a large amount of new code
has been added in the new release, even though there is a
drop in the issue density, the total number of issues might
have increased indicating the potential for a higher number
of vulnerabilities. Our analysis demonstrates that software
quality, as measured by our metrics does not always improve
with each new release. The introduction of large amounts of
new code can decrease quality. The discovery of new classes
of bug can lead to an increase in the rate of CVE entries
[19]. However, generally the rate of CVE entries begins to
drop three to ﬁve years after the initial release.

The degree of scrutiny is important. If there are few or
no issues known it does not mean that the software has no
security issues. As is shown by our analysis of the reporting
history for the databases in section 5, it may mean that the
degree of scrutiny by the security community is low. This
might be a reﬂection that the software in question is not of-
ten accessible directly as an Internet service. For example,
unlike email or web servers, databases are often only acces-
sible indirectly via other services such as application servers,
possibly reducing the incentive to scrutinize the software for
exploitable bugs. There could be a large number of bugs in
the software that are exploitable by an attacker if they can
somehow deliver the attack to the installation, perhaps by
one or more levels of indirection. The Stuxnet attack [9] on
industrial controllers is an example of this. Indirection was
used to target an endpoint whose software had been subject
to little scrutiny and which was not accessible directly to
the attackers.

Our results demonstrate that static source code analysis
cannot be used to compare accurately the number of vul-
nerabilities that are likely to be in diﬀerent software or re-
lease series. We demonstrated this by weak correlations with
CVE/yr to absolute values of metrics generated from the
analyzer and by a qualitative comparison of Apache httpd
2.0, Postﬁx and Sendmail. It is possible that a large order
of magnitude diﬀerence (e.g. greater than 10x) in absolute
values may be signiﬁcant, but the size of our dataset with
orders of magnitude diﬀerences is too small to be deﬁnitive.
There are a number of areas for potential future inves-
tigation. We used a single static analysis tool for our in-
vestigation. A possible next step is to compare diﬀerent
tools: to see how correlation with rate of CVE entries varies
between tools. It would be useful to match speciﬁc CVE en-
tries with corresponding issues identiﬁed by static analysis.
This would enable us to say what percentage of CVE entries
are detected by static analysis. However, the challenges in
doing so are numerous. For example, identifying the speciﬁc
line or lines of code that correspond to a CVE entry may re-
quire studying patches or sample exploits which is diﬃcult
to automate and therefore very time consuming. Another
potential area for future research is the eﬀect of API design,
leading to exploitable bugs when software is used as a com-
ponent of a larger system. For example in 2009, 29 of the 41
CVE entries for OpenSSL were due to mistakes in its use by

191third party software. We deliberately excluded such entries
from this study.

We close by expressing our gratitude to the large commu-
nity of developers and visionaries who have given and con-
tinue to give the Internet community so much useful software
over the years. Mistakes are inevitable in any new pioneering
human endeavor. We hope that our analysis has shown how
the community has learnt from these mistakes and continues
to learn.

8. ACKNOWLEDGMENTS

We thank Chris Dalton, Jonathan Griﬃn, Keith Harrison,
Jack Herrington, Bill Horne, Matias Madou, Brian Mon-
ahan, Miranda Mowbray, Martin Sadler, Jacob West and
Mike Wray for their help and advice. We are also extremely
grateful to the anonymous reviewers for their many con-
structive comments.

9. REFERENCES
[1] O. H. Alhazmi and Y. K. Malaiya. Quantitative
vulnerability assessment of systems software. In
Proceedings of the IEEE Reliability and
Maintainability Symposium, pp615-620, 2005.

[2] Apache release history. http://www.apachehaus.com/.
[3] N. Ayewah, W. Pugh, J. D. Morgenthaler, J. Penix,

and Y. Zhou. Evaluating static analysis defect
warnings on production software. In PASTE ’07
Proceedings of the 7th ACM SIGPLAN-SIGSOFT
workshop on Program analysis for software tools and
engineering, pp1-8, 2007.

[4] F. P. Brooks. The Mythical Man Month and Other
Essays on Software Engineering. Addison Wesley,
1975, 1995(2nd Ed.).

[5] B. Chess and J. West. Secure Programming with Static

Analysis. Pearson Education Inc., Boston,
Massachusetts, 2007.

[6] B. V. Chess. Improving computer security using
extended static checking. In Proceedings of IEEE
Symposium on Security and Privacy, pp160-173, 2002.

[7] S. Clark, S. Frei, M. Blaze, and J. Smith. Familiarity

breeds contempt: the honeymoon eﬀect and the role of
legacy code in zero-day vulnerabilities. In ACSAC ’10:
Proceedings of the 26th Annual Computer Security
Applications Conference, pp251-260, December 2010.

[8] M. Doyle and J. Walden. An empirical study of the

evolution of PHP web application security. In
International Workshop On Security Measurments and
Metrics, MetriSec, 2011.

[9] N. Falliere, L. O. Murchu, and E. Chien. W32.stuxnet

dossier, version 1.4 (February 2011).
http://www.symantec.com/.

[10] R. Gopalakrishna and E. H. Spaﬀord. A trend analysis

of vulnerabilities. In Technical Report 2005-05,
CERIAS, Purdue University, May 2005.

[11] W. Landi. Undecidability of static analysis. ACM

Letters on Programming Languages and Systems
(LOPLAS), 4(1):323–337, December 1992.

[12] The common vulnerabilities and exposures dictionary.

http://cve.mitre.org/.

[13] N. Nagappan and T. Ball. Static analysis tools as

early indicators of pre-release defect density. In ICSE

’05 Proceedings of the 27th international conference on
Software engineering, pp580-586, 2005.

[14] National vulnerability database. http://nvd.nist.gov/.
[15] October 2011 web server survey.

http://news.netcraft.com/.

[16] F. Nielson, H. R. Nielson, and C. Hankin. Principles

of Program Analysis. Springer-Verlag, Berlin,
Germany, 2005, 2nd Ed.

[17] V. Okun, W. Guthrie, R.Gaucher, and P. Black. Eﬀect

of static analysis tools on software security:
preliminary investigation. In QoP ’07: Proceedings of
the 2007 ACM workshop on Quality of protection
pp1-5, October 2007.

[18] A. Ozment. The likelihood of vulnerability rediscovery

and the social utility of vulnerability hunting. In
Workshop on the Economics of Information Security
(WEIS), Cambridge, MA, USA, June 2005.

[19] A. Ozment. Improving vulnerability discovery models.

In QoP ’07: Proceedings of the 2007 ACM workshop
on Quality of protection, pp6-11, October 2007.

[20] A. Ozment and S. E. Schechter. Milk or wine: does

software security improve with age? In Proceedings of
the 15th conference on USENIX Security Symposium -
Volume 15, pp93-104, 2006.

[21] E. Rescorla. Is ﬁnding security holes a good idea?

IEEE Security & Privacy, 3(1):14–19, February 2005.

[22] H. Rice. Classes of recursively enumerable sets and
their decision problems. Trans. Amer. Math. Soc.,
74(2):358–366, March 1953.

[23] N. Rutar, C. B. Almazan, and J. S. Foster. A

comparison of bug ﬁnding tools for java. In ISSRE ’04
Proceedings of the 15th International Symposium on
Software Reliability Engineering, pp245-256, October
2007.

[24] Securityfocus vulnerability database.

http://www.securityfocus.com/vulnerabilities.

[25] E. H. Spaﬀord. The internet worm program: An

analysis. ACM SIGCOMM Computer Communication
Review, 19(1):17–57, January 1989.

APPENDIX

A. ANALYSIS RESULTS

This appendix gives the analysis results for the software
discussed in this paper in tables 5 to 8. Table 4 gives the
number of CVE entries per year from 1999 to 2011 for the
analyzed software. “OS” in table 4 denotes OpenSSL. In
subsequent tables the software analyzed is identiﬁed by the
version number and release date. The release date was de-
termined from the time stamp on the software archive for
that release. In tables 5 to 8 “LOC” is the number of lines
of executable code as measured by SCA. “CI”, “HI” and “LI”
are respectively the number of “Critical” “High” and “Low”
issues measured by SCA. “CHI” is the sum of “CI” and “HI”.
“TI” is the “Total issues” — the sum of “CI”, “HI” and “LI”.
For a fuller explanation of these and the metrics shown in
other columns please see section 3. In table 5 the CVE/yr
calculation for release 8.7.6 of Sendmail does not take into
account 1996-1998 inclusive, since no CVE information is
available for these years. In table 7 “N/A” denotes not ap-
plicable — Apache 1.3.0 was displaced by 1.3.2 before CVE
data was available.

192Year
1999
2000
2001
2002
2003
2004
2005
2006
2007
2008
2009
2010
2011

Sendmail Postﬁx

httpd 1.3

Table 4: CVE Entries
httpd 2.0

httpd 2.2 OS 0.9.6 OS 0.9.7 OS 0.9.8 OS 1.0.0

16
3
6
6
7
0
0
5
1
0
2
0
0

0
0
1
0
2
2
1
0
0
4
0
0
2

4
5
11
11
10
10
3
3
6
3
0
1
4

0
0
2
10
16
16
7
4
8
5
1
1
7

0
0
0
0
0
0
0
6
15
6
7
7
9

0
0
1
4
6
4
3
5
1
3
4
4
4

0
0
0
4
7
4
3
5
2
3
3
4
4

0
0
0
0
0
0
1
5
4
7
9
9
7

0
0
0
0
0
0
0
0
0
0
3
7
8

Table 5: Sendmail
LI

C-density CH-density T-density CVE/yr

Version

Date

8.7.6
8.9.3
8.10.0
8.11.0
8.11.6
8.12.0
8.12.6
8.13.0
8.13.5
8.14.0
8.14.5

17/09/1996
05/02/1999
06/03/2000
19/07/2000
20/08/2001
08/09/2001
27/08/2002
20/06/2004
16/09/2005
01/02/2007
15/09/2011

LOC
11861
15099
10381
10617
10999
15769
16195
31668
31902
32146
32270

CI
136
118
120
124
121
76
76
84
84
87
87

HI
1332
1449

91
95
88
252
257
473
474
511
510

1080
931
451
453
456
220
230
255
255
246
244

CHI
1468
1567
211
219
209
328
333
557
558
598
597

TI
2548
2498
662
672
665
548
563
812
813
844
841

114.66
78.15
115.60
116.79
110.01
48.20
46.93
26.53
26.33
27.06
26.96

123.77
103.78
20.33
20.63
19.00
20.80
20.56
17.59
17.49
18.60
18.50

21.48
16.54
6.38
6.33
6.05
3.48
3.48
2.56
2.55
2.63
2.61

16.00
13.86
3.00
4.75
6.00
6.00
5.00
0.00
3.69
0.63
0.00

Version
19990317

19991231-pl06

20010121

1.1.0
1.1.13
2.0.9
2.1.0
2.1.6
2.2.10
2.4.3
2.4.8
2.4.11
2.6.6

Date

17/03/1999
30/03/2000
21/01/2001
17/01/2002
28/07/2003
18/04/2003
23/04/2004
05/05/2005
05/04/2006
31/05/2007
05/08/2008
12/05/2009
19/03/2010

LOC CI HI
52
10472
56
11925
60
13783
17714
83
82
17720
19827
90
102
22680
22727
102
113
25718
138
30110
138
30182
138
30183
32470
225

8
17
20
31
29
31
40
40
11
14
14
14
21

Table 6: Postﬁx

LI CHI TI C-density CH-density T-density CVE/yr
67
77
102
121
121
135
143
143
137
138
139
139
148

7.64
14.26
14.51
17.50
16.37
15.64
17.64
17.60
4.28
4.65
4.64
4.64
6.47

5.73
6.12
5.80
6.44
6.26
6.10
6.26
6.25
4.82
5.05
5.04
5.04
7.58

1.21
1.26
1.32
1.33
1.31
1.29
1.26
1.25
1.01
0.96
0.96
0.96
1.21

0.00
0.07
0.96
0.47
2.00
2.00
1.67
0.72
0.00
2.01
2.13
0.00
1.10

60
73
80
114
111
121
142
142
124
152
152
152
246

127
150
182
235
232
256
285
285
261
290
291
291
394

193Table 7: Apache httpd

Version

Date

1.3.0
1.3.2
1.3.11
1.3.14
1.3.19
1.3.22
1.3.27
1.3.29
1.3.32
1.3.34
1.3.37
1.3.39
1.3.41
1.3.42
2.0.43
2.0.48
2.0.52
2.0.55
2.0.59
2.0.61
2.0.63
2.0.64
2.2.0
2.2.3
2.2.6
2.2.10
2.2.14
2.2.17
2.2.18

01/06/1998
21/09/1998
20/01/2000
10/10/2000
26/02/2001
09/10/2001
02/10/2002
24/10/2003
18/10/2004
24/10/2005
27/07/2006
04/09/2007
10/01/2008
08/01/2010
03/10/2002
24/10/2003
23/09/2004
10/10/2005
27/07/2006
04/09/2007
01/01/2008
14/10/2010
29/11/2005
27/07/2006
04/09/2007
07/10/2008
24/09/2009
14/10/2010
11/05/2011

LOC CI HI
92
11079
103
11870
167
16826
241
16976
17099
241
239
13774
240
13854
239
14014
14119
239
241
14170
241
14169
241
14193
241
14198
14201
241
114
23982
121
24804
115
25325
25581
115
117
25594
97
25670
97
25679
25720
97
108
28057
108
27962
100
28200
113
29801
30157
115
116
30550
30655
119

28
35
61
91
91
91
91
92
92
92
92
92
92
92
33
35
35
35
35
35
35
35
20
20
20
21
22
23
23

LI CHI TI C-density CH-density T-density CVE/yr
122
125
153
180
180
170
169
167
167
168
168
168
168
168
158
149
148
151
149
150
150
150
167
165
162
166
165
158
159

25.27
29.49
36.25
53.61
53.22
66.07
65.69
65.65
65.16
64.93
64.93
64.82
64.80
64.78
13.76
14.11
13.82
13.68
13.68
13.63
13.63
13.61
7.13
7.15
7.09
7.05
7.30
7.53
7.50

10.83
11.63
13.55
19.56
19.42
23.96
23.89
23.62
23.44
23.50
23.50
23.46
23.45
23.45
6.13
6.29
5.92
5.86
5.94
5.14
5.14
5.13
4.56
4.58
4.26
4.50
4.54
4.55
4.63

120
138
228
332
332
330
331
331
331
333
333
333
333
333
147
156
150
150
152
132
132
132
128
128
120
134
137
139
142

242
263
381
512
512
500
500
498
498
501
501
501
501
501
305
305
298
301
301
282
282
282
295
293
282
300
302
297
301

2.18
2.22
2.26
3.02
2.99
3.63
3.61
3.55
3.53
3.54
3.54
3.53
3.53
3.53
1.27
1.23
1.18
1.18
1.18
1.10
1.10
1.10
1.05
1.05
1.00
1.01
1.00
0.97
0.98

N/A
3.20
5.00
7.42
11.00
11.00
10.24
10.00
4.42
3.00
4.83
5.79
1.48
2.51
14.60
16.00
9.36
4.86
6.44
8.00
2.44
5.93
5.18
11.48
8.68
6.76
7.00
8.24
9.00

Version

Date

0.9.6
0.9.6c
0.9.6h
0.9.6l
0.9.6m
0.9.7
0.9.7c
0.9.7e
0.9.7i
0.9.7l
0.9.7m
0.9.8
0.9.8d
0.9.8g
0.9.8i
0.9.8l
0.9.8p
0.9.8r

1.0.0-beta4

1.0.0
1.0.0b
1.0.0c
1.0.0.d

24/09/2000
21/12/2001
08/12/2002
04/11/2003
17/03/2004
31/12/2002
30/09/2003
25/10/2004
14/10/2005
28/09/2006
23/02/2007
05/07/2005
28/09/2006
19/10/2007
15/09/2008
05/11/2009
16/11/2010
08/02/2011
10/11/2009
29/03/2010
16/11/2010
02/12/2010
08/02/2011

Table 8: OpenSSL

LOC
44396
44732
45089
45161
45173
56216
56331
57762
59787
59930
59064
71529
72087
72587
73322
74770
75318
75324
87987
88629
88604
88597
88603

CI
20
17
17
16
16
22
18
0
8
8
8
123
123
118
136
144
141
141
129
133
134
134
134

HI
1481
1474
1473
1863
1861
132
132
138
150
150
146
316
315
317
275
282
288
288
293
295
295
295
295

LI
467
461
387
452
454
573
564
565
570
572
546
580
569
570
584
585
546
546
612
617
572
572
573

CHI
1501
1491
1490
1879
1877
154
150
138
158
158
154
439
438
435
411
426
429
429
422
428
429
429
429

TI
1968
1952
1877
2331
2331
727
714
703
728
730
700
1019
1007
1005
995
1011
975
975
1034
1045
1001
1001
1002

C-density CH-density T-density CVE/yr

4.50
3.80
3.77
3.54
3.54
3.91
3.20
0.00
1.34
1.33
1.35
17.20
17.06
16.26
18.55
19.26
18.72
18.72
14.66
15.01
15.12
15.12
15.12

33.81
33.33
33.05
41.61
41.55
2.74
2.66
2.39
2.64
2.64
2.61
6.14
6.08
5.99
5.61
5.70
5.70
5.70
4.80
4.83
4.84
4.84
4.84

4.43
4.36
4.16
5.16
5.16
1.29
1.27
1.22
1.22
1.22
1.19
1.42
1.40
1.38
1.36
1.35
1.29
1.29
1.18
1.18
1.13
1.13
1.13

0.78
3.91
5.85
4.87
3.49
6.99
4.71
3.19
4.55
3.93
3.23
3.40
4.25
6.33
8.48
9.00
8.10
7.00
5.50
7.00
7.00
7.56
8.00

194