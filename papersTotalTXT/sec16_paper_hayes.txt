k-fingerprinting: A Robust Scalable Website 

Fingerprinting Technique

Jamie Hayes and George Danezis, University College London

 https://www.usenix.org/conference/usenixsecurity16/technical-sessions/presentation/hayes

This paper is included in the Proceedings of the 25th USENIX Security SymposiumAugust 10–12, 2016 • Austin, TXISBN 978-1-931971-32-4Open access to the Proceedings of the 25th USENIX Security Symposium is sponsored by USENIX k-ﬁngerprinting: a Robust Scalable Website Fingerprinting Technique

Jamie Hayes

University College London

j.hayes@cs.ucl.ac.uk

George Danezis

University College London
g.danezis@ucl.ac.uk

Abstract

Website ﬁngerprinting enables an attacker to infer which
web page a client is browsing through encrypted or
anonymized network connections. We present a new
website ﬁngerprinting technique based on random deci-
sion forests and evaluate performance over standard web
pages as well as Tor hidden services, on a larger scale
than previous works. Our technique, k-ﬁngerprinting,
performs better than current state-of-the-art attacks even
against website ﬁngerprinting defenses, and we show
that it is possible to launch a website ﬁngerprinting at-
tack in the face of a large amount of noisy data. We
can correctly determine which of 30 monitored hidden
services a client is visiting with 85% true positive rate
(TPR), a false positive rate (FPR) as low as 0.02%, from
a world size of 100,000 unmonitored web pages. We fur-
ther show that error rates vary widely between web re-
sources, and thus some patterns of use will be predictably
more vulnerable to attack than others.
1
Traditional encryption obscures only the content of com-
munications and does not hide metadata such as the size
and direction of trafﬁc over time. Anonymous communi-
cation systems obscure both content and metadata, pre-
venting a passive attacker from observing the source or
destination of communication.

Introduction

Anonymous communications tools, such as Tor [11],
route trafﬁc through relays to hide its ultimate desti-
nation. Tor is designed to be a low-latency system to
support interactive activities such as instant messaging
and web browsing, and does not signiﬁcantly alter the
shape of network trafﬁc. This allows an attacker to ex-
ploit information leaked via the order, timing and vol-
ume of resources requested from a website. As a re-
sult, many works have shown that website ﬁngerprint-
ing attacks are possible even when a client is browsing
with encryption or using an anonymity tool such as Tor
[7, 16, 17, 21, 23, 27, 32, 36, 38, 39].

Website ﬁngerprinting is commonly formulated as a
classiﬁcation problem. An attacker wishes to know
whether a client browses one of n web pages. The at-
tacker ﬁrst collects many examples of trafﬁc traces from
each of the n web pages by performing web-requests
through the protection mechanism under attack; features
are extracted and a machine learning algorithm is trained
to classify the website using those features. When a
client browses a web page, the attacker passively collects
the trafﬁc, passes it in to their classiﬁer and checks if the
client visited one of the n web pages. In the literature
this is referred to as the closed-world setting – a client
is restricted to browse a limited number of web pages,
monitored by the attacker. However, the closed-world
model has been criticized for being unrealistic [17, 29]
since a client is unlikely to only browse a limited set of
web pages. The open-world setting attempts to model a
more realistic setting where the attacker monitors a small
number of web pages, but allows a client to additionally
browse to a large world size of unmonitored web pages.
Our attack is based on random decision forests [6], an
ensemble method using multiple decision trees. We ex-
tend the random forest technique to allow us to extract
ﬁngerprints to perform identiﬁcation in an open-world.

The key contributions of this work are as follows:

• A new attack, k-ﬁngerprinting, based on extracting a
ﬁngerprint for a web page via a novel variant of ran-
dom forests. We show k-ﬁngerprinting is more accu-
rate and faster than other state-of-the-art website ﬁn-
gerprinting attacks [7, 28, 39] even under proposed
website ﬁngerprinting defenses.

• An analysis of the features used in this and prior work
to determine which yield the most information about
an encrypted or anonymized web page. We show that
simple features such as counting the number of packets
in a sequence leaks more information about the iden-
tity of a web page than complex features such as packet
ordering or packet inter-arrival time features.

• An open-world setting that is an order of magnitude

USENIX Association  

25th USENIX Security Symposium  1187

1

larger than the previous open-world website ﬁnger-
printing work of 5,000 unmonitored web pages [39]1,
and nearly twice as large in terms of unique numbers
websites than [28], reﬂecting a more realistic website
ﬁngerprinting attack over multiple browsing sessions.
In total we tested k-ﬁngerprinting on 101,130 unique
websites2.

• We show that a highly accurate attack can be launched
by training a small fraction of the total data, greatly
reducing the start-up cost an attacker would need to
perform the attack.

• We observe that the error rate is uneven and so it may
be advantageous to throw away some training infor-
mation that could confuse a classiﬁer. An attacker can
learn the error rate of their attack from the training set,
and use this information to select which web pages
they wish to monitor in order to minimize their error
rates.

• We conﬁrm that browsing over Tor does not provide
any additional protection against ﬁngerprinting attacks
over browsing using a standard web browser. Further-
more we show that k-ﬁngerprinting is highly accurate
on Tor hidden services, and that they can be distin-
guished from standard web pages.

2 Related Work
Website Fingerprinting. Website ﬁngerprinting has
been studied extensively. Early work by Wagner and
Schneier [34], Cheng and Avnur [10] exposed the pos-
sibility that encrypted HTTP GET requests may leak in-
formation about the URL, conducting preliminary ex-
periments on a small number of websites. They asked
clients in a lab setting to browse a website for 5-10 min-
utes, pausing two seconds between page loading. With
caching disabled they were able to correctly identify 88
pages out of 92 using simple packet features. Early
website ﬁngerprinting defenses were usually designed
to safeguard against highly speciﬁc attacks.
In 2009,
Wright et al.
[40] designed ‘trafﬁc morphing’ that al-
lowed a client to shape their trafﬁc to look as if it was
generated from a different website. They were able to
show that this defense does well at defeating early web-
site ﬁngerprinting attacks that heavily relied on exploit-
ing unique packet length features [21, 32].

1[17] considers an open world size of ∼35K but only tried to sep-
arate monitored pages from unmonitored pages instead of further clas-
sifying the monitored pages to the correct website. The authors as-
sume the adversary monitors four pages: google.com, facebook.com,
wikipedia.org and twitter.com. They trained a classiﬁer using 36 traces
for each of the Alexa Top 100 web pages, including the web pages of
the monitored pages. The four traces for each of the monitored sites
plus one trace for each of the unmonitored sites up to ∼35K are used
for testing.
2All code will be made available through code repositories under
a liberal open source license and data will be deposited in open data
repositories.

In a similar fashion, Tor pads all packets to a ﬁxed-
size cells of 512 bytes. Tor also implemented random-
ized ordering of HTTP pipelines [30] in response to the
attack by Panchenko et al. [27] who used packet order-
ing features to train an SVM classiﬁer. This attack on Tor
achieved an accuracy of 55%, compared to a previous at-
tack that did not use such ﬁne grained features achieving
3% accuracy on the same data set using a Naive-Bayes
classiﬁer [16]. Other defenses such as the decoy defense
[27] loads a camouﬂage website in parallel to a legiti-
mate website, adding a layer of background noise. They
were able to show using this defense attack accuracy of
the SVM again dropped down to 3%.

Luo et al.

[24] designed the HTTPOS ﬁngerprint-
ing defense at the application layer. HTTPOS acts as
a proxy accepting HTTP requests and obfuscating them
before allowing them to be sent. It modiﬁes network fea-
tures on the TCP and HTTP layer such as packet size,
packet time and payload size, along with using HTTP
pipelining to obfuscate the number of outgoing packets.
They showed that HTTPOS was successful in defending
against a number of classiﬁers [5, 9, 21, 32].

More recently Dyer et al.

[12] created a defense,
BuFLO, that combines many previous countermeasures,
such as ﬁxed packet sizes and constant rate trafﬁc. Dyer
et al. showed this defense improved upon other defenses
at the expense of a high bandwidth overhead. Cai et al.
[8] made modiﬁcations to the BuFLO defense based on
rate adaptation again at the expense of a high bandwidth
overhead. Following this Nithyanand et al.
[25] pro-
posed Glove, that groups website trafﬁc into clusters that
cannot be distinguished from any other website in the
set. This provides information theoretic privacy guaran-
tees and reduces the bandwidth overhead by intelligently
grouping web trafﬁc in to similar sets.

Cai et al. [7] modiﬁed the kernel in Panchenko et al.’s
SVM to improve an attack on Tor, and was further im-
proved in an open-world setting by Wang and Goldberg
in 2013 [38], achieving a true positive rate (TPR) of over
0.95 and a false positive rate (FPR) of 0.002 when moni-
toring one web page. Wang et al. [39] conducted attacks
on Tor using large open-world sets. Using a k-nearest
neighbor classiﬁer they achieved a TPR of 0.85 and FPR
of 0.006 when monitoring 100 web pages out of 5100
web pages. More recently Wang and Goldberg [37] sug-
gested a defense using a browser in half-duplex mode –
meaning a client cannot send multiple requests to servers
in parallel. In addition to this simple modiﬁcation they
add random padding and show they can even foil an at-
tacker with perfect classiﬁcation accuracy with a com-
paratively (to other defenses) small bandwidth overhead.
Wang and Goldberg [36] then investigated the practical
deployment of website ﬁngerprinting attacks on Tor. By
maintaining an up-to-date training set and splitting a full

1188  25th USENIX Security Symposium 

USENIX Association

2

packet sequence in to components comprising of differ-
ent web page load traces they show that practical web-
site ﬁngerprinting attacks are possible. By considering a
time gap of 1.5 seconds between web page loads, their
splitting algorithm can successfully parse a single packet
sequence in to multiple packet sequences with no loss in
website ﬁngerprinting accuracy. Gu et al. [15] studied
website ﬁngerprinting in multi-tab browsing setting. Us-
ing a Naive Bayes classiﬁer on the 50 top ranked web-
sites in Alexa, they show when tabs are opened with a
delay of 2 seconds, they can classify the ﬁrst tab with
75.9% accuracy, and the background tab with 40.5%.
More recently, Kwon et al. [19] showed that website ﬁn-
gerprinting attacks can be applied to Tor hidden services,
and due to the long lived structure of hidden services, at-
tacks can be even more accurate than when compared to
non-hidden pages. They correctly deanonymize 50 mon-
itored hidden service servers with TPR of 88% and FPR
of 7.8% in an open world setting. We further improve on
this in our work, resulting in a more accurate attack on
the same data set.

In concurrent work, Panchenko et al.

[28] have ex-
perimented with large datasets. Using a mix of differ-
ent sources they produced two datasets, one of 34,580
unique websites (118,884 unique web pages) and another
of 65,409 unique websites (211,148 unique web pages).
Using a variation of a sequence of cumulative summa-
tions of packet sizes in a trafﬁc trace they show their at-
tack, CUMUL, was of similar accuracy to k-NN [39] un-
der normal browsing conditions. However, we show that
due to their feature set dependency on order and packet
counting, their attack suffers substantially under simple
website ﬁngerprinting defenses and is outperformed by
our technique, k-ﬁngerprinting.
Random Forests. Random forests are a classiﬁcation
technique consisting of an ensemble of decision trees,
taking a consensus vote of how to classify a new ob-
ject. They have been shown to perform well in classi-
ﬁcation, regression [6, 20] and anomaly detection [22].
Each tree in the forest is trained using labeled objects
represented as feature vectors of a ﬁxed size. Training
includes some randomness to prevent over-ﬁtting:
the
training set for each tree is sampled from the available
training set with replacement. Due to the bootstrap sam-
pling process there is no need for k-fold cross validation
to measure k-ﬁngerprinting performance, it is estimated
via the unused training samples on each tree [6]. This is
referred to as the out-of-bag score.
3 Attack Design
We consider an attacker that can passively collect a
client’s encrypted or anonymized web trafﬁc, and aims
to infer which web resource is being requested. Dealing
with an open-world, makes approaches based purely on

k-ﬁngerprints from random forests

classifying previously seen websites inapplicable. Our
technique, k-ﬁngerprinting, aims to deﬁne a distance-
based classiﬁer.
It automatically manages unbalanced
sized classes and assigns meaningful distances between
packet sequences, where close-by ‘ﬁngerprints’ denote
requests likely to be for the same resources.
3.1
In this work we use random forests to extract a ﬁnger-
print for each trafﬁc instance3, instead of using directly
the classiﬁcation output of the forest. We deﬁne a dis-
tance metric between two traces based on the output of
the forest: given a feature vector each tree in the for-
est associates a leaf identiﬁer with it, forming a vector
of leaf identiﬁers for the item, which we refer to as the
ﬁngerprint.

Once ﬁngerprint vectors are extracted for two traces,
we use the Hamming4 distance to calculate the distance
between these ﬁngerprints5. We classify a test instance
as the label of the closest k training instances via the
Hamming distance of ﬁngerprints – assuming all labels
agree. We evaluate the effect of varying k, the number of
ﬁngerprints used for comparison, in Sections 7, 8 and 9.
This leafs vector output represents a robust ﬁngerprint:
we expect similar trafﬁc sequences are more likely to
fall on the same leaves than dissimilar trafﬁc sequences.
Since the forest has been trained on a classiﬁcation task,
traces from the same websites are preferentially aggre-
gated in the same leaf nodes, and those from different
websites kept apart.

We can vary the number of training instances k a ﬁn-
gerprint should match, to allow an attacker to trade the
true positive rate (TPR) for false positive rate (FPR). This
is not possible using the direct classiﬁcation of the ran-
dom forest. By using a k closest ﬁngerprint technique
for classiﬁcation, the attacker can choose how they wish
to decide upon ﬁnal classiﬁcation6. For the closed-world
setting we do not need the additional ﬁngerprint layer for
classiﬁcation, we can simply use the classiﬁcation output
of the random forest since all classes are balanced and
our attack does not have to differentiate between False
Positives and False Negatives. For the closed-world set-
ting we measure the mean accuracy of the random forest.

3We deﬁne a trafﬁc instance as the network trafﬁc generated via a

web page load.

4We experimented with using the Hamming, Euclidean, Maha-
lanobis and Manhattan distance functions and found Hamming to pro-
vide the best results.

5For example, given the Hamming distance function d : V ×V → R,
where V is the space of leaf symbols, we expect given two packet se-
quences generated from loading google.com, with ﬁngerprints vectors
f1,
f2 and a packet sequence generated from loading facebook.com
with ﬁngerprint f3, that d( f1, f2) < d( f1, f3) and d( f1, f2) < d( f2, f3).
6We chose to classify a trafﬁc instance as a monitored page if all k
ﬁngerprints agree on the label, but an attacker could choose some other
metric such as majority label out of the k ﬁngerprints.

USENIX Association  

25th USENIX Security Symposium  1189

3

3.2 The k-ﬁngerprinting attack
Our k-ﬁngerprinting attack proceeds in two phases: The
attacker chooses which web pages they wish to moni-
tor and captures network trafﬁc generated via loading the
monitored web pages and a large number of unmonitored
web pages. These target traces for monitored websites,
along with some traces for unmonitored websites, are
used to train a random forest for classiﬁcation. Given
a packet sequence representing each training instance of
a monitored web page, it is converted to a ﬁxed length
ﬁngerprint as described in Section 3.1 and stored.

The attacker then passively collects instances of web
page loads from a client’s browsing session. A ﬁnger-
print is extracted from the newly collected packet se-
quence. The attacker then computes the Hamming dis-
tance of this new ﬁngerprint against the corpus of ﬁn-
gerprints collected during training and is classiﬁed as a
monitored page if and only if all k ﬁngerprints agree on
classiﬁcation, as described in Section 3.1, otherwise it is
classiﬁed as an unmonitored page.

We deﬁne the following performance measures for the

attack:
• True Positive Rate (TPR). The probability that a
monitored page is classiﬁed as the correct monitored
page.

• False Positive Rate (FPR). The probability that an un-
monitored page is incorrectly classiﬁed as a monitored
page.

• Bayesian Detection Rate (BDR). The probability that
a page corresponds to the correct monitored page given
that the classiﬁer recognized it as that monitored page.
Assuming a uniform distribution of pages BDR can be
found from TPR and FPR using the formula

T PR.Pr(M)

(T PR.Pr(M) +FPR. Pr(U))

where

Pr(M) = |Monitored|
|Total Pages|

, Pr(U) =1 − P(M).

Ultimately BDR indicates the practical feasibility of the
attack as it measures the main concern of the attacker, the
probability that the classiﬁer made a correct prediction.
4 Data gathering
We collect two data sets: one via Tor7 DSTor, and an-
other via a standard web browser, DSNorm. DSNorm
consists of 30 instances from each of 55 monitored
web pages, along with 7,000 unmonitored web pages
chosen from Alexa’s top 20,000 web sites [1]. We
collected DSNorm using a number of Amazon EC2
7The most recent version at the time of collection was used, Tor

Browser 5.0.6.

instances8, Selenium9 and the headless browser
PhantomJS10. We used tcpdump11 to collect network
traces for 20 seconds with 2 seconds between each web
page load. Monitored pages were collected in batches
of 30 and unmonitored web pages were collected suc-
cessively. Page loading was performed with no caches
and time gaps between multiple loads of the same web
page, as recommended by Wang and Goldberg [38]. We
chose to monitor web pages from Alexa’s top 100 web
sites [1] to provide a comparison with the real world cen-
sored web pages used in the Wang et al. [39] data set12.
DSTor was collected in a similar manner to DSNorm but
was collected via the Tor browser. DSTor consists of two
subsets of monitored web pages: (i) 100 instances from
each of the 55 top Alexa monitored web pages and (ii)
80 instances from each of 30 popular Tor hidden ser-
vices. A Tor hidden service is a website that is hosted
behind a Tor client’s Onion Proxy, which serves as the
interface between application and network. Tor hidden
services allow both a client accessing the website and the
server hosting the website to remain anonymous to one
another and any external observers. We chose hidden ser-
vices to ﬁngerprint based on popularity as listed by the
.onion search engine Ahmia13. The unmonitored set
is comprised of the top 100,000 Alexa web pages, ex-
cluding the top 55. We chose to ﬁngerprint web pages
as listed by Alexa as these constitute the most popular
web pages in the world over extended periods of time,
and hence provide a more realistic dataset than choosing
pages at random and/or using transiently popular website
links as included in Panchenko et al.’s recent work [28].
By including website visits to trending topics we argue
that this diminishes the ability to properly measure how
effective a website ﬁngerprinting attack will perform in
general.

For comparison to previous work, we evaluated our at-
tack on one of the previous largest website ﬁngerprinting
data sets [39], which collected 90 instances from each of
100 monitored sites, along with 5000 unmonitored web
pages. The Wang et al. monitored web pages are various
real-world censored websites from UK, Saudi Arabia and
China providing a realistic set of web pages an attacker
may wish to monitor. The unmonitored web pages are
chosen at random from Alexa’s top 10,000 websites –
with no intersection between monitored and unmonitored
web pages.

8https://aws.amazon.com/ec2/
9http://www.seleniumhq.org/
10http://phantomjs.org/
11http://www.tcpdump.org/
12We used TCP/IP packets for ﬁnal classiﬁcation over abstracting
to the Tor cell layer [38], preliminary experiments showed no consis-
tent improvements from using one data layer for classiﬁcation over the
other.

13http://www.ahmia.fi/

1190  25th USENIX Security Symposium 

USENIX Association

4

This allows us to validate k-ﬁngerprinting on two
different data sets while allowing for direct compari-
son against the state-of-the-art k-Nearest Neighbor at-
tack [39]. We can also infer how well the attack works
on censored web pages which may not have small land-
ing pages or be set up for caching like websites in the top
Alexa list. Testing k-ﬁngerprinting on both real-world
censored websites and top alexa websites indicates how
the attack performs across a wide range of websites.

For the sake of comparison, according to a study by
research ﬁrm Nielsen [3] the number of unique websites
visited per month by an average client in 2010 was 89.
Another study [17, 26] collected web site statistics from
80 volunteers in a virtual ofﬁce environment. Trafﬁc was
collected from each volunteer for a total of 40 hours. The
mean unique number of websites visited per volunteer
was 484, this is substantially smaller than the world sizes
we consider in our experiments.
5 Feature selection
Our ﬁrst contribution is a systematic analysis of fea-
ture importance. Despite some preliminary work by
Panchenko et al. [27], there is a notable absence of fea-
ture analysis in the website ﬁngerprinting literature. In-
stead features are picked based on heuristic arguments.
All feature importance experiments were performed with
the Wang et al. data set [39] so as to allow direct com-
parison with their attack results.

We train a random forest classiﬁer in the closed-world
setting using a feature vector comprised of features in
the literature, and labels corresponding to the moni-
tored sites. We use the gini coefﬁcient as the purity
criterion for splitting branches and estimate feature im-
portance using the standard methodology described by
Breiman [2, 6, 13]. Each time a decision tree branches
on a feature the weighted sum of the gini impurity index
for the two descendant nodes is higher than the purity of
the parent node. We add up the gini decrease for each in-
dividual feature over the entire forest to get a consistent
measure of feature importance.

Figure 1 illustrates the effect of using a subset of fea-
tures for random forest classiﬁcation. We ﬁrst train a
random forest classiﬁer to establish feature importance;
and then train new random forests with only subsets of
the most informative features in batches of ﬁve. As we
increase the number of features we observe a monotonic
increase in accuracy; however there are diminishing re-
turns as we can achieve nearly the same accuracy after
using the 30 most important features. We chose to use
150 features in all following experiments since the dif-
ference in training time when using fewer features was
negligible.

Figure 2 identiﬁes the top-20 ranked features and illus-
trates their variability across 100 repeated experiments.

Figure 1: Accuracy of k-ﬁngerprinting in a closed-world
setting as the number of features is varied.

As seen in Figure 1 there is a reduction in gradient when
combining the top 15 features compared to using the top
10 features. Figure 2 shows that the top 13 features are
comparatively much more important than the rest of the
top 20 features, hence there is only a slight increase in
accuracy when using the top 15 features compared to us-
ing the top 10. After the drop between the rank 13 and
rank 14 features, feature importance falls steadily until
feature rank 40, after which the reduction in feature im-
portance is less prominent14. Note that there is some
interchangeability in rank between features, we assign
ranks based on the average rank of a feature over the 100
experiments.
Feature Importance
From each packet sequence we extract the following fea-
tures:
• Number of packets statistics. The total number of
packets, along with the number of incoming and out-
going packets [12, 27, 39]. The number of incoming
packets during transmission is the most important fea-
ture, and together with the number of outgoing packets
during transmission are always two of the ﬁve most im-
portant features. The total number of packets in trans-
mission has rank 10.

• Incoming & outgoing packets as fraction of total
packets. The number of incoming and outgoing pack-
ets as a fraction of the total number of packets [27].
Always two of the ﬁve most important features.

• Packet ordering statistics. For each successive in-
coming and outgoing packet, the total number of pack-
ets seen before it in the sequence [7, 27, 39]. The stan-
dard deviation of the outgoing packet ordering list has
rank 4, the average of the outgoing packet ordering list
has rank 7. The standard deviation of the incoming
packet ordering list has rank 12 and the average of the

14The total feature importance table is shown in Appendix A.

5

USENIX Association  

25th USENIX Security Symposium  1191

packets feature list. The outgoing packets feature list
split into 20 evenly sized subsets and sum each sub-
set. This creates a new list of features. Similarly to the
concentration feature list, the alternative concentration
feature list are regularly in the top 20 most important
features and bottom 50 features. Note though concen-
tration features are never seen in the top 15 most im-
portant features whereas alternative concentration fea-
tures are, – at rank 14 and 15, – so information is
gained by summing the concentration subsets.

• Packet inter-arrival time statistics. For the total, in-
coming and outgoing packet streams extract the lists
of inter-arrival times between packets. For each list
extract the max, mean, standard deviation, and third
quartile [5]. These features have rank between 40 and
70.

• Transmission time statistics. For the total, incom-
ing and outgoing packet sequences we extract the ﬁrst,
second, third quartile and total transmission time [39].
These features have rank between 30 and 50. The total
transmission time for incoming and outgoing packet
streams are the most important out of this subset of
features.

• Alternative number of packets per second features.
For the number of packets per second feature list we
create 20 even sized subsets and sum each subset. The
sum of all subsets is the 9th most important feature.
The features produced by each subset are in the bottom
50 features - with rank 101 and below. The important
features in this subset are the ﬁrst few features with
rank between 66 and 78, that are calculated from the
ﬁrst few seconds of a packet sequence.
We conclude that the total number of incoming pack-
ets is the most informative feature. This is expected as
different web pages have different resource sizes, that
are poorly hidden by encryption or anonymization. The
number of incoming and outgoing packets as a fraction
of the total number of packets are also informative for
the same reason.

The least important features are from the padded con-
centration of outgoing packets list, since the original con-
centration of outgoing packets lists were of non-uniform
size and so have been padded with zeros to give uni-
form length. Clearly, if most packet sequences have been
padded with the same value this will provide a poor cri-
terion for splitting, hence being a feature of low impor-
tance. Packet concentration statistics, while making up
the bulk of “useless features” also regularly make up a
few of the top 30 most important features, they are the
ﬁrst few items that are unlikely to be zero.
In other
words, the ﬁrst few values in the packet concentration
list do split the data well.

Packet ordering features have rank 4, 7, 12 and 13,
indicating these features are a good criterion for classiﬁ-

Feature Description
Number of incoming packets.
Number of outgoing packets as a fraction of the total number
of packets.
Number of incoming packets as a fraction of the total number
of packets.
Standard deviation of the outgoing packet ordering list.
Number of outgoing packets.
Sum of all items in the alternative concentration feature list.
Average of the outgoing packet ordering list.
Sum of incoming, outgoing and total number of packets.
Sum of alternative number packets per second.
Total number of packets.
Packet concentration and ordering features list.
The total number of incoming packets stats in ﬁrst 30 packets.
The total number of outgoing packets stats in ﬁrst 30 packets.

1.
2.

3.

4.
5.
6.
7.
8.
9.
10.
11-18.
19.
20.

Figure 2: The 20 most important features.

incoming packet ordering list has rank 13.

• Concentration of outgoing packets. The packet se-
quence split into non-overlapping chunks of 20 pack-
ets. Count the number of outgoing packets in each of
the chunks. Along with the entire chunk sequence, we
extract the standard deviation (rank 16), mean (rank
11), median (rank 64) and max (rank 65) of the se-
quence of chunks. This provides a snapshot of where
outgoing packets are concentrated [39]. The features
that make up the concentration list are between the 15th
and 30th most important features, but also make up the
bulk of the 75 least important features.

• Concentration of incoming & outgoing packets in
ﬁrst & last 30 packets. The number of incoming and
outgoing packets in the ﬁrst and last 30 packets [39].
The number of incoming and outgoing packets in the
ﬁrst thirty packets has rank 19 and 20, respectively.
The number of incoming and outgoing packets in the
last thirty packets has rank 50 and 55, respectively.

• Number of packets per second. The number of pack-
ets per second, along with the mean (rank 44), standard
deviation (rank 38), min (rank 117), max (42), median
(rank 50).

• Alternative concentration features. This subset of
features is based on the concentration of outgoing

1192  25th USENIX Security Symposium 

USENIX Association

6

cation. Packet ordering features exploit the information
leaked via the way in which browsers request resources
and the end server orders the resources to be sent. This
supports conclusions in [7, 39] about the importance of
packet ordering features.

We also found that the number of incoming and out-
going packets in the ﬁrst thirty packets, with rank 19 and
20, were more important than the number of incoming
and outgoing packets in the last thirty packets, with rank
50 and 55.
In the alternative number packets per sec-
ond feature list the earlier features were a better criterion
for splitting than the later features in the list. This sup-
ports claims by Wang et al. [39] that the beginning of
a packet sequence leaks more information than the end
of a packet sequence.
[5]
we found packet inter-arrival time statistics, with rank
between 40 and 70, only slightly increase the attack ac-
curacy, despite being a key feature in their work.

In contrast to Bissias et al.

6 Attack on hardened defenses
For direct comparison we tested our random forest classi-
ﬁer in a closed-world setting on various defenses against
the k-NN attack and the more recent CUMUL [28] at-
tack using the Wang et al. data set [39]. Note that most
of these defenses require large bandwidth overheads that
may render them unusable for the average client. We test
against the following defenses:
• BuFLO [12]. This defense sends packets at a constant
size during ﬁxed time intervals. This potentially ex-
tends the length of transmission and requires dummy
packets to ﬁll in gaps.

• Decoy pages [27]. This defense loads a decoy page
whenever another page is loaded. This provides back-
ground noise that degrades the accuracy of an attack.
This is essentially a defense that employs multi-tab
browsing.

• Trafﬁc morphing [40]. Trafﬁc morphing shapes a
client’s trafﬁc to look like another set of web pages.
A client chooses the source web pages that they would
like to defend, as well as a set of target web pages that
they would like to make the source processes look like.
• Tamaraw [35]. Tamaraw operates similarly to Bu-
FLO but ﬁxes packet sizes depending on their direc-
tion. Outgoing trafﬁc is ﬁxed at a higher packet in-
terval, this reduces overhead as outgoing trafﬁc is less
frequent.

• Adaptive Padding (AP) [18, 31].

AP protects
anonymity by introducing trafﬁc in to statistically un-
likely delays between packets in a ﬂow. This limits the
amount of extra bandwidth required and does not in-
cur any latency costs. AP uses previously computed
histograms of inter-arrival packet times from website
loads to determine when a dummy packet should be in-

Table 1: Attack comparison under various website ﬁn-
gerprinting defenses.
Defenses This work
0.91± 0.01
0.90 ±0.03
0.37 ±0.01
0.30 ±0.04
0.21 ±0.02
0.10 ±0.01

CUMUL [28] Bandwidth overhead (%)
0.91± 0.04
0.75± 0.07
0.21± 0.02
0.16± 0.03
0.08± 0.03
0.08± 0.03

No defense
Morphing [40]
Decoy pages [27]
Adaptive Padding [31]
BuFLO [12]
Tamaraw [35]

k-NN [39]
0.91± 0.03
0.82± 0.06
0.30± 0.06
0.19± 0.03
0.10± 0.03
0.09± 0.02

50± 10
130± 20
190± 20
96± 9

0

54

jected15. This is currently the favored option if padding
were to be implemented in Tor [4].
Table 1 shows the performance of k-ﬁngerprinting
against k-NN and CUMUL under various website ﬁnger-
printing defenses in a closed-world setting. Under ev-
ery defense k-ﬁngerprinting is comparable or achieves
better results than the k-NN attack and performs signif-
icantly better than CUMUL. Note that k-ﬁngerprinting
does equally well when trafﬁc morphing is applied com-
pared to no defense. As Lu et al. [23] note, trafﬁc morph-
ing is only effective when the attacker restricts attention
to the same features targeted by the morphing process.
Our results conﬁrm that attacks can succeed even when
trafﬁc morphing is employed. k-ﬁngerprinting also per-
forms nearly 10% better than k-NN when decoy pages
are used, which is in effect a marker for how well the
attack performs on multi-tab browsing. Due to the de-
pendency of packet length and sequence length features,
CUMUL performs substantially worse than the other two
attacks under website ﬁngerprinting defenses. Though
CUMUL uses a similar number of features and is of sim-
ilar computational efﬁciency to k-ﬁngerprinting, simple
defenses remove the feature vector patterns between sim-
ilar web pages used in CUMUL, rendering the attack
ineffectual. More generally, any attack that uses a re-
stricted set of features will suffer greatly from a defense
that targets those features. k-ﬁngerprinting performs well
under defenses due to its feature set that captures trafﬁc
information not used in CUMUL such as packet timings
and burst patterns. The k-NN attack performs slightly
better than CUMUL but requires an order of magnitude
more features than both CUMUL and k-ﬁngerprinting.
Our attack is both more efﬁcient and more accurate than
CUMUL and k-NN under defenses.
k-ﬁngerprinting the Wang et al. data set
7
We ﬁrst evaluate k-ﬁngerprinting on the Wang et al. data
set [39]. This data set was collected over Tor, and thus
implements padding of packets to ﬁxed-size cells (512-
bytes) and randomization of request orders [30]. Thus

15As Juarez et al.

[18] note, the distribution of histogram bins is
dependent on the individual client bandwidth capacity. Optimizing his-
tograms for a large number of clients is an open problem. Here we
implement a naive version of AP with one master histogram for all
clients.

USENIX Association  

25th USENIX Security Symposium  1193

7

Figure 3: Attack results for 1500 unmonitored training
pages while varying the number of ﬁngerprints used for
comparison, k, over 10 experiments.

Figure 4: Accuracy of k-ﬁngerprinting as we vary the
number of trees in the forest.

Table 2: k-ﬁngerprinting results for k=3 while varying
the number of unmonitored training pages.

Training pages TPR

0
1500
2500
3500
4500

0.90± 0.02
0.88± 0.02
0.88± 0.01
0.88± 0.01
0.87± 0.02

FPR
0.750± 0.010
0.013± 0.007
0.007± 0.001
0.005± 0.001
0.009± 0.001

BDR
0.419
0.983
0.993
0.997
0.998

the only available information to k-ﬁngerprinting are tim-
ing and volume features. We train on 60 out of the 90
instances for each of the 100 monitored web pages; we
vary the number of pages on which we train from the
5000 unmonitored web pages. For the attack evaluation
we use ﬁngerprints of length 200 and 150 features. Final
classiﬁcation is as described in Section 3.2, if all k ﬁnger-
prints agree on classiﬁcation a test instance is classiﬁed
as a monitored web page, otherwise it is classiﬁed as an
unmonitored web page.

The scenario for the attack is as follows: an attacker
monitors 100 web pages; they wish to know whether a
client is visiting one of those pages, and establish which
one. The client can browse to any of these web pages
or to 5000 unmonitored web pages, which the attacker
classiﬁes in bulk as an unmonitored page.

Using the k-ﬁngerprinting method for classifying a
web page we measure a TPR of 0.88 ± 0.01 and a FPR of
0.005 ± 0.001 when training on 3500 unmonitored web
pages and k, the number of training instances used for
classiﬁcation, set at k=3. k-ﬁngerprinting achieves better
accuracy than the state-of-the-art k-NN attack that has a
TPR of 0.85 ± 0.04 and a FPR of 0.006 ± 0.004. Given
a monitored web page k-ﬁngerprinting will misclassify
this page 12% of the time, while k-NN will misclassify
with 15% probability.

Best results are achieved when training on 3500 un-
monitored web pages. Table 2 reports TPR and FPR
when using different numbers of unmonitored web pages
for training with k=3. As we train more unmonitored web
pages we decrease our FPR with almost no reduction in
TPR. After training 3500 unmonitored pages there is no
decrease in FPR and so no beneﬁt in training more un-
monitored web pages. This is conﬁrmed by the marginal
increase in BDR after training on at least some of the
unmonitored set. Furthermore without training on any
of the unmonitored web pages, despite the high FPR the
classiﬁer has more than 40% probability of being correct
when classifying a web page as monitored.

Figure 3 illustrates how classiﬁcation accuracy
changes as, k, the number of ﬁngerprints used for clas-
siﬁcation changes. For a low k the attack achieves a FPR
of around 1%, as we increase the value of k we reduce the
number of misclassiﬁcations since it is less likely that all
k ﬁngerprints will belong to the same label, but we also
reduce the TPR. Altering the number of ﬁngerprints used
for classiﬁcation allows an attacker to tune the classiﬁer
to either a low FPR or high TPR depending on the de-
sired application of the attack.

We ﬁnd that altering the number of ﬁngerprints used
for classiﬁcation, k, affects the TPR and FPR more than
the number of unmonitored training pages. This suggests
that while it is advantageous to have a large world size
of unmonitored pages, increasing the number of unmon-
itored training pages does not increase accuracy of the
classiﬁer dramatically. This supports Wang et al.’s [39]
claims to the same effect.
In practice an attacker will
need to train on at least some unmonitored pages to in-
crease the BDR, though this does not need to be a sub-
stantial amount; training 1500 unmonitored web pages
leads to a 98.3% chance the classiﬁer is correct when
claiming to have recognized a monitored web page.
Fingerprint length. Changing the length of the ﬁnger-
print vector will affect k-ﬁngerprinting accuracy. For a

1194  25th USENIX Security Symposium 

USENIX Association

8

Table 3: Attack results on top Alexa sites for k=2 while
varying the number of unmonitored training pages.

Table 4: Attack results on Tor hidden services for k=2
while varying the number of unmonitored training pages.

Training pages TPR

2000
4000
8000
16000

0.93± 0.03
0.93± 0.01
0.92± 0.01
0.91± 0.02

FPR
0.032± 0.010
0.018± 0.007
0.008± 0.002
0.003± 0.001

BDR
0.33
0.47
0.67
0.86

Training pages TPR

2000
4000
8000
16000

0.82± 0.03
0.82± 0.04
0.82± 0.02
0.81± 0.02

FPR
0.0020± 0.0015
0.0007± 0.0006
0.0002± 0.0001
0.0002± 0.0002

BDR
0.72
0.88
0.96
0.97

small ﬁngerprint length there may not be enough diver-
sity to provide an accurate measure of distance over all
packet sequences. Figure 4 shows the resulting TPR and
FPR as we change the length of ﬁngerprints in the Wang
et al. [39] data set. We set k=1 and train on 4000 unmon-
itored web pages. Using only a ﬁngerprint of length one
results in a TPR of 0.51 and FPR of 0.904. Clearly us-
ing a ﬁngerprint of length one results in a high FPR since
there is a small universe of leaf symbols from which to
create the ﬁngerprint. A ﬁngerprint of length 20 results
in a TPR of 0.87 and FPR of 0.013. After this there are
diminishing returns for increasing the length of the ﬁn-
gerprint vector.
8 Attack evaluation on DSTor
We now evaluate k-ﬁngerprinting on DSTor. First we
evaluate the attack given a monitored set of the top 55
Alexa web pages, with 100 instances for each web page.
Then we evaluate the attack given a monitored set of 30
Tor hidden services, with 80 instances for each hidden
service. The unmonitored set remains the same for both
evaluations, the top 100,000 Alexa web pages with one
instance for each web page.
8.1 Alexa web pages monitored set
Table 3 shows the accuracy of k-ﬁngerprinting as the
number of unmonitored training pages is varied. For the
monitored web pages, 70 instances per web page were
trained upon and testing was done on the remaining 30
instances of each web page. As expected, the FPR de-
creases as the number of unmonitored training samples
grows. Similar to Section 7 there is only a marginal de-
crease in TPR while we see a large reduction in the FPR
as the number of training samples grows. Meaning an at-
tacker will not have to compromise on TPR to decrease
the FPR; when scaling the number of unmonitored train-
ing samples from 2% to 16% of the entire set the TPR de-
creases from 93% to 91% while the FPR decreases from
3.2% to 0.3%. There is a more pronounced shift in BDR
with the increase of unmonitored training pages, however
an attacker needs to train on less than 10% of the entire
dataset to have nearly 70% conﬁdence that classiﬁer was
correct when it claims to have detected a monitored page.
Clearly the attack will improve as the number of train-

ing samples grows, but in reality an attacker may have
limited resources and training on a signiﬁcant fraction of
100,000 web pages may be unfeasible. Figure 5 shows
the TPR and FPR of k-ﬁngerprinting as the number of
unmonitored web pages used for testing grows while the
number of unmonitored web pages used for training is
kept at 2000, for different values of k. We may think of
this as the evaluation of success of k-ﬁngerprinting as a
client browses to more and more web pages over multiple
browsing sessions. Clearly for a small k, both TPR and
FPR will be comparatively high. Given that, with k=5
only 2.5% of unmonitored web pages are falsely identi-
ﬁed as monitored web pages, out of 98,000 unmonitored
web pages.
8.2 Hidden services monitored set
Table 4 shows the accuracy of k-ﬁngerprinting as the
number of unmonitored training pages is varied. For
the monitored set, 60 instances per hidden service were
trained upon and testing was done on the remaining 20
instances of each hidden service. Again we observe a
marginal loss of TPR and a large reduction in FPR as
the number of training samples grows. When scaling
the number of unmonitored training samples from 2%
to 16% of the entire set the TPR decreases from 82% to
81% while the FPR decreases by an order of magnitude
from 0.2% to 0.02%. As a result, when training on 16%
of the unmonitored set only 16 unmonitored web pages
out of 84,000 were misclassiﬁed as a Tor hidden service.
In comparison to the Alexa web pages monitored set the
TPR is around 10% lower, while the FPR is also greatly
reduced. This is evidence that Tor hidden services are
easy to distinguish from standard web pages loaded over
Tor. There is also a higher but more gradual increase in
BDR compared to standard web pages. An attacker need
only train on as little as 2% of unmonitored pages to have
over 70% conﬁdence that classiﬁcation of a monitored
page was correct, with this rising to 97% when training
on 16% of the unmonitored dataset.

Similarly to Figure 5, Figure 6 shows the TPR and
FPR of k-ﬁngerprinting as the number of unmonitored
web pages used for testing grows while the number of
unmonitored web pages used for training is kept at 2000,
for different values of k. Both the TPR and FPR is lower

USENIX Association  

25th USENIX Security Symposium  1195

9

than in Figure 5. For example using k=5, the FPR is 0.2%
which equates to only 196 out of 98,000 unmonitored
pages being falsely classiﬁed as monitored pages.

From Figure 7 we observe that the BDR of both stan-
dard web pages and hidden services monitored sets de-
pends heavily on not only the world size but the number
of ﬁngerprints used for classiﬁcation. With k=10, when
a web page is classiﬁed as a monitored hidden service
page, there is over an 80% chance that the classiﬁer was
correct, despite the unmonitored world size (98,000) be-
ing over 160 times larger than the monitored world size
(600). The high BDR regardless of the disparity in world
sizes makes it clear that our attack is highly effective un-
der realistic large world size conditions.

It is clear that an attacker need only train on a small
fraction of data to launch a powerful ﬁngerprinting at-
tack. It is also clear that Tor hidden services are easily
distinguished from standard web pages, rendering them
vulnerable to website ﬁngerprinting attacks. We attribute
the lower FPR of Tor hidden services when compared to
a monitored training set of standard web page trafﬁc to
this distinguishability. A standard web page over Tor is
more likely to be confused with another standard web
page than a Tor hidden service.
Comparison with Kwon et al. [19] hidden services re-
sults. For comparison we ran k-ﬁngerprinting on the data
set used in the Kwon et al. study on ﬁngerprinting hid-
den services. This data set simulated a client connecting
to a hidden service. The data set consists of 50 instances
for each of 50 monitored hidden services and an unmon-
itored set of 950 hidden services. When training on 100
of the unmonitored pages they report attack accuracy of
0.9 TPR and 0.4 FPR. k-ﬁngerprinting achieved a simi-
lar true positive rate but the FPR is reduced to 0.22. This
FPR reduction in comparison with Kwon et al. continued
regardless of the amount of data used for training.
9 Attack evaluation on DSNorm
Besides testing on DSTor, Wang et al. [39] data set and
the Kwon et al. [19] data set we tested the efﬁcacy of
k-ﬁngerprinting on DSNorm. This allows us to estab-
lish how accurate k-ﬁngerprinting is over a standard en-
crypted web browsing session or through a VPN.
9.1 Attack on encrypted browsing sessions
An encrypted browsing session does not pad packets to a
ﬁxed size so the attacker may extract the following fea-
tures in addition to time features:
• Size transmitted. For each packet sequence we ex-
tract the total size of packets transmitted, in addition,
we extract the total size of incoming packets and the
total size of outgoing packets.

• Size transmitted statistics. For each packet sequence
we extract the average, variance, standard deviation

Figure 5: Attack accuracy on DSTor with Alexa moni-
tored set.

Figure 6: Attack accuracy on DSTor with Tor hidden ser-
vices monitored set.

and maximum packet size of the total sequence, the
incoming sequence and the outgoing sequence.
Apart from this modiﬁcation in available features, the
attack setting is similar: An attacker monitors a client
browsing online and attempts to infer which web pages
they are visiting. The only difference is that browsing
with the Transport Layer Security (TLS) protocol, or Se-
cure Sockets Layer (SSL) protocol, versions 2.0 and 3.0,
exposes the destination IP address and port. The attack is
now trying to infer which web page the client is visiting
from the known website16.

The attacker monitors 55 web pages; they wish to
know if the client has visited one of these pages. The
client can browse to any of these web pages or to 7000
other web pages, which the attacker does not care to clas-
sify other than as unmonitored. We train on 20 out of the
30 instances for each monitored page and vary the num-
ber of unmonitored pages on which we train.

Despite more packet sequence information to exploit,
the larger cardinality of world size gives rise to more

16Note that the data sets are composed of trafﬁc instances from some
websites without SSL and TLS, as well as websites using the protocols.
We expect our experiment conditions are much larger than the number
possible web pages an attacker may wish to ﬁngerprinting from a stan-
dard website.

1196  25th USENIX Security Symposium 

USENIX Association

10

Table 5: Attack results with packet size features for a
varying number of unmonitored training pages.

Training pages TPR

0
2000
4000
6000

0.95± 0.01
0.90± 0.01
0.87± 0.02
0.86± 0.01

FPR
0.850± 0.010
0.010± 0.004
0.004± 0.001
0.005± 0.002

BDR
0.081
0.908
0.976
0.990

Table 6: Attack results without packet size features for a
varying number of unmonitored training pages.

Training pages TPR

0
2000
4000
6000

0.90± 0.01
0.83± 0.01
0.81± 0.02
0.80± 0.02

FPR
0.790± 0.020
0.009± 0.001
0.006± 0.001
0.005± 0.001

BDR
0.082
0.910
0.961
0.989

Removing packet size features reduces the TPR by over
0.05 and increases the FPR by 0.001. Clearly packet size
features improve our classiﬁer in terms of correct identi-
ﬁcations but do not decrease the number of unmonitored
test instances that were incorrectly classiﬁed as a moni-
tored page. Despite the difference in FPR when includ-
ing packet size information, the BDR is similar, suggest-
ing that BDR is dominated by the amount of information
that can be trained upon.
Closed-World.
In the closed-world setting in which
the client can only browse within the 55 monitored web
pages k-ﬁngerprinting is 0.91, compared to 0.96 when
packet size features are available.
In the closed-world
setting attack accuracy improves by 5% when we include
packet size features.
10 Fine grained open-world false positives

on Alexa monitored set of DSTor

We observe that the classiﬁcation error is not uniform
across all web pages17. Some pages are misclassiﬁed
many times, and confused with many others, while others
are never misclassiﬁed. An attacker can leverage this in-
formation to estimate the misclassiﬁcation rate of each of
the web page classes instead of using the global average
misclassiﬁcation rate. A naive approach to this problem
would be to ﬁrst ﬁnd which ﬁngerprints contribute to the
many misclassiﬁcations and remove them. Our analysis
shows that the naive approach of removing “bad” ﬁnger-
prints that contribute to many misclassiﬁcations will ul-
timately lead to a higher misclassiﬁcation rate. Figure 9
shows the 50 ﬁngerprints that cause the most misclassi-
ﬁcations, and also shows for those same ﬁngerprints the
number of correct classiﬁcations they contribute towards.

17See additional evidence in Appendix B.

Figure 7: BDR for hidden services monitored set (above)
and Alexa monitored set (below).

Figure 8: Attack results for 2000 unmonitored training
pages while varying the number of ﬁngerprints used for
comparison, k, over 10 experiments.

opportunities for incorrect classiﬁcations. The attack
achieves a TPR of 0.87 and a FPR of 0.004. We achieved
best results when training on 4000 unmonitored web
pages. Table 5 reports results for training on different
numbers of unmonitored web pages, with k = 2. Fig-
ure 8 shows our results when modifying the number of
ﬁngerprints used (k) and training on 2000 unmonitored
pages. We ﬁnd that altering the number of unmonitored
training pages decreases the FPR while only slightly de-
creasing the TPR. This mirrors our experimental ﬁndings
from DSTor and the Wang et al. data set.
9.2 Attack without packet size features
DSNorm was not collected via Tor and so also contains
packet size information. We remove this to allow for
comparison with DSTor and the Wang et al. data set
which was collected over Tor. This also gives us a
baseline for how much more powerful k-ﬁngerprinting is
when we have additional packet size features available.
We achieved a TPR of 0.81 and FPR of 0.005 when train-
ing on 5000 unmonitored web pages. Table 6 shows
our results at other sizes of training samples, with k=2.

USENIX Association  

25th USENIX Security Symposium  1197

11

Figure 9: The ﬁngerprints that lead to the most misclas-
siﬁcations and the correct classiﬁcations they contribute
towards. Training on 2% of unmonitored pages with k=3.

Figure 10: Rates for training on 1000 unmonitored
pages, testing on 1000, and comparison when training
on the full 2000 unmonitored pages and testing on the
remaining 98000 unmonitored pages in DSTor, k=3.

As we can see nearly all “bad” ﬁngerprints actually con-
tribute to many correct classiﬁcations. One may think it
may still be beneﬁcial to remove these ﬁngerprints as the
cumulative sum of misclassiﬁcations outweigh the num-
ber of correct classiﬁcations. This removal will then pro-
mote ﬁngerprints that are further away in terms of Ham-
ming distance from the ﬁngerprinting that is being tested,
which will lead to a greater number of misclassiﬁcations.
Instead an attacker can use their training set of web
pages to estimate the TPR and FPR of each web page
class, by splitting the training set in to a smaller train-
ing set and validation set. Since both sets are from the
original training set the attacker has access to the true la-
bels. The attacker then computes the TPR and FPR rates
of each monitored class, this is used as an estimation for
TPR and FPR when training on the entire training set
and testing on new trafﬁc instances. More speciﬁcally
we split, for the monitored training set of 70 instance for
each of the Alexa top 55 web pages, into smaller training
sets of 40 instances and validation sets of 30 instances.
This is used as a misclassiﬁcation estimator for the full
monitored training set against the monitored test set of
30 instances per class. Similarly we split the unmoni-
tored training in half, one set as a smaller training set
and the other as a validation set.

Figure 10 shows the TPR and FPR estimation accu-
racy for 2000 unmonitored training pages. Monitored
classes are ﬁrst ordered from best to worst in terms of
their classiﬁcation accuracy. Even with a small unmoni-
tored training set of 2000 web pages, which is then split
in to a training set of 1000 web pages and a validation set
of 1000 web pages, an attacker can accurately estimate
the FPR of the attack if some of the monitored classes
were removed. For example, using only the best 20 mon-
itored classes (in terms of TPR), an attacker would esti-
mate that using those 20 classes as a monitored set, the

Figure 11: Rates for training on 8000 unmonitored
pages, testing on 8000, and comparison when training
on the full 16000 unmonitored pages and testing on the
remaining 84000 unmonitored pages in DSTor, k=3.

FPR would be 0.012. Using the entire data set we see that
the true FPR of these 20 classes is 0.010; the attacker has
nearly precisely estimated the utility of removing a large
fraction of the original monitored set.

There is a small difference between estimated and the
actual FPR in both Figures 10 and 11. There is little
beneﬁt in training more unmonitored data if the attacker
wants to accurately estimate the FPR; Figure 10 has a
similar gap between the estimated FPR and true FPR
when compared to Figure 11.
It is evident even with
a small training set, an attacker can identify web pages
that are likely to be misclassiﬁed and then accurately
calculate the utility of removing these web pages from
their monitored set. Due to the overwhelmingly large
world size of unmonitored web pages the BDR of Fig-
ure 10 does not grow dramatically with the removal of
web pages that are likely to be misclassiﬁed; using the
entire monitored set the BDR is 0.33, removing half of
the monitored web pages the BDR is 0.35.

1198  25th USENIX Security Symposium 

USENIX Association

12

11 Attack Summary & Discussion
Attack Summary. Best attack results on data sets were
achieved when training on approximately two thirds of
the unmonitored web pages. Despite this, results from
DSTor show that an attacker can achieve a very small
false positive rate while only training on 2% of the un-
monitored data. Training on 2% of 100,000 unmoni-
tored web pages greatly reduces the attack set up costs
while only marginally reducing the accuracy compared
to training on more data, providing a realistic setting un-
der which an attack could be launched. Results on all
data sets also suggest that altering k, the number of ﬁn-
gerprints used for classiﬁcation, has a greater inﬂuence
on accuracy than the number of training samples18.

k-ﬁngerprinting is robust; our technique achieves the
same accuracy regardless of the type of monitored set
or the manner in which it was collected (through Tor or
standard web browsers). The monitored set in the Wang
et al. [39] data set consists of real world censored web-
sites, the Kwon et al. [19] monitored set consist of Tor
hidden services and the DSTor/Norm monitored sets were
taken from Tor hidden services and top Alexa websites.
We do see a reduction in FPR when the target monitored
set are Tor hidden services due to the distinguishability
between the hidden services and unmonitored standard
web pages.

We also highlight the non-uniformity of classiﬁcation
performance: when a monitored web page is misclas-
siﬁed, it is usually misclassiﬁed on multiple tests. We
show that an attacker can use their training set to esti-
mate the error rate of k-ﬁngerprinting per web page, and
select targets with low misclassiﬁcation rates.
Computational Efﬁciency. k-ﬁngerprinting is more ac-
curate and uses fewer features than state-of-the-art at-
tacks. Furthermore k-ﬁngerprinting is faster than cur-
rent state-of-the-art website ﬁngerprinting attacks. On
the Wang et al. data set training time for 6,000 monitored
and 2,500 unmonitored training pages is 30.738 CPU
seconds on an 1.4 GHz Intel Core i5z. The k-NN attack
[39] has training time per round of 0.064 CPU seconds
for 2500 unmonitored training pages. For 6,000 rounds
training time is 384.0 CPU seconds on an AMD Opteron
2.2 GHz cores. This can be compared to around 500 CPU
hours using the attack described by Cai et al. [7]. Testing
time per instance for k-ﬁngerprinting is around 0.1 CPU
seconds, compared to 0.1 CPU seconds to classify one
instance for k-NN and 450 CPU seconds for the attack
described by Cai et al. [7].
Discussion. Website ﬁngerprinting research has been
criticized for not being applicable to real-world scenarios

18Figure 17 illustrates that compared to training on a small number
of monitored instances increasing the size of the monitored training set
only incrementally increases accuracy.

[17, 29]. We have shown that a website ﬁngerprinting at-
tack can scale to the number of trafﬁc instance an attacker
may sample over long period of time with a high BDR
and low FPR. However, we did not consider the cases
where background trafﬁc may be present, for example
from multitab browsing, or the effect that short-lived
websites will have on our attack. Gu et al. [15] show
in their work that a simple Naive-Bayes attack achieves
highly accurate results even when a client browses in
multiple tabs. Wang and Goldberg [36] also show that
website ﬁngerprinting is effective in practical scenarios.
With no prior attack set-up to tailor to a multi-tab brows-
ing session our attack was able to classify nearly 40% of
monitored pages correctly when the decoy defense was
employed.

Website content rapidly changes which will negatively
affect the accuracy of a website ﬁngerprinting attack
[17]. As the content of a website changes so will the
generated packet sequences, if an attacker cannot train
on this new data then an attack will suffer. However
we note that an attack will suffer from the ephemeral
nature of websites at different rates depending on the
type of website being monitored. For example, an at-
tack monitoring a news or social media site can expect
a faster degradation in performance compared to an at-
tack monitoring a landing page of a top 10 Alexa site
[1]. Also note Tor does not cache by default, so if in the
realistic scenario where an attacker wanted to monitor
www.socialmediawebsite.com a client would be forced to
navigate to the social media website landing page, which
is likely to host content that is long lived and not sub-
ject to change. The problem of content change is weak-
ened when ﬁngerprinting Tor hidden services. As show
by Kwon et al. [19] hidden pages show minimal changes
in comparison to non-hidden pages, resulting in devastat-
ingly accurate attacks on hidden services that can persist.
12 Conclusion
We establish that website ﬁngerprinting attacks are a se-
rious threat to online privacy. Clients of both Tor and
standard web browsers are at risk from website ﬁnger-
printing attacks regardless of whether they browse to hid-
den services or standard websites. k-ﬁngerprinting im-
proves on state-of-the-art attacks in terms of both speed
and accuracy: current website ﬁngerprinting defenses ei-
ther do not defend against k-ﬁngerprinting or incur very
high bandwidth overheads. Our world size is an order
of magnitude larger than previous website ﬁngerprinting
studies, and twice as large in terms of unique website
than Panchenko et al.’s recent work [28]. We have val-
idated our attack on four separate datasets showing that
it is robust and not prone to overﬁt one dataset, and so is
applicable to real world browsing environments at scale.
k-ﬁngerprinting is highly accurate even when an attacker

USENIX Association  

25th USENIX Security Symposium  1199

13

trains on a small fraction of the total data. Untrustworthy
data within that small fraction can then be ﬁltered and
removed before the attack is launched to later yield bet-
ter results, showing that long term website ﬁngerprinting
attacks on a targeted client is a realistic threat.
References
[1] Alexa The Web Information Company, [Accessed

[2] Leo

August 2015]. URL http://alexa.com.

Random

Breiman.

[Ac-
cessed
https:
//www.stat.berkeley.edu/˜breiman/
RandomForests/.

Forests,
URL

2015].

July

[3] The Nielsen Company,

[Accessed July 2015].
http://www.nielsen.com/us/

URL
en/insights/news/2010/led-by-
facebook-twitter-global-time-
spent-on-social-media-sites-up-
82-year-over-year.html.

[4] Tor Proposal 254, [Accessed November 2015].
URL https://gitweb.torproject.org/
torspec.git/tree/proposals/254-
padding-negotiation.txt.

[5] George Dean Bissias, Marc Liberatore, David
Jensen, and Brian Neil Levine. ”Privacy Vulner-
abilities in Encrypted HTTP Streams”. In Proceed-
ings of the 5th International Conference on Privacy
Enhancing Technologies, pages 1–11, 2006.

[6] Leo Breiman. ”Random Forests”. Mach. Learn.,

45(1):5–32, 2001.

[7] Xiang Cai, Xin Cheng Zhang, Brijesh Joshi, and
Rob Johnson. ”Touching from a distance: web-
site ﬁngerprinting attacks and defenses”. In ACM
Conference on Computer and Communications Se-
curity, pages 605–616, 2012.

[8] Xiang Cai, Rishab Nithyanand, and Rob Johnson.
”CS-BuFLO: A Congestion Sensitive Website Fin-
gerprinting Defense”.
In Proceedings of the 13th
Workshop on Privacy in the Electronic Society,
pages 121–130, 2014.

[9] Shuo Chen, Rui Wang, XiaoFeng Wang, and Ke-
huan Zhang. ”Side-Channel Leaks in Web Appli-
cations: A Reality Today, a Challenge Tomorrow”.
In Proceedings of the 2010 IEEE Symposium on Se-
curity and Privacy, pages 191–206, 2010.

[10] Heyning Cheng, , Heyning Cheng, and Ron Avnur.
”Trafﬁc Analysis of SSL Encrypted Web Brows-
ing”, 1998.

[11] Roger Dingledine, Nick Mathewson, and Paul F.
Syverson.
”Tor: The Second-Generation Onion
Router”. In Proceedings of the 13th USENIX Se-
curity Symposium, pages 303–320, 2004.

[12] Kevin P. Dyer, Scott E. Coull, Thomas Ristenpart,
and Thomas Shrimpton. ”Peek-a-Boo, I Still See

You: Why Efﬁcient Trafﬁc Analysis Countermea-
sures Fail”. In Proceedings of the 2012 IEEE Sym-
posium on Security and Privacy, pages 332–346,
2012.

[13] Jerome H. Friedman. ”Greedy Function Approxi-
mation: A Gradient Boosting Machine”. Annals of
Statistics, 29:1189–1232, 2000.

[14] Pall Oskar Gislason, Jon Atli Benediktsson, and Jo-
hannes R. Sveinsson. ”Random Forests for Land
Cover Classiﬁcation”. Pattern Recogn. Lett., 27(4):
294–300, March 2006.

[15] Xiaodan Gu, Ming Yang, and Junzhou Luo. ”A
novel Website Fingerprinting attack against multi-
tab browsing behavior”. In 19th IEEE International
Conference on Computer Supported Cooperative
Work in Design, CSCWD, pages 234–239, 2015.

[16] Dominik Herrmann, Rolf Wendolsky, and Hannes
Federrath.
”Website Fingerprinting: Attacking
Popular Privacy Enhancing Technologies with the
Multinomial Naive-bayes Classiﬁer”. In Proceed-
ings of the 2009 ACM Workshop on Cloud Comput-
ing Security, pages 31–42, 2009.

[17] Marc Ju´arez, Sadia Afroz, Gunes Acar, Claudia
D´ıaz, and Rachel Greenstadt. ”A Critical Evalu-
ation of Website Fingerprinting Attacks”. In Pro-
ceedings of the 2014 ACM SIGSAC Conference
on Computer and Communications Security, pages
263–274, 2014.

[18] Marc Ju´arez, Mohsen Imani, Mike Perry, Clau-
dia D´ıaz, and Matthew Wright. ”WTF-PAD: to-
ward an efﬁcient website ﬁngerprinting defense for
tor”. CoRR, abs/1512.00524, 2015. URL http:
//arxiv.org/abs/1512.00524.

[19] Albert Kwon, Mashael AlSabah, David Lazar,
Marc Dacier, and Srinivas Devadas. ”Circuit Fin-
gerprinting Attacks: Passive Deanonymization of
Tor Hidden Services”.
In 24th USENIX Security
Symposium, pages 287–302, 2015.

[20] A. Liaw and M. Wiener. ”Classiﬁcation and Re-
gression by randomForest”. R News: The Newslet-
ter of the R Project, 2(3):18–22, 2002.

[21] Marc Liberatore and Brian Neil Levine. ”Inferring
the source of encrypted HTTP connections”.
In
Proceedings of the 13th ACM Conference on Com-
puter and Communications Security, pages 255–
263, 2006.

[22] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua
Zhou. ”Isolation-Based Anomaly Detection”. ACM
Trans. Knowl. Discov. Data, 6(1):3:1–3:39, March
2012.

[23] Liming Lu, Ee-Chien Chang, and Mun Choon
Chan. ”Website Fingerprinting and Identiﬁcation
Using Ordered Feature Sequences”.
In Proceed-
ings of the 15th European Conference on Research

1200  25th USENIX Security Symposium 

USENIX Association

14

[34] David Wagner and Bruce Schneier. ”Analysis of
the SSL 3.0 Protocol”. In Proceedings of the 2nd
Conference on Proceedings of the Second USENIX
Workshop on Electronic Commerce - Volume 2,
pages 4–4, 1996.

[35] T. Wang and I. Goldberg. ”Comparing website ﬁn-
gerprinting attacks and defenses”. Technical Re-
port, 2013.

[36] T. Wang and I. Goldberg. ”On Realistically Attack-
ing Tor with Website Fingerprinting”. Technical
Report, 2015.

[37] T. Wang and I. Goldberg. ”Walkie-Talkie: An Ef-
fective and Efﬁcient Defense against Website Fin-
gerprinting”. Technical Report, 2015.

[38] Tao Wang and Ian Goldberg. ”Improved Website
Fingerprinting on Tor”. In Proceedings of the 12th
ACM Workshop on Workshop on Privacy in the
Electronic Society, pages 201–212, 2013.

[39] Tao Wang, Xiang Cai, Rishab Nithyanand, Rob
Johnson, and Ian Goldberg. ”Effective Attacks and
Provable Defenses for Website Fingerprinting”. In
Proceedings of the 23rd USENIX Security Sympo-
sium, pages 143–157, 2014.

[40] Charles V. Wright, Scott E. Coull, and Fabian Mon-
rose.
”Trafﬁc Morphing: An Efﬁcient Defense
Against Statistical Trafﬁc Analysis”. In In Proceed-
ings of the 16th Network and Distributed Security
Symposium, pages 237–250, 2009.

in Computer Security, pages 199–214, 2010.

[24] Xiapu Luo, Peng Zhou, Edmond W. W. Chan,
Wenke Lee, Rocky K. C. Chang, and Roberto
Perdisci.
”HTTPOS: Sealing information leaks
with browser-side obfuscation of encrypted ﬂows”.
In In Proc. Network and Distributed Systems Sym-
posium (NDSS), 2011.

[25] Rishab Nithyanand, Xiang Cai, and Rob Johnson.
”Glove: A Bespoke Website Fingerprinting De-
fense”.
In Proceedings of the 13th Workshop on
Privacy in the Electronic Society, pages 131–134,
2014.

[26] A. Stolerman M. V. Ryan P. Brennan P. Juola, J.
I. Noecker Jr and R. Greenstadt. ”A Dataset for
Active Linguistic Authentication”.
In IFIP WG
11.9 International Conference on Digital Foren-
sics, 2013.

[27] Andriy Panchenko, Lukas Niessen, Andreas Zin-
nen, and Thomas Engel. ”Website ﬁngerprinting
in onion routing based anonymization networks”.
In Proceedings of the 10th annual ACM workshop
on Privacy in the electronic society, WPES, pages
103–114, 2011.

[28] Andriy Panchenko, Fabian Lanze, Andreas Zinnen,
Martin Henze, Jan Pennekamp, Klaus Wehrle, , and
Thomas Engel. ”Website Fingerprinting at Internet
Scale”. In Network and Distributed System Security
Symposium, 2016.

[29] Mike Perry.

”A Critique of Website Trafﬁc
https://blog.

Fingerprinting Attacks”.
torproject.org/blog/critique-
website-traffic-fingerprinting-
attacks, Accessed June 2015.

[30] Mike Perry.
trafﬁc

”Experimental defense web-
https:

ﬁngerprinting”.

site
//blog.torproject.org/blog/
experimental-defense-website-
traffic-fingerprinting, Accessed June
2015.

[31] Vitaly Shmatikov and Ming-Hsiu Wang. ”Timing
Analysis in Low-Latency Mix Networks: Attacks
and Defenses”. In ESORICS, 2006.

[32] Qixiang Sun, Daniel R. Simon, Yi-Min Wang, Wilf
Russell, Venkata N. Padmanabhan, and Lili Qiu.
”statistical identiﬁcation of encrypted web brows-
ing trafﬁc”. In Proceedings of the 2002 IEEE Sym-
posium on Security and Privacy, pages 19–, 2002.
[33] Vladimir Svetnik, Andy Liaw, Christopher Tong,
J. Christopher Culberson, Robert P. Sheridan, and
Bradley P. Feuston. ”Random Forest: A Classiﬁca-
tion and Regression Tool for Compound Classiﬁca-
tion and QSAR Modeling”. Journal of Chemical
Information and Computer Sciences, 43(6):1947–
1958, 2003.

USENIX Association  

25th USENIX Security Symposium  1201

15

A Total feature importance.

131 - 150 .

Feature Description
Packet concentration list features.

Figure 12: Feature importance score for all 150 features
in order. The table gives the description for the 20 least
important features.

B Closed World Error Rates
Figure 13 shows the confusion matrix in our closed-
world setting, that is, it shows the 49 misclassiﬁcations
(out of 550). We see that some persistent misclassiﬁca-
tion patterns of web pages appear, for example web page
54 is classiﬁed correctly four times but is misclassiﬁed as
web page 0 six times. The misclassiﬁcation rate in Fig-
ure 13 is 0.09 but this is the average error rate across all
web pages.

Figure 13 shows that the classiﬁcation error is not uni-
form across all web pages. Some pages are misclassiﬁed
many times, and confused with many others, while oth-
ers are never misclassiﬁed. An attacker can leverage this
information to estimate the misclassiﬁcation rate of each
web page instead of using the global average misclassiﬁ-
cation rate.

As in Section 10, an attacker can use their training
set of web pages to estimate the misclassiﬁcation rate of
each web page, by splitting the training set in to a smaller
training set and validation set. Since both sets are from
the original training set the attacker has access to the true
labels. The attacker then computes the misclassiﬁcation
rate of each web page, which they can use as an estima-
tion for the misclassiﬁcation rate when training on the
entire training set and testing on new trafﬁc instances.

Figures 14 and 15 show the global misclassiﬁcation
rate for a varying number of monitored pages. Moni-
tored pages are ﬁrst ordered in terms of the misclassiﬁ-
cation rate they have, ordered from smallest to largest.
From Figure 14, using the Wang et al. data set, we see
that if the attacker considers only the top 50% on web
pages in terms of per page misclassiﬁcation rate, the true

Figure 13: Confusion matrix for closed-world attack on
Tor using DSNorm. F1 score = 0.913, Accuracy: 0.915,
550 items.

Figure 14: The global misclassiﬁcation rate when con-
sidering different numbers of monitored pages from the
Wang et al. data set. The monitored pages are ordered in
terms of smallest misclassiﬁcation rate to largest.

global misclassiﬁcation rate and global misclassiﬁcation
rate estimated by the attacker drop by over 70%. Simi-
larly from Figure 15, using DSNorm, if the attacker con-
siders only the top 50% on web pages in terms of per
page misclassiﬁcation rate, the true global misclassiﬁca-
tion rate and global misclassiﬁcation rate estimated by
the attacker drop by over 80%. This allows an attacker
to train on monitored pages and then cull the pages that
have too high an error rate, allowing for more conﬁdence
in the classiﬁcation of the rest of the monitored pages.

The gap between the attacker’s estimate and the mis-
classiﬁcation rate of the test set is largely due to the size
of the data set. Figure 14 has a smaller error of estimate
than Figure 15 because the Wang et al. data set has 60
instances per monitored page, in comparison DSNorm has
20 instances per monitored page. In practice, an attacker

1202  25th USENIX Security Symposium 

USENIX Association

16

Figure 17: Attack out-of-bag score while varying the
number of monitored training pages.

size by 10,000 web pages. Training on approximately
30% of the 7000 unmonitored web pages k-ﬁngerprinting
gives a TPR of over 0.90 and a FPR of 0.01 for k=1.
Training on approximately 30% of the 17,000 unmoni-
tored web pages k-ﬁngerprinting gives a TPR of 0.90 and
a FPR of 0.006 for k=1.

The fraction of unmonitored pages that were incor-
rectly classiﬁed as a monitored page decreased as we in-
creased our world size. In other words, out of 12,000 un-
monitored pages only 72 were classiﬁed as a monitored
page, with this Figure dropping to 24 if we use k=10 for
classiﬁcation. This provides a strong indication that k-
ﬁngerprinting can scale to a real-world attack in which
a client is free to browse the entire internet, with no de-
crease in attack accuracy.
Number of monitored training pages in closed-world.
Figure 17 shows the out-of-bag score as we change the
number of monitored pages we train. We found that
training on any more than a third of the data gives
roughly the same accuracy.

Figure 15: The global misclassiﬁcation rate when con-
sidering different numbers of monitored pages from
DSNorm. The monitored pages are ordered in terms of
smallest misclassiﬁcation rate to largest.

Figure 16: Attack accuracy for 17,000 unmonitored web
pages. Each line represents a different number of un-
monitored web pages that were trained, while varying k,
the number of ﬁngerprints used for classiﬁcation.

cannot expect perfect alignment; they are generated from
two different sets of data, the training and training +
test set. Nevertheless the attacker can expect this dif-
ference to decrease with the collection of more training
instances.
C Attack on larger world size of DSNorm
We run k-ﬁngerprinting on DSNorm with the same number
of monitored sites but increase the numbered of unmon-
itored sites to 17,000. We evaluate when we have both
time and size features available.

Figure 16 shows the results of k-ﬁngerprinting while
varying the number of ﬁngerprints (k) used for classiﬁ-
cation, from between 1 and 10, for various experiments
trained with different numbers of unmonitored pages.
We see that the attack results are comparable to the attack
on 7000 unmonitored pages, meaning there is no degra-
dation in attack accuracy when we increase the world

USENIX Association  

25th USENIX Security Symposium  1203

17

