Moat: Verifying Conﬁdentiality of Enclave Programs

Rohit Sinha
UC Berkeley

rsinha@berkeley.edu

Sanjit A. Seshia

UC Berkeley

sseshia@berkeley.edu

ABSTRACT
Security-critical applications constantly face threats from
exploits in lower computing layers such as the operating
system, virtual machine monitors, or even attacks from ma-
licious administrators. To help protect application secrets
from such attacks, there is increasing interest in hardware
implementations of primitives for trusted computing, such
as Intel’s Software Guard Extensions (SGX) instructions.
These primitives enable hardware protection of memory re-
gions containing code and data, and provide a root of trust
for measurement, remote attestation, and cryptographic seal-
ing. However, vulnerabilities in the application itself, such
as the incorrect use of SGX instructions or memory safety
errors, can be exploited to divulge secrets. In this paper, we
introduce a new approach to formally model these primitives
and formally verify properties of so-called enclave programs
that use them. More speciﬁcally, we create formal models
of relevant aspects of SGX, develop several adversary mod-
els, and present a sound veriﬁcation methodology (based on
automated theorem proving and information ﬂow analysis)
for proving that an enclave program running on SGX does
not contain a vulnerability that causes it to reveal secrets
to the adversary. We introduce Moat, a tool which formally
veriﬁes conﬁdentiality properties of applications running on
SGX. We evaluate Moat on several applications, including a
one time password scheme, oﬀ-the-record messaging, notary
service, and secure query processing.

Categories and Subject Descriptors
D.4.6 [Operating Systems]: Security and Protection —
Information ﬂow controls, Veriﬁcation; D.2.4 [Software En-
gineering]: Software/Program Veriﬁcation — Formal meth-
ods, Model checking

Keywords
Enclave Programs; Secure Computation; Conﬁdentiality; For-
mal Veriﬁcation

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813608 .

Sriram Rajamani
Microsoft Research

sriram@microsoft.com

Kapil Vaswani
Microsoft Research

kapilv@microsoft.com

1.

INTRODUCTION

Building applications that do not leak secrets, i.e., pro-
vide conﬁdentiality guarantees, is a non-trivial task. There
are at least three kinds of attacks a developer must guard
against. The ﬁrst kind of attack, which we call protocol at-
tack, is relevant for distributed applications involving client
nodes and cloud-based services, and can arise from vulnera-
bilities in the cryptographic protocol used to establish trust
between various distributed components. Examples of pro-
tocol attacks include man-in-the-middle or replay attacks.
The second kind of attack, which we call application attack,
is due to errors or vulnerabilities in the application code it-
self which can be exploited to leak conﬁdential information
from the application (e.g., the Heartbleed bug [18]). The
third kind of attack, which we call infrastructure attack, is
due to exploits in the software stack (e.g. operating system
(OS), hypervisor) that the application relies upon, where
the privileged malware has full control of the CPU, mem-
ory, I/O devices, etc. Infrastructure attacks can result in an
attacker gaining control of any application’s memory and
reading secrets at will.

Several mitigation strategies have been proposed for each
of these kinds of attacks. In order to guard against proto-
col attacks, we can use protocol veriﬁers (e.g., ProVerif [11],
CryptoVerif [12]) to check for protocol errors. In order to
guard against application attacks, the application can be
developed in a memory-safe language with information ﬂow
control, such as Jif [28]. Infrastructure attacks are the hard-
est to protect against, since the attack can happen even if
the application is error free (i.e., without application vulner-
abilities or protocol vulnerabilities). While some eﬀorts are
under way to build a fully veriﬁed software stack ground-up
(e.g. [21, 25]), this approach is unlikely to scale to real-world
OS and system software.

An alternative approach to guarding against infrastruc-
ture attack is by hardware features that enable a user-level
application to be protected from privileged malware. For
instance, Intel SGX [23, 24] is an extension to the x86 in-
struction set architecture, which provides any application
the ability to create protected execution contexts called en-
claves containing code and data. SGX features include 1)
hardware-assisted isolation from privileged malware for en-
clave code and data, 2) measurement and attestation prim-
itives for detecting attacks on the enclave during creation,
and 3) sealing primitives for storing secrets onto untrusted
persistent storage. Using these extensions it is possible to
write applications with a small trusted computing base, which
includes only the enclave code and the SGX processor. All

1169other layers in the software stack including the operating
system and the hypervisor can be excluded from the trusted
computing base. However, establishing such a guarantee
requires precise understanding of the contract between the
hardware and software.
In particular, it requires specify-
ing a formal API-level semantics of SGX instructions, an
adversary model, and a veriﬁcation methodology (with tool
support) to detect insecure use of SGX instructions. Our
work addresses each of these aspects, as we describe below.

Semantics of SGX instructions. SGX provides instruc-
tions which enable applications to create enclaves, transfer
control to and from enclaves, perform remote attestation,
and seal and unseal secrets so that they can persist on the
platform. We give a formal semantic model of the interface
between the programmer and the implementation of SGX,
with an eye towards automatic formal veriﬁcation of the en-
clave code’s security. Our approach is similar in spirit to
previous work on ﬁnding API-level exploits [20] using the
UCLID modeling and veriﬁcation system [15].

Adversary model. We consider a powerful adversary who
can compromise the infrastructure code (OS, hypervisor,
etc.) and perform both passive and active attacks by access-
ing any non-enclave memory (i.e. memory not protected by
SGX), modifying page tables, generating interrupts, control-
ling I/O interaction, etc. It is non-trivial to reason about
enclave behaviors (and potential vulnerabilities) in the pres-
ence of such an adversary. As a ﬁrst step, using our formal
ISA-level semantics of the SGX instructions, we prove that
(if the application follows certain guidelines during enclave
creation) any combination of the above adversarial actions
can be simply modeled as an arbitrary update of non-enclave
memory. This simpliﬁes our reasoning of enclave execution
in the presence of privileged adversaries. In particular, we
show that a so-called havocing adversary who symbolically
modiﬁes all of the non-enclave memory after every instruc-
tion of the enclave code, and is able to observe all non-
enclave memory, is powerful enough to model all passive and
active adversaries. We consider this to be one of the key con-
tributions of this paper. It greatly simpliﬁes the construc-
tion of automated veriﬁers for checking security properties
of enclave code, and potentially even programming and rea-
soning about enclaves.

Veriﬁcation methodology and tool support. Even
though SGX oﬀers protection from infrastructure attacks,
the developer must take necessary steps to defend against
protocol and application attacks by using SGX instructions
correctly, using safe cryptographic protocols, avoiding tra-
ditional bugs due to memory safety violations, etc. For
instance, the enclave may suﬀer from exploits like Heart-
bleed [18] by using vulnerable SSL implementations, and
these exploits have been shown to leak secret cryptographic
keys from memory. To that end, our next step is to prove
that enclaves satisfy conﬁdentiality i.e. there is no execu-
tion that leaks a secret to the adversary-visible, non-enclave
memory. Proving conﬁdentiality involves tracking the ﬂow
of secrets within the application’s memory, and proving that
the adversary does not observe values that depend on se-
crets. While past research has produced several type sys-
tems that verify information ﬂows (e.g. Jif [28], Volpano
et al. [34], Balliu et al. [5]), they make a fundamental as-
sumption that the infrastructure (OS/VMM, etc.) on which
the code runs is safe, which is unrealistic due to privileged

malware attacks. Furthermore, extending traditional type
systems to enclave programs is non-trivial because the anal-
ysis must faithfully model the semantics of SGX instruc-
tions and infer information ﬂows to individual addresses
within enclave memory. Therefore, we develop a static ver-
iﬁer called Moat that analyzes the instruction-level behav-
ior of the enclave binary program. Moat employs a ﬂow-
and path-sensitive type checking algorithm (based on auto-
mated theorem proving using satisﬁability modulo theories
solving [8]) for automatically verifying whether an enclave
program (in the presence of an active adversary) provides
conﬁdentiality guarantees. For ill-typed programs, Moat re-
turns an exploit demonstrating a potential leak of secret to
non-enclave memory. By analyzing the binary, we remove
the compiler from our trusted computing base, and relax
several memory safety assumptions that are common in tra-
ditional information ﬂow type systems.

Although we do not focus on protocol attacks in this
paper, we brieﬂy describe how one can compose protocol-
level analysis (performed by a veriﬁer such as ProVerif [11])
with Moat to achieve end-to-end conﬁdentiality guarantees
against infrastructure, application, and protocol attacks.

In summary, the goal of this paper is to explore the con-
tract between the SGX hardware and the enclave developer
and provide a methodology and tool support for the pro-
grammer to write secure enclaves. We make the following
speciﬁc contributions:
• We develop the ﬁrst semantic API model of the SGX plat-
form and its new instruction set, working from publicly-
available documentation [24].
• We formally study active and passive adversaries for SGX
enclaves, and show that a havocing adversary who observes
and havocs non-enclave memory after every instruction in
the enclave is both powerful enough to model all such ad-
versaries, and amenable to be used in automated symbolic
veriﬁcation tools.
• We develop Moat, a system for statically verifying conﬁ-
dentiality properties of an enclave program in the face of
application and infrastructure attacks.

Though we study these issues in the context of Intel SGX,
similar issues arise in other architectures based on trusted
hardware such as ARM TrustZone [4] and Sancus [29], and
our approach is potentially applicable to them as well. The
theory we develop with regard to attacker models and our
veriﬁer is mostly independent of the speciﬁcs of SGX, and
our use of the term “enclave” is also intended in the more
general sense.

2. BACKGROUND
2.1 SGX

The SGX instructions allow a user-level host application
to instantiate a protected execution context, called an en-
clave, containing code and data. An enclave’s memory re-
sides within the untrusted host application’s virtual address
space, but is protected from accesses by that host applica-
tion or any privileged software — only the enclave code is
allowed to access enclave memory. Furthermore, to protect
against certain hardware attacks, the cache lines belonging

1170to enclave memory are encrypted and integrity protected by
the CPU prior to being written to oﬀ-chip memory.

The host application creates an enclave using a combina-
tion of instructions: ecreate, eadd, eextend, and einit.
The application invokes ecreate to reserve protected mem-
ory for enclave use. To populate the enclave with code
and data, the host application uses a sequence of eadd and
eextend instructions. eadd loads code and data pages from
non-enclave memory to enclave’s reserved memory. eextend
extends the current enclave measurement with the measure-
ment of the newly added page. Finally, einit terminates the
initialization phase, which prevents any further modiﬁcation
to the enclave state (and measurement) from non-enclave
code. The host application transfers control to the enclave
by invoking eenter, which targets a programmer deﬁned en-
try point inside the enclave (via a callgate-like mechanism).
The enclave executes until one of the following events occur:
(1) enclave code invokes eexit to transfer control to the
host application, (2) enclave code incurs a fault or excep-
tion (e.g. page fault, divide by 0 exception, etc.), and (3)
the CPU receives a hardware interrupt and transfers con-
trol to a privileged interrupt handler. In the case of faults,
exceptions, and interrupts, the CPU saves state (registers,
etc.) in State Save Area (SSA) pages in enclave memory,
and can resume the enclave in the same state once the OS /
VMM handles the event. Although a compromised OS may
launch denial of service attacks, we show that an enclave
can still guarantee properties such as data conﬁdentiality.

The reader may have observed that before enclave initial-
ization, code and data is open to eavesdropping and tamper-
ing by adversaries. For instance, an adversary may modify a
OTP enclave’s binary (on user’s machine) so that it leaks a
user’s login credentials. SGX provides an attestation prim-
itive called ereport to defend against this class of attacks.
The enclave participates in attestation by invoking ereport,
which generates a hardware-signed report of the enclave’s
measurement, and then sending the report to the verifying
party. The enclave can also use ereport to bind data to
its measurement, thereby adding authenticity to that data.
The enclave can use egetkey to attain a hardware-generated
sealing key, and store sealed secrets to untrusted storage.

2.2 Example

We demonstrate the use of SGX by an example of a one-
time password (OTP) service, although the exposition ex-
tends naturally to any secret provisioning protocol. OTP is
typically used in two factor authentication as an additional
step to traditional knowledge based authentication via user-
name and passphrase. A user demonstrates ownership of a
pre-shared secret by providing a fresh, one-time password
that is derived deterministically from that secret. For in-
stance, RSA SecurID R(cid:13) is a hardware-based OTP solution,
where possession of a tamper-resistant hardware token is
required during login. In this scheme, a pre-shared secret
is established between the OTP service and the hardware
token. From then on, they compute a fresh one-time pass-
word as a function of the pre-shared secret and time dura-
tion since the secret was provisioned to the token. The user
must provide the one-time password displayed on the to-
ken during authentication, in addition to her username and
passphrase. This OTP scheme is both expensive and in-
convenient because it requires distributing tamper-resistant
hardware tokens physically to the users. Although pure soft-

ware implementations have been attempted, they are often
prone to infrastructure attacks from malware, making them
untrustworthy.

The necessary primitives for implementing this protocol
securely are (1) ability to perform the cryptographic op-
erations (or any trusted computation) without interference
from the adversary, (2) protected memory for computing
and storing secrets, and (3) root of trust for measurement
and attestation. Intel SGX processors provide these primi-
tives. Hoekstra et al. [23] propose the following OTP scheme
based on SGX, which we implement (Figure 1) and verify
using Moat. In this protocol, a bank OTP server provisions
the pre-shared secret to a client, which is running on a po-
tentially infected machine with SGX hardware.

0. The host application on the client sets up an enclave that
contains trusted code for the client side of the protocol.

1. The server sends the client an attestation challenge nonce.
Consequent messages in the protocol use the nonce to
guarantee freshness.

2. The client and OTP server engage in an authenticated
Diﬃe-Hellman key exchange in order to establish a sym-
metric session_key. The client uses ereport instruction
to send a report containing a signature over the Diﬃe-
Hellman public key dh_pubkey and the enclave’s mea-
surement. The signature guarantees that the report was
generated by an enclave on an Intel SGX CPU, while the
measurement guarantees that the reporting enclave was
not tampered during initialization. After verifying the
signatures, both the client and OTP server compute the
symmetric session_key.

3. The OTP server sends the pre-shared OTP secret to the
client by ﬁrst encrypting it with the session_key, and
then signing the encrypted content with the bank’s pri-
vate TLS key. The client veriﬁes the signature and de-
crypts the message to retrieve the pre-shared otp_secret.

4. For future use, the client requests for sealing_key (us-
ing egetkey instruction), encrypts otp_secret using seal-
ing_key, and writes the sealed_secret to disk.

for untrusted computation.

A typical application (such as our OTP client) uses en-
clave code to implement trusted computation such as the
cryptographic operations, stores secrets in the enclave heap,
and uses non-enclave code (host application, OS, VMM,
etc.)
In fact, SGX prevents
the enclave code from invoking any privileged instructions
such as system calls, thus forcing the enclave to rely on non-
enclave code to issue system calls, perform I/O, etc. For
instance, to send the Diﬃe-Hellman public key to the server,
the enclave (1) invokes ereport with enclave_state.dh_pubkey,
(2) copies the report to non-enclave memory app_heap, (3)
invokes eexit to transfer control to the untrusted app, and
(4) waits for app to invoke the socket system calls to send
the report to the bank server. Over their lifetimes, app and
enclave perform several eenter and eexit while alternating
between trusted and untrusted computation. To facilitate
the interaction between an enclave and non-enclave code,
SGX allows the enclave to access the entire address space of
the host application.

While SGX implements the necessary primitives for doing
trusted computation, we need a methodology for writing se-
cure enclave programs. Conﬁdentiality requires protecting

1171Figure 1: Running OTP Example. The enclave performs trusted cryptographic operations, and the host
application performs untrusted tasks such as UI handling and network communications with the OTP server.

secrets, which requires understanding of the contract be-
tween the enclave developer and the SGX hardware. First,
the enclave developer must follow the enclave creation guide-
lines (see § 3.1) so that the hardware protects the enclave
from an attacker that has gained privileged access to the sys-
tem. Even then, the enclave developers needs to ensure that
their code does not leak secrets via application attacks and
protocol attacks. For instance, they should encrypt secrets
before writing them to non-enclave memory. They should
account for adversary modifying non-enclave memory at any
time, which could result in time-of-check-to-time-of-use at-
tacks. For example, the enclave code in Figure 1 has such a
vulnerability. The enclave code here copies encrypted data
from enclave memory to non-enclave memory, but the size of
the data copied is determined by a variable size_ﬁeld, which
resides in non-enclave memory. Thus, by manipulating the
value of this variable the adversary can trick the enclave code
into leaking secrets to non-enclave memory. Avoiding such
attacks is non-trivial. In this paper, we present a methodol-
ogy and tool support for detecting such errors, and proving
that enclaves are secure.
The rest of this paper is structured as follows. We provide
an overview of our approach in § 3. § 4 describes how Moat
constructs a formal model of the enclave program (including
its use of x86 and SGX instructions), and also formalizes an
active and a passive adversary. § 5 introduces the havocing
adversary, and shows how it can be used to model the en-
clave’s execution in the presence of an active adversary. The
results presented in § 5 allow us soundly verify any safety
property of enclave programs. Next, we focus our attention
on proving conﬁdentiality by ﬁrst formalizing it in § 6, and
then developing a veriﬁcation algorithm in § 7. § 8 describes
several case studies where we apply Moat.

3. OVERVIEW OF MOAT

We are interested in building secure distributed applica-
tions, which have components running in trusted and un-
trusted environments, where all communication channels are

untrusted. For the application to be secure, we need (1) se-
cure cryptographic protocols between the components (to
protect from protocol attack), and (2) secure implementa-
tion in each component to protect from application attack
and infrastructure attack. Our goal is to prove that, even
in the presence of such attacks, the enclave does not leak
its secrets to the adversary. Moat defends against applica-
tion and infrastructure attacks. Furthermore, we combine
Moat with oﬀ-the-shelf protocol veriﬁers to defend against
protocol attacks as well.
3.1 Protecting from Infrastructure Attacks

An infrastructure attack can generate interrupts, modify
page tables, modify any non-enclave memory, invoke any x86
and SGX instruction — § 4.3 formalizes the threat model.
We mandate that the enclave be created with the following
sequence that measures all pages in memenc, which denotes
memory reserved for the enclave. We do not ﬁnd this to be
a restriction in practice.

ecreate(size(memenc));
foreach page ∈ memenc : {eadd(page); eextend(page)};
einit

(1)

If some component of enclave state is not measured, then
the adversary may havoc that component of state during
initialization without being detected. This precondition on
the initialization sequence lets us prove that the SGX hard-
ware provides some very useful guarantees. For instance,
we prove Theorem 1 in § 5 which guarantees that an en-
clave initialized using this sequence is protected from each
adversarial operation in our threat model — we formally
model each SGX instruction (see § 4.2) to perform this proof.
Speciﬁcally, we prove a non-interference property [16] that
the enclave’s execution (i.e.
its set of reachable states) is
independent of the adversarial operations, with the caveat
that the enclave may read non-enclave memory for inputs.
However, we do not consider this to be an attack because the
enclave must read non-enclave memory for inputs, which are

1172untrusted by design. The utility of this proof is that all in-
frastructure attacks can now be modeled by a so-called hav-
ocing adversary that is only allowed to update non-enclave
memory, and this simpliﬁes our reasoning of enclave exe-
cution in the presence of privileged adversaries. We call
this havocing adversary H (deﬁned in § 5), and we allow
H to update all addresses in non-enclave memory between
any consecutive instructions executed by the enclave. Go-
ing forward, we focus primarily on application attacks, and
model H’s operations only for the purpose of reads from
non-enclave memory.
3.2 Protecting from Application Attacks

In this section, we give an overview of Moat’s approach
for proving conﬁdentiality properties of enclave code (de-
tailed exposition in § 4 through § 7). Moat accepts an en-
clave program in x86 Assembly, containing SGX instruc-
tions ereport, egetkey, and eexit. Moat is also given a
set of annotations, called Secrets, indicating 1) program
points where secrets values are generated (e.g. after decryp-
tion), and 2) memory locations where those secret values are
stored. In the OTP example, the Secrets include otp_secret,
session_key, and sealing_key. Moat proves that a privileged
software adversary running on the same machine does not
observe a value that depends on Secrets, regardless of any
operations performed by that adversary. We demonstrate
Moat’s proof methodology on a snippet of OTP enclave code
containing lines 22-26 from Figure 1, which is ﬁrst com-
piled to x86+SGX Assembly in Figure 2. Here, the enclave
invokes egetkey to retrieve a 128-bit sealing key, which is
stored in the byte array sealing_key. Next, the enclave en-
crypts otp_secret (using AES-GCM-128 encryption library
function called encrypt) to compute the sealed_secret. Fi-
nally, the enclave copies sealed_secret to untrusted memory
app_heap (to be written to disk). Observe that the size argu-
ment to memcpy (line 26 in Figure 1) is a variable size_ﬁeld
which resides in non-enclave memory. This buﬀer overrun
vulnerability can be exploited by the adversary, causing the
enclave to leak secrets from its stack.

egetkey
movl $0x8080AC,0x8(%esp)
lea -0x6e0(%ebp),%eax
mov %eax,0x4(%esp)
lea -0x720(%ebp),%eax
mov %eax,(%esp)
call <AES_GCM_encrypt>
mov 0x700048,%eax
movl %eax,0x8(%esp)
lea -0x720(%ebp),%eax
mov %eax,0x4(%esp)
movl $0x701000,(%esp)
call <memcpy>

mem := egetkey(mem, ebx, ecx);
mem := store(mem,add(esp,8),8080AC);
eax := sub(ebp, 6e0);
mem := store(mem,add(esp,4),eax);
eax := sub(ebp, 720);
mem := store(mem, esp, eax);
mem := AES_GCM_encrypt(mem, esp);
eax := load(mem, 700048);
mem := store(mem, add(esp,8), eax);
eax := sub(ebp, 720);
mem := store(mem, add(esp,4), eax);
mem := store(mem, esp, 701000);
mem := memcpy(mem, esp);

Figure 2: OTP enclave snippet (left) and penc (right)

To reason about enclave code and ﬁnd such vulnerabil-
ities, Moat ﬁrst extracts a model in an imperative veriﬁ-
cation language, as shown in Figure 2. We refer to the
model as penc. penc models x86 (e.g. load, store) and SGX
(e.g. egetkey) instructions as uninterpreted functions con-
strained with axioms. The axioms (presented in § 4.2) are
part of our machine model, and they encode the ISA-level se-
mantics of each instruction. penc uses BAP to model x86 [14]
precisely, including updates to CPU ﬂags. For brevity, Fig-

ure 2 omits all updates to ﬂags as they are irrelevant to this
code snippet. For reasons explained later, Moat does not
model the implementation of cryptographic routines (such
as AES_GCM_encrypt in Figure 2). It replaces all calls to
the cryptographic library with their speciﬁcations.
In the
case of AES_GCM_encrypt, the speciﬁcation ensures mem-
ory safety: only the output ciphertext buﬀer and memory
allocated to the library can be modiﬁed by this call.

Since penc executes in the presence of an active adver-
sary, we must model the eﬀects of adversarial operations
on penc’s execution. Section 3.1 introduces an active adver-
sary H (formalized in 5), which can perform the operation
“havoc mem¬epc” once between consecutive instructions along
any execution of penc. Here, memepc denotes memory re-
served by the SGX processor for enclave use, and mem¬epc
is all of untrusted, non-enclave memory; havoc mem¬epc up-
dates each address in mem¬epc with a non-deterministically
chosen value. We deﬁne H this way because a privileged
software adversary can interrupt penc at any point, perform
havoc mem¬epc, and then resume penc. We model the eﬀect of
H on penc’s behavior by instrumenting havoc mem¬epc in penc
(see Figure 3). The instrumented program is called penc−H.

1 havoc mem¬epc; mem := egetkey(mem, ebx, ecx);
2 havoc mem¬epc; mem := store(mem, add(esp,8), 8080AC);
3 havoc mem¬epc; eax := sub(ebp, 6e0);
4 havoc mem¬epc; mem := store(mem, add(esp,4), eax);
5 havoc mem¬epc; eax := sub(ebp, 720);
6 havoc mem¬epc; mem := store(mem, esp, eax);
7 havoc mem¬epc; mem := AES_GCM_encrypt(mem, esp);
8 havoc mem¬epc; eax := load(mem, 700048);
9 havoc mem¬epc; mem := store(mem, add(esp,8), eax);
10 havoc mem¬epc; eax := sub(ebp, 720);
11 havoc mem¬epc; mem := store(mem, add(esp,4), eax);
12 havoc mem¬epc; mem := store(mem, esp, 701000);
13 havoc mem¬epc; mem := memcpy(mem, esp);

Figure 3: penc−H constructed from OTP penc

As mentioned before, the OTP enclave implementation is
vulnerable. The size argument to memcpy (line 26 in Fig-
ure 1) is a ﬁeld within a data structure in non-enclave mem-
ory. This vulnerability manifests as a load (line 8 of Fig-
ure 3), which reads a value from non-enclave memory and
passes that value as the size argument to memcpy. To per-
form the exploit, H uses havoc mem¬epc (in line 8) to choose
the number of bytes that penc−H writes to non-enclave mem-
ory, starting at the base address of sealed_secret. By setting
this value to be greater than the size of sealed_secret, H
causes penc−H to leak the stack contents, which includes the
sealing_key. We can assume for now that writing sealed_secret
to the unprotected app_heap is safe because it is encrypted.
We formalize a conﬁdentiality property in § 6 that pre-
vents such vulnerabilities, and build a static type system
in § 7 which only admits programs that satisfy conﬁdential-
ity. Conﬁdentiality enforces that for any pair of traces of
penc−H that diﬀer in the values of Secrets, if H’s opera-
tions along the two traces are equivalent, then H’s obser-
In
vations along the two traces must also be equivalent.
other words, H’s observation of penc’s execution is indepen-
dent of Secrets. Note that we omit side channels from H’s
observation in this work.

1173Our type system checks conﬁdentiality by instrumenting
penc−H with ghost variables that track the ﬂow of Secrets
within registers and memory, akin to taint tracking but per-
formed using static analysis. Moat tracks both implicit and
explicit information ﬂows [30]. Figure 4 demonstrates how
Moat type-checks penc−H. For each state variable x, the type
system instruments a ghost variable Cx. Cx is updated on
each assignment that updates x, and is assigned to f alse
only if x’s value is independent of Secrets (details in § 7).
For instance, Cmem[esp] in line 13 is assigned to Ceax because
a secret in the eax register makes the written memory lo-
cation also secret. Furthermore, for each secret in Secrets,
we set the corresponding locations in Cmem to true. For in-
stance, lines 1-3 assign true to those 16 bytes in Cmem where
egetkey places the secret sealing_key. Information leaks can
only happen via store to mem¬enc, where memenc is a sub-
set of memepc that is reserved for use by penc, and mem¬enc
is either non-enclave memory or memory used by other en-
claves. enc(i) is true if i is an address in memenc. For each
store instruction, the type system instruments an assert
checking that a secret value is not written to mem¬enc (with
special treatment of memcpy for eﬃciency). For a program
to be well-typed, all assertions in the instrumented penc−H
must be valid along any feasible execution. Moat feeds the
instrumented program (Figure 4) to a static program ver-
iﬁer [6], which uses SMT solving to explore all executions
(i.e. all reachable states) and verify that the assertions are
valid along all executions. The assertion in line 30 is invalid
because Cmem is true for memory locations that hold the
sealing_key. Our type system rejects this enclave program.
A ﬁx to the OTP implementation is to replace size_ﬁeld
with the correct size, which is 64 bytes. Although mem-
ory safety vulnerabilities can be found using simpler static
analysis, Moat can identify several classes of vulnerabilities
using these typing assertions. The analysis in Moat is sound
(if penc terminates) i.e. Moat ﬁnds any vulnerable execution
of penc that leaks a secret to mem¬enc.

Declassiﬁcation.

In the previous section, we claim that writing sealed_secret
to app_heap is safe because it is encrypted using a secret
key. We now explain how Moat evaluates whether a par-
ticular enclave output is safe. As a pragmatic choice, Moat
does not reason about cryptographic operations for there
is signiﬁcant body of research on cryptographic protocol
veriﬁcation. For instance, if encryption uses a key estab-
lished by Diﬃe-Hellman, the veriﬁcation would need to rea-
son about the authentication and attestation scheme used in
that Diﬃe-Hellman exchange in order to derive that the key
can be safely used for encryption. Protocol veriﬁers (e.g.
ProVerif [11], CryptoVerif [12]) excel at this form of rea-
soning. Therefore, when Moat encounters a cryptographic
library call, it abstracts it as an uninterpreted function with
the conservative axiom that secret inputs produce secret out-
put. For instance in Figure 4, AES_GCM_encrypt on line 16
is an uninterpreted function, and C_AES_GCM_encrypt on
line 15 marks the ciphertext as secret if any byte of the
plain-text or encryption key is secret. This conservative ax-
iomatization is very unnecessary because a secret encrypted
with a key (that is unknown to the adversary) can be safely
output. To reduce this imprecision in Moat, we introduce
declassiﬁcation to our type system. A declassiﬁed output is
a intentional information leak of the program, which may be

mem[i];

mem := Cmem; havoc Cmem;

1 assert ¬Cecx; Cold
2 assume ∀i. (ecx ≤ i < ecx + 16) → Cmem[i] ↔ true;
3 assume ∀i. ¬(ecx ≤ i < ecx + 16) → Cmem[i] ↔ Cold
4 havoc mem¬epc; mem := egetkey(mem, ebx, ecx);
5 assert ¬Cesp; Cmem[add(esp, 8)] := false;
6 havoc mem¬epc; mem := store(mem, add(esp,8), 8080AC);
7 Ceax := Cebp;
8 havoc mem¬epc; eax := sub(ebp, 6e0);
9 assert ¬Cesp ∧ (¬enc(add(esp, 4)) → ¬Ceax); Cmem[add(esp, 4)] := Ceax;
10 havoc mem¬epc; mem := store(mem, add(esp,4), eax);
11 Ceax := Cebp;
12 havoc mem¬epc; eax := sub(ebp, 720);
13 assert ¬Cesp ∧ (¬enc(esp) → ¬Ceax); Cmem[esp] := Ceax;
14 havoc mem¬epc; mem := store(mem, esp, eax);
15 Cmem := C_AES_GCM_encrypt(Cmem, esp);
16 havoc mem¬epc; mem := AES_GCM_encrypt(mem, esp);
17 Ceax := Cmem[700048];
18 havoc mem¬epc; eax := load(mem, 700048);
19 assert ¬Cesp ∧ (¬enc(add(esp, 8)) → ¬Ceax); Cmem[add(esp, 8)] := Ceax;
20 havoc mem¬epc; mem := store(mem, add(esp,8), eax);
21 Ceax := Cebp;
22 havoc mem¬epc; eax := sub(ebp, 720);
23 assert ¬Cesp ∧ (¬enc(add(esp, 4)) → ¬Ceax); Cmem[add(esp, 4)] := Ceax;
24 havoc mem¬epc; mem := store(mem, add(esp,4), eax);
25 assert ¬Cesp; Cmem[esp] := false;
26 havoc mem¬epc; mem := store(mem, esp, 7001000);
27 Cmem := C_memcpy(Cmem, esp);
28 arg1 := load(mem, esp); arg3 := load(mem, add(esp, 8));
29 havoc mem¬epc; mem := memcpy(mem, esp);
30 assert ∀i. ((arg1 ≤ i < add(arg1, arg3)) ∧ ¬enc(i)) → ¬Cmem[i];

Figure 4: penc−H instrumented with typing asser-
tions

proven to be a safe information leak using other proof tech-
niques. In our experiments, we safely eliminate declassiﬁed
outputs from information leakage checking if the protocol
veriﬁer has already proven them to be safe outputs.

To collect the Declassif ed annotations, we manually model

the cryptographic protocol to verify using an oﬀ-the-shelf
protocol veriﬁer. The choice of protocol veriﬁer is orthogonal
to our work. A protocol veriﬁer accepts as input an abstract
model of the protocol (in a formalism such as pi calculus),
and proves properties such as conﬁdentiality of protocol-level
secrets. We brieﬂy describe how we use Moat in tandem with
a protocol veriﬁer. If Moat establishes that a particular value
generated by penc is secret, this can be added to the set of
secrecy assumptions made in the protocol veriﬁer. Similarly,
if the protocol veriﬁer establishes conﬁdentiality even while
assuming that a penc’s output is observable by the adversary,
then we can declassify that output while verifying penc with
Moat. This assume-guarantee reasoning is sound because the
adversary model used by Moat can simulate a network ad-
versary — a network adversary reorders, inserts, and deletes
messages, and the observable eﬀect of these operations can
be simulated by a havoc mem¬epc.

We demonstrate this assume-guarantee reasoning on lines
22-26 of the OTP enclave in Figure 1, where line 26 no longer
has the memory safety vulnerability i.e. it uses the constant
64 instead of size_ﬁeld. Despite the ﬁx, Moat is unable to

1174prove that memcpy in line 26 of Figure 1 is safe because its
axiomatization of aes_gcm_encrypt is imprecise. We proceed
by ﬁrst proving in Moat that the sealing_key (obtained using
egetkey) is not leaked to the adversary. Next, we annotate
the ProVerif model with the assumption that sealing_key is
secret, which allows ProVerif to prove that the outbound
message (via memcpy) is safe. Based on this ProVerif proof,
we annotate the sealed_secret as Declassif ied, hence telling
Moat that the assert on line 30 of Figure 4 is valid.

This illustrates that protocol veriﬁcation not only provides
Declassif ied annotations, but also speciﬁes which values
must be kept secret by the enclave to ensure that the pro-
tocol is safe. The combination of Secrets and Declassif ied
annotations is called a policy, and this policy forms an input
to Moat in addition to the enclave program.
3.3 Assumptions and Limitations

Our work has the following fundamental limitations:
• The Intel SGX hardware is in our trusted computing
base. Speciﬁcally, we assume that all x86 + SGX in-
structions fulﬁll the ISA-deﬁned semantics. This elim-
inates a class of attacks such as physical tampering of
the CPU and supply chain attacks.

• To make static analysis feasible, Moat assumes that the
enclave code cannot be modiﬁed at runtime (enforced
using page permissions), and is statically linked.

• We do not consider attacks from observing side chan-

nels such as memory access patterns, timing, etc.

• Although SGX allows an enclave to have multiple CPU
threads, we only consider single-threaded enclaves for
simplicity.

The current implementation of Moat makes the following

additional assumptions:

• Moat’s implementation uses the Boogie [6] program
veriﬁer, Z3 [17] SMT solver, and BAP [14] for mod-
eling x86 instructions. All these dependencies are in
our trusted computing base.

• We use trusted implementation of cryptographic rou-
tines (cryptopp library [1]) to develop our benchmarks.
Since Moat does not model their implementation, they
are in our trusted computing base.

• Moat assumes that the enclave program has control
ﬂow integrity. Moat does not ﬁnd vulnerabilities that
exploit the control ﬂow behavior (such as ROP at-
tacks). This assumption is not fundamental, and can
be removed using modern runtime defenses (e.g. [3]).
• We assume that the enclave code cannot cause excep-
tions, apart from page fault exceptions which are han-
dled seamlessly by the OS/VMM. In other words, we
terminate the enclave in the event of all other excep-
tions (such as divide by 0).

• Moat assumes that the enclave code does not read (via
load instruction) from static save area (SSA). Note
that this assumption does not prevent the untrusted
code from invoking eresume (which is necessary for re-
suming from asynchronous exits). We have not yet
found this to be a limiting assumption in our bench-
marks.

4. FORMAL MODEL OF THE ENCLAVE

PROGRAM AND THE ADVERSARY

The remainder of this paper describes our veriﬁcation ap-
proach for defending against application attacks, which is
the focus of this paper. Moat takes a binary enclave pro-
gram and proves conﬁdentiality i.e. it does not leak secrets
to a privileged adversary. In order to construct proofs about
enclave behavior, we ﬁrst model the enclave’s semantics in
a formal language that is amenable to veriﬁcation, and also
model the eﬀect of adversarial operations on enclave behav-
ior. This section describes (1) formal modeling of enclave
programs, (2) formal model of the x86+SGX instruction set,
and (3) formal modeling of active and passive adversaries.

4.1 Syntax and Semantics of Enclave Programs
Our model of a x86+SGX machine consists of an un-
bounded number of Intel SGX CPUs operating with shared
memory. Although SGX allows an enclave to have multi-
ple CPU threads, we restrict our focus to single-threaded
enclaves for simplicity, and model all other CPU threads
as running privileged adversarial code. A CPU thread is
a sequence of x86+SGX instructions.
In order to reason
about enclave execution, Moat models the semantics of all
x86+SGX instructions executed by that enclave. This sec-
tion describes Moat’s translation of x86+SGX Assembly pro-
gram to a formal model, called penc, as seen in Figure 2.

Moat ﬁrst uses BAP [14] to lift x86 instructions into a sim-
ple microarchitectural instruction set: load from mem, store
to mem, bitwise (e.g. xor) and arithmetic (e.g. add) opera-
tions on regs, conditional jumps cjmp, unconditional jumps
jmp, and user-mode SGX instructions (ereport, egetkey,
and eexit). We choose BAP for its precise modeling of
x86 instructions, which includes updating of CPU ﬂags. We
have added a minimal extension to BAP in order to decode
SGX instructions. Each microarchitectural instruction from
above is modeled in penc as a sequential composition of Boo-
giePL [6] statements (syntax described in Figure 5). Boo-
giePL is an intermediate veriﬁcation language supporting
assertions that can be statically checked for validity using
automated theorem provers. Within penc, Moat uses un-
interpreted Functions constrained with axioms (described
in § 4.2) to model the semantics of each microarchitectural
instruction. These axioms describe the eﬀect of microarchi-
tectural instructions on machine state variables Vars, which
include main memory mem, ISA-visible CPU registers regs,
etc. We deﬁne the state σ ∈ Σ of penc at a given pro-
gram location to be a valuation of all variables in Vars. The
semantics of a BoogiePL statement s ∈ Stmt is given by
a relation R(s) ⊆ 2Σ×Σ over pairs of pre and post states,
where (σ, σ(cid:48)) ∈ R(s) if and only if there is an execution of s
starting at σ and ending in σ(cid:48). We use standard axiomatic
semantics for each Stmt in Figure 5 [7].

Enclaves have an entrypoint which is conﬁgured at com-
pile time and enforced at runtime by a callgate-like mech-
anism. Therefore, Moat makes BAP disassemble instruc-
tions in the code region starting from the enclave entry-
point. Procedure calls are either inlined or abstracted away
as uninterpreted functions. Speciﬁcally, trusted library calls
(e.g. AES-GCM authenticated encryption) are abstracted as
uninterpreted functions with standard axioms — the cryp-
tographic library is in our trusted computing base. Fur-
thermore, Moat soundly unrolls loops to a bounded depth

1175by adding an assertion that any iteration beyond the un-
rolling depth is unreachable. We omit further details on
the translation from the microarchitectural instructions to
penc (in the language presented in Figure 5) because it is
mostly syntactic and standard — lack of indirect control ﬂow
transfers (due to inlining) makes control ﬂow reconstruction
simpler. Our penc model is sound under the following as-
sumptions: (1) control ﬂow integrity, (2) code pages are not
modiﬁed (which is enforced using page permissions), and (3)
the trusted cryptographic library implementation is memory
safe (i.e. it does not modify any memory outside of the al-
located result buﬀer or memory allocated to the library).

By bounding the number of loop iterations and recursion
depth, the resulting veriﬁcation problem becomes decidable,
and one that can be checked using a theorem prover. Several
eﬃcient techniques [7] transform this loop-free and call-free
procedure containing assertions into a compact logical for-
mula in the Satisﬁability Modulo Theories (SMT) format by
a process called veriﬁcation-condition generation. This for-
mula is valid if and only if penc does not fail any assertion
in any execution — validity checking is done by an auto-
mated theorem prover based on SMT solving [17]. In the
case of assertion failures, the SMT solver also constructs
a counter-example execution of penc demonstrating the as-
In § 7, we show how Moat uses assertions
sertion failure.
and veriﬁcation-condition generation to prove conﬁdential-
ity properties of penc.

∈ Vars
x, X
∈ Relations
q
f, g, h ∈ Functions
∈ Expr
e
∈ Formula
φ
∈ Stmt

s

::= x | X | X[e] | f (e, . . . , e)
::= true | false | e == e |
q(e, . . . , e) | φ ∧ φ | ¬φ
::= skip | assert φ | assume φ |
X := e | x := e | X[e] := e |
if (e) {s} else {s} | s; s

Figure 5: Syntax of programs.

4.2 Formal Model of x86 and SGX instruc-

tions

While formal models of x86 instructions using BoogiePL
has been done before (see for instance [35]), we are the ﬁrst
to model SGX instructions. In section 4.1, we lifted x86 to a
microarchitectural instruction sequence, and modeled each
microarchitectural instruction as an uninterpreted function
(e.g. xor, load, ereport). In this section, we add axioms to
these uninterpreted functions in order to model the eﬀect of
instructions on machine state.

A state σ is a valuation of all Vars, which consists of mem,
regs, and epcm. As their names suggest, physical memory
σ.mem is modeled as an unbounded array, with index type of
32 bits and element type of 8 bits. mem is partitioned by the
platform into two disjoint regions: protected memory for en-
clave use (memepc), and unprotected memory (mem¬epc). For
any physical address a, epc(a) is true iﬀ a is an address in
memepc. Furthermore, memenc is a subset of memepc that is re-
served for use by penc — memenc is virtually addressed and it
belongs to the host application’s virtual address space. For
any virtual address a, enc(a) is true iﬀ a is within memenc.
The epcm is a ﬁnite sized array of hardware-managed struc-
tures, where each structure stores security critical metadata

about a page in memepc. epcmenc is a subset of epcm that
stores metadata about each page in memenc — other epcm
structures are either free or in use by other enclaves. regs
is the set of ISA-visible CPU registers such as eax, esp, etc.
Each microarchitectural instruction in penc has side-eﬀects
on σ, which we model using axioms on the corresponding un-
interpreted functions. In Figure 6, we present our model of a
sample bitvector operation xor, sample memory instruction
load, and sample SGX instruction eexit. We use the the-
orem prover’s built-in bitvector theories (⊕ operator in line
1) for modeling microarchitectural instructions that perform
bitvector operations. For load, we model both traditional
checks (e.g. permission bits, valid page table mapping, etc.)
and SGX-speciﬁc security checks. First, load reads the page
table to translate the virtual address va to physical address
pa (line 7) using a traditional page walk, which we model
as an array lookup. Operations on arrays consist of reads
x := X[y] and writes X[y] := x, which are interpreted by
the Theory of Arrays [8]. The boolean variable ea denotes
whether this access is made by enclave code to memenc. If ea
is true, then load asserts (line 14) that the following security
checks succeed:
• the translated physical address pa resides in memepc (line 9)
• epcm contains a valid entry for address pa (lines 10 and

11)

• enclave’s epcm entry and the CPU’s control register both

agree that the enclave owns the page (line 12)

• the page’s mapping in pagetable is same as when enclave

was initialized (line 13)

If non-enclave code is accessing memepc, or if penc is accessing
some other enclave’s memory (i.e. within memepc but outside
memenc), then load returns a dummy value 0xff (line 16).
We refer the reader to [26] for details on SGX memory access
semantics. Figure 6 also contains a model of eexit, which
causes the control ﬂow to transfer to the host application.
Models of other SGX instructions are available at [2].

1 function xor(x: bv32, y: bv32) { return x ⊕ y; }
2 function load(mem:[bv32]bv8, va:bv32)
3 {
4 var check : bool; //EPCM security checks succeed?
5 var pa : bv32; //translated physical address
6 var ea : bool; //enclave access to enclave memory?
7 pa := pagetable[va];
8 ea := CR_ENCLAVE_MODE && enc(va);
9 check :=

epc(pa) &&
EPCM_VALID(epcm[pa]) &&
EPCM_PT(epcm[pa]) == PT_REG &&
EPCM_ENCLAVESECS(epcm[pa]) == CR_ACTIVE_SECS &&
EPCM_ENCLAVEADDRESS(epcm[pa]) == va;

10
11
12
13
14 assert (ea => check); //EPCM security checks
15 assert ...; //read bit set and pagetable has valid mapping
16 if (!ea && epc(pa)) {return 0xff;} else {return mem[pa];}
17 }
18 function eexit(mem:[bv32]bv8, ebx:bv32)
19 {
20 var mem’ := mem; var regs’ := regs;
21 regs’[rip] := 0bv32 ++ ebx;
22 regs’[CR_ENCLAVE_MODE] := false;
23 mem’[CR_TCS_PA] := 0x00;
24 return (mem’,regs’);
25 }

Figure 6: Axioms for xor, load, and eexit instruc-
tions

11764.3 Adversary Model

In this section, we formalize a passive and active adver-
sary, which is general enough to model an adversarial host
application and also privileged malware running in the OS-
/VMM layer. penc’s execution is interleaved with the host
application — host application transfers control to penc via
eenter or eresume, and penc returns control back to the
host application via eexit. Control may also transfer from
penc to the OS (i.e. privileged malware) in the event of an
interrupt, exception, or fault. For example, the adversary
may generate interrupts or control the page tables so that
any enclave memory access results in a page fault, which is
handled by the OS/VMM. The adversary may also force a
hardware interrupt at any time. Once control transfers to
adversary, it may execute any number of arbitrary x86+SGX
instructions before transferring control back to the enclave.
Therefore, our model of an active adversary performs an un-
bounded number of following adversarial transitions between
any consecutive microarchitectural instructions executed by
penc:
1. Havoc all non-enclave memory (denoted by havoc mem¬epc):

While the CPU protects the epc region, a privileged
software adversary can write to any memory location
in mem¬epc region. havoc mem¬epc is encoded in BoogiePL
as:

assume ∀a. epc(a) → memnew[a] == mem[a];
mem := memnew;

where memnew is an unconstrained symbolic value that
is type-equivalent to mem. Observe that the adversary
modiﬁes an unbounded number of memory locations.

2. Havoc page tables: A privileged adversary can modify
the page tables to any value. Since page tables reside in
mem¬epc, havoc mem¬epc models havoc on page tables.

3. Havoc CPU registers (denoted by havoc regs). regs
are modiﬁed only during adversary execution, and re-
trieve their original values once the enclave resumes.
havoc regs is encoded in BoogiePL as:

regs := regsnew;

where each register (e.g. eax ∈ regs) is set to an uncon-
strained symbolic value.

4. Generate interrupt (denoted by interrupt): The adver-
sary can generate interrupts at any point, causing the
CPU jump to the adversarial interrupt handler.

5. Invoke any SGX instruction with any operands (denoted
by call sgx): The attacker may invoke ecreate, eadd,
eextend, einit, eenter, and eresume to launch any
number of new enclaves with code and data of attacker’s
choosing.

Any x86+SGX instruction that an active adversary in-
vokes can be approximated by some ﬁnite-length combina-
tion of the above 5 transitions. Our adversary model is
sound because it allows the active adversary to invoke an
unbounded number of these transitions. Furthermore, the
active adversary is quite powerful in this model.
It may
control the entire software stack, from the host application
upto the OS/VMM layers, thereby modeling attacks from
malicious administrators and privileged malware. The ad-
versary controls all hardware peripherals, and may insert,

delete, modify and replay all communication with the exter-
nal world — these attacks can be modeled using the above
5 transitions. Side-channels are out of scope, where typical
side channels may include memory access patterns, timing
leaks, and I/O traﬃc patterns.

We deﬁne an active and passive adversary:
Definition 1. General Active Adversary G. Between
any consecutive statements along an execution of penc, G
may execute an unbounded number of transitions of type
havoc mem¬epc, havoc regs, interrupt, or call sgx, thereby
modifying a component σ|G of machine state σ. Following
each microarchitectural instruction in penc, G observes a pro-
jection σ|obs of machine state σ. Here, σ|obs
.
= (σ.mem¬epc),
and σ|G .

= (σ.mem¬enc, σ.regs, σ.epcm¬enc).

Definition 2. Passive Adversary P. The passive ad-
versary P observes a projection σ|obs of machine state σ after
each microarchitectural instruction in penc. Here, σ|obs
.
=
(σ.mem¬epc) includes the non-enclave memory. P does not
modify any state.

Note that we omit σ.regs from σ|obs because they cannot
be accessed by the adversary while the CPU operates in en-
clave mode — asynchronous exits clear their values, while
eexit removes the CPU from enclave mode. Enclave ex-
ecution may result in exceptions (such as divide by 0 and
page fault exception) or faults (such as general protection
fault), in which case the exception codes are conveyed to
the adversarial OS. We omit exception codes from σ|obs for
veriﬁcation ease, and terminate the enclave (at runtime) on
an exception, with the caveat of page fault exceptions which
are allowed. Since G can havoc page tables, it can cause
page fault exceptions at runtime. However, a page fault
only reveals memory access patterns (at the page granular-
ity), which we consider to be a side-channel observation.

5. COMPOSING ENCLAVE WITH THE AD-

VERSARY

Moat reasons about penc’s execution in the presence of
an adversary (P or G) by composing their state transition
systems. An execution of penc is a sequence of statements
[l1 : s1, l2 : s2, . . . , ln : sn], where each si is a load,
store, register assignment x := e, user-mode SGX instruc-
tion (ereport, egetkey, or eexit), or a conditional state-
ment. Since penc is made to be loop-free (by sound loop un-
rolling), each statement si has a distinct label li that relates
to the program counter. We assume that each microarchi-
tectural instruction (not the x86 instruction) executes atom-
ically, although the atomicity assumption is architecture de-
pendent.
Composing enclave penc with passive adversary P.
In the presence of P, penc undergoes a deterministic se-
quence of state transitions starting from initial state σ0. P
cannot update V ars, therefore P aﬀects penc’s execution
only via the initial state σ0. We denote this sequence of
states as trace t = [σ0, σ1, . . . , σn], where (σi, σi+1) ∈ R(si)
for each i ∈ 0, . . . , n− 1. We also write this as (cid:104)penc, σ0(cid:105) ⇓ t.
Composing enclave penc with active adversary G.

G can aﬀect penc at any step of execution by executing an
unbounded number of adversarial transitions. Therefore, to

1177model penc’s behaviour in the presence of G, we consider the
following composition of penc and G. For each penc state-
ment l : s, we transform it to:

1. penc invokes load(mem, a), where a is an address in mem¬epc.
G havocs mem¬epc and penc reads mem¬epc for inputs, so
this counter-example is not surprising.

adv1; . . . ; advk; l : s

(2)

This instrumentation (where k is unbounded) guarantees
that between any consecutive statements along an execution
of penc, G can execute an unbounded sequence of adversar-
ial transitions adv1; . . . ; advk, where each statement advi is
an adversarial transition of type havoc mem¬epc, havoc regs,
interrupt, or call sgx. This composed model, hereby
called penc−G, encodes all possible behaviours of penc in the
presence of G. An execution of penc−G is described by a se-
quence of states i.e. trace t = [α0, σ0, α1, σ1, . . . , αn, σn],
where each αi ∈ t denotes the state after the last adver-
sary transition advk (right before execution resumes in the
enclave). We coalesce the eﬀect of all adversary transitions
into a single state αi for cleaner notation. Following advk,
the composed model penc−G executes an enclave statement
l : s, taking the system from a state αi to state σi.

Given a trace t = [α0, σ0, α1, σ1, . . . , αn, σn], we deﬁne

t|obs

= [σ0|obs, σ1|obs, . . . , σn|obs]
.

denoting the adversary-observable projection of trace t, ig-
noring the adversary controlled α states. Correspondingly,
we deﬁne

t|G .

= [α0|G , α1|G , . . . , αn|G ]

capturing the adversary’s eﬀects within a trace t. We deﬁne
the enclave projection of σ to be

σ|enc

.
= (σ.memenc, σ.regs, σ.epcmenc)

This is the component of machine state σ that is accessible
only by penc. Correspondingly, we deﬁne

t|enc

= [σ0|enc, σ1|enc, . . . , σn|enc]
.

The transformation in (2) allows the adversary to perform
an unbounded number of operations adv1, . . . , advk, where
k is any natural number. Since we cannot verify unbounded
length programs using veriﬁcation-condition generation, we
consider the following alternatives:
• Bound the number of operations (k) that the adversary is
allowed to perform. Although this approach bounds the
length of penc−G, it unsoundly limits the G’s capabilities.
• Use alternative adversary models in lieu of G with the
hope of making the composed model both bounded and
sound.

We explore the latter option in Moat. Our initial idea was
to try substituting P for G. This would be the equivalent
of making k equal 0, and thus penc−G bounded in length.
However, for this to be sound, we must prove that G’s op-
erations can be removed without aﬀecting penc’s execution,
as required by the following property.

∀σ ∈ Σ. ∀ti, tj ∈ Σ∗. (cid:104)penc−G , σ(cid:105) ⇓ ti ∧ (cid:104)penc, σ(cid:105) ⇓ tj ⇒

∀i. ti|enc[i] = tj|enc[i]

(3)

If property (3) holds, then we can substitute P for G while
proving any safety (or k-safety [16]) property of penc. While
attempting to prove this property in the Boogie veriﬁer [6],
we quite expectedly discovered counter-examples that illus-
trate the diﬀerent ways in which G aﬀects penc’s execution:

2. penc invokes load(mem, a), where a is an address within
SSA pages. G can force an interrupt, causing the CPU to
save enclave state in SSA pages. If the enclave resumes
and reads from SSA pages, then the value read depends
on the enclave state at the time of last interrupt.

If we prevent penc from reading mem¬epc or the SSA pages,
we successfully prove property (3). From hereon, we con-
strain penc to not read from SSA pages; we do not ﬁnd this
to be limiting in our case studies. Note that this restric-
tion does not prevent the use of eresume instruction, which
causes the CPU to access the SSA pages directly. However,
the former constraint (not reading mem¬epc) is too restrictive
in practice because penc must read mem¬epc to receive inputs.
Therefore, we must explore alternative adversary models.
Instead of replacing G with P, we attempt replacing G with
H deﬁned below.

Definition 3. Havocing Active Adversary H.

Between any consecutive statements along an execution of
penc, H may execute a single havoc mem¬epc operation, thereby
modifying a component σ|H of machine state σ. Follow-
ing each microarchitectural instruction in penc, H observes a
projection σ|obs of machine state σ. Here, σ|obs
.
= (σ.mem¬epc),
and σ|H .

= (σ.mem¬epc).

Composing enclave penc with active adversary H.

To construct penc−H, we transform each penc statement

l : s to:

havoc mem¬epc; l : s

(4)

Figure 3 shows a sample transformation from penc to penc−H.
Similar to our previous exercise with P, we prove that it is
sound to replace G with H while reasoning about enclave
execution.

Theorem 1. Given an enclave program penc, let penc−G
be the composition of penc and G via the transformation in
(2) and penc−H be the composition of penc and H via the
transformation in (4). Then,
∀σ ∈ Σ. ∀t1 ∈ Σ∗. (cid:104)penc−G , σ(cid:105) ⇓ t1 ⇒

∃t2 ∈ Σ∗. (cid:104)penc−H, σ(cid:105) ⇓ t2 ∧ ∀i. t1|enc[i] = t2|enc[i]
Validity of this theorem implies that we can replace G with
H while proving any safety property or k-safety hyperprop-
erty of enclave behaviour [16]. We prove theorem 1 with the
use of lemma 1 and lemma 2, described next.
The transformation in (2) composed penc with G by in-
strumenting an unbounded number of adversary operations
adv1; . . . ; advk before each statement in penc. Let us further
instrument havoc mem¬epc after each advi ∈ {adv1; . . . ; advk}
— this is sound because a havoc on mem¬epc does not restrict
the allowed values of mem¬epc. The resulting instrumentation
for each statement l : s is:

adv1; havoc mem¬epc; . . . ; advk; havoc mem¬epc; l : s
(5)
Lemma 1 proves that the eﬀect of advi ∈ {adv1; . . . ; advk}
on penc can be simulated by a sequence of havocs to mem¬epc.

1178In order to deﬁne lemma 1, we introduce the following trans-
formation on each statement l : s of penc:

havoc mem¬epc; . . . ; havoc mem¬epc; l : s

(6)

Lemma 1. Given an enclave program penc, let penc−G∗ be
the composition of penc and adversary via the transformation
in (5) and penc−H∗ be the composition of penc and adversary
via the transformation in (6). Then,

∀σ ∈ Σ. ∀t1 ∈ Σ∗. (cid:104)penc−G∗, σ(cid:105) ⇓ t1 ⇒

∃t2 ∈ Σ∗. (cid:104)penc−H∗, σ(cid:105) ⇓ t2 ∧ ∀i. t1|enc[i] = t2|enc[i]

Proof : The intuition is that (if the enclave makes progress)
the other adversarial transitions do not aﬀect penc in any
way that is not already simulated by havoc mem¬epc. For ex-
ample, an interrupt causes the enclave to resume in the state
prior to the interrupt. In addition, the CPU detects mod-
iﬁcations to the page tables that aﬀect the enclave pages,
and prevents the enclave from progressing. Other enclaves
on the machine may aﬀect execution, but only by sending
messages via non-enclave memory, which is simulated by
havoc mem¬epc. We prove lemma 1 by induction as follows,
and also machine-check the proof in Boogie [6] (proof avail-
able at [2]). The inductive proof makes use of our modeling
of SGX instructions, and is setup as follows. This property
in Lemma 1 is a predicate over a pair of traces, making it a
2-safety hyperproperty [16]. A counter-example to this prop-
erty is a pair of traces ti, tj where G has caused ti to diverge
from tj. We rewrite the property as a 2-safety property and
prove it via 1-step induction over the length of the trace, as
follows. For any pair of states (σi,σj) that is indistinguish-
able to the enclave, we prove that after one transition, the
new pair of states (σ(cid:48)
j) is also indistinguishable. Here,
j) ∈ R(sj), where si is executed
(σi, σ(cid:48)
by penc−G∗ and sj is executed by penc−H∗. The state pred-
icate Init represents an enclave state after invoking einit
in the prescribed initialization sequence in (1). Property 7
is the base case and property 8 is the inductive step in the
proof by induction of lemma 1.

i) ∈ R(si) and (σj, σ(cid:48)

i,σ(cid:48)

∀σi, σj .Init(σi) ∧ Init(σj ) ⇒ σi|enc = σj|enc

(7)

i, σ(cid:48)

j , si, sj .

∀σi, σj , σ(cid:48)
σi|enc = σj|enc ∧ (σi, σ(cid:48)
⇒ σ(cid:48)

i|enc = σ(cid:48)

j|enc

i) ∈ R(si) ∧ (σj , σ(cid:48)

j ) ∈ R(sj ) ∧ p(si, sj )
(8)

where

p(si, sj )

.
=

si ∈ {egetkey, ereport, eexit, load, store} ∧ sj = si

si = s; havoc mem¬epc ∧ sj = havoc mem¬epc

where s ∈ {havoc mem¬epc, . . . , interrupt, call sgx}

Lemma 2. A sequential composition of unbounded num-
ber of havoc mem¬epc statements can be simulated by a single
havoc mem¬epc statement.

Proof : Lemma 2 requires a straightforward proof as it fol-
lows naturally from the semantics of havoc.

Combining lemma 1 and lemma 2, we prove that the ef-
fect of adv1; havoc mem¬epc; . . . ; advk; havoc mem¬epc (or adv1;
adv2;. . . ; advn) on enclave’s execution can be simulated by
havoc mem¬epc. By theorem 1, it is sound to prove any safety
(or k-safety) property on penc−H because penc−H allows all
traces allowed by penc−G. The beneﬁts of composing with
H are (1) penc−H is bounded in size, which allows using
standard veriﬁcation techniques to prove safety (or k-safety)

properties of enclave programs, and (2) H gives a conve-
nient mental model of an active adversary’s eﬀects on en-
clave execution. While we focus on proving conﬁdentiality
from hereon, we note that theorem 1 is valuable for soundly
proving any safety property of enclave programs.
6. FORMALIZING CONFIDENTIALITY

Moat’s deﬁnition of conﬁdentiality is inspired by standard

non-interference deﬁnition [27], but adapted to the instruction-
level modeling of the enclave programs. Conﬁdentiality can
be trivially achieved with the deﬁnition that H cannot dis-
tinguish between penc and an enclave that executes skip in
each step. However, such deﬁnition prevents penc from writ-
ing to mem¬epc, which it must write in order to return outputs
or send messages to remote parties. To that end, we weaken
this deﬁnition to allow for writes to mem¬epc, but constraining
the values to be independent of the secrets. An input to Moat
is a policy that deﬁnes Secrets = {(l, v) | l ∈ L, v ∈ Vars},
where a tuple (l, v) denotes that variable v holds a secret
value at program location l. In practice, since secrets typi-
cally occupy several bytes in memory, v is a range of sym-
bolic addresses in the enclave heap. We deﬁne the following
transformation from penc−H to penc−H−sec for formalizing
conﬁdentiality. For each (l, v) ∈ Secrets, we transform the
statement l : s to:

l : s; havoc v;

(9)

havoc v assigns an unconstrained symbolic value to variable
v. With this transformation, we deﬁne conﬁdentiality as
follows:

Definition 4. Conﬁdentiality For any pair of traces of
penc−H−sec that potentially diﬀer in the values of the Secret
variables, if H’s operations along the two traces are equiv-
alent, then H’s observations along the two traces must also
be equivalent.
∀σ ∈ Σ, t1, t2 ∈ Σ∗.((cid:104)penc−H−sec, σ(cid:105) ⇓ t1 ∧ (cid:104)penc−H−sec, σ(cid:105) ⇓ t2 ∧

∀i.t1|H[i] = t2|H[i]) ⇒ (∀i.t1|obs[i] = t2|obs[i])
(10)

The havoc on Secrets cause the secret variables to take
potentially diﬀering symbolic values in t1 and t2. Prop-
erty (10) requires t1|obs and t2|obs to be equivalent, which
is achieved only if the enclave does not leak secrets to H-
observable state.

While closer to the desired deﬁnition, it still prevents penc
from communicating declassiﬁed outputs that depend on se-
crets. For instance, recall that the OTP enclave outputs the
encrypted secret to be stored to disk.
In this case, since
diﬀerent values of secret produce diﬀerent values of cipher-
text, penc violates property (10). This is a false alarm if the
encryption uses a secret key to produce the ciphertext. To
remove such false alarms, we take the standard approach of
extending the policy with Declassif ied = {(l, v) | l ∈ L, v ∈
Vars}, where a tuple (l, v) denotes that variable v at loca-
tion l contains a declassiﬁed value. In practice, since out-
puts typically occupy several bytes in memory, v is a range
of symbolic addresses in the enclave heap. We can safely
eliminate declassiﬁed outputs from information leaks as the
protocol veriﬁer has already proven them to be safe outputs
(see Section 3.2). When declassiﬁcation is necessary, we use
the following property for checking conﬁdentiality.

Definition 5. Conﬁdentiality with Declassiﬁcation
For any pair of traces of penc−H−sec that potentially diﬀer

1179in the values of the Secret variables, if H’s operations along
the two traces are equivalent, then H’s observations (ignor-
ing Declassiﬁed outputs) along the two traces must also be
equivalent.
∀σ ∈ Σ, t1, t2 ∈ Σ∗.((cid:104)penc−H−sec, σ(cid:105) ⇓ t1 ∧ (cid:104)penc−H−sec, σ(cid:105) ⇓ t2

∧ ∀i.t1|H[i] = t2|H[i]) ⇒
(∀i, j. ¬epc(j) ⇒ ((i, mem[j]) ∈ Declassif ied
∨ t1|obs[i].mem[j] = t2|obs[i].mem[j]))
(11)

7. PROVING CONFIDENTIALITY

Moat automatically checks if penc−H satisﬁes conﬁdential-
ity (property 11). Since conﬁdentiality is a 2-safety hyper-
property (property over pairs of traces), we cannot use black
box program veriﬁcation techniques, which are tailored to-
wards safety properties. Hence, we create a security type
system in which type safety implies that penc−H satisﬁes
conﬁdentiality. We avoid a self-composition approach be-
cause of complications in encoding equivalence assumptions
over adversary operations in the two traces of penc−H−sec
(property 11). As in many type-based systems [34, 28], the
typing rules prevent programs that have explicit and implicit
information leaks. Explicit leaks occur via assignments of
secret values to H-observable state i.e. σ|obs. For instance,
mem := store(mem,a,d) is ill-typed if d’s value depends on a
secret and enc(a) is false i.e. it writes a secret to non-enclave
memory. An implicit leak occurs when a conditional state-
ment has a secret-dependent guard, but updates H-visible
state in either branch. For instance, if (d == 42) {mem :=
store(mem, a, 1)} else {skip} is ill-typed if d’s value depends
on a secret and enc(a) is false. In both examples above, H
learns the secret value d by reading mem at address a. In
addition to the store instruction, explicit and implicit leaks
may also be caused by unsafe use of SGX instructions. For
instance, egetkey returns a secret sealing key, which must
not be leaked from the enclave. Similarly, ereport generates
a signed report containing public values (e.g. measurement)
and potentially secret values (enclave code may bind upto
64 bytes of data, which may be secret). Our type system
models these details of SGX accurately, and accepts penc−H
only if it has no implicit or explicit leaks.
A security type is either (cid:62) (secret) or ⊥ (public). At each
program location, each memory location and CPU register
has a security type based on the x86+SGX instructions ex-
ecuted until that label. The security types are needed at
each program location because variables (especially regs)
may alternate between holding secret and public values.
As explained later in this section, Moat uses the security
types in order to decide whether instructions in penc−H have
implicit or explicit leaks. penc−H has Secrets = {(l, v)}
and Declassif ied = {(l, v)} annotations. However, there
are no other type declarations; therefore, Moat implements
a type inference algorithm based on computing reﬁnement
type constraints and checking their validity using a theorem
prover. In contrast, type checking without inference would
require the programmer to painstakingly provide security
types for each memory location and CPU register, at each
program location — ﬂow sensitivity and type inference are
key requirements of type checking machine code.

Moat’s type inference algorithm computes ﬁrst-order logi-
cal constraints under which an expression or statement takes
a security type. A typing judgment (cid:96) e : τ ⇒ ψ means that
the expression e has security type τ whenever the constraint

ψ is satisﬁed. An expression of the form op(v1, . . . , vn)
(where op is a relation or function) has type τ if all vari-
ables {v1, . . . , vn} have type τ or lower. That is, an expres-
sion may have type ⊥ iﬀ its value is independent of Secrets.
For a statement s to have type τ , every assignment in s
must update a state variable whose security class is τ or
higher. We write this typing judgment as [τ ] (cid:96) s ⇒ (cid:104)ψ,F(cid:105),
where ψ is a ﬁrst-order (SMT) formula and F is a set of
ﬁrst-order (SMT) formulae. Each satisﬁable interpretation
of ψ corresponds to a feasible execution of s. F contains a
SMT formula for each instruction in s, such that the formula
is valid iﬀ that instruction does not leak secrets. We present
our typing rules in Figure 7, which assume that penc−H is
ﬁrst converted to single static assignment form. s has type
τ if we derive [τ ] (cid:96) s ⇒ (cid:104)ψ,F(cid:105) using the typing rules, and
prove that all formulae in F are valid. If s has type (cid:62), then
s does not update H-visible state, and thus cannot contain
information leaks. Having type (cid:62) also allows s to execute in
a context where a secret value is implicitly known through
the guard of a conditional statement. On the other hand,
type ⊥ implies that s either does not update H-observable
state or the update is independent of Secrets.

By Theorem 1, penc−H models all potential runtime be-
haviours of penc in the presence of an active adversary, and
hence Moat feeds penc−H to the type checking algorithm. We
now explain some of our typing rules from Figure 7. For each
variable v ∈ Vars within penc−H, our typing rules introduce
a ghost variable Cv that is true iﬀ v has security type (cid:62). For
a scalar register v, Cv is a boolean; for an array variable v, Cv
(e.g. Cmem) is an array and Cv[i] denotes the security type
for each location i. exp1 rule allows inferring the type of
any expression e as (cid:62). exp2 rule allows inferring an expres-
sion type e as ⊥ if we derive Cv to be false for all variables
v in the expression e. storeL rule marks the memory loca-
tion as secret if the stored data is secret. In case of secret
data, we assert that the updated location is within mem¬enc;
we also assert that the address is public to prevent implicit
leaks. Since storeH rule types store instructions as (cid:62), it un-
conditionally marks the memory location as secret. This is
necessary because the store may execute in a context where
a secret is implicitly known through the guard of a condi-
tional statement. load marks the updated register as secret
if the memory location contains a secret value. ereportL rule
types the updated memory locations as per SGX semantics.
ereport takes 64 bytes of data (that the programmer intends
to bind to the measurement) at address in ecx, and copies
them to memory starting at address edx + 320; the rest of
the report has public data such as the MAC, measurement,
etc. Hence, Cmem retains the secrecy level for the 64 bytes
of data, and assumes f alse for the public data. Similar to
storeH, ereportH unconditionally marks all 432 bytes of the
report as secret. egetkey stores 16 bytes of the sealing key
at address ecx, hence the egetkey rule marks those 16 bytes
in Cmem as secret. Notice that we do not assert that ereport
and egetkey writes to enclave memory since this is enforced
by SGX. eexit jumps to the host application without clear-
ing regs. Hence, the eexit rule asserts that those regs hold
public values. We prove the following type soundness theo-
rem (machine-checked proof available at [2]).

Theorem 2. For any penc−H such that [τ ] (cid:96) penc−H ⇒
(ψ,F) is derivable (where τ is either (cid:62) or ⊥) and all for-
mulae in F are valid, penc−H satisﬁes property 11.

1180(cid:96) e : (cid:62) ⇒ true
[(cid:62)] (cid:96) s ⇒ (cid:104)ψ, A(cid:105)
[⊥] (cid:96) s ⇒ (cid:104)ψ, A(cid:105)

(exp1 )

(coercion)

(cid:96) e : ⊥ ⇒ (cid:86)

v∈V ars(e)

(exp2 )

¬Cv

[(cid:62)] (cid:96) skip ⇒ (cid:104)true,{∅}(cid:105)

(skip)

[τ ] (cid:96) assume φ ⇒ (cid:104)φ,{∅}(cid:105)

(assume)

[τ ] (cid:96) x(cid:48) := e ⇒ (cid:104)(x(cid:48) = e) ∧ (Cx(cid:48) ↔ (cid:87)

[τ ] (cid:96) assert φ ⇒ (cid:104)φ,{φ}(cid:105)

(assert)

Cv),{∅}(cid:105)

(scalar )

v∈V ars(e)

(cid:96) ea : ⊥ ⇒ ψa

[τ ] (cid:96) x(cid:48) := load(mem, ea) ⇒
(cid:104)(x(cid:48) = load(mem, ea)) ∧ (Cx(cid:48) ↔ Cmem[ea]),{ψa}(cid:105)

(load)

(cid:96) ea : ⊥ ⇒ ψa
(cid:48) := store(mem, ea, ed) ⇒

[(cid:62)] (cid:96) mem
(cid:104)mem
{ψa ∧ enc(ea)}(cid:105)

(cid:48) = store(mem, ea, ed) ∧ Cmem(cid:48) = Cmem[ea := true],

(storeH )

(cid:96) ea : ⊥ ⇒ ψa
(cid:48) := store(mem, ea, ed) ⇒

[⊥] (cid:96) mem
(cid:104)mem
{ψa ∧ (¬enc(ea) → ψd)}(cid:105)

(cid:48) = store(mem, ea, ed) ∧ Cmem(cid:48) = Cmem[ea := ψd],

(cid:96) ed : ⊥ ⇒ ψd

(storeL)

(ereportL)

(cid:48) = ereport(mem, ebx, ecx, edx) ∧

[⊥] (cid:96) mem
(cid:48) := ereport(mem, ebx, ecx, edx) ⇒
(cid:104)mem
∀i. (edx ≤ i < edx + 320) → Cmem(cid:48) [i] ↔ f alse
∧ (edx + 320 ≤ i < edx + 384) →
∧ (edx + 384 ≤ i < edx + 432) → Cmem(cid:48) [i] ↔ f alse
∧ ¬(edx ≤ i < edx + 432) → Cmem(cid:48) [i] ↔ Cmem[i],
{¬Cedx}(cid:105)

Cmem(cid:48) [i] ↔ Cmem[ecx + i − edx − 320]

(cid:48) = ereport(mem, ebx, ecx, edx) ∧

(cid:48) := ereport(mem, ebx, ecx, edx) ⇒

[(cid:62)] (cid:96) mem
(cid:104)mem
∀i. (edx ≤ i < edx + 432) → (Cmem(cid:48) [i] ↔ true)
∧ ¬(edx ≤ i < edx + 432) → (Cmem(cid:48) [i] ↔ Cmem[i]),
{¬Cedx}(cid:105)

(ereportH )

(cid:48) = egetkey(mem, ebx, ecx) ∧

(cid:48) := egetkey(mem, ebx, ecx) ⇒

[τ ] (cid:96) mem
(cid:104)mem
∀i. (ecx ≤ i < ecx + 16) → Cmem(cid:48) [i] ↔ true
∧ ¬(ecx ≤ i < ecx + 16) → Cmem(cid:48) [i] ↔ Cmem[i],
{¬Cecx}(cid:105)

(egetkey)

(eexit)

Moat implements this type system by replacing each state-
ment s in penc−H by I(s) using the instrumentation rules
in Figure 8. Observe that we introduce Cpc to track whether
conﬁdential information is implicitly known through the pro-
gram counter. If a conditional statement’s guard depends on
a secret value, then we set Cpc to true within the then and
else branches. Moat invokes I(penc−H) and applies the in-
strumentation rules in Figure 8 recursively. Figure 4 demon-
strates an example of instrumenting penc−H. Moat then feeds
the instrumented program I(penc−H) to an oﬀ-the-shelf pro-
gram veriﬁer, which proves validity all assertions or ﬁnds a
counter-example. Our implementation uses the Boogie [6]
program veriﬁer, which receives I(penc−H) and generates
veriﬁcation conditions in the SMT format. Boogie uses the
Z3 [17] theorem prover (SMT solver) to prove the veriﬁca-
tion conditions. An advantage of using SMT solving is that
a typing error is explained using counter-example execution,
demonstrating the information leak and exploit.

Statement s

Instrumented Statement I(s)

assert φ
assume φ
skip
havoc mem¬epc
x := e
x := load(mem, e)

mem :=
store
(mem, ea, ed)

assert φ
assume φ
skip
havoc mem¬epc

v∈V ars(e) Cv; x := e

Cx := Cpc ∨(cid:87)
assert (cid:86)
assert (cid:86)
assert (¬Cpc ∧ ¬enc(ea)) → ((cid:86)
Cmem[ea] := Cpc ∨(cid:87)

v∈V ars(e) ¬Cv;
v∈V ars(ea) ¬Cv; assert Cpc → enc(ea);
v∈V ars(ed ) ¬Cv);

Cx := Cpc ∨ Cmem[e]; x := load(mem, e)

v∈V ars(ed) Cv;

Cmem[i] = Cpc;

mem := Cmem; havoc Cmem;

Cmem[i] = (Cpc ∨ Cold

mem[ecx + i − edx − 320]);

mem := store(mem, ea, ed)
assert ¬Cedx; Cold
assume ∀i. (edx ≤ i < edx + 320) → Cmem[i] = Cpc;

mem :=
ereport
(mem, ebx, ecx, edx) assume ∀i. (edx + 320 ≤ i < edx + 384) →
assume ∀i. (edx + 384 ≤ i < edx + 432) →
assume ∀i. ¬(edx ≤ i < edx + 432) → Cmem[i] = Cold
mem := ereport(mem, ebx, ecx, edx)
assert ¬Cecx; Cold
assume ∀i. (ecx ≤ i < ecx + 16) → Cmem[i];
assume ∀i. ¬(ecx ≤ i < ecx + 16) → Cmem[i] = Cold
mem := egetkey(mem, ebx, ecx)
assert ∀r ∈ regs. ¬Cr;
mem, regs := eexit(mem)
I(s1); I(s2)
pc := Cpc;
v∈V ars(e) Cv;
if (e) {I(s1)} else {I(s2)};
Cpc := Cin
pc

mem, regs :=
eexit(mem, ebx)
s1; s2
if(e){s1}else{s2} Cin

Cpc := Cpc ∨(cid:87)

mem :=
egetkey
(mem, ebx, ecx)

mem := Cmem; havoc Cmem;

mem[i];

mem[i];

Figure 8: Instrumentation rules for penc−H

In summary, Moat’s type system is inspired by the type-
based approach for information ﬂow checking by Volpano et
al. [34]. The core modiﬁcations are as follows:
• Our type system includes rules for SGX instructions ereport,

egetkey, and eexit. The rules precisely model the mem-
ory locations written by these these instructions, and
whether the produced data is public or conﬁdential.

[τ ] (cid:96) mem
(cid:104)(mem

(cid:48), regs

(cid:48) := eexit(mem) ⇒

(cid:48), regs(cid:48)) = eexit(mem),{∀r ∈ regs. ¬Cr}(cid:105)
[τ ] (cid:96) s2 ⇒ (cid:104)ψ2,F2(cid:105)

[τ ] (cid:96) s1 ⇒ (cid:104)ψ1,F1(cid:105)

[τ ] (cid:96) s1; s2 ⇒ (cid:104)ψ1 ∧ ψ2,F1 ∪ {ψ1 → f2 | f2 ∈ F2}(cid:105)

(seq)

(cid:96) e : τ ⇒ ψ

[τ ] (cid:96) s1 ⇒ (cid:104)ψ1,F1(cid:105)

[τ ] (cid:96) s2 ⇒ (cid:104)ψ2,F2(cid:105)

[τ ] (cid:96) if (e) {s1} else {s2} ⇒
(cid:104)(e → ψ1) ∧ (¬e → ψ2),
{ψ} ∪ {e → f1 | f1 ∈ F1} ∪ {¬e → f2 | f2 ∈ F2}(cid:105)

Figure 7: Typing Rules for penc−H

(ite)

• Our type system is ﬂow-sensitive and path-sensitive, and
performs type inference. A program is well-typed if the
typing assertions are valid in all feasible executions. We
ensure soundness by using a sound program veriﬁer to ex-
plore all feasible executions of the instrumented penc−H.

1181• Our type system includes rules for updating unbounded
array variables (e.g. mem), without requiring that all in-
dices in the array take the same security type.

8. EVALUATION AND EXPERIENCE

Moat’s implementation comprises (1) translation from x86
+ SGX program to penc using BAP, (2) transformation to
penc−H using instrumentation in 4, (3) transformation to
I(penc−H) using Figure 8, and (4) invoking the Boogie veri-
ﬁer to prove validity of all assertions in I(penc−H) (modulo
declassiﬁcations from the protocol veriﬁcation step).
Optimizations: Our primary objective was to build a sound
veriﬁer for proving conﬁdentiality in the presence of an ac-
tive adversary. However, due to certain scalability chal-
lenges, we implement the following optimizations. First,
we only introduce havoc mem¬epc prior to load instructions
because only a load can be used to read mem¬epc. Further-
more, we axiomatize speciﬁc library calls such as memcpy
and memset, because their loopy implementations incur sig-
niﬁcant veriﬁcation overhead.

e

We now describe some case studies which we veriﬁed us-
ing Moat and ProVerif in tandem, and summarize the results
in Figure 9. We use the following standard crytpographic no-
tation and assumptions. m1| . . .|mn denotes tagged concate-
nation of n messages. We use a keyed-hash message authen-
tication function MACk(text) and hash function H(text),
both of which are assumed to be collision-resistant. For
asymmetric cryptography, K−1
and Ke are principal e’s pri-
vate and public signature keys, where we assume that Ke is
long-lived and distributed within certiﬁcates signed by a root
of trust authority. Digital signature using a key k is written
as Sigk(text); we assume unforgeability under chosen mes-
Intel provisions each SGX processor with a
sage attacks.
−1
SGX that is available to a special quot-
unique private key K
ing enclave. In combination with this quoting enclave, an en-
clave can invoke ereport to produce quotes, which is essen-
−1
tially a signature (using the private key K
SGX ) of the data
produced by the enclave and its measurement. We write a
quote produced on behalf of enclave e as Quotee(text), which
(H(text) | Me) — measurement of
is equivalent to SigK
enclave e is written as Me. N is used to denote nonce.
Finally, we write Enck(text) for the encryption of text, for
which we assume indistinguishability under chosen plaintext
attack. We also use AEnck(text) for authenticated encryp-
tion, for which we assume indistinguishability under chosen
plaintext attack and integrity of ciphertext. Recall that al-
though the enclave code contains calls to these cryptographic
primitives, Moat abstracts them as uninterpreted functions
with basic memory safety axioms. We use cryptopp [1] in
our case studies, and we do not verify its implementation.

−1
SGX

One-time Password Generator.
The abstract model of the OTP secret provisiong protocol
(from § 2), where client runs in a SGX enclave, bank is an
uncompromised service, and disk is under adversary control:
bank → client : N
client → bank : N | gc | Quoteclient(N | gc)
bank → client : N | gb | SigK
client → disk : AEncKseal (secret)

(N | gb) | AEncH(gbc)(secret)

−1
bank

First, we use Moat to prove that gbc and Kseal are not
leaked to H. Next, ProVerif uses secrecy assumption on gbc
and Kseal to prove that secret is not leaked to a network ad-
versary. This proof allows Moat to declassify client’s output
to disk while proving property 11. Moat successfully proves
that the client enclave satisﬁes conﬁdentiality.

Notary Service.

We implement a notary service introduced by [21] but
adapted to run on SGX. The notary enclave assigns logi-
cal timestamps to documents, giving them a total ordering.
The notary enclave responds to (1) a connect message for
obtaining the attestation report, and (2) a notarize message
for obtaining a signature over the document hash and the
current counter.

user → notary : connect | N
notary → user : Quotenotary(N )
user → notary : notarize | H(text)
notary → user : counter | H(text) |

SigK

−1
notary

(counter | H(text))

The only secret here is the private signature key K

−1
notary.
notary is not leaked to H.
−1
First, we use Moat to prove that K
This proof fails because the output of Sig (in the response
to notarize message) depends on the secret signature key —
Moat is unaware of cryptographic properties of Sig. ProVerif
−1
proves that this message does not leak K
notary to a network
adversary, which allows Moat to declassify this message and
prove that the notary enclave satisﬁes conﬁdentiality.

End-to-End Encrypted Instant Messaging.

We implement the oﬀ-the-record messaging protocol [13],
which provides perfect forward secrecy and repudiability
for messages exchanged between principals A and B. We
adapt this protocol to run on SGX, thus providing an addi-
tional guarantee that an infrastructure attack cannot com-
promise the ephemeral Diﬃe-Hellman keys, which encrypt
and integrity-protect the messages between A and B. We
only present a synchronous form of communication here for
simplicity.

A → B : ga1 | SigK
B → A : gb1 | SigK
A → B : ga2 | EncH(ga1b1 )(m1) | MACH(H(ga1b1 ))(ga2 |

(ga1 ) | QuoteA(SigK
(gb1 ) | QuoteB(SigK

−1
A
−1
B

−1
A
−1
B

(ga1 ))

(gb1 ))

EncH(ga1 b1 )(m1))

B → A : gb2 | EncH(ga2b1 )(m2) | MACH(H(ga2b1 ))(gb2 |

EncH(ga2 b1 )(m2))

A → B : ga3 | EncH(ga2b2 )(m3) | MACH(H(ga2b2 ))(ga3 |

EncH(ga2 b2 )(m3))

The OTR protocol only needs a digital signature on the

initial Diﬃe-Hellman exchange — future exchanges use MACs
to authenticate a new key using an older, known-authentic
key. For the same reason, we only append a SGX quote to
the initial key exchange. First, we use Moat to prove that the
Diﬃe-Hellman secrets computed by penc (i.e. ga1b1 , ga2b1 ,
ga2b2 ) are not leaked to H. Next, ProVerif uses this secrey
assumption to prove that messages m1, m2, and m3 are not

1182leaked to the network adversary. The ProVerif proofs al-
lows Moat to declassify all messages following the initial key
exchange, and successfully prove conﬁdentiality.

Query Processing over Encrypted Database.

In this case study, we evaluate Moat on a stand-alone ap-
plication, removing the possibility of protocol attacks and
therefore the need for any protocol veriﬁcation. We build a
database table containing two columns: name which is deter-
ministically encrypted, and amount which is nondeterminis-
tically encrypted. Alice wishes to select all rows with name
“Alice” and sum all the amounts. We partition this com-
putation into two parts: unprivileged computation (which
selects the rows) and enclave computation (which computes
the sum).

Benchmark

x86+SGX
instructions

BoogiePL
statements

OTP
Notary
OTR IM

Query

188
147
251
575

1774
1222
2191
4727

Moat
proof
9.9 sec
3.2 sec
7.8 sec
55 sec

Policy

Annotations

4
2
7
9

Summary of experimental

Figure 9:
results.
Columns are (1) instructions analyzed by Moat not
including crypto library, (2) size of I(penc−H), (3)
proof time, (4) number of secret and declassifed an-
notations

9. RELATED WORK
Our work relates three somewhat distinct areas in security.
Secure Systems on Trusted Hardware. In recent years,
there has been growing interest in building secure systems
on top of trusted hardware. Sancus [29] is a security archi-
tecture for networked embedded devices that seeks to pro-
vide security guarantees without trusting any infrastructural
software, only relying on trusted hardware. Intel SGX [23]
seeks to provide similar guarantees via extension to the x86
instruction set. There are some recent eﬀorts on using SGX
for trusted computation. Haven [10] is a system that exploits
Intel SGX for shielded execution of unmodiﬁed Windows ap-
plications. It links the application together with a runtime
library OS that implements the Windows 8 API. However, it
does not provide any conﬁdentiality or integrity guarantees,
and includes a signiﬁcant TCB. VC3 [33] uses SGX to run
map-reduce computations while protecting data and code
from an active adversary. However, VC3’s conﬁdentiality
guarantee is based on the assumption that enclave code does
not leak secrets, and we can use Moat to verify this assump-
tion. Santos et al. [32] seek to build a trusted language run-
time for mobile applications based on ARM TrustZone [4].
These design eﬀorts have thrown up very interesting associ-
ated veriﬁcation questions, and our paper seeks to address
these with a special focus on Intel SGX.
Verifying Information Flow on Programs. Checking
implementation code for safety is also a well studied prob-
lem. Type systems proposed by Sabelfeld et al. [31], Barthe
et al. [9], and Volpano et al. [34] enable the programmer to
annotate variables that hold secret values, and ensure that
these values do not leak. Balliu et al. [5] automate informa-
tion ﬂow analysis of ARMv7 machine code. Languages and

veriﬁcation techniques also exist for quantitative informa-
tion ﬂow (e.g., [22]). However, these works assume that the
infrastructure (OS/VMM, etc.) on which the code runs is
safe, which is unrealistic due to malware and other attacks.
Our approach builds upon this body of work, showing how
it can be adapted to the setting where programs run on
an adversarial OS/VMM, and instead rely on trusted SGX
hardware for information-ﬂow security.
Cryptographic Protocol Veriﬁcation. There is a vast
literature on cryptographic protocol veriﬁcation (e.g. [11,
12]). Our work builds on top of cryptographic protocol ver-
iﬁers showing how to use them to reason about protocol
attacks and to generate annotations for more precise ver-
iﬁcation of enclave programs.
In the future, it may also
be possible to connect our work to the work on correct-
by-construction generation of cryptographic protocol imple-
mentation [19].
10. CONCLUSION

This paper introduces a technique for verifying informa-
tion ﬂow properties of enclave programs. Moat is a ﬁrst step
towards building an end-to-end veriﬁer. Our current evalu-
ation uses separate models for Moat and ProVerif. In future,
we plan to design a high-level language from which we gen-
erate machine code, enclave model, and a protocol model.
11. ACKNOWLEDGMENTS

This research is supported in part by SRC contract 2460.001

and NSF STARSS grant 1528108. We gratefully acknowl-
edge Brent ByungHoon Kang and the anonymous reviewers
for their insightful feedback.
12. REFERENCES
[1] Available at http://www.cryptopp.com/.
[2] https://devmoat.github.io.
[3] P. Akritidis, C. Cadar, C. Raiciu, M. Costa, and

M. Castro. Preventing memory error exploits with wit.
In Proceedings of the 2008 IEEE Symposium on
Security and Privacy, SP ’08, pages 263–277,
Washington, DC, USA, 2008. IEEE Computer Society.
[4] ARM Security Technology - Building a Secure System

using TrustZone Technology. ARM Technical White
Paper.

[5] M. Balliu, M. Dam, and R. Guanciale. Automating

information ﬂow analysis of low level code. In
Proceedings of the 2014 ACM SIGSAC Conference on
Computer and Communications Security, CCS ’14,
pages 1080–1091, New York, NY, USA, 2014. ACM.
[6] M. Barnett, B. E. Chang, R. DeLine, B. Jacobs, and
K. R. M. Leino. Boogie: A modular reusable veriﬁer
for object-oriented programs. In FMCO ’05, LNCS
4111, pages 364–387, 2005.

[7] M. Barnett and K. R. M. Leino. Weakest-precondition

of unstructured programs. In PASTE ’05, pages
82–87, 2005.

[8] C. Barrett, R. Sebastiani, S. A. Seshia, and C. Tinelli.

Satisﬁability modulo theories. In A. Biere, H. van
Maaren, and T. Walsh, editors, Handbook of
Satisﬁability, volume 4, chapter 8. IOS Press, 2009.

[9] G. Barthe and L. P. Nieto. Secure information ﬂow for

a concurrent language with scheduling. In Journal of
Computer Security, pages 647–689. IOS Press, 2007.

1183[10] A. Baumann, M. Peinado, and G. Hunt. Shielding

[24] Intel Software Guard Extensions Programming

applications from an untrusted cloud with Haven. In
USENIX Symposium on Operating Systems Design
and Implementation (OSDI), 2014.

Reference. Available at https://software.intel.
com/sites/default/files/329298-001.pdf.
[25] G. Klein, K. Elphinstone, G. Heiser, J. Andronick,

[11] B. Blanchet. An Eﬃcient Cryptographic Protocol

Veriﬁer Based on Prolog Rules. In 14th IEEE
Computer Security Foundations Workshop, pages
82–96, Cape Breton, Canada, June 2001.

[12] B. Blanchet. A computationally sound automatic

prover for cryptographic protocols. In Workshop on
the link between formal and computational models,
Paris, France, June 2005.

[13] N. Borisov, I. Goldberg, and E. Brewer. Oﬀ-the-record
communication, or, why not to use pgp. In Proceedings
of the 2004 ACM Workshop on Privacy in the
Electronic Society, WPES ’04, pages 77–84, New York,
NY, USA, 2004. ACM.

[14] D. Brumley, I. Jager, T. Avgerinos, and E. J.

Schwartz. BAP: A binary analysis platform. In
Proceedings of the 23rd International Conference on
Computer Aided Veriﬁcation, CAV’11, pages 463–469,
2011.

[15] R. E. Bryant, S. K. Lahiri, and S. A. Seshia. Modeling

and Verifying Systems using a Logic of Counter
Arithmetic with Lambda Expressions and
Uninterpreted Functions. In Computer-Aided
Veriﬁcation (CAV’02), LNCS 2404, pages 78–92, July
2002.

[16] M. R. Clarkson and F. B. Schneider. Hyperproperties.
Journal of Computer Security, 18(6):1157–1210, Sept.
2010.

[17] L. de Moura and N. Bjørner. Z3: An eﬃcient SMT

solver. In TACAS ’08, pages 337–340, 2008.

[18] Z. Durumeric, J. Kasten, D. Adrian, J. A. Halderman,

M. Bailey, F. Li, N. Weaver, J. Amann, J. Beekman,
M. Payer, and V. Paxson. The matter of heartbleed.
In Proceedings of the 2014 Conference on Internet
Measurement Conference, pages 475–488, 2014.

[19] C. Fournet and T. Rezk. Cryptographically sound

implementations for typed information-ﬂow security.
In Proceedings 35th Symposium on Principles of
Programming Languages. G. Smith, 2008.

[20] V. Ganapathy, S. A. Seshia, S. Jha, T. W. Reps, and

R. E. Bryant. Automatic discovery of API-level
exploits. In Proceedings of the 27th International
Conference on Software Engineering (ICSE), pages
312–321, May 2005.

[21] C. Hawblitzel, J. Howell, J. R. Lorch, A. Narayan,

B. Parno, D. Zhang, and B. Zill. Ironclad apps:
end-to-end security via automated full-system
veriﬁcation. In Proceedings of the 11th USENIX
conference on Operating Systems Design and
Implementation, pages 165–181, 2014.

[22] J. Heusser and P. Malacaria. Quantifying information

leaks in software. In Proceedings of the 26th Annual
Computer Security Applications Conference, pages
261–269. ACM, 2010.

[23] M. Hoekstra, R. Lal, P. Pappachan, C. Rozas,

V. Phegade, and J. Cuvillo. Using innovative
instructions to create trustworthy software solutions.
In Workshop on Hardware and Architectural Support
for Security and Privacy, 2013.

D. Cock, P. Derrin, D. Elkaduwe, K. Engelhardt,
R. Kolanski, M. Norrish, T. Sewell, H. Tuch, and
S. Winwood. sel4: Formal veriﬁcation of an os kernel.
In Proceedings of the ACM SIGOPS 22nd Symposium
on Operating Systems Principles, SOSP ’09, pages
207–220, New York, USA, 2009.

[26] F. McKeen, I. Alexandrovich, A. Berenzon, C. V.

Rozas, H. Shaﬁ, V. Shanbhogue, and U. R.
Savagaonkar. Innovative instructions and software
model for isolated execution. In Proceedings of the 2nd
International Workshop on Hardware and
Architectural Support for Security and Privacy, HASP
’13, pages 10:1–10:1, New York, NY, USA, 2013.
ACM.

[27] J. Mclean. Proving noninterference and functional

correctness using traces. Journal of Computer
Security, 1:37–58, 1992.

[28] A. C. Myers and B. Liskov. A decentralized model for

information ﬂow control. In Proceedings of the 16th
ACM Symposium on Operating Systems Principles,
SOSP ’97, pages 129–142, New York, USA, 1997.

[29] J. Noorman, P. Agten, W. Daniels, R. Strackx,

A. Van Herrewege, C. Huygens, B. Preneel,
I. Verbauwhede, and F. Piessens. Sancus: Low-cost
trustworthy extensible networked devices with a
zero-software trusted computing base. In Proceedings
of the 22nd USENIX Conference on Security, pages
479–494, 2013.

[30] A. Sabelfeld and A. C. Myers. Language-based

information-ﬂow security. Selected Areas in
Communications, IEEE Journal on, 21(1):5–19, 2003.
[31] A. Sabelfeld and A. C. Myers. A model for delimited
information release. In In Proc. International Symp.
on Software Security, pages 174–191. Springer-Verlag,
2004.

[32] N. Santos, H. Raj, S. Saroiu, and A. Wolman. Using
ARM TrustZone to build a trusted language runtime
for mobile applications. In Proceedings of the 19th
international conference on Architectural support for
programming languages and operating systems
(ASPLOS), pages 67–80. ACM, 2014.

[33] F. Schuster, M. Costa, C. Fournet, C. Gkantsidis,
M. Peinado, G. Mainar-Ruiz, and M. Russinovich.
VC3: trustworthy data analytics in the cloud using
SGX. In 2015 IEEE Symposium on Security and
Privacy, SP 2015, San Jose, CA, USA, May 17-21,
2015, pages 38–54, 2015.

[34] D. Volpano, C. Irvine, and G. Smith. A sound type

system for secure ﬂow analysis. Journal of Computer
Security, 4(2-3):167–187, Jan. 1996.

[35] J. Yang and C. Hawblitzel. Safe to the last

instruction: Automated veriﬁcation of a type-safe
operating system. In Proceedings of the 31st
Conference on Programming Language Design and
Implementation, pages 99–110, 2010.

1184