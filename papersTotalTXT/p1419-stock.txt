From Facepalm to Brain Bender:

Exploring Client-Side Cross-Site Scripting

Ben Stock

FAU Erlangen-Nuremberg

ben.stock@fau.de

Stephan Pﬁstner

SAP SE

stephan.pﬁstner@sap.com

Bernd Kaiser

FAU Erlangen-Nuremberg

bk@dfjk.eu

Sebastian Lekies

Ruhr-University Bochum

paper@sebastian-lekies.de

Martin Johns

SAP SE

martin.johns@sap.com

ABSTRACT
Although studies have shown that at least one in ten Web
pages contains a client-side XSS vulnerability, the prevalent
causes for this class of Cross-Site Scripting have not been
studied in depth. Therefore, in this paper, we present a
large-scale study to gain insight into these causes. To this
end, we analyze a set of 1,273 real-world vulnerabilities con-
tained on the Alexa Top 10k domains using a speciﬁcally
designed architecture, consisting of an infrastructure which
allows us to persist and replay vulnerabilities to ensure a
sound analysis. In combination with a taint-aware browsing
engine, we can therefore collect important execution trace
information for all ﬂaws.

Based on the observable characteristics of the vulnerable
JavaScript, we derive a set of metrics to measure the com-
plexity of each ﬂaw. We subsequently classify all vulnerabil-
ities in our data set accordingly to enable a more systematic
analysis. In doing so, we ﬁnd that although a large portion
of all vulnerabilities have a low complexity rating, several
incur a signiﬁcant level of complexity and are repeatedly
caused by vulnerable third-party scripts.
In addition, we
gain insights into other factors related to the existence of
client-side XSS ﬂaws, such as missing knowledge of browser-
provided APIs, and ﬁnd that the root causes for Client-Side
Cross-Site Scripting range from unaware developers to in-
compatible ﬁrst- and third-party code.

Categories and Subject Descriptors
H.4.3 [Communications Applications]: Information
browsers; C.2.0 [General]: Security and protection

Keywords
Client-Side XSS; Analysis; Complexity Metrics

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
CCS’15, October 12–16, 2015, Denver, Colorado, USA.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3832-5/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2810103.2813625.

1.

INTRODUCTION

In recent years, Web applications showed a notable move
of functionality from the server side towards the client side,
where the functionality is realized by the Web browser’s
JavaScript engine. This trend was caused by various fac-
tors, such as the addition of new capabilities in the form
of powerful HTML5 APIs, the signiﬁcantly increased per-
formance of modern JavaScript engines, and the availability
of convenient mature JavaScript programming libraries like
jQuery. With the growing amount of JavaScript code exe-
cuted in the Web browser, an increase in purely client-side
vulnerabilities can be observed, with the class of Client-Side
Cross-site Scripting (also known as DOM-based XSS [11])
being among the most sever security issues. Our previous
work demonstrated that about 10% of the Alexa Top 10k
carry at least one exploitable client-side XSS vulnerability,
with current ﬁltering approaches being unable to stop such
exploits [27]. Like most security vulnerabilities, client-side
XSS is caused by insecure coding. The underlying reasons
for that insecure code, however, are mostly unexplored.

Are developers simply overwhelmed by the complexity of
the client-side code? The increase in functionality on the
client side naturally increases the amount of code that is ex-
ecuted in a Web page. This also holds true for code which
causes the client-side XSS ﬂaws and in turn leads to an in-
crease in the complexity of a developer’s task to spot vul-
nerable data ﬂows.

Alternatively, is the missing security awareness of Web
programmers the dominant causing factor? The DOM and
the current JavaScript engines oﬀer many diﬀerent methods
to turn arbitrary strings into executable code. Therefore,
the use of insecure APIs seems natural to an average devel-
oper for solving common problems like the interaction with
the page (e.g. using innerHTML) or parsing JSON (e.g. us-
ing eval). Although secure APIs exist in modern browsers,
these insecure ways of dealing with such problems are often
much faster and more convenient, albeit potentially making
a site susceptible to a client-side XSS ﬂaw.

Or is the rise in client-side vulnerabilities rooted in the
Web’s unique programming model? Compared to classic ap-
plication platforms and programming languages, the Web
model oﬀers numerous open and hidden complexities: The
combination of the browser’s Document Object Model API
(or DOM API ), the highly dynamic nature of JavaScript,
and the process in which Web content is assembled on the ﬂy
within the browser, frequently leads to non-obvious control
and data ﬂows that can potentially cause security problems.

1419In this paper, we systematically examine this problem
space. To do so, we investigate a large set of real-world
zero-day vulnerabilities to gain insights into the complexity
of client-side XSS vulnerabilities. Our goal is to determine
if such vulnerabilities are easy to spot by security-conscious
developers or whether the complex nature of the Web plat-
form hinders detection even for security professionals.

This paper makes the following contributions:
• We examine client-side XSS vulnerabilities and iden-
tify characteristic features of complex JavaScript code.
Based on these, we deﬁne a set of metrics to measure
that complexity (Section 3).
• We implement and present an infrastructure that al-
lows us to persist vulnerable code, and enables us to
gather all data needed to apply our metrics (Section 4).
• We report on a study of 1,273 real-world vulnerabili-
ties, derive a set of boundaries for our metrics and use
these to assign a complexity score to all of the vulner-
abilities (Section 5).
• We discuss additional insights gathered from our data,
showing that causes for client-side XSS are manifold
(Section 6).

2. CLIENT-SIDE XSS

Cross-Site Scripting (also called XSS ) is a class of code-
injection vulnerability in the browser. Web applications run
in a protected environment, such that only code from the
same origin as the application can interact with it. There-
fore, the goal of an XSS attacker is to execute arbitrary
JavaScript code in the browser of his victim, in the context
(or origin) of the vulnerable Web page. If successful, this
allows him to conduct any action in the name of the victim-
ized user, e.g., using the application as the user or retrieving
secret information such as cookies.

While server-side XSS attacks have been known for a num-
ber of years, the youngest representative, i.e., client-side
XSS, was ﬁrst discussed in 2005 by Amit Klein [11]. Rather
than being caused by vulnerable server-side code, this class
of vulnerability occurs if user-provided input is insecurely
processed on the client side, e.g., by using this data for a
call to document.write, as shown in Listing 1 using the lo-
cation, which returns the current URL.

From a conceptual standpoint, XSS is caused when an un-
ﬁltered data ﬂow occurs from an attacker-controlled source
to a security-sensitive sink. In the concrete case of client-side
XSS, such a source can be, e.g., the URL, whereas an exam-
ple for a sink is eval or document.write. Both these APIs
accept strings as parameters, which are subsequently parsed
and executed as code (JavaScript and HTML, respectively).
Therefore, passing attacker-controllable input to such func-
tions eventually leads to execution of the attacker-provided
code. There are additional sinks, which do not allow for di-
rect code execution (such as cookies or Web Storage). These
sinks, however, are not in the focus of our work.

3. CHARACTERIZING CLIENT-SIDE XSS
The goal of our work is to gather insights into the root
causes of client-side XSS ﬂaws on the Web, ﬁrst and foremost
to answer the question whether the complexity of client-side
code is the primary causing factor. In order to spot a vulner-
ability, an analyst has to follow the data ﬂow from source to
sink and fully understand all operations that are performed

on the data, with several properties increasing the diﬃculty
of this task. In the following, we discuss several measurable
properties of the perceived analysis complexity and present
metrics to measure them accordingly. In addition to these,
vulnerabilities have characteristics which do not necessarily
have an impact on the complexity of a ﬂaw, but provide
interesting additional insights. Therefore, after presenting
the measurable properties and matching metrics, we discuss
these additional characteristics.
3.1 Measurable Properties of JS Complexity
In this section, we outline properties which we deem use-
ful in determining how hard it is for an analyst to spot a
vulnerable ﬂow of data in JavaScript code. For each of the
properties, we then deﬁne a metric that measures the per-
ceived complexity.
3.1.1 Number of Operations on the Tainted Data
In our notion, data that originates from a user-controllable
source is considered tainted. A client-side XSS vulnerabil-
ity constitutes a ﬂow of such tainted data from a source to
a security-critical sink. An analyst therefore has to decide
whether the user-provided data is ﬁltered or encoded prop-
erly and, thus, must understand all operations conducted on
that data. Thus, one necessary consideration to make is that
each operation naturally increases the perceived complexity
as more code has to analyzed and understood. Such oper-
ations can either be read accesses, e.g., regular expression
checking, or write accesses to the data, such as concat.

Therefore, we deﬁne our ﬁrst metric, M1, to measure the
number of string-accessing operations which are conducted
in a read or write manner throughout the ﬂow of attacker-
controllable data from source to sink, including source and
sink access. In order to not over-approximate this number,
we join the same type of operations on a per-line level, e.g.,
several concatenating operations on the same line are treated
as a single operation. An example of such a case is shown
in Listing 1, where two concatenation operations occur in
one line. Therefore, our metric M1 measures this snippet to
have three operations, i.e., source access, concatenation and
sink access. In real-world vulnerabilities, the string which
eventually ends in a sink may consist of substrings that orig-
inated from diﬀerent sources. Therefore, we use M1 to mea-
sure the longest sequence of operations between any source
access and the ﬁnal sink access.

// actual code
document.write("<a href=’ " + location + "’>this page</a>")
// as interpreted by our metric
var x = location; // source access
var y = "<a href=’" + x + "’>this page</a>"; // joined concats
document.write(y); // sink access

Listing 1: Vulnerability Example

3.1.2 Number of Involved Functions
JavaScript, not unlike any other programming language,
employs the concept of functions, which can be used to split
up functionality into smaller units. While this is best prac-
tice in software engineering, it increases the diﬃculty a secu-
rity auditor has to overcome as he has to understand speciﬁ-
cally what each of these units does. Therefore, in addition to
the number of operations which are conducted on a tainted

1420string, the number of functions that are traversed is another
indicator for the complexity of a ﬂow or a vulnerability.

We therefore deﬁne our second metric, M2, to count the
number of functions which are passed between source and
sink access. Although all code may also reside in the top
execution level, i.e., not speciﬁcally within deﬁned functions
but rather in a virtual main function, the minimum number
of traversed functions must always be one. In this context,
we deﬁne the virtual main to be the main JavaScript ex-
ecution thread which executes all inline script blocks and
external JavaScript ﬁles.

3.1.3 Number of Involved Contexts
The Web’s model allows for JavaScript ﬁles to be included
from other sites, while inheriting the including page’s origin
and, thus, running in that origin. Therefore, a single Java-
Script block or ﬁle is not executed independently, but within
a Web page possible containing tens of other JavaScript re-
sources, potentially stemming from other domains and de-
velopers. The main interaction point of all these scripts is
the global object, in the case of the browser the window ob-
ject. Since all script elements within a Web page may gain
access to that object, diﬀerent snippets may register global
variable or functions to allow for interaction with other parts
of the executed code.

In our notion, we call each of these script elements, which
the analyst has to fully understand to decide whether a ﬂow
might be vulnerable, contexts. Thus, we deﬁne our third
metric, M3, to count the number of contexts which are tra-
versed in the execution of a vulnerable JavaScript program
between source and sink.

3.1.4 Code Locality of Source and Sink
In order to understand that a certain ﬂow constitutes a
vulnerability, an analyst has to inspect all the code between
the source and respective sink access. Naturally, this is eas-
ier if not only the number of operations that are conducted
is low, but also both source- and sink-accessing operations
are within a smaller number of lines of code. In contrast,
even if the number of operations is low, a vulnerability is
harder to detect if source and sink access are further apart
in the code, as the analyst has to speciﬁcally search for the
tainted variables to be used again.

Thus, as a fourth metric, M4, we measure the amount
of code between source and sink access. This metric, how-
ever, can only be applied to vulnerabilities ﬂow which the
source and sink access is conducted within the same ﬁle, i.e.,
either within two inline script blocks or the same external
JavaScript ﬁle.

3.1.5 Callstack Relation between Source and Sink
Another property that increases the perceived complexity
when dealing with potentially vulnerable JavaScript code
is the relation between source and sink access in the call
stack. As discussed before, the code responsible for an ex-
ploitable ﬂaw might be spread across multiple functions or
contexts, i.e., source and sink access do not have to be con-
tained within the same function or context.

In the easiest case, access to the source and sink is con-
ducted within the same function, i.e., on the same level in
the call stack. Figure 1 shows the diﬀerent relations between
these two operations with respect to the sink access being

conducted in the red script element SE #3. For our ﬁrst
relation, R1, the source access also occurs in this element.

The second scenario occurs when the source access is con-
ducted in the blue script element SE #1. In this case, the
tainted data is passed as a parameter to the function the
sink access resides in. From an analyst’s perspective, this
means that he can follow the ﬂow of data from the source to
the function. He can subsequently analyze the function with
the knowledge that the passed parameter contains tainted
and unﬁltered data, allowing him to decide whether the fol-
lowing sink access can be deemed safe or not. We refer to
this callstack relation as R2.

In contrast to the previous case, the source access may
also occur in an element which is lower in the callstack than
the sink access. This is depicted in Figure 1 when the source
access occurs in the yellow SE #4. In such a case, the ana-
lyst has to follow the called function to determine that user-
provided data is accessed before having to go up to SE #3
again to see how the tainted data is handled. This compli-
cates the analysis, since it requires a switch back and forth
between functions, and potentially contexts/ﬁles. We deem
this to be R3.

As a fourth scenario, we identify snippets of code in which
source and sink only share a common ancestor in the call-
stack, but neither are parents of the other in the callstack.
Figure 1 shows this for a source access in the orange ele-
ment SE #2, which shares the common parent SE #1 with
the sink-accessing SE #3. The increased complexity in this
scenario stems from the fact that the analyst must ﬁrst in-
vestigate the function which accesses the source, continue
his analysis of the common parent, and then decent into the
sink-accessing function. We denote this relation to be R4.

Finally, the most complex type of ﬂows occurs if source
and sink access share no common ancestors apart from the
virtual main function. This is enabled by the fact that all
JavaScript operates on the same global object, i.e., may set
global variables accessible by any other snippet of JS running
on the same Web page. This is shown in Figure 1 when
accessing the source in the purple SE #0. Hence, there is no
path of code an analyst can follow between source and sink
access. Instead, he has to understand that the ﬁrst script
element accesses the source and stores the retrieved value
in a global variable, and that in turn the second element
accesses this stored value before passing it to the sink. In
our notion, this is callstack relation R5.

Figure 1: Relations between source and sink access

14213.2 Additional Characteristics

In addition to the previously outlined properties which can
be measured to either result in a numerical value or a speciﬁc
relation between source and sink access, we ﬁnd that more
characteristics can be observed for vulnerable code. In this
section, we shed light on these and discuss their implications.

Non-Linear Flows: An additional important observable
characteristic of Client-Side Cross-Site Scripting is a classiﬁ-
cation of the vulnerability’s data and control ﬂows in respect
to their linearity:

In the context of this paper, we consider a data ﬂow to
be linear (LDF), if on the way from the source to the sink,
the tainted value is always passed to all involved functions
directly, i.e., in the form of a function parameter. In conse-
quence, a non-linear data ﬂow (NLDF) includes at least one
instance of transporting the tainted value implicitly, e.g.,
via a global variable or inside a container object. Manual
identiﬁcation of vulnerable data ﬂows in case of NLDFs is
signiﬁcantly harder, as no obvious relationship between the
tainted data and at least one of the ﬂow’s functions exist.

Furthermore, non-linear control ﬂows (NLCF) are instances
of interrupted JavaScript execution: A ﬁrst JavaScript exe-
cution thread accesses the tainted data source and stores it
in a semi-persistent location, such as a closure, event han-
dler or global variable, and later on a second JavaScript
thread uses the data in a sink access. Instances of NLCFs
can occur if the ﬂow’s code is distributed over several code
contexts, e.g., an inline and an external script, or in case
of asynchronous handling of events. Similar to NLDF, the
inspection of such ﬂows is signiﬁcantly more diﬃcult.

Code Origin: The previously outlined model of JavaScript
allows for externally hosted code to be executed in the con-
text of the including application. This implies that a vulner-
ability in included third-party code results in a vulnerability
in the including application and, in addition, an analyst may
need to follow the data ﬂow through code from diﬀerent au-
thors and origins. This switch between contexts is already
covered by M3 and, thus, we do not consider the code ori-
gin to be an additional indicator for the complexity of the
analyst’s task. It is, however, interesting to determine what
code caused the actual ﬂaw.

In terms of origin of the code involved in a ﬂow, we dif-
ferentiate between three cases: self-hosted by the Web page,
code which is only hosted on third-party pages, and a mixed
variant of the previous, where the ﬂow traverses both self-
hosted and third-party code. To distinguish the involved
domains, we use Alexa to match subdomains and Content
Delivery Networks (CDNs) to their parent domain. Thus,
code that is hosted on the CDN of a given site is treated as
self-hosted.

Multiﬂows: A single sink access may contain more than
one piece of user-provided data. This leaves an attacker with
a means of splitting up his malicious payload to avoid detec-
tion. As we [27] have shown, given the right circumstances,
such ﬂows can be used to bypass existing ﬁlter solutions such
as Chrome’s XSSAuditor [1].

Sinks: Another characteristic which is not directly related
to the complexity of a given ﬂow is the sink involved in the
ﬂaw. While a larger number of sinks exist, our data set
(which we present in Section 5.1) consists only of ﬂaws that
target innerHTML, document.write and eval. These also
contain conceptually similar sinks, such as outerHTML, doc-

ument.writeln and setTimeout, respectively. These sinks
diﬀer in terms of what kind of payload must be injected by
an attacker to successfully exploit a vulnerability.
Runtime-generated code: Through the use of the eval
function, JavaScript code can be dynamically created at run-
time and executed in the same context. While this enables
programmers to build ﬂexible Web applications, it also com-
plicates an analyst’s task of understanding a given piece of
code. However, the fact that a script uses eval to generate
code at runtime cannot be measured in a discrete manner,
thus we opt not to use it for complexity metric.
4.

INFRASTRUCTURE

The basis of our study is a data set of real-world vulnera-
bilities. While this enables us to investigate the complexities
of such ﬂaws at a large scale, it brings its own set of chal-
lenges we had to overcome. In this section, we discuss these
challenges and present the infrastructure we developed for
our experimentations.
4.1 Initial Data Set and Challenges

The basis of the results we present in this paper is a set
of exploits detected with the methodology we developed in
2013 [12]. In order to analyze this set of data in a sound and
reproducible way, we had to overcome several challenges.
First and foremost, interaction with live Web servers can
induce variance in the data as no two responses to the same
request are necessarily the same. Causes for such behavior
might reside in load balancing, third-party script rotation
or syntactically diﬀerent versions of semantically identical
code. Secondly, to gain insight into the actual vulnerabili-
ties, we needed to gather detailed information on data ﬂows,
such as all operations which were executed on said data.

Modern Web applications with complex client-side code
often utilize miniﬁcation to save bandwidth when delivering
JavaScript code to the clients. In this process, space is con-
served by removing white spaces as well as using identiﬁer
renaming. As an example, jQuery 2.1.3 can be delivered un-
compressed or miniﬁed, whereas the uncompressed version
is about three times as large as the miniﬁed variant. Our
analysis, however, requires a detailed mapping of vulnerabil-
ities to matching JavaScript code fragments, thus miniﬁed
code presents another obstacle to overcome.

Finally, in our notion, if access to a sink occurs in jQuery,
we assume that this is not actually a vulnerability of that
library, but rather insecure usage by the Web application’s
developer. Thus, to not create false positives when deter-
mining the cause of a vulnerability, we treat jQuery func-
tions as a direct sink and remove them from the trace infor-
mation we collect.
4.2 Persisting and Preparing Vulnerabilities
To allow for a reproducible vulnerability set, we needed
to implement a proxy capable of persisting the response to
all requests made by the browser when visiting a vulnerable
site. To achieve this, we built a proxy on top of mitm-
proxy [4], which provides two modes of operation. We ini-
tially set the mode to caching and crawled all exploits which
had previously triggered their payload and stored both re-
quest and response headers as well as the actual content. To
ensure for proper execution of all JavaScript and, thus, po-
tential additional requests to occur, we conducted the crawl
in a real browser rather than a headless engine. Also, this

1422allowed us to send an additional header from the browser
to the proxy, indicating what kind of resource was being re-
quested (e.g., HTML documents, JavaScript or images), as
content type detection is inherently unreliable [13].

As previously discussed, our analysis requires precise in-
formation on the statements that are executed.
In order
to ensure that a mapping between all operations which are
involved in the ﬂow of data and their corresponding source
line can be achieved, we need all JavaScript to be beautiﬁed.
Therefore, using the information provided by the browsing
engine regarding the requested type of content, we ﬁrst de-
termine the cached ﬁles which were referenced as external
JavaScript. We use the beautiﬁcation engine js-beautify to
ensure that the code is well-formatted and each line consists
only of a single JavaScript statement [6]. Subsequently, we
parse all HTML on disk, beautifying each script block con-
tained in the ﬁles and ﬁnally, save the ﬁles back to disk.

We now switch the operation mode to replay, i.e., all re-
sponses are served from disk and not from the original server.
To do so, the proxy simply queries its database for a match-
ing URL and returns the content from disk, while attaching
the response headers as originally retrieved from the remote
server. Some requests that are conducted at runtime by
JavaScript (such as jQuery XmlHttpRequest with the JSONP
option) carry a nonce in the URL to avoid a cached response
[28]. Therefore, if the proxy cannot ﬁnd the requested URL
in its database, it employs a fuzzy matching scheme which
uses normalized URLs to determine the correct ﬁle to re-
turn. Since our initial tests showed that nonces in all cases
consisted only of numbers, we normalize each URL by sim-
ply replacing each number in the URL with a ﬁxed value.
As we had a ground truth of vulnerable sites, we were able
to verify that our proxy would correctly replay all vulnera-
bilities. This might, however, not be true for random sites
and would then warrant further testing.

4.3 Taint-Aware Firefox Engine

Our analysis methodology relies on a dynamic analysis
of real-world vulnerable JavaScript. Naturally, this requires
the execution of the vulnerable code and the collection of
corresponding trace information. Although taint-aware en-
gines, such as DOMinator [7], exist, we opted to imple-
ment our own engine speciﬁcally tailored to ﬁt the needs
of our study. To ensure a ﬁne-grained analysis of vulnera-
ble ﬂows from sources to sinks, we patched the open-source
Web browser Firefox. The browser was enhanced to track
data originating from sources, across all processing steps in
the SpiderMonkey JavaScript engine as well as the Gecko
rendering engine, and into sinks.

Our previous work relied on numerical identiﬁers in shadow
data to mark a part of a string as originating from a spe-
ciﬁc user-provided source [12]. This, however, only allowed
for tracking of basic source and encoding information. Our
study aims at gathering additional insight into the inner
workings of data ﬂows, therefore a more elaborate approach
must be taken to ensure that all necessary information can
be stored. More precisely, for each vulnerable data ﬂow we
want to capture exact, step-by-step operations which im-
pacted the tainted data or provide additional knowledge
relevant to interpret the data ﬂow. This includes access
to source, calls to both built-in and user-deﬁned functions
which operate on a tainted string, as well as stack traces for
each operation to allow for execution context analysis.

Figure 2: In-Memory representation of taint information

To match these requirements and keep the performance
impact as low as possible, we designed a scheme for eﬃ-
ciently tracking data ﬂows while allowing to record all the
relevant ﬂow information and also enable propagation be-
tween strings. During execution, a tree-like memory struc-
ture is built consisting of nodes representing the operations,
which are annotated using the context information present
when executing the operation. Edges between the nodes
represent execution order and dependency: child operations
are executed after their parents and consume the output
of them. For each tainted substring, string objects contain
references to these nodes, thereby implicitly expressing the
complete history of the tainted slices. Following the ances-
tors of a taint node allows to recreate the chain of operations
that led to the current tainted string - beginning with the
source access. Derived strings copy the references and add
their taint information to the tree, keeping the existing in-
formation for the old strings intact.

Figure 2 shows a graphical representation of this mem-
ory model. The sink access is depicted at the lower part of
the ﬁgure; the shown string was written to the sink docu-
ment.write. To reconstruct all operations which eventually
led to the tainted string being written to the document, we
traverse upwards, detecting that the ﬁnal string was the re-
sult of a concatenation of an untainted part (depicted in
normal font weight) with the the output of another concate-
nation operation (1). This operation put together two addi-
tional strings, consisting of an untainted part and a tainted
string (shown in bold, (2)). In order to extract the source
of said tainted data, we traverse up the tree once more to
ﬁnd that it originated from location.href (3). Similarly,
the data structure can be traversed for sink accesses with
multiple pieces of tainted data.

As our study is based on data derived from previous work
which used a patched variant of Chromium to detect and
exploit ﬂows, we expect these vulnerabilities to be at least
partially dependent on Chromium’s encoding behavior with
respect to the URL, and more important, the URL fragment,
which is — in contrast to Firefox — not automatically en-
coded. Therefore, we opted to align our browsing engine’s
encoding behavior with that of Chromium, i.e., by not au-
tomatically encoding the fragment.
4.4 Post-Processing

Before the raw data gathered by our engine can be ana-
lyzed, it needs to processed to ensure a correct analysis. In
the following, we illustrate these post-processing steps.

Identifying Vulnerable Flows: Only a fraction of all
ﬂows which occur during the execution of a Web application

1423are vulnerable. Our browsing engine collects all ﬂows which
occur and sends them to our backend for storage. Thus, be-
fore an analysis can be conducted, we need to identify the
actual vulnerable ﬂows. We therefore discard all ﬂows which
do not end in document.write, innerHTML or eval. Next,
we determine if our payload was completely contained in the
string which was passed to the sink, i.e., we ensure that the
data only contains actual exploitable ﬂows.
jQuery Detection and Removal: One of the most com-
monly used libraries in many Web applications is jQuery [2,
29]. It provides programmers with easy access to function-
ality in the DOM, such as a wrapper to innerHTML of any
element. When analyzing the reports gathered from our
taint-aware browsing engine, the calls to such wrapper func-
tions increase the number of functions traversed by a ﬂow,
increasing the perceived complexity. This, however, is not
true since the vulnerability was introduced by using jQuery
in an insecure manner, not jQuery itself. Therefore, we se-
lect to ﬁlter jQuery functions and use them as sinks, i.e.,
the html and append functions of jQuery are treated like an
assignment to an element’s innerHTML property.

We therefore implemented a detection mechanism to pin-
point which functions were provided by jQuery. The ﬂowchart
in Figure 3 shows this process.
Initially, we iterate over
all entries in the stack traces collected by our taint-aware
browser and determine the ﬁle in which each line is con-
tained. We then check the hash sum of that ﬁle against
known jQuery variants (1).
If no hash match is found,
we utilize the methodology used by Retire.js [18] to detect
whether jQuery is contained in that ﬁle at all (2). If this
step indicates that the ﬁle contains jQuery, we proceed to
gathering script statistics (such as the number of strings,
identiﬁers and functions) and comparing them to known ver-
sions of jQuery, to assess whether the script solely consists of
jQuery (3). If this does not produce a conclusive match, we
resort to a full-text search of the line of code in a database
of all lines of all known versions of jQuery (4). If no match
is found, we mark the generated report for later manual
analysis. This happens when Web site owners use custom
packers, rendering full-text search infeasible. If any of these
checks indicate that the analyzed stack entry points to code
located within jQuery, we remove said entry from our trace
both at the bottom and the top of the stack. This allows to
remove artefacts from sink-like operations such as html and
jQuery-enabled uses of events.
Stack Gap Detection and Removal: After the jQuery
detection ﬁnishes, we proceed to the next phase of post-
processing. Our taint-aware engine is able to record all calls
to functions a tainted string is passed to.
In some cases,
however, an indirect data ﬂow occurs, i.e., the string is not
passed to a function but rather written to a global variable.
If another function then accesses the tainted data, the ac-
cessing operation is logged; nevertheless, the actual call to
said function is missing, although an analyst has to follow
this call in his analysis. To allow for a correct value for met-
ric M2, we close this stack gap by inserting virtual function
calls into the trace.
Operation Fusing: As discussed in Section 3.1, M1 mea-
sures the number of operations which were conducted on
tainted data. In terms of understanding a vulnerability, sev-
eral concatenations on a single line do not complicate the

Figure 3: Flowchart for jQuery detection process

analysis; therefore, all consecutive concat operations on the
same source code line are fused to form a single operation.
4.5 Overall Infrastructure

An overview of our complete infrastructure is depicted in
Figure 4. Initially, all responses to requests, which were con-
ducted when verifying the exploits, are stored into a cache
database (1). Afterwards, we beautify all HTML and Java-
Script ﬁles and store them alongside the cache (2). In the
next step, our taint-aware Firefox browsing engine visits the
cached entries, while the proxy serves beautiﬁed version of
HTML and JavaScript ﬁles as well as unmodiﬁed versions of
all other ﬁles, such as images (3). The engine then collects
trace information on all data ﬂows that occur during the
execution of the page and sends it to a central backend for
storage (4). After the data for all URLs under investigation
has been collected, the data is post-processed (5) and can
then be analyzed (6).

5. EMPIRICAL STUDY

In this section, we outline the execution of our study as
well as the results. We then present classiﬁcation boundaries
for the metrics discussed in Section 3.1 and describe the
results of that classiﬁcation.
5.1 Data Set and Study Execution

After having an infrastructure that allowed for persisting
and thus consistently reproducing vulnerabilities, we took
a set of known vulnerabilities derived by our methodology
presented in 2013 [12]. In total, this set consisted of 1,146
URLs in the Alexa Top 10k which contained at least one
veriﬁed vulnerability, i.e., a crafted URL which would trig-
ger our JavaScript to execute in the vulnerable document.
After persisting and beautifying the vulnerable code, we
crawled the URLs with our taint-enhanced Firefox, collect-
ing a total of 3,080 distinct trace reports, whereas a trace
report corresponds to one access to sink, potentially con-
sisting of more than one ﬂow. Out of these reports, only
1,273 could be attributed to actual vulnerabilities; the rest
of the sink accesses either occurred with sanitized data or
involved sinks which are not directly exploitable (such as
document.cookie).

As discussed in Section 3.1, a single sink access may use
data from several sources. In total, we found that the 1,273

Hash MatchStartScript jQuery onlyRetire.jsNoDetermine Script StatsCompare StatsFull-text MatchNo DiffYesjQuery LineYesYesManual Analysis1234DiffNo jQueryNoNo1424Figure 4: Overview of our analysis infrastructure

traces our engine gathered consisted of 2,128 pieces of data,
whereas the maximum number of involved sources was 35.
Note, that in this case data from a single source was used
multiple times.
5.2 Result Overview

In this section, we present an overview of the results from
our study. The presented data in then analyzed in further
detail in Section 5.4.
M1 Number of string-accessing operations: Figure 5
shows the distribution of the number of string-accessing op-
erations in relation to the number of vulnerabilities we en-
countered. Out of the 1,273 vulnerable ﬂows in our data
set, we ﬁnd that 1,040 only have less than 10 operations
(including source and sink access). The longest sequence of
operations had a total length of 291, consisting mostly of
regular expression checks for speciﬁc values.
M2 Number of involved functions: Apart from the num-
ber of contexts, we also studied the number of functions that
were involved in a particular ﬂow. We found that 579 ﬂows
only traversed a single function, i.e., no function was called
between source and sink access. In total, 1,117 ﬂows crossed
ﬁve or less functions. In the most complex case, 31 functions
were involved in operations that either accessed or modiﬁed
the data. The distribution is shown in Figure 6.
M3 Number of involved contexts: Out of the 1,273 vul-
nerabilities we analyzed, the exploitable ﬂow was contained
within one context in 782 cases, and 25 ﬂows traversed more
than three contexts (with the maximum number of contexts
being six). Figure 7 shows this, highlighting that more than
90% of the ﬂaws were spread across one or two contexts.
M4 Locality of source and sink access: For all vulner-
abilities which were contained either in one single external
ﬁle or inside the same HTML document, we determined the
lines of code between the two operations. Out of the 1,150
reports that matched this criterion, the contained vulnera-
bility was located on a single line in 349 cases, and within ten
lines of code 694 times. In contrast, 40 vulnerabilities had
a distance of over 500 lines of code between source and sink

Figure 6: Histogram for M2 (number of functions)

Figure 7: Histogram for M3 (number of contexts)

access. In the most complex case, the access to the source
occurred 6,831 lines before the sink access. The distribution
of this metric is shown in Figure 8.

M5 Relation between source and sink: In our analysis,
we found that the most common scenario was R1, which
applied to a total of 914 ﬂows. Second to this, in 180 cases,
the source was an ancestor of the sink ( R2), whereas this
relation was reversed 71 times (R3). Source and sink shared
a common ancestor in 49 ﬂows (R4) and ﬁnally, there was
no relation between source and sink in 59 traces (R5).
5.3 Additional Characteristics

In addition to the proposed metrics, several additional
characteristics can be observed for our data set of vulnera-
bilities, which we outline in the following.

Figure 5: Histogram for M1 (string-accessing ops)

Figure 8: Histogram for M4 (Code locality)

CacheBeautifiedProxyPost-processingAnalysis3124560510152025+050100150200250300350051015+0100200300400500600012345+02004006008000100200300400500+01002003004005006007001425As we discussed beforehand, both code and data ﬂows may
occur in a non-linear manner. Table 1 shows the distribu-
tion of this feature in our data set. Note, that a linear data
ﬂow cannot occur with a non-linear control ﬂow, since this
implies no relation between source and sink accessing opera-
tions. We observe that 59 cases, which are also matched by
R5, both data and control ﬂow are non-linear. In addition,
we found that in 98 of the vulnerable ﬂows, a non-linear data
ﬂow occured, i.e., the data was not passed as a parameter
to all functions it traversed.

In terms of code origin, our analysis revealed interesting
results. While the biggest fraction, namely 835 vulnerabil-
ities, was caused purely by self-hosted code, we found that
273 ﬂaws were contained exclusively in third-party scripts,
leaving the Web page exposed to a Cross-Site Scripting ﬂaw
to no fault of its developer. The remaining 165 ﬂaws oc-
curred in a combination of self-hosted and third-party code.
An attacker may leverage a single sink access which con-
tains more than one attacker-controllable piece of data to
circumvent popular Cross-Site Scripting ﬁlters [27]. There-
fore, an additional characteristic is whether a ﬂaw consti-
tutes a multiﬂow, which we discovered in 344 of the exploited
Web pages.

Although the sink in which the vulnerable ﬂow of data
ended is not directly related to the complexity of the vulner-
ability itself, it is relevant for remedies, as diﬀerent ﬁltering
steps must be taken depending on the type of sink. In our
study, we found that 732 exploitable ﬂows ended in doc-
ument.write, 495 in innerHTML and remaining 46 in eval
and its derivatives.

In addition to eval being a sink, we also observed ﬂows
in which it was used to generate code, which was ultimately
responsible for a ﬂow, at runtime. In total, only eleven of
such cases were contained in our data, while the most com-
mon scenario was deobfuscation of code at runtime, e.g., by
base64-decoding it.
5.4 Analysis

In Section 3.1, we deﬁned a set of metrics to measure
the complexity of a vulnerable ﬂow, which we then applied
to a set of real-world vulnerabilities in our study. To bet-
ter quantify the complexity of a ﬂaw, we need to translate
the numeric values derived by our metrics into a classify-
ing scheme. In the following, we introduce the classiﬁcation
boundaries for these measures; based on these boundaries,
we then classify each of the vulnerable ﬂows in our data set
to either have a low, medium or high complexity with re-
spect to each metric. Finally, we combine the classiﬁcation
results and highlight the necessity for a multi-dimensional
classiﬁcation scheme.

Classiﬁcation Scheme: Based on the gathered data of
all metrics, we derive the 80th and 95th percentiles, i.e.,

LCF NLCF

Sum

LDF
NLDF

1,116
98

— 1,116
59
157

Sum
1,273
Table 1: Data and code ﬂow matrix

1,214

59

Figure 9: Cumulative Sum for M1 (string-accessing ops)

Figure 10: Cumulative Sum for M2 (number of functions)

derive the numbers for which at least 80% and 95% of all
vulnerable ﬂows have a lower metric value, respectively. For
M1 and M2, the cumulative sums are depicted in Figures 9
and 10, highlighting also both the percentiles. Although
metric M5, which denotes the relation of source and sink
accessing operations, does not return a numerical value, the
perceived complexity rises with the identiﬁer, i.e., R2 is more
complex than R1 and so on and so forth. For this metric,
more than 80% of the ﬂows were contained in the relation
classes R1 and R2 and less than 5% of the ﬂows were made
up out of ﬂows which had no relation between source and
sink (R5).

We use the resulting percentiles as cut-oﬀ points for our
complexity metric classiﬁcation. Therefore, we set bound-
aries for all of our metrics accordingly (as depicted in Ta-
ble 2), such that any value maps to either a low (LC ),
medium (MC ) or high (HC ) complexity. We calculate the
overall complexity of a ﬂaw from the single highest rating
by any metric; this appeals naturally to the fact that a vul-
nerable ﬂow is already hard to understand if it is complex
in just a single dimension.

Classiﬁcation Results: Based on our classiﬁcation scheme,
we categorize each of the ﬂaws in our data set to a com-
plexity class. The results of this classiﬁcation scheme are
depicted in Table 3, where CMX denotes the results for MX.
Although the boundaries were derived from the 80th and
95th percentile, the results (especially for M2) are not evenly
distributed. This stems from the fact that the boundaries
are a ﬁxed value which denotes that at least 80% or 95% of

M1
M2
M3
M4
M5 R1, R2 R3, R4

MC HC
≤ 22
>22
≤ 10
>10
3
>3
≤ 394 >394
R5

LC
≤ 9
≤ 4
≤ 2
≤ 75

Table 2: Classiﬁcation boundaries

05101520250%20%40%60%80%100%0510150%20%40%60%80%100%1426CM1
CM2
CM3
CM4
CM5

Combined

LC MC

HC

1,079
1,161
1,035
920
1,094

134
85
178
179
120

60
27
60
51
59

813

199
63.9% 20.5% 15.6%

261

Table 3: Classiﬁcation by applied metrics

the ﬂows had a lower ranking, not necessarily exactly that
number. Note also that M4 can only be applied to subset
of 1.150 of the ﬂows in our data set, as only in these cases
source and sink access were contained within the same ﬁle.
By design, each of our metrics assigns at least 80% of the
ﬂaws to lowest complexity class. The results of the combina-
tion of all metrics is also shown in Table 3: we observe that
in combining the result, less than two thirds of the ﬂows are
categorized as having an overall low complexity, i.e., that
for each metric their value was below the 80th percentile.
This highlights the fact that while a ﬂow might be simple in
terms of a single metric, ﬂows are actually more complex if
all metrics are evaluated, putting emphasis on the diﬀerent
proposed metrics.
5.5 Summary of Our Findings

In summary, by grouping the results from each of the met-
rics, separated by the 80th and 95th percentile, and combin-
ing the resulting classiﬁcations into either low, medium or
high complexity, about two thirds of our data set is still la-
beled as having a low complexity, whereas 20% and 15% of
the ﬂows are labeled as having a medium or high complex-
ity, respectively. This shows that taking into account only
a single metric is not suﬃcient to ascertain the complexity
of a vulnerability, but that rather all dimensions must be
analyzed. Given the large fraction of simple ﬂows, which
consist of at most nine operations on the tainted data (in-
cluding source and sink access) and span no more than two
contexts, we ascertain that Client-Side Cross-Site Scripting
is often caused by developers who are unaware of the risks
and pitfalls of using attacker-controllable data in an unﬁl-
tered manner.

Although the fraction of straight-forward vulnerabilities
is very high, the unawareness of developers is only one root
cause of Client-Side Cross-Site Scripting. As this section has
shown, developers may also be overwhelmed by the sheer
amount code they have to understand, potentially passing
ﬁrst- and third-party code on the way. In addition, we found
273 cases in which the vulnerability was contained strictly
inside third-party code and, thus, the developer of the then
vulnerable application was not at fault.
5.6 Comparison to Randomly-Sampled Flows
From our data set, we gathered a large number of ﬂows
which appeared to be vulnerable, but did not trigger our
payload. We can, however, not state with certainty that
these ﬂows were indeed secure. For instance, the applied
ﬁltering might be incomplete, a condition that is not covered
by our exploit generator. Therefore, to put the results of
our study into perspective, we randomly sampled 1.273 ﬂows

80th
≤ 20
≤ 9
≤ 2

95th
≤ 44
≤ 19
3

100th

M1
M2
M3
M4
M5 R1, R2, R3

>44
>19
>3
≤ 189 ≤ 1,208 >1,208
R5

R4

Table 4: Percentiles for randomly-sampled ﬂows

from an attacker-controllable sink to a direct execution sink.
Table 4 shows the results for the percentiles of these ﬂows.
Interestingly, the percentile values are higher for each of the
metrics compared to vulnerable ﬂows. This shows that the
complexity of such ﬂows alone can not be the causing factor
for the widespread occurrence of client-side XSS.

6. ADDITIONAL INSIGHTS

Our analysis so far uncovered that the biggest fraction
of vulnerabilities are caused by programming errors which,
according to the results of our metrics, should be easy to
spot and correct. We conducted a more detailed analysis
into several low complexity ﬂaws as well as those ﬂaws which
were ranked as having a high complexity and found a number
of interesting cases, which shed additional light on the issues
that cause client-side XSS. Therefore, in the following, we
highlight four diﬀerent insights we gained in our analysis.
6.1 Involving Third Parties

In our analysis, we found that vulnerabilities were also
caused when involving code from third parties, either be-
cause ﬁrst- and third-party code were incompatible or be-
cause a vulnerable library introduced a ﬂaw.
Incompatible First- and Third-Party Code: A more
complex vulnerability, which was rated as being of medium
complexity for M3 and high complexity by M5, utilized meta
tags as a temporary sink/source. Listing 2 shows the code,
which extracts the URL fragment and stores it into a newly
created meta element called keywords. Since this code was
found in an inline script, we believe that it was put there
with intend by the page’s programmer.

var parts = window.location.href.split("#");
if (parts.length > 1) {

var kw = decodeURIComponent(parts.pop());
var meta = document.createElement(’meta’);
meta.setAttribute(’name’, ’keywords’);
meta.setAttribute(’content’, kw);
document.head.appendChild(meta);

}

Listing 2: Creating meta tags using JavaScript

This page also included a third-party script, which for the
most part consisted of the code shown in Listing 3. This code
extracts data from the meta tag and uses it to construct a
URL to advertisement. In this case, however, this data is
attacker-controllable (originating from the URL fragment)
and thus this constitutes a client-side XSS vulnerability.
This code is an example for a vulnerability which is caused
by the combination of two independent snippets, highlight-
ing the fact that the combined use of own and third-party
code can signiﬁcantly increase complexity and the potential

1427for an exploitable data ﬂow. In this concrete case, the Web
application’s programmer wanted to utilize the dynamic na-
ture of the DOM to generate keywords from user input, while
the third-party code provider reckoned that meta tags would
only be controllable by the site owner.

function getKwds() {

var th_metadata = document.getElementsByTagName("meta");
...

}
var kwds = getKwds();
document.write(’<iframe src="...&loc=’ + kwds + ’"></iframe>’);

Listing 3: Third-party code extracting previously set meta
tags

Vulnerable Libraries: An example for a vulnerability
which was rated as being of low complexity is related to
a vulnerable version of jQuery. The popular library jQuery
provides a programmer with the $ selector to ease the access
to a number of functions inside jQuery, such as the selection
by id (using the # tag) as well as the generation of a new
element in the DOM when passing HTML content to it. Up
until version 1.9.0b1 of jQuery, this selector was vulnerable
to client-side XSS attacks [10], if attacker-controllable con-
tent was passed to the function—even if a # tag was hard-
coded in at the beginning of that string. Listing 4 shows
an example of such a scenario, where the intended use case
is to call the fadeIn function for a section whose name is
provided via the hash. This ﬂaw could be exploited by an
attacker by simply putting his payload into the hash.

var section = location.href.slice(1);
$("#" + section + "_section").fadeIn();

Listing 4: Vulnerable code if used with jQuery before 1.9.0b1

In our study, we found that 25 vulnerabilities were caused
by this bug, although the vulnerability had been ﬁxed for
over three years at time of writing this paper. In total, we
discovered that 472 of the exploited Web pages contained
outdated and vulnerable versions of jQuery, albeit only a
fraction contained calls to the vulnerable functions. jQuery
was accompanied by a number of vulnerable plugins, such
as jquery-ui-autocomplete (92 times). Second to jQuery
came the YUI library, of which vulnerable versions were
included in 39 exploited documents. This highlights that
programmers should regularly check third-party libraries for
security updates or only include the latest version of the li-
brary into their pages.
6.2 Erroneous Patterns

Apart from the involvement of third-party code which
caused vulnerabilities, we found two additional patterns that
highlight issues related to client-side XSS, namely the im-
proper usage of browser-provided APIs and the explicit de-
coding of user-provided data.

Improper API Usage: In our data set, we found a vul-
nerability in the snippet shown in Listing 5, which was as-
signed the lowest complexity score by any of our metrics. In
this case, the user-provided data is passed to the outlined
function, which apparently aims at removing all script tags

inside this data. The author of this snippet, however, made
a grave error. Even though the newly created div element
is not yet attached to the DOM, assigning innerHTML will
invoke the HTML parser. While any script tag is not exe-
cuted when passed to innerHTML [9], the attacker can pass a
payload containing an img with an error handler [15]. The
HTML parser will subsequently try to download the refer-
enced image and in the case of a failure, will execute the
attacker-provided JavaScript code. While the eﬀort by the
programmer is commendable, this ﬁltering function ended
up being a vulnerability by itself. Next to this ﬂaw, we
found examples of the improper use of built-in functions,
such as parseInt and replace.

function escapeHtml(s) {

var div = document.createElement(’div’);
div.innerHTML = s;
var scripts = div.getElementsByTagName(’script’);
for (var i = 0; i < scripts.length; ++i) {

scripts[i].parentNode.removeChild(scripts[i]);

}
return div.innerHtml;

};

Listing 5: Improper use of innerHTML for sanitization

Explicit Decoding of Otherwise Safe Data: As out-
lined in Section 4.3, the automatic encoding behavior of
data retrieved from the document.location source varies
between browsers: Firefox will automatically escape all com-
ponents of the URL, while Chrome does not encode the frag-
ment, and IE does not encode any parts of the URL. In con-
sequence, some insecure data ﬂows may not be exploitable
in all browsers, with Firefox being the least susceptible of
the three, thanks to its automatic encoding.

The data set underlying our study was validated to be
exploitable if Chrome’s escaping behavior is present, which
leaves the fragment portion of the URL unaltered. Nev-
ertheless, we wanted to investigate how many vulnerabili-
ties would actually work in any browser, i.e., in how many
cases data was intentionally decoded before use in a security-
sensitive sink. Using an unmodiﬁed version of Firefox, we
crawled the persisted vulnerabilities again and found that
109 URLs still triggered our payload. This highlights the
fact that programmers are aware of such automatic encod-
ing, but simply decode user-provided data for convenience
without being aware of the security implications.
6.3 Summary of Our Insights

In summary, we ﬁnd that although a large fraction of
all vulnerabilities are proverbial facepalms, developers are
often confronted with additional obstacles even in cases,
where the complexity is relatively low.
In our work, we
have found evidence for faults which are not necessarily to
blame on the developer of the vulnerable Web application,
but rather are either a combination of incompatible ﬁrst- and
third-party code or even caused completely by third-party
libraries. This paradigm is enabled by the Web’s program-
ming model, which allows for third-party code to be included
in a Web page, gaining full access to that page’s DOM.

In addition, we have found patterns of mistakes, which
are caused by developers due to their misunderstanding of
browser-provided APIs or even the explicit decoding of user-
provided data to allow for a convenient use of such data.

1428Apart from the example shown here, we have found addi-
tional misguided attempts at securing applications, allowing
an attacker to easily bypass these measures. This leads us to
believe that even developers, who are aware of the potential
pitfalls of using attacker-controllable data in their applica-
tion, sometimes lack the knowledge of how to properly secure
their application.

7. RELATED WORK
Client-Side Cross-Site Scripting: In 2005, Amit Klein
coined the term of DOM-based XSS [11]. One of the ﬁrst
tools speciﬁcally designed to detect DOM-based XSS was
DOMinator, which used taint-tracking to detect vulnerable
ﬂows [7]. We extended the concept, presenting an automated
means of detecting and verifying client-side XSS vulnerabili-
ties, ﬁnding that about 10% of the Top 10k Web pages carry
at least one such vulnerability [12]. On top of this taint-
tracking engine, we built a taint-aware XSS ﬁlter aimed at
thwarting client-side XSS [27]. Previous work by Saxena
et al. has investigated so-called Client-Side Vulnerabilities,
mainly aiming at the exploitation of client-side XSS ﬂaws.
Using “taint enhanced blackbox fuzzing”, they were able to
discover eleven previously unknown vulnerabilities on real-
world Web pages [22].
In contrast to that, Criscione pre-
sented a brute-force blackbox testing approach [5]. To ﬁnd
bugs in complex JavaScript code, Saxena et al. developed a
symbolic execution framework for JavaScript, which helped
them to ﬁnd two previously unknown vulnerabilities [21].

JavaScript Analysis: Furthermore, attention has been
given to JavaScript error sources in general. Two research
groups presented empirical studies of JavaScript source code
included in popular sites. Richards et al.
investigated the
general runtime behavior of JavaScript and concluded that
the language is a “harsh terrain for static analysis”, conﬁrm-
ing our decision to use a dynamic approach [20]. Ocariza
et al. categorized the diﬀerent kinds of errors they encoun-
tered with speciﬁc test cases they created for the top 100
Web sites [17]. Although these sites were in a mature pro-
ductive state, the same well-deﬁned categories of errors were
discovered frequently.

In 2011, Guarnieri et al.

Richards et al. evaluated the use of eval in depth and
came to the conclusion that up to 82% of all Web sites uti-
lize the construct, often in a dangerous way. They did, how-
ever, ﬁnd that replacing eval altogether is still unfeasible
[8] presented ACTARUS,
[19].
which is capable of conducting taint-aware static analysis of
JavaScript on real-world Web sites, ﬁnding over 500 vulner-
abilities on eleven sites. Later, Meawad et al. developed
the tool Evalorizer, aiming to assist programmers in the
removal of unnecessary eval constructs [14]. As our work
has shown, Web sites often incorporate cross-domain Java-
Script code. In their work, Nikiforakis et al. investigated the
trust relationships between web pages of the Alexa Top 10k
sites [16], demonstrating that sites often open themselves to
attacks by including third-party content.

Vulnerability Analysis: Other research has focussed on
more general vulnerability analysis. In 2008, Shin et al. con-
ducted an analysis on how well code complexity metrics can
be used to predict vulnerabilities in the Mozilla JavaScript
Engine [26]. They conclude that while complexity metrics
can be useful in ﬁnding ﬂaws with a low false positive rate,
they carry a high false negative rate. Besides complexity,

Shin et al. looked at code churn and developer activity met-
rics to determine indicators of vulnerabilities and were able
to use them to “to prioritize inspection and testing eﬀort”
[25]. Complexity, coupling, and cohesion metrics with sim-
ilar results have also been used to discover vulnerabilities
statically by Chowdhury et al. [3]. Another approach, aim-
ing at ﬁnding improper input validation was presented by
Scholte et al. [23], using automatic data type inferring for
the validation functions. Doing so, they found that 65% of
the (server-side) XSS ﬂaws in their set were simple enough
to be stopped by their approach, without causing any has-
sle for the developers, which coincides with the number of
simple ﬂows we discovered in our study.Static taint analysis
has been used by Wassermann and Su to identify server-
side XSS using a policy of allowed inputs based on the W3C
recommendation [30]. Yamaguchi et al. have conducted ad-
ditional work to identify new vulnerabilities using machine
learning and a set of previously known vulnerabilities [31]
as well as based on Abstract Syntax Trees [32].

To summarize, while previous research has focussed on the
detection of client-side XSS, JavaScript security analysis and
general vulnerability analysis, no prior work has investigated
the underlying causes of client-side XSS ﬂaws. Our work
aims at closing this gap in previous research.

8. LIMITATIONS AND FUTURE WORK

The results of our analysis are naturally inﬂuenced by the
methodology used to ﬁnd the vulnerabilities. Most impor-
tantly, our method only ﬁnds vulnerabilities in code that is
executed during a normal page visit. Hence, ﬂows that are
dependent on a certain condition (such as a speciﬁc URL pa-
rameter) cannot be detected. Since no ground truth in terms
of all real-world client-side XSS vulnerabilities is known, we
have no means of ascertaining whether such ﬂaws are even
more complex than the ones underlying our study. Extend-
ing the detection methodology with static analysis to dis-
cover more vulnerabilities is therefore a promising extension
which could subsequently be transferred to our current work.
An interesting extension of our work is the application
of code coverage metrics by instrumenting the cached Java-
Script code [24]. This approach, however, has limitations of
its own, as rewriting cannot be conducted on code which is
dynamically generated at runtime using eval.

In addition, with the gathered data, we are able to inves-
tigate the usage of ﬁltering functions on the Web, e.g., the
regular expression used. Therefore, we believe the analysis
of these deployed ﬁlters might shed light on improper use of
such functions, especially as we have anecdotal evidence of
improperly used regular expression ﬁltering.

9. CONCLUSION

In this paper we investigated the root cause of client-side
XSS, i.e., the underlying issues related to this class of XSS
vulnerabilities. To do so, we throughly analyzed a set of
1,273 real-world vulnerabilities and classiﬁed them according
to their complexity.

Our work shows that a large number of ﬂaws are compar-
atively simple and, thus, most likely rooted in insuﬃcient
security awareness of the responsible developers. Based on
the classiﬁcation approach introduced in this paper, about
two thirds of all examined vulnerabilities fall into this cate-
gory. In contrast, about 15% of the discovered ﬂaws have a

1429high combined complexity rating, showing that developers
may also be overwhelmed by the complexity of the vulner-
able code; in 59 cases we even discovered interrupted code
ﬂows, signiﬁcantly impeding the ﬂaw discovery. Our study,
however, also found that for randomly sampled ﬂows, the
complexity metrics generally yield higher values, i.e., non-
exploitable code is often even more complex than vulnerable
code.

In addition to these ﬁndings, our gained insights highlight
that the aforementioned reasons are not the only causing
factors for client-side XSS. The presented ﬁndings show that
developers are not always to blame, as ﬂaws might be caused
by third-party code. In 273 of our vulnerabilities, this third-
party code was solely responsible for the ﬂaw, whereas an
additional 165 ﬂaws was caused by a combination of third-
and ﬁrst-party code, in parts related to the careless use of
outdated and vulnerable libraries. Our work also uncovered
patterns which highlight that developers lack the knowledge
of the inner workings of browser-provided APIs, in at least
one case actually introducing a vulnerability to begin with.
In summary, we ﬁnd that there is no single reason for the
existence of client-side XSS. Instead, the issues are caused
by a number of factors, ranging from developers who are
unaware of the security implications when using attacker-
controllable data and their failed attempts at securing ap-
plication to highly complex constructs of code and issues
introduced by third parties.

Acknowledgements
We would like to thank the anonymous reviewers for their
valuable feedback. This work was in parts supported by the
EU Project STREWS (FP7-318097).

10. REFERENCES
[1] D. Bates, A. Barth, and C. Jackson. Regular

expressions considered harmful in client-side XSS
ﬁlters. In WWW, 2010.

[2] BuiltWith. jQuery Usage Statistics.

http://goo.gl/czK9XU (accessed 16/05/15), 2015.
[3] I. Chowdhury and M. Zulkernine. Can Complexity,
Coupling, and Cohesion Metrics Be Used As Early
Indicators of Vulnerabilities? In SAC, 2010.

[4] A. Cortesi and M. Hils. mitmproxy.

https://goo.gl/VA9xw4 (accessed 16/05/15), 2014.
[5] C. Criscione. Drinking the Ocean - Finding XSS at
Google Scale. Talk at the Google Test Automation
Conference, (GTAC’13), http://goo.gl/8qqHA, 2013.
[6] M. E. Daggett. Enforcing Style. In Expert JavaScript.

2013.

[7] S. Di Paola. DominatorPro: Securing Next Generation

of Web Applications. https://goo.gl/L6tJth
(accessed 16/05/15), 2012.

[8] S. Guarnieri, M. Pistoia, O. Tripp, J. Dolby,

S. Teilhet, and R. Berg. Saving the World Wide Web
from Vulnerable JavaScript. In International
Symposium on Software Testing and Analysis, 2011.

[9] I. Hickson and D. Hyatt. HTML 5 - A vocabulary and

associated APIs for HTML and XHTML. W3c
working draft, W3C, 2008.

[10] jQuery Bug Tracker. SELECTOR INTERPRETED

AS HTML. http://goo.gl/JNggpp (accessed
16/05/15), 2012.

[11] A. Klein. DOM based cross site scripting or XSS of

the third kind. Web Application Security Consortium,
2005.

[12] S. Lekies, B. Stock, and M. Johns. 25 Million Flows
Later: Large-scale Detection of DOM-based XSS. In
CCS, 2013.

[13] M. McDaniel and M. H. Heydari. Content based ﬁle

type detection algorithms. In HICSS, 2003.

[14] F. Meawad, G. Richards, F. Morandat, and J. Vitek.

Eval begone!: semi-automated removal of eval from
javascript programs. ACM SIGPLAN Notices, 47,
2012.
[15] Mozilla Developer Network. Element.innerHTML -
Web API Interfaces | MDN. https://goo.gl/udFqtb
(accessed 16/05/15), 2015.

[16] N. Nikiforakis, L. Invernizzi, A. Kapravelos, S. Van

Acker, W. Joosen, C. Kruegel, F. Piessens, and
G. Vigna. You Are What You Include: Large-scale
Evaluation of Remote JavaScript Inclusions. In CCS,
2012.

[17] F. Ocariza, K. Pattabiraman, and B. Zorn. JavaScript

errors in the wild: An empirical study. In Software
Reliability Engineering, 2011.

[18] E. Oftedal. Retire.js - identify JavaScript libraries

with known vulnerabilities in your application.
http://goo.gl/r4BQoG (accessed 16/05/15), 2013.

[19] G. Richards, C. Hammer, B. Burg, and J. Vitek. The

eval that men do. In ECOOP. 2011.

[20] G. Richards, S. Lebresne, B. Burg, and J. Vitek. An

Analysis of the Dynamic Behavior of JavaScript
Programs. In PLDI, 2010.

[21] P. Saxena, D. Akhawe, S. Hanna, F. Mao,

S. McCamant, and D. Song. A Symbolic Execution
Framework for JavaScript. In IEEE S&P, 2010.

[22] P. Saxena, S. Hanna, P. Poosankam, and D. Song.
Flax: Systematic discovery of client-side validation
vulnerabilities in rich web applications. In NDSS,
2010.

[23] T. Scholte, W. Robertson, D. Balzarotti, and

E. Kirda. Preventing input validation vulnerabilities
in web applications through automated type analysis.
In Computer Software and Applications Conference.
IEEE, 2012.

[24] A. Seville. Blanket.js - seamless javascript code

coverage. http://goo.gl/hzJFTn (accessed 16/05/15),
2014.

[25] Y. Shin, A. Meneely, L. Williams, and J. Osborne.

Evaluating Complexity, Code Churn, and Developer
Activity Metrics as Indicators of Software
Vulnerabilities. Transactions on Software Engineering,
2011.

[26] Y. Shin and L. Williams. An Empirical Model to

Predict Security Vulnerabilities Using Code
Complexity Metrics. In International Symposium on
Empirical Software Engineering and Measurement,
2008.

[27] B. Stock, S. Lekies, T. Mueller, P. Spiegel, and
M. Johns. Precise client-side protection against
DOM-based cross-site scripting. In USENIX Security,
2014.

[28] The jQuery Foundation. Working with JSONP.

https://goo.gl/Wdqgo3 (accessed 16/05/15), 2015.

[29] W3Techs. Usage Statistics and Market Share of

JQuery for Websites, February 2015.
http://goo.gl/jyQEZR (accessed 16/05/15), 2015.

[30] G. Wassermann and Z. Su. Static detection of

cross-site scripting vulnerabilities. In International
Conference on Software Engineering, 2008.

[31] F. Yamaguchi, F. Lindner, and K. Rieck. Vulnerability

Extrapolation: Assisted Discovery of Vulnerabilities
Using Machine Learning. In USENIX WOOT, 2011.

[32] F. Yamaguchi, M. Lottmann, and K. Rieck.

Generalized Vulnerability Extrapolation Using
Abstract Syntax Trees. In ACSAC, 2012.

1430