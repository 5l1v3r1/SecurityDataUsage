Burst ORAM: Minimizing ORAM Response Times 

for Bursty Access Patterns

Jonathan Dautrich, University of California, Riverside; Emil Stefanov, University of California, 

Berkeley;  Elaine Shi, University of Maryland, College Park

https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/dautrich

This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXBurst ORAM: Minimizing ORAM Response

Times for Bursty Access Patterns

Jonathan Dautrich

University of California, Riverside

Emil Stefanov

University of California, Berkeley

Elaine Shi

University of Maryland, College Park

Abstract
We present Burst ORAM, the ﬁrst oblivious cloud stor-
age system to achieve both practical response times
and low total bandwidth consumption for bursty work-
loads. For real-world workloads, Burst ORAM can at-
tain response times that are nearly optimal and orders
of magnitude lower than the best existing ORAM sys-
tems by reducing online bandwidth costs and aggres-
sively rescheduling shufﬂing work to delay the bulk of
the IO until idle periods.

We evaluate our design on an enterprise ﬁle system
trace with about 7,500 clients over a 15 day period,
comparing to an insecure baseline encrypted block store
without ORAM. We show that when baseline response
times are low, Burst ORAM response times are compa-
rably low. In a 32TB ORAM with 50ms network latency
and sufﬁcient bandwidth capacity to ensure 90% of re-
quests have baseline response times under 53ms, 90% of
Burst ORAM requests have response times under 63ms,
while requiring only 30 times the total bandwidth con-
sumption of the insecure baseline. Similarly, with sufﬁ-
cient bandwidth to ensure 99.9% of requests have base-
line responses under 70ms, 99.9% of Burst ORAM re-
quests have response times under 76ms.
1
Cloud computing allows customers to outsource the bur-
den of data management and beneﬁt from economy of
scale, but privacy concerns hinder its growth [3]. En-
cryption alone is insufﬁcient to ensure privacy in storage
outsourcing applications, as information about the con-
tents of encrypted records may still leak via data access
patterns. Existing work has shown that access patterns
on an encrypted email repository may leak sensitive key-
word search queries [12], and that accesses to encrypted
database tuples may reveal ordering information [5].

Introduction

Oblivious RAM (ORAM), ﬁrst proposed in a ground-
breaking work by Goldreich and Ostrovsky [8, 9], is a
cryptographic protocol that allows a client to provably

hide access patterns from an untrusted storage server.
Recently, the research community has focused on mak-
ing ORAM schemes practical for real-world applica-
tions [7, 11, 21, 23–25, 27]. Unfortunately, even with re-
cent improvements, ORAMs still incur substantial band-
width and response time costs.

Many prior ORAM works focus on minimizing band-
width consumption. Several recent works on cloud-
based ORAMs achieve low bandwidth costs using a large
amount of client-side storage [11, 23, 24]. Others rely on
expensive primitives like PIR [17] or additional assump-
tions such as trusted hardware [15] or non-colluding
servers [22] to reduce bandwidth costs.

To be practical, ORAM must also minimize response
times observed by clients for each request. We propose
Burst ORAM, a novel ORAM that dramatically reduces
response times for realistic workloads with bursty char-
acteristics. Burst ORAM is based on ObliviStore [23],
the most bandwidth-efﬁcient existing ORAM.

Burst ORAM uses novel techniques to minimize the
online work of serving requests and delay ofﬂine block
shufﬂing until idle periods. Under realistic bursty loads,
Burst ORAM achieves orders of magnitude shorter re-
sponse times than existing ORAMs, while retaining total
bandwidth costs less than 50% higher than ObliviStore’s.
During long bursts, Burst ORAM’s behavior automat-
ically and gracefully degrades to be similar to that of
ObliviStore. Thus, even in a worst-case workload, Burst
ORAM’s response times and bandwidth costs are com-
petitive with those of existing ORAMs.

We simulate Burst ORAM on a real-world corporate
data access workload (7,500 clients and 15 days) to show
that it can be used practically in a corporate cloud stor-
age environment. We compare against an insecure base-
line encrypted block store without ORAM and show that
when baseline response times are low, Burst ORAM re-
sponse times are also low. In a 32TB ORAM with 50ms
network latency and sufﬁcient bandwidth capacity to en-
sure 90% of requests have baseline response times un-

USENIX Association  

23rd USENIX Security Symposium  749

der 53ms, 90% of Burst ORAM requests have response
times under 63ms. Similarly, with sufﬁcient bandwidth
to ensure 99.9% of requests have baseline responses un-
der 70ms, 99.9% of Burst ORAM requests have response
times under 76ms. Existing works exhibit response times
on the order of seconds or higher, due to high bandwidth
[11, 23, 25, 28] or computation [17] requirements. To
our knowledge, our work is the ﬁrst to evaluate ORAM
response times on a realistic, bursty workload.

As in previous ORAM schemes, we do not seek to hide
the timing of data requests. Thus, we assume request
start times and durations are known. To ensure security,
we do not allow the IO scheduler to make use of the data
access sequence or other sensitive information. We an-
alyze Burst ORAM security in Section 6.4.
1.1 Burst ORAM Contributions
Burst ORAM introduces several techniques for reducing
response times and keeping bandwidth costs low that dis-
tinguish it from ObliviStore and other predecessors.
Novel scheduling policies. Burst ORAM prioritizes the
online work that must be complete before requests are
satisﬁed. If possible, our scheduler delays shufﬂe work
until off-peak times. Delaying shufﬂe work consumes
client-side storage, so if a burst is sufﬁciently long, client
space will ﬁll, forcing shufﬂing to resume. By this time,
there are typically multiple shufﬂe jobs pending.

We use a greedy strategy to prioritize jobs that free the
most client-side space per unit of shufﬂing bandwidth
consumed. This strategy allows us to sustain lower re-
sponse times for longer during an extended burst.
Reduced online bandwidth costs. We propose a new
XOR technique that reduces the online bandwidth cost
from O(logN) blocks per request in ObliviStore to nearly
1, where N is the outsourced block count. The XOR tech-
nique can also be applied to other ORAM implementa-
tions such as SR-ORAM [26] (see Appendix B).
Level caching. We propose a new technique for us-
ing additional available client space to store small levels
from each partition. By caching these levels on the client,
we are able to reduce total bandwidth cost substantially.
1.2 Related Work
Oblivious RAM was ﬁrst proposed in a seminal work by
Goldreich and Ostrovsky [9]. Since then, a fair amount
of theoretic work has focused on improving its asymp-
totic performance [1, 4, 10, 11, 13, 18, 19, 21, 24, 27].
Recently, there has been much work designing and opti-
mizing ORAM for cloud-based storage outsourcing set-
tings, as noted below. Different ORAMs provide varying
trade-offs between bandwidth cost, client/server storage,
round complexity, and computation.

ORAM has been shown to be feasible for secure (co-)

processor prototypes, which prevent information leakage
due to physical tampering [6, 15, 16, 20]. Since on-chip
trusted cache is expensive, such ORAM schemes need
constant or logarithmic client-side storage, such as the
binary-tree ORAM [21] and its variants [7, 17, 25].
In cloud-based ORAMs, the client typically has more
space, capable of storing O(√N) blocks or a small
amount of per-block metadata [10, 23, 24, 28] that can
be used to reduce ORAM bandwidth costs. Burst ORAM
also makes such client space assumptions.

Online and ofﬂine costs for ORAM were ﬁrst made
explicit by Boneh et al. [1] They propose a construc-
tion that has O(1) online but O(√N) overall bandwidth
cost. The recent Path-PIR work by Mayberry et al. [17]
mixes ORAM and PIR to achieve O(1) online band-
width cost with an overall bandwidth cost of O(log2 N)
with constant client memory. Unfortunately, the PIR is
still computationally expensive, so their scheme requires
more than 40 seconds for a read from a 1TB database
[17]. Burst ORAM has O(1) online and O(logN) overall
bandwidth cost, without the added overhead of PIR.

Other ORAMs that do not rely on trusted hardware or
non-colluding servers have Ω(logN) online bandwidth
cost including works by Williams, Sion, et al. [27, 28];
by Goodrich, Mitzenmacher, Ohrimenko, and Tamassia
[10, 11]; by Kushilevitz et al. [13]; and by Stefonov, Shi,
et al. [21, 23–25]. Burst ORAM handles bursts much
more effectively by reducing the online cost to nearly 1
block transfer per block request during a burst, greatly
reducing response times.
2 Preliminaries
2.1 Bandwidth Costs
Bandwidth consumption is the primary cost in many
modern ORAMs, so it is important to deﬁne how we
measure its different aspects. Each block transferred be-
tween the client and server is a single unit of IO. We as-
sume that blocks are large in practice (at least 1KB), so
transferred meta-data (block IDs) have negligible size.

Deﬁnition 1 The bandwidth cost of a storage scheme is
given by the average number of blocks transferred in or-
der to read or write a single block.

We identify bandwidth costs by appending X to the num-
ber. A bandwidth cost of 2X indicates two blocks trans-
ferred per request, which is twice the cost of an unpro-
tected scheme. We consider online, ofﬂine, effective, and
overall IO and bandwidth costs, where each cost is given
by the average amount of the corresponding type of IO.
Online IO consists of the block transfers needed before
a request can be safely marked as satisﬁed, assuming the
scheme starts with no pending IO. The online bandwidth
cost of a storage scheme without ORAM is just 1X — the

750  23rd USENIX Security Symposium 

USENIX Association

2

Effective IO 

R1 

R2 

R3 

Request R1: 

Request R2: 

Request R3: 

Legend 
Online IO 
Offline IO 

Time 

R1 

Satisfied 

R2 

Satisfied 

R3 

Satisfied 

Figure 1: Simpliﬁed scheme with sequential IO and con-
trived capacity for delaying ofﬂine IO. 3 requests require
same online (2), ofﬂine (5), and overall (7) IO. Online
IO for R1 is handled immediately, so R1’s effective IO is
only 2. R2 waits for 2 units of ofﬂine IO from R1, so its
effective IO is 4. R3 waits for the rest of R1’s ofﬂine IO,
plus one unit of R2’s ofﬂine IO, so its effective IO is 6.

IO cost of downloading the desired block. In ORAM it
may be higher, as additional blocks may be downloaded
to hide the requested block’s identity.

Ofﬂine IO consists of transfers needed to prepare for
subsequent requests, but which may be performed after
the request is satisﬁed. Without ORAM, the ofﬂine band-
width cost is 0X. In ORAM it is generally higher, as addi-
tional shufﬂe IO is needed to obliviously permute blocks
in order to guarantee privacy for future requests.

Overall IO / bandwidth cost is just the sum of the on-

line and ofﬂine IO / bandwidth costs, respectively.

Effective IO consists of all online IO plus any pend-
ing ofﬂine IO from previous requests that must be is-
sued before the next request can be satisﬁed. Without
ORAM, effective IO and online IO are equal.
In tra-
ditional ORAMs, ofﬂine IO is issued immediately after
each request’s online IO, so effective and overall IO are
equal. In Burst ORAM, we delay some ofﬂine IO, reduc-
ing each request’s effective IO as illustrated in Figure 1.
Smaller effective costs mean less IO between requests,
and ultimately shorter response times.

ORAM reads and writes are indistinguishable, so

writes have the same bandwidth costs as reads.

2.2 Response Time

The response time of a block request (ORAM read/write
operation) is deﬁned as the lapse of wall-clock time be-
tween when the request is ﬁrst issued by the client and
when the client receives a response. The minimum re-
sponse time is the time needed to perform all online IO.
Response times increase when ofﬂine IO is needed be-
tween requests, increasing effective IO, or when requests
are issued rapidly in a burst, delaying later requests.

2.3 ObliviStore ORAM
Burst ORAM builds on ObliviStore [23], so we give an
overview of the scheme here. A full description of the
ObliviStore system and its ORAM algorithm spans about
55 pages [23, 24], so we describe it at a high level, fo-
cusing only on components relevant to Burst ORAM.
Partitions and levels. ObliviStore stores N logical data
blocks. Each block is encrypted using a standard sym-
metric key encryption scheme before it is stored on the
server. Every time a block is uploaded by the client, it is
re-encrypted using a new nonce to prevent linking.
ObliviStore securely splits blocks into O(√N) parti-
tions of O(√N) blocks each. Each partition is an ORAM
consisting of O(logN) levels with 2,4,8, . . . ,O(√N)
blocks each. Newly created levels are ﬁlled with half
encrypted real blocks and half encrypted dummies, ran-
domly permuted so that reals and dummies are indistin-
guishable to the server. Each level is occupied only half
the time on average. The client has space to store O(√N)
blocks and the locations of all N blocks.
Requests. When the client makes a block request,
whether a read or write, the block must ﬁrst be down-
loaded from the appropriate partition. To maintain obliv-
iousness, ObliviStore must fetch one block from every
non-empty level in the target partition (O(logN) blocks
of online IO). Only one fetched block is real, and the rest
are dummies, except in the case of early shufﬂe reads de-
scribed below. Once a dummy is fetched, it is discarded,
and new dummies are created later as needed. ObliviS-
tore securely processes multiple requests in parallel, en-
abling full utilization of available bandwidth capacity.
Eviction. Once the real block is fetched, it is updated
or returned to the client as necessary, then ranodmly as-
signed to a new partition p. The block is not immediately
uploaded, but is scheduled for eviction to p and stored in
a client-side data cache. An independent eviction pro-
cess later obliviously evicts the block from the cache to
p. The eviction triggers a write operation on p’s ORAM,
which creates or enlarges a shufﬂing job for p.
Shufﬂing Jobs. Each partition p has at most one pending
shufﬂe job. A job consists of downloading up to O(√N)
blocks from p, permuting them on the client with recent
evictions and new dummies, and uploading. Shufﬂe jobs
incur ofﬂine IO, and vary in size (amount of IO) from
O(1) to O(√N).
Intuitively, to ensure that non-empty
levels have at least one dummy left, we must re-shufﬂe
a level once half its blocks have been removed. Larger
levels need shufﬂing less often, so larger jobs occur less
frequently, keeping ofﬂine bandwidth costs at O(logN).
Shufﬂe IO scheduling. A ﬁxed amount of O(logN)
shufﬂe IO is performed after each request to amortize the
work required for large jobs. The IO for jobs from multi-

USENIX Association  

23rd USENIX Security Symposium  751

3

ple partitions may be executed in parallel: while waiting
on reads for one partition, we may issue reads or writes
for another. Jobs are started in the order they are created.
Early shufﬂe reads. Early shufﬂe reads, referred to as
early cache-ins or real cache-ins in ObliviStore, occur
when a request needs to fetch a block from a level, but at
least half the level’s original blocks have been removed.
In this case, we cannot guarantee that any dummies re-
main. Thus, early shufﬂe reads must be treated as real
blocks and stored separately by the client until they are
returned to the server as part of a shufﬂe job. We call such
reads early shufﬂe reads since the blocks would have
eventually been read during a shufﬂe job. Early shufﬂe
reads are infrequent, but made possible since ObliviStore
performs requests while shufﬂing is in progress.
Level compression. ObliviStore uses a technique called
level compression [24] to compress blocks uploaded dur-
ing shufﬂing. It allows the client to upload k real and k
dummy blocks using only k blocks of bandwidth with-
out revealing which k are dummies. Level compression
reduces only the ofﬂine (shufﬂing) bandwidth cost.
3 Overview of our Approach
Traditional ORAMs focus on reducing average and
worst-case overall bandwidth costs (per-request over-
all IO). However, even the most bandwidth-efﬁcient
schemes [23, 24] suffer from a 20X–35X bandwidth cost.
In this paper, we instead focus on reducing effective
IO by reducing online IO and delaying ofﬂine IO. We
can then satisfy bursts of requests quickly, delaying most
IO until idle periods. Figure 2 illustrates this concept.

Our approach allows many bursts to be satisﬁed with
nearly a 1X effective bandwidth cost. That is, during
the burst, we transfer just over one block for every block
requested. After the burst we do extra IO to catch up on
shufﬂing and prepare for future requests. Our approach
maintains an overall bandwidth cost less than 50% higher
than [23, 24] in practice (see Figure 12 in Section 7).
Bursts. Intuitively, a burst is a period of frequent block
requests from the client preceded and followed by rela-
tively idle periods. Many real-world workloads exhibit
bursty patterns (e.g. [2, 14]). Often, bursts are not dis-
crete events, such as when multiple network ﬁle system
users are operating concurrently. Thus we handle bursts
ﬂuidly: the more requests issued at once, the more Burst
ORAM tries to delay ofﬂine IO until idle periods.
Challenges. We are faced with two key challenges when
building a burst-friendly ORAM system. The ﬁrst is en-
suring that we maintain security. A naive approach to
reducing online IO may mark requests as satisﬁed before
enough blocks are read from the server, leaking informa-
tion about the requested block’s identity.

The second challenge is ensuring that we maximally

utilize client storage and available bandwidth while
avoiding deadlock. An excessively aggressive strategy
that delays too much IO may use so much client space
that we run out of room to shufﬂe. It may also under-
utilize available bandwidth, increasing response times.
On the other hand, an overly conservative strategy may
under-utilize client space or perform shufﬂing too early,
delaying online IO and increasing response times.
Techniques and Outline. In Burst ORAM, we address
these challenges by combining several novel techniques.
In Section 4 we introduce our XOR technique for reduc-
ing online bandwidth cost to nearly 1X. We also describe
our techniques for prioritizing online IO and delaying of-
ﬂine/shufﬂe IO until client memory is nearly full. In Sec-
tion 5 we show how Burst ORAM prioritizes efﬁcient
shufﬂe jobs in order to delay the bulk of the shufﬂe IO
even further, ensuring that we minimize effective IO dur-
ing long bursts. We then introduce a technique for using
available client space to cache small levels locally to re-
duce shufﬂe IO in both Burst ORAM and ObliviStore.

In Section 6 we discuss the system-level techniques
used in Burst ORAM, and present its design in detail.
In Section 7, we evaluate Burst ORAM’s performance
through micro-benchmarks and extensive simulations.
4 Prioritizing and Reducing Online IO
Existing ORAMs require high online and ofﬂine band-
width costs to obscure access patterns. ObliviStore must
fetch one block from every level in a partition (see
Section 2.3), requiring O(logN) online IO per request.
Figure 3 (left) illustrates this behavior. After each re-
quest, ObliviStore also requires O(logN) ofﬂine/shufﬂe
IO. Since ObliviStore issues online and ofﬂine IO before
satisfying the next request, its effective IO is high, lead-
ing to large response times during bursts. Other ORAMs
work differently, such as Path ORAM [25] which orga-
nizes data as a tree, but still have high effective costs.
We now show how Burst ORAM achieves lower effective
bandwidth costs and response times than ObliviStore.
4.1 Prioritizing Online IO
One way we achieve low response times in Burst ORAM
is by prioritizing online IO over shufﬂe IO. That is, we
suppress shufﬂe IO during bursts, delaying it until idle
periods. Requests are satisﬁed once online IO ﬁnishes,1
so prioritizing online IO allows us to satisfy all requests
before any shufﬂe IO starts, keeping response times low
even for later requests. Figure 2 illustrates this behavior.
During the burst, we continue processing requests by
fetching blocks from the server, but since shufﬂing is
suppressed, no blocks are uploaded. Thus, we must re-
sume shufﬂing once client storage ﬁlls. Section 5.2 dis-

1Each client write also incurs a read, so writes still incur online IO.

752  23rd USENIX Security Symposium 

USENIX Association

4

ObliviStore 

Legend 
Online IO 
Offline IO 

Less online IO 
More offline IO 

Burst ORAM 

Concurrent 

IO 

Offline IO (shuffling) is delayed 
until idle time between bursts 

Time 

Time 

Long response 

times 

Burst 1 
Start 

Online IO 
Complete 

Burst 2 
Start 

Long response 

times 

Online IO 
Complete 

Burst 1 
Start 

Online IO 
Complete 

Short response times 

Short response times 
Online IO 
Complete 

Burst 2 
Start 

Figure 2: Reducing response time. Because Burst ORAM (right) does much less online IO than ObliviStore (left)
and delays ofﬂine IO, it is able to respond to ORAM requests much faster. In this (overﬂy simpliﬁed) illustration, the
bandwidth capacity is enough to transfer 4 blocks concurrently. Both ORAM systems do the same amount of IO.

Server 

ObliviStore 

Server 

Burst ORAM 

O(log N)  
blocks 
Client 

XOR 

1 block 

Client 

Figure 3: Reducing online cost.
In ObliviStore (left)
the online bandwidth cost is O(logN) blocks of IO on
average. In Burst ORAM (right), we reduce online IO to
only one block, improving handling of bursty trafﬁc.

cusses how to delay shufﬂe IO even further. Section 6
details changes from the ObliviStore design required to
avoid deadlock and fully utilize client space.

When available bandwidths are large and bursts are
short, the response time saved by prioritizing online IO
is limited, as most IO needed for the burst can be issued
in parallel. However, when bandwidth is limited or bursts
are long, the savings can be substantial. With shufﬂe IO
delayed until idle times, online IO dominates the effec-
tive IO, becoming the bottleneck during bursts. Thus we
can further reduce response times by reducing online IO.
4.2 XOR Technique: Reducing Online IO
We introduce a new mechanism called the XOR tech-
nique that allows the Burst ORAM server to combine the
O(logN) blocks fetched during a request into a single
block that is returned to the client (Figure 3 right), re-
ducing the online bandwidth cost to O(1).

If we fetched only the desired block, we would reveal
its identity to the server. Instead, we XOR all the blocks
together and return the result. Since there is at most one
real block among the O(logN) returned, the client can
locally reconstruct the dummy block values and XOR

them with the returned block to recover the encrypted
real block. XOR technique steps are shown in Figure 4.
4.2.1 XOR Technique Details
In Burst ORAM, as in ObliviStore, each request needs to
retrieve a block from a single partition, which is a sim-
pliﬁed hierarchical ORAM resembling those in [9]. The
hierarchy contains L ≈ 1
2 log2 N levels with real-block ca-
pacities 1,2,4, . . . ,2 L−1 respectively.
To retrieve a requested block, the client must fetch ex-
actly one block from each of the L levels. The XOR
technique requires that the client be able to reconstruct
dummy blocks, and that dummies remain indistinguish-
able from real blocks. We achieve this property by en-
crypting a real block b residing in partition p, level (cid:29),
and offset off as AESskp,(cid:29) (off||B). We encrypt a dummy
block residing in partition p, level (cid:29), and offset off as
AESskp,(cid:29)(off). The key skp,(cid:29) is speciﬁc to partition p and
level (cid:29), and is randomized every time (cid:29) is rebuilt.

For simplicity, we start by considering the case with-
out early shufﬂe reads. In this case, exactly one of the L
blocks requested is the encryption of a real block, and the
rest are encryptions of dummy blocks. The server XORs
all L encrypted blocks together into a single block XQ that
it returns to the client. The client knows which blocks are
dummies, and knows p, (cid:29),off for each block, so it recon-
structs all the encrypted dummy blocks and XORs them
with XQ to obtain the encrypted requested/real block.
4.2.2 Handling early shufﬂe reads
An early shufﬂe read occurs when we need to read from
a level with no more than half its original blocks remain-
ing. Since such early shufﬂe reads may be real blocks,
they cannot be included in the XOR. Fortunately, the
number of blocks in a level is public, so the server al-
ready knows which levels will cause early shufﬂe reads.
Thus, the server simply returns early shufﬂe reads indi-
vidually, then XORs the remaining blocks, leaking no
information about the access sequence.

USENIX Association  

23rd USENIX Security Symposium  753

5

1. Client issues block requests to server, one per level
2. Server, to satisfy request

(a) Retrieves and returns early shufﬂe reads
(b) XORs remaining blocks together into single

combined block and returns it
3. Client, while waiting for response

(a) Regenerates encrypted dummy block for each

non-early-shufﬂe-read

(b) XORs all dummies to get subtraction block

4. Client receives combined block from server and
XORs with subtraction block to get requested block

5. Client decrypts requested block

Figure 4: XOR Technique Steps

Since each early shufﬂe read block must be transferred
individually, early shufﬂe reads increase online IO. For-
tunately, early shufﬂe reads are rare, even while shufﬂing
is suppressed during bursts, so the online bandwidth cost
stays under 2X and near 1X in practice (see Figure 7).
4.2.3 Comparison with ObliviStore
ObliviStore uses level compression to reduce shufﬂe IO.
When the client uploads a level to the server, it ﬁrst com-
presses the level down to the combined size of the level’s
real blocks. Since half the blocks are dummies, half the
upload shufﬂe IO is eliminated. For details on level com-
pression and its security, see [24].

Unfortunately, Burst ORAM’s XOR technique is in-
compatible with level compression due to discrepan-
cies in the ways dummy blocks must be formed. The
XOR technique requires that the client be able to recon-
struct dummy blocks locally, so in Burst ORAM, each
dummy’s position determines its contents. In level com-
pression, each level’s dummy block contents are a func-
tion of the level’s real block contents. Since the client
cannot know the contents of all real blocks in the level, it
cannot reconstruct the dummies locally.

Level compression and the XOR technique yield com-
parable overall IO reductions, though level compression
performs slightly better. For example, the experiment
in Figure 8 incurs roughly 23X and 26X overall band-
width cost using level compression and XOR respec-
tively. However, the XOR technique reduces online IO,
while level compression reduces ofﬂine IO, so the XOR
technique is more effective at reducing response times.
5 Scheduling and Reducing Shufﬂe IO
In Burst ORAM, once client space ﬁlls, we must start
shufﬂing in order to return blocks to the server and
continue the burst.
If we are not careful about shuf-
ﬂe IO scheduling, we may immediately start doing large
amounts of IO, dramatically increasing response times.
In this section, we show how Burst ORAM schedules

shufﬂe IO so that jobs that free the most client space us-
ing the least shufﬂe IO are prioritized. Thus, at all times,
Burst ORAM issues only the minimum amount of effec-
tive IO needed to continue the burst, keeping response
times lower for longer. We also show how to reduce over-
all IO by locally caching the smallest levels from each
partition. We start by deﬁning shufﬂe jobs.
5.1 Shufﬂe Jobs
In Burst ORAM, as in ObliviStore, shufﬂe IO is divided
into per-partition shufﬂe jobs. Each job represents the
work needed to shufﬂe a partition p and upload blocks
evicted to p. A shufﬂe job is deﬁned by ﬁve entities:

• A partition p to which the job belongs
• Blocks evicted to but not yet returned to p
• Levels to read blocks from
• Levels to write blocks to
• Blocks already read from p (early shufﬂe reads)

Each shufﬂe job moves through three phases:
Creation Phase. We create a shufﬂe job for p when a
block is evicted to p following a request. Every job starts
out inactive, meaning we have not started work on it.
If another block is evicted to p, we update the sets of
eviction blocks and read/write levels in p’s inactive job.
When Burst ORAM activates a job, it moves the job
to the Read Phase, freezing the eviction blocks and
read/write levels. Subsequent evictions to p will create
a new inactive shufﬂe job. At any time, there is at most
one active and one inactive shufﬂe job for each partition.
Read Phase. Once a shufﬂe job is activated, we begin
fetching all blocks still on the server that need to be shuf-
ﬂed. That is, all previously unread blocks from all the
job’s read levels. Once all such blocks are fetched, they
are shufﬂed with all blocks evicted to p and any early
shufﬂe reads from the read levels. Shufﬂing consists
of adding/removing dummies, pseudo-randomly permut-
ing the blocks, and then re-encrypting each block. Once
shufﬂing completes, we move the job to the Write Phase.
Write Phase. Once a job is shufﬂed we begin storing
all shufﬂed blocks to the job’s write levels on the server.
Once all writes ﬁnish, the job is marked complete, and
Burst ORAM is free to activate p’s inactive job, if any.
5.2 Prioritizing Efﬁcient Jobs
Since executing shufﬂe IO delays the online IO needed to
satisfy requests, we can reduce response times by doing
as little shufﬂing as is needed to free up space. The hope
is that we can delay the bulk of the shufﬂing until an idle
period, so that it does not interfere with pending requests.
By the time client space ﬁlls, there will be many par-
titions with inactive shufﬂe jobs. Since we can choose
jobs in any order, we can minimize the up-front shufﬂing
work by prioritizing the most efﬁcient shufﬂe jobs: those

754  23rd USENIX Security Symposium 

USENIX Association

6

that free up the most client space per unit of shufﬂe IO.
The space freed by completing a job for partition p is the
number of blocks evicted to p plus the number of early
shufﬂe reads from the job’s read levels. Thus, we can
deﬁne shufﬂe job efﬁciency as follows:

Job Efﬁciency =

# Evictions + # Early Shufﬂe Reads
# Blocks to Read + # Blocks to Write

Job efﬁciencies vary substantially. Most jobs start with
1 eviction and 0 early shufﬂe reads, so their relative efﬁ-
ciencies are determined strictly by the sizes of the job’s
read and write levels. If the partition’s bottom level is
empty, no levels need be read, and only the bottom must
be written, for an overall IO of 2 an an efﬁciency of 0.5.
If instead the bottom 4 levels are occupied, all 4 levels
must be read, and the 5th level written, for an total of
roughly 15 reads and 32 writes, yielding a much lower
efﬁciency of just over 0.02. Both jobs free equal amounts
of space, but the higher-efﬁciency job uses less IO.

Since small levels are written more often than large
ones, efﬁcient jobs are common. Further, by delaying an
unusually inefﬁcient job, we give it time to accumulate
more evictions. While such a job will also accumulate
more IO, the added write levels are generally small, so
the job’s efﬁciency tends to improve with time. Thus,
prioritizing efﬁcient jobs reduces shufﬂe IO during the
burst, thereby reducing response times.

Unlike Burst ORAM, ObliviStore does not use client
space to delay shufﬂing, so there are fewer shufﬂe jobs to
choose from at any one time. Thus, job scheduling is less
important and jobs are chosen in creation order. Since
ObliviStore is concerned with throughput, not response
times, it has no incentive to prioritize efﬁcient jobs.
5.3 Reducing Shufﬂe IO via Level Caching
Since small, efﬁcient shufﬂe jobs are common, Burst
ORAM spends a lot of time accessing small levels. If
we use client space to locally cache the smallest levels of
each partition, we can eliminate the shufﬂe IO associated
with those levels entirely. Since levels are shufﬂed with
a frequency inversely proportional to their size, each is
responsible for roughly the same fraction of shufﬂe IO.
Thus, we can greatly reduce shufﬂe IO by caching even
a few levels from each partition. Further, since caching
a level eliminates its early shufﬂe reads, which are com-
mon for small levels, caching can also reduce online IO.
We are therefore faced with a tradeoff between using
client space to store requested blocks, which reduces re-
sponse times for short bursts, and using it for local level
caching, which reduces overall bandwidth cost.
5.3.1 Level Caching in Burst ORAM
In Burst ORAM, we take a conservative approach, and
cache only as many levels as are guaranteed to ﬁt in the

Server Storage 

Online IO Prioritization and IO Rate Limiting 

Using Concurrent IO and Local Space Semaphores 

Online IO Reduction 
Via XOR Technique 

Online IO 

Requester 

Requests & 
Responses 

ORAM Main 

Efficient Job First 

Shuffle Prioritization 

Deadlock Avoidance Via 
Shuffle Buffer Semaphore 

Local Level Caching 

Shuffle IO 

Shuffler 

Figure 5: Burst ORAM Architecture. Solid boxes rep-
resent key system components, while dashed boxes rep-
resent functionality and the effects of the system on IO.

Since each level

worst case. More precisely, we identify the maximum
number λ such that the client could store all real blocks
from the smallest λ levels of every partition even if all
were full simultaneously. We cache levels by only up-
dating an inactive job when the number of evictions is
such that all the job’s write levels have index at least λ .
is only occupied half the time,
caching λ levels consumes at most half of the client’s
space on average, leaving the rest for requested blocks.
As we show experimentally in Section 7, level caching
greatly reduces overall bandwidth cost, and can even re-
duce response times since it avoids early shufﬂe reads.
6 Detailed Burst ORAM Design
The Burst ORAM design is based on ObliviStore, but
incorporates many fundamental functional and system-
level changes. For example, Burst ORAM replaces or
revises all the semaphores used in ObliviStore to achieve
our distinct goal of online IO prioritization while main-
taining security and avoiding deadlock. Burst ORAM
also maximizes client space utilization, implements the
XOR technique to reduce online IO, revises the shuf-
ﬂer to schedule efﬁcient jobs ﬁrst, and implements level
caching to reduce overall IO.
6.1 Overall Architecture
Figure 5 presents the basic architecture of Burst ORAM,
highlighting key components and functionality. Burst
ORAM consists of two primary components, the online
Requester and the ofﬂine Shufﬂer, which are controlled
by the main event loop ORAM Main. Client-side mem-
ory allocation is shown in Figure 6.

ORAM Main accepts new block requests (reads and
writes) from the client, and adds them to a Request

USENIX Association  

23rd USENIX Security Symposium  755

7

 

Position Map

 

Shuffle Buffer

Overflow Space

 

Local Space

Level Cache





Figure 6: Burst ORAM Client Space Allocation. Fixed
client space is reserved for the position map and shufﬂe
buffer. A small amount of overﬂow space is needed for
blocks assigned but not yet evicted (data cache in [24]).
Remaining space is managed by Local Space and con-
tains evictions, early shufﬂe reads, and the level cache.

Queue. On each iteration, ORAM Main tries advancing
the Requester ﬁrst, only advancing the Shufﬂer if the Re-
quester needs no IO, thereby prioritizing online IO. The
Requester and Shufﬂer use semaphores (Section 6.2) to
regulate access to network bandwidth and client space.

The Requester reads each request from the Request
Queue,
identiﬁes the desired block’s partition, and
fetches it along with any necessary dummies. To en-
sure oblivious behavior, the Requester must wait until
all dummy blocks have been fetched before marking the
request satisﬁed. All Requester IO is considered online.
The Shufﬂer re-encrypts blocks fetched by the Re-
quester, shufﬂes them with other blocks, and returns
them to the server. The Shufﬂer is responsible for manag-
ing shufﬂe jobs, including prioritizing efﬁcient jobs and
implementing level caching. All IO initiated by the shuf-
ﬂer is considered ofﬂine or shufﬂe IO.
6.2 Semaphores
Resources in Burst ORAM are managed via semaphores,
as in ObliviStore. Semaphores are updated using only
server-visible information, so ORAM can safely base its
behavior on semaphores without revealing new informa-
tion. Since Burst ORAM gives online IO strict priority
over shufﬂe IO, our use of semaphores is substantially
different than ObliviStore’s, which tries to issue the same
amount of IO after each request. ObliviStore uses four
semaphores: Shufﬂing Buffer, Early Cache-ins, Eviction,
and Shufﬂing IO. In Burst ORAM, we use three:

• Shufﬂe Buffer manages client space reserved for
blocks from active shufﬂe jobs, and differs from
ObliviStore’s Shufﬂing Buffer only in initial value.
• Local Space manages all remaining space, com-
bining ObliviStore’s Early Cache-in and Eviction
semaphores.

• Concurrent IO manages concurrent block trans-
fers based on network link capacity, preventing
It dif-
the Shufﬂer from starving the Requester.

fers fundamentally from ObliviStore’s Shufﬂing IO
semaphore, which manages per-request shufﬂe IO.
Shufﬂe Buffer semaphore.
Shufﬂe Buffer gives the
number of blocks that may be added to the client’s shuf-
ﬂe buffer. We initialize it to double the maximum parti-
tion size (under 2.4√N total for N > 210), to ensure that
the shufﬂe buffer is large enough to store at least two in-
progress shufﬂe jobs. When Shufﬂe Buffer reaches 0, the
Shufﬂer may not issue additional reads.
Local Space semaphore. Local Space gives the num-
ber of blocks that may still be stored in remaining client
space (space not reserved for the position map or shufﬂe
buffer). If Local Space is 0, the Requester may not fetch
more blocks. Blocks fetched by the Requester count to-
ward Local Space until their partition’s shufﬂe job is ac-
tivated and they are absorbed into Shufﬂe Buffer. Once a
block moves from Local Space to Shufﬂe Buffer, it is con-
sidered free from the client, and more requests may be
issued. The more client space, the higher Local Space’s
initial value, and the better our burst performance.
Concurrent IO semaphore. Concurrent IO is initial-
ized to the network link’s block capacity. Queuing a
block transfer decrements Concurrent IO, and complet-
ing a transfer increments Concurrent IO. The Shufﬂer
may only initiate a transfer if Concurrent IO > 0. How-
ever, the Requester may continue to initiate transfers and
decrement Concurrent IO even if it is negative. This
mechanism ensures that no new shufﬂe IO starts while
there is sufﬁcient online IO to fully utilize the link. If no
online IO starts, Concurrent IO eventually becomes pos-
itive, and shufﬂe IO resumes, ensuring full utilization.
6.3 Detailed System Behavior
We now describe the interaction between ORAM Main,
the Requester, the Shufﬂer, and the semaphores in detail.
Accompanying pseudocode can be found in Appendix A.
ORAM Main (Algorithm 1). Incoming read and write
requests are asynchronously added to the Request Queue.
During each iteration, ORAM Main ﬁrst tries to advance
the Requester, which attempts to satisfy the next request
from the Request Queue. If the queue is empty, or Local
Space too low, ORAM Main advances the Shufﬂer in-
stead. This mechanism suppresses new shufﬂe IO during
a new burst of requests until the Requester has fetched as
many blocks as possible.

For each request, we evict v blocks to randomly cho-
sen partitions, where v is the eviction rate, set to 1.3 as
in ObliviStore [23]. When evicting, if the Requester has
previously assigned a block to be evicted to partition p,
then we evict that block. If there are no assigned blocks,
then to maintain obliviousness we evict a new dummy
block instead. Eviction does not send a block to the
server immediately. It merely informs the Shufﬂer that

756  23rd USENIX Security Symposium 

USENIX Association

8

the block is ready to be shufﬂed into p.
Requester (Algorithm 2). To service a request, the Re-
quester ﬁrst identiﬁes the partition and level containing
the desired block. It then determines which levels require
early shufﬂe reads, and which need only standard reads.
If Local Space is large enough to accommodate the re-
trieved blocks, the requester issues an asynchronous re-
quest for the necessary blocks Else, control returns to
ORAM Main, giving the Shufﬂer a chance to free space.
The server asynchronously returns the early shufﬂe
read blocks and a single combined block obtained from
all standard-read blocks using the XOR technique (Sec-
tion 4). The Requester extracts the desired block from
the combined block or from an early shufﬂe read block,
then updates the block (write) or returns it to the client
(read). The Requester then assigns the desired block for
eviction to a randomly chosen partition.
Shufﬂer (Algorithm 3). The Shufﬂer may only proceed
if Concurrent IO > 0. Otherwise, there is pending on-
line IO, which takes priority over shufﬂe IO, so control
returns to ORAM Main without any shufﬂing.

The Shufﬂer places shufﬂe jobs into three queues
based on phase. The New Job Queue holds inactive jobs,
prioritized by efﬁciency. The Read Job Queue holds ac-
tive jobs for which some reads have been issued, but not
all reads are complete. The Write Job Queue holds active
jobs for which all reads, not writes, are complete.

If all reads have been issued for all jobs in the Read
Job Queue, the Shufﬂer activates the most efﬁcient job
from the New Job Queue, if any. Activating a job moves
it to the Read Job Queue and freezes its read/write lev-
els, preventing it from being updated by subsequent evic-
tions. It also moves the job’s eviction and early shufﬂe
read blocks from Local Space to Shufﬂe Buffer, freeing
up Local Space to handle online requests. By ensuring
that all reads for all active jobs are issued before activat-
ing new jobs, we avoid hastily activating inefﬁcient jobs.
The Shufﬂer then tries to decrement Shufﬂe Buffer to
determine whether a shufﬂe read may be issued. If so,
the Shufﬂer asynchronously fetches a block for a job in
the Read Job Queue. If not, the Shufﬂer asynchronously
writes a block from a job in the Write Job Queue instead.
Unlike reads, writes do not require Shufﬂe Buffer space,
so they can always be issued. The Shufﬂer prioritizes
reads since they are critical prerequisites to activating
new jobs and freeing up Local Space. The equally costly
writes can be delayed until Shufﬂe Buffer space runs out.
Once all reads for a job complete, the job is shufﬂed:
dummy blocks are added as needed, then all are per-
muted and re-encrypted. We then move the job to the
Write Job Queue. When all writes ﬁnish, we mark the
job complete and remove it from the Write Job Queue.

6.4 Burst ORAM Security
We assume the server knows public information such as
the values of each semaphore and the start and end times
of each request. The server also knows the level conﬁg-
uration of each partition and the size and phase of each
shufﬂe job, including which encrypted blocks have been
read from and written to the server. We must prevent
the server from learning the contents of any encrypted
block, or anything about which plaintext block is being
requested. Thus, the server may not know the location of
a given plaintext block, or even the prior location of any
previously requested encrypted block.

All of Burst ORAM’s publicly visible actions are, or
appear to the server to be, independent of the client’s sen-
sitive data access sequence. Since Burst ORAM treats
the server as a simple block store, the publicly visi-
ble actions consist entirely of deciding when to transfer
which blocks. Intuitively, we must show that each action
taken by Burst ORAM is either deterministic and depen-
dent only on public information, or appears random to
the server. Equivalently, we must be able to generate a
sequence of encrypted block transfers that appears in-
distinguishable from the actions of Burst ORAM using
only public information. We now show how each Burst
ORAM component meets these criteria.
ORAM Main & Client Security. ORAM Main (Algo-
rithm 1) chooses whether to advance the Requester or the
Shufﬂer, and depends on the size of the request queue
and the Local Space semaphore. Since the number of
pending requests and the semaphores are public, ORAM
Main is deterministic and based only on public informa-
tion. For each eviction, the choice of partition is made
randomly, and exactly one block will always be evicted.
Thus, every action in Algorithm 1 is either truly random
or based on public information, and is trivial to simulate.
Requester Security. The Requester (Algorithm 2) must
ﬁrst identify the partition containing a desired block.
Since the block was assigned to the partition randomly
and this is the ﬁrst time it is being retrieved since it was
assigned, the choice of partition appears random to the
server. Within each partition, the requester determin-
istically retrieves one block from each occupied level.
The choice from each level appears random, since blocks
were randomly permuted when the level was created.

The Requester singles out early shufﬂe reads and re-
turns them individually. The identity of levels that re-
turn early shufﬂe reads is public, since it depends on the
number of blocks in the level. The remaining blocks are
deterministically combined using XOR into a single re-
turned block. Finally, the request is marked satisﬁed only
after all blocks have been returned, so request completion
time depends only on public information.

The Requester’s behavior can be simulated using only

USENIX Association  

23rd USENIX Security Symposium  757

9

public information by randomly choosing a partition and
randomly selecting one block from each occupied level.
Blocks from levels with at most half their original blocks
remaining should be returned individually, and all others
combined using XOR and returned. Once all blocks have
been returned, the request is marked satisﬁed.
Shufﬂer Security. As in ObliviStore, Shufﬂer (Algo-
rithm 3) operations depend on public semaphores. Job
efﬁciency, which we use for prioritizing jobs, depends on
the number of blocks to be read and written to perform
shufﬂing, as well as the number of early shufﬂe reads
and blocks already evicted (not assigned). The identity
of early shufﬂe read levels and the number of evictions is
public. Further, the number of reads and writes depends
only on the partition’s level conﬁguration. Thus, job efﬁ-
ciency and job order depend only on public information.
Since the Shufﬂer’s actions are either truly random (e.g.
permuting blocks) or depend only on public information
(i.e. semaphores), it is trivial to simulate.
Client Space. Since fetched blocks are assigned ran-
domly to partitions, but evicted using an independent
process, the number of blocks awaiting eviction may
grow. The precise number of such blocks may leak in-
formation about where blocks were assigned, so it must
be kept secret, and the client must allocate a ﬁxed amount
of space dedicated to storing such blocks (see Overﬂow
Space in Figure 6). ObliviStore [23] relies on a proba-
bilistic bound on overﬂow space provided in [24]. Since
Burst ORAM uses ObliviStore’s assignment and evic-
tion processes, the bound holds for Burst ORAM as well.
Level caching uses space controlled by the Local Space
semaphore, so it depends only on public information.
7 Evaluation
We ran simulations comparing response times and band-
width costs of Burst ORAM with ObliviStore and an in-
secure baseline, using real and synthetic workloads.
7.1 Methodology
7.1.1 Baselines
We compare Burst ORAM and its variants against two
baselines. The ﬁrst is the ObliviStore ORAM described
in [23], including its level compression optimization. For
fairness, we allow ObliviStore to use extra client space
to locally cache the smallest levels in each partition. The
second baseline is an insecure scheme without ORAM
in which blocks are encrypted, but access patterns are
not hidden. It transfers exactly one block per request.

We evaluate Burst ORAM against ObliviStore since
ObliviStore is the most bandwidth-efﬁcient existing
ORAM scheme. Other schemes require less client stor-
age [25], but incur higher bandwidth costs, and thus
would yield higher response times. We did not include

We explicitly measure online, effective, and overall
bandwidth costs.
In the insecure baseline, all are 1X,
so response times are minimal. However, if a burst has
high enough frequency to saturate available bandwidth,
requests may still pile up, yielding large response times.
7.1.3 Workloads
We use three workloads. The ﬁrst consists of an end-
less burst of requests all issued at once, and compares
changes in bandwidth costs of each scheme as a func-
tion of burst length. The second consists of two identi-
cal bursts with equally-spaced requests, separated by an
idle period. It shows how response times change in each
scheme before and after the idle period.

The third workload is based on the NetApp Dataset [2,
14], a corporate workload containing ﬁle system accesses
from over 5000 corporate clients and 2500 engineering
clients during 100 days. The ﬁle system uses 22TB of its
31TB of available space. More details about the work-
load are provided in the work by Leung et al. [14].

results from Path-PIR [17] because it requires substan-
tially larger block sizes to be efﬁcient, and its response
times are dominated by the orthogonal consideration of
PIR computation. Path-PIR reports response times in the
40–50 second range for comparably-sized databases.
7.1.2 Metrics
We evaluate Burst ORAM and our baselines using re-
sponse time and bandwidth cost as metrics (see Section
2). We measure average, maximum, and p-percentile re-
sponse times for various p. A p-percentile response time
of t seconds indicates that p percent of the requests were
satisﬁed with response times under t seconds.

Our NetApp workload uses a 15 day period (Sept. 25
through Oct. 9) during which corporate and engineering
clients were active. Requested chunk sizes range from a
few bits to 64KB, with most at least 4KB [14]. Thus, we
chose a 4KB block size. In total, 312GB of data were
requested using 8.8· 107 4KB queries.
We conﬁgure the NetApp workload ORAM with a
32TB capacity, and allow 100GB of client space, for a
usable storage increase of 328 times. For Burst ORAM
and ObliviStore, at least 33GB is consumed by the posi-
tion map, and only 64GB is used for local block storage.
The total block count is N = 233. Blocks are divided into
(cid:30)217/3(cid:29) partitions to maximize server space utilization,
each with an upper-bound partition size of 218 blocks.
7.2 Simulator
We evaluated Burst ORAM’s bandwidth costs and re-
sponse times using a detailed simulator written in Java.
The simulator creates an asynchronous event for each
block to be transferred. We calculate the transfer’s ex-
pected end time from the network latency, the network
bandwidth, and the number of pending transfers.

758  23rd USENIX Security Symposium 

USENIX Association

10

Our simulator also measures results for ObliviStore
and the insecure baseline. In all schemes, block requests
are time-stamped as soon as they arrive, and serviced as
soon as possible. Requests pile up indeﬁnitely if they
arrive more frequently than the scheme can handle them.
Burst ORAM’s behavior is driven by semaphores and
appears data-independent to the server. Each request
reads from a partition that appears to be chosen uni-
formly at random, so bandwidth costs and response times
depend only on request arrival times, not on requested
block IDs or contents. Thus, the simulator need only
store counters representing the number of remaining
blocks in each level of each partition, and can avoid stor-
ing block IDs and contents explicitly.

Since the simulator need not represent blocks individ-
ually, it does not measure the costs of encryption, look-
ing up block IDs, or performing disk reads for blocks.
Thus, measured bandwidth costs and response times de-
pend entirely on network latency, bandwidth capacity, re-
quest arrival times, and the scheme itself.
7.2.1 Extrapolating Results to Real-World Settings
Burst ORAM can achieve near-optimal performance for
realistic bursty trafﬁc patterns.
In particular, in many
real-life cases bandwidth is overprovisioned to ensure
near-optimal response time under bursts – for the inse-
cure baseline. However, in between bursts, most of the
bandwidth is not utilized. Burst ORAM’s idea is leverag-
ing the available bandwidth in between bursts to ensure
near-optimal response time during bursts.

Our simulation applies mainly to scenarios where the
client-server bandwidth is the primary bandwidth bottle-
neck (i.e., client-server bandwidth is the narrowest pipe
in the system), which is likely to be the case in a real-life
outsourced storage scenario, such as a corporate enter-
prise outsourcing its storage to a cloud provider. While
the simulation assumes that there is a single server, in
practice, the server-side architecture could be more com-
plicated and involve multiple servers interacting with
each other. But as long as server-server bandwidth is not
the bottleneck, our simulation results would be applica-
ble. Similarly, we assume that the server’s disk band-
width is not a bottleneck. This is likely the case if fast
Solid State Drives (SSD) are employed. For example,
assuming 4KB blocks and only one such array of SSDs
with a 100µs random 4KB read latency, our single-array
throughput limits us to satisfying 10,000 requests per
second. In contrast, even a 1Gbps network connection
lets us satisfy only 32,000 requests per second. Thus,
with even six such arrays (3log2 N SSDs total), assign-
ing roughly √N/6 partitions to each array, we can expect
the client-server network to be the bottleneck.

Other than bandwidth, another factor is inherent sys-
tem latencies, e.g., network round-trip times, or inherent

 

i

 
t
s
o
C
h
t
d
w
d
n
a
B
e
n

 

i
l

n
O

Endless Burst - Online Bandwidth Cost 

(32 TB ORAM, 100 GB client storage) 

Burst ORAM 
ObliviStore 
Burst ORAM No Job Prioritization 
Burst ORAM No Level Caching 

overhead peaks from extra 
early cache-ins when most 

shuffling jobs delayed 

6X 
5X 
4X 
3X 
2X 
1X 
0X 

1E+0 

1E+2 

1E+4 
Request Index 

1E+6 

1E+8 

1E+10 

Figure 7: Online bandwidth costs as a burst lengthens.
Burst ORAM maintains low online cost regardless of
burst length, unlike ObliviStore.

Endless Burst - Effective Bandwidth Cost 

(32 TB ORAM, 100 GB client storage) 

Burst ORAM 
ObliviStore 
Burst ORAM No Job Prioritization 
Burst ORAM No Level Caching 

 

i

 
t
s
o
C
h
t
d
w
d
n
a
B
e
v
i
t
c
e
f
f
E

 

40X 
35X 
30X 
25X 
20X 
15X 
10X 
5X 
0X 

prioritizing efficient 
jobs defers shuffling 

client space full, 
shuffling begins 

1E+0 

1E+2 

1E+4 
Request Index 

1E+6 

effective cost 
converges to 
overall cost 

1E+8 

1E+10 

Figure 8: Effective bandwidth costs as burst grows. Burst
ORAM handles most bursts with ∼1X effective cost. Ef-
fective costs converge to overall costs for long bursts.

disk latencies. Under the same overall bandwidth conﬁg-
uration, increased latency is unlikely to affect the near-
optimality of Burst ORAM– while they would increase
Burst ORAM’s total response times, we would expect a
comparable increase in response times for the insecure
baseline.
7.3 Endless Burst Experiments
For the endless burst experiments, we use a 32TB
ORAM with N = 233 4KB blocks and 100GB client
space. We issue 233 requests at once, then start satisfy-
ing requests in order using each scheme. We record the
bandwidth costs of each request, averaged over requests
with similar indexes and over three trials. Figures 7 and
8 show online and effective costs, respectively. The in-
secure baseline is not shown, since its online, effective,
and overall bandwidth costs are all 1.

Figure 7 shows that Burst ORAM maintains 5X–
6X lower online cost than ObliviStore for bursts of all
lengths. When Burst ORAM starts to delay shufﬂing, it
incurs more early shufﬂe reads, increasing online cost,
but stays well under 2X on average. Burst ORAM effec-
tive costs can be near 1X because writes associated with
requests are not performed until blocks are shufﬂed.

Burst ORAM defers shufﬂing, so its effective cost
stays close to its online cost until client space ﬁlls, while
ObliviStore starts shufﬂing immediately, so its effective

USENIX Association  

23rd USENIX Security Symposium  759

11

cost stays constant (Figure 8). Thus, response times for
short bursts will be substantially lower in Burst ORAM
than in ObliviStore.

Eventually, client space ﬁlls completely, and even
Burst ORAM must shufﬂe continuously to keep up with
incoming requests. This behavior is seen at the far right
of Figure 8, where each scheme’s effective cost con-
verges to its overall cost. Burst ORAM’s XOR tech-
nique results in slightly higher overall cost than Oblivi-
Store’s level compression, so Burst ORAM is slightly
less efﬁcient for very long bursts. Without local level
caching, Burst ORAM spends much more time shuf-
ﬂing the smallest levels, yielding the poor performance
of Burst ORAM No Level Caching.

If shufﬂe jobs are started in arbitrary order, as for Burst
ORAM No Prioritization, the amount of shufﬂing per re-
quest quickly increases, pushing effective cost toward
overall cost. However, by prioritizing efﬁcient shufﬂe
jobs as in Burst ORAM proper, more shufﬂing can be
deferred, keeping effective costs lower for longer, and
maintaining shorter response times.
7.4 Two-Burst Experiments
Our Two-Burst experiments show how each scheme re-
sponds to idle time between bursts. We show that Burst
ORAM uses the idle time effectively, freeing up as much
client space as possible. The longer the gap between
bursts, the longer Burst ORAM maintains low effective
costs during Burst 2.

Figure 9 shows response times during two closely-
spaced bursts, each of ∼ 227 requests spread evenly over
72 seconds. The ORAM holds N = 225 blocks, and
the client has space for 218 blocks. Since we must also
store early shufﬂe reads and reserve space for the shuf-
ﬂe buffer, the client space is not quite enough to accom-
modate a single burst entirely. We simulate a 100Mbps
network connection with 50ms latency.

All ORAMs start with low response times during
Burst 1. ObliviStore response times quickly increase due
to ﬁxed shufﬂe work between successive requests. Burst
ORAMs delay shufﬂe work, so response times stay low
until client space ﬁlls. Without level caching, additional
early shufﬂe reads cause early shufﬂing and thus pre-
mature spikes in response times.

When Burst 1 ends, the ORAMs continue working,
satisfying pending requests and catching up on shufﬂing
during the idle period. Longer idle times allow more
shufﬂing and lower response times at the start of Burst
2. None of the ORAMs have time to fully catch up, so
response times increase sooner during Burst 2. ObliviS-
tore cannot even satisfy all Burst 1 requests before Burst
2 starts, so response times start high on Burst 2. Burst
ORAM does satisfy all Burst 1 requests, so it uses freed
client space to efﬁciently handle early Burst 2 requests.

 
)
s

m

 

i

(
 
e
m
T
e
s
n
o
p
s
e
R
 
t
s
e
u
q
e
R

 
)
s

m

 

i

(
 
e
m
T
e
s
n
o
p
s
e
R
 
t
s
e
u
q
e
R

 
)
s

m

 

i

(
 
e
m
T
e
s
n
o
p
s
e
R
 
t
s
e
u
q
e
R

1E+7 
1E+6 
1E+5 
1E+4 
1E+3 
1E+2 
1E+1 
1E+0 

1E+7 
1E+6 
1E+5 
1E+4 
1E+3 
1E+2 
1E+1 
1E+0 

1E+7 
1E+6 
1E+5 
1E+4 
1E+3 
1E+2 
1E+1 
1E+0 

Two-Burst Response Times 

80s Split 

Burst ORAM 
ObliviStore 
Burst ORAM No Job Prioritization 
Burst ORAM No Level Caching 

150 

200 

250 

100s Split 

Burst 1 
50 

0 

Gap 

Burst 2 
  

100 

Time Request Issued (seconds from start) 

Burst 1 
50 

Gap 

100 

0 

Burst 2 
  
150 

Time Request Issued (seconds from start) 

200 

250 

200s Split 

Burst ORAM response 
times low until client 

space fills again 

Burst 1 
50 

0 

Gap 

Burst 2 

100 

150 

200 

250 

Time Request Issued (seconds from start) 

Figure 9: Response times during two same-size bursts
of just over 217 requests spread evenly over 72 seconds.
Client has space for at most 218 blocks. No level caching
causes early spikes due to extra early shufﬂe reads.

Clearly, Burst ORAM performs better with shufﬂe pri-
oritization, as it allows more shufﬂing to be delayed to
the idle period, satisfying more requests quickly in both
bursts. Burst ORAM also does better with local level
caching. Without level caching, we start with more avail-
able client space, but the extra server levels yield more
early shufﬂe reads to store, ﬁlling client space sooner.
7.5 NetApp Workload Experiments
The NetApp experiments show how each scheme per-
forms on a realistic, bursty workload. Burst ORAM ex-
ploits the bursty request patterns, minimizing online IO
and delaying shufﬂe IO to achieve near-optimal response
times far lower than ObliviStore’s. Level caching keeps
Burst ORAM’s overall bandwidth costs low.

Figure 10 shows 99.9-percentile response times for
several schemes running the 15-day NetApp workload
for varying bandwidths. All experiments assume a 50ms
network latency. For most bandwidths, Burst ORAM re-
sponse times are orders of magnitude lower than those

760  23rd USENIX Security Symposium 

USENIX Association

12

99.9% Reponse Time Comparison on NetApp Trace
(50ms network latency, 32 TB ORAM, 100 GB client storage)

Burst ORAM
ObliviStore
Burst ORAM No Job Prioritization
Burst ORAM No Level Caching
Without ORAM (Optimal)

0

1

2

3

4

5

6

7

8

9

Available Bandwidth (Gpbs)

(Enlarged Sub-region)

 
)
s

m

(
 
d
a
e
h
r
e
v
O
e
s
n
o
p
s
e
R

 

1E+8 
1E+7 
1E+6 
1E+5 
1E+4 
1E+3 
1E+2 
1E+1 
1E+0 
1E-1 

1E+9
1E+8
1E+7
1E+6
1E+5
1E+4
1E+3
1E+2
1E+1
1E+0

)
s

m

(
 
 

 

i

e
m
T
e
s
n
o
p
s
e
R
%
9
.
9
9

 

140
130
120
110
100
90
80
80
70
70
60
60
50
40

1E+7 

 
)
s

m

(
 
 

 

i

e
m
T
e
s
n
o
p
s
e
R

1E+6 

1E+5 

1E+4 

1E+3 

1E+2 

1E+1 

1E+0 

Baseline Percentile Reponse Times, NetApp Trace 

Highest 
Bandwidths 
with Baseline 
over 100ms 

90% 
99% 
99.9% 
50ms Network Latency 

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

1.1 

1.2 

Available Bandwidth (Gpbs) 

Percentile Burst ORAM Overhead, NetApp Trace 

Corresponding 
ORAM Overheads 
under 100ms 

90% 
99% 
99.90% 
50ms Network Latency 

0.3

0.4

0.5

0.6

0.7

0.8

0.9

Figure 10: (Top) Burst ORAM achieves short response
times in bandwidth-constrained settings. Since Oblivi-
Store has high effective cost, it requires more available
client-server bandwidth to achieve short response times.
(Bottom) Burst ORAM response times are comparable
to those of the insecure (without ORAM) scheme.

of ObliviStore and comparable to those of the insecure
baseline. Shufﬂe prioritization and level caching notice-
ably reduce response times for bandwidths under 1Gbps.
Figure 11 compares p-percentile response times for
p values of 90%, 99%, and 99.9%.
It gives absolute
p-percentile response times for the insecure baseline,
and differences between the insecure baseline and Burst
ORAM p-percentile response times (Burst ORAM over-
head). When baseline response times are low, Burst
ORAM response times are also low across multiple p.

The NetApp dataset descriptions [2, 14] do not spec-
ify the total available network bandwidth, but since it
was likely sufﬁcient to allow decent performance, we ex-
pect from Figure 10 that it was at least between 200Mbps
and 400Mbps. Figure 12 compares the overall bandwidth
costs incurred by each scheme running the NetApp work-
load at 400Mbps. Costs for other bandwidths are simi-
lar. Burst ORAM clearly achieves an online cost several
times lower than ObliviStore’s.

Level caching reduces Burst ORAM’s overall cost
from 42X to 29X. Burst ORAM’s higher cost is due to a
combination of factors needed to achieve short response
times. First, Burst ORAM uses the XOR technique,
which is less efﬁcient overall than ObliviStore’s mutu-
ally exclusive level compression. Second, Burst ORAM
handles smaller jobs ﬁrst. Such jobs are more efﬁcient
in the short-term, but since they frequently write blocks

0 

0.1 

0.2 

0.3 

0.4 

0.5 

0.6 

0.7 

0.8 

0.9 

1 

1.1 

1.2 

Available Bandwidth (Gpbs) 

Figure 11:
(Top) Insecure baseline (no ORAM) p-
percentile response times for various p. (Bottom) Over-
head (difference) between insecure baseline and Burst
ORAM’s p-percentile response times. Marked nodes
show that when baseline p-percentile response times are
< 100ms, Burst ORAM overhead is also < 100ms.

NetApp Trace Bandwidth Costs 

(50ms network latency, 32 TB ORAM, 100 GB client storage, 400Mbps bandwidth) 

 

 
t
s
o
C
h
t
d
w
d
n
a
B

i

40X 

30X 

20X 

10X 

0X 

Offline Cost 
Online Cost 

ObliviStore  Burst ORAM 
without Job 
Prioritization 

Burst ORAM  Burst ORAM 
without Level 

Caching 

Without 
ORAM 
(Optimal) 

Figure 12: To achieve shorter response times, Burst
ORAM incurs higher overall bandwidth cost than Oblivi-
Store, most of which is consumed during idle periods.
Level caching keeps bandwidth costs in check. Job pri-
oritization does not affect overall cost, but does reduce
effective costs and response times (Figures 8, 10).

to small levels, they create more future shufﬂe work. In
ObliviStore, such jobs are often delayed during a large
job, so fewer levels are created, reducing overall cost.
8 Conclusion
We have presented Burst ORAM, a novel Oblivious
RAM scheme based on ObliviStore and tuned for practi-
cal response times on bursty workloads. We presented a

USENIX Association  

23rd USENIX Security Symposium  761

13

novel ORAM architecture for prioritizing online IO, and
introduced the XOR technique for reducing online IO.
We also introduced a novel scheduling mechanism for
delaying shufﬂe IO, and described a level caching mech-
anism that uses extra client space to reduce overall IO.

We simulated Burst ORAM on a real-world workload
and showed that it incurs low online and effective band-
width costs during bursts. Burst ORAM achieved near-
optimal response times that were orders of magnitude
lower than existing ORAM schemes.
Acknowledgements.. This work was supported in part
by grant N00014-07-C-0311 from ONR, the National
Physical Science Consortium Graduate Fellowship; by
NSF under grant number CNS-1314857, a Sloan Re-
search Fellowship, a Google Faculty Research Award;
by the NSF Graduate Research Fellowship under Grant
No. DGE-0946797, a DoD National Defense Science
and Engineering Graduate Fellowship, an Intel award
through the ISTC for Secure Computing, and a grant
from Amazon Web Services.
References
[1] BONEH, D., MAZIERES, D., AND POPA, R. A. Remote
oblivious storage: Making oblivious RAM practical. Manuscript,
http://dspace.mit.edu/bitstream/handle/
1721.1/62006/MIT-CSAIL-TR-2011-018.pdf, 2011.
[2] CHEN, Y., SRINIVASAN, K., GOODSON, G., AND KATZ, R.
Design implications for enterprise storage systems via multi-
dimensional trace analysis. In Proc. ACM SOSP (2011).

[3] CHOW, R., GOLLE, P., JAKOBSSON, M., SHI, E., STADDON,
J., MASUOKA, R., AND MOLINA, J. Controlling data in the
cloud: outsourcing computation without outsourcing control. In
Proc. ACM CCSW (2009), pp. 85–90.

[4] DAMG ˚ARD, I., MELDGAARD, S., AND NIELSEN, J. B. Per-
In TCC

fectly secure oblivious RAM without random oracles.
(2011), pp. 144–163.

[5] DAUTRICH, J., AND RAVISHANKAR, C. Compromising privacy

in precise query protocols. In Proc. EDBT (2013).

[6] FLETCHER, C., VAN DIJK, M., AND DEVADAS, S. Secure
Processor Architecture for Encrypted Computation on Untrusted
Programs.
In Proc. ACM CCS Workshop on Scalable Trusted
Computing (2012), pp. 3–8.

[7] GENTRY, C., GOLDMAN, K., HALEVI, S.,

JULTA, C.,
RAYKOVA, M., AND WICHS, D. Optimizing ORAM and using
it efﬁciently for secure computation. In PETS (2013).

[8] GOLDREICH, O. Towards a theory of software protection and

simulation by oblivious rams. In STOC (1987).

[9] GOLDREICH, O., AND OSTROVSKY, R. Software protection and
simulation on oblivious RAMs. Journal of the ACM (JACM) 43,
3 (1996), 431–473.

[10] GOODRICH, M., AND MITZENMACHER, M. Privacy-preserving
access of outsourced data via oblivious RAM simulation. Au-
tomata, Languages and Programming (2011), 576–587.

[11] GOODRICH, M. T., MITZENMACHER, M., OHRIMENKO, O.,
AND TAMASSIA, R. Privacy-preserving group data access via
stateless oblivious RAM simulation.
In Proc. SODA (2012),
SIAM, pp. 157–167.

[12] ISLAM, M., KUZU, M., AND KANTARCIOGLU, M. Access pat-
tern disclosure on searchable encryption: Ramiﬁcation, attack
and mitigation. In Proc. NDSS (2012).

[13] KUSHILEVITZ, E., LU, S., AND OSTROVSKY, R. On the
(in)security of hash-based oblivious RAM and a new balancing

scheme. In Proc. SODA (2012), SIAM, pp. 143–156.

[14] LEUNG, A. W., PASUPATHY, S., GOODSON, G., AND MILLER,
E. L. Measurement and analysis of large-scale network ﬁle sys-
tem workloads. In Proc. USENIX ATC (2008), USENIX Associ-
ation, pp. 213–226.

[15] LORCH, J. R., PARNO, B., MICKENS, J. W., RAYKOVA, M.,
AND SCHIFFMAN, J. Shroud: Ensuring private access to large-
scale data in the data center. FAST (2013), 199–213.

[16] MAAS, M., LOVE, E., STEFANOV, E., TIWARI, M., SHI, E.,
ASANOVIC, K., KUBIATOWICZ, J., AND SONG, D. PHAN-
TOM: Practical oblivious computation in a secure processor. In
ACM CCS (2013).

[17] MAYBERRY, T., BLASS, E.-O., AND CHAN, A. H. Efﬁcient
In NDSS

private ﬁle retrieval by combining ORAM and PIR.
(2014).

[18] OSTROVSKY, R., AND SHOUP, V. Private information storage

(extended abstract). In STOC (1997), pp. 294–303.

[19] PINKAS, B., AND REINMAN, T. Oblivious RAM revisited. In

CRYPTO (2010).

[20] REN, L., YU, X., FLETCHER, C. W., VAN DIJK, M., AND DE-
VADAS, S. Design space exploration and optimization of path
oblivious RAM in secure processors. In Proc. ISCA. 2013.

[21] SHI, E., CHAN, H., STEFANOV, E., AND LI, M. Oblivious
In Proc. ASIACRYPT

RAM with O((logN)3) worst-case cost.
(2011).

[22] STEFANOV, E., AND SHI, E. Multi-Cloud Oblivious Storage. In

CCS (2013).

[23] STEFANOV, E., AND SHI, E. ObliviStore: High performance
In IEEE Symposium on Security and

oblivious cloud storage.
Privacy (2013).

[24] STEFANOV, E., SHI, E., AND SONG, D. Towards practical obliv-

ious RAM. NDSS, 2012.

[25] STEFANOV, E., VAN DIJK, M., SHI, E., FLETCHER, C., REN,
L., YU, X., AND DEVADAS, S. Path ORAM: An extremely
simple oblivious RAM protocol. In ACM CCS (2013).

[26] WILLIAMS, P., AND SION, R. Sr-oram: Single round-trip obliv-

ious ram. ACNS, industrial track (2012), 19–33.

[27] WILLIAMS, P., SION, R., AND CARBUNAR, B. Building castles
out of mud: practical access pattern privacy and correctness on
untrusted storage. In Proc. ACM CCS (2008), pp. 139–148.

[28] WILLIAMS, P., SION, R., AND TOMESCU, A. PrivateFS: A

parallel oblivious ﬁle system. In CCS (2012).

A Pseudocode
Algorithms 1–4 give pseudocode for Burst ORAM, using
the notation summarized in Table 1. The algorithms are
described in detail in Section 6, but we clarify some of
the code and notation below.

The efﬁciency of shufﬂe job Jp is given by:

EJp =

VJp + AJp
RJp +WJp

.

(1)

Cp represents the state of partition p at the time p’s last
shufﬂe job completed, and determines the current set of
occupied levels in p. Vp represents the number of blocks
that have been evicted to p, since p’s last shufﬂe job com-
pleted. Cp +Vp determines which levels would be occu-
pied if p were to be completely shufﬂed.

VJp represents the number of evicted blocks that will
be shufﬂed into p by Jp. Thus, Cp and VJp together de-
termine Jp’s read and write levels.

If Jp is inactive, it is updated whenever Vp changes,
setting VJp ← Vp (Algorithm 1, Line 25). However, we

762  23rd USENIX Security Symposium 

USENIX Association

14

Table 1: Algorithm Notation

v
λ
p
Vp
Cp
b

Eviction rate: blocks evicted per request
Number of levels cached locally
A partition
# blocks evicted to p since p’s last shufﬂe end
p’s state after last shufﬂe (shufﬂed evictions)
Block ID
Plaintext contents of b
Encrypted contents of b
Server address/ID of b
Partition containing b, or random if none
Level containing b, or ⊥ if none
IDs of standard-read blocks to fetch
IDs of early shufﬂe read blocks to fetch
Combined block (XOR of all blocks in Q)
Subtraction block (XOR of dummies in Q)
Shufﬂe job for p
Number of evicted blocks Jp will shufﬂe
Efﬁciency of Jp
Number of early shufﬂe reads for Jp
Total blocks remaining to be read for Jp
Total blocks to write for Jp

D(b)
E(b)
S(b)
P(b)
L(b)
Q
C
XQ
X(cid:30)Q
Jp
VJp
EJp
AJp
RJp
WJp
NJQ New Job Queue
RJQ Read Job Queue
W JQ Write Job Queue

implement level caching by skipping those updates to Jp
that would cause Jp to write to levels with indexes less
than λ (Algorithm 1, Line 23). Once Jp is active, VJp
is no longer updated. When Jp completes, p’s state is
updated to reﬂect the blocks shufﬂed in by Jp, setting
Cp ← Cp +VJp (Algorithm 3, Line 37).
If p has no inactive shufﬂe job, the job is created fol-
lowing the ﬁrst eviction to p that would allow updating
(Algorithm 1, Line 24). If p has no active job, the inac-
tive job moves to the New Job Queue (NJQ) as soon as
the job is created (Algorithm 1, Line 27), where it stays
until the job is activated. If p does have an active shufﬂe
job, the inactive job is not added to NJQ until the active
job completes (Algorithm 3, Line 38).

Thus, NJQ contains only inactive shufﬂe jobs for
those partitions with no active job, ensuring that any job
in NJQ may be activated. NJQ is a priority queue serving
the most efﬁcient jobs ﬁrst. Job efﬁciency may change
while the job is in NJQ, since VJp can still be updated.
B Reducing Online Costs of SR-ORAM
We now brieﬂy describe how SR-ORAM [26] can beneﬁt
from our XOR technique. Like ObliviStore, SR-ORAM
requires only a single round-trip to satisfy a request, and
has online bandwidth cost O(logN). SR-ORAM uses an

Append b to RequestQueue
On REQUESTCALLBACK(D(b)), return D(b)

Append b to RequestQueue
On REQUESTCALLBACK(D(b)), write d to D(b)

b ← PEEK(RequestQueue)
if FETCH(b) then
RequestMade ← true
POP(RequestQueue)
MAKEEVICTIONS

Algorithm 1 Pseudocode for Client and ORAM Main
1: function CLIENTREAD(b)
2:
3:
4: procedure WRITE(b,d)
5:
6:
7: procedure ORAM MAIN
RequestMade ← f alse
8:
if RequestQueue (cid:28)= /0 then
9:
10:
11:
12:
13:
14:
15:
16:
17: procedure MAKEEVICTIONS
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:

p ← random partition
Evict new dummy or assigned real block to p
Vp = Vp + 1
if shufﬂing p only writes levels ≥ λ then

Jp ← p’s inactive job (cid:30) Create if needed
VJp ← Vp
if p has no active job then

PendingEvictions = PendingEvictions + v
while PendingEvictions ≥ 1 do

if RequestMade = f alse then

TRYSHUFFLEWORK

(cid:30) Request Issued

NJQ = NJQ∪ Jp

28:

PendingEvictions = PendingEvictions− 1

encrypted Bloom ﬁlter to let the server obliviously check
whether each level contains the requested block. The
server retrieves the requested block from its level, and
client-selected dummies from all others. Since at most
one block is real, the server can XOR all the blocks to-
gether and return a single combined block.

One difference in SR-ORAM is that the client does not
know a priori which level contains the requested block.
Thus, SR-ORAM must be modiﬁed to include the level
index of each retrieved block in its response. To al-
low the client to easily reconstruct dummies, we must
also change SR-ORAM to generate the contents of each
dummy block as in Burst ORAM. Since the client knows
the indexes of the dummy blocks it requested from each
level, it can infer the real block’s level from the server’s
response. The client then reconstructs the all dummy
block contents and XORs them with the returned block
to obtain the requested block, as in Burst ORAM.

SR-ORAM is a synchronous protocol, so it has no
notion equivalent to early shufﬂe reads. Thus,
the
XOR technique reduces SR-ORAM’s online bandwidth
cost from O(logN) to 1. The reduction in overall

USENIX Association  

23rd USENIX Security Symposium  763

15

P(b),L(b) ← position map lookup on b
Q = /0,C = /0
for level (cid:30) ∈ P(b) do

Algorithm 2 Pseudocode for Requester
1: function FETCH(b)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:

if (cid:30) is non-empty then
b(cid:30) ← b if (cid:30) = L(b)
b(cid:30) ← ID of next dummy in (cid:30) if (cid:30) (cid:29)= L(b)
if (cid:30) more than half full then
(cid:29) Standard read

Q ← Q∪ S(b(cid:30))
C ← C∪ S(b(cid:30))
Ret ← |C|+ MAX(|Q|,1)
if Not TRYDEC(Local Space, Ret) then

(cid:29) Early shufﬂe read
(cid:29) # blocks to return

else

return f alse (cid:29) Not enough space for blocks

return true

DEC(Concurrent IO, Ret)
Issue asynch. request for (C,Q) to server
When done, server calls:

12:
13:
14:
15:
16:
17:
18:
19:
20: procedure FETCHCALLBACK({E(ci)},XQ)
21:
22:
23:
24:
25:

X(cid:24)Q ← ⊕{E(qi) | S(qi) ∈ Q,qi (cid:29)= b}
E(b) ← XQ ⊕ X(cid:24)Q
E(b) ← E(ci) where ci = b

if b ∈ C then
D(b) ← decrypt E(b)
Assign b for eviction to random partition
REQUESTCALLBACK(D(b))

INC(Concurrent IO, 1)
if b ∈ Q then

26:
27:

28:
29:
30:

FETCHCALLBACK(E(C), XOR of E(Q))

(cid:29) Subtraction block, computed locally

cost is negligible, as SR-ORAM has an ofﬂine cost
O(log2 N loglogN). SR-ORAM contains only one hier-
archy of O(logN) levels, so XOR incurs only O(logN)
extra storage cost for the level-speciﬁc keys, ﬁtting into
SR-ORAM’s logarithmic client storage.

return

TRYACTIVATE

(cid:29) No shufﬂe work

(cid:29) Try to add job to RJQ

if Not TRYDEC(Concurrent IO, 1) then

if Not ReadIssued and Not WriteIssued then

ReadIssued,WriteIssued ← f alse
if All reads for jobs in RJQ issued then

if Jp ∈ RJQ has not issued read bR then
if TRYDEC(Shufﬂe Buffer, 1) then
Issue asynch. request for S(bR)
When done: READCALLBACK(E(bR))
ReadIssued ← true
if !ReadIssued and Jp ∈ W JQ has write bW then
Write E(bW ) to server
When done, call WRITECALLBACK(S(bW ))
WriteIssued ← true
INC(Concurrent IO, 1)

Algorithm 3 Pseudocode for Shufﬂer
1: procedure TRYSHUFFLEWORK
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18: procedure TRYACTIVATE
19:
Jp ← PEEK(NJQ)
20:
if TRYDEC(Shufﬂe Buffer, VJp + AJp) then
21:
Mark Jp active
22:
INC(Local Space, VJp + AJp)
23:
Move Jp from NJQ to RJQ
24:
25: procedure READCALLBACK(E(bR))
26:
27:
28:
29:
30:
31:
32: procedure WRITECALLBACK(S(bW ))
33:
34:
35:
36:
37:
38:

Mark Jp complete
Remove Jp from W JQ
Update Cp ← Cp +VJp,Vp ← Vp −VJp
Add p’s inactive job, if any, to NJQ

INC(Concurrent IO, 1)
Decrypt E(bR), place D(bR) in Shufﬂe Buffer
if all reads in Jp have ﬁnished then

Create dummy blocks to get WJp blocks total
Permute and re-encrypt the blocks
Move Jp from RJQ to W JQ

INC(Concurrent IO, 1)
if all writes in Jp have ﬁnished then

if NJQ (cid:29)= /0 then

(cid:29) Most efﬁcient job

(cid:29) VJp frozen

Algorithm 4 Pseudocode for semaphores
1: procedure DEC(Semaphore,Quantity)
Semaphore ← Semaphore− Quantity
2:
3: procedure INC(Semaphore,Quantity)
Semaphore ← Semaphore + Quantity
4:
5: function TRYDEC(Semaphore,Quantity)
if Semaphore < Quantity then return f alse
6:
DEC(Semaphore,Quantity); return true
7:

764  23rd USENIX Security Symposium 

USENIX Association

16

