Improving Malware Classiﬁcation: Bridging the

Static/Dynamic Gap

Blake Anderson
Los Alamos National

Laboratory

banderson@lanl.gov

Curtis Storlie

Los Alamos National

Laboratory

storlie@lanl.gov

Terran Lane

The University of New Mexico

terran@cs.unm.edu

ABSTRACT
Malware classiﬁcation systems have typically used some ma-
chine learning algorithm in conjunction with either static or
dynamic features collected from the binary. Recently, more
advanced malware has introduced mechanisms to avoid de-
tection in these views by using obfuscation techniques to
avoid static detection and execution-stalling techniques to
avoid dynamic detection. In this paper we construct a clas-
siﬁcation framework that is able to incorporate both static
and dynamic views into a uniﬁed framework in the hopes
that, while a malicious executable can disguise itself in some
views, disguising itself in every view while maintaining ma-
licious intent will prove to be substantially more diﬃcult.
Our method uses kernels to place a similarity metric on each
distinct view and then employs multiple kernel learning to
ﬁnd a weighted combination of the data sources which yields
the best classiﬁcation accuracy in a support vector machine
classiﬁer. Our approach opens up new avenues of malware
research which will allow the research community to ele-
gantly look at multiple facets of malware simultaneously,
and which can easily be extended to integrate any new data
sources that may become popular in the future.

Categories and Subject Descriptors
I.5.2 [Design Methodology]: Classiﬁer design and evalu-
ation; K.6.5 [Security and Protection]: Invasive software
(e.g., viruses, worms, Trojan horses

General Terms
Security, Algorithms, Experimentation

Keywords
Computer Security, Malware, Machine Learning, Multiple
Kernel Learning

1.

INTRODUCTION

In 2010, more than 286 million unique variants of mal-
ware were detected [47]. Despite the majority of this new
malware being created through polymorphism and simple
code obfuscation techniques, and thus being very similar to

Copyright 2012 Association for Computing Machinery. ACM acknowl-
edges that this contribution was authored or co-authored by an employee,
contractor or afﬁliate of the U.S. Government. As such, the Government re-
tains a nonexclusive, royalty-free right to publish or reproduce this article,
or to allow others to do so, for Government purposes only.
AISec’12, October 19, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1664-4/12/10 ...$10.00.

known malware, it will still not be detected by signature-
based anti-virus programs [13, 32]. To meet these challenges,
machine learning techniques have been developed that are
able to learn a generalized description of malware and apply
this knowledge to classify new, unseen instances of malware.
The machine learning techniques for malware classiﬁca-
tion have used a variety of data sources to learn discrimina-
tory functions that are able to diﬀerentiate benign and mali-
cious software. Some of the most popular data sources that
have been examined include binary ﬁles [21, 46], disassem-
bled ﬁles [10, 40], entropy measures on the binary [27], dy-
namic system call traces [9, 18], dynamic instruction traces
[4, 14], and control ﬂow graphs [13, 22].

The novel contribution of this paper is to show how to
combine diﬀerent data sources using multiple kernel learning
[43] to arrive at a new classiﬁcation system that integrates
all of the available information about a program into a uni-
ﬁed framework. The key insight of our approach is that each
data source provides complementary information about the
true nature of a program, but no one data source contains all
of this information. We aim to construct a classiﬁcation sys-
tem that does contain all aspects of a program’s true inten-
tions. We begin by deﬁning a kernel, a positive semi-deﬁnite
matrix where each entry in the matrix is a measure of sim-
ilarity between a pair of instances in the dataset, for each
data source. We then use multiple kernel learning [6, 43] to
ﬁnd the weights of each kernel, create a linear combination
of the kernels, and ﬁnally use a support vector machine [12]
to perform classiﬁcation.

Our framework is particularly appealing for three rea-
sons. First, we are able to elegantly combine both static
and dynamic features in a way which allows the learning al-
gorithm to take advantage of both simultaneously. Second,
our method is extendable in the sense that future popular
data sources could be easily added to the model without
complicating the ﬁnal result. Finally, the method presented
in this paper is highly parallelizable: computing the kernel
values for testing new malware can all be done in parallel,
the implication being that larger datasets can easily be han-
dled.

We present our results on a dataset composed of 776 be-
nign programs and 780 malicious programs. In addition to
this dataset, we test our methods on a separate validation
dataset composed of 20,936 malicious samples. For each
program we collect six data sources: the static binary, the
disassembled binary ﬁle, the control ﬂow graph from the
disassembled binary ﬁle, a dynamic instruction trace, a dy-
namic system call trace, and a ﬁle information feature vector
composed of information gathered from all of the previous
data sources. For the binary ﬁle, disassembled ﬁle, and two

3dynamic traces, we build kernels based on the Markov chain
graphs; for the control ﬂow graph we use a graphlet ker-
nel [41]; and for the ﬁle information feature vector we use a
standard Gaussian kernel as explained in Section 4.

We show that integrating multiple data sources increases
overall classiﬁcation performance with regard to accuracy,
receiver operating characteristic (ROC) curves, and area un-
der the ROC curve (AUC). We also provide results examin-
ing the eﬃcacy of each individual data source with regard
to diﬀerent metrics, including the time from receiving the
sample to making a classiﬁcation decision. We report kernel
combinations (in addition to the combination of all six data
sources) which can achieve reasonably high performance if
time and computing resources are at a premium. We also
demonstrate some of the pitfalls of the dynamic and static
methodologies, such as static data sources having diﬃcul-
ties with packed instances and/or instances with abnormal
entropies.

2. RELATED WORK

There has been a large volume of work applying machine
learning techniques to the malware classiﬁcation problem
[8, 14, 22, 35]. Most of this work has used a single data
source (i.e. static binary or dynamic system call data) and
some set of machine learning algorithms (i.e. support vector
machines or decision trees) to perform classiﬁcation and/or
clustering. Unlike these methods, we incorporate multiple
data sources, both static and dynamic, by using multiple
kernel learning [43] to perform classiﬁcation.

Combining diﬀerent features of static ﬁles has been ex-
amined [28]. Here the authors use n-grams and features
about the static binary to build a series of classiﬁers based
on various machine learning algorithms. They then use an
ensemble learning algorithm to combine the results of the
individual learners. In our work, we also incorporate learn-
ing with dynamic trace data, which has been shown to be
very important for classifying classes of malware which are
packed or obfuscated in other ways [29]. Also, the ﬁnal
combined kernel that we ﬁnd can be used in a kernel-based
clustering algorithm [26] to look at the phylogenetics of the
malware as discussed in Section 6.

Some authors have looked into combining static and dy-
namic features. In [24], the authors build diﬀerent cluster-
ings based on exploits, payloads, malware, and behavioral
features captured by the Anubis framework [5]. Then they
explore the relationships between the diﬀerent clusterings.
This is a results fusion model, whereas we are concerned with
a data fusion model. We do not build separate models for
each type of static and/or dynamic data source, but rather
combines all of the data sources using multiple kernel learn-
ing to arrive at a uniﬁed view of what it means to be mali-
cious. Another key diﬀerence of our approach is that we are
concerned with classiﬁcation and not clustering, with previ-
ous eﬀorts not being suitable for the classiﬁcation domain.
We also base our analysis on diﬀerent types of data. [24] uses
17 static features for the exploit/payload/malware clusters,
whereas we compare the Markov chain graphs based on the
binary and disassembled information, control ﬂow graphs,
and other ﬁle information statistics described in Section 3.
The behavioral features of Anubis are concerned with mod-
iﬁcations to the Windows registry and ﬁle system, or inter-
actions with other processes. Our dynamic analysis is based
on the Markov chain graphs of the dynamic instructions and
system calls performed.
It is important to note that the
framework presented in this paper is general enough to in-

clude the data sources of [24] once an appropriate Kernel
is deﬁned. Finally, we combine the information of the data
sources using a multiple kernel learning framework and build
a single uniﬁed classiﬁcation system, whereas the individual
clusterings found in [24] are based solely on their respective
data sources.

There has been some work done which combines static
and dynamic analysis to locate packed and/or obfuscated
code.
In [36], the authors ﬁrst disassemble the code, and
then run the code looking for sequences of instructions in
the dynamic trace that are not found in the disassembled
data. They are only concerned with locating the hidden code
which could not be disassembled, whereas we are concerned
with classifying instances of malware, whether they have
hidden code or not. We also make use of more data sources,
such as system calls and control ﬂow graphs.

3. DATA SOURCES

In this work, we take advantage of six diﬀerent types of
data with the aim of covering the most popular data sources
that have been used for malware classiﬁcation in the liter-
ature. These data sources also try to capture many of the
diﬀerent views of a program in the hopes that, while a mali-
cious executable can disguise itself in some views, disguising
itself in every view while maintaining malicious intent will
prove to be substantially more diﬃcult. We use three static
data sources: the binary ﬁle, the disassembled binary, and
the control ﬂow graph of the disassembled binary. We use
two dynamic data sources: the dynamic instruction trace
and the dynamic system call trace. Finally, we use a ﬁle
information data source which contains seven statistics that
provide a summary of the previous data sources.

Binary. We use the raw byte information contained in the
binary executable to construct our ﬁrst data source. There
is a long history of using this type of data to classify mal-
ware [21, 46]. Generally, the bytes are used in an n-gram
framework to construct a feature vector that is then given to
some machine learning classiﬁcation algorithm (i.e. boosted
decision trees [21]).
In contrast to these methods, we use
the 2-grams to condition a Markov chain and then perform
classiﬁcation in graph space as explained in Section 4. In
the Markov chain, the byte values (0-255) correspond to dif-
ferent vertices in the graph, and the transition probabilities
are estimated by the frequencies of the 2-grams.

Disassembled. The opcodes of the disassembled program
have also been used to generate malware detection schemes
[10, 40]. To generate the disassembled code, we use IDA
Pro [33]. Once we have the disassembled code, we build a
Markov chain similar to the way we built the Markov chain
for the binary ﬁles.
Instead of the byte values being the
vertices in the graph, we use the disassembled instructions
for the vertices in the graph. Unfortunately, the number of
unique instructions found in the disassembled ﬁles (∼1200)
gave us very large Markov chains that overﬁtted the data
resulting in poor initial performance. This is in part due to
the curse of dimensionality: the feature space becomes too
large and we do not have enough data to suﬃciently con-
dition the model. Furthermore, some instructions perform
identical or very similar tasks, resulting in many transitions
that are identical yet treated as distinct. To combat this, we
used several categorizations, each with increasing complex-
ity. The coarsest categorization contained eight categories
(math, logic, privileged, branch, memory, stack, nop, and

4Statistic
Entropy
Binary Size
Packed
Num Vertices (CFG)
Num Edges (CFG)
Num Static Instrs
Num Dynamic Instrs

Malware

7.52
.799

47.56%
5,829.69
7,189.58
50,982

Benign

6.34
2.678
19.59%

10,938.85
13,929.40

72,845

7,814,452

2,936,335

Table 1: Summary of the ﬁle information statistics used: the
average entropy, average size of the binary (in megabytes),
average number of vertices and edges in the control ﬂow
graph, the average number of instructions in the disassem-
bled ﬁles, and the average number of instructions/system
calls in the dynamic traces. The percentage of ﬁles known
to packed is also given.

using all 237 unique instructions. We found that mapping
each of the 237 unique instructions to 237 unique vertices
provided the best results.

Dynamic System Call Traces. System call traces have
been another popular dynamic data source [9, 18, 38]. Over
the 1556 traces, we recorded 2460 unique system calls. We
used the Markov chain graph representation, and like the
disassembled instruction set, we found that treating each
unique system call as a vertex in the Markov chain led to
poor initial performance. We grouped the system calls into
94 categories where each category represents semantically
similar groups of system calls, such as painting to the screen,
writing to ﬁles, or cryptographic functions.

Miscellaneous File Information. For this data source,
we collected seven pieces of information about the various
data sources described previously. This data is summarized
in Table 1. We look at the entropy and the size of the binary
ﬁle. Similar to previous work on entropy [27, 39], we found
the average entropy of the benign ﬁles in our dataset to be
6.34 and the average entropy of the malicious ﬁles in our
dataset to be 7.52. We also have a binary feature to look
at whether the binary executable has a recognizable packer
such as UPX [49] or Armadillo [48]. To ﬁnd whether a ﬁle
was packed or not, we used the PEID signature method [3].
For the disassembled binary feature, we took the number
of instructions found in the disassembled ﬁle. We also use
the number of edges and the number of vertices in the con-
trol ﬂow graph. Finally, we took the sum of the number of
dynamic instructions and dynamic system calls as the last
feature.

4. METHOD

In this section, we ﬁrst describe how we transform the six
data sources of Section 3 into more convenient representa-
tions. We then show how it is possible to deﬁne kernels,
or similarity measures, that are able to accurately compare
these data sources in their new representations. Finally, we
describe a method of multiple kernel learning that ﬁnds a
linear combination of these kernels which can then be used
in a support vector machine setting.

Data Representations. The six canonical data sources
described in Section 3 can be grouped into three sets. The
miscellaneous ﬁle information that we collect can be repre-
sented as a simple feature vector of length seven where each

Figure 1: An example of a control ﬂow graph demonstrating
jumps.

other). The other categorizations had 34, 68, 77, 86, 154,
and 172 categories. We found the categorization with 86
categories to perform the best. This categorization had sep-
arate categories for most of the initial 8086/8088 instructions
as well as categories for some extended instruction sets such
as SSE and MMX. Further research into an optimal catego-
rization that better represents program behavior is currently
being considered and is discussed in Section 6.

Control Flow Graph. The use of control ﬂow graphs has
become a very popular means to perform malware classiﬁca-
tion [13, 22]. A control ﬂow graph is a graph representation
that models all of the paths of execution that a program
might take during its lifetime (Figure 1).
In the graph,
the vertices are the basic blocks, sequential code without
branches or jump targets, of the program, and the edges
represent the jumps in control ﬂow of the program. One
of the advantages of this representation is that it has been
shown to be very diﬃcult for a polymorphic virus to create a
semantically similar version of itself while modifying its con-
trol ﬂow graph enough to avoid detection [22]. To compute
the similarity between diﬀerent control ﬂow graphs, we use
a simpliﬁed kernel based on previous work in the literature
[22], which works by counting similarly shaped subgraphs of
a certain size. This kernel is explained in detail in Section
4.

Dynamic Instruction Traces. Although static analysis
techniques for malware classiﬁcation have been proven to
perform quite well, it has been shown that these methods can
be evaded by using advanced obfuscation transformations
[29]. Because of these limits to static analysis, we chose to
include two dynamic data sources, the instruction traces and
the system call traces collected over a ﬁve minute run using
the Xen virtual machine [7] and the Intel Pin program [25].
Dynamic instructions traces are known to produce highly
accurate malware classiﬁcation results [4, 14]. Over the 1556
traces, we recorded 237 unique instructions. We built the
Markov chains in the same fashion as the disassembled code,
using the seven categorizations mentioned previously and

5mov
lea
push
or
or
jmp
mov
mov
sub
adc
jc
...

esi, 0x0040C000
edi, [esi-0xB000]

edi

ebp, 0xFF
ebp, 0xFF
0x00419532

ebx, [esi]
ebx, [esi]
esi, 0xFC
ebx, ebx

0x00419528

...

Figure 2: The left table shows an example of the trace data we collect. A hypothetical resulting graph representing a fragment
of the Markov chain is shown on the right. In a real Markov chain graph, all of the out-going edges would sum to 1.

of the seven statistics corresponds to a feature. The control
ﬂow graphs are represented in the same way as the stan-
dard in the literature [13, 22]. For the third set, the raw
binary, disassembled binary, dynamic instruction trace, and
dynamic system call trace, we use a Markov chain represen-
tation.

Given some data source, such as the dynamic instruction
trace P, we are interested in ﬁnding a new representation,
P′, such that we can make uniﬁed comparisons in graph
space while still capturing the sequential nature of the data.
We achieved this by transforming the data into a Markov
chain, which is represented as a weighted, directed graph. A
graph, G = hV, Ei, is composed of two sets, V and E. The
elements of V are vertices and the elements of E are edges.
In our representation, the edge weight, eij, between vertices
i and j corresponds to the transition probability from state
i to state j in a Markov chain, hence, we require the edge

weights for edges originating at vi to sum to 1,Pi;j eij = 1.

We use an n × n (n = |V |) adjacency matrix to represent
the graph, where each entry in the matrix, aij = eij.

As an illustrative example, we will now consider the dy-
namic instruction trace. We found 237 unique instructions
across all of the traces we collected. These instructions are
the vertices of the Markov chains. The 237 instructions are
irrespective of the operands used with those instructions.
By ignoring operands, we remove sensitivity to register al-
location and other compiler artifacts.
It is important to
note that rarely did the instruction trace of a single pro-
gram make use of all 237 unique instructions, and therefore,
the adjacency matrix of that Markov chain graph will con-
tain some rows of zeros. The decision to incorporate unused
instructions in the model allowed us to maintain a consistent
vertex set between all instruction trace graphs, granting us
the ability to make uniform comparisons in graph space.

To ﬁnd the transition probabilities of the Markov chain,
we ﬁrst scan the dynamic instruction trace, keeping counts
for each pair of successive instructions. After ﬁlling in the
adjacency matrix with these values, we normalize the ma-
trix such that all of the non-zero rows sum to one. This
process of estimating the transition probabilities ensures us
a well-formed Markov chain. Figure 2 shows a snippet of
dynamic instruction trace data with a resulting fragment of
the hypothetical Markov chain graph.

Kernels. A kernel, K(x, x′), is a generalized inner product
and can be thought of as a measure of similarity between
two objects [37]. The power of kernels lies in their ability to

compute the inner product between two objects in a possibly
much higher dimensional feature space, without explicitly
constructing this feature space. Let X be any dataset, then
a kernel, K : X × X → R, is deﬁned as:

K(x, x′) = hφ(x), φ(x′)i

(1)

where x, x′ ∈ X, h·, ·i is the dot product, and φ(·) is the
projection of the input object into feature space. A well-
deﬁned kernel must satisfy two properties:

1. A kernel must be symmetric:

for all x and y ∈ X,

K(x, y) = K(y, x).

2. A kernel must be positive-semideﬁnite: for any x1, . . .

j=1 cicjK(xi, xj) ≥ 0.

xn ∈ X and c ∈ Rn, Pn

i=1Pn

Kernels are appealing in a classiﬁcation setting due to
the kernel trick, which replaces inner products with kernel
evaluations [37]. The kernel trick uses the kernel function
to perform a non-linear projection of the data into a higher
dimensional space, where linear classiﬁcation in this higher
dimensional space is equivalent to non-linear classiﬁcation
in the original input space.

For the Markov chain representations and the ﬁle informa-
tion feature vector, we used a standard squared exponential
kernel:

KSE(x, x′) = σ2e− 1

2λ2 Pi(xi−x

′

i)2

(2)

where xi represents one of the seven features for the ﬁle
information data source, or a transition probability for the
Markov chain representations. σ and λ are the hyperpa-
rameters of the kernel function (estimated through cross-

the corresponding features.

validation), and Pi,j sums the squared distance between

For the control ﬂow graph data source, we attempted to
ﬁnd a kernel that closely matched the work done in the lit-
erature [22]. Although the approach we chose did not take
the instruction information of the basic blocks into account,
we settled on a graphlet kernel due to its computational ef-
ﬁciency [41]. A k-graphlet is deﬁned as a subgraph, of a
graph G, with the number of nodes of the subgraph equal
to k. If we let fG be a feature vector, where each feature is
the number of times a unique graphlet of size k occurs in G,
the normalized probability vector is:

DG =

fG

# of all graphlets of size k in G

and we have the following graphlet kernel:

Kg(G, G′) = DT

GDG′

(3)

(4)

6(a) Binary SE

(b) File Info SE

(c) CFG 4-graphlet

(d) Dynamic Instruction SE

(e) Static Instruction SE

(f) System Call SE

Figure 3: Heatmaps for the six individual kernels. The ﬁrst 780 samples (the top left block in the heatmaps) are the malware
samples and the second 776 samples (the bottom right block) are the benign samples. The oﬀ diagonal blocks are the
similarities between malware and benign samples.

We experimented with graphlets of size k ∈ {3, 4, 5} and
found k = 4 to be optimal with respect to both classiﬁcation
accuracy and AUC.

Figure 3 shows the heatmaps for the six individual kernels
and Figure 4 shows the heatmap of the combined kernel with
the weights of the individual kernels being found using mul-
tiple kernel learning (see Equation 10). The block structure
observed in these ﬁgures is very interesting as it shows that
the kernels and data sources we selected are able to discrim-
inate between malware and benign samples. This is very
apparent in Figure 4 where we see that the top left block
(the similarity between the malware samples) has very high
values compared with the rest of the image.

These ﬁgures, especially Figure 3 (b), (c), and (e), oﬀer
support in using the methodology presented in this paper
in a malware phylogenetic setting. The structured blocks
along the diagonal lend support to these samples coming
from similar families. Ideas to extend the current work for
use in malware phylogenetics is examined in Section 6.

If we have some set of M valid kernels, K1, K2, . . ., KM ,

we are assured that

Kcomb =

M

X1≤i≤M

Ki

(5)

is also a valid kernel [11]. This algebra on kernels allows
us to elegantly combine kernels that measure very diﬀerent
aspects of the input data, or even diﬀerent views of the data,
and is the object of study in multiple kernel learning [6, 43].

Multiple Kernel Learning. The goal of classical kernel-
based learning with support vector machines is to learn the
weight vector, α, describing each data instance’s contribu-
tion to the hyperplane that separates the points of the two
classes with a maximal margin [12] and can be found with

Figure 4: Heatmap for all six kernels combined with the
weights found using multiple kernel learning.

the following optimization problem:

min

2

α   1
|

n

Xi=1

n

Xj=1

αiαjyiyjK(xi, xj) −

Sk(α)

{z

(6)

n

Xi=1

αi!
}

subject to the constraints:

7αiyi = 0

(7)

n

Xi=1

0 ≤ αi ≤ C

where yi is the class label of instance xi. Equation 7 con-
strains the α’s to be non-negative and less than some con-
stant C. C allows for soft-margins, meaning that some of
the examples may fall between the margins. This helps to
prevent over-ﬁtting the training data and allows for better
generalization accuracy.

Given α found in Equation 6, we have the following deci-

Method
All Six Data Sources
Three Static Sources
Two Dynamic Sources
Binary
Disassembled Binary
CFG (4-graphlets)
Dynamic Instructions
Dynamic System Call
File Information
AV0
AV1
AV2

Acc (%) FPs FNs AUC
16
98.07%
.9978
36
.9934
96.14%
88
88.75%
.9509
93
.9437
88.11%
71
.9483
89.97%
88
.9361
88.05%
.9335
92
87.34%
88
87.08%
.9368
.9111
84.83% 126
n/a
78.46%
n/a
75.26%
71.79%
n/a

14
24
87
92
85
98
105
113
110
331
378
439

4
7
0

sion function:

f (x) = sgn  n
Xi

αiyiK(xi, x)!

(8)

Table 2: The classiﬁcation accuracy, number of false pos-
itives and false negatives, and the full AUC values for 776
instances of benign software versus 780 instances of malware.
Statistically signiﬁcant winners are bolded.

which returns class +1 if the summation is ≥ 0, and class -1
if the summation is < 0.

With multiple kernel learning, we are interested in ﬁnding
β, in addition to the standard α of support vector machines,
such that

Kcomb(xi, xj) =

βkKk(xi, xj)

(9)

M

Xk=1

is a convex combination of M kernels with βk ≥ 0, where
each kernel, Kk, uses a distinct set of features [43]. In our
case, each distinct set of features is a diﬀerent view of the
data given by our diﬀerent data sources (Section 3). The
general outline of the algorithm is to ﬁrst combine the ker-
nels with βk = 1/M , ﬁnd α, and then iteratively keep opti-
mizing for β and α until convergence.

To solve for β, assuming we have a ﬁxed set of support
vectors (α), the following semi-inﬁnite linear program has
been proposed [43]:

max

w.r.t.

θ
θ ∈ R, β ∈ RK

subject to the constraints:

0 ≤ β

(10)

(11)

βk = 1

Xk
Xk=1

M

βkSk(α) ≥ θ

for all α ∈ RN with 0 ≤ α ≤ 1C andPi yiαi = 0, and where

Sk(α) is deﬁned in Equation 6. M is the number of kernels
to be combined. This is a semi-inﬁnite linear program as all
of the constraints in Equation 11 are linear, and there are
inﬁnitely many of these constraints, one for each α ∈ RN

satisfying 0 ≤ α ≤ 1C and Pi yiαi = 0 [17].

To ﬁnd solutions for both α and β, an iterative algorithm
was proposed that ﬁrst uses a standard support vector ma-
chine algorithm to ﬁnd α (Equation 6), and then ﬁxes α
and solves Equation 10 to ﬁnd β. While this algorithm is
known to converge, there are no known convergence rates
[17]. Therefore, the following stopping criterion was pro-
posed [43]:

ǫt+1 ≥ ǫt :=(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

1 − PM

k=1 βt
kSk(αt)
θt

(cid:12)(cid:12)(cid:12)(cid:12)(cid:12)

(12)

This method of multiple kernel learning has been found
to be very eﬃcient. Solving for α and β with as many as
one million examples and twenty kernels has been shown to
take just over an hour [43].
It is important to note that
this optimization problem only needs to be solved once, as
the support vectors (α) and kernel weights (β) found can be
used to classify all of the newly collected data.

5. RESULTS

In this section, we present our results on a dataset com-
posed of 1,556 samples, 780 malicious programs and 776
benign programs. This dataset was provided by Oﬀensive
Computing [1]. In order to get a relevant sample set, the
Oﬀensive Computing samples were obtained from a fresh
malware feed. This feed takes place between security ven-
dors who harvest malware from a variety of sources such as
web spidering and customer uploads. These are distributed
in a batched format to researchers on a daily basis. We
also present results which demonstrates the generalization
accuracy of our methods on a separate dataset composed of
20,936 malicious samples. All of this data was gathered over
a six month period, from August 2011 to January 2012.

The metrics we use to quantify our results are classi-
ﬁcation accuracy, receiver operating characteristic (ROC)
curves, area under the ROC curve (AUC), and the average
time it takes to classify a new instance. We look at three
combination strategies: static sources, dynamic sources, and
a combination with all six of our data sources. We compare
our combined kernel method against methods based on a
single data source. We conclude this section with some inter-
esting observations we found with regard to the performance
of the static sources versus the dynamic sources.

Machine/Tools. To perform all of our experiments, we
used a machine with quad Xeon X5570s running at 2.93GHz
and having 24GB of memory. To perform the multiple ker-
nel learning, we used the modular Python interface of the
Shogun Machine Learning Toolbox [44].

Accuracy. Table 2 presents our results for the individual
kernels as well as the combined kernels using 10-fold cross
validation. The three best performing anti-virus programs
(out of 11 considered) are also shown. For the anti-virus
program results, it is important to emphasize that our ma-
licious dataset was not composed of zero-day malware, but
rather malware that is 9 months to a year old. Despite

8(a) Full ROC Image

(b) Zoomed ROC Image

Figure 5: ROC curves for all six individual kernels, a kernel based on the three static sources, a kernel based on the two
dynamic sources, and the combined kernel composed of all six data sources. It is easy to see that the kernel based on all six
data sources performs signiﬁcantly better than the other kernels. The full AUC values for the kernels are are listed in legend
in parenthesis.

Method
All Six Data Sources
Three Static Sources
Two Dynamic Sources
Binary
Disassembled Binary
CFG (4-graphlets)
Dynamic Instructions
Dynamic System Call
File Information

.01

.9467
.9224
.5000
.5369
.5574
.4182
.3401
.5266
.0946

.05

.9867
.9634
.8487
.7919
.7347
.6814
.6395
.7337
.4527

.1

.9933
.9812
.8882
.8523
.8699
.8724
.7211
.8580
.7703

.25
1.0

1.0
.9605
.9262
.9628
.9378
.9116
.9586
.9054

.5
1.0

1.0
.9803
.9597
.9878
.9675
.9796
.9763
.9730

Full AUC

.9978
.9934
.9509
.9437
.9483
.9361
.9335
.9368
.9111

Table 3: AUC values for the full ROC curve as well as values for ﬁve diﬀerent false positive rates: .01, .05, .1, .25, and .5.
The combined kernel composed of all six data sources performs the best at all values.

this, the worst performing data source based on the ﬁle in-
formation feature vector still had ∼6% better classiﬁcation
accuracy than the best anti-virus program. All but one of
the false positives found by the anti-virus programs were
actually conﬁrmed to be true positives as discussed later in
this section.

The best performing method was the combined kernel that
used all six data sources and achieved an accuracy of 98.07%.
Although using purely static sources performed very well
(96.14%), adding dynamic information signiﬁcantly improved
overall performance. All of the single data sources were be-
tween 84% to 89% with the single data source winner being
the disassembled binary at 89.97%. The disassembled binary
most likely has an advantage over the raw binary because it
was unpacked before it was disassembled.

ROC Curves / AUC Values. To analyze the diﬀerent
data sources with regard to diﬀerent false positive thresh-
olds, we looked at the ROC curves and various ROC points
representing diﬀerent false positive rates. Figure 5 plots all
the ROC curves together (including the combined kernels)
along with a zoomed version of the curve. Figure 5 (b) is
particularly informative as we can see that the combined
kernel, which includes all six data sources, performs bet-
ter than any single data source or the two other combined

kernels for all false positive rates. If there are certain time
and/or other resource constraints, Figure 5 (b) also shows
that we are able to achieve reasonably high results by just
using the kernel based on the three static sources.

Table 3 displays the full AUC value, as well as the AUC
values for ﬁve diﬀerent false positive rates: 0.01, 0.05, 0.1,
0.25, and 0.5. We see that by integrating all six data sources,
we can achieve an AUC value of .9467 with a .01 false pos-
itive rate, signiﬁcantly higher than any other kernel based
on a single data source, which adds more support to the
power of our approach and multiple kernel learning in gen-
eral. The ﬁle information data source is the worst perform-
ing data source with regard to this metric, and as we explain
at the end of this section, we expect this is due to misclassi-
fying a large percentage of ﬁles that are packed and/or have
abnormal entropy values.

Learned Kernel Weights. The kernel weights learned
from 10 are an interesting way to look at how informative
diﬀerent data sources are with regard to the classiﬁcation
accuracy. The weights we found for our data views are shown
in Table 5. It is interesting to note that the weights are not
representative of how well the single data source does at
classiﬁcation. For example, the dynamic instruction view
has the highest kernel weight among the weights for all six

9Method
All Six Data Sources
Three Static Sources
Two Dynamic Sources
Binary
Disassembled Binary
CFG (4-graphlets)
Dynamic Instructions
Dynamic System Call
File Information

n/a

300.0s

n/a
n/a
n/a

300.0s
300.0s
300.0s

3.12s
1.18s
1.94s
0.26s
0.63s
0.97s
1.10s
0.82s
1.41s

Trace Data Transformation Classify
300.0s

0.21s
0.13s
0.06s
0.05s
0.05s
0.06s
0.04s
0.01s
0.01s

Total
303.52s

2.03s

302.53s

0.31s
0.68s
1.03s

301.15s
300.83s
301.41s

Table 4: The average time it takes from receiving the raw data until a classiﬁcation decision can be made. This time includes
running the program for the dynamic sources (we allow 5 minutes for the tracing), transforming the data to the appropriate
representation (which includes the time to disassemble the code, collecting all of the 2-grams from the trace ﬁles, building the
Markov chains, etc.), and ﬁnally computing the class from the decision function of Equation 8.

Given a new instance to classify, there are two or three
steps, depending on whether we are using a dynamic data
source, that must be performed:

1. Run the instance in a virtual machine keeping a log
of the instructions and system calls the program per-
forms.

2. Transform the data source into one of our data repre-

sentations.

3. Classify the transformed data instance according to

Equation 8.

In our timing results, we allow ﬁve minutes to collect the
dynamic trace data. Transforming the data to our repre-
sentation could mean several diﬀerent things for each of the
diﬀerent data sources. For instance, we might have to disas-
semble the data, ﬁnd the 2-grams in the disassembled data,
and ﬁnally build the Markov chain; we might have to build
the control ﬂow graph and ﬁnd the number of graphlets with
a speciﬁc structure; we might have to collect the statistics
of Table 1 in the case of the ﬁle information data source;
etc. For the classiﬁcation step, it is important to note that
support vector machines are known to ﬁnd sparse α vectors
[12], easing the computational burden of Equation 8.

The timing results, which are broken down into the three
stages, are presented in Table 4 and are pictorially featured
in Figure 6. These results were averaged over the entire
dataset. As the reader can see, classifying an instance takes
very little time, and the only real bottleneck is the time to
collect the dynamic trace. The combined kernel composed
of the three static sources is especially impressive as we have
shown it to have great performance and it only takes 2.03
seconds on average to transform the data and make a clas-
siﬁcation decision. Using the combined kernel based on the
static data sources, assuming 2.03 seconds per data instance,
it is possible to classify up to 42,561 new samples each day!
Again, the methods presented in this paper are highly paral-
lelizable, and these methods will scale very easily with more
computational resources.

Testing on a Large Malware Sample. As explained
earlier in this section, obtaining a large benign dataset is
diﬃcult. On the other hand, we had an additional 20,936
malware samples at our disposal. We are treating this data
as a validation set to test the generalization accuracy of
our methods. We ﬁrst train on all of the 1,556 samples
(780 malicious and 776 benign), ﬁnd the β’s and α’s, and
ﬁnally classify the 20,936 malicious samples as either benign
or malicious according to Equation 8.

Figure 6: Plot demonstrating the trade-oﬀ between accuracy
and time to classify. Time is in seconds and the x-axis is
the time it takes to ﬁrst collect the dynamic trace (for the
dynamic data sources), transform the data instance into our
representation, and ﬁnally classify the instance.

Data View
Binary
Disassembled
CFG
Dyn Instrs
System Calls
File Info

All Views

Static Views Dyn Views

.2671
.3284
.4046

.2248
.2576
.1559
.3299

0.0

.0319

.5817
.4183

Table 5: The kernel weights learned from Equation 10 for
the diﬀerent kernel combinations we examine.

views, but in our experiments, the binary, disassembled and
CFG all individually have higher accuracy than the dynamic
instruction view.

Speed. Due to the fact that computing the kernel for each
dataset, ﬁnding the kernel weights for the combined ker-
nels, and ﬁnding the support vectors for the support vector
machine are all essentially O(1) operations (all of these cal-
culations only need to be done once, oﬄine), we will focus
our analysis of the timing results on the average amount of
time it takes to classify a new instance. As a note of interest,
the time to ﬁnd the kernel weights and support vectors for
the kernel composed of all six data sources, averaged over
10 runs, was only 0.86 seconds!

10Method
All Six Data Sources
Three Static Sources
Two Dynamic Sources
Binary
Disassembled Binary
Control Flow Graph (4-graphlets)
Dynamic Instructions
Dynamic System Call
File Information
AV0
AV1
AV2

Accuracy (%)

97.97%
95.27%
91.57%
86.76%
88.23%
85.92%
88.42%
86.38%
84.46%
57.55%
56.01%
55.32%

Table 6: The classiﬁcation accuracy on a validation dataset
consisting on 20,936 malicious samples.

The results for this experiment are shown in Table 6. The
classiﬁcation accuracies are similar to those of the dataset
with 780 malicious and 776 benign samples with the excep-
tion that the signature-based anti-virus programs do worse.
This is due to the fact that in the original dataset, these
methods would always get close to 100% accuracy on the
benign samples giving a positive skew to their results. As
with the previous results, we can see that by combining data
sources in our multiple kernel learning framework we were
able to achieve a large increase in classiﬁcation accuracy.
The results presented in Table 6 suggest that our methods
would generalize very well to larger datasets given that they
performed well on this validation set.

Observations. A well-known problem in any supervised
machine learning setting is the integrity of the training dataset.
In training our classiﬁer, we assumed that the labeled be-
nign samples in our dataset were actually benign. This was
reasonable as the executables were taken from clean installa-
tions of commercial software. To test this hypothesis, we ran
our classiﬁer based on the combined kernel, with all six data
sources, using 10-fold cross validation 50 times and counted
how many times a data instance was classiﬁed incorrectly.
We found 19 data instances that were consistently misclas-
siﬁed, 8 which were labeled as malicious and 11 which were
labeled as benign. 5 of the 11 benign samples were found
to actually be malicious using VirusTotal [2]. It is interest-
ing that our method was able to reduce the original 1,556
instance dataset to a manageable size of 19 suspicious sam-
ples, making closer manual inspection of these ﬁles much
more manageable. Note that we did not correct the dataset
for this paper, and these 5 ﬁles are still considered false pos-
itives in the previous results.

Traditional static analysis techniques have been shown to
be insuﬃcient given the rise of newer malware obfuscation
techniques [29]. Due to these limitations, we chose to include
dynamic data sources to improve malware classiﬁcation de-
spite the time constraints these data collection methods im-
pose. To further analyze some of the pitfalls of our static
data sources, we again ran the combined kernel with all six
data sources, a kernel with all of the static data sources, a
kernel with all of the dynamic data sources, and the six sep-
arate kernels, one for each of the six diﬀerent data sources
50 times, keeping track of the ﬁles that were consistently
misclassiﬁed with respect to each kernel.

Table 7 shows the percentage of ﬁles consistently misclas-
siﬁed which were packed. The kernel based on the binary
data had signiﬁcant problems classifying packed benign in-

Method
All Six Data Sources
Three Static Sources
Two Dynamic Sources
Binary
Disassembled Binary
CFG (4-graphlets)
Dynamic Instructions
Dynamic System Call
File Information

Benign Malicious
70.00%
0.00%
36.84%
20.00%
45.95%
18.75%
43.14%
43.75%
53.85%
10.20%
55.10%
13.89%
20.00%
38.24%
21.62% 32.56%
34.31%
28.09%

Table 7: % of ﬁles which were packed and consistently mis-
classiﬁed over 50 runs with our diﬀerent kernels. Note the
average percentage of packed ﬁles in the entire dataset is
19.59% and 47.56% for benign and malicious ﬁles, respec-
tively.

Method
All Six Data Sources
Three Static Sources
Two Dynamic Sources
Binary
Disassembled Binary
CFG (4-graphlets)
Dynamic Instructions
Dynamic System Call
File Information

Benign Malicious

7.43
7.41
6.26
7.42
6.15
6.12
6.29
6.41
7.77

6.77
6.91
7.50
7.01
7.58
7.66
7.57
7.55
5.98

Table 8: Average entropy of ﬁles consistently misclassiﬁed
over 50 runs with our diﬀerent kernels. Note that the average
entropies over the entire dataset are 6.34 and 7.52 for benign
and malicious ﬁles respectively.

stances, as one would expect, with 43.75% of the false pos-
itives being packed (only 19.59% of all benign instances in
the training data were packed). On the other hand, us-
ing dynamic data sources, the percentage of false positives
that were packed is the same as the packed percentage of
the training data, which would oﬀer support for the ker-
nels based on these data sources not being deceived by the
packer. Although the dynamic traces of packed ﬁles will
also have an unpacking “footprint”, it has been shown that 5
minutes for a dynamic trace is enough time for a signiﬁcant
number of the instructions to represent the true behavior
of the program [34]. A notable exception of static sources
stumbling on programs that were packed is the disassembled
data source (and the control ﬂow graph which is based on
the disassembled data source), but to get these data sources,
we ﬁrst had to unpack the binary.

Table 8 shows the average entropy of ﬁles which were con-
sistently misclassiﬁed. The link between entropy and ﬁles
being packed is well-known [27], therefore we see similar
results to Table 7. Again, the two dynamic sources’ clas-
siﬁcation accuracies seem to be independent of the entropy
as the average entropy of the ﬁles they misclassify corre-
sponds to the average entropy of the entire dataset. Also,
much like the previous results, the binary data source had
problems with classifying instances whose entropies diﬀer
signiﬁcantly from the norm. As we would expect, the ﬁle
information data source had the most problems with en-
tropy (remember that entropy is one of the seven features
for this source), with average entropy of misclassiﬁed benign
and malicious ﬁles being 7.77 (average in dataset: 6.34) and
5.98 (average in dataset: 7.52), respectively.

11Malware Sample K(Tracepin, Traceether)

sample0
sample1
sample2
sample3
sample4
sample5
sample6
sample7

.9010
.8392
.8171
.7719
.6424
.5864
.5399
.3725

Table 9: The kernel values between two Markov chains of
the same program’s dynamic instruction trace, one trace run
with Intel Pin, and one trace run with the Ether framework.
The kernel values were computed using Equation 2.

Having a dynamic tracing tool that is able to evade de-
tection from the program being traced is essential to get an
accurate picture of how the program actually behaves in the
wild. Unfortunately, there are malware that are able to de-
tect if they are being run in a sandboxed environment and
being traced [9]. We choose the Intel Pin program [25] be-
cause it allowed us to collect both instructions and system
calls simultaneously, but it does not make an eﬀort to be a
transparent tracing tool like the Ether framework [15]. The
dynamic instruction data source’s classiﬁcation accuracy in
this paper is lower than that reported in the literature [14]
and we suspect that this is in part due to Intel Pin not being
transparent and the malware altering its behavior.

To test this hypothesis, we looked at 8 malicious data in-
stances that were consistently misclassiﬁed (over 50 runs)
by the kernel based on the dynamic instruction data source.
We ﬁrst collected the dynamic traces with both Intel Pin
and Ether and then computed the kernel values between
the resulting Markov chains of these traces. These results
are displayed in Table 9. Note that a kernel value of zero
represents completely orthogonal behavior between the two
traces (no instruction transitions in common) and a kernel
value of one represents exactly the same program behavior
between the two traces, which would be highly unlikely even
if we used the same tracing tool for both samples (a kernel
value of 0 is also highly unlikely for obvious reasons). Al-
though this is a coarse measure as to whether the program
alters its behavior, it does give us useful information as to
why these instances could have been classiﬁed incorrectly.
The lower values of sample4, sample5, sample6, and sam-
ple7 are particularly interesting and do suggest that their
dynamic instruction traces are substantially diﬀerent under
the diﬀerent tracing tools.

6. LIMITATIONS AND FUTURE WORK

There has been a rising interest in learning how to cluster
malware so that researchers can gain insight into the phylo-
genetic structure of current viruses [8, 19, 23, 35]. Because
the majority of new viruses are derived from, or are compos-
ites of, established viruses, this information would allow for
more immediate responses and allow researchers to under-
stand the new virus much more quickly. Given our kernel
matrix, we can easily use spectral clustering [26] to partition
our dataset into groups with similar structure with regard to
the data sources we have chosen. We would ﬁrst construct

the weighted graph Laplacian, a |V | × |V | matrix:

1 − evv
dv
− euv√dudv
0

if u = v, and dv 6= 0,
if u and v are adjacent,
otherwise.

(13)

L =


where evv is the edge weight, in our case the entry in the ker-
nel matrix, and dv is the degree of the vertex, which would be
the sum of the corresponding row in the kernel matrix. Then
we would perform an eigen-decomposition on the Laplacian
and take the l-smallest eigenvectors. Finally, we would use
standard k-means clustering with the l-smallest eigenvectors
as the features [16].

With our method, we are not restricted to only using dif-
ferent data sources, but we also have the ﬂexibility to incor-
porate diﬀerent data representations for each data source
assuming a suitable kernel can be deﬁned. For instance, it
is common in the literature to use n-gram analysis [14, 45]
on the dynamic trace or static data. But these approaches
have the drawback of choosing the appropriate n.
In our
approach, we could use several values for n and then let
the multiple kernel learning optimization weight the choices
which it ﬁnds to be the most informative. This would be
an interesting avenue for future research as we can view the
same dataset in a variety of ways which would allow us to
extract even more information for classiﬁcation.

In this paper, we made the decision to use a standard
squared exponential kernel (Equation 2) for the majority of
the data sources. With our framework, we have the ability
to incorporate more advanced kernels that have the ability
to measure diﬀerent aspects of similarity of the diﬀerent data
sources. These kernels can be based on random walks over
the Markov chains, the eigen-structure of the graph Lapla-
cians, the number of shortest paths in the graphs, etc. [20].
And like using diﬀerent n-gram values, the multiple kernel
learning optimization problem would ﬁnd the kernel weights
it deems most appropriate.

Although we have shown that acceptable performance can
be achieved if certain time constraints are imposed by using
solely static data sources, the full multiple kernel learning
infrastructure, which includes the time it takes to collect
the dynamic trace data, would be too resource intensive to
be deployed on a normal user’s system. There has been a
rising interest in performing virus classiﬁcation in the cloud
[30, 31, 50] due to the constraints of users’ resources. Our
work would be perfect for a cloud-based solution as we could
dedicate machines to collect the dynamic trace data and
wouldn’t have to burden the user’s system with these ex-
pensive tracing tools.

As we explained in Section 3, we choose to use instruc-
tion/system call categorizations to reduce the size of the
vertex set of the resulting Markov chains to avoid the curse
of dimensionality with these models. Choosing an optimal
instruction categorization is a non-trivial task. For exam-
ple, using the coarse categorization with 8 categories on the
disassembled data gave us ∼10% less classiﬁcation accuracy
than the categorization with 86 categories. There is also the
possibility that diﬀerent categorizations could prove to be
better suited for diﬀerent tasks. Clustering could be easier
with a categorization that uses categories based on diﬀerent
instructions that are more likely to be used by diﬀerent com-
pilers. Or similarly, categories based on diﬀerent instruction
sets, such as SSE, MMX, AVX, and FMA, could be useful.
A uniﬁed data gathering infrastructure that would supply
us with both our static and dynamic data sources would be
an invaluable tool for our analysis. BitBlaze [42] aims to pro-

12vide this need, incorporating static analysis techniques such
as disassembled code generation, control ﬂow graph genera-
tion, and other data ﬂow analyses using the static analysis
component of BitBlaze, Vine. The dynamic analysis com-
ponent of BitBlaze, TEMU, allows for dynamic instruction
traces and memory taint analysis, among other things. Bit-
Blaze has been shown to provide reliable data that has been
used to produce very accurate malware classiﬁcation results
[51]. It would be interesting to take advantage of all of the
data sources provided by BitBlaze in our multiple kernel
learning framework.

7. CONCLUSIONS

In this paper we have shown that signiﬁcant beneﬁts, with
respect to both classiﬁcation accuracy and number of false
positives, can be gained when malware researchers use all
of the information about executables that are available to
perform classiﬁcation, and not just restricting malware clas-
siﬁcation to a single data source. We were able to achieve
an accuracy of 98.07% on a dataset of 780 malware and 776
benign instances. We showed that while we had 16 false pos-
itives in this dataset, several of these were conﬁrmed to be
true positives. Our ROC curve analysis showed signiﬁcant
increases in performance when the data sources were com-
bined, and also that acceptable performance can be achieved
with just static sources in a resource constrained environ-
ment.

We demonstrated several interesting observations about
our results, illustrating some of the pitfalls of both static
analysis and dynamic analysis techniques. Namely that static
data sources have problems classifying instances which were
packed and/or had abnormal entropy values. We also showed
the importance of having a dynamic tracing tool that is ca-
pable of evading detection from the malware so that we can
get a truly representative sample of how the malware actu-
ally behaves in the wild.

8. REFERENCES
[1] Oﬀensive Computing.

http://www.oﬀensivecomputing.net/, Accessed June
2011.

[2] Virus Total. http://www.virustotal.com/, Accessed

October 2011.

[3] Portable Executable iDentiﬁer. http://peid.info/,

Accessed 6 October 2011.

[4] Blake Anderson, Daniel Quist, Joshua Neil, Curtis

Storlie, and Terran Lane. Graph-Based Malware
Detection using Dynamic Analysis. Journal in
Computer Virology, 7:247–258, 2011.

[5] Anubis. http://anubis.iseclab.org/, 2009.
[6] Francis R. Bach, Gert R. G. Lanckriet, and Michael I.
Jordan. Multiple Kernel Learning, Conic Duality, and
the SMO Algorithm. In Proceedings of the
Twenty-First International Conference on Machine
Learning. ACM, 2004.

[7] Paul Barham, Boris Dragovic, Keir Fraser, Steven
Hand, Tim Harris, Alex Ho, Rolf Neugebauer, Ian
Pratt, and Andrew Warﬁeld. Xen and the Art of
Virtualization. In Proceedings of the Nineteenth ACM
Symposium on Operating Systems Principles, pages
164–177. ACM, 2003.

[8] Ulrich Bayer, Paolo Milani Comparetti, Clemens

Hlauschek, Christopher Kruegel, and Engin Kirda.
Scalable, Behavior-Based Malware Clustering. In

ISOC Network and Distributed System Security
Symposium. 2009.

[9] Ulrich Bayer, Andreas Moser, Christopher Kruegel,

and Engin Kirda. Dynamic Analysis of Malicious
Code. Journal in Computer Virology, 2:67–77, 2006.

[10] Daniel Bilar. Opcodes as Predictor for Malware.
International Journal of Electronic Security and
Digital Forensics, 1:156–168, January 2007.

[11] Christopher M. Bishop. Pattern Recognition and

Machine Learning (Information Science and
Statistics). Springer-Verlag New York, Inc., Secaucus,
NJ, USA, 2006.

[12] Christopher J. C. Burges. A Tutorial on Support

Vector Machines for Pattern Recognition. Data
Mining and Knowledge Discovery, 2:121–167, 1998.

[13] Mihai Christodorescu and Somesh Jha. Static Analysis

of Executables to Detect Malicious Patterns. In
Proceedings of the 12th USENIX Security Symposium,
pages 169–186, 2003.

[14] Jianyong Dai, Ratan Guha, and Joohan Lee. Eﬃcient

Virus Detection Using Dynamic Instruction
Sequences. Journal of Computers, 4(5), 2009.

[15] Artem Dinaburg, Paul Royal, Monirul Sharif, and

Wenke Lee. Ether: Malware Analysis Via Hardware
Virtualization Extensions. In Proceedings of the 15th
ACM Conference on Computer and Communications
Security, pages 51–62, 2008.

[16] J. A. Hartigan and M. A. Wong. Algorithm AS 136: A

K-Means Clustering Algorithm. Journal of the Royal
Statistical Society. Series C (Applied Statistics),
28(1):100–108, 1979.

[17] R. Hettich and K. O. Kortanek. Semi-Inﬁnite

Programming: Theory, Methods, and Applications.
SIAM Review, 35:380–429, September 1993.

[18] Steven A. Hofmeyr, Stephanie Forrest, and Anil

Somayaji. Intrusion Detection Using Sequences of
System Calls. Journal of Computer Security,
6(3):151–180, January 1998.

[19] Md. Karim, Andrew Walenstein, Arun Lakhotia, and
Laxmi Parida. Malware Phylogeny Generation Using
Permutations of Code. Journal in Computer Virology,
1:13–23, 2005.

[20] H. Kashima, K. Tsuda, and A. Inokuchi. Kernels for

Graphs. MIT Press, 2004.

[21] J. Zico Kolter and Marcus A. Maloof. Learning to
Detect and Classify Malicious Executables in the
Wild. The Journal of Machine Learning Research,
7:2721–2744, December 2006.

[22] Christopher Kruegel, Engin Kirda, Darren Mutz,

William Robertson, and Giovanni Vigna. Polymorphic
Worm Detection Using Structural Information of
Executables. In Recent Advances in Intrusion
Detection, pages 207–226. Springer Berlin /
Heidelberg, 2006.

[23] G. Jacob L. Nataraj, S. Karthikeyan and

B. Manjunath. Malware Images: Visualization and
Automatic Classi¨ıˇn ֒Acation. In Proceedings of VizSec,
2011.

[24] Corrado Leita, Ulrich Bayer, and Engin Kirda.

Exploiting Diverse Observation Perspectives to get
Insights on the Malware Landscape. In 2010
IEEE/IFIP International Conference on Dependable
Systems and Networks, pages 393–402, 2010.

13[25] Chi-Keung Luk, Robert Cohn, Robert Muth, Harish
Patil, Artur Klauser, Geoﬀ Lowney, Steven Wallace,
Vijay Janapa Reddi, and Kim Hazelwood. Pin:
Building Customized Program Analysis Tools with
Dynamic Instrumentation. ACM SIGPLAN
Conference on Programming Language Design and
Implementation, pages 190–200, June 2005.

[26] Ulrike Luxburg. A Tutorial on Spectral Clustering.

Statistics and Computing, 17(4):395–416, 2007.

[27] Robert Lyda and James Hamrock. Using Entropy
Analysis to Find Encrypted and Packed Malware.
IEEE Security & Privacy, 5(2):40–45, 2007.

[28] Eitan Menahem, Asaf Shabtai, Lior Rokach, and

Yuval Elovici. Improving Malware Detection by
Applying Multi-Inducer Ensemble. Computational
Statistics and Data Analysis, 53(4):1483–1494, 2009.

[29] Andreas Moser, Christopher Kruegel, and Engin

Kirda. Limits of Static Analysis for Malware
Detection. Computer Security Applications
Conference, Annual, 0:421–430, 2007.

[30] Jon Oberheide, Evan Cooke, and Farnam Jahanian.

CloudAV: N-version Antivirus in the Network Cloud.
In Proceedings of the 17th Conference on Security
Symposium, pages 91–106, 2008.

[31] Jon Oberheide, Kaushik Veeraraghavan, Evan Cooke,

Jason Flinn, and Farnam Jahanian. Virtualized
In-Cloud Security Services for Mobile Devices. In
Proceedings of the First Workshop on Virtualization in
Mobile Computing, MobiVirt, pages 31–35. ACM,
2008.

[32] Roberto Perdisci, David Dagon, Prahlad Fogla, and

Monirul Sharif. Misleading Worm Signature
Generators Using Deliberate Noise Injection. In In
Proceedings of the 2006 IEEE Symposium on Security
and Privacy, pages 17–31, 2006.

[33] IDA Pro.

http://www.hex-rays.com/products/ida/index.shtml,
2012.

[34] Daniel Quist, Lorie Liebrock, and Joshua Neil.
Improving Antivirus Accuracy with Hypervisor
Assisted Analysis. Journal in Computer Virology,
pages 1–11, 2010.

[35] Konrad Rieck, Thorsten Holz, Carsten Willems,

Patrick D ˜Aijssel, and Pavel Laskov. Learning and
Classiﬁcation of Malware Behavior. In Detection of
Intrusions and Malware, and Vulnerability
Assessment, volume 5137 of Lecture Notes in
Computer Science, pages 108–125. Springer Berlin /
Heidelberg, 2008.

[36] Paul Royal, Mitch Halpin, David Dagon, Robert

Edmonds, and Wenke Lee. PolyUnpack: Automating
the Hidden-Code Extraction of Unpack-Executing
Malware. In 22nd Annual Computer Security
Applications Conference (ACSAC), pages 289–300,
2006.

[37] Bernhard Sch¨olkopf and Alexander Johannes Smola.

Learning with Kernels. MIT Press, 2002.

[38] R. Sekar, M. Bendre, D. Dhurjati, and P. Bollineni. A

Fast Automaton-Based Method for Detecting
Anomalous Program Behaviors. In IEEE Symposium
on Security and Privacy, pages 144–155, 2001.

[39] M. Shaﬁq, Syed Khayam, and Muddassar Farooq.

Embedded Malware Detection Using Markov
n-Grams. In Detection of Intrusions and Malware, and

Vulnerability Assessment, volume 5137 of Lecture
Notes in Computer Science, pages 88–107. Springer
Berlin / Heidelberg, 2008.

[40] Madhu Shankarapani, Subbu Ramamoorthy, Ram

Movva, and Srinivas Mukkamala. Malware Detection
Using Assembly and API Call Sequences. Journal in
Computer Virology, 7(2):1–13, 2010.

[41] Nino Shervashidze, S. V. N. Vishwanathan, Tobias H.

Petri, Kurt Mehlhorn, and Karsten M. Borgwardt.
Eﬃcient Graphlet Kernels for Large Graph
Comparison. In Proceedings of the Twelfth
International Conference on Artiﬁcial Intelligence and
Statistics (AISTATS), volume 5, pages 488–495.
CSAIL, 2009.

[42] Dawn Song, David Brumley, Heng Yin, Juan

Caballero, Ivan Jager, Min Kang, Zhenkai Liang,
James Newsome, Pongsin Poosankam, and Prateek
Saxena. BitBlaze: A New Approach to Computer
Security via Binary Analysis. In Information Systems
Security, volume 5352 of Lecture Notes in Computer
Science, pages 1–25. Springer Berlin / Heidelberg,
2008.

[43] S¨oren Sonnenburg, Gunnar Raetsch, and Christin
Schaefer. A General and Eﬃcient Multiple Kernel
Learning Algorithm. Nineteenth Annual Conference
on Neural Information Processing Systems, 2005.

[44] S¨oren Sonnenburg, Gunnar R¨atsch, Sebastian

Henschel, Christian Widmer, Jonas Behr, Alexander
Zien, Fabio de Bona, Alexander Binder, Christian
Gehl, and Vojtˇech Franc. The SHOGUN Machine
Learning Toolbox. The Journal of Machine Learning
Research, 99:1799–1802, August 2010.

[45] Salvatore Stolfo, Ke Wang, and Wei-Jen Li. Towards

Stealthy Malware Detection. In Malware Detection,
volume 27 of Advances in Information Security, pages
231–249. Springer US, 2007.

[46] Salvatore J. Stolfo, Ke Wang, and Wei-Jen Li.

Fileprint Analysis for Malware Detection. In ACM
Workshop on Recurring/Rapid Malcode, 2005.

[47] Symantec. Internet Security Threat Report, Volume

16. White Paper, April 2011.

[48] The Silicon Realms Toolworks. Armadillo Software
Protection System. http://www.siliconrealms.com/,
Accessed 6 October 2011.

[49] UPX: The Ultimate Packer for eXecutables.

http://upx.sourceforge.net/, Accessed 6 October 2011.

[50] Yanfang Ye, Tao Li, Shenghuo Zhu, Weiwei Zhuang,
Egmen Tas, Umesh Gupta, and Melih Abdulhayoglu.
Combining File Content and File Relations for Cloud
Based Malware Detection. In Proceedings of the 17th
ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, 2011.

[51] Heng Yin, Dawn Song, Manuel Egele, Christopher
Kruegel, and Engin Kirda. Panorama: Capturing
System-Wide Information Flow for Malware Detection
and Analysis. In Proceedings of the 14th ACM
Conference on Computer and Communications
Security, CCS ’07, pages 116–127. ACM, 2007.

14