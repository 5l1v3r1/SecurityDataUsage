Evaluating the November 2015 Root DNS Event

Anycast vs. DDoS:

Giovane C. M. Moura1

Wouter B. de Vries2
1: SIDN Labs

Moritz Müller1
2: University of Twente

John Heidemann3
Ricardo de O. Schmidt2
Cristian Hesselman1
Lan Wei3
3: USC/Information Sciences Institute

leaving other sites unaﬀected.

ABSTRACT
Distributed Denial-of-Service (DDoS) attacks continue
to be a major threat on the Internet today. DDoS at-
tacks overwhelm target services with requests or other
traﬃc, causing requests from legitimate users to be shut
out. A common defense against DDoS is to replicate a
service in multiple physical locations/sites. If all sites
announce a common preﬁx, BGP will associate users
around the Internet with a nearby site, deﬁning the
catchment of that site. Anycast defends against DDoS
both by increasing aggregate capacity across many sites,
and allowing each site’s catchment to contain attack
traﬃc,
IP anycast is
widely used by commercial CDNs and for essential in-
frastructure such as DNS, but there is little evaluation
of anycast under stress. This paper provides the ﬁrst
evaluation of several IP anycast services under stress
with public data. Our subject is the Internet’s Root
Domain Name Service, made up of 13 independently
designed services (“letters”, 11 with IP anycast) run-
ning at more than 500 sites. Many of these services
were stressed by sustained traﬃc at 100× normal load
on Nov. 30 and Dec. 1, 2015. We use public data for
most of our analysis to examine how diﬀerent services
respond to stress, and identify two policies: sites may
absorb attack traﬃc, containing the damage but reduc-
ing service to some users, or they may withdraw routes
to shift both good and bad traﬃc to other sites. We
study how these deployment policies resulted in diﬀer-
ent levels of service to diﬀerent users during the events.
We also show evidence of collateral damage on other
services located near the attacks.

Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than the author(s) must be honored. Abstracting with credit is
permitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
IMC 2016, November 14 - 16, 2016, Santa Monica, CA, USA
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to
ACM. ISBN 978-1-4503-4526-2/16/11. . . $15.00
DOI: http://dx.doi.org/10.1145/2987443.2987446

1.

INTRODUCTION

Although not new, denial-of-service (DoS) attacks
are a continued and growing challenge for Internet ser-
vices [2, 3].
In most DoS attacks the attacker over-
whelms a service with large amounts of either bogus
traﬃc or seemingly legitimate requests. Actual legiti-
mate requests are lost due to limits in network or com-
pute resources at the service. Once overwhelmed, the
service is susceptible to extortion [42]. Persistent at-
tacks may drive clients to other services. In some cases,
attacks last for weeks [17].

Three factors enable today’s Distributed DoS (DDoS)
attacks:
source-address spooﬁng allows a single ma-
chine to masquerade as many machines, making ﬁlter-
ing diﬃcult. Second, some protocols amplify attacks
sent through a reﬂector, transforming each byte sent
by an attacker into 5 or 500 (or more) bytes delivered
to the victim [51]. Third, botnets of thousands of ma-
chines are widespread [31], making vast attacks possible
even without spooﬁng and ampliﬁcation. Large attacks
range from 50–540 Gb/s [4] in 2016, and 1 Tb/s attacks
are within reach.

Many protocol-level defenses against DNS-based
DDoS attacks have been proposed. Source-address val-
idation prevents spooﬁng [24]. Response-rate limit-
ing [57] reduces the eﬀect of ampliﬁcation. Protocol
changes such as DNS cookies [21] or broader use of
TCP [64] can blunt the risks of UDP. While these ap-
proaches reduce the eﬀects of a DoS attack, they cannot
eliminate it. Moreover, deployment rates of these ap-
proaches have been slow [9], in part because there is a
mismatch of incentives between who must deploy these
tools (all ISPs) and the victims of attacks.

Defenses

in protocols and ﬁltering are limited,
though—ultimately the best defense to a 10,000-node
botnet making legitimate-appearing requests is capac-
ity. Services can be replicated to many IP addresses,
and each IP address can use IP anycast to operate at
multiple locations. Many locations allow a single service
to provide large capacity for processing and bandwidth.
Many commercial services promise to defend against
DDoS, either by oﬀering DDoS-ﬁltering as a service (as
provided by Verizon, NTT, and many others), or by

255section observation

§3.1

§3.2
§3.3

§2.2 design choices under stress are withdraw or absorb;
best depends on attackers vs. capacity per catchment
event was at likely 35 Gb/s (50 Mq/s, an upper bound),
resulting in 150 Gb/s reply traﬃc
letters saw minimal to severe loss (1% to 95%)
loss was not uniform across each letter’s anycast sites;
overall loss does not predict user-observed loss at sites
some users “ﬂip” to other sites;
others stick to sometimes overloaded sites
at some sites, some servers suﬀered disproportionately
some collateral damage occurred to co-located services
not directly under attack

§3.5
§3.6

§3.4

Table 1: Key observations in this paper.

providing a service that adapts to DDoS attacks (such
as Akamai [28], Cloudﬂare, and others). Yet the speciﬁc
impact of DDoS on real infrastructure has not widely
been reported, often because commercial infrastructure
is proprietary.

The DNS is a common service, and the root servers
are a fundamental, high-proﬁle, and publicly visible ser-
vice that have been subject to DoS attacks in the past.
As a public service, they are monitored [45] and strive
to self-report their performance. Perhaps unique among
many large services, the Root DNS service is operated
by 12 diﬀerent organizations, with diﬀerent implemen-
tations and infrastructure. Although the internals of
each implementation are not public, some details (such
as the number of anycast sites) are.

To evaluate the eﬀects of DoS attacks on real-world
infrastructure, we analyze two speciﬁc events: the Root
DNS events of Nov. and Dec. 2015 (see §2.3 for dis-
cussion and references). We investigate how the DDoS
attack aﬀected reachability and performance of the any-
cast deployments. This paper is the ﬁrst to explore
the response of real infrastructure across several levels,
from speciﬁc anycast services (§3.2), physical sites of
those services (§3.3), and of individual servers (§3.5).
An important consequence of high load on sites is rout-
ing changes, as users “ﬂip” from one site to another after
a site becomes overloaded (§3.4). Table 1 summarizes
our key observations from these studies.

Although we consider only two speciﬁc events, we ex-
plore their eﬀects on 13 diﬀerent DNS deployments of
varying size and capacity. From the considerable varia-
tion in response across these deployments we identify a
set of potential responses, ﬁrst in theory (§2.2) and then
in practice (§3). Exploration of additional attacks, and
of the interplay of IP anycast and site select at other
layers (for example, in Bing [15]) is future work.

The main contribution of this paper is the ﬁrst eval-
uation of several IP anycast services under stress with
public data. Anycast is in wide use and commercial op-
erators have been subject to repeated attacks, some of
which have been reported [42, 43, 49, 58, 17, 50, 4],
but the details of those attacks are often withheld as

r1

...

rn

s1

...

s33

a

b

c

...

k

l

m

user

Servers
(internal
load balancing)

Sites
(unique location
and BGP route)

Root letters
(unique IP
anycast addr.)

(recursive resolver
and its root.hints)

Figure 1: Root DNS structure, terminology, and mech-
anisms in use at each level.

proprietary. We demonstrate that in large anycast in-
stances, site failures can occur even if the service as a
whole continues to operate. Anycast can both absorb
attack traﬃc inside sites, and also withdraw routes to
shift both good and bad traﬃc to other sites. We ex-
plore these policy choices in the context of a real-world
attack, and show that site ﬂips do not necessarily help
when the new site is also overloaded, or when the shift of
traﬃc overloads it. Finally, we show evidence of collat-
eral damage (§3.6) on services near the attacks. These
results and policies can be used by anycast operators to
guide management of their infrastructure. Finally, the
challenges we show suggest potential future research in
improving routing adaptation under stress and provi-
sioning anycast to tolerate attacks.

2. BACKGROUND AND DATASETS

Before studying anycast services under attack, we
ﬁrst summarize how IP anycast works. We describe
the events aﬀecting the Root DNS service on Nov. 30
and Dec. 1, 2015, and the datasets we use to study these
events.

2.1 Anycast Background and Terminol-

ogy

We next brieﬂy review how IP anycast and the Root
DNS service works. The Root DNS service is imple-
mented with several mechanisms operating at diﬀerent
levels (Figure 1): a root.hints ﬁle to bootstrap, mul-
tiple IP services, often anycast; BGP routing in each
anycast server; and often multiple servers at each site.
The Root DNS is implemented by 13 separate DNS
services (Table 2), each running on a diﬀerent IP ad-
dress, but sharing a common master data source. These
are called the 13 DNS Root Letter Services (or just the
“Root Letters” for short), since each is assigned a letter
from A to M and identiﬁed as <letter>.root-servers.net.
The letters are operated by 12 independent organiza-
tions (Verisign operates both A and J), and each letter
has a diﬀerent architecture, an intentional diversity de-
signed to provide robustness. This diversity happens
to provide a rich natural environment that allows us to

256letter

A
B
C
D
E
F
G
H
I
J
K
L
M

operator
Verisign
USC/ISI
Cogent

U. Maryland

NASA

ISC

U.S. DoD

ARL

Netnod
Verisign

RIPE
ICANN
WIDE

sites

reported
(5, 0)
5
1 (unicast)
(8, 0)
8
(18, 69)
87
(1, 11)
12
59
(5, 54)
6
(6, 0)
2 (pri/back)
(48, 0)
(66, 32)
(15, 18)
(144, 0)
(6, 1)

49
98
33
144
7

sites
observed
5
1
8
65
74
52
6
2
48
69
32
113
6

Table 2: The 13 Root Letters, each operating a separate
DNS service, with their reported architecture (number
of sites with local/global sites [48], B unicast, H pri-
mary/backup), plus the count of sites we observe (§3.3).

explore how diﬀerent approaches react to the stress of
common attacks.

Most Root Letters are operated using IP anycast [1].
At the time of the analyzed events, only B-Root was
unicast [48], and H-Root operated with primary-backup
routing [29]. In IP anycast, the same IP address is an-
nounced from multiple anycast sites (s1 to s33 in Fig-
ure 1), each at a diﬀerent physical location. BGP rout-
ing associates clients (recursive resolvers) who chose to
use that service with a nearby anycast site. The set of
users of each site deﬁnes the site’s anycast catchment.
Larger sites may employ multiple physical servers (r1
to rn in Figure 1), each an individual machine that
responds to queries. CHAOS queries are a diagno-
sis mechanism that return an identiﬁer speciﬁc to the
server [61]. Although their support is optional, re-
sponses can be spoofed, and the reply format is not
standardized, all letters reply with patterns they dis-
close or that can be inferred.
(Prior studies have
conﬁrmed that CHAOS mapping of anycast is gener-
ally complete and reliable, validating it against tracer-
oute and other approaches [23].) Properly interpreted
CHAOS queries, observed from many vantage points
around the Internet (§2.4.1), allow us to map the catch-
ment of each anycast site—the footprint of networks
that are routed to each sites.

Root Letters have diﬀerent policies, architectures,
and sizes, as shown in Table 2. Some letters constrain
routing to some sites to be local, using BGP policies
(such as NOPEER and NO_EXPORT) to limit routing to
that site to only its immediate or neighboring ASes.
Routing for global sites, by contrast, is not constrained.

2.2 Anycast vs. DDoS: Design Options

How should an anycast service react to the stress of
a DDoS attack? We ground empirical observations (§3)
with the following theoretical evaluation of options.

ISP0

ISP1

ISP2

ISP3

A0

c0

A1

c1

c2

c3

s1

s2

S3

anycast sites

clients and attackers

Figure 2: An example anycast deployment under stress.

A site under stress, overloaded with incoming traf-
ﬁc, has two options. It can withdraw routes to some or
all of its neighbors, shrinking its catchment and shift-
ing both legitimate and attack traﬃc to other anycast
sites. Possibly those sites will have greater capacity and
service the queries. Alternatively, it can be become a
degraded absorber, continuing to operate, but with over-
loaded ingress routers, dropping some incoming legiti-
mate requests due to queue overﬂow. However, contin-
ued operation will also absorb traﬃc from attackers in
its catchment, protecting other anycast sites [1].

These options represent diﬀerent uses of an anycast
deployment. A withdrawal strategy causes anycast to
respond as a waterbed, with stress displacing queries
from one site to others. The absorption strategy be-
haves as a conventional mattress, “compressing” under
load, with queries getting delayed or dropped. We see
both of these behaviors in practice and observe them
through site reachability and RTTs.

Although described as strategies and policies, these
outcomes are the result of several factors: the combi-
nation of operator and host ISP routing policy, routing
implementations withdrawing under load [55], the na-
ture of the attack, and the locations of the sites and
attackers. Some policies are explicit, such as the choice
of local-only anycast sites, or operators removing a site
for maintenance or modifying routing to manage load.
However, under stress, the choices of withdrawal and
absorption can also be results that emerge from a mix
of explicit choices and implementation details, such as
BGP timeout values. We speculate that more careful,
explicit, and automated management of policies may
provide stronger defenses to overload, an area of future
work.

Policies in Action: We can illustrate these policies
with the following thought experiment. Consider the
anycast system in Figure 2, it has three anycast sites:
s1, s2, S3, four clients c0 and c1 in s1’s catchment, with
c2 in s2 and c3 in S3’s. Let A0 represent both the iden-
tity of the attacker and the volume of its attack traﬃc,
and s1 represent the site and its capacity.

257The best choice of defense depends on the relative
sizes of attack traﬃc reaching each site. For simplicity,
we can ignore legitimate traﬃc (c∗), since DNS deploy-
ments are greatly overprovisioned (c∗ ≪ A∗). Overpro-
visioning by 3× peak traﬃc is expected [14], and 10×
to 100× overprovisioning is common. (For example, a
modest modern computer can handle an entire letter’s
typical traﬃc (30–60k queries/s, Table 3), and we see
at least 4 to more than 200 servers per letter in our
analysis.)

To consider alternative responses to attack we eval-
uate a deployment where s1 = s2 and S3 = 10s1, as
attack strength A0 = A1 increases. We measure the ef-
fects of the attack by the total number of served clients
(H, “happiness”).

1. If A0 + A1 < s1, then the attack does not hurt

users of the service, H = 4.

2. If A0 + A1 > s1 and A0 < s1 (and A1 < s2), then
s1 is overwhelmed (H = 2) but can shed load. If
it withdraws its route to ISP1, A1 and c1 shift to
s2 and all clients are served: H = 4.

3. If A0 > s1 and A0 + A1 < S3, then a attackers can
overwhelm a small site, but not the bigger site.
Both s1 and s2 should withdraw all routes and let
the large site S3 handle all traﬃc, for H = 4.

4. If A0 > s1, A0 + A1 > S3, but A1 < S3, one can
re-route ISP1 (with A1 and c1) to S3, for H = 3.

5. If A0 > S3, the attack can overwhelm any site;
making no change is optimal. s1 becomes a de-
graded absorber and protects the other sites from
the attack, at the cost of clients c0 and c1. H = 2.

(Withdrawing routes in response to attacks may also
increase latency as catchments change. Our deﬁnition
of H ignores latency as a secondary factor, focusing only
on ability to respond.)

Implications of this model: This model has sev-
eral important implications, both about the range of
possible policies, what policies are practical today, and
directions to explore in the future.

that

for

shows

thought experiment

small
This
attacks,
the withdraw policy can improve service
by spreading the attack (although perhaps counter-
intuitive, less can be more!). For large attacks, degraded
absorbers are necessary to protect some clients, at the
cost of others. We cannot directly apply these rules in
this paper, since we know neither site capacity (some-
thing generally kept private by operators as a defen-
sive measure), nor how much attack traﬃc reaches each
site (a function of how attackers align with catchment,
again, both unknown to us). Our hope is that the sce-
narios of this thought experiment can help us interpret
our observations of what actually occurs.

A second implication is that choice of optimal strat-
egy is very sensitive to actual conditions—which of the

ﬁve cases apply depend on attack rate, location, and
site capacity. The practical corollary is that choosing
the optimal strategy is not easy for operators, either.
Attack traﬃc volumes are unknown to operators, when
the attack exceeds capacity; attack locations are un-
known, due to source address spooﬁng; the eﬀects of
route changes are diﬃcult to predict, due to unknown
attack locations; and route changes are diﬃcult to im-
plement, since routing involves multiple parties. In the
face of uncertainty about attack size and location, ab-
sorption is a good default policy. However, route with-
drawals may occur due to BGP session failure, so both
policies may occur.

As an alternative to adjusting routing or absorbing
attacks, many websites use commercial anti-DDoS ser-
vices that do traﬃc “scrubbing”. Such services cap-
ture traﬃc using BGP, ﬁlter out the attack, and ﬁ-
nally forward the clean traﬃc to the original destina-
tion. While cloud-based scrubbing services have been
used by websites (for example, in the 540 Gb/s DDoS
attack against the Rio 2016 Olympic Games website [4]
or the DoS against ProtonMail [43]), to our knowledge
Root DNS providers do not use such services, likely
because Root DNS traﬃc is a very atypical workload
(DNS, not HTTP).

Finally, a key implication of this model is that there
can be better possible strategies than just absorbing at-
tacks. As described above, they require information
about attack volume and location that is not available
today, but their development is promising future work.

2.3 The Events of Nov. 30 and Dec. 1

On November 30, from 06:50 to 09:30 (UTC), then
again on December 1, 2015 from 05:10 to 06:10, many
of the Root DNS Letters experienced an unusual high
rate of requests [49]. Traﬃc rates peaked at about 5M
queries/s, at least at A-Root [58], more than 100× nor-
mal load. We sometimes characterize these events as
an “attack” here, since sustained traﬃc of this volume
seems unlikely to be accidental, but the intent of these
events is unclear.

An early report by the Root Operators stated that
several letters received high rates of queries for 160 min-
utes on Nov. 30 and 60 minutes on Dec. 1 [49]. Queries
used ﬁxed names, but source address were randomized.
Some letters saw up to 5 million DNS queries per sec-
ond, and some sites at some letters were overwhelmed
by this traﬃc, although several letters were continu-
ously reachable during the attack (either because they
had suﬃcient capacity or were not attacked). There
were no known reports of end-user visible errors, be-
cause top-level names are extensively cached, and the
DNS system is designed to retry and operate in the face
of partial failure.

A subsequent report by Verisign, operator of A- and
J-Root, provides additional details [58]. They stated
that it was limited to IPv4 and UDP packets, and
that D-, L-, and M-root were not attacked. They

258conﬁrm that the event queries used ﬁxed names, with
www.336901.com on Nov. 30 and www.916yy.com on
Dec. 1. They reported that A and J together saw 895M
diﬀerent IP addresses, strongly suggesting source ad-
dress spooﬁng, although the top 200 source addresses
accounted for 68% of the queries. They reported that
both A- and J-Root were attacked, with A continuing
to serve all regular queries throughout, and J suﬀer-
ing a small amount of packet loss. They reported that
Response Rate Limiting was eﬀective [58], identifying
duplicated queries to drop 60% of the responses, and
ﬁltering on the ﬁxed names was also able to reduce out-
going traﬃc. They suggested the traﬃc was caused by
a botnet.

Motivation: We do not have ﬁrm conclusions about
the motivation for these events. As Wessels ﬁrst ob-
served [60], the intent is unclear. The events do not
appear to be DNS ampliﬁcation to aﬀect others since
the spoofed sources spread reply traﬃc widely. They
might be a DDoS targeted at services at the ﬁxed names
listed above, but .com must resolve those names, not
the roots. Also, an attack on the ﬁxed names would be
much more eﬀective if the root lookup was cached and
not repeated. Possibly it was an attack on those targets
that went awry due to bugs in the attack code. It may
be a direct attack on the Root DNS, or even a diver-
sion of other activity.
Fortunately, the intent of the
event is irrelevant to our use of the event to understand
anycast systems under stress.

Generalizing: We analyze and provide data for both
events. Subsequent root events [50] diﬀer in the details
of the event, but pose the same operational choices of
how to react to an attack (§2.2).

We focus on speciﬁc IP anycast services providing
DNS under stress. Root DNS is provided by multiple
such services, and CDNs add DNS-based redirection as
another level of redundancy [15]. Although we brieﬂy
discuss overall performance (§3.2.2), full exploration of
these topics is future work that can build on our analysis
of IP anycast.

2.4 Datasets

We use these large events to assess anycast opera-
tion under stress. Our evaluation uses publicly avail-
able datasets provided by RIPE, several of the Root
operators, and the BGPmon project. We thank these
organizations for making this data available to us and
other researchers. We next describe these data sources
and how we analyze it. The resulting dataset from the
processing described next is publicly available at our
websites [41].

2.4.1 RIPE Atlas Datasets

RIPE Atlas is a measurement platform with more
than 9000 global devices (Atlas Probes) that provide
vantage points (VPs) that conduct network measure-
ments [30, 47]. All Atlas VPs regularly probe all Root
DNS Letters. A subset of this data appears in RIPE’s

DNSMON dashboard evaluating Root DNS [45]. RIPE
identiﬁes data from all VPs that probe each root letter
with a distinct measurement ID [46]. Our study con-
siders all available Atlas data (more than DNSMON
reports), with new processing as we describe below.

RIPE’s baseline measurements send a DNS CHAOS
query to each Root Letter every 4 minutes. At the
time of the event, A-Root was an exception and was
probed only every 30 minutes, too infrequent for our
analysis (§3.2) (it is now probed as frequently as the
other letters). Responses to CHAOS queries are speciﬁc
to root letters (after cleaning, described below) but each
letter follows a pattern that can be parsed to determine
the site and server that VP sees. For this report we
normalize identiﬁcation of roots in the format X-APT,
where X is the Root Letter (A to M) and APT is a
three-letter airport code near the site.

Due to space limitations, we provide examples of spe-
ciﬁc letters rather than reporting data for all anycast de-
ployments. We focus predominantly on E- and K-Root,
since they provide anycast deployments with dozens of
sites. These examples concretely illustrate of the oper-
ational choices (§2.2) all anycast deployments face.

Data cleaning: We take several steps to clean RIPE
data for using it in our analysis. Cleaning preserves
nearly all VPs (more than 9000 of the 9363 currently
active in May 2016), but discards data that appears in-
correct or provides outliers. We discard data from VPs
with Atlas ﬁrmware before version 4570. Atlas ﬁrmware
is regularly updated [44], and version 4570 was released
in early 2013. Out of caution, we discard measurements
from earlier ﬁrmware on non-updating VPs to provide
consistent (current) methods of measurement. More-
over, we also discard measurements of a few VPs where
traﬃc to a root appears to be served by third parties.
We identify hijacking in 74 VPs (less than 1%) by the
combination of a CHAOS reply that does not match
that letter’s known patterns and unusually short RTTs
(less than 7 ms), following prior work [23].

After cleaning we map all observations into a time
series with ten-minute bins. In each time bin we iden-
tify, for each Root Letter, the response: either a site the
VP sees, a response error code [39], or an absence of a
reply after 5 seconds (the Atlas timeout). Each time
bin represents 2.5 RIPE probing intervals, allowing us
to synchronize RIPE measurements that otherwise oc-
cur at arbitrary phases. (When we have diﬀering replies
in one bin, we prefer sites over errors, and errors over
missing replies.)

Limitations of RIPE Atlas: RIPE Atlas has
known limitations: although VPs are global, their lo-
cations are heavily biased towards Europe. This bias
means Europe is strongly over-represented in per-letter
reachability (§3.2), but it does not inﬂuence our anal-
ysis of speciﬁc user behavior (§3.4). The largest risk
uneven distribution of VPs poses is that some anycast
sites may have too few VPs to provide reliable report-
ing. While we report on all anycast sites we observe, we

259only consider sites whose catchments contain a median
of at least 20 VPs during the two days.

In addition, RIPE VPs query speciﬁc Root letters,
so they do not represent “user” queries. (Regular user
queries employ a recursive resolver selects one or more
letters to query.) We take advantage of this approach
to study speciﬁc letters and sites (§3), but it pre-
vents us from studying Root DNS reachability as a
whole (§3.2.2).

Finally, VPs fail independently. We focus our atten-
tion on sites typically seen by 20 or more VPs to avoid
bias from individual VP failure over the two days.

2.4.2 RSSAC-002

RSSAC-002 is a speciﬁcation for operationally-
relevant data about the Root DNS [52].
It provides
daily, per-letter query rates and distributions of query
sizes.

All Root Letters have committed to provide RSSAC-
002 data by 2017. At the time of the events, only
ﬁve services (A, H, J, K, and L) were providing this
data [48]. In addition, RSSAC-002 monitoring is a “best
eﬀort” activity that is not considered as essential as op-
erational service, so reporting may be incomplete, par-
ticularly at times of stress.

2.4.3 BGPmon

We use BGP routing data from BGPmon [62]. BGP-
mon has peers to dozens of routers providing full rout-
ing tables from diﬀerent locations around the Internet.
We use data from all available peers on the event days
(152 peers) to evaluate route changes at anycast sites
in §3.4.1.

3. ANALYSIS OF THE EVENTS

To evaluate the events we begin with overall estimates
of their size, then drill down on how the events aﬀected
speciﬁc Root Letters, sites in some letters, and individ-
ual servers at those sites. We then reconsider the eﬀects
of the attack as a whole, both on Root DNS service and
on other services.

3.1 How Big Were the Events?

We next estimate the size of the events. Under-
standing the size is important to gauge the level of
resources available to the traﬃc originator. We begin
with RSSAC-002 reports, but on Nov. 30, only a few
letters provided this data, and as previously described
(§2.4.2), best-eﬀort RSSAC-002 data is incomplete. We
therefore estimate an upper-bound on the event based
on inference from available data.

RSSAC-002 statistics over each day, so to estimate
the event size we deﬁne a baseline as the mean of the
seven days before the event. We then look at what
changed on the two event days (A-Root had an indepen-
dent attack on 2015-11-28, so we drop this data point
and scale proportionally). Query sizes are reported in

bins of 16 bytes. Verisign stated that the attacks were of
speciﬁc query names (see §2.3), and RSAAC-002 reports
query sizes in bins of 16 bytes, allowing us to identify
attacks by unusually popular bins.
For queries, the
32-to-47 B bin on Nov. 30 and the 16-to-32 B bin on
Dec. 1 while response sizes were between 480 and 495
bytes for both events. These sizes are for DNS payload
only. We conﬁrm total traﬃc size (with headers) in two
ways, both by adding 40 bytes to account for IP, UDP,
and DNS headers, and by generating queries with the
given attack names. We conﬁrm full packets (payload
and header) of 84 and 85 bytes for queries and 493 or
494 bytes for responses, consistent with RSSAC-002 re-
ports. We use these sizes to estimate incoming bitrates.
Table 3 gives our estimates on event traﬃc from
the ﬁve letters reporting RSSAC-002 statistics. The
baseline (right column) is only 1–10% of attack traﬃc
(mean: 3%); we subtract the baseline from queries and
responses therefore our estimations show the only the
extra (∆) traﬃc caused by the events. These reported
values diﬀer greatly across letters and between queries
and responses. We believe diﬀerences across letter rep-
resent measurement errors, with most letters under-
measuring traﬃc when under attack (under-reporting is
consistent with large amounts of lost queries described
in §3.2). We see fewer responses than requests, likely be-
cause of Response Rate Limiting [57] which suppresses
duplicate queries from the same source address [60]. We
provide both a lower-bound on attacks that considers
only known event traﬃc, and a scaled value that ac-
counts for the six sites known to have been attacked
that did not provide RSSAC-002 data at event time.
This lower bound has a large underestimate because 3
of the 4 reports were known to drop event traﬃc, and
there is an approximate 3% overestimate by including
baseline queries.

We propose an upper-bound for event size by cor-
recting for both of these types of under-reporting. To
correct, we accept that A-Root’s RSSAC-002 data mea-
sured the entire event. Verisign reported [60] A-Root
graphs of input traﬃc showing about 5Mq/s at both A-
and J-Root (although J’s RSSAC-002 reports are much
lower). They also report that 10 of 13 letters were at-
tacked (D, L, and M were not attacked). We add the
assumption that all attacked letters received equal traf-
ﬁc. We conﬁrm this assumption in two ways. First,
B-Root can conﬁrm [5] that it saw oﬀered load around
5 Mq/s, consistent with A-Root’s statement. Second,
we can infer event sizes by comparing accepted traﬃc
loads in Table 3 with observed loss rates from Figure 3.
That suggests H-Root should have received 1.6 Mq/s,
J-Root about 2.44 Mq/s, and K-Root about 1.6 Mq/s.
Given these reports and this assumption, our best es-
timate of actual attack strength is somewhere between
half and all of our upper-bound estimate. This esti-
mate is somewhat rough, but it provides strong evi-
dence that this was not a small attack. While 6× more
than the lower-bound of directly observed traﬃc, this

260RSSAC
reports Mq/s Gb/s M IPs (ratio) Mq/s Gb/s Mq/s Gb/s M IPs (ratio) Mq/s

∆ responses

∆ queries

∆ queries

∆ responses

2015-11-30 (160 min.)

2015-12-01 (60 min.)

Baseline
queries

Gb/s Mq/s M IPs

A

H

J

K

L

5.12

0.23

1.90

1.07

3.44

0.15

1.28

0.72

1,813 (340×)

3.84

15.13

36.14 (13.3×) —

765.24 (280×)

39.23 (14.4×)

1.10

0.48

—

4.32

0.32

5.21

0.32

2.29

1.12

3.54

0.22

1.56

0.76

1,345 (253×)

16.22 (6.5×)

355.68 (129×)

40.88 (15.0×)

3.93

—

1.43

0.28

15.53

—

5.66

1.09

0.05* 0.04* 36.15 (13.3×)* 0.05*

0.19*

0.10* 0.07* 16.22 (6.5×)*

0.09*

0.37*

bounds (lower and upper):

lower

5.59
8.32
(20.8) (14.0)
(scaled)
upper 51.22 34.42

–
–
–

19.77
(49.4)

6.08
8.94
5.42
(22.4) (15.2)
(13.5)
38.37 151.31 52.09 35.42

–
–
–

22.28
5.64
(55.2)
(14.1)
39.31 155.35

0.04

0.03

0.05

0.04

0.06

0.22

–

5.35

2.94

2.78

2.92

2.94

–
–

Table 3: RSSAC-002 reports for daily IPv4/UDP traﬃc for the two days of events, subtracted from the a 7-day
mean baseline, and lower- and upper-bounds on event sizes. *L-Root was not attacked and therefore excluded from
lower and upper bounds.

estimate reﬂects signiﬁcant query loss that occurs dur-
ing the event and in measurement systems tuned for
regular operation.

If our upper-bound estimate is correct, the aggregate
size of this attack across all letters is about 35–40 Gb/s.
Although attacks exceeding 100 Gb/s have been demon-
strated since 2012 [2, 4], such large attacks are usually
performed using ampliﬁcation [51] (for example, as the
reply traﬃc of 151 Gb/s on Table 3). Directly sourced
traﬃc of 35 Gb/s on the roots therefore represents a
large attack.

We can also see for all letters a large increase (by
a factor 6.5× to 340×) in the number of unique IPv4
addresses observed by each letter during the attacks.
This observation conforms with the initial reports on
the use of IP spooﬁng during these attacks [60].

3.2 How Were Individual Letters Af-

fected?

We next consider how each letter reacted to the event
and measure overall Root DNS performance. Letter-
speciﬁc queries from RIPE Atlas show that individual
root letters suﬀered minimal to severe loss rates. We
caution that these loss rates do not directly translate
to end-user delays, since recursive resolvers cache and
retry against diﬀerent letters (§3.2.2), and since users
interact with speciﬁc anycast sites (§3.3).

3.2.1 Reachability of Speciﬁc Letters

Figure 3 shows the reachability for each Root Letter
from RIPE Atlas. We plot D-, L-, and M-Root together
because they see no visible change, consistent with re-
ports that they were not attacked [58]. (In §3.6 we later
show that a few D-root sites appear slightly aﬀected by
the event.) On these dates, Atlas probed A-Root less
frequently than other letters (§2.4.1), so in this graph
we scale A’s observations to account for this diﬀerence.
Because infrequent probing of A-Root makes the event

 9000

 2000
 0

5000

0
9000

1000

7000

4500

0
9000

6000

0

s
e
i
r
e
u
q

 
l

u

f
s
s
e
c
c
u
s
 

h

t
i

w
 
s
P
V

 
f

o

 
r
e
b
m
u
n

B

E

G

I

K

C

F

H

J

A

D

L

M

0 5 10 15 20 25 30 35 40 45

0 5 10 15 20 25 30 35 40 45

hours after 2015-11-30t00:00 UTC

Figure 3: Number of VPs with successful queries (in
10-minute bins). (All plots are scaled consistently, with
nearly 9000 VPs across 48 hours of observation. In all
graphs, dotted lines highlight approximate event start
times. Here they also show the lowest values for the
dips.)

dynamics impossible to discern, we omit A-Root from
analysis in the rest of the paper.

All the other letters experience diﬀerent degrees of
reachability problems during the reported attack inter-
vals (§2.3). There is a strong correlation (R2 = 0.87)
between how many sites a letter has (Table 2) and their
worst responsiveness, measured by the smallest number
of Atlas VPs that successfully receive responses during
the events (more sites→more VPs receive responses).
B-Root, a unicast letter, suﬀered the most, followed by

261)
s
m

 

(
 
T
T
R
n
a
d
e
m

i

 350

 300

 250

 200

 150

 100

 50

 0

 0

B-Root
C-Root
G-Root
H-Root
K-Root

G-Root

B-Root

H-Root

C-Root

K-Root

 5

 10

 15

 20

 25

 30

 35

 40

 45

hours after 2015-11-30t00:00 UTC

Figure 4: Median RTT for some letters during the at-
tacks. Letters with no signiﬁcant change (A, D, E, F,
I, J, L, and M) are omitted.

H, with two sites and primary-secondary routing. With
many sites, J-Root sees some VPs lose service, but only
a few. We evaluate the causes for service loss in §3.3,
but this correlation reﬂects some combination of more
sites providing greater aggregate capacity and isolating
some users from some event traﬃc.

We can also evaluate overall performance for each
letter by the RTT of successful queries, as shown in
Figure 4. Note that each letter has a diﬀerent base-
line RTT, corresponding with the median distance from
Atlas VPs to anycast sites for that letter. Although B-
Root suﬀered the most in terms of reachability Figure 3,
it experienced little change in RTT when queries were
successful. G- and H-Root, in turn, see large changes in
latency. In the next section we show that anycast sites
can fail, causing routing to shift their traﬃc to other
locations. Thus we believe these shifts in RTT indi-
cate route changes that shift VP traﬃc to more distant
sites. For example, H-Root has sites on the U.S. East
and West coasts (north of Baltimore, Maryland, and in
San Diego, California). Most Atlas VPs are in Europe,
so we infer that the primary site for H is the U.S. East
coast, but when that route is withdrawn (during both
events) traﬃc shifts to the west coast. This assumption
is conﬁrmed by H’s median RTT at that time matching
B-Root’s RTT, since B-Root is also on the U.S. West
coast. We examine site route withdrawals in more de-
tails in §3.3.

3.2.2 Reachability of the Root DNS as a Whole
While we see that individual letters show degraded
responsiveness under stress, the DNS protocol has sev-
eral levels of redundancy, and a non-response from one
letter should be met by a retry at another letter. This
paper does not evaluate overall responsiveness of the
Root DNS, but our per-letter analysis shows some evi-
dence of this redundancy.

L-Root was not subject to these attacks [60], yet
Table 3 it exhibited that L-Root shows a signiﬁcant

increase in query rate during the second event, with
a 1.66× increase in queries-per-second. More impres-
sively, it sees a 6- or 13-fold increase in number of unique
IPs on both event dates. We later describe “site ﬂips”,
where VPs change anycast sites (§3.4.1); this coarse
data suggests letter ﬂips also occur, as recursive re-
solvers switch from one letter to another, perhaps to
prefer a shorter RTT [63, 36]. While not the focus of
this paper, these letter ﬂips show the multiple levels of
resilience in the Root DNS system.

3.3 How Were Anycast Sites Affected?

Overall loss rates for each letter (§3.2) may suggest
that query loss is uniform for all who use that letter. We
next show that these loss rates are not uniformly seen
by all users. Anycast services are composed of multiple
sites (Table 2), and anycast operators and their hosting
ISPs can design sites to withdraw routes or continue
as degraded absorbers when under stress (§2.2). We
next look at behavior across all sites of a given letter to
identify evidence of these policies in action.

3.3.1 Site Reachability

We ﬁrst consider site reachability: how many Atlas
VPs reach a letter’s sites over the two days of obser-
vations, measured in each ten-minute bin. The median
number of VPs over the observation provides a base-
line of “regular” behavior, calibrating how RIPE Atlas
maps to a given service. Atlas coverage is incomplete;
some sites have zero or a few VPs, while others have
thousands in their regular catchment. Our use of me-
dian normalizes coverage to identify trends, such as if
the site adds or loses VPs. Addition of VPs to one site
indicates withdrawal of routes to another site, possibly
in reaction to stress. Reduction in VPs indicates that
either that site withdrew some or all routes, or that it
was overloaded and simply lost queries—reduction can
therefore be caused by both withdrawal and absorption.
Figure 5 shows all sites for two letters (E- and
K-Root, selected as representatives with many sites).
Numbers in parenthesis show the median number of
VPs at each site, while the lines show how much that
site shrank or grew over the two days, normalized to
the median.

We see that sites show two responses indicating re-
duced capacity. Some (such as E-AMS) become com-
pletely unavailable, as shown by the minimum drop-
ping to zero; some become nearly unavailable, such
as K-LHR; K-Root conﬁrmed unavailability of some
sites [65]. Others (E-NRT, K-WAW) become partially
available.

In addition, several sites show an increase above me-
dian over the period (the maximum blue value is greater
than 1). Several of the well-observed K-Root sites show
some increase (K-AMS, K-LHR, K-LED, K-NRT), as
do many of the well-observe E-Root sites (E-FRA, E-
LHR, E-ARC, E-VIE, E-IAD).

262i

n
a
d
e
m
o

 

t
 
s
P
V

 
f

o

 

o

i
t

a
r

i

n
a
d
e
m
o

 

t
 
s
P
V

 
f

o

 

o

i
t

a
r

)
0
2
5
1
(
 

S
M
A
E

-

)
8
6
4
1
(
 

A
R
F
-
E

)
1
3
1
1
(
 

R
H
L
-
E

)
9
8
0
1
(
 

C
R
A
E

-

 3.5

 3

 2.5

 2

 1.5

 1

 0.5

 0

 3

 2.5

 2

 1.5

 1

 0.5

 0

)
9
9
3
(
 

I

E
V
E

-

)
3
8
3
(
 

G
D
C
E

-

)
1
7
3
(
 

C
M
E

-

)
2
6
2
(
 

G
P
Q
E

-

)
3
2
2
(
 

D
R
O
E

-

)
7
9
1
(
 

P
B
K
E

-

)
3
6
1
(
 

H
R
Z
-
E

)
0
5
1
(
 

D
A

I
-

E

)
8
3
1
(
 

O
A
P
E

-

)
4
3
1
(
 

X
D
P
E

-

)
6
2
1
(
 

W
A
W
E

-

)
2
1
1
(
 
L
T
A
E

-

)
7
0
1
(
 
F
O
S
E

-

)
9
9
(
 

R
E
B
E

-

)
8
9
(
 

B
X
D
E

-

)
5
9
(
 

A
E
S
E

-

)
4
9
(
 

D
Y
S
E

-

)
8
8
(
 

V
L
N
E

-

)
2
8
(
 

I

A
M
E

-

)
2
7
(
 

N
R
T
-
E

)
7
6
(
 
T
R
N
E

-

)
8
5
(
 

P
M
T
-
E

)
7
5
(
 
Z
Y
Y
E

-

)
5
5
(
 
L
G
K
E

-

)
3
5
(
 
L
K
A
E

-

)
6
4
(
 

N
A
M
E

-

)
5
4
(
 

R
U
B
E

-

)
0
4
(
 

A
G
L
-
E

)
9
3
(
 

B
N
J
-
E

)
7
3
(
 

B
D
S
E

-

)
6
2
(
 
T
K
J
-
E

)
5
2
(
 

O
B
N
E

-

)
5
2
(
 

E
Z
E
E

-

)
0
2
(
 
T
P
C
E

-

)
9
1
(
 

R
E
P
E

-

)
8
1
(
 
L
L
T
-
E

)
5
1
(
 

A
N
S
E

-

)
3
1
(
 
L
N
M
E

-

)
3
1
(
 

O
O
C
E

-

)
2
1
(
 

M
T
K
E

-

)
1
1
(
 

O
F
S
E

-

)
0
1
(
 

N
R
T
-
E

(a) E-Root sites

)
9
(
 

I

N
S
E

-

)
9
(
 
L
Y
E

-

)
9
(
 

R
M
E

-

)
8
(
 

A
B
L
-
E

)
8
(
 

R
A
D
E

-

)
7
(
 

N
Q
N
E

-

)
5
(
 

S
O
B
E

-

)
5
(
 

G
W
Y
E

-

)
4
(
 
I
L
V
E

-

)
4
(
 

C
Y
Y
E

-

)
4
(
 

R
A
D
E

-

)
4
(
 

O
J
S
E

-

)
4
(
 
L
J
B
E

-

)
2
(
 

S
O
L
-
E

)
2
(
 

B
X
D
E

-

)
2
(
 

W
O
Y
E

-

)
2
(
 
L
G
K
E

-

)
2
(
 
Z
L
B
E

-

)
1
(
 

A
Z
G
E

-

)
1
(
 

D
A
L
-
E

)
1
(
 

M
O
B
E

-

)
1
(
 

K
R
A
E

-

)
1
(
 

O
N
R
E

-

)
1
(
 

N
C

I
-

E

)
1
(
 

Y
E
B
E

-

)
1
(
 
T
R
K
E

-

)
3
3
4
2
(
 

S
M
A
K

-

)
6
4
4
1
(
 

R
H
L
-
K

)
4
7
7
(
 

A
R
F
-
K

)
0
6
7
(
 

I

A
M
K

-

)
4
8
6
(
 

I

E
V
K

-

)
6
1
5
(
 

D
E
L
-
K

)
1
3
4
(
 
T
R
N
K

-

)
9
4
2
(
 

I

L
M
K

-

)
2
4
2
(
 

H
R
Z
-
K

)
1
1
2
(
 

W
A
W
K

-

)
4
0
2
(
 

E
N
B
K

-

)
6
9
1
(
 

G
R
P
K

-

)
1
9
1
(
 

A
V
G
K

-

)
7
7
(
 

H
T
A
K

-

)
2
6
(
 

C
K
M
K

-

)
0
6
(
 

I

X
R
K

-

)
9
4
(
 

R
H
T
-
K

)
0
4
(
 

D
U
B
K

-

)
9
3
(
 

E
A
K
K

-

)
3
3
(
 

G
E
B
K

-

)
2
3
(
 

L
E
H
K

-

)
0
3
(
 

X
L
P
K

-

)
7
2
(
 

B
V
O
K

-

)
0
2
(
 
Z
O
P
K

-

)
2
1
(
 

O
B
A
K

-

)
1
1
(
 

Y
E
R
K

-

)
1
1
(
 

N
C
B
K

-

)
1
1
(
 

N
V
A
K

-

)
5
(
 

H
O
D
K

-

)
2
(
 
L
E
D
K

-

)
1
(
 

O
N
R
K

-

(b) K-Root sites

Figure 5: Minimum and maximum number of VPs, normalized to median (shown in parenthesis per individual site),
for sites from E- and K-Root. Observations are grouped into 10-minute bins over two days. Sites are ordered by
median number of VPs, and the red, shaded area highlights sites with fewer than 20 VPs (our threshold for stability,
§2.4.1)).

We conﬁrm that these swings in catchments are di-
rectly correlated with the events and are not typical be-
havior. We repeated the analysis of Figure 5 over two
days during the week following the events (2016-12-05
and 2016-12-06)1 . On these “normal” days, considering
sites with reasonable visibility (20 or more VPs, so me-
dians are stable), we see no variation in VPs per site for
K-Root, and only minor variation (mostly within 8%)
for 13 sites of E-Root.

Second, Figure 6 shows the size of each site’s catch-
ment during the events, for E- and K-Root. Each mini-
plot represents one site, with the line showing how many
VPs are mapped to it relative to the site’s median. From
this ﬁgure we see that sites from these two letters be-
haved completely diﬀerent. While most sites of E-Root
either see an increase or a decrease on their reachabil-
ity, most sites of K-Root seem to overlook the attack.
(Note that large increases observed for few sites, such
as E-DXB and K-DEL, are caused by a very low median
(two VPs)—any additional VP hitting this sites during
the attack can cause a peak on reachability.)

Figure 6 shows that ﬁve sites from E Root (E-AMS,
E-CDG, E-WAW, E-SYD and E-NLV) seem to “shut
down” after the attack of Dec. 1 (hour 29). These sites
also had reachability strongly compromised during the
ﬁrst event on Nov. 30 (hour 7).

1Graphs omitted for space, but present in an appendix
of our technical report [40].

What is interesting to see for the sites of both letters
in Figure 6 is that sites with large numbers of median
VPs in their catchments showed reachability problems.
An exception is K-AMS, with a large number of VPs
in its catchment, which took on more traﬃc than usual
during the whole period.

For E-Root, sites that show an increase over me-
dian suggest that some other sites are withdrawing
some routes at other sites. However, that does not
explain why letters show reduced overall reachability
(Figure 3): if overloaded sites fail and traﬃc shifts, all
queries should be answered. We next look for evidence
of degraded absorption.

3.3.2 Site RTT Performance

To assess if sites that remain accessible are overloaded
(implying they operate as degraded absorbers), we next
examine RTT of successful queries.

Figure 7 shows the median RTT for some K-Root
sites that show stress during the events.
Although
the K-AMS site remained up and showed minimal loss,
its median RTT showed a huge increase: from roughly
30 ms to 1 s on Nov. 30, and to almost 2 s on Dec. 1,
strongly suggesting the site was overloaded. K-NRT
shows similar behavior, with its median RTT rising
from 80 ms to 1 s and 1.7 s in the two events. Overload
does not always result in large latencies. B-Root (a sin-
gle site) showed only modest RTT increases (Figure 4),

263E-AMS (1534)

E-FRA (1445)

E-LHR (1128)

E-ARC (1089)

E-CDG (388)

E-VIE (349)

E-QPG (259)

E-ORD (222)

E-KBP (189)

E-ZRH (156)

E-IAD (145)

E-PAO (138)

E-WAW (127)

E-ATL (110)

E-BER (98)

E-SYD (94)

E-SEA (91)

E-NLV (88)

E-MIA (87)

E-NRT (66)

E-TRN (65)

E-AKL (53)

E-MAN (46)

E-BUR (45)

E-LGA (41)

E-PER (19)

E-SNA (15)

E-LBA (8)

E-SIN (8)

E-DXB (2)

E-KGL (2)

E-LAD (1)

K-AMS (2425)

K-LHR (1440)

K-FRA (775)

K-MIA (757)

K-VIE (686)

K-LED (514)

K-NRT (442)

K-MIL (249)

K-ZRH (241)

K-WAW (208)

K-BNE (204)

K-PRG (196)

K-GVA (192)

K-ATH (77)

K-MKC (63)

K-RIX (59)

K-THR (48)

K-BUD (39)

K-KAE (38)

K-BEG (33)

K-HEL (32)

K-PLX (30)

K-OVB (27)

K-POZ (20)

K-ABO (12)

K-AVN (11)

K-BCN (11)

K-REY (11)

K-DOH (5)

K-DEL (1)

K-RNO (1)

0

7

29

45

0

7

29

45

hours after 2015-11-30t00:00 UTC

hours after 2015-11-30t00:00 UTC

(a) E-Root sites

(b) K-Root sites

Figure 6: Reachability seen by VPs that received posi-
tive responses (RCODE 0) for sites of E- and K-Root.
The central line in each plot is the median, with the
lower line 0 and the upper line 5× and 3× the me-
dian for E- and K-Root respectively. Red lines below
the median indicate potential critical moments in which
reachability dropped below the by the median number
of VPs that can normally reach the site. Sites are sorted
by median, in parenthesis.

 2000

 1500

)
s
m

K-NRT

K-AMS

 1000

i

 

(
 
T
T
R
n
a
d
e
m

K-AMS
K-LHR
K-FRA
K-MIA
K-NRT

 500

 0

 0

K-MIA

 5

 10

 15

 20

 25

 30

 35

 40

 45

hours after 2015-11-30t00:00 UTC

Figure 7: Performance for selected K-Root sites.

since only few probes could reach it during the attack
(Figure 3). We hypothesize that large RTT increases in
sites performance are the result of an overloaded link
combined with large buﬀering at routers (industrial-
scale buﬀerbloat [27]).

3.4 How Can Services Partially Fail?

We have shown that letters report diﬀerent amounts
of service degradation (Figure 3), and that their sites
seem to follow two policies under stress (§3.3). We next
look at service reachability from a client perspective
to understand how services can partially fail, and how
some clients see persistent failures.

3.4.1 Site Flips: Evidence of Stress

A design goal of DNS and IP anycast is that service
is provided by multiple IP addresses (DNS) and sites
(anycast). Through their recursive resolvers, clients can
turn to service on other IP address (other Root Let-
ters), and through route changes at upstream ISPs, to
other anycast sites. A recursive DNS resolver will au-
tomatically retry with another name server if the ﬁrst
does not respond, which is intentional redundancy in
the protocol and an operational best practice [22, 63].
Redundancy inside most letters depends on IP anycast,
and the routing policies DNS service operators establish
at each anycast site (withdraw or absorbing, as in §2.2).
To study a client’s view of IP anycast redundancy,
we look for changes in site catchments. We measure
these as site ﬂips: when a VP changes from its current
anycast site to another. We expect each VP to have a
preferred site (hopefully with low RTT), and site ﬂips
to be rare, due to routing changes or site maintenance.
Figure 8 shows site ﬂips measured in RIPE Atlas VPs,
with bursts of site ﬂips during the event periods for
letters that saw event traﬃc. All letters see thousands
of site ﬂips during the event (note the scale of the y-
axis), with E, H and K seeing many ﬂips while C, I and
J see fewer.

To evaluate if these site ﬂips are actually due to route
withdrawals, we use route data from BGPmon (§2.4.3).

2644.5

4

3.5

)

3

0
1
x
(
 
s
p

i
l
f
 

e

t
i
s
 
f

o

 
r
e
b
m
u
n

3

2.5

2

1.5

1

0.5

0

F

E

J
H

F

C

K

E
J
M

0

L
C
A

I

F
K

J

L

I

A

10

M

C

20

E

K
H
E

F

C

30

H

L

M

K

I

C

L

J

K
C

I

A

40

50

hours after 2015-11-30t00:00 UTC

Figure 8: Number of site ﬂips per Root letter.

1

0.8

0.6

0.4

0.2

0

1

0.8

0.6

0.4

0.2

0

s
p

i
l
f
 
f

o

 

n
o

i
t
c
a
r
F

from K-FRA

K-A M S

K- M IL

K-VIE

K-L H

R

K-K A E

W

A

K-L E D

K-RIX

K-N

R T

K- W

K- M IA
to

from K-LHR

K-A M S

K- M IA

K-F R A

K-L E D

K-N

R T

K-VIE

K-P R

G

K-B N E

K-K A E

K- M K C
to

1

0.8

0.6

0.4

0.2

0

 1

 0.8

 0.6

 0.4

 0.2

 0

to K-AMS

K-L H

R

K-F R A

K-L E D

K- M IA

G

K-VIE

K-Z R

H

K-K A E

K- M K C

R T

K-P R
K-N
from

from K-AMS

K-L H

R

K-F R A

K-L E D

K- M IA

K- M K C

G

R T

H

K-Z R

K-N

K-VIE

K-K A E

K-P R
to

 
s
e
g
n
a
h
c
 

e

t

u
o
r
 

l

i

i

e
b
s
v
-
n
o
m
P
G
B

 250

 200

)
s
n
b

i

 
.

 150

i

 

n
m
0
1
(
 

 100

 50

 0

GG

J
JJ
E

E
H
F

EEEEE

C
C
K
C
C
C

C
D
C
E

K
A

A

H
EE
JJ

GGGGG

HH
FF
FFFFFFFFFFF

B
A
H

J

J
AA

J J

KK
K

AA
K

F

 0

 5

 10

 15

 20

 25

E
F

E
F
H

C
G
C
C
H
E
J
G

GGG

HHHH
FFF
 30

AA
EE

J

J

J

H

H
H
H

J

J

J
J
E

HH
 40

 45

 35

hours after 2015-11-30t00:00Z

Figure 9: Route changes for each Root Letter (10
minute bins, seen from BGPmon route collectors).

These BGPmon VPs are in diﬀerent locations from our
RIPE Atlas VPs, so we do not expect them to see ex-
actly the same results, but, if there were route with-
drawals, we expect to see more routing activity during
the events.

Figure 9 shows the route changes we observe across
all Root Letters. With BGPmon VPs and Root anycast
sites around the world, we see occasional route changes
over the whole time period. With 152 VPs, a routing
change near one site can often be seen at 100 or more
VPs. But the very frequent sets of changes shown by
many letters in the two event periods (4 to 6 hours and
around 29 hours) suggests event-driven route changes
for many letters (C, E, F, G, H, J, K). Route changes
for K-Root do not appear at our BGP observers for the
second event, and K’s BGP changes are lower than we
expect based on site ﬂips. We suspect that our BGP
vantage points are U.S.-based, while site ﬂips are VPs
that are much more numerous in Europe.

3.4.2 Case Study: K-Root

We next consider K-Root as a case study to show
what site ﬂips mean in practice. K-Root’s sites provide
good examples of diﬀerent policies under stress. We

Figure 10: Site ﬂips for selected K-Root instances over
the two days.

next consider VPs (one per row) that start at K-LHR
and K-FRA (London and Frankfurt) to see what hap-
pened to these clients during the event. We select these
sites to illustrate possible design choices (§2.2) and be-
cause they lost nearly all or about half of the VPs during
the event; they were more strongly aﬀected then most
K-Root sites. From Figure 3 we know that some clients
were unsuccessful, while the maximums in Figure 5b
show that some sites gained clients.

Figure 10 shows where sites from K-LHR and K-FRA
went over the measurement period—the left two graphs
show that about 70-80% of all VPs that shifted traﬃc
during the events shift to K-AMS (Amsterdam). The
top right (red background) graph shows where new VPs
that see K-AMS just were, conﬁrming they mostly ar-
rive from K-LHR and K-FRA. The bottom right graph
shows that K-AMS sites also shift back to K-LHR and
K-FRA as their preferred catchments after the events.
if traﬃc shifts to other sites
and K has excess capacity, why do some VPs fail to
reach K during the attack? VP query failure must result
from routing policies and implementation details (§2.2)
at each site and its hosts: those policies and details can
result in a site that will continue to receive traﬃc from
its peer and operate as a degraded absorber, or that will
withdraw its route and reallocate its catchment. We see
evidence of both outcomes.

However, we still ask:

To demonstrate these policies at work, we must look
at the actions of individual VPs. Figure 11 shows 300
randomly selected VPs that start at K-LHR (yellow or
light gray) and K-FRA (salmon or medium gray) for 36
hours. Each pixel represents the site choice of that VP
in 4-minute bins. Black indicates the VP got no reply,
while blue (or dark gray) and white indicate selection
of K-AMS or some other K-Root site.

We focus on the 40 VPs shown in Figure 11b and see
two behaviors during the event and three after. Dur-
ing the event, the top 10 VPs (labeled (1)) stick to
K-LHR, but only get occasional replies. They repre-

265(1)

(2)

(3)

(4)

(a) A sample of 300 VPs; start 2015-11-30t00:00Z for 36
hours.

(b) A smaller sample:
around the ﬁrst event.

40 K-LHR-preferring VPs

Figure 11: A sample of 300 VPs for K-Root that start at K-LHR (yellow or light gray) and K-FRA (salmon or
medium gray), with locations before, during, and after attacks. Other sites are K-AMS (blue or dark gray), with
white indicating other K sites, and black fail on getting a response (timeout). Dataset: RIPE Atlas.

sent a degraded absorbing peering relationship; these
clients seem “stuck” to the K-LHR site. The next group
labeled (2) shift to K-FRA (salmon or medium gray)
during the event and for a short period after, then re-
turn to K-LHR. However, during their visit to K-AMS
only about a third of their queries are successful. This
group shows that K-AMS is overloaded but up, and that
these VPs are in ASes that are not bound to K-LHR.
For the third group, marked (3), some stay at K-LHR
during the event, while others shift to other sites, but
all ﬁnd other sites after the event. Finally, the group
(4) shifts to K-FRA during the event and remains there
afterward. We see similar groups for the K-FRA sites
in the ﬁrst event and for both sites in the second event.
We believe this kind of partial failure represents a
success of anycast in isolating some traﬃc to keep other
sites functional, but this degraded absorbing policy re-
sults in some users suﬀering during the event due to
the overload at K-LHR. While this policy successfully
protects most K-Root sites during the event, it also sug-
gests opportunities for alternate policies during attack.
Rather than let sites fail or succeed, services may choose
to control routing to engineer traﬃc to provide good
service to more users. Alternatively, if attack traﬃc is
localized, services may choose to target routing so that
only one catchment is aﬀected—a policy particularly
appropriate for attacks where all traﬃc originates from
a single location, even if it spoofs source addresses.

3.5 How Were Individual Servers Af-

fected?

Large anycast sites may operate multiple servers be-
hind a load balancer (Figure 1). We now examine
how the events aﬀected individual servers within spe-
ciﬁc anycast sites. We look at two sites of K-Root,
K-FRA and K-NRT as examples, selected because they
show diﬀerent responses to stress. These behaviors are
also seen at other sites, but we do not identify or count
behaviors across all sites. These examples show it is

 800
 700
 600
 500
 400
 300
 200
 100
 0

 350
 300
 250
 200
 150
 100
 50
 0

s
P
V

 
f

o

 
r
e
b
m
u
n

-

2
S
A
R
F
-
K

K-FRA-S1
K-FRA-S2
K-FRA-S3

-

3
S
A
R
F
-
K

0

5

10

15

20

25

30

35

40

45

hours after 2015-11-30t00:00 UTC

K-NRT-S1
K-NRT-S2
K-NRT-S3

Figure 12: Reachability for individual servers from K-
FRA (top) and K-NRT (bottom).

important to use measurement strategies that consider
all servers at a given site.

Figure 12 shows a time series of servers that re-
spond at K-FRA (top) and K-NRT (bottom) during the
events. At K-FRA, we typically saw replies from each of
the three servers. As the load of each event rose, replies
shifted to come from only one server, with none from
the other two we previously saw replying. Which server
responded was diﬀerent in the two events, with K-FRA-
S2 replying in the ﬁrst event and -S3 in the second. We
do not know if the other two servers failed, or if they
were only serving attack traﬃc, or if traﬃc from these
VPs was somehow isolated from attack traﬃc. Either
way, this strategy seems to work reasonably well since
Figure 13 shows that, after a short increase in RTT at
the beginning of the attack, the median RTT for K-
FRA remains stable for successful replies throughout
the attack. However, K-FRA seems to be overloaded
and dropping queries, as shown in Figure 6b and Fig-
ure 11b.

K-Root’s Tokyo site (K-NRT) shows a diﬀerent re-
sult. Figure 12 (bottom) shows that VPs had diﬃ-

266 100

 80

 60

 40

 20

 0

 2500

)
s
m

 

(
 
T
T
R
n
a
d
e
m

i

 2000

 1500

K-FRA-S1
K-FRA-S2
K-FRA-S3

K-NRT-S1
K-NRT-S2
K-NRT-S3

 1000

 500

 0

0

5

10

15

20

25

30

35

40

45

hours after 2015-11-30t00:00 UTC

 660

 620

 580

 540

 120

 100

 80

 60

 40

 20

s
P
V

 
f

o

 
r
e
b
m
u
n

 0

 0

D-FRA

D-SYD

D-AKL

D-DUB

D-BUR

 5

 10

 15

 20

 25

 30

 35

 40

 45

hours after 2015-11-30t00:00 UTC

Figure 13: Performance for individual servers from K-
FRA (top) and K-NRT (bottom).

Figure 14: Reachability of those D-Root sites that were
aﬀected by the DDoS.

culty reaching all three servers from K-NRT during the
events. This diﬃculty suggests that the events aﬀected
all K-NRT servers, either because load balancing was
mixing our observations with attack traﬃc, or because
attack traﬃc was congesting a shared link. Figure 13
(bottom) shows larger latencies for successful queries at
K-NRT, perhaps suggesting queuing at the router. We
also observe that K-NRT-S2 seems more heavily loaded
than the other two servers at K-NRT.

These examples show that individual server perfor-
mance and reachability may not reﬂect overall site-wide
performance and reachability. Measurement studies of
anycast services should therefore insure they study all
servers at a site (not just speciﬁc servers) to get a
complete picture of site and end-user-perceived perfor-
mance.

3.6 Are There Signs of Collateral Dam-

age?

Servers today, such as the Root DNS servers we study,
sometimes are located in data centers that are shared
with other services. These services may be unrelated,
other infrastructure (such as other top-level domains,
TLDs), or even other Root DNS sites. Co-locating ser-
vices creates some degree of shared risk, in that stress
on one service may spill over into another causing collat-
eral damage. Collateral damage is a common side-eﬀect
of DDoS, and data centers and operators strive to mini-
mize collateral damage through redundancy, overcapac-
ity, and isolation. Prior reports describe it as a problem
but provide few details [43].

Hosting details are usually considered proprietary,
and commonality can exist at many layers, from the
physical facility to peering to upstream providers, mak-
ing it diﬃcult to assess shared risk. From public data
we therefore cannot establish direct causation in a spe-
ciﬁc common point. Instead, we assess shared risk by
end-to-end evaluation: we look for service problems in
other services not directly the target of event traﬃc.
We study two services: D-Root, a letter that was not

NL-FRA

NL-AMS

s
e
c
n
a

t
s
n

i
 
l

n

.

0

7

29

45

hours after 2015-11-30t00:00 UTC

Figure 15: Normalized number of queries for .nl, mea-
sured at the servers in 10 minutes bins.

directly attacked [58], and the .nl TLD. They are cho-
sen because they both show reduced end-to-end perfor-
mance with timing consistent with the events, strongly
suggesting a shared resource with event targets.

D-Root: Figure 14 shows the absolute counts of the
number of RIPE Atlas VPs that reach several D-Root
sites. D-Root has many sites; we report only subsets
that had at least a 10% decrease in reachability during
the time of the attacks and were reached by at least 20
RIPE Atlas probes.

These ﬁgures show that D-FRA and D-SYD sites
both lost VPs during the event. Which data centers
host these sites is not public, but correlation of these
changes with the events suggests potential collateral
damage. (Recall that RIPE VPs probe only one letter,
so a reduction in VPs to one site implies either query
loss or re-routing, not switching to another letter.)

Frankfurt: There are seven Root Letters hosted in
Frankfurt (A, C, D, E, F, I, and K), and we previously
observed that traﬃc shifted to K-FRA and yet that site
suﬀered loss (§3.4.2).

D-FRA sees only small decreases in traﬃc, suggest-
ing it was only slightly aﬀected by the events to sites
for other letter in the same city. However, this change
indicates some collateral damage for D-FRA.

The .nl Top-Level Domain: Finally, we have also
observed collateral damage at servers that are not part
of the Root DNS. We see evidence for collateral dam-
age occurring to the .nl top-level domain.
In addi-
tion to four unicast deployments, SIDN operates .nl on

267multiple anycast services. Figure 15 shows query rates
for two anycast deployments located near Root DNS
servers (exact rates and locations are anonymized). We
see both sites show nearly no queries during both events.
As a result of this collateral damage, during this period,
.nl service was carried by other .nl servers.

4. RELATED WORK

Distributed Denial-of-Service attacks is a broad area
of study and it has been addressed from many diﬀerent
angles in the past years. Studies have shown that DDoS
attacks are eﬀective [59].

DDoS attacks are common and growing: Arbor has
documented their increasing use and growth in size [2,
3], and there has been DDoS attacks currently reaching
540 Gb/s [4]. Very large attacks often use diﬀerent pro-
tocols to amplify basic attack traﬃc [51, 56, 20]. Yet
DDoS-for-hire (“Booter” services) are easily available for
purchase on the gray market—for only a few U.S. dol-
lars, Gb/s attacks can be ordered on demand [53, 32].
Some approaches have been proposed to mitigate am-
pliﬁcation [57, 33], spooﬁng [24], or collateral dam-
age [18]. The continued and growing attacks show that
mitigation has been incomplete and that spooﬁng re-
mains widespread [9].

Many studies have looked at the Root DNS server sys-
tem, considering performance [11, 26, 12, 35, 19, 7, 54,
38, 16, 34, 23, 37, 8], client-server aﬃnity [54, 10], and
eﬀects of routing on anycast [6, 13], as well a proposal
to improve anycast performance in CDNs [25]. We draw
on prior measurement approaches, particularly the use
of CHAOS queries to identify anycast catchments [23].
Closest to our work are prior analyses of the Nov. 30
events [49, 60, 58, 65]. These reports lend insight into
the events, but were high level [49, 65] or reported only
on speciﬁc letters [60, 58, 65].

To the best of our knowledge, our paper is the ﬁrst
to combine multiple sources of measurement data to as-
sess how a DDoS attack aﬀects the several layers of the
anycast deployment of Root DNS service. In addition,
we are aware of no prior public studies on diverse any-
cast infrastructure operating under stress, including at
the site and server level and its consequences on other
services (collateral damage).

5. FUTURE WORK

Study of new events [50] can always provide new ex-
amples to strengthen our analysis.
In addition, while
we focus on IP anycast under stress, a full evaluation of
Root DNS performance needs to consider the eﬀects of
caching and how recursive resolvers select and failover
across diﬀerent anycast services for the same DNS zone.
More important is to consider improving defenses.
While additional anycast sites increase capacity, our
work shows the importance of managing traﬃc across
diverse sites (varying in capacity), since attackers are

often unevenly distributed, and suggests potential di-
rections for future improvements (§2.2).

6. CONCLUSIONS

This paper provides the ﬁrst evaluation of anycast
services under DDoS. Our work evaluates the Nov. 30
and Dec. 1, 2015 events on the Root DNS, evaluating
the eﬀects of those events on 10 diﬀerent architectures,
with most analysis based on publicly available data.
Our analysis shows diﬀerent behaviors across diﬀerent
letters (each a separate anycast services), at diﬀerent
sites of each letter, and at servers inside some sites. We
identify the role of diﬀerent policies at overloaded any-
cast sites: the choice to absorb attack traﬃc to protect
other sites, or to withdraw service in hope that other
sites can cover. We believe overall DNS service was ro-
bust to this attack, due to caching and the availability
of multiple letters for service. However, we show that
large attacks can overwhelm some sites of some letters.
In addition, we show evidence that high traﬃc on one
service can result in collateral damage to other services,
possibly in the same data center. Our study shows the
need to understand anycast design for critical infras-
tructure, paving the way for future study in alternative
policies that may improve resilience.

Acknowledgments
The authors would like to thank Arjen Zonneveld, Jelte
Jansen, Duane Wessels, Ray Bellis, Romeo Zwart, Colin
Petrie, Matt Weinberg, Piet Barber, Alba Regalado, our
shepherd Dave Levin, and the anonymous IMC reviewers
for their valuable comments on paper drafts.

This research has been partially supported by measure-
ments obtained from RIPE Atlas, an open measurements
platform operated by RIPE NCC.

Giovane C. M. Moura, Moritz M¨uller and Cristian Hes-
selman developed this work as part of the SAND project
(http://www.sand-project.nl). Ricardo de O. Schmidt and
Wouter de Vries’ work is sponsored by the SAND and DAS
(http://www.das-project.nl) projects. John Heidemann and
Lan Wei’s work is partially sponsored by the U.S. Dept.
of Homeland Security (DHS) Science and Technology Di-
rectorate, HSARPA, Cyber Security Division, via SPAWAR
Systems Center Paciﬁc under Contract No. N66001-13-C-
3001, and via BAA 11-01-RIKA and Air Force Research
Laboratory, Information Directorate under agreement num-
bers FA8750-12-2-0344 and FA8750-15-2-0224. The U.S.
Government is authorized to make reprints for Governmen-
tal purposes notwithstanding any copyright. The views con-
tained in herein are those of the authors and do not neces-
sarily represent those of DHS or the U.S. Government.

7. REFERENCES
[1] Abley, J., and Lindqvist, K. Operation of anycast
services. RFC 4786, Internet Request For Comments,
Dec. 2006. (also Internet BCP-126).

[2] Arbor Networks. Worldwide infrastructure security

report, Sept. 2012. Volume VIII.

[3] Arbor Networks. Worldwide infrastructure security

report, Jan. 2014. Volume IX.

268[4] Arbor Networks. Rio Olympics Take the Gold for

540gb/sec Sustained DDoS Attacks!
https://www.arbornetworks.com/blog/asert/rio-oly
mpics-take-gold-540gbsec-sustained-ddos-attacks/,
Aug. 2016.

[5] B-Root Operators. Personal communication, Dec.

2015.

[6] Ballani, H., and Francis, P. Towards a Global IP

Anycast Service. In Proceedings of the 6th ACM
SIGCOMM conference on Internet measurement
(Aug. 2007), pp. 301–312.

[7] Ballani, H., Francis, P., and Ratnasamy, S. A

Measurement-based Deployment Proposal for IP
Anycast. In Proceedings of the ACM Internet
Measurement Conference (Oct. 2006), IMC, ACM,
pp. 231–244.

[8] Bellis, R. Researching F-root Anycast Placement

Using RIPE Atlas.
https://labs.ripe.net/Members/ray bellis/researching
-f-root-anycast-placement-using-ripe-atlas, Oct. 2015.
[9] Beverly, R., Berger, A., Hyun, Y., and Claffy,

K. Understanding the eﬃcacy of deployed Internet
source address validation ﬁltering. In Proceedings of
the ACM Internet Measurement Conference (Nov.
2009), IMC, ACM, pp. 356–369.

[10] Boothe, P., and Bush, R. Anycast Measurements
Used to Highlight Routing Instabilities. NANOG 34,
May 2005.

[11] Brownlee, N., Claffy, K., and Nemeth, E. DNS

Root/gTLD Performance Measurement. In
Proceedings of the USENIX Large Installation System
Administration conference (Dec. 2001), pp. 241–255.

[12] Brownlee, N., and Ziedins, I. Response Time

Distributions for Global Name Servers. In Proceedings
of the International conference on Passive and Active
Measurements (Mar. 2002), PAM.

[13] Bush, R. DNS Anycast Stability: Some Initial
Results. CAIDA/WIDE Workshop, Mar. 2005.

[14] Bush, R., Karrenberg, D., Kosters, M., and

Plzak, R. Root name server operational
requirements. RFC 2870, Internet Request For
Comments, June 2000. (also Internet BCP-40).
[15] Calder, M., Flavel, A., Katz-Bassett, E.,
Mahajan, R., and Padhye, J. Analyzing the
Performance of an Anycast CDN. In Proceedings of
the ACM Internet Measurement Conference (Oct.
2015), IMC, ACM, pp. 531–537.

[16] Castro, S., Wessels, D., Fomenkov, M., and

Claffy, K. A Day at the Root of the Internet. ACM
Computer Communication Review 38, 5 (Apr. 2008),
pp. 41–46.

[17] Chirgwin, R. Linode: Back at last after ten days of

hell. The Register,
http://www.theregister.co.uk/2016/01/04/linode bac
k at last after ten days of hell/, Jan. 2016.

[18] Chou, J. C.-Y., Lin, B., Sen, S., and Spatscheck,

O. Proactive surge protection: a defense mechanism
for bandwidth-based attacks. IEEE/ACM
Transactions on Networking (TON) 17, 6 (Dec. 2009),
pp. 1711–1723.

[19] Colitti, L. Eﬀect of anycast on K-root. 1st

DNS-OARC Workshop, July 2005.

[20] Czyz, J., Kallitsis, M., Gharaibeh, M.,

Papadopoulos, C., Bailey, M., and Karir, M.
Taming the 800 Pound Gorilla: The Rise and Decline

of NTP DDoS Attacks. In Proceedings of the ACM
Internet Measurement Conference (Nov. 2014), IMC,
ACM, pp. 435–448.

[21] Eastlake, D., and Andrews, M. Domain Name

System (DNS) Cookies. RFC 7873 (Proposed
Standard), May 2016.

[22] Elz, R., Bush, R., Bradner, S., and Patton, M.

Selection and operation of secondary DNS servers.
RFC 2182, Internet Request For Comments, July
1997. (also Internet BCP-16).

[23] Fan, X., Heidemann, J., and Govindan, R.

Evaluating anycast in the Domain Name System. In
Proceedings of the IEEE Infocom (Apr. 2013), IEEE,
pp. 1681–1689.

[24] Ferguson, P., and Senie, D. Network ingress

ﬁltering: Defeating denial of service attacks which
employ IP source address spooﬁng. RFC 2267,
Internet Request For Comments, May 2000.

[25] Flavel, A., Mani, P., Maltz, D., Holt, N., Liu,

J., Chen, Y., and Surmachev, O. Fastroute: A
scalable load-aware anycast routing architecture for
modern CDNs. In 12th USENIX Symposium on
Networked Systems Design and Implementation (May
2015), pp. 381–394.

[26] Fomenkov, M., Claffy, K., Huffaker, B., and

Moore, D. Macroscopic Internet Topology and
Performance Measurements From the DNS Root
Name Servers. In Proceedings of the USENIX Large
Installation System Administration conference (Dec.
2001), pp. 231–240.

[27] Gettys, J., and Nichols, K. Buﬀerbloat: dark

buﬀers in the Internet. Communications of the ACM
55, 1 (Jan. 2012), pp. 57–65.

[28] Gillman, D., Lin, Y., Maggs, B., and Sitaraman,

R. K. Protecting websites from attack with secure
delivery networks. IEEE Computer 48, 4 (Apr. 2015),
26–34.

[29] H-Root Operators. Personal communication, Apr.

2016.

[30] Holterbach, T., Pelsser, C., Bush, R., and
Vanbever, L. Quantifying interference between
measurements on the RIPE Atlas platform. In
Proceedings of the ACM Internet Measurement
Conference (Oct. 2015), IMC, ACM, pp. 437–443.

[31] John, J. P., Moshchuk, A., Gribble, S. D., and

Krishnamurthy, A. Studying spamming botnets
using Botlab. In Proceedings of the 6th USENIX
Symposium on Network Systems Design and
Implementation (Boston, Massachusetts, USA, Apr.
2009), USENIX.

[32] Krebs, B. Israeli Online Attack Service ‘vDOS’

Earned $ 600,000 in Two Years
http://krebsonsecurity.com/2016/09/israeli-online-att
ack-service-vdos-earned-600000-in-two-years/, Sept.
2016.

[33] K¨uhrer, M., Hupperich, T., Rossow, C., and
Holz, T. Exit from Hell? Reducing the Impact of
Ampliﬁcation DDoS Attacks. In 23rd USENIX
Security Symposium (Aug. 2014), pp. 111–125.

[34] Lee, B.-S., Tan, Y. S., Sekiya, Y., Narishige, A.,

and Date, S. Availability and Eﬀectiveness of Root
DNS servers: A long term study. In Proceedings of the
IEEE Network Operations and Management
Symposium (Apr. 2010), NOMS, pp. 862–865.

269[35] Lee, T., Huffaker, B., Fomenkov, M., and

[52] RSSAC. Advisory on measurements of the Root

Claffy, K. On the problem of optimzation of DNS
root servers’ placement. In Proceedings of the
International conference on Passive and Active
Measurements (Mar. 2003), PAM.

[36] Lentz, M., Levin, D., Castonguay, J., Spring,

N., and Bhattacharjee, B. D-mystifying the D-root
Address Change. In Proceedings of the ACM Internet
Measurement Conference (2013), IMC, ACM,
pp. 57–62.

[37] Liang, J., Jiang, J., Duan, H., Li, K., and Wu, J.
Measuring Query Latency of Top Level DNS Servers.
In Proceedings of the International conference on
Passive and Active Measurements (Mar. 2013), PAM,
pp. 145–154.

[38] Liu, Z., Huffaker, B., Fomenkov, M., Brownlee,

N., and Claffy, K. Two Days in the Life of the
DNS Anycast Root Servers. In Proceedings of the
International conference on Passive and Active
Measurements (Apr. 2007), PAM, pp. 125–134.

[39] Mockapetris, P. Domain names - implementation

and speciﬁcation. RFC 1035, Nov. 1987.

Server System, Nov. 2014.

[53] Santanna, J. J., van Rijswijk-Deij, R.,

Hofstede, R., Sperotto, A., Wierbosch, M.,
Zambenedetti Granville, L., and Pras, A.
Booters-An analysis of DDoS-as-a-service attacks. In
IFIP/IEEE Intl. Symposium on Integrated Network
Management (IM) (May 2015), IEEE, pp. 243–251.

[54] Sarat, S., Pappas, V., and Terzis, A. On the use

of Anycast in DNS. In Proceedings of the 15th
International Conference on Computer
Communications and Networks (Oct. 2006), pp. 71–78.

[55] Shaikh, A., Kalampoukas, L., Dube, R., and

Varma, A. Routing stability in congested networks:
Experimentation and analysis. In Proceedings of the
ACM SIGCOMM Conference (Aug. 2000), ACM,
pp. 163–174.

[56] van Rijswijk-Deij, R., Sperotto, A., and Pras,
A. DNSSEC and Its Potential for DDoS Attacks: a
comprehensive measurement study. In Proceedings of
the ACM Internet Measurement Conference (Nov.
2014), IMC, ACM, pp. 449–460.

[40] Moura, G. C. M., de O. Schmidt, R., Heidemann,

[57] Vixie, P. Response Rate Limiting in the Domain

J., de Vries, W. B., M¨uller, M., Wei, L., and
Hesselman, C. Anycast vs. ddos: Evaluating the
november 2015 root dns event (extended). Tech. Rep.
ISI-TR-2016-709b, USC/Information Sciences
Institute, May 2016.

[41] Moura, G. C. M., de O. Schmidt, R., Heidemann,

J., de Vries, W. B., M¨uller, M., Wei, L., and
Hesselman, C. Nov. 30 datasets.
http://traces.simpleweb.org/ and
https://ant.isi.edu/datasets/anycast/, 2016.

[42] Perlroth, N. Tally of cyber extortion attacks on
tech companies grows. New York Times Bits Blog,
http://bits.blogs.nytimes.com/2014/06/19/tally-of-c
yber-extortion-attacks-on-tech-companies-grows/,
June 2016.

[43] ProtonMail. Guide to DDoS protection.

https://protonmail.com/blog/ddos-protection-guide/,
Dec. 2015.

[44] Ripe Atlas. Graphs: Probe ﬁrmware versions

https://atlas.ripe.net/results/graphs/, Sept. 2016.

[45] RIPE NCC. DNSMON.

https://atlas.ripe.net/dnsmon/, 2015.

[46] RIPE NCC. RIPE Atlas root server data.

https://atlas.ripe.net/measurements/ID, 2015. ID is
the per-root-letter experiment ID: A: 10309, B: 10310,
C: 10311, D: 10312, E: 10313, F:10304, G: 10314, H:
10315, I: 10305, J: 10316, K: 10301, L: 10308, M:
10306.

[47] RIPE NCC Staff. RIPE Atlas: A global Internet

measurement network. The Internet Protocol Journal
18, 3 (Sept. 2015), pp. 2–26.

[48] Root Operators. http:// ww w.root-ser vers.org,

Apr. 2016.

[49] Root Server Operators. Events of 2015-11-30,

Nov. 2015.
http://root-servers.org/news/events-of-20151130.txt.

[50] Root Server Operators. Events of 2016-06-25,

June 2016.
http://root-servers.org/news/events-of-20160625.txt.
[51] Rossow, C. Ampliﬁcation Hell: Revisiting Network

Protocols for DDoS Abuse. In Network and Distributed
System Security (NDSS) Symposium (Feb. 2014).

Name System (DNS RRL). blog post
http://www.redbarn.org/dns/ratelimits, June 2012.

[58] Weinberg, M., and Wessels, D. Review and

analysis of anonmalous traﬃc to A-Root and J-Root
(Nov/Dec 2015). In 24th DNS-OARC Workshop (Apr.
2016). (presentation).

[59] Welzel, A., Rossow, C., and Bos, H. On

Measuring the Impact of DDoS Botnets. In 7th
European Workshop on System Security (Apr. 2014).

[60] Wessels, D. Verisign’s perspective on recent root

server attacks. CircleID
http://www.circleid.com/posts/20151215 verisign per
spective on recent root server attacks/, Dec. 15 2015.

[61] Woolf, S., and Conrad, D. Requirements for a

mechanism identifying a name server instance. RFC
4892, Internet Request For Comments, June 2007.

[62] Yan, H., Oliveira, R., Burnett, K., Matthews,

D., Zhang, L., and Massey, D. BGPmon: A
real-time, scalable, extensible monitoring system. In
Proceedings of the IEEE Cybersecurity Applications
and Technologies Conference for Homeland Security
(CATCH) (Mar. 2009), IEEE, pp. 212–223.

[63] Yu, Y., Wessels, D., Larson, M., and Zhang, L.
Authority Server Selection in DNS Caching Resolvers.
SIGCOMM Computer Communication Review 42, 2
(Mar. 2012), pp. 80–86.

[64] Zhu, L., Hu, Z., Heidemann, J., Wessels, D.,

Mankin, A., and Somaiya, N. Connection-oriented
DNS to improve privacy and security. In Proceedings
of the 36th IEEE Symposium on Security and Privacy
(May 2015), IEEE, pp. 171–186.

[65] Zwart, R., and Buddhdev, A. Report: K-root on
30 November and 1 December 2015. RIPE Labs blog
https://labs.ripe.net/Members/romeo zwart/report-o
n-the-traﬃc-load-event-at-k-root-on-2015-11-30, Feb.
2015.

270