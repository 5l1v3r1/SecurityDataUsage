Nonparametric Semi-Supervised Learning for Network

Intrusion Detection: Combining Performance
Improvements with Realistic In-Situ Training

Christopher T. Symons

Computational Sciences and Engineering

Oak Ridge National Laboratory

Oak Ridge, TN 37831
symonsct@ornl.gov

Justin M. Beaver

Computational Sciences and Engineering

Oak Ridge National Laboratory

Oak Ridge, TN 37831
beaverjm@ornl.gov

ABSTRACT
A barrier to the widespread adoption of learning-based net-
work intrusion detection tools is the in-situ training require-
ments for eﬀective discrimination of malicious traﬃc. Su-
pervised learning techniques necessitate a quantity of la-
beled examples that is often intractable, and at best cost-
prohibitive. Recent advances in semi-supervised techniques
have demonstrated the ability to generalize well based on a
signiﬁcantly smaller set of labeled samples. In network intru-
sion detection, placing reasonable requirements on the num-
ber of training examples provides realistic expectations that
a learning-based system can be trained in the environment
where it will be deployed. This in-situ training is necessary
to ensure that the assumptions associated with the learning
process hold, and thereby support a reasonable belief in the
generalization ability of the resulting model. In this paper,
we describe the application of a carefully selected nonpara-
metric, semi-supervised learning algorithm to the network
intrusion problem, and compare the performance to other
model types using feature-based data derived from an oper-
ational network. We demonstrate dramatic performance im-
provements over supervised learning and anomaly detection
in discriminating real, previously unseen, malicious network
traﬃc while generating an order of magnitude fewer false
alerts than any alternative, including a signature IDS tool
deployed on the same network.

Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: General—
security and protection; I.2 [Artiﬁcial Intelligence]: Ap-
plications and Expert Systems

Keywords
machine learning, network intrusion detection, nonparamet-
ric, semi-supervised

Copyright 2012 Association for Computing Machinery. ACM acknowl-
edges that this contribution was authored or co-authored by an employee,
contractor or afﬁliate of the U.S. Government. As such, the Government re-
tains a nonexclusive, royalty-free right to publish or reproduce this article,
or to allow others to do so, for Government purposes only.
AISec’12, October 19, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1664-4/12/10 ...$10.00.

1.

INTRODUCTION

Learning systems are becoming increasingly pervasive in
network intrusion detection research and practice. These
systems strive to generalize knowledge about network trans-
actions the way a human would and apply it to previously
unseen network transactions in order to identify malicious
traﬃc. Although there are many examples of the poten-
tial promise of these methods, a variety of factors combine
to make both research advances and practical deployment
of machine learning systems diﬃcult in the intrusion detec-
tion domain. Many of these factors are related to the lack
of available labeled data on operational networks. Attacks
captured in the wild are rarely made available, and even
then, they cannot be directly leveraged to learn models that
will be applied to diﬀerent networks. The critical nature of
the i.i.d. assumption (that all points used in training and
to which the model will be applied are pulled independently
and identically from the same distribution) underlying al-
most all machine learning methods, is often neglected due
to the belief that training a model in situ (in this context we
mean where it will be deployed) is too costly or impractical.
Prior results on synthetic datasets have utilized hundreds of
thousands or millions of training examples. Even previous
semi-supervised learning experiments in this domain have
utilized thousands of labeled examples. Obtaining exam-
ples in a new network is typically considered too costly to
support models that require such large numbers of labeled
events, particularly when they aren’t guaranteed to dramat-
ically outperform alternate methods.

Guarantees on the generalization performance of machine
learning approaches are based on theoretical error bounds
that do not apply if the assumptions of the method do not
match the reality of its utilization.
In other words, when
deploying a learning system in an environment with its own
idiosyncrasies, and keying oﬀ of network statistics and other
variables that are intertwined with the noise peculiar to the
network, the standard assumptions upon which the learning
algorithm depends need to be recognized. In nearly all cases,
the theoretical performance guarantees depend on the i.i.d.
assumption. In practice, this means that eﬀective learning
systems would ideally always be trained in situ, or using ex-
amples from the network environment in which they are de-
ployed. Therefore, discriminative learning based on known
attack data is potentially limited by the cost inherent in
identifying and/or generating real attacks in a new environ-
ment. In addition, there is some cost involved in ensuring

49that normal traﬃc in an existing network is indeed innocu-
ous. There are often model-speciﬁc assumptions that must
be recognized as well, and many model types are not well
suited to the intrusion detection domain.

Semi-supervised learning methods [10], which use unla-
beled data to augment the learning process, have been shown
in many domains to achieve better generalization perfor-
mance with far fewer labeled examples than would otherwise
be required.A practical question that arises is the following:
Can we learn an eﬀective intrusion-detection model using a
small number of labeled examples? If so, the costs associ-
ated with training in situ become less prohibitive. Having a
penetration testing team perform attacks on a network for a
few hours as opposed to weeks or months, or manually ver-
ifying dozens of network transactions as opposed to tens of
thousands, becomes a much more manageable requirement
to place on an organization for proper instrumentation of a
system.

In this work, we carefully select a nonparametric approach
to semi-supervised machine learning that has several im-
plicit assumptions that match the data generation process
in the network intrusion detection domain. We justify the
use of this model both theoretically and experimentally and
use this algorithm to provide strong evidence on data de-
rived from large-scale operational network data [29] that
we can indeed build very eﬀective models using a small
number of labeled examples. In addition to comparing our
model to multiple supervised and semi-supervised models,
we compare our results to published results using sophis-
ticated anomaly detection methods and to the output of
a signature-based intrusion detection system (IDS) applied
to the same data. An ability to generalize very eﬀectively
based on few observations is conﬁrmed, demonstrating clear
potential to augment current IDS tools with very simple fea-
tures and very realistic training requirements in a way that
can potentially provide strong alerting coverage against un-
known attacks with trivial false positive rates.

The experimental analysis uses data from Kyoto Univer-
sity that was recently made available to the public (see [29]).
While this is a carefully curated, valuable new resource,
it has limitations, and therefore, we only claim to provide
strong evidence that real tools with these characteristics are
currently viable. Our experiments are carefully designed to
demonstrate true generalization performance on unknown
attacks using minimal training sets, and the results on the
operational data show that we can catch nearly all previ-
ously unseen attacks with a false positive rate that is an
order of magnitude lower than any of the alternatives (in-
cluding the signature IDS, which cannot identify previously
unseen attacks).

2. BACKGROUND

In this section, we cover some background on the use of
machine learning for intrusion detection, and we describe
semi-supervised learning for the uninitiated.
2.1 Machine Learning in Intrusion Detection
Most operational network intrusion detections systems rely
on very speciﬁc rules, or signatures, to identify potentially
malicious traﬃc. Human experts generate the signatures af-
ter they have extensively analyzed an attack and determined
the attack’s indicative bit patterns and conditions. While
signatures are eﬀective at identifying a speciﬁc instance of

an attack, developing them is a time-consuming and man-
ually intensive process, during which the network remains
vulnerable. Furthermore, simple variants of the attack on
which the signature is based will often not trigger the sig-
nature pattern. As the frequency and diversity of attack
attempts rise, organizations are ﬁnding it increasingly diﬃ-
cult to keep pace in developing the raw number of signatures
required. A diﬀerent process for attack analysis is necessary
if computer network defense systems are to remain eﬀective.
The intrusion detection research community has responded
to the problem of signature development latency by explor-
ing machine-learning methods capable of learning the dis-
criminating characteristics of malicious traﬃc from exem-
plar network transaction data. The collective works cover a
broad range of techniques and are applied in various archi-
tectures in order to propose an optimum approach to net-
work traﬃc classiﬁcation. See [14, 30] for reviews of the ﬁeld.
Despite this signiﬁcant body of work, machine-learning ap-
proaches, and in particular supervised learning systems, are
sporadically deployed compared with the less sophisticated
signature-based approaches. We attribute this phenomenon
to both a low conﬁdence in the reported performance of
machine-learning-based intrusion detectors, and a poor un-
derstanding of how to operationally ﬁeld them.

2.1.1 Contrast with Anomaly Detection
Outside of the machine learning community, the phrase
anomaly detection [9] is often used interchangeably with and
regularly confused with machine learning. For people who
understand all of the subﬁelds that lie within these domains,
this is not an issue, but a large portion of the community
involved in cyber security is unaware of diﬀerences. One
early contributing factor to the intermixing of these terms
is due to a tendency to classify unsupervised learning, or
clustering, as a form of machine learning. Another factor
is the use of machine learning techniques to solve anomaly
detection problems, e.g. where clusters are taken as ground
truth for learning classiﬁcation models. However, we sub-
mit that there is a fundamental distinction between the two
areas of study based on theoretical foundations of machine
learning that have generalization performance as a critical
concept. Thus, while anomaly detection can be any mecha-
nism that looks for unusual patterns, machine learning looks
to generalize an expert-deﬁned distinction. Therefore, on a
fundamental level, it is perfectly natural in machine learning
to build a model purposely designed to distinguish between
malicious and benign network traﬃc and have optimal per-
formance on previously unseen events. On the other hand,
such a problem deﬁnition has very little connection with a
general deﬁnition of anomaly detection, since attacks aren’t
necessarily anomalous and normal behaviors often are.

In light of the above, it is important to point out that this
paper is written in a general context that diﬀerentiates learn-
ing from anomaly detection, in particular, based on the use
of classiﬁcation labels and the emphasis on generalization
performance in machine learning. Thus, real ground-truth
labels are a necessary component to the model building pro-
cess that we hope to address. The downside to using labels
is the cost of obtaining them, while the upside is the ability
to steer a model in a desired direction. In addition, when we
talk about using unlabeled data to augment label informa-
tion via semi-supervised learning, we do so based on a notion
of compatibility (see section 3.4) that uses concepts like reg-

50ularization to achieve better generalization performance on
a classiﬁcation task deﬁned by the labeled data.

In [28], the authors provide a good summary of the chal-
lenges inherent in the application of machine learning to
intrusion detection, but the problem being addressed is still
deﬁned to be ”outlier detection.” In other words, one of the
points being argued is that since the problem being solved is
anomaly detection, machine learning techniques, which op-
erate well on notions of similarity, are challenged. Our view
is that normal traﬃc can often be completely diﬀerent from
anything previously seen on the network. We also contend
that previously unseen attacks may not necessarily appear
to be anomalous in the originally deﬁned feature space, yet
have distinguishing characteristics such that they are more
similar to known attacks than normal traﬃc.
If these as-
sumptions are reasonably accurate, then outlier detection
is not the problem we want to solve. Instead, we operate
on the assumption that an expert-derived feature space can
capture information that allows previously unseen attacks,
whether anomalous or not, to be identiﬁed as sharing cer-
tain distinguishing characteristics with known attacks. The
generalization performance we observe when detecting pre-
viously unknown attacks on operational data (see section 4)
oﬀers strong evidence that new attacks do indeed resemble
known attacks in ways that allow them to be distinguished
from normal traﬃc, even on data where anomaly-detection
and signature-based systems struggle to reliably discrimi-
nate.

The problem becomes one of ﬁnding the right view through
which the desired distinction can be seen. Therefore, our ap-
proach is to solve a classiﬁcation problem where experts have
provided a small number of ground truth labels on the tar-
get network. Our goal is to show the power of using a model
whose assumptions very closely match the data generation
process in this domain (see section 3.2). We use the avail-
ability of the labeled operational data in the Kyoto2006+
dataset [29] to help demonstrate that the label requirements
for a machine learner can be made small enough, using cur-
rent methods, to realistically deploy eﬀective learning-based
intrusion detectors.

2.1.2 Data Limitations
The lack of conﬁdence that exists in academic evalua-
tions of machine-learning network intrusion detectors can
be traced to a shortage of publicly available data. Organi-
zations typically keep their network intrusion data hidden to
prevent publicizing any vulnerability. As a result, the major-
ity of academic studies present results that explore a singular
approach tailored to a speciﬁc environment, and are diﬃ-
cult to verify or validate more generally. A signiﬁcant gap
in the literature that applies machine-learning techniques to
the network intrusion detection problem is the absence of a
relevant network intrusion data set that can be used as a
basis for comparison. While the 1999 KDD cup ”classiﬁca-
tion task” data [21] provided an initial surge of interest in
machine-learning-based intrusion detection, the background
traﬃc was simulated and the data no longer accurately rep-
resents modern network traﬃc. The lack of other relevant,
public labeled data sets has severely limited the exploration
of machine learning methods in network intrusion detection.
The release of the Kyoto2006+ dataset [29], which captures
metric sets associated with real operational network ﬂows,
is therefore a very promising step toward more accessible re-

search in this area. We summarize some of the most relevant
characteristics of this dataset in section 4.
2.2 Semi-supervised learning

Semi-supervised learning [10] is generally deﬁned as any
learning method that uses both labeled and unlabeled data
during the model discovery process. Methods for incorpo-
rating the unlabeled information can vary. Some methods
include the use of data-dependent priors and low-density
separation, but the most common approach is graph-based
[10, 16]. Graph-based methods are typically designed based
on the assumption that the data naturally occur on an un-
derlying manifold, such that the true degrees of freedom
of the problem can be discovered using unlabeled data. In
essence, the intent is to ﬁnd structure in the ambient space
that can be exploited to constrain the search for a good
model.

A central construct in many methods is the graph Lapla-
cian [12]. A graph is constructed to represent a manifold (or
densely populated region of interest in the ambient space),
and the graph Laplacian facilitates the discovery of a low-
dimensional space that is smooth with respect to this graph.
In semi-supervised learning the goal is to augment learning
through the use of unlabeled samples, so the unlabeled data
is often used to ﬁnd a low-dimensional space on which learn-
ing via the labels can be more eﬀective.

There are many other approaches to semi-supervised learn-
ing. One that is particularly noteworthy for its ability to
use the labels in a robust manner is the predictive structure
framework of [2]. Their approach is based on multi-task
learning and attempts to ﬁnd structure that overlaps many
prediction problems that are formulated in such a way that
they always have labels. One drawback to the approach is
that the framework, as described, requires a diﬀerent kind
of domain expertise in terms of the construction of the for-
mulated tasks.
2.3 Semi-Supervised Intrusion Detection

Semi-supervised learning has begun to be explored in in-
trusion detection.
In [11], the authors explore the use of
transductive spectral methods and Gaussian random ﬁelds
on the 1999 KDD Cup dataset [21]. The transductive ap-
proach achieves the best results in that study, but unfor-
tunately the use of transductive methods is not practical
in real-time systems. Although it is probably safe to as-
sume that an out-of-sample extension would not suﬀer a
major drop in performance, the results only support incre-
mental improvement over supervised methods, and the au-
thors lament that they do not reach a level of performance
that would be valuable in practice. For example, although
the test setup is diﬀerent, the anomaly detection techniques
used in [1] appear to achieve signiﬁcantly better results on
the same dataset.

In [18], semi-supervised concepts are explored, but in a
involving partially observable
very untraditional manner,
markov decision processes (POMDPs), an area of reinforce-
ment learning that suﬀers from scalability issues [27]. The
method is intended to be a proposed framework in which to
place intrusion detection. As such, it has merit, but it does
not make signiﬁcant strides toward practical usage. Mao et
al.
[22] take an interesting approach based on multi-view,
semi-supervised learning and active learning, which requires
an interactive process with the user, and apply it to the

51KDD Cup data. They show improvement over their base-
line, which is single view learning without active learning,
but the amount of labeled data used is still very high and
the number of false positive alerts remains in an unusable
range.

3. LEARNING METHODS

Parametric methods in machine learning constrain mod-
els to a certain functional form deﬁned by the parameter
space. In real-world problems, it is often the case that an
appropriate functional form is not known. Nonparametric
techniques, on the other hand, oﬀer much more ﬂexibility to
represent complicated models. While nonparametric meth-
ods almost always include some parameters for the sake of
tractability, the core philosophy underlying these methods
is that the complexity of the model is allowed to increase
with the size of the data [23]. Because the use of some pa-
rameters is often inevitable, it is common to refer to some
of these methods as semi-parametric. The semi-supervised
methods that we apply in this paper are non-parametric,
which allows greater model ﬂexibility while trying to avoid
imposing unrealistic constraints on the form of the model.

For experimental comparison with supervised methods,
we also use a linear Support Vector Machine (SVM) and
a maximum entropy classiﬁer from the Minorthird machine
learning software library [13].

3.1 Graph-based classiﬁer

In this section we focus on the use of Laplacian Eigen-
maps for semi-supervised learning [6, 7]. Our main focus
will be on the Laplacian Regularized Least Squares algo-
rithm in section 3.2 We use the Laplacian Eigenmaps as an
alternate nonparametric semi-supervised learner in our ex-
periments.
In particular, comparison with this method is
useful because both semi-supervised methods use the graph
Laplacian, which is described below.

In our use of the Laplacian Eigenmap approach, we ﬁrst
construct a nearest neighbor graph using the six nearest
neighbors based on cosine similarity. Unlike some nonlin-
ear dimensionality reduction methods, the use of Laplacian
Eigenmaps does not automatically suggest the size of the
new space. Therefore, we retain a basis size that is twenty
percent of the number of labeled data points used to reﬂect
the suggestions from [7]. Note that in addition to the non-
parametric nature of the nearest neighbor graph, the number
of dimensions is a reﬂection of the labeled data size. Thus,
in this case, complexity of the model can grow with the size
of the unlabeled data through the graph Laplacian, and it
can also grow with the size of the labeled data.

The normalized graph Laplacian is a matrix deﬁned as

follows:

L(u, v) =

 1,

−1√
dudv
0,

,

if u = v and dv (cid:54)= 0
if u and v are adjacent
otherwise

(1)

where u and v are vertices in the graph, d is the degree
(number of incident edges) of a vertex, and adjacency refers
to a neighboring connection in the graph. We use the un-
normalized form given below:

 dv,

−1,
0,

L(u, v) =

if u = v
if u and v are adjacent
otherwise

(2)

Using the eigenvalues and associated eigenvectors of these
positive, semi-deﬁnite, symmetric matrices provides a method
for discovering dimensions that are smooth with respect to
the graph that deﬁnes it. If the graph varies smoothly with
respect to the target problem (i.e. examples from diﬀerent
classes or clusters are rarely linked, similar examples from
the perspective of the target problem are linked, etc.), then
it can be used to represent a manifold. The Laplacian of
the graph can then be used to ﬁnd a space that roughly rep-
resents that manifold. The eigenvector associated with the
smallest non-zero eigenvalue is smoothest with respect to the
graph, such that points connected in the graph will be close
together in the dimension deﬁned by said eigenvector. This
smoothness with respect to eigenvector-deﬁned dimensions
decreases as you progress to the larger eigenvalues.

Another useful property of the eigensystem is the fact that
the number of zero-value eigenvalues is equal to the num-
ber of connected components in the graph. In addition, an
eigenvector will not involve more than one component of
the graph. Thus, after counting the number of connected
components, which is an O(n) operation, you need to retain
at least that many dimensions in a new space in order to
distinguish between all points after they are mapped.

For one of the semi-supervised test models, we rely upon
dimensionality reduction as described above. Once the di-
mensionality reduction is achieved, an initial transductive
model is constructed. First, we construct a simple classiﬁer
in the new space in the same manner as the approach in [7],
in which the coeﬃcients for the new dimensions are set by
minimizing the sum of squared error on the labeled data. In
other words, the weights of our new dimensions are given by
the vector a in the following:

a = (EET )

−1Ec

(3)

where c is a vector representing the class labels, λk, vk are
the k-th eigenvalue and eigenvector, respectively, the entries
of E are λkvi,k, i is the index of the labeled point in the ma-
trix, and k is the index in the new low-dimensional space;
i.e. the k-th eigenvalue and eigenvector provide the mapping
into the new space for labeled point i. The number of con-
nected components in the graph is determined in order to
eliminate the zero-valued eigenvalues, and then the mapping
starts with the next eigenfunction.

Since the Laplacian Eigenmap approach is inherently trans-
ductive, it only creates a mapping for an unlabeled example
if it was part of the set used for graph construction. This
means that applying a method transductively would involve
solving the eigenvalue problem all over again for any new
point or set of points, which would be impractical for most
purposes, and in particular, for intrusion detection in real
time. For a nonparametric out-of-sample extension that al-
lows eﬃcient application to new points, we utilize the Nys-
trom Formula as described in [24]. The method has been
shown (and for the most part veriﬁed via our own exper-
iments) to provide inductive classiﬁcation results with no
signiﬁcant diﬀerence in accuracy from the transductive ap-
plication.
It simply uses the Laplacian matrix as a data-
dependent Kernel function KD in the following formula in

52this means that F can be set to correspond to the marginal
distribution of the data, X.

Given a ﬁxed sample from an uncertain distribution F
in the Dirichlet process (DP) model, the posterior is the
following Dirichlet process [19]:

F|Xn ∼ DP (α + n, Fn), Fn = (αF0 +

δxi )/(α + n) (6)

In [19], we see that if we want to predict the value of a
new point, x, based on our sample from F , i.e. based on
our labeled and unlabeled training data, then we want the
following:

(cid:90)

E[f|Xn] = an

k(x, u)w(u)dF0(u)+

−1(1 − an)

n

w(xi)k(x, xi)

(7)

n(cid:88)

i=1

n(cid:88)

i=1

where an = α/(α + n).
Then, assuming an uninformative prior, they take the

limit α → 0 to get the following representer form:

ˆfn(x) =

wik(x, xi)

(8)

Thus, according to [19, 20, 25], when the uncertainty
about the probability distribution function for the data, X,
is expressed using a Dirichlet process prior, then the func-
tion f can be approximated by the following formula over
labeled and unlabeled examples.

n(cid:88)

ˆf (x) =

wn,iK(x, xi) +

wn+nm,n+iK(x, xm
i )

(9)

i=1

i=1

This results in the exact same functional form as that
derived in [8]. And in fact, the graph Laplacian over the
observed data can then approximate the Laplacian on the
manifold by solving the following.

(cid:34)

n(cid:88)

ˆf (x) = argminf∈HK
1
n

i=1

V (f (xi), yi) + γA(cid:107)f(cid:107)2

K +

(cid:35)

(10)

γI

(n + nm)2 f T Lf

1 ), ..., f (xm

where L is the Laplacian derived from the data and f =
{f (x1), ..., f (xn), f (xm
nm )}. γA and γI are parame-
ters that control the amount of regularization in the ambient
space and intrinsic space, respectively.
3.3 Laplacian RLS Model Implementation

So, once again, we can use a method of semi-supervised
learning using the graph Laplacian. In our experiments, we
use the unnormalized form of the graph Laplacian here as
well (Equation 2).

The output function that is learned is the following:

F is the unknown distribution of the kernel knots, u,
where a knot is a data point on the manifold. In essence

f (x) =

αiK(xi, x),

(11)

order to map a new point into each dimension k of the new
decision space:

fk(x) =

vikKD(x, xi)

(4)

n(cid:88)

i=1

√

n
λk

where n is the size of the original dataset, and λk, vk are the
k-th eigenvalue and eigenvector, respectively.
3.2 Laplacian RLS/Bayesian Kernel Model

The main semi-supervised model that we focus on in this
paper is interesting from multiple perspectives. In fact, it
is possible to arrive at the same functional form for this
model based on two completely diﬀerent derivations.
In
other words, this model represents both the Laplacian Reg-
ularized Least Squares (Laplacian RLS) model in [8] and
the Bayesian Kernel Model in [19, 20, 25] with a Dirich-
let process prior. This is relevant to our discussion because
we hope to use models whose assumptions more realistically
match the realities of the data. And we can argue that both
of these derivations are in tune with how we hope to shape
(or avoid shaping) the model.

In the case of the Laplacian RLS [8], we are using un-
labeled data as a graph-based regularization term, which
essentially means that we can use as much unlabeled data
as we want to penalize models that would assign points
among the unlabeled data that are extremely close together
in our expert inspired feature space as belonging to diﬀerent
classes. This makes sense as long as the features are rela-
tively important to the problem domain. We believe this to
be true a priori due to the fact that they were derived by
experts.

In the case of the Bayesian Kernel Model [19, 20, 25],
a model is estimated by selecting from among functions in
the reproducing kernel Hilbert spaces (RKHS) induced by
the chosen kernel. We would like to assume that we have a
smooth function that we want to represent. In this case, we
can look at our kernel as data that falls on a smooth mani-
fold; i.e. that points in the original space actually vary along
a dense manifold that cuts through that space. However, we
also believe there are other unknown random processes gen-
erating the points that we see on the smooth manifold. Be-
cause each event may be generated based on its own random
process, we don’t want to restrict the form of each of these
processes. In addition, we don’t want to restrict the possible
number of processes that could be generating the events we
observe. This is a typical case in which a Dirichlet process
prior might be used. It also makes sense because while we
hope that the target function we are trying to learn lies in
a dense region that cuts through the original feature space,
it also allows us to represent each event as being generated
by its own random process.

We will see that these assumptions can also lead us to the
same functional form as the Laplacian RLS. The derivation
can be found in [19, 20, 25]. The relevant Bayesian kernel
derivation is based on integral operators. The form that is
used in [19] is the following:

(cid:90)

(cid:90)

f (x) =

k(x, u)dγ(u) =

k(x, u)w(u)dF (u)

(5)

n(cid:88)

i=1

nm(cid:88)

l+u(cid:88)

i=1

53where K is the (l + u) × (l + u) Gram matrix over la-
beled and unlabeled points, and α is the following learned
coeﬃcient vector:

α = (JK + γAlI +

γI l

(l + u)2 LK)

−1Y,

(12)

with L being the Laplacian matrix described above, I
being the (l + u) × (l + u) identity matrix, J being the
(l + u) × (l + u) diagonal matrix with the ﬁrst l diagonal
entries equal to 1 and the rest of the entries equal to 0, and
Y being the (l + u) label vector, Y = [y1, ..., yl, 0, ..., 0]. See
[8] for details.

This method does have two parameters that control the
amount of regularization. For all of our experiments, we use
the following parameters, as suggested for manifold regular-
ization in [8]: γAl = 0.005,
3.4 Additional Theoretical Context

(l+u)2 = 0.045.

γI l

Although a more thorough theoretical analysis is beyond
the scope of this paper, we point the interested reader to ex-
isting theoretical work related to error bounds on the gen-
eralization performance of these methods.
In [5, 4] semi-
supervised learning is analyzed as a notion of compatibility,
χ. The notion of compatibility is based on ﬁnding a model
that has a low unlabeled error rate. In the case of a graph
regularization approach like the Laplacian RLS model, this
can indicate that the function being learned agrees with the
graph and would seldom label two nodes sharing an edge
with diﬀerent class labels. Thus, the unlabeled data help
guide the model choice through the graph by penalizing
models that do not agree with it.

4. EXPERIMENTAL RESULTS

We use the Kyoto2006+ dataset [29] for all of the experi-
ments in this section. The dataset covers nearly three years
of network traﬃc through the end of 2008 over a collection
of both honeypots and regular servers that are operationally
deployed at Kyoto University. The data is provided in the
form of observations and statistical features that character-
ize terminated connections. We only use the ﬁrst 14 features
since any system would have access to the information re-
quired to construct these features, whereas the additional
features are unlikely to be available. The fact that the fea-
tures are pre-calculated allows for more accurate comparison
of diﬀerent model types, but it unfortunately restricts the
possible features to those provided.

Before using the data, we convert categorical features to
binary, and normalize all numeric data using a Softmax scal-
ing approach (with r = 1), which is purported to retain the
most information [26].

Unfortunately, the dataset does not provide information
on speciﬁc attack types. Therefore, we are unable to take
advantage of a cost-sensitive learning scheme, and we are
unable to determine how well we are doing with regard to
diﬀerentiating attack types or prioritizing alerts. Moreover,
there is a good deal of suspected labeling error. Even though
the number of errors is likely tiny compared to the size of
the dataset, this is an important point (see [29] for a detailed
description of the dataset).

The dataset essentially represents a two-class classiﬁca-
tion problem, where the classes represent malicious traﬃc
and non-malicious traﬃc in a network. There is a distinc-

tion made between known and unknown attack types, which
we leverage in some of our experiments to test the ability
to generalize knowledge to previously unseen attacks. Un-
known attacks are deﬁned as those that were not ﬂagged
by the signature IDS, but for which the Ashula tool de-
tected shellcodes. The only packet information available to
our models is the number of bytes sent by the source and
destination.
4.1 Comparative Analysis

All tests in this section are performed across the test data
used in [17], which comprises 12 days of traﬃc pulled from
the last six months of 2008. Table 1 shows the initial results
of training two supervised learners from the Minorthird li-
brary [13], a linear Support Vector Machine (SVM) and a
maximum entropy learner, using a full day’s labeled data
from January 1, 2008. For comparison, we also display the
alerting results from the intrusion detection system (IDS)
that are included in the dataset, and we list the results from
[17], which employed an anomaly detection approach using
multiple classiﬁers trained over 10 million training examples.
Next, we compare the semi-supervised learners to the su-
pervised learners using very small labeled datasets. The
semi-supervised learners are the Laplacian Eigenmap (LEM)
and the Laplacian Regularized Least Squares (RLS) algo-
rithms described above. The results are shown in Table 2.
Subsets of 100 labeled examples and approximately 3000
unlabeled examples from Jan. 1, 2008 are used for train-
ing, and testing is performed across the same test data as
above. There were 111,589 examples (terminated connec-
tions) on January 1, 2008. The classiﬁcation results are av-
eraged over 10 random selection of the labeled data. We ﬁrst
randomly select 100 examples as our labeled training set and
retain the rest as unlabeled examples for use by the semi-
supervised learners. However, we also remove redundancy
through an approximate similarity measure by hashing the
examples based on label value, binary feature values, and
10% ranges of the normalized numeric feature values. This
leaves an average of 56.6 labeled examples per experiment,
with a high of 69 and a low of 19.
It also preserves ap-
proximately 3000 unlabeled examples per experiment. We
report the average recall, false positive rate, and area under
the ROC curve, which is a plot of the tradeoﬀ between false
positive rate and recall as the decision threshold of the bi-
nary classiﬁer is varied (i.e. the AUC score, see [15] for an
interesting discussion of this measure). Keep in mind that
we purposely restricted the number of labeled examples to
an extreme in order to demonstrate the viability of training
such models in their deployment environments.
4.2 Training on Known to Catch Unknown

Of particular interest is the ability to catch previously un-
observed and unknown attacks after training on a small or
reasonable number of known attack types. Because the Ky-
oto2006+ dataset [29] diﬀerentiates between known and un-
known attacks, we can test this ability directly. In Table 3,
we examine the ability of the Laplacian RLS learner to catch
unknown attacks after being trained on normal traﬃc and
known attacks only. The setup is the same as before using
data from Jan. 1, 2008, such that the results are averaged
over 10 random selections of the labeled data. Each set has
100 labeled data points total to begin with, thus after elimi-
nating redundancy, we observe a combined total of under 70

54Table 1: Reported IDS results, multi-classiﬁer anomaly detection results, and results of using all (111,589) labeled examples
from Jan. 1, 2008 for supervised learning. Testing is performed across the same test data as in [17], which comprises 12 days
of traﬃc pulled from the last six months of 2008. *The signature IDS alerts are recorded in the dataset.

Classiﬁer
Signature IDS*
Anomaly Detection [17]
Maximum Entropy
Linear SVM

Recall
0.09004
0.8093
0.77292
0.98952

| False Positive Rate | AUC Score
N/A
N/A

0.01619
0.0590
0.02059
0.03528

0.72044
0.96295

Table 2: Classiﬁer comparison using small training sets of fewer than 100 labeled examples and approx. 3000 unlabeled
examples from Jan. 1, 2008. Testing is performed across the same test data as in [29], which comprises 12 days of traﬃc
pulled from the last six months of 2008. Results are averaged over 10 random selections of labeled examples.

Classiﬁer
Maximum Entropy
Linear SVM
Laplacian Eigenmap
Laplacian RLS

Recall
0.77292
0.96354
0.64112
0.89144

| False Positive Rate | AUC Score
0.72044
0.94802
0.75926
0.98651

0.02059
0.03029
0.08715
0.02667

labeled examples (combined number of normal and known-
attack terminated connections) for each classiﬁer, with as
few as 19 labeled examples. Once again, there are approx-
imately 3000 unlabeled examples per experiment. We also
count how often the IDS results recorded in the Kyoto2006+
dataset alerted on the data with normal and unknown at-
tacks only. There are a total of 398 unknown attacks that
occur during the 12 days in the test set.

If we look more closely at the individual results, the real
promise of the Laplacian RLS, and potentially other semi-
supervised methods whose assumptions match the domain,
shines through. In Table 4 we provide the results of each of
the 10 runs in order to demonstrate how low the number of
false positives can be bounded. The ﬁrst run has the low-
est AUC score of 0.99968, but has the lowest false positive
rate of 0.00022 (out of 808,108 normal events).
It is also
the only classiﬁer to have a recall of less than 100%, but
it still catches 99.75% of the unknown attacks. The binary
Laplacian RLS model uses a threshold, so the AUC score
indicates how much tradeoﬀ needs to occur between preci-
sion and recall. Therefore, since the model that catches 397
unknown attacks, while missing only one, only has 178 false
positive alerts and yet has the lowest AUC score, all of the
other models should be tunable to allow them to miss a sin-
gle attack while keeping their false positive number at 178
or lower, as well, since they require less of a tradeoﬀ than
the ﬁrst model.

Given the AUC scores in Table 4, it makes sense to add
an automatic threshold selection routine to the training step
in order to obtain better performance. Table 5 and Table 6
show the results of the Laplacian RLS classiﬁers when the
thresholds are tweaked during training (on training data)
to eliminate false positives. In this case, we used a method
whereby we rank all labeled training data by the score as-
signed by the model, and then we attempt to ﬁnd a thresh-
old that will guarantee a maximum false positive rate of
0.00000001 on the training data with the hope that this will
transfer to the test data. We ﬁnd the distance between this
discovered threshold and the maximum score of 1, multiple it

by 0.75, and add it to the old threshold to obtain a new one.
Unfortunately, our choice of 0.75 is rather arbitrary, so de-
spite the fact that the threshold is set on the training data, it
is likely that such a method would need to be tweaked man-
ually in practice based on the number of false positives that
a user could tolerate. However, it is clear that these models
are very powerful methods of ﬁnding unknown attacks, and
it is equally clear that if the intention is to ﬁnd previously
unseen attacks, then these methods hold great promise for
the defense of large networks. As mentioned above, the op-
timal threshold for each of these learners should guarantee
fewer than 178 false positives for any of the the classiﬁers.
Thus, the improvements shown in Table 6 can be improved
upon as well. Therefore, future work will include better
methods of automatic threshold generation, which is a par-
ticular challenge when the size of the training data is limited
to realistic numbers as in this paper.

4.3 Additional Insight

In addition to the experimental results, it is interesting
to observe the eﬀect that the unlabeled data has on the di-
mensionality reduction used by the Laplacian Eigenmap ap-
proach. Therefore, as an example of a real nonlinear trans-
formation using Laplacian Eigenmaps, Figure 1 shows the
values of the ﬁrst two non-zero eigenvectors for the labeled
points during training of a Laplacian Eigenmap classiﬁer
on the smallest subset (19 labeled examples) of the Ky-
oto University data from January 1, 2008. The addition of
the unlabeled data clearly improves the separability between
classes along the ﬁrst two dimensions (which are always the
smoothest ones with respect to the graph). Since the method
preserves closeness in the original graph, the addition of the
unlabeled data can be seen as increasing the density so that
it is possible to build a graph that does indeed represent a
smooth manifold along which the attacks and normals vary.
In terms of the Laplacian RLS, this means that the graph
should have a small unlabeled error rate (see section 3.4),
which appears to be conﬁrmed by the experimental results.

55Table 3: Alerting on unknown attacks. The Laplacian RLS classiﬁers were trained on subsets of fewer than 70 labeled data
comprising only known attacks and known normals. *The signature IDS alerts are recorded in the dataset [29].

Classiﬁer
Signature IDS*
Laplacian RLS

Recall
0.00000
0.99975

| False Positive Rate| AUC Score
N/A

0.01619
0.02538

0.99987

Table 4: Performance of the individual classiﬁers (randomly selected training sets). All classiﬁers require less tradeoﬀ between
precision and recall than classiﬁer 1. Therefore, they can all conceivably be tuned to achieve the same results: 178 or fewer
false positives, while alerting on 397 out of 398 unknown attacks. *The signature IDS alerts are recorded in the dataset [29].

Classiﬁer
Signature IDS*
Laplacian RLS 1
Laplacian RLS 2
Laplacian RLS 3
Laplacian RLS 4
Laplacian RLS 5
Laplacian RLS 6
Laplacian RLS 7
Laplacian RLS 8
Laplacian RLS 9
Laplacian RLS 10

|# Training Data|

N/A
19
57
58
60
64
65
59
69
57
58

Recall
0.00000
0.99749

1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0
1.0

|# False Negatives|

398
1
0
0
0
0
0
0
0
0
0

|# False Positives| AUC Score
N/A

13,074

178

14,753
28,498
28,498
25,621
17,456
18,278
28,498
28,498
14,707

0.99968
0.99993
0.99992
0.99970
0.99993
0.99993
0.99986
0.99986
0.99995
0.99995

Table 5: Alerting on unknown attacks. The Laplacian RLS classiﬁers were trained on subsets of fewer than 70 labeled data
comprising only known attacks and known normals, and they were built using automatic threshold-ﬁnding functions intended
to reduce false positive alerts.. *The signature IDS alerts are recorded in the dataset [29].

Classiﬁer
Signature IDS*
Laplacian RLS

Recall
0.00000
0.99749

| False Positive Rate| AUC Score
N/A

0.01619
0.00166

0.99987

Table 6: Performance of the individual classiﬁers (randomly selected training sets) when using an automatic threshold-
ﬁnding function during training. This function is intended to raise the threshold to avoid false positive alerts, but it only uses
training-data to ﬁnd the threshold. *The signature IDS alerts are recorded in the dataset [29].

Classiﬁer
Signature IDS*
Laplacian RLS 1
Laplacian RLS 2
Laplacian RLS 3
Laplacian RLS 4
Laplacian RLS 5
Laplacian RLS 6
Laplacian RLS 7
Laplacian RLS 8
Laplacian RLS 9
Laplacian RLS 10

|# Training Data|

N/A

19
57
58
60
64
65
59
69
57
58

Recall
0.00000
0.99749
0.99749
0.99749
0.99749
0.99749
0.99749
0.99749
0.99749
0.99749
0.99749

|# False Negatives|

398
1
1
1
1
1
1
1
1
1
1

|# False Positives| AUC Score
N/A

13,074

164
173
676
9807
166
167
1779
166
203
151

0.99968
0.99993
0.99992
0.99970
0.99993
0.99993
0.99986
0.99986
0.99995
0.99995

56(a)

(c)

(b)

(d)

Figure 1: Eigenvector values of the labeled data from the graph Laplacian based on nearest neighbors.
1a and 1b are
the values in the ﬁrst and second dimensions, respectively, using a graph based on labeled points only. 1c and 1d are the
values in the ﬁrst and second dimensions, respectively, when using a graph of the labeled and unlabeled data. In this case,
the addition of the unlabeled data provides a transformation that allows linear separation of the two classes along the ﬁrst
two dimensions.

5. DISCUSSION

This paper provides a demonstration of true generaliza-
tion performance on real operational network data using a
small, randomly selected, set of known attacks and normal
connections for training. The resulting classiﬁers trained on
data from Jan 1, 2008 were tested on twelve days of data
from the same environment from the latter half of 2008 (the
same test set used in [17]). Each of the models alerted on
397 out of 398 unknown attacks during the 12-day window,
while generating false positive alerts at a rate an order of
magnitude lower than that of a signature tool incapable of
catching previously unseen attacks. In fact, an alert from a
typical model would have a greater than two-thirds chance
of being an actual previously unseen attack as opposed to
a false alert. Given the diﬃculties that machine-learning-
based intrusion detection has faced in reaching operational
status, it is important to continue to improve the perfor-
mance of such classiﬁers, but it is equally important to do
so on recent operational data. Our hope is that dramatic
results such as those presented here on non-synthetic data
pulled from a real operational network can help this ﬁeld
move forward more rapidly into real systems. We are ex-
tremely grateful to the curators of the Kyoto University data
[29] for the release of their data and hope that others will
follow suit.

It should not be surprising that performance on unknown
attacks has the potential to be very high. While some known
attacks, such as probes and denial of service attacks, may
be nearly indistinguishable from normal traﬃc due to their
nature, more insidious attacks are harder to blend into the
environment while still accomplishing their goals. Although
this paper hopes to emphasize the critical nature of in-situ
training, we also demonstrate that such training can be

made to be extremely cost eﬀective when using the latest
semi-supervised models, if the employed models have been
carefully selected such that their implicit assumptions match
the data domain.

The base-rate fallacy [3] is a real issue in intrusion detec-
tion, but demonstrations like the one in this paper of real
classiﬁers with tiny false positive numbers, such that they
are fewer than one-third of real attacks detected, go a long
way to eliminating the problem. In particular, this is sig-
niﬁcant when the training requirements for such models can
be limited to extremely reasonable numbers.
In fact, the
in-situ training requirements for the models in this paper
are so small that they are dwarfed by the costs of deal-
ing with missed attacks or investigating false positives from
other tools. Our hope is that if we can begin to gather strong
evidence of useable performance on real datasets, such as the
results in this paper, then the community can move more ag-
gressively to ubiquitously deploy tools that are trainable in
place and that generalize eﬀectively to catch previously un-
seen attack types. Improvements beyond these approaches
may be necessary in the long run to deal with strong ad-
versarial learning scenarios, but we believe it is currently
possible for large network systems to augment the use of
signature tools with more dynamic, intelligent systems with
little cost in terms of training or the investigation of false
alerts.

6. ACKNOWLEDGMENTS

Research sponsored by the Laboratory Directed Research
and Development Program of Oak Ridge National Labora-
tory, managed by UT-Battelle, LLC, for the U.S. Depart-
ment of Energy under contract no. DE-AC05-00OR22725.
This manuscript has been authored by UT-Battelle, LLC,

57under contract DE-AC05-00OR22725 with the U.S. Depart-
ment of Energy. The United States Government retains and
the publisher, by accepting the article for publication, ac-
knowledges that the United States Government retains a
non-exclusive, paid-up,
irrevocable, world-wide license to
publish or reproduce the published form of this manuscript,
or allow others to do so, for United States Government pur-
poses..
7. REFERENCES
[1] N. Abe, B. Zadrozny, and J. Langford. Outlier
detection by active learning. In ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining, 2006.

[2] R. K. Ando and T. Zhang. A framework for learning

predictive structures from multiple tasks and
unlabeled data. Journal of Machine Learning
Research, 6:1817–1853, 2005.

[3] S. Axelsson. The base-rate fallacy and the diﬃculty of

intrusion detection. ACM Trans. Inf. Syst. Secur.,
3(3):186–205, Aug. 2000.

[4] M.-F. Balcan. New Theoretical Frameworks for

Machine Learning. Phd thesis, 2008.

[5] M.-F. Balcan and A. Blum. An Augmented PAC
Model for Semi-Supervised Learning. MIT Press,
Cambridge, MA, 2006.

[6] M. Belkin and P. Niyogi. Laplacian eigenmaps for
dimensionality reduction and data representation.
Neural Computation, 15(6):1373–1396, 2003.

[7] M. Belkin and P. Niyogi. Semi-supervised learning on
riemannian manifolds. Machine Learning, 56:209–239,
2004.

[8] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold
regularization: A geometric framework for learning
from labeled and unlabeled examples. Journal of
Machine Learning Research, 7:2399–2434, 2006.

[17] K. Kishimoto, H. Yamaki, and H. Takakura.

Improving performance of anomaly-based ids by
combining multiple classiﬁers. In Applications and the
Internet (SAINT), 2011 IEEE/IPSJ 11th
International Symposium on, pages 366–371, 2011.

[18] T. Lane. A Decision-Theoretic, Semi-Supervised Model

for Intrusion Detection. Springer London, 2006.

[19] F. Liang, K. Mao, M. Liao, S. Mukherjee, and

M. West. Nonparametric bayesian kernel models.
Technical report, Duke University, 2007.

[20] F. Liang, S. Mukherjee, and M. West. The use of
unlabeled data in predictive modeling. Statistical
Science, 22(2):189–205, 2007.

[21] R. P. Lippmann, D. J. Fried, I. Graf, J. W. Haines,

K. R. Kendall, D. McClung, D. Weber, S. E. Webster,
D. Wyschogrod, R. K. Cunningham, and M. A.
Zissman. Evaluating intrusion detection systems: The
1998 darpa oﬀ-line intrusion detection evaluation. In
the 2000 DARPA Information Survivability
Conference and Exposition, 2000.

[22] C.-H. Mao, H.-M. Lee, D. Parikh, T. Chen, and S.-Y.

Huang. Semi-supervised co-training and active
learning based approach for multi-view intrusion
detection. In Proceedings of the 2009 ACM symposium
on Applied Computing, SAC ’09, pages 2042–2048,
New York, NY, USA, 2009. ACM.

[23] P. Orbanz and Y. W. Teh. Bayesian Nonparametric

Models. Springer, 2010.

[24] M. Ouimet and Y. Bengio. Greedy spectral

embedding. In the 10th Workshop on Artiﬁcial
Intelligence and Statistics (AISTATS), 2005.

[25] N. S. Pillai, Q. Wu, F. Liang, S. Mukherjee, and R. L.

Wolpert. Characterizing the function space for
bayesian kernel models. Journal of Machine Learning
Research, 8:1769–1797, 2007.

[26] D. Pyle. Data Preparation for Data Mining, Volume 1.

[9] V. Chandola, A. Banerjee, and V. Kumar. Anomaly

Morgan Kaufmann, 1999.

detection - a survey. ACM Computing Surveys, 41(3),
2009.

[10] O. Chapelle, B. Scholkopf, and A. Zien.

Semi-Supervised Learning. MIT Press, Cambridge,
MA, 2006.

[11] C. Chen, Y. Gong, and Y. Tian. Semi-supervised

learning methods for network intrusion detection. In
Systems, Man and Cybernetics, 2008. SMC 2008.
IEEE International Conference on, pages 2603 –2608,
2008.

[12] F. R. K. Chung. Spectral Graph Theory. American

Mathematical Society, Providence, RI, 1997.

[13] W. W. Cohen. Minorthird: Methods for identifying

names and ontological relations in text using
heuristics for inducing regularities from data, 2004.

[14] S. Dua and X. Du. Data Mining and Machine
Learning in Cyber Security. CRC Press, 2011.
[15] P. Flach, J. Hernandez-Orallo, and C. Ferri. A
coherent interpretation of auc as a measure of
aggregated classiﬁcation performance. In the 28th
International Conference on Machine Learning, 2011.

[16] R. Johnson and T. Zhang. Graph-based

semi-supervised learning and spectral kernel design.
IEEE Transactions on Information Theory,
54(1):275–288, 2008.

[27] N. Roy, G. Gordon, and S. Thrun. Finding

approximate pomdp solutions through belief
compression. Journal of Artiﬁcial Intelligence
Research, 23:1–40, 2005.

[28] R. Sommer and V. Paxson. Outside the closed world:

On using machine learning for network intrusion
detection. In Proceedings of IEEE Symposium on
Security and Privacy, 2010.

[29] J. Song, H. Takakura, Y. Okabe, M. Eto, D. Inoue,
and K. Nakao. Statistical analysis of honeypot data
and building of kyoto 2006+ dataset for nids
evaluation. In Proceedings of the First Workshop on
Building Analysis Datasets and Gathering Experience
Returns, 2011.

[30] C.-F. Tsai, Y.-F. Hsu, C.-Y. Lin, and W.-Y. Lin.

Intrusion detection by machine learning: A review.
Expert Systems with Applications, 36(10):11994 –
12000, 2009.

58