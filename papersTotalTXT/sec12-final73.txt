Fuzzing with Code Fragments

Christian Holler

Mozilla Corporation∗
choller@mozilla.com

Kim Herzig

Saarland University

herzig@cs.uni-saarland.de

Andreas Zeller

Saarland University

zeller@cs.uni-saarland.de

Abstract
Fuzz testing is an automated technique providing random
data as input to a software system in the hope to expose
a vulnerability. In order to be effective, the fuzzed input
must be common enough to pass elementary consistency
checks; a JavaScript interpreter, for instance, would only
accept a semantically valid program. On the other hand,
the fuzzed input must be uncommon enough to trigger
exceptional behavior, such as a crash of the interpreter.
The LangFuzz approach resolves this conﬂict by using
a grammar to randomly generate valid programs; the
code fragments, however, partially stem from programs
known to have caused invalid behavior before. LangFuzz
is an effective tool for security testing: Applied on the
Mozilla JavaScript interpreter, it discovered a total of
105 new severe vulnerabilities within three months of
operation (and thus became one of the top security bug
bounty collectors within this period); applied on the PHP
interpreter, it discovered 18 new defects causing crashes.

1

Introduction

Software security issues are risky and expensive.
In 2008, the annual CSI Computer Crime & Security sur-
vey reported an average loss of 289,000 US$ for a single
security incident. Security testing employs a mix of tech-
niques to ﬁnd vulnerabilities in software. One of these
techniques is fuzz testing—a process that automatically
generates random data input. Crashes or unexpected be-
havior point to potential software vulnerabilities.

In web browsers, the JavaScript interpreter is partic-
ularly prone to security issues; in Mozilla Firefox, for
instance, it encompasses the majority of vulnerability
ﬁxes [13]. Hence, one could assume the JavaScript in-
terpreter would make a rewarding target for fuzz test-
ing. The problem, however, is that fuzzed input to a
∗At the time of this study, Christan Holler was writing his master

thesis at Saarland University. He is now employed at Mozilla.

JavaScript interpreter must follow the syntactic rules of
JavaScript. Otherwise, the JavaScript interpreter will re-
ject the input as invalid, and effectively restrict the test-
ing to its lexical and syntactic analysis, never reaching
areas like code transformation, in-time compilation, or
actual execution. To address this issue, fuzzing frame-
works include strategies to model the structure of the de-
sired input data; for fuzz testing a JavaScript interpreter,
this would require a built-in JavaScript grammar.

Surprisingly, the number of fuzzing frameworks that
generate test inputs on grammar basis is very limited [7,
17, 22]. For JavaScript, jsfunfuzz [17] is amongst the
most popular fuzzing tools, having discovered more that
1,000 defects in the Mozilla JavaScript engine. jsfunfuzz
is effective because it is hardcoded to target a speciﬁc
interpreter making use of speciﬁc knowledge about past
and common vulnerabilities. The question is: Can we
devise a generic fuzz testing approach that nonetheless
can exploit project-speciﬁc knowledge?

In this paper, we introduce a framework called
LangFuzz that allows black-box fuzz testing of engines
based on a context-free grammar. LangFuzz is not bound
against a speciﬁc test target in the sense that it takes the
grammar as its input: given a JavaScript grammar, it will
generate JavaScript programs; given a PHP grammar, it
will generate PHP programs. To adapt to speciﬁc targets,
LangFuzz can use its grammar to learn code fragments
from a given code base. Given a suite of previously fail-
ing programs, for instance, LangFuzz will use and re-
combine fragments of the provided test suite to generate
new programs—assuming that a recombination of pre-
viously problematic inputs has a higher chance to cause
new problems than random input.

The combination of fuzz testing based on a language
grammar and reusing project-speciﬁc issue-related code
fragments makes LangFuzz an effective tool for secu-
rity testing. Applied on the Mozilla JavaScript engine,
it discovered a total of 105 new vulnerabilities within
three months of operation. These bugs are serious and

1

1var haystack = "foo";
2var re text = "^foo";
3haystack += "x";
4 re text += "(x)";
5var re = new RegExp(re text);
6re. test (haystack);
7RegExp.input = Number();
8 print(RegExp.$1);

Figure 2: Test case generated by LangFuzz, crashing the
JavaScript interpreter when executing Line 8. The static
access of RegExp is deprecated but valid. Reported as
Mozilla bug 610223 [1].

Section 7 discusses threats to validity, and Section 8
closes with conclusion and future work.

2 Background

2.1 Previous Work
“Fuzz testing” was introduced in 1972 by Purdom [16].
It is one of the ﬁrst attempts to automatically test a parser
using the grammar it is based on. We especially adapted
Purdom’s idea of the “Shortest Terminal String Algo-
rithm” for LangFuzz.
In 1990, Miller et al. [10] were
among the ﬁrst to apply fuzz testing to real world appli-
cations. In their study, the authors used random gener-
ated program inputs to test various UNIX utilities. Since
then, the technique of fuzz testing has been used in many
different areas such as protocol testing [6,18], ﬁle format
testing [19, 20], or mutation of valid input [14, 20].

Most relevant for this paper are earlier studies on
grammar-based fuzz testing and test generations for com-
piler and interpreters. In 2005, Lindig [8] generated code
to speciﬁcally stress the C calling convention and check
the results later. In his work, the generator also uses re-
cursion on a small grammar combined with a ﬁxed test
generation scheme. Molnar et al. [12] presented a tool
called SmartFuzz which uses symbolic execution to trig-
ger integer related problems (overﬂows, wrong conver-
sion, signedness problems, etc.) in x86 binaries. In 2011,
Yang et al. [22] presented CSmith—a language-speciﬁc
fuzzer operating on the C programming language gram-
mar. CSmith is a pure generator-based fuzzer generat-
ing C programs for testing compilers and is based on
earlier work of the same authors and on the random C
program generator published by Turner [21]. In contrast
to LangFuzz, CSmith aims to target correctness bugs in-
stead of security bugs. Similar to our work, CSmith ran-
domly uses productions from its built-in C grammar to
create a program. In contrast to LangFuzz, their gram-
mar has non-uniform probability annotations. Further-
more, they already introduce semantic rules during their

Figure 1: LangFuzz workﬂow. Using a language gram-
mar, LangFuzz parses code fragments from sample code
and test cases from a test suite, and mutates the test cases
to incorporate these fragments. The resulting code is
then passed to the interpreter for execution.

valuable, as expressed by the 50.000$ bug bounties they
raised. Nearly all the detected bugs are memory safety
issues. At the same time, the approach can generically
handle arbitrary grammars, as long as they are weakly
typed: applied on the PHP interpreter, it discovered 18
new defects. All generated inputs are semantically cor-
rect and can be executed by the respective interpreters.

Figure 1 describes the structure of LangFuzz. The
framework requires three basic input sources: a language
grammar to be able to parse and generate code artifacts,
sample code used to learn language fragments, and a test
suite used for code mutation. Many test cases contain
code fragments that triggered past bugs. The test suite
can be used as sample code as well as mutation basis.
LangFuzz then generates new test cases using code mu-
tation and code generation strategies before passing the
generated test cases to a test driver executing the test
case—e.g. passing the generated code to an interpreter.
As an example of a generated test case exposing a se-
curity violation, consider Figure 2 that shows a secu-
rity issue in Mozzila’s JavaScript engine. RegExp.$1
(Line 8) is a pointer to the ﬁrst grouped regular expres-
sion match. This memory area can be altered by setting
a new input (Line 7). An attacker could use the pointer
to arbitrarily access memory contents. In this test case,
Lines 7 and 8 are newly generated by LangFuzz, whereas
Lines 1–6 stem from an existing test case.

The remainder of this paper is organized as follows.
Section 2 discusses the state of the art in fuzz testing
and provides fundamental deﬁnitions. Section 3 presents
how LangFuzz works, from code generation to actual
test execution; Section 4 details the actual implemen-
tation. Section 5 discusses our evaluation setup, where
we compare LangFuzz against jsfunfuzz and show that
LangFuzz detects several issues which jsfunfuzz misses.
Section 6 describes the application of LangFuzz on PHP.

2

LangFuzzSample CodeTest SuiteMutatedTestLanguageGrammarPhase IIIFeed test case into interpreter, check for crashes and assertionsPhase IILangFuzz generated  (mutated) test casesPhase ILearning code fragments from samplecode and test suitegeneration process by using ﬁlter functions, which allow
or disallow certain productions depending on the con-
text. This is reasonable when constructing a fuzzer for a
speciﬁc language, but very difﬁcult for a language inde-
pendent approach as we are aiming for.

Fuzzing web browsers and their components is a
promising ﬁeld. The most related work in this ﬁeld is the
work by Ruderman and his tool jsfunfuzz [17]. Jsfunfuzz
is a black-box fuzzing tool for the JavaScript engine that
had a large impact when written in 2007. Jsfunfuzz not
only searches for crashes but can also detect certain cor-
rectness errors by differential testing. Since the tool was
released, it has found over 1,000 defects in the Mozilla
JavaScript Engine and was quickly adopted by browser
developers. jsfunfuzz was the ﬁrst JavaScript fuzzer that
was publicly available (it has since been withdrawn) and
thus inspired LangFuzz. In contrast, LangFuzz does not
speciﬁcally aim at a single language, although this paper
uses JavaScript for evaluation and experiments. Instead,
our approaches aim to be solely based on grammar and
general language assumptions and to combine random
input generation with code mutation.

Miller and Peterson [11] evaluated these two
approaches—random test generation and modifying ex-
isting valid inputs—on PNG image formats showing that
mutation testing alone can miss a large amount of code
due to missing variety in the original inputs. Still, we
believe that mutating code snippets is an important step
that adds regression detection capabilities. Code that has
been shown to detect defects helps to detect incomplete
ﬁxes when changing their context or fragments, espe-
cially when combined with a generative approach.

LangFuzz is a pure black-box approach, requiring no
source code or other knowledge of the tested interpreter.
As shown by Godefroid et al. [7] in 2008, a grammar-
based fuzzing framework that produces JavaScript en-
gine input (Internet Explorer 7) can increase coverage
when linked to a constraint solver and coverage measure-
ment tools. While we consider coverage to be an insuf-
ﬁcient indicator for test quality in interpreters (just-in-
time compilation and the execution itself heavily depend
on the global engine state), such an extension may also
prove valuable for LangFuzz.

In 2011, Zalewski [23] presented the crossfuzz tool
that is specialized in DOM fuzzing and revealed some
problems in many popular browsers. The same author
has published even more fuzzers for speciﬁc purposes
like ref fuzz, mangleme, Canvas fuzzer or transfuzz.
They all target different functionality in browsers and
have found severe vulnerabilities.

2.2 Deﬁnitions
Throughout this paper, we will make use of the following
terminology and assumptions.

Defect. Within this paper, the term “defect” refers to er-
rors in code that cause abnormal termination only
(e.g. crash due to memory violation or an assertion
violation). All other software defects (e.g. defect
that produce false output without abnormal termi-
nation) will be disregarded, although such defects
might be detected under certain circumstances. We
think that this limitation is reasonable due to the
fact that detecting other types of defects using fuzz-
testing generally requires strong assumptions about
the target software under test.

Grammar. In this paper, the term “grammar” refers to
context-free grammars (Type-2 in the Chomsky hi-
erarchy) unless stated otherwise.

Interpreter. An “interpreter” in the sense of this paper
is any software system that receives a program in
source code form and then executes it. This also
includes just-in-time compilers which translate the
source to byte code before or during runtime of the
program. The main motivation to use grammar-
based fuzz testing is the fact that such interpreter
systems consist of lexer and parser stages that detect
malformed input which causes the system to reject
the input without even executing it.

3 How LangFuzz works

In fuzz testing, we can roughly distinguish between two
techniques: Generative approaches try to create new ran-
dom input, possibly using certain constraints or rules.
Mutative approaches try to derive new testing inputs
from existing data by randomly modifying it. For exam-
ple, both jsfunfuzz [17] and CSmith [22] use generative
approaches. LangFuzz makes use of both approaches,
but mutation is the primary technique. A purely gen-
erative design would likely fail due to certain semantic
rules not being respected (e.g. a variable must be de-
ﬁned before it is used).
Introducing semantic rules to
solve such problems would tie LangFuzz to certain lan-
guage semantics. Mutation, however, allows us to learn
and reuse existing semantic context. This way, LangFuzz
stays language-independent without losing the ability to
generate powerful semantic context to embed generated
or mutated code fragments.

3.1 Code mutation
The mutation process consists of two phases, a learn-
ing phase in the beginning and the main mutation phase.

3

In the learning phase, we process a certain set of sam-
ple input ﬁles using a parser for the given language (de-
rived from the language grammar). The parser will allow
us to separate the input ﬁle into code fragments which
are essentially examples for non-terminals in the gram-
mar. Of course, these fragments may overlap (e.g. an
expression might be contained in an ifStatement
which is a statement according to the grammar). Given
a large codebase, we can build up a fragment pool
consisting of expansions for all kinds of non-terminal
symbols. Once we have learned all of our input, the
mutation phase starts. For mutation, a single target ﬁle
is processed again using the parser. This time, we ran-
domly pick some of the fragments we saw during parsing
and replace them with other fragments of the same type.
These code fragments might of course be semantically
invalid or less useful without the context that surrounded
them originally, but we accept this trade-off for being in-
dependent of the language semantics. In Section 3.3, we
discuss one important semantic improvement performed
during fragment replacement.

As our primary target is to trigger defects in the tar-
get program, it is reasonable to assume that existing test
cases (especially regressions) written in the target lan-
guage should be helpful for this purpose; building and
maintaining such test suites is standard practice for de-
velopers of interpreters and compilers. Using the mu-
tation process described in the previous section, we can
process the whole test suite ﬁle by ﬁle, ﬁrst learning frag-
ments from it and then creating executable mutants based
on the original tests.

3.2 Code generation
With our mutation approach, we can only use those code
fragments as replacements that we have learned from our
code base before. Intuitively, it would also be useful if
we could generate fragments on our own, possibly yield-
ing constructs that cannot or can only hardly be produced
using the pure mutation approach.

Using a language grammar, it is natural to generate
fragments by random walk over the tree of possible ex-
pansion series. But performing a random walk with uni-
form probabilities is not guaranteed to terminate. How-
ever, terminating the walk without completing all expan-
sions might result in a syntactically invalid input.

Usually, this problem can be mitigated by restructur-
ing the grammar, adding non-uniform probabilities to the
edges and/or imposing additional semantic restrictions
during the production, as in the CSmith work [22].

Restructuring or annotating the grammar with prob-
abilities is not straightforward and requires additional
work for every single language.
It is even reasonable
to assume that using ﬁxed probabilities can only yield a

Figure 3: Example of a stepwise expansion on the syn-
tax tree: Dark nodes are unexpanded non-terminals (can
be expanded) while the other nodes have already been
expanded before.

coarse approximation as the real probabilities are condi-
tional, depending on the surrounding context. To over-
come these problems, we will use an algorithm that per-
forms the generation in a breadth-ﬁrst manner:

1. Set current expansion ecur to the start symbol S
2. Loop num iterations:

(a) Choose a random non-terminal n in ecur:

i. Find the set of productions Pn ⊆ P that

can be applied to n.

ii. Pick one production p from Pn randomly

and apply it to n, yielding p(n).

iii. Replace that occurrence of n in ecur by

p(n).

Figure 3 gives an example of such a stepwise expan-
sion, considering the code as a syntax tree. Dark nodes
are unexpanded non-terminals that can be considered for
expansion while the remaining nodes have already been
expanded before. This algorithm does not yield a valid
expansion after num iterations. We need to replace the re-
maining non-terminal symbols by sequences of terminal
symbols. In the learning phase of the mutation approach
we are equipped with many different examples for dif-
ferent types of non-terminals. We randomly select any
of these code fragments to replace our remaining non-
terminals. In the unlikely situation that there is no ex-
ample available, we can use the minimal expansion of
the non-terminal instead. During mutation, we can use
learned and generated code fragments.

3.3 Adjusting Fragments to Environment
When a fragment is replaced by a different fragment, the
new fragment might not ﬁt with respect to the semantics
of the remaining program. As LangFuzz does not aim to
semantically understand a speciﬁc language, we can only
perform corrections based on generic semantic assump-
tions. One example with a large impact are identiﬁers.

4

Many programming languages use identiﬁers to refer
to variables and functions, and some of them will throw
an error if an identiﬁer has not been declared prior to us-
ing it (e.g. in JavaScript, using an identiﬁer that is never
declared is considered to be a runtime error).

We can reduce the chances to have undeclared identi-
ﬁers within the new fragment by replacing all identiﬁers
in the fragment with identiﬁers that occur somewhere
in the rest of the program. Note that this can be done
purely at the syntactic level. LangFuzz only needs to
know which non-terminal in the grammar constitutes an
identiﬁer in order to be able to statically extract known
identiﬁers from the program and replace identiﬁers in the
new fragment. Thus, it is still possible that identiﬁers
are unknown at the time of executing a certain statement
(e.g. because the identiﬁer is declared afterwards), but
the chances of identiﬁer reuse are increased.

Some languages contain identiﬁers that can be used
without declaring them (usually built-in objects/globals).
The adjustment approach can be even more effective if
LangFuzz is aware of these global objects in order to ig-
nore them during the replacement process. The only way
to identify such global objects within LangFuzz is to re-
quire a list of these objects as (optional) argument. Such
global object lists are usually found in the speciﬁcation
of the respective language.

4 The LangFuzz Implementation

Based on the methods described so far, we now assem-
ble the different parts to get a proof-of-concept fuzzer
implementation that works as described in the overview
diagram (Figure 1) in the introduction.

Typically, LangFuzz starts with a learning phase
where the given sample code is parsed using the sup-
plied language grammar,
thereby learning code frag-
ments (Section 4.1). The input of this learning phase
can be either a sample code base or the test suite itself.
Once the learning step is complete, LangFuzz starts to
process the test suite. All tests are parsed and the results
are cached for performance reasons.

Then the tool starts the actual working phase:

1. From the next test to be mutated, several fragments
(determined by an adjustable parameter, typically
1–3) are randomly selected for replacement.

2. As a single fragment can be considered as multi-
ple types (e.g. if (true) {...} can be seen as
an if-statement but also more generally as a state-
ment), we randomly pick one of the possible inter-
pretations for each of those fragments.

3. Finally, the mutated test is executed and its result is

checked (Section 4.3).

5

4.1 Code Parsing
In the learning and mutation phase, we parse the given
source code. For this purpose, LangFuzz contains a
parser subsystem such that concrete parsers for different
languages can be added. We decided to use the ANTLR
parser generator framework [15] because it is widespread
and several grammars for different languages exist in the
community. The parser is ﬁrst used to learn fragments
from the given code base which LangFuzz then memo-
rizes as a token stream. When producing a mutated test,
the cached token stream is used to ﬁnd all fragments in
the test that could be replaced and to determine which
code can be replaced according to the syntax—we can
mutate directly on the cached token stream.

4.2 Code Generation
The code generation step uses the stepwise expansion
(Section 3.2) algorithm to generate a code fragment.
As this algorithm works on the language grammar,
LangFuzz also includes an ANTLR parser for ANTLR
grammars. However, because LangFuzz is a proof-of-
concept, this subsystem only understands a subset of
the ANTLR grammar syntax and certain features that
are only required for parsing (e.g. implications) are not
supported. It is therefore necessary to simplify the lan-
guage grammar slightly before feeding it into LangFuzz.
LangFuzz uses further simpliﬁcations internally to make
the algorithm easier: Rules containing quantiﬁers (’*’,
‘+’) and optionals (’?’) are de-sugared to remove these
operators by introducing additional rules according to the
following patterns:

X∗ (cid:32) (R → ε |XR)
X+ (cid:32) (R → X |XR)
X? (cid:32) (R → ε |X)

(zero or more)
(one or more)
(zero or one)

where X can be any complex expression. Furthermore,
sub-alternatives (e.g. R → ((A|B)C|D)), are split up into
separate rules as well. With these simpliﬁcations done,
the grammar only consists of rules for which each alter-
native is only a sequence of terminals and non-terminals.
While we can now skip special handling of quantiﬁers
and nested alternatives, these simpliﬁcations also intro-
duce a new problem: The additional rules (synthesized
rules) created for these simpliﬁcations have no counter-
part in the parser grammar and hence there are no code
examples available for them. In case our stepwise ex-
pansion contains one or more synthesized rules, we re-
place those by their minimal expansion as described in
Section 3.2. All other remaining non-terminals are re-
placed by learned code fragments as described earlier.
In our implementation, we introduced a size limitation

on these fragments to avoid placing huge code fragments
into small generated code fragments.

After code generation, the fragment replacement code
adjusts the new fragment to ﬁt its new environment as
described in Section 3.3. For this purpose, LangFuzz
searches the remaining test for available identiﬁers and
maps the identiﬁers in the new fragment to existing ones.
The mapping is done based on the identiﬁer name, not its
occurrence, i.e. when identiﬁer “a” is mapped to “b”, all
occurrences of “a” are replaced by “b”. Identiﬁers that
are on the built-in identiﬁer list (e.g. global objects) are
not replaced. LangFuzz can also actively map an identi-
ﬁer to a built-in identiﬁer with a certain probability.

4.3 Running Tests
In order to be able to run a mutated test, LangFuzz must
be able to run the test with its proper test harness which
contains deﬁnitions required for the test. A good exam-
ple is the Mozilla test suite: The top level directory con-
tains a ﬁle shell.js with deﬁnitions required for all tests.
Every subdirectory may contain an additional shell.js
with further deﬁnitions that might only be required for
the tests in that directory. To run a test, the JavaScript
engine must execute all shell ﬁles in the correct order,
followed by the test itself. LangFuzz implements this
logic in a test suite class which can be derived and ad-
justed easily for different test frameworks.

The simplest method to run a mutated test is to start the
JavaScript engine binary with the appropriate test har-
ness ﬁles and the mutated test. But starting the JavaScript
engine is slow and starting it over and over again would
cost enormous computation time. To solve this problem,
LangFuzz uses a persistent shell: A small JavaScript pro-
gram called the driver is started together with the test
harness. This way, we reduce the number of required
JavaScript engines to be started drastically. The driver
runs a set of tests within one single JavaScript engine
and signals completion when done. LangFuzz monitors
each persistent shell and records all input to it for later
reproduction. Of course the shell may not only be ter-
minated because of a crash, but also because of timeouts
or after a certain number of tests being run. The test
driver is language dependent and needs to be adapted for
other languages (see Section 6); such a test driver would
also be required if one implemented a new fuzzer from
scratch.

Although the original motivation to use persistent
shells was to increase test throughput it has an important
side-effect. It increased the number of defects detected.
Running multiple tests within a single shell allows indi-
vidual tests to inﬂuence each other. Different tests may
use the same variables or functions and cause crashes
that would not occur when running the individual tests

alone. In fact, most of the defects found in our experi-
ments required multiple tests to be executed in a row to
be triggered. This is especially the case for memory cor-
ruptions (e.g. garbage collector problems) that require
longer runs and a more complex setup than a single test
could provide.

Running multiple tests in one shell has the side effect
that it increases the number of source code lines executed
within each JavaScript shell. To determine which indi-
vidual tests are relevant for failure reproduction we use
the delta debugging algorithm [24] and the delta tool [9]
to ﬁlter out irrelevant test cases. The very same algorithm
also reduces the remaining number of executed source
code lines. The result is a suitably small test case.

4.4 Parameters
LangFuzz contains a large amount of adjustable param-
eters, e.g. probabilities and amounts that drive decisions
during the fuzzing process. In Table 3 (see Appendix)
we provide the most common/important parameters and
their default values. Please note that all default values are
chosen empirically. Because the evaluation of a certain
parameter set is very time consuming (1–3 days per set
and repeating each set hundreds of time times to elim-
inate the variance introduced by random generation), it
was not feasible to compare all possible parameter com-
binations and how they inﬂuence the results. We tried
to use reasonable values but cannot guarantee that these
values deliver the best performance.

5 Evaluation

To evaluate how well LangFuzz discovers undetected er-
rors in the JavaScript engines, we setup three different
experimental setups. The external validation compares
LangFuzz to the state of the art in JavaScript fuzzing.
The internal validation compares the two fragment re-
placement strategies used within LangFuzz:
random
code generation and code mutation. Finally, we con-
ducted a ﬁeld study to check whether LangFuzz is ac-
tually up to the task to detect real defects in current state
of the art JavaScript engines.

5.1 LangFuzz vs. jsfunfuzz
The state of the art fuzzer for JavaScript is the jsfunfuzz
tool written by Ruderman [17]. The tool is widely used
and has proven to be very successful in discovering de-
fect within various JavaScript engines.
jsfunfuzz is an
active part of Mozilla’s and Google’s quality assurance
and regularly used in their development.

The differences between jsfunfuzz and LangFuzz are
signiﬁcant and allow only unfair comparisons between

6

both tools. jsfunfuzz is highly adapted to test JavaScript
engines and contains multiple optimizations. jsfunfuzz is
designed to test new and previously untested JavaScript
features intensively. This of course required detailed
knowledge of the software project under test. Addition-
ally, jsfunfuzz has a certain level of semantic knowledge
and should be able to construct valid programs easier.
However, for every new language feature, the program
has to be adapted to incorporate these changes into the
testing process. Also, focusing on certain semantics can
exclude certain defects from being revealed at all.

In contrast, LangFuzz bases its testing strategy solely
on the grammar, existing programs (e.g. test suites) and
a very low amount of additional language-dependent in-
formation. In practice, this means that

• changes to the language under test do not re-
quire any program maintenance apart from possible
grammar updates; and
• through the choice of test cases, LangFuzz can be

set up to cover a certain application domain.

The use of existing programs like previous regression
tests allows LangFuzz to proﬁt from previously detected
defects. However, LangFuzz lacks a semantic back-
ground on the language which lowers the chances to ob-
tain sane programs and produce test cases that trigger a
high amount of interaction between individual parts of
the program.

Although both tools have some differences that make
a fair comparison difﬁcult, comparing both tools can un-
veil two very interesting questions:

Q1. To what extend do defects detected by LangFuzz and

jsfunfuzz overlap?

By overlap, we refer to the number of defects that both
tools are able to detect. A low overlap would indicate
that LangFuzz is able to detect new defects that were not
found and most likely will not be found by jsfunfuzz.
Therefore we deﬁne the overlap as the fraction of num-
ber of defects found by both tools and the number of de-
fects found in total. This gives us a value between zero
and one. A value of one would indicate that both tools
detected exactly the same defects. If both tools detected
totally different defects, the overlap would be zero.

overlap =

number of defects found by both tools

number of defects found in total

The second question to be answered by this compari-

son is targeted towards the effectiveness of LangFuzz.

Q2. How does LangFuzz’s detection rate compare to js-

funfuzz?

By effectiveness, we mean how many defects each tool
is able to locate in a given period of time. Even though
the overlap might be large, it might be the case that either
tool might detect certain defects much quicker or slower
than the respective other tool. To compare the effective-
ness of LangFuzz in comparison against jsfunfuzz, we
deﬁne the effectiveness as:

effectiveness =

number of defects found by LangFuzz
number of defects found by jsfunfuzz

.

This sets the number of defects found by LangFuzz
into relation to the number of defects found by jsfunfuzz.
Since both tools ran on the same time windows, the same
amount of time using identical amounts of resources (e.g.
CPU and RAM) we do not have to further normalize this
value.

Overall, this comparison answers the question whether
LangFuzz is a useful contribution to a quality assurance
process, even if a fuzzer such as jsfunfuzz is already
used. It is not our intention to show that either tool out-
performs the other tool by any means. We believe that
such comparisons are non-beneﬁcial since both jsfunfuzz
and LangFuzz operate on different assumptions and lev-
els.

5.1.1 Testing windows

We compared jsfunfuzz and LangFuzz using Mozilla’s
JavaScript engine TraceMonkey. There were two main
reasons why we decided to choose TraceMonkey as
comparison base. First, Mozilla’s development process
and related artifacts are publicly available—data that re-
quired internal permission was kindly provided by the
Mozilla development team. The second main reason was
that jsfunfuzz is used in Mozilla’s daily quality assurance
process which ensures that jsfunfuzz is fully functional
on TraceMonkey without investing any effort to setup
the external tool. But using TraceMonkey as reference
JavaScript engine also comes with a downside. Since
jsfunfuzz is used daily within Mozilla, jsfunfuzz had al-
ready run on every revision of the engine. This fact has
two major consequences: First, jsfunfuzz would most
likely not ﬁnd any new defects; and second, the num-
ber of potential defects that can be found by LangFuzz
is signiﬁcantly reduced. Consequently, it is not possi-
ble to measure effectiveness based on a single revision
of TraceMonkey. Instead we make use of the fact that
Mozilla maintains a list of defects found by jsfunfuzz.
Using this list, we used a test strategy which is based on
time windows in which no defect ﬁxes were applied that
are based on defect reports ﬁled by jsfunfuzz (see Fig-
ure 4). Within these periods, both tools will have equal
chances to ﬁnd defects within the TraceMoney engine.

7

defects that where introduced between A and B which is
the testing window.

For all tests, we used the TraceMonkey development
repository. Both the tested implementation and the test
cases (approximately 3,000 tests) are taken from the de-
velopment repository. As base revision, we chose re-
vision 46549 (03f3c7efaa5e) which is the ﬁrst revision
committed in July 2010, right after Mozilla Firefox 4
Beta 1 was released at June 30, 2010. Table 1 shows
the ﬁve test windows used for our experiments. The end
revision of the last testing window dates to the end of Au-
gust 2010, implying that we covered almost two months
of development activity using these ﬁve windows. For
each testing window, we ran both tools for 24 hours.2

To check whether a defect detected by either tool was
introduced in the current testing window, we have to
detect the lifetime of the issue. Usually, this can be
achieved by using the bisect command provided by the
Mercurial SCM. This command allows automated test-
ing through the revision history to ﬁnd the revision that
introduced or ﬁxed a certain defect. Additionally, we
tried to identify the corresponding issue report to check
whether jsfunfuzz found the defect in daily quality assur-
ance routine.

5.1.2 Result of the external comparison

During the experiment, jsfunfuzz identiﬁed 23 defects,
15 of which lay within the respective testing windows.
In contrast, LangFuzz found a total of 26 defects with
only 8 defects in the respective testing windows. The
larger proportion of defects outside the testing windows
for LangFuzz is not surprising since LangFuzz, unlike
jsfunfuzz, was never used on the source base before this
experiment. Figure 5 illustrates the number of defects
per fuzzer within the testing windows.

To address research question Q1, we identiﬁed three
defects found by both fuzzers. Using the deﬁnition from
Section 5.1 the overlap between LangFuzz and jsfunfuzz
is 15%. While a high overlap value would indicate that
both fuzzers could be replaced by each other, an overlap
value of 15% is a strong indication that both fuzzers ﬁnd
different defects and hence supplement each other.

LangFuzz and jsfunfuzz detect different defects

(overlap of 15%) and thus should be used

complementary to each other.

To answer research question Q2, we computed the ef-
fectiveness of LangFuzz for the defects found within the
experiment.

2For both tools we used the very same hardware. Each tool ran on
4 CPUs with the same speciﬁcation. Since jsfunfuzz does not support
threading, multiple instances will be used instead. LangFuzz’s param-
eters were set to default values (see Table 3).

Figure 4: Example of a testing window with the live cy-
cle of a single defect.

start revision
46569:03f3c7efaa5e
47557:3b1c3f0e98d8
48065:7ff4f93bddaa
48350:d7c7ba27b84e
49731:aaa87f0f1afe

end revision
47557:3b1c3f0e98d8
48065:7ff4f93bddaa
48350:d7c7ba27b84e
49731:aaa87f0f1afe
51607:f3e58c264932

W1
W2
W3
W4
W5

Table 1: The ﬁve testing windows used for the experi-
ments. Each window is given by Mercurial revisions of
the Mozilla version archive. All windows together cover
approximately two months of development activity.

Within the remainder of this paper, we call these periods
testing windows.

In detail, we applied the following test strategy for

both tools:

1. Start at some base revision f0. Run both tools for
a ﬁxed amount of time. Defects detected can solely
be used to analyze the overlap, not effectiveness.

2. Set n = 1 and repeat several times:

(a) Find the next revision fn starting at fn−1 that
ﬁxes a defect found in the list of jsfunfuzz de-
fects.
(b) Run both tools on fn − 1 for a ﬁxed amount
of time. The defects found by both tools can
be used for effectiveness measurement if and
only if the defect was introduced between fn−1
and fn − 1 (the preceding revision of fn)1. For
overlap measurement, all defects can be used.

Figure 4 illustrates how such a testing window could
look like. The window starts at revision A. At some
point, a bug is introduced and shortly afterwards, the bug
gets reported by jsfunfuzz. Finally, the bug is ﬁxed in
revision B + 1. At this point, our testing window ends
and we can use revision B for experiments and count all
1 fn − 1 is exactly one revision before fn and spans the testing win-
dow. The testing window starts at fn−1 and ends at fn − 1 because fn is
a jsfunfuzz-induced ﬁx.

8

RevisionsAB1Bug is introduced2jsfunfuzzdetects bug3Bug is ﬁxed(last  jsfunfuzz related ﬁx)testing windowwithout jsfunfuzzrelated ﬁxes3

1options(’tracejit’);
2for (var j = 0; uneval({’-1’:true}); ++j) {
(−0).toString();
4}
Figure 6: Test case generated by LangFuzz causing the
TraceMonkey JavaScript interpreter to violate an inter-
nal assertion when executed. Reported as Mozilla bug
626345 [2].

This experiment should clarify whether only one of
the approaches accounts for most of the results (and the
other only slightly improves it or is even dispensable) or
if both approaches must be combined to achieve good
results.

Q3. How important is it that LangFuzz generates new

code?

Q4. How important is it that LangFuzz uses learned

code when replacing code fragments?

To measure the inﬂuence of either approach, we re-
quire two independent runs of LangFuzz with different
conﬁgurations but using equal limitation on resources
and runtime. The ﬁrst conﬁguration forced LangFuzz to
use only learned code snippets for fragment replacement
(mutation conﬁguration). The second conﬁguration al-
lowed code fragmentation by code generation only (gen-
eration conﬁguration).

Intuitively, the second conﬁguration should perform
random code generation without any code mutation at
all—also not using parsed test cases as fragmentation re-
placement basis. Such a setup would mean to fall back to
a purely generative approach eliminating the basic idea
behind LangFuzz.
It would also lead to incomparable
results. The length of purely generated programs is usu-
ally small. The larger the generated code the higher the
chance to introduce errors leaving most of the generated
code meaningless. Also, when mutating code, we can
adjust the newly introduced fragment to the environment
(see Section 3.3). Using purely generated code instead,
this is not possible since there exists no consistent envi-
ronment around the location where a fragment is inserted
(in the syntax tree at the end of generation). Although it
would be possible to track the use of identiﬁers during
generation the result would most likely not be compara-
ble to results derived using code mutation.

Since we compared different LangFuzz conﬁgurations
only, there is no need to use the testing windows from the
previous experiment described in Section 5.1. Instead,
we used the two testing windows that showed most de-
fect detection potential when comparing LangFuzz with
jsfunfuzz (see Section 5.1): W1 and W5. Both windows

9

Figure 5: Number of defects found by each fuzzer within
the testing windows and their overlap.

Compared to the 15 defects that were exclusively de-
tected by jsfunfuzz LangFuzz with it’s eight exclusively
detected defect has an effectiveness of 15 : 8 = 53%. In
other words, LangFuzz is half as effective as jsfunfuzz.

A generic grammar-based fuzzer like LangFuzz can be

53% as effective as a language-speciﬁc fuzzer like

jsfunfuzz.

For us, it was not surprising that a tried-and-proven
language-speciﬁc fuzzer is more effective than our more
general approach. However, effectiveness does not imply
capability. The several severe issues newly discovered by
LangFuzz show that the tool is capable of ﬁnding bugs
not detected by jsfunfuzz.

5.1.3 Example for a defect missed by jsfunfuzz

For several defects (especially garbage collector related)
we believe that jsfunfuzz was not able to trigger them
due to their high complexity even after minimization.
Figure 6 shows an example of code that triggered an as-
sertion jsfunfuzz was not able to trigger. In the original
bug report, Jesse Ruderman conﬁrmed that he tweaked
jsfunfuzz in response to this report: “After seeing this
bug report, I tweaked jsfunfuzz to be able to trigger it.”
After adaptation, jsfunfuzz eventually produced an even
shorter code fragment triggering the assertion (we used
the tweaked version in our experiments).

5.2 Generation vs. Mutation
The last experiment compares LangFuzz with the state
of the art JavaScript engine fuzzer jsfunfuzz. The aim
of this experiment is to compare the internal fragment
replacement approaches of LangFuzz: code generation
against code mutation. The ﬁrst option learned code frag-
ments to replace code fragments while the second option
uses code generation (see Section 4.2) for replacement
instead.

81583 (15%)overlapLangFuzzdefectsjsfunfuzzdefects1 ( ’false’? length(input + ’’): delete(null?0:{}),0 ).

watch(’x’, function f() { });

Figure 8: Test case generated by LangFuzz using code
generation. The code cause the TraceMonkey JavaScript
interpreter to write to a null pointer. Reported as Mozilla
bug 626436 [3].

tation replacement strategy. Interestingly, this proportion
is reversed in test window W5. Here a total of 24 de-
fects and again the majority of 15 defects were found
using both conﬁgurations. But in W5 the number of de-
fects found by mutation conﬁguration exceeds the num-
ber of defects found by code generation. Combining the
number of defects found by either conﬁgurations exclu-
sively, code generation detected 13 defects that were not
detected by the mutation conﬁguration. Vice versa, code
mutation detected 9 defects that were not detected during
the code generation conﬁguration run. Although the ma-
jority of 29 defects found by both conﬁgurations, these
numbers and proportions show that both of LangFuzz in-
ternal fragmentation replacement approaches are crucial
for LangFuzz success and should be combined. Thus,
an ideal approach should be a mixed setting where both
code generation and direct fragment replacement is done,
both with a certain probability.

The combination of code mutation and code

generation detects defects not detected by either

internal approach alone. Combining both approaches

makes LangFuzz successful.

5.2.2 Example of defect detected by code generation

The code shown in Figure 8 triggered an error in the
parser subsystem of Mozilla TraceMonkey. This test was
partly produced by code generation. The complex and
unusual syntactic nesting here is unlikely to happen by
only mutating regular code.

5.2.3 Example for detected incomplete ﬁx

The bug example shown in Figure 9 caused an assertion
violation in the V8 project and is a good example for both
an incomplete ﬁx detected by LangFuzz and the beneﬁts
of mutating existing regression tests: Initially, the bug
had been reported and ﬁxed as usual. Fixes had been
merged into other branches and of course a new regres-
sion test based on the LangFuzz test has been added to
the repository. Shortly after, LangFuzz triggered exactly
the same assertion again using the newly added regres-
sion test in a mutated form. V8 developers conﬁrmed
that the initial ﬁx was incomplete and issued another ﬁx.

10

Figure 7: Defects found with/without code generation.

showed a high defect rate in the experimental setup de-
scribed in Section 5.1 and spanned over 5,000 revisions
giving each conﬁguration enough defect detection poten-
tial to get comparable results.

Limiting the number of testing windows to compare
the two different conﬁgurations of LangFuzz, we were
able to increase the runtime for each conﬁguration, thus
minimizing the randomization impact on the experiment
at the same time. Both conﬁgurations ran on both testing
windows for 72 hours (complete experiment time was 12
days). For all runs we used the default parameters (see
Table 3), except for the synth.prob parameter. This
parameter can be used to force LangFuzz to use code
generation only (set to 1.0) and to ignore code generation
completely (set to 0.0).

Since the internal process of LangFuzz is driven by
randomization, we have to keep in mind that both runs
are independent and thus produce results that are hard to
compare. Thus, we should use these results as indica-
tions only.

5.2.1 Result of the internal comparison

Figure 7 shows the results of the internal comparison
experiment as overlapping circles. The left overlapping
circle pair shows the result of the comparison using test
window W1, the right pair the results using test window
W5. The dark circles represent the runs using the genera-
tion conﬁguration while the white circles represent runs
using the mutation conﬁguration. The overlap of the dark
and white circles contains those defects that can be de-
tected using both fragment replacement strategies. The
numbers left and right of the overlap show the number of
defects found exclusively by the corresponding conﬁgu-
ration.

For W1 a total of 31 defect were found. The major-
ity of 14 defects is detected by both conﬁgurations. But
11 defects were only found when using code generation
whereas six defects could only be detected using the mu-

116147215generation conﬁguration mutation conﬁgurationfound with both conﬁgurationstesting window W1testing window W51var loop count = 5
2function innerArrayLiteral(n) {
3 var a = new Array(n);
for (var i = 0; i < n; i++) {
a[i] = void ! delete ’object’%

4

5

6 ˜ delete 4
7 }
8}
9function testConstructOfSizeSize(n) {
10 var str = innerArrayLiteral(n);
11}
12for (var i = 0; i < loop count; i++) {

for (var j = 1000; j < 12000; j += 1000) {
testConstructOfSizeSize(j);

13

14

15 }
16}
Figure 9: Test case generated by LangFuzz discovering
an incomplete ﬁx triggering an assertion failure in the
Google V8 JavaScript engine. Reported as Google V8
bug 1167 [4].

5.3 Field tests

The ﬁrst two experiments are targeting the external and
internal comparison of LangFuzz. But so far, we did
not check whether LangFuzz is actually capable of ﬁnd-
ing real world defects within software projects used
within industry products. To address this issue, we con-
ducted an experiment applying LangFuzz to three differ-
ent language interpreter engines: Mozilla TraceMonkey
(JavaScript), Google V8 (JavaScript), and the PHP en-
gine. In all experiments, we used the default conﬁgura-
tion of LangFuzz including code generation. An issue
was marked as security relevant if the corresponding de-
velopment team marked the issue accordingly as security
issue. Thus the security relevance classiﬁcation was ex-
ternal and done by experts.

Mozilla TraceMonkey We tested LangFuzz on the
trunk branch versions of Mozilla’s TraceMonkey
JavaScript engine that were part of the Firefox 4 re-
lease. At the time we ran this experiment, Firefox
4 and its corresponding TraceMonkey version were
pre-release (beta version). Changes to the Trace-
Monkey trunk branch were regularly merged back
into the main repository.
Additionally, we ran LangFuzz on Mozilla’s type
inference branch of TraceMonkey. At that time,
this branch had alpha status and has not been part
of Firefox 4 (but was eventually included in Fire-
fox 5). Since this branch was no product branch,
no security assessment was done for issues reported
against it.

11

Figure 10: Real defects found on Mozilla, Google V8,
and PHP. These defects were reported as customer de-
fects and their numbers are not comparable to the de-
fect numbers in earlier ﬁgures.∗The security lock might
hide the issue report from public because it might be ex-
ploitable. †Defects reported for PHP were not classiﬁed
security relevant.

Google V8 Similar to the Mozilla ﬁeld test, we tested
LangFuzz on the Google V8 JavaScript engine con-
tained within the development trunk branch. At
the time of testing, Chrome 10—including the new
V8 optimization technique “Crankshaft”—was in
beta stage and ﬁxes for this branch were regularly
merged back into the Chrome 10 beta branch.

PHP To verify LangFuzz’s language independence, we
performed a proof-of-concept adaptation to PHP;
see Section 6 for details. The experiment was con-
ducted on the PHP trunk branch (SVN revision
309115). The experiment lasted 14 days.

5.3.1 Can LangFuzz detect real undetected defects?

For all three JavaScript engines, LangFuzz found be-
tween 51 and 59 defects (see Figure 10). For the Mozilla
TraceMonkey (FF4 Beta) branch, most left group of bars
in Figure 10, 39% of the found security issues where
classiﬁed as security related by the Mozilla development
team. Only nine defects were classiﬁed as duplicates of
bug reports not being related to our experiments. The
relatively low number of duplicates within all defect sets
shows that LangFuzz detects defects that slipped through
the quality gate of the individual projects, showing the
usefulness of LangFuzz. Although the fraction of secu-
rity related defects for the Google V8 branch is lower
(19%), it is still a signiﬁcant number of new security re-
lated defects being found. The number of security is-
sues within the Mozilla TraceMonkey (Type Inference)
branch is reported as zero, simply because this branch
was not part of any product at the time of the experi-

total number of defectduplicates to non-LangFuzz defectssecurity related*604530150Mozilla TM(FF4 Beta)Mozilla TM(Type Inference)Chrome V8(Chrome 10 beta)PHP†512095400unknown45911182exactly those issue reports ﬁled during our ﬁeld experi-
ments. Due to the fact that some of the bug reports could
be used to exploit the corresponding browser version,
some issue reports are security locked requiring special
permission to open the bug report. At the time of writing
this paper, this affects all Google V8 issue reports.

LangFuzz detected 164 real world defects in popular

JavaScript engines within four months, including
31 security related defects. On PHP, LangFuzz

detected 20 defects within 14 days.

5.3.2 Economic value

The number of defects detected by LangFuzz must be
interpreted with regard to the actual value of these de-
fects. Many of the defects were rewarded by bug
bounty awards. Within nine month of experiment-
ing with LangFuzz, defects found by the tool obtained
18 Chromium Security Rewards and 12 Mozilla Security
Bug Bounty Awards. We can only speculate on the po-
tential damage these ﬁndings prevented; in real money,
however, the above awards translated into 50,000 US$ of
bug bounties. Indeed, during this period, LangFuzz be-
came one of the top bounty collectors for Mozilla Trace-
Monkey and Google V8.

6 Adaptation to PHP

Although adapting LangFuzz to a new language is kept
as simple as possible, some adaptations are required.
Changes related to reading/running the respective project
test suite, integrating the generated parser/lexer classes,
and supplying additional language-dependent informa-
tion (optional) are necessary. In most cases, the required
effort for these changes adaptation changes is consider-
ably lower than the effort required to write a new lan-
guage fuzzer from scratch. The following is a short de-
scription of the changes required for PHP and in general:

Integration of Parser/Lexer Classes. Given a gram-
mar for the language, we ﬁrst have to generate the
Parser/Lexer Java classes using ANTLR (automatic
step). For PHP, we choose the grammar supplied by
the PHPParser project [5].
LangFuzz uses so called high-level parser/lexer
classes that override all methods called when pars-
ing non-terminals. These classes extract the non-
terminals during parsing and can be automatically
generated from the classes provided by ANTLR.
All these classes are part of LangFuzz and get in-
tegrated into the internal language abstraction layer.
Integration of Tests. LangFuzz provides a test suite
class that must be derived and adjusted depending

Figure 11: Cumulative sum of the total number of de-
fects found depending on the length of the experiment.
Approximate 30 days for Mozilla TM (FF4 Beta) and
Google Chrome V8. We did no such analysis for PHP.

ment. This is why the Mozilla development team made
no security assessment for these issue reports.

For PHP, the number of detected defects is much lower
(see Figure 10). We considered the PHP experiment as
a proof-of-concept adaptation and invested considerably
less time into this experiment. Still, the total number of
18 defects was detected just within 14 days (1.3 defects
per day). The majority of these defects concerned mem-
ory safety violations in the PHP engine. We consider
these to be potential security vulnerabilities if an engine
is supposed to run on untrusted input. However, the PHP
development team did not classify reported issues as se-
curity relevant.

Figure 11 shows the cumulative sum of the total num-
ber of defects found depending on the length of the ex-
periment. The length of the different plotted lines corre-
sponds to the number of days each experiment was con-
ducted. The result for the Mozilla TraceMonkey (FF4
Beta) branch differs in length and shape. This stems from
the fact that during this experiment (which was our ﬁrst
experiment) we ﬁxed multiple issues within LangFuzz
which did not affect the two later applied experiments.
For both Mozilla projects, LangFuzz detected constantly
new defects. The curves of the two shorter experiments
show a very steep gradient right after starting LangFuzz.
The longer the experiment, the lower the number of de-
fects found per time period. Although this is no surpris-
ing ﬁnding, it manifests that LangFuzz quickly ﬁnd de-
fects produced by test cases that differ greatly from other
test cases used before.

The issue reports corresponding to the defects reported
during ﬁeld experiments can be found online using the
links shown in Table 2. Each link references a search
query within the corresponding issue tracker showing

12

020406080100number of days0102030405060total number of defects foundMozilla TM (FF4 Beta)MozillaTM (Type Inference)Chrome V8Experiment branch
Mozilla TM (FF4 Beta)
Mozilla TM (Type Inference)
Google V8

Link
http://tinyurl.com/lfgraph-search4
http://tinyurl.com/lfgraph-search2
http://tinyurl.com/lfgraph-search3

Table 2: Links to bug reports ﬁles during ﬁeld tests. Due to security locks it might be that certain issue reports require
extended permission rights and may not be listed or cannot be opened.

on the target test suite. In the case of PHP, the orig-
inal test suite is quite complex because each test is
made up of different sections (not a single source
code ﬁle). For our proof-of-concept experiment, we
only extracted the code portions from these tests,
ignoring setup/teardown procedures and other sur-
rounding instructions. The resulting code ﬁles are
compatible with the standard test runner, so our run-
ner class does not need any new implementation.

Adding Language-dependent Information (optional)
In this step, information about identiﬁers in the
grammar and global built-in objects can be pro-
vided (e.g. taken from a public speciﬁcation). In the
case of PHP, the grammar in use provides a single
non-terminal in the lexer for all identiﬁers used in
the source code which we can add to our language
class. Furthermore, the PHP online documentation
provides a list of all built-in functions which we
can add to LangFuzz through an external ﬁle.

Adapting LangFuzz to test different languages is easy:
provide language grammar and integrate tests. Adding

language dependent information is not required but

highly recommended.

7 Threats to Validity

Our ﬁeld experiments covered different JavaScript en-
gines and a proof-of-concept adaptation to a second weak
typed language (PHP). Nevertheless, we cannot general-
ize that LangFuzz will be able to detect defects in other
interpreters for different languages. It might also be the
case that there exist speciﬁc requirements or properties
that must be met in order to make LangFuzz be effective.
Our direct comparison with jsfunfuzz is limited to a
single implementation and limited to certain versions of
this implementation. We cannot generalize the results
from these experiments. Running LangFuzz and jsfun-
fuzz on different targets or testing windows might change
comparison results.

The size and quality of test suites used by LangFuzz
during learning and mutating have a major impact on it’s
performance. Setups with less test cases or biased test
suites might decrease LangFuzz’s performance.

Both jsfunfuzz and LangFuzz make extensive use of
randomness. While some defects show up very quickly
and frequently in all runs, others are harder to detect.
Their discovery heavily depend on the time spent and
the randomness involved. In our experiments, we tried to
ﬁnd a time limit that is large enough to minimize such ef-
fects but remains practical. Choosing different time lim-
its might impact the experimental results.

For most experiments, we report the number of de-
fects found. Some of the reported bugs might be dupli-
cates. Duplicates should be eliminated to prevent bias.
Although we invested a lot of efforts to identify such du-
plicates, we cannot ensure that we detected all of these
duplicates. This might impact the number of distinct de-
fects discovered through the experiments.

8 Conclusion

Fuzz testing is easy to apply, but needs language-
and project-speciﬁc knowledge to be most effective.
LangFuzz is an approach to fuzz testing that can easily be
adapted to new languages (by feeding it with an appropri-
ate grammar) and to new projects (by feeding it with an
appropriate set of test cases to mutate and extend). In our
evaluation, this made LangFuzz an effective tool in ﬁnd-
ing security violations, complementing project-speciﬁc
tools which had been tuned towards their test subject for
several years. The economic value of the bugs uncovered
by LangFuzz is best illustrated by the worth of its bugs,
as illustrated by the awards and bug bounties it raised.
We recommend our approach for simple and effective au-
tomated testing of processors of complex input, includ-
ing compilers and interpreters—especially those dealing
with user-deﬁned input.
Acknowledgments. We thank the Mozilla, Google and
PHP development teams for their support. Guillaume
Destuynder, Florian Gross, Clemens Hammacher, Chris-
tian Hammer, Matteo Maffei, and Eva May provided
helpful feedback on earlier revisions of this paper.

References

[1] https://bugzilla.mozilla.org/show_bug.cgi?id=

610223.

13

[2] https://bugzilla.mozilla.org/show_bug.cgi?id=

626345.

[3] https://bugzilla.mozilla.org/show_bug.cgi?id=

626436.

[4] https://code.google.com/p/v8/issues/detail?id=

1167.

[23] ZALEWSKI, M.

Blog En-
try. http://lcamtuf.blogspot.com/2011/01/announcing-crossfuzz-
potential-0-day-in.html, 2011.

Announcing cross fuzz.

[24] ZELLER, A., AND HILDEBRANDT, R. Simplifying and isolating
failure-inducing input. IEEE Transactions on Software Engineer-
ing (2002), 183–200.

[5] The

phpparser

project.

http://code.google.com/p/phpparser/.

Project

website.

Appendix

Parameter

synth.prob – Probability to generate
a required fragment
instead of using a
known one.

synth.maxsteps – The maximal number
of steps to make during the stepwise ex-
pansion. The actual amount is 3 + a ran-
domly chosen number between 1 and this
value.

fragment.max.replace – The maximal
number of fragments that are replaced dur-
ing test mutation. The actual amount is a
randomly chosen number between 1 and
this value.

identifier.whitelist.active.prob
– The probability to actively introduce a
built-in identiﬁer during fragment rewrit-
ing (i.e. a normal identiﬁer in the fragment
is replaced by a built-in identiﬁer).

Default
Value

0.5

5

2

0.1

Table 3: Common parameters in LangFuzz and their de-
fault values. See Section 4.4 on how these default values
were chosen.

[6] AITEL, D. The advantages of block-based protocol analysis for

security testing. Tech. rep., 2002.

[7] GODEFROID, P., KIEZUN, A., AND LEVIN, M. Y. Grammar-
based whitebox fuzzing. SIGPLAN Not. 43, 6 (2008), 206–215.
[8] LINDIG, C. Random testing of c calling conventions. Proc.

AADEBUG. (2005), 3–12.

[9] MCPEAK, S., AND WILKERSON, D. S. The delta tool. Project

website. http://delta.tigris.org/.

[10] MILLER, B. P., FREDRIKSEN, L., AND SO, B. An empirical
study of the reliability of unix utilities. Commun. ACM 33 (De-
cember 1990), 32–44.

[11] MILLER, C., AND PETERSON, Z. N. J. Analysis of Mutation
and Generation-Based Fuzzing. Tech. rep., Independent Security
Evaluators, Mar. 2007.

[12] MOLNAR, D., LI, X. C., AND WAGNER, D. A. Dynamic test
generation to ﬁnd integer bugs in x86 binary linux programs. In
Proceedings of the 18th conference on USENIX security sympo-
sium (Berkeley, CA, USA, 2009), SSYM’09, USENIX Associa-
tion, pp. 67–82.

[13] NEUHAUS, S., ZIMMERMANN, T., HOLLER, C., AND ZELLER,
A. Predicting vulnerable software components. In Proceedings
of the 14th ACM Conference on Computer and Communications
Security (October 2007).

[14] OEHLERT, P. Violating assumptions with fuzzing. IEEE Security

and Privacy 3 (March 2005), 58–62.

[15] PARR, T., AND QUONG, R. Antlr: A predicated-ll (k) parser
generator. Software: Practice and Experience 25, 7 (1995), 789–
810.

[16] PURDOM, P. A sentence generator for testing parsers. BIT Nu-
merical Mathematics 12 (1972), 366–375. 10.1007/BF01932308.
Blog Entry.

Introducing jsfunfuzz.

[17] RUDERMAN,

J.

http://www.squarefree.com/2007/08/02/introducing-jsfunfuzz/,
2007.

[18] SHU, G., HSU, Y., AND LEE, D. Detecting communication pro-
tocol security ﬂaws by formal fuzz testing and machine learning.
In Proceedings of the 28th IFIP WG 6.1 international confer-
ence on Formal Techniques for Networked and Distributed Sys-
tems (Berlin, Heidelberg, 2008), FORTE ’08, Springer-Verlag,
pp. 299–304.

[19] SUTTON, M., AND GREENE, A. The art of ﬁle format fuzzing.

In Blackhat USA Conference (2005).

[20] SUTTON, M., GREENE, A., AND AMINI, P. Fuzzing: Brute
Force Vulnerability Discovery. Addison-Wesley Professional,
2007.

[21] TURNER, B. Random c program generator. Project website.

http://sites.google.com/site/brturn2/randomcprogramgenerator,
2007.

[22] YANG, X., CHEN, Y., EIDE, E., AND REGEHR, J. Finding and
Understanding Bugs in C Compilers. In Proceedings of the 2011
ACM SIGPLAN Conference on Programming Language Design
and Implementation (June 2011), ACM SIGPLAN, ACM.

14

