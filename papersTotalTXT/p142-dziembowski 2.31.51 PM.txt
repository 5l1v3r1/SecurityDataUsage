Private Circuits III: Hardware Trojan-Resilience

via Testing Ampliﬁcation

Stefan Dziembowski

University of Warsaw
Institute of Informatics

stefan@dziembowski.net

Sebastian Faust
University of Bochum
Fakultät für Mathematik

sebastian.faust@gmail.com

François-Xavier Standaert
Université catholique de Louvain

ICTEAM – Crypto Group
fstandae@uclouvain.be

ABSTRACT
Security against hardware trojans is currently becoming an
essential ingredient to ensure trust in information systems.
A variety of solutions have been introduced to reach this
goal, ranging from reactive (i.e., detection-based) to pre-
ventive (i.e., trying to make the insertion of a trojan more
diﬃcult for the adversary). In this paper, we show how test-
ing (which is a typical detection tool) can be used to state
concrete security guarantees for preventive approaches to
trojan-resilience. For this purpose, we build on and formal-
ize two important previous works which introduced “input
scrambling” and “split manufacturing” as countermeasures
to hardware trojans. Using these ingredients, we present
a generic compiler that can transform any circuit into a
trojan-resilient one, for which we can state quantitative se-
curity guarantees on the number of correct executions of the
circuit thanks to a new tool denoted as “testing ampliﬁca-
tion”. Compared to previous works, our threat model covers
an extended range of hardware trojans while we stick with
the goal of minimizing the number of honest elements in
our transformed circuits. Since transformed circuits essen-
tially correspond to redundant multiparty computations of
the target functionality, they also allow reasonably eﬃcient
implementations, which can be further optimized if special-
ized to certain cryptographic primitives and security goals.

1.

INTRODUCTION

While modern cryptography generally assumes adversaries
with black box access to their target primitives, the last two
decades have witnessed the emergence of increasingly pow-
erful physical attacks that circumvent this abstract model.
Side-channel analysis [23, 24] and fault attacks [10, 9] are
typical examples of such concerns, where the adversary can
respectively observe physical leakages produced by an imple-
mentation, or force it to perform erroneous computations.
In this respect, hardware trojan attacks can be viewed as
the ultimate physical attack, where the adversary can even
modify the implementations at design time, in order to hide
a backdoor that may be used after deployment. This threat
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24 - 28, 2016, Vienna, Austria
c(cid:13) 2016 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978419

has recently gained attention, since the increasing complex-
ity of electronic systems, and the ongoing trend of outsourc-
ing chip fabrication to a few specialized foundries, has made
it more and more realistic, with possibly catastrophic conse-
quences for security and safety [2, 8]. As documented in [7,
28] the attacks by a malicious manufacturer are also hard to
prevent, since they can lead to very diverse attack vectors,
with various activation mechanisms and payloads.

In this context, and looking back at the already broad lit-
erature on countermeasures against side-channel and fault
attacks, an important lesson learned is that the most ef-
fective protections usually rely on a good separation of du-
ties between well-chosen (generally physical) assumptions
and sound mathematical principles to amplify them. Taking
one emblematic example, masking improves security against
side-channel attacks by relying on the assumption that phys-
ical leakages are noisy, and by ampliﬁying this noise thanks
to secret sharing [13]. Based on the similar (physical) na-
ture of hardware trojans, it is therefore reasonable to expect
that solutions to prevent them may follow a similar path.
In this respect, and starting at the hardware level, detection
thanks to side-channels possibly ampliﬁed by some ﬁnger-
printing has been studied, e.g., in [1, 3, 26]. Very summa-
rized, such approaches are powerful in the sense that they
are in principle able to detect any type of trojan, including
purely physical ones (e.g., triggered by a temperature change
and sending secret messages through an undocumented an-
tenna), which makes them an important part of the puzzle.
But they are also inherently limited by their heuristic na-
ture and sometimes strong assumptions. For example, they
work best in presence of a golden (trusted) chip that may
not be easily available, and the eﬀectiveness of the detection
decreases when reducing the size of the trojan circuitry.

In this paper, we therefore tackle the question whether
more formal solutions can help to rule out well-deﬁned classes
of hardware trojan attacks, and achieve stronger resistance
in practice. For this purpose, we build on two important
previous works. In the ﬁrst one, Waksman and Sethumad-
havan consider digitally triggered trojan attacks [30]. A dig-
itally triggered trojan is a malicious piece of hardware that
delivers its payload when a digital input is given to the de-
vice. This can be done, for instance, through so-called “cheat
codes” or “time bombs”. The ﬁrst type of attack triggers
the malicious behavior of the trojan when a certain input
is provided to the device, while “time bombs” activate the
trojan, e.g., after the device is executed for a certain num-
ber of times. The work of Waksman and Sethumadhavan
provides ad-hoc countermeasures against these two types of

142attacks. In particular, they propose to scramble the inputs
to defeat the cheat codes and use power resets to protect
against (volatile) time bombs. In the second one, Imeson et
al. introduce the concept of “split manufacturing” for obfus-
cation, as a way to make it hard for an adversary to identify
the gates of an implementation that he would need to mod-
ify to mount his attack [20]. The main contribution of our
work is to provide generic countermeasures for signiﬁcantly
broader classes of trojan attacks, and to provide a formal
framework in which these countermeasures can be analyzed
and concrete security bounds can be derived. We describe
our technical contribution in more detail below.

Types of hardware trojans. Similar to Waksman and
Sethumadhavan, we consider a setting where the produc-
tion of a device is outsourced to a potentially malicious
hardware manufacturer. The manufacturer produces a set
of devices D1, . . . , D(cid:96) that supposedly implement function-
alities Γ1, . . . , Γ(cid:96), but may contain trojans and react ma-
liciously. As in [30] we restrict the type of malicious be-
havior to hardware trojans that are digitally triggered, such
as cheat codes or time bombs. Besides formally modeling
such digitally triggered trojans, we also extend the model
of Waksman and Sethumadhavan by not only considering
volatile time bombs (i.e., where the clock needs to be pow-
ered) but also non-volatile ones (which may become hard to
detect in highly integrated electronic systems).

The trojan protection schemes. To protect against dig-
itally triggered hardware trojans we introduce so-called tro-
jan protection schemes. A trojan protection scheme consists
of two components: a circuit transformation TR and a tester
T. The transformation describes a method to compile an
arbitrary functionality described as an arithmetic circuit Γ
into a protected speciﬁcation consisting of a trusted master
circuit M and a set of circuits Γ1, . . . , Γ(cid:96). We assume that M
has to be implemented in a trusted way and its production is
not outsourced to the malicious hardware manufacturer A,
while the devices Di are produced by A. To obtain a stronger
result, we require that M is as simple as possible. For our
concrete construction M will consist of a couple of wires
and a small number of simple gates – in particular, the size
(counted as the number of gates) of M is independent of the
size of Γ. The implementation of our transformed circuits
therefore follows the same “split manufacturing” principles
as introduced by Imeson et al. [20]. The second component
of the trojan protection scheme is a tester T. The tester ver-
iﬁes if the devices Di correctly implement the functionality
Γi. Such tests typically involve whether the input/output
behavior of Di corresponds to the input/output behavior of
the honest speciﬁcation Γi.

Robustness of trojan protection schemes. The main
security guarantee that our trojan protection scheme shall
achieve is called robustness. Informally, robustness is mod-
eled by a game with two phases. First, in the testing phase
the tester T checks whether the devices Di implement the
corresponding speciﬁcation Γi. If the testing is passed the
adversary can in a second phase interact with the device
composed of the trusted master M and the devices Di. Ro-
bustness guarantees that for the same inputs, the outputs
produced in the second phase by the device are identical to
the outputs produced by the honest speciﬁcation Γ. Ro-
bustness is parameterized by two parameters t and n, where
t denotes the number of tests carried out by T and n is

the number of executions for which the output produced by
the device has to be identical to the honest speciﬁcation Γ.
Typically, for our constructions we require t > n.

A trojan protection scheme for any functionality Γ.
Our main contribution is the design of a trojan protection
scheme that achieves robustness for any functionality Γ. We
next give a high-level description of our trojan protection
scheme omitting several technical details.

As a ﬁrst step, the transformation compiles the speciﬁca-
tion Γ into three so-called mini-circuits (Γ0, Γ1, Γ2). These
mini-circuits emulate Γ using a passively secure 3-party pro-
tocol, where the inputs to (Γ0, Γ1, Γ2) are secret-shared by
the trusted master circuit M.

The ﬁrst observation in order to achieve robustness is that
if the mini-circuits Γi follow exactly the secure 3-party proto-
col, then they do not learn anything about the user provided
input. Hence, a malicious user is hindered in activating the
trojan by choosing a special input. Of course, once Γi gets
implemented by A nothing stops A to produce devices Di
that do not follow the protocol, e.g., by transmitting their
shares to the other devices. Such a behavior will, however,
be detected with good probability during the testing phase.
The above only prevents activation of the trojan by a ma-
licious input, but does not deal yet with an activation via
time-bombs. For instance, assume that the trojan is acti-
vated only after the (t + 1)-th execution. If we test devices
only for t times, the malicious behavior will not be detected
and achieving robustness is infeasible. To circumvent this,
we randomize the number of tests t(cid:48), where t(cid:48) is drawn uni-
formly from {1, . . . , t}. Since (i) the total number of exe-
cutions after testing is bounded by n and (ii) test and real
executions look the same from the device’s point of view (due
to the 3-party computation), we can bound the probability
that malicious behavior is triggered by time bombs.
Unfortunately, the above gives only a weak security bound.
It is, however, easy to amplify security by letting A produce
λ independent copies (D0
λ), so that
(cid:96) = 3λ, where each such tuple is tested for a random and
independent number of times ti. In our ﬁnal construction,
the master M then runs each of the tuples on independent
input sharings and takes the majority of the results to pro-
duce the ﬁnal output with good robustness. Concretely, we

can guarantee correct execution with probability(cid:0) n

(cid:1)λ/2.

1), . . . , (D0

1, D1

1, D2

λ, D1

λ, D2

t

Applications of trojan protection schemes. The re-
quirement that t > n can be viewed as a limitation of our
work, but we argue in Section 3 that this condition is in
fact necessary for testing-based security against hardware
trojans. Hence, such schemes are only applicable in settings
where there is an a-priori bound on the number of times the
device is used. Such bounded number of executions natu-
rally occurs when a user manually interacts with a device.
Since testing can be automatized it is then feasible to carry
out millions of test cases, while after deployment many de-
vices are used only a few thousand times. There are many
examples of such settings (e.g., opening doors). Their rele-
vance naturally increases with the sensitivity of the data to
protect and not too limited cost constraints, such as elec-
tronics in planes used for starting and landing (which have
natural restrictions on the number of executions). We stress
that nothing prevents the master circuit M to count the
number of runs, issue a warning when the executions limit
is reached, and then to re-perform a testing phase.

143Implementation issues. Despite our primary focus is on
the genericity and formal guarantees of the proposed coun-
termeasure against hardware trojans, we also take imple-
mentation issues into account in our developments. In this
respect, we ﬁrst limit the use of trusted components to some
routing and a couple of gates, as in [20]. We argue in Sec-
tion 3 why a minimum complexity (i.e., the presence of gates
in M) is necessary for testing-based security against hard-
ware trojans. Next, and as far as performances are con-
cerned, the main eﬃciency bottleneck in our trojan protec-
tion scheme is the use of a passively secure 3-party compu-
tation protocol. We discuss the time and area overheads it
implies for a mainstream cryptographic functionality such
as a standard block cipher in Section 5.2. And we conclude
the paper by showing that better eﬃciency can be achieved
if we aim for protecting speciﬁc cryptographic primitives. In
particular, we give constructions for a PRG and a MAC that
only increase the complexity by a linear factor (cid:96) (compared
to the unprotected scheme), while guaranteeing security ex-
cept with probability O(2−(cid:96)). Notice that these schemes
also have two additional beneﬁts compared to our generic
solution: ﬁrst, except for an initial secret sharing of the in-
puts their execution does not require the sometimes costly
generation of pseudorandomness, and second they require
almost no interaction between the sub-devices.
Related works. In a separate line of papers, Bayer and
Seifert [6] and very recently Wahby et al. and Ateniese et
al. [29, 5] consider a setting where an untrusted ASIC proves,
each time it performs a computation, that the execution is
correct. These papers build on a large literature on veriﬁable
computation, probabilistically checkable proofs and other,
related topics. Compared to our work, such approaches and
techniques correspond to a diﬀerent tradeoﬀ between secu-
rity and trust. On the one hand, they cover even broader
classes of hardware Trojans and achieve security for an arbi-
trary number of executions (unlike us who restrict the num-
ber of executions a-priori). On the other hand, they require
a trusted veriﬁer which is typically more complex than our
master circuit that does only routing and and uses a small
number of gates.1 These works also aim at diﬀerent goals
than ours. Namely, whenever a proof of correct execution is
not veriﬁed in [29, 5], the system stops. By contrast, we can
guarantee a number of correct executions and therefore are
also resistant against denial-of-service attacks.

The work of Haider et al. [19] also shares similarities with
ours, and provides a formal analysis of trojan detection using
pre-silicon logic testing tools.

Eventually, our constructions follow the seminal investi-
gations of Ishai et al. who introduced circuit transformation
in the ﬁeld of side-channel and fault attacks [21, 22]. Their
results on “Private Circuits” (I and II) motivated us to look
at generic compilers for trojan-resilient circuits, which is a
natural next step in the study of physical adversaries against
cryptographic hardware. Conceptually, the principles we ex-
ploit in our trojan protection schemes are also close to mask-
ing against side-channel attacks. Namely, masking exploits
secret sharing and multiparty computation in order to am-
plify the impact of noise in leaking cryptographic implemen-
tations. Similarly, we exploit these techniques to amplify the
impact of testing against hardware trojans.
1Comparing the eﬃciencies of prover sides is more challeng-
ing since application-dependent, and is therefore an inter-
esting scope for further research.

2. TROJAN PROTECTION SCHEMES
2.1 The model of computation

2.1.1 The circuit speciﬁcation
Computation carried out by an algorithm is abstractly
deﬁned via a speciﬁcation. We model the speciﬁcation as a
circuit Γ, which is represented by a Directed Acyclic Graph
(DAG). The set of vertices of the graph represents the gates
of the circuit, while the edges are the wires connecting the
gates. The wires in the circuit carry elements from a ﬁnite
ﬁeld F, while the gates carry out the operations in the ﬁnite
ﬁeld or take special task such as storing values. The sim-
plest case is when F is the binary ﬁeld, in which case the
wires carry bits, and the gates, for instance, represent the
Boolean operations AND (next denoted by (cid:12)) and XOR
(next denoted by ⊕). For simplicity, all the gates are as-
sumed to have at most fan-in two. On the other hand, gates
may have arbitrary fan-out, where we assume that all out-
put wires carry the same value. We will also consider the
arithmetic circuits, where F is a larger ﬁeld, and the gates
represent the corresponding arithmetic operations.

In addition to the standard Boolean/arithmetic gates, we
allow the speciﬁcation Γ to contain two additional gates: the
randomness gates rand and (volatile and non-volatile) mem-
ory gates. The randomness gate has no incoming wires but
can have an arbitrary number of outgoing wires, which carry
a random element from F. One may think of rand as a gate
producing true randomness. Next, the non-volatile memory
gates (next called registers), are used to store the results of
the computation’s diﬀerent (clock) cycles and only maintain
their state when the chip is powered. Registers have a single
incoming wire and an arbitrary number of outgoing wires.
They can be placed everywhere in the circuit, but we require
that each cycle of Γ contains at least one register. Eventu-
ally, non-volatile memory gates (next called memory gates
for short) play a similar role as volatile ones, but maintain
their state even if the chip is not powered.

To complete the description of the circuit, we also need to
explain how it processes inputs/outputs. The circuit takes
inputs (cid:126)x ∈ Fα and the outputs (cid:126)y ∈ Fβ as a result of the
computation are delivered to the user. One may view them
as wires carrying the inputs and outputs, respectively, that
are connected to the “outside world” of the circuit. For a
circuit that takes as input (cid:126)x and produces output (cid:126)y we write
(cid:126)x ← Γ((cid:126)y) and call it a run of Γ or a round, which usually
takes several clock cycles to be executed.

Beside the public inputs/outputs, the circuit also may
keep a (secret) state between runs of Γ. The secret state
of Γ is initially set through the Init operation and kept in
non-volatile memory gates. We write Init(Γ, (cid:126)m) when the
initial state of Γ is set to (cid:126)m. Notice that the state of Γ
may change via the public inputs (cid:126)x, in which case we say Γ
is stateful. Otherwise, if the state is only written once via
the Init procedure, we say that Γ is stateless. For a com-
putation on input (cid:126)x and an initial state (cid:126)m ∈ Fs, we write
(cid:126)x ← Γ[ (cid:126)m]((cid:126)y). If Γ has been run for many rounds then (cid:126)m
may already have been changed.

2.1.2 Circuit compilers
The goal of the circuit compiler (or transformer) TR =
(TR1, TR2) is to compile a speciﬁcation described as a cir-
cuit Γ into a protected speciﬁcation Γ(cid:48). We write for the

144compilation process: Γ(cid:48) ← TR1(1k, (cid:96), Γ), where k is the
computational security parameter (e.g., used for the PRG in
our following generic construction) and (cid:126)m(cid:48) ← TR2(1k, (cid:96), (cid:126)m)
for compiling the initial state. In the following, we will of-
ten abuse notation and omit to explicitly denote the com-
piled state by (cid:126)m(cid:48) since in our construction it will be just a
longer (secret-shared) vector. Also, we will sometimes omit
to mention explicitly the parameter (cid:96), when it is clear from
the context. The speciﬁcation of Γ(cid:48) consists of two parts.
First, a set of sub-circuits Γ1, . . . , Γ(cid:96) and second, a so-called
master circuit M.2 The role of the master circuit M is to
manage the communication between the sub-circuits Γi and
the user of these circuits. While these sub-circuits can be
constructed using the above described gates, for ease of no-
tation we choose to describe them with an additional feature
for communication. That is, we allow the sub-circuits Γi to
communicate with M and vice versa. To this end, we intro-
duce commands having the form (cmd, val), where cmd is a
label denoting the command that shall be executed and val
is an accompanied element in F (or a vector thereof). We
will consider the following types of commands:

1. The command ((send, j), (cid:126)x) is sent by Γi to M to spec-

ify that Γi wants to send message (cid:126)x to the circuit j.

2. The command (in, (cid:126)x) is sent by M to Γi to specify that

Γi receives message (cid:126)x as input.

3. The command (out, (cid:126)y) is sent by Γi to M to specify

that Γi’s output is ready and worth (cid:126)y.

The evaluation of the sub-circuits Γ1, . . . , Γ(cid:96) with master M
on input (cid:126)x with initial state (cid:126)m producing output (cid:126)y will next
be written as (cid:126)y ← (M ⇔ Γ1, . . . , Γ(cid:96))[ (cid:126)m]((cid:126)x), where (cid:126)m is the
initial compiled state. One may think of (M ⇔ Γ1, . . . , Γ(cid:96)) as
a circuit composed of the sub-circuits Γi and M, where the
composition is speciﬁed by the communication commands
between Γi and M.

In the following we will often need to describe the view of a
circuit Γi. The view of Γi includes all command/value pairs
denoted by (cmd, val) that Γi receives/sends from/to M. The
view of Γi is denoted by View(Γi[ (cid:126)m]((cid:126)x)) and contains tuples
of the form (cmd, val). Notice that the view also includes
the inputs/outputs given by M to Γi.

The simplest property that we require from the transfor-
mation is correctness. That is, for all (cid:126)m, (cid:126)xi it holds that
outputs produced by Γ on initial state (cid:126)m with input (cid:126)xi are
identical to the outputs produced by Γ(cid:48) on initial state (cid:126)m(cid:48)
with input (cid:126)xi. Notice that if Γ was a randomized circuit
(i.e., it uses rand gates), then we require that the output
distributions are computationally indistinguishable.

The second property that we require is robustness against
malicious manufacturers, which we introduce in the next sec-
tion. To look ahead, we will typically let the manufacturer
produce devices Di that take the role of Γi, while the master
M is required to be implemented honestly. Of course, due
to this assumption, the latter has to be as simple as possi-
ble (in our case typically M will only require wiring devices
together and a few very basic operations).
2.2 Security against malicious manufacturers
Consider a circuit speciﬁcation Γ with initial state (cid:126)m and
let (Γ(cid:48), (cid:126)m(cid:48)) ← (TR1(1k, (cid:96), Γ), TR2(1k, (cid:96), (cid:126)m)), where Γ(cid:48) =
(M, (Γ1, . . . , Γ(cid:96))). We are interested in a setting where a
2Notice that Γ1, . . . , Γ(cid:96) and M are described using the circuit
model from above and therefore may include memory cells.

potential malicious manufacturer A gets as input the speci-
ﬁcations (Γ1, . . . , Γ(cid:96)) and produces a set of devices D1, . . . D(cid:96),
where Di supposedly implements some functionality Γi. A
device Di takes some input (cid:126)x and produces an output (cid:126)y. In
order to compute (cid:126)y from the input (cid:126)x the device Di may com-
municate with the master circuit M, which is implemented
honestly. To this end, it can send and receive commands of
the form (cmd, val) to/from M. While the devices Di can in
principle implement any functionality (since they are built
by the malicious hardware manufacturer), we require that
an implementation of Di can be simulated using our circuit
model above, as formalized by the following assumption.

Assumption 1. Let Di be the devices output by A. We
require that there exists (possibly probabilistic) circuit speciﬁ-

cations(cid:101)Γi such that for all public inputs (cid:126)x ∈ Fα and any ini-
tial state (cid:126)m ∈ Fs, we have View(Di[ (cid:126)m]((cid:126)x)) ≡ View((cid:101)Γi[ (cid:126)m]((cid:126)x)).

Informally, the assumption says that as long as a trojan
attack can be modeled by a (possibly probabilistic) circuit,
then the attack is within our security model. Note that this
allows for fairly general trojan attacks. For instance, we will
not make any assumption of the computational complexity
of the trojan other than it was produced by a PPT adversary
A. This, e.g., means that it can be more complex than the
computation carried out by the honest speciﬁcation Γi. We
note also that some restriction on the power of the trojan
attack is necessary. For instance, if the trojan embeds an
antenna into the device that sends secret data via a side-
channel to the attacker, then security is hard to achieve.
Looking ahead, at a technical level Assumption 1 is also
crucial for the security proof and shows up in Theorem 1.3
We discuss the plausibility of Assumption 1 and the attacks
that are (not) incorporated in our model in Section 5.4.
2.2.1 Testing
Once the devices Di have been produced by the (mali-
cious) manufacturer, they are tested by a PPT tester T.
The goal of T is to verify whether each Di implements its
corresponding functionality given by the circuit speciﬁcation
Γi. We consider black-box testing. That is, T can specify
the inputs of Di and communicate with Di over the speciﬁed
interface. To this end, the tester will typically take the role
of the master M, i.e., the tester can run the manufactured
devices Di on chosen inputs and verify whether the results
correspond to the results produced by the honest function-
ality Γi. Notice that these tests typically also include the
veriﬁcation of the communication with M. We will write
b ← TD1(.),...,D(cid:96)(.)(1k, Γ), where T can interact with the de-
vices Di via the communication commands and at the end
of the test outputs a bit b indicating whether the test has
passed or failed. We call the tester T t-bounded, if each of
the Di is run for at most t times.
2.2.2 Trojan protection schemes
A trojan protection scheme Π := (TR, T) consists of the
circuit transformation TR and the testing algorithm T. We

3Concretely, in our construction the devices Di supposedly
run a passively secure 3-party protocol. At some point in
the proof we want to replace the physical devices Di by some
abstract description of a circuit ˜Γi (which is not necessarily
the same as Γi) that emulates the malicious behavior of Di.
At this point in the proof we need Assumption 1.

145Game ROBΠ(A, pub, Γ, (cid:126)m):
((M,{Γi}i), (cid:126)m(cid:48)) ← (TR1(1k, (cid:96), Γ), TR2(1k, (cid:96), (cid:126)m))
{Di}i ← A(1k, (M,{Γi}i))
Set the initial state of the devices Init({Di}i, (cid:126)m(cid:48))
If TD1(.),...,D(cid:96)(.)(1k, (M,{Γi}i)) = false then return 0
(cid:126)x1 ← A(1k)
For i = 1 to n repeat:

(cid:126)zi ← (M ⇔ D1, . . . , D(cid:96))[ (cid:126)m(cid:48)]((cid:126)xi)
(cid:126)yi ← Γ[ (cid:126)m]((cid:126)xi)
If (cid:126)yi (cid:54)= (cid:126)zi then return 1
(cid:126)xi+1 ← A(1k, (cid:126)zi)

Return 0.

Figure 1: The robustness game ROBΠ.

model security of the trojan protection scheme Π against
a malicious manufacturer by a robustness game denoted by
ROBΠ, given in Figure 1. In the game, we ﬁrst run the trans-
formation to obtain the speciﬁcation of the protected circuit
((M,{Γi}i), (cid:126)m). Next, the speciﬁcation is given to the ma-
licious manufacturer A who outputs a set of devices {Di}i.
The devices are tested by T, and if the testing succeeds then
A may interact with (cid:126)zi ← (M ⇔ D1, . . . , D(cid:96))[ (cid:126)m]((cid:126)xi) by spec-
ifying an input (cid:126)xi and receiving the output (cid:126)zi. We say that
A wins the game iﬀ after the testing has succeeded, he man-
ages to produce an output (cid:126)zi that diﬀers from the output
(cid:126)yi of a correct computation on input (cid:126)xi, i.e., (cid:126)yi ← Γ[ (cid:126)m]((cid:126)xi).
Note that for our constructions, we will require that the
number of tests t done by T is larger than the number of
executions n. We state the security properties of a trojan
protection scheme as:

Deﬁnition 1. Let (cid:96), n, t, and k be some natural parame-
ters. A trojan protection scheme Π = (TR, T) is (t, n, )-
trojan robust if the following two conditions hold:

1. The tester T is t-bounded,
2. For any manufacturer A, any circuit Γ and any initial

state (cid:126)m we have:

Pr[ROBΠ(A, (cid:96), n, t, k, Γ, (cid:126)m) = 1] ≤ ,

where the probability is taken over the internal coin
tosses of A and the coin tosses of the game ROBΠ.

To simplify the notation in the sequel we will use a symbol
pub as a shorthand for the tuple consisting of “public param-
eters” in ROB, i.e. we will set pub := ((cid:96), n, t, k), and write
ROBΠ(A, pub, Γ, (cid:126)m).

3.

IMPOSSIBILITY RESULTS

We now discuss some inherent limitations of the testing
techniques presented in the previous section. First, we argue
that in most of the realistic applications the maximal num-
ber t of testing rounds should be much larger than the num-
ber n of times that the device will be used. For this purpose,
consider a single device D and suppose that the malicious
manufacturer designed it in such a way that it behaves as
its speciﬁcation requires, except with probability  (whose
value we will determine later). More precisely let Badi de-
note the event that in the ith round of its life (during testing
or the real execution) D behaves wrongly (for example:
it
starts to produce wrong results, or it terminates). Assume
that the Badi’s are independent, and Pr(Badi) = .

The probability that this malicious actions are not de-
tected during testing is equal to Pr(¬Bad1 ∧ ··· ∧ ¬Badt0 ),
where t0 ≤ t is the number of rounds of test. This, clearly,
is at at least equal to (1 − )t. Similarly the probability
that a Bad event happened during one of the n rounds of
execution is equal to 1 − (1 − )n. Hence the probability p
that D passed the tests and failed during the execution is
at least equal to (1 − )t · (1 − (1 − )n). Now suppose that
the adversary sets  := 1 − (t/(n + t))1/n. Then p is at least
(t/(n + t))t/n · n/(n + t) = (1 + n/t)−t/n · n/(n + t), which
is at least equal to n/(e · (n + t)), where e is the base of the
natural logarithm (this is because (1 + n/t)−t/n ≥ e−1).
This in particular means that if t is small then with very
good probability (at least n/(e · (n + t))) the adversary’s
device behaves correctly during the testing, and incorrectly
during the real-life execution. This shows that in reality we
will usually need to have t (cid:29) n if we want to get high as-
surance that the device will not fail during the execution.
Also, since this probability is inversely proportional to the
number t of tests, thus, intuitively, to obtain error proba-
bility smaller than O(n−c) (for some c) we need to have at
least c devices D in the system.

This last statement is of course informal, since in order
to formalize it, we would need to restrict the power of the
master circuit (in principle every computation can be done
in a perfectly secure way if it is performed by the trusted
master). For this purpose, we next state some simple ob-
servations regarding the necessary complexity of the master
circuit. First, note that the above observations imply that,
in order to get any security beyond the “n/(e · (n + t))” bar-
rier, none of the Di gadgets can be “directly connected to
the output”, i.e., the master circuit M cannot just forward
the outputs from Di as its own output (without performing
any computation on this value). This is because the adver-
sary can make such “unprocessed” output to be wrong with
probability n/(e · (n + t)). It justiﬁes why we always need
some kind of “output processing” (which, in our case, will
be handled by a majority gate).

A similar fact can be shown about the input processing,
i.e., we can prove that in most of the cases no M can pass
its input directly to one of the Di gadgets. Observe, that
the above fact certainly cannot hold for all functionalities
Γ. For illustration, suppose that Γ ignores its input (e.g., it
is a pseudorandom generator whose output depends only in
the initial state and does not depend on the inputs). Then
it can be implemented by (M ⇔ D1, . . . , D(cid:96)) such that M
sends its inputs directly to the some “dummy” Di’s that do
not perform any actions. To be more formal, let us say
that a circuit Γ is simple if it contains no gates (i.e. it has
only wires). We say that a circuit Γ can be simpliﬁed if
for every initial state (cid:126)m there exists a sequence {Γi}i=1 of
simple circuits which, for every sequence {(cid:126)xi}i of inputs Γ
with initial state (cid:126)m and rounds inputs {(cid:126)xi}i, produces the
same output as {Γi}i=1 on inputs {(cid:126)xi}i (where in round i we
apply Γi to (cid:126)xi). Intuitively, a circuit, cannot be simpliﬁed
if it performs some non-trivial operations on its input.
We now show that every such a circuit Γ cannot be simu-
lated by a circuit Γ(cid:48) = (M ⇔ D1, . . . , D(cid:96)), where M is simple
(and in particular, every circuit with simple M can be bro-
ken with probability close to 1 for n that does not depend on
t). We consider circuits Γ that do not have any randomness
gates, but our argument can be generalized also to the case
of circuits with random gates.

146Lemma 1. Consider a trojan protection scheme Π = (TR,
T). Suppose it produces as output only circuits (M,{Γi}i)
such that M is simple. Let Γ be a circuit that cannot be
simpliﬁed, and suppose (cid:96)(k) is the number of sub-circuits Γi
that Π produces on input (Γ, 1k). Then the scheme Π is
not (t, n, )-trojan robust for any t, k, n = ((cid:96)(k) + 1) · k, and
 < 1 − ((cid:96)(k) · t + 1) · |F|−k.

The proof of the lemma appears in the extended version of
this paper [15]. Summarizing, the above statements high-
light that the complexity of both the testing phase and the
master circuits in the following constructions (measured in
number of tests and gates) is essentially necessary.

4. TROJAN RESILIENT CIRCUITS

To simplify our analyses, we ﬁrst consider the case when
Γ is deterministic and does not update its initial state. This
means that once the state has been initialized to (cid:126)m it is never
changed by the computation of Γ. AES implementations are
an example of such circuits. In Section 4.5 we then discuss
how to extend our results to circuits that update their state
(e.g., stream cipher) and are probabilistic.
4.1 Our basic construction

i , Γ2

i , Γ1

The compiler TR takes as input a description of a (bi-
nary/arithmetic) circuit Γ and outputs λ sub-circuits Γi :=
i ) for i ∈ [λ] and the master circuit M. Each sub-
(Γ0
circuit consist of three mini-circuits so that (cid:96) = 3λ. While
the λ sub-circuits operate independently from each other
(i.e., there is no communication between them), the mini-
circuits of each sub-circuit are connected through M.
The processing of an input (cid:126)x ∈ Fα with an initial secret
input (cid:126)m resulting in an output (cid:126)y ∈ Fβ proceeds in three
phases: (i) the input pre-processing phase, (ii) the compu-
tation phase and (iii) the output post-processing phase. The
bulk of the computation is carried out in phase (ii), while
phase (i) and (iii) are carried out by the master M. Since the
implementation of M has to be trustworthy, we will minimize
the work of M. In particular, we require that the number
of gates (but not the number of wires) used by M is inde-
pendent of the number of gates used by Γ; instead, it will
depend only on Γ’s input size α and output size β. The over-
all structure of the speciﬁcation of the transformed circuit
Γ(cid:48) is given in Figure 2.

In the pre-processing phase on input (cid:126)x the master circuit
M produces λ additive 2-out-of-2 secret sharings of (cid:126)x. More
formally, it proceeds as follows:

1. Repeat the following for i ∈ [λ]:

(a) Sample (cid:126)ri ← Fα using α rand gates.
(b) Compute (cid:126)si = (cid:126)x − (cid:126)ri.

2. Output {((cid:126)ri, (cid:126)si)}i.

Note that such a pre-processing does not imply that the
master cricuit needs to generate trusted randomness. As
discussed in Section 6.1, we can use an eﬃcient construction
of trojan-secure PRG for this purpose.

i , Γ2

In the computation phase, each triplet of sub-circuits Γi :=
(Γ0
i , Γ1
i ) implements computation of the circuit Γ using
a passively secure 3-party protocol. While in principle any
construction of a passively secure 3-party protocol will work,
we chose to present a particular protocol which is well-suited

Figure 2: Transformed circuit (global view).

for our application and allows eﬃcient hardware implemen-
tations.4 In our construction the λ sub-circuits Γi carry out
exactly the same computation, where Γi uses the public in-
put tuple ((cid:126)ri, (cid:126)si). Since the computation of each sub-circuit
is identical, to ease notation, in the following we omit to
explicitly mention the index i. The triplet (Γ0, Γ1, Γ2) eval-
uates the circuit Γ gate-by-gate. That is, each gate in Γ
is processed by the sub-circuit (Γ0, Γ1, Γ2) running a secure
3-party protocol emulating the operation of the gate in Γ.
In the computation phase, the role of the master M is re-
stricted to forward commands between mini-circuits. In par-
ticular, to initiate the computation of (Γ0, Γ1, Γ2) the master
M sends the following command to Γi:

1. (in, (cid:126)r) to Γ1 and (in, (cid:126)s) to Γ2, respectively.
2. (in, ∅) to Γ0. Notice that this means that Γ0 is inde-

pendent of the inputs of the computation.

On receiving the in command, the mini-circuits (Γ0, Γ1, Γ2)
will then run one of the protocols shown in Figure 3 depend-
ing on the type of gates in Γ. The basic invariant is that
(Γ0, Γ1, Γ2) guarantee that for a gate g in Γ that outputs
c, we have that at the end of the protocol Γ1 produces c1
while Γ2 computes c2 such that (c1, c2) represents a ran-
dom sharing of c. In other words: each value on a wire in
Γ is shared between Γ1 and Γ2. The mini-circuit Γ0 is in-
volved only for computing the ﬁeld multiplication by provid-
ing correlated randomness. To generate randomness, Γ0 will
use an implementation of a secure pseudorandom generator
prg : Fs → Fκ.5 To this, end it holds an initial state (cid:126)w ∈ Fs
in its internal memory gates and computes ( (cid:126)w, (cid:126)y) = prg( (cid:126)w).
4In principle, for our application a passively secure 2-party
protocol (e.g., [14], Chapter I, Section 4) would suﬃce. How-
ever, the security would need to rely on computational as-
sumptions for the OT protocols, which would result in a
less eﬃcient scheme. In the following, the OT protocol is
therefore performed by a third party, which samples an “OT-
tuple”, i.e., correlated randomness that is later used by the
two other parties to perform secure computation.
5Notice that common implementations of PRGs do not out-

1471. Transformation for ﬁeld addition, i.e., (cid:126)a(cid:98)⊕(cid:126)b = (cid:126)c: Γ1 holds the shares (a1, b1) that either were received from M via an in
2. Transformation for multiplication, i.e., (cid:126)a(cid:98)(cid:12)(cid:126)b = (cid:126)c: This involves the mini-circuits (Γ0, Γ1, Γ2) and the driver circuit

command, or resulted as an output from a previous gate. Similarly, Γ2 holds (a2, b2). Given these inputs Γ1 computes
c1 = a1 ⊕ b1 and Γ2 computes c2 = a2 ⊕ b2.

Evaluating the gates g of Γ by (Γ0, Γ1, Γ2)

to forward commands between the circuits Γi. To keep the description simple, we will not explicitly describe the
computation carried of by M as it only forwards commands. Initially, Γ1 holds (a1, b1) and Γ2 has (a2, b2). They proceed
as follows:
(a) Run jointly (u, v) ← MultShares(a1, b2) and (u(cid:48), v(cid:48)) ← MultShares(b1, a2) (see description below).
(b) Mini-circuit Γ1: Compute c1 = a1 (cid:12) b1 ⊕ u ⊕ u(cid:48) and output c1.
(c) Mini-circuit Γ2: Compute c2 = a2 (cid:12) b2 ⊕ v ⊕ v(cid:48) and output c2.

Sub-circuit (u, v) ← MultShares(x, y)

Initially, Γ1 holds x and Γ2 holds y. At the end Γ1 holds u and Γ2 has v such that v = x (cid:12) y ⊕ u and u ∈ F deﬁned below.
1. Mini-circuit Γ0[ (cid:126)w]: Γ0 has memory cells to store the internal state of the PRG (cid:126)w. Notice that Γ0 uses the contents
of its memory cells (cid:126)w and computes ( (cid:126)w, (u1, u2, u3, u4)) = prg( (cid:126)w), where the output (cid:126)w represents the secret output. It
then computes u = u3 ⊕ u4 (cid:9) u1 (cid:12) u2 and sends ((send, 1), (u, u2, u3)) and ((send, 2), (u1, u4)) to M.

2. Mini-circuit Γ2: on receiving ((send, 2), (u1, u4)) from M, compute z = y ⊕ u1 and send ((send, 1), z) to M.
3. Mini-circuit Γ1: on receiving ((send, 1), (u, u2, u3, z)) from M, compute e = (z (cid:12) x) ⊕ u3 and f = x ⊕ u2. Send
4. Mini-circuit Γ2: on receiving ((send, 2), (e, f )) from M, compute v = u4 ⊕ e (cid:9) f (cid:12) u1.

((send, 2), (e, f )) to M.

Figure 3: The computation of the gates by the sub-circuits (Γ0, Γ1, Γ2). All operations are ﬁeld operations in
the underlying ﬁeld F. The MultShares circuit is used as sub-circuit in the ﬁeld multiplication operation, where
the latter is also shown in Figure 6 explaining the communication in further detail.

Here, (cid:126)w is the internal state of the PRG and (cid:126)y is the out-
put. For our concrete construction, we require κ := s + 4.
Notice that for security it does not matter how prg is imple-
mented. Hence we misuse notation and let prg denote the
circuit computing the PRG. Finally notice that to simplify
the description all operations described in Figure 3 have fan-
out 1. An extension to larger fan-out is trivially possible by
just fanning out this single output.
Finally, in the output post-processing phase, we have that
for each i ∈ [λ] the sub-circuit Γ1
i sends
(out, (cid:126)di) to M. Here, ((cid:126)ci, (cid:126)di) are λ independent sharings of
the output (cid:126)y of Γ.
On receiving the out commands M proceeds as follows:

i sends (out, (cid:126)ci) and Γ2

1. For each i ∈ [λ] compute (cid:126)yi = (cid:126)ci + (cid:126)di.
2. Output MAJ((cid:126)y1, . . . , (cid:126)yλ), where MAJ returns the most
common value that occurs as an input; if two or more
inputs are most common, then it outputs the ﬁrst one
of them. Notice that MAJ can easily be implemented
using only standard arithmetic gates.

i , Γ2

the original circuit Γ). Of course, the memory cells may
be updated by the circuits (Γ1
i ) during the runs of the
circuit. In the following description, we will often neglect
mentioning the initial state explicitly as essentially it can be
treated in the security analysis as part of the public inputs
(this makes the adversary only stronger).
4.2 Correctness

Correctness of our construction follows by observing that
the output of a transformed operation satisﬁes the invariant
that it is a sharing of the corresponding value on the wire in

Γ. The only non-trivial operation is the transformation (cid:98)(cid:12) of

the ﬁeld multiplication, which requires interaction between
the mini-circuits. Hence, it results in connecting wires be-
tween the diﬀerent Γj. We show that the transformation for
the multiplication gate achieves correctness.

Lemma 2. For any (cid:126)a,(cid:126)b ∈ F2 we have c1 ⊕ c2 = (a1 ⊕

a2) (cid:12) (b1 ⊕ b2), where (c1, c2) = (cid:126)a(cid:98)(cid:12)(cid:126)b is the output of the

transformed multiplication operation.

We additionally need to describe how to handle the initial
secret state (cid:126)m. The initialization function Init produces for
each sub-circuit i ∈ [λ], a secret sharing of (cid:126)m as (cid:126)oi ← Fs
and (cid:126)pi = (cid:126)m − (cid:126)oi, and stores (cid:126)oi in the internal memory cells
of Γ1
i , respectively.
Notice that this implies that in total we require 2λs mem-
ory cells in the transformed speciﬁcation (compared to s in

i and (cid:126)pi in the internal memory cells of Γ2

put random ﬁeld elements, however, it is easy to do such a
mapping in practice and we believe that the most common
application of our techniques are binary circuits anyway, in
which case we may just use AES in counter mode.

The proof of this lemma appears in the extended version
of this paper [15]. To complete the correctness analysis,
observe that each of the sub-circuits Γi produces a shar-
ing of an output (cid:126)yi. When M receives the out command it
will re-combine the two shares to recover (cid:126)yi and compute
MAJ((cid:126)y1, . . . , (cid:126)yλ). Due to the correctness of the computation
phase all of them will be identical, and MAJ((cid:126)y1, . . . , (cid:126)yλ) out-
puts the correct result (cid:126)y ← Γ((cid:126)x). It is straightforward to
extend the correctness analysis to circuits that have secret
inputs/outputs. We can just view the secret inputs/outputs
as an additional public input/output of the circuit.

1484.3 Testing circuits

Besides the circuit transformation that outputs a pro-
tected speciﬁcation that supposedly is implemented by the
malicious manufacturer, the trojan protection scheme also
deﬁnes a tester T. The description of T is public, and uses
a probabilistic approach to defeat the malicious manufac-
turer A. Consider the (potential) malicious implementation
{Di}i ← A(1k, (M,{Γi}i)) output by A. Following Fig-
ure 2, our construction consists of λ sub-devices, each of
them made of three mini-devices (D0
i , D1
i ) which suppos-
edly implement the mini-circuits Γj
i . As the sub-devices Di
operate independently, we can test them independently.

i , D2

i , D2

i , D1

Let Di = (D0

i ) be one of the sub-devices. Denote
the joint view of the mini-devices Dj
i by View(Di((cid:126)r, (cid:126)s)) when
run as part of Di on public inputs ((cid:126)r, (cid:126)s) after the initial-
ization with (cid:126)m. Notice that in this view we have all tuples
of the form (cmd, val) exchanged between the mini-devices
Di
j and the master circuit M. As the outputs of Di are also
sent as a command, the view also contains the output shares
((cid:126)ci, (cid:126)di). Similarly, we denote by View(Γi((cid:126)r, (cid:126)s)) the view of
the mini-circuits Γj
At a high-level, T repeats the following process for each
i ∈ [λ]. First, it chooses a random value ti ← [t], where ti
denotes the number of test runs. In each of the ti runs the
public/secret inputs are chosen uniformly at random and we
execute once Di produced by A and once the speciﬁcation
Γi (in both cases using the same inputs). If the views diﬀer
in one of the runs we return false and the tester T aborts.
The formal description of the tester T is given in Figure 4.

i when run on public inputs ((cid:126)r, (cid:126)s).

The tester TD1(.),...,Dλ(.)(1k, λ)

Set the initial state Init({Di}i,(cid:126)0)
For i ∈ [λ] repeat the following:

Sample ti ← [t] and repeat for ti times:

Sample random sharing of public input (cid:126)r, (cid:126)s ← Fα
If View(Di((cid:126)r, (cid:126)s)) (cid:54)= View(Γi((cid:126)r, (cid:126)s)) return false

Return true.

Figure 4: The tester T for verifying whether the
devices follow the speciﬁcation given by Γ(cid:48).

4.4 Main theorem and security proof

The basic idea for the security of our construction is as
follows. Recall that in the testing phase each of the sub-
devices Di was tested for a random number of times ti ∈ [t].
Consider a mental experiment where instead of running the
sub-devices for ti times, we execute them for ti+n times, i.e.,
we view the real runs of Di also as test runs. Informally, the
malicious manufacturer A wins in the mental experiment if
the sub-devices Di succeed in the test runs, but he makes a
large fraction of Di fail in the following n real runs. We show
that the probability that A wins in the mental experiment
decreases exponentially with the number of sub-devices.
What remains to show is that for the devices Di the real
environment, where A can choose the inputs, looks (com-
putationally) indistinguishable from the test runs. In par-
ticular, we need to avoid that the adversary can choose the
inputs (cid:126)xi in order to signal to the devices that they are now
used outside of the test environment.6 The basic idea to

6Consider the input trigger attack where the adversary

Γ(1k, q,{(cid:126)xi}i∈[q], (cid:126)m):

The experiment Realj
Initialize circuit Γ by Init(Γ, (cid:126)m)

Output(cid:0)View(Γj(Share((cid:126)x1))), . . . , View(Γj(Share((cid:126)xq)))(cid:1)
Output(cid:0)View(Γj(Share((cid:126)z1))), . . . , View(Γj(Share((cid:126)zq)))(cid:1)

The experiment Randomj
Initialize circuit Γ by Init(Γ,(cid:126)0)
Sample (cid:126)z1, . . . , (cid:126)zq ← Fα uniformly at random

Γ(1k, q):

Figure 5: Views produced by a continuous real and
test execution of the speciﬁcation Γj by the mini-
circuits of our construction.

prevent such signaling is to let the mini-devices Dj
i run a
passively secure 3-party computation protocol on shares of
the input. This guarantees that none of the mini-devices ac-
tually knows the inputs on which it computes, and can start
to behave diﬀerently from the test environment. The rest
of this section is structured as follows. In Section 4.4.1 we
prove that the speciﬁcation of our construction satisﬁes the
property that real runs and test runs are indistinguishable.
In Section 4.4.2 we use this fact to prove robustness.

4.4.1 The transformed speciﬁcation
Before we move to the device-level, we prove a property
about the transformed speciﬁcation Γi := (Γ0
i ). Re-
call that each Γi is independent from the other sub-circuits
and speciﬁes exactly the same functionality. Hence, we con-
centrate in the following on a single sub-circuit and omit to
explicitly mention the parameter i. Let Γ denote one of the
sub-circuits and Γj are the corresponding mini-circuits.

i , Γ1

i , Γ2

In Figure 5 we deﬁne two distributions: The distribution
Γ(1k, q,{(cid:126)xi}i∈[q], (cid:126)m) considers the view of Γj on public
Realj
inputs (cid:126)xi and with secret initial input (cid:126)m. On the other
hand Randomj
Γ(1k, q) describes the view of Γj in q runs of
Γ with random inputs. The next lemma states that both
distributions are computationally close.

Lemma 3. Let q ∈ N denote the number of executions.
For any j ∈ [3], any set of public inputs {(cid:126)xi}i∈[q], and any
initial secret input (cid:126)m ∈ Fk, we have:

Realj

Γ(1k, q,{(cid:126)xi}i∈[q], (cid:126)m) ≈c Randomj

Γ(1k, q).

The proof appears in the extended version of this paper [15].

4.4.2 Trojan robustness of our construction
The theorem below shows the robustness of our construc-
tion. In particular, it states that trojan robustness increases
exponential with the number of devices.

Theorem 1. Let t, n, (cid:96), k ∈ N>0 with n < t and k be-
ing the computational security parameter. Π = (TR, T) is

(t, n, )-trojan robust for  :=(cid:0) n

(cid:1)λ/2 + negl(k).

t

We provide the proof in the extended version of this paper
[15]. Here we brieﬂy discuss the parameters given by the
theorem statement. The factor negl(k) can be ignored since
it comes from the security of the PRG. The dominating fac-

tor for realistic values of t, n, (cid:96) := 3λ is the value (cid:0) n

(cid:1)λ/2.

Let us give an example for the level of robustness we can

t

chooses a 128-bit random value at production time on which
the device is starting to deviate when received as input.

149achieve. Suppose we have λ = 10 sub-circuits, which re-
sults into 30 mini-devices that need to be produced by the
manufacturer. Suppose we test each of the sub-devices for
max t = 109 runs (which is realistic for simple hardware de-
vices), and want to use them for n = 105 executions later.
The theorem guarantees that except with probability 10−20
the resulting computation is correct.
4.5 Stateful and randomized circuits

So far we only discussed how to handle original circuits Γ
that are stateless (i.e., write their internal state only once)
and are deterministic (i.e., have no rand gates). We now
brieﬂy discuss how to extend our results to probabilistic and
stateful circuits. To handle the rand gates we do a simple
transformation before using our compiler TR. Namely, we
replace each rand gate by the output of a deterministic PRG.
Clearly, this reduces probabilistic computation to the deter-
ministic case we already discussed in the previous sections.
However, if the original circuit Γ was stateless, then after
replacing the rand gates in Γ by the PRG, the new circuit Γ(cid:48)
may become stateful. Hence, to complete our construction
we need to discuss how to handle stateful primitives (e.g.,
like a PRG or a stream cipher).

The problem with the stateful primitives is that in the
testing phase we test the diﬀerent sub-devices a diﬀerent
number of times, which makes the devices end up (after
the testing) in diﬀerent states. When then after the test-
ing phase in the real-execution we execute all sub-devices
jointly (driven by the master) and take the majority of their
outputs, then the majority will fail to provide the correct
result (since all the sub-devices are in a diﬀerent state). A
ﬁrst idea to deal with this issue is to reset the state of the
sub-devices after the testing. However, such reseting can be
noticed by the malicious sub-devices.7

To overcome this problem, we augment Γ with a special
circuit that carries out input-triggered re-initialization. For
instance, consider a Boolean circuit Γ that originally takes
inputs from {0, 1}α. We add an additional bit to the input
which signals re-initialization, i.e., the inputs of the aug-
mented Γ are now from {0, 1}α+1, where if the ﬁrst bit is 1,
then the following α bits are used to reset the state. Other-
wise, if the ﬁrst bit is 0, then the it is a normal execution of
Γ without updating the initial state. Such augmented cir-
cuitry can easily be implemented using our simple operations
from above. For instance, using the simple example above,
we compute for re-initialization: (cid:126)m = (1 − b) (cid:12) (cid:126)m ⊕ b (cid:12) (cid:126)x,
where b is the ﬁrst bit of the input, (cid:126)m is the initial state and
(cid:126)x are remaining α bits of the input. Clearly, if b = 0 the
state is not touched, while if b = 1 we rewrite the state with
the input. In order to achieve security, we then compile Γ
added with the augmented circuitry described above using
our compiler TR. Notice that this means not that we also
share the triggering bit b as otherwise the sub-devices can
notice that they are re-initialized.

Of course, the above approach has one drawback. It gives
the adversary in the real-execution the possibility to over-
write the state with a fresh adversarial chosen state. If (cid:126)m
is a key then this is an undesirable feature. However, this
can be easily ﬁxed by telling the master M to set the ﬁrst
bit permanently to 0 after re-initialization.

7For instance, the devices may just monitor their internal
state in some extra memory and hence can notice if the state
was changed outside of their normal execution pattern.

Figure 6: MultShares with threee mini-circuits.

5. DISCUSSIONS

In this section, we discuss the relevance of our circuit
model, the implementation cost of our transformed circuits
and testing phase, and the concrete attacks covered by our
threat model. Due to place constraints, we focus on general
observations and arguments in favor of the practicality of our
proposals and leave the concrete investigation of meaningful
case studies as a scope for further research.
5.1 Instantiation of the circuit model

In practice, the circuit speciﬁcation of Section 2.1 can be
simply instantiated with existing Hardware Description Lan-
guages (HDLs) such as VHDL or Verilog, and its communi-
cation commands with standard communication interfaces.
In fact, the only fundamental requirement for this circuit
speciﬁcation is that it allows describing and testing the func-
tional correctness of the devices implementing them.

Besides, since for our previous construction, we essentially
convert the original circuit Γ into a couple of passively secure
3-party implementations of this circuit, we use an abstract
representation based on addition and multiplication gates,
which allow us to describe a generic compiler. Yet, this is
not a strict requirement and any specialized compiler that
would lead to a more eﬃcient 3-party implementation of a
given circuit Γ (as long as it can be speciﬁed in a hardware
description language) is in fact eligible.
5.2 Cost of the transformed circuits

Concretely, our circuit transformation essentially requires
to design λ sub-circuits, each of them corresponding to a
3-party implementation of the functionality to protect. For
linear functionalities (in the binary/arithmetic ﬁeld we con-
sider) this implies overheads that are linear in the total num-
ber of devices (cid:96). So as usual in multiparty computation, the
most signiﬁcant overheads come from the non-linear opera-
tions. In order to estimate these overheads, an implemen-
tation of the MultShares circuit of Figure 3 is sketched in
Figure 6, where we can see that such an operation can be
carried out in 6 “abstract cycles” (denoted from C0 to C6 on
the ﬁgure) with a PRG and 10 arithmetic operations.

Therefore, in terms of timing/latency the best that we
can hope is a cycle count that is proportional to the logic
depth of the functionality to protect, which would happen if
we compute all the multiplications in parallel. Considering
that all the communications have to commute through the
master circuit, and that each send, in,out command can be
performed in c cycles, the latency of each multiplicative level
will be multiplied by a maximum factor 6c (since not all the
abstract cycles require communications).

150Figure 7: Implementation with 3D circuits.

In terms of circuit size, each sub-circuit will require a (con-
stant) multiplicative overhead (≈ ×10) due to the arithmetic
operations of MultShares, and a (constant) additive overhead
due to the PRG. The impact of the latter naturally depends
on the implementation size of this PRG compared to the
one of the functionality to protect. Taking the (expensive)
case where we compute several multiplications in parallel,
we could for example require to generate 128 pseudoran-
dom bits per cycle with an AES-based PRG, which remains
achievable, e.g., in low-cost FPGA devices.

Quite naturally, there may be additional overheads due to
representation issues. For example, standard block ciphers
are generally implemented thanks to table lookups, which
are not included in our circuit model. In this respect, we
ﬁrst note that such overheads can be mitigated by taking
advantage of cryptographic primitives designed for masking,
multiparty computation or fully homomorphic encryption
(which aim to minimize the multiplicative complexity and
depth of the circuits) [18, 4]. Besides, even for a standard
cipher such as the AES, the broad literature on masking
suggests that 3-party implementations similar to ours are
achievable in mainstream embedded devices (see, e.g. [25,
17] for software and hardware evaluations).

Eventually, we show in the next section that much more
eﬃcient specialized solutions can be obtained for certain im-
portant cryptographic functionalities.
5.3 Testing of the transformed circuits

As clear from the previous section, the security of our
trojan-resilient circuits depends on the possibility to test
sub- and mini-circuits, including all their communications.
In general, this can be implemented by connecting various
circuits to a master via standard communication interfaces.
However, we note that more compact solutions also exist, by
taking advantage of the 3D technologies of which the use-
fulness for trojan-resilient circuits was already put forward
in [20]. As illustrated in Figure 7, we can then easily embed
the sub-circuits as the diﬀerent tiers of a 3D hardware.

Besides, note that (as suggested in the right part of Fig-
ure 2), one can speed up the communication between the
mini-circuits by allowing them to communicate directly, given
that the tester can monitor these communications with “wires”
that would be used only during the testing phase, and of
which the monitoring would not be noticed by the mini-
circuits, i.e., under a “no hidden communications” assump-
tion. This could be achieved by equiping the tester with
specialized hardware capacities (e.g., an oscilloscope).

5.4 Attacks & limitations

We conclude this section by listing the attacks covered by

our threat model and its limitations.

Compared to [30], we prevent any digital input-triggered
hardware trojan (e.g., single-shot cheat codes and sequence
cheat codes). In this respect, we additionally cover the risk
of “infection attacks”, where one activated sub-circuit starts
to communicate with others sub-circuits, which is achieved
by limiting the communication between them.

Next, we prevent internally-triggered trojans (e.g., time
bombs) in a more general manner than [30]. Namely, this
previous work was limited to preventing volatile time bombs
with power resets. We also prevent non-volatile ones (e.g.,
a counter that would store the number of executions of the
circuit independent of its powering) thanks to our testing
phase. We believe this is an important improvement for
emerging technologies such as FRAM-based devices [16].

We also cover all the attacks considered in [20] and, as pre-
viously mentioned, are able to eﬃciently bound the success
rate of these attack to exponentially small probabilities.

By contrast, as mentioned in Section 2.2, we cannot pre-
vent physical trojan attacks since our testing phase is look-
ing for functional incorrectness. Yet, we note that exploit-
ing physical side-channels such as the power consumption or
electromagnetic radiation of a chip usually requires physical
proximity (which may be excluded by other means). As for
side-channels that are exploitable remotely, such as timing
attacks [11], they could be prevented by functional testing
(e.g., in order to ensure constant-time executions). In gen-
eral, the extension of our tools towards physical hardware
trojans is an important scope for further research.

Eventually, we mention one more type of attack which,
to the best of our knowledge, has not been mentioned in
the literature so far and is not covered by our tools, namely
“battery attacks”. In this case, the infected chip would go
on performing harmful operations (e.g., the increaing of a
counter) independent of whether the chip is performing any
computation. Interestingly, existing (e.g., lithium) battery
and energy harvesting technologies are currently based on
quite diﬀerent design techniques than digital ASICs [12, 27].
So it may be a reasonable hardware assumption to ask such
trojans to be detected by chip inspection (via microscopy
or other means), which we leave as another interesting chal-
lenge for hardware research.
6. EFFICIENT FUNCTIONALITIES

In this section, we brieﬂy discuss how to use testing am-
pliﬁcation to get better eﬃciency for certain cryptographic
primitives. We achieve the better eﬃciency by (a) focusing
on speciﬁc functionalities and (b) by only showing a weaker
security property. In particular, in contrast to trojan robust-
ness from Deﬁnition 1, which aims at correctness, we will
focus on a security property that is tailored to the particu-
lar functionality we want to protect. Notice that typically
the constructions presented in this section do not achieve
correctness and do not protect against the denial-of-service
attacks mentioned in the introduction. That is, a hardware
trojan can always disable the functionality completely.
6.1 Trojan secure PRGs

We ﬁrst describe how to construct a PRG that is trojan se-
cure, where “trojan security” is a weaker security guarantee
than trojan robustness from Deﬁnition 1. Nevertheless, we

151argue that for certain cryptographic primitives and certain
applications trojan security is a suﬃciently strong security
property.
In contrast to trojan robustness which requires
essentially that the malicious devices output correct results
(i.e., the same result as the honest speciﬁcation), trojan se-
curity of a PRG only guarantees that the malicious imple-
mentation of the PRG still outputs pseudorandomness.

Constructing a trojan secure PRG is very simple. Just
let the malicious manufacturer produce (cid:96) device D1, . . . , D(cid:96),
where each Di supposedly implements a cryptographically
strong PRG with binary output {0, 1}β.8 Each of the Di’s
is initialized with a random and independent initial secret
seed Ki. The master M then runs the devices Di and just
XORs the outputs of Di on each invocation. Observe that
since all keys Ki were sampled uniformly and independently
and we XOR the outputs of Di, we get that the output of
the composed device is pseudorandom as long as one device
Di outputs pseudorandomness.

De-randomization of our circuit compiler.

Let us now argue about the security of the above con-
struction. Testing the above implementation is easy: we
just use the same random testing approach as for our circuit
compiler. That is, each of the sub-devices Di is tested inde-
pendently for ti times. Next, we can use a similar analysis
as in Theorem 1 to show that if the Di’s pass the testing
phase, then with probability 1 − (n/t)(cid:96) at least one device
outputs the correct result for all n real executions. By the
above observation, this suﬃces to show that with probability
at least 1 − (n/t)(cid:96) the device outputs pseudorandomness.9
In our
circuit compiler, the master M is randomized since it needs
to secret-share the inputs of the device (which requires ran-
domness). We can use the above construction of the trojan
secure PRG to de-randomize M. To this end we let the mali-
cious manufacturer produce (cid:96) additional devices, where each
computes a PRG. Whenever M needs uniform randomness,
we replace it by the output of the above construction of a
trojan secure PRG. Notice that this further simpliﬁes the as-
sumptions that we put on M, since now the master M does
not need to run a trusted component for random number
generation. In this approach the complexity of M is reduced
to a small number of additions and multiplications.
6.2 Other cryptographic primitives

We conclude our paper with a short discussion on other
cryptographic primitives that can beneﬁt from the technique
of testing ampliﬁcation (i.e., having many independent de-
vices that are tested independently and the combined using
a master). For eﬃciency, we concentrate on the “trojan secu-
rity” (see Sect. 6.1 above) and because of the space reasons,
we only discuss how to construct an eﬃcient trojan secure
Message Authentication Code (MAC).

Recall that a message authentication code is a symmet-
ric cryptographic primitive that can be used to guarantee
the authenticity of messages. One way to protect a MAC
against trojan attacks is to use our generic compiler from
Section 4. We now describe a more eﬃcient way achieving
trojan security for MACs. Let us start by describing the

8It also may be a elements in a ﬁeld, but we only consider
the most simple case here.
9Observe that we obtain better parameters than for the
strong property of trojan robustness since we only require
that one sub-device behaves honestly. This allows us to save
a factor of 1/2 in the exponent.

security property we are aiming at. Let D be a device that
supposedly implements a secure MAC with key K, i.e., it
outputs tags with respect to the key K. Informally, trojan
security guarantees that valid tags can only be produced by
running the device D. Notice that this in particular implies
that an adversary interacting with the supposedly malicious
D in the n real executions does not learn anything about the
internal secret key K. More concretely, to specify the trojan
security of a MAC, we consider the following two phases (of
course, prior to these two phases we execute a testing phase
of the sub-devices):

1. In the learning phase, A interacts with the potentially
malicious implementation D. That is, A can ask for
MACs of messages of his choice and sees the output of
the MAC. Notice that this can be done for at most n
times (similar as in the robustness deﬁnition).

2. In the challenge phase the adversary has to provide a

forgery for the key K and a fresh message X.

is given back to the master M who computes Y = (cid:76)

In order to construct an eﬃcient trojan secure MAC, we
proceed as follows. Let F : {0, 1}k × {0, 1}α → {0, 1}β
be a secure pseudorandom function (for instance, instanti-
ated with an AES). We let the malicious manufacturer pro-
duce (cid:96) sub-devices D1, . . . , D(cid:96) where each supposedly imple-
ments the PRF F . The sub-devices Di are then combined
by the master M in the following way. On an input message
X ∈ {0, 1}α the master produces an (cid:96)-out-of-(cid:96) secret sharing
(X1, . . . , X(cid:96)) of X. Each share Xi is given to the sub-device
Di as input, which computes Yi = F (Ki, Xi). The value Yi
i Yi
and outputs the tag ((X1, . . . X(cid:96)), Y ). Notice that we can
de-randomize the master M by using our PRG construction
from Section 6.1. Veriﬁcation of the tag produced by the
above construction is simple. Essentially, since (X1, . . . X(cid:96))
are part of the tag the veriﬁer can use (K1, . . . , K(cid:96)) to ver-
ify the correctness of the MAC. The above construction has
the shortcoming that it increases the length of the tag by (cid:96)
times the message length. We leave it as an interesting open
question to improve the tag length.

The basic intuition why the above construction is trojan
secure is as follows. First, observe that the sub-devices Di
operate independently from each other (they all use inde-
pendent keys and no communication is needed between the
Di’s for computing F ). Second, they are run on shares of the
inputs X, so the adversary cannot initiate malicious behav-
ior by signaling it through the inputs. The random testing
guarantees that with probability 1 − (n/t)(cid:96) at least one de-
vice Di outputs the correct result for all n real executions.
Since we are XORing the outputs of all sub-devices Di, we
are guaranteed that as long as at least one device Di op-
erates honestly, it “blinds” the outputs of all other devices,
and hence hides the output of potential malicious devices
(that try to reveal their internal keys).

In general it can be observed that, informally speaking,
in order to construct eﬃcient trojan robust cryptographic
primitives using our technique of testing ampliﬁcation, we
need algorithms that are both input homomorphic and key
homomorphic (essentially this is what the use of the MPC
enables). We leave it as an interesting question for future
work to ﬁnd such cryptographic schemes.

Acknowledgments. Stefan Dziembowski is supported by
the Foundation for Polish Science. Sebastian Faust is funded
by the Emmy Noether Program FA 1320/1-1 of the German

152Research Foundation (DFG). Fran¸cois-Xavier Standaert Stan-
daert is a research associate of the Belgian Fund for Scientiﬁc
Research (FNRS-F.R.S.).

[17] V. Grosso, F. Standaert, and S. Faust. “Masking vs.

multiparty computation: how large is the gap for
AES?” In: J. Cryptographic Engineering 1 (2014).

[18] V. Grosso, G. Leurent, F. Standaert, and K. Varici.

“LS-Designs: Bitslice Encryption for Eﬃcient Masked
Software Implementations”. In: FSE. 2014.

[19] S. K. Haider, C. Jin, M. Ahmad, D. M. Shila,

O. Khan, and M. van Dijk. Advancing the
State-of-the-Art in Hardware Trojans Detection.
Cryptology ePrint Archive, Report 2014/943. 2014.

[20] F. Imeson, A. Emtenan, S. Garg, and

M. V. Tripunitara. “Securing Computer Hardware
Using 3D Integrated Circuit (IC) Technology and
Split Manufacturing for Obfuscation”. In: USENIX
Security Symposium. 2013.

[21] Y. Ishai, A. Sahai, and D. Wagner. “Private Circuits:

Securing Hardware against Probing Attacks”. In:
CRYPTO. 2003.

[22] Y. Ishai, M. Prabhakaran, A. Sahai, and D. Wagner.
“Private Circuits II: Keeping Secrets in Tamperable
Circuits”. In: EUROCRYPT. 2006.

[23] P. C. Kocher. “Timing Attacks on Implementations
of Diﬃe-Hellman, RSA, DSS, and Other Systems”.
In: CRYPTO. 1996.

[24] P. C. Kocher, J. Jaﬀe, and B. Jun. “Diﬀerential

Power Analysis”. In: CRYPTO. 1999.

[25] A. Moradi, A. Poschmann, S. Ling, C. Paar, and

H. Wang. “Pushing the Limits: A Very Compact and
a Threshold Implementation of AES”. In:
EUROCRYPT 2011. 2011.

[26] S. Narasimhan, D. Du, R. S. Chakraborty, S. Paul,

F. G. Wolﬀ, C. A. Papachristou, K. Roy, and
S. Bhunia. “Hardware Trojan Detection by
Multiple-Parameter Side-Channel Analysis”. In:
IEEE Trans. Computers 11 (2013).

[27] S. Priya and D. J. Inman. Energy harvesting

technologies. 2009.

[28] M. Tehranipoor and F. Koushanfar. “A Survey of

Hardware Trojan Taxonomy and Detection”. In:
IEEE Design & Test of Computers 1 (2010).

[29] R. S. Wahby, M. Howald, S. Garg, abhi shelat, and

M. Walﬁsh. Veriﬁable ASICs. Cryptology ePrint
Archive, Report 2015/1243. 2015.

[30] A. Waksman and S. Sethumadhavan. “Silencing

Hardware Backdoors”. In: IEEE S&P. 2011.

References
[1] J. Aarestad, D. Acharyya, R. M. Rad, and

J. Plusquellic. “Detecting Trojans Through Leakage
Current Analysis Using Multiple Supply Pad IDDQ
s”. In: IEEE Trans. Information Forensics and
Security 4 (2010).

[2] S. O. Adee. “The Hunt For The Kill Switch”. In:

IEEE Spectrum 5 (May 2008). issn: 0018-9235.

[3] D. Agrawal, S. Baktir, D. Karakoyunlu, P. Rohatgi,

and B. Sunar. “Trojan Detection using IC
Fingerprinting”. In: IEEE S&P. 2007.

[4] M. R. Albrecht, C. Rechberger, T. Schneider,

T. Tiessen, and M. Zohner. “Ciphers for MPC and
FHE”. In: EUROCRYPT. 2015.

[5] G. Ateniese, A. Kiayias, B. Magri, Y. Tselekounis,

and D. Venturi. Secure Outsourcing of Circuit
Manufacturing. Cryptology ePrint Archive, Report
2016/527. 2016.

[6] C. Bayer and J.-P. Seifert. “Trojan-resilient circuits”.

In: PROOFS. 2013.

[7] S. Bhunia, M. S. Hsiao, M. Banga, and

S. Narasimhan. “Hardware Trojan attacks: threat
analysis and countermeasures”. In: Proceedings of the
IEEE 8 (2014).

[8] E. Biham, Y. Carmeli, and A. Shamir. “Bug

Attacks”. In: CRYPTO. 2008.

[9] E. Biham and A. Shamir. “Diﬀerential Fault Analysis

of Secret Key Cryptosystems”. In: CRYPTO. 1997.

[10] D. Boneh, R. A. DeMillo, and R. J. Lipton. “On the
Importance of Eliminating Errors in Cryptographic
Computations”. In: J. Cryptology 2 (2001).

[11] B. B. Brumley and N. Tuveri. “Remote Timing

Attacks Are Still Practical”. In: ESORICS. 2011.

[12] C. K. Chan, H. Peng, G. Liu, K. McIlwrath,

X. F. Zhang, R. A. Huggins, and Y. Cui.
“High-performance lithium battery anodes using
silicon nanowires”. In: Nature nanotechnology 1
(2008).

[13] S. Chari, C. S. Jutla, J. R. Rao, and P. Rohatgi.

“Towards Sound Approaches to Counteract
Power-Analysis Attacks”. In: CRYPTO. 1999.

[14] R. Cramer. “Introduction to Secure Computation”.

In: Lectures on Data Security, Modern Cryptology in
Theory and Practice, Summer School, Aarhus,
Denmark, July 1998. 1998.

[15] S. Dziembowski, S. Faust, and F.-X. Standaert.

Private Circuits III: Hardware Trojan-Resilience via
Testing Ampliﬁcation. Cryptology ePrint Archive.
2016.

[16] G. Fox, F Chu, and T Davenport. “Current and

future ferroelectric nonvolatile memory technology”.
In: Journal of Vacuum Science & Technology B 5
(2001).

153