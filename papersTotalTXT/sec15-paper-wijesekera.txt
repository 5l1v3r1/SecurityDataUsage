Android Permissions Remystified:  
A Field Study on Contextual Integrity

Primal Wijesekera, University of British Columbia; Arjun Baokar, Ashkan Hosseini,  

Serge Egelman, and David Wagner, University of California, Berkeley;  

Konstantin Beznosov, University of British Columbia

https://www.usenix.org/conference/usenixsecurity15/technical-sessions/presentation/wijesekera

This paper is included in the Proceedings of the 24th USENIX Security SymposiumAugust 12–14, 2015 • Washington, D.C.ISBN 978-1-931971-232Open access to the Proceedings of  the 24th USENIX Security Symposium is sponsored by USENIXAndroid Permissions Remystiﬁed:

A Field Study on Contextual Integrity

Primal Wijesekera1, Arjun Baokar2, Ashkan Hosseini2, Serge Egelman2,

David Wagner2, and Konstantin Beznosov1

1University of British Columbia, Vancouver, Canada,

{primal,beznosov}@ece.ubc.ca

2University of California, Berkeley, Berkeley, USA,

{arjunbaokar,ashkan}@berkeley.edu, {egelman,daw}@cs.berkeley.edu

Abstract
We instrumented the Android platform to collect data re-
garding how often and under what circumstances smart-
phone applications access protected resources regulated
by permissions. We performed a 36-person ﬁeld study
to explore the notion of “contextual integrity,” i.e., how
often applications access protected resources when users
are not expecting it. Based on our collection of 27M data
points and exit interviews with participants, we exam-
ine the situations in which users would like the ability to
deny applications access to protected resources. At least
80% of our participants would have preferred to prevent
at least one permission request, and overall, they stated a
desire to block over a third of all requests. Our ﬁndings
pave the way for future systems to automatically deter-
mine the situations in which users would want to be con-
fronted with security decisions.

Introduction

1
Mobile platform permission models regulate how appli-
cations access certain resources, such as users’ personal
information or sensor data (e.g., camera, GPS, etc.). For
instance, previous versions of Android prompt the user
during application installation with a list of all the per-
missions that the application may use in the future; if the
user is uncomfortable granting any of these requests, her
only option is to discontinue installation [3]. On iOS and
Android M, the user is prompted at runtime the ﬁrst time
an application requests any of a handful of data types,
such as location, address book contacts, or photos [34].

Research has shown that few people read the Android
install-time permission requests and even fewer compre-
hend them [16]. Another problem is habituation: on av-
erage, Android applications present the user with four
permission requests during the installation process [13].
While iOS users are likely to see fewer permission re-
quests than Android users, because there are fewer pos-
sible permissions and they are only displayed the ﬁrst

time the data is actually requested, it is not clear whether
or not users are being prompted about access to data that
they actually ﬁnd concerning, or whether they would ap-
prove of subsequent requests [15].

Nissenbaum posited that the reason why most privacy
models fail to predict violations is that they fail to con-
sider contextual integrity [32]. That is, privacy violations
occur when personal information is used in ways that
defy users’ expectations. We believe that this notion of
“privacy as contextual integrity” can be applied to smart-
phone permission systems to yield more effective per-
missions by only prompting users when an application’s
access to sensitive data is likely to defy expectations. As
a ﬁrst step down this path, we examined how applica-
tions are currently accessing this data and then examined
whether or not it complied with users’ expectations.

We modiﬁed Android to log whenever an application
accessed a permission-protected resource and then gave
these modiﬁed smartphones to 36 participants who used
them as their primary phones for one week. The pur-
pose of this was to perform dynamic analysis to deter-
mine how often various applications are actually access-
ing protected resources under realistic circumstances.
Afterwards, subjects returned the phones to our labora-
tory and completed exit surveys. We showed them vari-
ous instances over the past week where applications had
accessed certain types of data and asked whether those
instances were expected, and whether they would have
wanted to deny access. Participants wanted to block a
third of the requests. Their decisions were governed pri-
marily by two factors: whether they had privacy concerns
surrounding the speciﬁc data type and whether they un-
derstood why the application needed it.

We contribute the following:

• To our knowledge, we performed the ﬁrst ﬁeld study
to quantify the permission usage by third-party ap-
plications under realistic circumstances.

USENIX Association  

24th USENIX Security Symposium  499

1

• We show that our participants wanted to block ac-
cess to protected resources a third of the time. This
suggests that some requests should be granted by
runtime consent dialogs, rather than Android’s pre-
vious all-or-nothing install-time approval approach.
• We show that the visibility of the requesting appli-
cation and the frequency at which requests occur are
two important factors which need to be taken into
account in designing a runtime consent platform.

2 Related Work
While users are required to approve Android application
permission requests during installation, most do not pay
attention and fewer comprehend these requests [16, 26].
In fact, even developers are not fully knowledgeable
about permissions [40], and are given a lot of free-
dom when posting an application to the Google Play
Store [7]. Applications often do not follow the principle
of least privilege, intentionally or unintentionally [44].
Other work has suggested improving the Android per-
mission model with better deﬁnitions and hierarchical
breakdowns [8]. Some researchers have experimented
with adding ﬁne-grained access control to the Android
model [11]. Providing users with more privacy informa-
tion and personal examples has been shown to help users
in choosing applications with fewer permissions [21,27].

Previous work has examined the overuse of permissions
by applications [13, 20], and attempted to identify mali-
cious applications through their permission requests [36]
or through natural language processing of application de-
scriptions [35]. Researchers have also developed static
analysis tools to analyze Android permission speciﬁca-
tions [6, 9, 13]. Our work complements this static anal-
ysis by applying dynamic analysis to permission us-
age. Other researchers have applied dynamic analysis to
native (non-Java) APIs among third-party mobile mar-
kets [39]; we apply it to the Java APIs available to devel-
opers in the Google Play Store.

Researchers examined user privacy expectations sur-
rounding application permissions, and found that users
were often surprised by the abilities of background ap-
plications to collect data [25, 42]. Their level of con-
cern varied from annoyance to seeking retribution when
presented with possible risks associated with permis-
sions [15]. Some studies employed crowdsourcing to
create a privacy model based on user expectations [30].

Researchers have designed systems to track or reduce
privacy violations by recommending applications based
on users’ security concerns [2, 12, 19, 24, 28, 46–48].
Other tools dynamically block runtime permission re-
quests [37]. Enck et al. found that a considerable number
of applications transmitted location or other user data to

third parties without requiring user consent [12]. Horny-
ack et al.’s AppFence system gave users the ability to
deny data to applications or substitute fake data [24].
However, this broke application functionality for one-
third of the applications tested.

Reducing the number of security decisions a user must
make is likely to decrease habituation, and therefore, it is
critical to identify which security decisions users should
be asked to make. Based on this theory, Felt et al. created
a decision tree to aid platform designers in determining
the most appropriate permission-granting mechanism for
a given resource (e.g., access to benign resources should
be granted automatically, whereas access to dangerous
resources should require approval) [14]. They concluded
that the majority of Android permissions can be automat-
ically granted, but 16% (corresponding to the 12 permis-
sions in Table 1) should be granted via runtime dialogs.

Nissenbaum’s theory of contextual integrity can help us
to analyze “the appropriateness of a ﬂow” in the con-
text of permissions granted to Android applications [32].
There is ambiguity in deﬁning when an application actu-
ally needs access to user data to run properly. It is quite
easy to see why a location-sharing application would
need access to GPS data, whereas that same request com-
ing from a game like Angry Birds is less obvious. “Con-
textual integrity is preserved if information ﬂows accord-
ing to contextual norms” [32], however, the lack of thor-
ough documentation on the Android permission model
makes it easier for programmers to neglect these norms,
whether intentionally or accidentally [38]. Deciding on
whether an application is violating users’ privacy can be
quite complicated since “the scope of privacy is wide-
ranging” [32]. To that end, we performed dynamic analy-
sis to measure how often (and under what circumstances)
applications were accessing protected resources, whether
this complied with users’ expectations, as well as how
often they might be prompted if we adopt Felt et al.’s
proposal to require runtime user conﬁrmation before ac-
cessing a subset of these resources [14]. Finally, we show
how it is possible to develop a classiﬁer to automatically
determine whether or not to prompt the user based on
varying contextual factors.

3 Methodology
Our long-term research goal is to minimize habituation
by only confronting users with necessary security de-
cisions and avoiding showing them permission requests
that are either expected, reversible, or unconcerning. Se-
lecting which permissions to ask about requires under-
standing how often users would be confronted with each
type of request (to assess the risk of habituation) and user
reactions to these requests (to assess the beneﬁt to users).
In this study, we explored the problem space in two parts:

500  24th USENIX Security Symposium 

USENIX Association

2

we instrumented Android so that we could collect actual
usage data to understand how often access to various pro-
tected resources is requested by applications in practice,
and then we surveyed our participants to understand the
requests that they would not have granted, if given the
option. This ﬁeld study involved 36 participants over the
course of one week of normal smartphone usage. In this
section, we describe the log data that we collected, our
recruitment procedure, and then our exit survey.

3.1 Tracking Access to Sensitive Data
In Android, when applications attempt to access pro-
tected resources (e.g., personal information, sensor data,
etc.)
at runtime, the operating system checks to see
whether or not the requesting application was previously
granted access during installation. We modiﬁed the An-
droid platform to add a logging framework so that we
could determine every time one of these resources was
accessed by an application at runtime. Because our target
device was a Samsung Nexus S smartphone, we modiﬁed
Android 4.1.1 (Jellybean), which was the newest version
of Android supported by our hardware.

3.1.1 Data Collection Architecture
Our goal was to collect as much data as possible about
each applications’ access to protected resources, while
minimizing our impact on system performance. Our
data collection framework consisted of two main com-
ponents: a series of “producers” that hooked various An-
droid API calls and a “consumer” embedded in the main
Android framework service that wrote the data to a log
ﬁle and uploaded it to our collection server.

We logged three kinds of permission requests. First, we
logged function calls checked by checkPermission()
in the Android Context implementation.
Instru-
menting the Context implementation, instead of the
ActivityManagerService or PackageManager, al-
lowed us to also log the function name invoked by the
user-space application. Next, we logged access to the
ContentProvider class, which veriﬁes the read and
write permissions of an application prior to it accessing
structured data (e.g., contacts or calendars) [5]. Finally,
we tracked permission checks during Intent transmis-
sion by instrumenting the ActivityManagerService
and BroadcastQueue. Intents allow an application to
pass messages to another application when an activity is
to be performed in that other application (e.g., opening a
URL in the web browser) [4].

We created a component called Producer that fetches
the data from the above instrumented points and sends it
back to the Consumer, which is responsible for logging
everything reported. Producers are scattered across
the Android Platform, since permission checks occur in

multiple places. The Producer that logged the most
data was in system server and recorded direct func-
tion calls to Android’s Java API. For a majority of priv-
ileged function calls, when a user application invokes
the function, it sends the request to system server
via Binder. Binder is the most prominent IPC mech-
anism implemented to communicate with the Android
Platform (whereas Intents communicate between ap-
plications). For requests that do not make IPC calls to the
system server, a Producer is placed in the user appli-
cation context (e.g., in the case of ContentProviders).

The Consumer class is responsible for logging data pro-
duced by each Producer. Additionally, the Consumer
also stores contextual information, which we describe in
Section 3.1.2. The Consumer syncs data with the ﬁlesys-
tem periodically to minimize impact on system perfor-
mance. All log data is written to the internal storage of
the device because the Android kernel is not allowed to
write to external storage for security reasons. Although
this protects our data from curious or careless users, it
also limits our storage capacity. Thus, we compressed
the log ﬁles once every two hours and upload them to
our collection servers whenever the phone had an active
Internet connection (the average uploaded and zipped log
ﬁle was around 108KB and contained 9,000 events).

Due to the high volume of permission checks we en-
countered and our goal of keeping system performance
at acceptable levels, we added rate-limiting logic to the
Consumer. Speciﬁcally,
if it has logged permission
checks for a particular application/permission combina-
tion more than 10,000 times, it examines whether it did
so while exceeding an average rate of 1 permission check
every 2 seconds. If so, the Consumer will only record
10% of all future requests for this application/permission
combination. When this rate-limiting is enabled, the
Consumer tracks these application/permission combina-
tions and updates all the Producers so that they start
dropping these log entries. Finally, the Consumer makes
a note of whenever this occurs so that we can extrapolate
the true number of permission checks that occurred.

3.1.2 Data Collection
We hooked the permission-checking APIs so that every
time the system checked whether an application had been
granted a particular permission, we logged the name of
the permission, the name of the application, and the API
method that resulted in the check. In addition to times-
tamps, we collected the following contextual data:

• Visibility—We categorized whether the requesting
application was visible to the user, using four cate-
gories: running (a) as a service with no user inter-
action; (b) as a service, but with user interaction via

USENIX Association  

24th USENIX Security Symposium  501

3

notiﬁcations or sounds; (c) as a foreground process,
but in the background due to multitasking; or (d) as
a foreground process with direct user interaction.
• Screen Status—Whether the screen was on/off.
• Connectivity—The phone’s WiFi connection state.
• Location—The user’s last known coordinates. In
order to preserve battery life, we collected cached
location data, rather than directly querying the GPS.
• View—The UI elements in the requesting applica-
tion that were exposed to the user at the time that a
protected resource was accessed. Speciﬁcally, since
the UI is built from an XML ﬁle, we recorded the
name of the screen as deﬁned in the DOM.

interacted prior to the requesting application.

• History—A list of applications with which the user
• Path—When access to a ContentProvider object

was requested, the path to the speciﬁc content.

Felt et al. proposed granting most Android permissions
without a priori user approval and granting 12 permis-
sions (Table 1) at runtime so that users have contextual
information to infer why the data might be needed [14].
The idea is that, if the user is asked to grant a permission
while using an application, she may have some under-
standing of why the application needs that permission
based on what she was doing. We initially wanted to
perform experience sampling by probabilistically ques-
tioning participants whenever any of these 12 permis-
sions were checked [29]. Our goal was to survey par-
ticipants about whether access to these resources was ex-
pected and whether it should proceed, but we were con-
cerned that this would prime them to the security focus
of our experiment, biasing their subsequent behaviors.
Instead, we instrumented the phones to probabilistically
take screenshots of what participants were doing when
these 12 permissions were checked so that we could ask
them about it during the exit survey. We used reservoir
sampling to minimize storage and performance impacts,
while also ensuring that the screenshots covered a broad
set of applications and permissions [43].

Figure 1 shows a screenshot captured during the study
along with its corresponding log entry. The user was
playing the Solitaire game while Spotify requested a
WiFi scan. Since this permission was of interest (Table
1), our instrumentation took a screenshot. Since Spotify
was not the application the participant was interacting
with, its visibility was set to false. The history shows that
prior to Spotify calling getScanResults(), the user
had viewed Solitaire, the call screen, the launcher, and
the list of MMS conversations.

Permission Type
WRITE SYNC
SETTINGS
ACCESS WIFI
STATE
INTERNET
NFC
READ HISTORY
BOOKMARKS
ACCESS FINE
LOCATION
ACCESS COARSE
LOCATION
LOCATION
HARDWARE
READ CALL LOG
ADD VOICEMAIL
READ SMS
SEND SMS

Activity
Change application sync settings
when the user is roaming

View nearby SSIDs

Access Internet when roaming
Communicate via NFC

Read users’ browser history

Read GPS location

Read network-inferred location
(i.e., cell tower and/or WiFi)

Directly access GPS data

Read call history
Read call history
Read sent/received/draft SMS
Send SMS

Table 1: The 12 permissions that Felt et al. recommend
be granted via runtime dialogs [14]. We randomly took
screenshots when these permissions were requested by
applications, and we asked about them in our exit survey.

3.2 Recruitment
We placed an online recruitment advertisement on
Craigslist in October of 2014, under the “et cetera jobs”
section.1 The title of the advertisement was “Research
Study on Android Smartphones,” and it stated that the
study was about how people interact with their smart-
phones. We made no mention of security or privacy.
Those interested in participating were directed to an on-
line consent form. Upon agreeing to the consent form,
potential participants were directed to a screening appli-
cation in the Google Play store. The screening applica-
tion asked for information about each potential partici-
pant’s age, gender, smartphone make and model. It also
collected data on their phones’ internal memory size and
the installed applications. We screened out applicants
who were under 18 years of age or used providers other
than T-Mobile, since our experimental phones could not
attain 3G speeds on other providers. We collected data on
participants’ installed applications so that we could pre-
install free applications prior to them visiting our labo-
ratory. (We copied paid applications from their phones,
since we could not download those ahead of time.)

We contacted participants who met our screening re-
quirements to schedule a time to do the initial setup.
Overall, 48 people showed up to our laboratory, and of
those, 40 qualiﬁed (8 were rejected because our screen-
ing application did not distinguish some Metro PCS users

1Approved by the UC Berkeley IRB under protocol #2013-02-4992

502  24th USENIX Security Symposium 

USENIX Association

4

cations, and we instructed them to use these phones as
they would their normal phones. Our logging framework
kept track of every protected resource accessed by a user-
level application along with the previously-mentioned
contextual data. Due to storage constraints on the de-
vices, our software uploaded log ﬁles to our server every
two hours. However, to preserve participants’ privacy,
screenshots remained on the phones during the course
of the week. At the end of the week, each participant
returned to our laboratory, completed an exit survey, re-
turned the phone, and then received an additional $100
gift card (i.e., slightly more than the value of the phone).

3.3 Exit Survey
When participants returned to our laboratory, they com-
pleted an exit survey. The exit survey software ran on
a laptop in a private room so that it could ask questions
about what they were doing on their phones during the
course of the week without raising privacy concerns. We
did not view their screenshots until participants gave us
permission. The survey had three components:

• Screenshots—Our software displayed a screenshot
taken after one of the 12 resources in Table 1 was
accessed. Next to the screenshot (Figure 2a), we
asked participants what they were doing on the
phone when the screenshot was taken (open-ended).
We also asked them to indicate which of several ac-
tions they believed the application was performing,
chosen from a multiple-choice list of permissions
presented in plain language (e.g., “reading browser
history,” “sending a SMS,” etc.). After answering
these questions, they proceeded to a second page of
questions (Figure 2b). We informed participants at
the top of this page of the resource that the appli-
cation had accessed when the screenshot was taken,
and asked them to indicate how much they expected
this (5-point Likert scale). Next, we asked, “if you
were given the choice, would you have prevented
the app from accessing this data,” and to explain
why or why not. Finally, we asked for permis-
sion to view the screenshot. This phase of the exit
survey was repeated for 10-15 different screenshots
per participant, based on the number of screenshots
saved by our reservoir sampling algorithm.

• Locked Screens—The second part of our survey
involved questions about the same protected re-
sources, though accessed while device screens were
off (i.e., participants were not using their phones).
Because there were no contextual cues (i.e., screen-
shots), we outright told participants which appli-
cations were accessing which resources and asked
them multiple choice questions about whether they
wanted to prevent this and the degree to which these

Name
Type
Permission
App Name
Timestamp
API Function
Visibility
Screen Status
Connectivity
Location

View

History

(a) Screenshot

Log Data
API FUNC
ACCESS WIFI STATE
com.spotify.music
1412888326273
getScanResults()
FALSE
SCREEN ON
NOT CONNECTED
Lat
-122.XXX -
1412538686641 (Time it was updated)
com.mobilityware.solitaire/.Solitaire
com.android.phone/.InCallScreen
com.android.launcher/com.android.-
launcher2.Launcher
com.android.mms/ConversationList

37.XXX Long

(b) Corresponding log entry

Figure 1: Screenshot (a) and corresponding log entry (b)
captured during the experiment.

from T-Mobile users). In the email, we noted that due
to the space constraints of our experimental phones, we
might not be able to install all the applications on their
existing phones, and therefore they needed to make a
note of the ones that they planned to use that week. The
initial setup took roughly 30 minutes and involved trans-
ferring their SIM cards, helping them set up their Google
and other accounts, and making sure they had all the ap-
plications they needed. We compensated each participant
with a $35 gift card for showing up at the setup session.
Out of 40 people who were given phones, 2 did not re-
turn them, and 2 did not regularly use them during the
study period. Of our 36 remaining participants who used
the phones regularly, 19 were male and 17 were female;
ages ranged from 20 to 63 years old (µ = 32, σ= 11).

After the initial setup session, participants used the ex-
perimental phones for one week in lieu of their normal
phones. They were allowed to install and uninstall appli-

USENIX Association  

24th USENIX Security Symposium  503

5

Three researchers independently coded 423 responses to
the open-ended question in the screenshot portion of the
survey. The number of responses per participant varied,
as they were randomly selected based on the number of
screenshots taken: participants who used their phones
more heavily had more screenshots, and thus answered
more questions. Prior to meeting to achieve consensus,
the three coders disagreed on 42 responses, which re-
sulted in an inter-rater agreement of 90%. Taking into
account the 9 possible codings for each response, Fleiss’
kappa yielded 0.61, indicating substantial agreement.

4 Application Behaviors
Over the week-long period, we logged 27M application
requests to protected resources governed by Android per-
missions. This translates to over 100,000 requests per
user/day. In this section, we quantify the circumstances
under which these resources were accessed. We focus on
the rate at which resources were accessed when partici-
pants were not actively using those applications (i.e., sit-
uations likely to defy users’ expectations), access to cer-
tain resources with particularly high frequency, and the
impact of replacing certain requests with runtime conﬁr-
mation dialogs (as per Felt et al.’s suggestion [14]).

Invisible Permission Requests

4.1
In many cases, it is entirely expected that an applica-
tion might make frequent requests to resources protected
by permissions. For instance, the INTERNET permis-
sion is used every time an application needs to open a
socket, ACCESS FINE LOCATION is used every time
the user’s location is checked by a mapping application,
and so on. However, in these cases, one expects users to
have certain contextual cues to help them understand that
these applications are running and making these requests.
Based on our log data, most requests occurred while par-
ticipants were not actually interacting with those appli-
cations, nor did they have any cues to indicate that the
applications were even running. When resources are ac-
cessed, applications can be in ﬁve different states, with
regard to their visibility to users:

1. Visible foreground application (12.04%): the user

is using the application requesting the resource.

2. Invisible background application (0.70%): due to
multitasking, the application is in the background.
3. Visible background service (12.86%): the appli-
cation is a background service, but the user may be
aware of its presence due to other cues (e.g., it is
playing music or is present in the notiﬁcation bar).
4. Invisible background service (14.40%): the appli-

cation is a background service without visibility.

5. Screen off (60.00%):

the application is running,
but the phone screen is off because it is not in use.

(a) On the ﬁrst screen, participants answered questions to estab-
lish awareness of the permission request based on the screenshot.

(b) On the second screen, they saw the resource accessed, stated
whether it was expected, and whether it should have been blocked.

Figure 2: Exit Survey Interface

behaviors were expected. They answered these
questions for up to 10 requests, similarly chosen by
our reservoir sampling algorithm to yield a breadth
of application/permission combinations.

• Personal Privacy Preferences—Finally, in order
to correlate survey responses with privacy prefer-
ences, participants completed two privacy scales.
Because of the numerous reliability problems with
the Westin index [45], we computed the average
of both Buchanan et al.’s Privacy Concerns Scale
(PCS) [10] and Malhotra et al.’s Internet Users’ In-
formation Privacy Concerns (IUIPC) scale [31].

After participants completed the exit survey, we re-
entered the room, answered any remaining questions,
and then assisted them in transferring their SIM cards
back into their personal phones. Finally, we compen-
sated each participant with a $100 gift card.

504  24th USENIX Security Symposium 

USENIX Association

6

Permission
ACCESS NETWORK STATE
WAKE LOCK
ACCESS FINE LOCATION
GET ACCOUNTS
ACCESS WIFI STATE
UPDATE DEVICE STATS
ACCESS COARSE LOCATION
AUTHENTICATE ACCOUNTS
READ SYNC SETTINGS
INTERNET

Requests
31,206
23,816
5,652
3,411
1,826
1,426
1,277
644
426
416

Application
Facebook
Google Location Reporting
Facebook Messenger
Taptu DJ
Google Maps
Google Gapps
Foursquare
Yahoo Weather
Devexpert Weather
Tile Game(Umoni)

Requests
36,346
31,747
22,008
10,662
5,483
4,472
3,527
2,659
2,567
2,239

Table 2: The most frequently requested permissions by
applications with zero visibility to the user.

Table 3: The applications making the most permission
requests while running invisibly to the user.

Combining the 3.3M (12.04% of 27M) requests that were
granted when the user was actively using the application
(Category 1) with the 3.5M (12.86% of 27M) requests
that were granted when the user had other contextual
cues to indicate that the application was running (Cat-
egory 3), we can see that fewer than one quarter of all
permission requests (24.90% of 27M) occurred when the
user had clear indications that those applications were
running. This suggests that during the vast majority of
the time, access to protected resources occurs opaquely
to users. We focus on these 20.3M “invisible” requests
(75.10% of 27M) in the remainder of this subsection.

Harbach et al. found that users’ phone screens are off
94% of the time on average [22]. We observed that
60% of permission requests occurred while participants’
phone screens were off, which suggests that permission
requests occurred less frequently than when participants
were using their phones. At the same time, certain appli-
cations made more requests when participants were not
using their phones: “Brave Frontier Service,” “Microsoft
Sky Drive,” and “Tile game by UMoni.” Our study col-
lected data on over 300 applications, and therefore it is
possible that with a larger sample size, we would ob-
serve other applications engaging in this behavior. All of
the aforementioned applications primarily requested AC-
CESS WIFI STATE and INTERNET. While a deﬁnitive
explanation for this behavior requires examining source
code or the call stacks of these applications, we hypothe-
size that they were continuously updating local data from
remote servers. For instance, Sky Drive may have been
updating documents, whereas the other two applications
may have been checking the status of multiplayer games.

Table 2 shows the most frequently requested permis-
sions from applications running invisibly to the user (i.e.,
Categories 2, 4, and 5); Table 3 shows the applica-
tions responsible for these requests (Appendix A lists
the permissions requested by these applications). We

normalized the numbers to show requests per user/day.
ACCESS NETWORK STATE was most frequently re-
quested, averaging 31,206 times per user/day—roughly
once every 3 seconds. This is due to applications con-
stantly checking for Internet connectivity. However, the
5,562 requests/day to ACCESS FINE LOCATION and
1,277 requests/day to ACCESS COARSE LOCATION
are more concerning, as this could enable detailed track-
ing of the user’s movement throughout the day. Sim-
ilarly, a user’s location can be inferred by using AC-
CESS WIFI STATE to get data on nearby WiFi SSIDs.

Contextual integrity means ensuring that information
ﬂows are appropriate, as determined by the user. Thus,
users need the ability to see information ﬂows. Current
mobile platforms have done some work to let the user
know about location tracking. For instance, recent ver-
sions of Android allow users to see which applications
have used location data recently. While attribution is a
positive step towards contextual integrity, attribution is
most beneﬁcial for actions that are reversible, whereas
the disclosure of location information is not something
that can be undone [14]. We observed that fewer than
1% of location requests were made when the applica-
tions were visible to the user or resulted in the display-
ing of a GPS notiﬁcation icon. Given that Thompson et
al. showed that most users do not understand that appli-
cations running in the background may have the same
abilities as applications running in the foreground [42],
it is likely that in the vast majority of cases, users do not
know when their locations are being disclosed.

This low visibility rate is because Android only shows a
notiﬁcation icon when the GPS sensor is accessed, while
offering alternative ways of inferring location. In 66.1%
of applications’ location requests, they directly queried
the TelephonyManager, which can be used to deter-
mine location via cellular tower information. In 33.3%
of the cases, applications requested the SSIDs of nearby
WiFi networks. In the remaining 0.6% of cases, applica-

USENIX Association  

24th USENIX Security Symposium  505

7

tions accessed location information using one of three
built-in location providers: GPS, network, or passive.
Applications accessed the GPS location provider only
6% of the time (which displayed a GPS notiﬁcation).
In the other 94% of the time, 13% queried the network
provider (i.e., approximate location based on nearby cel-
lular towers and WiFi SSIDs) and 81% queried the pas-
sive location provider. The passive location provider
caches prior requests made to either the GPS or network
providers. Thus, across all requests for location data, the
GPS notiﬁcation icon appeared 0.04% of the time.

While the alternatives to querying the GPS are less ac-
curate, users are still surprised by their accuracy [17].
This suggests a serious violation of contextual integrity,
since users likely have no idea their locations are being
requested in the vast majority of cases. Thus, runtime no-
tiﬁcations for location tracking need to be improved [18].

Apart from these invisible location requests, we also ob-
served applications reading stored SMS messages (125
times per user/day), reading browser history (5 times per
user/day), and accessing the camera (once per user/day).
Though the use of these permissions does not necessarily
lead to privacy violations, users have no contextual cues
to understand that these requests are occurring.

4.2 High Frequency Requests
Some permission requests occurred so frequently that a
few applications (i.e., Facebook, Facebook Messenger,
Google Location Reporting, Google Maps, Farm Heroes
Saga) had to be rate limited in our log ﬁles (see Sec-
tion 3.1.1), so that the logs would not ﬁll up users’ re-
maining storage or incur performance overhead. Table 4
shows the complete list of application/permission com-
binations that exceeded the threshold. For instance, the
most frequent requests came from Facebook requesting
ACCESS NETWORK STATE with an average interval
of 213.88 ms (i.e., almost 5 times per second).

it

is regained.

With the exception of Google’s applications, all rate-
limited applications made excessive requests for the
connectivity state. We hypothesize that once these
applications lose connectivity,
they continuously poll
the system until
Their use of the
getActiveNetworkInfo() method results in permis-
sion checks and returns NetworkInfo objects, which al-
low them to determine connection state (e.g., connected,
disconnected, etc.) and type (e.g., WiFi, Bluetooth, cel-
lular, etc.). Thus, these requests do not appear to be leak-
ing sensitive information per se, but their frequency may
have adverse effects on performance and battery life.
It is possible that using the ConnectivityManager’s
NetworkCallback method may be able to fulﬁll this
need with far fewer permission checks.

Application / Permission
com.facebook.katana
ACCESS NETWORK STATE
com.facebook.orca
ACCESS NETWORK STATE
com.google.android.apps.maps
ACCESS NETWORK STATE
com.google.process.gapps
AUTHENTICATE ACCOUNTS
com.google.process.gapps
WAKE LOCK
com.google.process.location
WAKE LOCK
com.google.process.location
ACCESS FINE LOCATION
com.google.process.location
GET ACCOUNTS
com.google.process.location
ACCESS WIFI STATE
com.king.farmheroessaga
ACCESS NETWORK STATE
com.pandora.android
ACCESS NETWORK STATE
com.taptu.streams
ACCESS NETWORK STATE

Peak (ms) Avg. (ms)

213.88

956.97

334.78

1146.05

247.89

624.61

315.31

315.31

898.94

1400.20

176.11

991.46

1387.26

1387.26

373.41

1878.88

1901.91

1901.91

284.02

731.27

541.37

541.37

1746.36

1746.36

Table 4: The application/permission combinations that
needed to be rate limited during the study. The last two
columns show the fastest interval recorded and the aver-
age of all the intervals recorded before rate-limiting.

4.3 Frequency of Data Exposure
Felt et al. posited that while most permissions can be
granted automatically in order to not habituate users to
relatively benign risks, certain requests should require
runtime consent [14]. They advocated using runtime di-
alogs before the following actions should proceed:

1. Reading location information (e.g., using conven-

tional location APIs, scanning WiFi SSIDs, etc.).

2. Reading the user’s web browser history.
3. Reading saved SMS messages.
4. Sending SMS messages that incur charges, or inap-

propriately spamming the user’s contact list.

These four actions are governed by the 12 Android per-
missions listed in Table 1. Of the 300 applications that
we observed during the experiment, 91 (30.3%) per-
formed one of these actions. On average, these permis-
sions were requested 213 times per hour/user—roughly
every 20 seconds. However, permission checks occur un-
der a variety of circumstances, only a subset of which ex-
pose sensitive resources. As a result, platform develop-

506  24th USENIX Security Symposium 

USENIX Association

8

Resource

Location
Read SMS data
Sending SMS
Browser History
Total

Visible

Data Exposed Requests
2,205
486
7
14
2,712

758
378
7
12
1,155

Invisible

Data Exposed Requests
8,755
125
1
5
8,886

3,881
72
1
2
3,956

Total

Data Exposed Requests
10,960
611
8
19
11,598

4,639
450
8
14
5,111

Table 5: The sensitive permission requests (per user/day) when requesting applications were visible/invisible to users.
“Data exposed” reﬂects the subset of permission-protected requests that resulted in sensitive data being accessed.

ers may decide to only show runtime warnings to users
when protected data is read or modiﬁed. Thus, we at-
tempted to quantify the frequency with which permission
checks actually result in access to sensitive resources for
each of these four categories. Table 5 shows the number
of requests seen per user/day under each of these four
categories, separating the instances in which sensitive
data was exposed from the total permission requests ob-
served. Unlike Section 4.1, we include “visible” permis-
sion requests (i.e., those occurring while the user was ac-
tively using the application or had other contextual infor-
mation to indicate it was running). We didn’t observe any
uses of NFC, READ CALL LOG, ADD VOICEMAIL,
accessing WRITE SYNC SETTINGS or INTERNET
while roaming in our dataset.

location provider

Of the location permission checks, a majority were
due to requests for
information
(e.g., getBestProvider() returns the best location
provider based on application requirements), or check-
ing WiFi state (e.g., getWifiState() only reveals
whether WiFi
is enabled). Only a portion of the
requests actually exposed participants’ locations (e.g.,
getLastKnownLocation() or getScanResults()
exposed SSIDs of nearby WiFi networks).

Although a majority of requests for the READ SMS per-
mission exposed content in the SMS store (e.g., Query()
reads the contents of the SMS store), a considerable por-
tion simply read information about the SMS store (e.g.,
renewMmsConnectivity() resets an applications’ con-
nection to the MMS store). An exception to this is the use
of SEND SMS, which resulted in the transmission of an
SMS message every time the permission was requested.

Regarding browser history, both accessing visited URLs
(getAllVisitedUrls()) and reorganizing bookmark
folders (addFolderToCurrent()) result in the same
permission being checked. However, the latter does not
expose speciﬁc URLs to the invoking application.

Our analysis of the API calls indicated that on average,
only half of all permission checks granted applications
access to sensitive data. For instance, across both visible

and invisible requests, 5,111 of the 11,598 (44.3%) per-
mission checks involving the 12 permissions in Table 1
resulted in the exposure of sensitive data (Table 5).

While limiting runtime permission requests to only the
cases in which protected resources are exposed will
greatly decrease the number of user interruptions, the fre-
quency with which these requests occur is still too great.
Prompting the user on the ﬁrst request is also not appro-
priate (e.g., `a la iOS and Android M), because our data
show that in the vast majority of cases, the user has no
contextual cues to understand when protected resources
are being accessed. Thus, a user may grant a request the
ﬁrst time an application asks, because it is appropriate in
that instance, but then she may be surprised to ﬁnd that
the application continues to access that resource in other
contexts (e.g., when the application is not actively used).
As a result, a more intelligent method is needed to de-
termine when a given permission request is likely to be
deemed appropriate by the user.

5 User Expectations and Reactions
To identify when users might want to be prompted
about permission requests, our exit survey focused on
participants’ reactions to the 12 permissions in Ta-
ble 1, limiting the number of requests shown to each
participant based on our reservoir sampling algorithm,
which was designed to ask participants about a diverse
set of permission/application combinations. We col-
lected participants’ reactions to 673 permission requests
(≈19/participant). Of these, 423 included screenshots
because participants were actively using their phones
when the requests were made, whereas 250 permission
requests were performed while device screens were off.2
Of the former, 243 screenshots were taken while the re-
questing application was visible (Category 1 and 3 from
Section 4.1), whereas 180 were taken while the applica-
tion was invisible (Category 2 and 4 from Section 4.1). In
this section, we describe the situations in which requests

2Our ﬁrst 11 participants did not answer questions about permission
requests occurring while not using their devices, and therefore the data
only corresponds to our last 25 participants.

USENIX Association  

24th USENIX Security Symposium  507

9

deﬁed users’ expectations. We present explanations for
why participants wanted to block certain requests, the
factors inﬂuencing those decisions, and how expectations
changed when devices were not in use.

5.1 Reasons for Blocking
When viewing screenshots of what they were doing
when an application requested a permission, 30 partic-
ipants (80% of 36) stated that they would have preferred
to block at least one request, whereas 6 stated a willing-
ness to allow all requests, regardless of resource type or
application. Across the entire study, participants wanted
to block 35% of these 423 permission requests. When we
asked participants to explain their rationales for these de-
cisions, two main themes emerged: the request did not—
in their minds—pertain to application functionality or it
involved information they were uncomfortable sharing.

5.1.1 Relevance to Application Functionality
When prompted for the reason behind blocking a permis-
sion request, 19 (53% of 36) participants did not believe
it was necessary for the application to perform its task.
Of the 149 (35% of 423) requests that participants would
have preferred to block, 79 (53%) were perceived as be-
ing irrelevant to the functionality of the application:

location.” (P1)

• “It wasn’t doing anything that needed my current
• “I don’t understand why this app would do anything

with SMS.” (P10)

Accordingly, functionality was the most common reason
for wanting a permission request to proceed. Out of the
274 permissible requests, 195 (71% of 274) were per-
ceived as necessary for the core functionality of the ap-
plication, as noted by thirty-one (86% of 36) participants:
• “Because it’s a weather app and it needs to
know where you are to give you weather informa-
tion.”(P13)

• “I think it needs to read the SMS to keep track of the

chat conversation.”(P12)

Beyond being necessary for core functionality, partici-
pants wanted 10% (27 of 274) of requests to proceed be-
cause they offered convenience; 90% of these requests
were for location data, and the majority of those appli-
cations were published under the Weather, Social, and
Travel & Local categories in the Google Play store:

scroll through the whole list.” (P0)

• “It selects the closest stop to me so I don’t have to
• “This app should read my current location. I’d like
for it to, so I won’t have to manually enter in my zip
code / area.” (P4)

Thus, requests were allowed when they were expected:
when participants rated the extent to which each request
was expected on a 5-point Likert scale, allowable re-
quests averaged 3.2, whereas blocked requests averaged
2.3 (lower is less expected).

5.1.2 Privacy Concerns
Participants also wanted to deny permission requests that
involved data that they considered sensitive, regardless
of whether they believed the application actually needed
the data to function. Nineteen (53% of 36) participants
noted privacy as a concern while blocking a request, and
of the 149 requests that participants wanted to block, 49
(32% of 149) requests were blocked for this reason:

• “SMS messages are quite personal.” (P14)
• “It is part of a personal conversation.” (P11)
• “Pictures could be very private and I wouldn’t like

for anybody to have access.” (P16)

Conversely, 24 participants (66% of 36) wanted requests
to proceed simply because they did not believe that the
data involved was particularly sensitive; this reasoning
accounted for 21% of the 274 allowable requests:

cerns.” (P3)

• “I’m ok with my location being recorded, no con-
• “No personal info being shared.” (P29)
5.2
Based on participants’ responses to the 423 permission
requests involving screenshots (i.e., requests occurring
while they were actively using their phones), we quan-
titatively examined how various factors inﬂuenced their
desire to block some of these requests.

Inﬂuential Factors

Effects of Identifying Permissions on Blocking: In the
exit survey, we asked participants to guess the permis-
sion an application was requesting, based on the screen-
shot of what they were doing at the time. The real an-
swer was among four other incorrect answers. Of the
149 cases where participants wanted to block permission
requests, they were only able to correctly state what per-
mission was being requested 24% of the time; whereas
when wanting a request to proceed, they correctly iden-
tiﬁed the requested permission 44% (120 of 274) of the
time. However, Pearson’s product-moment test on the
average number of blocked requests per user and the av-
erage number of correct answers per user3 did not yield a
statistically signiﬁcant correlation (r=−0.171, p<0.317).
Effects of Visibility on Expectations: We were particu-
larly interested in exploring if permission requests orig-
inating from foreground applications (i.e., visible to the

3Both measures were normally distributed.

508  24th USENIX Security Symposium 

USENIX Association

10

user) were more expected than ones from background ap-
plications. Of the 243 visible permission requests that
we asked about in our exit survey, participants correctly
identiﬁed the requested permission 44% of the time, and
their average rating on our expectation scale was 3.4. On
the other hand, participants correctly identiﬁed the re-
sources accessed by background applications only 29%
of the time (52 of 180), and their average rating on our
expectation scale was 3.0. A Wilcoxon Signed-Rank
test with continuity correction revealed a statistically sig-
niﬁcant difference in participants’ expectations between
these two groups (V=441.5, p<0.001).

Effects of Visibility on Blocking: Participants wanted
to block 71 (29% of 243) permission requests originat-
ing from applications running in the foreground, whereas
this increased by almost 50% when the applications were
in the background invisible to them (43% of 180). We
calculated the percentage of denials for each partici-
pant, for both visible and invisible requests. A Wilcoxon
Signed-Rank test with continuity correction revealed a
statistically signiﬁcant difference (V=58, p<0.001).

Effects of Privacy Preferences on Blocking: Partici-
pants completed the Privacy Concerns Scale (PCS) [10]
and the Internet Users’ Information Privacy Concerns
(IUIPC) scale [31]. A Spearman’s rank test yielded no
statistically signiﬁcant correlation between their privacy
preferences and their desire to block permission requests
(ρ = 0.156, p<0.364).

Effects of Expectations on Blocking: We examined
whether participants’ expectations surrounding requests
correlated with their desire to block them. For each par-
ticipant, we calculated their average Likert scores for
their expectations and the percentage of requests that
they wanted to block. Pearson’s product-moment test
showed a statistically signiﬁcant correlation (r=−0.39,
p<0.018). The negative correlation shows that partici-
pants were more likely to deny unexpected requests.

5.3 User Inactivity and Resource Access
In the second part of the exit survey, participants an-
swered questions about 10 resource requests that oc-
curred when the screen was off (not in use). Overall,
they were more likely to expect resource requests to oc-
cur when using their devices (µ = 3.26 versus µ = 2.66).
They also stated a willingness to block almost half of
the permission requests (49.6% of 250) when not in use,
compared to a third of the requests that occurred when
using their phones (35.2% of 423). However, neither of
these differences was statistically signiﬁcant.

6 Feasibility of Runtime Requests
Felt et al. posited that certain sensitive permissions (Ta-
ble 1) should require runtime consent [14], but in Section
4.3 we showed that the frequencies with which applica-
tions are requesting these permissions make it impracti-
cal to prompt the user each time a request occurs. In-
stead, the major mobile platforms have shifted towards a
model of prompting the user the ﬁrst time an application
requests access to certain resources: iOS does this for a
selected set of resources, such as location and contacts,
and Android M does this for “dangerous” permissions.

How many prompts would users see, if we added runtime
prompts for the ﬁrst use of these 12 permissions? We an-
alyzed a scheme where a runtime prompt is displayed at
most once for each unique triplet of (application, permis-
sion, application visibility), assuming the screen is on.
With a na¨ıve scheme, our study data indicates our partic-
ipants would have seen an average of 34 runtime prompts
(ranging from 13 to 77, σ=11). As a reﬁnement, we pro-
pose that the user should be prompted only if sensitive
data will be exposed (Section 4.3), reducing the average
number of prompts to 29.

Of these 29 prompts, 21 (72%) are related to location.
Apple iOS already prompts users when an application ac-
cesses location for the ﬁrst time, with no evidence of user
habituation or annoyance. Focusing on the remaining
prompts, we see that our policy would introduce an aver-
age of 8 new prompts per user: about 5 for reading SMS,
1 for sending SMS, and 2 for reading browser history.
Our data covers only the ﬁrst week of use, but as we only
prompt on ﬁrst use of a permission, we expect that the
number of prompts would decline greatly in subsequent
weeks, suggesting that this policy would likely not intro-
duce signiﬁcant risk of habituation or annoyance. Thus,
our results suggest adding runtime prompts for reading
SMS, sending SMS, and reading browser history would
be useful given their sensitivity and low frequency.

Our data suggests that taking visibility into account is
important.
If we ignore visibility and prompted only
once for each pair of (application, permission), users
would have no way to select a different policy for when
the application is visible or not visible. In contrast, “ask-
on-ﬁrst-use” for the triple (application, permission, visi-
bility) gives users the option to vary their decision based
on the visibility of the requesting application. We evalu-
ated these two policies by analyzing the exit survey data
(limited to situations where the screen was on) for cases
where the same user was asked twice in the survey about
situations with the same (application, permission) pair
or the same (application, permission, visibility) triplet,
to see whether the user’s ﬁrst decision to block or not
matched their subsequent decisions. For the former pol-

USENIX Association  

24th USENIX Security Symposium  509

11

icy, we saw only 51.3% agreement; for the latter, agree-
ment increased to 83.5%. This suggests that the (applica-
tion, permission, visibility) triplet captures many of the
contextual factors that users care about, and thus it is rea-
sonable to prompt only once per unique triplet.

A complicating factor is that applications can also run
even when the user is not actively using the phone. In
addition to the 29 prompts mentioned above, our data
indicates applications would have triggered an average
of 7 more prompts while the user was not actively using
the phone: 6 for location and one for reading SMS. It
is not clear how to handle prompts when the user is not
available to respond to the prompt: attribution might be
helpful, but further research is needed.

6.1 Modeling Users’ Decisions
We constructed several statistical models to examine
whether users’ desire to block certain permission re-
quests could be predicted using the contextual data that
we collected.
If such a relationship exists, a classiﬁer
could determine when to deny potentially unexpected
permission requests without user intervention. Con-
versely, the classiﬁer could be used to only prompt the
user about questionable data requests. Thus, the response
variable in our models is the user’s choice of whether to
block the given permission request. Our predictive vari-
ables consisted of the information that might be available
at runtime: permission type (with the restriction that the
invoked function exposes data), requesting application,
and visibility of that application. We constructed sev-
eral mixed effects binary logistic regression models to
account for both inter-subject and intra-subject effects.

6.1.1 Model Selection
In our mixed effects models, permission types and the
visibility of the requesting application were ﬁxed effects,
because all possible values for each variable existed in
our data set. Visibility had two values: visible (the user
is interacting with the application or has other contextual
cues to know that it is running) and invisible. Permission
types were categorized based on Table 5. The application
name and the participant ID were included as random ef-
fects, because our survey data did not have an exhaustive
list of all possible applications a user could run, and the
participant has a non-systematic effect on the data.

Table 6 shows two goodness-of-ﬁt metrics: the Akaike
Information Criterion (AIC) and Bayesian Information
Criterion (BIC). Lower values for AIC and BIC repre-
sent better ﬁt. Table 6 shows the different parameters
included in each model. We found no evidence of inter-
action effects and therefore did not include them. Visual
inspection of residual plots of each model did not reveal
obvious deviations from homoscedasticity or normality.

Predictors
UserCode
Application
Application
UserCode
Permission
Application
UserCode
Visibility
Application
UserCode
Permission
Visibility
Application
UserCode
UserCode
Application
Application
UserCode
Permission
Application
UserCode

AIC
490.60
545.98

BIC
498.69
554.07

Screen State
Screen On
Screen On

491.86

503.99

Screen On

494.69

527.05

Screen On

481.65

497.83

Screen On

484.23

520.64

Screen On

245.13
349.38

252.25
356.50

Screen Off
Screen Off

238.84

249.52

Screen Off

235.48

263.97

Screen Off

Table 6: Goodness-of-ﬁt metrics for various mixed ef-
fects logistic regression models on the exit survey data.

We initially included the phone’s screen state as another
variable. However, we found that creating two separate
models based on the screen state resulted in better ﬁt
than using a single model that accounted for screen state
as a ﬁxed effect. When the screen was on, the best ﬁt
was a model including application visibility and appli-
cation name, while controlling for subject effects. Here,
ﬁt improved once permission type was removed from the
model, which shows that the decision to block a permis-
sion request was based on contextual factors: users do
not categorically deny permission requests based solely
on the type of resource being accessed (i.e., they also ac-
count for their trust in the application, as well as whether
they happened to be actively using it). When the screen
was off, however, the effect of permission type was rela-
tively stronger. The strong subject effect in both models
indicates that these decisions vary from one user to the
next. As a result, any classiﬁer developed to automati-
cally decide whether to block a permission at runtime (or
prompt the user) will need to be tailored to that particular
user’s needs.

6.1.2 Predicting User Reactions
Using these two models, we built two classiﬁers to make
decisions about whether to block any of the sensitive per-
mission requests listed in Table 5. We used our exit sur-
vey data as ground truth, and used 5-fold cross-validation
to evaluate model accuracy.

510  24th USENIX Security Symposium 

USENIX Association

12

We calculated the receiver operating characteristic
(ROC) to capture the tradeoff between true-positive and
false-positive rate. The quality of the classiﬁer can be
quantiﬁed with a single value by calculating the area un-
der its ROC curve (AUC) [23]. The closer the AUC gets
to 1.0, the better the classiﬁer is. When screens were on,
the AUC was 0.7, which is 40% better than the random
baseline (0.5). When screens were off, the AUC was 0.8,
which is 60% better than a random baseline.

7 Discussion
During the study, 80% of our participants deemed at least
one permission request as inappropriate. This violates
Nissenbaum’s notion of “privacy as contextual integrity”
because applications were performing actions that deﬁed
users’ expectations [33]. Felt et al. posited that users may
be able to better understand why permission requests are
needed if some of these requests are granted via runtime
consent dialogs, rather than Android’s previous install-
time notiﬁcation approach [14]. By granting permissions
at runtime, users will have additional contextual infor-
mation; based on what they were doing at the time that
resources are requested, they may have a better idea of
why those resources are being requested.

We make two primary contributions that system design-
ers can use to make more usable permissions systems.
We show that the visibility of the requesting applica-
tion and the frequency at which requests occur are two
important factors in designing a runtime consent plat-
form. Also, we show that “prompt-on-ﬁrst-use” per
triplet could be implemented for some sensitive permis-
sions without risking user habituation or annoyance.

Based on the frequency with which runtime permissions
are requested (Section 4), it is infeasible to prompt users
every time. Doing so would overwhelm them and lead to
habituation. At the same time, drawing user attention to
the situations in which users are likely to be concerned
will lead to greater control and awareness. Thus, the
challenge is in acquiring their preferences by confronting
them minimally and then automatically inferring when
users are likely to ﬁnd a permission request unexpected,
and only prompting them in these cases. Our data sug-
gests that participants’ desires to block particular permis-
sions were heavily inﬂuenced by two main factors: their
understanding of the relevance of a permission request to
the functionality of the requesting application and their
individual privacy concerns.

Our models in Section 6.1 showed that individual char-
acteristics greatly explain the variance between what dif-
ferent users deem appropriate, in terms of access to pro-
tected resources. While responses to privacy scales failed
to explain these differences, this was not a surprise, as the

disconnect between stated privacy preferences and be-
haviors is well-documented (e.g., [1]). This means that
in order to accurately model user preferences, the sys-
tem will need to learn what a speciﬁc user deems in-
appropriate over time. Thus, a feedback loop is likely
needed: when devices are “new,” users will be required
to provide more input surrounding permission requests,
and then based on their responses, they will see fewer
requests in the future. Our data suggests that prompting
once for each unique (application, permission, applica-
tion visibility) triplet can serve as a practical mechanism
in acquiring users’ privacy preferences.

Beyond individual subject characteristics (i.e., personal
preferences), participants based their decisions to block
certain permission requests on the speciﬁc applications
making the requests and whether they had contextual
cues to indicate that the applications were running (and
therefore needed the data to function). Future systems
could take these factors into account when deciding
whether or not to draw user attention to a particular re-
quest. For example, when an application that a user is
not actively using requests access to a protected resource,
she should be shown a runtime prompt. Our data indi-
cates that, if the user decides to grant a request in this
situation, then with probability 0.84 the same decision
will hold in future situations where she is actively using
that same application, and therefore a subsequent prompt
may not be needed. At a minimum, platforms need to
treat permission requests from background applications
differently than those originating from foreground ap-
plications. Similarly, applications running in the back-
ground should use passive indicators to communicate
when they are accessing particular resources. Platforms
can also be designed to make decisions about whether
or not access to resources should be granted based on
whether contextual cues are present, or at its most basic,
whether the device screen is even on.

Finally, we built our models and analyzed our data within
the framework of what resources our participants be-
lieved were necessary for applications to correctly func-
tion. Obviously, their perceptions may have been incor-
rect: if they better understood why a particular resource
was necessary, they may have been more permissive.
Thus, it is incumbent on developers to adequately com-
municate why particular resources are needed, as this im-
pacts user notions of contextual integrity. Yet, no mecha-
nisms in Android exist for developers to do this as part of
the permission-granting process. For example, one could
imagine requiring metadata to be provided that explains
how each requested resource will be used, and then auto-
matically integrating this information into permission re-
quests. Tan et al. examined a similar feature on iOS that
allows developers to include free-form text in runtime

USENIX Association  

24th USENIX Security Symposium  511

13

permission dialogs and observed that users were more
likely to grant requests that included this text [41]. Thus,
we believe that including succinct explanations in these
requests would help preserve contextual integrity by pro-
moting greater transparency.
In conclusion, we believe this study was instructive in
showing the circumstances in which Android permission
requests are made under real-world usage. While prior
work has already identiﬁed some limitations of deployed
mobile permissions systems, we believe our study can
beneﬁt system designers by demonstrating several ways
in which contextual integrity can be improved, thereby
empowering users to make better security decisions.

Acknowledgments
This work was supported by NSF grant CNS-1318680,
by Intel through the ISTC for Secure Computing, and by
the AFOSR under MURI award FA9550-12-1-0040.

References
[1] ACQUISTI, A., AND GROSSKLAGS, J. Privacy and rational-
ity in individual decision making.
IEEE Security & Privacy
(January/February 2005), 24–30. http://www.dtc.umn.edu/
weis2004/acquisti.pdf.

[2] ALMOHRI, H. M., YAO, D. D., AND KAFURA, D. Droidbarrier:
Know what is executing on your android. In Proc. of the 4th ACM
Conf. on Data and Application Security and Privacy (New York,
NY, USA, 2014), CODASPY ’14, ACM, pp. 257–264.
System permissions.

[3] ANDROID DEVELOPERS.

http:

//developer.android.com/guide/topics/security/
permissions.html. Accessed: November 11, 2014.

[4] ANDROID DEVELOPERS.

Common Intents.

https://

developer.android.com/guide/components/intents-
common.html, 2014. Accessed: November 12, 2014.
Content Providers.

[5] ANDROID DEVELOPERS.

http:

//developer.android.com/guide/topics/providers/
content-providers.html, 2014. Accessed: Nov. 12, 2014.

[6] AU, K. W. Y., ZHOU, Y. F., HUANG, Z., AND LIE, D. Pscout:
Analyzing the android permission speciﬁcation. In Proc. of the
2012 ACM Conf. on Computer and Communications Security
(New York, NY, USA, 2012), CCS ’12, ACM, pp. 217–228.

[7] BARRERA, D., CLARK,

J., MCCARNEY, D., AND VAN
OORSCHOT, P. C. Understanding and improving app installation
security mechanisms through empirical analysis of android.
In
Proceedings of the Second ACM Workshop on Security and Pri-
vacy in Smartphones and Mobile Devices (New York, NY, USA,
2012), SPSM ’12, ACM, pp. 81–92.

[8] BARRERA, D., KAYACIK, H. G. U. C., VAN OORSCHOT, P. C.,
AND SOMAYAJI, A. A methodology for empirical analysis of
permission-based security models and its application to android.
In Proc. of the ACM Conf. on Comp. and Comm. Security (New
York, NY, USA, 2010), CCS ’10, ACM, pp. 73–84.

[9] BODDEN, E. Easily instrumenting android applications for secu-
rity purposes. In Proc. of the ACM Conf. on Comp. and Comm.
Sec. (NY, NY, USA, 2013), CCS ’13, ACM, pp. 1499–1502.

[10] BUCHANAN, T., PAINE, C., JOINSON, A. N., AND REIPS, U.-
D. Development of measures of online privacy concern and pro-
tection for use on the internet. Journal of the American Society
for Information Science and Technology 58, 2 (2007), 157–165.

[11] BUGIEL, S., HEUSER, S., AND SADEGHI, A.-R. Flexible and
ﬁne-grained mandatory access control on android for diverse se-
curity and privacy policies. In Proc. of the 22nd USENIX Security
Symposium (Berkeley, CA, USA, 2013), SEC’13, USENIX As-
sociation, pp. 131–146.

[12] ENCK, W., GILBERT, P., CHUN, B.-G., COX, L. P., JUNG, J.,
MCDANIEL, P., AND SHETH, A. N. Taintdroid: an information-
ﬂow tracking system for realtime privacy monitoring on smart-
phones. In Proceedings of the 9th USENIX Conference on Oper-
ating Systems Design and Implementation (Berkeley, CA, USA,
2010), OSDI’10, USENIX Association, pp. 1–6.

[13] FELT, A. P., CHIN, E., HANNA, S., SONG, D., AND WAGNER,
D. Android permissions demystiﬁed. In Proc. of the ACM Conf.
on Comp. and Comm. Sec. (New York, NY, USA, 2011), CCS
’11, ACM, pp. 627–638.

[14] FELT, A. P., EGELMAN, S., FINIFTER, M., AKHAWE, D., AND
WAGNER, D. How to ask for permission. In Proceedings of the
7th USENIX conference on Hot Topics in Security (Berkeley, CA,
USA, 2012), HotSec’12, USENIX Association, pp. 7–7.

[15] FELT, A. P., EGELMAN, S., AND WAGNER, D.

I’ve got 99
problems, but vibration ain’t one: a survey of smartphone users’
concerns. In Proc. of the 2nd ACM workshop on Security and Pri-
vacy in Smartphones and Mobile devices (New York, NY, USA,
2012), SPSM ’12, ACM, pp. 33–44.

[16] FELT, A. P., HA, E., EGELMAN, S., HANEY, A., CHIN, E.,
AND WAGNER, D. Android permissions: user attention, compre-
hension, and behavior. In Proceedings of the Eighth Symposium
on Usable Privacy and Security (New York, NY, USA, 2012),
SOUPS ’12, ACM, pp. 3:1–3:14.

[17] FU, H., AND LINDQVIST, J. General area or approximate loca-
tion?: How people understand location permissions. In Proceed-
ings of the 13th Workshop on Privacy in the Electronic Society
(2014), ACM, pp. 117–120.

[18] FU, H., YANG, Y., SHINGTE, N., LINDQVIST, J., AND
GRUTESER, M. A ﬁeld study of run-time location access dis-
closures on android smartphones. Proc. USEC 14 (2014).

[19] GIBLER, C., CRUSSELL, J., ERICKSON, J., AND CHEN, H. An-
droidleaks: Automatically detecting potential privacy leaks in an-
droid applications on a large scale. In Proc. of the 5th Intl. Conf.
on Trust and Trustworthy Computing (Berlin, Heidelberg, 2012),
TRUST’12, Springer-Verlag, pp. 291–307.

[20] GORLA, A., TAVECCHIA, I., GROSS, F., AND ZELLER, A.
Checking app behavior against app descriptions. In Proceedings
of the 36th International Conference on Software Engineering
(New York, NY, USA, 2014), ICSE 2014, ACM, pp. 1025–1035.
[21] HARBACH, M., HETTIG, M., WEBER, S., AND SMITH, M. Us-
ing personal examples to improve risk communication for secu-
rity & privacy decisions. In Proc. of the 32nd Annual ACM Conf.
on Human Factors in Computing Systems (New York, NY, USA,
2014), CHI ’14, ACM, pp. 2647–2656.

[22] HARBACH, M., VON ZEZSCHWITZ, E., FICHTNER, A.,
DE LUCA, A., AND SMITH, M.
It’sa hard lock life: A ﬁeld
study of smartphone (un) locking behavior and risk perception.
In Symposium on Usable Privacy and Security (SOUPS) (2014).
[23] HASTIE, T., TIBSHIRANI, R., FRIEDMAN, J., AND FRANKLIN,
J. The elements of statistical learning: data mining, inference and
prediction. The Mathematical Intelligencer 27, 2 (2005), 83–85.
[24] HORNYACK, P., HAN, S., JUNG, J., SCHECHTER, S., AND
WETHERALL, D. These aren’t the droids you’re looking for:
retroﬁtting android to protect data from imperious applications.
In Proc. of the ACM Conf. on Comp. and Comm. Sec. (New York,
NY, USA, 2011), CCS ’11, ACM, pp. 639–652.

512  24th USENIX Security Symposium 

USENIX Association

14

[25] JUNG, J., HAN, S., AND WETHERALL, D. Short paper: En-
hancing mobile application permissions with runtime feedback
and constraints. In Proceedings of the Second ACM Workshop on
Security and Privacy in Smartphones and Mobile Devices (New
York, NY, USA, 2012), SPSM ’12, ACM, pp. 45–50.

[40] STEVENS, R., GANZ, J., FILKOV, V., DEVANBU, P., AND
CHEN, H. Asking for (and about) permissions used by an-
droid apps. In Proc. of the 10th Working Conf. on Mining Soft-
ware Repositories (Piscataway, NJ, USA, 2013), MSR ’13, IEEE
Press, pp. 31–40.

[26] KELLEY, P. G., CONSOLVO, S., CRANOR, L. F., JUNG, J.,
SADEH, N., AND WETHERALL, D. A conundrum of permis-
sions: Installing applications on an android smartphone. In Proc.
of the 16th Intl. Conf. on Financial Cryptography and Data Sec.
(Berlin, Heidelberg, 2012), FC’12, Springer-Verlag, pp. 68–79.

[41] TAN,

J., NGUYEN, K., THEODORIDES, M., NEGRON-
ARROYO, H., THOMPSON, C., EGELMAN, S., AND WAGNER,
D. The effect of developer-speciﬁed explanations for permission
requests on smartphone user behavior. In Proc. of the SIGCHI
Conf. on Human Factors in Computing Systems (2014).

[27] KELLEY, P. G., CRANOR, L. F., AND SADEH, N. Privacy as
part of the app decision-making process. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems
(New York, NY, USA, 2013), CHI ’13, ACM, pp. 3393–3402.

[28] KLIEBER, W., FLYNN, L., BHOSALE, A., JIA, L., AND
BAUER, L. Android taint ﬂow analysis for app sets.
In Pro-
ceedings of the 3rd ACM SIGPLAN International Workshop on
the State of the Art in Java Program Analysis (New York, NY,
USA, 2014), SOAP ’14, ACM, pp. 1–6.

[29] LARSON, R., AND CSIKSZENTMIHALYI, M. New directions for
naturalistic methods in the behavioral sciences. In The Experi-
ence Sampling Method, H. Reis, Ed. Jossey-Bass, San Francisco,
1983, pp. 41–56.

[30] LIN, J., SADEH, N., AMINI, S., LINDQVIST, J., HONG, J. I.,
AND ZHANG, J. Expectation and purpose: understanding users’
mental models of mobile app privacy through crowdsourcing. In
Proc. of the 2012 ACM Conf. on Ubiquitous Computing (New
York, NY, USA, 2012), UbiComp ’12, ACM, pp. 501–510.

[31] MALHOTRA, N. K., KIM, S. S., AND AGARWAL, J. Internet
Users’ Information Privacy Concerns (IUIPC): The Construct,
The Scale, and A Causal Model. Information Systems Research
15, 4 (December 2004), 336–355.

[32] NISSENBAUM, H. Privacy as contextual integrity. Washington

Law Review 79 (February 2004), 119.

[33] NISSENBAUM, H. Privacy in context: Technology, policy, and

the integrity of social life. Stanford University Press, 2009.

[34] O’GRADY, J. D. New privacy enhancements coming to ios
http://www.zdnet.com/new-privacy-

8 in the fall.
enhancements-coming-to-ios-8-in-the-fall-
7000030903/, June 25 2014. Accessed: Nov. 11, 2014.

[35] PANDITA, R., XIAO, X., YANG, W., ENCK, W., AND XIE, T.
WHYPER: Towards Automating Risk Assessment of Mobile Ap-
plications. In Proc. of the 22nd USENIX Sec. Symp. (Berkeley,
CA, USA, 2013), SEC’13, USENIX Association, pp. 527–542.

[36] SARMA, B. P., LI, N., GATES, C., POTHARAJU, R., NITA-
ROTARU, C., AND MOLLOY, I. Android permissions: A per-
spective combining risks and beneﬁts. In Proceedings of the 17th
ACM Symposium on Access Control Models and Technologies
(New York, NY, USA, 2012), SACMAT ’12, ACM, pp. 13–22.

[37] SHEBARO, B., OLUWATIMI, O., MIDI, D., AND BERTINO, E.
Identidroid: Android can ﬁnally wear its anonymous suit. Trans.
Data Privacy 7, 1 (Apr. 2014), 27–50.

[38] SHKLOVSKI, I., MAINWARING, S. D., SK ´ULAD ´OTTIR, H. H.,
AND BORGTHORSSON, H. Leakiness and creepiness in app
space: Perceptions of privacy and mobile app use. In Proc. of the
32nd Ann. ACM Conf. on Human Factors in Computing Systems
(New York, NY, USA, 2014), CHI ’14, ACM, pp. 2347–2356.

[39] SPREITZENBARTH, M.,

FREILING,

F.,
SCHRECK, T., AND HOFFMANN, J. Mobile-sandbox: Having a
deeper look into android applications. In Proceedings of the 28th
Annual ACM Symposium on Applied Computing (New York, NY,
USA, 2013), SAC ’13, ACM, pp. 1808–1815.

F., ECHTLER,

[42] THOMPSON, C., JOHNSON, M., EGELMAN, S., WAGNER, D.,
AND KING, J. When it’s better to ask forgiveness than get per-
mission: Designing usable audit mechanisms for mobile permis-
sions. In Proceedings of the 2013 Symposium on Usable Privacy
and Security (SOUPS) (2013).

[43] VITTER, J. S. Random sampling with a reservoir. ACM Trans.

Math. Softw. 11, 1 (Mar. 1985), 37–57.

[44] WEI, X., GOMEZ, L., NEAMTIU, I., AND FALOUTSOS, M. Per-
mission evolution in the android ecosystem. In Proceedings of the
28th Annual Computer Security Applications Conference (New
York, NY, USA, 2012), ACSAC ’12, ACM, pp. 31–40.

[45] WOODRUFF, A., PIHUR, V., CONSOLVO, S., BRANDIMARTE,
L., AND ACQUISTI, A. Would a privacy fundamentalist sell
their dna for $1000...if nothing bad happened as a result? the
westin categories, behavioral intentions, and consequences.
In
Proceedings of the 2014 Symposium on Usable Privacy and Se-
curity (2014), USENIX Association, pp. 1–18.

[46] XU, R., SA¨IDI, H., AND ANDERSON, R. Aurasium: Practical
policy enforcement for android applications. In Proc. of the 21st
USENIX Sec. Symp. (Berkeley, CA, USA, 2012), Security’12,
USENIX Association, pp. 27–27.

[47] ZHANG, Y., YANG, M., XU, B., YANG, Z., GU, G., NING, P.,
WANG, X. S., AND ZANG, B. Vetting undesirable behaviors in
android apps with permission use analysis. In Proc. of the ACM
Conf. on Comp. and Comm. Sec. (New York, NY, USA, 2013),
CCS ’13, ACM, pp. 611–622.

[48] ZHU, H., XIONG, H., GE, Y., AND CHEN, E. Mobile app
recommendations with security and privacy awareness. In Pro-
ceedings of the 20th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining (New York, NY, USA,
2014), KDD ’14, ACM, pp. 951–960.

A Invisible requests
Following list shows the set of applications that have requested the most
number of permissions while executing invisibly to the user and the
most requested permission types by each respective application.

LOCATION, ACCESS WIFI STATE ,WAKE LOCK,

GET ACCOUNTS, ACCESS COARSE LOCATION,

WIFI STATE, WAKE LOCK, READ PHONE STATE,

• Facebook App— ACCESS NETWORK STATE, ACCESS FINE
• Google Location—WAKE LOCK, ACCESS FINE LOCATION,
• Facebook Messenger—ACCESS NETWORK STATE, ACCESS
• Taptu DJ—ACCESS NETWORK STATE, INTERNET, NFC
• Google Maps—ACCESS NETWORK STATE, GET AC-
• Google (Gapps)—WAKE LOCK, ACCESS FINE LOCA-
TION, AUTHENTICATE ACCOUNTS, ACCESS NETWORK
STATE,

COUNTS, WAKE LOCK, ACCESS FINE LOCATION,

FINE LOCATION, INTERNET,

• Fouraquare—ACCESS WIFI STATE, WAKE LOCK, ACCESS
• Yahoo Weather—ACCESS FINE LOCATION, ACCESS NET-

WORK STATE, INTERNET, ACCESS WIFI STATE,

USENIX Association  

24th USENIX Security Symposium  513

15

NET, ACCESS FINE LOCATION,

• Devexpert Weather—ACCESS NETWORK STATE, INTER-
• Tile Game(Umoni)—ACCESS NETWORK STATE, WAKE

LOCK, INTERNET, ACCESS WIFI STATE,

Following is the most frequently requested permission type by appli-
cations while running invisibly to the user and the applications who
requested the respective permission type most.

(GMS), Facebook App, GTalk.

(Location),

Google (Login), Google (GM), Google (Vending)

(Location), Google

(Gapps),

Facebook Messenger, Google (Gapps), Taptu - DJ

(Gapps), Facebook App, Yahoo Weather, Rhapsody (Music)

• ACCESS NETWORK STATE— Facebook App, Google Maps,
• WAKE LOCK—Google (Location), Google (Gapps), Google
• ACCESS FINE LOCATION—Google
Google
• GET ACCOUNTS—Google
• ACCESS WIFI STATE—Google (Location), Google (Gapps),
• UPDATE DEVICE STATS—Google (SystemUI), Google (Loca-
• ACCESS COARSE LOCATION—Google (Location), Google
• AUTHENTICATE ACCOUNTS—Google (Gapps), Google (Lo-
• READ SYNC SETTINGS—Google (GM), Google ( GMS ), an-
• INTERNET—Google (Vending), Google (Gapps), Google (GM),

(Gapps), Google (News), Facebook App, Google Maps

droid.process.acore, Google (Email), Google (Gapps)

Facebook App, Foursqaure, Facebook Messenger

tion), Google (Gapps)

gin), Twitter, Yahoo Mail, Google (GMS)

Facebook App, Google (Location)

B Distribution of Requests
The following graph shows the distribution of requests throughout a
given day averaged across the data set.

0
0
0
6

0
0
0
5

0
0
0
4

0
0
0
3

0
0
0
2

0
0
0
1

0

s
t
s
e
u
q
e
R

 
f

o

 
r
e
b
m
u
N

5

10

15

20

Hour of the day

C Permission Type Breakdown
This table lists the most frequently used permissions during the study
period. (per user / per day)

Permission Type
ACCESS NETWORK STATE
WAKE LOCK
ACCESS FINE LOCATION
GET ACCOUNTS
UPDATE DEVICE STATS
ACCESS WIFI STATE
ACCESS COARSE LOCATION
AUTHENTICATE ACCOUNTS
READ SYNC SETTINGS
VIBRATE
INTERNET
READ SMS
READ PHONE STATE
STATUS BAR
WRITE SYNC SETTINGS
CHANGE COMPONENT ENABLED STATE
CHANGE WIFI STATE
READ CALENDAR
ACCOUNT MANAGER
ACCESS ALL DOWNLOADS
READ EXTERNAL STORAGE
USE CREDENTIALS
READ LOGS

Requests
41077
27030
7400
4387
2873
2092
1468
1335
836
740
739
611
345
290
206
197
168
166
134
127
126
101
94

D User Application Breakdown
This table shows the applications that most frequently requested access
to protected resources during the study period. (per user / per day)

Application Name
facebook.katana
google.process.location
facebook.orca
taptu.streams
google.android.apps.maps
google.process.gapps
yahoo.mobile.client.android.weather
tumblr
king.farmheroessaga
joelapenna.foursquared
telenav.app.android.scout us
devexpert.weather
ch.bitspin.timely
umonistudio.tile
king.candycrushsaga
android.systemui
bambuna.podcastaddict
contapps.android
handcent.nextsms
foursquare.robin
qisiemoji.inputmethod
devian.tubemate.home
lookout

Requests
40041
32426
24702
15188
6501
5340
5505
4251
3862
3729
3335
2909
2549
2478
2448
2376
2087
1662
1543
1408
1384
1296
1158

514  24th USENIX Security Symposium 

USENIX Association

16

