RAPPOR: Randomized Aggregatable Privacy-Preserving

Ordinal Response

Úlfar Erlingsson

Google, Inc.

ulfar@google.com

Vasyl Pihur
Google, Inc.

vpihur@google.com

Aleksandra Korolova

University of Southern California

korolova@usc.edu

ABSTRACT
Randomized Aggregatable Privacy-Preserving Ordinal Re-
sponse, or RAPPOR, is a technology for crowdsourcing statis-
tics from end-user client software, anonymously, with strong
privacy guarantees. In short, RAPPORs allow the forest of
client data to be studied, without permitting the possibil-
ity of looking at individual trees. By applying randomized
response in a novel manner, RAPPOR provides the mecha-
nisms for such collection as well as for eﬃcient, high-utility
analysis of the collected data. In particular, RAPPOR per-
mits statistics to be collected on the population of client-side
strings with strong privacy guarantees for each client, and
without linkability of their reports.

This paper describes and motivates RAPPOR, details its
diﬀerential-privacy and utility guarantees, discusses its prac-
tical deployment and properties in the face of diﬀerent attack
models, and, ﬁnally, gives results of its application to both
synthetic and real-world data.

Introduction

1
Crowdsourcing data to make better, more informed deci-
sions is becoming increasingly commonplace. For any such
crowdsourcing, privacy-preservation mechanisms should be
applied to reduce and control the privacy risks introduced
by the data collection process, and balance that risk against
the beneﬁcial utility of the collected data. For this purpose
we introduce Randomized Aggregatable Privacy-Preserving
Ordinal Response, or RAPPOR, a widely-applicable, practi-
cal new mechanism that provides strong privacy guarantees
combined with high utility, yet is not founded on the use of
trusted third parties.

RAPPOR builds on the ideas of randomized response, a
surveying technique developed in the 1960s for collecting
statistics on sensitive topics where survey respondents wish
to retain conﬁdentiality [27]. An example commonly used
to describe this technique involves a question on a sensi-
tive topic, such as “Are you a member of the Communist
party?” [28]. For this question, the survey respondent is

Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/authors. Copyright is held by the authors.
CCS’14, November 3–7, 2014, Scottsdale, Arizona, USA.
ACM 978-1-4503-2957-6/14/11, http://dx.doi.org/10.1145/2660267.2660348.

asked to ﬂip a fair coin, in secret, and answer “Yes” if it
comes up heads, but tell the truth otherwise (if the coin
comes up tails). Using this procedure, each respondent re-
tains very strong deniability for any “Yes” answers, since
such answers are most likely attributable to the coin coming
up heads; as a reﬁnement, respondents can also choose the
untruthful answer by ﬂipping another coin in secret, and get
strong deniability for both “Yes” and “No” answers.

Surveys relying on randomized response enable easy com-
putations of accurate population statistics while preserving
the privacy of the individuals. Assuming absolute compli-
ance with the randomization protocol (an assumption that
may not hold for human subjects, and can even be non-
trivial for algorithmic implementations [23]), it is easy to
see that in a case where both “Yes” and “No” answers can
be denied (ﬂipping two fair coins), the true number of “Yes”
answers can be accurately estimated by 2(Y − 0.25), where
Y is the proportion of “Yes” responses. In expectation, re-
spondents will provide the true answer 75% of the time, as
is easy to see by a case analysis of the two fair coin ﬂips.

Importantly, for one-time collection, the above random-
ized survey mechanism will protect the privacy of any spe-
ciﬁc respondent, irrespective of any attacker’s prior knowl-
edge, as assessed via the -diﬀerential privacy guarantee [12].
Speciﬁcally, the respondents will have diﬀerential privacy at

the level  = ln(cid:0)0.75/(1 − 0.75)(cid:1) = ln(3). This said, this

privacy guarantee degrades if the survey is repeated—e.g.,
to get fresh, daily statistics—and data is collected multiple
times from the same respondent. In this case, to maintain
both diﬀerential privacy and utility, better mechanisms are
needed, like those we present in this paper.

Privacy-Preserving Aggregatable Randomized Response,
or RAPPORs, is a new mechanism for collecting statistics
from end-user, client-side software, in a manner that pro-
vides strong privacy protection using randomized response
techniques. RAPPOR is designed to permit collecting, over
large numbers of clients, statistics on client-side values and
strings, such as their categories, frequencies, histograms, and
other set statistics. For any given value reported, RAPPOR
gives a strong deniability guarantee for the reporting client,
which strictly limits private information disclosed, as mea-
sured by an -diﬀerential privacy bound, and holds even for
a single client that reports often on the same value.

A distinct contribution is RAPPOR’s ability to collect
statistics about an arbitrary set of strings by applying ran-
domized response to Bloom ﬁlters [5] with strong -diﬀerential
privacy guarantees. Another contribution is the elegant
manner in which RAPPOR protects the privacy of clients

1054from whom data is collected repeatedly (or even inﬁnitely
often), and how RAPPOR avoids addition of privacy exter-
nalities, such as those that might be created by maintain-
ing a database of contributing respondents (which might be
breached), or repeating a single, memoized response (which
would be linkable, and might be tracked). In comparison,
traditional randomized response does not provide any lon-
gitudinal privacy in the case when multiple responses are
collected from the same participant. Yet another contribu-
tion is that the RAPPOR mechanism is performed locally
on the client, and does not require a trusted third party.

Finally, RAPPOR provides a novel, high-utility decod-
ing framework for learning statistics based on a sophisti-
cated combination of hypotheses testing, least-squares solv-
ing, and LASSO regression [26].
1.1 The Motivating Application Domain
RAPPOR is a general technology for privacy-preserving data
collection and crowdsourcing of statistics, which could be
applied in a broad range of contexts.

In this paper, however, we focus on the speciﬁc application
domain that motivated the development of RAPPOR: the
need for Cloud service operators to collect up-to-date statis-
tics about the activity of their users and their client-side
software. In this domain, RAPPOR has already seen lim-
ited deployment in Google’s Chrome Web browser, where
it has been used to improve the data sent by users that
have opted-in to reporting statistics [9]. Section 5.4 brieﬂy
describes this real-world application, and the beneﬁts RAP-
POR has provided by shining a light on the unwanted or
malicious hijacking of user settings.

For a variety of reasons, understanding population statis-
tics is a key part of an eﬀective, reliable operation of on-
line services by Cloud service and software platform oper-
ators. These reasons are often as simple as observing how
frequently certain software features are used, and measuring
their performance and failure characteristics. Another, im-
portant set of reasons involve providing better security and
abuse protection to the users, their clients, and the service
itself. For example, to assess the prevalence of botnets or
hijacked clients, an operator may wish to monitor how many
clients have—in the last 24 hours—had critical preferences
overridden, e.g., to redirect the users’ Web searches to the
URL of a known-to-be-malicious search provider.

The collection of up-to-date crowdsourced statistics raises
a dilemma for service operators. On one hand, it will likely
be detrimental to the end-users’ privacy to directly collect
their information. (Note that even the search-provider pref-
erences of a user may be uniquely identifying, incriminat-
ing, or otherwise compromising for that user.) On the other
hand, not collecting any such information will also be to the
users’ detriment: if operators cannot gather the right statis-
tics, they cannot make many software and service improve-
ments that beneﬁt users (e.g., by detecting or preventing
malicious client-side activity). Typically, operators resolve
this dilemma by using techniques that derive only the nec-
essary high-order statistics, using mechanisms that limit the
users’ privacy risks—for example, by collecting only coarse-
granularity data, and by eliding data that is not shared by
a certain number of users.

Unfortunately, even for careful operators, willing to uti-
lize state-of-the-art techniques, there are few existing, prac-
tical mechanisms that oﬀer both privacy and utility, and

even fewer that provide clear privacy-protection guarantees.
Therefore, to reduce privacy risks, operators rely to a great
extent on pragmatic means and processes, that, for exam-
ple, avoid the collection of data, remove unique identiﬁers,
or otherwise systematically scrub data, perform mandatory
deletion of data after a certain time period, and, in gen-
eral, enforce access-control and auditing policies on data
use. However, these approaches are limited in their ability
to provide provably-strong privacy guarantees. In addition,
privacy externalities from individual data collections, such
as timestamps or linkable identiﬁers, may arise; the privacy
impact of those externalities may be even greater than that
of the data collected.

RAPPOR can help operators handle the signiﬁcant chal-
lenges, and potential privacy pitfalls, raised by this dilemma.

1.2 Crowdsourcing Statistics with RAPPOR
Service operators may apply RAPPOR to crowdsource statis-
tics in a manner that protects their users’ privacy, and thus
address the challenges described above.

As a simpliﬁcation, RAPPOR responses can be assumed
to be bit strings, where each bit corresponds to a randomized
response for some logical predicate on the reporting client’s
properties, such as its values, context, or history. (Without
loss of generality, this assumption is used for the remainder
of this paper.) For example, one bit in a RAPPOR response
may correspond to a predicate that indicates the stated gen-
der, male or female, of the client user, or—just as well—their
membership in the Communist party.

The structure of a RAPPOR response need not be other-
wise constrained; in particular, (i) the response bits may be
sequential, or unordered, (ii) the response predicates may
be independent, disjoint, or correlated, and (iii) the client’s
properties may be immutable, or changing over time. How-
ever, those details (e.g., any correlation of the response bits)
must be correctly accounted for, as they impact both the uti-
lization and privacy guarantees of RAPPOR—as outlined in
the next section, and detailed in later sections.

In particular, RAPPOR can be used to collect statistics on
categorical client properties, by having each bit in a client’s
response represent whether, or not, that client belongs to a
category. For example, those categorical predicates might
represent whether, or not, the client is utilizing a software
feature. In this case, if each client can use only one of three
disjoint features, X, Y , and Z, the collection of a three-bit
RAPPOR response from clients will allow measuring the
relative frequency by which the features are used by clients.
As regards to privacy, each client will be protected by the
manner in which the three bits are derived from a single
(at most) true predicate; as regards to utility, it will suﬃce
to count how many responses had the bit set, for each dis-
tinct response bit, to get a good statistical estimate of the
empirical distribution of the features’ use.

RAPPOR can also be used to gather population statis-
tics on numerical and ordinal values, e.g., by associating re-
sponse bits with predicates for diﬀerent ranges of numerical
values, or by reporting on disjoint categories for diﬀerent
logarithmic magnitudes of the values. For such numerical
RAPPOR statistics, the estimate may be improved by col-
lecting and utilizing relevant information about the priors
and shape of the empirical distribution, such as its smooth-
ness.

1055Finally, RAPPOR also allows collecting statistics on non-
categorical domains, or categories that cannot be enumer-
ated ahead of time, through the use of Bloom ﬁlters [5]. In
particular, RAPPOR allows collection of compact Bloom-
ﬁlter-based randomized responses on strings, instead of hav-
ing clients report when they match a set of hand-picked
strings, predeﬁned by the operator. Subsequently, those re-
sponses can be matched against candidate strings, as they
become known to the operator, and used to estimate both
known and unknown strings in the population. Advanced
statistical decoding techniques must be applied to accurately
interpret the randomized, noisy data in Bloom-ﬁlter-based
RAPPOR responses. However, as in the case of categories,
this analysis needs only consider the aggregate counts of
distinct bits set in RAPPOR responses to provide good es-
timators for population statistics, as detailed in Section 4.

Without loss of privacy, RAPPOR analysis can be re-run
on a collection of responses, e.g., to consider new strings
and cases missed in previous analyses, without the need to
re-run the data collection step. Individual responses can be
especially useful for exploratory or custom data analyses.
For example, if the geolocation of clients’ IP addresses are
collected alongside the RAPPOR reports of their sensitive
values, then the observed distributions of those values could
be compared across diﬀerent geolocations, e.g., by analyz-
ing diﬀerent subsets separately. Such analysis is compatible
with RAPPOR’s privacy guarantees, which hold true even
in the presence of auxiliary data, such as geolocation. By
limiting the number of correlated categories, or Bloom ﬁl-
ter hash functions, reported by any single client, RAPPOR
can maintain its diﬀerential-privacy guarantees even when
statistics are collected on multiple aspects of clients, as out-
lined next, and detailed in Sections 3 and 6.

1.3 RAPPOR and (Longitudinal) Attacks
Protecting privacy for both one-time and multiple collec-
tions requires consideration of several distinct attack mod-
els. A basic attacker is assumed to have access to a single
report and can be stopped with a single round of random-
ized response. A windowed attacker has access to multiple
reports over time from the same user. Without careful mod-
iﬁcation of the traditional randomized response techniques,
almost certainly full disclosure of private information would
happen. This is especially true if the window of observation
is large and the underlying value does not change much. An
attacker with complete access to all clients’ reports (for ex-
ample, an insider with unlimited access rights), is the hard-
est to stop, yet such an attack is also the most diﬃcult to
execute in practice. RAPPOR provides explicit trade-oﬀs
between diﬀerent attack models in terms of tunable privacy
protection for all three types of attackers.

RAPPOR builds on the basic idea of memoization and
provides a framework for one-time and longitudinal privacy
protection by playing the randomized response game twice
with a memoization step in between. The ﬁrst step, called a
Permanent randomized response, is used to create a “noisy”
answer which is memoized by the client and permanently
reused in place of the real answer. The second step, called an
Instantaneous randomized response, reports on the “noisy”
answer over time, eventually completely revealing it. Long-
term, longitudinal privacy is ensured by the use of the Per-
manent randomized response, while the use of an Instanta-

neous randomized response provides protection against pos-
sible tracking externalities.

The idea of underlying memoization turns out to be cru-
cial for privacy protection in the case where multiple re-
sponses are collected from the same participant over time.
For example, in the case of the question about the Commu-
nist party from the start of the paper, memoization can allow
us to provide ln(3)-diﬀerential privacy even with an inﬁnite
number of responses, as long as the underlying memoized
response has that level of diﬀerential privacy.

On the other hand, without memoization or other limita-
tion on responses, randomization is not suﬃcient to maintain
plausible deniability in the face of multiple collections. For
example, if 75 out of 100 responses are “Yes” for a single
client in the randomized-response scheme at the very start
of this paper, the true answer will have been “No” in a van-
ishingly unlikely 1.39 × 10−24 fraction of cases.

Memoization is absolutely eﬀective in providing longitudi-
nal privacy only in cases when the underlying true value does
not change or changes in an uncorrelated fashion. When
users’ consecutive reports are temporally correlated, diﬀer-
ential privacy guarantees deviate from their nominal levels
and become progressively weaker as correlations increase.
Taken to the extreme, when asking users to report daily on
their age in days, additional measures are required to pre-
vent full disclosure over time, such as stopping collection
after a certain number of reports or increasing the noise lev-
els exponentially, as discussed further in Section 6.

For a client that reports on a property that strictly alter-
nates between two true values, (a, b, a, b, a, b, a, b, . . .), the
two memoized Permanent randomized responses for a and
b will be reused, again and again, to generate RAPPOR re-
port data. Thus, an attacker that obtains a large enough
number of reports, could learn those memoized “noisy” val-
ues with arbitrary certainty—e.g., by separately analyzing
the even and odd subsequences. However, even in this case,
the attacker cannot be certain of the values of a and b be-
cause of memoization. This said, if a and b are correlated,
the attacker may still learn more than they otherwise would
have; maintaining privacy in the face of any such correlation
is discussed further in Sections 3 and 6 (see also [19]).

In the next section we will describe the RAPPOR algo-
rithm in detail. We then provide intuition and formal justi-
ﬁcation for the reasons why the proposed algorithm satisﬁes
the rigorous privacy guarantees of diﬀerential privacy. We
then devote several sections to discussion of the additional
technical aspects of RAPPOR that are crucial for its poten-
tial uses in practice, such as parameter selection, interpre-
tation of results via advanced statistical decoding, and ex-
periments illustrating what can be learned in practice. The
remaining sections discuss our experimental evaluation, the
attack models we consider, the limitations of the RAPPOR
technique, as well as related work.
2 The Fundamental RAPPOR Algorithm
Given a client’s value v, the RAPPOR algorithm executed
by the client’s machine, reports to the server a bit array
of size k, that encodes a “noisy” representation of its true
value v. The noisy representation of v is chosen in such a
way so as to reveal a controlled amount of information about
v, limiting the server’s ability to learn with conﬁdence what
v was. This remains true even for a client that submits an
inﬁnite number of reports on a particular value v.

1056Figure 1: Life of a RAPPOR report: The client value of the string “The number 68” is hashed onto the Bloom
ﬁlter B using h (here 4) hash functions. For this string, a Permanent randomized response B(cid:48) is produces and
memoized by the client, and this B(cid:48) is used (and reused in the future) to generate Instantaneous randomized
responses S (the bottom row), which are sent to the collecting service.

To provide such strong privacy guarantees, the RAPPOR
algorithm implements two separate defense mechanisms, both
of which are based on the idea of randomized response and
can be separately tuned depending on the desired level of
privacy protection at each level. Furthermore, additional
uncertainty is added through the use of Bloom ﬁlters which
serve not only to make reports compact, but also to compli-
cate the life of any attacker (since any one bit in the Bloom
ﬁlter may have multiple data items in its pre-image).

The RAPPOR algorithm takes in the client’s true value v
and parameters of execution k, h, f, p, q, and is executed lo-
cally on the client’s machine performing the following steps:

1. Signal. Hash client’s value v onto the Bloom ﬁlter B

of size k using h hash functions.

2. Permanent randomized response. For each client’s
value v and bit i, 0 ≤ i < k in B, create a binary re-
porting value B(cid:48)

i which equals to

1,

(cid:48)
i =

B

with probability 1
2 f
with probability 1
2 f

0,
Bi, with probability 1 − f

where f is a user-tunable parameter controlling the
level of longitudinal privacy guarantee.
Subsequently, this B(cid:48) is memoized and reused as the
basis for all future reports on this distinct value v.

3. Instantaneous randomized response. Allocate a
bit array S of size k and initialize to 0. Set each bit i
in S with probabilities

(cid:40)

P (Si = 1) =

q,
p,

if B(cid:48)
if B(cid:48)

i = 1.
i = 0.

4. Report. Send the generated report S to the server.

There are many diﬀerent variants of the above randomized
response mechanism. Our main objective for selecting these

two particular versions was to make the scheme intuitive and
easy to explain.

The Permanent randomized response (step 2) replaces the
real value B with a derived randomized noisy value B(cid:48). B(cid:48)
may or may not contain any information about B depend-
ing on whether signal bits from the Bloom ﬁlter are being
replaced by random 0’s with probability 1
2 f . The Perma-
nent randomized response ensures privacy because of the
adversary’s limited ability to diﬀerentiate between true and
“noisy” signal bits.
It is absolutely critical that all future
reporting on the information about B uses the same ran-
domized B(cid:48) value to avoid an “averaging” attack, in which
an adversary estimates the true value from observing multi-
ple noisy versions of it.

The Instantaneous randomized response (step 3) plays
Instead of directly reporting
several important functions.
B(cid:48) on every request, the client reports a randomized version
of B(cid:48). This modiﬁcation signiﬁcantly increases the diﬃculty
of tracking a client based on B(cid:48), which could otherwise be
viewed as a unique identiﬁer in longitudinal reporting sce-
narios. It also provides stronger short-term privacy guaran-
tees (since we are adding more noise to the report) which can
be independently tuned to balance short-term vs long-term
risks. Through tuning of the parameters of this mechanism
we can eﬀectively balance utility against diﬀerent attacker
models.

Figure 1 shows a random run of the RAPPOR algorithm.
Here, a client’s value is v = “68”, the size of the Bloom ﬁl-
ter is k = 256, the number of hash functions is h = 4, and
the tunable randomized response parameters are: p = 0.5,
q = 0.75, and f = 0.5. The reported bit array sent to the
server is shown at the bottom of the ﬁgure. 145 out of 256
bits are set in the report. Of the four Bloom ﬁlter bits in B
(second row), two are propagated to the noisy Bloom ﬁlter
B(cid:48). Of these two bits, both are turned on in the ﬁnal report.
The other two bits are never reported on by this client due
to the permanent nature of B(cid:48). With multiple collections
from this client on the value “68”, the most powerful attacker
would eventually learn B(cid:48) but would continue to have lim-

Bloom filter bitsParticipant 8456 in cohort 1183264128256"The number 68"||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||4 signal bits||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||69 bits on||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||145 bits onTrue value:Bloom filter (B):Fake Bloom  filter (B'):Report sent to server:011057ited ability to reason about the value of B, as measured by
diﬀerential privacy guarantee.
In practice, learning about
the actual client’s value v is even harder because multiple
values map to the same bits in the Bloom ﬁlter [4].
2.1 RAPPOR Modiﬁcations
The RAPPOR algorithm can be modiﬁed in a number of
ways depending on the particulars of the scenario in which
privacy-preserving data collection is needed. Here, we list
three common scenarios where omitting certain elements
from the RAPPOR algorithm leads to a more eﬃcient learn-
ing procedure, especially with smaller sample sizes.

• One-time RAPPOR. One time collection, enforced
by the client, does not require longitudinal privacy pro-
tection. The Instantaneous randomized response step
can be skipped in this case and a direct randomization
on the true client’s value is suﬃcient to provide strong
privacy protection.

• Basic RAPPOR. If the set of strings being collected
is relatively small and well-deﬁned, such that each
string can be deterministically mapped to a single bit
in the bit array, there is no need for using a Bloom ﬁlter
with multiple hash functions. For example, collecting
data on client’s gender could simply use a two-bit ar-
ray with “male” mapped to bit 1 and “female” mapped
to bit 2. This modiﬁcation would aﬀect step 1, where
a Bloom ﬁlter would be replaced by a deterministic
mapping of each candidate string to one and only one
bit in the bit array. In this case, the eﬀective number
of hash functions, h, would be 1.

• Basic One-time RAPPOR. This is the simplest
conﬁguration of the RAPPOR mechanism, combin-
ing the ﬁrst two modiﬁcations at the same time: one
round of randomization using a deterministic mapping
of strings into their own unique bits.

3 Differential Privacy of RAPPOR
The scale and availability of data in today’s world makes
increasingly sophisticated attacks feasible, and any system
that hopes to withstand such attacks should aim to ensure
rigorous, rather than merely intuitive privacy guarantees.
For our analysis, we adopt the rigorous notion of privacy, dif-
ferential privacy, which was introduced by Dwork et al [12]
and has been widely adopted [10]. The deﬁnition aims to en-
sure that the output of the algorithm does not signiﬁcantly
depend on any particular individual’s data. The quantiﬁca-
tion of the increased risk that participation in a service poses
to an individual can, therefore, empower clients to make a
better informed decision as to whether they want their data
to be part of the collection.

Formally, a randomized algorithm A satisﬁes -diﬀerential
privacy [12] if for all pairs of client’s values v1 and v2 and
for all R ⊆ Range(A),

P (A(v1) ∈ R) ≤ eP (A(v2) ∈ R).

We prove that the RAPPOR algorithm satisﬁes the deﬁni-
tion of diﬀerential privacy next. Intuitively, the Permanent
randomized response part ensures that the “noisy” value de-
rived from the true value protects privacy, and the Instanta-
neous randomized response provides protection against us-
age of that response by a longitudinal tracker.

3.1 Differential Privacy of the Permanent Ran-

domized Response

Theorem 1. The Permanent randomized response (Steps
1 and 2 of RAPPOR) satisﬁes ∞-diﬀerential privacy where
∞ = 2h ln

2 f

.

(cid:17)

(cid:16) 1− 1

1
2 f

Proof. Let S = s1, . . . , sk be a randomized report gen-
erated by the RAPPOR algorithm. Then the probability of
observing any given report S given the true client value v
and assuming that B(cid:48) is known is
P (S = s|V = v) = P (S = s|B, B

(cid:48)|B, v) · P (B|v)

, v) · P (B

(cid:48)

= P (S = s|B
= P (S = s|B

(cid:48)
(cid:48)

) · P (B
) · P (B

(cid:48)|B) · P (B|v)
(cid:48)|B).

Because S is conditionally independent of B given B(cid:48), the
ﬁrst probability provides no additional information about
B. P (B(cid:48)|B) is, however, critical for longitudinal privacy
protection. Relevant probabilities are

i = 1|bi = 1) =
(cid:48)
i = 1|bi = 0) =
(cid:48)

P (b

P (b

1
2
1
2

f + 1 − f = 1 − 1
2

f

and

f.

Without loss of generality, let the Bloom ﬁlter bits 1, . . . , h
be set, i.e., b∗ = {b1 = 1, . . . , bh = 1, bh+1 = 0, . . . , bk = 0}.
Then,

(cid:48)

(cid:48)|B = b

∗

= b

P (B

) =

1 × . . .

f

f

2

×

1 − 1
2
1 − 1
2

(cid:19)b(cid:48)
1(cid:18)
(cid:18) 1
h(cid:18)
(cid:18) 1
(cid:19)b(cid:48)
(cid:18)
(cid:19)b(cid:48)
h+1(cid:18) 1
k(cid:18) 1
(cid:19)b(cid:48)
(cid:18)

(cid:19)1−b(cid:48)
(cid:19)1−b(cid:48)
(cid:19)1−b(cid:48)
(cid:19)1−b(cid:48)

2
1 − 1
2
1 − 1
2

×

×

2

2

f

f

f

f

f

f

.

k

h × . . .

h+1 × . . .

Let RR∞ be the ratio of two such conditional probabil-
ities with distinct values of B, B1 and B2, i.e., RR∞ =
P (B(cid:48)∈R∗|B=B1)
P (B(cid:48)∈R∗|B=B2) . For the diﬀerential privacy condition to
hold, RR∞ needs to be bounded by exp(∞).

RR∞ =

=

P (B(cid:48) ∈ R∗|B = B1)
P (B(cid:48) ∈ R∗|B = B2)
i∈R∗ P (B(cid:48) = B(cid:48)
B(cid:48)
i∈R∗ P (B(cid:48) = B(cid:48)
B(cid:48)
≤ max
(cid:19)2(b(cid:48)
B(cid:48)
i∈R∗

P (B(cid:48) = B(cid:48)
P (B(cid:48) = B(cid:48)

2+...+b(cid:48)

1+b(cid:48)

=

f

i|B = B1)
i|B = B2)

i|B = B1)
i|B = B2)
h−b(cid:48)
h+1−b(cid:48)

(cid:19)2(b(cid:48)

1 − 1
2

f

(cid:80)
(cid:80)
(cid:18) 1
(cid:18)

2

×

1
2 f

(by Observation 8)

h+2−...−b(cid:48)

2h)

h+1+b(cid:48)

h+2+...+b(cid:48)

2h−b(cid:48)

1−b(cid:48)

2−...−b(cid:48)
h)

.

Sensitivity is maximized when b(cid:48)
h = 0. Then,
and ∞ = 2h ln

2 = . . . = b(cid:48)
1 = b(cid:48)
2 f

(cid:16) 1− 1

1 and b(cid:48)
RR∞ =

(cid:17)2h

h+1 = b(cid:48)

(cid:16) 1− 1

2 f

(cid:17)

.

1
2 f

h+2 = . . . = b(cid:48)

2h =

1058Note that ∞ is not a function of k.

It is true that a
smaller k, or a higher rate of Bloom ﬁlter bit collision, some-
times improves privacy protection, but, on its own, it is not
suﬃcient nor necessary to provide -diﬀerential privacy.
3.2 Differential Privacy of the Instantaneous

Randomized Response

With a single data collection from each client, the attacker’s
knowledge of B must come directly from a single report S
generated by applying the randomization twice, thus, pro-
viding a higher level of privacy protection than under the
assumption of complete knowledge of B(cid:48).

Because of a two-step randomization, probability of ob-
serving a 1 in a report is a function of both q and p as well
as f .

Lemma 1. Probability of observing 1 given that the un-

derlying Bloom ﬁlter bit was set is given by

∗

q

= P (Si = 1|bi = 1) =

f (p + q) + (1 − f )q.

1
2

Probability of observing 1 given that the underlying Bloom
ﬁlter bit was not set is given by
= P (Si = 1|bi = 0) =

f (p + q) + (1 − f )p.

p

∗

1
2

We omit the proof as the reasoning is straightforward that
probabilities in both cases are mixtures of random and true
responses with the mixing proportion f .

Theorem 2. The Instantaneous randomized response (Step

3 of RAPPOR) satisﬁes 1-diﬀerential privacy, where 1 =
h log

and q∗ and p∗ as deﬁned in Lemma 1.

(cid:16) q∗(1−p∗)

p∗(1−q∗)

(cid:17)

Proof. The proof is analogous to Theorem 1. Let RR1
be the ratio of two conditional probabilities, i.e., RR1 =
P (S∈R|B=B1)
P (S∈R|B=B2) . To satisfy the diﬀerential privacy condition,
this ratio must be bounded by exp(1).
P (S ∈ R|B = B1)
P (S ∈ R|B = B2)

RR1 =

=

≤ max
sj∈R

(cid:80)
(cid:80)
sj∈R P (S = sj|B = B1)
sj∈R P (S = sj|B = B2)
(cid:20) q∗(1 − p∗)

P (S = sj|B = B1)
P (S = sj|B = B2)

p∗(1 − q∗)

(cid:21)h
(cid:18) q∗(1 − p∗)

p∗(1 − q∗)

(cid:19)

.

1 = h log

=

and

The above proof naturally extends to N reports, since
each report that is not changed contributes a ﬁxed amount to
the total probability of observing all reports and enters both
nominator and denominator in a multiplicative way (because
of independence). Since our diﬀerential privacy framework
considers inputs that diﬀer only in a single record, j, (reports
set D1 becomes D2 diﬀering in a single report Sj), the rest

of the product terms end up canceling out in the ratio
P (S1 = s1, S2 = s2, . . . , Sj = sj, . . . , SN = sN|B1)
P (S1 = s1, S2 = s2, . . . , Sj = sj, . . . , SN = sN|B2)

(cid:81)N
(cid:81)N
i=1 P (Si = si|B1)
i=1 P (Si = si|B2)

=
P (Sj = sj|B1)
P (Sj = sj|B2)

.

=

Computing n for the nth collection cannot be made with-
out additional assumptions about how eﬀectively the at-
tacker can learn B(cid:48) from the collected reports. We continue
working on providing these bounds under various learning
strategies. Nevertheless, as N becomes large, the bound
approaches ∞ but always remains strictly smaller.

4 High-utility Decoding of Reports
In most cases, the goal of data collection using RAPPOR is
to learn which strings are present in the sampled population
and what their corresponding frequencies are. Because we
make use of the Bloom ﬁlter (loss of information) and pur-
posefully add noise for privacy protection, decoding requires
sophisticated statistical techniques.

To facilitate learning, before any data collection begins
each client is randomly assigned and becomes a permanent
member of one of m cohorts. Cohorts implement diﬀer-
ent sets of h hash functions for their Bloom ﬁlters, thereby
reducing the chance of accidental collisions of two strings
across all of them. Redundancy introduced by running m
cohorts simultaneously greatly improves the false positive
rate. The choice of m should be considered carefully, how-
ever. When m is too small, then collisions are still quite
likely, while when m is too large, then each individual co-
hort provides insuﬃcient signal due to its small sample size
(approximately N/m, where N is the number of reports).
Each client must report its cohort number with every sub-
mitted report, i.e., it is not private but made private.

We propose the following approach to learning from the

collected reports:

• Estimate the number of times each bit i within cohort
j, tij, is truly set in B for each cohort. Given the
number of times each bit i in cohort j, cij was set in
a set of Nj reports, the estimate is given by

tij =

cij − (p + 1

2 f q − 1
(1 − f )(q − p)

2 f p)Nj

.

Let Y be a vector of tij’s, i ∈ [1, k], j ∈ [1, m].

• Create a design matrix X of size km × M where M is
the number of candidate strings under consideration.
X is mostly 0 (sparse) with 1’s at the Bloom ﬁlter
bits for each string for each cohort. So each column
of X contains hm 1’s at positions where a particular
candidate string was mapped to by the Bloom ﬁlters in
all m cohorts. Use Lasso [26] regression to ﬁt a model
Y ∼ X and select candidate strings corresponding to
non-zero coeﬃcients.

• Fit a regular least-squares regression using the selected
variables to estimate counts, their standard errors and
p-values.

• Compare p-values to a Bonferroni corrected level of
α/M = 0.05/M to determine which frequencies are

1059Figure 2: Recall versus precision depending on choice of parameters k, h, and m. The ﬁrst panel shows the
true population distribution from which RAPPOR reports were sampled. The other three panels vary one
of the parameters while keeping the other two ﬁxed. Best precision and recall are achieved with using 2 hash
functions, while the choices of k and m do not show clear preferences.

statistically signiﬁcant from 0. Alternatively, control-
ling the False Discovery Rate (FDR) at level α using
the Benjamini-Hochberg procedure [3], for example,
could be used.

4.1 Parameter Selection

Practical implementation of the RAPPOR algorithm requires
speciﬁcation of a number of parameters. p, q, f and the num-
ber of hash functions h control the level of privacy for both
one-time and longitudinal collections. Clearly, if no longi-
tudinal data is being collected, then we can use One-time
RAPPOR modiﬁcation. With the exception of h, the choice
of values for these parameters should be driven exclusively
by the desired level of privacy .  itself can be picked de-
pending on the circumstances of the data collection process;
values in the literature range from 0.01 to 10 (see Table 1 in
[15]).

Bloom ﬁlter size, k, the number of cohorts, m, and h must
also be speciﬁed a priori. Besides h, neither k nor m are re-
lated to the worst-case privacy considerations and should be
selected based on the eﬃciency properties of the algorithm
in reconstructing the signal from the noisy reports.

We ran a number of simulations (averaged over 10 repli-
cates) to understand how these three parameters eﬀect de-
coding; see Figure 2. All scenarios assumed  = ln(3) pri-
vacy guarantee. Since only a single report from each user
was simulated, One-time RAPPOR was used. Population
sampled is shown in the ﬁrst panel and contains 100 non-
zero strings with 100 strings that had zero probability of
occurring. Frequencies of non-zero strings followed an expo-
nential distribution as shown in the ﬁgure.

In the other three panels, the x-axis shows the recall rate
and the y-axis shows the precision rate. In all three pan-
els, the same set of points are plotted and are only labeled
diﬀerently depending on which parameter changes in a par-
ticular panel. Each point represents an average recall and
precision for a unique combination of k, h, and m. For ex-
ample, the second panel shows the eﬀect of the Bloom ﬁlter
size on both precision and recall while keeping both h and m
ﬁxed. It is diﬃcult to make deﬁnitive conclusions about the
optimal size of the Bloom ﬁlter as diﬀerent sizes perform
similarly depending on the values of h and m. The third
panel, however, shows a clear preference for using only two
hash functions from the perspective of utility, as the decrease
in the number of hash functions used increases the expected

Population Used in ExperimentsFrequency0.000.020.040.060.08●●●●●●●●●●●●●●●●0.450.500.550.600.650.840.860.880.900.920.94Varying the Bloom Filter Sizesrecall (true positive / population)precision (true positive / detected)●1282565121024●●●●●●●●●●●●●●●●0.450.500.550.600.650.840.860.880.900.920.94Varying the Number of Hash Functionsrecall (true positive / population)precision (true positive / detected)●24816●●●●●●●●●●●●●●●●0.450.500.550.600.650.840.860.880.900.920.94Varying the Number of Cohortsrecall (true positive / population)precision (true positive / detected)●81632641060use of Bloom ﬁlter. Details of the calculations are shown in
the Appendix.

While providing ln(3)-diﬀerential privacy for one time col-
lection, if one would like to detect items with frequency 1%,
then one million samples are required, 0.1% would require a
sample size of 100 million and 0.01% items would be identi-
ﬁed only in a sample size of 10 billion.

Eﬃciency of the unmodiﬁed RAPPOR algorithm is sig-
niﬁcantly inferior when compared to the Basic One-time
RAPPOR (the price of compression). Even for the Ba-
sic One-time RAPPOR, the provided bound can be theo-
retically achieved only if the underlying distribution of the
strings’ frequencies is uniform (a condition under which the
smallest frequency is maximized). With the presence of sev-
eral high-frequency strings, there is less probability mass
left for the tail and, with the drop in their frequencies, their
detectability suﬀers.

5 Experiments and Evaluation
We demonstrate our approach using two simulated and two
real-world collection examples. The ﬁrst simulated one uses
the Basic One-time RAPPOR where we learn the shape of
the underlying Normal distribution. The second simulated
example uses unmodiﬁed RAPPOR to collect strings whose
frequencies exhibit exponential decay. The third example is
drawn from a real-world dataset on processes running on a
set of Windows machines. The last example is based on the
Chrome browser settings collections.
5.1 Reporting on the Normal Distribution
To get a sense of how eﬀectively we can learn the underlying
distribution of values reported through the Basic One-time
RAPPOR, we simulated learning the shape of the Normal
distribution (rounded to integers) with mean 50 and stan-
dard deviation 10. The privacy constraints were: q = 0.75
and p = 0.5 providing  = ln(3) diﬀerential privacy (f = 0).
Results are shown in Figure 4 for three diﬀerent sample sizes.
With 10,000 reports, results are just too noisy to obtain a
good estimate of the shape. The Normal bell curve begins
to emerge already with 100,000 reports and at one million
reports it is traced very closely. Notice the noise in the left
and right tails where there is essentially no signal. It is re-
quired by the diﬀerential privacy condition and also gives a
sense of how uncertain our estimated counts are.
5.2 Reporting on an Exponentially-distributed

Set of Strings

The true underlying distribution of strings from which we
sample is shown in Figure 5. It shows commonly encoun-
tered exponential decay in the frequency of strings with sev-
eral “heavy hitters” and the long tail. After sampling 1 mil-
lion values (one collection event per user) from this popu-
lation at random, we apply RAPPOR to generate 1 million
reports with p = 0.5, q = 0.75, f = 0.5, two hash functions,
Bloom ﬁlter size of 128 bits and 16 cohorts.

After the statistical analysis using the Bonferroni cor-
rection discussed above, 47 strings were estimated to have
counts signiﬁcantly diﬀerent from 0. Just 2 of the 47 strings
were false positives, meaning their true counts were truly 0
but estimated to be signiﬁcantly diﬀerent. The top-20 de-
tected strings with their count estimates, standard errors,
p-values and z-scores (SNR) are shown in Table 1. Small

Figure 3: Sample size vs the upper limit on the
strings whose frequency can be learned. Seven col-
ored lines represent diﬀerent cardinalities of the can-
didate string set. Here, p = 0.5, q = 0.75 and f = 0.

recall. The fourth panel, similarly to the second, does not
deﬁnitively indicate the optimal direction for choosing the
number of cohorts.

4.2 What Can We Learn?
In practice, it is common to use thresholds on the number of
unique submissions in order to ensure some privacy. How-
ever, arguments as to how those thresholds should be set
abound, and most of the time they are based on a ‘feel’ for
what is accepted and lack any objective justiﬁcation. RAP-
POR also requires , a user-tunable parameter, which by the
design of the algorithm translates into limits on frequency
domain, i.e., puts a lower limit on the number of times a
string needs to be observed in a sample before it can be reli-
ably identiﬁed and its frequency estimated. Figure 3 shows
the relationship between the sample size (x-axis) and the
theoretical upper limit (y-axis) on how many strings can
be detected at that sample size for a particular choice of
p = 0.5 and q = 0.75 (with f = 0) at a given conﬁdence
level α = 0.05.

It is perhaps surprising that we do not learn more at very
large sample sizes (e.g., one billion). The main reason is that
as the number of strings in the population becomes large,
their frequencies proportionally decrease and they become
hard to detect at those low frequencies.

We can only reliably detect about 10,000 strings in a sam-
ple of ten billion and about 1,000 with a sample of one hun-
N /10, where N
dred million. A general rule of thumb is
is the sample size. These theoretical calculations are based
on the Basic One-time RAPPOR algorithm (the third mod-
iﬁcation) and are the upper limit on what can be learned
since there is no additional uncertainty introduced by the

√

1e+021e+041e+061e+081e+10110100100010000Sample SizeMaximum Number of Discoverable StringsM = 10000M = 1e+05M = 1e+06M = 1e+07M = 1e+08M = 1e+091061Figure 4: Simulations of learning the normal distribution with mean 50 and standard deviation 10. The
RAPPOR privacy parameters are q = 0.75 and p = 0.5, corresponding to  = ln(3). True sample distribution
is shown in black; light green shows the estimated distribution based on the decoded RAPPOR reports. We
do not assume a priori knowledge of the Normal distribution in learning. If such prior information were
available, we could signiﬁcantly improve upon learning the shape of the distribution via smoothing.

String
V 1
V 2
V 5
V 7
V 4
V 3
V 8
V 6
V 10
V 9
V 12
V 11
V 14
V 19
V 13
V 15
V 20
V 18
V 17
V 21

Est.
48803
47388
41490
40682
40420
39509
36861
36220
34196
32207
30688
29630
27366
23860
22327
21752
20159
19521
18387
18267

Stdev
2808
2855
2801
2849
2811
2882
2842
2829
2828
2805
2822
2831
2850
2803
2826
2825
2821
2835
2811
2828

P.value
5.65E-63
5.82E-58
4.30E-47
4.58E-44
1.31E-44
7.03E-41
5.93E-37
4.44E-36
1.72E-32
1.45E-29
9.07E-27
5.62E-25
2.33E-21
3.41E-17
4.69E-15
2.15E-14
1.26E-12
7.74E-12
7.86E-11
1.33E-10

Truth Prop.
49884
0.05
0.05
47026
0.04
40077
0.04
36565
0.04
42747
44642
0.04
0.03
34895
0.04
38231
0.03
31234
0.03
33106
28295
0.03
0.03
29908
0.03
25984
0.02
20057
26913
0.03
0.02
24653
0.02
19110
0.02
20912
0.02
22141
17878
0.02

SNR
17.38
16.60
14.81
14.28
14.38
13.71
12.97
12.80
12.09
11.48
10.87
10.47
9.60
8.51
7.90
7.70
7.15
6.89
6.54
6.46

Table 1: Top-20 strings with their estimated fre-
quencies, standard deviations, p-values, true counts
and signal to noise ratios (SNR or z-scores).

This collection used 128 Bloom ﬁlter with 2 hash func-
tions and 8 cohorts. Privacy parameters were chosen such
that 1 = 1.0743 with q = 0.75, p = 0.5, and f = 0.5.
Given this conﬁguration, we optimistically expected to dis-
cover processes with frequency of at least 1.5%.

We identiﬁed 10 processes shown in Table 2 ranging in
frequency between 2.5% and 4.5%. They were identiﬁed by
controlling the False Discovery Rate at 5%. The “BADAP-
PLE.COM” process was estimated to have frequency of 2.6%.
The other 9 processes were common Windows tasks we would
expect to be running on almost every Windows machine.

Figure 5: Population of strings with their true fre-
quencies on the vertical axis (0.01 is 1%). Strings
detected by RAPPOR are shown in dark red.

p-values show high conﬁdence in our assessment that the
true counts are much larger than 0 and, in fact, comparing
columns 2 and 5 conﬁrms that. Figure 5 shows all 47 de-
tected strings in dark red. All common strings above the
frequency of approximately 1% were detected and the long
tail remained protected by the privacy mechanism.
5.3 Reporting on Windows Process Names
We collected 186,792 reports from 10,133 diﬀerent Windows
computers, sampling actively running processes on each ma-
chine. On average, just over 18 process names were collected
from each machine with the goal of recovering the most com-
mon ones and estimating the frequency of a particularly ma-
licious binary named “BADAPPLE.COM”.

N = 1e+040100200300400N = 1e+0501000200030004000N = 1e+0601000020000300000.000.010.020.030.041100200DetectedNot−detected1062Table 2: Windows processes detected.

Est.
Process Name
8054
RASERVER.EXE
7488
RUNDLL32.EXE
7451
CONHOST.EXE
6363
SPPSVC.EXE
5579
AITAGENT.EXE
MSIEXEC.EXE
5147
SILVERLIGHT.EXE 4915
4860
BADAPPLE.COM
4787
LPREMOVE.EXE
DEFRAG.EXE
4760

Stdev
1212
1212
1212
1212
1212
1212
1212
1212
1212
1212

P.value
1.56E-11
3.32E-10
4.02E-10
7.74E-08
2.11E-06
1.10E-05
2.53E-05
3.07E-05
3.95E-05
4.34E-05

Prop.
0.04
0.04
0.04
0.03
0.03
0.03
0.03
0.03
0.03
0.03

5.4 Reporting on Chrome Homepages
The Chrome Web browser has implemented and deployed
RAPPOR to collect data about Chrome clients [9]. Data
collection has been limited to some of the Chrome users
who have opted in to send usage statistics to Google, and to
certain Chrome settings, with daily collection from approx-
imately ∼14 million respondents.

Chrome settings, such as homepage, search engine and
others, are often targeted by malicious software and changed
without users’ consent. To understand who the main players
are, it is critical to know the distribution of these settings
on a large number of Chrome installations. Here, we focus
on learning the distribution of homepages and demonstrate
what can be learned from a dozen million reports with strong
privacy guarantees.

This collection used 128 Bloom ﬁlter with 2 hash functions
and 32 cohorts. Privacy parameters were chosen such that
1 = 0.5343 with q = 0.75, p = 0.5, and f = 0.75. Given
this conﬁguration, optimistically, RAPPOR analysis can dis-
cover homepage URL domains, with statistical conﬁdence,
if their frequency exceeds 0.1% of the responding popula-
tion. Practically, this means that more than ∼14 thousand
clients must report on the same URL domain, before it can
be identiﬁed in the population by RAPPOR analysis.

Figure 6 shows the relative frequencies of 31 unexpected
homepage domains discovered by RAPPOR analysis. (Since
not all of these are necessarily malicious, the ﬁgure does not
include the actual URL domain strings that were identiﬁed.)
As one might have expected, there are several popular home-
pages, likely intentionally set by users, along with a long tail
of relatively rare URLs. Even though less than 0.5% out of
8,616 candidate URLs provide enough statistical evidence
for their presence (after the FDR correction), they collec-
tively account for about 85% of the total probability mass.

6 Attack Models and Limitations
We consider three types of attackers with diﬀerent capabil-
ities for collecting RAPPOR reports.

The least powerful attacker has access to a single report
from each user and is limited by one-time diﬀerential pri-
vacy level 1 on how much knowledge gain is possible. This
attacker corresponds to an eavesdropper that has temporary
ability to snoop on the users’ reports.

A windowed attacker is presumed to have access to one
client’s data over a well-deﬁned period of time. This at-
tacker, depending on the sophistication of her learning model,
could learn more information about a user than the attacker

Figure 6: Relative frequencies of the top 31 unex-
pected Chrome homepage domains found by ana-
lyzing ∼14 million RAPPOR reports, excluding ex-
pected domains (the homepage “google.com”, etc.).

of the ﬁrst type. Nevertheless, the improvement in her abil-
ity to violate privacy is strictly bounded by the longitudinal
diﬀerential privacy guarantee of ∞. This more powerful at-
tacker may correspond to an adversary such as a malicious
Cloud service employee, who may have temporary access to
reports, or access to a time-bounded log of reports.

The third type of attacker is assumed to have unlimited
collection capabilities and can learn the Permanent random-
ized response B(cid:48) with absolute certainty. Because of the
randomization performed to obtain B(cid:48) from B, she is also
bounded by the privacy guarantee of ∞ and cannot im-
prove upon this bound with more data collection. This cor-
responds to a worst-case adversary, but still one that doesn’t
have direct access to the true data values on the client.

Despite envisioning a completely local privacy model, one
where users themselves release data in a privacy-preserving
fashion, operators of RAPPOR collections, however, can
easily manipulate the process to learn more information than
warranted by the nominal ∞. Soliciting users to partic-
ipate more than once in a particular collection results in
multiple Permanent randomized responses for each user and
partially defeats the beneﬁts of memoization. In the web-
centric world, users use multiple accounts and multiple de-
vices and can unknowingly participate multiple times, re-
leasing more information than what they expected. This
problem could be mitigated to some extent by running col-
lections per account and sharing a common Permanent ran-
domized response. Notice the role of the operator to ensure
that such processes are in place and the required or assumed
trust on the part of the user.

It is likely that some attackers will aim to target speciﬁc
users by isolating and analyzing reports from that user, or a
small group of users that includes them. Even so, some

123456789101112131415161718192021222324252627282930311063Figure 7: False Discovery Rate (FDR) as a function
of string frequency and f . Identifying rare strings
in a population without introducing a large num-
ber of false discoveries is infeasible. Also, FDR is
proportional to f .

Figure 8: Exact probabilities for inferring the true
value v given the two bits observed in a RAPPOR
report S corresponding to the two bits set by string
v. For rare strings, even when both bits are set to
1 (green lines), it is still much more likely that the
client did not report v, but some other value.

with probability (cid:0) 1

2 f(cid:1)h, clients will generate a Permanent

randomly-chosen users need not fear such attacks at all:

randomized response B(cid:48) with all 0s at the positions of set
Bloom ﬁlter bits. Since these clients are not contributing any
useful information to the collection process, targeting them
individually by an attacker is counter-productive. An at-
tacker has nothing to learn about this particular user. Also,
for all users, at all times, there is plausible deniability pro-
portional to the fraction of clients providing no information.
In one particular attack scenario, imagine an attacker that
is interested in learning whether a given client has a partic-
ular value v, whose population frequency is known to be
fv. The strongest evidence in support of v comes in the
form of both Bloom ﬁlter bits for v being set in the client’s
report (if two hash functions are used). The attacker can
formulate its target set by selecting all reports with these
two bits set. However, this set will miss some clients with
v and include other clients who did not report v. False dis-
covery rate (FDR) is the proportion of clients in the target
set who reported a value diﬀerent from v. Figure 7 shows
FDR as a function of fv, the frequency of the string v. No-
tably, for relatively rare values, most clients in the target set
will, in fact, have a value that is diﬀerent from v, which will
hopefully deter any would-be attackers.

The main reason for the high FDR rate at low frequencies
fv stems from the limited evidence provided by the observed
bits in support of v. This is clearly illustrated by Figure 8
where the probability that v was reported (1) or not re-
ported (0) by the client is plotted as a function of fv. For
relatively rare strings (those with less than 10% frequency),
even when both bits corresponding to v are set in the report,
the probability of v being reported is much smaller than of

it not being reported. Because the prior probability fv is
so small, a single client’s reports cannot provide suﬃcient
evidence in favor of v.
6.1 Caution and Correlations
Although it advances the state of the art, RAPPOR is not
a panacea, but rather simply a tool that can provide sig-
niﬁcant beneﬁts when used cautiously, and correctly, us-
ing parameters appropriate to its application context. Even
then, RAPPOR should be used only as part of a comprehen-
sive privacy-protection strategy, which should include lim-
ited data retention and other pragmatic processes mentioned
in Section 1.1, and already in use by Cloud operators.

As in previous work on diﬀerential privacy for database
records, RAPPOR provides privacy guarantees for the re-
sponses from individual clients. One of the limitations of
our approach has to do with “leakage” of additional informa-
tion when respondents use several clients that participate in
the same collection event. In the real world, this problem
is mitigated to some extent by intrinsic diﬃculty of linking
diﬀerent clients to the same participant. Similar issues occur
when highly correlated, or even exactly the same, predicates
are collected at the same time. This issue, however, can be
mostly handled with careful collection design.

Such inadvertent correlations can arise in many diﬀerent
ways in RAPPOR applications, in each case possibly lead-
ing to the collection of too much correlated information from
a single client, or user, and a corresponding degradation of
privacy guarantees. Obviously, this may be more likely to
happen if RAPPOR reports are collected, from each client,
on too many diﬀerent client properties. However, it may

0.00.20.40.60.81.0Frequency of string vFDRf = 0.25f = 0.5f = 0.7500.10.20.30.40.50.60.70.80.910.00.20.40.60.81.0Frequency of string vProbability of true value (0 or 1) given observed two bits00.10.20.30.40.50.60.70.80.91110100101064also happen in more subtle ways. For example, the number
of cohorts used in the collection design must be carefully
selected and changed over time, to avoid privacy implica-
tions; otherwise, cohorts may be so small as to facilitate
the tracking of clients, or clients may report as part of dif-
ferent cohorts over time, which will reduce their privacy.
RAPPOR responses can even aﬀect client anonymity, when
they are collected on immutable client values that are the
same across all clients:
if the responses contain too many
bits (e.g., the Bloom ﬁlters are too large), this can facilitate
tracking clients, since the bits of the Permanent randomized
responses are correlated. Some of these concerns may not
apply in practice (e.g., tracking responses may be infeasi-
ble, because of encryption), but all must be considered in
RAPPOR collection design.

In particular, longitudinal privacy protection guaranteed
by the Permanent randomized response assumes that client’s
value does not change over time. It is only slightly violated if
the value changes very slowly. In a case of rapidly changing,
correlated stream of values from a single user, additional
measures must be taken to guarantee longitudinal privacy.
The practical way to implement this would be to budget ∞
over time, spending a small portion on each report. In the
RAPPOR algorithm this would be equivalent to letting q
get closer and closer to p with each collection event.

Because diﬀerential privacy deals with the worst-case sce-
nario, the uncertainty introduced by the Bloom ﬁlter does
not play any role in the calculation of its bounds. Depend-
ing on the random draw, there may or may not be multiple
candidate strings mapping to the same h bits in the Bloom
ﬁlter. For the average-case privacy analysis, however, Bloom
ﬁlter does provide additional privacy protection (a ﬂavor of
k-anonymity) because of the diﬃculty in reliably inferring a
client’s value v from its Bloom ﬁlter representation B [4].

7 Related Work
Data collection from clients in a way that preserves their
privacy and at the same time enables meaningful aggregate
inferences is an active area of research both in academia and
industry. Our work ﬁts into a category of recently-explored
problems where an untrusted aggregator wishes to learn the
“heavy hitters” in the clients’ data—or run certain types of
learning algorithms on the aggregated data—while guaran-
teeing the privacy of each contributing client, and, in some
cases, restricting the amount of client communication to the
untrusted aggregator [7, 16, 18, 20]. Our contribution is to
suggest an alternative to those already explored that is in-
tuitive, easy-to-implement, and potentially more suitable to
certain learning problems, and to provide a detailed statisti-
cal decoding methodology for our approach, as well as exper-
imental data on its performance. Furthermore, in addition
to guaranteeing diﬀerential privacy, we make explicit algo-
rithmic steps towards protection against linkability across
reports from the same user.

It is natural to ask why we built our mechanisms upon
randomized response, rather than upon two primitives most
commonly used to achieve diﬀerential privacy: the Laplace
and Exponential mechanisms [12, 21]. The Laplace mech-
anism is not suitable because the client’s reported values
may be categorical, rather than numeric, in which case di-
rect noise addition does not make semantic sense. The Ex-
ponential mechanism is not applicable due to our desire to

implement the system in a local model, where the privacy
is ensured by each client individually without a need for a
trusted third party. In that case, the client does not have
suﬃcient information about the data space in order to do
the necessary biased sampling required by the Exponential
mechanism. Finally, randomized response has the additional
beneﬁt of being relatively easy to explain to the end user,
making the reasoning about the algorithm used to ensure
privacy more accessible than other mechanisms implement-
ing diﬀerential privacy.

Usage of various dimensionality reduction techniques in
order to improve the privacy properties of algorithms while
retaining utility is also fairly common [1, 17, 20, 22]. Al-
though our reliance on Bloom ﬁlters is driven by a desire
to obtain a compact representation of the data in order to
lower each client’s potential transmission costs and the de-
sire to use technologies that are already widely adopted in
practice [6], the related work in this space with regards to
privacy may be a source for optimism as well [4]. It is con-
ceivable that through a careful selection of hash functions, or
choice of other Bloom ﬁlter parameters, it may be possible
to further raise privacy defenses against attackers, although
we have not explored that direction in much detail.

The work most similar to ours is by Mishra and San-
dler [24]. One of the main additional contributions of our
work is the more extensive decoding step, that provides both
experimental and statistical analyses of collected data for
queries that are more complex than those considered in their
work. The second distinction is our use of the second ran-
domization step, the Instantaneous randomized response, in
order to make the task of linking reports from a single user
diﬃcult, along with more detailed models of attackers’ ca-
pabilities.

The challenge of eliminating the need for a trusted aggre-
gator has also been approached with distributed solutions,
that place trust in other clients [11]. In this manner, dif-
ferentially private protocols can be implemented, over dis-
tributed user data, by relying on honest-but-curious proxies
or aggregators, bound by certain commitments [2, 8].

Several lines of work aim to address the question of lon-
gitudinal data collection with privacy. Some recent work of
considers scenarios when many predicate queries are asked
against the same dataset, and it uses an approach that,
rather than providing randomization for each answer sep-
arately, attempts to reconstruct the answer to some queries
based on the answers previously given to other queries [25].
The high-level idea of RAPPOR bears some resemblance
to this technique–the Instantaneous randomized response is
reusing the result of the Permanent randomized response
step. However, the overall goal is diﬀerent—rather than
answering a diverse number of queries, RAPPOR collects
reports to the same query over data that may be changing
over time. Although it does not operate under the same local
model as RAPPOR, recent work on pan-private streaming
and on privacy under continual observation introduces addi-
tional ideas relevant for the longitudinal data collection with
privacy [13, 14].

8 Summary
RAPPOR is a ﬂexible, mathematically rigorous and practi-
cal platform for anonymous data collection for the purposes
of privacy-preserving crowdsourcing of population statistics

1065on client-side data. RAPPOR gracefully handles multiple
data collections from the same client by providing well-deﬁned
longitudinal diﬀerential privacy guarantees. Highly tunable
parameters allow to balance risk versus utility over time,
depending on one’s needs and assessment of likelihood of
diﬀerent attack models. RAPPOR is purely a client-based
privacy solution. It eliminates the need for a trusted third-
party server and puts control over client’s data back into
their own hands.
Acknowledgements
The authors would like to thank our many colleagues at
Google and its Chrome team who have helped with this
work, with special thanks due to Steve Holte and Moti Yung.
Thanks also to the CCS reviewers, and many others who
have provided insightful feedback on the ideas, and this
paper, in particular, Frank McSherry, Arvind Narayanan,
Elaine Shi, and Adam D. Smith.

9 References

[1] C. C. Aggarwal and P. S. Yu. On privacy-preservation

of text and sparse binary data with sketches. In
Proceedings of the 2007 SIAM International Conference
on Data Mining (SDM), pages 57–67, 2007.

[2] I. E. Akkus, R. Chen, M. Hardt, P. Francis, and

J. Gehrke. Non-tracking web analytics. In Proceedings of
the 2012 ACM Conference on Computer and
Communications Security (CCS), pages 687–698, 2012.

[3] Y. Benjamini and Y. Hochberg. Controlling the false
discovery rate: A practical and powerful approach to
multiple testing. Journal of the Royal Statistical Society
Series B (Methodological), 57(1):289–300, 1995.

[4] G. Bianchi, L. Bracciale, and P. Loreti. ‘Better Than

Nothing’ privacy with Bloom ﬁlters: To what extent? In
Proceedings of the 2012 International Conference on
Privacy in Statistical Databases (PSD), pages 348–363,
2012.

[5] B. H. Bloom. Space/time trade-oﬀs in hash coding with

allowable errors. Commun. ACM, 13(7):422–426, July
1970.

[6] A. Z. Broder and M. Mitzenmacher. Network

applications of Bloom ﬁlters: A Survey. Internet
Mathematics, 1(4):485–509, 2003.

[7] T.-H. H. Chan, M. Li, E. Shi, and W. Xu. Diﬀerentially

private continual monitoring of heavy hitters from
distributed streams. In Proceedings of the 12th
International Conference on Privacy Enhancing
Technologies (PETS), pages 140–159, 2012.

[8] R. Chen, A. Reznichenko, P. Francis, and J. Gehrke.

Towards statistical queries over distributed private user
data. In Proceedings of the 9th USENIX Conference on
Networked Systems Design and Implementation (NSDI),
pages 169–182, 2012.

[10] C. Dwork. A ﬁrm foundation for private data analysis.

Commun. ACM, 54(1):86–95, Jan. 2011.

[11] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,

and M. Naor. Our data, ourselves: Privacy via
distributed noise generation. In Proceedings of 25th
Annual International Conference on the Theory and
Applications of Cryptographic Techniques
(EUROCRYPT), pages 486–503, 2006.

[12] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data analysis.
In Proceedings of the 3rd Theory of Cryptography
Conference (TCC), pages 265–284, 2006.

[13] C. Dwork, M. Naor, T. Pitassi, and G. N. Rothblum.

Diﬀerential privacy under continual observation. In
Proceedings of the 42nd ACM Symposium on Theory of
Computing (STOC), pages 715–724, 2010.

[14] C. Dwork, M. Naor, T. Pitassi, G. N. Rothblum, and

S. Yekhanin. Pan-private streaming algorithms. In
Proceedings of The 1st Symposium on Innovations in
Computer Science (ICS), pages 66–80, 2010.

[15] J. Hsu, M. Gaboardi, A. Haeberlen, S. Khanna,

A. Narayan, B. C. Pierce, and A. Roth. Diﬀerential
privacy: An economic method for choosing epsilon. In
Proceedings of 27th IEEE Computer Security
Foundations Symposium (CSF), 2014.

[16] J. Hsu, S. Khanna, and A. Roth. Distributed private
heavy hitters. In Proceedings of the 39th International
Colloquium Conference on Automata, Languages, and
Programming (ICALP) - Volume Part I, pages 461–472,
2012.

[17] K. Kenthapadi, A. Korolova, I. Mironov, and

N. Mishra. Privacy via the Johnson-Lindenstrauss
transform. Journal of Privacy and Conﬁdentiality,
5(1):39–71, 2013.

[18] D. Keren, G. Sagy, A. Abboud, D. Ben-David,

A. Schuster, I. Sharfman, and A. Deligiannakis.
Monitoring distributed, heterogeneous data streams:
The emergence of safe zones. In Proceedings of the 1st
International Conference on Applied Algorithms
(ICAA), pages 17–28, 2014.

[19] D. Kifer and A. Machanavajjhala. No free lunch in

data privacy. In Proceedings of the ACM SIGMOD
International Conference on Management of Data
(SIGMOD), pages 193–204, 2011.

[20] B. Liu, Y. Jiang, F. Sha, and R. Govindan.

Cloud-enabled privacy-preserving collaborative learning
for mobile sensing. In Proceedings of the 10th ACM
Conference on Embedded Network Sensor Systems
(SenSys), pages 57–70, 2012.

[9] Chromium.org. Design Documents: RAPPOR

(Randomized Aggregatable Privacy Preserving Ordinal
Responses). http://www.chromium.org/developers/
design-documents/rappor.

[21] F. McSherry and K. Talwar. Mechanism design via
diﬀerential privacy. In Proceedings of the 48th Annual
IEEE Symposium on Foundations of Computer Science
(FOCS), pages 94–103, 2007.

1066[22] D. J. Mir, S. Muthukrishnan, A. Nikolov, and R. N.

Wright. Pan-private algorithms via statistics on sketches.
In Proceedings of Symposium on Principles of Database
Systems (PODS), pages 37–48, 2011.

[23] I. Mironov. On signiﬁcance of the least signiﬁcant bits

for diﬀerential privacy. In Proceedings of ACM
Conference on Computer and Communications Security
(CCS), pages 650–661, 2012.

[24] N. Mishra and M. Sandler. Privacy via pseudorandom

sketches. In Proceedings of Symposium on Principles of
Database Systems (PODS), pages 143–152, 2006.

[25] A. Roth and T. Roughgarden. Interactive privacy via
the median mechanism. In Proceedings of the 42nd ACM
Symposium on Theory of Computing (STOC), pages
765–774, 2010.

[26] R. Tibshirani. Regression shrinkage and selection via

the Lasso. Journal of the Royal Statistical Society, Series
B, 58:267–288, 1994.

[27] S. L. Warner. Randomized response: A survey

technique for eliminating evasive answer bias. Journal of
the American Statistical Association, 60(309):pp. 63–69,
1965.

[28] Wikipedia. Randomized response.

http://en.wikipedia.org/wiki/Randomized_response.

APPENDIX
Observation 1
For a, b ≥ 0 and c, d > 0 : a+b
Proof. Assume wlog that a

c+d ≤ max( a

c , b

d ).

c ≥ b

d , and suppose the state-
c . Then ac + bc > ac + ad or

ment is false, i.e., a+b
bc > ad, a contradiction with assumption that a

c+d > a

c ≥ b
d .

Deriving Limits on Learning
We consider a Basic One-time RAPPOR algorithm to estab-
lish theoretical limits on what can be learned using a par-
ticular parameter conﬁguration and a number of collected
reports N . Since the Basic One-time RAPPOR is more ef-
ﬁcient (lossless) than the original RAPPOR, the following
provides a strict upper bound for all RAPPOR modiﬁca-
tions.

Decoding for the Basic RAPPOR is quite simple. Here,
we assume that f = 0. The expected number that bit i is
set in a set of reports, Ci, is given by

E(Ci) = qTi + p(N − Ti),

where Ti is the number of times bit i was truly set (was the
signal bit). This immediately provides the estimator

ˆTi =

Ci − pN
q − p

.

It can be shown that the variance of our estimator under

the assumption that Ti = 0 is given by
p(1 − p)N
(q − p)2 .

V ar( ˆTi) =

Determining whether Ti is larger than 0 comes down to
statistical hypothesis testing with H0 : Ti = 0 vs H1 : Ti >
0. Under the null hypothesis H0 and letting p = 0.5, the
standard deviation of Ti equals

sd( ˆTi) =

We reject H0 when

√
N
2q − 1

.

ˆTi > Q × sd( ˆTi)

√

Q
N
2q − 1

,

>

where Q is the critical value from the standard normal distri-
bution Q = Φ−1(1− 0.05
M ) (Φ−1 is the inverse of the standard
Normal cdf). Here, M is the number of tests; in this case,
it is equal to k, the length of the bit array. Dividing by M ,
the Bonferroni correction, is necessary to adjust for multiple
testing to avoid a large number of false positive ﬁndings.

Let x be the largest number of bits for which this condition
is true (i.e., rejecting the null hypothesis). x is maximized
when x out of M items have a uniform distribution and a
combined probability mass of almost 1. The other M − x
bits have essentially 0 probability. In this case, each non-
zero bit will have frequency 1/x and its expected count will
be E( ˆTi) = N/x ∀i.
Thus we require

√

N
x

>

Q
N
2q − 1

,

N

.

where solving for x gives

√
x ≤ (2q − 1)

Q

1067