Practical Comprehensive Bounds 

on Surreptitious Communication Over DNS

Vern Paxson, University of California, Berkeley, and International Computer Science Institute;  
Mihai Christodorescu, Qualcomm Research; Mobin Javed, University of California, Berkeley; 

Josyula Rao, Reiner Sailer, Douglas Lee Schales, and Marc Ph. Stoecklin, IBM Research;  

Kurt Thomas, University of California, Berkeley; Wietse Venema, IBM Research;  

Nicholas Weaver, International Computer Science Institute  

and University of California, San Diego

Open access to the Proceedings of the 22nd USENIX Security Symposium is sponsored by USENIXThis paper is included in the Proceedings of the 22nd USENIX Security Symposium.August 14–16, 2013 • Washington, D.C., USAISBN 978-1-931971-03-4Practical Comprehensive Bounds on Surreptitious

Communication Over DNS

Vern Paxson(cid:31)∗ Mihai Christodorescu† Mobin Javed(cid:31) Josyula Rao‡ Reiner Sailer‡

Douglas Schales‡ Marc Ph. Stoecklin‡ Kurt Thomas(cid:31) Wietse Venema‡ Nicholas Weaver∗§

(cid:31)UC Berkeley

∗ICSI

†Qualcomm Research ‡IBM Research §UC San Diego

Abstract

DNS queries represent one of the most common forms of net-
work trafﬁc, and likely the least blocked by sites. As such, DNS
provides a highly attractive channel for attackers who wish to
communicate surreptitiously across a network perimeter, and
indeed a variety of tunneling toolkits exist [7, 10, 13–15]. We
develop a novel measurement procedure that fundamentally
limits the amount of information that a domain can receive sur-
reptitiously through DNS queries to an upper bound speciﬁed
by a site’s security policy, with the exact setting representing
a tradeoff between the scope of potential leakage versus the
quantity of possible detections that a site’s analysts must inves-
tigate.

Introduction

Rooted in lossless compression, our measurement procedure
is free from false negatives. For example, we address conven-
tional tunnels that embed the payload in the query names, tun-
nels that repeatedly query a ﬁxed alphabet of domain names
or varying query types,
tunnels that embed information in
query timing, and communication that employs combinations
of these. In an analysis of 230 billion lookups from real produc-
tion networks, our procedure detected 59 conﬁrmed tunnels.
For the enterprise datasets with lookups by individual clients,
detecting surreptitious communication that exceeds 4 kB/day
imposes an average analyst burden of 1–2 investigations/week.
1
Some of the most serious security threats that enterprises
face concern the potential use of surreptitious communi-
cation (Figure 1). One such scenario takes the form of
exﬁltration, when an attacker with internal access aims
to transmit documents or other substantive data out of
the enterprise to a remote location [4]. Another scenario
arises in the context of interactive remote access: an at-
tacker who has patiently compromised a local system
subsequently interacts with it over the network in order
to assay the information it holds and employ it as an in-
ternal stepping stone for further probing of the enterprise.
DNS plays a pervasive role in Internet communica-
tion; indeed, the vast majority of any Internet commu-
nication ultimately begins with DNS queries. Even sites
that are highly security-conscious will ﬁnd that they still

INTERNET

INTERNET

DNS server
DNS server
controlled
by attacker

DNS server

N ENTERPRISE
O
I
T
A
R
T
L
I
F
X
E
A
T
A
D

Hijacked system

 

S ENTERPRISE

 

S
E
C
C
A
E
T
O
M
E
R

DNS server

Hijacked system

Figure 1: Two examples of surreptitious communication via
DNS tunnels through perimeter ﬁrewalls.

must allow internal clients to issue DNS queries and
receive the replies. Unless sites can restrict their sys-
tems to only intra-enterprise communication, some of
these queries will necessarily reach external systems,
giving attackers the opportunity to piggyback their ac-
tual communication over seemingly benign DNS trafﬁc.
Thus, DNS provides a highly-attractive target for attack-
ers seeking a means of surreptitious communication.

We note that such communication fundamentally can-
not be detected at the level of individual DNS queries.
For example, an attacker could exﬁltrate only one bit
of information per day by having a local system under
their control each day issue a single query for either
www.attacker.com or mail.attacker.com, where the label
used (www or mail) conveys either a 0 bit or a 1 bit.1 It
will prove intractable for a site’s security analysts (or any
detection tool) to tell that such requests reﬂect adversar-
ial activity, absent a great deal of additional information.
In this work we develop a principled means—rooted
in assessments of information-theoretic entropy and free
from false negatives—by which sites can analyze their

1 We assume that the attacker controls the attacker.com DNS zone.

USENIX Association  

1

22nd USENIX Security Symposium  17

DNS activity and detect the presence of surreptitious
communication whose volume exceeds a conﬁgurable
bound. Simpler metrics, such as volume of DNS trafﬁc,
are not useful to distinguish tunnels from normal query
trafﬁc, because large-scale trafﬁc naturally exhibits a
high degree of diversity (§ 5.2). Approaches that focus
on speciﬁc syntactical patterns [29] will miss communi-
cation with different encodings. Our conﬁgurable bound
on the volume of surreptitious communication over DNS
allows sites to trade off analysis burden (detections re-
quiring investigation) versus assurance that such commu-
nication does not exceed a considerably low level.

We formulate this detection problem as having three
main components. The ﬁrst concerns constructing a
sound, fairly tight estimate of the amount of information
potentially encoded in a stream of DNS queries. Here
we need to comprehensively identify all potential infor-
mation vectors, i.e., aspects of DNS queries that can en-
code information. The second regards ensuring that we
can compute such estimates with reasonable efﬁciency
in the face of very high volumes of DNS activity (tens of
millions of lookups per day). Finally, we need to assess
to what degree benign DNS query streams encode sig-
niﬁcant amounts of information, and formulate effective
ways of minimizing the burden that such benign activity
imposes on a site’s security assessment process.

Thus, we conceptualize our overall goal as providing
a site’s security analysts with a high-quality, tractable set
of domains for which the corresponding DNS lookups
potentially reﬂect surreptitious communication. We view
it as acceptable that the analyst then needs to conduct a
manual assessment to determine which of the candidates
actually reﬂects a problem, provided that we keep the set
small and the process of eliminating a benign candidate
does not require much attention.

This work makes the following contributions:
• We introduce a principled means of detecting the
presence of surreptitious communication over DNS,
parameterized by a (conﬁgurable) bound on the
amount of information conveyed.

• Our approach is comprehensive because we root our
estimates of information conveyed in DNS lookups
in lossless compression of entire query streams.

• We perform an in-depth empirical analysis of
mostly-benign DNS trafﬁc on an extensive set
of traces comprising 230 billion queries observed
across a variety of sites. For enterprise datasets
with lookups by individual clients, we ﬁnd that a
bound of 4 kB/day per client and registered domain
name imposes an operationally viable analysis bur-
den. Thus, we argue that our procedure proves prac-
tical for real-world operational use.

After a summary of our information measurement pro-
In § 4 we

cedure, we deﬁne the threat model in § 3.

Bound on information content
Suffix: attacker.com, Client: 10.9.8.7

name
time
type

mincompr(A)

mincompr(D)  
+ mincompr(I)

min

sum

mincompr(x) = min( gzip(x), bzip2(x), ppmd(x) )
A: All Symbols  D: Distinct Symbols 

I: Index of Distinct Symbols

Figure 2: The information measurement procedure, summa-
rized in § 2. Figure 5 shows the full detection procedure.

present the extensive datasets used in our study. We
discuss information vectors potentially present in DNS
queries and ways to estimate their volume in § 5, and
explore implementation issues, including ﬁltering tech-
niques for reducing the resources required, in § 6. We
evaluate the efﬁcacy of our procedure in § 7, present a
real-time detector in § 8, discuss ﬁndings, limitations,
and future work in § 9, and review prior work in § 10.
2 Summary of the information measure-

ment procedure

As explained in § 6.3, we analyze DNS queries per client
and per registered domain name. For example, we aggre-
gate queries with names ending in site1.com, site2.co.uk,
and so on. We also aggregate PTR queries, but ignore
them here for clarity.

We measure the information in query name, time and
record-type sequences separately (§ 5.3). For example,
we transform a sequence of query names A to a sequence
of indices I into a table with distinct names D, and then
compress I and D with gzip. The size of the output then
gives us a measurement of the information in the input
sequence.

The key insight is that we will never under-estimate
the information in a query sequence as long as the trans-
formation and compression are reversible, i.e., we can
recover the original input sequence. Taking advantage
of this insight, we subject each query attribute sequence
to multiple (transformation, compressor) alternatives and
use the minimal result as the tightest (upper) bound.

Figure 2 illustrates our measurement procedure. For
each client and registered domain we compress both the
original and transformed query name sequences with
gzip, bzip2 and ppmd [23], and take the size of the small-
est output. We apply the same procedure to the record-
type sequences and 32-bit inter-query arrival time dis-
tances, and from these compute a combined score.

18  22nd USENIX Security Symposium 

2

USENIX Association

Site

INDLAB
LBL
NERSC
UCB
CHINA
SIE

Features

L,N,Q,T

N,Q
N,Q
N,Q,T
N,Q,T

N

Vantage
point

Notes

I
I
I
E
I/E
A

a
b
c, d

Time
span
1,212 d
2,776 d
1,642 d
45 d
5 d
53 d

Daily statistics: Average (Daily peak)

Clients
10k (16k)
6.8k (11k)
1.3k (3.3k)
2.1k (5.1k)
61k (101k)
123∗ (123∗)

Total lookups
47M (164M)
28M (154M)
9M (59M)
38M (52M)
13.9M (15.7M)
1.45B (1.84B)

Distinct lookups
(2.4M)
310k
(2.2M)
867k
44k
(114k)
3.3M (4.4M)
468k
(670k)
110M (129M)

Table 1: Summary of data sources. The available features are: 0x20-encoding [27] (C), caching lifetime derived from reply
time-to-live (L), query name (N), query type (Q), and timing (T). Sensor vantage points are: aggregated across multiple sites (A),
external to site (E), lookups associated with individual clients as seen at internal name servers (I), a mixture of these last two (I/E).
a The raw UCB dataset includes resolvers that employ 0x20-encoding [27] as well as a single system conducting high-volume DNS
lookups for research purposes. We preprocessed this dataset by removing lookups from the research system (totaling more than
250M) and downcasing lookups from 0x20-resolvers (cf. § 5.3). (Note that the dataset has 3 days with only partial information.)
b This dataset’s ﬁrst day starts at 7AM local time rather than midnight. The other days are complete.
c “Clients” in the SIE dataset instead reﬂect site resolvers, each with potentially thousands of actual clients.
d As discussed later, we omit from our evaluation the PTR reverse lookups in this data, which comprise about 10% of the lookups.

3 Threat Model

Our basic model assumes that the attacker controls both a
local system and a remote name server. The local system
will communicate with the remote name server solely by
issuing lookups for DNS names that the site’s resolver
will ultimately send to the attacker’s name server. The
attacker inspects both the content of these queries (i.e.,
the names and the associated query type, such as TXT or
AAAA) and their arrival timing.

We further assume that the internal system under the
attacker’s control makes standard queries, either because
the site’s ﬁrewalling requires internal systems to use the
site’s own resolvers, or because non-standard queries
made directly to the public Internet could expose the
communication’s anomalous nature.

For the investigation we develop in this work, we focus
(We
on communication outbound from local systems.
brieﬂy discuss inbound communication encoded in DNS
replies in § 9.) We view the outbound direction as the
most apt when concerned about exﬁltration threats. In
addition, for the interactive communication scenario, the
outbound direction corresponds to the replies generated
by a local login server in response to keystrokes sent by
a remote client. The outbound trafﬁc volume to the lo-
gin client is typically 20 times larger than the incoming
trafﬁc [21], making DNS queries embedding outbound
trafﬁc the larger target for that scenario, too.

We do not consider here communication that an at-
tacker spreads across multiple remote domains or mul-
tiple remote name servers (such as attacker1.com, . . .,
attackern.com), nor spread thinly across multiple local
clients. We discuss these and other evasion issues in § 9.

4 The Data
For our analysis we draw upon datasets that together
comprise 230 billion queries. The data was collected at
multiple locations across the US and China, with van-
tage points ranging from internal DNS servers to network
perimeters. We summarize each dataset and its daily traf-
ﬁc statistics in Table 1.

INDLAB: an industrial research laboratory. Collected
with a network sniffer near an internal DNS server, this
dataset contains queries from internal clients, the reply
time-to-live, and microsecond-resolution time stamps.

LBL: a national research laboratory. This dataset con-
tains DNS queries from local clients received by several
internal DNS servers. Covering a time span of 7.5 years,
this is the largest data set in our analysis.

NERSC: a super-computer center. The dataset con-
tains queries from local clients to the site’s DNS servers.
UCB: a university campus. This data was collected
on a perimeter network, providing an aggregate view of
(outbound) DNS query trafﬁc. This site includes servers
that use 0x20 encoding [27], which nearly doubles the
number of distinct lookup names.

CHINA: a caching server for several university net-
works in China, with visibility of individual client IP ad-
dresses.

SIE: the Security Information Exchange of the Inter-
net Software Consortium [24]. In this collaboration of
infrastructure providers, law enforcement, security com-
panies and researchers, participants2 mirror their DNS
reply trafﬁc from name servers across the Internet. (Note
that each reply contains a copy of the query.)

With a combined average of 1.5 billion replies a day,
SIE has by far the highest data rate in our collection.

2Heavily dominated by a single large U.S. ISP.

USENIX Association  

3

22nd USENIX Security Symposium  19

However, we note that we use it as a means of assess-
ing to what degree our detection procedure indeed can
ﬁnd actual instances of surreptitious communication over
DNS; we do not claim our procedure is tenable for actual
operational use in this environment, which is hugely ag-
gregated across (likely) millions of actual clients.
5 Establishing Communication Bounds
In this section we develop a principled approach for
bounding the amount of information possibly conveyed
by local systems to remote name servers. The next sec-
tion then presents a number of ﬁltering steps that reduce
the resources required for detecting communication that
exceeds these bounds.
5.1
We ﬁrst frame the basic communication mechanisms an
attacker could employ. In general terms, we consider an
attacker who wishes to communicate a signiﬁcant quan-
tity of information by sending DNS queries to a remote
domain (say D.com) whose name server(s) the attacker
controls. Such queries provide a number of information
vectors that the attacker can exploit to surreptitiously em-
bed data within the stream of queries.

Information Vectors

We note that attackers can potentially employ multiple
vectors at the same time. We emphasize that our detec-
tion scheme does not presume use of particular encod-
ings for a given vector; the encodings we give here are
just meant to illustrate the possibilities.
Query name-content vector. A conceptually straight-
forward way to embed data is for the attacker to devise a
data encoding that conforms with the requirements im-
posed on DNS labels, limiting each to no more than
63 bytes in length, and complete DNS names to no more
than 255 bytes [18]. For example, one could use Base-64
encoded data strings as such as VVNFQwo.D.com.

To our knowledge, all available tunneling-over-DNS

tools reﬂect this style of approach.
Query name-codebook vector. Rather than using each
DNS query to reﬂect a message many bytes long, at-
tackers can encode messages using a ﬁxed alphabet of
symbols and then transmit those symbols one at a time
using a series of queries. For example, to convey the bit-
string 00101111 one bit at a time, a client could issue the
queries: z.D.com, z.D.com, o.D.com, z.D.com, o.D.com,
o.D.com, o.D.com, o.D.com. They could of course also
use larger alphabets to obtain greater efﬁciency.

Encodings using this vector will in general generate
many more lookups of the same names over time com-
pared to those using the query name-content vector.
Query type vector. Along with the query name, clients
include in their requests the type of DNS Resource
Record they wish to resolve, such as PTR for reverse-
IP-address-to-hostname mappings, or AAAA to look up

IPv6 addresses. Attackers can encode a modest amount
of information per query using this 16-bit ﬁeld.
Query timing vector. A more subtle information vec-
tor exists in the speciﬁc timing of queries. For example,
if the attacker can resolve the arrival times of queries to
1 sec precision, then the attacker can use the number of
seconds between successive queries as a means of con-
veying information.3

A key issue for timing vectors concerns clock preci-
sion. With an extremely precise clock (and sufﬁciently
low jitter), intervals between queries can convey several
bytes of information without requiring very large inter-
query delays. For example, transmitting one query every
second using a clock with 1 msec precision can convey
lg103 bits per query, totaling more than 108 KB per day.
Other information vectors. Inspecting the DNS query
format reveals several additional ﬁelds possibly avail-
able for communicating information: query identiﬁers, a
number of ﬂags, options, the query count, and the 16-bit
address class ﬁeld included in each query. We argue that
none of these provide a reliable end-to-end information
vector for an attacker, given the assumption in our threat
model that the attacker’s client must relay its queries via
a site’s standard (non-cooperative) resolver. Such re-
layed queries will not preserve query identiﬁers. The
ﬂags either do not survive the relaying process (e.g., Re-
cursion Desired) or will appear highly anomalous if they
vary (e.g., requesting DNSSEC validation), and likewise
DNS options (EDNS0) do not survive relaying, as un-
known options return an error [26], and the current op-
tions themselves are generally implemented on a hop-
by-hop basis. Similarly, query counts other than 1 would
appear highly anomalous and likely fail to actually prop-
agate through the site’s name server. Likewise, use of
any address class value other than IN (Internet) would
be readily detectable as anomalous.
5.2 Challenge: Diversities Seen in Practice
A natural starting point when attempting to detect surrep-
titious DNS communication is to posit that the encodings
used for the communication will stand out as strikingly
different than typical DNS activity. If so, we can target
the nature of the encoding for our detection.

What we ﬁnd, however, is that while potential encod-
ings may differ from typical DNS activity, they do not
sufﬁciently stand out from the diverse range of benign
activity. When we monitor at a large scale—such as
analyzing the trafﬁc from the 1000s of systems in an
enterprise—we observe a striking degree of fairly ex-
treme forms of DNS lookups.

3In addition, the speciﬁc query received after the given interval
could also convey additional information using one of the previously
described vectors.

20  22nd USENIX Security Symposium 

4

USENIX Association

Figure 3: Distribution of the lengths of all individual (solid)
or all distinct (dashed) domain name preﬁxes queried during
a sample day of data from LBL. The horizontal lines mark
that 76K (all) and 58K (distinct) lookups were ≥ 100 bytes.
Lengths do not include the registered domain targeted by the
lookup. Note that the plot shows the upper 1% of all queries,
but the upper 18% of all distinct queries.

To illustrate, we consider DNS activity observed on
a sample day in 2011 at LBL. It includes 35M queries
issued from 9.4k hosts. These queries in total span
1.2M distinct names, and if we discard the ﬁrst com-
ponent of each name, 620K distinct subdomains. These
subdomains are themselves rooted in 137K distinct reg-
istered domains (i.e., one level under com or co.uk).

One natural question concerns the frequency with
which operational DNS trafﬁc exhibits peculiarly long
query names, since many natural encodings for surrep-
titious communication will aim to pack as much infor-
mation into each query as possible. Figure 3 shows the
distribution of domain name preﬁx lengths ≥ 50 bytes
(i.e., characters) looked up in our sample day. We see
that queries with names even larger than 100 bytes oc-
cur routinely: while rare in a relative sense (only 0.2% of
query names are this large), 76,523 such queries occurred
on that day. Restricting our analysis to distinct names
(dashed line) does not appreciably lower this prevalence.
For concreteness, here are some examples of what

such queries look like:
JohnsonHouse\032Officejet....sonhouse1.members.mac.com
www.10.1.2.3.static.becau....orant.edu.za.research.edu

awyvrvcataaaegdid5tmr7ete....ilu.license.crashplan.com

g63uar2ejiq5tlrkg3zezf2fk....emc6pi88tz.er.spotify.com

5.1o19sr00ors95qo0p73415p....7rn92.i.02.s.sophosxl.net
where we have elided between 63 characters (ﬁrst exam-
ple) and 197 characters (last example). See Appendix A
for the complete names.

Thus, simply attempting to detect queries that include
unusually large names does not appear viable. Similarly,
the examples above illustrate that benign trafﬁc already
includes DNS queries that use opaque encodings, so we
do not see a promising angle to pursue with regard to rec-
ognizing surreptitious communication due to the syntax
of its encoding.

Figure 4: Distribution of the total length of domain name pre-
ﬁxes sent to different registered domains, computed as the sum
of all names (solid) or distinct names (dashed). The horizon-
tal lines mark that 1,186 registered domains received ≥ 4kB of
names, while 114 received ≥ 4kB of distinct names.

A different perspective we might pursue is that if only
a small number of remote name servers receive the bulk
of the site’s queries, then we might be able to explic-
itly examine each such set of trafﬁc. Figure 3, however,
shows that large volumes of queries are spread across
numerous remote name servers. The plot shows how
many registered domains received a given total size of
queries (the sum of the lengths of all of the preﬁxes sent
to that domain). If we restrict our view to the total size of
distinct queries that a registered domain receives, more
than 100 registered domains each received in excess of
4kB of query names. If we include preﬁxes for repeated
lookups, the ﬁgure is ten times higher.

Surprising query diversity also manifests in other di-
mensions. For example, surreptitious communication
that leverages the transmission of repeated queries in a
codebook-like fashion requires using low-TTL answers
to prevent local caching from suppressing queries. How-
ever, we ﬁnd that in benign trafﬁc, low TTLs are not un-
usual: in a day of queries for external names that we ex-
amined, a little under 1% of the answers had TTLs of 0
or 1, and 38% are ≤ 60 sec. We also ﬁnd instances of
large numbers of repeated queries arising from benign
activity such as misconﬁgurations and failures.

In summary, the variations we ﬁnd operationally are
surprisingly rich—enough so to illustrate that our prob-
lem domain will not lend itself to conceptually simple
approaches due to the innate diversity that benign DNS
lookups manifest when observed at scale.

To illustrate the difﬁculty, we evaluated the perfor-
mance of a naive detector that simply sums up the vol-
ume of lookups sent to each domain, alerting on any
client sending the domain more than 4,096 bytes in one
day. In steady-state (using the same methodology as in
§ 7, including the Identiﬁed Domain List discussed be-
low), this detector produces 200x more alerts than our
actual procedure. If we alter the detector to only sum the
volume of distinct lookups, we still must abide 5x more

USENIX Association  

5

22nd USENIX Security Symposium  21

501001502001100Length of domain names (bytes)All lookupsDistinct lookupsNumberoflengths≥ X(log)100001e+001e+021e+041e+061e+08110010000Total size of domain names looked up (bytes)All lookupsDistinct lookupsNumberofsizes≥X[log]4kBalerts (and lose the ability to detect codebook-style en-
codings). We emphasize that because our actual proce-
dure has no false negatives, all of these additional alerts
represent false positives.
5.3 Establishing Accurate Bounds

on

Query Stream Information Content

Given that simple heuristic detection approaches will not
sufﬁce due to the innate diversity of DNS queries, we
now pursue developing principled, direct assessments of
upper bounds on the volume of data a given client poten-
tially transmits in its queries.

A key observation is that—provided we do not under-
estimate the potential data volume—we can avoid any
false negatives; our procedure will indeed identify any
actual surreptitious communication of a given size over
DNS. Given this tenet, the art then becomes formulating
a sufﬁciently tight upper bound so we do not erroneously
ﬂag lookups from a client to a given domain as reﬂecting
a signiﬁcantly larger volume of information than actually
transmitted.

We can obtain tight bounds by quantifying the size
of carefully chosen representations of a client’s query
stream. If we obtain these representations in a lossless
fashion (i.e., we can recover the original query stream
from the representation), then the bound is necessarily
conservative in the sense of never underestimating the
true information content of the queries. At the same time,
the representation must be compact enough to reduce any
redundancy from the query stream as efﬁciently as possi-
ble in order to obtain a tight estimate. Thus, the task we
face is to determine a representation of the query stream
that efﬁciently captures its elements, but does so in a re-
versible fashion. In general, we seek forms of lossless
compression with high compression ratios.

Conceptually, the heart of our approach is to take
encoded query streams and feed them to compression al-
gorithms such as gzip, using the size of the compressor’s
output as our estimate. While simple in abstract terms,
pursuing this effectively requires (1) care in encoding the
streams to try to achieve as tight a bound as possible,
and (2) structuring the analysis procedure to execute ef-
ﬁciently given a huge volume of data to process.

For the rest of this section, we address the ﬁrst of these

issues. We then discuss execution efﬁciency in § 6.
Character casing. The ﬁrst question regarding encoding
query streams concerns the most obvious source of varia-
tion, namely the particular names used in the queries. For
these, one signiﬁcant encoding issue concerns casing.
While the DNS speciﬁcation states that names are treated
in a case-insensitive manner, in practice resolvers tend to
forward along names with whatever casing a client em-
ploys when issuing the query to the resolver.

Together, these considerations mean that, for example,

a query for foo.D.com and FoO.D.COM will both arrive at
the same D.com name server, with the casing of the full
query name preserved. Accordingly, we must downcase
query name sufﬁxes in order to correctly group them to-
gether (i.e., to account for the fact that the same name
server will receive them), but preserve casing in terms of
computing information content, since indeed the attacker
can extract one bit of information per letter in a query
(including the domain itself) depending on its casing.
0x20-encoding. Preserving casing in queries can raise
a difﬁculty for formulating tight bounds on informa-
tion content due to the presence of 0x20-encoding [27],
which seeks to artiﬁcially increase the entropy in DNS
queries to thwart some forms of blind-spooﬁng attacks.
While the presence of arbitrary casing due to use of
0x20-encoding does indeed reﬂect an increase in the ac-
tual information content of a stream of queries, this par-
ticular source of variation is not of use to the attacker;
they cannot in fact extract information from it.

We found that unless we take care, our UCB dataset,
which includes queries from a number of resolvers that
employ 0x20-encoding, will indeed suffer from signiﬁ-
cant overestimates of query stream information content.
The presence of such resolvers however means that their
clients cannot exploit casing as an information vector,
since the resolver will destroy the client’s original cas-
ing. Accordingly, we developed a robust procedure (de-
tails omitted due to limited space) for identifying queries
emanating from resolvers that employ 0x20-encoding.
For those query sources we downcase the queries to ac-
curately reﬂect that casing does not provide any informa-
tion.

This procedure identiﬁed 205 clients in the UCB

dataset. Other than those clients, we left casing intact.
Employing codepoints. General compressors such as
gzip do not make any assumptions about the particular
structure of the data they process. However, our partic-
ular problem domain has certain characteristics that can
improve the compression process if we can arrange to
leverage them. In particular, we know that DNS query
streams often repeat at the granularity of entire queries.
We can expose this behavior to a general compressor
by constructing codepoints, as follows. We preprocess
a given client’s query stream, replacing each distinct
query with a small integer reﬂecting an index into a ta-
ble that enumerates the distinct names. For example, this
would reduce a query stream of foo.X.com, bar.X.com,
bar.X.com, foo.X.com, bar.X.com to the stream 1, 2, 2,
1, 2, plus a dictionary that maps 1 to foo.X.com and 2
to bar.X.com. The particular encoding we use employs
24-bit integers (we take care in our information-content
estimation to include the dictionary size).
Representing query types. For datasets that include
query types, we construct a separate, parallel compres-

22  22nd USENIX Security Symposium 

6

USENIX Association

sion stream for processing the corresponding 16-bit val-
ues, i.e., we do not intermingle the query types with the
query names.
Representing timing.
Individual query timings offer
only quite limited information content. Thus, for an at-
tacker to make effective use of timing, they will need to
send a large number of queries. This means that we likely
will beneﬁt from capturing not absolute timestamps but
intervals between queries. We compute such intervals as
32-bit integers representing multiples of R, our assumed
lower bound on the timing resolution the attacker can
achieve. Again we construct a separate, parallel com-
pression stream for processing these.

Clearly, the value of R can signiﬁcantly affect the
amount of information the attacker can extract from the
timing of queries; but R will be fundamentally limited by
network jitter. To formulate a defensible value of R, we
asked the authors of [17] regarding what sort of timing
variation their measurements found for end systems con-
ducting DNS queries. Using measurements from about a
quarter million distinct IP addresses, they computed the
maximum timing difference seen for each client in a set
of 10 DNS queries it issued. The median value of this
difference across all of the clients was 32 msec. Only
a quarter of the clients had a difference under 10 msec.
Accordingly, for our study we have set R to 10 msec.
Constructing uniﬁed estimates. As described above,
we separately process the query names, types, and tim-
ing. Formulating a ﬁnal estimated bound on a query
stream’s information content then is simply a matter
of adding the three corresponding estimates. We note,
though, that by tracking each separately, we can identify
which one contributes the most signiﬁcantly (per Fig-
ure 6 below).

Bakeoffs. Finally, as outlined above we have sev-
eral potential choices to make in formulating our upper-
bound information estimates: which compressor should
we employ? Should we use codepoints or allow the
compressor to operate without them (thus not imposing
the size of the dictionary)? We note that we do not in
fact have to make particular choices regarding these is-
sues; we can try each option separately, and then sim-
ply choose the one that happens to perform best (gener-
ates the lowest information estimates) in a given context.
Such “bakeoffs” are feasible since we employ lossless
techniques to construct our estimates; we know that each
estimate is sound, and thus the lowest of a set is indeed
the tightest upper bound we can obtain.

The drawback with trying multiple approaches, of
course, is that it requires additional computation. In the
next section we turn to how to minimize the computation
we must employ to formulate our estimates.

Implementation

6
The previous section described our approach to devel-
oping an accurate bounds on the amount of information
conveyed using DNS queries to a given domain’s name
server(s). Computing these estimates and acting upon
their corresponding detections, however, raises a number
of issues with regards to reducing the resources required
for employing this approach.

In this section we discuss practical issues that arise
when implementing our detection approach. One signif-
icant set of these concern ﬁltering: either restricting the
DNS queries we examine in order to conserve computing
(or memory) resources, or reducing the burden that our
detection imposes on a site’s security analysts. The key
property of these ﬁltering stages is their efﬁcacy in con-
cert, which is crucial for the scalability of our approach.
Figure 5 shows the different stages of processing in our
detection procedure and how they pare down in several
steps the volume of both the queries that we must exam-
ine and the number of domain name sufﬁxes to consider.
We describe our detection procedure as implemented
for off-line analysis here, and discuss our experiences
with a real-time detector in § 8.
6.1 Cached Query Filter
A query from a DNS client system cannot exﬁltrate in-
formation unless it is forwarded by the recursive resolver.
Thus a highly useful optimization for the internal van-
tage point (as discussed in § 4) is to model the recursive
resolver’s cache and not consider any query where the
resolver obtained the result from its cache.

We can accomplish this by observing the replies with
the TTL ﬁeld. We maintain a shadow cache based on
the query attributes (contained in the reply) and the reply
TTL values, and do not consider later queries until their
information expires from the shadow cache.

The result of this ﬁltering is to eliminate the disadvan-
tage of the internal vantage point, as this ﬁlter ensures
that later stages only process uncached requests. With
the INDLAB dataset, this reduces the number of detec-
tions by about 2x for the timing vector, and about 10%
for query names. Unfortunately not all of our datasets
support this ﬁltering.
6.2 Uninteresting Query Filter
We remove lookups that target domain names within the
local organization itself, or within closely-related orga-
nizations. Due to their relatively high volume, we ﬁnd
that such lookups can result in a large number of detec-
tions, but the likelihood that someone will actually use
a DNS tunnel between such domains will be negligible.
Likewise, we remove lookups of PTR (address-to-name)
records for local and reserved network address ranges.

USENIX Association  

7

22nd USENIX Security Symposium  23

Ingestion
of queries

Pre-processing 

of  queries

Grouping by 

suffixes & clients

Filtering on

suffix and client level

Bound on information content
Suffix: attacker.com, Client: 10.9.8.7

name
time
type

mincompr(A)

mincompr(D)  
+ mincompr(I)

min

sum

mincompr(x) = min( gzip(x), bzip2(x), ppmd(x) )
A: All Symbols  D: Distinct Symbols 

I: Index of Distinct Symbols

Investigate
4,089 queries
1 suffix

…

DNS 
queries
queries

Total input
45M queries
65K suffixes

…

(§ 6.3)

Cached query
filter (§ 6.1)
Removing
41M queries
0 suffixes

Uninteresting  query 

filter (§ 6.2)
Removing
1.5M queries
41 suffixes

Fast entropy
filter (§ 6.4)
Removing
1.8M queries
65K suffixes

<4kB information
content (§ 6.5)

Inspected 

Domain List (§ 6.7)

Removing
78K queries
34 suffixes

Removing
185K queries
11 suffixes

Figure 5: The full detection procedure. The numbers (grey) reﬂect a day at the INDLAB network for which the detection procedure
ﬂagged a new domain name (a relatively rare event).

Finally, we exclude names without a valid global top-
level domain. This eliminates numerous queries from
systems that are misconﬁgured or confused.
6.3 Grouping by Sufﬁx and Client
In this stage of our detection procedure, we compute
statistics per (lookup name sufﬁx, client)-pair that will
serve as input to the lightweight ﬁlter described in § 6.4.
Due to the voluminous nature of our data, we ag-
gregate these statistics at the level of registered domain
names (e.g., one level under com or co.uk). With IPv4
PTR lookups we aggregate at two and three labels un-
der in-addr.arpa (corresponding with /16 or /24 network
ranges), and with IPv6 PTR lookups we aggregate at
12 labels under ip6.arpa (corresponding with /48 net-
work ranges). The reasoning behind these choices is
that shorter PTR sufﬁxes will in general represent large
blocks that are parents to multiple organizations; thus,
the presence of tunneling associated with such sufﬁxes
would require compromise of a highly sensitive infras-
tructure system. In our results for PTR lookups we ﬁnd
no indications of surreptitious communication.

We then compute for each query sufﬁx and client the
numbers of unique and distinct lookup names includ-
ing that sufﬁx, as well as the combined length of those
lookup names. We group sufﬁxes in a case-insensitive
manner, but count as distinct any lookup names that dif-
fer only in case (cf. § 5.3).
6.4 Fast Filtering of Non-Tunnel Trafﬁc
The very high volume of DNS queries means we can ob-
tain signiﬁcant beneﬁt from considering additional mea-
sures for pre-ﬁltering the trafﬁc before we compute the
principled bounds described in § 5. For each domain
sufﬁx, we use computationally lightweight metrics that
overestimate the information content present in the in-

formation vectors described in § 5.1. We then compare
the sum of these metrics across all information vectors
against a minimum-information content threshold, I. If
the sum total (guaranteed to not underestimate) lies be-
low the threshold, the trafﬁc for the corresponding do-
main sufﬁx cannot represent communication of interest.
This approach allows us to short-circuit the detection
process and eliminate early on numerous domain suf-
ﬁxes.
Fast ﬁlter for the query name vector. We consider the
following quantities from a sequence of lookups made by
some host during one day: the total number of lookups
L, the number of distinct query names Dname in those
lookups, and the total number of bytes Cname in those dis-
tinct query names. We remark that we can determine all
three quantities with minimal computational and mem-
ory overhead.

Query name tunnels encode information in terms of
the characters and the repetition patterns of the names
looked up. Each character in a name may convey up to 1
byte of information, contributing up to Cname bytes in to-
tal. According to Shannon’s law, the number of bits con-
veyed per lookup amounts to at most log2 Dname. There-
fore the combined upper bound on information conveyed
in bytes by such a tunnel amounts to:

Iname = Cname + L·

log2 Dname

8

Fast ﬁlter for the query type vector. We ﬁlter the query
type vector similarly. Again, we consider a sequence of
DNS lookups with a given sufﬁx made by some host dur-
If we use Dtype to denote the number of
ing one day.
distinct query types in those lookups and Ctype the total
number of bytes in those distinct query types, we have:

Itype = Ctype + L·

log2 Dtype

8

24  22nd USENIX Security Symposium 

8

USENIX Association

Fast ﬁlter for the query timing vector. The timing vec-
tor is more complicated because we need to discretize
the time information and create symbols representing
the encoded data as it appears in the timing vector. We
parametrize this process by the time resolution R that the
network environment affords to the attacker.

Intuitively, for a given number of lookups L observed
over a day, the amount of potential information encoded
in time is maximal when the number of distinct inter-
arrival times, k, is maximal. This is due to the fact that,
without knowing the distribution of inter-arrival times,
the empirical entropy from the inter-arrival times may be
upper-bounded by L· log2 k, where log2 k is the number
of bits encoded by a single lookup.
As a consequence, to assess the upper-bound on the
information content for a ﬁxed L and an assumed time-
slot size (expressed as time resolution R), we need to
determine into how many distinct inter-arrival times k we
can partition one day into, while imposing as uniform a
distribution of inter-arrival times as possible (i.e., leading
to maximal entropy).

By maximizing k subject to the constraint that the dis-
tribution of distinct inter-arrival times is uniform (omit-
ting details for brevity), and upper-bounding k by L− 1
(the number of intervals), we ﬁnd that we can express
the upper bound on the information amount in the timing
vector as:

Itime = L· log2(cid:31)min(cid:31)L− 1,(cid:30) 2M

L− 1(cid:29) + 1(cid:28)(cid:28)

R denotes the number of time slots with

where M = 86,400
resolution R over one day (86,400 seconds).
Uniﬁed fast ﬁlter. From the above equations, we can
now formulate the following uniﬁed test condition to
handle all types of information vectors:

If Iname + Itype + Itime < I, the sufﬁx is not a
candidate tunnel.

We then eliminate from further detailed analysis the
name sufﬁxes that are not candidate tunnels.
Choosing the thresholds. The fast ﬁlter relies on two
parameters, the information content threshold I and the
time resolution R.
In order to select security-relevant
values for these parameters, we measured their impact
on the analyst’s workload. (Note that in § 5.3 we also
framed empirical evidence that R = 10 msec appears
fairly conservative.) It is clear that both reducing the in-
formation content threshold and reducing the time reso-
lution can increase the false positive rate, and relatedly
the analyst’s workload.

Figure 6 shows how varying these parameters affects
the analyst for INDLAB data. One can see, for exam-
ple, that decreasing the information content threshold I

d
a
o
l
k
r
o
w
t
s
y
l
a
n
A

)
k
e
e
w
r
e
p

s
e
x
ﬃ
u
s
(

100

10

1

1
10

)
k
e
e
w
r
e
p

s
e
x
ﬃ
u
s
(

d
a
o
l
k
r
o
w
t
s
y
l
a
n
A

3

2.5

2

1.5

256

512

1k

2k

4k

8k

16k

Detection threshold (bytes)

name + time + query type
time

name
query type

1

3

10

30

100

Time resolution (ms)

Figure 6: The impact of the information content threshold I
and the time resolution R on the number of sufﬁxes to val-
idate manually per week for the INDLAB dataset. The top
chart reﬂects a value R = 10 msec, and the bottom chart I
= 4,096 bytes.

from 4,096 to 256 bytes (and potentially increasing secu-
rity) would increase the number of domain name sufﬁxes
passed to the analyst for manual inspection 50-fold. The
plot also shows a clear power-law relationship between
analyst workload and I, with the former scaling as ap-
proximately x−1.38 in the latter.

Setting the information content

threshold I to
4,096 bytes and the time resolution R to 10 ms thus
provides a good balance between analyst workload and
potential detections. Sites might of course revisit these
parameters based on their particular threat models and
networking environments.
6.5 Bounding Information Content
For each (sufﬁx, client)-pair that remains after the pre-
ceding ﬁlter steps, we compute the size of gzip, bzip2
and ppmd [23] compression for the series of all corre-
sponding lookup names, selecting the lowest value. We
also assess a codepoint-oriented analysis (§ 5.3), com-
puting the gzip, bzip2 and ppmd compression sizes for
the series of distinct (unique) lookup names, selecting
the lowest value, and adding the lowest value of the gzip,
bzip2 and ppmd compression sizes for the corresponding
distinct lookup name indices. Given these two assess-
ments, we choose the smaller as the best (tightest) upper
bound on the amount of information potentially trans-
ferred through lookup names to the given domain sufﬁx
(cf. box “Bound on Information Content” in Figure 5).

Next, we apply the same procedure to the correspond-
ing inter-query arrival times (in R = 10 msec units) and
query record types, if this information is available. Fi-

USENIX Association  

9

22nd USENIX Security Symposium  25

nally, we add up the results from the lookup name, time
and type information vectors, and if their sum lies below
I, we discard the (sufﬁx, client)-pair.
6.6
Inspected Domain List
We expect sites to employ our analysis procedure over
an extended period of time. For example, once a site sets
it up, it might run as a daily batch job to process the last
24 hours of lookups. An analyst inspects the trafﬁc as-
sociated with any domains ﬂagged by the procedure and
renders a decision regarding whether the activity appears
benign or malicious.

An important observation is that the same benign do-
mains will often reappear day after day, due to the basic
nature of their lookups. However, the analyst needn’t
reexamine such domains, as the verdict will prove the
same. (See § 9 for further discussion of this point.) Given
this, we presume the use of an Inspected Domain List
(IDL) that accumulates previous decisions regarding do-
mains over time. For a given day’s detections, we omit
ﬂagging for the analyst any that already appear on the
IDL. Once populated, such a dynamic list can greatly
reduce the ongoing burden that our detection procedure
places on a site’s analysts.

A ﬁnal issue regarding the IDL concerns its granu-
larity. For example, if our procedure ﬂags s1.v4.ipv6-
exp.l.google.com and we put that precise domain on the
IDL, then this will not spare the analyst from having
to subsequently investigate i2.v4.ipv6-exp.l.google.com.4
However we note that the analyst’s decision process will
focus heavily on registered domains.
In this example,
the analyst will likely quickly decide to mark the detec-
tion as benign because for it to represent an actual prob-
lem would require subversion of some of Google’s name
servers, which would represent an event likely signiﬁ-
cantly more serious than an attacker communicating sur-
reptitiously out of the site. In addition, the analyst will
reach this conclusion simply by inspecting the registered
domain google.com, rather than studying all of the sub-
domains in depth.

Accordingly, once an analyst inspects a detection, we
place on the IDL the corresponding registered domain,
which we compute by consulting Mozilla’s Effective
TLD Names list [20]. In this example, com appears on
the list (meaning that any domain directly under it will
reﬂect a registration), so we add google.com to the IDL.
Any subsequent matching against the IDL likewise em-
ploys trimming of names using the same procedure.

We note that we could implement the IDL with ﬁner
granularity than described above. In particular, we could
frame it in terms of per-client ﬁltering, or using custom
entropy thresholds. We leave exploring these reﬁnements
for future work.

4 Both of these are actual detections.

Exﬁltration Scenario

Query name-content
Query name-codebook
Timing
Query type

Estimated Data Volume

Total Name Timing
111% 110%
109% 103%
105% 0.8%
111% 0.6%

Type
0.4% 0.01%
5.6% 0.1%
104% 0.2%
6.8% 104%

Table 2: Estimates of data volumes produced by our procedure
measured against speciﬁc exﬁltration scenarios, showing the
total estimate, and the individual contributions from the query
name, timing, and type information estimation.

7 Evaluation
In this section we evaluate the efﬁcacy of our detection
procedure in terms of assuring that it can detect explicit
instances of communication tunneled over DNS (§ 7.1)
and investigating its performance on data from produc-
tion networks (§ 7.2). For this latter, we assess both the
procedure’s ability to ﬁnd actual surreptitious communi-
cation, and, just as importantly, what sort of burden it
imposes on security analysts due to the events generated.
7.1 Validating on Synthetic Data
To validate our procedure’s ability to accurately measure
communication embedded in DNS queries, we assessed
what sort of estimates it produces for scenarios under
which we fully control the DNS communication used for
exﬁltration. Table 2 summarizes the results, comparing
the information vector used for exﬁltration vs. the esti-
mates of the volume of data present in the corresponding
lookups, both in total and when restricted to just consid-
ering a single information vector. All values are percent-
ages of the actual exﬁltration size, so a value of 105 indi-
cates an estimate that was 105% of the true size (i.e., the
estimate was 5% too high). Naturally, estimators that fo-
cus on information vectors different from those used in a
given exﬁltration scenario can greatly underestimate the
data volume if used in isolation, highlighting the need to
combine such estimators into a ﬁnal comprehensive sum.
Regarding the scenarios reﬂected in the table, to assess
tunnels based on encoding information directly in query
names, we recorded Iodine [10] queries while sending a
99,438-byte compressed ﬁle with scp. The 11 % differ-
ence (shown in the “Query name-content” row) between
measured content and actual content is nearly all due
to tunnel encapsulation overhead (SSH, TCP/IP headers,
Iodine framing). As we are not aware of any available
tunneling tools that leverage repeated (codebook-style)
queries, timing, or varying query types, we wrote sim-
ple proof-of-principle implementations for testing pur-
poses. The codebook-style implementation used 16 dis-
tinct names that each convey four data bits per query,

26  22nd USENIX Security Symposium 

USENIX Association

10

Type Of Activity \ Dataset
Lookups (days)
Detection threshold
Conﬁrmed DNS channel
Benign use
Malware
Misconﬁguration
IPv4 PTR
IPv6 PTR
Unknown
Total
Domains ﬂagged (ﬁrst week)
Domains ﬂagged (typical week)

INDLAB
57B (1,212)
4kB
0
286
2
49
11
0
14
362
16
2.0

LBL
73B (2,565)
4kB
2
306
2
62
29
5
27
433
5
1.1

NERSC
12B (1,642)
4kB
0
29
0
5
4
0
0
38
3
0.15

UCB CHINA
69M (5)
10kB
0
41
2
8
3
0
13
67
(67+)
N/A

1.7B (45)
10kB
0
200
5
126
26
1
13
371
199
32

SIE
77B (53)
10kB
57
4,815
74
310
N/A
N/A
1
5,256
3,002
358

SIEUNIQ
12B (53)
10kB
57
1,088
73
182
N/A
N/A
1
1,401
798
97

Table 3: Number of domains ﬂagged in each dataset, broken out by the type of activity that the use of the domain represents. The
INDLAB, UCB and CHINA analyses cover all information vectors: LBL and NERSC incorporate query names and types, but
not timing; SIE considers only query names; and SIEUNIQ only the contents of query names (not repetitions). SIE and SIEUNIQ
analyses includes additional considerations discussed in the Appendix.

while the timing-interval implementation used one name
and 16 distinct time intervals spaced 10 ms apart. The
query-type implementation used one name and 16 dis-
tinct query types.
In addition, these tunnels used ﬁve
distinct query names for command and control. We ex-
ﬁltrated a 10,000-byte compressed ﬁle and found that the
difference between the estimated exﬁltration volume and
the actual size ranged from 5–11 %.

These results conﬁrm that our procedure can readily
detect information that is encoded into query names, tim-
ing, or query record types, and that it can provide mean-
ingful upper bounds.
7.2 Evaluation on Operational Data
We now turn to evaluating our detection procedure as ap-
plied to the extensive datasets we gathered, comprising
230 billion lookups from the networks listed in Table 1.
A key question for whether our detector is opera-
tionally viable concerns the combination of (1) how
many domains it ﬂags for analysis, coupled with (2) how
quickly an analyst can identify the common case of a
ﬂagged domain not in fact posing a threat.

The ﬁltering steps in § 6 aim to address the ﬁrst issue.
Regarding the second issue, as we brieﬂy discussed in
§ 6.6 we ﬁnd that often analysts can rely on fate-sharing
to quickly determine they needn’t further investigate a
candidate domain. For example, a site’s analyst can rea-
son that a detection of google.com or mcafee.com is safe
to ignore, because if indeed an attacker has control over
those domains’ name servers, the site has (much) bigger
problems than simply the presence of surreptitious com-
munication to the sites.

Table 3 summarizes the ﬁndings across all of the
datasets. For each dataset, the row in bold gives the to-

tal number of different domains ﬂagged by our detector
(many appear in more than one day), and the bottom row
reﬂects the “steady state” burden on an analyst investi-
gating detections for the given environment. We parti-
tion the datasets into two groups. The logs for INDLAB,
LBL and NERSC include individual per-client lookups,
and thus these sites represent the sort of environments
for which we target our detection, using a threshold of
4 kB/day. The lookups recorded for UCB, CHINA and
SIE, on the other hand, are primarily aggregated across
many clients, and thus for these datasets we cannot per-
form per-client analysis. We do not aim to treat these
datasets as operational environments for our detection
procedure, but rather to assess what sort of surreptitious
communication the procedure can detect in real trafﬁc.
For them, we use a higher threshold of 10 kB/day to limit
our own analysis burden in assessing the resulting detec-
tions. Finally, the SIE dataset introduces some additional
complexities, as discussed in Appendix B.

We classiﬁed the detections based on manual analysis
to assign each to one of six general categories, as follows.
Conﬁrmed DNS channel reﬂects domains for which
we could amass strong evidence that indeed the detection
represents surreptitious communication over DNS. For
LBL, both ﬂagged domains correspond to tunnels that
staff members acknowledge having set up to obtain free
Internet access in WiFi hotspots that allow out DNS traf-
ﬁc without requiring payment. One used DNStunnel [8],
the other NSTX [13].

For SIE, we identiﬁed 3 types of tunnels. One type
(responsible for 42 domains) corresponds to a product of-
fered by Dynamic Internet Technology, a company that
builds tools to evade censorship [9]. These tunnels en-
code most requests in two 31-character labels, using only

USENIX Association  

11

22nd USENIX Security Symposium  27

alphanumerics, followed by an identiﬁer that appears to
identify the tunnel itself. Another 10 domains all have
whois information leading to MMC Networks Limited
(of Gibraltar), a company that provides a program of-
fering “Free WiFi” using tunneling [28]. The tunneling
technology used for these is a variant of Iodine, with the
main difference being use of only alphanumeric charac-
ters for the encoding. We also found 5 domains that use
Iodine, for reasons we have not been able to identify.

Finally, we examined an addition 150 billion DNS
records captured in a separate 259 days of monitoring
from SIE. Due to monitoring gaps, this expanded data is
unsuitable for analyzing long-term analyst burden. But
in it (using a somewhat higher detection threshold) we
detected 42 new tunnel instances, including a new tunnel
type belonging to vpnoverdns.com.

Benign use encompasses a number of different sce-
narios that we believe would lead an analyst to fairly
quickly decide that the corresponding activity does not
appear problematic. These scenarios include ﬂagging
of: (1) a well-known site (e.g., google.com), for which
a name server breach would reﬂect a catastrophe, so
very likely has not occurred (fate-sharing).
(2) A sis-
ter site (e.g., a partner institute), where a similar ar-
gument holds.
for which sometimes lo-
cal systems look up many hostnames corresponding
to end-user systems. For example,
in LBL we ob-
serve queries for numerous names such as 201-11-50-
242.mganm703.dsl.brasiltelecom.net.br.
(4) Directory-
style services offered over DNS, including blocklists,
user-generated content, and catalogs.
(5) Software li-
cense servers. (6) Cloud-based antivirus services.

(3) ISPs,

Malware indicates lookups associated with malware
activity or sites ﬂagged (for example, by McAfee’s
SiteAdvisor service) as malicious. For SIE these also
include lookups such as p9b-8-na-5w-2z3-djmu-...-njx-
2es.info, i.e., 62-character labels consisting of letters or
numbers separated by dashes. We concluded that these
lookups reﬂect malware activity because names follow-
ing the same pattern appeared in a trace generated by a
researcher running bots within a contained environment.
Misconﬁguration generally reﬂect clients making
large volumes of lookups due to conﬁguration prob-
lems that lead to repeated failures. For example, in one
LBL instance we observed more than 60,000 lookups
of 33 different names within a single domain, such as
ldap. tcp.standardname-...isi.fhg.de. Other problems we
observed include lookups apparently based on email ad-
dresses, such as itunes@new-music.itunes.com; subdo-
mains appearing to be IP addresses; repeated failures
of names with narrow, rigid structures; and domains in
search paths that have lookups encapsulating a client’s
entire stream of queries sent to other domains.

IPv4 PTR and IPv6 PTR reﬂect lookups under the in-

addr.arpa and ip6.arpa zones, respectively. These zones
provide a decentralized mapping from numeric IP ad-
dresses to domain names. As discussed in § 6.2, we do
not ﬂag PTR lookup sufﬁxes that correspond to address
ranges that are local to the organization, or that are re-
served. As noted in § 6.3, for IPv4 PTR lookups we only
ﬂag sufﬁxes corresponding to /16 or /24 netblocks, and
for IPv6, /48 netblocks.

Unknown reﬂects domains for which we could not ar-
rive via manual analysis at a conﬁdent determination re-
garding how to classify the activity. For example, one
striking instance concerns a number of domains (primar-
ily seen in CHINA trafﬁc, but also SIE) that issue thou-
sands of lookups such as:
wojnlbefrhpfumrupmsn.0ule365.net

jnrlciinsszxahnfrvxe.0ule365.net

okgjeqckeqrxdigktkua.0ule365.net
Here, the domain (0ule365.net) is associated with a Chi-
nese gaming site. Other instances following the same
pattern appear to be associated with phishing sites related
to such gaming sites.

Domains ﬂagged in ﬁrst week and in typical week
reﬂect the two extreme behaviors of our Inspected Do-
main List approach (§ 6.6). In the ﬁrst week of operation
our detector reports a peak number of domains; once the
list is primed, it ﬂags domains at a much lower rate. (We
special-case the ﬁgure for CHINA because that entire
dataset spans less than a week.)

Finally, the main conclusion we highlight regarding
the Total row is the low number of events that analysts
would have to inspect. (Even for SIE, the average load
aggregated across the more than 100 participating sites
comes to about 50 detections per day, given I increased
from 4 kB to 10 kB.)
8 Real-Time Operation
As developed so far, our analysis procedure operates in
an ofﬂine fashion, processing full days as a single unit.
While this sufﬁces to enable analysts to detect DNS exﬁl-
tration on a daily basis, real-time detection would enable
immediate identiﬁcation of such activity and thus much
quicker response. In this section we explore the viability
of adapting our scheme for such detection.

Our real-time variant uses gzip and bzip2 as the com-
pression functions. We can adapt both the cached query
ﬁlter and the “uninteresting query” ﬁlter to streaming op-
eration, with the only consideration being that we modify
the cached query ﬁlter to actively ﬂush all cache entries
as their TTLs expire to minimize statekeeping.

Adapting the fast ﬁlter and the compression-based ﬁl-
ters takes more consideration, since they naturally pro-
cess entire sets of activity as a unit. In addition, if we
try to use a compressor in a stream fashion, we must deal
with the compressor’s destructive operation: if we add

28  22nd USENIX Security Symposium 

12

USENIX Association

data to a stream and call flush() to obtain the size of
the compressed result, the flush() operation changes
the compressor’s internal state—adding more data and
calling flush() again can produce a larger output than
simply compressing all of the data at once.

Our approach combines the fast ﬁlter and the com-
pression measurement for each (domain, client) pair as
follows.
Initially, for each pair we only track the un-
compressed input. Upon receiving new input, we check
whether the total message length plus maximum possi-
ble entropy contribution from the timing, and query, and
query type could possibly lead to the pair generating an
alert. If not, we simply append the new information to
the list of previously seen queries.

If the total could cross our threshold, we allocate com-
pressors, feed them all of the recorded input, and in-
voke flush(). If the resulting entropy lies below the
alert threshold, we simply update the uncompressed data
threshold that could possibly generate an alert, discard
the compressed data, and continue. Otherwise, we gen-
erate an alert, create new compressors, feed them all the
previous data, and pass all subsequent data to them as it
arrives. These new compressors allow us to compute a
full 24-hour entropy total for the (domain, client) pair to
aid the analyst. After 24 hours we generate a summary
for each pair and discard the associated state.

For good performance we parallelized this approach,
running the cached-query and uninteresting-query ﬁlters
in a single process that dispatches each (sufﬁx, client)
pair to one of 15 distinct child processes. We veriﬁed
that the implementation produces a consistent analysis
by processing the same day of INDLAB data using both
the original batch implementation (with only gzip and
bzip2) and the real-time variant (70M DNS queries, 36M
non-empty replies). They fully agreed, with the real-
time implementation requiring 28 minutes and 4.5GB
of RAM to process the day of trafﬁc. The execution
totaled 53 CPU-core-minutes on a dual processor Intel
Xeon X5570 system. Given these results, we conclude
that real-time operation is quite viable.
9 Discussion
This paper demonstrates how we can comprehensively
measure the information content of an outbound DNS
query stream. Our lossless compression-based procedure
measures all information that an attacker can effectively
send via names, types, and timing, regardless of the ac-
tual encoding used. This procedure also has only two
tuning parameters, the threshold of detection and the tim-
ing precision.

Some minor DNS features remain that we have not
included in our analysis procedure. We have omitted
these for simplicity, since in their usual (benign) use,
they appear almost always to have a single value for a

given client. These information vectors include request-
ing DNSSEC information (single bit) and the query’s
class (which for modern trafﬁc is almost always type IN,
“Internet”). Similarly, future EDNS0 extensions could
appear that recursive resolvers will forward intact, pro-
viding a new information vector. For all such features,
we can simply employ an additional compressor opti-
mized with the use of a very low-cost special case of
using a single bit to indicate that for a given client, the
feature never changes.

Attackers can tunnel information in DNS replies as
well as in queries, and indeed existing tunnels do so.
Since replies can include domain names (returned for
example in CNAME records) or unstructured byte strings
(e.g., TXT records), replies can potentially convey large
volumes of data.
(We remind the reader that in this
work we have focused on analyzing DNS queries rather
than responses since for the scenarios of particular
interest—exﬁltration or remote interactive access—the
query streams will generally carry the bulk of the data.)
Attackers who can successfully mimic the appear-
ance of benign data-rich query streams (such as block-
list lookup services) can trick analysts into deeming their
surreptitious communication as harmless. Similarly, an
attacker who compromises a previously benign domain
can encode their trafﬁc using the same style of lookups
as the domain originally used. These problems are or-
thogonal to the question of ﬂagging the activity.

Attackers aware of our detection procedure can in ad-
dition design their tunnels to keep the information con-
tent below the 4 kB per day threshold. Given that we ag-
gregate information content metrics per domain, a sim-
ple evasion strategy would be to spread the trafﬁc across
K > 1 domains, and then send < 4 kB per day to each,
but in aggregate communicate K times that volume. A
possible detection approach we envision pursuing con-
sists of analyzing each client’s lookups in their entirety,
rather than on a per-destination-domain basis. Coupled
with an expanded Inspected Domain List (§ 6.6) to re-
move the major contributors to DNS trafﬁc, we would
aim with this approach to compute a bound on the total
information content each client communicates via all of
its external DNS queries.

Finally, attackers could spread their exﬁltration across
multiple compromised clients, so that each client’s query
stream remains below the detection threshold. Our expe-
riences with external vantage points such as UCB indi-
cates that we still might be able to ﬁnd the activity of
groups of clients, since that vantage point already ag-
gregates multiple clients into a single apparent source.
However, a combination of using multiple compromised
clients and K external name servers might prove exceed-
ingly difﬁcult to detect for the sort of thresholds we have
employed in this work.

USENIX Association  

13

22nd USENIX Security Symposium  29

10 Related Work
Four areas of prior work have particular relevance to our
study: covert communication; designing ways of tunnel-
ing communication over DNS trafﬁc; detecting such tun-
neling; and establishing bounds on the volume of covert
communication.

We adopt Moskowitz and Kang’s classiﬁcation of
covert communication channels [19].
In particular, a
storage channel is a covert channel where the output al-
phabet consists of different responses all taking the same
time to be transmitted, and a timing channel is a covert
channel where the output alphabet is made up of differ-
ent time values corresponding to the same response. Ac-
cordingly, we treat covert communication via DNS query
content (name, type and other attributes) as a storage
channel, and covert communication via query timing as
a timing channel.

Conventional DNS tunnels are similar in construction:
they are bi-directional, directly embedding the outbound
information ﬂow in query names, and the inbound ﬂow
in server responses.
In the absence of outbound data,
the client sends low-frequency queries to poll the tun-
nel server for any pending data. The functionality of
these tunnels ranges from a simple client-to-server vir-
tual circuit to full IP-level connectivity. Examples are
NSTX [13], dns2tcp [7], Iodine [10], OzymanDNS [15],
tcp-over-dns [25], and Heyoka [14]. DNS exﬁltration has
also been a tool in the attacker’s toolbox for a number of
years (per [22] and the references therein).

Beyond query names, the DNS message format con-
tains a variety of ﬁelds that could be used for embed-
In addition to the
ding data (as we detail in § 5.1).
DNS-speciﬁc message ﬁelds, timing (e.g., the timing of
queries) provides a rich vector for embedding data. This
is not unique to DNS trafﬁc, but present in all Internet
trafﬁc, allowing any message to be encoded in the inter-
arrival times between packets. Gianvecchio et al. [12]
showed how to automatically construct timing channels
that mimic the statistical properties of legitimate net-
work trafﬁc to evade detection. Our detection technique
avoids such complication by measuring information con-
tent rather than particular statistical properties.

One approach for detecting covert communication
over DNS examines the statistical properties of DNS traf-
ﬁc streams. Karasaridis et al. propose DNS tunnel detec-
tion by computing hourly the Kullback-Leibler distance
between baseline and observed DNS packet-size distri-
butions [16]. To defeat such temporal statistical anomaly
detectors, Butler et al. propose stealthy half-duplex and
full-duplex DNS tunneling schemes [5]. They also pro-
pose the use of Jensen-Shannon divergence of per-host
byte distributions of DNS payloads to detect tunneled
trafﬁc. Their detection technique only ﬂags whether
the aggregate trafﬁc contains tunneled communication; it

does not identify the potential tunneled domains. In addi-
tion, the detection rate depends to a large extent on the ra-
tio of tunneled trafﬁc to normal trafﬁc. In [3], the authors
show that domain names in legitimate DNS queries have
1-, 2-, and 3-gram ﬁngerprints following Zipf distribu-
tions, which distinguishes them from the higher-entropy
names used in DNS tunneling. The evaluations in these
works do not particularly address practicality for opera-
tional use, however, since the authors validate their hy-
potheses on short, low-volume benign and synthetic tun-
neled traces collected using free DNS tunneling tools. As
we discuss in § 5.2, large-scale DNS trafﬁc often exhibits
extensive diversity in multiple dimensions, which likely
will exacerbate issues of false positives.

Our work overlaps with work on algorithmically-
generated domain names by Yadav et al. [29]. The most
salient difference is that their algorithm assumes a spe-
ciﬁc model of name construction (distributions of letters
and bigrams). Instead of focusing on speciﬁc name pat-
terns and missing communication that uses different en-
codings, we measure the aggregate information content
of a query stream regardless of how encodings are gen-
erated for the query name, type or timing.

Detection of timing channels has been studied before,
and we mention here only a few recent results. Cabuk
et al. [6] observe that timing-based tunnels often in-
troduce artiﬁcial regularity in packet inter-arrival times
and present detection methods based on this characteris-
tic. More generally, Gianvecchio and Wang [11] identify
timing-based tunnels in general Internet trafﬁc (not just
DNS) by using conditional entropy measures to identify
the subtle distortions introduced by the tunnel in packet
inter-arrival time distributions. These works use time in-
tervals of 20 msec or more; we use a more conservative
10 msec timing resolution, and do not assume the pres-
ence of detectable distortions.

While the general problem of surreptitious communi-
cation has received extensive examination in the litera-
ture of covert channels and steganography, more closely
related to our work is previous research on bounding the
volume of surreptitious communication in other proto-
cols. Borders et al. studied this problem for HTTP, ob-
serving that covert communication is constrained to the
user-generated part of an outgoing request [1, 2]. By
removing ﬁxed protocol data and data derived from in-
bound communication, the authors show how to deter-
mine a close approximation to the true volume of infor-
mation ﬂows in HTTP requests. An analogous approach
for our problem domain would be to track the domain
names a system receives from remote sources (such as
web pages and incoming email), and to exclude lookups
for these names as potentially conveying information.
Such tracking, however, appears infeasible without re-
quiring extensive per-system monitoring.

30  22nd USENIX Security Symposium 

14

USENIX Association

11 Summary
We have presented a comprehensive procedure to de-
tect stealthy communication that an adversary transmits
via DNS queries. We root our detection in establishing
principled bounds on the information content of entire
query streams. Our approach combines careful encod-
ing and ﬁltering stages with the use of lossless compres-
sion, which provides guarantees that we never underes-
timate information content regardless of the speciﬁc en-
coding(s) an attacker employs.

We demonstrated that our procedure detects conven-
tional tunnels that encode information in query names,
as well as previously unexplored tunnels that repeatedly
query names from a ﬁxed alphabet, vary query types, or
embed information in query timing. We applied our de-
tection procedure to 230 billion lookups from a range
of production networks and addressed numerous chal-
lenges posed by anomalous-yet-benign DNS query traf-
ﬁc.
In our assessment we found that for datasets with
lookups by individual clients and a threshold of detect-
ing 4 kB/day of exﬁltrated data per client and domain, the
procedure typically ﬂags about 1–2 events per week for
enterprise sites. For a bound of 10 kB, it typically ﬂags
50 per day for extremely aggregated logs at the scale of
a national ISP. In addition, buried within this vast num-
ber of lookups our procedure found 59 conﬁrmed tunnels
used for surreptitious communication.
Acknowledgments
Our thanks to Partha Bannerjee, Scott Campbell, Haixin
Duan, Robin Sommer, and James Welcher for facilitating
some of the data and processing required for this work.
Our thanks too to Christian Rossow and the anonymous
reviewers for their valuable comments.

This work would not have been possible without the
support of IBM’s Open Collaboration Research awards
program. In addition, elements of this work were sup-
ported by the U.S. Army Research Ofﬁce under MURI
grant W911NF-09-1-0553, and by the National Sci-
ence Foundation under grants 1161799, 1223717, and
1237265. Any opinions, ﬁndings, and conclusions or
recommendations expressed in this material are those of
the authors and do not necessarily reﬂect the views of the
sponsors.
References
[1] BORDERS, K., AND PRAKASH, A. Towards Quantiﬁca-
tion of Network-Based Information Leaks via HTTP. In
Proceedings of the 3rd USENIX Workshop on Hot Topics
in Security (2008), USENIX Association.

[2] BORDERS, K., AND PRAKASH, A. Quantifying Infor-
mation Leaks in Outbound Web Trafﬁc. In Proceedings
of the IEEE Symposium on Security and Privacy (2009),
USENIX Association.

[3] BORN, K., AND GUSTAFSON, D. Detecting DNS Tun-
nels Using Character Frequency Analysis. In Proceedings
of the 9th Annual Security Conference (2010).

[4] BROMBERGER, S.

DNS as a Covert Channel
http://energy.gov/

Within Protected Networks.
sites/prod/ﬁles/oeprod/DocumentsandMedia/
DNS Exﬁltration 2011-01-01 v1.1.pdf, 2011.

[5] BUTLER, P., XU, K., AND YAO, D. Quantitatively an-
alyzing stealthy communication channels.
In Proceed-
ings of International Conference on Applied Cryptogra-
phy and Network Security (2011).

[6] CABUK, S., BRODLEY, C. E., AND SHIELDS, C.

Ip
covert timing channels: design and detection.
In Pro-
ceedings of the 11th ACM conference on Computer and
communications security (New York, NY, USA, 2004),
CCS ’04, ACM, pp. 178–187.

[7] DEMBOUR, O.

DNS2tcp.

http://www.hsc.fr/

ressources/outils/dns2tcp/index.html.en.

[8] DNStunnel. http://www.dnstunnel.de/.
[9] Dynamic Internet Technology. http://www.dit-inc.us/.
[10] EKMAN, E., AND ANDERSSON, B. Iodine, tunnel IPv4

over DNS. http://code.kryo.se/iodine/, 2011.

[11] GIANVECCHIO, S., AND WANG, H. An entropy-based
approach to detecting covert timing channels. Depend-
able and Secure Computing, IEEE Transactions on 8, 6
(Nov/Dec. 2011), 785–797.

[12] GIANVECCHIO, S., WANG, H., WIJESEKERA, D., AND
JAJODIA, S. Model-based covert timing channels: Au-
tomated modeling and evasion.
In Proceedings of the
11th international symposium on Recent Advances in In-
trusion Detection (Berlin, Heidelberg, 2008), RAID ’08,
Springer-Verlag, pp. 211–230.

[13] GIL, T. NSTX (IP-over-DNS). http://thomer.com/

howtos/nstx.html.

[14] Heyoka. http://heyoka.sourceforge.net/.
[15] KAMINSKY, D. OzyManDNS.
[16] KARASARIDIS, A., MEIER-HELLSTERN, K., AND
HOEFLIN, D. Detection of DNS anomalies using ﬂow
data analysis. In Global Telecommunications Conference
(GLOBECOM) (2006).

[17] KREIBICH, C., WEAVER, N., NECHAEV, B., AND PAX-
SON, V. Netalyzr: Illuminating the edge network. In Pro-
ceedings of the ACM Internet Measurement Conference
(IMC) (Melbourne, Australia, November 2010), pp. 246–
259.

[18] MOCKAPETRIS, P. Domain names—implementation
and speciﬁcation. RFC 1035, Internet Engineering Task
Force, Nov. 1987.

[19] MOSKOWITZ, I. S., AND KANG, M. H. Covert chan-
nels - here to stay? In Proceedings of the Ninth Annual
Conference on Computer Assurance (1994), pp. 235–244.
[20] MOZILLA. Public Sufﬁx List. Published online at http:

//publicsufﬁx.org/. Last accessed on May 4, 2012.

USENIX Association  

15

22nd USENIX Security Symposium  31

[21] PAXSON, V. Empirically-Derived Analytic Models of
IEEE/ACM Transactions

Wide-Area TCP Connections.
on Networking 2, 4 (Aug. 1994), 316–336.

[22] RICKS, B. DNS Data Exﬁltrationa Using SQL In-
http://www.defcon.org/images/defcon-16/

jection.
dc16-presentations/defcon-16-ricks.pdf, 2008.

[23] SHKARIN, D. PPMd. http://www.compression.ru/ds/

ppmdj1.rar, 2006.

[24] Security Information Exchange. http://sie.isc.org/.
[25] tcp-over-dns.

http://analogbit.com/software/

tcp-over-dns.

[26] VIXIE, P. Extension Mechanisms for DNS (EDNS0).

RFC 2671 (Proposed Standard), Aug. 1999.

[27] VIXIE, P., AND DAGON, D. Use of Bit 0x20 in DNS
Labels to Improve Transaction Identity. Work in progress,
Internet Engineering Task Force, 2008.

[28] Wi-Free. http://wi-free.com/.
[29] YADAV, S., REDDY, A. K. K., REDDY, A. N., AND
RANJAN, S. Detecting algorithmically generated mali-
cious domain names. In Proceedings of the 10th annual
conference on Internet measurement (2010), IMC ’10,
ACM, pp. 48–61.

A Full Names for Examples
For completeness, Figure 7 lists the full names of various DNS
lookups that in the main body of the text we elided portions
for readability. Note that for some names we introduced minor
changes for privacy considerations.
B Issues Evaluating the SIE Dataset
The SIE data’s extreme volume and qualitatively different na-
ture necessitated several changes to our analysis procedure.
Our access to the data was via a Hadoop cluster, requiring cod-
ing of our algorithms in the Pig and Scala languages. These
provide efﬁcient support for only a subset of the functionality
we employed when analyzing the other datasets. A signiﬁcant
difference in this regard was that we were conﬁned to only us-
ing gzip for compression; bzip2 and ppmd were not available.
Another important difference concerns the deﬁnition of
“client”. A single large American ISP dominates the SIE data,
representing roughly 90% of the trafﬁc. This ISP uses clusters
of resolvers to process requests. Thus, a single abstract resolver
manifests as multiple “client IP addresses”, which we deter-
mined come from the same /28 address preﬁx. Therefore we
treat query source IP addresses equivalent in their top 28 bits as
constituting a single source.

This extreme aggregation leads to signiﬁcant increases in de-
tections, as we are now measuring the information volume for
queries aggregated across potentially hundreds of thousands
of clients. One particular increase in benign alerts arises due
to popular names with short TTLs (e.g., www.google.com).
With so many clients, every popular name becomes immedi-
ately refetched whenever its TTL expires, leading to a steady
stream of closely-spaced lookups. This very high level of ag-
gregation also generates such a large volume of detections for

5.1o19sr00ors95qo0p73415p3r8r8q777634r5o86osn295ss2rqos
s3r9601ro3.1r1p7r4719o34393648s2345nn60qnqoop45psos37n
551s002n80850sr2r8n3.r1105qqq28r7pn82843rp76383qr6344q
qpq7rpnrp63o957687r980r.rrqs656p04pn614q6n76o97883op73
r0p787rn92.i.02.s.sophosxl.net

g63uar2ejiq5tlrkg3zezf2fksjrxpxyvro4ce5yz65udnjn.dagbuu
5pkocwcaxkntmxzwvkbulhg3qlj6ho7jwobeddjqvv.gepxfdwfhu7
6on6gza2nkringxp35e6g3ftpqlpl5h6uofgo.kukjy4jvybu7jhrl
hrgxe7es3lmkxdrpmpb4lg7wmbpygjg7.gef2uoemc6pi88tz.er.s
potify.com

awyvrvcataaaegdid5tmr7eteje2kst35frnnr3kupbfc6hr.gq3dey
4qnjvqtoltoj2dq5bxnmaauaaeaiaaeg7xa4ut3ilu.license.cra
shplan.com

www.10.1.2.3.static.because.dul.is.rfc.ignorant.edu.za.
static.because.dul.is.rfc.ignorant.edu.za.research.edu
JohnsonHouse\032Officejet\032J6400\032\032The\032Johnso
n\032MacBook. ipp. tcp.johnsonhouse1.members.mac.com
(a) Example DNS names with more than 100 bytes in length (cf. § 5.2).
1751913.86c0ade0d13143ab83d7e4f60cbd204c.00000000.xello

.xobni.com

1753942.86c0ade0d13143ab83d7e4f60cbd204c.00000000.xello

.xobni.com

1756950.86c0ade0d13143ab83d7e4f60cbd204c.00000000.xello

.xobni.com

1758762.86c0ade0d13143ab83d7e4f60cbd204c.00000000.xello

.xobni.com
(b) Example DNS names with little variation between consecutive
queries.

p9b-8-na-5w-2z3-djmu-7pk-qy-0-bok-re9-ym-v9h-av-njx-2es

.info

(c) Example DNS name reﬂecting malware activity (cf. § 7.2).
ldap. tcp.standardname-des-ersten-standorts. sites.dc.
msdcs.isi26.isi.fhg.de

(d) Example DNS name originating from client misconﬁguration
(cf. § 7.2).
Figure 7: Full names of examples used in the main text. We
line-break each name at 54/55 characters.

reverse lookups that we excluded them from the SIE analysis,
which removes about 10% of the queries.

As previously discussed in § 4, we emphasize that the role of
the SIE dataset for our evaluation is simply to give us a (huge)
target environment in which to validate that we can ﬁnd actual
tunnels. We do not envision our procedure as operationally vi-
able for this environment; nor does such an environment strike
us as making sense in terms of conforming with our threat
model, which focuses on tightly controlled enterprises, rather
than wide-open ISPs.

Given this perspective, to keep our own manual analysis
tractable, for SIE we used a detection threshold I of 10 kB
rather than the 4 kB value we use for the other datasets.

We also explored the effects of other analysis changes. First,
we investigated conducting our analysis on the SIE queries re-
duced to distinct, sorted names. This transformation removes
our opportunity of assessing query name-codebook informa-
tion vectors, but preserves our ability to estimate data con-
veyed through the query name-content vector—the only type
of encoding employed by known DNS tunneling tools. Table 2
shows this version of the SIE data as SIEUNIQ. The reduction
in analyst load is quite signiﬁcant, more than a factor of three.

32  22nd USENIX Security Symposium 

16

USENIX Association

