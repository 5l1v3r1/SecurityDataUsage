EpicRec: Towards Practical Differentially Private
Framework for Personalized Recommendation

Yilin Shen

Samsung Research America

665 Clyde Ave, Mountain View, CA 94043

yilin.shen@samsung.com

Hongxia Jin

Samsung Research America

665 Clyde Ave, Mountain View, CA 94043

hongxia.jin@samsung.com

ABSTRACT
Recommender systems typically require users’ history data
to provide a list of recommendations and such recommen-
dations usually reside on the cloud/server. However, the
release of such private data to the cloud has been shown to
put users at risk. It is highly desirable to provide users high-
quality personalized services while respecting their privacy.
In this paper, we develop the ﬁrst Enhanced Privacy-built-
In Client for Personalized Recommendation (EpicRec) sys-
tem that performs the data perturbation on the client side
to protect users’ privacy. Our system needs no assumption
of trusted server and no change on the recommendation al-
gorithms on the server side; and needs minimum user inter-
action in their preferred manner, which makes our solution
ﬁt very well into real world practical use.

The design of EpicRec system incorporates three main
modules: (1) usable privacy control interface that enables
two user preferred privacy controls, overall and category-
based controls, in the way they understand; (2) user privacy
level quantiﬁcation that automatically quantiﬁes user pri-
vacy concern level from these user understandable inputs;
(3) lightweight data perturbation algorithm that perturbs
user private data with provable guarantees on both diﬀeren-
tial privacy and data utility.

Using large-scale real world datasets, we show that, for
both overall and category-based privacy controls, EpicRec
performs best with respect to both perturbation quality and
personalized recommendation, with negligible computational
overhead. Therefore, EpicRec enables two contradictory
goals, privacy preservation and recommendation accuracy.
We also implement a proof-of-concept EpicRec system to
demonstrate a privacy-preserving personal computer for movie
recommendation with web-based privacy controls. We be-
lieve EpicRec is an important step towards designing a prac-
tical system that enables companies to monetize on user data
using high quality personalized services with strong provable
privacy protection to gain user acceptance and adoption of
their services.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
CCS’16, October 24-28, 2016, Vienna, Austria
c(cid:13) 2016 ACM. ISBN 978-1-4503-4139-4/16/10. . . $15.00
DOI: http://dx.doi.org/10.1145/2976749.2978316

Keywords
Privacy-Preserving Recommendation; Diﬀerential Privacy;
Privacy Paradox

1.

INTRODUCTION

The last few decades have witnessed wide applications
of recommender systems to provide personalized service to
users, such as intelligent personal assistant and smart TV
or other content recommendations. In fact, such personal-
ized services become key business drivers for many compa-
nies. As one can understand, personalized service is based
on user’s data and oftentimes requires substantial user data
in order to provide high-quality recommendation services.

However, many user concerns about recommender systems
have been raised from privacy perspectives due to the release
of users’ private data. Consumer fears over privacy continue
to escalate. Based on Pew Research, 68% consumers think
that current laws are insuﬃcient to protect their privacy
and demand tighter privacy laws; and 86% of Internet users
have taken proactive steps to remove or mask their digital
footprints. Responding to increasing user privacy concerns,
governments in US/EU are increasing and enforcing existing
regulations. LG TV, as another example, was caught in
lawsuit on illegally obtaining users’ private data.

To resolve the tensions between business intelligence and
user privacy, it is critically desirable to develop technologies
that can preserve and control user data privacy and in the
meanwhile still allow intelligence and personalization busi-
ness. Without such technology enabler, future users will
stop using services and companies will not be able to deploy
services due to privacy law constraints and user concerns.

A majority of existing methods [5, 6, 9, 10, 11, 19] are
developed based on the scenarios that recommender servers
are trusted such as the Netﬂix movie recommendation sys-
tem. One main reason for such assumption is that classic
recommender system algorithms, such as Collaborative Fil-
tering [25], require multiple users’ data in order to perform
personalized recommendation. It is easy to understand that
a trusted server collects all users’ data and can therefore per-
form such personalized recommendation. The most relevant
privacy preserving approach is proposed by McSherry et al.
[19], in which the server does the anonymization on user pri-
vate data in which random noises are added into each step
of aggregates in recommendation algorithm. All these meth-
ods attempted to protect user privacy when server releasing
user data to third party applications and business partners.
Unfortunately, in such device-cloud based recommender
systems, there are many other privacy attacks (as shown in

180Figure 1: Attacking Model

Figure 2: EpicRec System under Untrusted Server

Figure 1) that cannot be modeled and addressed in such a
trusted server setting and require other types of protections.
For example, when user data travels from device to cloud, at-
tackers can eavesdrop the transmission channel and launch
a “man-in-the-middle attack”, therefore data may need to
be encrypted during transmission. Malicious attackers can
break into the cloud/server and steal user data. This de-
mands many security measures to take to protect data on
the cloud, such as encrypting data in storage. Moreover,
server insiders may also leak user data to other parties.

As such, in this paper, we design a novel and practical
privacy-built-in client under untrusted server settings,
in
which user data is perturbed and anonymized on their pri-
vate devices before leaving their devices and users are given
more peace of mind. As one can understand, data pertur-
bation on device side under untrusted server settings poses
extra challenges than that under trusted server settings be-
cause data perturbation has to be done without knowing
other users’ data.

There are some existing approaches developed under such
untrusted server settings, including cryptography techniques
[3, 21], diﬀerential privacy-based techniques [24], and ran-
domization techniques [23]. Unfortunately, these approaches
cannot be applied in practice due to various reasons such as
computation cost, the need of an impractical trusted third
party, lack of usability and so on.

In this paper, we propose the ﬁrst practical Enhanced

Privacy-built-In Client for Personalized Recommendation (Epi-
cRec) system. As one can see in Figure 2, EpicRec, residing
on the user’s hub device (e.g., personal laptop, smartphone,
etc.), collects user private data from a variety of user’s de-
vices and perturbs the private data based on user’s privacy
concerns. More importantly, the existence of EpicRec on
user’s device not only satisﬁes users’ privacy needs but also
requires no assumption of trusted server and no changes of
recommendation algorithms, rendering EpicRec very prac-
tical. Our contributions are summarized as follows:

• We design the ﬁrst privacy-preserving EpicRec frame-
work on user client for personalized recommendation.
EpicRec collects user private data from various de-
vices, provides usable privacy control interfaces, quan-
tiﬁes user privacy control input and uses it to perturb
user data. EpicRec enables user preferred overall pri-
vacy control (S-EpicRec) and category-based privacy
control (M-EpicRec) to satisfy users’ diﬀerent needs;
and in the meanwhile maintains low user cognitive load
by minimizing the needs of user interactions.
• We design S-EpicRec and M-EpicRec systems respec-
tively, both based on the state-of-the-art diﬀerential
privacy and utility notions. We quantify the user pri-

vacy level by optimizing the utility based on the under-
lying data properties; and develop a light-weight and
data perturbation algorithm to preserve the category
aggregates with both privacy and utility theoretical
guarantees, which signiﬁcantly improves the existing
approach [24] from both privacy and utility aspects.
• We conduct extensive experiments to evaluate the per-
formance of EpicRec on large-scale real-world datasets.
The results show that, from both privacy and utility
perspectives, our proposed S-EpicRec and M-EpicRec
systems consistently outperform other (pseudo) com-
petitors that apply existing methods into some com-
ponents of our EpicRec system. In addition, our ap-
proach takes less than 1.5 seconds on personal com-
puters.
• We implement a proof-of-concept EpicRec system for
personalized movie recommendation with web-based
overall and category-based privacy concern controls.

The rest of paper is organized as follows. Section 2 dis-
cusses the related work. The background and architecture
design of EpicRec system are presented in Section 3. Sec-
tion 4 and 5 propose detailed design of S-EpicRec and M-
EpicRec systems supporting diﬀerent granularities of pri-
vacy controls. The experimental results and implementa-
tion of proof-of-concept system are presented in Section 6
and Section 7 respectively. Section 8 concludes the whole
paper and discusses some future work.

2. RELATED WORK

Privacy-preserving Recommendation. Table 1 shows
the comparison between our EpicRec system and existing
approaches for personalized recommendation under untrusted
server settings. The earliest work by Polat et al. [23] devel-
oped randomized mechanisms to perturb user private data
before releasing to recommender systems. However, their
method does not have provable privacy guarantees and was
later identiﬁed that using clustering method on their per-
turbed data can still accurately infer users’ original raw data
with accuracy up to more than 70% [30] and make the pri-
vacy protection useless.
In the meanwhile, cryptography-
based approaches [21, 3] are proposed with privacy guar-
antees. However, these approaches require a trusted third-
party (Cryptographic Service Provider (CSP)) and expen-
sive private computation. Another class of orthogonal ap-
proaches [24] are based on the state-of-the-art diﬀerential
privacy notion, with both privacy and utility guarantees.
Unfortunately, in addition to the above limitations, all ex-
isting approaches largely ignore the usable privacy control

Cloud Intrusion& Data LeakageUser Data EavesdropCloudAttackerUntrusted Recommender SystemRecommendationPerturbed User DataInsiderEpicRecServer: Existing analytics and recommendation algorithms with NO ChangesProvide Recommendation to UserPerturbed DataClientPrivacy Concerns181Table 1: Comparison between Privacy-Preserving Recommendation under Untrusted Server Settings

Approaches

Polat et al. [23]

Nikolaenko et al. [21]

Canny et al. [3]
Xin et al. [27]
Shen et al. [24]

EpicRec

No Change
of Service
Provider

(cid:88)



(cid:88)
(cid:88)

No Need
of Trusted
Third Party

(cid:88)

(cid:88)
(cid:88)
(cid:88)
(cid:88)

User

Privacy
Control
Single





Single

Single &

Category-based

User-friendly

Privacy Control

Privacy

Utility

Privacy

Interface

Quantiﬁcation Guarantee

Guarantee






(cid:88)






(cid:88)


(cid:88)
(cid:88)

(cid:88)
(cid:88)



Cryptography
Cryptography

2nd Order Privacy
Diﬀerential Privacy

Diﬀerential Privacy

such that users cannot provide their privacy concerns in a
way they understand.

In addition, there are some other privacy-preserving rec-
ommendation approaches under trusted server settings [19]
or some particular recommendation services [17]. Moreover,
system developers, and policy makers recently have been
coming up with solutions at diﬀerent levels [13, 29].

Our proposed EpicRec system provides a comprehensive
solution at all levels, towards a practical and usable privacy-
preserving client for untrusted recommender system, with
strong privacy and utility guarantees.

Diﬀerential Privacy. Diﬀerential privacy [7, 8] has be-
come the de facto standard for privacy preserving data an-
alytics. Dwork et al. [8] established the guideline to guar-
antee diﬀerential privacy for individual aggregate queries by
calibrating the Laplace noise to each query based on the
global sensitivity. Various works have adopted this deﬁnition
for publishing histograms [28], search logs [15], mining data
streams [4], and record linkage [1]. Later on, a probabilis-
tic relaxation was proposed by Machanavajjhala et al. [18],
called probabilistic diﬀerential privacy. This novel diﬀeren-
tial privacy notion allows the privacy preservation with high
probability, thereby improve the ﬂexibility of global sensi-
tivity analysis. An alternative approach for noise mitigation
was instance-based noise injection approaches by Nissim et
al. [22]. This paper ﬁrst introduced local sensitivity and its
upper bound smooth sensitivity, which allows the injection
of Admissible noise to ensure diﬀerential privacy. Unfortu-
nately, all these approaches require the strict satisfaction of
perturbed aggregates in the sanitized data and restrict their
applications to only statistical data publishing. A recent
work [24] was proposed to address the above constraints, by
perturbing data to guarantee both diﬀerential privacy and
recommendation quality. Our data perturbation module in
EpicRec framework provides better privacy and utility than
[24] from both theoretical and empirical perspectives.

3. EpicRec SYSTEM DESIGN

In this section, we present the framework design of our
proposed Enhanced Privacy-built-In Client for Recommen-
dation (EpicRec) system. The goal of EpicRec system is
three-fold: (1) enable user-friendly privacy concern control
on their private data in a way they understand; (2) quantify
user’s privacy level input from layman and user-understandable
language to quantiﬁed private budget for data perturbation;
(3) conduct light-weight perturbation of user private data on
their device such that the perturbed data can be released to
existing recommender systems to provide user high-quality
personalized recommendations. Next, we ﬁrst brieﬂy dis-
cuss about background to motivate and guide system design.

We then describe the overall architecture of EpicRec system
and the details of each component.

3.1 Background & Motivation

We ﬁrst give a succinct overview of the key results from
our two user studies (details are in separate papers to be
submitted with preliminary results in [29, 26]), in order to
motivate and guide the design of EpicRec system.

The ﬁrst user study focuses on the research question: Which

information is considered as private by users? Using a popu-
lar video recommender system as an example, we conducted
an online user study by recruiting 161 participants through
Amazon Mechanical Turk (MTurk) and studied 11 types of
data collected by smart TV. The majority of the participants
were male (66.5%) and White (75.8%). There are 77.0% of
participants aged between 20 and 40. We created 11 types
of data collected by smart TV including content, channel,
time(watch, change, service), status(on/oﬀ, on duration, if
using DVR), TV settings, clicked buttons and interacted ser-
vices. The results show that most participants raised their
most privacy concerns about their watching content history
for either personalized program recommendation or targeted
advertising purposes.

The second user study focuses on the research question:
How does level of control in a smart TV inﬂuence user’s
perceptions and behaviors? We recruited 505 participants
through MTurk and studied 15 diﬀerent privacy control mech-
anisms with diﬀerent levels and types of control. The major-
ity of the participants were male (57.0%) and White (75.8%).
The average age was 34.15 (range 19-72). We created 15
privacy control conditions by combining the following as-
pects: non-hierarchical and hierarchical controls on diﬀer-
ent content types (overall, category, maturity rating, watch-
ing time). The results show that the overall privacy control
and ﬁner-grained category-based privacy control are the best
two control interfaces the participants selected among the 15
diﬀerent designs. The participants rated them as the most
useful in helping to make data disclosure decisions; the least
privacy concerns; the most valuable in disclosing their in-
formation for personalized recommendations; and the most
likely to use in the future.

3.2 EpicRec Architecture

Figure 3 shows the overall architecture of EpicRec system.
Our focus is on user’s device side where EpicRec system sits
while the service provider remains unchanged. The goal of
device-side EpicRec system is to perform data perturbation
on user private data with user-speciﬁed privacy concern lev-
els, such that the format of perturbed data remains the same
and recommendation results remain accurate.

182releasing protected data, and “All Release” as releasing all
private data (Note that ”All Release” will release as much
information as possible if conﬂict happens in category-based
privacy control). The designs of EpicRec system with these
two privacy controls are later presented in Section 4 and Sec-
tion 5 respectively, referred to as S-EpicRec and M-EpicRec
systems.

C-4. Data Perturbation: perturb private user data
from C-2, using public information from C-1 and the pri-
vacy parameters from C-5. The perturbed data maintains
the same format as user private data, such that it can be
used by existing recommender algorithms. In addition, the
perturbed data needs to meet two conﬂict goals, privacy
preservation and recommendation accuracy. As such, this
data perturbation module is associated with two correspond-
ing notions: privacy notion and utility notion. Examples of
privacy notions can be diﬀerential privacy, k-anonymity, in-
formation gain, etc. while examples of utility notions can be
mean absolute error, root mean square error, TopK, etc.

C-5. Privacy Quantiﬁcation: quantify user speciﬁed
privacy concern levels to mathematical privacy parameters
to be used in data perturbation (C-4) component. The ex-
amples of these quantiﬁed parameters based on diﬀerent pri-
vacy notions can be as follows: (1) the privacy budget  in
diﬀerential privacy; (2) the value of k in k-anonymity pri-
vacy; (3) the threshold of information gain; etc.

C-6. Recommendation Output: output recommenda-
tion results (e.g., overall recommendation, per-category rec-
ommendation) to users obtained from service provider using
perturbed user data.

4. DESIGN OF S-EpicRec:

SINGLE-LEVEL PRIVACY CONTROL

In this section, we focus on the design of Single-level Epi-
cRec (S-EpicRec) to enable overall privacy control. In the
rest of this section, we ﬁrst introduce our focus of privacy
and utility notions, and some general notations. Then we
present the detailed design of the main components (data
perturbation (C-4) and privacy quantiﬁcation (C-5) compo-
nents) in S-EpicRec.
4.1 Privacy & Utility Notions
4.1.1 Privacy Notion
We consider using the state-of-the-art privacy notion, Dif-
ferential Privacy [8], which not only provides strong privacy
guarantee but also allows attackers to have unlimited back-
ground knowledge. Informally, an algorithm A is diﬀeren-
tially private if the output is insensitive to any particular
record in the dataset.

Definition 1

(-Differential Privacy). Let  > 0

be a small constant. A randomized algorithm A is -diﬀerentially
private if for all data sets D1 and D2 diﬀering on at most
one element, i.e., d(D1, D2) = 1, and all S ⊆ Range(A),

Pr[A(D1) ∈ S] ≤ exp()Pr[A(D2) ∈ S]
The probability is taken over the coin tosses of A.

(4.1)

The parameter  > 0 is called privacy budget, which allows
user to control the level of privacy. A smaller  suggests more
limit posed on the inﬂuence of an individual item, leading

Figure 3: Architecture of EpicRec System

Motivated by the user study results in Section 3.1, our
proposed EpicRec system provides users their most preferred
overall and category-based privacy controls. The goal of data
perturbation is to protect user concerned private history data.
Speciﬁcally, EpicRec enables the protection of both individ-
ual records and categories of history data. Last but not least,
EpicRec focuses on perturbing user’s history data rather than
rating data since users are more concerned about history data
than rating data (based on our ﬁrst user study) and history
data can be easily obtained implicitly from user’s client while
it is infeasible to continuously request user interaction to rate
each item in large amount of his history data.

As one can see in Figure 3, the dashed lines indicate the
interactions between user and EpicRec. A user inputs his
privacy levels using user privacy control input (C-3) and pro-
vides his private data on device to user private input (C-2).
On the other hand, the solid lines indicate the interactions
between diﬀerent components. We next discuss the details
of each component as well as the interactions between them:

C-1. Public Data Input: obtain public knowledge as-
sociated with user private data from either inside or outside
knowledge resources. Speciﬁcally, C-1 collects two types of
data from public resources: public data universe items and
their associated public categories. The goal of this compo-
nent is to preserve the quality of perturbed data without
sacriﬁcing any privacy breach that could be derived using
public information.

C-2. User Private Data Input: obtain user history
data without extra user interaction, such as location history
data on iPhone; user’s web browsing history based on the
websites users clicked, or history of watched TV programs,
movies on smart TV, etc.

C-3. User Privacy Control Input: provide user inter-
face to obtain user’s privacy concern levels. Motivated by
our user study results discussed in Section 3.1, C-3 provides
the following two granularities of privacy control interfaces:

Overall (Single-level) Privacy Control:

Provide users a single input of privacy concern level;

Category-based (Multiple-level) Privacy Control:

Provide users inputs of privacy concern levels for each
category;

In each control, we use three privacy concern levels: “No
Release” as releasing no information, “Perturbed Release” as

C‐4. Data PerturbationC‐4. Data PerturbationC‐5. Privacy QuantificationC‐5. Privacy QuantificationC‐3. User Privacy Control InputC‐3. User Privacy Control InputEpicRecon DeviceRecommendation using PerturbedUser DataRecommendation using PerturbedUser DataC‐2. User Private Data InputC‐2. User Private Data InputC‐1. Public Data InputC‐1. Public Data InputExisting Recommender SystemC‐6. Recommendation OutputC‐6. Recommendation OutputPerturbedUser DataPerturbedUser Data183Table 2: Notations

Symbol

Description

I
C
C
dr
dp
PT


public item set of size n
public category set of size c
public item-category correlation matrix of size n × c
user’s private item vector of size n
user’s perturbed item vector of size n
privacy concern level
quantiﬁed privacy budget

to stronger privacy protection. More importantly, the ap-
plication of diﬀerential privacy ensures perturbed data in-
dependent of any auxiliary knowledge [7] the adversary may
obtain from public data in C-2.
4.1.2 Utility Notion
Recommender systems typically require many users’ his-
tory data for providing each user a list of his/her personal-
ized recommendations. However, when perturbing data on
each user’s device side under untrusted server settings, the
device does not even know other users’ data (no matter pri-
vate or perturbed). Therefore, it is impractical to directly
use the quality of recommendation results as a utility notion.
As such, we alternatively consider using similar utility in
[24] to measure data category aggregates on each user’s per-
turbed data as our utility notion. More speciﬁcally, we use
expected Mean Absolute Error (MAE) between user’s raw
and perturbed category aggregates, which is shown later in
experiment to be suﬃcient to guarantee recommendation
accuracy even without knowing other users’ data.
4.2 Notations

We deﬁne notations based on in each component:
C-1: Let I be public universe/set of items of size |I| =
n. Public category set is deﬁned as C of size |C| = c, in
which each item is associated with a subset of categories
represented by a public item-category correlation matrix C
of size n × c. An item i is associated with category j if and
only if the entry cij in C is equal to 1.

C-2: User’s raw private history is denoted as a vector dr
of size n. The ith entry in dr is either 1 or 0, meaning
that item i does or does not belong to user’s private history.
Note that those items in private history but not in collected
public set will be simply be considered no release.
C-3: User’s privacy concern level, denoted as PT, belongs
to one of the following three levels, {“No Release”, “Per-
turbed Release”, “All Release” }. When PT is selected as
“No Release” or “All Release”, the device simply releases no
private or all private data to recommender system respec-
tively.

C-4: User’s perturbed data is denoted as a vector dp of
size n. The ith entry in dp is either 1 or 0, meaning that
item i does or does not belong to user’s perturbed data.

C-5:  is the quantiﬁed privacy parameter (privacy bud-
get in diﬀerential privacy) when PT is selected as Perturbed
Release.

For simplicity and consistency, we denote the ith entry in
a vector v as v(i) in the rest of paper. For reference, we list
all notations in Table 2.
4.3 Design of Data Perturbation (C-4)

As described in Section 3.2, data perturbation compo-
nent (C-4) generates perturbed user data to meet both pri-
vacy preservation and recommendation quality, speciﬁcally

via the privacy and utility notions in Section 4.1. The rest
of this subsection consists of problem deﬁnition, challenges,
proposal algorithm and theoretical analysis.
4.3.1 Problem Deﬁnition

Problem 1

(S-Perturbation Problem). Given a user’s

private item vector dr associated with public item set I, a
public item-category correlation matrix C, privacy budget
 > 0. The objective is to generate the user’s perturbed item
vector dp such that (1) (privacy goal) the category aggre-
gates (number of items belonging to each category) of per-
turbed data satisfy -diﬀerential privacy with the presence or
absence of an individual item to defend against privacy leak-
age via public category information; (2) (utility goal) the
quality of category aggregates on perturbed data is well main-
tained using metrics in Section 4.1.2. Speciﬁcally, the for-
mal mathematical deﬁnition of Expected Mean Absolute Er-
ror (MAE) is deﬁned as E
, given
user raw and perturbed category aggregates CR and CP.

j=1 |CR(j) − CP (j)|(cid:105)
(cid:80)c

(cid:104) 1

c

Remarks: Our deﬁned S-Perturbation problem targets on
a stronger privacy guarantee (-diﬀerential privacy rather
than (, δ)-diﬀerential privacy in [24]) and relaxes the objec-
tive (discard the maximization of diﬀerence between private
and perturbed data in [24]), which is shown in later experi-
ments that does not hurt the quality of data perturbation.
4.3.2 Challenges
Large Magnitude of Noises for Achieving  Dif-
ferential Privacy. One of the most widely used mecha-
nisms to achieve -diﬀerential privacy is Laplace mechanism
[8] (Theorem 1), which adds random noises to the numeric
output of a query, in which the magnitude of noises follows
Laplace distribution with variance ∆f
 where ∆f represents
the global sensitivity of query f (Deﬁnition 2).

Definition 2

(Global Sensitivity [8]). For a query

f : D → Rk, the global sensitivity ∆f of f is

∆f = max

d(D1,D2)=1

(cid:107)f (D1) − f (D2)(cid:107)1

(4.2)

for all neighboring datasets D1, D2, i.e., d(D1, D2) = 1.

Theorem 1

(Laplace Mechanism [8]). For f : D →
Rk, a randomized algorithm Af = f (D) + Lapk( ∆f
 ) is -
diﬀerentially private.
(The Laplace distribution with pa-
rameter β, denoted Lap(β), has probability density function
(z) = 1
β ) and cumulative distribution function
2 (1 + sgn(z)(1 − exp(− |z|

2β exp(− |z|

β ))).)

1

Unfortunately, as an item usually belongs to many cate-
gories, the naive application of Laplace mechanism results
in the signiﬁcantly large noise magnitude and uselessness of
perturbed data because of large global sensitivity.

Intractability of Generating Useful Perturbed Data.

Even after the noise magnitude is determined, the data per-
turbation still remains intractable (NP-hard) when we need
to guarantee the usefulness of perturbed data.
4.3.3 Proposed S-DPDP Approach
In this subsection, we propose a novel Single-Level Diﬀer-
entially Private Data Perturbation (S-DPDP) Algorithm to

184solve Problem 1. In general, S-DPDP algorithm consists of
two phases to overcome the aforementioned two challenges:
(1) Phase 1, noise calibration, focuses on selecting the mag-
nitude (denoted as z(j)) for each category using public do-
main knowledge that determines injected Lap(z(j)) noises
for each category; (2) Phase 2, data sanitization, aims to
generate the useful perturbed data based on the noisy cat-
egory aggregates. Note that this phase will not lead to any
privacy loss without the access to user private data. We
next present the details of these two phases (Algorithm 1).
Phase 1: Noise calibration. The selection of noise magni-
tude is determined by optimizing the expected MAE deﬁned
in 4.3.1. Here we denote z = (z(1), . . . , z(c)) in which z(j) is
the magnitude of Laplace noise for category j. For simplic-
1
ity, we also denote zI = ( 1
z(c) ). Therefore, the noise
magnitude of Laplace noises on each category aggregate can
be determined via the following mathematical programming
(4.3):

z(1) , . . . ,

(cid:107)z(cid:107)1

minimize
subject to CzI ≤ 1, z, zI ≥ 0

(4.3)

The objective in (4.3) is to minimize expected MAE of all
injected Laplace noises onto category aggregates since the
noise on each category aggregate has E[|Lap(z(j))|] = z(j)
and the injected noises are independent. The ﬁrst constraint
serves two purposes: First, it imposes the -diﬀerential pri-
vacy guarantee as later shown in privacy analysis (Theorem
2); Second, it captures the correlation between categories
from the public information that what categories each item
belongs to. The last two constraints ensure the non-negative
noise magnitude of z and zI.

As the formulation (4.3) is non-convex, we then transform
it into the convex programming (4.4) to obtain a global op-
timal solution. Speciﬁcally, we regard both z and zI as free
variables. For the sake of clariﬁcation, we introduce two
more variables z1 = z, z2 = zI. Then, we add additional
constraints z1(j)z2(j) = 1 for each category j to ensure their
reciprocal relationship. Moreover, we further relax this con-
straint to z1z2

T ≥ I.

(cid:107)z1(cid:107)1

minimize
subject to Cz2 ≤ 1, z1z2

T ≥ I, z1, z2 ≥ 0

(4.4)

In this phase, our data perturbation algorithm ﬁrst solves
the convex programming (4.4). Then, we set zI = z2 such
that the ﬁrst constraint in (4.4) is not violated; and set z by
letting each entry z(j) be the reciprocal of the jth entry in
zI. Thanks to the convexity property of (4.4), our optimized
noise calibration algorithm is guaranteed to outperform the
traditional Laplace mechanism.

Example 1. Figure 4 shows a running example that ex-
plains why our novel noise calibration approach (phase 1)
can always outperform the existing Laplace mechanism in
Theorem 1. In this tiny example, public set of items con-
tains ﬁve items and their associated ﬁve categories. A check
represents that an item belongs to this category (e.g., item
1 belongs to category 1,2,3). The row in gray shows our
novel category based sensitivity obtained by solving (4.4) with
 = 1. For a category associated with more items, the sen-
sitivity of this category intends to be larger since there is
higher probability that the aggregate of this category will be
aﬀected by adding or removing a single item. Therefore, the
last column of last two rows shows a better MAE error using

Input : private user data dr, item-category matrix C,

privacy budget 

Output: perturbed user’s data dp
// Phase 1: Noise calibration
1 Solve mathematical programming (4.4);
2 z ← reciprocal of each entry in zI;
3 Sample noises from Lap(z(j)) for each category j;
4 Set NA with each entry

N A(j) =(cid:80)

i∈I cij dr(i) + Lap(z(j));

// Phase 2: Data sanitization

5 Relax integral constraints in (4.5);
6 Solve the relaxed (4.5) by replacing dp with dr
7 foreach each element i in dr
8

Randomly select a number ξ between 0 and 1;
dp(i) ← 1 if ξ ≤ dr
p(i) and dp(i) ← 0 otherwise;

p do

9

p ∈ [0, 1]n;

10 end
11 return dp;

Algorithm 1: S-DPDP Algorithm

our category based sensitivity via (4.4) than using traditional
global sensitivity.

Figure 4: Running Example of Noise Calibration

Phase 2: Data sanitization. This phase takes the above
noise magnitude vector output z as input and generates the
useful perturbed user data. We ﬁrst quantify the useful-
ness of perturbed data by minimizing the error between the
category aggregates on perturbed data and the noisy cat-
egory aggregates. Speciﬁcally, we introduce two error vec-
tors l, r and consider the root mean square error (RMSE)
as 1
2. Then, we formulate the following optimization
formulation (4.5):

2(cid:107)l + r(cid:107)2

1

2(cid:107)l + r(cid:107)2

2

minimize
subject to NA − l ≤ CT dp ≤ NA + r, dp ∈ {0, 1}n (4.5)
where NA is the noisy category aggregate vector in which

each entry N A(j) =(cid:80)

i∈I cijdr(i) + Lap(z(j)).

It is not hard to see that solving (4.5) is NP-hard by re-
ducing it from Exact Cover problem (The proof is similar
to that in [24] and omitted due to space limit). Therefore,
our data perturbation algorithm solves the relaxed formula-
p ∈ [0, 1]n.
tion of (4.5) by replacing dp with dr
Then, we obtain dp by rounding each entry dr
p(i) to 1 with
probability dr
4.3.4 Theoretical Analysis
We theoretically analyze privacy and utility, as well as

p in which dr

p(i).

time complexity.

Privacy Analysis. We show the diﬀerential privacy guar-

antee of S-DPDP algorithm:

Theorem 2

(S-DPDP Privacy Analysis). S-DPDP al-

gorithm enjoys -diﬀerential privacy.

item/categorycategory 1category 2category 3category 4category 5MAEerror when privacy budget 𝝐=𝟏item 1item2item3item4item5Sensitivity3.612.363.342.361.382.61globalsensitivity equals to 3 for all categories3185(cid:89)

Proof. We observe that there is no privacy loss in phase
2 of S-DPDP algorithm as it is considered as post-processing
on diﬀerentially private category aggregates without the ac-
cess of user private data. Since any post-processing of the
answers cannot diminish this rigorous privacy guarantee ac-
cording to Hey et al. [12], we only need to focus on analyzing
the privacy guarantee in phase 1 of S-DPDP algorithm.

Let D1, D2 be neighboring datasets (i.e., d(D1, D2) = 1)
and f (Di) be the category aggregates of user’s private data
i). For any r = (r1, . . . , rc) ∈ Range(S-DPDP),
Di (w.r.t. dr
we have the following analysis:

Pr[S-DPDP(D1)(j) = r(j)]
Pr[S-DPDP(D2)(j) = r(j)]

Pr[S-DPDP(D1) = r]
Pr[S-DPDP(D2) = r]

(cid:16) −(cid:88)
(cid:16) − max

j∈C

1

z(j)

=

j∈C

|fj(D1) − fj(D2)|(cid:17)
(cid:88)

1

≥ exp

≥ exp

d(D1,D2)=1

z(j)

j∈C

|fj(D1) − fj(D2)|(cid:17) ≥ e

−

The ﬁrst step holds due to the independently injected noises
on each category aggregate; the second step is derived from
the injected Laplace noises and triangle inequality; and the
last step holds from the ﬁrst constraint in (4.4), that is,

max

d(D1,D2)=1

z(j)

j∈C

1

|fj(D1) − fj(D2)| = max
i∈I

1

cij ≤ 

z(j)

j∈C

(cid:88)

(cid:88)

The proof is complete.

Utility Analysis. We show the utility bound of MAE

on category aggregates between raw and perturbed data:

Theorem 3

(S-DPDP Utility Analysis). The expected

MAE between raw and perturbed category aggregates (via S-
DPDP algorithm) is upper bounded by 2(cid:107)z(cid:107)1/c, where z is
the optimal solution of (4.4).

Proof. Let z(j), lj, rj be the jth entry in vectors z, l, r.

E[c · MAE] = E

j=1

(cid:104) c(cid:88)
|Lap(z(j))|(cid:105)
(cid:105)
(cid:104)(cid:107)l + r(cid:107)1

≤ E

(cid:104) c(cid:88)
≤ c(cid:88)

j=1

j=1

= (cid:107)z(cid:107)1 + E

E[|Lap(z(j))|] + E

|CR(j) − CP (j)|(cid:105)
(cid:104) c(cid:88)
| max{lj, rj}|(cid:105)
(cid:104) c(cid:88)

(cid:105)

j=1

(lj + rj)

+ E

j=1

Then, let xj be the random variable representing noisy
aggregate on category j with probability density function
φ(xj). We analyze the bound of E

as follows:

E

···

(cid:105)
(cid:104)(cid:107)l + r(cid:107)1
(cid:90)
(cid:104)(cid:107)l + r(cid:107)1|x1, . . . , xc
(cid:90) (cid:16) c(cid:88)
(cid:104)(cid:90)
(cid:104)

(cid:90)
(cid:90)
c(cid:88)

···

(cid:104)

(cid:105)

j=1

E

E

lj + rj|xj

φ(xj)dxj

E

=

≤

=

(cid:105)

(cid:104)(cid:107)l + r(cid:107)1
(cid:105) c(cid:89)
(cid:105)(cid:17) c(cid:89)
(cid:105) ≤ c(cid:88)

j=1

j=1

j=1

j=1

lj + rj|x1, . . . , xc

φ(xj)dx1 ··· dxc

φ(xj)dx1 ··· dxc

|z(j)| = (cid:107)z(cid:107)1

(cid:104)
n(cid:88)

E

where the ﬁrst step derives from the law of total expectation;
the last step holds because for each category j ∈ C, we have
the following inequality:

(cid:105)

(cid:104)

(cid:105)

lj + rj|x1, . . . , xc

= E

lj + rj|xj

=

|cijdr

p(i) − N A(j)|dr

p(i) ≤ |xj| = |zj|

i=1

where the last step holds from the fact that dr
the optimal solution to relaxed (4.4).

p(i) ≤ 1 and

Time Complexity Analysis. We analyze the time com-
plexity to phase 1 and 2 respectively. The time complex-
ity of phase 1 is determined by solving (4.4).
It is not
hard to see that both vector variables z1 and z2 have di-
mension c and there are n + 3c constraints. In practice, c
is equal to the number of categories from public informa-
tion, which is actually a constant. Therefore, according to
Megiddo [20], the time complexity of solving (4.4) is O(n).
In phase 2, we ﬁrst solve the relaxed (4.5), which is equiv-
alent to solving the non-negative least square programming
p ≥ 0 that has been well studied in
1
literature [2] and will be shown in Section 6.2 with fast run-
ning time in practice. The last rounding phase takes another
O(n) time.
4.4 Design of Privacy Quantiﬁcation (C-5)

p − NA(cid:107)2

2(cid:107)CT dr

2, s.t. dr

In this subsection, we design the privacy quantiﬁcation
(C-5) component to quantify “Perturbed Release” level to a
privacy budget , which is used to feed into S-DPDP al-
gorithm in data perturbation component (C-4) discussed
above. Speciﬁcally, we devise a novel Single Privacy Budget
Quantiﬁcation (S-PBQ) algorithm to select a privacy budget
 to optimize the utility of perturbed data.

The idea of S-PBQ algorithm is based on our observation
that the utility loss of perturbed data will not signiﬁcantly de-
crease any more when privacy budget  is larger than some
threshold. In detail, S-PBQ algorithm ﬁrst understands the
distribution of noise magnitude to each category aggregate
and then search the optimal privacy budget after which util-
ity loss can be negligible. Next, we discuss about the details
of these two phases in S-PBQ algorithm.

Phase 1: Noise magnitude determination. We follow the
idea of phase 1 in S-DPDP algorithm. The key tweak is
based on the following observation: when we replace  in
phase 1 of Algorithm 1 with an arbitrary constant α, it is
easy to see that each entry in new solution z(cid:48) is propor-
tional to that in z obtained in phase 1 of Algorithm 1. The
proportionality constant is equal to 
plicity and then obtain z(cid:48)
ical programming similar as (4.4):

Therefore, in this phase, we consider using α = 1 for sim-
2 using the following mathemat-

α , i.e., z(cid:48) = 

1, z(cid:48)

α z.

(cid:107)z(cid:48)

1(cid:107)1

minimize
subject to Cz2 ≤ 1, z(cid:48)

1z(cid:48)

2

T ≥ I, z(cid:48)

1, z(cid:48)

2 ≥ 0

(4.6)

Our S-PBQ algorithm ﬁrst solves (4.6) and then sets z(cid:48)
I =
z(cid:48)
2 and each entry in z(cid:48) as the reciprocal of each entry in z(cid:48)
I.
Phase 2: Privacy budget optimization. This phase deter-
mines the privacy budget  using the above z(cid:48)
I, z(cid:48). Based on
the idea that the utility loss of perturbed data will not sig-
niﬁcantly decrease after  is larger than some threshold, we
search for the optimal  from 0 using a small learning step

186Input : item-category matrix C, learning step rate δ
Output: quantiﬁed optimal privacy budget 
// Phase 1: Noise magnitude determination

1 Solve mathematical programming (4.6);
2 z(cid:48) ← reciprocal of each entry in zI;

// Phase 2: Privacy budget calculation
3 Formulate (4.7) with sampled S and RandAs;
4 foreach i = 1, 2, . . . do
5

Solve relaxed (4.7) with I = 1
as line 5-10 in Algorithm 1);
err(i) ← 1
if i ≥ 2 & err(i) − err(i − 1) ≥ err(i − 1) − err(i − 2)
then

m(cid:107)l + r(cid:107)1 + I with rounded dp;

iδ and round dp (similar

6

7

8

9

 ← (i − 1)δ;
break;

end

10
11 end

// Repeat Phase 2

12 Repeat [Phase 2] N times and return averaged ;

Algorithm 2: S-PBQ Algorithm

rate δ (we choose δ = 0.02 as an experience value in prac-
tice). That is, we solve the following formulation (4.7) by
substituting I with 1
3δ , . . . The algorithm terminates
at ith iteration when the improvement of optimal utility loss
obtained by (4.7) using I = 1
(i−1)δ is no smaller
than that using I = 1
(i−2)δ . At last, we set
 = (i − 1)δ.

(i−1)δ over I = 1

iδ over I = 1

2δ , 1

δ , 1

minimize
subject to RandAs + I Sz(cid:48) − l ≤ CT dp

2

1

2(cid:107)l + r(cid:107)2
≤ RandAs + I Sz(cid:48) + r
dp ∈ {0, 1}n

(4.7)

where S = diag(s1, . . . , sm), in which each diagonal entry
sj is sampled from Laplace distribution Lap(1) such that
z(cid:48)(j)s(j) is a sample from Laplace distribution Lap(z(cid:48)(j))
due to the property of Laplace distribution kX ∼ Lap(kµ, kβ)
for a random variable X ∼ Lap(µ, β) for a positive constant
k [16]. In addition, in order to avoid potential privacy leak-
age caused by  quantiﬁcation, we do not use user’s private
history dr. Instead, we consider using RandAs, the cate-
gory aggregates on randomly selected items from all public
items. For simplicity, we use uniform sampling with sam-
pling rate as the averaged number of items for each user.

Our S-PBQ algorithm repeatedly samples diﬀerent S and
RandAs to obtain I by solving (4.7) in this phase. In the
experiment, we select the number of repeat times N to be 10
by considering the eﬃciency of S-PBQ algorithm. At last,
the averaged  is chosen as the quantiﬁed privacy budget.

Time Complexity Analysis. S-PBQ algorithm is sim-
ilar as S-DPDP algorithm with O(n) (phase 1 and rounding
steps in phase 2) and the running time to solve relaxed (4.7).
The second phase has its time complexity O(T ) similar as
that of solving relaxed (4.5). To provide strong privacy guar-
antee, we consider  ≤ 1 and therefore the repeat of phase
2 will take at most O(T + n). Our experimental results in
Figure 7 shows the eﬃcient running time in practice.
4.5 Overall Analysis of S-EpicRec System

We summarize the performance of S-EpicRec, that is, the
output of S-DPDP algorithm with the quantiﬁed privacy
budget using S-PBQ algorithm.

Privacy Guarantee. The perturbed data using S-EpicRec

satisﬁes -diﬀerential privacy where  is determined by S-
PBQ algorithm (Algorithm 2).

(cid:80)c

j=1 cij.

c

(cid:107)C(cid:107)1

), where (cid:107)C(cid:107)1 = max1≤i≤n

Utility Guarantee. The expected MAE between raw
and perturbed category aggregates (output of S-EpicRec) is
upper bounded by O(
It is not hard to see that the optimal solution of (4.6) is
upper bounded by (cid:107)C(cid:107)1. Therefore, the optimal solution of
(4.4) is upper bounded by O(
) based on Theorem 3, in
which the constant factor is dependent on the quantiﬁed .
Time Complexity. The overall time complexity of S-
EpicRec is O(n + T ) where T is the time complexity of solv-
ing relaxed (4.5) as discussed in [2].

(cid:107)C(cid:107)1

c

5. DESIGN OF M-EpicRec:

MULTI-LEVEL PRIVACY CONTROL

In this section, we further design a M-EpicRec framework
to enable the category-based privacy concern controls. The
idea of M-EpicRec is extended from S-EpicRec proposed in
Section 4. Basically, we consider using the same privacy
and utility notions for designing C-4 and C-5 components in
M-EpicRec system. The rest of this section focuses on the
diﬀerent parts between S-EpicRec and M-EpicRec, mainly
in terms of notations and the design of C-4 and C-5.
5.1 Notations

In M-EpicRec, the only diﬀerent notation from those in S-
EpicRec is in user privacy control component (C-3). Specif-
ically, we deﬁne the vector of privacy concern levels PT =
{PT(1), . . . , PT(c)} to replace a single PT. Each entry PT(j)
is the user-speciﬁed privacy concern level for category j,
which still belongs to one of the same three levels {“No
Release”, “Perturbed Release”, “All Release” }. Correspond-
ingly, we deﬁne the vector privacy budget  = {(1), . . . , (c)},
where (j) =  when PT(j) is selected as “Perturbed Release”
and (j) = 0 when PT(j) is selected as either “No Release”
or “All Release” indicating that no randomness is considered
for these categories.
Moreover, we deﬁne three vectors Ln, Lp, La with respect
to three privacy concern levels {“No Release”, “Perturbed
Release”, “All Release” }, based on PT. For example, when
PT=(no, perturbed, no, all, all, perturbed) w.r.t. 6 cate-
gories, we have Ln = (1, 0, 1, 0, 0, 0), Lp = (0, 1, 0, 0, 0, 1), La =
(0, 0, 0, 1, 1, 0). Note that each category has one privacy tol-
erance level and Ln + Lp + Ln = 1.
5.2 Design of Data Perturbation (C-4)

5.2.1 Problem Deﬁnition
We consider M-Perturbation Problem which diﬀers
from S-Perturbation Problem deﬁned in Section 4.3.1
from the following aspects:
(1) M-Perturbation problem
takes two diﬀerent inputs: the category-based privacy con-
cern levels Ln, Lp, La (derived from PT) and a privacy bud-
get  for categories with “perturbed release” privacy concern
level; (2) M-Perturbation problem targets on maintaining
the quality of category aggregates on perturbed data only
associated with “perturbed release” categories while releas-
ing no data in “no release” categories and all raw data only
associated with “all release” categories. (Note that we choose
to prioritize privacy protection when it conﬂicts with util-
ity. That is, we do not release an item as long as one of its
categories is set “no release”; we perturb an item if one of
its categories is set “perturbed release” privacy concern level

187and none of its categories is set “no release”; we release an
item only if all of its categories are set “all release”.)
5.2.2 Challenges
In addition to the challenges of S-Perturbation problem
described in Section 4.3.2, M-Perturbation problem poses
some additional challenges mainly from the constraints from
“no release” and “all release” categories. Especially when an
item belongs to multiple categories, the preservation of util-
ity on category aggregates on perturbed data in the “per-
turbed release” categories becomes more diﬃcult.
5.2.3 Proposed M-DPDP Approach
In this subsection, we focus on discussing about the novel
part of M-DPDP approach beyond S-DPDP approach (in
Section 4.3.3), which is developed to support multi-level pri-
vacy control. Speciﬁcally, as the key idea of these two ap-
proaches are consistent, we will mainly present the diﬀerence
in phase 1 and 2 respectively.

Phase 1: Noise calibration. The major diﬀerence between
phase 1 in M-DPDP and that in S-DPDP is that the noises
are injected only on categories with “perturbed release” pri-
vacy level while the noise magnitude on other categories are
enforced to be 0. Thus, (4.3) can be rewritten as follows:

minimize
subject to CzILpI ≤ 1, (Ln + La)T z = 0

(cid:107)z(cid:107)1
(Ln + La)T zI = 0, z, zI ≥ 0

(5.1)

where the ﬁrst constraint imposes the satisfaction of diﬀer-
ential privacy on these categories with “perturbed release”
privacy level; the next two constraints ensure no noise cal-
ibration into other categories. Accordingly, the quadratic
programming (4.4) can be rewritten as follows:

(cid:107)z1(cid:107)1

minimize
subject to Cz2LpI ≤ 1

(Ln + La)T z1 = 0, (Ln + La)T z2 = 0
z1z2

T ≥ IT LpI, z1, z2 ≥ 0

(5.2)

In this phase, M-DPDP algorithm solves (5.2) and sets
z2(j) if category j has “perturbed release” privacy

z(j) = 1
level and z(j) = 0 otherwise.

Phase 2: Data sanitization. The major diﬀerence in phase

2 is from the following two aspects:

First, in order to address the constraints of categories with
“no release” and “all release” privacy levels, we select “all
release” user data (denoted as da
r ) from user raw data dr,
where the ith entry in da
r is 1 if and only if this data belongs
to user raw data (ith entry in dr is 1) and all of its associated
categories have “all release” privacy levels.

Second, we inject Lap(z(j)) into the jth category for those
categories with “perturbed release” privacy level. Then, we
reformulate (4.7) as follows:

1

2

minimize
subject to NA − l ≤ CT dp ≤ NA + r

2(cid:107)l + r(cid:107)2
dp ≥ da
(Ln + La)T l = 0, (Ln + La)T r = 0

r , dp ∈ {0, 1}n

where N A(j) = (cid:80)
lease” privacy level; and N A(j) =(cid:80)

i∈I cijdr(i) for categories with “all re-
lease” privacy level; N A(j) = 0 for categories with “no re-
i∈I cijdr(i) + Lap(z(j))
for categories with “perturbed release” privacy level. The
second constraint imposes that the raw data only in cate-
gories with “all release” privacy level will be released; the

(5.3)

last two constraints guarantee the equivalence for categories
with “all release” and “no release” privacy levels.
5.2.4 Theoretical Analysis.

Theorem 4

(M-DPDP Privacy Analysis). M-DPDP

algorithm enjoys -diﬀerential privacy.

Theorem 5

(M-DPDP Utility Analysis). The expected

MAE between raw and perturbed aggregates (via M-DPDP
algorithm) is upper bounded by 2(cid:107)z(cid:107)1/c, where z is the op-
timal solution of (5.2).

The proofs are similar as Theorem 2, 3 and omitted due to
space limit. The time complexity of M-DPDP is also similar
as S-DPDP and omitted.
5.3 Design of Privacy Quantiﬁcation (C-5)

We propose M-PBQ approach in this section, extending
from S-PBQ algorithm in S-EpicRec system (Section 4.4).
Similarly, in phase 1, M-PBQ replace unknown  in (5.2) by
1 and solve it to obtain z(cid:48).

The diﬀerence between them mainly lies in the second
phase. In phase 2, we substitute (4.7) in Algorithm 2 with
the following formulation:

2

1

minimize
subject to RandAm + I Sz(cid:48) − l ≤ CT dp

2(cid:107)l + r(cid:107)2
≤ RandAm + I Sz(cid:48) + r
dp ≥ da
(Ln + La)T l = 0, (Ln + La)T r = 0

r , dp ∈ {0, 1}n

(5.4)

where S = diag(s1, . . . , sm) , in which the jth diagonal entry
s(j) is sampled from Laplace distribution Lap(1) if this cat-
egory has “perturbed release” privacy level and 0 otherwise;
RandAm is the category aggregates on randomly selected
items from all public items, in which Randm(j) = 0 for cat-
egories with “no release” privacy level. The time complexity
of M-PBQ is similar as S-PBQ and omitted.
5.4 Performance Analysis of M-EpicRec

We summarize the performance of M-EpicRec as follows:
Privacy Guarantee. The category aggregates of per-
turbed data from M-EpicRec satisfy -diﬀerential privacy
where  is determined by M-PBQ algorithm.

Utility Guarantee. The expectation of MAE between

(cid:80)m

raw and perturbed category aggregates (output of M-EpicRec)
is upper bounded by O(

), where (cid:107)C(cid:107)1 = max1≤i≤n

(cid:107)C(cid:107)1

j=1 cij.

c

Time Complexity. The overall time complexity of S-
EpicRec is O(n + T ) where T is the time complexity of solv-
ing relaxed (5.3) as discussed in [2].

6. EXPERIMENTAL EVALUATION
6.1 Datasets, Metrics, Competitors & Settings
Datasets. We test EpicRec on two real-world datasets:
MovieLens1: a movie rating dataset collected by the Grou-

pLens Research Project at the University of Minnesota through
the website movielens.umn.edu during the 7-month period
from September 19th, 1997 through April 22nd, 1998. The
number of movie categories is 18. We use the MovieLens-
1M, with 1000,209 ratings from 6,040 users on 3,883 movies.

1

http://grouplens.org/datasets/movielens

188Yelp2: a business rating data provided by RecSys Chal-
lenge 2013, in which Yelp reviews, businesses and users are
collected at Phoenix, AZ metropolitan area. The number
of business categories is 21. We use all reviews in training
dataset, with 229,907 reviews from 43,873 users on 11,537
businesses.

and M-EpicRec systems from the following aspects:

Metrics. We evaluate perturbation quality of S-EpicRec
• Perturbed Category Aggregates Quality: we use the ex-
pected MAE metric discussed in Section 4.1.2 with its math-
ematical deﬁnition in Section 4.3.4;
• Recommendation Accuracy: we consider the MAE Loss
between the recommendation results using raw and per-
p −GT ui|
r −GT ui| − 1, where
turbed data, deﬁned as
n is the number of items, U is the number of all users and
Recui
p are the elements in ith column and uth row
(item i for user u) in predicted recommendation matrices
Recr, Recp using user raw data dr and perturbed data dp.
In addition, we also show the scalability of our EpicRec

(cid:80)U
(cid:80)U
u=1 |Recui
u=1 |Recui

r , Recui

(cid:80)n
(cid:80)n

i=1

i=1

system is further validated via running time.

(Pseudo) Competitors. As this paper is the ﬁrst at-
tempt for designing a privacy preserving system to enable
user-understandable privacy concern control, there is no ex-
isting work to fairly compare with our approach. Therefore,
we consider embedding existing approaches into our system,
called pseudo-competitors. Speciﬁcally, we plug them into
S-DPDP/M-DPDP algorithms to replace phase 1 for noise
calibration only, which ﬁrst uses our quantiﬁed privacy bud-
get  from S-PBQ/M-PBQ algorithms and then sanitizes
data by phase 2 in S-DPDP/M-DPDP algorithms.

rithms into EpicRec system for comparison:

We plug the following two existing noise calibration algo-
• Pseudo-LPA (Pseudo Laplace Mechanism): the base-
line method that injects Laplace perturbation noise to each
count using domain-speciﬁc global sensitivity ∆f ;
• Pseudo-GS (Pseudo Grouping&Smoothing Mechanism):
the best method for aggregate release with grouping and
smoothing proposed in [14].

Note that we do not compare with the approach in [24]
since it only supports larger  ( > 1) which our proposed
EpicRec focuses on stronger privacy protection with  ≤ 1.
Settings. We conduct the classic recommender system
algorithm, collaborative ﬁltering [25], using GraphLab3. The
only parameter δ is set to 0.02 according to diﬀerential pri-
vacy literature. We test the experiments on personal com-
puter which is equipped with 1.9GHz CPU and 8GB RAM.
We run each experiment 10 times and report the average re-
sult. To evaluate recommendation accuracy, we use 10-fold
cross-validation and stochastic gradient descent algorithm
for collaborative ﬁltering. In M-EpicRec case, we randomly
select the privacy levels for each category for each user.
6.2 Evaluation Results

Perturbation Quality. Figure 5 reports the results of
S-EpicRec system. As shown in Figure 5(a), the perturbed
category aggregates quality of S-DPDP algorithm in S-EpicRec
system continuously outperforms other competitors up to
10% in both MovieLens and Yelp datasets. The reason is
that our S-EpicRec determines the calibrated noises based
on the underlying data property via the correlation between

2

3

https://www.kaggle.com/c/yelp-recsys-2013/data
http://select.cs.cmu.edu/code/graphlab/pmf.html

(a) Perturbed Category Aggregate Quality

(b) Recommendation Accuracy

Figure 5: S-EpicRec Results (L:MovieLens; R:Yelp)

categories as described in (4.3), thereby capturing the mini-
mum noise magnitude that is suﬃcient to ensure the privacy
guarantees. In the meanwhile, data sanitization (phase 2 in
Algorithm 1) can also take advantage of the category cor-
relations to minimize the error when sanitizing data. On
the other hand, the grouping and averaging of category ag-
gregates in Pseudo-GS loses a lot of correlation informa-
tion between categories and lead to a larger error. Likewise,
Pseudo-LPA applies the global sensitivity to determine the
noise magnitude, which only captures the maximum number
of categories an item can belong to and largely ignores the
correlation between categories. Moreover, the recommenda-
tion loss in Figure 5(b) is up to 5% and 3% in MovieLens
and Yelp datasets with strong privacy guarantees ( = 0.2
in MovieLens and  = 0.3 in Yelp).

Figure 6 shows the similar performance results in M-EpicRec

system that our M-DPDP algorithm outperforms other com-
petitors from both aspects. The recommendation quality of
M-DPDP algorithm (in Figure 6(b)) has slightly worse than
S-DPDP algorithm in S-EpicRec due to constraints from
items in “No Release” categories.

One may question that what if a user has privacy con-
cern on some category rather than particular items in this
category. The bottom two ﬁgures in Figure 6(b) show that
our proposed M-EpicRec framework can indeed provide this
function by allowing user to select “no release” for those cat-
egories. Thanks to the correlation between categories (an
item usually belongs to many categories), we did some addi-
tional experiments showing that the recommendation MAE
loss on movies in “no release” categories is less than 5% worse
than that in “perturbed release” categories.

Privacy Budget Quantiﬁcation. As we can see in Fig-
ure 5, the blue shadows show the range of quantiﬁed optimal
privacy budget  spanning in our 10 testings. It is interesting
to see that our quantiﬁed  values fall in the range of around
 = 0.23 in MovieLens dataset and  = 0.25 in Yelp dataset
respectively, after which the recommendation loss does not
reduce dramatically and maintains relatively stable. Sim-
ilar observations are shown in Figure 6 for category-based
privacy control, with  = 0.29 in MovieLens dataset and
 = 0.33 in Yelp dataset.

Scalability. Figure 7 shows that the running time of
both S-EpicRec and M-EpicRec systems is no longer than
0.7 and 1.5 seconds respectively. It takes slightly longer on
Yelp dataset since the number of public items is relatively

 0 5 10 15 20 25 30 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAE of Perturbed Category AggregatePrivacy Budget εS-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε 0 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAE of Perturbed Category AggregatePrivacy Budget εS-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε 0 10 20 30 40 50 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget εS-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε 0 5 10 15 20 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget εS-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε189(a) Perturbed Category Aggregate Quality

(b) Recommendation Accuracy (Top: “Perturbed Release”
Categories; Bottom: “No Release” Categories)

Figure 6: M-EpicRec Results(L:MovieLens; R:Yelp)

Figure 7: Running Time (L:MovieLens; R:Yelp)

larger than that on MovieLens dataset. M-EpicRec takes
longer than S-EpicRec due to its more complex optimiza-
tion process (with more constraints). As the perturbation
process is usually conducted oﬄine, the running time of our
proposed framework is considered good enough.

7.

IMPLEMENTATION

In this section, we present the implementation of a proof-
of-concept EpicRec system. As shown in Figure 8, we im-
plement a web-based EpicRec client for movie recommen-
dation, incorporated with a standard recommender server
using classic recommendation approaches. The rest of this
section ﬁrst brieﬂy discusses about server side implementa-
tion and then focuses on the implementation of each com-
ponent in EpicRec client. We implement EpicRec client on
a laptop with Ubuntu Desktop OS and the recommender
server on a workstation with Ubuntu Server OS.
7.1 Movie Recommendation Service Provider
In our PoC system, we use a workstation with Ubuntu
Server OS as the recommender system. We conduct the
personalized recommendation using collaborative ﬁltering
(stochastic gradient descent algorithm) via GraphLab3. Rrec-
ommendation results are ranked overall and in each category.
All transmissions between client and server are via SSL/TLS
security protocol.

7.2 EpicRec for Movie Recommendation

On the device side, we maintain a local database to store
the input from public data input (C-1) component and user
private data input (C-2) component. Then, we design and
implement user interfaces for user privacy control (C-3) com-
ponent and read the data from user input.
Public Data Input (C-1): We crawl ∼6,000 recent movies’
meta-data from the public “My API Films” website4, includ-
ing movie title, genre, plot description and poster image; we
then store them in the table “allMovies” in local database.
Moreover, the physical ﬁles of poster images are stored lo-
cally with the corresponding names as in the allMovies table.
Each movie/record in this table is associated with an addi-
tional boolean attribute “watched”, which is initialized as 0
(indicating that no movies have been watched).

User Private Data Input (C-2): In order to obtain user
private history of watched movies, we implement in a sim-
pliﬁed manner by scanning the history ﬁles of each web
browser. We mainly focus on two popular web browsers,
Google Chrome and Mozilla Firefox, where we download
the history ﬁles “∼/.conﬁg/google-chrome/Default/History”
and “∼/.mozilla/ﬁrefox/*.default /places.sqlite*” respectively.
We next search for each movie title in all these history ﬁles
and update the “watched” attribute to 1 if a movie’s title
exists in the history ﬁle.

User Privacy Control Input (C-3): We designed the user
interface for user privacy control input (see C-3 in Figure 8),
in which a user is allowed ﬁrst to select his overall privacy
concern level from “no release”, “perturbed release” or “all
release”. If “perturbed release” is selected, user can further
select if he wants to select diﬀerent privacy concern levels
for diﬀerent categories of movies. If so, a list of categories
is popped up with privacy concern level drop-down boxes.

Privacy Quantiﬁcation (C-5) & Data Perturbation (C-4):
If a user selects “perturbed release” and does not check the
box to set category-based privacy concern levels, we call C-4
and C-5 components in S-EpicRec system. (When “no re-
lease” or “all release” is selected, we simply release no data or
all raw data.) Otherwise, we call C-4 and C-5 components in
M-EpicRec system to support user speciﬁed category-based
multiple privacy concern levels.

Recommendation Output (C-6): We simply use a netﬂix-
style output to provide users overall and per-category top
movie recommendation. Moreover, the categories are ranked
on the client side by the number of movies the user actually
watched to capture user’s preference on diﬀerent categories.

8. CONCLUSION AND FUTURE WORK

Conclusion. In this paper, we designed a novel practical
privacy-preserving system on client, EpicRec, for person-
alized recommendation via state-of-the-art diﬀerential pri-
vacy. EpicRec provides users a privacy control interface
such that users can control their privacy concerns in a way
they understand and of their preferred granularities, either
overall or category-based concerns. EpicRec further quanti-
ﬁes these layman privacy concern levels to privacy budget,
which is next used as input to conduct data perturbation
algorithm via diﬀerential privacy. With these key compo-
nents, EpicRec can also work with other data collection and
output components. We believe this is an important step

4

http://www.myapiﬁlms.com/index.jsp

 0 1 2 3 4 5 6 7 8 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAE of Perturbed Category AggregatePrivacy Budget εM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε 0 1 2 3 4 5 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MAE of Perturbed Category AggregatePrivacy Budget εM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε 8 8.5 9 9.5 10 10.5 11 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget εM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε 4 6 8 10 12 14 16 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget εM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε 8 9 10 11 12 13 14 15 16 17 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget εM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε 6 8 10 12 14 16 18 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Recommendation MAE Loss (%)Privacy Budget εM-EpicRecPseudo-LPAPseudo-GSQuantified Optimal ε 300 400 500 600 700 800 900 1000 1100 1200 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Running Time (ms)Privacy Budget εS-EpicRecM-EpicRec 500 1000 1500 2000 2500 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Running Time (ms)Privacy Budget εS-EpicRecM-EpicRec190Figure 8: Proof-of-Concept Implementation of EpicRec for Movie/TV Recommendation

towards designing a practical privacy-preserving system for
personalized recommendation.

Future work. We will extend EpicRec into more com-
prehensive and practical cases from diﬀerent aspects: (1)
we will improve the implementation of C-1 and C-2 compo-
nents in our PoC system by potentially developing browser
extensions; (2) we will conduct a large-scale ﬁeld study to
observe and understand users’ natural behaviors, in-situ at-
titude and perceptions when using our browser extensions
to interact with EpicRec system; (3) we will continue to de-
velop data perturbation techniques to support user’s stream-
ing private data; diﬀerent types of user private data; and al-
low users to iteratively adjust their privacy levels for trading
oﬀ privacy and recommendation.

9. REFERENCES

[1] L. Bonomi, L. Xiong, and J. J. Lu. Linkit: privacy preserving

record linkage and integration via transformations. In
SIGMOD, pages 1029–1032, 2013.

[2] S. Boyd and L. Vandenberghe. Convex Optimization.

Cambridge University Press, 2004.

[3] J. Canny. Collaborative ﬁltering with privacy. In IEEE

Symposium on S&P, pages 45–57, 2002.

[4] T.-H. H. Chan, M. Li, E. Shi, and W. Xu. Diﬀerentially private
continual monitoring of heavy hitters from distributed streams.
In PETS, pages 140–159, 2012.

[5] K. Chaudhuri, A. Sarwate, and K. Sinha. Near-optimal

diﬀerentially private principal components. In NIPS, pages
989–997. 2012.

[6] K. Chaudhuri and S. A. Vinterbo. A stability-based validation
procedure for diﬀerentially private machine learning. In NIPS,
pages 2652–2660. 2013.

[7] C. Dwork. Diﬀerential privacy: A survey of results. In TAMC,

pages 1–19, 2008.

[8] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating

noise to sensitivity in private data analysis. In TCC, pages
265–284, 2006.

[9] B. C. M. Fung, K. Wang, R. Chen, and P. S. Yu.

Privacy-preserving data publishing: A survey of recent
developments. ACM Comput. Surv., 42(4):14:1–14:53, 2010.

[10] A. Guha Thakurta and A. Smith. (nearly) optimal algorithms

for private online learning in full-information and bandit
settings. In NIPS, pages 2733–2741. 2013.

[11] M. Hardt, K. Ligett, and F. Mcsherry. A simple and practical

algorithm for diﬀerentially private data release. In NIPS, pages
2339–2347. 2012.

[12] M. Hay, V. Rastogi, G. Miklau, and D. Suciu. Boosting the

accuracy of diﬀerentially private histograms through
consistency. VLDB, 3(1-2):1021–1032, 2010.

[13] B. Heitmann, J. G. Kim, A. Passant, C. Hayes, and H.-G. Kim.

An architecture for privacy-enabled user proﬁle portability on
the web of data. In HetRec, pages 16–23, 2010.

[14] G. Kellaris and S. Papadopoulos. Practical diﬀerential privacy
via grouping and smoothing. VLDB, 6(5):301–312, Mar. 2013.

[15] A. Korolova, K. Kenthapadi, N. Mishra, and A. Ntoulas.

Releasing search queries and clicks privately. In WWW, pages
171–180, 2009.

[16] S. Kotz, T. J. Kozubowski, and K. Podgorski. The Laplace

distribution and generalizations: a revisit with applications to
communications, economics, engineering, and ﬁnance. 2001.

[17] B. Liu and U. Hengartner. ptwitterrec: A privacy-preserving

personalized tweet recommendation framework. In Proceedings
of ASIA CCS, pages 365–376, 2014.

[18] A. Machanavajjhala, D. Kifer, J. Abowd, J. Gehrke, and

L. Vilhuber. Privacy: Theory meets practice on the map. In
ICDE, pages 277–286, 2008.

[19] F. McSherry and I. Mironov. Diﬀerentially private

recommender systems: Building privacy into the net. In KDD,
pages 627–636, 2009.

[20] N. Megiddo. Linear programming in linear time when the

dimension is ﬁxed. J. ACM, 31(1):114–127, Jan. 1984.

[21] V. Nikolaenko, S. Ioannidis, U. Weinsberg, M. Joye, N. Taft,

and D. Boneh. Privacy-preserving matrix factorization. In CCS,
pages 801–812, 2013.

[22] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity

and sampling in private data analysis. In STOC, pages 75–84,
2007.

[23] H. Polat and W. Du. Privacy-preserving collaborative ﬁltering

using randomized perturbation techniques. In ICDM, pages
625–628, 2003.

[24] Y. Shen and H. Jin. Privacy-preserving personalized

recommendation: An instance-based approach via diﬀerential
privacy. In ICDM, pages 540–549, 2014.

[25] P. Symeonidis, A. Nanopoulos, A. N. Papadopoulos, and

Y. Manolopoulos. Collaborative recommender systems:
Combining eﬀectiveness and eﬃciency. Expert Syst. Appl.,
34(4):2995–3013, 2008.

[26] J. Wang, N. Wang, and H. Jin. Context matters?: How adding

the obfuscation option aﬀects end users’ data disclosure
decisions. In IUI, pages 299–304, 2016.

[27] Y. Xin and T. Jaakkola. Controlling privacy in recommender

systems. In NIPS, pages 2618–2626. 2014.

[28] J. Xu, Z. Zhang, X. Xiao, Y. Yang, and G. Yu. Diﬀerentially

private histogram publication. In ICDE, pages 32–43, 2012.
[29] B. Zhang, N. Wang, and H. Jin. Privacy concerns in online
recommender systems: Inﬂuences of control and user data
input. In SOUPS, pages 159–173, 2014.

[30] S. Zhang, J. Ford, and F. Makedon. Deriving private

information from randomly perturbed ratings. In SDM, pages
59–69, 2006.

Local DatabaseC-1. Public Data InputC-2. User Private Data InputHistory on user’s device from various resourcesC-3. User Privacy Control InputImplementation of Data Perturbation Component C-4Recommendation Service Provider using collaborative filtering algorithm in GraphLabNetflix-like Recommendation Output to Client C-6EpicRecon Device for Movie RecommendationImplementation of Privacy Quantification Component C-5SSL/TLSSSL/TLSPrivacyBudget191