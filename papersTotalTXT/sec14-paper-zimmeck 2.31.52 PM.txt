Privee: An Architecture for Automatically 

Analyzing Web Privacy Policies

Sebastian Zimmeck and Steven M. Bellovin, Columbia University

https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/zimmeck

This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXPrivee: An Architecture for Automatically Analyzing Web Privacy Policies

Sebastian Zimmeck and Steven M. Bellovin

Department of Computer Science, Columbia University

{sebastian,smb}@cs.columbia.edu

Abstract
Privacy policies on websites are based on the notice-
and-choice principle. They notify Web users of their
privacy choices. However, many users do not read pri-
vacy policies or have difﬁculties understanding them.
In order to increase privacy transparency we propose
Privee—a software architecture for analyzing essential
policy terms based on crowdsourcing and automatic clas-
siﬁcation techniques. We implement Privee in a proof of
concept browser extension that retrieves policy analysis
results from an online privacy policy repository or, if
no such results are available, performs automatic clas-
siﬁcations. While our classiﬁers achieve an overall F-1
score of 90%, our experimental results suggest that clas-
siﬁer performance is inherently limited as it correlates
to the same variable to which human interpretations
correlate—the ambiguity of natural language. This ﬁnd-
ing might be interpreted to call the notice-and-choice
principle into question altogether. However, as our re-
sults further suggest that policy ambiguity decreases over
time, we believe that the principle is workable. Conse-
quently, we see Privee as a promising avenue for facilitat-
ing the notice-and-choice principle by accurately notify-
ing Web users of privacy practices and increasing privacy
transparency on the Web.

1 Introduction

Information privacy law in the U.S. and many other
countries is based on the free market notice-and-choice
principle [28].
Instead of statutory laws and regula-
tions, the privacy regime is of a contractual nature—the
provider of a Web service posts a privacy policy, which
a user accepts by using the site. In this sense, privacy
policies are fundamental building blocks of Web privacy.
The Federal Trade Commission (FTC) strictly enforces
companies’ violations of their promises in privacy poli-
cies. However, only few users read privacy policies and

those who do ﬁnd them oftentimes hard to understand
[58]. The resulting information asymmetry leaves users
uninformed about their privacy choices [58], can lead to
market failure [57], and ultimately casts doubt on the
notice-and-choice principle.

Various solutions were proposed to address the prob-
lem. However, none of them gained widespread accept-
ance—neither in the industry, nor among users. Most
prominently, The Platform for Privacy Preferences (P3P)
project [29, 32] was not widely adopted, mainly, be-
cause of a lack of incentive on part of the industry to ex-
press their policies in P3P format. In addition, P3P was
also criticized for not having enough expressive power
to describe privacy practices accurately and completely
[28, 11]. Further, existing crowdsourcing solutions, such
as Terms of Service; Didn’t Read (ToS;DR) [5], may not
scale well and still need to gain more popularity.
In-
formed by these experiences, which we address in more
detail in Section 2, we present Privee—a novel software
architecture for analyzing Web privacy policies. In par-
ticular, our contributions are:

• the Privee concept that combines rule and machine
learning (ML) classiﬁcation with privacy policy
crowdsourcing for seamless integration into the ex-
isting privacy regime on the Web (Section 3);

• an implementation of Privee in a Google Chrome
browser extension that interacts with privacy pol-
icy websites and the ToS;DR repository of crowd-
sourced privacy policy results (Section 4);

• a statistical analysis of our experimental results
showing that
the ambiguity of privacy policies
makes them inherently difﬁcult to understand for
both humans and automatic classiﬁers (Section 5);
• pointers for further research on notice-and-choice
and adaptations that extend Privee as the landscape
of privacy policy analysis changes and develops
(Section 6).

USENIX Association  

23rd USENIX Security Symposium  1

2 Related Work

While only few previous works are directly applicable,
our study is informed by four areas of previous research:
privacy policy languages (Section 2.1), legal information
extraction (Section 2.2), privacy policy crowdsourcing
(Section 2.3), and usable privacy (Section 2.4).

2.1 Privacy Policy Languages
Initial work on automatic privacy policy analysis focused
on making privacy policies machine-readable. That way
a browser or other user agent could read the policies
and alert the user of good and bad privacy practices.
Reidenberg [67] suggested early on that Web services
should represent their policies in the Platform for Inter-
net Content Selection (PICS) format [10]. This and sim-
ilar suggestions lead to the development of P3P [29, 32],
which provided a machine-readable language for spec-
ifying privacy policies and displaying their content to
users [33]. To that end, the designers of P3P imple-
mented various end users tools, such as Privacy Bird
[30], a browser extension for Microsoft’s Internet Ex-
plorer that notiﬁes users of the privacy practices of a Web
service whose site they visit, and Privacy Bird Search
[24], a P3P-enabled search engine that returns privacy
policy information alongside search results.

The development of P3P was complemented by var-
ious other languages and tools. Of particular relevance
was A P3P Preference Privacy Exchange Language (AP-
PEL) [31], which enabled users to express their privacy
preferences vis-`a-vis Web services. APPEL was further
extended in the XPath project [14] and inspired the User
Privacy Policy (UPP) language [15] for use in social net-
works. For industry use, the Platform for Enterprise Pri-
vacy Practices (E-P3P) [47] was developed allowing ser-
vice providers to formulate, supervise, and enforce pri-
vacy policies. Similar languages and frameworks are the
Enterprise Privacy Authorization Language (EPAL) [18],
the SPARCLE Policy Workbench [22, 23], Jeeves [78],
and XACML [12]. However, despite all efforts the adop-
tion rate of P3P policies among Web services remained
low [11], and the P3P working group was closed in 2006
due to lack of industry participation [28].

Instead of creating new machine-readable privacy pol-
icy formats we believe that it is more effective to use
what is already there—privacy policies in natural lan-
guage. The reasons are threefold: First, natural language
is the de-facto standard for privacy policies on the Web,
and the P3P experience shows that there is currently no
industry-incentive to move to a different standard. Sec-
ond, U.S. governmental agencies are in strong support of
the natural language format. In particular, the FTC, the
main privacy regulator, called for more industry-efforts

to increase policy standardization and comprehensibil-
ity [38]. Another agency, the National Science Founda-
tion, awarded $3.75 million to the Usable Privacy Policy
Project [9] to explore possibilities of automatic policy
analysis. Third, natural language has stronger expressive
power compared to a privacy policy language. It allows
for industry-speciﬁc formulation of privacy practices and
accounts for the changing legal landscape over time.

2.2 Legal Information Extraction
Given our decision to make use of natural language poli-
cies, the question becomes how salient information can
be extracted from unordered policy texts. While most
works in legal information extraction relate to domains
other than privacy, they still provide some guidance. For
example, Westerhout et al. [75, 76] had success in com-
bining a rule-based classiﬁer with an ML classiﬁer to
identify legal deﬁnitions.
In another line of work de
Maat et al.
[35, 36] aimed at distinguishing statutory
provisions according to types (such as procedural rules
or appendices) and patterns (such as deﬁnitions, rights,
or penal provisions). They concluded that it was unnec-
essary to employ something more complex than a simple
pattern recognizer [35, 36]. Other tasks focused on the
extraction of information from statutory and regulatory
laws [21, 20], the detection of legal arguments [59], or
the identiﬁcation of case law sections [54, 71].

To our knowledge, the only works in the privacy pol-
[16], Costante
icy domain are those by Ammar et al.
et al. [26, 27], and Stamey and Rossi [70]. As part of
the Usable Privacy Policy Project [9] Ammar et al. pre-
sented a pilot study [16] with a narrow focus on clas-
sifying provisions for the disclosure of information to
law enforcement ofﬁcials and users’ rights to terminate
their accounts. They concluded the feasibility of natural
language analysis in the privacy policy domain in gen-
eral. In their ﬁrst work [26] Costante et al. used gen-
eral natural language processing libraries to evaluate the
suitability of rule-based identiﬁcation of different types
of user information that Web services collect. Their re-
sults are promising and indicate the feasibility of rule-
based classiﬁers. In a second work [27] Costante et al.
selected an ML approach for assessing whether privacy
policies cover certain subject matters. Finally, Stamey
and Rossi [70] provided a program for identifying am-
biguous words in privacy policies.

The discussed works [16, 26, 27, 70] conﬁrm the suit-
ability of rule and ML classiﬁers in the privacy policy do-
main. However, neither provides a comprehensive con-
cept, nor addresses, for example, how to process the poli-
cies or how to make use of crowdsourcing results. The
latter point is especially important because, as shown in
Section 5, automatic policy classiﬁcation on its own is in-

2  23rd USENIX Security Symposium 

USENIX Association

                    Public Results

Crowdsourcing Repository
y

Crowd Submissions

I. Crowd-
-
sourcing
Analysis

II. Classifier
II.II.
AnaAna
Analysis

Policy Storage
e

Policy Websites

Policy Authors

Figure 1: Privee overview. When a user requests a privacy policy analysis, the program checks whether the analysis results are available at
a crowdsourcing repository (to which crowd contributors can submit analysis results of policies). If results are available, they are returned
and displayed to the user (I. Crowdsourcing Analysis). If no results are available, the policy text is fetched from the policy website, analyzed
by automatic classiﬁers on the client machine, and then the analysis results are displayed to the user (II. Classiﬁer Analysis).

herently limited. In addition, as the previous works’ pur-
pose is to generally show the viability of natural language
privacy policy analysis, they are constrained to classify-
ing one or two individual policy terms or features. As
they process each classiﬁcation task separately, there was
also no need to address questions of handling multiple
classiﬁers or discriminating which extracted features be-
long to which classiﬁcation task. Because of their limited
scope none of the previous works relieves the user from
actually reading the analyzed policy. In contrast, it is our
goal to provide users with a privacy policy summary in
lieu of the full policy. We want to condense a policy into
essential terms, make it more comprehensible, provide
guidance on the analyzed practices, and give an overall
evaluation of its privacy level.

2.3 Privacy Policy Crowdsourcing
There are various crowdsourcing repositories where
crowd contributors evaluate the content of privacy poli-
cies and submit their results into a centralized collection
for publication on the Web. Sometimes policies are also
graded. Among those repositories are ToS;DR [5], priva-
cychoice [4], TOSBack [7], and TOSBack2 [8]. Crowd-
sourcing has the advantage that it combines the knowl-
edge of a large number of contributors, which, in prin-
ciple, can lead to much more nuanced interpretations of
ambiguous policy provisions than current automatic clas-
siﬁers could provide. However, all crowdsourcing ap-
proaches suffer from a lack of participation and, conse-
quently, do not scale well. While the analysis results of
the most popular websites may be available, those for
many lesser known sites are not. In addition, some repos-
itories only provide the possibility to look up the results
on the Web without offering convenient user access, for
example, by means of a browser extension or other soft-
ware.

2.4 Usable Privacy
Whether the analysis of a privacy policy is based on
crowdsourcing or automatic classiﬁcations, in order to
notify users of the applicable privacy practices it is not
enough to analyze policy content, but rather the results
must also be presented in a comprehensible, preferably,
standardized format [60]. In this sense, usable privacy
is orthogonal to the other related areas: no matter how
the policies are analyzed, a concise, user-friendly no-
tiﬁcation is always desirable. In particular, privacy la-
bels may help to succinctly display privacy practices
[48, 49, 51, 65, 66]. Also, privacy icons, such as those
proposed by PrimeLife [39, 45], KnowPrivacy [11], and
the Privacy Icons project [3], can provide visual clues to
users. However, care must be taken that the meaning of
the icons is clear to the users [45]. In any case, it should
be noted that while usability is an important element of
the Privee concept, we have not done a usability study
for our Privee extension as it is just a proof of concept.

3 The Privee Concept

Figure 1 shows a conceptual overview of Privee. Privee
makes use of automatic classiﬁers and complements
them with privacy policy crowdsourcing. It integrates all
components of the current Web privacy ecosystem. Pol-
icy authors write their policies in natural language and
do not need to adopt any special machine-readable pol-
icy format. While authors certainly can express the same
semantics as with P3P, which we demonstrate in Section
4.6.2, they can also go beyond and use their language
much more freely and naturally.

When a user wants to analyze a privacy policy, Privee
leverages the discriminative power of crowdsourcing. As
we will see in Section 5 that classiﬁers and human inter-
pretations are inherently limited by ambiguous language,

USENIX Association  

23rd USENIX Security Symposium  3

it is especially important to resolve those ambiguities by
providing a forum for discussion and developing con-
sensus among many crowd contributors. Further, Privee
complements the crowdsourcing analysis with the ubiq-
uitous applicability of rule and ML classiﬁers for policies
that are not yet analyzed by the crowd. Because the com-
putational requirements are low, as shown in Section 5.3,
a real time analysis is possible.

As the P3P experience showed [28] that a large frac-
tion of Web services with P3P policies misrepresented
their privacy practices, presumably in order to prevent
user agents from blocking their cookies, any privacy pol-
icy analysis software must be guarded against manipu-
lation. However, natural language approaches, such as
Privee, have an advantage over P3P and other machine-
readable languages. Because it is not clear whether P3P
policies are legally binding [69] and the FTC never took
action to enforce them [55], the misrepresentation of pri-
vacy practices in those policies is a minor risk that many
Web services are willing to take. This is true for other
machine-readable policy solutions as well. In contrast,
natural language policies can be valid contracts [1] and
are subject to the FTC’s enforcement actions against un-
fair or deceptive acts or practices (15 U.S.C. §45(a)(1)).
Thus, we believe that Web services are more likely to
ensure that their natural language policies represent their
practices accurately.

Given that natural language policies attempt to truly
reﬂect privacy practices, it is important that the policy
text is captured completely and without additional text,
in particular, free from advertisements on the policy web-
site. Further, while it is true that an ill-intentioned pri-
vacy policy author might try to deliberately use ambigu-
ous language to trick the classiﬁer analysis, this strat-
egy can only go so far as ambiguous contract terms are
interpreted against the author (Restatement (Second) of
Contracts, §206) and might also cause the FTC to chal-
lenge them as unfair or deceptive. Beyond safeguarding
the classiﬁer analysis, it is also important to prevent the
manipulation of the crowdsourcing analysis. In this re-
gard, the literature on identifying fake reviews should be
brought to bear. For example, Wu et al.
[77] showed
that fake reviews can be identiﬁed by a suspicious grade
distribution and their posting time following negative re-
views. In order to ensure that the crowdsourcing analy-
sis returns the latest results the crowdsourcing repository
should also keep track of privacy policy updates.

4 The Privee Browser Extension

We implemented Privee as a proof of concept browser
extension for Google Chrome (version 35.0.1916.153).
Figure 2 shows a simpliﬁed overview of the program
ﬂow. We wrote our Privee extension in JavaScript using

User

Web Scraper

ToS;DR

Results

Available?

no

Crowd-
sourcing

Preprocessor

Rule Classiﬁer

and
ML

Preprocessor

example.com

Training Policies

yes

Training
Done?

no

Trainer

yes

ML Classiﬁer

Labeler

Label

Figure 2: Simpliﬁed program ﬂow. After the user has started
the extension, the Web scraper obtains the text of the privacy
policy to be analyzed (example.com) as well as the current URL
(http://example.com/). The crowdsourcing preprocessor then ex-
tracts from the URL the ToS;DR identiﬁer and checks the ToS;DR
repository for results. If results are available, they are retrieved
and forwarded to the labeler, which converts them to a label for
display to the user. However, if no results are available on ToS;DR
the policy text is analyzed. First, the rule classiﬁer attempts a rule-
based classiﬁcation. However, if that is not possible the ML prepro-
cessor prepares the ML classiﬁcation. It checks if the ML classiﬁer
is already trained. If that is the case, the policy is classiﬁed by the
ML classiﬁer, assigned a label according to the classiﬁcations, and
the results are displayed to the user. Otherwise, a set of training
policies is analyzed by the trainer ﬁrst and the program proceeds
to the ML classiﬁer and labeler afterwards. The set of training
policies is included in the extension package and only needs to be
analyzed for the ﬁrst run of the ML classiﬁer. Thereafter, the train-
ing results are kept in persistent storage until deletion by the user.

4  23rd USENIX Security Symposium 

USENIX Association

the jQuery library and Ajax functions for client-server
communication. While we designed our extension as an
end user tool, it can also be used for scientiﬁc or indus-
trial research, for example, in order to easily compare
different privacy policies to each other. In this Section
we describe the various stages of program execution.

4.1 Web Scraper
The user starts the Privee extension by clicking on its
icon in the Chrome toolbar. Then, the Web scraper ob-
tains the text of the privacy policy that the user wants
to analyze and retrieves the URL of the user’s current
website. While the rule and ML classiﬁer analysis only
works from the site that contains the policy to be ana-
lyzed, the crowdsourcing analysis works on any website
whose URL contains the policy’s ToS;DR identiﬁer.

4.2 Crowdsourcing Preprocessor
The crowdsourcing preprocessor is responsible for man-
aging the interaction with the ToS;DR repository. It re-
ceives the current URL from the Web scraper from which
it extracts the ToS;Dr identiﬁer. It then connects to the
API of ToS;DR and checks for the availability of anal-
ysis results, that is, short descriptions of privacy prac-
tices and sometimes an overall letter grade. The results,
if any, are forwarded to the labeler and displayed to the
user. Then the extension terminates. Otherwise, the pol-
icy text, which the crowdsourcing preprocessor also re-
ceived from the Web scraper, is forwarded to the rule
classiﬁer and ML preprocessor.

4.3 Rule Classiﬁer and ML Preprocessor
Generally, classiﬁers can be based on rule or ML algo-
rithms.
In our preliminary experiments we found that
for some classiﬁcation categories a rule classiﬁer worked
better, in others an ML classiﬁer, and in others again a
combination of both [71, 76]. We will discuss our classi-
ﬁer selection in Section 5.1 in more detail. In this Section
we will focus on the feature selection process for our rule
classiﬁer and ML preprocessor. Both rule classiﬁcation
and ML preprocessing are based on feature selection by
means of regular expressions.

Our preliminary experiments revealed that classiﬁca-
tion performance depends strongly on feature selection.
Ammar et al. [16] discuss a similar ﬁnding. Compara-
ble to other domains [76], feature selection is particularly
useful in our case for avoiding misclassiﬁcations due to
the heavily imbalanced structure of privacy policies. For
example, in many multi-page privacy policies there is
often only one phrase that determines whether the Web
service is allowed to combine the collected information

with information from third parties to create personal
proﬁles of users. Especially, supervised ML classiﬁers
do not work well in such cases, even with undersam-
pling (removal of uninteresting examples) or oversam-
pling (duplication of interesting examples) [52]. Possi-
ble solutions to the problem are the separation of poli-
cies into different content zones and applying a classiﬁer
only to relevant content zones [54] or—the approach we
adopted—running a classiﬁer only on carefully selected
features.

Our extension’s feature selection process begins with
the removal of all characters from the policy text that are
not letters or whitespace and conversion of all remaining
characters to lower case. However, the positions of re-
moved punctuations are preserved because, as noted by
Biagoli et al. [19], a correct analysis of the meaning of
legal documents often depends on the position of punctu-
ation. In order to identify the features that are most char-
acteristic for a certain class we used the term frequency-
inverse document frequency (tf-idf) statistic as a proxy.
The tf-idf statistic measures how concentrated into rel-
atively few documents the occurrences of a given word
are in a document corpus [64]. Thus, words with high tf-
idf values correlate strongly with the documents in which
they appear and can be used to identify topics in that doc-
ument that are not discussed in other documents. How-
ever, instead of using individual words as features we
observed that the use of bigrams lead to better classiﬁca-
tion performance, which was also discussed in previous
works [16, 59].

(ad|advertis.*) (compan.*|network.*|provider.*|
servin.*|serve.*|vendor.*)|(behav.*|context.*|
network.*|parti.*|serv.*) (ad|advertis.*)

Listing 1: Simpliﬁed pseudocode of the regular expression to
identify whether a policy allows advertising tracking. For example,
the regular expression would match “contextual advertising.”

The method by which our Privee extension selects
characteristic bigrams, which usually consist of two
words, but can also consist of a word and a punctua-
tion mark, is based on regular expressions. It applies a
three-step process that encompasses both rule classiﬁca-
tion and ML preprocessing. To give an example, for the
question whether the policy allows advertising tracking
(e.g., by ad cookies) the ﬁrst step consists of trying to
match the regular expression in Listing 1, which identi-
ﬁes bigrams that nearly always indicate that advertising
tracking is allowed. If any bigram in the policy matches,
no further analysis happens, and the policy is classiﬁed
by the rule classiﬁer as allowing advertising tracking. If
the regular expression does not match, the second step
attempts to extract further features that can be associated
with advertising tracking (which are, however, more gen-

USENIX Association  

23rd USENIX Security Symposium  5

eral than the previous ones). Listing 2 shows the regular
expression used for the second step.

the training results successively to the client’s Web stor-
age. After all results are added the ML classiﬁer is ready
for classiﬁcation.

(ad|advertis|market) (.+)|(.+) (ad|advertis|
market)

Listing 2: Simpliﬁed pseudocode of the regular expression to
extract relevant phrases for advertising tracking. For example, the
regular expression would match “no advertising.”

The second step—the ML preprocessing—is of par-
ticular importance for our analysis because it prepares
classiﬁcation of the most difﬁcult cases. It extracts the
features on which the ML classiﬁer will run later. To that
end, it ﬁrst uses the Porter stemmer [63] to reduce words
to their morphological root [19]. Such stemming has the
effect that words with common semantics are clustered
together [41]. For example, “collection,” “collected,”
and “collect” are all stemmed into “collect.” As a side
note, while stemming had some impact, we did not ﬁnd
a substantial performance increase for running the ML
classiﬁer on stemmed features compared to unstemmed
features. In the third step, if no features were extracted
in the two previous steps, the policy is classiﬁed as not
allowing advertising tracking.

4.4 Trainer
In the training stage our Privee extension checks whether
the ML classiﬁer is already trained.
If that is not the
case, a corpus of training policies is preprocessed and
analyzed. The analysis of a training policy is similar
to the analysis of a user-selected policy, except that the
extension does not check for crowdsourcing results and
only applies the second and third step of the rule classi-
ﬁer and ML preprocessor phase. The trainer’s purpose is
to gather statistical information about the features in the
training corpus in order to prepare the classiﬁcation of
the user-selected policy. It stores the training results lo-
cally in the user’s browser memory using persistent Web
storage, which is, in principle, similar to cookie storage.

4.5 Training Data
The training policies are held in a database that is in-
cluded in the extension package. The database holds a
total of 100 training policies. In order to obtain a repre-
sentative cross section of training policies, we selected
the majority of our policies randomly from the Alexa
top 500 websites for the U.S. [6] across various domains
(banking, car rental, social networking, etc.). However,
we also included a few random policies from lesser fre-
quented U.S. sites and sites from other countries that
published privacy policies in English. The trainer ac-
cesses these training policies one after another and adds

4.6 ML Classiﬁer
We now describe the ML classiﬁer design (Section 4.6.1)
and the classiﬁcation categories (Section 4.6.2).

4.6.1 ML Classiﬁer Design

In order to test the suitability of different ML algorithms
for analyzing privacy policies we performed preliminary
experiments using the Weka library [43]. Performance
for the different algorithms varied. We tested all algo-
rithms available on Weka, among others the Sequential
Minimal Optimization (SMO) algorithm with different
kernels (linear, polynomial, radial basis function), ran-
dom forest, J48 (C4.5), IBk nearest neighbor, and various
Bayesian algorithms (Bernoulli naive Bayes, multino-
mial naive Bayes, Bayes Net). Surprisingly, the Bayesian
algorithms were among the best performers. Therefore,
we implemented naive Bayes in its Bernoulli and multi-
nomial version. Because the multinomial version ulti-
mately proved to have better performance, we settled on
this algorithm.

As Manning et al. [56] observed, naive Bayes clas-
siﬁers have good accuracy for many tasks and are very
efﬁcient, especially, for high-dimensional vectors, and
they have the advantage that training and classiﬁcation
can be accomplished with one pass over the data. Our
naive Bayes implementation is based on their speciﬁca-
tion [56]. In general, naive Bayes classiﬁers make use of
Bayes’ theorem. The probability, P, of a document, d,
being in a category, c, is

P(c|d) ∝ P(c) ∏
1≤k≤nd

P(tk|c),

(1)

where P(c) is the prior probability of a document occur-
ring in category c, nd is the number of terms in d that
are used for the classiﬁcation decision, and P(tk|c) is the
conditional probability of term tk occurring in a docu-
ment of category c [56]. In other words, P(tk|c) is inter-
preted as a measure of how much evidence tk contributes
for c being the correct category [56]. The best category
to select for a document in a naive Bayes classiﬁcation is
the category for which it holds that

argmax

c∈C

ˆP(c|d) = argmax
c∈C

ˆP(c) ∏
1≤k≤nd

ˆP(tk|c),

(2)

where C is a set of categories, which, in our case, is al-
ways of size two (e.g., {ad tracking, no ad tracking}).

6  23rd USENIX Security Symposium 

USENIX Association

The naive assumption is that the probabilities of indi-
vidual terms within a document are independent of each
other given the category [41]. However, our implemen-
tation differs from the standard implementation and tries
to alleviate the independence assumption. Instead of pro-
cessing individual words of the policies we try to capture
some context by processing bigrams.

Analyzing the content of a privacy policy requires
multiple classiﬁcation decisions. For example, the clas-
siﬁer has to decide whether personal information can
be collected, disclosed to advertisers, retained indeﬁ-
nitely, and so on. This type of classiﬁcation is known
as multi-label classiﬁcation because each analyzed doc-
ument can receive more than one label. One commonly
used approach for multi-label classiﬁcation with L la-
bels consists of dividing the task into |L| binary clas-
siﬁcation tasks [74]. However, other solutions handle
multi-label data directly by extending speciﬁc learning
algorithms [74]. We found it simpler to implement the
ﬁrst approach. Speciﬁcally, at execution time we create
multiple classiﬁer instances—one for each classiﬁcation
category—by running the classiﬁer on category-speciﬁc
features extracted by the ML preprocessor.

4.6.2 Classiﬁcation Categories

For which types of information should privacy policies
actually be analyzed? In answering this question, one
starting point are fair information practices [25]. An-
other one are the policies themselves. After all, while
it is true that privacy law in the U.S. generally does not
require policies to have a particular content, it can be ob-
served that all policies conventionally touch upon four
different themes: information collection, disclosure, use,
and management (management refers to the handling
of information, for example, whether information is en-
crypted). The four themes can be analyzed on different
levels of abstraction. For example, for disclosure of in-
formation, it could simply be analyzed whether informa-
tion is disclosed to outside parties in general, or it could
be investigated more speciﬁcally whether information is
disclosed to service providers, advertisers, governmental
agencies, credit bureaus, and so on.

At this point it should be noted that not all information
needs to be analyzed. In some instances privacy policies
simply repeat mandatory law without creating any new
rights or obligations. For example, a federal statute in the
U.S.—18 U.S.C. §2703(c)(1)(A) and (B)—provides that
the government can demand the disclosure of customer
information from a Web service provider after obtain-
ing a warrant or suitable court order. As this law applies
independently of a privacy policy containing an explicit
statement to that end, the provision that the provider will
disclose information to a governmental entity under the

requirements of the law can be inferred from the law it-
self. In fact, even if a privacy policy states to the contrary,
it should be assumed that such information disclosure
will occur. Furthermore, if privacy policies stay silent
on certain subject matters, default rules might apply and
ﬁll the gaps.

Another good indicator of what information should be
classiﬁed is provided by user studies. According to one
study [30], knowing about sharing, use, and purpose of
information collection is very important to 79%, 75%,
and 74% of users, respectively. Similarly, in another
study [11] users showed concern for the types of personal
information collected, how personal information is col-
lected, behavioral proﬁling, and the purposes for which
the information may be used. While it was only an is-
sue of minor interest earlier [30], the question how long
a company keeps personal information about its users is
a topic of increasing importance [11]. Based on these
ﬁndings, we decided to perform six different binary clas-
siﬁcations, that is, whether or not a policy

• allows collection of personal

users (Collection);

information from

• provides encryption for information storage or

transmission (Encryption);

• allows ad tracking by means of ad cookies or other

trackers (Ad Tracking);

• restricts archiving of personal information to a lim-

ited time period (Limited Retention);

• allows the aggregation of information collected
from users with information from third parties (Pro-
ﬁling);

• allows disclosure of personal information to adver-

tisers (Ad Disclosure).

For purposes of our analysis, where applicable, it is
assumed that the user has an account with the Web ser-
vice whose policy is analyzed and is participating in any
offered sweepstakes or the like. Thus, for example, if a
policy states that the service provider only collects per-
sonal information from registered users, the policy is an-
alyzed from the perspective of a registered user. Also,
if certain actions are dependent on the user’s consent,
opt-in, or opt-out, it is assumed that the user consented,
opted in, or did not opt out, respectively. As it was our
goal to make the analysis results intuitively comprehen-
sible to casual users, which needs to be conﬁrmed by
user studies, we tried to avoid technical terms. In partic-
ular, the term “personal information” is identical to what
is known in the privacy community as personally identi-
ﬁable information (PII) (while “information” on its own
also encompasses non-PII, e.g., user agent information).

USENIX Association  

23rd USENIX Security Symposium  7

Figure 3: Privee extension screenshot and detailed label view. The result of the privacy policy analysis is shown to the user in a pop-up.

It is noteworthy that some of the analyzed criteria cor-
respond to the semantics of the P3P Compact Speciﬁca-
tion [2]. For example, the P3P token NOI indicates that
a Web service does not collect identiﬁed data while ALL
means that it has access to all identiﬁed data. Thus, NOI
and ALL correspond to our collection category. Also,
in P3P the token IND means that information is retained
for an indeterminate period of time, and, consequently,
is equivalently expressed when our classiﬁer comes to
the conclusion that no limited retention exists. Further,
PSA, PSD, IVA, and IVD are tokens similar to our pro-
ﬁling category. Generally, the correspondence between
the semantics of the P3P tokens and our categories sug-
gests that it is possible to automatically classify natural
language privacy policies to obtain the same information
that Web services would include in P3P policies without
actually requiring them to have such.

4.7 Labeler
Our extension’s labeler is responsible for creating an out-
put label. As it was shown that users casually familiar
with privacy questions were able to understand privacy
policies faster and more accurately when those policies
were presented in a standardized format [49] and that
most users had a preference for standardized labels over
full policy texts [49, 50], we created a short standard-
ized label format. Generally, a label can be structured in
one or multiple dimensions. The multidimensional ap-
proach has the advantage that it can succinctly display
different privacy practices for different types of informa-
tion. However, we chose a one-dimensional format as
such were shown to be substantially more comprehensi-
ble [51, 66].

In addition to the descriptions for the classiﬁcations,
the labeler also labels each policy with an overall let-
ter grade, which depends on the classiﬁcations. More
speciﬁcally, the grade is determined by the number of
points, p, a policy is assigned. For collection, proﬁling,

ad tracking, and ad disclosure a policy receives one mi-
nus point, respectively. However, for not allowing one
of these practices a policy receives one plus point. How-
ever, a policy receives a plus point for featuring limited
retention or encryption, respectively. As most policies in
the training set had zero points, we took zero points as a
mean and assigned grades as follows:

• A (above average overall privacy) if p > 1;
• B (average overall privacy) if 1 ≤ p ≥ −1;
• C (below average overall privacy) if p < −1.
After the points are assigned to a policy, the corre-
sponding label is displayed to the user as shown in Figure
3. As we intended to avoid confusion about the meaning
of icons [45], we used short descriptions instead. The
text in the pop-up is animated.
If the user moves the
mouse over it, further information is provided. The user
can also ﬁnd more detailed explanations about the cat-
egories and the grading by clicking on the blue ”Learn
More” link at the bottom of the label. It should be noted
that analysis results retrieved from ToS;DR usually differ
in content from our classiﬁcation results, and are, conse-
quently, displayed in a different label format.

5 Experimental Results

For our experiments we ran our Privee extension on a
test set of 50 policies. Before this test phase we trained
the ML classiﬁer (with the 100 training policies that are
included in the extension package) and tuned it (with a
validation set of 50 policies). During the training, valida-
tion, and test phases we disabled the retrieval of crowd-
sourcing results. Consequently, our experimental results
only refer to rule and ML classiﬁcation. The policies of
the test and validation sets were selected according to the
same criteria as described for the training set in Section

8  23rd USENIX Security Symposium 

USENIX Association

Overall
Collection
Encryption
Ad Tracking
L. Retention
Proﬁling
Ad Disclosure

Prec. Rec.

F-1
Base. Acc.
68% 84% 94% 89% 90%
100% 100% 100% 100% 100%
52% 98% 96% 100% 98%
64% 96% 94% 100% 97%
74% 90% 83% 77% 80%
52% 86% 100% 71% 83%
66% 76% 69% 53% 60%

Table 1: Privee extension performance overall and per category.
For the 300 test classiﬁcations (six classiﬁcations for each of the 50
test policies) we observed 27 misclassiﬁcations. 154 classiﬁcations
were made by the rule classiﬁer and 146 by the ML classiﬁer. The
rule classiﬁer had 11 misclassiﬁcations (2 false positives and 9 false
negatives) and the ML classiﬁer had 16 misclassiﬁcations (7 false
positives and 9 false negatives). It may be possible to decrease the
number of false negatives by adding more rules and training ex-
amples. For the ad tracking category the rule classiﬁer had an F-1
score of 98% and the ML classiﬁer had an F-1 score of 94%. For
the proﬁling category the rule classiﬁer had an F-1 score of 100%
and the ML classiﬁer had an F-1 score of 53%. 28% of the policies
received a grade of A, 50% a B, and 22% a C.

4.5. In this Section we ﬁrst discuss the classiﬁcation per-
formance (Section 5.1), then the gold standard that we
used to measure the performance (Section 5.2), and ﬁ-
nally the computational performance (Section 5.3).

5.1 Classiﬁcation Performance
In the validation phase we experimented with different
classiﬁer conﬁgurations for each of our six classiﬁcation
tasks. For the ad tracking and proﬁling categories the
combination of the rule and ML classiﬁer lead to the best
results. However, for collection, limited retention, and ad
disclosure the ML classiﬁer on its own was preferable.
Conversely, for the encryption category the rule classiﬁer
on its own was the best. It seems that the language used
for describing encryption practices is often very speciﬁc
making the rule classiﬁer the ﬁrst choice. Words such as
“ssl” are very distinctive identiﬁers for encryption pro-
visions. Other categories use more general language that
could be used in many contexts. For example, phrases re-
lated to time periods must not necessarily refer to limited
retention. For those instances the ML classiﬁer seems to
perform better. However, if categories exhibit both spe-
ciﬁc and general language the combination of the rule
and ML classiﬁer is preferable.

The results of our extension’s privacy policy analysis
are based on the processing of natural language. How-
ever, as natural language is often subject to different in-
terpretations, the question becomes how the results can
be veriﬁed in a meaningful way. Commonly applied met-
rics for verifying natural language classiﬁcation tasks are
accuracy (Acc.), precision (Prec.), recall (Rec.), and F-1

100%
98%

Collection

Encryption

Ad Tracking

Test
Training

Test
Training

Test
Training

48%

44%

64%
67%

L. Retention

Test
Training

26%
29%

Proﬁling

Test
Training

48%
46%

Ad Disclosure

Test
Train.

34%

23%

0%

25%

50%

75%

100%

Figure 4: Annotation of positive cases in percent for the 50 test
policies (blue) and the 100 training policies (white).

score (F-1). Accuracy is the fraction of classiﬁcations
that are correct [56]. Precision is the fraction of retrieved
documents that are relevant, and recall is the fraction of
relevant documents that are retrieved [56]. Precision and
recall are often combined in their harmonic mean, known
as the F-1 score [46].

In order to analyze our extension’s performance we
calculated the accuracy, precision, recall, and F-1 score
for the test policy set classiﬁcations. Table 1 shows the
overall performance and the performance for each clas-
siﬁcation category. We also calculated the baseline accu-
racy (Base.) for comparison against the actual accuracy.
The baseline accuracy for each category was determined
by always selecting the classiﬁcation corresponding to
the annotation that occurred the most in the training set
annotations, which we report in Figure 4. The baseline
accuracy for the overall performance is the mean of the
category baseline accuracies. Because the classiﬁcation
of privacy policies is a multi-label classiﬁcation task, as
described in Section 4.6.1, we calculated the overall re-
sults based on the method for measuring multi-label clas-
siﬁcations given by Godbole and Sarawagi [42]. Accord-
ing to their method, for each document, d j in set D, let
t j be the true set of labels and s j be the predicted set of
labels. Then we obtain the means by

,

Acc(D) =

Prec(D) =

Rec(D) =

1
|D|
1
|D|
1
|D|

i=1 |t j ∩ s j|
∑|D|
|t j ∪ s j|
i=1 |t j ∩ s j|
∑|D|
|s j|
i=1 |t j ∩ s j|
∑|D|
|t j|

,

,

(3)

(4)

(5)

USENIX Association  

23rd USENIX Security Symposium  9

F-1(D) =

1
|D|

∑|D|

i=1

2 Prec(d j) Rec(d j)
(Prec(d j) + Rec(d j))

.

(6)

From Table 1 it can be observed that the accuracies
are at least as good as the corresponding baseline accu-
racies. For example, in the case of limited retention the
baseline classiﬁes all policies as not providing for limited
retention because, as show in Figure 4, only 29% of the
training policies were annotated as having a limited re-
tention period, which would lead to a less accurate clas-
siﬁcation of 74% in the test set compared to the actual
accuracy of 90%. For the collection category it should
be noted that there is a strong bias because nearly ev-
ery policy allows the collection of personal information.
However, in our validation set we had two policies that
did not allow this practice, but still were correctly clas-
siﬁed by our extension. Generally, our F-1 performance
results fall squarely within the range reported in the ear-
lier works. For identifying law enforcement disclosures
Ammar et Al. [16] achieved an F-1 score of 76% and
Costante et al. reported a score of 83% for recognizing
types of collected information [26] and 92% for identi-
fying topics discussed in privacy policies [27].

In order to investigate the reasons behind our exten-
sion’s performance we used two binary logistic regres-
sion models. Binary logistic regression is a statistical
method for evaluating the dependence of a binary vari-
able (the dependent variable) on one or more other vari-
ables (the independent variable(s)).
In our ﬁrst model
each of the 50 test policies was represented by one data
point with the dependent variable identifying whether it
had any misclassiﬁcation and the independent variables
identifying (1) the policy’s length in words, (2) its mean
Semantic Diversity (SemD) value [44], and (3) whether
there was any disagreement among the annotators in an-
notating the policy (Disag.).
In our second model we
represented each of 185 individual test classiﬁcations by
one data point with the dependent variable identifying
whether it was a misclassiﬁcation and the independent
variables identifying (1) the length (in words) of the text
that the rule classiﬁer or ML preprocessor extracted for
the classiﬁcation, (2) the text’s mean SemD value, and
(3) whether there was annotator disagreement on the an-
notation corresponding to the classiﬁcation.

Hoffman et al.’s [44] SemD value is an ambiguity mea-
sure for words based on latent semantic analysis, that is,
the similarity of contexts in which words are used.
It
can range from 0 (highly unambiguous) to 2.5 (highly
ambiguous). We represented the semantic diversity of a
document (i.e., a policy or extracted text) by the mean
SemD value of its words. However, as Hoffman et al.
only provide SemD values for words on which they had
sufﬁcient analytical data (31,739 different words in to-
tal), some words could not be taken into account for cal-
culating a document’s mean SemD value. Thus, in order

to avoid skewing of mean SemD values in our models,
we only considered documents that had SemD values for
at least 80% of their words. In our ﬁrst model all test
policies were above this threshold. However, in our sec-
ond model we excluded some of the 300 classiﬁcations.
Particularly, all encryption classiﬁcations were excluded
because words, such as “encryption” and “ssl” occurred
often and had no SemD value. Also, in the second model
the mean SemD value of an extracted text was calculated
after stemming its words with the Porter stemmer and
obtaining the SemD values for the resulting word stems
(while the SemD value of each word stem was calculated
from the mean SemD value of all words that have the re-
spective word stem).

Per Policy
Mean
Signiﬁcance (P)
Odds Ratio (Z)
95% Conﬁdence
Interval (Z)

Length
2873.4
0.64
1.15
0.64-
2.08

SemD
2.08
0.74
1.11
0.61-
2.01

Disag.

0.6
0.34
0.54
0.16-
1.89

Table 2: Results of the ﬁrst logistic regression model. The Nagelk-
erke pseudo R2 is 0.03 and the Hosmer and Lemeshow value 0.13.

Per Extr. Text
Mean
Signiﬁcance (P)
Odds Ratio (Z)
95% Conﬁdence
Interval (Z)

Length
37.38
0.22
0.58
0.24-
1.38

SemD
1.87
0.02
2.07
1.12-
3.81

Disag.
0.17
0.81
0.86
0.25-
2.97

Table 3: Results of the second logistic regression model. The
Nagelkerke pseudo R2 is 0.11 and the Hosmer and Lemeshow value
0.051.

|
s
t
x
e
T
d
e
t
c
a
r
t
x
E

|

60

40

20

0

[1 .3 7 , 1 .4 7 )

[1 .4 7 , 1 .5 8 )

[1 .5 8 , 1 .6 9 )

[1 .6 9 , 1 .7 9 )

[1 .7 9 , 1 .9 )

[1 .9 , 2 .0 1 )

[2 .0 1 , 2 .1 1 )

[2 .1 1 , 2 .2 2 )

Mean SemD

Figure 5: Mean SemD value distribution for the 185 extracted
texts. The standard deviation is 0.17.

For our ﬁrst model the results of our analysis are
shown in Table 2 and for our second model in Table 3.
Figure 5 shows the distribution of mean SemD values
for the extracted texts in our second model. Using the

10  23rd USENIX Security Symposium 

USENIX Association

Wald test, we evaluated the relationship between an in-
dependent variable and the dependent variable through
the P value relating to the coefﬁcient of that independent
variable. If the P value is less than 0.05, we reject the
null hypothesis, i.e., that that coefﬁcient is zero. Look-
ing at our results, it is noteworthy that both models do
not reveal a statistically relevant correlation between the
annotator disagreements and misclassiﬁcations. Thus, a
document with a disagreement did not have a higher like-
lihood of being misclassiﬁed than one without. However,
it is striking that the second model has a P value of 0.02
for the SemD variable. Standardizing our data points into
Z scores and calculating the odds ratios it becomes clear
that an increase of the mean SemD value in an extracted
text by 0.17 (one standard deviation) increased the like-
lihood of a misclassiﬁcation by 2.07 times (odds ratio).
Consequently, our second model shows that the ambigu-
ity of text in privacy policies, as measured by semantic
diversity, has statistical signiﬁcance for whether a classi-
ﬁcation decision is more likely to succeed or fail.

Besides evaluating the statistical signiﬁcance of indi-
vidual variables, we also assessed the overall model ﬁt.
While the goodness of ﬁt of linear regression models is
usually evaluated based on the R2 value, which measures
the square of the sample correlation coefﬁcient between
the actual values of the dependent variable and the pre-
dicted values (in other words, the R2 value can be un-
derstood as the proportion of the variance in a depen-
dent variable attributable to the variance in the indepen-
dent variable), there is no consensus for measuring the ﬁt
of binary logistic regression models. Various pseudo R2
metrics are discussed. We used the Nagelkerke pseudo
R2 because it can range from 0 to 1 allowing an easy
comparison to the regular R2 (which, however, has to ac-
count for the fact that the Nagelkerke pseudo R2 is of-
ten substantially lower than the regular R2). While the
Nagelkerke pseudo R2 of 0.03 for our ﬁrst model indi-
cates a poor ﬁt, the value of 0.11 for our second model
can be interpreted as moderate. Further, the Hosmer and
Lemeshow test, whose values were over 0.05 for both of
our models, demonstrates the model ﬁt as well.

In addition to the experiments just discussed, we also
evaluated our models with further independent variables.
Speciﬁcally, we evaluated our ﬁrst model with the pol-
icy publication year,
the second model with the ex-
tracted texts’ mean tf-idf values, and both models with
Flesch-Kincaid readability scores as independent vari-
ables. Also, using only ML classiﬁcations we evaluated
our second model with the number of available training
examples as independent variable. Only for the latter we
found statistical signiﬁcance at the 0.05 level. The num-
ber of training examples correlated to ML classiﬁcation
performance, which conﬁrms Ammar et al.’s respective
conjecture [16]. The more training examples the ML

classiﬁer had, the less likely a misclassiﬁcation became.

5.2 Inter-annotator Agreement
Having discussed the classiﬁcation performance, we now
turn to the gold standard that we used to measure that
performance. For our performance results to be reliable
our gold standard must be reliable. One way of pro-
ducing a gold standard for privacy policies is to ask the
providers whose policies are analyzed to explain their
meaning [11]. However, this approach should not be
used, at least in the U.S., because the Restatement of
Contracts provides that a contract term is generally given
the meaning that all parties associate with it (Restate-
ment (Second) of Contracts, §201). Consequently, poli-
cies should be interpreted from the perspective of both
the provider and user. The interpretation would evaluate
whether their perspectives lead to identical meanings or,
if that is not the case, which one should prevail under
applicable principles of legal interpretation. In addition,
since technical terms are generally given technical mean-
ing (Restatement (Second) of Contracts, §202(3)(b)), it
would be advantageous if the interpretation is performed
by annotators familiar with the terminology commonly
used in privacy policies. The higher the number of anno-
tations on which the annotators agree, that is, the higher
the inter-annotator agreement, the more reliable the gold
standard will be.

Because the annotation of a large number of docu-
ments can be very laborious, it is sufﬁcient under current
best practices for producing a gold standard to measure
inter-annotator agreement only on a data sample [62],
such that it can be inferred that the annotation of the re-
mainder documents is reliable as well. Following this
practice, we only measured the inter-annotator agree-
ment for our test set, which would then provide an indi-
cator for the reliability of our training and validation set
annotation as well. To that end, one author annotated all
policies and additional annotations were obtained for the
test policies from two other annotators. All annotators
worked independently from each other. As the author
who annotated the policies studied law and has exper-
tise in privacy law and the two other annotators were law
students with training in privacy law, all annotators were
considered equally qualiﬁed, and the annotations for the
gold standard were selected according to majority vote
(i.e., at least two annotators agreed). After the annota-
tions of the test policies were made, we ran our extension
on these policies and compared its classiﬁcations to the
annotations, which gave us the results in Table 1.

The reliability of our gold standard depends on the de-
gree to which the annotators agreed on the annotations.
There are various measures for inter-annotator agree-
ment. One basic measure is the count of disagreements.

USENIX Association  

23rd USENIX Security Symposium  11

Overall
Collection
Encryption
Ad Tracking
L. Retention
Proﬁling
Ad Disclosure

Disag. % Ag.
84%
8.12
100%
0
88%
6
7
86%
82%
9
78%
11
16
68%

K.’s α/F.’s κ

0.77
1

0.84
0.8
0.68
0.71
0.56

Table 4: Inter-annotator agreement for the 50 test policies. The
values for Krippendorff’s α and Fleiss’ κ are identical.

Per Policy
Mean
Signiﬁcance (P)
Odds Ratio (Z)
95% Conﬁdence
Interval (Z)

Length
2873.4

0.2
1.65
0.78-
3.52

SemD
2.08
0.11
1.87
0.87-4

Flesch-K.

14.53
0.76
1.12
0.55-
2.29

Per Section
Mean
Signiﬁcance (P)
Odds Ratio (Z)
95% Conﬁdence
Interval (Z)

Length
306.76
0.29
1.18
0.87-
1.6

SemD
2.08
0.04
1.51
1.02-
2.22

Flesch-K.

15.59
0.49
0.86
0.56-
1.32

Table 6: Results of the fourth logistic regression model. The
Nagelkerke pseudo R2 is 0.05 and the Hosmer and Lemeshow value
0.83.

|
s
n
o
i
t
c
e
S

|

100
80
60
40
20
0

[1 .9 3 , 1 .9 6 )

[1 .9 6 , 1 .9 9 )

[1 .9 9 , 2 .0 2 )

[2 .0 2 , 2 .0 5 )

[2 .0 5 , 2 .0 8 )

[2 .0 8 , 2 .1 2 )

[2 .1 2 , 2 .1 5 )

[2 .1 5 , 2 .1 8 )

Mean SemD

Table 5: Results of the third logistic regression model. The Nagelk-
erke pseudo R2 is 0.19 and the Hosmer and Lemeshow value 0.52.

Figure 6: Mean SemD value distribution for the 240 policy sections.
The standard deviation is 0.03.

Another one is the percentage of agreement (% Ag.),
which is the fraction of documents on which the anno-
tators agree [17]. However, disagreement count and per-
centage of agreement have the disadvantage that they do
not account for chance agreement. In this regard, chance-
corrected measures, such as Krippendorff’s α (K.’s α)
[53] and Fleiss’ κ (F.’s κ) [40] are superior. For Krip-
pendorff’s α and Fleiss’ κ the possible values are con-
strained to the interval [−1;1], where 1 means perfect
agreement, −1 means perfect disagreement, and 0 means
that agreement is equal to chance [37]. Generally, values
above 0.8 are considered as good agreement, values be-
tween 0.67 and 0.8 as fair agreement, and values below
0.67 as dubious [56]. However, those ranges are only
guidelines [17]. Particularly, ML algorithms can tolerate
data with lower reliability as long as the disagreement
looks like random noise [68].

Based on the best practices and guidelines for inter-
preting inter-annotator agreement measurements, our re-
sults in Table 4 conﬁrm the general reliability of our an-
notations and, consequently, of our gold standard. For
every individual category, except for the ad disclosure
category, we obtained Krippendorff’s α values indicat-
ing fair or good agreement. In addition, the overall mean
agreement across categories is 0.77, and, therefore, pro-
vides evidence for fair overall agreement as well. For the
overall agreement it should be noted that, corresponding
to the multi-label classiﬁcation task, the annotation of
privacy policies is a multi-label annotation task as well.
However, there are only very few multi-label annotation

metrics, such as Passonneau’s Measuring Agreement on
Set-valued Items (MASI) [61]. As none of the metrics
were suitable for our purposes, we selected as overall
metric the mean over the results of the individual clas-
siﬁcation categories.

We investigated our inter-annotator agreement results
by applying a third and fourth binary logistic regression
model. In our third model each of the 50 test policies
was represented by one data point with the dependent
variable identifying whether the annotators had any dis-
agreement in annotating the policy and the independent
variables identifying (1) the policy’s length in words, (2)
its mean SemD value, and (3) its Flesch-Kincaid score.
In our fourth model we represented each of 240 indi-
vidual annotations by one data point with the dependent
variable identifying whether the annotators disagreed for
that annotation and the independent variables identifying
(1) the length (in words) of the policy text section that the
annotation is referring to, (2) the section’s mean SemD
value, and (3) its Flesch-Kincaid score. For the fourth
model we excluded some of the 300 annotations because
not every policy had a section for each category. For
example, some policies did not discuss advertisement or
disclosure of information. The Flesch-Kincaid readabil-
ity score measures the number of school years an average
reader would need to understand a text.

For our third and fourth model the results of our anal-
ysis are shown in Table 5 and 6, respectively. Figure
6 shows the distribution of mean SemD values for the
policy sections in our fourth model. Both models were

12  23rd USENIX Security Symposium 

USENIX Association

signiﬁcant, as indicated by their Nagelkerke and Hosmer
and Lemeshow values. Our results conﬁrm that the read-
ability of policies, as measured by the Flesch-Kincaid
score, does not impact their comprehensibility [58]. In
our third model we were unable to identify any statisti-
cally relevant variables (although, semantic diversity and
length may be statistically signiﬁcant in a larger data set).
However, our fourth model proved to be more meaning-
ful. Remarkably, corresponding to our ﬁnding in Section
5.1, according to which classiﬁer performance correlates
to semantic diversity, the statistically relevant P value of
0.04 for the mean SemD variable also indicates a corre-
lation of inter-annotator agreement to semantic diversity.
Standardizing our data points into Z scores and calculat-
ing the odds ratios it becomes clear that an increase of
the mean SemD value of a section by 0.03 (one standard
deviation) increased the likelihood of a disagreement by
1.51 times (odds ratio). It is astounding that even qual-
iﬁed annotators trained in privacy law had difﬁculties to
avoid disagreements when semantic diversity increased
to slightly above-mean levels.

While neither our ﬁrst nor our second model in Sec-
tion 5.1 showed a correlation between inter-annotator
agreement and classiﬁer performance, the results for our
second and fourth model demonstrate that performance
and agreement both correlate to one common variable—
semantic diversity. More speciﬁcally, performance cor-
relates to the semantic diversity of extracted text phrases
and agreement correlates to the semantic diversity of
policy sections. This result suggests, for example, that
the relatively high number of misclassiﬁcations and dis-
agreements in the ad disclosure category is inherent in
the nature of the category. Indeed, in cases of fuzzy cat-
egories disagreements among annotators do not neces-
sarily reﬂect a quality problem of the gold standard, but
rather a structural property of the annotation task, which
can serve as an important source of empirical informa-
tion about the structural properties of the investigated
category [13]. Thus, it is no surprise that for all six cate-
gories the values of Krippendorff’s α correlate to the F-1
scores. The higher the value of Krippendorff’s α, the
higher the F-1 score. Figure 7 shows the correlation.

As both classiﬁer performance and inter-annotator
agreement decrease with an increase in semantic diver-
sity, the practicability of the notice-and-choice principle
becomes questionable. After all, privacy policies can
only provide adequate notice (and choice) if they are not
too ambiguous. In order to further examine policy am-
biguity we calculated the mean SemD value for our test
policies over time. Our test set analysis exhibited a statis-
tically signiﬁcant trend of decreasing semantic diversity
with a P value of 0.049. Figure 8 illustrates our approach.
We can think of two explanations for the decrease over
time. First, it could be a consequence of the FTC’s en-

e
r
o
c
S
1
-
F

100

80

60

AT E

C

P

LR

AD
0.6

0.8

0.7
Krippendorff’s α

0.9

1

Figure 7: Linear regression plot with the F-1 score as dependent
variable and Krippendorff’s α as independent variable. The co-
ordinate labels identify the categories: AD = Ad Disclosure, LR =
Limited Retention, P = Proﬁling, AT = Ad Tracking, E = Encryp-
tion, and C = Collection. With an R2 value of 0.83 the model has an
excellent ﬁt, which, however, should be interpreted in light of the
small number of data points.

2.1

2.09

2.08

2.07

D
m
e
S
n
a
e

M

1

2

3

4

5

6

7

8

9

10 11

Policy Version Number

Figure 8: Linear regression plot for Symantec’s privacy policy
(which was part of our test set) with the mean SemD value of a
policy version as dependent variable and the policy version num-
ber as independent variable. The ﬁrst version of Symantec’s policy
[73] dates back to August 5, 1999, and the eleventh version [72] was
adopted on August 12, 2013. The mean SemD value of Symantec’s
privacy policy decreased from 2.1 in the ﬁrst version to 2.06 in the
eleventh version as shown. We observed a similar decrease for 29
out of 44 test policies (6 of the test policies were only available in
a single version and, therefore, could not be included in our analy-
sis. However, for the 44 included policies we obtained on average 8
different versions over time.).

forcement actions and its call for policies to “be clearer,
shorter, and more standardized” [38]. Second, we might
be in the midst of a consolidation process leading to more
standardized policy language. As de Maat et al [34] ob-
served, drafters of legal documents tend to use language
that adheres to writing conventions of earlier texts and
similar statements. Independent of the reason, our result
suggests that the notice-and-choice principle can over-
come the problem of ambiguity over time.

5.3 Computational Performance
We ﬁnish the discussion of our experimental results with
our extension’s computational performance. We report
the mean duration in seconds for obtaining analysis re-
sults for each of 50 randomly selected policies from
ToS;DR (Crowdsourcing), processing each of the 50 test
policies (Classiﬁer), and processing each of the 50 test

USENIX Association  

23rd USENIX Security Symposium  13

Per Policy Crowdsourcing Classiﬁer
0.78 sec
Mean

0.39 sec

Training
20.29 sec

Table 7: Computational performance of the Privee extension. The
performance was evaluated on a Windows laptop with Intel Core2
Duo CPU at 2.13 GHz with 4 GB RAM. The space requirements
for the installation on the hard disk are 2.11 MB (including 1.7 MB
of training data and 286 KB for the jQuery library) and additional
230 KB during the program execution for storing training results.

policies each with initial training (Training) in Table 7.
Notably, retrieving policy results from ToS;DR is twice
as fast as analyzing a policy with our classiﬁers.

6 Conclusion

We introduced Privee—a novel concept for analyzing
natural language privacy policies based on crowdsourc-
ing and automatic classiﬁcation techniques. We im-
plemented Privee in a proof of concept browser exten-
sion for Google Chrome, and our automatic classiﬁers
achieved an overall F-1 score of 90%. Our experimental
results revealed that the automatic classiﬁcation of pri-
vacy policies encounters the same constraint as human
interpretation—the ambiguity of natural language, as
measured by semantic diversity. Such ambiguity seems
to present an inherent limitation of what automatic pri-
vacy policy analysis can accomplish. Thus, on a more
fundamental level, the viability of the notice-and-choice
principle might be called into question altogether. How-
ever, based on the decrease of policy ambiguity over time
we would caution to draw such conclusion. We remain
optimistic that the current notice-and-choice ecosystem
is workable and can be successfully supplemented by
Privee.

The most important task for making the notice-and-
choice principle work is to decrease policy ambiguity.
However, other areas require work as well: What are the
types of information that policies should be analyzed for?
What is the most usable design? What are the best fea-
tures and algorithms? Are more intricate ML or natural
language processing algorithms better at resolving am-
biguities? What is the ideal size and composition of the
training set? How can the interaction between the classi-
ﬁer and crowdsourcing analysis be improved? In partic-
ular, how can a program connect to many crowdsourcing
repositories, and, possibly, decide which analysis is the
best? Can crowdsourced policy results be used by the
classiﬁers as training data? How can it be assured that the
crowdsourcing results are always up to date? How can
the quality and consistency of crowdsourcing and ML
analyses be guaranteed? And, ﬁnally, what solutions are
viable for different legal systems and the mobile world?

7 Acknowledgments

For their advice and help we thank Kapil Thadani, Re-
becca Passonneau, Florencia Marotta-Wurgler, Sydney
Archibald, Shaina Hyder, Jie S. Li, Gonzalo Mena,
Paul Schwartz, Kathleen McKeown, Tony Jebara, Jeroen
Geertzen, Michele Nelson, and the commentators at the
7th Annual Privacy Law Scholars Conference (2014)
and the Workshop on the Future of Privacy Notice and
Choice (2014). We also thank the anonymous reviewers
for their valuable feedback.

8 Availability

Our Privee extension is available at http://www.
sebastianzimmeck.de/publications.html.

References
[1] Claridge v. RockYou, Inc., 785 F. Supp. 2d 855 (N.D. Cal. 2011).
[2] P3P
http://
compactprivacypolicy.org/compact token reference.
htm. Last accessed: July 1, 2014.

cross-reference.

compact

policy

[3] Privacy Icons.

http://www.azarask.in/blog/post/

privacy-icons/. Last accessed: July 1, 2014.

[4] privacychoice. http://www.privacychoice.org. Last ac-

cessed: July 1, 2014.

[5] Terms of Service; Didn’t Read (ToS;DR). http://tosdr.org/

index.html. Last accessed: July 1, 2014.

[6] Top sites

in United States.

http://www.alexa.com/

topsites/countries/US. Last accessed: July 1, 2014.

[7] TOSBack. http://tosback.org/. Last accessed: July 1,

2014.

[8] TOSBack2. https://github.com/pde/tosback2. Last ac-

cessed: July 1, 2014.

[9] Usable Privacy Policy Project. http://www.usableprivacy.

org/home. Last accessed: July 1, 2014.

[10] Platform for Internet Content Selection (PICS). http://www.

w3.org/PICS/, 1997. Last accessed: July 1, 2014.

[11] KnowPrivacy. http://knowprivacy.org/, June 2009. Last

accessed: July 1, 2014.

[12] eXtensible Access Control Markup Language (XACML) version
3.0. http://docs.oasis-open.org/xacml/3.0/xacml-3.
0-core-spec-os-en.html, Jan. 2013. Last accessed: July 1,
2014.

[13] ADREEVSKAIA, A., AND BERGLER, S. Mining WordNet for
fuzzy sentiment: Sentiment tag extraction from WordNet glosses.
In 11th conference of the European chapter of the Association for
Computational Linguistics (Stroudsburg, PA, USA, 2006), EACL
’06, ACL, pp. 209–216.

[14] AGRAWAL, R., KIERNAN, J., SRIKANT, R., AND XU, Y. An
XPath-based preference language for P3P. In Proceedings of the
12th international conference on World Wide Web (New York,
NY, USA, 2003), WWW ’03, ACM, pp. 629–639.

[15] A¨IMEUR, E., GAMBS, S., AND HO, A. UPP: User privacy pol-
icy for social networking sites. In Fourth International Confer-
ence on Internet and Web applications and services (Washington,
DC, USA, 2009), ICIW ’09, IEEE Computer Society, pp. 267–
272.

14  23rd USENIX Security Symposium 

USENIX Association

[16] AMMAR, W., WILSON, S., SADEH, N., AND SMITH, N. Auto-
matic categorization of privacy policies: A pilot study. Tech. Rep.
CMU-ISR-12-114, CMU-LTI-12-019, Carnegie Mellon Univer-
sity, Dec. 2012.

[31] CRANOR, L. F., LANGHEINRICH, M., AND MARCHIORI, M.
A P3P Preference Exchange Language 1.0 (APPEL 1.0). World
Wide Web Consortium, Working Draft WD-P3P-preferences-
20020415, April 2002.

[17] ARTSTEIN, R., AND POESIO, M.

Inter-coder agreement for
computational linguistics. Comput. Linguist. 34, 4 (Dec. 2008),
555–596.

[18] ASHLEY, P., HADA, S., KARJOTH, G., POWERS, C., AND
SCHUNTER, M. Enterprise Privacy Authorization Language
(EPAL 1.2). Tech. rep., IBM, Nov. 2003.

[19] BIAGIOLI, C., FRANCESCONI, E., PASSERINI, A., MONTE-
MAGNI, S., AND SORIA, C. Automatic semantics extraction in
law documents.
In Proceedings of the 10th International Con-
ference on Artiﬁcial Intelligence and Law (New York, NY, USA,
2005), ICAIL ’05, ACM, pp. 133–140.

[20] BREAUX, T. D., AND ANT ´ON, A. I. Mining rule semantics to
understand legislative compliance.
In Proceedings of the 2005
ACM Workshop on Privacy in the Electronic Society (New York,
NY, USA, 2005), WPES ’05, ACM, pp. 51–54.

[21] BREAUX, T. D., AND ANT ´ON, A. I. Analyzing regulatory rules
for privacy and security requirements. IEEE Trans. Software Eng.
34, 1 (Jan. 2008), 5–20.

[22] BRODIE, C., KARAT, C.-M., KARAT, J., AND FENG, J. Usable
security and privacy: a case study of developing privacy manage-
ment tools. In Proceedings of the 2005 Symposium On Usable
Privacy and Security (New York, NY, USA, 2005), SOUPS ’05,
ACM, pp. 35–43.

[23] BRODIE, C. A., KARAT, C.-M., AND KARAT, J. An empirical
study of natural language parsing of privacy policy rules using the
SPARCLE policy workbench. In Proceedings of the second Sym-
posium On Usable Privacy and Security (New York, NY, USA,
2006), SOUPS ’06, ACM, pp. 8–19.

[24] BYERS, S., CRANOR, L. F., KORMANN, D., AND MCDANIEL,
P. Searching for privacy: design and implementation of a P3P-
enabled search engine. In Proceedings of the 4th international
conference on Privacy Enhancing Technologies (Berlin, Heidel-
berg, Germany, 2005), PET ’04, Springer, pp. 314–328.

[25] CIOCCHETTI, C. A. The future of privacy policies: A privacy
nutrition label ﬁlled with fair information practices. J. Marshall
J. Computer & Info. L. 26 (2008), 1–46.

[26] COSTANTE, E., DEN HARTOG, J., AND PETKOVIC, M. What
websites know about you.
In DPM/SETOP (Berlin, Heidel-
berg, Germany, 2012), R. D. Pietro, J. Herranz, E. Damiani, and
R. State, Eds., vol. 7731 of Lecture Notes in Computer Science,
Springer, pp. 146–159.

[27] COSTANTE, E., SUN, Y., PETKOVI ´C, M., AND DEN HARTOG,
J. A machine learning solution to assess privacy policy complete-
ness: (short paper). In Proceedings of the 2012 ACM Workshop
on Privacy in the Electronic Society (New York, NY, USA, 2012),
WPES ’12, ACM, pp. 91–96.

[32] CRANOR, L. F., LANGHEINRICH, M., MARCHIORI, M.,
PRESLER-MARSHALL, M., AND REAGLE, J. M. The Platform
for Privacy Preferences 1.0 (P3P1.0) speciﬁcation. World Wide
Web Consortium, Recommendation REC-P3P-20020416, April
2002.

[33] CRANOR, L. F., AND REIDENBERG, J. R. Can user agents ac-

curately represent privacy notices? TPRC (Sept. 2002).

[34] DE MAAT, E., KRABBEN, K., AND WINKELS, R. Machine
learning versus knowledge based classiﬁcation of legal texts. In
Proceedings of the 2010 conference on Legal Knowledge and In-
formation Systems: JURIX 2010: The Twenty-Third Annual Con-
ference (Amsterdam, The Netherlands, The Netherlands, 2010),
IOS Press, pp. 87–96.

[35] DE MAAT, E., AND WINKELS, R. Automatic classiﬁcation of
sentences in dutch laws. In Proceedings of the 2008 conference
on Legal Knowledge and Information Systems: JURIX 2008: The
Twenty-First Annual Conference (Amsterdam, The Netherlands,
The Netherlands, 2008), IOS Press, pp. 207–216.

[36] DE MAAT, E., AND WINKELS, R. A next step towards auto-
mated modelling of sources of law. In Proceedings of the 12th
International Conference on Artiﬁcial Intelligence and Law (New
York, NY, USA, 2009), ICAIL ’09, ACM, pp. 31–39.

[37] DI EUGENIO, B., AND GLASS, M. The kappa statistic: a second

look. Comput. Linguist. 30, 1 (Mar. 2004), 95–101.

[38] FEDERAL TRADE COMMISSION. Protecting consumer privacy
in an era of rapid change. http://www.ftc.gov/reports/
protecting-consumer-privacy-era-rapid-change-
recommendations-businesses-policymakers, Mar. 2012.
Last accessed: July 1, 2014.

[39] FISCHER-H ¨UBNER, S., AND ZWINGELBERG, H. UI prototypes:
Policy administration and presentation - version 2. Tech. Rep.
D4.3.2, Karlstad University, 2010.

[40] FLEISS, J. Measuring nominal scale agreement among many

raters. Psychological Bulletin 76, 5 (1971), 378–382.

[41] FRANCESCONI, E., AND PASSERINI, A. Automatic classiﬁca-
tion of provisions in legislative texts. Artif. Intell. Law 15, 1 (Mar.
2007), 1–17.

[42] GODBOLE, S., AND SARAWAGI, S. Discriminative methods for
multi-labeled classiﬁcation.
In Proceedings of the 8th Paciﬁc-
Asia Conference on Knowledge Discovery and Data Mining
(Berlin, Heidelberg, Germany, 2004), Springer, pp. 22–30.

[43] HALL, M., FRANK, E., HOLMES, G., PFAHRINGER, B.,
REUTEMANN, P., AND WITTEN, I. H. The WEKA data min-
ing software: An update. SIGKDD Explor. Newsl. 11, 1 (Nov.
2009), 10–18.

[28] CRANOR, L. F. Necessary but not sufﬁcient: Standardized mech-
anisms for privacy notice and choice. J. on Telecomm. and High
Tech. L. 10, 2 (2012), 273–307.

[44] HOFFMAN, P., RALPH, M. L., AND ROGERS, T. Semantic di-
versity: A measure of semantic ambiguity based on variability in
the contextual usage of words. BRM 45, 3 (2013), 718–730.

[29] CRANOR, L. F., DOBBS, B., EGELMAN, S., HOGBEN,
G., HUMPHREY, J., LANGHEINRICH, M., MARCHIORI, M.,
PRESLER-MARSHALL, M., REAGLE, J. M., SCHUNTER, M.,
STAMPLEY, D. A., AND WENNING, R. The Platform for Pri-
vacy Preferences 1.1 (P3P1.1) speciﬁcation. World Wide Web
Consortium, Note NOTE-P3P11-20061113, November 2006.

[45] HOLTZ, L.-E., NOCUN, K., AND HANSEN, M. Towards dis-
playing privacy information with icons.
In Privacy and Iden-
tity Management for Life (Berlin, Heidelberg, Germany, 2011),
S. Fischer H¨ubner, P. Duquenoy, M. Hansen, R. Leenes, and
G. Zhang, Eds., vol. 352 of IFIP Advances in Information and
Communication Technology, Springer, pp. 338–348.

[30] CRANOR, L. F., GUDURU, P., AND ARJULA, M. User interfaces
for privacy agents. ACM Trans. Comput.-Hum. Interact. 13, 2
(June 2006), 135–178.

[46] HRIPCSAK, G., AND ROTHSCHILD, A. S. Technical brief:
the F-measure, and reliability in information re-

Agreement,
trieval. JAMIA 12, 3 (2005), 296–298.

USENIX Association  

23rd USENIX Security Symposium  15

[47] KARJOTH, G., SCHUNTER, M., AND WAIDNER, M. Platform
for Enterprise Privacy Practices: privacy-enabled management
of customer data. In Proceedings of the 2nd international con-
ference on Privacy Enhancing Technologies (Berlin, Heidelberg,
Germany, 2003), PET ’02, Springer, pp. 69–84.

[48] KELLEY, P. G. Designing a privacy label: assisting consumer
understanding of online privacy practices. In CHI ’09 Extended
Abstracts on Human Factors in Computing Systems (New York,
NY, USA, 2009), CHI EA ’09, ACM, pp. 3347–3352.

[49] KELLEY, P. G., BRESEE, J., CRANOR, L. F., AND REEDER,
In Proceedings of the
R. W. A ”nutrition label” for privacy.
5th Symposium On Usable Privacy and Security (New York, NY,
USA, 2009), SOUPS ’09, ACM, pp. 4:1–4:12.

[50] KELLEY, P. G., CESCA, L., BRESEE, J., AND CRANOR, L. F.
Standardizing privacy notices: an online study of the nutrition la-
bel approach. In Proceedings of the SIGCHI Conference on Hu-
man Factors in Computing Systems (New York, NY, USA, 2010),
CHI ’10, ACM, pp. 1573–1582.

[51] KELLEY, P. G., CRANOR, L. F., AND SADEH, N. Privacy as
part of the app decision-making process. In Proceedings of the
SIGCHI Conference on Human Factors in Computing Systems
(New York, NY, USA, 2013), CHI ’13, ACM, pp. 3393–3402.

[52] KOBYLI ´NSKI, L., AND PRZEPI ´ORKOWSKI, A. Deﬁnition ex-
traction with balanced random forests.
In Proceedings of the
6th international conference on Advances in Natural Language
Processing (Berlin, Heidelberg, Germany, 2008), GoTAL ’08,
Springer, pp. 237–247.

[53] KRIPPENDORFF, K. Content analysis: An introduction to its

methodology. SAGE, Beverly Hills, CA, USA, 1980.

[54] KUHN, F. A description language for content zones of German
court decisions. In Proceedings of the LREC 2010 Workshop on
the Semantic Processing of Legal Texts (2010), SPLeT ’10, pp. 1–
7.

[55] LEON, P. G., CRANOR, L. F., MCDONALD, A. M., AND
MCGUIRE, R. Token attempt: The misrepresentation of web-
site privacy policies through the misuse of P3P compact policy
tokens.
In Proceedings of the 9th Annual ACM Workshop on
Privacy in the Electronic Society (New York, NY, USA, 2010),
WPES ’10, ACM, pp. 93–104.

[56] MANNING, C. D., RAGHAVAN, P., AND SCH ¨UTZE, H. Intro-
duction to Information Retrieval. Cambridge University Press,
New York, NY, USA, 2008.

[57] MAROTTA-WURGLER, F. Does contract disclosure matter?

JITE 168, 1 (2012), 94–119.

[58] MCDONALD, A. M., REEDER, R. W., KELLEY, P. G., AND
CRANOR, L. F. A comparative study of online privacy poli-
cies and formats.
In Proceedings of the 9th international sym-
posium on Privacy Enhancing Technologies (Berlin, Heidelberg,
Germany, 2009), PETS ’09, Springer, pp. 37–55.

[59] MOENS, M.-F., BOIY, E., PALAU, R. M., AND REED, C. Auto-
matic detection of arguments in legal texts. In Proceedings of the
11th International Conference on Artiﬁcial Intelligence and Law
(New York, NY, USA, 2007), ICAIL ’07, ACM, pp. 225–230.

[60] NATIONAL

TELECOMMUNICATIONS

INFORMA-
form notice code of
TION ADMINISTRATION.
conduct
to promote transparency in mobile app practices.
http://www.ntia.doc.gov/files/ntia/publications/
july 25 code draft.pdf, July 2013. Last accessed: July 1,
2014.

Short

AND

[61] PASSONNEAU, R. Measuring Agreement on Set-valued Items
(MASI) for semantic and pragmatic annotation. In Proceedings of
the international Conference on Language Resources and Evalu-
ation (2006), LREC ’06.

[62] PASSONNEAU, R. J., AND CARPENTER, B. The beneﬁts of a
model of annotation. In Proceedings of the 7th Linguistic Annota-
tion Workshop and Interoperability with Discourse (Stroudsburg,
PA, USA, 2013), ACL, pp. 187–195.

[63] PORTER, M. An algorithm for sufﬁx stripping. Program: elec-

tronic library and information systems 14, 3 (1980), 130–137.

[64] RAJARAMAN, A., AND ULLMAN, J. D. Mining of massive
datasets. Cambridge University Press, New York, NY, USA,
2012.

[65] REEDER, R. W. Expandable Grids: a user interface visualiza-
tion technique and a policy semantics to support fast, accurate
security and privacy policy authoring. PhD thesis, Pittsburgh,
PA, USA, 2008. AAI3321049.

[66] REEDER, R. W., KELLEY, P. G., MCDONALD, A. M., AND
CRANOR, L. F. A user study of the Expandable Grid applied to
P3P privacy policy visualization. In Proceedings of the 7th ACM
Workshop on Privacy in the Electronic Society (New York, NY,
USA, 2008), WPES ’08, ACM, pp. 45–54.

[67] REIDENBERG, J. R. The use of technology to assure internet
privacy: Adapting labels and ﬁlters for data protection. Lex Elec-
tronica 3, 2 (1997).

[68] REIDSMA, D., AND CARLETTA, J. Reliability measurement

without limits. Comput. Linguist. 34, 3 (Sept. 2008), 319–326.

[69] RUBINSTEIN, I. S. Privacy and regulatory innovation: Moving

beyond voluntary codes. ISJLP 6, 3 (2011), 355–423.

[70] STAMEY, J. W., AND ROSSI, R. A. Automatically identifying
relations in privacy policies. In 27th ACM International Confer-
ence on Design of Communication (New York, NY, USA, 2009),
SIGDOC ’09, ACM, pp. 233–238.

[71] STEDE, M., AND KUHN, F.

Identifying the content zones of
German court decisions. In Lecture Notes in Business Informa-
tion Processing (Berlin, Heidelberg, Germany, 2009), vol. 37 of
BIS ’09, Springer, pp. 310–315.
[72] SYMANTEC CORPORATION.

Complete privacy statement.

http://web.archive.org/web/20131028120625/http:
//www.symantec.com/about/profile/policies/
privacy.jsp. Last accessed: July 1, 2014.

[73] SYMANTEC CORPORATION.

for Symantec.

ment
19991012020231/http://symantec.com/legal/
privacy.html. Last accessed: July 1, 2014.

state-
http://web.archive.org/web/

privacy

Global

[74] TSOUMAKAS, G., AND KATAKIS, I. Multi label classiﬁcation:

An overview. IJDWM 3, 3 (2007), 1–13.

[75] WESTERHOUT, E. Deﬁnition extraction using linguistic and
structural features. In Proceedings of the 1st Workshop on Deﬁni-
tion Extraction (Stroudsburg, PA, USA, 2009), WDE ’09, ACL,
pp. 61–67.

[76] WESTERHOUT, E. Extraction of deﬁnitions using grammar-
enhanced machine learning. In Proceedings of the 12th Confer-
ence of the European Chapter of the Association for Computa-
tional Linguistics: Student Research Workshop (Stroudsburg, PA,
USA, 2009), EACL ’09, ACL, pp. 88–96.

[77] WU, G., GREENE, D., AND CUNNINGHAM, P. Merging multi-
ple criteria to identify suspicious reviews. In Proceedings of the
Fourth ACM Conference on Recommender Systems (New York,
NY, USA, 2010), RecSys ’10, ACM, pp. 241–244.

[78] YANG, J., YESSENOV, K., AND SOLAR-LEZAMA, A. A lan-
guage for automatically enforcing privacy policies. In Proceed-
ings of the 39th annual ACM SIGPLAN-SIGACT symposium on
Principles Of Programming Languages (New York, NY, USA,
2012), POPL ’12, ACM, pp. 85–96.

16  23rd USENIX Security Symposium 

USENIX Association

