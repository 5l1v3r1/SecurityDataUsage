How Much Can We Micro-Cache Web Pages?

Xiao Sophia Wang, Arvind Krishnamurthy, and David Wetherall

University of Washington

ABSTRACT
Browser caches are widely used to improve the performance
of Web page loads. Unfortunately, current object-based
caching is too coarse-grained to minimize the costs associ-
ated with small, localized updates to a Web object. In this
paper, we evaluate the beneﬁts if caching were performed
at a ﬁner granularity and at diﬀerent levels (i.e., computed
layout and compiled JavaScript). By analyzing Web pages
gathered over two years, we ﬁnd that both layout and code
are highly cacheable, suggesting that our proposal can rad-
ically reduce time to ﬁrst paint. We also ﬁnd that mobile
pages are similar to their desktop counterparts in terms of
the amount and composition of updates.

Categories and Subject Descriptors
C.4 [Computer Systems Organization]: Performance of
Systems—Design studies

Keywords
Caching, Micro-Caching, Web applications, Web pages

1.

INTRODUCTION

Over the years, Web pages have become more complex,
making Web page loads notoriously slow even though the
underlying network has signiﬁcantly improved. The situa-
tion is worse for the mobile scenario, partly due to the lower
compute power available on these devices, and partly due
to network ineﬃciencies. Reports have shown that mobile
Web applications1 are losing the market share to their native
counterparts [8, 9].

A signiﬁcant bottleneck of page load times comes from
Web page structures, which do little to minimize updates
and maximize caching of unmodiﬁed content. For example,
reloading a page with minor updates requires fetching the
entire HTML object, which often triggers a DNS lookup and
a TCP connection setup, taking up one third of the page
load time [19]. In contrast, a Web page’s native counterpart

1In this paper, we use Web pages and Web applications
interchangeably since their underlying mechanisms are the
same.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
IMC’14, November 5–7, 2014, Vancouver, BC, Canada.
Copyright 2014 ACM 978-1-4503-3213-2/14/11 ...$15.00.
http://dx.doi.org/10.1145/2663716.2663739.

can render right away, interacting with the network for more
resources only when necessary. The result is that the native
counterpart oﬀers much faster time to ﬁrst paint [17] (also
user-perceived page load time) than the Web page.

We argue that by restructuring Web pages the perfor-
mance gap between the Web and native applications can be
largely eliminated. The ﬁrst step is to minimize the impact
of page updates by employing micro-caching. Diﬀerent from
current object-based caching, micro-caching caches any por-
tion of an object at a much ﬁner granularity, which can be
an HTML element, a few lines of JavaScript, or a few CSS
rules.
Implementing micro-caching is possible given that
JavaScript now can control almost every aspect of a page
and also has access to browser storage.

The second step is to augment browsers with the ability to
distinguish between layout, code, and data. Doing so lets us
further cache the computed layout and compiled code, sub-
stantially reducing computation that is mostly redundant.
Note that caching layout, code, and data contributes diﬀer-
ently to improving page load times. Caching layout helps
the most because loading HTML is always on the critical
path of loading a page, which blocks all subsequent object
loads [19]. Caching code also helps since loading and evalu-
ating JavaScript are often on the critical path. But, caching
data helps little because only a small portion of data fetches
are on the critical path. Therefore, our focus is on caching
layout and code.

The idea of micro-caching is not entirely new. Delta en-
coding was proposed over a decade ago, and it advocates
that servers transparently compress pages and send diﬀer-
ences between versions of a page [3]. Edge Side Includes
(ESI) assembles Web pages at the edge to make use of edge
caching [4]. We ﬁnd the idea of micro-caching worth revisit-
ing given that Web pages have become more dynamic than
before. Diﬀerent from treating every bit equally (as in both
delta encoding and ESI), our proposal requires applications
to make updates explicit and to make distinctions regarding
layout, code, and data. As layout and code are likely on the
critical path of a page load, caching them separately is key
to improving page load times.

As a ﬁrst step towards micro-caching, this paper stud-
ies its eﬀectiveness — how much can we micro-cache Web
pages? To this end, we collect snapshots of hundred Web
pages over two years, and perform diﬀ-like analysis that
identiﬁes updates from one version of a page to another.
To infer whether they are from layout, code, or data, we
analyze the context of the updates.

While Web pages are traditionally treated as black boxes
by measurement studies, we embrace the opposite approach
that lets us uncover the redundant bits in Web traﬃc, and
that treats layout, code, and data diﬀerently so as to mit-

249igate the bottlenecks of loading modern Web pages. Our
main contributions are as follows.
• We propose micro-caching that distinguishes portions of
a page corresponding to layout, code, and data. We make
the case that micro-caching can radically reduce page load
times.
• For content-scarce pages, we ﬁnd that less than 20% (10%)
of the HTML page is changed when revisited after a month
(day), with updates mostly to code and data, and little
to layout. This means that such pages are highly micro-
cacheable, especially for layout.
• For content-rich pages, the amount of updates vary across
Web pages. In the best (worst) case, 20% (75%) of the
HTML page is changed over a month. Most changes are
made to data (e.g., links to images, titles) while little is
made to layout and code, indicating that layout and code
are highly micro-cacheable.
• Mobile pages are similar to desktop counterparts in terms
• About half of the object fetches are dynamic and thus the
idea of micro-caching is worth revisiting. Unlike CSS and
images, most HTML is dynamic. The large amount of
dynamic images is also surprising.

of the amount and composition of updates.

2. RELATED WORK

As a key approach to improving page loads, caching has
received lots of attention. One related work is delta encod-
ing that advocates compressing HTTP data transparently
in the form of diﬀerences [3]. Another related work, Edge
Side Includes (ESI), assembles Web pages at the edge to ex-
ploit edge caching [4]. Unlike delta encoding and ESI that
treats every bit equally, we isolate layout and code from
data, and cache their evaluated versions separately inside
browsers, so as to mitigate the bottlenecks in loading pages,
and to remove all the network interactions that block time
to ﬁrst paint in the usual case. Other work focuses solely
on object-based caching and treats each object as a black
box [14, 21, 16]. We propose micro-caching and are the ﬁrst
to quantitatively and qualitatively study changes in pages
at a ﬁne granularity.

There is also a large body of work on measuring other
aspects of Web performance. Ihm and Pai [6] presented a
[1] studied
longitudinal study of Web traﬃc, Ager et al.
Web performance with respect to CDNs, Huang et al.
[5]
measured the page load times with a slower cellular network,
and Wang et al.
[18] studied the use of the SPDY protocol
to improve page load times. These studies are invaluable for
improving our understanding of Web performance.

3. MICRO-CACHING
The case for micro-caching. Web applications are slower
than native applications, because Web applications require
more steps to launch. Launching a native application re-
quires only executing the code as it has been downloaded
beforehand, asking for more data or computation from the
server only when necessary (see Figure 1(a)). However,
launching a Web application incurs a more complex pro-
cedure. It starts with the client asking for the application
code from the server, followed by the server running server-
side code to generate client-side code and sending it to the
client, and ﬁnishes with the client running the client-side
code, performing layout, and painting (see Figure 1(b)). As

Figure 1: Workﬂows of native and mobile apps.
(solid: data ﬂows; dotted: control ﬂows)

a consequence, Web applications incur much higher time to
ﬁrst paint.

Why do Web applications need more steps to launch?
This is because the client side lacks visibility into updates
and the model-view-controller (MVC) [10] abstraction of
Web applications. The lack of visibility causes three inef-
ﬁciencies. First, a Web application has to fetch the execu-
tion code upon launch even if it hasn’t changed, while na-
tive applications can explicitly opt in for updates and avoid
reloading code. Second, a Web application has to compute
the layout from scratch during rendering, while the layout
of native applications can be cached. Third, running a Web
application always involves JavaScript compilation that hap-
pens just in time, while native applications are pre-compiled.
Worse for mobile devices, contacting the network sometimes
requires a few seconds due to radio interactions, and compu-
tation on mobile devices is slower due to the lack of compute
power [5, 20]. The result is that Web applications are losing
the mobile market to native applications [8, 9].

We argue that, to eliminate these ineﬃciencies, the client
should be provided with enough information — explicit up-
dates and explicit distinctions between layout, code, and
data (the MVC abstraction) – both of which are readily
available in native applications. The client can then cache
not only raw objects but also compiled JavaScript and com-
puted layout, and thus minimize network interactions and
computation. We envision the following (as illustrated in
Figure 1(c)): both compiled code and computed layout of
Web applications are always cached in browsers, incremen-
tally evolving themselves upon updates. When the URL of
a Web application is requested, the cached layout is imme-
diately rendered, and the cached code makes the application

app     browser     browser     open (1) more data/comp. (4) User has updates? (6) update? (8) data/result (5) done (3) yes (7) agree (9) update app (10) Client Server req. (1) update app (2) User app (4) done (6) Client Server data more data/comp. (4) update? (6) data/result (5) incrementally update app (7) Client Server code data layout code data layout req. (1) User done (3) (a) Native app (b) Web app (c) Web app with visibility usage flow update flow update mngr. app gen.	  app	  (3) render	  (2) render	  (2) layout,	  render(5) 250Figure 2: Analysis pipeline.

fully functional including checking the necessity of network
interactions, e.g., getting more user data or computation
from servers.

This approach removes the entire network interactions
that block page loads and portions of computation for the
usual case. Therefore, we expect page load times (and time
to ﬁrst paint) to be substantially reduced since our previous
study shows that the network time consumes most of the
page load time [19].

This approach does not sacriﬁce any of the existing ad-
vantages of Web applications, improves latency, energy con-
sumption, and data usage at the same time, and requires
minimal changes to browsers that cache computed layout2
and compiled code appropriately. Unfortunately, there is no
way to provide this visibility automatically without rewrit-
ing Web pages because most logics of micro-caching are im-
plemented at the Web page level (e.g., handling versions
using HTML5 localStorage). As websites routinely rewrite
Web pages, mostly for embracing more eﬃcient architectures
(e.g., Groupon and mobile LinkedIn migrated all their pages
to node.js [11, 12]), we believe that page rewriting is a viable
solution if the beneﬁts are indeed substantial.

Deﬁnition of micro-caching. We deﬁne micro-caching
as caching any portion of an object. Compared to current
object-based caching that caches the entire object, micro-
caching operates at a much ﬁner granularity. Enabling micro-
caching is necessary to provide explicit updates and explic-
itly distinguish layout, code, and data.

Eﬀectiveness of micro-caching. As a ﬁrst step towards
micro-caching, we study how much we can micro-cache Web
pages? We answer this by analyzing the amount of updates
from layout, code, and data respectively. This requires us
to (i) identify the diﬀerence between two versions of a Web
page, and (ii) infer whether the diﬀerence belongs to data,
layout, or code.

4. METHODOLOGY

Figure 2 shows the pipeline of our analysis that we elab-

orate below.
4.1 Diff-based analysis

We want to study the diﬀerence (or similarity) between
two versions of a page. A naive approach is to run a diff
command that identiﬁes the diﬀerences at the granularity
of lines. As the deﬁnition of a line in Web objects is frag-
ile, we pre-process the pages by pretty printing them using
the js-beautify library [7]. We use the classic dynamic-
programming algorithm that matches two versions of a page
to maximize the number of matched lines. A matrix Di,j

2Modern browsers are able to cache computed layout, but
use a diﬀerent policy to control the lifetime of this cache.

Figure 3: Example of inferring context of an HTML
document.

is used to denote the number of matched lines between the
ﬁrst i lines of the ﬁrst page and the ﬁrst j lines of the second
page. If the i-th line of the ﬁrst page and the j-th line of the
second page are matched, Di,j = Di−1,j−1 + 1; otherwise,
Di,j = max{Di−1,j, Di,j−1}. We start with Di,0 = 0 and
D0,j = 0, and increase the indice until we reach the last lines
of both pages. This algorithm incurs a O(n2) time complex-
ity where n is the number of lines of a page. As some lines
can be unexpectedly long and slow down the algorithm, we
accelerate this step by calculating an md5 hash of each line
and matching the hash values. Our algorithm outputs the
number of unmatched lines, bytes, and unmatched content.
We use similarity, deﬁned as one minus the fraction of dif-
ference in bytes using the above algorithm, to characterize
the amount of updates eliminated by micro-caching. Sim-
ilarity provides a lower bound on estimated savings since
unmatched lines of two pages are not entirely diﬀerent.

4.2 Context analysis

We ﬁrst infer whether a line is one of HTML markup, CSS,
and JavaScript, and then infer whether a string belongs to
layout, code, or data.

Inferring HTML/CSS/JS. Inferring HTML is straight-
forward; we infer a line as HTML if it is quoted by <> (pretty
printing helps here). However, diﬀerentiating between CSS
and JavaScript is non-trivial; for example, a CSS property is
similar to a property declaration of an object in JavaScript
(e.g., Line 5 and 10 in Figure 3). We notice the slight diﬀer-
ence that CSS property (JavaScript object property) ends
with a semi-colon (comma), and use it to distinguish be-
tween CSS and JavaScript. When we are unable to distin-
guish by looking at the line itself, we search backwards and
forwards until hitting a line that allows for inference.

Inferring layout/code/data. Before this inference, we
further identify the modiﬁed and added content at the gran-
ularity of strings. The diﬀ-based analysis gives pairs of un-
matched chunks (a few lines) between two pages, indicating
that a chunk on the ﬁrst page is modiﬁed into another chunk
on the second page. For example, insertions (removals) in-
cur an empty chunk on the ﬁrst (second) page. Here, we ﬁrst
break up a chunk into strings that are separated by either
spaces or the newline character (pretty printing also helps

hash each line Pre-processing pretty print Web pages context analysis diff-based analysis Web pages Web pages Input Output Analysis % updates updates by type <html>! <head>!  <style>!    body {!     background-color: black;!    }!  </style>!  <script>!   var u= “djoe”;!  </script>!!! </head>! <body onload=“$(‘#r9D7’).html(u);”>!  <div width=“100px” id=“r9D7”!   title=“your username”></div>!  <script>!   document.write(“<div>pw:</div>”);!  </script>! </body>!</html>!    body {!     background-color: black;!    }!   var a = {!    u: “djoe”,!    dob: “01/01/1991”,!   };!   document.write(“<div>pw</div>”);!js!html!css!layout!code!data!css selector css property js embeds data js embeds layout html embeds code html embeds data 1!2!3!4!5!6!7!8!9!10!11!12!13!14!15!16!17!18!19!20!21!251Content Country Category
Page
scarce
google.com
scarce
baidu.com
scarce
wikipedia.org
rich
amazon.com
taobao.com
rich
youtube.com rich
yahoo.co.jp
rich

Search
Search
Information
E-Commerce
E-Commerce
Video
News

US
China
–
US
China
US
Japan

Table 1: Seven pages for extensive analysis.

here). For each pair of unmatched chunks, we apply the diﬀ-
based analysis above to pinpoint the unmatched strings.

Inferring from CSS is trivial since all of CSS maps to lay-
out, but inferring from HTML and JavaScript are not triv-
ial. For HTML, attributes can be layout when they are
like CSS properties (e.g., width), can be code when they
start with on (e.g., onload), and can be data when they
are one of value, name, and so forth. JavaScript is code by
default, but can embed data (e.g., username), and can em-
bed code that is quoted by document.write() or eval().
To infer from HTML, we pre-classify all known attributes
into the appropriate layout, code, or data categories. We
are unable to classify self-deﬁned attributes. To infer from
JavaScript, we identify strings that are quoted by single or
double quotes as data. We do not encounter code that starts
with document.write or eval, and therefore we do not have
to handle that case3. Figure 3 illustrates the context infer-
ence of an HTML document.
4.3 Datasets

We collect our datasets using a measurement node at the
University of Washington, starting from April 2012. Our
datasets span two years. We extensively take snapshots of
top twenty Alexa [2] pages every hour, which is more fre-
quent than page updates. We also take snapshots of top
one hundred Web pages every day. To study mobile pages,
we collect a month-long dataset of top twenty mobile pages
every hour. To take a snapshot of a page, we use both Phan-
tomJS [13] to collect HTTP headers and Web page metadata
in the HAR format and wget to collect the content of Web
objects.

5. RESULTS

We study the beneﬁts of micro-caching and shed light on

existing caching schemes.
5.1 Beneﬁts of micro-caching
Filtering Web pages. The set of measured pages are ei-
ther content-rich pages that provide personalized or up-to-
date content (e.g., news, video, e-commerce), or content-
scarce pages that provide a single service (e.g., search). In
our study, we exclude pages that require logins to provide
content (e.g., social network) since these pages without lo-
gins are not representative of their real usage. We also ex-
clude pages that are similar to a chosen page (e.g., google.
de is similar to google.com). By excluding such pages, we
focus on seven of the top twenty pages for intensive analysis.
These pages (shown in Table 1) contain both content-rich

3eval is considered a bad practice, but was found on half of
the top 10,000 pages [15]. We do not ﬁnd it on our set of
pages, likely because the very top pages are optimized.

(a) google.com

(b) wikipedia.org

(c) taobao.com

Figure 5: Similarity over time that reﬂects how web-
sites do updates. Each curve represents similarity
between a ﬁxed version and versions after.

and content-scarce pages and span three countries and ﬁve
categories. We believe that intensively studying a small set
of top pages is valuable, because they are often the most
dynamic and are thus hardest to be micro-cached.

Estimating beneﬁts. A direct way of estimating beneﬁts
of micro-caching would be comparing the page load times
before and after micro-caching is applied. However, such
comparison is both inaccurate and unfair. It is hard to ac-
curately estimate the page load time beneﬁts without im-
plementing micro-caching since page load times depend on
a large number of factors. This includes network parameters,
compute power of the browser device, and the dependency
model of page load activities. A slight modiﬁcation to the
Web page can result in very diﬀerent page load times. To
enable micro-caching, we propose to modify the entire Web
pages so that pages are loaded directly from the browser
cache most of the time and are updated asynchronously only
when necessary. Comparing page load times of the modiﬁed
page and the original page would be unfair.

Here, we qualitatively estimate the beneﬁts. Our previous
study informs that the network time consumes most of the
page load times [19]. For example, contacting remote servers
often triggers DNS lookup, TCP connection setup, and SSL
handshakes when HTTPS is used; the cascades of latencies
make time to ﬁrst paint slow. Our proposal removes the
entire network interactions and portions of computation that
block page loads for the usual case. Thus, we expect page
load times to be largely reduced.

Instead of quantifying the beneﬁts of page load times, we
focus on studying the impact from Web page updates. The

 0.3 0.4 0.5 0.6 0.7 0.8 0.9 103/01/1205/01/1207/01/1209/01/1211/01/1201/01/1303/01/1305/01/1307/01/1309/01/1311/01/1301/01/1403/01/1405/01/14Similarity1234567 0 0.2 0.4 0.6 0.8 103/01/1205/01/1207/01/1209/01/1211/01/1201/01/1303/01/1305/01/1307/01/1309/01/1311/01/1301/01/1403/01/1405/01/14Similarity1234 0 0.2 0.4 0.6 0.8 103/01/1205/01/1207/01/1209/01/1211/01/1201/01/1303/01/1305/01/1307/01/1309/01/1311/01/1301/01/1403/01/1405/01/14Similarity1234252Figure 4: Similarity between two versions of pages by varying access interval. For a ﬁxed interval, we vary
the time of accessing the ﬁrst version and obtain a list of similarities. The candle sticks show minimum,
10-percentile, median, 90-percentile, and maximum.

Figure 6: Updated bytes broken down by layout,
code, and data. wikipedia.org is excluded as little is
updated.

less and less often a Web page is updated, the less and less
often an asynchronous network fetch needs to be issued, and
the more the page can be micro-cached.

5.1.1 Longitudinal study
We report on the two-year desktop dataset.

How much is updated. We ﬁrst look at the amount of
updates when visiting a page after a given interval. Here we
use similarity that indicates an upper bound on the amount
of updates. Figure 4 shows the results. For content-scarce
pages (left three), less than 20% (10%) of the HTML page
is changed when revisited after a month (day), meaning
that such pages are highly micro-cacheable. Content-rich
pages (right four) have high variance for micro-cacheability:
youtube.com updates signiﬁcantly every hour, while yahoo.
co.jp updates less than 20% even over a month.

How often pages are updated. We further look at how
often pages are updated by obtaining similarity over time
in Figure 5. Here, we focus on major updates that require
reloading a large portion of the page. Updates are indicated
by sharp drops in Figure 5 – the more a page is updated, the
sharper the drop is. We ﬁnd that google.com updates every
few weeks. wikipedia.org updates less often; we see one
major update at the end of May 2012 and one at the end of
November 2013. Unlike content-scarce pages, taobao.com
updates incrementally between two major updates. These
incremental updates are mostly data, but little layout or
code. We also plot the graphs for other pages, which we
do not show here due to space limits. They together show
that major updates are rare and the amount of updates in
layout and code between two major updates are moderate,

Figure 7: Similarity between two versions of mobile
pages by varying access frequency.

suggesting that micro-caching is practical to mitigate page
load bottlenecks.

What is updated. We break down the updated bytes
into layout, code, and data. Figure 6 shows that most up-
dates are made to data; baidu.com and yahoo.com update
only data. Updates to data are mostly through HTML at-
tributes (e.g., href, title) and inner HTML content, while
some updates to data are made through JavaScript. Con-
versely, little update is made to layout; only half of the pages
update layout, by a lesser amount. Also, little update is
made to code. These results together suggest that layout
and code are highly micro-cacheable for both content-scarce
and content-rich pages.

To be informative, we manually go through some of the
updated content. For example in google.com, we ﬁnd that
many updates are random strings/numbers that are gener-
ated by servers. Except for security considerations, random
string/number generation can be moved to the client side
so as to minimize updates. We also ﬁnd that some pages
change CSS that has no visible impact. This CSS is likely
being loaded speculatively that will be used when users per-
form an action. This kind of CSS can be loaded in the back-
ground without impairing the cacheability of layout. Some
other CSS shows signiﬁcant visual impact. Because we can
cache the previous layout, incremental layout incurs minimal
computation.

5.1.2 Mobile pages
We report on the month-long mobile dataset. We exclude
two pages that provide diﬀerent functionalities than their
desktop counterparts. Figure 7 shows the mobile counter-
parts of Figure 4. Clearly, the amount of updates in mobile
Web pages are similar to their desktop counterparts. The
variance is smaller here because the time of measurements

 0 0.2 0.4 0.6 0.8 1wikipedia.orgbaidu.comgoogle.comyahoo.co.jptaobao.comamazon.comyoutube.comSimilarityhourlydailyweeklymonthlyyearly 0 500 1000 1500 2000 2500 3000 3500 40001h1d1w1m1h1d1w1m1h1d1w1mbyteslayoutcodedatayahoogooglebaidu 0 10000 20000 30000 40000 50000 600001h1d1w1m1h1d1w1m1h1d1w1mbyteslayoutcodedata      youtube  amazontaobao 0 0.2 0.4 0.6 0.8 1wikipedia.orggoogle.comyahoo.co.jpamazon.comyoutube.comSimilarityhourlydailyweeklymonthly253Figure 8: Updated bytes of mobile pages broken
down by layout, code, and data.

Figure 10: Frequencies at which dynamic objects are
updated (breakdown by mime type).

Figure 9: Breakdown the number of static, dynamic,
and once objects by mime type.

is shorter. Figure 8 shows the mobile counterpart of Fig-
ure 6. They are alike except for some random spikes in Fig-
ure 8. We manually look through updates in mobile pages
and desktop pages and ﬁnd that updates in code are similar,
indicating that they are likely to share the same code base.
However, the format of updated data is diﬀerent, possibly
because of delivering content on a smaller screen.

5.1.3 Practices that undermine micro-caching
We summarize common practices suggested by our mea-
surements that undermine the eﬀectiveness of micro-caching.
• JavaScript obfuscation. Websites often obfuscate client-
side JavaScript to reduce readability which, however, also
reduces micro-cacheability. We ﬁnd variables in two ver-
sions of google.com performing the same logic while using
diﬀerent names.
• CSS reordering. The order of CSS rules does not matter
most of the time and we ﬁnd some websites reorder their
CSS rules. Using consistent ordering would help micro-
caching.
• CSS abbreviation. We ﬁnd equivalent CSS rules in dif-
ferent versions of a page. For example, body,a{rule} is
unfolded to body{rule} a{rule}, and border-top, border-
bottom, border-left, and border-right are condensed
into border. Using the same level of abbreviation consis-
tently would help micro-caching.
• Object sharding. In the HTTP/1.1 era, websites shard
JavaScript and CSS objects to exploit concurrent TCP
connections. Because sharded objects use random URLs
as their keys, they hurt caching.

5.2 Dynamics of Web objects

Our measurements also let us analyze the dynamics at the
granularity of a Web object. Static objects can be cached
inﬁnitely given enough disk space, while dynamic objects
need to evict their cache before updates. Studying the dy-
namics at the granularity of a Web object sheds light on
implementing expiration of object-based caching. Here, we
consider all pages we collected.

Figure 11: Lifetimes of static objects (breakdowns
by popular domain).

We ﬁnd that about half of the object fetches are dy-
namic, meaning that the idea of micro-caching is worth re-
visiting. Figure 9 shows the amount of static and dynamic
objects respectively broken down by MIME type. HTML
and JavaScript are more dynamic while CSS and images are
more static. Images are expected to be identiﬁed by a unique
URL, but we are surprised to learn that a signiﬁcant amount
of images are dynamic.

Figure 10 shows the frequencies at which dynamic objects
are updated. Most dynamic HTML, JavaScript, and images
are changing all the time (1 hour is the unit of measure-
ments), likely being backed by server-side scripts. In con-
trast, less than 40% of CSS is changing all the time. For all
kinds of dynamic objects, over 75% of them change within
a day, meaning that cache expiration for dynamic objects
should be mostly set to just a few hours.

Figure 11 shows the lifetimes of static objects. The me-
dian lifetime of static objects is about two days. However,
there is high variance in lifetimes regarding diﬀerent web-
sites. Over 75% of objects in wikipedia.org have a lifetime
of more than 1,000 hours, suggesting that most static ob-
jects in wikipedia.org should be cached. But half of the
static objects on most other pages have a lifetime of less
than a day, suggesting that caching these objects for more
than one day is likely to waste disk space.

6. CONCLUSION

This paper proposes to separately cache layout, code, and
data at a ﬁne granularity inside browsers to mitigate the
bottleneck of loading modern Web pages. By analyzing
two years of measurements of Web pages, we ﬁnd that lay-
out and code that block subsequent object loads are highly
cacheable.

Acknowledgements
We thank our shepherd, Andreas Haeberlen, and the anony-
mous reviewers for their feedback.

 0 1000 2000 3000 4000 50001h1d1w1m1h1d1w1m1h1d1w1mbyteslayoutcodedata    yahoo    googlewikipedia 0 20000 40000 60000 80000 100000 120000 1400001h1d1w1m1h1d1w1mbyteslayoutcodedata        youtubeamazon 100 1000 10000 100000 1e+06 1e+07 1e+08HTMLJSCSSImageOther# of object fetchesDynamicStaticAppeared once 0 0.2 0.4 0.6 0.8 1 0.1 1 10 100 1000CDFhourshtmljscssimageother 0 0.2 0.4 0.6 0.8 1 1 10 100 1000CDFhoursamazon.combaidu.comgoogle.comtaobao.comwikipedia.orgyahoo.co.jpyoutube.com2547. REFERENCES
[1] B. Ager, W. Muhlbauer, G. Smaragdakis, and

S. Uhlig. Web Content Cartography. In Proc. of the
SIGCOMM conference on Internet Measurement
Conference (IMC), 2011.

[2] Alexa - The Web Information Company.

http://www.alexa.com/topsites/countries/US.

[3] Delta encoding in HTTP. http://tools.ietf.org/

html/draft-mogul-http-delta-07.

[4] ESI Language Speciﬁcation 1.0.

http://www.w3.org/TR/esi-lang.

[5] J. Huang, Q. Xu, B. Tiwana, Z. M. Mao, M. Zhang,

and P. Bahl. Anatomizing application performance
diﬀerences on smartphones. In Proc. of the
international conference on Mobile systems,
applications, and services (Mobisys), 2010.

[6] S. Ihm and V. S. Pai. Towards understanding modern
web traﬃc. In Proc. of the SIGCOMM conference on
Internet Measurement Conference (IMC), 2011.

[7] JS Beautify. http://jsbeautifier.org/.
[8] The mobile web is still losing out to native apps, by

more than 6 to 1.
http://venturebeat.com/2014/04/01/the-mobile-
web-is-still-losing-out-to-native-apps-six-
years-into-the-mobile-revolution/.

[9] Apps Solidify Leadership Six Years into the Mobile
Revolution. http://www.flurry.com/bid/109749/
Apps-Solidify-Leadership-Six-Years-into-the-
Mobile-Revolution#.U2bgha2SzOQ.

[10] Model-view-controller. http://en.wikipedia.org/

wiki/Model%E2%80%93view%E2%80%93controller.

[11] Need for Speed: How Groupon Migrated to Node.js.

http:
//www.datacenterknowledge.com/archives/2013/
12/06/need-speed-groupon-migrated-node-js/.

[12] Many high-proﬁle companies – node.js.

http://nodejs.org/industry/.

[13] PhantomJS. http://phantomjs.org/.
[14] F. Qian, K. S. Quah, J. Huang, J. Erman, A. Gerber,
Z. M. Mao, S. Sen, and O. Spatscheck. Web Caching
on Smartphones: Ideal vs. Reality. In Proc. of the
ACM Mobisys, 2012.

[15] G. Richards, C. Hammer, B. Burg, and J. Vitek. The

Eval that Men Do. In Proc. of the 25th European
Conference on Object-Oriented Programming
(ECOOP), 2011.

[16] S. Sundaresan, N. Feamster, R. Teixeira, and
N. Magharei. Measuring and Mitigating Web
Performance Bottlenecks in Broadband Access
Networks. In Proc. of the ACM Sigcomm Internet
Measurement Conference (IMC), 2013.

[17] Transaction Perspective: User Experience Metrics.

http://www.keynote.com/products/web_
performance/performance_measurement/user-
experience-metrics.html.

[18] X. S. Wang, A. Balasubramanian, A. Krishnamurthy,
and D. Wetherall. Accelerating the Mobile Web with
Selective Oﬄoading. In Proc. of the USENIX
conference on Networked Systems Design and
Implementation (NSDI), 2014.

[19] X. S. Wang, A. Balasubramanian, A. Krishnamurthy,

and D. Wetherall. Demystifying page load
performance with WProf. In Proc. of the USENIX
conference on Networked Systems Design and
Implementation (NSDI), 2013.

[20] X. S. Wang, H. Shen, and D. Wetherall. Accelerating
the Mobile Web with Selective Oﬄoading. In Proc. of
the ACM Sigcomm Workshop on Mobile Cloud
Computing (MCC), 2013.

[21] Z. Wang, F. X. Lin, L. Zhong, and M. Chishtie. How

far can client-only solutions go for mobile browser
speed? In Proc. of the international conference on
World Wide Web (WWW), 2012.

255