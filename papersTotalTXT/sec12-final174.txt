Throttling Tor Bandwidth Parasites

Rob Jansen

Paul Syverson

U.S. Naval Research Laboratory

{rob.g.jansen, paul.syverson}@nrl.navy.mil

Nicholas Hopper

University of Minnesota

hopper@cs.umn.edu

Abstract

Tor is vulnerable to network congestion and performance
problems due to bulk data transfers. A large fraction of
the available network capacity is consumed by a small
percentage of Tor users, resulting in severe service degra-
dation for the majority. Bulk users continuously drain
relays of excess bandwidth, creating new network bot-
tlenecks and exacerbating the effects of existing ones.
While this problem may currently be attributed to ratio-
nal users utilizing the network, it may also be exploited
by a relatively low-resource adversary using similar tech-
niques to contribute to a network denial of service (DoS)
attack. Degraded service discourages the use of Tor, af-
fecting both Tor’s client diversity and anonymity.

Equipped with mechanisms from communication net-
works, we design and implement three Tor-speciﬁc al-
gorithms that throttle bulk transfers to reduce network
congestion and increase network responsiveness. Unlike
existing techniques, our algorithms adapt to network dy-
namics using only information local to a relay. We exper-
iment with full-network deployments of our algorithms
under a range of light to heavy network loads. We ﬁnd
that throttling results in signiﬁcant improvements to web
client performance while mitigating the negative effects
of bulk transfers. We also analyze how throttling affects
anonymity and compare the security of our algorithms
under adversarial attack. We ﬁnd that throttling reduces
information leakage compared to unthrottled Tor while
improving anonymity against realistic adversaries.

1

Introduction

The Tor [19] anonymity network was developed in an
attempt to improve anonymity on the Internet. Onion
Routing [23,48] serves as the cornerstone for Tor’s over-
lay network design. Tor clients encrypt messages in sev-
eral “layers” while packaging them into 512-byte packets
called cells, and send them through a collection of relays
called a circuit. Each relay decrypts its layer and for-
wards the message to the next relay in the circuit. The
last relay forwards the message to the user-speciﬁed des-
tination. Each relay can determine only its predecessor
and successor in the path from source to destination, pre-
venting any single relay from linking the sender and re-

ceiver. Clients choose their ﬁrst relay from a small set
of entry guards [44, 59] in order to help defend against
passive logging attacks [58]. Trafﬁc analysis is still pos-
sible [8,22,28,30,39,42,46,49], but slightly complicated
by the fact that each relay simultaneously services mul-
tiple circuits.

Tor relays are run by volunteers located throughout
the world and service hundreds of thousands of Tor
clients [37] with high bandwidth demands. A relay’s
utility to Tor is dependent on both the bandwidth ca-
pacity of its host network and the bandwidth restrictions
imposed by its operator. Although bandwidth donations
vary widely, the majority of relays offer less than 100
KiB/s and may become bottlenecks when chosen for a
circuit. Bandwidth bottlenecks lead to network conges-
tion and impair client performance.

Bottlenecks are further aggravated by bulk users,
which make up roughly ﬁve percent of connections and
forty percent of the bytes transferred through the net-
work [38]. Bulk trafﬁc increases network-wide conges-
tion and punishes interactive users as they attempt to
browse the web and run SSH sessions. Bulk trafﬁc also
constitutes a simple denial of service (DoS) attack on
the network as a whole: with nothing but a moderate
number of bulk clients, an adversary can intentionally
signiﬁcantly degrade the performance of the entire Tor
network for most users. This is a malicious attack as op-
posed to an opportunistic use of resources without regard
for the impact on legitimate users, and could be used by
censors [16] to discourage use of Tor. Bulk trafﬁc ef-
fectively averts potential users from Tor, decreasing both
Tor’s client diversity and anonymity [10, 18].

There are three general approaches to alleviate Tor’s
performance problems: increase network capacity; opti-
mize resource utilization; and reduce network load.
Increasing Capacity. One approach to reducing bottle-
necks and improving performance is to add additional
bandwidth to the network from new relays. Previous
work has explored recruiting new relays by offering per-
formance incentives to those who contribute [32, 41, 43].
While these approaches show potential, they have not
been deployed due to a lack of understanding of the
anonymity and economic implications they would im-
pose on Tor and its users. It is unclear how an incentive

1

scheme will affect users’ anonymity and motivation to
contribute: Acquisti et al. [6] discuss how differentiating
users by performance may reduce anonymity while com-
petition may reduce the sense of community and con-
vince users that contributions are no longer warranted.

New high-bandwidth relays may also be added by the
Tor Project [4] or other organizations. While effective
at improving network capacity, this approach is a short-
term solution that does not scale. As Tor increases speed
and bandwidth, it will likely attract more users. More
signiﬁcantly, it will attract more high-bandwidth and Bit-
Torrent users, resulting in a Tragedy of the Commons [26]
scenario: the bulk users attracted to the faster network
will continue to leech the additional bandwidth.
Optimizing Resource Utilization. Another approach to
improving performance is to better utilize the available
network resources. Tor’s path selection algorithm ig-
nores the slowest small fraction of relays while selecting
from the remaining relays in proportion to their available
bandwidth. The path selection algorithm also ignores cir-
cuits with long build times [12], removing the worst of
bottlenecks and improving usability. Congestion-aware
path selection [57] is another approach that aims to bal-
ance load by using opportunistic and active client mea-
surements while building paths. However, low band-
width relays must still be chosen for circuits to mitigate
anonymity problems, meaning there are still a large num-
ber of circuits with tight bandwidth bottlenecks.

Tang and Goldberg previously explored modiﬁcations
to the Tor circuit scheduler in order to prioritize bursty
(i.e. web) trafﬁc over bulk trafﬁc using an exponentially-
weighted moving average (EWMA) of relayed cells [52].
Early experiments show small improvements at a sin-
gle relay, but full-network experiments indicate that
the new scheduler has an insigniﬁcant effect on perfor-
mance [31]. It is unclear how performance is affected
when deployed to the live Tor network. This schedul-
ing approach attempts to shift network load to better uti-
lize the available bandwidth, but does not reduce bottle-
necks introduced by the massive amount of bulk trafﬁc
currently plaguing Tor.
Reducing Load. All of the previously discussed ap-
proaches attempt
to increase performance, but none
of them directly address or provide adequate defense
against performance degradation problems created by
bulk trafﬁc clients.
In this paper, we address these by
adaptively throttling bulk data transfers at the client’s en-
try into the Tor network.

We emphasize that throttling is fundamentally differ-
ent than scheduling, and the distinction is important in
the context of the Tor network. Schedulers optimize the
utilization of available bandwidth by following policies
set by the network engineer, allowing the enforcement
of fairness among ﬂows (e.g. max-min fairness [24, 34]

or proportional fairness [35]). However, throttling may
under-utilize local bandwidth resources by intentionally
imposing restrictions on clients’ throughput to reduce ag-
gregate network load.

By reducing bulk client throughput in Tor, we effec-
tively reduce the bulk data transfer rate through the net-
work, resulting in fewer bottlenecks and a less congested,
more responsive Tor network that can better handle the
burstiness of web trafﬁc. Tor has recently implemented
token buckets, a classic trafﬁc shaping mechanism [55],
to statically (non-adaptively) throttle client-to-guard con-
nections at a given rate [17], but currently deployed con-
ﬁgurations of Tor do not enable throttling by default. Un-
fortunately, the throttling algorithm implemented in Tor
requires static conﬁguration of throttling parameters: the
Tor network must determine network-wide settings that
work well and update them as the network changes. Fur-
ther, it is not possible to automatically tune each relay’s
throttling conﬁguration with the current algorithm.
Contributions. To the best of our knowledge, we are
the ﬁrst to explore throttling algorithms that adaptively
adjust to the ﬂuctuations and dynamics of Tor and each
relay independently without the need to adjust parame-
ters as the network changes. We also perform the ﬁrst
detailed investigation of the performance and anonymity
implications of throttling Tor client connections.

In Section 3, we introduce and test three algorithms
that dynamically and adaptively throttle Tor clients us-
ing a basic token bucket rate-limiter as the underlying
throttling mechanism. Our new adaptive algorithms use
local relay information to dynamically select which con-
nections get throttled and to adjust the rate at which
those connections are throttled. Adaptively tuned throt-
tling mechanisms are paramount to our algorithm de-
signs in order to avoid the need to re-evaluate parame-
ter choices as network capacity or relay load changes.
Our bit-splitting algorithm throttles each connection at
an adaptively adjusted, but reserved and equal portion
of a guard node’s bandwidth, our ﬂagging algorithm ag-
gressively throttles connections that have historically ex-
ceeded the statistically fair throughput, and our thresh-
old algorithm throttles connections above a throughput
quantile at a rate represented by that quantile.

We implement our algorithms in Tor1 and test their
effectiveness at improving performance in large scale,
full-network deployments. Section 4 compares our algo-
rithms to static (non-adaptive) throttling under a varied
range of network loads. We ﬁnd that the effectiveness
of static throttling is highly dependent on network load
and conﬁguration whereas our adaptive algorithms work
well under various loads with no conﬁguration changes
or parameter maintenance: web client performance was

1Software patches for our algorithms have been made publicly

available to the community [5].

2

Figure 1: A Tor relay’s internal architecture.

improved for every parameter setting we tested. We con-
clude that throttling is an effective approach to achieve a
more responsive network.

Having shown that our adaptive throttling algorithms
provide signiﬁcant performance beneﬁts for web clients
and have a profound impact on network responsiveness,
Section 5 analyzes the security of our algorithms under
adversarial attack. We discuss several realistic attacks on
anonymity and compare the information leaked by each
algorithm relative to unthrottled Tor. Against intuition,
we ﬁnd that throttling clients reduces information leak-
age and improves network anonymity while minimizing
the false positive impact on honest users.

2 Background

This section discusses Tor’s internal architecture, shown
in Figure 1, to facilitate an understanding of how internal
processes affect client trafﬁc ﬂowing through a Tor relay.
Multiplexed Connections. All relays in Tor commu-
nicate using pairwise TCP connections, i.e. each relay
forms a single TCP connection to each other relay with
which it communicates. Since a pair of relays may be
communicating data for several circuits at once, all cir-
cuits between the pair are multiplexed over their single
TCP connection. Each circuit may carry trafﬁc for mul-
tiple services or streams that a user may be accessing.
TCP offers reliability, in-order delivery of packets be-
tween relays, and potentially unfair kernel-level conges-
tion control when multiplexing connections [47]. The
distinction between and interaction of connections, cir-
cuits, and streams is important for understanding Tor.
Connection Input. Tor uses libevent [1] to handle input
and output to and from kernel TCP buffers. Tor regis-
ters sockets that it wants to read with libevent and con-
ﬁgures a notiﬁcation callback function. When data ar-
rives at the kernel TCP input buffer (Figure 1a), libevent
learns about the active socket through its polling in-
terface and asynchronously executes the corresponding

callback (Figure 1b). Upon execution, the read callback
determines read eligibility using token buckets.

Token buckets are used to rate-limit connections. Tor
ﬁlls the buckets as deﬁned by conﬁgured bandwidth lim-
its in one-second intervals while tokens are removed
from the buckets as data is read, although changing that
interval to improve performance is currently being ex-
plored [53]. There is a global read bucket that limits
bandwidth for reading from all connections as well as
a separate bucket for throttling on a per-connection ba-
sis (Figure 1c). A connection may ignore a read event
if either the global bucket or its connection bucket is
empty.
In practice, the per-connection token buckets
are only utilized for edge (non-relay) connections. Per-
connection throttling reduces network congestion by pe-
nalizing noisy connections, such as bulk transfers, and
generally leads to better performance [17].

When a TCP input buffer is eligible for reading, a
round-robin (RR) scheduling mechanism is used to read
the smaller of 16 KiB and 1
8 of the global token bucket
size per connection (Figure 1d). This limit is imposed in
an attempt at fairness so that a single connection can not
consume all the global tokens on a single read. However,
recent research shows that input/output scheduling leads
to unfair resource allocations [54]. The data read from
the TCP buffer is placed in a per-connection application
input buffer for processing (Figure 1e).
Flow Control. Tor uses an end-to-end ﬂow control algo-
rithm to assist in keeping a steady ﬂow of cells through
a circuit. Clients and exit relays constitute the edges of
a circuit: each are both an ingress and egress point for
data traversing the Tor network. Edges track data ﬂow
for both circuits and streams using cell counters called
windows. An ingress edge decrements the correspond-
ing stream and circuit windows when sending cells, stops
reading from a stream when its stream window reaches
zero, and stops reading from all streams multiplexed over
a circuit when the circuit window reaches zero. Win-
dows are incremented and reading resumes upon receipt
of SENDME acknowledgment cells from egress edges.

3

By default, circuit windows are initialized to 1000
cells (500 KiB) and stream windows to 500 cells (250
KiB). Circuit SENDMEs are sent to the ingress edge af-
ter the egress edge receives 100 cells (50 KiB), allowing
the ingress edge to read, package, and forward 100 ad-
ditional cells. Stream SENDMEs are sent after receiving
50 cells (25 KiB) and allow an additional 50 cells. Win-
dow sizes can have a signiﬁcant effect on performance
and recent work suggests an algorithm for dynamically
computing them [7].
Cell Processing and Queuing. Data is immediately
processed as it arrives in connection input buffers (Fig-
ure 1f) and each cell is either encrypted or decrypted de-
pending on its direction through the circuit. The cell is
then switched onto the circuit corresponding to the next
hop and placed into the circuit’s ﬁrst-in-ﬁrst-out (FIFO)
queue (Figure 1g). Cells wait in circuit queues until the
circuit scheduler selects them for writing.
Scheduling. When there is space available in a con-
nection’s output buffer, a relay decides which of sev-
eral multiplexed circuits to choose for writing. Al-
though historically this was done using round-robin, a
new exponentially-weighted moving average (EWMA)
scheduler was recently introduced into Tor [52] and is
currently used by default (Figure 1h). EWMA records
the number of packets it schedules for each circuit, expo-
nentially decaying packet counts over time. The sched-
uler writes one cell at a time chosen from the circuit with
the lowest packet count and then updates the count. The
decay means packets sent more recently have a higher
inﬂuence on the count while bursty trafﬁc does not sig-
niﬁcantly affect scheduling priorities.
Connection Output. A cell
that has been chosen
and written to a connection output buffer (Figure 1i)
causes an activation of the write event registered with
libevent for that connection. Once libevent determines
the TCP socket can be written, the write callback is asyn-
chronously executed (Figure 1j). Similar to connection
input, the relay checks both the global write bucket and
per-connection write bucket for tokens.
If the buckets
are not empty, the connection is eligible for writing (Fig-
ure 1k) and again is allowed to write the smaller of 16
KiB and 1
8 of the global token bucket size per connection
(Figure 1l). The data is written to the kernel-level TCP
buffer (Figure 1m) and sent to the next hop.

3 Throttling Client Connections

Client performance in Tor depends heavily on the traf-
ﬁc patterns of others in the system. A small number of
clients performing bulk transfers in Tor are the source
of a large fraction of total network trafﬁc [38]. The
overwhelming load these clients place on the network
increases congestion and creates additional bottlenecks,

Figure 2: Throttling occurs at the connection between the
client and guard to capture all streams to various destinations.

causing interactive applications, such as instant messag-
ing and remote SSH sessions, to lose responsiveness.

This section explores client throttling as a mechanism
to prevent bulk clients from overwhelming the network.
Although a relay may have enough bandwidth to han-
dle all trafﬁc locally, bulk clients that continue producing
additional trafﬁc cause bottlenecks at other low-capacity
relays. The faster a bulk downloader gets its data, the
faster it will pull more into the network. Throttling bulk
and other high-trafﬁc clients prevents them from pushing
or pulling too much data into the network too fast, reduc-
ing these bottlenecks and improving performance for the
majority of users. Therefore, interactive applications and
Tor in general will become much more usable, attracting
new users who improve client diversity and anonymity.
We emphasize that throttling algorithms are not a re-
placement for congestion control or scheduling algo-
rithms, although each approach may cooperate to achieve
a common goal. Scheduling algorithms are used to man-
age the utilization of bandwidth, throttling algorithms re-
duce the aggregate network load, and congestion con-
trol algorithms attempt to do both. The distinction be-
tween congestion control and throttling algorithms is
subtle but important: congestion control reduces circuit
load while attempting to maximize network utilization,
whereas throttling reduces network load in an attempt to
improve circuit performance by explicitly under-utilizing
connections to bulk clients using too many resources.
Each approach may independently affect performance,
and they may be combined to improve the network.

3.1 Static Throttling
Recently, Tor introduced the functionality to allow entry
guards to throttle connections to clients [17] (see Fig-
ure 2). This client-to-guard connection is targeted be-
cause all client trafﬁc (using this guard) will ﬂow over
this connection regardless of the number of streams or
the destination associated with each.2 The implemen-
tation uses a token bucket for each connection in addi-
tion to the global token bucket that already limits the to-
tal amount of bandwidth used by a relay. The size of
the per-connection token buckets can be speciﬁed us-
ing the PerConnBWBurst conﬁguration option, and
the bucket reﬁll rate can be speciﬁed by conﬁguring the
PerConnBWRate. The conﬁgured throttling rate en-

2This work does not consider modiﬁed Tor clients.

4

sures that all client-to-guard connections are throttled
to the speciﬁed long-term-average throughput while the
conﬁgured burst allows deviations from the throttling
rate to account for bursty trafﬁc. The conﬁguration op-
tions provide a static throttling mechanism: Tor will
throttle all connections using these values until directed
otherwise. Note that Tor does not enable or conﬁgure
static throttling by default.

While static throttling is simple, it has two main draw-
backs. First, static throttling requires constant monitor-
ing and measurements of the Tor network to determine
which conﬁgurations work well and which do not in or-
der to be effective. We have found that there are many
conﬁgurations of the algorithm that cause no change in
performance, and worse, there are conﬁgurations that
harm performance for interactive applications [33]. This
is the opposite of what throttling is attempting to achieve.
Second, it is not possible under the current algorithm
to auto-tune the throttling parameters for each Tor relay.
Conﬁgurations that appear to work well for the network
as a whole might not necessarily be tuned for a given
relay (we will show that this is indeed the case in Sec-
tion 4). Each relay has very different capabilities and
load patterns, and therefore may require different throt-
tling conﬁgurations to be most useful.

3.2 Adaptive Throttling
Given the drawbacks of static throttling, we now explore
and present three new algorithms that adaptively adjust
throttling parameters according to local relay informa-
tion. This section details our algorithms while Section 4
explores their effect on client performance and Section 5
analyzes throttling implications for anonymity.

There are two main issues to consider when design-
ing a client throttling algorithm: which connections to
throttle and at what rate to throttle them. The approach
discussed above in Section 3.1 throttles all client con-
nections at the statically speciﬁed rate. Each of our three
algorithms below answers these questions adaptively by
considering information local to each relay. Note that our
algorithms dynamically adjust the PerConnBWRate
while keeping a constant PerConnBWBurst.3
Bit-splitting. A simple approach to adaptive throttling
is to split a guard’s bandwidth equally among all active
client connections and throttle them all at this fair split
rate. The PerConnBWRate will therefore be adjusted
as new connections are created or old connections are
destroyed: more connections will result in lower rates.
No connection will be able to use more than its allot-

3Our experiments [33] indicate that a 2 MiB burst is ideal as it al-
lows directory requests to be downloaded unthrottled during bootstrap-
ping while also throttling bulk trafﬁc relatively quickly. The burst may
need to be increased if the directory information grows beyond 2 MiB.

Algorithm 1 Throttling clients by splitting bits.
1: B ← getRelayBandwidth()
2: L ← getConnectionList()
3: N ← L.length()
4: if N > 0 then
splitRate ← B
5:
N
for i ← 1 to N do
6:
7:
8:
9:
10:
11: end if

if L[i].isClientConnection() then
L[i].throttleRate ← splitRate

end if
end for

ted share of bandwidth unless it has unused tokens in its
bucket. Inspired by Quality of Service (QoS) work from
communication networks [11, 50, 60], bit-splitting will
prevent bulk clients from unfairly consuming bandwidth
and ensure that there is a minimum “reserved” bandwidth
for clients of all types.

Note that Internet Service Providers employ similar
techniques to throttle their customers, however, their
client base is much less dynamic than the connections an
entry guard handles. Therefore, our adaptive approach is
more suitable to Tor. We include this algorithm in our
analysis of throttling to determine what is possible with
such a simple approach.
Flagging Unfair Clients. The bit-splitting algorithm fo-
cuses on adjusting the throttle rate and applying this to
all client connections. Our next algorithm takes the op-
posite approach: conﬁgure a static throttling rate and ad-
just which connections get throttled. The intuition be-
hind this approach is that if we can properly identify the
connections that use too much bandwidth, we can throttle
them in order to maximize the beneﬁt we gain per throt-
tled connection. Therefore, our ﬂagging algorithm at-
tempts to classify and throttle bulk trafﬁc while it avoids
throttling web clients.

Since deep packet inspection is not desirable for pri-
vacy reasons, and is not possible on encrypted Tor trafﬁc,
we instead draw upon existing statistical ﬁngerprinting
classiﬁcation techniques [14, 29, 36] that classify trafﬁc
solely on its statistical properties. When designing the
ﬂagging algorithm, we recognize that Tor already con-
tains a statistical throughput measure for scheduling traf-
ﬁc on circuits using an exponentially-weighted moving
average (EWMA) of recently sent cells [52]. We can use
the same statistical measure on client connections to clas-
sify and throttle bulk trafﬁc.

The ﬂagging algorithm, shown in Algorithm 2, re-
quires that each guard keeps an EWMA of the number
of recently sent cells per client connection. The per-
connection cell EWMA is computed in much the same
way as the per-circuit cell EWMA: whenever the cir-

5

Algorithm 2 Throttling clients by ﬂagging bulk connections,
considering a moving average of throughput.
Require: f lagRate,P,H
1: B ← getRelayBandwidth()
2: L ← getConnectionList()
3: N ← L.length()
4: M ← getMetaEW MA()
5: if N > 0 then
6:
7: M ← M.increment(H,splitRate)
8:
9:
10:
11:
12:
13:

L[i]. f lag ← True
L[i].throttleRate ← f lagRate
L[i].EW MA < P ·M then

splitRate ← B
N
for i ← 1 to N do

else if L[i]. f lag = True ∧

if L[i].isClientConnection() then

if L[i].EW MA > M then

L[i]. f lag ← False
L[i].throttleRate ← in f inity

14:
15:
16:
17:
18:
19: end if

end if

end if
end for

7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end if

end if
end for

Algorithm 3 Throttling clients considering the loudest thresh-
old of connections.
Require: T ,R,F
1: L ← getClientConnectionList()
2: N ← L.length()
3: if N > 0 then
4:
5:
6:

selectIndex ← f loor(T · N)
L ← reverseSortEW MA(L)
thresholdRate ← L[selectIndex].
if thresholdRate < F then

getMeanT hroughput(R)

thresholdRate ← F

end if
for i ← 1 to N do

if i ≤ selectIndex then

L[i].throttleRate ← thresholdRate
L[i].throttleRate ← in f inity

else

cuit’s cell counter is incremented, so is the cell counter
of the connection to which that circuit belongs. Note
that clients can not affect others’ per-connection EWMA
since all of a client’s circuits are multiplexed over a
single throttled guard-to-client connection.4 The per-
connection EWMA is enabled and conﬁgured indepen-
dently of its circuit counterpart.

We rely on the observation that bulk connections will
have higher EWMA values than web connections since
bulk clients are steadily transferring data while web
clients “think” between each page download. Using this
to our advantage, we can ﬂag connections as containing
bulk trafﬁc as follows. Each relay keeps a single sepa-
rate meta-EWMA M of cells transferred. M is adjusted
by calculating the fair bandwidth split rate as in the bit-
splitting algorithm, and tracking its EWMA over time.
M does not correspond with any real trafﬁc, but rep-
resents the upper bound of a connection-level EWMA
if a connection were continuously sending only its fair
share of trafﬁc through the relay. Any connection whose
EWMA exceeds M is ﬂagged as containing bulk trafﬁc
and penalized by being throttled.
There are three main parameters for the algorithm. As
mentioned above, a per-connection half-life H allows
conﬁguration of the connection-level half-life indepen-
dent of that used for circuit scheduling. H affects how

4The same is not true for the unthrottled connections between relays
since each of them contain several circuits and each circuit may belong
to a different client (see Section 2).

6

long the algorithm remembers the amount of data a con-
nection has transferred, and has precisely the same mean-
ing as the circuit priority half-life [52]. Larger half-life
values increase the ability to differentiate bulk from web
connections while smaller half-life values make the algo-
rithm more immediately reactive to throttling bulk con-
nections. We would like to allow for a speciﬁcation of
the length of each penalty once a connection is ﬂagged
in order to recover and stop throttling connections that
may have been incorrectly ﬂagged. Therefore, we intro-
duce a penalty fraction parameter P that affects how long
each connection remains in a ﬂagged and throttled state.
If a connection’s cell count EWMA falls below P ·M,
its ﬂag is removed and the connection is no longer throt-
tled. Finally, the rate at which each ﬂagged connection is
throttled, i.e. the FlagRate, is statically deﬁned and is
not adjusted by the algorithm.

Note that the ﬂagging parameters need only be set
based on system-wide policy and generally do not re-
quire independent relay tuning, but provides the ﬂexi-
bility to allow individual relay operators to deviate from
system policy if they desire.
Throttling Using Thresholds. Recall the two main is-
sues a throttling algorithm must address: selecting which
connections to throttle and the rate at which to throttle
them. Our bit-splitting algorithm explored adaptively
adjusting the throttle rate and applying this to all con-
nections while our ﬂagging algorithm explored statically
conﬁguring a throttle rate and adaptively selecting the
throttled connections. We now describe our ﬁnal algo-
rithm which attempts to adaptively address both issues.

The threshold algorithm also makes use of a
connection-level cell EWMA, which is computed as de-
scribed above for the ﬂagging algorithm. However,
EWMA is used here to sort connections by the loudest
to quietest. We then select and throttle the loudest frac-
tion T of connections, where T is a conﬁgurable thresh-
old. For example, setting T to 0.1 means the loudest ten
percent of client connections will be throttled. The se-
lection is adaptive since the EWMA changes over time
according to each connection’s bandwidth usage.

We have adaptively selected which connections to
throttle and now must determine a throttle rate. To do
this, we require that each connection tracks its through-
put over time. We choose the average throughput rate
of the connection with the minimum EWMA from the
set of connections being throttled. For example, when T
= 0.1 and there are 100 client connections sorted from
loudest to quietest, the chosen throttle rate is the average
throughput of the tenth connection. Each of ﬁrst ten con-
nections is then throttled at this rate. In our prototype,
we approximate the throughput rate as the average num-
ber of bytes transferred over the last R seconds, where
R is conﬁgurable. R represents the period of time be-
tween which the algorithm re-selects the throttled con-
nections, adjusts the throttle rates, and resets each con-
nection’s throughput counters.

There is one caveat to the algorithm as described
above.
In our experiments in Section 4, we noticed
that occasionally the throttle rate chosen by the thresh-
old algorithm was zero. This would happen if the mean
throughput of the threshold connection (line 6 in Algo-
rithm 3) did not send data over the last R seconds. To
prevent a throttle rate of zero, we added a parameter to
statically conﬁgure a throttle rate ﬂoor F so that no con-
nection would ever be throttled below F. Algorithm 3
details threshold adaptive throttling.

4 Experiments

In this section we explore the performance beneﬁts possi-
ble with each throttling algorithm speciﬁed in Section 3.
We perform experiments with Shadow [2, 31], an accu-
rate and efﬁcient discrete event simulator that runs real
Tor code over a simulated network. Shadow allows us to
run an entire Tor network on a single machine and conﬁg-
ure characteristics such as network latency, bandwidth,
and topology. Since Shadow runs real Tor, it accurately
characterizes application behavior and allows us to focus
on experimental comparison of our algorithms. A direct
comparison between Tor and Shadow-Tor performance
is presented in [31].
Experimental Setup. Using Shadow, we conﬁgure a pri-
vate Tor network with 200 HTTP servers, 950 Tor web
clients, 50 Tor bulk clients, and 50 Tor relays. The dis-

tribution of clients in our experiments approximates that
found by McCoy et al. [38]. All of our nodes run inside
the Shadow simulation environment.

In our experiments, each client node runs Tor in client-
only mode as well as an HTTP client application conﬁg-
ured to download over Tor’s SOCKS proxy available on
the local interface. Each web client downloads a 320 KiB
ﬁle5 from a randomly selected one of our HTTP servers,
and pauses for a length of time drawn from the UNC
“think time” data set [27] before downloading the next
ﬁle. Each bulk client repeatedly downloads a 5 MiB ﬁle
from a randomly selected HTTP server without pausing.
Clients track the time to the ﬁrst and the last byte of the
download as indications of network responsiveness and
overall performance.

Tor relays are conﬁgured with bandwidth parameters
according to a Tor network consensus document.6 We
conﬁgure our network topology and latency between
nodes according to the geographical distribution of re-
lays and pairwise PlanetLab node ping times. Our sim-
ulated network mirrors a previously published Tor net-
work model [31] that has been compared to and shown to
closely approximate the load of the live Tor network [3].
We focus on the time to the ﬁrst data byte for web
clients as a measure of network responsiveness, and
the time to the last data byte—the download time—for
both web and bulk clients as a measure of overall per-
formance.
In our results, “vanilla” represents unmod-
iﬁed Tor using a round-robin circuit scheduler and no
throttling—the default settings in the Tor software—and
can be used to compare relative performance between
experiments. Each experiment uses network-wide de-
ployments of each conﬁguration. To further reduce ran-
dom variances, we ran all conﬁgurations ﬁve times each.
Therefore, every curve on every CDF shows the cumula-
tive results of ﬁve experiments.
Results. Our results focus on the algorithmic conﬁg-
urations that we found to maximize web client perfor-
mance [33] while we show how the algorithms perform
when the network load varies from light (25 bulk clients)
to medium (50 bulk clients) to heavy (100 bulk clients).
The experimental setup is otherwise unmodiﬁed from the
model described above. Running the algorithms under
various loads allows us to highlight the unique and novel
features each provides.

Figure 3 shows client performance for our algorithms.
The time to ﬁrst byte indicates network responsiveness
for web clients while the download time indicates overall
client performance for web and bulk clients. Client per-
formance is shown for the lightly loaded network in Fig-
ures 3a–3c, the normally loaded network in Figures 3d–
3f, and the heavily loaded network in Figures 3g–3i.

5The average webpage size reported by Google web metrics [45].
6Retrieved on 2011-04-27 and valid from 03-06:00:00

7

(a) 320 KiB clients, light load

(b) 320 KiB clients, light load

(c) 5 MiB clients, light load

(d) 320 KiB clients, medium load

(e) 320 KiB clients, medium load

(f) 5 MiB clients, medium load

(g) 320 KiB clients, heavy load

(h) 320 KiB clients, heavy load

(i) 5 MiB clients, heavy load

Figure 3: Comparison of client performance for each throttling algorithm and vanilla Tor, under various load. All experiments use
950 web clients. We vary the load between “light,” “medium,” and “heavy” by setting the number of bulk clients to 25 for 3a–3c,
to 50 for 3d–3f, and to 100 for 3g–3i. The time to ﬁrst byte indicates network responsiveness while the download time indicates
overall client performance. The parameters for each algorithm are tuned based on experiments presented in [33].

Overall, static throttling results in the least amount of
bulk trafﬁc throttling while providing the lowest bene-
ﬁt to web clients. For the bit-splitting algorithm, we
see improvements over static throttling for web clients
for both time to ﬁrst byte and overall download times,
while download times for bulk clients are also slightly
increased. Flagging and threshold throttling perform
somewhat more aggressive throttling of bulk trafﬁc and
therefore also provide the greatest improvements in web
client performance.

We ﬁnd that each algorithm is effective at throttling
bulk clients independent of network load, as evident in
Figures 3c, 3f and 3i. However, performance beneﬁts for
web clients vary slightly as the network load changes.
When the number of bulk clients is halved, throughput
in Figure 3b is fairly similar across algorithms. How-

ever, when the number of bulk clients is doubled, re-
sponsiveness in Figure 3g and throughput in Figure 3h
for both the static throttling and the adaptive bit-splitting
algorithm lag behind the performance of the ﬂagging and
threshold algorithms. Static throttling would likely re-
quire a reconﬁguration of throttling parameters while bit-
splitting adjusts the throttle rate less effectively than our
ﬂagging and threshold algorithms.

As seen in Figures 3a, 3d, and 3g, as the load
changes, the strengths of each algorithm become appar-
ent. The ﬂagging and threshold algorithms stand out as
the best approaches for both web client responsiveness
and throughput, and Figures 3c, 3f, and 3i show that
they are also most aggressive at throttling bulk clients.
The ﬂagging algorithm appears very effective at accu-
rately classifying bulk connections regardless of network

8

02468101214WebTimetoFirstByte(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh051015202530WebDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh0100200300400500BulkDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh02468101214WebTimetoFirstByte(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh051015202530WebDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh0100200300400500BulkDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh02468101214WebTimetoFirstByte(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh051015202530WebDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh0100200300400500BulkDownloadTime(s)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthreshh
g
i
l

t Data (GiB)
Web (%)
Bulk (%)
m Data (GiB)
u
Web (%)
i
d
e
Bulk (%)
m
y Data (GiB)
Web (%)
Bulk (%)

v
a
e
h

vanilla
88.3
74.5
25.5
92.2
65.8
34.2
94.7
55.8
44.2

static
80.3
83.7
16.3
88.6
72.4
27.6
91.1
60.5
39.5

split
78.3
85.9
14.1
84.7
75.0
25.0
85.0
64.3
35.7

ﬂag
72.1
92.7
7.3
77.7
86.2
13.8
81.7
75.4
24.6

thresh
69.8
90.1
9.9
76.3
82.8
17.2
85.0
71.2
28.8

Table 1: Total data downloaded in our simulations by client
type. Throttling reduces the bulk trafﬁc share of the load on the
network. The ﬂagging algorithm is the best at throttling bulk
trafﬁc under light, medium, and heavy loads of 25, 50, and 100
bulk clients, respectively.

load. The threshold algorithm maximizes web client per-
formance in our simulations among all loads and all al-
gorithms tested, since it effectively throttles the worst
bulk clients while utilizing extra bandwidth when possi-
ble. Both the threshold and ﬂagging algorithms perform
well over all network loads tested, and their usage in Tor
would require little-to-no maintenance while providing
signiﬁcant performance improvements for web clients.

Aggregate download statistics are shown in Table 1.
The results indicate that we are approximating the load
distribution measured by McCoy et al. [38] reasonably
well. The data also indicates that as the number of
bulk clients in our simulation increases, so does the total
amount of data downloaded and the bulk fraction of the
total as expected. The data also shows that all throttling
algorithms reduce the total network load. Static throt-
tling reduces load the least, while our adaptive ﬂagging
algorithm is both the best at reducing both overall load
and the bulk percentage of network trafﬁc. Each of our
adaptive algorithms are better at reducing load than static
throttling, due to their ability to adapt to network dynam-
ics. The relative difference between each algorithm’s ef-
fectiveness at reducing load roughly corresponds to the
relative difference in web client performance in our ex-
periments, as we discussed above.
Discussion. The best algorithm for Tor depends on mul-
tiple factors. Although not maximizing web client per-
formance, bit-splitting is the simplest, the most efﬁcient,
and the most network neutral approach (every connec-
tion is allowed the same portion of a guard’s capacity).
This “subtle” or “delicate” approach to throttling may be
favorable if supporting multiple client behaviors is de-
sirable. Conversly, the ﬂagging algorithm may be used
to identify a speciﬁc class of trafﬁc and throttle it ag-
gressively, creating the potential for the largest increase
in performance for unthrottled trafﬁc. We are currently
exploring improvements to our statistical classiﬁcation
techniques to reduce false positives and to improve the

9

control over trafﬁc of various types. For these reasons,
we feel the bit-splitting and ﬂagging algorithms will be
the most useful in various situations. We suggest that
perhaps bit-splitting is the most appropriate throttling al-
gorithm to use initially, even if something more aggres-
sive is desirable in the long term.

While requiring little maintenance, our algorithms
were designed to use only local relay information.
Therefore, they are incrementally deployable while re-
lay operators may choose the desired throttling algorithm
independent of others. Our algorithms are already imple-
mented in Tor and software patches are available [5].

5 Analysis and Discussion

Having shown the performance beneﬁts of throttling bulk
clients in Section 4, we now analyze the security of
throttling against adversarial attacks on anonymity. We
will discuss the direct impact of throttling on anonymity:
what an adversary can learn when guards throttle clients
and how the information leaked affects the anonymity of
the system. We lastly discuss potential strategies clients
may use to elude the throttles.

Before exploring practical attacks, we introduce two
techniques an adversary may use to gather information
about the network given that a generic throttling algo-
rithm is enabled at all guards. Similar techniques used
for throughput-based trafﬁc analysis outside the context
of throttling are discussed in detail by Mittal et al. [39].
Discussion about the security of our throttling algorithms
in the context of practical attacks will follow.

5.1 Gathering Information
Our analysis uses the following terminology. At time t,
the throughput of a connection between a client and a
guard is λt, the rate at which the client will be throttled is
αt, and the allowed data burst is β . Note that, as consis-
tent with our algorithms, the throttle rate may vary over
time but the burst is a static system-wide parameter.
Probing Guards. Using the above terminology, a con-
nection is throttled if, over the last s seconds, its through-
put exceeds the allowed initial burst and the long-term
throttle rate:

t

∑
k=t−s

(λk) ≥ β +

t

∑
k=t−s

(αk)

(1)

A client may perform a simple technique to probe a spe-
ciﬁc guard node and determine the rate at which it gets
throttled. The client may open a single circuit through
the guard, selecting other high-bandwidth relays to en-
sure that the circuit does not contain a bottleneck. Then,
it may download a large ﬁle and observe the change in
throughput after receiving a burst of β payload bytes.

(a)

(b)

(c)

Figure 4: 4a: Client’s may discover the throttle rate by probing guards. 4b: Information leaked by learning circuit throughputs.
4c: Information leaked by learning guards’ throttle rates.

If the ﬁrst β bytes are received at time t1 and the
download ﬁnishes at time t2 ≥ t1, the throttle rate at any
time t in this interval can be approximated by the mean
throughput leading up to t:

∀t ∈ [t1,t2], αt ≈ ∑t

(λk)

k=t1
t −t1

(2)

Therefore, αt2 approximates the actual throttle rate. Note
that this approximation may under-estimate the actual
throttle rate if the throughput falls below the throttle rate
during the measured interval.

We simulate probing in Shadow [2, 31] to show its ef-
fectiveness against the static throttling algorithm. As ap-
parent in Figure 4a, the throttle rate was conﬁgured at 5
KiB/s and the burst at 2 MiB. With enough resources, an
adversary may probe every guard node to form a com-
plete list of throttle rates.
Testing Circuit Throughput. A web server may deter-
mine the throughput of a connecting client’s circuit by
using a technique similar to that presented by Hopper
et al. [30]. When the server gets an HTTP request from
a client, it may inject either special JavaScript or a large
amount of garbage HTML into a form element included
in the response. The injected code will trigger a second
client request after the original response is received. The
server may adjust the amount of returned data and mea-
sure the time between when it sent the ﬁrst response and
when it received the second request to approximate the
throughput of the circuit.

5.2 Adversarial Attacks
We now explore several adversarial attacks in the con-
text of client throttling algorithms, and how an adversary
may use those attacks to learn information and affect the
anonymity of a client.
Attack 1. In our ﬁrst attack, an adversary obtains a dis-
tribution on throttle rates by probing all Tor guard relays.

10

We assume the adversary has resources to perform such
an attack, e.g. by utilizing a botnet or other distributed
network such as PlanetLab [13]. The adversary then ob-
tains access to a web server and tests the throughput of a
target circuit. With this information, the adversary may
reduce the anonymity set of the circuit’s potential guards
by eliminating those whose throttle rate is inconsistent
with the measured circuit throughput.

This attack is somewhat successful against all of
the throttling algorithms we have described. For bit-
splitting, the anonymity set of possible guard nodes will
consist of those whose bandwidth and number of active
connections would throttle to the throughput of the target
circuit or higher. By running the attack repeatedly over
time, an intersection will narrow the set to those whose
throttle rate is consistent with the target circuit through-
put at all measured times.

The ﬂagging algorithm throttles all ﬂagged connec-
tions to the same rate system-wide.
(We assume here
that the set of possible guards is already narrowed to
those whose bandwidth is consistent with the target cir-
cuit’s throughput irrespective of throttling.) A circuit
whose throughput matches the system-wide rate is either
ﬂagged at some guard or just coincidentally matches the
system-wide rate and is not ﬂagged because its EWMA
has remained below the splitRate (see Algorithm 2)
for its guard long enough to not be ﬂagged or become
unﬂagged. The throttling rate is thus not nearly as infor-
mative as for bit-splitting. If we run the attack repeatedly
however, we can eliminate from the anonymity set any
guard such that the EWMA of the target circuit should
have resulted in a throttling but did not. Also, if the
EWMA drops to the throttling rate at precise times (ig-
noring unusual coincidence), we can eliminate any guard
that would not have throttled at precisely those times.
Note that this determination must be made after the fact
to account for the burst bucket of the target circuit, but it
can still be made precisely.

0102030405060Time(m)0510152025303540Throughput(KiBps)0.00.51.01.52.02.53.0EntropyLost(bits)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthresh0.00.51.01.52.02.53.0EntropyLost(bits)0.00.20.40.60.81.0CumulativeFractionvanillastaticsplitﬂagthreshThe potential for information going to the attacker in
the threshold algorithm is a combination of the potential
in each of the above two algorithms. The timing of when
a circuit gets throttled (or does not when it should have
been) can narrow the anonymity set of entry guards as in
the ﬂagging algorithm. Once the circuit has been throt-
tled, then any ﬂuctuation in the throttling rate that sepa-
rates out the guard nodes can be used to further narrow
the set. Note that if a circuit consistently falls below the
throttling rate of all guards, an attacker can learn nothing
about its possible entry guard from this attack. Attack 2
considerably improves the situation for the adversary.

We simulated this attack in Shadow [2, 31]. An ad-
versary probes all guards and forms a distribution on the
throttle rate at which a connection would become throt-
tled. We then form a distribution on circuit throughputs
over each minute, and remove any guard whose throttle
rate is outside a range of one standard deviation of those
throughputs. Since there are 50 guards, the maximum
entropy is log2(50) ≈ 5.64; the entropy lost by this at-
tack for various throttling algorithms relative to vanilla
Tor is shown in Figure 4b. We can see that the static
algorithm actually loses no information, since all con-
nections are throttled to the same rate, while vanilla Tor
without throttling actually loses more information than
any of the throttling algorithms. Therefore, the distri-
bution on guard bandwidth leaks more information than
throttled circuits’ throughputs.
Attack 2. As in Attack 1, the adversary again obtains
a distribution on throttle rates of all guards in the sys-
tem. However, the adversary slightly modiﬁes its circuit
testing by continuously sending garbage responses. The
adversary adjusts the size of each response so that it may
compute the throughput of the circuit over time and ap-
proximates the rate at which the circuit is throttled. By
comparing the estimated throttle rate to the distribution
on guard throttle rates, the adversary may again reduce
the anonymity set by removing guards whose throttle rate
is inconsistent with the estimated circuit throttle rate.

For bit-splitting, by raising and lowering the rate of
garbage sent, the attacker can match this with the throt-
tled throughput of each guard. The only guards in the
anonymity set would be those that share the same throt-
tling rate that matches the ﬂooded circuit’s throughput
at all times. To maximize what he can learn from ﬂag-
ging, the adversary should raise the EWMA of the target
circuit at a rate that will allow him to maximally differ-
entiate guards with respect to when they would begin to
throttle a circuit. If this does not uniquely identify the
guard, he can also use the rate at which he diminishes
garbage trafﬁc to try to learn more from when the tar-
get circuit stops being throttled. As in Attack 1 from the
threshold algorithm, the adversary can match the signa-
ture of both ﬂuctuations in throttling rate over time and

the timing of when throttling is applied to narrow the set
of possible guards for a target circuit.

We simulated this attack using the same data set as
Attack 1. Figure 4c shows that a connection’s throttle
rate generally leaks slightly more information than its
throughput. As in Attack 1, guards’ bandwidth in our
simulation leaks more information than the throttle rate
of each connection for all but the ﬂagging algorithm.
Attack 3. An adversary controlling two malicious
servers can link streams of a client connecting to each
of them at the same time. The adversary uses the circuit
testing technique to send a response of β
2 bytes in size to
each of two requests. Then, small “test” responses are re-
turned after receiving the clients’ second requests. If the
throughput of each circuit when downloading the “test”
response is consistently throttled, then it is possible that
the requests are coming from the same client. This at-
tack relies on the observation that all trafﬁc on the same
client-to-guard connection will be throttled at the same
time since each connection has a single burst bucket.

This attack is intended to indicate if and when a circuit
is throttled, rather than the throttling rate. It will there-
fore not be effective against bit splitting, but will work
against ﬂagging or threshold throttling.
Attack 4. Our ﬁnal attack is an active denial of service
attack that can be used to conﬁrm a circuit’s entry guard
with high probability.
In this attack, the adversary at-
tempts to adjust the throttle rate of each guard in order
to identify whether it carries a target circuit. An adver-
sary in control of a malicious server may monitor the
throughput of a target circuit over time, and may then
open a large number of connections to each guard node
until a decrease in the target circuit’s throughput is ob-
served. To conﬁrm that a guard is on the target circuit,
the adversary can alternate between opening and closing
guard connections and continue to observe the through-
put of the target circuit. If the throughput is consistent
with the adversary’s behavior, it has found the circuit’s
guard with high probability.

The one thing not controlled by the adversary in
Attack 2 is a guard’s criterion for throttling at a
given time – splitRate for bit splitting and ﬂagging
and selectIndex for threshold throttling (see Algo-
rithms 1, 2, and 3). All of these are controlled by the
number of circuits at the guard, which Attack 4 places
under the control of the adversary. Thus, under Attack 4,
the adversary will have precise control over which cir-
cuits get throttled at which rate at all times and can there-
fore uniquely determine the entry guard.

Note that all of Attacks 1, 2, and 4 are intended to
learn about the possible entry guards for an attacked cir-
cuit. Even if completely successful, this does not fully
de-anonymize the circuit. But since guards themselves
are chosen for persistent use by a client, they can add

11

to pseudonymous proﬁling and can be combined with
other information, such as that uncovered by Attack 3,
to either reduce anonymity of the client or build a richer
pseudonymous proﬁle of it.

5.3 Eluding Throttles

A client may try multiple strategies to avoid being throt-
tled. A client may instrument its downloading applica-
tion and the Tor software to send application data over
multiple Tor circuits. However, these circuits will still be
subject to throttling since each of them uses the same
throttled TCP connection to the guard. A client may
avoid this by attempting to create multiple TCP con-
nections to the guard. In this case, the guard may eas-
ily recognize that the connection requests come from
the same client and can either deny the establishment
of multiple connections or aggregate the accounting of
all connections to that client. A client may use multi-
ple guard nodes and send application data over each sep-
arate guard connection, but the client signiﬁcantly de-
creases its anonymity by subverting the guard mecha-
nism [58, 59]. Finally, the client could run and use its
own guard node and avoid throttling itself. Although this
strategy may actually beneﬁt the network since it reduces
the amount of Tor’s capacity consumed by the client, the
cost of running a guard may be sufﬁcient to prevent its
wide-scale adoption.

Its important to note that the “cheating” techniques
outlined above do not decrease the security or perfor-
mance below what unthrottled Tor provides. At worst,
even if all clients somehow manage to elude the throttles,
performance and security both regress to that of unthrot-
tled Tor. In other words, throttling can only improve the
situation whether or not “cheating” occurs in practice.

6 Related Work

6.1

Improving Tor’s Performance

Recent work on improving Tor’s performance covers a
wide range of topics, which we now enumerate.
Incentives. A recognition that Tor is limited by its band-
width resources has resulted in several proposals for de-
veloping performance incentives for volunteering band-
width as a Tor relay. New relays would provide ad-
ditional resources and improve network performance.
Ngan et al. explore giving better performance to re-
lays that attain the fast and stable relay ﬂags [43].
These relays are marked with a “gold star” in the di-
rectory. Gold star relays may build circuits through
other gold star relays, improving download performance.
This scheme has a severe anonymity problem: any relay

on a gold star circuit can determine with absolute cer-
tainty that the client is also a gold star relay.
Jansen
et al. explore reducing anonymity problems from the
gold star approach by distributing anonymous tickets to
all clients [32]. Relays then collect tickets from clients in
exchange for prioritized service and can prioritize their
own trafﬁc in return. However, a centralized bank lim-
its the allowable number of tickets in circulation, leading
to spending strategies that may reduce anonymity. Fi-
nally, Moore et al. independently explored using static
throttling conﬁgurations as a way to produce incentives
for users to run relays in Tortoise [41]. Tortoise’s throt-
tling conﬁgurations must be monitored as network load
changes, and anonymity with Tortoise is slightly worse
than with the gold star scheme: the intersection attack
is improved since gold star nodes retain their gold stars
for several months after dropping from the consensus,
whereas Tortoise only unthrottles nodes that are in the
current consensus.
Relay Selection. Snader and Borisov [51] suggest an
algorithm where relays opportunistically measure their
peers’ performance, allowing clients to use empirical ag-
gregations to select relays for their circuits. A user-
tunable mechanism for selecting relays is built into the
algorithm: clients may adjust how often the fast re-
lays get chosen, trading off anonymity and performance
while not signiﬁcantly reducing either.
It was shown
that this approach increases accuracy of available band-
width estimates and reduces reaction time to changes
in network load while decreasing vulnerabilities to low-
resource routing attacks. Wang et al.
[57] propose a
congestion-aware path selection algorithm where clients
choose paths based on information gathered during op-
portunistic and active measurements of relays. Clients
use latency as an indication of congestion, and reject con-
gested relays when building circuits. Improvements were
realized for a single client, but its unclear how the new
strategy would affect the network if used by all clients.
Scheduling. Alternative scheduling approaches have re-
cently gained interest. Tang and Goldberg [52] sug-
gest each relay track the number of packets it sched-
ules for each circuit. After a conﬁgurable time-period,
packet counts are exponentially decayed so that data
sent more recently has a greater inﬂuence on the packet
count. For each scheduling decision, the relay ﬂushes
the circuit with the lowest cell count, favoring circuits
that have not sent much data recently while preventing
bursty trafﬁc from signiﬁcantly affecting scheduling pri-
orities. Jansen et al.
[32] investigate new schedulers
based on the proportional differentiation model [21] and
differentiable service classes. Relays track the delay of
each service class and prioritize scheduling so that rel-
ative delays are proportional to conﬁgurable differenti-
ation parameters, but the schedulers require a mecha-

12

nism (tickets) for differentiating trafﬁc into classes. Fi-
nally, Tor’s round-robin TCP read/write schedulers have
recently been noted as a source of unfairness for relays
that have an unbalanced number of circuits per TCP con-
nection [54]. Tschorsch and Scheuermann suggest that a
round-robin scheduler could approximate a max-min al-
gorithm [24] by choosing among all circuits rather than
all TCP connections. More work is required to determine
the suitability of this approach in Tor.
Congestion. Improving performance and reducing con-
gestion has been studied by taking an in-depth look at
Tor’s circuit and stream windows [7]. AlSabah et al. ex-
periment with dynamically adjusting window sizes and
ﬁnd that smaller window sizes effectively reduce queuing
delays, but also decrease bandwidth utilization and there-
fore hurt overall download performance. As a result, they
implement and test an algorithm from ATM networks
called the N23 scheme, a link-by-link ﬂow control al-
gorithm. Their adaptive N23 algorithm propagates infor-
mation about the available queue space to the next up-
stream router while dynamically adjusting the maximum
circuit queue size based on outgoing cell buffer delays,
leading to a quicker reaction to congestion. Their experi-
ments indicate slightly improved response and download
times for 300 KiB ﬁles.
Transport. Tor’s performance has also been analyzed
at the socket level, resulting in suggestions for a UDP-
based mechanism for data delivery [56] or using a user-
level TCP stack over a DTLS tunnel [47]. While Tor cur-
rently multiplexes all circuits over a single kernel TCP
stream to control information leakage, the TCP-over-
DTLS approach suggests separate user TCP streams for
each circuit and sends all TCP streams between two re-
lays over a single kernel DTLS-secured [40] UDP socket.
As a result, a circuit’s TCP window is not unfairly re-
duced when other high-bandwidth circuits cause queuing
delays or dropped packets.

6.2 Bandwidth Management
Our approach to bandwidth management in this paper
has been to use a token bucket rate-limiter, a classic traf-
ﬁc shaping mechanism [55], to ensure that trafﬁc con-
forms to the desired policies. We now brieﬂy discuss
other approaches to bandwidth management.
Quality of Service. Networks often want to provide
a certain quality of service (QoS) to their subscribers.
There are two main approaches to QoS: Integrated Ser-
vices (IntServ) and Differentiated Services (DiffServ).

In the IntServ [11, 50] model, applications request re-
sources from the network using the resource reservation
protocol [60]. Since the network must maintain the ex-
pected quality for its current commitments, it must en-
sure the load of the network remains below a certain

level. Therefore, new requests may be denied if the net-
work is unable to provide the resources requested. This
approach does not work well in an anonymity network
like Tor since clients would be able to request unbounded
resources without accountability and the network would
be unable to fulﬁll most requests due to bottlenecks.

In the DiffServ [9] model, applications notify the net-
work of the desired service type by setting bits in the IP
header. Routers then tailor performance toward an ex-
pected notion of fairness (e.g. max-min fairness [24, 34]
or proportional fairness [20,21,35]). Leaking this type of
information about a client’s trafﬁc ﬂows is a signiﬁcant
risk to privacy and ways to provide differentiated service
without such risk do not currently exist.
Scheduling. Scheduling algorithms, such as fair queu-
ing [15] and round robin [24, 25], affect the order in
which packets are sent out of a given node, but gen-
erally do not change the total number of packets being
sent. Therefore, unless the sending rate is explicitly re-
duced, the network will still contain similar load regard-
less of the relative priority of individual packets. As ex-
plained in Section 1 and Section 3, scheduling does not
directly reduce network congestion, but may cooperate
with other bandwidth management techniques to achieve
the desired performance characteristics of trafﬁc classes.

7 Conclusion

This paper analyzes client throttling by guard relays to
reduce Tor network bottlenecks and improve responsive-
ness. We explore static throttling conﬁgurations while
designing, implementing, and evaluating three new throt-
tling algorithms that adaptively select which connections
get throttled and dynamically adjust the throttle rate of
each connection. Our adaptive throttling techniques use
only local relay information and are considerably more
effective than static throttling since they do not require
re-evaluation of throttling parameters as network load
changes. We ﬁnd that client throttling is effective at
both improving performance for interactive clients and
increasing Tor’s network resilience. We also analyzed
the effects throttling has on anonymity and discussed the
security of our algorithms against realistic adversarial at-
tacks. We ﬁnd that throttling improves anonymity: a
guard’s bandwidth leaks more information about its cir-
cuits when throttling is disabled.
Future Work. There are many directions for future re-
search. Our current algorithms may be modiﬁed to op-
timize performance by improving classiﬁcation of bulk
trafﬁc, considering alternative strategies for distinguish-
ing web from bulk connections. Additional approaches
to rate-tuning are also of interest, e.g. it may be possi-
ble to further improve web client performance using pro-
portional fairness to schedule trafﬁc on circuits. Also of

13

interest is an analysis of throttling in the context of con-
gestion and ﬂow control to determine the interrelation
and effects the algorithms have on each other. Finally, a
deeper understanding of our algorithms and their effects
on client performance would be possible through analy-
sis on the live Tor network.
Acknowledgements. We thank Roger Dingledine for
helpful discussions regarding this work and the anony-
mous reviewers for their feedback and suggestions. This
research was supported by NFS grant CNS-0917154,
ONR, and DARPA.

References
[1] The Libevent Event Notiﬁcation Library, Version 2.0. http:

//monkey.org/˜provos/libevent/.

[2] The Shadow Simulator. http://shadow.cs.umn.edu/.
[3] The Tor Metrics Portal. http://metrics.torproject.

org/.

[4] The Tor Project. https://www.torproject.org/.
[5] Throttling Algorithms Code Repository. https://github.

com/robgjansen/torclone.

[6] ACQUISTI, A., DINGLEDINE, R., AND SYVERSON, P. On the
Economics of Anonymity. In Proceedings of Financial Cryptog-
raphy (January 2003), R. N. Wright, Ed., Springer-Verlag, LNCS
2742.

[7] ALSABAH, M., BAUER, K., GOLDBERG, I., GRUNWALD, D.,
MCCOY, D., SAVAGE, S., AND VOELKER, G. DefenestraTor:
Throwing out Windows in Tor. In Proceedings of the 11th Inter-
national Symposium on Privacy Enhancing Technologies (2011).
[8] BACK, A., MOLLER, U., AND STIGLIC, A. Trafﬁc Analysis
Attacks and Trade-offs in Anonymity Providing Systems. In Pro-
ceedings of Information Hiding Workshop (2001), pp. 245–257.
[9] BLAKE, S., BLACK, D., CARLSON, M., DAVIES, E., WANG,
Z., AND WEISS, W. An Architecture for Differentiated Services,
1998.

[10] BORISOV, N., DANEZIS, G., MITTAL, P., AND TABRIZ, P. De-
In Proceedings of the
nial of Service or Denial of Security?
14th ACM conference on Computer and communications secu-
rity (2007), ACM, pp. 92–102.

[11] BRADEN, B., CLARK, D., AND SHENKER, S. Integrated Ser-

vice in the Internet Architecture: an Overview.

[12] CHEN, F., AND PERRY, M.

Improving Tor Path Selection.

https://gitweb.torproject.org/torspec.git/
blob/HEAD:/proposals/151-path-selection-
improvements.txt.

[13] CHUN, B., CULLER, D., ROSCOE, T., BAVIER, A., PETER-
SON, L., WAWRZONIAK, M., AND BOWMAN, M. PlanetLab: an
Overlay Testbed for Broad-coverage Services. SIGCOMM Com-
puter Communication Review 33 (2003), 3–12.

[14] CROTTI, M., DUSI, M., GRINGOLI, F., AND SALGARELLI, L.
Trafﬁc Classiﬁcation Through Simple Statistical Fingerprinting.
SIGCOMM Comput. Commun. Rev. 37 (January 2007), 5–16.

[15] DEMERS, A., KESHAV, S., AND SHENKER, S. Analysis and
Simulation of a Fair Queueing Algorithm. In ACM SIGCOMM
Computer Communication Review (1989), vol. 19, ACM, pp. 1–
12.

[16] DINGLEDINE, R.

Iran Blocks Tor.

https://blog.

torproject.org/blog/iran-blocks-tor-tor-
releases-same-day-fix.

[17] DINGLEDINE, R. Research problem: adaptive throttling of Tor
https://blog.torproject.

clients by entry guards.
org/blog/research-problem-adaptive-
throttling-tor-clients-entry-guards.

[18] DINGLEDINE, R., AND MATHEWSON, N. Anonymity Loves
In Proceedings
Company: Usability and the Network Effect.
of the Fifth Workshop on the Economics of Information Security
(WEIS 2006), Cambridge, UK, June (2006).

[19] DINGLEDINE, R., MATHEWSON, N., AND SYVERSON, P. Tor:
The Second-Generation Onion Router. In Proceedings of the 13th
USENIX Security Symposium (2004).

[20] DOVROLIS, C., AND RAMANATHAN, P. A Case for Rela-
tive Differentiated Services and the Proportional Differentiation
Model. Network, IEEE 13, 5 (1999), 26–34.

[21] DOVROLIS, C., STILIADIS, D., AND RAMANATHAN, P. Propor-
tional Differentiated Services: Delay Differentiation and Packet
Scheduling. In Proceedings of the Conference on Applications,
Technologies, Architectures, and Protocols for Computer Com-
munication (1999), pp. 109–120.

[22] EVANS, N., DINGLEDINE, R., AND GROTHOFF, C. A Practical
Congestion Attack on Tor Using Long Paths. In Proceedings of
the 18th USENIX Security Symposium (2009), pp. 33–50.

[23] GOLDSCHLAG, D. M., REED, M. G., AND SYVERSON, P. F.
Hiding Routing Information. In Proceedings of Information Hid-
ing Workshop (1996), pp. 137–150.

[24] HAHNE, E. Round-robin Scheduling for Max-min Fairness in
Data Networks. IEEE Journal on Selected Areas in Communica-
tions 9, 7 (1991), 1024–1039.

[25] HAHNE, E., AND GALLAGER, R. Round-robin Scheduling for
Fair Flow Control in Data Communication Networks. NASA
STI/Recon Technical Report N 86 (1986), 30047.

[26] HARDIN, G. The Tragedy of the Commons. Science 162, 3859

(December 1968), 1243–1248.

[27] HERNANDEZ-CAMPOS, F., JEFFAY, K., AND SMITH, F. Track-
In The 11th
ing the Evolution of Web Trafﬁc: 1995-2003.
IEEE/ACM International Symposium on Modeling, Analysis, and
Simulation of Computer Telecommunications Systems (2003),
pp. 16–25.
[28] HINTZ, A.

Fingerprinting Websites using Trafﬁc Analysis.
In Proceedings of Privacy Enhancing Technologies Workshop
(2002), pp. 171–178.

[29] HJELMVIK, E., AND JOHN, W. Statistical Protocol Identiﬁcation
with SPID: Preliminary Results. In Swedish National Computer
Networking Workshop (2009).

[30] HOPPER, N., VASSERMAN, E., AND CHAN-TIN, E. How Much
Anonymity Does Network Latency Leak? ACM Transactions on
Information and System Security 13, 2 (2010), 1–28.

[31] JANSEN, R., AND HOPPER, N. Shadow: Running Tor in a Box
In Proceedings of
for Accurate and Efﬁcient Experimentation.
the 19th Network and Distributed System Security Symposium
(2012).

[32] JANSEN, R., HOPPER, N., AND KIM, Y. Recruiting New
In Proceedings of the 17th ACM
Tor Relays with BRAIDS.
Conference on Computer and Communications Security (2010),
pp. 319–328.

[33] JANSEN, R., SYVERSON, P., AND HOPPER, N. Throttling Tor
Bandwidth Parasites. Tech. Rep. 11-019, University of Min-
nesota, 2011.

[34] KATEVENIS, M. Fast Switching and Fair Control of Congested
Flow in Broadband Networks. Selected Areas in Communica-
tions, IEEE Journal on 5, 8 (1987), 1315–1326.

14

[47] REARDON, J., AND GOLDBERG, I. Improving Tor using a TCP-
over-DTLS tunnel. In Proceedings of the 18th USENIX Security
Symposium (2009).

[48] REED, M., SYVERSON, P., AND GOLDSCHLAG, D. Anony-
mous Connections and Onion Routing. IEEE Journal on Selected
Areas in Communications 16, 4 (1998), 482–494.

[49] SERJANTOV, A., AND SEWELL, P. Passive Attack Analysis
for Connection-based Anonymity Systems. Computer Security–
ESORICS (2003), 116–131.

[50] SHENKER, S., PARTRIDGE, C., AND GUERIN, R. RFC 2212:
Speciﬁcation of Guaranteed Quality of Service, Sept. 1997. Sta-
tus: PROPOSED STANDARD.

[51] SNADER, R., AND BORISOV, N. A Tune-up for Tor: Improving
Security and Performance in the Tor Network. In Proceedings of
the 16th Network and Distributed Security Symposium (2008).

[52] TANG, C., AND GOLDBERG, I. An Improved Algorithm for Tor
Circuit Scheduling. In Proceedings of the 17th ACM Conference
on Computer and Communications Security (2010), pp. 329–339.
In-
https://gitweb.torproject.org/

[53] TSCHORSCH, F., AND SCHEUERMANN, B.

tervals.
torspec.git/blob/HEAD:/proposals/183-
refillintervals.txt.

Reﬁll

[54] TSCHORSCH, F., AND SCHEUERMANN, B. Tor is Unfair–and

What to Do About It, 2011.

[55] TURNER, J. New directions in communications(or which way to
the information age?). IEEE communications Magazine 24, 10
(1986), 8–15.

[56] VIECCO, C. UDP-OR: A Fair Onion Transport Design.

In
Proceedings of Hot Topics in Privacy Enhancing Technologies
(2008).

[57] WANG, T., BAUER, K., FORERO, C., AND GOLDBERG, I.
Congestion-aware Path Selection for Tor. In Proceedings of Fi-
nancial Cryptography (2012).

[58] WRIGHT, M., ADLER, M., LEVINE, B. N., AND SHIELDS, C.
An Analysis of the Degradation of Anonymous Protocols. In Pro-
ceedings of the Network and Distributed Security Symposium -
NDSS ’02 (February 2002), IEEE.

[59] WRIGHT, M., ADLER, M., LEVINE, B. N., AND SHIELDS, C.
Defending Anonymous Communication Against Passive Logging
Attacks. In Proceedings of the 2003 IEEE Symposium on Security
and Privacy (May 2003), pp. 28–43.

[60] ZHANG, L., DEERING, S., ESTRIN, D., SHENKER, S., AND
ZAPPALA, D. Rsvp: A new resource reservation protocol. Net-
work, IEEE 7, 5 (1993), 8–18.

[35] KELLY, F., MAULLOO, A., AND TAN, D. Rate Control for
Communication Networks: Shadow Prices, Proportional Fairness
and Stability. Journal of the Operational Research society 49, 3
(1998), 237–252.

[36] KOHNEN, C., UBERALL, C., ADAMSKY, F., RAKOCEVIC, V.,
RAJARAJAN, M., AND JAGER, R. Enhancements to Statis-
tical Protocol IDentiﬁcation (SPID) for Self-Organised QoS in
In Computer Communications and Networks (ICCCN),
LANs.
2010 Proceedings of 19th International Conference on (2010),
IEEE, pp. 1–6.

[37] LOESING, K. Measuring the Tor network: Evaluation of client

requests to directories. Tech. rep., Tor Project, 2009.

[38] MCCOY, D., BAUER, K., GRUNWALD, D., KOHNO, T., AND
SICKER, D. Shining Light in Dark Places: Understanding the
Tor Network. In Proceedings of the 8th International Symposium
on Privacy Enhancing Technologies (2008), pp. 63–76.

[39] MITTAL, P., KHURSHID, A., JUEN, J., CAESAR, M., AND
BORISOV, N. Stealthy Trafﬁc Analysis of Low-Latency Anony-
mous Communication Using Throughput Fingerprinting. In Pro-
ceedings of the 18th ACM conference on Computer and Commu-
nications Security (October 2011).

[40] MODADUGU, N., AND RESCORLA, E. The Design and Imple-
mentation of Datagram TLS. In Proceedings of the 11th Network
and Distributed System Security Symposium (2004).

[41] MOORE, W. B., WACEK, C., AND SHERR, M. Exploring the
Potential Beneﬁts of Expanded Rate Limiting in Tor: Slow and
In Proceedings of 2011
Steady Wins the Race With Tortoise.
Annual Computer Security Applications Conference (December
2011).

[42] MURDOCH, S., AND DANEZIS, G. Low-cost Trafﬁc Analysis
In IEEE Symposium on Security and Privacy (2005),

of Tor.
pp. 183–195.

[43] NGAN, T.-W. J., DINGLEDINE, R., AND WALLACH, D. S.
In The Proceedings of Financial

Building Incentives into Tor.
Cryptography (2010).

[44] ØVERLIER, L., AND SYVERSON, P. Locating Hidden Servers.
In Proceedings of the 2006 IEEE Symposium on Security and Pri-
vacy (May 2006), IEEE CS.

[45] RAMACHANDRAN, S.

Size and Num-
ber of Resources. http://code.google.com/speed/
articles/web-metrics.html, 2010. Accessed February,
2012.

Web Metrics:

[46] RAYMOND, J. Trafﬁc Analysis: Protocols, Attacks, Design Is-
sues, and Open Problems. In Designing Privacy Enhancing Tech-
nologies (2001), pp. 10–29.

15

