Populated IP Addresses — Classiﬁcation and Applications

∗

cyhong@illinois.edu

Chi-Yao Hong

UIUC

Fang Yu

MSR Silicon Valley

fangyu@microsoft.com

Yinglian Xie

MSR Silicon Valley

yxie@microsoft.com

ABSTRACT
Populated IP addresses (PIP) — IP addresses that are as-
sociated with a large number of user requests — are im-
portant for online service providers to eﬃciently allocate
resources and to detect attacks. While some PIPs serve
legitimate users, many others are heavily abused by attack-
ers to conduct malicious activities such as scams, phishing,
and malware distribution. Unfortunately, commercial proxy
lists like Quova have a low coverage of PIP addresses and
oﬀer little support for distinguishing good PIPs from abused
ones. In this study, we propose PIPMiner, a fully automated
method to extract and classify PIPs through analyzing ser-
vice logs. Our methods combine machine learning and time
series analysis to distinguish good PIPs from abused ones
with over 99.6% accuracy. When applying the derived PIP
list to several applications, we can identify millions of mali-
cious Windows Live accounts right on the day of their sign-
ups, and detect millions of malicious Hotmail accounts well
before the current detection system captures them.
Categories and Subject Descriptors: C.2.0 [Computer-
Communication Networks]: General – security and protec-
tion
Keywords: Populated IP addresses, proxy, IP blacklist-
ing, spam detection.

1.

INTRODUCTION

Online services such as Web-based email, search, and on-
line social networks are becoming increasingly popular. While
these services have become everyday essentials for billions
of users, they are also heavily abused by attackers for ne-
farious activities such as spamming, phishing, and identity
theft [14, 29].

To limit the damage of these attacks, online service providers

often rely on IP addresses to perform blacklisting and service
throttling [15, 19, 24, 26, 29]. However, IP-based techniques
work eﬀectively on only those IP addresses that are relatively

∗Work was done while interning at MSR Silicon Valley.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.

static and related to a few users. For IP addresses that are
associated with a large number of user requests, they must
be treated diﬀerently, as blocking/rate-limiting one such ad-
dress may potentially aﬀect many individual users.

In this paper, we deﬁne IP addresses that are associ-
ated with a large number of user requests as Populated IP
(PIP) addresses.
It is related, but not equivalent to the
traditional concept of proxies, network address translators
(NATs), gateways, or other middleboxes.

On the one hand, not all proxies, NATs, or gateways are
PIP addresses. Some may be very infrequently used and
thus are not of interest to online service providers.

On the other hand, while some PIP addresses may be-
long to proxies or big NATs, many others are not real prox-
ies. Some are dial-up or mobile IPs that have high churn
rates. Others include IP addresses from large services, such
as Facebook that connects to Hotmail to obtain user email
contacts. Additionally, not all PIPs are associated with a
large number of actual users. Although many good PIPs
like enterprise-level proxies are associated with a large num-
ber of actual users, some abused PIPs may be associated
with few real users but a large number of fake user accounts
controlled by attackers. In an extreme case, bad PIPs may
be entirely set up by attackers. For example, we ﬁnd that
>30% of the IP addresses that issue more than 20 sign-up
requests to Windows Live per day are actually controlled by
attackers, with all sign-ups for malicious uses.

Therefore, the knowledge of PIPs and their abused scenar-
ios are critical to service providers and can help them make
informed decisions. For example, a relaxed rate-limiting
threshold should be given to good PIPs, and more strin-
gent rate-limiting policies should be applied to abused or
bad PIPs. In addition, the information of PIPs can be used
in conjunction with other defense mechanisms to detect at-
tacks faster and more conﬁdently.

Classifying PIPs is a challenging task for several reasons.
First, ISPs and network operators consider the size and dis-
tribution of customer populations conﬁdential and rarely
publish their network usage information. Second, some PIP
addresses are dynamic, e.g., those at small coﬀee shops with
user population sizes changing frequently. Third, good PIPs
and bad PIPs can locate next to each other in the IP address
space. For example, attackers can buy or compromise Web
hosting IPs that are right next to the IPs of legitimate ser-
vices. In addition, a good PIP can temporarily be abused.
Due to these challenges, not surprisingly, we ﬁnd that com-
mercial proxy lists oﬀer a low precision in identifying PIPs

329and provide no support for distinguishing good PIPs from
bad ones.

In this paper, we introduce PIPMiner, a fully automated
method to extract and classify PIPs. We leverage machine-
learning techniques to capture the patterns of good and bad
PIPs. In particular, we use support vector machine (SVM),
a state-of-the-art supervised classiﬁcation technique, to train
a robust classiﬁer using a number of salient features ex-
tracted from service logs. These features capture the invari-
ants of PIP properties, ranging from intrinsic population
characteristics to more advanced time series and network
level properties.

PIPMiner uses a small set of PIPs with labels to train
a classiﬁer and applies the classiﬁer to PIPs that do not
have labels. After the classiﬁcation, PIPMiner outputs a
conﬁdence score for each PIP to quantify the abuse like-
lihood, with high scores given to good PIPs,
intermedi-
ate scores given to abused PIPs, and low scores given to
attacker-created PIPs. We implement PIPMiner on top
of DryadLINQ [30], a distributed programming model for
large-scale computing. Using a 240-machine cluster, PIP-
Miner processes a month-long Hotmail user login dataset of
296 GB in only 1.5 hours.

Our primary contribution is the concept of PIPs, which is
diﬀerent from the traditional concept of proxy IP addresses.
The notion of good/bad PIPs is much more relevant in the
security context, as populated IP addresses often become se-
curity loopholes: some service providers just whitelist PIPs
because they cannot distinguish good PIPs from bad ones.
Our second contribution is a method to automatically de-
rive and classify PIPs. We implement PIPMiner and demon-
strate its value through multiple large-scale security appli-
cations. Our key results include:

• PIPMiner is eﬀective.

PIPMiner identiﬁes 1.7
million PIP addresses. Our evaluation demonstrates
that PIPMiner can classify PIP addresses with 99.59%
accuracy and is robust against evasion attacks (§4).

• PIP results can help detect attacks. Our PIP
list helps detect 1.2 million malicious Hotmail accounts
with 98.6% precision, months before the current user
reputation system captures them (§5.2).

• PIP results are applicable across datasets and
can help classify non-PIP traﬃc. When applying
the PIP list derived from the Hotmail log to the Win-
dows Live ID sign-up problem, we detect more than
3 million bad accounts right on the day of their sign-
ups with 97% precision. In particular, over 77% of the
detected bad accounts are registered using non-PIP
addresses (§5.1).

• PIP list is more applicable than proxy list. We
compare our PIP list to the GeoPoint data, a big com-
mercial IP list provided by Quova. Using GeoPoint’s
IP Routing Type (IPRT) ﬁeld, we ﬁnd that 99.9% of
the proxy IPs are not associated with any requests in
our application logs (§2). For the PIP addresses that
are associated with many good requests, Quova’s proxy
list misses a vast majority of them (Appendix B).

Keyword (case insensitive)

Percentage of coverage

PROXY

GATE ∪ GW

NAT

0.0054
0.0049
0.0046

Table 1: The keyword coverage of PIP addresses through
rDNS lookup. The PIP address list is derived by PIPMiner
using the Hotmail user login dataset (§3).

The rest of the paper is organized as follows. We start by
discussing related work and background (§2). We then de-
scribe our overall approach and design details (§3), followed
by a comprehensive performance evaluation of our approach
(§4). We apply the PIP results to two security applications
(§5) before we conclude (§6).

2. RELATED WORK

In this section, we ﬁrst discuss related work in classifying
populated IP addresses (§2.1). Since we use email spam de-
tection as the application of our work, we also brieﬂy review
existing approaches in this ﬁeld (§2.2).
2.1 Populated IP Classiﬁcation

To the best of our knowledge, PIPMiner is the ﬁrst at-
tempt towards automated PIP address classiﬁcation on a
global scale. Existing IP-based detection systems do not
work eﬀectively on PIPs because their characteristics diﬀer
signiﬁcantly from normal IP addresses [28]. Hence, PIPs are
often ignored in previous studies [14, 31]. Meanwhile, our
study suggests that they are heavily exploited by attackers.
PIPMiner focuses on these PIPs and ﬁlls the gap.

Next we review work on global IP addresses, proxies, NATs
and gateways because they could overlap with and are re-
lated to PIP addresses.

Some PIP addresses might be inferred by examining Re-
verse DNS (rDNS) [10] records. An rDNS record maps an IP
address into a domain name, oﬀering a potential way to in-
fer its address properties. For example, the rDNS record
for an IP address 64.12.116.6 corresponds to a domain
name cache-mtc-aa02.proxy.aol.com, which suggests that
the IP address is used as a cache proxy in the AOL net-
work. However, only a fraction of PIP addresses have rDNS
records (62.6% in our experiment), and only 1.49% of them
contain useful keywords for PIP identiﬁcation (Table 1).

Certain services like CoDeen [25] and ToR [9] provide
open access to a set of distributed nodes. These nodes are
PIP candidates because they are accessible by many Inter-
net users. While these IP addresses can be easily collected
from these services, they are merely a small fraction of the
total PIPs. For example, CoDeen has around 300 nodes on
PlanetLab and ToR has fewer than 1000 exit nodes [4]. Our
PIP list (1.7 million IP addresses; §4.1) covers most of these
IPs.

Quova oﬀers the most well-known commercial proxy list [3].
It produces two types of information that may be used to
identify proxy IPs. The ﬁrst is the IP routing type, such
as regional proxy and mobile gateway. The second is the
anonymizer status, which records whether an IP is associ-
ated with a known anonymous proxy. The ﬁrst type includes
more than 300 million proxy IP addresses as of July, 2011,
orders of magnitude more than the second type. However,
we ﬁnd that most of them do not display PIP-like behavior:

330dran et al. proposed novel detection approaches by ana-
lyzing the sending patterns of network traﬃc (e.g., lookup
traﬃc to DNSBL) [22, 23]. The main diﬀerence between
our approach and IP blacklisting is that we not only provide
a list of malicious IP addresses, but also a model to iden-
tify them. The model can help identify new malicious PIPs
over time.
In addition, the results of PIPMiner can be ap-
plied to diﬀerent applications, such as detecting spammers
(§5.2) or identifying malicious sign-ups (§5.1). Therefore, it
is more general than systems that detect spam only (e.g.,
SNARE [12]).

3. SYSTEM DESIGN

Our primary goal is to design an automated method to
identify PIPs and classify them. Towards this goal, we take
a data-driven approach using service logs that are readily
available to all service providers. However, simple history-
based anomaly detection methods often fail to diﬀerentiate
real attacks from normal behavior changes accurately (re-
sults in §4.2). Therefore, we employ a machine learning
based approach that extracts features from training data.
We propose a comprehensive set of features to capture the
invariants of PIP properties, from intrinsic population char-
acteristics to more advanced time series features.

However, real data is often noisy and there is no perfect
training data.
In addition, some PIPs are used by both
legitimate users and attackers, so they display mixed be-
haviors that make our classiﬁcation diﬃcult. To deal with
these issues, we train a non-linear support vector machine
classiﬁer that is highly tolerant of noise in input data. We
represent the classiﬁcation result as a score between −1 and
1 to indicate the likelihood of a PIP address being abused
by attackers.
3.1 System Flow

Figure 2 shows the overall system ﬂow of PIPMiner. The
system reads in logs from one or more online services.
It
ﬁrst selects PIPs and extracts features for each PIP. Then,
PIPMiner labels a subset of data based on application-level
information. For PIPs that have clear labels, PIPMiner uses
them as training data to produce a classiﬁer. Using the
classiﬁer, PIPMiner labels the remaining PIPs and outputs
a score quantifying the likelihood of a PIP being good (a
high value), bad (a low value), or abused (an intermediate
value). The output PIP list with scores will be later used to
combat attacks (§5).
Input Data: The primary online service log used in our
study is a month-long user login trace collected at Hot-
mail in August, 2010. Each entry in the data contains an
anonymized account ID, a source IP address, and a times-
tamp. To help derive application-speciﬁc features, we also
use Hotmail account data, which provides account-level in-
formation such as the registration month of an account.

PIPMiner can potentially be applied to data from other
online services. For example, it can help detect malicious
account creations, malicious postings to online social net-
works, or the abuse usage of free storage services.
It can
also help Web servers ﬁght against denial-of-service attacks
by distinguishing good PIPs from bad ones.
PIP Selection: The ﬁrst step of PIPMiner is to identify
PIP addresses. PIPMiner ﬁrst identiﬁes PIPs by selecting

Figure 1: The number of proxy IP addresses in the Quova’s
GeoPoint dataset (using IP Routing Type ﬁeld for proxy
identiﬁcation) as of July, 2011. The number of IP addresses
with ≥ 10 Windows Live ID sign-up requests in a month is
3 to 4 magnitudes smaller than that of the inactive ones.

over 99% of them are not associated with any user sign-
up request to Windows Live (Figure 1). Our work aims to
complement Quova with an automated PIP detection and
classiﬁcation method.

While there is no existing approach that automatically
distinguishes good PIP addresses from bad ones, a large
body of previous work has focused on identifying other im-
portant IP properties. Cai et al. studied how network ad-
dress blocks are utilized through frequent ICMP probing [6].
Katz-Bassett et al. proposed a novel method to estimate the
geographic locations of IP addresses by solving constraints
derived from Internet topology measurements [16]. The clos-
est study to PIP detection is the NAT and proxy detection
work by Casado and Freedman [7]. Also closely related, Met-
wally and Paduano proposed methods to estimate the user
population size per IP address
[18]. Here, PIPMiner dif-
fers from these earlier eﬀorts by focusing on PIP addresses,
with special emphases on distinguishing good PIPs from bad
ones.

2.2 Spam Detection

Email spamming has been a long studied topic and many
spam detection methods have been proposed. In this paper,
we focus on the email service abuse problem where attackers
register or compromise millions of accounts and use them to
send out spam emails.

To ﬁght against such service abuse, service providers of-
ten monitor user behaviors carefully and detect malicious
users by their actions, e.g., sending out spam emails, failing
CAPTCHA tests, or exhibiting suspicious correlated behav-
iors [27, 31]. Since these detection methods rely on user
activities, when they detect a malicious user, the user might
have already conducted some malicious activities. PIPMiner
complements these detection methods by focusing on PIP
addresses that a user leverages to perform sign-up or login.
Therefore, the proposed system can ﬂag new users right on
the day of the sign-ups, or the time they log in.

In addition, our derived malicious PIP list serves as an
early signal to help service providers make informed deci-
sions on whether to block or rate-limit PIP-associated ac-
counts, and how to better allocate resources to PIP ad-
dresses in the presence of attacks. In this regard, the PIP
list is related to IP blacklists such as DNS Blacklists (DNS-
BLs). DNSBLs is widely used for spam detection. For exam-
ple, Jung and Sit observed the existence of low-proﬁle spam
sources by characterizing DNSBL traﬃc [15]. Ramachan-

10^010^110^210^310^410^510^610^710^810^9AOL ProxyInternationalProxyRegionalProxyMobileGatewayCache ProxyAllNo activity>0 requests≥5 requests ≥10 requests Number of IP addresses in Quova Data 331Figure 2: PIPMiner system ﬂow.

IP addresses that are associated with at least rL requests.
However, simply counting the number of requests per IP is
not suﬃcient, as many of these requests could be from a
few user accounts, e.g., through Outlook client’s auto sync.
Therefore, PIPMiner also considers account population size.
PIPMiner marks an IP address as a PIP address if the IP
address has been used by at least uM accounts, together
accounting for at least rM requests. In our current imple-
mentation, we empirically set rL to 1, 000 requests, uM to
10 accounts, and rM to 300 requests. However, PIPMiner
performance is not very sensitive to these thresholds, which
we will later show in Figure 6d.
Data Labeling: PIPMiner relies on a small set of training
data obtained from service providers. In our implementa-
tion, Hotmail provides us with the account reputation scores
that are computed based on account behavior history (e.g.,
spam sent and failed CAPTCHA tests) and external reports
(e.g., rejected emails and user marked spams). Using the
reputation scores, we classify user accounts as good, sus-
picious, or malicious, and we label an IP address as good
if the majority of its requests are made by good accounts,
and an IP address as bad if most of its requests are issued
by malicious accounts. Since the reputation system relies
on carefully observing the account behaviors over time, it
is hard for the reputation system to accurately classify new
accounts with few activities. Therefore, only a fraction of
the PIPs have clear labels, while the remaining IP addresses
are unlabeled and their goodness remains unknown.
Feature Extraction: During feature extraction, the sys-
tem constructs a set of PIP features, ranging from account-
level features (e.g., how many user accounts send requests
via the PIP?), time series-related features (e.g., do the aggre-
gated requests exhibit human-like behavior such as diurnal
patterns in the time domain?), to PIP block level features
(e.g., aggregated features from neighboring PIPs). We will
explore the motivations behind these features in §3.2.
Training and Classiﬁcation:
In this study, we train a
non-linear support vector machine classiﬁer due to its ro-
bustness to noisy input, eﬃciency, and high accuracy. After
the classiﬁer is trained using labeled PIPs, we use it to clas-
sify unlabeled PIPs, resulting in a full list of classiﬁed PIPs.
We further convert the classiﬁer decision value to a score of
how likely a PIP address is abused by attackers. It can be
used to detect abused cases in the future and also in other
applications, as good PIPs can temporary turn bad when
they are abused. The details of the training and classiﬁca-
tion algorithms are in §3.3.
Having explained the system ﬂow, next we present the
proposed features (§3.2) and training methodologies (§3.3).
3.2 Features

We train a support vector machine with a collection of
robust features that discern between good and bad PIPs.

Feature
Population size
Request size
Request per account
New account rate

Account request dis-
tribution
Account stickiness dis-
tribution

Descriptions
The number of accounts
The number of requests
The number of requests per account
The percentage of accounts appear
only in the last x days
The distribution of the number of
requests per account
The distribution of the number of
PIPs associated with an account

Table 2: Population features

In particular, we propose three sets of quantitative features
that can be automatically derived from online service logs.
Population Features (§3.2.1) capture aggregated user char-
acteristics and help distinguish stable PIPs with many users
from those that are dominantly used by only a few heavy
users. Time Series Features (§3.2.2) model the detailed re-
quest patterns and help distinguish long-lasting good PIPs
from bad PIPs that have sudden peaks. IP Block Level Fea-
tures (§3.2.3) aggregate IP block level activities and help
recognize proxy farms.

A subset of the features that we use (user/request density,
user stickiness and diurnal pattern) have been used in other
security-related studies [12, 23, 28]. However, we introduce
new features and we later show that the performance im-
provement by our new features is sizable (§4.2).
3.2.1 Population Features
Population features capture the aggregated account char-
acteristics of each individual PIP. We consider basic statis-
tics such as the number of accounts, the percentage of new
accounts, the number of requests, and the average number
of requests per account, as shown in Table 2.

We also include features derived from basic distributions.
We ﬁrst look at activities of individual PIPs. The account-
request distribution records the distribution of the number
of requests per account. It reﬂects whether accounts on the
PIP have a roughly even distribution of request attempts, or
whether the requests are generated by just a few heavy ac-
counts. In addition, we also look at activities across multiple
PIPs. In particular, we look at account stickiness distribu-
tion, which records the number of other PIPs used by each
account.

After obtaining a distribution, we bucket the distribu-
tion into 20 bins and then extract features such as the peak
values, the central moments, and the percentage of the ac-
counts/requests in the top 1/3/5 bins.
3.2.2 Time Series Features
The population features alone may not be enough to dis-
tinguish good PIPs from bad ones. For example, we ﬁnd
that the aggregated distribution of the number of requests

Dataset #2Dataset #1… PIPaddress selectionRaw ServicelogsData labelingPIP logsFeature extractionClassificationTrainingLabelsFeaturesModelLabeledPIP listLabeled PIP list w/ scores332Feature
On/oﬀ period

Diurnal pattern

Weekly pattern

Account
time

binding

Descriptions
The fraction of time units having ≥x re-
quests
The percentage of requests and accounts
in the peak and the dip periods of the day
The percentage of requests in weekdays
and weekends
The distribution of accounts’ binding time
(time between the ﬁrst and the last re-
quests)

Inter request time The distribution of accounts’ medium

Predictability

inter-request time
The number and the density of anomalies
derived from the Holt-Winters forecasting
model

Table 3: Time series features.

(a)

(b)

(c)

per good PIP looks nearly identical to that of bad PIPs.
One reason is that population features do not capture the
detailed request-arriving patterns. PIPs that are active for
only a short but intensive period (a typical behavior of bad
PIPs) may have similar population features as PIPs with
activities that are spread out in a long duration with clear
diurnal patterns (a typical behavior of good PIPs). In this
subsection, we derive time series features to help distinguish
good PIPs from bad ones.
To derive time series features, we ﬁrst construct a time
series of request counts, denoted by T [·], in a 3-hour preci-
sion. In particular, T [i] is the number of logins in the i-th
time unit. Thus there are k = 8 time units per day.
Additionally, we construct a time-of-the-day series TD[·]
(cid:80)
such that the j-th element of the time series is the total
number of requests in the j-th time unit over days: TD[j] =
k=0,1,... T [8j + k]. Then we derive the peak time P =
arg maxj{TD[j]} and the dip time D = arg minj{TD[j]},
and the time diﬀerence between the peak and the dip, i.e.,
|P − D|.

Table 3 lists the features that we derive from the time se-
ries. It includes on/oﬀ period, diurnal pattern, and weekly
pattern. The on/oﬀ period feature records the fraction of
time units having ≥x requests in T [·]. To assess if the time-
of-the-day series displays a diurnal pattern, we check the
time and the load diﬀerence between the peak and the dip.
To check weekday pattern, we record the percentage of re-
quests and accounts in weekdays and weekends. Due to
timezone diﬀerences, the time for weekends of diﬀerent re-
gions might be diﬀerent. One option is to look up the time-
zone of each IP address. In our experiment, we adopt a sim-
per method — ﬁnding two consecutive days in a week such
that the sum of the number of requests from these two days
are minimal. For PIPs that have more traﬃc in weekends
than weekdays, e.g., coﬀee shop IPs, our approach considers
the least busy day during the week as weekend. But this
does not aﬀect the correctness because our main goal is to
detect the repeated weekly patterns, i.e., the least busy days
of the week repeat over diﬀerent weeks.

We also study two distributions that reﬂect the request-
arrival pattern of each account. The ﬁrst is the account-
binding-time distribution that captures the distribution be-
tween the last and the ﬁrst account request time. This re-
ﬂects the accounts’ binding to an IP address. In addition,

Figure 3: (a) and (b): Time series of the number of requests
from two randomly selected PIP addresses that belong to the
same PIP block. (c): Block level time series, i.e., the time
series of the total number of requests aggregated from all
the PIP addresses in the same PIP block. The selected PIP
block has 94 PIP addresses, and the ﬁgures are plotted in a
time granularity of 3 hours. All the PIPs in the PIP block
are labeled as good.

we check the inter-request time distribution that captures
the regularity of account requests.

We also apply a forecasting model on time series to detect

abnormal period, and we leave details to Appendix C.

IP Block Level Features

3.2.3
We ﬁnd that many regional PIPs manifest clear time-
series features such as coherent diurnal patterns and the
weekday-weekend pattern. For example, for oﬃce IP ad-
dresses, the daily peaks are usually around 4 PM, while the
daily dips are at around 4 AM. The peak load and the dip
load diﬀer signiﬁcantly and they are usually predictable us-
ing our forecasting models.

However, there are exceptions. In particular, large proxy
farms often redirect traﬃc to diﬀerent outgoing network in-
terfaces for load balancing purposes. They may rotate IP
addresses at random intervals or at pre-conﬁgured times-
tamps from a pool of neighboring IP addresses (e.g., [2]).
Such strategies will cause us to generate time series with high
variations (e.g., sudden increase), as shown in Figure 3a and
Figure 3b. We might mistakenly conclude that these cases
are suspicious if we look at time series of individual PIPs
in an isolated way. To resolve the problem, we aggregate
traﬃc using IP blocks and use information of neighboring
PIP addresses to help recognize PIP patterns, as shown in
Figure 3c.

We use the following two criteria to determine neighboring

IP addresses:

1. Neighboring IPs must be announced by the same AS.
This ensures that the neighboring IP addresses are un-
der a single administrative domain.

2. Neighboring IPs are continuous over the IP address
space, and each neighboring IP is itself a PIP. This

 0 5 10 15 20 256/15/20116/21/20116/27/20117/3/20117/9/2011Number ofrequests 0 3 6 9 12 156/15/20116/21/20116/27/20117/3/20117/9/2011Number ofrequests 0 140 280 420 560 7006/15/20116/21/20116/27/20117/3/20117/9/2011Number ofrequests333ensures that there is no signiﬁcant gap between a PIP
and its neighboring PIPs.

After identifying neighboring PIPs, PIP block IDs are as-
signed to PIP addresses such that PIP addresses have the
same block ID if and only if they are neighboring PIPs. Fi-
nally, we generate block-level features: for each PIP address,
we extract all the features (i.e., population and time series
features) using the requests aggregated from all the PIPs in
the same block.
3.3 Training and Classiﬁcation
Feature Preprocessing: For training data, we perform
feature-wise normalization to shift and re-scale each feature
value so that they lie within the range of [0, 1]. We apply
the same transformation as in the training set to the testing
set. However, certain feature values in the testing set can
still be outside the range of [0, 1], which we set to zero or
one as appropriate. This procedure ensures that all features
are given equal importance during the training.
Binary Classiﬁcation: We use support vector machine
(SVM) as our main classiﬁcation algorithm due to its robust-
ness and eﬃciency (comparison to other algorithms is shown
in §4.2). SVM classiﬁes an unlabeled PIP with feature vec-
tor x based on its distance to the decision hyperplane with
norm vector w and constant b:

(cid:88)

∀i

f (x) = wT x + b =

αiyik(xi, x)

(1)

where the sum is over each PIP i in the training set with
label yi ∈ {−1, 1}, feature vector xi, and coeﬃcient αi that
indicates whether the PIP is a support vector (αi > 0) or not
(αi = 0). The feature vectors xi are mapped into a higher
dimensional feature space by a non-linear kernel function
k(xi, x).

SVM tries to identify w and b, determining the optimal
decision hyperplane in the feature space such that PIPs on
one side of the plane are good, and the other side bad. It
is a maximum margin approach as it tries to maximize the
distance between training data and the decision hyperplane.
Therefore, the resulting boundaries are very robust to noisy
data, which are inevitable in our case because some PIPs
can have biased features due to the lack of enough user re-
quests.
In our study, we experimented with linear, radial
basis function, and polynomial kernel functions.
Probability Estimation: We convert the classiﬁer deci-
sion value to a score of how likely a PIP address is labeled
as good, i.e., a posterior class probability P (y = good|x).
Bayes’ rule suggests using a parametric form of a sigmoid
function to approximate the posterior [17]:

P (y = good|x) ≈

1

1 + exp(Af (x) + B)

.

(2)

The parameters A and B are ﬁt by using maximal likeli-
hood estimation on the training set. However, it has been
observed in [20] that the use of the same training set may
overﬁt the model (Equation 2), resulting in a biased esti-
mation. To solve this problem, we take a computationally
eﬃcient way to conduct a ﬁve-fold cross-validation before
the maximal likelihood estimation.
We label a PIP as good (or bad) only if the score is larger
than a threshold p (or smaller than 1 − p). The remaining

Figure 4: PIP score distribution.

Figure 5: PIP address distribution.

PIPs are classiﬁed as mixed, corresponding to good PIPs
being abused.
Parameter Selection: We check the parameters to assure
good generalization performance and to prevent overﬁtting.
We perform a grid search on kernel parameters using cross-
validation. Speciﬁcally, good parameters are automatically
identiﬁed by searching on the grid points, which are gener-
ally chosen on a logarithmic scale.

We implement PIPMiner on top of DryadLINQ [30], a
distributed programming model for large-scale computing.
We leave the implementation details to Appendix A.

4. PIP EVALUATION

This section presents our evaluation results of PIPMiner.
We ﬁrst describe the basic properties of the derived PIP list
(§4.1). Using a subset of pre-labeled PIPs as the training
data, we show that PIPMiner is highly accurate and eﬃcient,
and is generally applicable to various online services (§4.2).
For PIPs that do not initially have labels, we validate that
the majority of them have been correctly classiﬁed (§4.3).
4.1 PIP Addresses and Their Properties

We apply PIPMiner to a month-long Hotmail login log
pertaining to August 2010 and identify 1.7 million PIP ad-
dresses. Although the PIP addresses constitute only around
0.5% of the observed IP addresses, they are the source of
more than 20.1% of the total requests and are associated
with 13.7% of the total accounts in our dataset. This ﬁnd-
ing suggests that the traﬃc volume from a PIP is indeed
much higher than that from a normal IP address.
We classify a PIP as good (or bad) if the score given by the
classiﬁer is larger than 0.95 (or smaller than −0.95), which
is evident in Figure 4. In our current process, around 34%
and 53% PIPs are classiﬁed as good and bad, respectively.
The remaining 13% PIPs are classiﬁed as mixed (abused).

Figure 5 plots the distribution of the PIP addresses across
the IP address space. The distribution of the good PIP ad-
dresses diﬀers signiﬁcantly from that of the bad ones. For

0.30.40.50.60.70.80.91-1-0.8-0.6-0.4-0.200.20.40.60.81Cumulative fraction of PIPs Score Bad PIPs: 53% PIPs have a score of  ≤ -0.95 Good PIPs: 34% PIPs have a score of  ≥ +0.95 Mixed PIPs: 11% PIPs have a score in (-0.95, +0.95)  0 0.2 0.4 0.6 0.8 10.0.0.032.0.0.064.0.0.096.0.0.0128.0.0.0160.0.0.0192.0.0.0224.0.0.0Cumulative fractionof PIPsIP address spaceOur classiﬁed bad PIPsOur classiﬁed good PIPs334Algorithm

Accuracy= Precision= Recall=
#TPs+#TNs

#TPs

#TPs

#All

#TPs+#FPs

#TPs+#FNs

SVM Linear Kernel

SVM Polynomial Kernel

Bagged Decision Trees

LDA Linear

LDA Diagonal Linear
LDA Quadratic Linear

LDA Diagonal Quadratic

LDA Mahalanobis

Naive Bayes (Gaussian)

AdaBoost

J48 Decision Tree (C45)

96.72%
99.59%
96.27%
94.65%
89.54%
94.77%
92.50%
94.46%
92.60%
92.56%
92.20%

97.31%
99.66%
96.64%
96.88%
96.62%
95.67%
95.67%
99.33%
94.78%
92.79%
93.84%

97.61%
99.83%
97.61%
94.75%
86.80%
96.25%
92.57%
91.97%
93.73%
96.09%
94.11%

Table 4: Accuracy of classiﬁcation algorithms for initially
labeled cases.

bad PIP addresses, the majority of the IP addresses origi-
nate from two regions of the address space (64.0-96.255 and
186.0-202.255). Somewhat surprisingly, the distribution of
the bad PIP addresses is similar to that of the dynamic IP
addresses reported by Dynablock [28]. This observation sug-
gests that attackers are increasingly mounting their attacks
from dynamically assigned IP addresses, which likely corre-
spond to end-user machines that have been compromised.
To validate this hypothesis, we perform rDNS lookups to
check if the domain name of a PIP address contains keywords
that suggest the corresponding IP address as dynamic, e.g.,
dhcp, dsl, and dyn; we observe that 51.3% of the bad PIPs
have at least one of these keywords.
4.2 Accuracy Evaluation of Labeled Cases

Among 1.7 million PIP addresses, 973K of them can be
labeled based on the account reputation data. We train
our classiﬁer using the data sampled from these labeled PIP
addresses. For the training data, we use the primitive good
and bad PIP ratio, around 1:0.17. Note that, this ratio is
not critical: we tried a few other very diﬀerent ratios such
as 1:1 and 1:6, and they give us similar results.
Classiﬁcation Accuracy: Using 50,000 samples with 5-
fold cross validation, we compare the accuracy of our classi-
ﬁcation in Table 4, using diﬀerent learning algorithms based
on the labeled PIPs.
In general, we observe that all the
classiﬁcation algorithms achieve accuracies >89%. In par-
ticular, the SVM classiﬁer with a polynomial kernel pro-
vides the best overall performance. We observed that the
non-linear kernel of SVM outperforms the linear kernel, but
at a cost of higher training time. Other classiﬁcation al-
gorithms such as linear discriminant analysis (LDA), naive
Bayes, AdaBoost, and J48 decision trees all have very com-
petitive performance. All these algorithms can serve as a
building block of the PIPMiner system. Since SVM with a
polynomial kernel provides the best result, we choose it in
our implementation of the system.
Accuracy of Individual Components: Our classiﬁca-
tion framework relies on a broad set of features to achieve a
high detection accuracy. To understand the importance of
the features, we train a classiﬁer on each single feature and
Table 5 shows the performance breakdown of diﬀerent types
of features used for the training. Leveraging the account
request distribution provides the best accuracy among the
individual features, while the combination of more features
does help reduce the classiﬁcation error signiﬁcantly.

Feature

w/ Block-level

w/o Block-level

Accuracy Precision Accuracy Precision

Population Features

Population Size

Request Size

Requests per Account

New Account Rate

Account Request Dist.
Account Stickiness Dist.
Time Series Features

On/oﬀ Period
Diurnal Pattern
Weekly Pattern

Account Binding Time

Inter Request Time

Predictability
All Features

Previous Features

97.8%
92.6%
82.9%
89.2%
86.3%
94.2%
88.1%
96.1%
86.1%
85.8%
90.6%
90.8%
92.5%
90.8%
99.6%
93.7%

98.2%
92.6%
82.4%
89.1%
86.7%
94.0%
88.3%
97.2%
89.2%
85.1%
88.8%
90.2%
92.3%
89.6%
99.7%
93.5%

96.3%
90.5%
79.8%
89.2%
85.5%
93.6%
88.2%
95.5%
84.0%
85.5%
89.6%
90.3%
91.8%
86.8%
98.3%
92.7%

96.8%
90.5%
77.8%
88.9%
85.3%
93.4%
88.2%
96.4%
88.0%
85.1%
87.8%
90.3%
91.1%
82.6%
99.1%
93.1%

Table 5: Classiﬁcation accuracy when trained on one type
of feature. Account request distribution, population size,
and inter request time provide the best individual accuracy.
The classiﬁer achieves the best performance when combin-
ing all types of features. The last row presents the perfor-
mance when trained using the previously proposed features
(account/request volume, account stickiness and diurnal pat-
tern).

Comparison with Previous Proposed Features:
In
Table 5, we demonstrate the eﬀectiveness of our features by
providing a comparison of our results to the classiﬁcation re-
sults obtained by using the previously proposed features for
the training. The previous features include account/request
volume, account stickiness, and diurnal pattern. We observe
that our feature sets provide substantial performance gains
over the previous features, improving the detection accuracy
from 92.7% to 99.6%.
Accuracy against Data Length: To quickly react to
malicious attacks, the classiﬁer needs to accurately diﬀer-
entiate bad PIPs from good ones based on as little activity
history as possible. Figure 6a shows the classiﬁcation results
by forcing the classiﬁer to train and test on the most recent
k requests per PIP. A small value of k may yield biased
features, resulting in a lower classiﬁcation accuracy. Never-
theless we observe that the classiﬁer requires only k = 12
requests per PIP1 to achieve a very high accuracy. This
indicates that our system could eﬀectively identify newly
emerged attacks.
Comparison with History-based Approach: Given
that our classiﬁcation framework works very well even with
access to limited/partial history information, one could ask:
do we need machine learning techniques to distinguish good
PIPs from bad ones? To answer this question, we evaluate a
baseline algorithm that uses only the historical ground truth
to predict the PIP labels. For each sampled PIP from the
pool of the labeled PIPs, we construct a time series of the
reputation in a 1-day precision. Then we use an exponen-
tial weighted moving average (EWMA) model for reputation
prediction. In particular, the reputation is predicted from
the weighted average of all the past data points in the time

1Although the time series features become less useful with
the decrease of k (due to the incomplete view in the time
domain), PIPMiner can still achieve fairly high accuracy by
combining the population features with the block-level time
series features.

335(a)

(b)

(c)

(d)

Figure 6: Classiﬁcation Performance. (a) PIPMiner performance against data length, (b) Pure history-based approach perfor-
mance, predicting PIP labels using all available prior reputation score records. (c) PIPMiner performance against the number
of the PIPs for the training, (d) PIPMiner performance against the amount of malicious traﬃc. Accuracy=(tn+tp)/all;
Precision=tp/(tp+fp); Recall=tp/(tp+fn).

series. In this model, the weighting factors for each older
data point decreases exponentially. Figure 6b shows that
this baseline approach achieves only 82% accuracy with the
best smoothing factor using all history information. More-
over, unlike our machine learning based approach, the base-
line cannot be applied to the PIPs without labels. The
machine-learning approach essentially learns the good/bad
behavior patterns from many other PIPs, hence we do not
require long activity histories.
Service Scale: PIPMiner is designed for analyzing PIP
behaviors. One question is whether PIPMiner is useful for
only very large service providers? Can we deploy PIPMiner
for small-scale online services that have only a few PIPs? To
answer these questions, we conduct an experiment to artiﬁ-
cially remove part of the source IP addresses in the Hotmail
trace randomly. As shown in Figure 6c, we demonstrate that
PIPMiner still retains its high classiﬁcation accuracy with
only a few hundred labeled samples, which is fewer than
0.2% of the labeled PIPs in the trace. This suggests that
PIPMiner is useful across a wider variety of business sizes.
Robustness to Attacks: With any detection scheme,
adversaries will naturally try to invent methods to evade
detection. The simplest choice would be to hide behind
anonymized networks such as ToR [9]. However, such ef-
fort is not very eﬀective because ToR’s throughput is low
and ToR IP addresses are public. Therefore, it is very easy
for service providers to pay special attention to the requests
coming from ToR IPs. Due to these constraints, we ﬁnd that
attackers have chosen to use open proxies or set up their
own proxies to conduct malicious activities, rather than us-
ing ToR in our data. Such observation exactly motivates
our work to classify PIPs.

Although the individual features of PIPMiner may be
gamed, it is diﬃcult for an attacker to evade the combi-
nations of them. A malicious PIP that evades our classiﬁ-
cation (i.e., below the threshold of being classiﬁed as bad)
may still be given a lower score by PIPMiner as long as
it fails to emulate normal user behaviors for some features.
The resulting score can be used in conjunction with other
detection techniques to improve the detection conﬁdence.

Moreover, PIPMiner raises the bar signiﬁcantly for an at-
tacker to evade the detection. The population and time se-
ries features will drive attackers to reduce their attack scale
in order to approximate the behaviors from a real user pop-

ulation. To validate that PIPMiner can retain high accuracy
when attackers reduce their attack scales, we synthetically
remove traﬃc sent by classiﬁed malicious PIPs in both train-
ing and testing data. Figure 6d shows that PIPMiner is
robust to such evasion strategies.

4.3 Validation of Unlabeled Cases

For the PIPs with known labels, we have shown that our
classiﬁer distinguishes good and bad ones accurately. It re-
mains unclear whether we can accurately classify the unla-
beled PIPs, which comprise 42.8% of the total PIPs. They
are particularly hard to classify as many accounts on these
PIPs are new with little history or few activities, so even the
reputation data cannot help to generate their labels.

In the lack of ground truth for verifying the unlabeled
PIPs, we combine information from both the user registra-
tion month and future reputation for validation.
User Registration Month:
Intuitively, good PIPs are
shared by many diﬀerent normal users and hence exhibit
a diverse distribution in the user registration months. For
bad PIPs, as their requests are mostly generated by attacker-
created malicious accounts, it is highly likely that most of
these accounts were created in a short time window. To
quantify whether our labels are correct, for each PIP we
place the corresponding accounts into bins based on their
registration month. Figure 7 shows the CDF of the percent-
age of accounts in the top bin, which has the largest number
of accounts. Overall, we observe very diﬀerent distributions
between the classiﬁed good and bad PIPs. For the PIPs
that we classify as good, only 4% of them have over 45%
accounts registered in the same month. For the PIPs that
we classify as bad, in contract, around 82% of them have
at least 45% accounts registered in the same month. The
distinct account behaviors suggest that the majority of the
unlabeled PIPs are classiﬁed correctly.

We further validate the results using future reputation.
Future reputation is a score reported by the reputation sys-
tem months after our dataset’s collection time. It gives the
reputation system an opportunity to observe user activities
for an extensive period of time, thus has a better chance
of recognizing previously unclassiﬁed accounts, either good
or bad. In particular, we use the reputation score of July
2011, which provides us with an updated user reputation

 0.75 0.8 0.85 0.9 0.95 1 6 12 24 50 100 200 400 800 1600 3200The maximal number of userrequests per PIP that areused to calculate featuresAccuracyPrecisionRecall 0.75 0.8 0.85 0.9 0.95 1 0.1 0.3 0.5 0.7 0.9EWMA smoothing factorAccuracyPrecisionRecall 0.75 0.8 0.85 0.9 0.95 1 10 100 1000 10000The number of labeled PIPs thatare used to train a classiﬁerAccuracyPrecisionRecall 0.75 0.8 0.85 0.9 0.95 10.01 %0.1 %1 %10 %100 %The amount of malicious traﬃcsent from identiﬁed bad PIPs[relative to raw data]AccuracyPrecisionRecall336Figure 7: The cumulative fractions of the percentage of ac-
counts that registered in the top 1 month. The function is
plotted over initially unlabeled PIPs.

Future

reputation

Accounts on Accounts on
good PIPs

bad PIPs

Good

Intermediate

Bad

Deleted
Unknown

61.6%
7.4%
7.5%
20.6%
2.9%

4.1%
5.3%
10.7%
75.4%
4.5%

Table 6: The future reputation (as of July, 2011) of PIPs
that do not have labels in August, 2010.

score almost one year after the testing dataset’s creation
time (August, 2010).

Table 6 summarizes the evaluation results based on fu-
ture reputation. For the accounts on the initially unlabeled
PIPs that we classiﬁed as good, around 62% of them are
recognized as good after one year. For the accounts on the
initially unlabeled PIPs that we classiﬁed as bad, around
86.1% of them are either deleted or marked as malicious af-
ter one year. We notice that a large portion of the accounts
that were classiﬁed as bad had been deleted during the year
and no longer exist in the updated reputation system. An
account can be deleted if it displays malicious behaviors
against user terms, or if there are no activities for a long
period of time. The violation of terms includes spamming
and phishing activities, reported by other mail servers.

These results demonstrate that PIPMiner can classify PIP
addresses accurately. When combining information collected
from multiple PIP addresses, we can recognize malicious ac-
counts months before the existing system does, with a pre-
cision higher than 98.6% (§5.2).

5. APPLICATIONS

In this section, we investigate two applications of PIPs.
First, we apply the PIP list to the Windows Live ID sign-
up abuse problem to ﬂag malicious accounts right on the
day of their sign-ups (§5.1). Second, we apply the PIP list
to the Hotmail malicious user sign-in problem and detect
malicious accounts months before the Hotmail reputation
system does (§5.2).
5.1 Windows Live ID Sign-up Abuse Problem
Our ﬁrst application is to ﬂag malicious (i.e., bot-generated)

sign-ups using the PIP information. Since there is very little
information about accounts available at the sign-up time, it
is challenging to correctly distinguish all malicious accounts
from good ones, given that they are all new. The ﬂags that
oﬀered by PIP addresses could be used as a feature or a
start-up reputation to detect malicious users more quickly
and limit their damages.

Case

Original

New

# Users
[million]

% Good

1.0
2.8

19.0%
1.4%

% Inter-
mediate

39.9%
20.3%

% Bad

41.1%
78.3%

Table 7: Comparative study of our PIP list (ﬁrst row) and
the newly appeared “PIP-like” list (second row).

PIP

# Users

information [thousand]

% Good

% Inter-
mediate

% Bad

Good

Abused
Normal

Bad

216.0
397.5
410.5

10.6%
39.1%
3.9%

35.3%
34.0%
48.1%

54.1%
26.9%
48.0%

Table 8: User reputation distribution on diﬀerent types of
PIPs.

We apply the 1.7 million PIP list derived from the Hotmail
login log of August 2010 to the Windows Live ID sign-up
trace in the same month. We study the sign-up behavior on
two types of the PIP addresses. The ﬁrst is the 1.7 million
derived PIPs. The second is the set of IP addresses that have
more than 20 sign-ups from the Windows Live ID system,
but they are not included in the 1.7 million PIPs. We refer
to the second set as the newly appeared “PIP-like” addresses.
As there is little information available at the sign-up time
to distinguish good sign-ups from bad ones, we focus on the
sign-ups related to Hotmail2 and use the Hotmail reputation
trace in July, 2011 (after 11 months) to determine whether
a particular sign-up account was malicious or not.
Sign-ups from newly appeared PIP-like addresses:
Table 7 shows a comparative study of our derived PIP list
and the newly appeared PIP-like addresses. Although both
lists are associated with a large number of malicious sign-
ups, the fraction of bad sign-ups from PIP-like addresses
is signiﬁcantly higher (78.3% versus 41.1%) than that from
our derived PIP list. Only 1.4% of the sign-ups establish
good reputation scores after one year. This shows that good
PIPs usually have activities across multiple services, as they
are used by many normal users who have diverse interests.
The PIP-like addresses that are dominantly used by just
one service are more suspicious. They are more likely to be
attacker-controlled IPs and leveraged by attackers to con-
duct attacks to a speciﬁc service.
It is hard and also ex-
pensive for attackers to simulate normal user requests to all
possible Web services. Hence, correlating activities across
multiple services could yield a better PIP recognition accu-
racy and also attack-detection ability.
Sign-ups from our PIP addresses: Blindly applying
our PIP list to the sign-up data, we observe a mixed user
reputation (19.0% good, 41.1% bad, remaining as interme-
diate as shown in Table 7). To further classify the good
sign-ups from bad ones, we leverage PIP address properties
provided by our system: the goodness score and the number
of good users.

The results are summarized in Table 8. For the PIPs with
low goodness scores, not surprisingly, only 3.9% of the sign-
ups remain good after 11 months. For the PIPs with high

2Windows Live ID is a uniﬁed ID, supporting multiple ser-
vices such as Hotmail, Window Live Messenger, Xbox.

 0 0.2 0.4 0.6 0.8 1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Cumulativefraction of PIPs% of accounts registered in the top monthClassiﬁed as goodClassiﬁed as mixedClassiﬁed as bad337Actual

Our prediction

(after 11 months) Good
155K
106K

Good
Bad

Bad
78K

2506K

Table 9: Using PIPs to predict user reputation right at the
day of the sign-ups.

scores in the Hotmail login log, however, they could still be
temporarily abused by attackers in the sign-up log. Here,
we detect abuse scenarios by looking at the sign-up rate to
the sign-in rate. If the ratio is larger than a threshold (set to
0.1 in the current experiment as the sign-up rate is usually
a magnitude smaller than the sign-in rate from practical ex-
perience), we think that the IP address is potentially abused
by attackers.

For the PIPs with high goodness scores, around 35% of
them are potentially abused. Sign-ups from abused PIPs
have much lower reputation scores compared to non-abused
good PIPs. Among the 216K sign-ups from abused PIPs,
we observe that only 10.6% sign-ups have good reputation
scores. In contrast, almost 40% of the sign-ups from non-
abused PIPs have good reputation scores and 34.0% are in-
termediate cases. Although we still observe 26.9% of the
sign-ups from good PIPs display malicious behavior later,
these could be low rate malicious sign-ups that escape the
abuse detection system. As it is essentially hard to com-
pletely stop attackers from signing up malicious accounts
for spamming, the ability to push attackers into a low rate
mode and limit the number of their malicious accounts is of
great value to the service provider.

Using the goodness scores of PIP addresses and the abuse
detection method, we introduce the following sign-up early-
warning system that ﬂags new users right on the day of the
sign-ups. We label users related to the “normal” PIPs as po-
tentially good and label users related to the other cases as
potentially bad. Using this method, we successfully detect
more than 3 million bad sign-ups as shown in Table 9. We
have a precision of 97.0% in detecting malicious accounts
by misclassifying a small number of good users (i.e., false
positives). We also have a high recall of 96.0% by accept-
ing a moderate number of false negatives. The prediction of
good users is less accurate, as 40.6% of them turn out to be
malicious. While our techniques does not achieve a perfect
accuracy, they provide early signals on the day of sign-ups
and can be used in conjunction with previously proposed
techniques (e.g., the Hotmail user reputation system, the
anomaly detection methods) to speed up or improve conﬁ-
dence in malicious user detection. We leave the comparison
to Quova’s proxy list to Appendix B.
5.2 Hotmail Malicious Account Detection

In this section, we apply our PIP list to another appli-
cation — the Hotmail malicious user sign-in problem. We
use bad PIPs to help perform postmortem forensic analy-
sis to identify malicious accounts that slipped through the
Hotmail user reputation system.

Intuitively, there should not be many good users (with
high reputation scores) using bad PIPs. However, this as-
sumption might not be true because a normal user’s com-
puter can be hacked and turned into a bad PIP so this nor-

0.6%
0.3%

1.0% 32.1% 66.2% 0.1%
0.5% 21.6% 77.5% 0.1%

# Users % % Inter- % % De- % Un-
in 1,000 Good mediate Bad lected known

Case
A ∩ B
551.9
A ∪ B 1,154.2
Table 10: The user reputation in July 2011, for users who
had positive reputation back in August, 2010. We consider
two criteria: (A) user name is within the top 3 naming pat-
terns; (B) user registration date is in the top 1 registration
month. First row: Both criteria are true. Second row: Ei-
ther criteria is true.

mal user’s activities are mixed with bad users. To increase
conﬁdence, we correlated events from diﬀerent bad PIPs to-
gether and identify users that login from multiple bad PIPs.
To further distinguish bad users from good ones, we conser-
vatively use the following two criteria:
Top Registration Month: This is similar to the valida-
tion method we use in Section §4.3. For each PIP, we place
accounts into bins according to their registration month. As
a bad PIP is dominantly used by malicious users, an account
has a high chance of being malicious if it falls into the top
bin that has the most number of accounts.
Top 3 Naming Patterns: The Hotmail group extracts
the naming pattern (how the user name is structured, e.g.,
words plus 2 digits) of each user account, and we place ac-
counts into into bins according to their naming patterns.
Accounts that fall into the top 3 bins on a bad PIP are also
very suspicious.

For users with good reputation scores as of August, 2010,
but login from at least 2 bad PIPs in that month, we exam-
ine the above two criteria. We consider two cases here: (i)
Both criteria are true, and (ii) either criterion is true. Sim-
ilar to the previous sections, we check the reputation scores
according to the reputation system in July 2011. Table 10
shows that only less than 0.6% of the users remain good 11
months later for both cases.
In the “both true” case, we
observed that more than 66% of the accounts are deleted,
and more than 32% of the accounts are eventually classiﬁed
as bad. This suggests that our PIP list can eﬀectively de-
tect malicious users months before the Hotmail reputation
system does.

6. CONCLUSION

The ability to distinguish bad or abused populated IP ad-
dresses from good ones is critical to online service providers.
As a ﬁrst step towards this direction, we propose PIPMiner,
a fully automated method to classify PIPs with a high ac-
curacy. We demonstrate that the labeled PIP list can help
identify attacks months before other detection methods. Al-
though PIP lists are derived from service logs and thus may
be application-speciﬁc, we show that they can be applied
across datasets to detect new attacks. An interesting future
direction is to study the overlaps and correlations among
PIPs derived from diﬀerent services, and to merge these
PIPs for detecting attacks that would be hard to identify
by a single service provider alone.

338Acknowledgements
We thank Martin Abadi and Qifa Ke for their valuable ad-
vice and thank Mihai Budiu and Jon Currey their help on
DryadLINQ. We are also grateful to Linda McColm, Keiji
Oenoki, Krish Vitaldevara, Vasanth Vemula from the Hot-
mail team, and Jeﬀ Carnahan, Harry Katz, Ivan Osipkov,
Robert Sim from the Windows Live Safety Platform team
for providing us with data and valuable comments.
References
[1] GML AdaBoost Matlab Toolbox.

http://goo.gl/vh0R9.

[2] Networks enterprise data acquisition and IP rotation

services. http://x5.net.

[3] Quova. http://www.quova.com/.
[4] ToR network status.

http://torstatus.blutmagie.de/.

[5] J. D. Brutlag. Aberrant behavior detection in time

series for network monitoring. In USENIX Conference
on System Administration, 2000.

[6] X. Cai and J. Heidemann. Understanding block-level
address usage in the visible Internet. In SIGCOMM,
2010.

[7] M. Casado and M. J. Freedman. Peering through the
shroud: The eﬀect of edge opacity on IP-based client
identiﬁcation. In NSDI, 2007.

[8] C.-C. Chang and C.-J. Lin. LIBSVM: A library for

support vector machines. ACM Transactions on
Intelligent Systems and Technology, 2011.

[9] R. Dingledine, N. Mathewson, and P. Syverson. ToR:

The second-generation onion router. In USENIX
Security Symposium, 2004.

[10] H. Eidnes, G. de Groot, and P. Vixie. Classless
IN-ADDR.ARPA delegation. RFC 2317, 1998.

[11] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang,

and C.-J. Lin. LIBLINEAR: A library for large linear
classiﬁcation. Journal of Machine Learning Research,
2008.

[12] S. Hao, N. A. Syed, N. Feamster, A. G. Gray, and

S. Krasser. Detecting spammers with SNARE:
Spatio-temporal network-level automatic reputation
engine. In USENIX Security Symposium, 2009.

[13] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly.

Dryad: Distributed data-parallel programs from
sequential building blocks. In EuroSys, 2007.

[14] J. P. John, F. Yu, Y. Xie, M. Abadi, and

A. Krishnamurthy. Searching the searchers with
searchaudit. In USENIX Security, 2010.

[15] J. Jung and E. Sit. An empirical study of spam traﬃc

and the use of DNS black lists. In IMC, 2004.

[16] E. Katz-Bassett, J. P. John, A. Krishnamurthy,

D. Wetherall, T. Anderson, and Y. Chawathe.
Towards IP geolocation using delay and topology
measurements. In IMC, 2006.

[17] H.-T. Lin, C.-J. Lin, and R. C. Weng. A note on

Platt’s probabilistic outputs for support vector
machines. Mach. Learn., 2007.

[18] A. Metwally and M. Paduano. Estimating the number

of users behind IP addresses for combating abusive
traﬃc. In KDD, 2011.

[19] S. Nagaraja, P. Mittal, C.-Y. Hong, M. Caesar, and

N. Borisov. BotGrep: detecting P2P botnets using
structured graph analysis. In USENIX Security
Symposium, 2010.

[20] J. C. Platt. Probabilistic outputs for support vector
machines and comparisons to regularized likelihood
methods. In Advances in Large Margin Classiﬁers,
1999.

[21] J. R. Quinlan. C4.5: programs for machine learning.

Morgan Kaufmann Publishers Inc., 1993.

[22] A. Ramachandran, N. Feamster, and D. Dagon.

Revealing botnet membership using DNSBL
counter-intelligence. In Usenix Workshop on Steps to
Reducing Unwanted Traﬃc on the Internet (SRUTI),
2006.

[23] A. Ramachandran, N. Feamster, and S. Vempala.

Filtering spam with behavioral blacklisting. In CCS,
2007.

[24] G. Stringhini, T. Holz, B. Stone-Gross, C. Kruegel,
and G. Vigna. Botmagniﬁer: Locating spambots on
the Internet. In USENIX Security Symposium, 2011.

[25] L. Wang, K. S. Park, R. Pang, V. Pai, and

L. Peterson. Reliability and security in the codeen
content distribution network. In USENIX ATC, 2004.

[26] Y. Xie, V. Sekar, D. A. Maltz, M. K. Reiter, and

H. Zhang. Worm origin identiﬁcation using random
moonwalks. In IEEE Symposium on Security and
Privacy, 2005.

[27] Y. Xie, F. Yu, and M. Abadi. De-anonymizing the
internet using unreliable ids. In SIGCOMM, 2009.

[28] Y. Xie, F. Yu, K. Achan, E. Gillum, M. Goldszmidt,
and T. Wobber. How dynamic are IP addresses? In
SIGCOMM, 2007.

[29] Y. Xie, F. Yu, K. Achan, R. Panigrahy, G. Hulten,
and I. Osipkov. Spamming botnets: Signatures and
characteristics. In SIGCOMM, 2008.

[30] Y. Yu, M. Isard, D. Fetterly, M. Budiu, U. Erlingsson,

P. K. Gunda, and J. Currey. DryadLINQ: A system
for general-purpose distributed data-parallel
computing using a high-level language. In OSDI, 2008.

[31] Y. Zhao, Y. Xie, F. Yu, Q. Ke, Y. Yu, Y. Chen, and
E. Gillum. BotGraph: Large scale spamming botnet
detection. In NSDI, 2009.

APPENDIX
A.

IMPLEMENTATION

Our implementation consists of two stages.

In the ﬁrst
stage, we parse the data sets and extract PIPs with their
features and labels. We implement this stage in a parallel
fashion by harnessing a cluster of 240 machines. The sec-
ond stage is the training and testing stage. Fortunately, the
input to this stage is small enough (e.g., less than 200 MBs
for 1.7 million PIP addresses), allowing us to implement it
on a single machine.
Data Parsing and Feature Extraction: The ﬁrst stage
is implemented on top of Dryad, an execution engine allow-
ing distributed data-parallel computation [13]. Our primary
input data is a large collection of user login records. The
records are hash partitioned to a set of processing machines
according to the user login IP address, and then each ma-
chine independently computes per-IP account counts and

339per-IP request counts to derive PIP list and blocks. We
eﬃciently merge the remaining data sets such as the user
account data by performing distributed join operations. We
then partition the records that are associated with a PIP
address based on its block ID, ensuring that the records in
the same PIP blocks are processed by the same comput-
ing node. This is a critical phase to minimize the system
overhead, allowing computing nodes to derive PIP features
independently.

We further optimize the system performance using three
methods. First, we conﬁgure DryadLINQ to use dynamic
partitioning range keys, which are determined at run time
by sampling the input datasets. This helps balance the data
load on each machine, especially for our second partition
phase where the input key distribution is hard to estimate
in advance. Second, we minimize the cross-node communi-
cation cost by compressing the intermediate results. This re-
duces the communication overhead by ∼68%. Finally, some
PIP blocks consist of a very large number of IP addresses
(e.g., mobile gateways), each of which can potentially be as-
sociated with a disproportionate amount of records. This
can lead to an increased computation load on certain com-
puting nodes, making them the bottleneck of the operations,
especially for large data sets. To sidestep this, we randomly
sample requests of only the top 20 PIPs, which originally
carry the largest number of user requests. The per-PIP
sampling rate is automatically chosen such that the top 20
PIPs will ﬁnally have a similar number of user requests of
the 21st largest PIP. This scheme does not signiﬁcantly bias
the classiﬁcation results because PIPMiner requires only a
small number of requests per PIP to achieve high classiﬁca-
tion accuracy. This scheme, however, can reduce the overall
computation time for feature extraction by 35% − 55%.
Training and Testing: For training and testing, we ran
our experiments on a single machine with Quad Core CPU
and 8 GB memory. We use the LIBSVM [8] and LIBLIN-
EAR [11] toolkits in our implementation. To compare with
other classiﬁcation algorithms, we use GML AdaBoost [1]
and Quinlan’s C4.5 [21] decision tree. We also tested the
Matlab built-in classiﬁcation algorithms,
including Naive
Bayes, bagged decision trees and linear discriminant anal-
ysis (LDA), for performance comparison.

B. COMPARISON TO QUOVA PROXY LIST
We have shown that our PIP list can help ﬂag malicious
sign-ups right on the day of the sign-ups. This section
attempts to answer the question: Does our labeled PIP
list have wider applicability compared to commercial proxy
lists? To answer this question, we extract proxy IP addresses
in Quova’s GeoPoint data using the IP Routing Type ﬁeld
and apply the Quova proxy list to the Windows Live ID
sign-up abuse problem. We ﬁnd that >99.9% of the Quova
proxy IPs are not associated with any sign-up activities, and
99.8% of the PIPs that have good sign-ups are not included
by Quova’s list. For Quova proxy IPs that do have activities,
to ensure fair comparison, we only look at those that are as-
sociated with greater than 20 sign-up requests per month.
These proxies are categorized into diﬀerent types by Quova,
including mobile gateways, AOL proxies, regional proxies,
and international proxies. For each type of Quova prox-
ies, Figure 8 shows the fraction of good and bad sign-ups.

Figure 8: Given a certain type of IP addresses, the percent-
age of good and bad accounts. We use the Hotmail user
reputation trace in July, 2011 to classify users into good,
bad, intermediate, and unknown.

Case

|P|: Intl. Proxies (Quova)

|G|: Our good PIPs
|P ∩ G|: # Covered

|P ∩ G|/|P|: % Coverage

IP

Good

Address Sign-ups

1,164

3,951

203,087

397,539

939

80.6%

3,467
87.7%

Table 11: The number of good sign-ups that are originated
from our good PIPs and from Quova’s international proxies.

Clearly, mobile gateways, AOL proxies and regional proxies
all have mixed sign-up behaviors (20% of the sign-ups are
good and 40% of the sign-ups are bad), letting through a lot
of malicious sign-ups. Although Quova’s international prox-
ies are associated with a large percentage of good sign-ups,
our good PIP list already covers most of them: 80.6% of the
IPs and 87.7% of the good sign-ups as shown in Table 11.

C. TIME FORECASTING FEATURES

We apply forecasting models on data time series to ﬁnd
out abnormal periods, which might be the abuse periods of
good PIPs. As we observed strong periodicities from many
good PIPs, we adopt the additive Holt-Winters’ seasonal
forecasting model [5], which decouples a time series T [t] into
three factors: level L[t], trend B[t] and seasonal S[t]:

L[t] = α(T [t] − S[t − υ]) + (1 − α)(L[t − 1] + B[t − 1])
B[t] = β(L[t] − L[t − 1]) + (1 − β)(B[t − 1])
S[t] = γ(T [t] − L[t]) + (1 − γ)(S[t − υ])

The season length is denoted by υ, and the update rate pa-
rameters are α, β, and γ. Then the forecast F [t] simply adds
up these three factors, i.e., F [t] = L[t−1]+B[t−1]+S[t−υ].
To quantify whether the time series can be accurately pre-
dicted by the forecasting model, we call it an anomaly when
the forecast error is relatively large (i.e., the diﬀerence and
the ratio between the forecast value F [t] and the actual
value T [t] is larger than some thresholds). A grid search
is automatically performed to ﬁnd the best parameters and
thresholds that maximizing the cross-validation accuracy on
the sampled training data. For the anomaly threshold, we
search in diﬀerent multiples of the standard deviation of the
time series. Our features include the number and the density
of the low-volume anomalies (T [t] (cid:28) F [t]) and high-volume
anomalies (T [t] (cid:29) F [t]).

0%20%40%60%80%100%Quova Proxy Lists Our PIP List Good sign-ups: Bad sign-ups: Percentage of good/bad sign-up accounts Type of IP addresses 340