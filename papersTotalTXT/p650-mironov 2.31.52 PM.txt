On Signiﬁcance of the Least Signiﬁcant Bits For

Differential Privacy

Ilya Mironov

Microsoft Research Silicon Valley

1065 La Avenida

Mountain View, CA, 94043
mironov@microsoft.com

ABSTRACT
We describe a new type of vulnerability present in many im-
plementations of diﬀerentially private mechanisms. In par-
ticular, all four publicly available general purpose systems
for diﬀerentially private computations are susceptible to our
attack.

The vulnerability is based on irregularities of ﬂoating-
point implementations of the privacy-preserving Laplacian
mechanism. Unlike its mathematical abstraction, the text-
book sampling procedure results in a porous distribution
over double-precision numbers that allows one to breach dif-
ferential privacy with just a few queries into the mechanism.
We propose a mitigating strategy and prove that it sat-
isﬁes diﬀerential privacy under some mild assumptions on
available implementation of ﬂoating-point arithmetic.

Categories and Subject Descriptors
H.2.7 [Database Management]: Database Administra-
tion—Security, integrity, and protection

General Terms
Security

Keywords
diﬀerential privacy, ﬂoating point arithmetic

1.

INTRODUCTION

The line of work on privacy of statistical databases start-
ing with seminal papers by Dwork et al. [13, 9, 12] advanced
a general approach towards achieving privacy via random-
ization of outputs.
Instead of releasing accurate answers
to statistical queries, which potentially leads to a breach of
privacy, the curator of a database randomizes its responses.
For numerical outputs, the randomization process typically
involves adding to the accurate answer a secret random
number (noise) sampled from a publicly-known distribution,

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’12, October 16–18, 2012, Raleigh, North Carolina, USA.
Copyright 2012 ACM 978-1-4503-1651-4/12/10 ...$15.00.

whose shape and parameters are chosen to guarantee a cer-
tain level of privacy.

Realizations of diﬀerentially private mechanisms include
privacy-preserving histograms [28], logistic regressions [3],
recommender systems [33], network-analysis tools [32], and
many others.
In response to the need for solutions that
would simplify development of privacy-preserving systems,
with speciﬁc focus on cloud computing, several platforms
were proposed in recent years: PINQ [30], Airavat [39],
Fuzz [18], and GUPT [34]. These systems diﬀer in their im-
plementation languages, target operating systems, and pro-
gramming models, and yet they share essentially the same
noise-generation mechanism central to diﬀerential privacy.

We describe a new type of vulnerability present in all gen-
eral purpose systems and many implementations of diﬀeren-
tially private algorithms available to us. The vulnerability,
which can be remarkably eﬀective against diﬀerentially pri-
vate systems, exploits the porous distributions of textbook
implementations of the Laplacian mechanism resulting from
ﬁnite precision and rounding eﬀects of ﬂoating-point oper-
ations. While diﬀerential privacy requires all outputs to be
feasible for all possible inputs and having similar probabil-
ities when the inputs are close, the results of ﬂoating-point
arithmetic will be concentrated on a small subset of out-
puts. Careful examination of these subsets demonstrate that
they overlap only partially on close inputs, thus breaking the
guarantee of diﬀerential privacy, which would have applied
if all operations were computed with inﬁnite precision and
unlimited source of entropy.

Our attack defeats systems that were speciﬁcally designed
to eliminate side-channel vulnerabilities, employing strate-
gies such as LSM-based mandatory access control mecha-
nisms [39, 34], or a generalized linear-type system [18]. We
remark that while these systems carefully block all tradi-
tional side channels—timing and state—the output of the
diﬀerentially private mechanism, which is supposed to be
safe to release due to its mathematical properties, creates
a side channel of its own. The reason for this exploitable
vulnerability is that ﬂoating-point implementations of dif-
ferentially private algorithms deviate in important respects
from their mathematical abstractions.

Although workarounds are relatively simple and low over-
head, the fact that these independent and very diverse im-
plementations are all susceptible to the same attack, demon-
strates that subtle properties of ﬂoating-point arithmetic can
be easily overlooked.

The paper’s organization is as follows. We recall the def-
inition of diﬀerential privacy and describe the Laplacian

650mechanism in Section 2. Platforms for diﬀerentially private
computations are reviewed in Section 3, followed by discus-
sion of relevant facts of ﬂoating-point arithmetic and its im-
plications to implementations of the Laplacian mechanism
in Section 4. We describe our main attack in Section 4.5,
analyze its success probability in Section 4.6, and discuss
its implications for privacy. Section 5 on defense mecha-
nisms considers two ﬂawed approaches, followed by proof of
security of a ﬂoating-point implementation of the snapping
mechanism. Another example of vulnerability due to use of
ﬂoating-point arithmetic is given in Section 6.

2. DIFFERENTIAL PRIVACY

Diﬀerential privacy, introduced by Dwork et al. [9, 12] and
recently surveyed in [10], is a standard notion of privacy for
computations over databases. It is deﬁned for randomized
functions (i.e., distributions over the set of possible outputs)
and can be formally stated as follows:

Randomized function f : D 7→ R satisﬁes ϵ-diﬀerential
privacy if for all subsets S ⊂ R and for all pairs
of adjacent inputs D, D

′ ∈ D:

Pr[f (D) ∈ S] ≤ eϵ Pr[f (D

′

) ∈ S].

The exact deﬁnition of two databases being adjacent is do-
main-speciﬁc and dependent on the desired level of security
guarantee. Commonly used deﬁnitions are the user-level pri-
vacy, where two databases are considered adjacent if they
diﬀer in contribution of a single person, and the entry-level
privacy, where the protected entity is a single item, action,
or attribute.

An equivalent Bayesian interpretation of the deﬁnition of
diﬀerential privacy asserts that if f is ϵ-diﬀerentially-private
and O is the observed output of f , then for all priors p and
all pairs of adjacent D and D
p(D | O)
p(D′ | O)

≤ eϵ p(D)
p(D′)

.

′

If two database are adjacent when they diﬀer in records per-
taining to one individual, we say that the adversary’s prob-
ability of guessing whether any one person, chosen after in-
teracting with the mechanism, is present in the database
increases multiplicatively by at most eϵ (≈ 1 + ϵ for small ϵ).
This guarantee holds for any auxiliary information, modeled
as a prior on the database or that person.

One important consequence of this interpretation is that
all outputs of f that have non-zero support in f (D) must
also be feasible under f (D
). Otherwise, the adversary could
rule out D
for some outputs in violation of the deﬁnition of
diﬀerential privacy.

′

′

If the function of interest is not diﬀerentially private, such
as when it is deterministic, it can be approximated by a dif-
ferentially private function. Minimizing the loss of accuracy
due to approximation, or the noise level, is the major re-
search goal in diﬀerential privacy.

We emphasize that just adding noise to the output is nei-
ther necessary nor suﬃcient for achieving a meaningful level
of privacy. Noiseless privacy has been explored in the work
of Bhaskar et al., which taps into uncertainty that the adver-
sary may have about the input into the database [1]; Blocki
et al. describe a randomized functionality (the Johnson-
Lindenstrauss transform) that oﬀers a relaxation of diﬀer-
ential privacy without any additional noise [2].

More importantly, some (or even substantial) amount of
additive noise is not by itself a guarantee of diﬀerential pri-
vacy. For instance, several of the “doughnut-shaped” noise
distributions proposed for disclosure limitation of microdata
collected by the US Census Bureau [14] do not achieve dif-
ferential privacy despite introducing a non-trivial amount of
noise.

Consider a diﬀerent example, which captures the essence
of our attack. A counting query returns the number of
records in a database satisfying a certain predicate. If the
noise added to the accurate output of the query is always
even, then the adversary can trivially win the diﬀerentially
private game (guessing whether the query was evaluated on
, such that their counts diﬀer by 1) by
D or an adjacent D
looking at the parity of the output.
In this example, the
adversary’s probability of success is independent of the ab-
solute value of the noise, i.e., accuracy of the answer to the
query.
2.1 Laplacian mechanism

′

The basic method for achieving diﬀerential privacy is called
the Laplacian mechanism. The mechanism is additive, i.e.,
given the function f it approximates its output by comput-
ing the function exactly and then adding noise sampled from
a speciﬁc distribution. The distribution is Laplacian and in
order to preserve privacy of function f , the distribution is
scaled to f ’s sensitivity, deﬁned below.

We say that a deterministic real-valued function
f : D 7→ R has sensitivity at most ∆ if for all
adjacent databases D and D
|f (D) − f (D

′

The (zero-mean) Laplacian distribution Lap(λ)
is continuous probability distribution deﬁned by
its density function

′
:
)| ≤ ∆.
(
)

−|x|

.

λ

hλ(x) , 1
2λ

exp

The classic result from literature on diﬀerential privacy
establishes that the following transformation of f , called
Laplacian mechanism, is ϵ-diﬀerentially-private if f has sen-
sitivity ∆:

˜fϵ(D) , f (D) + Y, where Y ← Lap(∆/ϵ).

Ghosh at el. [15] proved that (a discrete version) of the
Laplacian mechanism is optimal and universal, i.e., it is the
only mechanism the curator has to implement to support
integer-valued functionalities and a large class of loss func-
tions.
′

The distribution of ˜fϵ on two inputs where f (D) = 0 and

f (D

) = 1 is plotted in Figure 1.

3. OVERVIEW OF PRIVACY-PRESERVING

PLATFORMS

Many sophisticated diﬀerentially private algorithms have

been implemented and evaluated on real data, such as privacy-
preserving logistic regressions [3], the matrix mechanism [27]
or the exponential mechanism with the multiplicative weights
update rule (MWEM) [19]. Doing it right, however, is an
error-prone and highly non-trivial task, akin to implement-
ing one’s own cryptography. Furthermore, custom analyses

651˜f (D)
′
˜f (D
)

0.4

0.3

0.2

0.1

0

-5 -4 -3 -2 -1

0

1

2

3

4

5

Figure 1: Distributions of ˜fϵ(D) = f (D) + Lap(∆/ϵ)
over two inputs: f (D) = 0, f (D
) = 1, sensitivity ∆ =
1, privacy parameter ϵ = 0.5.

′

of privacy-preserving algorithms are expensive and labor-
intensive eﬀorts requiring experts in the loop.
It limits
applications of diﬀerential privacy to problems that justify
that expense, excluding scenarios that allow exploration of
the query space by untrusted or non-expert participants.
Examples of these attractive scenarios include datamining,
where analysts are restricted to issuing diﬀerentially pri-
vate queries, and a Netﬂix-style crowdsourcing competition,
where participants train their systems on sensitive data via
a diﬀerentially private on-line interface.

The ﬁrst system whose ambition was to facilitate adop-
tion of diﬀerential privacy in practice by relieving developers
from having to certify privacy of their code was PINQ [30,
31]. PINQ (Privacy INtegrated Queries) is a declarative
programming language that guarantees diﬀerential privacy
as long as the dataset is accessed exclusively through PINQ
queries. PINQ’s research goal was to identify a set of data
transformation and aggregation operators that would be ex-
pressive enough for a range of analytical tasks and yet allow
for prove-once-run-everything approach. A PINQ prototype
is available as a C# LINQ library.

Airavat [39] is a Hadoop-based MapReduce programming
platform whose primary goal is to oﬀer end-to-end security
guarantees, while requiring minimal changes to the program-
ming model or execution environment of big data computa-
tions. In particular, it allows execution of untrusted map-
pers by integrating SELinux-style mandatory access control
to the Java Virtual Machine, the Hadoop framework, and
the distributed ﬁle system. In contrast with PINQ, whose
guarantees are language-based and whose current implemen-
tation treats developers as cooperating entities, Airavat ac-
cepts arbitrary Java bytecode as mappers and then uses
trusted reducers and system-level isolation mechanisms to
ensure compliance with privacy policies.

Fuzz [18] employs a novel type system [38] to facilitate
static analysis of sensitivity of arbitrary data transforma-
tions. It further limits the subset of allowed mappers through
the use of a domain-speciﬁc programming language, which
requires primitives to supply timeout information and en-
forces constant execution time on basic data manipulation
routines.

The most recent of these systems, GUPT [34], explores
a diﬀerent point in the design space of privacy-preserving
computations. It uses the sample-and-aggregate framework

due to Nissim et al. [36], which allows execution of an arbi-
trary client-provided program over a sample of the original
dataset. Accuracy is boosted by running the program mul-
tiple times over many samples and averaging the outputs,
and privacy is enforced by adding Laplacian noise to the
average.

PINQ, Airavat, Fuzz, and GUPT, while diﬀerent in their
design goals, philosophy and languages of implementation,
all expose instantiations of the Laplacian mechanism, where
the numerical output of a computation of bounded sensitiv-
ity is released after adding noise sampled from a Laplacian
distribution.

4. LSBS AS A SIDE CHANNEL

As we saw in the previous section, the Laplacian mech-
anism is the staple of general-purpose diﬀerentially private
systems. These systems are extremely diverse in their im-
plementation details, yet they share variants of the same
sampling procedure, which we reproduce below.
4.1 Sampling from Laplacian

The Laplacian distribution Lap(λ), sometimes called the
symmetric exponential distribution, can be thought of as the
exponential distribution assigned a randomly chosen sign.
The exponential distribution is deﬁned by its cumulative
distribution function (cdf)

F (t) , Pr[Y ≤ t] = 1 − e

−t/λ.

The most common method of generating Y is based on the
generic procedure, called the inverse sampling method. To
draw from the probability distribution given the inverse of
−1(·), and a source of uniform randomness U from
its cdf, F
the [0, 1) interval, compute the following:

Y ← F

−1(U ).

To see that the cdf of Y is indeed F (·), one can check that:

Pr[Y ≤ t] = Pr[F

−1(U ) ≤ t] = Pr[U ≤ F (t)] = F (t).

Computing the inverse cdf F

−1(·) happens to be partic-
ularly simple, which results in the following procedure for
sampling from the exponential distribution with parame-
ter λ:

Y ← F

−1(U ) = −λ ln(1 − U )

(the distribution 1 − U can be replaced by the uniform dis-
tribution over (0, 1]). This method appears in standard ref-
erences and libraries [23, 37].

To transform Y into the Laplacian distribution one chooses

sign at random, resulting in the following procedure:

Y ← (2Z − 1) · λ ln(U ),

(1)
where Z is an integer-valued variable uniform over {0, 1},
and U is a real-valued variable uniform over (0, 1]. In prac-
tice most random number generators sample from [0, 1), but
the probabilities of generating the exact zero or one are small
enough that they can be ignored. A twist on the method
above uses a single draw from the uniform distribution on
(0, 1) to generate the sign and the absolute value of the
Laplacian variable:

Y ← sign(r) · λ ln(1 − 2|r|), where r ← U − 0.5.

(2)

652All diﬀerentially private systems considered by this paper
use one of the two methods (1) and (2) to sample a Lapla-
cian1.
4.2 Floating-point numbers and notation

The procedure presented in the previous section is sim-
ple and eﬃcient, requiring only a single sample from the
(0, 1) interval. Unfortunately, simplicity of this procedure
is deceptive, since once implemented in ﬂoating-point arith-
metic, its behavior deviates markedly from its mathematical
abstraction.

Before we proceed with discussion of implementations of
diﬀerentially private mechanisms, we review basic facts about
ﬂoating-point arithmetic and introduce some notation. For
a brief introduction to the subject see Goldberg [17]; Muller
et al. serves as a current reference [35].

All systems considered in this paper work over double-
precision ﬂoating-point numbers or just doubles, for short.
Doubles occupy 64 bits and reserve 1 bit for sign, 11 bits
for exponent, and 52 bits of signiﬁcand or mantissa. The
exponent e is represented as an oﬀset from 1023, thus the
−1022 and the largest
smallest representable exponent is 2
exponent is 21023 (e = 0 is used to represent 0 and subnormal
numbers, e = 2047 represents inﬁnity and NaNs, which we
will ignore). The ﬁrst bit of signiﬁcand has an implicit value
of 1, which gives the total precision of 53 signiﬁcant bits.
If the sign is s, the exponent is e, and the signiﬁcand is
d1 . . . d52, the corresponding ﬂoating point number is

(−1)s(1.d1 . . . d52)2 × 2e−1023.

The take-away point is that values representable as doubles
are spaced non-uniformly throughout the real line. There
are exactly 252 representable reals in the interval [.5, 1), 252
reals in the interval [.25, .5), etc.

The ubiquitous IEEE ﬂoating-point standard [21] requires
the results of basic arithmetic operations (addition, subtrac-
tion, multiplication, division) to be exactly rounded, i.e., be
computed exactly and then rounded to the closest ﬂoating-
point number. We will use ability to compute over ﬂoating-
point numbers with maximal precision in Section 5, where
we introduce and analyze a sampling procedure.

To distinguish between real numbers and doubles, we de-
note the latter as D. The literature on ﬂoating-point arith-
metic has a name for the value of the least signiﬁcant digit of
a ﬂoating-point number, called the unit in the last place, or
ulp for short. For numbers from the set D ∩ (0, 1) (i.e., dou-
−1022
bles from the (0, 1) interval) their ulps vary between 2
and 2

−53.

To diﬀerentiate between exact, mathematical, functions
and arithmetic operations, we will use the following nota-
tion, common in the literature: ⊕,⊖,⊗,⊘ are ﬂoating-point
analogues of addition, subtraction, multiplication, and divi-
sion; LN(·) stands for a ﬂoating-point implementation of the
natural logarithm function.
4.3 Sampling from the uniform distribution

Given a non-uniform density of doubles, a uniform dis-
tribution over (0, 1) is not well deﬁned. It turns out that
standards usually oﬀer no guidance and various references

and libraries approximate the distribution diﬀerently (see
Table 1 for a summary). For example, the distribution gen-
erated by Java’s Random.nextDouble() is conﬁned to integer
multiples of 2

−53.

Reference and Library
Knuth [23]
“Numerical Recipes” [37]
C#
SSJ (Java) [26]
Python
OCaml

Uniform from [0, 1)
−53
−64

multiples of 2
multiples of 2

multiples of 1/(231 − 1)
−53
multiples of 2

−32 or 2
−53
−90

multiples of 2
multiples of 2

Table 1: Support of random doubles.

Many of these sources of randomness are not cryptographic,
i.e., having insuﬃcient entropy and/or predictable, and thus
not appropriate for security applications, including diﬀeren-
tial privacy. We enumerate various strategies for sampling
“uniform” ﬂoating-point numbers to emphasize lack of con-
sistency in this area.
4.4 Sampling from Laplacian: Examples

As we saw in the previous section, samples from the uni-
form distributions are most likely conﬁned to a small sub-
set of doubles, equally spaced throughout the (0, 1) interval.
Even if care is taken to include all doubles in support of the
uniform distribution, transforming the uniformly distributed
random variable into the Laplacian via formulas (1) or (2)
will generate artifacts of its own.

Consider the following, actual example of applying the
log function (from C++’s <math.h>) to the set of 10 double
numbers following 1/π (there is nothing special about this
number from the point of view of sampling Laplacians, which
is why it is chosen for illustrative purposes):

It is apparent that some output values are more frequent
than others. For some other starting points, another phe-
nomenon is in eﬀect: some potential output values from D
become missing (indicated on the diagram by empty squares):

1Current version of Airavat has a stub in lieu of its noise
generation routine; we instantiate it with SSJ [26]—the li-
brary used in reporting performance and accuracy results by
Roy et al. [39].

Multiplying the output of the transformation U 7→ − ln(U )
by λ, to generate a scaled Laplacian Lap(λ), results in some
values being repeated several times and some, none at all.

x=1⊘πx+9·2−54LN(x)LN(x+9·2−54)LN(·)......x=2⊘πx+5·2−53LN(x)LN(x+5·2−53)LN(·)......653We repeat the experiment with x = 1⊘ π but this time mul-
tiply the result by 3, which corresponds to sampling from
the Laplacian mechanism with ∆ = 1 and ϵ = 1/3:

Since most random number generators do not output all
potential doubles (see Table 1), their actual approximation
of the Laplacian distribution will have missing values and
values that appear more frequently than they should.
4.5 Attack on ﬂoating-point implementation

of the Laplacian mechanism

Recall that the reason for introducing Laplacian noise is
to convert a deterministic function f into a diﬀerentially pri-
vate one by smoothing its output distribution. The proof of
diﬀerential privacy of the Laplacian mechanism [12] depends
on the fact that the probabilities of seeing the same output
of ˜fϵ(·) on two adjacent inputs are multiplicatively close to
each other.

′

If the ﬂoating-point implementation of the Laplacian dis-
tribution misses some output values on input D and oth-
ers on input D
, the resulting mechanism cannot satisfy
diﬀerential privacy. This is exactly what happens when
the Laplacian mechanism is instantiated with ﬂoating-point
arithmetic without appropriate defense mechanisms.

To diﬀerentiate between ideal and real-world realizations
of the Laplacian distribution and the diﬀerentially private
mechanism ˜fϵ(·), we will denote ﬂoating-point implementa-
∗
p(λ) and the corre-
tions of the Laplacian distribution Lap
ϵ,p(D) = f (D)⊕Lap
∗
∗
sponding Laplacian mechanism ˜f
p(∆/ϵ),
where p is the precision with which the uniform distribution
over (0, 1) is sampled. If the precision is maximal, i.e., all
doubles D are in the support of the uniform distribution, the
p index is dropped.

′

) = 1. Consider the
(3) (striped squares) and

Let ∆ = 1, f (D) = 0 and f (D

1/3(D) = 0 ⊕ Lap
∗
distribution of ˜f
) = 1 ⊕ Lap
∗
˜f
1/3(D

∗

∗

′

(3) (solid squares) around 1.5:

It is apparent from this example that if the output of the
Laplacian mechanism is 1.5 + 2· 2
′
−52, then its input was D
,
and if the output is 1.5 + 3 · 2
−52, then the input must have
been D.

Observing an output that is infeasible under one distri-
bution, constitutes “smoking gun” evidence that rules out
the corresponding input. It gives a simple measure of the
distribution’s deﬁciency, or a lower (conservative) bound on
the adversary’s advantage of breaching diﬀerential privacy.
Let the support of the distribution be deﬁned as

ϵ (D)) , {x ∈ D : Pr[ ˜f
ϵ (D) = x] > 0}.
∗
∗
supp( ˜f

Then the probability of diﬀerential privacy guarantee’s being
broken is at least

ϵ (D) /∈ supp( ˜f
∗
∗
Pr[ ˜f
ϵ (D

′

))],

since it captures the probability of the event that a sam-
∗
ple from ˜f
ϵ (D) falls outside the support of the distribu-
∗
′
tion ˜f
).
ϵ (D

Figure 2 plots this probability, expressed as a function
of λ = .01 . . . 3, for two ﬂoating-point implementations of
the Laplacian mechanism: f
f (D) = 0 and f (D

1/λ,53(·), for ∆ = 1,
∗

1/λ(·) and f
∗

) = 1.

′

For smaller λ’s, which should provide a lower (but still
non-trivial) level of diﬀerential privacy, the probability that
a single sample reveals the input is close to 100%. For larger
λ’s, presumably yielding stronger diﬀerential privacy, the
probability of a compromise never becomes less than 35%.
Lowering resolution of the uniform distribution (sampling
−53 instead of D∩ (0, 1)) increases
from integer multiples of 2
the success rate of the attacker by making the distributions
on D and D

more divergent.

′

This graph, that plots the probability of a catastrophic
breach of diﬀerential privacy, demonstrates that textbook
ﬂoating-point implementations of the Laplacian mechanism
are insecure. The results are reproducible (with some varia-
tions due to compiler options, math libraries, and settings of
CPU ﬂags) across languages (C/C++, Java, C#, Python,
OCaml), operating systems (Windows, Linux, OS X), and
processors (x86, x64).
In particular, all four systems sur-
veyed in Section 3 return to the programmer double-precision
ﬂoating-point answers, and thus enabling this attack.
4.6 Analysis

In addition to empirical data on success probability of
the attack, we present heuristic analysis of one particularly
important attack scenario, of λ much larger than f (D) or
f (D

).

′

′

We ﬁrst observe that, somewhat counterintuitively, the
attack does not lose its eﬀectiveness for larger values of λ
(corresponding to smaller decrements to the privacy bud-
get). Consider the success probability of the attacker af-
ter a single interaction with the Laplacian mechanism with
λ = 106, f (D) = 100 and f (D
) = 101. The consumed pri-
−6, which would nominally imply a neg-
vacy budget is ϵ = 10
−6) ≈
ligible increase in the adversary’s conﬁdence (exp(10
−6) if its mathematical guarantee held true. Instead,
1 + 10
for the textbook implementation of the ﬂoating-point Lapla-
cian mechanism, the probability of ruling out one of two
adjacent databases is almost 40%. It eﬀectively means that
one can consume the privacy budget in arbitrary small incre-
ments, without reducing capacity of the side channel. This
ability will be exploited in the next section, where we show
that the entire database can be extracted by asking suﬃ-
ciently many adaptively chosen queries.
′

Another reason to consider the case of a large λ, is that it
) as long as they are

applies to all values of f (D) and f (D
both much smaller than λ.

Figure 3 plots two functions. The ﬁrst (smooth graph) is
∗
the probability density function of ˜f
1/λ(D) for λ = 106 and
f (D) = 100. The range of the x-axis is [−2λ, 2λ], which cov-
ers 1− exp(−2) ≈ 0.865 fraction of output values under that
distribution. The second (ragged) line is the probability,
computed over small intervals [a, a + µa], of the event that
∗
an output of ˜f
1/λ(D) in that range is outside the support of

x=1⊘πx+9·2−543⊗LN(x)3⊗LN(x+9·2−54)3⊗LN(·)......1.5˜f∗1/3(D)˜f∗1/3(D′)............1.5+3·2−521.5+2·2−52654100%

80%

60%

40%

20%

y
t
i
l
i

b
a
b
o
r
p
”
n
u
g

g
n
i
k
o
m
s
“

◦◦◦◦◦◦◦◦◦◦◦◦
••••••••••••

◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦◦
•••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••••

•

maximal precision
−53

precision 2

•
◦

0%

0.0

0.4

0.8

1.2

1.6

2.0

2.4

2.8

∗
∗
1/λ(D) (solid dots) or ˜f
Figure 2: Probability that outputs from ˜f
1/λ,53(D) (hollow dots) fall outside support of
∗
′
˜f
) = 1.
)), as a function of λ. f (D) = 0 and f (D
1/λ(D

∗
) (resp., ˜f
1/λ,53(D

′

′

λ

′

[
∗
the distribution ˜f
1/λ(D
):
x /∈ supp( ˜f
∗
1/λ(D
x ∈ supp( ˜f
1/λ(D)) ∩ [a, a + µa]
∗

s(a) , Pr

))

(cid:12)(cid:12)(cid:12)

′

]

.

We use µ = 0.001 and f (D

′

) = 101.

The plot suggests that s(a) (computed over suﬃciently
small intervals) is smooth except with sharp discontinuities
at powers of 2 and certain multiples of λ. Indeed, consider an
interval [2k·u, 2k·v], when f (D
),
and 1/2 < u < v < 1. The set of doubles in this interval is
uniformly spaced and has cardinality 253(v − u).
On the other hand, the interval in the range of the uniform
distribution that is mapped to [2k · u, 2k · v] is approximately
(ignoring for a moment the contribution of f (D) ≪ 2k):

)+2k < ln 2·λ, 2k ≫ f (D

′

′

[exp(−2kv/λ), exp(−2ku/λ)],

which, for 2k < ln 2 · λ is contained in the [1/2, 1) inter-
val. The total number of doubles in that interval is 253 ·
(exp(−2ku/λ) − exp(−2kv/λ)). The ratio of the doubles in
the range and the domain of the mapping x 7→ −λ ln(x) is
thus

253 · (exp(−2ku/λ) − exp(−2kv/λ))

253(v − u)

,

which can be approximated for v ≈ u as

− exp(−2ku/λ)

′

(u) =

exp(−2ku/λ).

2k
λ

As 2k gets smaller compared to λ, the ratio quickly ap-
proaches 0. It means that for smaller values of k, the number
of doubles in the image of the mapping is much larger than
the number of doubles in its range.

Recall that the doubles output by the uniform distribu-
tion are spaced evenly. The logarithm function destroys
the arithmetic progression, resulting in a sequence that is
not translation invariant. The Laplacian mechanism on in-
puts D and D
produces two distributions shifted by 1, and

′

we may heuristically assume that a random element from
∗
the support of ˜f
1/λ(D) on [a, b] belongs to the support of
∗
˜f
) in
1/λ(D
[a, b], which we estimated earlier. It follows that

∗
) with probability close to the density of ˜f
1/λ(D

′

′

s(a) ≈ 2k
λ

exp(−a/λ),

where 2k is the smallest power of 2 larger than a. This
expression is in remarkable agreement with Figure 3, giv-
ing a tight lower bound on the ragged curve in the range
[−(ln 2)λ, (ln 2)λ]. Although some breaches of diﬀerential
privacy happen outside that range, our analysis explains al-
most 90% of all violations for this setting of the parameters.
4.7 A practical attack

To demonstrate that the vulnerability described in the
paper is more powerful than a simple breach of diﬀerential
privacy, we adapt the technique from the previous section
to extract the entire content of a database.

Consider a list of alphanumerical records and a query lan-
guage that reports counts of records sharing a particular
preﬁx. All systems considered by this paper allow this ac-
cess mechanism.

If histogram counts are protected with Laplacian noise
generated with a method vulnerable to our attack, we may
reconstruct the entire database by issuing adaptively-chosen
queries. We begin by obtaining an upper bound on number
of records in the database, then ask a histogram query on the
ﬁrst character of all records, i.e., ask the number of records
that start with ‘\0’, ‘\1’,. . . ,‘\255’. Initially, all counts up
to the upper bound are feasible, but with every query, ap-
proximately 40% of the counts not equal to the exact answer
can be excluded.

After a unique count for each ﬁrst character is identiﬁed,
we append all possible characters to preﬁxes with non-zero
support, and iterate the process until the entire dataset is
recovered. Since a single run of the attack is eﬀective even
for very large λ according to the analysis of the previous

655100%

80%

60%

40%

20%

0%
−2λ

ln 2 · λ
1/λ(D)) that x /∈ supp( ˜f
∗
∗
Figure 3: The ragged curve plots the probability for x in supp( ˜f
1/λ(D
∗
) = 101, and λ = 106. The smooth curve is pdf( ˜f
small intervals, where f (D) = 100, f (D
1/λ(D)).

− ln 2 · λ

2λ

218 219

λ 220

′

′

−220 − λ −219−218 0

)) averaged over

section, the attack can be executed within an arbitrarily
small total privacy budget.

We implemented the attack against PINQ, and veriﬁed
its eﬀectiveness. We were able to reconstruct a database
consisting of 18K records in a fewer than 1000 queries with
the total privacy budget smaller than 10

−6.

5. DEFENSE

The primary reason why the attack of the previous sec-
tion succeeds is because the standard ﬂoating-point Lapla-
cian sampling procedures (Section 4.1) result in very porous
distributions, missing out on many possible output values.
That would not be a problem if the missing values were the
same for adjacent inputs. Unfortunately, the distributions
are not translation invariant, and after being shifted by f (D)
and f (D

), they diverge signiﬁcantly.

′

These eﬀects are observable in vanishingly small scales,
bearing almost no relation on numerical accuracy or the
number of usable bits of the outputs. Even if a non-private
f (·) returns answers with many signiﬁcant digits, the Lapla-
cian mechanism degrades accuracy of the resulting mecha-
nism, introducing error with the standard deviation of ∆/ϵ.
We leverage both observations in designing the snapping
mechanism, whose ﬂoating-point implementation is proved
diﬀerentially private in Section 5.2: the set of output val-
ues is made independent of the input, and output values
are spaced approximately λ apart. Before we introduce the
snapping mechanism, we discuss two approaches that are
simple but insecure.
5.1 False starts

An appealing goal for a defense mechanism would be to
come up with a method for sampling from a “ﬂoating-point
aware” Laplacian distribution whose addition guarantees dif-
ferential privacy even when realized in ﬂoating-point arith-
metic. One beneﬁt of this strategy is its modularity—it
would be suﬃcient to prove and implement such distribu-

tion once, and use it in a variety of contexts that require
additive noise.

∗

∗

′

∗
′

2(cid:0)32 .

(λ)⌉

(λ)⌉

2(cid:0)32 and f (D

(λ) to the closest integer multiple of, say, 2

We analyze two most straightforward approaches towards
this goal, and ﬁnd them ineﬀective and possibly even more
detrimental to privacy than current implementations.
Rounding. It may appear that since gaps in the double-
precision outputs of the Laplacian sampling procedure are
only noticeable at the smallest possible scale, dropping the
least signiﬁcant bits of the Laplacian could increase security.
Consider, for example, the eﬀect of rounding the out-
−32,
put of Lap
denoted as ⌊Lap
It is easy to verify that un-
der some mild assumptions on accuracy of a ﬂoating-point
implementation of the log function and access to the uni-
form distribution on D ∩ (0, 1), the resulting distributions
) + ⌊Lap
(λ)⌉
for f (D) + ⌊Lap
∗
2(cid:0)32 will
have identical supports if f (D) = 0 and f (D
) = 1. More-
over, the mechanism conﬁned to these two output values is
going to be 1/λ-diﬀerentially private.
Recall, however, that the sensitivity of f is an upper bound
on the diﬀerence of f on adjacent inputs. Suppose f (D) −
−33, which may easily be arranged by an adver-
f (D
sarially chosen function or input. Adding a random vari-
−32 will have the eﬀect that
able rounded to a multiple of 2
the supports of two distributions of the Laplacian mecha-
nism applied to D and D
become completely disjoint, and
privacy is going to be lost after a single invocation of the
mechanism.
Smoothing. Taking the opposite approach, and trying to
smooth the additive Laplacian noise, ensuring that all dou-
bles are in its support, also fails to achieve diﬀerential pri-
vacy, albeit for a diﬀerent reason.
) =
1, and additive Lap(1) noise. With probability ≈ 20% the
random variable x = f (D) ⊕ Lap(1) belongs to the inter-
val (0, 1/2). Conditional on x ∈ (0, 1/2), ulp(x) is strictly
−53. Since we assume Lap(1) takes all possible
less than 2

Again, consider two potential inputs f (D) = 0 and f (D

) = 2

′

′

′

656values, with probability at least 50% x will not be an in-
−53. On the other hand, if 1 ⊕ y = x,
teger multiple of 2
where y is sampled from Lap(1), then y ∈ (−1,−0.5), and
−53. Since addition is performed exactly in this
ulp(y) = 2
range, 1 ⊕ y is going to be an integer multiple of 2
−53. It
) ⊕ Lap(1) will
means that in this range the support of f (D
be a proper (and relatively small) subset of the support of
f (D) ⊕ Lap(1).
5.2 Snapping Mechanism

′

In contrast with the ﬁrst ﬂawed method of the previous
section, where the lower bits of the Laplacian noise are sim-
ply tossed out, it turns out that doing so after adding the
noise yields a diﬀerentially private mechanism. The tech-
nique has been proposed in a diﬀerent context by Dodis et
al. [7], where it was shown to be an eﬀective defense strategy
when the mechanism is instantiated with imperfect random-
ness, modeled as a Santha-Vazirani source [40].

∗

Before we deﬁne the mechanism, which is proved to be
diﬀerentially private in Theorem 1, we introduce some no-
be the uniform distribution over D ∩ (0, 1),
tation. Let U
such that each double number is output with probability
proportional to its ulp; S be uniform over {−1, +1}. LN(·)
denotes a ﬂoating-point implementation of the natural loga-
rithm with exact rounding. Function clampB(x) outputs B
if x > B, −B if x < −B and x otherwise. Λ is the smallest
power of 2 (including negative powers) greater than or equal
to λ, and ⌊·⌉
(cid:3) rounds to the closest multiple of Λ in D with
ties resolved towards +∞.

The snapping mechanism, parameterized by B and λ, is

deﬁned by the following distribution:

˜f (D) , clampB(⌊clampB(f (D)) ⊕ S ⊗ λ ⊗ LN(U

∗

)⌉

(cid:3)).

Several points are in order. A uniform distribution over
D ∩ (0, 1) can be generated by independently sampling an
exponent (from the geometric distribution with parameter
.5) and a signiﬁcand (by drawing a uniform string from
{0, 1}52). Eﬃcient algorithms for computing natural log-
arithm with exact rounding are known and available [6, 4].
Rounding to the closest multiple of a power of 2 can be done
exactly by manipulating the binary representation of a dou-
ble. Clamping can be done without introducing additional
error.

We additionally assume that the sensitivity of f is 1 (other
values can be handled by scaling f in proportion to its sen-
sitivity).

(1/λ + 2

Theorem 1. The distribution ˜f (·) deﬁned above satisﬁes
−49B/λ)-diﬀerential privacy when λ < B < 246 · λ.
Proof. The structure of the proof is as follows. We ﬁrst
deﬁne and prove diﬀerential privacy of an ideal version of the
mechanism ˜f (·), where all computations are performed per-
fectly with inﬁnite precision and U returns a truly uniform
distribution over (0, 1). We then consider for each possible
output x of the real mechanism the set of doubles from the
support of the U
distribution that are mapped to x, and
prove that its probability mass is well approximated (with a
bounded relative error) by the measure of the corresponding
set of the ideal mechanism.

∗

The ideal mechanism is deﬁned as

˜F (D) , clampB(⌊clampB(f (D)) ± λ ln(U )⌉

(cid:3)),

where U is uniform over (0, 1] and the sign is chosen from
‘+’ and ‘-’ with equal probabilities.

Since clampB is a stable transformation (i.e., if |x−y| ≤ 1,
then |clampB(x) − clampB(y)| ≤ 1), applying it to f be-

fore feeding the result into a diﬀerentially private mecha-
nism preserves the mechanism’s privacy guarantees. Post-
processing the output of the mechanism by snapping it to Λ
and clamping the result does not aﬀect privacy guarantees
either. Therefore, ˜F (·) yields 1/λ-diﬀerential privacy, since
after the inner and outer operations are removed, what is
left is the standard Laplacian mechanism.
Fix the sign in ˜F (D) to be a ‘+’ and similarly ﬁx S = 1 in
˜f (D). The case of the negative sign (and S = −1) is not fully
symmetric due to the direction in which ties are resolved by
the rounding operator, but its treatment is analogous to the
one below.

values mapped by ˜f (D) to x (when S = 1).

Take any x in the support of ˜F (D). Consider the set of U
outputs that are mapped by ˜F (D) to x. Since all operations
of ˜F (D) are monotone (with the sign ﬁxed), this set forms
an interval [L, R) ⊂ (0, 1). Similarly, let [l, r) ⊂ D∩ (0, 1) be
the set of U
We will argue that |L − R| ≈ |l − r| in terms of a relative
error, which translates into ˜f (·)’s guarantee of diﬀerential
privacy.
We may assume that f (D) ∈ [−B, B] (otherwise it would
be forced to the interval by the inner clamping operation).
Since S = 1 and the output of LN(·) is negative, x ≤
⌊f (D)⌉
(cid:3) (the instances
of x = −B and x = ⌊f (D)⌉
(cid:3) can be treated similarly). In
this case, the set of doubles mapped to x by the snapping
operator is precisely [x−Λ/2, x+Λ/2). Denote the endpoints
of this interval by a and b.
Under these assumptions, L and R can be computed di-
rectly: L = exp((a − f (D))/λ) and R = exp((b − f (D))/λ).
Notice that 0 < L < R < 1 and, since b− a = Λ, the ratio

Consider the case when −B < x < ⌊f (D)⌉

∗

(cid:3).

R/L = exp(Λ/λ) < exp(2).

The following lemma allows us to bound the relative er-
ror of replacing the [L, R) interval with [l, r). We do so by
proving upper and lower bounds on r and l.

Lemma 1. Let y ∈ [Λ/2, f (D)). Let u = exp((y−f (D))/λ)
−53 (usually called the machine epsilon). Deﬁne

and η = 2
the following two quantities:

u , exp[(y(1 + η) − f (D))/((1 + η)2λ)],
u , exp[(y/(1 + η) − f (D))(1 + η)2/λ],

then

f (D) ⊕ λ ⊗ LN(u) ≤ y ≤ f (D) ⊕ λ ⊗ LN(u).

Proof Lemma 1. This is the only place in the proof of
the main theorem where we use the facts that ⊕, ⊗, and
LN(·) are all implemented with exact rounding. The exact
rounding means that the result of an operation is correct
within .5ulp of the answer, which translates into the relative
error of 1 + 2

−53 = 1 + η.

The following bounds follow directly from the exact round-

657≥ y.

≤ y.

ing property of arithmetic operations and LN(·):

LN(u) ≥ ln(u)(1 + η)

λ ⊗ LN(u) ≥ λ ln(u)(1 + η)2

(since ln(u) < 0)

f (D) ⊕ λ ⊗ LN(u) ≥ (f (D) + λ ln(u)(1 + η)2)/(1 + η)

(by deﬁnition of u)

Similarly,

LN(u) ≤ ln(u)/(1 + η)

λ ⊗ LN(u) ≤ λ ln(u)/(1 + η)2

f (D) ⊕ λ ⊗ LN(u) ≤ (f (D) + λ ln(u)/(1 + η)2)(1 + η)

(by deﬁnition of u)

[Lemma 1]
Analogous bounds can be derived for y ≤ −Λ/2.

Lemma 2. Quantities y, u, u, u are deﬁned as in the

setting of Lemma 1. Then

exp(−3.1Bη/λ)u < u < u,

u < u < exp(2Bη/λ)u.

pressed as

u/u = exp

Proof Lemma 2. Consider the ratio u/u. It can be ex-

]}
− f (D))(1 + η)2 − (y − f (D))

y

1 + η

(
(−ηy − 2.1f (D)η)

}

[

{
{

1
λ

1
λ

We used (1+η)2 < 1+2.1η and that f (D), y ≤ B. Similarly,

]}

y(1 + η) − f (D)

(1 + η)2

}

− (y − f (D))

> exp
≥ exp(−3.1Bη/λ).
[

u/u = exp

{
{

1
λ

2f (D)η
< exp
≤ exp(2Bη/λ).

λ

We used the bound 1/(1 + η)2 > 1 − 2η. [Lemma 2]

To obtain an upper and lower bounds on |l − r| in terms

of |L − R|, we observe that

L ≤ l ≤ L,
R ≤ r ≤ R,
by applying Lemma 1 twice. Thus,

|L − R| ≤ |l − r| ≤ |L − R|.

(3)

For this we need L < R, which follows from R/L = exp(Λ/λ)
and the bounds of Lemma 2.

Applying the multiplicative bounds of Lemma 2, we ﬁnd

that
R − L > exp(−3.1Bη/λ)R − exp(2Bη/λ)L (by Lemma 2)

= L · (exp(Λ/λ − 3.1Bη/λ) − exp(2Bη/λ))
> L · (exp(Λ/λ) − 1) exp(−7Bη/λ)
= (R − L) exp(−7Bη/λ),

and
R − L < exp(2Bη/λ)R − exp(−3.1Bη/λ)L (by Lemma 2)

= L · (exp(Λ/λ + 2Bη/λ) − exp(−3.1Bη/λ))
< L · (exp(Λ/λ) − 1) exp(5Bη/λ)
= (R − L) exp(5Bη/λ),

(5)
−7
where both inequalities can be veriﬁed given Bη/λ < 2
and 1 ≤ Λ/λ < 2.
Together, the bounds on R − L and R − L, and inequal-
ities (3) imply that |l − r| deviates multiplicatively from
|L − R| by at most exp(7Bη/λ).
(each element of D ∩
(0, 1) is output with probability proportional to its ulp), we
observe that

According to our deﬁnition of U

∗

r − l = Pr[U

∗ ∈ [l, r)],

(6)

when l, r ∈ D ∩ (0, 1].
The only signiﬁcant diﬀerence between the cases of posi-
tive and negative signs in the mechanism ˜F (·) (correspond-
ing to S = +1 and S = −1 in the mechanism ˜f (·)), is due
to the direction of the rounding operation ⌊·⌉
(cid:3) in case of
ties.
It translates in the semiopen interval (l, r] replacing
[l, r) from above. The equality (6) has to be modiﬁed to ac-
count for the case when ulp(r) > ulp(l), which is a common
occurrence:

r − l ≤ Pr[U

∗ ∈ (l, r]]

= r + ulp(r) − (l + ulp(l))
≤ (r − l)(1 + 2η),

(7)

when l < r/2.

The above argument is independent from the actual value
). Call the corresponding

′

of f (D) and applies equally to f (D
quantities l

, R

, L

, r

.

′

′

′

′

Finally, putting these bounds together, we have

Pr[ ˜f (D) = x]

∗ ∈ [l, r) or U

= Pr[U
≤ (r − l)(1 + 2η)
≤ (L − R) exp(5Bη/λ + 2η)

∗ ∈ (l, r]] (depending on the sign)
(by (6) or (7))

(by (3) and (by Lemma 2))

< Pr[ ˜F (D) = x] exp(5Bη/λ + 2η)
< Pr[ ˜F (D

) = x] exp(1/λ + 5Bη/λ + 2η)

′

(by diﬀerential privacy of ˜F )

) exp(1/λ + 5Bη/λ + 2η)

′

) exp(1/λ + 12Bη/λ + 2η)
∗ ∈ [l

) or U

, r

′

′

(by (3) and (by Lemma 2))
∗ ∈ (l, r]] exp(1/λ + 12Bη/λ + 2η)
(by (6) or (7))

= (L

′ − R
′ − r
′
< (l
≤ Pr[U

= Pr[ ˜f (D

′

) = x] exp(1/λ + 12Bη/λ + 2η),

as needed for diﬀerential privacy of the ˜f (·) mechanism. The
expression can be further simpliﬁed by plugging in the nu-
merical value of η = 2

−53 and applying B > λ.

Since the mechanism “snaps” the output of a ﬂoating-point
computation to the nearest multiple of Λ, the error it intro-
duces relative to the underlying Laplacian mechanisms is at
most Λ/2 < λ. Before the outer clamping is applied, the

(4)

658mechanism is unbiased. Depending on the relative magni-
tude of the range of f (·), λ and B, the bias introduced by
clamping may be exceedingly small.

6. SENSITIVITY ANALYSIS

In this section we brieﬂy describe another vulnerability
at the intersection of diﬀerential privacy and ﬂoating-point
arithmetic.

Accurate analysis of sensitivity is essential to most meth-
ods in diﬀerential privacy. The function f may be a combi-
nation of user-provided and trusted codebase, such as Aira-
vat’s user-speciﬁed mappers and system reducers. For both
codebases, subtleties of ﬂoating-point arithmetic may dra-
matically increase sensitivity of the outcome, beyond what
mathematical analysis of the underlying mathematical ab-
straction would suggest.
Consider the following natural example of an aggregation
operator: f (x1, . . . , xn) ,
n
i=1 xi. If the sensitivity of in-
put is 1, i.e., a single user may change at most one input
variable xi by at most 1, then the sensitivity of f is obvi-
ously at most 1. However, f ’s implementation in ﬂoating-
point arithmetic, denoted by f
, has much higher sensitivity
as demonstrated by the following example:

∑

∗

−23, . . . , xn = −2

−23.

Not surprisingly,

∗

f

(x1, . . . , xn) =

xi = 230 − 230 · 2

−23 = 230 − 128.

n = 230 + 1,
x1 = 230,
x2 = −2
n∑

i=1

Let x

1 ⊕ x2
′
′
′
1 = x1 + 1. Since ulp(x
1) = 2
1⊕x2)⊕x3 will again produce
′
′
will result in x
1, computing (x
′
x
1, etc. In this case

−22, computing x

∗

′
′
1 = 230 + 1.
1, x2, . . . , xn) = x

f

(x

∗

The sensitivity of f

—a ﬂoating-point realization of f —on
this pair of inputs is 129 rather than 1. This is a manifes-
tation of the accumulated error phenomenon, which can be
counteracted by application of the Kahan summation algo-
rithm [22, 20]. More complex mechanisms are likely to ex-
hibit similar behavior on adversarially-chosen inputs, com-
plicating the task of validating their privacy claims.

7. RELATED WORK

Modulating the lower order bits of transmitted data is a
well-documented covert channel mechanism, whose primary
use is to enable communication between processes which are
otherwise not allowed to exchange information [25, 16].

The main diﬀerence between our attack and well-known
covert channels is in the intent of the transmitting party. All
platforms surveyed in Section 3 accept untrusted code and
have to confront the threat of covert communications. An
example of a timing attack, to which PINQ and Airavat are
vulnerable, and Fuzz and GUPT explicitly address, involves
a query that takes inordinate amount of time when it is
evaluated on some particular record. Measuring timing of
the response leaks information about the input.

The attack described in this paper does not require ad-
versarial code, or any active malicious intent. Any textbook

implementation of the ﬂoating-point Laplacian mechanism
is potentially vulnerable.

Fixed-point or integer-valued algorithms are immune to
our attack. Examples of such mechanisms include diﬀer-
entially private computations in a distributed setting [11],
privacy-preserving billing and aggregation [5, 24].

The snapping mechanism is closely related to the mecha-
nism proposed by Dodis et al. [7], which defends against an
adversary who exerts some control over the source of ran-
domness. The adversary in this model is capable of introduc-
ing arbitrary bias into the source, as long as each subsequent
bit has some constant entropy conditioned on all previously
output bits. In particular, the bias may depend on the previ-
ous bits and the description of the mechanism. The model is
known as the Santha-Vazirani source [40], and was shown to
imply negative results for existence of certain cryptographic
tasks [29, 8].
In contrast with Dodis et al., we assume a
perfect source of randomness but model inaccuracies due to
use of ﬂoating-point arithmetic.

8. CONCLUSIONS

Floating-point arithmetic is a notoriously leaky abstrac-
tion, which is diﬃcult to argue about formally and hard
to get right in applications. Non-associativity of basic arith-
metic operations, compounding errors, rounding rules, signed
zeros, denormals, NaNs, inﬁnities, CPU ﬂags, and hardware
bug are complicated enough even in the absence of security
concerns.

We initiate study of adapting ﬂoating-point algorithms
to applications in diﬀerential privacy. We describe a prac-
tical, eﬃcient attack on a textbook implementation of the
Laplacian mechanism that underlies all existing platforms
for general purpose diﬀerentially private computations.

We describe and prove a post-processing snapping proce-
dure, which does not perceptible increase the error intro-
duced by the Laplacian mechanism. The snapping mech-
anism is also known to preserve diﬀerential privacy when
instantiated with a class of weak sources of randomness.

In conclusion we observe that ﬂoating-point arithmetic
presents a unique security challenge to developers of real-
world applications.
It is diﬀerent from convenient math-
ematical abstractions in ways that are, on the one hand,
complex and riddled with many corner- and special cases.
On the other hand, its common implementations are stan-
dard, predictable, and deterministic, so that these problem-
atic cases can be forced and exploited by the adversary.

9. ACKNOWLEDGMENTS

The author thanks Yevgeniy Dodis, Frank McSherry, Ben-
jamin Pierce, and Indrajit Roy for their support and valu-
able comments.

10. REFERENCES
[1] R. Bhaskar, A. Bhowmick, V. Goyal, S. Laxman, and
A. Thakurta. Noiseless database privacy. In D. H. Lee
and X. Wang, editors, Advances in
Cryptology—ASIACRYPT 2011, volume 7073 of
Lecture Notes in Computer Science, pages 215–232.
Springer, 2011.

[2] J. Blocki, A. Blum, A. Datta, and O. Sheﬀet. The

Johnson-Lindenstrauss transform itself preserves
diﬀerential privacy. In 53rd Symposium on

659Foundations of Computer Science (FOCS 2012). IEEE
Computer Society, 2012. To appear.

[3] K. Chaudhuri and C. Monteleoni. Privacy-preserving

logistic regression. In D. Koller, D. Schuurmans,
Y. Bengio, and L. Bottou, editors, Advances in Neural
Information Processing Systems 21 (NIPS), pages
289–296. Curran Associates, Inc., 2008.

[4] Correctly rounded mathematical library.

http://lipforge.ens-lyon.fr/www/crlibm/.

[5] G. Danezis, M. Kohlweiss, and A. Rial. Diﬀerentially
private billing with rebates. In Proceedings of the 13th
international conference on Information hiding, IH’11,
pages 148–162, Berlin, Heidelberg, 2011.
Springer-Verlag.

[6] F. de Dinechin, C. Q. Lauter, and J.-M. Muller. Fast
and correctly rounded logarithms in double-precision.
Theoretical Informatics and Applications,
41(1):85–102, 2007.

[7] Y. Dodis, A. L´opez-Alt, I. Mironov, and S. Vadhan.

Diﬀerential privacy with imperfect randomness. In
Advances in Cryptology—CRYPTO 2012, 2012. To
appear. Full
version http://eprint.iacr.org/2012/435.

[8] Y. Dodis, S. J. Ong, M. Prabhakaran, and A. Sahai.

On the (im)possibility of cryptography with imperfect
randomness. In FOCS ’04, pages 196–205. IEEE
Computer Society, 2004.

[9] C. Dwork. Diﬀerential privacy. Invited talk. In

M. Bugliesi, B. Preneel, V. Sassone, and I. Wegener,
editors, Automata, Languages and
Programming—ICALP (2), volume 4052 of Lecture
Notes in Computer Science, pages 1–12. Springer,
2006.

[10] C. Dwork. A ﬁrm foundation for private data analysis.

Commun. ACM, 54(1):86–95, 2011.

[11] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,

and M. Naor. Our data, ourselves: Privacy via
distributed noise generation. In S. Vaudenay, editor,
Advances in Cryptology—EUROCRYPT 2006, volume
4004 of Lecture Notes in Computer Science, pages
486–503. Springer, 2006.

[12] C. Dwork, F. McSherry, K. Nissim, and A. Smith.

Calibrating noise to sensitivity in private data
analysis. In S. Halevi and T. Rabin, editors, Theory of
Cryptography Conference—TCC 2006, volume 3876 of
Lecture Notes in Computer Science, pages 265–284.
Springer, 2006.

[13] C. Dwork and K. Nissim. Privacy-preserving

datamining on vertically partitioned databases. In
M. K. Franklin, editor, Advances in
Cryptology—CRYPTO 2004, volume 3152 of Lecture
Notes in Computer Science, pages 528–544. Springer,
2004.

[14] T. Evans, L. Zayatz, and J. Slanta. Using noise for

disclosure limitation of establishment tabular data. J.
of Oﬃcial Statistics, 14(4):537–551, 1998.

[15] A. Ghosh, T. Roughgarden, and M. Sundararajan.

Universally utility-maximizing privacy mechanisms. In
M. Mitzenmacher, editor, Proceedings of the 41st
Annual ACM Symposium on Theory of Computing,
STOC 2009, pages 351–360. ACM, 2009.

[16] V. D. Gligor. A guide to understanding covert channel

analysis of trusted systems. Technical Report
NCSC-TG-030, National Computer Security Center,
Nov. 1993.

[17] D. Goldberg. What every computer scientist should

know about ﬂoating-point arithmetic. ACM
Computing Surveys, 23(1):5–48, Mar. 1991.

[18] A. Haeberlen, B. C. Pierce, and A. Narayan.

Diﬀerential privacy under ﬁre. In USENIX Security
Symposium. USENIX Association, 2011.

[19] M. Hardt, K. Ligett, and F. McSherry. A simple and

practical algorithm for diﬀerentially private data
release. CoRR, abs/1012.4763, 2010.

[20] N. J. Higham. Accuracy and Stability of Numerical

Algorithms. SIAM: Society for Industrial and Applied
Mathematics, 2nd edition, Aug. 2002.

[21] IEEE standard for ﬂoating-point arithmetic. Technical

report, Microprocessor Standards Committee of the
IEEE Computer Society, 3 Park Avenue, New York,
NY 10016-5997, USA, Aug. 2008.

[22] W. Kahan. Pracniques: further remarks on reducing

truncation errors. Commun. ACM, 8(1):40, Jan. 1965.
[23] D. E. Knuth. Seminumerical Algorithms, volume 2 of
The Art of Computer Programming. Addison-Wesley,
third edition, 1997.

[24] K. Kursawe, G. Danezis, and M. Kohlweiss.

Privacy-friendly aggregation for the smart-grid. In
S. Fischer-H¨ubner and N. Hopper, editors, PETS,
volume 6794 of Lecture Notes in Computer Science,
pages 175–191. Springer, 2011.

[25] B. W. Lampson. A note on the conﬁnement problem.

Communications of the ACM, 16(10):613–615, Oct.
1973.

[26] P. L’Ecuyer. SSJ: Stochastic simulation in Java.
http://www.iro.umontreal.ca/~simardr/ssj/
indexe.html.

[27] C. Li, M. Hay, V. Rastogi, G. Miklau, and

A. McGregor. Optimizing linear counting queries
under diﬀerential privacy. In J. Paredaens and D. V.
Gucht, editors, Symposium on Principles of Database
Systems, PODS 2010, pages 123–134. ACM, 2010.

[28] A. Machanavajjhala, D. Kifer, J. M. Abowd,

J. Gehrke, and L. Vilhuber. Privacy: Theory meets
practice on the map. In G. Alonso, J. A. Blakeley, and
A. L. P. Chen, editors, Proceedings of the 24th
International Conference on Data Engineering, ICDE
2008, pages 277–286. IEEE Computer Society, 2008.
[29] J. L. McInnes and B. Pinkas. On the impossibility of

private key cryptography with weakly random keys. In
A. Menezes and S. A. Vanstone, editors, Advances in
Cryptology—CRYPTO ’90, volume 537 of Lecture
Notes in Computer Science, pages 421–435. Springer,
1991.

[30] F. McSherry. Privacy integrated queries: an extensible

platform for privacy-preserving data analysis. In
U. ¸Cetintemel, S. B. Zdonik, D. Kossmann, and
N. Tatbul, editors, Proceedings of the ACM SIGMOD
International Conference on Management of Data,
SIGMOD 2009, pages 19–30. ACM, 2009.

[31] F. McSherry. Privacy integrated queries: an extensible

platform for privacy-preserving data analysis.
Commun. ACM, 53(9):89–97, 2010.

660[32] F. McSherry and R. Mahajan. Diﬀerentially-private

[37] W. H. Press, S. A. Teukolsky, W. T. Vetterling, and

network trace analysis. In S. Kalyanaraman, V. N.
Padmanabhan, K. K. Ramakrishnan, R. Shorey, and
G. M. Voelker, editors, SIGCOMM, pages 123–134.
ACM, 2010.

[33] F. McSherry and I. Mironov. Diﬀerentially private

recommender systems: Building privacy into the
Netﬂix Prize contenders. In J. F. Elder, IV,
F. Fogelman-Souli´e, P. A. Flach, and M. J. Zaki,
editors, Proceedings of the 15th ACM SIGKDD
International Conference on Knowledge Discovery and
Data Mining—KDD 2009, pages 627–636. ACM, June
2009.

[34] P. Mohan, A. G. Thakurta, E. Shi, D. Song, and D. E.
Culler. GUPT: Privacy preserving data analysis made
easy. In Proceedings of the 38th SIGMOD
international conference on Management of data,
SIGMOD ’12, New York, NY, USA, 2012. ACM.

[35] J.-M. Muller, N. Brisebarre, F. de Dinechin, C.-P.

Jeannerod, V. Lef`evre, G. Melquiond, N. Revol,
D. Stehl´e, and S. Torres. Handbook of Floating-Point
Arithmetic. Birkh¨auser Boston, 2010.

[36] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth
sensitivity and sampling in private data analysis. In
D. S. Johnson and U. Feige, editors, Proceedings of the
39th Annual ACM Symposium on Theory of
Computing (STOC), pages 75–84. ACM, 2007.

B. P. Flannery. Numerical Recipes: The Art of
Scientiﬁc Computing. Cambridge University Press,
New York, NY, USA, third edition, 2007.

[38] J. Reed and B. C. Pierce. Distance makes the types
grow stronger: a calculus for diﬀerential privacy. In
P. Hudak and S. Weirich, editors, Proceeding of the
15th ACM SIGPLAN international conference on
Functional programming, ICFP 2010, pages 157–168.
ACM, 2010.

[39] I. Roy, S. T. V. Setty, A. Kilzer, V. Shmatikov, and

E. Witchel. Airavat: Security and privacy for
MapReduce. In Proceedings of the 7th USENIX
Symposium on Networked Systems Design and
Implementation, NSDI 2010, pages 297–312. USENIX
Association, 2010.

[40] M. Santha and U. V. Vazirani. Generating

quasi-random sequences from semi-random sources. J.
Comput. Syst. Sci., 33(1):75–87, 1986.

661