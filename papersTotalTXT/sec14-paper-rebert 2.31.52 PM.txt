Optimizing Seed Selection for Fuzzing

Alexandre Rebert, Carnegie Mellon University and ForAllSecure; Sang Kil Cha and  

Thanassis Avgerinos, Carnegie Mellon University; Jonathan Foote and David Warren, 

Software Engineering Institute CERT;  Gustavo Grieco, Centro Internacional Franco 

Argentino de Ciencias de la Información y de Sistemas  (CIFASIS) and Consejo Nacional de 
Investigaciones Científicas y Técnicas (CONICET); David Brumley, Carnegie Mellon University

https://www.usenix.org/conference/usenixsecurity14/technical-sessions/presentation/rebert

This paper is included in the Proceedings of the 23rd USENIX Security Symposium.August 20–22, 2014 • San Diego, CAISBN 978-1-931971-15-7Open access to the Proceedings of  the 23rd USENIX Security Symposium is sponsored by USENIXOptimizing Seed Selection for Fuzzing

Alexandre Rebert‡,$
alex@forallsecure.com

Sang Kil Cha‡

sangkilc@cmu.edu

Thanassis Avgerinos‡
thanassis@cmu.edu

Jonathan Foote†
jmfoote@cert.org

David Warren†
dwarren@cert.org

Gustavo Grieco§

gg@cifasis-conicet.gov.ar

David Brumley‡
dbrumey@cmu.edu

‡ Carnegie Mellon University

$ ForAllSecure, Inc.

† Software Engineering Institute CERT

§ CIFASIS-CONICET

Abstract
Randomly mutating well-formed program inputs or sim-
ply fuzzing, is a highly effective and widely used strategy
to ﬁnd bugs in software. Other than showing fuzzers ﬁnd
bugs, there has been little systematic effort in understand-
ing the science of how to fuzz properly. In this paper,
we focus on how to mathematically formulate and reason
about one critical aspect in fuzzing: how best to pick seed
ﬁles to maximize the total number of bugs found during
a fuzz campaign. We design and evaluate six different
algorithms using over 650 CPU days on Amazon Elas-
tic Compute Cloud (EC2) to provide ground truth data.
Overall, we ﬁnd 240 bugs in 8 applications and show that
the choice of algorithm can greatly increase the number
of bugs found. We also show that current seed selection
strategies as found in Peach may fare no better than pick-
ing seeds at random. We make our data set and code
publicly available.

1

Introduction

Software bugs are expensive. A single software ﬂaw
is enough to take down spacecrafts [2], make nuclear
centrifuges spin out of control [17], or recall 100,000s of
faulty cars resulting in billions of dollars in damages [5].
In 2012, the software security market was estimated at
$19.2 billion [12], and recent forecasts predict a steady
increase in the future despite a sequestering economy [19].
The need for ﬁnding and ﬁxing bugs in software before
they are exploited by attackers has led to the development
of sophisticated automatic software testing tools.

Fuzzing is a popular and effective choice for ﬁnding
bugs in applications. For example, fuzzing is used as
part of the overall quality checking process employed by
Adobe [28], Microsoft [14], and Google [27], as well as

by security companies and consultants to ﬁnd bugs and
vulnerabilities in COTS systems.

One reason fuzzing is attractive is because it is rela-
tively straightforward and fast to get working. Given a
target application P, and a set of seed input ﬁles S, the
programmer needs to:

Step 1. Discover the command line arguments to P so
that it reads from a ﬁle. Popular examples include -f,
–file, and using stdin. This step can be manual,
or automated in many cases using simple heuristics
such as trying likely argument combinations.

Step 2. Determine the relevant ﬁle types for an applica-
tion automatically. For example, we are unlikely to
ﬁnd many bugs fuzzing a PDF viewer with a GIF
image. Currently this step is performed manually,
and like the above step the manual process does not
scale to large program bases.

Step 3. Determine a subset of seeds S0 ✓ S to fuzz the
program. For example, an analyst may consider the
possible set of seeds S as every PDF available from
a search engine. Clearly fuzzing on each seed is
computationally prohibitive, thus a seed selection
strategy is necessary. Two typical choices are choos-
ing the set of seed ﬁles ad-hoc, e.g., those immedi-
ately handy, and by ﬁnding the minimal set of seeds
necessary to achieve code coverage.

Step 4. Fuzz the program and reap risk-reducing, or prof-

itable, bugs.

Throughout this paper we assume maximizing the num-
ber of unique bugs found is the main goal. We make no
speciﬁc assumptions about the type of fuzzer, e.g., we
do not assume nor care whether black-box, white-box,
mutational, or any other type of fuzzing is used. For our

USENIX Association  

23rd USENIX Security Symposium  861

1

experiments, we use BFF, a typical fuzzer used in prac-
tice, though the general approach should apply to any
fuzzer using seeds. Our techniques also make no speciﬁc
assumptions about the fuzz scheduling algorithm, thus are
agnostic to the overall fuzzing infrastructure. To evaluate
seed selection strategies, we use popular scheduling algo-
rithms such as round-robin, as well as the best possible
(optimal) scheduling.

We motivate our research with the problem setting of
creating a hypothetical fuzzing testbed for our system,
called COVERSET. COVERSET periodically monitors the
internet, downloads programs, and fuzzes them. The goal
of COVERSET is to maximize the number of bugs found
within a limited time period or budget. Since budgets are
forever constrained, we wish to make intelligent design
decisions that employ the optimal algorithms wherever
possible. How shall we go about building such a system?
Realizing such an intelligent fuzzing system highlights

several deep questions:

Q1. Given millions, billions, or even trillions of PDF
ﬁles, which should you use when fuzzing a PDF
viewer? More generally, what algorithms produce
the best result for seed selection of S0 ✓ S in step 3?
Q2. How do you measure the quality of a seed selection
technique independently of the fuzzing scheduling al-
gorithm? For example, if we ran algorithm A on seed
set S1 and S2, and S1 maximized bugs, we would still
be left with the possibility that with a more intelli-
gent scheduling algorithm A0 would do better with
S2 rather than S1. Can we develop a theory to jus-
tify when one seed set is better than another with
the best possible fuzzing strategy, instead of speciﬁc
examples?

Q3. Can we converge on a "good" seed set for fuzzing
campaigns on programs for a particular ﬁle type?
Speciﬁcally, if S0 performs well on program P1, how
does it work on other similar applications P2,P3, . . .?
If there is one seed set that works well across all
programs, then we would only need to precompute it
once and forever use it to fuzz any application. Such
a strategy would save immense time and effort in
practice. If not, we will need to recompute the best
seed set for each new program.

Our main contribution are techniques for answering the
above questions. To the best of our knowledge, many of
the above problems have not been formalized or studied
systematically. In particular:

(cid:127) We formalize, implement, and test a number of ex-

isting and novel algorithms for seed selection.

(cid:127) We formalize the notion of ex post facto optimality
seed selection and give the ﬁrst strategy that pro-

vides an optimal algorithm even if the bugs found by
different seeds are correlated.

(cid:127) We develop evidence-driven techniques for identi-
fying the quality of a seed selection strategy with
respect to an optimal solution.

(cid:127) We perform extensive fuzzing experiments using
over 650 CPU days on Amazon EC2 to get ground
truth on representative applications. Overall, we ﬁnd
240 unique bugs in 8 widely-used applications, all of
which are on the attack surface (they are often used
to process untrusted input, e.g., images, network
ﬁles, etc.), most of which are security-critical.

While our techniques are general and can be used on
any data set (and are the main contribution of this work),
our particular result numbers (as any in this line of re-
search) are data dependent. In particular, our initial set
of seed ﬁles, programs under test, and time spent test-
ing are all important factors. We have addressed these
issues in several ways. First, we have picked several ap-
plications in each ﬁle type category that are typical of
fuzzing campaigns. This mitigates incorrect conclusions
from a non-representative data set or a particularly bad
program. Second, we have performed experiments with
reasonably long running times (12 hour campaigns per
ﬁle), accumulating over 650 CPU days of Amazon EC2
time. Third, we are making our data set and code avail-
able, so that: 1) others need not spend time and money
on fuzzing to replicate our data set, 2) others can further
analyze the statistics to dig out additional meaning (e.g.,
perform their own hypothesis testing), and 3) we help
lower the barrier for further improvements to the science
of vulnerability testing and fuzzing. For details, please
visit: http://security.ece.cmu.edu/coverset/.

2 Q1: Seed Selection

How shall we select seed ﬁles to use for the fuzzer? For
concreteness, we downloaded a set of seed ﬁles S con-
sisting of 4,912,142 distinct ﬁles and 274 ﬁle types from
Bing. The overall database of seed ﬁles is approximately
6TB. Fuzzing each program for a sufﬁcient amount of
time to be effective across all seed ﬁles is computationally
expensive. Further, sets of seed ﬁles are often duplicative
in the behavior elicited during fuzzing, e.g., s1 may pro-
duce the same bugs as s2, thus fuzzing both s1 and s2 is
wasteful. Which subset of seed ﬁles S0 ✓ S shall we use
for fuzzing?
Several papers [1, 11], presentations from well-
respected computer security professionals [8, 22, 26], as
well as tools such as Peach [9], suggest using executable
code coverage as a seed selection strategy. The intuition is
that many seed ﬁles likely execute the same code blocks,

862  23rd USENIX Security Symposium 

USENIX Association

2

Figure 1: The COVERSET pipeline.

and such seeds are likely to produce the same bugs. For
example, Miller reports a 1% increase in code coverage
increases the percentage of bugs found by .92% [22]. This
intuition can be formalized as an instance of the set cover
problem [1, 11]. Does set cover work? Is the minimal set
cover better than other set covers? Should we weight the
set cover, e.g., by how long it takes to fuzz a particular
seed? Previous work has shown a correlation between
coverage and bugs found, but has not performed compar-
ative studies among a number of approaches, nor studied
how to measure optimality (§ 3).

Recall that in the set cover problem (SCP) [6] we
are given a set X and a ﬁnite list of subsets F =
{S1,S2, . . . ,S n} such that every element of X belongs to
at least one subset of F:

X = [S2F

S

We say that a set C ✓ F is a set cover of X when:

X = [S2C

S

The seed selection strategy is formalized as:

Step 1. The user computes the coverage for each of the
n individual seed ﬁles. The output is the set of code
blocks 1 executed per seed. For example, suppose a
user is given n = 6 seeds such that each seed executes
the following code blocks:

S1 = {1,2,3,4,5,6} S2 = {5,6,8,9}
S4 = {2,5,7,8,11}
S3 = {1,4,7,10}
S6 = {10,11}
S5 = {3,6,9,12}

Step 2. The user computes the cummulative coverage

X =SSi, e.g., X = {1,2, . . . ,12} for the above.

1We assume code blocks, though any granularity of unit such as

instruction, function, etc. also work.

Step 3. The user computes a set cover to output a subset
C of seeds to use in a subsequent fuzzing campaign.
For example, C1 = {S1,S4,S3,S5} is one set cover,
as is C2 = {S3,S4,S5}, with C2 being optimal in the
unweighted case.

The goal of the minimal set cover problem (MSCP) is
to minimize the number of subsets in the set cover C ✓ F.
We call such a set C a minset. Note that a minset need
not be unique, i.e., there may be many possible subsets
of equal minimal cardinality. Each minset represents the
fewest seed ﬁles needed to elicit the maximal set of in-
structions with respect to S, thus represents the maximum
data seed reduction size.

In addition to coverage, we may also consider other
attributes, such as speed of execution, ﬁle size, etc. A
generalization of the set cover is to include a weight w(S)
for each S 2 F. The total cost of a set cover C is:

Cost (C) = Â
S2C

w(S)

The goal of the weighted minimal set cover problem
(WMSCP) is to ﬁnd the minimal cost cover set, i.e.,
argmin

Cost (C).

C
Both the MSCP and WMSCP can be augmented to take
an optional argument k (forming k-SCP and k-WSCP re-
spectively) specifying the maximum size of the returned
solution. For example, if k = 2 then the number of sub-
sets is restricted to at most 2 (|C|  2), and the goal is
to maximize the number of covered elements. Note the
returned set may not be a complete set cover.

Both MSCP and WMSCP are well-known NP-hard
problems. Recall that a common approach to dealing with
NP-hard problems in practice is to use an approximation
algorithm. An approximation algorithm is a polynomial-
time algorithm for approximating an optimal solution.
Such an algorithm has an approximation ratio r(n) if, for
any input of size n, the cost C of the solution produced by
the algorithm is within a factor of r(n) of the cost C⇤ of

USENIX Association  

23rd USENIX Security Symposium  863

3

BingCrawlerSeedSelectionPre-ComputationFuzzing [Online]ProgramsSeeds (S)File Type InferenceFuzzerReduced Seeds ((cid:54)(cid:182))Inferred Seedsan optimal solution. The minimal set cover and weighted
set cover problems both have a greedy polynomial-time
ln|X| + 1-approximation algorithm [4, 16], which is a
threshold below which set cover cannot be approximated
efﬁciently assuming NP does not have slightly super-
polynomial time algorithms, i.e., the greedy algorithm
is essentially the best algorithm possible in terms of the
approximation ratio it guarantees [10]. Since ln|X| grows
relatively slowly, we expect the greedy strategy to be
relatively close to optimal.

The optimal greedy polynomial-time approximation

algorithm2 for WSCP is:

GREEDY-WEIGHTED-SET-COVER(X,F)
1 U = X
2 C = /0
3 while U 6= /0
S = argmax
4
S2F
C = C[ S
U = U\S

5
6
7 return C

|S\U|/w(S)

Note that the unweighted minset can be solved using the
same algorithm by setting 8S : w(S) =1.

2.1 Seed Selection Algorithms
In this section we consider: the set cover algorithm from
Peach [9], a minimal set cover [1], a minimal set cover
weighted by execution time, a minimal set cover weighted
by size, and a hotset algorithm. The ﬁrst two algorithms
have previously been proposed in literature; the remain-
ing are additional design points we propose and evaluate
here. We put these algorithms to the test in our evalu-
ation section to determine the one that yields the best
results (see § 6).

All algorithms take the same set of parameters: given
|F| seed ﬁles, the goal is to calculate a data reduction to k
ﬁles where k ⌧ |F|. We assume we are given t seconds
to perform the data reduction, after which the selected
k ﬁles will be used in a fuzzing campaign (typically of
much greater length than t). We break ties between two
seed ﬁles by randomly choosing one.

PEACH SET. Peach 3.1.53 [9] has a class called MinSet
that calculates a cover set C as follows: 3

2Other algorithms exist to compute the weighted minset (see [6,

35-3.3]).

3This is a high-level abstraction of the Delta and RunCoverage
methods. We checked the Peach implementation after the paper sub-
mission, and noticed that the sorting was removed (At Line 4 of the
algorithm) in their MinSet implementation since Peach 3.1.95.

PEACH-MINSET(P,F)
1 C = /0
i = 1
2
for S in F
3
4
5
6
7
8
9
10

return C

if cov[i]\C 6= /0

C = C[ cov[i]

cov[i] = MeasureCoverage( S )
i = i + 1

sort( cov ) // sort seeds by coverage
for i = 1 to |F|

Despite having the name MinSet, the above routine
does not calculate the minimal set cover nor a proven
competitive approximation thereof.

RANDOM SET. Pick k seeds at random. This approach
serves as a baseline for other algorithms to beat. Since
the algorithm is randomized, RANDOM SET can have
high variance in terms of seed quality and performance.
To measure the effectiveness of RANDOM SET, unless
speciﬁed otherwise, we take the median out of a large
number of runs (100 in our experiments).

HOT SET. Fuzz each seed for t seconds and return the
top k seeds by number of unique bugs found. The ratio-
nale behind HOT SET is similar to multi-armed bandit
algorithms—a buggy program is more likely to have more
bugs. In our experiments, we fuzz each seeds for 5 min-
utes (t = 300) to compute the HOT SET.

UNWEIGHTED MINSET. Use an unweighted k-minset.
This corresponds
to standard coverage-based ap-
proaches [1, 23], and serves as a baseline for measuring
their effectiveness. To compute UNWEIGHTED MINSET
when k is greater than the minimum required to get full
coverage, the minset is padded with ﬁles sorted based
on the quality metric (coverage). We follow the same
approach for TIME MINSET and SIZE MINSET.

TIME MINSET. Return a k-execution time weighted
minset. This algorithm corresponds to Woo et al.’s ob-
servation that weighting by time in a multi-armed bandit
fuzzing algorithm tends to perform better than the un-
weighted version [29]. The intuition is that seeds that
are fast to execute ultimately lead to far more fuzz runs
during a campaign, and thus potentially more bugs.

SIZE MINSET. Return a k-size weighted minset.
Weighting by ﬁle size may change the ultimate minset,
e.g., many smaller ﬁles that cover a few code blocks may
be preferable to one very large ﬁle that covers many code
blocks—both in terms of time to execute and bits to ﬂip.

864  23rd USENIX Security Symposium 

USENIX Association

4

For example, SIZE MINSET will always select a 1KB
seed over a 100MB seed, all other things being equal.

2.2 Speciﬁc Research Questions
Previous wisdom has suggested using UNWEIGHTED
MINSET as the algorithm of choice for computing min-
sets [1, 23]. Is this justiﬁed? Further, computing the
minset requires measuring code coverage. This compu-
tation requires time, time that could be spent fuzzing as
in the HOT SET algorithm. Are coverage-based minsets
beneﬁcial and when?

More precisely, we formulate the following hypothesis:

Hypothesis 1 (MINSET > RANDOM.) Given the same
size parameter k, MINSET algorithms ﬁnd more bugs
than RANDOM SET.

Hypothesis 1 is testing whether the heuristics applied
by the algorithms presented above (§ 2.1) are useful. If
they are as useful as choosing a random set, then the
entire idea of using any of these MINSET algorithms is
fundamentally ﬂawed.

Hypothesis 2 (MINSET Beneﬁts > Cost.) Computing
the MINSET for a given application and set of seed
ﬁles and then starting fuzzing ﬁnds more bugs than just
fuzzing.

Hypothesis 2 tests whether the beneﬁts of the minset
outweigh the cost. Instead of spending time computing
code coverage of seed ﬁles, should we instead spend it
fuzzing? If yes, then the idea of reducing the ﬁles for
every fuzzed application is ﬂawed. It would also imply
that precomputing minsets is necessary for the minsets to
be useful. This observation leads to our next hypothesis.

Hypothesis 3 (MINSET Transferability.) Given appli-
cations A and B that accept the same ﬁletype F, MINSETA
F
ﬁnds the same or more bugs in application B as
MINSETB
F.

Hypothesis 3 tests the transferability of seeds across
applications that accept the same ﬁle type. For example,
is the MINSET for PDF viewer A effective on PDF viewer
B? If yes, we only need to compute a minset once per ﬁle
type, thus saving resources (even if Hypothesis 2 is false).

Hypothesis 4 (MINSET Data Reduction.) Given a tar-
get application A, a set of seed ﬁles F, and a MINSETA
F,
fuzzing with MINSETA
F ﬁnds more bugs than fuzzing with
the entire set F.

Hypothesis 4 tests the main premise of using a reduced
data set. Our MINSET contains fewer bugs than the full
set in total. Under what conditions is the reduction bene-
ﬁcial?

5

2

1

s1

s2

s3

b3

b1

b2

t1,1

c1,2

9

2

10

b2

b4

b2

b1

b3

b1

10

1

10

c3,1

···

···

···

Figure 2: An example of output from fuzzing 3 seeds.
Bugs may be found across seeds (e.g., b1 is found by all
seeds. A single seed may produce the same bug multiple
times, e.g., with s3. We also show the corresponding ILP
variables t (interarrival times) and c (crash ids).

3 Q2: Measuring Selection Quality

There are a variety of seed selection strategies, e.g., to use
minset or pick k seeds at random. How can we argue a
particular seed selection strategy performs well?

One strawman answer is to run seed selection algorithm
A to pick subset SA, algorithm B to pick subset SB. We
then fuzz SA and SB for an equal amount of time and
declare the fuzz campaign with the most bugs the winner.
The fuzz campaign will incrementally fuzz each seed in
each set according to its own scheduling algorithm. While
such an approach may ﬁnd the best seed selection for a
particular fuzzing strategy, it provides no evidence that a
particular subset is inherently better than another in the
limit.

The main intuition in our approach is to measure the
optimal case for bugs found with a particular subset of
seeds. The best case provides an upper bound on any
scheduling algorithm instead of on a particular scheduling
algorithm. Note the lower bound on the number of bugs
found for a subset is trivially zero, thus all we need is an
upper bound.

To calculate the optimal case, we fuzz each seed in si
for t seconds, recording as we fuzz the arrival rate of bugs.
Given n seeds, the total amount of time fuzzing is n⇤ t.
For example, given 3 seeds we may have a bug bi arrival
time given by Figure 2.

Post-fuzzing, we then calculate the ex post facto op-
timal search strategy to maximize the number of bugs
found. It may seem strange at ﬁrst to calculate the op-
timal seed selection strategy after all seeds have been
fuzzed at ﬁrst blush. However, by doing so we can mea-
sure the quality of the seed selection strategy with respect
to the optimal, thus give the desired upper bound. For
example, if the seed selection strategy picks s1 and s2, we
can calculate the maximum number of bugs that could be

USENIX Association  

23rd USENIX Security Symposium  865

5

found by any scheduler, and similarly for the set s2,s3 or
any other set. Note we are calculating the upper bound
to scientiﬁcally justify a particular strategy. For example,
our experiments suggest to use UNWEIGHTED MINSET
for seed selection. During a practical fuzzing campaign,
one would not recompute the upper bound for the new
dataset; instead, she would use the seed selection strategy
that was shown to empirically perform best in previous
tests.

3.1 Formalization
Let Fuzz be a single-threaded fuzzer that takes in a set
of seeds C ✓ F and a time threshold tthres and outputs a
sequence of unique bugs bi along with the seed ﬁles that
triggered them Si and timestamps ti:

Fuzz(C,tthres) ={(b 1,S1,t1), . . . ,(b n,Sn,tn)}

Given that we know the ground truth, i.e., we know
the value of Fuzz when applied on every singleton in F:
Fuzz({Si},tthres) = {(b1,Si,t1), . . . ,(b k,Si,tk)}, we can
model the computation of the optimal scheduling/seed
selection across all seed ﬁles in F. Note that the ground
truth is necessary, since any optimal solution can be only
computed in retrospect (if we know how each seed would
perform). We measure optimality of a scheduling/seed
selection by computing the maximum number of unique
bugs found.

The optimal budgeted ex post

facto scheduling
problem is given the ground truth for a set of
seeds Fuzz({Si},tthres) ={(b 1,Si,t1), . . . ,(b k,Si,tk)} and
a time threshold tthres, automatically compute the inter-
leaving of fuzzed seeds (time slice spent analyzing each
one) to maximize the number of bugs found. The number
of bugs found for a given minset gives an upper bound on
the performance of the set and can be used as a quality in-
dicator. Note that the same bug may be found by different
seeds and may take different amounts of time to ﬁnd.

Finding an optimal schedule for a given ground truth is
currently an open problem. Woo et al. come the closest,
but their algorithm assumes each seed produces indepen-
dent bugs [29]. We observe ﬁnding an optimal scheduling
algorithm is inherently an integer programming problem.
We formulate ﬁnding the exact optimal seed scheduling
as an Integer Linear Programming (ILP) problem. While
computing the optimal schedule is NP-hard, ILP formula-
tions tend to work well in practice.

First, we create an indicator variable for unique bugs

found during fuzzing.

bx =⇢ 1 The schedule includes ﬁnding unique bug x

0 Otherwise

The goal of the optimal schedule is to maximize the
number of bugs. However, we do not see bugs, we see

individual crashes arriving during fuzzing. We create an
indicator variable ci, j that determines whether the optimal
schedule includes the jth crash of seed i:

ci, j =⇢ 1 The schedule includes crash j for seed i

otherwise

0

Note that multiple crashes ci, j may correspond to the
same bug. Crashes are triaged to unique bugs via a unique-
ness function denoted by µ.
In our experiments, we
use stack hash [24], a non-perfect but industry standard
method. Thus, if the total number of unique stack hashes
is U, we say we found U unique bugs in total. The invari-
ant is:

bx = 1 iff 9 i, j : µ(ci, j) =x

(1)
Thus, if two crashes ci, j and ci0, j0 have the same hash, a
schedule can get at most one unique bug by including
either or both crashes.

Finally, we include a cost for ﬁnding each bug. We
associate with each crash the incremental fuzzing cost for
seed Si to ﬁnd the bug:

8i : ti, j =⇢ ai,1

ai, j − ai, j−1

, j = 1
, j > 1

where ai, j is the arrival time for the ci, j crash, and ti, j
represents interarrival time—the time interval between the
occurrences of ci, j−1 and ci, j. Figure 2 visually illustrates
the connection between ci, j, bx and ti, j.
We are now ready to phrase optimal scheduling with a

ﬁxed time-budget as an ILP maximization problem:

maximize

subject to

x

Â
8i, j
Â
8i, j
8x

i, j

bx

. ci, j+1  ci, j
ci, j ·ti, j  tthres
. ci, j  bx where µ(ci, j) =x
. bx  Â

ci, j where µ(ci, j) =x

i, j

(2)

(3)

(4)

(5)

Constraint (2) ensures that the schedule considers the
order of crashes found. In particular, if the j-th crash of
a seed is found, all the previous crashes must be found
as well. Constraint (3) ensures that the time to ﬁnd all
the crashes does not exceed our time budget tthres. Con-
straints (4) and (5) link crashes and unique bugs. Con-
straints (4) says that if a crash is found, its corresponding
bug (based on stack-hash) is found, and the next equa-
tion guarantees that if a bug is found, at least one crash
triggering this bug was found.

866  23rd USENIX Security Symposium 

USENIX Association

6

Additionally, by imposing one extra inequality:

Â

i

ci,1  k

(6)

we can bound the number of used seeds by k (if the ﬁrst
crash of a seed is not found, there is no value in fuzzing
the seed at all), thus getting k-bounded optimal budgeted
scheduling, which gives us the number of bugs found with
the optimal minset of size up to k.

Optimal Seed Selection for Round-Robin. The for-
mulation for optimal budgeted scheduling gives us a best
solution any scheduling algorithm could hope to achieve
both in terms of seeds to select (minset) and interleaving
between explored seeds (scheduling). We can also model
the optimal seed selection for speciﬁc scheduling algo-
rithms with the ILP formulation. We show below how
this can be achieved for Round-Robin, as this may be of
independent interest.

Round-Robin scheduling splits the time budget be-
tween the seeds equally. Given a time threshold tthres
and N seeds, each seed will be fuzzed for tthres
N units of
time. Round-Robin is a simple but effective scheduling
algorithm in many adversarial scenarios [29]. Simulating
Round-Robin for a given set of seeds is straightforward,
but computing the optimal subset of seeds of size k with
Round-Robin cannot be solved with a polynomial algo-
rithm. To obtain the optimal minset for Round-Robin, we
add the following inequality to Inequalities 2-6:

ci, j ·ti, j 

tthres
k

8i

. Â

j

(7)

The above inequality ensures that none of the seeds
will be explored for more than tthres
time units, thus guar-
k
anteeing that our solution will satisfy the Round-Robin
constraints. Similar extensions can be used to obtain
optimal minsets for other scheduling algorithms.

4 Q3: Transferability of Seed Files

Precomputing a good seed set for a single application P1
may be time intensive. For example, the ﬁrst step in a
minset-based approach is to run each seed dynamically to
collect coverage information. Collecting this information
may not be cheap. Collecting coverage data often requires
running the program in a dynamic analysis environment
like PIN [18] or Valgrind [25], which can slow down
execution by several orders of magnitude. In our own
experiments, collecting coverage information on our data
set took 7 hours. One way COVERSET could minimize
overall cost is to ﬁnd a “good” set of seeds and reuse them
from one application to another.

There are several reasons to believe this may work.
One reason is most programs rely on only a few libraries
for PDF, image, and text processing. For example, if ap-
plication P1 and P2 both link against the poppler PDF
library, both applications will likely crash on the same
inputs. However, shared libraries are typically easy to
detect, and such cases may be uninteresting. Suppose
instead P1 and P2 both have independent implementations,
e.g., P1 uses poppler and P2 uses the GhostScript graph-
ics library. One reason P1 and P2 may crash on similar
PDFs is that there are intrinsically hard portions of the
PDF standard to implement right, thus both are likely to
get it wrong. However, one could speculate any number
of reasons the bugs in applications would be independent.
To the best of our knowledge, there has been no previous
systematic investigation to resolve this question when the
bugs are found via fuzzing.

5 System Design

A precondition to fuzzing is conﬁguring the fuzzer to take
the seed ﬁle as input. In this step, we are given the entire
database of seeds and a particular program under test P.
To fuzz, we must:

1. Recover P’s command line options.

2. Determine which argument(s) causes P to read in
from the fuzzing source. For simplicity, we focus
on reading from a ﬁle, but the general approach may
work with other fuzzing sources.

3. Determine the proper ﬁle type, e.g., giving a PDF
reader a PDF as a seed is likely to work better than
giving a GIF. We say a ﬁle type is valid for a program
if it does non-trivial processing on the ﬁle.

Current fuzz campaigns typically require a human to spec-
ify the above values. For our work, we propose a set of
heuristics to help automate the above procedure.

In our approach, we ﬁrst use simple heuristics to infer
likely command lines. The heuristics try the obvious
and common command line arguments for accepting ﬁles,
e.g., -f file. We also brute force common help options
(e.g., –help) and parse the output for additional possible
command line arguments.

As we recover command lines, we check if they cause
P to read from a ﬁle as follows. We create a unique ﬁle
name x, run P x, and monitor for system calls that open
x.

In our data set we have 274 different ﬁle types, such
as JPEG, GIF, PNG, and video ﬁles. Once we know the
proper way to run P with seeds, the question becomes
which seed types should we give P? We infer appropriate
ﬁle types based on the following hypothesis: if s is a seed

USENIX Association  

23rd USENIX Security Symposium  867

7

handled by P and s0 is not, then we expect the coverage of
P(s) > P(s0). This hypothesis suggests an efﬁcient way
to infer ﬁle types accepted by an application. First, create
a set of sample ﬁle type seeds F, where each element
consists of a seed for a unique ﬁle type. Second, for each
si 2 F, count the number of basic blocks executed by
P(si). Third, select the top candidate (or candidates if
desired) by total execution blocks. Though simple, we
show in our evaluation this strategy works well in practice.

6 Experiments

We now present our experiments for checking the validity
of the hypotheses introduced in § 2.2 and evaluate the
overall performance in terms of bug discovered of COV-
ERSET. We start by describing our experimental setup.

Experimental Setup. All of our experiments were run
on medium and small VM instance types on Amazon
EC2 (the type of the instance used is mentioned in every
experiment). All VMs were running the same operating
system, Debian Linux 7.4. The fuzzer used throughout
our experiments is the CERT Basic Fuzzing Framework
(BFF) [15]. All seed ﬁles gathered for our fuzzing ex-
periments (4,912,142 ﬁles making up more than 6TB of
data) were automatically crawled from the internet us-
ing the Bing API. Speciﬁcally, ﬁle type information was
extracted from the open source Gnome Desktop applica-
tion launcher data ﬁles and passed to the Bing API such
that ﬁles of each type could be downloaded, ﬁltered, and
stored on Amazon S3. Coverage data was gathered by
instrumenting applications using the Intel PIN framework
and a standard block-based coverage collection PIN tool.

6.1 Establishing Ground Truth
To test the MINSET hypotheses, we need to obtain the
ground truth (recall from § 3.1) for a fuzzing cam-
paign that accounts for every possible seed selection and
scheduling. We now present our methodology for select-
ing the target applications, ﬁles to fuzz, and parameters
for computing the ground truth.

Target Applications. We selected 10 applications and
5 popular ﬁle formats: PDF, MP3, GIF, JPG and PNG
for our experiments. Our program selection contains GUI
and command line applications, media viewers, players,
and converters. We manually mapped each program to a
ﬁle format it accepts and formed 13 distinct (application,
ﬁle formats) to be fuzzed—shown in Table 2. We selected
at least two distinct command lines for each ﬁle type to
test transferability (Hypothesis 3).

Seed Files. For each ﬁle type used by the target appli-
cations, we sampled uniformly at random 100 seed ﬁles
(hence selecting |F| = 100 for the seed ﬁle pool size) of
the corresponding type from our seed ﬁle database. Note
that determining ground truth for a single seed requires 12
hours, thus ﬁnding ground truth on all 4,912,142 is—for
our resources—infeasible.

Fuzzing Parameters. Each of the target applications
was fuzzed for 12 hours with each of the 100 randomly
selected seed ﬁles of the right ﬁle type. Thus, each target
application was fuzzed for 1,200 hours for a total of 650
CPU-days on an EC2 (m1.small) instance. All detected
crashes were logged with timestamps and triaged based
on BFF’s stack hash algorithm.

The end result of our ground truth experiment is a log

of crashes for each (seed ﬁle, application) tuple:

BFF({Si},tthres = 12h) ={(b 1,Si,t1), . . . ,(b k,Si,tk)}

Fuzzing results. BFF found 2,941 unique crashes, iden-
tiﬁed by their stack hash. BFF crashed 8 programs out
of the 10 target applications. 2,702 of the unique crashes
were found on one application, mp3gain. Manual in-
spection showed that the crashes were due to a single
exploitable buffer overﬂow vulnerability that mangled the
stack and confused BFF’s stack-based uniqueness algo-
rithm. When reporting our results, we therefore count
the 2,702 unique crashes in mp3gain as one. With that
adjustment, BFF found 240 bugs. Developing and experi-
menting with more robust, effective, and accurate triaging
algorithms is an open research problem and a possible
direction for future work.

Simulation. The parameters of the experiment allow
us to run simulations and reason about all possible seed
selections (among the 100 seeds of the application) and
scheduling algorithms for a horizon of 12 hours on a
single CPU. Our simulator uses our ILP formulation
from § 3 to compute optimal seed selections and schedul-
ing for a given time budget. Using the ground truth,
we can run simulations to evaluate the performance of
hour-long fuzzing campaigns within minutes, following a
replay-based fuzzing simulation strategy similar to FUZ-
ZSIM [29].

We used the simulator and ran a set of experiments
to answer the following three questions: 1) how good
are seed selection algorithms when compared against
RANDOM SET (§ 6.2) and when compared against each
other (§ 6.2.1)?, 2) can we reuse reduced sets across pro-
grams (§ 6.3)?, and 3) can our algorithm correctly identify
ﬁle types for applications (§ 6.4)?

868  23rd USENIX Security Symposium 

USENIX Association

8

Optimal

Round−Robin

y
t
i
l
i

b
a
b
o
r
P

1

0

hotset minset peach sminset tminset

hotset minset peach sminset tminset

Algorithm

Better

Heuristic

Random

Same

Figure 3: Comparing bug-ﬁnding performance of seed
selection algorithms against RANDOM SET.

6.2 Are Seed Selection Algorithms Better

than Random Sampling?

Spending resources on a seed selection algorithm is only
useful if the selected seeds outperform random seed sam-
pling (RANDOM SET). In this experiment, we compare
the performance of selection algorithms as presented
in §2.1 against the random sampling baseline.

All selection algorithms are deterministic, while RAN-
DOM SET is randomized. Thus, we cannot show that
RANDOM SET is always better (or worse), but we can
instead compute the probability that RANDOM SET is
better (or worse). To estimate the probability, we setup
the following random experiment: we randomly sample
a set of seeds—the size of the set is the same (k = 10
in our experiment for an order of magnitude reduction)
as the competing reduced set—from the seed pool and
measure the number of bugs found. The experiment has
three possible outcomes: 1) the random set ﬁnds more
bugs, 2) the random set ﬁnds fewer bugs, or 3) the random
and the competitive set ﬁnd the same number of bugs.

We performed 13,000 repetitions of

the above
experiment—1,000 for each (application, ﬁle format)
tuple—and measured the frequency of each event when
the optimal scheduling algorithm is employed for both.
We then repeated the same experiment while using Round-
Robin as the scheduling algorithm. We calculated the
probability by dividing the frequency by the number of
samples. Figure 3 summarizes the results. For instance,
the left-most bar is the result for HOT SET with the op-
timal scheduling. You can see that HOT SET ﬁnds more
bugs than a RANDOM SET of the same size with a prob-
ability of 32.76%, and it is worse with a probability of
18.57%. They ﬁnd the same amount of bugs with a proba-
bility of 48.66%.

The ﬁrst pattern that seems to persist through schedul-
ing and selection algorithms (based on Figure 3) is that

HOT SET
PEACH SET
UNWEIGHTED MINSET
SIZE MINSET
TIME MINSET

Optimal Round-Robin
67.12%
63.58%
60.30%
50.64%
70.24%
75.24%
66.33%
75.78%
57.62%
52.60%

Table 1: Conditional probability of an algorithm outper-
forming RANDOM SET with k=10, given that they do not
have the same performance (Pwin).

there is a substantial number of ties—RANDOM SET
seems to behave as well as selection algorithms for the ma-
jority of the experiments. This is not surprising, since 3/13
(23%) of our (application, ﬁle format) combinations—
(mplayer, MP3), (eog, JPG), (jpegtran, JPG)—do not
crash at all. With no crash to ﬁnd, any algorithm will be
as good as random. Thus, to compare an algorithm to
RANDOM SET we focus on the cases where the two algo-
rithms differ, i.e., we compute the conditional probability
of winning when the two algorithms are not ﬁnding the
same number of bugs.

We use Pwin to denote the conditional probability of
an algorithm outperforming RANDOM SET, given that
they do not have the same performance. For example,
for SIZE MINSET, Pwin is deﬁned as: P[SIZE MINSET >
RANDOM SET | SIZE MINSET 6= RANDOM SET]. Ta-
ble 1 shows the values of Pwin for all algorithms for
sets of size k = 10. We see that UNWEIGHTED MINSET
and SIZE MINSET are the algorithms that more consis-
tently outperform RANDOM SET with a Pwin ranging from
66.33% to 75.78%. HOT SET immediately follows in the
63-67% range, and TIME MINSET, PEACH SET have the
worst performance. Note that PEACH SET has a Pwin of
50.64% in the optimal schedule effectively meaning that
it performs very close to a random sample on our dataset.

Conclusion: seed selection algorithms help. With the
exception of the PEACH SET and TIME MINSET algo-
rithms which perform very close to RANDOM SET, our
data shows that heuristics employed by seed selection
algorithms perform better than fully random sampling.
Thus, hypothesis 1 seems to hold. However, the bug dif-
ference is not sufﬁcient to show that any of the selection
algorithms is strictly better with statistical signiﬁcance.
Fuzzing for longer and/or obtaining the ground truth for a
larger seed pool are possible future directions for show-
ing that seed selection algorithms are strictly better than
choosing at random.

USENIX Association  

23rd USENIX Security Symposium  869

9

Files

PDF

MP3

GIF

JPG

PNG

Programs Crashes
706
xpdf
6,570
mupdf
pdf2svg
5,720
1
ffmpeg
0
mplayer
434,400
mp3gain
eog
9
72
convert
162,302
gif2png
0
eog
0
jpegtran
eog
123
2
convert
609,905

Total

Bugs
57
88
81
1
0
2,702
1
2
6
0
0
2
1
2,941

RANDOM
SET
#S
10
10
10
10
10
10
10
10
10
10
10
10
10

#B
7
13
14
0
0
92
0
1
4
0
0
1
0
132

HOT
SET
#S
10
10
10
10
10
10
10
10
10
10
10
10
10

#B
9
14
27
1
0
9
1
1
4
0
0
1
0
67

UNWEIGHTED
MINSET
#S
32
40
36
11
10
9
29
13
16
31
10
30
11
278

#B
19
29
48
0
0
150
0
1
5
0
0
2
1
255

TIME
MINSET
#S
32
43
39
11
12
8
27
14
17
31
12
30
12
288

#B
16
29
43
0
0
74
0
0
5
0
0
2
1
170

SIZE
MINSET
#S
40
49
45
22
14
10
43
24
29
47
21
45
17
406

#B
19
31
47
0
0
74
1
2
5
0
0
2
1
182

PEACH
SET
#S
54
59
53
19
23
14
44
22
33
53
23
49
16
462

#B
31
31
49
0
0
175
1
1
4
0
0
2
1
295

Table 2: Programs fuzzed to evaluate seed selection strategies and obtain ground truth. The columns include the number
of seed ﬁles (#S) obtained with each algorithm, and the number of bugs found (#B) with the optimal scheduling strategy.

6.2.1 Which Algorithm Performed Best?

Table 2 shows the full breakdown of the reduced sets
computed by each algorithm with the optimal schedul-
ing algorithm. Columns 1 and 2 show the ﬁle type and
program we are analyzing, while columns 3 and 4 show
the total number of crashes and unique bugs (identiﬁed
by stack hash) found during the ground truth experiment.
The next six columns show two main statistics (in sub-
columns) for each of the seed selection algorithms: 1) the
size of the set k (#S), and 2) the number of bugs (#B) iden-
tiﬁed with optimal scheduling. All set cover algorithms
(PEACH SET, UNWEIGHTED MINSET, TIME MINSET,
SIZE MINSET) were allowed to compute a full-cover, i.e.,
select as many ﬁles as required to cover all blocks. The
other two algorithms (RANDOM SET and HOT SET) were
restricted to sets of size k = 10.

Bug Distribution and Exploitability. The fuzzing
campaign found bugs in 10/13 conﬁgurations of
hprogram,ﬁle typei, as shown in table 2. In 9/10 conﬁgu-
rations we found less than 100 bugs, with one exception:
mp3gain. We investigated the outlier further, and discov-
ered that our fuzzing campaign identiﬁed an exploitable
stack overﬂow vulnerability—the mangled stack trace can
create duplicates in the stack hash algorithm. We veriﬁed
the bug is exploitable and notiﬁed the developers, who
promptly ﬁxed the issue.

Reduced Set Size. Table 2 reﬂects the ability of the set
cover algorithms to reduce the original dataset of 100 ﬁles.
As expected, UNWEIGHTED MINSET is the best in terms

20

s
g
u
b
#

10

0

optimal
minset
tminset
peach

random

sminset
hotset

0

5

10

15

20

25

30

35

# of files

Figure 4: Number of bugs found by different seed selec-
tion algorithms with optimal scheduling.

of reduction ability, with 278 ﬁles for obtaining full cover.
TIME MINSET requires slightly more ﬁles (288). SIZE
MINSET and PEACH SET require almost twice as many
ﬁles to obtain full cover (406 and 462 respectively).

Bug Finding. The PEACH SET algorithm ﬁnds the high-
est number of bugs (295), followed by UNWEIGHTED
MINSET (255), SIZE MINSET (182) and TIME MINSET
(170). HOT SET and RANDOM SET ﬁnd substantially
fewer bugs when restricted to subsets of size up to 10.
We emphasize again that bug counts are measured under
optimal scheduling and thus size of the reduced set is
analogous to the performance of the selection algorithm
(the highest number of bugs will be found when all seeds
are selected). Thus, to compare sets of seeds in terms of
bug-ﬁnding ability we need a head to head comparison
where sets have the same size k.

870  23rd USENIX Security Symposium 

USENIX Association

10

20

15

s
g
u
b
#

10

5

0

optimal
minset
tminset
sminset
random
peach
hotset

0

5

10

15

20

25

30

35

# of files

Figure 5: Number of bugs found by different seed selec-
tion algorithms with Round-Robin.

Figure 4 shows all selection algorithms and how they
perform in terms of average number of bugs found as a
function of the parameter k—the size of the seed ﬁle set.
The “⇥” symbols represent the size after which each algo-
rithm achieves a full cover (after that point extra ﬁles are
added sorted by the metric of the selection algorithm, e,g„
by coverage in UNWEIGHTED MINSET). As witnessed
in the comparison against RANDOM SET, UNWEIGHTED
MINSET consistently performs better than other seed se-
lection algorithms. TIME MINSET and PEACH SET also
eventually converge to the performance of UNWEIGHTED
MINSET under optimal scheduling, closely followed by
random. HOT SET performs the worst, showing that
spending time exploring all seeds can be wasteful. We
also note, that after obtaining full cover (at 20 seed ﬁles),
UNWEIGHTED MINSET’s performance does not improve
at the same rate—showing that adding new ﬁles that do
not increase code coverage is not beneﬁcial (even with
optimal scheduling).

We performed an additional simulation, where all re-
duced sets were run with Round-Robin as the scheduling
algorithm. Figure 5 shows the performance of each al-
gorithm as a function of the parameter k. Again, we
notice that that UNWEIGHTED MINSET is outperform-
ing the other algorithms. More interestingly, we also
note that UNWEIGHTED MINSET’s performance actually
drops after obtaining full cover. This shows that mini-
mizing the number of seeds is important; adding more
seeds in Round-Robin seems to hurt performance for all
algorithms.

Conclusion: UNWEIGHTED MINSET performed best.
UNWEIGHTED MINSET outperformed all other algo-
rithms in our experiments, both for optimal and Round-
Robin scheduling. This experiment conﬁrms conventional
wisdom that suggests collecting seeds with good coverage
for successful fuzzing. More importantly, it also shows
that computing a minimal cover with an approximation
with a proven competitiveness ratio (UNWEIGHTED MIN-

File

Application

PDF

MP3

GIF

JPG
PNG eog

xpdf
mupdf
pdf2svg
ffmpeg
mplayer
mp3gain
eog
convert
gif2png
eog
jpegtran

convert

FULL
SET

53%
83%
71%
1%
0%
95%
8%
12%
97%
0%
0%
22%
2%

UNWEIGHTED
MINSET (k=10)
70%
90%
80%
0%
0%
100%
0%
10%
100%
0%
0%
30%
10%

Table 3: Probability that a seed will produce a bug in 12
hours of fuzzing.

SET) is better than using an algorithm with no guaranteed
competitive ratio (PEACH SET).

6.2.2 Are reduced seed sets better than a full set?

Hypothesis 4 tests the premise of using a reduced data set.
Will a reduced set of seeds ﬁnd more bugs than the full
set? We simulated a fuzzing campaign with the full set,
and with different reduced sets. We compare the number
of bugs found by each technique.

Using the optimal scheduling, the full set will always
ﬁnd more, or the same amount of bugs, than any subsets
of seeds. Indeed, the potential schedules of the full set
is a superset of the potentiel schedules of any reduced
set. The optimal schedule of a reduced set of seeds is a
valid schedule of the full set, but the optimal schedule
of the full set might not be a valid schedule of a reduced
set. Hypothesis 4 is therefore false under the optimal
scheduling. We use a Round-Robin schedule to answer
this question more realistically.

The “⇥” symbols on Figure 5 shows the unpadded
size of the different selection algorithms. For those sizes,
UNWEIGHTED MINSET found 4 bugs on average, and the
other MINSET algorithms found between 2.5 and 3 bugs.
Fuzzing with the full set uncovered only 1 unique bug on
average.

We also measure the quality of a set of seeds by looking
at the average seed quality contained in that set. Our
hypothesis is that a reduced set increases the average seed
quality compared to the full set. To measure quality, we
computed the probability of a seed producing a bug after
fuzzing it for 12 hours, when the seed is picked from the
full set or the UNWEIGHTED MINSET. Table 3 lists the

USENIX Association  

23rd USENIX Security Symposium  871

11

results of this experiment. The UNWEIGHTED MINSET
had a higher seed quality than the full set in 7 cases, while
the opposite was true in 3 cases. They were tied on the 3
remaining cases.

Conclusion: Fuzzing with a reduced sets is more ef-
ﬁcient in practice. The UNWEIGHTED MINSET out-
performed the full set in our two experiments. Our data
demonstrates that using seed selection techniques is bene-
ﬁcial to fuzzing campaigns.

6.3 Are Reduced Sets Reusable Across Pro-

grams?

We showed that seed selection algorithms improve
fuzzing in terms of bug-ﬁnding performance. However,
performing the data reduction may be computationally
expensive; for instance, all set cover algorithms require
collecting coverage information for all the seeds. Is it
more proﬁtable to invest time computing the minset to
fuzz an efﬁcient reduced set, or to simply fuzz the full set
of seeds for the full time budget? In other words, is the
seed selection worth the effort to be performed online?

We answer that question by presenting parts of our
dataset. For example, our JPG bucket contains 530,727
distinct ﬁles crawled from the web. Our PIN tool requires
55 seconds (on average based on the 10 applications listed
in Table 2) to compute code coverage for a single seed.
Collecting coverage statistics for all our JPG ﬁles would
take 368 CPU-days. For fuzzing campaigns shorter than
a year, there would not be enough time to compute code
coverage, let alone ﬁnding more bugs than the full set.

The result above indicates that, while seed selection
techniques help improve the performance of fuzzing, their
beneﬁts may not outweigh the costs. It is impractical to
spend a CPU year of computation to perform a sepa-
rate seed selection for every new application that needs
fuzzing, thus indicating that Hypothesis 2 does not hold.
However, recomputing the reduced set for every appli-
cation may not be necessary. Instead, we can compute a
reduced set for every ﬁle type. Our intuition is a reduced
set that is of high-quality for application A should also
be high-quality for application B—assuming they accept
the same ﬁle type. Thus, precomputing reduced sets for
popular ﬁle types once, would allow us to instantly select
a high-quality set of seed ﬁles to start fuzzing. To test
transferability of reduced sets (Hypothesis 3), we measure
seed quality by computing code coverage achieved by a
MINSET across programs.

Do Reduced Sets Transfer Coverage? Using the seed
ﬁles from our ground truth experiment (§ 6.1) we mea-
sured the cumulative code coverage achieved in each

conﬁguration (program and ﬁle format) with reduced
UNWEIGHTED MINSETs computed on all other con-
ﬁgurations (for a total of 13⇥ 13⇥ 100 coverage mea-
surements). All measurements were performed on a
c1.medium instance on amazon.

Figure 6 is a heat map summarizing our results. The
conﬁgurations on the bottom (x-axis) represent all com-
puted UNWEIGHTED MINSETs, while the conﬁgura-
tions on the left (y-axis) represent the conﬁgurations
tested. Darker colors indicate that the selected UN-
WEIGHTED MINSET obtains higher coverage. For ex-
ample, if we select the pdf.mupdf MINSET from the
x-axis, we can see how it performs on all the other con-
ﬁgurations on the y-axis. For instance, we notice that
pdf.mupdf MINSET performs noticeably better on 5 con-
ﬁgurations: pdf.mupdf (expected since this is the conﬁg-
uration on which we computed the MINSET), pdf.xpdf
and pdf.pdf2svg (expected since these applications
also accept pdfs), and interestingly png.convert and
gif.convert. Initially we were surprised that a PDF
MINSET would perform so well on convert; it turns
out that this result is not surprising since convert can
also process PDF ﬁles. Similar patterns can be similarly
explained—for example, GIF MINSETs are performing
better than MP3 MINSETs for mplayer, simply because
mplayer can render GIF images.

The heat map allows us to see two clear patterns:

1. High coverage indicates the application accepts a
ﬁle type. For instance, by following the row of the
gif.eog conﬁguration we can immediately see that
eog accepts GIF, JPG, and PNG ﬁles, while it does
not process MP3s or PDFs. This is exactly the same
pattern we are exploiting in our ﬁle type inference
algorithm (§ 6.4).

2. Coverage transfers across applications that process
the same ﬁle type. For example, we clearly see
the PDF cluster forming across all PDF conﬁgura-
tions, despite differences in implementations. While
xpdf and pdf2svg both use the poppler library
for processing PDFs, mupdf has a completely in-
dependent implementation. Nevertheless, mupdf’s
MINSET performs well on xpdf and vice versa. Our
data shows that similar clusters appear throughout
conﬁgurations of the same ﬁle type, suggesting that
we can reuse MINSETs across applications that ac-
cept the same ﬁle type (Hypothesis 3).

Conclusion: Reduced sets are transferable. Our data
suggests that reduced sets can be transferred to programs
parsing the same ﬁle types with respect to code coverage.
Therefore, it is necessary to compute only one reduced
set per ﬁle type.

872  23rd USENIX Security Symposium 

USENIX Association

12

PNG

PDF

MP3

JPG

png.eog
png.convert
pdf.xpdf
pdf.pdf2svg
pdf.mupdf
mp3.mplayer
mp3.mp3gain
mp3.ffmpeg
jpg.jpegtran
jpg.eog
gif.gif2png
gif.eog
gif.convert

GIF

gif.convert

gif.eog

gif.gif2png

jpg.eog

jpg.jpegtran

mp3.ffmpeg

mp3.mp3gain

mp3.mplayer

pdf.mupdf

pdf.pdf2svg

pdf.xpdf

png.convert

png.eog

Figure 6: Transferability of UNWEIGHTED MINSET coverage across conﬁgurations. The base conﬁgurations, on which
the reduced sets were computed, are on the bottom; the tested conﬁgurations are on the left. Darker colors indicate
higher coverage.

Inferring File Types

6.4
In this experiment we ran our algorithm to automatically
infer ﬁle types for our 10 applications over a large body of
diverse ﬁle types (88 in total including AVI, MP3, MKV
and so forth). We then manually veriﬁed the inferred ﬁle
type, and measured accuracy. We report an error if a re-
ported ﬁle type is not recognized by the target application.
Table 6.4 summarizes our results. Our ﬁle type inference
algorithm successfully infers ﬁle types for every program
except mp3gain, where the ﬁle type inferred was CBR
(Comic Book Reader), instead of MP3.

We manually examined why mp3gain shows higher
code coverage for CBR ﬁles. It turns out that our sample
CBR ﬁle is larger than 15 MB, and it happens to have a
valid MP3 frame header signature in the middle of the ﬁle.
Since mp3gain searches for a valid MP3 frame header
regardless of the entire ﬁle format, it is possible to misin-
terpret an input ﬁle as a valid MP3 ﬁle. In other words,
this is probably not a false positive, because mp3gain
indeed takes in the CBR ﬁle and outputs a modiﬁed CBR
ﬁle, which is the expected behavior of the program.

7 Discussion & Future Work

Input Types. The focus of this paper is on ﬁle-parsing
applications. Thus, we do not target applications that
use command line arguments as their input sources (e.g.,
/bin/echo), or applications that receive input from the
network (e.g., /usr/bin/wget). File-based vulnerabilities

Program Inferred File Type
convert
eog
ffmpeg
gif2png
jpegtran
mp3gain
mplayer
mupdf
pdf2svg
xpdf

svg
png
divx
gif
jpeg
cbr
avi
pdf
pdf
pdf

Success

3
3
3
3
3
7
3
3
3
3

Table 4: File-type inference results on our dataset.

represent a signiﬁcant attack vector, since remote attacks
can be carried out by simply sending an attachment to the
victim over the network.

Handling argument inputs for applications is straight-
forward: we can extend our fuzzing framework to ran-
domly generate arguments to the exec system call. Treat-
ing network applications would be more elaborate since
we would have to update our seed database to include
network packets for various protocol types. One potential
extension is to utilize automatic protocol reversing [3, 7].
We leave it as future work to support more input types.

Statistical Signiﬁcance. We have performed initial hy-
pothesis testing based on the data. Currently, using UN-
WEIGHTED MINSET is assumed to outperform other al-

USENIX Association  

23rd USENIX Security Symposium  873

13

gorithms. Using our data (see Figure 3), we were able
to show that UNWEIGHTED MINSET is at least as good
as random with high probability. However, the data does
not show with statistical signiﬁcance that UNWEIGHTED
MINSET is strictly better. Fuzzing longer and with more
seed ﬁles and programs may yield more datapoints that
would allow a stronger conclusion—at the cost of a much
more costly ground truth computation: an additional seed
ﬁle requires 12 hours of extra fuzzing hours for each ap-
plication. We leave fuzzing for more than 650 days as
future work.

Command Line Inference. Currently, COVERSET uses
several heuristics to infer command line arguments. We
believe that command line inference is an important ﬁrst
step to fully automate the entire fuzzing process. For
example, COVERSET currently does not handle dependent
arguments, e.g., when option A is valid only when option
B is also selected. Developing systematic and effective
approaches for deriving command line arguments—e.g.,
based on white-box techniques—is a possible direction
for future work.

8 Related Work

In early 90s, Miller et al. [21] introduced the term
fuzzing. Since then, it has become one of the most widely-
deployed technique for ﬁnding bugs. There are two major
categories in fuzzing based on its ability to examine the
internal of the software under test: (1) black-box fuzzing
[20], and (2) white-box fuzzing [13, 14]. In this paper,
we use black-box mutational fuzzing as the underlying
technique for our data reduction algorithms.

Coverage-driven seed selection is not new. Several pa-
pers and security practitioners use similar heuristics to se-
lect seeds for fuzzing [1, 8, 9, 11, 22, 23, 26]. FuzzSim by
Woo et al. [29] is the closest work from academia, where
they tackle a seed scheduling problem using multi-armed
bandit algorithms. Our paper differs from their approach
in that we are not developing an online scheduling algo-
rithm, but an ofﬂine data-driven seed selection approach.
Therefore, our seed selection is complementary—when it
is used as a preprocessing step—to the FuzzSim schedul-
ing algorithm.

There are several previous works on recovering input
formats, which involves dynamic taint analysis [3, 7].
Our goal is being able to run fuzzing with appropriate
seed ﬁles, but not recovering the semantics of ﬁle for-
mat. Accommodating more precise ﬁle format inference
techniques is out of the scope of this paper.

9 Conclusion

In this paper we designed and evaluated six seed selection
techniques. In addition, we formulated the optimal ex
post facto seed selection scheduling problem as an integer
linear programming problem to measure the quality of
seed selection algorithms. We performed over 650 days
worth of fuzzing to determine ground truth values and
evaluated each algorithm. We found 240 new bugs. Our
results suggest how best to use seed selection algorithms
to maximize the number of bugs found.

Acknowledgments

We would like to thank Alan Hall, and our anonymous
reviewers for their comments and suggestions. This work
is supported in part by the NSF-CNS0953751, DARPA
CSSG-FA9750-10-C-0170, and the SEI-FA8721-05-C-
0003. This work reﬂects only the opinions of the authors,
not the sponsors.

References
[1] ABDELNUR, H., LUCANGELI, O., AND FESTOR, O. Spectral

fuzzing: Evaluation & feedback.

[2] ARIANE. The ariane catastrophe. http://www.around.com/

ariane.html.

[3] CABALLERO, J., YIN, H., LIANG, Z., AND SONG, D. Polyglot:
Automatic extraction of protocol message format using dynamic
binary analysis. In Proceedings of the ACM Conference on Com-
puter & Communications Security (2007).

[4] CHVATAL, V. A greedy heuristic for the set-covering problem.

Mathematics of Operations Research 4, 3 (1979), 233–235.

[5] CNN. Toyota recall costs: $2 billion. http://money.cnn.com/

2010/02/04/news/companies/toyota_earnings.cnnw/
index.htm, 2010.

[6] CORMEN, T. H., LEISERSON, C. E., AND RIVEST, R. L. Intro-

duction to Algorithms , Second Edition, vol. 7. 2001.

[7] CUI, W., PEINADO, M., AND CHEN, K. Tupni: Automatic
reverse engineering of input formats. In Proceedings of the 15th
ACM Conference on Computer and Communications Security
(2008).

[8] DURAN, D., WESTON, D., AND MILLER, M. Targeted taint

driven fuzzing using software metrics. In CanSecWest (2011).

[9] EDDINGTON, M. Peach fuzzer. http://peachfuzzer.com/.

[10] FEIGE, U. A threshold of lnn for approximating set cover. Journal

of the ACM 45, 4 (1998), 634–652.

[11] FRENCH, T., AND PROJECT, V. Closed loop fuzzing algorithms.

[12] GARTNER. Software security market at 19.2 billion. http://

www.gartner.com/newsroom/id/2500115, 2012.

[13] GODEFROID, P., LEVIN, M. Y., AND MOLNAR, D. Automated
whitebox fuzz testing. In Network and Distributed System Security
Symposium (2008), no. July.

874  23rd USENIX Security Symposium 

USENIX Association

14

[14] GODEFROID, P., LEVIN, M. Y., AND MOLNAR, D. Sage: White-
box fuzzing for security testing. Communications of the ACM 55,
3 (2012), 40–44.

[15] HOUSEHOLDER, A. D., AND FOOTE, J. M. Probability-based
parameter selection for black-box fuzz testing. Tech. Rep. August,
CERT, 2012.

[16] JOHNSON, D. S. Approximation algorithms for combinatorial
problems. Journal of Computer and System Sciences 9 (1974),
256–278.

[17] LANGNER, R. Stuxnet: Dissecting a cyberwarfare weapon. IEEE

Security & Privacy Magazine 9, 3 (May 2011), 49–51.

[18] LUK, C.-K., COHN, R., MUTH, R., PATIL, H., KLAUSER, A.,
LOWNEY, G., WALLACE, S., REDDI, V. J., AND HAZELWOOD,
K. Pin: Building customized program analysis tols with dynamic
instrumentation. In Programming Language Design and Imple-
mentation (2005), ACM, pp. 190–200.

[19] MARKET RESEARCH MEDIA. U.s. federal cybersecurity mar-
ket forecast 2013-2018. http://www.marketresearchmedia.
com/?p=206, 2013.

[20] MCNALLY, R., YIU, K., AND GROVE, D. Fuzzing : The state of

the art.

[21] MILLER, B. P., FREDRIKSEN, L., AND SO, B. An empirical
study of the reliability of unix utilities. Communications of the
ACM 33, 12 (1990), 32–44.

[22] MILLER, C. Fuzz by number. In CanSecWest (2008).

[23] MILLER, C. Babysitting an army of monkeys. In CanSecWest

(2010).

[24] MOLNAR, D., LI, X., AND WAGNER, D. Dynamic test generation
to ﬁnd integer bugs in x86 binary linux programs. In Proceedings
of the USENIX Security Symposium (2009), pp. 67–82.

[25] NETHERCOTE, N., AND SEWARD, J. Valgrind: A program
supervision framework. Electronic Notes in Theoretical Computer
Science 89, 2 (Oct. 2003), 44–66.

[26] OPSTAD, L., AND MOLNAR, D. Effective fuzzing strategies.

Tech. rep., 2010.

[27] TEAM, C. S. Clusterfuzz. https://code.google.com/p/

clusterfuzz/.

[28] UHLEY, P.

A basic distributed fuzzing framework for
https://blogs.adobe.com/security/2012/05/

foe.
a-basic-distributed-fuzzing-framework-for-foe.
html.

[29] WOO, M., CHA, S. K., GOTTLIEB, S., AND BRUMLEY, D.
Scheduling black-box mutational fuzzing. In Proceedings of the
2013 ACM Conference on Computer & Communications Security
(2013), pp. 511–522.

USENIX Association  

23rd USENIX Security Symposium  875

15

